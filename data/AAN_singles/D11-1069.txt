Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 748?758,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsClassifying Sentences as Speech Acts in Message Board PostsAshequl Qadir and Ellen RiloffSchool of ComputingUniversity of UtahSalt Lake City, UT 84112{asheq,riloff}@cs.utah.eduAbstractThis research studies the text genre of mes-sage board forums, which contain a mix-ture of expository sentences that present fac-tual information and conversational sentencesthat include communicative acts between thewriter and readers.
Our goal is to createsentence classifiers that can identify whethera sentence contains a speech act, and canrecognize sentences containing four differentspeech act classes: Commissives, Directives,Expressives, and Representatives.
We con-duct experiments using a wide variety of fea-tures, including lexical and syntactic features,speech act word lists from external resources,and domain-specific semantic class features.We evaluate our results on a collection of mes-sage board posts in the domain of veterinarymedicine.1 IntroductionIn the 1990?s, the natural language processing com-munity shifted much of its attention to corpus-basedlearning techniques.
Since then, most of the text cor-pora that have been annotated and studied are collec-tions of expository text (e.g., news articles, scientificliterature, etc.).
The intent of expository text is topresent or explain information to the reader.
In re-cent years, there has been a growing interest in textgenres that originate from Web sources, such as we-blogs and social media sites (e.g., tweets).
Thesetext genres offer new challenges for NLP, such asthe need to handle informal and loosely grammaticaltext, but they also pose new opportunities to studydiscourse and pragmatic phenomena that are funda-mentally different in these genres.Message boards are common on the WWW as aforum where people ask questions and post com-ments to members of a community.
They are typ-ically devoted to a specific topic or domain, such asfinance, genealogy, or Alzheimer?s disease.
Somemessage boards offer the opportunity to pose ques-tions to domain experts, while other communitiesare open to anyone who has an interest in the topic.From a natural language processing perspective,message board posts are an interesting hybrid textgenre because they consist of both expository textand conversational text.
Most obviously, the conver-sations appear as a thread, where different peoplerespond to each other?s questions in a sequence ofposts.
Studying the conversational threads, however,is not the focus of this paper.
Our research addressesthe issue of conversational pragmatics within indi-vidual message board posts.Most message board posts contain both exposi-tory sentences as well as speech acts.
The personposting a message (the ?writer?)
often engages inspeech acts with the readers.
The writer may explic-itly greet the readers (?Hi everyone!?
), request helpfrom the readers (?Anyone have a suggestion??
), orcommit to a future action (?I promise I will reportback soon.?).
But most posts contain factual infor-mation as well, such as general knowledge or per-sonal history describing a situation, experience, orpredicament.Our research goals are twofold: (1) to distin-guish between expository sentences and speech actsentences in message board posts, and (2) to clas-748sify speech act sentences into four types: Com-missives, Directives, Expressives, and Representa-tives, following Searle?s original taxonomy (Searle,1976).
Speech act classification could be usefulfor many applications.
Information extraction sys-tems could benefit from filtering speech act sen-tences (e.g., promises and questions) so that facts areonly extracted from the expository text.
IdentifyingDirective sentences could be used to summarize thequestions being asked in a forum over a period oftime.
Representative sentences could be extractedto highlight the conclusions and beliefs of domainexperts in response to a question.In this paper, we present sentence classifiers thatcan identify speech act sentences and classify themas Commissive, Directive, Expressive, and Repre-sentative.
First, we explain how each speech actclass is manifested in message board posts, whichcan be different from how they occur in spoken dia-logue.
Second, we train classifiers to identify speechact sentences using a variety of lexical, syntactic,and semantic features.
Finally, we evaluate our sys-tem on a collection of message board posts in thedomain of veterinary medicine.2 Related WorkThere has been relatively little work on applyingspeech act theory to written text genres, and mostof the previous work has focused on email classi-fication.
Cohen et al (2004) introduced the notionof ?email speech acts?
defined as specific verb-nounpairs following a pre-designed ontology.
They ap-proached the problem as a document classificationtask.
Goldstein and Sabin (2006) adopted this no-tion of email acts (Cohen et al, 2004) but focusedon verb lexicons to classify them.
Carvalho andCohen (2005) presented a classification scheme us-ing a dependency network, capturing the sequentialcorrelations with the context emails using transitionprobabilities from or to a target email.
Carvalho andCohen (2006) later employed N-gram sequence fea-tures to determine which N-grams are meaningfullyrelated to different email speech acts with a goaltowards improving their earlier email classificationbased on the writer?s intention.Lampert et al (2006) performed speech act clas-sification in email messages following a verbal re-sponse modes (VRM) speech act taxonomy.
Theyalso provided a comparison of VRM taxonomy withSearle?s taxonomy (Searle, 1976) of speech actclasses.
They evaluated several machine learning al-gorithms using syntactic, morphological, and lexi-cal features.
Mildinhall and Noyes (2008) presenteda stochastic speech act model based on verbal re-sponse modes (VRM) to classify email intentions.Some research has considered speech act classesin other means of online conversations.
Twitchelland Jr. (2004) and Twitchell et al (2004) employedspeech act profiling by plotting potential dialoguecategories in a radar graph to classify conversa-tions in instant messages and chat rooms.
Nas-tri et al (2006) performed an empirical analysis ofspeech acts in the away messages of instant mes-senger services to achieve a better understanding ofthe communication goals of such services.
Raviand Kim (2007) employed speech act profiling inonline threaded discussions to determine messageroles and to identify threads with questions, answers,and unanswered questions.
They designed their ownspeech act categories based on their analysis of stu-dent interactions in discussion threads.The work most closely related to ours is the re-search of Jeong et al (2009) on semi-supervisedspeech act recognition in both emails and forums.Like our work, their research also classifies indi-vidual sentences, as opposed to entire documents.However, they trained their classifier on spokentelephone (SWBD-DAMSL corpus) and meeting(MRDA corpus) conversations and mapped the la-belled dialog act classes of these corpora to 12 di-alog act classes that they found suitable for emailand forum text genres.
These dialog act classes (ad-dressed as speech acts by them) are somewhat differ-ent from Searle?s original speech act classes.
Theyalso used substantially different types of featuresthan we do, focusing primarily on syntactic subtreestructures.3 Classifying Speech Acts in MessageBoard Posts3.1 Speech Act Class DefinitionsSearle?s (Searle, 1976) early research on speech actswas seminal work in natural language processingthat opened up a new way of thinking about con-749versational dialogue and communication.
Our goalwas to try and use Searle?s original speech act def-initions and categories as the basis for our work tothe greatest extent possible, allowing for some inter-pretation as warranted by the WWW message boardtext genre.For the purposes of defining and evaluating ourwork, we created detailed annotation guidelines forfour of Searle?s speech act classes that commonlyoccur in message board posts: Commissives, Direc-tives, Expressives, and Representatives.
We omittedthe fifth of Searle?s original speech act classes, Dec-larations, because we virtually never saw declara-tive speech acts in our data set.1 The data set used inour study is a collection of message board posts inthe domain of veterinary medicine.
We designed ourdefinitions and guidelines to reflect language use inthe text genre of message board posts, trying to be asdomain-independent as possible so that these defini-tions should also apply to message board texts rep-resenting other topics.
However, we give examplesfrom the veterinary domain to illustrate how thesespeech act classes are manifested in our data set.Commissives: A Commissive speech act oc-curs when the speaker commits to a future courseof action.
In conversation, common Commissivespeech acts are promises and threats.
In messageboards, these types of Commissives are relativelyrare.
However, we found many statements where themain purpose was to confirm to the readers that thewriter would perform some action in the future.
Forexample, a doctor may write ?I plan to do surgery onthis patient tomorrow?
or ?I will post the test resultswhen I get them later today?.
We viewed such state-ments as implicit commitments to the reader aboutintended actions.
We also considered decisions notto take an action as Commissive speech acts (e.g., ?Iwill not do surgery on this cat because it would betoo risky.?).
However, statements indicating that anaction will not occur because of circumstances be-yond the writer?s control were considered to be fac-tual statements and not speech acts (e.g., ?I cannotdo an ultrasound because my machine is broken.?
).Directives: A Directive speech act occurs when1Searle defines Declarative speech acts as statements thatbring about a change in status or condition to an object by virtueof the statement itself.
For example, a statement declaring waror a statement that someone is fired.the speaker expects the listener to do something asa response.
For example, the speaker may ask aquestion, make a request, or issue an invitation.
Di-rective speech acts are common in message boardposts, especially in the initial post of each threadwhen the writer explicitly requests help or advice re-garding a specific topic.
Many Directive sentencesare posed as questions, so they are easy to identifyby the presence of a question mark.
However, thelanguage in message board forums is informal andoften ungrammatical, so many Directives are posedas a question but do not end in a question mark (e.g.,?What do you think.?).
Furthermore, many Direc-tive speech acts are not stated as a question but asa request for assistance.
For example, a doctor maywrite ?I need your opinion on what drug to give thispatient.?
Finally, some sentences that end in ques-tion marks are rhetorical in nature and do not repre-sent a Directive speech act, such as ?Can you believethat?
?.Expressives: An Expressive speech act occurs inconversation when a speaker expresses his or herpsychological state to the listener.
Typical cases arewhen the speaker thanks, apologizes, or welcomesthe listener.
Expressive speech acts are common inmessage boards because writers often greet readersat the beginning of a post (?Hi everyone!?)
or ex-press gratitude for help from the readers (?I reallyappreciate the suggestions.?).
We also found Ex-pressive speech acts in a variety of other contexts,such as apologies.Representatives: According to Searle, a Rep-resentative speech act commits the speaker to thetruth of an expressed proposition.
It represents thespeaker?s belief of something that can be evaluatedto be true or false.
These types of speech acts wereless common in our data set, but some cases did ex-ist.
In the veterinary domain, we considered sen-tences to be a Representative speech act when adoctor explicitly confirmed a diagnosis or expressedtheir suspicion or hypothesis about the presence (orabsence) of a disease or symptom.
For example, if adoctor writes that ?I suspect the patient has pancre-atitis.?
then this represents the doctor?s own propo-sition/belief about what the disease might be.Many sentences in our data set are stated as factbut could be reasonably inferred to be speech acts.For example, suppose a doctor writes ?The cat has750pancreatitis.?.
It would be reasonable to infer thatthe doctor writing the post diagnosed the cat withpancreatitis.
And in many cases, that is true.
How-ever, we saw many posts where that inference wouldhave been wrong.
For example, the following sen-tence might say ?The cat was diagnosed by a pre-vious vet but brought to me due to new complica-tions?
or ?The cat was diagnosed with it 8 yearsago as a kitten in the animal shelter?.
Consequently,we were very conservative in labelling sentences asRepresentative speech acts.
Any sentence presentedas fact was not considered to be a speech act.
A sen-tence was only labelled as a Representative speechact if the writer explicitly expressed his belief.3.2 Features for Speech Act ClassificationTo create speech act classifiers, we designed a vari-ety of lexical, syntactic, and semantic features.
Wetried to capture linguistic properties associated withspeech act expressions as well as discourse prop-erties associated with individual sentences and themessage board post as a whole.
We also incorpo-rated speech act word lists that were acquired fromexternal resources, and used two types of seman-tic features to represent semantic entities associatedwith the veterinary domain.
Except for the semanticfeatures, all of our features are domain-independentso should be able to recognize speech act sentencesacross different domains.
We experimented withdomain-specific semantic features to test our hy-pothesis that Commissive speech acts can be asso-ciated with domain-specific semantic entities.For the purposes of analysis, we partition the fea-ture set into three groups: Lexical and Syntactic(LexSyn) Features, Speech Act Clue Features, andSemantic Features.
Unless otherwise noted, all ofthe features had binary values indicating the pres-ence or absence of that feature.3.2.1 Lexical and Syntactic FeaturesWe designed a variety of features to capture lexicaland syntactic properties of words and sentences.
Wedescribed the feature set below, with the features cat-egorized based on the type of information that theycapture.Unigrams: We created bag-of-word features rep-resenting each unigram in the training set.
Numberswere replaced with a special # token.Personal Pronouns: We defined three features tolook for the presence of a 1st person pronoun, 2ndperson pronoun, and 3rd person pronoun.
We in-cluded the subjective, objective, and possessive formof each pronoun (e.g., he, him, and his).Tense: Speech acts such as Commissives can berelated to tense.
We created three features to iden-tify verb phrases that occur in the past, present, orfuture tense.
To recognize tense, we followed therules defined by Allen (1995).Tense + Person: We created four features that re-quire the presence of a first person subjective pro-noun (I, we) within a two word window on the left ofa verb phrase matching one of four tense representa-tions: past, present, future, and present progressive(a subset of the more general present tense represen-tation).Modals: One feature indicates whether the sen-tence contains a modal (may, must, shall, will,might, should, would, could).Infinitive VP: One feature looks for an infinitiveverb phrase (?to?
followed by a verb) that is precededby a first person pronoun (I, we) within a three wordwindow on the left.
This feature tries to capturecommon Commissive expressions (e.g., ?I definitelyplan to do the test tomorrow.?
).Plan Phrases: Commissives are often expressedas a plan, so we created a feature that recognizesfour types of plan expressions: ?I am going to?, ?Iam planning to?, ?I plan to?, and ?My plan is to?.Sentence contains Early Punctuation: One fea-ture checks for the following punctuation markswithin the first three tokens of the sentence: , : !
Thisfeature was designed to recognize greetings, such as:?Hi,?
, or ?Hiya everyone !
?.Sentence begins with Modal/Verb: One featurechecks if a sentence begins with a modal or verb.The intuition is to capture interrogative and impera-tive sentences, since they are likely to be Directives.Sentence begins with WH Question: One fea-ture checks if a sentence begins with a WH questionword (Who, When, Where, What, Which, What,How).Neighboring Question: One feature checkswhether the following sentence contains a questionmark ???.
We observed that in message boards, Di-rectives often occur in clusters.751Sentence Position: Four binary features repre-sent the relative position of the sentence in the post.One feature indicates whether it is the first sentence,one feature indicates whether it is the last sentence,one feature indicates whether it is the second to lastsentence, and one feature indicates whether the sen-tence occurs in the bottom 25% of the message.
Themotivation for these features is that Expressives of-ten occur at the beginning and end of the post, andDirectives tend to occur toward the end.Number of Verbs: One feature represents thenumber of verbs in the sentence using four possiblevalues: 0, 1, 2, >2.
Some speech acts classes (e.g.,Expressives) may occur with no verbs, and rarelyoccur in long, complex sentences.3.2.2 Speech Act Word CluesWe collected speech act word lists (mostly verbs)from two external sources.
In Searle?s original pa-per (Searle, 1976), he listed words that he consid-ered to be indicative of speech acts.
We discardeda few that we considered to be overly general, andwe added a few additional words.
We also collecteda list of speech act verbs published in (Wierzbicka,1987).
The details for these speech act clue lists aregiven below.
Our system recognized all derivationsof these words.Searle Keywords: We created one feature foreach speech act class.
The Representative keywordswere: (hypothesize, insist, boast, complain, con-clude, deduce, diagnose, and claim).
We discarded 3words from Searle?s list (suggest, call, believe) andadded 2 new words, assume and suspect.
The Direc-tive keywords were: (ask, order, command, request,beg, plead, pray, entreat, invite, permit, advise,dare, defy, challenge).
We added the word please.The Expressives keywords were: (thank, apolo-gize, congratulate, condole, deplore, welcome).
Weadded the words appreciate and sorry.
Searle didnot provide any hint on possible indicator words forCommissives, so we manually defined five likelyCommissive keywords: (plan, commit, promise, to-morrow, later).Wierzbicka Verbs: We created one feature thatincluded 228 speech act verbs listed in the book?English speech act verbs: a semantic dictionary?
(Wierzbicka, 1987)2.3.2.3 Semantic FeaturesAll of the previous features are domain-independent and should be useful for identifyingspeech acts sentences across many domains.
How-ever, we hypothesized that semantic entities maycorrelate with speech acts within a particular do-main.
For example, consider medical domains.
Rep-resentative speech acts may involve diagnoses andhypotheses regarding diseases and symptoms.
Sim-ilarly, Commissive speech acts may reveal a doc-tor?s plan or intention regarding the administrationof drugs or tests.
Thus, it may be beneficial for aclassifier to know whether a sentence contains cer-tain semantic entities.
We experimented with twodifferent sources of semantic information.Semantic Lexicon: Basilisk (Thelen and Riloff,2002) is a bootstrapping algorithm that has beenused to induce semantic lexicons for terrorist events(Thelen and Riloff, 2002), biomedical concepts(McIntosh, 2010), and subjective/objective nounsfor opinion analysis (Riloff et al, 2003).
Weran Basilisk over our collection of 15,383 veteri-nary message board posts to create a semantic lex-icon for veterinary medicine.
As input, Basiliskrequires seed words for each semantic category.To obtain seeds, we parsed the corpus using anoun phrase chunker, sorted the head nouns by fre-quency, and manually identified the 20 most fre-quent nouns belonging to four semantic categories:DISEASE/SYMPTOM, DRUG, TEST, and TREAT-MENT.However, the induced TREATMENT lexicon wasof relatively poor quality so we did not use it.
TheDISEASE/SYMPTOM lexicon appeared to be of goodquality, but it did not improve the performance ofour speech act classifiers.
We suspect that this is dueto the fact that diseases were not distinguised fromsymptoms in our lexicon.3 Representative speechacts are typically associated with disease diagnoses2openlibrary.org/b/OL2413134M/English_speech_act_verbs3We induced a single lexicon for diseases and symptoms be-cause it is difficult to draw a clear line between them seman-tically.
A veterinary consultant explained to us that the sameterm (e.g., diabetes) may be considered a symptom in one con-text if it is secondary to another condition (e.g., pancreatitis) buta disease in a different context if it is the primary diagnosis.752and hypotheses, rather than individual symptoms.In the end, we only used the DRUG and TEST se-mantic lexicon in our classifiers.
We used all 1000terms in the DRUG lexicon, but only used the top200 TEST words because the quality of the lexiconseemed questionable after that point.Semantic Tags: We also used bootstrapped con-textual semantic taggers (Huang and Riloff, 2010)that had been previously trained for the domain ofveterinary medicine.
These taggers assign seman-tic class labels to noun phrase instances based onthe surrounding context in a sentence.
The tag-gers were trained on 4,629 veterinary message boardposts using 10 seed words for each semantic cate-gory (see (Huang and Riloff, 2010) for details).
Toensure good precision, only tags that have a confi-dence value ?
1.0 were used.
Our speech act classi-fiers used the tags associated with two semantic cat-egories: DRUG and TEST.3.3 ClassificationTo create our classifiers, we used the Weka (Hall etal., 2009) machine learning toolkit.
We used Sup-port Vector Machines (SVMs) with a polynomialkernel and the default settings supplied by Weka.Because a sentence can include multiple speech acts,we created a set of binary classifiers, one for each ofthe four speech act classes.
All four classifiers wereapplied to each sentence, so a sentence could be as-signed multiple speech act classes.4 Evaluation4.1 Data SetOur data set consists of message board posts fromthe Veterinary Information Network (VIN), which isa web site (www.vin.com) for professionals in vet-erinary medicine.
Among other things, VIN hostsmessage board forums where veterinarians and otherveterinary professionals can discuss issues and posequestions to each other.
Over half of the small an-imal veterinarians in the U.S. and Canada use theVIN web site.We obtained 15,383 VIN message board threadsrepresenting three topics: cardiology, endocrinol-ogy, and feline internal medicine.
We did basiccleaning, removing html tags and tokenizing num-bers.
We then applied the Stanford part-of-speechtagger (Toutanova et al, 2003) to each sentence toobtain part-of-speech tags for the words.
For our ex-periments, we randomly selected 150 message boardthreads from this collection.
Since the goal of ourwork was to study speech acts in sentences, and notthe conversational dialogue between different writ-ers, we used only the initial post of each thread.These 150 message board posts contained a total of1,956 sentences, with an average of 13.04 sentencesper post.
In the next section, we explain how wemanually annotated each sentence in our data set tocreate gold standard speech act labels.4.2 Gold Standard AnnotationsTo create training and evaluation data for our re-search, we asked two human annotators to manuallylabel sentences in our message board posts.
Iden-tifying speech acts is not always obvious, even topeople, so we gave them detailed annotation guide-lines describing the four speech act classes discussedin Section 3.1.
Then we gave them the same set of50 message board posts from our collection to an-notate independently.
Each annotator was told toassign one or more speech act classes to each sen-tence (COM, DIR, EXP, REP), or to label the sen-tence as having no speech acts (NONE).
The vastmajority of sentences had either no speech acts orat most one speech act, but a small number of sen-tences contained multiple types of speech acts.We measured the inter-annotator agreement of thetwo human judges using the kappa (?)
score (Car-letta, 1996).
However, kappa agreement scores areonly applicable to labelling schemes where each in-stance receives a single label.
Therefore we com-puted kappa agreement in two different ways to lookat the results from two different perspectives.
In thefirst scheme, we discarded the small number of sen-tences that had multiple speech act labels and com-puted kappa on the rest.4 This produced a kappascore of .95, suggesting extremely high agreement.However, over 70% of the sentences in our data sethave no speech act at all, so NONE was by far themost common label.
Consequently, this agreementscore does not necessarily reflect how consistentlythe judges agreed on the four speech act classes.4Of the 594 sentences in these 50 posts, only 22 sentencescontained multiple speech act classes.753In the second scheme, we computed kappa foreach speech act category independently.
For eachcategory C, the judges were considered to be inagreement if both of them assigned category C tothe sentence or if neither of the judges assigned cat-egory C to the sentence.
Table 1 shows the ?
agree-ment scores using this approach.Speech Act Kappa (?)
scoreExpressive .97Directive .94Commissive .81Representative .77Table 1: Inter-annotator (?)
agreementInter-annotator agreement was very high for boththe Expressive and Directive classes.
Agreementwas lower for the Commissive and Representativeclasses, but still relatively good so we felt comfort-able that we had high-quality annotations.To create our final data set, the two judges adjudi-cated their disagreements on this set of 50 posts.
Wethen asked each annotator to label an additional (dif-ferent) set of 50 posts each.
All together, this gaveus a gold standard data set consisting of 150 anno-tated message board posts.
Table 2 shows the distri-bution of speech act labels in our data set.
71% ofthe sentences did not include any speech acts.
Thesewere usually expository sentences containing factualinformation.
29% of the sentences included one ormore speech acts, so nearly 13 of the sentences wereconversational in nature.
Directive and Expressivespeech acts are by far the most common, with nearly26% of all sentences containing one of these speechacts.
Commissive and Representative speech actsare less common, each occurring in less than 3% ofthe sentences.54.3 Experimental Results4.3.1 Speech Act FilteringFor our first experiment, we created a speech actfiltering classifier to distinguish sentences that con-tain one or more speech acts from sentences that donot contain any speech acts.
Sentences labelled as5These numbers do not add up to 100% because some sen-tences contain multiple speech acts.Speech Act # sentences distributionNone 1397 71.42%Directive 311 15.90%Expressive 194 9.92%Representative 57 2.91%Commissive 51 2.61%Table 2: Speech act class distribution in our data set.having one or more speech acts were positive in-stances, and sentences labelled as NONE were neg-ative instances.
Speech act filtering could be usefulfor many applications, such as information extrac-tion systems that only seek to extract facts.
For ex-ample, information may be posed as a question (ina Directive) rather than a fact, information may bementioned as part of a future plan (in a Commis-sive) that has not actually happened yet, or informa-tion may be stated as a hypothesis or suspicion (in aRepresentative) rather than as a fact.We performed 10-fold cross validation on our setof 150 annotated message board posts.
Initially, weused all of the features defined in Section 3.2.
How-ever, during the course of our research we discov-ered that only a small subset of the lexical and syn-tactic features seemed to be useful, and that remov-ing the unnecessary features improved performance.So we created a subset of minimal lexsyn features,which will be described in Section 4.3.2.
For speechact filtering, we used the minimal lexsyn featuresplus the speech act clues and semantic features.6Class P R FSpeech Act .86 .83 .84No Speech Act .93 .95 .94Table 3: Precision, Recall, F-measure for speech act fil-tering.Table 3 shows the performance for speech actfiltering with respect to Precision (P), Recall (R),and F-measure score (F).7 The classifier performedwell, recognizing 83% of the speech act sentenceswith 86% precision, and 95% of the expository (no6This is the same feature set used to produce the results forrow E of Table 4.7We computed an F1 score with equal weighting of preci-sion and recall.754Commissives Directives Expressives RepresentativesFeatures P R F P R F P R F P R FBaselinesCom baseline .45 .08 .14 - - - - - - - - -Dir baseline - - - .97 .73 .83 - - - - - -Exp baseline 1 - - - - - - .58 .18 .28 - - -Exp baseline 2 - - - - - - .97 .86 .91 - - -Rep baseline - - - - - - - - - 1.0 .05 .10ClassifiersU Unigram .45 .20 .27 .87 .84 .85 .97 .88 .92 .32 .12 .18A U+all lexsyn .52 .33 .40 .87 .84 .86 .98 .88 .92 .30 .14 .19B U+minimal lexsyn .59 .33 .42 .87 .85 .86 .98 .88 .92 .32 .14 .20C B+speechActClues .57 .31 .41 .86 .84 .85 .97 .91 .94 .33 .16 .21D C+semTest .64 .35 .46 .87 .84 .85 .97 .91 .94 .33 .16 .21E D+semDrug .63 .39 .48 .86 .84 .85 .97 .91 .94 .32 .16 .21Table 4: Precision, Recall, F-measure for four speech act classes.
The highest F score for each category appears inboldface.speech act) sentences with 93% precision.4.3.2 Speech Act CategorizationBASELINESOur next set of experiments focused on labellingsentences with the four specific speech act classes:Commissive, Directive.
Expressive, and Represen-tative.
To assess the difficulty of identifying eachspeech act category, we created several simple base-lines using our intuitions about each category.For Commissives, we created a heuristic to cap-ture the most obvious cases of future tense (becauseCommissive speech acts represent a writer?s com-mitment toward a future course of action).
For ex-ample, the presence of the phrases ?I will?
and ?Ishall?
were hypothesized by Cohen et al (2004) tobe useful bigram clues for Commissives.
This base-line looks for future tense verb phrases with a 1stperson pronoun within one or two words precedingthe verb phrase.
The Com baseline row of Table 4shows the results for this heuristic, which obtained8% recall with 45% precision.
The heuristic appliedto only 9 sentences in our test set, 4 of which con-tained a Commissive speech act.Directive speech acts are often questions, so wecreated a baseline system that labels all sentencescontaining a question mark as a Directive.
The Dirbaseline row of Table 4 shows that 97% of sentenceswith a question mark were indeed Directives.8 Butonly 73% of the Directive sentences contained aquestion mark.
The remaining 27% of Directivesdid not contain a question mark and generally fellinto two categories.
Some sentences asked a ques-tion but the writer ended the sentence with a period(e.g., ?Has anyone seen this before.?).
And many di-rectives were expressed as requests rather than ques-tions (e.g., ?Let me know if anyone has a sugges-tion.?
).For Expressives, we implemented two baselines.Exp baseline 1 simply looks for an exclamationmark, but this heuristic did not work well (18% re-call with 58% precision) because exclamation markswere often used for general emphasis (e.g., ?Theowner is frustrated with cleaning up urine!?).
Expbaseline 2 looks for the presence of four commonexpressive words (appreciate, hi, hello, thank), in-cluding morphological variations of appreciate andthank.
This baseline produced very good results,86% recall with 97% precision.
Obviously a smallset of common expressions account for most of theExpressive speech acts in our corpus.
However, theword ?hi?
did produce some false hits because it wasused as a shorthand for ?high?, usually when report-ing test results (e.g., ?hi calcium?
).8235 sentences contained a question mark, and 227 of themwere Directives.755Finally, as a baseline for the Representative classwe simply looked for the words diagnose(d) and sus-pect(ed).
The Rep baseline row of Table 4 showsthat this heuristic was 100% accurate, but only pro-duced 5% recall (matching 3 of the 57 Representa-tive sentences in our test set).CLASSIFIER RESULTSThe bottom portion of Table 4 shows the resultsfor our classifiers.
As we explained in Section 3.3,we created one classifier for each speech act cate-gory, and all four classifiers were applied to eachsentence.
So a sentence could receive anywherefrom 0-4 speech act labels indicating how many dif-ferent types of speech acts appeared in the sentence.We trained and evaluated each classifier using 10-fold cross-validation on our gold standard data set.The Unigram (U) row shows the performance ofclassifiers that use only unigram features.
For Di-rectives, we see a 2% F-score improvement over thebaseline, which reflects a recall gain of 11% buta corresponding precision loss of 10%.
The uni-grams are clearly helpful in identifying many Direc-tive sentences that do not end in a question mark,but at some cost to accuracy.
For Expressives, theunigram classifier achieves an F score of 92%, iden-tifying slightly more Expressive sentences than thebaseline with the same level of precision.
For Com-missives and Representatives, the unigram classi-fiers performed susbtantially better than their corre-sponding baseline systems, but performance is stillrelatively weak.Row A (U+ all lexsyn) in Table 4 shows the re-sults using unigram features plus all of the lexicaland syntactic features described in Section 3.2.1.The lexical and syntactic features dramatically im-prove performance on Commissives, increasing Fscore from 27% to 40%, and they produce a 2% re-call gain for Representatives but with a correspond-ing loss of precision.However, we observed that only a few of the lex-ical and syntactic features had much impact on per-formance.
We experimented with different subsetsof the features and obtained even better performancewhen using just 10 of them, which we will refer to asthe minimal lexsyn features.
The minimal lexsyn fea-ture set consists of the 4 Tense+Person features, theEarly Punctuation feature, the Sentence begins withModal/Verb feature, and the 4 Sentence Position fea-tures.
Row B shows the results using unigram fea-tures plus only these minimal lexsyn features.
Preci-sion improves for Commissives by an additional 7%and Representatives by 2% when using only theselexical and syntactic features.
Consequently, we usethe minimal lexsyn features for the rest of our exper-iments.Row C shows the results of adding the speech actclue words (see Section 3.2.2) to the feature set usedin Row B.
The speech act clue words produced anadditional recall gain of 3% for Expressives and 2%for Representatives, although performance on Com-missives dropped 2% in both recall and precision.Rows D and E show the results of adding the se-mantic features.
We added one semantic categoryat a time to measure the impact of them separately.Row D adds two semantic features for the TEST cat-egory, one from the Basilisk lexicon and one fromthe semantic tagger.
The TEST semantic featuresproduced an F-score gain of 5% for Commissives,improving recall by 4% and precision by 7%.
RowE adds two semantic features for the DRUG category.The DRUG features produced an additional F-scoregain of 2% for Commissives, improving recall by4% with a slight drop in precision.4.4 AnalysisTogether, the TEST and DRUG semantic features dra-matically improved the classifier?s ability to recog-nize Commissive speech acts, increasing its F scorefrom 41% ?
48%.
This result demonstrates thatin the domain of veterinary medicine, some typesof semantic entities are associated with speech acts.Our intuition behind this result is that commitmentsare usually related to future actions.
In veterinarymedicine, TESTS and DRUGS are associated with ac-tions performed by doctors.
Doctors help their pa-tients by prescribing or administering drugs and byconducting tests.
So these semantic entities mayserve as a proxy to implicitly represent actions thatthe doctor has done or may do.
In future work, ex-plicitly recognizing actions and events many be aworthwhile avenue to further improve results.We achieved good success at identifying both Di-rectives and Expressives, although simple heuristicsalso perform well on these categories.
We showedthat training a Directive classifier can help to iden-756tify Directive sentences that do not end with a ques-tion mark, although at the cost of some precision.The Commissive speech act class benefitted themost from the rich feature set.
Unigrams are clearlynot sufficient to identify Commissive sentences.Many different types of clues seem to be importantfor recognizing these sentences.
The improvementsobtained from adding semantic features also sug-gests that domain-specific semantics can be usefulfor recognizing some speech acts.
However, there isstill ample room for improvement, illustrating thatspeech act classification is a challenging problem.Representative speech acts were by far the mostdifficult to recognize.
We believe that there areseveral reasons for their low performance.
First,Representatives were sparse in the data set, occur-ring in only 2.91% of the sentences.
Consequently,the classifier had relatively few positive traininginstances.
Second, Representatives had the low-est inter-annotator agreement, indicating that humanjudges had difficulty recognizing these speech actstoo.
The judges often disagreed about whether ahypothesis or suspicion was the writer?s own beliefor whether it was stated as a fact reflecting generalmedical knowledge.
The message board text genreis especially challenging in this regard because thewriter is often presumed to be expressing his/her be-liefs even when the writer does not explicitly say so.Finally, our semantic features could not distinguishbetween diseases and symptoms.
Access to a re-source that can reliably identify disease terms couldpotentially improve performance in this domain.5 ConclusionsOur goal was to identify speech act sentences inmessage board posts and to classify the sentenceswith respect to four categories in Searle?s (1976)speech act taxonomy.
We achieved good results forspeech act filtering and the identification of Direc-tive and Expressive speech act sentences.
We foundthat Representative and Commissive speech acts aremuch more difficult to identify, although the per-formance of our Commissive classifier substantiallyimproved with the addition of lexical, syntactic, andsemantic features.
Except for the semantic classinformation, our feature set is domain-independentand could be used to recognize speech act sentencesin message boards for any domain.
Furthermore, ourfeatures only rely on part-of-speech tags and do notrequire parsing, which is of practical importance fortext genres such as message boards that are litteredwith ungrammatical text, typos, and shorthand nota-tions.In future work, we believe that segmenting sen-tences into clauses may help to train classifiers moreprecisely.
Ultimately, we would like to identifythe speech act expressions themselves because somesentences contain speech acts as well as factual in-formation.
Extracting the speech act expressionsand clauses from message boards and similar textgenres could provide better tracking of questionsand answers in web forums and be used for sum-marization.6 AcknowledgmentsWe gratefully acknowledge that this research wassupported in part by the National Science Founda-tion under grant IIS-1018314.
Any opinions, find-ings, and conclusion or recommendations expressedin this material are those of the authors and do notnecessarily reflect the view of the U.S. government.ReferencesJames Allen.
1995.
Natural language understanding(2nd ed.).
Benjamin-Cummings Publishing Co., Inc.,Redwood City, CA, USA.Jean Carletta.
1996.
Assessing agreement on classifi-cation tasks: the kappa statistic.
Comput.
Linguist.,22:249?254, June.Vitor R. Carvalho and William W. Cohen.
2005.
On thecollective classification of email ?speech acts?.
In SI-GIR ?05: Proceedings of the 28th annual internationalACM SIGIR conference on Research and developmentin information retrieval, pages 345?352, New York,NY, USA.
ACM Press.Vitor R. Carvalho and William W. Cohen.
2006.
Improv-ing ?email speech acts?
analysis via n-gram selection.In Proceedings of the HLT-NAACL 2006 Workshop onAnalyzing Conversations in Text and Speech, ACTS?09, pages 35?41, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.William W. Cohen, Vitor R. Carvalho, and Tom M.Mitchell.
2004.
Learning to classify email into?speech acts?.
In EMNLP, pages 309?316.
ACL.Jade Goldstein and Roberta Evans Sabin.
2006.
Usingspeech acts to categorize email and identify email gen-757res.
In Proceedings of the 39th Annual Hawaii Inter-national Conference on System Sciences - Volume 03,pages 50.2?, Washington, DC, USA.
IEEE ComputerSociety.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11:10?18, November.Ruihong Huang and Ellen Riloff.
2010.
Inducingdomain-specific semantic class taggers from (almost)nothing.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguistics, ACL?10, pages 275?285, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Minwoo Jeong, Chin-Yew Lin, and Gary Geunbae Lee.2009.
Semi-supervised speech act recognition inemails and forums.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 3 - Volume 3, EMNLP ?09, pages1250?1259, Stroudsburg, PA, USA.
Association forComputational Linguistics.Andrew Lampert, Robert Dale, and Cecile Paris.
2006.Classifying speech acts using verbal response modes.In Proceedings of the 2006 Australasian LanguageTechnology Workshop (ALTW2006), pages 34?41.Sydney Australia : ALTA.Tara McIntosh.
2010.
Unsupervised discovery of neg-ative categories in lexicon bootstrapping.
In Pro-ceedings of the 2010 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?10,pages 356?365, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.John Mildinhall and Jan Noyes.
2008.
Toward a stochas-tic speech act model of email behavior.
In CEAS.Jacqueline Nastri, Jorge Pena, and Jeffrey T. Hancock.2006.
The construction of away messages: A speechact analysis.
J. Computer-Mediated Communication,pages 1025?1045.Sujith Ravi and Jihie Kim.
2007.
Profiling student inter-actions in threaded discussions with speech act classi-fiers.
In Proceeding of the 2007 conference on Arti-ficial Intelligence in Education: Building TechnologyRich Learning Contexts That Work, pages 357?364,Amsterdam, The Netherlands, The Netherlands.
IOSPress.Ellen Riloff, Janyce Wiebe, and Theresa Wilson.
2003.Learning subjective nouns using extraction patternbootstrapping.
In Proceedings of the seventh confer-ence on Natural language learning at HLT-NAACL2003 - Volume 4, CONLL ?03, pages 25?32, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.John R. Searle.
1976.
A classification of illocutionaryacts.
Language in Society, 5(1):pp.
1?23.Michael Thelen and Ellen Riloff.
2002.
A bootstrappingmethod for learning semantic lexicons using extrac-tion pattern contexts.
In Proceedings of the ACL-02conference on Empirical methods in natural languageprocessing - Volume 10, EMNLP ?02, pages 214?221,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology - Volume 1,NAACL ?03, pages 173?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Douglas P. Twitchell and Jay F. Nunamaker Jr. 2004.Speech act profiling: a probabilistic method for ana-lyzing persistent conversations and their participants.In System Sciences, 2004.
Proceedings of the 37th An-nual Hawaii International Conference on, pages 1?10,January.Douglas P. Twitchell, Mark Adkins, Jay F.
NunamakerJr., and Judee K. Burgoon.
2004.
Using speech acttheory to model conversations for automated classi-fication and retrieval.
In Proceedings of the Inter-national Working Conference Language Action Per-spective Communication Modelling (LAP 2004), pages121?130.A.
Wierzbicka.
1987.
English speech act verbs: a se-mantic dictionary.
Academic Press, Sydney, Orlando.758
