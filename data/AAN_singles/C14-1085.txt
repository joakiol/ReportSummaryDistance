Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 895?904, Dublin, Ireland, August 23-29 2014.An Off-the-shelf Approach to Authorship AttributionJamal Abdul NasirDep.
of Computer ScienceLUMS LahorePakistanjamaln@lums.edu.pkNico G?ornitzDep.
of Computer ScienceTU BerlinGermanygoernitz@tu-berlin.deUlf BrefeldDep.
of Computer ScienceTU DarmstadtGermanybrefeld@cs.tu-darmstadt.deAbstractAuthorship detection is a challenging task due to many design choices the user has to decideon.
The performance highly depends on the right set of features, the amount of data, in-samplevs.
out-of-sample settings, and profile- vs. instance-based approaches.
So far, the variety ofcombinations renders off-the-shelf methods for authorship detection inappropriate.
We propose anovel and generally deployable method that does not share these limitations.
We treat authorshipattribution as an anomaly detection problem where author regions are learned in feature space.The choice of the right feature space for a given task is identified automatically by representingthe optimal solution as a linear mixture of multiple kernel functions (MKL).
Our approach allowsto include labelled as well as unlabelled examples to remedy the in-sample and out-of-sampleproblems.
Empirically, we observe our proposed novel technique either to be better or on parwith baseline competitors.
However, our method relieves the user from critical design choices(e.g., feature set) and can therefore be used as an off-the-shelf method for authorship attribution.1 IntroductionAutomatically attributing a piece of text to its author is one of the oldest problems studied in linguis-tics (Mendenhall, 1887).
Despite being an old problem, authorship attribution is still highly topicaland todays applications range from plagiarism detection (Maurer et al., 2006), identifying the origin ofanonymous harassments in emails, blogs, and chat rooms (Tan et al., 2013) to copyright and estate is-sues as well as resolving historical questions of disputed authorship (Mosteller and Wallace, 1964; Fung,2003).Intrinsically, the goal of authorship detection is to identify the characteristic traits of an author.
Theidea is that, these traits distinguish an author from others in terms of writing style, use of words, etc.
Thus,prior work often focuses on designing and extracting features from text to capture these traits.
There is agreat deal of features proposed for authorship detection, including word or character n-grams (Burrows,1987; Houvardas and Stamatatos, 2006), part-of-speech (Stamatatos et al., 2001), probabilistic context-free grammars (Raghavan et al., 2010), or linguistic features (Koppel et al., 2006).
However, indicativefeatures for one author do not necessarily help to characterise another.
A major problem in authorshipdetection is therefore to find the right set of features for a given task at hand (Forman, 2003).Algorithmically, a variety of different models have been studied in the context of authorship detection,ranging from probabilistic approaches (Seroussi et al., 2011) and similarity-based methods (Koppel etal., 2011) to vector space models (Fung, 2003; Li et al., 2006).
The approaches either treat documents asindependent (instance-based) or concatenate documents by the same author (profile-based).
Intuitively,the latter is helpful if an author has a concise way of expressing herself so that the concatenated documentallows to extract a statistic that is sufficient for capturing her style.
On the other hand, instance-basedapproaches are better suited for expressive authors and have advantages in sparse data scenarios.Another aspect in authorship attribution is the application scenario of the final model.
In transductive(in-sample) settings, the unlabelled documents of interest are already included in the training processThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/895?4 ?2 0 2 4?2?10123456?4 ?2 0 2 4?2?10123456?4 ?2 0 2 4?2?10123456Figure 1: Three solutions of an anomaly detection problem where data is represented by RBF kernelswith different band-width parameters.
Combining anomaly detection with multiple kernel learning al-lows to include all three kernels simultaneously in the optimisation and to find the optimal linear mixtureof the three (or more) kernels together automatically for a given task.and the model does not necessarily perform well on new and unseen texts.
By contrast, inductive (out-of-sample) scenarios generally allow to learn models that can be applied to any future text but requirelarger training samples to achieve accurate performances.In this paper, we propose a general machine learning-based approach to authorship detection.
Ourapproach remedies the above mentioned problems by fusing existing techniques: (i) We cast authorshipattribution as an anomaly detection problem where one model is learned for every author.
The idea is toidentify a concise region in feature space that contains (most of) the documents of the author of interestwhile other documents are considered outliers.
Thus, the model can be viewed as a profile-based ap-proach in feature space while the data is treated on an instance-based level.
(ii) We remedy the in-sample/ out-of-sample problem by providing a semi-supervised extension of the commonly unsupervised outlierdetection framework.
By doing so, we may include authorship labels for the already known documentsand leave the disputed ones unlabelled.
(iii) Finally, we devise our model consequentially as a memberof the multiple kernel learning family to automatically include a mathematically well founded featureselection framework that renders the method generally applicable.
The optimal solution is given by a(possibly sparse) linear mixture of kernel functions.Empirically, we observe that our approach consistently outperforms baseline competitors or confirmscommon knowledge with respect to the authorship of disputed articles.
The main advantage of themethod however lies in its simplicity.
Practitioners do not need to take critical design choices in terms ofwhich features to use and which not.
By contrast, all features (kernels) can be used in the optimisationand the method itself finds the optimal combination for the problem at hand.The remainder of this paper is structured as follows.
Section 2 reviews related work.
Our main contri-bution is presented in Section 3.
We report on empirical results in Section 4 and Section 5 concludes.2 Related WorkAuthorship attribution using linguistic and stylistic features has a long tradition and can be dated backto the nineteenth century.
As a first attempt, Mendenhall (Mendenhall, 1887) uses features based onword lengths to characterise the plays of Shakespeare.
Later in the first half of the 20th century, differenttextual statistics, such as Zipf?s distribution (Zipf, 1932) and Yule?s k-statistic (Yule, 1944) have beenproposed to quantify textual style.
A study conducted by (Mosteller and Wallace, 1964) is one of the mostinfluential modern work in authorship attribution.
They use a Bayesian approach to analyse frequenciesof a small set of function words for The Federalist Papers, a series of 85 political essays written by JohnJay, Alexander Hamilton, and James Madison.
Until the late 1990s, research in stylometry has beendominated by feature engineering to quantify writing style (Holmes, 1998) and about 1,000 differentmeasures have been proposed (Rudman, 1997).896Document representation is essential for author attribution tasks.
Features aim to capture character-istic traits of authors that persist across topics.
Traditional stylometric features include function andhigh-frequency words, hapax legomena, Yules k-statistic, syllable distributions, sentence length, wordlength and word frequencies, vocabulary richness functions as well as syntactic features.
Many stud-ies combine features of different types using multivariate analyses.
Some researchers use punctuationsymbols while others experiment with n-grams (Diederich et al., 2003).
Grammatical style markers withnatural language processing techniques are also used to extract features from the documents.Also in terms of technical approaches, authorship attribution has been studied with a wide range ofdifferent approaches.
The deployed techniques can be broadly divided into three categories: machinelearning (Diederich et al., 2003), multivariate/cluster analysis (Khmelev, 2000), and natural languageprocessing (Stamatatos et al., 2000).
Principal components analysis (PCA) is one of the widely usedtechinques for authorship studies, for instance, (Holmes and Crofts, 2010) apply PCA to identify theauthorship of unknown articles that have been attributed to Stephen Crane.
In addition, machine learning-based approaches, including neural networks (Neme et al., 2011) and support vector machines (SVMs)(Diederich et al., 2003), are frequently used to discriminate documents by different authors.
An excellentsurvey on the diversity of approaches for authorship detection is provided by (Stamatatos, 2009).Density level set estimation, also known as one-class learning (Tax and Duin, 1999; Sch?olkopf etal., 1999), is the problem of learning a representation of a single class of interest, rejecting data pointsthat deviate from the learned model of normality.
Thus, it has been proven very successful in anomalydetection scenarios such as network intrusion detection (G?ornitz et al., 2009).
Various extensions havebeen proposed, i.e.
to incorporate prior and additional knowledge (G?ornitz et al., 2013; Blanchard et al.,2010) in a semi-supervised fashion (Chapelle et al., 2006) and to learn linear combinations of kernels(Kloft et al., 2011; Rakotomamonjy et al., 2008) which is especially usefull whenever the right choice ofrepresentation is unknown.3 MethodologyIn this section, we cast semi-supervised anomaly detection as an instance of multiple kernel learning.The rational for this idea is shown in Figure 1.
The figure shows three solutions for an anomaly de-tection task.
The data is represented by RBF kernels with different band-width parameters.
As shownin the figure, the choice of the band width parameter is crucial and leads to very different solutions.Usually, kernel parameters are thus included in the model selection and their optimisation is often timeconsuming.
Fusing the anomaly detection with multiple kernel learning allows to include all three ker-nels simultaneously in the optimisation and to find the best linear mixture of the three (or more) kernelstogether with model parameters for the data at hand.We briefly review anomaly detection and semi-supervised anomaly detection in Sections 3.1 and 3.2,respectively, and present our main contribution, multiple kernel learning for anomaly detection, in Sec-tion 3.3.3.1 PreliminariesAnomaly detection is often cast as an unsupervised one-class problem where the goal is to find a hyper-plane that separates the majority of the input examples from the origin with maximum margin, so that, bydefinition, points not exceeding the hyperplane are considered outliers.
Analogously, we aim to learn aseparating hyperplane for articles by an author of interest, such that documents not exceeding the learnedhyperplane are written by other authors.Given a n articles x1, .
.
.
,xnof possibly different authors, a straight forward optimisation problemthat identifies the hyperplane in terms of its normal vector w and threshold ?
is known as one-classsupport vector machine (Tax and Duin, 1999; Sch?olkopf et al., 1999) and given byminw,?,?
?012?w ?22?
?+ ?un?i=1?is.t.
?ni=1: ?w, ?(xi)?
?
??
?i.897The hyperplane is realised by the decision functionf(x) = ?w, ?(x)?
?
?and new articles are credited to the author if f(x) > 0 and are considered work by someone else iff(x) ?
0.
The threshold ?
can be interpreted as a measure of expressiveness of an author.
E.g., authorswho have a very clear and concise style realise smaller thresholds than expressive authors that may adoptto different writing styles.3.2 Semi-supervised Anomaly DetectionUsing only unlabelled data is usually leading to inaccurate models in the presence of onlya few data points.
We therefore extend the problem setting to include m labeled examples(xn+1, yn+1), .
.
.
, (xn+m, yn+m) in addition to the n unlabelled ones.
Labels yi?
{+1,?1} are con-sidered binary, that is in case yi= +1, document xibelongs to the author of interest.
To combine sumsand hence, improve readability, we introduce labels yi= +1 ?
i = 1, .
.
.
, n for all unlabelled examplesand an indicator function Ic?
[c > n] to mask labeled examples; the function Icsimply returns 1 ifc > n and 0 otherwise.A semi-supervised generalisation of the hypersphere model of the previous section is the convex semi-supervised anomaly detection (SSAD) (G?ornitz et al., 2013) which uses an L2-regularizer together withthe hinge-loss.
Let ?
be the margin for the labeled examples and ?, ?u, and ?ltrade-off parameters, theoptimisation problem becomesminw,?,??0,?
?012?w ?22?
??
??
+n+m?i=1(Ii?l+ (1?
Ii)?u)?is.t.
?n+mi=1: yi?w, ?(xi)?
?
yi?+ Ii?
?
?i.The solution w admits a dual representation and can be written asw =n+m?i=1?iyi?
(xi)and hence, the decision function depends only on inner products of the input examples wich paves theway for kernel functions K?(x,x?)
= ??
(x), ?(x?)?
(see (M?uller et al., 2001) for an introduction tokernels).
It holdsf(x) =n+m?i=1?iyi??
(xi), ?(x)?
?
?
=n+m?i=1?iyiK?(xi,x)?
?.We omit the subscript ?
in the remainder to not clutter notation unnecessarily.3.3 Multiple Kernel Learning for Anomaly DetectionLearning linear combinations of multiple kernels is an appealing strategy when the right choice of rep-resentations is unknown.
We therefore generalise the semi-supervised anomaly detection of the previoussection as a member of the multiple kernel learning framework (Lanckriet et al., 2004).
Thus, we aim tolearn a weighted combination of T kernels with mixing coefficients ?
= (?1, .
.
.
, ?T):KMKL(x,x?)
:=T?t=1?tKt(x,x?)
=T?t=1?t?
?t(x), ?t(x?)?=T?t=1???t?t(x),??t?t(x?
)?.898In general, properties of the mixing coefficients include (i) non-negativity, hence ?t?
0 and (ii) normal-isation ??
?p= 1.
Recent work (Kloft et al., 2011) suggests to use the more general p-norm instead ofa common 1-norm (Lanckriet et al., 2004; Bach et al., 2004; Rakotomamonjy et al., 2008).
The latterusually leads to sparse mixing coefficients which is not appealing in every situation whereas p-norm with1 ?
p ?
inf admits sparsity adjustments for the problem at hand and thus adds flexibility.
Incorporatingmultiple feature representations in our model introduced in Section 3.1 leads tofMKL(x) =T?t=1??wt,??t?t(x)?
?
?
=T?t=1??t?
?wt, ?t(x)?
?
?.
(1)Due to technical reasons, i.e.
to preserve convexity, we substitute the model parameters wt=?
?t?wtand arrive at the revised primal MKL-SSAD optimisation problem:min{wt},?,??0,??0,??0?2??
?2p+12T?t=11?t?wt?22?
??
??
+n+m?i=1(Ii?l+ (1?
Ii)?u)?is.t.
?n+mi=1: yiT?t=1?wt, ?t(xi)?
?
yi?+ Ii?
?
?i.
(2)(Kloft et al., 2011) prove the equivalence of Tikhonov and Ivanov regularisation which allows to movethe regulariser on the mixing coefficients in the objective function.
We will exploit this relation on variousoccasions in this section.
Deriving the Lagrange dual problem, we arrive at the intermediate saddle pointproblemmax?min{wt},??0?2??
?2p+12T?t=11?t?wt?22?n+m?i=1?iyi?t?wt, ?t(xi)?s.t.
?
?n+m?i=1Ii?iand 0 ?
?i?
Ii?l+ (1?
Ii)?u?
iWe are solving the optimisation problem in a block-coordinate descent fashion by alternating betweenw and ?.
This enables us to compute the latter analytically assuming fixed variables w and setting thepartial derivative to zero:??tp?1??
?2?pp?
?wt?22?t2= 0.Therefore, given ?
?
0 we get?t= ?
?wt?2p+12.Furthermore, it holds that at any optimal point ??
?p= 1 and solving for ?
gives ?
=1/(?Tt=1?wt?2pp+12)1p.
Putting things together, gives the analytical update rule?t=?wt?2p+12(?Tt=1?wt?2pp+12)1p(3)which, since only norms are involved, ensures non-negativity for the mixing coefficients.
Substitutingwtusing the representer theorem wt= ?t?n+mi=1?iyi?t(xi) yields the final optimisation problem for899Algorithm 1 Proposed optimization algorithm for MKL-SSAD (2)Require: x,y, ?u, ?l, ?
& p?
normInitialize kernel mixture coefficients such that ?
?z=0?p= 1while Until Convergence doStep 1: solve the convex SSAD problem as stated in Eqn.
(4)?z+1= argmax?:0?
?i?Ii?l+(1?Ii)?uJ(?,?z) s.t.
?
?
?n+mi=1Ii?iStep 2: optimize the weights according to Eqn.
(3)?z+1= argmin?
?0 J(?z+1,?)
s.t.
??
?2p?
1z=z+1end whilereturn Trained parameter vector ?
?, weights ??MKL-SSAD:max?min?:??
?2p?1=:J(?,?)?
??
?
?12?i,j?i?jyiyjT?t=1?tKt(xi,xj) (4)s.t.
?
?n+m?i=1Ii?iand 0 ?
?i?
Ii?l+ (1?
Ii)?u?
iAs a block-coordinate descent method, we can iteratively alternate between the two optimisation blocksand every limit point of Algorithm 1 is a globally optimal point (cmp.
also (Kloft et al., 2011)).
Algo-rithm 1 summarises the proposed optimisation procedure.4 Empirical ResultsIn this section, we empirically evaluate the benefit of fusing semi-supervised anomaly detection withmultiple kernel learning.
We experiment on two data sets, the Reuters-50-50 corpus in Section 4.1 andthe Federalist Papers in Section 4.24.1 Reuters 50-50We use a subset of the Reuters 50-50 data set1to evaluate the performance of the aforementionedapproaches.
The reduced data contains 1000 articles written by 10 authors, Aaron Pressman, AlanCrosby, Alexander Smith, Benjamin Kang Lim, Bernard Hickey, Brad Dorfman, Darren Schuettler,David Lawder, Edna Fernandes, and Eric Auchard.We deploy the following four kernels to represent documents: the first kernel is made of 484 func-tion words taken from (Koppel and Schler, 2003), the second contains part-of-speech (POS) tags, thethird is assembled by 3-letter suffixes, the last one simply a bag-of-words (BOW) kernel.
We split thedata into training (90%) and test (10%) sets and conduct a 10-fold cross-validation on the training setfor model selection.
The best performing models are then evaluated on the test set.
In this set of ex-periments, we use a transductive setting where all training instances are labeled and only holdout andtest articles are unlabelled.
We compare the performance of our approach with different p-norms tothe SSAD which uses one kernel at a time.
For our MKL-based approach we use p-norms in the setp ?
{1, 1.7783, 3.1623, 5.6234, 10}.The results in terms of averaged micro- and macro-F1measures are shown in Table 1.
MKL con-sistently outperforms the single-kernel baseline for all p-norms.
That is, instead of extensively experi-menting with SSAD and different kernel functions and parameter selections, a single run with our MKLalready leads to better performances in both metrics.
The rightmost column in the table shows the resultfor SSAD using a sum of the input kernels.
Apparently, the performance is worse than using a bag-of-words kernel alone.
This result underlines the necessity of effective feature selection techniques for1https://archive.ics.uci.edu/ml/datasets/Reuter 50 50900Classifier 1Func POS Suf BOW11.77833.16235.623410Classifier 2Func POS Suf BOW11.77833.16235.623410Classifier 3Func POS Suf BOW11.77833.16235.623410Classifier 4Func POS Suf BOW11.77833.16235.623410Classifier 5Func POS Suf BOW11.77833.16235.623410Classifier 6Func POS Suf BOW11.77833.16235.623410Classifier 7Func POS Suf BOW11.77833.16235.623410Classifier 8Func POS Suf BOW11.77833.16235.623410Classifier 9Func POS Suf BOW11.77833.16235.623410Classifier 10Func POS Suf BOW11.77833.16235.623410 0.10.20.30.40.50.60.70.8Figure 2: Kernel mixture coefficients for the 10 classesauthorship attribution.
Note that our method can actually be viewed as an ensemble method that com-bines several models as shown in Equation (1).
However, compared to traditional ensembles, our methoduses a convex combination and hence returns the optimal ensemble given the data.Table 1: F-scores for the subset of Reuters 50-50p-norm MKL SSAD1 1.7783 3.1623 5.6234 10 func-w POS Suffix-3 BOW?Fmicro73.46 73.08 73.84 73.89 74.23 63.08 54.62 70.01 72.85 61.76Fmacro79.23 78.86 79.63 79.76 80.07 68.66 58.03 74.01 78.09 70.93Figure 2 visualises the resulting mixing coefficients for the 10 authors/classifiers.
While the modelsare very similar at first sight, small deviations indicate differences in the style of the authors.
Considerfor instance the top-left matrix.
The contribution of the part-of-speech tag kernel (second column) to thefinal solution is less than for the other authors.
By contrast, the importance of the Suffix-3 kernel has(slightly) more impact than for the remaining authors.
This result shows that author-dependent mixturesare found that help to capture characteristic traits of the respective writing styles.4.2 Revisiting the Federalist PapersThe Federalist Papers are a series of 85 articles and essays written during 1787?1788.
They were pub-lished anonymously to persuade the citizens of the State of New York to ratify the Constitution.
Later,these papers were credited to Alexander Hamilton, John Jay, and James Madison; 73 of the documentsare uniquely associated with one of the three authors while the remaining 12, also known as the disputedpapers, have been claimed by both, Hamilton and Madison.
Three of the 73 articles are considered jointwork by Hamilton and Madison.
Previous studies often assign all 12 disputed papers to Madison whichwe assume as ground-truth in the remainder (Mosteller and Wallace, 1964; Fung, 2003).To confirm or refuse these previous findings, we conduct an experiment using same four kernels asin the previous section, that is, a function words kernel (Koppel and Schler, 2003), a part-of-speech(POS) tag kernel, a Suffix-3 kernel, and a bag-of-words (BOW) kernel.
We compare the performanceof our approach (MKL) with semi-supervised anomaly detection (SSAD) (G?ornitz et al., 2013), supportvector data description (SVDD) (Tax and Duin, 1999), and the one-class SVM (OCSVM) (Sch?olkopf etal., 1999).
As before, the baselines cannot use all kernels at a time and are evaluated on every kernelseparately.
For simplicity, we show only the MKL results for parameter p = 2 as all other p-norms thatwe tried out lead to the same result.901We randomly divide the undisputed papers into training (80%) and holdout (20%) and use the 12disputed papers for testing.
We make sure that training sets contain at least three examples of everyauthor and two articles written jointly by Hamilton and Madison.
Otherwise we discard and draw again.We repeat experiments five times with randomly drawn training and holdout sets and report on averagedaccuracies for the disputed test set.Table 2: Results for the disputed articles of the Federalist papers.kernel H&M M J HMKL (all) 0 12 0 0SSAD484fw 0 12 0 0POS 9 0 3 0Suffix3 0 12 0 0BoW 0 0 0 12SVDD484fw 12 0 0 0POS 12 0 0 0Suffix3 12 0 0 0BoW 12 0 0 0OCSVM484fw 12 0 0 0POS 12 0 0 0Suffix3 12 0 0 0BoW 12 0 0 0The results are shown in Table 2.
The one-class SVM and SVDD constantly credit the 12 disputedarticles as joint work by Hamilton and Madison.
The outcome of SSAD highly depends on the kernelfunction; while the part-of-speech kernel distributes the papers on Jay (3) and Hamilton and Madison(9), respectively, the bag-of-words kernel assigns all documents to Hamilton.
By contrast, SVDD withfunction word and Suffix-3 kernels attribute the articles to Madison.
The same outcome is observedfor our novel MKL that also credits the 12 papers to Madison.
Thus, MKL and SSAD with functionwords and BoW kernel confirm todays assumption that all 12 papers have been written by Madison.However, choosing SSAD as the base classifier in the absence of prior knowledge leaves much room forinterpretations and the user in the need of deciding between three solutions, depending on which kernelshe prefers.
By using our MKL, selecting features and or kernel functions is no longer necessary as thelearning algorithm itself picks the right combination of kernels for the problem at hand.
Thus, the morekernels are thrown into, the richer the decision space for the MKL.5 ConclusionWe proposed a universal method for authorship detection.
Our approach built upon semi-supervisedanomaly detection and generalised existing techniques to utilise multiple kernels; a requirement whichis particularly beneficial for authorship attribution as features are usually tailored to tasks at hand and donot necessarily translate well to other authors.
Our method is proven to converge to the optimal solutionand simple to implement.
Our empirical results show the robustness of our approach as it consistentlyoutperforms baseline competitors on a subset of Reuters 50-50 or confirms common knowledge wrt theauthorship of disputed articles of the Federalist Papers.
The main advantage of the method however liesin its simplicity.
Practitioners do not need to take critical design choices in terms of which features to useand which not.
By contrast, all features (kernels) can be used in the optimisation and the method itselffinds the optimal combination for the problem at hand.AcknowledgementsJamal Abdul Nasir is supported by a grant from the Higher Education Commission, H-9 Islamabad,Pakistan.
Nico G?ornitz is supported by the German Bundesministerium f?ur Bildung und Forschung902(BMBF FKZ 01GQ0850 and 01IB001A).
Ulf Brefeld is also affiliated with the German Institute forEducational Research (DIPF), Frankfurt/Main, Germany.ReferencesF.
R. Bach, G. R. G. Lanckriet, and M. I. Jordan.
2004.
Multiple kernel learning, conic duality, and the SMOalgorithm.
In Proc.
of the International Conference on Machine Learning.Gilles Blanchard, Gyemin Lee, and Clayton Scott.
2010.
Semi-Supervised Novelty Detection.
Journal of MachineLearning Research, pages 2973?2973?3009?3009, December.J.
F. Burrows.
1987.
Computation into Criticism: A Study of Jane Austen?s Novels and an Experiment in Method.Oxford: Clarendon Press.Olivier Chapelle, Bernhard Sch?olkopf, and Alexander Zien.
2006.
Semi-supervised learning.
{MIT} Press.J.
Diederich, J. Kindermann, E. Leopold, and G. Paass.
2003.
Authorship attribution with support vector machines.Applied Intelligence, 19(1):109?123.G.
Forman.
2003.
An extensive empirical study of feature selection metrics for text classification.
Journal ofMachine Learning Research, 3:1289?1305.Glenn Fung.
2003.
The disputed federalist papers: Svm feature selection via concave minimization.
In RichardTapia Celebration of Diversity in Computing Conference.Nico G?ornitz, Marius Kloft, and Ulf Brefeld.
2009.
Active and semi-supervised data domain description.
InECML, pages 407?422.
Springer.Nico G?ornitz, Marius Kloft, Konrad Rieck, and Ulf Brefeld.
2013.
Toward Supervised Anomaly Detection.Journal of Artificial Intelligence Research (JAIR), 46:235?262.David I. Holmes and Daniel W. Crofts.
2010.
The diary of a public man: a case study in traditional and non-traditional authorship attribution.
LLC, 25(2):179?197.David I. Holmes.
1998.
The Evolution of Stylometry in Humanities Scholarship.
Literary and Linguistic Com-puting, 13(3):111?117, September.J.
Houvardas and E. Stamatatos.
2006.
N-gram feature selection for authorship identification.
In AIMSA.Dmitry V. Khmelev.
2000.
Disputed authorship resolution through using relative empirical entropy for markovchains of letters in human language texts.
Journal of Quantitative Linguistics, 7(3):201?207.M.
Kloft, U. Brefeld, S. Sonnenburg, and A. Zien.
2011. lp-Norm Multiple Kernel Learning.
JMLR, 12:953?997.M.
Koppel and J. Schler.
2003.
Exploiting stylistic idiosyncrasies for authorship attribution.
In Proceedings of theInternational Joint Conference on Artificial Ingelligence.M.
Koppel, N. Akiva, and I. Dagan.
2006.
Feature instability as a criterion for selecting potential style markers:Special topic section on computational analysis of style.
Journal of the American Society for InformationScience and Technology, 57(11):1519?1525.M.
Koppel, J. Schler, and S. Argamon.
2011.
Authorship attribution in the wild.
Language Resources & Evalua-tion, 45(1):83?94.G.
Lanckriet, N. Cristianini, L. E. Ghaoui, P. Bartlett, and M. I. Jordan.
2004.
Learning the kernel matrix withsemi-definite programming.
JMLR, 5:27?72.J.
Li, R. Zheng, and H. Chen.
2006.
From fingerprint to writeprint.
Communications of the ACM, 49(4):76?82.Hermann Maurer, Frank Kappe, and Bilal Zaka.
2006.
Plagiarism ?
a survey.
Journal of Universal ComputerScience, 12(8)8:1050?1084.T.
C. Mendenhall.
1887.
The characteristic curves of composition.
Science, ns-9(214S):237?246.Frederick Mosteller and David L. Wallace.
1964.
Inference and Disputed Authorship: The Federalist.
Springer-Verlag, New York.
2nd Edition appeared in 1984 and was called Applied Bayesian and Classical Inference.903Klaus-Robert M?uller, Sebastian Mika, Gunnar R?atsch, Koji Tsuda, and Bernhard Sch?olkopf.
2001.
An introduc-tion to kernel-based learning algorithms.
IEEE Transactions on Neural Networks, 12(2):181?201.Antonio Neme, Blanca Lugo, and Alejandra Cervera.
2011.
Authorship attribution as a case of anomaly detection:A neural network model.
Int.
J.
Hybrid Intell.
Syst., 8(4):225?235.S.
Raghavan, A. Kovashka, and R. Mooney.
2010.
Authorship attribution using probabilistic context-free gram-mars.
In Proceedings of the ACL 2010 Conference.Alain Rakotomamonjy, Francis R. Bach, Stephan Canu, and Yves Grandvalet.
2008.
SimpleMKL.
JMLR,9:2491?2521.Joseph Rudman.
1997.
The state of authorship attribution studies: Some problems and solutions.
Computers andthe Humanities, 31(4):351?365.B Sch?olkopf, J C Platt, J Shawe-Taylor, a J Smola, and R C Williamson.
1999.
Estimating the support of ahigh-dimensional distribution.
Technical report, July.Y.
Seroussi, I. Zukerman, and F. Bohnert.
2011.
Authorship attribution with latent dirichlet allocation.
In Pro-ceedings of the 15th International Conference on Computational Natural Language Learning.Efstathios Stamatatos, Nikos Fakotakis, and George K. Kokkinakis.
2000.
Automatic text categorization in termsof genre and author.
Computational Linguistics, 26(4):471?495.E.
Stamatatos, N. Fakotakis, and G. Kokkinakis.
2001.
Computer-based authorship attribution without lexicalmeasures.
In Computers and the Humanities.Efstathios Stamatatos.
2009.
A survey of modern authorship attribution methods.
Journal of the American Societyfor Information Science and Technology, 60(3):538?556.Enhua Tan, Lei Guo, Songqing Chen, Xiaodong Zhang, and Yihong (Eric) Zhao.
2013.
Unik: Unsupervised socialnetwork spam detection.
In Proceedings of CIKM.D.
Tax and R. Duin.
1999.
Data domain description using support vectors.
In Proceedings of the EuropeanSymposium on Artificial Neural Networks, volume 256, pages 251?256.
Citeseer.G.
Udnv Yule.
1944.
The Statistical Study of Literary Vocabulary.
Cambridge University Press.G.
K. Zipf.
1932.
Selective Studies and the Principle of Relative Frequency in Language.
Harvard UniversityPress.904
