Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 1?8,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsGuiding Statistical Word Alignment Models With Prior KnowledgeYonggang Deng and Yuqing GaoIBM T. J. Watson Research CenterYorktown Heights, NY 10598{ydeng,yuqing}@us.ibm.comAbstractWe present a general framework to incor-porate prior knowledge such as heuristicsor linguistic features in statistical generativeword alignment models.
Prior knowledgeplays a role of probabilistic soft constraintsbetween bilingual word pairs that shall beused to guide word alignment model train-ing.
We investigate knowledge that can bederived automatically from entropy princi-ple and bilingual latent semantic analysisand show how they can be applied to im-prove translation performance.1 IntroductionStatistical word alignment models learn word as-sociations between parallel sentences from statis-tics.
Most models are trained from corpora in anunsupervised manner whose success is heavily de-pendent on the quality and quantity of the trainingdata.
It has been shown that human knowledge,in the form of a small amount of manually anno-tated parallel data to be used to seed or guide modeltraining, can significantly improve word alignmentF-measure and translation performance (Ittycheriahand Roukos, 2005; Fraser and Marcu, 2006).As formulated in the competitive linking algo-rithm (Melamed, 2000), the problem of word align-ment can be regarded as a process of word link-age disambiguation, that is, choosing correct asso-ciations among all competing hypothesis.
The morereasonable constraints are imposed on this process,the easier the task would become.
For instance, themost relaxed IBM Model-1, which assumes that anysource word can be generated by any target wordequally regardless of distance, can be improved bydemanding a Markov process of alignments as inHMM-based models (Vogel et al, 1996), or imple-menting a distribution of number of target wordslinked to a source word as in IBM fertility-basedmodels (Brown et al, 1993).Following the path, we shall put more constraintson word alignment models and investigate ways ofimplementing them in a statistical framework.
Wehave seen examples showing that names tend toalign to names and function words are likely to belinked to function words.
These observations areindependent of language and can be understood bycommon sense.
Moreover, there are other linguis-tically motivated constraints.
For instance, wordsaligned to each other presumably are semanticallyconsistent; and likely to be, they are syntacticallyagreeable.
In these paper, we shall exploit some ofthese constraints in building better word alignmentsin the application of statistical machine translation.We propose a simple framework that can inte-grate prior knowledge into statistical word align-ment model training.
In the framework, prior knowl-edge serves as probabilistic soft constraints that willguide word alignment model training.
We presenttwo types of constraints that are derived in an un-supervised way: one is based on the entropy prin-ciple, the other comes from bilingual latent seman-tic analysis.
We investigate their impact on wordalignments and show their effectiveness in improv-ing translation performance.12 Constrained Word Alignment ModelsThe framework that we propose to incorporate sta-tistical constraints into word alignment models isgeneric.
It can be applied to complicated modelssuch IBM Model-4 (Brown et al, 1993).
We shalltake HMM-based word alignment model (Vogel etal., 1996) as an example and follow the notation of(Brown et al, 1993).
Let e = el1 represent a sourcestring and f = fm1 a target string.
The random vari-able a = am1 specifies the indices of source wordsthat target words are aligned to.In an HMM-based word alignment model, sourcewords are treated as Markov states while targetwords are observations that are generated whenjumping to states:P (a, f |e) =m?j=1P (aj |aj?1, e)t(fj |eaj )Notice that a target word f is generated from asource state e by a simple lookup of the translationtable, a.k.a., t-table t(f |e), as depicted in (A) of Fig-ure 1.
To incorporate prior knowledge or imposeconstraints, we introduce two nodes E and F repre-senting the hidden tags of the source word e and thetarget word f respectively, and organize the depen-dency structure as in (B) of Figure 1.
Given this gen-erative procedure, f will also depend on its tag F ,which is determined probabilistically by the sourcetag E. The dependency from E to F functions as asoft constraint showing how the two hidden tags areagreeable to each other.
Mathematically, the condi-tional distribution follows:P (f |e) =?E,FP (f,E, F |e)=?E,FP (E|e)P (F |E)P (f |e, F )= t(f |e) ?
Con(f, e), (1)whereCon(f, e) =?E,FP (E|e)P (F |E)P (F |f)/P (F ) (2)is the soft weight attached to the t-table entry.
It con-siders all possible hidden tags of e and f and servesas constraint between the link.fefe EFA BFigure 1: A simple table lookup (A) vs. a con-strained procedure (B) of generating a target wordf from a source word e.We do not change the value of Con(f, e) duringiterative model training but rather keep it constant asan indicator of how strong the word pair should beconsidered as a candidate.
This information is de-rived before word alignment model training and willact as soft constraints that need to be respected dur-ing training and alignments.
For a given word pair,the soft constraint can have different assignment indifferent sentence pairs since the word tags can becontext dependent.To understand why we take the ?detour?
of gen-erating a target word rather than directly from a t-table, consider the hidden tag as binary value in-dicating being a name or not.
Without these con-straints, t-table entries for names with low frequencytend to be flat and word alignments can be chosenrandomly without sufficient statistics or strong lexi-cal preference under maximum likelihood criterion.If we assume that a name is produced by a namewith a high probability but by a non-name with alow probability, i.e.
P (F = E) >> P (F 6= E),proper names with low counts then are encouragedto link to proper names during training; and conse-quently, conditional probability mass would be morefocused on correct name translations.
On the otherhand, names are discouraged to produce non-names.This will potentially avoid incorrect word associa-tions.
We are able to apply this type of constraintsince usually there are many monolingual resourcesavailable to build a high performance probabilisticname tagger.
The example suggests that putting rea-sonable constraints learned frommonolingual analy-sis can alleviate data spareness problem in bilingualapplications.The weights Con(f, e) are the prior knowledgethat shall be assigned with care but respected dur-ing training.
The baseline is to set al these weights2to 1, which is equivalent to placing no prior knowl-edge on model training.
The introduction of theseweights does not complicate parameter estimationprocedure.
Whenever a source word e is hypoth-esized to generate a target word f , the translationprobability t(f |e) should be weighted by Con(f, e).We point out that the constraints between f and ethrough their hidden tags are in probabilities.
Thereare no hard decisions made before training.
A strongpreference between two words can be expressed byassigning corresponding weights close to 1.
Thiswill affect the final alignment model.Depending on the hidden tags, there are many re-alizations of reasonable constraints that can be putbeforehand.
They can be semantic classes, syntacticannotations, or as simple as whether being a functionword or content word.
Moreover, the source side andthe target side do not have to share the same set oftags.
The framework is also flexible to support mul-tiple types of constraints that can be implemented inparallel or cascaded sequence.
Moreover, the con-straints between words can be dependent on contextwithin parallel sentences.
Next, we will describetwo types of constraints that we proposed.
Both ofthem are derived from data in an unsupervised way.2.1 Entropy PrincipleIt is assumed that generally speaking, a source func-tion word generates a target function word with ahigher probability than generating a target contentword; similar assumption applies to a source con-tent word as well.
We capture this type of constraintby defining the hidden tag E and F as binary labelsindicating being a content word or not.
Based onthe assumption, we design probabilistic relationshipbetween the two hidden tags as:P (E = F ) = 1?
P (E 6= F ) = ?,where ?
is a scalar whose value is close to 1, say0.9.
The bigger ?
is, the tighter constraint we put onword pairs to be connected requiring the same typeof label.To determine the probability of a word beinga function word, we apply the entropy principle.A function word, say ?of?,?in?
or ?have?, appearsmore frequently than a content word, say ?journal?or ?chemistry?, in a document or sentence.
We willapproximate the probability of a word as a functionword with the relative uncertainty of its being ob-served in a sentence.More specifically, suppose we have N parallelsentences in the training corpus.
For each word wi1,let cij be the number of word wi observed in the j-thsentence pair, and let ci be the total number of oc-currences of wi in the corpus.
We define the relativeentropy of word wi aswi = ?1logNN?j=1cijcilogcijci.With the entropy of a word, the likelihood of wordw being tagged as a function word is approximatedwith w(1) = w and being tagged as a content wordwith w(0) = 1?
w.We ignore the denominator in Equ.
(2) and findthe constraint under the entropy principle:Con(f, e) = ?
(e(0)f (0) + e(1)f (1)) +(1?
?
)(e(1)f (0) + e(0)f (1)).As can be seen, the connection between twowords is simulated with a binary symmetric chan-nel.
An example distribution of the constraint func-tion is illustrated in Figure 2.
A high value of ?encourages connecting word pairs with compara-ble entropy; When ?
= 0.5, Con(f, e) is constantwhich corresponds to applying no prior constraint;When ?
is close to 0, the function plays oppositerole on word alignment training where a high fre-quency word is pushed to associate with a low fre-quency word.2.2 Bilingual Latent Semantic AnalysisLatent Semantic Analysis (LSA) is a theory andmethod for extracting and representing the meaningof words by statistically analyzing word contextualusages in a collection of text.
It provides a methodby which to calculate the similarity of meaning ofgiven words and documents.
LSA has been success-fully applied to information retrieval (Deerwesteret al, 1990), statistical langauge modeling (Belle-garda, 2000) and etc.1We prefix ?E ?
to source words and ?F ?
to target wordsto distinguish words that have the same spelling but are fromdifferent languages.30  0.2  0.4  0.60.81 0 0.20.4 0.6 0.8 100.10.20.30.40.50.60.70.80.9Con(f,e)alpha=0.9e(0)f(0)0  0.2  0.4  0.60.81 0 0.20.4 0.6 0.8 10.10.20.30.40.50.60.70.80.9Con(f,e)alpha=0.1e(0)f(0)Figure 2: Distribution of the constraint functionbased on entropy principle when ?
= 0.9 on theleft and ?
= 0.1 on the right.We explore LSA techniques in bilingual environ-ment to derive semantic constraints as prior knowl-edge for guiding a word alignment model train-ing.
The idea is to find semantic representation ofsource words and target words in the so-called low-dimensional LSA-space, and then to use their sim-ilarities to quantitatively establish semantic consis-tencies.
We propose two different approaches.2.2.1 A Simple Bag-of-word ModelOne method we investigate is a simple bag-of-word model as in monolingual LSA.
We treat eachsentence pair as a document and do not distin-guish source words and target words as if theyare terms generated from the same vocabulary.
Asparse matrix W characterizing word-document co-occurrence is constructed.
Following the notation insection 2.1, the ij-th entry of the matrix W is de-fined as in (Bellegarda, 2000)Wij = (1?
wi)cijcj,where cj is the total number of words in the j-thsentence pair.
This construction considers the im-portance of words globally (corpus wide) and locally(within sentence pairs).
Alternative constructions ofthe matrix are possible using raw counts or TF-IDF(Deerwester et al, 1990).W is a M ?
N sparse matrix, where M is thesize of vocabulary including both source and targetwords.
To obtain a compact representation, singularvalue decomposition (SVD) is employed (cf.
Berryet al(1993)) to yield W ?
W?
= U ?
S ?
V Tas Figure 3 shows, where, for some order Rmin(M,N) of the decomposition, U is a M?R leftsingular matrix with rows ui, i = 1, ?
?
?
,M , S is aR?R diagonal matrix of singular values s1 ?
s2 ?.
.
.
?
sR  0, and V is N?R a right singular ma-trix with rows vj , j = 1, ?
?
?
, N .
For each i, thescaled R-vector uiS may be viewed as representingwi, the i-th word in the vocabulary, and similarly thescaled R-vector vjS as representing dj , j-th docu-ment in the corpus.
Note that the uiS?s and vjS?sboth belong to IRR, the so-called LSA-space.
Alltarget and source words are projected into the sameLSA-space too.NM?RM?RR?NR?R orthonormalvectorsDocuments1wMw WordsWUSTV1dNdR orthonormalvectorsFigure 3: SVD of the Sparse Matrix W .As Equ.
(2) suggested, to induce semantic con-straints in a straightforward way, one would proceedas follows: firstly, perform word semantic cluster-ing with, say, their compact representations in theLSA-space; secondly, construct cluster generatingdependencies by specifying the conditional distribu-tion of P (F |E); and finally, for each word pair, in-duce the semantic constraint by considering all pos-sible semantic labeling schemes.
We approximatethis long process with simply finding word similar-ities defined by their cosine distance in the low di-mension space:Con(f, e) =12(cos(ufS, ueS) + 1) (3)The linear mapping above is introduced to avoidnegative constraints and to set the maximum con-straint value as 1.In building word alignment models, a special?NULL?
word is usually introduced to address tar-get words that align to no source words.
Since thisphysically non-existing word is not in the vocabu-lary of the bilingual LSA, we use the centroid of allsource words as its vector representation in the LSA-space.
The semantic constraints between ?NULL?and any target words can be derived in the same way.However, this is chosen for mostly computational4convenience, and is not the only way to address theempty word issue.2.2.2 Utilizing Word Alignment StatisticsWhile the simple bag-of-word model puts allsource words and target words as rows in the ma-trix, another method of deriving semantic constraintconstructs the sparse matrix by taking source wordsas rows and target words as columns and uses statis-tics from word alignment training to form word pairco-occurrence association.More specifically, we regard each target word f asa ?document?
and each source word e as a ?term?.The number of occurrences of the source word e inthe document f is defined as the expected numberof times that f generates e in the parallel corpusunder the word alignment model.
This method re-quires training the baseline word alignment modelin another direction by taking fs as source wordsand es as target words, which is often done forsymmetric alignments, and then dumping out thesoft counts when model converges.
We thresholdthe minimum word-to-word translation probabilityto remove word pairs that have low co-occurrencecounts.Following the similarity induced semantic con-straints in section 2.2.1, we need to find the distancebetween a term and a document.
Let vf be the pro-jection of the document representing the target wordf and ue the projection of the term representing thesource word e after performing SVD on the sparsematrix, we calculate the similarity between (f, e)and then find their semantic constraint to beCon(f, e) =12(cos(vfS1/2, ueS1/2) + 1) (4)Unlike the method in section 2.2.1, there is noempty word issue here since we do have statisticsof the ?NULL?
word as a source word generating ewords and therefore there is a ?document?
assignedto it.3 Experimental ResultsWe test our framework on the task of large vocab-ulary translation from dialectical (Iraqi) Arabic ut-terances into English.
The task covers multiple do-mains including travel, emergency medical diagno-sis, defense-oriented force protection, security andetc.
To avoid impacts of speech recognition errors,we only report experiments from text to text transla-tion.The training corpus consists of 390K sentencepairs, with total 2.43M Arabic words and 3.38M En-glish words.
These sentences are in typical spokentranscription form, i.e., spelling errors, disfluencies,such as word or phrase repetition, and ungrammat-ical utterances are commonly observed.
Arabic ut-terance length ranges from 3 to 70 words with theaverage of 6 words.There are 25K entries in the English vocabularyand 90K in Arabic side.
Data sparseness severelychallenges word alignment model and consequentlyautomatic phrase translation induction.
There are42K singletons in Arabic vocabulary, and 14K Ara-bic words with occurrence of twice each in the cor-pus.
Since Arabic is a morphologically rich lan-guage where affixes are attached to stem words toindicate gender, tense, case and etc, in order to re-duce vocabulary size and address out-of-vocabularywords, we split Arabic words into affix and root ac-cording to a rule-based segmentation scheme (Xianget al, 2006) with the help from the Buckwalter ana-lyzer (LDC, 2002) output.
This reduces the size ofArabic vocabulary to 52K.Our test data consists of 1294 sentence pairs.They are split into two parts: half of them is used asthe development set, on which training parametersand decoding feature weights are tuned, the otherhalf is for test.3.1 Training and Translation SetupStarting from the collection of parallel training sen-tences, we train word alignment models in two trans-lation directions, from English to Iraqi Arabic andfrom Iraqi Arabic to English, and derive two setsof Viterbi alignments.
By combining word align-ments in two directions using heuristics (Och andNey, 2003), a single set of static word alignmentsis then formed.
All phrase pairs which respect tothe word alignment boundary constraint are iden-tified and pooled to build phrase translation tableswith the Maximum Likelihood criterion.
We prunephrase translation entries by their probabilities.
Themaximum number of tokens in Arabic phrases is setto 5 for all conditions.Our decoder is a phrase-based multi-stack imple-5mentation of the log-linear model similar to Pharaoh(Koehn et al, 2003).
Like other log-linear modelbased decoders, active features in our translation en-gine include translation models in two directions,lexicon weights in two directions, language model,distortion model, and sentence length penalty.
Thesefeature weights are tuned on the dev set to achieveoptimal translation performance using downhill sim-plex method (Och and Ney, 2002).
The languagemodel is a statistical trigram model estimated withModified Kneser-Ney smoothing (Chen and Good-man, 1996) using all English sentences in the paral-lel training data.We measure translation performance by theBLEU score (Papineni et al, 2002) and TranslationError Rate (TER) (Snover et al, 2006) with one ref-erence for each hypothesis.
Word alignment mod-els trained with different constraints are comparedto show their effects on the resulting phrase transla-tion tables and the final translation performance.3.2 Translation ResultsOur baseline word alignment model is the word-to-word Hidden Markov Model (Vogel et al, 1996).Basic models in two translation directions aretrained simultaneously where statistics of two direc-tions are shared to learn symmetric translation lexi-con and word alignments with high precision moti-vated by (Zens et al, 2004) and (Liang et al, 2006).The baseline translation results (BLEU and TER) onthe dev and test set are presented in the line ?HMM?of Table 1.
We also compare with results of IBMModel-4 word alignments implemented in GIZA++toolkit (Och and Ney, 2003).We study and compare two types of constraint andsee how they affect word alignments and translationoutput.
One is based on the entropy principle as de-scribed in Section 2.1, where ?
is set to 0.9; Theother is based on bilingual latent semantic analysis.For the simple bag-of-word bilingual LSA as de-scribed in Section 2.2.1, after SVD on the sparse ma-trix using the toolkit SVDPACK (Berry et al, 1993),all source and target words are projected into a low-dimensional (R = 88) LSA-space.
Word pair se-mantic constrains are calculated based on their sim-ilarity as in Equ.
3 before word alignment training.Like the baseline, we perform 6 iterations of IBMModel-1 training and then 4 iteration of HMM train-ing.
The semantic constraints are used to guide wordalignment model training for each iteration.
TheBLEU score and TER with this constraint are shownin the line ?BiLSA-1?
of Table 1.To exploit word alignment statistics in bilingualLSA as described in Section 2.2.2, we dump out thestatistics of the baseline word alignment model anduse them to construct the sparse matrix.
We findlow-dimensional representation (R = 67) of Englishwords and Arabic words and use their similarity toestablish semantic constraints as in Equ.
4.
Thetraining procedure is the same as the baseline and?BiLSA-1?.
The translation results with these wordalignments are shown as ?BiLSA-2?
in Table 1.As Table 1 shows, when the entropy based con-straints are applied, BLEU score improves 0.5 pointon the test set.
Clearly, when bilingual LSA con-straints are applied, translation performance can beimproved up to 1.6 BLEU points.
We also observethat TER can drop 2.1 points with the ?BiLSA-1?constraint.While ?BiLSA-1?
constraint performs better onthe test set, ?BiLSA-2?
constraint achieves slightlyhigher BLEU score on the dev set.
We thentry a simple combination of these two typesof constraints, that is the geometric mean ofConBiLSA?1(f, e) andConBiLSA?2(f, e), and findout that BLEU score can be improved a little bit fur-ther on both sets as the line ?Mix?
shows.We notice that the relatively simpler HMM modelcan perform comparable or better than the sophis-ticated Model-4 when proper constraints are activein guiding word alignment model training.
We alsotry to put constraints in Model-4.
As the Equation1 implies, when a word-to-word generative proba-bility is needed, one should multiply correspondinglexicon entry in the t-table with the word pair con-straint.
We simply modify the GIZA++ toolkit (Ochand Ney, 2003) by always weighting lexicon proba-bilities with soft constraints during iterative modeltraining, and obtain 0.7% TER reduction on bothsets and 0.4% BLEU improvement on the test set.3.3 AnalysisTo understand how prior knowledge encoded as softconstraints plays a role in guiding word alignmenttraining, we compare statistics of different wordalignment models.
We find that our baseline HMM6Table 1: Translation Results with different wordalignments.BLEU TERAlignmentsdev test dev testModel-4 0.310 0.296 0.528 0.530+Mix 0.306 0.300 0.521 0.523HMM 0.289 0.288 0.543 0.542+Entropy 0.289 0.293 0.534 0.536+BiLSA-1 0.294 0.300 0.531 0.521+BiLSA-2 0.298 0.292 0.530 0.528+Mix 0.302 0.304 0.532 0.524generates 2.6% less number of total word links thanthat of Model-4.
Part of the reason is that mod-els of two directions in the baseline are trained si-multaneously.
The requirement of bi-directional ev-idence places a certain constraint on word align-ments.
When ?BiLSA-1?
constraints are applied inthe baseline model, 2.7% less number of total wordlinks are hypothesized, and consequently, less num-ber of Arabic n-gram translations in the final phrasetranslation table are induced.
The observation sug-gests that the constraints improve word alignmentprecision and accuracy of phrase translation tablesas well.bAl_ mrM mAl _tkin your esophagusHMMbAl_ mrM mAl _tkin your esophagus+BiLSA-1bAl_ mrM mAl _tkin your esophagusModel-4(in) (esophagus) gloss (ownership) (yours)Figure 4: An example of word alignments under dif-ferent modelsFigure 4 shows example word alignments of a par-tial sentence pair.
The complete English sentence is?have you ever had like any reflux diseases in youresophagus?.
We notice that the Arabic word ?mrM?
(means esophagus) appears only once in the corpus.Some of the word pair constraints are listed in Ta-ble 2.
The example demos that due to reasonableconstraints placed in word alignment training, thelink to ?
tK?
is corrected and consequently we haveaccurate word translation for the Arabic singletonTable 2: Word pair constraint valuesEnglish e Arabic f ConBiLSA?1(f, e)esophagus mrM 0.6424mAl 0.1819tk 0.2897your mrM 0.6319mAl 0.4930tk 0.9672?mrM?.4 Related WorkHeuristics based on co-occurrence analysis, such aspoint-wise mutual information or Dice coefficients, have been shown to be indicative for word align-ments (Zhang and Vogel, 2005; Melamed, 2000).The framework presented in this paper demonstratesthe possibility of taking heuristics as constraintsguiding statistical generative word alignment modeltraining.
Their effectiveness can be expected espe-cially when data sparseness is severe.Discriminative word alignment models, such asIttycheriah and Roukos (2005); Moore (2005);Blunsom and Cohn (2006), have received greatamount of study recently.
They have proven that lin-guistic knowledge is useful in modeling word align-ments under log-linear distributions as morphologi-cal, semantic or syntactic features.
Our frameworkproposes to exploit these features differently by tak-ing them as soft constraints of translation lexicon un-der a generative model.While word alignments can help identifying se-mantic relations (van der Plas and Tiedemann,2006), we proceed in the reverse direction.
We in-vestigate the impact of semantic constraints on sta-tistical word alignment models as prior knowledge.In (Ma et al, 2004), bilingual semantic maps areconstructed to guide word alignment.
The frame-work we proposed seamlessly integrates derived se-mantic similarities into a statistical word alignmentmodel.
And we extended monolingual latent seman-tic analysis in bilingual applications.Toutanova et al (2002) augmented bilingual sen-tence pairs with part-of-speech tags as linguisticconstraints for HMM-based word alignments.
Theconstraints between tags are automatically learnedin a parallel generative procedure along with lex-7icon.
We have introduced hidden tags between aword pair to specialize their soft constraints, whichserve as prior knowledge that will be used in guidingword alignment model training.
Constraint betweentags are embedded into the word to word generativeprocess.5 Conclusions and Future WorkWe have presented a simple and effective frameworkto incorporate prior knowledge such as heuristicsor linguistic features into statistical generative wordalignment models.
Prior knowledge serves as softconstraints that shall be placed on translation lexi-con to guide word alignment model training and dis-ambiguation during Viterbi alignment process.
Westudied two types of constraints that can be obtainedautomatically from data and showed improved per-formance (up to 1.6% absolute BLEU increase or2.1% absolute TER reduction) in translating dialec-tical Arabic into English.
Future work includes im-plementing the idea in alternative alignment mod-els and also exploiting prior knowledge derived fromsuch as manually-aligned data and pre-existing lin-guistic resources.AcknowledgementWe thank Mohamed Afify fordiscussions and the anonymous reviewers for sug-gestions.ReferencesJ.
R. Bellegarda.
2000.
Exploiting latent semantic informa-tion in statistical language modeling.
Proc.
of the IEEE,88(8):1279?1296, August.M.
Berry, T. Do, and S. Varadhan.
1993.
Svdpackc (version1.0) user?s guide.
Tech.
report cs-93-194, University of Ten-nessee, Knoxville, TN.P.
Blunsom and T. Cohn.
2006.
Discriminative word alignmentwith conditional random fields.
In Proc.
of COLING/ACL,pages 65?72.P.
Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.
1993.The mathematics of machine translation: Parameter estima-tion.
Computational Linguistics, 19:263?312.S.
F. Chen and J. Goodman.
1996.
An empirical study ofsmoothing techniques for language modeling.
In Proc.
ofACL, pages 310?318.S.
C. Deerwester, S. T. Dumais, T. K. Landauer, G. W. Furnas,and R. A. Harshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society of InformationScience, 41(6):391?407.A.
Fraser and D. Marcu.
2006.
Semi-supervised training forstatistical word alignment.
In Proc.
of COLING/ACL, pages769?776.A.
Ittycheriah and S. Roukos.
2005.
A maximum entropy wordaligner for arabic-english machine translation.
In Proc.
ofHLT/EMNLP, pages 89?96.P.
Koehn, F. Och, and D. Marcu.
2003.
Statistical phrase-basedtranslation.
In Proc.
of HLT-NAACL.LDC, 2002.
Buckwalter Arabic Morphological Analyzer Ver-sion 1.0.
LDC Catalog Number LDC2002L49.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment by agree-ment.
In Proc.
of HLT/NAACL, pages 104?111.Q.
Ma, K. Kanzaki, Y. Zhang, M. Murata, and H. Isahara.2004.
Self-organizing semantic maps and its application toword alignment in japanese-chinese parallel corpora.
NeuralNetw., 17(8-9):1241?1253.I.
Dan.
Melamed.
2000.
Models of translational equivalenceamong words.
Computational Linguistics, 26(2):221?249.R.
C. Moore.
2005.
A discriminative framework for bilingualword alignment.
In Proc.
of HLT/EMNLP, pages 81?88.F.
J. Och and H. Ney.
2002.
Discriminative training and max-imum entropy models for statistical machine translation.
InProc.
of ACL, pages 295?302.F.
J. Och and H. Ney.
2003.
A systematic comparison of vari-ous statistical alignment models.
Computational Linguistics,29(1):19?51.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.
Bleu: amethod for automatic evaluation of machine translation.
InProc.
of ACL, pages 311?318.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, and J. Makhoul.2006.
A study of translation edit rate with targeted humanannotation.
In Proc.
of AMTA.K.
Toutanova, H. T. Ilhan, and C. Manning.
2002.
Extentionsto HMM-based statistical word alignment models.
In Proc.of EMNLP.Lonneke van der Plas and Jo?rg Tiedemann.
2006.
Finding syn-onyms using automatic word alignment and measures of dis-tributional similarity.
In Proc.
of the COLING/ACL 2006Main Conference Poster Sessions, pages 866?873.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM based wordalignment in statistical translation.
In Proc.
of COLING.B.
Xiang, K. Nguyen, L. Nguyen, R. Schwartz, and J. Makhoul.2006.
Morphological decomposition for arabic broadcastnews transcription.
In Proc.
of ICASSP, pages 1089?1092.R.
Zens, E. Matusov, and H. Ney.
2004.
Improved word align-ment using a symmetric lexicon model.
In Proc.
of COL-ING, pages 36?42.Y.
Zhang and S. Vogel.
2005.
Competitive grouping in inte-grated phrase segmentation and alignment model.
In Proc.of the ACL Workshop on Building and Using Parallel Texts,pages 159?162.8
