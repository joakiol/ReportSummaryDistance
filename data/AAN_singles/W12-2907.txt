NAACL-HLT 2012 Workshop on Speech and Language Processing for Assistive Technologies (SLPAT), pages 47?55,Montre?al, Canada, June 7?8, 2012. c?2012 Association for Computational LinguisticsCommunication strategies for a computerized caregiver for individuals withAlzheimer?s diseaseFrank Rudzicz1,2, ?
and Rozanne Wilson1 and Alex Mihailidis2 and Elizabeth Rochon11 Department of Speech-Language Pathology,2 Department of Occupational Science and Occupational TherapyUniversity of TorontoToronto CanadaCarol LeonardSchool of Rehabilitation SciencesUniversity of OttawaOttawa CanadaAbstractCurrently, health care costs associated withaging at home can be prohibitive if individ-uals require continual or periodic supervisionor assistance because of Alzheimer?s disease.These costs, normally associated with humancaregivers, can be mitigated to some extentgiven automated systems that mimic some oftheir functions.
In this paper, we present in-augural work towards producing a generic au-tomated system that assists individuals withAlzheimer?s to complete daily tasks using ver-bal communication.
Here, we show how toimprove rates of correct speech recognitionby preprocessing acoustic noise and by mod-ifying the vocabulary according to the task.We conclude by outlining current directions ofresearch including specialized grammars andautomatic detection of confusion.1 IntroductionIn the United States, approximately $100 billion arespent annually on the direct and indirect care of in-dividuals with Alzheimer?s disease (AD), the major-ity of which is attributed to long-term institutionalcare (Ernst et al, 1997).
As the population ages, theincidence of AD will double or triple, with Medi-care costs alone reaching $189 billion in the US by2015 (Bharucha et al, 2009).
Given the growingneed to support this population, there is an increas-ing interest in the design and development of tech-nologies that support this population at home andextend ones quality of life and autonomy (Mihailidiset al, 2008).
?Contact: frank@cs.toronto.eduAlzheimer?s disease is a type of progres-sive neuro-degenerative dementia characterized bymarked declines in mental acuity, specifically incognitive, social, and functional capacity.
A declinein memory (short- and long-term), executive capac-ity, visual-spacial reasoning, and linguistic abilityare all typical effects of AD (Cummings, 2004).These declines make the completion of activities ofdaily living (e.g., finances, preparing a meal) diffi-cult and more severe declines often necessitate care-giver assistance.
Caregivers who assist individualswith AD at home are common, but are often the pre-cursor to placement in a long-term care (LTC) facil-ity (Gaugler et al, 2009).We are building systems that automate, wherepossible, some of the support activities that currentlyrequire family or formal (i.e., employed) caregivers.Specifically, we are designing an intelligent dialogcomponent that can engage in two-way speech com-munication with an individual in order to help guidethat individual towards the completion of certaindaily household tasks, including washing ones handsand brushing ones teeth.
A typical installation setupin a bathroom, shown in figure 1, consists of videocameras that track a user?s hands and the area in andaround the sink, as well as microphones, speakers,and a screen that can display prompting informa-tion.
Similar installations are being tested in otherhousehold rooms as part of the COACH project (Mi-hailidis et al, 2008), according to the task; this isan example of ambient intelligence in which tech-nology embedded in the environment is sensitive tothe activities of the user with it (Spanoudakis et al,2010).47Our goal is to encode in software the kinds oftechniques used by caregivers to help their clientsachieve these activities; this includes automati-cally identifying and recovering from breakdownsin communication and flexibly adapting to the in-dividual over time.
Before such a system can be de-ployed, the underlying models need to be adjustedto the desired population and tasks.
Similarly, thespeech output component would need to be pro-grammed according to the vocabularies, grammars,and dialog strategies used by caregivers.
This paperpresents preliminary experiments towards dedicatedspeech recognition for such a system.
Evaluationdata were collected as part of a larger project exam-ining the use of communication strategies by formalcaregivers while assisting residents with moderate tosevere AD during the completion of toothbrushing(Wilson et al, 2012).2 Background ?
communication strategiesAutomated communicative systems that are moresensitive to the emotive and the mental states of theirusers are often more successful than more neutralconversational agents (Saini et al, 2005).
In order tobe useful in practice, these communicative systemsneed to mimic some of the techniques employedby caregivers of individuals with AD.
Often, thesecaregivers are employed by local clinics or medicalinstitutions and are trained by those institutions inideal verbal communication strategies for use withthose having dementia (Hopper, 2001; Goldfarb andPietro, 2004).
These include (Small et al, 2003) butare not limited to:1.
Relatively slow rate of speech rate.2.
Verbatim repetition of misunderstood prompts.3.
Closed-ended questions (i.e., that elicit yes/noresponses).4.
Simple sentences with reduced syntactic com-plexity.5.
Giving one question or one direction at a time.6.
Minimal use of pronouns.These strategies, though often based on observa-tional studies, are not necessarily based on quantita-tive empirical research and may not be generalizableacross relevant populations.
Indeed, Tomoeda et al(1990) showed that rates of speech that are too slow(a) Environmental setup(b) On-screen promptingFigure 1: Setup and on-screen prompting for COACH.The environment includes numerous sensors includingmicrophones and video cameras as well as a screen uponwhich prompts can be displayed.
In this example, theuser is prompted to lather their hands after having appliedsoap.
Images are copyright Intelligent Assistive Technol-ogy and Systems Lab).may interfere with comprehension if they introduce48problems of short-term retention of working mem-ory.
Small, Andersen, and Kempler (1997) showedthat paraphrased repetition is just as effective as ver-batim repetition (indeed, syntactic variation of com-mon semantics may assist comprehension).
Further-more, Rochon, Waters, and Caplan (2000) suggestedthat the syntactic complexity of utterances is notnecessarily the only predictor of comprehension inindividuals with AD; rather, correct comprehensionof the semantics of sentences is inversely related tothe increasing number of propositions used ?
it ispreferable to have as few clauses or core ideas aspossible, i.e., one-at-a-time.Although not the empirical subject of this pa-per, we are studying methods of automating theresolution of communication breakdown.
Much ofthis work is based on the Trouble Source-Repair(TSR) model in which difficulties in speaking, hear-ing, or understanding are identified and repairs areinitiated and carried out (Schegloff, Jefferson, andSacks, 1977).
Difficulties can arise in a numberof dimensions including phonological (i.e., mispro-nunciation), morphological/syntactic (e.g., incorrectagreement among constituents), semantic (e.g., dis-turbances related to lexical access, word retrieval,or word use), and discourse (i.e., misunderstandingof topic, shared knowledge, or cohesion) (Orange,Lubinsky, and Higginbotham, 1996).
The major-ity of TSR sequences involve self-correction of aspeaker?s own error, e.g., by repetition, elaboration,or reduction of a troublesome utterance (Schegloff,Jefferson, and Sacks, 1977).
Orange, Lubinsky,and Higginbotham (1996) showed that while 18%of non-AD dyad utterances involved TSR, whereas23.6% of early-stage AD dyads and 33% of middle-stage AD dyads involved TSR.
Of these, individu-als with middle-stage AD exhibited more discourse-related difficulties including inattention, failure totrack propositions and thematic information, anddeficits in working memory.
The most commonrepair initiators and repairs given communicationbreakdown involved frequent ?wh-questions and hy-potheses (e.g., ?Do you mean??).
Conversationalpartners of individuals with middle-stage AD initi-ated repair less frequently than conversational part-ners of control subjects, possibly aware of their de-teriorating ability, or to avoid possible further con-fusion.
An alternative although very closely relatedparadigm for measuring communication breakdownis Trouble Indicating Behavior (TIB) in which theconfused participant implicitly or explicitly requestsaid.
In a study of 7 seniors with moderate/severe de-mentia and 3 with mild/moderate dementia, Watson(1999) showed that there was a significant differencein TIB use (?
< 0.005) between individuals withAD and the general population.
Individuals withAD are most likely to exhibit dysfluency, lack of up-take in the dialog, metalinguistic comments (e.g., ?Ican?t think of the word?
), neutral requests for repeti-tion, whereas the general population are most likelyto exhibit hypothesis formation to resolve ambiguity(e.g., ?Oh, so you mean that you had a good time??
)or requests for more information.2.1 The task of handwashingOur current work is based on a study completed byWilson et al (2012) towards a systematic observa-tional representation of communication behavioursof formal caregivers assisting individuals with mod-erate to severe AD during hand washing.
In thatstudy, caregivers produced 1691 utterances, 78% ofwhich contained at least one communication strat-egy.
On average, 23.35 (?
= 14.11) verbal strate-gies and 7.81 (?
= 5.13) non-verbal strategies wereused per session.
The five most common communi-cation strategies employed by caregivers are rankedin table 1.
The one proposition strategy refers tousing a single direction, request, or idea in the utter-ance (e.g.
?turn the water on?).
The closed-endedquestion strategy refers to asking question with avery limited, typically binary, response (e.g., ?canyou turn the taps on??)
as opposed to questions elic-iting a more elaborate response or the inclusion ofadditional information.
The encouraging commentsstrategy refers to any verbal praise of the resident(e.g., ?you are doing a good job?).
The paraphrasedrepetition strategy is the restatement of a misunder-stood utterance using alternative syntactic or lexicalcontent (e.g., ?soap up your hands....please use soapon your hands?).
There was no significant differencebetween the use of paraphrased and verbatim repe-tition of misunderstood utterances.
Caregivers alsoreduced speech rate from an average baseline of 116words per minute (s.d.
36.8) to an average of 36.5words per minute (s.d.
19.8).The least frequently used communication strate-49Number of occurrences % use of strategy Uses per sessionVerbal strategy Overall Successful Overall Successful Mean SDOne proposition 619 441 35 36 8.6 6.7Closed-ended question 215 148 12 12 3.0 3.0Encouraging comments 180 148 10 12 2.9 2.5Use of resident?s name 178 131 10 11 2.8 2.5Paraphrased repetition 178 122 10 10 3.0 2.5Table 1: Most frequent verbal communication strategies according to their number of occurrences in dyad communi-cation.
The % use of strategy is normalized across all strategies, most of which are not listed.
These results are splitaccording to the total number of uses and the number of uses in successful resolution of a communication breakdown.Mean (and standard deviation) of uses per session are given across caregivers.
Adapted with permission from Wilsonet al (2012).gies employed by experienced caregivers involvedasking questions that required verification of a res-ident?s request or response (e.g., ?do you meanthat you are finished??
), explanation of current ac-tions (e.g., ?I am turning on the taps for you?
), andopen-ended questions (e.g., ?how do you wash yourhands??
).The most common non-verbal strategies em-ployed by experienced caregivers were guided touch(193 times, 122 of which were successful) in whichthe caregiver physically assists the resident in thecompletion of a task, demonstrating action (113times.
72 of which were successful) in which anaction is illustrated or mimicked by the caregiver,handing an object to the resident (107 times, 85 ofwhich were successful), and pointing to an object(105 times, 95 of which were successful) in whichthe direction to an object is visually indicated bythe caregiver.
Some of these strategies may be em-ployed by the proposed system; for example, videosdemonstrating an action may be displayed on thescreen shown in figure 1(a), which may replace tosome extent the mimicry by the caregiver.
A pos-sible replication of the fourth most common non-verbal strategy may be to highlight the required ob-ject with a flashing light, a spotlight, or by display-ing it on screen; these solutions require tangentialtechnologies that are beyond the scope of this cur-rent study, however.3 DataOur experiments are based on data collected by Wil-son et al (submitted) with individuals diagnosedwith moderate-to-severe AD who were recruitedfrom long-term care facilities (i.e., The Harold andGrace Baker Centre and the Lakeside Long-TermCare Centre) in Toronto.
Participants had no pre-vious history of stroke, depression, psychosis, alco-holism, drug abuse, or physical aggression towardscaregivers.
Updated measures of disease severitywere taken according to the Mini-Mental State Ex-amination (Folstein, Folstein, and McHugh, 1975).The average cognitive impairment among 7 individ-uals classified as having severe AD (scores below10/30) was 3.43 (?
= 3.36) and among 6 individ-uals classified as having moderate AD (scores be-tween 10/30 and 19/30) was 15.8 (?
= 4.07).
Theaverage age of residents was 81.4 years with an aver-age of 13.8 years of education and 3.1 years of resi-dency at their respective LTC facility.
Fifteen formalcaregivers participated in this study and were pairedwith the residents (i.e., as dyads) during the comple-tion of activities of daily living.
All but one care-giver were female and were comfortable with En-glish.
The average number of years of experienceworking with AD patients was 12.87 (?
= 9.61).The toothbrushing task follows the protocol of thehandwashing task.
In total, the data consists of 336utterances by the residents and 2623 utterances bytheir caregivers; this is manifested by residents utter-ing 1012 words and caregivers uttering 12166 wordsin total, using 747 unique terms.
The toothbrushingtask consists of 9 subtasks, namely: 1) get brush andpaste, 2) put paste on brush, 3) turn on water, 4) wettooth brush, 5) brush teeth, 6) rinse mouth, 7) rinsebrush, 8) turn off water, 9) dry mouth.These data were recorded as part of a largeproject to study communication strategies of care-givers rather than to study the acoustics of theirtransactions with residents.
As a result, the record-50ings were not of the highest acoustic quality; forexample, although the sampling rate and bit ratewere high (48 kHz and 384 kbps respectively), thevideo camera used was placed relatively far from thespeakers, who generally faced away from the mi-crophone towards the sink and running water.
Thedistribution of strategies employed by caregivers forthis task is the subject of ongoing work.4 Experiments in speech recognitionOur first component of an automated caregiveris the speech recognition subsystem.
We testtwo alternative systems, namely Carnegie Mellon?sSphinx framework and Microsoft?s Speech Plat-form.
Carnegie Mellon?s Sphinx framework (pock-etsphinx, specifically) is an open-source speechrecognition system that uses traditional N -gramlanguage modeling, sub-phonetic acoustic hiddenMarkov models (HMMs), Viterbi decoding andlexical-tree structures (Lamere et al, 2003).
Sphinxincludes tools to perform traditional Baum-Welchestimation of acoustic models, but there were notenough data for this purpose.
The second ASR sys-tem, Microsoft?s Speech Platform (version 11) isless open but exposes the ability to vary the lexicon,grammar, and semantics.
Traditionally, Microsofthas used continuous-density HMMs with 6000 tiedHMM states (senones), 20 Gaussians per state, andMel-cepstrum features (with delta and delta-delta).Given the toothbrushing data described in section3, two sets of experiments were devised to config-ure these systems to the task.
Specifically, we per-form preprocessing of the acoustics to remove envi-ronmental noise associated with toothbrushing andadapt the lexica of the two systems, as described inthe following subsections.4.1 Noise reductionAn emergent feature of the toothbrushing data isvery high levels of acoustic noise caused by therunning of water.
In fact, the estimated signal-to-noise ratio across utterances range from ?2.103 dBto 7.63 dB, which is extremely low; for comparisonclean speech typically has an SNR of approximately40 dB.
Since the resident is likely to be situated closeto this source of the acoustic noise, it becomes im-portant to isolate their speech in the incoming signal.Speech enhancement involves the removal ofacoustic noise d(t) in a signal y(t), including am-bient noise (e.g., running water, wind) and signaldegradation giving the clean ?source?
signal x(t).This involves an assumption that noise is strictly ad-ditive, as in the formula:y(t) = x(t) + d(t).
(1)Here, Yk, Xk, and Dk are the kth spectra of thenoisy observation y(t), source signal x(t), and un-correlated noise signal d(t), respectively.
Generally,the spectral magnitude of a signal is more importantthan its phase when assessing signal quality and per-forming speech enhancement.
Spectral subtraction(SS), as the name suggests, subtracts an estimate ofthe noisy spectrum from the measured signal (Boll,1979; Martin, 2001), where the estimate of the noisysignal is estimated from samples of the noise sourceexclusively.
That is, one has to learn estimates basedon pre-selected recordings of noise.
We apply SSspeech enhancement given sample recordings of wa-ter running.
The second method of enhancementwe consider is the log-spectral amplitude estimator(LSAE) which minimizes the mean squared error(MMSE) of the log spectra given a model for thesource speech Xk = Ak exp(j?k), where Ak is thespectral amplitude.
The LSAE method is a modifi-cation to the short-time spectral amplitude estima-tor that attempts to find some estimate A?k that min-imizes the distortionE[(logAk ?
log A?k)2], (2)such that the log-spectral amplitude estimate isA?k = exp (E [lnAk |Yk])=?k1 + ?kexp(12?
?vke?ttdt)Rk,(3)where ?k is the a priori SNR,Rk is the noisy spectralamplitude, vk =?k1+?k?k, and ?k is the a posterioriSNR (Erkelens, Jensen, and Heusdens, 2007).
Of-ten this is based on a Gaussian model of noise, asit is here (Ephraim and Malah, 1985).
We enhanceour recordings by both the SS and LSAE methods.Archetypal instances of typical, low, and (relatively)high SNR waveform recordings and their enhancedversions are shown in 4.1.51(a) Dyad1.1(b) Dyad4.2(c) Dyad11.1Figure 2: Representative samples of toothbrushing dataaudio.
Figures show normalized amplitude over time forsignals cleaned by the LSAE method overlaid over thelarger-amplitude original signals.We compare the effects of this enhanced audioacross two ASR systems.
For the Sphinx system,we use a continuous tristate HMM for each of the 40phones from the CMU dictionary trained with audiofrom the complete Wall Street Journal corpus andthe independent variable we changed was the num-ber of Gaussians per state (n.
?).
These parame-ters are not exposed by the Microsoft speech system,so we instead vary the minimum threshold of confi-dence C ?
[0..1] required to accept a word; in theorylower values of C would result in more insertion er-rors and higher values would result in more deletionerrors.
For each system, we used a common dic-tionary of 123, 611 unique words derived from theCarnegie Mellon phonemic dictionary.Table 2 shows the word error rate for each ofthe two systems.
Both the SS and LSAE methodsof speech enhancement result in significantly betterword error rates than with the original recordings atthe 99.9% level of confidence according to the one-tailed paired t-test across both systems.
The LSAEmethod has significantly better word error rates thanthe SS method at the 99% level of confidence withthis test.
Although these high WERs are impracticalfor a typical system, they are comparable to other re-sults for speech recognition in very low-SNR envi-ronments (Kim and Rose, 2003).
Deng et al (2000),for example, describe an ASR system trained withclean speech that has a WER of 87.11% given addi-tive white noise for a resulting 5 dB SNR signal fora comparable vocabulary of 5000 words.
An inter-esting observation is that even at the low confidencethreshold of C = 0.2, the number of insertion er-rors did not increase dramatically relative to for thehigher values in the Microsoft system; only 4.0% ofall word errors were insertion errors at C = 0.2, and2.7% of all word errors at C = 0.8.Given Levenshtein alignments between annotatedtarget (reference) and hypothesis word sequences,we separate word errors across residents and acrosscaregivers.
Specifically, table 3 shows the propor-tion of deletion and substitution word errors (relativeto totals for each system separately) across residentsand caregivers.
This analysis aims to uncover dif-ferences in rates of recognition between those withAD and the more general population.
For exam-ple, 12.6% of deletion errors made by Sphinx werewords spoken by residents.
It is not possible to at-52Word error rate %Parameters Original SS LSAESphinxn.
?
= 4 98.13 75.31 70.61n.
?
= 8 98.13 74.95 69.66n.
?
= 16 97.82 75.09 69.78n.
?
= 32 97.13 74.88 67.22MicrosoftC = 0.8 97.67 73.59 67.11C = 0.6 97.44 72.57 67.08C = 0.4 96.85 71.78 66.54C = 0.2 94.30 71.36 64.32Table 2: Word error rates for the Sphinx and MicrosoftASR systems according to their respective adjusted pa-rameters, i.e., number of Gaussians per HMM state (n.
?
)and minimum confidence threshold (C).
Results are givenon original recordings and waveforms enhanced by spec-tral subraction (SS) and MMSE with log-spectral ampli-tude estimates (LSAE).tribute word insertion errors to either the resident orcaregiver, in general.
If we assume that errors shouldbe distributed across residents and caregivers in thesame proportion as their respective total number ofwords uttered, then we can compute the Pearson ?2statistic of significance.
Given that 7.68% of allwords were uttered by residents, the observed num-ber of substitutions was significantly different thanthe expected value at the 99% level of confidencefor both the Sphinx and Microsoft systems, but thenumber of deletions was not significantly differenteven at the 95% level of confidence.
In either case,however, substantially more errors are made propor-tionally by residents than we might expect; this mayin part be caused by their relatively soft speech.Proportion of errorsSphinx MicrosoftRes.
Careg.
Res.
Careg.deletion 13.9 86.1 12.6 87.4substitution 23.2 76.8 18.4 81.6Table 3: Proportion of deletion and substitution errorsmade by both (Res)idents and (Careg)ivers.
Proportionsare relative to totals within each system.4.2 Task-specific vocabularyWe limit the common vocabulary used in eachspeech recognizer in order to be more specific to thetask.
Specifically, we begin with the 747 words ut-tered in the data as our most restricted vocabulary.Then, we expand this vocabulary according to twomethods.
The first method adds words that are se-mantically similar to those already present.
Thisis performed by taking the most common sense foreach noun, verb, adjective, and adverb, then addingeach entry in the respective synonym sets accord-ing to WordNet 3.0 (Miller, 1995).
This results ina vocabulary of 2890 words.
At this point, we it-eratively add increments of words at intervals of10, 000 (up to 120, 000) by selecting random wordsin the vocabulary and adding synonym sets for allsenses as well as antonyms, hypernyms, hyponyms,meronyms, and holonyms.
The result is a vocabu-lary whose semantic domain becomes increasinglygeneric.
The second approach to adjusting the vo-cabulary size is to add phonemic foils to more re-stricted vocabularies.
Specifically, as before, we be-gin with the restricted 747 words observed in thedata but then add increments of new words thatare phonemically similar to existing words.
Thisis done exhaustively by selecting a random wordand searching for minimal phonemic misalignments(i.e., edit distance) among out-of-vocabulary wordsin the Carnegie Mellon phonemic dictionary.
Thisapproach of adding decoy words is an attempt tomodel increasing generalization of the systems.
Ev-ery vocabulary is translated into the format expectedby each recognizer so that each test involves a com-mon set of words.Word error rates are measured for each vocabu-lary size across each ASR system and the manner inwhich those vocabularies were constructed (seman-tic or phonemic expansion).
The results are shownin figure 4.2 and are based on acoustics enhancedby the LSAE method.
Somewhat surprisingly, themethod used to alter the vocabulary did appear tohave a very large effect.
Indeed, the WER acrossthe semantic and phonemic methods were correlatedat ?
>= 0.99 across both ASR systems; there wasno significant difference between traces (within sys-tem) even at the 60% level of confidence using thetwo-tailed heteroscedastic t-test.5 Ongoing workThis work represents the first phase of developmenttowards a complete communicative artificial care-giver for the home.
Here, we are focusing on the53102 103 104 105 1063540455055606570Vocabulary sizeWord ErrorRate (%)Sphinx ?
PhonemicMicrosoft ?
PhonemicSphinx ?
SemanticMicrosoft ?
SemanticFigure 3: Word error rate versus size of vocabulary (logscale) for each of the Sphinx and Microsoft ASR systemsaccording to whether the vocabularies were expanded bysemantic or phonemic similarity.speech recognition component and have shown re-ductions in error of up to 72% (Sphinx ASR withn.?
= 4) and 63.1% (Sphinx ASR), relative to base-line rates of error.
While significant, baseline er-rors were so severe that other techniques will needto be explored.
We are now collecting additionaldata by fixing the Microsoft Kinect sensor in theenvironment, facing the resident; this is the defaultconfiguration and may overcome some of the ob-stacles present in our data.
Specifically, the beam-forming capabilities in the Kinect (generalizable toother multi-microphone arrays) can isolate speechevents from ambient environmental noise (Balan andRosca, 2002).
We are also collecting speech data fora separate study in which individuals with AD areplaced before directional microphones and completetasks related to the perception of emotion.As tasks can be broken down into non-linear (par-tially ordered) sets of subtasks (e.g., replacing thetoothbrush is a subtask of toothbrushing), we arespecifying grammars ?by hand?
specific to those sub-tasks.
Only some subset of all subtasks are possibleat any given time; e.g., one can only place tooth-paste on the brush once both items have been re-trieved.
The possibility of these subtasks depend onthe state of the world which can only be estimatedthrough imperfect techniques ?
typically computervision.
Given the uncertainty of the state of theworld, we are integrating subtask-specific grammarsinto a partially-observable Markov decision process(POMDP).
These grammars include the semanticstate variables of the world and break each taskdown into a graph-structure of interdependent ac-tions.
Each ?action?
is associated with its own gram-mar subset of words and phrases that are likely tobe uttered during its performance, as well as a setof prompts to be spoken by the system to aid theuser.
Along these lines, we we will attempt to gen-eralize the approach taken in section 4.2 to gener-ate specific sub-vocabularies automatically for eachsubtask.
The relative weighting of words will bemodeled based on ongoing data collection.AcknowledgmentsThis research was partially funded by Mitacs andan operating grant from the Canadian Institutes ofHealth Research and the American Alzheimer As-sociation (ETAC program).
The authors acknowl-edge and thank the administrative staff, caregivers,and residents at the Harold and Grace Baker Centreand the Lakeside Long-Term Care Centre.ReferencesBalan, Radu and Justinian Rosca.
2002.
MicrophoneArray Speech Enhancement by Bayesian Estimationof Spectral Amplitude and Phase.
In Proceedings ofIEEE Sensor Array and Multichannel Signal Process-ing Workshop.Bharucha, Ashok J., Vivek Anand, Jodi Forlizzi,Mary Amanda Dew, Charles F. Reynolds III, ScottStevens, and Howard Wactlar.
2009.
Intelligent assis-tive technology applications to dementia care: Currentcapabilities, limitations, and future challenges.
Amer-ican Journal of Geriatric Psychiatry, 17(2):88?104,February.Boll, S.F.
1979.
Suppression of acoustic noise in speechusing spectral subtraction.
IEEE Transactions onAcoustics, Speech, and Signal Processing, 27(2):113?120, April.Cummings, Jeffrey L. 2004.
Alzheimer?s disease.
NewEngland Journal of Medicine, 351(1):56?67.Deng, Li, Alex Acero, M. Plumpe, and Xuedong Huang.2000.
Large-vocabulary speech recognition under ad-verse acoustic environments.
In Proceedings of the In-ternational Conference on Spoken Language Process-ing, October.54Ephraim, Y. and D. Malah.
1985.
Speech enhancementusing a minimum mean-square error log-spectral am-plitude estimator.
Acoustics, Speech and Signal Pro-cessing, IEEE Transactions on, 33(2):443 ?
445, apr.Erkelens, Jan, Jesper Jensen, and Richard Heusdens.2007.
A data-driven approach to optimizing spectralspeech enhancement methods for various error crite-ria.
Speech Communication, 49:530?541.Ernst, Richard L., Joel W. Hay, Catharine Fenn, JaredTinklenberg, and Jerome A. Yesavage.
1997.
Cog-nitive function and the costs of alzheimer disease ?an exploratory study.
Archives of Neurology, 54:687?693.Folstein, Marshal F., Susan E. Folstein, and Paul R.McHugh.
1975.
Mini-mental state: A practicalmethod for grading the cognitive state of patientsfor the clinician.
Journal of Psychiatric Research,12(3):189?198, November.Gaugler, J. E., F. Yu, K. Krichbaum, and J.F.
Wyman.2009.
Predictors of nursing home admission for per-sons with dementia.
Medical Care, 47(2):191?198.Goldfarb, R. and M.J.S.
Pietro.
2004.
Support sys-tems: Older adults with neurogenic communicationdisorders.
Journal of Ambulatory Care Management,27(4):356?365.Hopper, T. 2001.
Indirect interventions to facilitate com-munication in Alzheimers disease.
Seminars in Speechand Language, 22(4):305?315.Kim, Hong Kook and Richard C. Rose.
2003.
Cepstrum-Domain Acoustic Feature Compensation Based on De-composition of Speech and Noise for ASR in NoisyEnvironments.
IEEE Transactions on Speech and Au-dio Processing, 11(5), September.Lamere, Paul, Philip Kwok, Evandro Gouvea, BhikshaRaj, Rita Singh, William Walker, M. Warmuth, andPeter Wolf.
2003.
The CMU Sphinx-4 speech recog-nition system.
In IEEE International Conference onAcoustics, Speech, and Signal Processing (ICASSP2003), Hong Kong, April.Martin, Rainer.
2001.
Noise power spectral density es-timation based on optimal smoothing and minimumstatistics.
IEEE Transactions of Speech and AudioProcessing, 9(5):504?512, July.Mihailidis, Alex, Jennifer N Boger, Tammy Craig, andJesse Hoey.
2008.
The COACH prompting system toassist older adults with dementia through handwash-ing: An efficacy study.
BMC Geriatrics, 8(28).Miller, George A.
1995.
WordNet: A Lexical Databasefor English.
Communications of the ACM, 38(11):39?41.Orange, J.B., Rosemary B. Lubinsky, and D. Jeffery Hig-ginbotham.
1996.
Conversational repair by individu-als with dementia of the alzheimer?s type.
Journal ofSpeech and Hearing Research, 39:881?895, August.Rochon, Elizabeth, Gloria S. Waters, and David Caplan.2000.
The Relationship Between Measures of Work-ing Memory and Sentence Comprehension in PatientsWith Alzheimer?s Disease.
Journal of Speech, Lan-guage, and Hearing Research, 43:395?413.Saini, Privender, Boris de Ruyter, Panos Markopoulos,and Albert van Breemen.
2005.
Benefits of social in-telligence in home dialogue systems.
In Proceedingsof INTERACT 2005, pages 510?521.Schegloff, Emanuel A., Gail Jefferson, and HarveySacks.
1977.
The preference for self-correctionin the organization of repair in conversation.
1977,53(2):361?382.Small, Jeff A., Elaine S. Andersen, and Daniel Kempler.1997.
Effects of working memory capacity on under-standing rate-altered speech.
Aging, Neuropsychology,and Cognition, 4(2):126?139.Small, Jeff A., Gloria Gutman, Saskia Makela, andBeth Hillhouse.
2003.
Effectiveness of communi-cation strategies used by caregivers of persons withalzheimer?s disease during activities of daily living.Journal of Speech, Language, and Hearing Research,46(2):353?367.Spanoudakis, Nikolaos, Boris Grabner, Christina Kot-siopoulou, Olga Lymperopoulou, Verena Moser-Siegmeth, Stylianos Pantelopoulos, Paraskevi Sakka,and Pavlos Moraitis.
2010.
A novel architecture andprocess for ambient assisted living - the hera approach.In Proceedings of the 10th IEEE International Confer-ence on Information Technology and Applications inBiomedicine (ITAB), pages 1?4.Tomoeda, Cheryl K., Kathryn A. Bayles, Daniel R.Boone, Alfred W. Kaszniak, and Thomas J. Slauson.1990.
Speech rate and syntactic complexity effectson the auditory comprehension of alzheimer patients.Journal of Communication Disorders, 23(2):151 ?161.Watson, Caroline M. 1999.
An analysis of trou-ble and repair in the natural conversations of peoplewith dementia of the Alzheimer?s type.
Aphasiology,13(3):195 ?
218.Wilson, Rozanne, Elizabeth Rochon, Alex Mihailidis,and Carol Le?onard.
2012.
Examining success of com-munication strategies used by formal caregivers assist-ing individuals with alzheimer?s disease during an ac-tivity of daily living.
Journal of Speech, Language,and Hearing Research, 55:328?341.Wilson, Rozanne, Elizabeth Rochon, Alex Mihailidis,and Carol Le?onard.
submitted.
Quantitative analy-sis of formal caregivers?
use of communication strate-gies while assisting individuals with moderate and se-vere alzheimer?s disease during oral care.
Journal ofSpeech, Language, and Hearing Research.55
