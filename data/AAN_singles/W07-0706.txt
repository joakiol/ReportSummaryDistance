Proceedings of the Second Workshop on Statistical Machine Translation, pages 40?47,Prague, June 2007. c?2007 Association for Computational LinguisticsA Dependency Treelet String CorrespondenceModel for Statistical Machine TranslationDeyi Xiong, Qun Liu and Shouxun LinKey Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesBeijing, China, 100080{dyxiong, liuqun, sxlin}@ict.ac.cnAbstractThis paper describes a novel model usingdependency structures on the source sidefor syntax-based statistical machine transla-tion: Dependency Treelet String Correspon-dence Model (DTSC).
The DTSC modelmaps source dependency structures to tar-get strings.
In this model translation pairs ofsource treelets and target strings with theirword alignments are learned automaticallyfrom the parsed and aligned corpus.
TheDTSC model allows source treelets and tar-get strings with variables so that the modelcan generalize to handle dependency struc-tures with the same head word but with dif-ferent modifiers and arguments.
Addition-ally, target strings can be also discontinuousby using gaps which are corresponding tothe uncovered nodes which are not includedin the source treelets.
A chart-style decod-ing algorithm with two basic operations?substituting and attaching?is designed forthe DTSC model.
We argue that the DTSCmodel proposed here is capable of lexical-ization, generalization, and handling discon-tinuous phrases which are very desirable formachine translation.
We finally evaluate ourcurrent implementation of a simplified ver-sion of DTSC for statistical machine trans-lation.1 IntroductionOver the last several years, various statistical syntax-based models were proposed to extend traditionalword/phrase based models in statistical machinetranslation (SMT) (Lin, 2004; Chiang, 2005; Dinget al, 2005; Quirk et al, 2005; Marcu et al, 2006;Liu et al, 2006).
It is believed that these modelscan improve the quality of SMT significantly.
Com-pared with phrase-based models, syntax-based mod-els lead to better reordering and higher flexibilityby introducing hierarchical structures and variableswhich make syntax-based models capable of hierar-chical reordering and generalization.
Due to theseadvantages, syntax-based approaches are becomingan active area of research in machine translation.In this paper, we propose a novel model based ondependency structures: Dependency Treelet StringCorrespondence Model (DTSC).
The DTSC modelmaps source dependency structures to target strings.It just needs a source language parser.
In contrast tothe work by Lin (2004) and by Quirk et al (2005),the DTSC model does not need to generate targetlanguage dependency structures using source struc-tures and word alignments.
On the source side, weextract treelets which are any connected subgraphsand consistent with word alignments.
While on thetarget side, we allow the aligned target sequencesto be generalized and discontinuous by introducingvariables and gaps.
The variables on the target sideare aligned to the corresponding variables of treelets,while gaps between words or variables are corre-sponding to the uncovered nodes which are not in-cluded by treelets.
To complete the translation pro-cess, we design two basic operations for the decod-ing: substituting and attaching.
Substituting is usedto replace variable nodes which have been alreadytranslated, while attaching is used to attach uncov-40ered nodes to treelets.In the remainder of the paper, we first define de-pendency treelet string correspondence in section2 and describe an algorithm for extracting DTSCsfrom the parsed and word-aligned corpus in section3.
Then we build our model based on DTSC in sec-tion 4.
The decoding algorithm and related pruningstrategies are introduced in section 5.
We also spec-ify the strategy to integrate phrases into our modelin section 6.
In section 7 we evaluate our currentimplementation of a simplified version of DTSC forstatistical machine translation.
And finally, we dis-cuss related work and conclude.2 Dependency Treelet StringCorrespondenceA dependency treelet string correspondence pi is atriple < D,S,A > which describes a translationpair < D,S > and their alignment A, where D isthe dependency treelet on the source side and S isthe translation string on the target side.
< D,S >must be consistent with the word alignment M ofthe corresponding sentence pair?
(i, j) ?
M, i ?
D ?
j ?
SA treelet is defined to be any connected subgraph,which is similar to the definition in (Quirk et al,2005).
Treelet is more representatively flexible thansubtree which is widely used in models based onphrase structures (Marcu et al, 2006; Liu et al,2006).
The most important distinction between thetreelet in (Quirk et al, 2005) and ours is that we al-low variables at positions of subnodes.
In our defini-tion, the root node must be lexicalized but the subn-odes can be replaced with a wild card.
The targetcounterpart of a wildcard node in S is also replacedwith a wild card.
The wildcards introduced in thisway generalize DTSC to match dependency struc-tures with the same head word but with differentmodifiers or arguments.Another unique feature of our DTSC is that we al-low target strings with gaps between words or wild-cards.
Since source treelets may not cover all subn-odes, the uncovered subnodes will generate a gap asits counterpart on the target side.
A sequence of con-tinuous gaps will be merged to be one gap and gapsat the beginning and the end of S will be removedautomatically.??eeeeeee????
?
?eeeeeees ss ss ss s?????
?eeeeeee????
]]]]]]]]]]the conference cooperation of the ??
?eeeeeeebbbbbbbbbbbbbs ss ss ss s?1w ww w?
YYYYYYYSSSSSSS ?
?2 ]]]]]]]]]]?1 keep a G with the ?2Figure 1: DTSC examples.
Note that ?
representsvariable and G represents gap.Gap can be considered as a special kind of vari-able whose counterpart on the source side is notpresent.
This makes the model more flexible tomatch more partial dependency structures on thesource side.
If only variables can be used, the modelhas to match subtrees rather than treelets on thesource side.
Furthermore, the positions of variableson the target side are fixed so that some reorderingsrelated with them can be recorded in DTSC.
The po-sitions of gaps on the target side, however, are notfixed until decoding.
The presence of one gap andits position can not be finalized until attaching op-eration is performed.
The introduction of gaps andthe related attaching operation in decoding is themost important distinction between our model andthe previous syntax-based models.Figure 1 shows several different DTSCs automat-ically extracted from our training corpus.
The topleft DTSC is totally lexicalized, while the top rightDTSC has one variable and the bottom has two vari-ables and one gap.
In the bottom DTSC, note thatthe node ?
which is aligned to the gap G of thetarget string is an uncovered node and therefore notincluded in the treelet actually.
Here we just wantto show there is an uncovered node aligned with thegap G.Each node at the source treelet has three attributes1.
The head word2.
The category, i.e.
the part of speech of the headword3.
The node order which specifies the local orderof the current node relative to its parent node.41?
?/VVeeeeeeebbbbbbbbbbbbbb ]]]]]]]]]]]]]]]]]]]]]???????/VV??
?DDDDD ?/P YYYYYYYXXXXXXXXXXXXXX?
?/NNeeeeeeez zz zz???
?/NR\\\\\\\\\\\\\\ ?
?/NNj j jj?
?go on providingfinancial aid to Palestine1 2 3 4 5 6 7Figure 2: An example dependency tree and its align-mentsNote that the node order is defined at the context ofthe extracted treelets but not the context of the orig-inal tree.
For example, the attributes for the node?in the bottom DTSC of Figure 1 are {?, P, -1}.
Fortwo treelets, if and only if their structures are iden-tical and each corresponding nodes share the sameattributes, we say they are matched.3 Extracting DTSCsTo extract DTSCs from the training corpus, firstlythe corpus must be parsed on the source side andaligned at the word level.
The source structures pro-duced by the parser are unlabelled, ordered depen-dency trees with each word annotated with a part-of-speech.
Figure 2 shows an example of dependencytree really used in our extractor.When the source language dependency trees andword alignments between source and target lan-guages are obtained, the DTSC extraction algorithmruns in two phases along the dependency trees andalignments.
In the first step, the extractor annotateseach node with specific attributes defined in section3.1.
These attributes are used in the second stepwhich extracts all possible DTSCs rooted at eachnode recursively.3.1 Node annotationFor each source dependency node n, we define threeattributes: word span, node span and crossed.Word span is defined to be the target word sequencealigned with the head word of n, while node span isdefined to be the closure of the union of node spansof all subnodes of n and its word span.
These two at-tributes are similar to those introduced by Lin (Lin,2004).
The third attribute crossed is an indicator thathas binary values.
If the node span of n overlapsthe word span of its parent node or the node spanof its siblings, the crossed indicator of n is 1 andn is therefore a crossed node, otherwise the crossedindicator is 0 and n is a non-crossed node.
Onlynon-crossed nodes can generate DTSCs because thetarget word sequence aligned with the whole subtreerooted at it does not overlap any other sequences andtherefore can be extracted independently.For the dependency tree and its alignments shownin Figure 2, only the node ??
is a crossed nodesince its node span ([4,5]) overlaps the word span([5,5]) of its parent node?
?.3.2 DTSCs extractionThe DTSC extraction algorithm (shown in Figure 3)runs recursively.
For each non-crossed node, the al-gorithm generates all possible DTSCs rooted at it bycombining DTSCs from some subsets of its directsubnodes.
If one subnode n selected in the com-bination is a crossed node, all other nodes whoseword/node spans overlap the node span of n must bealso selected in this combination.
This kind of com-bination is defined to be consistent with the wordalignment because the DTSC generated by this com-bination is consistent with the word alignment.
AllDTSCs generated in this way will be returned to thelast call and outputted.
For each crossed node, thealgorithm generates pseudo DTSCs1 using DTSCsfrom all of its subnodes.
These pseudo DTSCs willbe returned to the last call but not outputted.During the combination of DTSCs from subnodesinto larger DTSCs, there are two major tasks.
Onetask is to generate the treelet using treelets fromsubnodes and the current node.
This is a basic treegeneration operation.
It is worth mentioning thatsome non-crossed nodes are to be replaced with awild card so the algorithm can learn generalizedDTSCs described in section 2.
Currently, we re-place any non-crossed node alone or together withtheir sibling non-crossed nodes.
The second taskis to combine target strings.
The word sequencesaligned with uncovered nodes will be replaced witha gap.
The word sequences aligned with wildcardnodes will be replaced with a wild card.If a non-crossed node n has m direct subnodes,all 2m combinations will be considered.
This willgenerate a very large number of DTSCs, which is1Some words in the target string are aligned with nodeswhich are not included in the source treelet.42DTSCExtractor(Dnode n)< := ?
(DTSC container of n)for each subnode k of n doR := DTSCExtractor(k)L := L?Rend forif n.crossed!
= 1 and there are no subnodes whose spanoverlaps the word span of n thenCreate a DTSC pi =< D,S,A > where the dependencytreelet D only contains the node n (not including any chil-dren of it)output pifor each combination c of n?s subnodes doif c is consistent with the word alignment thenGenerate all DTSCs R by combining DTSCs (L)from the selected subnodes with the current node n< := <?Rend ifend foroutput <return <else if n.crossed == 1 thenCreate pseudo DTSCs P by combining all DTSCs fromn?s all subnodes.< := <?Preturn <end ifFigure 3: DTSC Extraction Algorithm.undesirable for training and decoding.
Therefore wefilter DTSCs according to the following restrictions1.
If the number of direct subnodes of node n islarger than 6, we only consider combining onesingle subnode with n each time because in thiscase reorderings of subnodes are always mono-tone.2.
On the source side, the number of direct subn-odes of each node is limited to be no greaterthan ary-limit; the height of treelet D is limitedto be no greater than depth-limit.3.
On the target side, the length of S (includinggaps and variables) is limited to be no greaterthan len-limit; the number of gaps in S is lim-ited to be no greater than gap-limit.4.
During DTSC combination, the DTSCs fromeach subnode are sorted by size (in descendingorder).
Only the top comb-limit DTSCs will beselected to generate larger DTSCs.As an example, for the dependency tree and itsalignments in Figure 2, all DTSCs extracted by theTreelet String(?
?/VV/0) go on(???
?/NR/0) Palestine(?/P/0) to(?/P/0 (???
?/NR/1)) to Palestine(?/P/0 (?/1)) to ?(?
?/NN/0 (?
?/NN/-1)) financial aid(?
?/VV/0) providing(?
?/VV/0 (?/1)) providing ?(?
?/VV/0 (?/-1)) providing G ?(?
?/VV/0 (?
?/VV/-1)) go on providing(?
?/VV/0 (?/-1)) ?
providing(?
?/VV/0 (?1/-1) (?2/1)) providing ?2 ?1(?
?/VV/0 (?1/-1 ) (?2/1)) ?1 providing ?2Table 1: Examples of DTSCs extracted from Figure2.
Alignments are not shown here because they areself-evident.algorithm with parameters { ary-limit = 2, depth-limit = 2, len-limit = 3, gap-limit = 1, comb-limit= 20 } are shown in the table 1.4 The ModelGiven an input dependency tree, the decoder gen-erates translations for each dependency node inbottom-up order.
For each node, our algorithm willsearch all matched DTSCs automatically learnedfrom the training corpus by the way mentioned insection 3.
When the root node is traversed, the trans-lating is finished.
This complicated procedure in-volves a large number of sequences of applicationsof DTSC rules.
Each sequence of applications ofDTSC rules can derive a translation.We define a derivation ?
as a sequence of appli-cations of DTSC rules, and let c(?)
and e(?)
be thesource dependency tree and the target yield of ?, re-spectively.
The score of ?
is defined to be the prod-uct of the score of the DTSC rules used in the trans-lation, and timed by other feature functions:?(?)
=?i?
(i) ?
plm(e)?lm ?
exp(??apA(?))
(1)where ?
(i) is the score of the ith application ofDTSC rules, plm(e) is the language model score,and exp(??apA(?))
is the attachment penalty,where A(?)
calculates the total number of attach-ments occurring in the derivation ?.
The attach-ment penalty gives some control over the selectionof DTSC rules which makes the model prefer rules43with more nodes covered and therefore less attach-ing operations involved.For the score of DTSC rule pi, we define it as fol-lows:?
(pi) =?jfj(pi)?j (2)where the fj are feature functions defined on DTSCrules.
Currently, we used features proved to be ef-fective in phrase-based SMT, which are:1.
The translation probability p(D|S).2.
The inverse translation probability p(S|D).3.
The lexical translation probability plex(D|S)which is computed over the words that occuron the source and target sides of a DTSC ruleby the IBM model 1.4.
The inverse lexical translation probabilityplex(S|D) which is computed over the wordsthat occur on the source and target sides of aDTSC rule by the IBM model 1.5.
The word penalty wp.6.
The DTSC penalty dp which allows the modelto favor longer or shorter derivations.It is worth mentioning how to integrate the N-gram language mode into our DTSC model.
Duringdecoding, we have to encounter many partial transla-tions with gaps and variables.
For these translations,firstly we only calculate the language model scoresfor word sequences in the translations.
Later we up-date the scores when gaps are removed or specifiedby attachments or variables are substituted.
Each up-dating involves merging two neighbor substrings sl(left) and sr (right) into one bigger string s. Let thesequence of n ?
1 (n is the order of N-gram lan-guage model used) rightmost words of sl be srl andthe sequence of n?1 leftmost words of sr be slr.
wehave:LM(s) = LM(sl) + LM(sr) + LM(srl slr)?LM(srl )?
LM(slr) (3)where LM is the logarithm of the language modelprobability.
We only need to compute the incrementof the language model score:4LM = LM(srl slr)?
LM(srl )?
LM(slr) (4)for each node n of the input tree T , in bottom-up order doGet al matched DTSCs rooted at nfor each matched DTSC pi dofor each wildcard node n?
in pi doSubstitute the corresponding wildcard on the targetside with translations from the stack of n?end forfor each uncovered node n@ by pi doAttach the translations from the stack of n@ to thetarget side at the attaching pointend forend forend forFigure 4: Chart-style Decoding Algorithm for theDTSC Model.Melamed (2004) also used a similar way to integratethe language model.5 DecodingOur decoding algorithm is similar to the bottom-upchart parsing.
The distinction is that the input is atree rather than a string and therefore the chart is in-dexed by nodes of the tree rather than spans of thestring.
Also, several other tree-based decoding al-gorithms introduced by Eisner (2003), Quirk et al(2005) and Liu et al (2006) can be classified as thechart-style parsing algorithm too.Our decoding algorithm is shown in Figure 4.Given an input dependency tree, firstly we generatethe bottom-up order by postorder transversal.
Thisorder guarantees that any subnodes of node n havebeen translated before node n is done.
For eachnode n in the bottom-up order, all matched DTSCsrooted at n are found, and a stack is also built for it tostore the candidate translations.
A DTSC pi is said tomatch the input dependency subtree T rooted at n ifand only if there is a treelet rooted at n that matches2 the treelet of pi on the source side.For each matched DTSC pi, two operations willbe performed on it.
The first one is substitutingwhich replaces a wildcard node with the correspond-ing translated node.
The second one is attachingwhich attaches an uncovered node to pi.
The two op-erations are shown in Figure 5.
For each wildcardnode n?, translations from the stack of it will be se-lected to replace the corresponding wildcard on the2The words, categories and orders of each correspondingnodes are matched.
Please refer to the definition of matchedin section 2.44(a) Aeeeeeee YYYYYYYBeeeeeee?
+ DC ?
De?e Ae Be CeSubstitute ?
(b) Aeeeeeee YYYYYYYBeeeeeeeD + EC ?
EeDe Ae Be CeAttach ?
(c) Aeeeeeee YYYYYYYBeeeeeee YYYYYYYDC EDe Ae Be Ee CeFigure 5: Substituting and attaching operations fordecoding.
Xe is the translation of X .
Node that ?
isa wildcard node to be substituted and node ?
is anuncovered node to be attached.target side and the scores of new translations will becalculated according to our model.
For each uncov-ered node n@, firstly we determine where transla-tions from the stack of n@ should be attached on thetarget side.
There are several different mechanismsfor choosing attaching points.
Currently, we imple-ment a heuristic way: on the source side, we find thenode n@p which is the nearest neighbor of n@ fromits parent and sibling nodes, then the attaching pointis the left/right of the counterpart of n@p on the targetside according to their relative order.
As an example,see the uncovered node ?
in Figure 5.
The nearestnode to it is node B.
Since node ?
is at the rightof node B, the attaching point is the right of Be.One can search all possible points using an orderingmodel.
And this ordering model can also use infor-mation from gaps on the target side.
We believe thisordering model can improve the performance and letit be one of directions for our future research.Note that the gaps on the target side are not neces-sarily attaching points in our current attaching mech-anism.
If they are not attaching point, they will beremoved automatically.The search space of the decoding algorithm isvery large, therefore some pruning techniques haveto be used.
To speed up the decoder, the followingpruning strategies are adopted.1.
Stack pruning.
We use three pruning ways.The first one is recombination which convertsthe search to dynamic programming.
Whentwo translations in the same stack have thesame w leftmost/rightmost words, where w de-pends on the order of the language model, theywill be recombined by discarding the transla-tion with lower score.
The second one is thethreshold pruning which discards translationsthat have a score worse than stack-thresholdtimes the best score in the same stack.
Thelast one is the histogram pruning which onlykeeps the top stack-limit best translations foreach stack.2.
Node pruning.
For each node, we only keepthe top node-limit matched DTSCs rooted atthat node, as ranked by the size of sourcetreelets.3.
Operation pruning.
For each operation, sub-stituting and attaching, the decoding will gen-erate a large number of partial translations3for the current node.
We only keep the topoperation-limit partial translations each timeaccording to their scores.6 Integrating PhrasesAlthough syntax-based models are good at dealingwith hierarchical reordering, but at the local level,translating idioms and similar complicated expres-sions can be a problem.
However, phrase-basedmodels are good at dealing with these translations.Therefore, integrating phrases into the syntax-basedmodels can improve the performance (Marcu et al,2006; Liu et al, 2006).
Since our DTSC model isbased on dependency structures and lexicalized nat-urally, DTSCs are more similar to phrases than othertranslation units based on phrase structures.
Thismeans that phrases will be easier to be integratedinto our model.The way to integrate phrases is quite straightfor-ward: if there is a treelet rooted at the current node,3There are wildcard nodes or uncovered nodes to be han-dled.45of which the word sequence is continuous and iden-tical to the source of some phrase, then a phrase-style DTSC will be generated which uses the targetstring of the phrase as its own target.
The procedureis finished during decoding.
In our experiments, in-tegrating phrases improves the performance greatly.7 Current ImplementationTo test our idea, we implemented the dependencytreelet string correspondence model in a Chinese-English machine translation system.
The current im-plementation in this system is actually a simplifiedversion of the DTSC model introduced above.
Inthis version, we used a simple heuristic way for theoperation of attaching rather than a sophisticated sta-tistical model which can learn ordering informationfrom the training corpus.
Since dependency struc-tures are more?flattened?
compared with phrasalstructures, there are many subnodes which will notbe covered even by generalized matched DTSCs.This means the attaching operation is very commonduring decoding.
Therefore better attaching modelwhich calculates the best point for attaching , we be-lieve, will improve the performance greatly and is amajor goal for our future research.To obtain the dependency structures of the sourceside, one can parse the source sentences with a de-pendency parser or parse them with a phrasal struc-ture parser and then convert the phrasal structuresinto dependency structures.
In our experiments weused a Chinese parser implemented by Xiong etal.
(2005) which generates phrasal structures.
Theparser was trained on articles 1-270 of Penn ChineseTreebank version 1.0 and achieved 79.4% (F1 mea-sure).
We then converted the phrasal structure treesinto dependency trees using the way introduced byXia (1999).To obtain the word alignments, we use the wayof Koehn et al (2005).
After running GIZA++(Och and Ney, 2000) in both directions, we applythe ?grow-diag-final?
refinement rule on the in-tersection alignments for each sentence pair.The training corpus consists of 31, 149 sentencepairs with 823K Chinese words and 927K Englishwords.
For the language model, we used SRI Lan-guage Modeling Toolkit (Stolcke, 2002) to train atrigram model with modified Kneser-Ney smooth-Systems BLEU-4PB 20.88 ?
0.87DTSC 20.20 ?
0.81DTSC + phrases 21.46 ?
0.83Table 2: BLEU-4 scores for our system and aphrase-based system.ing on the 31, 149 English sentences.
We selected580 short sentences of length at most 50 charactersfrom the 2002 NIST MT Evaluation test set as ourdevelopment corpus and used it to tune ?s by max-imizing the BLEU score (Och, 2003), and used the2005 NIST MT Evaluation test set as our test corpus.From the training corpus, we learned 2, 729,964 distinct DTSCs with the configuration { ary-limit = 4, depth-limit = 4, len-limit = 15, gap-limit= 2, comb-limit = 20 }.
Among them, 160,694DTSCs are used for the test set.
To run our de-coder on the development and test set, we set stack-thrshold = 0.0001, stack-limit = 100, node-limit =100, operation-limit = 20.We also ran a phrase-based system (PB) with adistortion reordering model (Xiong et al, 2006) onthe same corpus.
The results are shown in table 2.For all BLEU scores, we also show the 95% confi-dence intervals computed using Zhang?s significanttester (Zhang et al, 2004) which was modified toconform to NIST?s definition of the BLEU brevitypenalty.
The BLEU score of our current system withthe DTSC model is lower than that of the phrase-based system.
However, with phrases integrated, theperformance is improved greatly, and the new BLEUscore is higher than that of the phrase-based SMT.This difference is significant according to Zhang?stester.
This result can be improved further using abetter parser (Quirk et al, 2006) or using a statisti-cal attaching model.8 Related WorkThe DTSC model is different from previous workbased on dependency grammars by Eisner (2003),Lin (2004), Quirk et al (2005), Ding et al (2005)since they all deduce dependency structures on thetarget side.
Among them, the most similar work is(Quirk et al, 2005).
But there are still several majordifferences beyond the one mentioned above.
Our46treelets allow variables at any non-crossed nodes andtarget strings allow gaps, which are not available in(Quirk et al, 2005).
Our language model is calcu-lated during decoding while Quirk?s language modelis computed after decoding because of the complex-ity of their decoding.The DTSC model is also quite distinct from pre-vious tree-string models by Marcu et al (2006)and Liu et al (2006).
Firstly, their models arebased on phrase structure grammars.
Secondly, sub-trees instead of treelets are extracted in their mod-els.
Thirdly, it seems to be more difficult to integratephrases into their models.
And finally, our model al-low gaps on the target side, which is an advantageshared by (Melamed, 2004) and (Simard, 2005).9 Conclusions and Future WorkWe presented a novel syntax-based model usingdependency trees on the source side?dependencytreelet string correspondence model?for statisticalmachine translation.
We described an algorithm tolearn DTSCs automatically from the training corpusand a chart-style algorithm for decoding.Currently, we implemented a simple version ofthe DTSC model.
We believe that our performancecan be improved greatly using a more sophisticatedmechanism for determining attaching points.
There-fore the most important future work should be to de-sign a better attaching model.
Furthermore, we planto use larger corpora for training and n-best depen-dency trees for decoding, which both are helpful forthe improvement of translation quality.AcknowledgementsThis work was supported by National Natural Sci-ence Foundation of China, Contract No.
60603095and 60573188.ReferencesDavid Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proceedings of ACL.Yuan Ding and Martha Palmer.
2005.
Machine Translation Us-ing Probabilistic Synchronous Dependency Insertion Gram-mars.
In Proceedings of ACL.Jason Eisner.
2003.
Learning non-isomorphic tree mappingsfor machine translation.
In Proceedings of ACL.Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne, ChrisCallison-Burch, Miles Osborne and David Talbot.
2005.Edinburgh System Description for the 2005 IWSLT SpeechTranslation Evaluation.
In International Workshop on Spo-ken Language Translation.Daniel Marcu, Wei Wang, Abdessamad Echihabi, and KevinKnight.
2006.
SPMT: Statistical Machine Translation withSyntactified Target Language Phraases.
In Proceedings ofEMNLP.I.
Dan Melamed.
2004.
Algorithms for Syntax-Aware Statisti-cal Machine Translation.
In Proceedings of the Conferenceon Theoretical and Methodological Issues in Machine Trans-lation (TMI), Baltimore, MD.Dekang Lin.
2004.
A path-based transfer model for machinetranslation.
In Proceedings of COLING.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-StringAlignment Template for Statistical Machine Translation.
InProceedings of ACL.Franz Josef Och.
2003.
Minimum error rate training in statisti-cal machine translation.
In Proceedings of ACL.Franz Josef Och and Hermann Ney.
2000.
Improved statisticalalignment models.
In Proceedings of ACL.Chris Quirk, Arul Menezes and Colin Cherry.
2005.
Depen-dency Treelet Translation: Syntactically Informed PhrasalSMT.
In Proceedings of ACL.Chris Quirk and Simon Corston-Oliver.
2006.
The impact ofparse quality on syntactically-informed statistical machinetranslation.
In Proceedings of EMNLP, Sydney, Australia.Michel Simard, Nicola Cancedda, Bruno Cavestro, MarcDymetman, Eric Gaussier, Cyril Goutte, Kenji Yamada.2005.
Translating with non-contiguous phrases.
In Proceed-ings of HLT-EMNLP.Andreas Stolcke.
2002.
SRILM - an extensible language mod-eling toolkit.
In Proceedings of International Conference onSpoken Language Processing, volume 2, pages 901-904.Fei Xia.
1999.
Automatic Grammar Generation from Two Dif-ferent Perspectives.
PhD thesis, University of Pennsylvania.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
MaximumEntropy Based Phrase Reordering Model for Statistical Ma-chine Translation.
In Proceedings of COLING-ACL, Sydney,Australia.Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, YueliangQian.
2005.
Parsing the Penn Chinese Treebank with Se-mantic Knowledge.
In Proceedings of IJCNLP, Jeju Island,Korea.Ying Zhang, Stephan Vogel, and Alex Waibel.
2004.
Inter-preting BLEU/NIST scores: How much improvement do weneed to have a better system?
In Proceedings of LREC,pages 2051?
2054.47
