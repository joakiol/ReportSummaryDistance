CONTENT CHARACTERIZATION USING WORD SHAPE TOKENSPenelope Sibun and David S. FarrarFuji Xerox Palo Alto Laboratory, 3400 Hillview Avenue, Palo Alto, CA 94304s ibun@pal .xerox.com, farrar@pal.xerox.comAbstractBy quickly classifying character images into charactershape categories, il is possible to automatically extractsyntactic information from the text of document imageswithout optical character recognition.
Using word shapetokens composed of these charactershapecodes, a properlymr|ned text tagger can extract part-of.speech informationfronl scanned ocument images.
Later components of adocument processing system can then use this informationto locate topics, characterize document style, and assist illinlormation rctriewll.extract noun phrases and other content characteristicsusing only word shape tokens that have been tagged withtheir parts of speech.
Using this approach, we can processdocument images quickly to determine whether OCP, iswarranted, tbrexample, when a text is a likely match forkeywords in a database query.In the next two sections, we describe how word shapetokens are derived; in section four, we discuss part-of-speech tagging; in the following fonr sections, wcdescribe in detail parl-of-speech tagging nsing word shapetokens; in sections nine and ten we discuss our results.1 INTRODUCTIONThere are nlany text processing tasks that we wouldlike to accomplish, such as document classification, textdatabase structuring, matching documents with queries,and topic characterization.
The field of computatiomdlinguistics has developed a variety of techniques foraccomplishing these tasks for text &vuments representedby character codes (e.g., ASCII).
llowever, manydocuments for which we would like to use otn automatedtechniques arc not stored online in character-coded \[ornla\[,but instead exist only on paper.
Optical characterrecognition (OCR) is a technique for converting scanneddocument images into character codes.
By using ()CR,document images can \[y,2 converted into a form amenableto existing text processing techniques, t towcvcr, OCR isexpensive, slow, and o\[\[cn illaccnrate.
Because of thesedrawbacks, we would like to avoid OCR it we can, c.r atthe least, postpone using OCR until we are confident thata document wammts detailed processing.
In other words,we would like a high-bandwidth document processingsystem that is sensitive nough to detect desired ocumentIcatnrcs.Our document understanding goals at the Fuji XeroxPale Alto Laboratory include latlgaage determination(Nakayama and Spitz, 1993; Sibun and Spitz,forthcoming), (:otllettl ('hara(Terizalion, and stylecharucteri=alion.
Toward these goals, we are developing itset of methods for extracting inlk)rmation from documentimages which do not depend on OCR.
We have beenworking toward our goal of inexpensive contentcharacterization by adapting a part of-.v)eech tagger toprocess word shape tokens rather than character codedwords.
Part-el-speech tagging is a technique that has beendeveloped and refined over the past several years, and itprovides an inexpensive, last, and reliable source ofinlormation for recognizing noun phlases and othersyntax-related text features which help characterize adoeunlen\[rs content.In this paper, we describe how we combine ourtechnology for determining word shape tokens with text-tagging technology.
We are developing systems that can2 WORD SHAPE TOKEN CREATIONIn this section we briefly describe our system thatconstructs character shape codes and word shape tokensfrom a document linage (for more detail, see Nakayamaand Spitz, 1993; Sibun and Spitz, forthcoming).
Torecognize character shape codex from an image, SOnletransfornlatitnls alc first nlade \[o correct for variousscanning artifacts such as skew angle and text linecnrvature.
On each text line, four horizontal lines definethree significant zones: the area between the baseline andthe top of characters such as "x" is the x cone; the areaabove the x-height level is the ascender,~one; the areabelow the x-zone is the descender zone (figure 1).
Timtext line is furthcr divided into charactercells by verticalbonnda,ics which delineate the connected components ofeach character image.~ top  x -he ightbase l inebot tomFigure I: The text line parameter positions.The majority of characters can easily be mapped to asmall numher of distinct ccMes (\[igure 2).
1 Cllaracterswhich are contained entirely in the x-zone map to shapecode x ; characters which extend \[rom the baseline to alxwethe x-height line map to shape code A: and those whichextend from below the baseline to the xqmight line mapto shape code g. Characters which map to A, x, or g arecomposed o1 a single connected component.
Somecharacters con|ain Fnorc than one connected component:an x-height character with a single diacritical mark in theascender zone maps to i ; a character with a descender and asingle diacritical mark maps to j.
Most commonpunctuation marks map to unique shape codes; however,I If this nmppmg can bc done from docmncnt images, it canmore trivially bc aCCOlnplished frmn character codeddocmncnts, sllch as .,\St '.\[I text (providing, of course, that lhcmethod of encoding is known).686some are mapped into shape codes shared with alphabeticcharacters (e.g., "&" maps to shape code A).Shape OxtcA.
.
.
.
.
.
CtmtacterA -Zbdfh  kl t0 -9# $ &/(-:'1X ace  Irl I1O rs  U V tAX 7,i I a it  a e e e 1 I I o o o u tr t l  n ig g PqY~:J Jl:igurc 2: Character shape codes.3 St tAPE  CONVERSIONIn general, our approach to docmnent processingfinesses the problems iltllerent in mapping from an imagcto a character coded representation: we nlap instead frolltthe imagc to a shal)e basedr~Tn'esentalion.
This techniquecan transform evell a degraded ocument illage itlto arepresentation which provides useful abstractions about thetext of a document.
The shape-based representation thatwe construct is proving to be a relnarkably rich source o1information.
While our initial goal has beell to, use it lorlanguage identification in support of downstreanl OCRpr(x;esses, we are finding that this representation lnay be asufficient source of information for document contentcharacterization, such as that supported by part-of-speechlagging.In our tagging work, we have used character shapec~?tedtext derived froth normal character-c{~,led t xt.
Thisis simply because we dc, tlOt have access to enough inlagedocuments on which to train a taggef.
We call the processof creating a shape-Ntsed version ol  the dtxxttnent lroln thecharacter eerie based version shape conver.viotLFor the purlx~se of text tagging, then, we cltn think oIthe word shatx: token representation as an approximationof the representation composed of words.
We can thinkabout the relationship between words and word shapetokens its a mapping from a word to its correspondingword shape token.
For example, the word "apple" maps totile word shape token xggAx,  and tile word "apples" mapsto the word shape token x g g A x x.hi d(?
;uments, words exist its sur/ace.fi~rms, not itsmorphological systems; thus "apple" and "apples" aredifferent words.
Therefore, it is of no use to us to have alexicon organized in terms of stems and suffixes; i+rstcad,our lexicon is conlposed of stlrfaee forms like "apple" and"apples".
Throughout the rest of this paper, when we say"words", we rllean words as Sillface ftwll\]S.4 I 'ART-OF-S I+EECl i  TAGGINGA part of speech tagger is at system that uses contextto assign parts of speech to words.
Part-of-speechinformation facilitates higher-level analysis, such asrecognizing nOUll phrases and other patterns ill text?Several different approaches have been used for buildingtext taggers.
A particular fornl of Markov model has beenwidely used that assumes thai a word dcpendsprobabilistically on just its part-of-slx~eeh category, whichm turn depends olely on the categories of the plecedmgtwo words.
Training the trlodel is sonletinles doue bymeans of a large lagged corpus, but this is not necessary.The I~autn-Welch algorithm (Baum, 1972), also knowtt itsthe t;orward-l~,ackward lgorithm, carl be used.
In thisease, the model is called a hidden Markov nlodel (I IMM),since state transiticms (i.e., part-.of-speech ategories) areassunled to be unobseuvable.l:or this work, we use an 11MM-based text tagger thatis publicly available from Xerox PAP, C. As described inCutting el al.
(1902), the PAR(2 tagger is efficient andhighly flexible.
It is particularly important that the taggercan be trained on any eorptls el text, using ally lexicon.This flexibility allows us to shape-convert our trainingcorpus and lexicon, its described in section 5, withoutneeding to modify the tagger itself.
Below we outline tilebasic operation of tire PARC tagger; please refer toCutting el al.
(1902) for further detail.1.
Text destined for tire tagger first encotlllters atokenizer, whose duty is to eoltVel+t ext (a sequence lcharacters) into a sequence of tokens.
Each sentenceboundary is also identified by the tokenizer, and is passedits a special token.2.
The tokenizer passes tokens t?+ the lexicon, wheretokens are matched with a set of surface fofms, eachannotated with a Imrt-of-speech tag.
The set el  tagsconstitutes an ambiguily class.
The lexicon passes along astream of (.~'llrfilce.fi)rnt, ambigltily class) pairs.3a.
In training mode, the tagger takes long sequencesof ambiguity classes as input.
It uses the Baum-Welchalgorithm to produce a trained I IMM, which is used itsinput in tagging Inode.
Training is performed on somecorpus of interest; this corptlS lnay be of broad coverage ormay be genre specific.3b.
Ill lagging mode, tile tagger buflers sequences elambiguity classes between sentence boundmies.
'\['hescsequences are disambiguated by computing tile lnaximalpath through the I IMM with the Viterbi algorithm (lO67).Operating at sentence granuhuity does llot sacrifieeaccuracy, since sentence boundaries are unambiguous.Output consists of pairs of surface forms and tags.5 THE LEX ICONThe word shape tagging in our work fol lows tilet IMM4)ased process described above.
Both word shapetagging atld standard word tagging require a lexicon.5.1 Const ruct ing  tile LexiconA word shape lexicon can be derived from a standardlexicon of words.
The lexicon used with the standard texttagger contains a list of all the distinct surface formslikely to be encountered m the hmguage.
Associated witheach surface form is a list of the possible parts of sIx'echthat the ~ttrface form can hitve.
\];or exalllple:ijp~le noun~LP~ i)hual nouneat verbeats third person singular verbt~l noun, adjectivef.lle determiner( ) l iCe  We have a lexicon which consists of sttrface fonns,we can use it to create a lexicOlt of word shape tokens for687word shape tagging.
In particular, this transR)rmatl ,nconsists of the following steps:1.
Shape convert the surface forms to th, ircorresponding word shape tokens.2.
Sort the lexicon by surface form word shape.
Atthis stage there may be duplicate word shape tokens.3.
Eliminate duplicate ntries in the lexicon: collectall parts of speech behind one word shape token (combinetheir ambiguity classes).
At this stage each word shapetoken should be unique.4.
Eliminate duplicate parts of speech behind eachword shape token.
At this stage each part of speechshould be unique within each mnbiguity class.The lexicon fragment above would be converted to:x g g A x nounxggAxx plural normx x A verb, noun, adjectivex x A x third person singular verbA A x detelminer5.2 Analysis of the LexiconFor this work, we use a lexicon provided by XeroxPARC.
This lexicon is organized so that there is an entryfor each of roughly 150,000 surface forms, l:or wordshape tagging, we shape converted this lexicon.
As can beseen in the table, shape conversion results ill about 50,000distinct word shape surlace forms.
This suggests that, onaverage, each word shape token is a mapping of threesurlacc forms.
However, about 30,000 of the word shapetokens arc unique, that is, correspond to a single surfaceform.Surface Forms Count %TotalStandard Lexicon 148,703 I 0()+0Sh~.tpc-eonverted Lexicon 47,1()2 31.7Shape-converted Unique.
28,949 19.5Thus, the word shape lexicon is approximately one-third the size of the standard lexicon.
Clearly, informationhas been lost, but not as much as one might think.
Infact, the 20% of the word shape tokens that are uniquecarry exactly as much reformation as their correspondingcharacter-coded words.
While some surface forms that mapto unique word shape tokens are long and infrequent (like"flibbertigibbet", AA iAAxxA ig iAAxA) ,  many areshort, Ct/lnlylOn words:xggAxxggAxxAAigAthirst X AAixxAglifelike A iAxAiAxgxAxxggxgAxggxgAxgxWhile word shape tokens that are unique have the salneparts of speech as their corresponding surface forms, theothers will tend on average to have many more parts <)lspeech than an average stnTace form.
This defxzndssomewhat on the tagset (see section 6).
In general, wordshape tokens frequently have as many as 10 to 15 parts ofspeech, whereas tandard surlace forms rarely have morethan 4 or 5.6 DEVISING THE TAGSETThe lagset is implicit in the lexicon: it includes allparts of speech listed in any entry of the lexicon; it alsoincludes a small set of tags for punctuation, such ascomma, hyphen, and sentence boundary.
Although thetagset is not explicitly defined, we can modify it bymapping from selected tags fonnd in the lexicon to othertags of our choosing.
For example, the lexicondistinguishes between verb tenses and has separate tags fordifferent combinations of verb tense, person, and number:presenl tense verb, paxl lense verb, third pets'on singularpresent verb, etc.
If we preferred, we could map all thesedifferent verb forms to a single verb tag.
However, wetypically prefer to maintain such distinctions, as the texttaggcr can take advantage of differences in the surfaceforms of verbs with different enses in order to uniquelyidentify their parts of speech.Shape com,ersion collapses different surface lorms ontoone word shape and merges their ambiguity classes.
Theresult is that them tend to be tcwer distract surface forms,and that each surface form has, on average, a largerambiguity class.
If this ambiguity is problematic, oneway to reduce it may be to reduce the size of the tagset.t:or example, we may choose to have one undifferentiatedverb tag rather than a set which differentiates tense,person, and namber.
With fewer possible parts of speechto choose from, the HMM may find the part-of-speechselection more constrained.
This in turn may improve itsaccuracy at selecting one of the tags that are available.The uninteresting case, of COtll'Se, is where every wordshape has the same tag, that is, a tag set of one.
Thissituation yields no useful syntaclic inforlnation from thedoctlnlent.
Since the use of word shape tokens doesreducethe mnount of information that is mailable to the tagger,it may rexluce the number of different tags it can accuratelyassign.
The proper size of the tagset becomes conshainedon one hand by the anloun\[ oJ syntactic ill\[Ormation wewish to extract (more inlk~rmation with a larger tagset) andon the other by the size of the ambiguity classes of theword shape tokens (more ambiguity with a larger tagset).Its proper size is thus an empirical question.
For our testswe used tagsets vdth approximately 30parts of speech.7 TIlE TRAINING PROCESSJust as the hiddcn Markov model fc, r standard tcxttagging requires a large corpus of text to tram on, the wordshape HMM requires a large corpns of text that has beenconverted to word shape tokens.
We used at least 3.5megabytes of ASCII text for our standard text laggcr'scorpus; we then shape converted this text to create thecorpus for the word shape tagger.
This corpus consisted ofa variety of different writing styles (from colloquial toprofessional) and difficulty levels (from casual Io erudite).\[-'2xamplcs include ssays by huulorists, proposals lor newgovernment lx~lieies, and classic works o\[ literature.6888 Tt lE  TA( ;G IN( ;  PRO(~ENSWith the word Shal)C lexicon in place and tin adequatclytrained 1 \[MM, word shape tagging works just as stmldmdtext tagging does.
111 part(el.liar, word simpe taggingconsists of the following steps:I.
A stieanit of  tcxl is tokenized in(() a streani of w(,irdshap0 tok0ns segnlented itlto S0lltellces.2.
The slml)c-eonvcrted lexicon assigns an ambiguityclass to caeh word sl,iape tokcll.
Thc ucsult is i/ StlCi(lll ()l"sentence++ composed of (word shape Ioke., amhig.ilvclas.v) pairs.3.
The laggcr uses thc trained hidden Mark(,iv model tocomtmtc the highest probability part <11 speech for eachword shape t(~ken in a sen(cute.
The rcsult ix a stream of(word shape loken, part o/ speech) pairs, ~hich  aregrouped accordiilg to senletice bOUlMaties,W0 can limx us0 the r0sulting l,it/l'ts O( speeclit to i l l lOlM()thor se~+(litleltts of i,i t|OetllTIoiql ulldelSli l l ldil lg :;ystelll.
Theword shape ixut--ol-spcech laggcr tiros accepts w,ind shal)etokens grouped by solltei,iee blltuldaries; w i th in  thoseboundaries, it assigiis the inl)sl l ike ly part of speech t(~ca(hi word shat~c tok0n.9 RI?SULTSIn thlis section, we introduce i,i tool which etinrecognize noun phrases in sentences, and we use this toolto conipme the performalitcc (11 the standard taggcr and timwoix_l shape tagger.
We exempli ly the comparison withtWO texts: one on which the staitidard tagger perfoims verywel l ,  al,id oitic oitit which it does rehitti+ely p(+oity.
Whi lethe word shape tagger does h:ss well in each case, itsbehavit/r  tracks that (fl' the standard tagger, exhibit ingsiinihu" successes aild faihlrcs, l:or the partieuhu task (/Iiindiititg simple notln piuases, the word shape tagger'spei'l'01l/lililee is less than tJilit of the standard tlll~gcl's, bill ahu'ge !+l;aetion of the litOtliit phrases till are found.Wc have Lit s.ystcnl tht,it (:till ieeogiitiie sJlnplc lie(illphrases whcn givei,i its input the seq,ileilce of tags Iot aSOl,licit(co+ t{ach of these phrases conlpriscs a contigtloussequence o1 tags that satisfies a strut+h: gral,illilar, l"orexample, a II(,itlll pluase eltil be simi)ly a plonoull  t~ig or (,inall:l itlaly setitlellce (:,I lie(It1 lind ad.iective lags, pms ib lypreceded by a dctell, iHiler lag and possibly with tillembedded possessive lag.
2 The hlngesl possible S,ilehsequences itr0 I+otmd.
(\]oi,ij,ili,ictions ale l}oi lec~>gliized (ispiut of  a llOUll pinase, llOl is prcp(+sithmal Dhirase:alhlehnient perf()rii,icd.
We can bc eonl ident of f indingIll(lily shnple no(it/phmtses b0cause the t~ old "thC' hlas Ihetnlique word shape /% A x.
3 I,~ccognilion e l  i1(1(111 phrasesis a i irst sicp in topic idcnt i l ieat ion:  the topie (it ad(,iCUlilel,it is l ike ly t<l be indicaled t)3 its lnosl hequenttie(in phrases.li,i 0vahialing., the hit(gel e l le l  rnle, wc rise s0veralliiteaS,illes (s0c tables).
We calculate lhc pcleenlagc of IolaJerror~, thc percentage of Irh,Tal error& and the porcel,illlge?.
The i)osscssivc tap is tlsc(I for " 's " el ' r ,, as in "the cat'sl)ajanias' striF, es"3 Another I,inglish xvc)l'd, "lhl," also maps Io AAx;I'ollilllalcly, ill III+.
)SI Ct)lllL'XlS Ihis word is l{llC(~1 l~erniciouserror~ (there me tit few eiT()lS that do not fallin either of the latter categories).
Tagging 'lalaMning" ill"what the advocates a,e finding ahuming" as tit presenttxuticipk: rather than as an adjeclivc is an examplc /fl atrivial error.
Pernic ious errors typically invoh, ere(stagging nouns as verbs or verbs as nouns (in l';nglish,there ~tlC ilially stlrtitce IOIIDS that can be either l,i()lllHlal (,ilverbal).
These latter el'l"()i+s e0.11se probh:ms in h,itcrpl+oeessiitlg, suchl as dote.
(ring simple ititOUitl phrases, siticethey May (IbNctll'l: 1101111 phl+a+~es or illh+odtlce spurious(/lies.We compatc the standat+d tagger and the word shapetaggcr by counting the real(hem in the strcatns of outputtags.
We do not demand strict matches, but ms(cad allowthe rags to belong to pertinent equivalence classes.
I,'orexarnple, the standat+d tagger labels the noun "monitors" asa plural noun, at,id the word shape tagger la\[xelsx x x i A x x x simply (is a litOut,i.
We c()ititsider this a match,SillCe it l,i(Itllit \[ttitd a plkitit'ltl itit(3tllit iltl'e equal ly  wel l  recognizedas pttrt o1 it lit)till( phrase,Ahl,iosl all instances ,(,it niismatehes rcstllt from thestandard tagger being right and the word shape lagger beingwrong.
Very occasionally the situatiotit ix the reverse, butthis ix to be expected as within the normal range ofprobabilities.
More interesting is the observation thatalmost every pernicious error made by the standard taggerix repeated by Ihe word shape tagger.
Wc take this a+sc(,infirtnaltion of tim word shape tagger's ability toappmxintate he standard tagger's pcrtoimat,iee.The first COl/ll)arisoII of  tagger peMormance involves a30/!---w(,ird excell,it I+l+Ollit  govorl/I,ilelll, doctliititent.
Thestandard lagger's I)eitT()itmance is better than 95c)~ correct.
(itbettcr than 97% if trivial errors are disregarded.
The wordshape tagger's perl(irnuuitee is a 59% match (11 the standaFdtagger's (or 51% if only exact matches are considered).The noun phrase recogni/.er \[outld 113 sinlple limlnphrases in the standard tagger's (,itltptlt iitlitd 77 ((~b;%) o1these in the word shape taggcr's OUtl)Ut.S iandal 'd Tagger  Erl 'orsMatch ing  Outpu!
o lStandard  Tagger  and Word  Shape Taggerl)isregardmg Ineludiilg all\]t_ 'lh trial Misnultehes Mismatches 159% - .
.
.
.
.
.
-5"~ i}ll'lellt\[Nonscnsc l~l"/<: 38~ INoun Phr:tses Recognized f rom Tagger  Outpu lThe second comparison is of lags, big a 14+I word pieceel IIOItSeIISC VCI'SC.
The stiilldiild t+:.lg.gcr's i)et f+.
)rnlilncc is68989% correct, or 94% disregarding trivial errors.
The wordshape tagger's perfornmnce is a 47% match (or 38%considering only exact matches).
The noun phraserecognizer found 45 simple noun phrases in the slandm'dtagger's output and 17 (38%) of these in the word shapelagger's output.\[:urther study is needed to determine xactly howreliable word shape part-of-speech tagging and simplenoun phrase recognition will be in finding the topic ortopics in a document image, One means of improvingthis reliability may be our technique for grammaticalfunction assignment which uses only the output of thepart-of-speech taggerand phrase recognizers (Sibun 1991).However, we can abeady nse part-of-speech lagging andsimple noun phrase recognition as a tool for discerningsomething about the content of the document bydiscovering at least some of its noun phrases, Since ourdocument rceognition technology allows us to use wordshape tokens to index directly into the document image,we can also identify parts of the image as promisingcandidates for OCR,10 I ) ISCUSSIONAlthough the word shape tagger- tleals wilh greaterambiguity, it can still extract significant information froma text.
The increase in ambiguity is not as high as mightbe expected: a large number of word shapes remainunambiguous after the lexicon has been shape converted.As noted above, the creation of the word shape lexiconfrom the standard lexicon reduces the number of distinctentries to approximately one-third.
Vor example, distinctwords such as "cat" and "rat" map onto the same wordshape token xxA.
Nevertheless, the complexity ofEnglish spelling still allows a large proportion of surfaceforms to be distinguished merely by their word shapes.Several inlprovements on our technique remain to befully implemented.
We do not yet have a principled wayto determine the optimal tagset for a given corpus of texl.As noted alxwe, there is a tension between the size of thetagset and the amount of syntactic information that isavailable in the word shape tokens.We are also investigating computationally inexpensiveways of making finer distinctions between characte,s thatmap to the character shape codes x and A. Initially,parentheses and brackets were always classified as A antidistorted any word shape they were adjacent to: forexample, "(USA)" woukl be shape converted to A A A A A.Recently we have made progress m recognizing these noraalphabetic characters as wnrd shape token delimiters, ratherthan parts of the word shape tokens Ihemselves.
It mayalso be useful to distinguish more alphabetic haracterelasses by mapping scanned character bnages to a largerset of chmacter shape codes.
We can ext,'act more usefulinlknmation by distinguishing upper case letters fromlower case letters, such as "h" and "k", which malt to thecharacter shape code A.
A larger number of charactershape codes gives us more information about the wordshape tokens, and helps Io reduce ambiguity, l lowever,we must be careful to choose character shape featureswhich can bc easily dctccted in the image and quicklyclassi fled by a character shape ctx.le.In keeping with Vnji Xerox's multiqingual documentemphasis, we are also exploring ways in which thismethod may be applied to other Roman-alphabetlanguages, uch as French, German, Dutch, and Spanish.The technique will need to be evaluated separately for eachlanguage, however, to better understand how eachhmguage's typographic conventions may be reflected in itsword shape.1 1 CONCLI !S IONWe have presented a new technique for theunderstanding of English document images without opticalcharacter ecognition.
By scanning and categorizingcharacter shapes, it is possible to extract word shapes fromthe document text; these word shapes tokens can then beused as input to a tagger which determines part-of-speechreformation.
This part-of-speech inlormation can then beused to inform other document understanding techniques,including noun phrase recognition and topic identification.The lack of OCR means we cannot extract all of theinformation contained in the scanned ??
:tnnent's image;nevertheless, the information from the word shape tokensallows us to characterize the document's content withsignificant accuracy, and more quickly than if weperformed O(;\[,I.AcknowledgmentsWe thank Larry Spitz and Masa Ozaki for their usefulcomments.ReferencesBaum, 1,.
E. "An inequality anti associated maximizationtechnique in statistical estimation for pmbabilisticfunctions of a Markov process."
luequalilies, 3: l--g,1972.Cutting, Doug, Julian Kupiec, Jan Pedersen, and PenelopeSibun.
"A Practical Part-of-Speech Tagger."
InProceedings of the Third Cot{ference on AppliedNatural Language Processing (ACIJ, pp 133- 140,Trento, Italy, 1992.
Also Report SSL-924)l/P92-00(X)I, Xerox Palo Alto Research Center, 1992.Nakayama, Takehiro and A. L. Spitz.
"EuropeanLanguage Determination from Image."
In l~roeeedingsof the SeColld hllernational (~.onference on D(K3tlnlerl\[Amflysis and Recognition, pp 159-162, TsukubaScience City, Japan, 1993.Sibun, Penelope.
"Grammatical Function Assignment inUnrestricted Text."
Inte,nal Report, Xerox Palo AltoResearch Center, 1991.Sibun, Penelope and A. l,awrence Spitz.
"LanguageDetermination: Natural l,anguage Processing froluScanned l)ocnnmnt Images."
l:orthcoming.Viterbi, A. J.
"Error Ix)unds \[k)r convolution codes and ctnasymptotically optimal decoding algorithm."
llqffs'Transactions on lt{/brmalion Theory.
pit 260-269.April 1967.690
