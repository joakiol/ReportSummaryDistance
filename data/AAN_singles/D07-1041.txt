Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
390?399, Prague, June 2007. c?2007 Association for Computational LinguisticsPart-of-speech Tagging for Middle English through Alignment andProjection of Parallel Diachronic TextsTaesun Moon and Jason BaldridgeDepartment of LinguisticsUniversity of Texas at Austin1 University Station B5100Austin, TX 78712-0198 USAtsmoon, jbaldrid@mail.utexas.eduAbstractWe demonstrate an approach for inducing atagger for historical languages based on ex-isting resources for their modern varieties.Tags from Present Day English source textare projected to Middle English text usingalignments on parallel Biblical text.
Weexplore the use of multiple alignment ap-proaches and a bigram tagger to reduce thenoise in the projected tags.
Finally, we traina maximum entropy tagger on the output ofthe bigram tagger on the target Biblical textand test it on tagged Middle English text.This leads to tagging accuracy in the low80?s on Biblical test material and in the 60?son other Middle English material.
Our re-sults suggest that our bootstrapping meth-ods have considerable potential, and couldbe used to semi-automate an approach basedon incremental manual annotation.1 IntroductionAnnotated corpora of historical texts provide an im-portant resource for studies of syntactic variationand change in diachronic linguistics.
For example,the Penn-Helsinki Parsed Corpus of Middle English(PPCME) (Kroch and Taylor, 2000) has been usedto show the existence of syntactic dialectal differ-ences between northern and southern Middle En-glish (Kroch et al, 2000) and to examine the syn-tactic evolution of the English imperative construc-tion (Han, 2000).
However, their utility rests on theirhaving coverage of a significant amount of annotatedmaterial from which to draw patterns for such stud-ies, and creating resources such as the PPCME re-quire significant time and cost to produce.
Corpuslinguists interested in diachronic language studiesthus need efficient ways to produce such resources.One approach to get around the annotation bottle-neck is to use semi-automation.
For example, whenproducing part-of-speech tags for the Tycho Brahecorpus of Historical Portuguese (Britto et al, 2002),a set of seed sentences was manually tagged, and theBrill tagger (Brill, 1995) was then trained on thoseand consequently used to tag other sentences.
Theoutput was inspected for errors, the tagger was re-trained and used again to tag new sentences, for sev-eral iterations.We also seek to reduce the human effort involvedin producing part-of-speech tags for historical cor-pora.
However, our approach does so by leveragingexisting resources for a language?s modern varietiesalong with parallel diachronic texts to produce accu-rate taggers.
This general technique has worked wellfor bilingual bootstrapping of language processingresources for one language based on already avail-able resources from the other.
The first to explorethe idea were Yarowsky and Ngai (2001), who in-duced a part-of-speech tagger for French and basenoun phrase detectors for French and Chinese viatransfer from English resources.
They built a highlyaccurate POS tagger by labeling English text with anexisting tagger (trained on English resources), align-ing that text with parallel French, projecting the au-tomatically assigned English POS tags across thesealignments, and then using the automatically labeledFrench text to train a new French tagger.
This tech-390nique has since been used for other languages andtasks, e.g.
morphological analysis (Yarowsky et al,2001), fine-grained POS tagging for Czech (Dra?bekand Yarowsky, 2005), and tagging and inducing syn-tactic dependencies for Polish (Ozdowska, 2006).This methodology holds great promise for pro-ducing tools and annotated corpora for processingdiachronically related language pairs, such as Mod-ern English to Middle or Old English.
Historicallanguages suffer from a paucity of machine readabletext, inconsistencies in orthography, and grammati-cal diversity (in the broadest sense possible).
Thisdiversity is particularly acute given that diachronictexts of a given language encompass texts and gen-res spanning across centuries or millenia with aplethora of extra-linguistic influences to complicatethe data.
Furthermore, even in historically contem-poraneous texts, possible dialectal variations furtheramplify the differences in already idiosyncratic or-thographies and syntactic structure.The present study goes further than Britto et al(2002) by fully automating the alignment, POS taginduction, and noise elimination process.
It is able toutilize the source language to a greater degree thanthe previously mentioned studies that attempted lan-guage neutrality; that is, it directly exploits the ge-netic similarity between the source and target lan-guage.
Some amount of surface structural similaritybetween a diachronic dialect and its derivatives is tobe expected, and in the case of Middle English andModern English, such similarities are not negligible.The automation process is further aided throughthe use of two versions of the Bible, which obviatesthe need for sentence alignment.
The modern Bibleis tagged using the C&C maximum entropy tagger(Curran and Clark, 2003), and these tags are trans-ferred from source to target through high-confidencealignments aquired from two alignment approaches.A simple bigram tagger is trained from the resultingtarget texts and then used to relabel the same texts asMiddle English training material for the C&C tag-ger.
This tagger utilizes a rich set of features and awider context, so it can exploit surface similaritiesbetween the source and target language.
By train-ing it with both the original (Modern English) PennTreebank Wall Street Journal (WSJ) material andour automatically tagged Middle English Wycliffematerial, we achieve an accuracy of 84.8% on pre-dicting coarse tags, improving upon a 63.4% base-line of training C&C on the WSJ sentences alone.Furthermore, we show that the bootstrapped taggergreatly reduces the error rate on out-of-domain, non-Biblical Middle English texts.2 DataEnglish provides an ideal test case for our study be-cause of the existence of publically accessible di-achronic texts of English and their translations inelectronic format and because of the availability ofthe large, annotated Penn-Helsinki Parsed Corpus ofMiddle English.
The former allows us to create aPOS tagger via alignment and projection; the latterallows us to evaluate the tagger on large quantitiesof human-annotated tags.2.1 The Bible as a parallel corpusWe take two versions of the Bible as our parallel cor-pus.
For modern English, we utilize the NET Bible1.For Middle English (ME), we utilize John Wycliffe?sBible2.
The first five lines of Genesis in both Biblesare shown in Figure 1.The Bible offers some advantages beyond itsavailability.
All its translations are numbered, fa-cilitating assessment of accuracy for sentence align-ment models.
Also, the Bible is quite large fora single text: approximately 950,000 words forWycliffe?s version and 860,000 words for the NETbible.
Finally, Wycliffe?s Bible was released in thelate 14th century, a period when the transition of En-glish from a synthetic to analytical language wasfinalized.
Hence, word order was much closer toModern English and less flexible than Old English;also, nominal case distinctions were largely neutral-ized, though some verbal inflections such as dis-tinctions for the first and second person singular inthe present tense were still in place (Fennell, 2001).This places Wycliffe?s Bible as far back as possiblewithout introducing extreme nominal and verbal in-flections in word alignment.The two Bibles were cleaned and processed forthe present task and then examined for levels ofcorrespondence.
The two texts were compared for1The New English Translation Bible, which may be down-loaded from http://www.bible.org/page.php?page id=3086.2Available for download at:http://wesley.nnu.edu/biblical studies/wycliffe.3911 In the beginning God created the heavens and the earth.2 Now the earth was without shape and empty, and darkness was over the surface of the waterydeep, but the Spirit of God was moving over the surface of the water.3 God said, ?Let there be light.?
And there was light!4 God saw that the light was good, so God separated the light from the darkness.5 God called the light day and the darkness night.
There was evening, and there was morning,marking the first day.1 In the bigynnyng God made of nouyt heuene and erthe.2 Forsothe the erthe was idel and voide, and derknessis weren on the face of depthe; and the Spirytof the Lord was borun on the watris.3 And God seide, Liyt be maad, and liyt was maad.4 And God seiy the liyt, that it was good, and he departide the liyt fro derknessis; and he clepidethe liyt,5 dai, and the derknessis, nyyt.
And the euentid and morwetid was maad, o daie.Figure 1: The first five verses of Genesis the NET Bible (top) and Wycliffe?s Bible (below).whether there were gaps in the chapters and whetherone version had more chapters over the other.
If dis-crepancies were found, the non-corresponding chap-ters were removed.
Next, because we assume sen-tences are already aligned in our approach, discrep-ancies in verses between the two Bibles were culled.A total of some two hundred lines were removedfrom both Bibles.
This processing resulted in a totalof 67 books3, with 920,000 words for the WycliffeBible and 840,000 words for the NET Bible.2.2 The Penn-Helsinki Parsed Corpus ofMiddle EnglishThe Penn-Helsinki Parsed Corpus of Middle En-glish is a collection of text samples derived frommanuscripts dating 1150?1500 and composed dur-ing the same period or earlier.
It is based on andexpands upon the Diachronic Part of the HelsinkiCorpus of English Texts.
It contains approximately1,150,000 words of running text from 55 sources.The texts are provided in three forms: raw, POStagged, and parsed.Among the texts included are portions of theWycliffe Bible.
They comprise partial sections ofGenesis and Numbers from the Old Testament andJohn I.1?XI.56 from the New Testament.
In total,366 books shared by the churches and one book from theApocrypha.
A comparison of the two Bibles revealed thatthe NET Bible contained the Apocrypha, but only Baruch wasshared between the two versions.the sections of Wycliffe annotated in PPCME havesome 25,000 words in 1,845 sentences.
This wasused as part of the test material.
It is important tonote that there are significant spelling differencesfrom the full Wycliffe text that we use for alignment?
this is a common issue with early writings thatmakes building accurate taggers for them more diffi-cult than for the clean and consistent, edited moderntexts typically used to evaluate taggers.2.3 TagsetsThe PPCME uses a part-of-speech tag set that hassome differences from that used for the Penn Tree-bank, on which modern English taggers are gener-ally trained.
It has a total of 84 word tags comparedto the widely used Penn Treebank tag set?s 36 wordtags.4 One of the main reasons for the relative diver-sity of the PPCME tag set is that it maintains distinc-tions between the do, have, and be verbs in additionto non-auxiliary verbs.
The tag set is further com-plicated by the fact that composite POS tags are al-lowed as in another D+OTHER, midnyght ADJ+N,or armholes N+NS.To measure tagging accuracy, we consider twodifferent tag sets: PTB, and COARSE.
A measure-ment of accuracy is not possible with a direct com-parison to the PPMCE tags since our approach la-4In our evaluations, we collapse the many different punctu-ation tags down to a single tag, PUNC.392bels target text in Middle English with tags fromthe Penn Treebank.
Therefore, with PTB, all non-corresponding PPCME tags were conflated if neces-sary and mapped to the Penn Treebank tag set.
Be-tween the two sets, only 8 tags, EX, FW, MD, TO, VB,VBD, VBN, VBP, were found to be fully identical.In cases where tags from the two sets denoted thesame category/subcategory, one was simply mappedto the other.
When a PPCME tag made finer dis-tinctions than a related Penn tag and could be con-sidered a subcategory of that tag, it was mapped ac-cordingly.
For example, the aforementioned auxil-iary verb tags in the PPMCE were all mapped to cor-responding subcategories of the larger VB tag group,a case in point being the mapping of the perfect par-ticiple of have HVN to VBN, a plain verbal partici-ple.
For COARSE, the PTB tags were even furtherreduced to 15 category tags,5 which is still six morethan the core consensus tag set used in Yarkowskyand Ngai (2001).
Specifically, COARSE was mea-sured by comparing the first letter of each tag.
Forexample, NN and NNS are conflated to N.2.4 Penn Treebank Release 3The POS tagged Wall Street Journal, sections 2 to21, from the Penn Treebank Release 3 (Marcus etal., 1994) was used to train a Modern English taggerto automatically tag the NET Bible.
It was also usedto enhance the maximum likelihood estimates of abigram tagger used to label the target text.3 ApproachOur approach involves three components: (1) pro-jecting tags from Modern English to Middle Englishthrough alignment; (2) training a bigram tagger; and(3) bootstrapping the C&C tagger on Middle En-glish texts tagged by the bigram tagger.
This sectiondescribes these components in detail.3.1 Bootstrapping via alignmentYarowsky and Ngai (2001) were the first to proposethe use of parallel texts to bootstrap the creation oftaggers.
The approach first requires an alignmentto be induced between the words of the two texts;5Namely, adjective, adverb, cardinal number, complemen-tizer/preposition, conjunction, determiner, existential there, for-eign word, interjection, infinitival to, modal, noun, pronoun,verb, and wh-words.tags are then projected from words of the source lan-guage to words of the target language.
This natu-rally leads to the introduction of noise in the targetlanguage tags.
Yarowsky and Ngai deal with thisby (a) assuming that each target word can have atmost two tags and interpolating the probability oftags given a word between the probabilities of thetwo most likely tags for that word and (b) interpo-lating between probabilities for tags projected from1-to-1 alignments and those from 1-to-n alignments.Each of these interpolated probabilities is parame-terized by a single variable; however, Yarowsky andNgai do not provide details for how the two param-eter values were determined/optimized.Here, we overcome much of the noise by usingtwo alignment approaches, one of which exploitsword level similarities (present in genetically de-rived languages such as Middle English and PresentDay English) and builds a bilingual dictionary be-tween them.
We also fill in gaps in the alignmentby using a bigram tagger that is trained on the noisytags and then used to relabel the entire target text.The C&C tagger (Curran and Clark, 2003) wastrained on the Wall Street Journal texts in the PennTreebank and then used to tag the NET Bible (thesource text).
The POS tags were projected from thesource to the Wycliffe Bible based on two alignmentapproaches, the Dice coefficient and Giza++, as de-scribed below.3.1.1 Dice alignmentsA dictionary file is built using the variation ofthe Dice Coefficient (Dice (1945)) used by Kay andRo?scheisen (1993):D(v,w) = 2cNA(v) + NB(w)?
?Here, c is the number of cooccurring positions andNT (x) is the number of occurrences of word x incorpus T .
c is calculated only once for redundantoccurrences in an aligned sentence pair.
For exam-ple, it is a given that the will generally occur morethan once in each aligned sentence.
However, even ifthe occurs more than once in each of the sentences inaligned pair sA and sB, c is incremented only once.v and w are placed in the word alignment table ifthey exceed the threshold value ?, which is an em-pirically determined, heuristic measure.393The dictionary was structured to establish a sur-jective relation from the target language to thesource language.
Therefore, no lexeme in theWycliffe Bible was matched to more than one lex-eme in the NET Bible.
The Dice Coefficient wasmodified so that for a given target word vDv = arg maxwD(v,w)would be mapped to a corresponding word from thesource text, such that the Dice Coefficient would bemaximized.
Dictionary entries were further culledby removing (v,w) pairs whose maximum Dice Co-efficient was lower than the ?
threshold, for whichwe used the value 0.5.
Finally, each word which hada mapping from the target was sequentially mappedto a majority POS tag.
For example, the word likewhich had been assigned four different POS tags,IN, NN, RB, VB, by the C&C tagger in the NETBible was only mapped to IN since the pairings ofthe two occurred the most frequently.
The result isa mapping from one or more target lexemes to asource lexeme to a majority POS tag.
In the caseof like, two words from the target, as and lijk, weremapped thereto and to the majority tag IN.Later, we will refer to the Wycliffe text (partially)labeled with tags projected using the Dice coeffi-cient as DICE 1TO1.3.1.2 GIZA++ alignmentsGiza++ (Och and Ney, 2003) was also used to de-rive 1-to-n word alignments between the NET Bibleand the Wycliffe Bible.
This produces a tagged ver-sion of the Wycliffe text which we will refer to asGIZA 1TON.
In our alignment experiment, we useda combination of IBM Model 1, Model 3, Model 4,and an HMM model in configuring Giza++.GIZA 1TON was further processed to removenoise from the transferred tag set by creating a 1-to-1word alignment: each word in the target Middle En-glish text was given its majority tag based on the as-signment of tags to GIZA 1TON as a whole.
We callthis version of the tagged Wycliffe text GIZA 1TO1.3.2 Bigram taggerNote that because the projected tags in the Wycliffematerials produced from the alignments are incom-plete, there are words in the target text which haveno tag.
Nonetheless, a bigram tagger can be trainedfrom maximum likelihood estimates for the wordsand tag sequences which were successfully pro-jected.
This serves two functions: (1) it creates auseable bigram tagger and (2) the bigram tagger canbe used to fill in the gaps so that the more powerfulC&C tagger can be trained on the target text.A bigram tagger selects the most likely tag se-quence T for a word sequence W by:arg maxTP (T |W ) = P (W |T )P (T )Computing these terms requires knowing the transi-tion probabilities P (ti|ti?1) and the emission proba-bilities P (wi|ti).
We use straightforward maximumlikelihood estimates from data with projected tags:P (ti|ti?1) =f(ti?1, ti)f(ti?1)P (wi|ti) =f(wi, ti)f(ti)Estimates for unseen events were obtainedthrough add-one smoothing.In order to diversify the maximum likelihood es-timates and provide robustness against the errorsof any one alignment method, we concatenate sev-eral tagged versions of the Wycliffe Bible with tagsprojected from each of our methods (DICE 1TO1,GIZA 1TON, and GIZA 1TO1) and the NET Bible(and its tags from the C&C tagger).3.3 Training C&C on projected tagsThe bigram tagger learned from the aligned text hasvery limited context and cannot use rich featuressuch as prefixes and suffixes of words in making itspredictions.
In contrast, the C&C tagger, which isbased on that of Ratnaparkhi (1996), utilizes a widerange of features and a larger contextual window in-cluding the previous two tags and the two previousand two following words.
However, the C&C taggercannot train on texts which are not fully tagged forPOS, so we use the bigram tagger to produce a com-pletely labeled version of the Wycliffe text and trainthe C&C tagger on this material.
The idea is thateven though it is training on imperfect material, itwill actually be able to correct many errors by virtueof its greater discriminitive power.394Evaluate on Evaluate onPPCME Wycliffe PPCME TestModel PTB COARSE PTB COARSE(a) Baseline, tag NN 9.0 17.7 12.6 20.1(b) C&C, trained on gold WSJ 56.2 63.4 56.2 62.3(c) Bigram, trained on DICE 1TO1 and GIZA 1TON 68.0 73.1 43.9 49.8(d) Bigram, trained on DICE 1TO1 and GIZA 1TO1 74.8 80.5 58.0 63.9(e) C&C, trained on BOOTSTRAP (920k words) 78.8 84.1 61.3 67.8(f) C&C, trained on BOOTSTRAP and WSJ and NET 79.5 84.8 61.9 68.5(g) C&C, trained on (gold) PPCME Wycliffe (25k words) n/a n/a 71.0 76.0(h) C&C, trained on (gold) PPCME training set (327k words) 95.9 96.9 93.7 95.1Figure 2: Tagging results.
See section 4 for discussion.We will refer to the version of the Wycliffe text(fully) tagged in this way as BOOTSTRAP.4 ExperimentsThe M3 and M34 subsections6 of the Penn Helsinkicorpus were chosen for testing since it is not onlyfrom the same period as the Wycliffe Bible but sinceit also includes portions of the Wycliffe Bible.
Atraining set of 14 texts comprising 330,000 wordswas selected to train the C&C tagger and test thecost necessary to equal or exceed the automatic im-plementation.
The test set consists of 4 texts with110,000 words.
The sample Wycliffe Bible with thegold standard tags has some 25,000 words.The results of the various configurations are givenin Figure 2, and are discussed in detail below.4.1 BaselinesWe provide two baselines.
The first is the result ofgiving every word the common tag NN .
The sec-ond baseline was established by directly applyingthe C&C tagger, trained on the Penn Treebank, tothe PPCME data.
The results are given in lines (a)and (b) of Figure 2 for the first and second baselines,respectively.
As can be seen, the use of the Mod-ern English tagger already provides a strong startingpoint for both evaluation sets.6Composition dates and manuscript dates for M3 are 1350-1420.
The composition dates for M34 are the same but themanuscripts date 1420-15004.2 Bigram taggersIn section 3.1, we discuss three versions of theWycliffe target text labeled with tags projectedacross alignments from the NET Bible.
Themost straightforward of these were DICE 1TO1 andGIZA 1TON which directly use the alignments fromthe methods.
Training a bigram tagger on thesetwo sources leads to a large improvement over theC&C baseline on the PPCME Wycliffe sentences,as can be seen by comparing line (c) to line (b)in Figure 2.
However, performance drops on thePPCME Test sentences, which come from differentdomains than the bigram tagger?s automatically pro-duced Wycliffe training material.
This difference islikely to do good estimates of P (wi|ti), but poor es-timates of P (ti|ti?1) due to the noise introduced inGIZA 1TON.More conservative tags projection is thus likelyto have a large effect on the out-of-domain perfor-mance of the learned taggers.
To test this, we traineda bigram tagger on DICE 1TO1 and the more con-servative GIZA 1TO1 projection.
This produces fur-ther gains for the PPCME Wycliffe, and enormousimprovements on the PPCME Test data (see line (d)of Figure 2).
This result confirms that conservativitybeats wild guessing (at the risk of reduced coverage)for bootstrapping taggers in this way.
This is verymuch in line with the methodology of Yarowksy andNgai (2001), who project a small number of tags outof all those predicted by alignment.
They achievethis restriction by directly adjusting the probabalitymass assigned to projected tags; we do it by usingtwo versions of the target text with tags projected in395two different 1-to-1 ways.4.3 Bootstrapping the C&C taggerAs described in section 3.3, a bigram tagger trainedon DICE 1TO1 and GIZA 1TO1 (i.e., the tagger ofline (d)), was used to relabel the entire Wycliffe tar-get text to produce training material for C&C, whichwe call BOOTSTRAP.
The intention is to see whetherthe more powerful tagger can bootstrap off imper-fect tags and take advantage of its richer features toproduce a more accurate tagger.
As can be seen inrow (e) of Figure 2, it provides a 3-4% gain acrossthe board over the bigram tagger which produced itstraining material (row (d)).We also considered whether using all available(non-PPCME) training material would improve tag-ging accuracy by training C&C on BOOTSTRAP,the Modern English Wall Street Journal (from thePenn Treebank), and the automatically tagged NETtext7 It did produce slight gains on both test setsover C&C trained on BOOTSTRAP alone.
This islikely due to picking up some words that survivedunchanged to the Modern English.
Of course, theutility of modern material used directly in this man-ner will likely vary a great deal depending on thedistance between the two language variants.
What isperhaps most interesting is that adding the modernmaterial did not hurt performance.4.4 UpperboundsIt is apparent from the results that there is a strongdomain effect on the performance of both the bigramand C&C taggers which have been trained on auto-matically projected tags.
There is thus a question ofhow well we could ever hope to perform on PPCMETest given perfect tags from the Wycliffe texts.
Totest this, C&C was trained on the PPCME version ofWycliffe, which has human annotated standard tags,and then applied on the PPCME test set.
We alsocompare this to training on PPCME texts which aresimilar to those in PPCME Test.The results, given in lines (g) and (h) of Figure2, indicate that there is a likely performance cap onnon-Biblical texts when bootstrapping from parallelBiblical texts.
The results in line (h) also show thatthe non-Biblical texts are more difficult, even with7This essentially is partial self-training since C&C trainedon WSJ was used to produce the NET tags.gold training material.
This is likely due to the widevariety of authors and genres contained in these texts?
in a sense, everything is slightly out-of-domain.4.5 Learning curves with manual annotationThe upperbounds raise two questions.
One iswhether the performance gap between (g) and (h) inFigure 2 on PPCME Test is influenced by the signif-icant difference in the size of their training sets.
Theother is how much gold-standard PPCME trainingmaterial would be needed to match the performanceof our best bootstrapped tagger (line (f)).
This is anatural question to ask, as it hits at the heart of theutility of our essentially unsupervised approach ver-sus annotating target texts manually.To examine the cost of manually annotating thetarget language as compared to our unsupervisedmethod, the C&C tagger was also trained on ran-domly selected sets of sentences from PPCME (dis-joint from PPCME Test).
Accuracy was measuredon PPCME Wycliffe and Test for a range of trainingset sizes, sampled at exponentially increasing values(25, 50, 100, .
.
.
, 12800).
Though we trained on andpredicted the full tagset used by the PPCME, it wasevaluated on PTB to give an accurate comparison.8The learning curves on both test sets are shownin Figure 3.
The accuracy of the C&C tagger in-creases rapidly, and the accuracy exceeds our auto-mated method on PPCME Test with just 50 labeledsentences and on the PPCME Wycliffe with 400 ex-amples.
This shows the domain of the target text isserved much better with the projection approach.To see how much gold-standard PPCME Wycliffematerial is necessary to beat our best bootstrappedtagger, we trained the tagger as in (g) of Figure 2with varying amounts of material.
Roughly 600 la-beled sentences were required to beat the perfor-mance of 61.9%/68.5% (line (f), on both metrics).These learning curves suggest that when the do-main for which one wishes to produce a tagger issignificantly different from the aligned text one hasavailable (in this and in many cases, the Bible), thenlabeling a small number of examples by hand is aquite reasonable approach (provided random sam-pling is used).
However, if one is not careful, con-siderable effort could be put into labeling sentences8Evaluation with the full PPCME set produces accuracy fig-ures about 1% lower.39660657075808590951000  2000  4000  6000  8000  10000  12000  14000AccuracyNumber of sentencesPPCME WycliffePPCME TestFigure 3: Learning curve showing the accuracy forPTB tags of the C&C tagger on both Bible and Testas it is given more gold-standard PPCME trainingsentences.that are not optimal overall (imagine getting unluckyand starting out by manually annotating primarilyWycliffe sentences).
The automated methods wepresent here start producing good taggers immedi-ately, and there is much room for improving themfurther.
Additionally, they could be used to aid man-ual annotation by proposing high-confidence labelseven before any annotation has begun.5 Related workDespite the fact that the Bible has been translatedinto many languages and that it constitutes a solidsource for studies in NLP with a concentration onmachine translation or parallel text processing, thenumber of studies involving the Bible is fairly lim-ited.
A near exhaustive list is Chew et al(2006),Melamed(1998), Resnik et al(1999), and Yarowskyet al(2001).Yarowsky and Ngai (2001) is of central rele-vance to this study.
The study describes an unsu-pervised method for inducing a monolingual POStagger, base noun-phrase bracketer, named-entitytagger and morphological analyzers from trainingbased on parallel texts, among many of which theBible was included.
This is particularly useful giventhat no manually annotated data is necessary in thetarget language and that it works for two languagesfrom different families such as French and Chinese.In the case of POS tagging, only the results forEnglish-French are given and an accuracy of 96% isachieved.
Even though this accuracy figure is basedon a reduced tag set smaller than the COARSE usedin this study, it is still a significant increase over thatachieved here.
However, their method had the ad-vantage of working in a domain that overlaps withthe training data for their POS tagger.
Second, thethe French tag set utilized in that study is consider-ably smaller than the Penn Helsinki tag set, a possi-ble source of greater noise due to its size.Dra?bek and Yarowsky (2005) create a fine-grained tagger for Czech and French by enrichingthe tagset for parallel English text with additionalmorphological information, which, though not di-rectly attested by the impoverished English morpho-logical system (e.g.
number on adjectives), typicallydoes appear in other languages.6 ConclusionThe purpose of the study was to implement a POStagger for diachronic texts of maximal accuracy withminimal cost in terms of labor, regardless of theshortcuts taken.
Such taggers are the building blocksin the design of higher level tools which dependon POS data such as morphological analyzers andparsers, all of which are certain to contribute to di-achronic language studies and genetic studies of lan-guage change.We showed that using two conservative methodsfor projecting tags through alignment significantlyimproves bigram POS tagging accuracies over abaseline of applying a Modern English tagger toMiddle English text.
Results were improved furtherby training a more powerful maximum entropy tag-ger on the predictions of the bootstrapped bigramtagger, and we observed a further, small boost byusing Modern English tagged material in addition tothe projected tags when training the maximum en-tropy tagger.Nonetheless, our results show that there is stillmuch room for improvement.
A manually annotatedtraining set of 400?800 sentences surpassed our bestbootstrapped tagger.
However, it should be notedthat the learning curve approach was based on do-main neutral, fully randomized, incremental texts,which are not easily replicated in real world appli-cations.
The domain effect is particularly evident in397training on the sample Wycliffe and tagging on thetest PPCME set.
Of course, our approach can be in-tegrated with one based on annotation by using ourbootstrapped taggers to perform semi-automated an-notation, even before the first human-annotated taghas been labeled.It is not certain how our method would fare on thefar more numerous parallel diachronic texts whichdo not come prealigned.
It is also questionablewhether it would still be robust on texts predatingMiddle English, which might as well be written ina foreign language when compared to Modern En-glish.
These are all limitations that need to be ex-plored in the future.Immediate improvements can be sought for the al-gorithms themselves.
By restricting the mapping ofwords to only one POS tag in the Wycliffe Bible,this seriously handicapped the utility of a bigramtagger.
It should be relatively straightforward totransfer the probability mass of multiple POS tagsin a modern text to corresponding words in a di-achronic text and include this modified probabilityin the bigram tagger.
When further augmented forautomatic parameter adjustment with the forward-backward algorithm, accuracy rates might increasefurther.
Furthermore, different algorithms might bebetter able to take advantage of similarities in or-thography and syntactic structure when constructingword alignment tables.
Minimum Edit Distance al-gorithms seem particularly promising in this regard.Finally, it is evident that the utility of the Bibleas a potential resource of parallel texts has largelygone untapped in NLP research.
Considering thatit has probably been translated into more languagesthan any other single text, and that this richnessof parallelism holds not only for synchrony but di-achrony, its usefulness would apply not only to themost immediate concern of building language toolsfor many of the the world?s underdocumented lan-guages, but also to cross-linguistic studies of un-precedented scope at the level of language genera.This study shows that despite the fact that any twoBibles are rarely in a direct parallel relation, stan-dard NLP methods can be applied with success.ReferencesEric Brill.
1995.
Transformation-based error-drivenlearning and natural language processing: a case studyin part-of-speech tagging.
Computational Linguistics,21(4):543?565.Helena Britto, Marcelo Finger, and Charlotte Galves,2002.
Computational and linguistic aspects of theconstruction of The Tycho Brahe Parsed Corpus ofHistorical Portuguese.
Tu?bingen: Narr.Peter A. Chew, Steve J. Verzi, Travis L. Bauer, andJonathan T. McClain.
2006.
Evaluation of the bibleas a resource for cross-language information retrieval.In Proceedings of the Workshop on Multilingual Lan-guage Resources and Interoperability, Sydney, July2006, pages 68?74.James R Curran and Stephen Clark.
2003.
Investigat-ing gis and smoothing for maximum entropy taggers.In Proceedings of the 11th Meeting of the EuropeanChapter of the Association for Computational Linguis-tics (EACL-03).Lee R. Dice.
1945.
Measures of the amount of eco-logic association between species.
Journal of Ecology,26:297?302.Elliott Franco Dra?bek and David Yarowsky.
2005.
In-duction of fine-grained part-of-speech taggers via clas-sifier combination and crosslingual projection.
In Pro-ceedings of the ACL Workshop on Building and Us-ing Parallel Texts, pages 49?56, Ann Arbor, Michigan,June.
Association for Computational Linguistics.Barbara A. Fennell.
2001.
A History of English: A Soci-olinguistic Approach.
Blackwell, Oxford.Chung-Hye Han, 2000.
The Evolution of Do-Support InEnglish Imperatives, pages 275?295.
Oxford Univer-sity Press.Martin Kay and Martin Ro?scheisen.
1993.
Text-translation alignment.
Computational Linguistics,19(1):121?142.Anthony Kroch and Ann Taylor.
2000.
Penn-helsinkiparsed corpus of middle english, second edition.Anthony Kroch, Ann Taylor, and Donald Ringe.
2000.The middle english verb-second constraint: A casestudy in language contact and language change.
Ams-terdam Studies in the Theory and History of LinguisticScience Series, 4:353?392.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics, 19(2):313?330.398Dan I. Melamed.
1998.
Manual annotation of transla-tion equivalence: The blinker project.
In TechnicalReport 98-07, Institute for Research in Cognitive Sci-ence, Philadelphia.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Sylwia Ozdowska.
2006.
Projecting pos tags and syntac-tic dependencies from english and french to polish inaligned corpora.
In EACL 2006 Workshop on Cross-Language Knowledge Induction.Adwait Ratnaparkhi.
1996.
A maximum entropy modelfor part-of-speech tagging.
In Eric Brill and Ken-neth Church, editors, Proceedings of the Conferenceon Empirical Methods in Natural Language Process-ing, pages 133?142.
Association for ComputationalLinguistics, Somerset, New Jersey.Philip Resnik, Mari Broman Olsen, and Mona Diab.1999.
The bible as a parallel corpus: Annotating the?book of 2000 tongues?.
Computers and the Humani-ties, 33(1?2):129?153.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual pos taggers and np bracketers via robust pro-jection across aligned corpora.
In NAACL ?01: Sec-ond meeting of the North American Chapter of the As-sociation for Computational Linguistics on Languagetechnologies 2001, pages 1?8, Morristown, NJ, USA.Association for Computational Linguistics.David Yarowsky, Grace Ngai, and Richard Wicentowski.2001.
Inducing multilingual text analysis tools viarobust projection across aligned corpora.
In HLT?01: Proceedings of the first international conferenceon Human language technology research, pages 1?8,Morristown, NJ, USA.
Association for ComputationalLinguistics.AppendixFigure 4 provides the full mapping from PPCMEtags to the Penn Treebank Tags used in our evalu-ation.PPCME?PTB PPCME?PTBADJR?JJR N?NNADJS?JJS N$?NNADV?RB NEG?RBADVR?RBR NPR?NNPADVS?RBS NPR$?NNPALSO?RB NPRS?NNPSBAG?VBG NPRS$?NNPSBE?VB NS?NNSBED?VBD NS$?NNSBEI?VB NUM?CDBEN?VBN NUM$?CDBEP?VBZ ONE?PRPC?IN ONE$?PRP$CODE?CODE OTHER?PRPCONJ?CC OTHER$?PRPD?DT OTHERS?PRPDAG?VBG OTHERS$?PRPDAN?VBN P?INDO?VB PRO?PRPDOD?VBD PRO$?PRP$DOI?VB Q?JJDON?VBN Q$?JJDOP?VBP QR?RBRE S?E S QS?RBSELSE?RB RP?RBEX?EX SUCH?RBFOR?IN TO?TOFOR+TO?IN VAG?VBGFP?CC VAN?VBNFW?FW VB?VBHAG?VBG VBD?VBDHAN?VBN VBI?VBHV?VB VBN?VBNHVD?VBD VBP?VBPHVI?VB WADV?WRBHVN?VBN WARD?WARDHVP?VBP WD?WDTID?ID WPRO?WPINTJ?UH WPRO$?WP$MAN?PRP WQ?INMD?MD X?XMD0?MDFigure 4: Table of mappings from PPCME tags toPenn Treebank Tags.399
