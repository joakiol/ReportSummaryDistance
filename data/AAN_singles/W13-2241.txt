Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 337?342,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsSHEF-Lite: When Less is More for Translation Quality EstimationDaniel Beck and Kashif Shah and Trevor Cohn and Lucia SpeciaDepartment of Computer ScienceUniversity of SheffieldSheffield, United Kingdom{debeck1,kashif.shah,t.cohn,l.specia}@sheffield.ac.ukAbstractWe describe the results of our submissionsto the WMT13 Shared Task on QualityEstimation (subtasks 1.1 and 1.3).
Oursubmissions use the framework of Gaus-sian Processes to investigate lightweightapproaches for this problem.
We focus ontwo approaches, one based on feature se-lection and another based on active learn-ing.
Using only 25 (out of 160) fea-tures, our model resulting from featureselection ranked 1st place in the scoringvariant of subtask 1.1 and 3rd place inthe ranking variant of the subtask, whilethe active learning model reached 2ndplace in the scoring variant using only?25% of the available instances for train-ing.
These results give evidence thatGaussian Processes achieve the state ofthe art performance as a modelling ap-proach for translation quality estimation,and that carefully selecting features andinstances for the problem can further im-prove or at least maintain the same per-formance levels while making the problemless resource-intensive.1 IntroductionThe purpose of machine translation (MT) qualityestimation (QE) is to provide a quality predictionfor new, unseen machine translated texts, with-out relying on reference translations (Blatz et al2004; Specia et al 2009; Callison-burch et al2012).
A common use of quality predictions isthe decision between post-editing a given machinetranslated sentence and translating its source fromscratch, based on whether its post-editing effort isestimated to be lower than the effort of translatingthe source sentence.The WMT13 QE shared task defined a groupof tasks related to QE.
In this paper, we presentthe submissions by the University of Sheffieldteam.
Our models are based on Gaussian Pro-cesses (GP) (Rasmussen and Williams, 2006), anon-parametric probabilistic framework.
We ex-plore the application of GP models in two con-texts: 1) improving the prediction performance byapplying a feature selection step based on opti-mised hyperparameters and 2) reducing the datasetsize (and therefore the annotation effort) by per-forming Active Learning (AL).
We submitted en-tries for two of the four proposed tasks.Task 1.1 focused on predicting HTER scores(Human Translation Error Rate) (Snover et al2006) using a dataset composed of 2254 English-Spanish news sentences translated by Moses(Koehn et al 2007) and post-edited by a profes-sional translator.
The evaluation used a blind testset, measuring MAE (Mean Absolute Error) andRMSE (Root Mean Square Error), in the case ofthe scoring variant, and DeltaAvg and Spearman?srank correlation in the case of the ranking vari-ant.
Our submissions reached 1st (feature selec-tion) and 2nd (active learning) places in the scor-ing variant, the task the models were optimisedfor, and outperformed the baseline by a large mar-gin in the ranking variant.The aim of task 1.3 aimed at predicting post-editing time using a dataset composed of 800English-Spanish news sentences also translated byMoses but post-edited by five expert translators.Evaluation was done based on MAE and RMSEon a blind test set.
For this task our models werenot able to beat the baseline system, showing thatmore advanced modelling techniques should havebeen used for challenging quality annotation typesand datasets such as this.2 FeaturesIn our experiments, we used a set of 160 featureswhich are grouped into black box (BB) and glassbox (GB) features.
They were extracted using the337open source toolkit QuEst1 (Specia et al 2013).We briefly describe them here, for a detailed de-scription we refer the reader to the lists availableon the QuEst website.The 112 BB features are based on source andtarget segments and attempt to quantify the sourcecomplexity, the target fluency and the source-target adequacy.
Examples of them include:?
Word and n-gram based features:?
Number of tokens in source and targetsegments;?
Language model (LM) probability ofsource and target segments;?
Percentage of source 1?3-grams ob-served in different frequency quartiles ofthe source side of the MT training cor-pus;?
Average number of translations persource word in the segment as given byIBM 1 model with probabilities thresh-olded in different ways.?
POS-based features:?
Ratio of percentage of nouns/verbs/etcin the source and target segments;?
Ratio of punctuation symbols in sourceand target segments;?
Percentage of direct object personal orpossessive pronouns incorrectly trans-lated.?
Syntactic features:?
Source and target Probabilistic Context-free Grammar (PCFG) parse log-likelihood;?
Source and target PCFG average confi-dence of all possible parse trees in theparser?s n-best list;?
Difference between the number ofPP/NP/VP/ADJP/ADVP/CONJPphrases in the source and target;?
Other features:?
Kullback-Leibler divergence of sourceand target topic model distributions;?
Jensen-Shannon divergence of sourceand target topic model distributions;1http://www.quest.dcs.shef.ac.uk?
Source and target sentence intra-lingualmutual information;?
Source-target sentence inter-lingual mu-tual information;?
Geometric average of target word prob-abilities under a global lexicon model.The 48 GB features are based on informationprovided by the Moses decoder, and attempt to in-dicate the confidence of the system in producingthe translation.
They include:?
Features and global score of the SMT model;?
Number of distinct hypotheses in the n-bestlist;?
1?3-gram LM probabilities using translationsin the n-best to train the LM;?
Average size of the target phrases;?
Relative frequency of the words in the trans-lation in the n-best list;?
Ratio of SMT model score of the top transla-tion to the sum of the scores of all hypothesisin the n-best list;?
Average size of hypotheses in the n-best list;?
N-best list density (vocabulary size / averagesentence length);?
Fertility of the words in the source sentencecompared to the n-best list in terms of words(vocabulary size / source sentence length);?
Edit distance of the current hypothesis to thecentre hypothesis;?
Proportion of pruned search graph nodes;?
Proportion of recombined graph nodes.3 ModelGaussian Processes are a Bayesian non-parametricmachine learning framework considered the state-of-the-art for regression.
They assume the pres-ence of a latent function f : RF ?
R, which mapsa vector x from feature space F to a scalar value.Formally, this function is drawn from a GP prior:f(x) ?
GP(0, k(x,x?
))which is parameterized by a mean function (here,0) and a covariance kernel function k(x,x?).
Each338response value is then generated from the functionevaluated at the corresponding input, yi = f(xi)+?, where ?
?
N (0, ?2n) is added white-noise.Prediction is formulated as a Bayesian inferenceunder the posterior:p(y?|x?,D) =?fp(y?|x?, f)p(f |D)where x?
is a test input, y?
is the test responsevalue andD is the training set.
The predictive pos-terior can be solved analitically, resulting in:y?
?
N (kT?
(K + ?2nI)?1y,k(x?, x?)?
kT?
(K + ?2nI)?1k?
)where k?
= [k(x?,x1)k(x?,x2) .
.
.
k(x?,xd)]Tis the vector of kernel evaluations between thetraining set and the test input and K is the kernelmatrix over the training inputs.A nice property of this formulation is that y?is actually a probability distribution, encoding themodel uncertainty and making it possible to inte-grate it into subsequent processing.
In this work,we used the variance values given by the model inan active learning setting, as explained in Section4.The kernel function encodes the covariance(similarity) between each input pair.
While a vari-ety of kernel functions are available, here we fol-lowed previous work on QE using GP (Cohn andSpecia, 2013; Shah et al 2013) and employeda squared exponential (SE) kernel with automaticrelevance determination (ARD):k(x,x?)
= ?2f exp(?12F?i=1xi ?
x?ili)where F is the number of features, ?2f is the co-variance magnitude and li > 0 are the featurelength scales.The resulting model hyperparameters (SE vari-ance ?2f , noise variance ?2n and SE length scales li)were learned from data by maximising the modellikelihood.
In general, the likelihood function isnon-convex and the optimisation procedure maylead to local optima.
To avoid poor hyperparam-eter values due to this, we performed a two-stepprocedure where we first optimise a model with allthe SE length scales tied to the same value (whichis equivalent to an isotropic model) and we usedthe resulting values as starting point for the ARDoptimisation.All our models were trained using the GPy2toolkit, an open source implementation of GPswritten in Python.3.1 Feature SelectionTo perform feature selection, we followed the ap-proach used in Shah et al(2013) and ranked thefeatures according to their learned length scales(from the lowest to the highest).
The length scalesof a feature can be interpreted as the relevance ofsuch feature for the model.
Therefore, the out-come of a GP model using an ARD kernel can beviewed as a list of features ranked by relevance,and this information can be used for feature selec-tion by discarding the lowest ranked (least useful)ones.For task 1.1, we performed this feature selectionover all 160 features mentioned in Section 2.
Fortask 1.3, we used a subset of the 80 most generalBB features as in (Shah et al 2013), for which wehad all the necessary resources available for theextraction.
We selected the top 25 features for bothmodels, based on empirical results found by Shahet al(2013) for a number of datasets, and thenretrained the GP using only the selected features.4 Active LearningActive Learning (AL) is a machine learningparadigm that let the learner decide which data itwants to learn from (Settles, 2010).
The main goalof AL is to reduce the size of the dataset whilekeeping similar model performance (therefore re-ducing annotation costs).
In previous work with17 baseline features, we have shown that with only?30% of instances it is possible to achieve 99%of the full dataset performance in the case of theWMT12 QE dataset (Beck et al 2013).To investigate if a reduced dataset can achievecompetitive performance in a blind evaluation set-ting, we submitted an entry for both tasks 1.1 and1.3 composed of models trained on a subset of in-stances selected using AL, and paired with fea-ture selection.
Our AL procedure starts with amodel trained on a small number of randomly se-lected instances from the training set and then usesthis model to query the remaining instances in thetraining set (our query pool).
At every iteration,the model selects the more ?informative?
instance,asks an oracle for its true label (which in our caseis already given in the dataset, and therefore we2http://sheffieldml.github.io/GPy/339only simulate AL) and then adds it to the trainingset.
Our procedure started with 50 instances fortask 1.1 and 20 instances for task 1.3, given its re-duced training set size.
We optimised the GaussianProcess hyperparameters every 20 new instances,for both tasks.As a measure of informativeness we used Infor-mation Density (ID) (Settles and Craven, 2008).This measure leverages between the varianceamong instances and how dense the region (in thefeature space) where the instance is located is:ID(x) = V ar(y|x)?
(1UU?u=1sim(x,x(u)))?The ?
parameter controls the relative impor-tance of the density term.
In our experiments, weset it to 1, giving equal weights to variance anddensity.
The U term is the number of instancesin the query pool.
The variance values V ar(y|x)are given by the GP prediction while the similar-ity measure sim(x,x(u)) is defined as the cosinedistance between the feature vectors.In a real annotation setting, it is important todecide when to stop adding new instances to thetraining set.
In this work, we used the confidencemethod proposed by Vlachos (2008).
This is anmethod that measures the model?s confidence ona held-out non-annotated dataset every time a newinstance is added to the training set and stops theAL procedure when this confidence starts to drop.In our experiments, we used the average test setvariance as the confidence measure.In his work, Vlachos (2008) showed a correla-tion between the confidence and test error, whichmotivates its use as a stop criterion.
To check ifthis correlation also occurs in our task, we measurethe confidence and test set error for task 1.1 usingthe WMT12 split (1832/422 instances).
However,we observed a different behaviour in our experi-ments: Figure 1 shows that the confidence doesnot raise or drop according to the test error but itstabilises around a fixed value at the same point asthe test error also stabilises.
Therefore, instead ofusing the confidence drop as a stop criterion, weuse the point where the confidence stabilises.
InFigure 2 we can observe that the confidence curvefor the WMT13 test set stabilises after ?580 in-stances.
We took that point as our stop criterionand used the first 580 selected instances as the ALdataset.Figure 1: Test error and test confidence curvesfor HTER prediction (task 1.1) using the WMT12training and test sets.Figure 2: Test confidence for HTER prediction(task 1.1) using the official WMT13 training andtest sets.We repeated the experiment with task 1.3, mea-suring the relationship between test confidenceand error using a 700/100 instances split (shownon Figure 3).
For this task, the curves did not fol-low the same behaviour: the confidence do notseem to stabilise at any point in the AL proce-dure.
The same occurred when using the officialtraining and test sets (shown on Figure 4).
How-ever, the MAE curve is quite flat, stabilising afterabout 100 sentences.
This may simply be a conse-quence of the fact that our model is too simple forpost-editing time prediction.
Nevertheless, in or-der to analyse the performance of AL for this taskwe submitted an entry using the first 100 instanceschosen by the AL procedure for training.The observed peaks in the confidence curves re-340Task 1.1 - Ranking Task 1.1 - Scoring Task 1.3DeltaAvg ?
Spearman ?
MAE ?
RMSE ?
MAE ?
RMSE ?SHEF-Lite-FULL 9.76 0.57 12.42 15.74 55.91 103.11SHEF-Lite-AL 8.85 0.50 13.02 17.03 64.62 99.09Baseline 8.52 0.46 14.81 18.22 51.93 93.36Table 1: Submission results for tasks 1.1 and 1.3.
The bold value shows a winning entry in the sharedtask.Figure 3: Test error and test confidence curvesfor post-editing time prediction (task 1.3) using a700/100 split on the WMT13 training set.Figure 4: Test confidence for post-editing timeprediction (task 1.3) using the official WMT13training and test sets.sult from steps where the hyperparameter optimi-sation got stuck at bad local optima.
These de-generated results set the variances (?2f , ?2n) to veryhigh values, resulting in a model that considers alldata as pure noise.
Since this behaviour tends todisappear as more instances are added to the train-ing set, we believe that increasing the dataset sizehelps to tackle this problem.
We plan to investi-gate this issue in more depth in future work.For both AL datasets we repeated the feature se-lection procedure explained in Section 3.1, retrain-ing the models on the selected features.5 ResultsTable 1 shows the results for both tasks.
SHEF-Lite-FULL represents GP models trained on thefull dataset (relative to each task) with a featureselection step.
SHEF-Lite-AL corresponds to thesame models trained on datasets obtained fromeach active learning procedure and followed byfeature selection.For task 1.1, our submission SHEF-Lite-FULLwas the winning system in the scoring subtask, andranked third in the ranking subtask.
These resultsshow that GP models achieve the state of the artperformance in QE.
These are particularly positiveresults considering that there is room for improve-ment in the feature selection procedure to identifythe optimal number of features to be selected.
Re-sults for task 1.3 were below the baseline, onceagain evidencing the fact that the noise model usedin our experiments is probably too simple for post-editing time prediction.
Post-editing time is gener-ally more prone to large variations and noise thanHTER, especially when annotations are producedby multiple post-editors.
Therefore we believe thatkernels that encode more advanced noise models(such as the multi-task kernel used by Cohn andSpecia (2013)) should be used for better perfor-mance.
Another possible reason for that is thesmaller set of features used for this task (black-box features only).Our SHEF-Lite-AL submissions performed bet-ter than the baseline in both scoring and rankingin task 1.1, ranking 2nd place in the scoring sub-task.
Considering that the dataset is composed byonly ?25% of the full training set, these are veryencouraging results in terms of reducing data an-341notation needs.
We note however that these resultsare below those obtained with the full training set,but Figure 1 shows that it is possible to achievethe same or even better results with an AL dataset.Since the curves shown in Figure 1 were obtainedusing the full feature set, we believe that advancedfeature selection strategies can help AL datasets toachieve better results.6 ConclusionsThe results obtained by our submissions confirmthe potential of Gaussian Processes to become thestate of the art approach for Quality Estimation.Our models were able to achieve the best perfor-mance in predicting HTER.
They also offer the ad-vantage of inferring a probability distribution foreach prediction.
These distributions provide richerinformation (like variance values) that can be use-ful, for example, in active learning settings.In the future, we plan to further investigate thesemodels by devising more advanced kernels andfeature selection methods.
Specifically, we wantto employ our feature set in a multi-task kernel set-ting, similar to the one proposed by Cohn and Spe-cia (2013).
These kernels have the power to modelinter-annotator variance and noise, which can leadto better results in the prediction of post-editingtime.We also plan to pursue better active learningprocedures by investigating query methods specif-ically tailored for QE, as well as a better stop cri-teria.
Our goal is to be able to reduce the datasetsize significantly without hurting the performanceof the model.
This is specially interesting in thecase of QE, since it is a very task-specific problemthat may demand a large annotation effort.AcknowledgmentsThis work was supported by funding fromCNPq/Brazil (No.
237999/2012-9, Daniel Beck)and from the EU FP7-ICT QTLaunchPad project(No.
296347, Kashif Shah and Lucia Specia).ReferencesDaniel Beck, Lucia Specia, and Trevor Cohn.
2013.Reducing Annotation Effort for Quality Estimationvia Active Learning.
In Proceedings of ACL (to ap-pear).John Blatz, Erin Fitzgerald, and George Foster.
2004.Confidence estimation for machine translation.
InProceedings of the 20th Conference on Computa-tional Linguistics, pages 315?321.Chris Callison-burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of 7th Workshopon Statistical Machine Translation.Trevor Cohn and Lucia Specia.
2013.
ModellingAnnotator Bias with Multi-task Gaussian Processes:An Application to Machine Translation Quality Es-timation.
In Proceedings of ACL (to appear).Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 177?180.Carl Edward Rasmussen and Christopher K. I.Williams.
2006.
Gaussian processes for machinelearning, volume 1.
MIT Press Cambridge.Burr Settles and Mark Craven.
2008.
An analysisof active learning strategies for sequence labelingtasks.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages1070?1079.Burr Settles.
2010.
Active learning literature survey.Technical report.Kashif Shah, Trevor Cohn, and Lucia Specia.
2013.An Investigation on the Effectiveness of Features forTranslation Quality Estimation.
In Proceedings ofMT Summit XIV (to appear).Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Transla-tion in the Americas.Lucia Specia, Craig Saunders, Marco Turchi, ZhuoranWang, and John Shawe-Taylor.
2009.
Improvingthe confidence of machine translation quality esti-mates.
In Proceedings of MT Summit XII.Lucia Specia, Kashif Shah, Jose?
G. C. De Souza, andTrevor Cohn.
2013.
QuEst - A translation qual-ity estimation framework.
In Proceedings of ACLDemo Session (to appear).Andreas Vlachos.
2008.
A stopping criterion foractive learning.
Computer Speech & Language,22(3):295?312, July.342
