Proceedings of the 7th Workshop on Statistical Machine Translation, pages 127?132,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsThe UPC Submission to the WMT 2012 Shared Task on Quality EstimationDaniele Pighin Meritxell Gonza?lez Llu?
?s Ma`rquezUniversitat Polite`cnica de Catalunya, Barcelona{pighin,mgonzalez,lluism}@lsi.upc.eduAbstractIn this paper, we describe the UPC system thatparticipated in the WMT 2012 shared task onQuality Estimation for Machine Translation.Based on the empirical evidence that fluency-related features have a very high correlationwith post-editing effort, we present a set offeatures for the assessment of quality estima-tion for machine translation designed arounddifferent kinds of n-gram language models,plus another set of features that model thequality of dependency parses automaticallyprojected from source sentences to transla-tions.
We document the results obtained onthe shared task dataset, obtained by combiningthe features that we designed with the baselinefeatures provided by the task organizers.1 IntroductionQuality Estimation (QE) for Machine Translations(MT) is the task concerned with the prediction of thequality of automatic translations in the absence ofreference translations.
The WMT 2012 shared taskon QE for MT (Callison-Burch et al, 2012) requiredparticipants to score and rank a set of automaticEnglish to Spanish translations output by a state-of-the-art phrase based machine translation system.Task organizers provided a training dataset of 1, 832source sentences, together with reference, automaticand post-edited translations, as well as human qual-ity assessments for the automatic translations.
Post-editing effort, i.e., the amount of editing required toproduce an accurate translation, was selected as thequality criterion, with assessments ranging from 1(extremely bad) to 5 (good as it is).
The organizersalso provided a set of linguistic resources and pro-cessors to extract 17 global indicators of translationquality (baseline features) that participants could de-cide to employ for their models.
For the evaluation,these features are used to learn a baseline predictorsfor participants to compare against.
Systems partic-ipating in the evaluation are scored based on theirability to correctly rank the 422 test translations (us-ing DeltaAvg and Spearman correlation) and/or topredict the human quality assessment for each trans-lation (using Mean Average Error - MAE and RootMean Squared Error - RMSE).Our initial approach to the task consisted of sev-eral experiments in which we tried to identify com-mon translation errors and correlate them with qual-ity assessments.
However, we soon realized thatsimple regression models estimated on the baselinefeatures resulted in more consistent predictors oftranslation quality.
For this reason, we eventuallydecided to focus on the design of a set of global in-dicators of translation quality to be combined withthe strong features already computed by the baselinesystem.An analysis of the Pearson correlation of thebaseline features (Callison-Burch et al, 2012)1with human quality assessments shows that the twostrongest individual predictors of post-editing ef-fort are the n-gram language model perplexities es-timated on source and target sentences.
This ev-idence suggests that a reasonable approach to im-1Baseline features are also described in http://www.statmt.org/wmt12/quality-estimation-task.html.127Feature Pearson |r| Feature Pearson |r|BL/4 0.3618 DEP/C+/Q4/R 0.0749BL/5 0.3544 BL/13 0.0741BL/12 0.2823 DEP/C?/Q1/W 0.0726BL/14 0.2675 DEP/C+/Q4/W 0.0718BL/2 0.2667 DEP/C+/Q34/R 0.0687BL/1 0.2620 BL/3 0.0623BL/8 0.2575 DEP/C+/Q34/W 0.0573BL/6 0.2143 SEQ/sys-ref/W 0.0495DEP/C?/S 0.2072 SEQ/sys/W 0.0492BL/10 0.2033 SEQ/ref-sys/W 0.0390DEP/C?/Q12/S 0.1858 BL/7 0.0351BL/17 0.1824 SEQ/sys/SStop 0.0312BL/16 0.1725 SEQ/sys/RStop 0.0301DEP/C?/W 0.1584 SEQ/sys-ref/SStop 0.0291DEP/C?/R 0.1559 SEQ/sys-ref/RStop 0.0289DEP/C?/Q12/R 0.1447 DEP/Coverage/S 0.0286DEP/Coverage/W 0.1419 SEQ/ref-sys/S 0.0232DEP/C?/Q1/S 0.1413 SEQ/ref-sys/R 0.0205BL/15 0.1368 SEQ/ref-sys/RStop 0.0187DEP/C+/Q4/S 0.1257 SEQ/sys-ref/R 0.0184DEP/Coverage/R 0.1239 SEQ/sys/R 0.0177SEQ/ref-sys/PStop 0.1181 SEQ/ref-sys/Chains 0.0125SEQ/sys/PStop 0.1173 SEQ/ref-sys/SStop 0.0104SEQ/sys-ref/PStop 0.1170 SEQ/sys/S 0.0053DEP/C?/Q12/W 0.1159 SEQ/sys-ref/S 0.0051DEP/C?/Q1/R 0.1113 SEQ/sys/Chains 0.0032DEP/C+/Q34/S 0.0933 SEQ/sys-ref/Chains 0.0014BL/9 0.0889 BL/11 0.0001Table 1: Pearson correlation (in absolute value) of thebaseline (BL) features and the extended feature set (SEQand DEP) with the quality assessments.prove the accuracy of the baseline would be to con-centrate on the estimation of other n-gram languagemodels, possibly working at different levels of lin-guistic analysis and combining information comingfrom the source and the target sentence.
On top ofthat, we add another class of features that capturethe quality of grammatical dependencies projectedfrom source to target via automatic alignments, asthey could provide clues about translation qualitythat may not be captured by sequential models.The novel features that we incorporate are de-scribed in full detail in the next section; in Sec-tion 3 we describe the experimental setup and theresources that we employ, while in Section 4 wepresent the results of the evaluation; finally, in Sec-tion 5 we draw our conclusions.2 Extended features setWe extend the set of 17 baseline features with 35new features:SEQ: 21 features based on n-gram language mod-els estimated on reference and automatic trans-lations, combining lexical elements of the tar-get sentence and linguistic annotations (POS)automatically projected from the source;DEP: 18 features that estimate a language modelon dependency parse trees automatically pro-jected from source to target via unsupervisedalignments.All the related models are estimated on a cor-pus of 150K newswire sentences collected from thetraining/development corpora of previous WMT edi-tions (Callison-Burch et al, 2007; Callison-Burch etal., 2011).
We selected this resource because we pre-fer to estimate the models only on in-domain data.The models for SEQ features are computed basedon reference translations (ref ) and automatic trans-lations generated by the same Moses (Koehn et al,2007) configuration used by the organizers of thisQE task.
As features, we encode the perplexity ofobserved sequences with respect to the two models,or the ratio of these values.
For DEP features, we es-timate a model that explicitly captures the differencebetween reference and automatic translations for thesame sentence.2.1 Sequential features (SEQ)The simplest sequential models that we estimateare 3-gram language models2 on the following se-quences:W: (Word), the sequence of words as they appearin the target sentence;R: (Root), the sequence of the roots of the words inthe target;S: (Suffix) the sequence of the suffixes of the wordsin the target;As features, for each automatic translation we en-code:?
The perplexity of the corresponding sequenceaccording to automatic (sys) translations: for2We also considered using longer histories, i.e., 5-grams, butsince we could not observe any noticeable difference we finallyselected the least over-fitting alternative.128example, SEQ/sys/R and SEQ/sys/W are theroot-sequence and word-sequence perplexitiesestimated on the corpus of automatic transla-tions;?
The ratio between the perplexities accordingthe two sets of translations: for example,SEQ/ref-sys/S is the ratio between the perplex-ity of suffix-sequences on reference and auto-matic translations, and SEQ/sys-ref/S is its in-verse.3We also estimate 3-gram language models onthree variants of a sequence in which non-stop words(i.e., all words belonging to an open class) are re-placed with either:RStop: the root of the word;SStop: the suffix of the word;PStop: the POS of the aligned source word(s).This last model (PStop) is the only one that requiressource/target pairs in order to be estimated.
If thetarget word is aligned to more than one word, weuse the ordered concatenation of the source wordsPOS tags; if the word cannot be aligned, we replaceit with the placeholder ?
*?, e.g.
: ?el NN de * VBZJJ en muchos NNS .?.
Also in this case, differentfeatures encode the perplexity with respect to au-tomatic translations (e.g., SEQ/sys/PStop) or to theratio between automatic and reference translations(e.g., SEQ/ref-sys/RStop).Finally, a last class of sequences (Chains) col-lapses adjacent stop words into a single token.Content-words or isolated stop-words are not in-cluded in the sequence, e.g: ?mediante la de losde la y de las y la a los?.
Again, we considerthe same set of variants, e.g.
SEQ/sys/Chains orSEQ/sys-ref/Chains.Since there are 7 sequence types and 3 combinations(sys, sys-ref, ref-sys) we end up with 21 new fea-tures.3Features extracted solely from reference translations havebeen considered, but they were dropped during developmentsince we could not observe a noticeable effect on predictionquality.2.2 Dependency features (DEP)These features are based on the assumption thatby observing how dependency parses are projectedfrom source to target we can gather clues concern-ing translation quality that cannot be captured by se-quential models.
The features encode the extent towhich the edges of the projected dependency tree areobserved in reference-quality translations.The model for DEP features is estimated onthe same set of 150K English sentences and thecorresponding reference and automatic translations,based on the following algorithm:1.
Initialize two maps M+ and M?
to store edgecounts;2.
Then, for each source sentence s: parse s witha dependency parser;3.
Align the words of s with the reference and theautomatic translations r and a;4.
For each dependency relation ?d, sh, sm?
ob-served in the source, where d is the relationtype and sh and sm are the head and modifierwords, respectively:(a) Identify the aligned head/modifier wordsin r and a, i.e., ?rh, rm?
and ?ah, am?
;(b) If rh = ah and rm = am, then incre-ment M+?d,ah,am?
by one, otherwise incre-ment M?
?d,ah,am?.In other terms, M+ keeps track of how many timesa projected dependency is the same in the automaticand in the reference translation, while M?
accountsfor the cases in which the two projections differ.Let T be the set of dependency relations projectedon an automatic translation.
In the feature space werepresent:Coverage: The ratio of dependency edges found inM?
or M+ over the total number of projectededges, i.e.Coverage(T ) =?D?T M+D +M?D|T |;C+: The quantity C+ = 1|T |?D?TM+DM+D?M?D;129C?
: The quantity C?
= 1|T |?D?TM?DM+D?M?D.Intuitively, high values of C+ mean that most pro-jected dependencies have been observed in referencetranslations; conversely, high values of C?
suggestthat most of the projected dependencies were onlyobserved in automatic translations.Similarly to SEQ features, also in this case we ac-tually employ three variants of these features: one inwhich we use word forms (i.e., DEP/Coverage/W,DEP/C+/W and DEP/C?/W), one in which welook at roots (i.e., DEP/Coverage/R, DEP/C+/Rand DEP/C?/R) and one in which we only con-sider suffixes (i.e., DEP/Coverage/S, DEP/C+/S andDEP/C?/S).Moreover, we also estimate C+ in the top (Q4)and top two (Q34) fourths of edge scores, and C?
inthe bottom (Q1) and bottom two (Q12) fourths.
Asan example, the feature DEP/C+/Q4/R encodes thevalue of C+ within the top fourth of the ranked list ofprojected dependencies when only considering wordroots, while DEP/C?/W is the value of C?
on thewhole edge set estimated using word forms.3 Experiment setupTo extract the extended feature set we use an align-ment model, a POS tagger and a dependency parser.Concerning the former, we trained an unsupervisedmodel with the Berkeley aligner4, an implementa-tion of the symmetric word-alignment model de-scribed by Liang et al (2006).
The model is trainedon Europarl and newswire data released as part ofWMT 2011 (Callison-Burch et al, 2011) trainingdata.
For POS tagging and semantic role annota-tion we use SVMTool5 (Jesu?s Gime?nez and Llu?
?sMa`rquez, 2004) and Swirl6 (Surdeanu and Turmo,2005), respectively, with default configurations.
Toestimate the SEQ and DEP features we use refer-ence and automatic translations of the newswire sec-tion of WMT 2011 training data.
The automatictranslations are generated by the same configura-tion generating the data for the quality estimationtask.
The n-gram models are estimated with the4http://code.google.com/p/berkeleyaligner5http://www.lsi.upc.edu/?nlp/SVMTool/6http://www.surdeanu.name/mihai/swirl/Feature set DeltaAvg MAEBaseline 0.4664 0.6346Extended 0.4694 0.6248Table 2: Comparison of the baseline and extended featureset on development data.SRILM toolkit 7, with order equal to 3 and Kneser-Ney (Kneser and Ney, 1995) smoothing.As a learning framework we resort to SupportVector Regression (SVR) (Smola and Scho?lkopf,2004) and learn a linear separator using the SVM-Light optimizer by Joachims (1999)8.
We representfeature values by means of their z-scores, i.e., thenumber of standard deviations that separate a valuefrom the average of the feature distribution.
Wecarry out the system development via 5-fold crossevaluation on the 1,832 development sentences forwhich we have quality assessments.4 EvaluationIn Table 1 we show the absolute value of the Pear-son correlation of the features used in our model,i.e., the 17 baseline features (BL/*), the 21 sequence(SEQ/*) and the 18 dependency (DEP/*) features,with the human quality assessments.
The more cor-related features are in the top (left) part of the ta-ble.
At a first glance, we can see that 9 of the 10features having highest correlation are already en-coded by the baseline.
We can also observe thatDEP features show a higher correlation than SEQfeatures.
This evidence seems to contradict our ini-tial expectations, but it can be easily ascribed to thelimited size of the corpus used to estimate the n-gram models (150K sentences).
This point is alsoconfirmed by the fact that the three variants of the*PStop model (based on sequences of target stop-words interleaved by POS tags projected from thesource sentence and, hence, on a very small vocab-ulary) are the three sequential models sporting thehighest correlation.
Alas, the lack of lexical anchorsmakes them less useful as predictors of translationquality than BL/4 and BL/5.
Another interesting as-7http://www-speech.sri.com/projects/srilm8http://svmlight.joachims.org/130System DeltaAvg MAEBaseline 0.55 0.69Official Evaluation 0.22 0.84Amended Evaluation 0.51 0.71Table 3: Official and amended evaluation on test data ofthe extended feature sets.pect is that DEP/C?
features show higher correlationthan DEP/C+.
This is an expected behaviour, as be-ing indicators of possible errors they are intended tohave discriminative power with respect to the humanassessments.
Finally, we can see that more than 50%of the included features, including five baseline fea-tures, have negligible (less than 0.1) correlation withthe assessments.
Even though these features may nothave predictive power per se, their combination maybe useful to learn more accurate models of quality.9Table 2 shows a comparison of the baseline fea-tures against the extended feature set as the averageDeltaAvg score and Mean Absolute Error (MAE) onthe 10 most accurate development configurations.
Inboth cases, the extended feature set results in slightlymore accurate models, even though the improve-ment is hardly significant.Table 3 shows the results of the official evaluation.Our submission to the final evaluation (Official) wasplagued by a bug that affected the values of all thebaseline features on the test set.
As a consequence,the official performance of the model is extremelypoor.
The row labeled Amended shows the resultsthat we obtained after correcting the problem.
As wecan see, on both tasks the baseline outperforms ourmodel, even though the difference between the twois only marginal.
Ranking-wise, our official submis-sion is last on the ranking task and last-but-one onthe quality prediction task.
In contrast, the amendedmodel shows very similar accuracy to the baseline,as the majority of the systems that took part in theevaluation.9Our experiments on development data were not signifi-cantly affected by the presence or removal of low-correlationfeatures.
Given the relatively small feature space, we adopteda conservative strategy and included all the features in the finalmodels.5 Discussion and conclusionsWe have described the system with which we par-ticipated in the WMT 2012 shared task on qualityestimation.
The model incorporates all the base-line features, plus two sets of novel features basedon: 1) n-gram language models estimated on mixedsequences of target sentence words and linguisticannotations projected from the source sentence bymeans of automatic alignments; and 2) the likeli-hood of the projection of dependency relations fromsource to target.On development data we found out that the ex-tended feature set granted only a very marginal im-provement with respect to the strong feature set ofthe baseline.
In the official evaluation, our submis-sion was plagued by a bug affecting the generationof baseline features for the test set, and as a resultwe had an incredibly low performance.
After fix-ing the bug, re-evaluating on the test set confirmedthat the extended set of features, at least in the cur-rent implementation, does not have the potential tosignificantly improve over the baseline features.
Onthe contrary, the accuracy of the corrected model isslightly lower than the baseline on both the rankingand the quality estimation task.During system development it was clear that im-proving significantly over the results of the base-line features would be very difficult.
In our expe-rience, this is especially due to the presence amongthe baseline features of extremely strong predictorsof translation quality such as the perplexity of theautomatic translation.
We could also observe thatthe parametrization of the learning algorithm hada much stronger impact on the final accuracy thanthe inclusion/exclusion of specific features from themodel.We believe that the information that we encode,and in particular dependency parses and stop-wordsequences, has the potential to be quite relevant forthis task.
On the other hand, it may be necessary toestimate the models on much larger datasets in orderto compensate for their inherent sparsity.
Further-more, more refined methods may be required in or-der to incorporate the relevant information in a moredeterminant way.131AcknowledgmentsThis research has been partially funded bythe Spanish Ministry of Education and Science(OpenMT-2, TIN2009-14675-C03) and the Euro-pean Community?s Seventh Framework Programme(FP7/2007-2013) under grant agreement numbers247762 (FAUST project, FP7-ICT-2009-4-247762)and 247914 (MOLTO project, FP7-ICT-2009-4-247914).References[Callison-Burch et al2007] Chris Callison-Burch,Philipp Koehn, Cameron Shaw Fordyce, and ChristofMonz, editors.
2007.
Proceedings of the SecondWorkshop on Statistical Machine Translation.
ACL,Prague, Czech Republic.
[Callison-Burch et al2011] Chris Callison-Burch,Philipp Koehn, Christof Monz, and Omar F. Zaidan,editors.
2011.
Proceedings of the Sixth Workshopon Statistical Machine Translation.
Association forComputational Linguistics, Edinburgh, Scotland, July.
[Callison-Burch et al2012] Chris Callison-Burch,Philipp Koehn, Christof Monz, Matt Post, RaduSoricut, and Lucia Specia.
2012.
Findings of the2012 workshop on statistical machine translation.In Proceedings of the Seventh Workshop on Statis-tical Machine Translation, Montreal, Canada, June.Association for Computational Linguistics.
[Jesu?s Gime?nez and Llu?
?s Ma`rquez2004] Jesu?s Gime?nezand Llu?
?s Ma`rquez.
2004.
SVMTool: A general POStagger generator based on Support Vector Machines.In Proceedings of the 4th LREC.
[Joachims1999] Thorsten Joachims.
1999.
Making large-Scale SVM Learning Practical.
In B. Scho?lkopf,C.
Burges, and A. Smola, editors, Advances in KernelMethods - Support Vector Learning.
[Kneser and Ney1995] Reinhard Kneser and HermannNey.
1995.
Improved backing-off for m-gram lan-guage modeling.
In In Proceedings of the IEEE Inter-national Conference on Acoustics, Speech and SignalProcessing, volume I, pages 181?184, Detroit, Michi-gan, May.
[Koehn et al2007] Philipp Koehn, Hieu Hoang, Alexan-dra Birch, Chris Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-jar, Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational LinguisticsCompanion Volume Proceedings of the Demo andPoster Sessions, pages 177?180, Prague, Czech Re-public, June.
Association for Computational Linguis-tics.
[Liang et al2006] Percy Liang, Benjamin Taskar, andDan Klein.
2006.
Alignment by agreement.
In HLT-NAACL.
[Smola and Scho?lkopf2004] Alex J. Smola and BernhardScho?lkopf.
2004.
A tutorial on support vector regres-sion.
Statistics and Computing, 14(3):199?222, Au-gust.
[Surdeanu and Turmo2005] Mihai Surdeanu and JordiTurmo.
2005.
Semantic Role Labeling Using Com-plete Syntactic Analysis.
In Proceedings of theNinth Conference on Computational Natural Lan-guage Learning (CoNLL-2005), pages 221?224, AnnArbor, Michigan, June.132
