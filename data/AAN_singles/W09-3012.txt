Committed Belief Annotation and TaggingMona T. Diab Lori LevinCCLS LTIColumbia U. CMUmdiab@cs.columbia.edu lsl@cs.cmu.eduTeruko Mitamura Owen RambowLTI CCLSCMU Columbia U.teruko+@cs.cmu.edu rambow@ccls.columbia.eduVinodkumar Prabhakaran Weiwei GuoCS CSColumbia U. Columbia U.AbstractWe present a preliminary pilot study ofbelief annotation and automatic tagging.Our objective is to explore semantic mean-ing beyond surface propositions.
We aimto model people?s cognitive states, namelytheir beliefs as expressed through linguis-tic means.
We model the strength of theirbeliefs and their (the human) degree ofcommitment to their utterance.
We ex-plore only the perspective of the author ofa text.
We classify predicates into one ofthree possibilities: committed belief, noncommitted belief, or not applicable.
Weproceed to manually annotate data to thatend, then we build a supervised frame-work to test the feasibility of automati-cally predicting these belief states.
Eventhough the data is relatively small, weshow that automatic prediction of a beliefclass is a feasible task.
Using syntacticfeatures, we are able to obtain significantimprovements over a simple baseline of23% F-measure absolute points.
The bestperforming automatic tagging condition iswhere we use POS tag, word type fea-ture AlphaNumeric, and shallow syntac-tic chunk information CHUNK.
Our bestoverall performance is 53.97% F-measure.1 IntroductionAs access to large amounts of textual informa-tion increases, there is a strong realization thatsearches and processing purely based on surfacewords is highly limiting.
Researchers in infor-mation retrieval and natural language processing(NLP) have long used morphological and (in amore limited way) syntactic analysis to improveaccess and processing of text; recently, interest hasgrown in relating text to more abstract representa-tions of its propositional meaning, as witnessed bywork on semantic role labeling, word sense disam-biguation, and textual entailment.
However, thereare more levels to ?meaning?
than just proposi-tional content.
Consider the following examples,and suppose we find these sentences in the NewYork Times:1(1) a. GM will lay off workers.b.
A spokesman for GM said GM will lay offworkers.c.
GM may lay off workers.d.
The politician claimed that GM will layoff workers.e.
Some wish GM would lay of workers.f.
Will GM lay off workers?g.
Many wonder if GM will lay off workers.If we are searching text to find out whether GMwill lay off workers, all of the sentences in (1) con-1In this paper, we concentrate on written communication,and we use the terms reader and writer.
However, nothing inthe approach precludes applying it to spoken communication.tain the proposition LAYOFF(GM,WORKERS).However, the six sentences clearly allow us verydifferent inferences about whether GM will lay offworkers or not.
Supposing we consider the Timesa trustworthy news source, we would be fairly cer-tain with (1a) and (1b).
(1c) suggests the Times isnot certain about the layoffs, but considers thempossible.
When reading (1d), we know that some-one else thinks that GM will lay off workers, butthat the Times does not necessarily share this be-lief.
(1e), (1f), and (1g) do not tell us anythingabout whether anyone believes whether GM willlay off workers.In order to tease apart what is happening, weneed to refine a simple IR-ish view of text as arepository of propositions about the world.
We usetwo theories to aid us.
The first theory is that in ad-dition to facts about the world (GM will or will notlay off workers), we have facts about people?s cog-nitive states, and these cognitive states relate theirbearer to the facts in the world.
(Though perhapsthere are only cognitive states, and no facts aboutthe world.)
Following the literature in ArtificialIntelligence (Cohen and Levesque, 1990), we canmodel cognitive state as beliefs, desires, and inten-tions.
In this paper, we are only interested in be-liefs (and in distinguishing them from desires andintentions).
The second theory is that communi-cation is intention-driven, and understanding textactually means understanding the communicativeintention of the writer.
Furthermore, communica-tive intentions are intentions to affect the reader?scognitive state ?
his or her beliefs, desires, and/orintentions.
This view has been worked out in thetext generation and dialog community more thanin the text understanding community (Mann andThompson, 1987; Hovy, 1993; Moore, 1994).In this paper we are interested in exploring thefollowing: we would like to recognize what thetext wants to make us believe about various peo-ple?s cognitive states, including the speaker?s.
Asmentioned, we are only interested in people?s be-lief.
In this view, the result of text processing isnot a list of facts about the world, but a list of factsabout different people?s cognitive states.This paper is part of an on-going research effort.The goals of this paper are to summarize a pilotannotation effort, and to present the results of ini-tial experiments in automatically extracting factsabout people?s beliefs from open domain runningtext.2 Belief AnnotationWe have developed a manual for annotating be-lief, which we summarize here.
For more de-tailed information, we refer to the cited works.
Ingeneral, we are interested in the writer?s intentionas to making us believe that various people havecertain beliefs, desires, and intentions.
We sim-plify the annotation in two ways: we are only in-teretsed in beliefs, and we are only interested inthe writer?s beliefs.
This is not because we thinkthis is the only interesting information in text, butwe do this in order to obtain a manageable anno-tation in our pilot study.
Specifically, we annotatewhether the writer intends the reader to interpreta stated proposition as the writer?s strongly heldbelief, as a proposition which the writer does notbelieve strongly (but could), or as a propositiontowards which the writer has an entirely differ-ent cognitive attitude, such as desire or intention.We do not annotate subjectivity (Janyce Wiebe andMartin, 2004; Wilson and Wiebe, 2005), nor opin-ion (for example: (Somasundaran et al, 2008)):the nature of the proposition (opinion and type ofopinion, statement about interior world, externalworld) is not of interest.
Thus, this work is or-thogonal to the extensive literature on opinion de-tection.
And we do not annotate truth: real-world(encyclopedic) truth is not relevant.We have three categories:?
Committed belief (CB): the writer indicatesin this utterance that he or she believes theproposition.
For example, GM has laid offworkers, or, even stronger, We know that GMhas laid off workers.A subcase of committed belief concernspropositions about the future, such as GMwill lay off workers.
People can have equallystrong beliefs about the future as about thepast, though in practice probably we havestronger beliefs about the past than about thefuture.?
Non-committed belief (NCB): the writeridentifies the propositon as something whichhe or she could believe, but he or she hap-pens not to have a strong belief in.
There aretwo subcases.
First, there are cases in whichthe writer makes clear that the belief is notstrong, for example by using a modal auxil-iary:2 GM may lay off workers.
Second, inreported speech, the writer is not signaling tous what he or she believes about the reportedspeech: The politician claimed that GM willlay off workers.
However, sometimes, we canuse the speech act verb to infer the writer?sattitude,3 and we can use our own knowledge2The annotators must distinguish epistemic and deonticuses of modals.3Some languages may also use grammatical devices; forto infer the writer?s beliefs; for example, inA GM spokesman said that GM will lay offworkers, we can assume that the writer be-lieves that GM intends to lay off workers, notjust the spokesman.
However, this is not partof the annotation, and all reported speech isannotated as NCB.
Again, the issue of tenseis orthogonal.?
Not applicable (NA): for the writer, theproposition is not of the type in which he orshe is expressing a belief, or could express abelief.
Usually, this is because the proposi-tion does not have a truth value in this world(be it in the past or in the future).
This coversexpressions of desire (Some wish GM wouldlay of workers), questions (Will GM lay offworkers?
or Many wonder if GM will layoff workers, and expressions of requirements(GM is required to lay off workers or Lay offworkers!
).This sort of annotation is part of an annotationof all ?modalities?
that a text may express.
Weonly annotate belief.
A further complication isthat these modalities can be nested: one can ex-press a belief about someone else?s belief, and onemay be strong and the other weak (I believe Johnmay believe that GM will lay off workers).
At thisphase, we only annotate from the perspective ofthe writer, i.e.
what the writer of the text that isbeing annotated believes.The annotation units (annotatables) are, con-ceptually, propositions as defined by PropBank(Kingsbury et al, 2002).
In practice, annotatorsare asked to identify full lexical verbs (whetherin main or embedded clauses, whether finite ornon-finite).
In predicative constructions (John is adoctor/in the kitchen/drunk), we ask them to iden-tify the nominal, prepositional, or adjectival headrather than the form of to be, in order to also han-dle small clauses (I think [John an idiot]).The interest of the annotation is clear: we wantto be able to determine automatically from a giventext what beliefs we can ascribe to the writer,and with what strengths he or she holds them.Across languages, many different linguistic meansare used to denote this attitude towards an utteredproposition, including syntax, lexicon, and mor-phology.
To our knowledge, no systematic empir-ical study exists for English, and this annotation isa step towards that goal.example, in German, the choice between indicative mood andsubjunctive mood in reported speech can signal the writer?sattitude.3 Related WorkThe work of Roser et al (2006) is, in many re-spects, very similar to ours.
In particular, they areconcerned with extracting information about peo-ple?s beliefs and the strength of these beliefs fromtext.
However, their annotation is very differentfrom ours.
They extend the TimeML annotationscheme to include annotation of markers of beliefand strength of belief.
For example, in the sen-tence The Human Rights Committee regretted thatdiscrimination against women persisted in prac-tice, TimeML identifies the events associated withthe verbs regret and persist, and then the extensionto the annotation adds the mark that there is a ?fac-tive?
link between the regret event and the persistevent, i.e., if we regret something, then we assumethe truth of that something.
In contrast, in ourannotation, we directly annotate events with theirlevel of belief.
In this example, we would annotatepersist as being a committed belief of the HumanRights Committee (though in this paper we onlyreport on beliefs attributed to the writer).
This dif-ference is important, as in the annotation of Roseret al (2006), the annotator must analyze the situ-ation and find evidence for the level of belief at-tributed to an event.
As a result, we cannot usethe annotation to discover how natural languageexpresses level of belief.
Our annotation is moreprimitively semantic: we ask the annotators sim-ply to annotate meaning (does X believe the eventtakes place), as opposed to annotating the linguis-tic structures which express meaning.
As a conse-quence of the difference in annotation, we cannotcompare our automatic prediction results to theirs.Other related works explored belief systems inan inference scenario as opposed to an intentional-ity scenario.
In work by (Ralf Krestel and Bergler,2007; Krestel et al, 2008), the authors explorebelief in the context of news media exploring re-ported speech where they track newspaper textlooking for elements indicating evidentiality.
Thenotion of belief is more akin to finding statementsthat support or negate specific events with differ-ent degrees of support.
This is different from ournotion of committed belief in this work, since weseek to make explicit the intention of the author orthe speaker.4 Our Approach4.1 DataWe create a relatively small corpus of Englishmanually annotated for the three categories: CB,NCB, NA.
The data covers different domains andgenres from newswire, to blog data, to email cor-respondence, to letter correspondence, to tran-scribed dialogue data.
The data comprises 10Kwords of running text.
70% of the data was dou-bly annotated comprising 6188 potentially anno-tatable tokens.
Hence we had a 4 way manual clas-sification in essence between NONE, CB, NCB,and NA.
Most of the confusions between NONEand CB from both annotators, for 103 tokens.The next point of disagreement was on NCB andNONE for 48 tokens.They disagreed on NCB andCB for 32 of the tokens.
In general the interanno-tator agreements were high as they agreed 95.8%of the time on the annotatable and the exact beliefclassification.4 Here is an example of a disagree-ment between the two annotators, The Iraqi gov-ernment has agreed to let Rep Tony Hall visit thecountry next week to assess a humanitarian cri-sis that has festered since the Gulf War of 1991Hall?s office said Monday.
One annotator deemed?agreed?
a CB while the other considered it anNCB.4.2 Automatic approachOnce we had the data manually annotated and re-vised, we wanted to explore the feasibility of au-tomatically predicting belief states based on lin-guistic features.
We apply a supervised learningframework to the problem of both identifying andclassifying a belief annotatable token in context.This is a three way classification task where anannotatable token is tagged as one of our threeclasses: Committed Belief (CB), Non CommittedBelief (NCB), and Not Applicable (NA).
We adopta chunking approach to the problem using an In-side Outside Beginning (IOB) tagging frameworkfor performing the identification and classificationof belief tokens in context.
For chunk tagging,we use YamCha sequence labeling system.5 Yam-Cha is based on SVM technology.
We use the de-fault parameter settings most importantly the ker-nels are polynomial degree 2 with a c value of 0.5.We label each sentence with standard IOB tags.Since this is a ternary classification task, we have7 different tags: B-CB (Beginning of a commit-ted belief chunk), I-CB (Inside of a committed be-lief chunk), B-NCB (Beginning of non commit-ted belief chunk), I-NCB (Inside of a non com-mitted belief chunk), B-NA (Beginning of a notapplicable chunk), I-NA (Inside a not applicablechunk), and O (Outside a chunk) for the casesthat are not annotatable tokens.
As an exampleof the annotation, a sentence such as Hall saidhe wanted to investigate reports from relief agen-cies that a quarter of Iraqi children may be suffer-4This interannotator agreement number includes theNONE category.5http://www.tado-chasen.com/yamchaing from chronic malnutrition.
will be annotatedas follows: {Hall O said B-CB he O wanted B-NCB to B-NA investigate I-NA reports O from Orelief O agencies O that O a O quarter O of OIraqi O children O may O be O suffering B-NCBfrom O chronic O malnutrition O.
}We experiment with some basic features andsome more linguistically motivated ones.CXT: Since we adopt a sequence labelingparadigm, we experiment with different windowsizes for context ranging from ?/+2 tokens afterand before the token of interest to ?/+5.NGRAM: This is a character n-gram feature,explicity representing the first and last characterngrams of a word.
In this case we experiment withup to ?/+4 characters of a token.
This featureallows us to capture implicitly the word inflectionmorphology.POS: An important feature is the Part-of-Speech(POS) tag of the words.
Most of the annotatablesare predicates but not all predicates in the text areannotatables.
We obtain the POS tags from theTreeTagger POS tagger tool which is trained onthe Penn Treebank.6ALPHANUM: This feature indicates whetherthe word has a digit in it or not or if it is a nonalphanumeric token.VerbType: We classify the verbs as to whetherthey are modals (eg.
may, might, shall, will,should, can, etc.
), auxilliaries (eg.
do, be, have),7or regular verbs.
Many of our annotatables occurin the vicinity of modals and auxilliaries.
The listof modals and auxilliaries is deterministic.Syntactic Chunk (CHUNK): This feature ex-plicitly models the syntactic phrases in which ourtokens occur.
The possible phrases are shallowsyntactic representations that we obtain from theTreeTagger chunker:8 ADJC (Adjective Chunk),ADVC (Adverbial Chunk), CONJC (Conjunc-tional Chunk), INTJ (Interjunctional Chunk), LST(numbers 1, 2,3 etc), NC (Noun Chunk), PC(Prepositional Chunk), PRT (off,out,up etc), VC(Verb Chunk).5 Experiments and Results5.1 ConditionsSince the data is very small, we tested our au-tomatic annotation using 5 fold cross validation6http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/7We realize in some of the grammar books auxilliariesinclude modal verbs.8http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/where 10% of the data is set aside as developmentdata, then 70% is used for training and 20% fortesting.
The reported results are averaged over the5 folds for the Test data for each of our experimen-tal conditions.Our baseline condition is using the tokenizedwords only with no other features (TOK).
We em-pirically establish that a context size of ?/+3yields the best results in the baseline condition asevaluated on the development data set.
Hence allthe results are yielded from a CXT of size 3.The next conditions present the impact ofadding a single feature at a time and then combin-ing them.
It is worth noting that the results reflectthe ability of the classifier to identify a token thatcould be annotatable and also classify it correctlyas one of the possible classes.5.2 Evaluation MetricsWe use F?=1 (F-measure) as the harmonic meanbetween (P)recision and (R)ecall.
All the pre-sented results are the F-measure.
We report theresults separately for the three classes CB, NCB,and NA as well as the overall global F measure forany one condition averaged over the 5 folds of theTEST data set.5.3 ResultsIn Table 1 we present the results yielded per con-dition including the baseline TOK and presentedfor the three different classes as well as the overallF-measure.All the results yielded by our experimentsoutperform the baseline TOK.
We highlightthe highest performing conditions in Ta-ble 1: TOK+AlphaNum+POS +CHUNK,TOK+AN+POS and TOK+POS.
Even thoughall the features independently outperform thebaseline TOK in isolation, POS is the single mostcontributing feature.
The least contributing factorindependently is the AlphaNumeric feature AN.However combining AN with character NgramNG yields better results than using each of themindependently.
We note that adding NG to anyother feature combination is not helpful, in factit seems to add noise rather than signal to thelearning process in the presence of more sophis-ticated features such as POS or syntactic chunkinformation.
Adding the verbtype VT explicitlyas a feature is not helpful for all categories, itseems most effective with CB.
As mentionedearlier we deterministically considered all modalverbs to be modal.
This might not be the casefor all modal auxilliaries since some of themare used epistemically while others deontically,hence our feature could be introducing an elementof noise.
Adding syntactic chunk informationhelps boost the results by a small margin from53.5 to 53.97 F-measure.
All the results seem tosuggest the domination of the POS feature and it?simportance for such a tagging problem.
In generalour performance on CB is the highest, followedby NA then we note that NCB is the hardestcategory to predict.
Examining the data, NCBhas the lowest number of occurrence instancesin this data set across the board in the wholedata set and accordingly in the training data,which might explain the very low performance.Also in our annotation effort, it was the hardestcategory to annotate since the annotation takesmore than the sentential context into account.Hence a typical CB verb such as ?believe?
in thescope of a reporting predicate such as ?say?
asin the following example Mary said he believedthe suspect with no qualms.
The verb believedshould be tagged NCB however in most cases itis tagged as a CB.
Our syntactic feature CHUNKhelps a little but it does not capture the overalldependencies in the structure.
We believe thatrepresenting deeper syntactic structure shouldhelp tremendously as it will model these relativelylonger dependencies.We also calculated a confusion matrix for thedifferent classes.
The majority of the errors areidentification errors where an annotatable is con-sidered an O class as opposed to one of the 3 rel-evant classes.
This suggests that identifying theannotatable words is a harder task than classifica-tion into one of the three classes, which is consis-tent with our observation from the interannotatordisagreements where most of their disagreementswere on the annotatable tokens, though a smalloverall number of tokens, 103 tokens out of 6188,it was the most significant disagreement category.We find that for the TOK+POS condition, CBs aremistagged as un-annotatable O 55% of the time.We find most of the confusions between NA andCB, and NCB and CB, both cases favoring a CBtag.6 ConclusionWe presented a preliminary pilot study of beliefannotation and automatic tagging.
Even thoughthe data is relatively tiny, we show that automaticprediction of a belief class is a feasible task.
Us-ing syntactic features, we are able to obtain signif-icant improvements over a simple baseline of 23%F-measure absolute points.
The best performingautomatic tagging condition is where we use POStag, word type feature AlphaNumeric, and shallowsyntactic chunk information CHUNK.
Our bestoverall performance is 53.97% F-measure.CB NA NCB Overall FTOK 25.12 41.18 13.64 30.3TOK+NG 33.18 42.29 5 34.25TOK+AN 30.43 44.57 12.24 33.92TOK+AN+NG 37.17 42.46 9.3 36.61TOK+POS 54.8 59.23 13.95 53.5TOK+NG+POS 43.15 50.5 22.73 44.35TOK+AN+POS 54.79 58.97 22.64 53.54TOK+NG+AN+POS 43.09 54.98 18.18 45.91TOK+POS+CHUNK 55.45 57.5 15.38 52.77TOK+POS+VT+CHUNK 53.74 57.14 14.29 51.43TOK+AN+POS+CHUNK 55.89 59.59 22.58 53.97TOK+AN+POS+VT+CHUNK 56.27 58.87 12.9 52.89Table 1: Final results averaged over 5 folds of test data using different features and their combinations:NG is NGRAM, AN is AlphaNumeric, VT is verbtypeIn the future we are looking at ways of addingmore sophisticated deep syntactic and semanticfeatures using lexical chains from discourse struc-ture.
We will also be exploring belief annotation inArabic and Urdu on a parallel data collection sincethese languages express evidentiality in ways thatdiffer linguistically from English.
Finally we willexplore ways of automatically augmenting the la-beled data pool using active learning.AcknowledgementThis work was supported by grants from the Hu-man Language Technology Center of Excellence.Any opinions, findings, and conclusions or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the viewsof the sponsor.ReferencesPhilip R. Cohen and Hector J. Levesque.
1990.
Ratio-nal interaction as the basis for communication.
InJerry Morgan Philip Cohen and James Allen, edi-tors, Intentions in Communication.
MIT Press.Eduard H. Hovy.
1993.
Automated discourse gener-ation using discourse structure relations.
ArtificialIntelligence, 63:341?385.Rebecca Bruce Matthew Bell Janyce Wiebe,Theresa Wilson and Melanie Martin.
2004.Learning subjective language.
In ComputationalLinguistics, Volume 30 (3).Paul Kingsbury, Martha Palmer, and Mitch Marcus.2002.
Adding semantic annotation to the Penn Tree-Bank.
In Proceedings of the Human Language Tech-nology Conference, San Diego, CA.Ralf Krestel, Sabine Bergler, and Rene?
Witte.
2008.Minding the Source: Automatic Tagging of Re-ported Speech in Newspaper Articles.
In EuropeanLanguage Resources Association (ELRA), editor,Proceedings of the Sixth International Language Re-sources and Evaluation (LREC 2008), Marrakech,Morocco, May 28?30.William C. Mann and Sandra A. Thompson.
1987.Rhetorical Structure Theory: A theory of text orga-nization.
Technical Report ISI/RS-87-190, ISI.Johanna Moore.
1994.
Participating in ExplanatoryDialogues.
MIT Press.Rene?
Witte Ralf Krestel and Sabine Bergler.
2007.Processing of Beliefs extracted from ReportedSpeech in Newspaper Articles.
In InternationalConference on Recent Advances in Natural Lan-guage Processing (RANLP 2007), Borovets, Bul-garia, September 27?29.Saur??
Roser, Marc Verhagen, and James Pustejovsky.2006.
Annotating and Recognizing Event Modalityin Text.
In FLAIRS 2006, editor, In Proceedingsof the 19th International FLAIRS Conference, Mel-bourne Beach, Florida, May 11-13.Swapna Somasundaran, Janyce Wiebe, and Josef Rup-penhofer.
2008.
Discourse level opinion interpre-tation.
In Proceedings of the 22nd InternationalConference on Computational Linguistics (Coling2008), pages 801?808, Manchester, UK, August.Coling 2008 Organizing Committee.Theresa Wilson and Janyce Wiebe.
2005.
Annotat-ing attributions and private states.
In Proceedings ofthe Workshop on Frontiers in Corpus Annotations II:Pie in the Sky, pages 53?60, Ann Arbor, Michigan,June.
Association for Computational Linguistics.
