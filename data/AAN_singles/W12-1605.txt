Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 40?49,Seoul, South Korea, 5-6 July 2012. c?2012 Association for Computational LinguisticsUnsupervised Topic Modeling Approaches to Decision Summarization inSpoken MeetingsLu WangDepartment of Computer ScienceCornell UniversityIthaca, NY 14853luwang@cs.cornell.eduClaire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853cardie@cs.cornell.eduAbstractWe present a token-level decision summariza-tion framework that utilizes the latent topicstructures of utterances to identify ?summary-worthy?
words.
Concretely, a series ofunsupervised topic models is explored andexperimental results show that fine-grainedtopic models, which discover topics at theutterance-level rather than the document-level,can better identify the gist of the decision-making process.
Moreover, our proposedtoken-level summarization approach, whichis able to remove redundancies within utter-ances, outperforms existing utterance rankingbased summarization methods.
Finally, con-text information is also investigated to add ad-ditional relevant information to the summary.1 IntroductionMeetings are an important way for information shar-ing and collaboration, where people can discussproblems and make concrete decisions.
Not sur-prisingly, there is an increasing interest in develop-ing methods for extractive summarization for meet-ings and conversations (Zechner, 2002; Maskey andHirschberg, 2005; Galley, 2006; Lin and Chen,2010; Murray et al, 2010a).
Carenini et al (2011)describe the specific need for focused summaries ofmeetings, i.e., summaries of a particular aspect of ameeting rather than of the meeting as a whole.
Forexample, the decisions made, the action items thatemerged and the problems arised are all importantoutcomes of meetings.
In particular, decision sum-maries would allow participants to review decisionsfrom previous meetings and understand the relatedtopics quickly, which facilitates preparation for theupcoming meetings.A:We decided our target group is the focus on who canafford it , (1)B:Uh I?m kinda liking the idea of latex , if if spongy isthe in thing .
(2)B:what I?ve seen , just not related to this , but of latexcases before , is that [vocalsound] there?s uh like a hardplastic inside , and it?s just covered with the latex .
(2)C:Um [disfmarker] And I think if we wanna keep our costsdown , we should just go for pushbuttons , (3)D:but if it?s gonna be in a latex type thing and that?sgonna look cool , then that?s probably gonna have abigger impact than the scroll wheel .
(2)A:we?re gonna go with um type pushbuttons , (3)A:So we?re gonna have like a menu button , (4)C:uh volume , favourite channels , uh and menu .
(4)A:Pre-set channels (4)Decision Abstracts (Summary)DECISION 1: The target group comprises of individualswho can afford the product.DECISION 2: The remote will have a latex case.DECISION 3: The remote will have pushbuttons.DECISION 4: The remote will have a power button, volumebuttons, channel preset buttons, and a menu button.Figure 1: A clip of a meeting from the AMI meeting cor-pus (Carletta et al, 2005).
A, B, C and D refer to distinctspeakers; the numbers in parentheses indicate the associatedmeeting decision: DECISION 1, 2, 3 or 4.
Also shown is thegold-standard (manual) abstract (summary) for each decision.Meeting conversation is intrinsically differentfrom well-written text, as meetings may not be wellorganized and most utterances have low density ofsalient content.
Therefore, multiple problems needto be addressed for speech summarization.
Considerthe sample dialogue snippet in Figure 1 from theAMI meeting corpus (Carletta et al, 2005).
Onlydecision-related dialogue acts (DRDAs) ?
utter-40ances at least one decision made in the meeting1 ?are listed and ordered by time.
Each DRDA is la-beled numerically according to the decision it sup-ports; so the second and third utterances (in bold)support DECISION 2, as do the fifth utterance in thesnippet.
Manually constructed decision abstracts foreach decision are shown at the bottom of the figure.Besides the prevalent dialogue phenomena (suchas ?Uh I?m kinda liking?
in Figure 1), disfluenciesand off-topic expressions, we notice that single ut-terance is usually not informative enough to forma decision.
For instance, no single DRDA associ-ated with DECISION 4 corresponds all that well withits decision abstract: ?pushbuttons?, ?menu button?and ?Pre-set channels?
are mentioned in separateDAs.
As a result, extractive summarization methodsthat select individual utterance to form the summarywill perform poorly.Furthermore, it is difficult to identify the coretopic when multiple topics are discussed in one ut-terance.
For example, all of the bold DRDAs sup-porting DECISION 2 contain the word ?latex?.
How-ever, the last DA in bold also mentions ?bigger im-pact?
and ?the scroll wheel?, which are not specifi-cally relevant for DECISION 2.
Though this problemcan be approached by training a classifier to identifythe relevant phrases and ignore the irrelevant onesor dialogue phenomena, it needs expensive humanannotation and is limited to the specific domain.Note also that for DECISION 4, the ?power but-ton?
is not specified in any of the listed DRDAssupporting it.
By looking at the transcript, we find?power button?
mentioned in one of the preceding,but not decision-related DAs.
Consequently anotherchallenge would be to add complementary knowl-edge when the DRDAs cannot provide complete in-formation.Therefore, we need a summarization approachthat is tolerant of dialogue phenomena, can deter-mine the key semantic content and is easily trans-ferable between domains.
Recently, topic model-ing approaches have been investigated and achievedstate-of-the-art results in multi-document summa-rization (Haghighi and Vanderwende, 2009; Celiky-1These DRDAs are annotated in the AMI corpus and usuallycontain the decision content.
They are similar, but not com-pletely equivalent, to the decision dialogue acts (DDAs) of Buiet al (2009), Ferna?ndez et al (2008), Frampton et al (2009).ilmaz and Hakkani-Tur, 2010).
Thus, topic mod-els appear to be a better ref for document simi-larity w.r.t.
semantic concepts than simple literalword matching.
However, very little work has in-vestigated its role in spoken document summariza-tion (Chen and Chen, 2008; Hazen, 2011), and muchless conducted comparisons among topic modelingapproaches for focused summarization in meetings.In contrast to previous work, we study the un-supervised token-level decision summarization inmeetings by identifying a concise set of key wordsor phrases, which can either be output as a com-pact summary or be a starting point to generate ab-stractive summaries.
This paper addresses problemsmentioned above and make contributions as follows:?
As a step towards creating the abstractive sum-maries that people prefer when dealing with spo-ken language (Murray et al, 2010b), we propose atoken-level rather than sentence-level frameworkfor identifying components of the summary.
Ex-perimental results show that, compared to the sen-tence ranking based summarization algorithms,our token-level summarization framework can bet-ter identify the summary-worthy words and re-move the redundancies.?
Rather than employing supervised learning meth-ods that rely on costly manual annotation, we ex-plore and evaluate topic modeling approaches ofdifferent granularities for the unsupervised deci-sion summarization at both the token-level and di-alogue act-level.
We investigate three topic mod-els ?
Local LDA (LocalLDA) (Brody and El-hadad, 2010), Multi-grain LDA (MG-LDA) (Titovand McDonald, 2008) and Segmented TopicModel (STM) (Du et al, 2010) ?
which can uti-lize the latent topic structure on utterance levelinstead of document level.
Under our proposedtoken-level summarization framework, three fine-grained models outperform the basic LDA modeland two extractive baselines that select the longestand the most representative utterance for each de-cision, respectively.
(ROUGE-SU4 F score of14.82% for STM vs. 13.58% and 13.46% forthe baselines, given the perfect clusterings of DR-DAs.)?
In line with prior research that explore the role ofcontext for utterance-based extractive summariza-41tion (Murray and Renals, 2007), we investigate therole of context in our token-level summarizationframework.
For the given clusters of DRDAs, Westudy two types of context information ?
the DAspreceding and succeeding a DRDA and DAs ofhigh TF-IDF similarity with a DRDA.
We also in-vestigate two ways to select relevant words fromthe context DA.
Experimental results show thattwo types of context have comparable effect, butselecting words from the dominant topic of thecenter DRDA performs better than from the dom-inant topic of the context DA.
Moreover, by lever-aging context, the recall exceeds the provided up-perbound?s recall (ROUGE-1 recall: 48.10% vs.45.05% for upperbound by using DRDA only) al-though the F scores decrease after adding contextinformation.
Finally, we show that when the trueDRDA clusterings are not available, adding con-text can improve both the recall and F score.2 Related WorkSpeech and dialogue summarization has become im-portant in recent years as the number of multime-dia resources containing speech has grown.
A pri-mary goal for most speech summarization systemsis to account for the special characteristics of di-alogue.
Early work in this area investigated su-pervised learning methods, including maximum en-tropy, conditional random fields (CRFs), and sup-port vector machines (SVMs) (Buist et al, 2004;Galley, 2006; Xie et al, 2008).
For unsupervisedmethods, maximal marginal relevance (MMR) is in-vestigated in (Zechner, 2002) and (Xie and Liu,2010).
Gillick et al (2009) introduce a concept-based global optimization framework by using in-teger linear programming (ILP).Only in very recent works has decision sum-marization been addressed in (Ferna?ndez et al,2008), (Bui et al, 2009) and (Wang and Cardie,2011).
(Ferna?ndez et al, 2008) and (Bui et al, 2009)utilize semantic parser to identify candidate phrasesfor decision summaries and employ SVM to rankthose phrases.
They also train HMM and SVMdirectly on a set of decision-related dialogue actson token level and use the classifiers to identifysummary-worthy words.
Wang and Cardie (2011)provide an exploration on supervised and unsuper-vised learning for decision summarization on bothutterance- and token- level.Our work also arises out of applying topic mod-els to text summarization (Bhandari et al, 2008;Haghighi and Vanderwende, 2009; Celikyilmaz andHakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur,2010).
Mostly, the sentences are ranked according toimportance based on latent topic structures, and topones are selected as the summary.
There are someworks for applying document-level topic models tospeech summarization (Kong and shan Leek, 2006;Chen and Chen, 2008; Hazen, 2011).
Different fromtheir work, we further investigate the topic models offine granularity on sentence level and leverage con-text information for decision summarization task.Most existing approaches for speech summariza-tion result in a selection of utterances from the dia-logue, which cannot remove the redundancy withinutterances.
To eliminate the superfluous words, ourwork is also inspired by keyphrase extraction ofmeetings (Liu et al, 2009; Liu et al, 2011) andkeyphrase based summarization (Riedhammer et al,2010).
However, a small set of keyphrases are notenough to concretely display the content.
Instead ofonly picking up keyphrases, our work identifies allof the summary-worthy words and phrases, and re-moves redundancies within utterances.3 Summarization FrameworksIn this section, we first present our proposed token-level decision summarization framework ?
Dom-Sum ?
which utilizes latent topic structure in ut-terances to extract words from Dominant Topic (seedetails in Section 3.1) to form Summaries.
In Sec-tion 3.2, we describe four existing sentence scor-ing metrics denoted as OneTopic, MultiTopic, TMM-Sum and KLSum which are also based on latent topicdistributions.
We adopt them to the utterance-levelsummarization for comparison in Section 6.3.1 Token-level Summarization FrameworkDomsum takes as input the clusters of DRDAs (withor without additional context DAs), the topic distri-bution for each DA and the word distribution foreach topic.
The output is a set of topic-coherentsummary-worthy words which can be used directlyas the summary or to further generate abstractivesummary.
We introduce DomSum in two steps ac-cording to its input: taking clusters of DRDAs as theinput and with additional context information.42DRDAs Only.
Given clusters of DRDAs, we useAlgorithm 1 to produce the token-level summary foreach cluster.
Generally, Algorithm 1 chooses thetopic with the highest probability as the dominanttopic given the dialogue act (DA).
Then it collectsthe words with a high joint probability with the dom-inant topic from that DA.Input : Cluster C = {DAi}, P (Tj |DAi), P (wk|Tj)Output: SummarySummary?
?
(empty set)foreach DAi in C doDomTopic?
maxTj P (Tj |DAi) (*)Candidate?
?foreach word wk inDAi doSampleTopic?
maxTj P (wk|Tj)P (Tj |DAi)if DomTopic == SampleTopic thenCandidate?
Union(Candidate, wk)endendSummary?
Union(Summary, Candidate)endAlgorithm 1: DomSum ?
The token-level sum-marization framework.
DomSum takes as input theclusters of DRDAs and related probability distribu-tions.Leveraging Context.
For each DRDA (denoted as?center DA?
), we study two types of context infor-mation (denoted as ?context DAs?).
One is adjacentDAs, i.e., immediately preceding and succeedingDAs, the other is the DAs having top TF-IDF simi-larities with the center DA.
Context DAs are addedinto the cluster the corresponding center DA in.We also study two criteria of word selection fromthe context DAs.
For each context DA, we can takethe words appearing in the dominant topic of ei-ther this context DA or its center DRDA.
We willshow in Section 6.1 that the latter performs betteras it produces more topic-coherent summaries.
Al-gorithm 1 can be easily modified to leverage contextDAs by updating the input clusters and assigning theproper dominant topic for each DA accordingly ?this changes the step (?)
in Algorithm 1.3.2 Utterance-level Summarization MetricsWe also adopt four sentence scoring metrics basedon the latent topic structure for extractive summa-rization.
Though they are developed on differenttopic models, given the desired topic distributions asinput, they can rank the utterances according to theirimportance and provide utterance-level summariesfor comparison.OneTopic and MultiTopic.
In (Bhandari et al,2008), several sentence scoring functions are intro-duced based on Probabilistic Latent Semantic Index-ing.
We adopt two metrics, which are OneTopicand MultiTopic.
For OneTopic, topic T with high-est probability P (T ) is picked as the central topicper cluster C. The score for DA in C is:P (DA|T ) =?w?DA P (T |DA,w)?DA??C,w?DA?
P (T |DA?, w),MultiTopic modifies OneTopic by taking all of thetopics into consideration.
Given a cluster C, DA inC is scored as:?TP (DA|T )P (T ) =?T?w?DA P (T |DA,w)?DA??C,w?DA?
P (T |DA?, w)P (T )TMMSum.
Chen and Chen (2008) propose a Top-ical Mixture Model (TMM) for speech summariza-tion, where each dialogue act is modeled as a TMMfor generating the document.
TMM is shown toprovide better utterance-level extractive summariesfor spoken documents than other conventional unsu-pervised approaches, such as Vector Space Model(VSM) (Gong and Liu, 2001), Latent SemanticAnalysis (LSA) (Gong and Liu, 2001) and Max-imum Marginal Relevance (MMR) (Murray et al,2005).
The importance of a sentence S can be mea-sured by its generative probability P (D|S), whereD is the document S belongs to.
In our experiments,one decision is made per cluster of DAs.
So weadopt their scoring metric to compute the generativeprobability of the cluster C for each DA:P (C|DA) =?wi?C?TjP (wi|Tj)P (Tj |DA),KLSum.
Kullback-Lieber (KL) divergence is ex-plored for summarization in (Haghighi and Vander-wende, 2009) and (Lin et al, 2010), where it is usedto measure the distance of distributions between thedocument and the summary.
For a cluster C of DAs,given a length limit ?, a set of DAs S is selected as:S?
= arg minS:|S|<?KL(PC ||PS) = arg minS:|S|<?
?TiP (Ti|C)logP (Ti|C)P (Ti|S)4 Topic ModelsIn this section, we briefly describe the three fine-grained topic models employed to compute the la-tent topic distributions on utterance level in the43meetings.
According to the input of Algorithm 1,we are interested in estimating the topic distributionfor each DA P (T |DA) and the word distributionfor each topic P (w|T ).
For MG-LDA, P (T |DA)is computed as the expectation of local topic distri-butions with respect to the window distribution.4.1 Local LDALocal LDA (LocalLDA) (Brody and Elhadad, 2010)uses almost the same probabilistic generative modelas Latent Dirichlet Allocation (LDA) (Blei et al,2003), except that it treats each sentence as a sepa-rate document2.
Each DA d is generated as follows:1.
For each topic k:(a) Choose word distribution: ?k ?
Dir(?)2.
For each DA d:(a) Choose topic distribution: ?d ?
Dir(?
)(b) For each word w in DA d:i.
Choose topic: zd,w ?
?dii.
choose word: w ?
?zd,w4.2 Multi-grain LDAMulti-grain LDA (MG-LDA) (Titov and McDonald,2008) can model both the meeting specific topics(e.g.
the design of a remote control) and various con-crete aspects (e.g.
the cost or the functionality).
Thegenerative process is:1.
Choose a global topic distribution: ?glm ?
Dir(?gl)2.
For each sliding window v of size T :(a) Choose local topic distribution: ?locm,v ?
Dir(?loc)(b) Choose granularity mixture: pim,v ?
Beta(?mix)3.
For each DA d:(a) choose window distribution: ?m,d ?
Dir(?)4.
For each word w in DA d of meeting m:(a) Choose sliding window: vm,w ?
?m,d(b) Choose granularity: rm,w ?
pim,vm,w(c) If rm,w = gl, choose global topic: zm,w ?
?glm(d) If rm,w = loc, choose local topic: zm,w ?
?locm,vm,w(e) Choose word w from the word distribution: ?rm,wzm,w4.3 Segmented Topic ModelThe last model we utilize is Segmented Topic Model(STM) (Du et al, 2010), which jointly modelsdocument- and sentence-level latent topics usinga two-parameter Poisson Dirichlet Process (PDP).Given parameters ?, ?,?
and PDP parameters a, b,the generative process is:1.
Choose distribution of topics: ?m ?
Dir(?)2.
For each dialogue act d:2For the generative process of LDA, the DAs in the samemeeting make up the document, so ?each DA?
is changed to?each meeting?
in LocalLDA?s generative process.
(a) Choose distribution of topics: ?d ?
PDP (?m, a, b)3.
For each word w in dialogue act d:(a) Choose topic: zm,w ?
?d(b) Choose word: w ?
?zm,w5 Experimental SetupThe Corpus.
We evaluate our approach on theAMI meeting corpus (Carletta et al, 2005) that con-sists of 140 multi-party meetings.
The 129 scenario-driven meetings involve four participants playingdifferent roles on a design team.
A short (usuallyone-sentence) abstract is manually constructed tosummarize each decision discussed in the meetingand used as gold-standard summaries in our experi-ments.System Inputs.
Our summarization system re-quires as input a partitioning of the DRDAs accord-ing to the decision(s) that each supports (i.e., onecluster of DRDAs per decision).
As mentioned ear-lier, we assume for all experiments that the DRDAsfor each meeting have been identified.
For evalua-tion we consider two system input settings.
In theTrue Clusterings setting, we use the AMI annota-tions to create perfect partitionings of the DRDAsas the input; in the System Clusterings setting, weemploy a hierarchical agglomerative clustering algo-rithm used for this task in previous work (Wang andCardie, 2011).
The Wang and Cardie (2011) cluster-ing method groups DRDAs according to their LDAtopic distribution similarity.
As better approachesfor DRDA clustering become available, they couldbe employed instead.Evaluation Metric.
To evaluate the performanceof various summarization approaches, we use thewidely accepted ROUGE (Lin and Hovy, 2003) met-rics.
We use the stemming option of the ROUGEsoftware at http://berouge.com/ and removestopwords from both the system and gold-standardsummaries, same as Riedhammer et al (2010) do.Inference and Hyperparameters We use the im-plementation from (Lu et al, 2011) for the threetopic models in Section 4.
The collapsed GibbsSampling approach (Griffiths and Steyvers, 2004) isexploited for inference.
Hyperparameters are cho-sen according to (Brody and Elhadad, 2010), (Titovand McDonald, 2008) and (Du et al, 2010).
In LDAand LocalLDA, ?
and ?
are both set to 0.1 .
ForMG-LDA, ?gl, ?loc and ?mix are set to 0.1; ?
is 0.144and the window size T is 3.
And the number of lo-cal topic is set as the same number of global topic asdiscussed in (Titov and McDonald, 2008).
In STM,?, a and b are set to 0.5, 0.1 and 1, respectively.5.1 Baselines and ComparisonsWe compare our token-level summarization frame-work based on the fine-grained topic models to (1)two unsupervised baselines, (2) token-level summa-rization by LDA, (3) utterance-level summarizationby Topical Mixture Model (TMM) (Chen and Chen,2008), (4) utterance-level summarization based onthe fine-grained topic models using existing metrics(Section 3.2), (5) two supervised methods, and (6)an upperbound derived from the AMI gold standarddecision abstracts.
(1) and (6) are described below,others will be discussed in Section 6.The LONGEST DA Baseline.
As in (Riedhammeret al, 2010) and (Wang and Cardie, 2011), this base-line simply selects the longest DRDA in each clusteras the summary.
Thus, it performs utterance-leveldecision summarization.
This baseline and the nextallow us to determine summary quality when sum-maries are restricted to a single utterance.The PROTOTYPE DA Baseline.
Following Wangand Cardie (2011), the second baseline selects thedecision cluster prototype (i.e., the DRDA with thelargest TF-IDF similarity with the cluster centroid)as the summary.Upperbound.
We also compute an upperboundthat reflects the gap between the best possible ex-tractive summaries and the human-written abstractsaccording to the ROUGE score: for each cluster ofDRDAs, we select the words that also appear in theassociated decision abstract.6 Results and Discussion6.1 True ClusteringsHow do fine-grained topic models compare to ba-sic topic models or baselines?
Figure 2 demon-strates that by using the DomSum token-level sum-marization framework, the three fine-grained topicmodels uniformly outperform the two non-trivialbaselines and TMM (Chen and Chen, 2008) (reim-plemented by us) that generates utterance-level sum-maries.
Moreover, the fine-grained models also beatbasic LDA under the same DomSum token-levelsummarization framework.
This shows the fine-2 3 4 5 6 7 8 9 106789101112131415#TopicsROUGE?SU4 F(%)Comparison with Baselines, TMM and LDALocalLDAMG?LDASTMLDATMMBASELINE 1BASELINE 2Figure 2: With true clusterings of DRDAs as the input, we useDomSum to compare the performance of LocalLDA, MGLDAand STM against two baselines, LDA and TMM.
?# topic?
in-dicates the number of topics for the model.
For MGLDA, ?#topic?
is the number of local topics.2 3 4 5 6 7 8 9 101313.213.413.613.81414.214.414.614.815#TopicROUGE?SU4 F(%)Summarization from DRDAs by Different Metrics Based on STM (DRDA only)DomSumOneTopicMultiTopicTMMSumKLSumFigure 3: With true clusterings of DRDAs as the input, Dom-Sum is compared with four DA-level summarization metrics us-ing topic distributions from STM.
Results from LocalLDA andMGLDA are similar so they are not displayed.grained topic models that discover topic structureson utterance-level better identify gist information.Can the proposed token-level summarizationframework better identify important words andremove redundancies than utterance selectionmethods?
Figure 3 demonstrates the comparisonresults for our DomSum token-level summarizationframework with four existing utterance scoring met-rics discussed in Section 3.2, namely OneTopic,MultiTopic, TMMSum and KLSum.
The utterancewith highest score is extracted to form the summary.LocalLDA and STM are utilized to compute the in-put distributions, i.e., P (T |DA) and P (w|T ).
FromFigure 3, DomSum yields the best F scores which452 3 4 5 6 7 8 9 107.588.599.51010.511#TopicROUGE?SU4F (%)Leveraging Context by STMAdj+DomSum(Multi)Adj+DomSum(One)TFIDF+DomSum(Multi)TFIDF+DomSum(One)Figure 4: Under DomSum framework, two types of contextinformation are added: Adjacent DA (?Adj?)
and DAs with highTFIDF similarities (?TFIDF?).
For each context DA, selectingwords from the dominant topic of center DA (?One?)
or thecurrent context DA (?Multi?)
are investigated.2 3 4 5 6 7 8 9 107.588.599.51010.51111.51212.5#TopicROUGE?SU4F (%)Summarization by Different Metrics (adding Context)LocalLDA+DomSum(One)STM+DomSum(One)LocalLDA+OneTopicSTM+OneTopicLocalLDA+MultiTopicSTM+MultiTopicFigure 5: By using adjacent DAs as context, DomSum is com-pared with two DA-level summarization metrics: OneTopic andMultiTopic.
For DomSum, the words of context DA from dom-inant topic of the center DA (?One?)
is selected; For OneTopicand MultiTopic, three top ranked DAs are selected.shows that the token-level summarization approachis more effective than utterance-level methods.Which way is better for leveraging context infor-mation?
We explore two types of context infor-mation.
For adjacent content (Adj in Figure 4), 5DAs immediately preceding and 5 DAs succeedingthe center DRDA are selected.
For TF-IDF context(TFIDF in Figure 4), 10 DAs of highest TF-IDF sim-ilarity with the center DRDA are taken.
We alsoexplore two ways to extract summary-worthy wordsfrom the context DA ?
selecting words from thedominant topic of either the center DA (denoted as?One?
in parentheses in Figure 4) or the current con-text DA (denoted as ?multi?
in parentheses in Fig-True ClusteringsR-1 R-2 R-SU4PREC REC F1 F1 F1BaselinesLongest DA 34.06 31.28 32.61 12.03 13.58Prototype DA 40.72 28.21 33.32 12.18 13.46SupervisedMethodsCRF 52.89 26.77 35.53 11.48 14.03SVM 43.24 37.92 40.39 12.78 16.24Our Approach5 topicsLocalLDA 35.18 38.92 36.95 12.33 14.74+ context 17.26 45.34 25.00 8.40 11.05STM 34.06 41.30 37.32 12.42 14.82+ context 15.60 48.10 23.56 8.16 9.9810 topicsLocalLDA 36.20 36.81 36.50 12.04 14.34+ context 21.82 41.57 28.62 9.61 12.24STM 34.15 40.83 37.19 12.40 14.56+ context 17.87 46.57 25.82 8.89 10.97Upperbound 100.00 45.05 62.12 33.27 34.89Table 1: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4(R-SU4) scores for our proposed token-level summarization ap-proaches along with two baselines, supervised methods and theUpperbound (only using DRDAs).
?
all use True Clusteringsure 4).
Figure 4 indicates that the two types of con-text information do not have significant difference,while selecting the words from the dominant topicof the center DA results in better ROUGE-SU4 Fscores.
Notice that compared with Figure 3, the re-sults in Figure 4 have lower F scores when using thetrue clusterings of DRDAs.
This is because contextDAs bring in relevant words as well as noisy infor-mation.
We will show in Section 6.2 that when trueclusterings are not available, the context informationcan boost both recall and F score.How do the token-level summarization frame-work compared to utterance selection methodsfor leveraging context?
We also compare theability of leveraging context of DomSum to utter-ance scoring metrics, i.e., OneTopic and MultiTopic.5 DAs preceding and 5 DAs succeeding the centerDA are added as context information.
For contextDA under DomSum, we select words from the dom-inant topic of the center DA (denoted as ?One?
inparentheses in Figure 5).
For OneTopic and Mul-tiTopic, the top 3 DAs are extracted as the sum-mary.
Figure 5 demonstrates the combination of Lo-calLDA and STM with each of the metrics.
Dom-Sum, as a token-level summarization metrics, domi-nates other two metrics in leveraging context.46System ClusteringsR-1 R-2 R-SU4PREC REC F1 F1 F1BaselinesLongest DA 17.06 11.64 13.84 2.76 3.34Prototype DA 18.14 10.11 12.98 2.84 3.09SupervisedMethodsCRF 46.97 15.25 23.02 6.09 9.11SVM 39.05 18.45 25.06 6.11 9.82Our Approach5 topicsLocalLDA 25.57 16.57 20.11 4.03 5.87+ context 20.68 25.96 23.02 3.09 4.48STM 24.15 17.82 20.51 4.03 5.69+ context 20.64 30.03 24.47 3.59 4.7610 topicsLocalLDA 25.98 15.94 19.76 3.59 4.41+ context 23.98 21.92 22.90 3.45 4.10STM 26.32 19.14 22.16 4.07 5.88+ context 22.50 28.40 25.11 3.43 4.15Table 2: ROUGE-1 (R-1), ROUGE-2 (R-2) and ROUGE-SU4(R-SU4) scores for our proposed token-level summarization ap-proaches, compared with two baselines and supervised meth-ods.
?
all use System ClusteringsHow do our approach perform when comparedwith supervised learning approaches?
For a bet-ter comparison, we also provide summarizationresults by using supervised systems along withan upperbound.
We use Support Vector Ma-chines (Joachims, 1998) with RBF kernel and order-1 Conditional Random Fields (Lafferty et al, 2001)?
trained with the same features as (Wang andCardie, 2011) to identify the summary-worthy to-kens to include in the abstract.
A three-fold crossvalidation is conducted for both methods.
ROUGE-1, ROUGE-2 and ROUGE-SU4 scores are listed inTable 1.
From Table 1, our token-level summa-rization approaches based on LocalLDA and STMare shown to outperform the baselines and even theCRF.
Meanwhile, by adding context information,both LocalLDA and STM can get better ROUGE-1recall than the supervised methods, even higher thanthe provided upperbound which is computed by onlyusing DRDAs.
This shows the DomSum frameworkcan leverage context to compensate the summaries.6.2 System ClusteringsResults using the System Clusterings (Table 2)present similar findings, though all of the system andbaseline scores are lower.
By adding context infor-mation, the token-level summarization approachesbased on fine-grained topic models compare favor-DRDA (1): I think if we can if we can include them at not toomuch extra cost, then I?d put them in,DRDA (2): Uh um we we?re definitely going in for voicerecognition as well as LCDs, mm.DRDA (3): So we?ve basically worked out that we?re goingwith a simple battery,context DA (1):So it?s advanced integrated circuits?context DA (2):the advanced chipcontext DA (3): and a curved on one side case which is foldedin on itself , um made out of rubberDecision Abstract: It will have voice recognition, use a simplebattery, and contain an advanced chip.Longest DA & Prototype DA: Uh um we we?re definitely goingin for voice recognition as well as LCDs, mm.TMM: I think if we can if we can include them at not too muchextra cost, then I?d put them in,SVM: cost voice recognition simple batteryCRF: voice recognition batterySTM: extra cost, definitely going voice recognition LCDs,simple batterySTM + context: cost, company, advanced integrated circuits, goingvoice recognition, simple battery, advanced chip, curved case rubberTable 3: Sample system outputs by different methods are in thethird cell (methods?
names are in bold).
First cell contains threeDRDAs supporting the decision in the second cell and three ad-jacent DAs of them.ably to the supervised methods in F scores, and alsoget the best ROUGE-1 recalls.6.3 Sample System SummariesTo better exemplify the summaries generated bydifferent systems, sample output for each methodis shown in Table 3.
We see from the table thatutterance-level extractive summaries (Longest DA,Prototype DA, TMM) make more coherent but stillfar from concise and compact abstracts.
On the otherhand, the supervised methods (SVM, CRF) that pro-duce token-level extracts better identify the overallcontent of the decision abstract.
Unfortunately, theyrequire human annotation in the training phase.
Incomparison, the output of fine-grained topic modelscan cover the most useful information.7 ConclusionWe propose a token-level summarization frameworkbased on topic models and show that modeling topicstructure at the utterance-level is better at identify-ing relevant words and phrases than document-levelmodels.
The role of context is also studied andshown to be able to identify additional summary-worthy words.Acknowledgments This work was supported in part byNational Science Foundation Grants IIS-0968450 andIIS-1111176, and by a gift from Google.47ReferencesHarendra Bhandari, Takahiko Ito, Masashi Shimbo, andYuji Matsumoto.
2008.
Generic text summarizationusing probabilistic latent semantic indexing.
In Pro-ceedings of IJCNLP, pages 133?140.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022, March.Samuel Brody and Noemie Elhadad.
2010.
An unsu-pervised aspect-sentiment model for online reviews.In Human Language Technologies: The 2010 AnnualConference of the North American Chapter of theAssociation for Computational Linguistics, HLT ?10,pages 804?812, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Trung H. Bui, Matthew Frampton, John Dowding, andStanley Peters.
2009.
Extracting decisions frommulti-party dialogue using directed graphical modelsand semantic similarity.
In Proceedings of the SIG-DIAL 2009 Conference, pages 235?243.Anne Hendrik Buist, Wessel Kraaij, and Stephan Raaij-makers.
2004.
Automatic summarization of meetingdata: A feasibility study.
In Proc.
Meeting of Compu-tational Linguistics in the Netherlands (CLIN).Giuseppe Carenini, Gabriel Murray, and Raymond Ng.2011.
Methods for Mining and Summarizing Text Con-versations.
Morgan & Claypool Publishers.Jean Carletta, Simone Ashby, Sebastien Bourban,Mike Flynn, Thomas Hain, Jaroslav Kadlec, VasilisKaraiskos, Wessel Kraaij, Melissa Kronenthal, Guil-laume Lathoud, Mike Lincoln, Agnes Lisowska, andMccowan Wilfried Post Dennis Reidsma.
2005.
Theami meeting corpus: A pre-announcement.
In In Proc.MLMI, pages 28?39.Asli Celikyilmaz and Dilek Hakkani-Tur.
2010.
A hy-brid hierarchical model for multi-document summa-rization.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguistics, ACL?10, pages 815?824, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Berlin Chen and Yi-Ting Chen.
2008.
Extractivespoken document summarization for information re-trieval.
Pattern Recogn.
Lett., 29:426?437, March.Lan Du, Wray Buntine, and Huidong Jin.
2010.
Asegmented topic model based on the two-parameterpoisson-dirichlet process.
Mach.
Learn., 81:5?19, Oc-tober.Raquel Ferna?ndez, Matthew Frampton, John Dowding,Anish Adukuzhiyil, Patrick Ehlen, and Stanley Peters.2008.
Identifying relevant phrases to summarize de-cisions in spoken meetings.
INTERSPEECH-2008,pages 78?81.Matthew Frampton, Jia Huang, Trung Huu Bui, and Stan-ley Peters.
2009.
Real-time decision detection inmulti-party dialogue.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 3 - Volume 3, pages 1133?1141.Michel Galley.
2006.
A skip-chain conditional randomfield for ranking meeting utterances by importance.In Proceedings of the 2006 Conference on EmpiricalMethods in Natural Language Processing, pages 364?372.Dan Gillick, Korbinian Riedhammer, Benoit Favre, andDilek Hakkani-Tur.
2009.
A global optimizationframework for meeting summarization.
In Proceed-ings of the 2009 IEEE International Conference onAcoustics, Speech and Signal Processing, ICASSP?09, pages 4769?4772.
IEEE Computer Society.Yihong Gong and Xin Liu.
2001.
Generic text summa-rization using relevance measure and latent semanticanalysis.
In Proceedings of the 24th annual interna-tional ACM SIGIR conference on Research and devel-opment in information retrieval, SIGIR ?01, pages 19?25, New York, NY, USA.
ACM.T.
L. Griffiths and M. Steyvers.
2004.
Finding scien-tific topics.
Proceedings of the National Academy ofSciences, 101(Suppl.
1):5228?5235, April.Aria Haghighi and Lucy Vanderwende.
2009.
Exploringcontent models for multi-document summarization.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,NAACL ?09, pages 362?370.
Association for Compu-tational Linguistics.Timothy J. Hazen.
2011.
Latent topic modeling for au-dio corpus summarization.
In INTERSPEECH, pages913?916.Thorsten Joachims.
1998.
Text categorization with Sup-port Vector Machines: Learning with many relevantfeatures.
In Claire Ne?dellec and Ce?line Rouveirol,editors, Machine Learning: ECML-98, volume 1398,chapter 19, pages 137?142.
Berlin/Heidelberg.Sheng-Yi Kong and Lin shan Leek.
2006.
Improved spo-ken document summarization using probabilistic latentsemantic analysis (plsa).
In Proceedings of the 2006IEEE International Conference on Acoustics, Speechand Signal Processing, ICASSP ?06.John D. Lafferty, Andrew McCallum, and Fernando C. N.Pereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the Eighteenth InternationalConference on Machine Learning, ICML ?01, pages282?289, San Francisco, CA, USA.
Morgan Kauf-mann Publishers Inc.48Shih-Hsiang Lin and Berlin Chen.
2010.
A risk mini-mization framework for extractive speech summariza-tion.
In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics, ACL ?10,pages 79?87.
Association for Computational Linguis-tics.Chin-Yew Lin and Eduard Hovy.
2003.
Automatic evalu-ation of summaries using n-gram co-occurrence statis-tics.
In Proceedings of the 2003 Conference of theNorth American Chapter of the Association for Com-putational Linguistics on Human Language Technol-ogy - Volume 1, pages 71?78.S.-H. Lin, Y.-M. Yeh, and B. Chen.
2010.
Leveragingkullback-leibler divergence measures and information-rich cues for speech summarization.Feifan Liu, Deana Pennell, Fei Liu, and Yang Liu.
2009.Unsupervised approaches for automatic keyword ex-traction using meeting transcripts.
In Proceedingsof Human Language Technologies: The 2009 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, NAACL ?09,pages 620?628, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Fei Liu, Feifan Liu, and Yang Liu.
2011.
A supervisedframework for keyword extraction from meeting tran-scripts.
IEEE Transactions on Audio, Speech & Lan-guage Processing, 19(3):538?548.Bin Lu, Myle Ott, Claire Cardie, and Benjamin Tsou.2011.
Multi-aspect sentiment analysis with topic mod-els.
In Workshop on Sentiment Elicitation from Natu-ral Text for Information Retrieval and Extraction.Sameer Maskey and Julia Hirschberg.
2005.
ComparingLexical, Acoustic/Prosodic, Structural and DiscourseFeatures for Speech Summarization.
In Proc.
Euro-pean Conference on Speech Communication and Tech-nology (Eurospeech).Gabriel Murray and Steve Renals.
2007.
Towards on-line speech summarization.
In INTERSPEECH, pages2785?2788.Gabriel Murray, Steve Renals, and Jean Carletta.
2005.Extractive summarization of meeting recordings.
Inin Proceedings of the 9th European Conference onSpeech Communication and Technology, pages 593?596.Gabriel Murray, Giuseppe Carenini, and Raymond Ng.2010a.
Interpretation and transformation for abstract-ing conversations.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, HLT ?10, pages 894?902, Stroudsburg, PA, USA.Association for Computational Linguistics.Gabriel Murray, Giuseppe Carenini, and Raymond T. Ng.2010b.
Generating and validating abstracts of meetingconversations: a user study.
In INLG?10.Korbinian Riedhammer, Benoit Favre, and DilekHakkani-Tu?r.
2010.
Long story short - global unsu-pervised models for keyphrase based meeting summa-rization.
Speech Commun., 52(10):801?815, October.Ivan Titov and Ryan McDonald.
2008.
Modeling onlinereviews with multi-grain topic models.
In Proceed-ing of the 17th international conference on World WideWeb, WWW ?08, pages 111?120.
ACM.Lu Wang and Claire Cardie.
2011.
Summarizing deci-sions in spoken meetings.
In Proceedings of the Work-shop on Automatic Summarization for Different Gen-res, Media, and Languages, pages 16?24, Portland,Oregon, June.
Association for Computational Linguis-tics.Shasha Xie and Yang Liu.
2010.
Using confusion net-works for speech summarization.
In Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, HLT ?10, pages 46?54.
Associ-ation for Computational Linguistics.Shasha Xie, Yang Liu, and Hui Lin.
2008.
Evaluatingthe effectiveness of features and sampling in extrac-tive meeting summarization.
In Proc.
of IEEE SpokenLanguage Technology (SLT).Klaus Zechner.
2002.
Automatic summarization ofopen-domain multiparty dialogues in diverse genres.Comput.
Linguist., 28:447?485, December.49
