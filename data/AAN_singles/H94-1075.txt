(ALMOST)  AUTOMATIC  SEMANTIC  FEATUREEXTRACTION FROM TECHNICAL  TEXTRajeev Agarwal*rajeevOcs, msstaSe, duDepartment of Computer ScienceMississippi State UniversityMississippi State, MS 39762.ABSTRACTAcquisition of semantic information is necessary for properunderstanding ofnatural anguage text.
Such information isoften domain-speclfic in nature and must be acquized fromthe domain.
This causes a problem whenever a natural fan-guage processing (NLP) system is moved from one domain toanother.
The portability of an NLP system can be improvedif these semantic features can be acquired with \];mited hu-man intervention.
This paper proposes an approach towards(almost) automatic semantic feature xtraction.I.
INTRODUCTIONAcquisition of semantic information is necessary for properunderstanding ofnatural anguage text.
Such information isoften domaln-specific in nature and must be acquized fromthe domain.
When an NLP system is moved from one do-main to another, usually a substantial mount of time has tobe spent in tailoring the system to the new domain.
Most ofthis time is spent on acquiring the semantic features pecificto that domain.
It is important o automate the process ofacquisition of semantic information as much as possible, andfacilitate whatever human intervention is absolutely neces-sary.
Portability of NLP systems has been of concern to re-searchers for some time \[8, 5, 11, 9\].
This paper proposes anapproach to obtain the domaln-dependent semantic featuresof any given domain in a domain-independent manner.The next section will describe an existing NLP system(KUDZU) which has been developed at Mississippi State Uni-versity.
Section 3 will then present the motimstion behind theautomatic acquisition of the semantic features of a domain,and a brief outline of the methodology proposed to do it.
Sec-tion 4 will describe the dlf\[ereut steps in this methodology indetail Section 5 will focus on the app\]icatlons of the seman-tic features.
Section 6 compares the proposed approach toR;ml\]ar research efforts.
The last section presents ome finalcomments.2.
THE EXISTING KUDZU SYSTEMThe research described in this paper is part of a larger on-going project called the KUDZU (Knowledge Under Devel-opment from Zero Understanding) project.
This project isaimed at exploring the automation of extraction of infor-mation from technical texts.
The KUDZU system has twoprimary components - - an NLP component, and a Knowl-edge Analysis (KA) component.
This section desoribes thissystem in order to facilitate understanding of the approachdescribed in this paper.The NLP component consists of a tagger, a semi-purser, aprepositional phrase attachment specialist, a conjunct iden-tifier for coordinate conjunctions, and a restructuzer.
Thetagger is an u-gram based program that currently generatessyntactic/semantic ags for the words in the corpus.
Thesyntactic portion of the tag is mandatory and the seman-tic portion depends upon whether the word has any spe-cial domain-specific classification or not.
Currently onlynouns, gerunds, and adjectives are assigned semantic tags.For example, in the domain of veterinary medicine, adog"would be assigned the tag "nounmpatient, m "nasal" wouldbe "adj--body-part, m etc.The parsing strategy is based on the initial identification ofsimple phrases, which are later converted to deeper structureswith the help of separate specialist programs for coordinateconjunct identification and prepositional phrase attachment.The result for a given sentence is a single parse, many ofwhose elements are comparatively underspecified.
For exam-ple, the parses generated lack clause boundaries.
Neverthe-less, the results are surprisingly useful in the extraction ofrelationships from the corpus.The semi-parser ecognises noun-, verb-, prepositional-,gerund-, infinitive-, and s~iectival-phrases.
The preposi-tional phrase attachment specialist \[2\] uses case grammaranalysis to disambiguate the attachments of prepositionalphrases and is highly domain-dependent.
The current ira-plemcntation of this subcomponent is highly specific to thedomnin of veterinary medicine, the initial testbed for theKUDZU system.
Note that all the examples presented in thispaper will be taken from this domain.
The coordinate con-junction specialist identifies pairs of appropriate conjunctsfor the coordinate conjunctions in the text and is domain-independent in nature \[1\].
The restructurer puts togetherthe information acquired by the specialist programs in orderto provide a better (and deeper) structure to the parse.
"This research is supported by the NSP-ARPA grant numberIRI 9314963.Before being passed to the knowledge analysis portion of thesystem, some parses undergo manual modification, which is378facilitated by the help of an editing tool especially written forthis purpose.
A large percentage of the modifications can beattributed to the \];m;tation of the conjunct identifier in rec-ognising ouly pairs of conjuncts, as opposed to all conjunctsof coordinate conjunctions.The KA component receives appropriate parses from the NLPcomponent and uses them to populate an object-orientedknowledge base \[4\].
The nature of the knowledge base cre-ated is dependent on a domain schema which specifies theconcept hierarchy and the relationships of interest in the do-main.
Examples of the concept hierarchy specifications andrelationships present in the domain schema re given in Fig-ure 1.
(class (name patient)(parent animal))(relationship (name treatment)(role (name disease) (type mandatory) (class disorder))(role (name lrestmont) (type mandatory) (class PROCEDURE)(class MEDICATION))(role (name species) (type optional) (class PATIENT))(role (name Iocalion) (type optional) (class BODY-PART)))Figure I: Examples of Schema EntriesMany such relationships may be defined in the domainschema.
While processing a sentence, the KA componenthypothesises the types of relationships that may be presentin it.
However, before the actual relationship is instantiated,objects corresponding to the mandatory slots must be foundjeither directly or by the help of an algorithm that resolvesindirect and implied references.
If objects corresponding tothe optional slots are found, then they are also filled in,Currently, the domain schema has to be generated manuallyafter a careful evaluation of the domain.
This is a time-consuming process that often requ~.res the help of a domainexpert.
Once the schema has been specified, the rest of theKA component is domain independent \[4\], with the excep-tion of s domain-specific synonym table.
For each sentencethat is processed by the KA component, s set of semanticrelationships that were found in the sentence is produced.An interface to the KA component allows users to navigatethrough all instances of the different relationships that havebeen acquired from the corpus.
Two sample sentences fromthe veterinary medicine domain, their parsed output and therelations extracted from them are shown in Figure 2.3 .
OUTL INE  OF  THE PROPOSEDAPPROACHThe automatic acquisition of semantic features of a domainis an important issue, since it assists portability by reduc-ing the amount of human intervention.
In the context of theKUDZU system in particular, it is desired that the systembe moved from the domain of veterinary medicine to thatof physical chemistry.
As explained above, certain compo-nents of the system are domain-dependent and have to besignificantly modified before the system can be used for anew domain.
The current research aims to use the acquiredsemantic features in order to improve the portability of theKUDZU system.It is important o note that although the initial motivationfor this research came from the need to move the KUDZU sys-tern to a new domain, the underlying techniques are genericand can be used in a variety of applications.
The primarygoal is to acquire the semantic features of the domain withrn;nlmal human intervention, and ensure that these featurescan be applied to different systems rather easily.
In this re-search, two main types of semantic features are of interesta concept hierarchy for the domain, and lexico-semanticpatterns present in the domain.
These patterns are i;m;larto what are also known as "selectional constraints" or "selec-tional patterns" \[6, 5\] in systems which use them primarilyto determine the correct parse from a large number of parsesgenerated by a broad coverage grammar.
They are basicallyco-occurrence patterns between meaningful t nouns, gerunds,adjectives, and verbs.
For example, "DISORDER of BODY-PAKT' ,  "MEDICATION can be used to TREAT-VERB PA-TIENT", etc.
are legitimate lexico-semantic patterns fromthe veterinary medicine domain.The steps involved in the acquisition of semantic featuresfrom the domain can be briefly outlined as follows:I.
Generate the syntactic tags for all the words in the cor-pus.2.
A\]gorithmically identify the expUclt semantic dustersthat may exist in the current domain.
Apply the cluster-ing algorithm separately to nouns, gerunds, adjectives,and verbs.3.
Use the syntactic tags and semantic lasses to automatethe identification of lexico-semantic patterns that existin the given domain.This basic methodology is ~miIar to some other approachesadopted in the past \[8, 6\].
However, some important differ-ences exist which will be discussed later.Once the semantic features have been obtained, they can beused in a variety of ways depending upon the needs of theNLP system.
They can be helpful in improving the ports-bility of an NLP system by providing useful semantic infor-mation that may be needed by different components of thesystem.
In the KUDZU system, these features will be usedto improve the success rate of a domaln-independent syntac-tically based prepositional phrase attachment specialist, andfor automatic acquisition of the domain schema.It is easy to see how the lexico-semantic patterns can be helpIAs explained later, a memd~al  word is one that has a se-mantic tag auociated with it.379Sentences:Parses:Cataracts may accompany corneal opacification.In the Labrador Retriever, they may be associated with skeletal dysplasia of the forelegs.
(sent.mica(nounphrase ((w cataracts nounlplundlldiserder)))(verb_phrase ((w may aux) (w accompany verb)))(noun_phrase ((w cameal adjllbody_p~) (w opacificafion notmlldiserder))))(sclttcltlce(prep-phr~ (win prep)(noun_plmrse ((w the det) (w Labrador noun) (w Retriever noun))))(nounphrase ((w they prolplural)))(verb_phrase ((w may aux) (w be aux) (w associated verb))(noun.phrase ((w skeletal djllbody-lmrt ) (w dysplasia nounlldisoxder))(p~_~,hrffi~ (w of v,~)(norm_phrase ((w the det) (w foxelegs nounlpluralllbody-part))))))))Relationships:Relalicmhip: SymptomRole SYMPTOM: cataractsRole DISORDER: opacificafion (comesl)Relationship: PredispositionRole DISEASE: dysplasia (skeletal)Role PREDISPOSED: cataractsRole SPECIES: Labrador RetrieverRole LOCATION: forelegsFigure 2: Sample Sentences Processed by KUDZUful in the attachment of prepositions/ phrases.
All patternsthat have some preposition embedded within them will essen-tially provide selectional constraints for the classes of wordsthat may appear in the object and host slots.
These pat-terns ~ be used to improve the success rate of a domain-independent syntactically based prepositional phrase attach-ment specialist.
There is ample evidence \[10, 15\] that seman-tic categories and co\]\]ocational p tterns can be used effec-tively to assist in the process of prepositional phrase attach-ment.These semantic features will also be used to automaticallygenerate the domain schema for any given domain.
Figure 1contains examples of the semantic lass hierarchy and the re-lationships of interest, as defined in the domain schema.
Theformer may be acquired from the semantic lustering pro-cess.
The specification of the relationships can be achievedwith the help of the weighted lexico-seme~xtie patterns.
Someof the relationships can be acquired by an automated com-parlson of al\] patterns involving a given semantic verb class.Other relationships may be determined by comparing otherpatterns with common noun and gerund semantic classes.The resulting domain schema, in some sense, represents thesemantic structure of the domain.4.
ACQUIS IT ION OF SEMANTICFEATURES4.1.
Tagg ingIt was decided that the tag set used by the Penn treebankproject, with u few exceptions, be adopted for tagging thecorpus.
Unlike the Penn treebank tag set, we have separatetags for auxiliaries, gerunds, and subordinate conjunctions(rather than clumping subordinate conjunctions with prepo-sitions).
Therefore, as a first step in the process of acquisitionof semantic features, the corpus is tagged with appropriatetags.
Brill's rule-based tagger \[3\] is being used for this pur-pose.
This step is primarily domain-independent, althoughthe tagger may have to be further trained on a new domain.Since this tag set is purely syntactic in nature, the semanticclusters of the words must be acquired by a different method.4.2.
I dent i f i ca t ion  o f  Semant ic  C lus tersThe identification of such semantic lusters (which providethe concept hierarchy) is the next step.
Nouns, gerunds, ad-jectives, and verbs axe to be clustered into separate semantichierarchies.
A traditional clustering system m COBWEB/3is used to cluster words into their semantic categories.Since COBWEB/3 \[13\] requires attribute-value vectors asso-dated with the entities to be clustered, such vectors must bedefined.
The attributes used to define these vectors hould bechosen to reflect the lexico-syntactic context of the words be-cause the semantic ategory of s word is strongly influencedby the context in which it appears.
The proposed method-ology involves specifying a set of lexico-syntactic attributes380separately for nouns, gerunds, adjectives, and verbs.
Pre-sumably, the syntactic constraints that affect the semanticcategory of a noun are different from those that affect thecategory of gerunds, adjectives and verbs.
Currently, threeattributes are being used for noun clustering u subj,,.,.b (verbwhose subject is the current noun), obj,~.,.b (verb whose ob-ject is the current noun), and host~.p (preposition of whichthe current noun is an object).
The top i values that sat-isfy the attr ibute subj..,.b, top j values of ob~...b, and topk values of host~..p are of interest.
A cross-product of thesevalues yields the attribute-value vectors.
For example, ifi = 3, j = 3, and k = 2 are used, 3 x 3 x 2 = 18 vec-tors are generated for each noun.
These values are generatedby a program from the phrasal structures produced by thesemi-parser.
The same attributes can be used across differ-ent domains, and hence the attrlbute.value vectors neededfor semantic lustering can be generated with no human in-tervention.
Some examples of the semantic lusters that maybe identified in the domain of veterinary medicine are DIS-ORDER, PATIENT, BODY-PART, MEDICATION, etc.
fornouns; DIAGNOSTIC-PROC, TREATMENT-PROC, etc.for gerunds; DISORDER, BODY-PART, etc.
for adjectives;CAUSE-VERB, TREAT-VEP~B, etc.
for verbs.The clustering technique is not expected to generate com-pletely correct clusters in one pass.
However, the improp-erly classified words Hill not be manually reclassified at thisstage.
In order to attain proper hierarchical clusters, theprocess of clustering may have to be performed again afterlexico-semantic patterns have been discovered by the processdescribed below.
The only human intervention required atthe present stage is for the assignment of class identifiers tothe generated classes.
In fact, a human is shown small sub-clusters (each with 8-10 objects 2) of the generated lfierarchy,and is asked to label these sub-clusters with a semantic label,if possible.
Note that not all such sub-clusters should have se-mantic labels - -  several nouns in the corpus are generic nounsthat cannot be classified into any semantic lass.
However,a majority of the sub-clusters should represent the semanticclasses that exist in the domain.
The class identifiers thus as-signed are then associated as semantic tags with these wordsand used to discover the lex/co-semantic patterns in the nextstep.
Any word that has a semantic tag is considered to bemeaningful4.3.
Discovery of Lexico-SemanticPatternsThe semantic clusters obtained from the clustering proce-dure, after the manual assignment of class identifiers, areused to identify the lexico-semantic patterns.
The phrase.level parsed structures produced by the semi-parser are ana-lysed for dJ~erent patterns.
These patterns are of the formsubject-verb-object, noun-noun, adjective.noun, NP-PP, andVP-NP-PP, where NP, VP, and PP refer to noun-, verb-,and prepositional-phrases r pectively.
All patterns that oc-ZNote that this does not reflect he size of the ~na\] clusters gen-erated by the program, since words that eventually should belongto the same cluster may initially be in different sub-clusters.cur in the corpus more often than some pre.defined thresh-old are assumed to be important and are saved.
One re-striction currently being placed on these patterns is that stleast two meaningful words must be present in every pattern.The patterns are weighted on the bask of their frequency ofoccurrence, w/th the more frequent patterns getting higherweights.It seems reasonable to assume that if lexico-semantic pat-terns were already known to the system, the identification ofsemantic ategories would become easier and vice.versa.
Inthis research, we propose to first identify semantic ategoriesand then the patterns.
It has long been realized that there isan interdependence b tween the structure of a text and thesemantic lassification that is present in it.
HalUday \[7\] statedthat ~' .
.
.
there is no question of discovering one before theother.
~ We believe that an appro~mate classi.~catlon can beachieved before the structures are fully identified.
However,this interdependence b tween classification and structure winhave its adverse effects on the results.
It is anticipated thatthe results of both semantic lustering and pattern dlseov-ery Hill not be very accurate in the first pass.
Therefore, aniteratlve scheme is being proposed.After semantic lustering has been performed, human inter-vention is needed to assign class identifiers to the gener-ated clusters.
These identifiers assist in the proper discov-cry of lexico-semantic patterns.
The resulting set of patternsmay contain some Lrrelevant patterns and human interven-tion is needed to accept/reject the automatically generatedpatterns.
Both the accepted and rejected patterns axe storedby the system so that in future iterations, the same patternsdo not need human verification.
As has been shown before\[8, 6, 14\], such patterns place constraints on the semanticclasses of words appearing in particular contexts.
The setof selected patterns can, therefore, be used to reanalyse thecorpus in order to recogn~e the incorrectly clustered wordsin the previously generated class hierarchy and to suggest hecorrect class for these words.
For example, if the word "pen/-cfllin" is incorrectly clustered as a DISORDER, an analysisof the corpus win show that it appears most frequent|y asa MEDICATION in patterns llke "TKEAT-VEB.B DISOB.-DER with MEDICATION",  "MEDICATION can be used toTREAT-VER.B PATIENT",  etc.
and rarely as a DISORDERin the DISORDER patterns.
Hence, its semantic ategorycan be guessed to be MEDICATION.
This guess is added tothe fist of attributes for the words, and semantic lusteringis performed again.
This iterntlve mechanism assists clus-tering in two ways - -  firstly, the additional attribute helpsconvergence towards better clusters and secondly, the ten-tative semantic classes from the i,a iteration can be usedto generate values for attributes for the (i + 1) ta iteration s,thus reducing the sparsity of data.
This time better clustersshould be formed, and these will again be used to recognizelexico-semantic patterns.
We expect the system to convergeto a stable set of clusters and patterns after a small num-ber of iterations.
A simple diagram outlining the process ofa~Vhen attempting to duster nounJs, for exaxnple, the sem~nticclasses for gerunds, verbs, and adjectives are used.381Synla~cTawng J -IlSemanticCl~Classification rIdentificationManual Assignmentof Class Id~flersFLute 3: The Proposed ApproachManualAccedes/Reid/onof Patunmd Patternr I Discover/acquisitlon of semantic features is ~ven in FLute 3.5.
COMPARISON TO OTHERAPPROACHESThe most significant work that is similar to the proposedmethodology is that conducted on the Linguistic StringProject \[8\] and the PROTEUS project \[6\] at New York Uni-versity.
Recent efforts have been made by Grkhman andSterling \[6\] towards automatic acqnisition of selectlonal con-strnints.
The technique proposed here for the acquisitionof semantic features hould require only limited human in-tervention.
Since the semi-parsed structures can directlybe used to generate the attribute-value vectors needed forclustering, the only human intervention should be in a~n-ment of class identifiers, acceptance/rejection of the discov-ered patterns, and reelass~ca~on f some concepts that maynot get correctly classified even after the feedback from thelexico-semantic patterns.
Further, their approach \[8, 6\] usesa large broad coverage grammar, and often several parsesare produced for each sentence.
The basic parsing strategyadopted in the KUDZU system starts with simple phrasalparses which can be used to acquire the semantic features ofthe domaJ.u.
These semantic features can then be used fordisambigustion ofsyntactic attachments and thus in provid-ing a better and deeper structure to the parse.Sekine at el.
\[16\] have also worked on this idea of "grad-ual approximation" using an iterative mechanism.
They de-scribe a scenario for the determination of internal structuresof Japanese compound nouns.
They use syntactic tuples togenerate co\]location statistics for words which are then usedfor clustering.
The clusters produced by their program aremuch smaller in size (appror;mately 3 words per cluster) thanthe ones attempted in our research.
We intend to generatemuch larger clusters of words that intnitively belong to thesame semantic ategory in the ~ven dom~n.
The semanticcategories generated by the clustering process are used forthe identification of semantic relationships of interest in thedomain.
Most of the emphasis in the research undertakenat New York University on selectional constraints \[8, 6\] andthat in Se\]dne's work has been on using the co\]locations forimproved syntactic ans/y~s.
In addition to using them todisamb~uate prepositional phrase attachments, we will alsouse them to generate a domain schema which is fundamentalto the knowledge xtraction process.Our approach of consolidating several exico-semantic pat-terns into frame-Irks tructures that represent the semanticstructure of the domain is similar to one discussed by Marsh\[12\].
Several other efforts have been made towards n~ingsemantic features for doms/n-specific dictionary creation orparsing.6.
F INAL  COMMENTSThe methodology described in this paper should be usefulin acquiring semantic features of a domain with limited hu-man intervention.
We also believe that our parsing method-ology and the mech~n;Rms for semantic feature acquisitionlend themselves very nicely to the development of a simplerand smaller NLP system.
This is in contrast to NLP systemsthat are very large and often use large broad coverage gram-mars that may take several thousand person-hours to build.The simplicity behind starting with phrasal parses and thenusing these parses to acquire semantic information that leadsto better and deeper parses makes our approach s good "poorman's alternative".
The KUDZU system has demonstratedthat this simple approach can s/so yield reasonably good re-sults, at least for data or information extraction tasks.AcknowledgementsThe author is thankful to Dr. Lois Eoggess for carefullyediting an earlier version of this paper.
Thanks are also dueto Eric BrlU for his tagger and to Kevin Thompson and NASAAmes Research Center for making COBWEB/3 available forthis research.
Finally, thanks are due to ARPA and NSF forproviding the funding for this research.References1.
Rajeev Agarwal and Lois Boggess.
A simple but usefulapproach to conjunct identification.
In Proceedings ofthe 30th Annual Meeting of the Association for Compu-tational Linguistics, pages 15-21.
Association for Com-putational Linguistics, 1992.2.
Lois Boggess, B.ajeev Agarwal, and Ron Davis.
Dis-ambiguation of prepositional phrases in automaticallylabelled technical text.
In Proceeding8 ofthe Ninth Na-tional Conference on Artificial Intelligence, pages 155-159.
The AAAI Pre~/The MIT Press, 1991.3.
Eric Brfll.
A simple ruleobased part of speech tagger.
InProceedings of the Speech and Natural Language Work-shop, pages 112-116, February 1992.4.
Jose Cordova.
A Domain-Independent Approach to theEztraetion and Assimilation of Knowledge from Natural382Language Temk PhD thesis, Mississippi State University,August 1992.5.
Ralph Grlshman, Lynette HJzschman, and Ngo ThanhNhsa.
Discovery procedures for sublanguage s lections]patterns: Initial experiments.
Computational LinguiJ-ties, 12(3):205-215, July-September 1986.6.
Ralph Grishman and John Sterling.
Smoothing of au-tomatically generated selectional constraints.
In Pro-ceedings of the ARPA Workshop ors Human LanguageTechnolo911.
Morgan Kanfmsan Publishers, March 1993.7.
M. A. K. Hallldffiy.
Categories of the theory of grammar.Word, 17(3):241-292, 1961.8.
Lynette Hirechman.
Discovering sublanguage struc-tures.
In Ralph Grishman sad Richard Kittredge, edi-tors, Analyzing Langauage in Restricted Domains: Sub-language Description and Processing, chapter 12, pages211-234.
Lawrence Eribaum Associates, Hilisdale, NewJersey, 1988.9.
Lynette Hirechman, Francois-Michel Lang, John Dowd-ing, and Carl Weir.
Porting PUNDIT to the resourcemanagement domain.
In Proceedings of the Speech andNatural Language Workshop, pages 277-282, Philadel-phia, PA, February 1989.10.
Donald Hindie and Mats /tooth.
Structural ambigu-ity and ]exical relations.
Computational Linguistics,19(1):103-120, March 1993.11.
Robert Ingria and Lance/tamshaw.
Porting to new do-mains using the ]earner.
In Proceedings of the Speechand Natural Language Workshop, ages 241-244, CapeCod, MA, October 1989.12.
Elaine Marsh.
General semantic patterns in differentsublanguages.
In Ralph Grlshman and Richard Kit-tredge, ed/tore, Analyzing Language in Restricted Do-mains: Sublanguage Description and Procesing, chap-ter 7, pages 103-127.
Lawrence Erlbanm Associates,H;llndale, New Jersey, 1986.13.
Kathleen McKusick and Kevin Thompson.
COB-WEB/3: A portable implementation.
Technical report,NASA Ames Research Center, June 1990.14.
Phil Resuik.
Semantic elasses and syntactic ambiguity.In Proceedings of the ARPA Workshop on Human Lan-guage Technology.
Morgan Kaufm,L-n Publishers, March1993.15.
Phil Resnik and Mexti Hearst.
Structural ambiguity andconceptual relations.
In Proceedings ofthe Workshop onVery Large Corpora: Academic and Industrial Perspec-tives, pages 58-64, June 1993.16.
Satoshi Sekine, Sofia Anauiadou, Jeremy Carroll, andJun'ichi Tsujii.
Linguistic knowledge generator.
In Pro-ceedings of the 15th International Conference on Com-putational Linguistics (COLING-9~), pages 56--566,1992.383
