c?
2004 Association for Computational LinguisticsIntricacies of Collins?
Parsing ModelDaniel M. Bikel?University of PennsylvaniaThis article documents a large set of heretofore unpublished details Collins used in his parser, suchthat, along with Collins?
(1999) thesis, this article contains all information necessary to duplicateCollins?
benchmark results.
Indeed, these as-yet-unpublished details account for an 11% relativeincrease in error from an implementation including all details to a clean-room implementation ofCollins?
model.
We also show a cleaner and equally well-performing method for the handling ofpunctuation and conjunction and reveal certain other probabilistic oddities about Collins?
parser.We not only analyze the effect of the unpublished details, but also reanalyze the effect of certainwell-known details, revealing that bilexical dependencies are barely used by the model and thathead choice is not nearly as important to overall parsing performance as once thought.
Finally,we perform experiments that show that the true discriminative power of lexicalization appears tolie in the fact that unlexicalized syntactic structures are generated conditioning on the headwordand its part of speech.1.
IntroductionMichael Collins?
(1996, 1997, 1999) parsing models have been quite influential in thefield of natural language processing.
Not only did they achieve new performancebenchmarks on parsing the Penn Treebank (Marcus, Santorini, and Marcinkiewicz1993), and not only did they serve as the basis of Collins?
own future work (Collins2000; Collins and Duffy 2002), but they also served as the basis of important workon parser selection (Henderson and Brill 1999), an investigation of corpus variationand the effectiveness of bilexical dependencies (Gildea 2001), sample selection (Hwa2001), bootstrapping non-English parsers (Hwa, Resnik, and Weinberg 2002), and theautomatic labeling of semantic roles and predicate-argument extraction (Gildea andJurafsky 2000; Gildea and Palmer 2002), as well as that of other research efforts.Recently, in order to continue our work combining word sense with parsing (Bikel2000) and the study of language-dependent and -independent parsing features (Bikeland Chiang 2000), we built a multilingual parsing engine that is capable of instanti-ating a wide variety of generative statistical parsing models (Bikel 2002).1 As an ap-propriate baseline model, we chose to instantiate the parameters of Collins?
Model 2.This task proved more difficult than it initially appeared.
Starting with Collins?
(1999)thesis, we reproduced all the parameters described but did not achieve nearly thesame high performance on the well-established development test set of Section 00 of?
Department of Computer and Information Science, 3330 Walnut Street, Philadelphia, PA 19104.
E-mail:dbikel@linc.cis.upenn.edu1 This engine is publicly available at http://www.cis.upenn.edu/?dbikel/software.htmlSubmission received: 18 January 2003; Revised submission received: 20 March 2004; Accepted forpublication: 10 June 2004480Computational Linguistics Volume 30, Number 4the Penn Treebank.
Together with Collins?
thesis, this article contains all the informa-tion necessary to replicate Collins?
parsing results.2 Specifically, this article describesall the as-yet-unpublished details and features of Collins?
model and some analysisof the effect of these features with respect to parsing performance, as well as somecomparative analysis of the effects of published features.3 In particular, implementingCollins?
model using only the published details causes an 11% increase in relative errorover Collins?
own published results.
That is, taken together, all the unpublished detailshave a significant effect on overall parsing performance.
In addition to the effects ofthe unpublished details, we also have new evidence to show that the discriminativepower of Collins?
model does not lie where once thought: Bilexical dependencies playan extremely small role in Collins?
models (Gildea 2001), and head choice is not nearlyas critical as once thought.
This article also discusses the rationale for various param-eter choices.
In general, we will limit our discussion to Collins?
Model 2, but we makeoccasional reference to Model 3, as well.2.
MotivationThere are three primary motivations for this work.
First, Collins?
parsing model repre-sents a widely used and cited parsing model.
As such, if it is not desirable to use it asa black box (it has only recently been made publicly available), then it should be pos-sible to replicate the model in full, providing a necessary consistency among researchefforts employing it.
Careful examination of its intricacies will also allow researchersto deviate from the original model when they think it is warranted and accuratelydocument those deviations, as well as understand the implications of doing so.The second motivation is related to the first: science dictates that experiments bereplicable, for this is the way we may test and validate them.
The work described herecomes in the wake of several previous efforts to replicate this particular model, butthis is the first such effort to provide a faithful and equally well-performing emulationof the original.The third motivation is that a deep understanding of an existing model?its in-tricacies, the interplay of its many features?provides the necessary platform for ad-vancement to newer, ?better?
models.
This is especially true in an area like statis-tical parsing that has seen rapid maturation followed by a soft ?plateau?
in per-formance.
Rather than simply throwing features into a new model and measuringtheir effect in a crude way using standard evaluation metrics, this work aims toprovide a more thorough understanding of the nature of a model?s features.
Thisunderstanding not only is useful in its own right but should help point the waytoward newer features to model or better modeling techniques, for we are in thebest position for advancement when we understand existing strengths and limita-tions.2 In the course of replicating Collins?
results, it was brought to our attention that several otherresearchers had also tried to do this and had also gotten performance that fell short of Collins?published results.
For example, Gildea (2001) reimplemented Collins?
Model 1 but obtained resultswith roughly 16.7% more relative error than Collins?
reported results using that model.3 Discovering these details and features involved a great deal of reverse engineering, and ultimately,much discussion with Collins himself and perusal of his code.
Many thanks to Mike Collins for hisgenerosity.
As a word of caution, this article is exhaustive in its presentation of all such details andfeatures, and we cannot guarantee that every reader will find every detail interesting.481Bikel Intricacies of Collins?
Parsing Model3.
Model OverviewThe Collins parsing model decomposes the generation of a parse tree into many smallsteps, using reasonable independence assumptions to make the parameter estimationproblem tractable.
Even though decoding proceeds bottom-up, the model is definedin a top-down manner.
Every nonterminal label in every tree is lexicalized: the labelis augmented to include a unique headword (and that headword?s part of speech) thatthe node dominates.
The lexicalized PCFG that sits behind Model 2 has rules of theformP ?
LnLn?1 ?
?
?L1HR1 ?
?
?Rn?1Rn (1)where P, Li, Ri, and H are all lexicalized nonterminals, and P inherits its lexical headfrom its distinguished head-child, H. In this generative model, first P is generated, thenits head-child H, then each of the left- and right-modifying nonterminals are generatedfrom the head outward.
The modifying nonterminals Li and Ri are generated condi-tioning on P and H, as well as a distance metric (based on what material intervenesbetween the currently generated modifying nonterminal and H) and an incrementalsubcategorization frame feature (a multiset containing the arguments of H that haveyet to be generated on the side of H in which the currently generated nonterminalfalls).
Note that if the modifying nonterminals were generated completely indepen-dently, the model would be very impoverished, but in actuality, because it includesthe distance and subcategorization frame features, the model captures a crucial bit oflinguistic reality, namely, that words often have well-defined sets of complements andadjuncts, occurring with some well-defined distribution in the right-hand sides of a(context-free) rewriting system.The process proceeds recursively, treating each newly generated modifier as aparent and then generating its head and modifier children; the process terminateswhen (lexicalized) preterminals are generated.
As a way to guarantee the consistencyof the model, the model also generates two hidden +STOP+ nonterminals as the leftmostand rightmost children of every parent (see Figure 7).4.
Preprocessing Training TreesTo the casual reader of Collins?
thesis, it may not be immediately apparent that thereare quite a few preprocessing steps for each annotated training tree and that thesesteps are crucial to the performance of the parser.
We identified 11 preprocessing stepsnecessary to prepare training trees when using Collins?
parsing model:1. pruning of unnecessary nodes2.
adding base NP nodes (NPBs)3.
?repairing?
base NPs4.
adding gap information (applicable to Model 3 only)5. relabeling of sentences with no subjects (subjectless sentences)6. removing null elements7.
raising punctuation8.
identification of argument nonterminals9.
stripping unused nonterminal augmentations482Computational Linguistics Volume 30, Number 410.
?repairing?
subjectless sentences11.
head-findingThe order of presentation in the foregoing list is not arbitrary, as some of the stepsdepend on results produced in previous steps.
Also, we have separated the steps intotheir functional units; an implementation could combine steps that are independentof one another (for clarity, our implementation does not, however).
Finally, we notethat the final step, head-finding, is actually required by some of the previous stepsin certain cases; in our implementation, we selectively employ a head-finding moduleduring the first 10 steps where necessary.4.1 Coordinated PhrasesA few of the preprocessing steps rely on the notion of a coordinated phrase.
In thisarticle, the conditions under which a phrase is considered coordinated are slightlymore detailed than is described in Collins?
thesis.
A node represents a coordinatedphrase if?
it has a nonhead child that is a coordinating conjunction and?
that conjunction is either?
posthead but nonfinal, or?
immediately prehead but noninitial (where ?immediately?means ?with nothing intervening except punctuation?
).4In the Penn Treebank, a coordinating conjunction is any preterminal node with thelabel CC.
This definition essentially picks out all phrases in which the head-child istruly conjoined to some other phrase, as opposed to a phrase in which, say, there isan initial CC, such as an S that begins with the conjunction but.4.2 Pruning of Unnecessary NodesAs a preprocessing step, pruning of unnecessary nodes simply removes preterminalsthat should have little or no bearing on parser performance.
In the case of the EnglishTreebank, the pruned subtrees are all preterminal subtrees whose root label is one of{?
?, ?
?, .}.
There are two reasons to remove these types of subtrees when parsingthe English Treebank: First, in the treebanking guidelines (Bies 1995), quotation markswere given the lowest possible priority and thus cannot be expected to appear withinconstituent boundaries in any kind of consistent way, and second, neither of thesetypes of preterminals?nor any punctuation marks, for that matter?counts towardsthe parsing score.4.3 Adding Base NP NodesAn NP is basal when it does not itself dominate an NP; such NP nodes are relabeledNPB.
More accurately, an NP is basal when it dominates no other NPs except possessiveNPs, where a possessive NP is an NP that dominates POS, the preterminal possessive4 Our positional descriptions here, such as ?posthead but nonfinal,?
refer to positions within the list ofimmediately dominated children of the coordinated phrase node, as opposed to positions within theentire sentence.483Bikel Intricacies of Collins?
Parsing ModelNP??????NPNNPJohnCCandNPNNPJaneNP??????NPBNNPJohnCCandNPBNNPJaneNP?????
?NPNPBNNPJohnCCandNPNPBNNPJane(a) Coordinated phrase (b) Base NPs relabeled (c) Extra NP nodes insertedFigure 1An NP that constitutes a coordinated phrase.NP??????????NPB??
?the comedian,,NPB??
?Tom FooleryNP??????????NPB??
?the comedian,,NPNPB??
?Tom Foolery(a) Before extra NP addition(the NPB the comedian is the headchild).
(b) After extra NP insertion.Figure 2A nonhead NPB child of NP requires insertion of extra NP.marker for the Penn Treebank.
These possessive NPs are almost always themselvesbase NPs and are therefore (almost always) relabeled NPB.For consistency?s sake, when an NP has been relabeled as NPB, a normal NP nodeis often inserted as a parent nonterminal.
This insertion ensures that NPB nodes arealways dominated by NP nodes.
The conditions for inserting this ?extra?
NP level areslightly more detailed than is described in Collins?
thesis, however.
The extra NP levelis added if one of the following conditions holds:?
The parent of the NPB is not an NP.?
The parent of the NPB is an NP but constitutes a coordinated phrase (seeFigure 1).?
The parent of the NPB is an NP but?
the parent?s head-child is not the NPB, and?
the parent has not already been relabeled as an NPB (seeFigure 2).5In postprocessing, when an NPB is an only child of an NP node, the extra NP levelis removed by merging the two nodes into a single NP node, and all remaining NPBnodes are relabeled NP.5 Only applicable if relabeling of NPs is performed using a preorder tree traversal.484Computational Linguistics Volume 30, Number 4VP??????VBneedNPNPB????????DTtheNNwillS??
?to continueVP??????VBneedNP??????NPB??DTtheNNwillS??
?to continue(a) Before repair.
(b) After repair.Figure 3An NPB is ?repaired.
?4.4 Repairing Base NPsThe insertion of extra NP levels above certain NPB nodes achieves a degree of con-sistency for NPs, effectively causing the portion of the model that generates childrenof NP nodes to have less perplexity.
Collins appears to have made a similar effort toimprove the consistency of the NPB model.
NPB nodes that have sentential nodes astheir final (rightmost) child are ?repaired?
: The sentential child is raised so that itbecomes a new right-sibling of the NPB node (see Figure 3).6 While such a transforma-tion is reasonable, it is interesting to note that Collins?
parser performs no equivalentdetransformation when parsing is complete, meaning that when the parser producesthe ?repaired?
structure during testing, there is a spurious NP bracket.74.5 Adding Gap InformationThe gap feature is discussed extensively in chapter 7 of Collins?
thesis and is applicableonly to his Model 3.
The preprocessing step in which gap information is added locatesevery null element preterminal, finds its co-indexed WHNP antecedent higher up in thetree, replaces the null element preterminal with a special trace tag, and threads thegap feature in every nonterminal in the chain between the common ancestor of theantecedent and the trace.
The threaded-gap feature is represented by appending -g toevery node label in the chain.
The only detail we would like to highlight here is that animplementation of this preprocessing step should check for cases in which threadingis impossible, such as when two filler-gap dependencies cross.
An implementationshould be able to handle nested filler-gap dependencies, however.4.6 Relabeling Subjectless SentencesThe node labels of sentences with no subjects are transformed from S to SG.
This stepenables the parsing model to be sensitive to the different contexts in which such sub-jectless sentences occur as compared to normal S nodes, since the subjectless sentencesare functionally acting as noun phrases.
Collins?
example of[S [S Flying planes] is dangerous ]6 Collins defines a sentential node, for the purposes of repairing NPBs, to be any node that begins withthe letter S. For the Penn Treebank, this defines the set {S, SBAR, SBARQ, SINV, SQ}.7 Since, as mentioned above, the only time an NPB is merged with its parent is when it is the only childof an NP.485Bikel Intricacies of Collins?
Parsing ModelNP????????NP????NP?
?NNPJohn,,,,CCandNPNNPJane??NP???????????????????
?NPNPNNPJohn,,,,CCandNPNNPJaneFigure 4Raising punctuation: Perverse case in which multiple punctuation elements appear along afrontier of a subtree.illustrates the utility of this transformation.
However, the conditions under which anS may be relabeled are not spelled out; one might assume that every S whose subject(identified in the Penn Treebank with the -SBJ function tag) dominates a null elementshould be relabeled SG.
In actuality, the conditions are much stricter.
An S is relabeledSG when the following conditions hold:?
One of its children dominates a null element child marked with -SBJ.?
Its head-child is a VP.?
No arguments appear prior to the head-child (see Sections 4.9 and 4.11)The latter two conditions appear to be an effort to capture only those subjectlesssentences that are based around gerunds, as in the flying planes example.84.7 Removing Null ElementsRemoving null elements simply involves pruning the tree to eliminate any subtreethat dominates only null elements.
The special trace tag that is inserted in the stepthat adds gap information (Section 4.5) is excluded, as it is specifically chosen to besomething other than the null-element preterminal marker (which is -NONE- in thePenn Treebank).4.8 Raising PunctuationThe step in which punctuation is raised is discussed in detail in chapter 7 of Collins?thesis.
The main idea is to raise punctuation?which is any preterminal subtree inwhich the part of speech is either a comma or a colon?to the highest possible pointin the tree, so that it always sits between two other nonterminals.
Punctuation thatoccurs at the very beginning or end of a sentence is ?raised away,?
that is, pruned.
Inaddition, any implementation of this step should handle the case in which multiplepunctuation elements appear as the initial or final children of some node, as wellas the more pathological case in which multiple punctuation elements appear alongthe left or right frontier of a subtree (see Figure 4).
Finally, it is not clear what to dowith nodes that dominate only punctuation preterminals.
Our implementation simplyissues a warning in such cases and leaves the punctuation symbols untouched.8 We assume the G in the label SG was chosen to stand for the word gerund.486Computational Linguistics Volume 30, Number 4S????????????NP-ANNPElizabethVP-HEAD??????????VBD-HEADwasVP-A????????VBN-HEADelectedS-ANP-HEAD-ANPB???
?DTaNNdirectorFigure 5Head-children are not exempt from being relabeled as arguments.4.9 Identification of Argument NonterminalsCollins employs a small set of heuristics to mark certain nonterminals as arguments, byappending -A to the nonterminal label.
This section reveals three unpublished detailsabout Collins?
argument finding:?
The published argument-finding rule for PPs is to choose the firstnonterminal after the head-child.
In a large majority of cases, this marksthe NP argument of the preposition.
The actual rule used is slightlymore complicated: The first nonterminal to the right of the head-childthat is neither PRN nor a part-of-speech tag is marked as an argument.The nonterminal PRN in the Penn Treebank marks parentheticalexpressions, which can occur fairly often inside a PP, as in the phrase on(or above) the desk.?
Children that are part of a coordinated phrase (see Section 4.1) areexempt from being relabeled as argument nonterminals.?
Head-children are distinct from their siblings by virtue of thehead-generation parameter class in the parsing model.
In spite of this,Collins?
trainer actually does not exempt head-children from beingrelabeled as arguments (see Figure 5).94.10 Stripping Unused Nonterminal AugmentationsThis step simply involves stripping away all nonterminal augmentations, except thosethat have been added from other preprocessing steps (such as the -A augmentationfor argument labels).
This includes the stripping away of all function tags and indicesmarked by the Treebank annotators.9 It is not clear why this is done, and so in our parsing engine, we make such behavior optional via arun-time setting.487Bikel Intricacies of Collins?
Parsing ModelNP??????????NPNNPJohnCCandNP-HEADNNPJane??NP??????????NP-HEADNNPJohnCCandNPNNPJaneNPB?????????
?NNPJohnCCandNNP-HEADJaneFigure 6Head moves from right to left conjunct in a coordinated phrase, except when the parentnonterminal is NPB.4.11 Repairing Subjectless SentencesWith arguments identified as described in Section 4.9, if a subjectless sentence is foundto have an argument prior to its head, this step detransforms the SG so that it revertsto being an S.4.12 Head-FindingHead-finding is discussed at length in Collins?
thesis, and the head-finding rules usedare included in his Appendix A.
There are a few unpublished details worth mention-ing, however.There is no head-finding rule for NX nonterminals, so the default rule of pickingthe leftmost child is used.10 NX nodes roughly represent the N?
level of syntax and inpractice often denote base NPs.
As such, the default rule often picks out a less-than-ideal head-child, such as an adjective that is the leftmost child in a base NP.Collins?
thesis discusses a case in which the initial head is modified when it isfound to denote the right conjunct in a coordinated phrase.
That is, if the head rulespick out a head that is preceded by a CC that is non-initial, the head should be modifiedto be the nonterminal immediately to the left of the CC (see Figure 6).
An importantdetail is that such ?head movement?
does not occur inside base NPs.
That is, a phraseheaded by NPB may indeed look as though it constitutes a coordinated phrase?it hasa CC that is noninitial but to the left of the currently chosen head?but the currentlychosen head should remain chosen.11 As we shall see, there is exceptional behaviorfor base NPs in almost every part of the Collins parser.10 In our first attempt at replicating Collins?
results, we simply employed the same head-finding rule forNX nodes as for NP nodes.
This choice yields different?but not necessarily inferior?results.11 In Section 4.1, we defined coordinated phrases in terms of heads, but here we are discussing how thehead-finder itself needs to determine whether a phrase is coordinated.
It does this by considering thepotential new choice of head: If the head-finding rules pick out a head that is preceded by a noninitialCC (Jane), will moving the head to be a child to the left of the CC (John) yield a coordinated phrase?
Ifso, then the head should be moved?except when the parent is NPB.488Computational Linguistics Volume 30, Number 4VP?????????????????????????????????????????????????????
?+STOP+ VB-HEADneedADVPRBundoubtedlyNP??????NPB?
?the willS??
?to continue+STOP+Figure 7vi feature is true when generating right-hand +STOP+ nonterminal, because the NP the will tocontinue contains a verb.5.
TrainingThe trainer?s job is to decompose annotated training trees into a series of head- andmodifier-generation steps, recording the counts of each of these steps.
Referring to(1), each H, Li, and Ri are generated conditioning on previously generated items,and each of these events consisting of a generated item and some maximal historycontext is counted.
Even with all this decomposition, sparse data are still a problem,and so each probability estimate for some generated item given a maximal contextis smoothed with coarser distributions using less context, whose counts are derivedfrom these ?top-level?
head- and modifier-generation counts.5.1 Verb InterveningAs mentioned in Section 3, instead of generating each modifier independently, themodel conditions the generation of modifiers on certain aspects of the history.
Onesuch function of the history is the distance metric.
One of the two components ofthis distance metric is what we will call the ?verb intervening?
feature, which is apredicate vi that is true if a verb has been generated somewhere in the surface stringof the previously generated modifiers on the current side of the head.
For example,in Figure 7, when generating the right-hand +STOP+ nonterminal child of the VP, thevi predicate is true, because one of the previously generated modifiers on the rightside of the head dominates a verb, continue.12 More formally, this feature is most easilydefined in terms of a recursively defined cv (?contains verb?)
predicate, which is trueif and only if a node dominates a verb:cv(P) =???????
?M child of Pcv(M) if M is not a preterminaltrue if P is a verb preterminalfalse otherwise(2)12 Note that any word in the surface strings dominated by the previously generated modifiers will triggerthe vi predicate.
This is possible because in a history-based model (cf.
Black et al 1992), anythingpreviously generated?that is, anything in the history?can appear in the conditioning context.489Bikel Intricacies of Collins?
Parsing ModelReferring to (2), we define the verb-intervening predicate recursively on the first-orderMarkov process generating modifying nonterminals:vi(Li) ={false if i ?
1cv(Li?1) ?
vi(Li?2) if i > 1(3)and similarly for right modifiers.What is considered to be a verb?
While this is not spelled out, as it happens, averb is any word whose part-of-speech tag is one of {VB, VBD, VBG, VBN, VBP, VBZ}.
Thatis, the cv predicate returns true only for these preterminals and false for all otherpreterminals.
Crucially, this set omits MD, which is the marker for modal verbs.
Anothercrucial point about the vi predicate is that it does not include verbs that appear withinbase NPs.
Put another way, in order to emulate Collins?
model, we need to amend thedefinition of cv by stipulating that cv(NPB) = false.5.2 Skip Certain TreesOne oddity of Collins?
trainer that we mention here for the sake of completeness isthat it skips certain training trees.
For ?odd historical reasons,?13 the trainer skipsall trees with more than 500 tokens, where a token is considered in this context tobe a word, a nonterminal label, or a parenthesis.
This oddity entails that even somerelatively short sentences get skipped because they have lots of tree structure.
In thestandard Wall Street Journal training corpus, Sections 02?21 of the Penn Treebank,there are 120 such sentences that are skipped.
Unless there is something inherentlywrong with these trees, one would predict that adding them to the training set wouldimprove a parser?s performance.
As it happens, there is actually a minuscule (andprobably statistically insignificant) drop in performance (see Table 5) when these treesare included.5.3 Unknown Words5.3.1 The Threshold Problem.
Collins mentions in chapter 7 of his thesis that ?
[a]llwords occurring less than 5 times in training data, and words in test data which havenever been seen in training, are replaced with the ?UNKNOWN?
token (page 186).?
Thefrequency below which words are considered unknown is often called the unknown-word threshold.
Unfortunately, this term can also refer to the frequency above whichwords are considered known.
As it happens, the unknown-word threshold Collinsuses in his parser for English is six, not five.14 To be absolutely unambiguous, wordsthat occur fewer than six times, which is to say, words that occur five times or fewer, inthe data are considered ?unknown.
?5.3.2 Not Handled in a Uniform Way.
The obvious way to incorporate unknownwords into the parsing model, then, is simply to map all low-frequency words inthe training data to some special +UNKNOWN+ token before counting top-level eventsfor parameter estimation (where ?low-frequency?
means ?below the unknown-wordthreshold?).
Collins?
trainer actually does not do this.
Instead, it does not directlymodify any of the words in the original training trees and proceeds to break up theseunmodified trees into the top-level events.
After these events have been collected13 This phrase was taken from a comment in one of Collins?
preprocessing Perl scripts.14 As with many of the discovered discrepancies between the thesis and the implementation, wedetermined the different unknown-word threshold through reverse engineering, in this case, throughan analysis of the events output by Collins?
trainer.490Computational Linguistics Volume 30, Number 4and counted, the trainer selectively maps low-frequency words when deriving countsfor the various context (back-off) levels of the parameters that make use of bilexicalstatistics.
If this mapping were performed uniformly, then it would be identical tomapping low-frequency words prior to top-level event counting; this is not the case,however.
We describe the details of this unknown-word mapping in Section 6.9.2.While there is a negligible yet detrimental effect on overall parsing performancewhen one uses an unknown-word threshold of five instead of six, when this change iscombined with the ?obvious?
method for handling unknown words, there is actuallya minuscule improvement in overall parsing performance (see Table 5).6.
Parameter Classes and Their EstimationAll parameters that generate trees in Collins?
model are estimates of conditional prob-abilities.
Even though the following overview of parameter classes presents only themaximal contexts of the conditional probability estimates, it is important to bear inmind that the model always makes use of smoothed probability estimates that arethe linear interpolation of several raw maximum-likelihood estimates, using variousamounts of context (we explore smoothing in detail in Section 6.8).6.1 Mapped Versions of the Set of NonterminalsIn Sections 4.5 and 4.9, we saw how the raw Treebank nonterminal set is expanded toinclude nonterminals augmented with -A and -g. Although it is not made explicit inCollins?
thesis, Collins?
model uses two mapping functions to remove these augmenta-tions when including nonterminals in the history contexts of conditional probabilities.Presumably this was done to help alleviate sparse-data problems.
We denote the ?ar-gument removal?
mapping function as alpha and the ?gap removal?
mapping functionas gamma.
For example:?
?
(NP-A-g) = NP-g?
?
(NP-A-g) = NP-A?
?(?
(NP-A-g)) = NPSince gap augmentations are present only in Model 3, the gamma function effectivelyis the identity function in the context of Models 1 and 2.6.2 The Head Parameter ClassThe head nonterminal is generated conditioning on its parent nonterminal label, aswell as the headword and head tag which they share, since parents inherit their lexi-cal head information from their head-children.
More specifically, an unlexicalized headnonterminal label is generated conditioning on the fully lexicalized parent nontermi-nal.
We denote the parameter class as follows:PH(H | ?
(P), wh, th) (4)6.3 The Subcategorization Parameter ClassWhen the model generates a head-child nonterminal for some lexicalized parent non-terminal, it also generates a kind of subcategorization frame (subcat) on either side ofthe head-child, with the following maximal context:PsubcatL( subcatL|?(?(H)),?(?
(P)), wh, th) (5)491Bikel Intricacies of Collins?
Parsing ModelS(sat?VBD)???????
?NP-A(John?NNP)NNP(John?NNP)JohnVP(sat?VBD)VBD(sat?VBD)satFigure 8A fully lexicalized tree.
The VP node is the head-child of S.PsubcatR( subcatR|?(?(H)),?(?
(P)), wh, th) (6)Probabilistically, it is as though these subcats are generated with the head-child, viaapplication of the chain rule, but they are conditionally independent.15 These subcatsmay be thought of as lists of requirements on a particular side of a head.
For example,in Figure 8, after the root node of the tree has been generated (see Section 6.10), thehead child VP is generated, conditioning on both the parent label S and the headwordof that parent, sat?VBD.
Before any modifiers of the head-child are generated, both aleft- and right-subcat frame are generated.
In this case, the left subcat is {NP-A} andthe right subcat is {}, meaning that there are no required elements to be generated onthe right side of the head.
Subcats do not specify the order of the required arguments.They are dynamically updated multisets: When a requirement has been generated, itis removed from the multiset, and subsequent modifiers are generated conditioningon the updated multiset.16The implementation of subcats in Collins?
parser is even more specific: Subcatsare multisets containing various numbers of precisely six types of items: NP-A, S-A,SBAR-A, VP-A, g, and miscellaneous.
The g indicates that a gap must be generated andis applicable only to Model 3.
Miscellaneous items include all nonterminals that weremarked as arguments in the training data that were not any of the other named types.There are rules for determining whether NPs, Ss, SBARs, and VPs are arguments, andthe miscellaneous arguments occur as the result of the argument-finding rule for PPs,which states that the first non-PRN, non-part-of-speech tag that occurs after the headof a PP should be marked as an argument, and therefore nodes that are not one of thefour named types can be marked.6.4 The Modifying Nonterminal Parameter ClassAs mentioned above, after a head-child and its left and right subcats are generated,modifiers are generated from the head outward, as indicated by the modifier nonter-minal indices in Figure 1.
A fully lexicalized nonterminal has three components: thenonterminal label, the headword, and the headword?s part of speech.
Fully lexicalizedmodifying nonterminals are generated in two steps to allow for the parameters to beindependently smoothed, which, in turn, is done to avoid sparse-data problems.
Thesetwo steps estimate the joint event of all three components using the chain rule.
In the15 Using separate steps to generate subcats on either side of the head allows not only for conditionalindependence between the left and right subcats, but also for these parameters to be separatelysmoothed from the head-generation parameter.16 Our parsing engine allows an arbitrary mechanism for storage and discharge of requirements: Theycan be multisets, ordered lists, integers (simply to constrain the number of requirements), or any othermechanism.
The mechanism used is determined at runtime.492Computational Linguistics Volume 30, Number 4NP??????????????????????????????????????NPNPB????JJshortNNgrassNPNPB????JJtallNNStrees,,CCandNPNPB???
?JJbushyNNSbushesFigure 9A tree containing both punctuation and conjunction.first step, a partially lexicalized version of the nonterminal is generated, consistingof the unlexicalized label plus the part of speech of its headword.
These partially lex-icalized modifying nonterminals are generated conditioning on the parent label, thehead label, the headword, the head tag, the current state of the dynamic subcat, anda distance metric.
Symbolically, the parameter classes arePL(L(t)i |?
(P), ?
(H), wh, th, subcatL,?L) (7)PR(R(t)i |?
(P), ?
(H), wh, th, subcatR,?R) (8)where ?
denotes the distance metric.17 As discussed above, one of the two componentsof this distance metric is the vi predicate.
The other is a predicate that simply reportswhether the current modifier is the first modifier being generated, that is, whether i =1.
The second step is to generate the headword itself, where, because of the chain rule,the conditioning context consists of everything in the histories of expressions (7) and (8)plus the partially lexicalized modifier.
As there are some interesting idiosyncrasies withthese headword-generation parameters, we describe them in more detail in Section 6.9.6.5 The Punctuation and Coordinating Conjunction Parameter Classes6.5.1 Inconsistent Model.
As discussed in Section 4.8, punctuation is raised to thehighest position in the tree.
This means that in some sense, punctuation acts verymuch like a coordinating conjunction, in that it ?conjoins?
the two siblings betweenwhich it sits.
Observing that it might be helpful for conjunctions to be generatedconditioning on both of their conjuncts, Collins introduced two new parameter classesin his thesis parser, Ppunc and PCC.18As per the definition of a coordinated phrase in Section 4.1, conjunction via aCC node or a punctuation node always occurs posthead (i.e., as a right-sibling of thehead).
Put another way, if a conjunction or punctuation mark occurs prehead, it is17 Throughout this article we use the notation L(w, t)i to refer to the three items that constitute a fullylexicalized left-modifying nonterminal, which are the unlexicalized label Li, its headword wLi , and itspart of speech tLi , and similarly for right modifiers.
We use L(t)i to refer to the two items Li and tLi of apartially lexicalized nonterminal.
Finally, when we do not wish to distinguish between a left and rightmodifier, we use M(w, t)i, M(t)i, and Mi.18 Collins?
thesis does not say what the back-off structure of these new parameter classes is, that is, howthey should be smoothed.
We have included this information in the complete smoothing table in theAppendix.493Bikel Intricacies of Collins?
Parsing Modelnot generated via this mechanism.19 Furthermore, even if there is arbitrary materialbetween the right conjunct and the head, the parameters effectively assume that theleft conjunct is always the head-child.
For example, in Figure 9, the rightmost NP(bushy bushes) is considered to be conjoined to the leftmost NP (short grass), which isthe head-child, even though there is an intervening NP (tall trees).The new parameters are incorporated into the model by requiring that all mod-ifying nonterminals be generated with two boolean flags: coord, indicating that thenonterminal is conjoined to the head via a CC, and punc, indicating that the nonter-minal is conjoined to the head via a punctuation mark.
When either or both of theseflags is true, the intervening punctuation or conjunction is generated via appropriateinstances of the Ppunc/PCC parameter classes.For example, the model generates the five children in Figure 9 in the followingorder: first, the head-child is generated, which is the leftmost NP (short grass), con-ditioning on the parent label and the headword and tag.
Then, since modifiers arealways generated from the head outward, the right-sibling of the head, which is thetall trees NP, is generated with both the punc and CC flags false.
Then, the rightmostNP (bushy bushes) is generated with both the punc and CC booleans true, since it isconsidered to be conjoined to the head-child and requires the generation of an in-tervening punctuation mark and conjunction.
Finally, the intervening punctuation isgenerated conditioning on the parent, the head, and the right conjunct, including theheadwords of the two conjoined phrases, and the intervening CC is similarly generated.A simplified version of the probability of generating all these children is summarizedas follows:p?H(NP | NP,grass,NN)?p?R(NP(trees,NNS),punc=0,coord=0 | NP,NP,grass,NN)?p?R(NP(bushes,NNS),punc=1,coord=1 | NP,NP,grass,NN)?p?punc(,(,) | NP,NP,NP, bushes, NNS,grass,NN)?p?CC(CC(and) | NP,NP,NP,bushes,NNS,grass,NN) (9)The idea is that using the chain rule, the generation of two conjuncts and that whichconjoins them is estimated as one large joint event.20This scheme of using flags to trigger the Ppunc and PCC parameters is problematic,at least from a theoretical standpoint, as it causes the model to be inconsistent.
Fig-ure 10 shows three different trees that would all receive the same probability fromCollins?
model.
The problem is that coordinating conjunctions and punctuation arenot generated as first-class words, but only as triggered from these punc and coordflags, meaning that the number of such intervening conjunctive items (and the orderin which they are to be generated) is not specified.
So for a given sentence/tree paircontaining a conjunction and/or a punctuation mark, there is an infinite number ofsimilar sentence/tree pairs with arbitrary amounts of ?conjunctive?
material betweenthe same two nodes.
Because all of these trees have the same, nonzero probability, thesum?T P(T), where T is a possible tree generated by the model, diverges, meaningthe model is inconsistent (Booth and Thompson 1973).
Another consequence of notgenerating posthead conjunctions and punctuation as first-class words is that they19 In fact, if punctuation occurs before the head, it is not generated at all?a deficiency in the parsingmodel that appears to be a holdover from the deficient punctuation handling in the model of Collins(1997).20 In (9), for clarity we have left out subcat generation and the use of Collins?
distance metric in theconditioning contexts.
We have also glossed over the fact that lexicalized modifying nonterminals areactually generated in two steps, using two differently smoothed parameters.494Computational Linguistics Volume 30, Number 4(a)NP??????????????????????NPNPBNNfire,,CCand,,ADVP...RBultimately(b)NP?????????NPNPBNNfire,,CCandADVP...RBultimately(c)NP???????
?NPNPBNNfireCCand,,ADVP...RBultimatelyFigure 10The Collins model assigns equal probability to these three trees.do not count when calculating the head-adjacency component of Collins?
distancemetric.When emulating Collins?
model, instead of reproducing the Ppunc and PCC param-eter classes directly in our parsing engine, we chose to use a different mechanism thatdoes not yield an inconsistent model but still estimates the large joint event that wasthe motivation behind these parameters in the first place.6.5.2 History Mechanism.
In our emulation of Collins?
model, we use the history,rather than the dedicated parameter classes PCC and Ppunc, to estimate the joint eventof generating a conjunction (or punctuation mark) and its two conjuncts.
The firstbig change that results is that we treat punctuation preterminals and CCs as first-classobjects, meaning that they are generated in the same way as any other modifyingnonterminal.The second change is a little more involved.
First, we redefine the distance met-ric to consist solely of the vi predicate.
Then, we add to the conditioning contexta mapped version of the previously generated modifier according to the following495Bikel Intricacies of Collins?
Parsing Modelmapping function:?
(Mi) =??????
?+START+ if i = 0CC if Mi = CC+PUNC+ if Mi = , or Mi = :+OTHER+ otherwise(10)where Mi is some modifier Li or Ri.21 So, the maximal context for our modifyingnonterminal parameter class is now defined as follows:PM(M(t)i |?
(P), ?
(H), wh, th, subcatside, vi(Mi), ?
(Mi?1), side)(11)where side is a boolean-valued event that indicates whether the modifier is on the left orright side of the head.
By treating CC and punctuation nodes as first-class nonterminalsand by adding the mapped version of the previously generated modifier, we have, inone fell swoop, incorporated the ?no intervening?
component of Collins?
distancemetric (the i = 0 case of the delta function) and achieved an estimate of the jointevent of a conjunction and its conjuncts, albeit with different dependencies, that is, adifferent application of the chain rule.
To put this parameterization change in sharprelief, consider the abstract tree structureP????????????????.
.
.
.
.
.
H CC R1To a first approximation, under the old parameterization, the conjunction of some nodeR1 with a head H and a parent P looked like this:p?H(H |P) ?
p?R(R1, coord=1 |P, H) ?
p?CC(CC |P, H, R1)whereas under the new parameterization, it looks like this:p?H(H |P) ?
p?R(CC |P, H, +START+) ?
p?R(R1 |P, H, CC)Either way, the probability of the joint conditional event {H, CC, R1 |P} is being esti-mated, but with the new method, there is no need to add two new specialized pa-rameter classes, and the new method does not introduce inconsistency into the model.Using less simplification, the probability of generating the five children of Figure 9 isnowp?H(NP | NP,grass,NN)?p?M(NP(trees, NNS) | NP, NP, grass, NN, {}, false, +START+, right)?p?M(,(,, ,) | NP, NP, grass, NN, {}, false, +OTHER+, right)?p?M(CC(and, CC) | NP, NP, grass, NN, {}, false, +PUNC+, right)?p?M(NP(bushes, NNS) | NP, NP, grass, NN, {}, false, CC, right) (12)21 Originally, we had an additional mechanism that attempted to generate punctuation and conjunctionswith conditional independence.
One of our reviewers astutely pointed out that the mechanism led to adeficient model (the very thing we have been trying to avoid), and so we have subsequently removedit from our model.
The removal leads to a 0.05% absolute reduction in F-measure (which in this case isalso a 0.05% relative increase in error) on sentences of length ?
40 words in Section 00 of the PennTreebank.
As this difference is not at all statistically significant (according to a randomized stratifiedshuffling test [Cohen 1995]), all evaluations reported in this article are with the original model.496Computational Linguistics Volume 30, Number 4As shown in Section 8.1, this new parameterization yields virtually identical perfor-mance to that of the Collins model.226.6 The Base NP Model: A Model unto ItselfAs we have already seen, there are several ways in which base NPs are exceptionalin Collins?
parsing model.
This is partly because the flat structure of base NPs in thePenn Treebank suggested the use of a completely different model by which to generatethem.
Essentially, the model for generating children of NPB nodes is a ?bigrams ofnonterminals?
model.
That is, it looks a great deal like a bigram language model,except that the items being generated are not words, but lexicalized nonterminals.Heads of NPB nodes are generated using the normal head-generation parameter, butmodifiers are always generated conditioning not on the head, but on the previouslygenerated modifier.
That is, we modify expressions (7) and (8) to bePL,NPB(L(t)i |P, L(w, t)i?1) (13)PR,NPB(R(t)i |P, R(w, t)i?1) (14)Though it is not entirely spelled out in his thesis, Collins considers the previouslygenerated modifier to be the head-child, for all intents and purposes.
Thus, the subcatand distance metrics are always irrelevant, since it is as though the current modifier isright next to the head.23 Another consequence of this is that NPBs are never consideredto be coordinated phrases (as mentioned in Section 4.12), and thus CCs dominatedby NPB are never generated using a PCC parameter; instead, they are generated us-ing a normal modifying-nonterminal parameter.
Punctuation dominated by NPB, onthe other hand, is still, as always, generated via Ppunc parameters, but crucially, themodifier is always conjoined (via the punctuation mark) to the ?pseudohead?
thatis the previously generated modifier.
Consequently, when some right modifier Ri isgenerated, the previously generated modifier on the right side of the head, Ri?1, isnever a punctuation preterminal, but always the previous ?real?
(i.e., nonpunctuation)preterminal.24Base NPs are also exceptional with respect to determining chart item equality, thecomma-pruning rule, and general beam pruning (see Section 7.2 for details).6.7 Parameter Classes for Priors on Lexicalized NonterminalsTwo parameter classes that make their appearance only in Appendix E of Collins?thesis are those that compute priors on lexicalized nonterminals.
These priors areused as a crude proxy for the outside probability of a chart item (see Baker [1979] andLari and Young [1990] for full descriptions of the Inside?Outside algorithm).
Previouswork (Goodman 1997) has shown that the inside probability alone is an insufficientscoring metric when comparing chart items covering the same span during decodingand that some estimate of the outside probability of a chart item should be factoredinto the score.
A prior on the root (lexicalized) nonterminal label of the derivationforest represented by a particular chart item is used for this purpose in Collins?
parser.22 As described in Bikel (2002), our parsing engine allows easy experimentation with a wide variety ofdifferent generative models, including the ability to construct history contexts from arbitrary numbersof previously generated modifiers.
The mapping function delta and the transition function taupresented in this section are just two examples of this capability.23 This is the main reason that the cv (?contains verb?)
predicate is always false for NPBs, as thatpredicate applies only to material that intervenes between the current modifier and the head.24 Interestingly, unlike in the regular model, punctuation that occurs to the left of the head is generatedwhen it occurs within an NPB.
Thus, this particular?albeit small?deficiency of Collins?
punctuationhandling does not apply to the base NP model.497Bikel Intricacies of Collins?
Parsing ModelThe prior of a lexicalized nonterminal M(w, t) is broken down into two separateestimates using parameters from two new classes, Ppriorw and PpriorNT :Pprior(M(w, t)) = Ppriorw(w, t) ?
PpriorNT(M |w, t)where p?
(M |w, t) is smoothed with p?
(M | t) and estimates using the parameters of thePpriorw class are unsmoothed.6.8 Smoothing WeightsMany of the parameter classes in Collins?
model?and indeed, in most statistical pars-ing models?define conditional probabilities with very large conditioning contexts.
Inthis case, the conditioning contexts represent some subset of the history of the gen-erative process.
Even if there were orders of magnitude more training data available,the large size of these contexts would cause horrendous sparse-data problems.
Thesolution is to smooth these distributions that are made rough primarily by the abun-dance of zeros.
Collins uses the technique of deleted interpolation, which smoothesthe distributions based on full contexts with those from coarser models that use lessof the context, by successively deleting elements from the context at each back-offlevel.
As a simple example, the head parameter class smoothes PH0(H |P, wh, th) withPH1(H |P, th) and PH2(H |P).
For some conditional probability p(A |B), let us call thereduced context at the ith back-off level ?i(B), where typically ?0(B) = B.
Each esti-mate in the back-off chain is computed via maximum-likelihood (ML) estimation, andthe overall smoothed estimate with n back-off levels is computed using n?
1 smooth-ing weights, denoted ?0, .
.
.
,?n?2.
These weights are used in a recursive fashion: Thesmoothed version e?i = p?i(A |?i(B)) of an unsmoothed ML estimate ei = p?i(A |?i(B)) atback-off level i is computed via the formulae?i = ?iei + (1 ?
?i)e?i+1, 0 ?
i < n ?
1, e?n?1 = en?1 (15)So, for example, with three levels of back-off, the overall smoothed estimate would bedefined ase?0 = ?0e0 + (1 ?
?0)[?1e1 + (1 ?
?1)e2](16)It is easy to prove by structural induction that if0 ?
?i ?
1 and?Ap?i(A |?i(B)) = 1, 0 ?
i < n ?
1then?Ap?0(A |?0(B)) = 1 (17)Each smoothing weight can be conceptualized as the confidence in the estimatewith which it is being multiplied.
These confidence values can be derived in a numberof sensible ways; the technique used by Collins was adapted from that used in Bikelet al (1997), which makes use of a quantity called the diversity of the history context(Witten and Bell 1991), which is equal to the number of unique futures observed intraining for that history context.6.8.1 Deficient Model.
As previously mentioned, n back-off levels require n?1 smooth-ing weights.
Collins?
parser effectively uses n weights, because the estimator always498Computational Linguistics Volume 30, Number 4adds an extra, constant-valued estimate to the back-off chain.
Collins?
parser hard-codes this extra value to be a vanishingly small (but nonzero) ?probability?
of 10?19,resulting in smoothed estimates of the forme?0 = ?0e0 + (1 ?
?0)[?1e1 + (1 ?
?1)[?2e2 + (1 ?
?2) ?
10?19]](18)when there are three levels of back-off.
The addition of this constant-valued en =10?19 causes all estimates in the parser to be deficient, as it ends up throwing awayprobability mass.
More formally, the proof leading to equation (17) no longer holds:The ?distribution?
sums to less than one (there is no history context in the model forwhich there are 1019 possible outcomes).256.8.2 Smoothing Factors and Smoothing Terms.
The formula given in Collins?
thesisfor computing smoothing weights is?i =cici + 5uiwhere ci is the count of the history context ?i(B) and ui is the diversity of that context.26The multiplicative constant five is used to give less weight to the back-off levelswith more context and was optimized by looking at overall parsing performance onthe development test set, Section 00 of the Penn Treebank.
We call this constant thesmoothing factor and denote it as ff .
As it happens, the actual formula for computingsmoothing weights in Collins?
implementation is?i ={ cici+ft+ff uiif ci > 00 otherwise(19)where ft is an unmentioned smoothing term.
For every parameter class except thesubcat parameter class and Ppriorw , ft = 0 and ff = 5.0.
For the subcat parameter class,ft = 5.0 and ff = 0.
For Ppriorw , ft = 1.0 and ff = 0.0.
This curiously means that diversityis not used at all when smoothing subcat-generation probabilities.27The second case in (19) handles the situation in which the history context was neverobserved in training, that is, where ci = ui = 0, which would yield an undefined value25 Collins used this technique to ensure that even futures that were never seen with an observed historycontext would still have some probability mass, albeit a vanishingly small one (Collins, personalcommunication, January 2003).
Another commonly used technique would be to back off to the uniformdistribution, which has the desirable property of not producing deficient estimates.
As with all of thetreebank- or model-specific aspects of the Collins parser, our engine uses equation (16) or (18)depending on the value of a particular run-time setting.26 The smoothing weights can be viewed as confidence values for the probability estimates with whichthey are multiplied.
The Witten-Bell technique crucially makes use of the quantity ni =ciui, the averagenumber of transitions from the history context ?i(B) to a possible future.
With a little algebraicmanipulation, we have?i =nini + 5a quantity that is at its maximum when ni = ci and at its minimum when ni = 1, that is, when everyfuture observed in training was unique.
This latter case represents when the model is most?uncertain,?
in that the transition distribution from ?i(B) is uniform and poorly trained (oneobservation per possible transition).
Because these smoothing weights measure, in some sense, thecloseness of the observed distribution to uniform, they can be viewed as proxies for the entropy of thedistribution p(?
|?i(B)).27 As mentioned above, the Ppriorw parameters are unsmoothed.
However, as a result of the deficientestimation method, they still have an associated lambda value, the computation of which, just like thesubcat-generation probability estimates, does not make use of diversity.499Bikel Intricacies of Collins?
Parsing ModelTable 1Back-off levels for PLw /PRw , the modifier headword generation parameter classes.
wLi and tLiare, respectively, the headword and its part of speech of the nonterminal Li.
This table isbasically a reproduction of the last column of Table 7.1 in Collins?
thesis.Back-off PLw(wLi | .
.
.
)level PRw(wRi | .
.
.
)0 ?
(Li), tLi , coord, punc, ?
(P), ?
(H), wh, th,?L, subcat1 ?
(Li), tLi , coord, punc, ?
(P), ?
(H), th,?L, subcat2 tLiTable 2Our new parameter class for the generation of headwords of modifying nonterminals.Back-off level PMw(wMi | .
.
.
)0 ?
(Mi), tMi , ?
(P), ?
(H), wh, th, subcatside, vi(Mi), ?
(Mi?1), side1 ?
(Mi), tMi , ?
(P), ?
(H), th, subcatside, vi(Mi), ?
(Mi?1), side2 tMiwhen ft = 0.
In such situations, making ?i = 0 throws all remaining probability massto the smoothed back-off estimate, e?i+1.
This is a crucial part of the way smoothingis done: If a particular history context ?i(B) has never been observed in training, thesmoothed estimate using less context, ?i+1(B), is simply substituted as the ?best guess?for the estimate using more context; that is, e?i = e?i+1.286.9 Modifier Head-Word GenerationAs mentioned in Section 6.4, fully lexicalized modifying nonterminals are generated intwo steps.
First, the label and part-of-speech tag are generated with an instance of PLor PR.
Next, the headword is generated via an instance of one of two parameter classes,PLw or PRw .
The back-off contexts for the smoothed estimates of these parameters arespecified in Table 1.
Notice how the last level of back-off is markedly different fromthe previous two levels in that it removes nearly all the elements of the history: Inthe face of sparse data, the probability of generating the headword of a modifyingnonterminal is conditioned only on its part of speech.6.9.1 Smoothing and the Last Level of Back-Off.
Table 1 is misleading, however.
Inorder to capture the most data for the crucial last level of back-off, Collins uses wordsthat occur on either side of the headword, resulting in a general estimate p?
(w | t), asopposed to p?Lw(wLi | tLi).
Accordingly, in our emulation of Collins?
model, we replacethe left- and right-word parameter classes with a single modifier headword generationparameter class that, as with (11), includes a boolean side component that is deletedfrom the last level of back-off (see Table 2).Even with this change, there is still a problem.
Every headword in a lexicalizedparse tree is the modifier of some other headword?except the word that is the head ofthe entire sentence (i.e., the headword of the root nonterminal).
In order to properlyduplicate Collins?
model, an implementation must take care that the P(w | t) modelincludes counts for these important headwords.2928 This fact is crucial in understanding how little the Collins parsing model relies on bilexical statistics, asdescribed in Section 8.2 and the supporting experiment shown in Table 6.29 In our implementation, we add such counts by having our trainer generate a ?fake?
modifier event in500Computational Linguistics Volume 30, Number 4S(sat?VBD)????????????????????NP-A(Fido?NNP)NPB(Fido?NNP)???
?JJFaithfulNNPFidoADVP(faithfully?RB)RBfaithfullyVP(sat?VBD)VBDsatFigure 11The low-frequency word Fido is mapped to +UNKNOWN+, but only when it is generated, notwhen it is conditioned upon.
All the nonterminals have been lexicalized (except forpreterminals) to show where the heads are.6.9.2 Unknown-Word Mapping.
As mentioned above, instead of mapping every low-frequency word in the training data to some special +UNKNOWN+ token, Collins?
trainerinstead leaves the training data untouched and selectively maps words that appear inthe back-off levels of the parameters from the PLw and PRw parameter classes.
Rathercuriously, the trainer maps only words that appear in the futures of these parameters,but never in the histories.
Put another way, low-frequency words are generated as+UNKNOWN+ but are left unchanged when they are conditioned upon.
For example, inFigure 11, where we assume Fido is a low-frequency word, the trainer would derivecounts for the smoothed parameterpLw(+UNKNOWN+ | NP-A, NNP, coord = 0, punc = 0, S, VP, sat, VBD, .
.
.
)(20)However, when collecting events that condition on Fido, such as the parameterspL(JJ(JJ) | NPB, NNP, Fido)pLw(Faithful | JJ, JJ, NPB, NNP, Fido)the word would not be mapped.This strange mapping scheme has some interesting consequences.
First, imaginewhat happens to words that are truly unknown, that never occurred in the trainingdata.
Such words are mapped to the +UNKNOWN+ token outright before parsing.
When-ever the parser estimates a probability with such a truly unknown word in the history,it will necessarily throw all probability mass to the backed-off estimate (?e1 in our ear-lier notation), since +UNKNOWN+ effectively never occurred in a history context duringtraining.The second consequence is that the mapping scheme yields a ?superficient?30model, if all other parts of the model are probabilistically sound (which is actuallywhich the observed lexicalized root nonterminal is considered a modifier of +TOP+, the hiddennonterminal that is the parent of the observed root of every tree (see Section 6.10 for details on the+TOP+ nonterminal).30 The term deficient is used to denote a model in which one or more estimated distributions sums to lessthan 1.
We use the term superficient to denote a model in which one or more estimated distributionssums to greater than 1.501Bikel Intricacies of Collins?
Parsing ModelTable 3Back-off structure for PTOPNT and PTOPw , which estimate the probability of generating H(w, t) asthe root nonterminal of a parse tree.
PTOPNT is unsmoothed.
n/a: not applicable.Back-off level PTOPNT (H(t) | .
.
.)
PTOPw(w | .
.
.
)0 +TOP+ t, H, +TOP+1 n/a tnot the case here).
With a parsing model such as Collins?
that uses bilexical dependen-cies, generating words in the course of parsing is done very much as it is in a bigramlanguage model: Every word is generated conditioning on some previously generatedword, as well as some hidden material.
The only difference is that the word beingconditioned upon is often not the immediately preceding word in the sentence.
How-ever, one could plausibly construct a consistent bigram language model that generateswords with the same dependencies as those in a statistical parser that uses bilexicaldependencies derived from head-lexicalization.Collins (personal communication, January 2003) notes that his parser?s unknown-word-mapping scheme could be made consistent if one were to add a parameter classthat estimated p?
(w | +UNKNOWN+), where w ?
VL ?
{+UNKNOWN+}.
The values of theseestimates for a given sentence would be constant across all parses, meaning that the?superficiency?
of the model would be irrelevant when determining arg maxTP(T |S).6.10 The top Parameter ClassesIt is assumed that all trees that can be generated by the model have an implicit non-terminal +TOP+ that is the parent of the observed root.
The observed lexicalized rootnonterminal is generated conditioning on +TOP+ (which has a prior probability of 1.0)using a parameter from the class PTOP.
This special parameter class is mentioned in afootnote in chapter 7 of Collins?
thesis.
There are actually two parameter classes usedto generated observed roots, one for generating the partially lexicalized root nonter-minal, which we call PTOPNT , and the other for generating the headword of the entiresentence, which we call PTOPw .
Table 3 gives the unpublished back-off structure ofthese two additional parameter classes.Note that PTOPw backs off to simply estimating p?
(w | t).
Technically, it should beestimating p?NT(w | t), which is to say the probability of a word?s occurring with a tagin the space of lexicalized nonterminals.
This is different from the last level of back-offin the modifier headword parameter classes, which is effectively estimating p?
(w | t) inthe space of lexicalized preterminals.
The difference is that in the same sentence, thesame headword can occur with the same tag in multiple nodes, such as sat in Figure8, which occurs with the tag VBD three times (instead of just once) in the tree shownthere.
Despite this difference, Collins?
parser uses counts from the (shared) last levelof back-off of the PLw and PRw parameters when delivering e1 estimates for the PTOPwparameters.
Our parsing engine emulates this ?count sharing?
for PTOPw by default,by sharing counts from our PMw parameter class.7.
DecodingParsing, or decoding, is performed via a probabilistic version of the CKY chart-parsing algorithm.
As with normal CKY, even though the model is defined in a top-down, generative manner, decoding proceeds bottom-up.
Collins?
thesis gives a pseu-502Computational Linguistics Volume 30, Number 4docode version of his algorithm in an appendix.
This section contains a few practicaldetails.7.1 Chart Item EqualitySince the goal of the decoding process is to determine the maximally likely theory,if during decoding a proposed chart item is equal (or, technically, equivalent) to anitem that is already in the chart, the one with the greater score survives.
Chart itemequality is closely tied to the generative parameters used to construct theories: We wantto treat two chart items as unequal if they represent derivation forests that would beconsidered unequal according to the output elements and conditioning contexts of theparameters used to generate them, subject to the independence assumptions of themodel.
For example, for two chart items to be considered equal, they must have thesame label (the label of the root of their respective derivation forests?
subtrees), thesame headword and tag, and the same left and right subcat.
They must also have thesame head label (that is, label of the head-child).If a chart item?s root label is an NP node, its head label is most often an NPB node,given the ?extra?
NP levels that are added during preprocessing to ensure that NPBnodes are always dominated by NP nodes.
In such cases, the chart item will contain aback pointer to the chart item that represents the base NP.
Curiously, however, Collins?implementation considers the head label of the NP chart item not to be NPB, but ratherthe head label of the NPB chart item.
In other words, to get the head label of an NP chartitem, one must ?peek through?
the NPB and get at the NPB?s head label.
Presumably,this was done as a consideration for the NPB nodes?
being ?extra?
nodes, in some sense.It appears to have little effect on overall parsing accuracy, however.7.2 PruningIdeally, every parse theory could be kept in the chart, and when the root symbol hasbeen generated for all theories, the top-ranked one would ?win.?
In order to speedthings up, Collins employs three different types of pruning.
The first form of pruningis to use a beam: The chart memoizes the highest-scoring theory in each span, and if aproposed chart item for that span is not within a certain factor of the top-scoring item,it is not added to the chart.
Collins reports in his thesis that he uses a beam width of105.
As it happens, the beam width for his thesis experiments was 104.
Interestingly,there is a negligible difference in overall parsing accuracy when this wider beam isused (see Table 5).
An interesting modification to the standard beam in Collins?
parseris that for chart items representing NP or NP-A derivations with more than one child,the beam is expanded to be 104 ?
e3.
We suspect that Collins made this modificationafter he added the base NP model, to handle the greater perplexity associated withNPs.The second form of pruning employed is a comma constraint.
Collins observedthat in the Penn Treebank data, 96% of the time, when a constituent contained acomma, the word immediately following the end of the constituent?s span was eithera comma or the end of the sentence.
So for speed reasons, the decoder rejects alltheories that would generate constituents that violate this comma constraint.31 Thereis a subtlety to Collins?
implementation of this form of pruning, however.
Commasare quite common within parenthetical phrases.
Accordingly, if a comma in an input31 If one generates commas as first-class words, as we have done, one must take great care in applyingthis comma constraint, for otherwise, chart items that represent partially completed constituents (i.e.,constituents for which not all modifiers have been generated) may be incorrectly rejected.
This isespecially important for NPB constituents.503Bikel Intricacies of Collins?
Parsing ModelTable 4Overall parsing results using only details found in Collins (1997, 1999).
The first two linesshow the results of Collins?
parser and those of our parser in its ?complete?
emulation mode(i.e., including unpublished details).
All reported scores are for sentences of length ?
40words.
LR (labeled recall) and LP (labeled precision) are the primary scoring metrics.
CBs isthe number of crossing brackets.
0 CBs and ?
2 CBs are the percentages of sentences with 0and ?
2 crossing brackets, respectively.
F (the F-measure) is the evenly weighted harmonicmean of precision and recall, or LP?LR12 (LP+LR).Performance on Section 00Model LR LP CBs 0 CBs ?
2 CBs FCollins?
Model 2 89.75 90.19 0.77 69.10 88.31 89.97Baseline (Model 2 emulation) 89.89 90.14 0.78 68.82 89.21 90.01Clean-room Model 2 88.85 88.97 0.92 65.55 87.06 88.91sentence occurs after an opening parenthesis and before a closing parenthesis or theend of the sentence, it is not considered a comma for the purposes of the commaconstraint.
Another subtlety is that the comma constraint should effectively not beemployed when pursuing theories of an NPB subtree.
As it turns out, using the commaconstraint also affects accuracy, as shown in Section 8.1.The final form of pruning employed is rather subtle: Within each cell of the chartthat contains items covering some span of the sentence, Collins?
parser uses bucketsof items that share the same root nonterminal label for their respective derivations.Only 100 of the top-scoring items covering the same span with the same nonterminallabel are kept in a particular bucket, meaning that if a new item is proposed and thereare already 100 items covering the same span with the same label in the chart, then itwill be compared to the lowest-scoring item in the bucket.
If it has a higher score, itwill be added to the bucket and the lowest-scoring item will be removed; otherwise,it will not be added.
Apparently, this type of pruning has little effect, and so we havenot duplicated it in our engine.327.3 Unknown Words and Parts of SpeechWhen the parser encounters an unknown word, the first-best tag delivered by Ratna-parkhi?s (1996) tagger is used.
As it happens, the tag dictionary built up when trainingcontains entries for every word observed, even low-frequency words.
This means thatduring decoding, the output of the tagger is used only for those words that are trulyunknown, that is, that were never observed in training.
For all other words, the chartis seeded with a separate item for each tag observed with that word in training.8.
Evaluation8.1 Effects of Unpublished DetailsIn this section we present the results of effectively doing a ?clean-room?
implemen-tation of Collins?
parsing model, that is, using only information available in (Collins1997, 1999), as shown in Table 4.The clean-room model has a 10.6% increase in F-measure error compared toCollins?
parser and an 11.0% increase in F-measure error compared to our enginein its complete emulation of Collins?
Model 2.
This is comparable to the increase in32 Although we have implemented a version of this type of pruning that limits the number of items thatcan be collected in any one cell, that is, the maximum number of items that cover a particular span.504Computational Linguistics Volume 30, Number 4Table 5Effects of independently removing or changing individual details on overall parsingperformance.
All reported scores are for sentences of length ?
40 words.
?With beam width =105, processing time was 3.36 times longer than with standard beam (104).
?No count sharingwas performed for PTOPw (see Section 6.10), and p(w | t) estimates were side-specific (seeSection 6.9.1).
See Table 4 for definitions of column headings.Performance on Section 00Model description LR LP CBs 0 CBs ?
2 CBs FCollins?
Model 2 89.75 90.19 0.77 69.10 88.31 89.97Baseline (Model 2 emulation) 89.89 90.14 0.78 68.82 89.21 90.01Unknown word threshold = 5 and unknownwords handled in uniform way (see Sec-tion 5.3)89.94 90.22 0.77 68.99 89.27 90.08No training trees skipped (see Section 5.2) 89.85 90.12 0.78 68.71 89.16 89.98Beam width = 105?
89.90 90.14 0.78 68.93 89.16 90.02Nondeficient estimation (see Section 6.8.1) 89.75 90.00 0.80 68.82 88.88 89.87No comma constraint (see Section 7.2) 89.52 89.80 0.84 68.09 88.20 89.66No universal p(w | t) model?
89.40 89.17 0.88 66.14 87.92 89.28Clean-room Model 2 88.85 88.97 0.92 65.55 87.06 88.91error seen when removing such published features as the verb-intervening componentof the distance metric, which results in an F-measure error increase of 9.86%, or thesubcat feature, which results in a 7.62% increase in F-measure error.33 Therefore, whilethe collection of unpublished details presented in Sections 4?7 is disparate, in totothose details are every bit as important to overall parsing performance as certain ofthe published features.This does not mean that all the details are equally important.
Table 5 shows theeffect on overall parsing performance of independently removing or changing certainof the more than 30 unpublished details.34 Often, the detrimental effect of a particularchange is quite insignificant, even by the standards of the performance-obsessed worldof statistical parsing, and occasionally, the effect of a change is not even detrimental atall.
That is why we do not claim the importance of any single unpublished detail,but rather that of their totality, given that several of the unpublished details are,most likely, interacting.
However, we note that certain individual details, such as theuniversal p(w | t) model, do appear to have a much more marked effect on overallparsing accuracy than others.8.2 Bilexical DependenciesThe previous section accounts for the noticeable effects of all the unpublished detailsof Collins?
model.
But what of the details that were published?
In chapter 8 of histhesis, Collins gives an account on the motivation of various features of his model,including the distance metric, the model?s use of subcats (and their interaction with thedistance metric), and structural versus semantic preferences.
In the discussion of thislast issue, Collins points to the fact that structural preferences?which, in his model, are33 These F-measures and the differences between them were calculated from experiments presented inCollins (1999, page 201); these experiments, unlike those on which our reported numbers are based,were on all sentences, not just those of length ?
40 words.
As Collins notes, removing both the distancemetric and subcat features results in a gigantic drop in performance, since without both of thesefeatures, the model has no way to encode the fact that flatter structures should be avoided in severalcrucial cases, such as for PPs, which tend to prefer one argument to the right of their head-children.34 As a reviewer pointed out, the use of the comma constraint is a ?published?
detail.
However, thespecifics of how certain commas do not apply to the constraint is an ?unpublished detail,?
asmentioned in Section 7.2.505Bikel Intricacies of Collins?
Parsing ModelTable 6Number of times our parsing engine was able to deliver a probability for the various levels ofback-off of the modifier-word generation model, PMw , when testing on Section 00, havingtrained on Sections 02?21.
In other words, this table reports how often a context in theback-off chain of PMw that was needed during decoding was observed in training.Back-off level Number of accesses Percentage0 3,257,309 1.51 24,294,084 11.02 191,527,387 87.4Total 219,078,780 100.0modeled primarily by the PL and PR parameters?often provide the right informationfor disambiguating competing analyses, but that these structural preferences may be?overridden?
by semantic preferences.
Bilexical statistics (Eisner 1996), as representedby the maximal context of the PLw and PRw parameters, serve as a proxy for suchsemantic preferences, where the actual modifier word (as opposed to, say, merely itspart of speech) indicates the particular semantics of its head.
Indeed, such bilexicalstatistics were widely assumed for some time to be a source of great discriminativepower for several different parsing models, including that of Collins.However, Gildea (2001) reimplemented Collins?
Model 1 (essentially Model 2 butwithout subcats) and altered the PLw and PRw parameters so that they no longer hadthe top level of context that included the headword (he removed back-off level 0,as depicted in Table 1).
In other words, Gildea removed all bilexical statistics fromthe overall model.
Surprisingly, this resulted in only a 0.45% absolute reduction inF-measure (3.3% relative increase in error).
Unfortunately, this result was not entirelyconclusive, in that Gildea was able to reimplement Collins?
baseline model only par-tially, and the performance of his partial reimplementation was not quite as good asthat of Collins?
parser.35Training on Sections 02?21, we have duplicated Gildea?s bigram-removal exper-iment, except that our chosen test set is Section 00 instead of Section 23 and ourchosen model is the more widely used Model 2.
Using the mode that most closelyemulates Collins?
Model 2, with bigrams, our engine obtains a recall of 89.89% and aprecision of 90.14% on sentences of length ?
40 words (see Table 8, Model Mtw,tw).Without bigrams, performance drops only to 89.49% on recall, 89.95% on precision?an exceedingly small drop in performance (see Table 8, Model Mtw,t).
In an additionalexperiment, we have examined the number of times that the parser is able, while de-coding Section 00, to deliver a requested probability for the modifier-word generationmodel using the increasingly less-specific contexts of the three back-off levels.
Theresults are presented in Table 6.
Back-off level 0 indicates the use of the full historycontext, which contains the head-child?s headword.
Note that probabilities making useof this full context, that is, making use of bilexical dependencies, are available only1.49% of the time.
Combined with the results from the previous experiment, this sug-gests rather convincingly that such statistics are far less significant than once thoughtto the overall discriminative power of Collins?
models, confirming Gildea?s result forModel 2.3635 The reimplementation was necessarily only partial, as Gildea did not have access to all theunpublished details of Collins?
models that are presented in this article.36 On a separate note, it may come as a surprise that the decoder needs to access more than 219 millionprobabilities during the course of parsing the 1,917 sentences of Section 00.
Among other things, this506Computational Linguistics Volume 30, Number 4Table 7Results on Section 00 with simplified head rules.
The baseline model is our engine in itsclosest possible emulation of Collins?
Model 2.
See Table 4 for definitions of column headings.LR LP CBs 0 CBs ?
2 CBs FCollins?
Model 2 89.75 90.19 0.77 69.10 88.31 89.97Baseline (Model 2 emulation) 89.89 90.14 0.78 68.82 89.21 90.01Simplified head rules 88.55 88.80 0.86 67.25 87.42 88.678.3 Choice of HeadsIf not bilexical statistics, then surely, one might think, head-choice is critical to theperformance of a head-driven lexicalized statistical parsing model.
Partly to this end,in Chiang and Bikel (2002), we explored methods for recovering latent informationin treebanks.
The second half of that paper focused on a use of the Inside?Outsidealgorithm to reestimate the parameters of a model defined over an augmented treespace, where the observed data were considered to be the gold-standard labeled brack-etings found in the treebank, and the hidden data were considered to be the head-lexicalizations, one of the most notable tree augmentations performed by modernstatistical parsers.
These expectation maximization (EM) experiments were motivatedby the desire to overcome the limitations imposed by the heuristics that have beenheretofore used to perform head-lexicalization in treebanks.
In particular, it appearedthat the head rules used in Collins?
parser had been tweaked specifically for the En-glish Penn Treebank.
Using EM would mean that very little effort would need to bespent on developing head rules, since EM could take an initial model that used simpleheuristics and optimize it appropriately to maximize the likelihood of the unlexical-ized (observed) training trees.
To test this, we performed experiments with an initialmodel trained using an extremely simplified head-rule set in which all rules were ofthe form ?if the parent is X, then choose the left/rightmost child.?
A surprising sideresult was that even with this simplified set of head-rules, overall parsing performancestill remained quite high.
Using our simplified head-rule set for English, our engine inits ?Model 2 emulation mode?
achieved a recall of 88.55% and a precision of 88.80%for sentences of length ?40 words in Section 00 (see Table 7).
So contrary to our ex-pectations, the lack of careful head-choice is not crippling in allowing the parser todisambiguate competing theories and is a further indication that semantic preferences,as represented by conditioning on a headword, rarely override structural ones.8.4 Lexical Dependencies MatterGiven that bilexical dependencies are almost never used and have a surprisingly smalleffect on overall parsing performance, and given that the choice of head is not terriblycritical either, one might wonder what power, if any, head-lexicalization is providing.The answer is that even when one removes bilexical dependencies from the model,there are still plenty of lexico-structural dependencies, that is, structures being gen-erated conditioning on headwords and headwords being generated conditioning onstructures.To test the effect of such lexicostructural dependencies in our lexicalized PCFG-style formalism, we experimented with the removal of the head tag th and/or thehead word wh from the conditioning contexts of the PMw and PM parameters.
The re-certainly points to the utility of caching probabilities (the 219 million are tokens, not types).507Bikel Intricacies of Collins?
Parsing ModelTable 8Parsing performance with various models on Section 00 of the Penn Treebank.
PM is theparameter class for generating partially lexicalized modifying nonterminals (a nonterminallabel and part of speech).
PMw is the parameter class that generates the headword of amodifying nonterminal.
Together, PM and PMw generate a fully lexicalized modifyingnonterminal.
The check marks indicate the inclusion of the headword wh and its part ofspeech th of the lexicalized head nonterminal H(th, wh) in the conditioning contexts of PM andPMw .
See Table 4 for definitions of the remaining column headings.Parameter class PM PMw ScoreConditioning on th wh th wh LR LP CBs 0 CBs ?
2 CBs FModel Mtw,tw     89.89 90.14 0.78 68.82 89.21 90.01Mtw,t    89.49 89.95 0.80 67.98 88.82 89.72Mt,t   88.20 88.89 0.91 65.00 87.13 88.54Mtw,?
  89.24 89.86 0.81 66.80 88.76 89.55Mt,?
 88.01 88.96 0.91 63.93 86.91 88.48M?,?
87.01 88.75 0.96 61.08 86.00 87.87sults are shown in Table 8.
Model Mtw,tw shows our baseline, and Model M?,?
showsthe effect of removing all dependence on the headword and its part of speech, with theother models illustrating varying degrees of removing elements from the two parame-ter classes?
conditioning contexts.
Notably, including the headword wh in or removingit from the PM contexts appears to have a significant effect on overall performance, asshown by moving from Model Mtw,t to Model Mt,t and from Model Mtw,?
to ModelMt,?.
This reinforces the notion that particular headwords have structural preferences,so that making the PM parameters dependent on headwords would capture such pref-erences.
As for effects involving dependence on the head tag th, observe that movingfrom Model Mtw,t to Model Mtw,?
results in a small drop in both recall and precision,whereas making an analogous move from Model Mt,t to Model Mt,?
results in a dropin recall, but a slight gain in precision (the two moves are analogous in that in bothcases, th is dropped from the context of PMw ).
It is not evident why these two movesdo not produce similar performance losses, but in both cases, the performance dropsare small relative to those observed when eliminating wh from the conditioning con-texts, indicating that headwords matter far more than parts of speech for determiningstructural preferences, as one would expect.9.
ConclusionWe have documented what we believe is the complete set of heretofore unpublisheddetails Collins used in his parser, such that, along with Collins?
(1999) thesis, this article contains all information necessary to duplicate Collins?
benchmark results.Indeed, these as-yet-unpublished details account for an 11% relative increase in er-ror from an implementation including all details to a clean-room implementation ofCollins?
model.
We have also shown a cleaner and equally well-performing methodfor the handling of punctuation and conjunction, and we have revealed certain otherprobabilistic oddities about Collins?
parser.
We have not only analyzed the effect ofthe unpublished details but also reanalyzed the effect of certain well-known details,revealing that bilexical dependencies are barely used by the model and that headchoice is not nearly as important to overall parsing performance as once thought.
Fi-nally, we have performed experiments that show that the true discriminative powerof lexicalization appears to lie in the fact that unlexicalized syntactic structures aregenerated conditioning on the headword and head tag.
These results regarding the508Computational Linguistics Volume 30, Number 4lack of reliance on bilexical statistics suggest that generative models still have roomfor improvement through the employment of bilexical-class statistics, that is, depen-dencies among head-modifier word classes, where such classes may be defined by, say,WordNet synsets.
Such dependencies might finally be able to capture the semanticpreferences that were thought to be captured by standard bilexical statistics, as wellas to alleviate the sparse-data problems associated with standard bilexical statistics.This is the subject of our current research.Appendix: Complete List of Parameter ClassesThis section contains tables for all parameter classes in Collins?
Model 3, with appro-priate modifications and additions from the tables presented in Collins?
thesis.
Thenotation is that used throughout this article.
In particular, for notational brevity weuse M(w, t)i to refer to the three items Mi, tMi , and wMi that constitute some fully lexi-calized modifying nonterminal and similarly M(t)i to refer to the two items Mi and tMithat constitute some partially lexicalized modifying nonterminal.
The (unlexicalized)nonterminal-mapping functions alpha and gamma are defined in Section 6.1.
As ashorthand, ?
(M(t)i) = ?
(Mi), tMi .The head-generation parameter class, PH, gap-generation parameter class, PG, andsubcat-generation parameter classes, PsubcatL and PsubcatR , have back-off structures asfollows:Back-off level PH(H| .
.
.)
PG(G| .
.
.
)PsubcatL( subcatL| .
.
.
)PsubcatR( subcatR| .
.
.
)0 ?
(P), wh, th ?(?
(P)), ?(?
(H)), wh, th1 ?
(P), th ?(?
(P)), ?(?
(H)), th2 ?
(P) ?(?
(P)), ?(?
(H))The two parameter classes for generating modifying nonterminals that are notdominated by a base NP, PM and PMw , have the following back-off structures.
Recallthat back-off level 2 of the PMw parameters includes words that are the heads of theobserved roots of sentences (that is, the headword of the entire sentence).Back-off level PM(M(t)i, coord, punc | .
.
.
)0 ?
(P), ?
(H), wh, th, ?side, subcatside, side1 ?
(P), ?
(H), th, ?side, subcatside, side2 ?
(P), ?
(H), ?side, subcatside, sideBack-off level PMw(wMi | .
.
.
)0 ?
(M(t)i), coord, punc, ?
(P), ?
(H), wh, th, ?side, subcatside, side1 ?
(M(t)i), coord, punc, ?
(P), ?
(H), th, ?side, subcatside, side2 tMiThe two parameter classes for generating modifying nonterminals that are childrenof base NPs (NPB nodes), PM,NPB and PMw,NPB, have the following back-off structures.Back-off level 2 of the PMw,NPB parameters includes words that are the heads of theobserved roots of sentences (that is, the headword of the entire sentence).
Also, notethat there is no coord flag, as coordinating conjunctions are generated in the sameway as regular modifying nonterminals when they are dominated by NPB.
Finally, wedefine M0 = H, that is, the head nonterminal label of the base NP that was generatedusing a PH parameter.509Bikel Intricacies of Collins?
Parsing ModelBack-off level PM,NPB(M(t)i, punc| .
.
.
)PMw,NPB(wMi | .
.
.
)0 P, M(w, t)i?1, side Mi, tMi , punc, P, M(w, t)i?1, side1 P, M(t)i?1, side Mi, tMi , punc, P, M(t)i?1, side2 P, Mi?1, side tMiThe two parameter classes for generating punctuation and coordinating conjunc-tions, Ppunc and Pcoord, have the following back-off structures (Collins, personal com-munication, October 2001), where?
type is a flag that obtains the value p in the history contexts of Ppuncparameters and c in the history contexts of Pcoord parameters;?
M(w, t)i is the modifying preterminal that is being conjoined to thehead-child;?
tp or tc is the particular preterminal (part-of-speech tag) that is conjoiningthe modifier to the head-child (such as CC or :);?
wp or wc is the particular word that is conjoining the modifier to thehead-child (such as and or :).Back-off level Pcoord( tc| .
.
.)
Pcoordw(wc| .
.
.
)Ppunc( tp??
.
.
.)
Ppuncw(wp??
.
.
.
)0 wh, th, P, H, M(w, t)i, type ttype, wh, th, P, H, M(w, t)i, type1 th, P, H, M(t)i, type ttype, th, P, H, M(t)i, type2 type ttypeThe parameter classes for generating fully lexicalized root nonterminals given thehidden root +TOP+, PTOP and PTOPw , have the following back-off structures (identicalto Table 3; n/a: not applicable).Back-off level PTOPNT(H(t) | .
.
.)
PTOPw(w | .
.
.
)0 +TOP+ t, H, +TOP+1 n/a tThe parameter classes for generating prior probabilities on lexicalized nontermi-nals M(w, t), Ppriorw and PpriorNT , have the following back-off structures, where prior is adummy variable to indicate that Ppriorw is not smoothed (although the Ppriorw parametersstill have an associated smoothing weight; see note 27).Back-off level Ppriorw(w, t| .
.
.)
PpriorNT(M| .
.
.
)0 prior w, t1 prior tAcknowledgmentsI would especially like to thank MikeCollins for his invaluable assistance andgreat generosity while I was replicating histhesis results and for his comments on aprerelease draft of this article.
Many thanksto David Chiang and Dan Gildea for themany valuable discussions during thecourse of this work.
Also, thanks to theanonymous reviewers for their helpful andastute observations.
Finally, thanks to myPh.D.
advisor Mitch Marcus, who duringthe course of this work was, as ever, asource of keen insight and unbridledoptimism.
This work was supported in partby NSF grant no.
SBR-89-20239 and DARPAgrant no.
N66001-00-1-8915.510Computational Linguistics Volume 30, Number 4ReferencesBaker, J. K. 1979.
Trainable grammars forspeech recognition.
In Spring Conference ofthe Acoustical Society of America, pages547?550, Boston.Bies, A.
1995.
Bracketing guidelines forTreebank II style Penn Treebank Project.Available at ftp://ftp.cis.upenn.edu/pub/treebank/doc/manual/root.ps.gz.Bikel, Daniel M. 2000.
A statistical model forparsing and word-sense disambiguation.In Joint SIGDAT Conference on EmpiricalMethods in Natural Language Processing andVery Large Corpora, Hong Kong, October.Bikel, Daniel M. 2002.
Design of amulti-lingual, parallel-processingstatistical parsing engine.
In Proceedings ofHLT2002, San Diego.Bikel, Daniel M. and David Chiang.
2000.Two statistical parsing models applied tothe Chinese Treebank.
In Martha Palmer,Mitch Marcus, Aravind Joshi, and Fei Xia,editors, Proceedings of the Second ChineseLanguage Processing Workshop, pages 1?6,Hong Kong.Bikel, Daniel M., Richard Schwartz, RalphWeischedel, and Scott Miller.
1997.Nymble: A high-performance learningname-finder.
In Fifth Conference on AppliedNatural Language Processing, pages194?201, Washington, DC.Black, Ezra, Frederick Jelinek, John Lafferty,David Magerman, Robert Mercer, andSalim Roukos.
1992.
Towardshistory-based grammars: Using richermodels for probabilistic parsing.
InProceedings of the Fifth DARPA Speech andNatural Language Workshop, Harriman, NY.Booth, T. L. and R. A. Thompson.
1973.Applying probability measures to abstractlanguages.
IEEE Transactions on Computers,volume C-22: 442?450.Chiang, David and Daniel M. Bikel.
2002.Recovering latent information intreebanks.
In Proceedings of COLING?02,Taipei.Cohen, Paul R. 1995.
Empirical Methods forArtificial Intelligence.
MIT Press,Cambridge, MA.Collins, Michael.
1996.
A new statisticalparser based on bigram lexicaldependencies.
In Proceedings of the 34thAnnual Meeting of the Association forComputational Linguistics, pages 184?191,Santa Cruz, CA.Collins, Michael.
1997.
Three generative,lexicalised models for statistical parsing.In Proceedings of ACL-EACL ?97, pages16?23, Madrid.Collins, Michael.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D.thesis, University of Pennsylvania.Collins, Michael.
2000.
Discriminativereranking for natural language parsing.
InInternational Conference on MachineLearning, Stanford University, Stanford, CA.Collins, Michael and Nigel Duffy.
2002.New ranking algorithms for parsing andtagging: Kernels over discrete structures,and the voted perceptron.
In Proceedings ofACL-02, pages 263?270, Philadelphia.Eisner, Jason.
1996.
Three new probabilisticmodels for dependency parsing: Anexploration.
In Proceedings of the 16thInternational Conference on ComputationalLinguistics (COLING-96), pages 340?345,Copenhagen, August.Gildea, Daniel.
2001.
Corpus variation andparser performance.
In Proceedings of the2001 Conference on Empirical Methods inNatural Language Processing, Pittsburgh.Gildea, Daniel and Daniel Jurafsky.
2000.Automatic labeling of semantic roles.
InProceedings of ACL 2000, Hong Kong.Gildea, Daniel and Martha Palmer.
2002.The necessity of parsing for predicateargument recognition.
In Proceedings ofACL 2002, Philadelphia.Goodman, Joshua.
1997.
Global thresholdingand multiple-pass parsing.
In Proceedingsof the Second Conference on EmpiricalMethods in Natural Language Processing,Brown University, Providence, RI.Henderson, John C. and Eric Brill.
1999.Exploiting diversity in natural languageprocessing: Combining parsers.
InProceedings of the Fourth Conference onEmpirical Methods in Natural LanguageProcessing, College Park, MD.Hwa, Rebecca.
2001.
On minimizingtraining corpus for parser acquisition.
InProceedings of the Fifth ComputationalNatural Language Learning Workshop,Toulouse, France, July.Hwa, Rebecca, Philip Resnik, and AmyWeinberg.
2002.
Breaking the resourcebottleneck for multilingual parsing.
InWorkshop on Linguistic KnowledgeAcquisition and Representation: BootstrappingAnnotated Language Data, ThirdInternational Conference on LanguageResources and Evaluation (LREC-2002), LasPalmas, Canary Islands, June.Lari, K. and S. J.
Young.
1990.
Theestimation of stochastic context-freegrammars using the Inside-Outsidealgorithm.
Computer Speech and Language,4:35?56.511Bikel Intricacies of Collins?
Parsing ModelMarcus, Mitchell P., Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Buildinga large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19:313?330.Ratnaparkhi, Adwait.
1996.
A maximumentropy model for part-of-speech tagging.In Conference on Empirical Methods inNatural Language Processing, University ofPennsylvania, Philadelphia, May.Witten, I. T. and T. C. Bell.
1991.
Thezero-frequency problem: Estimating theprobabilities of novel events in adaptivetext compression.
IEEE Transactions onInformation Theory 37: 1085?1094.
