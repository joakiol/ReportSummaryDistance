Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 217?221, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational Linguisticsunimelb: Topic Modelling-based Word Sense Induction for Web SnippetClusteringJey Han Lau, Paul Cook and Timothy BaldwinDepartment of Computing and Information SystemsThe University of Melbournejhlau@csse.unimelb.edu.au, paulcook@unimelb.edu.au,tb@ldwin.netAbstractThis paper describes our system for Task 11of SemEval-2013.
In the task, participantsare provided with a set of ambiguous searchqueries and the snippets returned by a searchengine, and are asked to associate senses withthe snippets.
The snippets are then clus-tered using the sense assignments and sys-tems are evaluated based on the quality of thesnippet clusters.
Our system adopts a pre-existing Word Sense Induction (WSI) method-ology based on Hierarchical Dirichlet Process(HDP), a non-parametric topic model.
Oursystem is trained over extracts from the fulltext of English Wikipedia, and is shown to per-form well in the shared task.1 IntroductionThe basic premise behind research on word sensedisambiguation (WSD) is that there exists a static,discrete set of word senses that can be used to la-bel distinct usages of a given word (Agirre and Ed-monds, 2006; Navigli, 2009).
There are various pit-falls underlying this premise, including: (1) whatsense inventory is appropriate for a particular task(given that sense inventories can vary considerablyin their granularity and partitioning of word usages)?
(2) given that word senses tend to take the form ofprototypes, is discrete labelling a felicitous represen-tation of word usages, especially for non-standardword usages?
(3) how should novel word usages becaptured under this model?
and (4) given the rapidpace of language evolution on real-time social me-dia such as Twitter and Facebook, is it reasonableto assume a static sense inventory?
Given this back-drop, there has been a recent growth of interest in thetask of word sense induction (WSI), where the wordsense representation for a given word is automati-cally inferred from a given data source, and wordusages are labelled (often probabilistically) accord-ing to that data source.
While WSI has considerableappeal as a task, intrinsic cross-comparison of WSIsystems is fraught with many of the same issues asWSD (Agirre and Soroa, 2007; Manandhar et al2010), leading to a move towards task-based WSIevaluation, such as in Task 11 of SemEval-2013, ti-tled ?Evaluating Word Sense Induction & Disam-biguation within an End-User Application?.This paper presents the UNIMELB system entry toSemEval-2013 Task 11.
Our method is based heav-ily on the WSI methodology proposed by Lau etal.
(2012) for novel word sense detection.
Largelythe same methodology was also applied to SemEval-2013 Task 13 on WSI (Lau et al to appear).2 System DescriptionOur system is based on the WSI methodology pro-posed by Lau et al(2012) for the task of novel wordsense detection.
The core machinery of our sys-tem is driven by a Latent Dirichlet Allocation (LDA)topic model (Blei et al 2003).
In LDA, the modellearns latent topics for a collection of documents,and associates these latent topics with every docu-ment in the collection.
A topic is represented bya multinomial distribution of words, and the asso-ciation of topics with documents is represented by amultinomial distribution of topics, with one distribu-tion per document.
The generative process of LDA217for drawing word w in document d is as follows:1. draw latent topic z from document d;2. draw word w from the chosen latent topic z.The probability of selecting word w given a doc-ument d is thus given by:P (w|d) =T?z=1P (w|t = z)P (t = z|d).where t is the topic variable, and T is the number oftopics.The number of topics, T , is a parameter in LDA,and the model tends to be highly sensitive to this set-ting.
To remove the need for parameter tuning overdevelopment data, we make use of a non-parametricvariant of LDA, in the form of a Hierarchical Dirich-let Process (HDP: Teh et al(2006)).
HDP learns thenumber of topics based on data, and the concentra-tion parameters ?
and ?0 control the variability oftopics in the documents (for details of HDP pleaserefer to the original paper, Teh et al(2006)).To apply HDP in the context of WSI, the latenttopics are interpreted as the word senses, and thedocuments are usages that contain the target word ofinterest (or search query in the case of Task 11).
Thatis, given a search query (e.g.
Prince of Persia), a?document?
in our application is a sentence/snippetcontaining the target word.
In addition to the bag ofwords surrounding the target word, we also includepositional context word information, as used in theoriginal methodology of Lau et al(2012).
That is,we introduce an additional word feature for each ofthe three words to the left and right of the targetword.
An example of the topic model features fora context sentence is given in Table 1.2.1 Background Corpus and PreprocessingAs part of the task setup, we were provided withsnippets for each search query, constituting the doc-uments for the topic model for that query (eachsearch query is topic-modelled separately).
Our sys-tem uses only the text of the snippets as features, andignores the URL information.
The text of the snip-pets is tokenised and lemmatised using OpenNLPand Morpha (Minnen et al 2001).As there are only 64 snippets for each query inthe test dataset, which is very small by topic mod-elling standards, we turn to English Wikipedia toexpand the data, by extracting all context sentencesthat contain the search query in the full collectionof Wikipedia articles.1 Each extracted usage is athree-sentence context containing the search query:the original sentence that contains the actual usageand its preceding and succeeding sentences.
Theextraction of usages from Wikipedia significantlyincreases the amount of information for the topicmodel to learn the senses for the search queries.
Togive an estimate: for very ambiguous queries suchas queen we extracted almost 150,000 usages fromWikipedia; for most queries, however, this numbertends to be a few thousand usages.To summarise, for each search query we apply theHDP model to the combined collection of the 64snippets and the extracted usages from Wikipedia.The topic model learns the senses/topics for alldocuments in the collection, but we only use thesense/topic distribution for the 64 snippets as theyare the documents that are evaluated in the sharedtask.Our English Wikipedia collection is tokenised andlemmatised using OpenNLP and Morpha (Minnen etal., 2001).
The search queries provided in the task,however, are not lemmatised.
Two approaches areused to extract the usages of search queries fromWikipedia:HDP-CLUSTERS-LEMMA Search queries are lem-matised using Morpha (Minnen et al 2001),and both the original and lemmatised forms areused for extraction;2HDP-CLUSTERS-NOLEMMA Search queries arenot lemmatised and only their original formsare used for extraction.1The Wikipedia dump was retrieved on November 28th2009.2Morpha requires the part-of-speech (POS) of a given word,which is determined by the majority POS aggregated over all ofthat word?s occurrences in Wikipedia.218Search query dogsContext sentence Most breeds of dogs are at most a few hundred years oldBag-of-word features most, breeds, of, are, at, most, a, few, hundred, years, oldPositional word features most #-3, breeds #-2, of #-1, are #1, at #2, most #3Table 1: An example of topic model features.System F1 ARI RI JIAvg.
No.
of Avg.
ClusterClusters SizeHDP-CLUSTERS-LEMMA 0.6830 0.2131 0.6522 0.3302 6.6300 11.0756HDP-CLUSTERS-NOLEMMA 0.6803 0.2149 0.6486 0.3375 6.5400 11.6803TASK11.DULUTH.SYS1.PK2 0.5683 0.0574 0.5218 0.3179 2.5300 26.4533TASK11.DULUTH.SYS7.PK2 0.5878 0.0678 0.5204 0.3103 3.0100 25.1596TASK11.DULUTH.SYS9.PK2 0.5702 0.0259 0.5463 0.2224 3.3200 19.8400TASK11-SATTY-APPROACH1 0.6709 0.0719 0.5955 0.1505 9.9000 6.4631TASK11-UKP-WSI-WACKY-LLR 0.5826 0.0253 0.5002 0.3394 3.6400 32.3434TASK11-UKP-WSI-WP-LLR2 0.5864 0.0377 0.5109 0.3177 4.1700 21.8702TASK11-UKP-WSI-WP-PMI 0.6048 0.0364 0.5050 0.2932 5.8600 30.3098RAKESH 0.3949 0.0811 0.5876 0.3052 9.0700 2.9441SINGLETON 1.0000 0.0000 0.6009 0.0000 64.0000 1.0000ALLINONE 0.5442 0.0000 0.3990 0.3990 1.0000 64.0000GOLD 1.0000 0.9900 1.0000 1.0000 7.6900 11.5630Table 2: Cluster quality results for all systems.
The best result for each column is presented in boldface.
SINGLETONand ALLINONE are baseline systems and GOLD is the theoretical upper-bound for the task.3 Experiments and ResultsFollowing Lau et al(2012), we use the default pa-rameters (?
= 0.1 and ?0 = 1.0) for HDP.3 For eachsearch query, we apply HDP to induce the senses,and a distribution of senses is produced for each?document?
in the model.
As the snippets in the testdataset correspond to the documents in the modeland evaluation is based on ?hard?
clusters of snip-pets, we assign a sense to each snippet based on thesense (= topic) which has the highest probability forthat snippet.The task requires participants to produce a rankedlist of snippets for each induced sense, based on therelative fit between the snippet and the sense.
We in-duce the ranking based on the sense probabilities as-signed to the senses, such that snippets that have thehighest probability of the induced sense are rankedhighest, and snippets with lower sense probabilities3Our implementation can be accessed via https://github.com/jhlau/hdp-wsi.are ranked lower.Two classes of evaluation are used in the sharedtask:1. cluster quality measures: Jaccard Index (JI),RandIndex (RI), Adjusted RandIndex (ARI)and F1;2. diversification of search results: Subtopic Re-call@K and Subtopic Precision@r.Details of the evaluation measures are described inNavigli and Vannella (2013).The idea behind the second form of evaluation(i.e.
diversification of search results) is that searchengine results should cluster the results based onsenses (of the query term in the documents) given anambiguous query.
For example, if a user searches forapple, the search engine may return results related toboth the computer brand sense and the fruit sense ofapple.
Given this assumption, the best WSI/WSDsystem is the one that can correctly identify the di-versity of senses in the snippets.219Figure 1: Subtopic Recall@K for all participating systems.Cluster quality, subtopic recall@K and subtopicprecision@r results for all systems entered in thetask are presented in Table 2, Figure 1 and Figure 2,respectively.In terms of cluster quality, our systems(HDP-CLUSTERS-LEMMA and HDP-CLUSTERS-NOLEMMA) consistently outperform the other teamsfor all measures except for the Jaccard Index (wherewe rank second and third, by a narrow margin).
Theaverage number of induced clusters and the averagecluster size of our systems are similar to thoseof the gold standard system (GOLD), indicatingthat our systems are learning an appropriate sensegranularity.In terms of diversification of search results, oursystems perform markedly better than most teams,other than RAKESH which trails closely behind oursystems (despite a relatively low ranking in terms ofthe cluster quality evaluation).
Overall, the resultsare encouraging and our system performs very wellover the task.4 Discussion and ConclusionOur system adopts the WSI system proposed in Lauet al(2012) with no parameters tuned for this task,and performs very well over it.
Parameter tuning andexploiting URL information in the snippets couldpotentially boost the system performance further.Other background corpora (such as news articles)could also be used to increase the size of the trainingdata.
We leave these ideas for future work.Inspecting the difference between the HDP-CLUSTERS-LEMMA and HDP-CLUSTERS-NOLEMMA approaches, only 6 out of the 100lemmas have a lemmatised form which differs fromthe original query composition: pods (pod), tencommandments (ten commandment), guild wars(guild war), stand by me (stand by i), sisters ofmercy (sister of mercy) and lord of the flies (lord ofthe fly).
In most cases, including the lemmatisedquery results in the extraction of additional usefulusages, e.g.
using only the original form lord ofthe flies would extract no usages from Wikipedia(because this corpus has itself been lemmatised).In other cases, however, including the lemmatisedforms results in many common noun usages, e.g.the number of usages of the lemmatised pod issignificantly greater than that of the original formpods (which corresponds to proper noun usages inthe lemmatised corpus), resulting in senses beinginduced only for common noun usages of pods.
The220Figure 2: Subtopic Precision@r for all participating systems.advantages and disadvantages of both approachesare reflected in the results: performance is mixedand no one method clearly outperforms the other.To conclude, we apply a topic model-based WSImethodology to the task of web result clustering, us-ing English Wikipedia as an external resource for ex-tracting additional usages.
Our system is completelyunsupervised and requires no annotated resources,and appears to perform very well on the task.ReferencesEneko Agirre and Philip Edmonds.
2006.
WordSense Disambiguation: Algorithms and Applications.Springer, Dordrecht, Netherlands.Eneko Agirre and Aitor Soroa.
2007.
SemEval-2007 task02: Evaluating word sense induction and discrimina-tion systems.
In Proc.
of the 4th International Work-shop on Semantic Evaluations, pages 7?12, Prague,Czech Republic.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet alcation.
Journal of MachineLearning Research, 3:993?1022.Jey Han Lau, Paul Cook, Diana McCarthy, David New-man, and Timothy Baldwin.
2012.
Word sense induc-tion for novel sense detection.
In Proc.
of the 13thConference of the EACL (EACL 2012), pages 591?601, Avignon, France.Jey Han Lau, Paul Cook, and Timothy Baldwin.
to ap-pear.
unimelb: Topic modelling-based word sense in-duction.
In Proc.
of the 7th International Workshop onSemantic Evaluation (SemEval 2013).Suresh Manandhar, Ioannis Klapaftis, Dmitriy Dligach,and Sameer Pradhan.
2010.
SemEval-2010 Task 14:Word sense induction & disambiguation.
In Proceed-ings of the 5th International Workshop on SemanticEvaluation, pages 63?68, Uppsala, Sweden.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Natu-ral Language Engineering, 7(3):207?223.Roberto Navigli and Daniele Vannella.
2013.
SemEval-2013 task 11: Evaluating word sense induction & dis-ambiguation within an end-user application.
In Pro-ceedings of the 7th International Workshop on Seman-tic Evaluation (SemEval 2013), in conjunction withthe Second Joint Conference on Lexical and Compu-tational Semantcis (*SEM 2013), Atlanta, USA.Roberto Navigli.
2009.
Word sense disambiguation: Asurvey.
ACM Computing Surveys, 41(2).Yee Whye Teh, Michael I. Jordan, Matthew J. Beal, andDavid M. Blei.
2006.
Hierarchical Dirichlet pro-cesses.
Journal of the American Statistical Associa-tion, 101:1566?1581.221
