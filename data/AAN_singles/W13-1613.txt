Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 94?99,Atlanta, Georgia, 14 June 2013. c?2013 Association for Computational LinguisticsRA-SR: Using a ranking algorithm to automatically building resourcesfor subjectivity analysis over annotated corporaYoan Guti?rrez, Andy Gonz?lezUniversity of Matanzas, Cubayoan.gutierrez@umcc.cu,andy.gonzalez@infonet.umcc.cuAntonio Fern?ndez Orqu?n, Andr?sMontoyo, Rafael Mu?ozUniversity of Alicante, Spainantonybr@yahoo.com, {montoyo,rafael}@dlsi.ua.esAbstractIn this paper we propose a method thatuses corpora where phrases are annotatedas Positive, Negative, Objective andNeutral, to achieve new sentimentresources involving words dictionarieswith their associated polarity.
Ourmethod was created to build sentimentwords inventories based on senti-semantic evidences obtained afterexploring text with annotated sentimentpolarity information.
Through thisprocess a graph-based algorithm is usedto obtain auto-balanced values thatcharacterize sentiment polarities wellused on Sentiment Analysis tasks.
Toassessment effectiveness of the obtainedresource, sentiment classification wasmade, achieving objective instances over80%.1 IntroductionIn recent years, textual information has becomeone of the most important sources of knowledgeto extract useful data.
Texts can provide factualinformation, such as: descriptions, lists ofcharacteristics, or even instructions to opinion-based information, which would include reviews,emotions or feelings.
These facts have motivateddealing with the identification and extraction ofopinions and sentiments in texts that requirespecial attention.
Among most widely used termsin Natural Language Processing, in concrete inSentiment Analysis (SA) and Opinion Mining, isthe subjectivity term proposed by (Wiebe, 1994).This author defines it as ?linguistic expression ofsomebody?s opinions, sentiments, emotions,evaluations, beliefs and speculations?.
Anotherimportant aspect opposed to subjectivity is theobjectivity, which constitute a fact expression(Balahur, 2011).
Other interesting terms alsoproposed by (Wiebe et al 2005) considers,private state, theses terms involve opinions,beliefs, thoughts, feelings, emotions, goals,evaluations and judgments.Many researchers such as (Balahur et al 2010;Hatzivassiloglou et al 2000; Kim and Hovy,2006; Wiebe et al 2005) and many others havebeen working in this way and related areas.
Tobuild systems able to lead SA challenges it isnecessary to achieve sentiment resourcespreviously developed.
These resources could beannotated corpora, affective semantic structures,and sentiment dictionaries.In this paper we propose a method that usesannotated corpora where phrases are annotated asPositive, Negative, Objective and Neutral, toachieve new resources for subjectivity analysisinvolving words dictionaries with theirassociated polarity.The next section shows different sentiment andaffective resources and their main characteristics.After that, our proposal is developed in section 3.Section 4, present a new sentiment resourceobtained after evaluating RA-SR over manycorpora.
Section 5 described the evaluation andanalysis of the obtained resource, and also anassessment of the obtained resource in SentimentClassification task.
Finally, conclusion andfurther works are presented in section 6.2 Related workIt is known that the use of sentiment resourceshas proven to be a necessary step for training andevaluation for systems implementing sentimentanalysis, including also fine-grained opinionmining (Balahur, 2011).Different techniques have been used intoproduct reviews to obtain lexicons of subjectivewords with their associated polarity.
We canstudy the relevant research promoted by (Hu andLiu, 2004) which start with a set of seedadjectives (?good?
and ?bad?)
and reinforce thesemantic knowledge applying a expanding thelexicon with synonymy and antonymy relationsprovided by WordNet (Miller et al 1990).
Asresult of Hu and Liu researches an OpinionLexicon is obtained with around 6800 positive94and negative English words (Hu and Liu, 2004;Liu et al 2005).A similar approach has been used in buildingWordNet-Affect (Strapparava and Valitutti,2004).
In this case the building method startingfrom a larger of seed affective words set.
Thesewords are classified according to the six basiccategories of emotion (joy, sadness, fear,surprise, anger and disgust), are also expandedincrease the lexicon using paths in WordNet.Other widely used in SA has beenSentiWordNet resource (Esuli and Sebastiani,2006)).
The main idea that encouraged itsconstruction has been that ?terms with similarglosses in WordNet tend to have similarpolarity?.Another popular lexicon is MicroWNOp(Cerini et al 2007).
It contains opinion wordswith their associated polarity.
It has been built onthe basis of a set of terms extracted from theGeneral Inquirer1 (Stone et al 1996).The problem is that these resources do notconsider the context in which the words appear.Some methods tried to overcome this critiqueand built sentiment lexicons using the localcontext of words.We can mentioned to (Pang et al 2002) whombuilt a lexicon with associated polarity value,starting with a set of classified seed adjectivesand using conjunctions (?and?)
disjunctions(?or?, ?but?)
to deduce orientation of new wordsin a corpus.
(Turney, 2002) classifies words according totheir polarity based on the idea that terms withsimilar orientation tend to co-occur indocuments.On the contrary in (Balahur and Montoyo,2008b), is computed the polarity of new wordsusing ?polarity anchors?
(words whose polarityis known beforehand) and Normalized GoogleDistance (Cilibrasi and Vit?nyi, 2007) scoresusing as training examples opinion wordsextracted from ?pros and cons reviews?
from thesame domain.
This research achieved the lexicalresource Emotion Triggers (Balahur andMontoyo, 2008a).Another approach that uses the polarity of thelocal context for computing word polarity is theone presented by (Popescu and Etzioni, 2005),who use a weighting function of the wordsaround the context to be classified.All described resources have been obtainedmanually or semi-automatically.
Therefore, we1http://www.wjh.harvard.edu/~inquirer/focus our target in archiving automatically newsentiment resources supported over some ofaforementioned resources.
In particular, we willoffer contributions related with methods to buildsentiment lexicons using the local context ofwords.3 Our methodWe propose a method named RA-SR (usingRanking Algorithms to build SentimentResources) to build sentiment words inventoriesbased on senti-semantic evidences obtained afterexploring text with annotated sentiment polarityinformation.
Through this process a graph-basedalgorithm is used to obtain auto-balanced valuesthat characterize sentiment polarities widely usedon Sentiment Analysis tasks.
This methodconsists of three main stages: (I) Buildingcontextual words graphs; (II) Applying rankingalgorithm; and (III) Adjusting sentiment polarityvalues.Figure 1.
Resource walkthrough development process.These stages are represented in the diagram ofFigure 1, where the development process beginsintroducing two corpuses of annotated sentenceswith positive and negative sentencesrespectively.
Initially, a preprocessing of the textis made applying Freeling pos-tagger (Atserias etal., 2006) version 2.2 to convert all words tolemmas2.
After that, all lemmas lists obtained areintroduced in RA-SR, divided in two groups (i.e.positive and negative candidates, ????
and ????
).3.1 Building contextual words graphsGiving two sets of sentences (????
and ????
)annotated as positive and negative respectively,where ????
= [????
?, ?
, ?????]
and ????
=[????
?, ?
, ?????]
contains list ?
involvingwords lemmatized by Freeling 2.2 Pos-Tagger2Lemma denotes canonic form of the words.CorporaPhrase 3Phrase 2W1 W2 W3 W4W5 W3 W2W3 W4 W5 W6W1W7Phrase 1 PositvePhrasesW5 W6 W8 W9W8 W9 W7W6 W9 W10 W11W6W1 W8NegativePhrasesPhrase 3Phrase 2Phrase 1PositiveWordsNegativeWordsW1 W2 W3 W4W5W6 W7W5W6W7W8W9W10W11(I)(II) Reinforcing wordsWeight = 1(II) (II)(I)Weight =1Weight =1Weight =1Weight =1W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11W1 W2 W3 W4 W5 W6 W7 W8 W9 W10 W11(III)W1Default Weight = 1/N Default Weight = 1/N95(Atserias et al 2006), a process to build twolexical contextual graphs, ????
and ????
isapplied.
Those sentences are manually annotatedas positive and negative respectively.
Thesegraphs involve lemmas from the positive andnegative sentences respectively.A contextual graph ?
is defined as anundirected graph ?
=	 (?, ?)
, where ?
denotesthe set of vertices and ?
the set of edges.
Giventhe list ?
= [?1 	?
??]
a lemma graph is createdestablishing links among all lemmas of eachsentence, where words involved allow tointerconnect sentences ??
in ?
.
As a resultword/lemma networks ????
and ????
areobtained, where ?
= 	?
= [??
?
??]
and forevery edge (??
, ??)?
?
being ?
?, ???
?.
Therefore, ??
and ??
are the same.Then, having two graphs, we proceed toinitialize weight to apply graph-based rankingtechniques in order to auto-balance the particularimportance of each ??
into ????
and ???
?.3.2 Applying ranking algorithmTo apply a graph-based ranking process, it isnecessary to assign weights to the vertices of thegraph.
Words involved into ????
and ????
takethe default value 1/N as their weight to definethe weight of ?
vector, which is used in ourproposed ranking algorithm.
In the case wherewords are identified on the sentiment repositories(see Table 2) as positive or negative, in relationto their respective graph, a weight value of 1 (ina range [0?1] ) is assigned.
?
represents themaximum quantity of words in the current graph.Thereafter, a graph-based ranking algorithm isapplied in order to structurally raise the graphvertexes?
voting power.
Once the reinforcementvalues are applied, the proposed rankingalgorithm is able to increase the significance ofthe words related to these empowered vertices.The PageRank (Brin and Page, 1998)adaptation, which was popularized by (Agirreand Soroa, 2009) in Word Sense Disambiguationthematic, and the one that has obtained relevantresults, was an inspiration to us in this work.
Themain idea behind this algorithm is that, for eachedge between ?i and ?j in graph ?, a vote is madefrom ?i to ?j.
As a result, the relevance of ?j isincreased.On top of that, the vote strength from ?
to ?depends on ????
relevance.
The philosophybehind it is that, the more important the vertex is,the more strength the voter would have.
Thus,PageRank is generated by applying a randomwalkthrough from the internal interconnection of?, where the final relevance of ??
represents therandom walkthrough probability over ?
, andending on ?
?.In our system, we apply the following equationand configuration:??
= 	????
+	(1 ?
?)?
(1)Where: ?
is a probabilistic transition matrix?
?
?
, being ??,?
= ???
if a link from ?
i to ?
jexist, in other case zero is assigned; ?
is a vector ?
?
1	with values previously described in thissection; ??
is the probabilistic structural vectorobtained after a random walkthrough to arrive toany vertex; ?
is a dumping factor with value0.85, and like in (Agirre and Soroa, 2009) weused 30 iterations.A detailed explanation about the PageRankalgorithm can be found in (Agirre and Soroa,2009).After applying PageRank, in order to obtainstandardized values for both graphs, wenormalize the rank values by applying thefollowing equation:???
= ???/???(??)
(2)Where ???(??)
obtains the maximum rankvalue of ??
vector.3.3 Adjusting sentiment polarity valuesAfter applying the PageRank algorithm on ???
?and ????
, and having normalized their ranks,we proceed to obtain a final list of lemmas(named ?? )
while avoiding repeated elements.
??
is represented by ???
lemmas, which wouldhave, at that time, two assigned values: Positive,and Negative, which correspond to a calculatedrank obtained by the PageRank algorithm.At that point, for each lemma from ?
?,  thefollowing equations are applied in order to selectthe definitive subjectivity polarity for each one:???
= 	 ????
?
???
; 	???
> ??
?0																; ?????????
?
(3)???
= 	 ????
?
???
; 	???
> ??
?0																; ?????????
?
(4)Where ???
is the Positive value and ???
theNegative value related to each lemma in ?
?.In order to standardize the ???
and ???
valuesagain and making them more representative in a[0?1] scale, we proceed to apply anormalization process over the ???
and ??
?values.Following and based on the objective featurescommented by (Baccianella et al 2010), weassume their same premise to establish objectivevalues of the lemmas.
Equation (5) is used to this96proceeding, where ???
represent the objectivevalue.
???
= 1 ?
|???
?
??
?| (5)4 Sentiment Resource obtainedAt the same time we have obtained a ??
whereeach word is represented by ??
?, ???
and ??
?values, acquired automatically from annotatedsentiment corpora.
With our proposal we havebeen able to discover new sentiment words inconcordance of contexts in which the wordsappear.
Note that the new obtained resourceinvolves all lemmas identified into the annotatedcorpora.
??
?, ??
?, and ???
are nominal valuesbetween range [0?
1].5 EvaluationIn the construction of the sentiment resource weused the annotated sentences provided fromcorpora described on Table 1.
Note that we onlyused the sentences annotated positively andnegatively.
The resources involved into this tablewere a selection made to prove the functionalityof the words annotation proposal of subjectivityand objectivity.The sentiment lexicons used were providedfrom WordNetAffect_Categories 3 and opinion-words4 files and shown in detail in Table 2.Corpus Neg Pos Obj Neu Objor Neu Unknow Totalcomputational-intelligence5 6982 6172 - - - - 13154tweeti-b-sub.dist_out.tsv6 176 368 110 34 - - 688b1_tweeti-objorneu-b.dist_out.tsv6828 1972 788 1114 1045 - 5747stno7 1286 660384 - 10000 12330Total 9272 9172 898 1532 1045 10000 31919Table 1.
Corpora used to apply RA-SR.Sources Pos Neg TotalWordNet-Affects_Categories(Strapparava and Valitutti, 2004)629 907 1536opinion-words (Hu and Liu, 2004; Liuet al 2005)2006 4783 6789Total 2635 5690 8325Table 2.
Sentiment Lexicons.Some issues were taking into account throughthis process.
For example, after obtaining a3http://wndomains.fbk.eu/wnaffect.html4http://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html5A sentimental corpus obtained applying techniquesdeveloped by GPLSI department.
See(http://gplsi.dlsi.ua.es/gplsi11/allresourcespanel)6Train dataset of Semeval-2013 (Task 2.
SentimentAnalysis in Twitter, subtask b.
)7Test dataset of NTCIR Multilingual Opinion AnalysisTask (MOAT) http://research.nii.ac.jp/ntcir/ntcir-ws8/meeting/contextual graph ?
factotum words are present inmostly of the involved sentences (i.e.
verb ?tobe?).
This aspect is very dangerous afterapplying PageRank algorithm, because thisalgorithm because this algorithm strengthens thenodes possessing many linked elements.
For thatreason, the subtractions ???
?
???
and ???
????
are applied, where the most frequentlywords in all contexts obtains high values andbeing the subtraction a damping factor.Following an example; when we take the verb?to be?, before applying equation (2), verb ?tobe?
archives the highest values into each contextgraph (????
and ????
), 9.94 and 18.67 rankvalues respectively.
These values, applyingequation (2), are normalized obtaining both ???
= 	1  and ???
= 	1  in a range [0...1].Finally, when the next steps are executed(Equations (3) and (4)) verb ?to be?achieves ???
= 0 , ???
= 0  andtherefore 	???
= 1 .
Through this example itseems as we subjectively discarded words thatappear frequently in both contexts (Positive andNegative contexts).Using the corpora from Table 1 we obtain25792 sentimentally annotated lemmas with ??
?, ???
and ???
features.
Of them 12420 positiveand 11999 negative lemmas were discovered, ,and 1373 words already derived from existinglexical resources.Another contribution has been the ???
, ??
?and ???
scores assigned to words of lexicalinventory, which were used to reinforce thecontextual graphs in the building process.
Thosewords in concordance to our scenario count 842Positives and 383 Negatives.5.1 Sentiment Resource Applied onSentiment AnalysisTo know if our method offers resources thatimprove the SA state of the art, we propose abaseline supported on the sentiment dictionaries,and other method (Ranking Sentiment Resource(RSR)) supported over our obtained resource.The baseline consists on analyzing sentencesapplying Equation (6) and Equation (7).??????????
= ?????????????????
(6)??????????
= ?????????????????
(7)Where: ????????
is the total of positive words(aligned with the sentiment dictionaries) in thesentence; ????????
is the total of negativewords (aligned with the sentiment dictionaries)97in the sentence; ?????????
is the total ofwords in the sentence.Using these measures over the analyzedsentences, for each sentence, we obtain twoattributes, ??????????
and	??????????
; anda third attribute (named Classification)corresponding to its classification.On the other hand, we propose RSR.
This SAmethod uses in a different way the Equation (6)and Equation (7), and introduces Equation (8).??????????
= ?????????????????
(8)Being ????????
the sum of Positive rankingvalues of the sentence words, aligned with theobtained resource (??
); ????????
the sum ofNegative ranking values of the sentence words,aligned with the obtained resource (??
); and ???????
the sum of Objective ranking valuesof the sentence words, aligned with the obtainedresource (??
).In RSR method we proved with two approach,RSR (1/di) and RSR (1-(1/di)).
The first approachis based on a resource developed usingPageRank with  ??,?
= 1/??
and the otherapproach is using ??,?
= 1 ?
(1/??)
.
Table 3shows experimentation results.The evaluation has been applied over a corpusprovided by ?Task 2.
Sentiment Analysis inTwitter, subtask b?, in particular tweeti-b-sub.dist_out.tsv file.
This corpus contains 597annotated phrases, of them Positives (314),Negatives (155), Objectives (98) or Neutrals(30).
For our understanding this quantity ofinstances offers a representative perception ofRA-SR contribution; however we will think toevaluate RA-SR over other corpora in furtherresearches.C I R. Pos (%)R. Neg(%)R.
Obj(%)R.Neu(%)TotalP.(%)TotalR.
(%)Baseline 366 231 91.1 51.6 0.0 0.0 48.2 61.3RSR(1/di) 416 181 87.3 39.4 80.6 6.7% 67.8 69.7RSR(1-(1/di) 469 128 88.5 70.3 81.6 6.7% 76.8 78.6Table 3.
Logistic function (Cross-validation 10 folds)over tweeti-b-sub.dist_out.tsv8 corpus (597 instances).Recall (R), Precision (P), Correct (C), Incorrect (I).As we can see the baseline only is able todealing with negative and positive instances.
Isimportant to remark that our proposal starting upknowing only the words used in baseline and isable to growing sentiment information to otherwords related to them.
We can see this fact on8Semeval-2013 (Task 2.
Sentiment Analysis in Twitter,subtask b.
)Table 3, RSR is able to classify objectiveinstances over 80% of Recall and the baselinedoes not.Other relevant element is the recall differencebetween RSR (1/di) and RSR (1 ?
(1/??)
.Traditionally (1/??)
result value has beenassigned to ?
in PageRank algorithm.
We havedemonstrated that in lexical contexts RSR (1-(1/di)) approach offers a better performance ofPageRank algorithm, showing recall differencesaround 10 perceptual points.6 Conclusion and further worksAs a conclusion we can say that our proposal isable to automatically increase sentimentinformation, obtaining 25792 sentimentallyannotated lemmas with ???
, ???
and ???features.
Of them 12420 positive and 11999negative lemmas were discovered.In other hand, The RSR is capable to classifyobjective instances over 80% and negatives over70%.
We cannot tackle efficiently neutralinstances, perhaps it is due to the lack of neutralinformation in the sentiment resource we used.Also, it could be due to the low quantity ofneutral instances in the evaluated corpus.In further research we will evaluate RA-SRover different corpora, and we are also going todeal with the number of neutral instances.The variant RSR (1 ?
(1/??)
performs betterthan RSR(1/??)
one.
This demonstrates that inlexical contexts using PageRank with ??,?
= 1 ?(1/??)
offers a better performance.
Other furtherwork consists in exploring Social Medias toexpand our retrieved sentiment resourceobtaining real time evidences that occur in Web2.0.AcknowledgmentsThis research work has been partially funded bythe Spanish Government through the projectTEXT-MESS 2.0 (TIN2009-13391-C04),"An?lisis de Tendencias Mediante T?cnicas deOpini?n Sem?ntica" (TIN2012-38536-C03-03)and ?T?cnicas de Deconstrucci?n en laTecnolog?as del Lenguaje Humano?
(TIN2012-31224); and by the Valencian Governmentthrough the project PROMETEO(PROMETEO/2009/199).ReferencesAgirre, E. and A. Soroa.
Personalizing PageRank forWord Sense Disambiguation.
Proceedings of the12th conference of the European chapter of the98Association for Computational Linguistics (EACL-2009), Athens, Greece, 2009. p.Atserias, J.; B. Casas; E. Comelles; M. Gonz?lez; L.Padr?
and M. Padr?.
FreeLing 1.3: Syntactic andsemantic services in an opensource NLP library.Proceedings of LREC'06, Genoa, Italy, 2006. p.Baccianella, S.; A. Esuli and F. Sebastiani.SENTIWORDNET 3.0: An Enhanced LexicalResource for Sentiment Analysis and OpinionMining.
7th Language Resources and EvaluationConference, Valletta, MALTA., 2010.
2200-2204p.Balahur, A.
Methods and Resources for SentimentAnalysis in Multilingual Documents of DifferentText Types.
Department of Software andComputing Systems.
Alacant, Univeristy ofAlacant, 2011.
299. p.Balahur, A.; E. Boldrini; A. Montoyo and P.Martinez-Barco.
The OpAL System at NTCIR 8MOAT.
Proceedings of NTCIR-8 WorkshopMeeting, Tokyo, Japan., 2010.
241-245 p.Balahur, A. and A. Montoyo.
Applying a culturedependent emotion trigger database for textvalence and emotion classification.
Procesamientodel Lenguaje Natural, 2008a.
p.Balahur, A. and A. Montoyo.
Building arecommender system using community level socialfiltering.
5th International Workshop on NaturalLanguage and Cognitive Science (NLPCS), 2008b.32-41 p.Brin, S. and L. Page The anatomy of a large-scalehypertextual Web search engine ComputerNetworks and ISDN Systems, 1998, 30(1-7): 107-117.Cerini, S.; V. Compagnoni; A. Demontis; M.Formentelli and G. Gandini Language resourcesand linguistic theory: Typology, second languageacquisition, English linguistics (Forthcoming),chapter Micro-WNOp: A gold standard for theevaluation of automatically compiled lexicalresources for opinion mining., 2007.Cilibrasi, R. L. and P. M. B. Vit?nyi The GoogleSimilarity Distance IEEE TRANSACTIONS ONKNOWLEDGE AND DATA ENGINEERING,2007, VOL.
19, NO 3.Esuli, A. and F. Sebastiani.
SentiWordNet: A PubliclyAvailable Lexical Resource for Opinion Mining.Fifth international conference on LanguajeResources and Evaluation Genoa - ITaly., 2006.417-422 p.Hatzivassiloglou; Vasileios and J. Wiebe.
Effects ofAdjective Orientation and Gradability on SentenceSubjectivity.
International Conference onComputational Linguistics (COLING-2000), 2000.p.Hu, M. and B. Liu.
Mining and SummarizingCustomer Reviews.
Proceedings of the ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining (KDD-2004), USA,2004.
p.Kim, S.-M. and E. Hovy.
Extracting Opinions,Opinion Holders, and Topics Expressed in OnlineNews Media Text.
In Proceedings of workshop onsentiment and subjectivity in text at proceedings ofthe 21st international conference on computationallinguistics/the 44th annual meeting of theassociation for computational linguistics(COLING/ACL 2006), Sydney, Australia, 2006.
1-8 p.Liu, B.; M. Hu and J. Cheng.
Opinion Observer:Analyzing and Comparing Opinions on the Web.Proceedings of the 14th International World WideWeb conference (WWW-2005), Japan, 2005. p.Miller, G. A.; R. Beckwith; C. Fellbaum; D. Grossand K. Miller Introduction to WordNet: An On-lineLexical Database International Journal ofLexicography, 3(4):235-244., 1990.Pang, B.; L. Lee and S. Vaithyanathan.
Thumbs up?Sentiment Classification using machine learningtechniquies.
EMNLP -02, the Conference onEmpirical Methods in Natural LanguageProcessing, USA, 2002.
79-86 p.Popescu, A. M. and O. Etzioni.
Extracting productfeatures and opinions from reviews.
Proccedings ofHLT-EMNLP, Canada, 2005. p.Stone, P.; D. C.Dumphy; M. S. Smith and D. M.Ogilvie The General Inquirer: A ComputerApproach to Content Analysis The MIT Press,1996.Strapparava, C. and A. Valitutti.
WordNet-Affect: anaffective extension of WordNet.
Proceedings of the4th International Conference on LanguageResources and Evaluation (LREC 2004), Lisbon,2004.
1083-1086 p.Turney, P. D. Thumbs up or thumbs down?
Semanticorientation applied to unsupervised classificationof reviews.
Proceeding 40th Annual Meeting of theAssociation for Computational Linguistic.
ACL2002, USA, 2002.
417-424 p.Wiebe, J. Tracking point of view in narrativeComputational Linguistic, 1994, 20(2): 233-287.Wiebe, J.; T. Wilson and C. Cardie.
AnnotatingExpressions of Opinions and Emotions inLanguage.
Kluwer Academic Publishers,Netherlands, 2005. p.99
