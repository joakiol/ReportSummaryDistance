Proceedings of the 2010 Workshop on Biomedical Natural Language Processing, ACL 2010, pages 1?9,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsTwo strong baselines for the BioNLP 2009 event extraction taskAndreas VlachosComputer LaboratoryUniversity of Cambridgeav308@cl.cam.ac.ukAbstractThis paper presents two strong baselinesfor the BioNLP 2009 shared task on eventextraction.
First we re-implement a rule-based approach which allows us to ex-plore the task and the effect of domain-adapted parsing on it.
We then replace therule-based component with support vec-tor machine classifiers and achieve perfor-mance near the state-of-the-art without us-ing any external resources.
The good per-formances achieved and the relative sim-plicity of both approaches make them re-producible baselines.
We conclude withsuggestions for future work with respect tothe task representation.1 IntroductionThe term biomedical event extraction is used to re-fer to tasks whose aim is the extraction of informa-tion beyond the entity level.
It commonly involvesrecognizing actions and relations between one ormore entities.
The recent BioNLP 2009 sharedtask on event extraction (Kim et al, 2009) focusedon a number of relations of varying complexity inwhich an event consisted of a trigger and one ormore arguments.
It attracted 24 submissions andprovided a basis for system development.
The per-formances ranged from 16% to 52% in F-score.In this paper we describe two strong baselineapproaches for the main task (described in Sec.
2)with a focus on annotation costs and reproducibil-ity.
Both approaches rely on a dictionary of lem-mas associated with event types (Sec.
3).
First were-implement the rule-based approach of Vlachoset al (2009) using resources provided in the sharedtask.
While it is unlikely to reach the perfor-mance of approaches combining supervised ma-chine learning, exploring its potential can high-light what annotated data is useful and its poten-tial contribution to performance.
Also, given itsreliance on syntax, it allows us to assess the impor-tance of syntactic parsing.
Nevertheless, the per-formance achieved (35.39% F-score) is competi-tive with systems that used more annotated dataand/or other resources (Sec.
5).Building on the error analysis of the rule-basedapproach, we replace the rule-based componentwith support vector machine (SVM) classifierstrained on partial event annotation in the form oftrigger-argument associations (Sec.
6).
The useof a trainable classifier highlights issues concern-ing the suitability of the annotated data as train-ing material.
Using a simple feature representa-tion and no external resources, the performancerises to 47.89% in F-score, which would have beensecond best in the shared task (Sec.
7).
The er-ror analysis suggests that future work on event ex-traction should look into different task representa-tions which will allow more advanced models todemonstrate their potential (Sec.
8).
Both systemsshall become publically available.2 Definition, datasets and resourcesThe BioNLP 2009 shared task focused on extrac-tion of events involving proteins.
Protein recogni-tion was considered a given in order to focus theresearch efforts on the novel aspects of the task.Nine event types were defined in the main task,which can be broadly classified in two classes.Simple events, namely Gene expression, Tran-scription, Protein catabolism, Phosphorylation,Localization and Binding, which have proteinsas their Theme argument and Regulation events,namely Positive regulation, Negative regulationand (unspecified) Regulation which have an oblig-atory Theme argument and an optional Cause ar-gument which can be either a protein or anotherevent.
Every event has a trigger which is a con-tiguous textual string that can span over one ormore tokens, as well as a part of a token.
Triggersand arguments can be shared across events and1ID type trigger Theme CauseE1 Neg reg suppressed E2E2 Pos reg induced E3 gp41E3 Gene exp production IL-10Table 1: Shared task example annotation.the same textual string can be a trigger for eventsof different types.
In an example demonstratingthe complexity of the task: ?.
.
.
SQ 22536 sup-pressed gp41-induced IL-10 production in mono-cytes.?
Participating systems, given the two pro-teins (in bold), need to generate the three appro-priately nested events of Table 1.While event components can reside in differentsentences, we focus on events that are containedin a single sentence.
Participants were not pro-vided with resources to develop anaphora resolu-tion components and the anaphoric phenomena in-volved were rather complex, as we observed inVlachos et al (2009).
Extraction of events involv-ing anaphoric relations inside a single sentence isstill possible but it is likely to require rather com-plex patterns to be extracted.The shared task involved three datasets, train-ing, development and test, which consisted of 800,150 and 260 abstracts respectively taken from theGENIA event corpus.
Their annotation was tai-lored to the shared task definition.
A resourcemade available and used by the majority of the sys-tems was the output of four syntactic parsers:?
Bikel?s (2004) re-implementation of Collins?parsing model.
This parser was trained onnewswire data exclusively.?
The re-ranking parser of Charniak & Johnsonadapted to the biomedical domain (McCloskyand Charniak, 2008).
The in-domain, part-of-speech (PoS) tagger was trained on the GENIAcorpus (Kim et al, 2003) and the self-trainingof the re-ranking module used a part of the GE-NIA treebank as development data.?
The C&C Combinatory Categorial Grammar(CCG) parser adapted to the biomedical do-main (Rimell and Clark, 2009).
The PoS tag-ger was trained on the GENIA corpus, while1,000 sentences were annotated with lexicalcategories and added to the training data ofthe CCG supertagger and 600 sentences of theBioInfer corpus (Pyysalo et al, 2007) wereused for parameter tuning.?
The GDep dependency parser trained for thebiomedical domain in the experiments ofMiyao et al (2008).
This parser was trainedfor the biomedical domain using the GENIAtreebank.The native Penn TreeBank output of Bikel?s andMcClosky?s parser was converted to the StanfordDependency (SD) collapsed dependency format(de Marneffe and Manning, 2008).
The output ofthe CCG parser was also converted to the same de-pendency format, while the output of GDep wasprovided in a different dependency format usedfor the dependency parsing CoNLL 2007 sharedtask.
From the description above, it is clear thatthe various parsers have different levels of adap-tation to the biomedical domain.
While it is diffi-cult to assess quantitatively the actual annotationeffort involved, it is possible to make some com-parisons.
Bikel?s parser was not adapted to thedomain, therefore it would be the cheapest one todeploy.
McClosky and CCG used in-domain cor-pora annotated with PoS tags for training, whilethe latter using some additional annotation for lex-ical categories.
Furthermore, they were tuned us-ing in-domain syntactic treebanks.
Therefore, theyrepresent a more expensive option in terms of an-notation cost.
Finally, GDep was trained usingan in-domain treebanked corpus, thus representingthe alternative with the highest annotation cost.3 Trigger extractionWe perform trigger identification using a dictio-nary of lemmas associated with the event type theyindicate.
The underlying assumption is that a par-ticular lemma has the same semantic content in ev-ery occurrence, which results in extracting all ofits occurrences as triggers of the same event type.This is clearly an over-simplification, but the re-stricted domain and the task definition alleviatesmost of the problems caused.
For each lemma inthe dictionary, we extract all its occurrences in thetext as triggers, therefore over-generating, sincenot all occurrences denote a biomedical event.This can be either because they are not connectedwith appropriate arguments or because they areused with a sense irrelevant to the task.
Both is-sues are being resolved at the argument identifi-cation stage since superfluous triggers should notreceive arguments and not form events.The one-sense-per-term assumption is furtherchallenged by the fact that occurrences of the same2term can denote events of different types.
For ex-ample, ?expression?
is used as a trigger of fourdifferent event types in the training data, namelyGene expression, Transcription, Localization andPositive regulation.
While it can be argued that insome cases this is due to annotation inconsisten-cies, it is generally accepted that context can alterthe semantics of a token.
In order to amelioratethis problem, we define the concept of light trig-gers in analogy analogy to light verbs.
The latterare verbs whose semantics are lost when occur-ring in particular constructions, e.g.
?make?
as in?make mistakes?.
In the shared task, some lem-mas commonly associated with a particular eventtype, when modified by a term associated witha different event type, denote events of the typeof their modifier instead of their own.
For ex-ample, ?regulation?
generally denotes Regulationevents, unless it has a modifier of a different eventtype, e.g.
?positive?.
In these cases, ?regulation?becomes part of a multi-token Positive regulationtrigger (e.g.
?positive regulation?).
However, ifthe actual tokens are not adjacent, only ?regula-tion?
is annotated as a Positive regulation trigger,which is due to the requirement that triggers arecontiguous textual strings.
We refer to lemmasexhibiting this behaviour as light triggers.
Addi-tionally, we observe that some lemmas triggeredevents only when modified by another lemma as-sociated with an event type.
For example, ?ac-tivity?
when occurring without a modifier is notconsidered a trigger of any event, however, whenmodified by ?binding?
then it becomes a Bindingevent trigger.
We refer to lemmas exhibiting thisbehaviour as ultra-light triggers.1In order to construct the dictionary of termswith their associated event types we use the trig-ger annotation from the training data, but we ar-gue that such information could be obtained fromdomain experts.
First, we remove the triggers en-countered only once in the data in order to avoidprocessing non-indicative triggers.
Then, we lem-matize them with morpha (Minnen et al, 2001).We remove prepositions and other stopwords frommulti-token triggers such as ?in response to?
and?have a prominent increase?
in order to keep onlythe terms denoting the event type.
Then, usingthe single-token triggers only, we associate eachlemma with its most common event type.
In cases1Kilicoglu and Bergler (2009) made similar observationson the lemma ?activity?
without formalizing them.where a lemma consistently generates more thatone event trigger of different types (typically oneof the Simple event class and one of the Reg-ulation class, we associate the lemma with allthe relevant event types.
For example, ?overex-press?
consistently denotes Gene expression andPositive regulation events.
The last token of eachmulti-token trigger becomes a light trigger.
Fi-nally, if a lemma is encountered as part of a multi-token trigger of a different event type more of-ten than with the event type associated with it asa single-token trigger, then it becomes an ultra-light trigger.
We avoid stemming because suffixesdistinguish lemmas in an important way with re-spect to the task.
For example, ?activation?
de-notes Positive regulation events, while ?activity?is an ultra-light trigger.
We only keep lemmas as-sociated at least four times with a particular eventtype, since below that threshold the annotation wasrather inconsistent.During testing, we attempt to match each tokenwith one of the lemmas associated with an eventtype.
We perform this by relaxing the matchingsuccessively, using the token lemma first and if nomatch is found allowing a partial match in orderto deal with particles (e.g.
so that ?co-express?matches ?express?).
This process returns single-token triggers, some of which are processed fur-ther in case they are light or ultra-light using syn-tactic dependencies in the following stage.4 Rule-based argument identificationIn this stage, we connect the triggers extractedwith appropriate arguments using rules definedwith the Stanford dependency (SD) scheme (deMarneffe and Manning, 2008).
We re-implementthe set of rules of Vlachos et al (2009) using thesyntactic parsing resources provided by the orga-nizers for the development data.
Rule-based sys-tems need annotated data for tuning, but unliketheir supervised machine learning-based counter-parts they do not learn parameters from it, thus re-quiring less annotated data.
We consider this tobe the main advantage of rule-based systems andto demonstrate this point we explicitly avoid usingthe training data provided.
The rules define syn-tactic dependency paths that connect tokens con-taining triggers (trigger-tokens) with tokens con-taining their arguments (arg-tokens).
For multi-token protein names, it is sufficient that a pathreaches any of its tokens.
For Regulation event3class triggers we consider as arg-tokens not onlytokens containing (parts of) protein names but alsothe trigger-tokens found in the same sentence.
Therules defined are the following:?
If a trigger-token is the governor of an arg-token in subject relation (subj), then the latteris identified as the Theme argument of the for-mer, e.g.
?Stat1 expresses?.
The only excep-tion to this rule is that when the trigger denotesRegulation class events and the nominal sub-ject relation (nsubj) is observed, the arg-tokenis identified as a Cause argument, e.g.
?gp41induces?.?
If a trigger-token is the governor of an arg-token in a prepositional relation, then the lat-ter is identified as the Theme argument of theformer, e.g.
?expression of Stat1?.?
If a trigger-token is the governor of an arg-token in modifier relation then the latter isidentified as the Theme argument of the for-mer, e.g.
?Stat1 expression?.
We restrictthe definition of the modifier relation to sub-sume only the following relations: adjectivalmodifier (amod), infinitival modifier (infmod),participial modifier (partmod), adverbial mod-ifier (advmod), relative clause modifier (rc-mod), quantifier modifier (quantmod), tempo-ral modifier (tmod) and noun compound mod-ifier (nn) relations.
This restriction is placed inorder to avoid matches irrelevant to the task.?
If a trigger-token is the governor of an arg-token in object relation (obj) then the latter isidentified as the Theme argument, e.g.
?SQ22536 suppressed gp41?.?
If a Regulation event class trigger and a pro-tein name are found in the same token, thenthe protein name is identified as the Cause ar-gument, e.g.
?gp41-induced?.A pre-processing step taken was to propagatemodifier and prepositional relations over tokensthat were co-ordinated or in an appositive relation.This was necessary since the SD output providedby the organizers is in the collapsed format, whichtreats co-ordinated tokens asymmetrically withoutpropagating their dependencies.2For each Simple or Binding trigger-argumentpair, we generate a single event with the argu-2The organizers re-generated the dependencies in thepropagation format but we avoid using them in order to beable to compare against the shared task participants.ment marked as Theme.
This approach is expectedto deal adequately with all event types except forBinding, which can have multiple themes.
Wegenerate Regulation events for trigger-argumentpairs whose argument is a protein name or a trig-ger that has an already formed event.
Since Reg-ulation events can have other Regulation events asThemes or Causes, we repeat this process until nomore events can be formed.
Finally, at this stagewe generate the required Regulation class eventfor triggers that consistently denote two events.5 Rule-based system resultsWe report our results using the approximate spanmatching/approximate recursive matching variantof the evaluation.
This variant allows for an eventto be considered extracted correctly if its triggeris extracted with span within an one-token exten-sion of the correct trigger span.
Also in the case ofnested events, events below the top-level need onlytheir Theme argument to be correctly identified sothat the top-level event is considered correct.
Thisevaluation variant was used as the primary perfor-mance criterion in the shared task.We first compared the performances obtainedusing the output of the different parsers pro-vided by the organizers on the development data.The best F-score was achieved using McClosky(39.66%), followed by CCG (38.73%) and Bikel(36.97%).
As expected, the overall performancecorrelates roughly with the adaptation cost in-volved in the development of these parsers as de-scribed in Section 2.
Bikel, which is essentiallyunadapted, has the worst performance overall, butit would have been the cheapest to deploy.
Whilethis can be viewed as a task-based parser compar-ison, similar to the experiments of Miyao et al(2008), one should be careful with the interpreta-tion of the results.
As pointed out by the authors,this type of evaluation cannot substitute a parsingevaluation against an appropriately annotated cor-pus since in the context of a given task only someaspects of parsing are likely to be relevant.
Fur-thermore, in our experiments we are are not us-ing the native output of the parsers but its conver-sion to the SD format.
Therefore unavoidably weevaluate the conversion as well as the parsing.
Forthis reason we avoided using the output of GDepwhich was not provided in this format.Examining the lists of false positives and falsenegatives on the system using the McClosky4parser, we observe that the most common triggersof events not extracted correctly had lemmas thatwere included in the dictionary, such as ?binding?,?expression?, ?induction?
and ?activation?.
Thissuggests that most event extraction errors are dueto argument identification and that using a dictio-nary for trigger extraction is sufficient, despite therather strong assumptions it is based upon.
Dis-abling the processing of light and ultra-light trig-gers, the performance on the development datadrops to 39.28%, the main reason being the de-creased recall in Binding events.Based on the comparison performed on the de-velopment data, we run our system using the Mc-Closky parser on the test data (Table 2).
The over-all performance achieved (35.39%) is relativelyclose to the one obtained on the development set(4% lower).
This is important since rule-basedsystems are prone to overfitting their developmentdata due to the way they are built.
Compared to theperformances achieved by the shared task partici-pants, the system presented would be ranked sev-enth in overall performance.
We believe this is astrong result, since it surpasses systems that usedsupervised machine learning methods taking ad-vantage of the development and the training data.Restricting the comparison to rule-based systems,it would have the second best performance outof nine such systems, most of which used exter-nal knowledge sources in order to improve theirperformance.
The best rule-based system (Kil-icoglu and Bergler, 2009) had overall performanceof 44.62% in F-score, ranking third overall.
Themain difference is that it used a much larger setof lexicalized rules (27) which were extracted us-ing the training data.
Also, heuristics were em-ployed in order to correct syntactic parsing errors(Schuman and Bergler, 2006).
While the benefitsfrom these additional processing steps are indis-putable, they involved a lot of manual work, bothfor rule construction as well as for the annotationof the data used to extract the rules.
We arguethat these performance benefits could be obtainedusing machine learning methods aimed at ame-liorating the argument identification stage.
Com-pared to the rule-based approach of Vlachos etal.
(2009), the performance is improved substan-tially.
The main difference between that systemand the one presented here is that the former usesthe domain-independent RASP parser (Briscoe etal., 2006).
While its performance was reasonable(it was ranked 10th overall, 30.80% F-score), theseresults lag behind those reported here.
Note thata direct comparison using the output of RASP isnot possible since the latter uses its own syntacticdependency scheme and there is no lossless con-version to the SD scheme.Overall, the results of this section demonstratethat the use of domain-adapted parsing is bene-ficial to event extraction.
This is not surprisingsince the system presented depends heavily on theparsing output.
We argue that the annotation costof this adaptation is a good investment because,unlike the task-specific training data, improvedsyntactic parsing is likely to be useful for otherevent extraction tasks, or even other IE tasks, e.g.anaphora resolution.
Therefore, we suggest thatdomain-adaptation of syntactic parsing should beconsidered first, especially in tasks that are heavilydependent on it.6 Improving argument identificationwith partial annotation and supportvector machinesIn this section, we present an approach to argu-ment identification which attempts to overcomethe drawbacks of the rule-based approach.
Fol-lowing the trigger extraction stage, for each triggercombined with each of its candidate arguments wecreate a classification instance.
The classificationtask is to assign the correct argument type to theinstance.
Therefore, we construct a binary clas-sifier which determines whether a protein nameis the Theme argument of a Simple or a Bindingtrigger (ThemePositive or ThemeNegative) and aternary classifier which determines whether a pro-tein name or another trigger (and as consequenceits associated events) is the Theme or the Causeargument of a Regulation trigger (RegThemePosi-tive, RegCausePositive, RegNegative).In order to acquire labeled instances for train-ing, we decompose the gold standard (GS) eventsinto multiple events with single arguments.
Incases of events being arguments to Regulationevents, the former are replaced by their triggers.We match the triggers extracted with those in-cluded in the gold standard, ignoring the eventtype annotation.
Since we identify single-tokentriggers, we replicate the approximate span match-ing used in evaluation in order to achieve bettercoverage.
If the instance being considered has aSimple or a Binding trigger, and if the pair is in-5Rules (MC) SVM (MC+CCG)Event Type/Class recall precision F-score recall precision F-scoreGene expression 46.54 78.50 58.43 61.63 82.26 70.47Transcription 26.28 28.57 27.38 29.93 62.12 40.39Protein catabolism 28.57 100.00 44.44 42.86 85.71 57.14Phosphorylation 65.19 82.24 72.73 78.52 91.38 84.46Localization 32.18 88.89 47.26 40.80 95.95 57.26Simple (total) 43.99 71.43 54.45 56.60 83.21 67.37Binding 20.46 38.17 26.64 29.11 45.29 35.44Regulation 15.81 23.47 18.89 23.71 39.20 29.55Positive 21.16 33.02 25.79 37.03 43.65 40.07Negative 17.15 29.41 21.67 30.34 40.35 34.64Regulation (total) 19.30 30.47 23.63 33.15 42.32 37.18Total 28.60 46.40 35.39 41.42 56.76 47.89Table 2: Performance of the rule-based and the SVM-based systems on the test data.
Each horizontalcorresponds to an event type or class.
Binding events are not included in the Simple class aggregateperformance because they can have multiple Themes.cluded in the GS then it is labeled as ThemePos-itive, else it labeled as ThemeNegative.
If the in-stance being considered has a Regulation triggerthat has been matched with a GS trigger, and ifits argument is a protein name and their pair is in-cluded in the GS then it is labeled according tothe latter (Reg{Theme/Cause}Positive), else, if notfound in the GS it is labeled as RegThemeNega-tive.
The same process is followed if the argumentis an event trigger which has been matched witha GS trigger.
We consider only Regulation trig-gers that are matched in the GS in order to avoidvalid RegCausePositive instances being labeled asRegNegative.
Recall that the Cause argument isoptional, while the Theme is obligatory for Reg-ulation events.
This means that if an appropriateTheme argument is not present, then it is possiblethat a Cause argument that is present is not anno-tated.
Similarly, when considering event triggersas arguments, we acquire labels only for instancesinvolving triggers that were annotated in the GS.Since triggers without an appropriate Theme arenot annotated in the gold standard, it is possiblethat a valid RegThemePositive or RegCausePosi-tive is labeled as RegNegative instance not becauseof the actual relation between the trigger and theargument but because the argument did not havean appropriate Theme present.
In the examplementioned in Sec.
2, if ?IL-10?
was replaced by?protein?
then none of the events would be an-notated.
We argue that a human annotator wouldproduce these annotations implicitly, and that thispartial (with respect to the task definition) annota-tion scheme allows the encoding of this informa-tion in a more flexible way.
Also, this is likely tobe a more efficient way to use the annotation time,since annotators would be requested to annotatepre-determined trigger-argument pairs instead ofsearching for events from scratch, given only theprotein name annotation.For training data generation we consider thetriggers extracted using the dictionary instead ofthose in the GS.
This process is certain to intro-duce some noise as some triggers might be omit-ted due to limited dictionary coverage.
If the eventtype determined by the dictionary is incorrect, thisis unlikely to affect the argument identificationprocess, since the latter is dependent on the lemmaof the trigger rather than its type.
For example, theTheme argument of the trigger ?expression?
is un-likely to depend on whether the event denoted isGene expression or Transcription.The labeled instance acquisition process de-scribed results in 9,699 binary and 10,541 ternarylabels compared to 6,607 triggers and 9,597 eventsannotated in the training data provided.
However,it must be pointed out that in the shared task an-notation scheme negative instances are annotatedimplicitly, i.e.
non-events are not annotated.
If weconsider only the positive instances, then the anno-tation scheme describeed results in 3,517 Theme-Positive and 3,933 Reg{Theme/Cause}Positive in-stances, which are simpler since they do not needrequire textual span and event type specification.6For feature extraction, we find the shortest de-pendency path connecting each trigger-argumentpair using Dijkstra?s algorithm.
We allow pathsto follow either dependency direction by incor-porating the direction in the dependency labels.Apart from the dependency path, we extract asfeatures the trigger-token, the trigger event typeand the argument type (event type if the argumentis a trigger or Entity in case of protein names).We filtered the training set considering only in-stances in which the trigger was at a maximum dis-tance of 4 dependencies away from the argument,since longer paths were too sparse to be usefulin classifying unseen instances.
At classificationtime, we consider as {Theme/Reg}Negative anyinstances in which the dependency path has notbeen encountered in the training data, as well asinstances without a dependency path connectingtrigger and argument.
This is necessary in order toavoid instances being classified only on the basisof the trigger-token and the argument type.
Af-ter the classifier has assigned labels to the trigger-argument pairs, we construct events as describedin Sec.
4.
In cases where it is unclear (to the classi-fier) which is the trigger and which is the argumentin a given pair of Regulation event triggers the pro-cess can result in cyclic dependencies.
We resolvethem using the confidence of the classifier for eachdecision by removing the least confident RegThe-mePositive or RegCausePositive assignment.7 SVM-based system resultsIn our experiments we used the LIBSVM toolkit(Chang and Lin, 2001) which provides an imple-mentation of Support Vector Machines with vari-ous kernels and uses the one-against-one schemefor multiclass problems.
In all experiments, theGaussian kernel was used in order to capture po-tential non-linear feature combinations, e.g.
caseswhere the combination of dependency path andtrigger-token would result in a different decisionrather than each of them independently.
Prelimi-nary experiments with the linear kernel confirmedthis expectation.We focused on using the output of the twodomain-adapted parsers, namely CCG and Mc-Closky.
The reason for this is that, as argued inSec.
5, given the importance of syntactic parsingto event extraction one should consider domainadaptation of syntactic parsing before developingtask-specific training resources.
We first comparedthe performances obtained using the output of thedifferent parsers provided by the organizers us-ing the development data.
The main observationis that, using either parser, the results are muchimproved compared to those reported in Sec.
5,by approximately eight percentage points in F-score in either case (46.49% and 47.40% F-scorefor CCG and McClosky respectively).
Most ofthe improvement is due to higher recall, suggest-ing that the argument identification component isable to learn patterns that are relevant to the task.Overall, using the output of CCG results in higherprecision, while McClosky results in higher re-call.
These parsers have different theoretical foun-dations, therefore they are expected to make dif-ferent errors.
In an effort to take advantage ofboth parsers simultaneously, we combined themby adding for each trigger-argument pair the de-pendency paths extracted by both parsers.
Thisimproved performance further to 49.35% F-score.We then run the system combining the twoparsers on the test data, obtaining the results pre-sented in Table 2.
Overall, the system presentedwould have the second best performance in theshared task achieving 41.42%/56.76%/47.89% inRecall/Precision/F-score.
The top system (Bjorneet al, 2009) achieved 46.73%/58.48%/51.95%(R/P/F).
It followed a machine learning approachto trigger extraction which, while it is likely tobe responsible for the performance difference ob-served when compared to the other participatingsystems, requires explicit trigger annotation, thusbeing more expensive.
Furthermore, we argue thatthe data provided by the organizers are not suit-able to train a trigger extractor, since only triggersparticipating in events are annotated, and semanti-cally valid triggers without appropriate argumentspresent are ignored.
We hypothesize that this isthe reason the authors had to adjust the decisionsof their SVM classifiers.The second best system (Buyko et al, 2009)achieved 45.82%/47.52%/46.66% (R/P/F) usingmany external knowledge sources such as theGene Ontology Annotation database, the Uni-versal Protein Resourceand the Medical SubjectHeadings thesaurus.
While the use of these re-sources and their successful usage is commend-able, we believe it is important that the systempresented achieves comparable performance usingfewer resources.Furthermore, joint inference models such as7Markov Logic Networks were applied to theBioNLP 2009 event extraction shared task byRiedel et al (2009) and were ranked fourth.This result was improved upon recently by Poonand Vanderwende (2010) who achieved 50% F-score, 2.11 percentage points better than the re-sult achieved in this work.
Such models have greatpotential for event extraction and we believe thatthey can benefit from the insights presented here.Finally, despite the fact that we used the same ex-perimental setup as the shared task participants,we do not consider our results are directly com-parable to theirs since we did not work under thesame time constraints and we profited from theirexperiences.8 DiscussionOur error analysis on the output of the best systemon the development data discouraged us from pur-suing further improvements.
Echoing the observa-tions of Buyko et al (2009), we found that anno-tation inconsistency was affecting our results sig-nificantly.
In many cases the event triggers anno-tated in the development data were rather mislead-ing, e.g.
?negative?
as a Gene expression eventtrigger (abstract 8622883), ?increase the stabil-ity?
as a Positive regulation event trigger (abstract8626752), ?disappearance?
as a Binding eventtrigger (abstract 10455128).
Finally, some eventswere ignored by the annotation, such as ?regula-tion of thymidine kinase?
(abstract 8622883).An additional complication is that events thatare annotated due to anaphoric linking can havea disproportionate effect on the scores.
In an ex-ample from abstract 9794375: ?CD3, CD2, andCD28 are functionally distinct receptors on T lym-phocytes.
Engagement of any of these recep-tors induces the rapid tyrosine phosphorylationof a shared group of intracellular signaling pro-teins, including Vav, Cbl, p85 phosphoinositide 3-kinase, and the Src family kinases Lck and Fyn.
?Failing to recognize the anaphoric Binding eventsinvolving proteins ?CD2?
and ?CD28?, an other-wise perfect system would receive two false nega-tives for the Binding events, eight false negativesfor the missing Positive regulation events due tothe missing Causes and four false positives for theincomplete Positive regulation events extracted.Despite this criticism, we believe that theBioNLP 2009 shared task on event extraction wasa big step forward for biomedical information ex-traction and we are grateful to the organizers forthe effort and resources provided, without whichthe research presented here would not have beenpossible.
The performances achieved in the mainTask1 ranged from 16% to 52% in F-score, sug-gesting improvements in task definition, data an-notation and participating systems compared toprevious community-wide efforts.
Indicatively,in the protein-protein interaction pair subtask ofBioCreative II (Krallinger et al, 2008) the anno-tated datasets provided were produced by extract-ing curation information from relevant databases.This meant that there was no text-bound annota-tion, thus making the application and evaluationof existing NLP techniques difficult, resulting inrather low performances.
The best performanceachieved was 29% in F-score, while many of theteams scored below 10%.However, we believe that future work shouldlook at improving the annotation in order to beable to assess the progress in the systems devel-oped.
In particular, we argue that we should movetowards a dependency-based representation, simi-lar to the one introduced by Surdeanu et al (2008)for joint syntactic parsing and semantic role label-ing.
Such representation can express the nestednature of the events and evaluate the dependenciesbetween them directly.
Furthermore, given the im-portance of syntactic parsing via syntactic depen-dencies to event extraction, it would be interestingto see how performing these tasks jointly wouldhelp improve the performance.
A dependency-based representation would also allow for non-contiguous event components, as well as morecomplex phenomena such as the light triggers dis-cussed earlier.9 ConclusionsIn this paper we focused on the BioNLP 2009shared task on event extraction.
We developedtwo systems, a rule-based one that does not re-quire training data and a SVM-based one whichachieves near state-of-the-art performance.
Thegood performances achieved and their reliance onshared task resources exclusively makes them re-producible and strong baselines for future work.Furthermore, we demonstrated the importance ofdomain adaptation of syntactic parsing for eventextraction.
Finally, based on our error analysis wesuggest future directions for event extraction withrespect to the task representation.8AcknowledgementsThe author would like to thank Ted Briscoe, MarkCraven and the three anonymous reviewers fortheir feedback.ReferencesDaniel M. Bikel.
2004.
Intricacies of Collins?
parsingmodel.
Computational Linguistics, 30(4):479?511.Jari Bjorne, Juho Heimonen, Filip Ginter, Antti Airola,Tapio Pahikkala, and Tapio Salakoski.
2009.
Ex-tracting complex biological events with rich graph-based feature sets.
In Proceedings of the BioNLP?09Shared Task on Event Extraction, pages 10?18.Ted Briscoe, John Carroll, and Rebecca Watson.
2006.The second release of the RASP system.
In Pro-ceedings of the COLING/ACL Interactive presenta-tion sessions, pages 77?80, Morristown, NJ, USA.Association for Computational Linguistics.Ekaterina Buyko, Erik Faessler, Joachim Wermter, andUdo Hahn.
2009.
Event extraction from trimmeddependency graphs.
In Proceedings of the BioNLP2009 Workshop Companion Volume for Shared Task,pages 19?27.Chih-Chung Chang and Chih-Jen Lin,2001.
LIBSVM: a library for supportvector machines.
Software available athttp://www.csie.ntu.edu.tw/?cjlin/libsvm.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The stanford typed dependencies rep-resentation.
In CrossParser ?08: Coling 2008: Pro-ceedings of the Workshop on Cross-Framework andCross-Domain Parser Evaluation, pages 1?8.Halil Kilicoglu and Sabine Bergler.
2009.
Syntacticdependency based heuristics for biological event ex-traction.
In Proceedings of the BioNLP 2009 Work-shop Companion Volume for Shared Task, pages119?127.Jin-Dong Kim, Tomoko Ohta, Yuka Tateisi, andJun?ichi Tsujii.
2003.
GENIA corpus - a semanti-cally annotated corpus for bio-textmining.
In Bioin-formatics, volume 19, Suppl.
1, pages 180?182.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP?09 shared task on event extraction.
InBioNLP ?09: Proceedings of the Workshop onBioNLP, pages 1?9.Martin Krallinger, Florian Leitner, Carlos Rodriguez-Penagos, and Alfonso Valencia.
2008.
Overview ofthe protein-protein interaction annotation extractiontask of BioCreative II.
Genome Biology, 9(Suppl2):S4.David McClosky and Eugene Charniak.
2008.
Self-training for biomedical parsing.
In Proceedings ofthe 46th Annual Meeting of the Association of Com-putational Linguistics: Human Language Technolo-gies, pages 101?104.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of English.
Nat-ural Language Engineering, 7(3):207?223.Yusuke Miyao, Rune S?tre, Kenji Sagae, Takuya Mat-suzaki, and Jun?ichi Tsujii.
2008.
Task-orientedevaluation of syntactic parsers and their representa-tions.
In Proceedings of the 46th Annual Meeting ofthe Association of Computational Linguistics: Hu-man Language Technologies, pages 46?54.Hoifung Poon and Lucy Vanderwende.
2010.
Joint in-ference for knowledge extraction from biomedicalliterature.
In Proceedings of the North AmericanChapter of the Association for Computational Lin-guistics - Human Language Technologies 2010 con-ference.Sampo Pyysalo, Filip Ginter, Juho Heimonen, JariBjorne, Jorma Boberg, Jouni Jarvinen, and TapioSalakoski.
2007.
BioInfer: a corpus for informationextraction in the biomedical domain.
BMC Bioin-formatics, 8(1):50+.Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,and Jun?ichi Tsujii.
2009.
A Markov Logic Ap-proach to Bio-Molecular Event Extraction.
In Pro-ceedings of the BioNLP 2009 Workshop CompanionVolume for Shared Task, pages 41?49.Laura Rimell and Stephen Clark.
2009.
Port-ing a lexicalized-grammar parser to the biomedi-cal domain.
Journal of Biomedical Informatics,42(5):852?865.Jonathan Schuman and Sabine Bergler.
2006.
Post-nominal prepositional phrase attachment in pro-teomics.
In Proceedings of the Workshop on LinkingNatural Language Processing and Biology, pages82?89.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
TheCoNLL-2008 shared task on joint parsing of syn-tactic and semantic dependencies.
In CoNLL 2008:Proceedings of the Twelfth Conference on NaturalLanguage Learning, pages 159?177.Andreas Vlachos, Paula Buttery, DiarmuidO?
Se?aghdha, and Ted Briscoe.
2009.
Biomedicalevent extraction without training data.
In Proceed-ings of the BioNLP 2009 Workshop CompanionVolume for Shared Task, pages 37?40.9
