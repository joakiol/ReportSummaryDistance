Proceedings of the 43rd Annual Meeting of the ACL, pages 605?613,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsA Nonparametric Method for Extraction of Candidate Phrasal TermsPaul DeaneCenter for Assessment, Design and ScoringEducational Testing Servicepdeane@ets.orgAbstractThis paper introduces a new method foridentifying candidate phrasal terms (alsoknown as multiword units) which applies anonparametric, rank-based heuristic measure.Evaluation of this measure, the mutual rankratio metric, shows that it produces betterresults than standard statistical measures whenapplied to this task.1 IntroductionThe ordinary vocabulary of a language likeEnglish contains thousands of phrasal terms --multiword lexical units including compoundnouns, technical terms, idioms, and fixedcollocations.
The exact number of phrasal terms isdifficult to determine, as new ones are coinedregularly, and it is sometimes difficult to determinewhether a phrase is a fixed term or a regular,compositional expression.
Accurate identificationof phrasal terms is important in a variety ofcontexts, including natural language parsing,question answering systems, information retrievalsystems, among others.Insofar as phrasal terms function as lexical units,their component words tend to cooccur more often,to resist substitution or paraphrase, to follow fixedsyntactic patterns, and to display some degree ofsemantic noncompositionality (Manning,1999:183-186).
However, none of thesecharacteristics are amenable to a simplealgorithmic interpretation.
It is true that variousterm extraction systems have been developed, suchas Xtract (Smadja 1993), Termight (Dagan &Church 1994), and TERMS (Justeson & Katz1995) among others (cf.
Daille 1996, Jacquemin &Tzoukermann 1994, Jacquemin, Klavans, &Toukermann 1997, Boguraev & Kennedy 1999,Lin 2001).
Such systems typically rely on acombination of linguistic knowledge and statisticalassociation measures.
Grammatical patterns, suchas adjective-noun or noun-noun sequences areselected then ranked statistically, and the resultingranked list is either used directly or submitted formanual filtering.The linguistic filters used in typical termextraction systems have no obvious connectionwith the criteria that linguists would argue define aphrasal term (noncompositionality, fixed order,nonsubstitutability, etc.).
They function, instead, toreduce the number of a priori improbable termsand thus improve precision.
The associationmeasure does the actual work of distinguishingbetween terms and plausible nonterms.
A varietyof methods have been applied, ranging from simplefrequency (Justeson & Katz 1995),  modifiedfrequency measures such as c-values (Frantzi,Anadiou & Mima 2000, Maynard & Anadiou2000) and standard statistical significance testssuch as the t-test, the chi-squared test, and log-likelihood (Church and Hanks 1990, Dunning1993), and information-based methods, e.g.pointwise mutual information (Church & Hanks1990).Several studies of the performance of lexicalassociation metrics suggest significant room forimprovement, but also variability among tasks.One series of studies (Krenn 1998, 2000; Evert& Krenn 2001, Krenn & Evert 2001; also see Evert2004) focused on the use of association metrics toidentify the best candidates in particulargrammatical constructions, such as adjective-nounpairs or verb plus prepositional phraseconstructions, and compared the performance ofsimple frequency to several common measures (thelog-likelihood, the t-test, the chi-squared test, thedice coefficient, relative entropy and mutualinformation).
In Krenn & Evert 2001, frequencyoutperformed mutual information though not the t-test, while in Evert and Krenn 2001, log-likelihoodand the t-test gave the best results, and mutualinformation again performed worse thanfrequency.
However, in all these studiesperformance was generally low, with precisionfalling rapidly after the very highest rankedphrases in the list.By contrast, Schone and Jurafsky (2001)evaluate the identification of phrasal terms withoutgrammatical filtering on a 6.7 million word extractfrom the TREC databases, applying both WordNetand online dictionaries as gold standards.
Onceagain, the general level of performance was low,with precision falling off rapidly as larger portions605of the n-best list were included, but they reportbetter performance with statistical and informationtheoretic measures (including mutual information)than with frequency.
The overall pattern appears tobe one where lexical association measures ingeneral have very low precision and recall onunfiltered data, but perform far better whencombined with other features which selectlinguistic patterns likely to function as phrasalterms.The relatively low precision of lexicalassociation measures on unfiltered data no doubthas multiple explanations, but a logical candidateis the failure or inappropriacy of underlyingstatistical assumptions.
For instance, many of thetests assume a normal distribution, despite thehighly skewed nature of natural languagefrequency distributions, though this is not the mostimportant consideration except at very low n (cf.Moore 2004, Evert 2004, ch.
4).
More importantly,statistical and information-based metrics such asthe log-likelihood and mutual information measuresignificance or informativeness relative to theassumption that the selection of component termsis statistically independent.
But of course thepossibilities for combinations of words areanything but random and independent.
Use oflinguistic filters such as "attributive adjectivefollowed by noun" or "verb plus modifyingprepositional phrase" arguably has the effect ofselecting a subset of the language for which thestandard null hypothesis -- that any word mayfreely be combined with any other word -- may bemuch more accurate.
Additionally, many of theassociation measures are defined only for bigrams,and do not generalize well to phrasal terms ofvarying length.The purpose of this paper is to explore whetherthe identification of candidate phrasal terms can beimproved by adopting a heuristic which seeks totake certain of these statistical issues into account.The method to be presented here, the mutual rankratio, is a nonparametric rank-based approachwhich appears to perform significantly better thanthe standard association metrics.The body of the paper is organized as follows:Section 2 will introduce the statisticalconsiderations which provide a rationale for themutual rank ratio heuristic and outline how it iscalculated.
Section 3 will present the data sourcesand evaluation methodologies applied in the rest ofthe paper.
Section 4 will evaluate the mutual rankratio statistic and several other lexical associationmeasures on a larger corpus than has been used inprevious evaluations.
As will be shown below, themutual rank ratio statistic recognizes phrasal termsmore effectively than standard statistical measures.2 Statistical considerations2.1 Highly skewed distributionsAs first observed e.g.
by Zipf (1935, 1949) thefrequency of words and other linguistic units tendto follow highly skewed distributions in whichthere are a large number of rare events.
Zipf'sformulation of this relationship for single wordfrequency distributions (Zipf's first law) postulatesthat the frequency of a word is inverselyproportional to its rank in the frequencydistribution, or more generally if we rank words byfrequency and assign rank z, where the functionfz(z,N) gives the frequency of rank z for a sampleof size N, Zipf's first law states that:fz(z,N) = Cz?where C is a normalizing constant and ?
is a freeparameter that determines the exact degree ofskew; typically with single word frequency data, ?approximates 1 (Baayen 2001: 14).
Ideally, anassociation metric would be designed to maximizeits statistical validity with respect to thedistribution which underlies natural language text-- which is if not a pure Zipfian distribution at leastan LNRE (large number of rare events, cf.
Baayen2001) distribution with a very long tail, containingevents which differ in probability by many ordersof magnitude.
Unfortunately, research on LNREdistributions focuses primarily on unigramdistributions, and generalizations to bigram and n-gram distributions on large corpora are not as yetclearly feasible (Baayen 2001:221).
Yet many ofthe best-performing lexical association measures,such as the t-test, assume normal distributions, (cf.Dunning 1993) or else (as with mutualinformation) eschew significance testing in favorof a generic information-theoretic approach.Various strategies could be adopted in thissituation: finding a better model of thedistribution,or adopting a nonparametric method.2.2 The independence assumptionEven more importantly, many of the standardlexical association measures measure significance(or information content) against the defaultassumption that word-choices are statisticallyindependent events.
This assumption is built intothe highest-performing measures as observed inEvert & Krenn 2001, Krenn & Evert 2001 andSchone & Jurafsky 2001.This is of course untrue, and justifiable only as asimplifying idealization in the absence of a bettermodel.
The actual probability of any sequence ofwords is strongly influenced by the basegrammatical and semantic structure of language,particularly since phrasal terms usually conform to606the normal rules of linguistic structure.
Whatmakes a compound noun, or a verb-particleconstruction, into a phrasal term is not deviationfrom the base grammatical pattern for noun-nounor verb-particle structures, but rather a furtherpattern (of meaning and usage and thus heightenedfrequency) superimposed on the normal linguisticbase.
There are, of course, entirely aberrant phrasalterms, but they constitute the exception rather thanthe rule.This state of affairs poses something of achicken-and-the-egg problem, in that statisticalparsing models have to estimate probabilities fromthe same base data as the lexical associationmeasures, so the usual heuristic solution as notedabove is to impose a linguistic filter on the data,with the association measures being applied onlyto the subset thus selected.
The result is in effect aconstrained statistical model in which theindependence assumption is much more accurate.For instance, if the universe of statisticalpossibilities is restricted to the set of sequences inwhich an adjective is followed by a noun, the nullhypothesis that word choice is independent -- i.e.,that any adjective may precede any noun -- is areasonable idealization.
Without filtering, theindependence assumption yields the much lessplausible null hypothesis that any word may appearin any order.It is thus worth considering whether there areany ways to bring additional information to bear onthe problem of recognizing phrasal terms withoutpresupposing statistical independence.2.3 Variable length; alternative/overlappingphrasesPhrasal terms vary in length.
Typically theyrange from about two to six words in length, butcritically we cannot judge whether a phrase islexical without considering both shorter and longersequences.That is, the statistical comparison that needs tobe made must apply in principle to the entire set ofword sequences that must be distinguished fromphrasal terms, including longer sequences,subsequences, and overlapping sequences, despitethe fact that these are not statistically independentevents.
Of the association metrics mentioned thusfar, only the C-Value method attempts to takedirect notice of such word sequence information,and then only as a modification to the basicinformation provided by frequency.Any solution to the problem of variable lengthmust enable normalization allowing directcomparison of phrases of different length.
Ideally,the solution would also address the other issues --the independence assumption and the skeweddistributions typical of natural language data.2.4 Mutual expectationAn interesting proposal which seeks to overcomethe variable-length issue is the mutual expectationmetric presented in Dias, Guillor?, and Lopes(1999) and implemented in the SENTA system(Gil and Dias 2003a).
In their approach, thefrequency of a phrase is normalized by taking intoaccount the relative probability of each wordcompared to the phrase.Dias, Guillor?, and Lopes take as the foundationof their approach the idea that the cohesiveness ofa text unit can be measured by measuring howstrongly it resists the loss of any component term.This is implemented by considering, for any n-gram, the set of [continuous or discontinuous](n-1)-grams which can be formed by deleting oneword from the n-gram.
A normalized expectationfor the n-gram is then calculated as follows:1 21 2([ , ... ])([ , ... ])nnp w w wFPE w w wwhere [w1, w2 ... wn] is the phrase being evaluatedand FPE([w1, w2 ... wn]) is:1 2 11^1 ([ , ... ]) [ ... .... ]nn i nip w w w p w w wn=?
??
?+?
??
??
??
?
?where wi is the term omitted from the n-gram.They then calculate mutual expectation as theproduct of the probability of the n-gram and itsnormalized expectation.This statistic is of interest for two reasons:first, it provides a single statistic that can beapplied to n-grams of any length; second, it is notbased upon the independence assumption.
The corestatistic, normalized expectation, is essentiallyfrequency with a penalty if a phrase containscomponent parts significantly more frequent thanthe phrase itself.It is of course an empirical question howwell mutual expectation performs (and we shallexamine this below) but mutual expectation is notin any sense a significance test.
That is, if we areexamining a phrase like the east end, theconditional probability of east given [__ end] or ofend given [__ east] may be relatively low (sinceother words can appear in that context) and yet thephrase might still be very lexicalized if theassociation of both words with this context weresignificantly stronger than their association for607other phrases.
That is, to the extent that phrasalterms follow the regular patterns of the language, aphrase might have a relatively low conditionalprobability (given the wide range of alternativephrases following the same basic linguisticpatterns) and thus have a low mutual expectationyet still occur far more often than one wouldexpect from chance.In short, the fundamental insight -- assessinghow tightly each word is bound to a phrase -- isworth adopting.
There is, however, good reason tosuspect that one could improve on this method byassessing relative statistical significance for eachcomponent word without making the independenceassumption.
In the heuristic to be outlined below, anonparametric method is proposed.
This method isnovel: not a modification of mutual expectation,but a new technique based on ranks in a Zipfianfrequency distribution.2.5 Rank ratios and mutual rank ratiosThis technique can be justified as follows.
Foreach component word in the n-gram, we want toknow whether the n-gram is more probable for thatword than we would expect given its behavior withother words.
Since we do not know what theexpected shape of this distribution is going to be, anonparametric method using ranks is in order, andthere is some reason to think that frequency rankregardless of n-gram size will be useful.
Inparticular, Ha, Sicilia-Garcia, Ming and Smith(2002) show that Zipf's law can be extended to thecombined frequency distribution of n-grams ofvarying length up to rank 6, which entails that therelative rank of words in such a combineddistribution provide a useful estimate of relativeprobability.
The availability of new techniques forhandling large sets of n-gram data (e.g.
Gil & Dias2003b) make this a relatively feasible task.Thus, given a phrase like east end, we can rankhow often __ end appears with east in comparisonto how often other phrases appear with east.Thatis, if {__ end, __side, the __, toward the __, etc.}
isthe set of (variable length) n-gram contextsassociated with east (up to a length cutoff), thenthe actual rank of __ end is the rank we calculateby ordering all contexts by the frequency withwhich the actual word appears in the context.We also rank the set of contexts associated witheast by their overall corpus frequency.
Theresulting ranking is the expected rank of __ endbased upon how often the competing contextsappear regardless of which word fills the context.The rank ratio (RR) for the word given thecontext can then be defined as:RR(word,context)  = ( )( ),,ER word contextAR word contextwhere ER is the expected rank and AR is the actualrank.
A normalized, or mutual rank ratio for the n-gram can then be defined as2 11, [__ .... ] 2, [ __ ... ] , [ 1, 2... _]( )* ( )...* ( )n nw w w w n w wn RR w RR w RR wThe motivation for this method is that it attemptsto address each of the major issues outlined aboveby providing a nonparametric metric which doesnot make the independence assumption and allowsscores to be compared across n-grams of differentlengths.A few notes about the details of the method arein order.
Actual ranks are assigned by listing all thecontexts associated with each word in the corpus,and then ranking contexts by word, assigning themost frequent context for word n the rank 1, nextnext most frequent rank 2, etc.
Tied ranks aregiven the median value for the ranks occupied bythe tie, e.g., if two contexts with the samefrequency would occupy ranks 2 and 3, they areboth assigned rank 2.5.
Expected ranks arecalculated for the same set of contexts using thesame algorithm, but substituting the unconditionalfrequency of the (n-1)-gram for the gram'sfrequency with the target word.13 Data sources and methodologyThe Lexile Corpus is a collection of documentscovering a wide range of reading materials such asa child might encounter at school, more or lessevenly divided by Lexile (reading level) rating tocover all levels of textual complexity fromkindergarten to college.
It contains in excess of400 million words of running text, and has beenmade available to the Educational Testing Serviceunder a research license by MetametricsCorporation.This corpus was tokenized using an in-housetokenization program, toksent,  which treats mostpunctuation marks as separate tokens but  makessingle tokens out of common abbreviations,numbers like 1,500, and words like o'clock.
Itshould be noted that some of the associationmeasures are known to perform poorly ifpunctuation marks and common stopwords are1In this study the rank-ratio method was tested forbigrams and trigrams only, due to the small number ofWordNet gold standard items greater than two words inlength.
Work in progress will assess the metrics'performance on n-grams of orders four through six.608included; therefore, n-gram sequences containingpunctuation marks and the 160 most frequent wordforms were excluded from the analysis so as not tobias the results against them.
Separate lists ofbigrams and trigrams were extracted and rankedaccording to several standard word associationmetrics.
Rank ratios were calculated from acomparison set consisting of all contexts derivedby this method from bigrams and trigrams, e.g.,contexts of the form word1__, ___word2,___word1 word2, word1 ___ word3, and word1word2 ___.2Table 1 lists the standard lexical associationmeasures tested in section four3.The logical evaluation method for phrasal termidentification is to rank n-grams using each metricand then compare the results against a goldstandard containing known phrasal terms.
SinceSchone and Jurafsky (2001) demonstrated similarresults whether WordNet or online dictionarieswere used as a gold standard, WordNet wasselected.
Two separate lists were derivedcontaining two- and three-word phrases.
Thechoice of WordNet as a gold standard tests abilityto predict general dictionary headwords rather thantechnical terms, appropriate since the sourcecorpus consists of nontechnical text.Following Schone & Jurafsky (2001), the bigramand trigram lists were ranked by each statistic thenscored against the gold standard, with resultsevaluated using a figure of merit (FOM) roughlycharacterizable as the area under the precision-recall curve.
The formula is:11 kiiPK=?where Pi (precision at i) equals i/Hi, and Hi is thenumber of n-grams into the ranked n-gram listrequired to find the ith correct phrasal term.It should be noted, however, that one of the mostpressing issues with respect to phrasal terms is thatthey display the same skewed, long-taildistribution as ordinary words, with a large2Excluding the 160 most frequent words preventedevaluation of a subset of phrasal terms such as verbalidioms like act up or go on.
Experiments with smallercorpora during preliminary work indicated that thisexclusion did not appear to bias the results.3Schone & Jurafsky's results indicate similar resultsfor log-likelihood & T-score, and strong parallelismamong information-theoretic measures such as Chi-Squared, Selectional Association (Resnik 1996),Symmetric Conditional Probability (Ferreira and PereiraLopes, 1999) and the Z-Score (Smadja 1993).
Thus itwas not judged necessary to replicate results for allmethods covered in Schone & Jurafsky (2001).proportion of the total displaying very lowfrequencies.
This can be measured by consideringTable 1.
Some Lexical Association Measuresthe overlap between WordNet and the Lexilecorpus.
A list of 53,764 two-word phrases wereextracted from WordNet, and 7,613 three-wordphrases.
Even though the Lexile corpus is quitelarge -- in excess of 400 million words of runningtext -- only 19,939 of the two-word phrases and4Due to the computational cost of calculating C-Values over a very large corpus, C-Values werecalculated over bigrams and trigrams only.
Moresophisticated versions of the C-Value method such asNC-values were not included as these incorporatelinguistic knowledge and thus fall outside the scope ofthe study.METRIC FORMULAFrequency(Guiliano, 1964) x yfPointwiseMutualInformation[PMI](Church &Hanks, 1990)( )xy x y2log /P P PTrue MutualInformation[TMI](Manning,1999)( )xy 2 xy x ylog /P P P PChi-Squared( 2?
)(Church andGale, 1991) { }{ },,2( )i X XY Yi j i ji jjf ?????
?T-Score(Church &Hanks, 1990)1 22 21 21 2x xs sn n?+C-Values4(Frantzi,Anadiou &Mima 2000)2 is not nested2log ( )log ( )1( )( ) aab Tafff bP T?
??
??????
??
??
??
??
??
??
??
??
?
?where ?
is the candidate stringf(?)
is its frequency in the corpusT?
is the set of candidate terms thatcontain ?P(T?)
is the number of thesecandidate terms6091,700 of the three-word phrases are attested in theLexile corpus.
14,045 of the 19,939 attested two-word phrases occur at least 5 times, 11,384 occurat least 10 times, and only 5,366 occur at least 50times; in short, the strategy of cutting off the dataat a threshold sacrifices a large percent of  totalrecall.
Thus one of the issues that needs to beaddressed is the accuracy with which lexicalassociation measures can be extended to deal withrelatively sparse data, e.g., phrases that appear lessthan ten times in the source corpus.A second question of interest is the effect offiltering for particular linguistic patterns.
This isanother method of prescreening the source datawhich can improve precision but damage recall.
Inthe evaluation bigrams were classified as N-N andA-N sequences using a dictionary template, withthe expected effect.
For instance, if the WordNettwo word phrase list is limited only to those whichcould be interpreted as noun-noun or adjectivenoun sequences, N>=5, the total set of WordNetterms that can be retrieved is reduced to 9,757..4 EvaluationSchone and Jurafsky's (2001) study examinedthe performance of various association metrics ona corpus of 6.7 million words with a cutoff ofN=10.
The resulting n-gram set had a maximumrecall of 2,610 phrasal terms from the WordNetgold standard, and found the best figure of meritfor any of the association metrics even withlinguistic filterering to be 0.265.
On thesignificantly larger Lexile corpus N must be sethigher (around N=50) to make the resultscomparable.
The statistics were also calculated forN=50, N=10 and N=5 in order to see what theeffect of including more (relatively rare) n-gramswould be on the overall performance for eachstatistic.
Since many of the statistics are definedwithout interpolation only for bigrams, and thenumber of WordNet trigrams at N=50 is verysmall, the full set of scores were only calculated onthe bigram data.
For trigrams, in addition to rankratio and frequency scores, extended pointwisemutual  information and true mutual informationscores were calculated using the formulas log(Pxyz/PxPy Pz)) and Pxyz log (Pxyz/PxPy Pz)).
Also,since the standard lexical association metricscannot be calculated across different n-gram types,results for bigrams and trigrams are presentedseparately for purposes of comparison.The results are are shown in Tables 2-5.
Twopoints should should be noted in particular.
First,the rank ratio statistic outperformed the otherassociation measures tested across the board.
Itsbest performance, a score of 0.323 in the part ofspeech filtered condition with N=50, outdistancedMETRIC POS Filtered UnfilteredRankRatio 0.323 0.196MutualExpectancy0.144 0.069TMI 0.209 0.096PMI 0.287 0.166Chi-sqr 0.285 0.152T-Score 0.154 0.046C-Values 0.065 0.048Frequency 0.130 0.044Table 2.
Bigram Scores for Lexical AssociationMeasures with N=50METRIC POS Filtered UnfilteredRankRatio 0.218 0.125MutualExpectation 0.140 0.071TMI 0.150 0.070PMI 0.147 0.065Chi-sqr 0.145 0.065T-Score 0.112 0.048C-Values 0.096 0.036Frequency 0.093 0.034Table 3.
Bigram Scores for Lexical AssociationMeasures with N=10METRIC POS Filtered UnfilteredRankRatio 0.188 0.110MutualExpectancy0.141 0.073TMI 0.131 0.063PMI 0.108 0.047Chi-sqr 0.107 0.047T-Score 0.098 0.043C-Values 0.084 0.031Frequency 0.081 0.021Table 4.
Bigram Scores for Lexical AssociationMeasures with N=5METRIC N=50 N=10 N=5RankRatio 0.273 0.137 0.103PMI 0.219 0.121 0.059TMI 0.137 0.074 0.056Frequency 0.089 0.047 0.035Table 5.
Trigram scores for Lexical AssociationMeasures at N=50, 10 and 5 without linguisticfiltering.610the best score  in Schone & Jurafsky's study(0.265), and when large numbers of rare bigramswere included, at N=10 and N=5, it continued tooutperform the other measures.
Second, the resultswere generally consistent with those reported inthe literature, and confirmed Schone & Jurafsky'sobservation that the information-theoreticmeasures (such as mutual information and chi-squared) outperform frequency-based measures(such as the T-score and raw frequency.
)54.1 DiscussionOne of the potential strengths of this method isthat is allows for a comparison between n-grams ofvarying lengths.
The distribution of scores for thegold standard bigrams and trigrams appears to bearout the hypothesis that the numbers are comparableacross n-gram length.
Trigrams constituteapproximately four percent of the gold standardtest set, and appear in roughly the same percentageacross the rankings; for instance, they consistute3.8% of the top 10,000 ngrams ranked by mutualrank ratio.
Comparison of trigrams with theircomponent bigrams also seems consistent with thishypothesis; e.g., the bigram Booker T. has a highermutual rank ratio than the trigram Booker T.Washington, which has a higher rank that thebigram T. Washington.
These results suggest that itwould be worthwhile to examine how well themethod succeeds at ranking n-grams of varyinglengths, though the limitations of the currentevaluation set to bigrams and trigrams prevented afull evaluation of its effectiveness across n-gramsof varying length.The results of this study appear to support theconclusion that the Mutual Rank Ratio performsnotably better than other association measures onthis task.
The performance is superior to the next-best measure when N is set as low as 5 (0.110compared to 0.073 for Mutual Expectation and0.063 for true mutual information and less than .05for all other metrics).
While this score is still fairlylow, it indicates that the measure performsrelatively well even when large numbers of low-probability n-grams are included.
An examinationof the n-best list for the Mutual Rank ratio at N=5supports this contention.The top 10 bigrams are:5Schone and Jurafsky's results differ from Krenn &Evert (2001)'s results, which indicated that frequencyperformed better than the statistical measures in almostevery case.
However, Krenn and Evert's data consistedof n-grams preselected to fit particular collocationalpatterns.
Frequency-based metrics seem to beparticularly benefited by linguistic prefiltering.Julius Caesar, Winston Churchill, potato chips, peanutbutter, Frederick Douglass, Ronald Reagan, TiaDolores, Don Quixote, cash register, Santa ClausAt ranks 3,000 to 3,010, the bigrams are:Ted Williams, surgical technicians, Buffalo Bill, drugdealer, Lise Meitner, Butch Cassidy, Sandra Cisneros,Trey Granger,  senior prom, Ruta SkadiAt ranks 10,000 to 10,010, the bigrams are:egg beater, sperm cells, lowercase letters, methane gas,white settlers, training program, instantly recognizable,dried beef, television screens, vienna sausagesIn short, the n-best list returned by the mutualrank ratio statistic appears to consist primarily ofphrasal terms far down the list, even when N is aslow as 5.
False positives are typically: (i)morphological variants of established phrases; (ii)bigrams that are part of longer phrases, such ascream sundae (from ice cream sundae); (iii)examples of highly productive constructions suchas an artist, three categories or January 2.The results for trigrams are relatively sparse andthus less conclusive, but are consistent with thebigram results: the mutual rank ratio measureperforms best, with top ranking elementsconsistently being phrasal terms.Comparison with the n-best list for other metricsbears out the qualitative impression that the rankratio is performing better at selecting phrasal termseven without filtering.
The top ten bigrams for thetrue mutual information metric at N=5 are:a little, did not, this is, united states, new york, knowwhat, a good, a long, a moment, a smallRanks 3000 to 3010 are:waste time, heavily on, earlier than, daddy said, ethnicgroups, tropical rain, felt sure, raw materials, goldmedals, gold rushRanks 10,000 to 10,010 are:quite close, upstairs window, object is, lord god, privateschools, nat turner, fire going, bering sea,little higher,got lotsThe behavior is consistent with known weaknessesof true mutual information -- its tendency toovervalue frequent forms.Next, consider the n-best lists for log-likelihood at N=5.
The top ten n-grams are:sheriff poulson, simon huggett, robin redbreast, erictorrosian, colonel hillandale, colonel sapp, nurseleatheran, st. catherines, karen torrio, jenny yongeN-grams 3000 to 3010 are:comes then, stuff who, dinner get, captain see, tom see,couple get, fish see, picture go, building go, makes will,pointed way611N-grams 10000 to 10010 are:sayings is, writ this, llama on, undoing this, dwahro did,reno on, squirted on, hardens like, mora did, millicentis, vets didComparison thus seems to suggest that if anythingthe quality of the mutual rank ratio results arebeing understated by the evaluation metric, as themetric is returning a large number of phrasal termsin the higher portion of the n-best list that areabsent from the gold standard.ConclusionThis study has proposed a new method formeasuring strength of lexical association forcandidate phrasal terms based upon the use ofZipfian ranks over a frequency distributioncombining n-grams of varying length.
The methodis related in general philosophy of MutualExpectation, in that it assesses the strenght ofconnection for each word to the combined phrase;it differs by adopting a nonparametric measure ofstrength of association.
Evaluation indicates thatthis method may outperform standard lexicalassociation measures, including mutualinformation, chi-squared, log-likelihood, and theT-score.ReferencesBaayen, R. H. (2001) Word Frequency Distributions.Kluwer: Dordrecht.Boguraev, B. and  C. Kennedy (1999).
Applicationsof Term Identification Technology: DomainDescription and Content Characterization.
NaturalLanguage Engineering 5(1):17-44.Choueka, Y.
(1988).
Looking for needles in ahaystack or locating interesting collocationexpressions in large textual databases.
Proceedingsof the RIAO, pages 38-43.Church, K.W., and  P. Hanks (1990).
Wordassociation norms, mutual information, andlexicography.
Computational Linguistics 16(1):22-29.Dagan, I. and K.W.
Church (1994).
Termight:Identifying and translating technical terminology.ACM International Conference ProceedingSeries: Proceedings of the fourth conferenceon Applied natural language processing, pages39-40.Daille, B.
1996.
"Study and Implementation ofCombined Techniques from Automatic Extractionof Terminology".
Chap.
3 of "The Balancing Act":Combining Symbolic and Statistical Approaches toKanguage (Klavans, J., Resnik, P.
(eds.
)), pages49-66.Dias, G., S.
Guillor?, and J.G.
Pereira Lopes (1999),Language independent automatic acquisition ofrigid multiword units from unrestricted textcorpora.
TALN, p. 333-338.Dunning, T. (1993).
Accurate methods for thestatistics of surprise and coincidence.Computational Linguistics 19(1): 65-74.Evert, S. (2004).
The Statistics of WordCooccurrences: Word Pairs and Collocations.
PhdThesis, Institut f?r maschinelleSprachverarbeitung, University of Stuttgart.Evert, S. and B. Krenn.
(2001).
Methods for theQualitative Evaluation of Lexical AssociationMeasures.
Proceedings of the 39th Annual Meetingof the Association for Computational Linguistics,pages 188-195.Ferreira da Silva, J. and G. Pereira Lopes (1999).
Alocal maxima method and a fair dispersionnormalization for extracting multiword units fromcorpora.
Sixth Meeting on Mathematics ofLanguage, pages 369-381.Frantzi, K., S. Ananiadou, and H. Mima.
(2000).Automatic recognition of multiword terms: the C-Value and NC-Value Method.
InternationalJournal on Digital Libraries 3(2):115-130.Gil, A. and G. Dias.
(2003a).
Efficient Mining ofTextual Associations.
International Conference onNatural Language Processing and KnowledgeEngineering.
Chengqing Zong (eds.)
pages 26-29.Gil, A. and G. Dias (2003b).
Using masks, suffixarray-based data structures, and multidimensionalarrays to compute positional n-gram statistics fromcorpora.
In Proceedings of the Workshop onMultiword Expressions of the 41st Annual Meetingof the Association of Computational Linguistics,pages 25-33.Ha, L.Q., E.I.
Sicilia-Garcia, J. Ming and F.J.
Smith.
(2002), "Extension of Zipf's law to words andphrases", Proceedings of the 19th InternationalConference on Computational Linguistics(COLING'2002), pages 315-320.Jacquemin, C. and E. Tzoukermann.
(1999).
NLP forTerm Variant Extraction: Synergy betweenMorphology, Lexicon, and Syntax.
NaturalLanguage Processing Information Retrieval, pages25-74.
Kuwer, Boston, MA, U.S.A.Jacquemin, C., J.L.
Klavans and E. Tzoukermann(1997).
Expansion of multiword terms for indexingand retrieval using morphology and syntax.Proceedings of the 35th Annual Meeting of theAssociation for Computational Linguistics, pages24-31.612Johansson, C. 1994b, Catching the Cheshire Cat, InProceedings of COLING 94, Vol.
II, pages 1021 -1025.Johansson, C. 1996.
Good Bigrams.
In Proceedingsfrom the 16th International Conference onComputational Linguistics (COLING-96), pages592-597.Justeson, J.S.
and S.M.
Katz (1995).
Technicalterminology: some linguistic properties and analgorithm for identification in text.
NaturalLanguage Engineering 1:9-27.Krenn, B.
1998.
Acquisition of Phraseological Unitsfrom Linguistically Interpreted Corpora.
A CaseStudy on German PP-Verb Collocations.Proceedings of ISP-98, pages 359-371.Krenn, B.
2000.
Empirical Implications on LexicalAssociation Measures.
Proceedings of The NinthEURALEX International Congress.Krenn, B. and S. Evert.
2001.
Can we do better thanfrequency?
A case study on extracting PP-verbcollocations.
Proceedings of the ACL Workshopon Collocations, pages 39-46.Lin, D. 1998.
Extracting Collocations from TextCorpora.
First Workshop on ComputationalTerminology, pages 57-63Lin, D. 1999.
Automatic Identification of Non-compositional Phrases, In Proceedings of The 37thAnnual Meeting of the Association ForComputational Lingusitics, pages 317-324.Manning, C.D.
and H. Sch?tze.
(1999).
Foundationsof Statistical Natural Language Processing.
MITPress, Cambridge, MA, U.S.A.Maynard, D. and S. Ananiadou.
(2000).
IdentifyingTerms by their Family and Friends.
COLING2000, pages 530-536.Pantel, P. and D. Lin.
(2001).
A Statistical Corpus-Based Term Extractor.
In: Stroulia, E. and Matwin,S.
(Eds.)
AI 2001, Lecture Notes in ArtificialIntelligence, pages 36-46.
Springer-Verlag.Resnik, P. (1996).
Selectional constraints: aninformation-theoretic model and its computationalrealization.
Cognition 61: 127-159.Schone, P. and D. Jurafsky, 2001.
Is Knowledge-Free Induction of Multiword Unit DictionaryHeadwords a Solved Problem?
Proceedings ofEmpirical Methods in Natural LanguageProcessing, pages 100-108.Sekine, S., J. J. Carroll, S. Ananiadou, and J. Tsujii.1992.
Automatic  Learning for SemanticCollocation.
Proceedings of the 3rd Conference onApplied Natural Language Processing, pages 104-110.Shimohata, S., T. Sugio, and J. Nagata.
(1997).Retrieving collocations by co-occurrences andword order constraints.
Proceedings of the 35thAnnual Meeting of the Association forComputational Linguistics, pages 476-481.Smadja, F. (1993).
Retrieving collocations from text:Xtract.
Computational Linguistics, 19:143-177.Thanapoulos, A., N. Fakotakis and G. Kokkinkais.2002.
Comparaitve Evaluation of CollocationExtraction Metrics.
Proceedings of the LREC 2002Conference, pages 609-613.Zipf, P. (1935).
Psychobiology of Language.Houghton-Mifflin, New York, New York.Zipf, P.(1949).
Human Behavior and the Principle ofLeast Effort.
Addison-Wesley, Cambridge, Mass.613
