Proceedings of the COLING/ACL 2006 Student Research Workshop, pages 7?12,Sydney, July 2006. c?2006 Association for Computational LinguisticsUnsupervised Part-of-Speech TaggingEmploying Efficient Graph ClusteringChris BiemannUniversity of Leipzig, NLP DepartmentAugustusplatz 10/11, 04109 Leipzig, Germanybiem@informatik.uni-leipzig.deAbstractAn unsupervised part-of-speech (POS)tagging system that relies on graphclustering methods is described.
Unlikein current state-of-the-art approaches, thekind and number of different tags isgenerated by the method itself.
Wecompute and merge two partitionings ofword graphs: one based on contextsimilarity of high frequency words,another on log-likelihood statistics forwords of lower frequencies.
Using theresulting word clusters as a lexicon, aViterbi POS tagger is trained, which isrefined by a morphological component.The approach is evaluated on threedifferent languages by measuringagreement with existing taggers.1 Introduction1.1 MotivationAssigning syntactic categories to words is animportant pre-processing step for most NLPapplications.Essentially, two things are needed to constructa tagger: a lexicon that contains tags for wordsand a mechanism to assign tags to running wordsin a text.
There are words whose tags depend ontheir use.
Further, we also need to be able to tagpreviously unseen words.
Lexical resources haveto offer the possible tags, and our mechanism hasto choose the appropriate tag based on thecontext.Given a sufficient amount of manually taggedtext, several approaches have demonstrated theability to learn the instance of a taggingmechanism from manually labelled data andapply it successfully to unseen data.
Those high-quality resources are typically unavailable formany languages and their creation is labour-intensive.
We will describe an alternativeneeding much less human intervention.In this work, steps are undertaken to derive alexicon of syntactic categories from unstructuredtext without prior linguistic knowledge.
Weemploy two different techniques, one for high-and medium frequency terms, one for medium-and low frequency terms.
The categories will beused for the tagging of the same text where thecategories were derived from.
In this way,domain- or language-specific categories areautomatically discovered.1.2 Existing ApproachesThere are a number of approaches to derivesyntactic categories.
All of them employ asyntactic version of Harris?
distributionalhypothesis: Words of similar parts of speech canbe observed in the same syntactic contexts.Contexts in that sense are often restricted to themost frequent words.
The words used to describesyntactic contexts will be called feature words inthe remainder.
Target words, as opposed to this,are the words that are to be grouped intosyntactic clusters.The general methodology (Finch and Chater,1992; Sch?tze, 1995; inter al.)
for inducing wordclass information can be outlined as follows:1.
Collect global context vectors for targetwords by counting how often featurewords appear in neighbouring positions.2.
Apply a clustering algorithm on thesevectors to obtain word classesThroughout, feature words are the 150-250words with the highest frequency.
Contexts arethe feature words appearing in the immediateneighbourhood of a word.
The word?s globalcontext is the sum of all its contexts.For clustering, a similarity measure has to bedefined and a clustering algorithm has to bechosen.
Finch and Chater (1992) use theSpearman Rank Correlation Coefficient and ahierarchical clustering, Sch?tze (1995) uses thecosine between vector angles and Buckshotclustering.An extension to this generic scheme ispresented in (Clark, 2003), where morphological7information is used for determining the wordclass of rare words.
Freitag (2004) does not sumup the contexts of each word in a context vector,but the most frequent instances of four-wordwindows are used in a co-clustering algorithm.Regarding syntactic ambiguity, mostapproaches do not deal with this issue whileclustering, but try to resolve ambiguities at thelater tagging stage.A severe problem with most clusteringalgorithms is that they are parameterised by thenumber of clusters.
As there are as manydifferent word class schemes as tag sets, and theexact amount of word classes is not agreed uponintra- and interlingually, inputting the number ofdesired clusters beforehand is clearly adrawback.
In that way, the clustering algorithmis forced to split coherent clusters or to joinincompatible sub-clusters.
In contrast,unsupervised part-of-speech induction means theinduction of the tag set, which implies findingthe number of classes in an unguided way.1.3 OutlineThis work constructs an unsupervised POStagger from scratch.
Input to our system is aconsiderable amount of unlabeled, monolingualtext bar any POS information.
In a first stage, weemploy a clustering algorithm on distributionalsimilarity, which groups a subset of the mostfrequent 10,000 words of a corpus into severalhundred clusters (partitioning 1).
Second, we usesimilarity scores on neighbouring co-occurrenceprofiles to obtain again several hundred clustersof medium- and low frequency words(partitioning 2).
The combination of bothpartitionings yields a set of word formsbelonging to the same derived syntactic category.To gain on text coverage, we add ambiguoushigh-frequency words that were discarded forpartitioning 1 to the lexicon.
Finally, we train aViterbi tagger with this lexicon and augment itwith an affix classifier for unknown words.The resulting taggers are evaluated againstoutputs of supervised taggers for variouslanguages.2 MethodThe method employed here follows the coarsemethodology as described in the introduction,but differs from other works in several respects.Although we use 4-word context windows andthe top frequency words as features (as inSch?tze 1995), we transform the cosinesimilarity values between the vectors of ourtarget words into a graph representation.Additionally, we provide a methdology toidentify and incorporate POS-ambiguous wordsas well as low-frequency words into the lexicon.2.1 The Graph-Based ViewLet us consider a weighted, undirected graphG(V,E) (v?V vertices, (vi,vj,wij)?E edges withweights wij).
Vertices represent entities (here:words); the weight of an edge between twovertices indicates their similarity.As the data here is collected in feature vectors,the question arises why it should be transformedinto a graph representation.
The reason is, thatgraph-clustering algorithms such as e.g.
(vanDongen, 2000; Biemann 2006), find the numberof clusters automatically1.
Further, outliers arehandled naturally in that framework, as they arerepresented as singleton nodes (without edges)and can be excluded from the clustering.
Athreshold s on similarity serves as a parameter toinfluence the number of non-singleton nodes inthe resulting graph.For assigning classes, we use the ChineseWhispers (CW) graph-clustering algorithm,which has been proven useful in NLPapplications as described in (Biemann 2006).
It istime-linear with respect to the number of edges,making its application viable even for graphswith several million nodes and edges.
Further,CW is parameter-free, operates locally andresults in a partitioning of the graph, excludingsingletons (i.e.
nodes without edges).2.2 Obtaining the lexiconPartitioning 1: High and medium frequencywordsFour steps are executed in order to obtainpartitioning 1:1.
Determine 200 feature and 10.000 targetwords from frequency counts2.
construct graph from context statistics3.
Apply CW on graph.4.
Add the feature words not present in thepartitioning as one-member clusters.The graph construction in step 2 is conductedby adding an edge between two words a and b1 This is not an exclusive characteristic for graphclustering algorithms.
However, the graph modeldeals with that naturally while other models usuallybuild some meta-mechanism on top for determiningthe optimal number of clusters.8with weight w=1/(1-cos(a,b)), if w exceeds asimilarity threshold s. The latter influences thenumber of words that actually end up in thegraph and get clustered.
It might be desired tocluster fewer words with higher confidence asopposed to running in the danger of joining twounrelated clusters because of too manyambiguous words that connect them.After step 3, we already have a partition of asubset of our target words.
The distinctions arenormally more fine-grained than existing tagsets.As feature words form the bulk of tokens incorpora, it is clearly desired to make sure thatthey appear in the final partitioning, althoughthey might form word classes of their own2.
Thisis done in step 4.
We argue that assigningseparate word classes for high frequency wordsis a more robust choice then trying todisambiguate them while tagging.Lexicon size for partitioning 1 is limited bythe computational complexity of step 2, which istime-quadratic in the number of target words.
Foradding words with lower frequencies, we pursueanother strategy.Partitioning 2: Medium and low frequencywordsAs noted in (Dunning, 1993), log-likelihoodstatistics are able to capture word bi-gramregularities.
Given a word, its neighbouring co-occurrences as ranked by the log-likelihoodreflect the typical immediate contexts of theword.
Regarding the highest ranked neighboursas the profile of the word, it is possible to assignsimilarity scores between two words A and Baccording to how many neighbours they share,i.e.
to what extent the profiles of A and Boverlap.
This directly induces a graph, which canbe again clustered by CW.This procedure is parametrised by a log-likelihood threshold and the minimum number ofleft and right neighbours A and B share in orderto draw an edge between them in the resultinggraph.
For experiments, we chose a minimumlog-likelihood of 3.84 (corresponding tostatistical dependence on 5% level), and at leastfour shared neighbours of A and B on each side.Only words with a frequency rank higher than2,000 are taken into account.
Again, we obtainseveral hundred clusters, mostly of open wordclasses.
For computing partitioning 2, anefficient algorithm like CW is crucial: the graphs2 This might even be desired, e.g.
for English not.as used for the experiments consisted of52,857/691,241 (English), 85,827/702,349(Finnish) and 137,951/1,493,571 (German)nodes/edges.The procedure to construct the graphs is fasterthan the method used for partitioning 1, as onlywords that share at least one neighbour have tobe compared and therefore can handle morewords with reasonable computing time.Combination of  partitionings 1 and 2Now, we have two partitionings of two different,yet overlapping frequency bands.
A large portionof these 8,000 words in the overlapping region ispresent in both partitionings.
Again, we constructa graph, containing the clusters of bothpartitionings as nodes; weights of edges are thenumber of common elements, if at least twoelements are shared.
And again, CW is used tocluster this graph of clusters.
This results infewer clusters than before for the followingreason: While the granularities of partitionings 1and 2 are both high, they capture differentaspects as they are obtained from differentsources.
Nodes of large clusters (which usuallyconsist of open word classes) have many edgesto the other partitioning?s nodes, which in turnconnect to yet other clusters of the same wordclass.
Eventually, these clusters can be groupedinto one.Clusters that are not included in the graph ofclusters are treated differently, depending ontheir origin: clusters of partition 1 are added tothe result, as they are believed to containimportant closed word class groups.
Dropoutsfrom partitioning 2 are left out, as they mostlyconsist of small, yet semantically motivatedword sets.
Combining both partitionings in thisway, we arrive at about 200-500 clusters that willbe further used as a lexicon for tagging.Lexicon constructionA lexicon is constructed from the mergedpartitionings, which contains one possible tag(the cluster ID) per word.
To increase textcoverage, it is possible to include those wordsthat dropped out in the distributional step forpartitioning 1 into the lexicon.
It is assumed thatthese words dropped out because of ambiguity.From a graph with a lower similarity threshold s(here: such that the graph contained 9,500 targetwords), we obtain the neighbourhoods of thesewords one at a time.
The tags of thoseneighbours ?
if known ?
provide a distribution ofpossible tags for these words.92.3 Constructing the taggerUnlike in supervised scenarios, our task is not totrain a tagger model from a small corpus ofhand-tagged data, but from our clusters ofderived syntactic categories and a considerablylarge, yet unlabeled corpus.Basic Trigram ModelWe decided to use a simple trigram modelwithout re-estimation techniques.
Adopting astandard POS-tagging framework, we maximizethe probability of the joint occurrence of tokens(ti) and categories (ci) for a sequence of length n:?=?
?=niiiiii tcPcccPCTP121 )|(),|(),( .The transition probability P(ci|ci-1,ci-2) isestimated from word trigrams in the corpuswhose elements are all present in our lexicon.The last term of the product, namely P(ci|ti), isdependent on the lexicon3.
If the lexicon does notcontain (ti), then (ci) only depends onneighbouring categories.
Words like these arecalled out-of-vocabulary (OOV) words.Morphological ExtensionMorphologically motivated add-ons are used e.g.in (Clark, 2003) and (Freitag 2004) to guess amore appropriate category distribution based ona word?s suffix or its capitalization for OOVwords.
Here, we examine the effects of CompactPatricia Trie classifiers (CPT) trained on prefixesand suffixes.
We use the implementation of(Witschel and Biemann, 2005).
For OOV words,the category-wise product of both classifier?sdistributions serve as probabilities P(ci|ti): Letw=ab=cd be a word, a be the longest commonprefix of w that can be found in all lexiconwords, and d be the longest common suffix of wthat can be found in all lexicon words.
Then}|{})(class|{}|{})(class|{)|(ydvvicvydvvaxuuicuaxuuwicP ==?=?==?== .
Table 1: Characteristics of corpora: number of sentences, tokens, tagger and tagset size, corpuscoverage of top 200 and 10,000 words.CPTs do not only smoothly serve as asubstitute lexicon component, they also realizecapitalization, camel case and suffix endingsnaturally.3 Although (Charniak et al 1993) report that usingP(ti|ci) instead leads to superior results in thesupervised setting, we use the ?direct?
lexiconprobability.
Note that our training material size isconsiderably larger than hand-labelled POS corpora.3 Evaluation methodologyWe adopt the methodology of (Freitag 2004) andmeasure cluster-conditional tag perplexity PP asthe average amount of uncertainty to predict thetags of a POS-tagged corpus, given the taggingwith classes from the unsupervised method.
Let?
?=xX xPxPI )(ln)(be the entropy of a random variable X and?=xyXY yPxPyxPyxPM)()(),(ln),(be the mutual information between tworandom variables X and Y.
Then the cluster-conditional tag perplexity for a gold-standardtagging T and a tagging resulting from clusters Cis computed as)exp()exp( | TCTCT MIIPP ?== .Minimum PP is 1.0, connoting a perfectcongruence on gold standard tags.In the experiment section we report PP onlexicon words and OOV words separately.
Theobjective is to minimize the total PP.4 Experiments4.1 CorporaFor this study, we chose three corpora: theBritish National Corpus (BNC) for English, a 10Million sentences newspaper corpus fromProjekt Deutscher Wortschatz4 for German, and3 million sentences from a Finnish web corpus(from the same source).
Table 1 summarizessome characteristics.lang.
sent.
tok.
tagger nr.tags200cov.10Kcov.en 6M 100M BNC5 84 55% 90%fi 3M 43M Connexor6 31 30% 60%ger 10M 177M (Schmid,1994) 54 49% 78%Since a high coverage is reached with fewwords in English, a strategy that assigns only themost frequent words to sensible clusters will takeus very far here.
In the Finnish case, we canexpect a high OOV rate, hampering performance4 See http://corpora.informatik.uni-leipzig.de.5 Semi-automatic tags as provided by BNC.6 Thanks goes to www.connexor.com for an academiclicense; the tags do not include interpunctuationmarks, which are treated seperately.10of strategies that cannot cope well with lowfrequency or unseen words.4.2 BaselinesTo put our results in perspective, we computedthe following baselines on random samples ofthe same 1000 randomly chosen sentences thatwe used for evaluation:?
1: the trivial top clustering: all words are inthe same cluster?
200: The most frequent 199 words formclusters of their own; all the rest is put intoone cluster.?
400: same as 200, but with 399 mostfrequent wordsTable 2 summarizes the baselines.
We give PPfigures as well as tag-conditional clusterperplexity PPG (uncertainty to predict theclustering from the gold standard tags, inversedirection of PP):lang English Finnish Germanbase 1 200 400 1 200 400 1 200 400PP 29 3.6 3.1 20 6.1 5.3 19 3.4 2.9PPG 1.0 2.6 3.5 1.0 2.0 2.5 1.0 2.5 3.1Table 2: Baselines for various tag set sizes4.3 ResultsWe measured the quality of the resulting taggersfor combinations of several substeps:?
O: Partitioning 1?
M: the CPT morphology extension?
T: merging partitioning 1 and 2?
A: adding ambiguous words to the lexiconFigure 2 illustrates the influence of thesimilarity threshold s for O,  OM and OMA forGerman ?
the other languages showed similarresults.
Varying s influences coverage on the10,000 target words.
When clustering very fewwords, tagging performance on these wordsreaches a PP as low as 1.25 but the high OOVrate impairs the total performance.
Clustering toomany words results in deterioration of results -most words end up in one big partition.
In themedium ranges, higher coverage and lowerknown PP compensate each other, optimal totalPPs were observed at target coverages 4,000-8,000.
Adding ambiguous words results in aworse performance on lexicon words, yetimproves overall performance, especially forhigh thresholds.For all further experiments we fixed thethreshold in a way that partitioning 1 consisted of5,000 words, so only half of the top 10,000words are considered unambiguous.
At thisvalue, we found the best performance averagedover all corpora.Fig 2.
Influence of threshold s on taggerperformance: cluster-conditional tag perplexityPP as a function of target word coverage.lang  O OM OMA TM TMAtotal 2.66 2.43 2.08 2.27 2.05lex 1.25 1.51 1.58 1.83oov 6.74 6.70 5.82 9.89 7.64oov% 28.07 14.25 14.98 4.62ENtags 619 345total 4.91 3.96 3.79 3.36 3.22lex 1.60 2.04 1.99 2.29oov 8.58 7.90 7.05 7.54 6.94oov% 47.52 36.31 32.01 23.80FItags 625 466total 2.53 2.18 1.98 1.84 1.79lex 1.32 1.43 1.51 1.57oov 3.71 3.12 2.73 2.97 2.57oov% 31.34 23.60 19.12 13.80GERtags 781 440Table 3: results for English, Finnish, German.oov% is the fraction of non-lexicon words.Overall results are presented in table 3.
Thecombined strategy TMA reaches the lowest PPfor all languages.
The morphology extension (M)always improves the OOV scores.
Addingambiguous words (A) hurts the lexiconperformance, but largely reduces the OOV rate,which in turn leads to better overall performance.Combining both partitionings (T) does notalways decrease the total PP a lot, but lowers thenumber of tags significantly.
Finnish figures aregenerally worse than for the other languages,akin to higher baselines.The high OOV perplexities for English inexperiment TM and TMA can be explained asfollows: The smaller the OOV rate gets, the morelikely it is that the corresponding words werealso OOV in the gold standard tagger.
A remedy11would be to evaluate on hand-tagged data.Differences between languages are most obviouswhen comparing OMA and TM: whereas forEnglish it pays off much more to add ambiguouswords than to merge the two partitionings, it isthe other way around in the German and Finnishexperiments.To wrap up: all steps undertaken improve theperformance, yet their influence's strength varies.As a flavour of our system's output, consider theexample in table 4 that has been tagged by ourEnglish TMA model: as in the introductoryexample, "saw" is disambiguated correctly.Word cluster ID cluster members (size)I 166 I (1)saw 2 past tense verbs (3818)the 73 a, an, the (3)man 1 nouns (17418)with 13 prepositions (143)a 73 a, an, the (3)saw 1 nouns (17418).
116 .
!
?
(3)Table 4: Tagging exampleWe compare our results to (Freitag, 2004), asmost other works use different evaluationtechniques that are only indirectly measuringwhat we try to optimize here.
Unfortunately,(Freitag 2004) does not provide a total PP scorefor his 200 tags.
He experiments with an hand-tagged, clean English corpus we did not haveaccess to (the Penn Treebank).
Freitag reports aPP for known words of 1.57 for the top 5,000words (91% corpus coverage, baseline 1 at 23.6),a PP for unknown words without morphologicalextension of 4.8.
Using morphological featuresthe unknown PP score is lowered to 4.0.
Whenaugmenting the lexicon with low frequencywords via their distributional characteristics, aPP as low as 2.9 is obtained for the remaining9% of tokens.
His methodology, however, doesnot allow for class ambiguity in the lexicon, thelow number of OOV words is handled by aHidden Markov Model.5 Conclusion and further workWe presented a graph-based approach tounsupervised POS tagging.
To our knowledge,this is the first attempt to leave the decision ontag granularity to the tagger.
We supported theclaim of language-independence by validatingthe output of our system against supervisedsystems in three languages.The system is not very sensitive to parameterchanges: the number of feature words, thefrequency cutoffs, the log-likelihood thresholdand all other parameters did not change overallperformance considerably when altered inreasonable limits.
In this way it was possbile toarrive at a one-size-fits-all configuration thatallows the parameter-free unsupervised taggingof large corpora.To really judge the benefit of an unsupervisedtagging system, it should be evaluated in anapplication-based way.
Ideally, the applicationshould tell us the granularity of our tagger: e.g.semantic class learners could greatly benefitfrom the high-granular word sets arising in bothof our partitionings, which we endeavoured tolump into a coarser tagset here.ReferencesC.
Biemann.
2006.
Chinese Whispers - an EfficientGraph Clustering Algorithm and its Application toNatural Language Processing Problems.Proceedings of the HLT-NAACL-06 Workshop onTextgraphs-06, New York, USAE.
Charniak, C. Hendrickson, N. Jacobson and M.Perkowitz.
1993.
Equations for part-of-speechtagging.
In Proceedings of the 11th NationalConference on AI, pp.
784-789, Menlo ParkA.
Clark.
2003.
Combining Distributional andMorphological Information for Part of SpeechInduction, Proceedings of EACL-03T.
Dunning.
1993.
Accurate Methods for the Statisticsof Surprise and Coincidence, ComputationalLinguistics 19(1), pp.
61-74S.
Finch and N. Chater.
1992.
Bootstrapping SyntacticCategories Using Statistical Methods.
In Proc.
1stSHOE Workshop.
Tilburg, The NetherlandsD.
Freitag.
2004.
Toward unsupervised whole-corpustagging.
Proc.
of COLING-04, Geneva, 357-363.H.
Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In: Proceedings ofthe International Conference on New Methods inLanguage Processing, Manchester, UK, pp.
44-49H.
Sch?tze.
1995.
Distributional part-of-speechtagging.
In EACL 7, pages 141?148S.
van Dongen.
2000.
A cluster algorithm for graphs.Technical Report INS-R0010, National ResearchInstitute for Mathematics and Computer Science inthe Netherlands, Amsterdam.F.
Witschel, and C. Biemann.
2005.
Rigorousdimensionality reduction through linguisticallymotivated feature selection for text categorisation.Proc.
of NODALIDA 2005, Joensuu,  Finland12
