Progress Report  on the Chronus System:ATIS Benchmark  Resul tsRoberto Pieraccini, Evelyne Tzoukermann, Zakhar Gorelov,Esther Levin, Chin-Hui Lee, Jean-Luc GauvainAT&T Bel l  Laborator ies600 Mountain AvenueMurray Hill, NJ 07974ABSTRACTThe speech understanding system we propose in this paper isbased on the stochastic modeling of a sentence as a sequenceof elemental units that represent i s meaning.
According tothis paradigm, the original meaning of a sentence, can bedecoded using a dynamic programming algorithm, althoughthe small amount of training data currently available sug-gested the integration of the decoder with a more traditionaltechnique.
However, the advantage of this method consistsin the development of a framework in which a closed trainingloop reduces the amount of human supervision in the designphase of the understanding component.
The results reportedhere for the February 1992 DARPA ATIS test are extremelypromising, considering the small amount of hand tuning thesystem required.1.
INTRODUCTIONIn February 1991 \[1\] we proposed a novel paradigm thatrepresents the conceptual content of a spoken sentencein terms of a probabilistic finite state automaton.
Themotivations for developing this model are summarizedby the following points.?
Current natural language understanding systemsare generally based on the synthesis of rules oper-ated generally by an expert.
This procedure makesmaintenance, updating, and generalization of a sys-tem to other tasks a very expensive and difficult op-eration.
We bel:eve that an understanding systemshould incorporate a mechanism that allows, or issuitable for, unsupervised learning.
Only using thismechanism can the system easily take advantage oflarge amounts of training data.?
Systems that are based on parsing with formalgrammars (finite state, context free, etc) are gener-ally very rigid.
A system that has to be integratedwith a speech recognizer has to be quite insensitiveto recognition errors (substitution, insertion, dele-tion of words) as well as to speech disfluencies, likefalse starts, ungrammatical sentences, non-speechphenomena, nd so on.?
The understanding model should define a frame-work that allows an easy and natural integrationwith the speech recognizer.Following these considerations we formalized the speechunderstanding problem in terms of a communicationchannel whose input is the meaning of a sentence andwhose output is a sequence of acoustic observations.Here we assume that the meaning of a sentence canbe expressed by a sequence of basic meaning unitsM = ml,m2,...m~VMM and that there is a sequentialcorrespondence between each mj and a subsequence ofthe acoustic observation A = al,a2...a~v,.
This hy-pothesis, although very restrictive, was successfully in-troduced also in \[2\].
According to this model of thespoken sentence production, one can think of decod-ing the original sequence of meaning units directly fromthe acoustic observation.
The decoding process can bebased on the maximization of the a posteriori probabil-ity P (M \[ A).The problem now consists in defining a suitable repre-sentation of the meaning of a sentence in terms of basicunits.
The representation we chose was inspired by thesemar~tic network \[3, 4\] paradigm, where the meaningof a sentence can be represented as a relational graphwhose nodes belong to some category of concepts andwhose arcs represent relations between concepts or lin-guistic cases.
In our representation, each unit of meaningconsists of a pair rnj = (cj, vj), where cj is a conceptualrelation, (e.g.
origin, destination, meal in the ATIS do-main), and vj is the value with which cj is instantiated inthe actual sentence.
(e.g.
Boston, San Francisco, break-fast).
Given a certain application domain we can definetwo sets of symbols, C and V, such that cj E C, andvj E Y.
For an application like ATIS, the size of the dic-tionary of concept relations C is fairly small (around 50),while the dictionary of concept values 13 can be relativelylarge (consider for instance all the possible flight num-bers).
Moreover, due to the limited amount of trainingdata we may reasonably think of collecting in this task, itis advisable to have a relatively small number of param-eters to be estimated.
This consideration lead us to usethe model for representing only the sequence of conceptrelations cj.
The sequence of concept values is detectedusing more traditional techniques by a subsequent mod-67ule called the template generator, that uses both the de-coded concept name and the sequence of words.
Hence,according to the maximum a posteriori decoding crite-rion, given a sequence of acoustic observations A, wewant to find a sequence of conceptual relations C anda sequence of words W = wl , .
.
.
,wNw that maximizethe a posteriori probability P (W,  C I A).
The underly-ing model for computing and maximizing this probabilitywas chosen to be a HMM whose states represent conceptrelations and whose observation probabilities are state-local language models in the form of word bigrams \[1, 5\].2.
SYSTEM ARCHITECTUREThe task of the conceptual decoder (see Fig.
I)is that of providing a conceptual segmentationS = \[Cj, (WIt , WI3+N J )\], j = 1 , .
.
.
,  NM,  where(w G , wIj+Nj) = w G, wI3+1 .. .wG+N ~ is the subsequenceof words that express the concept relation cj within thegiven sentenceIn the current version of the CHRONUS understand-ing system the speech recognizer is used in a decoupledmode.
The best string of words produced by the recog-nizer is used by the decoder for generating the conceptualsegmentation.
Because in this particular task there arenumbers, acronyms and compound words, the string ispre-processed by a module called lezical parser that gen-erates a lattice with all the possible interpretations of thestring (e.g.
the substring "B SEVEN FOUR SEVEN"could be interpreted as "B 747" or "B7 47" or "B74 7",etc.
The conceptual decoder is then realized as a gener-alization of the Viterbi algorithm that works on a latticerather than on a string of words.The template generator \[6\] consists of a simple patternmatching procedure that, given the conceptual segmen-tation, produces for each concept relation cj the corre-sponding concept value vj.
Finally the SQL translatortranslates the meaning representation M into an SQLquery.3.
THE NL COMPONENT3.1.
Training the conceptual modelThe conceptual model, as explained in the introductionof this paper, consists of concept transition probabili-ties P(cg~ I cg,_l) and concept conditional bigram lan-guage models P(wi \[ w~-l, cg~), where cg~ is the con-cept expressed by the phrase in which word wi is in-cluded.
These probabilities were initially trained usinga set of 532 sentences whose conceptual segmentationwas provided by hand.
This initial model was used inthe experiments described in \[1, 5\] and gave satisfac-tory performance as far as the conceptual segmentationof test sentences was concerned.
Hand labeling train-CONCEPTUAL SEGMENTATIONSPEECH \]SPEECH L~__~\[ CONCEPTUAL\] \[ ~AT~iON ~ RECOGNIZER ~ DECODINGI TEMPLATE= IPARSER WORD TRANSLATO~T, F, NA LATTICECOMP TOR FORMATTERANSWERFigure i: Block diagram of the proposed understandingsystem68ing sentences is of course a rather expensive procedurewhose consistence is rather doubtful.
As of today, mostof the training sentences available are annotated witha reference file that includes the right answer.
How-ever, for taking advantage of the annotated sentences wemust use the whole understanding system in the train-ing phase, generate the answer, and compare the an-swer with the reference file (see Fig.
1).
Therefore thecomparator \[7\] provides the training procedure with afeedback signal that can be used to partially automatizethe training procedure.
As a first attempt o develop acompletely automatic training procedure, we designed atraining loop based on the following steps:1.
Start with a reasonable model.2.
Generate an answer for each sentence in the trainingset.3.
Compare each answer with the corresponding refer-ence answer.4.
Use the conceptual segmentation of the sentencesthat were given a correct answer to reestimate themodel parameters.5.
Update the model and go to step 2A certain number of sentences will still produce a wronganswer after several iterations of the training loop.
Theconceptual segmentation of these sentences may be thencorrected by hand and included in the training set fora final reestimation of the model parameters.
Table 1shows the sets of data used for testing the effectivenessof the training loop.
All sentences are class A (contextindependent) sentences and belong to the MADCOWdatabase.
The conceptual segmentation of the sentencesin set A was done by hand, set B and C were annotatedSetABCNumber  of  Descr ip t ionSentences532 handlabled446 annotated195 annotated(oct-91)Table 1: Description of the data sets used in the trainingexperimentwith reference files (set C corresponds to the official Oc-tober 91 test set).
The comparison with reference fileswas done using only the minimal answer.
The resultsof this experiment are reported in Table 2.
The firstline in the table shows the results (as the percentage ofcorrectly answered sentences) both on set B and on theOctober 91 test set when the initial model, trained on the532 hand labeled sentences, was used.
The second lineshows the results on October 91 when the initial modelis smoothed using the supervised smoothing describedin \[5\].
The third line.reports the accuracy (on both setB and October 91) when the sentences that were cor-rectly answered out of set B were added to the trainingset (this set is called T(B)) and their conceptual labelingwas used along with set A for reestimating the model.It is interesting to notice that the performance on theOctober 91 test set is higher than that obtained withsupervised smoothing.
The last line of Table 2 showsthat supervised smoothing increases the performance bya very small percentage.
The results of this experimentshow that the use of automatically produced conceptualsegmentation along with the feedback introduced by thecomparator improves the performance of the system ofan amount that is comparable with that obtained by asupervised procedure, like the supervised smoothing.3.2.
The dialog managerFor dealing with class D sentences we developed a mod-ule, within the template generator, called the dialogmanager.
The function of this module is to keep thehistory of the dialog.
In this version of the dialog man-T ra in ing  % cor rect  on % cor rect  onset set B set CA 48.2 63.5A+smooth 72.3A+T(B) 50.9 72.8A+T(B)+smooth 73.3Table 2: Results using the training loop described in thetext.
T(B) is the subset of B that was correctly answeredby the system.Class # T F NA W.  Err .A 402 256 96 50 60.2D 285 122 113 50 96.8A+D 687 378 209 100 75.4Table 3: Official NIST score for the NL ATIS February92 testager the history is kept by saving the template from theprevious sentence in the same session and merging itwith the newly formed template, according to a set ofapplication specific rules.3.3.
NL results on February  1992 testThe February 1992 test set includes 402 class A sentencesand 285 class D sentences.
This set of 687 sentences,used for scoring the NL performance, is part of a largerset that originally included 283 class X (unanswerable)sentences.
The test was carried out for the overall set of970 sentence, without knowing which class they belongto.
The official score given from NIST is summarizedin Table 3.
After the test we found an inaccuracy inthe module of the SQL translator that is responsible forthe CAS formatting.
We fixed the bug and rescored thewhole set of sentences, obtaining the results reported inTable 4.
In Table 5 we report a detailed analysis of theresults.
In this analysis we included only the sentencesthat generated a false response.
Conceptual decodingand template generator errors are generally due to thelack of training data.
SQL translator and dialog man-ager errors are generally due to the limited power of therepresentation we are currently using.
Finally for the er-rors attributed to the CAS format or labeled as ambiguoswe generated a correct internal meaning representationbut the format of the answer did not comply with theprinciples of interpretation, or our interpretation did notagree with the one given by the annotators.4.
THE SPEECH RECOGNIZERIn this section we give a description of the speech recog-nition system that was used in conjunction with the nat-ural language understanding system for the February 92ATIS test.
Other details can be found in \[8, 9\]Class # T F NA W.  Er rA 402 299 54 49 39.0D 285 167 67 51 64.9A+D 687 466 121 100 49.8Table 4: Score for the NL ATIS February 92 after theformat bug was removed69Er ror  type  Number  ofSentencesConceptual decoding 30Template generation 19SQL translator 24CAS format 16Dialog manager 20Ambiguous 12Table 5: Analysis of the errors for the NL ATIS February92 testThe Speech signal was first filtered from 100 Hz to 3.8KHz and down-sampled to an 8 kHz sampling rate.
10thorder LPC analysis was then performed every 10 msecon consecutive 30 msec windows with a 20 msec frameoverlap.
Based on the short-time LPC features, 12 LPC-derived cepstral coefficients and their first and secondderivatives, plus normalized log energy and its first andsecond derivatives were computed and concatenated toform a single 39-dimension feature vector.6259 spontaneous utterances from the MADCOW datawere used for training the acoustic models.
Context-dependent phone-like units \[10\], including double-context phones, left-context phones, right-contextphones, context-independent phones, word-juncture con-text dependent phones and position dependent phones,were modeled using continuous density hidden Markovmodels (HMM) with mixture Gaussian state observa-tion densities.
The inventory of acoustic units was de-termined through an occurrency selection rule.
Onlyunits that appear in the training database more than 20times were selected, resulting in a set of 2330 context-dependent phones.
A maximum of 16 mixture compo-nents was used for each acoustic HMM state.
The HMMparameters were estimated by means of the segmental k-means training procedure \[11\].The recognition lexicon consisted of 1153 lexical entriesincluding 1060 words appearing in the Feb91 benchmarkevaluation and 93 compound words which were mostlyconcatenation of letters to form acronyms.
Each en-try had a single pronunciation.
In addition, two non-phonetic units, one for modeling weak extraneous (outof vocabulary) speech events and the other for model-ing strong extraneous speech events, were included, likein \[12\].Word bigrams were used in the test.
They were esti-mated using the same set of 6259 annotated sentences,and smoothed with backoff probabilities.
The perplex-ity of the language defined by the bigram probabilities,computed on the training set, was found to be 17.Data  ~ ofor ig in u t te rancesMIT 193BBN 194CMU 193SRI 193ATT 197OVERALL 970word s t r inger ror  e r ro r9.7 47.213.1 58.817.8 75.121.5 68.428.3 76.117.5 64.6Table 6: Score for the SPREC ATIS February 92 test4 .1 .
SPREC resu l t s  on  February  1992tes tThe speech recognition results are summaried in Table 6Overall we observed 17.5% word error and 64.6% stringerror.In the current system configuration, only 6259 utterances(about 12 hours of speech) were used to create the acous-tic HMM models.
Out of the 218 speakers, 15 of themwere from the ATT training set and 17 of them werefrom the CMU training set, which amounts to about 90minutes of training data from each of them.
We can seefrom Table 6 that there is a problem due to an insuffi-cient training for ATT and CMU test data.
On the otherhand, since most of the training data we used were col-lected at BBN and MIT, the performance is better forBBN and MIT test speakers.94 out of the 427 deleted words were A and THE.
Shortfunction words amounted to over 90% of the deletion er-rors.
As for the 328 insertion errors, 46 of them wereinsertion of words A and THE.
Again, short functionwords contributed to over 90% of the insertion errors.Since function words, in most cases, did not affect themeaning of a recognized sentence, we expect that sucherrors did not degrade the performance of the NL mod-ule.Substitution errors had a greater impact on the SLSsystem performance than insertion and deletion errors.Most of the substitution errors can be categorized intothree types:1.
Out-of-vocabulary words;2.
Morphological inflections of words, which are dif-ficult to discriminate acoustically for band-limiteddata;3. short function words.Out of the 1153 substitution error, 66 were caused byout-of-vocabulary words, and 127 were caused by mor-phological inflections.
For the remaining 85% of the er-70Class # T F NA W.  Err .A 402 208 118 76 77.6D 285 92 115 78 108.1A+D 687 300 233 154 90.2cies found in the answer formatter, that we don't believeaffects the real performance of the CHRONUS system.Nevertheless, this suggests the importance of investigat-ing a more meaningful and more rubust scoring criterion.Table 7: Official NIST score for the SLS ATIS February92 testrors, about half involved short function words.5.
SLS RESULTS ON FEBRUARY1992 TESTThe integrated SLS system for the February 1992 testwas implemented by using the best first recognized stringfrom our speech recognizer as input to the NL system.Table 7 reports the official results from NIST and Table 8reports our results after the format bug was fixed.6.
CONCLUSIONSIn this paper we give a global outline of the CHRONUSspeech understanding system.
The system is builtaround the conceptual decoder, a Viterbi decoder thatuses a stochastic model for extracting the conceptualcontent of an input sentence.
Although the problem isformalized in such a way that the decoder could also ex-tract the actual value of the conceptual relations (notonly their category), the limited amount of training sen-tences currently available suggested the use of a moretraditional pattern matcher (the template generator)along with the conceptual decoder.
However, the ad-vantage of the stochastic formalization is the trainabil-ity of the model over a database of suitably annotatedexamples.
The annotation given with the MADCOWsentences and the comparator developed by NIST pro-vide a useful feedback signal that allows to automatizethe training procedure.
In a preliminary experiment de-signed to test this procedure we show that a significantimprovement of the accuracy of the system can be ob-tained without human supervision.The results on the February 92 ATIS test are then re-ported in the paper.
The big discrepancy between theofficial NIST score and the score obtained in a succes-sive assessment of the system is explained by inaccura-Class ~ TA 402 237D 285 117A+D 687 354F NA89 7690 78179 154W, Err ,63.290.574.5REFERENCES1.
Pieraccini, R., Levin, E., Lee, C. H., "Stochastic Rep-resentation of Conceptual Structure in the ATIS Task,"Proc.
of 4th DARPA Workshop on Speech and NaturalLanguage, Asilomar (CA), February 1991.2.
Prieto, N., Vidal, E., "Learning Language Modelsthrough the ECGI Method," Proc.
of EUROSPEECH91, Genova, Italy, September 1991.3.
Simmons, R. Semantic networks: their computation anduse for understanding English sentences, In Schank andColby, eds Computer Models of Thought and Language,Freeman: San Francisco, 1973.4.
Kittredge, Analyzing language in restricted domains:Sublanguage description and processing, In Grishman,ed.
Lawrence Erlbaum, 1986.5.
Pieraccini, R., Levin, E., "Stochastic Representation ofSemantic Structure for Speech Understanding," Proc.
ofEUROSPEECH 91, Genova, Italy, September 1991.6.
Pieraccini, R., Tzoukermann, E., Gorelov, Z., Gauvain,3.
L., Levin, E., Lee, C. H., Wilpon, J. G., "A SpeechUnderstanding System Based on Statistical Representa-tion of Semantics," Proc.
oflCASSP 92, San Francisco,CA, March 1992.7.
Bosen, S., Ramshaw, L., Ayuso, D., Bates, M., "A Pro-posal for SLS Evaluation," Proc.
of 2nd DARPA Work-shop on Speech and Natural Language, Cape Cod (MA),October 1989.8.
Lee, C. H., Giachin, E., Rabiner, L. R., Pieraccini,R., Rosenberg, A. E., "Improved Acoustic Modelingfor Speaker Independent Large Vocabulary ContinuousSpeech Recognition," Proc.
of ICASSP 1991, Toronto,Ontario, May 1991.9.
Gauvain, J. L., Lee, C. H., "Bayesian Learning fro Hid-den Markov Models with Gaussian Mixture State Obser-vation densities," Proc.
of EUROSPEECH 91, Genova,Italy, September 1991.10.
Lee, C. H., Rabiner, L. R., Pieraccini, R., Wilpon, J.G., "Acoustic Modeling for Large Vocabulary SpeechRecognition," Computer, Speech and Language, 4, pp.127-165, 1990.11.
Rabiner, L. R., , Wilpon, J. G., Juang, B. H., "Asegmental k-means training procedure for connectedword recognition based on whole word reference pat-terns," AT~T Technical Journal, vol.
65 no.
3, pp 21-31,May/June 198612.
Wilpon, J. G., Rabiner, L. R., Lee, C. H., Goldman,E.
R., "Automatic Recognition of Keywords in Uncon-strained Speech Using Hidden Markov Models," IEEETrans.
ASSP, Vol.
38, No.
11, pp.1870-1878Table 8: Score for the SLS ATIS February 92 after theformat bug was removed71
