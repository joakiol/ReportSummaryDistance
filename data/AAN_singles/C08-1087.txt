Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 689?696Manchester, August 2008Scientific Paper Summarization Using Citation Summary NetworksVahed QazvinianSchool of InformationUniversity of MichiganAnn Arbor, MIvahed@umich.eduDragomir R. RadevDepartment of EECS andSchool of InformationUniversity of MichiganAnn Arbor, MIradev@umich.eduAbstractQuickly moving to a new area of researchis painful for researchers due to the vastamount of scientific literature in each fieldof study.
One possible way to overcomethis problem is to summarize a scientifictopic.
In this paper, we propose a model ofsummarizing a single article, which can befurther used to summarize an entire topic.Our model is based on analyzing others?viewpoint of the target article?s contribu-tions and the study of its citation summarynetwork using a clustering approach.1 IntroductionIt is quite common for researchers to have toquickly move into a new area of research.
Forinstance, someone trained in text generation maywant to learn about parsing and someone whoknows summarization well, may need to learnabout question answering.
In our work, we try tomake this transition as painless as possible by au-tomatically generating summaries of an entire re-search topic.
This enables a researcher to find thechronological order and the progress in that par-ticular field of study.
An ideal such system will re-ceive a topic of research, as the user query, and willreturn a summary of related work on that topic.
Inthis paper, we take the first step toward buildingsuch a system.Studies have shown that different citations to thesame article often focus on different aspects of thatarticle, while none alone may cover a full descrip-tion of its entire contributions.
Hence, the set ofc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.citation summaries, can be a good resource to un-derstand the main contributions of a paper and howthat paper affects others.
The citation summary ofan article (A), as defined in (Elkiss et al, 2008),is a the set of citing sentences pointing to that ar-ticle.
Thus, this source contains information aboutA from others?
point of view.
Part of a sample ci-tation summary is as follows:In the context of DPs, this edge based factorization methodwas proposed by (Eisner, 1996).Eisner (1996) gave a generative model with a cubic parsingalgorithm based on an edge factorization of trees.Eisner (Eisner, 1996) proposed anO(n3) parsing algorithm for PDG.If the parse has to be projective, Eisner?sbottom-up-span algorithm (Eisner, 1996) can beused for the search.The problem of summarizing a whole scientifictopic, in its simpler form, may reduce to summa-rizing one particular article.
A citation summarycan be a good resource to make a summary of atarget paper.
Then using each paper?s summaryand some knowledge of the citation network, we?llbe able to generate a summary of an entire topic.Analyzing citation networks is an important com-ponent of this goal, and has been widely studiedbefore (Newman, 2001).Our main contribution in this paper is to use ci-tation summaries and network analysis techniquesto produce a summary of a single scientific articleas a framework for future research on topic sum-marization.
Given that the citation summary of anyarticle usually has more than a few sentences, themain challenge of this task is to find a subset ofthese sentences that will lead to a better and shortersummary.689Cluster Nodes EdgesDP 167 323PBMT 186 516Summ 839 1425QA 238 202TE 56 44Table 1: Clusters and their citation network size1.1 Related WorkAlthough there has been work on analyzing ci-tation and collaboration networks (Teufel et al,2006; Newman, 2001) and scientific article sum-marization (Teufel and Moens, 2002), to theknowledge of the author there is no previous workthat study the text of the citation summaries toproduce a summary.
(Bradshaw, 2003; Bradshaw,2002) get benefit from citations to determine thecontent of articles and introduce ?Reference Di-rected Indexing?
to improve the results of a searchengine.In other work, (Nanba et al, 2004b; Nanba etal., 2004a) analyze citation sentences and automat-ically categorize citations into three groups using160 pre-defined phrase-based rules.
This catego-rization is then used to build a tool for survey gen-eration.
(Nanba and Okumura, 1999) also discussthe same citation categorization to support a sys-tem for writing a survey.
(Nanba and Okumura,1999; Nanba et al, 2004b) report that co-citationimplies similarity by showing that the textual simi-larity of co-cited papers is proportional to the prox-imity of their citations in the citing article.Previous work has shown the importance ofthe citation summaries in understanding what apaper says.
The citation summary of an articleA is the set of sentences in other articles whichcite A.
(Elkiss et al, 2008) performed a large-scale study on citation summaries and their impor-tance.
They conducted several experiments on aset of 2, 497 articles from the free PubMed Cen-tral (PMC) repository1.
Results from this exper-iment confirmed that the ?Self Cohesion?
(Elkisset al, 2008) of a citation summary of an arti-cle is consistently higher than the that of its ab-stract.
(Elkiss et al, 2008) also conclude that ci-tation summaries are more focused than abstracts,and that they contain additional information thatdoes not appear in abstracts.
(Kupiec et al, 1995)use the abstracts of scientific articles as a targetsummary, where they use 188 Engineering Infor-mation summaries that are mostly indicative in na-1http://www.pubmedcentral.govture.
Abstracts tend to summarize the documentstopics well, however, they don?t include much useof metadata.
(Kan et al, 2002) use annotated bib-liographies to cover certain aspects of summariza-tion and suggest guidelines that summaries shouldalso include metadata and critical document fea-tures as well as the prominent content-based fea-tures.Siddharthan and Teufel describe a new task todecide the scientific attribution of an article (Sid-dharthan and Teufel, 2007) and show high humanagreement as well as an improvement in the per-formance of Argumentative Zoning (Teufel, 2005).Argumentative Zoning is a rhetorical classificationtask, in which sentences are labeled as one of Own,Other, Background, Textual, Aim, Basis, Contrastaccording to their role in the author?s argument.These all show the importance of citation sum-maries and the vast area for new work to analyzethem to produce a summary for a given topic.2 DataThe ACL Anthology is a collection of papers fromthe Computational Linguistics journal, and pro-ceedings from ACL conferences and workshopsand includes almost 11, 000 papers.
To producethe ACL Anthology Network (AAN), (Joseph andRadev, 2007) manually performed some prepro-cessing tasks including parsing references andbuilding the network metadata, the citation, andthe author collaboration networks.The full AAN includes all citation and collabo-ration data within the ACL papers, with the citationnetwork consisting of 8, 898 nodes and 38, 765 di-rected edges.2.1 ClustersWe built our corpus by extracting small clustersfrom the AAN data.
Each cluster includes pa-pers with a specific phrase in the title or con-tent.
We used a very simple approach to col-lect papers of a cluster, which are likely to betalking about the same topic.
Each cluster con-sists of a set of articles, in which the topicphrase is matched within the title or the contentof papers in AAN.
In particular, the five clus-ters that we collected this way, are: DependencyParsing (DP), Phrased Based Machine Translation(PBMT), Text Summarization (Summ), QuestionAnswering (QA), and Textual Entailment (TE).Table 1 shows the number of articles and citationsin each cluster.
For the evaluation purpose we690ACL-ID Title Year CS SizeDPC96-1058 Three New Probabilistic Models For Dependency Parsing: An Exploration 1996 66P97-1003 Three Generative, Lexicalized Models For Statistical Parsing 1997 55P99-1065 A Statistical Parser For Czech 1999 54P05-1013 Pseudo-Projective Dependency Parsing 2005 40P05-1012 On-line Large-Margin Training Of Dependency Parsers 2005 71PBMTN03-1017 Statistical Phrase-Based Translation 2003 180W03-0301 An Evaluation Exercise For Word Alignment 2003 14J04-4002 The Alignment Template Approach To Statistical Machine Translation 2004 50N04-1033 Improvements In Phrase-Based Statistical Machine Translation 2004 24P05-1033 A Hierarchical Phrase-Based Model For Statistical Machine Translation 2005 65SummA00-1043 Sentence Reduction For Automatic Text Summarization 2000 19A00-2024 Cut And Paste Based Text Summarization 2000 20C00-1072 The Automated Acquisition Of Topic Signatures For Text Summarization 2000 19W00-0403 Centroid-Based Summarization Of Multiple Documents: Sentence Extraction, ... 2000 31W03-0510 The Potential And Limitations Of Automatic Sentence Extraction For Summarization 2003 15QAA00-1023 A Question Answering System Supported By Information Extraction 2000 13W00-0603 A Rule-Based Question Answering System For Reading Comprehension Tests 2002 19P02-1006 Learning Surface Text Patterns For A Question Answering System 2002 74D03-1017 Towards Answering Opinion Questions: Separating Facts From Opinions ... 2003 42P03-1001 Offline Strategies For Online Question Answering: Answering Questions Before They Are Asked 2003 27TED04-9907 Scaling Web-Based Acquisition Of Entailment Relations 2004 12H05-1047 A Semantic Approach To Recognizing Textual Entailment 2005 8H05-1079 Recognising Textual Entailment With Logical Inference 2005 9W05-1203 Measuring The Semantic Similarity Of Texts 2005 17P05-1014 The Distributional Inclusion Hypotheses And Lexical Entailment 2005 10Table 2: Papers chosen from clusters for evaluation, with their publication year, and citation summarysizechose five articles from each cluster.
Table 2 showsthe title, year, and citation summary size for the 5papers chosen from each cluster.
The citation sum-mary size of a paper is the size of the set of citationsentences that cite that paper.3 Analysis3.1 Fact DistributionWe started with an annotation task on 25 papers,listed in Table 2, and asked a number of annota-tors to read the citation summary of each paperand extract a list of the main contributions of thatpaper.
Each item on the list is a non-overlappingcontribution (fact) perceived by reading the cita-tion summary.
We asked the annotators to focuson the citation summary to do the task and not ontheir background on this topic.As our next step we manually created the unionof the shared and similar facts by different anno-tators to make a list of facts for each paper.
Thisfact list made it possible to review all sentences inthe citation summary to see which facts each sen-tence contained.
There were also some unsharedfacts, facts that only appear in one annotator?s re-sult, which we ignored for this paper.Table 3 shows the shared and unshared facts ex-tracted by four annotators for P99-1065.The manual annotation of P99-1065 shows thatthe fact ?Czech DP?
appears in 10 sentences outof all 54 sentences in the citation summary.
Thisshows the importance of this fact, and that ?Depen-Fact OccurrencesSharedf4: ?Czech DP?
10f1: ?lexical rules?
6f3: ?POS/ tag classification?
6f2: ?constituency parsing?
5f5: ?Punctuation?
2f6: ?Reordering Technique?
2f7: ?Flat Rules?
2Unshared?Dependency conversion?
?80% UAS?
?97.0% F-measure?
?Generative model?
?Relabel coordinated phrases?
?Projective trees?
?Markovization?Table 3: Facts of P99-1065dency Parsing of Czech?
is one of the main contri-butions of this paper.
Table 3 also shows the num-ber of times each shared fact appears in P99-1065?scitation summary sorted by occurrence.After scanning through all sentences in the ci-tation summary, we can come up with a fact dis-tribution matrix for an article.
The rows of thismatrix represent sentences in the citation summaryand the columns show facts.
A 1 value in this ma-trix means that the sentence covers the fact.
Thematrix D shows the fact distribution of P99-1065.IDs in each row show the citing article?s ACL ID,and the sentence number in the citation summary.These matrices, created using annotations, are par-ticularly useful in the evaluation process.691D =0BBBBBBBBBBBBBBBBB@f1f2f3f4f5f6f7W06-2935:1 1 0 0 0 0 0 0W06-2935:2 0 0 0 0 0 0 0W06-2935:3 0 0 1 1 0 0 0W06-2935:4 0 0 0 0 0 0 1W06-2935:5 0 0 0 0 0 0 0W06-2935:6 0 0 0 0 1 0 0W05-1505:7 0 1 0 1 0 0 0W05-1505:8 0 0 0 0 0 1 0............W05-1518:54 0 0 0 0 0 0 01CCCCCCCCCCCCCCCCCA3.2 Similarity MeasuresWe want to build a network with citing sentencesas nodes and similarities of two sentences as edgeweights.
We?d like this network to have a nicecommunity structure, whereby each cluster corre-sponds to a fact.
So, a similarity measure in whichwe are interested is the one which results in highvalues for pairs of sentences that cover the samefacts.
On the other hand, it should return a lowvalue for pairs that do not share a common contri-bution of the target article.The following shows two sample sentences fromP99-1065 that cover the same fact and we want thechosen similarity measure to return a high valuefor them:So, Collins et al(1999) proposed a tag classification forparsing the Czech treebank.The Czech parser of Collins et al(1999) was run on a dif-ferent data set... .Conversely, we?d like the similarity of the two fol-lowing sentences that cover no shared facts, to bequite low:Collins (1999) explicitly added features to his parser to im-prove punctuation dependency parsing accuracy.The trees are then transformed into Penn Treebankstyle constituencies- using the technique described in(Collins et al 1999).We used P99-1065 as the training sample, onwhich similarity metrics were trained, and left theothers for the test.
To evaluate a similarity mea-sure for our purpose we use a simple approach.
Foreach measure, we sorted the similarity values of allpairs of sentences in P99-1065?s citation summaryin a descending order.
Then we simply counted thenumber of pairs that cover the same fact (out of 172such fact sharing pairs) in the top 100, 200 and 300highly similar ones out of total 2, 862 pairs.
Table4 shows the number of fact sharing pairs amongthe top highest similar pairs.
Table 4 shows howcosine similarity that uses a tf-idf measure outper-forms the others.
We tried three different poli-cies for computing IDF values to compute cosineMeasure Top 100 Top 200 Top 300tf-idf (General) 34 66 74tf-idf (AAN) 34 56 74LCSS 26 37 54tf 24 34 46tf2gen 13 26 35tf-idf (DP) 16 26 28Levenshtein 2 9 16Table 4: Different similarity measures and theirperformances in favoring fact-sharing sentences.Each column shows the number of fact-sharingpairs among top highly similar pairs.similarity: a general IDF, an AAN-specific IDFwhere IDF values are calculated only using thedocuments of AAN, and finally DP-specific IDFin which we only used all-DP data set.
Table 4also shows the results for an asymmetric similaritymeasure, generation probability (Erkan, 2006) aswell as two string edit distances: the longest com-mon substring and the Levenshtein distance (Lev-enshtein, 1966).4 MethodologyIn this section we discuss our graph clusteringmethod for article summarization, as well as otherbaseline methods used for comparisons.4.1 Network-Based ClusteringThe Citation Summary Network of an article A isa network in which each sentence in the citationsummary of A is a node.
This network is a com-plete undirected weighted graph where the weightof an edge between two nodes shows the similarityof the two corresponding sentences of those nodes.The similarity that we use, as described in sec-tion 3.2, is such that sentences with the same factshave high similarity values.
In other words, strongedges in the citation summary network are likelyto indicate a shared fact between two sentences.A graph clustering method tries to cluster thenodes of a graph in a way that the average intra-cluster similarity is maximum and the averageinter-cluster similarity is minimum.
To find thecommunities in the citation summary network weuse (Clauset et al, 2004), a hierarchical agglom-eration algorithm which works by greedily opti-mizing the modularity in a linear running time forsparse graphs.To evaluate how well the clustering method works,we calculated the purity for the clusters found ofeach paper.
Purity (Manning et al, 2008) is amethod in which each cluster is assigned to theclass with the majority vote in the cluster, and then692ACL-ID #Facts |C| #Clusters |?| Purity(?,C)DPC96-1058 4 4 0.636P97-1003 5 5 0.750P99-1065 7 7 0.724P05-1013 5 3 0.689P05-1012 7 5 0.500PBMTN03-1017 8 4 0.464W03-0301 3 3 0.777J04-4002 5 5 0.807N04-1033 5 4 0.615P05-1033 6 5 0.650SummA00-1043 5 4 0.812A00-2024 5 2 0.333C00-1072 3 4 0.857W00-0403 6 4 0.682W03-0510 4 3 0.727QAA00-1023 3 2 0.833W00-0603 7 4 0.692P02-1006 7 5 0.590D03-1017 7 4 0.500P03-1001 6 4 0.500TED04-9907 7 3 0.545H05-1047 4 3 0.833H05-1079 5 3 0.625W05-1203 3 3 0.583P05-1014 4 2 0.667Table 5: Number of real facts, clusters and purityfor each evaluated articlethe accuracy of this assignment is measured by di-viding the number of correctly assigned documentsby N .
More formally:purity(?,C) =1N?kmaxj|?k?
cj|where ?
= {?1, ?2, .
.
.
, ?K} is the set of clus-ters and C = {c1, c2, .
.
.
, cJ} is the set of classes.
?kis interpreted as the set of documents in ?kandcjas the set of documents in cj.
For each evalu-ated article, Table 5 shows the number of real facts(|C| = J), the number of clusters (|?| = K) andpurity(?,C) for each evaluated article.
Figure 1shows the clustering result for J04-4002, in whicheach color (number) shows a real fact, while theboundaries and capital labels show the clusteringresult.4.1.1 Sentence ExtractionOnce the graph is clustered and communities areformed, to build a summary we extract sentencesfrom the clusters.
We tried these two different sim-ple methods:?
Cluster Round-Robin (C-RR)We start with the largest cluster, and extractsentences in the order they appear in eachcluster.
So we extract first, the first sentencesfrom each cluster, then the second ones, andso on, until we reach the summary lengthlimit |S|.
Previously, we mentioned that factswith higher weights appear in greater num-ber of sentences, and clustering aims to clus-ter such fact-sharing sentences in the same"#$%&Figure 1: Each node is a sentence in the citationsummary for paper J04-4002.
Colors (numbers)represent facts and boundaries show the clusteringresultcommunities.
Thus, starting with the largestcommunity is important to ensure that thesystem summary first covers the facts thathave higher frequencies and therefore higherweights.?
Cluster Lexrank (C-lexrank)The second method we used was Lexrank(Erkan and Radev, 2004) inside each cluster.In other words, for each cluster ?iwe made alexical network of the sentences in that clus-ter (Ni) .
Using Lexrank we can find themost central sentences in Nias salient sen-tences of ?ito include in the main summary.We simply choose, for each cluster ?i, themost salient sentence of ?i, and if we havenot reached the summary length limit, we dothat for the second most salient sentences ofeach cluster, and so on.
The way of orderingclusters is again by decreasing size.Table 6 shows the two system summaries gen-erated with C-RR and C-lexrank methods for P99-1065.
The sentences in the table appear as theywere extracted automatically from the text files ofpapers, containing sentence fragments and malfor-mations occurring while doing the automatic seg-mentation.4.2 Baseline MethodsWe also conducted experiments with two baselineapproaches.
To produce the citation summary weused Mead?s (Radev et al, 2004) Random Sum-mary and Lexrank (Erkan and Radev, 2004) onthe entire citation summary network as baselinetechniques.
Lexrank is proved to work well inmulti-document summarization (Erkan and Radev,2004).
It first builds a lexical network, in which693ID SentenceC-RRW05-1505:9 3 Constituency Parsing for Dependency Trees A pragmatic justification for using constituency- based parser in orderto predict dependency struc- tures is that currently the best Czech dependency- tree parser is a constituency-based parser (Collins et al 1999; Zeman, 2004).W04-2407:27 However, since most previous studies instead use the mean attachment score per word (Eisner, 1996; Collins et al 1999), we will give this measure as well.J03-4003:33 3 We find lexical heads in Penn Treebank data using the rules described in Appendix A of Collins (1999).H05-1066:51 Furthermore, we can also see that the MST parsers perform favorably compared to the more powerfullexicalized phrase-structure parsers, such as those presented by Collins et al(1999) and Zeman (2004) that use expensive O(n5) parsing al- gorithms.E06-1011:21 5.2 Czech Results For the Czech data, we used the predefined train- ing, development and testing splitof the Prague Dependency Treebank (Hajic et al 2001), and the automatically generated POS tags supplied with the data,which we reduce to the POS tag set from Collins et al(1999).C-LexrankP05-1012:16 The Czech parser of Collins et al(1999) was run on a different data set and most other dependency parsers are evaluated using English.W04-2407:26 More precisely, parsing accuracy is measured by the attachment score, which isa standard measure used in studies of dependency parsing (Eisner, 1996; Collins et al 1999).W05-1505:14 In an attempt to extend a constituency-based pars- ing model to train on dependency trees,Collins transforms the PDT dependency trees into con- stituency trees (Collins et al 1999).P06-1033:31 More specifi- cally for PDT, Collins et al(1999) relabel coordi- nated phrases after converting dependency struc- tures to phrasestructures, and Zeman (2004) uses a kind of pattern matching, based on frequencies of the parts-of-speech of conjuncts and conjunc- tions.P05-1012:17 In par- ticular, we used the method of Collins et al(1999) to simplify part-of-speech tags sincethe rich tags used by Czech would have led to a large but rarely seen set of POS features.Table 6: System Summaries for P99-1065.
(a) Using C-RR, (b) using C-Lexrank with length of 5sentencesnodes are sentences and a weighted edge betweentwo nodes shows the lexical similarity.
Once thisnetwork is built, Lexrank performs a random walkto find the most central nodes in the graph and re-ports them as summary sentences.5 Experimental Setup5.1 Evaluation MethodFact-based evaluation systems have been used inseveral past projects (Lin and Demner-Fushman,2006; Marton and Radul, 2006), especially inthe TREC question answering track.
(Lin andDemner-Fushman, 2006) use stemmed unigramsimilarity of responses with nugget descriptions toproduce the evaluation results, whereas (Martonand Radul, 2006) uses both human judgments andhuman descriptions to evaluate a response.An ideal summary in our model is one that cov-ers more facts and more important facts.
Our def-inition for the properties of a ?good?
summary ofa paper is one that is relatively short and consistsof the main contributions of that paper.
From thisviewpoint, there are two criteria for our evaluationmetric.
First, summaries that contain more impor-tant facts are preferred over summaries that coverfewer relevant facts.
Second, facts should not beequally weighted in this model, as some of themmay show more important contributions of a pa-per, while others may not.To evaluate our system, we use the pyra-mid evaluation method (Nenkova and Passonneau,2004) at sentence level.
Each fact in the citationsummary of a paper is a summarization contentunit (SCU) (Nenkova and Passonneau, 2004), andthe fact distribution matrix, created by annotation,provides the information about the importance ofeach fact in the citation summary.The score given by the pyramid method for asummary is a ratio of the sum of the weights ofits facts to the sum of the weights of an optimalsummary.
This score ranges from 0 to 1, and highscores show the summary content contain moreheavily weighted facts.
We believe that if a factappears in more sentences of the citation summarythan another fact, it is more important, and thusshould be assigned a higher weight.
To weight thefacts we build a pyramid, and each fact falls in atier.
Each tier shows the number of sentences a factappears in.
Thus, the number of tiers in the pyra-mid is equal to the citation summary size.
If a factappears in more sentences, it falls in a higher tier.So, if the fact fiappears |fi| times in the citationsummary it is assigned to the tier T|fi|.The pyramid score formula that we use is com-puted as follows.
Suppose the pyramid has n tiers,Ti, where tier Tnon top and T1on the bottom.
Theweight of the facts in tier Tiwill be i (i.e.
they ap-peared in i sentences).
If |Ti| denotes the numberof facts in tier Ti, and Diis the number of facts inthe summary that appear in Ti, then the total factweight for the summary is D =?ni=1i?Di.
Ad-ditionally, the optimal pyramid score for a sum-mary with X facts, isMax =?ni=j+1i?|Ti|+j?(X?
?ni=j+1|Ti|)where j = maxi(?nt=i|Tt| ?
X).
Subsequently,the pyramid score for a summary is calculated asP =DMax.6945.2 Results and DiscussionBased on the described evaluation method we con-ducted a number of experiments to evaluate dif-ferent summaries of a given length.
In particular,we use a gold standard and a random summary todetermine how good a system summary is.
Thegold standard is a summary of a given length thatcovers as many highly weighted facts as possible.To make a gold summary we start picking sen-tences that cover new and highly weighted facts,until the summary length limit is reached.
On theother hand, in the random summary sentences areextracted from the citation summary in a randommanner.
We expect a good system summary to becloser to the gold than it is to the random one.Table 7 shows the value of pyramid score P , forthe experiments on the set of 25 papers.
A P scoreof less than 1 for a gold shows that there are morefacts than can be covered with a set of |S| sen-tences.This table suggests that C-lexrank has a higheraverage score, P , for the set of evaluated articlescomparing C-RR and Lexrank.As mentioned earlier in section 4.1.1, once thecitation summary network is clustered in the C-RRmethod, the sentences from each cluster are chosenin a round robin fashion, which will not guaranteethat a fact-bearing sentence is chosen.This is because all sentences, whether theycover any facts or not, are assigned to some clus-ter anyway and such sentences might appear as thefirst sentence in a cluster.
This will sometimes re-sult in a low P score, for which P05-1012 is a goodexample.6 Conclusion and Future WorkIn this work we use the citation summaries to un-derstand the main contributions of articles.
Thecitation summary size, in our experiments, rangesfrom a few sentences to a few hundred, of whichwe pick merely a few (5 in our experiments) mostimportant ones.As a method of summarizing a scientific paper,we propose a clustering approach where commu-nities in the citation summary?s lexical networkare formed and sentences are extracted from sep-arate clusters.
Our experiments show how ourclustering method outperforms one of the cur-rent state-of-art multi-document summarizing al-gorithms, Lexrank, on this particular problem.A future improvement will be to use a reorder-ing approach like Maximal Marginal RelevanceArticleGoldMead?sRandomLexrankC-RRC-lexrankDPC96-1058 1.00 0.27 0.73 0.73 0.73P97-1003 1.00 0.08 0.40 0.60 0.40P99-1065 0.94 0.30 0.54 0.82 0.67P05-1013 1.00 0.15 0.69 0.97 0.67P05-1012 0.95 0.14 0.57 0.26 0.62PBMTN03-1017 0.96 0.26 0.36 0.61 0.64W03-0301 1.00 0.60 1.00 1.00 1.00J04-4002 1.00 0.33 0.70 0.48 0.48N04-1033 1.00 0.38 0.38 0.31 0.85P05-1033 1.00 0.37 0.77 0.77 0.85SummA00-1043 1.00 0.66 0.95 0.71 0.95A00-2024 1.00 0.26 0.86 0.73 0.60C00-1072 1.00 0.85 0.85 0.93 0.93W00-0403 1.00 0.55 0.81 0.41 0.70W03-0510 1.00 0.58 1.00 0.83 0.83QAA00-1023 1.00 0.57 0.86 0.86 0.86W00-0603 1.00 0.33 0.53 0.53 0.60P02-1006 1.00 0.49 0.92 0.49 0.87D03-1017 1.00 0.00 0.53 0.26 0.85P03-1001 1.00 0.12 0.29 0.59 0.59TED04-9907 1.00 0.53 0.88 0.65 0.94H05-1047 1.00 0.83 0.66 0.83 1.00H05-1079 1.00 0.67 0.78 0.89 0.56W05-1203 1.00 0.50 0.71 1.00 0.71P05-1014 1.00 0.44 1.00 0.89 0.78Mean 0.99 0.41 0.71 0.69 0.75Table 7: Evaluation Results (|S| = 5)(MMR) (Carbonell and Goldstein, 1998) to re-rankclustered documents within each cluster in orderto reduce the redundancy in a final summary.
An-other possible approach is to assume the set of sen-tences in the citation summary as sentences talk-ing about the same event, yet generated in differ-ent sources.
Then one can apply the method in-spired by (Barzilay et al, 1999) to identify com-mon phrases across sentences and use languagegeneration to form a more coherent summary.
Theultimate goal, however, is to produce a topic sum-marizer system in which the query is a scientifictopic and the output is a summary of all previousworks in that topic, preferably sorted to preservechronology and topicality.7 AcknowledgmentsThe authors would like to thank Bonnie Dorr,Jimmy Lin, Saif Mohammad, Judith L. Klavans,Ben Shneiderman, and Aleks Aris from UMD,Bryan Gibson, Joshua Gerrish, Pradeep Muthukr-ishnan, Arzucan?Ozg?ur, Ahmed Hassan, and ThuyVu from University of Michigan for annotations.This paper is based upon work supported by theNational Science Foundation grant ?iOPENER: AFlexible Framework to Support Rapid Learning inUnfamiliar Research Domains?, jointly awardedto U. of Michigan and U. of Maryland as IIS0705832.
Any opinions, findings, and conclusionsor recommendations expressed in this paper are695those of the authors and do not necessarily reflectthe views of the National Science Foundation.ReferencesBarzilay, Regina, Kathleen R. McKeown, and MichaelElhadad.
1999.
Information fusion in the context ofmulti-document summarization.
In ACL?99, pages550?557.Bradshaw, Shannon.
2002.
Reference Directed Index-ing: Indexing Scientific Literature in the Context ofIts Use.
Ph.D. thesis, Northwestern University.Bradshaw, Shannon.
2003.
Reference directed index-ing: Redeeming relevance for subject search in ci-tation indexes.
In Proceedings of the 7th EuropeanConference on Research and Advanced Technologyfor Digital Libraries.Carbonell, Jaime G. and Jade Goldstein.
1998.
The useof MMR, diversity-based reranking for reorderingdocuments and producing summaries.
In SIGIR?98,pages 335?336.Clauset, Aaron, Mark E. J. Newman, and CristopherMoore.
2004.
Finding community structure in verylarge networks.
Phys.
Rev.
E, 70(6):066111, Dec.Elkiss, Aaron, Siwei Shen, Anthony Fader, G?unes?Erkan, David States, and Dragomir R. Radev.
2008.Blind men and elephants: What do citation sum-maries tell us about a research article?
Journal of theAmerican Society for Information Science and Tech-nology, 59(1):51?62.Erkan, G?unes?
and Dragomir R. Radev.
2004.
Lexrank:Graph-based centrality as salience in text summa-rization.
Journal of Artificial Intelligence Research(JAIR).Erkan, G?unes?.
2006.
Language model-based docu-ment clustering using random walks.
In Proceed-ings of the HLT-NAACL conference, pages 479?486,New York City, USA, June.
Association for Compu-tational Linguistics.Joseph, Mark T. and Dragomir R. Radev.
2007.
Ci-tation analysis, centrality, and the ACL Anthol-ogy.
Technical Report CSE-TR-535-07, Universityof Michigan.
Department of Electrical Engineeringand Computer Science.Kan, Min-Yen, Judith L. Klavans, and Kathleen R.McKeown.
2002.
Using the Annotated Bibliogra-phy as a Resource for Indicative Summarization.
InProceedings of LREC 2002, Las Palmas, Spain.Kupiec, Julian, Jan Pedersen, and Francine Chen.1995.
A trainable document summarizer.
In SIGIR?95, pages 68?73, New York, NY, USA.
ACM.Levenshtein, Vladimir I.
1966.
Binary Codes Capa-ble of Correcting Deletions, Insertions and Rever-sals.
Soviet Physics Doklady, 10:707.Lin, Jimmy J. and Dina Demner-Fushman.
2006.Methods for automatically evaluating answersto complex questions.
Information Retrieval,9(5):565?587.Manning, Christopher D., Prabhakar Raghavan, andHinrich Sch?utze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press.Marton, Gregory and Alexey Radul.
2006.
Nugge-teer: Automatic nugget-based evaluation using de-scriptions and judgements.
In Proceedings ofNAACL/HLT.Nanba, Hidetsugu and Manabu Okumura.
1999.
To-wards multi-paper summarization using reference in-formation.
In IJCAI1999, pages 926?931.Nanba, Hidetsugu, Takeshi Abekawa, Manabu Oku-mura, and Suguru Saito.
2004a.
Bilingual presri:Integration of multiple research paper databases.
InProceedings of RIAO 2004, pages 195?211, Avi-gnon, France.Nanba, Hidetsugu, Noriko Kando, and Manabu Oku-mura.
2004b.
Classification of research papers usingcitation links and citation types: Towards automaticreview article generation.
In Proceedings of the 11thSIG Classification Research Workshop, pages 117?134, Chicago, USA.Nenkova, Ani and Rebecca Passonneau.
2004.
Evalu-ating content selection in summarization: The pyra-mid method.
Proceedings of the HLT-NAACL con-ference.Newman, Mark E. J.
2001.
The structure of scientificcollaboration networks.
PNAS, 98(2):404?409.Radev, Dragomir, Timothy Allison, Sasha Blair-Goldensohn, John Blitzer, Arda C?elebi, Stanko Dim-itrov, Elliott Drabek, Ali Hakim, Wai Lam, DanyuLiu, Jahna Otterbacher, Hong Qi, Horacio Saggion,Simone Teufel, Michael Topper, Adam Winkel, andZhu Zhang.
2004.
MEAD - a platform for multi-document multilingual text summarization.
In LREC2004, Lisbon, Portugal, May.Siddharthan, Advaith and Simone Teufel.
2007.Whose idea was this, and why does it matter?
at-tributing scientific work to citations.
In Proceedingsof NAACL/HLT-07.Teufel, Simone and Marc Moens.
2002.
Summarizingscientific articles: experiments with relevance andrhetorical status.
Comput.
Linguist., 28(4):409?445.Teufel, Simone, Advaith Siddharthan, and Dan Tidhar.2006.
Automatic classification of citation function.In Proceedings of the EMNLP, Sydney, Australia,July.Teufel, Simone.
2005.
Argumentative Zoning for Im-proved Citation Indexing.
Computing Attitude andAffect in Text: Theory and Applications, pages 159?170.696
