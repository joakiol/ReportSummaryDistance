Proceedings of the 3rd Workshop on Predicting and Improving Text Readability for Target Reader Populations (PITR) @ EACL 2014, pages 84?93,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsAn Open Corpus of Everyday Documents for Simplification TasksDavid Pellow and Maxine EskenaziLanguage Technologies Institute, Carnegie Mellon UniversityPittsburgh PA USAdpellow@cs.cmu.edu, max@cs.cmu.eduAbstractIn recent years interest in creating statisti-cal automated text simplification systemshas increased.
Many of these systems haveused parallel corpora of articles taken fromWikipedia and Simple Wikipedia or fromSimple Wikipedia revision histories andgenerate Simple Wikipedia articles.
In thiswork we motivate the need to construct alarge, accessible corpus of everyday docu-ments along with their simplifications forthe development and evaluation of simpli-fication systems that make everyday doc-uments more accessible.
We present a de-tailed description of what this corpus willlook like and the basic corpus of every-day documents we have already collected.This latter contains everyday documentsfrom many domains including driver?s li-censing, government aid and banking.
Itcontains a total of over 120,000 sentences.We describe our preliminary work evaluat-ing the feasibility of using crowdsourcingto generate simplifications for these docu-ments.
This is the basis for our future ex-tended corpus which will be available tothe community of researchers interested insimplification of everyday documents.1 IntroductionPeople constantly interact with texts in everydaylife.
While many people read for enjoyment, sometexts must be read out of necessity.
For example, tofile taxes, open a bank account, apply for a driver?slicense or rent a house, one must read instructionsand the contents of forms, applications, and otherdocuments.
For people with limited reading ability- whether because they are not native speakers ofthe language, have an incomplete education, havea disability, or for some other reason - the readinglevel of these everyday documents can limit acces-sibility and affect their well-being.The need to present people with texts that areat a reading level which is suitable for them hasmotivated research into measuring readability ofany given text in order to assess whether automaticsimplification has rendered a more difficult textinto a more readable one.
Readability can be mea-sured using tools which assess the reading level ofa text.
We define simplification as the process ofchanging a text to lower its reading level withoutremoving necessary information or producing anungrammatical result.
This is similar to the def-inition of (cf.
(Zhu et al., 2010)), except that weavoid defining a specific, limited, set of simplifica-tion operations.
The Related Work section detailsresearch into measures of readability and work onautomatic simplification systems.We have begun to construct a large, accessi-ble corpus of everyday documents.
This corpuswill eventually contain thousands of these doc-uments, each having statistics characterising itscontents, and multiple readability measures.
Mul-tiple different simplifications will be collected forthe original documents and their content statisticsand readability measures will be included in thecorpus.
This type of large and accessible corpus isof vital importance in driving development of au-tomated text simplification.
It will provide trainingmaterial for the systems as well as a common basisof evaluating results from different systems.Thus far, we have collected a basic corpus of ev-eryday documents from a wide variety of sources.We plan to extend this basic corpus to create themuch larger and more structured corpus that wedescribe here.
We have also carried out a pre-liminary study to evaluate the feasibility of usingcrowdsourcing as one source of simplifications inthe extended corpus.
We have used Amazon Me-chanical Turk (AMT) and collected 10 simplifica-tions each for 200 sentences from the basic cor-84pus to determine feasibility, a good experimentaldesign, quality control of the simplifications, andtime and cost effectiveness.In the next section we discuss related work rel-evant to creating and evaluating a large corpus ofeveryday documents and their simplifications.
InSection 3 we further demonstrate the need for acorpus of everyday documents.
Section 4 presentsa description of our existing basic corpus.
Section5 describes the details of the extended corpus andpresents our evaluation of the feasibility of usingcrowdsourcing to generate human simplificationsfor the corpus.
Section 6 shows how the extendedcorpus will be made accessible.
Section 7 con-cludes and outlines the future work that we willundertake to develop the extended corpus.2 Related Work2.1 Readability EvaluationMeasures of readability are important becausethey help us assess the reading level of any doc-ument, provide a target for simplification systems,and help evaluate and compare the performanceof different simplification systems.
Several mea-sures of readability have been proposed; DuBay(2004) counted 200 such measures developed bythe 1980s and the number has grown, with moreadvanced automated measures introduced sincethen.Early measures of readability such as theFlesch-Kincaid grade level formula (Kincaid etal., 1975) use counts of surface features of the textsuch as number of words and number of sentences.While these older measures are less sophisticatedthan more modern reading level classifiers, theyare still widely used and reported and recent workhas shown that they can be a good first approxi-mation of more complex measures (?Stajner et al.,2012).More recent approaches use more complicatedfeatures and machine learning techniques to learnclassifiers that can predict readability.
For exam-ple, Heilman et al.
(2007) combine a naive Bayesclassifier that uses a vocabulary-based languagemodel with a k-Nearest Neighbors classifier us-ing grammatical features and interpolate the two topredict reading grade level.
Feng et al.
(2010) andFranc?ois and Miltsakaki (2012) examine a largenumber of possible textual features at various lev-els and compare SVM and Linear Regression clas-sifiers to predict grade level.
Vajjala and Meurers(2012) reported significantly higher accuracy on asimilar task using Multi-level Perceptron classifi-cation.The above two methods of measuring readabil-ity can be computed directly using the text of adocument itself.
To evaluate the performance ofa simplification system which aims to make textseasier to read and understand, it is also usefulto measure improvement in individuals?
readingand comprehension of the texts.
Siddharthan andKatsos (2012) recently studied sentence recall totest comprehension; and Temnikova and Maneva(2013) evaluated simplifications using the readers?ability to answer multiple choice questions aboutthe text.2.2 Automated Text Simplification SystemsSince the mid-90s several systems have been de-veloped to automatically simplify texts.
Early sys-tems used hand-crafted syntactic simplificationrules; for example, Chandrasekar et al.
(1996),one of the earliest attempts at automated simpli-fication.
Rule-based systems continue to be used,amongst others, Siddharthan (2006), Aluisio andGasperin (2010), and Bott et al.
(2012).Many of the more recent systems arestatistically-based adapting techniques devel-oped for statistical machine translation.
Zhuet al.
(2010) train a probabilistic model of avariety of sentence simplification rules usingexpectation maximization with a parallel corpusof aligned sentences from Wikipedia and SimpleWikipedia.
Woodsend and Lapata (2011) presenta system that uses quasi-synchronous grammarrules learned from Simple Wikipedia edit histo-ries.
They solve an integer linear programming(ILP) problem to select both which sentences aresimplified (based on a model learned from alignedWikipedia-Simple Wikipedia articles) and whatthe best simplification is.
Feblowitz and Kauchak(2013) use parallel sentences from Wikipediaand Simple Wikipedia to learn synchronous treesubstitution grammar rules.2.3 Corpora for Text SimplificationPresently there are limited resources for statisti-cal simplification methods that need to train on aparallel corpus of original and simplified texts.
Asmentioned in the previous section, common datasources are Simple Wikipedia revision historiesand aligned sentences from parallel Wikipedia andSimple Wikipedia articles.
Petersen and Ostendorf85(2007) present an analysis of a corpus of 104 orig-inal and abridged news articles, and Barzilay andElhadad (2003) present a system for aligning sen-tences trained on a corpus of parallel Encyclope-dia Britannica and Britannica Elementary articles.Other work generates parallel corpora of originaland simplified texts in languages other than En-glish for which Simple Wikipedia is not available.For example, Klerke and S?gaard (2012) built asentence-aligned corpus from 3701 original andsimplified Danish news articles, and Klaper et al.
(2013) collected 256 parallel German and simpleGerman articles.2.4 Crowdsourcing for Text Simplificationand Corpus GenerationCrowdsourcing uses the aggregate of work per-formed by many non-expert workers on smalltasks to generate high quality results for somelarger task.
To the best of our knowledge crowd-sourcing has not previously been explored indetail to generate text simplifications.
Crowd-sourcing has, however, been used to evaluatethe quality of automatically generated simplifica-tions.
Feblowitz and Kauchak (2013) used AMTto collect human judgements of the simplifica-tions generated by their system and De Clercq etal.
(2014) performed an extensive evaluation ofcrowdsourced readability judgements compared toexpert judgements.Crowdsourcing has also been used to gener-ate translations.
The recent statistical machinetranslation-inspired approaches to automated sim-plification motivate the possibility of using crowd-sourcing to collect simplifications.
Ambati andVogel (2010) and Zaidan and Callison-Burch(2011) both demonstrate the feasibility of collect-ing quality translations using AMT.
Post et al.
(2012) generated parallel corpora between Englishand six Indian languages using AMT.3 The Need for a Corpus of EverydayDocumentsA high quality parallel corpus is necessary to driveresearch in automated text simplification and eval-uation.
As shown in the Related Work section,most statistically driven simplification systemshave used parallel Wikipedia - Simple Wikipediaarticles and Simple Wikipedia edit histories.
Theresulting systems take Wikipedia articles as in-put and generate simplified versions of those ar-ticles.
While this demonstrates the possibility ofautomated text simplification, we believe that aprimary goal for simplification systems shouldbe to increase accessibility for those with poorreading skills to the texts which are most impor-tant to them.
Creating a corpus of everyday doc-uments will allow automated simplification tech-niques to be applied to texts from this domain.
Inaddition, systems trained using Simple Wikipediaonly target a single reading level - that of SimpleWikipedia.
A corpus containing multiple differentsimplifications at different reading levels for anygiven original will allow text simplification sys-tems to target specific reading levels.The research needs that this corpus aims to meetare:?
A large and accessible set of original every-day documents to:?
provide a training and test set for auto-mated text simplification?
A set of multiple human-generated simpli-fications at different reading levels for thesame set of original documents to provide:?
accessible training data for automatedtext simplification systems?
the ability to model how the same doc-ument is simplified to different readinglevels?
An accessible location to share simplifica-tions of the same documents that have beengenerated by different systems to enable:?
comparative evaluation of the perfor-mance of several systems?
easier identification and analysis of spe-cific challenges common to all systems4 Description of the Basic Corpus ofEveryday DocumentsWe have collected a first set of everyday docu-ments.
This will be extended to generate the cor-pus described in the following section.
The presentdocuments are heavily biased to the domain ofdriving since they include driving test preparationmaterials from all fifty U.S. states.
This sectionpresents the information collected about each doc-ument and its organisation in the basic corpus.
Thebasic corpus is available at: https://dialrc.org/simplification/data.html.864.1 Document FieldsEach document has a name which includesinformation about the source, contents, andtype of document.
For example the nameof the Alabama Driver Manual document isal dps driver man.
The corpus entry for eachdocument also includes the full title, the documentsource (url for documents available online), thedocument type and domain, the date retrieved, andthe date added to the corpus.
For each documentthe number of sentences, the number of words, theaverage sentence length, the Flesch-Kincaid gradelevel score, and the lexical (L) and grammatical(G) reading level scores described in Heilman etal.
(2007) are also reported.
An example of an en-try for the Alabama Driver Manual is shown inTable 1.
The documents are split so that each sen-tence is on a separate line to enable easy align-ments between the original and simplified versionsof the documents.Document Name al dps driver manFull Title Alabama Driver ManualDocument Type ManualDomain Driver?s Licensing# Sentences 1,626# Words 28,404Avg.
# words/sent 17.47F-K Grade Level 10.21Reading Level (L) 10Reading Level (G) 8.38Source http://1.usa.gov/1jjd4vwDate Added 10/01/2013Date Accessed 10/01/2013Table 1: Example basic corpus entry for AlabamaDriver Manual4.2 Corpus StatisticsThere is wide variation between the different doc-uments included in the corpus, across documentsfrom different domains and also for documentsfrom the same domain.
This includes variabilityin both document length and reading level.
For ex-ample, the driving manuals range from a lexicalreading level of 8.2 for New Mexico to 10.4 forNebraska.
Table 2 shows the statistics for the dif-ferent reading levels for the documents which havebeen collected, using the lexical readability mea-sure and rounding to the nearest grade level.
Ta-ble 3 shows the different domains for which docu-ments have been collected and the statistics for thedocuments in those domains.Reading Level (L) # Documents # Sentences4 1 235 0 06 4 2007 1 6958 6 1,8699 30 36,78310 54 83,12311 4 1,45712 1 461Table 2: Corpus statistics by lexical reading level5 Description of an Extended Corpus ofEveryday DocumentsTo meet the needs described in Section 3 the ba-sic corpus will be extended significantly.
We arestarting to collect more everyday documents fromeach of the domains in the basic corpus and toextend the corpus to other everyday documentdomains including prescription instructions, ad-vertising materials, mandatory educational test-ing, and operating manuals for common products.We are also collecting human-generated simpli-fications for these documents.
We will open upthe corpus for outside contributions of more doc-uments, readability statistics and simplificationsgenerated by various human and automated meth-ods.
This section describes what the extended cor-pus will contain and the preliminary work to gen-erate simplified versions of the documents wepresently have.5.1 Document FieldsThe extended corpus includes both original doc-uments and their simplified versions.
The originaldocuments will include all the same information asthe basic corpus, listed in Section 4.1.
Novel read-ability measures for each document can be con-tributed.
For each readability measure that is con-tributed, the name of the measure, document score,date added, as well as relevant references to thesystem used to calculate it will be included.Multiple simplified versions of each originaldocument can be contributed.
The simplificationfor each sentence in the original document will beon the same line in the simplified document as thecorresponding sentence in the original document.Each simplified version will include a brief de-scription of how it was simplified and relevant ref-erences to the simplification method.
As with theoriginal documents, the date added, optional com-ments and the same document statistics and read-87Domain # Doc-umentsAvg.
#SentencesAvg.
#WordsAvg.
#words/sentTotal #SentencesTotal #WordsAvg.
F-KGrade LevelAvg.
Read-ability (L)Avg.
Read-ability (G)Driver?sLicensing60 1927.6 30,352.6 16.1 115,657 1,821,155 9.54 9.6 7.9Vehicles 3 46.7 1,118.3 22.5 140 3355 13.3 8.2 7.9GovernmentDocuments11 150 2,242.8 16.4 1650 24,671 10.5 8.6 8.4Utilities 5 412.8 8,447.2 21.5 2,064 42,236 13.4 9.8 8.9Banking 3 158 2,900 17.6 474 8,700 11.4 10.5 8.9Leasing 4 101 2,386.8 23.8 404 9,547 13.7 9.0 8.7GovernmentAid10 317.4 5,197.5 17.4 3,174 51,975 10.7 9.2 8.8Shopping 3 281 5,266.7 19.7 843 15,800 12.2 9.9 9.0Other 2 102.5 1,634 16.0 205 3268 9.7 8.8 8.2All 101 1,233.8 19,611.0 17.2 124,611 1,980,707 10.4 9.4 8.2Table 3: Corpus statistics for the basic corpus documentsability metrics will be included.
Additional read-ability metrics can also be contributed and docu-mented.5.2 Generating Simplifications UsingCrowdsourcingWe conducted a preliminary study to determinethe feasibility of collecting simplifications usingcrowdsourcing.
We used AMT as the crowdsourc-ing platform to collect sentence-level simplifica-tion annotations for sentences randomly selectedfrom the basic corpus of everyday documents.5.2.1 AMT Task DetailsWe collected 10 simplification annotations foreach of the 200 sentences which we posted intwo sets of Human Intelligence Tasks (HITs) toAMT.
Each HIT included up to four sentences andincluded an optional comment box that allowedworkers to submit comments or suggestions aboutthe HIT.
Workers were paid $0.25 for each HIT,and 11 workers were given a $0.05 bonus for sub-mitting comments which helped improve the taskdesign and remove design errors in the first itera-tion of the HIT design.
The first set of HITs wascompleted in 20.5 hours and the second set in only6.25 hours.
The total cost for all 2000 simplifica-tion annotations was $163.51 for 592 HITs, eachwith up to four simplifications.
The breakdown ofthis cost is shown in Table 4.Item Cost592 HITs $148.0011 bonuses $0.55AMT fees $14.96Total $163.51Table 4: Breakdown of AMT costs5.2.2 Quality Control MeasuresTo ensure quality, we provided a training sessionwhich shows workers explanations, examples, andcounter-examples of multiple simplification tech-niques.
These include lexical simplification, re-ordering, sentence splitting, removing unneces-sary information, adding additional explanations,and making no change for sentences that are al-ready simple enough.
One of the training examplesis the following:Original Sentence: ?Do not use only parking lights, day ornight, when vehicle is in motion.
?Simplification: ?When your vehicle is moving do not useonly the parking lights.
This applies both at night and dur-ing the day.
?The explanations demonstrated how lexical sim-plification, sentence splitting, and reordering tech-niques were used.The training session also tested workers?
abili-ties to apply these techniques.
Workers were givenfour test sentences to simplify.
Test 1 required lex-ical simplification.
Test 2 was a counter-exampleof a sentence which did not require simplifica-tion.
Test 3 required sentence splitting.
Test 4 re-quired either moving or deleting an unclear modi-fying clause.
We chose the test sentences directlyfrom the corpus and modified them where neces-sary to ensure that they contained the features be-ing tested.
Workers could take the training sessionand submit answers as many times as they wanted,but could not work on a task without first success-fully completing the entire session.
After complet-ing the training session once, workers could com-plete as many HITs as were available to them.In addition to the training session, we blockedsubmissions with empty or garbage answers (de-fined as those with more than 15% of the words88not in a dictionary).
We also blocked copy-pastefunctions to discourage worker laziness.
Workerswho submitted multiple answers that were eithervery close to or very far from the original sentencewere flagged and their submissions were manuallyreviewed to determine whether to approve them.Similarity was measured using the ratio of Leven-shtein distance to alignment length; Levenshteindistance is a common, simple metric for measur-ing the edit distance between two strings.
TheLevenshtein ratio(1?Levenshtein dist.alignment length)providesa normalised similarity measure which is robustto length differences in the inputs.
We also askedworkers to rate their confidence in each simplifi-cation they submitted on a five point scale rangingfrom ?Not at all?
to ?Very confident?.5.2.3 Effectiveness of Quality ControlMeasuresTo determine the quality of the AMT simplifica-tions, we examine the effectiveness of the qual-ity control measures described in the previous sec-tion.Training: In addition to providing training andsimplification experience to workers who workedon the task, the training session effectively blockedworkers who were not able to complete it andspammers.
Of the 358 workers who looked at thetraining session only 184 completed it (51%) andwe found that no bots or spammers had completedthe training session.
Tables 5 and 6 show the per-formance on the four tests in the training sessionfor workers who completed the training sessionand for those who did not, respectively.# of workers 181Avg.
# Attempts Test 1 1.1Avg.
# Attempts Test 2 1.5Avg.
# Attempts Test 3 1.6Avg.
# Attempts Test 4 1.4Table 5: Training statistics for workers who com-pleted training# of workers 174# Completed Test 1 82# Completed Test 2 47# Completed Test 3 1Table 6: Training statistics for workers who did notcomplete trainingBlocking empty and garbage submissions:Empty simplifications and cut-paste functionswere blocked using client-side scripts and we didnot collect statistics of how many workers at-tempted either of these actions.
One worker sub-mitted a comment requesting that we do not blockcopy-paste functions.
In total only 0.6% of sub-missions were detected as garbage and blocked.Manual reviews: We (the first author) reviewedworkers who were automatically flagged five ormore times.
We found that this was not an effectiveway to detect work to be rejected since there weremany false positives and workers who did moreHITs were more likely to get flagged.
None of theworkers flagged for review had submitted simpli-fications that were rejected.5.2.4 Evaluating Simplification QualityTo determine whether it is feasible to use crowd-sourced simplifications to simplify documents forthe extended corpus, we examine the quality ofthe simplifications submitted.
The quality controlmeasures described in the previous sections aredesigned to ensure that workers know what ismeant by simplification and how to apply somesimplification techniques, to block spammers, andto limit worker laziness.
However, workers werefree to simplify sentences creatively and encour-aged to use their judgement in applying any tech-niques that seem best to them.It is difficult to verify the quality of the simplifi-cation annotations that were submitted or to deter-mine how to decide what simplification to choseas the ?correct?
one for the corpus.
For any givensentence there is no ?right?
answer for what thesimplification should be; there are many differentpossible simplifications, each of which could bevalid.
For example, below is an original sentencetaken from a driving manual with two of the sim-plifications that were submitted for it.Original Sentence: ?Vehicles in any lane, except the rightlane used for slower traffic, should be prepared to moveto another lane to allow faster traffic to pass.
?Simplification 1: ?Vehicles that are not in the right laneshould be prepared to move to another lane in order toallow faster traffic to pass.
?Simplification 2: ?Vehicles not in the right lane should beready to move to another lane so faster traffic can passthem.
The right lane is for slower traffic.
?There are a number of heuristics that couldbe used to detect which simplifications are mostlikely to be the best choice to use in the corpus.The average time for workers to complete oneHIT of up to four simplifications was 3.85 min-89utes.
This includes the time to complete the train-ing session during a worker?s first HIT; excludingthis, we estimate the average time per HIT is ap-proximately 2.75 minutes.
Simplifications whichare completed in significantly less time, especiallywhen the original sentence is long, can be flaggedfor review or simply thrown out if there are enoughother simplifications for the sentence.Workers?
confidence in their simplifications canalso be used to exclude simplifications which weresubmitted with low confidence (using worker con-fidence as a quality control filter was explored byParent and Eskenazi (2010)).
Table 7 shows thestatistics for the worker-submitted confidences.Again, simplifications with very low confidenceConfidence Level # of answers1 (Not at all) 92 (Somewhat confident) 1433 (Neutral) 2514 (Confident) 10305 (Very confident) 567Table 7: Self-assessed worker confidences in theirsimplificationscan either be reviewed or thrown out if there areenough other simplifications for the sentence.Worker agreement can also be used to detectsimplifications that are very different from thosesubmitted by other workers.
Using the similarityratio of Levenshtein distance to alignment length,we calculated which simplifications had at mostone other simplification with which they have asimilarity ratio above a specific threshold (here re-ferred to as ?outliers?).
Table 8 reports how manysimplifications are outliers while varying the sim-ilarity threshold.
Since there are many differentThreshold 90% 85% 75% 65% 50%# Outliers 1251 927 500 174 12Table 8: Number of outlier simplifications withsimilarity ratio above the threshold for at most oneother simplificationvalid simplifications possible for any given sen-tence this is not necessarily the best way to de-tect poor quality submissions.
For example, oneof the outliers, using the 50% threshold, was asimplification of the sentence ?When following atractor-trailer, observe its turn signals before try-ing to pass?
which simplified by using a negative- ?Don?t try to pass ... without ...?.
This outlierwas the only simplification of this sentence whichused the negative but it is not necessarily a poorone.
However, the results in Table 7 do show thatthere are many simplifications which are similar toeach other, indicating that multiple workers agreeon one simplification.
One of these similar sim-plifications could be used in the corpus, or multi-ple different possible simplifications could be in-cluded.To further verify that usable simplifications canbe generated using AMT the first author manu-ally reviewed the 1000 simplifications of 100 sen-tences submitted for the first set of HITs.
Wejudged whether each simplification was grammat-ical and whether it was a valid simplification.
Thisis a qualitative judgement, but simplifications werejudged to be invalid simplifications if they had sig-nificant missing or added information comparedto the original sentence or added significant ex-tra grammatical or lexical complexity for no ap-parent reason.
The remaining grammatical, validsimplifications were judged as more simple, neu-tral, or less simple than the original for each of thefollowing features: length, vocabulary, and gram-matical structure.
The results of this review areshown in Table 9.
These results show that approx-imately 15% of the simplifications were ungram-matical or invalid, further motivating the need touse the other features, such as worker agreementand confidence, to automatically remove poor sim-plifications.5.2.5 Extending the Corpus UsingCrowdsourcingThe preliminary work undertaken demonstratesthat it is feasible to quickly collect multiple sim-plifications for each sentence relatively inexpen-sively.
We have also presented an evaluation ofthe quality of the crowdsourced simplificationsand several methods of determining which sim-plifications could be used in the extended corpus.More work is still needed to determine the mostcost effective way of getting simplification resultsthat are of sufficient quality to use without gather-ing overly redundant simplifications for each sen-tence.
Additionally, simplifications of more sen-tences are needed to assess improvements in read-ing level since the reading level measures we useare not accurate for very short input texts.90Un-grammaticalInvalid(excludesungrammatical)SimplervocabularyLesssimplevocabularyEquivalentvocabularyGrammaticallysimplerLessgrammaticallysimpleEquivalentgrammarLonger Shorter Samelength35 122 383 21 596 455 21 524 99 537 364Table 9: Manual evaluation of 1000 AMT simplifications.
Numbers of simplifications with each feature.6 Contributing to & Accessing theCorpus6.1 Contributing to the Extended CorpusThe following items can be contributed to the cor-pus: original everyday copyright-free documents,manual or automated simplifications of the orig-inal documents (or parts of the documents), andreadability scores for original or simplified docu-ments.Original documents submitted to the corpus canbe from any domain.
Our working definition of aneveryday document is any document which peo-ple may have a need to access in their everydaylife.
Examples include government and licensingforms and their instructions, banking forms, pre-scription instructions, mandatory educational test-ing, leasing and rental agreements, loyalty pro-gram sign-up forms and other similar documents.We excluded Wikipedia pages because we foundthat many article pairs actually had few parallelsentences.
Documents should be in English and ofNorth American origin to avoid dialect-specific is-sues.Hand generated or automatically generated sim-plifications of everyday documents are also wel-come.
They should be accompanied the informa-tion detailed in Section 5.1.
The document statis-tics listed in Sections 4 and 5 will be added foreach simplified document.Readability scores can be contributed for any ofthe documents.They should also include the infor-mation detailed in Section 5.1 and pertinent infor-mation about the system that generated the scores.6.2 Accessing the Extended CorpusThe extended corpus will be made publicly acces-sible at the same location as the basic corpus.
Thenames and statistics of each of the documents willbe tabulated and both the original and simplifieddocuments, and their statistics, will be available todownload.
Users will submit their name or organi-zational affiliation along with a very brief descrip-tion of how they plan to use the data.
This willallow us to keep track of how the corpus is be-ing used and how it could be made more useful tothose researching simplification.The goal of this corpus is to make its contents asaccessible as possible.
However, many of the orig-inal documents from non-governmental sourcesmay not be freely distributed and will instead beincluded under a data license, unlike the remain-der of the corpus and the simplifications1.7 Conclusions & Future WorkIn this paper we have given the motivation for cre-ating a large and publicly accessible corpus of ev-eryday documents and their simplifications.
Thiscorpus will advance research into automated sim-plification and evaluation for everyday documents.We have already collected a basic corpus of every-day documents and demonstrated the feasibility ofcollecting large numbers of simplifications usingcrowdsourcing.
We have defined what informationthe extended corpus will contain and how contri-butions can be made to it.There is significantly more work which must becompleted in the future to create an extended cor-pus which meets the needs described in this paper.There are three tasks that we plan to undertake inorder to complete this corpus: we will collect sig-nificantly more everyday documents; we will man-age a large crowdsourcing task to generate simpli-fications for thousands of the sentences in thesedocuments; and we will create a website to enableaccess and contribution to the extended simplifi-cation corpus.
By making this work accessible wehope to motivate others to contribute to the corpusand to use it to advance automated text simplifica-tion and evaluation techniques for the domain ofeveryday documents.AcknowledgmentsThe authors would like to thank the anonymousreviewers for their detailed and helpful feedbackand comments on the paper.1Thanks to Professor Jamie Callan for explaining someof the issues with including these types of documents in ourdataset.91ReferencesSandra Aluisio and Caroline Gasperin.
2010.
Foster-ing digital inclusion and accessibility: The porsim-ples project for simplification of portuguese texts.In Proc.
of the NAACL HLT 2010 Young Investi-gators Workshop on Computational Approaches toLanguages of the Americas, pages 46?53.
Associa-tion for Computational Linguistics.Vamshi Ambati and Stephan Vogel.
2010.
Can crowdsbuild parallel corpora for machine translation sys-tems?
In Proc.
of the NAACL HLT 2010 Workshopon Creating Speech and Language Data with Ama-zon?s Mechanical Turk, pages 62?65.
Associationfor Computational Linguistics.Regina Barzilay and Noemie Elhadad.
2003.
Sentencealignment for monolingual comparable corpora.
InProc.
of the 2003 Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?03,pages 25?32.
Association for Computational Lin-guistics.Stefan Bott, Horacio Saggion, and David Figueroa.2012.
A hybrid system for spanish text simplifi-cation.
In Proc.
of the Third Workshop on Speechand Language Processing for Assistive Technolo-gies, pages 75?84.
Association for ComputationalLinguistics.R.
Chandrasekar, Christine Doran, and B. Srinivas.1996.
Motivations and methods for text simplifica-tion.
In Proc.
of the 16th Conference on Compu-tational Linguistics - Volume 2, COLING ?96, pages1041?1044.
Association for Computational Linguis-tics.Orph?ee De Clercq, Veronique Hoste, Bart Desmet,Philip van Oosten, Martine De Cock, and LieveMacken.
2014.
Using the crowd for readabil-ity prediction.
Natural Language Engineering,FirstView:1?33.William H. DuBay.
2004.
The Princi-ples of Readability.
Costa Mesa, CA:Impact Information, http://www.impact-information.com/impactinfo/readability02.pdf.Dan Feblowitz and David Kauchak.
2013.
Sentencesimplification as tree transduction.
In Proc.
of theSecond Workshop on Predicting and Improving TextReadability for Target Reader Populations, pages 1?10.
Association for Computational Linguistics.Lijun Feng, Martin Jansche, Matt Huenerfauth, andNo?emie Elhadad.
2010.
A comparison of fea-tures for automatic readability assessment.
In Col-ing 2010: Posters, pages 276?284.
Coling 2010 Or-ganizing Committee.Thomas Franc?ois and Eleni Miltsakaki.
2012.
Do nlpand machine learning improve traditional readabilityformulas?
In Proc.
of the First Workshop on Predict-ing and Improving Text Readability for target readerpopulations, pages 49?57.
Association for Compu-tational Linguistics.Michael Heilman, Kevyn Collins-Thompson, JamieCallan, and Maxine Eskenazi.
2007.
Combin-ing lexical and grammatical features to improvereadability measures for first and second languagetexts.
In HLT-NAACL 2007: Main Proceedings,pages 460?467.
Association for Computational Lin-guistics.J.
Peter Kincaid, Robert P. Fishburne Jr., Richard L.Rogers, and Brad S. Chissom.
1975.
Derivationof new readability formulas (automated readabilityindex, fog count and flesch reading ease formula)for navy enlisted personnel.
Technical report, NavalTechnical Training Command, Millington Tn.David Klaper, Sarah Ebling, and Martin Volk.
2013.Building a german/simple german parallel corpusfor automatic text simplification.
In Proc.
of theSecond Workshop on Predicting and Improving TextReadability for Target Reader Populations, pages11?19.
Association for Computational Linguistics.Sigrid Klerke and Anders S?gaard.
2012.
Dsim, a dan-ish parallel corpus for text simplification.
In Proc.of the Eighth Language Resources and EvaluationConference (LREC 2012), pages 4015?4018.
Euro-pean Language Resources Association (ELRA).Gabriel Parent and Maxine Eskenazi.
2010.
Towardbetter crowdsourced transcription: Transcription ofa year of the let?s go bus information system data.In SLT, pages 312?317.
IEEE.Sarah E Petersen and Mari Ostendorf.
2007.
Text sim-plification for language learners: a corpus analysis.In Proc.
of Workshop on Speech and Language Tech-nology for Education, pages 69?72.Matt Post, Chris Callison-Burch, and Miles Osborne.2012.
Constructing parallel corpora for six indianlanguages via crowdsourcing.
In Proc.
of the Sev-enth Workshop on Statistical Machine Translation,pages 401?409.
Association for Computational Lin-guistics.Advaith Siddharthan and Napoleon Katsos.
2012.
Of-fline sentence processing measures for testing read-ability with users.
In Proc.
of the First Workshopon Predicting and Improving Text Readability fortarget reader populations, pages 17?24.
Associationfor Computational Linguistics.Advaith Siddharthan.
2006.
Syntactic simplificationand text cohesion.
Research on Language and Com-putation, 4(1):77?109.Irina Temnikova and Galina Maneva.
2013.
The c-score ?
proposing a reading comprehension metricsas a common evaluation measure for text simplifica-tion.
In Proc.
of the Second Workshop on Predictingand Improving Text Readability for Target ReaderPopulations, pages 20?29.
Association for Compu-tational Linguistics.92Sowmya Vajjala and Detmar Meurers.
2012.
On im-proving the accuracy of readability classification us-ing insights from second language acquisition.
InProc.
of the Seventh Workshop on Building Educa-tional Applications Using NLP, pages 163?173.
As-sociation for Computational Linguistics.Sanja?Stajner, Richard Evans, Constantin Orasan, , andRuslan Mitkov.
2012.
What can readability mea-sures really tell us about text complexity?
In Proc.of the Workshop on Natural Language Processingfor Improving Textual Accessibility (NLP4ITA).Kristian Woodsend and Mirella Lapata.
2011.
Wik-isimple: Automatic simplification of wikipedia arti-cles.
In Proc.
of the Twenty-Fifth AAAI Conferenceon Artificial Intelligence, pages 927?932.Omar F. Zaidan and Chris Callison-Burch.
2011.Crowdsourcing translation: Professional qualityfrom non-professionals.
In Proc.
of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies, pages1220?1229.
Association for Computational Linguis-tics.Zhemin Zhu, Delphine Bernhard, and Iryna Gurevych.2010.
A monolingual tree-based translation modelfor sentence simplification.
In Proc.
of the 23rd In-ternational Conference on Computational Linguis-tics (Coling 2010), pages 1353?1361.93
