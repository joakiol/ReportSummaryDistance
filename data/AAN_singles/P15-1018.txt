Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 177?187,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsStacked Ensembles of Information Extractorsfor Knowledge-Base PopulationNazneen Fatema Rajani?Vidhoon Viswanathan?Yinon Bentor Raymond J. MooneyDepartment of Computer ScienceUniversity of Texas at AustinAustin, TX 78712, USA{nrajani,vidhoon,yinon,mooney}@cs.utexas.eduAbstractWe present results on using stacking to en-semble multiple systems for the Knowl-edge Base Population English Slot Fill-ing (KBP-ESF) task.
In addition to us-ing the output and confidence of each sys-tem as input to the stacked classifier, wealso use features capturing how well thesystems agree about the provenance ofthe information they extract.
We demon-strate that our stacking approach outper-forms the best system from the 2014 KBP-ESF competition as well as alternative en-sembling methods employed in the 2014KBP Slot Filler Validation task and severalother ensembling baselines.
Additionally,we demonstrate that including provenanceinformation further increases the perfor-mance of stacking.1 IntroductionUsing ensembles of multiple systems is a stan-dard approach to improving accuracy in machinelearning (Dietterich, 2000).
Ensembles have beenapplied to a wide variety of problems in naturallanguage processing, including parsing (Hender-son and Brill, 1999), word sense disambiguation(Pedersen, 2000), and sentiment analysis (White-head and Yaeger, 2010).
This paper presents a de-tailed study of ensembling methods for the TACKnowledge Base Population (KBP) English SlotFilling (ESF) task (Surdeanu, 2013; Surdeanu andJi, 2014).We demonstrate new state-of-the-art results onthis KBP task using stacking (Wolpert, 1992),which trains a final classifier to optimally com-bine the results of multiple systems.
We presentresults for stacking all systems that competed inboth the 2013 and 2014 KBP-ESF tracks, training?These authors contributed equallyon 2013 data and testing on 2014 data.
The re-sulting stacked ensemble outperforms all systemsin the 2014 competition, obtaining an F1 of 48.6%compared to 39.5% for the best performing systemin the most recent competition.Although the associated KBP Slot Filler Val-idation (SFV) Track (Wang et al, 2013; Yu etal., 2014; Sammons et al, 2014) is officially fo-cused on improving the precision of individual ex-isting systems by filtering their results, frequentlyparticipants in this track also combine the resultsof multiple systems and also report increased re-call through this use of ensembling.
However,SFV participants have not employed stacking, andwe demonstrate that our stacking approach out-performs existing published SFV ensembling sys-tems.KBP ESF systems must also provide prove-nance information, i.e.
each extracted slot-fillermust include a pointer to a document passage thatsupports it (Surdeanu and Ji, 2014).
Some SFVsystems have used this provenance information tohelp filter and combine extractions (Sammons etal., 2014).
Therefore, we also explored enhancingour stacking approach by including additional in-put features that capture provenance information.By including features that quantify how much theensembled systems agree on provenance, we fur-ther improved our F1 score for the 2014 ESF taskto 50.1%.The remainder of the paper is organized as fol-lows.
Section 2 provides background informationon existing KBP-ESF systems and stacking.
Sec-tion 3 provides general background on the KBP-ESF task.
Section 4 describes our stacking ap-proach, including how provenance information isused.
Section 5 presents comprehensive exper-iments comparing this approach to existing re-sults and several additional baselines, demonstrat-ing new state-of-the-art results on KBP-ESF.
Sec-tion 6 reviews prior related work on ensembling177for information extraction.
Section 7 presents ourfinal conclusions and proposed directions for fu-ture research.2 BackgroundFor the past few years, NIST has been conductingthe English Slot Filling (ESF) Task in the Knowl-edge Base Population (KBP) track among variousother tasks as a part of the Text Analysis Con-ference(TAC)(Surdeanu, 2013; Surdeanu and Ji,2014).
In the ESF task, the goal is to fill spe-cific slots of information for a given set of queryentities (people or organizations) based on a sup-plied text corpus.
The participating systems em-ploy a variety of techniques in different stagesof the slot filling pipeline, such as entity search,relevant document extraction, relation modelingand inference.
In 2014, the top performing sys-tem, DeepDive with Expert Advice from StanfordUniversity (Wazalwar et al, 2014), employed dis-tant supervision (Mintz et al, 2009) and MarkovLogic Networks (Domingos et al, 2008) in theirlearning and inferencing system.
Another system,RPI BLENDER (Hong et al, 2014), used a re-stricted fuzzy matching technique in a frameworkthat learned event triggers and employed them toextract relations from documents.Given the diverse set of slot-filling systemsavailable, it is interesting to explore methods forensembling these systems.
In this regard, TACalso conducts a Slot Filler Validation (SFV) taskwho goal is to improve the slot-filling performanceusing the output of existing systems.
The inputfor this task is the set of outputs from all slot-filling systems and the expected output is a filteredset of slot fills.
As with the ESF task, partici-pating systems employ a variety of techniques toperform validation.
For instance, RPI BLENDERused a Multi-dimensional Truth Finding model(Yu et al, 2014) which is an unsupervised vali-dation approach based on computing multidimen-sional credibility scores.
The UI CCG system(Sammons et al, 2014) developed two differentvalidation systems using entailment and majorityvoting.However, stacking (Sigletos et al, 2005;Wolpert, 1992) has not previously been employedfor ensembling KBP-ESF systems.
In stacking, ameta-classifier is learned from the output of multi-ple underlying systems.
In our work, we translatethis to the context of ensembling slot filling sys-tems and build a stacked meta-classifier that learnsto combine the results from individual slot fillingsystems.
We detail our stacking approach for en-sembling existing slot filling systems in Section 4.3 Overview of KBP Slot Filling TaskThe goal of the TAC KBP-ESF task (Surdeanu,2013; Surdeanu and Ji, 2014) is to collect infor-mation (fills) about specific attributes (slots) for aset of entities (queries) from a given corpus.
Thequeries vary in each year of the task and can beeither a person (PER) or an organization (ORG)entity.
The slots are fixed and are listed in Ta-ble 1 by entity type.
Some slots (like per:age) aresingle-valued while others (like per:children) arelist-valued i.e., they can take multiple slot fillers.3.1 Input and OutputThe input for the task is a set of queries and thecorpus in which to look for information.
Thequeries are provided in an XML format containingbasic information including an ID for the query,the name of the entity, and the type of entity (PERor ORG).
The corpus consists of documents for-mat from discussion forums, newswire and the In-ternet.
Each document is identified by a uniquedocument ID.The output for the task is a set of slot fills foreach input query.
Depending on the type, eachquery should have a NIL or one or more lines ofoutput for each of the corresponding slots.
Theoutput line for each slot fill contains the fieldsshown in Table 2.
The query ID in Column 1should match the ID of the query given as input.The slot name (Column 2) is one of the slots listedin Table 1 based on entity type.
Run ID (Column3) is a unique identifier for each system.
Column4 contains a NIL filler if the system could not findany relevant slot filler.
Otherwise, it contains therelation provenance.
Provenance is of the formdocid:startoffset-endoffset, where docid specifiesa source document from the corpus and the offsetsdemarcate the text in this document supporting therelation.
The offsets correspond to the spans ofthe candidate document that describe the relationbetween the query entity and the extracted slotfiller.
Column 5 contains the extracted slot filler.Column 6 is a filler provenance that is similar informat to relation provenance but in this case theoffset corresponds to the portion of the documentcontaining the extracted filler.
Column 7 is a confi-178Person Organizationper:alternate names per:cause of death org:country of headquarters org:founded byper:date of birth per:countries of residence org:stateorprovince of headquarters org:date dissolvedper:age per:statesorprovinces of residence org:city of headquarters org:websiteper:parents per:cities of residence org:shareholders org:date foundedper:spouse per:schools attended org:top members employees org:membersper:city of birth per:city of death org:political religious affiliation org:member ofper:origin per:stateorprovince of death org:number of employees members org:subsidiariesper:other family per:country of death org:alternate names org:parentsper:title per:employee or member ofper:religion per:stateorprovince of birthper:children per:country of birthper:siblings per:date of deathper:chargesTable 1: Slots for PER and ORG queriesdence score which systems can provide to indicatetheir certainty in the extracted information.3.2 ScoringThe scoring for the ESF task is carried out as fol-lows.
The responses from all slot-filling systemsare pooled and a key file is generated by havinghuman assessors judge the correctness of these re-sponses.
In addition, LDC includes a manual keyof fillers that were determined by human judges.Using the union of these keys as the gold standard,precision, recall, and F1 scores are computed.Column Field DescriptionColumn 1 Query IDColumn 2 Slot nameColumn 3 Run IDColumn 4 NIL or Relation ProvenanceColumn 5 Slot fillerColumn 6 Filler ProvenanceColumn 7 Confidence scoreTable 2: SF Output line fields4 Ensembling Slot-Filling SystemsGiven a set of query entities and a fixed set of slots,the goal of ensembling is to effectively combinethe output of different slot-filling systems.
The in-put to the ensembling system is the output of in-dividual systems (in the format described in previ-ous section) containing slot fillers and additionalinformation such as provenance and confidencescores.
The output of the ensembling system issimilar to the output of an individual system, butit productively aggregates the slot fillers from dif-ferent systems.4.1 AlgorithmThis section describes our ensembling approachwhich trains a final binary classifier using featuresthat help judge the reliability and thus correctnessof individual slot fills.
In a final post-processingstep, the slot fills that get classified as ?correct?
bythe classifier are kept while the others are set toNIL.4.1.1 StackingStacking is a popular ensembling method in ma-chine learning (Wolpert, 1992) and has been suc-cessfully used in many applications including thetop performing systems in the Netflix competition(Sill et al, 2009).
The idea is to employ multiplelearners and combine their predictions by traininga ?meta-classifier?
to weight and combine multi-ple models using their confidence scores as fea-tures.
By training on a set of supervised data thatis disjoint from that used to train the individualmodels, it learns how to combine their results intoan improved ensemble model.
We employ a singleclassifier to train and test on all slot types using anL1-regularized SVM with a linear kernel (Fan etal., 2008).4.1.2 Using ProvenanceAs discussed above, each system provides prove-nance information for every non-NIL slot filler.There are two kinds of provenance provided: therelation provenance and the filler provenance.
Inour algorithm, we only use the filler provenancefor a given slot fill.
This is because of the changesin the output formats for the ESF task from 2013 to2014.
Specifically, the 2013 specification requiresseparate entity and justification provenance fields,but the 2014 collapses these into a single relationprovenance field.
An additional filler provenance179field is common to both specifications.
Hence,we use the filler provenance that is common be-tween 2013 and 2014 formats.
As described ear-lier, every provenance has a docid and startoffset-endoffset that gives information about the docu-ment and offset in the document from where theslot fill has been extracted.
The UI-CCG SFV sys-tem Sammons et al (2014) effectively used thisprovenance information to help validate and filterslot fillers.
This motivated us to use provenancein our stacking approach as additional features asinput to the meta-classifier.We use provenance in two ways, first usingthe docid information, and second using the off-set information.
We use the docids to define adocument-based provenance score in the follow-ing way: for a given query and slot, if N sys-tems provide answers and a maximum of n ofthose systems give the same docid in their fillerprovenance, then the document provenance scorefor those n slot fills is n/N .
Similarly, other slotfills are given lower scores based on the fraction ofsystems whose provenance document agree withtheirs.
Since this provenance score is weightedby the number of systems that refer to the sameprovenance, it measures the reliability of a slotfill based on the document from where it was ex-tracted.Our second provenance measure uses offsets.The degree of overlap among the various systems?offsets can also be a good indicator of the reliabil-ity of the slot fill.
The Jaccard similarity coeffi-cient is a statistical measure of similarity betweensets and is thus useful in measuring the degree ofoverlap among the offsets of systems.
Slot fillshave variable lengths and thus the provenance off-set ranges are variable too.
A metric such as theJaccard coefficient captures the overlapping off-sets along with normalizing based on the unionand thus resolving the problem with variable offsetranges.
For a given query and slot, if N systemsthat attempt to fill it have the same docid in theirdocument provenance, then the offset provenance(OP) score for a slot fill by a system x is calculatedas follows:OP (x) =1|N |?
?i?N,i6=x|offsets(i) ?
offsets(x)||offsets(i) ?
offsets(x)|Per our definition, systems that extract slot fillsfrom different documents for the same query slothave zero overlap among offsets.
We note that theoffset provenance is always used along with thedocument provenance and thus useful in discrim-inating slot fills extracted from a different docu-ment for the same query slot.
Like the documentprovenance score, the offset provenance score isalso a weighted feature and is a measure of relia-bility of a slot fill based on the offsets in the docu-ment from where it is extracted.
Unlike past SFVsystems that use provenance for validation, our ap-proach does not need access to the large corpus ofdocuments from where the slot fills are extractedand is thus very computationally inexpensive.4.2 Eliminating Slot-Filler AliasesWhen combining the output of different ESF sys-tems, it is possible that some slot-filler entitiesmight overlap with each other.
An ESF systemcould extract a filler F1for a slot S while anotherESF system extracts another filler F2for the sameslot S. If the extracted fillers F1and F2are aliases(i.e.
different names for the same entity), the scor-ing system for the TAC KBP SF task considersthem redundant and penalizes the precision of thesystem.In order to eliminate aliases from the output ofensembled system, we employ a technique derivedby inverting the scheme used by the LSV ESF sys-tem (Roth et al, 2013) for query expansion.
LSVESF uses a Wikipedia anchor-text model (Rothand Klakow, 2010) to generate aliases for givenquery entities.
By including aliases for querynames, the ESF system increase the number ofcandidate sentences fetched for the query.To eliminate filler aliases, we apply the sametechnique to generate aliases for all slot fillers ofa given query and slot type.
Given a slot filler,we obtain the Wikipedia page that is most likelylinked to the filler text.
Then, we obtain the anchortexts and their respective counts from all otherWikipedia pages that link to this page.
Using thesecounts, we choose top N (we use N=10 as inLSV) and pick the corresponding anchor texts asaliases for the given slot filler.
Using the gener-ated aliases, we then verify if any of the slot fillersare redundant with respect to these aliases.
Thisscheme is not applicable to slot types whose fillersare not entities (like date or age).
Therefore, sim-pler matching schemes are used to eliminate re-dundancies for these slot types.180Common systems dataset All 2014 SFV systems datasetFigure 1: Precision-Recall curves for identifying the best voting performance on the two datasets5 Experimental EvaluationThis section describes a comprehensive set of ex-periments evaluating ensembling for the KBP ESFtask.
Our experiments are divided into two sub-sets based on the datasets they employ.
Sinceour stacking approach relies on 2013 SFV datafor training, we build a dataset of one run for ev-ery team that participated in both the 2013 and2014 competitions and call it the common systemsdataset.
There are 10 common teams of the 17teams that participated in ESF 2014.
The otherdataset comprises of all 2014 SFV systems (in-cluding all runs of all 17 teams that participated in2014).
There are 10 systems in the common sys-tems dataset, while there are 65 systems in the all2014 SFV dataset.
Table 3 gives a list of the com-mon systems for 2013 and 2014 ESF task.
ESFsystems do change from year to year and it?s not aperfect comparison, but systems generally get bet-ter every year and thus we are probably only un-derperforming.Common SystemsLSVIIRGUMass IESLStanfordBUPT PRISRPI BLENDERCMUMLNYUComprenoUWashingtonTable 3: Common teams for 2013 and 2014 ESF5.1 Methodology and ResultsFor our unsupervised ensembling baselines, weevaluate on both the common systems dataset aswell as the entire 2014 SFV dataset.
We compareour stacking approach to three unsupervised base-lines.
The first is Union which takes the combina-tion of values for all systems to maximize recall.If the slot type is list-valued, it classifies all slotfillers as correct and always includes them.
If theslot type is single-valued, if only one systems at-tempts to answer it, then it includes that system?sslot fill.
Otherwise if multiple systems producea response, it only includes the slot fill with thehighest confidence value as correct and discardsthe rest.The second baseline is Voting.
For this ap-proach, we vary the threshold on the number ofsystems that must agree on a slot fill from oneto all.
This gradually changes the system fromthe union to intersection of the slot fills, and weidentify the threshold that results in the highestF1 score.
We learn a threshold on the 2013 SFVdataset (containing 52 systems) that results in thebest F1 score.
We use this threshold for the votingbaseline on 2014 SFV dataset.
As we did for the2013 common systems dataset, we learn a thresh-old on the 2013 common systems that results in thebest F1 score and use this threshold for the votingbaseline on 2014 common systems.The third baseline is an ?oracle threshold?
ver-sion of Voting.
Since the best threshold for 2013may not necessarily be the best threshold for 2014,we identify the best threshold for 2014 by plot-ting a Precision-Recall curve and finding the bestF1 score for the voting baseline on both the SFVand common systems datasets.
Figure 1 shows the181Figure 2: Our system pipeline for evaluating supervised ensembling approachesBaseline Precision Recall F1Union 0.067 0.762 0.122Voting (threshold learned on 2013 data) 0.641 0.288 0.397Voting (optimal threshold for 2014 data) 0.547 0.376 0.445Table 4: Performance of baselines on all 2014 SFV dataset (65 systems)Approach Precision Recall F1Union 0.176 0.647 0.277Voting (threshold learned on 2013 data) 0.694 0.256 0.374Best ESF system in 2014 (Stanford) 0.585 0.298 0.395Voting (optimal threshold for 2014 data) 0.507 0.383 0.436Stacking 0.606 0.402 0.483Stacking + Relation 0.607 0.406 0.486Stacking + Provenance (document) 0.499 0.486 0.492Stacking + Provenance (document) + Relation 0.653 0.400 0.496Stacking + Provenance (document and offset) + Relation 0.541 0.466 0.501Table 5: Performance on the common systems dataset (10 systems) for various configurations.
Allapproaches except the Stanford system are our implementations.Precision-Recall curve for two datasets for findingthe best possible F1 score using the voting base-line.
We find that for the common systems dataset,a threshold of 3 (of 10) systems gives the best F1score, while for the entire 2014 SFV dataset, athreshold of 10 (of 65) systems gives the highestF1.
Note that this gives an upper bound on thebest results that can be achieved with voting, as-suming an optimal threshold is chosen.
Since theupper bound can not be predicted without usingthe 2014 dataset, this baseline has an unfair ad-vantage.
Table 4 shows the performance of all 3baselines on the all 2014 SFV systems dataset.For all our supervised ensembling approaches,we train on the 2013 SFV data and test on the2014 data for the common systems.
We have5 different supervised approaches.
Our first ap-proach is stacking the common systems usingtheir confidence scores to learn a classifier.
Asdiscussed earlier, in stacking we train a meta-classifier that combines the systems using theirconfidence scores as features.
Since the com-mon systems dataset has 10 systems, this classifieruses 10 features.
The second approach also pro-vides stacking with a nominal feature giving therelation name (as listed in Table 1) for the givenslot instance.
This allows the system to learn dif-ferent evidence-combining functions for differentslot types if the classifier finds this useful.
Forour third approach, we also provide the documentprovenance feature described in Section 4.1.
Al-together this approach has 11 features (10 confi-dence score + 1 document provenance score).
Thefourth approach uses confidences, the documentprovenance feature, and a one-hot encoding of therelation name for the slot instance.
Our final ap-proach also includes the offset provenance (OP)feature discussed in Section 4.1.
There are alto-gether 13 features in this approach.
All our su-pervised approaches use the Weka package (Hallet al, 2009) for training the meta-classifier, usingan L1-regularized SVM with a linear kernel (otherclassifiers gave similar results).
Figure 2 showsour system pipeline for evaluating supervised en-sembling approaches.
Table 5 gives the perfor-mance of all our supervised approaches as well as182our unsupervised baselines for the common sys-tems dataset.Analysis by Surdeanu and Ji (2014) suggeststhat 2014 ESF queries are more difficult than thosefor 2013.
They compare two systems by runningboth on 2013 and 2014 data and find there is a con-siderable drop in the performance of both the sys-tems.
We note that they run the same exact systemon 2013 and 2014 data.
Thus, in order to have abetter understanding of our results, we plot a learn-ing curve by training on different sizes of the 2013SFV data and using the scorer to measure the F1score on the 2014 SFV data for the 10 commonsystems.
Figure 3 shows the learning curve thusobtained.
Although there are certain parts of thedataset when the F1 score drops which we sus-pect is due to overfitting the 2013 data, there isstill a strong correlation between the 2013 trainingdata size and F1 score on the 2014 dataset.
Thuswe can infer that training on 2013 data is usefuleven though the 2013 and 2014 data are fairly dif-ferent.
Although the queries change, the commonsystems remain more-or-less the same and stack-ing enables a meta-classifier to weigh those com-mon systems based on their 2013 performance.Figure 3: Learning curve for training on 2013 andtesting on 2014 common systems datasetTo further validate our approach, we divide the2013 SFV data based on the systems that extractedthose slot fills.
Then we sort the systems, fromhigher to lower, based on the number of false pos-itives produced by them in the ensembling ap-proach.
Next we train a classifier in an incremen-tal fashion adding one system?s slot fills for train-ing at each step and analyzing the performance on2014 data.
This allows us to analyze the resultsat the system level.
Figure 4 shows the plot ofF1 score vs. the number of systems at each step.The figure shows huge improvement in F1 scoreat steps 6 and 7.
At step 6 the Stanford systemis added to the pool of systems which is the bestperforming ESF system in 2014 and fourth bestin 2013.
At step 7, the UMass system is addedto the pool and, although the system on it ownis weak, it boosts the performance of our ensem-bling approach.
This is because the UMass systemalone contributes approximately 24% of the 2013training data (Singh et al, 2013).
Thus addingthis one system significantly improves the trainingstep leading to better performance.
We also no-tice that our system becomes less conservative atthis step and has higher recall.
The reason for thisis that the systems from 1 to 5 had very high pre-cision and low recall whereas from system 6 on-wards the systems have high recall.
Thus addingthe UMass system enables our meta-classifier tohave a higher recall for small decrease in precisionand thus boosting the overall F1 measure.
With-out it, the classifier produces high precision butlow recall and decreases the overall F1 score byapproximately 6 points.Figure 4: Incrementally training on 2013 byadding a system at each step and testing on 2014common systems datasetWe also experimented with cross validationwithin the 2014 dataset.
Since we used only 2014data for this experiment, we also included the rela-tion provenance as discussed in Section 4.1.2.
Ta-ble 6 shows the results on 10-fold cross-validationon 2014 data with only the filler provenance andwith both the filler and relation provenance.
Theperformance of using only the filler provenance isslightly worse than training on 2013 because the2014 SFV data has many fewer instances but usesmore systems for learning compared to the 2013183Approach Precision Recall F1Stacking + Filler provenance + Relation 0.606 0.415 0.493Stacking + Filler and Relation provenance + Relation 0.609 0.434 0.506Table 6: 10-fold Cross-Validation on 2014 SFV dataset (65 systems)Baseline Precision Recall F1Union 0.054 0.877 0.101Voting (threshold learned on 2013 data) 0.637 0.406 0.496Voting (optimal threshold for 2014 data) 0.539 0.526 0.533Table 7: Baseline performance on all 2014 SFV dataset (65 systems) using unofficial scorerApproach Precision Recall F1Union 0.177 0.922 0.296Voting (threshold learned on 2013 data) 0.694 0.256 0.374Best published SFV result in 2014 (UIUC) 0.457 0.507 0.481Voting (optimal threshold for 2014 data) 0.507 0.543 0.525Stacking + Provenance(document) 0.498 0.688 0.578Stacking 0.613 0.562 0.586Stacking + Relation 0.613 0.567 0.589Stacking + Provenance (document and offset) + Relation 0.541 0.661 0.595Stacking + Provenance (document) + Relation 0.659 0.56 0.606Table 8: Performance on the common systems dataset (10 systems) for various configurations using theunofficial scorer.
All approaches except the UIUC system are our implementations.SFV data.The TAC KBP official scoring key for the ESFtask includes human annotated slot fills along withthe pooled slot fills obtained by all participatingsystems.
However, Sammons et al (2014) usean unofficial scoring key in their paper that doesnot include human annotated slot fills.
In orderto compare to their results, we also present resultsusing the same unofficial key.
Table 7 gives theperformance of our baseline systems on the 2014SFV dataset using the unofficial key for scoring.We note that our Union does not produce a recallof 1.0 on the unofficial scorer due to our single-valued slot selection strategy for multiple systems.As discussed earlier for the single-valued slot, weinclude the slot fill with highest confidence (whichmay not necessarily be correct) and thus may notmatch the unofficial scorer.Table 8 gives the performance of all our super-vised approaches along with the baselines on thecommon systems dataset using the unofficial keyfor scoring.
UIUC is one of the two teams par-ticipating in the SFV 2014 task and the only teamto report results, but they report 6 different sys-tem configurations and we show their best perfor-mance.5.2 DiscussionOur results indicate that stacking with provenanceinformation and relation type gives the best perfor-mance using both the official ESF scorer as wellas the unofficial scorer that excludes the human-generated slot fills.
Our stacking approach thatuses the 10 systems common between 2013 and2014 also outperforms the ensembling baselinesthat have the advantage of using all 65 of the 2014systems.
Our stacking approach would presum-ably perform even better if we had access to 2013training data for all 2014 systems.Of course, the best-performing ESF system for2014 did not have access to the pooled slot fillsof all participating systems.
Although poolingthe results has an advantage, naive pooling meth-ods such as the ensembling baselines, in particu-lar the voting approach, do not perform as well asour stacked ensembles.
Our best approach outper-forms the best baseline for both the datasets by atleast 6 F1 points using both the official and unof-184ficial scorer.As expected the Union baseline has the highestrecall.
Among the supervised approaches, stack-ing with document provenance produces the high-est precision and is significantly higher (approx-imately 5%) than the approach that produces thesecond highest precision.
As discussed earlier, wealso scored our approaches on the unofficial scorerso that we can compare our results to the UIUCsystem that was the best performer in the 2014SFV task.
Our best approach beats their best sys-tem configuration by a F1 score of 12 points.
Ourstacking approach also outperforms them on pre-cision and recall by a large margin.6 Related WorkOur system is part of a body of work on increas-ing the performance of relation extraction throughensemble methods.The use of stacked generalization for informa-tion extraction has been demonstrated to outper-form both majority voting and weighted votingmethods (Sigletos et al, 2005).
In relation ex-traction, a stacked classifier effectively combinesa supervised, closed-domain Conditional Ran-dom Field-based relation extractor with an open-domain CRF Open IE system, yielding a 10% in-crease in precision without harming recall (Bankoet al, 2008).
To our knowledge, we are the first toapply stacking to KBP and the first to use prove-nance as a feature in a stacking approach.Many KBP SFV systems cast validation asa single-document problem and apply a vari-ety of techniques, such as rule-based consistencychecks (Angeli et al, 2013), and techniques fromthe well-known Recognizing Textual Entailment(RTE) task (Cheng et al, 2013; Sammons et al,2014).
In contrast, the 2013 JHUAPL system ag-gregates the results of many different extractorsusing a constraint optimization framework, ex-ploiting confidence values reported by each inputsystem (Wang et al, 2013).
A second approach inthe UI CCG system (Sammons et al, 2014) aggre-gates results of multiple systems by using majorityvoting.In the database, web-search, and data-miningcommunities, a line of research into ?truth-finding?
or ?truth-discovery?
methods addressesthe related problem of combining evidence forfacts from multiple sources, each with a latentcredibility (Yin et al, 2008).
The RPI BLENDERKBP system (Yu et al, 2014) casts SFV in thisframework, using a graph propagation method thatmodeled the credibility of systems, sources, andresponse values.
However they only report scoreson the 2013 SFV data which contain less com-plicated and easier queries compared to the 2014data.
Therefore, we cannot directly compare oursystem?s performance to theirs.Google?s Knowledge Vault system (Dong et al,2014) combines the output of four diverse extrac-tion methods by building a boosted decision stumpclassifier (Reyzin and Schapire, 2006).
For eachproposed fact, the classifier considers both theconfidence value of each extractor and the numberof responsive documents found by the extractor.A separate classifier is trained for each predicate,and Platt Scaling (Platt, 1999) is used to calibrateconfidence scores.7 ConclusionThis paper has presented experimental resultsshowing that stacking is a very promising ap-proach to ensembling KBP systems.
From ourliterature survey, we observe that we are the firstto employ stacking and combine it with prove-nance information to ensemble KBP systems.
Ourstacked meta-classifier provides an F1 score of50.1% on 2014 KBP ESF, outperforming the bestESF and SFV systems from the 2014 competition,and thereby achieving a new state-of-the-art forthis task.
We found that provenance features in-creased accuracy, highlighting the importance ofprovenance information (even without accessingthe source corpus) in addition to confidence scoresfor ensembling information extraction systems.8 AcknowledgementsWe thank the anonymous reviewers for their valu-able feedback.
This research was supported bythe DARPA DEFT program under AFRL grantFA8750-13-2-0026.ReferencesGabor Angeli, Arun Chaganty, Angel Chang, KevinReschke, Julie Tibshirani, Jean Y Wu, Osbert Bas-tani, Keith Siilats, and Christopher D Manning.2013.
Stanford?s 2013 KBP system.
In Proceedingsof the Sixth Text Analysis Conference (TAC2013).Michele Banko, Oren Etzioni, and Turing Center.2008.
The tradeoffs between open and traditional185relation extraction.
In ACL08, volume 8, pages 28?36.Xiao Cheng, Bingling Chen, Rajhans Samdani, Kai-Wei Chang, Zhiye Fei, Mark Sammons, John Wi-eting, Subhro Roy, Chizheng Wang, and Dan Roth.2013.
Illinois cognitive computation group UI-CCGTAC 2013 entity linking and slot filler validationsystems.
In Proceedings of the Sixth Text AnalysisConference (TAC2013).T.
Dietterich.
2000.
Ensemble methods in machinelearning.
In J. Kittler and F. Roli, editors, FirstInternational Workshop on Multiple Classifier Sys-tems, Lecture Notes in Computer Science, pages 1?15.
Springer-Verlag.Pedro Domingos, Stanley Kok, Daniel Lowd, HoifungPoon, Matthew Richardson, and Parag Singla.
2008.Markov logic.
In Luc De Raedt, Paolo Frasconi,Kristian Kersting, and Stephen Muggleton, editors,Probabilistic Inductive Logic Programming, volume4911 of Lecture Notes in Computer Science, pages92?117.
Springer.Xin Dong, Evgeniy Gabrilovich, Geremy Heitz, WilkoHorn, Ni Lao, Kevin Murphy, Thomas Strohmann,Shaohua Sun, and Wei Zhang.
2014.
Knowl-edge vault: A web-scale approach to probabilisticknowledge fusion.
In Proceedings of the 20th ACMSIGKDD International Conference on KnowledgeDiscovery and Data Mining, pages 601?610.
ACM.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.2009.
The WEKA data mining software: an update.ACM SIGKDD explorations newsletter, 11(1):10?18.John C. Henderson and Eric Brill.
1999.
Exploitingdiversity in natural language processing: Combin-ing parsers.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processingand Very Large Corpora (EMNLP/VLC-99), pages187?194, College Park, MD.Yu Hong, Xiaobin Wang, Yadong Chen, Jian Wang,Tongtao Zhang, Jin Zheng, Dian Yu, Qi Li, BoliangZhang, Han Wang, et al 2014.
RPI BLENDERTAC-KBP2014 knowledge base population system.Proceedings of the Seventh Text Analysis Conference(TAC2014).Mike Mintz, Steven Bills, Rion Snow, and Dan Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP: Vol-ume 2-Volume 2, pages 1003?1011.
Association forComputational Linguistics.Ted Pedersen.
2000.
A simple approach to building en-sembles of naive Bayesian classifiers for word sensedisambiguation.
In Proceedings of the Meeting ofthe North American Association for ComputationalLinguistics, pages 63?69.John C. Platt.
1999.
Probabilistic outputs for sup-port vector machines and comparisons to regularizedlikelihood methods.
In Peter J. Bartlett, BernhardSch?olkopf, Dale Schuurmans, and Alex J. Smola,editors, Advances in Large Margin Classifiers, pages61?74.
MIT Press, Boston.Lev Reyzin and Robert E Schapire.
2006.
How boost-ing the margin can also boost classifier complexity.In Proceedings of the 23rd International Conferenceon Machine Learning, pages 753?760.
ACM.Benjamin Roth and Dietrich Klakow.
2010.
Cross-language retrieval using link-based language mod-els.
In Proceedings of the 33rd International ACMSIGIR Conference on Research and Development inInformation Retrieval, pages 773?774.
ACM.Benjamin Roth, Tassilo Barth, Michael Wiegand, et al2013.
Effective slot filling based on shallow distantsupervision methods.
Proceedings of the SeventhText Analysis Conference (TAC2013).Mark Sammons, Yangqiu Song, Ruichen Wang,Gourab Kundu, et al 2014.
Overview of UI-CCGsystems for event argument extraction, entity dis-covery and linking, and slot filler validation.
Pro-ceedings of the Seventh Text Analysis Conference(TAC2014).Georgios Sigletos, Georgios Paliouras, Constantine DSpyropoulos, and Michalis Hatzopoulos.
2005.Combining information extraction systems usingvoting and stacked generalization.
The Journal ofMachine Learning Research, 6:1751?1782.Joseph Sill, G?abor Tak?acs, Lester Mackey, and DavidLin.
2009.
Feature-weighted linear stacking.
arXivpreprint arXiv:0911.0460.Sameer Singh, Limin Yao, David Belanger, Ariel Ko-bren, Sam Anzaroot, Michael Wick, Alexandre Pas-sos, Harshal Pandya, Jinho Choi, Brian Martin, andAndrew McCallum.
2013.
Universal schema forslot filling and cold start: UMass IESL.Mihai Surdeanu and Heng Ji.
2014.
Overview of theEnglish slot filling track at the TAC2014 KnowledgeBase Population Evaluation.
In Proceedings of theSeventh Text Analysis Conference (TAC2014).Mihai Surdeanu.
2013.
Overview of the TAC2013knowledge base population evaluation: English slotfilling and temporal slot filling.
In Proceedings ofthe Sixth Text Analysis Conference (TAC 2013).I-Jeng Wang, Edwina Liu, Cash Costello, and ChristinePiatko.
2013.
JHUAPL TAC-KBP2013 slot fillervalidation system.
In Proceedings of the Sixth TextAnalysis Conference (TAC2013).186Anurag Wazalwar, Tushar Khot, Ce Zhang, Chris Re,Jude Shavlik, and Sriraam Natarajan.
2014.
TACKBP 2014 : English slot filling track DeepDive withexpert advice.
In Proceedings of the Seventh TextAnalysis Conference (TAC2014).Matthew Whitehead and Larry Yaeger.
2010.
Senti-ment mining using ensemble classification models.In Tarek Sobh, editor, Innovations and Advances inComputer Sciences and Engineering.
Springer Ver-lag, Berlin.David H. Wolpert.
1992.
Stacked generalization.
Neu-ral Networks, 5:241?259.Xiaoxin Yin, Jiawei Han, and Philip S Yu.
2008.Truth discovery with multiple conflicting informa-tion providers on the web.
Knowledge and Data En-gineering, IEEE Transactions on, 20(6):796?808.Dian Yu, Hongzhao Huang, Taylor Cassidy, Heng Ji,Chi Wang, Shi Zhi, Jiawei Han, Clare Voss, and Ma-lik Magdon-Ismail.
2014.
The wisdom of minority:Unsupervised slot filling validation based on multi-dimensional truth-finding.
In Proc.
The 25th Inter-national Conference on Computational Linguistics(COLING2014).187
