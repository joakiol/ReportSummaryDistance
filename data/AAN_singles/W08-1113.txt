Automated Metrics That Agree With Human JudgementsOn Generated Output for an Embodied Conversational AgentMary Ellen FosterInformatik VI: Robotics and Embedded SystemsTechnische Universita?t Mu?nchenBoltzmannstra?e 3, D-85748 Garching bei Mu?nchen, Germanyfoster@in.tum.deAbstractWhen evaluating a generation system, if a cor-pus of target outputs is available, a commonand simple strategy is to compare the systemoutput against the corpus contents.
However,cross-validation metrics that test whether thesystem makes exactly the same choices as thecorpus on each item have recently been shownnot to correlate well with human judgementsof quality.
An alternative evaluation strategyis to compute intrinsic, task-specific proper-ties of the generated output; this requires moredomain-specific metrics, but can often pro-duce a better assessment of the output.
In thispaper, a range of metrics using both of thesetechniques are used to evaluate three meth-ods for selecting the facial displays of an em-bodied conversational agent, and the predic-tions of the metrics are compared with humanjudgements of the same generated output.
Thecorpus-reproduction metrics show no relation-ship with the human judgements, while theintrinsic metrics that capture the number andvariety of facial displays show a significantcorrelation with the preferences of the humanusers.1 IntroductionEvaluating the output of a generation system isknown to be difficult: since generation is an open-ended task, the criteria for success can be difficultto define (cf.
Mellish and Dale, 1998).
In the cur-rent state of the art, there are two main strategiesfor evaluating the output of a generation system: thebehaviour or preferences of humans in response tothe output may be measured, or automated measuresmay be computed on the output itself.
A study in-volving human judges is the most complete and con-vincing evaluation of generated output.
However,such a study is not always practical, as recruitingsufficient subjects can be time-consuming and ex-pensive.
So automated metrics are also used in ad-dition to?or instead of?human studies.When automatically evaluating generated output,the goal is to find metrics that can easily be com-puted and that can also be shown to correlate withhuman judgements of quality.
Such metrics havebeen introduced in other fields, including PAR-ADISE (Walker et al, 1997) for spoken dialoguesystems, BLEU (Papineni et al, 2002) for ma-chine translation,1 and ROUGE (Lin, 2004) for sum-marisation.
Many automated generation evaluationsmeasure the similarity between the generated outputand a corpus of ?gold-standard?
target outputs, oftenusing measures such as precision and recall.
Suchmeasures of corpus similarity are straightforward tocompute and easy to interpret; however, they are notalways appropriate for generation systems.
One ofthe main advantages of choosing dynamic genera-tion over canned output is its flexibility and its abil-ity to produce a range of different outputs; as pointedout by Paris et al (2007), ?
[e]valuation studies thatignore the potential of the system to generate a rangeof appropriate outputs will be necessarily limited.
?Indeed, several recent studies (Stent et al, 2005;Belz and Reiter, 2006; Foster and White, 2007) haveshown that strict corpus-similarity measures tend tofavour repetitive generation strategies that do not di-verge much, on average, from the corpus data, whilehuman judges often prefer output with more variety.1Although Callison-Burch et al (2006) have recently calledinto question the utility of BLEU.95Automated metrics that take into account otherproperties than strict corpus similarity have alsobeen used to evaluate the output of generation sys-tems.
Walker (2005) describes several evaluationsthat used corpus data in a different way: each of thecorpus examples was associated with some rewardfunction (e.g., subjective user evaluation or task suc-cess), and machine-learning techniques such as re-inforcement learning or boosting were then used totrain the output planner.
Foster and White (2007)found that automated metrics based on factors otherthan corpus similarity (e.g., the amount of variationin the output) agreed better with user preferencesthan did the corpus-similarity scores.
Belz and Gatt(2008) compare the predictions of a range of mea-sures, both intrinsic and extrinsic, that were usedto evaluate the systems in a shared-task referring-expression generation challenge.
One main findingfrom this comparison was that there was no signif-icant correlation between the intrinsic and extrinsic(task success) measures for this task.All of the above studies considered only systemsthat generate text, but many of the same factors alsoapply to the generation of non-verbal behaviours foran embodied conversational agent (ECA) (Cassellet al, 2000).
The behaviour of such an agent is nor-mally based on recorded human behaviour, whichcan provide targets similar to those used in corpus-based evaluations of text-generation systems.
How-ever, just as in text generation, a multimodal systemthat scores well on corpus similarity tends to pro-duce highly repetitive non-verbal behaviours, so itis equally important to gather human judgements toaccompany any automated evaluation.This paper presents three corpus-driven methodsof selecting facial displays for an embodied conver-sational agent and describes two studies comparingthe output of the different methods.
All methods arebased on annotated data drawn from a corpus of hu-man facial displays, and each uses the corpus datain a different way.
The first evaluation study useshuman judges to compare the output of the selectionmethods against one another, while the second studyuses a range of automated metrics: several corpus-reproduction measures, along with metrics based onintrinsic properties of the outputs.
The results of thetwo studies are compared using multiple regression,and the implications are discussed.2 Corpus-based generation of facialdisplays for an ECAThe experiments in this paper make use of the out-put components of the COMIC multimodal dialoguesystem (Foster et al, 2005), which adds a multi-modal talking-head interface to a CAD-style systemfor redesigning bathrooms.
The studies focus on thetask of selecting appropriate ECA head and eyebrowmotions to accompany the turns in which the sys-tem describes and compares the options for tilingthe room, as those are the parts of the output withthe most interesting and varied content.The implementations were based on a corpus ofconversational facial displays derived from the be-haviour of a single speaker reading approximately450 scripted sentences generated by the COMICoutput-generation system.
The OpenCCG syntac-tic derivation trees (White, 2006) for the sentencesform the basis of the corpus.
The leaf nodes inthese trees correspond to the individual words, whilethe internal nodes correspond to multi-word con-stituents.
Every node in each tree was initiallylabelled with all of the applicable contextual fea-tures produced by the output planner: the user-preference evaluation of the tile design being de-scribed (positive/negative/neutral), the informationstatus (given/new) of each piece of information, andthe predicted speech-synthesiser prosody.
The an-notators then linked each facial display producedby the speaker to the node or span of nodes in thederivation tree covering the words temporally asso-ciated with the display.
Full details of this corpus aregiven in Foster (2007a).The most common display used by the speakerwas a downward nod, while the user-preferenceevaluation had the single largest differential effecton the displays used.
When the speaker describedfeatures of the design that the user was expected tolike, he was relatively more likely to turn to the rightand to raise his eyebrows (Figure 1(a)); on featuresthat the user was expected to dislike, on the otherhand, there was a higher probability of left leaning,lowered eyebrows, and narrowed eyes (Figure 1(b)).In a previous study, users were generally able torecognise these ?positive?
and ?negative?
displayswhen they were resynthesised on an embodied con-versational agent (Foster, 2007b).96(a) Positive (b) NegativeFigure 1: Characteristic facial displays from the corpusBased on this corpus, three different strategieswere implemented for selecting facial displays to ac-company the synthesised speech: one strategy us-ing only the three characteristic displays describedabove, along with two data-driven strategies drawingon the full corpus data.
All of the strategies use thesame basic process to select the displays to accom-pany a sentence.
Beginning with the contextually-annotated syntactic tree for the sentence, the systemproceeds depth-first, selecting a face-display combi-nation to accompany each node in turn.
The maindifference among the strategies is the way that eachselects the displays for a node as it is encountered.The rule-based strategy includes displays only onderivation-tree nodes corresponding to specific tile-design properties: that is, manufacturer and seriesnames, colours, and decorative motifs.
The dis-plays for such a node are entirely determined bythe user-preference evaluation of the property be-ing described, and are based on the corpus patternsdescribed above: for every node associated with apositive evaluation, this strategy selects a right turnand brow raise; for a negative node, it selects a leftturn, brow lower, and eye squint; while for all otherdesign-property nodes, it chooses a downward nod.While the rule-based strategy selects displaysonly on nodes describing tile-design features, thetwo data-driven strategies consider all nodes in thesyntactic tree for a sentence as possible sites for a fa-cial display.
To choose the displays for a given node,the system considers the set of displays that occurredon all nodes in the corpus with the same syntactic,semantic, and pragmatic context, and then chooses adisplay from this set in one of two ways.
The ma-jority strategy selects the most common option in allcases, while the weighted strategy makes a stochas-tic choice among all of the options based on the rel-ative frequency.
As a concrete example, consider ahypothetical context in which the speaker made nomotion 80% of the time, a downward nod 15% ofthe time, and a downward nod with a brow raise theother 5% of the time.
For nodes with this context,the majority strategy would always choose no mo-tion, while the weighted strategy would choose nomotion with probability 0.8, a downward nod withprobability 0.15, and a nod with a brow raise withprobability 0.05.Table 1 shows a sample sentence from the corpus,the original facial displays used by the speaker, andthe displays selected by each of the strategies.
In thefigure, nd=d indicates a downward nod, bw=u andbw=d a brow raise and lower, respectively, sq an eyesquint, ln=l a left lean, and tn=r a right turn.
Mostof the displays in these schedules are associated withleaf nodes in the derivation tree, and therefore withsingle words in the output.
However, both the leftlean in the original schedule and the right turn inthe weighted schedule are associated with internalnodes in the tree, and therefore cover more than oneword in the surface string.3 User-preference studiesAs a first comparison of the evaluation strategies,human judges were asked to compare videos basedon the output of each of the generation strategiesto one another and to resynthesised versions of theoriginal displays from the corpus.
This section givesthe details of a study in which the judges chose97Although it?s in the family style, the tiles are by Alessi.Original nd=d nd=d nd=d nd=d nd=d,bw=u.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
ln=l .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Rule-based ln=l,bw=d tn=r,bw=uMajority nd=d nd=dWeighted nd=d nd=d .
.
tn=r .
.Table 1: Face-display schedules for a sample sentenceFigure 2: RUTH talking headamong the original displays and the output of theweighted and rule-based strategies.
At the end ofthe section, the results of this study are discussed to-gether with the results of a similar previous studycomparing the two data-driven strategies to eachother; the full details of the earlier study are givenin Foster and Oberlander (2007).3.1 SubjectsSubjects were recruited for this experiment throughthe Language Experiments Portal,2 a website dedi-cated to online psycholinguistic experiments.
Therewere 36 subjects (20 female), 50% of whom identi-fied themselves as native English speakers; most ofthe subjects were between 20 and 29 years old.3.2 MaterialsThe materials for this experiment were based on 18randomly-selected sentences from the corpus.
Foreach sentence, face-display schedules were gener-ated using both the rule-based and the weightedstrategies.
The Festival speech synthesiser (Clark2http://www.language-experiments.org/et al, 2004) and the RUTH animated talking head(DeCarlo et al, 2004) (Figure 2) were used to createvideo clips of the two generated schedules for eachsentence, along with a video clip showing the origi-nal facial displays annotated in the corpus.3.3 MethodEach subject saw a series of pairs of videos.
Bothvideos in a pair had identical spoken content, but theface-display schedules differed: each trial includedtwo of rule-based, weighted, and original.
For eachpair of videos, the subject was asked to select whichof the two versions they preferred.
Subjects madeeach pairwise comparison between schedule typessix times?three times in each order?for a total of18 judgements.
All subjects saw the same set of sen-tences, in an individually randomly-selected order:the pairwise choices between schedule types werealso allocated to items at random.3.4 Results and analysisThe overall pairwise preferences of the subjects inthis study are shown in Figure 3(a).
A ?2 goodness-of-fit test can be used to evaluate the significanceof the choices made on each individual comparison.For the comparison between original and rule-basedschedules, the preference is significant: ?2(1,N =216)= 4.17, p< 0.05.
The results are similar for theoriginal vs. weighted comparison: ?2(1,N = 215) =4.47, p < 0.05.
However, the preferences for theweighted vs. rule-based comparison are not signifi-cant: ?2(1,N = 217) = 2.44, p?
0.12.Figure 3(b) shows the results from a similar pre-vious study (Foster and Oberlander, 2007) in whichthe subjects compared the two data-driven strate-gies to the original displays, using a design identi-cal to that used in the current study with 54 sub-jects and 24 sentences.
The responses given bythe subjects in this study also showed a signifi-9812393Original vs. Rule-basedWeighted vs. Rule-basedOriginal vs. Weighted02550751001251209712392(a) Original, weighted, rule-based295153Original vs. MajorityWeighted vs.
Majority Originalvs.
Weighted050100150200250300278170251197(b) Original, weighted, majority (Foster and Oberlander, 2007)Figure 3: Pairwise preferences from the user evaluationscant preference for the original schedules over theweighted ones (?2(1,N = 448) = 6.51, p < 0.05).Both the weighted and the original schedules werevery strongly preferred over the majority schedules(?2(1,N = 448) = 45 and 26, respectively; p0.0001).
The original vs. weighted comparison wasincluded in both studies (the rightmost pair of barson the two graphs in Figure 3), and the response pat-terns across the two studies for this comparison didnot differ significantly from each other: ?2(1,N =664) = 0.02, p?
0.89.3.5 DiscussionTaken together, the results of these two studies sug-gest a rough preference ordering among the dif-ferent strategies for generating facial displays.
Inboth studies, the judges significantly preferred theoriginal displays from the corpus over any of theautomatically-generated alternatives.
This suggeststhat, for this generation task, the data in the cor-pus can indeed be treated as a ?gold standard?
?unlike, for example, the corpus used by Belz andReiter (2006), where the human judges sometimespreferred generated output to the corpus data.
Theschedules generated by the majority strategy, on theother hand, were very obviously disliked by thejudges in the Foster and Oberlander (2007) study.The ranking between the rule-based and weightedschedules from the current study is less clear, al-though there was a tendency to prefer the latter.4 Automated evaluationSince the subjects in the user-preference studies gen-erally selected the corpus schedules over any ofthe alternatives, any automated metric for this taskshould favour output that resembles the examples inthe corpus.
The most obvious form of corpus simi-larity is exact reproduction of the displays in the cor-pus, which suggests using metrics such as precisionand recall that favour generation strategies whoseoutput on every item is as close as possible to whatwas annotated in the corpus for that sentence.
InSection 4.1, several such corpus-reproduction met-rics are described and their results presented.For this type of open-ended generation task,though, it can be overly restrictive to allow onlythe displays that were annotated in the corpus fora sentence and to penalise any deviation.
Indeed, asmentioned in the introduction, a number of previousstudies have found that the output of generation sys-tems that score well on this type of metric is oftendisliked in practice by users.
Section 4.2 thereforepresents several intrinsic metrics that aim to cap-ture corpus similarity of a different type: rather thanrequiring the system to exactly reproduce the cor-pus on each sentence, these metrics instead favourstrategies resulting in global behaviour that exhibitssimilar patterns to those found in the corpus, withoutnecessarily agreeing exactly with the corpus on anyspecific sentence.990.520.310.180.820.340.290.240.120.750.230.220.100.060.770.14PrecisionRecallF ScoreNode Acc.Beta00.20.40.60.81Majority WeightedRule-based(a) Corpus-reproduction metrics5.383.264.582.983.151.312.071.24TokensTypesTTR0123456OriginalWeightedMajority Rule-based0.640.69 0.480.6800.20.40.60.81(b) Intrinsic metricsFigure 4: Results of the automated evaluations4.1 Corpus-reproduction metricsThis first set of metrics compared the generatedschedules against the original schedules annotated inthe corpus, using 10-fold cross-validation.
The firstthree metrics that were tested are standard for thissort of corpus-comparison task: recall, precision,and F score.
Recall was computed as the propor-tion of the corpus displays for a sentence that werereproduced exactly in the generated output, whileprecision was the proportion of generated displaysthat had exact matches in the corpus; the F score fora sentence is then the harmonic mean of these twovalues, as usual.
The leftmost three columns in Ta-ble 2 show the precision, recall, and F score for thesample schedules in Table 1.In addition to the above commonly-used metrics,two other corpus-reproduction metrics were alsocomputed.
The first, node accuracy, represents theproportion of nodes in the derivation tree for a sen-tence where the proposed displays were correct, in-cluding those nodes where the system correctly se-lected no motion?a baseline system that never pro-poses any motion scores 0.79 on this measure.
Thefourth column of Table 2 shows the node-accuracyscore for the sample sentences.
The final corpus-reproduction metric compared the proposed displaysto the annotated corpus displays using the ?
agree-ment measure (Artstein and Poesio, 2005).
?
is aweighted measure that permits different levels ofP R F NAcc Tok Typ TTROriginal ?
?
?
?
6 3 0.5Rule-based 0 0 0 0.65 2 2 1Majority 0.50 0.14 0.11 0.70 2 1 0.5Weighted 0.67 0.29 0.20 0.74 3 2 0.67Table 2: Automated evaluation of the sample schedulesagreement when annotations overlap, and that cantherefore capture a more fine-grained form of agree-ment than other measures such as ?.Figure 4(a) shows the results for all of thesecorpus-reproduction measures, averaged across thesentences in the corpus; the results for the weightedand majority strategies are from Foster and Ober-lander (2007).
The majority strategy scored uni-formly higher than the weighted strategy on all ofthese measures?particularly on precision?whilethe weighted strategy in turn scored higher than therule-based strategy on all measures except for nodeaccuracy.
Using a Wilcoxon rank sum test with aBonferroni correction for multiple comparisons, allof the differences among the strategies on precision,recall, F score, and node accuracy are significant atp < 0.001.
Significance cannot be assessed for thedifferences in ?
scores, as noted by Artstein and Poe-sio (2005), but the results are similar.
Also, the nodeaccuracy score for the majority strategy is signifi-cantly better than the no-motion baseline of 0.79,while those for the weighted and rule-based strate-gies are worse (also all p < 0.001).100As expected?and as noted by Foster and Ober-lander (2007)?all of the corpus-reproduction met-rics strongly favoured the weighted strategy over theweighted strategy and generally penalised the rule-based strategy.
Since the majority strategy alwayschooses the most probable option, it is not surpris-ing that it agrees more often with the corpus thando the other strategies, which deliberately select lessfrequent options; this led to its relatively high scoreson the corpus-reproduction metrics.
It is also notsurprising that the weighted strategy beat the rule-based strategy on most of these metrics, as the for-mer selects from the most frequent options, whilethe latter uses the most marked options, which arenot generally the most frequent.4.2 Intrinsic metricsThe metrics in the preceding section compared thedisplays selected for a sentence against the displaysfound in the corpus for that sentence.
This sec-tion describes other measures that are computed di-rectly on the generated schedules, without any ref-erence to the corpus data.
For each sentence, thefollowing values were counted: the total number offace-display combinations (i.e., the number of dis-play tokens), and the number of different combina-tions (types).
In addition to being used as metricsthemselves, these two counts were also used to com-pute a third value: the type/token ratio (TTR) (i.e.,# types# tokens ), which captures the diversity of the dis-plays selected for each sentence.These intrinsic metrics were computed on eachsentence produced in the cross-validation study fromthe preceding section and then averaged to producethe final results.
Since these metrics do not requirethe original corpus data for comparison, they werealso computed on the original corpus schedules.
Therightmost columns in Table 2 show the intrinsic re-sults for the sample schedules in Table 1.The overall results for these metrics across the en-tire corpus are shown in Figure 4(b).
The originalcorpus had both the most displays types and the mosttokens; the values for weighted choice were a fairlyclose second, those for majority choice third, whilethe rule-based strategy scored lowest on both ofthese metrics.
Except for the difference between ma-jority and rule-based on the facial-display types?which is not significant?all of the differences be-tween schedule types on these two measures are sig-nificant at p < 0.001 on a Wilcoxon rank sum testwith Bonferroni correction.
When it comes to thetype/token ratio, the value for the majority-choiceschedule is significantly lower than that for the otherthree schedule types (all p < 0.0001), while thevalue for weighted choice is somewhat higher thanthat for the original schedules (p < 0.01); no otherdifferences are significant.Since the original corpus schedules scored thehighest on the user study, these metrics should beconsidered in the context of of how close the resultsare to those of the corpus.
Figure 4(b) shows thatthe weighted strategy is most similar to the corpus inboth the number and the diversity of displays it se-lects, while the other two strategies have much lowerdiversity.
However, even though the rule-based strat-egy selects fewer displays all of the other strategies,its TTR is more similar to that of the corpus and theweighted strategy, while the majority strategy has amuch lower TTR.
In fact, in the schedules generatedby the majority-choice strategy, nearly 90% of thedisplays that were selected were downward nods.5 Comparing the automated metrics withhuman preferencesQualitatively, the results of the corpus-reproductionmetrics differ greatly from the preferences of thehuman judges.
The users generally liked the ma-jority schedules the least, while all of these met-rics scored this strategy the highest.
Among theintrinsic metrics, the type and token counts placedthe weighted schedules closest to the corpus, whilethe majority and rule-based strategies were furtheraway; this agrees with the human results for thetwo data-driven strategies, but not for the rule-basedstrategy.
On the other hand, the TTR indicated thatthe output of the rule-based and weighted scheduleswas similar to the schedules found in the corpus,while the majority-choice strategy produced sen-tences with TTRs more different from the corpus,generally agreeing with the human results.To permit a more quantitative comparison be-tween the predictions of the automated metrics andthe judges?
preferences, the pairwise preferencesfrom the user study were converted into a numericvalue called the selection ratio.
The selection ratio101for an item (i.e., a sentence with a particular set offacial displays) was computed as the number of tri-als on which that item was selected, divided by thetotal number of trials on which that item was an op-tion.
For example, an item that was always preferredover any of the alternatives on all trials would score1.0 on this measure, while an item that was selecteda quarter of the time would score 0.25.
The selec-tion ratios of the items used in the human-preferencestudies ranged from 0.13 to 0.85.
As a concrete ex-ample, when the sentence in Table 1 was used in theFoster and Oberlander (2007) study, the selection ra-tios were 0.43 for the original version, 0.33 for themajority version, and 0.24 for the weighted version.The relationship between the selection ratio andthe full set of automated metrics from the preced-ing section was assessed through multiple linear re-gression.
An initial model including all of the auto-mated metrics as predictor variables had an adjustedR2 value of 0.413.
Performing stepwise selection onthis initial model resulted in a final model with twosignificant predictor variables?display tokens andTTR?and an adjusted R2 of 0.422.
The regressioncoefficients for both of these predictor variables arepositive, with high significance (p < 0.001).
Whilethe R2 values indicate that neither the initial nor thefinal model fully explains the selection ratios fromthe user study, the details of the models themselvesare relevant to the overall goal of finding automatedmetrics that agree with human preferences.The results of the stepwise selection have backedup the qualitative intuition that none of the corpus-reproduction metrics had any relationship to theusers?
preferences, while the number and diversityof displays per sentence appear to have contributedmuch more strongly to the choices made by the hu-man judges.
This adds to the growing body of ev-idence that intrinsic measures are the preferred op-tion for evaluating the output of generation systems,particularly those that are designed to incorporatevariation into their output, while measures based onstrict corpus similarity are less likely to be useful.6 ConclusionsThis paper has presented three methods for usingcorpus data to select facial displays for an embod-ied agent and shown the results from two studiescomparing the output generated by these methods.When human judges rated the output, they preferredthe original displays from the corpus and stronglydisliked the displays selected by a majority-choicestrategy, with the weighted and rule-based strategiesin between.
In the automated evaluation, the metricsthat directly compared the generated output againstthe corpus data favoured the majority strategy anddid not show any relationship with the user prefer-ences.
On the other hand, the number of displaysaccompanying a sentence and the diversity of thosedisplays both had a positive relationship with therate at which users selected that display schedule.These results confirm those of previous text-generation evaluations and extend these results tothe multimodal-generation case.
This adds to thebody of evidence that, even though direct corpus re-production is often the easiest factor to analyse au-tomatically, it is rarely an accurate reflection of userreactions to generated output.
If a system performswell on this type of metric, its output tends to be con-strained to a small space; for example, the majority-choice strategy used in these studies nearly alwaysselected a nodding expression.
For most generationtasks, output options beyond those in the corpus areoften equally valid, and users seem to prefer a sys-tem that makes use of this wider space of variations.This suggests that corpus-based generation sys-tems should use strategies that retain the full rangeof variation, and?perhaps most importantly?thatmetrics based on factors other than strict similarityare more likely to capture human preferences whenevaluating generated output.The user study described here was based only onthe preferences of human judges.
In future, it wouldbe informative to include more task-based measuressuch as task success and time taken, as user pref-erences do not always correlate with performance(Nielsen and Levy, 1994), to see if a different styleof automated measure agrees better with the resultsof this sort of user study.AcknowledgementsThis work was partly supported by the EU projectsCOMIC (IST-2001-32311) and JAST (FP6-003747-IP).
Thanks to Jon Oberlander, Jean Carletta, and theINLG reviewers for helpful feedback.102ReferencesR.
Artstein and M. Poesio.
2005.
Kappa3 = al-pha (or beta).
Technical Report CSM-437,University of Essex Department of Com-puter Science.
http://cswww.essex.ac.uk/technical-reports/2005/csm-437.pdf.A.
Belz and A. Gatt.
2008.
Intrinsic vs. extrinsic evalu-ation measures for referring expression generation.
InProceedings of ACL/HLT 2008.A.
Belz and E. Reiter.
2006.
Comparing automatic andhuman evaluation of NLG systems.
In Proceedings ofEACL 2006. acl:E06-1040.C.
Callison-Burch, M. Osborne, and P. Koehn.
2006.Re-evaluating the role of BLEU in machine transla-tion research.
In Proceedings of EACL 2006. acl:E06-1032.J.
Cassell, J. Sullivan, S. Prevost, and E. Churchill, ed-itors.
2000.
Embodied Conversational Agents.
MITPress.R.
A. J. Clark, K. Richmond, and S. King.
2004.
Festi-val 2 ?
build your own general purpose unit selectionspeech synthesiser.
In Proceedings of the 5th ISCAWorkshop on Speech Synthesis.
http://www.ssw5.org/papers/1047.pdf.R.
Dale and M. White, editors.
2007.
Reportfrom the Workshop on Shared Tasks and Com-parative Evaluation in Natural Language Genera-tion.
http://ling.ohio-state.edu/nlgeval07/NLGEval07-Report.pdf.D.
DeCarlo, M. Stone, C. Revilla, and J. Venditti.
2004.Specifying and animating facial signals for discoursein embodied conversational agents.
Computer Anima-tion and Virtual Worlds, 15(1):27?38.
doi:10.1002/cav.5.M.
E. Foster.
2007a.
Associating facial displays with syn-tactic constituents for generation.
In Proceedings ofthe ACL 2007 Linguistic Annotation Workshop.
acl:W07-1504.M.
E. Foster.
2007b.
Generating embodied descrip-tions tailored to user preferences.
In Proceedingsof Intelligent Virtual Agents 2007. doi:10.1007/978-3-540-74997-4_24.M.
E. Foster and J. Oberlander.
2007.
Corpus-basedgeneration of head and eyebrow motion for an em-bodied conversational agent.
Language Resourcesand Evaluation, 41(3?4):305?323.
doi:10.1007/s10579-007-9055-3.M.
E. Foster and M. White.
2007.
Avoiding repetition ingenerated text.
In Proceedings of ENLG 2007. acl:W07-2305.M.
E. Foster, M. White, A. Setzer, and R. Catizone.
2005.Multimodal generation in the COMIC dialogue sys-tem.
In Proceedings of the ACL 2005 Demo Session.acl:P05-3012.C.
Y. Lin.
2004.
ROUGE: A package for automatic eval-uation of summaries.
In Proceedings of the ACL 2004Workshop on Text Summarization.
acl:W04-1013.C.
Mellish and R. Dale.
1998.
Evaluation in the con-text of natural language generation.
Computer Speechand Language, 12(4):349?373.
doi:10.1006/csla.1998.0106.J.
Nielsen and J.
Levy.
1994.
Measuring usability: pref-erence vs. performance.
Communications of the ACM,37(4):66?75.
doi:10.1145/175276.175282.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.BLEU: A method for automatic evaluation of ma-chine translation.
In Proceedings of ACL 2002. acl:P02-1040.C.
Paris, D. Scott, N. Green, K. McCoy, , and D. Mc-Donald.
2007.
Desiderata for evaluation of natural lan-guage generation.
In Dale and White (2007), chapter 2.A.
Stent, M. Marge, and M. Singhai.
2005.
Evaluatingevaluation methods for generation in the presence ofvariation.
In Computational Linguistics and IntelligentText Processing, pages 341?351.
Springer.
doi:10.1007/b105772.M.
A. Walker.
2005.
Can we talk?
Methods for evalu-ation and training of spoken dialogue systems.
Lan-guage Resources and Evaluation, 39(1):65?75.
doi:10.1007/s10579-005-2696-1.M.
A. Walker, D. J. Litman, C. A. Kamm, and A. Abella.1997.
PARADISE: A general framework for eval-uating spoken dialogue agents.
In Proceedings ofACL/EACL 1997. acl:P97-1035.M.
White.
2006.
Efficient realization of coordinate struc-tures in Combinatory Categorial Grammar.
Researchon Language and Computation, 4(1):39?75.
doi:10.1007/s11168-006-9010-2.103
