Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 426?436,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsCrowdsourcing High-Quality Parallel Data Extraction from Twitter?Wang Ling123Lu?s Marujo123Chris Dyer2Alan Black2Isabel Trancoso13(1)L2F Spoken Systems Lab, INESC-ID, Lisbon, Portugal(2)Language Technologies Institute, Carnegie Mellon University, Pittsburgh, PA, USA(3)Instituto Superior T?cnico, Lisbon, Portugal{lingwang,lmarujo,cdyer,awb}@cs.cmu.eduisabel.trancoso@inesc-id.ptAbstractHigh-quality parallel data is crucial for arange of multilingual applications, fromtuning and evaluating machine translationsystems to cross-lingual annotation pro-jection.
Unfortunately, automatically ob-tained parallel data (which is availablein relative abundance) tends to be quitenoisy.
To obtain high-quality parallel data,we introduce a crowdsourcing paradigmin which workers with only basic bilin-gual proficiency identify translations froman automatically extracted corpus of par-allel microblog messages.
For less than$350, we obtained over 5000 parallel seg-ments in five language pairs.
Evaluatedagainst expert annotations, the quality ofthe crowdsourced corpus is significantlybetter than existing automatic methods:it obtains an performance comparable toexpert annotations when used in MERTtuning of a microblog MT system; andtraining a parallel sentence classifier withit leads also to improved results.
Thecrowdsourced corpora will be made avail-able in http://www.cs.cmu.edu/~lingwang/microtopia/.1 IntroductionHigh-quality parallel data is essential for tun-ing and evaluating statistical MT systems, andit plays a role in a wide range of multilingualNLP applications, such as word sense disambigua-tion (Gale et al., 1992; Ng et al., 2003; Speciaet al., 2005), paraphrasing (Bannard and Callison-burch, 2005; Ganitkevitch et al., 2012), annota-tion projection (Das and Petrov, 2011), and otherlanguage-specific applications (Schwarck et al.,?A sample of the crowdsourced corpora and the inter-faces used are available as supplementary material.2010; Liu et al., 2011).
While large amountsof parallel data can be easily obtained by miningthe web (Resnik and Smith, 2003), comparablecorpora (Munteanu and Marcu, 2005), and evensocial media sites (Ling et al., 2013), automati-cally extracted parallel tends to be noisy, and, as aresult, ?evaluation-quality?
parallel corpora havegenerally been produced at considerable expenseby targeted translation efforts (Bojar et al., 2013,inter alia).
Unfortunately, in some domains suchas microblogs, the only corpora that are availableare automatically extracted and noisy.While phrase-based translation models can ef-fectively learn translation rules from noisy paralleldata (Goutte et al., 2012), having a subset of high-quality parallel segments is nevertheless crucial.Firstly, the automatic parallel data extraction sys-tem?s parameters can be tuned by optimizing onthe gold standard data.
Secondly, even though theparallel data used to train MT systems can containa considerable amount of noise, it is conventionalto use human annotated parallel data to tune andevaluate the system.
Finally, other NLP applica-tions may not be as noise-robust as MT.We introduce a new crowdsourcing protocol forobtaining high-quality parallel data from noisy,automatically extracted parallel data (?3), focus-ing on the challenging case of identifying par-allel data in microblog messages (Ling et al.,2013).
In contrast to previous attempts to usecrowdsourcing to obtain parallel data, in whichworkers performed translation (Ambati and Vo-gel, 2010; Zaidan and Callison-Burch, 2011; Postet al., 2012; Ambati et al., 2012), our approachonly requires that they identify whether a candi-date message contains a translation, and if so, whatthe spans of the translated segments are.
This isa much simpler task than translation, and one thatcan often be completed by workers with only a ba-sic proficiency in the source and target languages.For evaluation (?4), we use our protocol to build426parallel datasets on a Chinese-English corpus orig-inally extracted from Sina Weibo and for which wehave expert annotations.
This lets us quantify theeffectiveness of our method under different taskvariations.
We also show that the crowdsourcedcorpus performs as well as expert annotation (andbetter than the automatically extracted corpus) fortuning an MT system with MERT.
We next applyour method on a corpus of five language pairs (en-ar, en-ja, en-ko, en-ru, en-zh) extracted from Twit-ter (?5), for which we have no gold-standard data.Using this data in a cross-validation setup, we trainand evaluate a maxent classifier for detecting par-allel data (?6), and then we conclude (?7).2 Related WorkOur work crosses crowdsourcing techniques andautomatic parallel data extraction from mi-croblogs.
In this section, we shall provide back-ground information and analysis of the work per-formed in these two fields.2.1 Parallel Data Extraction from MicroblogsMany sources of parallel data exist on theweb.
The most popular choice are parallel webpages (Resnik and Smith, 2003), while otherwork have looked at specific domains with largeamounts of data, such as Wikipedia (Smith etal., 2010).
Microblogs, such as Twitter and SinaWeibo, represent a subdomain of the Web.
Someof its characteristics is the informal language usedand the short nature of the messages that areposted.
Due to its large size and growing pop-ularity, work has been done on parallel data ex-traction from this domain.
Ling et al.
(2013) at-tempt to find naturally occurring parallel data fromSina Weibo and Twitter.
Some examples of whatis found are illustrated in Figure 1.
The extrac-tion process starts by finding the parallel segmentswithin the same message and the word alignmentsbetween those segments that maximize a hand-tuned model score.Another method (Jehl et al., 2012) leveragesCLIR (Cross Lingual Information Retrieval) tech-niques to find pairs of tweets that are translations.The main challenge in this approach is the largeamount of pairs of tweets that must be considered,which raises some scalability issues when process-ing billions of tweets.Our crowdsourcing method can be applied toannotate data from any naturally occurring source.In this paper, we will use the corpus developedby Ling et al.
(2013), since it is publicly availableand has parallel data for 6 languages from Twitter,and for 10 languages from Sina Weibo.2.2 Parallel Data using CrowdsourcingMost of the work done in building parallel datausing crowdsourcing (Ambati and Vogel, 2010;Zaidan and Callison-Burch, 2011; Post et al.,2012; Ambati et al., 2012) relies on using crowd-sourcing workers to translate.
These methodsmust address the fact that workers may producepoor and sometimes incorrect translations.
Thus,in order to find good translations, subsequentpostediting and/or ranking is generally necessary.In contrast, in our work, crowdsourcing is usedfor data extraction rather than translation, a sub-stantially simpler task than translation (in particu-lar, translation of informal text) that requires lessexpertise in the language pair (basic proficiency inthe two languages is generally sufficient to suc-cessfully complete the task).
Furthermore, assess-ing whether a worker performed the task correctlyand combining the outputs of different workers issimpler.
The time spent per item is also reduced:our annotation interface only requires the workerto make a few clicks on the tweet to completeeach annotation, meaning that tasks are completedfaster and with less effort, allowing us to obtaintranslations at lower cost.
On the other hand,the main drawback of our method is that it canonly obtain parallel data from translations that ex-ist, which corresponds to the amount of posts thathave been translated and posted.
This limits thepotential coverage of our method.
Furthermore,the resulting datasets may not be fully representa-tive of the Twitter domain, since not all types ofcontent are translated and follow the same distri-bution as the data in Twitter.3 Proposed Crowdsourcing ProtocolAs discussed above, automatically extracted par-allel is often noisy.
The sources of error rangefrom language detection errors, to errors determin-ing if material is actually translation, and errors inextracting the appropriate spans of the translatedmaterial.
Consider the fragment of the microblogparallel corpus mined by Ling et al.
(2013), whichis shown in Figure 1.
In the Korean-English mes-sage, the system may incorrectly added the un-translated word Hahah in the English segment,427and missed the translated word Weather.
At a highlevel, the task faced by annotators will be to iden-tify and resolve such errors.3.1 OverviewWe separate the tasks of identifying the parallelposts, which we shall denote by identification,and of locating the parallel segments, which wewill call location.
The justification for this is thatthe majority of the tweets are not parallel, as re-ported by Ling et al.
(2013), and the location ofthe parallel data is only applicable if the tweetactually contains parallel data.
This is also de-sirable because the identification task is simplerthan the location task.
Firstly, identifying whethera tweet contains translations requires much lessproficiency in the respective languages than locat-ing the parallel segments, since it only requiresthe worker to understand parts of the message.This means we can have more potential workerscapable of performing this task.
Secondly, thefirst task is a binary decision, and each annota-tion can be completed with only one action, whichmeans that the average required time for this taskis much lower than the second task and the pay-ment required for each hit will naturally be loweras well.
Finally, combining worker results for abinary decision is simpler than combining transla-tions, since the space of possible answers is sev-eral orders of magnitude lower.As crowdsourcing platform, we use Amazon?sMechanical Turk.
In this platform, the requesterscan submit tasks, where one can define the num-ber of workers n that will complete each task andwhat is the payment p for each task submission,henceforth denoted as job.
In our work, we had toconsider the following components:?
Interface - To submit a task, an interfacemust be provided, which workers will be us-ing to complete the job.?
Worker Quality Prediction - After submit-ting a job, the requester can accept and paythe agreed fee or reject the task.
It is cru-cial to have a method to automatically pre-dict whether workers have performed the jobproperly, and reject them otherwise.?
Result Combination - It is common for mul-tiple workers to complete the same task withdifferent results.
Thus, a method must be im-plemented to combine multiple responses forcorrectly predicting the desired response.We structured each of our tasks as a series of qquestions, which include a small number of refer-ences r, for which we know the answers.
Thus,the amount of answers we obtain for each dollar isgiven byq?rnp, where n is the number of workersper task and p is the payment for each task.
In or-der to maximize this quotient, we can either reducethe number of reference question r, the number ofworkers per task n, or the payment p. However,reducing r will also limit our capability of esti-mating the quality of the worker results, since wewill have less data to make such prediction.
Forthe same reason, reducing n will limit our abil-ity to combine results properly.
As for the pay-ment p, while there is no direct effect on our task,it has been noted that workers will perform thetask faster for higher payments (Post et al., 2012).In our work, we will propose methods to predictquality and combine results that will minimize therequirements for n and r, while maximizing thequality of the final results.3.2 Parallel Post IdentificationIn the identification task, for each question, wewill show a post, and solicit the worker to detect ifit contains translations in a given language pair.Interface The interface for this task is straight-forward.
We present to the worker each tweet in-dividually, together with a checkbox to be checkedin case the tweet contains parallel data.
The navi-gation between tweets is done by adding next andprevious buttons, allowing the user to go back andreview previous answers.
Finally, the worker canonly submit the HIT after traversing all 25 ques-tions.
Unlike the work in crowdsourcing transla-tion (Zaidan and Callison-Burch, 2011), where au-tomatic translation systems are discouraged, sinceit produces poor output, we allow its usage as longas this leads to correct annotations.
In fact, we adda button to automatically translate the tweet intoEnglish from the non-English language.Worker Quality Prediction We accept the jobif it answers enough reference questions correctly.We consider two different approaches to select ref-erences.
A random sampler that selects tweetsrandomly and a balanced sampler that selectsthe same number of positive and negative sam-ples.
As notation, we will denote as acceptor428Figure 1: Parallel microblog posts in 5 language pairs.
Shaded backgrounds mark the parallel segments(annotated manually), non shaded parts do not have translations.accept(rand, c, r) a setup where the worker?s jobis accepted if c out of r randomly sampled refer-ences are correctly answered.
Likewise, acceptoraccept(bal, c, r) denotes the same setup using bal-anced reference questions.Result Combination Given n jobs with answersfor a question that can be either positive or nega-tive, we calculate the weighted ratio of positive an-swers, given by?i=1..n?p(i)w(i)?i=1..nw(i), where ?pis one ifanswer i is positive and 0 otherwise, and w(i) isthe weight of the worker.
w(i) is defined as theratio of correct answers from job i in the referenceset.
If the weighted ratio is higher than 0.5, we la-bel the tweet as positive and otherwise as negative.3.3 Parallel Data LocationIn the location task, we also present one tweet perquestion, where the worker will be asked to iden-tify the parallel segments.
The worker can alsodefine that there are no translations in the tweet.Interface The interface for this task presents theuser with one tweet at a time, and allows the userto break the tweet into segments, by clicking be-tween characters.
Each segment can then be clas-sified as English, the non-English language (Ex:Mandarin), or non-parallel, which is the defaultoption.
To understand the concept of non-parallelsegments, notice that when we are locating par-allel data in tweets, we are essentially breakingthe tweet into the structure ?NleftPleftNmiddlePrightNright", where Pleftand Prightare the par-allel segments and Nleft, Nmiddleand Nrightaretextual segments that are non-parallel.
These maynot exist, for instance, the Arabic tweet in Fig-ure 1 (line 1) does not contain any non-parallel textand does not require any non-parallel segmentsto delineate the parallel data.
The Korean tweet(line 2), on the other hand, has an Nmiddlecorre-sponding to????????????????????????????
* and anNrightcorresponding to Hahah and requires twonon-parallel segments to locate the parallel data.Thus, if the worker does not commit any errors,each question can be answered with at most fourclicks, when all five segments exist, and two op-tion choices for identifying the parallel segments.In the easiest case, when only the parallel seg-ments exist, only one click and two option choicesare needed.
If there are no translations, the buttonno translations can be clicked.For instance, to annotate the Korean tweet inFigure 1, the worker must click immediately be-fore???
?, then before Weather and finally beforeHahah.
Then on the drop-down box of the firstand and third segments, the worker must chooseKorean and English, respectively.
The interfaceafter these operations is show in Figure 2.Work Quality Prediction To score the worker?sjobs, we use the scoring function devised in (Linget al., 2013), which measures the word overlapbetween the reference parallel segments segmentsand the predicted segments.
However, setting thescore threshold to accept a job is a challenge, sincescores are bound to change for different language-pairs and domains.
Moreover, some tweets areharder to annotate than others.
Learning thisthreshold automatically requires annotated data,which we do not have for all language pairs anddomains.
Thus, we propose a method to generatethresholds specifically for each sample.We consider a ?smart but lazy" pseudo worker,who will complete the same jobs automaticallyand generate scores that the real worker?s jobsmust beat to be accepted.
We say he is ?smart",429Figure 2: Location Interface (After the annotation is performed)since he knows the reference annotation, and?lazy" because he will only define a new non-parallel segment if it is significant, otherwise itwill just be left in the parallel segments.
By sig-nificant, we will define whether it is at least 20%larger (in number of characters) than the parallelsegments.
For instance, in the Korean example inFigure 1, Hahah would be left in the English par-allel segment, while ??????????????????????????
??
* would not be in the Korean segment.
Wewill accept a job if the average of the scores in thereference set is higher or equal than the pseudoworker?s scores.
This acceptor shall be denoted asaccept(lazy, a), where a is the number of refer-ences used.Another option is to use the automatic system?soutput as a baseline that workers must improve tobe accepted.
We will also test this option and callthis acceptor accept(auto, a).Result Combination Unlike the identificationtask, where the result is binary and combiningmultiple decisions is straightforward, the range ofresults from this task is larger and combining themis a challenge.
Thus, we score each job based onthe WER on the reference set and use annotationsof the highest scoring job.4 ExperimentsTo obtain results on the effectiveness of the meth-ods described in Section 3, we will first performexperiments using pre-annotated data.
We use theannotated dataset with tweets in Mandarin-Englishfrom Sina Weibo created in (Ling et al., 2013).It consists of approximately 4000 tweets crawledfrom Sina Weibo that were annotated on whetherthey contained parallel data and the location of theparallel segments.
In our experiment, we sample1000 tweets from this dataset, where 602 tweetswere parallel and 398 were not.1We will not submit the same tasks using differ-ent setups, since we would have to pay the cost ofthe tasks multiple times.
Furthermore, we knowthe answers for all the questions in this controlledexperiment, the quality of a job can be evalu-ated precisely by using all questions as references.Thus, we will perform the task once, with a largernumber of workers and accepting and rejectingjobs based on their real quality.
Then, we will usethe resulting datasets and simulate the conditionsusing different setups.430Acceptor avg(a) avg(r) daccept(rand, 2, 2) 0.44 0.00 0.44accept(rand, 3, 4) 0.44 0.00 0.44accept(rand, 4, 4) 0.55 0.04 0.51accept(bal, 2, 2) 0.69 0.09 0.60accept(bal, 3, 4) 0.64 0.03 0.61accept(bal, 4, 4) 0.76 0.15 0.61Table 1: Agreement with the expert annotationsfor different acceptors.4.1 Identification TaskThe 1000 tweets were distributed into 40 taskswith 25 questions each (q = 25).
Each task isto be performed by 5 workers (n = 5) and uponacceptance, a worker would be rewarded with 6cents (p = 0.06).
As we know the answers forall the questions in this case, we will calculate theCohen?s Kappa between the responses of each joband the expert annotator, and accept a job if it ishigher than 0.5.
We decided to use Cohen?s kappato evaluate a job, rather than accuracy, since eachset of 25 questions does not contain the same num-ber of positive and negative samples.
For instance,in a set of 20 negative samples, a worker wouldachieve an accuracy of 80% if he simply answersnegatively to all questions, which is not an ade-quate assessment of the job?s quality.
On the otherhand, the Cohen?s Kappa balances the positive andnegative question in each task by using their priorprobabilities.
In total, there were 566 jobs, where200 where accepted and 366 were rejected.Next, we pretended that we only have access to4 references, which will be used for quality es-timation and simulate the acceptances and rejec-tions for each strategy.
Table 1 shows the aver-ages of the real Kappa values of accepted (col-umn avg(a)) and rejected jobs (column avg(r))using different acceptors.
Our goal is to maximizethe number of acceptances with high Kappa val-ues and minimize those that have low Kappa val-ues.
Thus, we define d as the difference betweenavg(a) and avg(r).
From the results, we observethat using a balanced reference yields a much bet-ter estimation of the jobs quality using our metricd.
Similar conclusions can be reached by compar-ing accept(rand, 3, 4) with accept(bal, 3, 4) andaccept(rand, 4, 4) with accept(bal, 4, 4).
Qualitypredictors that use balanced reference sets achieve1We wished to annotate a sample where the number ofparallel posts is high, so that we would have enough samplesto perform the location task.Acceptor prec recall F1 acc ?Automatic 0.87 0.69 0.77 0.75 0.51All jobs 0.75 0.84 0.8 0.74 0.44accept(rand, 2, 2) 0.85 0.92 0.88 0.86 0.69accept(rand, 3, 4) 0.84 0.93 0.88 0.85 0.68accept(rand, 4, 4) 0.91 0.95 0.93 0.92 0.82accept(bal, 2, 2) 0.94 0.94 0.94 0.92 0.84accept(bal, 3, 4) 0.93 0.95 0.94 0.93 0.85accept(bal, 4, 4) 0.94 0.93 0.93 0.92 0.84Table 2: Parallel post prediction scores using dif-ferent acceptors.approximately the same results for d. However,the setup accept(bal, 3, 4) has a lower Kappas forboth avg(a) and avg(r), which means that it isless likely to reject good jobs at the cost of accept-ing more bad jobs.
This is desirable from an ethi-cal perspective, since workers are not responsiblefor errors in our quality prediction.
Furthermore,rejecting good jobs has a negative impact on theprogress of the task, since good workers may bediscouraged to perform more tasks.Results on the identification task, obtained forn = 3, are shown in Table 2.
Naturally, us-ing a balanced reference set yields better results,since these have a higher d value.
We can alsosee the importance of quality prediction, since notperforming quality estimation (row All jobs) willyield worse results than the automatic system.Next, we will compare results using differentnumbers of workers.
We fix the quality predic-tion methodology to accept(bal, 3, 4) and resultsare shown in Table 3.
We observe that in gen-eral, using more workers will generate better re-sults, but score gains from adding another workerbecomes lower as n increases.
One problem forn = 2 is the fact that there are many cases wheretwo workers with the same weight chose a posi-tive and a negative answer, in which case, no de-cision can be made, and we simply choose falseby default.
This explains the high recall and lowprecision values.
However, this problem seems tooccur much less with higher values of n.4.2 Location TaskFor the location task, we used the predicted par-allel posts the identification task with the setupaccept(bal, 3, 4) and n = 5.
We preferred to usethis rather than using the expert annotations, sinceit would not contain false positives, which does notsimulate a real situation.
Then, we used 500 out of431# workers prec recall F1 acc ?Automatic 0.87 0.69 0.77 0.75 0.511 0.86 0.85 0.85 0.82 0.642 0.85 0.95 0.90 0.87 0.723 0.93 0.95 0.94 0.93 0.854 0.94 0.96 0.95 0.94 0.875 0.96 0.96 0.96 0.95 0.90Table 3: Identification scores for different n.the 607 identified positive samples.
This makes20 tasks in total, with 25 questions (q = 25), andeach task would be run until 5 jobs are accepted(n = 5).
For this task, we set a payment of 30cents (p = 0.3), since it is a more complex task.Again, since we have the expert annotations for allquestions, we calculated the average WER on allanswers and rejected jobs scoring less than 0.62.This task is mainly focused on the quality pre-diction of the workers, as the result combinationis done by finding the job with the highest scorein the reference set.
This means, for an arbitrarylarge n, all quality estimation methods will pro-duce the same result, since we will find the bestjob on the references eventually.
However, bet-ter quality estimation will allow us to find the bestjobs with lower n, which makes the task less ex-pensive.
Table 4 shows results using different se-tups.
In these results, we set aside 4 questions tobe used as references.
We can see that for low n(1 or 2), if we simply accept all jobs, the qualityof the results will be lower than the automatic sys-tem.
For n = 4, this approach can achieve a WERscore of 0.06.
However, if we use the automaticsystem as a baseline that jobs must surpass, we canachieve this WER score with only two jobs, whichreduces the cost of this task by half.
Yet, this isstrongly dependent on the automatic system, as aworse system will be easier to match for the work-ers.
On the other hand, using the smart but lazypseudo worker, where we degrade the referenceannotations slightly, we can see that we can obtainthe 0.06 WER score using only the first worker.
Atn = 2, we can see that the WER improves to 0.05,which is lost for n = 3.
This is because the pre-diction of the quality of the job using the workersis not always precise.4.3 Machine Translation ResultsFinally, we will perform an extrinsic test to seehow the improvements obtained by using crowd-2Determined empiricallyNumber of jobs 1 2 3 4 5Automatic 0.16 0.16 0.16 0.16 0.16All Jobs 0.23 0.21 0.07 0.06 0.06accept(auto, 4) 0.09 0.06 0.06 0.06 0.06accept(lazy, 4) 0.06 0.05 0.06 0.06 0.06Table 4: Parallel data location scores for differentacceptors (rows) and different numbers of work-ers.
Each cell denotes the WER for that setup.Auto (Pos) Crowd Expert Auto (All)Size 483 479 483 908EN-ZH 10.21 10.49 10.51 10.71ZH-EN 7.59 7.87 7.82 8.02Table 5: BLEU score comparison using differentcorpora for MERT tuning.
The Size row denotesthe number of sentences of each corpus, and theEN-ZH and ZH-EN rows denote the BLEU scoresof the respective language pair and tuning dataset.sourcing map to Machine Translations.
We willbuild an out of domain MT system using the FBISdataset (LDC2003E14), a corpus of 300K sen-tence pairs from the news domain in the Chinese-English pair using the Moses (Koehn et al., 2007)pipeline.
Due to the small size of our crowd-sourced corpus, we will use it in the MERT tun-ing (Och, 2003), and test its effects compared toautomatically extracted parallel data and the ex-perts judgements.
As the test set, we will use1,500 sentence pairs from the Weibo gold standardfrom Ling et al.
(2013), that were not used in ourcrowdsourcing experiment to prevent data over-lap.
For reordering, we use the MSD reorderingmodel (Axelrod et al., 2005) and as the languagemodel, we use a 5-gram model with Kneser-Neysmoothing (Heafield, 2011).
Finally, results arepresented with BLEU-4 (Papineni et al., 2002).We build 3 tuning corpora, the automatically ex-tracted corpus (denoted Auto), the crowdsourcedcorpus (denoted Crowd) and the corpus annotatedby the expert (denoted Expert).
This is done bytaking the 1000 tweets used in this experiment, se-lect those that were identified as parallel accord-ing to each criteria.
For the automatic extraction,the authors in (Ling et al., 2013) simply use alltweets as parallel, which may influence the tun-ing results.
Thus, we test two versions of this cor-pus, one where we take all samples as parallel (de-noted Auto (All)), and one where we use the ex-pert?s decision for the identification task only (de-432Pair Parallel Avg(en) cost(I) cost(L) totalen-ar 1512 8.3 $35.7 $43.2 $76.2en-zh 1302 8.7 $35.7 $37.2 $70.2en-ja 1155 7.9 $35.7 $33.0 $68.7en-ko 1008 7.1 $35.7 $28.8 $64.5en-ru 798 6.3 $35.7 $22.8 $58.5all 5775 ?
$178.5 $165.0 $343.5Table 7: AMT costs for crowdsourced corporafrom Twitter.noted Auto (Pos)).
In the crowdsourcing case, weuse the accept(bal, 3, 4) setup, with n = 5, for theidentification task and the accept(lazy, 4) setup,with n = 2, for the location task.
From the re-sulting parallel tweets, we also remove all tweetsthat were used as reference in the accept(lazy, 4)quality estimator, as this would give an unfair ad-vantage to the crowdsourced corpora.Results are shown in Table 5, where each cellcontains the average BLEU score in 5 MERT runs,using a different tuning dataset.
Surprisingly, us-ing the whole set of automatically extracted cor-pora actually achieves better results than usingcarefully selected data that are parallel.
We be-lieve that is because many non-parallel segmentsactually contain comparable information that canbe used to improve the weights during MERT tun-ing.
However, this does not mean that the qual-ity of the automatically crawled corpus is betterthan the crowdsourced and expert annotated cor-pus.
When using a similar number of parallel sen-tences, we observe that using the crowdsourcedcorpus yields better scores than the automaticallyextracted corpora, comparable to experts annota-tions.
While results are not significantly betterthan automatically extracted corpora, this suggeststhat the crowdsourced corpora has a better overallquality than automatically extracted corpora.5 Five Language Twitter Parallel CorpusNow that we have established the effectiveness ofour technique for extracting high-quality paralleldata in a scenario where we have gold standardannotations, we apply it to creating parallel cor-pora in five languages on Twitter, for which wehave no gold-standard parallel data: Arabic, Man-darin, Japanese, Korean and Russian.
Once again,we use the extracted automatically Twitter cor-pus from Ling et al.
(2013) and deploy the taskin Mechanical Turk.
We use the setup that ob-tained the best results in Section 4.
For the identi-fication task, we used the accept(bal, 3, 4) setup,with n = 5.
The payment for each task was0.06 dollars.
Thus, for this task, each dollar spentyields 70 annotated tweets.
For the location task,we used the accept(lazy, 4) setup, with n = 2and each task was rewarded with 0.3 dollars.
Toobtain the tweet sample, we filtered the corporain Ling et al.
(2013) for tweets with alignmentscores higher than 0.1.
Then, we uniformly ex-tracted 2500 tweets for each language.
To gener-ate gold standard references, the authors manuallyannotated 40 samples for each pair.Table 7 contains information about the result-ing corpora.
The number of parallel sentences ex-tracted from the 2500 tweets in each language pairis shown in column Parallel and we can see thatthis differs given the language pair.
We can alsosee in column Avg(en) that the average number ofEnglish words is much smaller than what is seenin more formal domains.
Finally, Arabic paralleldata seems more predominant from our samplesfollowed by Mandarin, while Russian parallel dataseem scarcer.6 Discriminative Parallel Data DetectionWhile the work in (Ling et al., 2013) used a linearcombination of three models, the alignment, lan-guage and segment features, these weights weredetermined manually.
However, using the crowd-sourced corpus (in Section 5), we will apply previ-ously proposed methods that learn a classifier withmachine learning techniques as in related workon finding parallel data (Resnik and Smith, 2003;Munteanu and Marcu, 2005).
In our work, we usea max entropy classifier model, similar to that pre-sented by Munteanu and Marcu (2005) to detectparallel data in tweets.
Our features are:?
Alignment feature - The baseline feature isthe alignment score from the work in (Ling etal., 2013), and measures how well the paral-lel segments align, which is derived from thecontent-based matching methods for detect-ing parallel data (Resnik and Smith, 2003).?
User features - An observation in (Ling etal., 2013) is that a user that frequently postsin parallel is likely to post more parallel mes-sages.
Based on this, we added the aver-age alignment score from all messages of thesame user and the ratio of messages that arepredicted to be parallel as features.433Weibo (en-zh) Twitter (en-zh) Twitter (en-ar) Twitter (en-ru) Twitter (en-ko) Twitter (en-ja)Alignment 0.781 0.599 0.721 0.692 0.635 0.570+User 0.814 0.598 0.721 0.705 0.650 0.566+Length 0.839 0.603 0.725 0.706 0.650 0.569+Repetition 0.849 0.652 0.763 0.729 0.655 0.579+Language 0.849 0.668 0.782 0.737 0.747 0.584Table 6: Classification Results using a 10-fold cross validation over different datasets.
Each cell containsthe F-measure using a given dataset and an incremental set of features.?
Repetition features - There are many wordsthat are not translated, such as hashtags, atmentions, numbers and named entities.
So, ifwe see these repeated twice in the same post,it can be used as a strong cue that this wasthe result of a translation.
Hence, we definefeatures for each of these cases, that trigger ifeither of these occur in multiples of two timesin the same post.
Named Entities were iden-tified using a naive approach by consideringwords with capital letters.?
Length feature - It is known that the lengthdifferences between parallel sentences canbe modelled by a normal distribution (Galeand Church, 1991).
Hence, we used paralleldata in the respective language to determine(?
?, ?
?2), which lets us calculate the likelihoodof two hypothesized segments being parallel.Since we did not have annotated parallel datafor this domain, we used the top 2000 scoringparallel sentences from the respective Twitterdataset in (Ling et al., 2013).?
Language feature - It is common for non-English words to be found in English seg-ments, such as names of foreign celebri-ties, numbers and hashtags.
However, whenthis happens to the majority of the words ina segment that is supposed to be English,it may indicated that there was an error inthe language detection.
The same happenswith non-English segments.
We used thesame naive approach to detect languages asin (Ling et al., 2013), where we calculate theratio of number of words in the English seg-ment and the total number of words from thesegment detected as English and the ratio ofthe number of Foreign words and the totalnumber of words in the Foreign segment ,de-tected by their unicode ranges.
This was alsoincluded in the work in (Ling et al., 2013).Results using a 10 fold cross-validation areshown in Table 6.
In general, we can see that theclassifier performs worse in Twitter datasets com-pared to the Weibo dataset.
We believe that this isbecause parallel sentences extracted from Twitterare smaller, due to the 140 character limit, whichdoes not hold in Sina Weibo.
Each parallel En-glish segment from the Sina Weibo parallel datacontains 15.4 words on average.
On other hand,we see in Table 7 that this number is smaller inthe parallel data from Twitter.
This means that thealigner will have a much smaller range of words toalign when detecting parallel data, which makes itmore difficult to find parallel segments.As for the features, we observe that by defin-ing these simple features, we can get a signifi-cant improvement over previous baselines.
Forthe User feature, we see that the improvementsin the Weibo dataset are much larger than inthe Twitter datasets.
This is because the Twitterdataset was crawled uniformly, whereas the Weibodataset was focused on users that post paralleldata frequently.
Thus, in the Weibo dataset theremore posts that were posted by the same user,which does not happen as frequently in the Twitterdataset.
As for the Length feature, we can see thatit yields a small but consistent improvement overall datasets.
Repetition based features also lead toimprovements across all datasets, and produces a5% improvement in the English-Mandarin Twitterdataset.
Finally, language based features also addanother improvement over previous results.7 ConclusionsWe presented a crowdsourcing approach to extractparallel data from tweets.
As opposed to meth-ods to crowdsource translations, our tasks do notrequire workers to translate sentences, but to findthem in tweets.
Our method is divided into twotasks.
First, we identify which tweets containtranslations, and we show that multiple worker?sjobs can be combined to obtain results compara-434ble to those of expert annotators.
Secondly, tweetsthat are found to contain translations are givento other workers to locate the parallel segments,where we can also obtain high quality results.Then, we use our method to extract high qualityparallel data from Twitter in 5 language pairs.
Fi-nally, we improve the automatic identification oftweets with translations by using a max entropyclassifier trained on the crowdsourced data.We are currently extracting more data and thecrowdsourced parallel data from Twitter will madebe available to the public.References[Ambati and Vogel2010] Vamshi Ambati and StephanVogel.
2010.
Can crowds build parallel corporafor machine translation systems?
In Proceedingsof the NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s Mechan-ical Turk, Stroudsburg, PA, USA.
Association forComputational Linguistics.
[Ambati et al.2012] Vamshi Ambati, Stephan Vogel,and Jaime Carbonell.
2012.
Collaborative workflowfor crowdsourcing translation.
In Proceedings of theACM 2012 Conference on Computer Supported Co-operative Work, CSCW ?12, pages 1191?1194, NewYork, NY, USA.
ACM.
[Axelrod et al.2005] Amittai Axelrod, Ra Birch Mayne,Chris Callison-burch, Miles Osborne, and DavidTalbot.
2005.
Edinburgh system description for the2005 iwslt speech translation evaluation.
In Pro-ceedings International Workshop on Spoken Lan-guage Translation (IWSLT.
[Bannard and Callison-burch2005] Colin Bannard andChris Callison-burch.
2005.
Paraphrasing withbilingual parallel corpora.
In In ACL-2005, pages597?604.
[Bojar et al.2013] Ond?rej Bojar, Christian Buck, ChrisCallison-Burch, Christian Federmann, Barry Had-dow, Philipp Koehn, Christof Monz, Matt Post,Radu Soricut, and Lucia Specia.
2013.
Find-ings of the 2013 Workshop on Statistical MachineTranslation.
In Proceedings of the Eighth Work-shop on Statistical Machine Translation, pages 1?44, Sofia, Bulgaria, August.
Association for Com-putational Linguistics.
[Das and Petrov2011] Dipanjan Das and Slav Petrov.2011.
Unsupervised part-of-speech tagging withbilingual graph-based projections.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies - Volume 1, HLT ?11, pages 600?609,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.
[Gale and Church1991] William A. Gale and Ken-neth W. Church.
1991.
A program for aligningsentences in bilingual corpora.
In Proceedings ofthe 29th Annual Meeting on Association for Com-putational Linguistics, ACL ?91, pages 177?184,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.
[Gale et al.1992] William A. Gale, Kenneth W. Church,and David Yarowsky.
1992.
Using bilingual materi-als to develop word sense disambiguation methods.
[Ganitkevitch et al.2012] Juri Ganitkevitch, Yuan Cao,Jonathan Weese, Matt Post, and Chris Callison-Burch.
2012.
Joshua 4.0: Packing, PRO, and para-phrases.
In Proceedings of the Seventh Workshopon Statistical Machine Translation, pages 283?291,Montr?al, Canada, June.
Association for Computa-tional Linguistics.
[Goutte et al.2012] Cyril Goutte, Marine Carpuat, andGeorge Foster.
2012.
The impact of sentencealignment errors on phrase-based machine transla-tion performance.
In Proc.
of AMTA.
[Heafield2011] Kenneth Heafield.
2011.
KenLM:faster and smaller language model queries.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 187?197, Edinburgh, Scot-land, United Kingdom, July.
[Jehl et al.2012] Laura Jehl, Felix Hieber, and StefanRiezler.
2012.
Twitter translation using translation-based cross-lingual retrieval.
In Proceedings of theSeventh Workshop on Statistical Machine Transla-tion, pages 410?421, Montr?al, Canada, June.
Asso-ciation for Computational Linguistics.
[Koehn et al.2007] Philipp Koehn, Hieu Hoang,Alexandra Birch, Chris Callison-burch, RichardZens, Rwth Aachen, Alexandra Constantin, Mar-cello Federico, Nicola Bertoldi, Chris Dyer, BrookeCowan, Wade Shen, Christine Moran, and OndrejBojar.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proceedings ofthe 45th Annual Meeting of the Association forComputational Linguistics Companion VolumeProceedings of the Demo and Poster Sessions,pages 177?180, Prague, Czech Republic, June.Association for Computational Linguistics.
[Ling et al.2013] Wang Ling, Guang Xiang, Chris Dyer,Alan Black, and Isabel Trancoso.
2013.
Microblogsas parallel corpora.
In Proceedings of the 51st An-nual Meeting on Association for Computational Lin-guistics, ACL ?13.
Association for ComputationalLinguistics.
[Liu et al.2011] Feifan Liu, Fei Liu, and Yang Liu.2011.
Learning from chinese-english parallel datafor chinese tense prediction.
In IJCNLP, pages1116?1124.
[Munteanu and Marcu2005] Dragos Munteanu andDaniel Marcu.
2005.
Improving machine transla-tion performance by exploiting comparable corpora.Computational Linguistics, 31(4):477?504.435[Ng et al.2003] Hwee Tou Ng, Bin Wang, and Yee SengChan.
2003.
Exploiting parallel texts for word sensedisambiguation: An empirical study.
In Proceedingsof ACL03, pages 455?462.
[Och2003] Franz Josef Och.
2003.
Minimum error ratetraining in statistical machine translation.
In Pro-ceedings of the 41st Annual Meeting on Associationfor Computational Linguistics - Volume 1, ACL ?03,pages 160?167, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.
[Papineni et al.2002] Kishore Papineni, Salim Roukos,Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: amethod for automatic evaluation of machine trans-lation.
In Proceedings of the 40th Annual Meetingon Association for Computational Linguistics, ACL?02, pages 311?318, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.
[Post et al.2012] Matt Post, Chris Callison-Burch, andMiles Osborne.
2012.
Constructing parallel cor-pora for six indian languages via crowdsourcing.
InProceedings of the Seventh Workshop on Statisti-cal Machine Translation, pages 401?409, Montr?al,Canada, June.
Association for Computational Lin-guistics.
[Resnik and Smith2003] Philip Resnik and Noah A.Smith.
2003.
The web as a parallel corpus.
Compu-tational Linguistics, 29:349?380.
[Schwarck et al.2010] Florian Schwarck, AlexanderFraser, and Hinrich Sch?tze.
2010.
Bitext-basedresolution of german subject-object ambiguities.
InHuman Language Technologies: The 2010 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics, HLT?10, pages 737?740, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.
[Smith et al.2010] Jason R. Smith, Chris Quirk, andKristina Toutanova.
2010.
Extracting parallel sen-tences from comparable corpora using documentlevel alignment.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, HLT ?10, pages 403?411, Stroudsburg, PA,USA.
Association for Computational Linguistics.
[Specia et al.2005] Lucia Specia, Maria Das Gra?as,Volpe Nunes, and Mark Stevenson.
2005.
Exploit-ing parallel texts to produce a multilingual sensetagged corpus for word sense disambiguation.
InProceedings of RANLP-05, Borovets, pages 525?531.
[Zaidan and Callison-Burch2011] Omar F. Zaidan andChris Callison-Burch.
2011.
Crowdsourcing trans-lation: professional quality from non-professionals.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies - Volume 1, HLT ?11, pages1220?1229, Stroudsburg, PA, USA.
Association forComputational Linguistics.436
