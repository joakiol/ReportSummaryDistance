Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1744?1753,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsDistilling an Ensemble of Greedy Dependency Parsers into One MST ParserAdhiguna Kuncoro?
Miguel Ballesteros?
Lingpeng Kong?Chris Dyer??
Noah A.
Smith?
?School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA?NLP Group, Pompeu Fabra University, Barcelona, Spain?Google DeepMind, London, UK?Computer Science & Engineering, University of Washington, Seattle, WA, USA{akuncoro,cdyer,lingpenk}@cs.cmu.edumiguel.ballesteros@upf.edu, nasmith@cs.washington.eduAbstractWe introduce two first-order graph-based de-pendency parsers achieving a new state of theart.
The first is a consensus parser built froman ensemble of independently trained greedyLSTM transition-based parsers with differentrandom initializations.
We cast this approachas minimum Bayes risk decoding (under theHamming cost) and argue that weaker con-sensus within the ensemble is a useful signalof difficulty or ambiguity.
The second parseris a ?distillation?
of the ensemble into a sin-gle model.
We train the distillation parserusing a structured hinge loss objective witha novel cost that incorporates ensemble un-certainty estimates for each possible attach-ment, thereby avoiding the intractable cross-entropy computations required by applyingstandard distillation objectives to problemswith structured outputs.
The first-order distil-lation parser matches or surpasses the state ofthe art on English, Chinese, and German.1 IntroductionNeural network dependency parsers achieve state ofthe art performance (Dyer et al, 2015; Weiss etal., 2015; Andor et al, 2016), but training them in-volves gradient descent on non-convex objectives,which is unstable with respect to initial parametervalues.
For some tasks, an ensemble of neural net-works from different random initializations has beenfound to improve performance over individual mod-els (Sutskever et al, 2014; Vinyals et al, 2015, in-ter alia).
In ?3, we apply this idea to build a first-order graph-based (FOG) ensemble parser (Sagaeand Lavie, 2006) that seeks consensus among 20randomly-initialized stack LSTM parsers (Dyer etal., 2015), achieving nearly the best-reported per-formance on the standard Penn Treebank Stanforddependencies task (94.51 UAS, 92.70 LAS).We give a probabilistic interpretation to the en-semble parser (with a minor modification), viewingit as an instance of minimum Bayes risk inference.We propose that disagreements among the ensem-ble?s members may be taken as a signal that an at-tachment decision is difficult or ambiguous.Ensemble parsing is not a practical solution, how-ever, since an ensemble of N parsers requires Ntimes as much computation, plus the runtime of find-ing consensus.
We address this issue in ?5 by distill-ing the ensemble into a single FOG parser with dis-criminative training by defining a new cost function,inspired by the notion of ?soft targets?
(Hinton et al,2015).
The essential idea is to derive the cost of eachpossible attachment from the ensemble?s division ofvotes, and use this cost in discriminative learning.The application of distilliation to structured predic-tion is, to our knowledge, new, as is the idea of em-pirically estimating cost functions.The distilled model performs almost as well asthe ensemble consensus and much better than (i)a strong LSTM FOG parser trained using the con-ventional Hamming cost function, (ii) recently pub-lished strong LSTM FOG parsers (Kiperwasser andGoldberg, 2016; Wang and Chang, 2016), and (iii)many higher-order graph-based parsers (Koo andCollins, 2010; Martins et al, 2013; Le and Zuidema,2014).
It represents a new state of the art for graph-based dependency parsing for English, Chinese, and1744German.
The code to reproduce our results is pub-licly available.12 Notation and DefinitionsLet x = ?x1, .
.
.
, xn?
denote an n-length sentence.A dependency parse for x, denoted y, is a set oftuples (h,m, `), where h is the index of a head, mthe index of a modifier, and ` a dependency label(or relation type).
Most dependency parsers are con-strained to return y that form a directed tree.A first-order graph-based (FOG; also known as?arc-factored?)
dependency parser exactly solvesy?
(x) = arg maxy?T (x)?(h,m)?ys(h,m,x)?
??
?S(y,x), (1)where T (x) is the set of directed trees over x, ands is a local scoring function that considers only asingle dependency arc at a time.
(We suppress de-pendency labels; there are various ways to incorpo-rate them, discussed later.)
To define s, McDonaldet al (2005a) used hand-engineered features of thesurrounding and in-between context of xh and xm;more recently, Kiperwasser and Goldberg (2016)used a bidirectional LSTM followed by a single hid-den layer with non-linearity.The exact solution to Eq.
1 can be found usinga minimum (directed) spanning tree algorithm (Mc-Donald et al, 2005b) or, under a projectivity con-straint, a dynamic programming algorithm (Eisner,1996), in O(n2) or O(n3) runtime, respectively.
Werefer to parsing with a minimum spanning tree algo-rithm as MST parsing.An alternative that runs in linear time istransition-based parsing, which recasts parsing asa sequence of actions that manipulate auxiliary datastructures to incrementally build a parse tree (Nivre,2003).
Such parsers can return a solution in a fasterO(n) asymptotic runtime.
Unlike FOG parsers,transition-based parsers allow the use of scoringfunctions with history-based features, so that attach-ment decisions can interact more freely; the best per-forming parser at the time of this writing employneural networks (Andor et al, 2016).1https://github.com/adhigunasurya/distillation_parser.gitLet hy(m) denote the parent of xm in y (using aspecial null symbol when m is the root of the tree),and hy?
(m) denotes the parent of xm in the pre-dicted tree y?.
Given two dependency parses of thesame sentence, y and y?, the Hamming cost isCH(y,y?)
=n?m=1{0 if hy(m) = hy?
(m)1 otherwiseThis cost underlies the standard dependency pars-ing evaluation scores (unlabeled and labeled attach-ment scores, henceforth UAS and LAS).
More gen-erally, a cost functionC maps pairs of parses for thesame sentence to non-negative values interpreted asthe cost of mistaking one for the other, and a first-order cost function (FOC) is one that decomposesby attachments, like the Hamming cost.Given a cost function C and a probabilistic modelthat defines p(y | x), minimum Bayes risk (MBR)decoding is defined byy?MBR(x) = arg miny?T (x)?y?
?T (x)p(y?
| x) ?
C(y,y?
)= arg miny?T (x)Ep(Y |x)[C(y,Y )].
(2)Under the Hamming cost, MBR parsing equates al-gorithmically to FOG parsing with s(h,m,x) =p((h,m) ?
Y | x), the posterior marginal of theattachment under p. This is shown by linearity ofexpectation; see also Titov and Henderson (2006).Apart from MBR decoding, cost functions arealso used for discriminative training of a parser.
Forexample, suppose we seek to estimate the param-eters ?
of scoring function S?.
One approach isto minimize the structured hinge loss of a trainingdataset D with respect to ?:min??(x,y)?D[?
S?
(y,x)+ maxy?
?T (x)(S?
(y?,x) + C(y?,y))](3)Intuitively, this amounts to finding parameters thatseparate the model score of the correct parse fromany wrong parse by a distance proportional to thecost of the wrong parse.
With regularization, this isequivalent to the structured support vector machine1745(Taskar et al, 2005; Tsochantaridis et al, 2005),and if S?
is (sub)differentiable, many algorithmsare available.
Variants have been used extensivelyin training graph-based parsers (McDonald et al,2005b; Martins et al, 2009), which typically makeuse of Hamming cost, so that the inner max can besolved efficiently using FOG parsing with a slightlyrevised local scoring function:s?
(h,m,x) = s(h,m,x) +{0 if (h,m) ?
y1 otherwise(4)Plugging this into Eq.
1 is known as cost-augmented parsing.3 Consensus and Minimum Bayes RiskDespite the recent success of neural network depen-dency parsers, most prior works exclusively reportsingle-model performance.
Ensembling neural net-work models trained from different random start-ing points is a standard technique in a variety ofproblems, such as machine translation (Sutskeveret al, 2014) and constituency parsing (Vinyals etal., 2015).
We aim to investigate the benefit of en-sembling independently trained neural network de-pendency parsers by applying the parser ensemblingmethod of Sagae and Lavie (2006) to a collection ofN strong neural network base parsers.Here, each base parser is an instance of thegreedy, transition-based parser of Dyer et al (2015),known as the stack LSTM parser, trained from adifferent random initial estimate.
Given a sen-tence x, the consensus FOG parser (Eq.
1) definesscore s(h,m,x) as the number of base parsers thatinclude the attachment (h,m), which we denotevotes(h,m).2 An example of this scoring functionwith an ensemble of 20 models is shown in Figure 1We assign to dependency (h,m) the label most fre-quently selected by the base parsers that attach m toh.Next, note that if we let s(h,m,x) =votes(h,m)/N , this has no effect on the parser (wehave only scaled by a constant factor).
We can there-fore view s as a posterior marginal, and the ensembleparser as an MBR parser (Eq.
2).2An alternative to building an ensemble of stack LSTMparsers in this way would be to average the softmax decisionsat each timestep (transition), similar to Vinyals et al (2015).John saw the woman with a telescope191Figure 1: Our ensemble?s votes (20 models) on an am-biguous PP attachment of with.
The ensemble is nearlybut not perfectly unanimous in selecting saw as the head.Model UAS LAS UEMAndor et al (2016) 94.61 92.79 -N = 1 (stack LSTM) 93.10 90.90 47.60ensemble, N = 5, MST 93.91 91.94 50.12ensemble, N = 10, MST 94.34 92.47 52.07ensemble, N = 15, MST 94.40 92.57 51.86ensemble, N = 20, MST 94.51 92.70 52.44Table 1: PTB-SD task: ensembles improve over a stronggreedy baseline.
UEM indicates unlabeled exact match.Experiment.
We consider this approach on theStanford dependencies version 3.3.0 (De Marneffeand Manning, 2008) Penn Treebank task.
As noted,the base parsers instantiate the greedy stack LSTMparser (Dyer et al, 2015).3Table 1 shows that ensembles, even with smallN , strongly outperform a single stack LSTM parser.Our ensembles of greedy, locally normalized parsersperform comparably to the best previously reported,due to Andor et al (2016), which uses a beam (width32) for training and decoding.4 What is Ensemble Uncertainty?While previous works have already demonstratedthe merit of ensembling in dependency parsing(Sagae and Lavie, 2006; Surdeanu and Manning,2010), usually with diverse base parsers, we con-sider whether the posterior marginals estimated byp?
((h,m) ?
Y | x) = votes(h,m)/N can be in-terpreted.
We conjecture that disagreement amongbase parsers about where to attach xm (i.e., uncer-tainty in the posterior) is a sign of difficulty or am-3We use the standard data split (02?21 for training, 22for development, 23 for test), automatically predicted part-of-speech tags, same pretrained word embedding as Dyer etal.
(2015), and recommended hyperparameters; https://github.com/clab/lstm-parser, each with a differentrandom initialization; this differs from past work on ensembles,which often uses different base model architectures.1746Sentence: It will go for work ranging from refinerymodification to changes in the distribution system,including the way service stations pump fuel intocars.xh posterior new cost Hamminggo 0.143 0.143 1work 0.095 0.191 1modification 0.190 0.096 1changes 0.286 0.000 0system 0.095 0.191 1pump 0.190 0.096 1stations 0.000 0.286 1Table 2: An ambiguous sentence from the training set andthe posteriors4 of various possible parents for including.The last two columns are, respectively, the contributionsto the distillation cost CD (explained in ?5.1, Eq.
5) andthe standard Hamming cost CH .
The most probable headunder the ensemble is changes, which is also the correctanswer.biguity.
If this is true, then the ensemble providesinformation about which confusions are more or lessreasonable?information we will exploit in our dis-tilled parser (?5).A complete linguistic study is out of scope here;instead, we provide a motivating example beforeempirically validating our conjecture.
Table 2 showsan example where there is considerable disagree-ment among base parsers over the attachment of aword (including).
We invite the reader to attempt toselect the correct attachment and gauge the difficultyof doing so, before reading on.Regardless of whether our intuition that this is aninherently difficult and perhaps ambiguous case iscorrect, it is uncontroversial to say that the wordsin the sentence not listed, which received zero votes(e.g., both instances of the), are obviously implausi-ble attachments.Our next idea is to transform ensemble uncer-tainty into a new estimate of cost?a replacement4In ?3, we used 20 models.
Since those 20 models weretrained on the whole training set, they cannot be used to obtainthe uncertainty estimates on the training set, where the examplesentence in Table 2 comes from.
Therefore we trained a newensemble of 21 models from scratch with five-way jackknifing.The same jackknifing setting is used in the distillation parser(?6).for the Hamming cost?and use it in discriminativetraining of a single FOG parser.
This allows us todistill what has been learned by the ensemble into asingle model.5 Distilling the EnsembleDespite its state of the art performance, our ensem-ble requires N parsing calls to decode each sen-tence.
To reduce the computational cost, we intro-duce a method for ?distilling?
the ensemble?s knowl-edge into a single parser, making use of a novel costfunction to communicate this knowledge from theensemble to the distilled model.
While models thatcombine the outputs of other parsing models havebeen proposed before (Martins et al, 2008; Nivreand McDonald, 2008; Zhang and Clark, 2008, in-ter alia), these works incorporated the scores or out-puts of the baseline parsers as features and as suchrequire running the first-stage models at test-time.Creating a cost function from a data analysis proce-dure is, to our knowledge, a new idea.The idea is attractive because cost functions aremodel-agnostic; they can be used with any parseramenable to discriminative training.
Further, onlythe training procedure changes; parsing at test timedoes not require consulting the ensemble at all,avoiding the costly application of the N parsers tonew data, unlike model combination techniques likestacking and beam search.Distilling an ensemble of classifiers into one sim-pler classifer that behaves similarly is due to Bucila?et al (2006) and Hinton et al (2015); they werelikewise motivated by a desire to create a simplermodel that was cheaper to run at test time.
In theirwork, the ensemble provides a probability distribu-tion over labels for each input, and this predicteddistribution serves as the training target for the dis-tilled model (a sum of two cross entropies objectiveis used, one targeting the empirical training distribu-tion and the other targeting the ensemble?s posteriordistribution).
This can be contrasted with the super-vision provided by the training data alone, whichconventionally provides a single correct label foreach instance.
These are respectively called ?soft?and ?hard?
targets.We propose a novel adaptation of the soft targetidea to the structured output case.
Since a sentence1747Sentence: John saw the woman with a telescopexh soft hardJohn 0.0 0saw 0.95 1the 0.0 0woman 0.05 0a 0.0 0telescope 0.0 0Table 3: Example of soft targets (taken from our 20-model ensemble?s uncertainty on the sentence) and hardtargets (taken from the gold standard) for possible parentsof with.
The soft target corresponds with the posterior(second column) in Table 2, but the hard target differsfrom the Hamming cost (last column of Table 2) sincethe hard target assigns a value of 1 to the correct answerand 0 to all others (the reverse is true for Hamming cost).has an exponential (in its length) number of parses,representing the posterior distribution over parsespredicted by the ensemble is nontrivial.
We solvethis problem by taking a single parse from eachmodel, representing the N -sized ensemble?s parsedistribution using N samples.Second, rather than considering uncertainty at thelevel of complete parse trees (which would be anal-ogous to the classification case) or larger structures,we instead consider uncertainty about individual at-tachments, and seek to ?soften?
the attachment tar-gets used in training the parser.
An illustrationfor the prepositional phrase attachment ambiguity inFig.
1, taken from the ensemble output for the sen-tence, is shown in Table 3.
Soft targets allow us toencode the notion that mistaking woman as the par-ent of with is less bad than attaching with to Johnor telescope.
Hard targets alone do not capture thisinformation.5.1 Distillation Cost FunctionThe natural place to exploit this additional informa-tion when training a parser is in the cost function.When incorporated into discriminative training, theHamming cost encodes hard targets: the correct at-tachment should receive a higher score than all in-correct ones, with the same margin.
Our distillationcost function aims to reduce the cost of decisionsthat?based on the ensemble uncertainty?appear tobe more difficult, or where there may be multipleplausible attachments.Let pi(h,m) =1?
p?
((h,m) ?
Y | x) = N ?
votes(h,m)N .Our new cost function is defined by CD(y,y?)
=?nm=1 max{0, pi(hy?(m),m)?
pi(hy(m),m)}=?nm=1 max{0, p?(hy(m),m)?
p?(hy?(m),m)}.
(5)Recall that y denotes the correct parse, accordingto the training data, and y?
is a candidate parse.This function has several attractive properties:1.
When a word xm has more than one plausi-ble (according to the ensemble) but incorrect(according to the annotations) attachment, eachone has a diminished cost (relative to Hammingcost and all implausible attachments).2.
The correct attachment (according to the gold-standard training data) always has zero costsince hy(m) = hy?
(m) and Eq.
5 cancels out.3.
When the ensemble is confident, cost for itschoice(s) is lower than it would be under Ham-ming cost?even when the ensemble is wrong.This means that we are largely training the dis-tilled parser to simulate the ensemble, includ-ing mistakes and correct predictions.
This en-courages the model to replicate the state of theart ensemble performance.4.
Further, when the ensemble is perfectly con-fident and correct, every incorrect attachmenthas a cost of 1, just as in Hamming cost.5.
The cost of any attachment is bounded aboveby the proportion of votes assigned to the cor-rect attachment.One way to understand this cost function is toimagine that it gives the parser more ways to achievea zero-cost5 attachment.
The first is to correctly at-tach a word to its correct parent.
The second isto predict a parent that the ensemble prefers to thecorrect parent, i.e., pi(hy?
(m),m) < pi(hy(m),m).Any other decision will incur a non-zero cost that is5It is important to note the difference between cost (Eq.
5)and loss (Eq.
3).1748proportional to the implausibility of the attachment,according to the ensemble.
Hence the model is su-pervised both by the hard targets in the training dataannotations and the soft targets from the ensemble.While it may seem counter-intuitive to place zerocost on an incorrect attachment, recall that the costis merely a margin that must separate the scoresof parses containing correct and incorrect arcs.
Incontrast, the loss (in our case, the structured hingeloss) is the ?penalty?
the learner tries to minimizewhile training the graph-based parser, which de-pends on both the cost and model score as definedin Equation 3.
When an incorrect arc is preferredby the ensemble over the gold arc (hence assigned acost/margin of 0), the model will still incur a lossif s(hy(m),m,x) < s(hy?(m),m,x).
In otherwords, the score of any incorrect arc (includingthose strongly preferred by the ensemble) cannot behigher than the score of the gold arc.The learner only incurs 0 loss ifs(hy(m),m,x) ?
s(hy?(m),m,x).
This meansthat the gold score and the predicted score can havea margin of 0 (i.e., have the same score and incur noloss) when the ensemble is highly confident of thatprediction, but the score of the correct parse cannotbe lower regardless of how confident the ensembleis (hence the objective does not encourage incorrecttrees at the expense of gold ones).In the example in Table 2, we show the (additive)contribution to the distillation cost by each attach-ment decision (column labeled ?new cost?).
Notethat more plausible attachments according to the en-semble have a lower cost than less plausible ones(e.g., the cost for modification is less than system,though both are incorrect).
While in the last line sta-tions received no votes in the ensemble (implausibleattachment), its contribution to the cost is boundedby the proportion of votes for correct attachment.The intuition is that, when the ensemble is not cer-tain of the correct answer, it should not assign a largecost to implausible attachments.
In contrast, Ham-ming cost would assign a cost of 1 (column labeled?Hamming?)
in all incorrect cases.5.2 Distilled ParserOur distilled parser is trained discriminatively withthe structured hinge loss (Eq.
3).
This is a naturalchoice because it makes the cost function explicitand central to learning.6 Further, because our en-semble?s posterior gives us information about eachattachment individually, the cost function we con-struct can be first-order, which simplifies trainingwith exact inference.This approach to training a model is well-studied for a FOG parser, but not for a transition-based parser, which is comprised of a collectionof classifiers trained to choose good sequences oftransitions?not to score whole trees for good at-tachment accuracy.
Transition-based approaches aretherefore unsuitable for our proposed distillationcost function, even though they are asymptoticallyfaster.
We proceed with a FOG parser (with Eis-ner?s algorithm for English and Chinese, and MSTfor German since it contains a considerable numberof non-projective trees) as the distilled model.Concretely, we use a bidirectional LSTM fol-lowed by a hidden layer of non-linearity to calculatethe scoring function s(h,m,x), following Kiper-wasser and Goldberg (2016) with minor modifica-tions.
The bidirectional LSTM maps each word xito a vector x?i that embeds the word in context (i.e.,x1:i?1 and xi+1:n).
Local attachment scores aregiven by:s(h,m,x) = v> tanh (W[x?h; x?m] + b) (6)where the model parameters are v, W, and b, plusthe bidirectional LSTM parameters.
We will refer tothis parsing model as neural FOG.Our model architecture is nearly identical to thatof Kiperwasser and Goldberg (2016), with two pri-mary differences.
The first difference is that we fixthe pretrained word embeddings and compose themwith learned embeddings and POS tag embeddings(Dyer et al, 2015), allowing the model to simulta-neously leverage pretrained vectors and learn a task-specific representation.7 Unlike Kiperwasser andGoldberg (2016), we did not observe any degrada-tion by incorporating the pretrained vectors.
Second,6Alternatives that do not use cost functions include proba-bilistic parsers, whether locally normalized like the stack LSTMparser used within our ensemble, or globally normalized, as inAndor et al (2016); cost functions can be incorporated in suchcases with minimum risk training (Smith and Eisner, 2006) orsoftmax margin (Gimpel and Smith, 2010).7To our understanding, Kiperwasser and Goldberg (2016)initialized with pretrained vectors and backpropagated duringtraining.1749we apply a per-epoch learning rate decay of 0.05 tothe Adam optimizer.
While the Adam optimizer au-tomatically adjusts the global learning rate accord-ing to past gradient magnitudes, we find that this ad-ditional per-epoch decay consistently improves per-formance across all settings and languages.6 ExperimentsWe ran experiments on the English PTB-SD version3.3.0, Penn Chinese Treebank (Xue et al, 2002), andGerman CoNLL 2009 (Hajic?
et al, 2009) tasks.Experimental settings.
We used the standardsplits for all languages.
Like Chen and Manning(2014) and Dyer et al (2015), we use predicted tagswith the Stanford tagger (Toutanova et al, 2003)for English and gold tags for Chinese.
For Germanwe use the predicted tags provided by the CoNLL2009 shared task organizers.
All models were aug-mented with pretrained structured-skipgram (Ling etal., 2015) embeddings; for English we used the Gi-gaword corpus and 100 dimensions, for Chinese Gi-gaword and 80, and for German WMT 2010 mono-lingual data and 64.Hyperparameters.
The hyperparameters forneural FOG are summarized in Table 4.
For theAdam optimizer we use the default settings in theCNN neural network library.8 Since the ensembleis used to obtain the uncertainty on the training set,it is imperative that the stack LSTMs do not overfitthe training set.
To address this issue, we performedfive-way jackknifing of the training data for eachstack LSTM model to obtain the training data uncer-tainty under the ensemble.
To obtain the ensembleuncertainty on each language, we use 21 base mod-els for English (see footnote 4), 17 for Chinese, and11 for German.Speed.
One potential drawback of using aquadratic or cubic time parser to distill an ensembleof linear-time transition-based models is speed.
OurFOG model is implemented using the same CNN li-brary as the stack LSTM transition-based parser.
Onthe same single-thread CPU hardware, the distilledMST parser9 parses 20 sentences per second with-out any pruning, while a single stack LSTM model8https://github.com/clab/cnn.git9The runtime of the Hamming-cost bidirectional LSTMFOG parser is the same as the distilled parser.Bi-LSTM dimension 100Bi-LSTM layers 2POS tag embedding 12Learned word embedding 32Hidden Layer Units 100Labeler Hiden Layer Units 100Optimizer AdamLearning rate decay 0.05Table 4: Hyperparameters for the distilled FOG parser.Both the model architecture and the hyperparameters arenearly identical with Kiperwasser and Goldberg (2016).We apply a per-epoch learning rate decay to the Adam op-timizer, which consistently improves performance acrossall datasets.is only three times faster at 60 sentences per second.Running an ensemble of 20 stack LSTMs is at least20 times slower (without multi-threading), not in-cluding consensus parsing.
In the end, the distilledparser is more than ten times faster than the ensem-ble pipeline.Accuracy.
All scores are shown in Table 5.
First,consider the neural FOG parser trained with Ham-ming cost (CH in the second-to-last row).
This is avery strong benchmark, outperforming many higher-order graph-based and neural network models on allthree datasets.
Nonetheless, training the same modelwith distillation cost gives consistent improvementsfor all languages.
For English, we see that thismodel comes close to the slower ensemble it wastrained to simulate.
For Chinese, it achieves thebest published scores, for German the best publishedUAS scores, and just after Bohnet and Nivre (2012)for LAS.Effects of Pre-trained Word Embedding.
Asan ablation study, we ran experiments on Englishwithout pre-trained word embedding, both with theHamming and distillation costs.
The model trainedwith Hamming cost achieved 93.1 UAS and 90.9LAS, compared to 93.6 UAS and 91.1 LAS forthe model with distillation cost.
This result furthershowcases the consistent improvements from usingthe distillation cost across different settings and lan-guages.We conclude that ?soft targets?
derived from en-semble uncertainty offer useful guidance, throughthe distillation cost function and discriminativetraining of a graph-based parser.
Here we consid-1750System Method P?
PTB-SD CTBGermanCoNLL?09UAS LAS UAS LAS UAS LASZhang and Nivre (2011) Transition (beam) - - 86.0 84.4 - -Bohnet and Nivre (2012)?
Transition (beam) - - 87.3 85.9 91.37 89.38Chen and Manning (2014) Transition (greedy) X 91.8 89.6 83.9 82.4 - -Dyer et al (2015) Transition (greedy) X 93.1 90.9 87.2 85.7 - -Weiss et al (2015) Transition (beam) X 94.0 92.0 - - - -Yazdani and Henderson (2015) Transition (beam) - - - - 89.6 86.0Ballesteros et al (2015) Transition (greedy) 91.63 89.44 85.30 83.72 88.83 86.10Ballesteros et al (2016) Transition (greedy) X 93.56 91.42 87.65 86.21 - -Kiperwasser and Goldberg (2016) Transition (greedy) X 93.9 91.9 87.6 86.1 - -Andor et al (2016) Transition (beam) X 94.61 92.79 - - 90.91 89.15Ma and Zhao (2012) Graph (4th order) - - 87.74 - - -Martins et al (2013) Graph (3rd order) 93.1 - - - - -Le and Zuidema (2014) Reranking/blend X 93.8 91.5 - - - -Zhu et al (2015) Reranking/blend X - - 85.7 - - -Kiperwasser and Goldberg (2016) Graph (1st order) 93.1 91.0 86.6 85.1 - -Wang and Chang (2016) Graph (1st order) X 94.08 91.82 87.55 86.23 - -This work: ensemble, N = 20, MST Transition (greedy) X 94.51 92.70 89.80 88.56 91.86 89.98This work: neural FOG, CH Graph (1st order) X 93.76 91.60 87.32 85.82 91.22 88.82This work: neural FOG, CD (distilled) Graph (1st order) X 94.26 92.06 88.87 87.30 91.60 89.24Table 5: Dependency parsing performance on English, Chinese, and German tasks.
The ?P??
column indicates the useof pretrained word embeddings.
Reranking/blend indicates that the reranker score is interpolated with the base model?sscore.
Note that previous works might use different predicted tags for English.
We report accuracy without punctuationfor English and Chinese, and with punctuation for German, using the standard evaluation script in each case.
We onlyconsider systems that do not use additional training data.
The best overall results are indicated with bold (this wasachieved by the ensemble of greedy stack LSTMs in Chinese and German), while the best non-ensemble model isdenoted with an underline.
The ?
sign indicates the use of predicted tags for Chinese in the original publication,although we report accuracy using gold Chinese tags based on private correspondence with the authors.ered a FOG parser, though future work might inves-tigate any parser amenable to training to minimize acost-aware loss like the structured hinge.7 Related WorkOur work on ensembling dependency parsers isbased on Sagae and Lavie (2006) and Surdeanu andManning (2010); an additional contribution of thiswork is to show that the normalized ensemble votescorrespond to MBR parsing.
Petrov (2010) pro-posed a similar model combination with random ini-tializations for phrase-structure parsing, using prod-ucts of constituent marginals.
The local optima inhis base model?s training objective arise from latentvariables instead of neural networks (in our case).Model distillation was proposed by Bucila?
et al(2006), who used a single neural network to simu-late a large ensemble of classifiers.
More recently,Ba and Caruana (2014) showed that a single shal-low neural network can closely replicate the per-formance of an ensemble of deep neural networksin phoneme recognition and object detection.
Ourwork is closer to Hinton et al (2015), in the sensethat we do not simply compress the ensemble andhit the ?soft target,?
but also the ?hard target?
at thesame time10.
These previous works only used modelcompression and distillation for classification; weextend the work to a structured prediction problem(dependency parsing).Ta?ckstro?m et al (2013) similarly used an ensem-ble of other parsers to guide the prediction of a seedmodel, though in a different context of ?ambiguity-aware?
ensemble training to re-lexicalize a trans-fer model for a target language.
We similarly usean ensemble of models as a supervision for a sin-10Our cost is zero when the correct arc is predicted, regard-less of what the soft target thinks, something a compressionmodel without gold supervision cannot do.1751gle model.
By incorporating the ensemble uncer-tainty estimates in the cost function, our approachis cheaper, not requiring any marginalization duringtraining.
An additional difference is that we learnfrom the gold labels (?hard targets?)
rather than onlyensemble estimates on unlabeled data.Kim and Rush (2016) proposed a distillationmodel at the sequence level, with application insequence-to-sequence neural machine translation.There are two primary differences with this work.First, we use a global model to distill the ensemble,instead of a sequential one.
Second, Kim and Rush(2016) aim to distill a larger model into a smallerone, while we propose to distill an ensemble insteadof a single model.8 ConclusionsWe demonstrate that an ensemble of 20 greedy stackLSTMs (Dyer et al, 2015) can achieve state of theart accuracy on English dependency parsing.
Thisapproach corresponds to minimum Bayes risk de-coding, and we conjecture that the arc attachmentposterior marginals quantify a notion of uncertaintythat may indicate difficulty or ambiguity.
Since run-ning an ensemble is computationally expensive, weproposed discriminative training of a graph-basedmodel with a novel cost function that distills the en-semble uncertainty.
Deriving a cost function from astatistical model and extending distillation to struc-tured prediction are new contributions.
This dis-tilled model, trained to simulate the slower ensembleparser, improves over the state of the art on Chineseand German.AcknowledgmentsWe thank Swabha Swayamdipta, Sam Thomson,Jesse Dodge, Dallas Card, Yuichiro Sawai, Gra-ham Neubig, and the anonymous reviewers for use-ful feedback.
We also thank Juntao Yu and BerndBohnet for re-running the parser of Bohnet andNivre (2012) on Chinese with gold tags.
This workwas sponsored in part by the Defense Advanced Re-search Projects Agency (DARPA) Information Inno-vation Office (I2O) under the Low Resource Lan-guages for Emergent Incidents (LORELEI) programissued by DARPA/I2O under Contract No.
HR0011-15-C-0114; it was also supported in part by ContractNo.
W911NF-15-1-0543 with the DARPA and theArmy Research Office (ARO).
Approved for publicrelease, distribution unlimited.
The views expressedare those of the authors and do not reflect the of-ficial policy or position of the Department of De-fense or the U.S. Government.
Miguel Ballesteroswas supported by the European Commission un-der the contract numbers FP7-ICT-610411 (projectMULTISENSOR) and H2020-RIA-645012 (projectKRISTINA).ReferencesDaniel Andor, Chris Alberti, David Weiss, AliakseiSeveryn, Alessandro Presta, Kuzman Ganchev, SlavPetrov, and Michael Collins.
2016.
Globally nor-malized transition-based neural networks.
In Proc.
ofACL.Jimmy Ba and Rich Caruana.
2014.
Do deep nets reallyneed to be deep?
In Proc.
of NIPS.Miguel Ballesteros, Chris Dyer, and Noah A. Smith.2015.
Improved transition-based parsing by modelingcharacters instead of words with LSTMs.
In Proc.
ofEMNLP.Miguel Ballesteros, Yoav Goldberg, Chris Dyer, andNoah A. Smith.
2016.
Training with explorationimproves a greedy stack-LSTM parser.
In Proc.
ofEMNLP.Bernd Bohnet and Joakim Nivre.
2012.
A transition-based system for joint part-of-speech tagging and la-beled non-projective dependency parsing.
In Proc.
ofEMNLP-CoNLL.Cristian Bucila?, Rich Caruana, and Alexandru Niculescu-Mizil.
2006.
Model compression.
In Proc.
of KDD.Danqi Chen and Christopher Manning.
2014.
A fast andaccurate dependency parser using neural networks.
InProc.
of EMNLP.Marie-Catherine De Marneffe and Christopher D. Man-ning.
2008.
Stanford typed dependencies manual.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A. Smith.
2015.
Transition-based dependency parsing with stack long short-termmemory.
In Proc.
of ACL.Jason M. Eisner.
1996.
Three new probabilistic modelsfor dependency parsing: An exploration.
In Proc.
ofCOLING.Kevin Gimpel and Noah A Smith.
2010.
Softmax-margin CRFs: Training log-linear models with costfunctions.
In Proc.
of NAACL.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, Sebastian1752Pado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 Shared Task: Syntactic and semantic dependen-cies in multiple languages.
In Proc.
of CONLL 2009Shared Task.Geoffrey E. Hinton, Oriol Vinyals, and Jeffrey Dean.2015.
Distilling the knowledge in a neural network.CoRR, abs/1503.02531.Yoon Kim and Alexander M. Rush.
2016.
Sequence-level knowledge distillation.
In Proc.
of EMNLP.Eliyahu Kiperwasser and Yoav Goldberg.
2016.
Simpleand accurate dependency parsing using bidirectionalLSTM feature representations.
TACL.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proc.
of ACL.Phong Le and Willem Zuidema.
2014.
The inside-outside recursive neural network model for depen-dency parsing.
In Proc.
of EMNLP.Wang Ling, Chris Dyer, Alan W Black, and Isabel Tran-coso.
2015.
Two/too simple adaptations of word2vecfor syntax problems.
In Proc.
of NAACL.Xuezhe Ma and Hai Zhao.
2012.
Fourth-order depen-dency parsing.
In Proc.
of COLING.Andre?
F. T. Martins, Dipanjan Das, Noah A. Smith, andEric P. Xing.
2008.
Stacking dependency parsers.
InProc.
of EMNLP.Andre?
F. T. Martins, Noah A. Smith, and Eric P. Xing.2009.
Polyhedral outer approximations with applica-tion to natural language parsing.
In Proc.
of ICML.Andre?
F. T. Martins, Mariana S.C. Almeida, and Noah A.Smith.
2013.
Turning on the turbo: Fast third-ordernon-projective turbo parsers.
In Proc.
of ACL.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005a.
Online large-margin training of dependencyparsers.
In Proc.
of ACL.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005b.
Non-projective dependency parsingusing spanning tree algorithms.
In Proc.
of EMNLP.Joakim Nivre and Ryan McDonald.
2008.
Integratinggraph-based and transition-based dependency parsers.In Proc.
of ACL.Joakim Nivre.
2003.
An efficient algorithm for projec-tive dependency parsing.
In Proc.
of IWPT.Slav Petrov.
2010.
Products of random latent variablegrammars.
In Proc.of NAACL.Kenji Sagae and Alon Lavie.
2006.
Parser combinationby reparsing.
In Proc.
of NAACL.David A. Smith and Jason Eisner.
2006.
Minimum riskannealing for training log-linear models.
In Proc.
ofACL.Mihai Surdeanu and Christopher D. Manning.
2010.
En-semble models for dependency parsing: Cheap andgood?
In Proc.
of NAACL.Ilya Sutskever, Oriol Vinyals, and Quoc V. Le.
2014.Sequence to sequence learning with neural networks.In Proc.
of NIPS.Oscar Ta?ckstro?m, Ryan T. McDonald, and Joakim Nivre.2013.
Target language adaptation of discriminativetransfer parsers.
In Proc.
of NAACL.Ben Taskar, Vassil Chatalbashev, Daphne Koller, andCarlos Guestrin.
2005.
Learning structured predictionmodels: A large margin approach.
In Proc.
of ICML.Ivan Titov and James Henderson.
2006.
Bayes risk min-imization in natural language parsing.
Technical re-port, University of Geneva.Kristina Toutanova, Dan Klein, Christopher D. Manning,and Yoram Singer.
2003.
Feature-rich part-of-speechtagging with a cyclic dependency network.
In Proc.
ofNAACL.Ioannis Tsochantaridis, Thorsten Joachims, Thomas Hof-mann, and Yasemin Altun.
2005.
Large margin meth-ods for structured and interdependent output variables.JMLR.Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2015.
Grammaras a foreign language.
In Proc.
of NIPS.Wenhui Wang and Baobao Chang.
2016.
Graph-baseddependency parsing with bidirectional LSTM.
InProc.
of ACL.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural networktransition-based parsing.
In Proc.
of ACL.Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.2002.
Building a large-scale annotated chinese cor-pus.
In Proc.
of COLING.Majid Yazdani and James Henderson.
2015.
Incre-mental recurrent neural network dependency parserwith search-based discriminative training.
In Proc.
ofCoNLL.Yue Zhang and Stephen Clark.
2008.
A tale oftwo parsers: Investigating and combining graph-basedand transition-based dependency parsing using beam-search.
In Proc.
of EMNLP.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProc.
of ACL.Chenxi Zhu, Xipeng Qiu, Xinchi Chen, and XuanjingHuang.
2015.
A re-ranking model for dependencyparser with recursive convolutional neural network.
InProc.
of ACL-IJCNLP.1753
