Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 890?895,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA New Set of Norms for Semantic Relatedness MeasuresSean SzumlanskiDepartment of EECSUniversity of Central Floridaseansz@cs.ucf.eduFernando GomezDepartment of EECSUniversity of Central Floridagomez@eecs.ucf.eduValerie K. SimsDepartment of PsychologyUniversity of Central FloridaValerie.Sims@ucf.eduAbstractWe have elicited human quantitative judg-ments of semantic relatedness for 122pairs of nouns and compiled them into anew set of relatedness norms that we callRel-122.
Judgments from individual sub-jects in our study exhibit high average cor-relation to the resulting relatedness means(r = 0.77, ?
= 0.09, N = 73), although notas high as Resnik?s (1995) upper boundfor expected average human correlation tosimilarity means (r = 0.90).
This suggeststhat human perceptions of relatedness areless strictly constrained than perceptionsof similarity and establishes a clearer ex-pectation for what constitutes human-likeperformance by a computational measureof semantic relatedness.We compare the results of severalWordNet-based similarity and relatednessmeasures to our Rel-122 norms anddemonstrate the limitations of WordNetfor discovering general indications ofsemantic relatedness.
We also offer a cri-tique of the field?s reliance upon similaritynorms to evaluate relatedness measures.1 IntroductionDespite the well-established technical distinc-tion between semantic similarity and relatedness(Agirre et al 2009; Budanitsky and Hirst, 2006;Resnik, 1995), comparison to established similar-ity norms from psychology remains part of thestandard evaluative procedure for assessing com-putational measures of semantic relatedness.
Be-cause similarity is only one particular type of re-latedness, comparison to similarity norms fails togive a complete view of a relatedness measure?sefficacy.In keeping with Budanitsky and Hirst?s (2006)observation that ?comparison with human judg-ments is the ideal way to evaluate a measure ofsimilarity or relatedness,?
we have undertaken thecreation of a new set of relatedness norms.2 BackgroundThe similarity norms of Rubenstein and Goode-nough (1965; henceforth R&G) and Miller andCharles (1991; henceforth M&C) have seen ubiq-uitous use in evaluation of computational mea-sures of semantic similarity and relatedness.R&G established their similarity norms by pre-senting subjects with 65 slips of paper, each ofwhich contained a pair of nouns.
Subjects weredirected to read through all 65 noun pairs, thensort the pairs ?according to amount of ?similarityof meaning.??
Subjects then assigned similarityscores to each pair on a scale of 0.0 (completelydissimilar) to 4.0 (strongly synonymous).The R&G results have proven to be highly repli-cable.
M&C repeated R&G?s study using a subsetof 30 of the original word pairs, and their resultingsimilarity norms correlated to the R&G norms atr = 0.97.
Resnik?s (1995) subsequent replicationof M&C?s study similarly yielded a correlation ofr = 0.96.
The M&C pairs were also included in asimilarity study by Finkelstein et al(2002), whichyielded correlation of r = 0.95 to the M&C norms.2.1 WordSim353WordSim353 (Finkelstein et al 2002) has re-cently emerged as a potential surrogate dataset forevaluating relatedness measures.
Several studieshave reported correlation to WordSim353 normsas part of their evaluation procedures, with somestudies explicitly referring to it as a collection ofhuman-assigned relatedness scores (Gabrilovichand Markovitch, 2007; Hughes and Ramage,2007; Milne and Witten, 2008).890Yet, the instructions presented to Finkelstein etal.
?s subjects give us pause to reconsider Word-Sim353?s classification as a set of relatednessnorms.
They repeatedly framed the task as one inwhich subjects were expected to assign word simi-larity scores, although participants were instructedto extend their definition of similarity to includeantonymy, which perhaps explains why the au-thors later referred to their data as ?relatedness?norms rather than merely ?similarity?
norms.Jarmasz and Szpakowicz (2003) have raised fur-ther methodological concerns about the construc-tion of WordSim353, including: (a) similarity wasrated on a scale of 0.0 to 10.0, which is intrin-sically more difficult for humans to manage thanthe scale of 0.0 to 4.0 used by R&G and M&C,and (b) the inclusion of proper nouns introducedan element of cultural bias into the dataset (e.g.,the evaluation of the pair Arafat?terror).Cognizant of the problematic conflation of sim-ilarity and relatedness in WordSim353, Agirre etal.
(2009) partitioned the data into two sets:one containing noun pairs exhibiting similarity,and one containing pairs of related but dissimilarnouns.
However, pairs in the latter set were notassessed for scoring distribution validity to ensurethat strongly related word pairs were not penalizedby human subjects for being dissimilar.13 MethodologyIn our experiments, we elicited human ratings ofsemantic relatedness for 122 noun pairs.
In doingso, we followed the methodology of Rubensteinand Goodenough (1965) as closely as possible:participants were instructed to read through a setof noun pairs, sort them by how strongly relatedthey were, and then assign each pair a relatednessscore on a scale of 0.0 (?completely unrelated?)
to4.0 (?very strongly related?
).We made two notable modifications to the ex-perimental procedure of Rubenstein and Goode-nough.
First, instead of asking participants tojudge ?amount of ?similarity of meaning,??
weasked them to judge ?how closely related in mean-ing?
each pair of nouns was.
Second, we used aWeb interface to collect data in our study; insteadof reordering a deck of cards, participants werepresented with a grid of cards that they were able1Perhaps not surprisingly, the highest scores in Word-Sim353 (all ratings from 9.0 to 10.0) were assigned to pairsthat Agirre et alplaced in their similarity partition.to rearrange interactively with the use of a mouseor any touch-enabled device, such as a tablet PC.23.1 Experimental ConditionsEach participant in our study was randomly as-signed to one of four conditions.
Each conditioncontained 32 noun pairs for evaluation.Of those pairs, 10 were randomly selectedfrom from WordNet++ (Ponzetto and Navigli,2010) and 10 from SGN (Szumlanski and Gomez,2010)?two semantic networks that categori-cally indicate strong relatedness between Word-Net noun senses.
10 additional pairs were gen-erated by randomly pairing words from a list ofall nouns occurring in Wikipedia.
The nounsin the pairs we used from each of these threesources were matched for frequency of occurrencein Wikipedia.We manually selected two additional pairs thatappeared across all four conditions: leaves?rakeand lion?cage.
These control pairs were includedto ensure that each condition contained examplesof strong semantic relatedness, and potentially tohelp identify and eliminate data from participantswho assigned random relatedness scores.
Withineach condition, the 32 word pairs were presentedto all subjects in the same random order.
Acrossconditions, the two control pairs were always pre-sented in the same positions in the word pair grid.Each word pair was subjected to additionalscrutiny before being included in our dataset.
Weeliminated any pairs falling into one or moreof the following categories: (a) pairs containingproper nouns, (b) pairs in which one or both nounsmight easily be mistaken for adjectives or verbs,(c) pairs with advanced vocabulary or words thatmight require domain-specific knowledge in or-der to be properly evaluated, and (d) pairs withshared stems or common head nouns (e.g., firstcousin?second cousin and sinner?sinning).
Thelatter were eliminated to prevent subjects fromlatching onto superficial lexical commonalities asindicators of strong semantic relatedness withoutreflecting upon meaning.3.2 ParticipantsParticipants in our study were recruited from in-troductory undergraduate courses in psychologyand computer science at the University of Cen-tral Florida.
Students from the psychology courses2Online demo: http://www.cs.ucf.edu/?seansz/rel-122891participated for course credit and accounted for89% of respondents.92 participants provided data for our study.
Ofthese, we identified 19 as outliers, and their datawere excluded from our norms to prevent interfer-ence from individuals who appeared to be assign-ing random scores to noun pairs.
We consideredan outlier to be any individual whose numeric rat-ings fell outside two standard deviations from themeans for more than 10% of the word pairs theyevaluated (i.e., at least four word pairs, since eachcondition contained 32 word pairs).For outlier detection, means and standard de-viations were computed using leave-one-out sam-pling.
That is, data from individual J were not in-corporated into means or standard deviations whenconsidering whether to eliminate J as an outlier.3Of the 73 participants remaining after outlierelimination, there was a near-even split betweenmales (37) and females (35), with one individualdeclining to provide any demographic data.
Theaverage age of participants was 20.32 (?
= 4.08,N = 72).
Most students were freshmen (49), fol-lowed in frequency by sophomores (16), seniors(4), and juniors (3).
Participants earned an averagescore of 42% on a standardized test of advancedvocabulary (?
= 16%, N = 72) (Test I ?
V-4 fromEkstrom et al(1976)).4 ResultsEach word pair in Rel-122 was evaluated by atleast 20 human subjects.
After outlier removal(described above), each word pair retained eval-uations from 14 to 22 individuals.
The resultingrelatedness means are available online.4An excerpt of the Rel-122 norms is shown inTable 1.
We note that the highest rated pairs in ourdataset are not strictly similar entities; exactly halfof the 10 most strongly related nouns in Table 1 aredissimilar (e.g., digital camera?photographer).Judgments from individual subjects in our studyexhibited high average correlation to the elicitedrelatedness means (r = 0.769, ?
= 0.09, N =73).
Resnik (1995), in his replication of the3We used this sampling method to prevent extreme out-liers from masking their own aberration during outlier de-tection, which is potentially problematic when dealing withsmall populations.
Without leave-one-out-sampling, wewould have identified fewer outliers (14 instead of 19), butthe resulting means would still have correlated strongly toour final relatedness norms (r = 0.991, p < 0.01).4http://www.cs.ucf.edu/?seansz/rel-122# Word Pair ?1.
underwear lingerie 3.942. digital camera photographer 3.853. tuition fee 3.854. leaves rake 3.825. symptom fever 3.796. fertility ovary 3.787. beef slaughterhouse 3.788. broadcast commentator 3.759. apparel jewellery 3.7210. arrest detention 3.69. .
.122. gladiator plastic bag 0.13Table 1: Excerpt of Rel-122 norms.M&C study, reported average individual correla-tion of r = 0.90 (?
= 0.07, N = 10) to similar-ity means elicited from a population of 10 gradu-ate students and postdoctoral researchers.
Presum-ably Resnik?s subjects had advanced knowledge ofwhat constitutes semantic similarity, as he estab-lished r = 0.90 as an upper bound for expectedhuman correlation on that task.The fact that average human correlation in ourstudy is weaker than in previous studies suggeststhat human perceptions of relatedness are lessstrictly constrained than perceptions of similarity,and that a reasonable computational measure of re-latedness might only approach a correlation of r =0.769 to relatedness norms.In Table 2, we present the performance of a va-riety of relatedness and similarity measures on ournew set of relatedness means.5 Coefficients of cor-relation are given for Pearson?s product-momentcorrelation (r), as well as Spearman?s rank corre-lation (?).
For comparison, we include results forthe correlation of these measures to the M&C andR&G similarity means.The generally weak performance of theWordNet-based measures on this task is notsurprising, given WordNet?s strong dispositiontoward codifying semantic similarity, whichmakes it an impoverished resource for discoveringgeneral semantic relatedness.
We note that thethree WordNet-based measures from Table 2that are regarded in the literature as relatednessmeasures (Banerjee and Pedersen, 2003; Hirst andSt-Onge, 1998; Patwardhan and Pedersen, 2006)5Results based on standard implementations in the Word-Net::Similarity Perl module of Pedersen et al(2004) (v2.05).892Rel-122 M&C R&GMeasure r ?
r ?
r ?
* Szumlanski and Gomez (2010) 0.654 0.534 0.852 0.859 0.824 0.841* Patwardhan and Pedersen (2006) 0.341 0.364 0.865 0.906 0.793 0.795Path Length 0.225 0.183 0.755 0.715 0.784 0.783* Banerjee and Pedersen (2003) 0.210 0.258 0.356 0.804 0.340 0.718Resnik (1995) 0.203 0.182 0.806 0.741 0.822 0.757Jiang and Conrath (1997) 0.188 0.133 0.473 0.663 0.575 0.592Leacock and Chodorow (1998) 0.173 0.167 0.779 0.715 0.839 0.783Wu and Palmer (1994) 0.187 0.180 0.764 0.732 0.797 0.768Lin (1998) 0.145 0.148 0.739 0.687 0.726 0.636* Hirst and St-Onge (1998) 0.141 0.160 0.667 0.782 0.726 0.797Table 2: Correlation of similarity and relatedness measures to Rel-122, M&C, and R&G.
Starred rows(*) are considered relatedness measures.
All measures are WordNet-based, except for the scoring metricof Szumlanski and Gomez (2010), which is based on lexical co-occurrence frequency in Wikipedia.# Noun Pair Sim.
Rel.
# Noun Pair Sim.
Rel.1.
car automobile 3.92 4.00 16. lad brother 1.66 2.682. gem jewel 3.84 3.98 17. journey car 1.16 3.003. journey voyage 3.84 3.97 18. monk oracle 1.10 2.544. boy lad 3.76 3.97 19. cemetery woodland 0.95 1.695. coast shore 3.70 3.97 20. food rooster 0.89 2.596. asylum madhouse 3.61 3.91 21. coast hill 0.87 1.597. magician wizard 3.50 3.58 22. forest graveyard 0.84 2.018. midday noon 3.42 4.00 23. shore woodland 0.63 1.639. furnace stove 3.11 3.67 24. monk slave 0.55 1.3110. food fruit 3.08 3.91 25. coast forest 0.42 1.8911. bird cock 3.05 3.71 26. lad wizard 0.42 2.1212. bird crane 2.97 3.96 27. chord smile 0.13 0.6813. tool implement 2.95 2.86 28. glass magician 0.11 1.3014. brother monk 2.82 2.89 29. rooster voyage 0.08 0.6315. crane implement 1.68 0.90 30. noon string 0.08 0.14Table 3: Comparison of relatedness means to M&C similarity means.
Correlation is r = 0.91.have been hampered by their reliance upon Word-Net.
The disparity between their performance onRel-122 and the M&C and R&G norms suggeststhe shortcomings of using similarity norms forevaluating measures of relatedness.5 (Re-)Evaluating Similarity NormsAfter establishing our relatedness norms, we cre-ated two additional experimental conditions inwhich subjects evaluated the relatedness of nounpairs from the M&C study.
Each condition againhad 32 noun pairs: 15 from M&C and 17 fromRel-122.
Pairs from M&C and Rel-122 were uni-formly distributed between these two new condi-tions based on matched normative similarity or re-latedness scores from their respective datasets.Results from this second phase of our study areshown in Table 3.
The correlation of our relat-edness means on this set to the similarity meansof M&C was strong (r = 0.91), but not as strongas in replications of the study that asked subjectsto evaluate similarity (e.g.
r = 0.96 in Resnik?s(1995) replication and r = 0.95 in Finkelstein etal.
?s (2002) M&C subset).That the synonymous M&C pairs garner highrelatedness ratings in our study is not surprising;strong similarity is, after all, one type of strongrelatedness.
The more interesting result from893our study, shown in Table 3, is that relatednessnorms for pairs that are related but dissimilar (e.g.,journey?car and forest?graveyard) deviate signif-icantly from established similarity norms.
This in-dicates that asking subjects to evaluate ?similar-ity?
instead of ?relatedness?
can significantly im-pact the norms established in such studies.6 ConclusionsWe have established a new set of relatednessnorms, Rel-122, that is offered as a supplementaryevaluative standard for assessing semantic related-ness measures.We have also demonstrated the shortcomingsof using similarity norms to evaluate such mea-sures.
Namely, since similarity is only one type ofrelatedness, comparison to similarity norms failsto provide a complete view of a measure?s abil-ity to capture more general types of relatedness.This is particularly problematic when evaluatingWordNet-based measures, which naturally excel atcapturing similarity, given the nature of the Word-Net ontology.Furthermore, we have found that asking judgesto evaluate ?relatedness?
of terms, rather than?similarity,?
has a substantive impact on resultingnorms, particularly with respect to the M&C sim-ilarity dataset.
Correlation of individual judges?ratings to resulting means was also significantlylower on average in our study than in previousstudies that focused on similarity (e.g., Resnik,1995).
These results suggest that human percep-tions of relatedness are less strictly constrainedthan perceptions of similarity and validate theneed for new relatedness norms to supplement ex-isting gold standard similarity norms in the evalu-ation of relatedness measures.ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and WordNet-based approaches.
In Proceed-ings of the North American Chapter of the Associa-tion for Computational Linguistics (NAACL), pages19?27.Satanjeev Banerjee and Ted Pedersen.
2003.
Ex-tended gloss overlaps as a measure of semantic re-latedness.
In Proceedings of the 18th InternationalJoint Conference on Artificial Intelligence (IJCAI),pages 805?810.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating WordNet-based measures of lexical semanticrelatedness.
Computational Linguistics, 32(1):13?47.Ruth B. Ekstrom, John W. French, Harry H. Harman,and Diran Dermen.
1976.
Manual for Kit of Factor-Referenced Cognitive Tests.
Educational TestingService, Princeton, NJ.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: Theconcept revisited.
ACM Transactions on Informa-tion Systems (TOIS), 20(1):116?131.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using Wikipedia-based explicit semantic analysis.
In Proceedings ofthe 20th International Joint Conference on ArtificialIntelligence, pages 1606?1611.Graeme Hirst and David St-Onge.
1998.
Lexicalchains as representations of context for the detec-tion and correction of malapropisms.
In ChristianeFellbaum, editor, WordNet: An Electronic LexicalDatabase, pages 305?332.
MIT Press.Thad Hughes and Daniel Ramage.
2007.
Lexi-cal semantic relatedness with random graph walks.In Proceedings of the 2007 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 581?589, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Mario Jarmasz and Stan Szpakowicz.
2003.
Roget?sthesaurus and semantic similarity.
In Proceedings ofthe International Conference on Recent Advances inNatural Language Processing (RANLP), pages 212?219.Jay J. Jiang and David W. Conrath.
1997.
Semanticsimilarity based on corpus statistics and lexical tax-onomy.
In Proceedings of the International Confer-ence on Research in Computational Linguistics (RO-CLING), pages 19?33.Claudia Leacock and Martin Chodorow.
1998.
Com-bining local context and WordNet similarity forword sense identification.
In Christiane Fellbaum,editor, WordNet: An Electronic Lexical Database,pages 265?283.
MIT Press.Dekang Lin.
1998.
An information-theoretic defini-tion of similarity.
In Proceedings of the 15th Inter-national Conference on Machine Learning (ICML),pages 296?304.George A. Miller and Walter G. Charles.
1991.
Con-textual correlates of semantic similarity.
Languageand Cognitive Processes, 6(1):1?28.894David Milne and Ian H. Witten.
2008.
An effective,low-cost measure of semantic relatedness obtainedfrom Wikipedia links.
In Proceedings of the FirstAAAI Workshop on Wikipedia and Artificial Intelli-gence (WIKIAI), pages 25?30.Siddharth Patwardhan and Ted Pedersen.
2006.
Us-ing WordNet-based context vectors to estimate thesemantic relatedness of concepts.
In Proceedingsof the 11th Conference of the European Chapter ofthe Association for Computational Linguistics Work-shop on Making Sense of Sense, pages 1?8.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
WordNet::Similarity ?
Measuringthe relatedness of concepts.
In Proceedings of the5th Annual Meeting of the North American Chap-ter of the Association for Computational Linguistics(NAACL), pages 38?11.Simone Paolo Ponzetto and Roberto Navigli.
2010.Knowledge-rich word sense disambiguation rivalingsupervised systems.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics (ACL), pages 1522?1531.Philip Resnik.
1995.
Using information content toevaluate semantic similarity in a taxonomy.
In Pro-ceedings of the 14th International Joint Conferenceon Artificial Intelligence (IJCAI), pages 448?453.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Communica-tions of the ACM, 8(10):627?633.Sean Szumlanski and Fernando Gomez.
2010.
Au-tomatically acquiring a semantic network of relatedconcepts.
In Proceedings of the 19th ACM Inter-national Conference on Information and KnowledgeManagement (CIKM), pages 19?28.Zhibiao Wu and Martha Palmer.
1994.
Verb seman-tics and lexical selection.
In Proceedings of the32nd Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 133?139.895
