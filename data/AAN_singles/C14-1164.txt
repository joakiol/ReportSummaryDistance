Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1740?1751, Dublin, Ireland, August 23-29 2014.Detecting Learner Errors in the Choice of Content WordsUsing Compositional Distributional SemanticsEkaterina KochmarComputer LaboratoryUniversity of Cambridgeek358@cl.cam.ac.ukTed BriscoeComputer LaboratoryUniversity of Cambridgeejb@cl.cam.ac.ukAbstractWe describe a novel approach to error detection in adjective?noun combinations.
We present andrelease a new dataset of annotated errors where the examples are extracted from learner texts andannotated with error types.
We show how compositional distributional semantic approaches canbe applied to discriminate between correct and incorrect word combinations from learner data.Finally, we show how the output of the compositional distributional semantic models can be usedas features in a classifier yielding good precision and accuracy.1 IntroductionThe task of error detection and correction (henceforth, EDC) in non-native writing in English has beena focus of research in recent years.
However, usually research in this area focuses on EDC in the use offunction words, such as articles or prepositions (Leacock et al., 2010; Dale et al., 2012), while much lessattention has been paid to errors in the choice of content words.Errors in function words are some of the most common error types in learner writing (Dalgish, 1985;Leacock et al., 2010), so it is important for any EDC system to be able to deal with such errors.
Certainproperties of these errors facilitate their detection and correction.
As function words belong to closedclasses, the set of possible corrections is limited by the size of the function word set.
Since errors infunction words are systematic and highly recurrent, in practice, each article or preposition has an evensmaller number of appropriate alternatives.
We illustrate this point with the following examples on (1)article and (2) preposition errors:(1) I am 0*/a student.
(2) Last October, I came in*/to Tokyo.In (1) an EDC system would consider {a, an, the} as possible corrections for the missing article.
Tocorrect the preposition in in (2), an EDC system would consider the most frequent prepositions {on,from, for, of, about, to, at, with, by}, among which at or to would have a higher chance to be appropriatecorrections as these are most often confused with in.
Confusion sets can be learnt from learner texts, andprobabilities can be set up according to the distribution of the confusions (Rozovskaya and Roth, 2011).EDC is usually cast as a multi-class classification task, with the number of classes equal to the numberof target corrections.
Detection and correction can occur simultaneously: an error is detected when anEDC system suggests using a word different from the one originally used by the learner, and the sug-gested word can be used as a correction.
Each occurrence of a function word is represented with a featurevector, where features are derived from the surrounding context.
This is usually highly informative forfunction words: for example, a context of I am and student or a similar noun requires the use of anindefinite article, while the only correct preposition to relate a verb of movement like come to a locativelike Tokyo is to.In this work, however, we focus on errors in the choice of content words, which have received muchless attention in spite of being the third most frequent error type in learner writing (Leacock et al., 2010).Errors in content words are more challenging than errors in function words, since the number of possibleThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1740confusions and corrections cannot be reduced to a finite set.
For example, consider incorrect choice ofadjectives in the following sentences extracted from learner data:(3) A big*/great damage has been made to the environment.
(4) I have tried a rock?n?roll dance and a classic*/classical dance already.The confusion in (3) is caused by semantic similarity of the adjectives big and great, while in (4) it isdue to similarity in form between classic and classical.
It is much harder to cast the EDC in content wordsas multi-class classification, unless we consider the full set of English adjectives as possible classes.
Thesurrounding contexts are much sparser and less informative, and in addition to that, often contain furthererrors.
In this work, we address error detection and focus on adjective-noun combinations (ANs), whichare representative of the more general task of EDC in content word combinations and are a frequent errortype in learner text.We have created a dataset of ANs, where the combinations are extracted from learner texts and man-ually error-coded using a novel annotation scheme.
This scheme is motivated by observations abouttypical learner confusions in the choice of adjectives and nouns ?
for example, semantically-related orform-related confusions.
Since errors in content words are related to semantics, we derive semantically-motivated features through models of compositional distributional semantics and use these features forerror detection.
We treat error detection as a binary classification task, following the usual convention inEDC.The original contributions of this paper are that we:?
present and release an error-annotated AN dataset extracted from learner data;?
show how compositional distributional semantic models can be applied to detect semantic anomaliesin this dataset;?
demonstrate that the output of these models can be used to derive features for error detection in ANcombinations.2 Previous work2.1 Error Detection in Content WordsPrevious work on EDC for content words has either focused on correction alone assuming that errorsare already detected (Liu et al., 2009; Dahlmeier and Ng, 2011), or has reformulated the task as writingimprovement (Shei and Pain, 2000; Wible et al., 2003; Chang et al., 2008; Futagi et al., 2008; Park et al.,2008; Yi et al., 2008;?Ostling and Knutsson, 2009).In the first case, the task is reduced to the search for the most suitable correction among the alternativestypically composed of synonyms, homophones or L1-related paraphrases (Dahlmeier and Ng, 2011),while the more challenging error detection step is omitted.
In the second case, error detection is integratedinto suggestion of alternatives and their comparison to the originally used word combination accordingto some metric of collocational strength.
Such approaches aim to improve the fluency of non-nativetexts by correcting erroneous idioms or collocations, where low frequency or low collocational strengthclearly signifies an error.These approaches might be useful for correcting collocations, but they are less suitable for error detec-tion in free word combinations.
As they compare original word combinations to their alternatives usingcorpus statistics, they are not applicable to unseen word combinations, while learner texts contain manypreviously unseen combinations, not all of which are errors.
Moreover, some word combinations maybe correct even though less fluent than some of their alternatives.
For example, appropriate concern,though it is correct, would have lower collocational strength than its alternative proper concern, andwould, according to this approach, be tagged as an error.
From the educational point of view, tagging anacceptable combination as an error is misleading for language learners and should be avoided.We implement a baseline model inspired by such comparison-based approaches and demonstrate that itcannot be usefully applied to error detection in content word combinations.
Then we present an approachthat is capable of dealing with unseen data and does not rely on direct corpus-based comparison.17412.2 Semantic Anomaly DetectionLearner errors in content words often result from a semantic mismatch between the chosen words.
Asimilar problem of semantic anomaly detection in content word combinations has been addressed withcompositional distributional semantic models.These models are based on distributional representations for words which are then composed to derivephrase representations.
They rely on the assumption that a word meaning can be approximated by itsdistribution across its contexts of use.
Words are represented as vectors in a high-dimensional space witheach dimension encoding a word?s co-occurrence with one of its contextual elements.
Distributionalmodels are less suitable for representing content word combinations directly since these will be verysparse and will often remain unattested even in an extremely large corpus.A promising solution is provided by compositional distributional semantic models, which combinedistributional vectors for the component words using some function over such vectors.
Compositionaldistributional semantic representations have been previously used to detect semantic anomaly in ANcombinations (Vecchi et al., 2011).
Vecchi et al.
have applied the additive and multiplicative modelsof Mitchell and Lapata (2008) and adjective-specific linear maps of Baroni and Zamparelli (2010) to aset of corpus-unattested ANs.
They show that there is a distinguishable difference in the compositionalsemantic representations for the semantically acceptable and anomalous combinations, suggesting thatcompositional distributional models can be used to detect semantic anomaly without relying directly oncorpus statistics.Kochmar and Briscoe (2013) have applied the same models of semantic composition to distinguishbetween correct and incorrect ANs extracted from learner texts.
Their results support the assumptionthat there is a distinguishable difference between the composite vectors for the correct and incorrectANs, but they did not address the question of how to integrate these semantic models into an errordetection system.Recent work by Lazaridou et al.
(2013) has shown that measures used for quantifying the degree ofsemantic anomaly in phrases derived from their compositional distributional semantic representationscan be used as features by a classifier to help resolve syntactic ambiguities.Our goals are to test, using a new and larger AN dataset, whether semantic models can distinguishbetween correct and incorrect AN combinations, which cannot be dealt with using simpler error detectionapproaches, and to implement an error detection system using these semantically-based features.3 Data AnnotationWe present and release a dataset of AN combinations which, on the one hand, exemplify the typicalerrors committed by language learners in the choice of content words within such combinations, and, onthe other hand, are challenging for an EDC system.For that, we examined the publicly available CLC-FCE dataset (Yannakoudakis et al., 2011), usedthe error annotation (Nicholls, 2003), and analysed the typical errors in AN combinations committed bylanguage learners.
We have compiled a list of 61 adjectives that are most problematic for learners.Most typically, learners confuse semantically related words: for example, they are unable to distin-guish between synonyms, near-synonyms or co-hyponyms and choose an appropriate one from the set.Our list of adjectives contains some frequent ones that are confused with each other due to their similarityin meaning.
For example, the adjectives within the set {big, large, great} are frequently confused witheach other as in:(5) big*/large quantity (6) big*/great importanceAnother common source of error related to the high-frequency adjectives involves using them insteadof more specific ones: in such cases, learners are unable to distinguish between the more specific termsand they choose the most frequent adjective, usually encompassing a variety of related meanings, torepresent the whole class of similar words.
For example, adjectives big and large encompass a variety ofmeanings including those of high, wide or broad.
As learners often lack intuitions about which of these1742more specific adjectives should be chosen, they use the ones with more general meaning.
This results inerrors like:(7) big*/long history(8) bigger*/wider variety(9) greatest*/highest revenue(10) large*/broad knowledgeThe reverse of this ?
an incorrect selection of a more specific term instead of the more general one ?also leads to learner errors.Form-related confusions represent another typical source of learner errors, and we have included pairsof adjectives such as classic and classical, economic and economical and the like in our dataset:(11) classic*/classical dance (12) economical*/economic crisisUsing this set of 61 adjectives, we have extracted AN combinations from the Cambridge LearnerCorpus (CLC),1a large corpus of texts produced by English language learners, sitting Cambridge As-sessment?s examinations.2We have focused on AN combinations previously unseen in a native Englishcorpus, as we hypothesise that they would have a higher chance of containing an error.
Such combina-tions are more challenging for EDC algorithms since:?
these ANs cannot be effectively handled with simple comparison-based approaches like the onesoverviewed in section 2.1;?
language learners are creative in their writing, so there is a substantial number of such previouslyunseen combinations;?
as no corpus could cover all possible acceptable content word combinations in language, the fact thatthese combinations are not seen in the corpus cannot be used as definitive evidence of incorrectness.To summarise, it is important for an EDC algorithm to handle such combinations, but their absence ina native corpus of English makes it impossible to rely on simpler approaches and suggests that semanticanalysis of such combinations would be more effective.
In our research, we used the British NationalCorpus (BNC)3to select the corpus-unattested combinations.We have compiled a set of 798 AN combinations.4An annotation scheme has been devised to annotatethese examples as correct or incorrect, and for the incorrect combinations, to identify the locus of error(adjective, noun or both) and the type of confusion (incorrect synonym, form-related word, or non-relatedword).
The most appropriate corrections are included in the dataset.We also distinguish between out-of-context (OOC) and in-context (IC) annotation.
The motivationbehind this distinction is as follows: some combinations may appear to be correct when consideredout of their original context of use, because there might be other contexts where the same combinationwould be appropriate.
For example, classic dance is annotated as correct out of context because onecould imagine using it in a context where it would denote some typical dance like:(13) They performed a classic Ceilidh dance.However, in practice, the AN classical dance is used much more frequently, and classic dance is mostoften errorful in context, as in (4) above.Some ANs in our dataset are represented with more than one context of use, and in that case thein-context annotation can be conditioned on each context, or used to derive the most typical annotationfor the AN.
Both types of information are useful, as EDC systems which make use of the surroundingcontext should rely on the annotation in each particular context of use and, for example, be able to detect1http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom/item3646603/Cambridge-International-Corpus-Cambridge-Learner-Corpus/2http://www.cambridgeenglish.org3http://www.natcorp.ox.ac.uk/4This dataset is released and publicly-available at http://www.ilexir.com/1743Type Cor.
Incor.
LB UBOOC 633 165 0.7932 0.8650IC 394 404 0.5063 0.7467Table 1: Distribution of correct (cor.)
and incorrect (incor.)
ANs in the dataset.that classic dance is correct in one specific context, while in others it is incorrect.
EDC systems that donot make use of the context can simply rely on the most frequent in-context annotation and detect thatclassic dance is typically an error in learner writing.To create the two-level annotation, the annotators were first presented with an AN combination andasked to tag each word as correct or incorrect depending on whether they can think of some appropriatecontexts of use for it.
Next, the same combination was presented in its context of use from the CLC andthe annotators were asked to annotate it with respect to its context.The dataset was primarily annotated by a professional linguist.
To ensure that the annotation schemeis clear and efficient, the dataset was split into 100 and 698 ANs, and the 100 ANs were first annotatedby the same professional annotator and three other annotators.
We have measured the inter-annotatoragreement for the two levels of annotation using the mean values for the observed agreement withineach pair of annotators, as well as mean Cohen?s kappa value (Cohen, 1960).
In Table 1 we reportthe mean inter-annotator agreement for the correct versus incorrect combinations at the two annotationlevels, which represents the upper bound (UB) in our experiments.
We have obtained the mean kappavalues of 0.65 and 0.49 at the two levels of annotation, which are interpreted as substantial and mediumagreement between annotators and confirm that the annotation scheme is clear.5Table 1 presents thedistribution of ANs and the majority class baseline which we further use as a lower bound (LB).4 Semantic Models for Error DetectionWe replicate the semantic approaches, which have previously shown promising results in detecting se-mantic anomaly and content word errors (Vecchi et al., 2011; Kochmar and Briscoe, 2013), and test theirperformance on our dataset of corpus-unattested correct and incorrect AN combinations.4.1 Experimental SettingWe use the additive (add) and multiplicative (mult) models of Mitchell and Lapata (2008), and theadjective-specific linear maps (alm) of Baroni and Zamparelli (2010).The first two models derive the composite phrase vector through addition and multiplication of thecomponents of the word vectors.
These models have a clear mathematical interpretation and requireno training.
Their principal weakness is that they are symmetric, and fail to represent the difference ingrammatical function of the component words.
The alm model provides a theoretically more appropriateway of representing ANs based on this asymmetry: nouns are represented by their distributional vectors,while attributive adjectives are functions mapping from noun meanings to a composite noun-like vectorfor the ANs.
Adjectives are represented as weight matrices which are learned from corpus-attestedexamples of noun?AN mappings, and composition is defined by matrix-by-vector multiplication.We use the experimental setting previously described (Vecchi et al., 2011; Kochmar and Briscoe,2013) and populate the semantic space with the constituent nouns and adjectives from the test ANs,frequent nouns and adjectives from the BNC and the AN combinations containing these frequent words.We use about 8K nouns, 4K adjectives and 64K ANs following Kochmar and Briscoe (2013).
Thesemantic space is represented by a matrix encoding word co-occurrences, where the rows represent the76K elements mentioned above, and the columns represent a selected set of 10K context elements.The 10K context elements include the most frequent nouns, adjectives and verbs from the corpus.
Theword co-occurrence counts are estimated using the BNC.
The corpora have been lemmatized, tagged andparsed with the RASP system (Briscoe et al., 2006; Andersen et al., 2008; Yannakoudakis et al., 2011),and all statistics are extracted at the lemma level.5Further details of the annotation experiment are described in the dataset release.1744We transform the raw sentence-internal co-occurrence counts into Local Mutual Informationscores (Baroni and Zamparelli, 2010; Evert, 2005), and perform dimensionality reduction applying Sin-gular Value Decomposition to the noun and adjective matrix rows, projecting the AN rows onto the samereduced space following Baroni and Zamparelli (2010).
The original 76K ?
10K matrix is reduced to a76K ?
300 matrix.
This allows us to perform training and other calculations in the semantic space moreefficiently.The weight coefficients for the alm model are estimated with multivariate partial least squares re-gression using the RPLS package (Mevik and Wehrens, 2007).
The weight matrix is learned for eachadjective separately.4.2 Semantic CuesIn previous work (Vecchi et al., 2011; Kochmar and Briscoe, 2013) several semantic measures for de-tecting semantic anomaly have been introduced.
We reimplement these measures (1 to 8), but also testsome additional measures (9 to 13) that we hypothesise can also help distinguish between correct andincorrect word combinations:1.
Vector length (VLen): vectors for correct and incorrect combinations may differ with respect totheir length, and the latter are expected to be shorter;2.
Cosine to the input noun (cosN): the distance between the model-generated AN vector and theinput noun vector is expected to be greater for the incorrect combinations, as the noun meaning istypically ?distorted?;3.
Cosine to the input adjective (cosA): analogical to cosN measure, the adjective meaning might be?distorted?
as well, especially as two of the composition functions are symmetric;4.
Density of the neighbourhood populated by 10 nearest neighbours (dens) is calculated as theaverage distance from the model-generated vector to the 10 nearest neighbours in the original se-mantic space, and is expected to be higher for the correct ANs;5.
Density among the 10 nearest neighbours (densAll) is a modification of dens, which is estimatedas an average for the 11 density values calculated for each member within the set consisting of theAN vector and its 10 neighbours;6.
Ranked density in close proximity (Rdens) relies on the notion of close proximity, which is definedas a neighbourhood populated by some very close neighbours (for example, within a distance of?
0.8).
It is calculated as: RDens =?Ni=1rankidistanceiwith N being the total number ofclose neighbours within close proximity, each with its rank and distance;7.
Number of neighbours within close proximity (num) is used as another measure, and is assumedto be lower for incorrect combinations, which are expected to be more isolated in the semanticspace;8.
Overlap between the 10 nearest neighbours and constituent noun/adjective (OverAN) assumescorrect ANs should be surrounded by similar words and combinations.
It is calculated as the pro-portion of the 10 nearest neighbours containing the same constituent words as in the tested ANs;9.
Overlap between the 10 nearest neighbours and input noun (OverN) is a variant of the OverANwith only the noun considered;10.
Overlap between the 10 nearest neighbours and input adjective (OverA) is a variant of theOverAN with only the adjective considered;11.
Overlap between the 10 nearest neighbours for the AN and constituent noun/adjective(NOverAN) assumes that correct ANs and their constituent words should be placed in similar neigh-bourhoods.
It is calculated as the proportion of the common neighbours among the 10 nearestneighbours for the model-generated AN and the constituent words;1745Metric add mult almVLen 0.7589 0.7690 0.1676cosN 0.1621 0.0248 0.0227cosA 0.0029 0.4782 0.0921dens 0.6731 0.1182 0.1024densAll 0.4967 0.1026 0.1176RDens 0.2786 0.8754 0.1970num 0.3132 0.4673 0.3765OverAN 0.8529 0.1622 0.2808OverA 0.0151 0.6377 0.4886OverN 0.0138 0.0764 0.4118NOverAN 0.3941 0.6730 0.0858NOverA 0.0009 0.3342 0.1575NOverN 0.0018 0.1463 0.1497Table 2: p values, out-of-context annotationMetric add mult almVLen 0.6675 0.0027 0.0111cosN 0.0417 0.0070 0.1845cosA 0.00003 0.1791 0.1442dens 0.4756 0.7120 0.1278densAll 0.2262 0.7139 0.5310RDens 0.8934 0.8664 0.1985num 0.7077 0.7415 0.4259OverAN 0.1962 0.8635 0.5669OverA 0.00007 0.7271 0.6229OverN 0.0017 0.9680 0.7733NOverAN 0.0227 0.3473 0.1587NOverA 0.000004 0.3749 0.1576NOverN 0.0001 0.6651 0.2610Table 3: p values, in-context annotation12.
Overlap between the 10 nearest neighbours for the AN and input noun (NOverN) is a variantof the NOverAN with only the noun considered;13.
Overlap between the 10 nearest neighbours for the AN and input adjective (NOverA) is avariant of the NOverAN with only the adjective considered.4.3 ResultsWe evaluate the models and report the results following the procedure that has been used before in Vecchiet al.
(2011) and Kochmar and Briscoe (2013).
For each model and semantic measure, we report the pvalue denoting statistical significance of the difference between the groups of correct and incorrect ANs.The statistical significance is reported at the p<0.05 level, and if a measure applied to the two groups ofANs shows statistically significant difference we interpret that as an ability of this measure to distinguishthe correct ANs from the incorrect ones in general.
The results for the out-of-context annotation arereported in Table 2, and those for the in-context annotation in Table 3.The results show that the difference between the vector representations for the correct and incorrect ANcombinations can be reliably detected with a number of the proposed measures.
Measures which showstatistically significant results with at least one model are marked in bold.
These results also suggest thatthe values for the semantic measures can be used to derive discriminative features for a classifier.5 Error Detection as Classification Task5.1 Baseline SystemWe implement a simple comparison-based baseline system inspired by previous work on error detectionin content words (see section 2.1).
For every AN, we create a set of possible alternatives crossing theconfusion set for the adjective with that for the noun, and compare the collocational strength of theoriginal combination with that for each of the alternatives.
If an alternative has higher collocationalstrength than the original combination, the original combination is tagged as an error and the alternativeis chosen as a correction.
Since semantically related confusions are a rich source of learner errors incontent word combinations, we include adjective synonyms in the confusion set for an adjective, andnoun synonyms and hyponyms in the confusion set for a noun.
All synonyms and hyponyms are retrievedusing WordNet 3.0 without word sense disambiguation.We measure collocational strength using normalized pointwise mutual information (npmi) of the ad-jective a and noun n, which is defined as:1746npmi(a, n) =pmi(a, n)?log[p(a, n)](1) pmi(a, n) = logp(a, n)p(a)p(n)(2)All probabilities are estimated from the BNC.
This approach performs poorly on the unseen ANs inour dataset, since any alternative AN seen in the BNC would be preferred by this system over the originalunseen AN.
This ensures that less fluent (in this case, unseen) word combinations are substituted withmore fluent (seen) ones.
As a result, even though an original AN important conversation in our datasetis correct, it is still ?corrected?
by this system to serious conversation.
At the same time, some incorrectcombinations are not recognised if no appropriate alternative is found (e.g., *high shyness).
It shows thatthis approach lacks deeper semantic analysis and is also too dependent on the set of alternatives foundfor a word combination.We measure accuracy (acc) as the proportion of true positives (TP) and true negatives (TN) to the totalnumber of test items:Acc =TP + TNTP + FP + TN + FN(3)Accuracy reflects how often an error detection system correctly identifies that an AN is correct orincorrect.
We compare the results to the lower and upper bounds set as the majority class distributionand inter-annotator agreement, respectively (see section 3).With this approach we get quite low accuracy of 0.3897 on the out-of-context annotation since mostof the test items are correct out of context (LB=0.7932), and the baseline system overcorrects many ofthose.
Accuracy of the baseline system on the in-context annotation is 0.5147, which is slightly abovethe lower bound of 0.5063.
These results are used as a baseline and included in Table 4.Type Accuracy Baseline LB UBOOC 0.8113 ?
0.0149 0.3897 0.7932 0.8650IC 0.6535 ?
0.0189 0.5147 0.5063 0.7467Table 4: Decision Tree classification resultsType P (correct) P (incorrect)OOC 0.8193 0.7500IC 0.6241 0.6850Table 5: Classification precision5.2 ClassificationWe implement a supervised classifier which uses output of the semantic models as features.
We havetested a number of classifier models but the best results so far have been obtained with the DecisionTree classifier using NLTK (Bird et al., 2009).
We assume that this classifier effectively learns theinter-dependencies between the features within the small feature set that we use in our experiments.
Weuse feature binning where the whole range of feature values is divided into 10 bins according to thedistribution of values for each feature.
This feature representation technique combined with the classifierhelps generalise over feature values, reducing feature space dimensionality.
The order of the featureapplication to the data is determined by the classifier on the basis of the information gain for the featuresand their values.We apply 5-fold cross-validation and report average accuracy over the folds.
The 798 ANs are splitinto 5 subsets with 80% in each of the splits used for training and 20% for testing.
We keep the AN errorrate in the training and test sets, as well as for each adjective, approximately the same across the splits toavoid any bias.
Error detection is cast as a binary classification task.
The output of the semantic modelsis used to derive numeric features for the classifier.
Most values are in the range of [0, 1], and we applynormalisation to VLen, RDens and num which originally have a different range.The full feature set contains 14 features, with 13 features derived from the semantic measures, and1 feature representing adjective identity.
We hypothesise that introduction of this feature might helpclassifier learn that, for example, an AN containing an adjective classic has a higher chance of beingincorrect, as most of the ANs with this adjective in the learner data are incorrect and involve confusionswith classical.
We also hypothesise that it facilitates learning correlations between the adjective and other1747feature values: it might be the case that ANs with an adjective adj1, on the average, have higher cosNvalues than ANs with an adjective adj2.
This feature helps the classifier establish such dependenciesbetween the adjective and the values of the semantic measures.
For instance, in our data ANs withthe adjective true have significantly higher cosine between AN vectors and vectors for their constituentnouns than ANs with the adjective false: this is in accordance with an intuition that, for example, truehappiness is more similar to happiness than false happiness is.The best results in our experiments have been obtained with the mult model.
We have performedablation tests incrementally removing features that did not improve classifier performance in order tofind an optimal feature set.
The best-performing feature set we found for the mult model on the out-of-context annotation uses adjective, cosN and RDens features, while for the in-context annotation thebest-performing feature set found uses a combination of features including adjective, VLen, densAll,NOverA, NOverN, RDens and num features.We note that the sets of best performing features in the classification experiments do not coincide withthe semantic measures that showed the highest statistically significant difference (Tables 2 and 3).
Weconclude that although the p values reported in Tables 2 and 3 show that some semantic measures candistinguish one group of ANs from another on the basis of the statistically significant difference betweenthe means of the two groups, when the measures are used as features for a classifier the results dependon how these features interact with each other as well as on their individual discriminativeness across thetest dataset.
For example, Figure 1 illustrates a small part of the decision tree constructed using the bestperforming feature set on the in-context annotation:Figure 1: Decision Tree classifier pseudocode.Figure 1 shows how interaction of feature values for num and VLen in combination with the adjectiveidentity feature can help classify the two ANs containing adjective large as correct (1) or incorrect (-1).In Table 4 we report results for the out-of-context (OOC) and in-context (IC) annotation.
The accuracyis reported with its mean ?
standard deviation over the 5 data splits.
We compare the Decision Treeclassifier results to those obtained with the baseline system, as well as to the lower and upper bounds setas before (see section 3).
The results show that a classifier that uses output of the semantic models asfeatures outperforms the comparison-based baseline system by a large margin.6 DiscussionIn the previous section, we showed that a classifier that uses output of the semantic models as featuresoutperforms the comparison-based baseline system and shows good accuracy.
In this section, we analysethe classifier?s performance in more detail.We note that, from an educational point of view, it is important for an EDC system to have highprecision.
For example, it has been shown that grammatical error detection systems with high preci-sion maximize learning effect, and that systems with high precision but lower recall are more usefulin language learning than systems with high recall and lower precision (Nagata and Nakatani, 2010).This suggests that learners might be misled and confused if they are frequently notified by a system thatsomething is an error when it is not.Since precision is measured as the proportion of true positives (TP) to the sum of true positives andfalse positives (FP):1748P =TPTP + FP(4)an EDC system that achieves precision less than 0.5 is, in fact, misleading for language learners: forexample, precision of less than 0.5 on the class of errors means that the system misidentifies correct useas an error more frequently than it correctly detects an error.Our classifier achieves good precision values with respect to both out-of-context and in-context anno-tations, on correct and incorrect examples.
Precision (P ) values are reported in Table 5.
As precisionfigures are higher than 0.5 in each case, it shows that the implemented error detection system would, onbalance, help guide a learner to text regions in need of reformulation.With respect to the out-of-context annotation, the error detection system has good precision and recallon correct examples (P = 0.8193, R = 0.9762).
Precision on the incorrect examples is also high(P = 0.7500).
This is a very encouraging result, suggesting the system would rarely misidentify anoriginally correct AN combination as an error.For the in-context annotation, both precision and recall on correct and incorrect examples are quitehigh: P = 0.6241 and R = 0.7169 on the correct examples, and P = 0.6850 and R = 0.5849 on theincorrect examples.Error analysis on the classifier?s output shows that the majority of the incorrect examples misclassifiedas correct (missed errors) contain semantically-related confusions.
It appears that the classifier relyingon semantically-motivated features misses a number of cases where the original AN and its correctionare semantically similar: for example, it misses the errors in big*/great anger, biggest*/greatest painterand small*/short speech.
Since the ANs in these pairs are semantically similar, the features based ontheir semantic representations might not be discriminative enough.
In contrast, the classifier is moreeffective in detecting errors in cases where the original AN and its correction are only similar in form, ornot related to each other.7 ConclusionWe have presented and released a dataset of learner errors in ANs, which has been extracted from learnertexts and annotated with error types and corrections.
The dataset contains examples not seen in a nativecorpus of English, and error annotation shows that a substantial number of such examples are correct.Error detection in this dataset is a challenging task, since absence of the ANs in a corpus of Englishcannot be used as definitive evidence of incorrectness.
We have implemented a simple baseline systeminspired by previous work on improving content word combinations and shown that such a system wouldnot be effective for error detection in our dataset.We have cast error detection as a binary classification task and implemented a supervised classifierthat uses semantically-motivated features.
The features are derived from the compositional distributionalsemantic representations of the AN combinations.
We use a number of semantic measures that describeand distinguish between semantic representations for correct and incorrect combinations.
We have intro-duced new semantic measures in addition to the ones used in previous work and show that they can beeffectively applied to this task.The best results in our experiments are obtained with a Decision Tree classifier, and we show that theresulting error detection system can identify errors with high precision and accuracy.
We aim to extendthis system to perform error correction on ANs, as well as error detection and correction on other typesof content word combinations.AcknowledgmentsWe are grateful to Cambridge English Language Assessment and Cambridge University Press for sup-porting this research and for granting us access to the CLC for research purposes.
We would like tothank ?istein Andersen for providing us with the annotation tool, Diane Nicholls for undertaking thebulk of the annotation work, and Helen Yannakoudakis and the anonymous reviewers for their valuablecomments.1749References?istein Andersen, Julien Nioche, Ted Briscoe and John Carroll 2008.
The BNC parsed with RASP4UIMA.
InProceedings of the 6th International Conference on Language Resources and Evaluation (LREC), pp.
865?869.Marco Baroni and Roberto Zamparelli 2010.
Nouns are vectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.
In Proceedings of the EMNLP-2010, pp.
1183?1193.Steven Bird, Ewan Klein, and Edward Loper 2009.
Natural Language Processing with Python ?
Analyzing Textwith the Natural Language Toolkit.
O?Reilly Media.Ted Briscoe, John Carroll and Rebecca Watson 2006.
The Second Release of the RASP System.
In Proceedings ofthe COLING/ACL-2006 Interactive Presentation Sessions, pp.
59?68.Yu-Chia Chang, Jason S. Chang, Hao-Jan Chen and Hsien-Chin Liou 2008.
An automatic collocation writingassistant for Taiwanese EFL learners: A case of corpus-based NLP technology.
Computer Assisted LanguageLearning, 21(3), pp.
283?299.Jacob Cohen 1960.
A coefficient of agreement for nominal scales.
Educational and Psychological Measurement,20(1), pp.
37?46.Robert Dale, Ilya Anisimoff and George Narroway 2012.
HOO 2012: A Report on the Preposition and DeterminerError Correction Shared Task.
In Proceedings of the 7th Workshop on Innovative Use of NLP for BuildingEducational Applications, pp.
54?62.Daniel Dahlmeier and Hwee Tou Ng 2011.
Correcting Semantic Collocation Errors with L1-induced Paraphrases.In Proceedings of the EMNLP-2011, pp.
107?117.Gerard M. Dalgish 1985.
Computer-assisted ESL research.
In CALICO Journal, 2(2), pp.
32?37.Stefan Evert 2005.
The Statistics of Word Cooccurrences.
Dissertation, Stuttgart University.Yoko Futagi, Paul Deane, Martin Chodorow and Joel Tetreault 2009.
A computational approach to detectingcollocation errors in the writing of non-native speakers of English.
Computer Assisted Language Learning,21(4), pp.
353?367.Ekaterina Kochmar and Ted Briscoe 2013.
Capturing Anomalies in the Choice of Content Words in Composi-tional Distributional Semantic Space.
In Proceedings of the Recent Advances in Natural Language Processing(RANLP-2013).Angeliki Lazaridou, Eva Maria Vecchi and Marco Baroni 2013.
Fish transporters and miracle homes: How com-positional distributional semantics can help NP parsing.
In Proceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing, pp.
1908?1913.Claudia Leacock, Martin Chodorow, Michael Gamon and Joel Tetreault 2010.
Automated Grammatical ErrorDetection for Language Learners.
Morgan and Claypool Publishers.Anne Li-E Liu, David Wible and Nai-Lung Tsao 2009.
Automated suggestions for miscollocations.
In Proceed-ings of the 4th Workshop on Innovative Use of NLP for Building Educational Applications, pp.
47?50.Bj?rn-Helge Mevik and Ron Wehrens 2007.
The pls package: Principal component and partial least squaresregression in R. Journal of Statistical Software, 18(2), pp.
1?24.Jeff Mitchell and Mirella Lapata 2008.
Vector-based models of semantic composition.
In Proceedings of ACL,pp.
236?244.Jeff Mitchell and Mirella Lapata 2010.
Composition in distributional models of semantics.
Cognitive Science, 34,pp.
1388?1429.Ryo Nagata and Kazuhide Nakatani 2010.
Evaluating performance of grammatical error detection to maximizelearning effect.
In Proceedings of COLING 2010, pp.
894?900.Diane Nicholls 2003.
The Cambridge Learner Corpus: Error coding and analysis for lexicography and ELT.
InProceedings of the Corpus Linguistics conference, pp.
572?581.Robert?Ostling and Ola Knutsson 2009.
A corpus-based tool for helping writers with Swedish collocations.
InProceedings of the Workshop on Extracting and Using Constructions in NLP, pp.
28?33.1750Taehyun Park, Edward Lank, Pascal Poupart, Michael Terry 2008.
Is the sky pure today?
AwkChecker: an assistivetool for detecting and correcting collocation errors.
In Proceedings of the 21st annual ACM symposium on Userinterface software and technology, pp.
121?130.Alla Rozovskaya and Dan Roth 2011.
Algorithm Selection and Model Adaptation for ESL Correction Tasks.InProceeding of the 49th Annual Meeting of the Association for Computational Linguistics: Human LanguageTechnologies ?
Volume 1, pp.
924?933.Chi-Chiang Shei and Helen Pain 2000.
An ESL Writer?s Collocation Aid.
Computer Assisted Language Learning,13(2), pp.
167?182.Eva Maria Vecchi, Marco Baroni and Roberto Zamparelli 2011.
(Linear) maps of the impossible: Capturingsemantic anomalies in distributional space.
In Proceedings of the DISCO Workshop at ACL-2011, pp.
1?9.David Wible, Chin-Hwa Kuo, Nai-Lung Tsao, Anne Liu and H.-L. Lin 2003.
Bootstrapping in a language-learning environment.
Journal of Computer Assisted Learning, 19(4), pp.
90?102.Helen Yannakoudakis, Ted Briscoe and Ben Medlock 2011.
A New Dataset and Method for Automatically Grad-ing ESOL Texts.
In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:Human Language Technologies, 1, pp.
180?189.Xing Yi, Jianfeng Gao and William B. Dolan 2008.
A Web-based English Proofing System for English as a SecondLanguage Users.
In Proceedings of the third International Joint Conference on Natural Language Processing(IJCNLP), pp.
619?624.1751
