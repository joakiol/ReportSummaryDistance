Proceedings of the ACL 2014 Workshop on Language Technologies and Computational Social Science, pages 61?65,Baltimore, Maryland, USA, June 26, 2014.c?2014 Association for Computational LinguisticsTemporal Analysis of Language through Neural Language ModelsYoon Kim?Yi-I Chiu?Kentaro Hanaki?Darshan Hegde?Slav Petrov?New York University, New YorkGoogle Inc., New York{yhk255, yic211, kh1615, dh1806}@nyu.eduslav@google.comAbstractWe provide a method for automaticallydetecting change in language across timethrough a chronologically trained neurallanguage model.
We train the model onthe Google Books Ngram corpus to ob-tain word vector representations specificto each year, and identify words that havechanged significantly from 1900 to 2009.The model identifies words such as celland gay as having changed during thattime period.
The model simultaneouslyidentifies the specific years during whichsuch words underwent change.1 IntroductionLanguage changes across time.
Existing wordsadopt additional senses (gay), new words are cre-ated (internet), and some words ?die out?
(manyirregular verbs, such as burnt, are being replacedby their regularized counterparts (Lieberman et al.,2007)).
Traditionally, scarcity of digitized histori-cal corpora has prevented applications of contem-porary machine learning algorithms?which typi-cally require large amounts of data?in such tem-poral analyses.
Publication of the Google BooksNgram corpus in 2009, however, has contributedto an increased interest in culturomics, whereinresearchers analyze changes in human culturethrough digitized texts (Michel et al., 2011).Developing computational methods for detect-ing and quantifying change in language is of in-terest to theoretical linguists as well as NLP re-searchers working with diachronic corpora.
Meth-ods employed in previous work have been var-ied, from analyses of word frequencies to more in-volved techniques (Guolordava et al.
(2011); Mi-halcea and Nataste (2012)).
In our framework,we train a Neural Language Model (NLM) onyearly corpora to obtain word vectors for each yearfrom 1900 to 2009.
We chronologically train themodel by initializing word vectors for subsequentyears with the word vectors obtained from previ-ous years.We compare the cosine similarity of the wordvectors for same words in different years to iden-tify words that have moved significantly in thevector space during that time period.
Our modelidentifies words such as cell and gay as havingchanged between 1900?2009.
The model addi-tionally identifies words whose change is moresubtle.
We also analyze the yearly movement ofwords across the vector space to identify the spe-cific periods during which they changed.
Thetrained word vectors are publicly available.12 Related WorkPreviously, researchers have computationally in-vestigated diachronic language change in variousways.
Mihalcea and Nastase (2012) take a super-vised learning approach and predict the time pe-riod to which a word belongs given its surroundingcontext.
Sagi et al.
(2009) use a variation of LatentSemantic Analysis to identify semantic change ofspecific words from early to modern English.
Wi-jaya and Yeniterzi (2011) utilize a Topics-over-Time model and K-means clustering to identifyperiods during which selected words move fromone topic/cluster to another.
They correlate theirfindings with the underlying historical events dur-ing that time.
Gulordava and Baroni (2011) useco-occurrence counts of words from 1960s and1990s to detect semantic change.
They find thatthe words identified by the model are consistentwith evaluations from human raters.
Popescu andStrapparava (2013) employ statistical tests on fre-quencies of political, social, and emotional wordsto identify and characterize epochs.Our work contributes to the domain in sev-1http://www.yoon.io61eral ways.
Whereas previous work has generallyinvolved researchers manually identifying wordsthat have changed (with the exception of Gulor-dava and Baroni (2011)), we are able to automat-ically identify them.
We are additionally able tocapture a word?s yearly movement and identifyperiods of rapid change.
In contrast to previouswork, we simultaneously identify words that havechanged and also the specific periods during whichthey changed.3 Neural Language ModelsSimilar to traditional language models, NLMs in-volve predicting a set of future word given somehistory of previous words.
In NLMs however,words are projected from a sparse, 1-of-V encod-ing (where V is the size of the vocabulary) onto alower dimensional vector space via a hidden layer.This allows for better representation of semanticproperties of words compared to traditional lan-guage models (wherein words are represented asindices in a vocabulary set).
Thus, words thatare semantically close to one another would haveword vectors that are likewise ?close?
(as measuredby a distance metric) in the vector space.
In fact,Mikolov et al.
(2013a) report that word vectors ob-tained through NLMs capture much deeper levelof semantic information than had been previouslythought.
For example, if xwis the word vectorfor word w, they note that xapple?
xapples?xcar?
xcars?
xfamily?
xfamilies.
That is, theconcept of pluralization is learned by the vectorrepresentations (see Mikolov et al.
(2013a) formore examples).NLMs are but one of many methods to ob-tain word vectors?other techniques include La-tent Semantic Analysis (LSA) (Deerwester et al.,1990), Latent Dirichlet Allocation (LDA) (Blei etal., 2003), and variations thereof.
And even withinNLMs there exist various architectures for learn-ing word vectors (Bengio et al.
(2003); Mikolovet al.
(2010); Collobert et al.
(2011); Yih etal.
(2011)).
We utilize an architecture introducedby Mikolov et al.
(2013b), called the Skip-gram,which allows for efficient estimation of word vec-tors from large corpora.In a Skip-gram model, each word in the corpusis used to predict a window of surrounding words(Figure 1).
To ensure that words closer to the cur-rent word are given more weight in training, dis-Figure 1: Architecture of a Skip-gram model (Mikolov et al.,2013b).tant words are sampled less frequently.2Trainingis done through stochastic gradient descent andbackpropagation.
The word representations arefound in the hidden layer.
Despite its simplicity?and thus, computational efficiency?compared toother NLMs, Mikolov et al.
(2013b) note that theSkip-gram is competitive with other vector spacemodels in the Semantic-Syntactic Word Relation-ship test set when trained on the same data.3.1 TrainingThe Google Books Ngram corpus containsNgrams from approximately 8 million books, or6% of all books published (Lin et al., 2012).
Wesample 10 million 5-grams from the English fic-tion corpus for every year from 1850?2009.
Welower-case all words after sampling and restrict thevocabulary to words that occurred at least 10 timesin the 1850?2009 corpus.For the model, we use a window size of 4 anddimensionality of 200 for the word vectors.
Withineach year, we iterate over epochs until conver-gence, where the measure of convergence is de-fined as the average angular change in word vec-tors between epochs.
That is, if V (y) is the vo-cabulary set for year y, and xw(y, e) is the wordvector for word w in year y and epoch number e,we continue iterating over epochs until,1|V (y)|?w?V (y)arccosxw(y, e) ?
xw(y, e?
1)?xw(y, e)?
?xw(y, e?
1)?is below some threshold.
The learning rate is setto 0.01 at the start of each epoch and linearly de-creased to 0.0001.2Specifically, given a maximum window size of W , a ran-dom integer R is picked from range [1, W ] for each trainingword.
The current training word is used to predict R previousand R future words.62Most Changed Least ChangedWord Similarity Word Similaritychecked 0.3831 by 0.9331check 0.4073 than 0.9327gay 0.4079 for 0.9313actually 0.4086 more 0.9274supposed 0.4232 other 0.9272guess 0.4233 an 0.9268cell 0.4413 own 0.9259headed 0.4453 with 0.9257ass 0.4549 down 0.9252mail 0.4573 very 0.9239Table 1: Top 10 most/least changed words from 1900?2009,based on cosine similarity of words in 2009 against their 1900counterparts.
Infrequent words (words that occurred less than500 times) are omitted.Once the word vectors for year y have con-verged, we initialize the word vectors for year y+1with the previous year?s word vectors and trainon the y + 1 data until convergence.
We repeatthis process for 1850?2009.
Using an open sourceimplementation in the gensim package, trainingtook approximately 4 days on a 2.9 GHz machine.4 Results and DiscussionFor the analysis, we treat 1850?1899 as an initial-ization period and begin our study from 1900.4.1 Word ComparisonsBy comparing the cosine similarity between samewords across different time periods, we are ableto detect words whose usage has changed.
We arealso able to identify words that did not change.
Ta-ble 1 has a list of 10 most/least changed words be-tween 1900 and 2009.
We note that almost all ofthe least changed words are function words.
Forthe changed words, many of the identified wordsagree with intuition (e.g.
gay, cell, ass).
Oth-ers are not so obvious (e.g.
checked, headed, ac-tually).
To better understand how these wordshave changed, we look at the composition of theirneighboring words for 1900 and 2009 (Table 2).As a further check, we search Google Booksfor sentences that contain the above words.
Beloware some example sentences from 1900 and 2009with the word checked:1900: ?However, he checked himself in time, saying ?
?1900: ?She was about to say something further, but shechecked herself.
?2009: ?He?d checked his facts on a notepad from his backpocket.
?2009: ?I checked out the house before I let them go inside.
?WordNeighboring Words in1900 2009gaycheerful lesbianpleasant bisexualbrilliant lesbianscellcloset phonedungeon cordlesstent cellularcheckedchecking checkingrecollecting consultedstraightened checkheadedhaired headingfaced sprintedskinned marchedactuallyevidently reallyaccidentally obviouslyalready nonethelessTable 2: Top 3 neighboring words (based on cosine similar-ity) specific to each time period for the words identified ashaving changed.At the risk of oversimplifying, the resulting sen-tences indicate that in the past, checked was morefrequently used with the meaning ?to hold in re-straint?, whereas now, it is more frequently usedwith the meaning ?to verify by consulting an au-thority?
or ?to inspect so as to determine accu-racy?.
Given that check is a highly polysemousword, this seems to be a case in which the popu-larity of a word?s sense changed over time.Conducting a similar exercise for actually, weobtain the following sentences:1900: ?But if ever he actually came into property, she mustrecognize the change in his position.
?1900: ?Whenever a young gentleman was not actuallyengaged with his knife and fork or spoon ?
?2009: ?I can?t believe he actually did that!
?2009: ?Our date was actually one of the most fun andcreative ones I had in years.
?Like the above, this seems to be a case inwhich the popularity of a word?s sense changedover time (from ?to refer to what is true or real?to ?to express wonder or surprise?
).4.2 Periods of ChangeAs we chronologically train the model year-by-year, we can plot the time series of a word?sdistance to its neighboring words (from differ-ent years) to detect periods of change.
Figure 2(above) has such a plot for the word cell comparedto its early neighbors, closet and dungeon, and themore recent neighbors, phone and cordless.
Fig-ure 2 (below) has a similar plot for gay.Such plots allow us to identify a word?s pe-riod of change relative to its neighboring words,63Figure 2: (Above) Time trend of the cosine similarity be-tween cell and its neighboring words in 1900 (closet, dun-geon) and 2009 (phone, cordless).
(Below) Similar plot ofgay and its neighboring words in 1900 (cheerful, pleasant)and 2009 (lesbian, bisexual).and thus provide context as to how it evolved.This may be of use to researchers interested inunderstanding (say) when gay started being usedas a synonym for homosexual.
We can also iden-tify periods of change independent of neighboringwords by analyzing the cosine similarity of a wordagainst itself from a reference year (Figure 3).
Assome of the change is due to sampling and randomdrift, we additionally plot the average cosine simi-larity of all words against their reference points inFigure 3.
This allows us to detect whether a word?schange during a given period is greater (or less)than would be expected from chance.
We notethat for cell, the identified period of change (1985?2009) coincides with the introduction?and sub-sequent adoption?of the cell phone by the gen-eral public.3Likewise, the period of change forgay agrees with the gay movement which beganaround the 1970s (Wijaya and Yeniterzi, 2011).4.3 LimitationsIn the present work, identification of a changedword is conditioned on its occurring often enough3http://library.thinkquest.org/04oct/02001/origin.htmFigure 3: Plot of the cosine similarity of changed (gay, cell)and unchanged (by, than) words against their 1900 startingpoints.
Middle line is the average cosine similarity of allwords against their starting points in 1900.
Shaded regioncorresponds to one standard deviation of errors.in the study period.
If a word?s usage decreaseddramatically (or stopped being used altogether),its word vector will have remained the same andhence it will not show up as having changed.One way to overcome this may be to combinethe cosine distance and the frequency to define anew metric that measures how a word?s usage haschanged.5 Conclusions and Future WorkIn this paper we provided a method for analyz-ing change in the written language across timethrough word vectors obtained from a chronolog-ically trained neural language model.
Extendingprevious work, we are able to not only automat-ically identify words that have changed but alsothe periods during which they changed.
While wehave not extensively looked for connections be-tween periods identified by the model and real his-torical events, they are nevertheless apparent.An interesting direction of research could in-volve analysis and characterization of the differ-ent types of change.
With a few exceptions, wehave been deliberately general in our analysis bysaying that a word?s usage has changed.
We haveavoided inferring the type of change (e.g.
semanticvs syntactic, broadening vs narrowing, pejorationvs amelioration).
It may be the case that words thatundergo (say) a broadening in senses exhibit reg-ularities in how they move about the vector space,allowing researchers to characterize the type ofchange that occurred.64ReferencesY.
Bengio, R. Ducharme, P. Vincent.
2003.
Neu-ral Probabilitistic Language Model.
Journal of Ma-chine Learning Research 3:1137?1155.D.
Blei, A. Ng, M. Jordan, J. Lafferty.
2003.
LatentDirichlet Allocation.
Journal of Machine LearningResearch 3:993?1022.R.
Collobert, J. Weston, L. Bottou, M. Karlen, K.Kavukcuglu, P. Kuksa.
2011.
Natural LanguageProcessing (Almost) from Scratch.
Journal of Ma-chine Learning Research 12:2493?2537.S.
Deerwester, S. Dumais, G. Furnas, T. Landauer, R.Harshman.
2011.
Indexing by Latent SemanticAnalysis.
Journal of the American Society for In-formation Science, 41(6):391?407.K.
Gulordava, M. Baroni.
2011.
A DistributionalSimilarity Approach to the Detection of SemanticChange in the Google Books Ngram Corpus.
Pro-ceedings of the GEMS 2011 Workshop.E.
Lieberman, J.B. Michel, J. Jackson, T. Tang, M.A.Nowak.
2007.
Quantifying the evolutionary dynam-ics of language.
Nature, 449: 716?716, October.Y.
Lin, J.B. Michel, E.L. Aiden, J. Orwant, W. Brock-man, S. Petrov.
2012.
Syntactic Annotations for theGoogle Books Ngram Corpus.
Proceedings of theAssociation for Computational Linguistics 2012.J.B Michel, Y.K.
Shen, A.P.
Aiden, A. Veres, M.K.Gray, J.P. Pickett, D. Hoiberg, D. Clancy, P. Norvig,J.
Orwant, S.Pinker, M.A.
Nowak, E.L. Aiden.2011.
Quantitative Analysis of Culture Using Mil-lions of Digitized Books.
Science, 331(6014): 176?182, January.R.
Mihalcea, V. Nastase.
2012.
Word Epoch Disam-biguation: Finding How Words Change Over Time.Proceedings of the Association for ComputationalLinguistics 2012.T.
Mikolov, M. Karafiat, L. Burget, J. Cernocky, S.Khudanpur.
2010.
Recurrent Neural NetworkBased Language Model.
Proceedings of Inter-speech.T.
Mikolov, W.T Yih, G. Zweig.
2013a.
LinguisticRegularities in Continuous Space Word Representa-tions.
Proceedings of NAACL-HLT 2013, 746?751.T.
Mikolov, K. Chen, G. Corrado, J.Dean.
2013b.
Effi-cient Estimation of Word Representations in VectorSpace arXiv Preprint.O.
Popescu, C. Strapparava.
2013.
Behind the Times:Detecting Epoch Changes using Large Corpora.
In-ternational Joint Conference on Natural LanguageProcessing, 347?355E.
Sagi, S. Kaufmann, B. Clark 2009.
SemanticDensity Analysis: Comparing Word Meaning acrossTime and Phonetic Space.
Proceedings of the EACL2009 Workshop on GEMS: 104?111.D.T.
Wijaya, R. Yeniterzi.
2011.
Understanding se-mantic change of words over centuries.
Proceed-ings of the 2011 international workshop on DEtect-ing and Exploiting Cultural diversiTy on the socialweb: 35?40.W.
Yih, K. Toutanova, J. Platt, C. Meek.
2011.
Learn-ing Discriminative Projections for Text SimilarityMeasures.
Proceedings of the Fifteenth Confer-ence on Computational Natural Language Learning,247?256.65
