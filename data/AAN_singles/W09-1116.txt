Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL), pages 120?128,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsGlen, Glenda or Glendale:Unsupervised and Semi-supervised Learning of English Noun GenderShane BergsmaDepartment of Computing ScienceUniversity of AlbertaEdmonton, AlbertaCanada, T6G 2E8bergsma@cs.ualberta.caDekang LinGoogle, Inc.1600 Amphitheatre ParkwayMountain ViewCalifornia, 94301lindek@google.comRandy GoebelDepartment of Computing ScienceUniversity of AlbertaEdmonton, AlbertaCanada, T6G 2E8goebel@cs.ualberta.caAbstractEnglish pronouns like he and they reliably re-flect the gender and number of the entities towhich they refer.
Pronoun resolution systemscan use this fact to filter noun candidates thatdo not agree with the pronoun gender.
In-deed, broad-coverage models of noun genderhave proved to be the most important sourceof world knowledge in automatic pronoun res-olution systems.Previous approaches predict gender by count-ing the co-occurrence of nouns with pronounsof each gender class.
While this provides use-ful statistics for frequent nouns, many infre-quent nouns cannot be classified using thismethod.
Rather than using co-occurrence in-formation directly, we use it to automaticallyannotate training examples for a large-scalediscriminative gender model.
Our model col-lectively classifies all occurrences of a nounin a document using a wide variety of con-textual, morphological, and categorical genderfeatures.
By leveraging large volumes of un-labeled data, our full semi-supervised systemreduces error by 50% over the existing state-of-the-art in gender classification.1 IntroductionPronoun resolution is the process of determiningwhich preceding nouns are referred to by a partic-ular pronoun in text.
Consider the sentence:(1) Glen told Glenda that she was wrong aboutGlendale.A pronoun resolution system should determine thatthe pronoun she refers to the noun Glenda.
Pro-noun resolution is challenging because it requires alot of world knowledge (general knowledge of wordtypes).
If she is replaced with the pronoun he in (1),Glen becomes the antecedent.
Pronoun resolutionsystems need the knowledge of noun gender that ad-vises that Glen is usually masculine (and thus re-ferred to by he) while Glenda is feminine.English third-person pronouns are grouped in fourgender/number categories: masculine (he, his, him,himself ), feminine (she, her, herself ), neutral (it, its,itself ), and plural (they, their, them, themselves).
Webroadly refer to these gender and number classessimply as gender.
The objective of our work is tocorrectly assign gender to English noun tokens, incontext; to determine which class of pronoun willrefer to a given noun.One successful approach to this problem is tobuild a statistical gender model from a noun?s asso-ciation with pronouns in text.
For example, Ge et al(1998) learn Ford has a 94% chance of being neu-tral, based on its frequent co-occurrence with neu-tral pronouns in text.
Such estimates are noisy butuseful.
Both Ge et al (1998) and Bergsma and Lin(2006) show that learned gender is the most impor-tant feature in their pronoun resolution systems.English differs from other languages like Frenchand German in that gender is not an inherent gram-matical property of an English noun, but rather aproperty of a real-world entity that is being referredto.
A common noun like lawyer can be (semanti-cally) masculine in one document and feminine inanother.
While previous statistical gender modelslearn gender for noun types only, we use documentcontext to correctly determine the current genderclass of noun tokens, making dynamic decisions oncommon nouns like lawyer and ambiguous nameslike Ford.
Furthermore, if a noun type has not yet120been observed (an unknown word), previous ap-proaches cannot estimate the gender.
Our system,on the other hand, is able to correctly determine thatunknown words corroborators and propeller-headsare plural, while Pope Formosus is masculine, usinglearned contextual and morphological cues.Our approach is based on the key observation thatwhile gender information from noun-pronoun co-occurrence provides imperfect noun coverage, it cannevertheless provide rich and accurate training datafor a large-scale discriminative classifier.
The clas-sifier leverages a wide variety of noun properties togeneralize from the automatically-labeled examples.The steps in our approach are:1.
Training:(a) Automatically extract a set of seed(noun,gender) pairs from high-quality in-stances in a statistical gender database.
(b) In a large corpus of text, find documents con-taining these nouns.
(c) For all instances of each noun in each document,create a single, composite feature vector repre-senting all the contexts of the noun in the docu-ment, as well as encoding other selected proper-ties of the noun type.
(d) Label each feature vector with the seed noun?scorresponding gender.
(e) Train a 4-way gender classifier (masculine, fem-inine, neutral, plural) from the automatically-labeled vectors.2.
Testing:(a) Given a new document, create a composite fea-ture vector for all occurrences of each noun.
(b) Use the learned classifier to assign gender toeach feature vector, and thus all occurrences ofall nouns in the document.This algorithm achieves significantly better per-formance than the existing state-of-the-art statisti-cal gender classifier, while requiring no manually-labeled examples to train.
Furthermore, by trainingon a small number of manually-labeled examples,we can combine the predictions of this system withthe counts from the original gender database.
Thissemi-supervised extension achieves 95.5% accuracyon final unseen test data, an impressive 50% reduc-tion in error over previous work.2 Path-based Statistical Noun GenderSeed (noun,gender) examples can be extracted re-liably and automatically from raw text, providingthe training data for our discriminative classifier.We call these examples pseudo-seeds because theyare created fully automatically, unlike the small setof manually-created seeds used to initialize otherbootstrapping approaches (cf.
the bootstrapping ap-proaches discussed in Section 6).We adopt a statistical approach to acquire thepseudo-seed (noun,gender) pairs.
All previous sta-tistical approaches rely on a similar observation: ifa noun like Glen is often referred to by masculinepronouns, like he or his, then Glen is likely a mas-culine noun.
But for most nouns we have no an-notated data recording their coreference with pro-nouns, and thus no data from which we can ex-tract the co-occurrence statistics.
Thus previous ap-proaches rely on either hand-crafted coreference-indicating patterns (Bergsma, 2005), or iterativelyguess and improve gender models through expec-tation maximization of pronoun resolution (Cherryand Bergsma, 2005; Charniak and Elsner, 2009).
Instatistical approaches, the more frequent the noun,the more accurate the assignment of gender.We use the approach of Bergsma and Lin (2006),both because it achieves state-of-the-art genderclassification performance, and because a databaseof the obtained noun genders is available online.1Bergsma and Lin (2006) use an unsupervisedalgorithm to identify syntactic paths along which anoun and pronoun are highly likely to corefer.
Toextract gender information, they processed a largecorpus of news text, and obtained co-occurrencecounts for nouns and pronouns connected with thesepaths in the corpus.
In their database, each noun islisted with its corresponding masculine, feminine,neutral, and plural pronoun co-occurrence counts,e.g.
:glen 555 42 32 34glenda 8 102 0 11glendale 24 2 167 18glendalians 0 0 0 1glenn 3182 207 95 54glenna 0 6 0 01Available at http://www.cs.ualberta.ca/?bergsma/Gender/121This sample of the gender data shows that thenoun glenda, for example, occurs 8 times with mas-culine pronouns, 102 times with feminine pronouns,0 times with neutral pronouns, and 11 times withplural pronouns; 84% of the time glenda co-occurswith a feminine pronoun.
Note that all nouns in thedata have been converted to lower-case.2There are gender counts for 3.1 million Englishnouns in the online database.
These counts form thebasis for the state-of-the-art gender classifier.
Wecan either take the most-frequent pronoun-gender(MFPG) as the class (e.g.
feminine for glenda), orwe can supply the logarithm of the counts as featuresin a 4-way multi-class classifier.
We implement thelatter approach as a comparison system and refer toit as PATHGENDER in our experiments.In our approach, rather than using these countsdirectly, we process the database to automaticallyextract a high-coverage but also high-quality set ofpseudo-seed (noun,gender) pairs.
First, we filternouns that occur less than fifty times and whoseMFPG accounts for less than 85% of counts.
Next,we note that the most reliable nouns should occurrelatively often in a coreferent path.
For exam-ple, note that importance occurs twice as often onthe web as Clinton, but has twenty-four times lesscounts in the gender database.
This is because im-portance is unlikely to be a pronoun?s antecedent.We plan to investigate this idea further in futurework as a possible filter on antecedent candidatesfor pronoun resolution.
For the present work, sim-ply note that a high ratio of database-count to web-count provides a good indication of the reliability ofa noun?s gender counts, and thus we filter nouns thathave such ratios below a threshold.3 After this fil-tering, we have about 45 thousand nouns to whichwe automatically assign gender according to theirMFPG.
These (noun,gender) pairs provide the seedexamples for the training process described in the2Statistical approaches can adapt to the idiosyncrasies of theparticular text domain.
In the news text from which this datawas generated, for example, both the word ships and specificinstances of ships (the USS Cole, the Titanic, etc.)
are neutral.In Wikipedia, on the other hand, feminine pronouns are oftenused for ships.
Such differences can be learned automatically.3We roughly tuned all the thresholds to obtain the highestnumber of seeds such that almost all of them looked correct(e.g.
Figure 1).
Further work is needed to determine whether adifferent precision/recall tradeoff can improve performance.. .
.stefaniesteffi grafsteinemstella mccartneystellar jaynestepdaughterstephaniestephanie hersethstephanie whitestepmotherstewardess.
.
.Figure 1: Sample feminine seed nounsfollowing section.
Figure 1 provides a portion of theordered feminine seed nouns that we extracted.3 Discriminative Learning of GenderOnce we have extracted a number of pseudo-seed(noun,gender) pairs, we use them to automatically-label nouns (in context) in raw text.
The auto-labeled examples provide training data for discrimi-native learning of noun gender.Since the training pairs are acquired from asparse and imperfect model of gender, what canwe gain by training over them?
We can regard theBergsma and Lin (2006) approach and our discrim-inative system as two orthogonal views of gender,in a co-training sense (Blum and Mitchell, 1998).Some nouns can be accurately labeled by noun-pronoun co-occurrence (a view based on pronounco-occurrence), and these examples can be used todeduce other gender-indicating regularities (a viewbased on other features, described below).We presently explain how examples are extractedusing our pseudo-seed pairs, turned into auto-labeled feature vectors, and then used to train a su-pervised classifier.3.1 Automatic example extractionOur example-extraction module processes a largecollection of documents (roughly a million docu-ments in our experiments).
For each document, weextract all the nouns, including context words within?5 tokens of each noun.
We then group the nouns by122Class=masculine String=?Lee?Contexts =?led some to suggest that ?
, who was born in???
also downloaded secret files to???
says he was just making?
?by mishandling the investigation of ?
.?.
.
.Figure 2: Sample noun training instancetheir (lower-case) string.
If a group?s noun-string isin our set of seed (noun,gender) pairs, we assign thecorresponding gender to be the class of the group.Otherwise, we discard the group.
To prevent fre-quent nouns from dominating our training data, weonly keep the first 200 groups corresponding to eachnoun string.
Figure 2 gives an example training noungroup with some (selected) context sentences.
Attest time, all nouns in the test documents are con-verted to this format for further processing.We group nouns because there is a strong ten-dency for nouns to have only one sense (and hencegender) per discourse.
We extract contexts becausenearby words provide good clues about which gen-der is being used.
The notion that nouns have onlyone sense per discourse/collocation was also ex-ploited by Yarowsky (1995) in his seminal work onbootstrapping for word sense disambiguation.3.2 Feature vectorsOnce the training instances are extracted, they areconverted to labeled feature vectors for supervisedlearning.
The automatically-determined gender pro-vides the class label (e.g., masculine for the groupin Figure 2).
The features identify properties of thenoun and its context that potentially correlate with aparticular gender category.
We divide the featuresinto two sets: those that depend on the contextswithin the document (Context features: features ofthe tokens in the document), and those that dependon the noun string only (Type features).
In bothcases we induce the feature space from the train-ing examples, keeping only those features that occurmore than 5 times.3.2.1 Context featuresThe first set of features represent the contexts ofthe word, using all the contexts in the noun group.To illustrate the potential utility of the context infor-mation, consider the context sentences for the mas-culine noun in Figure 2.
Even if these snippets wereall the information we were given, it would be easyto guess the gender of the noun.We use binary attribute-value features to flag, forany of the contexts, the presence of all words at con-text positions ?1,?2, etc.
(sometimes called col-location features (Golding and Roth, 1999)).
Forexample, feature 255920 flags that the word two-to-the-right of the noun is he.
We also provide fea-tures for the presence of all words anywhere within?5 tokens of the noun (sometimes called contextwords).
We also parse the sentence and provide afeature for the noun?s parent (and relationship withthe parent) in the parse tree.
For example, the in-stance in Figure 2 has features downloaded(subject),says(subject), etc.
Since plural nouns should be gov-erned by plural verbs, this feature is likely to be es-pecially helpful for number classification.3.2.2 Type featuresThe next group of features represent morpholog-ical properties of the noun.
Binary features flag thepresence of all prefixes and suffixes of one-to-fourcharacters.
For multi-token nouns, we have featuresfor the first and last token in the noun.
Thus we hopeto learn that Bob begins masculine nouns while inc.ends neutral ones.Finally, we have features that indicate if the nounor parts of the noun occur on various lists.
Indica-tor features specify if any token occurs on in-houselists of given names, family names, cities, provinces,countries, corporations, languages, etc.
A featurealso indicates if a token is a corporate designation(like inc. or ltd.) or a human one (like Mr. or Sheik).We also made use of the person-name/instancepairs automatically extracted by Fleischman et al(2003).4 This data provides counts for pairs suchas (Zhang Qiyue, spokeswoman) and (ThorvaldStoltenberg, mediator).
We have features for all con-cepts (like spokeswoman and mediator) and there-fore learn their association with each gender.3.3 Supervised learning and classificationOnce all the feature vectors have been extracted,they are passed to a supervised machine learn-4Available at http://www.mit.edu/?mbf/instances.txt.gz123ing algorithm.
We train and classify using amulti-class linear-kernel Support Vector Machine(SVM) (Crammer and Singer, 2001).
SVMs aremaximum-margin classifiers that achieve good per-formance on a range of tasks.
At test time, nouns intest documents are processed exactly as the traininginstances described above, converting them to fea-ture vectors.
The test vectors are classified by theSVM, providing gender classes for all the nouns inthe test document.
Since all training examples arelabeled automatically (auto-trained), we denote sys-tems using this approach as -AUTO.3.4 Semi-supervised extensionAlthough a good gender classifier can be learnedfrom the automatically-labeled examples alone, wecan also use a small quantity of gold-standard la-beled examples to achieve better performance.Combining information from our two sets of la-beled data is akin to a domain adaptation prob-lem.
The gold-standard data can be regarded ashigh-quality in-domain data, and the automatically-labeled examples can be regarded as the weaker, butlarger, out-of-domain evidence.There is a simple but effective method for com-bining information from two domains using predic-tions as features.
We train a classifier on the full setof automatically-labeled data (as described in Sec-tion 3.3), and then use this classifier?s predictions asfeatures in a separate classifier, which is trained onthe gold-standard data.
This is like the competitiveFeats domain-adaptation system in Daume?
III andMarcu (2006).For our particular SVM classifier (Section 4.1),predictions take the form of four numerical scorescorresponding to the four different genders.
Ourgold-standard classifier has features for these fourpredictions plus features for the original path-basedgender counts (Section 2).5 Since this approach usesboth automatically-labeled and gold-standard data ina semi-supervised learning framework, we denotesystems using this approach as -SEMI.5We actually use 12 features for the path-based counts: the4 original, and then 4 each for counts for the first and last tokenin the noun string.
See PATHGENDER+ in Section 4.2.4 Experiments4.1 Set-upWe parsed the 3 GB AQUAINT corpus (Vorhees,2002) using Minipar (Lin, 1998) to create our un-labeled data.
We process this data as described inSection 3, making feature vectors from the first 4million noun groups.
We train from these exam-ples using a linear-kernel SVM via the the efficientSVMmulticlass instance of the SVMstruct softwarepackage (Tsochantaridis et al, 2004).To create our gold-standard gender data, we fol-low Bergsma (2005) in extracting gender informa-tion from the anaphora-annotated portion6 of theAmerican National Corpus (ANC) (Ide and Sud-erman, 2004).
In each document, we first groupall nouns with a common lower-case string (exactlyas done for our example extraction (Section 3.1)).Next, for each group we determine if a third-personpronoun refers to any noun in that group.
If so, welabel all nouns in the group with the gender of thereferring pronoun.
For example, if the pronoun herefers to a noun Brown, then all instances of Brownin the document are labeled as masculine.
We ex-tract the genders for 2794 nouns in the ANC train-ing set (in 798 noun groups) and 2596 nouns in theANC test set (in 642 groups).
We apply this methodto other annotated corpora (including MUC corpora)to create a development set.The gold standard ANC training set is used toset the weights on the counts in the PATHGENDERclassifiers, and to train the semi-supervised ap-proaches.
We also use an SVM to learn theseweights.
We use the development set to tune theSVM?s regularization parameter, both for systemstrained on automatically-generated data, and for sys-tems trained on gold-standard data.
We also opti-mize each automatically-trained system on the de-velopment set when we include this system?s pre-dictions as features in the semi-supervised exten-sion.
We evaluate and state performance for all ap-proaches on the final unseen ANC test set.4.2 EvaluationThe primary purpose of our experiments is to de-termine if we can improve on the existing state-of-the-art in gender classification (path-based gender6Available at http://www.cs.ualberta.ca/?bergsma/CorefTags/124counts).
We test systems both trained purely onautomatically-labeled data (Section 3.3), and thosethat leverage some gold-standard annotations in asemi-supervised setting (Section 3.4).
Another pur-pose of our experiments is to investigate the relativevalue of our context-based features and type-basedfeatures.
We accomplish these objectives by imple-menting and evaluating the following systems:1.
PATHGENDER:A classifier with the four path-based gendercounts as features (Section 2).2.
PATHGENDER+:A method of back-off to help classify unseennouns: For multi-token nouns (like Bob John-son), we also include the four gender countsaggregated over all nouns sharing the first to-ken (Bob .
*), and the four gender counts overall nouns sharing the last token (.
* Johnson).3.
CONTEXT-AUTO:Auto-trained system using only context fea-tures (Section 3.2.1).4.
TYPE-AUTO:Auto-trained system using only type features(Section 3.2.2).5.
FULL-AUTO:Auto-trained system using all features.6.
CONTEXT-SEMI:Semi-sup.
combination of the PATHGENDER+features and the CONTEXT-AUTO predictions.7.
TYPE-SEMI:Semi-sup.
combination of the PATHGENDER+features and the TYPE-AUTO predictions.8.
FULL-SEMI:Semi-sup.
combination of the PATHGENDER+features and the FULL-AUTO predictions.We evaluate using accuracy: the percentage oflabeled nouns that are correctly assigned a genderclass.
As a baseline, note that always choosingneutral achieves 38.1% accuracy on our test data.5 Results and Discussion5.1 Main resultsTable 1 provides our experimental results.
The orig-inal gender counts already do an excellent job clas-sifying the nouns; PATHGENDER achieves 91.0%accuracy by looking for exact noun matches.
Our1.
PATHGENDER 91.02.
PATHGENDER+ 92.13.
CONTEXT-AUTO 79.14.
TYPE-AUTO 89.15.
FULL-AUTO 92.66.
CONTEXT-SEMI 92.47.
TYPE-SEMI 91.38.
FULL-SEMI 95.5Table 1: Noun gender classification accuracy (%)simple method of using back-off counts for the firstand last token, PATHGENDER+, achieves 92.1%.While PATHGENDER+ uses gold standard data todetermine optimum weights on the twelve counts,FULL-AUTO achieves 92.6% accuracy using nogold standard training data.
This confirms that ouralgorithm, using no manually-labeled training data,can produce a competitive gender classifier.Both PATHGENDER and PATHGENDER+ dopoorly on the noun types that have low counts inthe gender database, achieving only 63% and 66%on nouns with less than ten counts.
On thesesame nouns, FULL-AUTO achieves 88% perfor-mance, demonstrating the robustness of the learnedclassifier on the most difficult examples for previ-ous approaches (FULL-SEMI achieves 94% on thesenouns).If we break down the contribution of the two fea-ture types in FULL-AUTO, we find that we achieve89.1% accuracy by only using type features, whilewe achieve 79.1% with only context features.
Whilenot as high as the type-based accuracy, it is impres-sive that almost four out of five nouns can be classi-fied correctly based purely on the document context,using no information about the noun itself.
This isinformation that has not previously been systemati-cally exploited in gender classification models.We examine the relationship between trainingdata size and accuracy by plotting a (logarithmic-scale) learning curve for FULL-AUTO (Figure 3).Although using four million noun groups originallyseemed sufficient, performance appears to still be in-creasing.
Since more training data can be generatedautomatically, it appears we have not yet reached thefull power of the FULL-AUTO system.
Of course,even with orders of magnitude more data, the system1257075808590951001000  10000  100000  1e+06  1e+07Accuracy(%)Number of training examplesFigure 3: Noun gender classification learning curve forFULL-AUTOdoes not appear destined to reach the performanceobtained through other means described below.We achieve even higher accuracy when the outputof the -AUTO systems are combined with the orig-inal gender counts (the semi-supervised extension).The relative value of the context and type-based fea-tures is now reversed: using only context-based fea-tures (CONTEXT-SEMI) achieves 92.4%, while us-ing only type-based features (TYPE-SEMI) achieves91.3%.
This is because much of the type informa-tion is already implicit in the PATHGENDER counts.The TYPE-AUTO predictions contribute little infor-mation, only fragmenting the data and leading toover-training and lower accuracy.
On the other hand,the CONTEXT-AUTO predictions improve accuracy,as these scores provide orthogonal and hence helpfulinformation for the semi-supervised classifier.Combining FULL-AUTO with our enhanced pathgender counts, PATHGENDER+, results in the over-all best performance, 95.5% for FULL-SEMI, signif-icantly better than PATHGENDER+ alone.7 This isa 50% error reduction over the PATHGENDER sys-tem, strongly confirming the benefit of our semi-supervised approach.To illustrate the importance of the unlabeled data,we created a system that uses all features, includingthe PATHGENDER+ counts, and trained this systemusing only the gold standard training data.
This sys-tem was unable to leverage the extra features to im-prove performance; its accuracy was 92.0%, roughlyequal to PATHGENDER+ alone.
While SVMs work7We evaluate significance using McNemar?s test, p<0.01.Since McNemar?s test assumes independent classifications, weapply the test to the classification of noun groups, not instances.well with high-dimensional data, they simply cannotexploit features that do not occur in the training set.5.2 Further improvementsWe can improve performance further by doing somesimple coreference before assigning gender.
Cur-rently, we only group nouns with the same string,and then decide gender collectively for the group.There are a few cases, however, where an ambiguoussurname, such as Willey, can only be classified cor-rectly if we link the surname to an earlier instance ofthe full name, e.g.
Katherine Willey.
We thus addedthe following simple post-processing rule: If a nounis classified as masculine or feminine (like the am-biguous Willey), and it was observed earlier as thelast part of a larger noun, then re-assign the genderto masculine or feminine if one of these is the mostcommon path-gender count for the larger noun.
Weback off to counts for the first name (e.g.
Kathleen.
*) if the full name is unobserved.This enhancement improved the PATHGENDERand PATHGENDER+ systems to 93.3% and 94.3%,respectively, while raising the accuracy of ourFULL-SEMI system to 96.7%.
This demonstratesthat the surname-matching post-processor is a sim-ple but worthwhile extension to a gender predictor.8The remaining errors represent a number of chal-lenging cases: United States, group, and public la-beled as plural but classified as neutral ; spectatorclassified as neutral , etc.
Some of these may yieldto more sophisticated joint classification of corefer-ence and gender, perhaps along the lines of work innamed-entity classification (Bunescu and Mooney,2004) or anaphoricity (Denis and Baldridge, 2007).While gender has been shown to be the key fea-ture for statistical pronoun resolution (Ge et al,1998; Bergsma and Lin, 2006), it remains to beseen whether the exceptional accuracy obtained herewill translate into improvements in resolution per-formance.
However, given the clear utility of genderin coreference, substantial error reductions in gender8One might wonder, why not provide special features so thatthe system can learn how to handle ambiguous nouns that oc-curred as sub-phrases in earlier names?
The nature of our train-ing data precludes this approach.
We only include unambiguousexamples as pseudo-seeds in the learning process.
Withoutproviding ambiguous (but labeled) surnames in some way, thelearner will not take advantage of features to help classify them.126assignment will likely be a helpful contribution.6 Related WorkMost coreference and pronoun resolution papersmention that they use gender information, but fewexplain how it is acquired.
Kennedy and Boguraev(1996) use gender information produced by their en-hanced part-of-speech tagger.
Gender mistakes ac-count for 35% of their system?s errors.
Gender isless crucial in some genres, like computer manuals;most nouns are either neutral or plural and gendercan be determined accurately based solely on mor-phological information (Lappin and Leass, 1994).A number of researchers (Evans and Ora?san,2000; Soon et al, 2001; Harabagiu et al, 2001) useWordNet classes to infer gender knowledge.
Unfor-tunately, manually-constructed databases like Word-Net suffer from both low coverage and rare senses.Pantel and Ravichandran (2004) note that the nounscomputer and company both have a WordNet sensethat is a hyponym of person, falsely indicating thesenouns would be compatible with pronouns like heor she.
In addition to using WordNet classes, Soonet al (2001) assign gender if the noun has a gen-dered designator (like Mr. or Mrs.) or if the firsttoken is present on a list of common human firstnames.
Note that we incorporate such contextualand categorical information (among many other in-formation sources) automatically in our discrimina-tive classifier, while they manually specify a fewhigh-precision rules for particular gender cues.Ge et al (1998) pioneered the statistical approachto gender determination.
Like others, they considergender and number separately, only learning statis-tical gender for the masculine, feminine, and neu-tral classes.
While gender and number can be han-dled together for pronoun resolution, it might be use-ful to learn them separately for other applications.Other statistical approaches to English noun genderare discussed in Section 2.In languages with ?grammatical?
gender and plen-tiful gold standard data, gender can be tagged alongwith other word properties using standard super-vised tagging techniques (Hajic?
and Hladka?, 1997).While our approach is the first to exploit a dualor orthogonal representation of English noun gen-der, a bootstrapping approach has been applied todetermining grammatical gender in other languagesby Cucerzan and Yarowsky (2003).
In their work,the two orthogonal views are: 1) the context of thenoun, and 2) the noun?s morphological properties.Bootstrapping with these views is possible in otherlanguages where context is highly predictive of gen-der class, since contextual words like adjectives anddeterminers inflect to agree with the grammaticalnoun gender.
We initially attempted a similar systemfor English noun gender but found context alone tobe insufficiently predictive.Bootstrapping is also used in general informationextraction.
Brin (1998) shows how to alternate be-tween extracting instances of a class and inducingnew instance-extracting patterns.
Collins and Singer(1999) and Cucerzan and Yarowsky (1999) applybootstrapping to the related task of named-entityrecognition.
Our approach was directly influencedby the hypernym-extractor of Snow et al (2005) andwe provided an analogous summary in Section 1.While their approach uses WordNet to label hyper-nyms in raw text, our initial labels are generated au-tomatically.
Etzioni et al (2005) also require no la-beled data or hand-labeled seeds for their named-entity extractor, but by comparison their classifieronly uses a very small number of both features andautomatically-generated training examples.7 ConclusionWe have shown how noun-pronoun co-occurrencecounts can be used to automatically annotate thegender of millions of nouns in unlabeled text.
Train-ing from these examples produced a classifier thatclearly exceeds the state-of-the-art in gender classi-fication.
We incorporated thousands of useful butpreviously unexplored indicators of noun gender asfeatures in our classifier.
By combining the pre-dictions of this classifier with the original gendercounts, we were able to produce a gender predic-tor that achieves 95.5% classification accuracy on2596 test nouns, a 50% reduction in error over thecurrent state-of-the-art.
A further name-matchingpost-processor reduced error even further, resultingin 96.7% accuracy on the test data.
Our final systemis the broadest and most accurate gender model yetcreated, and should be of value to many pronoun andcoreference resolution systems.127ReferencesShane Bergsma and Dekang Lin.
2006.
Bootstrap-ping path-based pronoun resolution.
In COLING-ACL, pages 33?40.Shane Bergsma.
2005.
Automatic acquisition of gen-der information for anaphora resolution.
In CanadianConference on Artificial Intelligence, pages 342?353.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In COLT,pages 92?100.Sergey Brin.
1998.
Extracting patterns and relationsfrom the world wide web.
In WebDB Workshop at6th International Conference on Extending DatabaseTechnology, pages 172?183.Razvan Bunescu and Raymond J. Mooney.
2004.
Col-lective information extraction with relational Markovnetworks.
In ACL, pages 438?445.Eugene Charniak and Micha Elsner.
2009.
EM works forpronoun anaphora resolution.
In EACL.Colin Cherry and Shane Bergsma.
2005.
An expecta-tion maximization approach to pronoun resolution.
InCoNLL, pages 88?95.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In EMNLP-VLC, pages 100?110.Koby Crammer and Yoram Singer.
2001.
On the al-gorithmic implementation of multiclass kernel-basedvector machines.
Journal of Machine Learning Re-search, 2:265?292.Silviu Cucerzan and David Yarowsky.
1999.
Languageindependent named entity recognition combining mor-phological and contextual evidence.
In EMNLP-VLC,pages 90?99.Silviu Cucerzan and David Yarowsky.
2003.
Mini-mally supervised induction of grammatical gender.
InNAACL, pages 40?47.Hal Daume?
III and Daniel Marcu.
2006.
Domain adap-tation for statistical classifiers.
Journal of ArtificialIntelligence Research, 26:101?126.Pascal Denis and Jason Baldridge.
2007.
Joint determi-nation of anaphoricity and coreference using integerprogramming.
In NAACL-HLT, pages 236?243.Oren Etzioni, Michael Cafarella, Doug Downey, Ana-Maria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld, and Alexander Yates.
2005.
Unsu-pervised named-entity extraction from the web: an ex-perimental study.
Artif.
Intell., 165(1):91?134.Richard Evans and Constantin Ora?san.
2000.
Improvinganaphora resolution by identifying animate entities intexts.
In DAARC, pages 154?162.Michael Fleischman, Eduard Hovy, and AbdessamadEchihabi.
2003.
Offline strategies for online questionanswering: answering questions before they are asked.In ACL, pages 1?7.Niyu Ge, John Hale, and Eugene Charniak.
1998.
A sta-tistical approach to anaphora resolution.
In Proceed-ings of the Sixth Workshop on Very Large Corpora,pages 161?171.Andrew R. Golding and Dan Roth.
1999.
A Winnow-based approach to context-sensitive spelling correc-tion.
Machine Learning, 34(1-3):107?130.Jan Hajic?
and Barbora Hladka?.
1997.
Probabilistic andrule-based tagger of an inflective language: a compar-ison.
In ANLP, pages 111?118.Sanda Harabagiu, Razvan Bunescu, and Steven Maio-rano.
2001.
Text and knowledge mining for coref-erence resolution.
In NAACL, pages 55?62.Nancy Ide and Keith Suderman.
2004.
The AmericanNational Corpus first release.
In LREC, pages 1681?84.Christopher Kennedy and Branimir Boguraev.
1996.Anaphora for everyone: Pronominal anaphora resolu-tion without a parser.
In COLING, pages 113?118.Shalom Lappin and Herbert J. Leass.
1994.
An algo-rithm for pronominal anaphora resolution.
Computa-tional Linguistics, 20(4):535?561.Dekang Lin.
1998.
Dependency-based evaluation ofMINIPAR.
In LREC Workshop on the Evaluation ofParsing Systems.Patrick Pantel and Deepak Ravichandran.
2004.
Auto-matically labeling semantic classes.
In HLT-NAACL,pages 321?328.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning syntactic patterns for automatic hypernymdiscovery.
In NIPS, pages 1297?1304.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics, 27(4):521?544.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun.
2004.
Support vec-tor machine learning for interdependent and structuredoutput spaces.
In ICML.Ellen Vorhees.
2002.
Overview of the TREC 2002 ques-tion answering track.
In Proceedings of the EleventhText REtrieval Conference (TREC).David Yarowsky.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In ACL, pages189?196.128
