Special Section on Restricted-DomainQuestion AnsweringQuestion Answering in Restricted Domains:An OverviewDiego Molla?
?Macquarie University, AustraliaJose?
Luis Vicedo?University of Alicante, SpainAutomated question answering has been a topic of research and development since the earliest AIapplications.
Computing power has increased since the first such systems were developed, andthe general methodology has changed from the use of hand-encoded knowledge bases about simpledomains to the use of text collections as the main knowledge source over more complex domains.Still, many research issues remain.
The focus of this article is on the use of restricted domainsfor automated question answering.
The article contains a historical perspective on questionanswering over restricted domains and an overview of the current methods and applicationsused in restricted domains.
A main characteristic of question answering in restricted domains isthe integration of domain-specific information that is either developed for question answering orthat has been developed for other purposes.
We explore the main methods developed to leveragethis domain-specific information.1.
IntroductionThere has been an interest in representing knowledge and automatically processingit from the time of the first generation of computers.
This interest has increased fromthe end of the 1980s to become an urgent necessity.
Decisive factors in this increase ofinterest are an unprecedented growth in the amount of digital information available, anexplosion of growth in the use of computers for communications, and the increasingnumber of users that have access to all this information.These circumstances have fostered research into information systems that can facil-itate the localization, retrieval, and manipulation of these enormous quantities of data.Question Answering (QA) is one of these research fields.In this article, QA is defined as the task whereby an automated machine (such asa computer) answers arbitrary questions formulated in natural language.
QA systemsare especially useful in situations in which a user needs to know a very specific piece ofinformation and does not have the time?or just does not want?to read all the availabledocumentation related to the search topic in order to solve the problem at hand.?
Division of Information and Communication Sciences, Macquarie University, New South Wales 2109,Australia.
E-mail: diego@ics.mq.edu.au.?
Departamento de Lenguajes y Sistemas Informa?ticos, Universidad de Alicante, Campus de San Vicentedel Raspeig, Apdo.
99.
Alicante, Spain.
E-mail: vicedo@dlsi.ua.es.Submission received: 2 June 2006; revised submission received: 15 October 2006; accepted for publication:23 October 2006.?
2007 Association for Computational LinguisticsComputational Linguistics Volume 33, Number 1Research in QA has been developed from two different scientific perspectives,artificial intelligence (AI) and information retrieval (IR).Work in QA since the early stages of AI has led to systems that respond to questionsusing the knowledge encoded in databases as an information source.
Obviously, thesesystems can only provide answers concerning the information previously encoded inthe database.
The benefit of this approach is that having a conceptual model of theapplication domain represented in the database structure allows the use of advancedtechniques such as theorem proving and deep reasoning in order to address complexinformation needs.Currently we are witnessing a surge of activity in the area from the perspective ofIR, initiated by the Question Answering track of TREC1 in 1999 (Voorhees 2001).
Sincethen, increasingly powerful systems have participated in TREC and other evaluationfora such as CLEF2 (Vallin et al 2005) and NTCIR3 (Kando 2005).
From this perspective,question answering focuses on finding text excerpts that contain the answer withinlarge collections of documents.
The tasks set in these conferences have molded a specifickind of question answering that is easy to evaluate and that focuses on the use of fastand shallow methods that are generally independent of the application domain.
Inother words, current research focuses on text-based, open-domain question answering.Both trends have developed in parallel and represent the opposite ends of a spec-trum connecting what we might label as structured knowledge-based and free text-based question answering.
Whereas structured knowledge-based QA systems are welladapted to applications managing complex queries in a very structured informationenvironment, the kind of research developed in TREC, CLEF, and NTCIR is probablybetter suited to broad-purpose generic applications dealing with simple factual ques-tions such as World Wide Web?based question answering.However, both approaches have serious disadvantages when they attempt to tackleimportant real applications that handle complex questions by combining domain-specific information typically expressed in different sources (structured, semistructured,unstructured, etc.)
using reasoning techniques.
Examples of such applications are:Interfaces to machine-readable technical manuals: Many software applications arevery complex and they are accompanied by extensive documentation.
A QA sys-tem that finds specific answers to a user?s question based on such documentationwould be very useful.Front-ends to knowledge sources: Many disciplines and areas of human activity havetheir own specific knowledge sources.
An example is the medical domain, which,as we shall see in this article, contains a wealth of technical information andresources that can be used for a QA system targeting this kind of information.Help desk systems in large organizations: Help desk staff in large organizations needto quickly satisfy the customer?s need for information.
Although many suchrequests for information will be found in FAQs available to the help desk staff,there will always be requests that are unique and that require staff to have accessto fast methods to find the relevant information.
End systems tailored to such staff(who can be trained) are different from QA systems designed for the end user, butthey still need to leverage the organization domain.1 Text REtrieval Conference (http://trec.nist.gov/).2 Cross Language Evaluation Forum (http://clef.iei.pi.cnr.it/).3 NII-NACSIS Test Collection for IR Systems (http://research.nii.ac.jp/ntcir/).42Molla?
and Vicedo Question Answering in Restricted Domains: An OverviewIt might be argued that focusing research on restricted domains is limiting becausethe results are too specific and not open to generalization.
This may have been the casewith early work in natural language processing (NLP), which focused on restricteddomains simply because of limitations in computing power and in theoretical cov-erage.
This is not the case nowadays.
The availability of comprehensive and reliableresources in complex domains enables interesting and fruitful research to be carried outin restricted-domain natural language processing.In short, research in restricted-domain question answering (RDQA) addressesproblems related to the incorporation of domain-specific information into currentstate-of-the-art QA technology with the hope of achieving deep reasoning capabili-ties and reliable accuracy performance in real world applications.
In fact, as a not-too-long-term vision, we are convinced that research in restricted domains will drivethe convergence between structured knowledge-based and free text-based questionanswering.In this article we survey past and current work on question answering in restricteddomains.
In the process we will highlight the advantages of developing systems basedon restricted domains.
Section 2 provides a historical note on question answering,with an emphasis on restricted domains, and focusing mainly on early work.
Sec-tion 3 presents desirable characteristics of restricted domains for the development ofNLP research in general, and question answering in particular.
Section 4 commentson some of the main factors that distinguish question answering in an open domainfrom question answering in a restricted domain.
Section 5 focuses on the use ofdomain-specific resources for question answering.
Section 6 outlines current restricted-domain question answering methods.
Section 7 notes the main aspects to considerwhen building a restricted-domain question answering system.
Section 8 introducesthe articles in this special section of the journal, and finally Section 9 presents someconclusions.2.
Early WorkTwo examples of early question-answering systems are BASEBALL and LUNAR.
BASE-BALL answered questions about baseball games played in the American league overone season (Green et al 1961), and LUNAR answered questions about the analysisof rock samples from the Apollo moon missions (Woods 1997).
Both systems werevery successful in their chosen domains.
In particular, LUNAR was demonstrated ata lunar science convention in 1971, where it was able to answer 90% of questionsposed by geologists without prior instructions with regard to the allowable phrasing(Hirschman and Gaizauskas 2001).
Both LUNAR and BASEBALL are examples of whathave been described as natural language interfaces to databases, that is, their sourceof information was a database that contained the relevant information about the topic.The user?s question was converted into a database query, and the database output wasgiven as the answer.
The very specific nature of the domains enabled the construction ofappropriately comprehensive databases, and a domain-specific question analysis thatenabled a mapping from the meaning of the user?s question onto the correspondingdatabase query.LUNAR and BASEBALL are only two of the most salient examples of early work onquestion answering, but there were many other such systems.
Simmons?s (1965) surveydescribed a variety of early QA systems.
Most of these focused on restricted domains bydeveloping a database of knowledge and providing a natural language interface.
Still,43Computational Linguistics Volume 33, Number 1many of these early systems (including LUNAR and BASEBALL) were no more than?toy systems?
that focused on very limited domains.
Those early systems that used acorpus of text as the inherent information source typically processed small volumes oftext and would rely on a human to disambiguate the corpus sentences or convert themto a simplified version of English.During the 1970s and 1980s there was intensive research on the development of the-oretical bases for computational linguistics.
This research prompted the development ofQA systems on domains that were more complex than those of the earlier systems.
Themain goal of this research was to use QA as an application framework within whichgeneral NLP theories could be tested.
This work culminated in large and ambitiousprojects such as the Berkeley Unix Consultant (Wilensky et al 1994).The Berkeley Unix Consultant project (UC) used the domain of the UNIX operat-ing system to develop a help system that combined research in planning, reasoning,natural language processing, and knowledge representation.
The user?s question wasanalyzed and a meaning representation corresponding to the question was encoded ina knowledge representation formalism.
Then, UC hypothesized the actual informationneeds of the user by consulting the user model and applying goal analysis.
The answerwas tailored to the user?s expertise and goals.
The sample dialogues provided werecertainly impressive.
However, no transcripts of real-world dialogues were providedand therefore it cannot be determined whether the methods and theories developed inUC were robust enough for practical use.Most of the early work attempted to implement QA systems from the early per-spective of AI or computational linguistics.
As noted earlier, due to the limitationsof the time, question answering focused on restricted domains.
A turning point wasreached in 1999, with the introduction of the QA track in the TREC (Voorhees 1999).
Thepopularity of the QA track in TREC has enabled research on QA from an IR perspective.From the IR community?s point of view, the task of question answering is reducedto the task of finding the text that contains the answer to the question and extractingthe answer.
Text documents are viewed as a source of unstructured information thatis structured by indexing it.
Indexing the documents makes it feasible to locate thefragments that are closely related to the question terms by applying term-matchingtechniques.A consequence of this new perspective is the application of domain-independentmethods, allowing what has been called open-domain question-answering.
This ap-proach is largely used in current QA systems.
It is beyond the scope of this article tosurvey the techniques and systems used in open-domain QA; the interested reader isreferred to the proceedings of TREC, which are available on-line.4 Instead, in the sub-sequent sections we will review current approaches to question answering in restricteddomains.
But before that, let us analyze what a restricted domain is.3.
Characteristics of Restricted DomainsThe nature of a particular restricted domain affects the kinds of questions asked andanswers that can be expected.
Consequently, different restricted domains benefit fromdifferent QA techniques.
Some domains are particularly appropriate for the develop-ment of question answering systems.
Minock (2005) lists three desiderata for a restricted4 http://trec.nist.gov.44Molla?
and Vicedo Question Answering in Restricted Domains: An Overviewdomain within the context of World Wide Web?based question answering?that is,question answering that relies on documents taken from the World Wide Web as themain source for finding answers.
According to Minock, a restricted domain must meetthe following desiderata:1.
It should be circumscribed.2.
It should be complex.3.
It should be practical.The same desiderata apply, with some modifications, to restricted domains on questionanswering that is not World Wide Web?based.3.1 CircumscriptionMinock?s original description of a circumscribed domain is motivated by the user?s needto know what to expect of the World Wide Web?based QA system at hand and to knowwhat questions are appropriate to the domain at hand.
An example of a domain thatwould fare low in this desideratum is that of current events, because the user mighthave difficulty in ascertaining what questions can be asked.
An example of a gooddomain according to this desideratum would be a science domain such as astronomy orchemistry.If the QA system is not World Wide Web?based and, furthermore, is intended foruse within a corporation, however, users do not face the problem of wondering whatquestions are appropriate.
Rather, a more important motivation for a circumscribeddomain is the need for clearly defined knowledge sources.
The range of techniques usedin a restricted domain should not need to use extensive knowledge from outside thechosen domain.
Rather, a domain that has authoritative and comprehensive resources isto be preferred.
Examples of resources include actual databases containing the requiredinformation.It is natural to assume that the more restricted the domain is and the more circum-scribed it is, the more possible it is to obtain such comprehensive databases.
For morecomplex domains, useful resources are terminology databases and domain ontologies.An added value is the existence of well-accepted terminology and ontology standards.3.2 ComplexityA domain should be complex enough to warrant the use of a QA system.
This mayseem an obvious statement, but it is important to bear in mind that, in a desire to finda domain that is fully circumscribed, one might attempt to develop a QA system in adomain where a simple list of facts or a FAQ would be sufficient to satisfy the user?sneed for information.
There is no need for a QA system in such domains.Developing a system for a simple domain does not advance research in any signif-icant area.
Such domains are to be left to those who are more interested in capitalizingon current research advances to develop practical applications, rather than extendingcurrent research boundaries.
In general, the more complex a domain is, the more inter-esting it becomes for the researcher and the more useful it presumably is to the user.There is a balance to be achieved between the need for a complex domain andthat of a circumscribed domain, because these two desiderata are in conflict.
At somepoint, if a domain is complex enough, it becomes difficult to manage and there is a45Computational Linguistics Volume 33, Number 1higher probability of requiring resources belonging to other domains; in other words,the domain becomes less circumscribed.3.3 PracticalityPracticality is an important desideratum to consider when developing a QA system.The domain should be of use to a relatively large group of people.
Otherwise one riskswasting effort on a system that nobody would use, such as for an artificially constructedtoy domain.
The choice of domain affects the kind of users to target.
Therefore, for eachdomain it is important to determine the kinds of questions asked in the specific domain(question style and terminology used are two important factors to consider), the sort ofinformation that is most commonly requested, and the level of detail expected in theanswers.4.
Open-Domain versus Restricted-Domain Question AnsweringThere are various factors that determine the best techniques to use in restricted-domainquestion answering, and whether techniques used in open-domain question answeringwould be effective in restricted-domain question answering.
In this section we willbriefly introduce some of these factors.4.1 The Size of the DataA well-known method used in open-domain QA is derived from redundancy-basedtechniques.
These techniques were first discussed by Brill et al (2001), who observedthat, as the size of the text corpus increases, it becomes more likely that the answerto a specific question can be found with data-intensive methods that do not require acomplex language model.
Thus, if the question is Who killed Abraham Lincoln?, it is easierto find the answer in John Wilkes Booth killed Abraham Lincoln than in John Wilkes Boothis perhaps America?s most infamous assassin.
He is best known for having fired the bullet thatended Abraham Lincoln?s life.
Redundancy-based techniques are likely to have a weakerimpact in restricted-domain QA, especially in the case of domains with relatively smallcorpora.Domains with relatively small corpora will naturally have relatively fewer sen-tences that contain the answer.
In those domains it becomes important to use so-phisticated language processing techniques, including the resolution of inferences, ifnecessary, to find the answer.
The haystack of a restricted domain is relatively small,but it also has fewer needles.Note that, if the size of the corpus is relatively small, it becomes possible to applycomplex NLP techniques to the complete corpus off-line.
Nowadays it is possible toparse the entire corpus used in the QA track of TREC and to extract all its named entities.It is therefore feasible to parse and extract the named entities of corpora of restricteddomains.4.2 Domain ContextThe actual domain provides a specific context to the question-answering process.
Con-sequently the set of senses available to words is typically a subset of all the availablesenses.
The impact of word-sense disambiguation is possibly reduced in RDQA, though46Molla?
and Vicedo Question Answering in Restricted Domains: An Overviewit should be noted that some words would still have several senses available andtherefore word-sense disambiguation still plays a role.
We are not aware of any studieson the impact of word-sense disambiguation on restricted domains and certainly thisarea is worth exploring.The kinds of questions asked in a restricted domain are naturally different fromthose asked in an open domain.
Users of a restricted domain, and especially users whoare experts in the domain, will use specific terminology and will pose rather technicalquestions that require very specific answers.
Questions asked by such users are muchmore complex than those of casual users of open-domain QA systems.
This is certainlythe case in the medical domain, as the articles included in this special section showconvincingly.
The challenges related to solving those questions are certainly worth theeffort in pursuing research in RDQA.4.3 ResourcesAn important difference between open-domain and restricted-domain QA is the ex-istence of specific resources for restricted domains that can be used.
In the followingsections we will comment on these resources.5.
Use of Domain-Specific ResourcesIntuitively, a good method for answering questions in a restricted domain needs toleverage any information available about the domain in order to be able to addressusers?
information needs with the specificity and depth required.The type of information available for a particular domain is intrinsically relatedto the complexity of the domain and the particular needs of the domain users.
Hence,domain knowledge representation can range from simple lists of specialized entitiesand terms to high-level ontologies where all the domain knowledge is unambiguouslyrepresented.Within computer science, an ontology is usually defined as a formal explicit de-scription of concepts in the domain of discourse, together with their attributes, roles,restrictions, and other defining features (Noy and McGuinness 2001).
The relationsbetween the concepts are also expressed formally.
The two most common relationsshown in an ontology are subclass (?is a subtype of?)
and instance (?is an instance of?
),but other relations can be included, such as meronymy (?part of?)
and, in the case ofWordNet (Fellbaum 1998), entailment.For the purposes of this article, we will refer to all the possible domain knowledgerepresentations as ontological resources.Generally, domains that are complex, circumscribed, and practical are likely tohave available ontological resources that can be used to quick-start QA research anddevelopment.
These resources are typically developed for the domain users to helpthem categorize the domain knowledge and agree on notational standards, and to helpthem retrieve information using conventional information retrieval applications.5.1 Open-Domain OntologiesThere are ontologies that are designed without a specific domain in mind.
These arereferred here as open-domain ontologies.
A widely used open-domain ontology isWordNet (Fellbaum 1998).
WordNet contains a large list of open-class words grouped47Computational Linguistics Volume 33, Number 1into synonym sets (the ?synsets?).
A range of synset relations is encoded, such as hyper-nymy/hyponymy, meronymy, and entailment.
WordNet alo includes word relations,such as antonymy.
A departure from ontologies like Cyc (Lenat and Guha 1990) is thatWordNet does not include formal definitions of the features of the objects.
Still, for thepurposes of this article, WordNet is an ontology.
This view is supported by its use inmany systems, including open-domain question answering systems (Moldovan andNovischi 2002).Open-domain ontologies like WordNet, however, are of limited use for QA in re-stricted domains.
This is so because the information is unlikely to be well balanced withrespect to the chosen domain.
For example, parts of open-domain ontologies are toocoarse-grained for specific restricted domains, whereas other parts are too fine-grained.And worse, open-domain ontologies may contain information that is not appropriatefor specific restricted domains.Open-domain ontologies are too coarse-grained.
Restricted domains, and especially tech-nical domains, abound in terms that are specific to the domain and largely unknownin other domains.
Open-domain ontologies typically do not include these specificterms.
In some domains, however, these terms may be used widely.
Consequently,open-domain ontologies will need to be complemented with terminology lists or localontologies.Open-domain ontologies are too fine-grained.
Open-domain ontologies that map words toconcepts, as is the case with WordNet, face the problem of polysemous words, that is,words with multiple meanings.
However, those ambiguous words are usually unam-biguous in restricted domains.
Take the noun file.
WordNet 1.7.1 lists four meanings,shown in Table 1.
Of the four meanings, only the first one (?a set of related records kepttogether?)
is relevant within domains related to software development.
Open-domainontologies therefore risk overloading the system with concepts that are rarely, if ever,used within the chosen restricted domain.Open-domain ontologies may have information that is not appropriate for the domain.
Themost damaging property of open-domain ontologies is that they may contain informa-tion that is misleading in certain restricted domains.
Restricted domains notoriouslyoverload some terms commonly used outside their domain.
For example, the usualmeaning of the verb print is to render something into printed matter.
However, withinthe domain of computer programming, the verb print usually means to display onthe computer monitor.
Consequently, a system that uses an open-domain ontologywould possibly misinterpret the meaning of print in the question Which C++ instruc-Table 1Four senses of the noun file in WordNet 1.7.1.Sense Gloss1.
A set of related records (either written or electronic) kept together2.
A line of persons or things ranged one behind the other3.
A container for keeping papers in order4.
A steel hand tool with small sharp teeth on some or all of its surfaces; used forsmoothing wood or metal48Molla?
and Vicedo Question Answering in Restricted Domains: An Overviewtion prints words onto the screen?
This sense of print is not available in WordNet5 andtherefore it is not possible to apply word-sense disambiguation techniques to find theappropriate sense.5.2 Uses of Ontological ResourcesOntological resources define a common vocabulary for accessing information in a do-main and this makes it easier to manage domain information as regards the following(Noy and McGuinness 2001): sharing common understanding of the structure of information amongpeople or software agents enabling reuse of domain knowledge making domain assumptions explicit separating domain knowledge from the operational knowledge making possible different analysis of the domain knowledgeAmong theses concerns, enabling the separation of domain knowledge and operationalknowledge is probably the most valuable characteristic for QA purposes.
This fact al-lows the separation of the process of representing the concepts expressed in a documentfrom the use of the relations between concepts for deduction or reasoning processes.On the other hand, formalisms, theories, and algorithms either designed for domaindocument representation or reasoning may be made independent from the chosendomain ontology and can also be applied to different domains, thus enhancing systemportability between domains.Research on using ontologies for QA has benefited from the following: The increasing availability of ontologies encoding different kinds ofknowledge.
We can find ontologies ranging from general worldknowledge resources, such as WordNet (Fellbaum 1998), EuroWordNet(Vossen 1998), Cyc (Lenat and Guha 1990), and FrameNet (Johnson andFillmore 2000, to very specific domain knowledge, such as the medicaldomain (Lindberg, Humphreys, and McCray 1993) or the chemistrydomain (Barker et al 2004). Steady achievements in knowledge representation and reasoning (KR&R)techniques, which enable precise representation of both domain-relatedinformation and domain-related reasoning and deduction mechanisms(Barker et al 2004). Advances in the development of modular and robust natural languageprocessing systems (Abney 1996; Hobbs et al 1997; Basili and Zanzotto2002) in the context of the use of ontological resources for both textualinterpretation and representation (Ait-Mokhtar and Chanod 1997) anddatabase access (Popescu, Etzioni, and Kautz 2003).5 This was the case for the on-line version of WordNet 2.1 (http://wordnet.princeton.edu/) on8 October 2006.49Computational Linguistics Volume 33, Number 1 Increasing success in the development of ontology-based QA frameworkswhere answers are derived from reasoning processes over questions anddocument ontological representations (Zajac 2001).Ontology-based question answering systems attack the answer-retrieval problemby means of an internal unambiguous knowledge representation.
Both questions andknowledge are represented using specific knowledge models based on ontological en-tities, concepts, and relations.
The answering of questions is performed by applyingdifferent reasoning and proof techniques that allow the detection of textual entail-ment, which is useful in determining whether a given sentence answers a particularquestion.6.
The State of the Art in RDQACurrent work on QA in restricted domains tends to exploit the characteristics of thedomain in order to improve the accuracy and practicability of the system.
This isdone largely by determining the types of information needs in the chosen domain, bystudying the format of questions asked, and by leveraging the ontological informationavailable in the domain.Some domains are complex domains that have a history of users attempting tostreamline the process to find specific information.
An example of such a domain is thatof medicine.
It is important for a doctor to quickly diagnose the illness of a patient, andto determine if a patient is developing a new variation of an illness that has occurredbefore.
Given the importance of finding the correct diagnosis and treatment, the domainof medicine has developed trusted resources that can be used for question answering inthis domain.
Zweigenbaum (2003) provides examples of resources for terminology andcorpora of authoritative material.Demner-Fushman and Lin (2005) operationalize knowledge extraction for populat-ing a database with PICO (Population, Intervention, Comparison, and Outcome) ele-ments from medical abstracts obtained from MEDLINE.
PICO structures are the framesused for evidence-based medicine.
Sang, Bouma, and de Rijke (2005a) describe severalstrategies for populating a database with medical information related to diseases, symp-toms, and treatments, which is automatically extracted from medical texts.
This struc-tured information is used for answering medical-related questions.
Niu and Hirst (2004)describe a method for identifying semantic classes and the relations between them inmedical texts.
This approach is able to build an ontology for the domain automatically.Yu, Sable, and Zhu (2005) present an algorithm to classify medical questions basedon a well-known hierarchical evidence taxonomy (Ely et al 2002).
Rinaldi, Dowdall,and Schneider (2004) describe the difficulties in adapting an existing RDQA systemdeveloped for assisting questions on UNIX technical manuals (Molla?
et al 2000) to theGenomics domain.Benamara (2004) reports in detail on one of the currently most advanced RDQAsystems.
WEBCOOP is a logic-based system that integrates knowledge representationand advanced reasoning procedures to generate responses to natural queries.
Thissystem has been developed for the tourism domain.As for any knowledge intensive application, using ontologies for QA has as alimitation the restrictions imposed by the underlying knowledge representation models.Thus, in the following subsections we will focus on the efforts that are being employedfrom both historical trends in QA research (structured knowledge?based and free-text?50Molla?
and Vicedo Question Answering in Restricted Domains: An Overviewbased perspectives) to provide systems with deep reasoning capabilities supported byontological domain information.
We introduce several works that aim at combining theuse of various ontologies and we also describe current attempts to separate the process-ing of domain-dependent information from generic domain-independent informationwith the goal of increasing portability across domains.6.1 Ontologies and Structured Knowledge?Based QAAs noted earlier, the first QA systems focused on the development of natural languageinterfaces to databases (NLIDBs).
This is a natural approach to follow in circumscribeddomains that are not very complex.
The idea is to produce a structured informationresource containing comprehensive information on the contents of the domain.
Thisinformation resource is produced before any question is asked and is queried over whenthe user asks a question.There is a wealth of research in the area of NLIDBs and it is not within the scopeof this article to survey this important area of research.
Rather, we refer the reader toAndroutsopoulos, Ritchie, and Thanisch (1995).
Work in NLIDBs assumes an existingdatabase that is queried over.
If the database does not exist, it is created by using meth-ods based on information-extraction technology.
The aim is to extract all the informationthat might be used as an answer.
A clear candidate is the use of named entities, but thecreation of templates has also been tried in open domains (Srihari and Li 2000) andrestricted domains (Weischedel, Xu, and Licuanan 2004).There are other systems that support this kind of knowledge-based question-answering, including some dealing with questions unanticipated at the time of systemconstruction.
These include the AP Chemistry question-answering system (Barker et al2004), Cyc (Lenat and Guha 1990), the Botany Knowledge Base system (Porter et al1988), the two systems developed for DARPA?s High Performance Knowledge Base(HPKB) project (Cohen et al 1988), and the two systems developed for DARPA?s RapidKnowledge Formation (RKF) project (Schrag et al 2002).6.2 Ontologies and Free-Text?Based QAIn this approach, users pose questions in natural language to knowledge bases madeup of documents also written in natural language.
In this case ontologies are used todefine a language in which questions and documents can be represented and exploitedto obtain the required answers.
The translation from natural language to the internalrepresentation is automatic; this presupposes fully unambiguous representations thatare currently beyond our capabilities.The main characteristic of these approaches is the intensive use of an ontologyin the different parts of the question answering system.
For instance, the ontology isused in the representation of the question and the documents, in the refinement ofthe initial query, in the reasoning processes carried out over the classes and subclassesfrom the ontology, and in the similarity algorithms employed for answer retrieval andextraction.Zajac (2001) presents an ontology-based semantic framework for question answer-ing where both questions and source texts are parsed into underspecified semanticexpressions where names of the semantic atoms and predicates are defined in an in-terlingual ontology.
Answer retrieval is done using subsumption and unification, andqueries are expanded using ontological rules.51Computational Linguistics Volume 33, Number 16.3 Integrating Heterogeneous Sources of InformationMore interesting than using a single database is the combination of databases withsemistructured information (such as text with some XML markup) or even unstruc-tured information (i.e., plain text).
This has been proposed for World Wide Web?basedquestion answering (Lin 2002), given the availability of pockets of information storedin databases on the World Wide Web.
The idea is to analyze the question and find therelevant database among a preselected list if this is possible.
If there are no suitable data-bases or it is not possible to determine the appropriate database query, then standardquestion-answering techniques are applied using the World Wide Web as a resource.The same strategy can be applied to question answering over restricted domains bykeeping a set of relevant databases and a corpus of documents to query over in case thequestion is not covered in the databases.There are two main issues that need to be handled by a QA system that relies onheterogeneous sources:Interface: The resources in each domain will have their own formats and interfaces,which must be unified by the QA system.Selection: The QA system needs to determine the actual resource within which to lookfor the answer.Given that the actual domain-specific resources range from simple word lists to struc-tured databases, interfacing to them is by no means simple.
Two approaches are envis-aged (Lin 2002):Slurp: Extract all the information from the multiple sources and create a databasecontaining all the information.
By having all the information in a unified database,the interfacing problem is easily solved and it is even possible to handle queriesthat the original databases were unable to handle (such as queries that rely onknowledge from various domains).
This method is practical if the actual databasesare available locally and their format is known.
However, some databases areavailable as on-line resources only and any attempt to slurp all their informationthrough methodical queries may be frowned upon by the database owners.Wrap: Provide an application program interface (API) to the individual databases.
Theset of databases can be seen as a federated database system.
The choice of pro-viding an API has the obvious disadvantage that it may not be possible to devisea unified API that makes the best of what is available in the domain resources.The compromise would be a set of APIs that may or may not be able to query theresource with the full power of the original resource interface.A step beyond portable QA systems is to build a meta-domain QA system.
A meta-domain QA system specializes in several restricted domains by acting as a knowledgebroker to specialized domain modules.
An example of such a system is START (Katz1997), which currently is a World Wide Web?based QA system that uses a wide rangeof structured data available on the Internet.MOSES (Basili et al 2004) is an ontology-based QA system in which users posequestions in natural language to knowledge bases of facts extracted from a federationof Web sites and organized in topic map repositories.
This approach uses an ontology-based methodology to search, create, maintain, and adapt semantically structuredWorld Wide Web content according to the vision of the Semantic Web in a domainrelated to university World Wide Web sites.52Molla?
and Vicedo Question Answering in Restricted Domains: An OverviewAQUA (Vargas-Vera, Motta, and Domingue 2003) combines knowledge encodedin a database with domain-related documents through an ontology that describes aca-demic life.
AQUA tries to answer a question using its knowledge base.
If a query cannotbe satisfied via the database, it tries to find an answer on domain-related World WideWeb pages.The L&C system (Ceusters, Smith, and Van Mol 2003) is one of the most ambitiousworks in the medical domain; it tries to combine authoritative medical knowledgewith information about patients.
The information needed by physicians is of two sorts.First, there is information concerning patients, such as the changes in Mr. X?s bloodpressure over the past three days, or the substances to which Ms. Y is allergic.
Second,there is what can be defined as medical knowledge, that is, the information found intextbooks, journal articles, clinical studies, and so on.
The final objective of this workis to combine these two types of information so that the QA system, when asked,for example, whether it is safe to give the patient an additional shot of a hypoten-sive agent in order to reduce bleeding, would respond with: Can you please wait for45 seconds because the patient?s blood pressure has been dropping slightly already for the last2 minutes?6.4 Porting to Other DomainsDeveloping a system in a specific domain could be time-consuming.
It is natural to thinkof ways to reuse technologies (or even code) in QA systems from other domains or fromopen-domain QA systems.
A topic that is intimately related is that of portability to otherdomains.Some question-answering systems are designed with the goals of re-usability andportability in mind.
These are generic systems that can be localized to specific domains.For example, JAVELIN (Nyberg et al 2005) is an open-domain QA system that can beextended to focus on restricted domains.
Special care was taken to leverage ontologiesspecific to the chosen domain by developing a Java API.
The specific ontological in-formation extracted is the type hierarchy and sets of synonyms (AKA, or ?also knownas?
extraction).
Another example that demonstrates efforts in adapting an open-domainQA system to a specialized geographical environment can be seen in the work by Ferre?sand Rodr?
?guez (2006).Another approach, developed by Frank et al (2005), is based on the use of struc-tured knowledge sources.
This approach applies deep linguistic analysis to the questionand transforms it into an internal representation based on conceptual and semanticcharacteristics.
This representation is domain-independent and provides a natural in-terface to the underlying knowledge databases.
This approach has been implementedas a prototype for two application domains: the domain of Nobel prize winners and thedomain of language technology.Another issue is that of localizing an open-domain QA system to a restricted do-main.
Nyberg et al (2005) provides a case study that describes the problems in adaptingan existing open-domain QA system to be able to deal with knowledge from existingdomain ontologies.7.
Building a Restricted-Domain QA System: Main ConsiderationsIt is difficult to imagine a general methodology for the development of an RDQAsystem.
On the one hand, current systems are overly influenced by the specific53Computational Linguistics Volume 33, Number 1characteristics and requirements of the domains, from the different types of questionsto be answered to the heterogeneity of the knowledge available for the domain.
On theother hand, the known methodological proposals (Minock 2005) are so general that theycould be used to design any kind of information system.Rather than propose a design methodology, we want to emphasize the main pointsto be taken into consideration when designing a QA system for a specific domain.
Thesepoints are related to the analysis and modeling of the domain information and theselection of the appropriate technology required by the QA system.
They can be listedas follows: domain query system analysis domain knowledge selection domain knowledge acquisition and representation system interface design technological requirements selectionDomain query system analysis: Knowing in detail all the different ways users ask forinformation is a prerequisite for being effective in a restricted-domain scenario.Questions need to be analyzed, classified, and associated with the different typesof information the users request.
The kinds of questions in a restricted domainmay vary from general open-domain factoid and definition questions to veryspecial kinds of questions that depend on the selected domain.Domain knowledge selection: The amount and type of authoritative knowledgeavailable for computational treatment is especially variable across differentdomains.
For instance, there are plenty of resources for biomedical (Zweigenbaum2003) or technical related domains, whereas, on the other hand, less populardomains (such as the legal domain) have minimal elaborated knowledge buthave the advantage of enormous quantities of raw text.
Domain informationcan be represented in different formats: from unstructured plain text documentsto semi-structured (e.g., templates, SGML annotated text) or highly structuredknowledge encoded in large databases and authoritative ontologies.
Selecting theappropriate domain knowledge resources in each particular case is an importantaspect in the design of an RDQA.Domain knowledge acquisition and representation: Using the domain knowledgefor QA purposes requires the definition of an internal representation modelthat allows the integration or combination of the different information sourcesavailable for the domain.
The complexity of the representation model used willbe proportional to the complexity of the information sources needed for encodingdomain knowledge.
The model selected for domain knowledge representationwill also determine the kind of operational processes and reasoning techniquesallowed in the domain.System interface design: In order to obtain a natural mode of communication betweenusers and the system, the interaction needs to be tailored according to thedomain characteristics and the user requirements.
Usually, natural language(NL) interfaces are preferred because they allow fluent communication betweenthe users and the system.
Nevertheless, as current natural language processing54Molla?
and Vicedo Question Answering in Restricted Domains: An Overviewtechnologies do not allow the automatic translation of natural language text intoa fully unambiguous content representation, NL interfaces may be sometimessubstituted by template-like interfaces or unambiguous formal outputs (onlyuseful for expert users) when exact knowledge understanding and representationis required.Technological requirements selection: The abilities we expect from an RDQA systemdepend explicitly on the different aspects of the domain analysis that we havepresented before.
Decisions on the specific technology and methods to use will betaken according to the type of questions to be solved, the availability of specializedresources, and the representational model used for encoding the domain knowl-edge.
As discussed in previous sections (see Sections 4 and 6), QA in restricteddomains usually requires techniques that differ substantially from the techniquesused in open-domain systems.
Restricted domains enable the possibility of usingcomprehensive ontological knowledge, thus making it possible to perform morecomplex inferences than in open-domain QA and therefore leveraging the possi-bility of answering more complex questions.
From this perspective taking accuratedesign decisions customized to the task requirements and the domain resources isessential.8.
Introduction to the Articles in this Special SectionDemner-Fushman and Lin?s article (Answering clinical questions with knowledge-basedand statistical techniques) extends previous work by the authors (Demner-Fushman andLin 2005) on a QA system in the medical domain.
The system is designed to satisfyinformation needs within the framework of evidence-based medicine (EBM) wherebya doctor needs to gather the current best evidence, namely, high-quality patient-centered clinical research.
The data source used by the system is the set of MEDLINEabstracts, a large bibliographic database that is accessed on-line via PubMed.
Inputquestions in this domain are highly specific and complex.
Following practice in thedomain, the input questions are formulated as PICO-based frames representing themajor elements of a query in EBM: Problem/Population, Intervention, Comparison,and Outcome.
A central task of the system is the automatic identification of PICOelements in the MEDLINE abstracts and their matching with the input query frame.In the process the system uses the Unified Medical Language System (UMLS), anextensive ontology specialized on this domain.
This system is a clear example of theadaptation of the task of question answering to a specific and highly practical domainusing specialized resources in order to satisfy information needs formulated as complex,structured questions.Hallett, Scott, and Power?s article (Composing questions through conceptual authoring)focuses on the stage of question formulation.
Questions in a QA system over a spe-cialized domain where the users are domain experts are typically complex in nature.This results in a problem both for the user, who needs to provide all the specificinformation in the question, and to the system that needs to analyze the question.The solution proposed in this article is to facilitate question formulation by means ofConceptual Authoring, whereby the user edits a formal representation of the query andreceives feedback from an automatically generated natural language representation ofthat query.
The article describes this method within the context of a QA system for adatabase of electronic health records.
An analysis of the question model in this domain55Computational Linguistics Volume 33, Number 1is presented, together with an evaluation of the usability of the method.
This articlepresents a concept of complex query formulation that can potentially be ported to otherspecialized domains.9.
ConclusionsIn this article we have presented an overview of methods used in QA in restricteddomains and we have argued for developing research in this area.
To conclude wewould like to comment on two reasons for developing question answering in restricteddomains:Development of vertical systems: Restricted domains allow the development of sys-tems that can provide the full range of processing levels and achieve a com-plete, end-to-end application.
It therefore becomes possible to develop completesystems that can be used without the need for any time-consuming training onthe methods required to formulate questions or to interpret the system results.Furthermore, restricted domains can provide a focus for the research and develop-ment of generic theories on complex question answering in particular and naturallanguage processing in general.
A clear example is the UC project developed inthe 1980s.
By reducing the research space it becomes possible to focus on solvingcomplex problems that would not be attempted if the main drive was to producea system that works in an open-domain fashion.Applicability to current needs: General and broad scope systems are not effective indomains restricted to the interests of different kind of users: from employees ininstitutions and companies trying to find information in manuals and procedures,to professionals in specialized domains like law, medicine, biology, mechanics,programming, and so on.
Notice that professionals in each of these areas re-quire different types of information in their daily activities (e.g., there is a con-siderable difference between looking for general information on the Internet asopposed to looking for the empty weight of a wing of the Airbus A319 in atechnical manual).A major difference between open-domain question answering and restricted-domainquestion answering is the existence of domain-dependent information that can be usedto improve the accuracy of the system.
Much of the focus of this article has been onforms of tapping information from these resources.Some domains are more appropriate for developing question answering systemsthan others.
A domain must be circumscribed enough so that a comprehensive on-tological resource can be built for the domain.
A domain must be complex enoughso that it presents challenging research problems in the area of natural languageprocessing.
Finally, a domain must be practical enough so that the end product isuseful to a significant segment of the population.
Domains (such as, for example,biomedicine) that meet al these properties are naturally more popular for researchersand developers.
Consequently they have some of the best ontological information andlarge corpora of texts and questions that can be used for the development of suchQA systems.Question answering on restricted domains requires the processing of complex ques-tions and offers the opportunity to carry out complex analysis of the text sources andthe questions.
Restricted domains also provide comprehensive ontologies and domain56Molla?
and Vicedo Question Answering in Restricted Domains: An Overviewresources that can help in the task of processing complex questions and finding theanswers.
The challenges and opportunities are there for us to take.ReferencesAbney, S. 1996.
Part-of-speech Tagging andPartial Parsing.
Corpus-Based Methods inLanguage and Speech.
Kluwer AcademicPublishers, Dordrecht.Ait-Mokhtar, Salah and Jean-Pierre Chanod.1997.
Incremental finite-state parsing.
InFifth Conference on Applied Natural LanguageProcessing (ANLP 97), pages 72?79,Washington, DC.Androutsopoulos, I., G. D. Ritchie, andP.
Thanisch.
1995.
Natural languageinterfaces to databases?an introduction.Natural Language Engineering, 1(1):29?81.Barker, Ken, Vinay K. Chaudhri, Shaw YiChaw, Peter Clark, James Fan, DavidIsrael, Sunil Mishra, Bruce W. Porter,Pedro Romero, Dan Tecuci, and Peter Z.Yeh.
2004.
A Question-answering systemfor AP chemistry: Assessing KR&Rtechnologies.
In Principles of KnowledgeRepresentation and Reasoning: Proceedingsof the Ninth International Conference(KR2004), pages 488?497, Whistler,Canada.Basili, Roberto, Dorte H. Hansen, PatriziaPaggio, Maria Teresa Pazienza, andFabio Massimo Zanzotto.
2004.Ontological resources and questionanswering.
In Workshop on Pragmatics ofQuestion Answering, held jointly withNAACL 2004, Boston, Massachusetts.Basili, Roberto and Fabio Massimo Zanzotto.2002.
Parsing engineering and empiricalrobustness.
Natural Language Engineering,8(2/3): 97?120.Benamara, Farah.
2004.
Cooperative questionanswering in restricted domains: TheWEBCOOP Experiments.
In Workshop onQuestion Answering in Restricted Domains.42nd Annual Meeting of the Association forComputational Linguistics (ACL-2004),pages 31?38, Barcelona, Spain.Brill, Eric, Jimmy Lin, Michele Banko,Susan Dumais, and Andrew Ng.
2001.Data-intensive question answering.
InProceedings TREC 2001, number 500?250in NIST Special Publications.
NIST,pages 393?400, Gaithersberg, MD.Ceusters, Werner, Barry Smith, andMaarten Van Mol.
2003.
Usingontology in query answering systems:Scenarios, requirements and challenges.In 2nd CoLogNET-ElsNET Symposium.Questions and Answers: Theoretical andApplied Perspectives, Amsterdam.Chung, Hoojung, Young-In Song,Kyoung-Soo Han, Do-Sang Yoon,Joo-Young Lee, and Hae-Chang Rim.2004.
A Practical QA System in RestrictedDomains.
In Workshop on QuestionAnswering in Restricted Domains.
42ndAnnual Meeting of the Association forComputational Linguistics (ACL-2004),pages 39?45, Barcelona, Spain.Cohen, P., R. Schrag, E. Jones, A. Pease,A.
Lin, B. Starr, D. Easter, D. Gunning,and M. Burke.
1988.
The DARPA highperformance knowledge bases project.AI Magazine, 19(4):25?49.Demner-Fushman, Dina and Jimmy Lin.2005.
Knowledge extraction for clinicalquestion answering: Preliminary results.In Workshop on Question Answering inRestricted Domains.
20th National Conferenceon Artificial Intelligence (AAAI-05),pages 1?9, Pittsburgh, PA.Diekema, Anne R., Ozgur Yilmazel, andElizabeth D. Liddy.
2004.
Evaluation ofrestricted domain question-answeringsystems.
In Workshop on QuestionAnswering in Restricted Domains.
42ndAnnual Meeting of the Association forComputational Linguistics (ACL-2004),pages 2?7, Barcelona, Spain.Doan-Nguyen, Hai and Leila Kosseim.2004.
The problem of precision inrestricted-domain question-answering.Some proposed methods of improvement.In Workshop on Question Answering inRestricted Domains.
42nd Annual Meetingof the Association for ComputationalLinguistics (ACL-2004), pages 8?15,Barcelona, Spain.Ely, J., J. Osheroff, M. Ebell, M. Chambliss,D.
Vinson, J. Stevermer, and E. Pifer.2002.
Obstacles to answering doctors?questions about patient care withevidence: Qualitative study.
BritishMedical Journal, 324:710?713.Fellbaum, Christiane.
1998.
WordNet:Introduction.
In Christiane Fellbaum,editor, WordNet: An Electronic LexicalDatabase, Language, Speech, andCommunication.
MIT Press, Cambrige,MA, pages 1?19.Ferre?s, Daniel and Horacio Rodr??guez.2006.
Experiments adapting anopen-domain question answeringsystem to the geographical domainusing scope-based resources.
In 11thConference of the European Chapter of the57Computational Linguistics Volume 33, Number 1Association of Computational Linguistics.Workshop on Multilingual QuestionAnswering - MLQA?06, Trento, Italy.Frank, Anette, Hans-Ulrich Krieger, FeiyuXu, Hans Uszkoreit, Berthold Crysmann,Brigitte Jo?rg, and Ulrich Scha?fer.
2006.Question answering from structuredknowledge sources.
Journal of AppliedLogic, Special Issue on Questions andAnswers: Theoretical and AppliedPerspectives, 1:29.Frank, Anette, Hans-Ulrich Krieger, FeiyuXu, Hans Uszkoreit, Berthold Crysmann,Brigitte Jo?rg, and Ulrich Scha?fer.
2005.Querying structured knowledge sources.In Workshop on Question Answering inRestricted Domains.
20th National Conferenceon Artificial Intelligence (AAAI-05),pages 10?19, Pittsburgh, PA.Gabbay, Igal.
2004.
Retrieving Definitions fromScientific Text in the Salmon Fish Domain byLexical Pattern Matching.
Ph.D. thesis,University of Limerick.Galitsky, Boris.
2001a.
A natural languagequestion answering system for humangenome domain.
In Proceedings of the2nd IEEE International Symposium onBioinformatics and Bioengineering,Bethesda, MD.Galitsky, Boris.
2001b.
Semi-structuredknowledge representation for theautomated financial advisor.
In Proceedingsof the Fourteenth International Conference onIndustrial and Engineering Applications ofArtificial Intelligence and Expert Systems,pages 874?879, Budapest, Hungary.Green, B. F., A. K. Wolf, C. Chomsky, andK.
Laughery.
1961.
Baseball: An automaticquestion answerer.
In Proceedings WesternComputing Conference, volume 19,pages 219?224.Hejazi, Mahmoud R., Maryam S. Mirian,Kourosh Neshatian, Bahador R. Ofoghi,and Ehsan Darudi.
2004.
Anontology-based question answeringsystem with auto extraction andcategorization capabilities in thedomain of telecommunications.The CSI Journal on Computer Scienceand Engineering, 2(1).Hirschman, Lynette and Rob Gaizauskas.2001.
Natural language questionanswering: The view from here.
NaturalLanguage Engineering, 7(4):275?300.Hobbs, Jerry R., Douglas Appelt, John Bear,David Israel, Megumi Kameyama,Mark Stickel, and Mabry Tyson.
1997.FASTUS: A Cascaded Finite-stateTransducer for Extracting Information fromNatural-Language Text.
MIT Press,Cambridge, MA.Johnson, Christopher and Charles J.Fillmore.
2000.
The FrameNet tagset forframe-semantic and syntactic coding ofpredicate-argument structure.
In JanyceWiebe, editor, Proceedings of the 1st Meetingof the North American Chapter of theAssociation for Computational Linguistics,Seattle, WA.Kacmarcik, Gary.
2005.
Question answeringin role-playing games.
In Workshopon Question Answering in RestrictedDomains.
20th National Conference onArtificial Intelligence (AAAI-05),pages 51?55.
Pittsburgh, PA.Kando, Noriko.
2005.
Overview of thefifth NTCIR workshop.
In Proceedingsof the Fifth NTCIR Workshop Meeting onEvaluation of Information Access Technologies:Information Retrieval, Question Answeringand Cross-Lingual Information Access,Tokyo, Japan.Katz, Boris.
1997.
From sentence processingto information access on the World WideWeb.
In AAAI Spring Symposium on NaturalLanguage Processing for the World Wide Web,pages 77?94, Stanford, CA.Katz, Boris, Sue Felshin, Deniz Yuret, AliIbrahim, Jimmy Lin, Gregory Marton,Alton Jerome McFarland, and BarisTemelkuran.
2002.
Omnibase: Uniformaccess to heterogeneous data for questionanswering.
In Proceedings of the 6thInternational Conference on Applications ofNatural Language to Information Systems,pages 230?234, Stockholm, Sweden.Katz, Boris, Jimmy J. Lin, and Sue Felshin.2002.
The START multimedia informationsystem: Current technology and futuredirections.
In Proceedings of the InternationalWorkshop on Multimedia InformationSystems, Tempe, AZ.Kim, Soo-Min and Eduard Hovy.
2005.Identifying opinion holders forquestion answering in opinion texts.In Workshop on Question Answeringin Restricted Domains.
20th NationalConference on Artificial Intelligence(AAAI-05), pages 20?26, Pittsburgh, PA.Lenat, D. and R. V. Guha.
1990.
Building LargeKnowledge-Based Systems: Representation andInference in the Cyc Project.
Addison-Wesley.Lin, Jimmy.
2002.
The Web as a resourcefor question answering: Perspectivesand challenges.
In Proceedings of the ThirdInternational Conference on LanguageResources and Evaluation, pages 2120?2127,Las Palmas, Spain.58Molla?
and Vicedo Question Answering in Restricted Domains: An OverviewLindberg, D. A., B. L. Humphreys, andA.
T. McCray.
1993.
The unified medicallanguage system.
Methods of Information inMedicine, 32(4):281?291.Minock, Michael.
2005.
Where are the ?killerapplications?
of restricted domain questionanswering?
In Proceedings of the IJCAIWorkshop on Knowledge Reasoning inQuestion Answering, page 4, Edinburgh,Scotland.Moldovan, Dan and Adrian Novischi.
2002.Lexical chains for question answering.In Proceedings of the 19th InternationalConference on Computational Linguistics,Taipei, Taiwan.Molla?, Diego, Rolf Schwitter, Michael Hess,and Rachel Fournier.
2000.
Extrans, ananswer extraction system.
TraitementAutomatique des Langues, 41(2):495?522.Molla?, Diego and Jose?
Luis Vicedo, editors.2004.
Workshop on Question Answering inRestricted Domains.
42th Annual Meeting ofthe Association for Computational Linguistics(ACL-2004), Barcelona, Spain.Molla?, Diego and Jose?
Luis Vicedo, editors.2005.
Workshop on Question Answering inRestricted Domains.
Twentieth NationalConference on Artificial Intelligence(AAAI-05), Pittsburgh, Pennsylvania, USA.Niu, Yun and Graeme Hirst.
2004.
Analysisof semantic classes in medical textfor question answering.
In Workshopon Question Answering in RestrictedDomains.
42nd Annual Meeting ofthe Association for ComputationalLinguistics (ACL-2004), pages 54?61,Barcelona, Spain.Noy, N. F. and D. L. McGuinness.
2001.Ontology development 101: A guide tocreating your first ontology.
TechnicalReport KSL-01-05, Knowledge SystemsLaboratory.Nyberg, Eric, Teruko Mitamura, RobertFrederking, Vasco Pedro, Matthew W.Bilotti, Andrew Schlaikjer, and KerryHannan.
2005.
Extending the JAVELINsystem with domain semantics.In Question Answering in RestrictedDomains: Papers from the AAAI Workshop,pages 36?40, Pittsburgh, PA.Popescu, Ana-Maria, Oren Etzioni,and Henry Kautz.
2003.
Towards atheory of natural language interfaces todatabases.
In Proceedings of the 2003International Conference on IntelligentUser Interfaces (IUI-03), pages 149?157,New York.Porter, B., J. Lester, K. Murray, K. Pittman,A.
Souther, L. Acker, and T. Jones.1988.
AI research in the context of amultifunctional knowledge base:The botany knowledge base project.Technical Report, AI-88-88, Departmentof Computer Sciences, University ofTexas at Austin.Rinaldi, Fabio, James Dowdall, and GeroldSchneider.
2004.
Answering questionsin the genomics domain.
In Proceedings ofthe ACL04 Workshop on Question Answeringin Restricted Domains, pages 46?53,Barcelona, Spain.Rinaldi, Fabio, Michael Hess, JamesDowdall, Diego Molla?, and Rolf Schwitter.2004.
Question answering interminology-rich technical domains.
InMark T. Maybury, editor, New Directions inQuestion Answering.
AAAI Press/MITPress, Cambridge, MA, pages 71?82.Rotaru, Mihai and Diane J. Litman.
2005.Improving question answering forreading comprehension tests bycombining multiple systems.
In Workshopon Question Answering in RestrictedDomains.
20th National Conferenceon Artificial Intelligence (AAAI-05),pages 46?50, Pittsburgh, PA.Sang, Erik Tjong Kim, Gosse Bouma, andMaarten de Rijke.
2005a.
Developingoffline strategies for answering medicalquestions.
In Workshop on QuestionAnswering in Restricted Domains.20th National Conference on ArtificialIntelligence (AAAI-05), pages 41?45,Pittsburgh, PA.Schrag, R., M. Pool, V. Chaudhri, R. C.Kahlert, J.
Powers, P. Cohen,J.
Fitzgerald, and S. Mishra.
2002.Experimental evaluation of subjectmatter expert-oriented knowledge baseauthoring tools.
In Proceedings of the2002 PerMIS Workshop: Measuring thePerformance and Intelligence of Systems,pages 272?279, Gaithersburg, MD.Simmons, R. F. 1965.
Answering Englishquestions by computer: A survey.Communications of the ACM, 8(1):53?70.Srihari, Rohini and Wei Li.
2000.
Informationextraction supported question answering.In Proceedings of TREC 8 (1999),pages 185?196, Gaithersburg, MD.Tsur, Oren, Maarten de Rijke, and KhalilSima?an.
2004.
BioGrapher: Biographyquestions as a restricted domain questionanswering task.
In Workshop on QuestionAnswering in Restricted Domains.
42ndAnnual Meeting of the Association forComputational Linguistics (ACL-2004),pages 23?30, Barcelona, Spain.59Computational Linguistics Volume 33, Number 1Vallin, Alessandro, Bernardo Magnini,Danilo Giampiccolo, Lili Aunimo,Christelle Ayache, Petya Osenova,Anselmo Pe nas, Maarten de Rijke,Bogdan Sacaleanu, Diana Santos, andRichard Sutcliffe.
2005.
Overview ofthe CLEF 2005 multilingual questionanswering track.
In Proceedings ofCLEF 2005, Vienna, Austria.Vargas-Vera, Maria and Enrico Motta.
2004.AQUA: A question answering system forheterogeneous sources.
Technical ReportKMI-04-20, KMI.Vargas-Vera, Maria, Enrico Motta, andJohn Domingue.
2003.
AQUA: Anontology-driven question answeringsystem.
In Mark T. Maybury, editor, NewDirections in Question Answering, Papersfrom 2003 AAAI Spring Symposium,Stanford University, pages 53?57.Stanford, CA.Voorhees, Ellen M. 1999.
The TREC-8question answering track report.
InProceedings of TREC-8, pages 77?82,Gaithersburg, MD.Voorhees, Ellen M. 2001.
The TREC questionanswering track.
Natural LanguageEngineering, 7(4):361?378.Vossen, Piek, editor.
1998.
Euro WordNet:A Multilingual Database with LexicalSemantic Networks.
Kluwer AcademicPublishers, Dordrecht, Holland.Weischedel, Ralph, Jinxi Xu, andAna Licuanan.
2004.
A hybridapproach to answering biographicalquestions.
In Mark T. Maybury,editor, New Directions in QuestionAnswering.
AAAI Press/MITPress, Cambridge, MA, chapter 5,pages 59?69.Wilensky, Robert, David N. Chin, MarcLuria, James Martin, James Mayfield,and Dekai Wu.
1994.
The BerkeleyUnix Consultant project.
ComputationalLinguistics, 14(4):35?84.Woods, William A.
1997.
Conceptualindexing: A better way to organizeknowledge.
Technical Report SMLITR-97-61, Sun Microsystems, Inc.Yu, Hong, Carl Sable, and Hai Ran Zhu.2005.
Classifying medical questionsbased on an Evidence Taxonomy.
InWorkshop on Question Answering inRestricted Domains.
20th NationalConference on Artificial Intelligence(AAAI-05), pages 27?35, Pittsburgh, PA.Zajac, Re?mi.
2001.
Towards ontologicalquestion answering.
In Proceedings ofACL2001, Workshop on Open DomainQA, Toulouse.Zweigenbaum, Pierre.
2003.
Questionanswering in biomedicine.
In Proceedingsof EACL2003, Workshop on NLP forQuestion Answering, Budapest.Appendix A: List of QA Systems in Restricted DomainsThe following list is by no means exhaustive.
Our purpose in presenting this list is toshow the breadth of current research and applications in RDQA.
We welcome updatesand additions to the list, which will be maintained at http://www.ics.mq.edu.au/?diego/answerfinder/rdqa/.1.
Generic systems JAVELIN (Nyberg et al 2005)?
http://www.cs.cmu.edu/?ehn/JAVELIN/ QUETAL (Frank et al 2005, 2006)?
http://www.dfki.de/pas/f2w.cgi?ltp/quetal-e AQUA (Vargas-Vera and Motta 2004; Vargas-Vera, Motta, andDomingue 2003)?
http://kmi.open.ac.uk/projects/akt/aqua/?
http://kmi.open.ac.uk/projects/akt/publications.cfm START (Katz 1997; Katz et al 2002; Katz, Lin, and Felshin 2002)?
http://start.csail.mit.edu/2.
Collaborative learning for engineering education KAAS (Diekema, Yilmazel, and Liddy 2004)60Molla?
and Vicedo Question Answering in Restricted Domains: An Overview3.
Services provided by a large company Concordia University system (Doan-Nguyen and Kosseim 2004)4.
Salmon fish biology SOK-I (Gabbay 2004)5.
Biography information BioGrapher (Tsur, de Rijke, and Sima?an 2004) BBN Technologies (Weischedel, Xu, and Licuanan 2004)6.
Tourism WEBCOOP (Benamara 2004)7.
Weather forecasts System by Korea University and Sangmyung University(Chung et al 2004)8.
Technical domains ExtrAns (Rinaldi et al 2004)?
http://www.ifi.unizh.ch/cl/extrans/ TeLQAS (Hejazi et al 2004)?
http://www.neshatian.org/projects/telqas/9.
Genomics ExtrAns (Rinaldi, Dowdall, and Schneider 2004) System by KnowledgeTrail (Galitsky 2001a)10.
Financial System by KnowledgeTrail (Galitsky 2001b)11.
Medical domain EpoCare (Niu and Hirst 2004) system by University of Maryland (Demner-Fushman andLin 2005) question classification by Columbia University and Cooper Union(Yu, Sable, and Zhu 2005) IMIX12.
Geographic domain System by UPC (Ferra?s and Rodr?
?guez 2006)13.
Nobel prizes System by DFKI (Frank et al 2005)14.
Language technology System by DFKI (Frank et al 2005)15.
Opinion texts System by University of Southern California (Kim and Hovy 2005)16.
Reading comprehension texts RC QA (Rotaru and Litman 2005)17.
Role-playing games System by Microsoft Research (Kacmarcik 2005)61
