Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 377?380,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsGeneralizing Syntactic Structures for Product Attribute CandidateExtractionYanyan Zhao, Bing Qin, Shen Hu, Ting LiuHarbin Institute of Technology, Harbin, China{yyzhao,bqin,shu,tliu}@ir.hit.edu.cnAbstractNoun phrases (NP) in a product review arealways considered as the product attributecandidates in previous work.
However, thismethod limits the recall of the product at-tribute extraction.
We therefore proposea novel approach by generalizing syntacticstructures of the product attributes with twostrategies: intuitive heuristics and syntacticstructure similarity.
Experiments show thatthe proposed approach is effective.1 IntroductionProduct attribute extraction is a fundamental task ofsentiment analysis.
It aims to extract the product at-tributes from a product review, such as ?picture qual-ity?
in the sentence ?The picture quality of Canon isperfect.?
This task is usually performed in two steps:product attribute candidate extraction and candidateclassification.Almost all the previous work pays more attentionto the second step, fewer researchers make in-depthresearch on the first step.
They simply choose theNPs in a product review as the product attribute can-didates (Hu and Liu, 2004; Popescu and Etzioni,2005; Yi et al, 2003).
However, this method lim-its the recall of the product attribute extraction fortwo reasons.
First, there exist other structures of theproduct attributes except NPs.
Second, the syntacticparsing is not perfect, especially for the Non-Englishlanguages, such as Chinese.
Experiments on threeChinese datasets1 show that nearly 15% product at-tributes are lost, when only using NPs as the can-didates.
Obviously, if using the candidate classifi-cation techniques on these NP candidates, it would1It refers to the training data in Section 3.1.lead to poor performance (especially for recall) forthe final product attribute extraction.Based on the above discussion, it can be observedthat product attribute candidate extraction is wellworth studying.
In this paper, we propose an ap-proach by generalizing the syntactic structures of theproduct attributes to solve this problem.
Figure 1lists some syntactic structure samples from an an-notated corpus, including the special forms of NPsin Figure 1(a) and other syntactic structures, such asVP or IP in Figure 1(b).
We can find that the syntac-tic structures can not only cover more phrase typesbesides NP, but also describe the detailed forms ofthe product attributes.NPNN??(screen)NPNN??NPNN???
(screen    resolution)NPQPCD?NPNN??
(single    track)NPADJPJJ?NPNN??
(front         seats)NPVB??NPNN??
(photographing function)VPNPNN??VPVB??
(screen   display)IP(a) syntactic structure samples of NP(b) syntactic structure samples of other phrasesFigure 1: Syntactic structure samples of the product at-tributes (acquired by an automatic phrase parser).In order to exploit more and useful syntactic struc-tures, two generalization strategies: intuitive heuris-tics and syntactic structure similarity are used.
Ex-periments on three Chinese domain-specific datasetsshow that our approach can significantly improve therecall of the product attribute candidate extraction,and furthermore, improve the performance of the fi-nal product attribute extraction.3772 ApproachThe standard syntactic structures of the product at-tributes can be collected from a training set2.
Thena simple method of exact matching can be used toselect the product attribute candidates from the testset.
In particular, for a syntactic structure3 T inthe test set, if T exactly matches with one of thestandard syntactic structures, then its correspondingstring can be treated as a product attribute candidate.However, this method fails to handle similar syn-tactic structures, such as the two structures in Fig-ure 2.
Besides, this method treats the syntactic struc-ture as a whole during exact matching, without con-sidering any structural information.
Therefore, it isdifficult to describe the syntactic structure informa-tion explicitly.
All of these prevent this method fromgeneralizing unseen data well.To overcome the above problems, two generaliza-tion strategies are proposed in this paper.
One is togeneralize the syntactic structures with two intuitiveheuristics.
The other is to deeply mine the syntacticstructure by decomposing it into several substruc-tures.
Both strategies will be introduced in the fol-lowing subsections.2.1 Intuitive HeuristicsTwo intuitive heuristics are adopted to generalize thesyntactic structures.Heu1: For the near-synonymic grammar tags insyntactic structures, we can generalize them by anormalized one.
Such as the red boxes in Figure 2,the POSs ?NNS?
and ?NN?
show the same syntacticmeaning, we can generalize ?NNS?
with ?NN?.
Thenear-synonymic grammar tags are listed in Table 1.NPVP NPVB NNS NPNNNPVP NPVB NN NNHeu2Heu1Figure 2: Generalizing a syntactic structure with two in-tuitive heuristics.Heu2: For the sequence of identical grammar tagsin syntactic structures, we can replace them with2We use Dan Bikel?s phrase parser for syntactic parsing.3We simply select the syntactic structures of the strings un-der three words or four words with ???(?of?
in English).Replaced by Near-synonymic grammar tagsJJ JJR, JJSNN NNS, NNP, NNPS, CD, NRRB RBR, RBSVB VBD, VBG, VBN, VBP, VBZ, VVS SBAR, SBARQ, SINU, SQTable 1: The near-synonymic grammar tags.one.
The reason is that the sequential grammar tagsalways describe the same syntactic function as onegrammar tag.
Such as the blue circles in Figure 2.2.2 Syntactic Structure SimilarityThe heuristic generalization strategy is too restric-tive to give a good coverage.
Moreover, after thiskind of generalization, the syntactic structure is usedas a whole in exact matching all the same.
Thus,as an alternative to the exact matching, tree kernelbased methods can be used to implicitly explore thesubstructures of the syntactic structure in a high-dimensional space.
This kind of methods can di-rectly calculate the similarity between two substruc-ture vectors using a kernel function.
Tree kernelbased methods are effective in modeling structuredfeatures, which are widely used in many naturallanguage processing tasks, such as syntactic pars-ing (Collins and Duffy, 2001) and semantic role la-beling (Che et al, 2008) and so on.NPNNVPVBIPNP VPVBIPNPNNVPVB NPNNVPVBIPNPNNVPIPNP VPIPNPNNVPVBIP IPFigure 3: Substructures from a syntactic structure.In this paper, the syntactic structure for a productattribute can be decomposed into several substruc-tures, such as in Figure 3.
Correspondingly, the syn-tactic structure T can be represented by a vector ofinteger counts of each substructure type:?
(T ) = (?1(T ), ?2(T ), ..., ?n(T ))= (# of substructures of type 1,= # of substructures of type 2,...,= # of substructures of type n)378After syntactic structure decomposition, we cancount the number of the common substructures asthe similarity between two syntactic structures.
Thecommonly used convolution tree kernel is applied inthis paper.
Its kernel function is defined as follows:K(T1, T2) = ??(T1),?
(T2)?=?i(?i(T1) ?
?i(T2))Based on these, for a syntactic structure T in thetest set, we can compute the similarity between Tand all the standard syntactic structures by the abovekernel function.
A similarity threshold thsim4 is setto determine whether the string from T is a correctproduct attribute candidate.3 Experiments3.1 Datasets and Evaluation MetricsThree domain-specific datasets are used in the ex-periments, which is from an official Chinese Opin-ion Analysis Evaluation 2008 (COAE2008) (Zhao etal., 2008).
Table 2 shows the statistics of the threedatasets, each of which is divided into training, de-velopment and test data in a proportion of 2:1:1.Domain # of sentences # of standardproduct attributesCamera 1,780 1,894Car 2,166 2,504Phone 2,196 2,293Table 2: The datasets for three product domains.Two evaluation metrics, recall and noise ratio, aredesigned to evaluate the performance of the prod-uct attribute candidate extraction.
Recall refers tothe proportion of correctly identified attribute candi-dates in all standard product attributes.
Noise ratiorefers to the proportion of incorrectly identified at-tribute candidates in all candidates.3.2 Comparative methodsWe choose the method, which considers NPs as theproduct attribute candidates, as the baseline (shownas NPs based).Besides, in order to assess the two generaliza-tion strategies?
effectiveness, four experiments aredesigned as follows:4In the experiments, thsim is set to 0.7, which is tuned onthe development set.SynStru based: It refers to the syntactic struc-ture exact matching method, which is implementedwithout the two proposed generation strategies.SynStru h: It refers to the strategy only using thefirst generalization.SynStru kernel: It refers to the strategy only us-ing the second generalization.SynStru h+kernel: It refers to the strategy us-ing both two generalizations, i.e., it refers to our ap-proach in this paper.3.3 ResultsTable 3 lists the comparative performances on thetest data between our approach and the comparativemethods for product attribute candidate extraction.Domain Method Recall Noise ratioCameraNPs based 81.20% 63.64%SynStru based 84.80% 67.67%SynStru h 92.08% 74.74%SynStru kernel 92.51% 75.92%SynStru h+kernel 92.72% 76.25%CarNPs based 85.25% 69.35%SynStru based 86.31% 72.66%SynStru h 93.78% 78.01%SynStru kernel 94.56% 79.50%SynStru h+kernel 94.71% 80.44%PhoneNPs based 84.11% 63.76%SynStru based 86.26% 67.09%SynStru h 93.13% 73.62%SynStru kernel 93.47% 75.11%SynStru h+kernel 93.63% 75.35%Table 3: Comparisons between our approach and thecomparative methods for product attribute candidate ex-traction.Analyzing the recalls in Table 3, we can find that:1.
The performance of SynStru based methodis better than NPs based method for each domain.This can illustrate that syntactic structures can covermore forms of the product attributes.
However, therecall of SynStru based method is not high, either.2.
The two generalization strategies, SynStru hand SynStru kernel can both significantly improvethe performance for each domain, comparing to theSynStru based method.
This can illustrate that ourtwo generalization strategies are helpful.3.
Our approach SynStru h+kernel achieves thebest performance.
This can illustrate that the twogeneralization strategies are complementary to each379other.
And further, mining and generalizing the syn-tactic structures is effective for candidate extraction.However, the noise ratio for each domain is in-creasing when employing our approach.
That?s be-cause, more kinds of syntactic structures are consid-ered, more noise is added.
However, we can easilyremove the noise in the candidate classification step.Thus in the next section, we will assess our candi-date extraction approach by applying it to the prod-uct attribute extraction task.4 Application in Product AttributeExtractionFor the extracted product attribute candidates, wetrain a maximum entropy (ME) based binary clas-sifier to find the correct product attributes.
Severalcommonly used features are listed in Table 4.Feature Descriptionlexicalthe words of the product attribute(PA)the POS for each word of the PAthree words before the PAthree words after the PAthe words?
number of the PAsyntactic the syntactic structure of the PAIs there a stop word in the PA?binary Is there a polarity word in the PA?
(Y/N) Is there an English word or number in the PA?Table 4: The feature set for product attribute extraction.Table 5 shows the product attribute extraction per-formances on the test data.
We can find that theperformance (F1) of our approach is better thanNPs based method for each domain.
We discuss theresults as follows:1.
Comparing to the NPs based method, the re-call of our approach increases a lot for each domain.This demonstrates that generalized syntactic struc-tures can cover more forms of product attributes.2.
Comparing to the NPs based method, the pre-cision of our approach also increases for each do-main.
That?s because syntactic structures are morespecialized than the phrase forms (such as NP, VP)in the previous work, which can filter some noisesfrom the phrase(NP) candidates.5 ConclusionThis paper describes a simple but effective way toextract the product attribute candidates from productDomain Method R (%) P (%) F1 (%)Camera NPs based 59.62 68.38 63.70Our approach 62.96 73.32 67.74Car NPs based 59.94 64.87 62.31Our approach 67.34 65.90 66.61Phone NPs based 58.53 71.14 64.22Our approach 67.84 76.13 71.74Table 5: Comparisons between our approach and theNPs based method for product attribute extraction.reviews.
The proposed approach is based on deepanalysis into syntactic structures of the product at-tributes, via intuitive heuristics and syntactic struc-ture decomposition.
Experimental results indicatethat our approach is promising.
In future, we will trymore syntactic structure generalization strategies.AcknowledgmentsThis work was supported by National NaturalScience Foundation of China (NSFC) via grant60803093, 60975055, and the ?863?NationalHigh- Tech Research and Development of China viagrant 2008AA01Z144.ReferencesWanxiang Che, Min Zhang, AiTi Aw, Chew Lim Tan,Ting Liu, and Sheng Li.
2008.
Using a hybrid con-volution tree kernel for semantic role labeling.
ACMTrans.
Asian Lang.
Inf.
Process., 7(4).Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In NIPS, pages 625?632.Minqing Hu and Bing Liu.
2004.
Mining opinion fea-tures in customer reviews.
In Proceedings of AAAI-2004, pages 755?760.Ana-Maria Popescu and Oren Etzioni.
2005.
Extract-ing product features and opinions from reviews.
Inhltemnlp2005, pages 339?346.Jeonghee Yi, Tetsuya Nasukawa, Razvan Bunescu, andWayne Niblack.
2003.
Sentiment analyzer: Extract-ing sentiments about a given topic using natural lan-guage processing techniques.
In Proceedings of theIEEE International Conference on Data Mining.Jun Zhao, Hongbo Xu, Xuanjing Huang, Songbo Tan,Kang Liu, and Qi Zhang.
2008.
Overview of chineseopinion analysis evaluation 2008.380
