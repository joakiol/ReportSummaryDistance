A PROBABILISTIC PARSERRoger Garside and Fanny LeechUnit for Computer Research on the English LanguageUniversity of LancasterBailriggLancaster LAI 4YT, U.K.ABSTRACTThe UCREL team at the University of Lancasteris engaged in the development of a robustparsing mechanism, which will assign the appro-priate grammatical structure to sentencesin unconstrained English text.
The techniquesused involve the calculation of probabilitiesfor competing structures, and are based onthe techniques successfully used in tagging(i.e.
assigning grammatical word classes)to the LOB (Lancaster-Oslo/Bergen) corpus.The first step in the parsing process involvesdictionary lookup of successive pairs of gramm-atically tagged words, to give a number ofpossible continuations to the current parse.Since this lookup will often not be ableunambiguously to distinguish the point atwhich a grammatical constituent should beclosed, the second step of the parsing processwill have to insert closures and distinguishbetween alternative parses.
It will generatetrees representing these possible alternatives,insert closure points for the constituents,and compute a probability for each parse treefrom the probability of each constituent withinthe tree.
It will then be able to selecta preferred parse or parses for output.The probability of a grammatical constituentis derived from a bank of manually parsedsentences.INTRODUCTIONIn this paper we present an overview ofone part of the work currently being carriedout at the Unit for Computer Research on theEnglish Language (UCREL) in the Universityof Lancaster, under SERC research grant numberGR/C/47700.
This work involves the automaticsyntactic analysis or parsing of the LOB corpus,using the statistical or constituent-likelihood(CL) grammar ideas of Atwell (1983).
Thework is based on the grammatical tagging ofthe LOB corpus, both as providing a partiallyanalysed text and because of the techniquesused in assigning tags.
We therefore beginby briefly describing this earlier project.The grammatical tagging of the LOB corpusis described in detail elsewhere (see, forexample, Leech, Garside and Atwell 1983,Marshall 1983, Beale 1985), but in essencethere are three stages.
The first stage takesthe original corpus, on which a certain amountof pre-editing (both automatic and manual)has been performed.
It assigns to each wordin the corpus a set of possible tags, and itis assumed that the correct tag is in thisset.
The set of possible tags is chosen withoutat this stage considering the context in whichthe word appears, and the choice is made byusing an ordered set of decision rules, themost commonly used of which (in about 65-70%of cases) is to look the word up in a dictionaryof some 7000 words.The third stage involves looking at thosecases where the first stage has resulted inmore than one tag being assigned to a word.In this case we calculate the probability ofeach possible sequence of ambiguous tags, andthe most likely sequence is chosen as the correctone.
In most cases the probability of a sequenceof tags is calculated by multiplying togetherthe pairwise probabilities of one tag followinganother, and these pairwise probabilities werederived from a statistical analysis of co-occurrence of tags in the tagged Brown corpus(Francis and Kucera 1964).A further stage was later inserted betweenthe two stages described above.
This stageinvolves the ability to look for patterns ofsequences of words and putative tags assignedby the first stage, and to modify the setsof tags assigned to words.
This enables variousproblematical situations to be resolved orclarified in order to improve the disambiguatingability of the third stage.After the third stage (when the appropriatetag will have been automatically selected some96,5% of the time), the remaining errors areremoved by a manual post-editing phase.The fundamental idea on which our syntacticanalysis is based, originally formulated inAtwell (1983), is that the general principlesbehind the tagging system could be used atthe parsing level.
Thus a first stage of parsingcould be to look up a tag in a dictionary toderive a set of possible constituents (or"hypertags") containing this tag.
Similarly,in the third stage, the probability of anyparticular constituent being constructed outof a particular set of constituents or word-166classes at the next lower level could be usedto disambiguate a set of constituents positedat the first stage.
To this end some 2000sentences from the LOB corpus have been manuallyparsed, and the results stored as a "treebank"or database of information on the frequencyof occurrence of possible grammatical structures.Thus, for each possible "mother" constituent,there will be stored a set of sequences ofdaughter constituents or word-classes, togetherwith their frequencies.The second stage generalises to a searchfor particular syntactic patterns which arerecognisable in context, and the resolutionof which will improve the accuracy of thethird stage.
We develop these ideas in theremainder of the paper.INPUT TO THE ANALYSIS SYSTEMThe input to the analysis system is essentiallythe output from the tagging system describedabove.
An example of this is given in figurei.BOI 9 001BOI 9 010 there EXBOI 9 020 is BEZBOI 9 030 the ATBOI 9 040 possibility NNBOI 9 050 that CSBOI 9 060 it PP3BOI 9 070 will MDBOI 9 080 not XNOTBOI 9 090 be BEBOI 9 100 settled VBNBOI 9 Ii0 at INBOI 9 120 this DTBOI i0 010 conference NNBOI i0 011 .BOI I0 012Figure i.
Input to the System.Each line of the tagged LOB corpus containsone word or punctuation mark, and each sentenceis separated from the preceding one by thesentence initial marker, here representedby a horizontal line.
Each line consists ofthree main fields; a reference number specifyingthe genre, text number, line number, andposition within the line; the word or punctua-tion mark itself; and the correct tag.
Thetags are taken from a set of 134 tags, basedon the Brown tagset (Greene and Rubin 1971),but modified where we felt it was desirable.OUTPUT FROM THE ANALYSIS SYSTEMTypical output from the analysis systemwould look like figure 2.BOI 9 001BO1 90 lO  there EX IS\[ElBO1 9 020 is BEZ \[V\]BOI 9 030 the AT \[NBOI 9 040 possibility NNBOI 9 050 that CS \[FnBOI 9 060 it PP3 IN\]BOI 9 070 will MD \[VeBOI 9 080 not XNOTBOI 9 090 be BEBOI 9 i00 settled VBN Ve\]BOI 9 II0 at IN \[PBOI 9 120 this DT \[NB01 i0 010 conference NN N\]P\]Fn\]N\]BOI I0 011 .
S\]BOI i0 012Figure 2.
Output from the System.The field on the right is meant to representa typical parse tree, but in a columnar form.Each constituent is represented by a an uppercase letter; thus S is the sentence, N isa noun phrase, and F indicates a subordinateclause.
The upper case letter may be followedby one or more lower case letters, indicatingfeatures of interest in the constituent; thusFn indicates a nominal clause.
The boundariesof a constituent are given by open and closesquare brackets, so that for instance thesubordinate clause indicated by Fn startsat the word "that" and ends at the word"conference".STAGE ONE - ASSIGNMENTIt is clear that a tag, or a pair of consec-utive tags, is partially diagnostic of thebeginning, continuation or termination ofa constituent.
Thus, for example, the pair"noun-verb" tends to indicate the end of anoun phase and the beginning of a verb phase,and the pair "noun-noun" tends to indicatethe continuation of a noun phase.
The firststep in the syntactic analysis is thereforeto deduce from the sequence of tags a tentativesequence of markings for the type and boundariesof the constituents.
Since the beginningsof constituents tend to be marked, but notthe ends, this sequence of markings will tendto omit many of the right-hand or closingbrackets, and these are inserted at a laterstage.The first stage of parsing is thereforeto look up each (tag, tag) pair in a dictionary,and this results in one or more possiblesequences of open and close brackets and con-stituent markings - each of these sequencesis, for historical reasons, called a "T-tag".A T-tag consists of a left-hand and a right-hand part.
The left-hand part consists ofan indication of what constituent should becurrent (i.e.
at the top of the stack of openconstituents) at this stage, perhaps followedby one or more closing brackets.
The right-hand part normally consists of an indicationthat one or more new constituents should beopened, that some particular constituent should167be continued, or more rarely that a new constit-uent should be (and this will be deduced lateron in the analysis process).
Thus the tag-,,pair "noun followed by subordinating conjunctionindicates two possible T-tags, either "Y\]\[F" or "Y ~".
The first means close the currentconstituent whatever it is (Y matches anyconstituent) and open a new subordinate clause(F) constituent, while the second means continuethe current constituent and open an F constituent.The look-up procedure as described aboverequires a dictionary entry for each possiblepair of tags, which is inefficient and difficultto relate to meaningful linguistic categories.Instead the 134 tags are subsumed in a setof 33 "cover symbols" (the term is taken fromthe Brown tagging system).
Thus all the differ-ent forms of noun word tag are subsumed inthe cover symbols N* (singular noun), *S (pluralnoun) and *$ (noun with genitive marker).The required tag-pair dictionary will thereforerequire only an entry for each cover-symbolpair (together with a list of exceptions, wherethe tag rather than the cover symbol is diag-nostic of the appropriate T-tags).
A furthersimplification is that in many cases (becauseof the admissibility of the "wild" constituentmarker Y) the first tag of the pair is irrelevantand the second tag in the pair determines theset of T-tag options.I said that the T-tag dictionary look-upwould often result in more than one possibleT-tag, rather than just one.
Some of theseoptions can be eliminated immediately by matchingthe current constituent with the putative exten-sion, but others need to be retained for laterdisambiguation.CONSTRUCTING THE T-TAG DICTIONARYThe original version of the T-tag dictionarywas generated using linguistic intuition.If there are several possible T-tags to anentry, they are given in approximately decreasinglikelihood and rare T-tags are marked as such.The treebank of manually parsed sentences cannow be used to extract information about whatconstituent types and boundaries are associatedwith what pairs of tags.
We have thereforewritten a program which takes a current versionof the T-tag dictionary and a set of parsedsentences, and generates;(a) information about putative exceptions tothe curent T-tag dictionary, in the form ofcases where the effective T-tag in the parsedsentence is not among those proposed by theT-tag dictionary, and(b) where the effective T-tag is among thoseproposed by the T-tag dictionary, statisticsare gathered as to the differential probabilitiesof the various T-tags associated with a parti-cular tagpair.The first set of information is used toguide the intuition of a linguist in decidinghow to modify the original T-tag table.
Thiscannot (at least at present) be done automat-ically, since there are various unsystematicdifferences between the T-tag as looked upin the dictionary and the sequence of constituenttypes and boundaries as they appear in theparsed sentences.
We are thus using informationfrom the parsed corpus texts to generateimproved versions of the T-tag dictionary.The frequency information about the optionalT-tags associated with a particular tagpairis not at present used by the analysis system,but we feel that it may be a further factorto be taken into account when deciding ona preferred parse in the third stage of analysis.The information is of course being used torefine linguistic intuition about the orderingof possible T-tags in the dictionary a~d theirmarking for rarity.STAGE THREE - TREE-CLOSINGThe output from the first stage consistsof indications of a number of constituentsand where they begin, but in many cases theending position of a constituent is unknown,or at least is located ambiguously at oneof several positions.
The main task of thethird stage is to insert these constituentclosures.
There is a further stage betweenT-tag assignment and tree-closure which wewill return to in a later section.The third stage proceeds as follows.
Abackward search is made from the end of thesentence to find a position at which choicesand/or decisions have to be made.
At thefirst such point the alternative trees areconstructed and then all unclosed constituentsare completed, by means of likelihood calcula-tions based on the database of probabilities.To effect closure, the last unclosed constituentis selected and a subtree data structure iscreated to represent this constituent.
Theparser then attempts to attach to it as daughtersany constituents (word-classes or constituents)lying positionally below it.
As a consequenceof each successive attachment there existsa distinct mother-daughter sequence pattern,the probability of which can be extractedfrom the mother-daughter table derived fromthe treebank (the parser will not attemptto build subtrees with probabilities belowa certain threshold).
If a sequence of cons-tituents is attached as daughters, then anyremaining constituents lying below the lastattached daughter are attached to the subtreeas sisters.
Thus the constituent is closedin all statistically possible ways, and theparser is once again positioned at the endof the sentence.The parser again selects the next unclosedconstituent, this time passing over the newlyclosed constituent (which is now representedas a subtree), and it proceeds to close thenew constituent in the manner described above.However when attaching as daughter or sisterthe newly closed constituent from the previous168selection it attaches a set of subtrees thatrepresents all its possible closure patterns.This process is repeated until the top levelis reached.
If the head of the sentence hasbeen reached, then many sub-trees are discardedbecause at this level all other constituentsmust be daughters and not sisters.
If morethan one tree is to be completed from a choice,then this process is repeated until all thealternative trees have been closed.STATISTICS FOR THE MOTHER-DAUGHTER SEQUENCESThe main problem is how to store the frequencyinformation on possible daughter sequencesfor each mother constituent.
Originally themanually parsed sentences collected in thetreebank were decomposed into a mother cons-tituent and each of its daughter sequencesin its entirety.
So for a mother constituentN (noun phrase) a possible daughter is "ATI,JJ, NNS, Fr" (i.e.
determiner, adjective,plural noun, subordinate clause).The main problem with this is that, forall the most common daughter sequences, thestatistics were too dependent on exactly whichsentences had occurred.
This also impliesthat the parser has to match very specificpatterns when a subtree is being investigated.To produce statistical tables of sufficientgenerality, each daughter sequence was decomposedinto its individual pairs of elements (eachdaughtser sequence in its entirety havingimplied opening and closing delimiters, repre-sented by the symbols '\[' and '\]' respectively)and all like pairs were added together.
Thefrequency information now consists of themother constituent and a set of daughter pairs.Now, for the parser to assess the probabilityof any daughter sequence, this sequence hasfirst to be decomposed into pairs, which arelooked up in the mother-daughter table, andthe probabilities of the pairs aggregatedtogether to give the overall probability ofthe complete sequence.
For the sequencedescribed above the individual pairs wouldbe "\[ATI, ATI JJ, JJ NNS, NNS Fr, Fr \]".It seems clear that in some cases the aggre-gation of the probabilities of two or morepairs does not give a reasonable approximationto the original statistics, because of longer-distance dependencies, It is likely thereforethat this technique will need a dictionaryof pairs together with a dictionary of excep-tional triples, quadruples, etc., to correctthe pairs dictionary where necessary.STAGE TWO - HEURISTICSThe first stage of T-tag assignment intro- ?duces constituent types and boundary markingsonly if they can be expressed in terms oflook-up in a dictionary of tag-pairs.
Howeverthere are a number of cases where a more complexform of processing seems desirable, in orderto produce a more suitable partial parse tobe fed to the third stage.
We are thereforedesigning a second stage, analogous to thesecond stage of the tagging system, whichis able to look for various patterns of tagsand the constituent markings already assignedby the first stage, and then add to or modifythe constituent markings passed to the thirdstage; an area where this will be importantis in coordinated structures.I have suggested in the above that the parsingsystem is constructed as three separate stages,which pass their output to the next stage.In fact this is mainly for expository anddevelopmental reasons, and we envisage aninterconnection between at least some of thestages, so that earlier stages may be ableto take account of information provided bylater stages.PROBLEMS AND CONCLUSIONSI have described the basic structure ofthe parsing system that we are currently devel-oping at Lancaster.
There are of course anumber of areas where the techniques describedwill need to be extended to take account oflingustic structures not provided for.
Butour technique with the tagging project wasto develop basic mechanisms to cope with alarge portion of the texts being processed,and then to modify then to perform more accur-ately in particular areas where they weredeficient, and we expect to follow this proce-dure with the current project.The two main features of the technique weare using seem to be(a) the use of probabilistic methods fordisambiguation of linguistic structures, and(b) the use of a corpus of unconstrainedEnglish text as a testbed for our methods,as a source of information about the statisticalproperties of language, and as an indicatorof what are the important areas of inadequacyin each stage of the analysis system.Because of the success of these techniquesin the tagging system, and because of thepromising results already achieved in applyingthese techniques to the syntactic analysisof a number of simple sentences, we have everyhope of being able to develop a robust andeconomic parsing system able to operate overunconstrained English text with a high degreeof accuracy.RF~CESAtwell, E.S.
(1983), "Constituent-Likelihoo dGrammar".
Newsletter of the InternationalComputer Archive of Modern English (ICAMENews) 7, 34-66.Beale, A.D. (1985), "Grammatical Analysisby Computer of the Lancaster-Oslo/Bergen (LOB)Corpus of British English Texts".
Proceedingsof the Second ACL European Conference (To169appear).Francis, W.N.
and Kucera, H. (1964), "Manualof Information to Accompany a Standard Sampleof Present-Day Edited American English, forUse with Digital Computers".
Department ofLinguistics, Brown University.Greene, B.B.
and Rubin, GoM.
(1971).
"Auto-matic Grammatical Tagging of English".
Depart-ment of Linguistics, Brown University.Leech, G.N., Garside, R.G.
and Atwell, E.S.(1983).
"The Automatic Grammatical Taggingof the LOB Corpus".
Newsletter of the Inter-national Computer Archive of Modern English(ICAME News) 7, 13-33.Marshall, I.
(1983), "Choice of GrammaticalWord-Class without Global Syntactic Analysis:Tagging Words in the LOB Corpus".
Computersand the Humanities 17, 139-50.170
