Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1169?1178, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsLanguage Model Rest Costs and Space-Efficient StorageKenneth Heafield?,?
Philipp Koehn?
Alon Lavie??
Language Technologies InstituteCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213, USA{heafield,alavie}@cs.cmu.edu?
School of InformaticsUniversity of Edinburgh10 Crichton StreetEdinburgh EH8 9AB, UKpkoehn@inf.ed.ac.ukAbstractApproximate search algorithms, such as cubepruning in syntactic machine translation, relyon the language model to estimate probabili-ties of sentence fragments.
We contribute twochanges that trade between accuracy of theseestimates and memory, holding sentence-levelscores constant.
Common practice uses lower-order entries in an N -gram model to scorethe first few words of a fragment; this vio-lates assumptions made by common smooth-ing strategies, including Kneser-Ney.
Instead,we use a unigram model to score the firstword, a bigram for the second, etc.
This im-proves search at the expense of memory.
Con-versely, we show how to save memory by col-lapsing probability and backoff into a singlevalue without changing sentence-level scores,at the expense of less accurate estimates forsentence fragments.
These changes can bestacked, achieving better estimates with un-changed memory usage.
In order to interpretchanges in search accuracy, we adjust the poplimit so that accuracy is unchanged and re-port the change in CPU time.
In a German-English Moses system with target-side syntax,improved estimates yielded a 63% reductionin CPU time; for a Hiero-style version, thereduction is 21%.
The compressed languagemodel uses 26% less RAM while equivalentsearch quality takes 27% more CPU.
Sourcecode is released as part of KenLM.1 IntroductionLanguage model storage is typically evaluated interms of speed, space, and accuracy.
We introducea fourth dimension, rest cost quality, that captureshow well the model scores sentence fragments forpurposes of approximate search.
Rest cost quality isdistinct from accuracy in the sense that the score ofa complete sentence is held constant.
We first showhow to improve rest cost quality over standard prac-tice by using additional space.
Then, conversely, weshow how to compress the language model by mak-ing a pessimistic rest cost assumption1.Language models are designed to assign probabil-ity to sentences.
However, approximate search algo-rithms use estimates for sentence fragments.
If thelanguage model has order N (an N -gram model),then the first N ?
1 words of the fragment have in-complete context and the last N ?
1 words have notbeen completely used as context.
Our baseline iscommon practice (Koehn et al 2007; Dyer et al2010; Li et al 2009) that uses lower-order entriesfrom the language model for the first words in thefragment and no rest cost adjustment for the last fewwords.
Formally, the baseline estimate for sentencefragment wk1 is(N?1?n=1pN (wn|wn?11 ))(k?n=NpN (wn|wn?1n?N+1))where each wn is a word and pN is an N -gram lan-guage model.The problem with the baseline estimate lies inlower order entries pN (wn|wn?11 ).
Commonly usedKneser-Ney (Kneser and Ney, 1995) smoothing,1Here, the term rest cost means an adjustment to the score ofa sentence fragment but not to whole sentences.
The adjustmentmay be good or bad for approximate search.1169including the modified version (Chen and Good-man, 1998), assumes that a lower-order entry willonly be used because a longer match could notbe found2.
Formally, these entries actually eval-uate pN (wn|wn?11 , did not find wn0 ).
For purposesof scoring sentence fragments, additional context issimply indeterminate, and the assumption may nothold.As an example, we built 5-gram and unigram lan-guage models with Kneser-Ney smoothing on thesame data.
Sentence fragments frequently beginwith ?the?.
Using a lower-order entry from the 5-gram model, log10 p5(the) = ?2.49417.
The uni-gram model does not condition on backing off, as-signing log10 p1(the) = ?1.28504.
Intuitively, the5-gram model is surprised, by more than an order ofmagnitude, to see ?the?
without matching words thatprecede it.To remedy the situation, we train N languagemodels on the same data.
Each model pn is an n-gram model (it has order n).
We then use pn toscore the nth word of a sentence fragment.
Thus,a unigram model scores the first word of a sentencefragment, a bigram model scores the second word,and so on until either the n-gram is not present inthe model or the first N?1 words have been scored.Storing probabilities from these models requires oneadditional value per n-gram in the model, except forN -grams where this probability is already stored.Conversely, we can lower memory consumptionrelative to the baseline at the expense of poorer restcosts.
Baseline models store two entries per n-gram:probability and backoff.
We will show that the prob-ability and backoff values in a language model canbe collapsed into a single value for each n-gramwithout changing sentence probability.
This trans-formation saves memory by halving the number ofvalues stored per entry, but it makes rest cost esti-mates worse.
Specifically, the rest cost pessimisti-cally assumes that the model will back off to uni-grams immediately following the sentence fragment.The two modifications can be used independentlyor simultaneously.
To measure the impact of theirdifferent rest costs, we experiment with cube prun-ing (Chiang, 2007) in syntactic machine transla-2Other smoothing techniques, including Witten-Bell (Wittenand Bell, 1991), do not make this assumption.tion.
Cube pruning?s goal is to find high-scoringsentence fragments for the root non-terminal in theparse tree.
It does so by going bottom-up in the parsetree, searching for high-scoring sentence fragmentsfor each non-terminal.
Within each non-terminal, itgenerates a fixed number of high-scoring sentencefragments; this is known as the pop limit.
Increasingthe pop limit therefore makes search more accuratebut costs more time.
By moderating the pop limit,improved accuracy can be interpreted as a reductionin CPU time and vice-versa.2 Related WorkVilar and Ney (2011) study several modifications tocube pruning and cube growing (Huang and Chiang,2007).
Most relevant is their use of a class-basedlanguage model for the first of two decoding passes.This first pass is cheaper because translation alter-natives are likely to fall into the same class.
Entriesare scored with the maximum probability over classmembers (thereby making them no longer normal-ized).
Thus, paths that score highly in this first passmay contain high-scoring paths under the lexicalizedlanguage model, so the second pass more fully ex-plores these options.
The rest cost estimates we de-scribe here could be applied in both passes, so ourwork is largely orthogonal.Zens and Ney (2008) present rest costs for phrase-based translation.
These rest costs are based on fac-tors external to the sentence fragment, namely out-put that the decoder may generate in the future.
Ourrest costs examine words internal to the sentencefragment, namely the first and last few words.
Wealso differ by focusing on syntactic translation.A wide variety of work has been done on languagemodel compression.
While data structure compres-sion (Raj and Whittaker, 2003; Heafield, 2011) andrandomized data structures (Talbot and Osborne,2007; Guthrie and Hepple, 2010) are useful, herewe are concerned solely with the values stored bythese data structures.
Quantization (Whittaker andRaj, 2001; Federico and Bertoldi, 2006) uses lessbits to store each numerical value at the expenseof model quality, including scores of full sentences,and is compatible with our approach.
In fact, thelower-order probabilities might be quantized furtherthan normal since these are used solely for rest cost1170purposes.
Our compression technique reduces stor-age from two values, probability and backoff, to onevalue, theoretically halving the bits per value (ex-cept N -grams which all have backoff 1).
This makesthe storage requirement for higher-quality modifiedKneser-Ney smoothing comparable to stupid back-off (Brants et al 2007).
Whether to use one smooth-ing technique or the other then becomes largely anissue of training costs and quality after quantization.3 Contribution3.1 Better Rest CostsAs alluded to in the introduction, the first few wordsof a sentence fragment are typically scored us-ing lower-order entries from an N -gram languagemodel.
However, Kneser-Ney smoothing (Kneserand Ney, 1995) conditions lower-order probabilitieson backing off.
Specifically, lower-order counts areadjusted to represent the number of unique exten-sions an n-gram has:a(wn1 ) ={|{w0 : c(wn0 ) > 0}| if n < Nc(wn1 ) if n = Nwhere c(wn1 ) is the number of times wn1 appears inthe training data3.
This adjustment is also performedfor modified Kneser-Ney smoothing.
The intuitionis based on the fact that the language model willbase its probability on the longest possible match.
Ifan N -gram was seen in the training data, the modelwill match it fully and use the smoothed count.
Oth-erwise, the full N -gram was not seen in the train-ing data and the model resorts to a shorter n-grammatch.
Probability of this shorter match is based onhow often the n-gram is seen in different contexts.Thus, these shorter n-gram probabilities are not rep-resentative of cases where context is short simplybecause additional context is unknown at the time ofscoring.In some cases, we are able to determine thatthe model will back off and therefore the lower-order probability makes the appropriate assumption.Specifically, if vwn1 does not appear in the model forany word v, then computing p(wn|vwn?11 ) will al-3Counts are not modified for n-grams bound to the begin-ning of sentence, namely those with w1 = <s>.ways back off to wn?11 or fewer words4.
This crite-rion is the same as used to minimize the length of leftlanguage model state (Li and Khudanpur, 2008) andcan be retrieved for each n-gram without using addi-tional memory in common data structures (Heafieldet al 2011).Where it is unknown if the model will back off,we use a language model of the same order to pro-duce a rest cost.
Specifically, there are N languagemodels, one of each order from 1 to N .
The mod-els are trained on the same corpus with the samesmoothing parameters to the extent that they apply.We then compile these into one data structure whereeach n-gram record has three values:1.
Probability pn from the n-gram languagemodel2.
Probability pN from the N -gram languagemodel3.
Backoff b from the N -gram language modelFor N -grams, the two probabilities are the same andbackoff is always 1, so only one value is stored.Without pruning, the n-gram model contains thesame n-grams as the N -gram model.
With prun-ing, the two sets may be different, so we query then-gram model in the normal way to score every n-gram in the N -gram model.
The idea is that pn is theaverage conditional probability that will be encoun-tered once additional context becomes known.
Wealso tried more complicated estimates by addition-ally interpolating upper bound, lower bound, and pNwith weights trained on cube pruning logs; none ofthese improved results in any meaningful way.Formalizing the above, let wk1 be a sentence frag-ment.
Choose the largest s so that vws1 appears inthe model for some v; equivalently ws1 is the leftstate described in Li and Khudanpur (2008).
The4Usually, this happens because wn1 does not appear, thoughit can also happen that wn1 appears but all vwn1 were removedby pruning or filtering.1171baseline estimate ispb(wk1) =(s?n=1pN (wn|wn?11 ))?
(N+1?n=s+1pN (wn|wn?11 ))?
(1)(k?n=NpN (wn|wn?1n?N+1))while our improved estimate ispr(wk1) =(s?n=1pn(wn|wn?11 ))?
(N+1?n=s+1pN (wn|wn?11 ))?
(2)(k?n=NpN (wn|wn?1n?N+1))The difference between these equations is that pn isused for words in the left state i.e.
1 ?
n ?
s.We have also abused notation by using pN to denoteboth probabilities stored explicitly in the model andthe model?s backoff-smoothed probabilities whennot present.
It is not necessary to store backoffs forpn because s was chosen such that all queried n-grams appear in the model.This modification to the language model improvesrest costs (and therefore quality or CPU time) at theexpense of using more memory to store pn.
In thenext section, we do the opposite: make rest costsworse to reduce storage size.3.2 Less MemoryMany language model smoothing strategies, includ-ing modified Kneser-Ney smoothing, use the back-off algorithm shown in Figure 1.
Given an n-gramwn1 , the backoff algorithm bases probability on asmuch context as possible.
Equivalently, it findsthe minimum f so that wnf is in the model thenuses p(wn|wn?1f ) as a basis.
Backoff penalties bare charged because a longer match was not found,forming the productp(wn|wn?11 ) = p(wn|wn?1f )f?1?j=1b(wn?1j ) (3)Notably, the backoff penalties {b(wn?1j )}n?1j=1 are in-dependent of wn, though which backoff penalties arecharged depends on f and therefore wn.backoff?
1for f = 1?
n doif wnf is in the model thenreturn p(wn|wn?1f ) ?
backoffelseif wn?1f is in the model thenbackoff?
backoff ?
b(wn?1f )end ifend ifend forFigure 1: The baseline backoff algorithm to com-pute p(wn|wn?11 ).
It always terminates with a prob-ability because even unknown words are treated as aunigram.for f = 1?
n doif wnf is in the model thenreturn q(wn|wn?1f )end ifend forFigure 2: The run-time pessimistic backoff algo-rithm to find q(wn|wn?11 ).
It assumes that q has beencomputed at model building time.In order to save memory, we propose to accountfor backoff in a different way, defining qq(wn|wn?11 ) =p(wn|wn?1f )?nj=f b(wnj )?n?1j=f b(wn?1j )where again wnf is the longest matching entry in themodel.
The idea is that q is a term in the telescop-ing series that scores a sentence fragment, shownin equation (1) or (2).
The numerator pessimisti-cally charges all backoff penalties, as if the nextword wn+1 will only match a unigram.
When wn+1is scored, the denominator of q(wn+1|wn1 ) cancelsout backoff terms that were wrongly charged.
Oncethese terms are canceled, all that is left is p, the cor-rect backoff penalties, and terms on the edge of theseries.1172Proposition 1.
The terms of q telescope.
Formally,let wk1 be a sentence fragment and f take the mini-mum value so that wkf is in the model.
Then,q(wk1) = p(wk1)k?j=fb(wkj )Proof.
By induction on k. When k = 1, f = 1 sincethe word w1 is either in the vocabulary or mapped to<unk> and treated like a unigram.q(w1) =p(w1)?1j=1 b(w1j )?0j=1 b(w0j )= p(w1)b(w1)For k > 1,q(wk1) = q(wk?11 )q(wk|wk?11 )=q(wk?11 )p(wk|wk?1f )?kj=f b(wkj )?k?1j=f b(wk?1j )where f has the lowest value such that wkf is in themodel.
Applying the inductive hypothesis to expandq(wk?11 ), we obtainp(wk?11 )(?k?1j=e b(wk?1j ))p(wk|wk?1f )?kj=f b(wkj )?k?1j=f b(wk?1j )where e has the lowest value such that wk?1e is in themodel.
The backoff terms cancel to yieldp(wk?11 )?
?f?1?j=eb(wk?1j )??
p(wk|wk?1f )k?j=fb(wkj )By construction of e, wk?1j is not in the model for allj < e. Hence, b(wk?1j ) = 1 implicitly for all j < e.Multiplying by 1,p(wk?11 )?
?f?1?j=1b(wk?1j )??
p(wk|wk?1f )k?j=fb(wkj )Recognizing the backoff equation (3) to simplify,p(wk?11 )p(wk|wk?11 )k?j=fb(wkj )Finally, the conditional probability folds as desiredq(wk1) = p(wk1)k?j=fb(wkj )We note that entries ending in </s> have back-off 1, so it follows from Proposition 1 that sentence-level scores are unchanged.q(<s> wk1 </s>) = p(<s> wk1 </s>)Proposition 1 characterizes q as a pessimistic restcost on sentence fragments that scores sentences inexactly the same way as the baseline using p andb.
To save memory, we simply store q in lieu ofp and b.
Compared with the baseline, this halvesnumber of values from two to one float per n-gram,except N -grams that already have one value.
Theimpact of this reduction is substantial, as seen inSection 4.3.
Run-time scoring is also simplifiedas shown in Figure 2 since the language model lo-cates the longest match wnf then returns the valueq(wn|wn?11 ) = q(wn|wn?1f ) without any calcula-tion or additional lookup.
Baseline language mod-els either retrieve backoffs values with additionallookups (Stolcke, 2002; Federico et al 2008) ormodify the decoder to annotate sentence fragmentswith backoff information (Heafield, 2011); we haveeffectively moved this step to preprocessing.
Thedisadvantage is that q is not a proper probability andit produces worse rest costs than does the baseline.Language models are actually applied at twopoints in syntactic machine translation: scoring lexi-cal items in grammar rules and during cube pruning.Grammar scoring is an offline and embarrassinglyparallel process where memory is not as tight (sincethe phrase table is streamed) and fewer queriesare made, so slow non-lossy compression and evennetwork-based sharding can be used.
We there-fore use an ordinary language model for grammarscoring and only apply the compressed model dur-ing cube pruning.
Grammar scoring impacts gram-mar pruning (by selecting only top-scoring grammarrules) and the order in which rules are tried duringcube pruning.11733.3 Combined SchemeOur two language model modifications can be triv-ially combined by using lower-order probabilities onthe left of a fragment and by charging all backoffpenalties on the right of a fragment.
The net result isa language model that uses the same memory as thebaseline but has better rest cost estimates.4 ExperimentsTo measure the impact of different rest costs, weuse the Moses chart decoder (Koehn et al 2007)for the WMT 2011 German-English translation task(Callison-Burch et al 2011).
Using the Mosespipeline, we trained two syntactic German-Englishsystems, one with target-side syntax and the otherhierarchical with unlabeled grammar rules (Chiang,2007).
Grammar rules were extracted from Europarl(Koehn, 2005) using the Collins parser (Collins,1999) for syntax on the English side.
The languagemodel interpolates, on the WMT 2010 test set, sep-arate models built on Europarl, news commentary,and the WMT news data for each year.
Models werebuilt and interpolated using SRILM (Stolcke, 2002)with modified Kneser-Ney smoothing (Kneser andNey, 1995; Chen and Goodman, 1998) and the de-fault pruning settings.
In all scenarios, the primarylanguage model has order 5.
For lower-order restcosts, we also built models with orders 1 through 4then used the n-gram model to score n-grams in the5-gram model.
Feature weights were trained withMERT (Och, 2003) on the baseline using a pop limitof 1000 and 100-best output.
Since final feature val-ues are unchanged, we did not re-run MERT in eachcondition.
Measurements were collected by runningthe decoder on the 3003-sentence test set.4.1 Rest Costs as PredictionScoring the first few words of a sentence fragmentis a prediction task.
The goal is to predict whatthe probability will be when more context becomesknown.
In order to measure performance on thistask, we ran the decoder on the hierarchical systemwith a pop limit of 1000.
Every time more contextbecame known, we logged5 the prediction error (es-timated log probability minus updated log probabil-5Logging was only enabled for this experiment.Lower Baselinen Mean Bias MSE Var Bias MSE Var1 -3.21 .10 .84 .83 -.12 .87 .862 -2.27 .04 .18 .17 -.14 .23 .243 -1.80 .02 .07 .07 -.09 .10 .094 -1.29 .01 .04 .04 -.10 .09 .08Table 1: Bias (mean error), mean squared error, andvariance (of the error) for the lower-order rest costand the baseline.
Error is the estimated log prob-ability minus the final probability.
Statistics werecomputed separately for the first word of a fragment(n = 1), the second word (n = 2), etc.
The lower-order estimates are better across the board, reducingerror in cube pruning.
All numbers are in log baseten, as is standard for ARPA-format language mod-els.
Statistics were only collected for words withincomplete context.ity) for both lower-order rest costs and the baseline.Table 1 shows the results.Cube pruning uses relative scores, so bias mat-ters less, though positive bias will favor rules withmore arity.
Variance matters the most because lowervariance means cube pruning?s relative rankings aremore accurate.
Our lower-order rest costs are bet-ter across the board in terms of absolute bias, meansquared error, and variance.4.2 Pop Limit Trade-OffsThe cube pruning pop limit is a trade-off betweensearch accuracy and CPU time.
Here, we mea-sure how our rest costs improve (or degrade) thattrade-off.
Search accuracy is measured by the aver-age model score of single-best translations.
Modelscores are scale-invariant and include a large con-stant factor; higher is better.
We also measure over-all performance with uncased BLEU (Papineni et al2002).
CPU time is the sum of user and system timeused by Moses divided by the number of sentences(3003).
Timing includes time to load, though fileswere forced into the disk cache in advance.
Our testmachine has 64 GB of RAM and 32 cores.
Resultsare shown in Figures 3 and 4.Lower-order rest costs perform better in both sys-tems, reaching plateau model scores and BLEU withless CPU time.
The gain is much larger for tar-1174Baseline Lower Order Pessimistic CombinedPop CPU Model BLEU CPU Model BLEU CPU Model BLEU CPU Model BLEU2 3.29 -105.56 20.45 3.68 -105.44 20.79 3.74 -105.62 20.01 3.18 -105.49 20.4310 5.21 -104.74 21.13 5.50 -104.72 21.26 5.43 -104.77 20.85 5.67 -104.75 21.1050 23.30 -104.31 21.36 23.51 -104.24 21.38 23.68 -104.33 21.25 24.29 -104.22 21.34500 54.61 -104.25 21.33 55.92 -104.15 21.38 54.23 -104.26 21.31 55.74 -104.15 21.40700 64.08 -104.25 21.34 87.02 -104.14 21.42 68.74 -104.25 21.29 78.84 -104.15 21.41(a) Numerical results for select pop limits.-104.6-104.5-104.4-104.3-104.2-104.10 10 20 30 40 50 60 70 80 90AveragemodelscoreCPU seconds/sentenceLowerCombinedBaselinePessimistic2121.0521.121.1521.221.2521.321.3521.40 10 20 30 40 50 60 70 80 90UncasedBLEUCPU seconds/sentenceLowerCombinedBaselinePessimistic(b) Model and BLEU scores near the plateau.Figure 3: Target-syntax performance.
CPU time and model score are averaged over 3003 sentences.get syntax, where a pop limit of 50 outperforms thebaseline with pop limit 700.
CPU time per sen-tence is reduced to 23.5 seconds from 64.0 seconds,a 63.3% reduction.
The combined setting, using thesame memory as the baseline, shows a similar 62.1%reduction in CPU time.
We attribute this differ-ence to improved grammar rule scoring that impactspruning and sorting.
In the target syntax model,the grammar is not saturated (i.e.
less pruning willstill improve scores) but we nonetheless prune fortractability reasons.
The lower-order rest costs areparticularly useful for grammar pruning because lex-ical items are typically less than five words long (andfrequently only word).The hierarchical grammar is nearly saturated withrespect to grammar pruning, so improvement there isdue mostly to better search.
In the hierarchical sys-tem, peak BLEU 22.34 is achieved under the lower-order condition with pop limits 50 and 200, whileother scenarios are still climbing to the plateau.
Witha pop limit of 1000, the baseline?s average modelscore is -101.3867.
Better average models scoresare obtained from the lower-order model with poplimit 690 using 79% of baseline CPU, the combinedmodel with pop limit 900 using 97% CPU, and thepessimistic model with pop limit 1350 using 127%CPU.Pessimistic compression does worsen search, re-quiring 27% more CPU in the hierarchical system toachieve the same quality.
This is worthwhile to fitlarge-scale language models in memory, especiallyif the alternative is a remote language model.4.3 Memory UsageOur rest costs add a value (for lower-order prob-abilities) or remove a value (pessimistic compres-sion) for each n-gram except those of highest order(n = N ).
The combined condition adds one value1175Baseline Lower Order Pessimistic CombinedPop CPU Model BLEU CPU Model BLEU CPU Model BLEU CPU Model BLEU2 2.96 -101.85 21.19 2.44 -101.80 21.63 2.71 -101.90 20.85 3.05 -101.84 21.3710 2.80 -101.60 21.90 2.42 -101.58 22.20 2.95 -101.63 21.74 2.69 -101.60 21.9850 3.02 -101.47 22.18 3.11 -101.46 22.34 3.46 -101.48 22.08 2.67 -101.47 22.14690 10.83 -101.39 22.28 11.45 -101.39 22.25 10.88 -101.40 22.25 11.19 -101.39 22.23900 13.41 -101.39 22.27 14.00 -101.38 22.24 13.38 -101.39 22.25 14.09 -101.39 22.221000 14.50 -101.39 22.27 15.17 -101.38 22.25 15.09 -101.39 22.26 15.23 -101.39 22.231350 18.52 -101.38 22.27 19.16 -101.38 22.23 18.46 -101.39 22.25 18.61 -101.38 22.235000 59.67 -101.38 22.24 61.41 -101.38 22.22 59.76 -101.38 22.27 61.38 -101.38 22.22(a) Numerical results for select pop limits.-101.42-101.415-101.41-101.405-101.4-101.395-101.39-101.385-101.380 5 10 15 20 25AveragemodelscoreCPU seconds/sentenceLowerCombinedBaselinePessimistic21.952222.0522.122.1522.222.2522.322.350 5 10 15 20 25UncasedBLEUCPU seconds/sentenceLowerCombinedBaselinePessimistic(b) Model and BLEU scores near the plateau.Figure 4: Hierarchical system performance.
All values are averaged over 3003 sentences.and removes another, so it uses the same memoryas the baseline.
The memory footprint of adding orremoving a value depends on the number of such n-grams, the underlying data structure, and the extentof quantization.
Our test language model has 135million n-grams for n < 5 and 56 million 5-grams.Memory usage was measured for KenLM data struc-tures (Heafield, 2011) and minimal perfect hashing(Guthrie and Hepple, 2010).
For minimal perfecthashing, we assume the Compress, Hash and Dis-place algorithm (Belazzougui et al 2008) with 8-bitsignatures and 8-bit quantization.
Table 2 shows theresults.
Storage size of the smallest model is reducedby 26%, bringing higher-quality smoothed modelsin line with stupid backoff models that also store onevalue per n-gram.Structure Baseline Change %Probing 4,072 517 13%Trie 2,647 506 19%8-bit quantized trie 1,236 140 11%8-bit minimal perfect hash 540 140 26%Table 2: Size in megabytes of our language model,excluding operating system overhead.
Change is thecost of adding an additional value to store lower-order probabilities.
Equivalently, it is the savingsfrom pessimistic compression.11765 ConclusionOur techniques reach plateau-level BLEU scoreswith less time or less memory.
Efficiently stor-ing lower-order probabilities and using them as restcosts improves both cube pruning (21% CPU reduc-tion in a hierarchical system) and model filtering(net 63% CPU time reduction with target syntax) atthe expense of 13-26% more RAM for the languagemodel.
This model filtering improvement is surpris-ing both in the impact relative to changing the poplimit and simplicity of implementation, since it canbe done offline.
Compressing the language model tohalve the number of values per n-gram (except N -grams) results in a 13-26% reduction in RAM with26% over the smallest model, costing 27% moreCPU and leaving overall sentence scores unchanged.This compression technique is likely to have moregeneral application outside of machine translation,especially where only sentence-level scores are re-quired.
Source code is being released6 under theLGPL as part of KenLM (Heafield, 2011).AcknowledgementsThis work was supported by the National Sci-ence Foundation under grants DGE-0750271, IIS-0713402, and IIS-0915327; by the EuroMatrixPlusproject funded by the European Commission (7thFramework Programme), and by the DARPA GALEprogram.
Benchmarks were run on Trestles at theSan Diego Supercomputer Center under allocationTG-CCR110017.
Trestles is part of the ExtremeScience and Engineering Discovery Environment(XSEDE), which is supported by National ScienceFoundation grant number OCI-1053575.ReferencesDjamal Belazzougui, Fabiano C. Botelho, and Martin Di-etzfelbinger.
2008.
Hash, displace, and compress.
InProceedings of the 35th international colloquium onAutomata, Languages and Programming (ICALP ?08),pages 385?396.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J.Och, and Jeffrey Dean.
2007.
Large language mod-els in machine translation.
In Proceedings of the 2007Joint Conference on Empirical Methods in Natural6http://kheafield.com/code/kenlm/Language Processing and Computational LanguageLearning, pages 858?867, June.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011 work-shop on statistical machine translation.
In Proceedingsof the Sixth Workshop on Statistical Machine Transla-tion, pages 22?64, Edinburgh, Scotland, July.
Associ-ation for Computational Linguistics.Stanley Chen and Joshua Goodman.
1998.
An empiricalstudy of smoothing techniques for language modeling.Technical Report TR-10-98, Harvard University, Au-gust.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33:201?228, June.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec:A decoder, alignment, and learning framework forfinite-state and context-free translation models.
InProceedings of the ACL 2010 System Demonstrations,ACLDemos ?10, pages 7?12.Marcello Federico and Nicola Bertoldi.
2006.
Howmany bits are needed to store probabilities for phrase-based translation?
In Proceedings of the Workshop onStatistical Machine Translation, pages 94?101, NewYork City, June.Marcello Federico, Nicola Bertoldi, and Mauro Cettolo.2008.
IRSTLM: an open source toolkit for handlinglarge scale language models.
In Proceedings of Inter-speech, Brisbane, Australia.David Guthrie and Mark Hepple.
2010.
Storing the webin memory: Space efficient language models with con-stant time retrieval.
In Proceedings of EMNLP 2010,Los Angeles, CA.Kenneth Heafield, Hieu Hoang, Philipp Koehn, TetsuoKiso, and Marcello Federico.
2011.
Left languagemodel state for syntactic machine translation.
In Pro-ceedings of the International Workshop on SpokenLanguage Translation, San Francisco, CA, USA, De-cember.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, Edin-burgh, UK, July.
Association for Computational Lin-guistics.Liang Huang and David Chiang.
2007.
Forest rescoring:Faster decoding with integrated language models.
InProceedings of the 45th Annual Meeting of the Asso-ciation for Computational Linguistics, Prague, CzechRepublic.1177Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
In Pro-ceedings of the IEEE International Conference onAcoustics, Speech and Signal Processing, pages 181?184.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In AnnualMeeting of the Association for Computational Linguis-tics (ACL), Prague, Czech Republic, June.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of MTSummit.Zhifei Li and Sanjeev Khudanpur.
2008.
A scalabledecoder for parsing-based machine translation withequivalent language model state maintenance.
In Pro-ceedings of the Second ACL Workshop on Syntax andStructure in Statistical Translation (SSST-2), pages10?18, Columbus, Ohio, June.Zhifei Li, Chris Callison-Burch, Chris Dyer, SanjeevKhudanpur, Lane Schwartz, Wren Thornton, JonathanWeese, and Omar Zaidan.
2009.
Joshua: An opensource toolkit for parsing-based machine translation.In Proceedings of the Fourth Workshop on StatisticalMachine Translation, pages 135?139, Athens, Greece,March.
Association for Computational Linguistics.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In ACL ?03: Pro-ceedings of the 41st Annual Meeting on Associationfor Computational Linguistics, pages 160?167, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevalution of machine translation.
In Proceedings 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 311?318, Philadelphia, PA, July.Bhiksha Raj and Ed Whittaker.
2003.
Lossless compres-sion of language model structure and word identifiers.In Proceedings of IEEE International Conference onAcoustics, Speech and Signal Processing, pages 388?391.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of the Seventh Inter-national Conference on Spoken Language Processing,pages 901?904.David Talbot and Miles Osborne.
2007.
Randomisedlanguage modelling for statistical machine translation.In Proceedings of ACL, pages 512?519, Prague, CzechRepublic.David Vilar and Hermann Ney.
2011.
Cardinality prun-ing and language model heuristics for hierarchicalphrase-based translation.
Machine Translation, pages1?38, November.
DOI 10.1007/s10590-011-9119-4.Ed Whittaker and Bhiksha Raj.
2001.
Quantization-based language model compression.
In Proceedingsof EUROSPEECH, pages 33?36, September.Ian H. Witten and Timothy C. Bell.
1991.
The zero-frequency problem: Estimating the probabilities ofnovel events in adaptive text compression.
IEEETransactions on Information Theory, 37(4):1085?1094.Richard Zens and Hermann Ney.
2008.
Improvements indynamic programming beam search for phrase-basedstatistical machine translation.
In Proceedings of theInternational Workshop on Spoken Language Transla-tion (IWSLT), Honolulu, Hawaii, October.1178
