Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 160?167,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsMildly Context-Sensitive Dependency LanguagesMarco KuhlmannProgramming Systems LabSaarland UniversitySaarbr?cken, Germanykuhlmann@ps.uni-sb.deMathias M?hlProgramming Systems LabSaarland UniversitySaarbr?cken, Germanymmohl@ps.uni-sb.deAbstractDependency-based representations of natu-ral language syntax require a fine balancebetween structural flexibility and computa-tional complexity.
In previous work, severalconstraints have been proposed to identifyclasses of dependency structures that are well-balanced in this sense; the best-known butalso most restrictive of these is projectivity.Most constraints are formulated on fully spec-ified structures, which makes them hard to in-tegrate into models where structures are com-posed from lexical information.
In this paper,we show how two empirically relevant relax-ations of projectivity can be lexicalized, andhow combining the resulting lexicons with aregular means of syntactic composition givesrise to a hierarchy of mildly context-sensitivedependency languages.1 IntroductionSyntactic representations based on word-to-word de-pendencies have a long tradition in descriptive lin-guistics.
Lately, they have also been used in manycomputational tasks, such as relation extraction (Cu-lotta and Sorensen, 2004), parsing (McDonald et al,2005), and machine translation (Quirk et al, 2005).Especially in recent work on parsing, there is a par-ticular interest in non-projective dependency struc-tures, in which a word and its dependents may bespread out over a discontinuous region of the sen-tence.
These structures naturally arise in the syntacticanalysis of languages with flexible word order, suchas Czech (Vesel?
et al, 2004).
Unfortunately, mostformal results on non-projectivity are discouraging:While grammar-driven dependency parsers that arerestricted to projective structures can be as efficientas parsers for lexicalized context-free grammar (Eis-ner and Satta, 1999), parsing is prohibitively expen-sive when unrestricted forms of non-projectivity arepermitted (Neuhaus and Br?ker, 1997).
Data-drivendependency parsing with non-projective structures isquadratic when all attachment decisions are assumedto be independent of one another (McDonald et al,2005), but becomes intractable when this assumptionis abandoned (McDonald and Pereira, 2006).In search of a balance between structural flexibilityand computational complexity, several authors haveproposed constraints to identify classes of non-projec-tive dependency structures that are computationallywell-behaved (Bodirsky et al, 2005; Nivre, 2006).In this paper, we focus on two of these proposals:the gap-degree restriction, which puts a bound onthe number of discontinuities in the region of a sen-tence covered by a word and its dependents, and thewell-nestedness condition, which constrains the ar-rangement of dependency subtrees.
Both constraintshave been shown to be in very good fit with data fromdependency treebanks (Kuhlmann and Nivre, 2006).However, like all other such proposals, they are for-mulated on fully specified structures, which makes ithard to integrate them into a generative model, wheredependency structures are composed from elemen-tary units of lexicalized information.
Consequently,little is known about the generative capacity and com-putational complexity of languages over restrictednon-projective dependency structures.160Contents of the paper In this paper, we show howthe gap-degree restriction and the well-nestednesscondition can be captured in dependency lexicons,and how combining such lexicons with a regularmeans of syntactic composition gives rise to an infi-nite hierarchy of mildly context-sensitive languages.The technical key to these results is a procedureto encode arbitrary, even non-projective dependencystructures into trees (terms) over a signature of localorder-annotations.
The constructors of these treescan be read as lexical entries, and both the gap-de-gree restriction and the well-nestedness conditioncan be couched as syntactic properties of these en-tries.
Sets of gap-restricted dependency structurescan be described using regular tree grammars.
Thisgives rise to a notion of regular dependency lan-guages, and allows us to establish a formal relationbetween the structural constraints and mildly con-text-sensitive grammar formalisms (Joshi, 1985): Weshow that regular dependency languages correspondto the sets of derivations of lexicalized Linear Con-text-Free Rewriting Systems (lcfrs) (Vijay-Shankeret al, 1987), and that the gap-degree measure is thestructural correspondent of the concept of ?fan-out?in this formalism (Satta, 1992).
We also show thatadding the well-nestedness condition correspondsto the restriction of lcfrs to Coupled Context-FreeGrammars (Hotz and Pitsch, 1996), and that regu-lar sets of well-nested structures with a gap-degreeof at most 1 are exactly the class of sets of deriva-tions of Lexicalized Tree Adjoining Grammar (ltag).This result generalizes previous work on the relationbetween ltag and dependency representations (Ram-bow and Joshi, 1997; Bodirsky et al, 2005).Structure of the paper The remainder of this pa-per is structured as follows.
Section 2 contains somebasic notions related to trees and dependency struc-tures.
In Section 3 we present the encoding of depen-dency structures as order-annotated trees, and showhow this encoding allows us to give a lexicalized re-formulation of both the gap-degree restriction and thewell-nestedness condition.
Section 4 introduces thenotion of regular dependency languages.
In Section 5we show how different combinations of restrictionson non-projectivity in these languages correspondto different mildly context-sensitive grammar for-malisms.
Section 6 concludes the paper.2 PreliminariesThroughout the paper, we write ?n?
for the set of allpositive natural numbers up to and including n. Theset of all strings over a set A is denoted by A, theempty string is denoted by ", and the concatenationof two strings x and y is denoted either by xy, or,where this is ambiguous, by x  y.2.1 TreesIn this paper, we regard trees as terms.
We expect thereader to be familiar with the basic concepts relatedto this framework, and only introduce our particularnotation.
Let ?
be a set of labels.
The set of (finite,unranked) trees over ?
is defined recursively by theequation T?
?
f .x/ j  2 ?
; x 2 T ?
g. The setof nodes of a tree t 2 T?
is defined asN..t1    tn//?
f"g [ f iu j i 2 ?n?
; u 2 N.ti / g :For two nodes u; v 2 N.t/, we say that u governs v,and write u E v, if v can be written as v D ux, forsome sequence x 2 N.
Note that the governancerelation is both reflexive and transitive.
The converseof government is called dependency, so u E v canalso be read as ?v depends on u?.
The yield of anode u 2 N.t/, buc, is the set of all dependents of uin t : buc ?
f v 2 N.t/ j u E v g. We also use thenotations t .u/ for the label at the node u of t , andt=u for the subtree of t rooted at u.
A tree languageover ?
is a subset of T?
.2.2 Dependency structuresFor the purposes of this paper, a dependency structureover ?
is a pair d D .t; x/, where t 2 T?
is a tree,and x is a list of the nodes in t .
We write D?
torefer to the set of all dependency structures over ?
.Independently of the governance relation in d , thelist x defines a total order on the nodes in t ; wewrite u  v to denote that u precedes v in this order.Note that, like governance, the precedence relation isboth reflexive and transitive.
A dependency languageover ?
is a subset of D?
.Example.
The left half of Figure 1 shows how wevisualize dependency structures: circles representnodes, arrows represent the relation of (immediate)governance, the left-to-right order of the nodes repre-sents their order in the precedence relation, and thedotted lines indicate the labelling.
161a b c d e f21111hf; 0ihe; 01iha; 012ihc ; 0ihd ; 10ihb ; 01iFigure 1: A projective dependency structure3 Lexicalizing the precedence relationIn this section, we show how the precedence relationof dependency structures can be encoded as, anddecoded from, a collection of node-specific orderannotations.
Under the assumption that the nodes ofa dependency structure correspond to lexemic units,this result demonstrates how word-order informationcan be captured in a dependency lexicon.3.1 Projective structuresLexicalizing the precedence relation of a dependencystructure is particularly easy if the structure underconsideration meets the condition of projectivity.
Adependency structure is projective, if each of itsyields forms an interval with respect to the prece-dence order (Kuhlmann and Nivre, 2006).In a projective structure, the interval that corre-sponds to a yield buc decomposes into the singletoninterval ?u; u?, and the collection of the intervals thatcorrespond to the yields of the immediate dependentsof u.
To reconstruct the global precedence relation,it suffices to annotate each node u with the relativeprecedences among the constituent parts of its yield.We represent this ?local?
order as a string over thealphabet N0, where the symbol 0 represents the sin-gleton interval ?u; u?, and a symbol i ?
0 representsthe interval that corresponds to the yield of the i thdirect dependent of u.
An order-annotated tree is atree labelled with pairs h; !i, where  is the labelproper, and !
is a local order annotation.
In whatfollows, we will use the functional notations .u/and !.u/ to refer to the label and order annotationof u, respectively.Example.
Figure 1 shows a projective dependencystructure together with its representation as an order-annotated tree.
We now present procedures for encoding projec-tive dependency structures into order-annotated trees,and for reversing this encoding.Encoding The representation of a projective depen-dency structure .t; x/ as an order-annotated tree canbe computed in a single left-to-right sweep over x.Starting with a copy of the tree t in which everynode is annotated with the empty string, for each newnode u in x, we update the order annotation of uthrough the assignment !.u/?
!.u/ 0 .
If u D vifor some i 2 N (that is, if u is an inner node), wealso update the order annotation of the parent v of uthrough the assignment !.v/?
!.v/  i .Decoding To decode an order-annotated tree t , wefirst linearize the nodes of t into a sequence x, andthen remove all order annotations.
Linearization pro-ceeds in a way that is very close to a pre-order traver-sal of the tree, except that the relative position ofthe root node of a subtree is explicitly specified inthe order annotation.
Specifically, to linearize an or-der-annotated tree, we look into the local order !.u/annotated at the root node of the tree, and concatenatethe linearizations of its constituent parts.
A symbol iin !.u/ represents either the singleton interval ?u; u?
(i D 0), or the interval corresponding to some directdependent ui of u (i ?
0), in which case we pro-ceed recursively.
Formally, the linearization of u iscaptured by the following three equations:lin.u/ D lin0.u; !.u//lin0.u; i1    in/ D lin00.u; i1/    lin00.u; in/lin00.u; i/ D if i D 0 then u else lin.ui/Both encoding and decoding can be done in timelinear in the number of nodes of the dependencystructure or order-annotated tree.3.2 Non-projective structuresIt is straightforward to see that our representation ofdependency structures is insufficient if the structuresunder consideration are non-projective.
To witness,consider the structure shown in Figure 2.
Encodingthis structure using the procedure presented aboveyields the same order-annotated tree as the one shownin Figure 1, which demonstrates that the encoding isnot reversible.162a b c de f12111ha; h01212iihc; h0iihe; h0; 1iihf ; h0iihb ; h01; 1iihd ; h1; 0iiFigure 2: A non-projective dependency structureBlocks In a non-projective dependency structure,the yield of a node may be spread out over more thanone interval; we will refer to these intervals as blocks.Two nodes v;w belong to the same block of a node u,if all nodes between v and w are governed by u.Example.
Consider the nodes b; c; d in the struc-tures depicted in Figures 1 and 2.
In Figure 1, thesenodes belong to the same block of b.
In Figure 2,the three nodes are spread out over two blocks of b(marked by the boxes): c and d are separated by anode (e) not governed by b.
Blocks have a recursive structure that is closely re-lated to the recursive structure of yields: the blocks ofa node u can be decomposed into the singleton ?u; u?,and the blocks of the direct dependents of u.
Just asa projective dependency structure can be representedby annotating each yield with an order on its con-stituents, an unrestricted structure can be representedby annotating each block.Extended order annotations To represent orderson blocks, we extend our annotation scheme as fol-lows.
First, instead of a single string, an annotation!.u/ now is a tuple of strings, where the kth com-ponent specifies the order among the constituents ofthe kth block of u.
Second, instead of one, the an-notation may now contain multiple occurrences ofthe same dependent; the kth occurrence of i in !.u/represents the kth block of the node ui .We write !.u/k to refer to the kth component ofthe order annotation of u.
We also use the notation.i#k/u to refer to the kth occurrence of i in !.u/,and omit the subscript when the node u is implicit.Example.
In the annotated tree shown in Figure 2,!.b/1 D .0#1/.1#1/, and !.b/2 D .1#2/.
Encoding To encode a dependency structure .t; x/as an extended order-annotated tree, we do a post-order traversal of t as follows.
For a given node u, letus represent a constituent of a block of u as a triplei W ?vl ; vr ?, where i denotes the node that contributesthe constituent, and vl and vr denote the constituent?sleftmost and rightmost elements.
At each node u, wehave access to the singleton block 0 W ?u; u?, and theconstituent blocks of the immediate dependents of u.We say that two blocks i W ?vl ; vr ?
; j W ?wl ; wr ?
canbe merged, if the node vr immediately precedes thenode wl .
The result of the merger is a new block ij W?vl ; wr ?
that represents the information that the twomerged constituents belong to the same block of u.By exhaustive merging, we obtain the constituentstructure of all blocks of u.
From this structure, wecan read off the order annotation !.u/.Example.
The yield of the node b in Figure 2 de-composes into 0 W ?b; b?, 1 W ?c; c?, and 1 W ?d; d ?.Since b and c are adjacent, the first two of these con-stituents can be merged into a new block 01 W ?b; c?
;the third constituent remains unchanged.
This givesrise to the order annotation h01; 1i for b.
When using a global data-structure to keep trackof the constituent blocks, the encoding procedure canbe implemented to run in time linear in the numberof blocks in the dependency structure.
In particular,for projective dependency structures, it still runs intime linear in the number of nodes.Decoding To linearize the kth block of a node u,we look into the kth component of the order anno-tated at u, and concatenate the linearizations of itsconstituent parts.
Each occurrence .i#k/ in a com-ponent of !.u/ represents either the node u itself(i D 0), or the kth block of some direct dependent uiof u (i ?
0), in which case we proceed recursively:lin.u; k/ D lin0.u; !.u/k/lin0.u; i1    in/ D lin00.u; i1/    lin00.u; in/lin00.u; .i#k/u/ D if i D 0 then u else lin.ui; k/The root node of a dependency structure has onlyone block.
Therefore, to linearize a tree t , we onlyneed to linearize the first block of the tree?s root node:lin.t/ D lin.
"; 1/.163Consistent order annotations Every dependencystructure over?
can be encoded as a tree over the set?
?, where ?
is the set of all order annotations.The converse of this statement does not hold: to beinterpretable as a dependency structure, tree structureand order annotation in an order-annotated tree mustbe consistent, in the following sense.Property C1: Every annotation !.u/ in a tree tcontains all and only the symbols in the collectionf0g [ f i j ui 2 N.t/ g, i.e., one symbol for u, andone symbol for every direct dependent of u.Property C2: The number of occurrences of asymbol i ?
0 in !.u/ is identical to the number ofcomponents in the annotation of the node ui .
Further-more, the number of components in the annotationof the root node is 1.With this notion of consistency, we can prove thefollowing technical result about the relation betweendependency structures and annotated trees.
We write?
.s/ for the tree obtained from a tree s 2 T??by re-labelling every node u with .u/.Proposition 1.
For every dependency structure.t; x/ over ?
, there exists a tree s over ?
?
suchthat ?
.s/ D t and lin.s/ D x. Conversely, forevery consistently order-annotated tree s 2 T??
,there exists a uniquely determined dependency struc-ture .t; x/ with these properties.
3.3 Local versions of structural constraintsThe encoding of dependency structures as order-an-notated trees allows us to reformulate two constraintson non-projectivity originally defined on fully speci-fied dependency structures (Bodirsky et al, 2005) interms of syntactic properties of the order annotationsthat they induce:Gap-degree The gap-degree of a dependencystructure is the maximum over the number of dis-continuities in any yield of that structure.Example.
The structure depicted in Figure 2 hasgap-degree 1: the yield of b has one discontinuity,marked by the node e, and this is the maximal numberof discontinuities in any yield of the structure.
Since a discontinuity in a yield is delimited by twoblocks, and since the number of blocks of a node uequals the number of components in the order anno-tation of u, the following result is obvious:Proposition 2.
A dependency structure has gap-de-gree k if and only if the maximal number of compo-nents among the annotations !.u/ is k C 1.
In particular, a dependency structure is projective iffall of its annotations consist of just one component.Well-nestedness The well-nestedness conditionconstrains the arrangement of subtrees in a depen-dency structure.
Two subtrees t=u1; t=u2 interleave,if there are nodes v1l ; v1r 2 t=u1 and v2l ; v2r 2 t=u2such that v1l  v2l  v1r  v2r .
A dependency struc-ture is well-nested, if no two of its disjoint subtreesinterleave.
We can prove the following result:Proposition 3.
A dependency structure is well-nested if and only if no annotation !.u/ containsa substring i    j    i    j , for i; j 2 N. Example.
The dependency structure in Figure 1 iswell-nested, the structure depicted in Figure 2 is not:the subtrees rooted at the nodes b and e interleave.To see this, notice that b  e  d  f .
Also noticethat !.a/ contains the substring 1212.
4 Regular dependency languagesThe encoding of dependency structures as order-an-notated trees gives rise to an encoding of dependencylanguages as tree languages.
More specifically, de-pendency languages over a set ?
can be encodedas tree languages over the set ?
 ?, where ?
isthe set of all order annotations.
Via this encoding,we can study dependency languages using the toolsand results of the well-developed formal theory oftree languages.
In this section, we discuss depen-dency languages that can be encoded as regular treelanguages.4.1 Regular tree grammarsThe class of regular tree languages, REGT for short,is a very natural class with many characterizations(G?cseg and Steinby, 1997): it is generated by regulartree grammars, recognized by finite tree automata,and expressible in monadic second-order logic.
Herewe use the characterization in terms of grammars.Regular tree grammars are natural candidates for theformalization of dependency lexicons, as each rulein such a grammar can be seen as the specification ofa word and the syntactic categories or grammaticalfunctions of its immediate dependents.164Formally, a (normalized) regular tree grammar isa construct G D .NG ; ?G ; SG ; PG/, in which NGand ?G are finite sets of non-terminal and termi-nal symbols, respectively, SG 2 NG is a dedicatedstart symbol, and PG is a finite set of productionsof the form A !
.A1   An/, where  2 ?G ,A 2 NG , and Ai 2 NG , for every i 2 ?n?.
The (di-rect) derivation relation associated to G is the binaryrelation)G on the set T?G[NG defined as follows:t 2 T?G[NG t=u D A .A!
s/ 2 PGt )G t ?u 7!
s?Informally, each step in a derivation replaces a non-terminal-labelled leaf by the right-hand side of amatching production.
The tree language generatedby G is the set of all terminal trees that can eventu-ally be derived from the trivial tree formed by its startsymbol: L.G/ D f t 2 T?G j SG )G t g.4.2 Regular dependency grammarsWe call a dependency language regular, if its encod-ing as a set of trees over ?
?
forms a regular treelanguage, and write REGD for the class of all regulardependency languages.
For every regular dependencylanguage L, there is a regular tree grammar with ter-minal alphabet ?
?
that generates the encodingof L. Similar to the situation with individual struc-tures, the converse of this statement does not hold:the consistency properties mentioned above imposecorresponding syntactic restrictions on the rules ofgrammars G that generate the encoding of L.Property C10: The !-component of every pro-ductionA!
h; !i.A1   An/ inG contains all andonly symbols in the set f0g [ f i j i 2 ?n?
g.Property C20: For every non-terminal X 2 NG ,there is a uniquely determined integer dX such thatfor every production A !
h; !i.A1   An/ in G,dAi gives the number of occurrences of i in !, dAgives the number of components in !, and dSG D 1.It turns out that these properties are in fact sufficientto characterize the class of regular tree grammars thatgenerate encodings of dependency languages.
In butslight abuse of terminology, we will refer to suchgrammars as regular dependency grammars.Example.
Figure 3 shows a regular tree grammarthat generates a set of non-projective dependencystructures with string language f anbn j n  1 g. a b b baaBBBSAAS !
ha; h01ii.B/ j ha; h0121ii.A;B/A !
ha; h0; 1ii.B/ j ha; h01; 21ii.A;B/B !
hb; h0iiFigure 3: A grammar for a language in REGD.1/5 Structural constraints and formal powerIn this section, we present our results on the genera-tive capacity of regular dependency languages, link-ing them to a large class of mildly context-sensitivegrammar formalisms.5.1 Gap-restricted dependency languagesA dependency language L is called gap-restricted, ifthere is a constant cL  0 such that no structure in Lhas a gap-degree higher than cL.
It is plain to see thatevery regular dependency language is gap-restricted:the gap-degree of a structure is directly reflected inthe number of components of its order annotations,and every regular dependency grammar makes use ofonly a finite number of these annotations.
We writeREGD.k/ to refer to the class of regular dependencylanguages with a gap-degree bounded by k.Linear Context-Free Rewriting Systems Gap-re-stricted dependency languages are closely relatedto Linear Context-Free Rewriting Systems (lcfrs)(Vijay-Shanker et al, 1987), a class of formal sys-tems that generalizes several mildly context-sensitivegrammar formalisms.
An lcfrs consists of a regulartree grammar G and an interpretation of the terminalsymbols of this grammar as linear, non-erasing func-tions into tuples of strings.
By these functions, eachtree in L.G/ can be evaluated to a string.Example.
Here is an example for a function:f .hx11 ; x21i; hx12i/ D hax11 ; x12x21iThis function states that in order to compute the pairof strings that corresponds to a tree whose root nodeis labelled with the symbol f , one first has to com-pute the pair of strings corresponding to the first child165of the root node (hx11 ; x21i) and the single string cor-responding to the second child (hx12i), and then con-catenate the individual components in the specifiedorder, preceded by the terminal symbol a.
We call a function lexicalized, if it contributes ex-actly one terminal symbol.
In an lcfrs in which allfunctions are lexicalized, there is a one-to-one cor-respondence between the nodes in an evaluated treeand the positions in the string that the tree evaluatesto.
Therefore, tree and string implicitly form a depen-dency structure, and we can speak of the dependencylanguage generated by a lexicalized lcfrs.Equivalence We can prove that every regular de-pendency grammar can be transformed into a lexi-calized lcfrs that generates the same dependencylanguage, and vice versa.
The basic insight in thisproof is that every order annotation in a regular de-pendency grammar can be interpreted as a compactdescription of a function in the corresponding lcfrs.The number of components in the order-annotation,and hence, the gap-degree of the resulting depen-dency language, corresponds to the fan-out of thefunction: the highest number of components amongthe arguments of the function (Satta, 1992).1 A tech-nical difficulty is caused by the fact that lcfrs canswap components: f .hx11 ; x21i/ D hax21 ; x11i.
Thiscommutativity needs to be compiled out during thetranslation into a regular dependency grammar.We write LLCFRL.k/ for the class of all depen-dency languages generated by lexicalized lcfrs witha fan-out of at most k.Proposition 4.
REGD.k/ D LLCFRL.k C 1/ In particular, the class REGD.0/ of regular depen-dency languages over projective structures is exactlythe class of dependency languages generated by lexi-calized context-free grammars.Example.
The gap-degree of the language generatedby the grammar in Figure 3 is bounded by 1.
Therules for the non-terminal A can be translated intothe following functions of an equivalent lcfrs:fha;h0;1ii.hx11i/ D ha; x11ifha;h01;21ii.hx11 ; x21i; hx12i/ D hax11 ; x12x21iThe fan-out of these functions is 2.
1More precisely, gap-degree D fan-out   1.5.2 Well-nested dependency languagesThe absence of the substring i    j    i    j in theorder annotations of well-nested dependency struc-tures corresponds to a restriction to ?well-bracketed?compositions of sub-structures.
This restriction iscentral to the formalism of Coupled-Context-FreeGrammar (ccfg) (Hotz and Pitsch, 1996).It is straightforward to see that every ccfg canbe translated into an equivalent lcfrs.
We can alsoprove that every lcfrs obtained from a regular depen-dency grammar with well-nested order annotationscan be translated back into an equivalent ccfg.
Wewrite REGDwn.k/ for the well-nested subclass ofREGD.k/, and LCCFL.k/ for the class of all depen-dency languages generated by lexicalized ccfgs witha fan-out of at most k.Proposition 5.
REGDwn.k/ D LCCFL.k C 1/ As a special case, Coupled-Context-Free Grammarswith fan-out 2 are equivalent to Tree Adjoining Gram-mars (tags) (Hotz and Pitsch, 1996).
This enablesus to generalize a previous result on the class of de-pendency structures generated by lexicalized tags(Bodirsky et al, 2005) to the class of generated de-pendency languages, LTAL.Proposition 6.
REGDwn.1/ D LTAL 6 ConclusionIn this paper, we have presented a lexicalized refor-mulation of two structural constraints on non-pro-jective dependency representations, and shown thatcombining dependency lexicons that satisfy theseconstraints with a regular means of syntactic com-position yields classes of mildly context-sensitivedependency languages.
Our results make a signif-icant contribution to a better understanding of therelation between the phenomenon of non-projectivityand notions of formal power.The close link between restricted forms of non-projective dependency languages and mildly context-sensitive grammar formalisms provides a promisingstarting point for future work.
On the practical side,it should allow us to benefit from the experiencein building parsers for mildly context-sensitive for-malisms when addressing the task of efficient non-projective dependency parsing, at least in the frame-166work of grammar-driven parsing.
This may even-tually lead to a better trade-off between structuralflexibility and computational efficiency than that ob-tained with current systems.
On a more theoreticallevel, our results provide a basis for comparing a va-riety of formally rather distinct grammar formalismswith respect to the sets of dependency structures thatthey can generate.
Such a comparison may be empir-ically more adequate than one based on traditionalnotions of generative capacity (Kallmeyer, 2006).Acknowledgements We thank Guido Tack, StefanThater, and the anonymous reviewers of this paperfor their detailed comments.
The work of the authorsis funded by the German Research Foundation.ReferencesManuel Bodirsky, Marco Kuhlmann, and Mathias M?hl.2005.
Well-nested drawings as models of syntacticstructure.
In Tenth Conference on Formal Grammarand Ninth Meeting on Mathematics of Language, Edin-burgh, Scotland, UK.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In 42nd AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 423?429, Barcelona, Spain.Jason Eisner and Giorgio Satta.
1999.
Efficient parsingfor bilexical context-free grammars and head automa-ton grammars.
In 37th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), pages457?464, College Park, Maryland, USA.Ferenc G?cseg and Magnus Steinby.
1997.
Tree lan-guages.
In Grzegorz Rozenberg and Arto Salomaa,editors, Handbook of Formal Languages, volume 3,pages 1?68.
Springer-Verlag, New York, USA.G?nter Hotz and Gisela Pitsch.
1996.
On parsing coupled-context-free languages.
Theoretical Computer Science,161:205?233.Aravind K. Joshi.
1985.
Tree adjoining grammars: Howmuch context-sensitivity is required to provide reason-able structural descriptions?
In David R. Dowty, LauriKarttunen, and Arnold M. Zwicky, editors, Natural Lan-guage Parsing, pages 206?250.
Cambridge UniversityPress, Cambridge, UK.Laura Kallmeyer.
2006.
Comparing lexicalized grammarformalisms in an empirically adequate way: The notionof generative attachment capacity.
In InternationalConference on Linguistic Evidence, pages 154?156,T?bingen, Germany.Marco Kuhlmann and Joakim Nivre.
2006.
Mildly non-projective dependency structures.
In 21st InternationalConference on Computational Linguistics and 44th An-nual Meeting of the Association for Computational Lin-guistics (COLING-ACL) Main Conference Poster Ses-sions, pages 507?514, Sydney, Australia.Ryan McDonald and Fernando Pereira.
2006.
On-line learning of approximate dependency parsing al-gorithms.
In Eleventh Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL), pages 81?88, Trento, Italy.Ryan McDonald, Fernando Pereira, Kiril Ribarov, and JanHajic?.
2005.
Non-projective dependency parsing usingspanning tree algorithms.
In Human Language Technol-ogy Conference (HLT) and Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 523?530, Vancouver, British Columbia, Canada.Peter Neuhaus and Norbert Br?ker.
1997.
The complexityof recognition of linguistically adequate dependencygrammars.
In 35th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 337?343,Madrid, Spain.Joakim Nivre.
2006.
Constraints on non-projective depen-dency parsing.
In Eleventh Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL), pages 73?80, Trento, Italy.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.Dependency treelet translation: Syntactically informedphrasal smt.
In 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 271?279,Ann Arbor, USA.Owen Rambow and Aravind K. Joshi.
1997.
A for-mal look at dependency grammars and phrase-structuregrammars.
In Leo Wanner, editor, Recent Trends inMeaning-Text Theory, volume 39 of Studies in Lan-guage, Companion Series, pages 167?190.
John Ben-jamins, Amsterdam, The Netherlands.Giorgio Satta.
1992.
Recognition of linear context-freerewriting systems.
In 30th Annual Meeting of the As-sociation for Computational Linguistics (ACL), pages89?95, Newark, Delaware, USA.Katerina Vesel?, Jir?i Havelka, and Eva Hajic?ova.
2004.Condition of projectivity in the underlying depen-dency structures.
In 20th International Conference onComputational Linguistics (COLING), pages 289?295,Geneva, Switzerland.K.
Vijay-Shanker, David J. Weir, and Aravind K. Joshi.1987.
Characterizing structural descriptions producedby various grammatical formalisms.
In 25th AnnualMeeting of the Association for Computational Linguis-tics (ACL), pages 104?111, Stanford, California, USA.167
