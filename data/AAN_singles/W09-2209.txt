66Proceedings of the NAACL HLT Workshop on Semi-supervised Learning for Natural Language Processing, pages 66?74,Boulder, Colorado, June 2009. ?
2009 Association for Computational LinguisticsCan One Language Bootstrap the Other:A Case Study on Event ExtractionZheng Chen Heng JiThe Graduate Center Queens College and The Graduate CenterThe City University of New York The City University of New Yorkzchen1@gc.cuny.edu hengji@cs.qc.cuny.eduAbstractThis paper proposes a new bootstrappingframework using cross-lingual information pro-jection.
We demonstrate that this framework isparticularly effective for a challenging NLP taskwhich is situated at the end of a pipeline andthus suffers from the errors propagated from up-stream processing and has low-performancebaseline.
Using Chinese event extraction as acase study and bitexts as a new source of infor-mation, we present three bootstrapping tech-niques.
We first conclude that the standardmono-lingual bootstrapping approach is not soeffective.
Then we exploit a second approachthat potentially benefits from the extra informa-tion captured by an English event extraction sys-tem and projected into Chinese.
Such a cross-lingual scheme produces significant performancegain.
Finally we show that the combination ofmono-lingual and cross-lingual information inbootstrapping can further enhance the perfor-mance.
Ultimately this new framework obtained10.1% relative improvement in trigger labeling(F-measure) and 9.5% relative improvement inargument-labeling.1 IntroductionBootstrapping methods can reduce the effortsneeded to develop a training set and have shownpromise in improving the performance of manytasks such as name tagging (Miller et al, 2004; Jiand Grishman, 2006), semantic class extraction(Lin et al, 2003), chunking (Ando and Zhang,2005), coreference resolution (Bean and Riloff,2004) and text classification (Blum and Mitchell,1998).
Most of these bootstrapping methods impli-citly assume that:?
There exists a high-accuracy ?seed set?
or ?seedmodel?
as the baseline;?
There exists unlabeled data which is reliableand relevant to the test set in some aspects, e.g.from similar time frames and news sources;and therefore the unlabeled data supports theacquisition of new information, to provide newevidence to be incorporated to bootstrap themodel and reduce the sparse data problem.?
The seeds and unlabeled data won?t make theold estimates worse by adding too many incor-rect instances.However, for some more comprehensive andchallenging tasks such as event extraction, the per-formance of the seed model suffers from the li-mited annotated training data and also from theerrors propagated from upstream processing suchas part-of-speech tagging and parsing.
In addition,simply relying upon large unlabeled corpora can-not compensate for these limitations because moreerrors can be propagated from upstream processingsuch as entity extraction and temporal expressionidentification.Inspired from the idea of co-training (Blum andMitchell, 1998), in this paper we intend to boot-strap an event extraction system in one language(Chinese) by exploring new evidences from theevent extraction system in another language (Eng-lish) via cross-lingual projection.
We conjecturethat the cross-lingual bootstrapping for event ex-traction can naturally fit the co-training model:  asame event is represented in two ?views?
(de-scribed in two languages).
Furthermore, the cross-lingual bootstrapping can benefit from the differentsources of training data.
For example, the Chinesetraining corpus includes articles from Chinese newagencies in 2000 while most of English trainingdata are from the US news agencies in 2003, thus67English and Chinese event extraction systems havethe nature of generating different results on paralleldocuments and may complement each other.
In thispaper, we explore approaches of exploiting theincreasingly available bilingual parallel texts (bi-texts).We first investigate whether we can improve aChinese event extraction system by simply usingthe Chinese side of bitexts in a regular monolin-gual bootstrapping framework.
By gradually in-creasing the size of the corpus with unlabeled data,we did not get much improvement for trigger labe-ling and even observed performance deteriorationfor argument labeling.
But then by aligning thetexts at the word level, we found that the Englishevent extraction results can be projected into Chi-nese for bootstrapping and lead to significant im-provement.
We also obtained clear furtherimprovement by combining mono-lingual andcross-lingual bootstrapping.The main contributions of this paper are two-fold.
We formulate a new algorithm of cross-lingual bootstrapping, and demonstrate its effec-tiveness in a challenging task of event extraction;and we conclude that, for some applications be-sides machine translation, effective use of bitextscan be beneficial.The remainder of the paper is organized as fol-lows.
Section 2 formalizes the event extraction taskaddressed in this paper.
Section 3 discusses eventextraction bootstrapping techniques.
Section 4 re-ports our experimental results.
Section 5 presentsrelated work.
Section 6 concludes this paper andpoints out future directions.2 Event Extraction2.1 Task Definition and TerminologyThe event extraction that we address in this paperis specified in the Automatic Content Extraction(ACE) 1?
event trigger: the word that most clearly ex-presses an event?s occurrenceprogram.
The ACE 2005 Evaluation de-fines the following terminology for the event ex-traction task:?
event argument:  an entity, a temporal expres-sion or a value that plays a certain role in theevent instance1 http://www.nist.gov/speech/tests/ace/?
event mention: a phrase or sentence with adistinguished trigger and participant argumentsThe event extraction task in our paper is todetect certain types of event mentions that are indi-cated by event triggers (trigger labeling), recog-nize the event participants e.g., who, when, where,how (argument labeling) and merge the co-referenced event mentions into a unified event(post-processing).
In this paper, we focus on dis-cussing trigger labeling and argument labeling.In the following example,Mike got married in 2008.The event extraction system should identify?married?
as the event trigger which indicates theevent type of ?Life?
and subtype of ?Marry?.
Fur-thermore, it should detect ?Mike?
and ?2008?
asarguments in which ?Mike?
has a role of ?Person?and ?2008?
has a role of ?Time-Within?.2.2 A Pipeline of Event ExtractionOur pipeline framework of event extraction in-cludes trigger labeling, argument labeling andpost-processing, similar to (Grishman et al, 2005),(Ahn, 2006) and (Chen and Ji, 2009).
We depictthe framework as Figure 1.Figure 1.
Pipeline of Event ExtractionThe event extraction system takes raw docu-ments as input and conducts some pre-processingsteps.
The texts are automatically annotated withArgument labelingTrigger labelingTrigger classificationTrigger identificationArgument identificationPre-processingArgument classificationPost-processing68word segmentation, Part-of-Speech tags, parsingstructures, entities, time expressions, and relations.The annotated documents are then sent to the fol-lowing four components.
Each component is aclassifier and produces confidence values;?
Trigger identification: the classifier recognizesa word or a phrase as the event trigger.?
Trigger classification: the classifier assigns anevent type to an identified trigger.?
Argument identification: the classifier recog-nizes whether an entity, temporal expression orvalue is an argument associated with a particu-lar trigger in the same sentence.?
Argument classification: the classifier assignsa role to the argument.The post-processing merges co-referenced eventmentions into a unified representation of event.2.3 Two Monolingual Event Extraction Sys-temsWe use two monolingual event extraction systems,one for English, and the other for Chinese.
Bothsystems employ the above framework and useMaximum Entropy based classifiers.
The corres-ponding classifiers in both systems also share somelanguage-independent features, for example, intrigger identification, both classifiers use the ?pre-vious word?
and ?next word?
as features, however,there are some language-dependent features thatonly work well for one monolingual system, forexample, in argument identification, the next wordof the candidate argument is a good feature forChinese system but not for English system.
To il-lustrate this, in the Chinese ???
(of) structure, theword ???
(of) strongly suggests that the entity onthe left side of ???
is not an argument.
For a spe-cific example, in ????????
(The mayor ofNew York City), ?????
(New York City) on theleft side of ???
(of) cannot be considered as anargument because it is a modifier of the noun ????(mayor).
Unlike Chinese, ?of?
(???)
appearsahead of the entity in the English phrase.Table 1 lists the overall Precision (P), Recall (R)and F-Measure (F) scores for trigger labeling andargument labeling in our two monolingual eventextraction systems.
For comparison, we also listthe performance of an English human annotatorand a Chinese human annotator.Table 1 shows that event extraction is a difficultNLP task because even human annotators cannotachieve satisfying performance.
Both monolingualsystems relied on expensive human labeled data(much more expensive than other NLP tasks due tothe extra tagging tasks of entities and temporal ex-pressions), thus a natural question arises: can themonolingual system benefit from bootstrappingtechniques with a relative small set of training data?The other question is: can a monolingual systembenefit from the other monolingual system bycross-lingual bootstrapping?PerformanceSystem/HumanTriggerLabelingArgumentLabelingP R F P R FEnglishSystem64.3 59.4 61.8 49.2 34.7 40.7ChineseSystem78.8 48.3 59.9 60.6 34.3 43.8EnglishAnnotator59.2 59.4 59.3 51.6 59.5 55.3ChineseAnnotator75.2 74.6 74.9 58.6 60.9 59.7Table 1.Performance of Two Monolingual EventExtraction Systems and Human Annotators3 Bootstrapping Event Extraction3.1 General Bootstrapping AlgorithmBootstrapping algorithms have attracted much at-tention from researchers because a large number ofunlabeled examples are available and can be uti-lized to boost the performance of a system trainedon a small set of labeled examples.
The generalbootstrapping algorithm is depicted in Figure 2,similar to (Mihalcea, 2004).Self-training and Co-training are two mostcommonly used bootstrapping methods.A typical self-training process is described asfollows: it starts with a set of training examplesand builds a classifier with the full integrated fea-ture set.
The classifier is then used to label an addi-tional portion of the unlabeled examples.
Amongthe resulting labeled examples, put the most confi-dent ones into the training set, and re-train the clas-sifier.
This iterates until a certain condition issatisfied (e.g., all the unlabeled examples havebeen labeled, or it reaches a certain number of ite-rations).Co-training(Blum and Mitchell, 1998) differsfrom self-training in that it assumes that the datacan be represented using two or more separate69?views?
(thus the whole feature set is split into dis-joint feature subsets) and each classifier can betrained on one view of the data.
For each iteration,both classifiers label an additional portion of theunlabeled examples and put the most confidentones to the training set.
Then the two classifiers areretrained on the new training set and iterate until acertain condition is satisfied.Both self-training and co-training can fit in thegeneral bootstrapping process.
If the number ofclassifiers is set to one, it is a self-training process,and it is a co-training process if there are two dif-ferent classifiers that interact in the bootstrappingprocess.Figure 2.
General Bootstrapping Algorithm.In the following sections, we adapt the boot-strapping techniques discussed in this section to alarger scale (system level).
In other words, we aimto bootstrap the overall performance of the systemwhich may include multiple classifiers, rather thanjust improve the performance of a single classifierin the system.
It is worth noting that for the pipe-line event extraction depicted in Section 2.2, thereare two major steps that determine the overall sys-tem performance: trigger labeling and argumentlabeling.
Furthermore, the performance of triggerlabeling can directly affect the performance of ar-gument labeling because the involving argumentsare constructed according to the trigger.
If a triggeris wrongly recognized, all the involving argumentswill be considered as wrong arguments.
If a triggeris missing, all the attached arguments will be con-sidered as missing arguments.3.2 Monolingual Self-trainingIt is rather smooth to adapt the idea of traditionalself-training to monolingual self-training if weconsider our monolingual event extraction systemas a black box or even a single classifier that de-termines whether an event combining the result oftrigger labeling and argument labeling is a reporta-ble event.Thus the monolingual self-training procedure forevent extraction is quite similar with the one de-scribed in Section 3.1.
The monolingual event ex-traction system is first trained on a starting set oflabeled documents, and then tag on an additionalportion of unlabeled documents.
Note that in eachlabeled document, multiple events could be taggedand confidence score is assigned to each event.Then the labeled documents are added into thetraining set and the system is retrained based onthe events with high confidence.
This iterates untilall the unlabeled documents have been tagged.3.3 Cross-lingual Co-TrainingWe extend the idea of co-training to cross-lingualco-training.
The intuition behind cross-lingual co-training is that the same event has different ?views?described in different languages, because the lexi-cal unit, the grammar and sentence constructiondiffer from one language to the other.
Thus onemonolingual event extraction system probably uti-lizes the language dependent features that cannotwork well for the other monolingual event extrac-tion systems.
Blum and Mitchell (1998) derivedPAC-like guarantees on learning under two as-sumptions: 1) the two views are individually suffi-cient for classification and 2) the two views areconditionally independent given the class.
Ob-viously, the first assumption can be satisfied incross-lingual co-training for event extraction, sinceeach monolingual event extraction system is suffi-cient for event extraction task.
However, we re-serve our opinion on the second assumption.Although the two monolingual event extractionsystems may apply the same language-independentfeatures such as the part-of-speech, the next wordand the previous word, the features are exhibited intheir own context of language, thus it is too subjec-tive to conclude that the two feature sets are or areInput:L : a set of labeled examples,U : a set of unlabeled examples{ iC }: a set of classifiersInitialization:Create a pool U ?
of examples by choosing Prandom examples from ULoop until a condition is satisfied (e.g., U ?
?
, oriteration counter reaches a preset number I )?
Train each classifier iC  on L , and labelthe examples in U ??
For each classifier iC ,select the most con-fidently labeled examples (e.g., the confi-dence score is above a preset threshold ?or the top  K ) and add them to L?
Refill U ?with examples from U , and keepthe size of U ?
as constant P70not conditionally independent.
It is left to be anunsolved issue which needs further strict analysisand supporting experiments.The cross-lingual co-training differs from tradi-tional co-training in that the two systems in cross-lingual co-training are not initially trained from thesame labeled data.
Furthermore, in the bootstrap-ping phase, each system only labels half portion ofthe bitexts in its own language.
In order to utilizethe labeling result by the other system, we need toconduct an extra step named cross-lingual projec-tion that transforms tagged events from one lan-guage to the other.3.3.1 A Cross-lingual Co-training AlgorithmThe algorithm for cross-lingual co-training is de-picted in Figure 3.Figure 3.
Cross-lingual Co-training Algorithm3.3.2 Cross-lingual Semi-co-trainingCross-lingual semi-co-training is a variation ofcross-lingual co-training, and it differs from cross-lingual co-training in that it tries to bootstrap onlyone system by the other fine-trained system.
Thistechnique is helpful when we have relatively largeamount of training data in one language while wehave scarce data in the other language.Thus we only need to make a small modificationin the cross-lingual co-training algorithm so that itcan soon be adapted to cross-lingual semi-co-training, i.e., we retrain one system and do not re-train the other.
In this paper, we will conduct expe-riments to investigate whether a fine-trainedEnglish event extraction system can bootstrap theChinese event extraction system, starting from asmall set of training data.3.3.3 Cross-lingual ProjectionCross-lingual projection is a key operation in thecross-lingual co-training algorithm.
In the case ofevent extraction, we need to project the triggersand the participant arguments from one languageinto the other language according to the alignmentinformation provided by bitexts.
Figure 4 shows anexample of projecting an English event into thecorresponding Chinese event.Before projection:<event ID="chtb_282-EV1" TYPE="Contact" SUBTYPE="Meet"><event_mention ID="chtb_282-EV1-1" p="1.000"><trigger><charseq START="2259" END="2265">meeting</charseq></trigger><event_mention_argument ROLE="Entity" p="0.704"pRole="0.924"/><extent><charseq START="2238" END="2244">Gan Luo</charseq></extent></event_mention_argument></event_mention></event>After projection:<event ID="chtb_282-EV1" TYPE="Contact" SUBTYPE="Meet"><event_mention ID="chtb_282-EV1-1" p="1.000"><trigger><charseq START="454" END="455">?
?</charseq></trigger><event_mention_argument ROLE="Entity" p="0.704"pRole="0.924"/><extent><charseq START="449" END="450">?
?</charseq></extent></event_mention_argument></event_mention></event>Figure 4.
An Example of Cross-lingual ProjectionInput:1L : a set of labeled examples in language A2L : a set of labeled examples in language BU : a set of unlabeled bilingual examples  (bi-texts) with alignment information{ 1 2,S S }: two monolingual systems, one forlanguage A and the other for language B.Initialization:Create a pool U ?
of examples by choosing Prandom examples from ULoop until a condition is satisfied (e.g., U ?
?
,or iteration counter reaches a preset number I )?
Train 1S on 1L and 2S on 2L?
Use 1S to label the examples in U ?
(the por-tion in Language A) and use 2S to label theexamples in U ?
(the portion in Language B)?
For 1S , select the most confidently labeledexamples (e.g., the confidence score isabove a preset threshold ?
or the top K ) ,apply the operation of cross-lingual projec-tion, transform the selected examples fromLanguage A to Language B, and put theminto 2L .
The same procedure applies to 2S .?
Refill U ?with examples from U , and keepthe size of U ?
as constant P714 Experiments and Results4.1 Data and Scoring MetricWe used the ACE 2005 corpus to set up two mono-lingual event extraction systems, one for English,the other for Chinese.The ACE 2005 corpus contains 560 Englishdocuments from 6 sources: newswire, broadcastnews, broadcast conversations, weblogs, new-sgroups and conversational telephone speech;meanwhile the corpus contains 633 Chinese docu-ments from 3 sources: newswire, broadcast newsand weblogs.We then use 159 texts from the LDC ChineseTreebank English Parallel corpus with manualalignment for our cross-lingual bootstrapping ex-periments.We define the following standards to determinethe correctness of an event mention:?
A trigger is correctly labeled if its event typeand offsets match a reference trigger.?
An argument is correctly labeled if its eventtype, offsets, and role match any of the refer-ence argument mentions.4.2 Monolingual Self-training on ACE 2005DataWe first investigate whether our Chinese eventextraction system can benefit from monolingualself-training on ACE data.
We reserve 66 Chinesedocuments for testing purpose and set the size ofseed training set to 100.
For a single trial of theexperiment, we randomly select 100 documents astraining set and use the remaining documents asself-training data.
For each iteration of the self-training, we keep the pool size as 50, in otherwords, we always pick another 50 ACE documentsto self-train the system.
The iteration continuesuntil all the unlabeled ACE documents have beentagged and thus it completes one trial of the expe-riment.
We conduct the same experiment for 100trials and compute the average scores.The most important motivation for us to conductself-training experiments on ACE data is that theACE data provide ground-truth entities and tem-poral expressions so that we do not have to takeinto account the effects of propagated errors fromupstream processing such as entity extraction andtemporal expression identification.For one setting of the experiments, we set theconfidence threshold to 0, in other words, we keepall the labeling results for retraining.
The resultsare given in Figure 5 (trigger labeling) and Figure6 (argument labeling).
It shows that when thenumber of self-trained ACE documents reaches450, we obtain a gain of 3.4% (F-Measure) abovethe baseline for trigger labeling and a gain of 1.4%for argument labeling.For the other setting of the experiments, we setthe confidence threshold to 0.8, and the results arepresented in Figure 7 and Figure 8.
Surprisingly,retraining on the high confidence examples doesnot lead to much improvement.
We obtain a gainof 3.7% above the baseline for trigger labeling and1.5% for argument labeling when the number ofself-trained documents reaches 450.Figure 5.
Self-training for trigger labeling(confidence threshold = 0)Figure 6.
Self-training for argument labeling(confidence threshold= 0)72Figure 7.
Self-training for trigger labeling(confidence threshold = 0.8)Figure 8.
Self-training for argument labeling(confidence threshold = 0.8)4.3 Cross-lingual Semi-co-training on BitextsThe experiments in Section 4.2 show that we canobtain gain in performance by monolingual self-training on data with ground-truth entities andtemporal expressions, but what if we do not havesuch ground-truth data, then how the errors propa-gated from entity extraction and temporal expres-sion identification will affect the overallperformance of our event extraction system?
Andif these errors are compounded in event extraction,can the cross-lingual semi-co-training alleviate theimpact?To investigate all these issues, we use 159 textsfrom LDC Chinese Treebank English Parallel cor-pus to conduct cross-lingual semi-co-training.
Theexperimental results are summarized in Figure 9and Figure 10.For monolingual self-training on the bitexts, weconduct experiments exactly as section 4.2 exceptthat the entities are tagged by the IE system and thelabeling pool size is set to 20.
When the number ofbitexts reaches 159, we obtain a little gain of 0.4%above the baseline for trigger labeling and a loss of0.1% below the baseline for argument labeling.The deterioration tendency of the self-trainingcurve in Figure 10 indicates that entity extractionerrors do have counteractive impacts on argumentlabeling.We then conduct the cross-lingual semi-co-training experiments as follows: we set up an Eng-lish event extraction system trained on a relativelarge training set (500 documents).
For each trialof the experiment, we randomly select 100 ACEChinese document as seed training set, and then itenters a cross-lingual semi-co-training process: foreach iteration, the English system labels the Eng-lish portions of the 20 bitexts and by cross-lingualprojection, the labeled results are transformed intoChinese and put into the training set of Chinesesystem.
From Figure 9 and Figure 10 we can seethat when the number of bitexts reaches 159, weobtain a gain of 1.7% for trigger labeling and 0.7%for argument labeling.We then apply a third approach to bootstrap ourChinese system: during each iteration, the Chinesesystem also labels the Chinese portions of the 20bitexts.
Then we combine the results from bothmonolingual systems using the following rules:?
If the event labeled by English system is notlabeled by Chinese system, add the event toChinese system?
If the event labeled by Chinese system is notlabeled by English system, keep the event inthe Chinese system?
If both systems label the same event but withdifferent event types and arguments, select theone with higher confidenceFrom Figure 9 and Figure 10 we can see that thisapproach leads to even further improvement in per-formance, shown as the ?Combined-labeled?curves.
When the number of bitexts reaches 159,we obtain a gain of 3.1% for trigger labeling and2.1% for argument labeling.In order to check how robust our approachis, we conducted the Wilcoxon Matched-PairsSigned-Ranks Test on F-measures for all these 100trials.
The results show that we can rejectthe hypotheses that the improvements using Cross-lingual Semi-co-training were random at a 99.99%confidence level, for both trigger labeling and ar-gument labeling.73Figure 9.
Self-training, and Semi-co-training(English- labeled & Combined-labeled)for Trigger LabelingFigure 10.
Self-training, and Semi-co-training(English- labeled & Combined-labeled)for Argument Labeling5 Related WorkThere is a huge literature on utilizing parallel cor-pus for monolingual improvement.
To our know-ledge, it can retrace to (Dagan et.al 1991).
Weapologize to those whose work is not cited due tospace constraints.
The work described here com-plements some recent research using bitexts ortranslation techniques as feedback to improve enti-ty extraction.
Huang and Vogel (2002) presentedan effective integrated approach that can improvethe extracted named entity translation dictionaryand the entity annotation in a bilingual trainingcorpus.
Ji and Grishman (2007) expanded this ideaof alignment consistency to the task of entity ex-traction in a monolingual test corpus without refer-ence translations, and applied sophisticated infe-inference rules to enhance both entity extractionand translation.
Zitouni and Florian (2008) appliedEnglish mention detection on translated texts andadded the results as additional features to improvemention detection in other languages.In this paper we share the similar idea of import-ing evidences from English with richer resourcesto improve extraction in other languages.
However,to the best of our knowledge this is the first workof incorporating cross-lingual feedback to improvethe event extraction task.
More importantly, it isthe first attempt of combining cross-lingual projec-tion with bootstrapping methods, which can avoidthe efforts of designing sophisticated inferencerules or features.6 Conclusions and Future WorkEvent extraction remains a difficult task not onlybecause it is situated at the end of an IE pipelineand thus suffers from the errors propagated fromupstream processing, but also because the labeleddata are expensive and thus suffers from data scar-city.
In this paper, we proposed a new co-trainingframework using cross-lingual information projec-tion and demonstrate that the additional informa-tion from English system can be used to bootstrapa Chinese event extraction system.To move a step forward, we would like to con-duct experiments on cross-lingual co-training andinvestigate whether the two systems on both sidescan benefit from each other.
A main issue existingin cross-lingual co-training is that the cross-lingualprojection may not be perfect due to the wordalignment problem.
In this paper, we used a corpuswith manual alignment, but in the future we intendto investigate the effect of automatic alignmenterrors.We believe that the proposed cross-lingual boot-strapping framework can also be applied to manyother challenging NLP tasks such as relation ex-traction.
However, we still need to provide a theo-retical analysis of the framework.AcknowledgmentsThis material is based upon work supported by theDefense Advanced Research Projects Agency un-der Contract No.
HR0011-06-C-0023 via 27-001022, and the CUNY Research EnhancementProgram and GRTI Program.74ReferencesAvrim Blum and Tom Mitchell.
1998.
Combining La-beled and Unlabeled Data with Co-training.
Proc.
ofthe Workshop on Computational Learning Theory.Morgan Kaufmann Publishers.David Ahn.
2006.
The stages of event extraction.
Proc.COLING/ACL 2006 Workshop on Annotating andReasoning about Time and Events.
Sydney, Aus-tralia.David Bean and Ellen Riloff.
2004.
UnsupervisedLearning of Contextual Role Knowledge for Corefe-rence Resolution.
Proc.
HLT-NAACL2004.
pp.
297-304.
Boston, USA.Fei Huang and Stephan Vogel.
2002.
Improved NamedEntity Translation and Bilingual Named Entity Ex-traction.
Proc.
ICMI 2002.
Pittsburgh, PA, US.Heng Ji and Ralph Grishman.
2006.
Data Selection inSemi-supervised Learning for Name Tagging.
InACL 2006 Workshop on Information ExtractionBeyond the Document:48-55.
Sydney, Australia.Heng Ji and Ralph Grishman.
2007.
Collaborative Enti-ty Extraction and Translation.
Proc.
InternationalConference on Recent Advances in Natural Lan-guage Processing 2007.
Borovets, Bulgaria.Ido Dagan, Alon Itai and Ulrike Schwall.
1991.
Twolanguages are more informative than one.
Proc.
ACL1991.Imed Zitouni and Radu Florian.
2008.
Mention Detec-tion Crossing the Language Barrier.
Proc.
EMNLP.Honolulu, Hawaii.Michael Collins and Yoram Singer.
1999.
UnsupervisedModels for Named Entity Classification.
Proc.
ofEMNLP/VLC-99.Rada Mihalcea.
2004.
Co-training and self-training forword sense disambiguation.
In Proceedings of theConference on Computational Natural LanguageLearning (CoNLL-2004).Ralph Grishman, David Westbrook and Adam Meyers.2005.
NYU?s English ACE 2005 System Descrip-tion.
Proc.
ACE 2005 Evaluation Workshop.
Wash-ington, US.Rie Ando and Tong Zhang.
2005.
A High-PerformanceSemi-Supervised Learning Methods for Text Chunk-ing.
Proc.
ACL2005.
pp.
1-8.
Ann Arbor, USAScott Miller, Jethran Guinness and Alex Zamanian.2004.Name Tagging with Word Clusters and Discrimina-tive Training.
Proc.
HLT-NAACL2004.
pp.
337-342.Boston, USAWinston Lin, Roman Yangarber and Ralph Grishman.2003.
Bootstrapping Learning of Semantic Classesfrom Positive and Negative Examples.
Proc.
ICML-2003 Workshop on The Continuum from Labeled toUnlabeled Data.
Washington, D.C.Zheng Chen and Heng Ji.
2009.
Language Specific Is-sue and Feature Exploration in Chinese Event Extrac-tion.
Proc.
HLT-NAACL 2009.
Boulder, Co.
