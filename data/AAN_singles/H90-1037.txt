Fast Search Algorithmsfor Connected Phone RecognitionUsing the Stochastic Segment ModelV.Digalakist M.Ostendorf t J.R. Rohlicekt Boston University44 Cummington St.Boston, MA 02215:~ BBN Inc.10 Moulton St.Cambridge, MA 02138ABSTRACTIn this paper we present methods for reducing the compu-tation time of joint segmentation a d recognition of phonesusing the Stochastic Segment Model (SSM).
Our approachto the problem is twofold: first, we present a fast segmentclassification method that reduces computation by a factorof 2 to 4, depending on the confidence of choosing the mostprobable model.
Second, we propose a Split and Merge seg-mentation algorithm as an alternative to the typical DynamicProgramming solution of the segmentation a d recognitionproblem, with computation savings increasing proportionallywith model complexity.
Even though our current recognizeruses context-independent phone models, the results that wereport on the TIMIT database for speaker independent jointsegmentation a d recognition are comparable to that of sys-tems that use context information.IntroductionThe Stochastic Segment Model \[6, 10, 9\] was proposed asan alternative to hidden Markov models (HMMs), in orderto overcome the limiting assumptions of the latter that ob-servation features are conditionally independent given theunderlying state sequence.
The main disadvantage of theSSM over other methods is its computational complexity,which can be attributed to the fact that dropping the con-ditional independence assumption on the state sequence in-creases the size of the effective state space.
Having ad-dressed several issues on the classification accuracy of theSSM in our previous work \[2\], we now move to speaker in-dependent connected phone recognition and choose to con-centrate on the important issue of computational complexityof the SSM recognition algorithm.
We present wo new al-gorithms which together achieve a significant computationreduction.This paper is organized as follows.
A brief overview ofthe SSM is given in this section.
For a more detailed e-scription of the model the reader is referred to \[6, 10\].
Inthe next section, we present a new algorithm for fast phoneclassification with the SSM.
Next, we address the issue ofjoint segmentation a d recognition.
The originally proposedsolution for this problem used a Dynamic Programming al-gorithm to find the globally maximum likelihood phone se-quence.
After we examine the computational complexityof this method, we look at an alternative approach to thisproblem based on local search algorithms.
We propose adifferent search strategy that reduces the computation sig-nificantly, and can be used in conjunction with the classifi-cation algorithm.
Although theoretically the new algorithmmay not find the global maximum, experimental results onthe TIMIT database, show that the recognition performanceof the fast algorithm is very close to the optimum one.An observed segment of speech (e.g., a phoneme) is rep-resented by a sequence of q-dimensional feature vectorsY = \[Yx Y2 .
.
.
YL\], where the length L is variable.
Thestochastic segment model for Y has two components \[10\]:1) a time transformation TL to model the variable-lengthobserved segment in terms of a fixed-length unobserved se-quence Y = XTz, where X = \[Zl z2 ... zu \ ] ,  and 2)a probabilistic representation f the unobserved feature se-quence X.
The conditional density of the observed segmentY given phone a and duration L is:p(Y\[~, L) =/x p(XIa)dX" (1):Y=XTzWhen the observed length, L, is less than or equal to M, thelength of X, Tz is a time-warping transformation which ob-tains Y by selecting a subset of columns of X and the densityp(Yla) is a marginal distribution of p(Xla ).
In this work, asin previous work, the time transformation, TL, is chosen tomap each observed sample Yl to the nearest model samplezj according to a linear time-warping criterion.
The distri-bution p(Xlc, ) for the segment X given the phone a is thenmodelled using an Mq-dimensional multi-variate Gaussiandistribution.Fast Segment ClassificationLet Y = \[yl y2 .
.
.
Yz\] be the observed segment of speech.Then, the MAP detection rule for segment classification re-duces toa* = art n~ax p(Yla, L)p(L la )p (a )with the terms p(Lla) representing duration probabilities.The use of an Mq - dimensional Oaussian distributionfor the SSM makes the calculation of the log-likelihoodsl (a)  ~ log p(a) + l ogp(L la )  + logp(Yla , L) computationally173expensive.
In this section, we present an algorithm for fastsegment classification, based on obtaining an upper boundL(~) of l(~) and eliminating candidate phones based on thisbound.
The same idea for HMM's has also been used in\[1\], but in the case of the SSM the computation can be donerecursively.Specifically, definei i y/k = \[m y2 ... yk #k+l,k .
.
.
#z,k\]whereg,~ = E{y~ I,~,, w, ...,  y~}, J >for all phones oei.
Then, because of the Gaussian assump-tion,= lnp(~i) + lnp(Llai) + In p(Ylc~i, L)< lnp(al) + lnp(Llai) + lnp(Y~ lal, L) -~k = 1 ,2 , .
.
.
, LLk(al),Notice that the bounds get tighter as k increases, Lk+l(c0 <Lk(c0.
A fast classification scheme for the SSM is thendescribed by the following algorithm, where by C we de-note the set of remaining candidates and by I2 the set of allcandidates.Initialization Set C =/2, k = 1Step 1 For all ai  6 Ccompute Lk (ai)Step 2 Pick ~* = argmax~ Lk(ai)Step 3 Compute the actual ikelihood l(~*)Step 4 For a l la i6Cif Lk(ai) < l(a*) remove ai  from CStep 5 Increment k. If It\] > 1 and k < L go to step 1.Because of the Gaussian assumption, the computation ofthe bounds Lk0 in step 1 of iteration k actually correspondsto a single q-dimensional Gaussian score evaluation for ev-ery remaining class.
The savings in computation are directlydependent on the discriminant power of the feature vector,and can be further increased if the frame scores are com-puted in a permuted order, so that frames with the largestdiscriminant information (for example the middle portion ofa phone) are computed first.The proposed method has the desirable property of beingexact - at the end the best candidate according to the MAPrule is obtained.
On the other hand, the computation sav-ings can be increased ff instead of pruning candidates basedon the strict bound, we use the bound at each iteration toobtain a biased estimate of the true likelihood, \[k(cO, anduse it in place of Lk0 in step 4 of the algorithm.
In thechoice of the estimate there is a trade-off between compu-tation savings and the confidence level of choosing the truebest candidate.
In our phonetic classification experiments,using a segment model with length M = 5 and assumingindependent frames, we achieved a 50% computation reduc-tion with the exact method.
The savings increased to 75%without any effect to classification accuracy when pruningbased on the estimates of likelihoods.
We can also argue thatthe computation reduction should be bigger for a segmentlonger than 5 frames and also when the frame independenceassumption is dropped, because in that case the distributionswill be sharper.J o in t  Segmentat ion /Recogn i t ionBefore we present he problem of joint segmentation andrecognition (JSR), it is useful to introduce some notation.A segmentation S of a sentence that is N frames long andt will be represented by consists of n segments s rS ~ S(n)  = {8~t, 81"2~-, +1, ?
?
?, 8 ; : _ t+,} ,l<r l<r2<.
.
.<rn_ l  < r,~ = NThe sequence of the observed data can be partitioned inaccordance to the segmentation S(n) asY "= Y(S(n))== {Y(1, rl), Y(rx + 1, r2), .
.
.
,  Y(r,~_, + 1, rn)}withY(r, t) = \[y,-, y,-+l, .
.
.
,  yt\]Finally, an n-long sequence of phone labels will be repre-sented aso:(n)  = {~0,  CX l , .
.
.
,an -1}When the phone boundaries are unknown, the joint like-lihood of the observed data and an n-long phone sequenceistCc, Cn)) = ~ v(Y(S(n)), S(n)l,~(n))p(,~(n)) (2)S(n)and under the assumption of independent phones,p(Y(S(n)), S(n)l~(n))p(~(n)) =Ti+I 'ri+l 1I\]~o I p(Y(ri + 1, ri+lllal, s,.,.l)P(S,,+l \[cq)p(al)where the terms p(s~+Lla) =p(Lla ) represent duration prob-abilities.
For the automatic recognition problem, the MAPrule for detecting the most likely phonetic sequence wouldinvolve calculations of the form (2).
Because of the com-plexity of (2), in a similar fashion to HMM's where Viterbidecoding is used instead of the forward algorithm, we chooseto jointly select the segmentation and the phone sequencethat maximize the a posteriori likelihood(a* (n*), S* (n *), n *) =arg max(,~o~),so,),,~){p(Y(S(n)), S(n)la(n))p(a(n))}In addition to the phone independence assumption, if wefurther assume that there is no upper bound in the range ofallowable phone durations, there are 2 N-  x possible segmen-tations.
If the second assumption is dropped, and the rangeof phone durations is 1 < L < D, the number of configura-tions among which we optimize drops to 2 N-1 - 2 N-D + 1.174Dynamic Programming SearchThe joint segmentation a d recognition, being a shortest pathproblem, has a solution using the following Dynamic Pro-gramming recursion \[6\] under the assumption of independentphones:= 0J~ = max {J~* + ln\[p(Y(r + 1, t)ls)\] + ln\[p(st~+lls)\]"r<t~a+ ln\[p(s)\] + C}with C being a constant hat controls the insertion rate.
Atthe last frame N we obtain the solution:N = n n)), sCn))p(s(n))) +nfez ),nWe shall express the complexity of a joint segmentationand recognition search algorithm in terms of segment scoreevaluations:t cr = maax{In\[p(Y(r , ~)Is)\] + In\[p(str Is)\] + In\[p(s)\]} (3)Clearly, a segment score evaluation is dominated by thefirst term, which is a (t - ~- + 1)q - dimensional Gaussianevaluation.
We shall also use the number of q-dimensionalGanssian score evaluations as a measure of complexity.
Asegment score evaluation consists of Inl(t- r?
1) Gaussianscore evaluations, with D the set of all phones.The DP solution is efficient with a complexity of O(N 2)segment score evaluations, which drops to O(DN) if thesegment length is restricted to be less than D. However, interms of Gaussian score evaluations (1) this approach iscom-putationally expensive.
If we assume that feature vectors areindependent in time, then for each frame in the sentence thescores of all models and possible positions within a segmentwill effectively be computed and stored.
This translates toa complexity of O(M x N x Inl) q-dimensional Gaussianevaluations, or simply M Gaussian evaluations per frameper model, where M is the model length.
This complexityis further increased when the independence assumption isdropped, in which case the number of Gaussian scores thatmust be computed increases drastically and is equal to thesquare of allowable phone durations per frame per model.For large q (as in \[2\]), the DP solution is impractical.
In thefollowing sections we present a local search algorithm thatachieves a significant computation reduction.Local Search  A lgor i thmsWe now give the description of a general ocal search al-gorithm for the joint segmentation a d recognition problem.The set of all segmentations isN = {s is  = ,?1}}The neighborhood of a segmentation S is defined throughthe mapping from ~ to the power set of 3c:N :.7: --_~ 2 7A local search algorithm for the JSR problem is then de-fined as: given any segmentation S, the neighborhood N(S)is searched for a segmentation S' with I(S') > l(S).
If suchS' exists in N(S), then it replaces S and the search con-tinues, otherwise a local (perhaps) optimum with respect othe given neighborhood is found.
The choice of the neigh-borhood is critical, as discussed in \[7\].
In general it mustbe powerful enough to help avoid local optima and on theother hand small, so that it can be searched efficiently.An important question for any local search algorithm isthe size of the minimal exact local search neighborhood: thatis, given any starting configuration S, what is the smallestpossible neighborhood that must be searched so that we areguaranteed convergence to the global optimum after a finitenumber of steps.
For the JSR problem, a segmentation S' isin the minimal exact search neighborhood of S if it has nocommon boundaries in a single part of the sentence, exceptthe first and last frames of this part, and the scenario beforeand after those two frames is the same.
A more formaldefinition of the exact neighborhood and a proof of the abovestated result is out of the scope of this paper.The size of the minimal exact neighborhood of S can beshown to be exponential in N.  It also depends on the numberof segments, and is larger for segmentations with longersegments.
Of course an explicit search of this neighborhoodis infeasible and furthermore we already have a solutionwith a complexity of O(N 2) segment evaluations.
In thenext section we shall propose a strategy that searches overa subset of N,=,~a(S) with size proportional to the numberof segments only.Two common local search strategies are the splitting andmerging schemes.
To describe those strategies we shallmake use of the concept of a segmentation tree: a nodeof this tree is a single segment s~ and can have childrens~l, st+it2 with tl _< t < t2.
The children are not unique, be-cause of the freedom in the choice of the boundary t. Theroot of the tree is the segment s~ r (whole sentence).
A seg-mentation S is then a node cutset separating the root fromall the leaves \[3\].
A splitting scheme starts from the top ofthe segmentation tree (initial segmentation is a single seg-ment) and moves towards a finer segmentation by searchingat each point a subset of the exact neighborhood that consistsof all possible splits for all nodes of the current segmenta-tion.
The size of this search neighborhood is O(N).
Theopposite, merging strategy, is to start with an initial segmen-tation with one frame long segments (bottom of the tree) andsearch over all configurations with a pair of two consecutivesegments of the original segmentation merged into a singlesegment.
The latter scheme, has in general a smaller searchneighborhood, O(n).
We can argue though that this type ofsearch is more effective for the JSR problem: the size ofthe minimal exact neighborhood is much smaller for a seg-mentation that consists of single frame segments than for asegmentation with a single segment.
Since the search neigh-borhoods for both the splitting and merging schemes haveapproximately the same size, it is much easier to fall intolocal optima near the top of the tree (splitting scheme) thanat the bottom because we are searching a smaller portion ofthe exact neighborhood.As an example in speech processing, the search strat-egy followed in the MIT SUMMIT system for finding a175.
\ [  In lt io leegmentotlone ?I I I  I1.
Split 2.
MergeI I  I"half segment" 5.
Spl it  ond Merge RightneighborsI I I4.
Spl it  end Merge Left5.Figure 1: Split and Merge segmentation neighborsdendrogram\[11\] is of the latter form.
The method is a merg-ing scheme that tries to minimize some measure of spectraldissimilarity and constrain the segmentation space for the fi-nal search.
Similar search methods have also been used forimage segmentation and are referred to as region growingtechniques.A Split and Merge Algor i thmIn this paper we propose to use a combined split and mergestrategy for the JSR problem.
Such a method has originallybeen used for image segmentation \[3\].
This approach as asearch neighborhood that is the union of the neighborhoodsof the splitting and merging methods discussed previously.The advantages of such a method are first that it is harderto fall into local optima because of the larger neighborhoodsize and second that it converges faster, if we assume thatwe start from a point closer to the true segmentation thanthe top or the bottom.To relate this algorithm to the general family of localsearch algorithms, consider the neighborhood that consistsof segmentations with segments restricted to be not smallerthan half the size of the segments in the current segmen-tation.
In Figure 1 we show the corresponding single andtwo segment neighbors over one and two segments that alsobelong to the exact neighborhood.
In the same way, we candefine the "half segment" neighbors over three or more con-secutive segments.
As we shall see though, we found thatextending the search above two-segment eighbors was un-necessary.
The original split and merge neighborhood con-sists of neighbors 1 and 2 in Figure 1.
We can either splita segment in half (at the middle frame) or merge it with thenext segment.
Let us denote the corresponding neighbor-hoods by Ns (S), Nm(S).
Then the basic Split and Mergealgorithm consists of a local search over all segmentationsin the union of N6(S) and Nm(S).
Furthermore, we chooseto follow a "steepest ascent" search strategy, rather than afirst improvement one.
Specifically, given that the segmen-tation at iteration k is Sk, we choose to replace it with thesegmentation Sk+1S~?1 = arg max I(S)S E No (Sh)u N., (Sh)if it improves the current score,/(Sk+l) > l(Sk).Convergence in a finite number of steps is guaranteedbecause at each step the likelihood can only increase andthere are only a finite number of possible steps.Improvements to the basic algorithmSeveral other modifications to the basic algorithm were con-sidered to help avoid local optima.
We found it useful tochange our splitting scheme to a two step process: first, thedecision for a split at the middle point is taken, and thenthe boundary between the two children segments i adjustedby examining the likelihood ratio of the last and first dis-tributions of the corresponding models.
The boundary isfinally adjusted to the new frame if this action increasesthe likelihood.
This method does not actually increase thesearch neighborhood, because boundary adjustment will beclone only ff the decision for a split was originally taken.However, it was adopted as a good compromise betweencomputational load and performance.A second method that helped avoid local optima was theuse of combined actions.
In addition to single splits ormerges, we search over segmentations produced by split-ting a segment and merging the first or second half with theprevious or next segment respectively.
In essence, this ex-pands the search neighborhood by including the neighbors3 and 4 in figure 1.We also considered several other methods to escape localoptima, like taking random or negative steps at the end.However, since the performance of the algorithm with theprevious improvements was very close to the optimum aswe shall see, the additional overload was not justified.Constrained searchesIn this section, we describe how the split and merge searchcan be constrained when the phones are not independent,for example when bigram probabilities are used.
So far, theindependence assumption allowed us to associate the mostlikely candidate to each segment and then perform a searchover all possible segmentations.
When this assumption ist is t associated to the segment s,.
no longer valid, the cost c~.not unique for all possible segmentations that include thissegment, so the search must be performed over all possiblesegmentations and phone sequences.
For the DP search,if bigram probabilities of the form P(ak+llak) are used, thesearch should be done over all allowable phone durations andall phone labels at each time, which means that the searchspace size is multiplied by the number of phone models.
Asuboptimal search can be performed with the split and mergesegmentation algorithm as follows:At each iteration of the algorithm, the scores of all thecandidate phones are computed and stored for all segments176and all segmentations in the search neighborhood, withoutusing the constraint probabilities (we make the assumptionthat we have probabilistic onstraints, in the form of a statis-tical grammar as above).
Next, an action (split or merge) istaken if by changing only the phone labels of the segmentsthe global likelihood is increased, this time including theconstraint probabilities.
This guarantees that the likelihoodof the new segmentation is bigger than the previous one.
Ifthe decision is to take the action, we fix the segmentationand then perform a search over the phone labels only.
Forexample, if we use bigram probabilities, this can be donewith a Dynamic Programming search over the phonetic la-bels with the acoustic scores of the current segmentation.This last step is necessary, because once an action is takenand a local change is made, we are not guaranteed that thecurrent scenario is globally optimum for the given segmen-tation because of the constraints (e.g.
independent phones).As in the independent phone case, convergence of the al-gorithm is ensured because after each iteration of the algo-rithm a new configuration will be adopted only if it increasesthe likelihood.In the case of non-independent phones, the exact mini-mal search neighborhood can be shown to be the set of allsegmentations.
It is therefore much easier for a local searchalgorithm to get trapped into local optima.
For this reason,it is important that for constrained searches the above men-tioned algorithm starts with a good initial segmentation.
Inpractice we obtained a good starting segmentation by firstrunning an unconstrained search using only unigram proba-bilities and incorporating the constraints as mentioned abovein a second pass.ComplexityThe size of the search neighborhood at each iteration of thesplit and merge algorithm is O(n), with n the number ofcurrent segments.
However, if the scores at each iterationof the algorithm are stored, then the number of segmentscore evaluations as defined in (3) has the order of the num-ber of iterations with an additional initial cost.
In the caseof constrained searches, we can argue that there may existpathological situations where all possible configurations arevisited, and the number of iterations is exponential with N.As we shall see though, we found experimentally that thenumber of iterations was roughly equal to n.ResultsThe methods that we presented in this paper were evaluatedon the TIMIT database \[4\].
The phone-classifier that weused in our experiments was identical to the baseline sys-tem described in \[2\].
We used Mel-warped cepstra nd theirderivatives together with the derivative of log power.
Thelength of the segment model was M = 5 and had indepen-dent samples.
We used 61 phonetic models, but in countingerrors we folded homophones together and effectively usedthe reduced CMU/MIT 39 symbol set.
However, we didnot remove the glottal stop Q in either training or recogni-tion, and allowed substitutions of a closure-stop pair by thecorresponding single closure or stop as acceptable rrors.Those two differences from the conditions used in \[5, 8\]have effects on the performance estimates which offset oneanother.The training and test sets that we used consist of 317speakers (2536 sentences) and 103 speakers (824 sentences)respectively.
We report our baseline results on this large testset, but our algorithm development was done on a subset ofthis set, with 12 speakers (96 sentences), and the comparisonof the split and merge algorithm to the DP search is doneon this small test set.DP / Split and Merge comparisonThe average sentence length in our small test set was 3seconds, or 300 flames when the cepstral analysis is doneevery 10 msec.
The number of segment score evaluationsper sentence for a DP search is 15,000 when we restrictthe length of the phones to be less than 50 frames, and canbe reduced to approximately 4,000 if we further estrict heaccuracy of the search to 2 frames.
In addition, for 61 phonemodels and a M = 5 long segment, here are 90,000 differentGaussian score evaluations under the frame independenceassumption that will eventually be computed uring the DPsearch.The average number of iterations for the non-constrainedsplit and merge search over these sentences was 114, with750 segment and 40,000 Gaussian score evaluations, whenwe also include the fast classification scheme.
The split andmerge search was on average 3 times faster than a two frameaccurate DP search under these conditions, and 5 times fasterthan a single frame accurate DP search.
Furthermore, thesavings increase with the length of the model: we verifiedexperimentally that doubling the length of the model hadlittle effect on the split and merge search, whereas it doubledthe recognition time of the DP search.The computation savings remain the same in the case ofconstrained searches also.
The additional segment and framescore evaluations in the second phase of the split and mergeare offset by the increased search space in the DP search.Finally, we should notice that the fast classification schemehas only a small effect when a DP search is used.A fast suboptimal search for the JSR problem is justifiedif the solutions it provides are not far from the global opti-mum.
Starting with a uniform initial segmentation near thebottom of the tree, we found that the split and merge searchhad almost identical performance to the DP search, withoutand with the bigram probabilities.
The results on our smalltest set are summarized in Table 1, counting substitutionsand deletions as errors in percent correct and with accuracydefined to be 100% minus the percentage of substitutions,deletions and insertions.Baseline results and DiscussionAfter adjusting the parameters of our recognizer, we testedit on the long test set.
The results for the split and mergealgorithm and the dynamic programming search, both us-ing bigram probabilities, are shown in Table 2.
On the177UnigramBigram1-Frame DP2-Frame DPSplit & mergeSegment/FrameScore Eval.15,000/90,0003,750180,000750/40,000Correct/Accuracy70.5 / 64.870.6 / 64.969.7 / 64.62-Frame DP 3,750180,000 72.3 / 66.7Split & merge 950160,000 71.9 / 66.7Table 1: DP - Split and Merge comparisonfull 61 TIMIT symbol set our recognition rate is 60% cor-rect/54% accuracy.
On the reduced symbol set, our baselineresult of 70% correct/64% accuracy compares favorably withother systems using context independent models.
For exam-ple, Lee and Hon \[5\] reported 64% correct/53% accuracyon the same database using discrete density HMMs.
Theirrecognition rate increases to 74% correct/66% accuracy withright-context-dependent phone modelling.
Lately, Robinsonand Fallside \[8\] reported 75% correct/69% accuracy usingconnectionist techniques.
Their system also uses context in-formation through a state feedback mechanism (left context)and a delay in the decision (right context).In conclusion, we achieved the main goal of this work,a significant computation reduction for connected phonerecognition using the SSM with no loss in recognition per-formance.
Our current recognition rate is close to that ofsystems that use context dependent modelling.
However, weexpect o achieve significant additional improvements whenwe incorporate context information and time correlation inthe segment model.AcknowledgmentThis work was jointly supported by NSF and DARPA underNSF grant # IRI-8902124.References\[1\] L.Bahl, P.S.Gopalakrishnan, D.Kanevskyand D.Nahamoo, "Matrix Fast Match: A Fast Methodfor Identifying a Short List of Candidate Words forDecoding", in IEEE Int.
Conf.
Acoust., Speech, SignalProcessing, Glasgow, Scotland, May 1989.\[2\] V. Digalakis, M. Ostendorf and J. R. Rohlicek, "Im-provements in the Stochastic Segment Model forPhoneme Recognition," Proceedings of the SecondDARPA Workshop on Speech and Natural Language,pp.
332-338, October 1989.\[3\] S. L. Horowitz and T. Pavlidis, "Picture segmentationby a tree traversal algorithm," Journal Assoc.
Comput.Mach., Vol.
23, No.
2, pp.
368-388, April 1976.\[4\] L.F. Lamel, R. H. Kassel and S. Seneff, "SpeechDatabase Development: Design and Analysis of theAcoustic-Phonetic Corpus," in Proc.
DARPA SpeechRecognition Workshop, Report No.
SAIC-86/1546, pp.100-109, Feb. 1986.\[5\] K.-F. Lee and H.-W. Hon, "Speaker-independent phonerecognition using Hidden Markov Models," IEEETrans.
on Acoust., Speech and Signal Proc., Vol.
ASSP-37(11), pp.
1641-1648, November 1989.\[6\] M. Ostendorf and S. Roucos, "A stochastic segmentmodel for phoneme-based continuous peech recogni-tion," In IEEE Trans.
Acoustic Speech and Signal Pro-cessing, ASSP-37(12): 1857-1869, December 1989.\[7\] C.H.Papadimitriou and K.Steiglittz, Combinatorial Op-timization, Algorithms and Complexity, Prentice-Hall,New Jersey 1982.\[8\] T. Robinson and F. Fallside, "Phoneme Recognitionfrom the TIMIT database using Recurrent Error Propa-gation Networks," Cambridge University technical re-port No.
CUED/F-INFENG/TR.42, March 1990.\[9\] S. Roucos and M. Ostendorf Dunham, "A stochasticsegment model for phoneme-based continuous peechrecognition," In IEEE Int.
Conf.
Acoust., Speech, Sig-nal Processing, pages 73-89, Dallas, TX, April 1987.Paper No.
3.3.\[10\] S. Roucos, M. Ostendorf, H. Gish, and A. Derr,"Stochastic segment modeling using the estimate-maximize algorithm," In IEEE Int.
Conf.
Acoust.,Speech, Signal Processing, pages 127-130, New York,New York, April 1988.\[11\] V. Zue, J.
Glass, M. Philips and S. Seneff, "Acousticsegmentation a d Phonetic classification i  the SUM-MIT system," in IEEE Int.
Conf.
Acoust., Speech, Sig-nal Processing, Glasgow, Scotland, May 1989.CorrectSubstitutionsDeletionsInsertionsAccuracySplit & Merge DP search69.7% (22,285)19.8% (6,339)10.5% (3,350)6.0% (1,905)63.7%70.0% (22,387)19.9% (6,350)10.1% (3,237)5.8% (1,868)64.2%Table 2: Baseline results178
