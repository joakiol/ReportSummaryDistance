Learning a Translation Lexicon from Monolingual CorporaPhilipp Koehn and Kevin KnightInformation Sciences Institute, University of Southern California4676 Admiralty Way, Marina del Rey, CA 90292koehn@isi.edu, knight@isi.eduAbstractThis paper presents work on the taskof constructing a word-level translationlexicon purely from unrelated mono-lingual corpora.
We combine vari-ous clues such as cognates, similarcontext, preservation of word similar-ity, and word frequency.
Experimen-tal results for the construction of aGerman-English noun lexicon are re-ported.
Noun translation accuracy of39% scored against a parallel test cor-pus could be achieved.1 IntroductionRecently, there has been a surge in researchin machine translation that is based on em-pirical methods.
The seminal work by Brownet al [1990] at IBM on the Candide system laidthe foundation for much of the current workin Statistical Machine Translation (SMT).Some of this work has been re-implementedand is freely available for research purposes [Al-Onaizan et al, 1999].Roughly speaking, SMT divides the task oftranslation into two steps: a word-level trans-lation model and a model for word reorderingduring the translation process.The statistical models are trained on parallelcorpora: large amounts of text in one languagealong with their translation in another.
Var-ious parallel texts have recently become avail-able, mostly from government sources such asparliament proceedings (the Canadian Hansard,the minutes of the European parliament1) or lawtexts (from Hong Kong).Still, for most language pairs, parallel textsare hard to come by.
This is clearly the case forlow-density languages such as Tamil, Swahili, orTetun.
Furthermore, texts derived from parlia-ment speeches may not be appropriate for a par-ticular targeted domain.
Specific parallel textscan be constructed by hand for the purpose oftraining an SMT system, but this is a very costlyendeavor.On the other hand, the digital revolution andthe wide-spread use of the World Wide Webhave proliferated vast amounts of monolingualcorpora.
Publishing text in one language is amuch more natural human activity than produc-ing parallel texts.
To illustrate this point: Theworld wide web alone contains currently overtwo billion pages, a number that is still grow-ing exponentially.
According to Google,2 theword directory occurs 61 million times, empathy383,000 times, and reflex 787,000 times.
In theHansard, each of these words occurs only once.The objective of this research to build a trans-lation lexicon solely from monolingual corpora.Specifically, we want to automatically generatea one-to-one mapping of German and Englishnouns.
We are testing our mappings againsta bilingual lexicon of 9,206 German and 10,645English nouns.The two monolingual corpora should be in afairly comparable domain.
For our experimentswe use the 1990-1992 Wall Street Journal corpus1Available for download at http://www.isi.edu/?koehn/publications/europarl/2http://www.google.com/July 2002, pp.
9-16.
Association for Computational Linguistics.ACL Special Interest Group on the Lexicon (SIGLEX), Philadelphia,Unsupervised Lexical Acquisition: Proceedings of the Workshop of theon the English side and the 1995-1996 Germannews wire (DPA) corpus on the German side.Both corpora are news sources in the generalsense.
However, they span different time peri-ods and have a different orientation: the WorldStreet Journal covers mostly business news, theGerman news wire mostly German politics.For experiments on training probabilistictranslation lexicons from parallel corpora andsimilar tasks on the same test corpus, refer toour earlier work [Koehn and Knight, 2000, 2001].2 CluesThis section will describe clues that enable us tofind translations of words of the two monolingualcorpora.
We will examine each clue separately.The following clues are considered:?
Identical words ?
Two languages containa certain number of identical words, such ascomputer or email.?
Similar Spelling ?
Some words may havevery similarly written translations due tocommon language roots (e.g.
Freund andfriend) or adopted words (e.g.
Webseite andwebsite).?
Context ?
Words that occur in a certaincontext window in one language have trans-lations that are likely to occur in a similarcontext window in the other language (e.g.Wirtschaft co-occurs frequently with Wach-stum, as economy does with growth).?
Similarity ?
Words that are used simi-larly in one language should have transla-tions that are also similar (e.g.
Wednesdayis similar to Thursday as Mittwoch is similarto Donnerstag).?
Frequency ?
For comparable corpora, fre-quent words in one corpus should havetranslations that are frequent in the othercorpus (e.g.
for news corpora, government ismore frequent than flower, as its translationRegierung is more frequent than Blume.We will now look in detail how these cluesmay contribute to building a German-Englishtranslation lexicon.2.1 Identical wordsDue to cultural exchange, a large number ofwords that originate in one language are adoptedby others.
Recently, this phenomenon can beseen with words such as Internet, or Aids.These terms may be adopted verbatim, orchanged by well-established rules.
For instance,immigration (German and English) has the Por-tuguese translation immigrac?a?o, as many wordsending in -tion have translations with the samespelling except for the ending changed to -c?a?o.We examined the German words in our lex-icon and tried to find English words that havethe exact same spelling.
Surprisingly, we couldcount a total of 976 such words.
When check-ing them against a benchmark lexicon, we foundthese mappings to be 88% correct.The correctness of word mappings acquiredin this fashion depends highly on word length.This is illustrated in Table 1: While identical 3-letter words are only translations of each other60% of the time, this is true for 98% of 10-letterwords.
Clearly, for shorter words, the acciden-tal existence of an identically spelled word inthe other language word is much higher.
Thisincludes words such as fee, ton, art, and tag.Length Number of words Accuracycorrect wrong3 33 22 60%4 127 48 69%5 129 22 85%6 162 13 93%7 131 4 97%8 86 4 96%9 80 4 95%10 57 1 98%11+ 50 3 94%Table 1: Testing the assumption that identicallyspelled words are in fact translations of eachother: The accuracy of this assumption dependshighly on the length of the words (see Section2.1)Knowing this allows us to restrict the wordlength to be able to increase the accuracy of thecollected word pairs.
For instance, by relyingonly on words at least of length 6, we could col-lect 622 word pairs with 96% accuracy.
In ourexperiments, however, we included all the wordspairs.As already mentioned, there are some well-established transformation rules for the adop-tion of words from a foreign language.
For Ger-man to English, this includes replacing the let-ters k and z by c and changing the ending -ta?tby -ty.
Both these rules can be observed in theword pair Elektrizita?t and electricity.By using these two rules, we can gather 363additional word pairs of which 330, or 91%, arein fact translations of each other.
The combinedtotal of 1339 (976+363) word pairs are separatedand form the seed for some of the following steps.2.2 Similar SpellingWhen words are adopted into another language,their spelling might change slightly in a mannerthat can not be simply generalized in a rule.
Ob-serve, for instance website and Webseite.
This iseven more the case for words that can be tracedback to common language roots, such as friendand Freund, or president and Pra?sident.Still, these words ?
often called cognates ?maintain a very similar spelling.
This can bedefined as differing in very few letters.
Thismeasurement can be formalized as the numberof letters common in sequence between the twowords, divided by the length of the longer word.The example word pair friend and freundshares 5 letters (fr-e-nd), and both words havelength 6, hence there spelling similarity is 5/6, or0.83.
This measurement is called longest com-mon subsequence ratio [Melamed, 1995].
Inrelated work, string edit distance (or, Lev-enshtein distance) has been used [Mann andYarowski, 2001].With this computational means at hand, wecan now measure the spelling similarity betweenevery German and English word, and sort pos-sible word pairs accordingly.
By going throughthis list starting at the top we can collect newword pairs.
We do this is in a greedy fashion ?once a word is assigned to a word pair, we donot look for another match.
Table 2 gives thetop 24 generated word pairs by this algorithm.German English ScoreOrganisation organization 0.92 correctPra?sident president 0.90 correctIndustrie industries 0.90 correctParlament parliament 0.90 correctInteresse interests 0.89 correctInstitut institute 0.89 correctSatellit satellite 0.89 correctDividende dividend 0.89 correctMaschine machine 0.88 correctMagazin magazine 0.88 correctFebruar february 0.88 correctProgramm program 0.88 correctGremium premium 0.86 wrongBranche branch 0.86 wrongVolumen volume 0.86 correctJanuar january 0.86 correctWarnung warning 0.86 correctPartie parties 0.86 correctDebatte debate 0.86 correctExperte expert 0.86 correctInvestition investigation 0.85 wrongMutter matter 0.83 wrongBruder border 0.83 wrongNummer number 0.83 correctTable 2: First 24 word pairs collected by find-ing words with most similar spelling in a greedyfashion.The applied measurement of spelling similar-ity does not take into account that certain letterchanges (such as z to s, or dropping of the fi-nal e) are less harmful than others.
Tiedemann[1999] explores the automatic construction of astring similarity measure that learns which let-ter changes occur more likely between cognatesof two languages.
This measure is trained, how-ever, on parallel sentence-aligned text, which isnot available here.Obviously, the vast majority of word pairs cannot be collected this way, since their spellingshows no resemblance at all.
For instance,Spiegel and mirror share only one vowel, whichis rather accidental.2.3 Similar ContextIf our monolingual corpora are comparable, wecan assume a word that occurs in a certain con-text should have a translation that occurs in asimilar context.Context, as we understand it here, is definedby the frequencies of context words in surround-ing positions.
This local context has to be trans-lated into the other language, and we can searchthe word with the most similar context.This idea has already been investigated in ear-lier work.
Rapp [1995, 1999] proposes to collectcounts over words occurring in a four word win-dow around the target word.
For each occur-rence of a target word, counts are collected overhow often certain context words occur in the twopositions directly ahead of the target word andthe two following positions.
The counts are col-lected separately for each position and then en-tered into in a context vector with an dimensionfor each context word in each position.
Finally,the raw counts are normalized, so that for eachof the four word positions the vector values addup to one.
Vector comparison is done by addingall absolute differences of all components.Fung and Yee [1998] propose a similar ap-proach: They count how often another word oc-curs in the same sentence as the target word.The counts are then normalized by a using thetf/idf method which is often used in informationretrieval [Jones, 1979].The need for translating the context poses achicken-and-egg problem: If we already have atranslation lexicon we can translate the contextvectors.
But we can only construct a translationlexicon with this approach if we are already ableto translate the context vectors.Theoretically, it is possible to use these meth-ods to build a translation lexicon from scratch[Rapp, 1995].
The number of possible mappingshas complexity O(n!
), and the computing cost ofeach mapping has quadratic complexity O(n2).For a large number of words n ?
at least morethan 10,000, maybe more than 100,000 ?
thecombined complexity becomes prohibitively ex-pensive.Because of this, both Rapp and Fung focus onexpanding an existing large lexicon to add a fewnovel terms.Clearly, a seed lexicon to bootstrap thesemethods is needed.
Fortunately, we have out-lined in Section 2.1 how such a seed lexicon canbe obtained: by finding words spelled identicallyin both languages.We can then construct context vectors thatcontain information about how a new unmappedword co-occurs with the seed words.
This vec-tor can be translated into the other language,since we already know the translations of theseed words.Finally, we can look for the best matchingcontext vector in the target language, and de-cide upon the corresponding word to constructa word mapping.Again, as in Section 2.2, we have to com-pute all possible word ?
or context vector ?matches.
We collect then the best word matchesin a greedy fashion.
Table 3 displays the top 15generated word pairs by this algorithm.
Thecontext vectors are constructed in the way pro-posed by Rapp [1999], with the difference thatwe collect counts over a four noun window, not afour word window, by dropping all intermediatewords.German English ScoreJahr mr 5.03024 wrongRegierung government 5.54937 correctProzent percent 5.57756 correctAngabe us 5.73654 wrongMittwoch company 5.83199 wrongDonnerstag time 5.90623 wrongPra?sident president 5.93884 correctDienstag year 5.94611 wrongStaat state 5.96725 correctZeit people 6.05552 wrongFreitag officials 6.11668 wrongMontag week 6.13604 wrongKrieg war 6.13604 correctWoche yesterday 6.15378 wrongKrankheit disease 6.20817 correctKirche church 6.21477 correctUnternehmen companies 6.22896 correctEnde money 6.28154 wrongStreik strike 6.28690 correctEnergie energy 6.29883 correctO?l oil 6.30794 correctMarkt market 6.31116 correctWirtschaft economy 6.34883 correctSonntag group 6.34917 wrongTable 3: First 24 word pairs collected by find-ing words with most similar context vectors ina greedy fashion.2.4 Preserving Word SimilarityIntuitively it is obvious that pairs of words thatare similar in one language should have trans-lations that are similar in the other language.For instance, Wednesday is similar to Thursdayas Mittwoch is similar to Donnerstag.
Or: dog issimilar to cat in English, as Hund is similar toKatze in German.The challenge is now to come up with a quan-tifiable measurement of word similarity.
Onestrategy is to define two words as similar if theyoccur in a similar context.
Clearly, this is thecase for Wednesday and Thursday, as well as fordog and cat.Exactly this similarity measurement is usedin the work by Diab and Finch [2000].
Theirapproach to constructing and comparing con-text vectors differs significantly from methodsdiscussed in the previous section.For each word in the lexicon, the context vec-tor consists of co-occurrence counts in respect to150 so-called peripheral tokens, basically themost frequent words.
These counts are collectedfor each position in a 4-word window around theword in focus.
This results in a 600-dimensionalvector.Instead of comparing these co-occurrencecounts directly, the Spearman rank ordercorrelation is applied: For each position thetokens are compared in frequency and the fre-quency count is replaced by the frequency rank?
the most frequent token count is replaced by1, the least frequent by n = 150.
The similarityof two context vectors a = (ai) and b = (bi) isthen defined by:3R(a, b) = 1?6?
(ai?bi)24n(n2?1)The result of all this is a matrix with similar-ity scores between all German words, and secondone with similarity scores between all Englishwords.
Such matrices could also be constructedusing the definitions of context we reviewed inthe previous section.
The important point hereis that we have generated a similarity matrix,which we will use now to find new translationword pairs.Again, as in the previous Section 2.3, we as-3In the given formula we fixed two mistakes of theoriginal presentation [Diab and Finch, 2000]: The squareof the differences is used, and the denominator containsthe additional factor 4, since essentially 4 150-word vec-tors are compared.sume that we will already have a seed lexicon.For a new word we can look up its similarityscores to the seed words, thus creating a simi-larity vector.
Such a vector can be translatedinto the other language ?
recall that dimensionsof the vector are the similarity scores to seedwords, for which we already have translations.The translated vector can be compared to othervectors in the second language.As before, we search greedily for the bestmatching similarity vectors and add the corre-sponding words to the lexicon.2.5 Word FrequencyFinally, another simple clue is the observationthat in comparable corpora, the same conceptsshould be used with similar frequencies.
Even ifthe most frequent word in the German corpus isnot necessarily the translation of the most fre-quent English word, it should also be very fre-quent.Table 4 illustrates the situation with our cor-pora.
It contains the top 10 German and En-glish words, together with the frequency ranksof their best translations.
For both languages, 4of the 10 words have translations that also rankin the top 10.Clearly, simply aligning the nth frequent Ger-man word with the nth frequent English word isnot a viable strategy.
In our case, this is addi-tionally hampered by the different orientation ofthe news sources.
The frequent financial termsin the English WSJ corpus (stock, bank, sales,etc.)
are rather rare in the German corpus.For most words, especially for more compara-ble corpora, there is a considerable correlationbetween the frequency of a word and its transla-tion.
Our frequency measurement is defined asratio of the word frequencies, normalized by thecorpus sizes.3 ExperimentsThis section provides more detail on the exper-iments we have carried out to test the methodsjust outlined.Rank German English Rank1 Jahr year 32 Land country 1123 Regierung government 184 Prozent percent 15 Pra?sident president 86 Staat state 247 Million million 228 Angabe statement 3359 Mittwoch wednesday 29810 USA us 54 Prozent percent 1308 Herr mr 21 Jahr year 372 Unternehmen company 410 USA us 558 Markt market 6150 Aktie stock 75 Pra?sident president 852 Bank bank 9119 Umsatz sales 10Table 4: The frequency ranks of the most fre-quent German and English words and theirtranslations.3.1 Evaluation measurementsWe are trying to build a one-to-one German-English translation lexicon for the use in a ma-chine translation system.To evaluate this performance we use two dif-ferent measurements: Firstly, we record howmany correct word-pairs we have constructed.This is done by checking the generated word-pairs against an existing bilingual lexicon.4 Inessence, we try to recreate this lexicon, whichcontains 9,206 distinct German and 10,645 dis-tinct English nouns and 19,782 lexicon entries.For a machine translation system, it is of-ten more important to get more frequently usedwords right than obscure ones.
Thus, our secondevaluation measurement tests the word transla-tions proposed by the acquired lexicon againstthe actual word-level translations in a 5,000 sen-tence aligned parallel corpus.5The starting point to extending the lexiconis the seed lexicon of identically spelled words,as described in Section 2.1.
It consists of 1339entries, of which are (88.9%) correct according4extracted from LEO, http://dict.leo.org/5extracted from the German radio news corpus de-news, http://www.mathematik.uni-ulm.de/de-news/to the existing bilingual lexicon.
Due to com-putational constraints,6 we focus on the addi-tional mapping of only 1,000 German and En-glish words.These 1,000 words are chosen from the 1,000most frequent lexicon entries in the dictionary,without duplications of words.
This frequencyis defined by the sum of two word frequencies ofthe words in the entry, as found in the mono-lingual corpora.
We did not collect statistics ofthe actual use of lexical entries in, say, a parallelcorpus.In a different experimental set-up we also sim-ply tried to match the 1,000 most frequent Ger-man words with the 1,000 most frequent Englishwords.
The results do not differ significantly.3.2 Greedy extensionEach of the four clues described in the Sections2.2 to 2.5 provide a matching score between aGerman and an English word.
The likelihoodof these two words being actual translations ofeach other should correlate to these scores.There are many ways one could search for thebest set of lexicon entries based on these scores.We could perform an exhaustive search: con-struct all possible mappings and find the high-est combined score of all entries.
Since there areO(n!)
possible mappings, a brute force approachto this is practically impossible.We therefore employed a greedy search: Firstwe search for the highest score for any word pair.We add this word pair to the lexicon, and dropword pairs that include either the German andEnglish word from further search.
Again, wesearch for the highest score and add the corre-sponding word pair, drop these words from fur-ther search, and so on.
This is done iteratively,until all words are used up.Tables 2 and 3 illustrate this process for thespelling and context similarity clues, when ap-plied separately.6For matching 1,000 words, the described algorithmsrun up to 3 days.
Since the complexity of these algo-rithms is O(n2) in regard to the number of words, a fullrun on 10,000 would take almost a year.
Of course, thismay be alleviated by more efficient implementation andparallelization.3.3 ResultsThe results are summarized in Table 5.
Recallthat for each word that we are trying to map tothe other language, a thousand possible targetwords exist, but only one is correct.
The base-line for this task, choosing words at random, re-sults on average in only 1 correct mapping inthe entire lexicon.
A perfect lexicon, of course,contains 1000 correct entries.The starting point for the corpus score isthe 15.8% that are already achieved with theseed lexicon from Section 2.1.
In an experimentwhere we identified the best lexical entries usinga very large parallel corpus, we could achieve89% accuracy on this test corpus.Clues Entries CorpusIdentical Words (1339 Seed) - 15.8%Spelling 140 25.4%Context 107 31.9%Preserving Similarity 2 15.8%Frequency 2 17.0%Spelling+Context 185 38.6%Spelling+Frequency 151 27.4%Spelling+Context+Similarity 186 39.0%All clues 186 39.0%Table 5: Overview of results.
We evaluate howmany correct lexicon entries where added (En-tries), and how well the resulting translationlexicon performs compared to the actual word-level translations in a parallel corpus (Corpus).For all experiments the starting point was theseed lexicon of 1339 identical spelled words de-scribed in Section 2.1. which achieve 15.8% Cor-pus score.Taken alone, both the context and spellingclues learn over a hundred lexicon entries cor-rectly.
The similarity and frequency clues, how-ever, seem to be too imprecise to pinpoint thesearch to the correct translations.A closer look of the spelling and context scoresreveals that while the spelling clue allows tolearn more correct lexicon entries (140 opposedto 107), the context clue does better with themore frequently used lexicon entries, as foundin the test corpus (accuracy of 31.9% opposedto 25.4%).3.4 Combining CluesCombining different clues is quite simple: Wecan simply add up the matching scores.
Thescores can be weighted.
Initially we simplyweighted all clues equally.
We then changed theweights to see, if we can obtain better results.We found that there is generally a broad rangeof weights that result in similar performance.When using the spelling clue in combinationwith others, we found it useful to define a cutoff.If two words agree in 30% of their letters thisis generally as bad as if they do not agree inany ?
the agreements are purely coincidental.Therefore we counted all spelling scores below0.3 as 0.3.Combining the context and the spelling cluesyields a significantly better result than usingeach clue by itself.
A total of 185 correct lexicalentries are learned with a corpus score of 38.6%.Adding in the other scores, however, does notseem to be beneficial: only adding the frequencyclue to the spelling clue provides some improve-ment.
In all other cases, these scores are nothelpful.Besides this linear combination of scores fromthe different clues, more sophisticated methodsmay be possible [Koehn, 2002].4 ConclusionsWe have attempted to learn a one-to-one trans-lation lexicon purely from unrelated monolin-gual corpora.
Using identically spelled wordsproved to be a good starting point.
Beyond this,we examined four different clues.
Two of them,matching similar spelled words and words withthe same context, helped us to learn a significantnumber of additional correct lexical entries.Our experiments have been restricted tonouns.
Verbs, adjectives, adverbs and other partof speech may be tackled in a similar way.
Theymight also provide useful context informationthat is beneficial to building a noun lexicon.These methods may be also useful given adifferent starting point: For efforts in buildingmachine translation systems, some small paral-lel text should be available.
From these, somehigh-quality lexical entries can be learned, butthere will always be many words that are miss-ing.
These may be learned using the describedmethods.ReferencesAl-Onaizan, Y., Curin, J., Jahr, M., Knight,K., Lafferty, J., Melamed, D., Och,F.-J., Purdy, D., Smith, N. A., andYarowsky, D. (1999).
Statistical ma-chine translation.
Technical report, JohnHopkins University Summer Workshophttp://www.clsp.jhu.edu/ws99/projects/mt/.Brown, P., Cocke, J., Pietra, S. A. D., Pietra,V.
J. D., Jelinek, F., Lafferty, J. D., Mercer,R.
L., and Rossin, P. (1990).
A statistical ap-proach to machine translation.
ComputationalLinguistics, 16(2):76?85.Diab, M. and Finch, S. (2000).
A statisticalword-level translation model for comparablecorpora.
In Proceedings of the Conference onContent-based multimedia information access(RIAO).Fung, P. and Yee, L. Y.
(1998).
An IR approachfor translating new words from nonparallel,comparable texts.
In Proceedings of ACL 36,pages 414?420.Jones, K. S. (1979).
Experiments in relevanceweighting of search terms.
In InformationProcessing and Management, pages 133?144.Koehn, P. (2002).
Combining multiclass maxi-mum entropy classifiers with neural networkvoting.
In Proceedings of PorTAL.Koehn, P. and Knight, K. (2000).
Estimatingword translation probabilities from unrelatedmonolingual corpora using the EM algorithm.In Proceedings of AAAI.Koehn, P. and Knight, K. (2001).
Knowl-edge sources for word-level translation mod-els.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Process-ing, EMNLP.Mann, G. S. and Yarowski, D. (2001).
Multi-path translation lexicon induction via bridgelanguages.
In Proceedings of NAACL, pages151?158.Melamed, D. (1995).
Automatic evaluation anduniform filter cascades for inducing n-besttranslation lexicons.
In Third Workshop onVery Large Corpora.Rapp, R. (1995).
Identifying word translationsin non-parallel texts.
In Proceedings of ACL33, pages 320?322.Rapp, R. (1999).
Automatic identification ofword translations from unrelated English andGerman corpora.
In Proceedings of ACL 37,pages 519?526.Tiedemann, J.
(1999).
Automatic construc-tion of weighted string similarity measures.In Proceedings of the Conference on Empir-ical Methods in Natural Language Processing,EMNLP.
