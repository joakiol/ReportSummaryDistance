Proceedings of the 10th Conference on Parsing Technologies, pages 156?167,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsThree-Dimensional Parametrization for ParsingMorphologically Rich LanguagesReut Tsarfaty and Khalil Sima?anInstitute for Logic, Language and ComputationUniversity of AmsterdamPlantage Muidergracht 24, 1018TV Amsterdam, The Netherlands{rtsarfat,simaan}@science.uva.nlAbstractCurrent parameters of accurate unlexical-ized parsers based on Probabilistic Context-Free Grammars (PCFGs) form a two-dimensional grid in which rewrite eventsare conditioned on both horizontal (head-outward) and vertical (parental) histories.In Semitic languages, where argumentsmay move around rather freely and phrase-structures are often shallow, there are ad-ditional morphological factors that governthe generation process.
Here we pro-pose that agreement features percolated upthe parse-tree form a third dimension ofparametrization that is orthogonal to the pre-vious two.
This dimension differs frommere ?state-splits?
as it applies to a wholeset of categories rather than to individualones and encodes linguistically motivatedco-occurrences between them.
This paperpresents extensive experiments with exten-sions of unlexicalized PCFGs for parsingModern Hebrew in which tuning the param-eters in three dimensions gradually leads toimproved performance.
Our best result in-troduces a new, stronger, lower bound on theperformance of treebank grammars for pars-ing Modern Hebrew, and is on a par withcurrent results for parsing Modern StandardArabic obtained by a fully lexicalized parsertrained on a much larger treebank.1 Dimensions of Unlexicalized ParsingProbabilistic Context Free Grammars (PCFGs) arethe formal backbone of most high-accuracy statisti-cal parsers for English, and a variety of techniqueswas developed to enhance their performance rela-tive to the na?
?ve treebank implementation ?
fromunlexicalized extensions exploiting simple categorysplits (Johnson, 1998; Klein and Manning, 2003)to fully lexicalized parsers that condition events be-low a constituent upon the head and additional lexi-cal content (Collins, 2003; Charniak, 1997).
Whileit is clear that conditioning on lexical content im-proves the grammar?s disambiguation capabilities,Klein and Manning (2003) demonstrate that a well-crafted unlexicalized PCFG can close the gap, to alarge extent, with current state-of-the-art lexicalizedparsers for English.The factor that sets apart vanilla PCFGs (Char-niak, 1996) from their unlexicalized extensions pro-posed by, e.g., (Johnson, 1998; Klein and Manning,2003), is the choice for statistical parametrizationthat weakens the independence assumptions implicitin the treebank grammar.
Studies on accurate unlex-icalized parsing models outline two dimensions ofparametrization.
The first, proposed by (Johnson,1998), is the annotation of parental history, and thesecond encodes a head-outward generation process(Collins, 2003).
Johnson (1998) augments node la-bels with the label of their parent, thus incorporat-ing a dependency on the node?s grandparent.
Collins(2003) proposes to generate the head of a phrase firstand then generate its sisters using Markovian pro-cesses, thereby exploiting head/sister-dependencies.156Klein and Manning (2003) systematize the dis-tinction between these two forms of parametrizationby drawing them on a horizontal-vertical grid: par-ent encoding is vertical (external to the rule) whereashead-outward generation is horizontal (internal tothe rule).
By varying the value of the parame-ters along the grid, Klein and Manning (2003) tunetheir treebank grammar to achieve improved perfor-mance.
This two-dimensional parametrization hasbeen instrumental in devising parsing models thatimprove disambiguation capabilities for English aswell as other languages, such as German (Dubey andKeller, 2003) Czech (Collins et al, 1999) and Chi-nese (Bikel and Chiang, 2000).
However, accuracyresults for parsing languages other than English stilllag behind.1We propose that for various languages includ-ing the Semitic family, e.g.
Modern Hebrew (MH)and Modern Standard Arabic (MSA), a third di-mension of parametrization is necessary for encod-ing linguistic information relevant for breaking falseindependence assumptions.
In Semitic languages,arguments may move around rather freely and thephrase-structure of clause-level categories is oftenshallow.
For such languages agreement features playa role in disambiguation at least as important as thevertical and horizontal conditioning.
We propose athird dimension of parameterizations that encodesmorphological features such as those realizing syn-tactic agreement.
These features are percolated fromsurface forms in a bottom-up fashion and expressinformation that is complementary to the horizon-tal and vertical generation histories proposed before.Such morphological information refines syntacticcategories based on their morpho-syntactic role, andcaptures linguistically motivated co-occurrences anddependencies manifested via, e.g., morpho-syntacticagreement.This work aims at parsing MH and explores theempirical contribution of the three dimensions ofparameters specified above.
We present extensiveexperiments that gradually lead to improved perfor-mance as we extend the degree to which the threedimensions are exploited.
Our best model uses allthree dimensions of parametrization, and our best re-1The learning curves over increasing training data (e.g., forGerman (Dubey and Keller, 2003)) show that treebank size can-not be the sole factor to account for the inferior performance.sult is on a par with those achieved for MSA using afully lexicalized parser and a much larger treebank.The remainder of this document is organized as fol-lows.
In section 2 we review characteristic aspectsof MH (and other Semitic languages) and illustratethe special role of morphology and dependenciesdisplayed by morpho-syntactic processes using thecase of syntactic definiteness in MH.
In section 3 wedefine our three-dimensional parametrization space.In section 4 we spell out the method and procedurefor the empirical evaluation of one, two and threeparametrization dimensions, and in section 5 we re-port and analyze results for different parametrizationchoices.
Finally, section 6 discusses related workand in section 7 we summarize and conclude.2 Dimensions of Modern Hebrew SyntaxParsing MH is in its infancy.
Although a syntacti-cally annotated corpus has been available for quitesome time (Sima?an et al, 2001), we know of onlytwo studies attempting to parse MH using statisticalmethods (see section 6).
One reason for the sparse-ness in this field is that the adaptation of existingmodels to parsing MH is technically involved yetdoes not guarantee to yield comparable results asthe processes that license grammatical structures ofphrases and sentences in MH differ from those as-sumed for English.
This section outlines differencesbetween English and MH and discusses their reflec-tion in the MH treebank annotation scheme.
Weargue that on top of syntactic processes exploitedby current parsers there is an orthogonal morpho-syntactic dimension which is invaluable for syntac-tic disambiguation, and it can be effectively learnedusing simple treebank grammars.2.1 Modern Hebrew StructurePhrases and sentences in MH, as well as in Arabicand other Semitic languages, have a relatively flexi-ble phrase structure.
Subjects, verbs and objects canbe inverted and prepositional phrases, adjuncts andverbal modifiers can move around rather freely.
Thefactors that affect word-order in the language are notexclusively syntactic and have to do with rhetoricaland pragmatic factors as well.22See, for instance, (Melnik, 2002) for an InformationStructure-syntactic account of verb initial sentences.157(a) SNP.MP-SBJCD.MPsnitwo.MPN.MPhildimthe-children.MPVP.MPV.MPaklwate.MPNP.FS-OBJN.FSewghcake.FS(b) SNP.FS-OBJN.FSewghcake.FSVP.MPV.MPaklwate.MPNP.MP-SBJCD.MPsnitwo.MPN.MPhildimthe-children.MPFigure 1: Word Order and Agreement Features in MHPhrases: Agreement on MP features reveals the subject-predicate dependency between surface forms and their dom-inating constituents in a variable phrase-structure (markingM(asculine), F(eminine), S(ingular), P(lural).
)It would be too strong a claim, however, to clas-sify MH (and similar languages) as a free-word-order language in the canonical sense.
The level offreedom in the order and number of internal con-stituents varies between syntactic categories.
Withina verb phrase or a sentential clause, for instance,the order of constituents obeys less strict rules thanwithin, e.g., a noun phrase.3 Figure 1 illustrates twosyntactic structures that express the same grammat-ical relations yet vary in their internal order of con-stituents.
Within the noun phrase constituents, how-ever, determiners always precede nouns.Within the flexible phrase structure it is typicallymorphological information that provides cues for thegrammatical relations between surface forms.
Infigure 1, for example, it is agreement on genderand number that reveals the subject-predicate depen-dency between surface forms.
Figure 1 also showsthat agreement features help to reveal such relationsbetween higher levels of constituents as well.Determining the child constituents that contributeeach of the features is not a trivial matter either.
Toillustrate the extent and the complexity of that matterlet us consider definiteness in MH, which is morpho-logically marked (as an h prefix to the stem, glossedhere explicitly as ?the-?)
and behaves as a syntactic3See (Wintner, 2000) and (Goldberg et al, 2006) for formaland statistical accounts (respectively) of noun phrases in MH.
(a) NP.FS.DNP.FS.Dsganit hmnhldeputy.FS the-manager.MS.DADJP.FS.Dhmswrhthe-dedicated.FS.D(a) SNP.FS.Dsganit hmnhldeputy.FS the-manager.MS.DPREDP.FSmswrhdedicated.FSFigure 2: Definiteness in MH as a Phrase-Level AgreementFeature: Agreement on definiteness helps to determine the in-ternal structure of a higher level NP (a), and the absence thereofhelps to determine the attachment to a predicate in a verb-lesssentence (b) (marking D(efiniteness))(a) SNP.FS.DNNT.FSsganitdeputy.FSN.MS.Dhmnhlthe-manager.MS.DVP.FSV.FShtpjrhresigned.FS(b) S?V?NP?NNT?.FS.DNNT.FSsganitdeputy.FSN.MS.Dhmnhlthe-manager.MS.DVP?V?
).FSV.FShtpjrhresigned.FSFigure 3: Phrase-Level Agreement Features and Head-Dependencies in MH: The direction of percolating definitenessin MH is distinct of that of the head (marking ?head-tag?
)property (Danon, 2001).
Definite noun-phrases ex-hibit agreement with other modifying phrases, andsuch agreement helps to determine the internal struc-ture, labels, and the correct level of attachment asillustrated in figure 2.
The agreement on definite-ness helps to determine the internal structure of nounphrases 2(a), and the absence thereof helps in de-termining the attachment to predicates in verb-lesssentences, as in 2(b).
Finally, definiteness may bepercolated from a different form than the one deter-mining the gender and number of a phrase.
In figure3(a), for instance, the definiteness feature (markedas D) percolates from ?hmnhl ?
(the-manager.MS.D)while the gender and number are percolated from?sganit ?
(deputy.FS).
The direction of percolationof definiteness may be distinct of that of percolat-ing head information, as can be seem in figure 3(b).
(The direction of head-dependencies in MH typi-cally coincides with that of percolating gender.
)To summarize, agreement features are helpful inanalyzing and disambiguating syntactic structures inMH, not only at the lexical level, but also at higherlevels of constituency.
In MH, features percolatedfrom different surface forms jointly determine thefeatures of higher-level constituents, and such fea-tures manifest multiple dependencies, which in turncannot be collapsed onto a single head.1582.2 The Modern Hebrew Treebank SchemeThe annotation scheme of version 2.0 of the MHtreebank (Sima?an et al, 2001)4 aims to capture themorphological and syntactic properties of MH justdescribed.
This results in several aspects that dis-tinguish the MH treebank from, e.g., the WSJ Penntreebank annotation scheme (Marcus et al, 1994).The MH treebank is built over word segments.This means that the yields of the syntactic trees donot correspond to space delimited words but ratherto morphological segments that carry distinct syn-tactic roles, i.e., each segment corresponds to a sin-gle POS tag.
(This in turn means that prefixesmarking determiners, relativizers, prepositions anddefinite articles are segmented away and appear asleaves in a syntactic parse tree.)
The POS categoriesassigned to segmented words are decorated with fea-tures such as gender, number, person and tense, andthese features are percolated higher up the tree ac-cording to pre-defined syntactic dependencies (Kry-molowski et al, 2007).
Since agreement featuresof non-terminal constituents may be contributed bymore than one child, the annotation scheme definesmultiple dependency labels that guide the percola-tion of the different features higher up the tree.
Def-initeness in the MH treebank is treated as a segmentat the POS tags level and as a feature at the level ofnon-terminals.
As any other feature, it is percolatedhigher up the tree according to marked dependencylabels.
Table 1 lists the features and values annotatedon top of syntactic categories and table 2 describesthe dependencies according to which these featuresare percolated from child constituents to their par-ents.In order to comply with the flexible phrase struc-ture in MH, clausal categories (S, SBAR and FRAGand their corresponding interrogatives SQ, SQBARand FRAGQ) are annotated as flat structures.
Verbs(VB tags) always attach to a VP mother, howeveronly non-finite VBs can accept complements un-der the same VP parent, meaning that all inflectedverb forms are represented as unary productionsunder an inflected VP.
NP and PP are annotated4Version 2.0 of the MH treebank is publicly availableat http://mila.cs.technion.ac.il/english/index.html along with a complete overview of the MHannotation scheme and illustrative examples (Krymolowski etal., 2007).Feature:Value Value Encodedgender:Z masculinegender:N femininegender:B bothnumber:Y singularnumber:R pluralnumber:B bothdefiniteness:H definitedefiniteness:U underspecifiedTable 1: Features and Values in the MH TreebankDependency Type Features PercolatedDEP HEAD allDEP MAJOR at least genderDEP NUMBER numberDEP DEFINITE definitenessDEP ACCUSATIVE caseDEP MULTIPLE all (e.g., conjunction)Table 2: Dependency Labels in the MH Treebankas nested structures capturing the recursive struc-ture of construct-state nouns, numerical expressionsand possession.
An additional category, PREDP, isadded in the treebank scheme to account for sen-tences in MH that lack a copular element, and it mayalso be decorated with inflectional features agreeingwith the subject.
The MH treebank scheme also fea-tures null elements that mark traces and additionallabels that mark functional features (e.g., SBJ,OBJ)which we strip off and ignore throughout this study.Morphological features percolated up the treemanifest dependencies that are marked locally yethave a global effect.
We propose to learn treebankgrammars in which the syntactic categories are aug-mented with morphological features at all levels ofthe hierarchy.
This allows to learn finer-grainedcategories with subtle differences in their syntacticbehavior and to capture non-independence betweencertain parts of the syntactic parse-tree.3 Refining the Parameter Space(Klein and Manning, 2003) argue that parent en-coding on top of syntactic categories and RHSmarkovization of CFG productions are two instancesof the same idea, namely that of encoding the gener-ation history of a node to a varying degree.
Theysubsequently describe two dimensions that definetheir parameters?
space.
The vertical dimension (v),capturing the history of the node?s ancestors in a top-159down generation process (e.g., its parent and grand-parent), and the horizontal dimension (h), capturingthe previously generated horizontal ancestors of anode (effectively, its sisters) in a head-outward gen-eration process.
By varying the value of h and valong this two-dimensional grid they improve per-formance of their induced treebank grammar.Formally, the probability of a parse tree pi is cal-culated as the probability of its derivation, the se-quential application of rewrite rules.
This in turnis calculated as the product of rules?
probabilities,approximated by assuming independence betweenthem P (pi) = ?i P (ri|r1 ?
... ?
ri?1) ?
?i P (ri).The vertical dimension v can be thought of as a func-tion ?0 selecting features from the generation his-tory of the constituent thus restoring selected depen-dencies:P (ri) = P (ri|?0(r1 ?
.. ?
ri?1))The horizontal dimension h can be thought of as twofunctions ?1,?2 over decomposed rules, where ?1selects hidden internal features of the parent, and?2 selects previously generated sisters in a head-outward Markovian process (we retain here the as-sumption that the head child H always matters).P (ri) = Ph(H|?1(LHS(ri)))?
?C?RHS(ri)?HPC(C|?2(RHS(ri)),H)The fact that the default notion of a treebankgrammar takes v = 1 (i.e., ?0(r1 ?
.. ?
ri?1) = ?
)and h = ?
(RHS cannot decompose) is, accordingto Klein and Manning (2003), a historical accident.We claim that languages with freeer word orderand richer morphology call for an additional dimen-sion of parametrization.
The additional parametershows to what extent morphological features en-coded in a specialized structure back up the deriva-tion of the tree.
This dimension can be thought ofas a function ?3 selecting aspects of morphologicalorthogonal analysis of the rules, where MA denotesmorphological analysis of the syntactic categories inboth LHS and RHS of the rule.P (ri) = P (ri|?3(MA(ri)))The fact that in current parsers ?3(MA(ri)) = ?
is,we claim, another historical accident.
Parsing En-glish is quite remarkable in that it can be done withFigure 4: The Three-Dimensional Parametrization Spaceimpoverished morphological treatment, but for lan-guages in which morphological processes are morepertinent, we argue, bi-dimensional parametrizationshall not suffice.The emerging picture is as follows.
Bare-categoryskeletons reside in a bi-dimensional parametrizationspace (figure 3(a)) in which the vertical (figure 3(b))and horizontal (figure 3(c)) parameter instantiationselaborate the generation history of a non-terminalnode.
Specialized structures enriched with (an in-creasing amount of) morphological features residedeeper along a third dimension we refer to as depth(d).
Figure 4 illustrates an instantiation of d = 1with a single definiteness feature.
Higher d valueswould imply adding more (accumulating) features.Klein and Manning (2003) view the verticaland horizontal parametrization dimensions as im-plementing external and internal annotation strate-gies respectively.
External parameters indicate fea-tures of the external environment that influence thenode?s expansion possibilities, and internal parame-ters mark aspects of hidden internal content whichinfluence constituents?
external distribution.
Weview the third dimension of parametrization as im-plementing a relational strategy of annotation en-coding the way different constituents may combineto form phrases and sentences.
In a bottom up pro-cess this annotation strategy imposes soft constraintson a the top-down head-outward generation process.Figure 6(a) focuses on a selected NP node high-lighted in figure 4 and shows its expansion possibil-ities in three dimensions.
Figure 6(b) illustrates howthe depth expansion interacts with both parent anno-160(a) The horizontal/vertical Grid (b) The vertical dimension (c) The horizontal dimensionFigure 5: The Two-Dimensional Space: The horizontal and vertical dimensions outlined by (Klein and Manning, 2003)tation and neighbor dependencies thereby affectingboth distributions.3.1 A Note on State-SplitsRecent studies (Klein and Manning, 2003; Mat-suzaki et al, 2005; Prescher, 2005; Petrov et al,2006) suggest that category-splits help in enhanc-ing the performance of treebank grammars, and aprevious study on MH (Tsarfaty, 2006) outlines spe-cific POS-tags splits that improve MH parsing ac-curacy.
Yet, there is a major difference betweencategory-splits, whether manually or automaticallyacquired, and the kind of state-splits that arise fromagreement features that refine phrasal categories.While category-splits aim at each category in iso-lation, agreement features apply to a whole setof categories all at once, thereby capturing refine-ment of the categories as well as linguistically mo-tivated co-occurrences between them.
Individualcategory-splits are viewed as taking place in a two-dimensional space and it is hard to analyze and em-pirically evaluate their interaction with other annota-tion strategies.
Here we propose a principled way tostatistically model the interaction between differentlinguistic processes that license grammatical struc-tures and empirically contrast their contribution.3.2 A Note on Stochastic AV grammarsThe practice of having morphological features or-thogonal to a constituency structure is not a newone and is familiar from formal theories of syntaxsuch as HPSG (Sag et al, 2003) and LFG (Ka-plan and Bresnan, 1982).
Here we propose to re-frame systematic morphological decoration of syn-tactic categories at all levels of the hierarchy as(a) (b)Figure 6: The Expansion Possibilities of a Non-TerminalNode: Expanding the NP from figure 4 in a three-dimensionalparameterization Spacean additional dimension of statistical estimation forlearning unlexicalized treebank PCFGs.
Our pro-posal deviates from various stochastic extensions ofsuch constraints-based grammatical formalisms (cf.
(Abney, 1997)) and has the advantage of elegantlybypassing the issue of loosing probability mass tofailed derivations due to unification failures.
To thebest of our knowledge, this proposal has not beenempirically explored before.4 Experimental SetupOur goal is to determine the optimal strategy forlearning treebank grammars for MH and to contrastit with bi-dimensional strategies explored for En-glish.
The methodology we use is adopted from(Klein and Manning, 2003) and our procedure isidentical to the one described in (Johnson, 1998).We define transformations over the treebank that ac-cept as input specific points in the (h, v, d) space de-picted in figure 7.
We use the transformed trainingsets for learning different treebank PCFGs which wethen used to parse unseen sentences, and detrans-form the parses for the purpose of evaluation.55Previous studied on MH used different portions of the tree-bank and its annotation scheme due to its gradual development161Data We use version 2.0 of the MH treebankwhich consists of 6501 sentences from the dailynewspaper ?Ha?aretz?.
We employ the syntactic cat-egories, POS categories and morphological featuresannotated therein.
The data set is split into 13 sec-tions consisting of 500 sentences each.
We use thefirst section (section 0) as our development set andthe last section (section 12) as our test set.
The re-maining sentences (sections 1?11) are all used fortraining.
After removing empty sentences, sentenceswith uneven bracketing and sentences that do notmatch the annotation scheme6 we remain with a de-vset of 483 sentences (average length in word seg-ments 48), a trainset of 5241 sentences (53) anda testset of 496 sentences (58).
Since this workis only the first step towards the development of abroad-coverage statistical parser for MH (and otherSemitic languages) we use the development set forparameter-tuning and error analysis and use the testset only for confirming our best results.Models The models we implement use one-, two-or three-dimensional parametrization and differentinstantiation of values thereof.
(Due to the smallsize of our data set we only use the values {0, 1}as possible instantiations.
)The v dimension is implemented using a trans-form as in (Johnson, 1998) where v = 0 correspondsto bare syntactic categories and v = 1 augmentsnode labels with the label of their parent node.The h dimension is peculiar in that it distinguishesPCFGs (h = ?
), where RHS cannot decompose,from their head-driven unlexicalized variety.
To im-plement h 6= ?
we use a PCFG transformation em-ulating (Collins, 2003)?s first model, in which sistersare generated conditioned on the head tag and a sim-ple ?distance?
function (Hageloh, 2007).7 The in-process.
As the MH treebank is approaching maturity we feelthat the time is ripe to standardize its use for MH statisticalparsing.
The software we implemented will be made availablefor non-commercial use upon request to the author(s) and thefeature percolation software by (Krymolowski et al, 2007) ispublicly available through the Knowledge Center for Process-ing Hebrew.
By this we hope to increase the interest in MHwithin the parsing community and to facilitate the applicationof more sophisticated models by cutting down on setup time.6Marked as ?NO MATCH?
in the treebank.7A formal overview of the transformation and its corre-spondence to (Collins, 2003)?s models is available at (Hageloh,2007).
We use the distance function defined therein, markingthe direction and whether it is the first node to be generated.stantiated value of h then selects the number of pre-viously generated (non-head) sisters to be taken intoaccount when generating the next sister in a Marko-vian process (?2 in our formal exposition).The d dimension we proposed is implemented us-ing a transformation that augments syntactic cate-gories with morphological features percolated up thetree.
We use d = 0 to select bare syntactic cate-gories and instantiate d = 1 with the definitenessfeature.
The decision to select definiteness (ratherthan, e.g., gender or number) is rather pragmatic asits direction of percolation may be distinct of headinformation and the question remains whether thecombination of such non-overlapping dependenciesis instrumental for parsing MH.Our baseline model is a vanilla treebank PCFGas described in (Charniak, 1996) which we locateon the (?, 0, 0) point of our coordinates-system.In a first set of experiments we implement simplePCFG extensions of the treebank trees based on se-lected points on the (?, v, d) plain.
In a secondset of experiments we use an unlexicalized head-driven baseline a` la (Collins, 2003) located on the(0, 0, 0) coordinate.
We transform the treebank treesin correspondence with different points in the three-dimensional space defined by (h, v, d).
The modelswe implement are marked in the coordinate-systemdepicted in figure 7.
The implementation details ofthe transformations we use are spelled out in tables3?4.Procedure We implement different models thatcorrespond to different instantiations of h, v and d.For each instantiation we transform the training setand learn a PCFG using Maximum Likelihood es-timates, and we use BitPar (Schmidt, 2004), an ef-ficient general-purpose parser, to parse unseen sen-tences.
The input to the parser is a sequence of wordsegments where each segment corresponds to a sin-gle POS tag, possibly decorated with morphologi-cal features.
This setup assumes partial morpholog-ical disambiguation (namely, segmentation) but cru-cially we do not disambiguate their respective POScategories.
This setup is more appropriate for us-ing general-purpose parsing tools and it makes ourresults comparable to studies in other languages.88Our working assumption is that better performance of aparsing model in our setup will improve performance also162Transliterate The lexical items (leaves) in the MH treebank are written left-to-write and are encodedin utf8.
A transliteration software is used to convert the utf encoding into Latin characters and to reversetheir order, essentially allowing for standard left-to-right processing.Correct The manual annotation resulted in unavoidable errors in the annotation scheme, such as typos(e.g., SQBQR instead of SQBAR) wrong delimiters (e.g., ?-?
instead of ?
?)
or wrong feature order (e.g.,number-gender instead of gender-number).
We used an automatic script to detect these error, we manuallydetermine their correction.
Then we created an automatic script to apply all fixes (57 errors in 1% sentences).Re-attach VB elements are attached by convention to a VP which inherits its morphological features.9 VB instances in the treebank are mistakenly attached to an S parent without an intermediate VP level.Our software re-attaches those VB elements to a VP parent and percolates its morphological features.Disjoint Due to recursive processes of generating noun phrases and numerical expression (smixut)in MH the sets of POS and syntactic categories are not disjoint.
This is a major concern for PCFG parsersthat assume disjoint sets of pre- and non-terminals.
The overlap between the sets also introduces additionalinfinite derivations to which we loose probability mass.
Our software takes care to decorate POS categoriesused as non-terminal with an additional ?P?, creating a new set of categories encoding partial derivations.Lexicalize A pre-condition for applying horizontal parameterizations a` la Collins is the annotation ofheads of syntactic phrases.
The treebank provided by the knowledge center does not define unique heads,but rather, mark multiple dependencies for some categories and none for others.
Our software uses rulesfor choosing the syntactic head according to specified dependencies and a head table when none are specified.Linearize In order to implement the head-outward constituents?
generation process we use software madeavailable to us by (Hageloh, 2007) which converts PCFG production such as the generation of a head is followed by left and rightmarkovized derivation processes.
We used two versions of Markovization, one which conditions only on thehead and a distance function, and another which conditions also on immediately neighboring sister(s).Decorate Our software implements an additional general transform which selects the features that are to beannotated on top of syntactic categories to implement various parametrization decisions.
This transform can beused for, e.g., displaying parent information, selecting morphological features, etc.Table 3: Transforms over the MH Treebank: We clean and correct the treebank using Transliterate, Correct, Re-attach andDisjoint, and transform the training set according to certain parametrization decisions using Lexicalize, Linearize and Decorate.Smoothing pre-terminal rules is done explicitly bycollecting statistics on ?rare word?
occurrences andproviding the parser with possible open class cat-egories and their corresponding frequency counts.The frequency threshold defining ?rare words?
wastuned empirically and set to 1.
The resulting testparses are detransformed and to skeletal constituentstructures, and are compared against the gold parsesto evaluate parsing accuracy.Evaluation We evaluate our models using EVALBin accordance with standard PARSEVAL evaluationmetrics.
The evaluation of all models focuses onLabeled Precision and Recall considering bare syn-tactic categories (stripping off all morphological orparental features and removing intermediate nodesfor linearization).
We report the average F-measurefor sentences of length up to 40 and for all sentences(F?40 and FAll respectively).
We report the resultswithin an integrated model for morphological and syntactic dis-ambiguation in the spirit of (Tsarfaty, 2006).
We conjecturethat the kind of models developed here which takes into accountmorphological information is more appropriate for the morpho-logical disambiguation task defined therein.for two evaluation options, once including punctua-tion marks (WP ) and once excluding them (WOP ).5 ResultsOur baseline for the first set of experiments isa vanilla PCFG as described in (Charniak, 1996)(without a preceding POS tagging phase and withoutright branching corrections).
We transform the tree-bank trees based on various points in the (?, v, d)two-dimensional space to evaluate the performanceof the resulting PCFG extensions.Table 5 reports the accuracy results for all modelson section 0 (devset) of the treebank.
The accuracyresults for the vanilla PCFG are approximately 10%lower than reported by (Charniak, 1996) for Englishdemonstrating that parsing MH using the currentlyavailable treebank is a harder task.
For all unlexical-ized extensions learned from the transfromed tree-banks, the resulting grammars show enhanced dis-ambiguation capabilities and improved parsing ac-curacy.
We observe that the vertical dimension con-tributes the most from both one-dimensional mod-163Name Params Description Transforms usedDIST h = 0 0-order Markov process Lexicalize(category), Linearize(distance)MRK h = 1 1-order Markov process Lexicalize(category), Linearize(distance, neighbor)PA v = 1 Parent Annotation Decorate(parent)DEF d = 1 Definiteness feature percolation Decorate(definiteness)Table 4: Implementing Different Parametrization Options using TransformsImplementation (h, v, d) FALL F?40 FALL F?40WP WP WOP WOPPCFG (?, 0, 0) 65.17 66.63 66.17 67.7PA (?, 0, 1) 70.6 71.96 70.96 72.18DEF (?, 1, 0) 67.53 68.78 68.82 70.06PA+DEF (?, 1, 1) 72.63 73.89 73.01 74.11Table 5: PCFG Two-Dimensional Extensions: Accuracy re-sults for parsing the devest (section 0)els.
A qualitative error analysis reveals that parentannotation strategy distinguishes effectively variouskinds of distributions clustered together under a sin-gle category.
For example, S categories that appearunder TOP tend to be more flat than S categories ap-pearing under SBAR (SBAR clauses typically gen-erate a non-finite VP node under which additionalPP modifiers can be attached).Orthogonal morphological marking provide addi-tional information that is indicative of the kind ofdependencies that exist between a category and itsvarious child constituents, and we see that the d di-mension instantiated with definiteness not only con-tribute more than 2% to the overall parsing accuracyof a vanilla PCFG, but also contributes as much tothe improvement obtained from a treebank alreadyannotated with the vertical dimension.
The contribu-tions are thus additive providing preliminary empir-ical support to our claim that these two dimensionsprovide information that is complementary.In our next set of experiments we evaluate thecontribution of the depth dimension to extensions ofthe head-driven unlexicalized variety a` la (Collins,2003).
We set our baseline at the (0, 0, 0) coordi-nate and evaluate models that combine one, two andthree dimensions of parametrization.
Table 6 showsthe accuracy results for parsing section 0 using theresulting models.The first outcome of these experiments is that ournew baseline improves on the accuracy results ofa simple treebank PCFG.
This result indicates thathead-dependencies which play a role in determin-ing grammatical structures in English are also in-strumental for parsing MH.
However, the marginalcontribution of the head-driven variation is surpris-ingly low.
Next we observe that for one-dimensionalmodels the vertical dimension still contributes themost to parsing accuracy.
However, morphologi-cal information represented by the depth dimensioncontributes more to parsing accuracy than informa-tion concerning immediately preceding sisters onthe horizontal dimension.
This outcome is consis-tent with our observation that the grammar of MHputs less significance on the position of constituentsrelative to one others and that morphological in-formation is more indicative of the kind of syntac-tic relations that appear between them.
For two-dimensional models, incorporating the depth dimen-sion (orthogonal morphological marking) is betterthan not doing so, and relying solely on horizon-tal/vertical parameters performs slightly worse thanthe vertical/depth combination.
The best performingmodel for two-dimensional head-driven extensionsis the one combining vertical history and morpho-logical depth.
This is again consistent with the prop-erties of MH highlighted in section 2 ?
parental in-formation gives cues about the possible expansionon the current node, and morphological informationindicates possible interrelation between child con-stituents that may be generated in a flexible order.Our second set of experiments shows that a three-dimensional annotation strategy strikes the best bal-ance between bias and variance and achieves the bestaccuracy results among all models.
Different dimen-sions provide different sorts of information whichare complementary, resulting in a model that is ca-pable of generalizing better.
The total error reduc-tion from a plain PCFG is more than 20%, and ourbest result is on a par with those achieved for otherlanguages (e.g., 75% for MSA).164Implementation Params FALL F?40 FALL F?40(h, v, d) WP WP WOP WOPDIST (0, 0, 0) 66.56 68.20 67.59 69.24MRK (1, 0, 0) 66.69 68.14 67.93 69.37PA (0, 1, 0) 68.87 70.48 69.64 70.91DEF (0, 0, 1) 68.85 69.92 70.42 71.45PA+MRK (1, 1, 0) 69.97 71.48 70.69 71.98MRK+DEF (1, 0, 1) 69.46 70.79 71.05 72.37PA+DEF (0, 1, 1) 71.15 72.34 71.98 72.91PA+MRK+DEF (1, 1, 1) 72.34 73.63 73.27 74.41Table 6: Head-Driven Three-Dimensional Extensions: Ac-curacy results for parsing the devest (section 0)Implementation Params FALL F?40 FALL F?40(h, v, d) WP WP WOP WOPPCFG (?, 0, 0) 65.08 67.31 65.82 68.22PCFG+PA+DEF (?, 1, 1) 72.26 74.46 72.42 74.52DIST (0, 0, 0) 66.33 68.79 67.06 69.47PA+MRK+DEF (1, 1, 1) 72.64 74.64 73.21 75.25Table 7: PCFG and Head-Driven Unlexicalized Models:Accuracy Results for parsing the testst (section 12)Figure 8 shows the FAll(WOP ) results for allmodels we implemented.
In general, we see that forparsing MH higher dimensionality is better.
More-over, we see that for all points on the (v, h, 0) plainthe corresponding models on the (v, h, 1) plain al-ways perform better.
We further see that the contri-bution of the depth dimension to a parent annotatedPCFG can compensate, to a large extent on the lackof head-dependency information.
These accumula-tive results, then, provide empirical evidence to theimportance of morphological and morpho-syntacticprocesses such as definiteness for syntactic analysisand disambiguation as argued for in section 2.We confirm our results on the testset and reportin table 7 our results on section 12 of the treebank.The performance has slightly increased and we ob-tain better results for our best strategy.
We retain thehigh error-reduction rate and propose our best result,75.25% for sentences of length ?
40, as an empiri-cally established string baseline on the performanceof treebank grammars for MH.6 Related WorkThe MH treebank (Sima?an et al, 2001), a mor-phologically and syntactically annotated corpus, hasbeen successfully used for various NLP tasks such asmorphological disambiguation, POS tagging (Bar-Haim et al, 2007) and NP chunking (Goldberg etal., 2006).
However its use for statistical parsing hasbeen more scarce and less successful.
The only pre-vious studies attempting to parse MH we know ofare (Sima?an et al, 2001), applying a variation of theDOP tree-gram model to 500 sentences, and (Tsar-faty, 2006), using a treebank PCFG in an integratedsystem for morphological and syntactic disambigua-tion.9 The adaptation of state-of-the-art parsingmodels to MH is not immediate as the flat variablestructures of phrases are hard to parse and a plen-tiful of morphological features that would facilitatedisambiguation are not exploited by currently avail-able parsers.
Also, the MH treebank is much smallerthan the ones for, e.g., English (Marcus et al, 1994)and Arabic (Maamouri and Bies, 2004), making ithard to apply data-intensive methods such as the all-subtrees approach (Bod, 1992) or full lexicalization(Collins, 2003).
Our best performing model incor-porates three dimensions of parametrization and ourbest result (75.25%) is similar to the one obtainedby the parser of (Bikel, 2004) for Modern StandardArabic (75%) using a fully lexicalized model anda training corpus about three times as large as ournewest MH treebank.This work has shown that devising an adequatebaseline for parsing MH requires more than sim-ple category-splits and sophisticated head-driven ex-tensions, and our results provide preliminary evi-dence for the variation in performance of differentparametrization strategies relative to the propertiesand structure of a given language.
The compari-son with parsing accuracy for MSA suggests thatparametrizing an orthogonal depth dimension maybe able to compensate, to some extent, on the lackof sister-dependencies, lexical information, and per-haps even the lack of annotated data, but establish-ing empirically its contribution to parsing MSA is amatter for further research.
In the future we intendto further investigate the significance of the depth di-mension by extending our models to include moremorphological features, more variation in the pa-9Both studies acheived between 60%?70% accuracy, how-ever the results are not comparable to our study because of theuse of different training sets, different annotation conventions,and different evaluation schemes.165Figure 7: All Models: Locating Unlexicalized Parsing Modelsin a Three-Dimensional Parametrization SpaceFigure 8: All Results: Parsing Results for Unlexicalized Mod-els in a Three-Dimensional Parametrization Spacerameter space, and applications to more languages.7 ConclusionMorphologically rich languages introduce a new di-mension into the expansion possibilities of a non-terminal node in a syntactic parse tree.
This di-mension is orthogonal to the vertical (Collins, 2003)and horizontal (Johnson, 1998) dimensions previ-ously outlined by Klein and Manning (2003), andit cannot be collapsed into any one of the previoustwo.
These additional dependencies exist alongsidethe syntactic head dependency and are attested usingmorphosyntactic phenomena such as long distanceagreement.
We demonstrate using syntactic defi-niteness in MH that incorporating morphologicallymarked features as a third, orthogonal dimensionfor annotating syntactic categories is invaluable forweakening the independence assumptions implicitin a treebank PCFG and increasing the model?s dis-ambiguation capabilities.
Using a three-dimensionalmodel we establish a new, stronger, lower bound onthe performance of unlexicalized parsing models forModern Hebrew, comparable to those achieved forother languages (Czech, Chinese, German and Ara-bic) with much larger corpora.Tuning the dimensions and value of the parame-ters for learning treebank grammars is largely an em-pirical matter, and we do not wish to claim here thata three-dimensional annotation strategy is the bestfor any given language.
Rather, we argue that fordifferent languages different optimal parametriza-tion strategies may apply.
MH is not a free-word-order language in the canonical sense, and our qual-itative analysis shows that all dimensions contributeto the models?
disambiguation capabilities.
Orthog-onal dimensions provide complementary informa-tion that is invaluable for the parsing process to theextent that the relevant linguistic phenomena licensegrammatical structures in the language.
Our resultspoint out a principled way to quantitatively charac-terizing differences between languages, thus guid-ing the selection of parameters for the developmentof annotated resources, custom parsers and cross-linguistic robust parsing engines.Acknowledgments We thank the KnowledgeCenter for Processing Hebrew and Dalia Bojan forproviding us with the newest version of the MHtreebank.
We are particularly grateful to the devel-opment team of version 2.0, Adi Mile?a and YuvalKrymolowsky, supervised by Yoad Winter for con-tinued collaboration and technical support.
We fur-ther thank Felix Hageloh for allowing us to use thesoftware resulting from his M.Sc.
thesis work.
Wealso like to thank Remko Scha, Jelle Zuidema, YoavSeginer and three anonymous reviewers for helpfulcomments on the text, and Noa Tsarfaty for techni-cal help in the graphical display.
The work of thefirst author is funded by the Netherlands Organiza-tion for Scientific Research (NWO), grant number017.001.271, for which we are grateful.166ReferencesS.
Abney.
1997.
Stochastic Attribute-Value Grammars.Computational Linguistics, 23 (4):597?618.R.
Bar-Haim, K. Sima?an, and Y.
Winter.
2007.
Part-of-Speech Tagging of Modern Hebrew Text.
Journal ofNatural Language Engineering.D.
Bikel and D. Chiang.
2000.
Two Statistical ParsingModels Applied to the Chinese Treebank.
In SecondChinese Language Processing Workshop, Hong Kong.D.
Bikel.
2004.
Intricacies of Collins?
Parsing Model.Computational Linguistics, 4(30).R.
Bod.
1992.
Data Oriented Parsing.
In Proceedings ofCOLING.E.
Charniak.
1996.
Tree-Bank Grammars.
InAAAI/IAAI, Vol.
2, pages 1031?1036.E.
Charniak.
1997.
Statistical Parsing with a Context-Free Grammar and Word Statistics.
In AAAI/IAAI,pages 598?603.M.
Collins, J. Hajic, L. Ramshaw, and C. Tillmann.
1999.A Statistical Parser for Czech.
In Proceedings of ACL,College Park, Maryland.M.
Collins.
2003.
Head-Driven Statistical Models forNatural Language Parsing.
Computational Linguis-tics, 29(4).G.
Danon.
2001.
Syntactic Definiteness in the Grammarof Modern Hebrew.
Linguistics, 6(39):1071?1116.A.
Dubey and F. Keller.
2003.
Probabilistic Parsing forGerman using Sister-Head Dependencies.
In Proceed-ings of ACL.Y.
Goldberg, M. Adler, and M. Elhadad.
2006.
NounPhrase Chunking in Hebrew: Influence of Lexical andMorphological Features.
In Proceedings of COLING-ACL.F.
Hageloh.
2007.
Parsing using Transforms over Tree-banks.
Master?s thesis, University of Amsterdam.M.
Johnson.
1998.
PCFG Models of LinguisticTree Representations.
Computational Linguistics,24(4):613?632.R.
Kaplan and J. Bresnan.
1982.
Lexical-FunctionalGrammar: A formal system for grammatical represen-tation.
In J. Bresnan, editor, The Mental Representa-tion of Grammatical Relations, Cambridge, MA.
TheMIT Press.D.
Klein and C. Manning.
2003.
Accurate UnlexicalizedParsing.
In Proceedings of ACL, pages 423?430.Y.
Krymolowski, Y. Adiel, N. Guthmann, S. Kenan,A.
Milea, N. Nativ, R. Tenzman, and P. Veisberg.2007.
Treebank Annotation Guide.
MILA, Knowl-edge Center for Hebrew Processing.M.
Maamouri and A. Bies.
2004.
Developing an Ara-bic Treebank: Methods, Guidelines, Procedures, andTools.
In Proceedings of COLING.M.
Marcus, G. Kim, M. Marcinkiewicz, R. MacIntyre,A.
Bies, M. Ferguson, K. Katz, and B. Schasberger.1994.
The Penn Treebank: Annotating Predicate-Argument Structure.T.
Matsuzaki, Y. Miyao, and J. Tsujii.
2005.
Probabilis-tic CFG with Latent Annotations.
In Proceedings ofACL?05.N.
Melnik.
2002.
Verb-Initial Constructions in ModernHebrew.
Ph.D. thesis, Berkeley University of Califor-nia.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning Accurate, Compact, and Interpretable TreeAnnotation.
In Proceedings of ACL-COLING, pages433?440, Sydney, Australia, July.D.
Prescher.
2005.
Head-Driven PCFGs with Latent-Head Statistics.
In In Proceedings of the InternationalWorkshop on Parsing Technologies.I.
A.
Sag, T. Wasow, and E. M. Bender.
2003.
SyntacticTheory: A Formal Introduction.
CSLI Publications,address, second edition.H.
Schmidt.
2004.
Efficient Parsing of Highly Ambigu-ous Context-Free Grammars with Bit Vectors.
In Pro-ceedings of COLING, Geneva, Switzerland.K.
Sima?an, A. Itai, Y.
Winter, A. Altman, and N. Nativ.2001.
Building a Tree-Bank of Modern Hebrew Text.In Traitment Automatique des Langues.R.
Tsarfaty.
2006.
Integrated Morphological and Syntac-tic Disambiguation for Modern Hebrew.
In Proceed-ing of SRW COLING-ACL.S.
Wintner.
2000.
Definiteness in the Hebrew NounPhrase.
Journal of Linguistics, 36:319?363.167
