Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 801?808,Sydney, July 2006. c?2006 Association for Computational LinguisticsSemantic Taxonomy Induction from Heterogenous EvidenceRion SnowComputer Science DepartmentStanford UniversityStanford, CA 94305rion@cs.stanford.eduDaniel JurafskyLinguistics DepartmentStanford UniversityStanford, CA 94305jurafsky@stanford.eduAndrew Y. NgComputer Science DepartmentStanford UniversityStanford, CA 94305ang@cs.stanford.eduAbstractWe propose a novel algorithm for inducing seman-tic taxonomies.
Previous algorithms for taxonomyinduction have typically focused on independentclassifiers for discovering new single relationshipsbased on hand-constructed or automatically discov-ered textual patterns.
By contrast, our algorithmflexibly incorporates evidence from multiple clas-sifiers over heterogenous relationships to optimizethe entire structure of the taxonomy, using knowl-edge of a word?s coordinate terms to help in deter-mining its hypernyms, and vice versa.
We apply ouralgorithm on the problem of sense-disambiguatednoun hyponym acquisition, where we combine thepredictions of hypernym and coordinate term clas-sifiers with the knowledge in a preexisting seman-tic taxonomy (WordNet 2.1).
We add 10, 000 novelsynsets to WordNet 2.1 at 84% precision, a rela-tive error reduction of 70% over a non-joint algo-rithm using the same component classifiers.
Fi-nally, we show that a taxonomy built using our al-gorithm shows a 23% relative F-score improvementover WordNet 2.1 on an independent testset of hy-pernym pairs.1 IntroductionThe goal of capturing structured relational knowl-edge about lexical terms has been the motivatingforce underlying many projects in lexical acquisi-tion, information extraction, and the constructionof semantic taxonomies.
Broad-coverage seman-tic taxonomies such as WordNet (Fellbaum, 1998)and CYC (Lenat, 1995) have been constructed byhand at great cost; while a crucial source of knowl-edge about the relations between words, these tax-onomies still suffer from sparse coverage.Many algorithms with the potential for auto-matically extending lexical resources have beenproposed, including work in lexical acquisition(Riloff and Shepherd, 1997; Roark and Charniak,1998) and in discovering instances, named enti-ties, and alternate glosses (Etzioni et al, 2005;Pasc?a, 2005).
Additionally, a wide variety ofrelationship-specific classifiers have been pro-posed, including pattern-based classifiers for hy-ponyms (Hearst, 1992), meronyms (Girju, 2003),synonyms (Lin et al, 2003), a variety of verb re-lations (Chklovski and Pantel, 2004), and generalpurpose analogy relations (Turney et al, 2003).Such classifiers use hand-written or automatically-induced patterns like Such NPy as NPx or NPylike NPx to determine, for example that NPy is ahyponym of NPx (i.e., NPy IS-A NPx).
Whilesuch classifiers have achieved some degree of suc-cess, they frequently lack the global knowledgenecessary to integrate their predictions into a com-plex taxonomy with multiple relations.Past work on semantic taxonomy induction in-cludes the noun hypernym hierarchy created in(Caraballo, 2001), the part-whole taxonomies in(Girju, 2003), and a great deal of recent work de-scribed in (Buitelaar et al, 2005).
Such work hastypically either focused on only inferring smalltaxonomies over a single relation, or as in (Cara-ballo, 2001), has used evidence for multiple rela-tions independently from one another, by for ex-ample first focusing strictly on inferring clustersof coordinate terms, and then by inferring hyper-nyms over those clusters.Another major shortfall in previous techniquesfor taxonomy induction has been the inability tohandle lexical ambiguity.
Previous approacheshave typically sidestepped the issue of polysemyaltogether by making the assumption of only a sin-gle sense per word, and inferring taxonomies ex-plicitly over words and not senses.
Enforcing afalse monosemy has the downside of making po-tentially erroneous inferences; for example, col-lapsing the polysemous term Bush into a singlesense might lead one to infer by transitivity thata rose bush is a kind of U.S. president.Our approach simultaneously provides a solu-tion to the problems of jointly considering evi-dence about multiple relationships as well as lexi-cal ambiguity within a single probabilistic frame-work.
The key contribution of this work is to offera solution to two crucial problems in taxonomy in-801duction and hyponym acquisition: the problem ofcombining heterogenous sources of evidence in aflexible way, and the problem of correctly identi-fying the appropriate word sense of each new wordadded to the taxonomy.12 A Probabilistic Framework forTaxonomy InductionIn section 2.1 we introduce our definitions for tax-onomies, relations, and the taxonomic constraintsthat enforce dependencies between relations; insection 2.2 we give a probabilistic model for defin-ing the conditional probability of a set of relationalevidence given a taxonomy; in section 2.3 we for-mulate a local search algorithm to find the taxon-omy maximizing this conditional probability; andin section 2.4 we extend our framework to dealwith lexical ambiguity.2.1 Taxonomies, Relations, and TaxonomicConstraintsWe define a taxonomy T as a set of pairwise re-lations R over some domain of objects DT.
Forexample, the relations in WordNet include hyper-nymy, holonymy, verb entailment, and many oth-ers; the objects of WordNet between which theserelations hold are its word senses or synsets.
Wedefine that each relation R ?
R is a set of orderedor unordered pairs of objects (i, j) ?
DT; we de-fine Rij ?
T if relationship R holds over objects(i, j) in T.Relations for Hyponym AcquisitionFor the case of hyponym acquisition, the ob-jects in our taxonomy are WordNet synsets.
Inthis paper we focus on two of the many possiblerelationships between senses: the hypernym rela-tion and the coordinate term relation.
We treat thehypernym or ISA relation as atomic; we use thenotation Hnij if a sense j is the n-th ancestor of asense i in the hypernym hierarchy.
We will sim-ply use Hij to indicate that j is an ancestor of iat some unspecified level.
Two senses are typi-cally considered to be ?coordinate terms?
or ?tax-onomic sisters?
if they share an immediate parentin the hypernym hierarchy.
We generalize this no-tion of siblinghood to state that two senses i andj are (m,n)-cousins if their closest least common1The taxonomies discussed in this paper are available fordownload at http://ai.stanford.edu/?rion/swn.subsumer (LCS)2 is within exactly m and n links,respectively.3 We use the notation Cmnij to denotethat i and j are (m,n)-cousins.
Thus coordinateterms are (1, 1)-cousins; technically the hypernymrelation may also be seen as a specific case of thisrepresentation; an immediate parent in the hyper-nym hierarchy is a (1, 0)-cousin, and the k-th an-cestor is a (k, 0)-cousin.Taxonomic ConstraintsA semantic taxonomy such as WordNet en-forces certain taxonomic constraints which disal-low particular taxonomies T. For example, theISA transitivity constraint in WordNet requiresthat each synset inherits the hypernyms of its hy-pernym, and the part-inheritance constraint re-quires that each synset inherits the meronyms ofits hypernyms.For the case of hyponym acquisition we enforcethe following two taxonomic constraints on thehypernym and (m,n)-cousin relations:1.
ISA Transitivity:Hmij ?Hnjk ?
Hm+nik .2.
Definition of (m,n)-cousinhood:Cmnij ?
?k.k = LCS(i, j) ?Hmik ?Hnjk.Constraint (1) requires that the each synset inheritsthe hypernyms of its direct hypernym; constraint(2) simply defines the (m,n)-cousin relation interms of the atomic hypernym relation.The addition of any new hypernym relation to apreexisting taxonomy will usually necessitate theaddition of a set of other novel relations as impliedby the taxonomic constraints.
We refer to the fullset of novel relations implied by a new link Rij asI(Rij); we discuss the efficient computation of theset of implied links for the purpose of hyponymacquisition in Section 3.4.2.2 A Probabilistic FormulationWe propose that the event Rij ?
T has someprior probability P (Rij ?
T), and P (Rij ?2A least common subsumer LCS(i, j) is defined as asynset that is an ancestor in the hypernym hierarchy of bothi and j which has no child that is also an ancestor of both iand j.
When there is more than one LCS (due to multipleinheritance), we refer to the closest LCS, i.e.,the LCS thatminimizes the maximum distance to i and j.3An (m,n)-cousin for m ?
2 corresponds to the Englishkinship relation ?
(m?1)-th cousin |m?n|-times removed.
?802T) + P (Rij 6?
T) = 1.
We define the probabilityof the taxonomy as a whole as the joint probabilityof its component relations; given a partition of allpossible relations R = {A,B} where A ?
T andB 6?
T, we define:P (T) = P (A ?
T, B 6?
T).We assume that we have some set of observed evi-dence E consisting of observed features over pairsof objects in some domain DE; we?ll begin withthe assumption that our features are over pairs ofwords, and that the objects in the taxonomy alsocorrespond directly to words.4 Given a set of fea-tures ERij ?
E, we assume we have some modelfor inferring P (Rij ?
T|ERij), i.e., the posteriorprobability of the event Rij ?
T given the corre-sponding evidence ERij for that relation.
For exam-ple, evidence for the hypernym relation EHij mightbe the set of all observed lexico-syntactic patternscontaining i and j in all sentences in some corpus.For simplicity we make the following indepen-dence assumptions: first, we assume that eachitem of observed evidence ERij is independent ofall other observed evidence given the taxonomyT,i.e., P (E|T) = ?ERij?E P (ERij |T).Further, we assume that each item of observedevidence ERij depends on the taxonomy T only byway of the corresponding relation Rij , i.e.,P (ERij |T) ={ P (ERij |Rij ?
T) if Rij ?
TP (ERij |Rij 6?
T) if Rij 6?
TFor example, if our evidence EHij is a set of ob-served lexico-syntactic patterns indicative of hy-pernymy between two words i and j, we assumethat whatever dependence the relations in T haveon our observations may be explained entirely bydependence on the existence or non-existence ofthe single hypernym relation H(i, j).Applying these two independence assumptionswe may express the conditional probability of ourevidence given the taxonomy:P (E|T) =?Rij?TP (ERij |Rij ?
T)?
?Rij 6?TP (ERij |Rij 6?
T).Rewriting the conditional probability in termsof our estimates of the posterior probabilities4In section 2.4 we drop this assumption, extending ourmodel to manage lexical ambiguity.P (Rij |ERij) using Bayes Rule, we obtain:P (E|T) =?Rij?TP (Rij ?
T|ERij)P (ERij)P (Rij ?
T)?
?Rij 6?TP (Rij 6?
T|ERij)P (ERij)P (Rij 6?
T) .Within our model we define the goal of taxon-omy induction to be to find the taxonomy T?
thatmaximizes the conditional probability of our ob-servations E given the relationships of T, i.e., tofindT?
= argmaxTP (E|T).2.3 Local Search Over TaxonomiesWe propose a search algorithm for finding T?
forthe case of hyponym acquisition.
We assume webegin with some initial (possibly empty) taxon-omy T. We restrict our consideration of possiblenew taxonomies to those created by the single op-eration ADD-RELATION(Rij ,T), which adds thesingle relation Rij to T.We define the multiplicative change ?T(Rij)to the conditional probability P (E|T) given theaddition of a single relation Rij :?T(Rij) = P (E|T?
)/P (E|T)= P (Rij ?
T|ERij)P (ERij)P (Rij 6?
T|ERij)P (ERij)?
P (Rij 6?
T)P (Rij ?
T)= k??
P(Rij ?
T|ERij)1?
P(Rij ?
T|ERij)??
.Here k is the inverse odds of the prior on the eventRij ?
T; we consider this to be a constant inde-pendent of i, j, and the taxonomy T.To enforce the taxonomic constraints in T, foreach application of the ADD-RELATION operatorwe must add all new relations in the implied setI(Rij) not already in T.5 Thus we define the mul-tiplicative change of the full set of implied rela-tions as the product over all new relations:?T(I(Rij)) =?R?I(Rij)?T(R).5For example, in order to add the new synsetmicrosoft under the noun synset company#n#1in WordNet 2.1, we must necessarily add thenew relations H2(microsoft, institution#n#1)C11(microsoft, dotcom#n#1), and so on.803This definition leads to the following best-firstsearch algorithm for hyponym acquisition, whichat each iteration defines the new taxonomy as theunion of the previous taxonomy T and the set ofnovel relations implied by the relation Rij thatmaximizes ?T(I(Rij)) and thus maximizes theconditional probability of the evidence over allpossible single relations:WHILE maxRij 6?T?T(I(Rij)) > 1T ?
T ?
I(arg maxRij 6?T?T(I(Rij))).2.4 Extending the Model to Manage LexicalAmbiguitySince word senses are not directly observable, ifthe objects in the taxonomy are word senses (as inWordNet), we must extend our model to allow fora many-to-many mapping (e.g., a word-to-sensemapping) between DE and DT.
For this settingwe assume we know the function senses(i), map-ping from the word i to all of i?s possible corre-sponding senses.We assume that each set of word-pair evidenceERij we possess is in fact sense-pair evidence ERklfor a specific pair of senses k0 ?
senses(i), l0 ?senses(j).
Further, we assume that a new relationbetween two words is probable only between thecorrect sense pair, i.e.
:P (Rkl|ERij) = 1{k = k0, l = l0} ?
P (Rij |ERij).When computing the conditional probability of aspecific new relation Rkl ?
I(Rab), we assumethat the relevant sense pair k0, l0 is the one whichmaximizes the probability of the new relation, i.e.for k ?
senses(i), l ?
senses(j),(k0, l0) = argmaxk,l P (Rkl ?
T|ERij).Our independence assumptions for this exten-sion need only to be changed slightly; we now as-sume that the evidence ERij depends on the taxon-omy T via only a single relation between sense-pairs Rkl.
Using this revised independence as-sumption the derivation for best-first search overtaxonomies for hyponym acquisition remains un-changed.
One side effect of this revised indepen-dence assumption is that the addition of the single?sense-collapsed?
relation Rkl in the taxonomy Twill explain the evidence ERij for the relation overwords i and j now that such evidence has been re-vealed to concern only the specific senses k and l.3 Extending WordNetWe demonstrate the ability of our model to useevidence from multiple relations to extend Word-Net with novel noun hyponyms.
While in prin-ciple we could use any number of relations, forsimplicity we consider two primary sources of ev-idence: the probability of two words in WordNetbeing in a hypernym relation, and the probabilityof two words in WordNet being in a coordinate re-lation.In sections 3.1 and 3.2 we describe the construc-tion of our hypernym and coordinate classifiers,respectively; in section 3.3 we outline the efficientalgorithm we use to perform local search overhyponym-extended WordNets; and in section 3.4we give an example of the implicit structure-basedword sense disambiguation performed within ourframework.3.1 Hyponym ClassificationOur classifier for the hypernym relation is derivedfrom the ?hypernym-only?
classifier described in(Snow et al, 2005).
The features used for pre-dicting the hypernym relationship are obtained byparsing a large corpus of newswire and encyclo-pedia text with MINIPAR (Lin, 1998).
From theresulting dependency trees the evidence EHij foreach word pair (i, j) is constructed; the evidencetakes the form of a vector of counts of occurrencesthat each labeled syntactic dependency path wasfound as the shortest path connecting i and j insome dependency tree.
The labeled training set isconstructed by labeling the collected feature vec-tors as positive ?known hypernym?
or negative?known non-hypernym?
examples using WordNet2.0; 49,922 feature vectors were labeled as pos-itive training examples, and 800,828 noun pairswere labeled as negative training examples.
Themodel for predicting P (Hij |EHij ) is then trainedusing logistic regression, predicting the noun-pairhypernymy label from WordNet from the featurevector of lexico-syntactic patterns.The hypernym classifier described above pre-dicts the probability of the generalized hypernym-ancestor relation over words P (Hij |EHij ).
Forthe purposes of taxonomy induction, we wouldprefer an ancestor-distance specific set of clas-sifiers over senses, i.e., for k ?
senses(i), l ?senses(j), the set of classifiers estimating{P (H1kl|EHij ), P (H2kl|EHij ), .
.
.
}.804One problem that arises from directly assign-ing the probability P (Hnij |EHij ) ?
P (Hij |EHij ) forall n is the possibility of adding a novel hyponymto an overly-specific hypernym, which might stillsatisfy P (Hnij |EHij ) for a very large n. In or-der to discourage unnecessary overspecification,we penalize each probability P (Hkij |EHij ) by afactor ?k?1 for some ?
< 1, and renormalize:P (Hkij |EHij ) ?
?k?1P (Hij |EHij ).
In our experi-ments we set ?
= 0.95.3.2 (m,n)-cousin ClassificationThe classifier for learning coordinate terms relieson the notion of distributional similarity, i.e., theidea that two words with similar meanings will beused in similar contexts (Hindle, 1990).
We ex-tend this notion to suggest that words with similarmeanings should be near each other in a seman-tic taxonomy, and in particular will likely share ahypernym as a near parent.Our classifier for (m,n)-cousins is derivedfrom the algorithm and corpus given in (Ravichan-dran et al, 2005).
In that work an efficient ran-domized algorithm is derived for computing clus-ters of similar nouns.
We use a set of more than1000 distinct clusters of English nouns collectedby their algorithm over 70 million webpages6,with each noun i having a score representing itscosine similarity to the centroid c of the cluster towhich it belongs, cos(?
(i, c)).We use the cluster scores of noun pairs as inputto our own algorithm for predicting the (m,n)-cousin relationship between the senses of twowords i and j.
If two words i and j appear ina cluster together, with cluster centroid c, we setour single coordinate input feature to be the mini-mum cluster score min(cos(?
(i, c)), cos(?
(j, c))),and zero otherwise.
For each such noun pair fea-ture, we construct a labeled training set of (m,n)-cousin relation labels from WordNet 2.1.
We de-fine a noun pair (i, j) to be a ?known (m,n)-cousin?
if for some senses k ?
senses(i), l ?senses(j), Cmnij ?
WordNet; if more than onesuch relation exists, we assume the relation withsmallest sum m + n, breaking ties by smallestabsolute difference |m ?
n|.
We consider allsuch labeled relationships from WordNet with 0 ?m,n ?
7; pairs of words that have no correspond-ing pair of synsets connected in the hypernym hi-6As a preprocessing step we hand-edit the clusters to re-move those containing non-English words, terms related toadult content, and other webpage-specific clusters.erarchy, or with min(m,n) > 7, are assigned toa single class C?.
Further, due to the symme-try of the similarity score, we merge each classCmn = Cmn ?
Cnm; this implies that the result-ing classifier will predict, as expected given a sym-metric input, P (Cmnkl |ECij ) = P (Cnmkl |ECij ).We find 333,473 noun synset pairs in our train-ing set with similarity score greater than 0.15.
Wenext apply softmax regression to learn a classifierthat predicts P (Cmnij |ECij ), predicting the Word-Net class labels from the single similarity scorederived from the noun pair?s cluster similarity.3.3 Details of our ImplementationHyponym acquisition is among the simplest andmost straightforward of the possible applicationsof our model; here we show how we efficientlyimplement our algorithm for this problem.
First,we identify the set of all the word pairs (i, j) overwhich we have hypernym and/or coordinate ev-idence, and which might represent additions ofa novel hyponym to the WordNet 2.1 taxonomy(i.e., that has a known noun hypernym and an un-known hyponym, or has a known noun coordi-nate term and an unknown coordinate term).
Thisyields a list of 95,000 single links over thresholdP (Rij) > 0.12.For each unknown hyponym i we may haveseveral pieces of evidence; for example, for theunknown term continental we have 21 relevantpieces of hypernym evidence, with links to possi-ble hypernyms {carrier, airline, unit, .
.
.
}; and wehave 5 pieces of coordinate evidence, with links topossible coordinate terms {airline, american ea-gle, airbus, .
.
.
}.For each proposed hypernym or coordinate linkinvolved with the novel hyponym i, we computethe set of candidate hypernyms for i; in practicewe consider all senses of the immediate hypernymj for each potential novel hypernym, and all sensesof the coordinate term k and its first two hypernymancestors for each potential coordinate.In the continental example, from the 26 individ-ual pieces of evidence over words we construct theset of 99 unique synsets that we will consider aspossible hypernyms; these include the two sensesof the word airline, the ten senses of the word car-rier, and so forth.Next, we iterate through each of the possi-ble hypernym synsets l under which we mightadd the new word i; for each synset l we com-805pute the change in taxonomy score resulting fromadding the implied relations I(H1il) required bythe taxonomic constraints of T. Since typicallyour set of all evidence involving i will be muchsmaller than the set of possible relations in I(H1il),we may efficiently check whether, for each senses ?
senses(w), for all words where we havesome evidence ERiw, whether s participates insome relation with i in the set of implied rela-tions I(H1il).7 If there is more than one senses ?
senses(w), we add to I(H1il) the single re-lationship Ris that maximizes the taxonomy like-lihood, i.e.
argmaxs?senses(w) ?T(Ris).3.4 Hypernym Sense DisambiguationA major strength of our model is its ability to cor-rectly choose the sense of a hypernym to whichto add a novel hyponym, despite collecting ev-idence over untagged word pairs.
In our algo-rithm word sense disambiguation is an implicitside-effect of our algorithm; since our algorithmchooses to add the single link which, with its im-plied links, yields the most likely taxonomy, andsince each distinct synset in WordNet has a differ-ent immediate neighborhood of relations, our al-gorithm simply disambiguates each node based onits surrounding structural information.As an example of sense disambiguation in prac-tice, consider our example of continental.
Sup-pose we are iterating through each of the 99 pos-sible synsets under which we might add conti-nental as a hyponym, and we come to the synsetairline#n#2 in WordNet 2.1, i.e.
?a commer-cial organization serving as a common carrier.
?In this case we will iterate through each pieceof hypernym and coordinate evidence; we findthat the relation H(continental, carrier) is satis-fied with high probability for the specific synsetcarrier#n#5, the grandparent of airline#n#2; thusthe factor ?T(H3(continental, carrier#n#5)) isincluded in the factor of the set of implied rela-tions ?T(I(H1(continental, airline#n#2))).Suppose we instead evaluate the first synsetof airline, i.e., airline#n#1, with the gloss ?ahose that carries air under pressure.?
For thissynset none of the other 20 relationships di-rectly implied by hypernym evidence or the5 relationships implied by the coordinate ev-7Checking whether or not Ris ?
I(H1il) may be effi-ciently computed by checking whether s is in the hypernymancestors of l or if it shares a least common subsumer with lwithin 7 steps.idence are implied by adding the single linkH1(continental,airline#n#1); thus the resultingchange in the set of implied links given by the cor-rect ?carrier?
sense of airline is much higher thanthat of the ?hose?
sense.
In fact it is the largest ofall the 99 considered hypernym links for continen-tal; H1(continental, airline#n#2) is link #18,736added to the taxonomy by our algorithm.4 EvaluationIn order to evaluate our framework for taxonomyinduction, we have applied hyponym acquisitionto construct several distinct taxonomies, startingwith the base of WordNet 2.1 and only addingnovel noun hyponyms.
Further, we have con-structed taxonomies using a baseline algorithm,which uses the identical hypernym and coordinateclassifiers used in our joint algorithm, but whichdoes not combine the evidence of the classifiers.In section 4.1 we describe our evaluationmethodology; in sections 4.2 and 4.3 we analyzethe fine-grained precision and disambiguation pre-cision of our algorithm compared to the baseline;in section 4.4 we compare the coarse-grained pre-cision of our links (motivated by categories de-fined by the WordNet supersenses) against thebaseline algorithm and against an ?oracle?
fornamed entity recognition.Finally, in section 4.5 we evaluate the tax-onomies inferred by our algorithm directly againstthe WordNet 2.1 taxonomy; we perform this eval-uation by testing each taxonomy on a set of humanjudgments of hypernym and non-hypernym nounpairs sampled from newswire text.4.1 MethodologyWe evaluate the quality of our acquired hy-ponyms by direct judgment.
In four sep-arate annotation sessions, two judges labeled{50,100,100,100} samples uniformly generatedfrom the first {100,1000,10000,20000} singlelinks added by our algorithm.For the direct measure of fine-grained precision,we simply ask for each link H(X,Y ) added by thesystem, is X a Y ?
In addition to the fine-grainedprecision, we give a coarse-grained evaluation, in-spired by the idea of supersense-tagging in (Cia-ramita and Johnson, 2003).
The 26 supersensesused in WordNet 2.1 are listed in Table 1; we labela hyponym link as correct in the coarse-grainedevaluation if the novel hyponym is placed underthe appropriate supersense.
This evaluation task8061 Tops 8 communication 15 object 22 relation2 act 9 event 16 person 23 shape3 animal 10 feeling 17 phenomenon 24 state4 artifact 11 food 18 plant 25 substance5 attribute 12 group 19 possession 26 time6 body 13 location 20 process7 cognition 14 motive 21 quantityTable 1: The 26 WordNet supersensesis similar to a fine-grained Named Entity Recog-nition (Fleischman and Hovy, 2002) task with 26categories; for example, if our algorithm mistak-enly inserts a novel non-capital city under the hy-ponym state capital, it will inherit the correct su-persense location.
Finally, we evaluate the abil-ity of our algorithm to correctly choose the ap-propriate sense of the hypernym under which anovel hyponym is being added.
Our labelers cate-gorize each candidate sense-disambiguated hyper-nym synset suggested by our algorithm into thefollowing categories:c1: Correct sense-disambiguated hypernym.c2: Correct hypernym word, but incorrect sense ofthat word.c3: Incorrect hypernym, but correct supersense.c4: Any other relation is considered incorrect.A single hyponym/hypernym pair is allowed to besimultaneously labeled 2 and 3.4.2 Fine-grained evaluationTable 2 displays the results of our evaluation offine-grained precision for the baseline non-jointalgorithm (Base) and our joint algorithm (Joint),as well as the relative error reduction (ER) of ouralgorithm over the baseline.
We use the mini-mum of the two judges?
scores.
Here we definefine-grained precision as c1/total.
We see thatour joint algorithm strongly outperforms the base-line, and has high precision for predicting novelhyponyms up to 10,000 links.4.3 Hypernym sense disambiguationAlso in Table 2 we compare the sense dis-ambiguation precision of our algorithm and thebaseline.
Here we measure the precision ofsense-disambiguation among all examples whereeach algorithm found a correct hyponym word;our calculation for disambiguation precision isc1/ (c1 + c2).
Again our joint algorithm outper-forms the baseline algorithm at all levels of re-call.
Interestingly the baseline disambiguationprecision improves with higher recall; this mayFine-grained Pre.
Disambiguation Pre.#Links Base Joint ER Base Joint ER100 0.60 1.00 100% 0.86 1.00 100%1000 0.52 0.93 85% 0.84 1.00 100%10000 0.46 0.84 70% 0.90 1.00 100%20000 0.46 0.68 41% 0.94 0.98 68%Table 2: Fine-grained and disambiguation preci-sion and error reduction for hyponym acquisition# Links NER Base Joint ER vs. ER vs.Oracle NER Base100 1.00 0.72 1.00 0% 100%1000 0.69 0.68 0.99 97% 85%10000 0.45 0.69 0.96 93% 70%20000 0.54 0.69 0.92 83% 41%Table 3: Coarse-grained precision and error reduc-tion vs. Non-joint baseline and NER Oraclebe attributed to the observation that the highest-confidence hypernyms predicted by individualclassifiers are likely to be polysemous, whereashypernyms of lower confidence are more fre-quently monosemous (and thus trivially easy todisambiguate).4.4 Coarse-grained evaluationWe compute coarse-grained precision as (c1 +c3)/total.
Inferring the correct coarse-grained su-persense of a novel hyponym can be viewed as afine-grained (26-category) Named Entity Recog-nition task; our algorithm for taxonomy inductioncan thus be viewed as performing high-accuracyfine-grained NER.
Here we compare against boththe baseline non-joint algorithm as well as an?oracle?
algorithm for Named Entity Recogni-tion, which perfectly classifies the supersense ofall nouns that fall under the four supersenses{person, group, location, quantity}, but worksonly for those supersenses.
Table 3 shows theresults of this coarse-grained evaluation.
We seethat the baseline non-joint algorithm has higherprecision than the NER oracle as 10,000 and20,000 links; however, both are significantly out-performed by our joint algorithm, which main-tains high coarse-grained precision (92%) even at20,000 links.4.5 Comparison of inferred taxonomies andWordNetFor our final evaluation we compare our learnedtaxonomies directly against the currently exist-ing hypernym links in WordNet 2.1.
In order tocompare taxonomies we use a hand-labeled test807WN +10K +20K +30K +40KPRE 0.524 0.524 0.574 0.583 0.571REC 0.165 0.165 0.203 0.211 0.211F 0.251 0.251 0.300 0.309 0.307Table 4: Taxonomy hypernym classification vs.WordNet 2.1 on hand-labeled testsetset of over 5,000 noun pairs, randomly-sampledfrom newswire corpora (described in (Snow et al,2005)).
We measured the performance of both ourinferred taxonomies and WordNet against this testset.8 The performance and comparison of the bestWordNet classifier vs. our taxonomies is given inTable 4.
Our best-performing inferred taxonomyon this test set is achieved after adding 30,000novel hyponyms, achieving an 23% relative im-provement in F-score over the WN2.1 classifier.5 ConclusionsWe have presented an algorithm for inducing se-mantic taxonomies which attempts to globallyoptimize the entire structure of the taxonomy.Our probabilistic architecture also includes a newmodel for learning coordinate terms based on(m,n)-cousin classification.
The model?s abilityto integrate heterogeneous evidence from differentclassifiers offers a solution to the key problem ofchoosing the correct word sense to which to attacha new hypernym.AcknowledgementsThanks to Christiane Fellbaum, Rajat Raina, BillMacCartney, and Allison Buckley for useful dis-cussions and assistance annotating data.
RionSnow is supported by an NDSEG Fellowshipsponsored by the DOD and AFOSR.
This workwas supported in part by the Disruptive Technol-ogy Office (DTO)?s Advanced Question Answer-ing for Intelligence (AQUAINT) Program.ReferencesP.
Buitelaar, P. Cimiano and B. Magnini.
2005.
Ontol-ogy Learning from Text: Methods, Evaluation andApplications.
Volume 123 Frontiers in Artificial In-telligence and Applications.S.
Caraballo.
2001.
Automatic Acquisition ofa Hypernym-Labeled Noun Hierarchy from Text.Brown University Ph.D. Thesis.8We found that the WordNet 2.1 model achieving thehighest F-score used only the first sense of each hyponym,and allowed a maximum distance of 4 edges between eachhyponym and its hypernym.S.
Cederberg and D. Widdows.
2003.
Using LSA andNoun Coordination Information to Improve the Pre-cision and Recall of Automatic Hyponymy Extrac-tion.
Proc.
CoNLL-2003, pp.
111?118.T.
Chklovski and P. Pantel.
2004.
VerbOcean: Miningthe Web for Fine-Grained Semantic Verb Relations.Proc.
EMNLP-2004.M.
Ciaramita and M. Johnson.
2003.
SupersenseTagging of Unknown Nouns in WordNet.
Proc.EMNLP-2003.O.
Etzioni, M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.2005.
Unsupervised Named-Entity Extraction fromthe Web: An Experimental Study.
Artificial Intelli-gence, 165(1):91?134.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
Cambridge, MA: MIT Press.R.
Girju, A. Badulescu, and D. Moldovan.
2003.Learning Semantic Constraints for the AutomaticDiscovery of Part-Whole Relations.
Proc.
HLT-03.M.
Fleischman and E. Hovy.
2002.
Fine grained clas-sification of named entities.
Proc.
COLING-02.M.
Hearst.
1992.
Automatic Acquisition of Hyponymsfrom Large Text Corpora.
Proc.
COLING-92.D.
Hindle.
1990.
Noun classification from predicate-argument structures.
Proc.
ACL-90.D.
Lenat.
1995.
CYC: A Large-Scale Investment inKnowledge Infrastructure, Communications of theACM, 38:11, 33?35.D.
Lin.
1998.
Dependency-based Evaluation of MINI-PAR.
Workshop on the Evaluation of Parsing Sys-tems, Granada, Spain.D.
Lin, S. Zhao, L. Qin and M. Zhou.
2003.
Iden-tifying Synonyms among Distributionally SimilarWords.
Proc.
IJCAI-03.M.
Pasc?a.
2005.
Finding Instance Names and Alter-native Glosses on the Web: WordNet Reloaded.
CI-CLing 2005, pp.
280-292.D.
Ravichandran, P. Pantel, and E. Hovy.
2002.
Ran-domized Algorithms and NLP: Using Locality Sen-sitive Hash Function for High Speed Noun Cluster-ing.
Proc.
ACL-2002.E.
Riloff and J. Shepherd.
1997.
A Corpus-BasedApproach for Building Semantic Lexicons.
ProcEMNLP-1997.B.
Roark and E. Charniak.
1998.
Noun-phrase co-occurerence statistics for semi-automatic-semanticlexicon construction.
Proc.
ACL-1998.R.
Snow, D. Jurafsky, and A. Y. Ng.
2005.
Learn-ing syntactic patterns for automatic hypernym dis-covery.
NIPS 2005.P.
Turney, M. Littman, J. Bigham, and V. Shnay-der.
2003.
Combining independent modules tosolve multiple-choice synonym and analogy prob-lems.
Proc.
RANLP-2003, pp.
482?489.808
