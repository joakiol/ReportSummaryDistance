Generic Sentence Fusion is an Ill-Defined Summarization TaskHal Daume?
III and Daniel MarcuInformation Sciences InstituteUniversity of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292{hdaume,marcu}@isi.eduAbstractWe report on a series of human evaluations of thetask of sentence fusion.
In this task, a human isgiven two sentences and asked to produce a singlecoherent sentence that contains only the importantinformation from the original two.
Thus, this is ahighly constrained summarization task.
Our inves-tigations show that even at this restricted level, thereis no measurable agreement between humans re-garding what information should be considered im-portant.
We further investigate the ability of sepa-rate evaluators to assess summaries, and find simi-larly disturbing lack of agreement.1 Introduction and MotivationThe practices of automatic summarization varywidely across many dimensions, including sourcelength, summary length, style, source, topic, lan-guage, and structure.
Most typical are summariesof a single news document down to a headline orshort summary, or of a collection of news docu-ments down to a headline or short summary (Hahnand Harman, 2002).
A few researchers have focusedon other aspects of summarization, including sin-gle sentence (Knight and Marcu, 2002), paragraphor short document (Daume?
III and Marcu, 2002),query-focused (Berger and Mittal, 2000), or speech(Hori et al, 2003).The techniques relevant to, and the challengesfaced in each of these tasks can be quite different.Nevertheless, they all rely on one critical assump-tion: there exists a notion of (relative) importancebetween pieces of information in a document (or ut-terance), regardless of whether we can detect thisor not.
Indeed, recent research has looked at thisquestion in detail, and can be rather cleanly dividedinto two partitions.
The first partition aims to de-velop manual evaluation criteria for determining thequality of a summary, and is typified by the exten-sive research done in single-document summariza-tion by Halteren and Teufel (2003) and by the evalu-ation strategy proposed by Nenkova and Passonneau(2004).
The other half aims to develop automaticevaluation criteria to imitate the manual evaluationmethods (or at least to complement them).
Work inthis area includes that of Lin and Hovy (2003) andPastra and Saggion (2003), both of whom inspectthe use of Bleu-like metrics (Papineni et al, 2002)in summarization.The results of these investigations have beenmixed.
In the DUC competitions (Hahn and Har-man, 2002), when manual evaluation has been em-ployed, it has been commonly observed that human-written summaries grossly outscore any machine-produced summary.
All machine-produced sum-maries tend to show little (statistically significant)difference from one another.
Moreover, a baselinesystem that simply takes the first sentences of a doc-ument performs just as well or better than intelli-gently crafted systems when summarizing news sto-ries.
Additionally, studies of vast numbers of sum-maries of the same document (Halteren and Teufel,2003) have shown that there is little agreementamong different humans as to what information be-longs in a single document summary.
This has beenleveraged by Nenkova and Passonneau (2004) toproduce a manual scoring method for summaries,though the fact that humans show so little agree-ment in this task is somewhat disheartening.
All ofthese evaluations rely strongly on the issue of mul-tiple references, in order to achieve consensus.Opinions voiced at DUC meetings indicate thatdifferent researchers attribute this apparent lack ofagreement to one (or more) of many factors (in ad-dition, see (Mani and Maybury, 1999)).
Many be-lieve that the fact that we are typically working in anews genre is to blame, though this complaint tendsto be directed more at the excellent performance ofthe baseline than at the issue of human agreement.Others believe that in order to observe more agree-ment, one needs to move to query-focused sum-maries; it seems reasonable that if the person writ-ing the summary knew how it would be used, hewould be more guided in what information to re-tain.
Yet others attribute the lack of agreement sim-Connecting Point has become the single largest Mac retailer after tripling it ?s Macintosh sales since January 1989 .Connecting Point Systems tripled it ?s sales of Apple Macintosh systems since last January .
It is now the single largest seller of Macintosh .Figure 1: Example ?document, abstract?
alignment.ply to the vast space of possible choices a summa-rizer could make, and see the disagreement simplyas par for the course.2 Our StudyIn this paper, we report on a study of the perfor-mance of humans producing summaries.
We con-cern ourselves with the task of sentence fusion.
Inthis task, we assume that two sentences are providedand that the summarizer must produce as output asingle sentence that contains the important informa-tion contained in the input sentences (we will de-scribe later how we obtain such data).
We wouldlike to show that this task is well-defined: if weshow many humans the same two sentences, theywill produce similar summaries.
Of course we donot penalize one human for using different wordsthan another.The sentence fusion task is interesting after per-forming sentence extraction, the extracted sen-tences often contain superfluous information.
Ithas been further observed that simply compress-ing sentences individually and concatenating the re-sults leads to suboptimal summaries (Daume?
III andMarcu, 2002).
The use of sentence fusion in multi-document summarization has been extensively ex-plored by Barzilay in her thesis (Barzilay, 2003;Barzilay et al, 1999), though in the multi-documentsetting, one has redundancy to fall back on.
Addi-tionally, the sentence fusion task is sufficiently con-strained that it makes possible more complex andlinguistically motivated manipulations than are rea-sonable for full document or multi-document sum-maries (and for which simple extraction techniquesare unlikely to suffice).3 Data CollectionOur data comes from a collection of computer prod-uct reviews from the Ziff-Davis corporation.
Thiscorpus consists of roughly seven thousand docu-ments paired with human written abstracts.
The av-erage document was 1080 words in length, with anabstract of length 136 words, a compression rate ofroughly 87.5%.3.1 Examples Based on AlignmentsFor 50 of these ?document, abstract?
pairs, wehave human-created word-for-word and phrase-for-phrase alignments.
An example alignment is shownin Figure 1.
Moreover, using a generalization of ahidden Markov model, we are able to create (in anunsupervised fashion) similar alignments for all ofthe documents (Daume?
III and Marcu, 2004).
Thissystem achieves a precision, recall and f-score of0.528, 0.668 and 0.590, respectively (which is a sig-nificant increase in performance (f = 0.407) overthe IBM models or the Cut & Paste method (Jing,2002)).Based on these alignments (be they manually cre-ated or automatically created), we are able to lookfor examples of sentence fusions within the data.In particular, we search for sentences in the ab-stracts which are aligned to exactly two documentsentences, for which at least 80% of the summarysentence is aligned and for which at least 20% ofthe words in the summary sentence come from eachof the two document sentences.This leaves us with pairs that consist of two doc-ument sentences and one abstract sentence, exactlythe sort of data we are looking to use.
We randomlyselect 25 such pairs from the data collected from thehuman-aligned portion of the corpus and 25 pairsfrom the automatically aligned portion, giving us 50pairs in all.3.2 Examples Based on ElicitationIn addition to collecting data from the Ziff-Daviscorpus, we also elicited data from human subjectswith a variety of different backgrounds (thoughall were familiar with computers and technology).These people were presented with the pairs of docu-ment sentences and, independently of the rest of thedocument, asked to produce a single summary sen-tence that contained the ?important?
information.Their summary was to be about half the length ofthe original (this is what was observed in the pairsextracted from the corpus) They were given no ad-ditional specific instructions.The summaries thus elicited ranged rather dra-matically from highly cut and paste summaries tohighly abstractive summaries.
An example is shownin Table 1.
In this table, we show the original pair ofORIG: After years of pursuing separate and conflicting paths, AT&T and Digital Equipment Corp. agreedin June to settle their computer-to-PBX differences.The two will jointly develop an applications interface that can be shared by computers and PBXs ofany stripe.REF: AT&T and DEC have a joint agreement from June to develop an applications interface to be sharedby various models of computers and PBXs.HUM 1: AT&T and Digital Equipment Corp. agreed in June to settle their computer-to-PBX differences anddevelop an applications interface that can be shared by any computer or PBX.HUM 2: After years of pursuing different paths, AT&T and Digital agreed to jointly develop an applicationsinterface that can be shared by computers and PBXs of any stripe.HUM 3: After working separately for years, AT&T will jointly develop an interface between computers andPBXs.Table 1: Example of elicited data.document sentences, the ?reference?
summary (i.e.,the one that came from the original abstract), andthe responses of three of the eight human subjectsare shown (the first is the most ?cut and paste,?
thesecond is typical of the ?middle set?
and the last isunusually abstractive).3.3 Baseline SummariesIn addition to the human elicited data, we gener-ate three baseline summaries.
The first baseline,LONGER, simply selects the longer of the two sen-tences as the summary (typically the sentences areroughly the same length; thus this is nearly random).The second baseline, DROPSTOP first catenates thesentences (in random order), then removes punctu-ation and stop words, finally cutting off at the 50%mark.
The third baseline, COMP is the documentcompression system developed by Daume?
III andMarcu (2002), which compresses documents by cut-ting out constituents in a combined syntax and dis-course tree.4 Evaluation of SummariesWe perform three types of manual evaluation on thesummaries from the previous section.
In the first,the ranked evaluation, we present evaluators withoriginal two document sentences; they also see alist of hypothesis summaries and are asked to rankthem relative to one another.
In the second evalu-ation, the absolute evaluation, evaluators are pre-sented with the reference summary and a hypothe-sis and are asked to produce an absolute score forthe hypothesis.
In the third, the factoid evaluation,we manually inspect the information content of eachhypothesis.4.1 Ranked EvaluationIn the ranked evaluation, human evaluators are pre-sented with the original two document sentences.They also see a list of 12 hypothesis summaries:the reference summary, the eight summaries elicitedfrom human subjects, and the three baseline sum-maries.
They are asked to produce a ranking of the12 summaries based both on their faithfulness to theoriginal document sentences and on their grammat-icality.
They were allowed to assign the same scoreto two systems if they felt neither was any better (orworse) than the other.
They ranked the systems from1 (best) to 12 (worst), though typically enough sys-tems performed ?equally well?
that a rank of 12 wasnot assigned.
Three humans performed this evalua-tion.4.2 Absolute EvaluationIn the absolute evaluation, human evaluators areshown the reference summary and a single hypoth-esis summary.
In order to partially assuage the is-sue of humans doing little more than string match-ing (Coughlin, 2001), the reference and hypothe-sis were shown on separate pages and humans wereasked not to go ?back?
during the evaluation.
Due totime constraints, only three systems were evaluatedin this manner, one of the humans (the human out-put was selected so that it was neither too cut-and-paste nor too generative), the LONGER and COMPsystems.
Three humans performed this task (eachshown a single different system output for each ref-erence summary) and scored outputs on a scale from1 (best) to 5 (worst).
They were told to deduct pointsfor any information contained in the reference notcontained in the hypothesis, any information con-tained in the hypothesis not contained in the refer-ence, and ungrammaticality.4.3 Factoid EvaluationThe third evaluation we perform ourselves, due toits difficulty.
This follows the general rubric de-scribed by Nenkova and Passonneau?s (2004) pyra-mid scoring scheme, though it differs in the sensethat we base our evaluation not on a reference sum-mary, but on the original two document sentences.Our methodology is described below.REF LONGER COMP HUM 1 HUM 2 HUM 3 FactoidF F F F F CP has taken leadershipF F leadership by volumedoug kass is analysis at dataquest incdq is a market research codq is in san josekass said CP has taken leadershipF F F F F analysts sayF F F F F F CP has a wide variety of storesF F F F F F CP endorsed apple?s earned investment programF F F F CP has become the low-price leaderF F F F CP hasn?t sacrificed technical supportTable 2: Factoid-based evaluation scheme for the sentence pair ?Connecting Point has taken leadership by volume,volume, volume,?
said Doug Kass, an analyst at Dataquest Inc., a market research company in San Jose.
Analysts andobservers say Connecting Point?s wide variety of stores and endorsement of Apple?s earned investment program havehelped it become the low-price leader without sacrificing technical support.
?We assume that we are given the original pairof sentences from the document and the hypothesissummaries for many systems (in our experiments,we used the original reference summary, the outputsof three representative humans, and the LONGERand COMP baselines).
Given this data, we first seg-ment the original pair of sentences into ?factoids?in the style of Halteren and Teufel (2003).
Then, foreach hypothesis summary and each factoid, we in-dicate whether the summary contained that factoid.Grammaticality of summary hypotheses entersinto the calculation of the factoid agreement num-bers.
A system only gets credit for a factoid ifits summary contains that factoid in a sufficientlygrammatical form that the following test could bepassed: given any reasonable question one couldpose about this factoid, and given the hypothesissummary, could one answer the question correctly.An example is shown in Table 2.Based on this information, it is possible to se-lect one or more of the outputs as the ?gold stan-dard?
and compare the rest in the pyramid scor-ing scheme described by Nenkova and Passonneau(2004).
If only one output is used as the gold stan-dard, then it is sufficient to compute precision andrecall against that gold standard, and then use thesenumbers to compute an F-score, which essentiallymeasures agreement between the chosen gold stan-dard and another hypothesis.
In the remainder ofthis analysis, when we report an F-score over thefactoid, this is calculated when the REF summary istaken as the standard.5 Evaluation ResultsThe fundamental question we would like to answeris whether humans agree in terms of what informa-tion should be preserved in a summary.
Given ourdata, there are two ways of looking at this.
First:HUM 1 HUM 2 HUM 3REF 0.182 0.188 0.251HUM 1 - 0.201 0.347HUM 2 - - 0.470Table 3: Agreement (kappa) scores for differentcombinations of systems and humansdo the humans from whom we elicited data selectthe same information as the reference?
Second: dothese humans agree with each other.
Both of thesequestions can be answered by looking at the resultsof the factoid evaluation.For any set of columns in the factoid evaluation,we can compute the agreement based on the kappastatistic (Krippendorff, 1980).
Researchers have ob-served that kappa scores over 0.8 indicate strongagreement, while scores between 0.6 and 0.8 indi-cate reasonable agreement.
Kappa values below 0.6indicate little to no agreement.
The kappa valuesfor various combinations of columns are shown inTable 3.As we can see from this table, there is essen-tially no agreement found anywhere.
The maximumagreement is between HUMAN 2 and HUMAN 3, buteven a kappa value of 0.470 is regarded as virtuallyno agreement.
Furthermore, the kappa values com-paring the human outputs to the reference outputs iseven lower, attaining a maximum of 0.251; again,no agreement.
One is forced to conclude that in thetask of generic sentence fusion, people will not pro-duce a summary containing the same informationas the original reference sentence, and will not pro-duce summaries that contain the same informationas another person in the same situation.Despite the fact that humans do not agree onwhat information should go into a summary, there isstill the chance that when presented with two sum-System F-Score Absolute RelativeHUM 4 0.652 2.605 2.066HUM 3 0.608 - 2.276HUM 5 0.574 - 2.434LONGER 0.419 3.000 3.368REF 1.000 - 3.500COMP 0.475 3.842 4.184Table 4: Factoid F-score, absolute score and relativeranking for 6 outputsmaries, they will be able to distinguish one as some-how better than another.
Answering this question isthe aim of the other two evaluations.First, we consider the absolute rankings.
Recallthat in this evaluation, humans are presented withthe reference summary as the gold standard sum-mary.
Since, in addition to grammaticality, this issupposed to measure the correctness of informationpreservation, it is reasonable to compare these num-bers to the F-scores that can be computed based onthe factoid evaluation.
These results are shown inTable 4.
For the first column (F-Score), higher num-bers are better; for the second and third columns,lower scores are better.
We can see that the evalua-tion prefers the human output to the outputs of eitherof the systems.
However, the factoid scoring prefersthe COMP model to the LONGER model, though theAbsolute scoring rates them in the opposite direc-tion.As we can see from the Relative column in Ta-ble 4, human elicited summaries are consistentlypreferred to any of the others.
This is good news:even if people cannot agree on what informationshould go into a summary, they at least prefer hu-man written summaries to others.
After the hu-man elicited summaries, there is a relatively largejump to the LONGER baseline, which is unfortu-nately preferred to the REFERENCE summary.
Afterthe reference summary, there are two large jumps,first to the document compression model and thento the DROPSTOP baseline.
However, when com-paring the relative scores to the F-Score, we see that,again, the factoid metric prefers the COMP model tothe LONGER model, but this is not reflected in therelative scoring metric.6 Analysis of ResultsThere are two conclusions that can be drawn fromthese data.
The first, related specifically to thekappa statistic over the factoids as depicted in Ta-ble 3, is that even in this modest task of compress-ing two sentences into one, the task is ill-defined.The second, related to the two other evaluations, isthat while humans seem able to agree on the rela-tive quality of sentence fusions, judgments elicitedby direct comparison do not reflect whether systemsare correctly able to select content.6.1 Disagreement of ImportanceAs indicated in Section 5, when humans are giventhe task of compressing two sentences into one,there is no measurable agreement between any twoas to what information should be retained.The first thing worth noting is that there is mod-erately more agreement between two elicited, non-expert data points than between the elicited data andthe original reference.
This can be attributed eitherto the lack of context available to the non-experts,or to their respective lack of expertise.
Regardless,the level of agreement between such non-expert hu-mans is so low that this matters little.
Furthermore,from an automatic sentence fusion perspective, acomputer program is much more like a non-experthuman with no context than an expert with an entiredocument to borrow from.It might be argued that looking at only two sen-tences does not provide sufficient context for hu-mans to be able to judge relative importance.
Thisargument is supported by the fact that, upon mov-ing to multi-document summarization, there is (rel-atively) more agreement between humans regardingwhat pieces of information should be kept.
In or-der to make the transition from two-sentence fusionto multi-document summarization, one essentiallyneeds to make two inductive steps: the first fromtwo sentences, to three and so on up to a full sin-gle document; the second from a single documentto multiple documents.The analysis we have performed does not com-ment on either of these inductive steps.
However,it is much more likely that it is the second, notthe first, that breaks down and enables humans toagree more when creating summaries of collectionsof documents.
On the one hand, it seems unrea-sonable to posit that there is some ?magic?
num-ber of sentences needed, such that once two humansread that many sentences, they are able to agree onwhat information is relevant.
On the other hand, inall evaluations that have considered multi-documentsummarization, the collection of documents to besummarized has been selected by a human with aparticular interest in mind.
While this interest is not(necessarily) communicated to the summarizers di-rectly, it is indirectly suggested by the selection ofdocuments.
This is why the use of redundancy inmulti-document summarization is so important.
If,on the other hand, humans were given a set of mod-erately related or unrelated documents, we believethat there would be even less agreement on whatmakes a good summary1.6.2 Human Perception of QualityWe have presented two sets of results regarding hu-man perception of the quality of summaries.
In thefirst (see Table 4), humans are presented with theREF summary and then with either a human-elicitedsummary, a summary that is simply the longer of thetwo sentences (recall that they do not see the origi-nal two sentences, so they have no way of knowinghow this summary was created) and the output ofthe COMP system.
If one accepts that the F-Scoreover factoids is a high-quality measure of summaryquality, then there should be strong correlation be-tween this F-Score and the absolute scoring of thesystem outputs.
This is not observed.
In fact, theF-Score strongly prefers the COMP system over theLONGER system, while human scoring prefers theLONGER system.Since the humans performing this evaluationwere told explicitly to count off for missing infor-mation, extraneous information or lack of grammat-icality, the only reasonable explanation for this dis-crepancy is that the evaluators were sufficiently putoff by the grammatical errors made by the COMPsystem that they penalized it heavily.
Grammatical-ity does enter into the factoids evaluation, thoughperhaps not as strongly.In the relative ranking evaluation (see Table 4),there are two disturbing observations we can make.First, as in the absolute scoring, the factoid evalua-tion prefers the COMP system to the LONGER sys-tem, but the relative ranking puts them in the otherorder.
Second, the LONGER baseline outperformsthe reference summary.As before, we can explain this first discrepancyby the issue of grammaticality.
This is especiallyimportant in this case: since the evaluators are notgiven a reference summary that explicitly tells themwhat information is important and what informationis not, they are required to make this decision ontheir own.
As we have observed, this act is veryimprecise, and it is likely the people performingthe evaluation have recognized this.
Since there isno longer a clear cut distinction between importantand unimportant information, and since they are re-quired to make a decision, they have no choice butto fall back on grammaticality as the primary moti-vating factor for their decisions.1Summarizing a set of unrelated documents may be an un-realistic and unimportant task; nevertheless, it is interesting toconsider such a task in order to better understand why humansagree more readily in multi-document summarization than insingle document summarization or in sentence fusion.The second discrepancy is particularly disturb-ing.
Before discussing its possible causes, webriefly consider the implications of this finding.
Inorder to build an automatic sentence fusion sys-tem, one would like to be able to automatically col-lect training data.
Our method for doing so is byconstructing word-for-word and phrase-for-phrasealignments between documents and abstracts andleveraging these alignments to select such pairs.In theory, one could extract many thousands ofsuch examples from the plethora of existing docu-ment/summary pairs available.
Unfortunately, thisresult tells us that even if we are able to build asystem that perfectly mimics these collected data,a simple baseline will be preferred by humans in anevaluation.One might wish to attribute this discrepancy to er-rors made by the largely imperfect automatic align-ments.
However, we have calculated the results sep-arately for pairs derived from human alignments andfrom automatic alignments, and observe no differ-ences.This leaves two remaining factors to explain thisdifference.
First, the original summary is createdby a trained human professional, who is very famil-iar with the domain (while our elicited data comesfrom technologically proficient adults, the topicsdiscussed in the data are typically about technicalsystems from the late eighties, topics our summa-rizers know very little about).
Second, the originalsummarizers had the rest of the document availablewhen creating these fusions.
Though without per-forming relevant experiments, it is impossible to saywhat the results would be.However, from a system-building perspective,one can view fusion in many applications and itis highly desirable to be able to perform such fu-sions without knowing the rest of the document.From a document summarization perspective, onemight wish to perform sentence extraction to re-duce the document to a few sentences and then usesentence fusion to compress these further.
In thiscase, the primary motivation for performing this ina pipelined fashion would be to remove the com-plexity of dealing with the entire document whenthe more complex fusion models are applied.
Inanother possible application of question answering,one can imagine answering a question by fusion to-gether several sentences returned as the result of aninformation retrieval engine.
In this case, it is nearlyimpossible to include the remainder of the docu-ments in such an analysis.7 Summary and ConclusionsWe have performed an analysis of agreement be-tween humans in the highly constrained task of fus-ing two sentences together.
This task has appli-cations in summarization, question answering andpure natural language generation.
We have shownthat this task is not well defined, when viewed inisolation.
Furthermore, we have shown that us-ing automatically extracted data for training cannotlead to systems that outperform a simple baseline ofchoosing the longer of the two sentences..These results are disheartening, though by per-forming such experiments a priori, we are able tobetter judge which courses of research are and arenot worth pursuing.
Questions regarding the agree-ment between people in the area of single docu-ment summarization and multi-document summa-rization have already been raised and are currentlyonly partially answered (Halteren and Teufel, 2003;Nenkova and Passonneau, 2004; Marcu and Ger-ber, 2001).
We have shown that even in this con-strained domain, it is very unlikely that any signif-icant agreement will be found, without specificallyguiding the summarizers, either by a query, a usermodel, or some other external knowledge.
We haveargued that it is likely that this lack of agreementwill not be subverted by adding more sentences,though this should be confirmed experimentally.The issues of multiple references and of addingcontext (essentially by allowing the summarizers tosee the document from which these two sentenceswere extracted) has not been addressed in this work;either might serve to increase agreement.
However,one of the goals of this methodology for automat-ically extracting pairs of sentences from automat-ically aligned corpora is to be able to get data onwhich to train and test a system without having hu-mans write it.
To require one to elicit multiple ref-erences to obtain any agreement obviates this goal(moreover, that agreement between humans and theoriginal summary sentence is even lower than be-tween a pair of humans makes this practice ques-tionable).
Regarding context, it is reasonable to hy-pothesize (though this would need to be verified)that the addition of context would result in higherkappa scores.
Unfortunately, if a human is givenaccess to this information, it would only be fair togive a system access to the same information.
Thismeans that we would no longer be able to viewgeneric sentence fusion as an isolated task, makingfusion-specific research advances very difficult.8 AcknowledgementsWe wish to thank Kevin Knight, Eduard Hovy, JerryHobbs and the anonymous reviewers for their help-ful and insightful comments.
This work was par-tially supported by DARPA-ITO grant N66001-00-1-9814, NSF grant IIS-0097846, and a USC DeanFellowship to Hal Daume?
III.ReferencesR.
Barzilay, K. McKeown, and M. Elhadad.
1999.Information fusion in the context of multi-document summarization.
In Proceedings ofACL.R.
Barzilay.
2003.
Information Fusion for Mut-lidocument Summarization: Paraphrasing andGeneration.
Ph.D. thesis, Columbia University.A.
Berger and V. Mittal.
2000.
Query-relevant sum-marization using FAQs.
In Proceedings of ACL.D.
Coughlin.
2001.
Correlating automated and hu-man assessments of machine translation quality.In Proceedings of MT Summit IX.H.
Daume?
III and D. Marcu.
2002.
A noisy-channelmodel for document compression.
In Proceed-ings of ACL.H.
Daume?
III and D. Marcu.
2004.
A phrase-basedHMM approach to document/abstract alignment.In preparation.U.
Hahn and D. Harman, editors.
2002.
SecondDocument Understanding Conference (DUC-2002).H.
Halteren and S. Teufel.
2003.
Examining theconsensus between human summaries: Initial ex-periments with factoid analysis.
In HLT-NAACLDUC Workshop.C.
Hori, S. Furui, R. Malkin, H. Yu, and A. Waibel.2003.
A statistical approach to automatic speechsummarization.
Journal on Applied Signal Pro-cessing, 3:128?139.H.
Jing.
2002.
Using hidden Markov modeling todecompose human-written summaries.
Compu-tational Linguistics, 28(4):527 ?
544, December.K.
Knight and D. Marcu.
2002.
Summarizationbeyond sentence extraction: A probabilistic ap-proach to sentence compression.
Artificial Intel-ligence.K.
Krippendorff.
1980.
Content analysis: An Intro-duction to its Methodology.
Sage Publications,CA.C.Y.
Lin and E. Hovy.
2003.
Automatic evaluationof summaries using n-gram co-occurrence statis-tics.
In Proceedings of HLT-NAACL.C.Y.
Lin.
2003.
Improving summarization perfor-mance by sentence compression - a pilot study.In Proceedings of IRAL Workshop.I.
Mani and M. Maybury, editors.
1999.
Ad-vances in Automatic Text Summarization.
TheMIT Press, Cambridge, MA.D.
Marcu and L. Gerber.
2001.
An inquiry intothe nature of multidocument abstracts, extracts,and their evaluation.
In NAACL SummarizationWorkshop.A.
Nenkova and R. Passonneau.
2004.
Evaluatingcontent selection in summarization: The pyramidmethod.
In Proceedings of HLT-NAACL.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.2002.
Bleu: a method for automatic evaluationof machine translation.
In Proceedings of ACL.K.
Pastra and H. Saggion.
2003.
Colouring sum-maries BLEU.
In EACL.
