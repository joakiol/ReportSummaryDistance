Proceedings of the 2009 Workshop on Language Generation and Summarisation, ACL-IJCNLP 2009, pages 63?66,Suntec, Singapore, 6 August 2009.c?2009 ACL and AFNLPReducing redundancy in multi-document summarizationusing lexical semantic similarityIris Hendrickx, Walter DaelemansUniversity of AntwerpAntwerpen, Belgiumiris.hendrickx@ua.ac.bewalter.daelemans@ua.ac.beErwin Marsi, Emiel KrahmerTilburg UniversityTilburg, The Netherlandse.j.krahmer@uvt.nle.c.marsi@uvt.nlAbstractWe present an automatic multi-documentsummarization system for Dutch based onthe MEAD system.
We focus on redun-dancy detection, an essential ingredient ofmulti-document summarization.
We in-troduce a semantic overlap detection tool,which goes beyond simple string match-ing.
Our results so far do not confirmour expectation that this tool would out-perform the other tested methods.1 IntroductionOne of the main issues in automatic multi-document summarization is avoiding redundancy.As the source documents are all related to thesame topic, at least some of their content is likelyto overlap.
In fact, this is in part what makesmulti-document summarization feasible.
For ex-ample, news articles that report on a particularevent, or that are based on the same source, oftencontain similar information expressed in differ-ent ways.
A multi-document summarizer shouldinclude this overlapping information not morethan once.
The backbone of most current ap-proaches to automatic summarization is a vectorspace model in which a sentence is regarded asa bag of words and a weighted cosine similaritymeasure is used to quantify the amount of sharedinformation between a pair of sentences.
Cosinesimilarity (in this context) essentially amounts tocalculating word overlap, albeit with weighting ofthe terms and normalization for differences in sen-tence length.
It is clear that this approach to detect-ing redundancy is far from satisfactory, because itonly covers redundancy in its most trivial form,i.e., identical words.
In contrast, the redundancythat we ultimately want to avoid in summarizationis that at the semantic level.
As an extreme casein point, two sentences with no words in commoncan still carry virtually the same meaning.The remainder of this paper is structured inthe following way.
In Section 2 we introduce atool for detecting semantic overlap.
In section 3we present a Dutch multi-document summariza-tion system, based on the MEAD summarizationtoolkit (Radev et al, 2004).
Next, in section 4 wedescribe the experimental setup and the data setthat we used.
Section 5 reports on the results, andwe conclude in section 6.2 Detecting semantic overlapIn this section, we detail the semantic overlap de-tection tool and the resources we build on.Parallel/comparable text corpus The basis forour semantic overlap detection tool is a mono-lingual parallel/comparable tree-bank of 1 millionwords of Dutch text (Marsi and Krahmer, 2007).Half of the text material has so far been manuallyaligned at the sentence level.
Subsequently, thesentences have been parsed and the resulting parsetrees have been aligned at the level of syntacticnodes.
Moreover, aligned nodes have been labeledaccording to a set of semantic similarity labels thatexpress the type of similarity relation between thenodes.
The following five labels are used: gen-eralize, specify, intersect, restate, and equal.
Thecorpus serves as the basis for developing tools forautomatic alignment and relation labeling.Word aligner The word alignment tool takes asinput a pair of source and target sentences andproduces a matching between the words, that is,a (possibly partial) one-to-one mapping of sourceto target words.
This aligner is a part of the fullfledged tree aligner currently under development.The alignment task comprises several subtasks.First, the input sentences are tokenized and parsedwith the Alpino syntactic parser for Dutch (Boumaet al, 2001).
Apart from the syntactic analysis,which we disregard in the current work, the parser63performs lemmatization, part-of-speech taggingand compound analysis, all of which are used here.In addition, the aligner uses lexical-semanticknowledge from Cornetto, a lexical database forDutch (40K entries) similar to the well-known En-glish WordNet (Vossen et al, 2008).
The rela-tions we use are synonym, hyperonym, and xpos-near-synonym (align near synonyms with differ-ent POS labels).
In addition we check whethera pair of content words has a least common sub-sumer (LCS) in the hyperonym hierarchy.
As pathlength has been shown to be a poor predictor inthis respect, we calculate the Lin similarity, whichcombines the Information Content of the words in-volved (Lin, 1998).
A current limitation is thatwe lack word sense disambiguation, hence we takethe maximal score over all the senses of the words.The components described above can be con-sidered as experts which predict word alignmentswith a certain probability.
Since alignments cansupport, complement or contradict each other, weare faced with the problem of how to combinethe evidence.
Our approach is to view the align-ment as a weighted bipartite multigraph.
That is,a graph where source and target nodes are in dis-joint sets, multiple edges are allowed between thesame pair of nodes, and edges have an associatedweight.
Our goal is on the one hand to maximizethe sum of the edge weights, and on the other handto reduce this graph to a model in which everynode can have at most one associated edge.
Thisis a combinatorial optimization problem known asthe assignment problem for which efficient algo-rithms exist.
We use a variant of the The Hungar-ian Algorithm1(Kuhn, 1955), for the computationof the matches.Sentence similarity score Given a word align-ment between a pair of sentences, a similarityscore is required to measure the amount of se-mantic overlap or redundancy.
Evidently the sim-ilarity score should be proportional to the relativenumber of aligned words.
However, some align-ments are more important than others.
For exam-ple, the alignment between two determiners (e.g.the) is less significant than that between two com-mon nouns.
This is modeled in our similarity scoreby weighting alignments according to the idf (in-verse document frequency) (Sp?arck Jones, 1972)of the words involved.1Also known as the Munkres algorithmsim(s1, s2) =?wi?Aidf(wi)?wj?Sidf(wj)(1)Here s1and s2are sentences, S is the longest ofthe two sentences, wjare the words in S, A is thesubsequence of aligned words in S, and wiare thewords in A.3 Multi-document summarizationThe Dutch Multi-Document Summarizer pre-sented here is based on the MEAD summariza-tion toolkit (Radev et al, 2004), which offers awide range of summarization algorithms and has aflexible structure.
The system creates a summaryby extracting a subset of sentences from the orig-inal documents.
The summarizer reads in a clus-ter of documents, i.e.
a set of documents relevantfor the same topic, and for each sentence it ex-tracts a set of features.
These features are com-bined to determine an importance score for eachsentence.
Next the sentences are sorted accord-ing to their importance score.
The system starts asummary by adding the sentence with the highestweight.
Then it examines the second most impor-tant sentence and measures the similarity with thesentence that is already added.
If the overlap islimited, the sentence is added to the summary, oth-erwise it is disregarded.
This process is repeateduntil the intended summary size is reached.
Themodule that performs this last step of determiningwhich sentences end up in the final summary iscalled the reranker.We use two baseline systems: the random base-line system randomly selects a set of sentencesand the lead-based system which selects a sub-set of initial sentences as summary.
We investi-gated the following features.
A simple and effec-tive features is the position: each sentence gets ascore of 1/position where ?position?
is the placein the document.
The length feature is a filter thatremoves sentences shorter than the given thresh-old.
The simwf feature presents the overlap of asentence with the title of the document computedwith cosine similarity.
One of MEAD?s main fea-tures is centroid-based summarization.
Centroidsof clusters are used to determine which wordsare important for the cluster and sentences con-taining these words are considered to be centralsentences.
The words are weighted with tf*idf.64The aim of query-based summarization is to cre-ate summaries that are relevant with respect to aparticular query.
This can easily be done with fea-tures that express the overlap between the queryand a source sentence.
We examined three differ-ent query-based features that measure simple wordoverlap between the query and the sentence, co-sine similarity with tf*idf weighting of words andcosine similarity without tf*idf weighting.The MEAD toolkit implements multiplereranker modules, we investigated the followingthree: the cosine-reranker, the mmr-reranker andnovelty-reranker.
We compare these rerankersagainst the semantic overlap detection (sod)tool detailed in section 2.
The cosine-rerankerrepresents two sentences as tf*idf weighted wordvectors and computes a cosine similarity scorebetween them.
Sentences with a cosine similarityabove the threshold are disregarded.
The mmr-reranker module is based on the maximal marginrelevance criterion (Carbonell and Goldstein,1998).
MMR models the trade-off between afocused summary and a summary with a widescope.
The novelty-reranker is an extension of thecosine-reranker and boosts sentences occurringafter an important sentence by multiplying with1.2.
The reranker tries to mimic human behavioras people tend to pick clusters of sentences whensummarizing.4 Experimental setupTo perform proper evaluation of the summariza-tion system we constructed a new data set for eval-uating Dutch multi-document summarization.
Itconsists of 30 query-based document clusters.
Thedocument clusters were created manually follow-ing the guidelines of DUC 2006 (Dang, 2006).Each cluster contains a query description and 5 to25 newspaper articles relevant for that particularquestion.
For each cluster five annotators wrotean abstract of approximately 250 words.
Thesesummaries serve as a gold standard for compari-son with automatically generated extracts.We split our data set in a test set of 20 clus-ters and a development set of 10 clusters.
We usethe development set for parameter tuning and fea-ture selection for the summarizer.
We try out eachof the characteristics discussed in section 3.
Thebest combination found on the development set isthe feature combination position, centroid, lengthwith cut-off 13, and queryCosine.
We tested thedifferent rerankers and vary the similarity thresh-olds to determine their optimal threshold value.
Asthe novelty-reranker scored lower than the otherrerankers on the development set, we did not in-clude it in our experiments on the test set.For the experiments on the development set, wecompare each of the automatically produced ex-tracts with five manually written summaries andreport macro-average Rouge-2 and Rouge-SU4scores (Lin and Hovy, 2003).
For the experimentson the test set, we also perform a manual evalu-ation.
We follow the DUC 2006 guidelines formanual evaluation of responsiveness and the lin-guistic quality of the produced summaries.
The re-sponsiveness scores express the information con-tent of the summary with respect to the query.
Thelinguistic quality is evaluated on five different ob-jectives: grammaticality, non-redundancy, coher-ence, referential clarity and focus.
The annotatorscan choose a value on a five point scale where1 means ?very poor?
and 5 means ?very good?.We use two independent annotators to evaluate thesummaries and we report the average scores.5 ResultsThe evaluation of the results on the test set areshown in table 1.
The Rouge scores of the differentrerankers are all above both baselines, and they arevery close to each other.
The scores for the contentmeasure and responsiveness show that the valuesfor the automatic summaries are between 2 (poor)and 3 (barely acceptable).
The optimized summa-rizers score higher than the two baselines on thispoint.We are most interested in the aspect of ?non-redundancy?.
The random baseline systemachieves a good result here, and the optimizedsummarizers all score lower.
The chance of over-lap between randomly selected sentences seemsto be lower than when an automatic summarizertries to select only the most important sentences.When we compare the three optimized systemswith different rerankers on this aspect we see thatthe scores are very close.
Our semantic overlap de-tection (sod) reranker does not do any better thanthe other two.
The optimized summarizers do per-form better than the baseline systems with respectto focus and structure.65setting Rouge-2 Rouge-SU4 gram redun ref focus struct responsrand baseline 0.101 0.153 4.08 3.9 2.58 2.6 2 2.25lead baseline 0.139 0.179 3.05 3.6 3.25 2.88 2.38 2.4optim-cosine 0.152 0.193 3.9 3.18 2.65 3.15 2.43 2.75optim-mmr 0.149 0.191 3.98 3.13 2.55 3.13 2.38 2.7optim-sod 0.150 0.193 4.05 3.13 2.85 3.23 2.5 2.7Table 1: Macro-average Rouge scores and manual evaluation on the test set on these aspects:grammaticality, non-redundancy, referential clarity, focus, structure and responsiveness.6 Discussion and conclusionWe presented an automatic multi-document sum-marization system for Dutch based on the MEADsystem, supporting the claim that MEAD is largelylanguage-independent.
We experimented with dif-ferent features and parameter settings of the sum-marizer, and optimized it for summarization ofDutch newspaper text.
We presented a semanticoverlap detection tool, developed on the basis of amonolingual corpus of parallel/comparable Dutchtext, which goes beyond simple string matching.We expected this tool to improve the sentencereranking step, thereby reducing redundancy in thesummaries.
However, we were unable to show asignificant effect.
We have several possible expla-nations for this.
First, many of the sentence pairsthat share the same semantic content, also share anumber of identical words.
To detect these cases,therefore, computing cosine similarity may be justas effective.
Second, the accuracy of the align-ment tool may not be good enough, partly becauseof errors in the linguistic analysis or lack of cover-age, and partly because certain types of knowledge(word sense, syntactic structure) are not yet ex-ploited.
Third, reranking of sentences is unlikelyto improve the summary in cases where the pre-ceding step of sentence ranking within documentsperforms poorly.
We are currently still investigat-ing this matter and hope to obtain significant re-sults with an improved version of our tool for de-tecting semantic overlap.We plan to work on a more refined version thatnot only uses word alignment but also considersalignments at the parse tree level.
This idea isin line with the work of Barzilay and McKeown(2005) who use this type of technique to fuse sim-ilar sentences for multi-document summarization.Acknowledgements This work was conducted within theDAESO http://daeso.uvt.nl project funded by theStevin program (De Nederlandse Taalunie).
The constructionof the evaluation corpus described in this paper was financedby KP BOF 2008, University of Antwerp.
We would like tothank NIST for kindly sharing their DUC 2006 guidelines.ReferencesRegina Barzilay and Kathleen R. McKeown.
2005.
Sentencefusion for multidocument news summarization.
Compu-tational Linguistics, 31(3):297?328.Gosse Bouma, Gertjan van Noord, and Robert Malouf.
2001.Alpino: Wide-coverage computational analysis of Dutch.In Computational Linguistics in the Netherlands 2000.,pages 45?59.
Rodopi, Amsterdam, New York.Jaime Carbonell and Jade Goldstein.
1998.
The use ofmmr, diversity-based reranking for reordering documentsand producing summaries.
In Proceedings of SIGIR 1998,pages 335?336, New York, NY, USA.
ACM.H.T.
Dang.
2006.
Overview of DUC 2006.
In Proceedingsof the Document Understanding Workshop, pages 1?10,Brooklyn, USA.Harold W. Kuhn.
1955.
The Hungarian Method for the as-signment problem.
Naval Research Logistics Quarterly,2:83?97.C.-Y.
Lin and E.H. Hovy.
2003.
Automatic evaluationof summaries using n-gram co-occurrence statistics.
InProceedings of HLT-NAACL, pages 71 ?
78, Edmonton,Canada.D.
Lin.
1998.
An information-theoretic definition of similar-ity.
In Proceedings of the ICML, pages 296?304.Erwin Marsi and Emiel Krahmer.
2007.
Annotating a par-allel monolingual treebank with semantic similarity re-lations.
In Proceedings of the 6th International Work-shop on Treebanks and Linguistic Theories, pages 85?96,Bergen, Norway.Dragomir Radev et al 2004.
Mead - a platform for multidoc-ument multilingual text summarization.
In Proceedings ofLREC 2004, Lisabon, Portugal.Karen Sp?arck Jones.
1972.
A statistical interpretation ofterm specificity and its application in retrieval.
Journalof Documentation, 28(1):11?21.P.
Vossen, I. Maks, R. Segers, and H. van der Vliet.
2008.Integrating lexical units, synsets and ontology in the Cor-netto Database.
In Proceedings of LREC 2008, Mar-rakech, Morocco.66
