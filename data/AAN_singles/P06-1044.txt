Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 345?352,Sydney, July 2006. c?2006 Association for Computational LinguisticsAutomatic Classification of Verbs in Biomedical TextsAnna KorhonenUniversity of CambridgeComputer Laboratory15 JJ Thomson AvenueCambridge CB3 0GD, UKalk23@cl.cam.ac.ukYuval KrymolowskiDept.
of Computer ScienceTechnionHaifa 32000Israelyuvalkr@cs.technion.ac.ilNigel CollierNational Institute of InformaticsHitotsubashi 2-1-2Chiyoda-ku, Tokyo 101-8430Japancollier@nii.ac.jpAbstractLexical classes, when tailored to the appli-cation and domain in question, can providean effective means to deal with a num-ber of natural language processing (NLP)tasks.
While manual construction of suchclasses is difficult, recent research showsthat it is possible to automatically induceverb classes from cross-domain corporawith promising accuracy.
We report anovel experiment where similar technol-ogy is applied to the important, challeng-ing domain of biomedicine.
We show thatthe resulting classification, acquired froma corpus of biomedical journal articles,is highly accurate and strongly domain-specific.
It can be used to aid BIO-NLPdirectly or as useful material for investi-gating the syntax and semantics of verbsin biomedical texts.1 IntroductionLexical classes which capture the close relationbetween the syntax and semantics of verbs haveattracted considerable interest in NLP (Jackendoff,1990; Levin, 1993; Dorr, 1997; Prescher et al,2000).
Such classes are useful for their ability tocapture generalizations about a range of linguis-tic properties.
For example, verbs which share themeaning of ?manner of motion?
(such as travel,run, walk), behave similarly also in terms ofsubcategorization (I traveled/ran/walked, I trav-eled/ran/walked to London, I traveled/ran/walkedfive miles).
Although the correspondence betweenthe syntax and semantics of words is not perfectand the classes do not provide means for full se-mantic inferencing, their predictive power is nev-ertheless considerable.NLP systems can benefit from lexical classesin many ways.
Such classes define the mappingfrom surface realization of arguments to predicate-argument structure, and are therefore an impor-tant component of any system which needs thelatter.
As the classes can capture higher levelabstractions they can be used as a means to ab-stract away from individual words when required.They are also helpful in many operational contextswhere lexical information must be acquired fromsmall application-specific corpora.
Their predic-tive power can help compensate for lack of datafully exemplifying the behavior of relevant words.Lexical verb classes have been used to sup-port various (multilingual) tasks, such as compu-tational lexicography, language generation, ma-chine translation, word sense disambiguation, se-mantic role labeling, and subcategorization acqui-sition (Dorr, 1997; Prescher et al, 2000; Korho-nen, 2002).
However, large-scale exploitation ofthe classes in real-world or domain-sensitive taskshas not been possible because the existing classi-fications, e.g.
(Levin, 1993), are incomprehensiveand unsuitable for specific domains.While manual classification of large numbers ofwords has proved difficult and time-consuming,recent research shows that it is possible to auto-matically induce lexical classes from corpus datawith promising accuracy (Merlo and Stevenson,2001; Brew and Schulte im Walde, 2002; Ko-rhonen et al, 2003).
A number of ML methodshave been applied to classify words using featurespertaining to mainly syntactic structure (e.g.
sta-tistical distributions of subcategorization frames(SCFs) or general patterns of syntactic behaviour,e.g.
transitivity, passivisability) which have beenextracted from corpora using e.g.
part-of-speechtagging or robust statistical parsing techniques.345This research has been encouraging but it hasso far concentrated on general language.
Domain-specific lexical classification remains unexplored,although it is arguably important: existing clas-sifications are unsuitable for domain-specific ap-plications and these often challenging applicationsmight benefit from improved performance by uti-lizing lexical classes the most.In this paper, we extend an existing approachto lexical classification (Korhonen et al, 2003)and apply it (without any domain specific tun-ing) to the domain of biomedicine.
We focus onbiomedicine for several reasons: (i) NLP is criti-cally needed to assist the processing, mining andextraction of knowledge from the rapidly growingliterature in this area, (ii) the domain lexical re-sources (e.g.
UMLS metathesaurus and lexicon1)do not provide sufficient information about verbsand (iii) being linguistically challenging, the do-main provides a good test case for examining thepotential of automatic classification.We report an experiment where a classifica-tion is induced for 192 relatively frequent verbsfrom a corpus of 2230 biomedical journal articles.The results, evaluated with domain experts, showthat the approach is capable of acquiring classeswith accuracy higher than that reported in previouswork on general language.
We discuss reasons forthis and show that the resulting classes differ sub-stantially from those in extant lexical resources.They constitute the first syntactic-semantic verbclassification for the biomedical domain and couldbe readily applied to support BIO-NLP.We discuss the domain-specific issues related toour task in section 2.
The approach to automaticclassification is presented in section 3.
Details ofthe experimental evaluation are supplied in sec-tion 4.
Section 5 provides discussion and section6 concludes with directions for future work.2 The Biomedical Domain and Our TaskRecent years have seen a massive growth in thescientific literature in the domain of biomedicine.For example, the MEDLINE database2 which cur-rently contains around 16M references to journalarticles, expands with 0.5M new references eachyear.
Because future research in the biomedicalsciences depends on making use of all this existingknowledge, there is a strong need for the develop-1http://www.nlm.nih.gov/research/umls2http://www.ncbi.nlm.nih.gov/PubMed/ment of NLP tools which can be used to automat-ically locate, organize and manage facts related topublished experimental results.In recent years, major progress has been madeon information retrieval and on the extraction ofspecific relations e.g.
between proteins and celltypes from biomedical texts (Hirschman et al,2002).
Other tasks, such as the extraction of fac-tual information, remain a bigger challenge.
Thisis partly due to the challenging nature of biomedi-cal texts.
They are complex both in terms of syn-tax and semantics, containing complex nominals,modal subordination, anaphoric links, etc.Researchers have recently began to use deeperNLP techniques (e.g.
statistical parsing) in the do-main because they are not challenged by the com-plex structures to the same extent than shallowtechniques (e.g.
regular expression patterns) are(Lease and Charniak, 2005).
However, deepertechniques require richer domain-specific lexicalinformation for optimal performance than is pro-vided by existing lexicons (e.g.
UMLS).
This isparticularly important for verbs, which are centralto the structure and meaning of sentences.Where the lexical information is absent, lexicalclasses can compensate for it or aid in obtainingit in the ways described in section 1.
Considere.g.
the INDICATE and ACTIVATE verb classes inFigure 1.
They capture the fact that their membersare similar in terms of syntax and semantics: theyhave similar SCFs and selectional preferences, andthey can be used to make similar statements whichdescribe similar events.
Such information can beused to build a richer lexicon capable of support-ing key tasks such as parsing, predicate-argumentidentification, event extraction and the identifica-tion of biomedical (e.g.
interaction) relations.While an abundance of work has been con-ducted on semantic classification of biomedicalterms and nouns, less work has been done on the(manual or automatic) semantic classification ofverbs in the biomedical domain (Friedman et al,2002; Hatzivassiloglou and Weng, 2002; Spasic etal., 2005).
No previous work exists in this domainon the type of lexical (i.e.
syntactic-semantic) verbclassification this paper focuses on.To get an initial idea about the differences be-tween our target classification and a general lan-guage classification, we examined the extent towhich individual verbs and their frequencies dif-fer in biomedical and general language texts.
We346PROTEINS: p53p53Tp53Dmp53...ACTIVATEsuggestsdemonstratesindicatesimplies...GENES: WAF1WAF1CIP1p21...ItINDICATEthatactivatesup-regulatesinducesstimulates......Figure 1: Sample lexical classesBIO BNCshow dosuggest sayuse makeindicate gocontain seedescribe takeexpress getbind knowrequire comeobserve givefind thinkdetermine usedemonstrate findperform lookinduce wantTable 1: The 15 most frequent verbs in thebiomedical data and in the BNCcreated a corpus of 2230 biomedical journal arti-cles (see section 4.1 for details) and compared thedistribution of verbs in this corpus with that in theBritish National Corpus (BNC) (Leech, 1992).
Wecalculated the Spearman rank correlation betweenthe 1165 verbs which occurred in both corpora.The result was only a weak correlation: 0.37 ?0.03.
When the scope was restricted to the 100most frequent verbs in the biomedical data, thecorrelation was 0.12 ?
0.10 which is only 1.2?away from zero.
The dissimilarity between thedistributions is further indicated by the Kullback-Leibler distance of 0.97.
Table 1 illustrates someof these big differences by showing the list of 15most frequent verbs in the two corpora.3 ApproachWe extended the system of Korhonen et al (2003)with additional clustering techniques (introducedin sections 3.2.2 and 3.2.4) and used it to ob-tain the classification for the biomedical domain.The system (i) extracts features from corpus dataand (ii) clusters them using five different methods.These steps are described in the following two sec-tions, respectively.3.1 Feature ExtractionWe employ as features distributions of SCFs spe-cific to given verbs.
We extract them from cor-pus data using the comprehensive subcategoriza-tion acquisition system of Briscoe and Carroll(1997) (Korhonen, 2002).
The system incorpo-rates RASP, a domain-independent robust statis-tical parser (Briscoe and Carroll, 2002), whichtags, lemmatizes and parses data yielding com-plete though shallow parses and a SCF classifierwhich incorporates an extensive inventory of 163verbal SCFs3.
The SCFs abstract over specificlexically-governed particles and prepositions andspecific predicate selectional preferences.
In ourwork, we parameterized two high frequency SCFsfor prepositions (PP and NP + PP SCFs).
No filter-ing of potentially noisy SCFs was done to provideclustering with as much information as possible.3.2 ClassificationThe SCF frequency distributions constitute the in-put data to automatic classification.
We experi-ment with five clustering methods: the simple hardnearest neighbours method and four probabilis-tic methods ?
two variants of Probabilistic LatentSemantic Analysis and two information theoreticmethods (the Information Bottleneck and the In-formation Distortion).3.2.1 Nearest NeighboursThe first method collects the nearest neighbours(NN) of each verb.
It (i) calculates the Jensen-Shannon divergence (JS) between the SCF distri-butions of each pair of verbs, (ii) connects eachverb with the most similar other verb, and finally(iii) finds all the connected components.
The NNmethod is very simple.
It outputs only one clus-tering configuration and therefore does not allowexamining different cluster granularities.3.2.2 Probabilistic Latent Semantic AnalysisThe Probabilistic Latent Semantic Analysis(PLSA, Hoffman (2001)) assumes a generativemodel for the data, defined by selecting (i) a verbverbi, (ii) a semantic class classk from the dis-tribution p(Classes | verbi), and (iii) a SCF scfjfrom the distribution p(SCFs | classk).
PLSA usesExpectation Maximization (EM) to find the dis-tribution p?
(SCFs |Clusters, V erbs) which max-imises the likelihood of the observed counts.
Itdoes this by minimising the cost functionF = ??
log Likelihood(p?
| data) +H(p?)
.3See http://www.cl.cam.ac.uk/users/alk23/subcat/subcat.htmlfor further detail.347For ?
= 1 minimising F is equivalent to the stan-dard EM procedure while for ?
< 1 the distri-bution p?
tends to be more evenly spread.
We use?
= 1 (PLSA/EM) and ?
= 0.75 (PLSA?=0.75).We currently ?harden?
the output and assign eachverb to the most probable cluster only4.3.2.3 Information BottleneckThe Information Bottleneck (Tishby et al,1999) (IB) is an information-theoretic methodwhich controls the balance between: (i) theloss of information by representing verbs asclusters (I(Clusters;V erbs)), which has to beminimal, and (ii) the relevance of the outputclusters for representing the SCF distribution(I(Clusters; SCFs)) which has to be maximal.The balance between these two quantities ensuresoptimal compression of data through clusters.
Thetrade-off between the two constraints is realizedthrough minimising the cost function:LIB = I(Clusters;V erbs)?
?I(Clusters; SCFs) ,where ?
is a parameter that balances the con-straints.
IB takes three inputs: (i) SCF-verb dis-tributions, (ii) the desired number of clusters K,and (iii) the initial value of ?.
It then looks forthe minimal ?
that decreases LIB compared to itsvalue with the initial ?, using the given K. IB de-livers as output the probabilities p(K|V ).
It givesan indication for the most informative number ofoutput configurations: the ones for which the rele-vance information increases more sharply betweenK ?
1 and K clusters than between K and K + 1.3.2.4 Information DistortionThe Information Distortion method (Dimitrovand Miller, 2001) (ID) is otherwise similar to IBbut LID differs from LIB by an additional term thatadds a bias towards clusters of similar size:LID = ?H(Clusters |V erbs)?
?I(Clusters; SCFs)= LIB ?H(Clusters) .ID yields more evenly divided clusters than IB.4 Experimental Evaluation4.1 DataWe downloaded the data for our experiment fromthe MEDLINE database, from three of the 10 lead-4The same approach was used with the information theo-retic methods.
It made sense in this initial work on biomedi-cal classification.
In the future we could use soft clustering ameans to investigate polysemy.ing journals in biomedicine: 1) Genes & Devel-opment (molecular biology, molecular genetics),2) Journal of Biological Chemistry (biochemistryand molecular biology) and 3) Journal of Cell Bi-ology (cellular structure and function).
2230 full-text articles from years 2003-2004 were used.
Thedata included 11.5M words and 323,307 sentencesin total.
192 medium to high frequency verbs (withthe minimum of 300 occurrences in the data) wereselected for experimentation5.
This test set wasbig enough to produce a useful classification butsmall enough to enable thorough evaluation in thisfirst attempt to classify verbs in the biomedical do-main.4.2 Processing the DataThe data was first processed using the feature ex-traction module.
233 (preposition-specific) SCFtypes appeared in the resulting lexicon, 36 per verbon average.6 The classification module was thenapplied.
NN produced Knn = 42 clusters.
Fromthe other methods we requested K = 2 to 60 clus-ters.
We chose for evaluation the outputs corre-sponding to the most informative values of K: 20,33, 53 for IB, and 17, 33, 53 for ID.4.3 Gold StandardBecause no target lexical classification was avail-able for the biomedical domain, human experts (4domain experts and 2 linguists) were used to cre-ate the gold standard.
They were asked to examinewhether the test verbs similar in terms of their syn-tactic properties (i.e.
verbs with similar SCF distri-butions) are similar also in terms of semantics (i.e.they share a common meaning).
Where this wasthe case, a verb class was identified and named.The domain experts examined the 116 verbswhose analysis required domain knowledge(e.g.
activate, solubilize, harvest), while the lin-guists analysed the remaining 76 general or scien-tific text verbs (e.g.
demonstrate, hypothesize, ap-pear).
The linguists used Levin (1993) classes asgold standard classes whenever possible and cre-ated novel ones when needed.
The domain ex-perts used two purely semantic classifications ofbiomedical verbs (Friedman et al, 2002; Spasic etal., 2005)7 as a starting point where this was pos-5230 verbs were employed initially but 38 were droppedlater so that each (coarse-grained) class would have the min-imum of 2 members in the gold standard.6This number is high because no filtering of potentiallynoisy SCFs was done.7See http://www.cbr-masterclass.org.3481 Have an effect on activity (BIO/29) 8 Physical Relation1.1 Activate / Inactivate Between Molecules (BIO/20)1.1.1 Change activity: activate, inhibit 8.1 Binding: bind, attach1.1.2 Suppress: suppress, repress 8.2 Translocate and Segregate1.1.3 Stimulate: stimulate 8.2.1 Translocate: shift, switch1.1.4 Inactivate: delay, diminish 8.2.2 Segregate: segregate, export1.2 Affect 8.3 Transmit1.2.1 Modulate: stabilize, modulate 8.3.1 Transport: deliver, transmit1.2.2 Regulate: control, support 8.3.2 Link: connect, map1.3 Increase / decrease: increase, decrease 9 Report (GEN/30)1.4 Modify: modify, catalyze 9.1 Investigate2 Biochemical events (BIO/12) 9.1.1 Examine: evaluate, analyze2.1 Express: express, overexpress 9.1.2 Establish: test, investigate2.2 Modification 9.1.3 Confirm: verify, determine2.2.1 Biochemical modification: 9.2 Suggestdephosphorylate, phosphorylate 9.2.1 Presentational:2.2.2 Cleave: cleave hypothesize, conclude2.3 Interact: react, interfere 9.2.2 Cognitive:3 Removal (BIO/6) consider, believe3.1 Omit: displace, deplete 9.3 Indicate: demonstrate, imply3.2 Subtract: draw, dissect 10 Perform (GEN/10)4 Experimental Procedures (BIO/30) 10.1 Quantify4.1 Prepare 10.1.1 Quantitate: quantify, measure4.1.1 Wash: wash, rinse 10.1.2 Calculate: calculate, record4.1.2 Mix: mix 10.1.3 Conduct: perform, conduct4.1.3 Label: stain, immunoblot 10.2 Score: score, count4.1.4 Incubate: preincubate, incubate 11 Release (BIO/4): detach, dissociate4.1.5 Elute: elute 12 Use (GEN/4): utilize, employ4.2 Precipitate: coprecipitate 13 Include (GEN/11)coimmunoprecipitate 13.1 Encompass: encompass, span4.3 Solubilize: solubilize,lyse 13.2 Include: contain, carry4.4 Dissolve: homogenize, dissolve 14 Call (GEN/3): name, designate4.5 Place: load, mount 15 Move (GEN/12)5 Process (BIO/5): linearize, overlap 15.1 Proceed:6 Transfect (BIO/4): inject, microinject progress, proceed7 Collect (BIO/6) 15.2 Emerge:7.1 Collect: harvest, select arise, emerge7.2 Process: centrifuge, recover 16 Appear (GEN/6): appear, occurTable 2: The gold standard classification with afew example verbs per classsible (i.e.
where they included our test verbs andalso captured their relevant senses)8.The experts created a 3-level gold standardwhich includes both broad and finer-grainedclasses.
Only those classes / memberships wereincluded which all the experts (in the two teams)agreed on.9 The resulting gold standard includ-ing 16, 34 and 50 classes is illustrated in table 2with 1-2 example verbs per class.
The table in-dicates which classes were created by domain ex-perts (BIO) and which by linguists (GEN).
Eachclass was associated with 1-30 member verbs10.The total number of verbs is indicated in the table(e.g.
10 for PERFORM class).4.4 MeasuresThe clusters were evaluated against the gold stan-dard using measures which are applicable to all the8Purely semantic classes tend to be finer-grained than lex-ical classes and not necessarily syntactic in nature.
Only thesetwo classifications were found to be similar enough to our tar-get classification to provide a useful starting point.
Section 5includes a summary of the similarities/differences betweenour gold standard and these other classifications.9Experts were allowed to discuss the problematic casesto obtain maximal accuracy - hence no inter-annotator agree-ment is reported.10The minimum of 2 member verbs were required at thecoarser-grained levels of 16 and 34 classes.classification methods and which deliver a numer-ical value easy to interpret.The first measure, the adjusted pairwise preci-sion, evaluates clusters in terms of verb pairs:APP = 1KK?i=1num.
of correct pairs in kinum.
of pairs in ki ?|ki|?1|ki|+1APP is the average proportion of all within-cluster pairs that are correctly co-assigned.
Multi-plied by a factor that increases with cluster size itcompensates for a bias towards small clusters.The second measure is modified purity, a globalmeasure which evaluates the mean precision ofclusters.
Each cluster is associated with its preva-lent class.
The number of verbs in a cluster K thattake this class is denoted by nprevalent(K).
Verbsthat do not take it are considered as errors.
Clus-ters where nprevalent(K) = 1 are disregarded asnot to introduce a bias towards singletons:mPUR =?nprevalent(ki)?2nprevalent(ki)number of verbsThe third measure is the weighted class accu-racy, the proportion of members of dominant clus-ters DOM-CLUSTi within all classes ci.ACC =C?i=1verbs in DOM-CLUSTinumber of verbsmPUR can be seen to measure the precision ofclusters and ACC the recall.
We define an F mea-sure as the harmonic mean of mPUR and ACC:F = 2 ?mPUR ?
ACCmPUR + ACCThe statistical significance of the results is mea-sured by randomisation tests where verbs areswapped between the clusters and the resultingclusters are evaluated.
The swapping is repeated100 times for each output and the average avswapsand the standard deviation ?swaps is measured.The significance is the scaled difference signif =(result?
avswaps)/?swaps .4.5 Results from Quantitative EvaluationTable 3 shows the performance of the five clus-tering methods for K = 42 clusters (as producedby the NN method) at the 3 levels of gold stan-dard classification.
Although the two PLSA vari-ants (particularly PLSA?=0.75) produce a fairly ac-curate coarse grained classification, they performworse than all the other methods at the finer-grained levels of gold standard, particularly ac-cording to the global measures.
Being based on34916 Classes 34 Classes 50 ClassesAPP mPUR ACC F APP mPUR ACC F APP mPUR ACC FNN 81 86 39 53 64 74 62 67 54 67 73 69IB 74 88 47 61 61 76 74 75 55 69 87 76ID 79 89 37 52 63 78 65 70 53 70 77 73PLSA/EM 55 72 49 58 43 53 61 57 35 47 66 55PLSA?=0.75 65 71 68 70 53 48 76 58 41 34 77 47Table 3: The performance of the NN, PLSA, IB and ID methods with Knn = 42 clusters16 Classes 34 Classes 50 ClassesK APP mPUR ACC F APP mPUR ACC F APP mPUR ACC F20 IB 74 77 66 71 60 56 86 67 54 48 93 6317 ID 67 76 60 67 43 56 81 66 34 46 91 6133 IB 78 87 52 65 69 75 81 77 61 67 93 77ID 81 88 43 57 65 75 70 72 54 67 82 7353 IB 71 87 41 55 61 78 66 71 54 72 79 75ID 79 89 33 48 66 79 55 64 53 72 68 69Table 4: The performance of IB and ID for the 3 levels of class hierarchy for informative values of Kpairwise similarities, NN shows mostly better per-formance than IB and ID on the pairwise measureAPP but the global measures are better for IB andID.
The differences are smaller in mPUR (yet sig-nificant: 2?
between NN and IB and 3?
betweenNN and ID) but more notable in ACC (which ise.g.
8 ?
12% better for IB than for NN).
Alsothe F results suggest that the two information the-oretic methods are better overall than the simpleNN method.IB and ID also have the advantage (over NN) thatthey can be used to produce a hierarchical verbclassification.
Table 4 shows the results for IB andID for the informative values of K. The bold fontindicates the results when the match between thevalues of K and the number of classes at the par-ticular level of the gold standard is the closest.IB is clearly better than ID at all levels of goldstandard.
It yields its best results at the mediumlevel (34 classes) with K = 33: F = 77 and APP= 69 (the results for ID are F = 72 and APP =65).
At the most fine-grained level (50 classes),IB is equally good according to F with K = 33,but APP is 8% lower.
Although ID is occasion-ally better than IB according to APP and mPUR(see e.g.
the results for 16 classes with K = 53)this never happens in the case where the corre-spondence between the number of gold standardclasses and the values of K is the closest.
In otherwords, the informative values of K prove reallyinformative for IB.
The lower performance of IDseems to be due to its tendency to create evenlysized clusters.All the methods perform significantly betterthan our random baseline.
The significance of theresults with respect to two swaps was at the 2?level, corresponding to a 97% confidence that theresults are above random.4.6 Qualitative EvaluationWe performed further, qualitative analysis of clus-ters produced by the best performing method IB.Consider the following clusters:A: inject, transfect, microinfect, contransfect (6)B: harvest, select, collect (7.1)centrifuge, process, recover (7.2)C: wash, rinse (4.1.1)immunoblot (4.1.3)overlap (5)D: activate (1.1.1)When looking at coarse-grained outputs, in-terestingly, K as low as 8 learned the broaddistinction between biomedical and general lan-guage verbs (the two verb types appeared onlyrarely in the same clusters) and produced large se-mantically meaningful groups of classes (e.g.
thecoarse-grained classes EXPERIMENTAL PROCE-DURES, TRANSFECT and COLLECT were mappedtogether).
K = 12 was sufficient to iden-tify several classes with very particular syntaxOne of them was TRANSFECT (see A above)whose members were distinguished easily be-cause of their typical SCFs (e.g.
inject /trans-fect/microinfect/contransfect X with/into Y).On the other hand, even K = 53 could not iden-tify classes with very similar (yet un-identical)syntax.
These included many semantically similarsub-classes (e.g.
the two sub-classes of COLLECT350shown in B whose members take similar NP andPP SCFs).
However, also a few semantically dif-ferent verbs clustered wrongly because of this rea-son, such as the ones exemplified in C. In C, im-munoblot (from the LABEL class) is still somewhatrelated to wash and rinse (the WASH class) becausethey all belong to the larger EXPERIMENTAL PRO-CEDURES class, but overlap (from the PROCESSclass) shows up in the cluster merely because ofsyntactic idiosyncracy.While parser errors caused by the challeng-ing biomedical texts were visible in some SCFs(e.g.
looking at a sample of SCFs, some adjunctinstances were listed in the argument slots of theframes), the cases where this resulted in incorrectclassification were not numerous11.One representative singleton resulting fromthese errors is exemplified in D. Activate ap-pears in relatively complicated sentence struc-tures, which gives rise to incorrect SCFs.
For ex-ample, MECs cultured on 2D planar substratestransiently activate MAP kinase in response toEGF, whereas... gets incorrectly analysed as SCFNP-NP, while The effect of the constitutively ac-tivated ARF6-Q67L mutant was investigated... re-ceives the incorrect SCF analysis NP-SCOMP.
Mostparser errors are caused by unknown domain-specific words and phrases.5 DiscussionDue to differences in the task and experimentalsetup, direct comparison of our results with pre-viously published ones is impossible.
The clos-est possible comparison point is (Korhonen et al,2003) which reported 50-59% mPUR and 15-19%APP on using IB to assign 110 polysemous (gen-eral language) verbs into 34 classes.
Our resultsare substantially better, although we made no ef-fort to restrict our scope to monosemous verbs12and although we focussed on a linguistically chal-lenging domain.It seems that our better result is largely dueto the higher uniformity of verb senses in thebiomedical domain.
We could not investigate thiseffect systematically because no manually sense11This is partly because the mistakes of the parser aresomewhat consistent (similar for similar verbs) and partly be-cause the SCFs gather data from hundreds of corpus instances,many of which are analysed correctly.12Most of our test verbs are polysemous according toWordNet (WN) (Miller, 1990), but this is not a fully reliableindication because WN is not specific to this domain.annotated data (or a comprehensive list of verbsenses) exists for the domain.
However, exami-nation of a number of corpus instances suggeststhat the use of verbs is fairly conventionalized inour data13.
Where verbs show less sense varia-tion, they show less SCF variation, which aids thediscovery of verb classes.
Korhonen et al (2003)observed the opposite with general language data.We examined, class by class, to what extent ourdomain-specific gold standard differs from the re-lated general (Levin, 1993) and domain classifica-tions (Spasic et al, 2005; Friedman et al, 2002)(recall that the latter were purely semantic clas-sifications as no lexical ones were available forbiomedicine):33 (of the 50) classes in the gold standard arebiomedical.
Only 6 of these correspond (fully ormostly) to the semantic classes in the domain clas-sifications.
17 are unrelated to any of the classes inLevin (1993) while 16 bear vague resemblance tothem (e.g.
our TRANSPORT verbs are also listedunder Levin?s SEND verbs) but are too different(semantically and syntactically) to be combined.17 (of the 50) classes are general (scientific)classes.
4 of these are absent in Levin (e.g.
QUAN-TITATE).
13 are included in Levin, but 8 of themhave a more restricted sense (and fewer members)than the corresponding Levin class.
Only the re-maining 5 classes are identical (in terms of mem-bers and their properties) to Levin classes.These results highlight the importance of build-ing or tuning lexical resources specific to differentdomains, and demonstrate the usefulness of auto-matic lexical acquisition for this work.6 ConclusionThis paper has shown that current domain-independent NLP and ML technology can be usedto automatically induce a relatively high accu-racy verb classification from a linguistically chal-lenging corpus of biomedical texts.
The lexicalclassification resulting from our work is stronglydomain-specific (it differs substantially from pre-vious ones) and it can be readily used to aid BIO-NLP.
It can provide useful material for investigat-ing the syntax and semantics of verbs in biomed-ical data or for supplementing existing domainlexical resources with additional information (e.g.13The different sub-domains of the biomedical domainmay, of course, be even more conventionalized (Friedman etal., 2002).351semantic classifications with additional memberverbs).
Lexical resources enriched with verb classinformation can, in turn, better benefit practicaltasks such as parsing, predicate-argument identifi-cation, event extraction, identification of biomedi-cal relation patterns, among others.In the future, we plan to improve the accu-racy of automatic classification by seeding it withdomain-specific information (e.g.
using named en-tity recognition and anaphoric linking techniquessimilar to those of Vlachos et al (2006)).
We alsoplan to conduct a bigger experiment with a largernumber of verbs and demonstrate the usefulness ofthe bigger classification for practical BIO-NLP ap-plication tasks.
In addition, we plan to apply sim-ilar technology to other interesting domains (e.g.tourism, law, astronomy).
This will not only en-able us to experiment with cross-domain lexicalclass variation but also help to determine whetherautomatic acquisition techniques benefit, in gen-eral, from domain-specific tuning.AcknowledgementWe would like to thank Yoko Mizuta, ShokoKawamato, Sven Demiya, and Parantu Shah fortheir help in creating the gold standard.ReferencesC.
Brew and S. Schulte im Walde.
2002.
Spectralclustering for German verbs.
In Conference on Em-pirical Methods in Natural Language Processing,Philadelphia, USA.E.
J. Briscoe and J. Carroll.
1997.
Automatic extrac-tion of subcategorization from corpora.
In 5th ACLConference on Applied Natural Language Process-ing, pages 356?363, Washington DC.E.
J. Briscoe and J. Carroll.
2002.
Robust accuratestatistical annotation of general text.
In 3rd Interna-tional Conference on Language Resources and Eval-uation, pages 1499?1504, Las Palmas, Gran Ca-naria.A.
G. Dimitrov and J. P. Miller.
2001.
Neural codingand decoding: communication channels and quanti-zation.
Network: Computation in Neural Systems,12(4):441?472.B.
Dorr.
1997.
Large-scale dictionary construction forforeign language tutoring and interlingual machinetranslation.
Machine Translation, 12(4):271?325.C.
Friedman, P. Kra, and A. Rzhetsky.
2002.
Twobiomedical sublanguages: a description based on thetheories of Zellig Harris.
Journal of Biomedical In-formatics, 35(4):222?235.V.
Hatzivassiloglou and W. Weng.
2002.
Learning an-chor verbs for biological interaction patterns frompublished text articles.
International Journal ofMedical Inf., 67:19?32.L.
Hirschman, J. C. Park, J. Tsujii, L. Wong, and C. H.Wu.
2002.
Accomplishments and challenges in lit-erature data mining for biology.
Journal of Bioin-formatics, 18(12):1553?1561.T.
Hoffman.
2001.
Unsupervised learning by proba-bilistic latent semantic analysis.
Machine Learning,42(1):177?196.R.
Jackendoff.
1990.
Semantic Structures.
MIT Press,Cambridge, Massachusetts.A.
Korhonen, Y. Krymolowski, and Z. Marx.
2003.Clustering polysemic subcategorization frame distri-butions semantically.
In Proceedings of the 41st An-nual Meeting of the Association for ComputationalLinguistics, pages 64?71, Sapporo, Japan.A.
Korhonen.
2002.
Subcategorization Acquisition.Ph.D.
thesis, University of Cambridge, UK.M.
Lease and E. Charniak.
2005.
Parsing biomedicalliterature.
In Second International Joint Conferenceon Natural Language Processing, pages 58?69.G.
Leech.
1992.
100 million words of English:the British National Corpus.
Language Research,28(1):1?13.B.
Levin.
1993.
English Verb Classes and Alterna-tions.
Chicago University Press, Chicago.P.
Merlo and S. Stevenson.
2001.
Automatic verbclassification based on statistical distributions ofargument structure.
Computational Linguistics,27(3):373?408.G.
A. Miller.
1990.
WordNet: An on-line lexicaldatabase.
International Journal of Lexicography,3(4):235?312.D.
Prescher, S. Riezler, and M. Rooth.
2000.
Usinga probabilistic class-based lexicon for lexical am-biguity resolution.
In 18th International Confer-ence on Computational Linguistics, pages 649?655,Saarbru?cken, Germany.I.
Spasic, S. Ananiadou, and J. Tsujii.
2005.
Master-class: A case-based reasoning system for the clas-sification of biomedical terms.
Journal of Bioinfor-matics, 21(11):2748?2758.N.
Tishby, F. C. Pereira, and W. Bialek.
1999.
Theinformation bottleneck method.
In Proc.
of the37th Annual Allerton Conference on Communica-tion, Control and Computing, pages 368?377.A.
Vlachos, C. Gasperin, I. Lewin, and E. J. Briscoe.2006.
Bootstrapping the recognition and anaphoriclinking of named entitites in drosophila articles.
InPacific Symposium in Biocomputing.352
