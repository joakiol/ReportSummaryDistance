Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition, pages 25?32,Prague, Czech Republic, June 2007 c?2007 Association for Computational LinguisticsHigh-accuracy Annotation and Parsing of CHILDES TranscriptsKenji SagaeDepartment of Computer ScienceUniversity of TokyoHongo 7-3-1, Bunkyo-ku, Tokyo, Japansagae@is.s.u-tokyo.ac.jpEric DavisLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213dhdavis@cs.cmu.eduAlon LavieLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213alavie@cs.cmu.eduBrian MacWhinneyDepartment of PsychologyCarnegie Mellon UniversityPittsburgh, PA 15213macw@cmu.eduShuly WintnerDepartment of Computer ScienceUniversity of Haifa31905 Haifa, Israelshuly@cs.haifa.ac.ilAbstractCorpora of child language are essential forpsycholinguistic research.
Linguistic anno-tation of the corpora provides researcherswith better means for exploring the develop-ment of grammatical constructions and theirusage.
We describe an ongoing project thataims to annotate the English section of theCHILDES database with grammatical re-lations in the form of labeled dependencystructures.
To date, we have produced a cor-pus of over 65,000 words with manually cu-rated gold-standard grammatical relation an-notations.
Using this corpus, we have devel-oped a highly accurate data-driven parser forEnglish CHILDES data.
The parser and themanually annotated data are freely availablefor research purposes.1 IntroductionIn order to investigate the development of child lan-guage, corpora which document linguistic interac-tions involving children are needed.
The CHILDESdatabase (MacWhinney, 2000), containing tran-scripts of spoken interactions between children atvarious stages of language development with theirparents, provides vast amounts of useful data for lin-guistic, psychological, and sociological studies ofchild language development.
The raw information inCHILDES corpora was gradually enriched by pro-viding a layer of morphological information.
In par-ticular, the English section of the database is aug-mented by part of speech (POS) tags for each word.However, this information is usually insufficient forinvestigations dealing with the syntactic, semanticor pragmatic aspects of the data.In this paper we describe an ongoing effort aim-ing to annotate the English portion of the CHILDESdatabase with syntactic information based on gram-matical relations represented as labeled dependencystructures.
Although an annotation scheme for syn-tactic information in CHILDES data has been pro-posed (Sagae et al, 2004), until now no significantamount of annotated data had been made publiclyavailable.
In the process of manually annotating sev-eral thousands of words, we updated the annotationscheme, mostly by extending it to cover syntacticphenomena that occur in real data but were unac-counted for in the original annotation scheme.The contributions of this work fall into three maincategories: revision and extension of the annota-tion scheme for representing syntactic informationin CHILDES data; creation of a manually annotated65,000 word corpus with gold-standard syntacticanalyses; and implementation of a complete parserthat can automatically annotate additional data withhigh accuracy.
Both the gold-standard annotateddata and the parser are freely available.
In addi-tion to introducing the parser and the data, we re-port on many of the specific annotation issues thatwe encountered during the manual annotation pro-25cess, which should be helpful for those who mayuse the annotated data or the parser.
The anno-tated corpora and the parser are freely available fromhttp://childes.psy.cmu.edu/.We describe the annotation scheme in the nextsection, along with issues we faced during the pro-cess of manual annotation.
Section 3 describes theparser, and an evaluation of the parser is presented insection 4.
We analyze the remaining parsing errorsin section 5 and conclude with some applications ofthe parser and directions for future research in sec-tion 6.2 Syntactic annotationThe English section of the CHILDES database isaugmented with automatically produced ambiguouspart-of-speech and morphological tags (MacWhin-ney, 2000).
Some of these data have been manuallydisambiguated, but we found that some annotationdecisions had to be revised to facilitate syntactic an-notation.
We discuss below some of the revisions weintroduced, as well as some details of the syntacticconstructions that we account for.2.1 The morphological annotation schemeThe English morphological analyzer incorporatedin CHILDES produces various part-of-speech tags(there are 31 distinct POS tags in the CHILDEStagset), including ADJective, ADVerb, COmmuni-cator, CONJunction, DETerminer, FILler, Noun,NUMeral, ONomatopoeia, PREPosition, PROnoun,ParTicLe, QuaNtifier, RELativizer and Verb1.
Inmost cases, the correct annotation of a word is obvi-ous from the context in which the word occurs, butsometimes a more subtle distinction must be made.We discuss some common problematic issues below.Adverb vs. preposition vs. particle The wordsabout, across, after, away, back, down, in, off, on,out, over, up belong to three categories: ADVerb,PREPosition and ParTicLe.
To correctly annotatethem in context, we apply the following criteria.First, a preposition must have a prepositional ob-ject, which is typically realized as a noun phrase(which may be topicalized, or even elided).
Sec-ond, a preposition forms a constituent with its noun1We use capital letters to denote the actual tag names in theCHILDES tagset.phrase object.
Third, a prepositional object can befronted (for example, he sat on the chair becomesthe chair on which he sat), whereas a particle-NPsequence cannot (*the phone number up which helooked cannot be obtained from he looked up thephone number).
Finally, a manner adverb can beplaced between the verb and a preposition, but notbetween a verb and a particle.To distinguish between an adverb and a particle,the meaning of the head verb is considered.
If themeaning of the verb and the target word, taken to-gether, cannot be predicted from the meanings of theverb and the target word separately, then the targetword is a particle.
In all other cases it is an adverb.Verbs vs. auxiliaries Distinguishing betweenVerb and AUXiliary is often straightforward, butspecial attention is given when tagging the verbs be,do and have.
If the target word is accompanied by annon-finite verb in the same clause, as in I have hadenough or I do not like eggs, it is an auxiliary.
Ad-ditionally, in interrogative sentences, the auxiliary ismoved to the beginning of the clause, as in have Ihad enough?
and do I like eggs?, whereas the mainverb is not.
However, this test does not always workfor the verb be, which may head a non-verbal pred-icate, as in John is a teacher, vs. John is smiling.
Inverb-participle constructions headed by the verb be,if the participle is in the progressive tense, then thehead verb is labeled as auxiliary.Communicators vs. locative adverbs COmmu-nicators can be hard to distinguish from locative ad-verbs, especially at the beginning of a sentence.
Ourconvention is that CO must modify an entire sen-tence, so if a word appears by itself, it cannot be aCO.
For example, utterances like here or there arelabeled as ADVerb.
However, if these words appearat the beginning of a sentence, are followed by abreak or pause, and do not clearly express a location,then they are labeled CO. Additionally, in here/thereyou are/go, here and there are labeled CO.2.2 The syntactic annotation schemeOur annotation scheme for representing grammati-cal relations, or GRs (such as subjects, objects andadjuncts), in CHILDES transcripts is a slightly ex-tended version of the scheme proposed by Sagae etal.
(2004), which was inspired by a general annota-26tion scheme for grammatical relations (Carroll et al,1998), but adapted specifically for CHILDES data.Our scheme contains 37 distinct GR types.
Sagaeet al reported 96.5% interannotator agreement, andwe do not believe our minor updates to the annota-tion scheme should affect interannotator agreementsignificantly.The scheme distinguishes among SUBJects, (fi-nite) Clausal SUBJects2 (e.g., that he cried movedher) and XSUBJects (eating vegetables is impor-tant).
Similarly, we distinguish among OBJects,OBJect2, which is the second object of a ditran-sitive verb, and IOBjects, which are required verbcomplements introduced by prepositions.
Verb com-plements that are realized as clauses are labeledCOMP if they are finite (I think that was Fraser) andXCOMP otherwise (you stop throwing the blocks).Additionally, we mark required locative adjectivalor prepositional phrase arguments of verbs as LOCa-tives, as in put the toys in the box/back.PREDicates are nominal, adjectival or prepo-sitional complements of verbs such as get, beand become, as in I?m not sure.
Again, wespecifically mark Clausal PREDicates (This ishow I drink my coffee) and XPREDicates (My goalis to win the competition).Adjuncts (denoted by JCT) are optional modi-fiers of verbs, adjectives or adverbs, and we dis-tinguish among non-clausal ones (That?s much bet-ter; sit on the stool), finite clausal ones (CJCT, Maryleft after she saw John) and non-finite clausal ones(XJCT, Mary left after seeing John).MODifiers, which modify or complement nouns,again come in three flavors: MOD (That?s a nicebox); CMOD (the movie that I saw was good ); andXMOD (the student reading a book is tall ).We then identify AUXiliary verbs, as in did youdo it?
; NEGation (Fraser is not drinking his coffee);DETerminers (a fly); QUANTifiers (some juice); theobjects of prepositions (POBJ, on the stool); verbParTicLes (can you get the blocks out?
); ComPle-mentiZeRs (wait until the noodles are cool ); COM-municators (oh, I took it); the INfinitival to; VOCa-tives (Thank you, Eve); and TAG questions (youknow how to count, don?t you?
).2As with the POS tags, we use capital letters to represent theactual GR tags used in the annotation scheme.Finally, we added some specific relations for han-dling problematic issues.
For example, we useENUMeration for constructions such as one, two,three, go or a, b, c. In COORDination construc-tions, each conjunct is marked as a dependent of theconjunction (e.g., go and get your telephone).
Weuse TOPicalization to indicate an argument that istopicalized, as in tapioca, there is no tapioca.
Weuse SeRiaL to indicate serial verbs as in come seeif we can find it or go play with your toys.
Finally,we mark sequences of proper names which form thesame entity (e.g., New York ) as NAME.The format of the grammatical relation (GR) an-notation, which we use in the examples that follow,associates with each word in a sentence a triple i|j|g,where i is the index of the word in the sentence, j theindex of the word?s syntactic head, and g is the nameof the grammatical relation represented by the syn-tactic dependency between the i-th and j-th words.If the topmost head of the utterance is the i-th word,it is labeled i|0|ROOT.
For example, in:a cookie .1|2|DET 2|0|ROOT 3|2|PUNCTthe first word a is a DETerminer of word 2 (cookie),which is itself the ROOT of the utterance.2.3 Manual annotation of the corpusWe focused our manual annotation on a set ofCHILDES transcripts for a particular child, Eve(Brown, 1973), and we refer to these transcripts,distributed in a set of 20 files, as the Eve corpus.We hand-annotated (including correcting POS tags)the first 15 files of the Eve corpus following theGR scheme outlined above.
The annotation pro-cess started with purely manual annotation of 5,000words.
This initial annotated corpus was used totrain a data-driven parser, as described later.
Thisparser was then used to label an additional 20,000words automatically, followed by a thorough manualchecking stage, where each syntactic annotation wasmanually verified and corrected if necessary.
We re-trained the parser with the newly annotated data, andproceeded in this fashion until 15 files had been an-notated and thoroughly manually checked.Annotating child language proved to be challeng-ing, and as we progressed through the data, we no-ticed grammatical constructions that the GRs could27not adequately handle.
For example, the original GRscheme did not differentiate between locative argu-ments and locative adjuncts, so we created a new GRlabel, LOC, to handle required verbal locative argu-ments such as on in put it on the table.
Put licensesa prepositional argument, and the existing JCT rela-tion could not capture this requirement.In addition to adding new GRs, we also facedchallenges with telegraphic child utterances lack-ing verbs or other content words.
For instance,Mommy telephone could have one of several mean-ings: Mommy this is a telephone, Mommy I wantthe telephone, that is Mommy?s telephone, etc.
Wetried to be as consistent as possible in annotatingsuch utterances and determined their GRs from con-text.
It was often possible to determine the VOCreading vs.the MOD (Mommy?s telephone) readingby looking at context.
If it was not possible to deter-mine the correct annotation from context, we anno-tated such utterances as VOC relations.After annotating the 15 Eve files, we had 18,863fully hand-annotated utterances, 10,280 adultand 8,563 child.
The utterances consist of 84,226GRs (including punctuation) and 65,363 words.The average utterance length is 5.3 words (in-cluding punctuation) for adult utterances, 3.6 forchild, 4.5 overall.
The annotated Eve corpusis available at http://childes.psy.cmu.edu/data/Eng-USA/brown.zip.
It was usedfor the Domain adaptation task at the CoNLL-2007dependency parsing shared task (Nivre, 2007).3 ParsingAlthough the CHILDES annotation scheme pro-posed by Sagae et al (2004) has been used in prac-tice for automatic parsing of child language tran-scripts (Sagae et al, 2004; Sagae et al, 2005), suchwork relied mainly on a statistical parser (Char-niak, 2000) trained on the Wall Street Journal por-tion of the Penn Treebank, since a large enough cor-pus of annotated CHILDES data was not availableto train a domain-specific parser.
Having a corpusof 65,000 words of CHILDES data annotated withgrammatical relations represented as labeled depen-dencies allows us to develop a parser tailored for theCHILDES domain.Our overall parsing approach uses a best-firstprobabilistic shift-reduce algorithm, working left-to-right to find labeled dependencies one at a time.
Thealgorithm is essentially a dependency version of thedata-driven constituent parsing algorithm for prob-abilistic GLR-like parsing described by Sagae andLavie (2006).
Because CHILDES syntactic annota-tions are represented as labeled dependencies, usinga dependency parsing approach allows us to workwith that representation directly.This dependency parser has been shown to havestate-of-the-art accuracy in the CoNLL shared taskson dependency parsing (Buchholz and Marsi, 2006;Nivre, 2007)3.
Sagae and Tsujii (2007) present adetailed description of the parsing approach used inour work, including the parsing algorithm.
In sum-mary, the parser uses an algorithm similar to the LRparsing algorithm (Knuth, 1965), keeping a stack ofpartially built syntactic structures, and a queue ofremaining input tokens.
At each step in the pars-ing process, the parser can apply a shift action (re-move a token from the front of the queue and placeit on top of the stack), or a reduce action (pop thetwo topmost stack items, and push a new item com-posed of the two popped items combined in a sin-gle structure).
This parsing approach is very similarto the one used successfully by Nivre et al (2006),but we use a maximum entropy classifier (Berger etal., 1996) to determine parser actions, which makesparsing extremely fast.
In addition, our parsing ap-proach performs a search over the space of possibleparser actions, while Nivre et al?s approach is de-terministic.
See Sagae and Tsujii (2007) for moreinformation on the parser.Features used in classification to determinewhether the parser takes a shift or a reduce actionat any point during parsing are derived from theparser?s current configuration (contents of the stackand queue) at that point.
The specific features usedare:4?
Word and its POS tag: s(1), q(2), and q(1).?
POS: s(3) and q(2).3The parser used in this work is the same as the probabilisticshift-reduce parser referred to as ?Sagae?
in the cited sharedtask descriptions.
In the 2007 shared task, an ensemble of shift-reduce parsers was used, but only a single parser is used here.4s(n) denotes the n-th item from the top of the stack (wheres(1) is the item on the top of the stack), and q(n) denotes then-th item from the front of the queue.28?
The dependency label of the most recently at-tached dependent of: s(1) and s(2).?
The previous parser action.4 Evaluation4.1 MethodologyWe first evaluate the parser by 15-fold cross-validation on the 15 manually curated gold-standardEve files (to evaluate the parser on each file, the re-maining 14 files are used to train the parser).
Single-word utterances (excluding punctuation) were ig-nored, since their analysis is trivial and their inclu-sion would artificially inflate parser accuracy mea-surements.
The size of the Eve evaluation corpus(with single-word utterances removed) was 64,558words (or 59,873 words excluding punctuation).
Ofthese, 41,369 words come from utterances spokenby adults, and 18,504 come from utterances spo-ken by the child.
To evaluate the parser?s portabil-ity to other CHILDES corpora, we also tested theparser (trained only on the entire Eve set) on two ad-ditional sets, one taken from the MacWhinney cor-pus (MacWhinney, 2000) (5,658 total words, 3,896words in adult utterances and 1,762 words in childutterances), and one taken from the Seth corpus (Pe-ters, 1987; Wilson and Peters, 1988) (1,749 words,1,059 adult and 690 child).The parser is highly efficient: training on the en-tire Eve corpus takes less that 20 minutes on stan-dard hardware, and once trained, parsing the Evecorpus takes 18 seconds, or over 3,500 words persecond.Following recent work on dependency parsing(Nivre, 2007), we report two evaluation measures:labeled accuracy score (LAS) and unlabeled accu-racy score (UAS).
LAS is the percentage of tokensfor which the parser predicts the correct head-wordand dependency label.
UAS ignores the dependencylabels, and therefore corresponds to the percentageof words for which the correct head was found.
Inaddition to LAS and UAS, we also report precisionand recall of certain grammatical relations.For example, compare the parser output of go buyan apple to the gold standard (Figure 1).
This se-quence of GRs has two labeled dependency errorsand one unlabeled dependency error.
1|2|COORDfor the parser versus 1|2|SRL is a labeled error be-cause the dependency label produced by the parser(COORD) does not match the gold-standard anno-tation (SRL), although the unlabeled dependency iscorrect, since the headword assignment, 1|2, is thesame for both.
On the other hand, 5|1|PUNCT ver-sus 5|2|PUNCT is both a labeled dependency errorand an unlabeled dependency error, since the head-word assignment produced by the parser does notmatch the gold-standard.4.2 ResultsTrained on domain-specific data, the parser per-formed well on held-out data, even though the train-ing corpus is relatively small (about 60,000 words).The results are listed in Table 1.LAS UASEve cross-validation 92.0 93.8Table 1: Average cross-validation results, EveThe labeled dependency error rate is about 8%and the unlabeled error rate is slightly over 6%.
Per-formance in individual files ranged between the bestlabeled error rate of 6.2% and labeled error rate of4.4% for the fifth file, and the worst error rates of8.9% and 7.8% for labeled and unlabeled respec-tively in the fifteenth file.
For comparison, Sagae etal.
(2005) report 86.9% LAS on about 2,000 wordsof Eve data, using the Charniak (2000) parser witha separate dependency-labeling step.
Part of the rea-son we obtain levels of accuracy higher than usu-ally reported for dependency parsers is that the aver-age sentence length in CHILDES transcripts is muchlower than in, for example, newspaper text.
The av-erage sentence length for adult utterances in the Evecorpus is 6.1 tokens, and 4.3 tokens for child utter-ances5.Certain GRs are easily identifiable, such as DET,AUX, and INF.
The parser has precision and recallof nearly 1.00 for those.
For all GRs that occur morethan 1,000 times in the Eve corpus (which contrainsmore than 60,000 tokens), precision and recall areabove 0.90, with the exception of COORD, which5This differs from the figures in section 2.3 because for thepurpose of parser evaluation we ignore sentences composedonly of a single word plus punctuation.29go buy an apple .parser: 1|2|COORD 2|0|ROOT 3|4|DET 4|2|OBJ 5|1|PUNCTgold: 1|2|SRL 2|0|ROOT 3|4|DET 4|2|OBJ 5|2|PUNCTFigure 1: Example output: parser vs. gold annotationoccurs 1,163 times in the gold-standard data.
Theparser?s precision for COORD is 0.73, and recallis 0.84.
Other interesting GRs include SUBJ, OBJ,JCT (adjunct), COM, LOC, COMP, XCOMP, CJCT(subordinate clause acting as an adjunct), and PTL(verb particle, easily confusable with prepositionsand adverbs).
Their precision and recall is shownin table 2.GR Precision Recall F-scoreSUBJ 0.96 0.96 0.96OBJ 0.93 0.94 0.93JCT 0.91 0.90 0.90COM 0.96 0.95 0.95LOC 0.95 0.90 0.92COMP 0.83 0.86 0.84XCOMP 0.86 0.87 0.87CJCT 0.61 0.59 0.60PTL 0.97 0.96 0.96COORD 0.73 0.84 0.78Table 2: Precision, recall and f-score of selectedGRs in the Eve corpusWe also tested the accuracy of the parser on childutterances and adult utterances separately.
To dothis, we split the gold standard files into child andadult utterances, producing gold standard files forboth child and adult utterances.
We then trainedthe parser on 14 of the 15 Eve files with both childand adult utterances, and parsed the individual childand adult files.
Not surprisingly, the parser per-formed slightly better on the adult utterances due totheir grammaticality and the fact that there was moreadult training data than child training data.
The re-sults are listed in Table 3.LAS UASEve - Child 90.0 91.7Eve - Adult 93.1 94.8Table 3: Average child vs. adult results, EveOur final evaluation of the parser involved test-ing the parser on data taken from a different parts ofthe CHILDES database.
First, the parser was trainedon all gold-standard Eve files, and tested on man-ually annotated data taken from the MacWhinneytranscripts.
Although accuracy was lower for adultutterances (85.8% LAS) than on Eve data, the accu-racy for child utterances was slightly higher (92.3%LAS), even though child utterances were longer onaverage (4.7 tokens) than in the Eve corpus.Finally, because a few aspects of the many tran-script sets in the CHILDES database may vary inways not accounted for in the design of the parseror the annotation of the training data, we also re-port results on evaluation of the Eve-trained parseron a particularly challenging test set, the Seth cor-pus.
Because the Seth corpus contains transcriptionsof language phenomena not seen in the Eve corpus(see section 5), parser performance is expected tosuffer.
Although accuracy on adult utterances is high(92.2% LAS), accuracy on child utterances is verylow (72.7% LAS).
This is due to heavy use of a GRlabel that does not appear at all in the Eve corpusthat was used to train the parser.
This GR is used torepresent relations involving filler syllables, whichappear in nearly 45% of the child utterances in theSeth corpus.
Accuracy on the sentences that do notcontain filler syllables is at the same level as in theother corpora (91.1% LAS).
Although we do not ex-pect to encounter many sets of transcripts that are asproblematic as this one in the CHILDES database, itis interesting to see what can be expected from theparser under unfavorable conditions.The results of the parser on the MacWhinney andSeth test sets are summarized in table 4, where Seth(clean) refers to the Seth corpus without utterancesthat contain filler sylables.5 Error AnalysisA major source for parser errors on the Eve cor-pus (112 out of 5181 errors) was telegraphic speech,30LAS UASMacWhinney - Child 92.3 94.8MacWhinney - Adult 85.8 89.4MacWhinney - Total 88.0 91.2Seth - Child 72.7 82.0Seth - Adult 92.2 94.4Seth - Total 84.6 89.5Seth (clean) - Child 91.1 92.7Seth (clean) - Total 92.0 93.9Table 4: Training on Eve, testing on MacWhinneyand Sethas in Mommy telephone or Fraser tape+recorderfloor.
Telegraphic speech may be the most chal-lenging, since even for a human annotator, deter-mining a GR is difficult.
The parser usually labeledsuch utterances with the noun as the ROOT and theproper noun as the MOD, while the gold annotationis context-dependent as described above.Another category of errors, with about 150 in-stances, is XCOMP errors.
The majority of the er-rors in this category revolve around dropped wordsin the main clause, for example want eat cookie.
Of-ten, the parser labels such utterances with COMPGRs, because of the lack of to.
Exclusive training onutterances of this type may resolve the issue.
Manyof the errors of this type occur with want : the parsercould be conditioned to assign an XCOMP GR withwant as the ROOT of an utterance.COORD and PRED errors would both benefitfrom more data as well.
The parser performs ad-mirably on simple coordination and predicate con-structions, but has troubles with less common con-structions such as PRED GRs with get, e.g., don?tlet your hands get dirty (69 errors), and coordina-tion of prepositional objects, as in a birthday cakewith Cathy and Becky (154 errors).The performance drop on the Seth corpus can beexplained by a number of factors.
First and fore-most, Seth is widely considered in the literature tobe the child who is most likely to invalidate any the-ory (Wilson and Peters, 1988).
He exhibits falsestarts and filler syllables extensively, and his syn-tax violates many ?universal?
principles.
This isreflected in the annotation scheme: the Seth cor-pus, following the annotation of Peters (1983), isabundant with filler syllables.
Because there wasno appropriate GR label for representing the syn-tactic relationships involving the filler syllables, weannotated those with a special GR (not used duringparser training), which the parser is understandablynot able to produce.
Filler syllables usually occurnear the start of the sentence, and once the parserfailed to label them, it could not accurately label theremaining GRs.
Other difficulties in the Seth cor-pus include the usage of dates, of which there wereno instances in the Eve corpus.
The parser had notbeen trained on the new DATE GR and subsequentlyfailed to parse it.6 ConclusionWe described an annotation scheme for represent-ing syntactic information as grammatical relationsin CHILDES data, a manually curated gold-standardcorpus of 65,000 words annotated according to thisGR scheme, and a parser that was trained on the an-notated corpus and produces highly accurate gram-matical relations for both child and adult utterances.These resources are now freely available to the re-search community, and we expect them to be in-strumental in psycholinguistic investigations of lan-guage acquisition and child language.Syntactic analysis of child language transcriptsusing a GR scheme of this kind has already beenshown to be effective in a practical setting, namelyin automatic measurement of syntactic developmentin children (Sagae et al, 2005).
That work relied ona phrase-structure statistical parser (Charniak, 2000)trained on the Penn Treebank, and the output of thatparser had to be converted into CHILDES grammat-ical relations.
Despite the obvious disadvantage ofusing a parser trained on a completely different lan-guage genre, Sagae et al (2005) demonstrated howcurrent natural language processing techniques canbe used effectively in child language work, achiev-ing results that are close to those obtained by man-ual computation of syntactic development scores forchild transcripts.
Still, the use of tools not tailoredfor child language and extra effort necessary to makethem work with community standards for child lan-guage transcription present a disincentive for childlanguage researchers to incorporate automatic syn-tactic analysis into their work.
We hope that the GR31representation scheme and the parser presented herewill make it possible and convenient for the childlanguage community to take advantage of some ofthe recent developments in natural language parsing,as was the case with part-of-speech tagging whenCHILDES specific tools were first made available.Our immediate plans include continued improve-ment of the parser, which can be achieved at least inpart by the creation of additional training data fromother English CHILDES corpora.
We also plan torelease automatic syntactic analyses for the entireEnglish portion of CHILDES.Although we have so far focused exclusively onEnglish CHILDES data, dependency schemes basedon functional relationships exist for a number of lan-guages (Buchholz and Marsi, 2006), and the generalparsing techniques used in the present work havebeen shown to be effective in several of them (Nivreet al, 2006).
As future work, we plan to adaptexisting dependency-based annotation schemes andapply our current syntactic annotation and pars-ing framework to other languages in the CHILDESdatabase.AcknowledgmentsWe thank Marina Fedner for her help with annota-tion of the Eve corpus.
This work was supported inpart by the National Science Foundation under grantIIS-0414630.ReferencesA.
Berger, S. A. Della Pietra, and V. J. Della Pietra.
1996.Amaximum entropy approach to natural language pro-cessing.
Computational Linguistics, 22(1):39?71.Roger Brown.
1973.
A first language: the early stages.George Allen & Unwin Ltd., London.Sabine Buchholz and Erwin Marsi.
2006.
Conll-x sharedtask on multilingual dependency parsing.
In Proceed-ings of the Tenth Conference on Computational Nat-ural Language Learning (CoNLL-X), pages 149?164,New York City, June.
Association for ComputationalLinguistics.John Carroll, Edward Briscoe, and Antonio Sanfilippo.1998.
Parser evaluation: a survey and a new proposal.In Proceedings of the 1st International Conference onLanguage Resources and Evaluation, pages 447?454,Granada, Spain.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the first conference on NorthAmerican chapter of the Association for Computa-tional Linguistics, pages 132?139, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.D.
Knuth.
1965.
On the translation of languages fromleft to right.
Information and Control, 8(6):607?639.Brian MacWhinney.
2000.
The CHILDES Project: Toolsfor Analyzing Talk.
Lawrence Erlbaum Associates,Mahwah, NJ, third edition.Joakim Nivre, Johan Hall, Jens Nilsson, Gulsen Eryigit,and Svetoslav Marinov.
2006.
Labeled pseudo-projective dependency parsing with support vector ma-chines.
In Proceedings of the Tenth Conference onComputational Natural Language Learning.Joakim Nivre, editor.
2007.
CoNLL-XI Shared Task onMultilingual Dependency Parsing, Prague, June.
As-sociation for Computational Linguistics.Ann M. Peters.
1983.
The Units of Language Acquisi-tion.
Monographs in Applied Psycholinguistics.
Cam-bridge University Press, New York.Ann M. Peters.
1987.
The role of immitation in the de-veloping syntax of a blind child.
Text, 7:289?311.Kenji Sagae and Alon Lavie.
2006.
A best-first prob-abilistic shift-reduce parser.
In Proceedings of theCOLING/ACL 2006 Main Conference Poster Sessions,pages 691?698, Sydney, Australia, July.
Associationfor Computational Linguistics.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependency pars-ing and domain adaptation with lr models and parserensembles.
In Proceedings of the Eleventh Conferenceon Computational Natural Language Learning.Kenji Sagae, Alon Lavie, and Brian MacWhinney.
2004.Adding syntactic annotations to transcripts of parent-child dialogs.
In Proceedings of the Fourth Interna-tional Conference on Language Resources and Evalu-ation (LREC 2004), Lisbon, Portugal.Kenji Sagae, Alon Lavie, and Brian MacWhinney.
2005.Automatic measurement of syntactic development inchild language.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics (ACL?05), pages 197?204, Ann Arbor, Michigan,June.
Association for Computational Linguistics.B.
Wilson and Ann M. Peters.
1988.
What are youcookin?
on a hot?
: A three-year-old blind child?s ?vi-olation?
of universal constraints on constituent move-ment.
Language, 64:249?273.32
