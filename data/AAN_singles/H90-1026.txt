BBN ATIS System Progress Report- June 1990M.
Bates, R. Bobrow, S. Boisen, R. Ingria, D. StallardBBN Systems and Technologies Corp.l0 Moulton SlreetCambridge, MA 02138AbstractThis paper eports recent progress on the development ofthe Delphi natural anguage component of the BBN spokenlanguage system for the ATIS domain, focussing on thecomparative evaluation performed by NIST in Jtme, 1990.1.
Delphi and Parlance/LearnerDelphi is BBN's research NL system (formerly calledCFG), which is based on a unification grarnmax and whichincorporates emantics into the unification framework.Delphi is the NL component of the BBN HARC System.Delphi can be changed very quickly, but has no easy way tobuild knowledge bases rapidly.The Parlance TM system is a commercial NL interface torelational databases, and is based on an ATN parser andgrammar.
Parlance has the advantage of an extensiveknowledge acquisition system called the Learner TM, so ourpreviously reported approach (Ingria and Ramshaw 1989) hasbeen to use the Learner to create a lexicon withmorphological nd syntactic information, a domain modelwith semantic information, and mapping rules from thedomain model to the database.
These knowledge bases werethen imported for use by Delphi.As a side-effect of using the Learner, a Parlanceconfiguration for the ATIS domain was created, so resultsusing that system are reported here for comparison.2.
Analysis of Delphi's Performanceon the Blind TestThe original score of the BBN Delphi system on the 93sentence ATIS blind test was 53 sentences correct, 2sentences not correct and 38 sentences not answered.Of the two sentences judged not correct, one is the resultof a mistake in the canonical answer set provided by HIST.The other esulted from a lack of agreement on which table afare is computed from.
(There are a few cases in the ATISdatabase where information can be retrieved by severaldifferent paths, sometimes resulting in different data.)
Thesereulsts indicate that the production of incorrect answers isnot a significant problem for Delphi, so the remainder ofthis section focusses on the sentences not answered.Several causes account for the 38 sentences not answered,with the primm-y one being words of word senses that werenot seen in the training material, or which simply did getentered into the lexicon.
Another important cause was thefailure on the part of the system to infer a relationshipbetween known word senses that a human user would haverecognized as implicit.
A quantitative breakdown is givenin Table 1, and more detailed iscussion follows.ReasonWord senses missing:Lack of inference:Grammar:SNOR bugs:Miscellaneous:#14105%37%26%13%5%7 18%TOTAL 38Table 1.
Analysis of BBN Delphi Performance on"No Answer" Queries.2.1 Word Senses Not Previously EncounteredAn example of a word not seen in the training is the verb"to service', seen in the following test set sentence:I need information on airlines servicing Boston flyingfrom Dallas.The word "service" was present only in noun form in theIraining data, as in "class of service'.
Here it clearly hasthe same meaning as the verb "serve'.Another example of a word meaning not seen in thetraining but a ~ g  in the test was "fly" in the following:.What type of aircraft iv flying United Airlines fli&ht 953?Normally, "fly" appears intransitively, and is something aflight does by itself.
In the sense of "fly" seen above, itwould appear that "flYing" is something an aircraft does TOa fright.
Both this example and the one above are trivial tocorrect: the relevant word sense simply needs to be added tothe lexicon.The major concept in the domain that we did not coverwas the notion of a "ticket'.
Radwx than associate the word125"dcket" with a concept on its own, as something differentfrom the meanings of "flight" and "fare', we chose to makeit synonymous with the word "fare'.
This meant hat thefollowing sentence did not appear meaningful to Delphi:Are there any excursion fares for round-trip tickets fromDallas to Boston?interpretations which a human being could easily haveproduced in response to the same queries (e.g., whether "thedistance from San Francisco airport to downtown" refersonly to downtown San Francisco o?
whether it also includesdowntown Oakland); !
came from a minor problem inhandling SNOR input; 1 was the result of a wrong canonicalanswer, only I represented a bug in the system.Phrases such as "excursion fares for round-trip fares"would have similarly been meaningless.
The solution forthis case simply requires Omta new concept for "ticket" beadded to the domain model, that its relations with alre~0yexisting concepts be estabfished, and that the word "ticket"be associated with this concept.
Eight sentences in theblind test used the word "ticket" in this way.2.2 Lack of InferenceThe second most important category is one wherecombinations of existing word senses occur which, whilemeaningful to a human being, do not appear meaningful tothe system because it is unable to infer a missing element.An example from the ATIS test setOf the 28 queries Parlance did not answer, 13 involvednew words of word senses, 4 required inference beyond thecapabilities of the system, 5 were not parsable be~uase theyinvolved structures that we believe occur commonly only inspoken language (Parlance's grammar has been conmuctedfor typed input, and we did not make changes in thegrammar specifically to accommodate spoken language), 2involved a minor pcoblem with SNOR input, and 4 were theresult of miscellaneous other" problems.Most of the problem queries could be handled by Parlancewith very little additional effort; within a day after the blindtest, Parlance understood correctly 83 of the 93 test queries(S9%).I need flight times from Boston to Dallas leaving onSaturday morning.The problem here is that *flight times" are times, andLimes are not "from Boston", nor do they "leave on Saturdaymorning'.
The utterance would make sense, of course,given the following paraphrase which any human speakercould easily supply:I need flight times for flights from Bostom to Dallasleaving on Saturday morning.There are number of examples in the ATIS test set of thiskind, including several in which information about ticketsthat "fly" f~0m place A to place B is req _u,~___e~l~Building an inference facility into the system has been agoal of our project even before the present test exercise, andwe feel that the structure of our knowledge representationmechanisms (see particularly the description in Bobrow,Ingria & Stallard, 1990) will enable us to undertake thiseffort in the near feting.2.3 Other ReasonsOther understanding failures arose due to grammar issues(notably, the lack of preposed PP complements), a minorproblem in our handling of SNOR input (e.g. ""
12rid" for"" 12th") which affected the performance of 3 queries, anderrors in other phases of the system.3.
Parlance PerformanceA few comments are in order alx)m the performance of theParlance system on the test data.
Parlance achieved 58correct answers, 7 not conec~ and 28 not answered.
Of the7 which were classified as incorrect, 4 of them came from4.
ConclusionsThere is evidence that intra-speaker varability in linguisticslxucture is farily low, but that inter-speakers variability isvery high.
In other words, a given speaker, at least in asingle session, tends to use the same forms over and overagain (e.g., "tickets flying'), and each new speaker (at leastso far) tends to use locutions different from previousThis leads us to conclude that much more training dam isneeded in order to adequately prepare for evaluations,particularly when the test material is drawn from subjectsnot represented in the training ,t~m It would further inoeasethe validity of the test if more than one domain weae used.Ot~ approach of having using the Learner for knowledgeacquisition and the Delphi system as the primary NLcomponent of HARC is successful and should be continued.ReferencesIngria, Robert J.P. and Lance Ramshaw, Porting to NewDomains Using the Learner TM, in Proceedings of theDAPRA Speech and Natural Language W~rkzhop, ~r ,1989.Bobrow, Robert, Robert J.P. lngria, and David Stallard,Syntactic and Semantic Knowledge inUnification Granunar,this volume.126
