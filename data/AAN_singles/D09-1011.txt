Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 101?110,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPGraphical Models over Multiple Strings?Markus Dreyer and Jason EisnerDepartment of Computer Science / Johns Hopkins UniversityBaltimore, MD 21218, USA{markus,jason}@cs.jhu.eduAbstractWe study graphical modeling in the case of string-valued random variables.
Whereas a weightedfinite-state transducer can model the probabilis-tic relationship between two strings, we are inter-ested in building up joint models of three or morestrings.
This is needed for inflectional paradigmsin morphology, cognate modeling or language re-construction, and multiple-string alignment.
Wepropose a Markov Random Field in which eachfactor (potential function) is a weighted finite-statemachine, typically a transducer that evaluates therelationship between just two of the strings.
Thefull joint distribution is then a product of these fac-tors.
Though decoding is actually undecidable ingeneral, we can still do efficient joint inferenceusing approximate belief propagation; the nec-essary computations and messages are all finite-state.
We demonstrate the methods by jointly pre-dicting morphological forms.1 OverviewThis paper considers what happens if a graphicalmodel?s variables can range over strings of un-bounded length, rather than over the typical finitedomains such as booleans, words, or tags.
Vari-ables that are connected in the graphical model arerelated by some weighted finite-state transduction.Graphical models have become popular in ma-chine learning as a principled way to work withcollections of interrelated random variables.
Mostoften they are used as follows:1.
Build: Manually specify the n variables ofinterest; their domains; and the possible di-rect interactions among them.2.
Train: Train this model?s parameters ?
toobtain a specific joint probability distributionp(V1, .
.
.
, Vn) over the n variables.3.
Infer: Use this joint distribution to predictthe values of various unobserved variablesfrom observed ones.
?Supported by the Human Language Technology Centerof Excellence at Johns Hopkins University, and by NationalScience Foundation grant No.
0347822 to the second author.Note that 1. requires intuitions about the domain;2. requires some choice of training procedure; and3.
requires a choice of exact or approximate infer-ence algorithm.Our graphical models over strings are naturalobjects to investigate.
We motivate them withsome natural applications in computational lin-guistics (section 2).
We then give our formalism:a Markov Random Field whose potential functionsare rational weighted languages and relations (sec-tion 3).
Next, we point out that inference is in gen-eral undecidable, and explain how to do approxi-mate inference using message-passing algorithmssuch as belief propagation (section 4).
The mes-sages are represented as weighted finite-state ma-chines.Finally, we report on some initial experimentsusing these methods (section 7).
We use incom-plete data to train a joint model of morphologicalparadigms, then use the trained model to completethe data by predicting unseen forms.2 MotivationThe problem of mapping between different formsand representations of strings is ubiquitous in nat-ural language processing and computational lin-guistics.
This is typically done between stringpairs, where a pronunciation is mapped to itsspelling, an inflected form to its lemma, a spellingvariant to its canonical spelling, or a name istransliterated from one alphabet into another.However, many problems involve more than justtwo strings:?
in morphology, the inflected forms of a (possi-bly irregular) verb are naturally considered to-gether as a whole morphological paradigm inwhich different forms reinforce one another;?
mapping an English word to its foreign translit-eration may be easier when one considers theorthographic and phonological forms of bothwords;?
similar cognates in multiple languages are nat-urally described together, in orthographic orphonological representations, or both;101?
modern and ancestral word forms form a phylo-genetic tree in historical linguistics;?
in bioinformatics and in system combination,multiple sequences need to be aligned in orderto identify regions of similarity.We propose a unified model for multiple stringsthat is suitable for all the problems mentionedabove.
It is robust and configurable and canmake use of task-specific overlapping features.
Itlearns from observed and unobserved, or latent, in-formation, making it useful in supervised, semi-supervised, and unsupervised settings.3 Formal Modeling Approach3.1 VariablesA Markov Random Field (MRF) is a joint modelof a set of random variables, V = {V1, .
.
.
, Vn}.We assume that all variables are string-valued, i.e.the value of Vimay be any string ?
?
?i, where ?iis some finite alphabet.We may use meaningful names for the integersi, such as V2SAfor the 2nd singular past form of averb.The assumption that all variables are string-valued is not crucial; it merely simplifies ourpresentation.
It is, however, sufficient for manypractical purposes, since most other discrete ob-jects can be easily encoded as strings.
For exam-ple, if V1is a part of speech tag, it may be en-coded as a length-1 string over the finite alphabet?1def= {Noun,Verb, .
.
.
}.3.2 FactorsA Markov Random Field defines a probability foreach assignment A of values to the variables in V:p(A)def=1Zm?j=1Fj(A) (1)This distribution over assignments is specified bythe collection of factors Fj: A 7?
R?0.
Eachfactor (or potential function) is a function that de-pends on only a subset of A.Fig.
1 displays an undirected factor graph, inwhich each factor is connected to the variablesthat it depends on.
F1, F3, F5in this example areunary factors because each one scores the valueof a single variable, while F2, F4, F6are binaryfactors.F2F6F5F3F1F4VinfV2SAV3SEFigure 1: Example of a factor graph.
Black boxes representfactors, circles represent variables (infinitive, 2nd past, and3rd present-tense forms of the same verb; different samplesfrom the MRF correspond to different verbs).
Binary factorsevaluate how well one string can be transduced into another,summing over all transducer paths (i.e., alignments, whichare not observed in training).In our setting, we will assume that each unaryfactor is specified by a weighted finite-state au-tomaton (WFSA) whose weights fall in the semir-ing (R?0,+,?).
Thus the score F3(.
.
.
, V2SA=x, .
.
.)
is the total weight of all paths in the F3?sWFSA that accept the string x ?
??2SA.
Eachpath?s weight is the product of its component arcs?weights, which are non-negative.Similarly, we assume that each binary factor isspecified by a weighted finite-state transducer(WFST).
Such a model is essentially a generaliza-tion of stochastic edit distance (Ristad and Yian-ilos, 1996) in which the edit probabilities can bemade sensitive to a finite summary of context.Formally, a WFST is an automaton that resem-bles a weighted FSA, but it nondeterministicallyreads two strings x, y in parallel from left to right.The score of (x, y) is given by the total weight ofall accepting paths in the WFST that map x to y.For example, different paths may consider variousmonotonic alignments of x with y, and we sumover these mutually exclusive possibilities.1A factor might depend on k > 2 variables.
Thisrequires a k-tape weighted finite-state machine(WFSM), an obvious generalization where eachpath reads k strings in some alignment.2To ensure that Z is finite in equation (1), we canrequire each factor to be a ?proper?
WFSM, i.e.,its accepting paths have finite total weight (even ifthe WFSM is cyclic, with infinitely many paths).1Each string is said to be on a different ?tape,?
which hasits own ?read head,?
allowing the WFSM to maintain a sep-arate position in each string.
Thus, a path in a WFST mayconsume any number of characters from x before consumingthe next character from y.2Weighted acceptors and transducers are the cases k = 1and k = 2, which are said to define rational languages andrational relations.1023.3 ParametersOur probability model has trainable parameters: avector of feature weights ?
?
R. Each arc in eachWFSM has a real-valued weight that depends on ?.Thus, tuning ?
during training will change the arcweights, hence the path weights, the factor func-tions, and the whole probability distribution p(A).Designing the probability model includes spec-ifying the topology and weights of each WFSM.Eisner (2002) explains how to specify and trainsuch parameterized WFSMs.
Typically, theweight of an arc is a simple sum like ?12+ ?55+?72, where ?12is included on all arcs that sharefeature 12.
However, more interesting parameter-izations arise if the WFSM is constructed by op-erations such as transducer composition, or from aweighted regular expression.3.4 Power of the formalismFactored finite-state string models (1) were orig-inally suggested by the second author, in Kempeet al (2004).
That paper showed that even in theunweighted case, such models could be used to en-code relations that could not be recognized by anyk-tape FSM.
We offer a more linguistic exampleas a small puzzle.
We invite the reader to spec-ify a factored model (consisting of three FSTs asin Fig.
1) that assigns positive probability to justthose triples of character strings (x, y, z) that havethe form (red ball, ball red, red), (white house,house white, white), etc.
This uses the auxiliaryvariable Z to help encode a relation between Xand Y that swaps words of unbounded length.
Bycontrast, no FSM can accomplish such unboundedswapping, even with 3 or more tapes.Such extra power might be linguistically useful.Troublingly, however, Kempe et al (2004) alsoobserved that the framework is powerful enough toexpress computationally undecidable problems.3This implies that to work with arbitrary models,we will need approximate methods.4Fortunately,the graphical models community has already de-3Consider a simple model with two variables and two bi-nary factors: p(V1, V2)def=1Z?
F1(V1, V2) ?
F2(V1, V2).
Sup-pose F1is 1 or 0 according to whether its arguments areequal.
Under this model, p() < 1 iff there exists a stringx 6=  that can be transduced to itself by the unweightedtransducer F2.
This question can be used to encode any in-stance of Post?s Correspondence Problem, so is undecidable.4Notice that the simplest approximation to cure undecid-ability would be to impose an arbitrary maximum on stringlength, so that the random variables have a finite domain, justas in most discrete graphical models.VFU?V?F ?F?UFigure 2: Illustration of messages being passed from variableto factor and factor to variable.
Each message is representedby a finite-state acceptor.veloped many such methods, to deal with the com-putational intractability (if not undecidability) ofexact inference.4 Approximate InferenceIn this paper, we focus on how belief propagation(BP)?a simple well-known method for approxi-mate inference in MRFs (Bishop, 2006)?can beused in our setting.
BP in its general form hasnot yet been widely used in the NLP community.5However, it is just a generalization to arbitraryfactor graphs of the familiar forward-backward al-gorithm (which operates only on chain-structuredfactor graphs).
The algorithm becomes approxi-mate (and may not even converge) when the factorgraphs have cycles.
(In that case it is more prop-erly called ?loopy belief propagation.?
)4.1 Belief propagationWe first sketch how BP works in general.
Eachvariable V in the graphical model maintains a be-lief about its value, in the form of a marginal dis-tribution p?Vover the possible values of V .
Thefinal beliefs are the output of the algorithm.Beliefs arise from messages that are sent be-tween the variables and factors along the edges ofthe factor graph.
Variable V sends factor F a mes-sage ?V?F, which is an (unnormalized) probabil-ity distribution over V ?s values v, computed by?V?F(v) :=?F?
?N (V ),F?6=F?F?
?V(v) (2)where N is the set of neighbors of V in the graph-ical model.
This message represents a consensusof V ?s other neighboring factors concerning V ?svalue.
It is how V tells F what its belief p?Vwouldbe if F were absent.
Informally, it communicatesto F : Here is what my value would be if it were upto my other neighboring factors F?to determine.5Notable exceptions are Sutton et al (2004) for chunkingand tagging, Sutton and McCallum (2004) for informationextraction, Smith and Eisner (2008) for dependency parsing,and Cromier`es and Kurohashi (2009) for alignment.103The factor F can then collect such incomingmessages from neighboring variables and send itsown message on to another neighbor U .
Such amessage ?F?Usuggests good values for U , in theform of an (unnormalized) distribution over U ?svalues u, computed by?F?U(u) :=?A s.t.A[U ]=uF (A)?U?
?N (F ),U?6=U?U??F(A[U?
])(3)where A is an assignment to all variables, andA[U ] is the value of variable U in that assign-ment.
This message represents F ?s prediction ofU ?s value based on its other neighboring variablesU?.
Informally, via this message, F tells U : Hereis what I would like your value to be, based onthe messages that my other neighboring variableshave sent me about their values, and how I wouldprefer you to relate to them.Thus, each edge of the factor graph maintainstwo messages ?V?F, ?F?V.
All messages areupdated repeatedly, in some order, using the twoequations above, until some stopping criterion isreached.6The beliefs are then computed:p?V(v)def=?F?N (V )?F?V(v) (4)If variable V is observed, then the right-handsides of equations (2) and (4) are modified to tellV that it must have the observed value v. This isdone by multiplying in an extra message ?obs?Vthat puts probability 1 on v7and 0 on other val-ues.
That affects other messages and beliefs.
Thefinal belief at each variable estimates its posteriormarginal under the MRF (1), given all observa-tions.4.2 Finite-state messages in BPBoth ?V?Fand ?F?Vare unnormalized distribu-tions over the possible values of V ?in our case,strings.
A distribution over strings is naturallyrepresented by a WFSA.
Thus, belief propagationtranslates to our setting as follows:?
Each message is a WFSA.?
Messages are typically initialized to a one-stateWFSA that accepts all strings in ?
?, each with6Preferably when the beliefs converge to some fixed point(a local minimum of the Bethe free energy).
However, con-vergence is not guaranteed.7More generally, on all possible observed variables.weight 1.8?
Taking a pointwise product of messages to V inequation (2) corresponds to WFSA intersection.?
If F in equation (3) is binary,9then there is onlyone U?.
Then the outgoing message ?F?U, aWFSA, is computed as domain(F ?
?U?
?F).Here ?
composes the factor WFST with the in-coming message WFSA, yielding a WFST thatgives a joint distribution over (U,U?).
Thedomain operator projects this WFST onto the Uside to obtain a WFSA, which corresponds tomarginalizing to obtain a distribution over U .?
In general, F is a k-tape WFSM.
Equation (3)?composes?
k ?
1 of its tapes with k ?
1 in-coming messages ?U?
?F, to construct a jointdistribution over the k variables in N (F ), thenprojects onto the kthtape to marginalize over thek?1 U?variables and get a distribution over U .All this can be accomplished by theWFSM gen-eralized composition operator  (Kempe et al,2004).After projecting, it is desirable to determinizethe WFSA.
Otherwise, the summation in (3) isonly implicit?the summands remain as distinctpaths in theWFSA10?and thus theWFSAs wouldget larger and larger as BP proceeds.
Unfortu-nately, determinizing a WFSA still does not guar-antee a small result.
In fact it can lead to expo-nential blowup, or even infinite blowup.11Thus,in practice we recommend against determinizingthe messages, which may be inherently complex.To shrink a message, it is safer to approximate itwith a small deterministic WFSA, as discussed inthe next section.4.3 Approximation of messagesIn our domain, it is possible for the finite-statemessages to grow unboundedly in size as they flowaround a cycle.
After all, our messages are notjust multinomial distributions over a fixed finite8This is an (improper) uniform distribution over ??.
Al-though is not a proper WFSA (see section 3.2), there is anupper bound on the weights it assigns to strings.
That guar-antees that all the messages and beliefs computed by (2)?
(4)will be proper FSMs, provided that all the factors are properWFSMs.9If it is unary, (3) trivially reduces to ?F?U= F .10The usual implementation of projection does not changethe topology of the WFST, but only deletes the U?part of itsarc labels.
Thus, multiple paths that accept the same value ofU remain distinct according to the distinct values of U?thatthey were paired with before projection.11If there is no deterministic equivalent (Mohri, 1997).104set.
They are distributions over the infinite set ?
?.A WFSA represents this in finite space, but morecomplex distributions require bigger WFSAs, withmore distinct states and arc weights.Facing the same problem for distributions overthe infinite set R, Sudderth et al (2002) simplifiedeach message ?V?F, approximating a complexGaussian mixture by using fewer components.We could act similarly, variationally approxi-mating a large WFSA P with a smaller one Q.Choose a family of message approximations (suchas bigram models) by specifying the topology fora (small) deterministic WFSA Q.
Then chooseQ?s edge weights to minimize the KL divergenceKL(P ?Q).
This can be done in closed form.12Another possible procedure?used in the ex-periments of this paper?approximates ?V?Fbypruning it back to a finite set of most plausiblestrings.13Equation (2) requests an intersectionof several WFSAs, e.g., ?F1?V?
?F2?V?
?
?
?
.List all strings that appear on any of the 1000-best paths in any of these WFSAs, removing du-plicates.
Let?Q be a uniform distribution over thiscombined list of plausible strings, represented asa determinized, minimized, acyclic WFSA.
Nowapproximate the intersection of equation (2) as((?Q ?
?F1?V) ?
?F2?V) ?
?
?
?
.
This is efficientto compute and has the same topology as?Q.5 Training the Model ParametersAny standard training method for MRFs willtransfer naturally to our setting.
In all cases wedraw on Eisner (2002), who showed how to trainthe parameters ?
of a single WFST, F , to (locally)maximize the joint or conditional probability offully or partially observed training data.
This in-volves computing the gradient of that likelihoodfunction with respect to ?.1412See Li et al (2009, footnote 9) for a sketch of the con-struction, which finds locally normalized edge weights.
Orif Q is large but parameterized by some compact parametervector ?, so we are only allowed to control its edge weightsvia ?, then Li and Eisner (2009, section 6) explain how tominimize KL(P ?Q) by gradient descent.
In both cases Qmust be deterministic.We remark that if a factor F were specified by a syn-chronous grammar rather than a WFSM, then its outgoingmessages would be weighted context-free languages.
Exactintersection of these is undecidable, but they too can be ap-proximated variationally by WFSAs, with the same methods.13We are also considering other ways of adaptively choos-ing the topology of WFSA approximations at runtime, partic-ularly in conjunction with expectation propagation.14The likelihood is usually non-convex; even when thetwo strings are observed (supervised training), their acceptingWe must generalize this to train a product ofWFSMs.
Typically, training data for an MRF (1)consists of some fully or partially observed IIDsamples of the joint distribution p(V1, .
.
.
Vn).
Itis well-known how to tune an MRF?s parameters ?by stochastic gradient descent to locally maximizethe probability of this training set, even thoughboth the probability and its gradient are in generalintractable to compute in an MRF.
The gradient isa sum of quantities, one for each factor Fj.
Whilethe summand for Fjcannot be computed exactly,it can be estimated using the BP messages to Fj.Roughly speaking, the gradient for Fjis computedmuch as in supervised training (see above), buttreating any message ?Vi?Fjas an uncertain ob-servation of Vi?a form of noisy supervision.15Our concerns about training are the same asfor any MRF.
First of all, BP is approximate.Kulesza and Pereira (2008) warn that its estimatesof the gradient can be misleading.
Second, semi-supervised training (which we will attempt below)is always difficult and prone to local optima.
Asin EM, a small number of supervised examples forsome variable may be drowned out by many nois-ily reconstructed examples.Faster and potentially more stable approachesinclude the piecewise training methods of Sut-ton and McCallum (2008), which train the factorsindependently or in small groups.
In the semi-supervised case, each factor can be trained on onlythe supervised forms available for it.
It might beuseful to reweight the trained factors (cf.
Smith etal.
(2005)), or train the factors consecutively (cf.Fahlman and Lebiere (1990)), in a way that mini-mizes the loss of BP decoding on held-out data.6 Comparison With Other Approaches6.1 Multi-tape WFSMsIn principle, one could use a 100-tape WFSM tojointly model the 100 distinct forms of a typicalPolish verb.
In other words, the WFSM would de-scribe the distribution of a random variable~V =?V1, .
.
.
, V100?, where each Viis a string.
Onewould train the parameters of the WFSM on asample of~V , each sample being a fully or partiallyobserved paradigm for some Polish verb.
The re-sulting distribution could be used to infer missingforms for these or other verbs.path through the WFST may be ambiguous and unobserved.15See Bishop (2006), or consult Smith and Eisner (2008)for notation close to that of this paper.105As a simple example, either a morphologicalgenerator or a morphological analyzer might needthe probability that krzycza?oby is the neuter third-person singular conditional imperfective of krzy-cze?c, despite never having observed it in training.The model determines this probability based onother observed and hypothesized forms of krzy-cze?c, using its knowledge of how neuter third-person singular conditional imperfectives are re-lated to these other forms in other verbs.Unfortunately, such a 100-tape WFSM wouldbe huge, with an astronomical number of arcs(each representing a possible 100-way edit opera-tion).
Our approach is to factor the problem into anumber of (e.g.)
pairwise relationships among theverb forms.
Using a factored distribution has sev-eral benefits over the k-tape WFSM: (1) a smallerrepresentation in memory, (2) a small numberof parameters to learn, (3) efficient approximatecomputation that takes advantage of the factoredstructure, (4) the ability to reuse WFSAs and WF-STs previously developed for smaller problems,(5) additional modeling power.6.2 Simpler graphical models on stringsSome previous researchers have used factoredjoint models of several strings.
To our knowledge,they have all chosen acyclic, directed graphicalmodels.
The acyclicity meant that exact inferencewas at least possible for them, if not necessarily ef-ficient.
The factors in these past models have beenWFSTs (though typically simpler than the ones wewill use).Many papers have used cascades of probabilis-tic finite-state transducers.
Such a cascade maybe regarded as a directed graphical model with alinear-chain structure.
Pereira and Riley (1997)built a speech recognizer in this way, relatingacoustic to phonetic to lexical strings.
Simi-larly, Knight and Graehl (1997) presented a gen-erative cascade using 4 variables and 5 factors:p(w, e, j, k, o)def= p(w) ?p(e | w) ?p(j | e) ?p(k | j)?p(o | k) where e is an English word sequence, wits pronunciation, j a Japanese version of the pro-nunciation, k a katakana rendering of the Japanesepronunciation, and o an OCR-corrupted version ofthe katakana.
Knight and Graehl used finite-stateoperations to perform inference at test time, ob-serving o and recovering the most likely w, whilemarginalizing out e, j, and k.Bouchard-C?ot?e et al (2009) reconstructed an-cient word forms given modern equivalents.
Theyused a directed graphical model, whose tree struc-ture reflected the evolutionary development of themodern languages, and which included latent vari-ables for historical intermediate forms that werenever observed in training data.
They used Gibbssampling rather than an exact solution (possible ontrees) or a variational approximation (like our BP).Our work seeks to be general in terms of thegraphical model structures used, as well as effi-cient through the use of BP with approximate mes-sages.
We also seek to avoid local normalization,using a globally normalized model.166.3 Unbounded objects in graphical modelsWe distinguish our work from ?dynamic?
graph-ical models such as Dynamic Bayesian Networksand Conditional Random Fields, where the stringbrechen would be represented by creating 7 letter-valued variables.
Those methods can representstrings (or paths) of any length?but the length foreach training or test string must be specified in ad-vance, not inferred.
Furthermore, it is awkwardand costly to model unknown alignments, sincethe variables are position-specific, and any posi-tion in brechen could in principle align with anyposition in brichst.
WFSTs are a much more natu-ral and flexible model of string pairs.We also distinguish our work from current non-parametric Bayesian models, which sometimesgenerate unbounded strings, trees, or grammars.
Ifthey generate two unbounded objects, they modeltheir relationship by a single synchronous genera-tion process (akin to Section 6.1), rather than bya globally normalized product of overlapping fac-tors.7 ExperimentsTo study our approach, we conducted initial ex-periments that reconstruct missing word forms inmorphological paradigms.
In inflectional mor-phology, each uninflected verb form (lemma) isassociated with a vector of forms that are inflectedfor tense, person, number, etc.
Some inflectedforms may be observed frequently in natural text,others rarely.
Two variables that are usually pre-dictable from each other may or may not keep thisrelationship in the case of an irregular verb.16Although we do normalize locally during piecewisetraining (see section 7.3).106(a) # paradigms 9,393(b) # finite forms per paradigm 9(c) # hidden finite forms per paradigm (avg.)
8.3(d) # paradigms with some finite form(s) observed 2,176(e) In (d), # of finite forms observed (avg.)
3.4Table 1: Statistics of our training data.Our task is to reconstruct (generate) specific un-observed morphological forms in a paradigm bylearning from observed ones.
This is a particu-larly interesting semisupervised scenario, becausedifferent subsets of the variables are observed ondifferent examples.7.1 Experimental dataWe used orthographic rather than phonologicalforms.
We extracted morphological paradigms forall 9393 German verbs in the CELEX morpholog-ical database.
Each paradigm lists 5 present-tenseand 4 past-tense indicative forms, as well as theverb?s lemma, for a total of 10 string-valued vari-ables.17In each paradigm, we removed, or hid,verb forms that occur only rarely in natural text,i.e, verb forms with a small frequency figure pro-vided by CELEX.18All paradigms other than sein(?to be?)
were now incompletely observed.
Table 1gives some statistics.7.2 Model factors and parametersOur current MRF uses only binary factors.
Eachfactor is aWFST that is trained to relate 2 of the 10variables (morphological forms).
Each WFST canscore an aligned pair using a log-linear model thatcounts features in a sliding 3-character window.To score an unaligned pair, it sums over all pos-sible alignments.
Specifically, our WFST topol-ogy and parameterization follow the state-of-the-art approach to supervised morphology in Dreyeret al (2008), although we dropped some of theirfeatures to speed up these early experiments.19We17Some pairs of forms are always identical in German,hence are treated as a single form by CELEX.
We likewiseuse a single variable?these are the ?1,3?
variables in Fig.
3.Occasionally a form is listed as UNKNOWN.
We neithertrain nor evaluate on such forms, although the model will stillpredict them.18The frequency figure for each word form is based oncounts in the Mannheim News corpus.
We hide forms withfrequency < 10.19We dropped their latent classes and regions as well asfeatures that detected which characters were orthographicvowels.
Also, we retained their ?target language model fea-tures?
only in the baseline ?U?
model, since elsewhere theyimplemented and manipulated all WFSMs usingthe OpenFST library (Allauzen et al, 2007).7.3 Training in the experimentsWe trained ?
on the incompletely observedparadigms.
As suggested in section 5, we useda variant of piecewise pseudolikelihood training(Sutton and McCallum, 2008).
Suppose there isa binary factor F attached to forms U and V .
Forany value of ?, we can define pUV(U | V ) fromthe tiny MRF consisting only of U , V , and F .We can therefore compute the goodness LUVdef=log pUV(ui| vi)+logV U(vi| ui),20summed overall observed (U, V ) pairs in training data.
We at-tempted to tune ?
to maximize the total LUVoverall U, V pairs,21regularized by subtracting ||?||2.Note that different factors thus enjoyed differentamounts of observed training data, but trainingwas fully supervised (except for the unobservedalignments between uiand vi).7.4 Inference in the experimentsAt test time, we are given each lemma (e.g.brechen) and all its observed (frequent) inflectedforms (e.g., brachen, bricht,.
.
.
), and are asked topredict the remaining (rarer) forms (e.g., breche,brichst, .
.
.
).We run approximate joint inference using be-lief propagation.22We extract our output from thefinal beliefs: for each unseen variable V , we pre-seemed to hurt in our current training setup.We followed Dreyer et al (2008) in slightly pruning thespace of possible alignments.
We compensated by replacingtheir WFST, F , with the union F ?
10?12(0.999?
?
?
)?.This ensured that the factor could still map any string to anyother string (though perhaps with very low weight), guaran-teeing that the intersection at the end of section 4.3 would benon-empty.20The second term is omitted if V is the lemma.
We donot train the model to predict the lemma since it is alwaysobserved in test data.21Unfortunately, just before press time we discovered thatthis was not quite what we had done.
A shortcut in our im-plementation trained pUV(U | V ) and pV U(V | U) sepa-rately.
This let them make different use of the (unobserved)alignments?so that even if each individually liked the pair(u, v), they might not have been able to agree on the sameaccepting path for it at test time.
This could have slightlyharmed our joint inference results, though not our baselines.22To derive the update order for message passing, we takean arbitrary spanning tree over the factor graph, and let O bea list of all factors and variables that is topologically sortedaccording to the spanning tree, with the leaves of the treecoming first.
We then discard the spanning tree.
A single it-eration visits all factors and variables in order of O, updatingeach one?s messages to later variables and factors, and thenvisits all factors and variables in reverse order, updating eachone?s messages to earlier variables and factors.107dict its value to be argmaxvp?V(v).
This predic-tion considers the values of all other unseen vari-ables but sums over their possibilities.
This is theBayes-optimal decoder for our scoring function,since that function reports the fraction of individ-ual forms that were predicted perfectly.237.5 Model selection of MRF topologyIt is hard to know a priori what the causal relation-ships might be in a morphological paradigm.
Inprinciple, one would like to automatically choosewhich factors to have in the MRF.
Or one couldstart with many factors, but use methods such asthose suggested in section 5 to learn that certainless useful factors should be left weak to avoidconfusing loopy BP.For our present experiments, we simply com-pared several fixed model topologies (Fig.
3).These were variously unconnected (U), chaingraphs (C1,.
.
.
, C4), trees (T1, T2), or loopygraphs (L1,.
.
.
, L4).
We used several factor graphsthat differ only by one or two added factors andcompared the results.
The graphs were designedby hand; they connect some forms with similarmorphological properties more or less densely.We trained different models using the observedforms in the 9393 paradigms as training data.
Thefirst 100 paradigms were then used as develop-ment data for model selection:24we were giventhe answers to their hidden forms, enabling us tocompare the models.
The best model was thenevaluated on the 9293 remaining paradigms.7.6 Development data resultsThe models are compared on development datain Table 2.
Among the factor graphs we evalu-ated, we find that L4 (see Fig.
3) performs bestoverall (whole-word accuracy 82.1).
Note that theunconnected graph U does not perform very well(69.0), but using factor graphs with more connect-ing factors generally helps overall accuracy (seeC1?C3).
Note, however, that in some cases the ad-ditional structure hurts: The chain model C4 andthe loopy model L1 perform relatively badly.
The23If we instead wished to maximize the fraction of entireparadigms that were predicted perfectly, then we would haveapproximated full MAP decoding over the paradigm (Viterbidecoding) by using max-product BP.
Other loss functions(e.g., edit distance) would motivate other decoding methods.24Using these paradigms was simply a quick way to avoidmodel selection by cross-validation.
If data were really assparse as our training setup pretends (see Table 2), then 100complete paradigms would be too valuable to squander asmere development data.
(U)1Pres PastSingularPlural231,321,3221,31231,321,3221,31231,321,3221,3(C1) (C2)(C3)1231,321,3221,31231,321,3221,3(C4)1231,321,3221,3(T1)1231,321,3221,3(T2) (L1)1231,321,3221,3(L2)1231,321,3221,31231,321,3221,3(L3)Pres Past Pres Past Pres Past Pres PastSingularPlural1231,321,3221,3(L4)Figure 3: The graphs that we evaluate on development data.The nodes represent morphological forms, e.g.
the first nodein the left of each graph represents the first person singularpresent.
Each variable is also connected to the lemma (notshown).
See results in Table 2.reason for such a performance degradation is thatundertrained factors were used: The factors relat-ing second-person to second-person forms, for ex-ample, are trained from only 8 available examples.Non-loopy models always converge (exactly) inone iteration (see footnote 22).
But even our loopymodels appeared to converge in accuracy withintwo iterations.
Only L3 and L4 required the sec-ond iteration, which made tiny improvements.7.7 Test data resultsBased on the development results, we selectedmodel L4 and tested on the remaining 9293paradigms.We regard the unconnected model U as a base-line to improve upon.
We also tried a rather differ-ent baseline as in (Dreyer et al, 2008).
We trainedthe machine translation toolkit Moses (Koehn etal., 2007) to translate groups of letters rather thangroups of words (?phrases?).
For each form fto be predicted, we trained a Moses model onall supervised form pairs (l, f) available in thedata, to learn a prediction for the form given thelemma l. The M,3 condition restricted Moses use?phrases?
no longer than 3 letters, comparable toour own trigram-based factors (see section 7.2).M,15 could use up to 15 letters.Again, our novel L4 model far outperformedthe others overall.
Breaking the results down byform, we find that this advantage mainly comesfrom the 3 forms with the fewest observed train-ing examples (Table 3, first 3 rows).
The M andU models are barely able to predict these forms atall from the lemma, but L4 can predict them bet-108Unconn.
Chains Trees LoopsU C1 C2 C3 C4 T1 T2 L1 L2 L3 L469.0 72.9 73.4 74.8 65.2 78.1 78.7 62.3 79.6 78.9 82.1Table 2: Whole-word accuracies of the different models in reconstructing the missing forms in morphological paradigms, hereon 100 verbs (development data).
The names refer to the graphs in Fig.
3.
We selected L4 as final model (Table 3).Form # obs.
M,3 M,15 U L42.Sg.Pa.
4 0.0 0.2 0.8 69.72.Pl.Pa.
9 0.9 1.1 1.4 45.62.Sg.Pr.
166 49.4 62.6 74.7 90.51.Sg.Pr.
285 99.6 98.8 99.3 97.21,3.Pl.Pa.
673 46.5 78.3 75.0 75.61,3.Sg.Pa.
1124 65.0 88.8 84.0 74.82.Pl.Pr.
1274 98.3 99.2 99.0 96.43.Sg.Pr.
1410 91.0 95.9 95.2 88.21,3.Pl.Pr.
1688 99.8 98.9 99.8 98.0All 6633 59.2 67.3 68.0 81.2Table 3: Whole-word accuracies on the missing forms from9293 test paradigms.
The Moses baselines and our un-connected model (U) predict each form separately from thelemma, which is always observed.
L4 uses all observationsjointly, running belief propagation for decoding.
Moses,15memorizes phrases of length up to 15, all other models usemax length 3.
The table is sorted by the column ?# obs.
?,which reports the numbers of observations for a given form.ter by exploiting other observed or latent forms.By contrast, well-trained forms were already easyenough for the M and U models that L4 had littlenew to offer and in fact suffered from its approxi-mate training and/or inference.Leaving aside the comparisons, it was useful toconfirm that loopy BP could be used in this set-ting at all.
8014 of the 9293 test paradigms had?
2 observed forms (in addition to the lemma)but ?
7 missing forms.
One might have expectedthat loopy BP would have failed to converge, orconverged to the wrong thing.
Nonetheless, itachieved quite respectable success at exactly pre-dicting various inflected forms.For the curious, Table 4 shows accuraciesgrouped by different categories of paradigms,where the category is determined by the numberof missing forms to predict.
Most paradigms fallin the category where 7 to 9 forms are missing, sothe accuracies in that line are similar to the overallaccuracies in Table 3.8 ConclusionsWe have proposed that one can jointly model sev-eral multiple strings by using Markov RandomFields.
We described this formally as an undi-# missing # paradig.
M,3 M,15 U L41?3 205 20.3 20.8 26.8 74.44?6 1037 44.2 50.5 52.7 82.87?9 8014 60.6 68.8 69.4 81.1Table 4: Accuracy on test data, reported separately forparadigms in which 1?3, 4?6, or 7?9 forms are missing.Missing words have CELEX frequency count< 10; these arethe ones to predict.
(The numbers in col. 2 add up to 9256,not 9293, since some paradigms are incomplete in CELEX tobegin with, with no forms to be removed or evaluated.
)rected graphical model with string-valued vari-ables and whose factors (potential functions) aredefined by weighted finite-state transducers.
Eachfactor evaluates some subset of the strings.Approximate inference can be done by loopybelief propagation.
The messages take the formof weighted finite-state acceptors, and are con-structed by standard operations.
We explainedwhy the messages might become large, and gavemethods for approximating them with smallermessages.
We also discussed training methods.We presented some pilot experiments on thetask of jointly predicting multiple missing verbforms in morphological paradigms.
The factorswere simplified versions of statistical finite-statemodels for supervised morphology.
Our MRFfor this task might be used not only to conjugateverbs (e.g., in MT), but to guide further learningof morphology?either active learning from a hu-man or semi-supervised learning from the distri-butional properties of a raw text corpus.Our modeling approach is potentially applicableto a wide range of other tasks, including translit-eration, phonology, cognate modeling, multiple-sequence alignment and system combination.Our work ties into a broader vision of using al-gorithms like belief propagation to coordinate thework of several NLP models and algorithms.
Eachindividual factor considers some portion of a jointproblem, using classical statistical NLP methods(weighted grammars, transducers, dynamic pro-gramming).
The factors coordinate their work bypassing marginal probabilities.
Smith and Eisner(2008) reported complementary work in this vein.109ReferencesCyril Allauzen, Michael Riley, Johan Schalkwyk, Wo-jciech Skut, and Mehryar Mohri.
2007.
OpenFst: Ageneral and efficient weighted finite-state transducerlibrary.
In Proc.
of CIAA, volume 4783 of LectureNotes in Computer Science, pages 11?23.Christopher M. Bishop.
2006.
Pattern Recognitionand Machine Learning.
Springer.Alexandre Bouchard-C?ot?e, Thomas L. Griffiths, andDan Klein.
2009.
Improved reconstruction of pro-tolanguage word forms.
In Proc.
of HLT-NAACL,pages 65?73, Boulder, Colorado, June.
Associationfor Computational Linguistics.Fabien Cromier`es and Sadao Kurohashi.
2009.
Analignment algorithm using belief propagation and astructure-based distortion model.
In Proc.
of EACL,pages 166?174, Athens, Greece, March.
Associationfor Computational Linguistics.Markus Dreyer, Jason Smith, and Jason Eisner.
2008.Latent-variable modeling of string transductionswith finite-state methods.
In Proc.
of EMNLP, Hon-olulu, Hawaii, October.Jason Eisner.
2002.
Parameter estimation for prob-abilistic finite-state transducers.
In Proc.
of ACL,pages 1?8, Philadelphia, July.Scott E. Fahlman and Christian Lebiere.
1990.
Thecascade-correlation learning architecture.
TechnicalReport CMU-CS-90-100, School of Computer Sci-ence, Carnegie Mellon University.Andr?e Kempe, Jean-Marc Champarnaud, and JasonEisner.
2004.
A note on join and auto-intersectionof n-ary rational relations.
In Loek Cleophas andBruce Watson, editors, Proceedings of the Eind-hoven FASTAR Days (Computer Science Techni-cal Report 04-40).
Department of Mathematics andComputer Science, Technische Universiteit Eind-hoven, Netherlands.Kevin Knight and Jonathan Graehl.
1997.
Machinetransliteration.
In Proc.
of ACL, pages 128?135.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProc.
of ACL, Companion Volume, pages 177?180,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Alex Kulesza and Fernando Pereira.
2008.
Structuredlearning with approximate inference.
In Proc.
ofNIPS.Zhifei Li and Jason Eisner.
2009.
First- and second-order expectation semirings with applications tominimum-risk training on translation forests.
InProc.
of EMNLP.Zhifei Li, Jason Eisner, and Sanjeev Khudanpur.
2009.Variational decoding for statistical machine transla-tion.
In Proc.
of ACL.Mehryar Mohri.
1997.
Finite-state transducers in lan-guage and speech processing.
Computational Lin-guistics, 23(2).Fernando C. N. Pereira and Michael Riley.
1997.Speech recognition by composition of weighted fi-nite automata.
In Emmanuel Roche and YvesSchabes, editors, Finite-State Language Processing.MIT Press, Cambridge, MA.Eric Sven Ristad and Peter N. Yianilos.
1996.
Learn-ing string edit distance.
Technical Report CS-TR-532-96, Princeton University, Department of Com-puter Science, October.David Smith and Jason Eisner.
2008.
Dependencyparsing by belief propagation.
In Proc.
of EMNLP.Andrew Smith, Trevor Cohn, and Miles Osborne.2005.
Logarithmic opinion pools for conditionalrandom fields.
In Proc.
of ACL, pages 18?25, June.Erik B. Sudderth, Alexander T. Ihler, Er T. Ihler,William T. Freeman, and Alan S. Willsky.
2002.Nonparametric belief propagation.
In Proc.
ofCVPR, pages 605?612.Charles Sutton and Andrew McCallum.
2004.
Collec-tive segmentation and labeling of distant entities ininformation extraction.
In ICML Workshop on Sta-tistical Relational Learning and Its Connections toOther Fields.Charles Sutton and Andrew McCallum.
2008.
Piece-wise training for structured prediction.
MachineLearning.
In submission.Charles Sutton, Khashayar Rohanimanesh, and An-drew McCallum.
2004.
Dynamic conditional ran-dom fields: Factorized probabilistic models for la-beling and segmenting sequence data.
In Proc.
ofICML.110
