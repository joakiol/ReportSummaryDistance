Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 11?21,Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational LinguisticsAcoustic transformations to improve the intelligibility of dysarthric speechFrank RudziczUniversity of Toronto, Department of Computer Science6 King?s College RoadToronto, Ontario, Canadafrank@cs.toronto.eduAbstractThis paper describes modifications to acous-tic speech signals produced by speakers withdysarthria in order to make those utter-ances more intelligible to typical listeners.These modifications include the correction oftempo, the adjustment of formant frequenciesin sonorants, the removal of aberrant voic-ing, the deletion of phoneme insertion errors,and the replacement of erroneously droppedphonemes.
Through simple evaluations of in-telligibility with na?
?ve listeners, we show thatthe correction of phoneme errors results in thegreatest increase in intelligibility and is there-fore a desirable mechanism for the eventualcreation of augmentative application softwarefor individuals with dysarthria.1 IntroductionDysarthria is a set of neuromotor disorders that im-pair the physical production of speech.
These im-pairments reduce the normal control of the primaryvocal articulators but do not affect the regular com-prehension or production of meaningful, syntacti-cally correct language.
For example, damage to therecurrent laryngeal nerve reduces control of vocalfold vibration (i.e., phonation), which can result inaberrant voicing.
Inadequate control of soft palatemovement caused by disruption of the vagus cra-nial nerve may lead to a disproportionate amount ofair being released through the nose during speech(i.e., hypernasality).
The lack of articulatory controlalso leads to various involuntary non-speech soundsincluding velopharyngeal or glottal noise (Rosenand Yampolsky, 2000).
More commonly, a lackof tongue and lip dexterity often produces heavilyslurred speech and a more diffuse and less differen-tiable vowel target space (Kent and Rosen, 2004).The neurological damage that causes dysarthriausually affects other physical activity as well whichcan have a drastically adverse affect on mobilityand computer interaction.
For instance, severelydysarthric speakers are 150 to 300 times slower thantypical users in keyboard interaction (Hosom et al,2003; Hux et al, 2000).
However, since dysarthricspeech is often only 10 to 17 times slower than thatof typical speakers (Patel, 1998), speech is a viableinput modality for computer-assisted interaction.Consider a dysarthric individual who must travelinto a city by public transportation.
This might in-volve purchasing tickets, asking for directions, or in-dicating intentions to fellow passengers, all withina noisy and crowded environment.
A personalportable communication device in this scenario (ei-ther hand-held or attached to a wheelchair) wouldtransform relatively unintelligible speech spokeninto a microphone to make it more intelligible beforebeing played over a set of speakers.
Such a systemcould facilitate interaction and overcome difficult orfailed attempts at communication in daily life.We propose a system that avoids drawbacks ofother voice-output communication aids that outputonly synthetic speech.
Before software for such adevice is designed, our goal is to establish and evalu-ate a set of modifications to dysarthric speech to pro-duce a more intelligible equivalent.
Understandingthe utility of each of these techniques will be crucialto effectively designing the proposed system.112 Background and related workHawley et al (2007) described an experiment inwhich 8 dysarthric individuals (with either cere-bral palsy or multiple sclerosis) controlled non-critical devices in their home (e.g., TV) with auto-matic speech recognition.
Command vocabulariesconsisted of very simple phrases (e.g., ?TV chan-nel up?, ?Radio volume down?)
and feedback wasprovided to the user either by visual displays orby auditory cues.
This speech-based environmen-tal control was compared with a ?scanning?
inter-face in which a button is physically pressed to it-eratively cycle through a list of alternative com-mands, words, or phrases.
While the speech inter-face made more errors (between 90.8% and 100%accuracy after training) than the scanning inter-face (100% accuracy), the former was significantlyfaster (7.7s vs 16.9s, on average).
Participants com-mented that speech was significantly less tiring thanthe scanning interface, and just as subjectively ap-pealing (Hawley et al, 2007).
Similar results wereobtained in other comparisons of speech and scan-ning interfaces (Havstam, Buchholz, and Hartelius,2003), and command-and-control systems (Green etal., 2003).
Speech is a desirable method of expres-sion for individuals with dysarthria.
There are manyaugmentative communication devices that employsynthetic text-to-speech in which messages can bewritten on a specialized keyboard or played backfrom a repository of pre-recorded phrases (Messinaand Messina, 2007).
This basic system architec-ture can be modified to allow for the replacementof textual input with spoken input.
However, sucha scenario would involve some degree of automaticspeech recognition, which is still susceptible to faultdespite recent advances (Rudzicz, 2011).
Moreover,the type of synthetic speech output produced by suchsystems often lacks a sufficient degree of individualaffectation or natural expression that one might ex-pect in typical human speech (Kain et al, 2007).
Theuse of prosody to convey personal information suchas one?s emotional state is generally not supportedby such systems but is nevertheless a key part of ageneral communicative ability.Transforming one?s speech in a way that pre-serves the natural prosody will similarly also pre-serve extra-linguistic information such as emotions,and is therefore a pertinent response to the limita-tions of current technology.
Kain et al (2007) pro-posed the voice transformation system shown in fig-ure 1 which produced output speech by concatenat-ing together original unvoiced segments with syn-thesized voiced segments that consisted of a super-position of the original high-bandwidth signal withsynthesized low-bandwidth formants.
These synthe-sized formants were produced by modifications toinput energy, pitch generation, and formant mod-ifications.
Modifications to energy and formantswere performed by Gaussian mixture mapping, asdescribed below, in which learned relationships be-tween dysarthric and target acoustics were used toproduce output closer to the target space.
This pro-cess was intended to be automated, but Kain et al(2007) performed extensive hand-tuning and manu-ally identified formants in the input.
This will obvi-ously be impossible in a real-time system, but theseprocesses can to some extent be automated.
For ex-ample, voicing boundaries can be identified by theweighted combination of various acoustic features(e.g., energy, zero-crossing rate) (Kida and Kawa-hara, 2005; Hess, 2008), and formants can be iden-tified by the Burg algorithm (Press et al, 1992) orthrough simple linear predictive analysis with con-tinuity constraints on the identified resonances be-tween adjacent frames (O?Shaughnessy, 2008).Spectral modifications traditionally involve fil-tering or amplification methods such as spectralsubtraction or harmonic filtering (O?Shaughnessy,2000), but these are not useful for dealing with moreserious mispronounciations (e.g., /t/ for /n/).
Ho-som et al (2003) showed that Gaussian mixturemapping can be used to transform audio from oneset of spectral acoustic features to another.
Duringanalysis, context-independent frames of speech areanalyzed for bark-scaled energy and their 24th ordercepstral coefficients.For synthesis, a cepstral analysis approximatesthe original spectrum, and a high-order linear pre-dictive filter is applied to each frame, and excitedby impulses or white noise (for voiced and unvoicedsegments).
Hosom et al (2003) showed that given99% human accuracy in recognizing normal speechdata, this method of reconstruction gave 93% accu-racy on the same data.
They then trained a transfor-mative model between dysarthric and regular speech12Audio recordingsVoicingdetector 2-bandfilter bankEnergyanalysis FormantanalysisEnergymodification F0generation FormantmodificationFormantsynthesis+Overlapp-addInput speechvoicedunvoicedhighpass lowpassenergyenergy'CV boundariesformantsformants'F0''voiced'Figure 1: Voice transformation system proposed by Kainet al (2007).using aligned, phoneme-annotated, and orthograph-ically identical sentences spoken by dysarthric andregular speakers, and a Gaussian Mixture Model(GMM) to model the probability distribution of thedysarthric source spectral features x as the sum ofD normal distributions with mean vector ?, diago-nal covariance matrix ?, and prior probability ?
:p(x) =D?d=1?dN (x;?d,?d) .
(1)The GMM parameters were trained in an unsuper-vised mode using the expectation-maximization al-gorithm and 1, 2, 4, 8, and 16 mixture components,with D = 4 apparently being optimal.
A probabilis-tic least-squares regression mapped the source fea-tures x onto the target (regular speaker) features y,producing the model Wd(x) + bd for each class, anda simple spectral distortion is performed to produceregularized versions of dysarthic speech y?:y?
(x) =D?d=1hd(x) (Wd(x) + bd) (2)for posterior probabilities hd(x).
This model is in-teresting in that it explicitly maps the acoustic differ-ences for different features between disordered andregular speech1.
Reconstructing the dysarthric spec-trum in this way to sound more ?typical?
while leav-ing pitch (F0), timing, and energy characteristics in-tact resulted in a 59.4% relative error rate reduction(68% to 87% accuracy) among a group of 18 naivehuman listeners each of whom annotated a total of206 dysarthric test words (Hosom et al, 2003).3 The TORGOMorph transformationsTORGOMorph encapsulates of a number of trans-formations of the acoustics uttered by speakers withdysarthria.
Each modification is implemented in re-action to a particular effect of dysarthria on intelligi-bility as determined by observations on the TORGOdatabase of dysarthric speech (Rudzicz, Namasi-vayam, and Wolff, 2011).
Currently, these modifica-tions are uniformly preceded by noise reduction us-ing spectral subtraction and either phonological orphonemic annotations.
This latter step is currentlynecessary, since certain modifications require eitherknowledge of the manner of articulation or the iden-tities of the vowel segments, as explained below.The purpose of this exercise is to determine whichmodifications result in the most significant improve-ments to intelligibility, so the correct annotation se-quence is vital to avoid the introduction of an ad-ditional dimension of error.
Therefore, the annota-tions used below are extracted directly from the pro-fessional markup in the TORGO database.
In prac-tice, however, phonemic annotations determined au-tomatically by speech recognition would be imper-fect, which is why investigations of this type oftenforgo that automation altogether (e.g., see Kain etal.
(2007)).
Possible alternatives to full ASR are dis-cussed in section 5.In some cases, the dysarthric speech must be com-pared or supplemented with another vocal source.Here, we synthesize segments of speech using atext-to-speech application developed by Black andLenzo (2004).
This system is based on the Uni-versity of Edinburgh?s Festival tool and synthesizesphonemes using a standard method based on lin-1This model can also be used to measure the difference be-tween any two types of speech.13ear predictive coding with a pronunciation lexiconand part-of-speech tagger that assists in the selectionof intonation parameters (Taylor, Black, and Caley,1998).
This system is invoked by providing the ex-pected text uttered by the dysarthic speaker.
In or-der to properly combine this purely synthetic sig-nal and the original waveforms we require identicalsampling rates, so we resample the former by a ra-tional factor using a polyphase filter with low-passfiltering to avoid aliasing (Hayes, 1999).
Since thediscrete phoneme sequences themselves can differ,we find an ideal alignment between the two by theLevenshtein algorithm (Levenshtein, 1966), whichprovides the total number of insertion, deletion, andsubstitution errors.The following sections detail the components ofTORGOMorph, which is outlined in figure 2.
Thesecomponents allow for a cascade of one transfor-mation followed by another, although we can alsoperform these steps independently to isolate theireffects.
In all cases, the spectrogram is derivedwith the fast Fourier transform given 2048 bins onthe range of 0?5 kHz.
Voicing boundaries are ex-tracted in a unidimensional vector aligned with thespectrogram using the method of Kida and Kawa-hara (2005) which uses GMMs trained with zero-crossing rate, amplitude, and the spectrum as in-put parameters.
A pitch (F0) contour is also ex-tracted from the source by the method proposed byKawahara et al (2005), which uses a Viterbi-like po-tential decoding of F0 traces described by cepstraland temporal features.
That work showed an errorrate of less than 0.14% in estimating F0 contours ascompared with simultaneously-recorded electroglot-tograph data.
These contours are not in general mod-ified by the methods proposed below, since Kain etal.
(2007) showed that using original F0 results inthe highest intelligibility among alternative systems.Over a few segments, however, these contours cansometimes be decimated in time during the modi-fication proposed in section 3.3 and in some casesremoved entirely (along with all other acoustics) inthe modification proposed in section 3.2.3.1 High-pass filter on unvoiced consonantsThe first acoustic modification is based on the ob-servation that unvoiced consonants are improperlyvoiced in up to 18.7% of plosives (e.g.
/d/ for /t/)Input acousticsTransformed acousticsSpectral subtractionHigh-pass filtering  of voiceless consonants (section 3.1)Splicing: correcting pronunciation errors (section 3.2)Morphing in time (section 3.3) Morphing in frequency (section 3.4)Figure 2: Outline of the TORGOMorph system.
Theblack path indicates the cascade to be used in practice.Solid arrows indicate paths taken during evaluation.and up to 8.5% of fricatives (e.g.
/v/ for /f/) indysarthric speech in the TORGO database.
Voicedconsonants are typically differentiated from their un-voiced counterparts by the presence of the voice bar,which is a concentration of energy below 150 Hzindicative of vocal fold vibration that often persiststhroughout the consonant or during the closure be-fore a plosive (Stevens, 1998).
Empirical analysisof TORGO data suggests that for at least two maledysarthric speakers this voice bar extends consider-ably higher, up to 250 Hz.In order to correct these mispronunciations, thevoice bar is filtered out of all acoustic sub-sequencesannotated as unvoiced consonants.
For this task weuse a high-pass Butterworth filter, which is ?maxi-mally flat?
in the passband2 and monotonic in mag-nitude in the frequency domain (Butterworth, 1930).Here, this filter is computed on a normalized fre-quency range respecting the Nyquist frequency, sothat if a waveform?s sampling rate is 16 kHz, thenormalized cutoff frequency for this component isf?Norm = 250/(1.6?
104/2) = 3.125?
10?2.
TheButterworth filter is an all-pole transfer function be-tween signals, and we use the 10th-order low-pass2The passband is the frequency range in which the compo-nent magnitudes in the original signal should not be changed.14Butterworth filter whose magnitude response is|B(z; 10)|2 = |H(z; 10)|2 =11 +(jz/jz?Norm)2?10(3)where z is the complex frequency in polar coordi-nates and z?Norm is the cutoff frequency in that do-main (Hayes, 1999).
This allows the transfer func-tionB(z; 10) = H(z; 10) =11 + z10 +?10i=1 ciz10?i(4)whose poles occur at known symmetric intervalsaround the unit complex-domain circle (Butter-worth, 1930).
These poles are then transformedby the Matlab function zp2ss, which produces thestate-space coefficients ?i and ?i that describe theoutput signal resulting from applying the low-passButterworth filter to the discrete signal x[n].
Thesecoefficients are further converted by~a = z?Norm~?
?1~b = ?z?Norm(~??1~?)
(5)giving the high-pass Butterworth filter with the samecutoff frequency of z?Norm.
This continuous systemis converted to the discrete equivalent through theimpulse-invariant discretization method and is im-plemented by the difference equationy[n] =10?k=1aky[n?
k] +10?k=0bkx[n?
k].
(6)As previously mentioned, this equation is applied toeach acoustic sub-sequence annotated as unvoicedconsonants, thereby smoothly removing the energybelow 250 Hz.3.2 Splicing: correcting dropped and insertedphoneme errorsThe Levenshtein algorithm finds a best possiblealignment of the phoneme sequence in actually ut-tered speech and the expected phoneme sequence,given the known word sequence.
Isolating phonemeinsertions and deletions are therefore a simple matterof iteratively adjusting the source speech accordingto that alignment.
There are two cases where actionis required:insertion error In this case a phoneme is presentwhere it ought not be.
In the TORGO database,these insertion errors tend to be repetitions ofphonemes occurring in the first syllable of aword, according to the International SpeechLexicon Dictionary (Hasegawa-Johnson andFleck, 2007).
When an insertion error is iden-tified the entire associated segment of the sig-nal is simply removed.
In the case that the as-sociated segment is not surrounded by silence,adjacent phonemes can be merged togetherwith time-domain pitch-synchronous overlap-add (Moulines and Charpentier, 1990).deletion error The vast majority of accidentallydeleted phonemes in the TORGO database arefricatives, affricates, and plosives.
Often, theseinvolve not properly pluralizing nouns (e.g.,book instead of books).
Given their high pre-ponderance of error, these phonemes are theonly ones we insert into the dysarthric sourcespeech.
Specifically, when the deletion of aphoneme is recognized with the Levenshteinalgorithm, we simply extract the associatedsegment from the aligned synthesized speechand insert it into the appropriate spot in thedysarthric speech.
For all unvoiced fricatives,affricates, and plosives no further action is re-quired.
When these phonemes are voiced, how-ever, we first extract and remove the F0 curvefrom the synthetic speech, linearly interpolatethe F0 curve from adjacent phonemes in thesource dysarthric speech, and resynthesize withthe synthetic spectrum and interpolated F0.
Ifinterpolation is not possible (e.g., the syntheticvoiced phoneme is to be inserted beside an un-voiced phoneme), we simply generate a flat F0equal to the nearest natural F0 curve.3.3 Morphing in timeFigure 3 exemplifies that vowels uttered bydysarthric speakers are significantly slower thanthose uttered by typical speakers.
In fact, sonorantscan be twice as long in dysarthric speech, on aver-age (Rudzicz, Namasivayam, and Wolff, 2011).
Inthis modification, phoneme sequences identified assonorant are simply contracted in time in order to beequal in extent to the greater of half their original15(a) (b)Figure 3: Repetitions of /iy p ah/ over 1.5s by (a) a malespeaker with athetoid CP, and (b) a female control in theTORGO database.
Dysarthric speech is notably slowerand more strained than regular speech.length or the equivalent synthetic phoneme?s length.In all cases this involved shortening the dysarthricsource sonorant.Since we wish to contract the length of a signalsegment here without affecting its pitch or frequencycharacteristics, we use a phase vocoder based ondigital short-time Fourier analysis (Portnoff, 1976).Here, Hamming-windowed segments of the sourcephoneme are analyzed with a z-transform givingboth frequency and phase estimates for up to 2048frequency bands.
During pitch-preserving time-scaled warping, we specify the magnitude spectrumdirectly from the input magnitude spectrum withphase values chosen to ensure continuity (Sethares,2007).
Specifically, for the frequency band at fre-quency F and frames j and k > j in the modifiedspectrogram, the phase ?
is predicted by?
(F )k = ?
(F )j + 2piF (j ?
k).
(7)In our case the discrete warping of the spectrograminvolves simple decimation by a constant factor.
Thespectrogram is then converted into a time-domainsignal modified in tempo but not in pitch relativeto the original phoneme segment.
This conversionis accomplished simply through the inverse Fouriertransform.3.4 Morphing in frequencyFormant trajectories inform the listener as to theidentities of vowels, but the vowel space ofdysarthric speakers tends to be constrained (Kainet al, 2007).
In order to improve a listener?s abil-ity to differentiate between the vowels, this modifi-cation component identifies formant trajectories inthe acoustics and modifies these according to theknown vowel identity of a segment.
Here, formantsare identified with a 14th-order linear-predictivecoder with continuity constraints on the identi-fied resonances between adjacent frames (Snell andMilinazzo, 1993; O?Shaughnessy, 2008).
Band-widths are determined by the negative natural log-arithm of the pole magnitude, as implemented inthe STRAIGHT analysis system (Banno et al, 2007;Kawahara, 2006).For each identified vowel in the dysarthricspeech3, formant candidates are identified at eachframe in time up to 5 kHz.
Only those time frameshaving at least 3 such candidates within 250 Hz ofexpected values are considered.
The expected valuesof formants are derived from analyses performed byAllen et al (1987).
Given these subsets of candidatetime frames in the vowel, the one having the highestspectral energy within the middle 50% of the lengthof the vowel is established as the anchor position,and the three formant candidates within the expectedranges are established as the anchor frequencies forformants F1 to F3.
If more than one formant can-didate falls within expected ranges, the one with thelowest bandwidth becomes the anchor frequency.Given identified anchor points and targetsonorant-specific frequencies and bandwidths,there are several methods to modify the spectrum.The most common may be to learn a statisticalconversion function based on Gaussian mixturemapping, as described earlier, typically preceded byalignment of sequences using dynamic time warping(Stylianou, 2008).
Here, we use the STRAIGHTmorphing implemented by Kawahara and Matsui(2003), among others.
The transformation of aframe of speech xA for speaker A is performed witha multivariate frequency-transformation functionTA?
given known targets ?
usingTA?
(xA) =?
xA0exp(log(?TA?(?)??))??=?
xA0exp((1?
r) log(?TAA(?)??
)+ r log(?TA?(?)??))??=?
xA0(?TA?(?)??)r?
?,(8)3Accidentally inserted vowels are also included here, unlesspreviously removed by the splicing technique in section 3.2.16time (ms)frequency (Hz)1300 1400 1500 1600 1700 1800 1900 20000500100015002000250030003500400045005000(a)time (ms)frequency (Hz)1300 1400 1500 1600 1700 1800 1900 20000500100015002000250030003500400045005000(b)Figure 4: Spectrograms for (a) the dysarthric original and(b) the frequency-modified renditions of the word fear.Circles represent indicative formant locations.where ?
is the frame-based time dimension andwhere 0 ?
r ?
1 is an interpolative rate at whichto perform morphing (i.e., r = 1 implies completeconversion of the parameters of speakerA to param-eter set ?
and r = 0 implies no conversion.)
(Kawa-hara et al, 2009).
An example of the results of thismorphing technique is shown in figure 4 in whichthe three identified formants are shifted to their ex-pected frequencies.This method tracks formants and warps the fre-quency space automatically, whereas Kain et al(2007) perform these functions manually.
A futureimplementation may use Kalman filters to reduce thenoise inherent in trajectory tracking.
Such an ap-proach has shown significant improvements in for-mant tracking, especially for F1 (Yan et al, 2007).4 Intelligibility experiments withTORGOMorphThe intelligibility of both purely synthetic and mod-ified speech signals can be measured objectively bysimply having a set of participants transcribe whatthey hear from a selection of word, phrase, or sen-tence prompts (Spiegel et al, 1990), although no sin-gle standard has emerged as pre-eminent (Schroeter,2008).
Hustad (2006) suggested that orthographictranscriptions provide a more accurate predictor ofintelligibility among dysarthric speakers than themore subjective estimates used in clinical settings,e.g., Enderby (1983).
That study had 80 listenerswho transcribed audio (which is an atypically largegroup for this task) and showed that intelligibilityincreased from 61.9% given only acoustic stimuli to66.75% given audiovisual stimuli on the transcrip-tion task in normal speech.
In the current work, wemodify only the acoustics of dysarthric speech; how-ever future work might consider how to prompt lis-teners in a more multimodal context.In order to gauge the intelligibility of our mod-ifications, we designed a simple experiment inwhich human listeners attempt to identify words insentence-level utterances under a number of acousticscenarios.
Sentences are either uttered by a speakerwith dysarthria, modified from their original sourceacoustics, or manufactured by a text-to-speech syn-thesizer.
Each participant is seated at a personalcomputer with a simple graphical user interface witha button which plays or replays the audio (up to 5times), a text box in which to write responses, anda second button to submit those responses.
Audio isplayed over a pair of headphones.
The participantsare told to only transcribe the words with which theyare reasonably confident and to ignore those thatthey cannot discern.
They are also informed thatthe sentences are grammatically correct but not nec-essarily semantically coherent, and that there is noprofanity.
Each participant listens to 20 sentencesselected at random with the constraints that at leasttwo utterances are taken from each category of au-dio, described below, and that at least five utter-ances are also provided to another listener, in orderto evaluate inter-annotator agreement.
Participantsare self-selected to have no extensive prior experi-ence in speaking with individuals with dysarthria,in order to reflect the general population.
Althoughdysarthric utterances are likely to be contextualizedwithin meaningful conversations in real-world situ-ations, such pragmatic aspects of discourse are notconsidered here in order to concentrate on acousticeffects alone.
No cues as to the topic or semanticcontext of the sentences are given, as there is noevidence that such aids to comprehension affect in-telligibility (Hustad and Beukelman, 2002).
In thisstudy we use sentence-level utterances uttered bymale speakers from the TORGO database.Baseline performance is measured on the originaldysarthric speech.
Two other systems are used forreference:Synthetic Word sequences are produced by theCepstral commercial text-to-speech system us-ing the U.S. English voice ?David?.
This sys-17tem is based on Festival in almost every respect,including its use of linguistic pre-processing(e.g., part-of-speech tagging) and rule-basedgeneration (Taylor, Black, and Caley, 1998).This approach has the advantage that every as-pect of the synthesized speech (e.g., the wordsequence) can be controlled although here, asin practice, synthesized speech will not mimicthe user?s own acoustic patterns, and will of-ten sound more ?mechanical?
due to artificialprosody (Black and Lenzo, 2007).GMM This system uses the Gaussian mixture map-ping type of modification suggested by Toda,Black, and Tokuda (2005) and Kain et al(2007).
Here, we use the FestVox implementa-tion of this algorithm, which includes pitch ex-traction, some phonological knowledge (Tothand Black, 2005), and a method for resynthe-sis.
Parameters for this model are trained by theFestVox system using a standard expectation-maximization approach with 24th-order cep-stral coefficients and 4 Gaussian components.The training set consists of all vowels utteredby a male speaker in the TORGO databaseand their synthetic realizations produced by themethod above.Performance is evaluated on the three other acous-tic transformations, namely those described in sec-tions 3.2, 3.3, and 3.4 above.
Tables 1 and 2 respec-tively show the percentage of words and phonemescorrectly identified by each listener relative to theexpected word sequence under each acoustic con-dition.
In each case, annotator transcriptions werealigned with the ?true?
or expected sequences us-ing the Levenshtein algorithm described in section3.
Plural forms of singular words, for example,are considered incorrect in word alignment althoughone obvious spelling mistake (i.e., ?skilfully?)
is cor-rected before evaluation.
Words are split into com-ponent phonemes according to the CMU dictionary,with words having multiple pronunciations given thefirst decomposition therein.In these experiments there is not enough data fromwhich to make definitive claims of statistical signifi-cance, but it is clear that the purely synthetic speechhas a far greater intelligibility than other approaches,more than doubling the average accuracy of theOrig.
GMM Synth.
Splice Time Freq.L01 22.1 15.6 82.0 40.2 34.7 35.2L02 27.8 12.2 75.5 44.9 39.4 33.8L03 38.3 14.8 76.3 37.5 12.9 21.4L04 24.7 10.8 72.1 32.6 22.2 18.4Avg.
28.2 13.6 76.5 38.8 27.3 27.2Table 1: Percentage of words correctly identified by eachlistener (L0*) relative to the expected sequence.
Sections3.2, 3.3, and 3.4 discuss the ?Splice?, ?Time?, and ?Freq.
?techniques, respectively.Orig.
GMM Synth.
Splice Time Freq.L01 52.0 43.1 98.2 64.7 47.8 55.1L02 57.8 38.2 92.9 68.9 50.6 53.3L03 50.1 41.4 96.8 57.1 30.7 46.7L04 51.6 33.8 88.7 51.9 43.2 45.0Avg.
52.9 39.1 94.2 60.7 43.1 50.0Table 2: Percentage of phonemes correctly identified byeach listener relative to the expected sequence.
Sections3.2, 3.3, and 3.4 discuss the ?Splice?, ?Time?, and ?Freq.
?techniques, respectively.TORGOMorph modifications.
The GMM transfor-mation method proposed by Kain et al (2007) gavepoor performance, although our experiments are dis-tinguished from theirs in that our formant traces aredetected automatically, rather than by hand.
The rel-ative success of the synthetic approach is not an ar-gument against the type of modifications proposedhere and by Kain et al (2007), since our aim is toavoid the use of impersonal and invariant utterances.Indeed, future study in this area should incorporatesubjective measures of ?naturalness?.
Further usesof acoustic modifications not attainable by text-to-speech synthesis are discussed in section 5.In all cases, the splicing technique of removingaccidentally inserted phonemes and inserting miss-ing ones gives the highest intelligibility relative toall acoustic transformation methods.
Although morestudy is required, this result emphasizes the impor-tance of lexically correct phoneme sequences.
In theword-recognition experiment, there are an averageof 5.2 substitution errors per sentence in the unmod-ified dysarthric speech against 2.75 in the syntheticspeech.
There are also 2.6 substitution errors on av-erage per sentence for the speech modified in fre-quency, but 3.1 deletion errors, on average, against0.24 in synthetic speech.
No correlation is found be-18tween the ?loudness?
of the speech (determined bythe overall energy in the sonorants) and intelligibil-ity results, although this might change with the ac-quisition of more data.
Neel (2009), for instance,found that loud or amplified speech from individu-als with Parkinson?s disease was more intelligible tohuman listeners than quieter speech.Our results are comparable in many respects to theexperiments of Kain et al (2007), although they onlylooked at simple consonant-vowel-consonant stim-uli.
Their results showed an average of 92% correctsynthetic vowel recognition (compared with 94.2%phoneme recognition in table 2) and 48% correctdysarthric vowel recognition (compared with 52.9%in table 2).
Our results, however, show that modi-fied timing and modified frequencies do not actuallybenefit intelligibility in either the word or phonemecases.
This disparity may in part be due to the factthat our stimuli are much more complex (quickersentences do not necessarily improve intelligibility).5 DiscussionThis work represents an inaugural step towardsspeech modification systems for human-human andhuman-computer interaction.
Tolba and Torgoman(2009) claimed that significant improvements in au-tomatic recognition of dysarthric speech are attain-able by modifying formants F1 and F2 to be moresimilar to expected values.
In that study, formantswere identified using standard linear predictive cod-ing techniques, although no information was pro-vided as to how these formants were modified norhow their targets were determined.
However, theyclaimed that modified dysarthric speech resultedin ?recognition rates?
(by which they presumablymeant word-accuracy) of 71.4% in the HTK speechrecognition system, as compared with 28% on theunmodified dysarthric speech from 7 individuals.The results in section 4 show that human listen-ers are more likely to correctly identify utterancesin which phoneme insertion and deletion errors arecorrected than those in which formant frequenciesare adjusted.
Therefore, one might hypothesizethat such pre-processing might provide even greatergains than those reported by Tolba and Torgoman(2009).
Ongoing work ought to confirm or deny thishypothesis.A prototypical client-based application based onour research for unrestricted speech transformationof novel sentences is currently in development.
Suchwork will involve improving factors such as accu-racy and accessibility for individuals whose neuro-motor disabilities limit the use of modern speechrecognition, and for whom alternative interactionmodalities are insufficient.
This application is beingdeveloped under the assumption that it will be usedin a mobile device embeddable within a wheelchair.If word-prediction is to be incorporated, the pre-dicted continuations of uttered sentence fragmentscan be synthesized without requiring acoustic input.In practice, the modifications presented here willhave to be based on automatically-generated anno-tations of the source audio.
This is especially im-portant to the ?splicing?
module in which word-identification is crucial.
There are a number of tech-niques that can be exercised in this area.
Czyzewski,Kaczmarek, and Kostek (2003) apply both a vari-ety of neural networks and rough sets to the taskof classifying segments of speech according to thepresence of stop-gaps, vowel prolongations, and in-correct syllable repetitions.
In each case, input in-cludes source waveforms and detected formant fre-quencies.
They found that stop-gaps and vowel pro-longations could be detected with up to 97.2% ac-curacy and that vowel repetitions could be detectedwith up to 90% accuracy using the rough set method.Accuracy was similar although slightly lower us-ing traditional neural networks (Czyzewski, Kacz-marek, and Kostek, 2003).
These results appeargenerally invariant even under frequency modifica-tions to the source speech.
Arbisi-Kelm (2010), forexample, suggest that disfluent repetitions can beidentified reliably through the use of pitch, dura-tion, and pause detection (with precision up to 93%(Nakatani, 1993)).
If more traditional models ofspeech recognition are to be deployed to identifyvowels, the probabilities that they generate acrosshypothesized words might be used to weight themanner in which acoustic transformations are made.The use of one?s own voice to communicate is adesirable goal, and continuations of this research aretherefore focused on the practical aspects of this re-search towards usable and portable systems.19ReferencesAllen, Jonathan, M. Sharon Hunnicutt, Dennis H. Klatt,Robert C. Armstrong, and David B. Pisoni.
1987.From text to speech: the MITalk system.
CambridgeUniversity Press, New York, NY, USA.Arbisi-Kelm, Timothy.
2010.
Intonation structure anddisfluency detection in stuttering.
Laboratory Phonol-ogy 10, 4:405?432.Banno, Hideki, Hiroaki Hata, Masanori Morise, ToruTakahashi, Toshio Irino, and Hideki Kawahara.
2007.Implementation of realtime STRAIGHT speech ma-nipulation system: Report on its first implementation.Acoustical Science and Technology, 28(3):140?146.Black, Alan W. and Kevin A. Lenzo.
2004.
Multilingualtext-to-speech synthesis.
In 2004 IEEE InternationalConference on Acoustics, Speech, and Signal Process-ing (ICASSP 2004).Black, Alan W. and Kevin A. Lenzo.
2007.
Buildingsynthetic voices.
http://www.festvox.org/festvox/bsv.ps.gz.Butterworth, Stephen.
1930.
On the theory of filter am-plifiers.
Experimental Wireless and the Wireless Engi-neer, 7:536?541.Czyzewski, Andrzej, Andrzej Kaczmarek, and BozenaKostek.
2003.
Intelligent processing of stutteredspeech.
Journal of Intelligent Information Systems,21(2):143?171.Enderby, Pamela M. 1983.
Frenchay Dysarthria Assess-ment.
College Hill Press.Green, Phil, James Carmichael, Athanassios Hatzis, PamEnderby, Mark Hawley, and Mark Parker.
2003.
Au-tomatic speech recognition with sparse training datafor dysarthric speakers.
In Proceedings of Eurospeech2003, pages 1189?1192, Geneva.Hasegawa-Johnson, Mark and Margaret Fleck.
2007.
In-ternational Speech Lexicon Project.
http://www.isle.illinois.edu/dict/.Havstam, Christina, Margret Buchholz, and LenaHartelius.
2003.
Speech recognition and dysarthria: asingle subject study of two individuals with profoundimpairment of speech and motor control.
LogopedicsPhoniatrics Vocology, 28:81?90(10), August.Hawley, Mark S., Pam Enderby, Phil Green, StuartCunningham, Simon Brownsell, James Carmichael,Mark Parker, Athanassios Hatzis, Peter O?Neill,and Rebecca Palmer.
2007.
A speech-controlledenvironmental control system for people with se-vere dysarthria.
Medical Engineering & Physics,29(5):586?593, June.Hayes, Monson H. 1999.
Digital Signal Processing.Schaum?s Outlines.
McGraw Hill.Hess, Wolfgang J.
2008.
Pitch and voicing determinationof speech with an extension toward music signal.
InJacob Benesty, M. Mohan Sondhi, and Yiteng Huang,editors, Speech Processing.
Springer.Hosom, John-Paul, Alexander B. Kain, Taniya Mishra,Jan P. H. van Santen, Melanie Fried-Oken, and Jan-ice Staehely.
2003.
Intelligibility of modifications todysarthric speech.
In Proceedings of the IEEE Inter-national Conference on Acoustics, Speech, and SignalProcessing (ICASSP ?03), volume 1, pages 924?927,April.Hustad, Katherine C. 2006.
Estimating the intelligibil-ity of speakers with dysarthria.
Folia Phoniatrica etLogopaedica, 58(3):217?228.Hustad, Katherine C. and David R. Beukelman.
2002.Listener comprehension of severely dysarthric speech:Effects of linguistic cues and stimulus cohesion.
Jour-nal of Speech, Language, and Hearing Research,45:545?558, June.Hux, Karen, Joan Rankin-Erickson, Nancy Manasse, andElizabeth Lauritzen.
2000.
Accuracy of three speechrecognition systems: Case study of dysarthric speech.Augmentative and Alternative Communication (AAC),16(3):186 ?196, January.Kain, Alexander B., John-Paul Hosom, Xiaochuan Niu,Jan P.H.
van Santen, Melanie Fried-Oken, and Jan-ice Staehely.
2007.
Improving the intelligibil-ity of dysarthric speech.
Speech Communication,49(9):743?759, September.Kawahara, H. and H. Matsui.
2003.
Auditory mor-phing based on an elastic perceptual distance metricin an interference-free time-frequency representation.In Acoustics, Speech, and Signal Processing, 2003.Proceedings.
(ICASSP ?03).
2003 IEEE InternationalConference on, volume 1, pages I?256 ?
I?259 vol.1,April.Kawahara, H., R. Nisimura, T. Irino, M. Morise, T. Taka-hashi, and H. Banno.
2009.
Temporally variablemulti-aspect auditory morphing enabling extrapolationwithout objective and perceptual breakdown.
In Pro-ceedings of IEEE International Conference on Acous-tics, Speech and Signal Processing (ICASSP 2009),pages 3905?3908, April.Kawahara, Hideki.
2006.
STRAIGHT, exploitation ofthe other aspect of VOCODER: Perceptually isomor-phic decomposition of speech sounds.
Acoustical Sci-ence and Technology, 27(6):349?353.Kawahara, Hideki, Alain de Cheveigne?, Hideki Banno,Toru Takahashi, and Toshio Irino.
2005.
NearlyDefect-Free F0 Trajectory Extraction for ExpressiveSpeech Modifications Based on STRAIGHT.
In Pro-ceedings of INTERSPEECH 2005, pages 537?540,September.Kent, Ray D. and Kristin Rosen.
2004.
Motor con-trol perspectives on motor speech disorders.
In Ben20Maassen, Raymond Kent, Herman Peters, Pascal VanLieshout, and Wouter Hulstijn, editors, Speech Mo-tor Control in Normal and Disordered Speech.
OxfordUniversity Press, Oxford, chapter 12, pages 285?311.Kida, Yusuke and Tatsuya Kawahara.
2005.
Voiceactivity detection based on optimally weighted com-bination of multiple features.
In Proceedings ofINTERSPEECH-2005, pages 2621?2624.Levenshtein, Vladimir I.
1966.
Binary codes capable ofcorrecting deletions, insertions, and reversals.
Cyber-netics and Control Theory, 10(8):707?710.Messina, James J. and Constance M. Messina.2007.
Description of AAC devices.
http://www.coping.org/specialneeds/assistech/aacdev.htm, April.Moulines, Eric and Francis Charpentier.
1990.
Pitch-synchronous waveform processing techniques for text-to-speech synthesis using diphones.
Speech Commu-nication, 9:453?467, December.Nakatani, Christine.
1993.
A speech-first model for re-pair detection and correction.
In Proceedings of the31st Annual Meeting of the Association for Computa-tional Linguistics, pages 46?53.Neel, Amy T. 2009.
Effects of loud and amplified speechon sentence and word intelligibility in parkinson dis-ease.
Journal of Speech, Language, and Hearing Re-search, 52:1021?1033, August.O?Shaughnessy, Douglas.
2000.
Speech Communica-tions ?
Human and Machine.
IEEE Press, New York,NY, USA.O?Shaughnessy, Douglas.
2008.
Formant estimation andtracking.
In Jacob Benesty, M. Mohan Sondhi, andYiteng Huang, editors, Speech Processing.
Springer.Patel, Rupal.
1998.
Control of prosodic parameters byan individual with severe dysarthria.
Technical report,University of Toronto, December.Portnoff, Michael R. 1976.
Implementation of the dig-ital phase vocoder using the fast Fourier transform.IEEE Transactions on Acoustics, Speech and SignalProcessing, 24(3):243?248.Press, William H., Saul A. Teukolsky, William T. Vetter-ling, and Brian P. Flannery.
1992.
Numerical Recipesin C: the art of scientific computing.
Cambridge Uni-versity Press, second edition.Rosen, Kristin and Sasha Yampolsky.
2000.
Automaticspeech recognition and a review of its functioning withdysarthric speech.
Augmentative & Alternative Com-munication, 16(1):48?60, Jan.Rudzicz, Frank.
2011.
Production knowledge in therecognition of dysarthric speech.
Ph.D. thesis, Uni-versity of Toronto, Department of Computer Science.Rudzicz, Frank, Aravind Kumar Namasivayam, andTalya Wolff.
2011.
The TORGO database of acousticand articulatory speech from speakers with dysarthria.Language Resources and Evaluation, (in press).Schroeter, Juergen.
2008.
Basic principles of speechsynthesis.
In Jacob Benesty, M. Mohan Sondhi, andYiteng Huang, editors, Speech Processing.
Springer.Sethares, William Arthur.
2007.
Rhythm and Trans-forms.
Springer.Snell, Roy C. and Fausto Milinazzo.
1993.
Formant Lo-cation from LPC Analysis Data.
IEEE Transactionson Speech and Audio Processing, 1(2), April.Spiegel, Murray F., Mary Jo Altom, Marian J. Macchi,and Karen L. Wallace.
1990.
Comprehensive assess-ment of the telephone intelligibility of synthesized andnatural speech.
Speech Communication, 9(4):279 ?291.Stevens, Kenneth N. 1998.
Acoustic Phonetics.
MITPress, Cambridge, Massachussetts.Stylianou, Yannis.
2008.
Voice transformation.
In Ja-cob Benesty, M. Mohan Sondhi, and Yiteng Huang,editors, Speech Processing.
Springer.Taylor, Paul, Alan W. Black, and Richard Caley.
1998.The architecture of the Festival speech synthesis sys-tem.
In Proceedings of the 3rd ESCA Workshop onSpeech Synthesis, pages 147?151, Jenolan Caves, Aus-tralia.Toda, Tomoki, Alan W. Black, and Keiichi Tokuda.
2005.Spectral conversion based on maximum likelihood es-timation considering global variance of converted pa-rameter.
In Proceedings of the 2005 InternationalConference on Acoustics, Speech, and Signal Process-ing (ICASSP 2005), Philadelphia, Pennsylvania.Tolba, Hesham and Ahmed S. El Torgoman.
2009.
To-wards the improvement of automatic recognition ofdysarthric speech.
In International Conference onComputer Science and Information Technology, pages277?281, Los Alamitos, CA, USA.
IEEE ComputerSociety.Toth, Arthur R. and Alan W. Black.
2005.
Cross-speakerarticulatory position data for phonetic feature predic-tion.
In Proceedings of Interspeech 2005, Lisbon, Por-tugal.Yan, Qin, Saeed Vaseghi, Esfandiar Zavarehei, Ben Mil-ner, Jonathan Darch, Paul White, and Ioannis An-drianakis.
2007.
Formant tracking linear predic-tion model usng HMMs and Kalman filters for noisyspeech processing.
Computer Speech and Language,21:543?561.21
