Coling 2008: Proceedings of the workshop on Speech Processing for Safety Critical Translation and Pervasive Applications, pages 48?53Manchester, August 2008Speech Translation for Triage of Emergency Phonecalls in MinorityLanguagesUdhyakumar Nallasamy, Alan W Black, TanjaSchultz, Robert FrederkingLanguage Technologies InstituteCarnegie Mellon University5000 Forbes AvenuePittsburgh, PA 15213  USAudhay@cmu.edu,{awb,ref,tanja}@cs.cmu.eduJerry WeltmanLouisiana State UniversityBaton Rouge,Louisiana 70802  USAjweltm2@lsu.eduAbstractWe describe Ayudame, a system de-signed to recognize and translate Spanishemergency calls for better dispatching.We analyze the research challenges inadapting speech translation technology to9-1-1 domain.
We report our initial re-search in 9-1-1 translation system design,ASR experiments, and utterance classifi-cation for translation.1 IntroductionIn the development of real-world-applicable lan-guage technologies, it is good to find an applica-tion with a significant need, and with a complex-ity that appears to be within the capabilities ofcurrent existing technology.
Based on our ex-perience in building speech-to-speech translation,we believe that some important potential uses ofthe technology do not require a full, completespeech-to-speech translation system; somethingmuch more lightweight can be sufficient to aidthe end users (Gao et al 2006).A particular task of this kind is dealing withemergency call dispatch for police, ambulance,fire and other emergency services (in the US theemergency number is 9-1-1).
A dispatcher mustanswer a large variety of calls and, due to themultilingual nature of American society, theymay receive non-English calls and be unable toservice them due to lack of knowledge of thecaller language.?
2008.
Licensed under the Creative Commons At-tribution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.Figure 1.
Ayudame system architectureAs a part of a pilot study into the feasibility ofdealing with non-English calls by a mono-lingualEnglish-speaking dispatcher, we have designed atranslation system that will aid the dispatcher incommunicating without understanding thecaller?s language.48The fundamental idea is to use utterance clas-sification of the non-English input.
The non-English is first recognized by a speech recogni-tion system; then the output is classified into asmall number of domain-specific classes calledDomain Acts (DAs) that can indicate directly tothe dispatcher the general intended meaning ofthe spoken phrase.
Each DA may have a fewimportant parameters to be translated, such asstreet addresses (Levin et al 2003; Langley2003).
The dispatcher can then select from a lim-ited number of canned responses to this througha simple menu system.
We believe the reductionin complexity of such a system compared to afull speech-to-speech translation will be advanta-geous because it should be much cheaper to con-struct, easier to port to new languages, and, im-portantly, sufficient to do the job of processingemergency calls.In the ?NineOneOne?
project, we have de-signed an initial prototype system, which we call?Ayudame?
(Spanish word for ?Help me?).
Fig-ure 1 gives an overview of the system architec-ture.2 The NineOneOne DomainOur initial interest in this domain was due to con-tact from the Cape Coral Police Department(CCPD) in Florida.
They were interested in in-vestigating how speech-to-speech translationscould be used in emergency 9-1-1 dispatch sys-tems.
Most current emergency dispatching cen-ters use some proprietary human translation ser-vice, such as Language Line (Language LineServices).
Although this service provides humantranslation services for some 180 languages, it isfar from ideal.
Once the dispatcher notes that thecaller cannot speak/understand English, theymust initiate the call to Language Line, includingidentifying themselves to the Language Line op-erator, before the call can actually continue.
Thisdelay can be up to a minute, which is not ideal inan emergency situation.After consulting with CCPD, and collecting anumber of example calls, it was clear that fullspeech-to-speech translation was not necessaryand that a limited form of translation throughutterance classification (Lavie et al 2001) mightbe sufficient to provide a rapid response to non-English calls.
The language for our study isSpanish.
Cape Coral is on the Gulf Coast ofFlorida and has fewer Spanish speakers than e.g.the Miami area, but still sufficient that a numberof calls are made to their emergency service inSpanish, yet many of their operators are notsufficiently fluent in Spanish to deal with thecalls.There are a number of key pieces ofinformation that a dispatcher tries to collectbefore passing on the information to theappropriate emergency service.
This includesthings like location, type of emergency, urgency,if anyone is hurt, if the situation is dangerous,etc.
In fact many dispaching organizations haveexisting, well-defined  policies on whatinformation they should collect for differenttypes of emergencies.3 Initial system designBased on the domain's characteristics, in additionto avoiding full-blown translation, we are follow-ing a highly asymmetrical design for the system(Frederking et al 2000).
The dispatcher is al-ready seated at a workstation, and we intend tokeep them ?in the loop?, for both technical andsocial reasons.
So in the dispatcher-to-caller di-rection, we can work with text and menus, sim-plifying the technology and avoiding some cog-nitive complexity for the operator.
So in the dis-patcher-to-caller direction we require no English ASR, no true English-to-Spanish MT, and simple, domain-limited, Spanish speechsynthesis.The caller-to-dispatcher direction is much moreinteresting.
In this direction we require Spanish ASR that can handle emotionalspontaneous telephone speech in mixeddialects, Spanish-to-English MT, but no English Speech Synthesis.We have begun to consider the user interfacesfor Ayudame as well.
For ease of integrationwith pre-existing dispatcher workstations, wehave chosen to use a web-based graphical inter-face.
For initial testing of the prototype, we planto run in ?shadow?
mode, in parallel with livedispatching using the traditional approach.
ThusAyudame will have a listen-only connection tothe telephone line, and will run a web server tointeract with the dispatcher.
Figure 2 shows aninitial design of the web-based interface.
Thereare sections for a transcript, the current callerutterance, the current dispatcher responsechoices, and a button to transfer the interaction toa human translator as a fall-back option.
Foreach utterance, the DA classification is displayedin addition to the actual utterance (in case thedispatcher knows some Spanish).49Figure 2.
Example of initial GUI design4 Automatic Speech RecognitionAn important requirement for such a system isthe ability to be able to recognize the incomingnon-English speech with a word error rate suffi-ciently low for utterance classification and pa-rameter translation to be possible.
The issues inspeech recognition for this particular domain in-clude: telephone speech (which is through a lim-ited bandwidth channel); background noise (thecalls are often from outside or in noisy places);various dialects of Spanish, and potential stressedspeech.
Although initially we expected a sub-stantial issue with recognizing stressed speakers,as one might expect in emergency situations, inthe calls we have collected so far, although it isnot a negligible issue, it is far less important thatwe first expected.The Spanish ASR system is built using theJanus Recognition Toolkit (JRTk) (Finke et al1997) featuring the HMM-based IBIS decoder(Soltau et al 2001).
Our speech corpus consistsof 75 transcribed 9-1-1 calls, with average callduration of 6.73 minutes (min: 2.31 minutes,max: 13.47 minutes).
The average duration ofSpanish speech (between interpreter and caller)amounts to 4.8 minutes per call.
Each call hasanywhere from 46 to 182 speaker turns with anaverage of 113 speaker turns per call.
The turnsthat have significant overlap between speakersare omitted from the training and test set.
Theacoustic models are trained on 50 Spanish 9-1-1calls, which amount to 4 hours of speech data.The system uses three-state, left-to-right, sub-phonetically tied acoustic models with 400 con-text-dependent distributions with the same num-ber of codebooks.
Each codebook has 32 gaus-sians per state.
The front-end feature extractionuses standard 39 dimensional Mel-scale cepstralcoefficients and applies Linear DiscriminantAnalysis (LDA) calculated from the trainingdata.
The acoustic models are seeded with initialalignments from GlobalPhone Spanish acousticmodels trained on 20 hours of speech recordedfrom native Spanish speakers (Schultz et al1997).
The vocabulary size is 65K words.
Thelanguage model consists of a trigram modeltrained on the manual transcriptions of 40 callsand interpolated with a background modeltrained on GlobalPhone Spanish text data con-sisting of 1.5 million words (Schultz et al 1997).The interpolation weights are determined usingthe transcriptions of 10 calls (development set).The test data consists of 15 telephone calls fromdifferent speakers, which amounts to a total of 1hour.
Both development and test set calls con-sisted of manually segmented and transcribedspeaker turns that do not have a significant over-lap with other speakers.
The perplexity of the testset according to the language model is 96.7.The accuracy of the Spanish ASR on the testset is 76.5%.
This is a good result for spontane-ous telephone-quality speech by multiple un-known speakers, and compares favourably to theASR accuracy of other spoken dialog systems.We had initially planned to investigate novelASR techniques designed for stressed speech andmultiple dialects, but to our surprise these do not50seem to be required for this application.
Notethat critical information such as addresses will besynthesized back to the caller for confirmation inthe full system.
So, for the time-being we willconcentrate on the accuracy of the DA classifica-tion until we can show that improving ASR accu-racy would significantly help.5 Utterance ClassificationAs mentioned above, the translation approach weare using is based on utterance classification.
TheSpanish to English translation in the Ayudamesystem is a two-step process.
The ASRhypothesis is first classified into domain-specificDomain Acts (DA).
Each DA has apredetermined set of parameters.
Theseparameters are identified and translated using arule-based framework.
For this approach to beaccomplished with reasonable effort levels, thetotal number of types of parameters and theircomplexity must be fairly limited in the domain,such as addresses and injury types.
This sectionexplains our DA tagset and classificationexperiments.5.1 Initial classification and resultsThe initial evaluation (Nallasamy et al 2008)included a total of 845 manually labeled turns inour 9-1-1 corpus.
We used a set of 10 tags to an-notate the dialog turns.
The distribution of thetags are listed belowTag (Representation) FrequencyGiving Name 80Giving Address 118Giving Phone number 29Requesting Ambulance 8Requesting Fire Service 11Requesting Police 24Reporting Injury/Urgency 61Yes 119No 24Others 371Table 1.
Distribution of first-pass tags in thecorpus.We extracted bag-of-word features and trained aSupport Vector Machine (SVM) classifier (Bur-ges, 1998) using the above dataset.
A 10-foldstratified cross-validation has produced an aver-age accuracy of 60.12%.
The accuracies of indi-vidual tags are listed below.Tag Accuracy(%)Giving Name 57.50Giving Address 38.98Giving Phone number 48.28Req.
Ambulance 62.50Req.
Fire Service 54.55Req.
Police 41.67Reporting Injury/Urgency 39.34Yes 52.94No 54.17Others 75.74Table 2.
Classification accuracies of first-passtags.5.2 Tag-set improvementsWe improved both the DA tagset and theclassification framework in our second-passclassification, compared to our initialexperiment.
We had identified several issues inour first-pass classification: We had forced each dialog turn to have asingle tag.
However, the tags and thedialog turns don?t conform to thisassumption.
For example, the dialog?Yes, my husband has breathing prob-lem.
We are at two sixty-one OakStreet?1 should get 3 tags: ?Yes?, ?Giv-ing-Address?, ?Requesting-Ambulance?. Our analysis of the dataset alo showedthat the initial set of tags are notexhaustive enough to cover the wholerange of dialogs required to be translatedand conveyed to the dispatcher.We made several iterations over the tagset toensure that it is both compact and achievesrequisite coverage.
The final tag set consists of67 entries.
We manually annotated 59 calls withour new tagset using a web interface.
Thedistribution of the top 20 tags is listed below.The whole list of tags can be found in theNineOneOne project webpage:http://www.cs.cmu.edu/~911/1The dialog is English Translation of  ?s?, mi esposo le faltael aire.
es ac?
en el dos sesenta y uno Oak Street?.
It isextracted from the transcription of a CCPD 9-1-1emergency call, with address modified to protect privacy51Tag (Representation) FrequencyYes 227Giving-Address 133Giving-Location 113Giving-Name 107No 106Other 94OK 81Thank-You 51Reporting-Conflict 43Describing-Vehicle 42Giving-Telephone-Number 40Hello 36Reporting-Urgency-Or-Injury 34Describing-Residence 28Dont-Know 19Dont-Understand 16Giving-Age 15Goodbye 15Giving-Medical-Symptoms 14Requesting-Police 12Table 3.
Distribution of top 20 second-passtagsThe new tagset is hierarchical, which allowsus to evaluate the classifier at different levels ofthe hierarchy, and eventually select the besttrade-off between the number of tags andclassification accuracy.
For example, the firstlevel of tags for reporting incidents includes thefive most common incidents, viz, Reporting-Conflict, Reporting-Robbery, Reporting-Traffic-accident, Reporting-Urgency-or-Injury andReporting-Fire.
The second level of tags are usedto convey more detailed information about theabove incidents (eg.
Reporting-Weapons in thecase of conflict) or rare incidents (eg.
Reporting-Animal-Problem).5.3 Second-pass classification and ResultsWe also improved our classificationframework to allow multiple tags for a singleturn and to easily accomodate any new tags inthe future.
Our earlier DA classification used amulti-class classifier, as each turn was restrictedto have a single tag.
To accomodate multiple tagsfor a single turn, we trained binary classifiers foreach tag.
All the utterances of the correspondingtag are marked positive examples and the rest aremarked as negative examples.
Our new data sethas 1140 dialog turns and 1331 annotations.
Notethat the number of annotations is more than thenumber of labelled turns as each turn may havemultiple tags.
We report classification accuraciesin the following table for each tag based on 10-fold cross-validation:Tag (Representation) Accuracy(%)Yes 87.32Giving-Address 42.71Giving-Location 87.32Giving-Name 42.71No 37.63Other 54.98OK 72.5Thank-You 41.14Reporting-Conflict 79.33Describing-Vehicle 96.82Giving-Telephone-Number 39.37Hello 38.79Reporting-Urgency-Or-Injury 49.8Describing-Residence 92.75Dont-Know 41.67Dont-Understand 36.03Giving-Age 64.95Goodbye 87.27Giving-Medical-Symptoms 47.44Requesting-Police 79.94Table 4.
Classification accuracies ofindividual second-pass tagsThe average accuracy of the 20 tags is58.42%.
Although multiple classifiers increasethe computational complexity during run-time,they are independent of each other, so we can runthem in parallel.
To ensure the consistency andclarity of the new tag set, we had a secondannotator label 39 calls.
The inter-coderagreement (Kappa coefficient) between the twoannotators is 0.67.
This is considered substantialagreement between the annotators, and confirmsthe consistency of the tag set.6 ConclusionThe work reported here demonstrates that we canproduce Spanish ASR for Spanish emergencycalls with reasonable accuracy (76.5%), and clas-sify manual transcriptions of these calls with rea-sonable accuracy (60.12% on the original tagset,5258.42% on the new, improved tagset).
We be-lieve these results are good enough to justify thenext phase of research, in which we will develop,user-test, and evaluate a full pilot system.
We arealso investigating a number of additional tech-niques to improve the DA classification accura-cies.
Further we believe that we can design theoverall dialog system to ameliorate the inevitableremaining misclassifications, based in part on theconfusion matrix of actual errors (Nallasamy etal, 2008).
But only actual user tests of a pilotsystem will allow us to know whether an even-tual deployable system is really feasible.AcknowledgementsThis project is funded by NSF Grant No: IIS-0627957 ?NineOneOne: Exploratory Researchon Recognizing Non-English Speech for Emer-gency Triage in Disaster Response?.
Any opin-ions, findings, and conclusions or recommenda-tions expressed in this material are those of theauthors and do not necessarily reflect the viewsof sponsors.ReferencesBurges C J C, A tutorial on support vector machinesfor pattern recognition, In Proc.
Data Mining andKnowledge Discovery, pp 2(2):955-974, USA,1998Finke M, Geutner P, Hild H, Kemp T, Ries K andWestphal M, The Karlsruhe-Verbmobil SpeechRecognition Engine, In Proc.
IEEE InternationalConference on Acoustics, Speech, and SignalProcessing (ICASSP), pp.
83-86, Germany, 1997Frederking R, Rudnicky A, Hogan C and Lenzo K,Interactive Speech Translation in the DiplomatProject, Machine Translation Journal 15(1-2),Special issue on Spoken Language Translation, pp.61-66, USA, 2000Gao Y, Zhou B, Sarikaya R, Afify M, Kuo H, Zhu W,Deng Y, Prosser C, Zhang W and Besacier L, IBMMASTOR SYSTEM: Multilingual AutomaticSpeech-to-Speech Translator, In Proc.
First Inter-national Workshop on Medical Speech Translation,pp.
53-56, USA, 2006Langley C, Domain Action Classification and Argu-ment Parsing for Interlingua-based Spoken Lan-guage Translation.
PhD thesis, Carnegie MellonUniversity, Pittsburgh, PA, 2003Language Line Services http://www.languageline.comLavie A, Balducci F, Coletti P, Langley C, Lazzari G,Pianesi F, Taddei L and Waibel A, Architectureand Design Considerations in NESPOLE!
: aSpeech Translation System for E-Commerce Ap-plications,.
In Proc.
Human Language Technolo-gies (HLT), pp 31-34, USA, 2001Levin L, Langley C, Lavie A, Gates D, Wallace D andPeterson K, Domain Specific Speech Acts for Spo-ken Language Translation, In Proc.
4th SIGdialWorkshop on Discourse and Dialogue, pp.
208-217, Japan, 2003Nallasamy U, Black A, Schultz T and Frederking R,NineOneOne: Recognizing and Classifying Speechfor Handling Minority Language Emergency Calls,In Proc.
6th International conference on LanguageResources and Evaluation (LREC), Morocco, 2008NineOneOne project webpage[www.cs.cmu.edu/~911]Schultz T, Westphal M and Waibel A, TheGlobalPhone Project: Multilingual LVCSR withJANUS-3, In Proc.
Multilingual Information Re-trieval Dialogs: 2nd SQEL Workshop, pp.
20-27,Czech Republic, 1997Soltau H, Metze F, F?ugen C and Waibel A, A OnePass-Decoder Based on Polymorphic LinguisticContext Assignment, In Proc.
IEEE workshop onAutomatic Speech Recognition and Understanding(ASRU), Italy, 200153
