Proceedings of the 5th International Workshop on Health Text Mining and Information Analysis (Louhi) @ EACL 2014, pages 21?29,Gothenburg, Sweden, April 26-30 2014. c?2014 Association for Computational LinguisticsDomain Adaptation with Active Learning for Coreference ResolutionShanheng ZhaoElance441 Logue AveMountain View, CA 94043, USAszhao@elance.comHwee Tou NgDepartment of Computer ScienceNational University of Singapore13 Computing Drive, Singapore 117417nght@comp.nus.edu.sgAbstractIn the literature, most prior work oncoreference resolution centered on thenewswire domain.
Although a coreferenceresolution system trained on the newswiredomain performs well on newswire texts,there is a huge performance drop when it isapplied to the biomedical domain.
In thispaper, we present an approach integrat-ing domain adaptation with active learningto adapt coreference resolution from thenewswire domain to the biomedical do-main.
We explore the effect of domainadaptation, active learning, and target do-main instance weighting for coreferenceresolution.
Experimental results showthat domain adaptation with active learn-ing and target domain instance weightingachieves performance on MEDLINE ab-stracts similar to a system trained on coref-erence annotation of only target domaintraining instances, but with a greatly re-duced number of target domain traininginstances that we need to annotate.1 IntroductionCoreference resolution is the task of determin-ing whether two or more noun phrases (NPs) ina text refer to the same entity.
Successful coref-erence resolution benefits many natural languageprocessing (NLP) tasks, such as information ex-traction and question answering.
In the literature,most prior work on coreference resolution recaststhe problem as a two-class classification problem.Machine learning-based classifiers are applied todetermine whether a candidate anaphor and a po-tential antecedent are coreferential (Soon et al.,2001; Ng and Cardie, 2002; Stoyanov et al., 2009;Zhao and Ng, 2010).In recent years, with the advances in biologi-cal and life science research, there is a rapidly in-creasing number of biomedical texts, including re-search papers, patent documents, etc.
This resultsin an increasing demand for applying natural lan-guage processing and information retrieval tech-niques to efficiently exploit information containedin these large amounts of texts.
However, corefer-ence resolution, one of the core tasks in NLP, hasonly a relatively small body of prior research inthe biomedical domain (Kim et al., 2011a; Kim etal., 2011b).A large body of prior research on coreferenceresolution focuses on texts in the newswire do-main.
Standardized data sets, such as MUC(DARPA Message Understanding Conference,(MUC-6, 1995; MUC-7, 1998)) and ACE (NISTAutomatic Content Extraction Entity Detectionand Tracking task, (NIST, 2002)) data sets arewidely used in the study of coreference resolution.Traditionally, in order to apply supervised ma-chine learning approaches to an NLP task in a spe-cific domain, one needs to collect a text corpusin the domain and annotate it to serve as trainingdata.
Compared to other NLP tasks, e.g., part-of-speech (POS) tagging or named entity (NE) tag-ging, the annotation for coreference resolution ismuch more challenging and time-consuming.
Thereason is that in tasks like POS tagging, an annota-tor only needs to focus on each markable (a word,in the case of POS tagging) and a small windowof its neighboring words.
In contrast, to annotatea coreferential relation, an annotator needs to firstrecognize whether a certain text span is a mark-able, and then scan through the text preceding themarkable (a potential anaphor) to look for the an-tecedent.
It also requires the annotator to under-stand the text in order to annotate coreferential re-lations, which are semantic in nature.
If a mark-able is non-anaphoric, the annotator has to scan tothe beginning of the text to realize that.
Cohenet al.
(2010) reported that it took an average of 20hours to annotate coreferential relations in a single21document with an average length of 6,155 words,while an annotator could annotate 3,000 words perhour in POS tag annotation (Marcus et al., 1993).The simplest approach to avoid the time-consuming data annotation in a new domain isto train a coreference resolution system on aresource-rich domain and apply it to a differenttarget domain without any additional data anno-tation.
Although coreference resolution systemswork well on test texts in the same domain asthe training texts, there is a huge performancedrop when they are tested on a different domain.This motivates the usage of domain adaptationtechniques for coreference resolution: adapting acoreference resolution system from one source do-main in which we have a large collection of an-notated data, to a second target domain in whichwe need good performance.
It is almost inevitablethat we annotate some data in the target domain toachieve good coreference resolution performance.The question is how to minimize the amount of an-notation needed.
In the literature, active learninghas been exploited to reduce the amount of anno-tation needed (Lewis and Gale, 1994).
In contrastto annotating the entire data set, active learning se-lects only a subset of the data to annotate in an iter-ative process.
How to apply active learning and in-tegrate it with domain adaptation remains an openproblem for coreference resolution.In this paper, we explore domain adaptationfor coreference resolution from the resource-richnewswire domain to the biomedical domain.
Ourapproach comprises domain adaptation, activelearning, and target domain instance weightingto leverage the existing annotated corpora fromthe newswire domain, so as to reduce the costof developing a coreference resolution system inthe biomedical domain.
Our approach achievescomparable coreference resolution performanceon MEDLINE abstracts, but with a large reductionin the number of training instances that we need toannotate.
To the best of our knowledge, our workis the first to combine domain adaptation and ac-tive learning for coreference resolution.The rest of this paper is organized as follows.We first review the related work in Section 2.
Thenwe describe the coreference resolution system inSection 3, and the domain adaptation and activelearning techniques in Section 4.
Experimental re-sults are presented in Section 5.
Finally, we ana-lyze the results in Section 6 and conclude in Sec-tion 7.2 Related WorkNot only is there a relatively small body of priorresearch on coreference resolution in the biomed-ical domain, there are also fewer annotated cor-pora in this domain.
Castan?o et al.
(2002) wereamong the first to annotate coreferential relationsin the biomedical domain.
Their annotation onlyconcerned the pronominal and nominal anaphoricexpressions in 46 biomedical abstracts.
Gasperinand Briscoe (2007) annotated coreferential rela-tions on 5 full articles in the biomedical domain,but only on noun phrases referring to bio-entities.Yang et al.
(2004) annotated full NP coreferentialrelations on biomedical abstracts of the GENIAcorpus.
The ongoing project of the CRAFT cor-pus is expected to annotate all coreferential rela-tions on full text of biomedical articles (Cohen etal., 2010).Unlike the work of (Castan?o et al., 2002),(Gasperin and Briscoe, 2008), and (Gasperin,2009) that resolved coreferential relations on cer-tain restricted entities in the biomedical domain,we resolve all NP coreferential relations.
Al-though the GENIA corpus contains 1,999 biomed-ical abstracts, Yang et al.
(2004) tested only on 200abstracts under 5-fold cross validation.
In contrast,we randomly selected 399 abstracts in the 1,999MEDLINE abstracts of the GENIA-MEDCo cor-pus as the test set, and as such our evaluation wascarried out on a larger scale.Domain adaptation has been studied and suc-cessfully applied to many natural language pro-cessing tasks (Jiang and Zhai, 2007; Daume III,2007; Dahlmeier and Ng, 2010; Yang et al., 2012).On the other hand, active learning has also beenapplied to NLP tasks to reduce the need of data an-notation in the literature (Tang et al., 2002; Lawset al., 2012; Miller et al., 2012).
Unlike the afore-mentioned work that applied only one of domainadaptation or active learning to NLP tasks, wecombine both.
There is relatively less researchon combining domain adaptation and active learn-ing together for NLP tasks (Chan and Ng, 2007;Zhong et al., 2008; Rai et al., 2010).
Chan andNg (2007) and Zhong et al.
(2008) used countmerging and augment, respectively, as their do-main adaptation techniques whereas we apply andcompare multiple state-of-the-art domain adapta-tion techniques.
Rai et al.
(2010) exploited a22streaming active learning setting whereas ours ispool-based.Dahlmeier and Ng (2010) evaluated the perfor-mance of three previously proposed domain adap-tation algorithms for semantic role labeling.
Theyevaluated the performance of domain adaptationwith different sizes of target domain training data.In each of their experiments with a certain targetdomain training data size, the target domain train-ing data were added all at once.
In contrast, we addthe target domain training instances selectively inan iterative process.
Different from (Dahlmeierand Ng, 2010), we weight the target domain in-stances to further boost the performance of do-main adaptation.
Our work is the first system-atic study of domain adaptation with active learn-ing for coreference resolution.
Although Gasperin(2009) tried to apply active learning for anaphoraresolution, her results were negative: using ac-tive learning was not better than randomly select-ing instances in her work.
Miwa et al.
(2012)incorporated a rule-based coreference resolutionsystem for automatic biomedical event extraction,and showed that by adding training data from otherdomains as supplementary training data and us-ing domain adaptation, one can achieve a higherF-measure in event extraction.3 Coreference ResolutionThe gold standard annotation and the output by acoreference resolution system are called the keyand the response, respectively.
In both the key andthe response, a coreference chain is formed by aset of coreferential markables.
A markable is anoun phrase which satisfies the markable defini-tion in an individual corpus.
Here is an example:When the same MTHC lines are ex-posed to TNF-alpha in combination withIFN-gamma, the cells instead becomeDC.In the above sentence, the same MTHC linesand the cells are referring to the same entity andhence are coreferential.
It is possible that morethan two markables are coreferential in a text.
Thetask of coreference resolution is to determine theserelations in a given text.To evaluate the performance of coreference res-olution, we follow the MUC evaluation metric in-troduced by (Vilain et al., 1995).
Let Sibe anequivalence class generated by the key (i.e., Siis a coreference chain), and p(Si) be a partitionof Sirelative to the response.
Recall is the num-ber of correctly identified links over the number oflinks in the key: Recall =?(|Si|?|p(Si)|)?(|Si|?1).
Pre-cision, on the other hand, is defined in the oppo-site way by switching the role of key and response.F-measure is a trade-off between recall and preci-sion: F = 2?Recall?PrecisionRecall+Precision.4 Domain Adaptation with ActiveLearning4.1 Domain AdaptationDomain adaptation is applicable when one hasa large amount of annotated training data in thesource domain and a small amount or none ofthe annotated training data in the target domain.We evaluate the AUGMENT technique introducedby (Daume III, 2007), as well as the INSTANCEWEIGHTING (IW) and the INSTANCE PRUNING(IP) techniques introduced by (Jiang and Zhai,2007).4.1.1 AUGMENTDaume III (2007) introduced a simple domainadaptation technique by feature space augmenta-tion.
It maps the feature space of each instanceinto a feature space of higher dimension.
Supposex is the feature vector of an instance.
Define ?sand ?t to be the mappings of an instance fromthe original feature space to an augmented featurespace in the source and the target domain, respec-tively:?s(x) = ?x, x,0?
(1)?t(x) = ?x,0, x?
(2)where 0 = ?0, 0, .
.
.
, 0?
is a zero vector of length|x|.
The mapping can be treated as taking eachfeature in the original feature space and makingthree versions of it: a general version, a source-specific version, and a target-specific version.
Theaugmented source domain data will contain onlythe general and the source-specific versions, whilethe augmented target domain data will containonly the general and the target-specific versions.4.1.2 INSTANCE WEIGHTING and INSTANCEPRUNINGLet x and y be the feature vector and the corre-sponding true label of an instance, respectively.23Jiang and Zhai (2007) pointed out that when ap-plying a classifier trained on a source domain toa target domain, the joint probability Pt(x, y) inthe target domain may be different from the jointprobability Ps(x, y) in the source domain.
Theyproposed a general framework to use Ps(x, y) toestimate Pt(x, y).
The joint probability P (x, y)can be factored into P (x, y) = P (y|x)P (x).
Theadaptation of the first component is labeling adap-tation, while the adaptation of the second compo-nent is instance adaptation.
We explore only label-ing adaptation.To calibrate the conditional probability P (y|x)from the source domain to the target domain, ide-ally each source domain training instance (xi, yi)should be given a weight Pt(ysi|xsi)Ps(ysi|xsi).
AlthoughPs(ysi|xsi) can be estimated from the source do-main training data, the estimation of Pt(ysi|xsi)is much harder.
Jiang and Zhai(2007) proposedtwo methods to estimate Pt(ysi|xsi): INSTANCEWEIGHTING and INSTANCE PRUNING.
Bothmethods first train a classifier with a small amountof target domain training data.
Then, INSTANCEWEIGHTING directly estimates Pt(ysi|xsi) usingthe trained classifier.
INSTANCE PRUNING, on theother hand, removes the top N source domain in-stances that are predicted wrongly, ranked by theprediction confidence.4.1.3 Target Domain Instance WeightingBoth INSTANCE WEIGHTING and INSTANCEPRUNING set the weights of the source domaininstances.
In domain adaptation, there are typi-cally many more source domain training instancesthan target domain training instances.
Target do-main instance weighting can effectively reduce theimbalance.
Unlike INSTANCE WEIGHTING andINSTANCE PRUNING in which each source do-main instance is weighted individually, we giveall target domain instances the same weight.
Thistarget domain instance weighting scheme is notonly complementary to INSTANCE WEIGHTINGand INSTANCE PRUNING, but is also applicableto AUGMENT.4.2 Active LearningActive learning iteratively selects the most infor-mative instances to label, adds them to the train-ing data pool, and trains a new classifier with theenlarged data pool.
We follow (Lewis and Gale,1994) and use the uncertainty sampling strategy inour active learning setting.Ds?
the set of source domain training instancesDt?
the set of target domain training instancesDa?
???
coreference resolution system trained on DsT ?
number of iterationsfor i from 1 to T dofor each di?
Dtdo?di?
prediction of diusing ?pi?
prediction confidence of ?diend forD?a?
top N instances with the lowest piDa?
Da+D?aDt?
Dt?D?aprovide correct labels to the unlabeled instances in D?a?
?
coreference resolution system trained on DsandDausing the chosen domain adaptation techniqueend forFigure 1: An algorithm for domain adaptationwith active learning4.3 Domain Adaptation with Active LearningCombining domain adaptation and active learningtogether, the algorithm we use is shown in Figure1.In our domain adaptation setting, there is a pa-rameter ?tfor target domain instance weighting.Because the number of target domain instances isdifferent in each iteration, the weight should be ad-justed in each iteration.
We give all target domaintraining instances an equal weight of ?t= Ns/Nt,where Nsand Ntare the numbers of instances inthe source domain and the target domain in thecurrent iteration, respectively.
We set N = 10 toadd 10 instances in each iteration to speed up theactive learning process.To provide the correct labels, the labeling pro-cess shows the text on the screen, highlights thetwo NPs, and asks the annotator to decide if theyare coreferential.
In our experiments, this is simu-lated by providing the gold standard coreferentialinformation on this NP pair to the active learningprocess.5 Experiments5.1 The CorporaWe explore domain adaptation from the newswiredomain to the biomedical domain.
The newswireand biomedical domain data that we use are theACE Phase-2 corpora and the GENIA-MEDCocorpus, respectively.
The ACE corpora con-tain 422 and 92 training and test texts, re-spectively (NIST, 2002).
The texts come from24three newswire sources: BNEWS, NPAPER, andNWIRE.
The GENIA-MEDCo corpus contains1,999 MEDLINE abstracts1.
We randomly splitthe GENIA corpus into a training set and a testset, containing 1,600 and 399 texts, respectively.5.2 The Coreference Resolution SystemIn this study, we use Reconcile, a state-of-the-art coreference resolution system implemented by(Stoyanov et al., 2009).
The input to the corefer-ence resolution system is raw text, and we apply asequence of preprocessing components to processit.
Following Reconcile, the individual prepro-cessing steps include: 1) sentence segmentation(using the OpenNLP toolkit2); 2) tokenization (us-ing the OpenNLP toolkit); 3) POS tagging (usingthe OpenNLP toolkit); 4) syntactic parsing (usingthe Berkeley Parser3); and 5) named entity recog-nition (using the Stanford NER4).
Markables areextracted as defined in each individual corpus.
Allpossible markable pairs in the training and test setare extracted to form training and test instances,respectively.
The learning algorithm we use ismaximum entropy modeling, implemented in theDALR package5 (Jiang and Zhai, 2007).
Thecoreference resolution system employs a compre-hensive set of 62 features to represent each train-ing and test instance, including lexical, proximity,grammatical, and semantic features (Stoyanov etal., 2009).
We do not introduce additional featuresmotivated from the biomedical domain, but use thesame feature set for both the source and target do-mains.5.3 PreprocessingFor the ACE corpora, all preprocessing compo-nents use the original models (provided by theOpenNLP toolkit, the Berkeley Parser, and theStanford NER).
For the GENIA corpus, since it isfrom a very different domain, the original modelsdo not perform well.
However, the GENIA cor-pus contains multiple layers of annotations.
Weuse these annotations to re-train each of the pre-processing components (except tokenization) us-ing the 1,600 training texts of the GENIA cor-1http://nlp.i2r.a-star.edu.sg/medco.html2http://opennlp.sourceforge.net/3http://code.google.com/p/berkeleyparser/4http://nlp.stanford.edu/ner/5http://www.mysmu.edu/faculty/jingjiang/software/DALR.htmlNPAPER NPAPER GENIA GENIATRAIN TEST TRAIN TESTNumber of Docs76 17 1,600 399Number of WordsTotal 68,463 17,350 391,380 95,405Avg.
900.8 1,020.6 244.6 239.1Number of MarkablesTotal 21,492 5,153 99,408 24,397Avg.
282.8 303.1 62.1 61.1Number of InstancesTotal 3,365,680 871,314 3,335,640 798,844Avg.
44,285.3 51,253.8 2,084.8 2,002.1Table 1: Statistics of the NPAPER and GENIAdata setspus6.
We do not use any texts from the test setwhen training these models.
Also, we do not useany NLP toolkits from the biomedical domain, butonly use general toolkits trained with biomedicaltraining data.
These re-trained preprocessing com-ponents are then applied to process the entire GE-NIA corpus, including both the training and testsets.Instead of using the entire ACE corpora, wechoose the NPAPER portion of the ACE corporaas the source domain in the experiments, becauseit is the best performing one among the three por-tions.
Under these preprocessing settings, therecall percentages of markable extraction on thetraining and test set of the NPAPER corpus are94.5% and 95.5% respectively, while the recallpercentages of markable extraction on the trainingand test set of the GENIA corpus are 87.6% and86.6% respectively.
The statistics of the NPAPERand the GENIA corpora are listed in Table 1.5.4 Baseline ResultsUnder our experimental settings, a coreferenceresolution system that is trained on the NPA-PER training set and tested on the NPAPER testset achieves recall, precision, and F-measure of59.0%, 70.6%, and 64.3%, respectively.
Thisis comparable to the state-of-the-art performance(Stoyanov et al., 2009).
Table 2 compares the per-formance of testing on the GENIA test set, buttraining with the GENIA training set or the NPA-PER training set.
Training with in-domain dataachieves an F-measure that is 9.1% higher thantraining with out-of-domain data.
Training with6It turned out that the re-trained tokenization model gavepoorer performance and produced many errors on punctua-tion symbols.
Thus, we stuck to using the original tokeniza-tion model.25Training Set Recall Precision F-measureGENIA Training Set 37.7 71.9 49.5NPAPER Training Set 30.3 60.7 40.4Table 2: MUC F-measures on the GENIA test setin-domain data is better than training with out-of-domain data for both recall and precision.
Thisconfirms the impact of domain difference betweenthe newswire and the biomedical domain.5.5 Domain Adaptation with Active LearningIn the experiments on domain adaptation with ac-tive learning for coreference resolution, we as-sume that the source domain training data are an-notated.
The target domain training data are notannotated but are used as a data pool for instanceselection.
The algorithm selects the instances inthe data pool to annotate and add them to the train-ing data to update the classifier.
The target domaintest set is strictly separated from this data pool, i.e.,none of the target domain test data are used in theinstance selection process of active learning.From Table 1, one can see that both training setsin the NPAPER and the GENIA corpora containlarge numbers of training instances.
Instead of us-ing the entire training sets in the experiments, weuse a smaller subset due to several reasons.
First,to train a coreference resolution classifier, we donot need so much training data (Soon et al., 2001).Second, a large number of training instances willslow the active learning process.
Third, a smallersource domain training corpus suggests a moremodest annotation effort even on the source do-main.
Lastly, a smaller target domain training cor-pus means that fewer words need to be read byhuman annotators to label the data.We randomly choose 10 NPAPER texts as thesource domain training set.
A coreference resolu-tion system that is trained on these 10 texts andtested on the entire NPAPER test set achieves re-call, precision, and F-measure of 60.3%, 70.6%,and 65.0%, respectively.
This is comparable to(actually slightly better than) a system trained onthe entire NPAPER training set.
As for the GE-NIA training set, we randomly choose 40 texts asthe target domain training data.
To avoid selec-tion bias, we perform 5 random trials, i.e., choos-ing 5 sets, each containing 40 randomly selectedGENIA training texts.
In the rest of this paper, allperformances of using 40 GENIA training texts arethe average scores over 5 runs, each of which usesa different set of 40 texts.In the previous section, we have presented thedomain adaptation techniques, the active learningalgorithm, as well as the target domain instanceweighting scheme.
In the rest of this section, wepresent the experimental results to show how do-main adaptation, active learning, and target do-main instance weighting help coreference resolu-tion in a new domain.
We use Augment, IW, andIP to denote the three domain adaptation tech-niques: AUGMENT, INSTANCE WEIGHTING, andINSTANCE PRUNING, respectively.
For a furthercomparison, we explore another baseline method,which is simply a concatenation of the source andtarget domain data together, called Combine in therest of this paper.
In all the experiments with ac-tive learning, we run 100 iterations, which resultin the selection of 1,000 target domain instances.The first experiment is to measure the effective-ness of target domain instance weighting.
We fixon the use of uncertainty-based active learning,and compare weighting and without weighting oftarget domain instances (denoted as Weighted andUnweighted).
The learning curves are shown inFigure 2.
For Combine, Augment, and IP, it can beseen that Weighted is a clear winner.
As for IW, atthe beginning of active learning, Unweighted out-performs Weighted, though it is unstable.
At theend of 100 iterations, Weighted outperforms Un-weighted.Since Weighted outperforms Unweighted, wefix on the use of Weighted and explore the effec-tiveness of active learning.
For comparison, we tryanother iterative process that randomly selects 10instances in each iteration.
We found that selectionof instances using active learning achieved betterperformance than random selection in all cases.This is because random selection may select in-stances that the classifier has very high confidencein, which will not help in improving the classifier.In the third experiment, we fix on the use ofWeighted and Uncertainty since they perform thebest, and evaluate the effect of different domainadaptation techniques.
The learning curves areshown in Figure 3.
It can be seen that Augmentis the best performing system.
For a closer look,we tabulate the results in Table 3, with the statisti-cal significance levels indicated.
Statistical signif-icance tests were conducted following (Chinchor,2011).260 20 40 60 80 10020253035404550IterationMUCF?measureWeighted + UncertaintyUnweighted + Uncertainty(a) Combine0 20 40 60 80 10020253035404550IterationMUCF?measureWeighted + UncertaintyUnweighted + Uncertainty(b) Augment0 20 40 60 80 10020253035404550IterationMUCF?measureWeighted + UncertaintyUnweighted + Uncertainty(c) IW0 20 40 60 80 10020253035404550IterationMUCF?measureWeighted + UncertaintyUnweighted + Uncertainty(d) IPFigure 2: Learning curves of comparing target domain instances weighted vs. unweighted.
All systemsuse uncertainty-based active learning.Iteration 0 10 20 30 40 60 80 100Combine+Unweighted 39.8 40.7 40.9 41.1 41.4 40.4 41.6 42.1Combine+Weighted 39.8 40.9 44.0** 44.8** 45.2** 48.0** 47.7** 47.6**Augment+Weighted 39.8 44.1**??
46.0**??
47.0**??
47.8**??
49.1**??
49.1**??
49.0**?
?IW+Weighted 39.8 24.3 33.1 36.8 38.1 45.0** 48.2**??
48.3**?
?IP+Weighted 39.8 34.4 40.7 43.4** 46.2**??
48.0** 48.5**??
48.5**?
?Table 3: MUC F-measures of different active learning settings on the GENIA test set.
All systems useUncertainty.
Statistical significance is compared against Combine+Unweighted, where * and ** standfor p < 0.05 and p < 0.01, respectively, and compared against Combine+Weighted, where ?and ?
?standfor p < 0.05 and p < 0.01, respectively.6 AnalysisUsing only the source domain training data,a coreference resolution system achieves an F-measure of 39.8% on the GENIA test set (the col-umn of ?Iteration 0?
in Table 3).
From Figure 3and Table 3, we can see that in the first few iter-ations of active learning, domain adaptation doesnot perform as well as using only the source do-main training data.
This is because when thereare very limited target domain data, the estima-tion of the target domain is unreliable.
Dahlmeierand Ng (2010) reported similar findings thoughthey did not use active learning.
With more iter-ations, i.e., more target domain training data, do-main adaptation is clearly superior.
Among thethree domain adaptation techniques, Augment isbetter than IW and IP.
It not only achieves a higherF-measure, but also a faster speed to adapt to anew domain in active learning.
Also, similar to(Dahlmeier and Ng, 2010), we find that IP is gen-erally better than IW.
All systems (except IW)with Weighted performs much better than Com-bine+Unweighted.
This shows the effectivenessof target domain instance weighting.
The aver-age recall, precision, and F-measure of our bestmodel, Augment+Weighted, after 100 iterationsare 37.3%, 71.5%, and 49.0%, respectively.
Com-pared to training with only the NPAPER trainingdata, not only the F-measure, but also both the re-call and precision are greatly improved (cf Table2).Among all the target domain instances that wereselected in Augment+Weighted, the average dis-270 20 40 60 80 1003035404550IterationMUC F?measureCombineAugmentIWIPFigure 3: Learning curves of different domainadaptation methods.
All systems use Weighted andUncertainty.tance of the two markables in an instance (mea-sured in sentence) is 3.4 (averaged over the 5runs), which means an annotator needs to read 4sentences on average to annotate an instance.We also investigate the difference of corefer-ence resolution between the newswire domain andthe biomedical domain, and the instances thatwere selected in active learning which representthis difference.
One of the reasons that corefer-ence resolution differs in the two domains is thatscientific writing in biomedical texts frequentlycompares entities.
For example,In Cushing?s syndrome, the CR of GRwas normal in spite of the fact that theCR of plasma cortisol was disturbed.The two CRs refer to different entities and henceare not coreferential.
However, a system trainedon NPAPER predicts them as coreferential.
Inthe newswire domain, comparisons are less likely,especially for named entities.
For example, inthe newswire domain, London in most cases iscoreferential to other Londons.
However, in thebiomedical domain, DNAs as in DNA of humanbeings and DNA of monkeys are different enti-ties.
A coreference resolution system trained onthe newswire domain is unable to capture the dif-ference between these two named entities, hencepredicting them as coreferential.
This also jus-tifies the need for domain adaptation for corefer-ence resolution.
For the above sentence, after ap-plying our method, the adapted coreference res-olution system is able to predict the two CRs asnon-coreferential.Next, we show the effectiveness of our sys-tem using domain adaptation with active learningcompared to a system trained with full corefer-ence annotations.
Averaged over 5 runs, a systemtrained on a single GENIA training text achievesan F-measure of 25.9%, which is significantlylower than that achieved by our method.
Withmore GENIA training texts added, the F-measureincreases.
After 80 texts are used, the systemtrained on full annotations finally achieves an F-measure of 49.2%, which is 0.2% higher than Aug-ment+Weighted after 100 iterations.
However, af-ter 100 iterations, only 1,000 target domain in-stances are annotated under our framework.
Con-sidering that one single text in the GENIA corpuscontains an average of over 2,000 instances (cf Ta-ble 1), effectively we annotate only half of a text.Compared to the 80 training texts needed, this is ahuge reduction.
In order to achieve similar perfor-mance, we only need to annotate 1/160 or 0.63%of the complete set of training instances under ourframework of domain adaptation with active learn-ing.Lastly, although in this paper we reported exper-imental results with the MUC evaluation metric,we also evaluated our approach with other evalu-ation metrics for coreference resolution, e.g., theB-CUBED metric, and obtained similar findings.7 ConclusionIn this paper, we presented an approach usingdomain adaptation with active learning to adaptcoreference resolution from the newswire domainto the biomedical domain.
We explored the ef-fect of domain adaptation, active learning, andtarget domain instance weighting for coreferenceresolution.
Experimental results showed that do-main adaptation with active learning and the tar-get instance weighting scheme achieved a simi-lar performance on MEDLINE abstracts but witha greatly reduced number of annotated traininginstances, compared to a system trained on fullcoreference annotations.AcknowledgmentsThis research is supported by the Singapore Na-tional Research Foundation under its InternationalResearch Centre @ Singapore Funding Initiativeand administered by the IDM Programme Office.ReferencesJose?
Castan?o, Jason Zhang, and James Pustejovsky.2002.
Anaphora resolution in biomedical literature.In Proceedings of the International Symposium onReference Resolution.28Yee Seng Chan and Hwee Tou Ng.
2007.
Domainadaptation with active learning for word sense dis-ambiguation.
In Proceedings of the ACL2007.Nancy Chinchor.
2011.
Statistical significance ofMUC-6 results.
In Proceedings of the Sixth Mes-sage Understanding Conference.K.
Bretonnel Cohen, Arrick Lanfranchi, WilliamCorvey, William A. Baumgartner Jr., ChristopheRoeder, Philip V. Ogren, Martha Palmer, andLawrence Hunter.
2010.
Annotation of all coref-erence in biomedical text: Guideline selection andadaptation.
In BioTxtM 2010.Daniel Dahlmeier and Hwee Tou Ng.
2010.
Domainadaptation for semantic role labeling in the biomed-ical domain.
Bioinformatics, 26(8):1098?1104.Hal Daume III.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of the ACL2007.Caroline Gasperin and Ted Briscoe.
2008.
Statisticalanaphora resolution in biomedical texts.
In Proceed-ings of the COLING2008.Caroline Gasperin, Nikiforos Karamanis, and RuthSeal.
2007.
Annotation of anaphoric relations inbiomedical full-text articles using a domain-relevantscheme.
In Proceedings of the DAARC2007.Caroline Gasperin.
2009.
Active learning for anaphoraresolution.
In Proceedings of the NAACL-HLT2009Workshop on Active Learning for Natural LanguageProcessing.Jing Jiang and ChengXiang Zhai.
2007.
Instanceweighting for domain adaptation in NLP.
In Pro-ceedings of the ACL2007.Jin-Dong Kim, Sampo Pyysalo, Tomoko Ohta, RobertBossy, Ngan Nguyen, and Jun?ichi Tsujii.
2011a.Overview of BioNLP shared task 2011.
In Proceed-ings of BioNLP Shared Task 2011 Workshop.Youngjun Kim, Ellen Riloff, and Nathan Gilbert.2011b.
The taming of Reconcile as a biomedicalcoreference resolver.
In Proceedings of BioNLPShared Task 2011 Workshop.Florian Laws, Florian Heimerl, and Hinrich Schu?tze.2012.
Active learning for coreference resolution.
InProceedings of the NAACL2012.David D. Lewis and William A. Gale.
1994.
A se-quential algorithm for training text classifiers.
InProceedings of the SIGIR1994.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Timothy A. Miller, Dmitriy Dligach, and Guergana K.Savova.
2012.
Active learning for coreference reso-lution.
In Proceedings of the BioNLP2012.Makoto Miwa, Paul Thompson, and Sophia Ana-niadou.
2012.
Boosting automatic event ex-traction from the literature using domain adapta-tion and coreference resolution.
Bioinformatics,28(13):1759?1765.MUC-6.
1995.
Coreference task definition (v2.3, 8Sep 95).
In Proceedings of the Sixth Message Un-derstanding Conference (MUC-6).MUC-7.
1998.
Coreference task definition (v3.0, 13Jul 97).
In Proceedings of the Seventh Message Un-derstanding Conference (MUC-7).Vincent Ng and Claire Cardie.
2002.
Improving ma-chine learning approaches to coreference resolution.In Proceedings of the ACL2002.NIST.
2002.
The ACE 2002 evaluationplan.
ftp://jaguar.ncsl.nist.gov/ace/doc/ACE-EvalPlan-2002-v06.pdf.Piyush Rai, Avishek Saha, Hal Daume, and SureshVenkatasubramanian.
2010.
Domain adapta-tion meets active learning.
In Proceedings of theNAACL-HLT2010 Workshop on Active Learning forNatural Language Processing.Wee Meng Soon, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational Linguistics, 27(4):521?544.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, andEllen Riloff.
2009.
Conundrums in noun phrasecoreference resolution: Making sense of the state-of-the-art.
In Proceedings of the ACL-IJCNLP2009.Min Tang, Xiaoqiang Luo, and Salim Roukos.
2002.Active learning for statistical natural language pars-ing.
In Proceedings of the ACL2002.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceed-ings of the MUC-6.Xiaofeng Yang, Guodong Zhou, Jian Su, andChew Lim Tan.
2004.
Improving noun phrasecoreference resolution by matching strings.
In Pro-ceedings of the IJCNLP2004.Jian Bo Yang, Qi Mao, Qiao Liang Xiang, Ivor W.Tsang, Kian Ming A. Chai, and Hai Leong Chieu.2012.
Domain adaptation for coreference resolu-tion: An adaptive ensemble approach.
In Proceed-ings of the EMNLP2012.Shanheng Zhao and Hwee Tou Ng.
2010.
Maximummetric score training for coreference resolution.
InProceedings of the COLING2010.Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan.
2008.Word sense disambiguation using OntoNotes: Anempirical study.
In Proceedings of the EMNLP2008.29
