Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 209?212,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsConstruct State Modification in the Arabic TreebankRyan GabbardDepartment of Computer and Information ScienceUniversity of Pennsylvaniagabbard@seas.upenn.eduSeth KulickLinguistic Data ConsortiumInstitute for Research in Cognitive ScienceUniversity of Pennsylvaniaskulick@seas.upenn.eduAbstractEarlier work in parsing Arabic has specu-lated that attachment to construct state con-structions decreases parsing performance.
Wemake this speculation precise and define theproblem of attachment to construct state con-structions in the Arabic Treebank.
We presentthe first statistics that quantify the problem.We provide a baseline and the results from afirst attempt at a discriminative learning pro-cedure for this task, achieving 80% accuracy.1 IntroductionEarlier work on parsing the Arabic Treebank (Kulicket al, 2006) noted that prepositional phrase attach-ment was significantly worse on the Arabic Tree-bank (ATB) than the English Penn Treebank (PTB)and speculated that this was due to the ubiquitouspresence of construct state NPs in the ATB.
Con-struct state NPs, also known as iDAfa1 (??A?
@) con-structions, are those in which (roughly) two or morewords, usually nouns, are grouped tightly together,often corresponding to what in English would beexpressed with a noun-noun compound or a pos-sessive construction (Ryding, 2005)[pp.205?227].In the ATB these constructions are annotated as aNP headed by a NOUN with an NP complement.
(Kulick et al, 2006) noted that this created verydifferent contexts for PP attachment to ?base NPs?,likely leading to the lower results for PP attachment.1Throughout this paper we use the Buckwalter Arabictransliteration scheme (Buckwalter, 2004).In this paper we make their speculation preciseand define the problem of attachment to constructstate constructions in the ATB by extracting out suchiDAfa constructions2 and their modifiers.
We pro-vide the first statistics we are aware of that quantifythe number and complexity of iDAfas in the ATBand the variety of modifier attachments within them.Additionally, we provide the first baseline for thisproblem as well as preliminary results from a dis-criminative learning procedure for the task.2 The Problem in More DetailAs mentioned above, iDAfa constructions in theATB are annotated as a NOUN with an NP comple-ment (ATB, 2008).
This can also be recursive, in thatthe NP complement can itself be an iDAfa construc-tion.
For example, Figure 1 shows such a complexiDAfa.
We refer to an iDAfa of the form (NP NOUN(NP NOUN)) as a two?level iDAfa, one of theform (NP NOUN (NP NOUN (NP NOUN))) asa three?level iDAfa (as in Figure 1), and so on.
Mod-ification can take place at any of the NPs in theseiDAfas, using the usual adjunction structure, as inFigure 2 (in which the modifier itself contains aniDAfa as the object of the PREP fiy).3This annotation of the iDAfa construction has acrucial impact upon the usual problem of PP at-tachment.
Consider first the PP attachment prob-lem for the PTB.
The PTB annotation style (Bies et2Throughout the rest of this paper, we will refer for conve-nience to iDAfa constructions instead of ?construct state NPs?.3In all these tree examples we leave out the Part of Speechtags to lessen clutter, and likewise for the nonterminal functiontags.209(NP $awAriE[streets](NP madiyn+ap[city](NP luwnog byt$)))[Long] [Beach]?JK.?K???JKY?
?P@??Figure 1: A three level idafa, meaning the streets of thecity of Long Beach(NP $awAriE[streets](NP (NP madiyn+ap[city](NP luwnog byt$))[Long] [Beach](PP fiy[in](NP wilAy+ap[state](NP kAliyfuwrniyA)))))[California]AJKP???
A??KB????JK.?K???JKY?
?P@??Figure 2: Three level iDAfa with modification, meaningthe streets of the city of Long Beach in the state of Cali-forniaal., 1995) forces multiple PP modifiers of the sameNP to be at the same level, disallowing the struc-ture (B) in favor of structure (A) in Figure 3, andparsers can take advantage of this restriction.
Forexample, (Collins, 1999)[pp.
211-12] uses the no-tion of a ?base NP?
(roughly, a non?recursive NP,that is, one without an internal NP) to control PP at-tachment, so that the parser will not mistakenly gen-erate the (B) structure, since it learns that PPs attachto non?recursive, but not recursive, NPs.Now consider again the PP attachment problemin the ATB.
The ATB guidelines also enforce the re-striction in Figure 3, so that multiple modifiers of anNP within an iDAfa will be at the same level (e.g.,another PP modifier of ?the city of Long Beach?in Figure 2 would be at the same level as ?in thestate...?).
However, the iDAfa annotation, indepen-dently of this annotation constraint, results in the PPmodification of many NPs that are not base NPs, aswith the PP modifier ?in the state...?
in Figure 2.
Oneway to view what is happening here is that Arabicuses the iDAfa construction to express what is often(A) multi-level PP attachment at same level ?
al-lowed(NP (NP ...)(PP ....)(PP ....))(B) multi-level PP attachment at different levels ?not allowed(NP (NP (NP ...)(PP ....)(PP ....))Figure 3: Multiple PP attachment in the PTB(NP (NP streets)(PP of(NP (NP the city)(PP of (NP Long Beach))(PP in (NP the state)(PP of(NP California))))))Figure 4: The English analog of Figure 2a PP in English.
The PTB analog of the troublesomeiDAfa with PP attachment in Figure 2 would be thesimpler structure in Figure 4, with two PP attach-ments to the base NP ?the city.?
The PP modifier ?ofLong Beach?
in English becomes part of iDAfa con-struction in Arabic.In addition, PPs can modify any level in an iDAfaconstruction, so there can be modification within aniDAfa of either a recursive or base NP.
There canalso be modifiers of multiple terms in an iDAfa.4The upshot is that the PP modification is more freein the ATB than in the PTB, and base NPs are nolonger adequate to control PP attachment.
(Kulick etal., 2006) present data showing that PP attachment toa non?recursive NP is virtually non?existent in thePTB, while it is the 16th most frequent dependencyin the ATB, and that the performance of the parserthey worked with (the Bikel implementation (Bikel,2004) of the Collins parser) was significantly loweron PP attachment for the ATB than for PTB.The data we used was the recently completed re-vision of 100K words from the ATB3 ANNAHARcorpus (Maamouri et al, 2007).
We extracted all oc-4An iDAfa cannot be interrupted by modifiers for non-finalterms, meaning that multiple modifiers will be grouped togetherfollowing the iDAfa.
Also, a single adjective can modify a nounwithin the lowest NP, i.e., inside the base NP.210Number of Modifiers Percent of iDAfas1 72.42 20.63 5.24 1.05 0.28 0.6Table 1: Number of modifiers per iDAfaDepth Percent of Idafas2 75.53 19.94 3.85 0.86 0.1Table 2: Distribution of depths of iDAfascurrences of NP constituents with a NOUN or NOUN?like head (NOUN PROP, NUM, NOUN QUANT) and aNP complement.This extraction results in 9472 iDAfas of which3877 of which have modifiers.
The average numberof idafas per sentence is 3.06.3 Some ResultsIn the usual manner, we divided the data into train-ing, development test, and test sections according toan 80/10/10 division.
As the work in this paper ispreliminary, the test section is not used and all re-sults are from the dev?test section.By extracting counts from the training section,we obtained some information about the behaviorof iDAfas.
In Table 1 we see that of iDAfas whichhave at least one modifier, most (72%) have onlyone modifier, and a sizable number (21%) have two,while a handful have as many as eight.
Almost alliDAfas are of depth three or less (Table 2), with thedeepest depth in our training set being six.Finally, we observe that the distributions of at-tachment depths of modifiers differs significantly fordifferent depths of iDAfas (table 3).
All depths havesomewhat of a preference for attachment at the bot-tom (43% for depth two and 36% for depths threeand four), but the top is a much more popular attach-ment site for depth two idafas (39%) than it is forDepth 2 Depth 3 Depth 4Level 0 39.0 19.3 16.1Level 1 17.9 34.8 14.1Level 2 43.0 9.9 23.6Level 3 36.0 10.1Level 4 36.2Table 3: For each total iDAfa depth, the percentage ofattachments at each level.
iDAfa depths of five or aboveare omitted due to the small number of such cases.deeper ones.
Level one attachments are very com-mon for depth three iDAfas for reasons which areunclear.Based on these observations, we would expect asimple baseline which attaches at the most commondepth to do quite poorly.
We confirm this by buildinga statistical model for iDAfa attachment, which wethen use for exploring some features which mightbe useful for the task, either as a separate post?processing step or within a parser.To simplify the learning task, we make the inde-pendence assumption that all modifier attachmentsto an iDAfa are independent of one another subjectto the constraint that later attachments may not at-tach deeper than earlier ones.
We then model theprobabilities of each of these attachments with amaximum entropy model and use a straightforwarddynamic programming search to find the most prob-able assignment to all the attachments together.
For-mally, we assign each attachment a numerical depth(0 for top, 1 for the position below the top, and soon) and then we findargmaxa1,...,ann?1P (a1) .
.
.
P (an) s.t.
?x : ax <= ax?1Our baseline system uses only the depth of theattachment as a feature.
We built further systemswhich used the following bundles of features:AttSym Adds the part?of?speech tag or non?terminal symbol of the modifier.Lex Pairs the headword of the modifier with thenoun it is modifying.TotDepth Conjunction of the attachment location,the AttSym feature, and the total depth of theiDAfa.211Features AccuracyBase 39.7Base+AttSym 76.1Base+Lex 58.4Base+Lex+AttSym 79.9Base+Lex+AttSym+TotDepth 78.7Base+Lex+AttSym+GenAgr 79.3Table 4: Attachment accuracy on development test datafor our model trained on various feature bundles.GenAgr A ?full?
gender feature consisting of theAttSym feature conjoined with the the pair ofthe gender and number suffixes of the head ofthe modifier and the word being modified and a?simple?
gender feature which is the same ex-cept it omits number.Results are in table 4.
Our most useful feature isclearly AttSym, with Lex also providing significantinformation.
Combining them allows us to achieve80% accuracy.
However, attempts to improve onthis by using gender agreement or taking advantageof the differing attachment distributions for differ-ent iDAfa depths (3) were ineffective.
In the caseof gender agreement, it may be ineffective becausenon?human plurals have feminine singular genderagreement, but there is no annotation for humannessin the ATB.4 ConclusionWe have presented an initial exploration of theiDAfa attachment problem in Arabic and have pre-sented the first data on iDAfa attachment distribu-tions.
We have also demonstrated that a combina-tion of lexical information and the top symbols ofmodifiers can achieve 80% accuracy on the task.There is much room for further work here.
Itis possible a more sophisticated statistical modelwhich eliminates the assumption that modifier at-tachments are independent of each other and whichdoes global rather than local normalization would bemore effective.
We also plan to look into addingmore features or enhancing existing features (e.g.try to get more effective gender agreement by ap-proximating annotation for humanness).
Some con-structions, such as the false iDAfa, require more in-vestigation, and we can also expand the range of in-vestigation to include coordination within an iDAfa.The more general plan is to incorporate this workwithin a larger Arabic NLP system.
This could per-haps be as a phase following a base phrase chunker(Diab, 2007), or after a parser, either correcting orcompleting the parser output.AcknowledgmentsWe thank Mitch Marcus, Ann Bies, MohamedMaamouri, and the members of the Arabic Treebankproject for helpful discussions.
This work was sup-ported in part under the GALE program of the De-fense Advanced Research Projects Agency, ContractNos.
HR0011-06-C-0022 and HR0011-06-1-0003.The content of this paper does not necessarily reflectthe position or the policy of the Government, and noofficial endorsement should be inferred.ReferencesATB.
2008.
Arabic Treebank Mor-phological and Syntactic guidelines.http://projects.ldc.upenn.edu/ArabicTreebank.Ann Bies, Mark Ferguson, Karen Karz, and Robert Mac-Intyre.
1995.
Bracketing guidelines for Treebank II-style Penn Treebank project.
Technical report, Univer-sity of Pennsylvania.Daniel M. Bikel.
2004.
Intricacies of Collins?
parsingmodel.
Computational Linguistics, 30(4).Tim Buckwalter.
2004.
Arabic morphological analyzerversion 2.0.
LDC2004L02.
Linguistic Data Consor-tium.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Depart-ment of Computer and Information Sciences, Univer-sity of Pennsylvania.Mona Diab.
2007.
Improved Arabic base phrase chunk-ing with a new enriched pos tag set.
In Proceedings ofthe 2007 Workshop on Computational Approaches toSemitic Languages.Seth Kulick, Ryan Gabbard, and Mitchell Marcus.
2006.Parsing the Arabic Treebank: Analysis and improve-ments.
In Proceedings of TLT 2006.
Treebanks andLinguistic Theories.Mohamed Maamouri, Ann Bies, Seth Kulick, FatmaGadeche, and Wigdan Mekki.
2007.
Arabic treebank3(a) - v2.6.
LDC2007E65.
Linguistic Data Consor-tium.Karin C. Ryding.
2005.
A Reference Grammar of Mod-ern Standard Arabic.
Cambridge University Press.212
