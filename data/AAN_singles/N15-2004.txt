Proceedings of NAACL-HLT 2015 Student Research Workshop (SRW), pages 25?32,Denver, Colorado, June 1, 2015.c?2015 Association for Computational LinguisticsRelation extraction pattern ranking using word similarityKonstantinos Lambrou-Latreille(1)?Ecole Polytechnique de Montr?eal(2) Centre de Recherche Informatique de Montr?ealMontr?eal, Qu?ebec, Canadakonstantinos.lambrou-latreille@polymtl.caAbstractOur thesis proposal aims at integrating wordsimilarity measures in pattern ranking for rela-tion extraction bootstrapping algorithms.
Wenote that although many contributions havebeen done on pattern ranking schemas, fewexplored the use of word-level semantic sim-ilarity.
Our hypothesis is that word similar-ity would allow better pattern comparison andbetter pattern ranking, resulting in less seman-tic drift commonly problematic in bootstrap-ping algorithms.
In this paper, as a first stepinto this research, we explore different pat-tern representations, various existing patternranking approaches and some word similaritymeasures.
We also present a methodology andevaluation approach to test our hypothesis.1 IntroductionIn this thesis, we look at the problem of informationextraction from the web; more precisely at the prob-lem of extracting structured information, in the formof triples (predicate, subject, object), e.g.
(Object-MadeFromMaterial, table, wood) from unstructuredtext.
This topic of Relation Extraction (RE), is a cur-rent and popular research topic within NLP, giventhe large amount of unstructured text on the WWW.In the literature, machine learning algorithmshave shown to be very useful for RE from tex-tual resources.
Although supervised (Culotta andSorensen, 2004; Bunescu and Mooney, 2005)and unsupervised learning (Hasegawa et al, 2004;Zhang et al, 2005) have been used for RE, in thisthesis, we will focus on semi-supervised bootstrap-ping algorithms.In such algorithms (Brin, 1999; Agichtein andGravano, 2000; Alfonseca et al, 2006a), the inputis a set of related pairs called seed instances (e.g.,(table,wood), (bottle, glass)) for a specific relation(e.g., ObjectMadeFromMaterial).
These seed in-stances are used to collect a set of candidate pat-terns representing the relation in a corpus.
A sub-set containing the best candidate patterns is added inthe set of promoted patterns.
The promoted patternsare used to collect candidate instances.
A subsetcontaining the best candidate instances is selected toform the set of promoted instances.
The promotedinstances are either added to the initial seed set orused to replace it.
With the new seed set, the algo-rithm is repeated until a stopping criterion is met.The advantage of bootstrapping algorithms is thatthey require little human annotation.
Unfortunately,the system may introduce wrongly extracted in-stances.
Due to its iterative approach, errors canquickly cumulate in the next few iterations; there-fore, precision will suffer.
This problem is calledsemantic drift.
Different researchers have studiedhow to counter semantic drift by using better patternrepresentations, by filtering unreliable patterns, andfiltering wrongly extracted instances (Brin, 1999;Agichtein and Gravano, 2000; Alfonseca et al,2006a).
Nevertheless, this challenge is far from be-ing resolved, and we hope to make a contribution inthat direction.The semantic drift is directly related to which can-didate patterns become promoted patterns.
A cru-cial decision at that point is how to establish pat-tern confidence so as to rank the patterns.
There aremany ways to estimate the confidence of a pattern.25Blohm et al (2007) identified general types of pat-tern filtering functions for well-known systems.
Aswe review pattern ranking approaches, we note thatmany include a notion of ?resemblance?, as eithercomparing patterns between successive iterations,or comparing instances generated at an iteration toinstances in the seed set, etc.
Although this no-tion of resemblance seems important to many rank-ing schemas, we do not find much research whichcombines word similarity approaches within patternranking.
This is where we hope to make a researchcontribution and where our hypothesis lies, that us-ing word similarity would allow for better patternranking.In order to suggest better pattern ranking ap-proaches incorporating word similarity, we need tolook at the different pattern representations sug-gested in the literature and understand how theyaffect pattern similarity measures.
This is intro-duced in Section 2.
Then, section 3 provides a non-exhaustive survey of pattern ranking approacheswith an analysis of commonality and differences;Section 4 presents a few word similarity approaches;Section 5 presents the challenges we face, as wellas our methodology toward the validation of our hy-pothesis; Section 6 briefly explores other anticipatedissues (e.g.
seed selection) in relation to our maincontribution and Section 7 presents the conclusion.2 Pattern representationIn the literature, pattern representations are classi-fied as lexical or syntactic.Lexical patterns represent lexical terms arounda relation instance as a pattern.
For relation in-stance (X,Y) where X and Y are valid noun phrases,Brin (1999), Agichtein and Gravano (2000), Pascaet al (2006), Alfonseca et al (2006a) take N wordsbefore X, N words after Y and all intervening wordsbetween X and Y to form a pattern (e.g., well-knownauthor X worked on Y daily.).
Extremes for thechoice of N exist, as in the CPL subsystem ofNELL (Carlson et al, 2010) setting N = 0 andthe opposite in Espresso (Pantel and Pennacchiotti,2006) where the whole sentence is used.Syntactic patterns convert a sentence containinga relation instance to a structured form such as aparse tree or a dependency tree.
Yangarber (2003)and Stevenson and Greenwood (2005) use Subject-Verb-Object (SVO) dependency tree patterns suchas [Company appoint Person] or [Person quit].
Cu-lotta (2004) uses full dependency trees on whicha tree kernel will be used to measure similarity.Bunescu and Mooney (2005) and Sun and Grish-man (2010) use the shortest dependency path (SDP)between a relation instance in the dependency treeas a pattern (e.g., ?nsubj?met?
prep in?).
Zhanget al (2014) add a semantic constraint to the SDP;they define the semantic shortest dependency path(SSDP) as a SDP containing at least one triggerword representing the relation, if any.
Trigger wordsare defined as words most representative of the tar-get relation (e.g.
home, house, live, for the relationPersonResidesIn).We anticipate the use of word similarity to bepossible when comparing either lexical or syntac-tic patterns, adapting to either words in sequence,or nodes within parse or dependency trees.
In fact,as researchers have explored pattern generalization,some have already looked at ways of grouping sim-ilar words.
For example, Alfonseca et al (2006a)present a simple algorithm to generalize the setof lexical patterns using an edit-distance similarity.Also, Pasca et al (2006) add term generalizationto a pattern representation similar to Agichtein andGravano (2000); terms are replaced with their cor-responding classes of distributionally similar words,if any (e.g., let CL3 = {March, October, April,...} inthe pattern CL3 00th : X?s Birthday (Y)).3 Pattern ranking approachesWe now survey pattern ranking algorithms to bet-ter understand in which ones similarity measureswould be more likely to have an impact.
Wefollow a categorization introduced in Blohm etal.
(2007) as they quantified the impact of differentrelation pattern/instance filtering functions on theirgeneric bootstrapping algorithm.
The filtering func-tions proposed by Brin (1999), Agichtein and Gra-vano (2000), Pantel and Pennacchiotti (2006) andEtzioni et al (2004) were described in their work.Although non-exhaustive, our survey includesfurther pattern ranking approaches found in the lit-erature, in order to best illustrate Blohm?s differentcategories.
A potential use of those categories would26be to define a pattern ranking measure composed ofvoting experts representing each category.
A com-bination of these votes might provide a better confi-dence measure for a pattern.We define the following notation, as to allow thedescription of the different measures in a coherentway.
Let p be a pattern and i be an instance; I is theset of promoted instances; P is the set of promotedpatterns;H(p) is the set of unique instances matchedby p; K(i) is the set of unique patterns matchingi; count(i, p) is the number of times p matches i;count(p) is the number of p occurs in a corpus; S isthe set of seed instances.3.1 Syntactic assessmentThis filtering assessment is purely based on the syn-tactic criteria (e.g., length, structure, etc.)
of the pat-tern.
Brin (1999) uses the length of the pattern tomeasure its specificity.3.2 Pattern comparisonBlohm et al (2007) named this category inter-pattern comparison.
Their intuition was that can-didate patterns could be rated based on how similartheir generated instances are in comparison to theinstances generated by the promoted patterns.
Wegeneralize this category to also include rating of can-didate patterns based directly on their semantic sim-ilarity with promoted pattern.Stevenson and Greenwood (2005) assign a scoreon a candidate pattern based on the similarity withpromoted patterns.
The pattern scoring functionuses the Jiang and Conrath (1997) WordNet-basedword similarity for pattern similarity.
They rep-resent the SVO pattern as a vector (e.g., [sub-ject COMPANY, verb fired, object ceo], or [sub-ject chairman, verb resign]).
The similarity be-tween two pattern vectors is measured as :sim(~a,~b) =~a?W ?~bT|~a| ?
|~b|(1)where W is a matrix that contains the word sim-ilarity between every possible element-filler pairs(e.g., subject COMPANY, verb fired, object ceo)contained in every SVO pattern extracted from a cor-pus.
The top-N (e.g., 4) patterns with a score largerthan 95% are promoted.Zhang et al (2014) defines a bottom-up kernel(BUK) to filter undesired relation patterns.
TheBUK measures the similarity between two depen-dency tree patterns.
The system accepts new pat-terns that are the most similar to seed patterns.
TheBUK defines a matching function t and a similarityfunction k on dependency trees.
Let dep be the pair(rel, w) where rel is the dependency relation and wis the word of the relation (e.g., (nsubj, son)).
Thematching function is defined as:t(dep1, dep2) ={1 if dep1.w, dep2.w ?Wtr0 otherwise(2)where Wtris the set of trigger words for the targetrelation.
The similarity function is defined as:k(dep1, dep2) =?????
?1+ ?2if dep1.rel = dep2.rel && dep1.w = dep2.w?1if dep1.w = dep2.w0 otherwise(3)where ?1and ?2are manually defined weights forattributes dep.w and dep.rel respectively.
The wordcomparison is string-based.3.3 Support-based assessmentThis ranking assessment estimates the quality of apattern based on the set of occurrences/patterns thatgenerated this pattern.
This assessment is usuallyused for patterns that were created by a general-ization procedure.
For example, if pattern X BEmostly/usually made of/from Y was generated by pat-terns X is usually made of Y and X are mostly madefrom Y, then the quality of the generalized patternwill be based on the last two patterns.
Brin (1999)filters patterns if (specificity(p) ?
n) > t, wheren is the occurrence count of pattern p applied in acorpus and t is a manually set threshold.3.4 Performance-based assessmentThe quality of a candidate pattern can be estimatedby the comparing its correctly produced instanceswith the set of promoted instances.Blohm et al (2007) defines a precision formulasimilar to Agichtein and Gravano (2000) to approx-imate a performance-based precision:prec(p) =|H(p) ?
S||H(p)|(4)Alfonseca et al (2006b) propose a procedure tomeasure the precision of candidate patterns in order27to filter overly-general patterns.
For every relation,and every hook X and target Y of the set of pro-moted instances (X,Y), a hook and target corpus isextracted from corpus C; C contains only sentenceswhich contain X or Y.
For every pattern p, instancesof H(p) are extracted.
Then, a set of heuristics labelevery instance as correct/incorrect.
The precision ofp is number of correct extracted instances divided bythe total number of extracted instances.NELL (Carlson et al, 2010) ranks relation pat-terns by their precision:prec(p) =?i?Icount(i, p)count(p)(5)Sijia et al (2013) filters noisy candidate relationpatterns that generate instances which appear in theseed set of relations other than the target relation.3.5 Instance-Pattern correlationPattern quality can be assessed by measuring its cor-relation with the set of promoted instances.
Thesemeasures estimate the correlation by counting pat-tern occurrences, promoted instance occurrences,and pattern occurrences with a specific promoted in-stance.Blohm et al (2007) classified Espresso (Panteland Pennacchiotti, 2006) and KnowItAll (Etzioni etal., 2004) in this category.Pantel et Pennacchiotti (2006) ranks candidate re-lation patterns by the following reliability score:rpi(p) =?i?I(pmi(i,p)maxpmi?
rl(i))|I|(6)where maxpmiis the maximum PMI between allpattern and all instances, and pmi(i, p) can be esti-mated using the following formula:pmi(i, p) = log(|x, p, y||x, ?, y| ?
|?, p, ?|)(7)where i is an instance (x,y), |x, p, y| is the occur-rence of pattern p with terms x and y and (*) repre-sents a wildcard.
The reliability of an instance rl(i)is defined as:rl(i) =?p?P(pmi(i,p)maxpmi?
rpi(p))|P |(8)Since rl(i) and rpi(p) are defined recursively,rl(i) = 1 for any seed instance.
The top-N patternsare promoted where N is the number of patterns ofthe previous bootstrapping iteration plus one.Sun and Grishman (2010) accept the top-N rankedcandidate pattern by the following confidence for-mula:Conf(p) =Sup(p)|H(p)|?
logSup(p) (9)where Sup(p) =?i?H(p)Conf(i) is the supportcandidate pattern p can get from the set of matchedinstances.
Every relation instance in Sun and Gr-ishman (2010) has a cluster membership, where acluster contains similar patterns.
The confidence ofan newly extracted instance i is defined as:Conf(i) = 2?Semi Conf(i)?Cluster Conf(i)Semi Conf(i)+Cluster Conf(i)(10)Semi Conf(i) = 1??p?K(p)(1?
Prec(p)) (11)Cluster Conf(i) = Prob(i ?
Ct)=?p?Ctcount(i, p)|K(i)|(12)whereCtis the target cluster where the patterns ofthe target relation belong, Semi Conf(i) is definedas the confidence given by the patterns matching thecandidate relation instance and Cluster Conf(i) isdefined how strongly a candidate instance is associ-ated with the target cluster.4 Word similarityWithin the pattern ranking survey, we often sawthe idea of comparing patterns and/or instances, butonly once, was there a direct use of word similaritymeasures.
Stevenson and Greenwood (2005) assigna score to a candidate pattern based on its similarityto promoted patterns using a WordNet-based wordsimilarity measure (Jiang and Conrath, 1997).
Thismeasure is only one among many WordNet-basedapproaches, as can be found in (Lesk, 1986; Wuand Palmer, 1994; Resnik, 1995; Jiang and Conrath,1997; Lin, 1998; Leacock and Chodorow, 1998;Banerjee and Pedersen, 2002).There are limitations to these approaches, mainlythat WordNet (Miller, 1995), although large, is stillincomplete.
Other similarity approaches are corpus-based (e.g.
(Agirre et al, 2009)) where the distribu-tional similarity between words is measured.
Words28are no longer primitives, but they are represented bya feature vector.
The feature vector could contain theco-occurrences, the syntactic dependencies, etc.
ofthe word with their corresponding frequencies froma corpus.
The cosine similarity (among many pos-sible measures) between the feature vector of twowords indicates their semantic similarity.Newer approaches to word similarity are basedon neural network word embeddings.
Mikolov etal.
(2013) present algorithms to learn those dis-tributed word representations which can then becompared to provide word similarity estimations.Word similarity could be in itself the topic of athesis.
Therefore, we will not attempt at develop-ing new word similarity measures, but rather we willsearch for measures which are intrinsically good andvaluable for the pattern ranking task.
The few men-tioned above are a good start toward a more ex-tensive survey.
The methods found can be evalu-ated on existing datasets such as RG (Rubensteinand Goodenough, 1965), MC (Miller and Charles,1991), WordSim353 (Finkelstein et al, 2001; Agirreet al, 2009), MTurk (Radinsky et al, 2011) andMEN (Bruni et al, 2013) datasets.
However, thesedatasets are limited, since they contain only nouns(except MEN).
When using word similarity in pat-tern ranking schemas, we will likely want to mea-sure similarity between nouns, verbs, adjectives andadverbs.
Still, these datasets provide a good startingpoint for evaluation of word similarity.5 Word similarity in pattern rankingThe hypothesis of our research is that the use ofword similarity will allow better pattern ranking tobetter prevent semantic drift.
We face three mainchallenges in supporting this hypothesis.
First, weneed to understand the interdependence of the threeelements presented in the three previous sections:pattern representation, pattern confidence estima-tion, and word similarity.
Second, we need to devisean appropriate set-up to perform our bootstrappingapproach.
Third, we need to properly evaluate therole of the different variations in preventing seman-tic drift.An important exploration will be to decide wherethe word similarity has the largest potential.
Forexample, in the work of Stevenson and Green-wood (2005), similarity is directly applied on partsof the triples found (Subject, Verb predicate or Ob-ject), or in the work of Zhang et al (2014), wordsimilarity would be integrated in the matching andsimilarity functions over dependency trees, insteadof using string equality.As we see, the integration of word similarity mea-sures would be different depending on the type ofpattern representation used.
Furthermore, in somerepresentation, there is already a notion of patterngeneralisation, such as in the work of Pasca etal.
(2006), where words are replaced with more gen-eral classes, if any.
In such case, word similaritymeasures are used at the core of the pattern repre-sentation, and will further impact pattern ranking.As we will eventually be building a complex sys-tem, we intend to follow a standard methodology ofstarting with a baseline system for which we havean evaluation, and then further evaluate the differ-ent variations to measure their impact.
As the num-ber of combination of possible variations will betoo large, time will be spent also on partial evalua-tion, to determine most promising candidates amongword similarity measures, and/or pattern representa-tion and/or pattern confidence estimation, to under-stand strength and weaknesses of each aspect inde-pendently of the others.Our proposed methodology is to take promisingranking approaches among the one presented in Sec-tion 3, and promising pattern representations fromwhat was presented in Section 2.
We can evalu-ate their combined performance throughN differentiteration intervals and incorporate different similar-ity measures (some best measures chosen from theevaluation on known datasets) to measure the per-formance of the system.As our baseline system, we are inspired by CPLsubsystem of NELL (Carlson et al, 2010) since itis one of the largest, currently active, bootstrappingsystem in the literature.
As in NELL, we will useClueWeb1as our corpus, and for the set of relations,we will use the same seed instances and relations asin the evaluation of NELL (Carlson et al, 2010).As for the bootstrapping RE system, to evaluatethe precision, we will randomly sample knowledgefrom the knowledge base and evaluate them by sev-1http://www.lemurproject.org/clueweb09/29eral human judges.
The extracted knowledge couldbe validated using a crowdsourcing application suchas MTurk.
This method is based on NELL (Carlsonet al, 2010).
To evaluate its recall, we have to con-centrate on already annotated relations.
For exam-ple, Pasca et al (2006) evaluates the relation Person-BornIN-Year.
As a Gold Standard, 6617 instanceswere automatically extracted from Wikipedia.
In-stead of measuring recall for specific relation, wecould use relative recall (Pantel et al, 2004; Pan-tel and Pennacchiotti, 2006).
We can evaluate ourcontributions by the relative recall of system A (oursystem) given system B (baseline).6 Related issues in pattern rankingOur main contribution on the impact of word simi-larity on pattern ranking will necessarily bring for-ward other interesting questions that we will addresswithin our thesis.6.1 Choice of seedAs we saw, pattern ranking is often dependent onthe comparison of instances found from one iterationto the next.
At iteration 0, we start with a seed ofinstances.
We can imagine that the manual selectionof these seeds will have an impact on the followingdecisions.
As our similarity measures are used tocompare candidate instances to seed instances, andas we will start with NELL seed set, we will want toevaluate its impact on the bootstrapping process.It was shown that the performance of bootstrap-ping algorithms highly depend on the seed instanceselection (Kozareva and Hovy, 2010).
Ehara etal.
(2013) proposed an iterative approach where un-labelled instances are chosen to be labelled depend-ing on their similarity with the seed instances andare added in the seed set.6.2 Automatic selection of patternsSomething noticeable among our surveyed patternranking approaches is the inclusion of empiricallyset thresholds that will definitely have an impact onthe semantic drift, but which impact is not discussed.Most authors (e.g (Carlson et al, 2010; Sun and Gr-ishman, 2010; McIntosh and Yencken, 2011; Zhanget al, 2014) among recent ones) select the top-Nbest ranked patterns to be promoted to next iteration.Other authors (Pasca et al, 2006; Dang and Aizawa,2008; Carlson et al, 2010) select the top-M rankedinstances to add in the seed set for the next iteration.Other authors (Brin, 1999; Agichtein and Gravano,2000; Sijia et al, 2013) only apply a filtering stepwithout limiting pattern/instance selection.In our work, including word similarity within pat-tern ranking will certainly impact the decision on thenumber of patterns to be promoted.
We hope to con-tribute in developing a pattern selection mechanismthat will be based on the pattern confidence them-selves rather than on an empirically set N or M.7 ConclusionIn this paper, we have presented our research pro-posal, aiming at determining the impact of employ-ing word similarity measures within pattern rankingapproaches in bootstrapping systems for relation ex-traction.
We presented two aspects of pattern rank-ing on which the integration of word similarity willbe dependent, that of pattern representation and pat-tern ranking schemas.
We showed that there areminimally lexical and syntactic pattern representa-tions on which different methods of generalizationscan be applied.
We performed a non-exhaustive sur-vey of pattern ranking measures classified in five dif-ferent categories.
We also briefly looked into differ-ent word similarity approaches.This sets the ground for the methodology that wewill pursue, that of implementing a baseline boot-strapping system (inspired by NELL, and workingwith ClueWeb as a corpus), and then measuring theimpact of modifying the pattern representation andthe pattern ranking approaches, with and without theuse of word similarity measures.
There is certainlya complex intricate mutual influence of the preced-ing aspects which we need to look into.
Lastly, webriefly discussed two related issues: the choice ofseed set and better estimation of number of patternsto promote.AcknowledgmentsThis work has seen the day with the help and ad-vice of Caroline Barri`ere, my research supervisorat CRIM.
This research project is partly funded byan NSERC grant RDCPJ417968-11, titled Toward asecond generation of an automatic product codingsystem.30ReferencesEugene Agichtein and Luis Gravano.
2000.
Snowball:Extracting Relations from Large Plain-Text Collec-tions.
In Proceedings of the fifth ACM conference onDigital libraries - DL ?00, pages 85?94, New York,New York, USA.
ACM Press.Eneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and WordNet-based approaches.
In Proceedingsof Human Language Technologies: The 2009 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics on - NAACL?09, pages 19?27, Morristown, NJ, USA.
Associationfor Computational Linguistics.Enrique Alfonseca, Pablo Castells, Manabu Okumara,and Maria Ruiz-Casado.
2006a.
A Rote Ex-tractor with Edit Distance-based Generalisation andMulti-corpora Precision Calculation.
In COLING-ACL?06 Proceedings of the COLING/ACL Poster Ses-sion, pages 9?16, Morristown, NJ, USA.
Associationfor Computational Linguistics.Enrique Alfonseca, Maria Ruiz-Casado, Manabu Oku-mura, and Pablo Castells.
2006b.
Towards Large-scale Non-taxonomic Relation Extraction : Estimatingthe Precision of Rote Extractors.
In Proceedings ofthe second workshop on ontology learning and popu-lation, Coling-ACL?2006, pages 49?56.Satanjeev Banerjee and Ted Pedersen.
2002.
An AdaptedLesk Algorithm for Word Sense Disambiguation Us-ing WordNet.
In Alexander Gelbukh, editor, Com-putational linguistics and intelligent text processing,volume 2276 of Lecture Notes in Computer Science,pages 136?145.
Springer Berlin Heidelberg, Berlin,Heidelberg, February.Sebastian Blohm, Philipp Cimiano, and E Stemle.
2007.Harvesting Relations from the Web - Quantifiying theImpact of Filtering Functions.
In Proceedings of theNational Conference on Artificial Intelligence, pages1316?1321.Sergey Brin.
1999.
Extracting Patterns and Relationsfrom the World Wide Web.
In Paolo Atzeni, Al-berto Mendelzon, and Giansalvatore Mecca, editors,The World Wide Web and Databases, Lecture Notesin Computer Science, chapter Extracting, pages 172?183.
Springer Berlin Heidelberg, Berlin, Heidelberg,March.Elia Bruni, Nam Khan Tran, and Marco Baroni.
2013.Multimodal distributional semantics.
Journal of Arti-cifial Intelligence Research, 48:1?47.Razvan C Bunescu and Raymond J Mooney.
2005.
AShortest Path Dependency Kernel for Relation Extrac-tion.
In Proceedings of Human Language Technol-ogy Conference and Conference on Empirical Methodsin Naural Language Processing (HLT/EMNLP), pages724?731, Vancouver, Canada.Andrew Carlson, Justin Betteridge, and Bryan Kisiel.2010.
Toward an Architecture for Never-Ending Lan-guage Learning.
In Proceedings of the Conference onArtificial Intelligence (AAAI), pages 1306?1313.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedings ofthe 42nd Annual Meeting on Association for Computa-tional Linguistics - ACL ?04, pages 423?429, Morris-town, NJ, USA.
Association for Computational Lin-guistics.VB Dang and Akiko Aizawa.
2008.
Multi-class namedentity recognition via bootstrapping with dependencytree-based patterns.
In Proceedings of the 12th Pacific-Asia conference on Advances in knowledge discoveryand data mining, pages 76?87.Yo Ehara, Issei Sato, Hidekazu Oiwa, and Hiroshi Nak-agawa.
2013.
Understanding seed selection in boot-strapping.
In Proceedings of the TextGraphs-8 Work-shop, pages 44?52, Seattle, Washington, USA.Oren Etzioni, Michael Cafarella, Doug Downey, StanleyKok, Ana-maria Popescu, Tal Shaked, Stephen Soder-land, Daniel S Weld, and Alexander Yates.
2004.Web-scale information extraction in knowitall: (pre-liminary results).
In Proceedings of the 13th confer-ence on World Wide Web - WWW ?04, page 100, NewYork, New York, USA.
ACM Press.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2001.
Placing search in context.
In Proceed-ings of the tenth international conference on WorldWide Web - WWW ?01, pages 406?414, New York,New York, USA.
ACM Press.Takaaki Hasegawa, Satoshi Sekine, and Ralph Grishman.2004.
Discovering relations among named entitiesfrom large corpora.
In Proceedings of the 42nd An-nual Meeting on Association for Computational Lin-guistics - ACL ?04, pages 415?422, Morristown, NJ,USA.
Association for Computational Linguistics.JJ Jiang and DW Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
In Inthe Proceedings of ROCLING X, Taiwan, pages 19?33.Zornitsa Kozareva and Eduard Hovy.
2010.
Not AllSeeds Are Equal : Measuring the Quality of Text Min-ing Seeds.
In Proceeding HLT ?10 Human LanguageTechnologies: The 2010 Annual Conference of theNorth American Chapter of the Association for Com-putational Linguistics, pages 618?626.Claudia Leacock and Martin Chodorow.
1998.
Combin-ing Local Context and WordNet Similarity for WordSense Identification.
In Christiane Fellbaum, editor,31WordNet: An electronic lexical database., pages 265?283.
MIT Press.Michael Lesk.
1986.
Automatic sense disambiguationusing machine readable dictionaries.
In Proceedingsof the 5th annual international conference on Sys-tems documentation - SIGDOC ?86, pages 24?26, NewYork, New York, USA.
ACM Press.Dekang Lin.
1998.
An Information-Theoretic Defini-tion of Similarity.
In ICML ?98 Proceedings of the Fif-teenth International Conference on Machine Learning,pages 296?304.T McIntosh and Lars Yencken.
2011.
Relation guidedbootstrapping of semantic lexicons.
In ProceedingHLT ?11 Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies: short papers - Volume2, pages 266?270.Tomas Mikolov, Greg Corrado, Kai Chen, and JeffreyDean.
2013.
Efficient Estimation of Word Represen-tations in Vector Space.
In Proceedings of the Interna-tional Conference on Learning Representations (ICLR2013), pages 1?12.George A. Miller and Walter G. Charles.
1991.
Contex-tual correlates of semantic similarity.
Language andCognitive Processes, 6(1):1?28.George A. Miller.
1995.
WordNet: A Lexical Databasefor English.
Communications of the ACM, 38(11):39?41.Patrick Pantel and Marco Pennacchiotti.
2006.
Espresso:leveraging generic patterns for automatically harvest-ing semantic relations.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand the 44th annual meeting of the ACL - ACL ?06,pages 113?120, Morristown, NJ, USA.
Association forComputational Linguistics.Patrick Pantel, Deepak Ravichandran, and Eduard Hovy.2004.
Towards terascale knowledge acquisition.
InProceedings of the 20th international conference onComputational Linguistics COLING 04, pages 771?777.M Pasca, Dekang Lin, Jeffrey Bigham, Andrei Lifchits,and A Jain.
2006.
Organizing and searching the worldwide web of facts-step one: the one-million fact ex-traction challenge.
AAAI, 6:1400?1405.Kira Radinsky, Eugene Agichtein, Evgeniy Gabrilovich,and Shaul Markovitch.
2011.
A word at a time: com-puting word relatedness using temporal semantic anal-ysis.
In Proceedings of the 20th international confer-ence on World wide web - WWW ?11, page 337, NewYork, New York, USA.
ACM Press.Philip Resnik.
1995.
Using IC to Evaluation the Seman-tic Similarity in a Taxonomy.
In Proceeding IJCAI?95Proceedings of the 14th international joint conferenceon Artificial intelligence - Volume 1, pages 448?453.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Communicationsof the ACM, 8(10):627?633, October.Chen Sijia, Li Yan, and Chen Guang.
2013.
Reduc-ing semantic drift in bootstrapping for entity relationextraction.
In Proceedings 2013 International Con-ference on Mechatronic Sciences, Electric Engineer-ing and Computer (MEC), pages 1947?1950.
Ieee, De-cember.Mark Stevenson and Mark A Greenwood.
2005.
A se-mantic approach to IE pattern induction.
In Proceed-ings of the 43rd Annual Meeting on Association forComputational Linguistics - ACL ?05, pages 379?386,Morristown, NJ, USA.
Association for ComputationalLinguistics.Ang Sun and Ralph Grishman.
2010.
Semi-supervisedSemantic Pattern Discovery with Guidance from Un-supervised Pattern Clusters.
In Proceedings of the23rd International Conference on Computational Lin-guistics: Posters, pages 1194?1202, Beijing.Zhibiao Wu and Martha Palmer.
1994.
Verbs seman-tics and lexical selection.
In Proceedings of the 32ndannual meeting on Association for Computational Lin-guistics -, pages 133?138, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Roman Yangarber.
2003.
Counter-training in discoveryof semantic patterns.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics - ACL ?03, volume 1, pages 343?350, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Min Zhang, Jian Su, Danmei Wang, Guodong Zhou, andChew Lim Tan.
2005.
Discovering Relations BetweenNamed Entities form a Large Raw Corpus Using TreeSimilarity-based Clustering.
In Robert Dale, Kam-FaiWong, Jian Su, and Oi Yee Kwong, editors, NaturalLanguage Processing ?
IJCNLP 2005, Lecture Notesin Computer Science, pages 378?389.
Springer BerlinHeidelberg, Berlin, Heidelberg.Chunyun Zhang, Weiran Xu, Sheng Gao, and Jun Guo.2014.
A bottom-up kernel of pattern learning for re-lation extraction.
In The 9th International Symposiumon Chinese Spoken Language Processing, pages 609?613.
IEEE, September.32
