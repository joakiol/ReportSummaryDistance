Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 523?530,Sydney, July 2006. c?2006 Association for Computational LinguisticsThe Role of Information Retrieval in Answering Complex QuestionsJimmy LinCollege of Information StudiesDepartment of Computer ScienceInstitute for Advanced Computer StudiesUniversity of MarylandCollege Park, MD 20742, USAjimmylin@umd.eduAbstractThis paper explores the role of informa-tion retrieval in answering ?relationship?questions, a new class complex informa-tion needs formally introduced in TREC2005.
Since information retrieval is of-ten an integral component of many ques-tion answering strategies, it is importantto understand the impact of different term-based techniques.
Within a framework ofsentence retrieval, we examine three fac-tors that contribute to question answer-ing performance: the use of different re-trieval engines, relevance (both at the doc-ument and sentence level), and redun-dancy.
Results point out the limitationsof purely term-based methods to this chal-lenging task.
Nevertheless, IR-based tech-niques provide a strong baseline on topof which more sophisticated language pro-cessing techniques can be deployed.1 IntroductionThe field of question answering arose from therecognition that the document does not occupy aprivileged position in the space of information ob-jects as the most ideal unit of retrieval.
Indeed, forcertain types of information needs, sub-documentsegments are preferred?an example is answers tofactoid questions such as ?Who won the NobelPrize for literature in 1972??
By leveraging so-phisticated language processing capabilities, fac-toid question answering systems are able to pin-point the exact span of text that directly satisfiesan information need.Nevertheless, IR engines remain integral com-ponents of question answering systems, primar-ily as a source of candidate documents that aresubsequently analyzed in greater detail.
Al-though this two-stage architecture was initiallyconceived as an expedient to overcome the com-putational processing bottleneck associated withmore sophisticated but slower language process-ing technology, it has worked quite well in prac-tice.
The architecture has since evolved into awidely-accepted paradigm for building workingsystems (Hirschman and Gaizauskas, 2001).Due to the reliance of QA systems on IR tech-nology, the relationship between them is an im-portant area of study.
For example, how sensi-tive is answer extraction performance to the ini-tial quality of the result set?
Does better docu-ment retrieval necessarily translate into more ac-curate answer extraction?
These answers can-not be solely determined from first principles,but must be addressed through empirical experi-ments.
Indeed, a number of works have specifi-cally examined the effects of information retrievalon question answering (Monz, 2003; Tellex et al,2003), including a dedicated workshop at SIGIR2004 (Gaizauskas et al, 2004).
More recently, theimportance of document retrieval has promptedNIST to introduce a document ranking subtask in-side the TREC 2005 QA track.However, the connection between QA and IRhas mostly been explored in the context of factoidquestions such as ?Who shot Abraham Lincoln?
?,which represent only a small fraction of all infor-mation needs.
In contrast to factoid questions,which can be answered by short phrases foundwithin an individual document, there is a largeclass of questions whose answers require synthe-sis of information from multiple sources.
The so-called definition/other questions at recent TRECevaluations (Voorhees, 2005) serve as good exam-ples: ?good answers?
to these questions include in-523Qid 25: The analyst is interested in the status of Fidel Castro?s brother.
Specifically, the analyst would likeinformation on his current plans and what role he may play after Fidel Castro?s death.vital Raul Castro was formally designated his brother?s successorvital Raul is the head of the Armed Forcesokay Raul is five years younger than Castrookay Raul has enjoyed a more public role in running Cuba?s Government.okay Raul is the number two man in the government?s ruling Council of StateFigure 1: An example relationship question from TREC 2005 with its answer nuggets.teresting ?nuggets?
about a particular person, or-ganization, entity, or event.
No single documentcan provide a complete answer, and hence systemsmust integrate information from multiple sources;cf.
(Amigo?
et al, 2004; Dang, 2005).This work focuses on so-called relationshipquestions, which represent a new and underex-plored area in question answering.
Although theyrequire systems to extract information nuggetsfrommultiple documents (just like definition/otherquestions), relationship questions demand a differ-ent approach (see Section 2).
This paper exploresthe role of information retrieval in answering suchquestions, focusing primarily on three aspects:document retrieval performance, term-based mea-sures of relevance, and term-based approaches toreducing redundancy.
The overall goal is to pushthe limits of information retrieval technology andprovide strong baselines against which linguisticprocessing capabilities can be compared.The rest of this paper is organized as follows:Section 2 provides an overview of relationshipquestions.
Section 3 describes experiments fo-cused on document retrieval performance.
An ap-proach to answering relationship questions basedon sentence retrieval is discussed in Section 4.
Asimple utility model that incorporates both rele-vance and redundancy is explored in Section 5.Before concluding, we discuss the implications ofour experimental results in Section 6.2 Relationship QuestionsRelationship questions represent a new class of in-formation needs formally introduced as a subtaskin the NIST-sponsored TREC QA evaluations in2005 (Voorhees, 2005).
Previously, they were thefocus of a small pilot study within the AQUAINTprogram, which resulted in an understanding of a?relationship?
as the ability for one object to in-fluence another.
Objects in these questions candenote either entities (people, organization, coun-tries, etc.)
or events.
Consider the following ex-amples:?
Has pressure from China affected America?swillingness to sell weaponry to Taiwan??
Do the military personnel exchanges betweenIsrael and India show an increase in cooper-ation?
If so, what are the driving factors be-hind this increase?Evidence for a relationship includes both themeans to influence some entity and the motiva-tion for doing so.
Eight types of relationships(?spheres of influence?)
were noted: financial,movement of goods, family ties, co-location, com-mon interest, and temporal connection.Relationship questions are significantly dif-ferent from definition questions, which can beparaphrased as ?Tell me interesting things aboutx.?
Definition questions have received significantamounts of attention recently, e.g., (Hildebrandt etal., 2004; Prager et al, 2004; Xu et al, 2004; Cuiet al, 2005).
Research has shown that certain cuephrases serve as strong indicators for nuggets, andthus an approach based on matching surface pat-terns (e.g., appositives, parenthetical expressions)works quite well.
Unfortunately, such techniquesdo not generalize to relationship questions becausetheir answers are not usually captured by patternsor marked by surface cues.Unlike answers to factoid questions, answers torelationship questions consist of an unsorted setof passages.
For assessing system output, NISTemploys the nugget-based evaluation methodol-ogy originally developed for definition questions;see (Voorhees, 2005) for a detailed description.Answers consist of units of information called?nuggets?, which assessors manually create fromsystem submissions and their own research (seeexample in Figure 1).
Nuggets are divided into524two types (?vital?
and ?okay?
), and this distinc-tion plays an important role in scoring.
The offi-cial metric is an F3-score, where nugget recall iscomputed on vital nuggets, and precision is basedon a length allowance derived from the number ofboth vital and okay nuggets retrieved.In the original NIST setup, human assessorswere required to manually determine whether aparticular system?s response contained a nugget.This posed a problem for researchers who wishedto conduct formative evaluations outside the an-nual TREC cycle?the necessity of human in-volvement meant that system responses couldnot be rapidly, consistently, and automaticallyassessed.
However, the recent introduction ofPOURPRE, an automatic evaluation metric for thenugget-based evaluation methodology (Lin andDemner-Fushman, 2005), fills this evaluation gapand makes possible the work reported here; cf.Nuggeteer (Marton and Radul, 2006).This paper describes experiments with the 25relationship questions used in the secondary taskof the TREC 2005 QA track (Voorhees, 2005),which attracted a total of eleven submissions.
Sys-tems used the AQUAINT corpus, a three gigabytecollection of approximately one million news ar-ticles from the Associated Press, the New YorkTimes, and the Xinhua News Agency.3 Document RetrievalSince information retrieval systems supply the ini-tial set of documents on which a question answer-ing system operates, it makes sense to optimizedocument retrieval performance in isolation.
Theissue of end?to?end system performance will betaken up in Section 4.Retrieval performance can be evaluated basedon the assumption that documents which containone or more relevant nuggets (either vital or okay)are themselves relevant.
From system submissionsto TREC 2005, we created a set of relevance judg-ments, which averaged 8.96 relevant documentsper question (median 7, min 1, max 21).Our first goal was to examine the effectof different retrieval systems on performance.Two freely-available IR engines were compared:Lucene and Indri.
The former is an open-sourceimplementation of what amounts to be a modifiedtf.idf weighting scheme, while the latter employsa language modeling approach.
In addition, weexperimented with blind relevance feedback, a re-MAP R50Lucene 0.206 0.469Lucene+brf 0.190 (?7.6%)?
0.442 (?5.6%)?Indri 0.195 (?5.2%)?
0.442 (?5.6%)?Indri+brf 0.158 (?23.3%)O 0.377 (?19.5%)OTable 1: Document retrieval performance, withand without blind relevance feedback.trieval technique commonly employed to improveperformance (Salton and Buckley, 1990).
Fol-lowing settings in typical IR experiments, the toptwenty terms (by tf.idf value) from the top twentydocuments were added to the original query in thefeedback iteration.For each question, fifty documents from theAQUAINT collection were retrieved, represent-ing the number of documents that a typical QAsystem might consider.
The question itself wasused verbatim as the IR query (see Section 6 fordiscussion).
Performance is shown in Table 1.We measured Mean Average Precision (MAP), themost informative single-point metric for rankedretrieval, and recall, since it places an upper boundon the number of relevant documents available forsubsequent downstream processing.For all experiments reported in this paper, weapplied the Wilcoxon signed-rank test to deter-mine the statistical significance of the results.
Thistest is commonly used in information retrievalresearch because it makes minimal assumptionsabout the underlying distribution of differences.Significance at the 0.90 level is denoted with a ?or ?, depending on the direction of change; at the0.95 level, M or O; at the 0.99 level, N or H. Differ-ences not statistically significant are marked with?.
Although the differences between Lucene andIndri are not significant, blind relevance feedbackwas found to hurt performance, significantly so inthe case of Indri.
These results are consistent withthe findings of Monz (2003), who made the sameobservation in the factoid QA task.There are a few caveats to consider when in-terpreting these results.
First, the test set of 25questions is rather small.
Second, the number ofrelevant documents per question is also relativelysmall, and hence likely to be incomplete.
Buck-ley and Voorhees (2004) have shown that evalua-tion metrics are not stable with respect to incom-plete relevance judgments.
Third, the distributionof relevant documents may be biased due to thesmall number of submissions, many of which used525Lucene.
Due to these factors, one should interpretthe results reported here as suggestive, not defini-tive.
Follow-up experiments with larger data setsare required to produce more conclusive results.4 Selecting Relevant SentencesWe adopted an extractive approach to answeringrelationship questions that views the task as sen-tence retrieval, a conception in line with the think-ing of many researchers today (but see discussionin Section 6).
Although oversimplified, there areseveral reasons why this formulation is produc-tive: since answers consist of unordered text seg-ments, the task is similar to passage retrieval, awell-studied problem (Callan, 1994; Tellex et al,2003) where sentences form a natural unit of re-trieval.
In addition, the TREC novelty tracks havespecifically tackled the questions of relevance andredundancy at the sentence level (Harman, 2002).Empirically, a sentence retrieval approach per-forms quite well: when definition questionswere first introduced in TREC 2003, a simplesentence-ranking algorithm outperformed all butthe highest-scoring system (Voorhees, 2003).
Inaddition, viewing the task of answering relation-ship questions as sentence retrieval allows oneto leverage work in multi-document summariza-tion, where extractive approaches have been ex-tensively studied.
This section examines the taskof independently selecting the best sentences forinclusion in an answer; attempts to reduce redun-dancy will be discussed in the next section.There are a number of term-based features as-sociated with a candidate sentence that may con-tribute to its relevance.
In general, such featurescan be divided into two types: properties of thedocument containing the sentence and propertiesof the sentence itself.
Regarding the former type,two features come into play: the relevance scoreof the document (from the IR engine) and its rankin the result set.
For sentence-based features, weexperimented with the following:?
Passage match score, which sums the idf val-ues of unique terms that appear in both thecandidate sentence (S) and the question (Q):?t?S?Qidf(t)?
Term idf precision and recall scores; cf.
(Katzet al, 2005):P =?t?S?Q idf(t)?t?A idf(t),R =?t?S?Q idf(t)?t?Q idf(t)?
Length of the sentence (in non-whitespacecharacters).Note that precision and recall values arebounded between zero and one, while the passagematch score and the length of the sentence are bothunbounded features.Our baseline sentence retriever employed thepassage match score to rank all sentences in thetop n retrieved documents.
By default, we useddocuments retrieved by Lucene, using the ques-tion verbatim as the query.
To generate answers,the system selected sentences based on their scoresuntil a hard length quota has been filled (trim-ming the final sentence if necessary).
After ex-perimenting with different values, we discoveredthat a document cutoff of ten yielded the highestperformance in terms of POURPRE scores, i.e., allbut the ten top-ranking documents were discarded.In addition, we built a linear regression modelthat employed the above features to predict thenugget score of a sentence (the dependent vari-able).
For the training samples, the nugget match-ing component within POURPRE was employedto compute the nugget score?this value quanti-fied the ?goodness?
of a particular sentence interms of nugget content.1 Due to known issueswith the vital/okay distinction (Hildebrandt et al,2004), it was ignored for this computation; how-ever, see (Lin and Demner-Fushman, 2006b) forrecent attempts to address this issue.When presented with a test question, the sys-tem ranked all sentences from the top ten retrieveddocuments using the regression model.
Answerswere generated by filling a quota of characters,just as in the baseline.
Once again, no attempt wasmade to reduce redundancy.We conducted a five-fold cross validation ex-periment using all sentences from the top 100Lucene documents as training samples.
After ex-perimenting with different features, we discov-ered that a regression model with the followingperformed best: passage match score, documentscore, and sentence length.
Surprisingly, adding1Since the count variant of POURPRE achieved the highestcorrelation with official rankings, the nugget score is simplythe highest fraction in terms of word overlap between the sen-tence and any of the reference nuggets.526Length 1000 2000 3000 4000 5000F-Scorebaseline 0.275 0.268 0.255 0.234 0.225regression 0.294 (+7.0%)?
0.268 (+0.0%)?
0.257 (+1.0%)?
0.240 (+2.5%)?
0.228 (+1.6%)?Recallbaseline 0.282 0.308 0.333 0.336 0.352regression 0.302 (+7.2%)?
0.308 (+0.0%)?
0.336 (+0.8%)?
0.343 (+2.3%)?
0.358 (+1.7%)?F-Score (all-vital)baseline 0.699 0.672 0.632 0.592 0.558regression 0.722 (+3.3%)?
0.672 (+0.0%)?
0.632 (+0.0%)?
0.593 (+0.2%)?
0.554 (?0.7%)?Recall (all-vital)baseline 0.723 0.774 0.816 0.834 0.856regression 0.747 (+3.3%)?
0.774 (+0.0%)?
0.814 (?0.2%)?
0.834 (+0.0%)?
0.848 (?0.8%)?Table 2: Question answering performance at different answer length cutoffs, as measured by POURPRE.Length 1000 2000 3000 4000 5000F-ScoreLucene 0.275 0.268 0.255 0.234 0.225Lucene+brf 0.278 (+1.3%)?
0.268 (+0.0%)?
0.251 (?1.6%)?
0.231 (?1.2%)?
0.215 (?4.3%)?Indri 0.264 (?4.1%)?
0.260 (?2.7%)?
0.241 (?5.4%)?
0.222 (?5.0%)?
0.212 (?5.8%)?Indri+brf 0.270 (?1.8%)?
0.257 (?3.8%)?
0.235 (?7.8%)?
0.221 (?5.7%)?
0.206 (?8.2%)?RecallLucene 0.282 0.308 0.333 0.336 0.352Lucene+brf 0.285 (+1.3%)?
0.308 (+0.0%)?
0.319 (?4.2%)?
0.322 (?4.2%)?
0.324 (?7.9%)?Indri 0.270 (?4.1%)?
0.300 (?2.5%)?
0.306 (?8.2%)?
0.308 (?8.1%)?
0.320 (?9.2%)?Indri+brf 0.276 (?2.0%)?
0.296 (?3.6%)?
0.299 (?10.4%)?
0.307 (?8.5%)?
0.312 (?11.3%)?Table 3: The effect of using different document retrieval systems on answer quality.the term match precision and recall features to theregression model decreased overall performanceslightly.
We believe that precision and recall en-codes information already captured by the otherfeatures.Results of our experiments are shown in Ta-ble 2 for different answer lengths.
Followingthe TREC QA track convention, all lengths aremeasured in non-whitespace characters.
Both thebaseline and regression conditions employed thetop ten documents supplied by Lucene.
In addi-tion to the F3-score, we report the recall compo-nent only (on vital nuggets).
For this and all sub-sequent experiments, we used the (count, macro)variant of POURPRE, which was validated as pro-ducing the highest correlation with official rank-ings.
The regression model yields higher scoresat shorter lengths, although none of these differ-ences were significant.
In general, performancedecreases with longer answers because both vari-ants tend to rank relevant sentences before non-relevant ones.Our results compare favorably to runs submit-ted to the TREC 2005 relationship task.
In thatevaluation, the best performing automatic run ob-tained a POURPRE score of 0.243, with an averageanswer length of 4051 character per question.Since the vital/okay nugget distinction was ig-nored when training our regression model, we alsoevaluated system output under the assumption thatall nuggets were vital.
These scores are also shownin Table 2.
Once again, results show higher POUR-PRE scores for shorter answers, but these differ-ences are not statistically significant.
Why mightthis be so?
It appears that features based on termstatistics alone are insufficient to capture nuggetrelevance.
We verified this hypothesis by buildinga regression model for all 25 questions: the modelexhibited an R2 value of only 0.207.How does IR performance affect the final sys-tem output?
To find out, we applied the base-line sentence retrieval algorithm (which uses thepassage match score only) on the output of differ-ent document retrieval variants.
These results areshown in Table 3 for the four conditions discussedin the previous section: Lucene and Indri, with andwithout blind relevance feedback.Just as with the document retrieval results,Lucene alone (without blind relevance feedback)yielded the highest POURPRE scores.
However,none of the differences observed were statisticallysignificant.
These numbers point to an interestinginteraction between document retrieval and ques-tion answering.
The decreases in performance at-527Length 1000 2000 3000 4000 5000F-Scorebaseline 0.275 0.268 0.255 0.234 0.225baseline+max 0.311 (+13.2%)?
0.302 (+12.8%)N 0.281 (+10.5%)N 0.256 (+9.5%)M 0.235 (+4.6%)?baseline+avg 0.301 (+9.6%)?
0.294 (+9.8%)?
0.271 (+6.5%)?
0.256 (+9.5%)M 0.237 (+5.6%)?regression+max 0.275 (+0.3%)?
0.303 (+13.3%)?
0.275 (+8.1%)?
0.258 (+10.4%)?
0.244 (+8.4%)?Recallbaseline 0.282 0.308 0.333 0.336 0.352baseline+max 0.324 (+15.1%)?
0.355 (+15.4%)M 0.369 (+10.6%)M 0.369 (+9.8%)M 0.369 (+4.7%)?baseline+avg 0.314 (+11.4%)?
0.346 (+12.3%)?
0.354 (+6.2%)?
0.369 (+9.8%)M 0.371 (+5.5%)?regression+max 0.287 (+2.0%)?
0.357 (+16.1%)?
0.360 (+8.0%)?
0.371 (+10.4%)?
0.379 (+7.6%)?Table 4: Evaluation of different utility settings.tributed to blind relevance feedback in end?to?endQA were in general less than the drops observedin the document retrieval runs.
It appears possi-ble that the sentence retrieval algorithm was ableto recover from a lower-quality result set, i.e., onewith relevant documents ranked lower.
Neverthe-less, just as with factoid QA, the coupling betweenIR and answer extraction merits further study.5 Reducing RedundancyThe methods described in the previous sectionfor choosing relevant sentences do not take intoaccount information that may be conveyed morethan once.
Drawing inspiration from research insentence-level redundancy within the context ofthe TREC novelty track (Allan et al, 2003) andwork in multi-document summarization, we ex-perimented with term-based approaches to reduc-ing redundancy.Instead of selecting sentences for inclusion inthe answer based on relevance alone, we imple-mented a simple utility model, which takes intoaccount sentences that have already been added tothe answer A.
For each candidate c, utility is de-fined as follows:Utility(c) = Relevance(c)?
?maxs?Asim(s, c)This model is the baseline variant of the Maxi-mal Marginal Relevance method for summariza-tion (Goldstein et al, 2000).
Each candidate iscompared to all sentences that have already beenselected for inclusion in the answer.
The maxi-mum of these pairwise similarity comparisons isdeducted from the relevance score of the sentence,subjected to ?, a parameter that we tune.
For ourexperiments, we used cosine distance as the simi-larity function.
All relevance scores were normal-ized to a range between zero and one.At each step in the answer generation process,utility values are computed for all candidate sen-tences.
The one with the highest score is selectedfor inclusion in the final answer.
Utility values arethen recomputed, and the process iterates until thelength quota has been filled.We experimented with two different sourcesfor the relevance scores: the baseline sentence re-triever (passage match score only) and the regres-sion model.
In addition to taking the max of allpairwise similarity values, as in the above formula,we also experimented with the average.Results of our runs are shown in Table 4.
Wereport values for the baseline relevance score withthe max and avg aggregation functions, as well asthe regression relevance scores with max.
Theseexperimental conditions were compared againstthe baseline run that used the relevance score only(no redundancy penalty).
To compute the optimal?, we swept across the parameter space from zeroto one in increments of a tenth.
We determined theoptimal value of ?
by averaging POURPRE scoresacross all length intervals.
For all three conditions,we discovered 0.4 to be the optimal value.These experiments suggest that a simple term-based approach to reducing redundancy yields sta-tistically significant gains in performance.
Thisresult is not surprising since similar techniqueshave proven effective in multi-document summa-rization.
Empirically, we found that the max op-erator outperforms the avg operator in quantify-ing the degree of redundancy.
The observationthat performance improvements are more notice-able at shorter answer lengths confirms our intu-itions.
Redundancy is better tolerated in longeranswers because a redundant nugget is less likelyto ?squeeze out?
a relevant, novel nugget.While it is productive to model the relationshiptask as sentence retrieval where independent de-cisions are made about sentence-level relevance,528this simplification fails to capture overlap in infor-mation content, and leads to redundant answers.We found that a simple term-based approach waseffective in tackling this issue.6 DiscussionAlthough this work represents the first formalstudy of relationship questions that we are awareof, by no means are we claiming a solution?wesee this as merely the first step in addressing acomplex problem.
Nevertheless, information re-trieval techniques lay the groundwork for systemsaimed at answering complex questions.
The meth-ods described here will hopefully serve as a start-ing point for future work.Relationship questions represent an importantproblem because they exemplify complex infor-mation needs, generally acknowledged as the fu-ture of QA research.
Other types of complex needsinclude analytical questions such as ?How close isIran to acquiring nuclear weapons?
?, which are thefocus of the AQUAINT program in the U.S., andopinion questions such as ?How does the Chileangovernment view attempts at having Pinochet triedin Spanish Court?
?, which were explored in a 2005pilot study also funded by AQUAINT.
In 2006,there will be a dedicated task within the TRECQA track exploring complex questions within aninteractive setting.
Furthermore, we note the con-vergence of the QA and summarization commu-nities, as demonstrated by the shift from genericto query-focused summaries starting with DUC2005 (Dang, 2005).
This development is alsocompatible with the conception of ?distillation?in the current DARPA GALE program.
All thesetrends point to same problem: how do we buildadvanced information systems to address complexinformation needs?The value of this work lies in the generalityof IR-based approaches.
Sophisticated linguis-tic processing algorithms are typically unable tocope with the enormous quantities of text avail-able.
To render analysis more computationallytractable, researchers commonly employ IR tech-niques to reduce the amount of text under consid-eration.
We believe that the techniques introducedin this paper are applicable to the different typesof information needs discussed above.While information retrieval techniques form astrong baseline for answering relationship ques-tions, there are clear limitations of term-based ap-proaches.
Although we certainly did not exper-iment with every possible method, this work ex-amined several common IR techniques (e.g., rel-evance feedback, different term-based features,etc.).
In our regression experiments, we discov-ered that our feature set was unable to adequatelycapture sentence relevance.
On the other hand,simple IR-based techniques appeared to work wellat reducing redundancy, suggesting that determin-ing content overlap is a simpler problem.To answer relationship questions well, NLPtechnology must take over where IR techniquesleave off.
Yet, there are a number of challenges,the biggest of which is that question classificationand named-entity recognition, which have workedwell for factoid questions, are not applicable to re-lationship questions, since answer types are diffi-cult to anticipate.
For factoids, there exists a sig-nificant amount of work on question analysis?theresults of which include important query terms andthe expected answer type (e.g., person, organiza-tion, etc.).
Relationship questions are more diffi-cult to process: for one, they are often not phrasedas direct wh-questions, but rather as indirect re-quests for information, statements of doubt, etc.Furthermore, since these complex questions can-not be answered by short noun phrases, existinganswer type ontologies are not very useful.
For ourexperiments, we decided to simply use the ques-tion verbatim as the query to the IR systems, butundoubtedly performance can be gained by bet-ter query formulation strategies.
These are diffi-cult challenges, but recent work on applying se-mantic models to QA (Narayanan and Harabagiu,2004; Lin and Demner-Fushman, 2006a) providea promising direction.While our formulation of answering relation-ship questions as sentence retrieval is produc-tive, it clearly has limitations.
The assumptionthat information nuggets do not span sentenceboundaries is false and neglects important work inanaphora resolution and discourse modeling.
Thecurrent setup of the task, where answers consistof unordered strings, does not place any value oncoherence and readability of the responses, whichwill be important if the answers are intended forhuman consumption.
Clearly, there are ample op-portunities here for NLP techniques to shine.The other value of this work lies in its use of anautomatic evaluation metric (POURPRE) for sys-tem development?the first instance in complex529QA that we are aware of.
Prior to the introduc-tion of this automatic scoring technique, studiessuch as this were difficult to conduct due to thenecessity of involving humans in the evaluationprocess.
POURPRE was developed to enable rapidexploration of the solution space, and experimentsreported here demonstrate its usefulness in doingjust that.
Although automatic evaluation metricsare no stranger to other fields such as machinetranslation (e.g., BLEU) and document summa-rization (e.g., ROUGE, BE, etc.
), this represents anew development in question answering research.7 ConclusionAlthough many findings in this paper are negative,the conclusions are positive for NLP researchers.An exploration of a variety of term-based ap-proaches for answering relationship questions hasdemonstrated the impact of different techniques,but more importantly, this work highlights limita-tions of purely IR-based methods.
With a strongbaseline as a foundation, the door is wide open forthe integration of natural language understandingtechniques.8 AcknowledgmentsThis work has been supported in part by DARPAcontract HR0011-06-2-0001 (GALE).
I would liketo thank Esther and Kiri for their loving support.ReferencesJ.
Allan, C. Wade, and A. Bolivar.
2003.
Retrievaland novelty detection at the sentence level.
In SIGIR2003.E.
Amigo?, J. Gonzalo, V. Peinado, A.
Pen?as, andF.
Verdejo.
2004.
An empirical study of informa-tion synthesis task.
In ACL 2004.C.
Buckley and E. Voorhees.
2004.
Retrieval evalua-tion with incomplete information.
In SIGIR 2004.J.
Callan.
1994.
Passage-level evidence in documentretrieval.
In SIGIR 1994.H.
Cui, M.-Y.
Kan, and T.-S. Chua.
2005.
Generic softpattern models for definitional question answering.In SIGIR 2005.H.
Dang.
2005.
Overview of DUC 2005.
In DUC2005.R.
Gaizauskas, M. Hepple, and M. Greenwood.
2004.Proceedings of the SIGIR 2004 Workshop on Infor-mation Retrieval for Question Answering (IR4QA).J.
Goldstein, V. Mittal, J. Carbonell, and J. Callan.2000.
Creating and evaluating multi-document sen-tence extract summaries.
In CIKM 2000.D.
Harman.
2002.
Overview of the TREC 2002 nov-elty track.
In TREC 2002.W.
Hildebrandt, B. Katz, and J. Lin.
2004.
Answer-ing definition questions with multiple knowledgesources.
In HLT/NAACL 2004.L.
Hirschman and R. Gaizauskas.
2001.
Naturallanguage question answering: The view from here.Natural Language Engineering, 7(4):275?300.B.
Katz, G. Marton, G. Borchardt, A. Brownell,S.
Felshin, D. Loreto, J. Louis-Rosenberg, B. Lu,F.
Mora, S. Stiller, O. Uzuner, and A. Wilcox.
2005.External knowledge sources for question answering.In TREC 2005.J.
Lin and D. Demner-Fushman.
2005.
Automati-cally evaluating answers to definition questions.
InHLT/EMNLP 2005.J.
Lin and D. Demner-Fushman.
2006a.
The role ofknowledge in conceptual retrieval: A study in thedomain of clinical medicine.
In SIGIR 2006.J.
Lin and D. Demner-Fushman.
2006b.
Will pyramidsbuilt of nuggets topple over?
In HLT/NAACL 2006.G.
Marton and A. Radul.
2006.
Nuggeteer: Au-tomatic nugget-based evaluation using descriptionsand judgements.
In HLT/NAACL 2006.C.
Monz.
2003.
From Document Retrieval to QuestionAnswering.
Ph.D. thesis, Institute for Logic, Lan-guage, and Computation, University of Amsterdam.S.
Narayanan and S. Harabagiu.
2004.
Question an-swering based on semantic structures.
In COLING2004.J.
Prager, J. Chu-Carroll, and K. Czuba.
2004.
Ques-tion answering using constraint satisfaction: QA?by?Dossier?with?Constraints.
In ACL 2004.G.
Salton and C. Buckley.
1990.
Improving re-trieval performance by relevance feedback.
Jour-nal of the American Society for Information Science,41(4):288?297.S.
Tellex, B. Katz, J. Lin, G. Marton, and A. Fernandes.2003.
Quantitative evaluation of passage retrievalalgorithms for question answering.
In SIGIR 2003.E.
Voorhees.
2003.
Overview of the TREC 2003 ques-tion answering track.
In TREC 2003.E.
Voorhees.
2005.
Overview of the TREC 2005 ques-tion answering track.
In TREC 2005.J.
Xu, R. Weischedel, and A. Licuanan.
2004.
Evalu-ation of an extraction-based approach to answeringdefinition questions.
In SIGIR 2004.530
