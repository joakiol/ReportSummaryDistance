Proceedings of the 6th EACL Workshop on Language Technology for Cultural Heritage, Social Sciences, and Humanities, pages 85?93,Avignon, France, 24 April 2012. c?2012 Association for Computational LinguisticsComputing Similarity between Cultural HeritageItems using Multimodal FeaturesNikolaos AletrasDepartment of Computer ScienceUniversity of SheffieldRegent Court, 211 PortobelloSheffield, S1 4DP, UKn.aletras@dcs.shef.ac.ukMark StevensonDepartment of Computer ScienceUniversity of SheffieldRegent Court, 211 PortobelloSheffield, S1 4DP, UKm.stevenson@dcs.shef.ac.ukAbstractA significant amount of information aboutCultural Heritage artefacts is now availablein digital format and has been made avail-able in digital libraries.
Being able to iden-tify items that are similar would be use-ful for search and navigation through thesedata sets.
Information about items in theserepositories is often multimodal, such aspictures of the artefact and an accompa-nying textual description.
This paper ex-plores the use of information from thesevarious media for computing similarity be-tween Cultural Heritage artefacts.
Resultsshow that combining information from im-ages and text produces better estimates ofsimilarity than when only a single mediumis considered.1 Introduction and MotivationIn recent years a vast amount of Cultural Heritage(CH) artefacts have been digitised and made avail-able on-line.
For example, the Louvre and theBritish Museum provide information about ex-hibits on their web pages1.
In addition, informa-tion is also available via sites that aggregate CHinformation from multiple resources.
A typicalexample is Europeana2, a web-portal to collec-tions from several European institutions that pro-vides access to over 20 million items includingpaintings, films, books, archives and museum ex-hibits.However, online information about CH arte-facts is often unstructured and varies by collec-1http://www.louvre.fr/,http://www.britishmuseum.org/2http://www.europeana.eution.
This makes it difficult to identify informa-tion of interest in sites that aggregate informa-tion from multiple sources, such as Europeana,or to compare information across multiple collec-tions (such as the Louvre and British Museum).These problems form a significant barrier to ac-cessing the information available in these onlinecollections.
A first step towards improving accesswould be to identify similar items in collections.This could assist with several applications that areof interest to those working in CH including rec-ommendation of interesting items (Pechenizkzyand Calders, 2007; Wang et al, 2008), generationof virtual tours (Joachims et al, 1997; Wang et al,2009), visualisation of collections (Kauppinen etal., 2009; Hornbaek and Hertzum, 2011) and ex-ploratory search (Marchionini, 2006; Amin et al,2008).Information in digital CH collections often in-cludes multiple types of media such as text, im-ages and audio.
It seems likely that informa-tion from all of these types would help humansto identify similar items and that it could help toidentify them automatically.
However, previouswork on computing similarity in the CH domainhas been limited and, in particular, has not madeuse of information from multiple types of media.For example, Grieser et al (2011) computed sim-ilarity between exhibits in Melbourne Museumby applying a range of text similarity measuresbut did not make use of other media.
Tech-niques for exploiting information from multi-media collections have been developed and arecommonly applied to a wide range of problemssuch as Content-based Image Retrieval (Datta etal., 2008) and image annotation (Feng and Lap-ata, 2010).85This paper makes use of information from twomedia (text and images) to compute the similar-ity between items in a large collection of CHitems (Europeana).
A range of similarity mea-sures for text and images are compared and com-bined.
Evaluation is carried out using a set ofitems from Europeana with similarity judgementsthat were obtained in a crowdsourcing experi-ment.
We find that combining information fromboth media produces better results than when ei-ther is used alone.The main contribution of this paper is todemonstrate the usefulness of applying informa-tion from more than one medium when compar-ing CH items.
In addition, it explores the effec-tiveness of different similarity measures when ap-plied to this domain and introduces a data set ofsimilarity judgements that can be used as a bench-mark.The remainder of this paper is structured as fol-lows.
Section 2 describes some relevant previouswork.
Sections 3, 4 and 5 describe the text and im-age similarity measures applied in this paper andhow they are combined.
Sections 6 describes theexperiments used in this paper and the results arereported in Section 7.
Finally, Section 8 drawsthe conclusions and provides suggestions for fu-ture work.2 Background2.1 Text SimilarityTwo main approaches for determining the similar-ity between two texts have been explored: corpus-based and knowledge-based methods.
Corpus-based methods rely on statistics that they learnfrom corpora while knowledge-based methodsmake use of some external knowledge source,such as a thesaurus, dictionary or semantic net-work (Agirre et al, 2009; Gabrilovich andMarkovitch, 2007).A previous study (Aletras et al, 2012) com-pared the effectiveness of various methods forcomputing the similarity between items in a CHcollection based on text extracted from theirdescriptions, including both corpus-based andknowledge-based approaches.
The corpus-basedapproaches varied from simple word counting ap-proaches (Manning and Schutze, 1999) to morecomplex ones based on techniques from Infor-mation Retrieval (Baeza-Yates and Ribeiro-Neto,1999) and topic models (Blei et al, 2003).
Theknowledge-based approaches relied on Wikipedia(Milne, 2007).
Aletras et al (2012) concludedthat corpus-based measures were more effectivethan knowledge-based ones for computing simi-larity between these items.2.2 Image SimilarityDetermining the similarity between images hasbeen explored in the fields such as Computer Vi-sion (Szeliski, 2010) and Content-based ImageRetrieval (CBIR) (Datta et al, 2008).
A first stepin computing the similarity between images is totransform them into an appropriate set of features.Some major feature types which have been usedare colour, shape, texture or salient points.
Fea-tures are also commonly categorised into globaland local features.Global features characterise an entire image.For example, the average of the intensities of red,green and blue colours gives an estimation of theoverall colour distribution in the image.
The mainadvantages of global features are that they can becomputed efficiently.
However, they are unableto represent information about elements in an im-age (Datta et al, 2008).
On the other hand, lo-cal features aim to identify interesting areas inthe image, such as where significant differencesin colour intensity between adjacent pixels is de-tected.Colour is one of the most commonly usedglobal features and has been applied in sev-eral fields including image retrieval (Jacobs etal., 1995; Sebe and Michael S. Lew, 2001;Yu et al, 2002), image clustering (Cai et al,2004; Strong and Gong, 2009), database index-ing (Swain and Ballard, 1991) and, object/scenerecognition (Schiele and Crowley, 1996; Ndjiki-Nya et al, 2004; Sande et al, 2008).
A commonmethod for measuring similarity between imagesis to compare the colour distributions of their his-tograms.
A histogram is a graphical representa-tion of collected counts for predefined categoriesof data.
To create a histogram we have to specifythe range of the data values, the number of dimen-sions and the bins (intervals into which rangesof values are combined).
A colour histogramrecords the number of the pixels that fall in theinterval of each bin.
Schiele and Crowley (1996)describe several common metrics for comparingcolour histograms including ?2, correlation and86intersection.2.3 Combining Text and Image FeaturesThe integration of information from text and im-age features has been explored in several fields.In Content-based Image Retrieval image featuresare combined together with words from captionsto retrieve images relevant to a query (La Cas-cia et al, 1998; Srihari et al, 2000; Barnardand Forsyth, 2001; Westerveld, 2002; Zhou andHuang, 2002; Wang et al, 2004).
Image cluster-ing methods have been developed to combine in-formation from images and text to create clustersof similar images (Loeff et al, 2006; Bekkermanand Jeon, 2007).
Techniques for automatic imageannotation that generate models as a mixture ofword and image features have also been described(Jeon et al, 2003; Blei and Jordan, 2003; Fengand Lapata, 2010).2.4 Similarity in Cultural HeritageDespite the potential usefulness of similarity inCH, there has been little previous work on thearea.
An exception is the work of Grieser et al(2011).
They computed the similarity between aset of 40 exhibits from Melbourne Museum byanalysing the museum?s web pages and physi-cal layout.
They applied a range of text similar-ity techniques (see Section 2.1) to the web pagesas well as similarity measures that made use ofWikipedia.
However, the Wikipedia-based tech-niques relied on a manual mapping between theitems and an appropriate Wikipedia article.
Al-though the web pages often contained images ofthe exhibits, Grieser et al (2011) did not make useof them.3 Text SimilarityWe make use of various corpus-based approachesfor computing similarity between CH items sinceprevious experiments (see Section 2.1) haveshown that these outperformed knowledge-basedmethods in a comparison of text-based similaritymethods for the CH domain.We assume that we wish to compute the simi-larity between a pair of items, A and B, and thateach item has both text and an image associatedwith it.
The text is denoted as At and Bt whilethe images are denoted by Ai and Bi.3.1 Word OverlapA common approach to computing similarity is tocount the number of common words (Lesk, 1986).The text associated with each item is comparedand the similarity is computed as the number ofwords (tokens) they have in common normalisedby the combined total:simWO(A,B) =|At ?Bt||At ?Bt|3.2 N-gram OverlapThe Word Overlap approach is a bag of wordsmethod that does not take account of the orderin which words appear, despite the fact that thisis potentially useful information for determiningsimilarity.
One way in which this information canbe used is to compare n-grams derived from a text.Patwardhan et al (2003) used this approach to ex-tend the Word Overlap measure.
This approachidentifies n-grams in common between the twotext and increases the score by n2 for each onethat is found, where n is the length of the n-gram.More formally,simngram(A,B) =?n ?
n?gram(At,Bt)n2|At ?Bt|where n?gram(At, Bt) is the set of n-grams thatoccur in both At and Bt.3.3 TF.IDFThe word and n-gram overlap measures assignthe same importance to each word but some aremore important for determining similarity be-tween texts than others.
A widely used approachto computing similarity between documents is torepresent them as vectors in which each term isassigned a weighting based on its estimated im-portance (Manning and Schutze, 1999).
The vec-tors can then be compared using the cosine met-ric.
A widely used scheme for weighting termsis tf.idf, which takes account of the frequency ofeach term in individual documents and the num-ber of documents in a corpus in which it occurs.3.4 Latent Dirichlet AllocationTopic models (Blei et al, 2003) are a useful tech-nique for representing the underlying content ofdocuments.
LDA is a widely used topic model87that assumes each document is composed of anumber of topics.
For each document LDA re-turns a probability distribution over a set of topicsthat have been derived from an unlabeled corpus.Similarity between documents can be computedby converting these distributions into vectors andusing the cosine metric.4 Image SimilarityTwo approaches are compared for computing thesimilarity between images.
These are largelybased on colour features and are more suitable forthe images in the data set we use for evaluation(see Section 6).4.1 Colour Similarity (RGB)The first approach is based on comparison ofcolour histograms derived from images.In the RGB (Red Green Blue) colour model,each pixel is represented as an integer in range of0-255 in three dimensions (Red, Green and Blue).One histogram is created for each dimension.
Forgrey-scale images it is assumed that the value ofeach dimension is the same in each pixel and asingle histogram, called the luminosity histogram,is created.
Similarity between the histograms ineach colour channel is computed using the inter-section metric.
The intersection metric (Swainand Ballard, 1991) measures the number of cor-responding pixels that have same colour in twoimages.
It is defined as follows:Inter(h1, h2) =?Imin(h1(I), h2(I))where hi is the histogram of image i, I is the setof histogram bins and min(a, b) is the minimumbetween corresponding pixel colour values.The final similarity score is computed as the av-erage of the red, green and blue histogram simi-larity scores:simRGB(Ai, Bi) =?i?
{R,G,B}Inter(hAi , hBi)34.2 Image Querying Metric (imgSeek)Jacobs et al (1995) described an image similar-ity metric developed for Content-based Image Re-trieval.
It makes use of Haar wavelet decompo-sition (Beylkin et al, 1991) to create signaturesof images that contain colour and basic shape in-formation.
Images are compared by determiningthe number of significant coefficients they have incommon using the following function:distimgSeek(Ai, Bi) = w0|CAi(0, 0)?
CBi(0, 0)|+?i,j:C?Ai (i,j) 6=0wbin(i,j)(C?Ai(i, j) 6= C?Bi(i, j))where wb are weights, CI represents a singlecolour channel for an image I , CI(0, 0) are scal-ing function coefficients of the overall average in-tensity of the colour channel and C?I(i, j) is the(i, j)-th truncated, quantised wavelet coefficientof image I .
For more details please refer to Ja-cobs et al (1995).Note that this function measures the distancebetween two images with low scores indicatingsimilar images and high scores dis-similar ones.We assign the negative sign to this metric to assignhigh scores to similar images.
It is converted intoa similarity metric as follows:simimgSeek(Ai, Bi) = ?distimgSeek(Ai, Bi)5 Combining Text and Image SimilarityA simple weighted linear combination is used tocombine the results of the text and image similar-ities, simimg and simt.
The similarity between apair of items is computed as followssimT+I(A,B) = w1 ?
simt(At, Bt)+ w2 ?
simimg(Ai, Bi)where wi are weights learned using linear regres-sion (see Section 6.4).6 EvaluationThis section describes experiments used to evalu-ate the similarity measures described in the previ-ous sections.6.1 EuropeanaThe similarity measures are evaluated using infor-mation from Europeana3, a web-portal that pro-vides access to information CH artefacts.
Over2,000 institutions through out Europe have con-tributed to Europeana and the portal provides ac-cess to information about over 20 million CH arte-facts, making it one of the largest repositories3http://www.europeana.eu88of digital information about CH currently avail-able.
It contains information about a wide vari-ety of types of artefacts including paintings, pho-tographs and newspaper archives.
The informa-tion is in a range of European languages, withover 1 million items in English.
The diverse na-ture of Europeana makes it an interesting resourcefor exploring similarity measures.The Europeana portal provides various types ofinformation about each artefact, including textualinformation, thumbnail images of the items andlinks to additional information available for theproviding institution?s web site.
The textual in-formation is derived from metadata obtained fromthe providing institution and includes title, de-scription as well as details of the subject, mediumand creator.An example artefact from the Europeana por-tal is shown in Figure 1.
This particular artefactis an image showing detail of an architect?s officein Nottingham, United Kingdom.
The informa-tion provided for this item is relatively rich com-pared to other items in Europeana since the title isinformative and the textual description is of rea-sonable length.
However, the amount of informa-tion associated with items in Europeana is quitevaried and it is common for items to have shorttitles, which may be uninformative, or have verylimited textual descriptions.
In addition, the meta-data associated with items in Europeana is poten-tially a valuable source of information that couldbe used for, among other things, computing simi-larity between items.
However, the various pro-viding institutions do not use consistent codingschemes to populate these fields which makes itdifficult to compare items provided by differentinstitutions.
These differences in the informationprovided by the various institutions form a signif-icant challenge in processing the Europeana dataautomatically.6.2 Evaluation DataA data set was created by selecting 300 pairs ofitems added to Europeana by two providers: Cul-ture Grid4 and Scran5.
The items added to Eu-ropeana by these providers represent the major-ity that are in English and they contain differenttypes of items such as objects, archives, videosand audio files.
We removed five pairs that did4http://www.culturegrid.org.uk/5http://www.scran.ac.uk/not have any images associated with one of theitems.
(These items were audiofiles.)
The result-ing dataset consists of 295 pairs of items and isreferred to as Europeana295.Each item corresponds to a metadata recordconsisting of textual information together with aURI and a link to its thumbnail.
Figure 1 shows anitem taken from the Europeana website.
The title,description and subject fields have been shownto be useful information for computing similar-ity (Aletras et al, 2012).
These are extracted andconcatenated to form the textual information as-sociated with each item.
In addition, the accom-panying thumbnail image (or ?preview?)
was alsoextracted to be used as the visual information.
Thesize of these images varies from 7,000 to 10,000pixels.We have pre-processed the data by removingstop words and applying stemming.
For thetf.idf and LDA the training corpus was a total of759,896 Europeana items.
We have filtered outall items that have no description and have a ti-tle shorter than 4 words, or have a title which hasbeen repeated more than 100 times.6.3 Human Judgements of SimilarityCrowdflower6, a crowdsourcing platform, wasused to obtain human judgements of the simi-larity between each pair of items.
Participantswere asked to rate each item pair using a 5 pointscale where 4 indicated that the pair of items werehighly similar or near-identical while 0 indicatedthat they were completely different.
Participantswere presented with a page containing 10 pairs ofitems and asked to rate all of them.
Participantswere free to rate as many pages as they wanted upto a maximum of 30 pages (i.e.
the complete Eu-ropeana295 data set).
To ensure that the annota-tors were not returning random answers each pagecontained a pair for which the similarity had beenpre-identified as being at one end of the similarityscale (i.e.
either near-identical or completely dif-ferent).
Annotations from participants that failedto answer correctly these questions or participantsthat have given same rating to all of their answerswere removed.
A total of 3,261 useful annotationswere collected from 99 participants and each pairwas rated by at least 10 participants.The final similarity score for each pair was gen-6http://crowdflower.com/89Figure 1: Example item from Europeana portal showing how both textual and image information are displayed.
(Taken from http://www.europeana.eu/portal/)erated by averaging the ratings.
Inter-annotatoragreement was computed as the average of thePearson correlation between the ratings of eachparticipant and the average ratings of the otherparticipants, a methodology used by Grieser etal.
(2011).
The inter-annotator agreement for thedata set was ?
= +0.553, which is comparablewith the agreement score of ?
= +0.507 previ-ously reported by Grieser et al (2011).6.4 ExperimentsExperiments were carried out comparing the re-sults of the various techniques for computing textand image similarity (Sections 3 and 4) and theircombination (Section 5).
Performance is mea-sured as the Pearson?s correlation coefficient withthe gold-standard data.The combination of text and image similarity(Section 5) relies on a linear combination of textand image similarities.
The weights for this com-bination are obtained using a linear regressionmodel.
The input values were the results obtainedfor the individual text and similarity methods andthe target value was the gold-standard score foreach pair in the dataset.
10-fold cross-validationwas used for evaluation.7 ResultsAn overview of the results obtained is shown inTable 1.
Results for the text and image similaritymethods used alone are shown in the left and toppart of the table while the results for their combi-Image SimilarityRGB imgSeekText Similarity 0.254 0.370Word Overlap 0.487 0.450 0.554tf.idf 0.437 0.426 0.520N-gram overlap 0.399 0.384 0.504LDA 0.442 0.419 0.517Table 1: Performance of similarity measures appliedto Europeana295 data set (Pearson?s correlation coef-ficient).nation are in the main body.The best performance for text similarity (0.487)is achieved by Word Overlap and the lowest byN-gram Overlap (0.399).
The results are surpris-ing since the simplest approach produces the bestresults.
It is likely that the reason for these re-sults is the nature of the textual data in Europeana.The documents are often short, in some cases thedescription missing or the subject information isidentical to the title.For image similarity, results using imgSeek arehigher than RGB (0.370 and 0.254 respectively).There is also a clear difference between the per-formance of the text and image similarity meth-ods and results obtained from both image similar-ity measures is lower than all four that are basedon text.
The reason for these results is the natureof the Europeana images.
There are a large num-ber of black-and-white image pairs which meansthat colour information cannot be obtained from90many of them.
In addition, the images are lowresolution, since they are thumbnails, which lim-its the amount of shape information that can bederived from them, restricting the effectiveness ofimgSeek.
However, the fact that performance isbetter for imgSeek and RGB suggests that it is stillpossible to obtain useful information about shapefrom these images.When the image and text similarity measuresare combined the highest performance is achievedby the combination of the Word Overlap andimgSeek (0.554), the best performing text and im-age similarity measures when applied individu-ally.
The performance of all text similarity mea-sures improves when combined with imgSeek.All results are above 0.5 with the highest gainobserved for N-gram Overlap (from 0.399 to0.504), the worst performing text similarity mea-sure when applied individually.
On the otherhand, combining text similarity measures withRGB consistently leads to performance that islower than when the text similarity measure isused alone.These results demonstrate that improvementsin similarity scores can be obtained by makinguse of information from both text and images.
Inaddition, better results are obtained for the textsimilarity methods and this is likely to be causedby the nature of the images which are associatedwith the items in our data set.
It is also impor-tant to make use of an appropriate image similar-ity method since combing text similarity methodswith RGB reduces performance.8 ConclusionThis paper demonstrates how information fromtext and images describing CH artefacts can becombined to improve estimates of the similaritybetween them.
Four corpus-based and two image-based similarity measures are explored and eval-uated on a data set consisting of 295 manually-annotated pairs of items from Europeana.
Resultsshowed that combing information from text andimage similarity improves performance and thatimgSeek similarity method consistently improvesperformance of text similarity methods.In future work we intend to make use of othertypes of image features including the low-levelones used by approaches such as Scale Invari-ant Feature Transformation (SIFT) (Lowe, 1999;Lowe, 2004) and the bag-of-visual words model(Szeliski, 2010).
In addition we plan to applythese approaches to higher resolution images todetermine how the quality and size of an imageaffects similarity algorithms.AcknowledgmentsThe research leading to these results wascarried out as part of the PATHS project(http://paths-project.eu) funded bythe European Community?s Seventh FrameworkProgramme (FP7/2007-2013) under grant agree-ment no.
270082.ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pasca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and wordnet-based approaches.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics(NAACL ?09), pages 19?27, Boulder, Colorado.Nikolaos Aletras, Mark Stevenson, and Paul Clough.2012.
Computing similarity between items in a dig-ital library of cultural heritage.
Submitted.Alia Amin, Jacco van Ossenbruggen, Lynda Hard-man, and Annelies van Nispen.
2008.
Understand-ing Cultural Heritage Experts?
Information SeekingNeeds.
In Proceedings of the 8th ACM/IEEE-CSJoint Conference on Digital Libraries, pages 39?47,Pittsburgh, PA.Ricardo Baeza-Yates and Berthier Ribeiro-Neto.1999.
Modern Information Retrieval.
AddisonWesley Longman Limited, Essex.Kobus Barnard and David Forsyth.
2001.
Learn-ing the Semantics of Words and Pictures.
Pro-ceedings Eighth IEEE International Conference onComputer Vision (ICCV ?01), 2:408?415.Ron Bekkerman and Jiwoon Jeon.
2007.
Multi-modalclustering for multimedia collections.
In IEEE Con-ference on Computer Vision and Pattern Recogni-tion (CVPR ?07), pages 1?8.Gregory Beylkin, Ronald Coifman, and VladimirRokhlin.
1991.
Fast Wavelet Transforms and Nu-merical Algorithms I.
Communications on Pureand Applied Mathematics, 44:141?183.David M. Blei and Michael I. Jordan.
2003.
ModelingAnnotated Data.
Proceedings of the 26th annualinternational ACM SIGIR conference on Researchand Development in Information Retrieval (SIGIR?03), pages 127?134.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research, 3:993?1022.91Deng Cai, Xiaofei He, Zhiwei Li, Wei-Ying Ma, andJi-Rong Wen.
2004.
Hierarchical Clustering ofWWW Image Search Results Using Visual, Textualand Link Information.
Proceedings of the 12th an-nual ACM international conference on Multimedia(MULTIMEDIA ?04), pages 952?959.Ritendra Datta, Dhiraj Joshi, Jia Li, and James Z.Wang.
2008.
Image Retrieval: Ideas, Influences,and Trends of the New Age.
ACM Computing Sur-veys, 40(2):1?60.Yansong Feng and Mirella Lapata.
2010.
TopicModels for Image Annotation and Text Illustration.In Proceedings of Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 831?839, Los Angeles, California,June.Evgeniy Gabrilovich and Shaul Markovitch.
2007.Computing semantic relatedness using wikipedia-based explicit semantic analysis.
In Proceedings ofthe International Joint Conference on Artificial In-telligence (IJCAI ?07), pages 1606?1611.Karl Grieser, Timothy Baldwin, Fabian Bohnert, andLiz Sonenberg.
2011.
Using Ontological and Doc-ument Similarity to Estimate Museum Exhibit Re-latedness.
Journal on Computing and Cultural Her-itage (JOCCH), 3(3):10:1?10:20.Kasper Hornbaek and Morten Hertzum.
2011.
Thenotion of overview in information visualization.
In-ternational Journal of Human-Computer Studies,69:509?525.Charles E. Jacobs, Adam Finkelstein, and David H.Salesin.
1995.
Fast multiresolution image query-ing.
In Proceedings of the 22nd annual conferenceon Computer Graphics and Interactive Techniques(SIGGRAPH ?95), pages 277?286, New York, NY,USA.Jiwoon Jeon, Victor Lavrenko, and Raghavan Man-matha.
2003.
Automatic image annotation and re-trieval using cross-media relevance models.
In Pro-ceedings of the 26th annual international ACM SI-GIR Conference on Research and Development inInformation Retrieval (SIGIR ?03), pages 119?126,New York, NY, USA.Thorsten Joachims, Dayne Freitag, and Tom Mitchell.1997.
Webwatcher: A tour guide for the world wideweb.
In Proceedings of the International Joint Con-ference on Artificial Intelligence (IJCAI ?97), pages770?777.Tomi Kauppinen, Kimmo Puputti, Panu Paakkarinen,Heini Kuittinen, Jari Va?a?ta?inen, and Eero Hyvo?nen.2009.
Learning and visualizing cultural heritageconnections between places on the semantic web.In Proceedings of the Workshop on Inductive Rea-soning and Machine Learning on the Semantic Web(IRMLeS2009) and the 6th Annual European Se-mantic Web Conference (ESWC2009), Heraklion,Crete, Greece.Marco La Cascia, Sarathendu Sethi, and Stan Sclaroff.1998.
Combining textual and visual cues forcontent-based image retrieval on the world wideweb.
In IEEE Workshop on Content-Based Accessof Image and Video Libraries, pages 24?28.Michael Lesk.
1986.
Automatic Sense Disambigua-tion using Machine Readable Dictionaries: how totell a pine cone from an ice cream cone.
In Proceed-ings of the ACM Special Interest Group on the De-sign of Communication Conference (SIGDOC ?86),pages 24?26, Toronto, Canada.Nicolas Loeff, Cecilia Ovesdotter Alm, and David A.Forsyth.
2006.
Discriminating image senses byclustering with multimodal features.
In Proceed-ings of the COLING/ACL on Main ConferencePoster Sessions (COLING-ACL ?06), pages 547?554, Stroudsburg, PA, USA.David G. Lowe.
1999.
Object Recognition from LocalScale-invariant Features.
Proceedings of the Sev-enth IEEE International Conference on ComputerVision, pages 1150?1157.David G. Lowe.
2004.
Distinctive Image Fea-tures from Scale-Invariant Keypoints.
InternationalJournal of Computer Vision, 60(2):91?110.Christopher D. Manning and Hinrich Schutze.
1999.Foundations of Statistical Natural Language Pro-cessing.
The MIT Press.Gary Marchionini.
2006.
Exploratory Search: fromFinding to Understanding.
Communications of theACM, 49(1):41?46.David Milne.
2007.
Computing Semantic Relatednessusing Wikipedia Link Structure.
In Proceedings ofthe New Zealand Computer Science Research Stu-dent Conference.Patrick Ndjiki-Nya, Oleg Novychny, and Thomas Wie-gand.
2004.
Merging MPEG 7 Descriptors for Im-age Content Analysis.
In Proceedings of IEEE In-ternational Conference on Acoustics, Speech, andSignal Processing (ICASSP ?04), pages 5?8.Siddhard Patwardhan, Satanjeev Banerjee, and TedPedersen.
2003.
Using Measures of Semantic Re-latedness for Word Sense Disambiguation.
In Pro-ceedings of the 4th International Conference on In-telligent Text Processing and Computational Lin-guistics, pages 241?257.Mykola Pechenizkzy and Toon Calders.
2007.
Aframework for guiding the museum tours personal-ization.
In Proceedings of the Workshop on Person-alised Access to Cultural Heritage (PATCH ?07),pages 11?28.Koen E.A.
Sande, Theo Gevers, and Cees G. M.Snoek.
2008.
Evaluation of Color Descriptors forObject and Scene Recognition.
In Proceedings ofthe IEEE Computer Society Conference on Com-puter Vision and Pattern Recognition (CVPR ?08),pages 1?8.92Bernt Schiele and James L. Crowley.
1996.
Objectrecognition using multidimensional receptive fieldhistograms.
In Proceedings of the 4th EuropeanConference on Computer Vision (ECCV ?96), pages610?619, London, UK.Nicu Sebe and Michael S. Lew.
2001.
Color-basedRetrieval.
Pattern Recognition Letters, 22:223?230, February.Rohini K. Srihari, Aibing Rao, Benjamin Han,Srikanth Munirathnam, and Xiaoyun Wu.
2000.
Amodel for multimodal information retrieval.
In Pro-ceedings of the IEEE International Conference onMultimedia and Expo (ICME ?00), pages 701?704.Grant Strong and Minglun Gong.
2009.
Organizingand Browsing Photos using Different Feature Vec-tors and their Evaluations.
Proceedings of the ACMInternational Conference on Image and Video Re-trieval (CIVR ?09), pages 3:1?3:8.Michael J. Swain and Dana H. Ballard.
1991.
Colorindexing.
International Journal of Computer Vi-sion, 7:11?32.Richard Szeliski.
2010.
Computer Vision: Algorithmsand Applications.
Springer-Verlag Inc. New York.Xin-Jing Wang, Wei-Ying Ma, Gui-Rong Xue, andXing Li.
2004.
Multi-model similarity propaga-tion and its application for web image retrieval.In Proceedings of the 12th annual ACM Interna-tional Conference on Multimedia (MULTIMEDIA?04), pages 944?951, New York, NY, USA.Yiwen Wang, Natalia Stash, Lora Aroyo, PeterGorgels, Lloyd Rutledge, and Guus Schreiber.2008.
Recommendations based on semantically-enriched museum collections.
Journal of Web Se-mantics: Science, Services and Agents on the WorldWide Web, 6(4):43?50.Yiwen Wang, Lora Aroyo, Natalia Stash, Rody Sam-beek, Schuurmans Yuri, Guus Schreiber, and Pe-ter Gorgels.
2009.
Cultivating personalized mu-seum tours online and on-site.
Journal of Interdis-ciplinary Science Reviews, 34(2):141?156.Thijs Westerveld.
2002.
Probabilistic multimedia re-trieval.
In Proceedings of the 25th Annual Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval (SIGIR ?02),pages 437?438, New York, NY, USA.Hui Yu, Mingjing Li, Hong-Jiang Zhang, and JufuFeng.
2002.
Color Texture Moments for Content-based Image Retrieval.
In Proceedings of theIEEE International Conference on Image Process-ing (ICIP ?02), pages 929?932.Xiang Sean Zhou and Thomas S. Huang.
2002.
Uni-fying keywords and visual contents in image re-trieval.
IEEE Multimedia, 9(2):23 ?33.93
