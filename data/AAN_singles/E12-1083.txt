Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 818?828,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsStructural and Topical Dimensions in Multi-Task Patent TranslationKatharina Wa?schle and Stefan RiezlerDepartment of Computational LinguisticsHeidelberg University, Germany{waeschle,riezler}@cl.uni-heidelberg.deAbstractPatent translation is a complex problem dueto the highly specialized technical vocab-ulary and the peculiar textual structure ofpatent documents.
In this paper we analyzepatents along the orthogonal dimensions oftopic and textual structure.
We view differ-ent patent classes and different patent textsections such as title, abstract, and claims,as separate translation tasks, and investi-gate the influence of such tasks on machinetranslation performance.
We study multi-task learning techniques that exploit com-monalities between tasks by mixtures oftranslation models or by multi-task meta-parameter tuning.
We find small but sig-nificant gains over task-specific trainingby techniques that model commonalitiesthrough shared parameters.
A by-productof our work is a parallel patent corpus of 23million German-English sentence pairs.1 IntroductionPatents are an important tool for the protectionof intellectual property and also play a significantrole in business strategies in modern economies.Patent translation is an enabling technique forpatent prior art search which aims to detect apatent?s novelty and thus needs to be cross-lingualfor a multitude of languages.
Patent translation iscomplicated by a highly specialized vocabulary,consisting of technical terms specific to the fieldof invention the patent relates to.
Patents are writ-ten in a sophisticated legal jargon (?patentese?
)that is not found in everyday language and ex-hibits a complex textual structure.
Also, patentsare often intentionally ambiguous or vague in or-der to maximize the coverage of the claims.In this paper, we analyze patents with respectto the orthogonal dimensions of topic ?
the tech-nical field covered by the patent ?
and structure?
a patent?s text sections ?, with respect to theirinfluence on machine translation performance.The topical dimension of patents is charac-terized by the International Patent Classification(IPC)1 which categorizes patents hierarchicallyinto 8 sections, 120 classes, 600 subclasses, downto 70,000 subgroups at the leaf level.
Table 1shows the 8 top level sections.A Human NecessitiesB Performing Operations, TransportingC Chemistry, MetallurgyD Textiles, PaperE Fixed ConstructionsF Mechanical Engineering, Lighting,Heating, WeaponsG PhysicsH ElectricityTable 1: IPC top level sections.Orthogonal to the patent classification, patentdocuments can be sub-categorized along the di-mension of textual structure.
Article 78.1 of theEuropean Patent Convention (EPC) lists all sec-tions required in a patent document2:?A European patent application shallcontain:(a) a request for the grant of a Euro-pean patent;1http://www.wipo.int/classifications/ipc/en/2Highlights by the authors.818(b) a description of the invention;(c) one or more claims;(d) any drawings referred to in the de-scription or the claims;(e) an abstract,and satisfy the requirements laid downin the Implementing Regulations.
?The request for grant contains the patent title; thusa patent document comprises the textual elementsof title, description, claim, and abstract.We investigate whether it is worthwhile to treatdifferent values along the structural and topicaldimensions as different tasks that are not com-pletely independent of each other but share somecommonalities, yet differ enough to counter asimple pooling of data.
For example, we con-sider different tasks such as patents from differentIPC classes, or along an orthogonal dimension,patent documents of all IPC classes but consistingonly of titles or only of claims.
We ask whethersuch tasks should be addressed as separate trans-lation tasks, or whether translation performancecan be improved by learning several tasks simul-taneously through shared models that are more so-phisticated than simple data pooling.
Our goal isto learn a patent translation system that performswell across several different tasks, thus benefitsfrom shared information, but is yet able to addressthe specifics of each task.One contribution of this paper is a thoroughanalysis of the differences and similarities of mul-tilingual patent data along the dimensions of tex-tual structure and topic.
The second contributionis the experimental investigation of the influenceof various such tasks on patent translation perfor-mance.
Starting from baseline models that aretrained on individual tasks or on data pooled fromall tasks, we apply mixtures of translation mod-els and multi-task minimum error rate training tomultiple patent translation tasks.
A by-product ofour research is a parallel patent corpus of over 23million sentence pairs.2 Related workMulti-task learning has mostly been discussed un-der the name of multi-domain adaptation in thearea of statistical machine translation (SMT).
Ifwe consider domains as tasks, domain adapta-tion is a special two-task case of multi-task learn-ing.
Most previous work has concentrated onadapting unsupervised generative modules suchas translation models or language models to newtasks.
For example, transductive approaches haveused automatic translations of monolingual cor-pora for self-training modules of the generativeSMT pipeline (Ueffing et al 2007; Schwenk,2008; Bertoldi and Federico, 2009).
Other ap-proaches have extracted parallel data from similaror comparable corpora (Zhao et al 2004; Snoveret al 2008).
Several approaches have been pre-sented that train separate translation and languagemodels on task-specific subsets of the data andcombine them in different mixture models (Fos-ter and Kuhn, 2007; Koehn and Schroeder, 2007;Foster et al 2010).
The latter kind of approach isapplied in our work to multiple patent tasks.Multi-task learning efforts in patent transla-tion have so far been restricted to experimentalcombinations of translation and language mod-els from different sets of IPC sections.
For ex-ample, Utiyama and Isahara (2007) and Tinsleyet al(2010) investigate translation and languagemodels trained on different sets of patent sections,with larger pools of parallel data improving re-sults.
Ceaus?u et al(2011) find that language mod-els always and translation model mostly benefitfrom larger pools of data from different sections.Models trained on pooled patent data are used asbaselines in our approach.The machine learning community has devel-oped several different formalizations of the cen-tral idea of trading off optimality of parametervectors for each task-specific model and close-ness of these model parameters to the average pa-rameter vector across models.
For example, start-ing from a separate SVM for each task, Evgeniouand Pontil (2004) present a regularization methodthat trades off optimization of the task-specific pa-rameter vectors and the distance of each SVM tothe average SVM.
Equivalent formalizations re-place parameter regularization by Bayesian priordistributions on the parameters (Finkel and Man-ning, 2009) or by augmentation of the featurespace with domain independent features (Daume?,2007).
Besides SVMs, several learning algo-rithms have been extended to the multi-task sce-nario in a parameter regularization setting, e.g.,perceptron-type algorithms (Dredze et al 2010)or boosting (Chapelle et al 2011).
Further vari-ants include different formalizations of norms forparameter regularization, e.g., `1,2 regularization819(Obozinski et al 2010) or `1,?
regularization(Quattoni et al 2009), where only the featuresthat are most important across all tasks are kept inthe model.
In our experiments, we apply parame-ter regularization for multi-task learning to mini-mum error rate training for patent translation.3 Extraction of a parallel patent corpusfrom comparable dataOur work on patent translation is based on theMAREC3 patent data corpus.
MAREC con-tains over 19 million patent applications andgranted patents in a standardized format fromfour patent organizations (European Patent Of-fice (EP), World Intellectual Property Organiza-tion (WO), United States Patent and TrademarkOffice (US), Japan Patent Office (JP)), from 1976to 2008.
The data for our experiments are ex-tracted from the EP and WO collections whichcontain patent documents that include translationsof some of the patent text.
To extract such parallelpatent sections, we first determine the longest in-stance, if different kinds4 exist for a patent.
Weassume titles to be sentence-aligned by default,and define sections with a token ratio larger than0.7 as parallel.
For the language pair German-English we extracted a total of 2,101,107 paralleltitles, 291,716 parallel abstracts, and 735,667 par-allel claims sections.The lack of directly translated descriptionsposes a serious limitation for patent translation,since this section constitutes the largest part of thedocument.
It is possible to obtain comparable de-scriptions from related patents that have been filedin different countries and are connected throughthe patent family id.
We extracted 172,472 patentsthat were both filed with the USPTO and the EPOand contain an English and a German description,respectively.For sentence alignment, we used the Gargan-tua5 tool (Braune and Fraser, 2010) that fil-ters a sentence-length based alignment with IBMModel-1 lexical word translation probabilities, es-timated on parallel data obtained from the first-3http://www.ir-facility.org/prototypes/marec4A patent kind code indicates the document stage in thefiling process, e.g., A for applications and B for grantedpatents, with publication levels from 1-9.
See http://www.wipo.int/standards/en/part\_03.html.5http://gargantua.sourceforge.netpass alignment.
This yields the parallel corpuslisted in table 2 with high input-output ratios forclaims, and much lower ratios for abstracts anddescriptions, showing that claims exhibit a nat-ural parallelism due to their structure, while ab-stracts and descriptions are considerably less par-allel.
Removing duplicates and adding parallel ti-tles results in a corpus of over 23 million parallelsentence pairs.output de ratio en ratioabstract 720,571 92.36% 76.81%claims 8,346,863 97.82% 96.17%descr.
14,082,381 86.23% 82.67%Table 2: Number of parallel sentences in output withinput/output ratio of sentence aligner.Differences between the text sections becomevisible in an analysis of token to type ratios.
Ta-ble 3 gives the average number of tokens com-pared to the average type frequencies for a win-dow of 100,000 tokens from every subsection.
Itshows that titles contain considerably fewer to-kens than other sections, however, the disadvan-tage is partially made up by a relatively largeamount of types, indicated by a lower averagetype frequency.tokens typesde en de entitle 6.5 8.0 2.9 4.8abstract 37.4 43.2 4.3 9.0claims 53.2 61.3 5.5 9.5description 27.5 35.5 4.0 7.0Table 3: Average number of tokens and average typefrequencies in text sections.We reserved patent data published between1979 and 2007 for training and documents pub-lished in 2008 for tuning and testing in SMT.For the dimension of text sections, we sampled500,000 sentences ?
distributed across all IPCsections ?
for training and 2,000 sentences foreach text section for development and testing.
Be-cause of a relatively high number of identical sen-tences in test and training set for titles, we re-moved the overlap for this section.Table 4 shows the distribution of IPC sectionson claims, with the smallest class accounting for820around 300,000 parallel sentences.
In order to ob-tain similar amounts of training data for each taskalong the topical dimension, we sampled 300,000sentences from each IPC class for training, and2,000 sentences for each IPC class for develop-ment and testing.A 1,947,542B 2,522,995C 2,263,375D 299,742E 353,910F 1,012,808G 2,066,132H 1,754,573Table 4: Distribution of IPC sections on claims.4 Machine translation experiments4.1 Individual task baselinesFor our experiments we used the phrase-based,open-source SMT toolkit Moses6 (Koehn et al2007).
For language modeling, we computed5-gram models using IRSTLM7 (Federico etal., 2008) and queried the model with KenLM(Heafield, 2011).
BLEU (Papineni et al 2001)scores were computed up to 4-grams on lower-cased data.Europarl-v6 MARECBLEU OOV BLEU OOVabstract 0.1726 14.40% 0.3721 3.00%claim 0.2301 15.80% 0.4711 4.20%title 0.0964 26.00% 0.3228 9.20%Table 5: BLEU scores and OOV rate for Europarl base-line and MAREC model.Table 5 shows a first comparison of results ofMoses models trained on 500,000 parallel sen-tences from patent text sections balanced over IPCclasses, against Moses trained on 1.7 Million sen-tences of parliament proceedings from Europarl8(Koehn, 2005).
The best result on each section isindicated in bold face.
The Europarl model per-forms very poorly on all three sections in compar-6http://statmt.org/moses/7http://sourceforge.net/projects/irstlm/8http://www.statmt.org/europarl/ison to the task-specific MAREC model, althoughthe former has been learned on more than threetimes the amount of data.
An analysis of the out-put of both system shows that the Europarl modelsuffers from two problems: Firstly, there is an ob-vious out of vocabulary (OOV) problem of theEuroparl model compared to the MAREC model.Secondly, the Europarl model suffers from incor-rect word sense disambiguation, as illustrated bythe samples in table 6.source steuerbar leitetEuroparl taxable is in charge ofMAREC controllable guidingreference controllable guidesTable 6: Output of Europarl model on MAREC data.Table 7 shows the results of the evaluationacross text sections; we measured the perfor-mance of separately trained and tuned individualmodels on every section.
The results allow someconclusions about the textual characteristics of thesections and indicate similarities.
Naturally, ev-ery task is best translated with a model trainedon the respective section, as the BLEU scoreson the diagonal are the highest in every column.Accordingly, we are interested in the runner-upon each section, which is indicated in bold font.The results on abstracts suggest that this sectionbears the strongest resemblance to claims, sincethe model trained on claims achieves a respectablescore.
The abstract model seems to be the mostrobust and varied model, yielding the runner-upscore on all other sections.
Claims are easiest totranslate, yielding the highest overall BLEU scoreof 0.4879.
In contrast to that, all models scoreconsiderably lower on titles.testtrain abstract claim title desc.abstract 0.3737 0.4076 0.2681 0.2812claim 0.3416 0.4879 0.2420 0.2623title 0.2839 0.3512 0.3196 0.1743desc.
0.32189 0.403 0.2342 0.3347Table 7: BLEU scores for 500k individual text sectionmodels.The cross-section evaluation on the IPC classes(table 8) shows similar patterns.
Each section821is best translated with a model trained on datafrom the same section.
Note that best sectionscores vary considerably, ranging from 0.5719 onC to 0.4714 on H, indicating that higher-scoringclasses, such as C and A, are more homogeneousand therefore easier to translate.
C, the Chem-istry section, presumably benefits from the factthat the data contain chemical formulae, whichare language-independent and do not have to betranslated.
Again, for determining the relation-ship between the classes, we examine the bestrunner-up on each section, considering the BLEUscore, although asymmetrical, as a kind of mea-sure of similarity between classes.
We can es-tablish symmetric relationships between sectionsA and C, B and F as well as G and H, whichmeans that the models are mutual runner-up onthe other?s test section.The similarities of translation tasks estab-lished in the previous section can be confirmedby information-theoretic similarity measures thatperform a pairwise comparison of the vocabularyprobability distribution of each task-specific cor-pus.
This distribution is calculated on the basis ofthe 500 most frequent words in the union of twocorpora, normalized by vocabulary size.
As met-ric we use the A-distance measure of Kifer et al(2004).
IfA is the set of events on which the worddistributions of two corpora are defined, then theA-distance is the supremum of the difference ofprobabilities assigned to the same event.
Low dis-tance means higher similarity.Table 9 shows the A-distance of corpora spe-cific to IPC classes.
The most similar section orsections ?
apart from the section itself on the di-agonal ?
is indicated in bold face.
The pairwisesimilarity of A and C, B and F, G and H obtainedby BLEU score is confirmed.
Furthermore, a closesimilarity between E and F is indicated.
G andH (electricity and physics, respectively) are verysimilar to each other but not close to any othersection apart from B.4.2 Task pooling and mixtureOne straightforward technique to exploit com-monalities between tasks is pooling data fromseparate tasks into a single training set.
Instead ofa trivial enlargement of training data by pooling,we train the pooled models on the same amountof sentences as the individual models.
For in-stance, the pooled model for the pairing of IPCsection B and C is trained on a data set composedof 150,000 sentences from each IPC section.
Thepooled model for pairing data from abstracts andclaims is trained on data composed of 250,000sentences from each text section.Another approach to exploit commonalities be-tween tasks is to train separate language and trans-lation models9 on the sentences from each taskand combine the models in the global log-linearmodel of the SMT framework, following Fos-ter and Kuhn (2007) and Koehn and Schroeder(2007).
Model combination is accomplished byadding additional language model and translationmodel features to the log-linear model and tuningthe additional meta-parameters by standard mini-mum error rate training (Bertoldi et al 2009).We try out mixture and pooling for all pairwisecombinations of the three structural sections, forwhich we have high-quality data, i.e.
abstract,claims and title.
Due to the large number of pos-sible combinations of IPC sections, we limit theexperiments to pairs of similar sections, based onthe A-distance measure.Table 10 lists the results for two combinationsof data from different sections: a log-linear mix-ture of separately trained models and simple pool-ing, i.e.
concatenation, of the training data.
Over-all, the mixture models perform slightly betterthan the pooled models on the text sections, al-though the difference is significant only in twocases.
This is indicated by highlighting best re-sults in bold face (with more than one result high-lighted if the difference is not significant).10We investigate the same mixture and poolingtechniques on the IPC sections we consideredpairwise similar (see table 11).
Somehow contra-dicting the former results, the mixture models per-form significantly worse than the pooled model onthree sections.
This might be the result of inade-quate tuning, since most of the time the MERTalgorithm did not converge after the maximumnumber of iterations, due to the larger number offeatures when using several models.9Following Duh et al(2010), we use the alignmentmodel trained on the pooled data set in the phrase extractionphase of the separate models.
Similarly, we use a globallytrained lexical reordering model.10For assessing significance, we apply the approximaterandomization method described in Riezler and Maxwell(2005).
We consider pairwise differing results scoring a p-value smaller than 0.05 as significant; the assessment is re-peated three times and the average value is taken.822testtrain A B C D E F G HA 0.5349 0.4475 0.5472 0.4746 0.4438 0.4523 0.4318 0.4109B 0.4846 0.4736 0.5161 0.4847 0.4578 0.4734 0.4396 0.4248C 0.5047 0.4257 0.5719 0.462 0.4134 0.4249 0.409 0.3845D 0.47 0.4387 0.5106 0.5167 0.4344 0.4435 0.407 0.3917E 0.4486 0.4458 0.4681 0.4531 0.4771 0.4591 0.4073 0.4028F 0.4595 0.4588 0.4761 0.4655 0.4517 0.4909 0.422 0.4188G 0.4935 0.4489 0.5239 0.4629 0.4414 0.4565 0.4748 0.4532H 0.4628 0.4484 0.4914 0.4621 0.4421 0.4616 0.4588 0.4714Table 8: BLEU scores for 300k individual IPC section models.A B C D E F G HA 0 0.1303 0.1317 0.1311 0.188 0.186 0.164 0.1906B 0.1302 0 0.2388 0.1242 0.0974 0.0875 0.1417 0.1514C 0.1317 0.2388 0 0.1992 0.311 0.3068 0.2506 0.2825D 0.1311 0.1242 0.1992 0 0.1811 0.1808 0.1876 0.201E 0.188 0.0974 0.311 0.1811 0 0.0921 0.2058 0.2025F 0.186 0.0875 0.3068 0.1808 0.0921 0 0.1824 0.1743G 0.164 0.1417 0.2506 0.1876 0.2056 0.1824 0 0.064H 0.1906 0.1514 0.2825 0.201 0.2025 0.1743 0.064 0Table 9: Pairwise A-distance for 300k IPC training sets.train test pooling mixtureabstract-claim abstract 0.3703 0.3704claim 0.4809 0.4834claim-title claim 0.4799 0.4789title 0.3269 0.328title-abstract title 0.3311 0.3275abstract 0.3643 0.366Table 10: Mixture and pooling on text sections.A comparison of the results for pooling andmixture with the respective results for individualmodels (tables 7 and 8) shows that replacing datafrom the same task by data from related tasksdecreases translation performance in almost allcases.
The exception is the title model that bene-fits from pooling and mixing with both abstractsand claims due to their richer data structure.4.3 Multi-task minimum error rate trainingIn contrast to task pooling and task mixtures, thespecific setting addressed by multi-task minimumerror rate training is one in which the generativetrain test pooling mixtureA-C A 0.5271 0.5274C 0.5664 0.5632B-F B 0.4696 0.4354F 0.4859 0.4769G-H G 0.4735 0.4754H 0.4634 0.467Table 11: Mixture and pooling on IPC sections.SMT pipeline is not adaptable.
Such situationsarise if there are not enough data to train transla-tion models or language models on the new tasks.However, we assume that there are enough paral-lel data available to perform meta-parameter tun-ing by minimum error rate training (MERT) (Och,2003; Bertoldi et al 2009) for each task.A generic algorithm for multi-task learningcan be motivated as follows: Multi-task learningaims to take advantage of commonalities sharedamong tasks by learning several independent butrelated tasks together.
Information is shared be-tween tasks through a joint representation and in-823tuningtest individual pooled average MMERT MMERT-averageabstract 0.3721 0.362 0.3657?+ 0.3719+ 0.3685?+claim 0.4711 0.4681 0.4749?+ 0.475?+ 0.4734?+title 0.3228 0.3152 0.3326?+ 0.3268?+ 0.3325?+Table 12: Multi-task tuning on text sections.tuningtest individual pooled average MMERT MMERT-averageA 0.5187 0.5199 0.5213?+ 0.5195 0.5196B 0.4877 0.4885 0.4908?+ 0.4911?+ 0.4921?+C 0.5214 0.5175 0.5199?+ 0.5218+ 0.5162?+D 0.4724 0.4730 0.4733 0.4736 0.4734E 0.4666 0.4661 0.4679?+ 0.4669+ 0.4685?+F 0.4794 0.4801 0.4811?
0.4821?+ 0.4830?+G 0.4596 0.4576 0.4607+ 0.4606+ 0.4610?+H 0.4573 0.4560 0.4578 0.4581+ 0.4581+Table 13: Multi-task tuning on IPC sections.troduces an inductive bias.
Evgeniou and Pon-til (2004) propose a regularization method thatbalances task-specific parameter vectors and theirdistance to the average.
The learning objective isto minimize task-specific loss functions ld acrossall tasks d with weight vectors wd, while keep-ing each parameter vector close to the average1D?Dd=1wd = wavg.
This is enforced by min-imizing the norm (here the `1-norm) of the dif-ference of each task-specific weight vector to theavarage weight vector.minw1,...,wDD?d=1ld(wd) + ?D?d=1||wd ?
wavg||1 (1)The MMERT algorithm is given in figure 1.The algorithm starts with initial weights w(0).
Ateach iteration step, the average of the parame-ter vectors from the previous iteration is com-puted.
For each task d ?
D, one iteration of stan-dard MERT is called, continuing from weight vec-tor w(t?1)d and minimizing translation loss func-tion ld on the data from task d. The individu-ally tuned weight vectors returned by MERT arethen moved towards the previously calculated av-erage by adding or subtracting a penalty term ?for each weight component w(t)d [k].
If a weightmoves beyond the average, it is clipped to the av-erage value.
The process is iterated until a stop-ping criterion is met, e.g.
a threshold on the max-imum change in the average weight vector.
Theparameter ?
controls the influence of the regular-ization.
A larger ?
pulls the weights closer to theaverage, a smaller ?
leaves more freedom to theindividual tasks.MMERT(w(0), D, {ld}Dd=1):for t = 1, .
.
.
, T dow(t)avg = 1D?Dd=1w(t?1)dfor d = 1, .
.
.
, D parallel dow(t)d = MERT(w(t?1)d , ld)for k = 1, .
.
.
,K doif w[k](t)d ?
w(t)avg[k] > 0 thenw(t)d [k] = max(w(t)avg[k], w(t)d [k]??
)else if w(t)d [k]?
w(t)avg[k] < 0 thenw(t)d [k] = min(w(t)avg[k], w(t)d [k] + ?
)end ifend forend forend forreturn w(T )1 , .
.
.
, w(T )D , w(T )avgFigure 1: Multi-task MERT.824The weight updates and the clipping strategycan be motivated in a framework of gradient de-scent optimization under `1-regularization (Tsu-ruoka et al 2009).
Assuming MERT as algorith-mic minimizer11 of the loss function ld in equa-tion 1, the weight update towards the averagefollows from the subgradient of the `1 regular-izer.
Since w(t)avg is taken as average over weightsw(t?1)d from the step before, the term w(t)avg is con-stant with respect to w(t)d , leading to the follow-ing subgradient (where sgn(x) = 1 if x > 0,sgn(x) = ?1 if x < 0, and sgn(x) = 0 if x = 0):?
?w(t)r [k]?D?d=1????
?w(t)d ?1DD?s=1w(t?1)s????
?1= ?
sgn(w(t)r [k]?1DD?s=1w(t?1)s [k]).Gradient descent minimization tells us to move inthe opposite direction of the subgradient, thus mo-tivating the addition or subtraction of the regular-ization penalty.
Clipping is motivated by the de-sire to avoid oscillating parameter weights and inorder to to enforce parameter sharing.Experimental results for multi-task MERT(MMERT) are reported for both dimensions ofpatent tasks.
For the IPC sections we traineda pooled model on 1,000,000 sentences sampledfrom abstracts and claims from all sections.
Wedid not balance the sections but kept their orig-inal distribution, reflecting a real-life task wherethe distribution of sections is unknown.
We thenextend this experiment to the structural dimen-sion.
Since we do not have an intuitive notion of anatural distribution for the text sections, we traina balanced pooled model on a corpus composedof 170,000 sentences each from abstracts, claimsand titles, i.e.
510,000 sentences in total.
Forboth dimensions, for each task, we sampled 2,000parallel sentences for development, development-testing, and testing from patents that were pub-lished in different years than the training data.We compare the multi-task experiments withtwo baselines.
The first baseline is individualtask learning, corresponding to standard separateMERT tuning on each section (individual).
Thisresults in three separately learned weight vectors11MERT as presented in Och (2003) is not a gradient-based optimization techniquem, thus MMERT is strictlyspeaking only ?inspired?
by gradient descent optimization.for each task, where no information has beenshared between the tasks.
The second baselinesimulates the setting where the sections are notdifferentiated at all.
We tune the model on apooled development set of 2,000 sentences thatcombines the same amount of data from all sec-tions (pooled).
This yields a single joint weightvector for all tasks optimized to perform wellacross all sections.
Furthermore, we comparemulti-task MERT tuning with two parameter av-eraging methods.
The first method computes thearithmetic mean of the weight vectors returned bythe individual baseline for each weight compo-nent, yielding a joint average vector for all tasks(average).
The second method takes the last av-erage vector computed during multi-task MERTtuning (MMERT-average).12Tables 12 and 13 give the results for multi-tasklearning on text and IPC sections.
The latter re-sults have been presented earlier in Simianer et al(2011).
The former table extends the techniqueof multi-task MERT to the structural dimensionof patent SMT tasks.
In all experiments, the pa-rameter ?
was adjusted to 0.001 after evaluatingdifferent settings on a development set.
The bestresult on each section is indicated in bold face; *indicates significance with respect to the individ-ual baseline, + the same for the pooled baseline.We observe statistically significant improvementsof 0.5 to 1% BLEU over the individual baseline forclaims and titles; for abstracts, the multi-task vari-ant yields the same result as the baseline, whilethe averaging methods perform worse.
Multi-taskMERT yields the best result for claims; on titles,the simple average and the last MMERT averagedominate.
Pooled tuning always performs signifi-cantly worse than any other method, confirmingthat it is beneficial to differentiate between thetext section sections.Similarly for IPC sections, small but statisti-cally significant improvements over the individualand pooled baselines are achieved by multi-tasktuning and averaging over IPC sections, except-ing C and D. However, an advantage of multi-tasktuning over averaging is hard to establish.Note that the averaging techniques implicitlybenefit from a larger tuning set.
In order to ascer-tain that the improvements by averaging are not12The aspect of averaging found in all of our multi-tasklearning techniques effectively controls for optimizer insta-bility as mentioned in Clark et al(2011).825test pooled-6k significanceabstract 0.3628 <claim 0.4696 <title 0.3174 <Table 14: Multi-task tuning on 6,000 sentences pooledfrom text sections.
?<?
denotes a statistically signifi-cant difference to the best result.simply due to increasing the size of the tuning set,we ran a control experiment where we tuned themodel on a pooled development set of 3 ?
2, 000sentences for text sections and on a developmentset of 8 ?
2, 000 sentences for IPC sections.
Theresults given in table 14 show that tuning on apooled set of 6,000 text sections yields only min-imal differences to tuning on 2,000 sentence pairssuch that the BLEU scores for the new pooledmodels are still significantly lower than the bestresults in table 12 (indicated by ?<?).
However,increasing the tuning set to 16,000 sentence pairsfor IPC sections makes the pooled baseline per-form as well as the best results in table 13, exceptfor two cases (indicated by ?<?)
(see table 15).This is due to the smaller differences between bestand worst results for tuning on IPC sections com-pared to tuning on text sections, indicating thatIPC sections are less well suited for multi-tasktuning than the textual domains.test pooled-16k significanceA 0.5177 <B 0.4920C 0.5133 <D 0.4737E 0.4685F 0.4832G 0.4608H 0.4579Table 15: Multi-task tuning on 16,000 sentencespooled from IPC sections.
?<?
denotes a statisticallysignificant difference to the best result.5 ConclusionThe most straightforward approach to improvemachine translation performance on patents is toenlarge the training set to include all availabledata.
This question has been investigated by Tins-ley et al(2010) and Utiyama and Isahara (2007).A caveat in this situation is that data need to befrom the general patent domain, as shown by theinferior performance of a large Europarl-trainedmodel compared to a small patent-trained model.The goal of this paper is to analyze patent dataalong the topical dimension of IPC classes andalong the structural dimension of textual sections.Instead of trying to beat a pooling baseline thatsimply increases the data size, our research goalis to investigate whether different subtasks alongthese dimensions share commonalities that canfruitfully be exploited by multi-task learning inmachine translation.
We thus aim to investigatethe benefits of multi-task learning in realistic sit-uations where a simple enlargement of trainingdata is not possible.Starting from baseline models that are trainedon individual tasks or on data pooled from alltasks, we apply mixtures of translation modelsand multi-task MERT tuning to multiple patenttranslation tasks.
We find small, but statisticallysignificant improvements for multi-task MERTtuning and parameter averaging techniques.
Im-provements are more pronounced for multi-tasklearning on textual domains than on IPC domains.This might indicate that the IPC sections are lesswell delimitated than the structural domains.
Fur-thermore, this is owing to the limited expressive-ness of a standard linear model including 14-20features in tuning.
The available features are verycoarse and more likely to capture structural dif-ferences, such as sentence length, than the lexi-cal differences that differentiate the semantic do-mains.
We expect to see larger gains due to multi-task learning for discriminatively trained SMTmodels that involve very large numbers of fea-tures, especially when multi-task learning is donein a framework that combines parameter regular-ization with feature selection (Obozinski et al2010).
In future work, we will explore a combina-tion of large-scale discriminative training (Lianget al 2006) with multi-task learning for SMT.AcknowledgmentsThis work was supported in part by DFG grant?Cross-language Learning-to-Rank for Patent Re-trieval?.826ReferencesNicola Bertoldi and Marcello Federico.
2009.
Do-main adaptation for statistical machine translationwith monolingual resources.
In Proceedings of the4th EACL Workshop on Statistical Machine Trans-lation, Athens, Greece.Nicola Bertoldi, Barry Haddow, and Jean-BaptisteFouet.
2009.
Improved minimum error rate train-ing in Moses.
The Prague Bulletin of MathematicalLinguistics, 91:7?16.Fabienne Braune and Alexander Fraser.
2010.
Im-proved unsupervised sentence alignment for sym-metrical and asymmetrical parallel corpora.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics (COLING?10), Beijing,China.Alexandru Ceaus?u, John Tinsley, Jian Zhang, andAndy Way.
2011.
Experiments on domain adap-tation for patent machine translation in the PLuTOproject.
In Proceedings of the 15th Conference ofthe European Assocation for Machine Translation(EAMT 2011), Leuven, Belgium.Olivier Chapelle, Pannagadatta Shivaswamy, SrinivasVadrevu, Kilian Weinberger, Ya Zhang, and BelleTseng.
2011.
Boosted multi-task learning.
Ma-chine Learning.Jonathan Clark, Chris Dyer, Alon Lavie, and NoahSmith.
2011.
Better hypothesis testing for statis-tical machine translation: Controlling for optimizerinstability.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics (ACL?11), Portland, OR.Hal Daume?.
2007.
Frustratingly easy domain adap-tation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguis-tics (ACL?07), Prague, Czech Republic.Mark Dredze, Alex Kulesza, and Koby Crammer.2010.
Multi-domain learning by confidence-weighted parameter combination.
Machine Learn-ing, 79:123?149.Kevin Duh, Katsuhito Sudoh, and Hajime Tsukada.2010.
Analysis of translation model adaptation instatistical machine translation.
In Proceedings ofthe International Workshop on Spoken LanguageTranslation (IWSLT?10), Paris, France.Theodoros Evgeniou and Massimiliano Pontil.
2004.Regularized multi-task learning.
In Proceedings ofthe 10th ACM SIGKDD conference on knowledgediscovery and data mining (KDD?04), Seattle, WA.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: an open source toolkit forhandling large scale language models.
In Proceed-ings of Interspeech, Brisbane, Australia.Jenny Rose Finkel and Christopher D. Manning.
2009.Hierarchical bayesian domain adaptation.
In Pro-ceedings of the Conference of the North AmericanChapter of the Association for Computational Lin-guistics - Human Language Technologies (NAACL-HLT?09), Boulder, CO.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for SMT.
In Proceedings of theSecond Workshop on Statistical Machine Transla-tion, Prague, Czech Republic.George Foster, Pierre Isabelle, and Roland Kuhn.2010.
Translating structured documents.
In Pro-ceedings of the 9th Conference of the Associationfor Machine Translation in the Americas (AMTA2010), Denver, CO.Kenneth Heafield.
2011.
KenLM: faster and smallerlanguage model queries.
In Proceedings of theEMNLP 2011 Sixth Workshop on Statistical Ma-chine Translation (WMT?11), Edinburgh, UK.Daniel Kifer, Shain Ben-David, and Johannes Gehrke.2004.
Detecting change in data streams.
In Pro-ceedings of the 30th international conference onVery large data bases, Toronta, Ontario, Canada.Philipp Koehn and Josh Schroeder.
2007.
Experi-ments in domain adaptation for statistical machinetranslation.
In Proceedings of the Second Workshopon Statistical Machine Translation, Prague, CzechRepublic.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Birch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the ACL 2007 Demo and Poster Ses-sions, Prague, Czech Republic.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings ofMachine Translation Summit X, Phuket, Thailand.Percy Liang, Alexandre Bouchard-Co?te?, Dan Klein,and Ben Taskar.
2006.
An end-to-end dis-criminative approach to machine translation.
InProceedings of the joint conference of the Inter-national Committee on Computational Linguisticsand the Association for Computational Linguistics(COLING-ACL?06), Sydney, Australia.Guillaume Obozinski, Ben Taskar, and Michael I. Jor-dan.
2010.
Joint covariate selection and joint sub-space selection for multiple classification problems.Statistics and Computing, 20:231?252.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceed-ings of the Human Language Technology Confer-ence and the 3rd Meeting of the North AmericanChapter of the Association for Computational Lin-guistics (HLT-NAACL?03), Edmonton, Cananda.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2001.
Bleu: a method for auto-matic evaluation of machine translation.
TechnicalReport IBM Research Division Technical Report,RC22176 (W0190-022), Yorktown Heights, N.Y.827Ariadna Quattoni, Xavier Carreras, Michael Collins,and Trevor Darrell.
2009.
An efficient projec-tion for `1,?
regularization.
In Proceedings of the26th International Conference on Machine Learn-ing (ICML?09), Montreal, Canada.Stefan Riezler and John Maxwell.
2005.
On some pit-falls in automatic evaluation and significance testingfor MT.
In Proceedings of the ACL-05 Workshop onIntrinsic and Extrinsic Evaluation Measures for MTand/or Summarization, Ann Arbor, MI.Holger Schwenk.
2008.
Investigations on large-scale lightly-supervised training for statistical ma-chine translation.
In Proceedings of the Interna-tional Workshop on Spoken Language Translation(IWSLT?08), Hawaii.Patrick Simianer, Katharina Wa?schle, and Stefan Rie-zler.
2011.
Multi-task minimum error rate train-ing for SMT.
The Prague Bulletin of MathematicalLinguistics, 96:99?108.Matthew Snover, Bonnie Dorr, and Richard Schwartz.2008.
Language and translation model adaptationusing comparable corpora.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP?08), Honolulu, Hawaii.John Tinsley, Andy Way, and Paraic Sheridan.
2010.PLuTO: MT for online patent translation.
In Pro-ceedings of the 9th Conference of the Associationfor Machine Translation in the Americas (AMTA2010), Denver, CO.Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-niadou.
2009.
Stochastic gradient descent train-ing for `1-regularized log-linear models with cumu-lative penalty.
In Proceedings of the 47th AnnualMeeting of the Association for Computational Lin-guistics (ACL-IJCNLP?09), Singapore.Nicola Ueffing, Gholamreza Haffari, and AnoopSarkar.
2007.
Transductive learning for statisticalmachine translation.
In Proceedings of the 45th An-nual Meeting of the Association of ComputationalLinguistics (ACL?07), Prague, Czech Republic.Masao Utiyama and Hitoshi Isahara.
2007.
AJapanese-English patent parallel corpus.
In Pro-ceedings of MT Summit XI, Copenhagen, Denmark.Bing Zhao, Matthias Eck, and Stephan Vogel.
2004.Language model adaptation for statistical machinetranslation with structured query models.
In Pro-ceedings of the 20th International Conference onComputational Linguistics (COLING?04), Geneva,Switzerland.828
