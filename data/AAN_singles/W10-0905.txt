Proceedings of the NAACL HLT 2010 First International Workshop on Formalisms and Methodology for Learning by Reading, pages 34?42,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsMining Script-Like Structures from the WebNiels KaschDepartment of Computer Scienceand Electrical EngineeringUniversity of Maryland,Baltimore CountyBaltimore, MD 21250, USAnkasch1@umbc.eduTim OatesDepartment of Computer Scienceand Electrical EngineeringUniversity of Maryland,Baltimore CountyBaltimore, MD 21250, USAoates@cs.umbc.eduAbstractThis paper presents preliminary work to ex-tract script-like structures, called events andevent sets, from collections of web docu-ments.
Our approach, contrary to existingmethods, is topic-driven in the sense that eventsets are extracted for a specified topic.
Weintroduce an iterative system architecture andpresent methods to reduce noise problemswith web corpora.
Preliminary results showthat LSA-based event relatedness yields bet-ter event sets from web corpora than previousmethods.1 IntroductionIn this paper, we present a preliminary system to ex-tract script-like structures in a goal-directed fashionfrom the web.
For language processing purposes,humans appear to have knowledge of many stylizedsituations, such as what typically happens when go-ing to a restaurant or riding a bus.
This knowledgeis shared among a large part of the population andlets us predict the next step in a sequence in a fa-miliar situation, allows us to act appropriately, andenables us to omit details when communicating withothers while ensuring common ground is maintainedbetween communication partners.
It seems we havesuch knowledge for a vast variety of situations andscenarios, and thus natural language processing sys-tems need access to equivalent information if theyare to understand, converse, or reason about thesesituations.These knowledge structures, comparable toscripts (Schank and Abelson, 1977) or narrativechains (Chambers and Jurafsky, 2008), describe typ-ical sequences of events in a particular context.Given the number of potential scripts, their develop-ment by hand becomes a resource intensive process.In the past, some work has been devoted to automat-ically construct script-like structures from compiledcorpora (Fujiki et al, 2003) (Chambers and Juraf-sky, 2008).
Such approaches, however, only producescripts that are directly related to the topics repre-sented in such corpora.
Therefore, newspaper cor-pora (e.g., the Reuters Corpus) are likely to containscripts relating to government, crime and financials,but neglect other subject areas.
We present a systemthat extracts scripts from the web and removes theconstraints of specialized corpora and domain lim-itations.
We hope our iterative technique will pro-duce scripts for a vast variety of topics, and has thepotential to produce more complete scripts.Another drawback of existing approaches lieswith their passive extraction mechanisms.
Auser/system does not have the ability to obtainscripts for a specific topic, but rather is bound toobtain the most prevalent scripts for the underlyingcorpus.
Furthermore, scripts derived in this fashionlack an absolute labeling or description of their top-ics.
This can be problematic when a user/system islooking for specific scripts to apply to a given sce-nario.
In contrast, our system facilitates the searchfor scripts given a topic.
This goal oriented approachis superior in that (1) scripts are labeled by a de-scriptive topic and can be organized, accessed andsearched by topic, (2) scripts can be constructed bytopic and are not reliant on existing and potentiallylimiting corpora and (3) script coarseness and de-34tail can be be controlled through iterative script im-provement and augmentation based on additional in-formation retrieved from the web.2 Related WorkLin and Pantel describe an unsupervised algorithmfor discovering inference rules from text (DIRT)(Lin and Pantel, 2001a) (Lin and Pantel, 2001b).
In-ference rules are derived from paths in dependencytrees.
If two paths occur in similar contexts (i.e., thewords/fillers of their slots are distributionally simi-lar) then the meaning of the paths is similar.
Ver-bOcean (Chklovski and Pantel, 2004) is a resourceof strongly associated verb pairs and their semanticrelationship.
Verbs are considered strongly associ-ated if DIRT deems dependency paths, which con-tain the verbs, as being similar.
A form of mutualinformation between verb pairs and lexico-syntacticpatterns indicative of semantic relationship types isused to categorize the verb pairs according to sim-ilarity, strength, antonymy, happens-before and en-ablement.
(Fujiki et al, 2003) describe a method to ex-tract script knowledge from the first paragraph ofJapanese newspaper articles.
The first paragraph ofsuch articles is assumed to narrate its contents intemporal order.
This circumvents the need to orderevents as they can be extracted in presumed order.Events are defined in terms of actions, where an ac-tion consists of a tuple composed of a transitive verband its subject and object.
The method?s goal is tofind sequences of pairs of actions by (1) using co-occurrence of subjects and objects in neighboringsentences, (2) locating sentences where two verbsshare the same subject and (3) identifying sentenceswhere two verbs share the same object.
Once pairsof events are extracted, their subject and objects aregeneralized into semantic entities similar to seman-tic roles.
(Chambers and Jurafsky, 2008) attempt to identifynarrative chains in newspaper corpora.
They utilizethe notion of protagonist overlap or verbs sharingco-referring arguments to establish semantic coher-ence in a story.
Co-referring arguments are takenas indicators of a common discourse structure.
Thisassumption is used to find pairwise events in an un-supervised fashion.
Point wise mutual information(PMI) is used to indicate the relatedness betweenevent pairs.
A global narrative score, aiming to max-imize the PMI of a set of events is utilized to gener-ate a ranked list of events most likely to participatein the narrative chain.
Temporal order is establishedby labeling events with temporal attributes and us-ing those labels, along with other linguistic features,to classify the relationship (before or other) betweentwo events.For the purposes of our work, finding documentsrelated to a term and identifying similar terms is animportant step in the script creation process.
(Deer-wester et al, 1990) describe Latent Semantic Anal-ysis/Indexing (LSA) as a technique superior to termmatching document retrieval.
LSA aims to facili-tate document retrieval based on the conceptual con-tent of documents, thereby avoiding problems withsynonomy and polysemy of individual search terms(or in documents).
LSA employs singular-value-decomposition (SVD) of a term-by-document ma-trix to construct a ?semantic space?
in which relateddocuments and terms are clustered together.3 ApproachIn this work we aim to extract scripts from theweb.
We define a script as a collection of typi-cally related events that participate in temporal re-lationships amongst each other.
For example, e1happens-before e2 denotes a relationship suchthat event e1 occurs prior to event e2.
An event is de-fined as a tuple consisting of a verb, a grammaticalfunction and a set of arguments (i.e., words) whichact out the grammatical function in relation to theverb.
Figure 1 shows the structure of events.e [verb, grammatical function, {set of arguments}]Figure 1: The structure of an event.
An event is a tupleconsisting of a verb, a grammatical function and a set ofarguments (i.e., instances of words filling the grammati-cal function).The set of arguments represents actual instancesfound during the script extraction process.
Figure 2illustrates an incomplete script for the topic eatingat a restaurant.3.1 The TaskWe define the task of goal driven script extraction as:(1) given a topic, compile a ?relevant?
corpus of doc-35e1 [ enter, nsubj, {customer, John}) ]e2 [ enter, dobj, {restaurant} ]e3 [ order, nsubj, {customer} ]e4 [ order, dobj, {food} ]e5 [ eat, nsubj, {customer} ]e6 [ eat, dobj, {food} ]e7 [ pay, nsubj, {customer} ]e8 [ pay, dobj, {bill} ]e9 [ leave, nsubj, {customer} ]e10 [ leave, dobj, {restaurant} ]Temporal Ordering = e1 < e2 < e3 < e4e4 < e5 < e6 < e7 < e8 < e9 < e10Figure 2: An excerpt of a script for the topic eating ata restaurant.
The script denotes the stylized actions of acustomer dining at a restaurant.
The < relation denotesevent ei happens before event ej .uments from a subset of documents on the web, (2)extract events relevant for the topic, (3) (optional)refine the topic and restart at 1, and (4) establish atemporal ordering for the events.We currently impose restrictions on the form ofacceptable topics.
For our purposes, a topic is a shortdescription of a script, and contains at least a verband a noun from the script?s intended domain.
Forexample, the topic for a passenger?s typical actionswhile using public transportation (i.e.
a bus) can bedescribed by the topic riding on a bus.3.2 System ArchitectureThe script extraction system consists of a variety ofmodules where each module is responsible for a cer-tain task.
Modules are combined in a mixed fash-ion such that sequential processing is combined withan iterative improvement procedure.
Figure 3 illus-trates the system architecture and flow of informa-tion between modules.
The following sections de-scribe each module in detail.3.2.1 Document RetrievalOur system utilizes the web as its underlying in-formation source to circumvent domain limitationsof fixed corpora.
However, using the entire web toextract a script for a specific topic is, on one hand,infeasible due to the size of the web and, on theother hand, impractical in term of document rele-vancy.
Since only a subset of pages is potentiallyFigure 3: System Architecture and flow of information.relevant to a given topic, the web needs to be filteredsuch that mostly relevant web pages are retrieved.The document retrieval module makes use of exist-ing search engines for this purpose.1The document retrieval module is presented withthe topic for a script and issues this topic as a queryto the search engines.
The search engines produce arelevancy ranked list of documents/URLs (Brin andPage, 1998) which, in turn, are downloaded.
Thenumber of downloaded pages depends on the cur-rent iteration number of the system (i.e., how oftenthe retrieval-analysis cycle has been executed for agiven topic2).The document retrieval module is also responsi-ble for cleaning the documents.
The cleaning pro-cess aims to remove ?boilerplate?
elements such asnavigational menus and advertising from web pageswhile preserving content elements.3 The collectionof cleaned documents for a given topic is consideredto be a topic-specific corpus.3.2.2 Latent Semantic Analysis (LSA)The aim of the LSA module is to identify words(verbs, nouns and adjectives) that are closely related1The Google and Yahoo API?s are used to establish commu-nication with these search engines.2At the first iteration, we have arbitrarily choosesn to re-trieve the first 1000 unduplicated documents.3The Special Interest Group of the ACL on Web as Cor-pus (SIGWAC) is interested in web cleaning methods for corpusconstruction.
Our web page cleaner uses a support vector ma-chine to classify blocks of a web page as content or non-content.The cleaner achieves ?
85% F1 on a random set of web pages.36to the topic presented to the document retrieval mod-ule.
To find such words, the topic-specific corpus is(1) part-of-speech tagged and (2) transformed intoa term-document matrix.
Each cell represents thelog-entropy for its respective term in a document.Note that we consider a term to be a tuple consist-ing of a word and its POS.
The advantage of us-ing word-POS tag combinations over words only isthe ability to query LSA?s concept space for wordsby their word class.
A concept space is created byapplying SVD to the term-document matrix, reduc-ing the dimensionality of the scaling matrix and re-constructing the term-document matrix using the re-duced scaling matrix.Once the concept space is constructed, the spaceis searched for all terms having a high correlationwith the original topic.
Terms from the original topicare located in the concept space (i.e., term vectorsare located) and other term vectors with high co-sine similarity are retrieved from the space.
A list ofn = 50 terms4 for each word class ?
{verb, noun,adjective} is obtained and filtered using a stop list.The stop list currently contains the 100 most com-mon words in the English language.
The idea behindthe stop list is to remove low content words from thelist.
The resulting set of words is deemed to havehigh information content with respect to the topic.This set is used for two purposes: (1) to augmentthe original topic and to restart the document col-lection process using the augmented topic and (2)identify event pairs constructed by the event findingmodule which contain these highly correlated terms(either as events or event arguments).
The first pur-pose aims to use an iterative process to construct ahigher quality topic-specific corpus.
A new corpuscreated in this fashion presumably represents docu-ments that are richer and more relevant to the aug-mented topic.
The second purpose steers the extrac-tion of events towards events containing those con-stituents judged most relevant.
This fact can be in-corporated into a maximization calculation based onpointwise mutual information to find highly corre-lated events.4The number was chosen experimentally and is based onthe correlation score (cosine similarity) between word vectors.After about 50 words, the correlation score begins to drop sig-nificantly indicating weaker relatedness.3.2.3 Event FindingThe collection of documents (or topic-specificcorpus) is then processed to facilitate finding eventpairs.
Finding event pairs involves the notion ofverb argument overlap using the assumption that twoevents in a story are related if they share at leastone semantic argument across grammatical func-tions.
This virtue of discourse structure of coherentstories has been described in (Trabasso et al, 1984)and applied by (Fujiki et al, 2003) as subject and ob-ject overlap and by (Chambers and Jurafsky, 2008)as following a common protagonist in a story.
Forexample, in the sentences ?John ordered a drink.
Heenjoyed it very much.?
we can establish that eventsorder and enjoy are part of a common theme becausethe arguments (or loosely semantic roles) of orderand enjoy refer to the same entities, that is John =He and a drink = it.Figure 4: Example processing of the sentences ?Yester-day, Joe ordered coffee.
It was so hot, he couldn?t drinkit right away?.
The output after dependency parsing, ref-erence resolution and event finding is a set of event pairs.37To identify such pairs, the topic specific corpusis (1) co-reference resolved5 and (2) dependencyparsed6.
Sentences containing elements referringto the same mention of an element are inspectedfor verb argument overlap.
Figure 4 illustrates thisprocedure for the sentences ?Yesterday, Joe orderedcoffee.
It was so hot, he couldn?t drink it rightaway?.Co-reference resolution tells us that mention herefers to Joe and mention(s) it refer to coffee.
Byour previous assumption of discourse coherence, itis possible to deduce that events was and drink areassociated with event order.
In a similar fashion,event drink is associated with event was.
This isdue to the fact that all events share at least one ar-gument (in the case of events order and drink twoarguments are shared).
For each pair of events shar-ing arguments in a particular grammatic function, anevent pair is generated indicating where the overlapoccurred.3.2.4 Constructing Event SetsSets of events representing script-like structuresare constructed through the use of pointwise mutualinformation in combination with the lists of relatedwords found by Latent Semantic Analysis.
We uti-lize the definition of PMI described in (Chambersand Jurafsky, 2008).
For two events e1 and e2pmi(e1, e2) = logP (e1, e2)P (e1)P (e2)(1)whereP (e1, e2) =C(e1, e2)?i,j C(ei, ej)(2)and C(e1, e2) is the number of times events e1 ande2 had coreferring arguments.We extend the definition of PMI between events toassign more weight to events whose constituents arecontained in the list of words (verbs, nouns and ad-jectives) judged by Latent Semantic Analysis to bemost relevant to the topic.
For notational purposes,these lists are denoted L. Thus, we can calculate theweighted LSA PMI LP (e1, e2) as follows:LP (e1, e2) = P (e1, e2) + LSA(e1, e2) (3)5OpenNLP?s Co-Reference Engine is utilized http://opennlp.sourceforge.net/.6The Stanford Parser is utilized http://nlp.stanford.edu/software/lex-parser.shtmlwhereLSA(e1, e2) = ?
?
(E(e1) + E(e2)) (4)?
={2 if e1verb ?
L and e2verb ?
L1 otherwise(5)E(e) = (||everb ?
L|| + 1)?
( ||eArgs?L||||eArgs|| + 1)(6)To construct the set of n events related to thetopic, the LP scores are first calculated for eachevent pair in the corpus.
The set can then be con-structed by maximizing:maxi<k?nk?1?i=0LP (ei, ek) (7)Therefore, events that share a larger number ofconstituents with the LSA relevancy list are pre-ferred for inclusion in the event set.
This prac-tice distributes the relatedness weight among the fre-quency of events and LSA.
The noisy nature of ourproposed corpus generation method makes such atechnique essential as we will see in section 4.3.3.2.5 Ordering EventsAt this time, we only establish a naive temporalordering on the events.
The ordering process simplyassumes that an event appearing in the corpus priorto another event also occurs earlier in time.
We re-alize that this assumption does not always hold, butdelay a more sophisticated ordering process as fu-ture work.4 ExperimentsThis section describes experimental results, obsta-cles we have encountered, various approaches toovercome these obstacles and lessons learned fromour work.
Unless mentioned otherwise, the resultspertain to the topic eating at a restaurant.
This topichas been chosen for our investigation since previ-ous work (Schank and Abelson, 1977) establishes acomprehensive reference as to what a script for thisdomain may entail.4.1 Domain RichnessThe first step in our work was to confirm the no-tion that the web can be used as the underlying in-formation source for topic-specific script extraction.38The overall goal was to investigate whether a topic-specific corpus contains sufficiently useful informa-tion which is conducive to the script extraction task.Latent Semantic Analysis was performed on thePart-of-Speech tagged topic specific corpus.
Thesemantic space was queried using the main con-stituents of the original topic.
Hence, this resulted intwo queries, namely eating and restaurant.
For eachquery, we identified the most related verbs, nounsand adjectives/adverbs and placed them in respec-tive lists.
Lists are then combined according to wordclass, lemmatized, and pruned.
Auxiliary verbs suchas be, do and have consistently rank in the top 10most similar words in the un-pruned lists.
This re-sult is expected due to the frequency distributionof auxiliaries in the English language.
It is a nat-ural conclusion to exclude auxiliaries from furtherconsideration since their information content is rela-tively low.
Furthermore, we extend this notion to ex-clude the 100 most frequent words in English fromthese lists using the same justification.
By the in-verse reasoning, it is desirable to include words infurther processing that occur infrequently in naturallanguage.
We can hypothesize that such words aresignificant to a given script because their frequencyappears to be elevated in the corpus.
Table 1 (left)shows the resulting word class lists for both queries.Duplicates (i.e., words with identical lemma) havebeen removed.The table reveals that some words also appearin the restaurant script as suggested by (Schankand Abelson, 1977).
In particular, bold verbs re-semble Schank?s scenes and bold nouns resemblehis props.
We can also see that the list of ad-verbs/adjectives appear to not contribute any signif-icant information.
Note that any bold words havebeen hand selected using a human selector?s subjec-tive experience about the eating at a restaurant do-main.
Furthermore, while some script informationappears to be encoded in these lists, there is a signif-icant amount of noise, i.e., normal font words thatare seemingly unimportant to the script at hand.For our purposes, we aim to model this noise sothat it can be reduced or removed to some degree.Such a model is based on the notion of overlap ofnoisy terms in the LSA lists derived from indepen-dent topic related corpora for the main constituentsof the original topic.
For example, for the topic eat-eating at a restaurant Overlap removedVerbs Nouns A&A Verbs Nouns A&Akeep home own order home fryneed place still set hand amazehelp table last expect bowl greendine lot open share plate grilllove part full drink cook chainorder hand off try fish dietfeel reason long cut soup cleanavoid course fat decide service smartadd side right watch break totallet number down process drink relatestay experience busy save cheese worstinclude water fast offer rice blacktend point single provide serve fitset dish low hear chance lighttell bowl free fill portion existfound plate white forget body emptybring bite wrong write party livelocate cook ready follow resteat fish true travel creamleave soup close tasteTable 1: (Left) 20 most relevant terms (after pruning)for LSA queries eating and restaurant on the eating ata restaurant corpus.
(Right) Terms remaining after noisemodeling and overlap removal.
Bold terms in the tablewere manually judged by a human to be relevant.ing at a restaurant, we obtain two additional corporausing the method described in Section 3.2.1, i.e., onecorpus for constituent eating and another for the sec-ond main constituent of the original topic, restau-rant.
Both corpora are subjected to LSA analysisfrom which two (one for each corpus) LSA wordlists are obtained.
Each list was created using therespective corpus query as the LSA query.
The as-sumption is made that words which are shared (pair-wise) between all three lists (i.e., the two new LSAlists and the LSA list for topic eating at a restaurant)are noisy due to the fact that they occur independentof the original topic.Table 1 (right) illustrates the LSA list for topiceating at a restaurant after removing overlappingterms with the two other LSA lists.
Bold words werejudged by a human selector to be relevant to the in-tended script.
From the table we can observe that:1.
A significant amount of words have been re-moved.
The original table contains 50 wordsfor each word class.
The overlap reduced tablecontains only 19 verbs, 29 nouns, and 17 adjec-tives, a reduction of what we consider noise by39?
57%2.
More words (bold) were judged to be related tothe script (e.g., 6 vs. 5 relevant verbs, 12 vs. 9nouns, and 3 vs. 0 adjectives/adverbs)3.
More relevant words appear in the top part ofthe list (words in the list are ordered by rele-vancy)4.
Some words judged to be relevant were re-moved (e.g., dine, bring, eat).Using the information from the table (left andright) and personal knowledge about eating at arestaurant, a human could re-arrange the verbs andnouns into a partial script-like format of the form7:e1 [ offer, nsubj, {waiter}) ]e2 [ offer, dobj, {drink}) ]Example: waiter offers a drinke2 [ order, nsubj, {customer} ]e3 [ order, dobj, {fish, soup, rice} ]Example: customer orders fishe4 [ serve/bring, nsubj, {waiter} ]e5 [ serve/bring, nsubj, {bowl, plate} ]Example: waiter serves/bings the bowl, platee6 [ eat, nsubj, {customer} ]e7 [ eat, dobj, {portion, cheese} ]Example: customer eats the portion, cheesee8 [ leave, nsubj, {customer} ]e9 [ leave, dobj, {table} ]Example: customer leaves the tableNote that this script-like information was not ob-tained by direct derivation from the information inthe table.
It is merely an illustration that some scriptinformation is revealed by LSA.
Table 1 neither im-plies any ordering nor suggest semantic argumentsfor a verb.
However, the analysis confirms that theweb contains information that can be used in thescript extraction process.4.2 Processing ErrorsAs mentioned in section 3.2.3, events with co-referring arguments are extracted in a pairwise fash-ion.
In the following section we describe observa-tions about the characteristics of events extracted7Bold terms do not appear in the LSA lists, but were addedfor readability.this way.
However, we note that each step in oursystem architecture is imperfect, meaning that er-rors are introduced in each module as the result ofprocessing.
We have already seen such errors inthe form of words with incorrect word class in theLSA lists as the result of incorrect POS tagging.Such errors are amplified through imprecise pars-ing (syntactic and dependency parsing).
Other er-rors, such as omissions, false positives and incor-rect class detection, are introduced by the named en-tity recognizer and the co-reference module.
Withthis in mind, it comes as no surprise that some ex-tracted events, as seen later, are malformed.
Forexample, human analysis reveals that the verb slotof these events are sometimes ?littered?
with wordsfrom other word classes, or that the arguments of averb were incorrectly detected.
A majority of theseerrors can be attributed to ungrammatical sentencesand phrases in the topic-specific corpus, the remain-der is due to the current state of the art of the parsersand reference resolution engine.4.3 Observations about eventsTo compare relations between events, we looked atthree different metrics.
The first metric M1 simplyobserves the frequency counts of pairwise events inthe corpus.
The second metric M2 utilizes pointwise mutual information as defined in (Chambersand Jurafsky, 2008).
The third metric M3 is ourLSA based PMI calculation as defined in section3.2.4.M1 reveals that uninformative event pairs tendto have a high number of occurrences.
These pairsare composed of low content, frequently occurringevents.
For example, event pair [e [ have, nsubj,{} ],e [ say, nsubj, {} ]] occurred 123 times in our topic-specific corpus.
More sophisticated metrics, such asM2, consider the frequency distributions of individ-ual events and allocate more weight to co-occurringevents with lower frequency counts of their individ-ual events.In this fashion, M2 is capable of identifyingstrongly related events.
For example, Table 2 liststhe five pairwise events with highest PMI for ourtopic-specific corpus.From Table 2, it is apparent that these pairs partic-ipate in mostly meaningful (in terms of human com-prehensibility) relationships.
For example, it does40Event Pairse[sack, dobj, {the, employees}] e[reassign, dobj, {them}]e[identify, nsubj, {we, Willett}] e[assert, nsubj, {Willett}]e[pour, dobj, {a sweet sauce}] e[slide, dobj, {the eggs}]e[walk, nsubj, {you, his sister}] e[fell, nsubj, {Daniel}]e[use, nsubj, {the menu}] e[access, dobj, {you}]Table 2: Pairwise events with highest PMI according toEquation 1.not require a leap of faith to connect that sackingemployees is related to reassigning them in the con-text of a corporate environment.e(eat, nsubj), e(gobble, nsubj), e(give, nsubj),e(live, nsubj), e(know, nsubj), e(go, nsubj),e(need, nsubj), e(buy, nsubj), e(have, nsubj),e(make, nsubj), e(say, nsubj), e(work, nsubj),e(try, nsubj), e(like, nsubj), e(tell, dobj),e(begin, nsubj), e(think, nsubj), e(tailor, nsubj),e(take, nsubj), e(open, nsubj),e(be, nsubj)Figure 5: An event set obtained through metric M2 (us-ing the PMI between events).
Temporal ordering is notimplied.
Event arguments are omitted.
Bold events indi-cate subjectively judged strong relatedness to the eatingat a restaurant topic.Figure 5 shows a set of events for our topic.
Theset was created by greedily adding event en suchthat for all events e1, e2, ...en?1 already in the set?n?1i pmi(ei, en) is largest (see (Chambers and Ju-rafsky, 2008)).
The topic constituent eating (i.e.,eat) was used as the initial event in the set.
If thisset is intended to approximate a script for the eatingat a restaurant domain, then it is easy to see that vir-tually no information from Schank?s restaurant ref-erence script is represented.
Furthermore, by humanstandards, the presented set appears to be incoherent.From this observation, we can conclude that M2 isunsuitable for topic-specific script extraction.Figure 6 illustrates an event set constructed usingmetric M3.
Note that the sets in Figures 5 and 6do not imply any ordering on the events.
The boldevents indicate events that appear in our referencescript or were judged by a human evaluator to belogically coherent with the eating at a restaurant do-main.
The evaluation was conducted using the eval-uators personal experience of the domain.
In the fu-ture, we intend to formalize this evaluation process.Figure 6 signifies an improvement of the results ofe(eat, nsubj), e(wait, dobj), e(total, nsubj)e(write, dobj), e(place, dobj), e(complete, dobj)e(exist, nsubj), e(include, dobj), e(top, nsubj)e(found, dobj), e(keep, dobj), e(open, dobj)e(offer, dobj), e(average, nsubj), e(fill, dobj)e(taste, nsubj), e(drink, dobj), e(cook, dobj)e(read, dobj), e(enjoy, dobj),e(buy, dobj)Figure 6: An event set obtained through metric M3(weighing PMI and LSA between events).
Temporal or-dering is not implied.
Event arguments are omitted.
Boldevents indicate subjectively judged strong relatedness tothe eating at a restaurant topic.Figure 5 in terms of the number of events judged tobelong to the restaurant script.
This leads us to theconclusion that a metric based on scaled PMI andLSA appears more suitable for the web based topicdriven script extraction task.
Once a temporal orderis imposed on the events in Figure 6, these eventscould, by themselves, serve as a partial event set fortheir domain.5 Discussion and Future WorkWe have presented preliminary work on extractingscript-like information in a topic driven fashion fromthe web.
Our work shows promise to identify scriptknowledge in a topic-specific corpus derived froman unordered collection of web pages.
We haveshown that while web documents contain signifi-cant amounts of noise (both boilerplate elements andtopic unrelated content), a subset of content can beidentified as script-like knowledge.Latent Semantic Analysis allows for the filter-ing and pruning of lists of related words by wordclasses.
LSA furthermore facilitates noise removalthrough overlap detection of word class list elementsbetween independent corpora of topic constituents.Our method of weighted LSA and PMI for event re-latedness produces more promising partial event setsthan existing metrics.For future work, we leave the automated evalua-tion of partial sets and the establishing of temporalrelations between events in a set.
Our system archi-tecture features an iterative model to event set im-provement.
We hope that this approach will allowus to improve upon the quality of event sets by us-ing extracted sets from one iteration to bootstrap anew iteration of event extraction.41ReferencesSergey Brin and Lawrence Page.
1998.
The anatomy of alarge-scale hypertextual web search engine.
Comput.Netw.
ISDN Syst., 30(1-7):107?117.Nathanael Chambers and Dan Jurafsky.
2008.
Unsu-pervised learning of narrative event chains.
In Pro-ceedings of ACL-08: HLT, pages 789?797, Columbus,Ohio, June.
Association for Computational Linguis-tics.Timothy Chklovski and Patrick Pantel.
2004.
VerbO-cean: Mining the Web for Fine-Grained Semantic VerbRelations.
In Proceedings of Conference on EmpiricalMethods in Natural Language Processing, pages 33?40, Barcelona, Spain, July.
Association for Computa-tional Linguistics.Scott Deerwester, T. Susan Dumais, W. George Furnas,K.
Thomas Landauer, and Richard Harshman.
1990.Indexing by Latent Semantic Analysis.
Journal of theAmerican Society for Information Science, 41:391?407.Toshiaki Fujiki, Hidetsugu Nanba, and Manabu Oku-mura.
2003.
Automatic acquisition of script knowl-edge from a text collection.
In EACL ?03: Proceedingsof the tenth conference on European chapter of the As-sociation for Computational Linguistics, pages 91?94,Morristown, NJ, USA.
Association for ComputationalLinguistics.Dekang Lin and Patrick Pantel.
2001a.
DIRT - Discov-ery of Inference Rules from Text.
In KDD ?01: Pro-ceedings of the seventh ACM SIGKDD internationalconference on Knowledge discovery and data mining,pages 323?328, New York, NY, USA.
ACM.Dekang Lin and Patrick Pantel.
2001b.
Discovery ofInference Rules for Question-Answering.
Nat.
Lang.Eng., 7(4):343?360.Roger C. Schank and Robert P. Abelson.
1977.
Scripts,Plans, Goals and Understanding.
Lawrence ErlbaumAssociates, Hillsdale, NJ.Tom Trabasso, T. Secco, and Paul van den Broek.
1984.Causal Cohesion and Story Coherence.
In HeinzMandl, N.L.
Stein and Tom Trabasso (eds).
Learningand Comprehension of Text., pages 83?111, Hillsdale,NJ, USA.
Lawrence Erlbaum Associates.42
