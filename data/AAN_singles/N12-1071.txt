2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 587?591,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsPortable Features for Classifying Emotional TextSaif MohammadNational Research Council CanadaOttawa, Canada, K1A 0R6saif.mohammad@nrc-cnrc.gc.caAbstractAre word-level affect lexicons useful in de-tecting emotions at sentence level?
Some priorresearch finds no gain over and above what isobtained with ngram features?arguably themost widely used features in text classifica-tion.
Here, we experiment with two very dif-ferent emotion lexicons and show that even insupervised settings, an affect lexicon can pro-vide significant gains.
We further show thatwhile ngram features tend to be accurate, theyare often unsuitable for use in new domains.On the other hand, affect lexicon features tendto generalize and produce better results thanngrams when applied to a new domain.1 IntroductionAutomatically identifying emotions expressed intext has a number of applications, including track-ing customer satisfaction (Bougie et al, 2003), de-termining popularity of politicians and governmentpolicies (Mohammad and Yang, 2011), depressiondetection (Osgood and Walker, 1959; Pestian etal., 2008; Matykiewicz et al, 2009; Cherry et al,2012), affect-based search (Mohammad, 2011), andimproving human-computer interaction (Vela?squez,1997; Ravaja et al, 2006).Supervised methods for classifying emotions ex-pressed in a sentence tend to perform better than un-supervised ones.
They use features such as unigramsand bigrams (Alm et al, 2005; Aman and Szpakow-icz, 2007; Neviarouskaya et al, 2009; Chaffar andInkpen, 2011).
For example, a system can learn thatthe word excruciating tends to occur in sentences la-beled with sadness, and use this word as a feature inclassifying new sentences.Approaches that do not rely on supervised train-ing with sentence-level annotations often use affectlexicons.
An affect lexicon, in its simplest form, isa list of words and associated emotions and senti-ments.
For example, the word excruciating may beassociated with the emotions of sadness and fear.Note that such lexicons are at best indicators ofprobable emotions, and that in any given sentence,the full context may suggest that a completely differ-ent emotion is being expressed.
Therefore, it is un-clear how useful such word-level emotion lexiconsare for detecting emotions and meanings expressedin sentences, especially since supervised systems re-lying on tens of thousands of unigrams and bigramscan produce results that are hard to surpass.
For ex-ample, it is possible that classifiers can learn fromunigram features alone that excruciating is associ-ated with sadness and fear.In this paper, we investigate whether word?emotion association lexicons can provide gains inaddition to those already provided by ngram fea-tures.
We conduct experiments with different affectlexicons and determine their usefulness in this ex-trinsic task.
We also conduct experiments to deter-mine how portable the ngram features and the emo-tion lexicon features are to a new domain.2 Affect LexiconsThe WordNet Affect Lexicon (Strapparava and Val-itutti, 2004) has a few thousand words annotatedfor associations with a number of affect categories.This includes 1536 words annotated for associations587with six emotions considered to be the most basic?joy, sadness, fear, disgust, anger, and surprise (Ek-man, 1992).1 It was created by manually identifyingthe emotions of a few seed words and then label-ing all their WordNet synonyms with the same emo-tion.
Affective Norms for English Words has plea-sure (happy?unhappy), arousal (excited?calm), anddominance (controlled?in control) ratings for 1034words.2 Mohammad and Turney (2010; 2012) com-piled manual annotations for eight emotions (the sixof Ekman, plus trust and anticipation) as well asfor positive and negative sentiment.3 The lexiconwas created by crowdsourcing to Mechanical Turk.This lexicon, referred to as the NRC word-emotionlexicon (NRC-10) version 0.91, has annotations forabout 14,000 words.4We evaluate the affect lexicons that have annota-tions for the Ekman emotions?the WordNet AffectLexicon and the NRC-10.
We also experimentedwith a subset of NRC-10, which we will call NRC-6, that has annotations for only the six Ekman emo-tions (no trust and anticipation annotations; and nopositive and negative sentiment annotations).3 Sentence Classification SystemWe created binary classifiers for each of the sixemotions using Weka (Hall et al, 2009).5 Forexample, the Fear?NotFear classifier determinedwhether a sentence expressed fear or not.
We exper-imented with Logistic Regression (le Cessie and vanHouwelingen, 1992) and Support Vector Machines(SVM).
We used binary features that captured thepresence or absence of unigrams and bigrams.
Wealso used integer-valued affect features that capturedthe number of word tokens in a sentence associatedwith different affect labels in the affect lexicon be-ing used.6 For example, if a sentence has two joywords and one surprise word, then the joy featurehas value 2, surprise has value 1, and all remainingaffect labels have value 0.1http://wndomains.fbk.eu/wnaffect.html2http://csea.phhp.ufl.edu/media/anewmessage.html3Plutchik (1985) proposed a model of 8 basic emotions.4Please send an email to the author to obtain a copy of theNRC emotion lexicon.
Details of the lexicon are available at:http://www.purl.org/net/saif.mohammad/research5http://www.cs.waikato.ac.nz/ml/weka6Normalizing by sentence length did not give better results.# of % ofemotion instances instances ranger 132 13.2 0.50disgust 43 4.3 0.45fear 247 24.7 0.64joy 344 34.4 0.60sadness 283 28.3 0.68surprise 253 25.3 0.36simple average 0.54frequency-based average 0.43Table 1: Inter-annotator agreement (Pearson?s correla-tion) amongst 6 annotators on the 1000-headlines dataset.3.1 Training and Testing within domainAs a source of labeled data for training and testing,we used the SemEval-2007 Affective Text corpuswherein newspaper headlines were labeled with thesix Ekman emotions by six annotators (Strapparavaand Mihalcea, 2007).
For each headline?emotionpair, the annotators gave scores from 0 to 100 indi-cating how strongly the headline expressed the emo-tion.
The inter-annotator agreement as determinedby calculating the Pearson?s product moment corre-lation (r) between the scores given by each anno-tator and the average of the other five annotators isshown in Table 1.
For our experiments, we consid-ered scores greater than 25 to indicate that the head-line expresses the corresponding emotion.The dataset was created for an unsupervised com-petition, and consisted of 250 sentences of trial dataand 1000 sentences of test data.
We will refer tothem as the 250-headlines and the 1000-headlinesdatasets respectively.
In order to use these datasetsin a supervised framework, we follow Chaffar andInkpen (2011) and report results under two settings:(1) ten-fold cross-validation on the 1000-headlinesand (2) using the 1000-headlines as training data andtesting on the 250-headlines dataset.Table 2 shows results obtained by classifiers whentrained on the 1000-headlines text and tested onthe 250-headlines text.
The rows under I give abreakdown of results obtained by the EmotionX?NotEmotionX classifiers when using both n-gramand NRC-10 affect features (where X is one of thesix Ekman emotions).
gold is the number of head-lines expressing a particular emotion X .
right isthe number of instances that the classifier correctly588Classifier gold right guess P R FI.
Using affect and ngram features:a. NRC-10, unigrams, bigramsanger 66 23 55 41.8 34.8 38.0disgust 52 8 17 47.1 15.4 23.2fear 74 59 100 59.0 79.7 67.8joy 77 52 102 51.0 67.5 58.1sadness 105 71 108 65.7 67.6 66.7surprise 43 14 67 20.9 32.6 25.4ALL 417 227 449 50.6 54.4 52.4b.
NRC-6, unigrams, bigramsALL 417 219 437 50.1 52.5 51.3c.
WordNet Affect, unigrams, bigramsALL 417 212 490 43.3 50.8 46.7II.
Using affect features only:a. NRC-10ALL 417 282 810 34.8 67.6 46.0b.
NRC-6ALL 417 243 715 34.0 58.3 42.9c.
WordNet AffectALL 417 409 1435 28.5 98.0 44.1III.
Using ngrams features only:ALL 417 210 486 43.2 50.4 46.5IV.
Random guessing:ALL 417 208 750 27.8 50.0 35.7Table 2: Results on the 250-headlines dataset.marked as expressing X .
guess is the number ofinstances marked as expressing X by the classifier.Precision (P ) and recall (R) are calculated as shownbelow:P =rightguesses?
100 (1)R =rightgold?
100 (2)F is the balanced F-score.
The ALL row shows thesums of values for all six emotions for the gold,right, and guess columns.
The overall precisionand recall are calculated by plugging these values inequations 1 and 2.
Thus 52.4 is the macro-averageF-score obtained by the I.a.
classifiers.I.b.
and I.c.
show results obtained using ngramswith NRC-6 and WordNet Affect features respec-tively.
We do not show a breakdown of results byemotions for them and for the rows in II, III, and IVdue to space constraints.The rows in II correspond to the use of differentaffect features alone (no ngrams).
III shows the re-Classifier P R FI.
Using affect and ngram features:a. NRC-10, ngrams 44.4 61.8 51.6b.
NRC-6, ngrams 42.7 61.4 50.4c.
WA, ngrams 41.9 58.8 49.0II.
Using affect features only:a. NRC-10 24.1 95.0 38.4b.
NRC-6 24.1 95.0 38.4c.
WA 23.5 95.4 37.7III.
Using ngrams only: 42.0 59.8 49.3IV.
Random guessing: 21.7 50.0 30.3Table 3: Cross-validation results on 1000-headlines.sults obtained using only ngrams, and IV shows theresults obtained by a system that guesses randomly.7Table 3 gives results obtained by cross-validationon the 1000-headlines dataset.
The results in Tables2 and 3 lead to the following observations:?
On both datasets, using the NRC-10 in additionto the ngram features gives significantly higherscores than using ngrams alone.
This was nottrue, however, for WordNet affect.?
Using NRC-10 alone obtains almost as goodscores as those obtained by the ngrams in the250-headlines test data, even though the num-ber of affect features (10) is much smaller thanthe ngram features (many thousands).?
Using annotations for all ten affect labels inNRC-10 instead of just the Ekman six gives mi-nor improvements.?
The automatic methods perform best forclasses with the high inter-annotator agreement(sadness and fear), and worst for classes withthe low agreement (surprise and disgust) (Ta-ble 1).We used the Fisher Exact Test and a confidence in-terval of 95% for all precision and recall significancetesting reported in this paper.
Experiments with sup-port vector machines gave slightly lower F-scoresthan those obtained with logistic regression, but allof the above observations held true even in those ex-periments (we do not show those results here due tothe limited space available).7A system that randomly guesses whether an instance is ex-pressing an emotion X or not will get half of the gold instancesright.
Further, it will mark half of all the instances as expressingemotion X .
For ALL, right = gold2 , and guess =instances?62 .589Emotions: anger 3.47 joy -0.25anticipn 0.08 sadness -0.51disgust 0.97 surprise -1.87fear 0.25 trust 0.12Sentiment: negative 2.38 positive -0.31Table 4: The coefficients of the features learned by logis-tic regression for the Anger?NoAnger classifier.The coefficients of the features learned by the lo-gistic regression algorithm are weights that indicatehow strongly the individual features influence thedecision of the classifier.
The affect features of theAnger?NoAnger classifier learned from the 1000-sentences dataset and NRC-10 are shown in Table 4.We see that the anger feature has the highest weightand plays the biggest role in predicting whether asentence expresses anger or not.
The negative sen-timent feature is also a strong indicator of anger.Similarly, the weights for other emotion classifierswere consistent with our intuition: joy had the high-est weight in the Joy?NotJoy classifier, sadness inthe Sadness?NotSadness classifier, and so on.3.2 Testing on data from another domainHand-labeled training data is helpful for automaticclassifiers, but it is usually not available for most do-mains.
We now describe experiments to determinehow well the classifiers and features cope with train-ing on data from one source domain and testing on anew target domain.
We will use the 1000-headlinesdataset from the previous section as the source do-main training data.
As test data we will now use sen-tences compiled by Aman and Szpakowicz (2007)from blogs.
This dataset has 4090 sentences anno-tated with the Ekman emotions by four annotators.The inter-annotator agreement for the different emo-tions ranged from 0.6 to 0.8 Cohen?s kappa.Table 5 shows the results.
Observe that now thengrams perform quite poorly; the NRC-10 affectfeatures perform significantly better, despite eachsentence being represented by only ten features.
Therows in II give a breakdown of results obtained byindividual EmotionX?NotEmotionX classifiers.
Ob-serve that the distribution of instances in this blogdataset (gold column) is different from that in the1000-headlines (Table 1).
The larger proportion ofneutral instances in the blog data compared to 1000-headlines, leads to a much lower precision and F-Classifier gold right guess P R FI.
Using affect (NRC-10) and ngram features:ALL 1290 515 6717 7.7 39.9 12.9II.
Using affect (NRC-10) features only:anger 179 22 70 31.4 12.3 17.7disgust 172 16 48 33.3 9.3 14.5fear 115 32 110 29.1 27.8 28.4joy 536 299 838 35.7 55.8 43.5sadness 173 61 282 21.6 35.3 26.8surprise 115 9 158 5.7 7.8 6.6ALL 1290 439 1506 29.2 34.0 31.4III.
Using ngram features only:ALL 1290 375 7414 5.1 29.1 8.6IV.
Random guessing:ALL 1290 645 12270 5.3 50.0 9.6Table 5: Results obtained on the blog dataset.score of the randomly-guessing classifier on the blogdataset (row IV) than on the 1000-headlines dataset.Nonetheless, the NRC-10 affect features obtainsignificantly higher results than the random base-line.
The ngram features (row III), on the otherhand, lead to scores lower than the random base-line.
This suggests that they are especially domain-sensitive.
Manual inspection of the regression coef-ficients confirms the over-fitting of ngram features.The overfitting is less for affect features, probablybecause of the small number of features.4 ConclusionsEven though context plays a significant role in themeaning and emotion conveyed by a word, weshowed that using word-level affect lexicons canprovide significant improvements in sentence-levelemotion classification?over and above those ob-tained by unigrams and bigrams alone.
The gainsprovided by the lexicons may be correlated withtheir sizes.
The NRC lexicon has fourteen times asmany entries as the WordNet Affect lexicon and itgives significantly better results.We also showed that ngram features tend tobe markedly domain-specific and work well onlywithin domains.
On the other hand, affect lexiconfeatures worked significantly better than ngram fea-tures when applied to a new domain for which therewas no training data.AcknowledgmentsWe thank Colin Cherry, Peter Turney, and Tara Small.590ReferencesC.
Alm, D. Roth, and R. Sproat.
2005.
Emotions fromtext: Machine learning for text-based emotion predic-tion.
In Proceedings of HLT?EMNLP, Vancouver.S.
Aman and S. Szpakowicz.
2007.
Identifying expres-sions of emotion in text.
In Text, Speech and Dialogue,volume 4629, pages 196?205.
Springer.J.
R. G. Bougie, R. Pieters, and M. Zeelenberg.
2003.Angry customers don?t come back, they get back: Theexperience and behavioral implications of anger anddissatisfaction in services.
Open access publicationsfrom tilburg university, Tilburg University.S.
Chaffar and D. Inkpen.
2011.
Using a heterogeneousdataset for emotion analysis in text.
In Canadian Con-ference on AI, pages 62?67.C.
Cherry, S. M. Mohammad, and B de Bruijn.
2012.Binary classifiers and latent sequence models for emo-tion detection in suicide notes.
Biomedical Informat-ics Insights, 5:147?154.P.
Ekman.
1992.
An argument for basic emotions.
Cog-nition and Emotion, 6(3):169?200.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I. Witten.
2009.
The WEKA data miningsoftware: an update.
SIGKDD, 11:10?18.S.
le Cessie and J. van Houwelingen.
1992.
Ridgeestimators in logistic regression.
Applied Statistics,41(1):191?201.P.
Matykiewicz, W. Duch, and J. P. Pestian.
2009.
Clus-tering semantic spaces of suicide notes and news-groups articles.
In Proceedings of the Workshop onCurrent Trends in Biomedical Natural Language Pro-cessing, BioNLP ?09, pages 179?184, Stroudsburg,PA, USA.
Association for Computational Linguistics.S.
M. Mohammad and P. D. Turney.
2010.
Emotionsevoked by common words and phrases: Using me-chanical turk to create an emotion lexicon.
In Pro-ceedings of Workshop on Computational Approachesto Analysis and Generation of Emotion in Text, LA,California.S.
M. Mohammad and P. D. Turney.
2012.
Crowdsourc-ing a word?emotion association lexicon.
Computa-tional Intelligence.S.
M. Mohammad and T. Yang.
2011.
Tracking Sen-timent in Mail: How Genders Differ on EmotionalAxes.
In Proceedings of the 2nd Workshop on Com-putational Approaches to Subjectivity and SentimentAnalysis (WASSA 2.011), pages 70?79, Portland, Ore-gon.
Association for Computational Linguistics.S.
M. Mohammad.
2011.
From once upon a time tohappily ever after: Tracking emotions in novels andfairy tales.
In Proceedings of the ACL 2011 Work-shop on Language Technology for Cultural Heritage,Social Sciences, and Humanities (LaTeCH), Portland,OR, USA.A.
Neviarouskaya, H. Prendinger, and M. Ishizuka.2009.
Compositionality principle in recognition offine-grained emotions from text.
In Proceedings ofICWSM, pages 278?281, San Jose, California.C.
E. Osgood and E. G. Walker.
1959.
Motivationand language behavior: A content analysis of suicidenotes.
Journal of Abnormal and Social Psychology,59(1):58?67.J.
P. Pestian, P. Matykiewicz, and J. Grupp-Phelan.
2008.Using natural language processing to classify suicidenotes.
In Proceedings of the Workshop on CurrentTrends in Biomedical Natural Language Processing,BioNLP ?08, pages 96?97, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.R.
Plutchik.
1985.
On emotion: The chicken-and-eggproblem revisited.
Motivation and Emotion, 9(2):197?200.N.
Ravaja, T. Saari, M. Turpeinen, J. Laarni, M. Salmi-nen, and M. Kivikangas.
2006.
Spatial presence andemotions during video game playing: Does it matterwith whom you play?
Presence: Teleoperators andVirtual Environments, 15(4):381?392.C.
Strapparava and R. Mihalcea.
2007.
Semeval-2007task 14: Affective text.
In Proceedings of SemEval-2007, pages 70?74, Prague, Czech Republic.C.
Strapparava and A. Valitutti.
2004.
Wordnet-Affect:An affective extension of WordNet.
In Proceedings ofLREC, pages 1083?1086, Lisbon, Portugal.J.
D. Vela?squez.
1997.
Modeling emotions and othermotivations in synthetic agents.
In Proceedings ofthe fourteenth national conference on artificial in-telligence and ninth conference on Innovative appli-cations of artificial intelligence, AAAI?97/IAAI?97,pages 10?15.
AAAI Press.591
