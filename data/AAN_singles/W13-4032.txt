Proceedings of the SIGDIAL 2013 Conference, pages 193?202,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsVerbal indicators of psychological distress in interactive dialogue with avirtual humanDavid DeVault, Kallirroi Georgila, Ron Artstein, Fabrizio Morbini, David Traum,Stefan Scherer, Albert (Skip) Rizzo, Louis-Philippe MorencyUniversity of Southern California, Institute for Creative TechnologiesPlaya Vista, CAdevault@ict.usc.eduAbstractWe explore the presence of indicators ofpsychological distress in the linguistic be-havior of subjects in a corpus of semi-structured virtual human interviews.
Atthe level of aggregate dialogue-level fea-tures, we identify several significant dif-ferences between subjects with depres-sion and PTSD when compared to non-distressed subjects.
At a more fine-grainedlevel, we show that significant differencescan also be found among features thatrepresent subject behavior during specificmoments in the dialogues.
Finally, wepresent statistical classification results thatsuggest the potential for automatic assess-ment of psychological distress in individ-ual interactions with a virtual human dia-logue system.1 IntroductionOne of the first steps toward dealing with psy-chological disorders such as depression and PTSDis diagnosing the problem.
However, there is of-ten a shortage of trained health care professionals,or of access to those professionals, especially forcertain segments of the population such as mili-tary personnel and veterans (Johnson et al 2007).One possible partial remedy is to use virtual hu-man characters to do a preliminary triage screen-ing, so that mental healthcare providers can focustheir attention on those who are most likely to needhelp.
The virtual human would engage an indi-vidual in an interview and analyze some of theirbehavioral characteristics.
In addition to servinga triage function, this automated interview couldproduce valuable information to help the health-care provider make their expert diagnosis.In this paper, we investigate whether featuresin the linguistic behavior of participants in a con-versation with a virtual human could be usedfor recognizing psychological distress.
We focusspecifically on indicators of depression and post-traumatic stress disorder (PTSD) in the verbal be-havior of participants in a Wizard-of-Oz corpus.The results and analysis presented here are partof a broader effort to create an automated, interac-tive virtual human dialogue system that can detectindicators of psychological distress in the multi-modal communicative behavior of its users.
Re-alizing this vision requires a careful and strate-gic design of the virtual human?s dialogue behav-ior, and in concert with the system?s behavior, theidentification of robust ?indicator?
features in theverbal and nonverbal responses of human intervie-wees.
These indicators should be specific behaviorpatterns that are empirically correlated with spe-cific psychological disorders, and that can informa triage screening process or facilitate the diagno-sis or treatment performed by a clinician.In this paper, we report on several kinds of suchindicators we have observed in a corpus of 43Wizard-of-Oz interactions collected with our pro-totype virtual human, Ellie, pictured in Figure 1.We begin in Section 2 with a brief discussion ofbackground and related work on the communica-tive behavior associated with psychological dis-tress.
In Section 3, we describe our Wizard-of-Ozdata set.
Section 4 presents an analysis of indicatorfeatures we have explored in this data set, identi-fying several significant differences between sub-jects with depression and PTSD when comparedto non-distressed subjects.
In Section 5 we presentstatistical classification results that suggest the po-tential for automatic assessment of psychologicaldistress based on individual interactions with a vir-tual human dialogue system.
We conclude in Sec-tion 6.2 Background and Related WorkThere has been a range of psychological and clin-ical research that has identified differences in the193Figure 1: Ellie.communicative behavior of patients with specificpsychological disorders such as depression.
In thissection, we briefly summarize some closely re-lated work.Most work has observed the behavior of patientsin human-human interactions, such as clinical in-terviews and doctor-patient interactions.
PTSD isgenerally less well studied than depression.Examples of the kinds of differences that havebeen observed in non-verbal behavior include dif-ferences in rates of mutual gaze and other gazepatterns, downward angling of the head, mouthmovements, frowns, amount of gesturing, fidget-ing, emotional expressivity, and voice quality; seeScherer et al(2013) for a recent review.In terms of verbal behavior, our exploration offeatures here is guided by several previous obser-vations in the literature.
Cohn and colleagues haveidentified increased speaker-switch durations anddecreased variability of vocal fundamental fre-quency as indicators of depression, and have ex-plored the use of these features for classification(Cohn et al 2009).
That work studied these fea-tures in human-human clinical interviews, ratherthan in virtual human interactions as reported here.In clinical studies, acute depression has been as-sociated with decreased speech, slow speech, de-lays in delivery, and long silent pauses (Hall et al1995).
Aggregate differences in lexical frequen-cies have also been observed.
For example, inwritten essays, Rude et al(2004) observed thatdepressed participants used more negatively va-lenced words and used the first-person pronoun ?I?more frequently than never-depressed individuals.Heeman et al(2010) observed differences in chil-dren with autism in how long they pause beforespeaking and in their use of fillers, acknowledg-ments, and discourse markers.
Some of these fea-tures are similar to those studied here, but lookedat children communicating with clinicians ratherthan a virtual human dialogue system.Recent work on machine classification hasdemonstrated the ability to discriminate betweenschizophrenic patients and healthy controls basedon transcriptions of spoken narratives (Hong et al2012), and to predict patient adherence to med-ical treatment from word-level features of dia-logue transcripts (Howes et al 2012).
Automaticspeech recognition and word alignment has alsobeen shown to give good results in scoring narra-tive recall tests for identification of cognitive im-pairment (Prud?hommeaux and Roark, 2011; Lehret al 2012).3 Data SetIn this section, we introduce the Wizard-of-Ozdata set that forms the basis for this paper.
Inthis virtual human dialogue system, the charac-ter Ellie depicted in Figure 1 carries out a semi-structured interview with a single user.
The sys-tem was designed after a careful analysis of aset of face-to-face interviews in the same do-main.
The face-to-face interviews make up thelarge human-human Distress Assessment Inter-view Corpus (DAIC) that is described in Schereret al(2013).
Drawing on observations of inter-viewer behavior in the face-to-face dialogues, El-lie was designed to serve as an interviewer whois also a good listener, providing empathetic re-sponses, backchannels, and continuation promptsto elicit more extended replies to specific ques-tions.
The data set used in this paper is the re-sult of a set of 43 Wizard-of-Oz interactions wherethe virtual human interacts verbally and nonver-bally in a semi-structured manner with a partici-pant.
Excerpts from the transcripts of two interac-tions in this Wizard-of-Oz data set are provided inthe appendix in Figure 5.13.1 ProcedureThe participants were recruited via Craigslist andwere recorded at the USC Institute for Creative1A sample demonstration video of an interaction be-tween the virtual agent and a human actor can be seen here:http://www.youtube.com/watch?v=ejczMs6b1Q4194Technologies.
In total 64 participants interactedwith the virtual human.
All participants who metrequirements (i.e.
age greater than 18, and ad-equate eyesight) were accepted.
In this paper,we focus on a subset of 43 of these participantswho were told that they would be interacting withan automated system.
(The other participants,which we exclude from our analysis, were awarethat they were interacting with a human-controlledsystem.)
The mean age of the 43 participants inour data set was 36.6 years, with 23 males and 20females.We adhered to the following procedure for datacollection: After a short explanation of the studyand giving consent, participants completed a seriesof questionnaires.
These questionnaires includedthe PTSD Checklist-Civilian version (PCL-C) andthe Patient Health Questionnaire, depression mod-ule (PHQ-9) (Scherer et al 2013) along with otherquestions.
Then participants engage in an inter-view with the virtual human, Ellie.
After the di-alogue concludes, participants are then debriefed(i.e.
the wizard control is revealed), paid $25 to$35, and escorted out.The interaction between the participants and El-lie was designed as follows: Ellie explains the pur-pose of the interaction and that she will ask a seriesof questions.
She then tries to build rapport withthe participant in the beginning of the interactionwith a series of casual questions about Los Ange-les.
Then the main interview begins, including arange of questions such as:What would you say are some of yourbest qualities?What are some things that usually putyou in a good mood?Do you have disturbing thoughts?What are some things that make you re-ally mad?How old were you when you enlisted?What did you study at school?Ellie?s behavior was controlled by two human?wizards?
in a separate room, who used a graph-ical user interface to select Ellie?s nonverbal be-havior (e.g.
head nods, smiles, back-channels)and verbal utterances (including the interviewquestions, verbal back-channels, and empathy re-sponses).
This Wizard-of-Oz setup allows us toprove the utility of the protocol and collect trainingdata for the eventual fully automatic interaction.The speech for each question was pre-recorded us-ing an amateur voice actress (who was also one ofthe wizards).
The virtual human?s performance ofthese utterances is animated using the SmartBodyanimation system (Thiebaux et al 2008).3.2 Condition AssessmentThe PHQ-9 and PCL-C scales provide researcherswith guidelines on how to assess the participants?conditions based on the responses.
Among the43 participants, 13 scored above 10 on the PHQ-9, which corresponds to moderate depression andabove (Kroenke et al 2001).
We consider these13 participants as positive for depression in thisstudy.
20 participants scored positive for PTSD,following the PCL-C classification.
The two pos-itive conditions overlap strongly, as the evalu-ated measurements PHQ-9 and PCL-C correlatestrongly (Pearson?s r > 0.8, as reported in Schereret al(2013)).4 Feature Analysis4.1 Transcription and timing of speechWe have a set D = {d1, ..., d43} of 43 dialogues.The user utterances in each dialogue were tran-scribed using ELAN (Wittenburg et al 2006),with start and end timestamps for each utterance.2At each pause of 300ms or longer in the user?sspeech, a new transcription segment was started.The resulting speech segments were subsequentlyreviewed and corrected for accuracy.For each dialogue di ?
D, this process resultedin a sequence of user speech segments.
We repre-sent each segment as a tuple ?s, e, t?, where s and eare the starting and ending timestamps in seconds,and t is the manual text transcription of the corre-sponding audio segment.
The system speech seg-ments, including their starting and ending times-tamps and verbatim transcripts of system utter-ances, were recovered from the system log files.To explore aggregate statistical features basedon user turn-taking behavior in the dialogues, weemploy a simple approach to identifying turnswithin the dialogues.
First, all user and systemspeech segments are sorted in increasing order of2ELAN is a tool that supports annotation ofvideo and audio, from the Max Planck Insti-tute for Psycholinguistics, The Language Archive,Nijmegen, The Netherlands.
It is available athttp://tla.mpi.nl/tools/tla-tools/elan/.195Segment level features(a) mean speaking rate of each user segment(b) mean onset time of first segment in each user turn(c) mean onset time of non-first segments in user turns(d) mean length of user segments(e) mean minimum valence in user segments(f) mean mean valence in user segments(g) mean maximum valence in user segments(h) mean number of filled pauses in user segments(i) mean filled pause rate in user segmentsDialogue level features(j) total number of user segments(k) total length of all user segmentsFigure 2: List of context-independent features.their starting timestamps.
All consecutive seg-ments with the same speaker are then designatedas constituting a single turn.
While this simplescheme does not provide a detailed treatment ofrelevant phenomena such as overlapping speech,backchannels, and the interactive process of ne-gotiating the turn in dialogue (Yang and Heeman,2010), it provides a conceptually simple model forthe definition of features for aggregate statisticalanalysis.4.2 Context-independent feature analysisWe begin by analyzing a set of shallow featureswhich we describe as context-independent, as theyapply to user speech segments independently ofwhat the system has recently said.
Most of theseare features that apply to many or all user speechsegments.
We describe our context-independentfeatures in Section 4.2.1, and present our resultsfor these features in Section 4.2.2.4.2.1 Context-independent featuresWe summarize our context-independent featuresin Figure 2.Speaking rate and onset times Based on previ-ous clinical observations related to slowed speechand increased onset time for depressed individuals(Section 2), we defined features for speaking rateand onset time of user speech segments.We quantify the speaking rate of a user speechsegment ?s, e, t?, where t = ?w1, ..., wN ?, asN/(e ?
s).
Feature (a) is the mean value ofthis feature across all user speech segments withineach dialogue.Onset time is calculated using the notion of userturns.
For each user turn, we extracted the firstuser speech segment in the turn fu = ?su, eu, tu?,and the most recent system speech segment ls =?ss, es, ts?.
We define the onset time of such a firstuser segment as su ?
es, and for each dialogue,feature (b) is the intra-dialogue mean of these on-set times.In order to also quantify pause length betweenuser speech segments within a turn, we define fea-ture (c), a similar feature that measures the meanonset time between non-first user speech segmentswithin a user turn in relation to the preceding userspeech segment.Length of user segments As one way to quan-tify the amount of speech, feature (d) reports themean length of all user speech segments within adialogue (measured in words).Valence features for user speech Features (e)-(g) are meant to explore the idea that distressedusers might use more negative or less positive vo-cabulary than non-distressed subjects.
As an ex-ploratory approach to this topic, we used Senti-WordNet 3.0 (Baccianella and Sebastiani, 2010),a lexical sentiment dictionary, to assign valenceto individual words spoken by users in our study.The dictionary contains approximately 117,000entries.
In general, each word w may appear inmultiple entries, corresponding to different partsof speech and word senses.
To assign a single va-lence score v(w) to each word in the dictionary, inour features we compute the average score acrossall parts of speech and word senses:v(w) =?e?E(w) PosScoree(w)?NegScoree(w)|E(w)|where E(w) is the set of entries for the word w,PosScoree(w) is the positive score for w in entrye, and NegScoree(w) is the negative score for win entry e. This is similar to the ?averaging acrosssenses?
method described in Taboada et al(2011).We use several different measures of the va-lence of each speech segment with transcript t =?w1, ..., wn?.
We compute the min, mean, and maxvalence of each transcript:minimum valence of t = minwi?t v(wi)mean valence of t = 1n?wi?t v(wi)maximum valence of t = maxwi?t v(wi)Features (e)-(f) then are intra-dialogue mean196values for these three segment-level valence mea-sures.Filled pauses Another feature that we exploredis the presence of filled pauses in user speech seg-ments.
To do so, we counted the number of timesany of the tokens uh, um, uhh, umm, mm, or mmmappeared in each speech segment.
For each dia-logue, feature (h) is the mean number of these to-kens per user speech segment.
In order to accountfor the varying length of speech segments, we alsonormalize the raw token counts in each segmentby dividing them by the length of the segment, toproduce a filled pause rate for the segment.
Fea-ture (i) is the mean value of the filled pause ratefor all speech segments in the dialogue.Dialogue level features We also included twodialogue level measures of how ?talkative?
theuser is.
Feature (j) is the total number of userspeech segments throughout the dialogue.
Feature(k) is the total length (in words) of all speech seg-ments throughout the dialogue.Standard deviation features For the classifica-tion experiments reported in Section 5, we also in-cluded a standard deviation variant of each of thefeatures (a)-(i) in Figure 2.
These variants are de-fined as the intra-dialogue standard deviation ofthe underlying value, rather than the mean.
Wediscuss examples of standard deviation featuresfurther in Section 5.4.2.2 Results for context-independentfeaturesWe summarize the observed significant effects inour Wizard-of-Oz corpus in Table 1.Onset time We report our findings for individu-als with and without depression and PTSD for fea-ture (b) in Table 1 and in Figure 3.
The units areseconds.
While an increase in onset time for in-dividuals with depression has previously been ob-served in human-human interaction (Cohn et al2009; Hall et al 1995), here we show that thiseffect transfers to interactions between individualswith depression and virtual humans.
We find thatmean onset time is significantly increased for indi-viduals with depression in their interactions withour virtual human Ellie (p = 0.018, Wilcoxonrank sum test).Additionally, while to our knowledge onset timefor individuals with PTSD has not been reported,we also found a significant increase in onset timeMeanonsetdelayoffirstparticipantsegment(seconds)01234No depr.??Depr.
?Meanonsetdelayoffirstparticipantsegment(seconds)01234?PTSD?PTSD?Figure 3: Onset time.for individuals with PTSD (p = 0.019, Wilcoxonrank sum test).Filled pauses We report our findings for individ-uals with and without depression and PTSD underfeature (h) in Table 1 and in Figure 4.
We observeda significant reduction in this feature for both in-dividuals with depression (p = 0.012, Wilcoxonrank sum test) and PTSD (p = 0.014, Wilcoxonrank sum test).
We believe this may be relatedto the trend we observed toward shorter speechsegments from distressed individuals (though thistrend did not reach significance).
There is a pos-itive correlation, ?
= 0.55 (p = 0.0001), be-tween mean segment length (d) and mean numberof filled pauses in segments (h).Other features We did not observe significantdifferences in the values of the other context-independent features in Figure 2.4.3 Context-dependent featuresOur data set alws us to zoom in and look atspecific contextual moments in the dialogues, andobserve how users respond to specific Ellie ques-tions.
As an example, one of Ellie?s utterances,which has system ID happy lasttime, is:happy lasttime = Tell me about the lasttime you felt really happy.In our data set of 43 dialogues, this question wasasked in 42 dialogues, including 12 users positivefor depression and 19 users positive for PTSD.197Feature Depression (13 yes, 30 no) PTSD (20 yes, 23 no)(b) mean onset time of firstsegment in each user turn?Depr.
: 1.72 (0.89)No Depr.
: 1.08 (0.56)p = 0.018?PTSD.
: 1.56 (0.80)No PTSD.
: 1.03 (0.57)p = 0.019(h) mean number of filled pausesin user segments?Depr.
: 0.32 (0.19)No Depr.
: 0.48 (0.23)p = 0.012?PTSD: 0.36 (0.24)No PTSD: 0.49 (0.21)p = 0.014Table 1: Results for context-independent features.
For each feature and condition, we provide the mean(standard deviation) for individuals with and without the condition.
P-values for individual Wilcoxonrank sum tests are provided.
An up arrow (?)
indicates a significant trend toward increased feature valuesfor positive individuals.
A down arrow (?)
indicates a significant trend toward decreased feature valuesfor positive individuals.Meanfilledpausesinparticipantsegment(tokens)00.20.40.60.81.01.2No depr.?Depr.
?Meanfilledpausesinparticipantsegment(tokens)00.20.40.60.81.01.2?PTSD PTSD?Figure 4: Number of filled pauses per speech seg-ment.This question is one of 95 topic setting utter-ances in Ellie?s repertoire.
(Ellie has additionalutterances that serve as continuation prompts,backchannels, and empathy responses, which canbe used as a topic is discussed.
)To define context-dependent features, we asso-ciate with each user segment the most recent ofEllie?s topic setting utterances that has occurred inthe dialogue.
We then focus our analysis on thoseuser segments and turns that follow specific topicsetting utterances.
In Table 2, we present some ex-amples of our findings for context-dependent fea-tures for happy lasttime.33While we provide significance test results here at the p <0.05 level, it should be noted that because of the large numberof context-dependent features that may be defined in a smallcorpus such as ours, we report these merely as observations inour corpus.
We do not claim that these results transfer beyondThe contextual feature labeled (g?)
in Table 2 isthe mean of the maximum valence feature acrossall segments for which happy lasttime is the mostrecent topic setting system utterance.
We providea full example of this feature calculation in Fig-ure 5 in the appendix.As the figure shows, we find that users withboth PTSD and depression show a sharp reduc-tion in the mean maximum valence in their speechsegments that respond to this question.
This sug-gests that in these virtual human interactions, thisquestion plays a useful role in eliciting differen-tial responses from subjects with these psycholog-ical disorders.
We observed three additional ques-tions which showed a significant difference in themean maximum valence feature.
One example isthe question, How would your best friend describeyou?.With feature (b?)
in Table 2, we find an in-creased onset time in responses to this question forsubjects with depression.4 Feature (d?)
shows thatsubjects with PTSD exhibit shorter speech seg-ments in their responses to this question.We observed a range of findings of this sort forvarious combinations of Ellie?s topic setting utter-ances and specific context-dependent features.
Infuture work, we would like to study the optimalcombinations of context-dependent questions thatyield the most information about the user?s distressstatus.this data set.4In comparing Table 2 with Table 1, this question seemsto induce a higher mean onset time for distressed users thanthe average system utterance does.
This does not seem to bethe case for non-distressed users.198Feature Depression (12 yes, 30 no) PTSD (19 yes, 23 no)(g?)
mean maximum valencein user segments followinghappy lasttime?Depr.
: 0.15 (0.07)No Depr.
: 0.26 (0.12)p = 0.003?PTSD: 0.16 (0.08)No PTSD: 0.28 (0.11)p = 0.0003(b?)
mean onset time of firstsegments in user turnsfollowing happy lasttime?Depr.
: 2.64 (2.70)No Depr.
: 0.94 (1.80)p = 0.030n.s.PTSD: 2.18 (2.48)No PTSD: 0.80 (1.76)p = 0.077(d?)
mean length of usersegments followinghappy lasttimen.s.Depr.
: 5.95 (1.80)No Depr.
: 10.03 (6.99)p = 0.077?PTSD: 6.82 (5.12)No PTSD: 10.55 (6.68)p = 0.012Table 2: Example results for context-dependent features.
For each feature and condition, we providethe mean (standard deviation) for individuals with and without the condition.
P-values for individualWilcoxon rank sum tests are provided.
An up arrow (?)
indicates a significant trend toward increasedfeature values for positive individuals.
A down arrow (?)
indicates a significant trend toward decreasedfeature values for positive individuals.5 Classification ResultsIn this section, we present some suggestive clas-sification results for our data set.
We constructthree binary classifiers that use the kinds of fea-tures described in Section 4 to predict the pres-ence of three conditions: PTSD, depression, anddistress.
For the third condition, we define dis-tress to be present if and only if PTSD, depres-sion, or both are present.
Such a notion of distressthat collapses distinctions between disorders maybe the most appropriate type of classification for apotential application in which distressed users ofany type are prioritized for access to health careprofessionals (who will make a more informed as-sessment of specific conditions).For each individual dialogue, each of the threeclassifiers emits a single binary label.
We trainand evaluate the classifiers in a 10-fold cross-validation using Weka (Hall et al 2009).While our data set of 43 dialogues is perhapsof a typical size for a study of a research proto-type dialogue system, it remains very small froma machine learning perspective.
We report heretwo kinds of results that help provide perspectiveon the prospects for classification of these condi-tions.
The first kind looks at classification basedon all the context-independent features describedin Section 4.2.1.
The second looks at classifica-tion based on individual features from this set.In the first set of experiments, we trained aNa?
?ve Bayes classifier for each condition usingall the context-independent features.
We presentour results in Table 3, comparing each classifier toa baseline that always predicts the majority class(i.e.
no condition for PTSD, no condition for de-pression, and with condition for distress).We note first that the trained classifiers all out-perform the baseline in terms of weighted F-score,weighted precision, weighted recall, and accuracy.The accuracy improvement over baseline is sub-stantial for PTSD (20.9% absolute improvement)and distress (23.2% absolute improvement).
Theaccuracy improvement over baseline is more mod-est for depression (2.3% absolute).
We believeone factor in the relative difficulty of classifyingdepression more accurately is the relatively smallnumber of depressed participants in our study(13).While it has been shown in prior work (Cohn etal., 2009) that depression can be classified abovebaseline performance using features observed inclinical human-human interactions, here we haveshown that classification above baseline perfor-mance is possible in interactions between humanparticipants and a virtual human dialogue system.Further, we have shown classification results forPTSD and distress as well as depression.We tried incorporating context-dependent fea-tures, and also unigram features, but found thatneither improved performance.
We believe ourdata set is too small for effective training withthese very large extended feature sets.199Disorder Model Weighted F-score Weighted Precision Weighted Recall AccuracyPTSD Na?
?ve Bayes 0.738 0.754 0.744 74.4%Majority Baseline 0.373 0.286 0.535 53.5%Depression Na?
?ve Bayes 0.721 0.721 0.721 72.1%Majority Baseline 0.574 0.487 0.698 69.8%Distress Na?
?ve Bayes 0.743 0.750 0.744 74.4%Majority Baseline 0.347 0.262 0.512 51.2%Table 3: Classification results.In our second set of experiments, we sought togain understanding of which features were pro-viding the greatest value to classification perfor-mance.
We therefore retrained Na?
?ve Bayes classi-fiers using only one feature at a time.
We summa-rize here some of the highest performing features.For depression, we found that the feature stan-dard deviation in onset time of first segment ineach user turn yielded very strong performanceby itself.
In our corpus, we observed that de-pressed individuals show a greater standard devia-tion in the onset time of their responses to Ellie?squestions (p = 0.024, Wilcoxon rank sum test).The value of this feature in classification comple-ments the clinical finding that depressed individu-als show greater onset times in their responses tointerview questions (Cohn et al 2009).For distress, we found that the feature meanmaximum valence in user segments was the mostvaluable.
We discussed findings for a context-dependent version of this feature in Section 4.3.This finding for distress can be related to previ-ous observations that individuals with depressionuse more negatively valenced words (Rude et al2004).For PTSD, we found that the feature mean num-ber of filled pauses in user segments was amongthe most informative.Based on our observation of classification per-formance using individual features, we believethere remains much room for improvement in fea-ture selection and training.
A larger data set wouldenable feature selection approaches that use heldout data, and would likely result in both increasedperformance and deeper insights into the mostvaluable combination of features for classification.6 ConclusionIn this paper, we have explored the presence of in-dicators of psychological distress in the linguisticbehavior of subjects in a corpus of semi-structuredvirtual human interviews.
In our data set, wehave identified several significant differences be-tween subjects with depression and PTSD whencompared to non-distressed subjects.
Drawing onthese features, we have presented statistical classi-fication results that suggest the potential for auto-matic assessment of psychological distress in indi-vidual interactions with a virtual human dialoguesystem.AcknowledgmentsThis work is supported by DARPA under con-tract (W911NF-04-D-0005) and the U.S. ArmyResearch, Development, and Engineering Com-mand.
The content does not necessarily reflect theposition or the policy of the Government, and noofficial endorsement should be inferred.ReferencesAndrea Esuli Stefano Baccianella and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexi-cal resource for sentiment analysis and opinion min-ing.
In Proceedings of the Seventh conference onInternational Language Resources and Evaluation(LREC?10), Valletta, Malta, May.
European Lan-guage Resources Association (ELRA).Jeffery F. Cohn, Tomas Simon Kruez, Iain Matthews,Ying Yang, Minh Hoai Nguyen, Margara TejeraPadilla, Feng Zhou, and Fernando De la Torre.2009.
Detecting depression from facial actions andvocal prosody.
In Affective Computing and Intelli-gent Interaction (ACII), September.Judith A.
Hall, Jinni A. Harrigan, and Robert Rosen-thal.
1995.
Nonverbal behavior in clinician-patientinteraction.
Applied and Preventive Psychology,4(1):21 ?
37.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: an update.SIGKDD Explor.
Newsl., 11(1):10?18, November.Peter A Heeman, Rebecca Lunsford, Ethan Selfridge,Lois Black, and Jan Van Santen.
2010.
Autism and200interactional aspects of dialogue.
In Proceedingsof the 11th Annual Meeting of the Special InterestGroup on Discourse and Dialogue, pages 249?252.Association for Computational Linguistics.Kai Hong, Christian G. Kohler, Mary E. March, Am-ber A. Parker, and Ani Nenkova.
2012.
Lexi-cal differences in autobiographical narratives fromschizophrenic patients and healthy controls.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 37?47, Jeju Island, Korea, July.
Association for Compu-tational Linguistics.Christine Howes, Matthew Purver, Rose McCabe,Patrick G. T. Healey, and Mary Lavelle.
2012.Predicting adherence to treatment for schizophreniafrom dialogue transcripts.
In Proceedings of the13th Annual Meeting of the Special Interest Groupon Discourse and Dialogue, pages 79?83, Seoul,South Korea, July.
Association for ComputationalLinguistics.Shannon J Johnson, Michelle D Sherman, Jeanne SHoffman, Larry C James, Patti L Johnson, John ELochman, Thomas N Magee, David Riggs, Jes-sica Henderson Daniel, Ronald S Palomares, et al2007.
The psychological needs of US military ser-vice members and their families: A preliminary re-port.
American Psychological Association Presi-dential Task Force on Military Deployment Servicesfor Youth, Families and Service Members.Kurt Kroenke, Robert L. Spitzer, and Janet B. W.Williams.
2001.
The phq-9.
Journal of GeneralInternal Medicine, 16(9):606?613.Maider Lehr, Emily Prud?hommeaux, Izhak Shafran,and Brian Roark.
2012.
Fully automated neuropsy-chological assessment for detecting mild cognitiveimpairment.
In Interspeech 2012: 13th Annual Con-ference of the International Speech CommunicationAssociation, Portland, Oregon, September.Emily Prud?hommeaux and Brian Roark.
2011.
Ex-traction of narrative recall patterns for neuropsycho-logical assessment.
In Interspeech 2011: 12th An-nual Conference of the International Speech Com-munication Association, pages 3021?3024, Flo-rence, Italy, August.Stephanie Rude, Eva-Maria Gortner, and James Pen-nebaker.
2004.
Language use of depressed anddepression-vulnerable college students.
Cognition& Emotion, 18(8):1121?1133.S.
Scherer, G. Stratou, M. Mahmoud, J. Boberg,J.
Gratch, A. Rizzo, and L.-P. Morency.
2013.
Au-tomatic behavior descriptors for psychological dis-order analysis.
In IEEE Conference on AutomaticFace and Gesture Recognition.Maite Taboada, Julian Brooke, Milan Tofiloski, Kim-berly Voll, and Manfred Stede.
2011.
Lexicon-based methods for sentiment analysis.
Computa-tional Linguistics, 37(2):267?307, June.Marcus Thiebaux, Stacy Marsella, Andrew N. Mar-shall, and Marcelo Kallmann.
2008.
Smartbody:behavior realization for embodied conversationalagents.
In Proceedings of the 7th internationaljoint conference on Autonomous agents and multi-agent systems - Volume 1, AAMAS ?08, pages 151?158, Richland, SC.
International Foundation for Au-tonomous Agents and Multiagent Systems.Peter Wittenburg, Hennie Brugman, Albert Russel,Alex Klassmann, and Han Sloetjes.
2006.
Elan: aprofessional framework for multimodality research.In Proceedings of LREC 2006, Fifth InternationalConference on Language Resources and Evaluation.Fan Yang and Peter A. Heeman.
2010.
Initiative con-flicts in task-oriented dialogue.
Computer Speech &Language, 24(2):175 ?
189.201Appendix A. Wizard-of-Oz Dialogue ExcerptsExample user with PTSD and depression Example non-distressed usermaxvalencetranscriptEllie (happy lasttime) tell meabout the last time youfelt really happyUser 0.014 when i play with my dogEllie when was the last timethat happened?User 0.000 this morningEllie awEllie tell me more about thatUser 0.312 i don?t know i justUser 0.010 whenever i getUser 0.312 super bored or i justneed some time toUser 0.312 for myself i just playwith the dogEllie that?s goodUser 0.000 yeah0.1371 = meanmaximum valence inuser segments followinghappy lasttimeEllie (ideal weekendC) tellme how you spend yourideal weekendmaxvalencetranscriptEllie (happy lasttime) tell meabout the last time youfelt really happyUser 0.562 um the last time i feltreally happy wasUser 0.000 hmUser 0.000 todayEllie tell me more about thatUser 0.688 uh just from the momenti woke up it was abeautiful sunny dayUser -0.062 iUser 0.565 went to see some friendswe had a good timewent to schoolUser 0.565 had some good gradeson some papers umwrote a good essayUser 0.292 feel pretty accomplishedandUser 0.312 i feel like my day is justUser 0.565 a good dayEllie that?s so good to hear0.3487 = meanmaximum valence inuser segments followinghappy lasttimeEllie (BF describe) howwould your best frienddescribe you?Figure 5: Examples of maximum valence feature.202
