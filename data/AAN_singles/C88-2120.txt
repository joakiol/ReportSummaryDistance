An Augmented Context Free Grammar for DiscourseAbstractThis paper presents an augmented context free grammar whichdescribes important features of the surface structure and the semanticsof discourse in a formal way, integrating new as well as previouslyexisting insights into a unified framework.
The structures covered includelists, narratives, subordinating and coordinating rhetorical relations, topicchains and interruptions.
The paper discusses the problem of parsingdiscourse, and compares different grammatical formalisms which couldbe used for describing discourse structure.Remko SCHA and  L iv ia  POLANYIBBN Laborator ies10 Mou l ton  St reetCambr idge ,  MA 02238latter case, a correct constituent analysis of the discourse is necessary toestablish the arguments of the rhetorical relations.
On the other hand,the rhetorical relations themselves constitute an important structure-building component of discourse.1.
IntroductionThough a wealth of insights on the structure and meaning ofdiscourse ha,,; been gathered by researchers in linguistics, psychology,ethnomethodelogy and artificial intelligence, these insights have notbeen integrated into formal grammars which display the breadth, depthand precision ef formal treatments of sentential syntax and semantics.
Inthe present paper we make a step towards a formal, integrateddescription of the surface structure and semantic interpretation ofdiscourse.
We introduce a formalism which uses augmented contextfree rules for specifying discourse grammars, and demonstrate itsviability by developing a set of syntactic/semantic rules which covers anumber of important discourse phenomena.
We discuss the issue ofparsing discourse in a semi-deterministic left-to-right fashion, and relatethe grammar presented here to the strategies outlined in \[24\] [25\] forbuilding up a structural description of an unfolding discourse.
Finally, wecompare the formalism used here to some possible alternatives.2.
Discourse Structure and Discourse Semant icsThe semantic interpretation of the utterances in a discourse hasbeen shown to depend on the structural relations obtaining among thesegments of that discourse.
\[8\] [17\] In developing a grammar for thesurface structure of discourse, it is our aim to account for thesemantically relevant aspects of its structure.
Two phenomena offundamental importance for semantic interpretation depend crucially ondiscourse structure: context dependence and rhetorical structure.Context Dependence of Utterance MeaningsContext dependence of utterance meanings is a pervasivephenomenon in language.
When an utterance is analysed in isolation, itsmeaning is underdetermined in many ways as an effect of the followingphenomena:?
Indexicallty.
One rarely makes a statement or asks aquestion about the universe at large: every utterancepresupposes an implicit temporal, spatial and topicalframework that constrains the scopes of the meanings of theconstituents of the utterance.
Every sentence musttherefore be evaluated with respect to a frame of referencewhich i,,; usually left implicit in the sentence itself.?
Anaph?}ra.
Many utterances contain pronouns and definitedescriptions which constitute overt references to theprevious discourse.
An analysis of the structure of thepreviou:~ discourse is necessary to resolve such anaphoricreferences correctly.,,Implicit arguments.
Many natural language words aresemantically unsaturated, needing externally providedarguments in order to be interpreted felicitously.Nevertheless, such constructions can be used in sentenceswhich do not mention their arguments explicitly, if thesearguments can be inherited from previous parts of thediscourse.
("John is ta//er \[than Peter\]."
What is the speed\[of John's car\]?")
(Cf.
\[41)The Rhetorical Structure of DiscourseThe insiflht of work focused on the rhetorical structure of discourseis that a speaker engaged in.
a discourse may perform speech actswhose illocutionary force has scope over complex propositions which arebuilt up out of individual sentence meanings and the rhetorical relationsbetween them.
(See, for example, \[13\], \[19\], and \[20\].)
These rhetoricalrelations may be overtly expressed, or they may have to be abducted onthe basis of the sentence meanings.
Paradigmatic examples of suchcomplex propositions are ",4 caused B", "A was caused by B", "Aprovides evklence for B", etc.
-- where A and B may stand forpropositions expressed by individual sentences, or may themselves becomplex prepositions expressed by discourse segments.
Because of theIn the approach to discourse semantics outlined in this paper, everysentence is initially interpreted in a local, context-independent fashion.This results in a meaning representation which will usually contain freevariables, standing for the discourse-dependent elements in theutterance meaning.
When a sentence is integrated into the ongoingdiscourse, these variables are bound to values picked up from thecontext into which the sentence is inserted.
The next section describesthis in more detail.3.
An Augmented  Context  Free Grammar  for  D iscourse .Discourses have a hierarchical structure.
They are built uprecursively out of units of various kinds which can occur as constituentsof each other.
To account for this, a discourse grammar must be able toassign a tree structure to a discourse.
We call this tree structure thediscourse parse tree.
To describe in a formal way how a discourseparse tree is constructed out of constituent sentences, we use a contextfree grammar whose non-terminal symbols are augmented withattribute/value pairs.
(Distinct non-terminal categories have distinct setsof attributes.)
Context free rules describe how the constituent segmentsof a discourse (which we call discourse constituent units or dcu's) arebuilt up out of their subconstituents.
The values of the attributes on anon-terminal represent the relevant structural and semantic properties ofthe dcu generated by that non-terminal.
Every attribute has a fixed set ofpossible value-expressions.
The value-expressions may be of differentkinds: they may be atomic, they may themselves be sets ofattribute/value pairs, or they may be logical expressions.Value-expressions often store parfia/ information; therefore, theymay contain free variables.
A value-expression stands for the set of itsground instances.
(A value-expression without variables thus stands fora singleton set.)
When an attribute has a value-expression which standsfor the empty set, the complex category symbol that contains this value-expression fails to label a possible dcu.The context-free rules enforce agreement and upwards-inheritanceof the relevant properties of different constituents through tile fact thatdifferent occurrences of the same variable take on identical values.
Toput this in more precise terms, we define the meaning of an augmentedcontext free rule as follows.
If A, B, C, Y, Z stand for complex categorysymbols (including attribute/value-expression pairs) and Idcul\]?
and\[dcu2\]z are legitimate dcu's, the rule "A => B C" legitimizes the dcu~(\[\[dcul\]y \[dcu2\]z\]A ) iff the substitution ~ is the most genera/unifier I ofthe terms <B,C> and <Y,Z>, and G(A) is a legitimate complex categorysymbol, not containing empty attribute-value xpressions.3.1.
Attribute Values and Semantic Interpretation.The propagation of attribute values between dcu's plays animportant part in establishing the semantic interpretation of theutterances in a discourse.
We now list some of these attributes, anddiscuss their role in semantic interpretation and discourse processing.The Semantics attribute records a logical formula representing themeaning of the discourse constituent unit that it is associated with.Anaphorlc elements which have not been resolved inside the dcu arerepresented by free variables.
There are two mechanisms for anaphorresolution: (1) unification with value-expressions of attributes of otherdcu's, and (2) explicit search processes involving the DiscourseReferents Set of accessible dcu's.The Discourse Referents Set records the entities introduced in thediscourse unit that it is associated with.
These, plus the entities in theDiscourse Referents,Sets of the embedding units (dominating nodes inthe tree) are the entities which are available for anaphodc reference inan utterance which extends or expands that discourse unit.
EveryDiscourse Referent is a pair consisting of (1) the linguistic expressionthat introduced it, and (2) the semantic representation that the systemattached to that expression.
Discourse Referent Sets are accessed indifferent ways by a number of algorithms, which resolve the meanings of~Tho concepts we use here ere essentially the ones that were developed for termunilication in first-order logic theorem proving ~30\].
We assume some straighfloPaLU~generalization ofthese concepts which deals with the fact that our "terms" have a structtji#which, though not interestingly different, issomewhat richer.573different types of context-dependent expressions such as definitedescriptions, anaphoric pronouns~ demonstrative pronouns, the words"one" and "ones", and implicit arguments of function nouns,comparatives, etc.The Reference-Time records the time-interval which is to serve asthe temporal index for extensions and expansions of the discourse unitunder consideration..The Reference-Time can be' reset by theoccurrence of explicit tempera( adverbs.
Narratives constitute a specifictype of discourse units in which time plays a special role; they aremarked by the occurrence of sentences with non-durative aspect(event-sentences).
In a narrative, the reference-time is reset wheneveran event occurs.
\[15l \[12\]The Spatial  Index and the Modal Index play a role in the semanticinterpretation of the utterances which is similar to the role of theReference-Time: they specify a value for components of the index ofevaluation of the semantic formula.
In English, they can only be updatedby the occurrence of spatial and modal adverbials or other similarlyexplicit information.In the next section we describe the rules of a discourse grammar.They use some other attributes besides the ones just mentioned.
Thesecan be most easily explained as we introduce the rules that use them.3.2.
Grammar RulesThe grammar we present here, though necessarily limited andschematic in many ways, covers a wide range of phenomena that areusually discussed separately.
An adequate account o f  discoursestructure, however, depends on their integration into a consistentmechanism.
The semantic phenomena that we pay attention to includeanaphor resolution, scope of modal indices, movement of reference time,and rhetorical relations.
The grammar consists of rules which descdbehow to build up various kinds of structurally different discourseconstituent units.
We distinguish the following kinds of dcu's:?
Subordinations.
These are binary structures in which thefirst element remains accessible; we view them as units inwhich all or most of the structurally relevant features areinherited from the left constituent.
(In discourse, unlike inthe sentence, the subordinating element is always to the leftof the subordinated one.)
In semantic subordinations thereis a semantic relation between the two constituents; this isthe case in rhetorical subordinations and in topic-dominantchains.
Interruptions, on the other hand; though structurallyanalogous, are semantically very different: in this case,there is no semantic connection whatsoever between thetwo constituents.?
Binary Coordinations.
These are binary structures in whichthe second element has equal status to the first, thusmaking the first one inaccessible.
Under this category weinclude rhetorical coordinations (the counterparts of therhetorical subordinations), and adjacency pairs which areconcerned with the interactiona/dimension of the discourse(we include question~answer pairs and request~responsepairs).?
N-ary Coordinations.
These are flat structures which cancontain arbitrarily many elements, of which, at any time; onlythe .most recent one is accessible.
We include fists,monotonic lists, and narratives.
To generate n-arycoordinations by means of a context free grammar, we mustassign them a recursive structure: we build them up bymeans of binary rules which extend them to the right.This classification is not necessarily complete, and should be thetopic of more extensive discussion.
But it does cover the most importantstructures, and brings some order into the grammar rules that we presentbelow.Notation: Category symbols have the form "cat \[~t:o~t ...... ~n:(~n\]",where "cat" is the basic non-terminal symbol of the context-treegrammar, ~t ..... ~n are the attributes, and ~l ..... ~n are expressionswhich stand for the value-sets of these attributes.
Variables areindicated by italicized strings.
If in a phrase structure rule an attributehas as its value-expression a variable that does not occur anywhere elsein the rule, we will, for the sake of readability, leave the attribute out ofour specification of the rule.The grammar does not have a particular start symbol; dcu's of allkinds are recognized as well-formed "discourses".Z Sentences areassumed as elementary dcu's.
The following rules specify the internalstructure of some important kinds of complex discourse constituent units.We discuss further details about our approach and about the grammarformalism as we introduce these rules.Lists:list \[drs: dIud2, schema:s, sere: s(x) & s(y)\]=> dcu1\[drs:dl, sere: s(x)\]dcu2 \[drs:d2, sere: s(y)\]The intuitive idea behind this rule is that if two adjacent dcu's can beanalyzed as having semantically parallel structures, they can beconjoined in a fist structure.
Note that there is no constraint on thecategory of the dcu's: the main category symbols in the right hand side orthe rule are variables.The formal criterion for the application of the rule is the existence ela Z-formula s such that the semantic content of each of the two dcu's hasthe form s(u).
s We require that the semantic representation of the dcualready has the structure s(u), or can be put in that form by means Of avery limited repertoire of logical equivalence transformations.
(Thisrepertoire needs to be specified In detail.
It will probably at least includeZ-abstraction.)
This restriction excludes undesirable trivial values of ssuch as (;L u: If u = SEM1 then SEM1 else SEM2).
The types of the;L-variables of s are required to be elements in a predefined hierarchy of"natural kinds" which classifies the'meanings of lexical entries.
Thevalue of s, representing the common denominator between the meaningsof the two constituents, is stored as the value of the "schema" attributeon the list-dcu.Note that this rule often creates ambiguity.
To take a simpleexample: if "John likes Mary" is followed by "Peter likes Mary", one canabstract out several different schema's, even without consideringdifferent choices for the types of ~.-variables.
~.
P: ,?
(MARY), or ;~ R,x:R(x, MARY), or ;~ x,y: LIKE(x,y ), or Z x: LIKE(x, MARY).
The last ofthese is the preferred one, because it is most specific.
Rather thanconstraining the grammar rule to apply only in the most specific way toevery case, we assume that a preference for the most specific rule-application is applied as a heuristic principle during the parsing process.The "sam" attribute stores the semantics of a dcu, The operation ofthe "&"-operator which is used here to build up the semantics is moregeneral than logical conjunction: its arguments can have differentspeakers and have different ilfocutionary force operators.
The semanticrepresentation built up by "&" stores the propositional content ofindividual utterances in a richly indexed data structure.
We shall notattempt o specify this data structure in detail in the.
present paper.
Weintend that for the case of purely assertional monologue, the operation of"&" reduces to logical conjunction.
(The movement of narrative time, forinstance, will therefore not be hidden inside the meaning of "&".
)Note that the notion of "list" that we use here is more general thanthe one we have used in previous discussions \[23\].
For instance, thisnotion of "list" subsumes the notion of a "topic chain".In the formula stored as the value of the "sem"-attribute of a dcu,typed free variables are used to represent unresolved anaphors, such aspronouns and destressed definite descriptions.
The unification processwhich matches the "sem"-attrtbutes of two dcu's when they are joined toconstitute a list may substitute expressions for these vadables and thusresolve the anaphoric reference.
Thus, some strong reference resolutionpreferences are explained as following directly from theacknowledgement of parallel structure; in this case, anaphoric elementsget resolved without any search through the space of available discoursereferents.
This accounts for anaphoric reference to topic or attentionalfocus \[32\], as well as anaphoric reference to the corresponding elementin a semantically parallel structure.To accumulate the appropriate candidates for further anaphorresolution processes, the discourse referents (values of the "drs"attribute) of both constituent dcu's are synthesized into the discoursereferent set of the list-dcu.To extend a list-dcu to become a list with one more element, thereis the following additional rule:list \[schema:s, drs: drsl u drs2, sem: p & s(y)\]=> list \[schema:s, drs:drsl, sere:p\]dcu \[drs:drs2, sere: s(y) \]A list can be extended by another dcu if this dcu instantlates thestructure described by the schema-attribute of the list.Monoton ic  Lists:m-list \[schema;s, drs: drs l u drs2,direction: (If x<y then increlse  It x>y then decr e lse  fail),last:y, sem: s(x) & s(y)\]=> dcu \[drs: drst, sere: s(x)\]dcu \[drs:drs2, sere: s(y)\]2Social constraints on discourses as units of interaction \[concerning greetings, farewells,and proper ways of embedding a discourse ina social situation), are dealt with by separatesets of rules, describing "interactions" and "Speech Events".
See \[23} for discussion.3If the Z-function s takes more than one argument, we viaw it as working on n-tuptes; thevalues of x and y are n-tupIos in this case.If in a list the various arguments of the "sChema"-function areelements of a linearly ordered domain, the list may be a monotonic list.To make it possible to ascertain whether a next dcu can be added to amonotonic list, such lists carry the value of the most recent argument("last"-attribute), and their "direction" ("increasing" or "decreasing").
("fail"is a special constant which is not allowed as an attribute value-expression.
)m-list \[schema:s, drs: drsl ~J drs2,direction: (It x<y ^  p=incr then increlse If x>y ^  p=decr then decr else fail),last:y, sere: q & s(y) \]=> m-list \[schema:s, drs:drsl, incr: p, last:x, sere:q\]dcu \[drs:drs2, sere: s(y) ^  y ~ d\]A monotonic list can be extended by another dcu which instantiatesthe structure described by its schema:attribute, provided that theincreasing or decreasing ordering is maintained.
(Monotonic lists arediscussed in \[24\].
)Narrat ives:"Narratives* are used to express that a series of states of affairsobtain at successive points along a timeline (reference times \[29\]).
Howa next main clause of a narrative will interact with the previouslyestablished reference time of the narrative, depends on its aspect,durattve clauses behave differently from non-durative ones \[35\].
"Wetherefore need hvo different rules for extending a narrative.
We first givethe rule for duratives,narrative \[drs: drsl ~J drs2, reference-time:rt,tense: (if tt ~ xt then  t l  e lse  fail), sere: p & \[s\],f\]=> narrative \[drs:drsl, reference-time:rt, tense: tt, sere:p\]dcu \[drs: drs2, reference-time:rt, x-tenses: xt,aspect: durative, sere: s\]The reference-time attribute indicates the time-interval where theprogression of narrative time has arrived, Le., an interval after the lastevent in the narrative so tar.
A durative dcu which extends a narrative isevaluated at the reference-time of that narrative.The value of the "tense" attribute on a narrative dcu marks thetemporally distinct modes of narrative that a particular language allows.For English this includes the distinction between past, present,pluperfect, and future narratives.
On a sentence (or more complex dcu)which is to be integrated into a narrative, the relevant attribute is not"tense" (which stores the tense of the dcu), but "x-tenses", which storesthe tenses that may be externally imposed on the dcu.
A sentence withPRESENT as its tense can be used in a context where the tense is set toeither PRESENT, PAST, or FUTURE, Similarly, a PAST tense sentenceis compatible with either a PAST or a PLUPERFECT framework.
Asentence with the PLUPERFECT tense, however, is only compatible witha PLUPERFECT timeframe.
When a dcu extends a/narrative, the "tense"of the narrative must be an element of the "x-t~'nse~" of/the dcu.narrative \[drs: drs l u drs2, reference-time:v,tense: (it t l  ~ xtthee t l  else fail),sem: p & \[S\]u & t< i u <i v\]=> narrative \[drs:drsl, reference-time:t, ense: tl, sem:p\]dcu \[drs: drs2, reference-time:u,aspect:event, x-tenses: xt, sam: s\]An event-dcu which extends a narrative is evaluated at an interval ufollowing the then current reference-time t of the narrative.
The newreference-time of the extended narrative is established to be anotherinterval v after u.
(Notation: a <i b means "a immediately precedes b".
)The effect of this is that a durative dcu without explicit time-adverbials isinterpreted at an interval immediately alter the last event, which will beclosed off by the next event.
(Validity beyond this Interval, in bothdirections, can often be inferred from this by invoking plausibleassumptions about the universe of discourse.)
Another effect is, thatsuccessive events are always separated by a time-gap (though nothingis said about the size of this time-gap).The above rules only define how to extend a narrative that isalready underway.
The rules for beginning a narrative are similar havebeen omitted for reasons of space.Rhetor ica l  subord inat ions:  5dcul \[t~l: Ix I .....
(~.
: cx., index: i, sere: a & R(a,\[b\] i )\]=> dcul  \[~1: ~I .....
~.
: o~., index: i, sam: a\]dcu2lsem: ~.
x: R(x,b)\](pop-marker)4Formulated in terms of Vendler's\[34\] dassilicaticn, durative dcu's describeaccomplishments orachievements, while non-durative oees describe stares or activities.STo enhance th*~ icglbllity of the rules, we will from now on leave out the description ofthe upwards propagation of discourse referents.
It occurs uniformly in the same way as inthe rules glvenbet~)re.This rule parses semantic subordinations which involve an explicitlyindicated subordinating rhetorical relation R ("for instance," "because").The meaning of this relation is assumed to be incorporated in thesemantics of the subordinated dcu; this dcu therefore has as a value ofits "sem"-attribute a ;~-function which expects a propositional argument, sThe attributes and Values of a subordination are inherited from thesubordinating constituent.The subordinated constituent is optionally followed by a pop-marker(e.g.
"so", "anyway").
All clue-words (push-markers, pop-markers,interruption-markers) are treated as independent units, separate from thesentences that they precede or follow.In the formulation of the semantic subordination rule we haveassumed an attribute called "index", containing reference time as well asspatial and modal index.The rule shows how the subordinated discourseconstituent unit is semantically contextualized by the subordinating one.dcul \[~1: rxl ..... ~,: c~,,, index:i, sere: a & R(a,\[b\]i)\]=> dcul \[~1: ~Xl .....
~.
: ~x., index:i, sam: a\](push-marker)dcu2 \[sem: b\](pop-marker)This rule parses semantic subordinations for which the rhetoricalrelation involved is not overtly marked, The variable R ranges over allsubordinating rhetorical relations.Since its value is not stated explicitly, itmust be abducted on the basis of plausibility considerations regardingthe resulting semantics, The subordinated constituent is optionallypreceded by a push-marker (e.g.
"like"), and optionally followed by apop-marker.For subordinations we need a more elaborate treatment ofsemantics than the one assumed in this paper.
We need to distinguishbetween the total accumulated meaning of a discourse constituent unitand its "core meaning", which is considered in computations regardingsemantic relations with other dcu's.
It is a characteristic property ofsubordination dcu's that they allow for interpretations in which the coremeaning is identical to the core meaning of the subordinating constituent,without any contribution from the subordinated constituent.
To representthis, we would need to assume at least two different "sam" attributes, ora more complicated structure for the value of the "sem" attribute.Rhetor ical  coord inat ions:dcu \[~l: mscg((Xr 131) .....
~.
: mscg(o~,,.
13.
), sem: a & b & R(a,b)\]=> dcul \[~1: ~l ..... ~,,: %,, sere: a\]dcu2\[~l: 131 .....
~.
: 13., sem: ~.
x: R(x,b)\]A This rule parses semantic coordinations which involve anexplicitly indicated binary coordinating rhetorical relation R ("therefore,""thus," "accordingly'), <Ref.
Mann, Talmy> As in the subordination casedescribed before, the meaning of the relation is incorporated in thesemantics of the clause in which it occurs, which therefore denotes apredicate on propositions.The function "mscg" computes the "most specific commongeneralization" of its arguments in the hierarchy of value-expressions ofthe relevant attribute.
(When there is no proper hierarchy defined on thevalue-expressions of an attribute, mscg degenerates into a functionwhich yields the value of its arguments when the two arguments areequal and which yields a new free variable when they are not.
)dcu \[~1: mscg(~xl, 131) .....
~.
: mscg(o~., 13,,), sam: a & b & R(a,b)\]=> dcu\[~l: o~!
.....
~.
: ~., sem: a\]dcu \[~j: ~1 .....
~.
: ~., sere: b\]This rule parses binary semantic coordinations which are not overtlymarked as such, Therefore, the semantics of the second dcu is aproposition rather than a predicate on propositions, The variable Rranges over all binary coordinating rhetorical relations.
As in thecorresponding subordination case, the value of R must be computed byabduction, magic, or a similar A.I.
technique.Top ic -dominant  chaining:dcul \[~1:?~1 .....
~.
: o~, index: i, sem: st(y)(x) & \[s2(Y)\]i\]=> dcu1\[~l:o~ I .....
~.
:o~., index:Lsem:sl(y)(x)\]dcu2\[~F ~1 ..... ~,,: ~,,, ~em: s2(y)\](pop-marker)Topic-dominant ,chatnlngs are subordination structures.
In thesestructures, the subordinated dcu gives information about a constituent ofthe predicate in the semantics of the subordinating dcu.
The aboverulerequires that there exists an element y such that the predicate of the leftdcu and the semantics of the right dcu can both be formulated asexpressions with the structure f(y) -- where, as before, only a limitedeNota that most other ules implicitly assume that hey operate on dcu's with a "sem ''~value which is a proposition, In the current formulation, they therefore do not operatecorrectly on explicitly subordinated clauses.
The appropriate r finements are not difficult toimagine, but go beyond the limited scope of the present paper.575repertoire of logical transformations can be used to achieve thisformulation, starting from formulas which correspond directly to theSurface structure of the dcu's.
These limitations are to be defined in sucha way that the possible values of y correspond to the constituents eligiblefor dominance in \[6}, or the forward-/ooking centers of \[9\].The heuristics of the parsing process prefers applying rules forconstructing list-structures to the rule for topic-dominant chaining?
(Cf.\[3\])Adjacency Pairs:QA \[sem: a(b)\]=> c/cut \[mood: interrogative, sem: b\]dcu2 \[sem: a(b)\](pop-marker)This rule parses question/answer pairs.
The semantics of a yes/noquestion is assumed to be a proposition; the semantics of a wh-questionis assumed to be a set-denoting expression (cf.
\[31\]).
The semantics ofan answer is a predication on the question-semantics.RR \[sem: a(b)\]=> dcul\[mood: request, speaker:pl, addressee:p2, sere:b\]dcu2 \[speaker:p2, addressee:p1, sem:a(b)\](pop-marker)This rule parses request/response pairs.
Semantically, these arevery similar to question/answer pairs.
We have chosen to exclude"rhetorical requests" by requiring the speaker/addressee relation to flipbetween request and response.Interruptions:tic.
1\[~,\]=> dcu 1\[o~\](interruption-marker)dcu2 \[J3\](pop-marker)This rule allows for semantically unrelated interruptions of anongoing discourse (cf.
\[26\] [10\] )interruptions may be introduced byspecific markers such as "Oh!".4.
Discourse ParsingWe consider the development of a formal grammar of discoursestructure, such as the one sketched in the previous section, to be the firststep towards a formal account of the process of discourse paining.
Wenow briefly review some of the issues that would be involved in such anaccount.The most important issue in discourse parsing is the necessity ofsemi-determinism.
Spontaneous dialogue involves unpremeditated turn-taking and interruption.
In order for this to be possible, there mustregularly be points in an interaction at which the interpretation ofutterances so far is mutually established as independent of the discoursewhich is to follow.
Moreover, an unanticipated next utterance canoperate in a mutually understood way on the structure of the discourseso far (for instance, by abandoning a digression to pop to a previouslyinterrupted dcu).
Therefore, at points where such a move is allowed,there must also be mutual agreement on the structure of the discourseso far.The granularity of this "unpredictability without misunderstanding"seems to be the clause or sentence level?
We therefore postulate anincremental left-to-right parsing process at this level of granularity, whichoperates in essentially deterministic mode.
In \[24\] we gave an informaldescription of such a parsing process, which processes every incomingsentence incrementally, extending an existing discourse tree to the rightby node insertion.
An important assumption of the parsing process asdescribed there, is that at any point it only uses information on the rightedge of the existing discourse tree.
This means that interlocutors justneed to be.
aware of the stack of information which corresponds to thelabels on the right edge of the tree, rather than the complete details ofthe discourse that went befOre.Inspection of the grammar rules in the previous section suggeststhat this grammar Is compatible with the parsing strategy outlined in \[24\]:relevant information is always propagated up to the right edge of thetree, and dcu interpretations get propagated up without being influencedby the nature of the intervening nodes.5.
Formalisms for Discourse Grammar.The augmented context free grammar developed above should betaken as a demonstration of the possibility and utility of formal grammarsfor decribing structural and semantic phenomena in natural languagediscourse.
We expect,that work of this kind, especially if carried out on alarger scale, will constitute a more fruitful path to new insights thanapproaches which are .oriented towards essayistic description orunprincipled implementation..576We should not make exaggerated claims concerning the formalismwe have used here.
Much more work is needed before it will be clearwhat kind of formal framework has the best fit with the phenomena.
Butit is probably useful to articulate our thoughts on how the augmentedcontext free grammar formalism compares to other formalisms that couldhave been used in this work.The formalism we have used is a context free grammar augmentedwith attributes, which propagates feature values through term unificationon value-expressions containing variables.
Similar formalisms have beenused for sentence-level syntax.
The closest is the ACFG formalism usedfor the grammar of BBN's Spoken Language System\[11\] \[2\], whichmainly differs in assuming a more limited syntax of attribute-valueexpressions.
Definite Clause Grammars \[21\], if decoupled from theircommitment o Prolog programming, are also very similar.Generalized Phrase Structure Grammar\[7\], as well as relatedtheories such as HPSG and LFG, share many aspects of our approach:ar~ emphasis on context free surface structure, and the use of aunification process to enforce the desired agreement and inheritancebehaviour on the values of attributes.
However, these frameworks useunification on graphs rather than logical term-expressions.
This probablycreates additional expressive power, but it goes at the cost of ease ofimplementation and of conceptual clarity.It is a major advantage of logical term unification that there is anobvious and simple semantics for it: any object which may containvariables, be it an attribute value, a syntactic tree, or a context free rule,can be viewed as an abbreviation for the set of its ground instances;applying the most general unifier to a set of terms yields a term standingfor the intersection of the sets of their ground instances.
Compared to theconceptual and computational simplicity of logical term unification, thegraph unification formalisms used in modern linguistic frameworks arerather cumbersome.
There do not seem to be good linguistic reasoris forpreferring graph unification.An interesting perspective on the kind of grammar we have usedfollows from the realization that it can be viewed as a particular instanceof an attribute grammar as defined by Knuth\[16\]: one in which thevalues of the attributes on a node:are always synthesized from thevalues on the nodes of its immediate constituents.
This raises thequesti0n whether one might want to formulate this directionaldependency explicitly in an attribute grammar notation.
Moreinterestingly, it raises the question whether there are phenomena thatcould be more elegantly described as inheritance from a top node to itsconstituents, rather than the other way around.
We expect suchphenomena to occur if we would want to Integrate more globalconstraintsl related to people's tasks, goals and plans, into ourdescription of discourse.Finally, we want to reflect on the Augmented Transition Networkformulation of discourse structure that we used in previous, moreinformal papers \[23}.
ATNs do have some properties which are attractivefor discourse.
A looping arc which updates a register constitutes apowerful device that doesn't have a direct equivalent in other formalisms.In the current grammar We emulate the effect of such an arc by arecursive binary structure: lists and narratives are built up by repeatedextension to the right.
Intuitively, one sees lists and narratives as flatstructures, and we have described them in those terms in previouspapers \[24\].
The power of ATNs thus makes it possible to account moredirectly for the structures that seem plausible.The framework used in the present paper is conceptually simple,and more limited than any of its alternatives.
Nevertheless, it seemspowerful enough to describe the phenomena we encountered indeveloping the grammar presented here.
Subsequent research will haveto answer the question whether It is ultimately powerful enough todescribe the full range of discourse phenomena in a felicitous way.In the present discussion we have ignored what in previous work wehave called the level of speech event structure, which is concerned withthe structure of discourse as a social activity.
\[22\] [23\] By the sametoken, we have left unaddressed the fact that discourse structures mayreflect the tasks, goals and plans of the discourse participants, as theywould be construed in A.I.
based approaches to discourse analysis.Observations reported in \[36\] show (1) that speech event structure andlinguistic discourse structure may be at odds with each other, and (2)that the linguistic discourse structure has the most direct semanticrelevance in such cases.
Though speech event structure and taskstructure have in fact considerable semantic relevance and mustultimately be factored In, we want to hold off on dealing with thecomplexities involved in this issue.The grammar presented in this paper provides a formalcharacterization of Discourse Parse Trees by means of the bottom-uprules used in its construction.
The problem of mating these bottom-uprules with necessary high-level, top-down rules involving phenomenaoccurring in the task domain or interaction is essentially the problem ofplan-recognition.
In an operational system, the functionality of alinguistically based discourse parser would thus be very much enhancedby an efficient Plan Recognizer of the type envisioned in "plan based"and "intention based" pragmatic discourse models \[1\], \[14\], \[18\], \[33\].
Onthe other hand, incorporating a discourse grammar would improve thefunctionality of existing plan based models, which lack explicitmechanisms for relating sentential syntax and semantics to pragmaticplan structures.AcknowledgmentsAndrew Haas and Robert Ingria asked the right questions about anearlier draft of this paper.
Andras Kornai made substantial technicalcontributions and was a source of support and stimulation throughoutthis enterprise.\[1\] Allen, James.Reco qnlzlng Intentions from Natural Language Utterances.In Brady, M., and Berwick, R. (editors), Computational Models ofDiscourse.
MIT Press, Cambridge, MA, 1983.\[2\] Ayuso, D., Y. Chow, A. Haas, R. Ingria, S. Roucos, R. Scha,D.
St~dlard.Integlation of Speech and Natural Language.Technical Report 6813, BBN Laboratories, Cambridge, MA, April,1988.\[3\] Brennan, Susan E.; Friedman, Marilyn W.; Pollard, Carl.A Centering Approach to Pronouns.In 25th Annual Meeting of the Association for ComputationalLinguistics, pages 155-162.
Stanford University, Stanford,CA, July, 1987.\[4\] de Bruin, Jos and Remko Scha.The Interpretation of Relational Nouns.In Proceedings of the 26th Annual Meeting of the ACL.
SUNY,Buffalo,NY, June, 1988.\[5\] Erteschik-Shir, N. and Lappin, S.Domiflance and the Functional Explanation of IslandPhenomena.Theoretical Linguistics, 6:1:41-86, 1979.\[6\] Gazdar, Gerald, Ewan Klein, Geoffrey K. Pullum, and IvanA.
Sa!
;I.Generalized Phrase Structure Grammar.Harvard University Press, Cambridge, Mass., 1985.\[7\] Grosz, Barbara \[Deutsch\].The Structure of Task Oriented Dialogs.In IEEE Symposium on Speech Recognition: Contributed Papers,pages 250-253.
Carnegie Mellon University ComputerScience Dept., Pittsburgh, PA, 1974.\[8\] Grosz, B.J., Joshi, A.K., Weinstein, S.Providing a Unified Account of Definite Noun Phrases inDiscourse.In Proceedings of the 21st Annual Meeting of the Association forComputational Linguistics, pages 44-50.
Association forComputational Linguistics, Cambridge, MA, June, 1983.\[9\] Grosz, B. and Sidner, C.Discourse Structure and the Proper Treatment of Interruptions.In Proceedings of the 9th International Joint Conference onAritificial Intelligence, 1985, pages 832-839.
IJCAI 1985, LosAngeles, CA, August 18-23, 1985.\[10\] Haas, Andrew.Parallel Parsing for Unification Grammar.In Proceedings of the lOth IJCAL Milan, Italy, August, 1987.\[11\] Hinrichs, E.Temporal Anaphora in Discourse of English.Linguistics and Philosophy 9 (1):63-82, 1986.\[12\] Hobbs, Jerry R.A Computational Approach to Discourse Analysis.Technical Report, SRI International, December, 1976.\[13\] Hobb.% J. and Evans, D.Conw~rsation as planned behavior.Cognitive Science 4(4):349-377, 1980.\[14\] Kamp, H.Events, Instants and Temporal Reference.In U. t-=gli and A. van Stechow (editors), Semantics from aMultiple Point of View, pages 376-471. de Gruyter, Berlin,1979.\[15\] Knuth, Donald E.Semantics of Context-Free Languages.Mathematical Systems Theory 2(2): 127-145, 1968.\[16\] Linde.
C.Focus of Attention and the Choice of Pronouns in Discourse.tn T. Given (editor), Syntax and Semantics, Vol.
12 of Discourseand Syntax, pages 337-354.
Academic Press, Inc., New York,New York, 1979.\[17\] Litman, Diane.Plan Recognition and Discourse Analysis: An IntegratedApproach for Understanding Dialogues.PhD thesis, University of Rochester, 1985.\[18\] Longacre, R.E.An Anatomy of Speech Notions.the Peter de Ridder Press, Lisse, 1976.\[19\] W.C. Mann and S.A. Thompson.Relational Propositions in Discourse.Technical Report RR-83-115, Information Sciences Institute,Marina del Rey, CA, November, 1983.\[20\] Pereira, Fernando C.N.
and David H.D.
Warren.Definite clause grammars for language analysis - a survey of theformalism and a comparison with augmented transitionnetworks.Artificial Intelligence 13:231-278, 1980.\[21\] Polanyi, Livia and Scha, Retake.Tl~e Syntax of Discourse.TEXT3:3:271-290, 1983.\[22\] Polanyi, L. and Scha, R.A Syntactic Approach to Discourse Semantics.In Proceedings of the international Conference on ComputationalLinguistics, pages 413-419.
Stanford University, Stanford,CA; 1984.\[23\] Polanyi, Livla.The Linguistic Discourse Model: Towards A Formal Theory ofDiscourse Structure.Technical Report 6409, BBN Labs, Cambridge:MA, November,1986.\[24\] Polanyi, L.A Formal Model of the Structure of Discourse.Journal of Pragmatics 2/3, 1988.\[25\] Polanyi, L.A Theory of Discourse Structure and Discourse Coherence.In 21st Regional Meeting of the Chicago Linguistic Society,pages 306-322.
Chicago Linguistic Society, University ofChicago, April, 1985.\[26\] Reichenbach, H.Elements of Symbolic Logic.London:Macmillan, 1947.\[27\] Robinson, J.A.A Machine-Oriented Logic Based on the Resolution Principle.Journal of the ACM 12(1), January, 1965.\[28\] Scha, R.J.H.Logical Foundations for Question Answering.Technical Report, Eindhoven: Philips Research Labs, M.S.12.331., 1983.\[29\] Sidner, C. L.Focusing in the Comprehension of Definite Anaphora.In Michael Brady and Robed C. Berwick (editors), ComputationalModels of Discourse, chapter 5, pages 267-330.
MIT Press,Cambridge, MA, 1983.\[30\] Sidner, C.L.What the Speaker Means: The Recognition of Speakers' Plans inDiscourse.International Journal of Computers and Mathematics, SpecialIssue in Computational Linguistics 9( 1 ):71-82, 1983.\[31\] Vendler, Z.Linguistics and Philosophy.Cornell University Press, Ithica, NY, 1967.\[32\] Verkuyl, H.On the Compositional Nature of the Aspects.D.
Reidel, Dordrecht, 1972.\[33\] de Witte, Llesbeth.Interacciones Habladas en una Zapateria Madrile~'a.
Un Estudiosobre la Estructura Sint~ctica y Sem~ntica del Discurso.Unpublished Master's Thesis, Spanish Department, University ofAmsterdam.1987577
