Ensemble Methods for Automatic Thesaurus ExtractionJames R. CurranInstitute for Communicating and Collaborative SystemsUniversity of Edinburgh2 Buccleuch Place, Edinburgh EH8 9LWUnited Kingdomjamesc@cogsci.ed.ac.ukAbstractEnsemble methods are state of the artfor many NLP tasks.
Recent work byBanko and Brill (2001) suggests that thiswould not necessarily be true if very largetraining corpora were available.
However,their results are limited by the simplic-ity of their evaluation task and individualclassifiers.Our work explores ensemble efficacy forthe more complex task of automatic the-saurus extraction on up to 300 millionwords.
We examine our conflicting resultsin terms of the constraints on, and com-plexity of, different contextual representa-tions, which contribute to the sparseness-and noise-induced bias behaviour of NLPsystems on very large corpora.1 IntroductionEnsemble learning is a machine learning techniquethat combines the output of several different classi-fiers with the goal of improving classification per-formance.
The classifiers within the ensemble maydiffer in several ways, such as the learning algorithmor knowledge representation used, or data they weretrained on.
Ensemble learning has been successfullyapplied to numerous NLP tasks, including POS tag-ging (Brill and Wu, 1998; van Halteren et al, 1998),chunking (Tjong Kim Sang, 2000), word sense dis-ambiguation (Pederson, 2000) and statistical pars-ing (Henderson and Brill, 1999).
Dietterich (2000)presents a good introduction to ensemble methods.Ensemble methods ameliorate learner bias byamortising individual classifier bias of over differ-ent systems.
For an ensemble to be more effec-tive than its constituents, the individual classifiersmust have better than 50% accuracy and must pro-duce diverse erroneous classifications (Dietterich,2000).
Brill and Wu (1998) call this complementarydisagreement complementarity.
Although ensem-bles are often effective on problems with small train-ing sets, recent work suggests this may not be true asdataset size increases.
Banko and Brill (2001) foundthat for confusion set disambiguation with corporalarger than 100 million words, the best individualclassifiers outperformed ensemble methods.One limitation of their results is the simplicity ofthe task and methods used to examine the efficacyof ensemble methods.
However, both the task andapplied methods are constrained by the ambitioususe of one billion words of training material.
Dis-ambiguation is relatively simple because confusionsets are rarely larger than four elements.
The indi-vidual methods must be inexpensive because of thecomputational burden of the massive training set, sothey must perform limited processing of the trainingcorpus and can only consider a fairly narrow contextsurrounding each instance.We explore the value of ensemble methods for themore complex task of automatic thesaurus extrac-tion, training on corpora of up to 300 million words.The increased complexity leads to results contradict-ing Banko and Brill (2001), which we explore usingensembles of different contextual complexity.
Thiswork emphasises the link between contextual com-plexity and the problems of representation sparse-ness and noise as corpus size increases, which inturn impacts on learner bias and ensemble efficacy.Association for Computational Linguistics.Language Processing (EMNLP), Philadelphia, July 2002, pp.
222-229.Proceedings of the Conference on Empirical Methods in Natural2 Automatic Thesaurus ExtractionThe development of large thesauri and semantic re-sources, such as WordNet (Fellbaum, 1998), has al-lowed lexical semantic information to be leveragedto solve NLP tasks, including collocation discov-ery (Pearce, 2001), model estimation (Brown et al,1992; Clark and Weir, 2001) and text classification(Baker and McCallum, 1998).Unfortunately, thesauri are expensive and time-consuming to create manually, and tend to sufferfrom problems of bias, inconsistency, and limitedcoverage.
In addition, thesaurus compilers cannotkeep up with constantly evolving language use andcannot afford to build new thesauri for the many sub-domains that NLP techniques are being applied to.There is a clear need for automatic thesaurus extrac-tion methods.Much of the existing work on thesaurus extrac-tion and word clustering is based on the observa-tions that related terms will appear in similar con-texts.
These systems differ primarily in their defi-nition of ?context?
and the way they calculate simi-larity from the contexts each term appears in.
Manysystems extract co-occurrence and syntactic infor-mation from the words surrounding the target term,which is then converted into a vector-space repre-sentation of the contexts that each target term ap-pears in (Pereira et al, 1993; Ruge, 1997; Lin,1998b).
Curran and Moens (2002b) evaluate the-saurus extractors based on several different modelsof context on large corpora.
The context modelsused in our experiments are described in Section 3.We define a context relation instance as a tuple(w, r,w?)
where w is a thesaurus term, occurring in arelation of type r, with another word w?
in the sen-tence.
We refer to the tuple (r,w?)
as an attributeof w. The relation type may be grammatical or itmay label the position of w?
in a context window:e.g.
the tuple (dog, direct-obj, walk) indicatesthat the term dog, was the direct object of the verbwalk.
After the contexts have been extracted fromthe raw text, they are compiled into attribute vec-tors describing all of the contexts each term appearsin.
The thesaurus extractor then uses clustering ornearest-neighbour matching to select similar termsbased on a vector similarity measure.Our experiments use k-nearest-neighbour match-(adjective, good) 2005(adjective, faintest) 89(direct-obj, have) 1836(indirect-obj, toy) 74(adjective, preconceived) 42(adjective, foggiest) 15Figure 1: Example attributes of the noun ideaing for thesaurus extraction, which calculates thepairwise similarity of the target term with every po-tential synonym.
Given n terms and up to m at-tributes for each term, the asymptotic time complex-ity of k-nearest-neighbour algorithm is O(n2m).
Wereduce the number of terms by introducing a mini-mum occurrence filter that eliminates potential syn-onyms with a frequency less than five.3 Individual MethodsThe individual methods in these ensemble experi-ments are based on different extractors of contex-tual information.
All the systems use the JACCARDsimilarity metric and TTEST weighting function thatwere found to be most effective for thesaurus extrac-tion by Curran and Moens (2002a).The simplest and fastest contexts to extract arethe word(s) surrounding each thesaurus term up tosome fixed distance.
These window methods are la-belled W(L1R1), where L1R1 indicates that windowextends one word on either side of the target term.Methods marked with an asterisk, e.g.
W(L1R1?
),do not record the word?s position in the relation type.The more complex methods extract grammaticalrelations using shallow statistical tools or a broadcoverage parser.
We use the grammatical relationsextracted from the parse trees of Lin?s broad cov-erage principle-based parser, MINIPAR (Lin, 1998a)and Abney?s cascaded finite-state parser, CASS (Ab-ney, 1996).
Finally, we have implemented our ownrelation extractor, based on Grefenstette?s SEXTANT(Grefenstette, 1994), which we describe below as anexample of the NLP system used to extract relationsfrom the raw text.Processing begins with POS tagging and NP/VPchunking using a Na?
?ve Bayes classifier trainedon the Penn Treebank.
Noun phrases separatedby prepositions and conjunctions are then concate-nated, and the relation attaching algorithm is run onthe sentence.
This involves four passes over the sen-Corpus Sentences WordsBritish National Corpus 6.2M 114MReuters Corpus Vol 1 8.7M 193MTable 1: Training Corpora Statisticstence, associating each noun with the modifiers andverbs from the syntactic contexts they appear in:1. nouns with pre-modifiers (left to right)2. nouns with post-modifiers (right to left)3. verbs with subjects/objects (right to left)4. verbs with subjects/objects (left to right)This results in relations representing the contexts:1. term is the subject of a verb2.
term is the (direct/indirect) object of a verb3.
term is modified by a noun or adjective4.
term is modified by a prepositional phraseThe relation tuple is then converted to root formusing the Sussex morphological analyser (Minnen etal., 2000) and the POS tags are stripped.
The re-lations for each term are collected together produc-ing a context vector of attributes and their frequen-cies in the corpus.
Figure 1 shows the most stronglyweighted attributes and their frequencies for idea.4 ExperimentsOur experiments use a large quantity of text whichwe have grouped into a range of corpus sizes.
Theapproximately 300 million word corpus is a randomconflation of the BNC and the Reuters corpus (re-spective sizes in Table 1).
We then create corpussubsets down to 1128 th (2.3 million words) of theoriginal corpus by randomly sentence selection.Ensemble voting methods for this task are quiteinteresting because the result consists of an orderedset of extracted synonyms rather than a single classlabel.
To test for subtle ranking effects we imple-mented three different methods of combination:MEAN mean rank of each term over the ensemble;HARMONIC the harmonic mean rank of each term;MIXTURE ranking based on the mean score foreach term.
The individual extractor scores arenot normalised because each extractor uses thesame similarity measure and weight function.We assigned a rank of 201 and similarity score ofzero to terms that did not appear in the 200 syn-onyms returned by the individual extractors.
Finally,we build ensembles from all the available extractormethods (e.g.
MEAN(?))
and the top three perform-ing extractors (e.g.
MEAN(3)).To measure the complementary disagreement be-tween ensemble constituents we calculated both thecomplementarity C and the Spearman rank-ordercorrelation Rs.C(A, B) = (1 ?
| errors(A) ?
errors(B)|| errors(A)| ) ?
100% (1)Rs(A, B) =?i(r(Ai) ?
r(A))(r(Bi) ?
r(B))?
?i(r(Ai) ?
r(A))2?
?i(r(Bi) ?
r(B))2(2)where r(x) is the rank of synonym x.
The Spearmanrank-order correlation coefficient is the linear corre-lation coefficient between the rankings of elementsof A and B. Rs is a useful non-parametric compari-son for when the rank order is more relevant than theactual values in the distribution.5 EvaluationThe evaluation is performed on thesaurus entries ex-tracted for 70 single word noun terms.
To avoidsample bias, the words were randomly selected fromWordNet such that they covered a range of values forthe following word properties:frequency Penn Treebank and BNC frequencies;number of senses WordNet and Macquarie senses;specificity depth in the WordNet hierarchy;concreteness distribution across WordNet subtrees.Table 2 shows some of the selected terms with fre-quency and synonym set information.
For each termwe extracted a thesaurus entry with 200 potentialsynonyms and their similarity scores.The simplest evaluation measure is direct com-parison of the extracted thesaurus with a manually-created gold standard (Grefenstette, 1994).
How-ever, on smaller corpora direct matching is often tooWord PTB Rank PTB # BNC # Reuters # Macquarie # WordNet # Min / Max WordNet subtree rootscompany 38 4076 52779 456580 8 9 3 / 6 entity, group, stateinterest 138 919 37454 146043 12 12 3 / 8 abs., act, group, poss., stateproblem 418 622 56361 63333 4 3 3 / 7 abs., psych., statechange 681 406 35641 55081 8 10 2 / 12 abs., act, entity, event, phenom.house 896 223 47801 45651 10 12 3 / 6 act, entity, groupidea 1227 134 32754 13527 10 5 3 / 7 entity, psych.opinion 1947 78 9122 16320 4 6 4 / 8 abs., act, psych.radio 2278 59 9046 20913 2 3 6 / 8 entitystar 5130 29 8301 6586 11 7 4 / 8 abs., entityknowledge 5197 19 14580 2813 3 1 1 / 1 psych.pants 13264 5 429 282 3 2 6 / 9 entitytightness 30817 1 119 2020 5 3 4 / 5 abs., stateTable 2: Examples of the 70 thesaurus evaluation termscoarse-grained and thesaurus coverage is a problem.To help overcome limited coverage, our evaluationuses a combination of three electronic thesauri: thetopic-ordered Macquarie (Bernard, 1990) and Ro-get?s (Roget, 1911) thesauri and the head orderedMoby (Ward, 1996) thesaurus.
Since the extractedthesaurus does not separate senses we transform Ro-get?s and Macquarie into head ordered format bycollapsing the sense sets containing the term.
For the70 terms we create a gold standard from the union ofthe synonym lists of the three thesauri, resulting in atotal of 23,207 synonyms.With this gold standard resource in place, it is pos-sible to use precision and recall measures to evaluatethe quality of the extracted thesaurus.
To help over-come the problems of coarse-grained direct com-parisons we use several measures of system per-formance: direct matches (DIRECT), inverse rank(INVR), and top n synonyms precision (P(n)).INVR is the sum of the inverse rank of eachmatching synonym, e.g.
gold standard matches atranks 3, 5 and 28 give an inverse rank score of13 +15 +128 ?
0.569.
With at most 200 synonyms,the maximum INVR score is 5.878.
Top n precisionis the percentage of matching synonyms in the top nextracted synonyms.
We use n = 1, 5 and 10.6 ResultsFigure 2 shows the performance trends for the indi-vidual extractors on corpora ranging from 2.3 mil-lion up to 300 million words.
The best individ-ual context extractors are SEXTANT, MINIPAR andW(L1R1), with SEXTANT outperforming MINIPARbeyond approximately 200 million words.
These0 50 100 150 200 250 300 350Corpus Size (millions of words)800100012001400160018002000Direct MatchesCassMiniparSextantW(L1,2)W(L1R1)W(L1R1*)Figure 2: Single extractor performance to 300MWthree extractors are combined to form the top-threeensemble.
CASS and the other window methods per-form significantly worse than SEXTANT and MINI-PAR.
Interestingly, W(L1R1?)
performs almost aswell as W(L1R1) on larger corpora, suggesting thatposition information is not as useful with large cor-pora, perhaps because the left and right set of wordsfor each term becomes relatively disjoint.Table 3 presents the evaluation results for all theindividual extractors and the six ensembles on thefull corpus.
At 300 million words all of the ensemblemethods outperform the individual extractors.
Theseresults disagree with those Banko and Brill (2001)obtained for confusion set disambiguation.
The bestperforming ensembles, MIXTURE(?)
and MEAN(?
),combine the results from all of the individual ex-tractors.
MIXTURE(?)
performs approximately 5%better than SEXTANT, the best individual extractor.Figure 3 compares the performance behaviour overthe range of corpus sizes for the best three individ-System DIRECT P(1) P(5) P(10) INVRCASS 1483 50% 41% 33% 1.58MINIPAR 1703 59% 48% 40% 1.86SEXTANT 1772 61% 47% 39% 1.87W(L1,2) 1525 54% 43% 37% 1.68W(L1R1) 1623 57% 46% 38% 1.76W(L1R1?)
1576 63% 44% 38% 1.78MEAN(?)
1850 66% 50% 43% 2.00MEAN(3) 1802 63% 50% 44% 1.98HARMONIC(?)
1821 64% 51% 43% 2.00HARMONIC(3) 1796 63% 51% 43% 1.96MIXTURE(?)
1858 64% 52% 44% 2.03MIXTURE(3) 1794 63% 51% 44% 1.99Table 3: Extractor performance at 300MW0 50 100 150 200 250 300 350Corpus Size (millions of words)100012001400160018002000Direct MatchesMiniparSextantW(L1R1)Mean(*)Harmonic(*)Mixture(*)Figure 3: Ensemble performance to 300MWual methods and the full ensembles.
SEXTANT isthe only competitive individual method as the corpussize increases.
Figure 3 shows that ensemble meth-ods are of more value (at least in percentage terms)for smaller training sets.
The trend in the graph sug-gests that the individual extractors will not outper-form the ensemble methods, unless the behaviourchanges as corpus size is increased further.From Table 3 we can also see that full ensembles,combining all the individual extractors, outperformensembles combining only the top three extractors.This seems rather surprising at first, given that theother individual extractors seem to perform signifi-cantly worse than the top three.
It is interesting tosee how the weaker methods still contribute to theensembles performance.Firstly, for thesaurus extraction, there is no clearconcept of accuracy greater than 50% since it is nota simple classification task.
So, although most ofthe evaluation results are significantly less than 50%,Ensemble Rs CEnsemble(?)
on 2.3M words 0.467 69.2%Ensemble(3) on 2.3M words 0.470 69.8%Ensemble(?)
on 300M words 0.481 54.1%Ensemble(3) on 300M words 0.466 51.2%Table 4: Agreement between ensemble membersSystem CASS MINI SEXT W(L1,2 ) W(L1 R1) W(L1 R1?
)CASS 0% 58% 59% 65% 63% 69%MINI 57% 0% 47% 57% 54% 60%SEXT 58% 47% 0% 54% 53% 58%W(L1,2) 65% 58% 55% 0% 40% 43%W(L1R1) 63% 54% 54% 39% 0% 33%W(L1R1?)
69% 60% 58% 43% 33% 0%Table 5: Complementarity for extractorsthis does not represent a failure of a necessary condi-tion of ensemble improvement.
If we constrain the-saurus extraction to selecting a single synonym clas-sification using the P(1) scores, then all of the meth-ods achieve 50% or greater accuracy.
Consideringthe complementarity and rank-order correlation co-efficients for the constituents of the different ensem-bles proves to be more informative.
Table 4 showsthese values for the smallest and largest corpora andTable 5 shows the pairwise complementarity for theensemble constituents.It turns out that the average Spearman rank-ordercorrelation is not sensitive enough to errors forthe purposes of comparing favourable disagreementwithin ensembles.
However, the average comple-mentarity clearly shows the convergence of the en-semble constituents, which partially explains the re-duced efficacy of ensemble methods for large cor-pora.
Since the top-three ensembles suffer this to agreater degree, they perform significantly worse at300 million words.
Further, the full ensembles canamortise the individual biases better since they aver-age over a larger number of ensemble methods withdifferent biases.7 AnalysisUnderstanding ensemble behaviour on very largecorpora is important because ensemble classifiersare state of the art for many NLP tasks.
This sectionexplores possible explanations for why our resultsdisagree with Banko and Brill (2001).Thesaurus extraction and confusion set disam-biguation are quite different tasks.
In thesaurus ex-traction, contextual information is collected from theentire corpus into a single description of the environ-ments that each term appears in and classification, assuch, involves comparing these collections of data.In confusion set disambiguation on the other hand,each instance must be classified individually withonly a limited amount of context.
The disambiguatorhas far less information available to determine eachclassification.
This has implications for representa-tion sparseness and noise that a larger corpus helpsto overcome, which in turn, affects the performanceof ensemble methods against individual classifiers.The complexity of the contextual representationand the strength of the correlation between targetterm and the context also plays a significant role.Curran and Moens (2002b) have demonstrated thatmore complex and constrained contexts can yieldsuperior performance, since the correlation betweencontext and target term is stronger than simple win-dow methods.
Further, structural and grammaticalrelation methods can encode extra syntactic and se-mantic information in the relation type.
Althoughthe contextual representation is less susceptible tonoise, it is often sparse because fewer context rela-tions are extracted from each sentence.The less complex window methods exhibit the op-posite behaviour.
Depending on the window param-eters, the context relations can be poorly correlatedwith the target term, and so we find a very largenumber of irrelevant relations with low and unstablefrequency counts, that is, a noisy contextual repre-sentation.
Since confusion set disambiguation useslimited contexts from single occurrences, it is likelyto suffer the same problems as the window thesaurusextractors.To evaluate an ensemble?s ability to reduce thedata sparseness and noise problems suffered by dif-ferent context models, we constructed ensemblesbased on context extractors with different levels ofcomplexity and constraints.Table 6 shows the performance on the full cor-pus for the three syntactic extractors, the top threeperforming extractors and their corresponding meanrank ensembles.
For these more complex and con-strained context extractors, the ensembles continueto outperform individual learners, since the contextrepresentation are still reasonably sparse.
The aver-System DIRECT P(1) P(5) P(10) INVRCASS 1483 50% 41% 33% 1.58MINIPAR 1703 59% 48% 40% 1.86SEXTANT 1772 61% 47% 39% 1.87MEAN(P) 1803 60% 48% 42% 1.89W(L1R1) 1623 57% 46% 38% 1.76MINIPAR 1703 59% 48% 40% 1.86SEXTANT 1772 61% 47% 39% 1.87MEAN(3) 1802 63% 50% 44% 1.98Table 6: Complex ensembles better than individualsSystem DIRECT P(1) P(5) P(10) INVRW(L1) 1566 59% 42% 35% 1.70W(L2) 1235 44% 36% 31% 1.38W(R1) 1198 44% 28% 24% 1.19W(R2) 1200 49% 30% 24% 1.25MEAN(D1|2) 1447 54% 46% 37% 1.74W(L1,2) 1525 54% 43% 37% 1.68W(L1R1) 1623 57% 46% 38% 1.76W(R1,2) 1348 53% 32% 29% 1.40MEAN(D1,2) 1550 63% 46% 39% 1.81W(L1,2?)
1500 50% 41% 36% 1.60W(L1R1?)
1576 63% 44% 38% 1.78W(R1,2?)
1270 46% 29% 27% 1.28MEAN(D1,2?)
1499 64% 46% 39% 1.82Table 7: Simple ensembles worse than individualsage complementarity is greater than 50%.Table 7 shows the performance on the full cor-pus for a wide range of window-based extractorsand corresponding mean rank ensembles.
Most ofthe individual learners perform poorly because theextracted contexts are only weakly correlated withthe target terms.
Although the ensemble performsbetter than most individuals, they fail to outperformthe best individual on direct match evaluation.
Sincethe average complementarity for these ensembles issimilar to the methods above, we must conclude thatit is a result of the individual methods themselves.
Inthis case, the most correlated context extractor, e.g.W(L1R1), extracts a relatively noise free representa-tion which performs better than amortising the biasof the other noisy ensemble constituents.Finally, confusion set disambiguation yields a sin-gle classification from a small set of classes, whereasthesaurus extraction yields an ordered set contain-ing every potential synonym.
The more flexible setof ranked results allow ensemble methods to exhibitmore subtle variations in rank than simply selectinga single class.We can contrast the two tasks using the single syn-onym, P(1), and rank sensitive, INVR, evaluationmeasures.
The results for P(1) do not appear to formany trend, although the results show that ensemblemethods do not always improve single class selec-tion.
However, if we consider the INVR measure,all of the ensemble methods outperform their con-stituent methods, and we see a significant improve-ment of approximately 10% with the MEAN(3) en-semble.8 ConclusionThis paper demonstrates the effectiveness of ensem-ble methods for thesaurus extraction and investigatesthe performance of ensemble extractors on corporaranging up to 300 million words in size.
Contraryto work reported by Banko and Brill (2001), the en-semble methods continue to outperform the best in-dividual systems for very large corpora.
The trend inFigure 3 suggests that this may continue for corporaeven larger than we have experimented with.Further, this paper examines the differences be-tween thesaurus extraction and confusion set dis-ambiguation, and links ensemble efficacy to the na-ture of each task and the problems of representationsparseness and noise.
This is done by evaluating en-sembles with varying levels of contextual complex-ity and constraints.The poorly constrained window methods, wherecontextual correlation is often low, outperformedthe ensembles, which parallels results from (Bankoand Brill, 2001).
This suggests that large train-ing sets ameliorate the predominantly noise-inducedbias of the best individual learner better than amor-tising the bias over many similar ensemble con-stituents.
Noise is reduced as occurrence counts sta-bilise with larger corpora, improving individual clas-sifier performance, which in turn causes ensembleconstituents to converge, reducing complementarity.This reduces the efficacy of classifier combinationand contributes to individual classifiers outperform-ing the ensemble methods.For more complex, constrained methods the sameprinciples apply.
Since the correlation betweencontext and target is much stronger, there is lessnoise in the representation.
However, the addedconstraints reduce the number of contextual rela-tions extracted from each sentence, leading to datasparseness.
These factors combine so that ensemblemethods continued to outperform the best individualmethods.Finally, corpus size must be considered with re-spect to the parameters of the contextual representa-tion extracted from the corpus.
The value of largercorpora is partly dependent on how much informa-tion is extracted from each sentence of training ma-terial.
We fully expect individual thesaurus extrac-tors to eventually outperform ensemble methods assparseness and complementarity are reduced, butthis is not true for 100 or 300 million words sincethe best performing representations extract very fewcontexts per sentence.We would like to further investigate the relation-ship between contextual complexity, data sparse-ness, noise and learner bias on very large corpora.This includes extending these experiments to aneven larger corpus with the hope of establishing thecross over point for thesaurus extraction.
Finally, al-though wider machine learning research uses largeensembles, many NLP ensembles use only a handfulof classifiers.
It would be very interesting to exper-iment with a large number of classifiers using bag-ging and boosting techniques on very large corpora.AcknowledgementsWe would like to thank Miles Osborne for initial dis-cussions which led to this work, and Marc Moens,Steve Finch and Tara Murphy for their feedback ondrafts of this paper.
This research is supported bya Commonwealth scholarship and a Sydney Univer-sity Travelling scholarship.ReferencesSteve Abney.
1996.
Partial parsing via finite-state cas-cades.
Journal of Natural Language Engineering,2(4):337?344, December.L.
Douglas Baker and Andrew McCallum.
1998.
Distri-butional clustering of words for text classification.
InProceedings of the 21st annual international ACM SI-GIR conference on Research and Development in In-formation Retrieval, pages 96?103, Melbourne, Aus-tralia, 24?28 August.Michele Banko and Eric Brill.
2001.
Scaling to veryvery large corpora for natural language disambigua-tion.
In Proceedings of the 39th annual meeting of theAssociation for Computational Linguistics, pages 26?33, Toulouse, France, 9?11 July.John R. L. Bernard, editor.
1990.
The Macquarie Ency-clopedic Thesaurus.
The Macquarie Library, Sydney,Australia.Eric Brill and Jun Wu.
1998.
Classifier combination forimproved lexical disambiguation.
In Proceedings ofthe 17th International Conference on ComputationalLinguistics and of the 36th Annual Meeting of the As-sociation for Computational Linguistics, pages 191?195, Montre?al, Que?bec, Canada, 10?14 August.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,Jennifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?479, December.Stephen Clark and David Weir.
2001.
Class-based prob-ability estimation using a semantic hierarchy.
In Pro-ceedings of the Second Meeting of the North AmericanChapter of the Association for Computational Linguis-tics, pages 95?102, Pittsburgh, PA USA, 2?7 June.James R. Curran and Marc Moens.
2002a.
Improve-ments in automatic thesaurus extraction.
In ACL-SIGLEX Workshop on Unsupervised Lexical Acquisi-tion, Philadelphia, PA USA, 12 July.
(to appear).James R. Curran and Marc Moens.
2002b.
Scaling con-text space.
In Proceedings of the 40th annual meet-ing of the Association for Computational Linguistics,Philadelphia, PA USA, 7?12 July.
(to appear).Thomas G. Dietterich.
2000.
Ensemble methods inmachine learning.
In Proceedings of the First In-ternational Workshop on Multiple Classifier Systems(LNCS 1857), pages 1?15.
Springer-Verlag, Cagliari,Sardinia, Italy.Cristiane Fellbaum, editor.
1998.
WordNet: an elec-tronic lexical database.
The MIT Press, Cambridge,MA USA.Gregory Grefenstette.
1994.
Explorations in AutomaticThesaurus Discovery.
Kluwer Academic Publishers,Boston, USA.John C. Henderson and Eric Brill.
1999.
Exploitingdiversity in natural language processing: Combiningparsers.
In Proceedings of the Fourth Conference onEmpirical Methods in Natural Language Processing(EMNLP-99), pages 187?194, College Park, Mary-land, USA.Dekang Lin.
1998a.
Dependency-based evaluation ofMINIPAR.
In Workshop on the Evaluation of ParsingSystems, Proceedings of the First International Con-ference on Language Resources and Evaluation, pages234?241, Granada, Spain, 28?30 May.Dekang Lin.
1998b.
An information-theoretic definitionof similarity.
In Proceedings of the Fifteen Interna-tional Conference on Machine Learning, pages 296?304, Madison, WI USA, 24?27 July.Guido Minnen, John Carroll, and Darren Pearce.
2000.Robust applied morphological generation.
In In Pro-ceedings of the First International Natural LanguageGeneration Conference, pages 201?208, 12?16 June.Darren Pearce.
2001.
Synonymy in collocation extrac-tion.
In Workshop on WordNet and Other LexicalResources: Applications, Extensions and Customiza-tions, (NAACL 2001), pages 41?46, Pittsburgh, PAUSA, 2?7 June.Ted Pederson.
2000.
A simple approach to building en-sembles of naive bayesian classifiers for word sensedisambiguation.
In Proceedings of the 6th AppliedNatural Language Processing Conference and the 1stMeeting of the North American Chapter of the As-sociation of Computational Linguistics, pages 63?69,Seattle, WA USA, 29 April?4 May.Fernando Pereira, Naftali Tishby, and Lillian Lee.
1993.Distributional clustering of English words.
In Pro-ceedings of the 31st annual meeting of the Associ-ation for Computational Linguistics, pages 183?190,Columbus, Ohio USA, 22?26 June.Peter Roget.
1911.
Thesaurus of English words andphrases.
Longmans, Green and Co., London, UK.Gerda Ruge.
1997.
Automatic detection of thesaurus re-lations for information retrieval applications.
In Foun-dations of Computer Science: Potential - Theory -Cognition, Lecture Notes in Computer Science, vol-ume LNCS 1337, pages 499?506.
Springer Verlag,Berlin, Germany.Erik F. Tjong Kim Sang.
2000.
Noun phrase recog-nition by system combination.
In Proceedings ofthe Language Technology Joint Conference ANLP-NAACL2000, pages 50?55, Seattle, Washington, USA,29 April?4 May.Hans van Halteren, Jakub Zavrel, and Walter Daelemans.1998.
Improving data driven wordclass tagging bysystem combination.
In Proceedings of the 17th In-ternational Conference on Computational Linguisticsand of the 36th Annual Meeting of the Association forComputational Linguistics, pages 491?497, Montre?al,Que?bec, Canada, 10?14 August.Grady Ward.
1996.
Moby Thesaurus.
Moby Project.
