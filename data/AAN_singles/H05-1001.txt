Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 1?8, Vancouver, October 2005. c?2005 Association for Computational LinguisticsImproving LSA-based Summarization with Anaphora ResolutionJosef SteinbergerUniversity of West BohemiaUniverzitni 22, Pilsen 30614,Czech Republicjstein@kiv.zcu.czMijail A. KabadjovUniversity of EssexWivenhoe Park, Colchester CO4 3SQ,United Kingdommalexa@essex.ac.ukMassimo PoesioUniversity of EssexWivenhoe Park, Colchester CO4 3SQ,United Kingdompoesio@essex.ac.ukOlivia Sanchez-GrailletUniversity of EssexWivenhoe Park, Colchester CO4 3SQ,United Kingdomosanch@essex.ac.ukAbstractWe propose an approach to summarizationexploiting both lexical information andthe output of an automatic anaphoric re-solver, and using Singular Value Decom-position (SVD) to identify the main terms.We demonstrate that adding anaphoricinformation results in significant perfor-mance improvements over a previouslydeveloped system, in which only lexicalterms are used as the input to SVD.
How-ever, we also show that how anaphoric in-formation is used is crucial: whereas usingthis information to add new terms does re-sult in improved performance, simple sub-stitution makes the performance worse.1 IntroductionMany approaches to summarization can be verybroadly characterized as TERM-BASED: they at-tempt to identify the main ?topics,?
which gen-erally are TERMS, and then to extract from thedocument the most important information aboutthese terms (Hovy and Lin, 1997).
These ap-proaches can be divided again very broadly in ?lex-ical?
approaches, among which we would includeLSA-based approaches, and ?coreference-based?
ap-proaches .
Lexical approaches to term-based sum-marization use lexical relations to identify cen-tral terms (Barzilay and Elhadad, 1997; Gong andLiu, 2002); coreference- (or anaphora-) based ap-proaches (Baldwin and Morton, 1998; Boguraev andKennedy, 1999; Azzam et al, 1999; Bergler et al,2003; Stuckardt, 2003) identify these terms by run-ning a coreference- or anaphoric resolver over thetext.1 We are not aware, however, of any attempt touse both lexical and anaphoric information to iden-tify the main terms.
In addition, to our knowledge noauthors have convincingly demonstrated that feed-ing anaphoric information to a summarizer signif-icantly improves the performance of a summarizerusing a standard evaluation procedure (a referencecorpus and baseline, and widely accepted evaluationmeasures).In this paper we compare two sentence extraction-based summarizers.
Both use Latent SemanticAnalysis (LSA) (Landauer, 1997) to identify themain terms of a text for summarization; however,the first system (Steinberger and Jezek, 2004), dis-cussed in Section 2, only uses lexical informationto identify the main topics, whereas the second sys-tem exploits both lexical and anaphoric information.This second system uses an existing anaphora reso-lution system to resolve anaphoric expressions, GUI-TAR (Poesio and Kabadjov, 2004); but, crucially,two different ways of using this information forsummarization were tested.
(Section 3.)
Both sum-marizers were tested over the CAST corpus (Orasanet al, 2003), as discussed in Section 4, and sig-1The terms ?anaphora resolution?
and ?coreference resolu-tion?
have been variously defined (Stuckardt, 2003), but the lat-ter term is generally used to refer to the coreference task as de-fined in MUC and ACE.
We use the term ?anaphora resolution?
torefer to the task of identifying successive mentions of the samediscourse entity, realized via any type of noun phrase (propernoun, definite description, or pronoun), and whether such dis-course entities ?refer?
to objects in the world or not.1nificant improvements were observed over both thebaseline CAST system and our previous LSA-basedsummarizer.2 An LSA-based Summarizer UsingLexical Information OnlyLSA (Landauer, 1997) is a technique for extractingthe ?hidden?
dimensions of the semantic representa-tion of terms, sentences, or documents, on the basisof their contextual use.
It is a very powerful tech-nique already used for NLP applications such as in-formation retrieval (Berry et al, 1995) and text seg-mentation (Choi et al, 2001) and, more recently,multi- and single-document summarization.The approach to using LSA in text summariza-tion we followed in this paper was proposed in(Gong and Liu, 2002).
Gong and Liu propose tostart by creating a term by sentences matrix A =[A1, A2, .
.
.
, An], where each column vector Ai rep-resents the weighted term-frequency vector of sen-tence i in the document under consideration.
If thereare a total of m terms and n sentences in the docu-ment, then we will have an m ?
n matrix A for thedocument.
The next step is to apply Singular ValueDecomposition (SVD) to matrix A.
Given an m?
nmatrix A, the SVD of A is defined as:(1) A = U?V Twhere U = [uij ] is an m ?
n column-orthonormalmatrix whose columns are called left singular vec-tors, ?
= diag(?1, ?2, .
.
.
, ?n) is an n ?
n di-agonal matrix, whose diagonal elements are non-negative singular values sorted in descending order,and V = [vij ] is an n?n orthonormal matrix, whosecolumns are called right singular vectors.From a mathematical point of view, applyingSVD to a matrix derives a mapping between the m-dimensional space spawned by the weighted term-frequency vectors and the r-dimensional singularvector space.
From a NLP perspective, what the SVDdoes is to derive the latent semantic structure of thedocument represented by matrix A: a breakdownof the original document into r linearly-independentbase vectors (?topics?).
Each term and sentence fromthe document is jointly indexed by these ?topics?.A unique SVD feature is that it is capable of cap-turing and modelling interrelationships among termsso that it can semantically cluster terms and sen-tences.
Furthermore, as demonstrated in (Berry etal., 1995), if a word combination pattern is salientand recurring in document, this pattern will be cap-tured and represented by one of the singular vec-tors.
The magnitude of the corresponding singularvalue indicates the importance degree of this patternwithin the document.
Any sentences containing thisword combination pattern will be projected alongthis singular vector, and the sentence that best repre-sents this pattern will have the largest index valuewith this vector.
As each particular word combi-nation pattern describes a certain topic in the doc-ument, each singular vector can be viewed as repre-senting a salient topic of the document, and the mag-nitude of its corresponding singular value representsthe degree of importance of the salient topic.The summarization method proposed by Gongand Liu (2002) should now be easy to understand.The matrix V T describes the importance degree ofeach ?implicit topic?
in each sentence: the summa-rization process simply chooses the most informa-tive sentence for each term.
In other words, the kthsentence chosen is the one with the largest indexvalue in the kth right singular vector in matrix V T .The summarization method proposed by Gongand Liu has some disadvantages as well, the main ofwhich is that it is necessary to use the same numberof dimensions as is the number of sentences we wantto choose for a summary.
However, the higher thenumber of dimensions of reduced space is, the lesssignificant topic we take into a summary.
In orderto remedy this problem, we (Steinberger and Jezek,2004) proposed the following modifications to Gongand Liu?s summarization method.
After computingthe SVD of a term by sentences matrix, we computethe length of each sentence vector in matrix V .
Thisis to favour the index values in the matrix V thatcorrespond to the highest singular values (the mostsignificant topics).
Formally:(2) sk =?
?ri=1 v2k,i ?
?2i ,where sk is the length of the vector of k?th sentencein the modified latent vector space, and its signif-icance score for summarization too.
The level ofdimensionality reduction (r) is essentially learnedfrom the data.
Finally, we put into the summary thesentences with the highest values in vector s. Weshowed in previous work (Steinberger and Jezek,22004) that this modification results in a significantimprovement over Gong and Liu?s method.3 Using Anaphora Resolution forSummarization3.1 The case for anaphora resolutionWords are the most basic type of ?term?
that canbe used to characterize the content of a document.However, being able to identify the most importantobjects mentioned in the document clearly wouldlead to an improved analysis of what is important ina text, as shown by the following news article citedby Boguraev and Kennedy (1999):(3) PRIEST IS CHARGED WITH POPE ATTACKA Spanish priest was charged here today with attempt-ing to murder the Pope.
Juan Fernandez Krohn, aged32, was arrested after a man armed with a bayonet ap-proached the Pope while he was saying prayers at Fa-tima on Wednesday night.
According to the police, Fer-nandez told the investigators today that he trained forthe past six months for the assault.
.
.
.
If found guilty,the Spaniard faces a prison sentence of 15-20 years.As Boguraev and Kennedy point out, the title of thearticle is an excellent summary of the content: an en-tity (the priest) did something to another entity (thepope).
Intuitively, understanding that Fernandez andthe pope are the central characters is crucial to pro-vide a good summary of texts like these.2 Amongthe clues that help us to identify such ?main charac-ters?, the fact that an entity is repeatedly mentionedis clearly important.Purely lexical methods, including the LSA-basedmethods discussed in the previous section, can onlycapture part of the information about which enti-ties are frequently repeated in the text.
As exam-ple (3) shows, stylistic conventions forbid verbatimrepetition, hence the six mentions of Fernandez inthe text above contain only one lexical repetition,?Fernandez?.
The main problem are pronouns, thattend to share the least lexical similarity with theform used to express the antecedent (and anyway areusually removed by stopword lists, therefore do not2It should be noted that for many newspaper articles, indeedmany non-educational texts, only a ?entity-centered?
structurecan be clearly identified, as opposed to a ?relation-centered?structure of the type hypothesized in Rhetorical Structures The-ory (Knott et al, 2001; Poesio et al, 2004).get included in the SVD matrix).
The form of defi-nite descriptions (the Spaniard) doesn?t always over-lap with that of their antecedent, either, especiallywhen the antecedent was expressed with a propername.
The form of mention which more often over-laps to a degree with previous mentions is propernouns, and even then at least some way of dealingwith acronyms is necessary (cfr.
European Union/ E.U.).
The motivation for anaphora resolution isthat it should tell us which entities are repeatedlymentioned.In this work, we tested a mixed approach to in-tegrate anaphoric and word information: using theoutput of the anaphoric resolver GUITAR to modifythe SVD matrix used to determine the sentences toextract.
In the rest of this section we first briefly in-troduce GUITAR, then discuss the two methods wetested to use its output to help summarization.3.2 GUITAR: A General-Purpose AnaphoricResolverThe system we used in these experiments, GUITAR(Poesio and Kabadjov, 2004), is an anaphora resolu-tion system designed to be high precision, modular,and usable as an off-the-shelf component of a NLprocessing pipeline.
The current version of the sys-tem includes an implementation of the MARS pro-noun resolution algorithm (Mitkov, 1998) and a par-tial implementation of the algorithm for resolvingdefinite descriptions proposed by Vieira and Poe-sio (2000).
The current version of GUITAR does notinclude methods for resolving proper nouns.3.2.1 Pronoun ResolutionMitkov (1998) developed a robust approach topronoun resolution which only requires input textto be part-of-speech tagged and noun phrases to beidentified.
Mitkov?s algorithm operates on the ba-sis of antecedent-tracking preferences (referred tohereafter as ?antecedent indicators?).
The approachworks as follows: the system identifies the nounphrases which precede the anaphor within a distanceof 2 sentences, checks them for gender and numberagreement with the anaphor, and then applies genre-specific antecedent indicators to the remaining can-didates (Mitkov, 1998).
The noun phrase with thehighest aggregate score is proposed as antecedent.33.2.2 Definite Description ResolutionThe Vieira / Poesio algorithm (Vieira and Poesio,2000) attempts to classify each definite descriptionas either direct anaphora, discourse-new, or bridg-ing description.
The first class includes definite de-scriptions whose head is identical to that of their an-tecedent, as in a house .
.
.
the house.
Discourse-new descriptions are definite descriptions that referto objects not already mentioned in the text and notrelated to any such object.
Bridging descriptions areall definite descriptions whose resolution dependson knowledge of relations between objects, such asdefinite descriptions that refer to an object relatedto an entity already introduced in the discourse bya relation other than identity, as in the flat .
.
.
theliving room.
The Vieira / Poesio algorithm also at-tempts to identify the antecedents of anaphoric de-scriptions and the anchors of bridging ones.
Thecurrent version of GUITAR incorporates an algorithmfor resolving direct anaphora derived quite directlyfrom Vieira / Poesio, as well as a statistical versionof the methods for detecting discourse new descrip-tions (Poesio et al, 2005).3.3 SVD over Lexical and Anaphoric TermsSVD can be used to identify the ?implicit topics?
ormain terms of a document not only when on the basisof words, but also of coreference chains, or a mix-ture of both.
We tested two ways of combining thesetwo types of information.3.3.1 The Substitution MethodThe simplest way of integrating anaphoric in-formation with the methods used in our earlierwork is to use anaphora resolution simply as a pre-processing stage of the SVD input matrix creation.Firstly, all anaphoric relations are identified by theanaphoric resolver, and anaphoric chains are identi-fied.
Then a second document is produced, in whichall anaphoric nominal expressions are replaced bythe first element of their anaphoric chain.
For exam-ple, suppose we have the text in (4).
(4) S1: Australia?s new conservative government onWednesday began selling its tough deficit-slashing bud-get, which sparked violent protests by Aborigines,unions, students and welfare groups even before it wasannounced.S2: Two days of anti-budget street protests precededspending cuts officially unveiled by Treasurer PeterCostello.S3: ?If we don?t do it now, Australia is going to be indeficit and debt into the next century.
?S4: As the protesters had feared, Costello revealed acut to the government?s Aboriginal welfare commissionamong the hundreds of measures implemented to clawback the deficit.An ideal resolver would find 8 anaphoric chains:Chain 1 Australia - we - AustraliaChain 2 its new conservative government (Australia?s newconservative government) - the governmentChain 3 its tough deficit-slashing budget (Australia?s toughdeficit-slashing budget) - itChain 4 violent protests by Aborigines, unions, students andwelfare groups - anti-budget street protestsChain 5 Aborigines, unions, students and welfare groups - theprotestersChain 6 spending cuts - it - the hundreds of measures imple-mented to claw back the deficitChain 7 Treasurer Peter Costello - CostelloChain 8 deficit - the deficitBy replacing each element of the 8 chains abovein the text in (4) with the first element of the chain,we get the text in (5).
(5) S1: Australia?s new conservative government onWednesday began selling Australia?s tough deficit-slashing budget, which sparked violent protests by Abo-rigines, unions, students and welfare groups even be-fore Australia?s tough deficit-slashing budget was an-nounced.S2: Two days of violent protests by Aborigines, unions,students and welfare groups preceded spending cuts of-ficially unveiled by Treasurer Peter Costello.S3: ?If Australia doesn?t do spending cuts now, Aus-tralia is going to be in deficit and debt into the nextcentury.
?S4: As Aborigines, unions, students and welfaregroups had feared, Treasurer Peter Costello revealed acut to Australia?s new conservative government?s Abo-riginal welfare commission among the spending cuts.This text is then used to create the SVD input matrix,as done in the first system.3.3.2 The Addition MethodAn alternative approach is to use SVD to identify?topics?
on the basis of two types of ?terms?
: terms inthe lexical sense (i.e., words) and terms in the senseof objects, which can be represented by anaphoric4chains.
In other words, our representation of sen-tences would specify not only if they contain a cer-tain word, but also if they contain a mention of adiscourse entity (See Figure 1.)
This matrix wouldthen be used as input to SVD.Figure 1: Addition method.The chain ?terms?
tie together sentences that con-tain the same anaphoric chain.
If the terms arelexically the same (direct anaphors - like deficitand the deficit) the basic summarizer works suffi-ciently.
However, Gong and Liu showed that the bestweighting scheme is boolean (i.e., all terms have thesame weight); our own previous results confirmedthis.
The advantage of the addition method is theopportunity to give higher weights to anaphors.4 Evaluation4.1 The CAST CorpusTo evaluate our system, we used the corpus ofmanually produced summaries created by the CASTproject3 (Orasan et al, 2003).
The CAST cor-pus contains news articles taken from the ReutersCorpus and a few popular science texts from theBritish National Corpus.
It contains informationabout the importance of the sentences (Hasler etal., 2003).
Sentences are marked as essential or im-portant.
The corpus also contains annotations for3The goal of this project was to investigate to what extentComputer-Aided Summarization can help humans to producehigh quality summaries with less effort.linked sentences, which are not significant enoughto be marked as important/essential, but which haveto be considered as they contain information essen-tial for the understanding of the content of other sen-tences marked as essential/important.Four annotators were used for the annotation,three graduate students and one postgraduate.
Threeof the annotators were native English speakers, andthe fourth had advanced knowledge of English.
Un-fortunately, not all of the documents were annotatedby all of the annotators.
To maximize the reliabilityof the summaries used for evaluation, we chose thedocuments annotated by the greatest number of theannotators; in total, our evaluation corpus contained37 documents.For acquiring manual summaries at specifiedlengths and getting the sentence scores (for relativeutility evaluation) we assigned a score 3 to the sen-tences marked as essential, a score 2 to importantsentences and a score 1 to linked sentences.
Thesentences with highest scores are then selected forideal summary (at specified lenght).4.2 Evaluation MeasuresEvaluating summarization is a notoriously hardproblem, for which standard measures like Preci-sion and Recall are not very appropriate.
The mainproblem with P&R is that human judges often dis-agree what are the top n% most important sentencesin a document.
Using P&R creates the possibilitythat two equally good extracts are judged very dif-ferently.
Suppose that a manual summary containssentences [1 2] from a document.
Suppose also thattwo systems, A and B, produce summaries consist-ing of sentences [1 2] and [1 3], respectively.
Us-ing P&R, system A will be ranked much higher thansystem B.
It is quite possible that sentences 2 and 3are equally important, in which case the two systemsshould get the same score.To address the problem with precision and recallwe used a combination of evaluation measures.
Thefirst of these, relative utility (RU) (Radev et al,2000) allows model summaries to consist of sen-tences with variable ranking.
With RU, the modelsummary represents all sentences of the input doc-ument with confidence values for their inclusion inthe summary.
For example, a document with fivesentences [1 2 3 4 5] is represented as [1/5 2/4 3/45Evaluation Lexical LSA Manual ManualMethod Substitution AddititionRelative Utility 0.595 0.573 0.662F-score 0.420 0.410 0.489Cosine Similarity 0.774 0.806 0.823Main Topic Similarity 0.686 0.682 0.747Table 1: Evaluation of the manual annotation improvement - summarization ratio: 15%.Evaluation Lexical LSA Manual ManualMethod Substitution AdditionRelative Utility 0.645 0.662 0.688F-score 0.557 0.549 0.583Cosine Similarity 0.863 0.878 0.886Main Topic Similarity 0.836 0.829 0.866Table 2: Evaluation of the manual annotation improvement - summarization ratio: 30%.4/1 5/2].
The second number in each pair indicatesthe degree to which the given sentence should bepart of the summary according to a human judge.This number is called the utility of the sentence.Utility depends on the input document, the summarylength, and the judge.
In the example, the systemthat selects sentences [1 2] will not get a higher scorethan a system that chooses sentences [1 3] giventhat both summaries [1 2] and [1 3] carry the samenumber of utility points (5+4).
Given that no othercombination of two sentences carries a higher util-ity, both systems [1 2] and [1 3] produce optimalextracts.
To compute relative utility, a number ofjudges, (N ?
1) are asked to assign utility scores toall n sentences in a document.
The top e sentencesaccording to utility score4 are then called a sentenceextract of size e. We can then define the followingsystem performance metric:(6) RU =?nj=1 ?j?Ni=1 uij?nj=1 ?j?Ni=1 uij,where uij is a utility score of sentence j from anno-tator i, ?j is 1 for the top e sentences according to thesum of utility scores from all judges and ?j is equalto 1 for the top e sentences extracted by the system.For details see (Radev et al, 2000).The second measure we used is Cosine Similarity,according to the standard formula:(7) cos(X,Y ) =?i xi?yi??i(xi)2??
?i(yi)2,4In the case of ties, some arbitrary but consistent mecha-nism is used to decide which sentences should be included inthe summary.where X and Y are representations of a system sum-mary and its reference summary based on the vectorspace model.
The third measure is Main Topic Sim-ilarity.
This is a content-based evaluation methodbased on measuring the cosine of the angle betweenfirst left singular vectors of a system summary?sand its reference summary?s SVDs.
(For details see(Steinberger and Jezek, 2004).)
Finally, we mea-sured ROUGE scores, with the same settings as in theDocument Understanding Conference (DUC) 2004.4.3 How Much May Anaphora ResolutionHelp?
An Upper BoundWe annotated all the anaphoric relations in the 37documents in our evaluation corpus by hand us-ing the annotation tool MMAX (Mueller and Strube,2003).5 Apart from measuring the performance ofGUITAR over the corpus, this allowed us to establishthe upper bound on the performance improvementsthat could be obtained by adding an anaphoric re-solver to our summarizer.
We tested both methodsof adding the anaphoric knowledge to the summa-rizer discussed above.
Results for the 15% and 30%ratios6 are presented in Tables 1 and 2.
The baselineis our own previously developed LSA-based sum-marizer without anaphoric knowledge.
The resultis that the substitution method did not lead to sig-nificant improvement, but the addition method did:5We annotated personal pronouns, possessive pronouns, def-inite descriptions and also proper nouns, who will be handled bya future GUITAR version.6We used the same summarization ratios as in CAST.6Evaluation Lexical LSA CAST GUITAR GUITARMethod Substitution AdditionRelative Utility 0.595 0.527 0.530 0.640F-score 0.420 0.348 0.347 0.441Cosine Similarity 0.774 0.726 0.804 0.805Main Topic Similarity 0.686 0.630 0.643 0.699Table 3: Evaluation of the GUITAR improvement - summarization ratio: 15%.Evaluation Lexical LSA CAST GUITAR GUITARMethod Substitution AddittionRelative Utility 0.645 0.618 0.626 0.678F-score 0.557 0.522 0.524 0.573Cosine Similarity 0.863 0.855 0.873 0.879Main Topic Similarity 0.836 0.810 0.818 0.868Table 4: Evaluation of the GUITAR improvement - summarization ratio: 30%.addition could lead to an improvement in RelativeUtility score from .595 to .662 for the 15% ratio, andfrom .645 to .688 for the 30% ratio.
Both of theseimprovements were significant by t-test at 95% con-fidence.4.4 Results with GUITARTo use GUITAR, we first parsed the texts using Char-niak?s parser (Charniak, 2000).
The output of theparser was then converted into the MAS-XML for-mat expected by GUITAR by one of the preproces-sors that come with the system.
(This step includesheuristic methods for guessing agreement features.
)Finally, GUITAR was ran to add anaphoric infor-mation to the files.
The resulting files were thenprocessed by the summarizer.GUITAR achieved a precision of 56% and a recallof 51% over the 37 documents.
For definite descrip-tion resolution, we found a precision of 69% anda recall of 53%; for possessive pronoun resolution,the precision was 53%, recall was 53%; for personalpronouns, the precision was 44%, recall was 46%.The results with the summarizer are presentedin Tables 3 and 4 (relative utility, f-score, cosine,and main topic).
The contribution of the differ-ent anaphora resolution components is addressed in(Kabadjov et al, 2005).
All versions of our summa-rizer (the baseline version without anaphora resolu-tion and those using substitution and addition) out-performed the CAST summarizer, but we have to em-phasize that CAST did not aim at producing a high-performance generic summarizer; only a system thatcould be easily used for didactical purposes.
How-ever, our tables also show that using GUITAR and theaddition method lead to significant improvementsover our baseline LSA summarizer.
The improve-ment in Relative Utility measure was significant byt-test at 95% confidence.
Using the ROUGE mea-sure we obtained improvement (but not significant).On the other hand, the substitution method did notlead to significant improvements, as was to be ex-pected given that no improvement was obtained with?perfect?
anaphora resolution (see previous section).5 Conclusion and Further ResearchOur main result in this paper is to show that usinganaphora resolution in summarization can lead tosignificant improvements, not only when ?perfect?anaphora information is available, but also whenan automatic resolver is used, provided that theanaphoric resolver has reasonable performance.
Asfar as we are aware, this is the first time that sucha result has been obtained using standard evaluationmeasures over a reference corpus.
We also showedhowever that the way in which anaphoric informa-tion is used matters: with our set of documents atleast, substitution would not result in significant im-provements even with perfect anaphoric knowledge.Further work will include, in addition to extend-ing the set of documents and testing the system withother collections, evaluating the improvement to beachieved by adding a proper noun resolution algo-rithm to GUITAR.7ReferencesS.
Azzam, K. Humphreys and R. Gaizauskas.
1999.
Usingcoreference chains for text summarization.
In Proceedingsof the ACL Workshop on Coreference.
Maryland.B.
Baldwin and T. S. Morton.
1998.
Dynamic coreference-based summarization.
In Proceedings of EMNLP.
Granada,Spain.R.
Barzilay and M. Elhadad.
1997.
Using lexical chains for textsummarization.
In Proceedings of the ACL/EACL Workshopon Intelligent Scalable Text Summarization.
Madrid, Spain.S.
Bergler, R. Witte, M. Khalife, Z. Li, and F. Rudzicz.2003.
Using Knowledge-poor Coreference Resolution forText Summarization.
In Proceedings of DUC.
Edmonton.M.
W. Berry, S. T. Dumais and G. W. O?Brien.
1995.
UsingLinear Algebra for Intelligent IR.
In SIAM Review, 37(4).B.
Boguraev and C. Kennedy.
1999.
Salience-based contentcharacterization of text documents.
In I. Mani and M. T.Maybury (eds), Advances in Automatic Text Summarization,MIT Press.
Cambridge, MA.E.
Charniak.
2000.
A maximum-entropy-inspired parser.
InProceedings of NAACL.
Philadelphia.F.
Y. Y. Choi, P. Wiemer-Hastings and J. D. Moore.
2001.
La-tent Semantic Analysis for Text Segmentation.
In Proceed-ings of EMNLP.
Pittsburgh.Y.
Gong and X. Liu.
2002.
Generic Text Summarization Us-ing Relevance Measure and Latent Semantic Analysis.
InProceedings of ACM SIGIR.
New Orleans.L.
Hasler, C. Orasan and R. Mitkov.
2003.
Building bettercorpora for summarization.
In Proceedings of Corpus Lin-guistics.
Lancaster, United Kingdom.E.
Hovy and C. Lin.
1997.
Automated text summarization inSUMMARIST.
In ACL/EACL Workshop on Intelligent Scal-able Text Summarization.
Madrid, Spain.M.
A. Kabadjov, M. Poesio and J. Steinberger.
2005.
Task-Based Evaluation of Anaphora Resolution: The Case ofSummarization.
In RANLP Workshop ?Crossing Barriersin Text Summarization Research?.
Borovets, Bulgaria.A.
Knott, J. Oberlander, M. O?Donnell, and C. Mellish.
2001.Beyond elaboration: The interaction of relations and focus incoherent text.
In Sanders, T., Schilperoord, J., and Spooren,W.
(eds), Text representation: linguistic and psycholinguisticaspects.
John Benjamins.T.
K. Landauer and S. T. Dumais.
1997.
A solution to Plato?sproblem: The latent semantic analysis theory of the acqui-sition, induction, and representation of knowledge.
In Psy-chological Review, 104, 211-240.R.
Mitkov.
1998.
Robust pronoun resolution with limitedknowledge.
In Proceedings of COLING.
Montreal.C.
Mueller and M. Strube.
2001.
MMAX: A Tool for the Anno-tation of Multi-modal Corpora.
In Proceedings of the IJCAIWorkshop on Knowledge and Reasoning in Practical Dia-logue Systems.
Seattle.C.
Orasan, R. Mitkov and L. Hasler.
2003.
CAST: a Computer-Aided Summarization Tool.
In Proceedings of EACL.
Bu-dapest, Hungary.M.
Poesio and M. A. Kabadjov.
2004.
A General-Purpose, off-the-shelf Anaphora Resolution Module: Implementation andPreliminary Evaluation.
In Proceedings of LREC.
Lisbon,Portugal.M.
Poesio, R. Stevenson, B.
Di Eugenio, and J. M. Hitzeman.2004.
Centering: A parametric theory and its instantiations.Computational Linguistics, 30(3).M.
Poesio, M. A. Kabadjov, R. Vieira, R. Goulart, andO.
Uryupina.
2005.
Do discourse-new detectors help def-inite description resolution?
In Proceedings of IWCS.Tilburg, The Netherlands.D.
R. Radev, H. Jing, and M. Budzikowska.
2000.Centroid-based summarization of multiple documents.
InANLP/NAACL Workshop on Automatic Summarization.Seattle.J.
Steinberger and K. Jezek.
2004.
Text Summarization andSingular Value Decomposition.
In Proceedings of ADVIS.Izmir, Turkey.R.
Stuckardt.
2003.
Coreference-Based Summarization andQuestion Answering: a Case for High Precision AnaphorResolution.
In International Symposium on Reference Reso-lution.
Venice, Italy.R.
Vieira and M. Poesio.
2000.
An empirically-based systemfor processing definite descriptions.
In Computational Lin-guistics, 26(4).8
