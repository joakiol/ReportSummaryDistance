Multi-Modal Question-Answering: Questions without KeyboardsGary KacmarcikNatural Language Processing GroupMicrosoft Researchgarykac@microsoft.comAbstractThis paper describes our work to allowplayers in a virtual world to pose ques-tions without relying on textual input.Our approach is to create enhanced vir-tual photographs by annotating themwith semantic information from the 3Denvironment?s scene graph.
The playercan then use these annotated photos tointeract with inhabitants of the worldthrough automatically generated que-ries that are guaranteed to be relevant,grammatical and unambiguous.
Whilethe range of queries is more limitedthan a text input system would permit,in the gaming environment that we areexploring these limitations are offset bythe practical concerns that make textinput inappropriate.1 IntroductionThe question-posing part of Question-Answering (QA) has long relied on the coopera-tive nature of the person posing the question.This assumption is not unreasonable because itgenerally behooves the querent to assist the QAsystem wherever possible.However, even given this cooperative nature,QA systems that rely on text input still have todeal with input that is malformed, underspeci-fied or problematic in some other way.
Thisproblem is further compounded when the systemis open to users who may find it more entertain-ing to explore the boundaries, limitations andhumorous errors of the text input and parsingsystem instead of using the system as intended.Thus, any system that is intended to be releasedto a wide audience needs to be designed to han-dle these problems in a robust manner.Additionally, there are applications whereQA technologies would be beneficial, but thereliance on text input renders them impractical.The focus of the present work, interactive virtualgame worlds, is one such area where text inputis not desirable ?
both because it interrupts thegame flow and because many game systems donot have keyboards available.In this paper, we explore one method of cre-ating a non-text input mode for QA that relies onspecially annotated virtual photographs.Our approach is to create a virtual gameworld where all of the objects (and some non-objects) are annotated with semantic informationthat is constructed automatically by parsingnatural language text descriptions.
By interact-ing with world objects, a player is actually se-lecting portions of the semantic network that canin turn be used to enable a limited QA dialogwith denizens of the game world.
While thismethod is clearly not as flexible as full naturallanguage input, it successfully avoids most ofthe serious natural language input problems inmuch the same way that Tennent et al (1983)avoided the ambiguity, paraphrase and incorrectinput problems in their NLMENU system.
In ad-dition, our system does so without the awk-wardness of forcing players to build utterancesword-by-word from a series of menus.In our system, players interact with non-player characters (NPCs: characters in theworld whose actions are controlled by the com-puter) by taking virtual photographs of objectsin the world that they want to discuss and thenshowing the photos to the NPCs.
When thephoto is taken, all of the relevant semantic in-formation from the world is attached to thephoto so that it acts as a standalone object ?even if the game world changes, the contents ofthe photo are still valid and consistent.
By com-bining these photo annotations with informationin the NPC?s knowledgebase (KB), we can cre-ate the illusion that the NPC has a rudimentaryunderstanding of the photo contents and create anovel interaction modality that gives the player a167wide range of expression.It is worth noting that while we discuss usingthese annotations in the context of a virtualphoto, the annotations can also be applied inrealtime interactive systems.
In this work, werestrict ourselves to the use of virtual photosprimarily because it allows us to interact with astatic scene, thus eliminating the temporal diffi-culties (graphical and linguistic) that would becaused by interacting with a dynamic dataset.2 Previous WorkA wide variety of work has been done on inte-grating graphics and/or virtual environmentswith natural language dating back to Winograd?s(1972) classic ?blockworld?
simulation.
Morerecently, researchers have been investigatinghow graphics and natural language can worktogether to create more compelling interfaces.2.1 Multimodal InterfacesA large body of work has been created on mul-timodal interfaces ?
combining multiple modesof interaction so that the advantages of onemode offset the limitations of another.
In thespecific case of combining natural language andgraphics, there have been two main areas ofstudy: interacting with graphical elements toresolve ambiguous references on the natural lan-guage side (Bolt, 1980; Kobsa et al, 1986); andgenerating coordinated text and graphic presen-tations using information from a knowledgebase(Andr?
and Rist (1994); Towns et al (1998)).In addition to these two main areas, earlywork by Tennant (1983) experimented with us-ing a predictive left-corner parser to populatedynamic menus that the user would navigate toconstruct queries that were guaranteed to be cor-rect and task-relevant.Our work contains elements from all of thesecategories in that we use input gestures to re-solve reference ambiguity and we make use of aKB to coordinate the linguistic and graphicalinformation.
We were also inspired byTennant?s work on restricting the player?s inputto avoid parsing problems.
However, our workdiffers from previous efforts in that we:?
Do not use text input at runtime?
Use virtual cameras and input gestures for in-teraction?
Do not require that interactions be built oneunit (word or graphical references) at a time?
Focus primarily on text generation2.2 Virtual PhotographsThe concept of a virtual photograph has existedas long as people have taken screenshots of theirview into a 3D environment.
Recently, however,there have been a few applications that have ex-perimented with adding a limited amount of in-teractivity to these static images.
Video games,notably POK?MON SNAP (Nintendo, 1999), in-corporate a limited form of interactive virtualphotos.
While there is no published informationabout the techniques used in these games, wecan infer much by examining the level of inter-action permitted.In POK?MON SNAP, the player zooms aroundeach level on a rail car taking as many photo-graphs of ?wild?
pok?mon as possible.
Scoringin the game is based not only on the number ofunique subjects found (and successfully photo-graphed), but also on the quality of the individ-ual photographs.
The judging criteria include:?
Is the subject centered in the photo??
Is the face visible?
(for identifiability)?
Does the subject occupy a large percentageof the image??
Are there multiple pok?mon (same type)??
What is the subject doing?
(pose)In order to properly evaluate the photos, thegame must perform some photo annotationwhen the photo is taken.
However, since interac-tion with the photo is limited to scoring and dis-play, these annotations are easily reduced to theset of values necessary to calculate the score.From the players?
perspective, since there is nomechanism for interacting with the contents ofthe photo, all interaction is completed by thetime the photo is taken - the photo merely servesas an additional game object.2.3 Interactive ImagesRecently, a lot of work has gone on in the fieldof making images (including electronic versionsof real photographs) more interactive by manu-ally or automatically annotating image contentsor by making use of existing image metadata.The most commonly used example of this arethe HTML image maps (Berners-Lee and Con-nolly, 1995) supported by most web browsers.An example that is more relevant to our work168is the ALFRESCO system (Stock, 1991), whichuses graphical representations of Italian frescosand allows the user to query using a combina-tion of natural language and pointing gestures.Beyond the obvious difference that our systemdoesn?t permit direct natural language input, ourwork also differs in that we annotate the imageswith scene information beyond a simple objectID and we calculate the image regions automati-cally from the objects in the virtual world.3 Interacting with Virtual PhotosAs mentioned in the Introduction, virtual photoscan become a useful metaphor for interactionwith NPCs in games.
Ideally, the player shouldbe able to take a picture of anything in the vir-tual world and then show that photo to an NPCto engage in a dialog about the photo contents.In our implementation, the player interactswith the NPC by clicking on an object in thephoto to pull up a menu of context-dependentnatural language queries.
When the player se-lects an item from this menu, the query is sent tothe NPC that the player is currently ?talking to?.This menu of context sensitive queries is crucialto the interaction because a pointing gesturewithout an accompanying description is am-biguous (Schmauks, 1987) and it is through thismenu selection that the player expresses intentand restricts the scope of the dialog.There are two obvious benefits to approach-ing the QA interaction in this way.
First, eventhough the topic is limited by the objects in thephoto, the player is given control over the direc-tion of the dialog.
This is an improvement overthe traditional scripted NPC interaction wherethe player has little control over the dialog.
Theother benefit is that while the player is givencontrol over the content, the player is notgranted too much control since the photo meta-phor limits the topic to things that are relevant tothe game.
This effectively avoids the out-of-domain, paraphrase and ambiguity problems thatcommonly plague natural language interfaces.3.1 AnnotationsThe quality of player-NPC interaction is di-rectly dependent on the kind of annotations thatare used.
For example, associating a literal textstring with each object would result in a systemwhere the NPCs would not exhibit individualitysince they would all produce the exact same an-swer to a query.
Alternately, using a global ob-ject identifier would also cause problemsbecause in a dynamically changing world wewould need to create a system to keep track ofdifferences from object at the time of the photoand the object?s current state.It is for these reasons that we record for eachobject an abstract representation that we canmanipulate and merge with data from othersources like the NPC?s KB.
Beyond providing aplace to record information about the objectsthat are specific to a particular photo, this alsoallows us to individualize the NPC responsesand create a more interesting QA interaction.3.2  Example InteractionAs a simple example, imagine a photo taken bya player that shows a few houses in a town.
Tak-ing this photo to an NPC and clicking on one ofthe houses will bring up a menu of possiblequestions that is determined by the object andthe contents of the NPC?s KB.
Selecting the de-fault ?What is this??
query for an NPC that hasno special knowledge of the objects in this photowill result in the generic description (stored inthe photo) being used for the NPC?s response(e.g., ?That is a blue house?
).If, however, the NPC has some knowledgeabout the object, then the NPC will be able toprovide information beyond that provided withinthe photo.
Given the following information:This is John?s house.My name for John is my father.the NPC can piece it all together and generate?That is my father?s house?
as an answer.4 Representing KnowledgeA key component of our system is the semanticrepresentation that is used to encode not only theinformation that the NPC has about the sur-roundings, but also to encode the contents of thevirtual photo.
These KBs, which are createdfrom text documents containing natural lan-guage descriptions, form the core document seton which the QA process operates.4.1 Semantic RepresentationWhile there are a variety of representations thatcan be used to encode semantic information, weopted to use a representation that is automati-169cally extracted from natural language text.
Wechose this representation because we desired anotation that:?
Is easy to create?
Provides broad coverage over structuresfound in natural language?
Is easy to manipulate, and?
Is easy to convert into text for displayBecause of these requirements, we use a predi-cate-argument style representation (Campbelland Suzuki, 2002) that is produced by ourparser.
These structures, called logical forms(LFs), are the forms that are stored in the KB.This tree structure has many advantages.First, since it is based on our broad coveragegrammar it provides a reasonable representationfor all of the things that a player or NPC is likelyto want to talk about in a game.
We also arereadily able to generate output text from thisrepresentation by making use of our generationcomponent.
In addition, the fact that this repre-sentation is created directly from natural lan-guage input means that game designers cancreate these KBs without any special training inknowledge representation.Another advantage of this tree structure isthat it is easy to manipulate by copying subtreesfrom one tree into another.
Passing this manipu-lated tree to our generation component results inthe text output that is presented to the user.
Theease with which we can manipulate these struc-tures allows us to dynamically create new treesand provide the NPC with the ability to talkabout a wide array of subjects without having toauthor all of the interactions.4.2 AnaphoraAs mentioned, once these sentences for the KBhave been authored, our parser automaticallyhandles the work required to create the LFs fromthe text.
However, we do not have a fully auto-matic solution for the issue of reference resolu-tion or anaphora.
For this, we currently rely onthe person creating the KB to resolve referencesto objects within the text or KB (endophora) andin the virtual world (exophora).5 Posing QuestionsIn our system questions are posed by first nar-rowing down the scope of the query by selectingan object in a virtual photo, and then choosing aquery from a list that is automatically producedby the QA system.
This architecture places aheavy burden on the query generation compo-nent since that is the component that determinesthe ultimate limitations of the system.5.1 Query GenerationIn a system where the only automatically gener-ated queries are allowed, it is important to beable to create a set of interesting queries to avoidfrustrating the user.
Beyond the straightforward?Who/What/Where is this?
?-style of questions,we also use a question generator (originally de-scribed by Schwartz et al (2004) in the contextof language learning) to produce a set of an-swerable questions about the selected object.Once the player selects a query, the final stepin query generation is to create the LF represen-tation of the question.
This is required so that wecan more easily find matches in the KB.
Fortu-nately, because the queries are either formulaic(e.g., the ?Who/What/Where?
queries), or ex-tracted from the KB, the LF is trivially createdwith requiring a runtime parsing system.5.2 Knowledgebase MatchingWhen the player poses a query to an NPC, weneed to find an appropriate match in the KB.
Todo this, we perform subtree matches between thequery?s LF and the contents of the KB, after firstmodifying the original query so that questionwords (e.g., Who, What, ...) are replaced withspecial identifiers that permit wildcard matches.When a match is found, a complete, grammati-cal response is created by replacing the wildcardnode with the matching subtree and then passingthis structure to the text generation component.5.3 DeixisIn order to make the NPC?s responses believ-able, the final step is to incorporate deictic refer-ences into the utterance.
These are referencesthat depend on the extralinguistic context, suchas the identity, time or location of the speaker orlistener.
Because the semantic structures areeasy to manipulate, we can easily replace thesereferences with the appropriate reference.
Anexample of this was given earlier when the sub-tree corresponding to ?my father?
was used torefer to the owner of the house.This capability gives us a convenient way to170support having separate KBs for shared knowl-edge and individual knowledge.
General infor-mation can be placed in the shared KB, whileknowledge that is specific to an individual (likethe fact that John is ?my father?)
is stored in aseparate KB that is specific to that individual.This allows us to avoid having to re-author theknowledge for each NPC while still allowingindividualized responses.6 Creating Annotated PhotographsOur virtual photos consist of three major parts:the image, the object locator map and the objectdescriptors.
In addition, we define some simplemetadata.
We use the term ?annotations?
to referto the combination of the object locator map, thedescriptors and the metadata.While the photo image is trivially created byrecording the camera view when the photo istaken, the other parts require special techniquesand are described in the following sections.6.1 The Object Locator Map (OLM)The object locator map (OLM) is an image-space map that corresponds 1-to-1 with the pix-els in the virtual photograph image.
For eachimage pixel, the corresponding OLM ?pixel?contains information about the object that corre-sponds to that image-space location.
We createthe OLM using the back buffer technique attrib-uted originally to Weghorst et al (1984).6.2 The Object DescriptorsThe object descriptors contain the semantic de-scription of the objects plus some metadata thathelps determine how the player and NPC caninteract with the objects in the photo.In our system, we use the semantic annota-tions associated with each object as a genericdescription that contains information that wouldbe readily apparent to someone looking at theobject.
Thus, these descriptions focus on thephysical characteristics (derived from the objectdescription) or current actions (derived from thecurrent animation state) of the object.7 3D ModelingThe modeling of 3D scenes and objects hastypically been done in isolation, where onlygraphical (display and performance) concernswere considered.
In this section, we discusssome of the changes that are required on themodeling side to better support our interaction.7.1 EnhancementsBeyond the enhancement of attaching abstractsemantic descriptions (rather than simple textlabels as in Feiner et al (1992)) to each object inthe virtual world?s scene graph, we introduce afew other features to enhance the interactivity ofthe virtual photos.Semantic AnchorsA limitation of attaching the semantic descrip-tions to objects in the 3D world is that this onlycovers concrete objects that have a physical rep-resentation in the world.
Semi-abstract objects(called ?negative parts?
by Landau andJackendoff (1993)) like a cave or a hole do nothave a direct representation in the world andthus do not have objects onto which semanticdescriptions can be attached.
However, it is cer-tainly possible that the player might wish to re-fer to these objects in the course of a game.We provide support for these referable, non-physical objects through the use of semanticanchors, which are invisible objects in the worldthat provide anchor points onto which we canattach information.
For example, abstract objectslike a hole or a cave can be filled with a seman-tic anchor so that when a photo is taken of a re-gion that includes the cave, the player can clickon that region and get a meaningful result.Since these objects are not displayed, there isno requirement that they be closed 3D forms.This gives us the flexibility to create view-dependent semantic anchors by tagging regionsof space based on the current view.
For exam-ple, a cave entrance could be labeled simply as a?cave?
for viewpoints outside the cave whilethis same portal can be termed an ?exit?
(or leftunlabeled) from vantage points inside the cave.By orienting these open forms correctly, we canrely on the graphic engine?s backface culling1 toautomatically remove the anchors that are inap-1Backface culling is an optimization technique that re-moves back-facing surfaces (i.e., surfaces on the side of theobject away from the viewer) from the graphic engine pipe-line so that resources are not wasted processing them.
Thistechnique relies on the assumption that all the objects in thevirtual world are closed 3D forms so that drawing only thefront-facing surfaces doesn?t change the resulting image.171propriate for the current view.Action DescriptionsIn addition to attaching semantic descriptions toobjects, we also allow semantic descriptions beadded to animation sequences in the game.
Thisprovides a convenient mechanism for identify-ing what a person is doing in a photo so thatquestions relating to action can be proposed.Key FeaturesWe also permit key features to be defined (aswas apparently done for POK?MON SNAP) sothat we can approximate object identifiability.
Inour implementation, we require that (at least aportion of) all key features are visible to satisfythis requirement.The advantage of this approach is that it iseasy to implement (since there?s no need to de-termine if the entire key feature is visible), but itrequires that the key features be chosen carefullyin order to produce reasonable results.7.2 LimitationsEven with the proposed enhancements, there areclear limitations to the annotated 3D model ap-proach that will require further investigation.First, there is an unfortunate disconnect be-tween the modeled structures and semanticstructures.
When a designer creates a 3D model,the only consideration is the graphical presenta-tion of the model and so joints like a wrist orelbow are likely to be modeled as a single point.This contrasts with a more semantic representa-tion, which would have the wrist extend slightlyinto the hand and forearm.Another problem is the creation of relation-ships between the objects in the photo.
This isdifficult because many relationships (like ?nextto?
or ?behind?)
can mean different things inworld-space (as they are in the virtual world)and image-space (as they appear in the photo).And finally, there is the standard ?pickingfrom an object hierarchy?
problem where, whena node in the hierarchy is selected, the user?sintent is ambiguous since the intended itemcould be the node or any of its parent nodes.8 ConclusionsIn this paper we have described our approach toallowing users to specify queries by interactingwith virtual photographs that have been anno-tated with semantic information.
While this ap-proach is clearly more limited than allowing fulltext input, it is useful for applications like gamesthat do not always have a keyboard available.ReferencesE.
Andr?
and T. Rist.
1994.
?Referring to World Ob-jects with Text and Pictures?.
In Proceedings ofCOLING 1994.
530-534.T.
Berners-Lee and D. Connolly.
1995.
?HyperTextMarkup Language Spec.
- 2.0?.
W3C RFC1866.R.A.
Bolt.
1980.
??Put-that-there?
: Voice and Gestureat the Graphics Interface?.
SIGGRAPH 80.
ACMPress.
262-270.R.
Campbell and H. Suzuki.
2002.
?Language Neu-tral Representation of Syntactic Structure?.
InProceedings of SCANALU 2002.S.
Feiner.
B. MacIntyre and D. Seligmann.
1992.?Annotating the real world with knowledge-basedgraphics on a see-through head-mounted display?.Graphics Interface 92.
78-85.A.
Kobsa, J. Allgayer, C. Reddig, N. Reithinger, D.Schmauks, K. Harbusch and W. Wahlster.
1986.?Combining Deictic Gestures and Natural Lan-guage for Referent Identification?.
In Proceedingsof COLING 1986.
356-361.B.
Landau and R. Jackendoff.
1993.
??What?
and?Where?
in spatial language and spatial cognition?.Behavioral and Brain Sciences, 16, 217-265.D.
Schmauks.
1987.
?Natural and Simulated Point-ing?.
In Proceedings of the 3rd European ACLConference, Copenhagen.
179-185.L.
Schwartz, T. Aikawa, and M. Pahud, ?DynamicLanguage Learning Tools?, in InSTIL/ICALLSymp.
2004: NLP and Speech Technologies inAdv.
Lang.
Learning, Venice, Italy, June 2004.O.
Stock.
1991.
?AlFresco: Enjoying the Combina-tion of NLP and Hypermedia for Information Ex-ploration?, In AAAI Workshop on IntelligentMultimedia Interfaces 1991.
197-224.H.
Tennant, K. Ross, R. Saenz, C. Thompson and J.Miller.
1983.
?Menu-based natural language un-derstanding?.
In Proc.
of ACL ?83.
151-158.S.
Towns, C. Callaway and J. Lester.
1998.
?Generat-ing Coordinated Natural Language and 3D Anima-tions for Complex Spatial Explanations?.
In Proc.of the 15th National Conf.
on AI.
112-119.T.
Winograd.
1972.
Understanding Natural Lan-guage.
Academic Press.
1972.H.
Weghorst, G. Greenberg and D. Greenberg.
1984.?Improved Computational Methods for Ray Trac-ing?.
ACM Transactions on Graphics 3, 1, 52-69.172
