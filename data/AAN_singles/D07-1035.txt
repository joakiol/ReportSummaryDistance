Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and ComputationalNatural Language Learning, pp.
334?342, Prague, June 2007. c?2007 Association for Computational LinguisticsLow-Quality Product Review Detection in Opinion SummarizationJingjing LiuNankai UniversityTianjin, Chinav-jingil@microsoft.comYunbo CaoMicrosoft Research AsiaBeijing, Chinayucao@microsoft.comChin-Yew LinMicrosoft Research AsiaBeijing, Chinacyl@microsoft.comYalou HuangNankai UniversityTianjin, Chinahuangyl@nankai.edu.cnMing ZhouMicrosoft Research AsiaBeijing, Chinamingzhou@microsoft.comAbstractProduct reviews posted at online shoppingsites vary greatly in quality.
This paper ad-dresses the problem of detecting low-quality product reviews.
Three types of bi-ases in the existing evaluation standard ofproduct reviews are discovered.
To assessthe quality of product reviews, a set of spe-cifications for judging the quality of re-views is first defined.
A classification-based approach is proposed to detect thelow-quality reviews.
We apply the pro-posed approach to enhance opinion sum-marization in a two-stage framework.
Ex-perimental results show that the proposedapproach effectively (1) discriminates low-quality reviews from high-quality ones and(2) enhances the task of opinion summari-zation by detecting and filtering low-quality reviews.1 IntroductionIn the past few years, there has been an increasinginterest in mining opinions from product reviews(Pang, et al 2002; Liu, et al 2004; Popescu andEtzioni, 2005).
However, due to the lack ofeditorial and quality control, reviews on productsvary greatly in quality.
Thus, it is crucial to have amechanism capable of assessing the quality ofreviews and detecting low-quality/noisy reviews.Some shopping sites already provide a functionof assessing the quality of reviews.
For example,Amazon1 allows users to vote for the helpfulnessof each review and then ranks the reviews based onthe accumulated votes.
However, according to oursurvey in Section 3, users?
votes at Amazon havethree kinds of biases as follows: (1) imbalance votebias, (2) winner circle bias, and (3) early bird bias.Existing studies (Kim et al 2006; Zhang and Va-radarajan, 2006) used these users?
votes for train-ing ranking models to assess the quality of reviews,which therefore are subject to these biases.In this paper, we demonstrate the aforemen-tioned biases and define a standard specification tomeasure the quality of product reviews.
We thenmanually annotate a set of ground-truth with realworld product review data conforming to the speci-fication.To automatically detect low-quality product re-views, we propose a classification-based approachlearned from the annotated ground-truth.
The pro-posed approach explores three aspects of productreviews, namely informativeness, readability, andsubjectiveness.We apply the proposed approach to opinionsummarization, a typical opinion mining task.
Theproposed approach enhances the existing work in atwo-stage framework, where the low-quality re-view detection is applied right before the summari-zation stage.Experimental results show that the proposed ap-proach can discriminate low-quality reviews fromhigh-quality ones effectively.
In addition, the taskof opinion summarization can be enhanced by de-tecting and filtering low-quality reviews.1 http://www.amazon.com334The rest of the paper is organized as follows:Section 2 introduces the related work.
In Section 3,we define the quality of product reviews.
In Sec-tion 4, we present our approach to detecting low-quality reviews.
In Section 5, we empirically verifythe effectiveness of the proposed approach and itsuse for opinion summarization.
Section 6 summa-rizes our work in this paper and points out the fu-ture work.2 Related Work2.1 Evaluating Helpfulness of ReviewsThe problem of evaluating helpfulness of reviews(Kim et al 2006), also known as learning utility ofreviews (Zhang and Varadarajan, 2006), is quitesimilar to our problem of assessing the quality ofreviews.In practice, researchers in this area consideredthe problem as a ranking problem and solved itwith regression models.
In the process of modeltraining and testing, they used the ground-truthderived from users?
votes of helpfulness providedby Amazon.
As we will show later in Section 3,these models all suffered from three types of vot-ing bias.In our work, we avoid using users?
votes by de-veloping a specification on the quality of reviewsand building a ground-truth according to the speci-fication.2.2 Mining Opinions from ReviewsOne area of research on opinion mining fromproduct reviews is to judge whether a reviewexpresses a positive or a negative opinion.
Forexample, Turney (2006) presented a simpleunsupervised learning algorithm in judgingreviews as ?thumbs up?
(recommended) or?thumbs down?
(not recommended).
Pang et al(2002) considered the same problem and presenteda set of supervised machine learning approaches toit.
For other work see also Dave et al (2003), Pangand Lee (2004, 2005).Another area of research on opinion mining is toextract and summarize users?
opinions from prod-uct reviews (Hu and Liu, 2004; Liu et al, 2005;Popescu and Etzioni, 2005).
Typically, a sentenceor a text segment in the reviews is treated as thebasic unit.
The polarity of users?
sentiments on aproduct feature in each unit is extracted.
Then theaggregation of the polarities of individual senti-ments is presented to users so that they can have anat-a-glance view on how other experienced usersrated on a certain product.
The major weakness inthe existing studies is that all the reviews, includ-ing low-quality ones, are taken into considerationand treated equally for generating the summary.
Inthis paper, we enhance the application by detectingand filtering low-quality reviews.
In order toachieve that, we first define what the quality ofreviews is.3 Quality of Product ReviewsIn this section, we will first show three biases ofusers?
votes observed on Amazon, and then presentour specification on the quality of product reviews.3.1 Amazon Ground-truthIn our study, we use the product reviews on digitalcameras crawled from Amazon as our data set.
Thedata set consists of 23,141 reviews on 946 digitalcameras.
At the Amazon site, users could vote fora review with a ?helpful?
or ?unhelpful?
label.Thus, for each review there are two numbersindicating the statistics of these two labels, namelythe number of ?helpful?
votes and that of?unhelpful?
ones.
Kim et al(2006) used thepercentage of ?helpful?
votes as the measure ofevaluating the ?quality of reviews?
in theirexperiments.
We call the ground-truth based onthis measure as ?Amazon ground-truth?.Certainly, the ground-truth has the advantage ofconvenience.
However, we identify three types ofbiases that make the Amazon ground-truth not al-ways suitable for determining the quality of re-views.
We describe these biases in details in therest of this section.3.1.1 Imbalance Vote BiasFigure 1.
Reviews?
percentage scoresAt the Amazon site, users tend to value others?opinions positively rather than negatively.
FromFigure 1, we can see that a half of the 23,14102000400060008000100000 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1#ReviewsPercentage of 'helpful' votes335reviews (corresponding to the two bars on the rightof the figure) have more than 90% ?helpful?
votes,including 9,100 reviews with 100% ?helpful?votes.
From an in-depth investigation on thesehighly-voted reviews, we observed that some didnot really have as good quality as the votes hint.For example, in Figure 2, the review about CanonPowerShot S500 receives 40 ?helpful?
votes out of40 votes although it only gives very briefdescription on the product features in its secondparagraph.
We call this type of bias ?imbalancevote?
bias.This is my second Canon digital elph camera.
Both were greatcameras.
Recently upgraded to the S500.
About 6 months later I getthe dreaded E18 error.
I searched the Internet and found numerouspeople having problems.
When I determined the problem to be thelens not fully extending I decided to give it a tug.
It clicked and thecamera came on, ready to take pictures.
Turning it off and on pro-duced the E18 again.
While turning it on I gave it a nice little bumpon the side (where the USB connector is) and the lens popped outon its own.
No problems since.It?s a nice compact and light camera and takes great photos andvideos.
Only complaint (other than E18) is the limit of 30-secondvideos on 640x480 mode.
I've got a 512MB compact flash card, Ishould be able to take as much footage as I have memory in onetake.Figure 2.
An example review3.1.2 Winner Circle BiasFigure 3.
Votes of the top-50 ranked reviewsThere also exists a bootstrapping effect of ?hot?reviews at the Amazon site.
Figure 3 shows the?helpful?
votes for the top 50 ranked reviews.
Thenumbers are averaged over 127 digital cameraswhich have no less than 50 reviews.
As shown inthis figure, the top two reviews hold more than 250and 140 votes respectively on average; while thenumbers of votes held by lower-ranked reviewsdecrease exponentially.
This is so-called the?winner circle?
bias: the more votes a reviewgains, the more default authority it would appear tothe readers, which in turn will influence theobjectivity of the readers?
votes.
Also, the higherranked reviews would attract more eyeballs andtherefore gain more people?s votes.
This mutualinfluence among labelers should be avoided whenthe votes are used as the evaluation standard.3.1.3 Early Bird BiasFigure 4.
Dependency on publication datePublication date can influence the accumulation ofusers?
votes.
In Figure 4, the n?th publication daterepresents the n?th month after the product isreleased.
The number in the figure is averaged overall the digital cameras in the data set.
We canobserve a clear trend that the earlier a review isposted, the more votes it will get.
This is simplybecause reviews posted earlier are exposed to usersfor a longer time.
Therefore, some high qualityreviews may get fewer users?
vote because of laterpublication.
We call this ?early bird?
bias.3.2 Specification of QualityBesides these aforementioned biases, using the rawrating from readers directly also fails to provide aclear guideline for what a good review consists of.In this section, we provide such a guideline, whichwe name as the specification (SPEC).In the SPEC, we define four categories of re-view quality which represent different values ofthe reviews to users?
purchase decision: ?best re-view?, ?good review?, ?fair review?, and ?bad re-view?.
A generic description of the SPEC is as fol-lows:A best review must be a rather complete and de-tailed comment on a product.
It presents severalaspects of a product and provides convincing opi-nions with enough evidence.
Usually a best reviewcould be taken as the main reference that users on-ly need to read before making their purchase deci-sion on a certain product.
The first review in Fig-ure 5 is a best review.
It presents several productfeatures and provides convincing opinions withsufficient evidence.
It is also in a good format forreaders to easily understand.
Note that we omitsome words in the example to save the space.0501001502002503001 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49#VotesheldbyreviewsRanking positions of reviews01020304050601 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49#VotesheldbyreviewsPublication Date336A good review is a relatively complete commenton a product, but not with as much supporting evi-dence as necessary.
It could be used as a strongand influential reference, but not as the only rec-ommendation.
The second review in Figure 5 issuch an example.A fair review contains a very brief descriptionon a product.
It does not supply detailed evaluationon the product, but only comments on some as-pects of the product.
For example, the third reviewin Figure 5 mainly talks about ?the delay betweenpictures?, but less about other aspects of the cam-era.A bad review is usually an incorrect descriptionof a product with misleading information.
It talkslittle about a specific product but much about somegeneral topics (e.g.
photography).
For example, thelast review in Figure 5 talks about the topic of ?ge-neric battery?, but does not specify any digitalcamera.
A bad review is an ?unhelpful?
review thatcan be ignored.Best Review:I purchased this camera about six months ago after my KodakEasyshare camera completely died on me.
I did a little researchand read only good things about this Canon camera so I decided togo with it because it was very reasonably priced (about $200).
Notonly did the camera live up to my expectations, it surpassed themby leaps and bounds!
Here are the things I have loved about thiscamera:BATTERY - this camera has the best battery of any digital cam-era I have ever owned or used.
?EASY TO USE - I was able to ?PICTURE QUALITY - all of the pictures I've taken and printedout have been great.
?FEATURES - I love the ability to quickly and easily ?LCD SCREEN - I was hoping ?SD MEMORY CARD - I was also looking for a camera that usedSD memory cards.
Mostly because?I cannot stress how highly I recommend this camera.
I will neverbuy another digital camera besides Canon again.
And the A610 (aswell as the A620 - the 7.0MP version) is the best digital camera I'veever used.Good Review:The Sony DSC "P10" Digital Camera is the top pick for CSC.Running against cameras like Olympus stylus, Canon Powereshot,Sony V1, Nikon, Fuji, and More.
The new release of 5.0 mega pix-els has shot prices for digital cameras up to $1000+.
This camera Ipurchased through a Private Dealer cost me $400.86.
The RetailPrice is Running $499.00 to $599.00.
Purchase this camera from awholesale dealer for the best price $377.00.
Great Photo Even indim light w/o a flash.
The p10 is very compact.
Can easily fit intoany pocket.
The camera can record 90 minutes of mpeg like a homemovie.
There are a lot of great digital cameras on the market thatshoot good pictures and video.
What makes the p10 the top pick isit comes with a rechargeable lithium battery.
Many use AA batte-ries, the digital camera consumes theses AA batteries in about twohours time while the unit is on.
That can add continuous expense tothe camera.
It's also the best resolution on the market.
6.0 megapixis out, though only a few.
And the smallest that we found.
Also thebest price for a major brand.Fair Review:There is nothing wrong with the 2100 except for the very notice-able delay between pics.
The camera's digital processor takesabout 5 seconds after a photo is snapped to ready itself for the nextone.
Otherwise, the optics, the 3X optical zoom and the 2 megapixelresolution are fine for anything from Internet apps to 8" x 10" printenlarging.
It is competent, not spectacular, but it gets the job doneat an agreeable price point.Bad Review:I want to point out that you should never buy a generic battery,like the person from San Diego who reviewed the S410 on May 15,2004, was recommending.
Yes you'd save money, but there havebeen many reports of generic batteries exploding when charged fortoo long.
And don't think if your generic battery explodes you cansue somebody and win millions.
These batteries are made in sweat-shops in China, India and Korea, and I doubt you can find anybodyto sue.
So play it safe, both for your own sake and the camera'ssake.
If you want a spare, get a real Canon one.Figure 5.
Example reviews3.3 Annotation of QualityAccording to the SPEC defined above, we built aground-truth from the Amazon data set.
Werandomly selected 100 digital cameras and 50reviews for each camera.
Totally we have 4,909reviews since some digital cameras have fewerthan 50 unique reviews.
Then we hired twoannotators to label the reviews with the SPEC astheir guideline.
As the result, we have twoindependent copies of annotations on 4,909reviews, with the labels of ?best?, ?good?, ?fair?,and ?bad?.
Table 1 shows the confusion matrixbetween the two copies of annotation.
The value ofthe kappa statistic (Cohen, 1960) calculated fromthe matrix is 0.8142.
This shows that the twoannotators achieved highly consistent results byfollowing the SPEC, although they workedindependently.Annota-tion 1Annotation 2best good fair bad totalbest 294 44 2 0 340good 66 639 113 0 818fair 0 200 1,472 113 1,785bad 1 2 78 1,885 1,966total 361 885 1,665 1,998 4,909Table 1.
Confusion matrix bet.
the annotationsIn order to examine the difference between ourannotations and Amazon ground-truth, we evaluatethe Amazon ground-truth against the annotations,337with the measure of ?error rate of preference pairs?
(Herbrich et al 1999).?????????
=|?????????
??????????
?????
||???
??????????
????
?|(1)where the ?preference pair?
is defined as a pair ofreviews with a order.
For example, a best reviewand a good review correspond to a preference pairwith the order of ?best review preferring to goodreview?.
The ?all preference pairs?
are collectedfrom one of the annotations (the annotation 1 orthe annotation 2) by ignoring the pairs from thesame category.
The ?incorrect preference pairs?are the preference pairs collected from the Ama-zon ground-truth but not with the same order asthat in the all preference pairs.
The order of thepreference pair collected from the Amazonground-truth is evaluated on the basis of the per-centage score as described in Section 3.1.The error rate of preference pairs based on theannotation 1 and that based on the annotation 2 are0.448 and 0.446, respectively, averaged over 100digital cameras.
The high error rate of preferencepairs demonstrates that the Amazon ground-truthdiverges from the annotations (our ground-truth)significantly.To discover which kind of ground-truth is morereasonable, we ask an additional annotator (thethird annotator) to compare these two kinds ofground-truth.
More specifically, we randomly se-lected 100 preference pairs whose orders the twokinds of ground-truth don?t agree on (called incor-rect preference pairs in the evaluation above).
Asfor our ground-truth, we choose the Annotation 1in the new test.
Then, the third annotator is askedto assign a preference order for each selected pair.Note that the third annotator is blind to both ourspecification and the existing preference order.Last, we evaluate the two kinds of ground-truthwith the new annotation.
Among 100 pairs, ourground-truth agrees to the new annotation on 85pairs while the Amazon ground-truth agrees to thenew annotation on 15 pairs.
To confirm the result,yet another annotator (the fourth annotator) iscalled to repeat the same annotation independentlyas the third one.
And we obtain the same statisticalresult (85 vs. 15) although the fourth annotatordoes not agree with the third annotator on somepairs.In practice, we treat the reviews in the first threecategories (?best?, ?good?
and ?fair?)
as high-quality reviews and those in the ?bad?
category aslow-quality reviews, since our goal is to identifylow quality reviews that should not be consideredwhen creating product review summaries.4 Classification of Product ReviewsWe employ a statistical machine learning approachto address the problem of detecting low-qualityproducts reviews.Given a training data set?
=  ??
,??
1?
, weconstruct a model that can minimize the error inprediction of y given x (generalization error).
Here??
?
?
and ??
= {????
???????
, ???
???????
}represents a product review and a label,respectively.
When applied to a new instance x, themodel predicts the corresponding y and outputs thescore of the prediction.4.1 The Learning ModelIn our study, we focus on differentiating low-quality product reviews from high-quality ones.Thus, we treat the task as a binary classificationproblem.We employ SVM (Support Vector Machines)(Vapnik, 1995) as the model of classification.Given an instance x (product review), SVM assignsa score to it based on?
?
= ???
+ ?
(2)where w denotes a vector of weights and b denotesan intercept.
The higher the value of f(x) is, thehigher the quality of the instance x is.
Inclassification, the sign of f(x) is used.
If it ispositive, then x is classified into the positivecategory (high-quality reviews), otherwise into thenegative category (low-quality reviews).The construction of SVM needs labeled trainingdata (in our case, the categories are ?high-qualityreviews?
and ?low-quality reviews?).
Briefly, thelearning algorithm creates the ?hyper plane?
in (2),such that the hyper plane separates the positive andnegative instances in the training data with thelargest ?margin?.4.2 Product Feature ResolutionProduct features (e.g., ?image quality?
for digitalcamera) in a review are good indicators of reviewquality.
However, different product features mayrefer to the same meaning (e.g., ?battery life?
and?power?
), which will bring redundancy in thestudy.
In this paper, we formulize the problem asthe ?resolution of product features?.
Thus, the338problem is reduced to how to determine the equi-valence of a product feature in different forms.In (Hu and Liu, 2004), the matching of differentproduct features is mentioned briefly and ad-dressed by fuzzy matching.
However, there existmany cases where the method fails to match themultiple mentions, e.g., ?battery life?
and ?power?,because it only considers string similarity.
In thispaper we propose to resolve the problem by leve-raging two kinds of evidence: one is ?surface string?evidence, the other is ?contextual evidence?.We use edit distance (Ukkonen, 1985) to com-pare the similarity between the surface strings oftwo mentions, and use contextual similarity to re-flect the semantic similarity between two mentions.When using contextual similarity, we split allthe reviews into sentences.
For each mention of aproduct feature, we take it as a query and searchfor all the relevant sentences.
Then we construct avector for the mention, by taking each unique termin the relevant sentences as a dimension of the vec-tor.
The cosine similarity between two vectors ofmentions is then present to measure the contextualsimilarity between two mentions.4.3 Feature Development for LearningTo detect low-quality reviews, our proposedapproach explores three aspects of product reviews,namely informativeness, subjectiveness, andreadability.
We denote the features employed forlearning as ?learning features?, discriminative fromthe ?product features?
we discussed above.4.3.1 Features on InformativenessAs for informativeness, the resolution of productfeatures is employed when we generate thelearning features as listed below.
Pairs mapping tothe same product feature will be treated as thesame product feature, when we calculate thefrequency and the number of product features.
Weapply the approach proposed in (Hu and Liu, 2004)to extract product features.We also use a list of product names and a list ofbrand names to generate the learning features.
Bothlists can be collected from the Amazon site be-cause they are relatively stable within a time inter-val.The learning features on the informativeness ofa review are as follows.?
Sentence level (SL)?
The number of sentences in the review?
The average length of sentences?
The number of sentences with product features?
Word level (WL)?
The number of words in the review?
The number of products (e.g., DMC-FZ50,EX-Z1000) in the review?
The number of products in the title of a review?
The number of brand names (e.g., Canon, Sony)in the review?
The number of brand names in the title of areview?
Product feature level (PFL)?
The number of product features in the review?
The total frequency of product features in thereview?
The average frequency of product features inthe review?
The number of product features in the title of areview?
The total frequency of product features in thetitle of a review4.3.2 Features on ReadabilityWe make use of several features at paragraph levelwhich indicate the underlying structure of thereviews.
These features include,?
The number of paragraphs in the review?
The average length of paragraphs in the review?
The number of paragraph separators in the re-viewHere, we refer to the keywords, such as ?Pros?vs.
?Cons?
as ?paragraph separators?.
The key-words usually appear at the beginning of para-graphs for categorizing two contrasting aspects ofa product.
We extract the nouns and noun phrasesat the beginning of each paragraph from the 4,909reviews and use the most frequent 30 pairs of key-words as paragraph separators.
Table 2 providessome examples of the extracted separators.Separators SeparatorsPositive Negative Positive NegativePros Cons The Good The BadStrength Weakness Thumb up BummerPLUSES MINUSES Positive NegativeAdvantages Drawbacks Likes DislikesThe  upsides DownsidesGOODTHINGSBADTHINGSTable 2.
Examples of paragraph separators3394.3.3 Features on SubjectivenessWe also take the subjectiveness of reviews intoconsideration.
Unlike previous work (Kim et al2006; Zhang and Varadarajan, 2006) using shallowsyntactic information directly, we use a sentimentanalysis tool (Hu and Liu, 2004) which aggregatesa set of shallow syntactic information.
The tool is aclassifier capable of determining the sentimentpolarity of each sentence.
We create three learningfeatures regarding the subjectiveness of reviews.?
The percentage of positive sentences in thereview?
The percentage of negative sentences in thereview?
The percentage of subjective sentences (re-gardless of positive or negative) in the review5 ExperimentsIn this section, we describe our experiments withthe proposed classification-based approach to low-quality review detection, and its effectiveness onthe task of opinion summarization.5.1 Detecting Low-quality ReviewsIn our proposed approach, the problem of assessingquality of reviews is formalized as a binary classi-fication problem.
We conduct experiments by tak-ing reviews in the categories of ?best?, ?good?, and?fair?
as high-quality reviews and those in the?bad?
category as low-quality reviews.As for classification model, we utilize theSVMLight toolkit (Joachims, 2004).
We randomlydivide the 100 queries of digital cameras into twosets, namely a training set of 50 queries and a testset of 50 queries.
For the two copies of annota-tions, we use the same division.
We use the train-ing set from ?annotation 1?
to train the model andapply the model to the test sets from both ?annota-tion 1?
and ?annotation 2?, respectively.
Table 3reports the accuracies of our approach to reviewclassification.
The accuracy is defined as the per-centage of correctly classified reviews.We take the approach that utilizes only the cate-gory of features on sentence level (SL) as the base-line, and incrementally add other categories of fea-tures on informativeness, readability and subjec-tiveness.
We can see that both the features on wordlevel (WL) and those on product feature level (PFL)can improve the performance of classificationmuch.
The features on readability can still increasethe accuracy although the contribution is muchless.
The features on subjectiveness, however,make no contribution.Feature Category Annotation1 Annotation2Informative-nessSL 73.59% 72.81%WL 80.41% 79.15%PFL 83.30% 82.37%Readability 83.93% 82.91%Subjectiveness 83.84% 82.96%Table 3.
Low-quality reviews detectionWe also conduct a more detailed analysis oneach individual feature.
Two categories of featureson ?title?
and ?brand name?
have poor perfor-mance, which is due to the lack of information inthe title and the low coverage of brand names in areview, respectively.5.2 Summarizing Sentiments of ReviewsOne potential application of low-quality reviewdetection is the opinion summarization of reviews.The process of opinion summarization of re-views with regards to a query of a product consistsof the following steps (Liu et al 2005):1.
From each of the reviews, identify every textsegment with opinion in the review, and de-termine the polarities of the opinion segments.2.
For each product feature, generate a positiveopinion set and a negative opinion set of opi-nion segments, denoted as POS(?
)and NOS(?).3.
For each product feature, aggregate the num-bers of segments in POS(?)
andNOS(?)
, asopinion summarization on the product feature.In this process, all the reviews contribute thesame.
However, different reviews do hold differentauthorities.
A positive/negative opinion from ahigh-quality review should not have the sameweight as that from a low-quality review.We use a two-stage approach to enhance the re-liability of summarization.
That is, we add aprocess of low-quality review detection before thesummarization process, so that the summarizationresult is obtained based on the high-quality reviewsonly.
We are to demonstrate how much differencethe proposed two-stage approach can bring into theopinion summarization.We use the best classification model trained asdescribed in Section 5.1 to filter low-quality re-views, and do summarization on the high-quality340reviews associated to the 50 test queries.
We de-note the proposed approach and the old approachas ?two-stage?
and ?one-stage?, respectively.
Dueto the limited space, we only give a visual compar-ison of the two approaches on ?image quality?
inFigure 6.
The upper figure shows the summariza-tion of positive opinions and the lower figureshows that of negative opinions.
From the figureswe can see that the two-stage approach preservesfewer text segments as the result of filtering outmany low-quality product reviews.Figure 6.
Summarization on ?image quality?To show the comparison on more features in acompressed space, we give the statistic ratio ofchange between two approaches instead.
As for theevaluation measure, we define ?RatioOfChange?
(ROC) on a feature f as,ROC ?
=Rateone?stage  ?
?
Ratetwo?stage (?
)Rateone?stage (?
)(3)where Rate *(f) is defined as,Rate?(?)
=|POS(?)||POS(?
)| + |NOS(?
)|(4)Table 4 shows some statistic results on ROC onfive product features, namely ?image quality?
(IQ),?battery?, ?LCD screen?
(LCD), ?flash?
and ?mov-ie mode?
(MM).
The values in the cells are thepercentage of queries whose ROC is larger/smallerthan the respective thresholds.
We can see that alarge portion of queries have big changes on thevalues of ROC.
This means that the result achievedby the two-stage approach is substantially differentfrom that achieved by the one-stage approach.%QueryRatioOfChange (+)>0.30 >0.25 >0.20 >0.15 >0.10 >0.05IQ 2% 4% 4% 10% 14% 22%Battery 10% 14% 18% 30% 38% 50%LCD  12% 18% 20% 22% 24% 28%Flash  6% 10% 16% 20% 26% 42%MM 6% 8% 8% 12% 18% 26%%QueryRatioOfChange (-)<-0.30 <-0.25 <-0.20 <-0.15 <-0.10 <-0.05IQ 4% 6% 10% 14% 18% 44%Battery 2% 4% 4% 10% 14% 22%LCD  4% 4% 8% 12% 22% 28%Flash  4% 6% 8% 16% 18% 28%MM 8% 10% 16% 18% 34% 42%Table 4.
RatioOfChange on five featuresThere is no standard way to evaluate the qualityof opinion summarization as it is rather a subjec-tive problem.
In order to demonstrate the impact ofthe two-stage approach, we turn to external author-itative sources other than Amazon.com as the ob-jective evaluation reference.
We observe thatCNET2 provides a professional ?editor?s review?for many products, which gives a rating in therange of 1~10 on product features.
9 digital cam-eras out of the 50 test queries are found to have theeditor?s rating on ?image quality?
at CNET.
Weuse this rating to compare with the results of ouropinion summarization.
We rescale the Rate scoresobtained by both the one-stage approach and thetwo-stage approach into the range of 1-10 in orderto perform the comparison.Figure 7 provides the visual comparison.
Wecan see that the result achieved by the two-stageapproach has a much better (closer) resemblance toCNET rating than one-stage approach does.
Thisindicates that our two-stage approach can achieve amore consistent summarization result to the profes-sional evaluations by the editors.
Although theCNET rating is not the absolute standard for prod-uct evaluation, it provides a professional yet objec-tive evaluation of the products.
Therefore, the ex-perimental results demonstrate that our proposedapproach could achieve more reliable opinionsummarization which is closer to the generic eval-uation from authoritative sources.2 http://www.cnet.com03060901201 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49Numberofsupportingsentences(Positive)QueryIDOne-stage Two-stage0204060801 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49Numberofsupportingsentences(Negative)QueryIDOne-stage Two-stage341Figure 7.
Comparison with CNET rating6 ConclusionIn this paper, we studied the problem of detectinglow-quality product reviews.
Our contribution canbe summarized in two-fold: (1) we discoveredthree types of biases in the ground-truth used ex-tensively in the existing work, and proposed a spe-cification on the quality of product reviews.
Thethree biases that we discovered are imbalance votebias, winner circle bias, and early bird bias.
(2)Rooting on the new ground-truth (conforming tothe proposed specification), we proposed a classi-fication-based approach to low-quality productreview detection, which yields better performanceof opinion summarization.We hope to explore our future work in severalareas, such as further consolidating the newground-truth from different points of view and ve-rifying the effectiveness of low-quality review de-tection with other applications.ReferencesJacob Cohen.
1960.
A coefficient of agreement for no-minal scales, Educational and Psychological Mea-surement 20: 37?46.Kushal Dave, Steve Lawrence, and David M. Pennock.2003.
Mining the peanut gallery: opinion extractionand semantic classification of product reviews.WWW?03.Harris Drucker, Chris J.C., Burges Linda Kaufman,Alex Smola and Vladimir Vapnik.
1997.
Supportvector regression machines.
Advances in Neural In-formation Processing Systems.Christiane Fellbaum.
1998.
WordNet: an ElectronicLexical Database, MIT Press.Ralf Herbrich, Thore Graepel, and Klaus Obermayer.1999.
Support Vector Learning for Ordinal Regres-sion.
In Proc.
of the 9th International Conference onArtificial Neural Networks.Minqing Hu and Bing Liu.
2004a.
Mining and Summa-rizing Customer Reviews.
KDD?04.Minqing Hu and Bing Liu.
2004b.
Mining Opinion Fea-tures in Customer Reviews.
AAAI?04.Kalervo Jarvelin & Jaana Kekalainen.
2000.
IR: evalua-tion methods for retrieving highly relevant docu-ments.
SIGIR?00.Nitin Jindal and Bing Liu.
2006.
Identifying Compara-tive Sentences in Text Documents.
SIGIR?06.Nitin Jindal and Bing Liu.
2006.
Mining comparativesentences and relations.
AAAI?06.Thorsten Joachims.
SVMlight -- Support Vector Ma-chine.
http://svmlight.joachims.org/, 2004.Soo-Min Kim, Patrick Pantel, Tim Chklovski, MarcoPennacchiotti.
2006.
Automatically Assessing Re-view Helpfulness.
EMNLP?06.Dekang Lin.
1998, Automatic retrieval and clustering ofsimilar words.
COLING-ACL?98.Bing Liu, Minqing Hu, and Junsheng Cheng.
2005.Opinion observer: analyzing and comparing opinionson the web.
WWW ?05.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: Sentiment analysis using subjectivity summari-zation based on minimum cuts.
ACL?04.Bo Pang and Lillian Lee.
2005.
Seeing stars: Exploitingclass relationships for sentiment categorization withrespect to rating scales.
ACL?05.Bo Pang and Lillian Lee, and S. Vaithyanathan.
2002.Thumbs up?
sentiment classification using machinelearning techniques.
EMNLP?02.Ana-Maria Popescu and O Etzioni.
2005.
Extractingproduct    features and opinions from reviews.
HLT-EMNLP?05.Peter D. Turney.
2001.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classifi-cation of reviews.
ACL?02Esko Ukkonen.
1985.
Algorithms for approximate stringmatching.
Information and Control, pp.
100 ?
118.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer.Zhu Zhang and Balaji Varadarajan.
2006.
Utility Scor-ing of Product Reviews.
CIKM?0634567891 2 3 4 5 6 7 8 9RatingScoreQueryIDOne-stageTwo-stageCNET Ground-truth342
