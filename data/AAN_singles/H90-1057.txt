Representat ion Quality in Text Classification:An Introduction and Exper imentDav id  D. LewisComputer and Information Science Dept.University of MassachusettsAmherst~ MA 01003lewis @cs.
umass, eduABSTRACTThe way in which text is represented has a strong im-pact on the performance of text classification (retrievaland categorization) systems.
We discuss the operation oftext classification systems, introduce a theoretical modelof how text representation impacts their performance,and describe how the performance of text classificationsystems is evaluated.
We then present he results of anexperiment on improving text representation quahty, aswell as an analysis of the results and the directions theysuggest for future research.1 The  Task  o f  Text  C lass i f i ca t ionText-based systems can be broadly classified into classifi-cation systems and comprehension systems.
Text classi-fication systems include traditional information retrieval(IR) systems, which retrieve texts in response to a userquery, as well as categorization systems, which assigntexts to one or more of a fixed set of categories.
Textcomprehension systems go beyond classification to trans-form text in some way, such as producing summaries,answering questions, or extracting data.Text classification systems can be viewed as comput-ing a function from documents to one or more classvalues.
Most commercial text retrieval systems requireusers to enter such a function directly in the form of aboolean query.
For example, the query(language OR speech) AND A U = Smithspecifies a 1-ary 2-valued (boolean) function that takeson the value TRUE for documents that are authoredby Smith and contain the word language or the wordspeech.
In statistical IR systems, which have long beeninvestigated by researchers and are beginning to reachthe marketplace, the user typically enters a natural an-guage query, such asShow me uses of speech recogni$ion.The assumption is made that the attributes (contentwords, in this case) used in the query will be stronglyassociated with documents that should be retrieved.
Astatistical IR system uses these attributes to construct aclassification function, such as:f (x )  .~ Cl ~shaw Jr C2~3ttse s -Jr C3Y3speech -~- C4~recog~zitio~r tThis function assumes that there is an attribute corre-sponding to each word, and that attribute takes on somevalue for each document, such as the number of occur-rences of the word in the document.
The coefficients c~indicate the weight given to each attribute.
The func-tion produces a numeric score for each document, andthese scores can be used to determine which documentsto retrieve or, more usefully, to display documents to theuser in ranked order:Speech  Recogn i t ion  Applications 0.88Jones Gives Speech  at Trade Show 0.65Speech  and Speech  Based Systems 0.57Most methods for deriving classification functionsfrom natural language queries use statistics of word oc-currences to set the coefficients of a linear discriminantfunction \[5,20\].
The best results are obtained when su-pervised machine learning, in the guise of relevance feed-back, is used \[21,6\].Text categorization systems can also be viewed ascomputing a function defined over documents, in thiscase a k-ary function, where k is the number of cate-gories into which documents can be sorted.
Rather thanderiving this function from a natural language query, itis typically constructed irectly by experts \[28\], perhapsusing a complex pattern matching language \[12\].
Alter-nately, the function may be induced by machine learningtechniques from large numbers of previously categorizeddocuments \[17,11,2\].1 .1  Text  Representat ion  and  The  Con-cept  Learn ing  Mode lAny text classification function assumes a particularrepresentation of documents.
With the exception of afew experimental knowledge-based IR systems \[15\], thesetext representations map documents into vectors of at-tribute values, usually boolean or numeric.
For example,the document itle "Speech and Speech Based Systems"might be represented as288(F,F,F, T,F,F, T,F, T,F,F,F...)in a system which uses boolean attribute values andomits common function words (such as and) from thetext representation.
The T's correspond to the wordsspeech, based, and systems.
The same title might be rep-resented as(0, O, O, 1.0, O, O, 0.5, O, 0.5, O, O, 0 ...)in a statistical retrieval systems where each attribute isgiven a weight equal to the number of occurrences ofthe word in the document, divided by the number ofoccurrences of the most frequent word in the document.Information retrieval researchers have experimentedwith a wide range of text representations, including vari-ations on words from the original text, manually as-signed keywords, citation and publication information,and structures produced by NLP analysis \[15\].
Besidesthis empirical work, there have also been a few attemptsto theoretically characterize the properties of differentrepresentations and relate them to retrieval system per-formance.
The most notable of these attempts i Salton'sterm discrimination model \[19\] which says that a goodtext attribute is one that increases the average distancebetween all pairs of document vectors.However, none of the proposed models of text repre-sentation quahty addresses the following anomaly: sincemost text representations have very high dimensional-ity (large number of attributes), there is usually a legalclassification function that will produce any desired par-tition of the document collection.
This means that es-sentially all proposed text representations have the sameupper bound performance.
Therefore, in order to under-stand why one text representation is better than another,we need to take into consideration the limited ability ofboth humans and machine learning algorithms to pro-duce classification functions.The concept learning model of text classification \[14\]assumes that both machine production of classificationfunctions (as in translation of natural language queriesand relevance feedback) and human production of classi-fication functions (as in user querying or expert construc-tion of categorization systems) can usefully be viewedas machine learning.
Whether this is a useful model ofhuman production of classification functions is a ques-tion for experiment.
If so, useful view (which remainsto be determined), a wide range of theoretical resultsand practical techniques from machine learning, patternrecognition, and statistics will take on new significancefor text classification systems.We survey a variety of representations from the stand-point of the concept learning model in \[15\].
We are cur-rently conducting several experiments o test the predic-tions of the model \[14\].
One such experiment is describedin Section 2 of this paper.
First, however, we discuss howtext classification systems are evaluated.1 .2  Eva luat ion  o f  Text  C lass i f i ca t ionSystemsWe have refered several times to the "performance" oftext classification systems, so we should say somethingabout how performance is measured.
Retrieval systemsare typically evaluated using test collections \[24\].
A testcollection consists of, at minimum, a set of documents,a set of sample user queries, and a set of relevance judg-ments.
The relevance judgments tell which documentsare relevant (i.e.
should be retrieved) for each query.The retrieval system can be applied to each query inturn, producing either a set of retrieved ocuments, orranking all documents in the order in which they wouldbe retrieved.Two performance figures can be computed for a setof retrieved documents.
Recall is the percentage of allrelevant documents which show up in the retrieved set,while precision is the percentage of documents in the re-trieved set which are actually relevant.
Recall and pre-cision figures can be averaged over the group of queries,or the recall precision pair for each query plotted on ascatterplot.For systems which produce a ranking rather than asingle retrieved set, there is a recall and precision figurecorresponding to each point in the ranking.
The aver-age performance for a set of queries can be displayed interms of average precision levels at various recall levels(as in Table 1) or the averages at various points can begraphed as a recall precision curve.
Both methods dis-play for a particular technique how much precision mustbe sacrificed to reach a particular ecall level.A single performance figure which is often used tocompare systems is the average precision at 10 standardrecall levels (again as in Table 1), which is an approxi-mation to the area under the recall precision curve.
Adifference of 5% in these figures is traditionally callednoticeable and 10% is considered material \[22\].
Othersingle figures of merit have also been proposed \[27\].A large number of test collections have been used inIR research, with some being widely distributed and usedby many researchers.
The superiority of a new techniqueis not widely accepted until it has been demonstrated onseveral test collections.
Test collections range in sizefrom a few hundreds to a few tens of thousands of docu-ments, with anywhere from 20 to a few hundred queries.Results on the smaller collections have often turned outto be unrehable, so the larger collections are preferred.Evaluation is still a research issue in IR.
The exhaus-tive relevance judgments assumed for traditional testcollections are not possible with larger collections, norwhen evaluating highly interactive retrieval systems \[6\].For more on evaluation in IR, the reader is referred toSparck 3ones' excellent collection on the subject \[25\].Evaluation of text categorization systems also needsmore attention.
One approach is to treat each cate-gory as a query and compute average recall and precisionacross categories \[12\], but other approaches are possible\[2\] and no standards have been arrived at.2892 An  Exper iment  on Improv ingText  Representat ionOne method of improving text representation that hasseen considerable recent attention is the use of syntac-tic parsing to create indexing phrases.
These syntacticphrases are single attributes corresponding to pairs ofwords in one of several specified syntactic relationshipsin the original text (e.g.
verb and head noun of subject,noun and modifying adjective, etc.).
For instance, thedocument titleJones Gives Speech at Trade Showmight be represented not just by the attributesJones, gives, speech, trade, showbut also by the attributes<Jones gives>, <gives speech>, <speech show>,<evade show>.on their tendency to occur in documents assigned to thesame Computing Reviews categories.
2 Each of the 6922phrases which occurred in two or more documents wasused as the seed for a cluster, so 6922 clusters wereformed.
A variety of thresholds on cluster size andminimum similarity were explored.
Document scoreswere computed using the formulae for word and phraseweights used in Fagan's tudy of phrasal indexing \[8\] andCrouch's work on cluster indexing \[7\].Precision figures at 10 recall levels are shown in Table1 for words, phrases combined with words, and clusterscombined with words.
While phrase clusters did im-prove performance, asis not always the case with clustersof individual words, the hypothesis that phrase clusterswould be better identifiers than individual phrases wasnot supported.
A number of variations on the criteria formembership n a cluster were tried, but none were foundto give significantly better results.
In the next sectionwe discuss a number of possible causes for the observedperformance l vels.Previous experiments have shown only small retrievalperformance improvements from the use of syntacticphrases.
Syntactic phrases are desirable text attributessince they are less ambiguous than words and have nar-rower meanings.
On the other hand, their statisticalproperties are inferior to those of words.
In particu-lar, the large number of different phrases and the lowfrequency of occurrence of individual phrases makes ithard to estimate the relative frequency of occurrence ofphrases, as is necessary for statistical retrieval methods.Furthermore, a syntactic phrase representation is highlyredundant (there are large numbers of phrases with es-sentially the same meaning), and noisy (since redundantphrases are not assigned to the same set of documents).2.1 Clustering of Syntactic PhrasesThe concept learning model predicts that if the statis-tical properties of syntactic phrases could be corrected,without degrading their desirable semantic properties,then the quality of this form of representation will beimproved.
A number of dimensionality reduction tech-niques from pattern recognition potentially would havethis effect \[13\].
One approach is to use cluster analy-sis \[1\] to recognize groups of redundant attributes andreplace them with a single attribute.We recently conducted a prehminary experiment test-ing this approach.
1 The titles and abstracts of the 3204documents in the CACM-3204 test collection \[9\] weresyntactically parsed and phrases extracted.
Each phrasecorresponded to a pair of content words in a direct gram-matical relation.
Words were stemmed \[18\] and the orig-inal relationship between the words was not stored.
(Thewords are unordered in the phrases.
)Phrases were clustered using a nearest neighbor clus-tering technique, with similarity between phrases based1Full details arc found in \[16\].2.2 AnalysisCan we conclude from Table 1 that clustering of syn-tactic phrases is not a useful technique for informationretrieval?
No--the generation of performance figures isonly the beginning of the analysis of a text classificationtechnique.
Controlhng for all variables that might affectperformance is usually impossible due to the complexityof the techniques used and the richness and variety ofthe texts which might be input to these systems.
Fur-ther analysis, and usually further experiment, is neces-sary before strong conclusions can be reached.In this section we examine a range of possible reasonsfor the failure of syntactic phrase clusters to significantlyimprove retrieval performance.
Our goal is to discoverwhat the most significant influences were on the perfor-mance of syntactic phrase clusters, and so suggest whatdirection this research should take in the future.2.2.1 Document  Scor ing MethodThe first possibihty to consider is that there is noth-ing wrong with the clusters themselves, but only withhow we used them.
In other words, the coefficients ofthe classification functions derived from queries, or thenumeric values assigned to the cluster attributes, mighthave been inappropriate.
There is some merit in thissuggestion, since the cluster and phrase weighting meth-ods currently used are heuristic, and are based on exper-iments on relatively few collections.
More theoreticallysound methods of phrase and cluster weighting are beinginvestigated \[6,26\].On the other hand, scoring is unlikely to be the onlyproblem.
Simply examining a random selection of clus-ters (the seed member for each is underhned)2Only 1425 of the 3204 CACM documents  had Computing Re-views categories assigned, so only phrases that  appeared in thesedocuments  were clustered.290PrecisionRecall Clusters + Terms PhrasesLevel Size 2 Size 4 Size 8 Size 12 + Terms Terms0.10 55.5 55.5 57.9 57.1 58.1 56.30.20 43.2 42.0 42.2 41.9 45.4 41.00.30 37.7 37.0 36.5 36.2 38.0 35.70.40 31.1 30.5 30.8 30.0 30.2 29.60.50 23.3 23.3 22.2 22.3 23.4 22.00.60 19.5 19.3 18.2 18.3 19.0 18.80.70 13.5 13.3 13.3 13.3 13.7 13.80.80 9.2 9.4 9.4 9.3 9.5 9.90.90 5.5 5.8 5.6 5.6 5.6 6.11.00 4.2 4.1 4.1 4.1 4.1 4.7Avg.
Prec.
24.3 24.0 24.0 23.8 24.7 23.8Change +2.1% +0.8% +0.8% +0.0% +3.8%Table 1: Performance Using Phrase Clusters, Individual Phrases, and TermsCollectionFrequency(in 1425 Docs)123456789+StemmedNumber ofDistinct Phrases324704056128457630921810890281Total PhraseOccurrences34689886642992584173515038558145176Total 39392 60521Table 2: Statistics on Phrase Generation for 1425 CACMDocuments{<linear function>, < compui measur> , < producresult>, <log bound> }{ <princip featur>, < draw design>, < draw display>,< basi spline>, <system repres> }{<error ule>, < ezplain techniqu> , <programinvolv>, <key dala> }{ < subsgant increas> , < time respect>, <increaseprogram>, < respect program> }shows they leave much to be desired as content indi-cators.
We therefore need to consider reasons why theclusters formed were inappropriate.2.2 .2  Stat is t ica l  P rob lemsThe simplest explanation for the low quality of clustersis that not enough text was used in forming them.
Table2 gives considerable vidence that this is the case.
Themajority of occurrences of phrases were of phrases thatoccurred only once, and only 17.6% of distinct phrasesoccurred two or more times.
We restricted cluster for-mation to phrases that occurred at least twice, and mostof these phrases occurred exactly twice.
This means thatwe were trying to group phrases based on the similarityof distributions estimated from very little data.
Church\[3\] and others have stressed the need for large amountsof data in studying statistical properties of words, andthis is even more necessary when studying phrases, withtheir lower frequency of occurrence.Another statistical issue arises in the calculation ofsimilarities between phrases.
We associated with eachphrase a vector of values of the form npc /~ nqc, wherenpc is the number of occurrences of phrase p in docu-ments assigned to Computing Reviews category c, andthe denominator is the total number of occurrences ofall phrases in category c. This is the maximum likeli-hood estimator of the probability that a randomly se-lected phrase from documents in the category will bethe given phrase.
Similarity between phrases was com-puted by applying the cosine correlation \[1\] to these vec-tors.
Problems with the maximum likelihood estimatorfor small samples are well known \[10,4\], so it is possi-ble that clustering will be improved by the use of betterestimators.Another question is whether the clustering methodused might be inappropriate.
Previous research in IRhas not found large differences between different meth-ods for clustering words, and all clustering methods arelikely to be affected by the other problems described inthis section, so experimenting with different clusteringmethods probably deserves lower priority than address-ing the other problems discussed.A final issue is raised by the fact that using clus-ters and phrases together (see Table 3) produced perfor-mance superior to using either clusters or phrases alone.One way of interpreting this is that the seed phrase ofa cluster is a better piece of evidence for the presenceof the cluster than are the other cluster members.
Thisraises the possibility that explicit clusters should not beformed at all, but rather that every phrase be consideredgood evidence for its own presence, and somewhat lessgood evidence for the presence of phrases with similardistributions.
3 Again, investigating this is not likely to3Ken Church suggested this idea to us.291PrecisionRecall Clusters + Phrases + Terms PhrasesLevel Size 2 Size 4 Size 8 Size 12 + Terms Terms0.10 57.4 60.0 59.3 58.5 58.5 56.30.20 46.4 46.4 46.1 45.0 45.4 41.00.30 38.8 39.5 38.9 37.7 38.0 35.70.40 31.3 31.1 31.1 30.8 30.2 29.60.50 23.0 23.1 23.1 23.1 23.4 22.00.60 19.3 19.5 19.5 19.5 19.0 18.80.70 13.9 13.9 13.8 13.7 13.7 13.80.80 9.6 9.8 9.7 9.6 9.5 9.90.90 5.7 5.7 5.7 5.7 5.6 6.11.00 4.2 4.2 4.2 4.2 4.1 4.7Avg.
Prec.
25.0 25.3 25.1 24.8 24.7 23.8Change +5.0% +6.3% +5.5% +4.2% +3.8%Table 3: Performance Using Clusters, Phrases, and Termsbe profitable until other problems are addressed.2.2.8 Weaknesses  in Syntact i c  Phrase  Forma-t ionAnother set of factors potentially affecting the perfor-mance of phrase clustering is the phrases themselves.Our syntactic parsing is by no means perfect, and incor-rectly produced phrases could both cause bad matchesbetween queries and documents, and interfere with thedistributional estimates that clustering is based on.It is difficult to gauge directly the latter effect, but wecan measure whether syntactically malformed phrasesseem to be significantly worse content identifiers thansyntactically correct ones.
To determine this we found allmatches between queries and relevant documents on syn-tactic phrases.
We examined the original query text tosee whether the phrase was correctly formed or whetherit was the result of a parsing error, and did the same forthe phrase occurrence in the document.
We then gath-ered the same data for about 20% of the matches (ran-domly selected) between queries and nonrelevant docu-ments.The results are shown in Table 4.
We see that forboth relevant and nonrelevant documents, the majorityof matches are on syntactically correct phrases.
The pro-portion of invahd matches is somewhat higher for non-relevant documents, but the relatively small differencesuggests that syntactically malformed phrases are not aprimary problem.2.2.4 Cor rect  Phrases  wi th  Poor  Semant icsIn proposing the clustering of syntactic phrases, we ar-gued that the semantic properties of individual phraseswere good, and only their statistical properties neededimproving.
This clearly was not completely true, sincephrases uch as paper gives (from sentences such as Thispaper gives resul$s on...) are clearly very bad indicatorsof a document's content.We believed, however, that such phrases would tendQuery / Relevant Document Matches(229 Pairs Total)Correct Phrasein DocFlawed Phrasein DocCorrect Phrase 84.3% (193) 6.6% (15)in QueryFlawed Phrase 3.5% (8) 4.8% (11)in QueryQuery / Nonrelevant Document Matches(424 Pairs in Random Sample)Correct Phrasein DocFlawed Phrasein DocCorrect Phrase 77.6% (324) 13.0% (55)in QueryFlawed Phrase 4.5% (19) 5.0% (21)in QueryTable 4: Syntactic Correctness of Query Phrases andtheir Occurrences in Documentsto cluster together, and none of the phrases in these clus-ters would match query phrases.
Unfortunately, almostthe opposite happened.
While we did not gather statis-tics, it appeared that these bad phrases, with their rel-atively fiat distribution, proved to be similar to manyother phrases and so were included in many otherwisecoherent clusters.Some of the low quahty phrases had fairly high fre-quency.
Since IR research on clustering of individualwords has shown omitting high frequency words fromclusters to be useful, we experimented with omittinghigh frequency phrases from clustering.
This actuallydegraded performance.
Either frequency is less corre-lated with attribute quahty for phrases than for words,or our sample was too small for rehable frequency esti-mates, or both.Fagan, who did the most comprehensive study \[8\] ofphrasal indexing to date, used a number of techniquesto screen out low quality phrases.
For instance, he only292formed phrases which contained a head noun and oneof its modifiers, while we formed phrases from all pairsof syntactically connected content words.
Since manyof our low quality phrases resulted from main verb /argument combinations, we will reconsider this choice.Fagan also maintained a number of lists of semanti-cally general content words that were to be omitted fromphrases, and which triggered special purpose phrase for-mation rules.
We chose not to replicate this technique,due to the modifications required to our phrase gener-ator, and our misgivings about a technique that mightrequire a separate list of exemption words for each cor-pus.We did, however, conduct a simpler experiment whichsuggests that distinguishing between phrases of varyingqualities will be important.
We had a student from ourlab who was not working on the phrase clustering ex-periments identify for each CACM query a set of pairsof words he felt to be good content identifiers.
We thentreated these pairs of words just as if they had beenthe set of syntactic phrases produced from the query.This gave the results shown in Table 5.
As can be seen,retrieval performance was considerably improved, eventhough the phrases assigned to documents and to clus-ters did not change.
(More results on eliciting good iden-tifiers from users are discussed in [6].
)Given this evidence that not all syntactic phrases wereequally desirable identifiers, we tried one more experi-ment.
We have mentioned that many poor phrases hadrelatively flat distributions across the Computing Re-views categories.
Potentially this very flatness might beused to detect and screen out these low quality phrases.To test this belief, we ranked all phrases which occurredin 8 or more documents by the similarity of their Com-puting Reviews vectors to that of a hypothetical phrasewith even distribution across all categories.The top-ranked phrases, i.e.
those with the flattestdistributions, are found in Table 6.
Unfortunately, whilesome of these phrases are bad identifiers, others are rea-sonably good.
More apparent is a strong correlation be-tween flatness of distribution and occurrence in a largenumber of documents.
This suggests that once again weare being tripped up by small sample estimation prob-lems, this time manifesting itself as disproportionatelyskewed distributions of low frequency phrases.
The useof better estimators may help this technique, but onceagain a larger corpus is clearly needed.3 Future WorkThe fact that phrase clusters provided small improve-ments in performance is encouraging, but the most clearconclusion from the above analysis is that syntacticphrase clustering needs to be tried on much larger cor-pora.
This fact poses some problems for evaluation, sincethe CACM collection is one of the larger of the currentlyavailable IR test collections.
The need for larger IR testcollections is widely recognized, and methods for theirPhraseDESCRIB PAPERALGORITHM PRESENTDESIGN SYSTEMCOMPUT SYSTEMSYSTEM USEPAPER PRESENTLANGUAG PROGRAMDESCRIB SYSTEMREQUIR TIMEDATA STRUCTURPROCESS SYSTEMINFORM SYSTEMOPER SYSTEMPROGRAM USEMODEL SYSTEMEXECUT TIMEPROBLEM SOLUTREQUIR STORAGTECHNIQU USEGENER SYSTEMNo.
Docs5 76454754347712 6223 82 12 659272 62 84524402 2ITable 6: Syntactic Phrases with Least Skewed Distribu-tion Across Computing Reviews Categories, and Numberof Documents They Appear Inconstruction have been planned in detail [23], but finan-cial support has not yet materialized.-LSimilarityUntil larger IR test collections are available, we arepursuing two other approaches for experimenting withphrase clustering.
The first is to form clusters on a cor-pus different from the one on which the retrieval experi-ments are performed.
If the content and style of the textsare similar enough, the clusters should still be usable.
Tothis end, we have obtained a collection of approximately167,000 MEDLINE records (including abstracts and ti-tles, but no queries or relevance judgments) to be usedin forming clusters.
The clusters will be tested on twoIR test collections which, while much smaller, are alsobased on MEDLINE records.1A second approach is to  experiment with text cate-gorization, rather than text retrieval, since large collec-tions of categorized text are available.
The same largeMEDLINE subset described above can be used for thiskind of experiment, and we have also obtained the train-ing and test data (roughly 30,000 newswire stories) usedin building the CONSTRUE text categorization system1121.Besides the need for repeating the above experimentswith more text, our analysis also suggests that somemethod of screening out low quality phrases is needed.We plan to experiment first with restricting phrases tonouns plus modifiers, as Fagan did, and with screeningout phrases based on flatness of distribution, using moretext and better small sample estimatiors.
Improving thesyntactic parsing method does not seem to be an  imme-diate need.PrecisionRecall Clusters + Terms PhrasesLevel Size 2 Size 4 Size 8 Size 12 + Terms Terms0.10 60.7 61.9 61.5 61.4 61.4 56.30.20 45.8 45.9 45.9 45.9 45.2 41200.30 40.6 40.3 39.8 39.8 39.5 35.70.40 34.2 33.4 33.5 33.5 33.2 29.60.50 25.0 25.1 25.2 25.2 25.3 22.00.60 19.8 20.7 20.7 20.6 20.9 18.80.70 13.8 14.6 14.5 14.6 14.6 13.80.80 9.4 10.2 10.0 10.0 10.0 9.90.90 5.6 6.3 6.2 6.3 6.2 6.11.00 4.2 4.9 4.9 4.9 4.9 4.7Avg.
Prec.
25.9 26.3 26.2 26.2 26.1 23.8Change +8.8% +10.5% +10.1% +10.1% +9.7%Table 5: Performance With Human-Selected Query Phrases4 SummaryText-based systems of all kinds are an area of increasingresearch interest, as evidenced by the recent AAAI Sym-posium on Text-Based Intelligent Systems, by fundinginitiatives uch as the T IPSTER portion of the DARPAStrategic Computing Program, and by an increase inthe number of research papers proposing the applicationof various artificial intelligence techniques to text clas-sification problems.
This interest is driven by both anundeniable need to cope with large amounts of data inthe form of online text, and by the resource that thistext represents for intelligent systems.In this paper we have discussed the nature of textclassification, which is the central task of most currenttext-based systems, and which is an important compo-nent of most proposed text comprehension systems, aswell.
We introduced a theoretical model of how textrepresentation impacts the performance of text classifi-cation systems, and described how the performance ofthese systems is typically evaluated.We also summarized the results of our ongoing re-search on syntactic phrase clustering.
Perhaps the mostimportant point to stress about this work is the com-plexity of evaluating text classification systems, particu-larly those involving natural anguage processing or ma-chine learning techniques, and the need to examine re-sults carefully.This should not discourage valuation, however.
If itis difficult to verify good text classification techniquesthrough controlled experiments, it is impossible to do sopurely through intuition or theoretical arguments.
Thehistory of IR is full of plausible techniques which exper-iment has shown to be ineffective.
Only through carefulevaluation will progress be likely.AcknowledgementsThe work reported here is part of my dissertation workunder the supervision of W. Bruce Croft, whose guid-ance has been invaluable.
We thank Longman Group,Ltd.
for making available to us the online version ofthe Longman Dictionary of Contemporary English.
Thisresearch was supported by the NSF under grant IRI-8814790, by AFOSR under grant AFOSR-90-0110, andby an NSF Graduate Fellowship.
Anil Jain, Mike Suther-land, and Mel Janowicz provided advice on cluster anal-ysis.
The syntactic parser used was designed and builtby John Brolio, who was also the principal designer ofthe syntactic grammar.
Raj Das generated the hand-selected phrases for the CACM queries.
Robert Krovetzand Howard Turtle made helpful comments on a draft ofthis paper.References\[1\] Michael R. Anderberg.
Cluster Analysis for Appli-cations.
Academic Press, New York, 1973.\[2\] Peter Biebricher, Norbert Fuhr, Gerhard Lustig,Michael Schwantner, and Gerhard Knorz.
Theautomatic indexing system AIR/PHYS-- f rom re-search to application.
In Eleventh InternationalConference on Research 64 Development in Infor-mation Retrieval, pages 333-342, 1988.\[3\] Kenneth Church, William Gale, Patrick Hanks, andDonald Hindle.
Parsing, word associations, andtypical predicate-argument relations.
In SecondDARPA Speech and Natural Language Workshop,pages 75-81, Cape Cod, MA, October 1990.
Alsoappeared in Proceedings of the International Work-shop on Parsing Technologies, CMU, 1989.\[4\] Kenneth W. Church and William A. Gale.
En-hanced Good-Turing and Cat-Cal: Two new meth-ods for estimating probabilities of English bigrams.In Second DARPA Speech and Natural LanguageWorkshop, pages 82-91, Cape Cod, MA, October1990.\[5\] W. B. Croft.
Experiments with representation ia document retrieval system.
Information Technol-294ogy: Research and Development, 2:1-21, 1983.\[6\] W. Bruce Croft and Raj Das.
Experiments withquery acquisition and use in document retrieval sys-tems.
In Thirteenth Annual International A CM SI-GIR Conference on Research and Development inInformation Retrieval, 1990.
To appear.\[7\] Carolyn J. Crouch.
A cluster-based approach tothesaurus construction.
In Eleventh InternationalConference on Research ~ Development in Infor-mation Retrieval, pages 309-320, 1988.\[8\] Joel L. Fagan.
Experiments in Automatic PhraseIndexing for Document Retrieval: A Comparison ofSyntactic and Non-Syntactic Methods.
PhD thesis,Department of Computer Science~ Cornell Univer-sity, September 1987.\[9\] Edward A.
Fox, Gary L. Nunn, and Whay C. Lee.Coefficients for combining concept classes in a col-lection.
In Eleventh International Conference onResearch ~4 Development in Information Retrieval,pages 291-307, 1988.\[10\] Norbert Fuhr and Hubert Huther.
Optimum prob-ability estimation from empirical distributions.
In-formation Processing and Management, pages 493-507, 1989.\[11\] Karen A. Hamill and Antonio Zamora.
The use oftitles for automatic document classification.
Jour-nal of the American Society for Information Sci-ence, pages 396-402, 1980.\[12\] Philip J. Hayes, Laura E. Knecht, and Monica J.Cellio.
A news story categorization system.
In Sec-ond Conference on Applied Natural Language Pro-cessing, pages 9-17, 1988.\[13\] J. Kittler.
Feature selection and extraction.
InTzay Y.
Young and King-Sun Fu, editors, Hand-book of Pattern Recognition and Image Processing,pages 59-83.
Academic Press, Orlando, 1986.\[14\] David D. Lewis.
Representation and Learning inInformation Retrieval.
PhD thesis, University ofMassachusetts at Amherst, 1990.
In preparation.\[15\] David D. Lewis.
Text representation for text clas-sification.
In P. S. Jacobs, editor, Text-Based In-telligent Systems: Current Research in Text Anal-ysis, Information Extraction, and Reirieval, 1990.Selected papers from the AAAI Spring Symposiumon Text-Based Intelligent Systems.
Available as atechnical report from General Electric Research &Development, Schenectady, NY, 12301.\[17\] M. E. Maron.
Automatic indexing: An experimen-tal inquiry.
Journal of the Association for Comput-ing Machinery, 8:404-417, 1961.\[18\] M. F. Porter.
An algorithm for suffix stripping.Program, 14(3):130-137, July 1980.\[19\] G. Salton, C. S. Yang, and C. T. Yu.
A theory ofterm importance in automatic text analysis.
Jour-nal of the American Society for Information Sci-ence, pages 33-44, January-February 1975.\[20\] Gerard Salton.
Another look at automatic text-retrieval systems.
Communications of the A CM,29(7):648-656, July 1986.\[21\] Gerard Salton and Chris Buckley.
Improving re-trieval performance by relevance feedback.
Journalof the American Society for Information Science,41(4):288-297, 1990.\[22\] K. Sparck Jones and R. G. Bates.
Research on auto-matic indexing 1974 - 1976 (2 volumes).
Technicalreport, Computer Laboratory.
University of Cam-bridge, 1977.\[23\] K. Sparck Jones and C. J. van Rijsbergen.
Reporton the need for and provision of an 'ideal' informa-tion retrieval test collection.
Technical report, Uni-versity Computer Laboratory; University of Cam-bridged 1975.\[24\] K. Sparck Jones and C. J. van Rijsbergen.
Infor-mation retrieval test collections.
Journal of Docu-mentation, 32(1):59-75, 1976.\[25\] Karen Sparck Jones, editor.
Information RetrievalExperiment.
Butterworths, London, 1981.\[26\] Howard Turtle and W. Bruce Croft.
Inference net-works for document retrieval.
In Thirteenth AnnualInternational A CM SIGIR Conference on Researchand Development in Information Retrieval, 1990.\[27\] Cornel.ls J. van Rijsbergen.
Retrieval effectiveness.In Karen Sparck Jones, editor, Information Re-trieval Experiment, chapter 3.
Butterworths, Lon-don, 1981.\[28\] Natasha Vleduts-Stokolov.
Concept recognition ian automatic text-processing system for the life sci-ences.
Journal of the American Society for Infor-mation Science, 38:269-287, 1987.\[16\] David D. Lewis and W. Bruce Croft.
Term clus-tering of syntactic phrases.
In Thirteenth AnnualInternational A CM SIGIR Conference on Researchand Development in Information Retrieval, 1990.To appear.295
