Combining Stochastic and Rule-Based Methods for Disambiguationin Agglutinative LanguagesEzeiza N., Alegria I., Arriola J.M., Urizar R.Informatika Fakultatea649 P.K Donostia E-20080jibecran@si.ehu.eshttp://ixa.si.ehu.esAbstractIn this paper we present the results of thecombination of stochastic and rule-baseddisambiguation methods applied to Basquelanguagel.
The methods we have used indisambiguation are Constraint Grammarformalism and an HMM based taggerdeveloped within the MULTEXT project.As Basque is an agglutinative language, amorphological nalyser is needed to attachall possible readings to each word.
Then,CG rules are applied using all themorphological features and this processdecreases morphological ambiguity oftexts.
Finally, we use the MULTEXTproject tools to select just one from thepossible remaining tags.Using only the stochastic method the errorrate is about 14%, but the accuracy may beincreased by about 2% enriching the lexi-con with the unknown words.
When bothmethods are combined, the error rate of thewhole process is 3.5%.
Considering thatthe training corpus is quite small, that theHMM model is a first order one and thatConstraint Grammar of Basque language isstill in progress, we think that this com-bined method can achieve good results,and it would be appropriate for otheragglutinative languages.IntroductionBased on the results of the combination ofstochastic and rule-based disambiguationmethods applied to Basque language, we willshow that the results of the combination aresignificantly better than the ones obtainedapplying the methods eparately.As Basque is an agglutinative and highly in-This research has been supported by the EducationDepartment of the Government of the BasqueCountry and the Interministerial Commision forScience and Technology.Aduriz I.UZEIAldapeta, 20.Donostia E-20009uzei@sarenet.esfleeted language, a morphological nalyser isneeded to attach all possible interpretations toeach word.
This process, which may not benecessary in other languages such as English,makes the tagging task more complex.
We useMORFEUS, a robust morphological nalyserfor Basque developed at the University of theBasque Country (Alegria et al, 1996).
Wepresent it briefly in section 1, in the overviewof the whole system, the lemmatiser/tagger forBasque EUSLEM.We have added to MOKFEUS a lemma dis-ambiguation process, described in section 2,which discards some of the analyses of theword based on statistical measures.Another important issue concerning a tagger isthe tagset itself.
We discuss the design of thetagset in section 3.In section 4, we present the results of the ap-plication of rule-based and stochastic disambi-guation methods to Basque.These results are deeply improved by combin-ing both methods as explained in section 5.Finally, we discuss some possible improve-ments of the system and future research.1 Overv iew of  the systemThe disambiguation system is integrated inEUSLEM, a lemmatiser/tagger fo Basque(Aduriz et al, 1996).
EUSLEM has three mainmodules:?
MORFEUS, the morphological analyserbased on the two-level formalism.
It is a ro-bust and wide coverage analyser for Basque.?
the module that treats multiword lexicalunits.
It has not been used in the experimentsin order to simplify the process.?
the disambiguation module, which will bedescribed in sections 5and 6.MORFEUS plays an important role in thelemmatiser/tagger, because it assigns every to-ken all the morphological features.
The mostimportant functions are:?
incremental analysis, which is divided in380three phases, using the two level formalismin all of them: 1) the standard analyser pro-cesses words according to the standard lexi-con and standard rules of the language; 2)the analyser of linguistic variants analysesdialectal variants and competence errors2;and 3) the analyser of unknown words orguesser processes the remaining words.?
lemma disambiguation, presented below.2 Lemma disambiguationThe lemma disambiguation has been added tothe previously developed analyser for two mainreasons:?
the average number of interpretations i  un-known words is significantly higher than instandard words.?
there could be more than one lemma per tag.Since the disambiguation module won't dealwith this kind of ambiguity, it has to besolved to lemmatise the text.We use different methods for the disambigua-tion of linguistic variants and unknown words.In the case of linguistic variants we try to selectthe lemma that is "nearest" o the standard oneaccording to the number of non-standard mor-phemes and rules.
We choose the interpretationthat has less non-standard uses.before aftervariants 2.58 2.52unknown 13.1 6.21Table 1- Number of readings.In the case of unknown words, the procedureuses the following criteria:?
for each category and subcategory pair, leaveat least one interpretation.?
assign a weight o each lemma according tothe final trigram and the category and subca-tegory pair.?
select he lemma according to its length andweight -best combination ofhigh weight andshort lemma.These procedures have been tested with a smallcorpus and the produced error-rate is 0.2%.This is insignificant considering that the avera-ge number of interpretations of unknownwords decreases by 7, as shown in table 1.3 Designing the tagsetThe choice of a tagset is a critical aspect whendesigning a tagger.
Before defining the tagset2 This module is very useful since Basque is still innormalisation process.we have had to take some aspects into account:there was not any exhaustive tagset for auto-matic use, and the output of the morphologicalanalyser is too rich and does not offer a directlyapplicable tagset.While designing the general tagset, we tried tomeet he following requirements:?
it had to take into account all the problemsconcerning ellipsis, derivation and composi-tion (Aduriz et al, 1995).?
in addition, it had to be general, far from adhoc tagsets.?
it had to be coherent with the informationprovided by the morphological nalyser.Bearing all these considerations in mind, thetagset has been structured in four levels:?
in the first level, general categories are inclu-ded (noun, verb, etc.).
There are 20 tags.?
in the second level each category tag is fur-ther refined by subcategory tags.
There are48 tags.?
the third level includes other interesting in-formation, as declension case, verb tense,etc.
There are 318 tags in the training cor-pus, but using a larger corpus we found 185new tags.?
the output of the morphological analysisconstitutes the last level of tagging.
There are2,943 different interpretations i  this trainingcorpus, but we have found more than 9,000in a larger cfirstorpus.ambi~;uity rate ta~s/token35.11% 1.48second 40.68%62.24% thirdfourth 64.42%1.572.203.48Table 2- Ambiguity of each level.The morphological mbiguity will differ de-pending on the level of tagging used in eachcase, as shown in table 2.4 Morphological DisambiguationThere are two kinds of methods for morpho-logical disambiguation: onone hand, statisticalmethods need little effort and obtain very goodresults (Church, 1988; Cutting etal., 1992), atleast when applied to English, but when we tryto apply them to Basque we encounter addi-tional problems; on the other hand, somerule-based systems (Brill, 1992; Voutilainen etaL, 1992) are at least as good as statisticalsystems and are better adapted to free-orderlanguages and agglutinative languages.
So, we381have selected one of each group: ConstraintGrammar formalism (Karlsson et aL, 1995)and the HMM based TATOO tagger(Armstrong et aL, 1995), which has been de-signed to be applied it to the output of a mor-phological analyser and the tagset can beswitched easily without changing the inputtext.?
second \[\] third 70M M* M+CG M*+CGFigure 1-Initial ambiguity3.We have used the second and third levelstagsets for the experiments and a small corpus-28,300 words- divided in a training corpus of27,000 words and a text of 1,300 words fortesting.?
second \[\] thirdM M* M+CG M*+CGFigure 2- Number of tags per token.The initial ambiguity of the training corpus isrelatively high, as shown infig.
1, and the ave-rage number of tags per token is also higherthan in other languages - ee fig.
2.
The num-ber of ambiguity classes is also high -290 and1138 respectively- and some of the classes inthe test corpus aren't in the training corpus,specially in the 3rd level tagset.
This meansthat the training corpus doesn't cover all thephenomena ofthe language, so we would needa larger corpus to assure that it is general andrepresentative of the language.We tried both supervised and unsupervised 43 These measures are taken after the process denotedin each column: M- '  morphological nalysis; M*morphological nalysis with enriched lexicon;CG --, Contraint Grammar.4 Even if we used the same corpus for both training382training using the 2nd level tagset and only su-pervised training using the third level tagset.The results are shown infig.
3(S).
Accuracy isbelow 90% and 75% respectively.
Using un-known words to enrich the lexicon, the resultsare improved -seefig.
3(S*)-, but are still farfrom the accuracy of other systems.We have also written some biases -to be exact11- to correct he most evident errors in the2nd level.
We didn't write more biases for thefollowing reasons:?
They can use just the previous tag to changethe probabilities, and in some cases we needa wider context to the left and/or to the right.?
They can't use the lemma or the word.?
From the beginning of this research, our in-tention was to combine this method withConstraint Grammar.Using these biases, the error rate decreases by5% in supervised training and by 7% in unsu-pervised one-fig.
3(S+B).We also used biases 5with the enriched lexiconand the accuracy increases by less than 2% inboth experiments -fig.
3(S+B*).
This is not agreat improvement when trying to decrease anerror rate greater than 10%, but the enrichmentof the lexicon may be a good way to improvethe system.The logical conclusions of these experimentsare:?
the statistical approach might not be a goodapproach for agglutinative and free-orderlanguages -as pointed out by Oflazer andKuruOz (1994).?
writing good disambiguation rules may real-ly improve the accuracy of the disambigua-tion task.As we mentioned above, it is difficult o defineaccurate rules using stochastic models, so weuse the Constraint Grammar for Basque 6(Aduriz et al, 1997) for this purpose.The morphological disambiguator uses around800 constraint rules that discard illegitimateanalyses on the basis of local or global contextmethods to compare the results, the latterperformed better using a larger corpus.These biases were written taking into account heerrors made in the first experiment.The rules were designed having syntactic analysisas the main goal.conditions.
The application of CG formalism 7is quite satisfactory, obtaining a recall of99,8% but there are still 2.16 readings per to-ken.
The ambiguity rate after applying CG ofBasque drop from 41% to 12% using 2nd leveltagset and 64% to 22% using 3rd level tagset-fig.
2- and the error rate in terms of theta ~sets i approximately 1%.r.
)Figure 3- Accuracy of the experiments 8.5 Combining methodsThere have been some approaches tothe com-bination of statistical and linguistic methodsapplied to POS disambiguation (Leech et al,1994; Tapanainen and Voutilainen, 1994;Oflazer and Tiar, 1997) to improve the accuracyof the systems.Oflazer and "FOr (1997) use simple statistical in-formation and constraint rules.
They include aconstraint application paradigm to make thedisambiguation i dependent of the rule se-quence.The approach of Tapanainen and Voutilainen(1994) disambiguates the text using XT andENGCG independently; then the ambiguitiesremaining in ENGCG are solved using the re-suits of XT.We propose a similar combination, applyingboth disambiguation methods one after theother, but training the stochastic tagger on theoutput of the CG disambiguator.Since in the output of CG of Basque the avera-7 These results were obtained using the CG-2 parser,which allows grouping the rules in different orderedsubgrammars depending on their accuracy.
Thismorphological disam-biguator uses only the firsttwo subgrammars.s S '--* stochastic; * --* with enriched lexicon;B --, with biases; CG --, Constraint Grammar.ge number of possible tags is still high -1.13-1.14 for 2nd level tagset and 1.29-1.3 for 3rdlevel tagset- and the stochastic tagger producesrelatively high error rate -around 15% in 2ndlevel and almost 30% in 3rd level-, we firstapply constraint rules and then train thestochastic tagger on the output of the rule-based disambiguator.Fig.
I(CG) shows the ambiguity left byBasque CG in terms of the tagsets.
Althoughthe ambiguity rate is significantly lower than inprevious experiments, the remaining ambigui-ties are hard to solve even using all the lingu|s-tic information available.We have also experimented with the enrichedlexicon and the results are very encouraging, asshown in fig.
3(CG+S*).
Considering that thenumber of ambiguity classes is still high-around 240 in the 2nd level and more than1000 in the 3rd level-, we think that the resultsare very good.For the 2nd level tagging, the error rate aftercombining both methods is less than 3.5%,half of it comes from MORFEUS and BasqueCG and the rest is made by the stochastic dis-ambiguation.
This is due to the fact that gene-rally the types of ambiguity remaining after CGis applied are hard to solve.Examining the errors, we find that half of themare made in unknown words trying to distin-guish between proper names of persons andplaces.
We use two different tags because it isinteresting for some applications and the tagsetwas defined based on morphological features.This kind of ambiguity is very hard to solveand in some applications this distinction is notimportant.
So in this case the accuracy of thetagger would be 98%.The accuracy in the third level tagset is around91% using the combined method, which is nottoo bad bearing in mind the number of tags-310-, the precision of the input-1.29tags/token- and that the training corpus doesnot cover all the phenomena of the language 9.We want to point out that the experiments withthe 3rd level tagset show even clearer that thecombined method performs much better thanthe stochastic.
Moreover, we think that CGdisambiguation is even convenient at this levelbecause of the initial ambiguity -63%.9 In a corpus of around 900,000 words we found 185new tags and more than 1700 new classes.383ConclusionWe have presented the results of applyingdifferent disambiguation methods to an agglu-tinative and highly inflected language with arelatively free order in sentences.On one hand, this latter characteristic ofBasque makes it difficult to learn appropriateprobabilities, particularly first order stochasticmodels.
We solve this problem in part with CGfor Basque, which uses a larger context andcan tackle the free word-order problem.However, it is a very hard work to write a fullgrammar and disambiguate t xts completelyusing CG formalism, so we have complemen-ted this method with a stochastic disambigua-tion process and the results are quiteencouraging.Comparing the results of Tapanainen andVoutilainen (1994) with ours, we see that theyachieve 98.5% recall combining 1.02-1.04readings from ENGCG and 96% accuracy inXT, while we begin with 1.13-1.14 readings,the quality of our stochastic tagger is less than90% and our result is better than 96%.Unlike Tapanainen and Voutilainen (1994), wethink that training on the output of the CG thestatistical disambiguation works quite better 10,at least using such a small training corpus.
Inthe future we will compile a larger corpus andto decrease the number of readings left by CG.On the other hand, we think that the informa-tion given by the second level tag is not suffi-cient to decide which of the choices is thecorrect one, but the training corpus is quitesmall.
However, translating the results of the3rd level to the 2nd one we obtain around 97%of accuracy.
So, we think that improving the3rd level tagging would improve the 2nd leveltagging too.
We also want to experiment unsu-pervised learning in the 3rd level tagging with alarge training corpus.Along with this, the future research will focuson the following processes:?
morphosyntactic reatment for the elaborationof morphological information (nominalisa-tion, ellipsis, etc.).?
treatment of multiword lexical units(MWLU).
We are planning to integrate thismodule to process unambiguous MWLU, todecreases the ambiguity rate and to make theinput of the disambiguation more precise.10 With their method accuracy is 2% lower.AcknowledgementWe are in debt with the research-team of theGeneral Linguistics Department of theUniversity of Helsinki for giving uspermission to use CG Parser.
We also want tothank Gilbert Robert for tuning TATOO.ReferencesAduriz I., Aldezabal I., Alegria I., Artola X.,Ezeiza N., Urizar R. (1996) EUSLEM: Alemmatiser/tagger fo Basque.
EURALEX.Aduriz I., Alegria I., Arriola J.M., Artola X.,Diaz de Ilarraza A., Ezeiza N., Gojenola K.,Maritxalar M. (1995) Different issues in thedesign of a lemmatizer/tagger fo Basque.
"From text to tag" SlGDAT, EACLWorkshop.Aduriz, I., Arriola, J.M., Artola, X., Diaz deIllarraza, A., Gojenola, K., Maritxalar, M.(1997) Morphosyntactic Disambiguation forBasque based on the Constraint GrammarFormalism.
RANLP, Bulgaria.Alegria, I., Sarasola, K., Urkia, M. (1996)Automatic morphological nalysis of Basque.Literary and Linguistic Computing Vol 11, N.4.Armstrong S., Russel G., Petitpierre D., RobertG.
(1995) An open architecture forMultilingual Text Processing.
EACL'95.
vol 1,101-106.Brill E. (1992) A simple rule-based part ofspeech tagger.
ANLP, 152-155.Church K. W. (1988) A stochastic parts pro-gram and phrase parser for unrestricted text.ANLP, 136-143.Cutting D., Kupiec J., Pedersen J., Sibun P.(1992) A practical part-of-speech tagger.ANLP, 133-140.Karlsson F., Voutilainen A., Heikkil~i J., AnttilaA.
(1995)Constraint Grammar: Language-in-dependent System for Parsing UnrestrictedText.
Mouton de Gruyter.Leech G., Garside R., Bryan M. (1994)CLAWS4: The tagging of the British NationalCorpus.
COLING, 622-628.Oflazer K., Kururz I.
(1994) Tagging andMorphological Disambiguation of TurkishText.
ANLP, 144-149.Oflazer K., Tiir G. (1997) MorphologicalDisambiguation by Voting Constraints.
ACL-EACL, 222-229.Tapanainen P., Voutilainen A.
(1994) TaggingAccurately - Don "t guess if  you know.
ANLP,47-52.384
