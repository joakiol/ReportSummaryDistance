Maximum Entropy Models for FrameNet ClassificationMichael Fleischman, Namhee Kwon and Eduard HovyUSC Information Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292-6695{fleisch, nkwon, hovy }@ISI.eduAbstractThe development of FrameNet, a largedatabase of semantically annotated sen-tences, has primed research into statisticalmethods for semantic tagging.
We ad-vance previous work by adopting aMaximum Entropy approach and by usingprevious tag information to find the high-est probability tag sequence for a givensentence.
Further we examine the use ofsentence level syntactic pattern features toincrease performance.
We analyze ourstrategy on both human annotated andautomatically identified frame elements,and compare performance to previouswork on identical test data.
Experimentsindicate a statistically significant im-provement (p<0.01) of over 6%.1 IntroductionRecent work in the development of FrameNet, alarge database of semantically annotated sentences,has laid the foundation for statistical approaches tothe task of automatic semantic classification.The FrameNet project seeks to annotate a largesubset of the British National Corpus with seman-tic information.
Annotations are based on FrameSemantics (Fillmore, 1976), in which frames aredefined as schematic representations of situationsinvolving various frame elements such as partici-pants, props, and other conceptual roles.In each FrameNet sentence, a single targetpredicate is identified and all of its relevant frameelements are tagged with their semantic role (e.g.,Agent, Judge), their syntactic phrase type (e.g.,NP, PP), and their grammatical function (e.g., ex-ternal argument, object argument).
Figure 1 showsan example of an annotated sentence and its appro-priate semantic frame.She  clapped  her hands  in inspiration.Frame:        Body-MovementFrame Elements:Agent     Body Part Cause-NP            -NP  -PP-Ext             -Obj -CompFigure 1.
Frame for lemma ?clap?
shown with threecore frame elements and a sentence annotated with ele-ment type, phrase type, and grammatical function.As of its first release in June 2002, FrameNethas made available 49,000 annotated sentences.The release contains 99,000 annotated frame ele-ments for 1462 distinct lexical predicates (927verbs, 339 nouns, and 175 adjectives).While considerable in scale, the FrameNet da-tabase does not yet approach the magnitude of re-sources available for other NLP tasks.
Each targetpredicate, for example, has on average only 30 sen-tences tagged.
This data sparsity makes the task oflearning a semantic classifier formidable, and in-creases the importance of the modeling frameworkthat is employed.2 Related WorkTo our knowledge, Gildea and Jurafsky (2002)is the only work to use FrameNet to build a statis-tically based semantic classifier.
They split theproblem into two distinct sub-tasks: frame elementidentification and frame element classification.
Inthe identification phase, syntactic information isextracted from a parse tree to learn the boundariesof the frame elements in a sentence.
In the classi-fication phase, similar syntactic information isused to classify those elements into their semanticroles.In both phases Gildea and Jurafsky (2002)build a model of the conditional probabilities of theclassification given a vector of syntactic features.The full conditional probability is decomposed intosimpler conditional probabilities that are then in-terpolated to make the classification.
Their bestperformance on held out test data is achieved usinga linear interpolation model:where r is the class to be predicted, x is the vectorof syntactic features, xi is a subset of those fea-tures, ?i is the weight given to that subset condi-tional probability (as determined using the EMalgorithm), and m is the total number of subsetsused.
Using this method, they report a test set ac-curacy of 78.5% on classifying semantic roles andprecision/recall scores of .726/.631 on frame ele-ment identification.We extend Gildea and Jurafsky (2002)?s initialeffort in three ways.
First, we adopt a maximumentropy (ME) framework in order to learn a moreaccurate classification model.
Second, we includefeatures that look at previous tags and use previoustag information to find the highest probability se-mantic role sequence for a given sentence.
Finally,we examine sentence-level patterns that exploitmore global information in order to classify frameelements.
We compare the results of our classifierto that of Gildea and Jurafsky (2002) on matchedtest sets of both human annotated and automati-cally identified frame elements.3Semantic Role ClassificationTraining (36,993 sentences / 75,548 frame ele-ments), development (4,000 sentences / 8,167frame elements), and held out test sets (3,865 sen-tences / 7,899 frame elements) were obtained inorder to exactly match those used in Gildea andJurafsky (2002)1 .
In the experiments presentedbelow, features are extracted for each frame ele-ment in a sentence and used to classify that ele-ment into one of 120 semantic role categories.
Theboundaries of each frame element are given basedon the human annotations in FrameNet.
In Section4, experiments are performed using automaticallyidentified frame elements.3.11 Data sets (including parse trees) were obtained from DanGildea via personal communication.FeaturesFor each frame element, features are extractedfrom the surface text of the sentence and from anautomatically generated syntactic parse tree(Collins, 1997).
The features used are describedbelow:)|()|(0imii xrpxrp ?== ?
?
Target predicate (tar): Although there maybe many predicates in a sentence with associ-ated frame elements, classification operates ononly one target predicate at a time.
The targetpredicate is the only feature that is not ex-tracted from the sentence itself and must begiven by the user.
Note that the frame whichthe target predicate instantiates is not given,leaving any word sense ambiguities to be han-dled implicitly by the classifier.2?
Phrase type (pt):  The syntactic phrase type ofthe frame element (e.g.
NP, PP) is extractedfrom the parse tree of the sentence by findingthe constituent in the tree whose boundariesmatch the human annotated boundaries of theelement.
In cases where there exists no con-stituent that perfectly matches the element, theconstituent is chosen which matches the largesttext span of the element and has the same left-most boundary.?
Syntactic head (head): The syntactic heads ofthe frame elements are extracted from theframe element?s matching constituent (as de-scribed above) using a heuristic method de-scribed by Michael Collins.
3   This methodextracts the syntactic heads of constituents;thus, for example, the second frame element inFigure 1 has head ?hands,?
while the thirdframe element has head ?in.??
Logical Function (lf): A simplification of thegrammatical function annotation (see section1) is extracted from the parse tree.
Unlike the2 Because of the interaction of head word features with thetarget predicate, we suspect that ambiguous lexical items donot account for much error.
This question, however, will beaddressed explicitly in future work.3 http://www.ai.mit.edu/people/mcollins/papers/headsTable 1.
Feature sets used in ME frame element classifier.
Shows individual feature sets, example featurefunction from that set, and total number of feature functions in the set.
Examples taken from frame element?in inspiration,?
shown in Figure 1.Number Feature Set Example function Number of Functionsin Feature Set0 f(r, tar) f(CAUSE, ?clap?
)=1 6,5181 f(r, tar, pt) f(CAUSE, ?clap?, PP)=1 12,0302 f(r, tar, pt, lf) f(CAUSE, ?clap?, PP, other)=1 14,6153 f(r, pt, pos, voice) f(CAUSE, NP, ?clap?, active)=1 1,2154 f(r, pt, pos, voice ,tar) f(CAUSE, PP, after, active, ?clap?
)=1 15,6025 f(r ,head) f(CAUSE, ?in?
)=1 18,5046 f(r, head, tar) f(CAUSE, ?in?, ?clap?
)=1 38,2237 f(r, head, tar, pt) f(CAUSE, ?in?, ?clap?, PP)=1 39,7408 f(r, order, syn) f(CAUSE, 2,[NP-Ext,Target,NP-Obj,PP-other])=113,2289 f(r, tar, order, syn) f(CAUSE, ?clap?, 2,[NP-Ext,Target,NP-Obj,PP-other])=140,58010 f(r,r_-1) f(CAUSE, BODYPART)=1 1,15811 f(r,r_-1,r_-2) f(CAUSE, BODYPART, AGENT)=1 2,030Total Number of Features:  203,443full grammatical function, the lf can have onlyone of three values: external argument, objectargument, other.
A node is considered an ex-ternal argument if it is an ancestor of an Snode, an object argument if it is an ancestor ofa VP node, and other for all other cases.
Thisfeature is only applied to frame elementswhose phrase type is NP.?
Position (pos): The position of the frame ele-ment relative to the target (before, after) is ex-tracted based on the surface text of thesentence.?
Voice (voice): The voice of the sentence (ac-tive, passive) is determined using a simpleregular expression passed over the surface textof the sentence.?
Order (order): The position of the frame ele-ment relative to the other frame elements in thesentence.
For example, in the sentence fromFigure 1, the element ?She?
has order=0, while?in inspiration?
has order=2.?
Syntactic pattern (pat): The sentence levelsyntactic pattern of the sentence is generatedby looking at the phrase types and logicalfunctions of each frame element in the sen-tence.
For example, in the sentence: ?Alexan-dra bent her head;?
?Alexandra?
is an externalargument Noun Phrase, ?bent?
is a targetpredicate, and ?her head?
is an object argu-ment Noun Phrase.
Thus, the syntactic patternassociated with the sentence is [NP-ext, target,NP-obj].These syntactic patterns can be highly in-formative for classification.
For example, inthe training data, a syntactic pattern of [NP-ext, target, NP-obj] given the predicate bendwas associated 100% of the time with theFrame Element pattern: ?AGENT TARGETBODYPART.??
Previous role (r_n): Frame elements do notoccur in isolation, but rather, depend verymuch on the other elements in a sentence.This dependency can be exploited in classifica-tion by using the semantic roles of previouslyclassified frame elements as features in theclassification of a current element.
This strat-egy takes advantage of the fact that, for exam-ple, if a frame element is tagged as an AGENTit is highly unlikely that the next element willalso be an AGENT.The previous role feature indicates theclassification that the n-previous frame ele-ment received.
During training, this informa-tion is provided by simply looking at the trueclasses of the frame element occurring n posi-tions before the target element.
During testing,hypothesized classes of the n elements are usedand Viterbi search is performed to find themost probable tag sequence for a sentence.3.2 Maximum EntropyME models implement the intuition that the bestmodel will be the one that is consistent with the setof constrains imposed by the evidence, but other-wise is as uniform as possible (Berger et al, 1996).We model the probability of a semantic role rgiven a vector of features x according to the MEformulation below:3.33.4ExperimentsWe present three experiments in which differentfeature sets are used to train the ME classifier.
Thefirst experiment uses only those feature combina-tions described in Gildea and Jurafsky (2002) (fea-ture sets 0-7 from Table 1).
The secondexperiment uses a super set of the first and incor-porates the syntactic pattern features describedabove (feature sets 0-9).
The final experiment usesthe previous tags and implements Viterbi search tofind the best tag sequence (feature sets 0-11).
?==niixxrfZxrp0i )],(exp[1)|( ?Here Zx is a normalization constant, fi(r,x) is a fea-ture function which maps each role and vectorelement (or combination of elements) to a binaryvalue, n is the total number of feature functions,and ?i is the weight for a given feature function.The final classification is just the role with highestprobability given its feature vector and the model.We further investigate the effect of varying twoaspects of classifier training: the standard deviationof the Gaussian priors used for smoothing, and thenumber of sentences used for training.
To examinethe effect of optimizing the standard deviation, arange of values was chosen and a classifier wastrained using each value until performance on adevelopment set ceased to improve.The feature functions that we employ can bedivided into feature sets based upon the types andcombinations of features on which they operate.Table 1 lists the feature sets that we use, as well asthe number of individual feature functions theycontain.
The feature combinations were chosenbased both on previous work and trial and error.
Infuture work we will examine more principled fea-ture selection techniques.To examine the effect of training set size onperformance, five data sets were generated fromthe original set with 36, 367, 3674, 7349, and24496 sentences, respectively.
These data setswere created by going through the original set andselecting every thousandth, hundredth, tenth, fifth,and every second and third sentence, respectively.It is important to note that the feature functionsdescribed here are not equivalent to the subsetconditional distributions that are used in the Gildeaand Jurafsky model.
ME models are log-linearmodels in which feature functions map specificinstances of syntactic features and classes to binaryvalues (e.g., if a training element has head=?in?and role=CAUSE, then, for that element, the featurefunction f(CAUSE, ?in?)
will equal 1).
Thus, ME isnot here being used as another way to find weightsfor an interpolated model.
Rather, the ME ap-proach provides an overarching framework inwhich the full distribution of semantic roles givensyntactic features can be modeled.We train the ME models using the GIS algo-rithm (Darroch and Ratcliff, 1972) as implementedin the YASMET ME package (Och, 2002).
Weuse the YASMET MEtagger (Bender et al, 2003)to perform the Viterbi search.
The classifier wastrained until performance on the development setceased to improve.
Feature weights weresmoothed using Gaussian priors with mean 0(Chen and Rosenfeld, 1999).
The standard devia-tion of this distribution was optimized on the de-velopment set for each experiment.Classifier Performance on Test Set81.783.684.778.5767880828486G&J Exp 1 Exp 2 Exp 3%CorrectFigure 2.
Performance of models on test data usinghand annotated frame element boundaries.
G&J refersto the results of Gildea and Jurafsky (2002).
Exp 1 in-corporates feature sets 0-7 from Table 1; Exp 2 featuresets 0-9; Exp 3 features 0-11.ResultsFigure 2 shows the results of our experimentsalongside those of (Gildea and Jurafsky, 2002) onidentical held out test sets.
The difference in per-formance between each classifier is statisticallysignificant at (p<0.01) (Mitchell, 1997), with theexception of Exp 2 and Exp 3, whose difference isstatistically significant at (p<0.05).Table 2.
Effect of different smoothing parameter (std.dev.)
values on classification performance.Std.
Dev.
% Correct1 79.92 82.14 81.9Table 2 shows the effect of varying the stan-dard deviation of the Gaussian priors used forsmoothing in Experiment 1.
The difference in per-formance between the classifiers trained usingstandard deviation 1 and 2 is statistically signifi-cant at (p<0.01).10%20%30%40%50%60%70%80%90%10 100 1000 10000 100000# Sentences in Training%CorrectFigure 3.
Effect of training set size on semantic roleclassification.Figure 3 shows the change in performance as afunction of training set size.
Classifiers weretrained using the full set of features described forExperiment 3.Table 3 shows the confusion matrix for a subsetof semantic roles.
Five roles were chosen for pres-entation based upon their high contribution to clas-sifier error.
Confusion between these five accountfor 27% of all errors made amongst the 120 possi-ble roles.
The tenth role, other, represents the sumof the remaining 115 roles.
Table 4 presents ex-ample errors for five of the most confused roles.3.5 DiscussionIt is clear that the ME models improve perform-ance on frame element classification.
There are anumber of reasons for this improvement.First, for this task the log-linear model employedin the ME framework is better than the linearinterpolation model used by Gildea and Jurafsky.One possible reason for this is that semantic roleclassification benefits from the ME model?s biasfor more uniform probability distributions that sat-isfy the constraints placed on the model by thetraining data.Another reason for improved performance comesfrom ME?s simpler design.
Instead of having toworry about finding proper backoff strategiesamongst distributions of features subsets, ME al-lows one to include many features in a singlemodel and automatically adjusts the weights ofthese features appropriately.Table 3.
Confusion matrix for five roles which contrib-ute most to overall system error.
Columns refer to ac-tual role.
Rows refer to the model?s hypothesis.
Otherrefers to combination of all other roles.Area Spkr Goal Msg Path Other Prec.Area 98  6  18 16 0.710Spkr  373  23  41 0.853Goal 11  431  28 50 0.828Msg  18 1 315  33 0.858Path 32  36  415 41 0.791Other 15 21 26 24 33 5784 0.979Recall 0.628 0.905 0.862 0.87 0.84 0.969Also, because the ME models find weights formany thousands of features, they have many moredegrees of freedom than the linear interpolatedmodels of Gildea and Jurafsky.
Although manydegrees of freedom can lead to overfitting of thetraining data, the smoothing procedure employedin our experiments helps to counteract this prob-lem.
As evidenced in Table 2, by optimizing thestandard deviation used in smoothing the MEmodels are able to show significant increases inperformance on held out test data.Finally, by including in our model sentence-level pattern features and information about previ-ous classes, global information can be exploited forimproved classification.
The accuracy gained byincluding such global information confirms theintuition that the semantic role of an element ismuch related to the entire sentence of which it is apart.Having discussed the advantages of the modelspresented here, it is interesting to look at the errorsthat the system makes.
It is clear from the confu-sion matrix in Table 3 that a great deal of the sys-tem error comes from relatively few semanticroles.4  Table 4 offers some insight into why theseerrors occur.
For example, the confusions exem-plified in 1 and 2 are both due to the fact that theparticular phrases employed can be used in multi-ple roles (including the roles hypothesized by thesystem).
Thus, while ?across the counter?
may beconsidered a goal when one is talking about a per-son and their head, the same phrase would be con-sidered a path if one were talking about a mousewho is running.Table 4.
Example errors for five of the most often con-fused semantic rolesActual Proposed Example Sentence1 Goal Path The barman craned his headacross the counter.2 Area Path Mr. Glass began hallucinating,throwing books around theclassroom.3 Message Speaker Debate lasted until 20 Septem-ber, opposition being voicedby a number of Italian andSpanish prelates.4 Addressee Speaker Furious staff claim they wereeven called in from holiday tobe grilled by a specialist secu-rity firm5 Reason Evaluee We cannot but admire theefficiency with which shetook control of her own life.Examples 3 and 4, while showing phrases withsimilar confusions, stand out as being errors causedby an inability to deal with passive sentences.Such errors are not unexpected; for, even thoughthe voice of the sentence is an explicit feature, thesystem suffers from the paucity of passive sen-tences in the data (approximately 5%).Finally, example 5 shows an error that is basedon the difficult nature of the decision itself (i.e., itis unclear whether ?the efficiency?
is the reason foradmiration, or what is being admired).
Oftentimes, phrases are assigned semantic roles that arenot obvious even to human evaluators.
In suchcases it is difficult to determine what informationmight be useful for the system.Having looked at the types of errors that arecommon for the system, it becomes interesting toexamine what strategy may be best to overcomesuch errors.
Aside from new features, one solutionis obvious: more data.
The curve in Figure 2shows that there is still a great deal of performanceto be gained by training the current ME models onmore data.
The slope of the curve indicates thatwe are far from a plateau, and that even constantincreases in the amount of available training datamay push classifier performance above 90% accu-racy.Having demonstrated the effectiveness of theME approach on frame element classificationgiven hand annotated frame element boundaries,we next examine the value of the approach givenautomatically identified boundaries.44.1Frame Element IdentificationGildea and Jurafsky equate the task of locatingframe element boundaries to one of identifyingframe elements amongst the parse tree constituentsof a given sentence.
Because not all frame elementboundaries exactly match constituent boundaries,this approach can perform no better than 86.9%(i.e.
the number of elements that match constitu-ents (6864) divided by the total number of ele-ments (7899)) on the test set.FeaturesFrame element identification is a binary classifica-tion problem in which each constituent in a parsetree is described by a feature vector and, based onthat vector, tagged as either a frame element or not.In generating feature vectors we use a subset of thefeatures described for role tagging as well as anadditional path feature.Figure 4.
Generation of path features used in frameelement tagging.
The path from the constituent ?in in-spiration?
to the target predicate ?clapped?
is repre-sented as the string PP?VP?VBD.Gildea and Jurafsky introduce the path featurein order to capture the structural relationship be-tween a constituent and the target predicate.
The  4 44% of all error is due to confusion between only nine roles.Table 5.
Results of frame element identification.
G&J represents results reported in (Gildea and Jurafsky, 2002),ME results for the experiments reported here.
The second column shows precision, recall, and F-scores for the taskof frame element identification, the third column for the combined task of identification and classification.FE ID only FE ID + FE Classification MethodPrecision Recall F-Score Precision Recall F-ScoreG&J Boundary id + baseline role labeler .726 .631 .675 .67 .468 .551ME Boundary id + ME role labeler .736 .679 .706 .6 .554 .576path of a constituent is represented by the nodesthrough which one passes while traveling up thetree from the constituent and then down throughthe governing category to the target.
Figure 4shows an example of this feature for a frame ele-ment from the sentence presented in Figure 1.4.24.3ExperimentsWe use the ME formulation described in Section3.2 to build a binary classifier.
The classifier fea-tures follow closely those used in Gildea and Juraf-sky.
We model the data using the feature sets: f(fe,path), f(fe, path, tar), and f(fe, head, tar), where ferepresents the binary classification of the constitu-ent.
While this experiment only uses three featuresets, the heterogeneity of the path feature is sogreat that the classifier itself uses 1,119,331 uniquebinary features.With the constituents having been labeled, weapply the ME frame element classifier describedabove.
Results are presented using the classifier ofExperiment 1, described in section 3.3.
We theninvestigate the effect of varying the number ofconstituents used for training on identification per-formance.
Five data sets of approximately 100,00010,000, 1,000, and 100 constituents were generatedfrom the original set by random selection and usedto train ME models as described above.ResultsTable 5 compares the results of Gildea and Juraf-sky (2002) and the ME frame element identifier onboth the task of frame element identification alone,and the combined task of frame element identifica-tion and classification.
In order to be counted cor-rect on the combined task, the constituent musthave been correctly identified as a frame element,and then must have been correctly classified intoone of the 120 semantic categories.Recall is calculated based on the total numberof frame elements in the test set, not on the totalnumber of elements that have matching parse con-stituents.
Thus, the upper limit is 86.9%, not100%.
Precision is calculated as the number ofcorrect positive classifications divided by the num-ber of total positive classifications.The difference in the F-scores on the identifica-tion task alone and on the combined task are statis-tically significant at the (p<0.01) level 5 .
Theaccuracy of the ME semantic classifier on theautomatically identified frame elements is 81.5%,not a statistically significant difference from itsperformance on hand labeled elements, but a statis-tically significant difference from the classifier ofGildea and Jurafsky (2002) (p<0.01).00.10.20.30.40.50.60.70.8100 1000 10000 100000 1000000# Constituents in TrainingF-ScoreFigure 5.
Effect of training set size on frame elementboundary identification.Figure 5 shows the results of varying the train-ing set size on identification performance.
Foreach data set, thresholds were chosen to maximizeF-Score.4.4DiscussionIt is clear from the results above that the perform-ance of the ME model for frame element classifica-tion is robust to the use of automatically identifiedframe element boundaries.
Further, the ME5 G&J?s results for the combined task were generated with athreshold applied to the FE classifier (Dan Gildea, personalcommunication).
This is why their precision/recall scores aredissimilar to their accuracy scores, as reported in section 3.Because the ME classifier does not employ a threshold, com-parisons must be based on F-score.framework yields better results on the frame ele-ment identification task than the simple linear in-terpolation model of Gildea and Jurafsky.
Thisresult is not surprising given the discussion in Sec-tion 3.What is striking, however, is the drastic overallreduction in performance on the combinedidentification and classification task.
Thebottleneck here is the identification of frameelement boundaries.
Unlike with classificationthough, Figure 5 indicates that a plateau in thelearning curve has been reached, and thus, moredata will not yield as dramatic an improvement forthe given feature set and model.5 ConclusionThe results reported here show that ME modelsprovide higher performance on frame element clas-sification tasks, given both human and automati-cally identified frame element boundaries, than thelinear interpolation models examined in previouswork.
We attribute this increase to the benefits ofthe ME framework itself, the incorporation of sen-tence-level syntactic patterns into our feature set,and the use of previous tag information to find themost probable sequence of roles for a sentence.But perhaps most striking in our results are theeffects of varying training set size on the perform-ance of the classification and identification models.While for classification, the learning curve appearsto be still increasing with training set size, thelearning curve for identification appears to havealready begun to plateau.
This suggests that whileclassification will continue to improve as the Fra-meNet database gets larger, increased performanceon identification will rely on the development ofmore sophisticated models.In future work, we intend to apply the lessonslearned here to the problem of frame element iden-tification.
Gildea and Jurafsky have shown thatimprovements in identification can be had by moreclosely integrating the task with classification (theyreport an F-Score of .719 using an integratedmodel).
We are currently exploring a ME ap-proach which integrates these two tasks under atagging framework.
Initial results show that sig-nificant improvements can be had using techniquessimilar to those described above.AcknowledgmentsThe authors would like to thank Dan Gildea whogenerously allowed us access to his data files andOliver Bender for making the MEtagger softwareavailable.
Finally, we thank Franz Och whose helpand expertise was invaluable.ReferencesO.
Bender, K. Macherey, F. J. Och, and H. Ney.2003.
Comparison of Alignment Templates andMaximum Entropy Models for Natural Lan-guage Processing.
Proc.
of EACL-2003.
Buda-pest, Hungary.A.
Berger, S. Della Pietra and V. Della Pietra,1996.
A Maximum Entropy Approach to Natu-ral Language Processing.
Computational Lin-guistics, vol.
22, no.
1.S.
F. Chen and R. Rosenfeld.
1999.
A Gaussianprior for smoothing maximum entropy models.Technical Report CMUCS -99-108, CarnegieMellon UniversityM.
Collins.
1997.
Three generative, lexicalizedmodels for statistical parsing.
Proc.
of the 35thAnnual Meeting of the ACL.
pages 16-23, Ma-drid, Spain.J.
N. Darroch and D. Ratcliff.
1972.
Generalizediterative scaling for log-linear models.
Annalsof Mathematical Statistics, 43:1470-1480.C.
Fillmore 1976.
Frame semantics and the natureof language.
Annals of the New York Academyof Sciences: Conference on the Origin and De-velopment of Language and Speech, Volume280 (pp.
20-32).D.
Gildea and D. Jurafsky.
2002.
Automatic La-beling of Semantic Roles, Computational Lin-guistics, 28(3) 245-288 14.T.
Mitchell.
1997.
Machine Learning.
McGraw-Hill International Editions, New York, NY.Pages 143-145.F.J.
Och.
2002.
Yet another maxent toolkit:YASMET.
www-i6.informatik.rwth-aachen.de/Colleagues/och/.
