ROBUST TEXT PROCESSING IN AUTOMATED INFORMATION RETRIEVALTomek StrzalkowskiCourant Institute o f  Mathematical SciencesNew York University715 Broadway,  rm.
704New York, NY 10003tomek@cs.nyu.eduABSTRACTThis paper outlines a prototype text retrieval systemwhich uses relatively advanced natural language pro-cessing techniques in order to enhance the effective-ness of statistical document retrieval.
The backboneof our system is a traditional retrieval engine whichbuilds inverted index files from pre-processed docu-ments, and then searches and ranks the documents inresponse to user queries.
Natural anguage process-ing is used to (1) preprocess the documents in orderto extract contents-carrying terms, (2) discover inter-term dependencies and build a conceptual hierarchyspecific to the database domain, and (3) processuser's natural language requests into effective searchqueries.
The basic assumption of this design is thatterm-based representation f contents is in principlesufficient o build an effective if not optimal searchquery out of any user's request.
This has beenconfirmed by an experiment that compared effective-ness of expert-user prepared queries with thosederived automatically from an initial narrative infor-mation request.
In this paper we show that large-scale natural anguage processing (hundreds of mil-lions of words and more) is not only required for abetter etrieval, but it is also doable, given appropri-ate resources.
We report on selected preliminaryresults of experiments with 500 MByte database ofWall Street Journal articles, as well as some earlierresults with a smaller document collection.INTRODUCTIONA typical information retrieval OR) task is toselect documents from a d~!ahase in response to auser's query, and rank these documents according torelevance.
This has been usually accomplished usingstatistical methods (often coupled with manualencoding) that (a) select terms (words, phrases, andother units) from documents that are deemed to bestrepresent their contents, and (b) create an invertedindex file (or files) that provide and easy access todocuments containing these terms.
An importantissue here is that of finding an appropriatecombination of term weights which would reflecteach term's relative contribution to the informationcontents of the document.
Among many possibleweighting schemes the inverted document frequencyOdD has come to be recognized as universally appli-cable across variety of different text collections.Once the index is created, the search processwill attempt to match a preprocessed user query (orqueries) against representations of documents in eachcase determining a degree of relevance between thetwo which depends upon the number and types ofmatching terms.
Although many sophisticated searchand matching methods are available, the crucial prob-lem remains to be that of an adequate representationof contents for both the documents and the queries.The simplest word-based representations ofcontents are usually inadequate since single wordsare rarely specific enough for accurate discrimina-tion, and their grouping is often accidental.
A bettermethod is to identify groups of words that createmeaningful phrases, especially if these phrasesdenote important concepts in database domain.
Forexample, joint venture is an important term in WallStreet Journal (WSJ henceforth) database, while nei-ther joint nor venture are important by themselves.
Inthe retrieval experiments with the WSJ database, wenoticed that both joint and venture were droppedfrom the list of terms by the system because their idfweights were too low.
In large databases, uch asTIPSTEK/TREC, the use of phrasal terms is not justdesirable, it becomes necessary.The question thus becomes, how to identify thecorrect phrases in the text?
Both statistical and syn-tactic methods were used before with only limitedsuccess.
Statistical methods based on word co-occurrences and mutual information are prone to higherror rates, turning out many unwanted associations.Syntactic methods uffered from low quality of gen-erated parse structures that could be attributed to lim-ited coverage grammars and the lack of adequate l x-icons.
In fact.
the difficulties encountered in applyingcomputational linguistics technologies to text pro-cessing have contributed to a wide-spread belief that9automated natural language processing may not besuitable in IR.
These difficulties includedinefficiency, lack of robustness, and prohibitive costof manual effort required to build lexicons andknowledge bases for each new text domain.
On theother hand, while numerous experiments did notestablish the usefulness of linguistic methods in IR,they cannot be considered conclusive because of theirlimited scale.
\]The rapid progress in Computational Linguis-tics over the last few years has changed this equationin various ways.
First of all, large-scale resourcesbecame available: on-line lexicons, including OxfordAdvanced Learner's Dictionary (OALD), LongmanDictionary of Contemporary English (LDOCE),Webster's Dictionary, Oxford English Dictionary,Collins Dictionary, and others, as well as large textcorpora, many of which can now be obtained forresearch purposes.
Robust text-oriented softwaretools have been built, including part of speechtaggers (stochastic and otherwise), and fast parserscapable of processing text at speeds of 4200 wordsper minute or more (e.g., T IP  parser developed bythe author).
While many of the fast parsers are notvery accurate (they are usually partial analyzers bydesign), 2 some, like TIP,  perform in fact no worsethan standard full-analysis parsers which are manytimes slower and far less robust.
3An accurate syntactic analysis is an essentialprerequisite for term selection, but it is by no meanssufficient.
Syntactic parsing of the database contentsis usually attempted in order to extract linguisticallymotivated phrases, which presumably are better indi-cators of contents than "statistical phrases" wherewords are grouped solely on the basis of physicalproximity (e.g., "college junior" is not the same as"junior college').
However, creation of such com-pound terms makes term matching process morecomplex since in addition to the usual problems ofsynonymy and subsumption, one must deal with theirstructure (e.g., "college junior" is the same as "juniorin college").
In order to deal with structure, parser'st Standard IR benchmark collectiot~s are statistically toosmall and the experiments can easily produce cotm~rinmitiveresults.
For example, Cnmfield collection is only approx.
180,000English words, while CACM-3204 collection is approx.
200.000words.2 Partial parsing is usually fast enough, but it also generatesno isy  data: as numy as 50% of all generated phrases cotild be in-correct (Lewis and Croft, 1990).3 "I'rP has been shown to produce parse structures which sumno worse  m recall, precision and crossing rate than those generatedby flill-setle lmguisuc parsers when compared to hand-codedTreebank parse tree,.output needs to be "normalized" or "regularized" sothat complex terms with the same or closely relatedmeanings would indeed receive matching representa-tions.
This goal has been achieved to a certain extentin the present work.
As it will be discussed in moredetail below, indexing terms were selected fromamong head-modifier pairs extracted from predicate-argument representations of sentences.The next important task is to achieve normali-zation across diferent terms with close or relatedmeaning.
This can be accomplished by discoveringvarious semantic relationships among words andphrases, such as synonymy and subsumption.
Forexample, the term natural language can be con-sidered, in certain domains at least2 to subsume anyterm denoting a specific human language, such asEnglish.
Therefore, a query containing the formermay be expected to retrieve documents containingthe latter.
The system presented here computes termassociations from text on word and fixed phrase leveland then uses these associations in query expansion.A fairly primitive filter is employed to separatesynonymy and subsumption relationships from othersincluding antonymy and complementation, some ofwhich are strongly domain-dependent.
This processhas led to an increased retrieval precision in experi-ments with smaller and more cohesive collections(CACM-3204).In the following sections we present an over-view of our system, with the emphasis on its text-processing components.
We would like to point outhere that the system is completely automated, i.e., allthe processing steps, those performed by the statisti-cal core.
and these performed by the natural languageprocessing components, are done automatically, andno human intervention or manual encoding isrequired.OVERALL DESIGNOur information retrieval system consists of atraditional statistical backbone (NIST's PRISE sys-tem; Harman and Candela, 1989) augmented withvarious natural anguage processing components hatassist the system in database processing (stemming,indexing, word and phrase clustering, selectional res-trictions), and translate a user's information requestinto an effective query.
This design is a carefulcompromise between purely statistical non-linguisticapproaches and those requiring rather accomplished(and expensive) semantic analysis of data~ oftenreferred to as 'conceptual retrieval'.In our system the database text is first pro-cessed with a fast syntactic parser.
Subsequently cer-tain types of phrases are extracted from the parse3.0trees and used as compound indexing terms in addi-tion to single-word terms.
The extracted phrases arestatistically analyzed as syntactic ontexts in order todiscover a variety of similarity links between smallersubphrases and words occurring in them.
A furtherfiltering process maps these similarity links ontosemantic relations (generalization, specialization,synonymy, etc.)
after which they are used totransform user's request into a search query.The user's natural language request is alsoparsed, and all indexing terms occurring in them areidentified.
Certain highly ambiguous, usually single-word terms may be dropped, provided that they alsooccur as elements in some compound terms.
At thesame time, other terms may be added, namely thosewhich are linked to some query term through admis-sible similarity relations.
For example, "unlawfulactivity" is added to a query containing the com-pound term "illegal activity" via a synonymy linkbetween "illegal" and "unlawful".
After the finalquery is constructed, the database search follows, anda ranked list of documents i returned.The purpose of this elaborate linguistic pro-cessing is to create a better epresentation f docu-ments and to generate best possible queries out ofuser's initial requests.
Despite limitations of term-and-weight type representation (or boolean versionsthereof), very good queries can be produced byhuman experts.
In order to imitate an expert, the sys-tem must be able to learn about its database, in par-ticular about various correlations among index terms.FAST PARSING WITH TTP PARSER"I'I'P (Tagged Text Parser) is based on theLinguistic String Grammar developed by Sager(1981).
The parser currently encompasses some 400grammar productions, but it is by no means complete.The parser's output is a regularized parse treerepresentation f each sentence, that is, a representa-tion that reflects the sentence's logical predicate-argument structure.
For example, logical subject andlogical object are identified in both passive and activesentences, and noun phrases are organized aroundtheir head elements.
The significance of thisrepresentation will be discussed below.
The parser isequipped with a powerful skip-and-fit recoverymechanism that allows it to operate ffectively in thefaze of ill-formed input or under a severe time pres-sure.
In the runs with approximately 83 million wordsof TREC's Wall Street Journal texts~ the parser's4 Approximately 0.5 GBytes of text.
over 4 million sen-teilc?~.speed averaged between 0.3 and 0.5 seconds per sen-tence, or up to 4200 words per minute, on a Sun'sSparcStation-2.
'I'I'P is a full grammar parser, and initially, itattempts to generate a complete analysis for eachsentence.
However, unlike an ordinary parser, it has abuilt-in timer which regulates the amount of timeallowed for parsing any one sentence.
If a parse is notreturned before the allotted time elapses, the parserenters the skip-and-fit mode in which it will try to"fit" the parse.
While in the skip-and-fit mode.
theparser will attempt o forcibly reduce incompleteconstituents, possibly skipping portions of input inorder to restart processing at a next unattempted con-stituent.
In other words, the parser will favor reduc-tion to backtracking while in the skip-and-fit mode.The result of this strategy is an approximate parse,partially fitted using top-down predictions.
The frag-ments skipped in the first pass are not thrown out,instead they are analyzed by a simple phrasal parserthat looks for noun phrases and relative clauses andthen attaches the recovered material to the main parsestructure.
As an illustration, consider the followingsentence taken from the CACM-3204 corpus:The method is illustrated by the automatic con-struction of both reeursive and iterative pro-grams operating on natural numbers, lists, andtrees, in order to construct aprogram satisfyingcertain specifications a theorem induced bythose specifications is proved, and the destredprogram is extracted from the proof.The italicized fragment is likely to cause additionalcomplications in parsing this lengthy string, and theparser may be better off ignoring this fragment alto-gether.
To do so successfully, the parser must closethe currently open constituent (i.e., reduce a programsatisfying certain specifications to NP), and possiblya few of its parent constituents, removingcorresponding productions from further considera-tion, until an appropriate production is reactivated.In this case, T IP  may force the following reductions:SI -~ to V NP,  SA --~ SI; S -.~ NP V NP SA, until theproduction S ~ S and S is reached.
Next, the parserskips input to find and, and resumes normal process-ing.As may be expected, the skip-and-fit strategywill only be effective if the input skipping can be per-formed with a degree of determinism.
This meansthat most of the iexical level ambiguity must beremoved from the input text.
prior to parsing.
Weachieve this using a stochastic parts of speech taggerto preprocess the text.
Full details of the parser canbe found in (Strzalkowski, 1992).11PART OF SPEECH TAGGEROne way of dealing with lexical ambiguity is touse a tagger to preprocess the input marking eachword with a tag that indicates its syntactic ategoriza-tion: a part of speech with selected morphologicalfeatures such as number, tense, mode, case anddegree.
The following are tagged sentences from theCACM-32(M collection: 5The/dt paper/nn presents/vbz aldt proposal/nnfor~in structured/vbn representation/nn of/inmuhiprogramming/vbg in~in a/dt high/jj level/nnlanguage/nn ./perThe/dt notation/nn used/vbn explicitly/rbassociates/vbz a/dt data/nns structure/nnshared/vbn by~in concurrent/jj processes/nnswith~in operations/nns defined/vbn on~in it/pp./perThe tags are understood as follows: dt - determiner,nn - singular noun, nns - plural noun, in - preposition,jj - adjective, vbz - verb in present ense third personsingular, to - particle "to", vbg - present participle,vbn - past participle, vbd - past tense verb, vb -infinitive verb, cc - coordinate conjunction.Tagging of the input text substantially reducesthe search space of a top-down parser since itresolves most of the lexical level ambiguities.
In theexamples above, tagging of presents as "vbz" in thefirst sentence cuts off a potentially long and costly"garden path" with presents as a plural noun followedby a headless relative clause starting with (that) aproposal ....
In the second sentence, tagging resolvesambiguity of used (vbn vs. vbd), and associates (vbzvs.
nns).
Perhaps more importantly, elimination ofword-level lexical ambiguity allows the parser tomake projection about the input which is yet to beparsed, using a simple lookahead; in particular,phrase boundaries can be determined with a degreeof confidence (Church, 1988).
This latter property iscritical for implementing skip-and-fit recovery tech-nique outlined in the previous ection.Tagging of input also helps to reduce thenumber of parse structures that can be assigned to asentence, decreases the demand for consulting of thedictionary, and simplifies dealing with unknownwords.
Since every item in the sentence is assigned atag, so are the words for which we have no entry inthe lexicon.
Many of these words will be tagged as"rip" (proper noun), however, the surrounding tagsmay force other selections.
In the following exam-ple, chinese, which does not appear in the dictionary,s Tagged using the 35-tag Penn Treebank Tagset created atthe Univemty of Penn~Ivtnntis tagged as -jj,,:6this~dr paper/nn dates/vbz back/rb the~drgenesis/nn of~in binary/jj conception/nn circa~in5000/cd years/nns ago/rb ,~corn as/rbderived/vbn by~in the~dr chinese/jj ancients/nns./perWORD SUFFIX TRIMMERWord stemming has been an effective way ofimproving document recall since it reduces words totheir common morphological root, thus allowingmore successful matches.
On the other hand, stem-ming tends to decrease retrieval precision, if care isnot taken to prevent situations where otherwise unre-lated words are reduced to the same stem.
In our sys-tem we replaced a traditional morphological stemmerwith a conservative dictionary-assisted suffix trim-mer.
7 The suffix trimmer performs essentially twotasks: (1) it reduces inflected word forms to their rootforms as specified in the dictionary, and (2) it con-verts nominalized verb forms (e.g., "implementa-tion", "storage") to the root forms of correspondingverbs (i.e., "implement", "store").
This is accom-plished by removing a standard suffix, e.g.."stor+age", replacing it with a standard root endingC+e"), and checking the newly created word againstthe dictionary, i.e., we check whether the new root("store") is indeed a legal word, and whether the ori-ginal root ("storage") is defined using the new root("store") or one of its standard inflectional forms(e.g., "storing").
For example, the followingdefinitions are excerpted from the Oxford AdvancedLearner's Dictionary (OALD):storage n \[13\] (space used for, money paid for)the storing of goods ...diversion n \[U\] diverting ...procession n It \]  number of persons, vehicles,etc moving forward and following each other inan orderly way.Therefore, we can reduce "diversion" to "divert" byremoving the suffix "+sion" and adding root formsuffix "+t".
On the other hand, "process+ion" is notreduced to "process".Earlier experiments with CACM-3204 collec-tion showed an improvement in retrieval precision by6% to 8% over the base system equipped with a stan-dard morphological stemmer (the SMART stemmer).6 We use the machine ~_d_~ie version of  the Oxford Ad-vanced Learner's Dictionary (OALD).7 Dealing with prefixes is a more complicated matter, sincethey may have quite strong effect upon the meaning of the result-ing tenn.
e.g., un- usually introduces explicit negation.3.2HEAD-MODIFIER STRUCTURESSyntactic phrases extracted from T IP  parsetrees are head-modifier pairs.
The head in such a pairis a central element of a phrase (main verb, mainnoun, etc.
), while the modifier is one of the adjunctarguments of the head.
In the TREC experimentsreported here we extracted head-modifier word andfixed-phrase pairs only.
While TREC WSJ databaseis large enough to warrant generation of larger com-pounds, we were in no position to verify their effec-tiveness in indexing.
This was largely because of thetight schedule, but also because of rapidly escalatingcomplexity of the indexing process: even with 2-word phrases, compound terms accounted for nearly96% of all index entries, in other words, including 2-word phrases has increased the index size 25 times!Let us consider a specific example from WSJdatabase:The former Soviet president has been a localhero ever since a Russian tank invaded Wiscon-Silt.The tagged sentence is given below, followed by theregularized parse structure generated by 'FI'P, givenin Figure I.The~dr former/j/' Soviet/jj president/nn has/vbzbeen/vbn aldt local/jj hero/nn ever/rb since~ineddt Russian/jj tanklnn invaded/vbdWisconsin/rip ./perIt should be noted that the parser's output is apredicate-argument structure centered around mainelements of various phrases.
In Figure 1, BE is themain predicate (modified by HAVE) with 2 argu-ments (subject, object) and 2 adjuncts (adv, sub_oral).INVADE is the predicate in the subordinate clausewith 2 arguments (subject.
object).
The subject ofBE is a noun phrase with PRESIDENT as the headelement, two modifiers (FORMER, SOVIET) and adeterminer (THE).
From this structure, we extracthead-modifier pairs that become candidates for com-pound terms.
The following types of pairs are con-sidered: (1) a head noun and its left adjective or nounadjunct, (2) a head noun and the head of its rightadjunct, (3) the main verb of a clause and the head ofits object phrase, and (4) the head of the subjectphrase and the main verb.
These types of pairsaccount for most of the syntactic variants for relatingtwo words (or simple phrases) into pairs carryingcompatible semantic ontent.
For example, the pairretrieve+information will be extracted from any ofthe following fragments: information retrieval sys-tem; retrieval of information from databases;, andinformation that can be retrieved by a user-controlled interactive search process.
In the exampleat hand, the following head-modifier pairs areextracted (pairs containing low-contents elements,\]asserl\ [ \ [~  \[HAVE\]\]llverb \[BE\]\]\[subject\[npIn PRESIDENT\]\[tpos THE}\[adj \[FORMER\]\]ladj \[SOVIET\]Ill\[objectInpIn HERO\]It .pos A\]ladj \[LOCAL\]Illlady EVER}lsub_ord\[SINCEIlverb \[INVADEII\[subject\[npIn TANK\]\[t_pos A\]ladj \[RUSSIANII\]IIobjexl\]npIname \[WISCONSIN\]IIlIIIIIIFigure 1.
Predicale-argum~at parse structure.such as BE and FORMER, or names, such asWISCONSIN, will be later discarded):\[PRESIDENT,BE\]\[PRESIDENT,FORMER\]\[PRESIDENT,SOVIET\]\[BE,HEROI\[HERO,LOCAL\]\[TANK.INVADE\]flANK.RUSSIAN\]\[INVADE,WlSCONSlN\]We may note that the three-word phrase formerSoviet president has been broken into two pairsformer president and Soviet president, both of whichdenote things that are potentially quite different fromwhat the original phrase refers to, and this fact mayhave potentially negative ffect on retrieval preci-sion.
This is one place where a longer phrase appearsmore appropriate.
The representation f this sentencemay therefore contain the following terms:PRESIDENT.
SOVIET, PRESIDENT+SOVIET.PRESIDENT+FORMEIL HERO, HERO+LOCAL,INVADE.
TANK.
TANK+INVADE.
TANK+RUSSIAN.RUSSIAN.
INVADE+WISCONSIN.
WISCONSIN.The particular way of interpreting syntacticcontexts was dictated, to some degree at least, by sta-tistical considerations.
Our original experiments13were performed on a relatively small collection(CACM-3204), and therefore we combined pairsobtained from different syntactic relations (e.g.,verb-object, subject-verb, noun-adjunct, etc.)
in orderto increase frequencies of some associations.
Thisbecame largely unnecessary in a large collection suchas TIPSTER, but we had no means to test alternativeoptions, and thus decided to stay with the original.
Itshould not be difficult to see that this was acompromise solution, since many important distinc-tions were potentially lost, and strong associationscould be produced where there weren't any.
A way toimprove things is to consider different syntactic rela-tions independently, perhaps as independent sourcesof evidence that could lend support (or not) to certainterm similarity predictions.
We have already startedtesting this option.One difficulty in obtaining head-modifier pairsof highest accuracy is the notorious ambiguity ofnominal compounds.
For example, the phrase naturallanguage processing should generatelanguage+natural nd processing+language, whiledynamic information processing is expected to yieldprocessing+dynamic and processing+information.
Astill another case is executive vice president wherethe association president+executive may be stretch-ing things a bit too far.
Since our parser has noknowledge about the text domain, and uses nosemantic preferences, it does not attempt to guess anyinternal associations within such phrases.
Instead,this task is passed to the pair extractor module whichprocesses ambiguous parse smactures in two phases.In phase one, all and only unambiguous head-modifier pairs are extracted, and the frequencies oftheir occurrences are recorded.
In phase two, fre-quency information about pairs generated in the firstpass is used to form associations from ambiguousstructures.
For example, if language+natural hasoccurred unambiguously a number times in contextssuch as parser for natural language, whileprocessing+natural has occurred significantly fewertimes or perhaps none at all, then we will prefer theformer association as valid.TERM CORRELATIONS FROM TEXTHead-modifier pairs form compound termsused in database indexing.
They also serve asoccurrence contexts for smaller terms, includingsingle-word terms.
If two terms tend to be modifiedwith a number of common modifiers and otherwiseappear in few distinct contexts, we assign them asimilarity coefficient, a real number between 0 and 1.The similarity is determined by comparing distribu-tion characteristics for both terms within the corpus:how much information contents do they carry, dotheir information contribution over contexts vat3'greatly, are the common contexts in which theseterms occur specific enough?
In general we willcredit high-contents erms appearing in identical con-texts, especially if these contexts are not too com-monplace, s The relative similarity between twowords xi and x2 can be obtained using the followingformula (ct is a large constant): 9SIM (x l ,x2) = log (a ~ simy(x l ,x 2) )Ywheresimy(x l ,x2) = MIN (IC (x l ,\[x l,y \]),IC (x ~,\[x 2, y \]))* MIN(IC(y, \[xl .y\]),lC(y, \[x2,y\]))and IC is the Information Contribution measure indi-cating the strength of word pairings, and defined asIC (x, \[x,y \]) = - -  A,yn~+d~-Iwhere f~,y is the absolute frequency of pair Ix,y\] inthe corpus, nx is the frequency of term x at the headposition, and dx is a dispersion parameter understoodas the number of distinct syntactic ontexts in whichterm x is found.
The similarity function is furthernormalized with respect to SIM (x i ,x I ).
Examplesimilarities are listed in Table 1.We also considered a term clustering optionwhich, unlike the shnilatity formula above, producesclusters of related words and phrases, but will notgenerate uniform term similarity ranking across clus-ters.
We used a variant of weighted Tanimoto'smeasure described in (Grefenstette, 1992):SIM (x I .x2) =with~j~4tN (W (\[x,att \]),W (\[y,att \])all~_~MAX (W (\[x,att \]).
W (D',att \])anW (\[x, y \]) = GW (x )* log (A.,)GW (x) = 1 - n~log (N)s It would not be appropriate to predict similarity betweenlanguage mad logar/thm on the basis of their co--occur~nee wlthnatural .tTh/s was inspired by ?
formula used by Hind\]e (1990).14Sample clusters obtained from approx.
100 MByte(17 million words) sample of WSJ are given in Table2.In order to generate better similarities and clus-ters, we require that words x\] and x2 appear in atleast M distinct common contexts, where a commoncontext is a couple of pairs \[x\],y\] and \[x2,y\], or\[y,x l \] and \[y,x 2\] such that they each occurred at leasttwice.
Thus, banana and Baltic will not be con-sidered for similarity relation on the basis of theiroccurrences in the common context of republic, nomatter how frequent, unless there is another suchcommon context comparably frequent (there wasn'tany in TREC WSJ database).
For smaller or narrowdomain databases M=2 is usually sufficient.
For largedatabases covering rather diverse subject matter, likeTIPSTER or even WSJ, we used M_>3) ?It may be worth pointing out that the similari-ties are calculated using term co-occurrences in syn-tactic rather than in document-size contexts, the latterbeing the usual practice in non-linguistic lustering(e.g., Sparck Jones and Barber, 1971; Crouch, 1988;Lewis and Croft, 1990).
Although the two methods ofterm clustering may be considered mutually comple-mentary in certain situations, we believe that moreand stronger associations can be obtained throughsyntactic-context clustering, given sufficient amountof data and a reasonably accurate syntactic parser) ~QUERY EXPANSIONSimilarity relations are used to expand userqueries with new terms, in an attempt o make thefinal search query more comprehensive (addingsynonyms) and/or more pointed (adding specializa-tions)) 2 It follows that not all similarity relations willbe equally useful in query expansion, for instance,complementary and antonymous relations like the1o For example banana and Dominican were found to havetwo common contexts: republic and plant, although this second oc-in appare, nfly different senses in Dominican plant and bana.na p/ant."
Nun-syntactic contexts cross sentence boundaries with nofuss.
which is helpful with short, succinct documents (such asCACM absuacts), but less so with longer texts; see also (Gnsimaanet al, 1986).
:2 Query expansion (in the sense considered here, though notquite in the same way) has been used in information retrievalrescacch before (e.g., Sparc~ Jones and Tait.
1984; Hamum,  1988).usually with nuxcd ~csults.
An ahemanve is to use term clusters tocreate new teans, "meta~nns".
and use them to index the databaseinstead {e.g.. Crouch.
1988; lewis and Croft, 1990).
We found thatthe query expansion approach gives the system more flexibility, forinstance, by making room for hypenext-style topic exploration viauser feedback.one between Australian and Canadian, or accept andreject may actually harm system's performance,since we may end up retrieving many irrelevantdocuments.
Similarly, the effectiveness of a querycontaining vitamin is likely to diminish if we add asimilar but far more general term such as acid.
Onthe other hand, database search is likely to missrelevant documents if we overlook the fact that for-tran is a programming language, or that infant is ababy and baby is a child.
We noted that an averageset of similarities generated from a text corpus con-tains about as many "good" relations (synonymy,specialization) as "bad" relations (antonymy.
comple-mentation, generalization), as seen from the queryexpansion viewpoint.
Therefore any attempt toseparate these two classes and to increase the propor-tion of "good" relations should result in improvedretrieval.
This has indeed been confirmed in ourexperiments where a relatively crude filter has visiblyincreased retrieval precision.In order to create an appropriate filter, we dev-ised a global term specificity measure (GTS) which iscalculated for each term across all contexts in whichit occurs.
The general philosophy here is that a morespecific word/phrase would have a more limited use,i.e., a more specific term would appear in fewer dis-tinct contexts.
In this respect, GTS is similar to thestandard inverted ocument frequency (idjO measureexcept hat term frequency is measured over syntacticunits rather than document size units.
13 Terms withhigher GTS values are generally considered morespecific, but the specificity comparison is only mean-ingful for terms which are already known to be simi-lar.
The new function is calculated according to thefollowing formula:ICL(w) if both exist ICR(w)GTS(w)=I~R(w) otherwiseif?nlylCR(w)existswhere (with nw, d~ > 0):~wICt(w) = IC (\[w,_ \]) = a~(nw+d~-l)nw tCR(w) = tc (\[_,w\]) =d,,(nw+a~- I)For any two terms w~ and w 2, and a constant ~ > I.if GTS(w 2) > 8 * GTS(w\])  then w 2 is consideredmore specific than w\].
In addition, if" We believe that measuring term specificity overdocument-size contexts (e.g.. Sparck Jones.
1972) may not be ap-propnate in this case.
In pameular, s3mtax-based contexts allow forpr~:essmg texts without any internal docmnent structure.2.5SlM,,o,~(w i,w2) = o > O,where 0 is an empirically established threshold, thenw2 can be added to the query containing term w 1with weight o.
14 For example, the following wereobtained from TREC WSJ training database:GTS (child) = 0.000001GTS (baby) = 0.000013GTS (infant) = 0.000055withSIM(child,infant) =0.131381SIM (baby,child) = 0.183064SIM (baby,infant) = 0.323121Therefore both baby and infant can be used to spe-cialize child.
With this filter, the relationship betweenbaby and infant had to be discarded, as we are unableto tell synonymous or near synonymous relationshipsfrom those which are primarily complementary, e.g.,man and woman.SUMMARY OF RESULTSWe have processed the total of 500 MBytes ofarticles from Wall Street Journal section of TRECdatabase.
Retrieval experiments involved 50 userinformation requests (topics) (TREC topics 51-100)consisting of several fields that included both text anduser supplied keywords.
A typical topic is shownbelow:<:top><head> Tipster Topic Description<hum> Number:.
059<dora> Domain: Environment<title> Topic: Weather Related Fatalities<desc> Description:Document will report a type of weather event which hasdirectly caused at least one fatality in some location..~narr> Narrative:A relevant document will include the number of peoplekilled and injured by the weather veat, as well asreporting the type of we .~er  event and the locationof the event.<con> Cmc~(s):For CAC'M-3204 colle~ion the filter was most effective ato = 0..5"7.
For TREC- I  we changed the similarity formula slightlyin order to obtain ~ nonnahza~vns m all cases.
This howeverlowered smailanty coefficients in general and a new threshold hadto be selected.
We used o = 0.1 m TREC-I rims, although it tamedom tobcapoor  choice.
In all ?au~Svaried between 10and I00.I.
lightning, avalanche, tornado, typhoon, humcane.heat.
heat wave.
flood, snow.
rain.
downpour.blizzard, storm, freezing temperatures2.
dead.
killed, fatal, death, fatality, victim3.
NOT man-made disasters, NOT war-induced famine4.
NOT earthquakes, NOT volcanic ernptions</top>Note that this topic actually consists of two differentstatements of the same query: the natural anguagespecification consisting of <desc> and <nan-> fields.and an expert-selected list of key terms which areoften far more informative than the narrative part.Results obtained for queries using text fields only andthose involving both text and keyword fields arereported separately.
Further experiments have sug-gested that natural language processing impact issignificant but may be severely limited by the expres-siveness of the term-based representation.
Since the<con> field is considered the expert-user's renderingof the 'optimal" search query, our system is able todiscover much of it from a less completespecification in the text section of the request viaquery expansion.
In fact, we noted that therecall/precision gap between automatically generatedqueries and those supplied by the user was largelyclosed when NLP was used.
Moreover, even with thekeyword field included in the query along with otherfields, NLP's impact on the system's performance isstill noticeable.Other results on the impact of different fields inTREC topics on the final recall/precision results werereported by Broglio and Croft (1993) at the ARPAHLT workshop, although text-only runs were notincluded.
One of the most striking observations theyhave made is that the narrative field is entirelydisposable, and moreover that its inclusion in thequery actually hurts the system's performance.
It hasto be pointed out, however, that they do littlelanguage processing.
15Summary statistics for these runs are shown inTable 4.
These results are fairly tentative and shouldbe regarded with some caution.
For one, the columnnamed txt reports performance of <dcsc> and <narr>fields which have been processed with our suffix-~rimmer.
This means some N IP  has been donealready (tagging + lexicon), and therefore what wesee there is not the performance of 'pure' statisticalsystem.
The same applies to con column.
(Foru Brace Cmfl (personal communication.
1992) has suggest-ed that excluding Ill expert-made fields (i.e.. <ctm> and <:lac>)would make the queries quite ineffective.
Broglio (personal com-mumeanvc, 1993) co.rims Ibis showing thaz text-only retrieval(i.e.. with <desc> and ~narr'>) shows an average prnc:sion at mornthan 30% below that of <con>-based retrieval.16word 1 word2 SIMnormabmabsenceacceptaccordacquirespeechadjustablemaxsaveraffairaffordablediseasemedium+rangeaircraftaircraftairlinealienanniversaryanti+ageanti+clotcontracandidatecontendpropertyattemptawaitstealthchildbaggagebanbearishbeeroller+coasttwo+incometelevisionsoldiertreasuryresearchwithdrawal*anti+ballistic*maternityacquirepactpurchaseaddressone+year*advance+purchasescandallow+income*ailment*air+to+air*jetlinerplanecarrierimmigrate*bicentennialanti+wrinklecholesterol+lower*anti + sandinista*aspirant*aspirantassetbidpend*b+l*babyluggagerestrictbullish*honeybee*bumpytwo+earnerIvtroop*short+termstudy*pullout0.5348940.2330820.1790780.4923320.4493620.2637890.8240530.7340080.6848770.1817950.2473820.8745080.1667770.4238310.3454900.2704120.5882100.1539180.8567120.2946770.1160250.1434590.2852990.6415920.5729600.8775820.1830640.6073330.3219430.8471030.4610230.8982780.2931040.8O6O180.3744100.6611330.2092570.622558Table 1.
Selecte filtered word similarities (* indicatesthe more specific term).word clustertakeover merge, buy-outacquisition, bidstock share, issue, bond, pricestaff personnel, employee, forceshare stock, issue,fundsensitive crucial, difficult, criticalrumor speculatepresident director, executivechairman, manageoutlook forecast, prospecttrend, picturelaw rule, legislatebill, regulateearnings revenue, incomepor(olio asset, invest, loanproperty, holdinflate growth, earnings, riseindustry business, company, markethelp additional, support, involvegrowth increase, rise, gaindecline, earnings, profitfirm bank, concern, group, unitenviron climate, conditionsituation, trenddebt loan, secure, bondcustom( er ) client, investorbuyer, consume(r)counsel attorneycompute machine, softwarecompetitor rival, partner, buyercompany business, firm, bankmarket, industry, concernbig large, major, hugebase facile, sourcereserve, supportasset property, loan,fund, investshare, stock, moneyTable 2.
Selected clusters obtained from approx.
107words of text with weighted Tanimoto formula.17comparison, see Table 3 where runs with CACM-3204 collection included 'pure' statistics run (base),and note the impact our suffix trimmer is having.
)Nonetheless, one may notice that automated NLP canbe very effective at discovering the right query froman imprecise narrative specification: as much as 82%of the effectiveness ofthe expert-generated query canbe attained.CONCLUSIONSWe presented insome detail a natural languageinformation retrieval system consisting of anadvanced NLP module and a 'pure' statistical coreengine.
While many problems remain to be resolved,including the question of adequacy of term-basedrepresentation f document contents, we attempted todemonstrate that the architecture described here isnonetheless viable.
In particular, we demonstratedthat natural language processing can now be done ona fairly large scale and that its speed and robustnesscan match those of traditional statistical programssuch as key-word indexing or statistical phraseextraction.
We suggest, with some caution until moreexperiments are run, that natural language processingcan be very effective in creating appropriate searchqueries out of user's initial specifications which canbe frequently imprecise or vague.On the other hand, we must be aware of thelimits of NLP technologies at our disposal.
Whilepart-of-speech tagging, lexicon-based stemming, andparsing can be done on large amounts of text (hun-dreds of millions of words and more), other, moreadvanced processing involving conceptual structur-ing, logical forms, etc., is still beyond reach, compu-rationally.
It may be assumed that these super-advanced techniques will prove even more effective,since they address the problem of representation-level limits, however the experimental evidence issparse and necessarily limited to rather small scaletests (e.g., Mauldin, 1991).ACKNOWLEDGEMENTSWe would like to thank Donna Harman ofNIST for making her PRISE system available to us.We would also like to thank Ralph Weischedel andHeidi Fox of BBN for providing and assisting in theuse of the part of speech tagger.
Jose Perez Carballohas contributed a number of valuable observationsduring the course of this work, and his assistance inprocessing the TREC data was critical.
This paper isbased upon work supported by the DefenseAdvanced Research Project Agency under ContractN00014-90-J-1851 from the Office of NavalResearch.
under Contract N00600-88-D-3717 fromPRC Inc., and the National Science Foundation underGrant IRI-89-02304.
We also acknowledge supportfrom Canadian Institute for Robotics and IntelligentSystems (IRIS).REFERENCESBroglio, John and W. Bruce Croft.
1993.
"'QueryProcessing for Retrieval from Large Text Bases.
"Proceedings of ARPA HLT Workshop.
March21-24, Plainsboro, NJ.Church, Kenneth Ward and Hanks, Patrick.
1990.
"Word association orms, mutual information,and lexicography.'"
Computational Linguistics,16(1), MIT Press.
pp.
22-29.Crouch, Carolyn J.
1988.
"A cluster-based approachto thesaurus construction."
Proceedings of ACMSIGIR-88, pp.
309-320.Grefenstette, Gregory.
1992.
"Use of Syntactic Con-text To Produce Term Association Lists for TextRetrieval."
Proceedings of SIGIR-92.Copenhagen, Denmark.
pp.
89-97.Grishman, Ralph.
Lynette Hirschman, and Ngo T.Nhan.
1986.
"Discovery procedures for sub-language selectional patterns: initial experi-ments".
Computational Linguistics.
12(3), pp.205-215.Grishman, Ralph and Tomek Strzalkowski.
1991.
"Information Retrieval and Natural LanguageProcessing."
Position paper at the workshop onFuture Directions in Natural Language Processingin Information Retrieval, Chicago.Harman, Donna.
1988.
"Towards interactive queryexpansion."
Proceedings of ACM SIGIR-88, pp.321-331.I-larman, Donna and Gerald Candela.
1989.
"'Retrieving Records from a Gigabyte of text on aMinicomputer Using Statistical Ranldng.'"
Jour-nal of the American Society for Information Sci-ence, 41(8), pp.
581-589.I-lindle, Donald.
1990.
"'Noun classification frompredicate-argument structures."
Proc.
28 Meet-ing of the ACL.
Pittsburgh.
PA, pp.
268-275.Lewis, David D. and W. Bruce Croft.
1990.
"'TermClustering of Syntactic Phrases".
Proceedings ofACM SIGIR-90, pp.
385-405.Mauldin.
Michael.
1991.
"Retrieval Performance inFerret: A Conceptual Information Retrieval Sys-tem."
Proceedings of ACM SIGIR-91, pp.
347-355.Meteer, Marie, Richard Schwartz, and RalphWeischedel.
1991.
"Studies in Part of SpeechLabeling."
Proceedings of the 4th DARPASpeech and Natural Language Workshop.1BMorgan-Kaufman, San Mateo, CA.
pp.
331-336.Sager, Naomi.
1981.
Natural Language InformationProcessing.
Addison-Wesley.Sparck Jones, Karen.
1972.
"'Statistical interpreta-tion of term specificity and its application inretrieval."
Journal of Documentation, 28(1), pp.11-20.Sparck Jones, K. and E. O. Barber.
1971.
"Whatmakes automatic keyword classification effec-tive?"
Journal of the American Society for Infor-mation Science, May-June, pp.
166-175.Sparck Jones, K. and J. I. Tait.
1984.
"Automaticsearch term variant generation."
Journal ofDocumentation, 40(1), pp.
50-66.Strzalkowski, Tomek and Barbara Vauthey.
1991.
"Fast Text Processing for InformationRetrieval."
Proceedings of the 4th DARPASpeech and Natural Language Workshop,Morgan-Kaufman, pp.
346-351.Strzalkowski, Tomek and Barbara Vauthey.
1992.
"Information Retrieval Using Robust NaturalLanguage Processing."
Proc.
of the 30th ACLMeeting, Newark, DE, June-July.
pp.
104-111.Strzalkowski, Tomek.
1992.
"TrP: A Fast andRobust Parser for Natural Language."
Proceed-ings of the 14th International Conference onComputational Linguistics (COLING), Nantes,France, July 1992. pp.
198-204.Runs base surf.trim query exp.R ecall Precision Averages0.000.100.200.300.400.500.600.700.800.901.003-pt Avg.%chg0.7640.6740.5470.4490.3870.3290.2730.1980.1460.0930.0790.3280.775 0.7930.688 0.7000.547 0.5730.479 0A860.421 0A210.356 0.3720.280 0.3040.222 0.2260.170 0.174O.112 0.I 140.087 0.0900.356 0.371+8.3 +13.1Table 3.
Run statistics for CACM-3204 da-tabase: with no NLP; with suffix trimmer,and with both phrases and similarities.Ran txt txt+nlp l conQueries 50 50 50con+nip50Tot.
number of docs over all queriesRetRelRelRet%chg9980 \] 99806228 62281598 1835+14.8978862281927+20.6Recall Prec~ion Averages0.000.100.200.300.400.500.600.700.800.901.000.64200.37270.24760.15430.10930.06110.02980.01600.00460.00000.00000.69170.41940.29590.21500.15130.09590.03960.01750.00470.00270.00000.70210.44760.33530.22020.14430.08510.04030.01870.00480.00000.0(K~997562282062+29.00.75390.48480.36410.26740.17350.
I0010.06650.01030.00240.00100.0010Average Precisionsll-pt 0.1489 0.1758 0.1817 0.2023%chg +18.0 +22.0 +35.83-pt 0.1044 0.1322 I 0.1417 0.1555%chg +26.6 \[+35.7 +48.9at 5 0.4360 0.5000 0.4680 0.4800%chg +14.6 +7.3 +10.0at 15 0.3453 0.3827 0.3880 0.4107%chg +10.8 ~ +12.3 +18.9iat 100 0.2108 0.2384 0.2498 0.2712%chg +13.0 +18.5 +28.6Table 4.
Ran statistics with TIPSTER WSJ databasewith top 200 documents considered per each query:(1) txt - with <narr> and <desc> fields only: (2)txt+nlp - with <hart> and <desc> only including syn-tactic phrase terms and similarities: (3) con - with<desc> and <con> fields only; and (4) con+nip - with<desc> and <con> fields including phrases and simi-larities.
In all cases documents preprocessed withlexicon-based suffix-trimmer.19
