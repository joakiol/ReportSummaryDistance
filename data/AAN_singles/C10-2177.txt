Coling 2010: Poster Volume, pages 1550?1557,Beijing, August 2010Imposing Hierarchical Browsing Structures onto Spoken DocumentsXiaodan Zhu & Colin CherryInstitute for Information TechnologyNational Research Council Canada{Xiaodan.Zhu,Colin.Cherry}@nrc-cnrc.gc.caGerald PennDepartment of Computer ScienceUniversity of Torontogpenn@cs.toronto.eduAbstractThis paper studies the problem of im-posing a known hierarchical structureonto an unstructured spoken document,aiming to help browse such archives.We formulate our solutions within adynamic-programming-based alignmentframework and use minimum error-rate training to combine a number ofglobal and hierarchical constraints.
Thispragmatic approach is computationallyefficient.
Results show that it outperformsa baseline that ignores the hierarchicaland global features and the improvementis consistent on transcripts with differentWERs.
Directly imposing such hierar-chical structures onto raw speech withoutusing transcripts yields competitiveresults.1 IntroductionThough speech has long served as a basic methodof human communication, revisiting and brows-ing speech content had never been a possibilitybefore human can record their own voice.
Re-cent technological advances in recording, com-pressing, and distributing such archives have ledto the consistently increasing availability of spo-ken content.Along with this availability comes a demand forbetter ways to browse such archives, which is in-herently more difficult than browsing text.
In re-lying on human beings?
ability to browse text, asolution is therefore to reduce the speech brows-ing problem to a text browsing task through tech-nologies that can automatically convert speech totext, i.e., the automatic speech recognition (ASR).Research along this line has implicitly changedthe traditional speaking-for-hearing and writing-for-reading construals: now speech can be readthrough its transcripts, though it was not originallyintended for this purpose, which in turn raises anew set of problems.The efficiency and convenience of reading spo-ken documents are affected by at least two facts.First, the quality of transcripts can impair brows-ing efficiency, e.g., as shown in (Stark et al, 2000;Munteanu et al, 2006), though if the goal is onlyto browse salient excerpts, recognition errors onthe extracts can be reduced by considering theconfidence scores assigned by ASR (Zechner andWaibel, 2000; Hori and Furui, 2003).Even if transcription quality is not a problem,browsing transcripts is not straightforward.
Whenintended to be read, written documents are al-most always presented as more than uninterruptedstrings of text.
Consider that for many writ-ten documents, e.g., books, indicative structuressuch as section/subsection headings and tables-of-contents are standard constituents created manu-ally to help readers.
Structures of this kind, how-ever, are rarely aligned with spoken documents.In this paper, we are interested in addressingthe second issue: adding hierarchical browsablestructures to speech transcripts.
We define a hi-erarchical browsable structure as a set of nestedlabelled bracketing which, when placed in text,partition the document into labeled segments.
Ex-amples include the sequence of numbered sec-tion headings in this paper, or the hierarchicalslide/bullet structure in the slides of a presenta-tion.1550An ideal solution to this task would directly in-fer both the hierarchical structure and the labelsfrom unstructured spoken documents.
However,this is a very complex task, involving the analysisof not only local but also high-level discourse overlarge spans of transcribed speech.
Specifically forspoken documents, spoken-language characteris-tics as well as the lack of formality and thematicboundaries in transcripts violate many conditionsthat a reliable algorithm (Marcu, 2000) relies onand therefore make the task even harder.In this paper, we aim at a less ambitious butnaturally occurring problem: imposing a knownhierarchical structure, e.g., presentation slides,onto the corresponding document, e.g., presenta-tion transcripts.
Given an ordered, nested set oftopic labels, we must place the labels so as tocorrectly segment the document into appropriateunits.
Such an alignment would provide a usefultool for presentation browsing, where a user couldeasily navigate through a presentation by clickingon bullets in the presentation slides.
The solutionto this task should also provide insights and tech-niques that will be useful in the harder structure-inference task, where hierarchies and labels arenot given.We present a dynamic-programming-basedalignment framework that considers global docu-ment features and local hierarchical features.
Thispragmatic approach is computationally efficientand outperforms a baseline alignment that ignoresthe hierarchical structure of bullets within slides.We also explore the impact of speech recognitionerrors on this task.
Furthermore, we study thefeasibility of directly aligning a structure to rawspeech, as opposed to a transcript.2 Related workTopic/slide boundary detection The previouswork most directly related to ours is research thatattempts to find flat structures of spoken docu-ments, such as topic and slide boundaries.
Forexample, the work of (Chen and Heng, 2003;Ruddarraju, 2006; Zhu et al, 2008) aims to findslide boundaries in the corresponding lecture tran-scripts.
Malioutov et al (2007) developed an ap-proach to detecting topic boundaries of lecturerecordings by finding repeated acoustic patterns.None of this work, however, has involved hierar-chical structures that exist at different levels of adocument.In addition, researchers have also analyzedother multimedia channels, e.g., video (Liu et al,2002; Wang et al, 2003; Fan et al, 2006), to de-tect slide transitions.
Such approaches, however,are unlikely to find semantic structures that aremore detailed than slide transitions, e.g., the bullethierarchical structures that we are interested in.Building tables-of-contents on written text Anotable effort going further than topic segmenta-tion is the work by Branavan et al (2007), whichaims at the ultimate goal of building tables-of-contents for written texts.
However, the authorsassumed the availability of the hierarchical struc-tures and the corresponding text spans.
Therefore,their problem was restricted to generating titles foreach span.
Our work here can be thought of as theinverse problem, in which the title of each sectionis known, but the corresponding segments in thespoken documents are unknown.
Once the corre-spondence is found, an existing hierarchical struc-ture along with its indicative titles is automaticallyimposed on the speech recordings.
Moreover, thispaper studies spoken documents instead of writ-ten text.
We believe it is more attractive not onlybecause of the necessity of browsing spoken con-tent in a more efficient way but also the generalabsence of helpful browsing structures that are of-ten available in written text, as we have alreadydiscussed above.Rhetoric analysis In general, analyzing dis-course structures can provide thematic skeletons(often represented as trees) of a document as wellas relationship between the nodes in the trees.
Ex-amples include the widely known discourse pars-ing work by Marcu (2000).
However, when thetask involves the understanding of high-level dis-course, it becomes more challenging than justfinding local discourse conveyed on small spans oftext; e.g., the latter is more likely to benefit fromthe presence of discourse markers.
Specificallyfor spoken documents, spoken-language charac-teristics as well as the absence of formality andthematic boundaries in transcripts pose additional1551difficulty.
For example, the boundaries of sen-tences, paragraphs, and larger text blocks like sec-tions are often missing.
Together with speechrecognition errors as well as other speech charac-teristics such as speech disfluences, they will im-pair the conditions on which an effective and reli-able algorithm of discourse analysis is often built.3 Problem formulationWe are given a speech sequence U =u1, u2, ..., um, where ui is an utterance.
De-pending on the application, ui can either standfor the audio or transcript of the utterance.
Weare also given a corresponding hierarchical struc-ture.
In our work, this is a sequence of lectureslides containing a set of slide titles and bullets,B = {b1, b2, ..., bn}, organized in a tree structureT (?,?,?
), where ?
is the root of the tree thatconcatenates all slides of a lecture; i.e., each slideis a child of the root ?
and each slide?s bulletsform a subtree.
In the rest of this paper, the wordbullet means both the title of a slide (if any) andany bullet in it.
?
is the set of nodes of the tree(both terminal and non-terminals, excluding theroot ?
), each corresponding to a bullet bi in theslides.
?
is the edge set.
With the definitions, ourtask is herein to find the triple (bi, uk, ul), denot-ing that a bullet bi starts from the kth utteranceuk and ends at the lth.
Constrained by the treestructure, the text span corresponding to an an-cestor bullet contains those corresponding to itsdescendants; i.e., if a bullet bi is the ancestor ofanother bullet bj in the tree, the acquired bound-ary triples (bi, uk1, ul1) and (bj , uk2, ul2) shouldsatisfy uk1 ?
uk2 and ul1 ?
ul2.
In implemen-tation, we only need to find the starting point of abullet, i.e., a pair (bi, uk), since we know the treestructure in advance and therefore we know thatthe starting position of the next sibling bullet isthe ending boundary for the current bullet.4 Our approachesOur task is to find the correspondence betweenslide bullets and a speech sequence or its tran-scripts.
Research on finding correspondences be-tween parallel texts pervades natural languageprocessing.
For example, aligning bilingual sen-tence pairs is an essential step in training ma-chine translation models.
In text summarization,the correspondence between human-written sum-maries and their original texts has been identified(Jing, 2002), too.
In speech recognition, forcedalignment is applied to align speech and tran-scripts.
In this paper, we keep the general frame-work of alignment in solving our problem.Our solution, however, should be flexible toconsider multiple constraints such as those con-veyed in hierarchical bullet structures and globalword distribution.
Accordingly, the model pro-posed in this paper depends on two orthogonalstrategies to ensure efficiency and richness of themodel.
First of all, we formulate all our solutionswithin a classic dynamic programming frameworkto enforce computational efficiency (section 4.1).On the other hand, we explore the approach to in-corporating hierarchical and global features intothe alignment framework (Section 4.2).
The as-sociated parameters are then optimized with Pow-ell?s algorithm (Section 4.3).4.1 A pre-order walk of bullet treesWe formulate our solutions within the classicdynamic-programming-based alignment frame-work, dynamic time warping (DTW).
To this end,we need to sequentialize the given hierarchies,i.e., bullet trees.
We propose to do so through apre-order walk of a bullet tree; i.e., at any stepof a recursive traversal of the tree, the alignmentmodel always visits the root first, followed by itschildren in a left-to-right order.
This sequential-ization actually corresponds to a reasonable as-sumption: words appearing earlier on a given slideare spoken earlier by the speaker.
The pre-orderwalk is also used by (Branavan et al, 2007) toreduce the search space of their discriminativetable-of-contents generation.
Our sequentializa-tion strategy can be intuitively thought of as re-moving indentations that lead each bullet.
Asshown in Figure 1, the right panel is a bullet arrayresulting from a pre-walk of the slide in the leftpanel.
In our baseline model, the resulted bulletarray is directly aligned with lecture utterances.Other orders of bullet traversal could also beconsidered, e.g., when speech does not strictly fol-low bullet orders.
In general, one can regard our1552task here as a tagging problem to allow furtherflexibility on bullet-utterance correspondence, inwhich bullets are thought of as tags.
However,considering the fact that bullets are created to or-ganize speech and in most cases they correspondto the development of speech content monotoni-cally, this paper focuses on addressing the prob-lem in the alignment framework.Figure 1: A pre-order walk of a bullet tree.4.2 Incorporating hierarchical and globalfeaturesOur models should be flexible enough to considerconstraints that could be helpful, e.g., the hierar-chical bullet structures and global word distribu-tion.
We propose to consider all these constraintsin the phase of estimating similarity matrices.
Tothis end, we use two levels of similarity matricesto capture local tree constraints and global worddistributions, respectively.First of all, information conveyed in the hierar-chies of bullet trees should be considered, such asthe potentially discriminative nature between twosibling bullets (Branavan et al, 2007) and the re-lationships between ancestor and descendant bul-lets.
We incorporate them in the bullet-utterancesimilarity matrices.
Specifically, when estimatingthe similarity between a bullet bi and an utteranceuj , we consider local tree constraints based onwhere the node bi is located on the slide.
We doso by accounting for first and second-order treefeatures.
Given a bullet, bi, we first represent itas multiple vectors, one for each of the following:its own words, the words appearing in its parentbullet, grandparent, children, grandchildren, andthe bullets immediately adjacent to bi.
That is, biis now represented as 6 vectors of words (we donot discriminate between its left and right siblingsand put these words in the same vector).
Simi-larity between the bullet bi and an utterance uj iscalculated by taking a weighted average over thesimilarities between each of the 6 vectors and theutterance uj .
A linear combination is used and theweights are optimized on a development set.Global property of word distributions could behelpful, too.
A general term often has less dis-criminative power in the alignment frameworkthan a word that is localized to a subsection ofthe document and is related to specific subtopics.For example, in a lecture that teaches introductorycomputer science topics, aligning a general term?computer?
should receive a smaller weight thanaligning some topic-specific terms such as ?au-tomaton.?
The latter word is more likely to appearin a more narrow text span.
It is not straightfor-ward to directly calculate idf scores unless a lec-ture is split into smaller segments in some way.Instead, in our models, the distribution propertyof a word is considered in word-level similaritymatrices with the following formula.sim(wi, wj) ={0 : i 6= j1?
?
var(wi)maxk(var(wk)) : i = jAligning different words receives no bonus,while matching the same word between bulletsand utterances receives a score of 1 minus a dis-tribution penalty, as shown in the formula above.The function var(wi) calculates the standard vari-ance of the positions where the word wi appears.Divided by the maximal standard variance of wordpositions in the same lecture, the score is normal-ized to [0,1].
This distribution penalty is weightedby ?, which is tuned in a development set.
Again,a general term is expected to have a larger posi-tional variance.Once a word-level matrix is acquired, it is com-bined with the bullet-utterance level matrix dis-cussed above.
Specifically, when measuring thesimilarity between a word vector (one of the 6vectors) and the transcripts of an utterance, wesum up the word-level similarity scores of allmatching words between them, normalize the re-sulted score by the length of the vector and ut-terance, and then renormalize it to the range1553[0, 1] within the same spoken document.
Thefinal bullet-utterance similarity matrix is incor-porated into the pre-order-walk suquentializationdiscussed above, when alignment is conducted.4.3 Parameter optimizationPowell?s algorithm (Press et al, 2007) is used tofind the optimal weights for the constraints we in-corporated above, to directly minimize the objec-tive function, i.e., the Pk and WindowDiff scoresthat we will discuss later.
As a summary, we have7 weights to tune: a weight for each of the fol-lowing: parent bullet, grandparent, adjacent sib-lings, children, grandchildren, and the current bul-let, plus the word distribution penalty ?.
The val-ues of these weights are determined on a develop-ment set.Note that the model we propose here does notexclude the use of further features; instead, manyother features, such as smoothed word similarityscores, can be easily added to this model.
Weare conservative on our model complexity here,in terms of number of weights need to be tuned,for the consideration of the size of data that wecan used to estimate these weights.
Finally, withall the 7 weights being determined, we apply thestandard dynamic time warping (DTW).5 Experimental set-up5.1 DataWe use a corpus of lectures recorded at a largeresearch university.
The correspondence betweenbullets and speech utterances are manually an-notated in a subset of this lecture corpus, whichcontains approximately 30,000 word tokens inits manual transcripts.
Intuitively, this roughlyequals a 120-page double-spaced essay in length.The lecturer?s voice was recorded with a head-mounted microphone with a 16kHz sampling rateand 16-bit samples.
Students?
comments andquestions were not recorded.
The speech is splitinto utterances by pauses longer than 200ms, re-sulting in around 4000 utterances.
There are 119slides that are composed of 921 bullets.
A sub-set containing around 25% consecutive slides andtheir corresponding speech/transcripts are used asour development set to tune the parameters dis-cussed earlier; the rest data are used as our testset.5.2 Evaluation metricWe evaluate our systems according to how wellthe segmentation implied by the inferred bulletalignment matches that of the manually anno-tated gold-standard bullet algnment.
Though onemay consider that different bullets may be of dif-ferent importance, in this paper we do not useany heuristics to judge this and we treat all bul-lets equally in our evaluation.
We evaluate oursystems with the Pk and WindowDiff metrics(Malioutov et al, 2007; Beeferman et al, 1999;Pevsner and Hearst, 2002).
Note that for bothmetrics, the lower a score is, the better the per-formance of a system is.
The Pk score computesthe probability of a randomly chosen pair of wordsbeing inconsistently separated.
The WindowDiffis a variant of Pk; it penalizes false positives andnear misses equally.6 Experimental results6.1 Alignment performanceTable 1 presents the results on automatic tran-scripts with a 39% WER, a typical WER in realis-tic and uncontrolled lecture conditions (Leeuwiset al, 2003; Hsu and Glass, 2006).
The tran-scripts were generated with the SONIC toolkit(Pellom, 2001).
The acoustic model was trainedon the Wall Street Journal dictation corpus.
Thelanguage model was trained on corpora obtainedfrom the Web through searching the words ap-pearing on slides as suggested by (Munteanu etal., 2007).Pk WindowDiffUNI 0.481 0.545TT 0.469 0.534B-ALN 0.283 0.376HG-ALN 0.266 0.359Table 1: The Pk and WindowDiff scores of uni-form segmentation (UNI), TextTiling (TT), base-line alignment (B-ALN), and alignment with hier-archical and global information (HG-ALN).From Table 1, we can see that the model that1554utilizes the hierarchical structures of slides andglobal distribution of words, i.e., the HG-ALNmodel, reduces both Pk and WindowDiff scoresover the baseline model, B-ALN.
As discussedearlier, the baseline is a re-implementation ofstandard dynamic time warping based only on apre-order walk of the slides, while the HG-ALNmodel incorporates also hierarchical bullet con-straints and global word distribution.Table 1 also presents the performance of atypical topic segmentation algorithm, TextTiling(Hearst, 1997).
Note that similar to (Malioutov etal., 2007), we force the number of predicted topicsegments to be the target number, i.e., in our task,the number of bullets.
The results show that boththe Pk and WindowDiff scores of TextTiling aresignificantly higher than those of the alignment al-gorithms.
Our manual analysis suggests that manysegments are as short as several utterances and thedifference between two consecutive segments istoo subtle to be captured by a lexical cohesion-based method such as TextTiling.
For compari-son, We also present the results of uniform seg-mentation (UNI), which simply splits the tran-script of each lecture evenly into segments withsame numbers of words.6.2 Performance under different WERsSpeech recognition errors within reasonableranges often have very small impact on many spo-ken language processing tasks such as spoken lan-guage retrieval (Garofolo et al, 2000) and speechsummarization (Christensen et al, 2004; Maskey,2008; Murray, 2008; Zhu, 2010).
To study theimpact of speech recognition errors on our taskhere, we experimented with the alignment mod-els on manual transcripts as well as on automatictranscripts with different WERs, including a 39%and a 46% WER produced by two real recogni-tion systems.
To increase the spectrum of our ob-servation, we also overfit our ASR models to ob-tain smaller WERs at the levels of 11%, 19%, and30%.From Figure 2, we can see that at all levels ofthese different WERs, the HG-ALN model con-sistently outperforms the B-ALN system (the AU-DIO model will be discussed below).
The Pkand WindowDiff curves also show that the align-0 0.1 0.2 0.3 0.40.240.260.280.30.32Pk under different WERsPkWord error rateB?ALNHG?ALNAUDIO0 0.1 0.2 0.3 0.40.340.360.380.4WindowDiff under different WERsWindowDiffWord error rateB?ALNHG?ALNAUDIOFigure 2: The impact of different WERs on thealignment models.
The performance of an audio-based model (AUDIO) is also presented.ment performance is sensitive to recognition er-rors, particularly when the WER is in the range of30%?45%, suggesting that the problem we studyhere can benefit from the improvement of currentASR systems in this range, e.g., the recent ad-vance achieved in (Glass et al, 2007).6.3 Imposing hierarchical structures ontoraw speechWe can actually impose hierarchical structures di-rectly onto raw speech, through estimating thesimilarity between bullets and speech.
This en-ables navigation through the raw speech by usingslides; e.g., one can hear different parts of speechby clicking a bullet.
We apply keyword spotting tosolve this problem, which detects the occurrencesof each bullet word in the corresponding lectureaudio.1555In this paper, we use a token-passing based al-gorithm provided in the ASR toolkit SONIC (Pel-lom, 2001).
Since the slides are given in advance,we manually add into the pronunciation dictio-nary the words that appear in slides but not inthe pronunciation dictionary.
To estimate sim-ilarity between a word vector (discussed earlierin Section 4.2) and an utterance, we sum up allkeyword-spotting confidence scores assigned be-tween them, normalize the resulted score by thelength of the vector and the duration of the utter-ance, and then renormalize it to the range [0, 1]within the same spoken lecture.We present the performance of our bullet-audioalignment model (AUDIO) in Figure 2 so that onecan compare its effectiveness with the transcrip-tion based methods.
The figure shows that theperformance of the AUDIO model is comparableto the baseline transcription-based model, i.e., B-ALN, when the WERs of the transcripts are in therange of 37%?39%.
The performance is compara-ble to the HG-ALN model when WERs are in therange of 42%?44%.
Also, this suggests that incor-porating hierarchical and global features compen-sates for the performance degradation of speechrecognition in this range when the WER is 4%-6% higher.Note that we did not observe that the perfor-mance is different when incorporating hierarchi-cal information and global word distributions intothe AUDIO model, so the AUDIO results in Fig-ure 2 are the performance of both types of meth-ods.
The current keyword spotting componentyields a high false-positive rate; e.g., it incorrectlyreports many words that are acoustically similar toparts of other words that really appear in an utter-ance.
This happened even when a high thresholdis set.
The noise impairs the benefit of hierarchicaland distribution features.7 Conclusions and discussionsThis paper investigates the problem of imposinga known hierarchical structure onto an unstruc-tured spoken document.
Results show that incor-porating local hierarchical constraints and globalword distributions in the efficient dynamic pro-gramming framework yields a better performanceover the baseline.
Further experiments on a widerange of WERs confirm that the improvement isconsistent, and show that both types of modelsare sensitive to speech recognition errors, partic-ularly when WER increases to 30% and above.Moreover, directly imposing hierarchical struc-tures onto raw speech through keyword spottingachieves competitive performance.ReferencesBeeferman, D., A. Berger, and J. Lafferty.
1999.Statistical models for text segmentation.
MachineLearning, 34(1-3):177?210.Branavan, S., Deshpande P., and Barzilay R. 2007.Generating a table-of-contents: A hierarchical dis-criminative approach.
In Proc.
of Annual Meetingof the Association for Computational Linguistics.Chen, Y. and W. J. Heng.
2003.
Automatic synchro-nization of speech transcript and slides in presenta-tion.
In Proc.
International Symposium on Circuitsand Systems.Christensen, H., B. Kolluru, Y. Gotoh, and S. Re-nals.
2004.
From text summarisation to style-specific summarisation for broadcast news.
In Proc.of the 26th European Conference on InformationRetrieval, pages 223?237.Fan, Q., K. Barnard, A. Amir, A. Efrat, and M. Lin.2006.
Matching slides to presentation videos usingsift and scene background.
In Proc.
of ACM Inter-national Workshop on Multimedia Information Re-trieval, pages 239?248.Garofolo, J., G. Auzanne, and E. Voorhees.
2000.The trec spoken document retrieval track: A successstory.
In Proc.
of Text Retrieval Conference, pages16?19.Glass, J., T. Hazen, S. Cyphers, I. Malioutov,D.
Huynh, and R. Barzilay.
2007.
Recent progressin the mit spoken lecture processing project.
Proc.of Annual Conference of the International SpeechCommunication Association, pages 2553?2556.Hearst, M. 1997.
Texttiling: Segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23(1):33?64.Hori, C. and S. Furui.
2003.
A new approach to au-tomatic speech summarization.
IEEE Transactionson Multimedia, 5(3):368?378.Hsu, B. and J.
Glass.
2006.
Style and topic languagemodel adaptation using hmm-lda.
In Proc.
of Con-ference on Empirical Methods in Natural LanguageProcessing.1556Jing, H. 2002.
Using hidden markov modeling todecompose human-written summaries.
Computa-tional Linguistics, 28(4):527?543.Leeuwis, E., M. Federico, and M. Cettolo.
2003.
Lan-guage modeling and transcription of the ted corpuslectures.
In Proc.
of IEEE International Conferenceon Acoustics, Speech and Signal Processing.Liu, T., R. Hjelsvold, and J. R. Kender.
2002.
Analysisand enhancement of videos of electronic slide pre-sentations.
In Proc.
IEEE International Conferenceon Multimedia and Expo.Malioutov, I., A.
Park, B. Barzilay, and J.
Glass.
2007.Making sense of sound: Unsupervised topic seg-mentation over acoustic input.
In Proc.
of AnnualMeeting of the Association for Computational Lin-guistics, pages 504?511.Marcu, D. 2000.
The theory and practice of discourseparsing and summarization.
The MIT Press.Maskey, S. 2008.
Automatic Broadcast News SpeechSummarization.
Ph.D. thesis, Columbia University.Munteanu, C., R. Baecker, G. Penn, E. Toms, andE.
James.
2006.
Effect of speech recognition accu-racy rates on the usefulness and usability of webcastarchives.
In Proc.
of ACM Conference on HumanFactors in Computing Systems, pages 493?502.Munteanu, C., G. Penn, and R. Baecker.
2007.Web-based language modelling for automatic lec-ture transcription.
In Proc.
of Annual Conferenceof the International Speech Communication Associ-ation.Murray, G. 2008.
Using Speech-Specific Character-istics for Automatic Speech Summarization.
Ph.D.thesis, University of Edinburgh.Pellom, B. L. 2001.
Sonic: The university of col-orado continuous speech recognizer.
Tech.
Rep. TR-CSLR-2001-01, University of Colorado.Pevsner, L. and M. Hearst.
2002.
A critique and im-provement of an evaluation metric for text segmen-tation.
Computational Linguistics, 28:19?36.Press, W.H., S.A. Teukolsky, W.T.
Vetterling, and B.P.Flannery.
2007.
Numerical recipes: The art of sci-ence computing.
Cambridge University Press.Ruddarraju, R. 2006.
Indexing Presentations UsingMultiple Media Streams.
Ph.D. thesis, Georgia In-stitute of Technology.
M.S.
Thesis.Stark, L., S. Whittaker, and J. Hirschberg.
2000.
Find-ing information in audio: A new paradigm for au-dio browsing and retrieval.
In Proc.
of InternationalConference on Spoken Language Processing.Wang, F., C. W. Ngo, and T. C. Pong.
2003.
Synchro-nization of lecture videos and electronic slides byvideo text analysis.
In Proc.
of ACM InternationalConference on Multimedia.Zechner, K. and A. Waibel.
2000.
Minimizing worderror rate in textual summaries of spoken language.In Proc.
of Applied Natural Language ProcessingConference and Meeting of the North AmericanChapter of the Association for Computational Lin-guistics, pages 186?193.Zhu, X., X.
He, C. Munteanu, and G. Penn.
2008.
Us-ing latent dirichlet alocation to incorporate domainknowledge for topic transition detection.
In Proc.of Annual Conference of the International SpeechCommunication Association.Zhu, X.
2010.
Summarizing Spoken DocumentsThrough Utterance Selection.
Ph.D. thesis, Univer-sity of Toronto.1557
