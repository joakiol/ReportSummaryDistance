Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 644?652,Beijing, August 2010Improving Corpus Comparability for Bilingual Lexicon Extraction fromComparable CorporaBo Li, Eric GaussierLaboratoire d?Informatique de Grenoble (LIG)Universite?
de Grenoblefirstname.lastname@imag.frAbstractPrevious work on bilingual lexicon extrac-tion from comparable corpora aimed atfinding a good representation for the usagepatterns of source and target words and atcomparing these patterns efficiently.
Inthis paper, we try to work it out in an-other way: improving the quality of thecomparable corpus from which the bilin-gual lexicon has to be extracted.
To doso, we propose a measure of comparabil-ity and a strategy to improve the qual-ity of a given corpus through an iterativeconstruction process.
Our approach, be-ing general, can be used with any existingbilingual lexicon extraction method.
Weshow here that it leads to a significant im-provement over standard bilingual lexiconextraction methods.1 IntroductionBilingual dictionaries are an essential resourcein many multilingual natural language process-ing (NLP) tasks such as machine translation (Ochand Ney, 2003) and cross-language informationretrieval (CLIR) (Ballesteros and Croft, 1997).Hand-coded dictionaries are of high quality, butexpensive to build and researchers have tried,since the end of the 1980s, to automaticallyextract bilingual lexicons from parallel corpora(see (Chen, 1993; Kay and Ro?scheisen, 1993;Melamed, 1997a; Melamed, 1997b) for earlywork).
Parallel corpora are however difficult toget at in several domains, and the majority ofbilingual collections are comparable and not par-allel.
Due to their low cost of acquisition, sev-eral researchers have tried to exploit such com-parable corpora for bilingual lexicon extraction(Fung and McKeown, 1997; Fung and Yee, 1998;Rapp, 1999; De?jean et al, 2002; Gaussier et al,2004; Robitaille et al, 2006; Morin et al, 2007;Yu and Tsujii, 2009).
The notion of comparabilityis however a loose one, and comparable corporarange from lowly comparable ones to highly com-parable ones and parallel ones.
For data-drivenNLP techniques, using better corpora often leadsto better results, a fact which should be true forthe task of bilingual lexicon extraction.
This pointhas largely been ignored in previous work on thesubject.
In this paper, we develop a well-foundedstrategy to improve the quality of a comparablecorpus, so as to improve in turn the quality of thebilingual lexicon extracted.
To do so, we first pro-pose a measure of comparability which we thenuse in a method to improve the quality of the ex-isting corpus.The remainder of the paper is organized as fol-lows: Section 2 introduces the experimental mate-rials used for the different evaluations; compara-bility measures are then presented and evaluatedin Section 3; in Section 4, we detail and evaluatea strategy to improve the quality of a given corpuswhile preserving its vocabulary; the method usedfor bilingual lexicon extraction is then describedand evaluated in Section 5.
Section 6 is then de-voted to a discussion, prior to the conclusion givenin Section 7.2 Experimental MaterialsFor the experiments reported here, several cor-pora were used: the parallel English-FrenchEuroparl corpus (Koehn, 2005), the TREC644(http://trec.nist.gov/) Associated Press corpus(AP, English) and the corpora used in themultilingual track of CLEF (http://www.clef-campaign.org) which includes the Los AngelesTimes (LAT94, English), Glasgow Herald (GH95,English), Le Monde (MON94, French), SDAFrench 94 (SDA94, French) and SDA French 95(SDA95, French).
In addition to these exist-ing corpora, two monolingual corpora from theWikipedia dump1 were built.
For English, allthe articles below the root category Society witha depth less than 42 were retained.
For French,all the articles with a depth less than 7 below thecategory Socie?te?
are extracted.
As a result, theEnglish corpus Wiki-En consists of 367,918 doc-uments and the French one Wiki-Fr consists of378,297 documents.The bilingual dictionary used in our experi-ments is constructed from an online dictionary.It consists of 33,372 distinct English words and27,733 distinct French words, which constitutes75,845 translation pairs.
Standard preprocessingsteps: tokenization, POS-tagging and lemmatiza-tion are performed on all the linguistic resources.We will directly work on lemmatized forms ofcontent words (nouns, verbs, adjectives, adverbs).3 Measuring ComparabilityAs far as we can tell, there are no practical mea-sures with which we can judge the degree of com-parability of a bilingual corpus.
In this paper, wepropose a comparability measure based on the ex-pectation of finding the translation for each wordin the corpus.
The measure is light-weighted anddoes not depend on complex resources like themachine translation system.
For convenience, thefollowing discussions will be made in the contextof the English-French comparable corpus.3.1 The Comparability MeasureFor the comparable corpus C, if we consider thetranslation process from the English part Ce to the1The Wikipedia dump files can be downloaded athttp://download.wikimedia.org.
In this paper, we use the En-glish dump file on July 13, 2009 and the French dump file onJuly 7, 2009.2There are several cycles in the category tree ofWikipedia.
It is thus necessary to define a threshold on thedepth to make the iterative process feasible.French part Cf , a comparability measure Mef canbe defined on the basis of the expectation of find-ing, for each English word we in the vocabularyCve of Ce, its translation in the vocabulary Cvf of Cf .Let ?
be a function indicating whether a transla-tion from the translation set Tw of w is found inthe vocabulary Cv of a corpus C, i.e.:?
(w, Cv) ={1 iff Tw ?
Cv 6= ?0 elseMef is then defined as:Mef (Ce, Cf ) = E(?
(w, Cvf )|w ?
Cve )=?w?Cve?
(w, Cvf ) ?
Pr(w ?
Cve )?
??
?Aw= |Cve ||Cve ?
Dve |?w?Cve?DveAwwhere Dve is the English part of a given, inde-pendent bilingual dictionaryD, and where the lastequality is based on the fact that, the compara-ble corpus and the bilingual dictionary being in-dependent of one another, the probability of find-ing the translation in Cvf of a word w is the samefor w is in Cve ?
Dve and in Cve\Dve 3.
Furthermore,the presence of common words suggests that oneshould rely on a presence/absence criterion ratherthan on the number of occurrences to avoid a biastowards common words.
Given the natural lan-guage text, our evaluation will show that the sim-ple presence/absence criterion can perform verywell.
This leads to Pr(w ?
Cve ) = 1/|Cve |, andfinally to:Mef (Ce, Cf ) =1|Cve ?
Dve |?w?Cve?Dve?
(w, Cvf )This formula shows that Mef is actually the pro-portion of English words translated in the Frenchpart of the comparable corpus.
Similarly, thecounterpart of Mef , Mfe, is defined as:Mfe(Ce, Cf ) =1|Cvf ?
Dvf |?w?Cvf?Dvf?
(w, Cve )3The fact can be reliable only when a substantial part ofthe corpus vocabulary is covered by the dictionary.
Fortu-nately, the constraint is satisfied in most applications wherethe common but not the specialized corpora like the medicalcorpora are involved.645and measures the proportion of French words inCvf translated in the English part of the compara-ble corpus.
A symmetric version of these mea-sures is obtained by considering the proportion ofthe words (both English and French) for which atranslation can be found in the corpus:M(Ce, Cf )=?w?Cve?Dve ?
(w, Cvf ) +?w?Cvf?Dvf ?
(w, Cve )|Cve ?
Dve |+ |Cvf ?
Dvf |We now present an evaluation of these measureson artificial test corpora.3.2 ValidationIn order to test the comparability measures, we de-veloped gold-standard comparability scores fromthe Europarl and AP corpora.
We start from theparallel corpus, Europarl, of which we degradethe comparability by gradually importing somedocuments from either Europarl or AP.
Threegroups (Ga, Gb, Gc) of comparable corpora arebuilt in this fashion.
Each group consists of testcorpora with a gold-standard comparability rang-ing, arbitrarily, from 0 to 1 and corresponding tothe proportion of documents in ?parallel?
transla-tion.
The first group Ga is built from Europarlonly.
First, the Europarl corpus is split into 10equal parts, leading to 10 parallel corpora (P1, P2,.
.
.
, P10) with a gold-standard comparability arbi-trarily set to 1.
Then for each parallel corpus, e.g.Pi, we replace a certain proportion p of the En-glish part with documents of the same size fromanother parallel corpus Pj(j 6= i), producing thenew corpus P ?i with less comparability which isthe gold-standard comparability 1 ?
p. For eachPi, as p increases, we obtain several comparablecorpora with a decreasing gold-standard compara-bility score.
All the Pi and their descendant cor-pora constitute the group Ga.
The only differencebetweenGb andGa is that, inGb, the replacementin Pi is done with documents from the AP cor-pus and not from Europarl.
In Gc, we start with10 final, comparable corpora P ?i from Ga. Thesecorpora have a gold-standard comparability of 0in Ga, and of 1 in Gc.
Then each P ?i is furtherdegraded by replacing certain portions with docu-ments from the AP corpus.0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.00.830.840.850.860.870.88MGold-standard comparability scoresScores from the comparabilitymetricFigure 1: Evolution of M wrt gold-standardon the corpus group Gc (x-axis: gold-standardscores, y-axis: M scores)We then computed, for each comparable cor-pus in each group, its comparability accordingto one of the comparability measures.
Figure 1plots the measure M for ten comparable corporaand their descendants from Gc, according to theirgold-standard comparability scores.
As one cannote, the measure M is able to capture almost allthe differences in comparability and is stronglycorrelated with the gold-standard.
The correla-tion between the different measures and the gold-standard is finally computed with Pearson corre-lation coefficient.
The results obtained are listedin Table 1.
As one can note, Mfe performs worstamong the three measures, the reason being thatthe process to construct Gb and Gc yields unbal-anced bilingual corpora, the English section beinglarger than the French one.
Translations of Frenchwords are still likely to be found in the Englishcorpus, even though the corpora are not compara-ble.
On all the 3 groups,M performs best and cor-relates very well with the gold standard, meaningthat M was able to capture all the differences incomparability artificially introduced in the degra-dation process we have considered.
This is themeasure we will retain in the following parts.Mef Mfe MGa 0.897 0.770 0.936Gb 0.955 0.190 0.979Gc 0.940 -0.595 0.960Table 1: Correlation scores for the different com-parability measures on the 3 groups of corpora646Having established a measure for the degree ofcomparability of bilingual corpora, we now turnto the problem of improving the quality of com-parable corpora.4 Improving Corpus QualityWe here try to improve the quality of a given cor-pus C, which we will refer to as the base corpus,by extracting the highly comparable subpart CHwhich is above a certain degree of comparability?
(Step 1), and by enriching the lowly comparablepart CL with texts from other sources (Step 2).
Aswe are interested in extracting information relatedto the vocabulary of the base corpus, we want thenewly built corpus to contain a substantial part ofthe base corpus.
This can be achieved by preserv-ing in Step 1 as many documents from the basecorpus as possible (e.g.
by considering low valuesof ?
), and by using in step 2 sources close to thebase corpus.4.1 Step 1: Extracting CHThe strategy consisting of building all the possiblesub-corpora of a given size from a given compa-rable corpora is not realistic as soon as the num-ber of documents making up the corpora is largerthan a few thousands.
In such cases, better waysfor extracting subparts have to be designed.
Thestrategy we have adopted here aims at efficientlyextracting a subpart of C above a certain degree ofcomparability and is based on the following prop-erty.Property 1.
Let d1e and d2e (resp.
d1f and d2f )be two English (resp.
French) documents from abilingual corpus C. We consider, as before, thatthe bilingual dictionary D is independent from C.Let (d1e ?, d1f ?)
be such that: d1e ?
?
d1e, d1f ?
?
d1f ,which means d1e ?
is a subpart of d1e and d1f ?
is asubpart of d1f .We assume:(i) |d1e?d2e||d2e| =|d1f?d2f ||d2f |(ii) Mef (d1e ?, d1f ) ?Mef (d2e, d2f )Mfe(d1e, d1f?)
?Mfe(d2e, d2f )Then:M(d2e, d2f ) ?M(d1e ?
d2e, d1f ?
d2f )Proof [sketch]: Let B = (d1e ?
d2e) ?
Dve )\(d2e ?Dve ).
One can show, by exploiting condition (ii),that:?w?B?
(w, d1f ?
d2f ) ?
|B|Mef (d2e, d2f )and similarly that:?w?d2e?Dve?
(w, d1f ?
d2f ) ?
|d2e ?
Dve |Mef (d2e, d2f )Then exploiting condition (i), and the indepen-dence between the corpus and the dictionary, onearrives at:?w?
(d1e?d2e)?Dve ?
(w, d1f ?
d2f )|(d1e ?
d2e) ?
Dve |+ |(d1f ?
d2f ) ?
Dvf |?|d2e ?
Dve |Mef (d2e, d2f )|d2e ?
Dve |+ |d2f ?
Dvf |The same development on Mfe completes theproof.
2Property 1 shows that one can incrementally ex-tract from a bilingual corpus a subpart with a guar-anteed minimum degree of comparability ?
by it-eratively adding new elements, provided (a) thatthe new elements have a degree of comparabilityof at least ?
and (b) that they are less comparablethan the currently extracted subpart (conditions(ii)).
This strategy is described in Algorithm 1.Since the degree of comparability is always abovea certain threshold and since the new documentsselected (d2e, d2f ) are the most comparable amongthe remaining documents, condition (i) is likelyto be satisfied, as this condition states that the in-crease in the vocabulary from the second docu-ments to the union of the two is the same in bothlanguages.
Similarly, considering new elementsby decreasing comparability scores is a necessarystep for the satisfaction of condition (ii), whichstates that the current subpart should be uniformlymore comparable than the element to be added.Hence, the conditions for property 1 to hold aremet in Algorithm 1, which finally yields a corpuswith a degree of comparability of at least ?.4.2 Step 2: Enriching CLThis step tries to absorb knowledge from otherresources, which will be called external corpus,647Algorithm 1Input:English document set Cde of CFrench document set Cdf of CThreshold ?Output:CH , consisting of the English document set Seand the French document set Sf1: Initialize Se = ?,Sf = ?, temp = 0;2: repeat3: (de, df ) = argmaxde?Cde ,df?CdfM(de, df );4: temp = maxde?Cde ,df?CdfM(de, df );5: if temp ?
?
then6: Add de into Se and add df into Sf ;7: Cde = Cde\de, Cdf = Cdf\df ;8: end if9: until Cde = ?
or Cdf = ?
or temp < ?10: return CH ;to enrich the lowly comparable part CL which isthe left part in C during the creation of CH .
Onechoice for obtaining the external corpus CT is tofetch documents which are likely to be compara-ble from the Internet.
In this case, we first ex-tract representative words for each document inCL, translate them using the bilingual dictionaryand retrieve associated documents via a search en-gine.
An alternative approach is of course to useexisting bilingual corpora.
Once CT has been con-structed, the lowly comparable part CL can be en-riched in exactly the same way as in section 4.1:First, Algorithm 1 is used on the English part ofCL and the French part of CT to get the high-quality document pairs.
Then the French part ofCL is enriched with the English part of CT by thesame algorithm.
All the high-quality documentpairs are then added to CH to constitute the finalresult.4.3 ValidationWe use here GH95 and SDA95 as the base cor-pus C0.
In order to illustrate that the efficiencyof the proposed algorithm is not confined to aspecific external resource, we consider two ex-ternal resources: (a) C1T made of LAT94, MON94and SDA94, and (b) C2T consisting of Wiki-En andWiki-Fr.
The number of documents in all the cor-pora after elimination of short documents (< 30words) is listed in Table 2.C0 C1T C2TEnglish 55,989 109,476 367,918French 42,463 87,086 378,297Table 2: The size of the corpora in the experimentsFor the extraction of the highly comparable partCH from the base corpus C0, we set ?
to 0.3so as to extract a substantial subpart of C0.
Af-ter this step, corresponding to Algorithm 1, wehave 20,124 English-French document pairs inCH .
The second step is to enrich the lowly compa-rable part CL of the base corpus documents fromthe external resources C1T and C2T .
The final cor-pora we obtain consist of 46,996 document pairsfor C1 (with C1T ) and of 54,402 document pairs forC2 (with C2T ), size similar to the one of C0.
Theproportion of documents (columns ?D-e?
and ?D-f?
), sentences (columns ?S-e?
and ?S-f?)
and vo-cabulary (columns ?V-e?
and ?V-f?)
of C0 foundin C1 and C2 is given in Table 3.
As one can note,the final corpora obtained through the method pre-sented above preserve most of the informationfrom the base corpus.
Especially for the vocab-ulary, the final corpora cover nearly all the vocab-ulary of the base corpus.
Considering the compa-rability scores, the comparability of C1 is 0.912and the one of C2 is 0.916.
Both of them aremore comparable than the base corpus of whichthe comparability is 0.882.From these results of the intrinsic evaluation,one can conclude that the strategy developed toimprove the corpus quality while preserving mostof its information is efficient: The corpora ob-tained here, C1 and C2, are more comparable thanthe base corpus C0 and preserve most of its infor-mation.
We now turn to the problem of extractingbilingual lexicons from these corpora.5 Bilingual Lexicon ExtractionFollowing standard practice in bilingual lexiconextraction from comparable corpora, we rely onthe approach proposed by Fung and Yee (1998).In this approach, each word w is represented as a648D-e D-f S-e S-f V-e V-fC1 0.669 0.698 0.821 0.805 0.937 0.981C2 0.785 0.719 0.893 0.807 0.968 0.987Table 3: Proportion of documents, sentences andvocabulary of C0 covered by the result corporacontext vector consisting of the weight a(wc) ofeach context word wc, the context being extractedfrom a window running through the corpus.
Oncecontext vectors for English and French words havebeen constructed, a general bilingual dictionaryDcan be used to bridge them by accumulating thecontributions from words that are translation ofeach other.
Standard similarity measures, as thecosine or the Jaccard coefficient, can then be ap-plied to compute the similarity between vectors.For example, the cosine leads to:sc(we, wf ) =?
(wce,wcf )?D a(wce)a(wcf )???we?
?
???wf?
(1)5.1 Using Algorithm 1 pseudo-AlignmentsThe process we have defined in the previous sec-tion to improve the quality of a given corpus whilepreserving its vocabulary makes use of highlycomparable document pairs, and thus providessome loose alignments between the two corpora.One can thus try to leverage the above approachto bilingual lexicon extraction by re-weightingsc(we, wf ) by a quantity which is large if we andwf appear in many document pairs with a highcomparability score, and small otherwise.
In thissection, we can not use the alignments in algo-rithm 1 directly because the alignments in thecomparable corpus should not be 1 to 1 and wedid not try to find the precise 1 to 1 alignments inalgorithm 1.Let ?
be the threshold used in algorithm 1 toconstruct the improved corpus and let ?
(de, df )be defined as:?
(de, df ) ={1 iff M(de, df ) ?
?0 elseLet He (resp.
Hf ) be the set of documents con-taining word we (resp.
wf ).
We define the jointprobability of we and wf as being proportionalto the number of comparable document pairs theybelong to, where two documents are comparableif their comparability score is above ?, that is:p(we, wf ) ??de?He,df?Hf?
(de, df )The marginal probability p(we) can then be writ-ten as:p(we)?
?wf?Cvfp(we, wf )?
?de?He?df?Cdf|df | ?
?
(de, df )Assuming that all df in Cdf have roughly thesame vocabulary size and all de have the samenumber of comparable counterparts in Cdf , thenthe marginal probability can be simplified as:p(we) ?
|He|.
By resorting to the exponentialof the point-wise mutual information, one finallyobtains the following weight:pi(we, wf ) =p(we, wf )p(we) ?
p(wf )?
1|He| ?
|Hf |?de?He,df?Hf?
(de, df )which has the desired property: It is large if thetwo words appear in comparable document pairsmore often than chance would predict, and smallotherwise.
We thus obtain the revised similarityscore for we and wf :scr(we, wf ) = sc(we, wf ) ?
pi(we, wf ) (2)5.2 ValidationIn order to measure the performance of the bilin-gual lexicon extraction method presented above,we divided the original dictionary into 2 parts:10% of the English words (3,338 words) togetherwith their translations are randomly chosen andused as the evaluation set, the remaining words(30,034 words) being used to compute contextvectors and similarity between them.
In thisstudy, the weight a(wc) used in the context vec-tors (see above) are taken to be the tf-idf scoreof wc: a(wc) = tf-idf(wc).
English words not649present in Cve or with no translation in Cvf are ex-cluded from the evaluation set.
For each Englishword in the evaluation set, all the French wordsin Cvf are then ranked according to their similar-ity with the English word (using either equation 1or 2).
To evaluate the quality of the lexicons ex-tracted, we first retain for each English word itsN first translations, and then measure the preci-sion of the lists obtained, which amounts in thiscase to the proportion of lists containing the cor-rect translation (in case of multiple translations, alist is deemed to contain the correct translation assoon as one of the possible translations is present).This evaluation procedure has been used in pre-vious work (e.g.
(Gaussier et al, 2004)) and isnow standard for the evaluation of lexicons ex-tracted from comparable corpora.
In this study,N is set to 20.
Furthermore, several studies haveshown that it is easier to find the correct transla-tions for frequent words than for infrequent ones(Pekar et al, 2006).
To take this fact into account,we distinguished different frequency ranges to as-sess the validity of our approach for all frequencyranges.
Words with frequency less than 100 aredefined as low-frequency words (WL), whereaswords with frequency larger than 400 are high-frequency words (WH ), and words with frequencyin between are medium-frequency words (WM ).We then tested the standard method based onthe cosine similarity (equation 1) on the corporaC0, CH , C?H , C1 and C2.
The results obtained aredisplayed in Table 4, and correspond to columns2-6.
They show that the standard approach per-forms significantly better on the improved corporaC1/C2 than on the base corpus C0.
The overall pre-cision is increased by 5.3% on C1 (correspondingto a relative increase of 26%) and 9.5% on C2 (cor-responding to a relative increase of 51%), eventhough the low-frequency words, which dominatethe overall precision, account for a higher pro-portion in C1 (61.3%) and C2 (61.3%) than inC0 (56.2%).
For the medium and high frequencywords, the precision is increased by over 11% onC1 and 16% on C2.
As pointed out in other stud-ies, the performance for the low-frequency wordsis usually bad due to the lack of context informa-tion.
This explains the relatively small improve-ment obtained here (only 2.2% on C1 and 6.7%on C2).
It should also be noticed that the perfor-mance of the standard approach is better on C2than on C1, which may be due to the fact that C2is slightly larger than C1 and thus provides moreinformation or to the actual content of these cor-pora.
Lastly, if we consider the results on the cor-pus CH which is produced by only choosing thehighly comparable part from C0, the overall preci-sion is increased by only 1.9%, which might comefrom the fact that the size of CH is less than halfthe size of C0.
We also notice the better results onCH than on C?H of the same size which consists ofrandomly choosing documents from C0.The results obtained with the refined approachmaking use of the comparable document pairsfound in the improved corpus (equation 2) arealso displayed in Table 4 (columns ?C1 new?
and?C2 new?).
From these results, one can see thatthe overall precision is further improved by 2.0%on C1 and 2.3% on C2, compared with the stan-dard approach.
For all the low, medium andhigh-frequency words, the precision has been im-proved, which demonstrates that the informationobtained through the corpus enrichment processcontributes to improve the quality of the extractedbilingual lexicons.
Compared with the originalbase corpus C0, the overall improvement of theprecision on both C1 and C2 with the refined ap-proach is significant and important (respectivelycorresponding to a relative improvement of 35%and 62%), which also demonstrates that the effi-ciency of the refined approach is not confined to aspecific external corpus.6 DiscussionIt is in a way useless to deploy bilingual lexiconextraction techniques if translation equivalents arenot present in the corpus.
This simple fact is at thebasis of our approach which consists in construct-ing comparable corpora close to the original cor-pus and which are more likely to contain transla-tion equivalents as they have a guaranteed degreeof comparability.
The pseudo-alignments identi-fied in the construction process are then used toleverage state-of-the-art bilingual lexicon extrac-tion methods.
This approach to bilingual lexiconextraction from comparable corpora radically dif-fers, to our knowledge, from previous approaches650C0 CH C?H C1 C2 C1 new > C1, > C0 C2 new > C2, > C0WL 0.114 0.144 0.125 0.136 0.181 0.156 2.0%, 4.2% 0.205 2.4%, 9.1%WM 0.233 0.313 0.270 0.345 0.401 0.369 2.4%, 3.6% 0.433 3.2%, 20.0%WH 0.417 0.456 0.377 0.568 0.633 0.581 1.3%, 16.4% 0.643 1.0%, 22.6%All 0.205 0.224 0.189 0.258 0.310 0.278 2.0%, 7.3% 0.333 2.3%, 12.8%Table 4: Precision of the different approaches on different corporawhich are mainly variants of the standard methodproposed in (Fung and Yee, 1998) and (Rapp,1999).
For example, the method developed in(De?jean et al, 2002) and (Chiao and Zweigen-baum, 2002) involves a representation of dictio-nary entries with context vectors onto which newwords are mapped.
Pekar et al (2006) smooththe context vectors used in the standard approachin order to better deal with low frequency words.A nice geometric interpretation of these processesis proposed in (Gaussier et al, 2004), which fur-thermore introduces variants based on Fisher ker-nels, Canonical Correlation Analysis and a com-bination of them, leading to an improvement ofthe F1-score of 2% (from 0.14 to 0.16) when con-sidering the top 20 candidates.
In contrast, the ap-proach we have developed yields an improvementof 7% (from 0.13 to 0.20) of the F-1 score on C2,again considering the top 20 candidates.
More im-portant, however, is the fact that the approach wehave developed can be used in conjunction withany existing bilingual extraction method, as thestrategies for improving the corpus quality and there-weighting formula (equation 2) are general.
Wewill assess in the future whether substantial gainsare also attained with other methods.Some studies have tried to extract subparts ofcomparable corpora to complement existing par-allel corpora.
Munteanu (2004) thus developed amaximum entropy classifier aiming at extractingthose sentence pairs which can be deemed paral-lel.
The step for choosing similar document pairsin this work resembles some of our steps.
How-ever their work focuses on high quality and spe-cific documents pairs, as opposed to the entire cor-pus of guaranteed quality we want to build.
Inthis latter case, the cross-interaction between doc-uments impacts the overall comparability score,and new methods, as the one we have introduced,need to be proposed.
Similarly, Munteanu andMarcu (2006) propose a method to extract sub-sentential fragments from non-parallel corpora.Again, the targeted elements are very specific(parallel sentences or sub-sentences) and limited,and the focus is put on a few sentences which canbe considered parallel.
As already mentioned, werather focus here on building a new corpus whichpreserves most of the information in the originalcorpus.
The construction process we have pre-sented is theoretically justified and allows one topreserve ca.
95% of the original vocabulary.7 ConclusionWe have first introduced in this paper a compara-bility measure based on the expectation of find-ing translation word pairs in the corpus.
We havethen designed a strategy to construct an improvedcomparable corpus by (a) extracting a subpart ofthe original corpus with a guaranteed compara-bility level, and (b) by completing the remainingsubpart with external resources, in our case otherexisting bilingual corpora.
We have then shownhow the information obtained during the construc-tion process could be used to improve state-of-the-art bilingual lexicon extraction methods.
Wehave furthermore assessed the various steps ofour approach and shown: (a) that the compara-bility measure we introduced captures variationsin the degree of comparability between corpora,(b) that the construction process we introducedleads to an improved corpus preserving most ofthe original vocabulary, and (c) that the use ofpseudo-alignments through simple re-weightingyields bilingual lexicons of higher quality.AcknowledgementsThis work was supported by the French NationalResearch Agency grant ANR-08-CORD-009.651ReferencesBallesteros, Lisa and W. Bruce Croft.
1997.
Phrasaltranslation and query expansion techniques forcross-language information retrieval.
In Proceed-ings of the 20th ACM SIGIR, pages 84?91, Philadel-phia, Pennsylvania, USA.Chen, Stanley F. 1993.
Aligning sentences in bilingualcorpora using lexical information.
In Proceedingsof the 31st Annual Conference of the Association forComputational Linguistics, pages 9?16, Columbus,Ohio, USA.Chiao, Yun-Chuang and Pierre Zweigenbaum.
2002.Looking for candidate translational equivalents inspecialized, comparable corpora.
In Proceedingsof the 19th International Conference on Computa-tional Linguistics, pages 1?7, Taipei, Taiwan.De?jean, Herve?, Eric Gaussier, and Fatia Sadat.
2002.An approach based on multilingual thesauri andmodel combination for bilingual lexicon extraction.In Proceedings of the 19th International Conferenceon Computational Linguistics, pages 1?7, Taipei,Taiwan.Fung, Pascale and Kathleen McKeown.
1997.
Findingterminology translations from non-parallel corpora.In Proceedings of the 5th Annual Workshop on VeryLarge Corpora, pages 192?202, Hong Kong.Fung, Pascale and Lo Yuen Yee.
1998.
An IR ap-proach for translating new words from nonparallel,comparable texts.
In Proceedings of the 17th inter-national conference on Computational linguistics,pages 414?420, Montreal, Quebec, Canada.Gaussier, E., J.-M. Renders, I. Matveeva, C. Goutte,and H. De?jean.
2004.
A geometric view on bilin-gual lexicon extraction from comparable corpora.In Proceedings of the 42nd Annual Meeting of theAssociation for Computational Linguistics, pages526?533, Barcelona, Spain.Kay, Martin and Martin Ro?scheisen.
1993.
Text-translation alignment.
Computational Linguistics,19(1):121?142.Koehn, Philipp.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedingsof MT Summit 2005.Melamed, I. Dan.
1997a.
A portable algorithmfor mapping bitext correspondence.
In Proceed-ings of the 35th Annual Meeting of the Associationfor Computational Linguistics and the 8th Confer-ence of the European Chapter of the Association forComputational Linguistics, pages 305?312, Madrid,Spain.Melamed, I. Dan.
1997b.
A word-to-word modelof translational equivalence.
In Proceedings of the35th Annual Meeting of the Association for Compu-tational Linguistics and the 8th Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, pages 490?497, Madrid, Spain.Morin, Emmanuel, Be?atrice Daille, Koichi Takeuchi,and Kyo Kageura.
2007.
Bilingual terminologymining - using brain, not brawn comparable cor-pora.
In Proceedings of the 45th Annual Meetingof the Association for Computational Linguistics,pages 664?671, Prague, Czech Republic.Munteanu, Dragos Stefan and Daniel Marcu.
2006.Extracting parallel sub-sentential fragments fromnon-parallel corpora.
In Proceedings of the 21st In-ternational Conference on Computational Linguis-tics and the 44th annual meeting of the Associationfor Computational Linguistics, pages 81?88, Syd-ney, Australia.Munteanu, Dragos Stefan, Alexander Fraser, andDaniel Marcu.
2004.
Improved machine translationperformance via parallel sentence extraction fromcomparable corpora.
In Proceedings of the HLT-NAACL 2004, pages 265?272, Boston, MA., USA.Och, Franz Josef and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Pekar, Viktor, Ruslan Mitkov, Dimitar Blagoev, andAndrea Mulloni.
2006.
Finding translations forlow-frequency words in comparable corpora.
Ma-chine Translation, 20(4):247?266.Rapp, Reinhard.
1999.
Automatic identification ofword translations from unrelated English and Ger-man corpora.
In Proceedings of the 37th AnnualMeeting of the Association for Computational Lin-guistics, pages 519?526, College Park, Maryland,USA.Robitaille, Xavier, Yasuhiro Sasaki, MasatsuguTonoike, Satoshi Sato, and Takehito Utsuro.
2006.Compiling French-Japanese terminologies from theweb.
In Proceedings of the 11st Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, pages 225?232, Trento, Italy.Yu, Kun and Junichi Tsujii.
2009.
Extracting bilin-gual dictionary from comparable corpora with de-pendency heterogeneity.
In Proceedings of HLT-NAACL 2009, pages 121?124, Boulder, Colorado,USA.652
