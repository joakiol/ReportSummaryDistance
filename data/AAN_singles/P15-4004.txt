Proceedings of ACL-IJCNLP 2015 System Demonstrations, pages 19?24,Beijing, China, July 26-31, 2015.c?2015 ACL and AFNLPKeLP: a Kernel-based Learning Platform for Natural LanguageProcessingSimone Filice(?
), Giuseppe Castellucci(?
), Danilo Croce(?
), Roberto Basili(?)(?)
Dept.
of Civil Engineering and Computer Science Engineering(?)
Dept.
of Electronic Engineering(?)
Dept.
of Enterprise EngineeringUniversity of Roma, Tor Vergata, Italy{filice,croce,basili}@info.uniroma2.it; castellucci@ing.uniroma2.itAbstractKernel-based learning algorithms havebeen shown to achieve state-of-the-art re-sults in many Natural Language Process-ing (NLP) tasks.
We present KELP, a Javaframework that supports the implementa-tion of both kernel-based learning algo-rithms and kernel functions over genericdata representation, e.g.
vectorial data ordiscrete structures.
The framework hasbeen designed to decouple kernel func-tions and learning algorithms: once a newkernel function has been implemented itcan be adopted in all the available kernel-machine algorithms.
The platform in-cludes different Online and Batch Learn-ing algorithms for Classification, Regres-sion and Clustering, as well as several Ker-nel functions, ranging from vector-basedto structural kernels.
This paper will showthe main aspects of the framework by ap-plying it to different NLP tasks.1 IntroductionMost of the existing Machine Learning (ML) plat-forms assume that instances are represented asvectors in a feature space, e.g.
(Joachims, 1999;Hall et al., 2009; Chang and Lin, 2011), that mustbe defined beforehand.
In Natural Language Pro-cessing (NLP) the definition of a feature space of-ten requires a complex feature engineering phase.Let us consider any NLP task in which syntacticinformation is crucial, e.g.
Boundary Detection inSemantic Role Labeling (Carreras and M`arquez,2005).
Understanding which syntactic patternsshould be captured is non-trivial and usually theresulting feature vector model is a poor approxi-mation.
Instead, a more natural approach is oper-ating directly with the parse tree of sentences.
Ker-nel methods (Shawe-Taylor and Cristianini, 2004)provide an efficient and effective solution, allow-ing to represent data at a more abstract level, whiletheir computation still looks at the informativeproperties of them.
For instance, Tree Kernels(Collins and Duffy, 2001) take in input two syntac-tic parse trees, and compute a similarity measureby looking at the shared sub-structures.In this paper, KELP, a Java kernel based learn-ing platform is presented.
It supports the imple-mentation of Kernel-based learning algorithms, aswell as kernel functions over generic data repre-sentations, e.g.
vectorial data or discrete struc-tures, such as trees and sequences.
The frameworkhas been designed to decouple data structures, ker-nel functions and learning algorithms in order tomaximize the re-use of existing functionalities: asan example, a new kernel can be included inherit-ing existing algorithms and vice versa.
KELP sup-ports XML and JSON serialization of kernel func-tions and algorithms, enabling the agile definitionof kernel-based learning systems without writingadditional lines of code.
KELP can effectivelytackle a wide variety of learning problems.
In par-ticular, in this paper we will show how vectorialand structured data can be exploited by KELP inthree NLP tasks: Twitter Sentiment Analysis, TextCategorization and Question Classification.2 Framework OverviewKELP is a machine learning library completelywritten in Java.
The Java language has been cho-sen in order to be compatible with many JavaNLP/IR tools that are developed by the commu-19nity, such as Stanford CoreNLP1, OpenNLP2orLucene3.
KELP is released as open source soft-ware under the Apache 2.0 license and the sourcecode is available on github4.
Furthermore it canbe imported via Maven.
A detailed documentationof KELP with helpful examples and use cases isavailable on the website of the Semantic AnalyticsGroup5of the University of Roma, Tor Vergata.In this Section, a closer look at the implementa-tion of different kinds of data representations, ker-nel functions and kernel-based learning algorithmsis provided.2.1 Data RepresentationsKELP supports both vectorial and structureddata to model learning instances.
For ex-ample, SparseVector can host a Bag-of-Words model, while DenseVector can rep-resent data derived from low dimensional em-beddings.
TreeRepresentation can modela parse tree and SequenceRepresentationcan be adopted to represent sequences of charac-ters or sequences of words.
Moreover, the plat-form enables the definition of more complex formsof data such as pairs, which are useful in model-ing those problems where instances can be natu-rally represented as pairs of texts, such as questionand answer in Q/A re-ranking (Severyn and Mos-chitti, 2013), text and hypothesis in textual entail-ment (Zanzotto et al., 2009) or sentence pairs inparaphrasing detection (Filice et al., 2015).2.2 KernelsMany ML algorithms rely on the notion of similar-ity between examples.
Kernel methods (Shawe-Taylor and Cristianini, 2004) leverage on theso-called kernel functions, which compute thesimilarity between instances in an implicit high-dimensional feature space without explicitly com-puting the coordinates of the data in that space.The kernel operation is often cheaper from a com-putational perspective and specific kernels havebeen defined for sequences, graphs, trees, texts,images, as well as vectors.Kernels can be combined and composed tocreate richer similarity metrics, where infor-mation from different Representations can1http://nlp.stanford.edu/software/corenlp.shtml2https://opennlp.apache.org/3http://lucene.apache.org/4https://github.com/SAG-KeLP5http://sag.art.uniroma2.it/demo-software/kelp/be exploited at the same time.
This flexibil-ity is completely supported by KELP, which isalso easy to extend with new kernels.
Amongthe currently available implementations of ker-nels, there are various standard kernels, suchas LinearKernel, PolynomialKernel orRbfKernel.
A large set of kernels specificallydesigned for NLP applications will be describedin the following section.2.2.1 Kernels for NLPMany tasks in NLP cannot be properly tackledconsidering only a Bag-of-Words approach and re-quire the exploration of deep syntactic aspects.
Inquestion classification the syntactic information iscrucial has largely demonstrated in (Croce et al.,2011).
In Textual Entailment Recognition or inParaphrase Detection a pure lexical similarity be-tween text and hypothesis cannot capture any dif-ference between Federer won against Nadal andNadal won against Federer.
A manual definitionof an artificial feature set accounting for syntaxis a very expensive operation that requires a deepknowledge of the linguistic phenomena character-izing a specific task.
Moreover, every task hasspecific patterns that must be considered, makinga manual feature engineering an extremely com-plex and not portable operation.
How can linguis-tic patterns characterizing a question be automat-ically discovered?
How can linguistic rewritingrules in paraphrasing be learnt?
How can seman-tic and syntactic relations in textual entailment beautomatically captured?
An elegant and efficientapproach to solve NLP problems involving the us-age of syntax is provided by tree kernels (Collinsand Duffy, 2001).
Instead of trying to design asynthetic feature space, tree kernels directly oper-ate on the parse tree of sentences evaluating thetree fragments shared by two trees.
This operationimplicitly corresponds to a dot product in the fea-ture space of all possible tree fragments.
The di-mensionality of such space is extremely large andoperating directly on it is not viable.Many tree kernels are implemented in KELP,and they differ by the type of tree fragmentconsidered in the evaluation of the matchingstructures.
In the SubTreeKernel (Collinsand Duffy, 2001) valid fragments are subtrees(ST), i.e.
any node of a tree along withall its descendants.
A subset tree (SST) ex-ploited by the SubSetTreeKernel is a moregeneral structure since its leaves can be non-20SVPPPNPNNPNadalINagainstVBDwonNPNNPFederer=?NPNNPFedererNNPFedererVPPPNPNNPNadalINagainstVBDwonVBDwonPPNPNNPNadalINagainstINagainstNPNNPNadala) b)SVPNPNPNNPVPPPVBZINagainstSNPNNPFedererPPNPNNPNadalPPINVPPPNPINPPNPNNPNadalINPPNPNNPINSVPNPNNPFedererSVPVBDwonNPNNPFedererSVPPPINNPNNPFedererc) d)Figure 1: a) Constituent parse tree of the sentenceFederer won against Nadal.
b) some subtrees.
c)some subset trees.
d) some partial trees.terminal symbols.
The SSTs satisfy the con-straint that grammatical rules cannot be bro-ken.
PartialTreeKernel (Moschitti, 2006)relaxes this constraint considering partial trees(PT), i.e.
fragments generated by the applica-tion of partial production rules.
Examples of dif-ferent kinds of tree fragments are shown in Fig-ure 1.
The SmoothedPartialTreeKernel(SPTK) (Croce et al., 2011) allows to match thosefragments that are not identical but that are se-mantically related, by relying on the similaritybetween lexical items, e.g.
by applying a wordsimilarity metric (e.g.
WordNet or word em-beddings similarities).
The adopted implementa-tion allows to easily extend the notion of simi-larity between nodes, enabling the implementa-tion of more expressive kernels, as the Compo-sitionally Smoothed Partial Tree Kernel (CSPTK)that embeds algebraic operators of DistributionalCompositional Semantics (Annesi et al., 2014).Moreover, the SequenceKernel (Bunescu andMooney, 2005) is included in the library, and itallows to compare two texts evaluating the num-ber of common sub-sequences.
This implicitlycorresponds to operate on the space of all possi-ble N-grams.
Kernels operating over pairs, suchas the PreferenceKernel (Shen and Joshi,2003) for re-ranking, are also included in KELP.2.3 Machine Learning AlgorithmsIn ML, a plethora of learning algorithms havebeen defined for different purposes, and manyvariations of the existing ones as well as com-pletely new learning methods are often proposed.KELP provides a large number of learning algo-rithms6ranging from batch, e.g.
Support Vec-tor Machines (Vapnik, 1995), to online learningmodels, e.g.
PassiveAggressive algorithms(Crammer et al., 2006), and from linear to kernel-based methods, for tackling classification, regres-sion or clustering tasks.
Moreover, algorithmscan be composed in meta-learning schemas, likemulti-class classification (e.g.
One-VS-One andOne-VS-All, (Rifkin and Klautau, 2004)) andmulti-label classification, or can be combined inensembles.
A simple interface taxonomy allowsto easily extend the platform with new customlearning algorithms.
A complete support for tack-ling NLP tasks is thus provided.
For exam-ple, in scenarios where the syntactic informa-tion is necessary for achieving good accuracy,C-SVM or ?-SVM (Chang and Lin, 2011) oper-ating on trees with kernels can be effectively ap-plied.
When dealing with large datasets, manyefficient learning algorithm can be adopted, likelinear methods, e.g.
Pegasos (Shalev-Shwartzet al., 2007) or LibLinear, (Fan et al., 2008),or like budgeted kernel-based algorithms, e.g.RandomizedPerceptron (Cesa-Bianchi andGentile, 2006).Listing 1: A JSON example.
{"algorithm" : "oneVsAll","baseAlgorithm" : {"algorithm" : "binaryCSvmClassification","c" : 10,"kernel" : {"kernelType" : "linearComb","weights" : [1,1],"toCombine" : [{"kernelType" : "norm","baseKernel" : {"kernelType" : "ptk","mu" : 0.4,"lambda" : 0.4,"representation" : "parseTree"}},{"kernelType" : "linear","representation" : "Bag-of-Words"}]}}}2.4 A JSON exampleKernel functions and algorithms are serializable inJSON or XML.
This is useful for instantiating anew algorithm without writing a single line of Java6All the algorithms are completely re-implemented inJava and they do not wrap any external library21code, i.e.
the algorithm description can be pro-vided in JSON to an interpreter that will instantiateit.
Listing 1 reports a JSON example of a kernel-based Support Vector Machine operating in a one-vs-all schema, where a kernel linear combinationbetween a normalized Partial Tree Kernel and alinear kernel is adopted.
As the listing shows ker-nels and algorithms can be easily composed andcombined in order to create new training models.3 Case Studies in NLPIn this Section, the functionalities and use of thelearning platform are shown.
We apply KELP tovery different NLP tasks, i.e.
Sentiment Analysisin Twitter, Text Categorization and Question Clas-sification, providing examples of kernel-based andlinear learning algorithms.
Further examples areavailable on the KELP website7where it is shownhow to instantiate each algorithm or kernel viaJSON and how to add new algorithms, represen-tations and kernels.3.1 Sentiment Analysis in TwitterThe task of Sentiment Analysis in Twitter has beenproposed in 2013 during the SemEval competi-tion (Nakov et al., 2013).
We built a classifierfor the subtask B, i.e.
the classification of a tweetwith respect to the positive, negative and neutralclasses.
The contribution of different kernel func-tions is evaluated using the Support Vector Ma-chine learning algorithm.
As shown in Table 1, weapply linear (Lin), polynomial (Poly) and Gaus-sian (Rbf) kernels on two different data represen-tations: a Bag-Of-Words model of tweets (BoW )and a distributional representation (WS).
Thelast is obtained by linearly combining the distri-butional vectors corresponding to the words of amessage; these vectors are obtained by applying aSkip-gram model (Mikolov et al., 2013) with theword2vec tool8over 20 million of tweets.
The lin-ear combination of the proposed kernel functionsis also applied, e.g.
PolyBow+RbfWS.
The meanF1-measure of the positive and negative classes(pn)9as well as of all the classes (pnn) is shownin Table 1.3.2 Text CategorizationIn order to show the scalability of the platform,a second evaluation considers linear algorithms.7http://sag.art.uniroma2.it/demo-software/kelp/8https://code.google.com/p/word2vec/9pn was the official metric of the SemEval competition.Kernel MeanF1(pn) MeanF1(pnn)LinBoW59.72 63.53PolyBoW54.58 59.27LinWS60.79 63.94RbfWS61.68 65.05LinBoW+LinWS66.12 68.56PolyBoW+RbfWS64.92 68.10Table 1: Results of Sentiment AnalysisWe selected the Text Categorization task on theRCV1 dataset (Lewis et al., 2004) with the settingthat can be found on the LibLinear website10.
Inthis version of the dataset, CCAT and ECAT arecollapsed into a positive class, while GCAT andMCAT are the negative class, resulting in a datasetcomposed by 20, 242 examples.
As shown in Ta-ble 2, we applied the LibLinear, Pegasos and Lin-ear Passive-Aggressive implementations, comput-ing the accuracy and the standard deviation withrespect to a 5-fold cross validation strategy.Task Accuracy StdLibLinear 96.74% 0.0029Pegasos 95.31% 0.0033Passive Aggressive 96.60% 0.0024Table 2: Text Categorization Accuracy3.3 Question ClassificationThe third case study explores the application ofTree Kernels to Question Classification (QC), aninference task required in many Question Answer-ing processes.
In this problem, questions writ-ten in natural language are assigned to differentclasses.
A QC system should select the correctclass given an instance question.
In this setting,Tree Kernels allow to directly model the examplesin terms of their parse trees.
The reference cor-pus is the UIUC dataset (Li and Roth, 2002), in-cluding 5,452 questions for training and 500 ques-tions for test11, organized in six coarse-grainedclasses, such as HUMAN or LOCATION.
Again,Kernel-based SVM has been evaluated adoptingthe same setup of (Croce et al., 2011).
A pure lex-ical model based on a linear kernel over a Bag-of-Words (BoW) is considered a baseline.
The con-tribution of the syntactic information is demon-strated by the results achieved by the Partial TreeKernel (PTK), the Smoothed Partial Tree Kernels(SPTK) and the Compositionally Smoothed Par-tial Tree Kernel (CSPTK), as shown in Table 3.10http://www.csie.ntu.edu.tw/?cjlin/libsvmtools/datasets/11http://cogcomp.cs.illinois.edu/Data/QA/QC/22Kernel AccuracyBoW 87.2%PolyBoW88.8%PTK 91.6%SPTK 94.6%CSPTK 95.0%Table 3: Question Classification Accuracy.4 Related WorkMany software tools for computational linguis-tic research already exist.
Tools like Stan-ford CoreNLP or OpenNLP provide a completepipeline for performing linguistic tasks such asstemming, lemmatization, Part-of-Speech taggingor parsing.
They are complementary to KELP:they can be used in the feature extraction phase,while KELP will care about the machine learningpart.
Regarding other machine learning platformsthere are plenty of available possibilities, but fordifferent reasons no one can provide somethingclose to what the proposed library offers.Weka (Hall et al., 2009) is a collection of ma-chine learning algorithms for data mining tasks.The algorithms can either be applied directly toa dataset or called from Java.
It contains vari-ous tools for different data mining activities: datapre-processing, classification, regression, cluster-ing and visualization.Mallet (McCallum, 2002) is more oriented toNLP applications.
It is entirely in Java and in-cludes feature extraction tools for converting textinto vectors and statistical analysis tools for docu-ment classification, clustering, topic modeling, in-formation extraction, and other machine learningapplications to text.
Regarding the kernel-basedlearning both Weka and Mallet leverage on Lib-SVM, and obviously inherit its limits.LibSVM (Chang and Lin, 2011) is a machinelearning platform focusing on Support Vector Ma-chines.
It is written in C++ language and itincludes different SVM formulations: C-svm,Nu-svm and OneClass-svm, as well as a one-vs-one multi classification schema.
It implementsalso regression support vector solvers.
It has beenported in different languages, including Java.
Thebatch learning part of KELP is strongly inspiredby LibSVM formulations and implementations.LibSVM is mainly intended for plain users anddoes not provide any support for extendibility.
Itcan operate only on sparse feature vectors via stan-dard kernel functions.
No structured representa-tions are considered.Another very popular Support Vector Machines(SVM) package is SvmLight (Joachims, 1999).
Itis entirely written in C language and its main fea-ture is speed.
It solves classification and regres-sion problems, as well as ranking problems.
Itsefficiency is paid in terms of extensibility: C lan-guage does not allow a fast prototyping of new ma-chine learning kernels or algorithms.
Many timesin research contexts fast prototyping is more im-portant than performances: the proposed platformhas been developed with extensibility in mind.The most similar platform to ours is JKernel-Machines (Picard et al., 2013).
It is a Java basedpackage focused on Kernel machines.
Just like theproposed library, JKernelMachines is primary de-signed to deal with custom kernels that cannot beeasily found in standard libraries.
Standard SVMoptimization algorithms are implemented, but alsomore sophisticated learning-based kernel combi-nation methods such as Multiple Kernel Learn-ing (MKL).
However, many features covered byKELP are not offered by JKernelMachines, justlike tree kernels, regression and clustering.
More-over, different architectural choices have been ap-plied in KELP in order to support an easier com-position and combination of representations, ker-nels as well as learning algorithms.5 ConclusionsThis paper presented KELP, a Java frameworkto support the application of Kernel-based learn-ing methods with a particular attention to Lan-guage Learning tasks.
The library implements alarge variety of kernel functions used in NLP (suchas Tree Kernels or Sequence Kernels) as well asmany learning algorithms useful in classification,regression, novelty detection or clustering prob-lems.
KELP can be imported via Maven but itsusage is not restricted to a Java-compliant environ-ment as it allows to build complex kernel machinebased systems, leveraging on JSON/XML inter-faces to instantiate classifiers.
The entire frame-work has been designed to support researchers inthe development of new kernel functions or algo-rithms, providing a principled decoupling of thedata structures in order to maximize the re-use ofexisting functionalities.
The benefits of the pro-posed environment have been shown in three NLPtasks, where results in line with the state-of-the-arthave been reached with the simple application ofvarious kernel functions available in KELP.23ReferencesPaolo Annesi, Danilo Croce, and Roberto Basili.
2014.Semantic compositionality in tree kernels.
In Proc.of CIKM 2014, pages 1029?1038, New York, NY,USA.
ACM.Razvan C. Bunescu and Raymond J. Mooney.
2005.Subsequence kernels for relation extraction.
InNIPS.Xavier Carreras and Llu?
?s M`arquez.
2005.
Intro-duction to the conll-2005 shared task: Semanticrole labeling.
In Proceedings of the Ninth Confer-ence on Computational Natural Language Learn-ing, CONLL ?05, pages 152?164, Stroudsburg, PA,USA.
Association for Computational Linguistics.Nicol`o Cesa-Bianchi and Claudio Gentile.
2006.Tracking the best hyperplane with a simple budgetperceptron.
In In Proc.
of the 19th Annual Con-ference on Computational Learning Theory, pages483?498.
Springer-Verlag.Chih-Chung Chang and Chih-Jen Lin.
2011.
LIB-SVM: A library for support vector machines.
ACMTransactions on Intelligent Systems and Technology,2:27:1?27:27.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In NIPS.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
JMLR, 7:551?585,December.Danilo Croce, Alessandro Moschitti, and RobertoBasili.
2011.
Structured lexical similarity via con-volution kernels on dependency trees.
In EMNLP,Edinburgh.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: A li-brary for large linear classification.
J. Mach.
Learn.Res., 9:1871?1874, June.Simone Filice, Giovanni Da San Martino, and Alessan-dro Moschitti.
2015.
Structural representations forlearning relations between pairs of texts.
In Pro-ceedings of the 53rd Annual Meeting of the Associa-tion for Computational Linguistics, Beijing, China,July.
Association for Computational Linguistics.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The weka data mining software: An update.sigkdd explor., 11(1).T.
Joachims.
1999.
Making large-scale SVM learningpractical.
In B. Sch?olkopf, C. Burges, and A. Smola,editors, Advances in Kernel Methods - Support Vec-tor Learning, pages 169?184.
MIT Press.David D. Lewis, Yiming Yang, Tony G. Rose, and FanLi.
2004.
Rcv1: A new benchmark collection fortext categorization research.
J. Mach.
Learn.
Res.,5:361?397, December.X.
Li and D. Roth.
2002.
Learning question classifiers.In Proceedings of ACL ?02, COLING ?02, pages 1?7, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Tomas Mikolov, Kai Chen, Greg Corrado, and Jef-frey Dean.
2013.
Efficient estimation of wordrepresentations in vector space.
arXiv preprintarXiv:1301.3781.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In ECML, Berlin, Germany, September.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
Semeval-2013 task 2: Sentiment analysisin twitter.
In Proceedings of SemEval 2013, pages312?320, Atlanta, USA.
ACL.David Picard, Nicolas Thome, and Matthieu Cord.2013.
Jkernelmachines: A simple framework forkernel machines.
Journal of Machine Learning Re-search, 14:1417?1421.Ryan Rifkin and Aldebaro Klautau.
2004.
In defenseof one-vs-all classification.
J. Mach.
Learn.
Res.,5:101?141, December.Aliaksei Severyn and Alessandro Moschitti.
2013.
Au-tomatic feature engineering for answer selection andextraction.
In Proceedings of the 2013 Conferenceon EMNLP, pages 458?467, Seattle, USA.
ACL.S.
Shalev-Shwartz, Y.
Singer, and N. Srebro.
2007.Pegasos: Primal estimated sub?gradient solver forSVM.
In Proc.
of ICML.John Shawe-Taylor and Nello Cristianini.
2004.
Ker-nel Methods for Pattern Analysis.
Cambridge Uni-versity Press.Libin Shen and Aravind K. Joshi.
2003.
An svm basedvoting algorithm with application to parse reranking.In In Proc.
of CoNLL 2003, pages 9?16.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer-Verlag New York, Inc.,New York, NY, USA.Fabio massimo Zanzotto, Marco Pennacchiotti, andAlessandro Moschitti.
2009.
A machine learn-ing approach to textual entailment recognition.
Nat.Lang.
Eng., 15(4):551?582, October.24
