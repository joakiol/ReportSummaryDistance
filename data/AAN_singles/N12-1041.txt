2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 377?381,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsIntra-Speaker Topic Modeling for Improved Multi-Party MeetingSummarization with Integrated Random WalkYun-Nung Chen and Florian MetzeSchool of Computer Science, Carnegie Mellon University5000 Forbes Avenue, Pittsburgh, PA 15213-3891, USA{yvchen,fmetze}@cs.cmu.eduAbstractThis paper proposes an improved approach to extrac-tive summarization of spoken multi-party interac-tion, in which integrated random walk is performedon a graph constructed on topical/ lexical relations.Each utterance is represented as a node of the graph,and the edges?
weights are computed from the topi-cal similarity between the utterances, evaluated us-ing probabilistic latent semantic analysis (PLSA),and from word overlap.
We model intra-speakertopics by partially sharing the topics from the samespeaker in the graph.
In this paper, we perform ex-periments on automatically and manually generatedtranscripts.
For automatic transcripts, our resultsshow that intra-speaker topic sharing and integratingtopical/ lexical relations can help include the impor-tant utterances.1 IntroductionSpeech summarization is an active and important topic ofresearch (Lee and Chen, 2005), because multimedia/ spo-ken documents are more difficult to browse than text orimage content.
While earlier work was focused primarilyon broadcast news content, recent effort has been increas-ingly directed to new domains such as lectures (Glasset al, 2007; Chen et al, 2011) and multi-party interac-tion (Banerjee and Rudnicky, 2008; Liu and Liu, 2010).We describe experiments on multi-party interaction foundin meeting recordings, performing extractive summariza-tion (Liu et al, 2010) on transcripts generated by auto-matic speech recognition (ASR) and human annotators.Graph-based methods for computing lexical centralityas importance to extract summaries (Erkan and Radev,2004) have been investigated in the context of text sum-marization.
Some works focus on maximizing cover-age of summaries using the objective function (Gillick,2011).
Speech summarization carries intrinsic difficul-ties due to the presence of recognition errors, sponta-neous speech effect, and lack of segmentation.
A gen-eral approach has been found very successful (Furui etal., 2004), in which each utterance in the document d,U = t1t2...ti...tn, represented as a sequence of terms ti,is given an importance score:I(U, d) =1nn?i=1[?1s(ti, d) + ?2l(ti) (1)+ ?3c(ti) + ?4g(ti)] + ?5b(U),where s(ti, d), l(ti), c(ti), g(ti) are respectively somestatistical measure (such as TF-IDF), linguistic measure(e.g., different part-of-speech tags are given differentweights), confidence score and N-gram score for the termti, and b(U) is calculated from the grammatical structureof the utterance U , and ?1, ?2, ?3, ?4 and ?5 are weight-ing parameters.
For each document, the utterances to beused in the summary are then selected based on this score.In recent work, Chen (2011) proposed a graphicalstructure to rescore I(U, d), which can model the topicalcoherence between utterances using random walk withindocuments.
Similarly, we now use a graph-based ap-proach to consider the importance of terms and the simi-larity between utterances, where topical and lexical simi-larity are integrated in the graph, so that utterances topi-cally or lexically similar to more important utterances aregiven higher scores.
Using topical similarity can com-pensate the negative effects of recognition errors on sim-ilarity evaluated on word overlap to some extent.
In addi-tion, this paper proposes an approach of modeling intra-speaker topics in the graph to improve meeting summa-rization (Garg et al, 2009) using information from multi-party interaction, which is not available in lectures orbroadcast news.2 Proposed ApproachWe apply word stemming and noise utterance filtering forutterances in all meetings.
Then we construct a graph tocompute the importance of all utterances.377U1U2U3 U4U5U6pt(4, 3)pt(3, 4)A4t = {U3, U5, U6}B4t= {U1, U3, U6}Figure 1: A simplified example of the graph considered.We formulate the utterance selection problem as ran-dom walk on a directed graph, in which each utteranceis a node and the edges between them are weighted bytopical and lexical similarity.
The basic idea is that anutterance similar to more important utterances should bemore important (Chen et al, 2011).
We formulate twotypes of directed edge, topical edges and lexical edges,which are weighted by topical and lexical similarity re-spectively.
We then keep only the top N outgoing edgeswith the highest weights from each node, while considerincoming edges to each node for importance propagationin the graph.
A simplified example for such a graph withtopical edges is in Figure 1, in which Ati and Bti are thesets of neighbors of the node Ui connected respectivelyby outgoing and incoming topical edges.2.1 Parameters from PLSAProbabilistic latent semantic analysis (PLSA) (Hofmann,1999) has been widely used to analyze the semanticsof documents based on a set of latent topics.
Givena set of documents {dj , j = 1, 2, ..., J} and all terms{ti, i = 1, 2, ...,M} they include, PLSA uses a set oflatent topic variables, {Tk, k = 1, 2, ...,K}, to charac-terize the ?term-document?
co-occurrence relationships.The PLSA model can be optimized with EM algorithmby maximizing a likelihood function.
We utilize two pa-rameters from PLSA, latent topic significance (LTS) andlatent topic entropy (LTE) (Kong and Lee, 2011) in thepaper.Latent Topic Significance (LTS) for a given term tiwith respect to a topic Tk can be defined asLTSti(Tk) =?dj?Dn(ti, dj)P (Tk | dj)?dj?Dn(ti, dj)[1?
P (Tk | dj)], (2)where n(ti, dj) is the occurrence count of term ti in adocument dj .
Thus, a higher LTSti(Tk) indicates theterm ti is more significant for the latent topic Tk.Latent Topic Entropy (LTE), for a given term ti can becalculated from the topic distribution P (Tk | ti):LTE(ti) = ?K?k=1P (Tk | ti) logP (Tk | ti), (3)where the topic distribution P (Tk | ti) can be estimatedfrom PLSA.
LTE(ti) is a measure of how the term ti isfocused on a few topics, so a lower latent topic entropyimplies the term carries more topical information.2.2 Statistical Measures of a TermThe statistical measure of a term ti, s(ti, d) in (1) can bedefined in terms of LTE(ti) in (3) ass(ti, d) =?
?
n(ti, d)LTE(ti), (4)where ?
is a scaling factor such that 0 ?
s(ti, d) ?
1; thescore s(ti, d) is inversely proportion to the latent topicentropy LTE(ti).
Some works (Kong and Lee, 2011)showed that the use in (1) of s(ti, d) as defined in (4) out-performed the very successful ?significance score?
(Furuiet al, 2004) in speech summarization; then, we use it asthe baseline.2.3 Similarity between UtterancesWithin a document d, we can first compute the probabil-ity that the topic Tk is addressed by an utterance Ui:P (Tk | Ui) =?t?Uin(t, Ui)P (Tk | t)?t?Uin(t, Ui).
(5)Then an asymmetric topical similarity TopicSim(Ui, Uj)for utterances Ui to Uj (with direction Ui ?
Uj) canbe defined by accumulating LTSt(Tk) in (2) weighted byP (Tk | Ui) for all terms t in Uj over all latent topics:TopicSim(Ui, Uj) =?t?UjK?k=1LTSt(Tk)P (Tk | Ui),(6)where the idea is very similar to the generative probabilityin IR.
We call it generative significance of Ui given Uj .Within a document d, the lexical similarity is the mea-sure of word overlap between the utterance Ui and Uj .We compute LexSim(Ui, Uj) as the cosine similarity be-tween two TF-IDF vectors from Ui and Uj like well-known LexRank (Erkan and Radev, 2004).
Note thatLexSim(Ui, Uj) = LexSim(Uj , Ui)2.4 Intra-Speaker Topic ModelingWe assume a single speaker usually focuses on similartopics, so if an utterance is important, the scores of theutterances from the same speaker should be increased.Then we increase the similarity between the utterancesfrom the same speaker to share the topics:TopicSim?k(Ui, Uj) =??????
?TopicSim(Ui, Uj)1+w, if Ui ?
Sk and Uj ?
SkTopicSim(Ui, Uj)1?w, otherwise(7)378where Sk is the set including all utterances from speakerk, and w is a weighting parameter for modeling thespeaker relation, which means the level of coherence oftopics within a single speaker.
Here the topics from thesame speaker can partially shared.2.5 Integrated Random WalkWe modify random walk (Hsu and Kennedy, 2007; Chenet al, 2011) to integrate two types of similarity over thegraph obtained above.
v(i) is the new score for node Ui,which is the interpolation of three scores, the normalizedinitial importance r(i) for node Ui and the score con-tributed by all neighboring nodes Uj of node Ui weightedby pt(j, i) and pl(j, i),v(i) = (1?
??
?
)r(i) (8)+ ?
?Uj?Btipt(j, i)v(j) + ?
?Uj?Blipl(j, i)v(j),where ?
and ?
are the interpolation weights, Bti is the setof neighbors connected to node Ui via topical incomingedges,Bli is the set of neighbors connected to node Ui vialexical incoming edges, andr(i) =I(Ui, d)?UjI(Uj , d)(9)is normalized importance scores of utterance Ui, I(Ui, d)in (1).
We normalize topical similarity by the total sim-ilarity summed over the set of outgoing edges, to pro-duce the weight pt(j, i) for the edge from Uj to Ui on thegraph.
Similarly, pl(j, i) is normalized in lexical edges.
(8) can be iteratively solved with the approach verysimilar to that for the PageRank problem (Page et al,1998).
Let v = [v(i), i = 1, 2, ..., L]T and r = [r(i), i =1, 2, ..., L]T be the column vectors for v(i) and r(i) for allutterances in the document, where L is the total numberof utterances in the document d and T represents trans-pose.
(8) then has a vector form below,v = (1?
??
?
)r+ ?Ptv + ?Plv (10)=((1?
??
?
)reT + ?Pt + ?Pl)v = P?v,where Pt and Pl areL?Lmatrices of pt(j, i) and pl(j, i)respectively, and e = [1, 1, ..., 1]T. It has been shownthat the solution v of (10) is the dominant eigenvectorof P?
(Langville and Meyer, 2006), or the eigenvectorcorresponding to the largest absolute eigenvalue of P?.The solution v(i) can then be obtained.3 Experiments3.1 CorpusThe corpus used in this research consists of a sequence ofnaturally occuring meetings, which featured largely over-lapping participant sets and topics of discussion.
For eachmeeting, SmartNotes (Banerjee and Rudnicky, 2008) wasused to record both the audio from each participant aswell as his notes.
The meetings were transcribed bothmanually and using a speech recognizer; the word errorrate is around 44%.
In this paper we use 10 meetings heldfrom April to June of 2006.
On average each meeting hadabout 28 minutes of speech.
Across these 10 meetingsthere were 6 unique participants; each meeting featuredbetween 2 and 4 of these participants (average: 3.7).
Thetotal number of utterances is 9837 across 10 meetings.
Inthis paper, we separate dev set (2 meetings) and test set(8 meetings).
Dev set is used to tune the parameters suchas ?, ?, w.The reference summaries are given by the set of ?note-worthy utterances?
: two annotators manually labelled thedegree (three levels) of ?noteworthiness?
for each utter-ance, and we extract the utterances with the top level of?noteworthiness?
to form the summary of each meeting.In the following experiments, for each meeting, we ex-tract the top 30% number of terms as the summary.3.2 Evaluation MetricsAutomated evaluation utilizes the standard DUC eval-uation metric ROUGE (Lin, 2004) which representsrecall over various n-grams statistics from a system-generated summary against a set of human generated peersummaries.
F-measures for ROUGE-1 (unigram) andROUGE-L (longest common subsequence) can be eval-uated in exactly the same way, which are used in the fol-lowing results.3.3 ResultsTable 1 shows the performance achieved by all proposedapproaches.
In these experiments, the damping factor,(1 ?
?
?
?)
in (8), is empirically set to 0.1.
Row (a)is the baseline, which use LTE-based statistical measureto compute the importance of utterances I(U, d).
Row(b) is the result only considering lexical similarity; row(c) only uses topical similarity.
Row (d) are the re-sults additionally including speaker information such asTopicSim?
(Ui, Uj).
Row (e) is the result performed byintegrated random walk (with ?
6= 0 and ?
6= 0) usingparameters that have been optimized on the dev set.3.3.1 Graph-Based ApproachWe can see the performance after graph-based re-computation, shown in rows (b) and (c), is significantlybetter than the baseline, shown in row (a), for both ASRand manual transcripts.
For ASR transcripts, topical sim-ilarity and lexical similarity give similar results.
For man-ual transcripts, topical similarity performs slightly worsethan lexical similarity, because manual transcripts don?tcontain the recognition errors, and therefore word overlapcan accurately measure the similarity between two utter-379F-measureASR Transcripts Manual TranscriptsROUGE-1 ROUGE-L ROUGE-1 ROUGE-L(a) Baseline: LTE 46.816 46.256 44.987 44.162(b) LexSim (?
= 0, ?
= 0.9) 48.940 48.504 46.540 45.858(c) TopicSim (?
= 0.9, ?
= 0) 49.058 48.436 46.199 45.392(d) Intra-Speaker TopicSim 49.212 48.351 47.104 46.299(e) Integrated Random Walk 49.792 49.156 46.714 46.064MAX RI +6.357 +6.269 +4.706 +4.839Table 1: Maximum relative improvement (RI) with respect to the baseline for all proposed approaches (%).4848.54949.5500.9 0.8 0.7 0.6 0.5 0.4 0.3 0.2 0.1 0.00.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9ROUGE-1ROUGE-L?
?F-measureFigure 2: The performance from integrated random walk withdifferent combination weights, ?
and ?
(?
+ ?
= 0.9 in allcases) for ASR transcripts.ances.
However, for ASR transcripts, although topicalsimilarity is not as accurate as lexical similarity, it cancompensate for recognition errors, so that the approacheshave similar performance.
Thus, graph-based approachescan significantly improve the baseline results.3.3.2 Effectiveness of Intra-Speaker ModelingWe find that modeling intra-speaker topics can improvethe performance (row (c) and row (d)), which meansspeaker information is useful to model the topical simi-larity.
The experiment shows intra-speaker modeling canhelp us include the important utterances for both ASRand manual transcripts.3.3.3 Integration of Topical and Lexical SimilarityRow (e) shows the result of the proposed approach,which integrates topical and lexical similarity into a sin-gle graph, considering two types of relations together.For ASR transcripts, row (e) is better than row (b) androw (d), which means topical similarity and lexical sim-ilarity can model different types of relations, because ofrecognition errors.
Figure 2 shows the sensitivity of thecombination weights for integrated random walk.
We cansee topical similarity and lexical similarity are additive,i.e.
they can compensate each other, improving the per-formance by integrating two types of edges in a singlegraph.
Note that the exact values of ?
and ?
do not mat-ter so much for the performance.For manual transcripts, row (e) cannot perform betterby combing two types of similarity, which means topicalsimilarity can dominate lexical similarity, since withoutrecognition errors topical similarity can model the rela-tions accurately and additionally modeling intra-speakertopics can effectively improve the performance.In addition, Banerjee and Rudnicky (2008) used su-pervised learning to detect noteworthy utterances on thesame corpus, and achieved ROGURE-1 scores of around43% for ASR, and 47% for manual transcriptions.
Ourunsupervised approach performs better, especially forASR transcripts.Note that the performance on ASR is better than onmanual transcripts.
Because a higher percentage ofrecognition errors occurs on ?unimportant?
words, thesewords tend to receive lower scores; we can then excludethe utterances with more errors, and achieve better sum-marization results.
Other recent work has also demon-strated better performance for ASR than manual tran-scripts (Chen et al, 2011; Kong and Lee, 2011).4 Conclusion and Future WorkExtensive experiments and evaluation with ROUGE met-rics showed that intra-speaker topics can be modeledin topical similarity and that integrated random walkcan combine the advantages from two types of edgesfor imperfect ASR transcripts, where we achieved morethan 6% relative improvement.
We plan to model inter-speaker topics in the graph-based approach in the future.AcknowledgementsThe first author was supported by the Institute of Edu-cation Science, U.S. Department of Education, throughGrants R305A080628 to Carnegie Mellon University.Any opinions, findings, and conclusions or recommen-dations expressed in this publication are those of the au-thors and do not necessarily reflect the views or officialpolicies, either expressed or implied of the Institute orthe U.S. Department of Education.380ReferencesBanerjee, S. and Rudnicky, A. I.
2008.
An extractive-summarizaion baseline for the automatic detection of note-worthy utterances in multi-party human-human dialog.
Proc.of SLT.Chen, Y.-N., Huang, Y., Yeh, C.-F., and Lee, L.-S. 2011.
Spo-ken lecture summarization by random walk over a graph con-structed with automatically extracted key terms.
Proc.
of In-terSpeech.Erkan, G. and D. R.
Radev., D. R. 2004.
LexRank: Graph-based lexical centrality as salience in text summarization.Journal of Artificial Intelligence Research, Vol.
22.Furui, S., Kikuchi, T., Shinnaka, Y., and Hori, C. 2004.Speech-to-text and speech-to-speech summarization of spon-taneous speech.
IEEE Trans.
on Speech and Audio Process-ing.Garg, N., Favre, B., Reidhammer, K., and Hakkani-Tu?r 2009.ClusterRank: A graph based method for meeting summariza-tion.
Proc.
of InterSpeech.Gillick, D. J.
2011.
The elements of automatic summarization.PhD thesis, EECS, UC Berkeley.Glass J., Hazen, T. J., Cyphers, S., Malioutov, I., Huynh, D.,and Barzilay, R. 2007.
Recent progress in the MIT spokenlecture processing project.
Proc.
of InterSpeech.Hofmann, T. 1999.
Probabilistic latent semantic indexing.Proc.
of SIGIR.Hsu, W. and Kennedy, L. 2007.
Video search reranking throughrandom walk over document-level context graph.
Proc.
ofMM.Kong, S.-Y.
and Lee, L.-S. 2011.
Semantic analysis and orga-nization of spoken documents based on parameters derivedfrom latent topics.
IEEE Trans.
on Audio, Speech and Lan-guage Processing, 19(7): 1875-1889.Langville, A. and Meyer, C. 2005.
A survey of eigenvectormethods for web information retrieval.
SIAM Review.Lee, L.-S. and Chen, B.
2005.
Spoken document understandingand organization.
IEEE Signal Processing Magazine.Lin, C. 2004.
Rouge: A package for automatic evaluationof summaries.
Proc.
of Workshop on Text SummarizationBranches Out.Liu, F. and Liu, Y.
2010.
Using spoken utterance compressionfor meeting summarization: A pilot study.
Proc.
of SLT.Liu Y., Xie, S., and Liu, F. 2010.
Using N-best recognitionoutput for extractive summarization and keyword extractionin meeting speech.
Proc.
of ICASSP.Page, L., Brin, S., Motwani, R., Winograd, T. 1998.
The pager-ank citation ranking: bringing order to the web.
TechnicalReport, Stanford Digital Library Technologies Project.381
