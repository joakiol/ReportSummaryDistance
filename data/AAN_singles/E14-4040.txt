Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 205?209,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsA New Entity Salience Task with Millions of Training ExamplesJesse DunietzComputer Science DepartmentCarnegie Mellon UniversityPittsburgh, PA 15213, USAjdunietz@cs.cmu.eduDan GillickGoogle Research1600 Amphitheatre ParkwayMountain View, CA 94043, USAdgillick@google.comAbstractAlthough many NLP systems are movingtoward entity-based processing, most stillidentify important phrases using classi-cal keyword-based approaches.
To bridgethis gap, we introduce the task of entitysalience: assigning a relevance score toeach entity in a document.
We demon-strate how a labeled corpus for the taskcan be automatically generated from a cor-pus of documents and accompanying ab-stracts.
We then show how a classifierwith features derived from a standard NLPpipeline outperforms a strong baseline by34%.
Finally, we outline initial experi-ments on further improving accuracy byleveraging background knowledge aboutthe relationships between entities.1 IntroductionInformation retrieval, summarization, and onlineadvertising rely on identifying the most importantwords and phrases in web documents.
While tradi-tional techniques treat documents as collections ofkeywords, many NLP systems are shifting towardunderstanding documents in terms of entities.
Ac-cordingly, we need new algorithms to determinethe prominence ?
the salience ?
of each entity inthe document.Toward this end, we describe three primary con-tributions.
First, we show how a labeled cor-pus for this task can be automatically constructedfrom a corpus of documents with accompanyingabstracts.
We also demonstrate the validity ofthe corpus with a manual annotation study.
Sec-ond, we train an entity salience model using fea-tures derived from a coreference resolution sys-tem.
This model significantly outperforms a base-line model based on sentence position.
Third, wesuggest how our model can be improved by lever-aging background information about the entitiesand their relationships ?
information not specifi-cally provided in the document in question.Our notion of salience is similar to that of Bogu-raev and Kenney (1997): ?discourse objects withhigh salience are the focus of attention?, inspiredby earlier work on Centering Theory (Walker etal., 1998).
Here we take a more empirical ap-proach: salient entities are those that human read-ers deem most relevant to the document.The entity salience task in particular is brieflyalluded to by Cornolti et al.
(2013), and addressedin the context of Twitter messages by Meij et.
al(2012).
It is also similar in spirit to the much morecommon keyword extraction task (Tomokiyo andHurst, 2003; Hulth, 2003).2 Generating an entity salience corpusRather than manually annotating a corpus, we au-tomatically generate salience labels for an existingcorpus of document/abstract pairs.
We derive thelabels using the assumption that the salient entitieswill be mentioned in the abstract, so we identifyand align the entities in each text.Given a document and abstract, we run a stan-dard NLP pipeline on both.
This includes a POStagger and dependency parser, comparable in ac-curacy to the current Stanford dependency parser(Klein and Manning, 2003); an NP extractor thatuses POS tags and dependency edges to identifya set of entity mentions; a coreference resolver,comparable to that of Haghighi and Klein, (2009)for clustering mentions; and an entity resolver thatlinks entities to Freebase profiles.
The entity re-solver is described in detail by Lao, et al.
(2012).We then apply a simple heuristic to align theentities in the abstract and document: Let MEbethe set of mentions of an entity E that are propernames.
An entityEAfrom the abstract aligns to anentity EDfrom the document if the syntactic headtoken of some mention in MEAmatches the headtoken of some mention in MED.
If EAaligns withmore than one document entity, we align it withthe document entity that appears earliest.In general, aligning an abstract to its source doc-ument is difficult (Daum?e III and Marcu, 2005).205We avoid most of this complexity by aligning onlyentities with at least one proper-name mention, forwhich there is little ambiguity.
Generic mentionslike CEO or state are often more ambiguous, so re-solving them would be closer to the difficult prob-lem of word sense disambiguation.Once we have entity alignments, we assumethat a document entity is salient only if it hasbeen aligned to some abstract entity.
Ideally, wewould like to induce a salience ranking over enti-ties.
Given the limitations of short abstracts, how-ever, we settle for binary classification, which stillcaptures enough salience information to be useful.2.1 The New York Times corpusOur corpus of document/abstract pairs is the anno-tated New York Times corpus (Sandhaus, 2008).It includes 1.8 million articles published betweenJanuary 1987 and June 2007; some 650,000 in-clude a summary written by one of the newspa-per?s library scientists.
We selected a subset of thesummarized articles from 2003-2007 by filteringout articles and summaries that were very short orvery long, as well as several special article types(e.g., corrections and letters to the editor).Our full labeled dataset includes 110,639 docu-ments with 2,229,728 labeled entities; about 14%are marked as salient.
For comparison, the averagesummary is about 6% of the length (in tokens) ofthe associated article.
We use the 9,719 documentsfrom 2007 as test data and the rest as training.2.2 Validating salience via manual evaluationTo validate our alignment method for inferring en-tity salience, we conducted a manual evaluation.Two expert linguists discussed the task and gen-erated a rubric, giving them a chance to calibratetheir scores.
They then independently annotatedall detected entities in 50 random documents fromour corpus (a total of 744 entities), without read-ing the accompanying abstracts.
Each entity wasassigned a salience score in {1, 2, 3, 4}, where 1 ismost salient.
We then thresholded the annotators?scores as salient/non-salient for comparison to thebinary NYT labels.Table 1 summarizes the agreement results, mea-sured by Cohen?s kappa.
The experts?
agreementis probably best described as moderate,1indicat-ing that this is a difficult, subjective task, thoughdeciding on the most salient entities (with score 1)is easier.
Even without calibrating to the induced1For comparison, word sense disambiguation tasks havereported agreement as low as ?
= 0.3 (Yong and Foo, 1999).NYT salience scores, the expert vs. NYT agree-ment is close enough to the inter-expert agreementto convince us that our induced labels are a rea-sonable if somewhat noisy proxy for the experts?definition of salience.Comparison ?
{1,2} ?
{1}A1 vs. A2 0.56 0.69A1 vs. NYT 0.36 0.48A2 vs. NYT 0.39 0.35A1 & A2 vs. NYT 0.43 0.38Table 1: Annotator agreement for entity salienceas a binary classification.
A1 and A2 are expert an-notators; NYT represents the induced labels.
Thefirst ?
column assumes annotator scores {1, 2} aresalient and {3, 4} are non-salient, while the second?
column assumes only scores of 1 are salient.3 Salience classificationWe built a regularized binary logistic regressionmodel to predict the probability that an entity issalient.
To simplify feature selection and to addsome further regularization, we used feature hash-ing (Ganchev and Dredze, 2008) to randomly mapeach feature string to an integer in [1, 100000];larger alphabet sizes yielded no improvement.
Themodel was trained with L-BGFS.3.1 Positional baselineFor news documents, it is well known that sen-tence position is a very strong indicator for rele-vance.
Thus, our baseline is a system that identi-fies an entity as salient if it is mentioned in the firstsentence of the document.
(Including the next fewsentences did not significantly change the score.
)3.2 Model featuresTable 2 describes our feature classes; each indi-vidual feature in the model is a binary indicator.Count features are bucketed by applying the func-tion f(x) = round(log(k(x + 1))), where k canbe used to control the number of buckets.
We sim-ply set k = 10 in all cases.3.3 Experimental resultsTable 3 shows experimental results on our test set.Each experiment uses a classification threshold of0.3 to determine salience, which in each case isvery close to the threshold that maximizes F1.
Forcomparison, a classifier that always predicts themajority class, non-salient, has F1= 23.9 (for thesalient class).206Feature name Description1st-loc Index of the sentence inwhich the first mention of theentity appears.head-count Number of times the headword of the entity?s first men-tion appears.mentions Conjuction of the numbersof named (Barack Obama),nominal (president), pronom-inal (he), and total mentionsof the entity.headline POS tag of each word that ap-pears in at least one mentionand also in the headline.head-lex Lowercased head word of thefirst mention.Table 2: The feature classes used by the classifier.Lines 2 and 3 serve as a comparison betweentraditional keyword counts and the mention countsderived from our coreference resolution system.Named, nominal, and pronominal mention countsclearly add significant information despite coref-erence errors.
Lines 4-8 show results when ourmodel features are incrementally added.
Each fea-ture raises accuracy, and together our simple set offeatures improves on the baseline by 34%.4 Entity centralityAll the features described above use only infor-mation available within the document.
But arti-cles are written with the assumption that the readerknows something about at least some of the enti-ties involved.
Inspired by results using Wikipediato improve keyword extraction tasks (Mihalceaand Csomai, 2007; Xu et al., 2010), we experi-mented with a simple method for including back-ground knowledge about each entity: an adapta-tion of PageRank (Page et al., 1999) to a graphof connected entities, in the spirit of Erkan andRadev?s work (2004) on summarization.Consider, for example, an article about a recentcongressional budget debate.
Although HouseSpeaker John Boehner may be mentioned justonce, we know he is likely salient because he isclosely related to other entities in the article, suchas Congress, the Republican Party, and BarackObama.
On the other hand, the Federal Emer-gency Management Agency may be mentioned re-peatedly because it happened to host a major pres-idential speech, but it is less related to the story?s# Description P R F11 Positional baseline 59.5 37.8 46.22 head-count 37.3 54.7 44.43 mentions 57.2 51.3 54.14 1st-loc 46.1 60.2 52.25 + head-count 52.6 63.4 57.56 + mentions 59.3 61.3 60.37 + headline 59.1 61.9 60.58 + head-lex 59.7 63.6 61.69 + centrality 60.5 63.5 62.0Table 3: Test set (P)recision, (R)ecall, and (F)measure of the salient class for some com-binations of features listed in Table 2.
Thecentrality feature is discussed in Section 4.key figures and less central to the article?s point.Our intuition about these relationships, mostlynot explicit in the document, can be formalized ina local PageRank computation on the entity graph.4.1 PageRank for computing centralityIn the weighted version of the PageRank algorithm(Xing and Ghorbani, 2004), a web link is con-sidered a weighted vote by the containing pagefor the landing page ?
a directed edge in a graphwhere each node is a webpage.
In place of the webgraph, we consider the graph of Freebase entitiesthat appear in the document.
The nodes are theentities, and a directed edge from E1to E2repre-sents P (E2|E1), the probability of observing E2in a document given that we have observed E1.We estimate P (E2|E1) by counting the number oftraining documents in which E1and E2co-occurand normalizing by the number of training docu-ments in which E1occurs.The nodes?
initial PageRank values act as aprior, where the uniform distribution, used in theclassic PageRank algorithm, indicates a lack ofprior knowledge.
Since we have some prior sig-nal about salience, we initialize the node values tothe normalized mention counts of the entities inthe document.
We use a damping factor d, allow-ing random jumps between nodes with probability1?
d, with the standard value d = 0.85.We implemented the iterative version ofweighted PageRank, which tends to converge inunder 10 iterations.
The centrality featuresin Table 3 are indicators for the rank orders of theconverged entity scores.
The improvement fromadding centrality features is small but statisticallysignificant at p ?
0.001.207ObamaObamaBoehnerFEMABoehnerFEMARepublicanPartyRepublicanPartyFigure 1: A graphical representation of the centrality computation on a toy example.
Circle size andarrow thickness represent node value and edge weight, respectively.
The initial node values, based onmention count, are shown on the left.
The final node values are on the right; dotted circles show theinitial sizes for comparison.
Edge weights remain constant.4.2 DiscussionWe experimented with a number of variations onthis algorithm, but none gave much meaningfulimprovement.
In particular, we tried to includethe neighbors of all entities to increase the sizeof the graph, with the values of neighbor enti-ties not in the document initialized to some smallvalue k. We set a minimum co-occurrence countfor an edge to be included, varying it from 1to 100 (where 1 results in very large graphs).We also tried using Freebase relations betweenentities (rather than raw co-occurrence counts)to determine the set of neighbors.
Finally, weexperimented with undirected graphs using un-normalized co-occurrence counts.While the ranked centrality scores look reason-able for most documents, the addition of these fea-tures does not produce a substantial improvement.One potential problem is our reliance on the entityresolver.
Because the PageRank computation linksall of a document?s entities, a single resolver errorcan significantly alter all the centrality scores.
Per-haps more importantly, the resolver is incomplete:many tail entities are not included in Freebase.Still, it seems likely that even with perfect reso-lution, entity centrality would not significantly im-prove the accuracy of our model.
The mentionsfeatures are sufficiently powerful that entity cen-trality seems to add little information to the modelbeyond what these features already provide.5 ConclusionsWe have demonstrated how a simple alignmentof entities in documents with entities in their ac-companying abstracts provides salience labels thatroughly agree with manual salience annotations.This allows us to create a large corpus ?
over100,000 labeled documents with over 2 million la-beled entities ?
that we use to train a classifier forpredicting entity salience.Our experiments show that features derivedfrom a coreference system are more robust thansimple word count features typical of a keywordextraction system.
These features combine nicelywith positional features (and a few others) to givea large improvement over a first-sentence baseline.There is likely significant room for improve-ment, especially by leveraging background infor-mation about the entities, and we have presentedsome initial experiments in that direction.
Perhapsfeatures more directly linked to Wikipedia, as inrelated work on keyword extraction, can providemore focused background information.We believe entity salience is an important taskwith many applications.
To facilitate further re-search, our automatically generated salience an-notations, along with resolved entity ids, for thesubset of the NYT corpus discussed in this paperare available here:https://code.google.com/p/nyt-salience/208ReferencesBranimir Boguraev and Christopher Kennedy.
1997.Salience-based content characterisation of text doc-uments.
In Proceedings of the ACL, volume 97,pages 2?9.Marco Cornolti, Paolo Ferragina, and MassimilianoCiaramita.
2013.
A framework for benchmarkingentity-annotation systems.
In Proceedings of the22nd international conference on World Wide Web,pages 249?260.Hal Daum?e III and Daniel Marcu.
2005.
Inductionof word and phrase alignments for automatic doc-ument summarization.
Computational Linguistics,31(4):505?530.G?unes Erkan and Dragomir R Radev.
2004.
Lexrank:Graph-based lexical centrality as salience in textsummarization.
Journal of Artificial IntelligenceResearch (JAIR), 22(1):457?479.Kuzman Ganchev and Mark Dredze.
2008.
Small sta-tistical models by random feature mixing.
In Pro-ceedings of the ACL08 HLT Workshop on MobileLanguage Processing, pages 19?20.Aria Haghighi and Dan Klein.
2009.
Simple coref-erence resolution with rich syntactic and semanticfeatures.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing: Volume 3-Volume 3, pages 1152?1161.
Asso-ciation for Computational Linguistics.Anette Hulth.
2003.
Improved automatic keyword ex-traction given more linguistic knowledge.
In Pro-ceedings of the 2003 conference on Empirical meth-ods in natural language processing, pages 216?223.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.Ni Lao, Amarnag Subramanya, Fernando Pereira, andWilliam W Cohen.
2012.
Reading the web withlearned syntactic-semantic inference rules.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1017?1026.
Association for Computational Linguis-tics.Edgar Meij, Wouter Weerkamp, and Maarten de Ri-jke.
2012.
Adding semantics to microblog posts.In Proceedings of the fifth ACM international con-ference on Web search and data mining, pages 563?572.
ACM.Rada Mihalcea and Andras Csomai.
2007.
Wikify!
:linking documents to encyclopedic knowledge.
InProceedings of the sixteenth ACM conference onConference on information and knowledge manage-ment, pages 233?242.Lawrence Page, Sergey Brin, Rajeev Motwani, andTerry Winograd.
1999.
The PageRank citationranking: Bringing order to the web.
Technical Re-port 1999-66, Stanford InfoLab.Evan Sandhaus.
2008.
The New York Times annotatedcorpus.
Linguistic Data Consortium, Philadelphia,6(12):e26752.Takashi Tomokiyo and Matthew Hurst.
2003.
Alanguage model approach to keyphrase extraction.In Proceedings of the ACL 2003 workshop onMultiword expressions: analysis, acquisition andtreatment-Volume 18, pages 33?40.Marilyn A Walker, Aravind Krishna Joshi, andEllen Friedman Prince.
1998.
Centering theory indiscourse.
Oxford University Press.Wenpu Xing and Ali Ghorbani.
2004.
Weighted page-rank algorithm.
In Communication Networks andServices Research, pages 305?314.
IEEE.Songhua Xu, Shaohui Yang, and Francis Chi-MoonLau.
2010.
Keyword extraction and headline gener-ation using novel word features.
In AAAI.Chung Yong and Shou King Foo.
1999.
A case studyon inter-annotator agreement for word sense disam-biguation.209
