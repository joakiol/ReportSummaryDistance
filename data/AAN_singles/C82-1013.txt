COLING 82, Jr. Horec~ le~ \]North-Holland Publishing Company?
Academia, 1982FORWARD AND BACKWARD REASONING IN AUTOMATIC ABSTRACTINGDanilo FUM ?
Giovanni GUIDA, Carlo TASSOIstituto di Matematica, Informatica e Sistemistica, Universita' diUdine.
Italy.o Laboratorio di Psicologia E.E., Universita' di Trieste, Italy.The paper is devoted to present a new approach to automaticabstracting which is supported by the development of SUSY, anexperimental system currently being implemented at theUniversity of Udine (Italy).
The original contribution of theresearch reported is mostly focused on the role of forwardand backward reasoning in the abstracting activity.
In thepaper the specifications and basic methodologies of SUSY areintroduced, its architecture is illustrated with particularattention to the organization of the basic algorithms, and anexample to support the novel approach proposed is described.INTRODUCTIONFor its theoretic and practical implications automatic abstracting has recentlyemerged as one of the most promising and interesting research topics in thefield of natural language studies covered by computational linguistics.artificial intelligence, and psycholinguistics.
In this paper we present thefirst results of a research project aimed at developing a new approach toautomatic abstracting which is supported by the development of SUSY (SUmmarizingSYstem), an experimental system which is currently being implemented onVAX-11/780 at the University of Udine (Italy).
The system is conceived to acceptin input a natural language text (a scientific paper in the current application)together with the user's requirements and to produce as output a summary of thespecified kind.
SUSY relies on two basic assumptions~- to ground parsing, summarizing, and generation activities mostly on thesemantics of the language, and to avoid any kind of reasoning merely based onsyntactic or structural properties which are not adequate for an intelligent andeffective summarizer;- to take strongly into account recent results of psycholinguistic research(Kintsch.
1974; Kintsch and van Dijk, 1978) as a conceptual background and a validstandpoint for designing a general purpose summarizing method.The most relevant and original features of SUSY consist, in our opinion, in theremarkable flexibility of the system which allows the user to obtain differentabstracts depending on his particular goals and needs, and in the strategiesused to summarize (i.e.. forward and backward processing) that simulate at acertain level of abstraction those utilized'by humans.8384 D. FUM, G. GUIDA and C. TASSOSPECIFICATIONSAND BASIC METHODOLOGIESIn defining SUSY's specifications we have tried to implement at a certain levelof abstraction an important human feature: the capability to generate summariesof different content and extent depending on the user's goals.
The system istherefore able to process a text following the two principles of variable-lengthprocessing and of user-taylored abstracting.
With variable-length processing wemean the Capability to generate, starting from the same text.
summaries ofdifferent length, complexity, and level of abstraction depending on the user'srequirements.
With user-taylored abstractingwe mean the capability to generates~r ies  of different content depending on the user's goals and needs.Together with the input text, SUSY can therefore receive in input the user'srequirements describing with more or less details the organization, content, andextent of the output summary.
This is done through a summary schema which can beinteractively supplied at the beginning of the session.
The user can alsoprovide the system with a text schema which is constituted by a set ofsuggestions on how the input text can be interpreted.
The text schema has atwofold motivation: to help the system in capturing from the input text only themost relevant parts, and to increase s~r i z ing  effectiveness.Turning now our attention to the methodological aspects of SUSY.
we notice that.in general, the surmnsrizing activity can be performed in two distinct andcomplementary ways.
The first one.
or meaning-based, is grounded on thecomprehension of the text to be summarized: in this case the summarizer has tocapture the most important information contained in the text.
The secondpossible way is structure-based and it does not rely on the meaning of the textbut rather on its structure: the summary is obtained by eliminating, withoutunderstanding, parts of the text (for example adjectives, relative sentences.etc.)
which a priori are considered less relevant.
Both these ways can becombined with the two basic methodologies we have conceived for the system, i.e.forward and backward processing.With the term forward processing we mean the capability to understand the wholenatural language text and to produce in output, possibly through the iterativeapplication of summarizing rules, the desired summary.
This is clearly abottom-up approach which constantly focuses on the input text.
In backwardprocessing, on the other hand.
the focus is on the s~ry  schema.
The systemworks now top-down, searching for those parts of the text that can be utilizedto build up the summary according to the specifications contained in the summaryschema.
In the SUSY sistem we have chosen to implement both forward and backwardprocessing within a meaning-based approach.SYSTEM ARCHITECTURE AND BASIC ALGORITHMSThe architecture of the system is organized in two main parts: the first one isdevoted to collect the user's requirements and suggestions and to perform apreprocessing activity on them, the second one implements the actual parsing.summarizing, and generation activities.The first par~ of the system constitutes an interactive interface centeredaround a main module called schema builder.
This module is devoted to engage a/ /FORWARD AND BACKWARD REASONING IN AUTOMATIC ABSTRACTING " .
85bounded scope dialogue with the user in order to collect his suggestions aboutthe structure and content of the texts to be su~mmrized, and his requirements onthe summary to be generated.
This information is embedded in two differentframeworks called working_ text schema and working summmar~, schema which containthe user's suggestions and requirements, respectively.
The schemas willconstitute a fundamental input for the following phases of the system operation.The working schemas are defined by the user.
under the continuous guidance ofthe schema builder, through three different activities:- choosin~ the most appropriate schema from a library of basic text and summaryschemas or from a library of working text and summary schemas which contain theschemas utilised in previous surm~arizing sessions;- tuning a selected schema by assigning (or reassigning) same parameterscontained in it;- defining a fully new (basic) schema.It is understood that working schemas are not requested to be always defined atthe same level of detail and completeness; they are allowed to embed more orless information according to the adequacy and richness of the specificationssupplied by the user.
For both text and summary schemas there exist defaultvalues to be utilized when the user is unable or unwilling to supply its ownspecifications.The second part of the system is devoted to the parsing, surmnarizing, andgeneration activities.
These are conceivedin SUSY as three sequential stepswhich conlnunicate through precisely defined data interfaces representingintermediate results of the processing.The parser constructs the internal representation of the input text on which thesummarizer will afterwards perform its activity.
The operation of this module isbased on a semantics-directed parsing algorithm which aims to supply a fullunderstanding of the input text along the following two main lines:- the text is parsed in a uniform way.
independently of any expectation thatcould be possibly made (by considering the current working schemas) about therelevance of the different parts of the text in relation with the summary to beproduced;- the parsing is performed at a generally high level of abstraction, withoutdecomposing objects into very elementary semantic primitives (Schank.
1975) butonly considering the basic attributes and relations which are necessary for thesummarizing task.The semantics directed parsing algorithm utilises two kinds of information: theelementary knowledge about words and simple constructs contained in thevocabulary, and a set of semantic rules that specify the basic properties andrelations of the elementary semantic entities which are supposed to play a rolein the application domain in which the system operates (Guida and Tasso.
1982).The internal representation constructed by the parser shares many features withthat proposed by Kintsch (Kintsch, 1974; Kintsch and van Dijk.
1978) and isconstituted by a sequence of labelled linear propositions each one conveying aunit of information.
Every proposition is composed by a predicate with one ormore arguments.
Predicates and arguments can be considered as concepts or typesto which the words in the input text (tokens) refer.
The same type may be86' D. FUM, G. GU1DA and C. TASSOinstantiated by different tokens which are therefore considered as synonlms.Arguments can be types or labels of propositions and~ in any case, they play.precise semantic roles (agent.
object, patient etc.).
Every predicate imposessome constraints (linguistic or derived from the world knowledge possessed bythe system) on the number and nature of its arguments.
The proposions areconnected to each other through shared terms in such a way to represent anactual network structure.The activity of the summarizer has been split, according to the basicmethodology illustrated in the previous section, in two sequential steps: aforward one performed by the weighter and a backward one implemented by theselector.
The weighter is devoted to organize the internal representation, whichis originally a flat and homogeneous network, into a structured framework inwhich the different levels of relevance and detail of the single propositionsare clearly defined.
This is obtained by assigning an integer weight to eachproposition in such a way to generate a weighted network called weightedrepresentation.
The weighter utilizes for its operation the working text schemaand a set of general purpose weighting rules.
The selector is devoted to prunethe ~ighted internal representation in such a way to obtain the selectedrepresentation i.e.
the internal representation of the desired sunmmry.
It takesinto account the working summary schema and operates through a set of generalpurpose selecting rules.
The pruning it performs is generally not uniform withrespect to the weights attached to the weighted representation, but it is biasedand tuned by the requirements contained in the sun~nary schema.It is easy to recognize that weighting is indeed a forward activity which mainlyfocuses on the input text.
while selecting represents a backward process whichis generally directed by the consideration of the summary to be generated.
Letus outline that the completeness and depth of the weighting and selectingactivities strongly depend on the quality and richness of the text and summaryschemas, respectively.
Generally.
these steps are not equally balanced and.
insome cases, one of themmay even be nearly void.
as text schema or summaryschema may be almost empty or even missed.
In such cases we obtain a pureforward or backward strategy.The last step of the system operation is the actual generation of naturallanguage summary that is performed by the generator.
Its activity is organisedin two phases:- retrieval from the input text of the basic linguistic elements (words.phrases, whole sentences etc.)
necessary to compose the summary;- appropriate assembly of these elements into a correct and acceptable text.In the second phase it utilizes a set of sentence models which supply the mostbasic and usual rules for constructing correct sentences in a simple and plainstyle.AN EXAMPLEOwing to space restrictions we present in this section only a short workingexample of SUSY's performance, focusing on the most relevant features of theinternal representation and of the weighting and selecting activities.FORWARD AND BACKWARD REASONING IN AUTOMATIC ABSTRACTING 8?The input text in this example is a slightly adapted version ~f the firstsentence of an article entitled "Fast Breeder Reactors" taken from Meyer (1975).
"The need to generate enormous additional amounts of electricpower while at the same time protecting the environment isone of the major social and technological problems that oursociety must solve in the next future.
"The parser maps this text into the internal representation:I.
NEED (2)2.
GENERATE ($, POWER)3.
QUANTITY OF (POWER.
LOTS)4.
MORE (3)5.
ELECTRIC (POWER)6.
WHILE (2.7)7.
PROTECT ($0ENVIRO~ENT)8.
PROBL~ (I)9.
BIG (8)10.
SOCIAL (8)11.
TECHNOLOGICAL (8)12.
MSOLVE (SOCIETY.8)13.
OUR (SOCIETY)14.
TIME OF (12,FUTURE)15.
NEXT (FUTURE)The internal representation is then passed to the weighter in order to attach.following the suggestions contained in the text schema, an integer weigth toeach proposition.
As a result the weighted representation is obtained, which isgraphically expressed as a network:IIiOQ ~ ~ 2 ~ 3 ~-- 4We mention here the three most relevant rules applied by the weighter togenerate this network:W.RULEI.
IF a proposition i is referred to by a differentproposition j, THEN assign weigths w such as w(i) < w(j).W.RULE2.
IF the predicate of a proposition i is constitutedby a modifier AND (the proposition i is referred to by aproposition j OR the proposition i has among its argumentsone which has already appeared in a preceding proposition j)THEN assign weigths w such as w(j)< w(i).W.RULE3.
IF a proposition i has among its arguments onewhich has already appeared in a preceding proposition j ANDW.
RULE2 is not applied.
THEN a~sign weigths w such as ~i )<w(j).Let us note that modifiers in our approach are constituted by types that88 D. FL~, G. GUIDA md C. TASSOgrammatically can be classified as adjectives or adverbials, and types such asTIME OF, QUANTITY OF, LOCATIVE OF and so on.The weighted representation is then supplied to the selector which chooses acertain number of PrOPOsitions that will constitute the selected internalrepresentation of the stmmmry and will be passed to the generator in order toproduce the final output summary.
This choice is driven by the specificationscontained in the summary schema.In our example, through the application of the selecting rule:S.RULEI Choose the n most weighted propositions discardingthe leaves.where n is a parameter which takes into account the length of the desiredsummary (in the example, n=5), we can select the propositions that appearencircled in the network.These propositions are eventually passed to the generator which gives the finaloutput summary:"The society must solve in the future the problem of the needto generate power while protecting the environment.
"The specifications given by the user through the text schema and the summaryschema may of course activate different weighting and selecting rules- and thusgenerate different summaries.CONCLUSIONAt the end of the paper, let us mention some of the most promising researchdirections for future activity:- to develop a new parsing algorit~mwhich, taking into account the text andsummary schemas, allows the generation of a variable-depth internalrepresentation;- to implement a more advanced weighter which attaches weights not only topropositions but also to their elementary components;- to expand the knowledge representation method adopted for constructing theinternal representation into a more sophisticated language suitable to express.whenever requested, very elementary semantic primitives which allow limiteddeduction and reasoning capabilities.REFERENCESI.
GUIDA, G. and TASSO, C., ~q~l: A robust interface for natural languageperson-machine communication, International Journal of Man-MachineStudies (1982), in press.2.
KINTSCH, W., The Representation of Meaning in Memory (Lawrence ErlbaumAss., Hillsdale: N.J., 197.4).3.
KINTSCH, W. and VAN DIJK, T., Toward a model of text comprehension andproduction, PsYchological ~eview, 85 (1978) 363-394.4.
MEYER.
B..
The Organization of Prose and Its Effects on Memory(North-Holland, Amsterdmm, 1975).5.
SCHANK, R.C.
Conceptual DePendency Theory (North Holland.
Amsterdam,1975).
