In: Proceedings of CoNLL-2000 and LLL-2000, pages 91-94, Lisbon, Portugal, 2000.Inducing Syntactic Categories by Context Distribution ClusteringAlexander  ClarkSchool of Cognitive and Computing SciencesUniversity of Sussexa lexc?cogs ,  susx.
ac.
ukAbst rac tThis paper addresses the issue of the automaticinduction of syntactic ategories from unanno-tared corpora.
Previous techniques give goodresults, but fail to cope well with ambiguity orrare words.
An algorithm, context distributionclustering (CDC), is presented which can benaturally extended to handle these problems.1 In t roduct ionIn this paper I present a novel program that in-duces syntactic ategories from comparativelysmall corpora of unlabelled text, using only dis-tributional information.
There are various mo-tivations for this task, which affect the algo-rithms employed.
Many NLP systems use aset of tags, largely syntactic in motivation, thathave been selected according to various criteria.In many circumstances it would be desirable forengineering reasons to generate a larger set oftags, or a set of domain-specific tags for a par-ticular corpus.
Furthermore, the constructionof cognitive models of language acquisit ion-that will almost certainly involve some notionof syntactic ategory - requires an explanationof the acquisition of that set of syntactic ate-gories.
The amount of data used in this studyis 12 million words, which is consistent with apessimistic lower bound on the linguistic experi-ence of the infant language learner in the periodfrom 2 to 5 years of age, and has had capitalisa-tion removed as being information ot availablein that circumstance.2 Prev ious  WorkPrevious work falls into two categories.
A num-ber of researchers have obtained good resultsusing pattern recognition techniques.
Finchand Chater (1992), (1995) and Schfitze (1993),(1997) use a set of features derived from theco-occurrence statistics of common words to-gether with standard clustering and informationextraction techniques.
For sufficiently frequentwords this method produces satisfactory esults.Brown et al (1992) use a very large amountof data, and a well-founded information theo-retic model to induce large numbers of plausi-ble semantic and syntactic lusters.
Both ap-proaches have two flaws: they cannot deal wellwith ambiguity, though Schfitze addresses thisissue partially, and they do not cope well withrare words.
Since rare and ambiguous words arevery common in natural language, these limita-tions are serious.3 Context  D is t r ibut ionsWhereas earlier methods all share the same ba-sic intuition, i.e.
that similar words occur insimilar contexts, I formalise this in a slightlydifferent way: each word defines a probabilitydistribution over all contexts, namely the prob-ability of the context given the word.
If thecontext is restricted to the word on either side,I can define the context distribution to be a dis-tribution over all ordered pairs of words: theword before and the word after.
The contextdistribution of a word can be estimated fromthe observed contexts in a corpus.
We can thenmeasure the similarity of words by the simi-larity of their context distributions, using theKullback-Leibler (KL) divergence as a distancefunction.Unfortunately it is not possible to clusterbased directly on the context distributions fortwo reasons: first the data is too sparse to es-timate the context distributions adequately forany but the most frequent words, and secondlysome words which intuitively are very similar91(Schfitze's example is 'a' and 'an') have rad-ically different context distributions.
Both ofthese problems can be overcome in the normalway by using clusters: approximate the contextdistribution as being a probability distributionover ordered pairs of clusters multiplied by theconditional distributions of the words given theclusters :p(< Wl, W2 >) -= p(< Cl, C2 >)p(wlICl)p(w2\[c2)I use an iterative algorithm, starting with atrivial clustering, with each of the K clustersfilled with the kth most frequent word in thecorpus.
At each iteration, I calculate the con-text distribution of each cluster, which is theweighted average of the context distributions ofeach word in the cluster.
The distribution is cal-culated with respect o the K current clustersand a further ground cluster of all unclassifiedwords: each distribution therefore has (K + 1) 2parameters.
For every word that occurs morethan 50 times in the corpus, I calculate the con-text distribution, and then find the cluster withthe lowest KL divergence from that distribution.I then sort the words by the divergence fromthe cluster that is closest to them, and selectthe best as being the members of the clusterfor the next iteration.
This is repeated, grad-ually increasing the number of words includedat each iteration, until a high enough propor-tion has been clustered, for example 80%.
Af-ter each iteration, if the distance between twoclusters falls below a threshhold value, the clus-ters are merged, and a new cluster is formedfrom the most frequent unclustered word.
Sincethere will be zeroes in the context distributions,they are smoothed using Good-Turing smooth-ing(Good, 1953) to avoid singularities in the KLdivergence.
At this point we have a preliminaryclustering - no very rare words will be included,and some common words will also not be as-signed, because they are ambiguous or have id-iosyncratic distributional properties.4 Ambiguity and SparsenessAmbiguity can be handled naturally withinthis framework.
The context distribution p(W)of a particular ambiguous word w can bemodelled as a linear combination of the con-text distributions of the various clusters.
Wecan find the mixing coefficients by minimisingD(p(W)ll (w) a~w) oLi qi) where the are some co-efficients that sum to unity and the qi are thecontext distributions of the clusters.
A mini-mum of this function can be found using theEM algorithm(Dempster et al, 1977).
Thereare often several ocal minima - in practice thisdoes not seem to be a major problem.Note that with rare words, the KL divergencereduces to the log likelihood of the word's con-text distribution plus a constant factor.
How-ever, the observed context distributions of rarewords may be insufficient to make a definite de-termination of its cluster membership.
In thiscase, under the assumption that the word isunambiguous, which is only valid for compar-atively rare words, we can use Bayes's rule tocalculate the posterior probability that it is ineach class, using as a prior probability the dis-tribution of rare words in each class.
This in-corporates the fact that rare words are muchmore likely to be adjectives or nouns than, forexample, pronouns.5 ResultsI used 12 million words of the British Na-tional Corpus as training data, and ran this al-gorithm with various numbers of clusters (77,100 and 150).
All of the results in this paperare produced with 77 clusters corresponding tothe number of tags in the CLAWS tagset usedto tag the BNC, plus a distinguished sentenceboundary token.
In each case, the clusters in-duced contained accurate classes correspondingto the major syntactic categories, and varioussubgroups of them such as prepositional verbs,first names, last names and so on.
Appendix Ashows the five most frequent words in a cluster-ing with 77 clusters.
In general, as can be seen,the clusters correspond to traditional syntacticclasses.
There are a few errors - notably, theright bracket is classified with adverbial parti-cles like "UP".For each word w, I then calculated the opti-mal coefficents c~ w).
Table 1 shows some sam-ple ambiguous words, together with the clusterswith largest values of c~ i.
Each cluster is repre-sented by the most frequent member of the clus-ter.
Note that "US" is a proper noun cluster.As there is more than one common noun clus-ter, for many unambiguous nouns the optimumis a mixture of the various classes.92Word ClustersROSEVANMAYUSHERTHISCAME CHARLES GROUPJOHN TIME GROUPWILL US JOHNYOU US NEWTHE YOUTHE IT LASTTable 1: Ambiguous words.
For each word, theclusters that have the highest a are shown, ifa > 0.01.ModelFreq1 0.66 0.212 0.64 0.273 0.68 0.365 0.69 0.4010 0.72 0.5020 0.73 0.61CDC Brown CDC BrownNN1 NN1 A J0 A J00.77 0.410.77 0.580.82 0.730.83 0.810.92 0.940.91 0.94Table 2: Accuracy of classification ofrare wordswith tags NN1 (common oun) and A J0 (adjec-tive).Table 2 shows the accuracy of cluster assign-ment for rare words.
For two CLAWS tags, A J0(adjective) and NNl(singular common noun)that occur frequently among rare words in thecorpus, I selected all of the words that oc-curred n times in the corpus, and at least halfthe time had that CLAWS tag.
I then testedthe accuracy of my assignment algorithm bymarking it as correct if it assigned the wordto a 'plausible' cluster - for A J0, either of theclusters "NEW" or "IMPORTANT", and forNN1, one of the clusters "TIME", "PEOPLE","WORLD", "GROUP" or "FACT".
I did thisfor n in {1, 2, 3, 5, 10, 20}.
I proceeded similarlyfor the Brown clustering algorithm, selectingtwo clusters for NN1 and four for A J0.
This canonly be approximate, since the choice of accept-able clusters is rather arbitrary, and the BNCtags are not perfectly accurate, but the resultsare quite clear; for words that occur 5 times orless the CDC algorithm is clearly more accurate.Evaluation is in general difficult with unsu-pervised learning algorithms.
Previous authorshave relied on both informal evaluations of theplausibility of the classes produced, and moreformal statistical methods.
Comparison againstexisting tag-sets is not meaningful - one set ofTest set 1 2 3 4CLAWS 411 301 478 413Brown et al 380 252 444 369CDC 372 255 427 354Mean395354346Table 3: Perplexities of class tri-gram modelson 4 test sets of 100,000 words, together withgeometric mean.tags chosen by linguists would score very badlyagainst another without his implying any faultas there is no 'gold standard'.
I therefore choseto use an objective statistical measure, the per-plexity of a very simple finite state model, tocompare the tags generated with this cluster-ing technique against he BNC tags, which usesthe CLAWS-4 tag set (Leech et al, 1994) whichhad 76 tags.
I tagged 12 million words of BNCtext with the 77 tags, assigning each word tothe cluster with the highest a posteriori proba-bility given its prior cluster distribution and itscontext.I then trained 2nd-order Markov models(equivalently class trigram models) on the orig-inal BNC tags, on the outputs from my algo-rithm (CDC), and for comparision on the out-put from the Brown algorithm.
The perplexitieson held-out data are shown in table 3.
As canbe seen, the perplexity is lower with the modeltrained on data tagged with the new algorithm.This does not imply that the new tagset is bet-ter; it merely shows that it is capturing statisti-cal significant generalisations.
In absolute termsthe perplexities are rather high; I deliberatelychose a rather crude model without backing offand only the minimum amount of smoothing,which I felt might sharpen the contrast.6 Conc lus ionThe work of Chater and Finch can be seen assimilar to the work presented here given an in-dependence assumption.
We can model the con-text distribution as being the product of inde-pendent distributions for each relative position;in this case the KL divergence is the sum ofthe divergences for each independent distribu-tion.
This independence assumption is mostclearly false when the word is ambiguous; thisperhaps explains the poor performance of thesealgorithms with ambiguous words.
The newalgorithm currently does not use information93about the orthography of the word, an impor-tant source of information.
In future work, I willintegrate this with a morphology-learning pro-gram.
I am currently applying this approachto the induction of phrase structure rules, andpreliminary experiments have shown encourag-ing results.In summary, the new method avoids the limi-tations of other approaches, and is better suitedto integration into a complete unsupervised lan-guage acquisition system.ReferencesPeter F. Brown, Vincent J. Della Pietra, Peter V.de Souza, Jenifer C. Lai, and Robert Mercer.1992.
Class-based n-gram models of natural an-guage.
Computational Linguistics, 18:467-479.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via theEM algorithm.
Journal o/ the Royal StatisticalSociety Series B, 39:1-38.S.
Finch and N. Chater.
1992.
Bootstrapping syn-tactic categories.
In Proceedings o/ the l~th An-nual Meeting of the Cognitive Science Society,pages 820-825.S.
Finch, N. Chater, and Redington M. 1995.
Ac-quiring syntactic information from distributionalstatistics.
In Joseph P. Levy, Dimitrios Bairak-taris, John A. Bullinaria, and Paul Cairns, edi-tors, Connectionist Models o/Memory and Lan-guage.
UCL Press.I.
J.
Good.
1953.
The population frequencies ofspecies and the estimation of population parame-ters.
Biometrika, 40:237-264.G.
Leech, R. Garside, and M Bryant.
1994.CLAWS4: the tagging of the British NationalCorpus.
In Proceedings o/the 15th InternationalCon/erence on Computational Linguistics, pages622-628.Hinrich Schfitze.
1993.
Part of speech inductionfrom scratch.
In Proceedings o/ the 31st an-nual meeting o/ the Association /or Computa-tional Linguistics, pages 251-258.Hinrich Schfitze.
1997.
Ambiguity Resolution inLanguage Learning.
CSLI Publications.A C lus tersHere are the five most ~equent words in each of the77 clusters, one cluster per line except where indi-cated with a double slash \ \THE A HIS THIS ANPEOPLE WORK LIFE RIGHT ENDOF IN FOR 0N WITH \\ , ~MDASH ( : ;NEW OTHER FIRST OWN G00D~SENTENCE \\ .
?
!AND AS 0R UNTIL SUCHuASNOT BEEN N'T $0 0NLYIS WAS HAD HAS DIDMADE USED FOUND LEFT PUT0NE ALL MORE S0ME TWOTIME WAY YEAR DAY MAN \\ T0WORLD GOVERNMENT PARTY FAMILY WESTBE HAVE D0 MAKE GETHE I THEY SHE WEUS BRITAIN LONDON GOD LABOURBUT WHEN IF WHERE BECAUSE) UP 0UTBACK DOWNWILL WOULD CAN C0ULD MAYUSE HELP FORM CHANGE SUPPORTTHAT BEFOREABOVE 0UTSIDE BEL0WIT EVERYBODY GINAGROUP NUMBER SYSTEM 0FFICE CENTREY0U THEM HIM ME THEMSELVES~BQU0 \\ ~EQU0 \\ ARE WERE \\ 'S 'CHARLES MARK PHILIP HENRY MARYWHAT HOW WHY HAVING MAKINGIMPORTANT POSSIBLE CLEAR HARD CLOSEWHICH WH0CAME WENT LOOKED SEEMED BEGANJOHN SIR DAVID ST DEYEARS PERuCENT DAYS TIMES MONTHSGOING ABLE LOOKING TRYING COMINGTHOUGHT FELT KNEW DECIDED HOPESEE SAY FEEL MEAN REMEMBERSAID SAYS WROTE EXPLAINED REPLIEDGO COME TRY CONTINUE APPEAR \\ THEREL00K RUN LIVE MOVE TALKSUCH USING PROVIDING DEVELOPING WINNINGT00K TOLD SAW GAVE MAKESHOWEVER 0FuCOURSE FORuEXAMPLE INDEEDPART S0RT THINKING LACK NONESOMETHING ANYTHING SOMEONE EVERYTHINGMR MRS DR HONG MR.NEED NEEDS SEEM ATTEMPT OPPORTUNITYWANT WANTED TRIED WISH WANTSBASED RESPONSIBLE COMPARED INTERESTEDTHAN \\ LAST NEXT GOLDEN FT-SE \\ THOSETHINK BELIEVE SUPPOSE INSIST RECKONKNOWUNDERSTAND REALISELATER AG0 EARLIER THEREAFTERBETTER WORSE LONGER BIGGER STRONGERaHELLIP ..ASKED LIKED WATCHED SMILED INVITED'M AM \\ 'DFACT IMPRESSION ASSUMPTION IMPLICATIONNOTHING NOWHERE RISENBEC0ME \\ ENOUGH \\ FAR INFINITELY'LL \\ 'RE \\ 'VE \\ CA W0 AICOPE DEPEND C0NCENTRATE SUCCEED C0MPETERO HVK AMENKLERK CLOWES HOWE C0LI GAULLENEZ KHMER94
