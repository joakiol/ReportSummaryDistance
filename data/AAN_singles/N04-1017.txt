Lattice-Based Search for Spoken Utterance RetrievalMurat SaraclarAT&T Labs ?
Research180 Park Ave. Florham Park, NJ 07932murat@research.att.comRichard SproatUniversity of Illinois at Urbana-ChampaignUrbana, IL 61801rws@uiuc.eduAbstractRecent work on spoken document retrieval hassuggested that it is adequate to take the single-best output of ASR, and perform text retrievalon this output.
This is reasonable enough forthe task of retrieving broadcast news stories,where word error rates are relatively low, andthe stories are long enough to contain muchredundancy.
But it is patently not reasonableif one?s task is to retrieve a short snippet ofspeech in a domain where WER?s can be ashigh as 50%; such would be the situation withteleconference speech, where one?s task is tofind if and when a participant uttered a certainphrase.In this paper we propose an indexing proce-dure for spoken utterance retrieval that workson lattices rather than just single-best text.
Wedemonstrate that this procedure can improve Fscores by over five points compared to single-best retrieval on tasks with poor WER and lowredundancy.
The representation is flexible sothat we can represent both word lattices, aswell as phone lattices, the latter being impor-tant for improving performance when search-ing for phrases containing OOV words.1 IntroductionAutomatic systems for indexing, archiving, searching andbrowsing of large amounts of spoken communicationshave become a reality in the last decade.
Most such sys-tems use an automatic speech recognition (ASR) compo-nent to convert speech to text which is then used as aninput to a standard text based information retrieval (IR)component.
This strategy works reasonably well whenspeech recognition output is mostly correct or the docu-ments are long enough so that some occurrences of thequery terms are recognized correctly.Most of the research has concentrated on retrieval ofBroadcast News type of spoken documents where speechis relatively clean and the documents are relatively long.In addition it is possible to find large amounts of text withsimilar content in order to build better language modelsand enhance retrieval through use of similar documents.We are interested in extending this to telephone con-versations and teleconferences.
Our task is locating oc-currences of a query in spoken communications to aidbrowsing.
This is not exactly spoken document retrieval.In fact, it is more similar to word spotting.
Each docu-ment is a short segment of audio.Although reasonable retrieval performance can be ob-tained using the best ASR hypothesis for tasks withmoderate (?
20%) word error rates, tasks with higher(40?
50%) word error rates require use of multiple ASRhypotheses.
Use of ASR lattices makes the system morerobust to recognition errors.Almost all ASR systems have a closed vocabulary.This restriction comes from run-time requirements aswell as the finite amount of data used for training thelanguage models of the ASR systems.
Typically therecognition vocabulary is taken to be the words appear-ing in the language model training corpus.
Sometimesthe vocabulary is further reduced to only include themost frequent words in the corpus.
The words that arenot in this closed vocabulary ?
the out of vocabulary(OOV) words ?
will not be recognized by the ASR sys-tem, contributing to recognition errors.
The effects ofOOV words in spoken document retrieval are discussedby Woodland et al (2000).
Using phonetic search helpsretrieve OOV words.This paper is organized as follows.
In Section 2 wegive an overview of related work, focusing on methodsdealing with speech recognition errors and OOV queries.We present the methods used in this study in Section 3.Experimental setup and results are given in Section 4.
Fi-nally, our conclusions are presented in Section 5.2 Related WorkThere are commercial systems including Nexidia/Fast-Talk (www.nexidia.com), Virage/AudioLogger(www.virage.com), Convera (www.convera.com)as well as research systems like AT&T DVL (Cox etal., 1998), AT&T ScanMail (Hirschberg et al, 2001),BBN Rough?n?Ready (Makhoul et al, 2000), CMUInformedia (www.informedia.cs.cmu.edu),SpeechBot (www.speechbot.com), among others.Also between 1997 and 2000, the Test REtrieval Con-ference (TREC) had a spoken document retrieval (SDR)track with many participants (Garofolo et al, 2000).NIST TREC-9 SDR Web Site (2000) states that:The results of the TREC-9 2000 SDR eval-uation presented at TREC on November 14,2000 showed that retrieval performance forsites on their own recognizer transcripts wasvirtually the same as their performance on thehuman reference transcripts.
Therefore, re-trieval of excerpts from broadcast news usingautomatic speech recognition for transcriptionwas deemed to be a solved problem - even withword error rates of 30%.PhD Theses written on this topic include James (1995),Wechsler (1998), Siegler (1999) and Ng (2000).Jones et al (1996) describe a system that com-bines a large vocabulary continuous speech recognition(LVCSR) system and a phone-lattice word spotter (WS)for retrieval of voice and video mail messages (Brownet al, 1996).
Witbrock and Hauptmann (1997) presenta system where a phonetic transcript is obtained fromthe word transcript and retrieval is performed usingboth word and phone indices.
Wechsler et al (1998)present new techniques including a new method todetect occurrences of query features, a new methodto estimate occurrence probabilities, a collection-wideprobability re-estimation technique and feature lengthweighting.
Srinivasan and Petkovic (2000) introduce amethod for phonetic retrieval based on the probabilis-tic formulation of term weighting using phone confu-sion data.
Amir et al (2001) use indexing based on con-fusable phone groups and a Bayesian phonetic edit dis-tance for phonetic speech retrieval.
Logan et al (2002)compare three indexing methods based on words,syllable-like particles, and phonemes to study theproblem of OOV queries in audio indexing systems.Logan and Van Thong (2002) give an alternate approachto the OOV query problem by expanding query wordsinto in-vocabulary phrases while taking acoustic confus-ability and language model scores into account.Of the previous work, the most similar approach to theone proposed here is that of Jones et al (1996), in thatthey used phone lattices to aid in word spotting, in ad-dition to single-best output from LVCSR.
Our proposalmight be thought of as a generalization of their approachin that we use lattices as the sole representation overwhich retrieval is performed.
We believe that lattices area more natural representation for retrieval in cases wherethere is a high degree of uncertainty about what was said,which is typically the case in LVCSR systems for con-versational speech.
We feel that our results, presentedbelow, bear out this belief.
Also novel in our approach isthe use of indexed lattices allowing for efficient retrieval.As we note below, in the limit where one is using one-bestoutput, the indexed lattices reduce to the normal invertedindex used in text retrieval.3 MethodsIn this section we describe the overall structure of oursystem and give details of the techniques used in ourinvestigations.
The system consists of three main com-ponents.
First, the ASR component is used to convertspeech into a lattice representation, together with timinginformation.
Second, this representation is indexed forefficient retrieval.
These two steps are performed off-line.Finally, when the user enters a query the index is searchedand matching audio segments are returned.3.1 Automatic Speech RecognitionWe use a state-of-the-art HMM based large vocabularycontinuous speech recognition (LVCSR) system.
Theacoustic models consist of decision tree state clusteredtriphones and the output distributions are mixtures ofGaussians.
The language models are pruned backoff tri-gram models.
The pronunciation dictionaries contain fewalternative pronunciations.
Pronunciations that are notin our baseline pronunciation dictionary (including OOVquery words) are generated using a text-to-speech (TTS)frontend.
The TTS frontend can produce multiple pro-nunciations.
The ASR systems used in this study aresingle pass systems.
The recognition networks are rep-resented as weighted finite state machines (FSMs).The output of the ASR system is also represented as anFSM and may be in the form of a best hypothesis stringor a lattice of alternate hypotheses.
The labels on the arcsof the FSM may be words or phones, and the conversionbetween the two can easily be done using FSM composi-tion.
The costs on the arcs are negative log likelihoods.Additionally, timing information can also be present inthe output.3.2 Lattice Indexing and RetrievalIn the case of lattices, we store a set of indices, one foreach arc label (word or phone) l, that records the lat-tice number L[a], input-state k[a] of each arc a labeledwith l in each lattice, along with the probability massf(k[a]) leading to that state, the probability of the arcitself p(a|k[a]) and an index for the next state.
To re-trieve a single label from a set of lattices representing aspeech corpus one simply retrieves all arcs in each latticefrom the label index.
The lattices are first normalized byweight pushing (Mohri et al, 2002) so that the probabil-ity of the set of all paths leading from the arc to the finalstate is 1.
After weight pushing, for a given arc a, theprobability of the set of all paths containing that arc isgiven byp(a) =?pi?L:a?pip(pi) = f(k[a])p(a|k[a])namely the probability of all paths leading into that arc,multiplied by the probability of the arc itself.
For a latticeL we construct a ?count?
C(l|L) for a given label l usingthe information stored in the index I(l) as follows,C(l|L) =?pi?Lp(pi)C(l|pi)=?pi?L(p(pi)?a?pi?
(a, l))=?a?L(?
(a, l)?pi?L:a?pip(pi))=?a?I(l):L[a]=Lp(a)=?a?I(l):L[a]=Lf(k[a])p(a|k[a])where C(l|pi) is the number of times l is seen on path piand ?
(a, l) is 1 if arc a has the label l and 0 otherwise.
Re-trieval can be thresholded so that matches below a certaincount are not returned.To search a multilabel expression (e.g.
a multi-word phrase) w1w2 .
.
.
wn we seek on each label inthe expression, and then for each (wi, wi+1) join theoutput states of wi with the matching input states ofwi+1; in this way we retrieve just those path seg-ments in each lattice that match the entire multi-labelexpression.
The probability of each match is de-fined as f(k[a1])p(a1|k[a1])p(a2|k[a2]) .
.
.
p(an|k[an]),where p(ai|k[ai]) is the probability of the ith arc in theexpression starting in arc a1.
The total ?count?
for thelattice is computed as defined above.Note that in the limit case where each lattice is an un-weighted single path ?
i.e.
a string of labels ?
the abovescheme reduces to a standard inverted index.The count C(l|L) can be interpreted as a lattice-basedconfidence measure.
Although it may be possible to usemore sophisticated confidence measures, use of (poste-rior) probabilities allows for a simple factorization whichmakes indexing efficient.3.3 Indexing Using Sub-word UnitsIn order to deal with queries that contain OOV words weinvestigate the use of sub-word units for indexing.
In thisstudy we use phones as the sub-word units.
There are twomethods for obtaining phonetic representation of an inpututterance.1.
Phone recognition using an ASR system whererecognition units are phones.
This is achieved byusing a phone level language model instead of theword level language model used in the baseline ASRsystem.2.
Converting the word level representation of the ut-terance into a phone level representation.
This isachieved by using the baseline ASR system and re-placing each word in the output by its pronuncia-tion(s) in terms of phones.Both methods have their shortcomings.
Phone recogni-tion is known to be less accurate than word recognition.On the other hand, the second method can only generatephone strings that are substrings of the pronunciations ofin-vocabulary word strings.
An alternative is to use hy-brid language models used for OOV word detection (Yaz-gan and Saraclar, 2004).For retrieval, each query word is converted into phonestring(s) by using its pronunciation(s).
The phone indexcan then be searched for each phone string.
Note that thisapproach will generate many false alarms, particularly forshort query words, which are likely to be substrings oflonger words.
In order to control for this a bound on min-imum pronunciation length can be utilized.
Since mostshort words are in vocabulary this bound has little effecton recall.3.4 Using Both Word and Sub-word IndicesGiven a word index and a sub-word index, it is possible toimprove the retrieval performance of the system by usingboth indices.
There are many strategies for doing this.1.
combination:Search both the word index and the sub-word index,combine the results.2.
vocabulary cascade:Search the word index for in-vocabulary queries,search the sub-word index for OOV queries.3.
search cascade:Search the word index,if no result is returned search the sub-word index.In the first case, if the indices are obtained from ASRbest hypotheses, then the result combination is a simpleunion of the separate sets of results.
However, if indicesare obtained from lattices, then in addition to taking aunion of results, retrieval can be done using a combinedscore.
Given a query q, let Cw(q) and Cp(q) be the latticecounts obtained from the word index and the phone indexrespectively.
We also define the normalized lattice countfor the phone index asCnormp (q) = (Cp(q))1|pron(q)|where |pron(q)| is the length of the pronunciation ofquery q.
We then define the combined score to beCwp(q) = Cw(q) + ?Cnormp (q)where ?
is an empirically determined scaling factor.In the other cases, instead of using two different thresh-olds we use a single threshold on Cw(q) and Cnormp (q)during retrieval.4 Experiments4.1 Evaluation MetricsFor evaluating ASR performance we use the standardword error rate (WER) as our metric.
Since we are in-terested in retrieval we use OOV rate by type to measurethe OOV word characteristics.
For evaluating retrievalperformance we use precision and recall with respect tomanual transcriptions.
Let Correct(q) be the number oftimes the query q is found correctly, Answer(q) be thenumber of answers to the query q, and Reference(q) bethe number of times q is found in the reference.Precision(q) =Correct(q)Answer(q)Recall(q) =Correct(q)Reference(q)We compute precision and recall rates for each query andreport the average over all queries.
The set of queries Qconsists of all the words seen in the reference except fora stoplist of 100 most common words.
The measurementis not weighted by frequency ?
i.e.
each query q ?
Qis presented to the system only once, independent of thenumber of occurences of q in the transcriptions.Precision =1|Q|?q?QPrecision(q)Recall =1|Q|?q?QRecall(q)For lattice based retrieval methods, different operatingpoints can be obtained by changing the threshold.
Theprecision and recall at these operating points can be plot-ted as a curve.In addition to individual precision-recall values wealso compute the F-measure defined asF =2?
Precision?
RecallPrecision + Recalland report the maximum F-measure (maxF) to summa-rize the information in a precision-recall curve.4.2 CorporaWe use three different corpora to assess the effectivenessof different retrieval techniques.The first corpus is the DARPA Broadcast News cor-pus consisting of excerpts from TV or radio programsincluding various acoustic conditions.
The test set isthe 1998 Hub-4 Broadcast News (hub4e98) evaluationtest set (available from LDC, Catalog no.
LDC2000S86)which is 3 hours long and was manually segmented into940 segments.
It contains 32411 word tokens and 4885word types.
For ASR we use a real-time system (Saraclaret al, 2002).
Since the system was designed for SDR,the recognition vocabulary of the system has over 200Kwords.
The pronunciation dictionary has 1.25 pronuncia-tions per word.The second corpus is the Switchboard corpus consist-ing of two party telephone conversations.
The test set isthe RT02 evaluation test set which is 5 hours long, has120 conversation sides and was manually segmented into6266 segments.
It contains 65255 word tokens and 3788word types.
For ASR we use the first pass of the evalua-tion system (Ljolje et al, 2002).
The recognition vocab-ulary of the system has over 45K words.
For these wordsthe average number of pronunciations per word is 1.07.The third corpus is named Teleconferences since it con-sists of multiparty teleconferences on various topics.
Theaudio from the legs of the conference are summed andrecorded as a single channel.
A test set of six telecon-ferences (about 3.5 hours) was transcribed.
It contains31106 word tokens and 2779 word types.
Calls are auto-matically segmented into a total of 1157 segments priorto ASR, using an algorithm that detects changes in theacoustics.
We again use the first pass of the Switchboardevaluation system for ASR.In Table 1 we present the ASR performance on thesethree tasks as well as the OOV Rate by type of the cor-pora.
It is important to note that the recognition vocab-ulary for the Switchboard and Teleconferences tasks arethe same and no data from the Teleconferences task wasused while building the ASR systems.
The mismatch be-tween the Teleconference data and the models trained onthe Switchboard corpus contributes to the significant in-crease in WER.4.3 Using ASR Best Word HypothesesAs a baseline, we use the best word hypotheses of theASR system for indexing and retrieval.
The performanceTask WER OOV Rate by TypeBroadcast News ?20% 0.6%Switchboard ?40% 6%Teleconferences ?50% 12%Table 1: Word Error Rate (WER) and OOV Rate (bytype) of various LVCSR tasksof this baseline system is given in Table 2.
As ex-pected, we obtain very good performance on the Broad-cast News corpus.
It is interesting to note that when mov-ing from Switchboard to Teleconferences the degradationin precision-recall is the same as the degradation in WER.Task WER Precision RecallBroadcast News ?20% 92% 77%Switchboard ?40% 74% 47%Teleconferences ?50% 65% 37%Table 2: Precision Recall for ASR 1-best4.4 Using ASR Word LatticesIn the second set of experiments we investigate the useof ASR word lattices.
In order to reduce storage require-ments, lattices can be pruned to contain only the pathswhose costs (i.e.
negative log likelihood) are within athreshold with respect to the best path.
The smaller thiscost threshold is, the smaller the lattices and the indexfiles are.
In Figure 1 we present the precision-recallcurves for different pruning thresholds on the Telecon-ferences task.0 20 40 60 80 100020406080100 Precision vs Recall on TeleconferencesPrecisionRecall1?best word hypothesisword latticesword lattices (prune=6)word lattices (prune=4)word lattices (prune=2)Figure 1: Precision Recall using word lattices for tele-conferencesIn Table 3 the resulting index sizes and maximum F-measure values are given.
On the teleconferences task weobserved that cost=6 yields good results, and used thisvalue for the rest of the experiments.
Note that this in-creases the index size with respect to the ASR 1-best caseby 3 times for Broadcast News, by 5 times for Switch-board and by 9 times for Teleconferences.Task Pruning Size (MB) maxFBroadcast News nbest=1 29 84.0Broadcast News cost=6 91 84.8Switchboard nbest=1 18 57.1Switchboard cost=6 90 58.4Teleconferences nbest=1 16 47.4Teleconferences cost=2 29 49.5Teleconferences cost=4 62 50.0Teleconferences cost=6 142 50.3Teleconferences cost=12 3100 50.1Table 3: Comparison of index sizes4.5 Using ASR Phone LatticesNext, we compare using the two methods of phonetictranscription discussed in Section 3.3 ?
phone recogni-tion and word-to-phone conversion ?
for retrieval usingonly phone lattices.
In Table 4 the precision and recallvalues that yield the maximum F-measure as well as themaximum F-measure values are presented.
These resultsclearly indicate that phone recognition is inferior for ourpurposes.Source for Indexing Precision Recall maxFPhone Recognition 25.6 37.3 30.4Conversion from Words 43.1 48.5 45.6Table 4: Comparison of different sources for the phoneindex on the Teleconferences corpus4.6 Using ASR Word and Phone LatticesWe investigated using the strategies mentioned in Sec-tion 3.4, and found strategy 3 ?
search the word index, ifno result is returned search the phone index ?
to be su-perior to others.
We give a comparison of the maximumF-values for the three strategies in Table 5.Strategy maxF1.combination 50.52.vocabulary cascade 51.03.search cascade 52.8Table 5: Comparison of different strategies for usingword and phone indicesIn Figure 2 we present results for this strategy on theTeleconferences corpus.
The phone indices used in theseexperiments were obtained by converting the word lat-tices into phone lattices.
Using the phone indices ob-tained by phone recognition gave significantly worse re-sults.0 20 40 60 80 100020406080100 Precision vs Recall on TeleconferencesPrecisionRecall1?best word hypothesisword latticesword and phone latticesFigure 2: Comparison of word lattices and word/phonehybrid strategies for teleconferences4.7 Effect of Minimum Pronunciation Length forQueriesWhen searching for words with short pronunciations inthe phone index the system will produce many falsealarms.
One way of reducing the number of false alarmsis to disallow queries with short pronunciations.
In Fig-ure 3 we show the effect of imposing a minimum pronun-ciation length for queries.
For a query to be answered itspronunciation has to have more than minphone phones,otherwise no answers are returned.
Best maximum F-measure result is obtained using minphone=3.4.8 Effects of Recognition Vocabulary SizeIn Figure 4 we present results for different recognitionvocabulary sizes (5k, 20k, 45k) on the Switchboard cor-pus.
The OOV rates by type are 32%, 10% and 6% re-spectively.
The word error rates are 41.5%, 40.1% and40.1% respectively.
The precision recall curves are al-most the same for 20k and 45k vocabulary sizes.4.9 Using Word Pair QueriesSo far, in all the experiments the query list consisted ofsingle words.
In order to observe the behavior of variousmethods when faced with longer queries we used a set of0 20 40 60 80 100020406080100 Effect of Minimum Pronunciation LengthPrecisionRecall1?best word hypothesisword latticesminphone=0minphone=3minphone=5Figure 3: Effect of minimum pronunciation length usinga word/phone hybrid strategy for teleconferencesword pair queries.
Instead of using all the word pairs seenin the reference transcriptions, we chose the ones whichwere more likely to occur together than with other words.For this, we sorted the word pairs (w1, w2) according totheir pointwise mutual informationlogp(w1, w2)p(w1)p(w2)and used the top pairs as queries in our experiments.
Notethat in these experiments only the query set is changedand the indices remain the same as before.As it turns out, the precision of the system is very highon this type of queries.
For this reason, it is more in-teresting to look at the operating point that achieves themaximum F-measure for each technique, which in thiscase coincides with the point that yields the highest re-call.
In Table 6 we present results on the Switchboardcorpus using 1004 word pair queries.
Using word lat-tices it is possible to increase the recall of the system by16.4% while degrading the precision by only 2.2%.
Us-ing phone lattices we can get another 3.7% increase inrecall for 1.2% loss in precision.
The final system stillhas 95% precision.System Precision Recall maxFWord 1-best 98.3 29.7 45.6Word lattices 96.1 46.1 62.3Word+Phone lattices 94.9 49.8 65.4Table 6: Results for word pair queries on Switchboard0 20 40 60 80 100020406080100 Effect of Recognition Vocabulary SizePrecisionRecallword (45k)word (20k)word (5k)word+phone (45k)word+phone (20k)word+phone (5k)Figure 4: Comparison of various recognition vocabularysizes for Switchboard4.10 Summary of Results on Different CorporaFinally, we make a comparison of various techniques ondifferent tasks.
In Table 7 maximum F-measure (maxF)is given.
Using word lattices yields a relative gain of 3-5% in maxF over using best word hypotheses.
For thefinal system that uses both word and phone lattices, therelative gain over the baseline increases to 8-12%.Task System1-best W Lats W+P LatsBroadcast News 84.0 84.8 86.0Switchboard 57.1 58.4 60.5Teleconferences 47.4 50.3 52.8Table 7: Maximum F-measure for various systems andtasksIn Figure 5 we present the precision recall curves.The gain from using better techniques utilizing wordand phone lattices increases as retrieval performance getsworse.5 ConclusionWe proposed an indexing procedure for spoken utter-ance retrieval that works on ASR lattices rather than justsingle-best text.
We demonstrated that this procedure canimprove maximum F-measure by over five points com-pared to single-best retrieval on tasks with poor WERand low redundancy.
The representation is flexible sothat we can represent both word lattices, as well as phonelattices, the latter being important for improving per-formance when searching for phrases containing OOV0 20 40 60 80 100020406080100 Precision vs Recall ComparisonPrecisionRecallTeleconferencesSwitchboardBroadcast NewsFigure 5: Precision Recall for various techniques on dif-ferent tasks.
The tasks are Broadcast News (+), Switch-board (x), and Teleconferences (o).
The techniques areusing best word hypotheses (single points), using wordlattices (solid lines), and using word and phone lattices(dashed lines).words.
It is important to note that spoken utterance re-trieval for conversational speech has different propertiesthan spoken document retrieval for broadcast news.
Al-though consistent improvements were observed on a va-riety of tasks including Broadcast News, the procedureproposed here is most beneficial for more difficult con-versational speech tasks like Switchboard and Telecon-ferences.ReferencesA.
Amir, A. Efrat, and S. Srinivasan.
2001.
Advancesin phonetic word spotting.
In Proceedings of the TenthInternational Conference on Information and Knowl-edge Management, pages 580?582, Atlanta, Georgia,USA.M.
G. Brown, J. T. Foote, G. J. F. Jones, K. Sparck Jones,and S. J.
Young.
1996.
Open-vocabulary speech in-dexing for voice and video mail retrieval.
In Proc.ACM Multimedia 96, pages 307?316, Boston, Novem-ber.R.
V. Cox, B. Haskell, Y. LeCun, B. Shahraray, and L. Ra-biner.
1998.
On the application of multimedia pro-cessing to telecommunications.
Proceedings of theIEEE, 86(5):755?824, May.J.
Garofolo, G. Auzanne, and E. Voorhees.
2000.
TheTREC spoken document retrieval track: A successstory.
In Proceedings of the Recherche d?InformationsAssiste par Ordinateur: Content Based Multimedia In-formation Access Conference.J.
Hirschberg, M. Bacchiani, D. Hindle, P. Isenhour,A.
Rosenberg, L. Stark, L. Stead, S. Whittaker, andG.
Zamchick.
2001.
Scanmail: Browsing and search-ing speech data by content.
In Proceedings of theEuropean Conference on Speech Communication andTechnology (Eurospeech), Aalborg, Denmark.David Anthony James.
1995.
The Application of Classi-cal Information Retrieval Techniques to Spoken Docu-ments.
Ph.D. thesis, University of Cambridge, Down-ing College.G.
J. F. Jones, J. T. Foote, K. Sparck Jones, and S. J.Young.
1996.
Retrieving spoken documents by com-bining multiple index sources.
In Proc.
SIGIR 96,pages 30?38, Zu?rich, August.A.
Ljolje, M. Saraclar, M. Bacchiani, M. Collins, andB.
Roark.
2002.
The AT&T RT-02 STT system.
InProc.
RT02 Workshop, Vienna, Virginia.B.
Logan and JM Van Thong.
2002.
Confusion-basedquery expansion for OOV words in spoken documentretrieval.
In Proceedings of the International Confer-ence on Spoken Language Processing (ICSLP), Den-ver, Colorado, USA.B.
Logan, P. Moreno, and O. Deshmukh.
2002.
Wordand sub-word indexing approaches for reducing the ef-fects of OOV queries on spoken audio.
In Proc.
HLT.J.
Makhoul, F. Kubala, T. Leek, D. Liu, L. Nguyen,R.
Schwartz, and A. Srivastava.
2000.
Speech andlanguage technologies for audio indexing and retrieval.Proceedings of the IEEE, 88(8):1338?1353, August.M.
Mohri, F. Pereira, and M. Riley.
2002.
Weightedfinite-state transducers in speech recognition.
Com-puter Speech and Language, 16(1):69?88.Kenney Ng.
2000.
Subword-Based Approaches for Spo-ken Document Retrieval.
Ph.D. thesis, MassachusettsInstitute of Technology.NIST TREC-9 SDR Web Site.
2000.www.nist.gov/speech/tests/sdr/sdr2000/sdr2000.htm.M.
Saraclar, M. Riley, E. Bocchieri, and V. Goffin.
2002.Towards automatic closed captioning: Low latencyreal time broadcast news transcription.
In Proceedingsof the International Conference on Spoken LanguageProcessing (ICSLP), Denver, Colorado, USA.Matthew A. Siegler.
1999.
Integration of ContinuousSpeech Recognition and Information Retrieval for Mu-tually Optimal Performance.
Ph.D. thesis, CarnegieMellon University.S.
Srinivasan and D. Petkovic.
2000.
Phonetic confu-sion matrix based spoken document retrieval.
In Pro-ceedings of the 23rd Annual International ACM SIGIRConference on Research and Development in Informa-tion Retrieval, pages 81?87.M.
Wechsler, E. Munteanu, and P. Sca?uble.
1998.
Newtechniques for open-vocabulary spoken document re-trieval.
In Proceedings of the 21st Annual Interna-tional ACM SIGIR Conference on Research and De-velopment in Information Retrieval, pages 20?27, Mel-bourne, Australia.Martin Wechsler.
1998.
Spoken Document RetrievalBased on Phoneme Recognition.
Ph.D. thesis, SwissFederal Institute of Technology (ETH), Zurich.M.
Witbrock and A. Hauptmann.
1997.
Using words andphonetic strings for efficient information retrieval fromimperfectly transcribed spoken documents.
In 2ndACM International Conference on Digital Libraries(DL?97), pages 30?35, Philadelphia, PA, July.P.C.
Woodland, S.E.
Johnson, P. Jourlin, and K.SparckJones.
2000.
Effects of out of vocabulary wordsin spoken document retrieval.
In Proc.
SIGIR, pages372?374, Athens, Greece.A.
Yazgan and M. Saraclar.
2004.
Hybrid language mod-els for out of vocabulary word detection in large vocab-ulary conversational speech recognition.
In Proceed-ings of the IEEE International Conference on Acous-tics, Speech and Signal Processing (ICASSP), Mon-treal, Canada.
