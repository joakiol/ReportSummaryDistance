Proceedings of the 7th SIGdial Workshop on Discourse and Dialogue, pages 153?160,Sydney, July 2006. c?2006 Association for Computational LinguisticsA computational model of multi-modal groundingfor human robot interactionShuyin Li, Britta Wrede, and Gerhard SagererApplied Computer Science, Faculty of TechnologyBielefeld University, 33594 Bielefeld, Germanyshuyinli, bwrede, sagerer@techfak.uni-bielefeld.deAbstractDialog systems for mobile robots operat-ing in the real world should enable mixed-initiative dialog style, handle multi-modalinformation involved in the communica-tion and be relatively independent of thedomain knowledge.
Most dialog systemsdeveloped for mobile robots today, how-ever, are often system-oriented and havelimited capabilities.
We present an agent-based dialog model that are specially de-signed for human-robot interaction andprovide evidence for its efficiency with ourimplemented system.1 IntroductionNatural language is the most intuitive way to com-municate for human beings (Allen et al, 2001).
Itis, therefore, very important to enable dialog capa-bility for personal service robots that should helppeople in their everyday life.
However, the inter-action with a robot as a mobile, autonomous de-vice is different than with many other computercontrolled devices which affects the dialog model-ing.
Here we want to first clarify the most essen-tial requirements for dialog management systemsfor human-robot interaction (HRI) and then out-line state-of-the-art dialog modeling approaches toposition ourselves.The first requirement results from the situated-ness (Brooks, 1986) of HRI.
A mobile robot issituated ?here and now?
and cohabits the samephysical world as the user.
Environmental changescan have massive influence on the task execution.For example, a robot should fetch a cup from thekitchen but the door is locked.
Under this cir-cumstance the dialog system must support mixed-initiative dialog style to receive user commands onthe one side and to report on the perceived envi-ronmental changes on the other side.
Otherwisethe robot had to break up the task execution andthere is no way for the user to find out the reason.The second challenge for HRI dialog manage-ment is the embodiment of a robot which changesthe way of interaction.
Empirical studies show thatthe visual access to the interlocutor?s body affectsthe conversation in the way that non-verbal behav-iors are used as communicative signals (Nakano etal., 2003).
For example, to refer to a cup that isvisible to both dialog partners, the speaker tendsto say ?this cup?
while pointing to it.
The samestrategy is considerably ineffective during a phonecall.
This example shows, an HRI dialog systemmust account for multi-modal communication.The third, probably the unique challenge forHRI dialog management is the implication of thelearning ability of such a robot.
Since a personalservice robot is intended to help human in theirindividual household it is impossible to hard-codeall the knowledge it will need into the system, e.g.,where the cup is and what should be served forlunch.
Thus, it is essential for such a robot tobe able to learn new knowledge and tasks.
Thisability, however, has the implication for the dia-log system that it can not rely on comprehensive,hard-coded knowledge to do dialog planning.
In-stead, it must be designed in a way that it has aloose relationship with the domain knowledge.Many dialog modeling approaches already ex-ist.
McTear (2002) classified them into three maintypes: finite state-based, frame-based, and agent-based.
In the first two approaches the dialog struc-ture is closely coupled with pre-defined task stepsand can therefore only handle well-structuredtasks for which one-side led dialog styles are suf-ficient.
In the agent-based approach, the com-153munication is viewed as a collaboration betweentwo intelligent agents.
Different approaches in-spired by psychology and linguistics are in usewithin this category.
For example, within theTRAINS/TRIPS project several complex dialogsystems for collaborative problem solving havebeen developed (Allen et al, 2001).
Here the dia-log system is viewed as a conversational agent thatperforms communicative acts.
During a conver-sation, the dialog system selects the communica-tive goal based on its current belief about the do-main and general conversational obligations.
Suchsystems make use of communication and domainmodel to enable mixed-initiative dialog style andto handle more complex tasks.
In the HRI field,due to the complexity of the overall systems, usu-ally the finite-state-based strategy is employed(Matsui et al, 1999; Bischoff and Graefe, 2002;Aoyama and Shimomura, 2005).
As to the is-sue of multi-modality, one strand of the researchconcerns the fusion and representation of multi-modal information such as (Pfleger et al, 2003)and the other strand focuses on the generalisationof human-like conversational behaviors for virtualagents.
In this strand, Cassell (2000) proposes ageneral architecture for multi-modal conversationand Traum (2002) extends his information-statebased dialog model by adding more conversationallayers to account for multi-modality.In this paper we present an agent-based dialogmodel for HRI.
As described in section 2, the twomain contributions of this model are the new mod-eling approach of Clark?s grounding mechanismand the extension of this model to handle multi-modal grounding.
In section 3 we outline the ca-pabilities of the implemented system and presentsome quantitative evaluation results.2 Dialog ModelWe view a dialog as a collaboration between twoagents.
Agents are subject to common conversa-tional rules and participate in a conversation byissuing multi-modal contributions (e.g., by say-ing something or displaying a facial expression).In subsection 2.1 we show how we handle con-versational tasks by modeling the conversationalrules based on grounding and in subsection 2.2 wepresent how we model individual contributions totackle the issue of multi-modality.
In subsection2.3 we put these two things together to completethe model description.
In this section, we also putconcrete examples from the robot domain to clar-ify the relatively abstract model.2.1 GroundingOne of the most influential theories on the collab-orative nature of dialog is the common ground the-ory of Clark (1992).
In his opinion, agents needto coordinate their mental states based on theirmutual understanding about the current tasks, in-tentions, and goals during a conversation.
Clarktermed this process as grounding and proposed acontribution model.
In this model, ?contributions?from conversational agents are considered to bethe basic component of a conversation.
Each con-tribution has two phases: a Presentation phase andan Acceptance phase.
In the Presentation phase thespeaker presents an utterance to the listener, in theAcceptance phase the listener issues an evidenceof understanding to the speaker.
The speaker canonly be sure that the utterance she presented previ-ously has become a part of their common groundif this evidence is available.Although this well established theory providescomprehensive insight into human conversationtwo issues in this theory remain critical when be-ing applied to model dialog.
The first one is the re-cursivity of Acceptance.
Clark claimed, since ev-erything said by one agent needs to be understoodby her interlocutor, each Acceptance should alsoplay the role of Presentation which needs to be ac-cepted, too.
The contributions are thus to be or-ganized as a graph.
However, this implies that thegrounding process may never really end (Traum,1994).
The second critical issue is taking con-tributions as the most basic grounding units.
InClark?s view, the basic grounding unit, i.e., the unitof conversation at which grounding takes place,is the contribution.
To provide Acceptance for acontribution agents may need to issue clarificationquestions or repair.
But when modeling a dialog,especially a task-oriented dialog, it is hard to mapone single contribution from one agent to a domaintask since tasks are always cooperately done bythe two agents (Cahn and Brennan, 1999).
Traum(1994) addressed the first issue by introducing afinite-state based grounding mechanism and Cahnand Brennan (1999) used ?exchanges??
as the ba-sic grounding unit to tackle the second critical is-sue.
We combine the advantages of their work andpresent a grounding mechanism based on an aug-mented push-down automaton as described below.154Basic grounding unit: As Cahn and Brennanwe take exchange as the most basic groundingunit.
An exchange is a pair of contributions ini-tiated by the two conversational agents.
They rep-resent the idea of adjacency pairs (Schegloff andSacks, 1973).
The first contribution of the ex-change is the Presentation and the second contri-bution is the Acceptance, e.g., if one asks a ques-tion and the other answers it, then the question isthe Presentation and the answer is the Acceptance.In our model, a contribution only represents onespeech act.
For example, if an agent says ?Hello,my name is Tom, what is your name??
this ut-terances is segmented into three Presentations (agreeting, a statement, and a question) althoughthey occur in one turn.
These three Presentationsinitiate three exchanges and each of them needs tobe accepted by the interlocutor.Changing status of grounding units: Also asproposed by Cahn and Brennan, an exchange hastwo states: not (yet) grounded and grounded.
Anexchange is grounded if the Acceptance of thePresentation is available.
Note, the Acceptancecan be an implicit one, e.g, in form of ?contin-ued attention?
in Clark?s term.
Taking the exam-ple above, the other agent would reply ?Hello, myname is Jane.?
without explicitely commentingTom?s name, yet the three exchanges that Tom ini-tiated were all accepted.Organization of grounding units: In accor-dance with Traum we do not think that the Pre-sentation of one exchange should play the roleof the Acceptance of its previous exchange.
In-stead, we organize exchanges in a stack.
The stackrepresents the whole ungrounded discourse: un-grounded exchanges are pushed onto it and thegrounded ones are popped out of it.
One majorquestion of this representation is: What has thegrounding status of individual exchange to do withthe grounding status of the whole stack?
Jane?sAcceptance of Tom?s greeting has no apparent re-lation to the remaining two still ungrounded ex-changes initiated by Tom.
But in the center em-bedding example in Fig.
1, the Acceptance of B1(utterance A2) contributes to the Acceptance ofA1 (utterance B2).
These examples show that thegrounding status of the whole discourse dependson (1) the grounding status of the individual ex-changes and (2) the relationship between these ex-changes, the grounding relation.
These relationsare introduced by the Presentation of each ex-change because they start an exchange.
We identi-fied 4 types of grounding relations: Default, Sup-port, Correct, and Delete.
In the following welook at these relations in more detail and refer toexchanges with relation x to its immediately pre-ceding exchange (IPE) as ?x exchange?, e.g., Sup-port exchange:Default: The current Presentation introduces anew account that is independent of the previousexchange in terms of grounding, e.g., what Tomsaid to Jane constructs three Presentations that ini-tiate three default exchanges.
Such exchanges canbe grounded independently of each other.Support: If an agent can not provide Accep-tance for the given Presentation she will initiatea new exchange to support the grounding processof the ungrounded exchange.
A typical exam-ple of such an exchange is a clarification ques-tion like ?I beg your pardon??.
If a Support ex-change is grounded its initiator will try to groundthe IPE again with the newly collected informationthrough the supporting exchange.Correct: Some exchanges are created to correctthe content of the IPE, e.g., in case that the lis-tener misunderstood the speaker and the speakercorrects it.
Similar to Support, after such an ex-change is grounded its IPE is updated with newinformation and has to be grounded again.Delete: Agents can give up their effort to build acommon ground with her interlocutor, e.g., by say-ing ?Forget it.?.
If the interlocutor agrees, such ex-changes have the effect that all the ungrounded ex-changes from the initial Default exchange up to thecurrent state are no longer relevant and the agentsdo not need to ground them any more.Note, once an exchange is grounded it is imme-diately removed from the stack so that its IPE be-comes the IPE of the next exchange.
This modelis described as an augmented push-down automa-ton (Fig.
2).
It is augmented in so far that transi-tions can trigger actions and a variable number ofexchanges can be popped or pushed in one step.There are five states in this APDA and they rep-resent the fact what kind of ungrounded exchangeis on the top of the stack.
Along the arrows thatconnect the states the input (denoted as I), the re-sulting stack operation (denoted as S) and the pos-sible action that is triggered (denoted as A) aregiven.
The input of this automaton includes Pre-sentation (e.g., ?defaultP?
stands for ?Default Pre-sentation?)
and Acceptance.155A1: What do you think about Mr. Watton?B1: Mr. Watton?
our music teacher?A2: Yes.
(accept B1)B2: Well, he is OK. (accept A1)Figure 1: An example of center embeddingTopSupportTop DeleteTopDefaultTopStartCorrectI:acc;S:pop(ex);A:update(IPE)I:supportP;S:push(ex)I:correctP; S:push(ex)I:acc;S:pop(ex);A:correct(IPE)I:supportP; S:push(ex)I:acc; S:pop(all)deleteP; S:I[supportP|correctP|I:acc; S:pop(ex)I:defaultP; S:push(ex)S:pop(ex)I:accS:push(ex)I:defaultPI:defaultP; S:pop(all)&push(ex)I:acc; S:pop(ex); A:correct(IPE) I:acc; S:pop(ex)IcorrectP; S:push(ex)I:acc; S:pop(all)I:deleteP; S:push(ex)I:supportP; S:push(ex)I:acc; S:pop(ex); A:update(IPE)I:acc; S:pop(ex)I:defaultP; S:pop(all)&push(ex)S:pop(ex)I:acc S:push(ex)I:supportPI:deleteP; S:push(ex)I:acc; S:pop(ex)I:deleteP; S:push(ex)I:acc; S:pop(ex)I:correctP; S:push(ex)Figure 2: Augmented push-down automaton forgrounding (ex: exchange)As long as there is an ungrounded exchangeat the top of the stack, the addressee will try toground it by providing Acceptance, unless its va-lidity is deleted.
For the reason of space, we onlyexplain the APDA with the center embedding ex-ample in Fig.
1.
Contribution A1 introduces aquestion into the discourse which initiates a De-fault exchange, say Ex1.
This exchange is pushedonto the stack.
Instead of providing Acceptanceto A1, contribution B1 initiates a new exchange,say Ex2, with grounding relation Support to Ex1and is pushed onto the stack.
Then contributionA2 acknowledges B1 so that Ex2 is grounded andpopped out of the stack.
The top element of thestack is now the ungrounded Ex1.
Since Ex2 sup-ported Ex1, the Ex1 is updated with the infor-mation contained in Ex2 (The music teacher wasmeant) and B2 then successfully grounds this up-dated Ex1.In our model, every exchange can be individu-ally grounded and contributes to the grounding ofthe whole ungrounded discourse by acting on theIPE according to their grounding relations.
Thisway we can organize the discourse in a sequencewithout losing the local grounding flexibility.
Foran implemented system, this means that both theuser and the system can easily take initiative orissue clarification questions.
To implement thismodel, however, two points are crucial.
The firstone is the recognition of the user?s contributiontype: for every user contribution, the dialog sys-tem needs to decide whether it is a Presentation oran Acceptance.
If it is a Presentation, the systemneeds further to decide whether it initiates a newaccount, corrects or supports the current one, ordeletes it.
This issue of intention recognition is aclassical challenge for dialog systems.
We presentour solution in section 3.
The second point is thatthe dialog system needs to know when to create anexchange of certain grounding relation by generat-ing an appropriate Presentation and when to createan Acceptance.
For that we need to first look at thestructure of individual contributions more closelyin the next subsection.2.2 The structure of agents?
contributionsTo represent the structure of the individual contri-butions we take into account the whole languagegeneration process which enables us to come upwith a powerful solution as described below.The layers of a contribution: What we canobserve in a conversation are only exchanges ofagents?
contributions in verbal or non-verbal form.But in fact the contributions are the end-productof a complex cognitive process: language produc-tion.
Levelt (1989) identified three phases of lan-guage production: conceptualization, formulation,and articulation.
The production of an utterancestarts from the conception of a communicative in-tention and the semantic organization in the con-ceptualization phase before the utterance can beformulated and articulated in the next two phases.Intentions can arise from the previous discourse orfrom other motivations such as needs for help orinformation.
This finding motivates us to set up atwo-layered structure of contributions.
One layeris the so-called intention layer where communi-cation intentions are conceived.
For a robot thecommunication intentions come from the analysisof the previous discourse or from the robot controlsystem.
The other layer is the conversation layer.The communication intentions are formulated andarticulated here1.
These two layers represent theintention conception and the language generationprocess, respectively.
We term this two-layeredstructure of contribution interaction unit (IU).The issue of multi-modality: Face-to-faceconversations are multi-modal.
Speech and bodylanguage (e.g., gesture) can happen simultane-ously.
McNeill (1992) stated that gesture andspeech arise from the same semantic source, the1Since most robot systems use speech synthesizer to gen-erate acoustic output which replaces the articulation process,only formulation is performed on this layer.156so-called ?idea unit?
and are co-expressive.
Sincesemantic representation is created out of commu-nicative intentions (Levelt, 1989) we assume thecommunication intentions are the modality inde-pendent base that governs the multi-modal lan-guage production.
We, therefore, extend our struc-ture above by introducing two generators on theconversation layer: one verbal and one non-verbalgenerator that represent the verbal and non-verballanguage generation mechanism based on thecommunication intentions created on the intentionlayer.
The relationship between these two genera-tors is variable.
For example, Iverson et al (1999)identified three types of informational relationship- Conversation Layer -verbalgenerator non-verbalgeneratorintention conception- Intention Layer -Figure 3: IUbetween speech and gesture:reinforcement (gesture rein-forces the message conveyedin speech, e.g., emphatic ges-ture), disambiguation (ges-ture serves as the precise ref-erent of the speech, e.g., deic-tic gesture accompanying theutterance ?this cup?
), and adding-information(e.g., saying ?The ball is so big.?
and shapingthe size with hands).
In our work, when process-ing users?
multi-modal contributions we focus onthe disambiguation relation; when creating multi-modal contributions for the robot we are also inter-ested in other informational relations 2.
The struc-ture of an IU is illustrated in Fig.
3.Operation flow within an interaction unit:During a conversation an agent either initiatesan account or replies to the interlocutor?s ac-count.
The communication intentions can thus beself-motivated or other-motivated.
For a robot,self-motivated intentions can be triggered by therobot control system, e.g., observed environmen-tal changes.
In this case, an IU is created withits intention layer importing the message from therobot control system and exporting an intention.This intention is transfered to the conversationlayer which then formulates a verbal message withthe verbal generator and/or constructs a body lan-guage expression with the non-verbal generator.Other-motivated intentions can be triggered by theneeds of the on-going conversation, e.g., the needto answer a question, or be triggered by robot?s ex-ecution results of the tasks specified previously bythe user.
The operation flow is similar to that of2This policy has a practical reason: it is much more diffi-cult in computer science to correctly recognize and interprethuman motion than to simulate it.the self-motivation apart from the fact that, in caseof intentions motivated by conversational needs,the intention layer of the IU does not import anyrobot control system message but creates an inten-tion directly.
Note, the IUs that are initiated by therobot and by the user have identical structure.
Butin case of user initiated IUs we do not make anyassumption of their underlying intention buildingprocess and the intention layer of their IUs are thusalways empty.With the IUs, we can integrate the non-verbalbehavior systematically into the communicationprocess and model multi-modal dialog.
Althoughit is not the focus of our work, our model can alsohandle purely non-verbal contributions, since theverbal generator does not always need to be acti-vated if the non-verbal generator already providesenough information about the speaker?s intention.Possible scenarios are: the user looks tired (pre-sentation) and the robot offers ?I can do that foryou.?
(acceptance) or the user says something(presentation) and robot nods (acceptance).2.3 Putting things togetherTill now we have discussed our concept of usinga grounding mechanism to organize contributionsand of representing individual contributions as IU.Now it is time to look at the still open point at theend of the section 2.1: when to create an IU asPresentation and when an IU as Acceptance.Self-motivated intentions usually trigger thecreation of an IU as Presentation with Default re-lation to its IPE.
For example, if the robot needsto report something to the user it can create a De-fault exchange by generating an IU as its Presen-tation.
The user is then expected to signal her Ac-ceptance.
Other-motivated intentions can, accord-ing to the context, result in either Presentation orAcceptance.
To make the correct decision we de-veloped criteria based on the joint intention theoryof Levesque et al (1990) which predicts that dur-ing a collaboration the partners are committed toa joint goal that they will always try to conformtill they reach the goal or give up.
Note, this doesnot mean that one will always agree with her inter-locutor, but they will behave in the way that theythink is the best to achieve the goal.
This theorycan be applied to human-robot dialog in a twofoldsense: Firstly, a dialog can be generally seen asa collaboration as Clark proposed.
Secondly, thehuman-robot dialog is mostly task-oriented, i.e.,157the human and the robot work towards the samegoal.
With this theory in mind we describe howwe process other-motivated contributions below.The precondition of language production basedon other-motivated intentions is language percep-tion.
Before reacting, i.e., before creating her ownIU, an agent first needs to understand the inten-tion conveyed by her interlocutor?s IU by study-ing its conversation layer.
Since we focus on dis-ambiguation function of non-verbal behavior weassume that agents first study the generated ver-bal information, if the intention can not be fullyrecognized here, one will further study the infor-mation provided by the non-verbal generator (e.g.,a gesture) and fuse the verbal and non-verbal in-formation.
If the intention recognition is still un-successful, the agent can not provide Acceptancefor the given IU.
If she is still committed to thedialog she will issue a clarification question, i.e.,she generates an IU as Presentation that initiatesa Support exchange to the current ungrounded ex-change.
If the intention of her interlocutor is suc-cessfully recognized the language perception pro-cess ends and the agent tries to create her own IU.As described in subsection 2.2 the creation of theIU starts from the creation of an intention on theintention layer.
In case of a robot, the dialog sys-tem accesses the robot control system and awaitsits reaction to the conveyed information (e.g., auser instruction).
Usually, a robot is designatedto do something for the user, i.e., the robot is com-mitted to the goal proposed by the user, so we de-fine the robot can only provide acceptance if thetask is successfully executed.
In this case, the robotcompletes the current IU with the filled intentionlayer by generating an confirmation on its conver-sation layer.
Afterwards, this grounded exchangecan be popped from the stack.
If the robot can notexecute the task for some reasons, then the currentexchange can not be grounded and the robot willtake the current IU with the filled intention layeras another Presentation that initiates a Support orCorrect exchange to the current ungrounded ex-change, similar as the case in Fig.
1.
The conversa-tion layer of this IU can thus formulate somethinglike ?Sorry, I can?t do that because...?
and presenta sorrowful face.
This new Support or Correct ex-change is pushed onto the stack.
Figure 4 illus-trates this process as a UML activity diagram.In our model we only do general conversationalplanning instead of domain specific task planning.study non?verbal info on the CLintention recognized?intention recognized?intention conformsthe joint goal?complete IU as Acceptance create IU as Presentationpush Exchange with the interlocutor?s IU as presentationnyes noyesnonoyesstudy verbal info on the interlocutor?s CLSupport or Correct relationground exchange npop exchange ncreate exchange n+1 withpush exchange n+1(access robot control system)create one?s own ILFigure 4: Handling other-motivated contribution(CL: Conversation layer; IL: Intention Layer)What the dialog system needs to know from therobot control system is what processing results itcan produce.
The association of these results withrobot intentions in terms of whether they start anew account, support or correct one, or delete it,can be configured externally and thus easily up-dated or replaced.
Based on this configuration IUsare generated that operate according to the ground-ing mechanism as described in section 2.1.3 ImplementationThis dialog model was implemented for our robotBIRON, a personal robot with learning abilities.It can detect and follow persons, focus on objects(according to human deictic gestures) and storecollected information into a memory.
Our imple-mentation scenario is the so-called home tour: auser shows a new robot her home to prepare it forfuture tasks.
The robot should be able to learn andremember features of objects that the user men-tions and it ?sees?, e.g., name, color, images etc.Besides, our system was also successfully portedto a humanoid robot BARTHOC for studies ofemotional and social factors of HRI (see.
Fig.
5).Figure 5: Robots BIRON and BARTHOCThe dialog manager is linked to a speech under-standing system which transforms parts of speech158from a speech recognizer into a speech-act-basedform.
To recognize the user?s intention, the dia-log system classifies this input into 10 categoriesof three groups according to heuristics, e.g., in-struction, description, and query initiate new tasksand thus a new Default exchange; deletion andcorrection initiate Delete and Correct exchangesthat are related to early exchanges; and confirma-tion, negation, etc.
can only be responses andare, therefore, viewed as user?s Acceptance of ex-changes that the robot initiates.
The main part ofthe dialog system is the Dialog Manager that car-ries out grounding analysis and stack operations.The Robot Control Manager receives messagesfrom the robot control system and calls the Dia-log Manager to do relevant operations.
This dialogsystem enables multi-modal, mixed-initiative dia-log style and can handle complex repair behaviors.With the example dialog in Fig.
6, which is tran-scripted from a real interaction session between auser and BIRON, we discuss the two most impor-tant features of our system.Ex3R1: Hello human!Who are you?U1: Hello!something I can remember it.U2: OK, um, look, this is a mug.U3: This is a mug, umm, here.R4: Sorry, can you use some other words?Ex5Ex2 R2: My name is BIRON, if you show meR5: Sorry, I can?t find it.
Are you pointingto it with your finger?U5: Yes!R6: I know it is sometimes difficult with me,but don?t feel discouraged!R7: OK, I?ve found it, it is really nice!Ex6R3: I beg your pardon?U4: This is a mug!U6: (laugh)Ex7Ex4Ex1 (looking friendly)(looking friendly)(looking puzzled)(looking sad)(looking sad)(looking ashamed)(looking happily)Figure 6: A dialog example with the extrovertBIRON.
(U: user, R: robot, Ex: Exchange)Taking Initiative and robot personality: Ini-tiatives that a dialog system can take often dependson its back-end application.
Since BIRON doesnot have a task planner which would be ideal todemonstrate this ability we implemented an extro-vert personality for it (additionally to its basic per-sonality) that takes communication-related initia-tives.
The basic BIRON behaves in a rather pas-sive way and only says something when addressedby the user.
In contrast, the extrovert BIRONgreets persons actively (R1 in Table 6) and re-marks on its own performance (R6).
When therobot control system detects a person the dialogsystem initiates a Default exchange to greet her.BIRON can also measure its own performance bycounting the number of Support exchanges it hasinitiated for the current topic.
Since the Supportexchanges are only created if BIRON can not pro-vide Acceptance to the user?s Presentation (be-cause it does not understand the user or it cannot execute a task), the amount of the Support ex-changes thus has direct correlation to the robot?soverall performance.
On the other hand, the moreDefault exchanges there are, the better is the per-formance because the agents can proceed to an-other topic only if the current one is grounded (ordeleted).
Based on this performance indicationBIRON does remarks to motivate users.Resolving multi-modal object references: Ithappens quite frequently in the home tour scenariothat the user points to some objects and says ?Thisis a z?.
BIRON needs to associate its symbolicname (and eventually other features) mentioned bythe user with the image of the object.
The reso-lution of such multi-modal object references (U4-R7 in Table 6) is solved as following: the DialogManager creates an IU for the user-initiated utter-ance (e.g., ?this is a cup?)
and studies the verbaland non-verbal generator on its conversation layer.In the verbal generator, what the pronoun ?this?refers to is unclear, but it indicates that the usermight be using a gesture.
Therefore, the DialogManager further studies the non-verbal generator.The responsible robot vision module is activatedhere to search for a gesture and to identify the ob-ject cup.
If the cup is found in the scene, this mod-ule assigns an ID to the image and stores it in thememory.
After the Dialog Manager receives thisID, the processing of the conversation layer of theuser IU ends, the Dialog Manager proceeds to cre-ate its own IU to react to the user?s IU.
Problemswith the object identification indicate failure of theintention recognition process on the user conversa-tion layer.
In this case, the Dialog Manager createsa Support exchange to ask the user which objectshe refers to and retries it if she does not oppose(R5-R7).
This process and the associated multi-modality fusion and representation are describedin (Li et al, 2005) in detail.The evaluation of dialog systems for humanrobot interaction is still an open issue.
A robotsystem is usually a complex system including a159large number of modules that claim plenty of pro-cessing time or are subject to environmental con-ditions.
For the dialog system, this means that thecorrect interpretation and transaction of user utter-ances is by no means a guaranty for a prompt re-sponse or successful task execution.
Thus, the per-formance of the dialog system can not be directlymeasured with the performance of the overall sys-tem like most desktop dialog applications.
We arestill working at evaluation metrics for HRI dialogsystems (Green et al, 2006).
But the efficiencyof our system is already visible in the small ef-fort associated with the porting of this system toanother robot platform and in the pilot user studywith BIRON.
In this study, each of the 14 users in-teracted with BIRON twice.
In the total 28 runsthe dialog system generated 903 exchanges forthe 813 user utterances.
Among these exchanges,34% initiated clarification questions.
This resultcorrelated with the evaluation result of our speechunderstanding system which fully understood 65%of all the user utterances.
18.6% of the exchangeswere Support exchanges created due to executionfailure of the robot control system which corre-sponds to the performance of the robot control sys-tem.
The average processing time of the dialogsystem was 11 msec.4 ConclusionIn this paper we presented an agent-based dialogmodel for HRI.
The implemented system enablesmulti-modal, mixed-initiative dialog style and isrelatively domain independent.
The real-time test-ing of the system proves its efficiency.
We willwork out detailed evaluation metrics for our sys-tem to be able to draw more general conclusionabout the strength and weakness of our model.ReferencesJ.
Allen, D. K. Byron, M. Dzikovska, G. Ferguson,L.
Galescu, and A. Stent.
2001.
Towards conversationalhuman-computer interaction.
AI Magazine, 22(4).K.
Aoyama and H. Shimomura.
2005.
Real world speechinteraction with a humanoid robot on a layered robot be-havior control architecture.
In Proc.
Int.
Conf.
on Roboticsand Automation.R.
Bischoff and V. Graefe.
2002.
Dependable multimodalcommunication and interaction with robotic assistants.
InProc.
Int.
Workshop on Robot-Human Interactive Commu-nication (ROMAN).R.
A. Brooks.
1986.
A robust layered control system for amobile robot.
IEEE Journal of Robotics and Automation,2(1):14?23.J.
E. Cahn and S. E. Brennan.
1999.
A psychological modelof grounding and repair in dialog.
In Proc.
Fall 1999 AAAISymposium on Psychological Models of Communicationin Collaborative Systems.J.
Cassell, T. Bickmore, L. Campbell, and H. Vilhjalmsson.2000.
Human conversation as a system framework: De-signing embodied conversational agents.
In J. Cassell,J.
Sullivan, S. Prevost, and E. Churchill, editors, Embod-ied conversational agents.
MIT Press.H.
H. Clark, editor.
1992.
Arenas of Language Use.
Univer-sity of Chicago Press.A.
Green, K. Severinson-Eklundh, B. Wrede, and S. Li.2006.
Integrating miscommunication analysis in naturallanguage interface design for a service robot.
In Proc.
Int.Conf.
on Intelligent Robots and Systems.
submitted.J.
M. Iverson, O. Capirci, E. Longobardi, and M. C. Caselli.1999.
Gesturing in mother-child interactions.
CognitiveDevelpment, 14(1):57?75.W.
Levelt.
1989.
Speaking: From intention to articulation.Cambridge, MA: MIT Press.H.
J. Levesque, P. R. Cohen, and J. H. T. Nunnes.
1990.
Onacting together.
In Proc.
Nat.
Conf.
on Artificial Intelli-gence (AAAI).S.
Li, A. Haasch, B. Wrede, J. Fritsch, and G. Sagerer.2005.
Human-style interaction with a robot for coopera-tive learning of scene objects.
In Proc.
Int.
Conf.
on Mul-timodal Interfaces.T.
Matsui, H. Asoh, J. Fry, Y. Motomura, F. Asano, T. Kurita,I.
Hara, and N. Otsu.
1999.
Integrated natural spokendialogue system of jijo-2 mobile robot for office services,.In Proc.
AAAI Nat.
Conf.
and Innovative Applications ofArtificial Intelligence Conf.D.
McNeill.
1992.
Hand and Mind: What Gesture Revealabout Thought.
University of Chicago Press.M.
F. McTear.
2002.
Spoken dialogue technology: enablingthe conversational interface.
ACM Computing Surveys,34(1).Y.
I. Nakano, G. Reinstein, T. Stocky, and J. Cassell.
2003.Towards a model of face-to-face grounding.
In Proc.
An-nual Meeting of the Association for Computational Lin-guistics.N.
Pfleger, J. Alexandersson, and T. Becker.
2003.
A ro-bust and generic discourse model for multimodal dialogue.In Proc.
3rd Workshop on Knowledge and Reasoning inPractical Dialogue Systems.E.
A. Schegloff and H. Sacks.
1973.
Opening up closings.Semiotica, pages 289?327.D.
Traum and J. Rickel.
2002.
Embodied agents for multi-party dialogue in immersive virtual world.
In Proc.
1st Int.Conf on Autonomous Agents and Multi-agent Systems.D.
Traum.
1994.
A Computational Theory of Grounding inNatural Language Conversation.
Ph.D. thesis, Universityof Rochester.160
