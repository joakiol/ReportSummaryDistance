Coling 2008: Proceedings of the workshop on Cross-Framework and Cross-Domain Parser Evaluation, pages 44?50Manchester, August 2008Constructing a Parser Evaluation SchemeLaura Rimell and Stephen ClarkOxford University Computing LaboratoryWolfson Building, Parks RoadOxford, OX1 3QD, United Kingdom{laura.rimell,stephen.clark}@comlab.ox.ac.ukAbstractIn this paper we examine the process ofdeveloping a relational parser evaluationscheme, identifying a number of decisionswhich must be made by the designer ofsuch a scheme.
Making the process moremodular may help the parsing communityconverge on a single scheme.
Examplesfrom the shared task at the COLING parserevaluation workshop are used to highlightdecisions made by various developers, andthe impact these decisions have on any re-sulting scoring mechanism.
We show thatquite subtle distinctions, such as howmanygrammatical relations are used to encode alinguistic construction, can have a signifi-cant effect on the resulting scores.1 IntroductionIn this paper we examine the various decisionsmade by designers of parser evaluation schemesbased on grammatical relations (Lin, 1995; Car-roll et al, 1998).
Following Carroll et al (1998),we use the term grammatical relations to referto syntactic dependencies between heads and de-pendents.
We assume that grammatical relationschemes are currently the best method availablefor parser evaluation due to their relative inde-pendence of any particular parser or linguistictheory.
There are several grammatical relationschemes currently available, for example Carroll etal.
(1998), King et al (2003), and de Marneffe etal.
(2006).
However, there has been little analysisof the decisions made by the designers in creatingc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.what turns out to be a complex set of dependen-cies for naturally occurring sentences.
In particu-lar, in this paper we consider how the process canbe made more modular to help the parsing commu-nity converge on a single scheme.The first decision to be made by the schemedesigner is what types of linguistic constructionsshould be covered by the scheme.
By constructionwe mean syntactic phenomena such as subject ofverb, direct object of verb, passive voice, coordina-tion, relative clause, apposition, and so on.
In thispaper we assume that the constructions of interesthave already been identified (and there does appearto be broad agreement on this point across the ex-isting schemes).
A construction can be thought ofas a unitary linguistic object, although it is oftenrepresented by several grammatical relations.The second decision to be made is which wordsare involved in a particular construction.
This isimportant because a subset of these words willbe arguments of the grammatical relations repre-senting the construction.
Again, we assume thatthere is already broad agreement among the exist-ing schemes regarding this question.
One possiblepoint of disagreement is whether to include emptyelements in the representation, for example whena passive verb has no overt subject, but we will notaddress that issue here.The next question, somewhat orthogonal to theprevious one, and a source of disagreement be-tween schemes, is how informative the represen-tation should be.
By informative we mean theamount of linguistic information represented in thescheme.
As well as relations between heads, someschemes include one or more features, each ofwhich expresses information about an individualhead.
These features can be the locus of richer lin-guistic information than is represented in the de-44pendencies.
A useful example here is tense andmood information for verbs.
This is included in thePARC scheme, for example, but not in the Briscoeand Carroll or Stanford schemes; PARC is in gen-eral more informative and detailed than competingschemes.
Although features are technically dif-ferent from relations, they form part of an overallevaluation scheme and must be considered by thescheme designer.
We will not consider here thequestion of how informative schemes should be;we only note the importance of this question forthe resulting scoring mechanism.The penultimate question, also a source ofdisagreement among existing schemes, is whichwords among all those involved in the construc-tion should be used to represent it in the scheme.This decision may arise when identifying syntac-tic heads; for example, in the sentence Brownsaid house prices will continue to fall, we assumethere is no disagreement about which words areinvolved in the clausal complement construction({said, house, prices, will, continue, to, fall}), butthere may be disagreement about which subset touse to represent the construction in the grammat-ical relations.
Here, either will or continue couldbe used to represent the complement of said.
Thisdecision may also be theory-dependent to some de-gree, for example whether to use the determiner orthe noun as the head of a noun phrase.The final decision to make is the choice of rela-tions and their arguments.
This can also be thoughtof as the choice of how the set of representativewords should be grouped into relations.
For exam-ple, in a relative clause construction, the schemedesigner must decide whether the relation betweenthe relative pronoun and the head noun is impor-tant, or the relation between the relative pronounand the verb, between the head noun and the verb,or some subset of these.
The choice of label foreach relation will be a natural part of this decision.An important property of the representation,closely related to the choices made about represen-tative words and how they are grouped into rela-tions, is the number of relations used for a partic-ular construction.
We refer to this as the compact-ness property.
Compactness essentially boils downto the valency of each relation and the informationencoded in the label(s) used for the relation.
Weshow that this property is closely related to the as-signing of partial credit ?
awarding points evenwhen a construction is not recovered completelycorrectly ?
and that it can have a significant effecton the resulting scoring mechanism.The dividing lines between the various ques-tions we have described are subtle, and in partic-ular the last two questions (which words shouldrepresent the construction and which relations touse, and consequently how compactly the rela-tions are represented) have significant overlap withone another.
For example, if the auxiliary arein the passive construction prices are affected ischosen as one of the representative words, thena relation type which relates are to either pricesor affected must also be chosen.
For the relativeclause construction woman who likes apples andpears, if the words and relations chosen includea representation along the lines of relative-clause-subject(likes, woman) and subject(likes, who), thenit is unlikely that the more compact relationrelative-clause(likes, woman, who) would also bechosen.
Despite the overlap, each question canprovide a useful perspective for the designer of anevaluation scheme.Decisions must be made not only about the rep-resentations of the individual constructions, butalso about the interfaces between constructions.For example, in the sentenceMary likes apples andpears, the coordination structure apples and pearsserves as direct object of likes, and it must be de-termined which word(s) are used to represent thecoordination in the direct object relation.We will illustrate some of the consequences ofthe decisions described here with detailed exam-ples of three construction types.
We focus on pas-sive, coordination, and relative clause construc-tions, as analysed in the PARC (King et al, 2003),GR (Briscoe and Carroll, 2006), and Stanford (deMarneffe et al, 2006) evaluation schemes, usingsentences from the shared task of the COLING 2008parser evaluation workshop.1These three con-structions were chosen because we believe theyprovide particularly good illustrations of the var-ious decisions and their consequences for scoring.Furthermore, they are constructions whose repre-sentation differs across at least two of the threegrammatical relation schemes under dicsussion,which makes them more interesting as examples.We believe that the principles involved, however,1The shared task includes a number of additional formatsbesides the three grammatical relation schemes that we con-sider here, but the representations are sufficiently differentthat we don?t consider a comparison fruitful for the presentdiscussion.45apply to any linguistic construction.We also wish to point out that at this stage we arenot recommending any particular scheme or anyanswers to the questions we raise, but only sug-gesting ways to clarify the decision points.
Nor dowe intend to imply that the ideal representation ofany linguistic construction, for any particular pur-pose, is one of the representations in an existingscheme; we merely use the existing schemes asconcrete and familiar illustrations of the issues in-volved.2 The Passive ConstructionThe following is an extract from Sentence 9 of theshared task:how many things are made out of eggsWe expect general agreement that this is a pas-sive construction, and that it should be included inthe evaluation scheme.2We also expect agreementthat all the words in this extract are involved in theconstruction.Potential disagreements arise when we considerwhich words should represent the construction.Things, as the head of the noun phrase which is theunderlying object of the passive, and made, as themain verb, seem uncontroversial.
We discard howand many as modifiers of things, and the prepo-sitional phrase out of eggs as a modifier of made;again we consider these decisions to be straightfor-ward.
More controversial is whether to include theauxiliary verb are.
PARC, for example, does notinclude it in the scheme at all, considering it an in-herent part of the passive construction.
Even if theauxiliary verb is included in the overall scheme, itis debatable whether this word should be consid-ered part of the passive construction or part of aseparate verb-auxiliary construction.
Stanford, forexample, uses the label auxpass for the relation be-tween made and are, indicating that it is part of thepassive construction.The next decision to be made is what relationsto use.
We consider it uncontroversial to includea relation between things and made, which will besome kind of subject relation.
We also want to rep-resent the fact that made is in the passive voice,since this is an essential part of the constructionand makes it possible to derive the underlying ob-ject position of things.
If the auxiliary are is in-2PARC recognises it as an interrogative as well as a passiveconstruction.cluded, then there should be a verb-auxiliary rela-tion between made and are, and perhaps a subjectrelation between are and things (although none ofthe schemes under consideration use the latter rela-tion).
PARC includes a variety of additional infor-mation about the selected words in the construc-tion, including person and number information forthe nouns, as well as tense and mood for the verbs.Since this is not included in the other two schemes,we ignore it here.The relevant relations from the three schemesunder consideration are shown below.3PARCpassive(make, +)subj(make, thing)GR(ncsubj made things obj)(passive made)(aux made are)Stanfordnsubjpass(made, things)auxpass(made, are)PARC encodes the grammatical relations lesscompactly, with one subject relation joining makeand thing, and a separate relation expressing thefact that make is in the passive voice.
Stanfordis more compact, with a single relation nsubj-pass that expresses both verb-subject (via the argu-ments) and passive voice (via the label).
GR has anequally compact relation since the obj marker sig-nifies passive when found in the ncsubj relation.GR, however, also includes an additional featurepassive, which redundantly encodes the fact thatmade is in passive voice.4Table 1 shows how different kinds of parsing er-rors are scored in the three schemes.
First note thedifferences in the ?everything correct?
row, whichshows how many points are available for the con-struction.
A parser that is good at identifying pas-sives will earn more points in GR than in PARCand Stanford.
Of course, it is always possible tolook at accuracy figures by dependency type in or-der to understand what a parser is good at, as rec-ommended by Briscoe and Carroll (2006), but it is3Schemes typically include indices on the words for iden-tification, but we omit these from the examples unless re-quired for disambiguation.
Note also that PARC uses thelemma rather than the inflected form for the head words.4Although passive is technically a feature and not a rela-tion, as long as it is included in the evaluation the effect willbe of double scoring.46PARC GR StanfEverything correct 2 3 2Misidentify subject 1 2 1Misidentify verb 0 0 0Miss passive constr 1 1 0Miss auxiliary 2 2 1Table 1: Scores for passive construction.also desirable to have a single score reflecting theoverall accuracy of a parser, which means that theconstruction?s overall contribution to the score isrelevant.5Observe also that partial credit is assigned dif-ferently in the three schemes.
If the parser recog-nises the subject of made but misses the fact thatthe construction is a passive, for example, it willearn one out of two possible points in PARC, oneout of three in GR (if it recognizes the auxiliary),but zero out of two in Stanford.
This type of errormay seem unlikely, yet examples are readily avail-able.
In related work we have evaluated the C&Cparser of Clark and Curran (2007) on the BioIn-fer corpus of biomedical abstracts (Pyysalo et al,2007), which includes the following sentence:Acanthamoeba profilin was cross-linkedto actin via a zero-length isopeptidebond using carbodiimide.The parser correctly identifies profilin as the sub-ject of cross-linked, yet because it misidentifiescross-linked as an adjectival rather than verbalpredicate, it misses the passive construction.Finally, note an asymmetry in the partial creditscoring: a parser that misidentifies the subject (e.g.by selecting the wrong head), but basically gets theconstruction correct, will receive partial credit inall three schemes; misidentifying the verb, how-ever (again, this would likely occur by selectingthe wrong head within the verb phrase) will causethe parser to lose all points for the construction.3 The Coordination ConstructionThe coordination construction is particularly inter-esting with regard to the questions at hand, bothbecause there are many options for representingthe construction itself and because the interfacewith other constructions is non-trivial.
Here we5We assume that the overall score will be an F-score overall dependencies/features in the relevant test set.consider an extract from Sentence 1 of the sharedtask:electronic, computer and building prod-uctsThe coordination here is of nominal modifiers,which means that there is a decision to make abouthow the coordination interfaces with the modifiednoun.
All the conjuncts could interact with thenoun, or there could be a single relationship, usu-ally represented as a relationship between the con-junction and and the noun.Again we consider the decisions about whetherto represent coordination constructions in an eval-uation scheme, and about which words are in-volved in the construction, to be generally agreedupon.
The choice of words to represent theconstruction in the grammatical relations is quitestraightforward: we need all three conjuncts, elec-tronic, computer, and building, and also the con-junction itself since this is contentful.
It also seemsreasonably uncontroversial to discard the comma(although we know of at least one parser thatoutputs relations involving the comma, the C&Cparser).The most difficult decision here is whether theconjuncts should be related to one another or tothe conjunction (or both).
Shown below is how thethree schemes represent the coordination, consid-ering also the interface of the coordination and thenominal modification construction.PARCadjunct(product, coord)adjunct type(coord, nominal)conj(coord, building)conj(coord, computer)conj(coord, electronic)coord form(coord, and)coord level(coord, AP)GR(conj and electronic)(conj and computer)(conj and building)(ncmod products and)Stanfordconj and(electronic, computer)conj and(electronic, building)amod(products, electronic)amod(products, computer)amod(products, building)47Table 2 shows the range of scores assigned forcorrect and partially correct parses across the threeschemes.
A parser that analyses the entire con-struction correctly will earn anywhere from fourpoints in GR, to seven points in PARC.
Therefore,a parser that does very well (or poorly) at coordi-nation will earn (or lose) points disproportionatelyin the different schemes.Parc GR StanfEverything correct 7 4 5Misidentifyconjunction 6 0 3Misidentify oneconjunct 6a3 3bMisidentify twoconjuncts 5a2 1aThe parser might also be incorrect about the co-ord level relation if the conjuncts are misidentified.bThe score would be 2 if it is the first conjunct thatis misidentified.Table 2: Scores for coordination, includinginterface with nominal modification.A parser that recognises the conjuncts correctlybut misidentifies the conjunction would lose onlyone point in PARC, where the conjunction is sep-arated out into a single coord form relation, butwould lose all four available points in GR, becausethe word and itself takes part in all four GR de-pendencies.
Only two points are lost in Stanford(and it is worth noting that there is also an ?uncol-lapsed?
variant of the Stanford scheme in whichthe coordination type is not rolled into the depen-dency label, in which case only one point would belost).Note also an oddity in Stanford which meansthat if the first conjunct is missed, all the dependen-cies are compromised, because the first conjunctenters into relations with all the others.
The moreconjuncts there are in the construction, the morepoints are lost for a single parsing error, which caneasily result from an error in head selection.Another issue is how the conjuncts are repre-sented relative to the nominal modifier construc-tion.
In PARC and GR, the conjunct and stands infor all the conjuncts in the modifier relation.
Thismeans that if a conjunct is missed, no extra pointsare lost on the modifier relation; whereas in Stan-ford, points are lost doubly ?
on the relations in-volving both conjunction and modification.4 The Relative Clause ConstructionFor the relative clause construction, as for coordi-nation, the choice of words used to represent theconstruction is straightforward, but the choice ofrelations is less so.
Consider the following relativeclause construction from Sentence 2 of the sharedtask:not all those who wroteAll three schemes under consideration use the set{those, who, wrote} to describe this construction.6PARCpron form(pro3, those)adjunct(pro3, write)adjunct type(write, relative)pron form(pro4, who)pron type(pro4, relative)pron rel(write, pro4)topic rel(write, pro4)GR(cmod who those wrote)(ncsubj wrote those )Stanfordnsubj(wrote, those)rel(wrote, who)rcmod(those, wrote)Note that PARC represents the pronouns whoand those, as it does all pronouns, at a more ab-stract level than GR or Stanford, creating a rep-resentation that is less compact than the others.GR and Stanford differ in terms of compactness aswell: GR?s cmod relation contains all three words;in fact, the ncsubj relationship might be consideredredundant from the point of view of an evaluationscheme, since an error in ncsubj entails an error incmod.
Stanford?s representation is less compact,containing only binary relations, although there isalso a redundancy between nsubj and rcmod sincethe two relations are mirror images of each other.For the sake of comparison, we include here twoadditional hypothetical schemes which have dif-ferent characteristics from those of the three tar-get schemes.
In Hypothetical Scheme 1 (HS1),there are three relations: one between the headnoun and the relative clause verb, one between the6PARC also encodes the fact that pro3is a demonstrativepronoun, but we don?t consider this part of the relative clauseconstruction.48PARC GR Stanf HS1 HS2Everything correct 7 2 3 3 1Misidentify head noun 6 0 1 1 0Misidentify verb 3 0 0 2 0Miss relative clause construction 3 0 0 1 0Table 3: Scores for relative clauses.relative pronoun and the relative clause verb, anda third which relates the relative pronoun to thehead noun.
This third relation is not included inany of the other schemes.
Hypothetical Scheme 2(HS2) involves only one relation, which includesthe same words as GR?s cmod relation; the repre-sentation as a whole is quite compact since onlyone dependency is involved and it includes allthree words.Hypothetical Scheme 1relative-subject(wrote, those)subject(wrote, who)relative-pronoun(those, who)Hypothetical Scheme 2relative-clause(wrote, those, who)Table 3 shows the range of scores that can be at-tained in the different schemes.
The total possiblescore varies from one for HS2, to three for Stan-ford and HS1, and up to seven for PARC.Observe that any of the three types of error inTable 3 will immediately lose all points in both GRand HS2.
Since all the schemes use the same setof words, this is due solely to the choice of rela-tions and the compactness of the representations.Neither GR nor HS2 allow for partial credit, evenwhen the parser assigns an essentially correct rel-ative clause structure.
This is a scenario whichcould easily occur due to a head selection error.For example, consider the following phrase fromthe shared task GENIA (Kim et al, 2003) data set ,Sentence 8:.
.
.
the RelA ( p65 ) subunit of NF-kappaB , which activates transcription of the c-rel gene .
.
.The C&C parser correctly identifies the relativeclause structure, including the pronoun which andthe verb activates, but incorrectly identifies thehead noun as B instead of subunit.Even between GR and HS2, which share thecharacteristic of not allowing for partial credit,there is a difference in scoring.
Because GR startswith two dependencies, there is a loss of twopoints, rather than just one, for any error, whichmeans errors in relative clauses are weighted moreheavily in GR than in HS2.Stanford also has a problematic redundancy,since the nsubj and rcmod relations are mirror im-ages of each other.
It therefore duplicates the GRcharacteristic of penalising the parser by at leasttwo points if either the head noun or the relativeclause verb is misidentified (in fact three points forthe verb).Observe also the asymmetry between misidenti-fying the head noun (one out of seven points lost inPARC, two out of three lost in Stanford and HS1)compared to misidentifying the verb (three pointslost in PARC, all three lost in Stanford, but only onepoint lost in HS1).
This reflects a difference be-tween the schemes in whether the relative pronounenters into a relation with the subject, the verb, orboth.5 ConclusionIn this paper we have shown how the design pro-cess for a relational parser evaluation scheme canbe broken up into a number of decisions, and howthese decisions can significantly affect the scoringmechanism for the scheme.
Although we have fo-cused in detail on three construction types, we be-lieve the decisions involved are relevant to any lin-guistic construction, although some decisions willbe more difficult than others for certain construc-tions.
A direct object construction, for example,will normally be represented by a single relationbetween a verbal head and a nominal head, and in-deed this is so in all three schemes considered here.This does not mean that the representation is triv-ial, however.
The choice of which heads will rep-resent the construction is important.
In addition,Stanford distinguishes objects of prepositions fromobjects of verbs, while PARC and GR collapse thetwo into a single relation.
Although part of speechinformation can be used to distinguish the two, a49parser which produces PARC- or GR-style outputin this regard will lose points in Stanford withoutsome additional processing.We have made no judgements about which deci-sions are best in the evaluation scheme design pro-cess.
There are no easy answers to the questionsraised here, and it may be that different solutionswill suit different evaluation situations.
We leavethese questions for the parsing community to de-cide.
This process may be aided by an empiricalstudy of how the decisions affect the scores givento various parsers.
For example, it might be use-ful to know whether one parser could be made toscore significantly higher than another simply bychanging the way coordination is represented.
Weleave this for future work.ReferencesBriscoe, Ted and John Carroll.
2006.
Evaluating theaccuracy of an unlexicalized statistical parser on thePARC DepBank.
In Proceedings of the ACL-Coling?06 Main Conf.
Poster Session, pages 41?48, Syd-ney, Austrailia.Carroll, John, Ted Briscoe, and Antonio Sanfilippo.1998.
Parser evaluation: a survey and a new pro-posal.
In Proceedings of the 1st LREC Conference,pages 447?454, Granada, Spain.Clark, Stephen and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.de Marneffe, Marie-Catherine, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the 5th LREC Conference, pages449?454, Genoa, Italy.Kim, Jin-Dong, Tomoko Ohta, Yuka Teteisi, andJun?ichi Tsujii.
2003.
GENIA corpus ?
a seman-tically annotated corpus for bio-textmining.
Bioin-formatics, 19:i180?i182.King, Tracy H., Richard Crouch, Stefan Riezler, MaryDalrymple, and Ronald M. Kaplan.
2003.
ThePARC 700 Dependency Bank.
In Proceedings of the4th International Workshop on Linguistically Inter-preted Corpora, Budapest, Hungary.Lin, Dekang.
1995.
A dependency-based method forevaluating broad-coverage parsers.
In Proceedingsof IJCAI-95, pages 1420?1425, Montreal, Canada.Pyysalo, Sampo, Filip Ginter, Juho Heimonen, JariBj?orne, Jorma Boberg, Jouni J?arvinen, and TapioSalakoski.
2007.
BioInfer: A corpus for informationextraction in the biomedical domain.
BMC Bioinfor-matics, 8:50.50
