Proceedings of the 7th Workshop on Statistical Machine Translation, pages 84?90,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsQuality Estimation for Machine Translation outputusing linguistic analysis and decoding featuresEleftherios AvramidisGerman Research Center for Artificial Intelligence (DFKI)Berlin, Germanyeleftherios.avramidis@dfki.deAbstractWe describe a submission to the WMT12Quality Estimation task, including an exten-sive Machine Learning experimentation.
Datawere augmented with features from linguis-tic analysis and statistical features from theSMT search graph.
Several Feature Selec-tion algorithms were employed.
The QualityEstimation problem was addressed both as aregression task and as a discretised classifi-cation task, but the latter did not generalisewell on the unseen testset.
The most success-ful regression methods had an RMSE of 0.86and were trained with a feature set given byCorrelation-based Feature Selection.
Indica-tions that RMSE is not always sufficient formeasuring performance were observed.1 IntroductionAs Machine Translation (MT) gradually gains a po-sition into production environments, the need for es-timating the quality of its output is increasing.
Vari-ous use cases refer to it as input assessment for Hu-man Post-editing, as an extension for Hybrid MT orSystem Combination, or even a method for improv-ing components of existing MT systems.With the current submission we are trying toaddress the problem of assigning a quality scoreto a single MT output per source sentence.
Pre-vious work includes regression methods for in-dicating a binary value of correctness (Quirk,2001; Blatz et al, 2004; Ueffing and Ney, 2007),human-likeness (Gamon et al, 2005) or continu-ous scores (Specia et al, 2009).
As we also workwith continuous scores, we are making an effortto combine previous feature acquisition sources,such as language modelling (Raybaud et al, 2009),language fluency checking (Parton et al, 2011),parsing (Sa?nchez-Martinez, 2011; Avramidis et al,2011) and decoding statistics (Specia et al, 2009;Avramidis, 2011).
The current submission combinessuch previous observations in a combinatory experi-mentation on feature sets, feature selection methodsand Machine Learning (ML) algorithms.The structure of the submission is as follows: Theapproach is defined and the methods are describedin section 2, including features acquisition, featureselection and learning.
Section 3 includes informa-tion about the experiment setup whereas the resultsare discussed in Section 4.2 Methods2.1 Data and basic approachThis contribution has been built based on the datareleased for the Quality Estimation task of theWorkshop on Machine Translation (WMT) 2012(Callison-Burch et al, 2012).
The organizers pro-vided an English-to-Spanish development set and atest set of 1832 and 422 sentences respectively, de-rived from WMT09 and WMT10 datasets.
For eachsource sentence of the development set, participantswere offered one translation generated by a state-of-the-art phrase-based SMT system.
The quality ofeach SMT translation was assessed by human evalu-ators, who provided a quality score in the range 1-5.Additionally, statistics and processing informationfrom the execution of the SMT decoding algorithmwere given.The approach presented here is making use of thesource sentences, the SMT output and the qualityscores in order to follow a typical ML paradigm:84sentence suggestion.
.
.
los l?
?deres de la Unio?n han descrito como deducciones pol?
?tico .
.
.
number agreementLa articular y ideolo?gicamente convencido de asesino de masas .
.
.
transform ?y?
to ?e?Right after hearing about it, he described it as a ?challenge.
.
.
?
disambiguate -ingTable 1: Sample suggestions generated by rule-based language checking tools, observed in development dataeach source and target sentence of the developmentset are being analyzed to generate a feature vector.One training sample is formed out of the feature vec-tor and the quality score (i.e.
as a class value) of eachsentence.
A ML algorithm is consequently used totrain a model given the training samples.
The per-formance of each model is evaluated upon a part ofthe development set that was kept-out from training.2.2 Acquiring FeaturesThe features were obtained from two sources: thedecoding process and the analysis of the text of thesource and the target sentence.
The two steps areexplained below.2.2.1 Features from text analysisThe following features were generated with the useof tools for the statistical and/or linguistic analysisof the text.
The baseline features included:?
Tokens count: Count of tokens in the sourceand the translated sentence and their ratio, un-known words and also occurrences of the targetword within the translated sentence (averagedfor all words in the hypothesis - type/token ra-tio)?
IBM1-model lookup: Average number oftranslations per source word in the sentence,unweighted or weighted by the inverse fre-quency of each word in the source corpus?
Language modeling: Language model proba-bility of the source and translated sentence?
Corpus lookup: percentage of unigrams / bi-grams / trigrams in quartiles 1 and 4 of fre-quency (lower and higher frequency words) ina corpus of the source languageAdditionally, the following linguistically motivatedfeatures were also included:?
Parsing: PCFG Parse (Petrov et al, 2006) log-likelihood, size of n-best tree list, confidencefor the best parse, average confidence of allparse trees.
Ratios of the mentioned target fea-tures to the corresponding source features.?
Shallow grammatical match: The numberof occurences of particular node tags on boththe source and the target was counted on thePCFG parses.
Additionally, the ratio of theoccurences of each tag in the target sentenceby the corresponding occurences on the sourcesentence.?
Language quality check: Source and targetsentences were subject to automatic rule-basedlanguage quality checking, providing a widerange of quality suggestions concerning style,grammar and terminology, summed up in anoverall quality score.
The process employed786 rules for English and 70 rules for Spanish.We counted the occurences of every rule matchin each sentence and the number of characters itaffected.
Sample rule suggestions can be seenin Table 1.2.2.2 Features from the decoding processThe organisers provided a verbose output of the de-coding process, including probabilistic scores fromall steps of the execution of the translation search.We added the scores appearing once per sentence(i.e.
referring to the best hypothesis), whereas forthe ones being modified over the generation graph,their average (avg), variance (var) and standard de-viation (std) was calculated.
These features are:?
the log of the phrase translation probability(pC) and the phrase future cost estimate (c)?
the score component vector including the dis-tortion scores (d1...7), word penalty, translationscores (e.g.
a1: inverse phrase translation prob-ability, a2: inverse lexical weighting)852.3 Feature SelectionExperience has shown difficulties in including hun-dreds of features into training a statistical model.Several algorithms (such as Na?
?ve Bayes) requirestatistically-independent features.
For others, asearch space of hundreds of features may imposeincreased computational complexity, which is oftenunsustainable in the time and resources allocated.In these cases we therefore applied several commonFeature Selection approaches, in order to reduce theavailable features to an affordable number.We used the Feature Selection algorithms of Re-lieff (Kononenko, 1994), Information Gain andGain Ratio (Kullback and Leibler, 1951), andCorrelation-based Feature Selection (Hall, 2000).The latter is known for producing feature sets highlycorrelated with the class, yet uncorrelated with eachother; selection was done in two variations, greedystepwise and best first.The data were discretised according to the algo-rithm requirements and features were scored in a 10-fold cross-validation.2.4 Machine LearningWe tried to approach the issue with two distinctmodelling approaches, classification and regression.2.4.1 Classification algorithmsIn an effort to interpret Quality Estimation as aclassification problem, we expect to build modelsthat are able to assign a discrete value, as a mea-sure of sentence quality.
This bears some relation tothe way the quality scores were generated; humanswere asked to provide an (integer) quality score inthe range 1-5.
In our case, we try to build classifiersthat do the same, but are also able to assign valueswith smaller intervals.
For this purpose, we set up4 sub-experiments, where the class value in our datawas rounded up to intervals of 0.25, 0.5, 0.7 and 1.0respectively.In this part of the experiment we used the Na?
?veBayes, k-nearest-neighbours (kNN), Support VectorMachines (SVM) and Tree classification algorithms.Na?
?ve Bayes?
probabilities for our continuous fea-tures were estimated with locally weighted linear re-gression (Cleveland, 1979).2.4.2 Regression algorithmsRegression algorithms produce a model for di-rectly predicting a quality score with continu-ous values.
Experimentation here included Par-tial Least Squares Regression (Stone and Brooks,1990), Multivariate Adaptive Regression Splines ?MARS (Friedman, 1991), Lasso (Tibshirani, 1994)and Linear Regression.3 Experiment and Results3.1 ImplementationPCFG parsing features were generated on the out-put of the Berkeley Parser (Petrov and Klein,2007), trained over an English and a Spanish tree-bank (Mariona Taule?
and Recasens, 2008).
N-gram features have been generated with the SRILMtoolkit (Stolcke, 2002).
The Acrolinx IQ1 was usedto parse the source side, whereas the Language Tool2was applied on both sides.The feature selection and learning algorithmswere implemented with the Orange (Dems?ar et al,2004) and Weka (Hall et al, 2009) toolkits.3.2 Experiment structureThe methods explained in the previous section pro-vide a wide range of experiment parameters.
Con-sequently, we tried to extensively test all the possi-ble parameter combinations.
The development datawere separated in two sets, one ?training?
set andone ?keep-out?
set, used to test the predictions.
Inorder to give learners better coverage over the data,the development set was split in two ways (70%training - 30% test and 90% training - 10% test), sothat all experiments get performed under both set-tings.
The scores of these two were averaged3.3.3 ResultsThe small size of the dataset alowed for fast train-ing and testing of the discrete classification problem,where we could execute 370 experiments.
The re-gression problem was considerably slower, as only36 experiments concluded in time.1http://www.acrolinx.com (proprietary)2http://languagetool.org (open-source)3Given the disparity of the test sizes, it would have in prin-ciple been better to use a weighted average.
Though, this wouldnot have lead to significant differences in the results.865-fold avg 70-30%, 90-10% foldsalgorithm feat.
set discr.
CA AUC RMSE MAE intervalTree #17, #20 0.25 15.40 54.10 0.84 0.67 1.5 5.0Tree #23 0.25 14.60 53.50 0.85 0.68 2.0 5.0Tree #12 0.25 13.90 52.00 0.86 0.69 1.8 5.0Tree #4 0.25 14.50 53.70 0.86 0.69 2.0 5.0SVM #16 0.25 16.00 60.40 0.86 0.69 3.2 3.2kNN #22 0.25 12.30 55.50 1.00 0.78 2.0 5.0Tree #21 0.50 22.70 54.60 0.87 0.69 2.0 5.0SVM #19 0.50 22.40 60.20 0.91 0.73 2.8 5.0kNN #12 0.50 20.00 54.70 0.98 0.78 2.2 5.0Naive #6 0.50 21.20 59.40 0.99 0.76 1.2 5.0Tree #9 0.70 32.70 53.30 0.89 0.71 3.5 4.9kNN #12 0.70 28.20 56.10 0.93 0.73 2.5 4.9SVM #18 0.70 30.90 55.60 0.97 0.77 3.5 4.2Tree #22 1.00 40.30 55.70 0.90 0.71 2.0 5.0kNN #22 1.00 40.90 59.10 0.96 0.76 2.5 5.0Naive #23 1.00 41.00 65.50 1.02 0.78 1.2 5.0SVM #6 1.00 36.60 51.10 1.02 0.84 3.0 4.0Table 2: Indicative discretised classification results, sorted by best performance and discretisation interval.
Classifica-tion Accuracy (AC), Area Under Curve (AUC), Root Mean Square Error (RMSE) and Mean Average Error (MAE),Largest Error Percentage (LEP) and Smallest Error Percentage (SEP)Feature generation resulted (described in Section2.2) into 266 features, while 90 of them derived fromlanguage checking.
Feature selection suggested sev-eral feature sets containing between 30 and 80 fea-tures.
We ended up defining 22 feature sets, includ-ing the full feature set, the baseline feature set anda couple of manually selected feature sets.
Unfor-tunately, due to size restrictions, not all features canbe listed; though, indicative feature sets are listed inTable 5.The most important results of the classificationapproach can be seen in Table 2 and the results ofthe regression approach in Tables 3 (developmentset) and 4 (shared task test set).4 Discussion4.1 Machine Learning ConclusionsDiscrete classifiers (section 2.4.1) do not yield en-couraging accuracy, as acceptable levels of accu-racies appear only with a discretisation interval of1.00, which though cannot be accepted due to itshigh Root Mean Square Error (RMSE).
On the de-velopment keep-out set, the discretised Tree classi-fier seemingly outperforms all other methods (in-cluding the regression learners), since it yields aRMSE of 0.84, given several different feature vec-tors.
Unfortunately, when applied to the final un-known test data, these classifiers performed obvi-ously bad, providing the same single value for allsentences.
We could attribute this to overfitting vs.sparse data and consider how we can handle this bet-ter in further work.Another remarkable observation was the incapa-bility of the RMSE to objectively show the qual-ity of the model, in situations where the predictedvalues are very close or equal to the average ofall real values.
A Support Vector Machine withRMSE = 0.86 ranked 3rd among the classifiers, al-though it ?cheated?
by producing only the averagevalue: 3.25.
This leads to the conclusion that theselection of the best algorithm is not just dictatedby the lowest RMSE, but it should consider severalother indications such as the standard deviation.We therefore resort to the regression learners(section 2.4.2), whose scores are not worse, havinga RMSE of 0.855.
We have to notice that the four87avg.
70-30%, 90-10% foldsalgorithm f. set RMSE MAE intervalPLS #19 0.86 0.69 2.5 4.3Lasso #19 0.86 0.68 2.7 4.4Linear #19 0.86 0.68 2.6 4.5MARS #19 0.86 0.68 2.6 4.7PLS #18 0.86 0.69 2.7 4.4Linear #18 0.86 0.69 2.8 4.4Lasso #18 0.86 0.69 2.8 4.4MARS #16 0.87 0.69 2.4 4.6MARS #18 0.86 0.69 2.4 4.5MARS #4 0.86 0.69 3.4 4.5PLS #16 0.87 0.70 2.1 4.8PLS #4 0.87 0.70 2.1 5.4Linear #4 0.88 0.70 2.4 4.8Linear #16 0.88 0.70 1.4 4.9Lasso #4 0.88 0.70 1.9 5.3MARS #2 0.90 0.72 3.0 4.5Lasso #16 0.90 0.71 2.7 4.5Linear #2 0.90 0.72 3.0 4.0Lasso #2 0.90 0.72 3.0 4.0PLS #2 0.90 0.73 3.0 3.9Tree #21 1.08 0.86 1.5 5.0Tree #19 1.19 0.96 1.6 5.0Tree #16 1.23 0.98 1.6 5.0Tree #18 1.25 0.98 1.4 5.0Table 3: Regression results.
Root Mean Square Error(RMSE) and Mean Average Error (MAE), Largest ErrorPercentage (LEP) and Smallest Error Percentage (SEP).Bold face indicates submitted setsregression algorithms have comparable performancegiven the same features.The best-performing feature set (#19) which waschosen as the first submission (DFKI cfs-plsreg)trained with PLS regression, contains features in-dicated by Correlation-based Feature Selection, runwith bestfirst on a 10-fold cross-validation.
We usedthe features which were selected on the 100% or90% of the folds.
An equally best-performing fea-ture set (#18) has resulted from exactly the same fea-ture selection execution, but contains only featureswhich were selected in all folds.The second submission (DFKI grcfs-mars) waschosen to differentiate both the feature set and thelearning method, with respect to a decent interval.Feature set #16 is the result of the Correlation-basedlearner feat.
name RMSE MAEMARS #16 grcfs?mars 0.98 0.82PLS #19 cfs-plsreg 0.99 0.82Table 4: Results of the submitted methods on the officialtestsetFeature Selection, run in a greedy-stepwise mode.The regression was trained with MARS.The baseline feature set (#2) performed worse.Noticeable was the RMSE of the feature set #4, withfeatures selected based on their Gain Ratio, but wedid not submit this due to its very narrow interval.4.2 Feature conclusionsThe best performing feature set gives interestinghints on what worked as a best indication of trans-lation quality.
We would try to summarize them asfollows:?
The language checking of the source sen-tence detected complex or embedded sentences,which are often not handled properly by SMTdue to their complicated structure.?
The language checking of the target sentencedetected several agreement issues.?
Parsing provided of source and target countof verbs, nouns, adjectives and secondary sen-tences; with the assumption that translationsare relatively isomorphic, the loss of a verb ora noun or the inability to properly handle a sec-ondary sentence, would mean a considerablybad translation outcome.
The number of parsetrees generated for each sentence can be an in-dication of ambiguity.?
Punctuation (dots, commas) often indicates acomplex sentence structure.?
The most useful decoding features were the in-verse phrase translation probability (a1), the in-verse lexical weighting (a2), the phrase proba-bility (pC) and future cost estimate (c) as wellas statistics over their incremental values alongthe search graph.88featureset type source target#19 Baseline LM, %bi q4, punct LM, punctChecker complex sent, embedded sent pp v plural, nom adj mascParsing trees, CC, NP, NN, JJ, comma trees, S, CC, VB, VP, NN, JJ, dotDecoding avg(a2), a1, a2#16 Baseline LM, seen, punct, %uni q1, %bi q1,%bi q4, %tri q4LM, target occChecker score: style, spelling, quality;verb: agr, form, obj inf, close to subj;avoid parenth, complex sent,these those noun, np num agr,noun adj conf, repeat subj, wrong seq,wrong word, disamb that, use rel pron,use article, avoid dangling, repeat modal,use complementdouble punct, to too confusion,word repeat, det nom sing, pp v plural,pp v sing, nom adj plural,comma parenth space, nom adj fem,nom adj masc, nom adj sing,det nom fem, del nom sing,del nom masc, det nom plurParsing trees, S, CC, JJ, comma, VB, NP, NN, VP trees, S, CC, JJ, NP, VB, NN, VP, dot, PPDecoding avg(pC), avg(a1), std(pC), var(c), std(lm),avg(a2), d2, std(c), a1, a2Table 5: Indicative feature sets for the most successful quality estimation models.
Features explained at section 2.2AcknowledgmentsThis work has been developed within the TaraXU?project financed by TSB Technologiestiftung Berlin?
Zukunftsfonds Berlin, co-financed by the Euro-pean Union ?
European fund for regional develop-ment.
Many thanks to Lukas Poustka for technicalhelp on feature acquisition, to Melanie Siegel for theproprietary language checking tool, and to the re-viewers for the useful comments.ReferencesEleftherios Avramidis, Maja Popovic, David Vilar,Aljoscha Burchardt, and Maja Popovic?.
2011.
Evalu-ate with Confidence Estimation : Machine ranking oftranslation outputs using grammatical features.
Pro-ceedings of the Sixth Workshop on Statistical MachineTranslation, pages 65?70, July.Eleftherios Avramidis.
2011.
DFKI System Combina-tion with Sentence Ranking at ML4HMT-2011.
InProceedings of the International Workshop on UsingLinguistic Information for Hybrid Machine Transla-tion (LIHMT 2011) and of the Shared Task on ApplyingMachine Learning Techniques to Optimising the Di-vision of Labour in Hybrid Machine Translation (M.Sha.
Center for Language and Speech Technologiesand Applications (TALP), Technical University of Cat-alonia.John Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2004.
Confidence estimation formachine translation.
In Proceedings of the 20th in-ternational conference on Computational Linguistics,COLING ?04, Stroudsburg, PA, USA.
Association forComputational Linguistics.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 workshop on statistical machinetranslation.
In Proceedings of the Seventh Workshopon Statistical Machine Translation, Montreal, Canada,June.
Association for Computational Linguistics.William S Cleveland.
1979.
Robust locally weightedregression and smoothing scatterplots.
Journal of theAmerican statistical association, 74(368):829?836.Janez Dems?ar, Blaz Zupan, Gregor Leban, and TomazCurk.
2004.
Orange: From Experimental MachineLearning to Interactive Data Mining.
In Principles ofData Mining and Knowledge Discovery, pages 537?539.Jerome H. Friedman.
1991.
Multivariate Adaptive Re-gression Splines.
The Annals of Statistics, 19(1):1?67,March.Michael Gamon, Anthony Aue, and Martine Smets.2005.
Sentence-level MT evaluation without referencetranslations : Beyond language modeling.
Language,(2001):103?111.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H Witten.892009.
The WEKA data mining software: an update.SIGKDD Explorations, 11(1):10?18.Mark A Hall.
2000.
Correlation-based Feature Selec-tion for Discrete and Numeric Class Machine Learn-ing.
In Pat Langley, editor, Proceedings of 17th In-ternational Conference on Machine Learning, pages359?366.
Morgan Kaufmann Publishers Inc.Igor Kononenko.
1994.
Estimating attributes: anal-ysis and extensions of RELIEF.
In Proceedings ofthe European conference on machine learning on Ma-chine Learning, pages 171?182, Secaucus, NJ, USA.Springer-Verlag New York, Inc.S Kullback and R A Leibler.
1951.
On information andsufficiency.
Annals of Mathematical Statistics, 22:49?86.M Anto`nia Mart??
Mariona Taule?
and Marta Recasens.2008.
AnCora: Multilevel Annotated Corpora forCatalan and Spanish.
In Proceedings of the Sixth In-ternational Conference on Language Resources andEvaluation (LREC?08), Marrakech, Morocco, May.European Language Resources Association (ELRA).Kristen Parton, Joel Tetreault, Nitin Madnani, and MartinChodorow.
2011.
E-rating Machine Translation.
InProceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 108?115, Edinburgh, Scot-land, July.
Association for Computational Linguistics.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In In HLT-NAACL 07.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning Accurate, Compact, and Inter-pretable Tree Annotation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and 44th Annual Meeting of the Association forComputational Linguistics, pages 433?440, Sydney,Australia, July.
Association for Computational Lin-guistics.Christopher B Quirk.
2001.
Training a Sentence-LevelMachine Translation Confidence Measure.
Evalua-tion, pages 825?828.Sylvain Raybaud, Caroline Lavecchia, David Langlois,and Kamel Sma??li.
2009.
New Confidence Measuresfor Statistical Machine Translation.
Proceedings of theInternational Conference on Agents, pages 394?401.Felipe Sa?nchez-Martinez.
2011.
Choosing the best ma-chine translation system to translate a sentence by us-ing only source-language information.
In Mikel L For-cada, Heidi Depraetere, and Vincent Vandeghinste, ed-itors, Proceedings of the 15th Annual Conference ofthe European Associtation for Machine Translation,number May, pages 97?104, Leuve, Belgium.
Euro-pean Association for Machine Translation.Lucia Specia, M. Turchi, N. Cancedda, M. Dymetman,and N. Cristianini.
2009.
Estimating the Sentence-Level Quality of Machine Translation Systems.
In13th Annual Meeting of the European Association forMachine Translation (EAMT-2009), pages pp.
28?35,Barcelona, Spain.Andreas Stolcke.
2002.
SRILM ?
An Extensible Lan-guage Modeling Toolkit.
In Proceedings of the Sev-enth International Conference on Spoken LanguageProcessing, pages 901?904.
ISCA, September.M Stone and R J Brooks.
1990.
Continuum regres-sion: cross-validated sequentially constructed predic-tion embracing ordinary least squares, partial leastsquares and principal components regression.
Journalof the Royal Statistical Society Series B Methodologi-cal, 52(2):237?269.R Tibshirani.
1994.
Regression shrinkage and selectionvia the lasso.Nicola Ueffing and Hermann Ney.
2007.
Word-Level Confidence Estimation for Machine Translation.Computational Linguistics, 33(1):9?40.90
