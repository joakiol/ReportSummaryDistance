Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1493?1503,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsResolving Language and Vision Ambiguities Together: JointSegmentation & Prepositional Attachment Resolution in Captioned ScenesGordon Christie1,?, Ankit Laddha2,?, Aishwarya Agrawal1, Stanislaw Antol1Yash Goyal1, Kevin Kochersberger1, Dhruv Batra3,11Virginia Tech 2Carnegie Mellon University 3Georgia Institute of Technologyankit1991laddha@gmail.com{gordonac,aish,santol,ygoyal,kbk,dbatra}@vt.eduAbstractWe present an approach to simultaneously per-form semantic segmentation and prepositionalphrase attachment resolution for captionedimages.
Some ambiguities in language can-not be resolved without simultaneously rea-soning about an associated image.
If we con-sider the sentence ?I shot an elephant in mypajamas?, looking at language alone (and notusing common sense), it is unclear if it is theperson or the elephant wearing the pajamasor both.
Our approach produces a diverseset of plausible hypotheses for both semanticsegmentation and prepositional phrase attach-ment resolution that are then jointly rerankedto select the most consistent pair.
We showthat our semantic segmentation and preposi-tional phrase attachment resolution moduleshave complementary strengths, and that jointreasoning produces more accurate results thanany module operating in isolation.
Multiplehypotheses are also shown to be crucial to im-proved multiple-module reasoning.
Our vi-sion and language approach significantly out-performs the Stanford Parser (De Marneffe etal., 2006) by 17.91% (28.69% relative) and12.83% (25.28% relative) in two different ex-periments.
We also make small improvementsover DeepLab-CRF (Chen et al, 2015).1 IntroductionPerception and intelligence problems are hard.Whether we are interested in understanding an im-* Denotes equal contributionPASCAL Sentence DatasetConsistentNLP: Sentence ParsingAmbiguity: (woman on couch) vs (dog on couch)Output:      Parse Tree?A dog is standing next to a woman on a couch?Vision: Semantic SegmentationLabels:        Chairs, desks, etc.PersonCouchCouchPersonDogSolution#1Solution#MAmbiguity: ?
(dog ?next ?to ?woman) ?on ?couchvs ?dog ?next ?to ?
(woman ?o  ?
ouch)Ambiguity: ?
(dog ?ne t ?to ?woman) ?on ?
ouchvs ?dog ?next ?to ?
(woman ?on ?couch)Figure 1: Overview of our approach.
We propose a modelfor simultaneous 2D semantic segmentation and preposi-tional phrase attachment resolution by reasoning aboutsentence parses.
The language and vision modules eachproduce M diverse hypotheses, and the goal is to selecta pair of consistent hypotheses.
In this example the am-biguity to be resolved from the image caption is whetherthe dog is standing on or next to the couch.
Both modulesbenefit by selecting a pair of compatible hypotheses.age or a sentence, our algorithms must operate un-der tremendous levels of ambiguity.
When a hu-man reads the sentence ?I eat sushi with tuna?, itis clear that the preposition phrase ?with tuna?
mod-ifies ?sushi?
and not the act of eating, but this maybe ambiguous to a machine.
This problem of deter-mining whether a prepositional phrase (?with tuna?
)modifies a noun phrase (?sushi?)
or verb phrase(?eating?)
is formally known as Prepositional PhraseAttachment Resolution (PPAR) (Ratnaparkhi et al,1994).
Consider the captioned scene shown in Fig-1493ure 1.
The caption ?A dog is standing next to awoman on a couch?
exhibits a PP attachment am-biguity ?
?
(dog next to woman) on couch?
vs ?dognext to (woman on couch)?.
It is clear that havingaccess to image segmentations can help resolve thisambiguity, and having access to the correct PP at-tachment can help image segmentation.There are two main roadblocks that keep us fromwriting a single unified model (say a graphicalmodel) to perform both tasks: (1) Inaccurate Mod-els ?
empirical studies (Meltzer et al, 2005, Szeliskiet al, 2008, Kappes et al, 2013) have repeatedlyfound that models are often inaccurate and miscali-brated ?
their ?most-likely?
beliefs are placed on so-lutions far from the ground-truth.
(2) Search SpaceExplosion ?
jointly reasoning about multiple modal-ities is difficult due to the combinatorial explosion ofsearch space ({exponentially-many segmentations}?
{exponentially-many sentence-parses}).Proposed Approach and Contributions.
In thispaper, we address the problem of simultaneous ob-ject segmentation (also called semantic segmenta-tion) and PPAR in captioned scenes.
To the best ofour knowledge this is the first paper to do so.Our main thesis is that a set of diverse plausiblehypotheses can serve as a concise interpretable sum-mary of uncertainty in vision and language ?mod-ules?
(What does the semantic segmentation mod-ule see in the world?
What does the PPAR mod-ule describe?)
and form the basis for tractable jointreasoning (How do we reconcile what the semanticsegmentation module sees in the world with how thePPAR module describes it?
).Given our two modules with M hypotheses each,how can we integrate beliefs across the segmenta-tion and sentence parse modules to pick the bestpair of hypotheses?
Our key focus is consistency?
correct hypotheses from different modules will becorrect in a consistent way, but incorrect hypotheseswill be incorrect in incompatible ways.
Specifically,we develop a MEDIATOR model that scores pairs forconsistency and searches over all M2 pairs to pickthe highest scoring one.
We demonstrate our ap-proach on three datasets ?
ABSTRACT-50S (Vedan-tam et al, 2014), PASCAL-50S, and PASCAL-Context-50S (Mottaghi et al, 2014).
We show thatour vision+language approach significantly outper-forms the Stanford Parser (De Marneffe et al, 2006)by 20.66% (36.42% relative) for ABSTRACT-50S,17.91% (28.69% relative) for PASCAL-50S, andby 12.83% (25.28% relative) for PASCAL-Context-50S.
We also make small but consistent improve-ments over DeepLab-CRF (Chen et al, 2015).2 Related WorkMost works at the intersection of vision and NLPtend to be ?pipeline?
systems, where vision taskstake 1-best inputs from NLP (e.g., sentence pars-ings) without trying to improve NLP performanceand vice-versa.
For instance, Fidler et al (2013)use prepositions to improve object segmentation andscene classification, but only consider the most-likely parse of the sentence and do not resolve ambi-guities in text.
Analogously, Yatskar et al (2014) in-vestigate the role of object, attribute, and action clas-sification annotations for generating human-like de-scriptions.
While they achieve impressive results atgenerating descriptions, they assume perfect visionmodules to generate sentences.
Our work uses cur-rent (still imperfect) vision and NLP modules to rea-son about images and provided captions, and simul-taneously improve both vision and language mod-ules.
Similar to our philosophy, an earlier work byBarnard and Johnson (2005) used images to helpdisambiguate word senses (e.g.
piggy banks vs snowbanks).
In a more recent work, Gella et al (2016)studied the problem of reasoning about an image anda verb, where they attempt to pick the correct senseof the verb that describes the action depicted in theimage.
Berzak et al (2015) resolve linguistic ambi-guities in sentences coupled with videos that repre-sent different interpretations of the sentences.
Per-haps the work closest to us is Kong et al (2014),who leverage information from an RGBD imageand its sentential description to improve 3D seman-tic parsing and resolve ambiguities related to co-reference resolution in the sentences (e.g., what ?it?refers to).
We focus on a different kind of ambiguity?
the Prepositional Phrase (PP) attachment resolu-tion.
In the classification of parsing ambiguities, co-reference resolution is considered a discourse am-biguity (Poesio and Artstein, 2005) (arising out oftwo different words across sentences for the sameobject), while PP attachment is considered a syntac-tic ambiguity (arising out of multiple valid sentence1494structures) and is typically considered much moredifficult to resolve (Bach, 2016, Davis, 2016).A number of recent works have studied problemsat the intersection of vision and language, such asVisual Question Answering (Antol et al, 2015, Ge-man et al, 2014, Malinowski et al, 2015), Vi-sual Madlibs (Yu et al, 2015), and image caption-ing (Vinyals et al, 2015, Fang et al, 2015).
Ourwork falls in this domain with a key difference thatwe produce both vision and NLP outputs.Our work also has similarities with works on?spatial relation learning?
(Malinowski and Fritz,2014, Lan et al, 2012), i.e.
learning a visual rep-resentation for noun-preposition-noun triplets (?caron road?).
While our approach can certainly utilizesuch spatial relation classifiers if available, the focusof our work is different.
Our goal is to improve se-mantic segmentation and PPAR by jointly rerankingsegmentation-parsing solution pairs.
Our approachimplicitly learns spatial relationships for preposi-tions (?on?, ?above?)
but these are simply emergentlatent representations that help our reranker pick outthe most consistent pair of solutions.Our work utilizes a line of work (Batra et al,2012, Batra, 2012, Prasad et al, 2014) on pro-ducing diverse plausible solutions from probabilis-tic models, which has been successfully appliedto a number of problem domains (Guzman-Riveraet al, 2013, Yadollahpour et al, 2013, Gimpel etal., 2013, Premachandran et al, 2014, Sun et al,2015, Ahmed et al, 2015).3 ApproachIn order to emphasize the generality of our approach,and to show that our approach is compatible with awide class of implementations of semantic segmen-tation and PPAR modules, we present our approachwith the modules abstracted as ?black boxes?
thatsatisfy a few general requirements and minimal as-sumptions.
In Section 4, we describe each of themodules in detail, making concrete their respectivefeatures, and other details.3.1 What is a Module?The goal of a module is to take input variablesx ?
X (images or sentences), and predict out-put variables y ?
Y (semantic segmentation) andz ?
Z (prepositional attachment expressed in sen-tence parse).
The two requirements on a module arethat it needs to be able to produce scores S(y|x) forpotential solutions and a list of plausible hypothesesY = {y1,y2, .
.
.
,yM}.Multiple Hypotheses.
In order to be useful, theset Y of hypotheses must provide an accurate sum-mary of the score landscape.
Thus, the hypothesesshould be plausible (i.e., high-scoring) and mutu-ally non-redundant (i.e., diverse).
Our approach (de-scribed next) is applicable to any choice of diversehypothesis generators.
In our experiments, we usethe k-best algorithm of Huang and Chiang (2005)for the sentence parsing module and the DivMBestalgorithm (Batra et al, 2012) for the semantic seg-mentation module.
Once we instantiate the modulesin Section 4, we describe the diverse solution gener-ation in more detail.3.2 Joint Reasoning Across Multiple ModulesWe now show how to intergrate information fromboth segmentation and PPAR modules.
Recall thatour key focus is consistency ?
correct hypothesesfrom different modules will be correct in a consis-tent way, but incorrect hypotheses will be incorrectin incompatible ways.
Thus, our goal is to searchfor a pair (semantic segmentation, sentence parsing)that is mutually consistent.Let Y = {y1, .
.
.
,yM} denote the M seman-tic segmentation hypotheses and Z = {z1, .
.
.
, zM}denote the M PPAR hypotheses.MEDIATOR Model.
We develop a ?mediator?model that identifies high-scoring hypotheses acrossmodules in agreement with each other.
Concretely,we can express the MEDIATOR model as a fac-tor graph where each node corresponds to a mod-ule (semantic segmentation and PPAR).
Workingwith such a factor graph is typically completely in-tractable because each node y, z has exponentially-many states (image segmentations, sentence pars-ing).
As illustrated in Figure 2, in this factor-graphview, the hypothesis sets Y,Z can be considered?delta-approximations?
for reducing the size of theoutput spaces.Unary factors S(?)
capture the score/likelihoodof each hypothesis provided by the correspondingmodule for the image/sentence at hand.
Pairwisefactors C(?, ?)
represent consistency factors.
Impor-1495DeltaApproximationSemantic ?Segmentation Sentence ?
?ParsingDeltaApproximation Score(z)zScore(y)yS(yi) S(zj)yScore(z)zyScore(y)C(yi, zj)zFigure 2: Illustrative inter-module factor graph.
Each node takes exponentially-many or infinitely-many states and weuse a ?delta approximation?
to limit support.tantly, since we have restricted each module vari-ables to just M states, we are free to capture ar-bitrary domain-specific high-order relationships forconsistency, without any optimization concerns.
Infact, as we describe in our experiments, these con-sistency factors may be designed to exploit domainknowledge in fairly sophisticated ways.Consistency Inference.
We perform exhaustiveinference over all possible tuples.argmaxi,j?
{1,...,M}{M(yi, zj) = S(yi) + S(zj) + C(yi, zj)}.
(1)Notice that the search space with M hypotheseseach isM2.
In our experiments, we allow each mod-ule to take a different value for M , and typicallyuse around 10 solutions for each module, leading toa mere 100 pairs, which is easily enumerable.
Wefound that even with such a small set, at least one ofthe solutions in the set tends to be highly accurate,meaning that the hypothesis sets have relatively highrecall.
This shows the power of using a small set ofdiverse hypotheses.
For a large M , we can exploit anumber of standard ideas from the graphical modelsliterature (e.g.
dual decomposition or belief propaga-tion).
In fact, this is one reason we show the factorin Figure 2; there is a natural decomposition of theproblem into modules.Training MEDIATOR.
We can express the ME-DIATOR score as M(yi, zj) = w??
(x,yi, zj), asa linear function of score and consistency features?
(x,yi, zj) = [?S(yi);?S(zj);?C(yi, zj)] , where?S(?)
are the single-module (semantic segmentationand PPAR module) score features, and ?C(?, ?)
arethe inter-module consistency features.
We describethese features in detail in the experiments.
We learnthese consistency weights w from a dataset anno-tated with ground-truth for the two modules y, z.Let {y?, z?}
denote the oracle pair, composed ofthe most accurate solutions in the hypothesis sets.We learn the MEDIATOR parameters in a discrimina-tive learning fashion by solving the following Struc-tured SVM problem:minw,?ij12w?w + C?ij?ij (2a)s.t.
w??
(x,y?, z?)?
??
?Score of oracle tuple?
w??
(x,yi, zj)?
??
?Score of any other tuple?
1????Margin?
?ijL(yi, zj)?
??
?Slack scaled by loss?i, j ?
{1, .
.
.
,M}.
(2b)Intuitively, we can see that the constraint (2b) triesto maximize the (soft) margin between the score ofthe oracle pair and all other pairs in the hypothe-sis sets.
Importantly, the slack (or violation in themargin) is scaled by the loss of the tuple.
Thus,if there are other good pairs not too much worsethan the oracle, the margin for such tuples willnot be tightly enforced.
On the other hand, the mar-gin between the oracle and bad tuples will be verystrictly enforced.This learning procedure requires us to define theloss function L(yi, zj), i.e., the cost of predictinga tuple (semantic segmentation, sentence parsing).We use a weighted average of individual losses:L(yi, zj) = ?`(ygt,yi) + (1?
?
)`(zgt, zj) (3)The standard measure for evaluating semantic seg-mentation is average Jaccard Index (or Intersection-over-Union) (Everingham et al, 2010), while forevaluating sentence parses w.r.t.
their prepositionalphrase attachment, we use the fraction of preposi-tions correctly attached.
In our experiments, we re-port results with such a convex combination of mod-ule loss functions (for different values of ?
).14964 ExperimentsWe now describe the setup of our experiments, pro-vide implementation details of the modules, and de-scribe the consistency features.Datasets.
Access to rich annotated image +caption datasets is crucial for performing quanti-tative evaluations.
Since this is the first paperto study the problem of joint segmentation andPPAR, no standard datasets for this task exist sowe had to curate our own annotations for PPARon three image caption datasets ?
ABSTRACT-50S (Vedantam et al, 2014), PASCAL-50S (Vedan-tam et al, 2014) (expands the UIUC PASCALsentence dataset (Rashtchian et al, 2010) from 5captions per image to 50), and PASCAL-Context-50S (Mottaghi et al, 2014) (which uses the PAS-CAL Context image annotations and the same sen-tences as PASCAL-50S).
Our annotations are pub-licly available on the authors?
webpages.
To cu-rate the PASCAL-Context-50S PPAR annotations,we first select all sentences that have prepositionphrase attachment ambiguities.
We then plotted thedistribution of prepositions in these sentences.
Thetop 7 prepositions are used, as there is a large dropin the frequencies beyond these.
The 7 prepositionsare: ?on?, ?with?, ?next to?, ?in front of?, ?by?,?near?, and ?down?.
We then further sampled sen-tences to ensure uniform distribution across prepo-sitions.
We perform a similar filtering for PASCAL-50S and ABSTRACT-50S (using the top-6 preposi-tions for ABSTRACT-50S).
Details are in the sup-plement.
We consider a preposition ambiguous ifthere are at least two parsings where one of the twoobjects in the preposition dependency is the sameacross the two parsings while the other object is dif-ferent (e.g.
(dog on couch) and (woman on couch)).To summarize the statistics of all three datasets:1.
ABSTRACT-50S (Vedantam et al, 2014):25,000 sentences (50 per image) with 500images from abstract scenes made from cli-part.
Filtering for captions containing the top-6prepositions resulted in 399 sentences describ-ing 201 unique images.
These 6 prepositionsare: ?with?, ?next to?, ?on top of?, ?in frontof?, ?behind?, and ?under?.
Overall, there are502 total prepositions, 406 ambiguous preposi-tions, 80.88% ambiguity rate and 60 sentenceswith multiple ambiguous prepositions.2.
PASCAL-50S (Vedantam et al, 2014): 50,000sentences (50 per image) for the images in theUIUC PASCAL sentence dataset (Rashtchianet al, 2010).
Filtering for the top-7 preposi-tions resulted in a total of 30 unique images,and 100 image-caption pairs, where ground-truth PPAR were carefully annotated by twovision + NLP graduate students.
Overall,there are 213 total prepositions, 147 ambigu-ous prepositions, 69.01% ambiguity rate and35 sentences with multiple ambiguous prepo-sitions.3.
PASCAL-Context-50S (Mottaghi et al,2014): We use images and captions fromPASCAL-50S, but with PASCAL Contextsegmentation annotations (60 categories in-stead of 21).
This makes the vision task morechallenging.
Filtering this dataset for the top-7prepositions resulted in a total of 966 uniqueimages and 1,822 image-caption pairs.
Groundtruth annotations for the PPAR were collectedusing Amazon Mechanical Turk.
Workerswere shown an image and a prepositionalattachment (extracted from the correspondingparsing of the caption) as a phrase (?womanon couch?
), and asked if it was correct.
Ascreenshot of our interface is available in thesupplement.
Overall, there are 2,540 totalprepositions, 2,147 ambiguous prepositions,84.53% ambiguity rate and 283 sentences withmultiple ambiguous prepositions.Setup.
Single Module: We first show that visualfeatures help PPAR by using the ABSTRACT-50Sdataset, which contains clipart scenes where the ex-tent and position of all the objects in the scene isknown.
This allows us to consider a scenario with aperfect vision system.Multiple Modules: In this experiment we useimperfect language and vision modules, and showimprovements on the PASCAL-50S and PASCAL-Context-50S datasets.Module 1: Semantic Segmentation (SS) y. Weuse DeepLab-CRF (Chen et al, 2015) and Di-vMBest (Batra et al, 2012) to produce M diversesegmentations of the images.
To evaluate we useimage-level class-averaged Jaccard Index.Module 2: PP Attachment Resolution (PPAR)1497z.
We use a recent version (v3.3.1; released 2014)of the PCFG Stanford parser module (De Marn-effe et al, 2006, Huang and Chiang, 2005) to pro-duce M parsings of the sentence.
In addition tothe parse trees, the module can also output depen-dencies, which make syntactical relationships moreexplicit.
Dependencies come in the form depen-dency type(word1, word2), such as the prepositiondependency prep on(woman-8, couch-11) (the num-ber indicates the word position in sentence).
To eval-uate, we count the percentage of preposition attach-ments that the parse gets correct.Baselines:?
INDEP.
In our experiments, we compare ourproposed approach (MEDIATOR) to the highestscoring solution predicted independently fromeach module.
For semantic segmentation this isthe output of DeepLab-CRF (Chen et al, 2015)and for the PPAR module this is the 1-best out-put of the Stanford Parser (De Marneffe et al,2006, Huang and Chiang, 2005).
Since our hy-pothesis lists are generated by greedy M-Bestalgorithms, this corresponds to predicting the(y1, z1) tuple.
This comparison establishes theimportance of joint reasoning.
To the best ofour knowledge, there is no existing (or evennatural) joint model to compare to.?
DOMAIN ADAPTATION.
We learn a rerankeron the parses.
Note that domain adaptation isonly needed for PPAR since the Stanford parseris trained on Penn Treebank (Wall Street Jour-nal text) and not on text about images (such asimage captions).
Such domain adaptation is notnecessary for semantic segmentation.
This isa competitive single-module baseline.
Specifi-cally, we use the same parse-based features asour approach, and learn a reranker over the Mzparse trees (Mz = 10).Our approach (MEDIATOR) significantly outper-forms both baselines.
The improvements over IN-DEP show that joint reasoning produces more ac-curate results than any module (vision or language)operating in isolation.
The improvements over DO-MAIN ADAPTATION establish the source of im-provements is indeed vision, and not the rerankingstep.
Simply adapting the parse from its originaltraining domain (Wall Street Journal) to our domain(image captions) is not enough.Ablative Study.
Ours-CASCADE: This ablationstudies the importance of multiple hypothesis.
Foreach module (say y), we feed the single-best out-put of the other module z1 as input.
Each modulelearns its own weight w using exactly the same con-sistency features and learning algorithm as MEDI-ATOR and predicts one of the plausible hypothesesy?CASCADE = argmaxy?Y w??
(x,y, z1).
This ab-lation of our system is similar to (Heitz et al, 2008)and helps us in disentangling the benefits of multiplehypothesis and joint reasoning.Finally, we note that Ours-CASCADE can beviewed as special cases of MEDIATOR.
Let MEDI-ATOR-(My,Mz) denote our approach run with Myhypotheses for the first module and Mz for the sec-ond.
Then INDEP corresponds to MEDIATOR-(1, 1)and CASCADE corresponds to predicting the y so-lution from MEDIATOR-(My, 1) and the z solutionfrom MEDIATOR-(1,Mz).
To get an upper-boundon our approach, we report oracle, the accuracyof the most accurate tuple in 10?
10 tuples.In the main paper, our results are presented whereMEDIATOR was trained with equally weighted loss(?
= 0.5), but we provide additional results forvarying values of ?
in the supplement.MEDIATOR and Consistency Features.
Recallthat we have two types of features ?
(1) score fea-tures ?S(yi) and ?S(zj), which try to capture howlikely solutions yi and zj are respectively, and (2)consistency features ?C(yi, zj), which capture howconsistent the PP attachments in zj are with thesegmentation in yi.
For each (object1, preposi-tion, object2) in zj , we compute 6 features betweenobject1 and object2 segmentations in yi.
Since thehumans writing the captions may use multiple syn-onymous words (e.g.
dog, puppy) for the same vi-sual entity, we use word2vec (Mikolov et al, 2013)similarities to map the nouns in the sentences to thecorresponding dataset categories.?
Semantic Segmentation Score Features(?S(yi)) (2-dim): We use ranks and solutionscores from DeepLab-CRF (Chen et al, 2015).?
PPAR Score Features (?S(zi)) (9-dim): Weuse ranks and the log probability of parsesfrom (De Marneffe et al, 2006), and 7 binaryindicators for PASCAL (6 for ABSTRACT-50S) denoting which prepositions are presentin the parse.1498Figure 3: Example on PASCAL-50S (?A dog is stand-ing next to a woman on a couch.?).
The ambiguity in thissentence ?
(dog next to woman) on couch?
vs ?dog next to(woman on couch)?.
We calculate the horizontal and ver-tical distances between the segmentation centers of ?per-son?
and ?couch?
and between the segmentation centersof ?dog?
and ?couch?.
We see that the ?dog?
is much fur-ther below the couch (53.91) than the woman (2.65).
So,if the MEDIATOR model learned that ?on?
means the firstobject is above the second object, we would expect it tochoose the ?person on couch?
preposition parsing.?
Inter-Module Consistency Features (56-dim): For each of the 7 prepositions, 8 featuresare calculated:?
One feature is the Euclidean distancebetween the center of the segmentationmasks of the two objects connected bythe preposition.
These two objects in thesegmentation correspond to the categorieswith which the soft similarity of the twoobjects in the sentence is highest amongall PASCAL categories.?
Four features capture max{0, (normalized-directional-distance)}, where directional-distance measures above/below/left/rightdisplacements between the two objects inthe segmentation, and normalization in-volves dividing by height/width.?
One feature is the ratio of sizes betweenobject1 and object2 in the segmentation.?
Two features capture the word2vec sim-ilarity between the two objects in PPAR(say ?puppy?
and ?kitty?)
with their mostsimilar PASCAL category (say ?dog?
and?cat?
), where these features are 0 if the cat-egories are not present in segmentation.A visual illustration for some of these featuresfor PASCAL can be seen in Figure 3.
In thecase where an object parsed from zj is notpresent in the segmentation yi, the distancefeatures are set to 0.
The ratio of areas fea-tures (area of smaller object / area of larger ob-ject) are also set to 0 assuming that the smallerobject is missing.
In the case where an ob-ject has two or more connected components inthe segmentation, the distances are computedw.r.t.
the centroid of the segmentation and thearea is computed as the number of pixels inthe union of the instance segmentation masks.We also calculate 20 features for PASCAL-50Sand 59 features for PASCAL-Context-50S thatcapture that consistency between yi and zj , interms of presence/absence of PASCAL cate-gories.
For each noun in PPAR we computeits word2vec similarity with all PASCAL cat-egories.
For each of the PASCAL categories,the feature is the sum of similarities (with thePASCAL category) over all nouns if the cate-gory is present in segmentation, and is -1 timesthe sum of similarities over all nouns otherwise.This feature set was not used for ABSTRACT-50S, since these features were intended to helpimprove the accuracy of the semantic segmen-tation module.
For ABSTRACT-50S, we onlyuse the 5 distance features, resulting in a 30-dim feature vector.4.1 Single-Module ResultsWe performed a 10-fold cross-validation on theABSTRACT-50S dataset to pick M (=10) and theweight on the hinge-loss for MEDIATOR (C).
Theresults are presented in Table 1.
Our approach sig-nificantly outpeforms 1-best outputs of the Stan-ford Parser (De Marneffe et al, 2006) by 20.66%(36.42% relative).
This shows a need for diverse hy-potheses and reasoning about visual features whenpicking a sentence parse.
oracle denotes the bestachievable performance using these 10 hypotheses.Module StanfordParserDomainAdaptation Ours oraclePPAR 56.73 57.23 77.39 97.53Table 1: Results on our subset of ABSTRACT-50S.4.2 Multiple-Module ResultsWe performed 10-fold cross-val for our results ofPASCAL-50S and PASCAL-Context-50S, with 81499(a) ABSTRACT-50S (b) PASCAL-50S (c) PASCAL-Context-50SFigure 4: (a) Validation accuracies for different values of M on ABSTRACT-50S, (b) for different values of My,Mzon PASCAL-50S, (c) for different values of My,Mz on PASCAL-Context-50S.PASCAL-50S PASCAL-Context-50SInstance-LevelJaccard Index PPAR Acc.
AverageInstance-LevelJaccard Index PPAR Acc.
AverageDeepLab-CRF 66.83 - - 43.94 - -Stanford Parser - 62.42 - - 50.75 -Average - - 64.63 - - 47.345Domain Adaptation - 72.08 - - 58.32 -Ours CASCADE 67.56 75.00 71.28 43.94 63.58 53.76Ours MEDIATOR 67.58 80.33 73.96 43.94 63.58 53.76oracle 69.96 96.50 83.23 49.21 75.75 62.48Table 2: Results on our subset of the PASCAL-50S and PASCAL-Context-50S datasets.
We are able to significantlyoutperform the Stanford Parser and make small improvements over DeepLab-CRF for PASCAL-50S.train folds, 1 val fold, and 1 test fold, wherethe val fold was used to pick My, Mz, and C. Fig-ure 4 shows the average combined accuracy on val,which was found to be maximal atMy = 5,Mz = 3for PASCAL-50S, and My = 1,Mz = 10 forPASCAL-Context-50S, which are used at test time.We present our results in Table 2.
Ourapproach significantly outperforms the StanfordParser (De Marneffe et al, 2006) by 17.91%(28.69% relative) for PASCAL-50S, and 12.83%(25.28% relative) for PASCAL-Context-50S.
Wealso make small improvements over DeepLab-CRF (Chen et al, 2015) in the case of PASCAL-50S.To measure statistical significance of our results, weperformed paired t-tests between MEDIATOR andINDEP.
For both modules (and average), the nullhypothesis (that the accuracies of our approach andbaseline come from the same distribution) can besuccessfully rejected at p-value 0.05.
For sake ofcompleteness, we also compared MEDIATOR withour ablated system (CASCADE) and found statisti-cally significant differences only in PPAR.These results demonstrate a need for each mod-ule to produce a diverse set of plausible hypothe-ses for our MEDIATOR model to reason about.
Inthe case of PASCAL-Context-50S, MEDIATOR per-forms identical to CASCADE since My is chosenas 1 (which is the CASCADE setting) in cross-validation.
Recall that MEDIATOR is a larger modelclass than CASCADE (in fact, CASCADE is a specialcase of MEDIATOR with My = 1).
It is interestingto see that the large model class does not hurt, andMEDIATOR gracefully reduces to a smaller capac-ity model (CASCADE) if the amount of data is notenough to warrant the extra capacity.
We hypothe-size that in the presence of more training data, cross-validation may pick a different setting of My andMz, resulting in full utilization of the model capac-ity.
Also note that our domain adaptation baselineachieved an accuracy higher than MAP/Stanford-Parser, but significantly lower than our approach forboth PASCAL-50S and PASCAL-Context-50S.
Wealso performed this for our single-module experi-ment and picked Mz (=10) with cross-validation,1500on by withFigure 5: Visualizations for3 different prepositions (red =high scores, blue = low scores).We can see that our modelhas implicitly learned spatial ar-rangements unlike other spatialrelation learning (SRL) works.PASCAL-50S PASCAL-Context-50SFeature set Instance-LevelJaccard Index PPAR Acc.
PPAR Acc.All features 67.58 80.33 63.58Drop all consistency 66.96 66.67 61.47Drop Euclidean distance 67.27 77.33 63.77Drop directional distance 67.12 78.67 63.63Drop word2vec 67.58 78.33 62.72Drop category presence 67.48 79.25 61.19Table 3: Ablation study of different feature combinations.
Only PPAR Acc.
isshown for PASCAL-Context-50S because My = 1.which resulted in an accuracy of 57.23%.
Again,this is higher than MAP/Stanford-Parser (56.73%),but significantly lower than our approach (77.39%).Clearly, domain adaptation alone is not sufficient.We also see that oracle performance is fairly high,suggesting that when there is ambiguity and roomfor improvement, MEDIATOR is able to rerank ef-fectively.Ablation Study for Features.
Table 3 displaysresults of an ablation study on PASCAL-50S andPASCAL-Context-50S to show the importance ofthe different features.
In each row, we retain themodule score features and drop a single set of con-sistency features.
We can see all consistency fea-tures contribute to the performance of MEDIATOR.Visualizing Prepositions.
Figure 5 shows a vi-sualization for what our MEDIATOR model has im-plicitly learned about 3 prepositions (?on?, ?by?,?with?).
These visualizations show the score ob-tained by taking the dot product of distance fea-tures (Euclidean and directional) between object1and object2 connected by the preposition with thecorresponding learned weights of the model, consid-ering object2 to be at the center of the visualization.Notice that these were learned without explicit train-ing for spatial learning as in spatial relation learning(SRL) works (Malinowski and Fritz, 2014, Lan etal., 2012).
These were simply recovered as an in-termediate step towards reranking SS + PPAR hy-potheses.
Also note that SRL cannot handle multi-ple segmentation hypotheses, which our work showsare important (Table 2 CASCADE).
In addition, ourapproach is more general.5 Discussions and ConclusionWe presented an approach to the simultaneous rea-soning about prepositional phrase attachment res-olution of captions and semantic segmentation inimages that integrates beliefs across the modulesto pick the best pair of a diverse set of hypothe-ses.
Our full model (MEDIATOR) significantlyimproves the accuracy of PPAR over the Stan-ford Parser by 17.91% for PASCAL-50S and by12.83% for PASCAL-Context-50S, and achievesa small improvement on semantic segmentationover DeepLab-CRF for PASCAL-50S.
These resultsdemonstrate a need for information exchange be-tween the modules, as well as a need for a diverse setof hypotheses to concisely capture the uncertaintiesof each module.
Large gains in PPAR validate ourintuition that vision is very helpful for dealing withambiguity in language.
Furthermore, we see evenlarger gains are possible from the oracle accuracies.While we have demonstrated our approach ona task involving simultaneous reasoning about lan-guage and vision, our approach is general and canbe used for other applications.
Overall, we hope ourapproach will be useful in a number of settings.AcknowledgementsWe thank Larry Zitnick, Mohit Bansal, Kevin Gim-pel, and Devi Parikh for helpful discussions, sugges-tions, and feedback included in this work.
A major-ity of this work was done while AL was an internat Virginia Tech.
This work was partially supportedby a National Science Foundation CAREER award,an Army Research Office YIP Award, an Office ofNaval Research grant N00014-14-1-0679, and GPUdonations by NVIDIA, all awarded to DB.
GC wassupported by DTRA grant HDTRA1-13-1-0015 pro-vided by KK.
The views and conclusions containedherein are those of the authors and should not be in-terpreted as necessarily representing official policiesor endorsements, either expressed or implied, of theU.S.
Government or any sponsor.1501ReferencesFaruk Ahmed, Dany Tarlow, and Dhruv Batra.
2015.Optimizing Expected Intersection-over-Union withCandidate-Constrained CRFs.
In ICCV.Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Mar-garet Mitchell, Dhruv Batra, C. Lawrence Zitnick, andDevi Parikh.
2015.
VQA: Visual Question Answer-ing.
In ICCV.Kent Bach.
2016.
Routledge encyclopedia of phi-losophy entry.
http://online.sfsu.edu/kbach/ambguity.html.Kobus Barnard and Matthew Johnson.
2005.
Word SenseDisambiguation with Pictures.
Artificial Intelligence,167.Dhruv Batra, Payman Yadollahpour, Abner Guzman-Rivera, and Gregory Shakhnarovich.
2012.
Di-verse M-Best Solutions in Markov Random Fields.
InECCV.Dhruv Batra.
2012.
An Efficient Message-Passing Algo-rithm for the M-Best MAP Problem.
In UAI.Yevgeni Berzak, Andrei Barbu, Daniel Harari, BorisKatz, and Shimon Ullman.
2015.
Do You See WhatI Mean?
Visual Resolution of Linguistic Ambiguities.In EMNLP.Liang-Chieh Chen, George Papandreou, Iasonas Kokki-nos, Kevin Murphy, and Alan L Yuille.
2015.
Seman-tic Image Segmentation with Deep Convolutional Netsand Fully Connected CRFs.
In ICLR.Ernest Davis.
2016.
Notes on ambiguity.http://cs.nyu.edu/faculty/davise/ai/ambiguity.html.Marie-Catherine De Marneffe, Bill MacCartney, andChristopher D Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InLREC.M.
Everingham, L. Van Gool, C. K. I. Williams, J. Winn,and A. Zisserman.
2010.
The Pascal Visual ObjectClasses (VOC) Challenge.
IJCV, 88(2).Hao Fang, Saurabh Gupta, Forrest N. Iandola, Ru-pesh Srivastava, Li Deng, Piotr Dolla?r, JianfengGao, Xiaodong He, Margaret Mitchell, John C. Platt,C.
Lawrence Zitnick, and Geoffrey Zweig.
2015.From Captions to Visual Concepts and Back.
InCVPR.Sanja Fidler, Abhishek Sharma, and Raquel Urtasun.2013.
A Sentence is Worth a Thousand Pixels.
InCVPR.Spandana Gella, Mirella Lapata, and Frank Keller.
2016.Unsupervised Visual Sense Disambiguation for Verbsusing Multimodal Embeddings.
In NAACL HLT.Donald Geman, Stuart Geman, Neil Hallonquist, andLaurent Younes.
2014.
A Visual Turing Test for Com-puter Vision Systems.
In PNAS.K.
Gimpel, D. Batra, C. Dyer, and G. Shakhnarovich.2013.
A Systematic Exploration of Diversity in Ma-chine Translation.
In EMNLP.Abner Guzman-Rivera, Pushmeet Kohli, and Dhruv Ba-tra.
2013.
DivMCuts: Faster Training of StructuralSVMs with Diverse M-Best Cutting-Planes.
In AIS-TATS.Geremy Heitz, Stephen Gould, Ashutosh Saxena, andDaphne Koller.
2008.
Cascaded Classification Mod-els: Combining Models for Holistic Scene Under-standing.
In NIPS.Liang Huang and David Chiang.
2005.
Better k-bestParsing.
In IWPT, pages 53?64.Jo?rg H. Kappes, Bjoern Andres, Fred A. Hamprecht,Christoph Schno?rr, Sebastian Nowozin, Dhruv Batra,Sungwoong Kim, Bernhard X. Kausler, Jan Lellmann,Nikos Komodakis, and Carsten Rother.
2013.
A Com-parative Study of Modern Inference Techniques forDiscrete Energy Minimization Problems.
In CVPR.Chen Kong, Dahua Lin, Mohit Bansal, Raquel Urtasun,and Sanja Fidler.
2014.
What are you talking about?Text-to-Image Coreference.
In CVPR.Tian Lan, Weilong Yang, Yang Wang, and Greg Mori.2012.
Image Retrieval with Structured Object QueriesUsing Latent Ranking SVM.
In ECCV.Mateusz Malinowski and Mario Fritz.
2014.
Apooling approach to modelling spatial relations forimage retrieval and annotation.
arXiv preprintarXiv:1411.5190.Mateusz Malinowski, Marcus Rohrbach, and Mario Fritz.2015.
Ask Your Neurons: A Neural-based Approachto Answering Questions about Images.
In ICCV.Talya Meltzer, Chen Yanover, and Yair Weiss.
2005.Globally Optimal Solutions for Energy Minimizationin Stereo Vision Using Reweighted Belief Propaga-tion.
In ICCV.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient Estimation of Word Represen-tations in Vector Space.
In ICLR.Roozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-GyuCho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun,and Alan Yuille.
2014.
The Role of Context for ObjectDetection and Semantic Segmentation in the Wild.
InCVPR.Massimo Poesio and Ron Artstein.
2005.
Annotating(Anaphoric) ambiguity.
In Corpus Linguistics Confer-ence.Adarsh Prasad, Stefanie Jegelka, and Dhruv Batra.
2014.Submodular meets Structured: Finding Diverse Sub-sets in Exponentially-Large Structured Item Sets.
InNIPS.Vittal Premachandran, Daniel Tarlow, and Dhruv Batra.2014.
Empirical Minimum Bayes Risk Prediction:1502How to extract an extra few% performance from visionmodels with just three more parameters.
In CVPR.Cyrus Rashtchian, Peter Young, Micah Hodosh, and JuliaHockenmaier.
2010.
Collecting Image AnnotationsUsing Amazon?s Mechanical Turk.
In NAACL HLTWorkshop on Creating Speech and Language Datawith Amazon?s Mechanical Turk.Adwait Ratnaparkhi, Jeff Reynar, and Salim Roukos.1994.
A Maximum Entropy Model for PrepositionalPhrase Attachment.
In Proceedings of the workshopon Human Language Technology.
ACL.Qing Sun, Ankit Laddha, and Dhruv Batra.
2015.
Ac-tive Learning for Structured Probabilistic Models WithHistogram Approximation.
In CVPR.Richard Szeliski, Ramin Zabih, Daniel Scharstein, OlgaVeksler, Vladimir Kolmogorov, Aseem Agarwala,Marshall Tappen, and Carsten Rother.
2008.
A Com-parative Study of Energy Minimization Methods forMarkov Random Fields with Smoothness-Based Pri-ors.
PAMI, 30(6).Ramakrishna Vedantam, C. Lawrence Zitnick, and DeviParikh.
2014.
CIDEr: Consensus-based Image De-scription Evaluation.
In CVPR.Oriol Vinyals, Alexander Toshev, Samy Bengio, and Du-mitru Erhan.
2015.
Show and Tell: A Neural ImageCaption Generator.
In CVPR.Payman Yadollahpour, Dhruv Batra, and GregShakhnarovich.
2013.
Discriminative Re-ranking ofDiverse Segmentations.
In CVPR.Mark Yatskar, Michel Galley, Lucy Vanderwende, andLuke Zettlemoyer.
2014.
See No Evil, Say No Evil:Description Generation from Densely Labeled Images.In Lexical and Computational Semantics.Licheng Yu, Eunbyung Park, Alexander C. Berg, andTamara L. Berg.
2015.
Visual Madlibs: Fill in theBlank Description Generation and Question Answer-ing.
In ICCV.1503
