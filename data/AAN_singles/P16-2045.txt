Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 275?280,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsPhrase-Level Combination of SMT and TMUsing Constrained Word LatticeLiangyou Li and Andy Way and Qun LiuADAPT Centre, School of ComputingDublin City UniversityDublin 9, Ireland{liangyouli,away,qliu}@computing.dcu.ieAbstractConstrained translation has improved sta-tistical machine translation (SMT) bycombining it with translation memory(TM) at sentence-level.
In this paper, wepropose using a constrained word lattice,which encodes input phrases and TM con-straints together, to combine SMT and TMat phrase-level.
Experiments on English?Chinese and English?French show thatour approach is significantly better thanprevious combination methods, includingsentence-level constrained translation anda recent phrase-level combination.1 IntroductionThe combination of statistical machine translation(SMT) and translation memory (TM) has provento be beneficial in improving translation qualityand has drawn attention from many researchers(Bic?ici and Dymetman, 2008; He et al, 2010;Koehn and Senellart, 2010; Ma et al, 2011; Wanget al, 2013; Li et al, 2014).
Among variouscombination approaches, constrained translation(Koehn and Senellart, 2010; Ma et al, 2011) isa simple one and can be readily adopted.Given an input sentence, constrained translationretrieves similar TM instances and uses matchedsegments to constrain the translation space of theinput by generating a constrained input.
Then anSMT engine is used to search for a complete trans-lation of the constrained input.Despite its effectiveness in improving SMT,previous constrained translation works at thesentence-level, which means that matched seg-ments in a TM instance are either all adopted orall abandoned regardless of their individual qual-ity (Wang et al, 2013).
In this paper, we proposea phrase-level constrained translation approachwhich uses a constrained word lattice to encodethe input and constraints from the TM together andallows a decoder to directly optimize the selectionof constraints towards translation quality (Section2).We conduct experiments (Section 3) onEnglish?Chinese (EN?ZH) and English?French(EN?FR) TM data.
Results show that our methodis significantly better than previous combinationapproaches, including sentence-level constrainedmethods and a recent phrase-level combinationmethod.
Specifically, it improves the BLEU (Pap-ineni et al, 2002) score by up to +5.5% on EN?ZHand +2.4% on EN?FR over a phrase-based base-line (Koehn et al, 2003) and decreases the TER(Snover et al, 2006) error by up to -4.3%/-2.2%,respectively.2 Constrained Word LatticeA word lattice G = (V,E,?, ?, ?)
is a directedacyclic graph, where V is a set of nodes, includinga start point and an end point, E ?
V ?
V is aset of edges, ?
is a set of symbols, a label function?
: E ?
?
and a weight function ?
: E ?
R.1A constrained word lattice is a special case of aword lattice, which extends ?
with extra symbols(i.e.
constraints).A constraint is a target phrase which will ap-pear in the final translation.
Constraints can be ob-tained in two ways: addition (Ma et al, 2011) andsubtraction (Koehn and Senellart, 2010).2Figure1 exemplifies the differences between them.The construction of a constrained lattice is verysimilar to that of a word lattice, except that weneed to label some edges with constraints.
Thegeneral process is:1In this paper, edge weights are set to 1.2Addition means that constraints are added from a TM tar-get to an input, while subtraction means that some constraintsare removed from the TM target.275Figure 1: An example of generating a constrained input in two ways: addition and subtraction.
Whileaddition replaces an input phrase with a target phrase from a TM instance (an example is marked bylighter gray), subtraction removes mismatched target words and inserts mismatched input words (darkergray).
Constraints are specified by <>.
Sentences are taken from Koehn and Senellart (2010).1.
Building an initial lattice for an input sen-tence.
This produces a chain.2.
Adding phrasal constraints into the latticewhich produces extra nodes and edges.Figure 2 shows an example of a constrained latticefor the sentence in Figure 1.In the rest of this section, we explain how to useaddition and subtraction to build a constrained lat-tice and the decoder for translating the lattice.
No-tations we use in this section are: an input f and aTM instance ?f?, e?, A?where f?is the TM source,e?is the TM target and A is a word alignment be-tween f?and e?.2.1 AdditionIn addition, matched input words are directly re-placed by their translations from a retrieved TM,which means that addition follows the word orderof an input sentence.
This property makes it easyto obtain constraints for an input phrase.For an input phrase f , we firstly find its matchedphrase f?from f?via string edits3between f andf?, so that f = f?.
Then, we extract its translatione?from e?, which is consistent with the alignmentA (Och and Ney, 2004).To build a lattice using addition, we directly adda new edge to the lattice which covers f and islabeled by e?.
For example, dash-dotted lines inFigure 2 are labeled by constraints from addition.3String edits, as used in the Levenshtein distance (Leven-shtein, 1966), include match, substitution, deletion, and in-sertion with a priority in this paper: match > substitution> deletion > insertion.2.2 SubtractionIn subtraction, mismatched input words in f areinserted into e?and mismatched words in e?areremoved.
The inserted position is determined byA.
The advantage of subtraction is that it keepsthe word order of e?.
This is important since thereordering of target words is one of the fundamen-tal problems in SMT, especially for language pairswhich have a high degree of syntactic reordering.However, this property makes it hard to builda lattice from subtraction, as ?
different from theaddition ?
subtraction does not directly producea constraint for an input phrase.
Thus, for somegenerated constraints, there is not a specific cor-responding phrase in the input.
In addition, whenadding a constraint to the lattice, we need to con-sider its context so that the lattice keeps targetword order.To solve this problem, in this paper we proposeto segment an input sentence into a sequence ofphrases according to information from a matchedTM (i.e.
the string edit and word alignment) andthen create a constrained input for each phrase andadd them to the lattice.Formally, we produce a monotonic segmen-tation,?f1, f?1, e?1??
?
?
?fN, f?N, e?N?, for eachsentence triple: ?f, f?, e??.
Each?fi, f?i, e?i?tu-ple is obtained in two phases: (1) According tothe alignment A, f?iand e?iare produced.
(2)Based on string edits between f and f?, fiis rec-ognized.
The resulting tuple is subject to severalrestrictions:1.
Each < f?i, e?i> is consistent with the wordalignment A and at least one word in f?iisaligned to words in e?i.276Figure 2: An example of constructing a constrained word lattice for the sentence in Figure 1.
Dash-dottedlines are generated by addition and dotted lines are generated by subtraction.
Constraints are specifiedby <>.2.
Each boundary word in f?iis either the firstword or the last word of f?or aligned to atleast one word in e?, so that mismatched in-put words in fiwhich are unaligned can findtheir position in the current tuple.3.
The string edit for the first word of fi, wherei 6= 1, is not ?deletion?.
That means the firstword is not an extra input word.
This is be-cause, in subtraction, the inserted position ofa mismatched unaligned word depends on thealignment of the word before it.4.
No smaller tuples may be extracted withoutviolating restrictions 1?3.
This allows us toobtain a unique segmentation where each tu-ple is minimal.After obtaining the segmentation, we create aconstrained input for each fiusing subtractionand add it to the lattice by creating a path cover-ing fi.
The path contains one or more edges, eachof which is labeled either by an input word or aconstraint in the constrained input.2.3 DecodingThe decoder for integrating word lattices into thephrase-based model (Koehn et al, 2003) workssimilarly to the phrase-based decoder, except thatit tracks nodes instead of words (Dyer et al, 2008):given the topological order of nodes in a lattice,the decoder builds a translation hypothesis fromleft to right by selecting a range of untranslatednodes.The decoder for a constrained lattice works sim-ilarly except that, for a constrained edge, the de-coder can only build its translation directly fromthe constraint.
For example, in Figure 2, the trans-lation of the edge ?1 ?
5?
is ?, le texte dudeuxi`eme alin?ea?.EN?ZH Sentences W/S (EN) W/S (ZH)Train 84,871 13.5 13.8Dev 734 14.3 14.5Test 943 17.4 17.4EN?FR Sentences W/S (EN) W/S (FR)Train 751,548 26.9 29.3Dev 2,665 26.8 29.2Test 2,655 27.1 29.4Table 1: Summary of English?Chinese (EN?ZH)and English?French (EN?FR) datasets3 ExperimentIn our experiments, a baseline system PB is builtwith the phrase-based model in Moses (Koehn etal., 2007).
We compare our approach with threeother combination methods.
ADD combines PBwith addition (Ma et al, 2011), while SUB com-bines PB with subtraction (Koehn and Senellart,2010).
WANG combines SMT and TM at phrase-level during decoding (Wang et al, 2013; Li etal., 2014).
For each phrase pair applied to trans-late an input phrase, WANG finds its correspond-ing phrase pairs in a TM instance and then ex-tracts features which are directly added to the log-linear framework (Och and Ney, 2002) as sparsefeatures.
We build three systems based on our ap-proach: CWLaddonly uses constraints from addi-tion; CWLsubonly uses constraints from subtrac-tion; CWLbothuses constraints from both.Table 1 shows a summary of our datasets.
TheEN?ZH dataset is a translation memory fromSymantec.
Our EN?FR dataset is from the pub-licly available JRC-Acquis corpus.4Word align-ment is performed by GIZA++ (Och and Ney,2003) with heuristic function grow-diag-final-and.4http://ipsc.jrc.ec.europa.eu/index.php?id=198277SystemsEN?ZH EN?FRBLEU?
TER?
BLEU?
TER?PB 44.3 40.0 65.7 25.9Sentence-Level CombinationADD 45.6* 39.2* 64.2 27.2SUB 49.4* 36.3* 64.2 27.3Phrase-Level CombinationWANG 44.7* 39.3* 66.1* 25.7*CWLadd49.8* 35.7* 68.1* 23.7*CWLSub51.4* 33.7* 68.6* 23.4*CWLboth51.2* 33.8* 68.3* 23.6*Table 2: Experimental results of comparing ourapproach (CWLx) with previous work.
All scoresreported are an average of 3 runs.
Scores with?are significantly better than that of the baseline PBat p < 0.01.
Bold scores are significantly betterthan that of all previous work at p < 0.01.We use SRILM (Stolcke, 2002) to train a 5-gramlanguage model on the target side of our train-ing data with modified Kneser-Ney discounting(Chen and Goodman, 1996).
Batch MIRA (Cherryand Foster, 2012) is used to tune weights.
Case-insensitive BLEU [%] and TER [%] are used toevaluate translation results.3.1 ResultsTable 2 shows experimental results on EN?ZH andEN?FR.
We find that our method (CWLx) signif-icantly improves the baseline system PB on EN?ZH by up to +5.5% BLEU score and by +2.4%BLEU score on EN?FR.
In terms of TER, our sys-tem significantly decreases the error by up to -4.3%/-2.2% on EN?ZH and EN?FR, respectively.Although, compared to the baseline PB, ADDand SUB work well on EN?ZH, they reduce thetranslation quality on EN?FR.
By contrast, theirphrase-level countparts (CWLaddand CWLsub)bring consistent improvements over the baselineon both language pairs.
This suggests that a com-bination approach based on constrained word lat-tices is more effective and robust than sentence-level constrained translation.
Compared to systemWANG, our method produces significantly bettertranslations as well.
In addition, our approach issimpler and easier to adopt than WANG.Compared with CWLadd, CWLsubproducesbetter translations.
This may suggest that, for aconstrained word lattice, subtraction generates abetter sequence of constraints than addition sinceit keeps target words and the word order.
However,Ranges Sentence W/S (EN)[0.8, 1.0) 198 16.4[0.6, 0.8) 195 14.7[0.4, 0.6) 318 16.8(0.0, 0.4) 223 21.5(a) English?ChineseRanges Sentences W/S (EN)[0.9, 1.0) 313 32.5[0.8, 0.9) 258 28.3[0.7, 0.8) 216 28.4[0.6, 0.7) 156 33.3[0.5, 0.6) 171 34.1[0.4, 0.5) 168 34.3[0.3, 0.4) 277 40.3(0.0, 0.3) 360 54.7(b) English?FrenchTable 3: Composition of test subsets basedon fuzzy match scores on English?Chinese andEnglish?French data.combining them together (i.e.
CWLboth) does notbring a further improvement.
We assume the rea-son for this is that addition and subtraction shareparts of the constraints generated from the sameTM.
For example, in Figure 2, the edge ?1 ?
5?based on addition and the edge ?11 ?
7?
basedon subtraction are labeled by the same constraint.3.2 Influence of Fuzzy Match ScoresSince a fuzzy match scorer5is used to select thebest TM instance for an input and thus is an impor-tant factor for combining SMT and TM, it is inter-esting to know what impact it has on the transla-tion quality of various approaches.
Table 3 showsstatistics of each test subset on EN?ZH and EN?FR where sentences are grouped by their fuzzymatch scores.Figure 3 shows BLEU scores of systems eval-uated on these subsets.
We find that BLEUscores increasingly grow when match scores be-come higher.
While ADD achieves better BLEUscores than SUB on lower fuzzy ranges, SUB per-forms better than ADD on higher fuzzy scores.
Inaddition, our approaches (CWLx) are better thanthe baseline on all ranges but show much more im-provement on ranges with higher fuzzy scores.5In this paper, we use a lexical fuzzy match score (Koehnand Senellart, 2010) based on Levenshtein distance to findthe best match.278(0,0.4) [0.4,0.6)30354045PBADDSUBCWLaddCWLsubCWLboth[0.6,0.8) [0.8,1)5560657075(0,0.3) [0.3,0.4)35404550[0.4,0.5) [0.5,0.6)556065[0.6,0.7) [0.7,0.8)707580[0.8,0.9) [0.9,1)808284868890BLEU[%]EN?ZHEN?FREN?FRRanges of Fuzzy Match ScoresFigure 3: BLEU scores of systems evaluated onsentences which fall into different ranges accord-ing to fuzzy match scores on EN?ZH and EN?FR.All scores are averaged over 3 runs.4 ConclusionIn this paper, we propose a constrained word lat-tice to combine SMT and TM at phrase-level.This method uses a word lattice to encode allpossible phrasal constraints together.
These con-straints come from two sentence-level constrainedapproaches, including addition and subtraction.Experiments on English?Chinese and English?French show that compared with previous com-bination methods, our approach produces signifi-cantly better translation results.In the future, we would like to consider gener-ating constraints from more than one fuzzy matchand using fuzzy match scores or a more sophisti-cated function to weight constraints.
It would alsobe interesting to know if our method will work bet-ter when discarding fuzzy matches with very lowscores.AcknowledgmentsThis research has received funding from the Peo-ple Programme (Marie Curie Actions) of the Euro-pean Union?s Framework Programme (FP7/2007-2013) under REA grant agreement no317471.
TheADAPT Centre for Digital Content Technologyis funded under the SFI Research Centres Pro-gramme (Grant 13/RC/2106) and is co-funded un-der the European Regional Development Fund.The authors thank all anonymous reviewers fortheir insightful comments and suggestions.ReferencesErgun Bic?ici and Marc Dymetman.
2008.
Dy-namic Translation Memory: Using Statistical Ma-chine Translation to Improve Translation MemoryFuzzy Matches.
In Proceedings of the 9th Interna-tional Conference on Computational Linguistics andIntelligent Text Processing, pages 454?465, Haifa,Israel, February.Stanley F. Chen and Joshua Goodman.
1996.
AnEmpirical Study of Smoothing Techniques for Lan-guage Modeling.
In Proceedings of the 34th AnnualMeeting on Association for Computational Linguis-tics, pages 310?318, Santa Cruz, California, June.Colin Cherry and George Foster.
2012.
Batch Tun-ing Strategies for Statistical Machine Translation.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 427?436, Montreal, Canada, June.279Christopher Dyer, Smaranda Muresan, and PhilipResnik.
2008.
Generalizing Word Lattice Trans-lation.
In Proceedings of the 46th Annual Meetingof the Association for Computational Linguistics:Human Language Technologies, Columbus, Ohio,June.Yifan He, Yanjun Ma, Josef van Genabith, and AndyWay.
2010.
Bridging SMT and TM with TranslationRecommendation.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, pages 622?630, Uppsala, Sweden, July.Philipp Koehn and Jean Senellart.
2010.
Conver-gence of Translation Memory and Statistical Ma-chine Translation.
In Proceedings of AMTA Work-shop on MT Research and the Translation Industry,pages 21?31, Denver, Colorado, USA, November.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-based Translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics on Human Language Technology - Vol-ume 1, pages 48?54, Edmonton, Canada.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open Source Toolkit for Statistical Machine Trans-lation.
In Proceedings of the 45th Annual Meetingof the ACL on Interactive Poster and DemonstrationSessions, pages 177?180, Prague, Czech Republic,June.Vladimir Iosifovich Levenshtein.
1966.
Binary CodesCapable of Correcting Deletions, Insertions and Re-versals.
Soviet Physics Doklady, 10:707.Liangyou Li, Andy Way, and Qun Liu.
2014.
ADiscriminative Framework of Integrating Transla-tion Memory Features into SMT.
In Proceedings ofthe 11th Conference of the Association for MachineTranslation in the Americas, Vol.
1: MT ResearchersTrack, pages 249?260, Vancouver, BC, Canada, Oc-tober.Yanjun Ma, Yifan He, Andy Way, and Josef van Gen-abith.
2011.
Consistent Translation using Discrim-inative Learning - A Translation Memory-InspiredApproach.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics: Human Language Technologies, pages 1239?1248, Portland, Oregon, USA, June.Franz Josef Och and Hermann Ney.
2002.
Discrimi-native Training and Maximum Entropy Models forStatistical Machine Translation.
In Proceedings ofthe 40th Annual Meeting on Association for Com-putational Linguistics, pages 295?302, Philadelphia,Pennsylvania, July.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51,March.Franz Josef Och and Hermann Ney.
2004.
TheAlignment Template Approach to Statistical Ma-chine Translation.
Compututational Linguistics,30(4):417?449, December.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A Method for AutomaticEvaluation of Machine Translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, July.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, andJ.
Makhoul.
2006.
A Study of Translation Edit Ratewith Targeted Human Annotation.
In Proceedingsof Association for Machine Translation in the Amer-icas, pages 223?231, Cambridge, Massachusetts,USA, August.Andreas Stolcke.
2002.
SRILM-an Extensible Lan-guage Modeling Toolkit.
In Proceedings of the 7thInternational Conference on Spoken Language Pro-cessing, pages 257?286, Denver, Colorado, USA,November.Kun Wang, Chengqing Zong, and Keh-Yih Su.
2013.Integrating Translation Memory into Phrase-BasedMachine Translation during Decoding.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 1: Long Pa-pers), pages 11?21, Sofia, Bulgaria, August.280
