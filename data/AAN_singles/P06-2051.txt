Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 391?398,Sydney, July 2006. c?2006 Association for Computational LinguisticsSpontaneous Speech Understanding for Robust Multi-ModalHuman-Robot CommunicationSonja Hu?wel, Britta WredeFaculty of Technology, Applied Computer ScienceBielefeld University, 33594 Bielefeld, Germanyshuewel,bwrede@techfak.uni-bielefeld.deAbstractThis paper presents a speech understand-ing component for enabling robust situatedhuman-robot communication.
The aim isto gain semantic interpretations of utter-ances that serve as a basis for multi-modaldialog management also in cases wherethe recognized word-stream is not gram-matically correct.
For the understand-ing process, we designed semantic pro-cessable units, which are adapted to thedomain of situated communication.
Ourframework supports the specific character-istics of spontaneous speech used in com-bination with gestures in a real world sce-nario.
It also provides information aboutthe dialog acts.
Finally, we present a pro-cessing mechanism using these conceptstructures to generate the most likely se-mantic interpretation of the utterances andto evaluate the interpretation with respectto semantic coherence.1 IntroductionOver the past years interest in mobile robot ap-plications has increased.
One aim is to allow forintuitive interaction with a personal robot which isbased on the idea that people want to communi-cate in a natural way (Breazeal et al, 2004)(Daut-enhahn, 2004).
Although often people use speechas the main modality, they tend to revert to addi-tional modalities such as gestures and mimics inface-to-face situations.
Also, they refer to objects1This work has been supported by the European Unionwithin the ?Cognitive Robot Companion?
(COGNIRON)project (FP6-IST-002020) and by the German ResearchFoundation within the Graduate Program ?Task OrientedCommunication?.in the physical environment.
Furthermore, speech,gestures and information of the environment areused in combination in instructions for the robot.When participants perceive a shared environmentand act in it we call this communication ?situated?
(Milde et al, 1997).
In addition to these featuresthat are characteristic for situated communication,situated dialog systems have to deal with severalproblems caused by spontaneous speech phenom-ena like ellipses, indirect speech acts or incom-plete sentences.
Large pauses or breaks occur in-side an utterance and people tend to correct them-selves.
Utterances often do not follow a standardgrammar as written text.Service robots have not only to be able to copewith this special kind of communication but theyalso have to cope with noise that is produced bytheir own actuators or the environment.
Speechrecognition in such scenarios is a complex and dif-ficult task, leading to severe degradations of therecognition performance.
The goal of this paperis to present a framework for human-robot inter-action (HRI) that enables robust interpretation ofutterances under the specific conditions in HRI.2 Related WorkSome of the most explored speech processingsystems are telephone-based information systems.Their design rather differs from that of situatedHRI.
They are uni-modal so that every informationhas to be gathered from speech.
However, speechinput is different as users utter longer phraseswhich are generally grammatically correct.
Thesesystems are often based on a large corpus and cantherefore be well trained to perform satisfactoryspeech recognition results.
A prominent examplefor this is the telephone based weather forecast in-formation service JUPITER (Zue et al, 2000).391Over the past years interest increased in mo-bile robot applications where the challenges areeven more complex.
While many of these prob-lems (person tracking, attention, path finding) arealready in the focus of research, robust speech un-derstanding has not yet been extensively exploredin the context of HRI.
Moreover, interpretationof situated dialogs in combination with additionalknowledge sources is rarely considered.
Recentprojects with related scope are the mobile robotsCARL (Lopes et al, 2005) and ALBERT (Ro-galla et al, 2002), and the robotic chandelier Elvis(Juster and Roy, 2004).
The main task of the robotCARL is robust language understanding in con-text of knowledge acquisition and management.It combines deep and shallow parsing to achieverobustness.
ALBERT is designed to understandspeech commands in combination with gesturesand object detection with the task to handle dishes.The home lighting robot Elvis gets instructionsabout lighting preferences of a user via speech andgestural input.
The robot itself has a fixed positionbut the user may walk around in the entire room.It uses keyword spotting to analyze the semanticcontent of speech.
As speech recognition in suchrobot scenarios is a complex and difficult task, inthese systems the speech understanding analysisis constrained to a small set of commands and notoriented towards spontaneous speech.
However,deep speech understanding is necessary for morecomplex human robot interaction.There is only little research in semantic speechanalysis of spontaneous speech.
A widely used ap-proach of interpreting sentences is the idea of casegrammar (Bruce, 1975).
Each verb has a set ofnamed slots, that can be filled by other slots, typ-ically nouns.
Syntactic case information of wordsinside a sentence marks the semantic roles andthus, the corresponding slots can be filled.
An-other approach of processing spontaneous speechby using semantic information for the Air TravelInformation Service (ATIS) task is implementedin the Phoenix system (Ward, 1994).
Slots inframes represent the basic semantic entities knownto the system.
A parser using semantic gram-mars maps input onto these frame representations.The idea of our approach is similar to that of thePhoenix system, in that we also use semantic en-tities for extracting information.
Much effort hasbeen made in the field of parsing strategies com-bined with semantic information.
These systemssupport preferably task oriented dialog systems,e.g., the ATIS task as in (Popescu et al, 2004)and (Milward, 2000), or virtual world scenarios(Gorniak and Roy, 2005), which do not have todeal with uncertain visual input.
The aim of theFrameNet project (Fillmore and Baker, 2001) is tocreate a lexicon resource for English, where everyentry receives a semantic frame description.In contrast to other presented approaches we fo-cus on deep semantic analysis of situated sponta-neous speech.Written language applications havethe advantage to be trainable on large corpora,which is not the case for situated speech based ap-plications.
And furthermore, interpretation of sit-uated speech depends on environmental informa-tion.
Utterances in this context are normally lesscomplex, still our approach is based on a lexiconthat allows a broad variety of utterances.
It alsotakes speech recognition problems into accountby ignoring non-consistent word hypotheses andscoring interpretations according to their semanticcompleteness.
By adding pragmatic information,natural dialog processing is facilitated.3 Situated Dialog CorpusWith our robot BIRON we want to improve so-cial and functional behavior by enabling the sys-tem to carry out a more sophisticated dialog forhandling instructions.
One scenario is a home-tourwhere a user is supposed to show the robot aroundthe home.
Another scenario is a plant-wateringtask, where the robot is instructed to water differ-ent plants.
There is only little research on multi-modal HRI with speech-based robots.
A studyhow users interact with mobile office robots is re-ported in (Hu?ttenrauch et al, 2003).
However, inthis evaluation, the integration of different modal-ities was not analyzed explicitly.
But even thoughthe subjects were not allowed to use speech andgestures in combination, the results support thatpeople tended to communicate in a multi-modalway, nevertheless.To receive more detailed information about theinstructions that users are likely to give to an as-sistant in home or office we simulated this sce-nario and recorded 14 dialogs from German nativespeakers.
Their task was to instruct the robot towater plants.
Since our focus in this stage of thedevelopment of our system lies on the situatednessof the conversation, the robot was simply replacedby a human pretending to be a robot.
The subjects392were asked to act as if it would be a robot.
As pro-posed in (Lauriar et al, 2001), a preliminary userstudy is necessary to reduce the number of repairdialogs between user and system, such as queries.The corpus provides data necessary for the designof the dialog components for multi-modal interac-tion.
We also determined the lexicon and obtainedthe SSUs that describe the scene and tasks for therobot.The recorded dialogs feature the specific na-ture of dialog situations in multi-modal commu-nication situations.
The analysis of the corpus ispresented in more detail in (Hu?wel and Kummert,2004).
It confirms that spontaneously spoken ut-terances seldom respect the standard grammar andstructure of written sentences.
People tend to useshort phrases or single words.
Large pauses of-ten occur during an utterance or the utterance isincomplete.
More interestingly, the multi-modaldata shows that 13 out of 14 persons used pointinggestures in the dialogs to refer to objects.
Such ut-terances cannot be interpreted without additionalinformation of the scene.
For example, an utter-ance such as ?this one?
is used with a pointinggesture to an object in the environment.
We re-alize, of course, that for more realistic behaviortowards a robot a real experiment has to be per-formed.
However this time- and resource-efficientprocedure allowed us to build a system capable offacilitating situated communication with a robot.The implemented system has been evaluated witha real robot (see section 7).
In the prior version weused German as language, now the dialog systemhas adapted to English.4 The Robot Assistant BIRONThe aim of our project is to enable intuitive inter-action between a human and a mobile robot.
Thebasis for this project is the robot system BIRON(et.
al, 2004).
The robot is able to visually trackpersons and to detect and localize sound sources.GenerationLanguageRecognitionGestureObjectRecognitionObjectAttentionSystem SceneModellexicon+ SSUdatabasefusionengineUnderstandingSpeechRobotControlManagerDialogSpeechRecognitionhistoryFigure 1: Overview of the BIRON dialog systemarchitectureThe robot expresses its focus of attention by turn-ing the camera into the direction of the personcurrently speaking.
From the orientation of theperson?s head it is deduced whether the speakeraddresses the robot or not.
The main modalityof the robot system is speech but the system canalso detect gestures and objects.
Figure 1 givesan overview of the architecture of BIRON?s multi-modal interaction system.
For the communica-tion between these modules we use an XML basedcommunication framework (Fritsch et al, 2005).In the following we will briefly outline the inter-acting modules of the entire dialog system withthe speech understanding component.Speech recognition: If the user addressesBIRON by looking in its direction and starting tospeak, the speech recognition system starts to an-alyze the speech data.
This means that once theattention system has detected that the user is prob-ably addressing the robot it will route the speechsignal to the speech recognizer.
The end of theutterance is detected by a voice activation detec-tor.
Since both components can produce errors thespeech signal sent to the recognizer may containwrong or truncated parts of speech.
The speechrecognition itself is performed with an incremen-tal speaker-independent system (Wachsmuth et al,1998), based on Hidden Markov Models.
It com-bines statistical and declarative language modelsto compute the most likely word chain.Dialog manager: The dialog managementserves as the interface between speech analysisand the robot control system.
It also generates an-swers for the user.
Thus, the speech analysis sys-tem transforms utterances with respect to gesturaland scene information, such as pointing gesturesor objects in the environment, into instructions forthe robot.
The dialog manager in our application isagent-based and enables a multi-modal, mixed ini-393tiative interaction style (Li et al, 2005).
It is basedon semantic entities which reflect the informationthe user uttered as well as discourse informationbased on speech-acts.
The dialog system classifiesthis input into different categories as e.g., instruc-tion, query or social interaction.
For this purposewe use discourse segments proposed by Grosz andSidner (Grosz and Sidner, 1986) to describe thekind of utterances during the interaction.
Then thedialog manager can react appropriately if it knowswhether the user asked a question or instructedthe robot.
As gesture and object detection in ourscenario is not very reliable and time-consuming,the system needs verbal hints of scene informationsuch as pointing gestures or object descriptions togather information of the gesture detection and ob-ject attention system.5 Situated Concept RepresentationsBased on the situated conversational data, we de-signed ?situated semantic units?
(SSUs) which aresuitable for fast and automatic speech understand-ing.
These SSUs basically establish a network ofstrong (mandatory) and weak (optional) relationsof sematic concepts which represent world anddiscourse knowledge.
They also provide ontolog-ical information and additional structures for theintegration of other modalities.
Our structures areinspired by the idea of frames which provide se-mantic relations between parts of sentences (Fill-more, 1976).Till now, about 1300 lexical entries are storedin our database that are related to 150 SSUs.
Bothtypes are represented in form of XML structures.The lexicon and the concept database are based onour experimental data of situated communication(see section 3) and also on data of a home-tourscenario with a real robot.
This data has been an-notated by hand with the aim to provide an ap-propriate foundation for human-robot interaction.It is also planned to integrate more tasks for therobot as, e.g., courier service.
This can be done byonly adding new lexical entries and correspond-ing SSUs without spending much time in reorga-nization.
Each lexical entry in our database con-tains a semantic association to the related SSUs.Therefore, equivalent lexical entries are providedfor homonyms as they are associated to differentconcepts.In figure 2 the SSU Showing has an open linkto the SSUs Actor and Object.
Missing links toInstructionObjectActortopopt?framesTimemand?framesPerson_involvedSSU  ShowingFigure 2: Schematic SSU ?Showing?
for utter-ances like ?I show you my poster tomorrow?.strongly connected SSUs are interpreted as miss-ing information and are thus indicators for the di-alog management system to initiate a clarificationquestion or to look for information already storedin the scene model (see fig.
1).
The SSUs alsohave connections to optional arguments, but theyare less important for the entire understanding pro-cess.The SSUs also include ontological information,so that the relations between SSUs can be de-scribed as general as possible.
For example, theSSU Building subpart is a sub-category of Object.In our scenario this is important as for example theunit Building subpart related to the concept?wall?has a fixed position and can be used as navigation-support in contrast to other objects.
The top-category is stored in the entry top, a special itemof the SSU.
By the use of ontological information,SSUs also differentiate between task and commu-nication related information and thereby supportthe strategy of the dialog manager to decouple taskfrom communication structure.
This is importantin order to make the dialog system independentof the task and enable scalable interaction capa-bilities.
For example the SSU Showing belongs tothe discourse type Instruction.
Other types impor-tant for our domain are Socialization, Description,Conrmation, Negation, Correction, and Query.Further types may be included, if necessary.In our domain, missing information in an utter-ance can often be acquired from the scene.
Forexample the utterance ?look at this?
and a point-ing gesture to a table will be merged to the mean-ing ?look at the table?.
To resolve this meaning,we use hints of co-verbal gestures in the utter-ance.
Words as ?this one?
or ?here?
are linkedto the SSU Potential gesture, indicating a relationbetween speech and gesture.
The timestamp of theutterance enables temporal alignment of speechand gesture.
Since gesture recognition is expen-sive in computing time and often not well-defined,such linguistic hints can reduce these costs dra-394matically.The utterance ?that?
can also represent ananaphora, and is analyzed in both ways, asanaphora and as gesture hint.
Only if there is nogesture, the dialog manager will decide that theword probably was used in an anaphoric manner.Since we focus on spontaneous speech, we can-not rely on the grammar, and therefore the se-mantic units serve as the connections between thewords in an utterance.
If there are open connec-tions interpretable as missing information, it canbe inferred what is missing and be integrated bythe contextual knowledge.
This structure makesit easy to merge the constituents of an utterancesolely by semantic relations without additionalknowledge of the syntactic properties.
By this,we lose information that might be necessary inseveral cases for disambiguation of complex ut-terances.
However, spontaneous speech is hardto parse especially since speech recognition errorsoften occur on syntactically relevant morphemes.We therefore neglect the cases which tend to occurvery rarely in HRI scenarios.6 Semantic ProcessingIn order to generate a semantic interpretation ofan utterance, we use a special mechanism, whichunifies words of an utterance into a single struc-ture.
The system also considers the ontological in-formation of the SSUs to generate the most likelyinterpretation of the utterance.
For this purpose,the mechanism first associates lexical entries ofall words in the utterance with the correspondingSSUs.
Then the system tries to link all SSUs to-gether into one connected uniform.
Some SSUsprovide open links to other SSUs, which can befilled by semantic related SSUs.
The SSU Be-side for example provides an open link to Object.This SSU can be linked to all Object entities andto all subtypes of Object.
Thus, an utterance as?next to the door?
can be linked together to forma single structure (see fig.
3).
The SSUs whichpossess open links are central for this mechanism,they represent roots for parts of utterances.
How-ever, these units can be connected by other roots,likewise to generate a tree representing semanticrelations inside an utterance.The fusion mechanism computes in its best casein linear time and in worst case in square time.A scoring function underlies the mechanism: themore words can be combined, the better is the rat-ontological linkstrong referencelexical mappingBuilding_subpart"next to   the door"BesideObjectFigure 3: Simplied parse tree example .ing.
The system finally chooses the structure withthe highest score.
Thus, it is possible to handle se-mantic variations of an utterance in parallel, suchas homonyms.
Additionally, the rating is help-ful to decide whether the speech recognition resultis reliable or not.
In this case, the dialog man-ager can ask the user for clarification.
In the nextversion we will use a more elaborate evaluationtechnique to yield better results such as rating theamount of concept-relations and missing relations,distinguish between important and optional rela-tions, and prefer relations to words nearby.A converter forwards the result of the mech-anism as an XML-structure to the dialog man-ager.
A segment of the result for the dialog man-ager is presented in Figure 4.
With the category-descriptions the dialog-module can react fast onthe user?s utterance without any further calcula-tion.
It uses them to create inquiries to the useror to send a command to the robot control system,such as ?look for a gesture?, ?look for a blue ob-ject?, or ?follow person?.
If the interpreted utter-ance does not fit to any category it gets the valuefragment.
These utterances are currently inter-preted in the same way as partial understandingsand the dialog manager asks the user to providemore meaningful information.Figure 1 illustrates the entire architecture of thespeech understanding system and its interfaces toother modules.
The SSUs and the lexicon arestored in an external XML-databases.
As thespeech understanding module starts, it first readsthese databases and converts them into internaldata-structures stored in a fast accessible hash ta-ble.
As soon as the module receives results fromspeech recognition, it starts to merge.
The mech-anism also uses a history, where former parts ofutterances are stored and which are also integratedin the fusing mechanism.
The speech understand-ing system then converts the best scored result intoa semantic XML-structure (see Figure 4) for thedialog manager.395<metaInfo><time>1125573609635</time><status>full</status></metaInfo><semanticInfo><u>what can you do</u><category>query</category><content><unit = Question_action><name>what</name><unit = Action><name>do</name><unit = Ability><name>can</name><unit = Proxy><name>you</name>...<u>this is a green cup</u><category>description</category><content><unit = Existence><name>is</name><unit = Object_kitchen><name>cup</name><unit = Potential_gesture><name>this</name></unit><unit = Color><name>green</name></unit>...Figure 4: Two segments of the speech understand-ing results for the utterances ?what can you do?and ?this is a green cup?.6.1 Situated Speech ProcessingOur approach has various advantages dealing withspontaneous speech.
Double uttered words as inthe utterance ?look - look here?
are ignored in ourapproach.
The system still can interprete the ut-terance, then only one word is linked to the otherwords.
Corrections inside an utterance as ?the leftem right cube?
are handled similar.
The systemgenerates two interpretations of the utterance, theone containing left the other right.
The systemchooses the last one, since we assume that cor-rections occur later in time and therefore moreto the right.
The system deals with pauses in-side utterances by integrating former parts of ut-terances stored in the history.
The mechanism alsoprocesses incomplete or syntactic incorrect utter-ances.
To prevent sending wrong interpretations tothe dialog-manager the scoring function rates thequality of the interpretation as described above.
Inour system we also use scene information to eval-uate the entire correctness so that we do not onlyhave to rely on the speech input.
In case of doubtthe dialog-manager requests to the user.For future work it is planned to integrate addi-tional information sources, e.g., inquiries of thedialog manager to the user.
The module will alsoUser1: Robot look - do you see?This - is a cow.
Funny.Do you like it?
...User2: Look here robot - a cup.Look here a - a keyboard.Let?s try that one.
...User3: Can you walk in this room?Sorry, can you repeat your answer?How fast can you move?
...Figure 5: Excerptions of the utterances during theexperiment setting.store these information in the history which will beused for anaphora resolution and can also be usedto verify the output of the speech recognition.7 EvaluationFor the evaluation of the entire robot systemBIRON we recruited 14 naive user between 12and 37 years with the goal to test the intuitive-ness and the robustness of all system modules aswell as its performance.
Therefore, in the first oftwo runs the users were asked to familiarize them-selves with the robot without any further informa-tion of the system.
In the second run the userswere given more information about technical de-tails of BIRON (such as its limited vocabulary).We observed similar effects as described in section2.
In average, one utterance contained 3.23 wordsindicating that the users are more likely to uttershort phrases.
They also tend to pause in the mid-dle of an utterance and they often uttered so calledmeta-comments such as ?that?s fine?.
In figure 5some excerptions of the dialogs during the experi-ment settings are presented.Thus, not surprisingly the speech recognitionerror rate in the first run was 60% which decreasedin the second run to 42%, with an average of 52%.High error rate seems to be a general problem insettings with spontaneous speech as other systemsalso observed this problem (see also (Gorniak andRoy, 2005)).
But even in such a restricted exper-iment setting, speech understanding will have todeal with speech recognition error which can neverbe avoided.In order to address the two questions of (1)how well our approach of automatic speech un-derstanding (ASU) can deal with automatic speechrecognition (ASR) errors and (2) how its perfor-mance compares to syntactic analysis, we per-formed two analyses.
In order to answer ques-tion (1) we compared the results from the semanticanalysis based on the real speech recognition re-396sults with an accuracy of 52% with those based onthe really uttered words as transcribed manually,thus simulating a recognition rate of 100%.
In to-tal, the semantic speech processing received 1642utterances from the speech recognition system.From these utterances 418 utterances were ran-domly chosen for manual transcription and syntac-tic analysis.
All 1642 utterances were processedand performed on a standard PC with an averageprocessing time of 20ms, which fully fulfills therequirements of real-time applications.
As shownin Table 1 39% of the results were rated as com-plete or partial misunderstandings and 61% as cor-rect utterances with full semantic meaning.
Only4% of the utterances which were correctly recog-nized were misinterpreted or refused by the speechunderstanding system.
Most errors occurred dueto missing words in the lexicon.Thus, the performance of the speech under-standing system (ASU) decreases to the samedegree as that of the speech recognition system(ASR): with a 50% ASR recognition rate the num-ber of non-interpretable utterances is doubled in-dicating a linear relationship between ASR andASU.For the second question we performed a manualclassification of the utterances into syntacticallycorrect (and thus parseable by a standard pars-ing algorithm) and not-correct.
Utterances fol-lowing the English standard grammar (e.g.
im-perative, descriptive, interrogative) or containinga single word or an NP, as to be expected in an-swers, were classified as correct.
Incomplete ut-terances or utterances with a non-standard struc-ture (as occurred often in the baby-talk style ut-terances) were rated as not-correct.
In detail, 58utterances were either truncated at the end or be-ginning due to errors of the attention system, re-sulting in utterances such as ?where is?, ?can youfind?, or ?is a cube?.
These utterances also includeinstances where users interrupted themselves.
In51 utterances we found words missing in our lex-icon database.
314 utterances where syntacticallycorrect, whereas in 28 of these utterances a lexiconentry is missing in the system and therefore wouldASR=100% ASR=52%ASU not or part.
interpret.
15% 39%ASU fully interpretable 84% 61%Table 1: Semantic Processing results based on dif-ferent word recognition accuracies.lead to a failure of the parsing mechanism.
104 ut-terances have been classified as syntactically not-correct.In contrast, the result from our mechanism per-formed significantly better.
Our system was ableto interprete 352 utterances and generate a full se-mantic interpretation, whereas 66 utterances couldonly be partially interpreted or were marked asnot interpretable.
21 interpretations of the utter-ances were semantically incorrect (labeled fromthe system wrongly as correct) or were not as-signed to the correct speech act, e.g., ?okay?
wasassigned to no speech act (fragment) instead toconrmation.
Missing lexicon entries often leadto partial interpretations (20 times) or sometimesto complete misinterpretations (8 times).
But stillin many cases the system was able to interprete theutterance correctly (23 times).
For example ?canyou go for a walk with me?
was interpreted as ?canyou go with me?
only ignoring the unknown ?fora walk?.The utterance ?can you come closer?
wasinterpreted as a partial understanding ?can youcome?
(ignoring the unknown word ?closer?).
Theresults are summarized in Table 2.As can be seen the semantic error rate with 15%non-interpretable utterances is just half of the syn-tactic correctness with 31%.
This indicates thatthe semantic analysis can recover about half of theinformation that would not be recoverable fromsyntactic analysis.ASU Synt.
cor.not or part.
interpret.
15% not-correct 31%fully interpret.
84% correct 68%Table 2: Comparison of semantic processing resultwith syntactic correctness based on a 100% wordrecognition rate.8 Conclusion and OutlookIn this paper we have presented a new approachof robust speech understanding for mobile robotassistants.
It takes into account the special char-acteristics of situated communication and also thedifficulty for the speech recognition to process ut-terances correctly.
We use special concept struc-tures for situated communication combined withan automatic fusion mechanism to generate se-mantic structures which are necessary for the di-alog manager of the robot system in order to re-spond adequately.This mechanism combined with the use of our397SSUs has several benefits.
First, speech is in-terpreted even if speech recognition does not al-ways guarantee correct results and speech input isnot always grammatically correct.
Secondly, thespeech understanding component incorporates in-formation about gestures and references to the en-vironment.
Furthermore, the mechanism itself isdomain-independent.
Both, concepts and lexiconcan be exchanged in context of a different domain.This semantic analysis already produces elab-orated interpretations of utterances in a fast wayand furthermore, helps to improve robustness ofthe entire speech processing system.
Nevertheless,we can improve the system.
In our next phase wewill use a more elaborate scoring function tech-nique and use the correlations of mandatory andoptional links to other concepts to perform a betterevaluation and also to help the dialog manager tofind clues for missing information both in speechand scene.
We will also use the evaluation resultsto improve the SSUs to get better results for thesemantic interpretation.ReferencesC.
Breazeal, A. Brooks, J.
Gray, G. Hoffman, C. Kidd,H.
Lee, J. Lieberman, A. Lockerd, and D. Mulanda.2004.
Humanoid robots as cooperative partners forpeople.
Int.
Journal of Humanoid Robots.B.
Bruce.
1975.
Case systems for natural language.Artificial Intelligence, 6:327?360.K.
Dautenhahn.
2004.
Robots we like to live with?!
-a developmental perspective on a personalized, life-long robot companion.
In Proc.
Int.
Workshop onRobot and Human Interactive Communication (RO-MAN).A.
Haasch et.
al.
2004.
BIRON ?
The Bielefeld RobotCompanion.
In E. Prassler, G. Lawitzky, P. Fior-ini, and M. Ha?gele, editors, Proc.
Int.
Workshop onAdvances in Service Robotics, pages 27?32.
Fraun-hofer IRB Verlag.C.
J. Fillmore and C. F. Baker.
2001.
Frame seman-tics for text understanding.
In Proc.
of WordNet andOther Lexical Recources Workshop.
NACCL.C.
J. Fillmore.
1976.
Frame semantics and the natureof language.
In Annals of the New York Academy ofSciences: Conf.
on the Origin and Development ofLanguage and Speech, volume 280, pages 20?32.J.
Fritsch, M. Kleinehagenbrock, A. Haasch, S. Wrede,and G. Sagerer.
2005.
A flexible infrastructure forthe development of a robot companion with exten-sible HRI-capabilities.
In Proc.
IEEE Int.
Conf.
onRobotics and Automation, pages 3419?3425.P.
Gorniak and D. Roy.
2005.
Probabilistic Ground-ing of Situated Speech using Plan Recognition andReference Resolution.
In ICMI.
ACM Press.B.
J. Grosz and C. L. Sidner.
1986.
Attention, inten-tion, and the structure of discourse.
ComputationalLinguistics, 12(3):175?204.H.
Hu?ttenrauch, A.
Green, K. Severinson-Eklundh,L.
Oestreicher, and M. Norman.
2003.
Involvingusers in the design of a mobile office robot.
IEEETransactions on Systems, Man and Cybernetics, PartC.S.
Hu?wel and F. Kummert.
2004.
Interpretation ofsituated human-robot dialogues.
In Proc.
of the 7thAnnual CLUK, pages 120?125.J.
Juster and D. Roy.
2004.
Elvis: Situated Speech andGesture Understanding for a Robotic Chandelier.
InProc.
Int.
Conf.
Multimodal Interfaces.S.
Lauriar, G. Bugmann, T. Kyriacou, J. Bos, andE.
Klein.
2001.
Personal robot training via natu-ral language instructions.
IEEE Intelligent Systems,16:3, pages 38?45.S.
Li, A. Haasch, B. Wrede, J. Fritsch, and G. Sagerer.2005.
Human-style interaction with a robot for co-operative learning of scene objects.
In Proc.
Int.Conf.
on Multimodal Interfaces.L.
Seabra Lopes, A. Teixeira, M. Quindere, and M. Ro-drigues.
2005.
From robust spoken language under-standing to knowledge aquisition and management.In EUROSPEECH 2005.J.
T. Milde, K. Peters, and S. Strippgen.
1997.
Situatedcommunication with robots.
In First Int.
Workshopon Human-Computer-Conversation.D.
Milward.
2000.
Distributing representation for ro-bust interpretation of dialogue utterances.
In ACL.A.-M. Popescu, A. Armanasu, O. Etzioni, D. Ko, andA.
Yates.
2004.
Modern natural language interfacesto databases: Composing statistical parsing with se-mantic tractability.
In Proc.
of COLING.O.
Rogalla, M. Ehrenmann, R. Zo?llner, R. Becher, andR.
Dillmann.
2002.
Using gesture and speech con-trol for commanding a robot assistant.
In Proc.
ofthe 11th IEEE Int.
Workshop on Robot and Humaninteractive Communication, pages 454?459.
RO-MAN.S.
Wachsmuth, G. A. Fink, and G. Sagerer.
1998.
Inte-gration of parsing and incremental speech recogni-tion.
In EUSIPCO, volume 1, pages 371?375.W.
Ward.
1994.
Extracting Information From Sponta-neous Speech.
In ICRA, pages 83?86.
IEEE Press.V.
Zue, S. Seneff, J.
Glass, J. Polifronti, C. Pao, T. J.Hazen, and L. Hetherington.
2000.
JUPITER:A telephone-based conversational interface forweather information.
IEEE Transactions on Speechand Audio Processing, pages 100?112, January.398
