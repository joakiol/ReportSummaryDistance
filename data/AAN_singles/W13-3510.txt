Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 84?93,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsDocuments and Dependencies: an Exploration ofVector Space Models for Semantic CompositionAlona Fyshe, Partha Talukdar, Brian Murphy and Tom MitchellMachine Learning Department &Center for the Neural Basis of CognitionCarnegie Mellon University, Pittsburgh{afyshe|ppt|bmurphy|tom.mitchell}@cs.cmu.eduAbstractIn most previous research on distribu-tional semantics, Vector Space Models(VSMs) of words are built either fromtopical information (e.g., documents inwhich a word is present), or from syntac-tic/semantic types of words (e.g., depen-dency parse links of a word in sentences),but not both.
In this paper, we explore theutility of combining these two representa-tions to build VSM for the task of seman-tic composition of adjective-noun phrases.Through extensive experiments on bench-mark datasets, we find that even thougha type-based VSM is effective for seman-tic composition, it is often outperformedby a VSM built using a combination oftopic- and type-based statistics.
We alsointroduce a new evaluation task whereinwe predict the composed vector represen-tation of a phrase from the brain activity ofa human subject reading that phrase.
Weexploit a large syntactically parsed corpusof 16 billion tokens to build our VSMs,with vectors for both phrases and words,and make them publicly available.1 IntroductionVector space models (VSMs) of word semanticsuse large collections of text to represent wordmeanings.
Each word vector is composed of fea-tures, where features can be derived from globalcorpus co-occurrence patterns (e.g.
how often aword appears in each document), or local corpusco-occurrence patterns patterns (e.g.
how oftentwo words appear together in the same sentence,or are linked together in dependency parsed sen-tences).
These two feature types represent dif-ferent aspects of word meaning (Murphy et al2012c), and can be compared with the paradig-matic/syntagmatic distinction (Sahlgren, 2006).Global patterns give a more topic-based mean-ing (e.g.
judge might appear in documents alsocontaining court and verdict).
Certain local pat-terns give a more type-based meaning (e.g.
thenoun judge might be modified by the adjectiveharsh, or be the subject of decide, as would relatedand substitutable words such as referee or con-ductor).
Global patterns have been used in LatentSemantic Analysis (Landauer and Dumais, 1997)and LDA Topic models (Blei et al 2003).
Localpatterns based on word co-occurrence in a fixedwidth window were used in Hyperspace Analogueto Language (Lund and Burgess, 1996).
Subse-quent models added increasing linguistic sophisti-cation, up to full syntactic and dependency parses(Lin, 1998; Pado?
and Lapata, 2007; Baroni andLenci, 2010).In this paper we systematically explore the util-ity of a global, topic-based VSM built from whatwe call Document features, and a local, type-basedVSM built from Dependency features.
Our Doc-ument VSM represents each word w by a vectorwhere each feature is a specific document, and thefeature value is the number of mentions of wordw in that document.
Our Dependency VSM rep-resents word w with a vector where each featureis a dependency parse link (e.g., the word w is thesubject of the verb ?eat?
), and the feature value isthe number of instances of this dependency fea-ture for word w across a large text corpus.
Wealso consider a third Combined VSM in whichthe word vector is the concatenation of its Doc-ument and Dependency features.
All three mod-els subsequently normalize frequencies using pos-itive pointwise mutual-information (PPMI), and84are dimensionality reduced using singular valuedecomposition (SVD).
This is the first systematicstudy of the utility of Document and Dependencyfeatures for semantic composition.
We constructall three VSMs (Dependencies, Documents, Com-bined) using the same text corpus and preprocess-ing pipeline, and make the resulting VSMs avail-able for download (http://www.cs.cmu.edu/?afyshe/papers/conll2013/).
Toour knowledge, this is the first freely availableVSM that includes entries for both words andadjective-noun phrases, and it is built from a muchlarger corpus than previously shared resources (16billion words, 50 million documents).
Our maincontributions include:?
We systematically study complementarity oftopical (Document) and type (Dependency)features in Vector Space Model (VSM)for semantic composition of adjective-nounphrases.
To the best of our knowledge, this isone of the first studies of this kind.?
Through extensive experiments on standardbenchmark datasets, we find that a VSM builtfrom a combination of topical and type fea-tures is more effective for semantic compo-sition, compared to a VSM built from Docu-ment and Dependency features alone.?
We introduce a novel task: to predict the vec-tor representation of a composed phrase fromthe brain activity of human subjects readingthat phrase.?
We explore two composition methods, addi-tion and dilation, and find that while additionperforms well on corpus-only tasks, dilationperforms best on the brain activity task.?
We build our VSMs, for both phrases andwords, from a large syntactically parsed textcorpus of 16 billion tokens.
We also makethe resulting VSM publicly available.2 Related WorkMitchell and Lapata (2010) explored severalmethods of combining adjective and noun vec-tors to estimate phrase vectors, and comparedthe similarity judgements of humans to the sim-ilarity of their predicted phrase vectors.
Theyfound that for adjective-noun phrases, type-basedmodels outperformed Latent Dirichlet Allocation(LDA) topic models.
For the type-based mod-els, multiplication performed the best, followedby weighted addition and a dilation model (for de-tails on composition functions see Section 4.2).However, Mitchell and Lapata did not combinethe topic- and type-based models, an idea we ex-plore in detail in this paper.Baroni and Zamparelli (2010) extended the typ-ical vector representation of words.
Their modelused matrices to represent adjectives, while nounswere represented with column vectors.
The vec-tors for nouns and adjective-noun phrases werederived from local word co-occurrence statistics.The matrix to represent the adjective was esti-mated with partial least squares regression wherethe product of the learned adjective matrix andthe observed noun vector should equal the ob-served adjective-noun vector.
Socher et al(2012)also extended word representations beyond sim-ple vectors.
Their model assigns each word a vec-tor and a matrix, which are composed via an non-linear function (e.g.
tanh) to create phrase rep-resentations consisting of another vector/matrixpair.
This process can proceed recursively, follow-ing a parse tree to produce a composite sentencemeaning.
Other general semantic compositionframeworks have been suggested, e.g.
(Sadrzadehand Grefenstette, 2011) who focus on the opera-tional nature of composition, rather than the rep-resentations that are supplied to the framework.Here we focus on creating word representationsthat are useful for semantic composition.Turney (2012) published an exploration of theimpact of domain- and function-specific vectorspace models, analogous to the topic and typemeanings encoded by our Document and Depen-dency models respectively.
In Turney?s work,domain-specific information was represented bynoun token co-occurrence statistics within a lo-cal window, and functional roles were repre-sented by generalized token/part-of-speech co-occurrence patterns with verbs - both of whichare relatively local and shallow when comparedwith this work.
Similar local context-based fea-tures were used to cluster phrases in (Lin and Wu,2009).
Though the models discussed here arenot entirely comparable to it, a recent comparisonsuggested that broader, deeper features such asours may result in representations that are superiorfor tasks involving neural activation data (Murphyet al 2012b).85In contrast to the composite model in (Griffithset al 2005), in this paper we explore the com-plementarity of semantics captured by topical in-formation and syntactic/semantic types.
We fo-cus on learning VSMs (involving both words andphrases) for semantic composition, and use moreexpressive dependency-based features in our type-based VSM.
A comparison of vector-space repre-sentations was recently published (Blacoe and La-pata, 2012), in which the authors compared sev-eral methods of combining single words vectorsto create phrase vectors.
They found that the bestperformance for adjective-noun composition usedpoint-wise multiplication and a model based ontype-based word co-occurrence patterns.3 Creating a Vector-SpaceTo create the Dependency vectors, a 16 billionword subset of ClueWeb09 (Callan and Hoy,2009) was dependency parsed using the Maltparser (Hall et al 2007).
Dependency statisticswere then collected for a predetermined list oftarget words and adjective-noun phrases, and forarbitrary adjective-noun phrases observed in thecorpus.
The list was composed of the 40 thou-sand most frequent single tokens in the Ameri-can National Corpus (Ide and Suderman, 2006),and a small number of words and phrases usedas stimuli in our brain imaging experiments.
Ad-ditionally, we included any phrase found in thecorpus whose maximal token span matched thePoS pattern J+N+, where J and N denote adjec-tive and noun PoS tags respectively.
For eachunit (i.e., word or phrase) in this augmented list,counts of all unit-external dependencies incidenton the head word were aggregated across the cor-pus, while unit-internal dependencies were ig-nored.
Each token was appended with its PoS tag,and the dependency edge label was also included.This resulted in the extraction of 498 million de-pendency tuples.
For example, the dependency tu-ple (a/DT, NMOD, 27-inch/JJ television/NN,14),indicates that a/DT was found as a child of 27-inch/JJ television/NN with a frequency of 14 inthe corpus.To create Document vectors, word-documentco-occurrence counts were taken from the samesubset of Clueweb, which covered 50 million doc-uments.
We applied feature-selection for compu-tational efficiency reasons, ranking documents bythe number of target word/phrase types they con-tained and choosing the top 10 million.A series of three additional filtering stepsselected target words/phrases, and Docu-ment/Dependency features for which there wasadequate data.1 First, a co-occurrence frequencycut-off was used to reduce the dimensionalityof the matrices, and to discard noisy estimates.A cutoff of 20 was applied to the dependencycounts, and of 2 to document counts.
Positivepointwise-mutual-information (PPMI) was usedas an association measure to normalize theobserved co-occurrence frequency for the varyingfrequency of the target word and its features,and to discard negative associations.
Second, thetarget list was filtered to the 57 thousand wordsand phrases which had at least 20 non-?stopword?
Dependency co-occurrence types, wherea ?stop word?
was one of the 100 most frequentDependency features observed (so named be-cause the dependencies were largely incident onfunction words).
Third, features observed forno more than one target were removed, as wereempty target entries.
The result was a Documentco-occurrence matrix of 55 thousand targets by5.2 million features (total 172 million non-zeroentries), and a Dependency matrix of 57 thousandtargets by 1.25 million features (total 35 millionnon-zero entries).A singular value decomposition (SVD) matrixfactorization was computed separately on the De-pendency and Document statistics matrices, with1000 latent dimensions retained.
For this stepwe used Python/Scipy implementation of the Im-plicitly Restarted Arnoldi method (Lehoucq et al1998; Jones et al 2001).
This method is com-patible with PPMI normalization, since a zerovalue represents both negative target-feature asso-ciations, and those that were not observed or fellbelow the frequency cut-off.
To combine Docu-ment and Dependency information, we concate-nate vectors.4 ExperimentsTo evaluate how Document and Dependency di-mensions can interact and compliment each other,1In earlier experiments with more than 500 thousandphrasal entries, we found that the majority of targets weredominated by non-distinctive stop word co-occurrences, re-sulting in semantically vacuous representations.86Table 1: The nearest neighbors of three queries under three VSMs: all 2000 dimensions (Deps & Docs);1000 Document dimensions (Docs); 1000 Dependency dimensions (Deps).Query Deps & Docs Docs Depsbeautiful/JJ wonderful/JJ wonderful/JJ lovely/JJlovely/JJ fantastic/JJ gorgeous/JJexcellent/JJ unspoiled/JJ wonderful/JJdog/NN cat/NN dogs/NNS cat/NNdogs/NNS vet/NN the/DT dog/NNpet/NN leash/NN dogs/NNSbad/JJ publicity/NN negative/JJ publicity/NN fast/JJ cash/NN loan/NN negative/JJ publicity/NNbad/JJ press/NN small/JJ business/NN loan/NN bad/JJ press/NNunpleasantness/NN important/JJ cities/NNS unpleasantness/NNConcrete Cats Mixed Cats Concrete Sim Mixed Sim Mixed Related00.10.20.30.40.50.60.70.80.91Performance of Documents and Dependency Dimensions for Single Word TasksTaskPerformanceDocs onlyDeps onlyDocs & DepsFigure 1: Performance of VSMs for single wordbehavioral tasks as we vary Document and Depen-dency inclusion.we can perform a qualitative comparison betweenthe nearest neighbors (NNs) of words and phrasesin the three VSMs ?
Dependency, Document, andCombined (Dependency & Document).
Resultsappear in Table 1.
Note that single words andphrases can be neighbors of each other, demon-strating that our VSMs can generalize across syn-tactic types.
In the Document VSM, we get moretopically related words as NNs (e.g., vet and leashfor dog); and in the Dependency VSM, we seewords that might substitute for one another in asentence (e.g., gorgeous for beautiful).
The twofeature sets can work together to up-weight themost suitable NNs (as in beautiful), or help todrown out noise (as in the NNs for bad publicityin the Document VSM).4.1 Judgements of Word SimilarityAs an initial test of the informativeness of Doc-ument and Dependency features, we evaluatethe representation of single words.
Behavioraljudgement benchmarks have been widely used toevaluate vector space representations (Lund andBurgess, 1996; Rapp, 2003; Sahlgren, 2006).Here we used five such tests.
Two tests are catego-rization tests, where we evaluate how well an au-tomatic clustering of our word vectors correspondto pre-defined word categories.
The first ?Con-crete Categories?
test-set consists of 82 nouns,each assigned to one of 10 concrete classes (Battigand Montague, 1969).
The second ?Mixed Cat-egories?
test-set contains 402 nouns in a rangeof 21 concrete and abstract classes from Word-Net (Almuhareb and Poesio, 2004; Miller et al1990).
Both categorization tests were performedwith the Cluto clustering package (Karypis, 2003)using cosine distances.
Success was measured aspercentage purity over clusters based on their plu-rality class, with chance performance at 10% and5% respectively for the ?Concrete Categories?
and?Mixed Categories?
tests.The remaining three tests use group judgementsof similarity: the ?Concrete Similarity?
set of65 concrete word pairs (Rubenstein and Goode-nough, 1965); and two variations on the Word-Sim353 test-set (Finkelstein et al 2002), par-titioned into subsets corresponding to strict at-tributional similarity (?Mixed Similarity?, 203noun pairs), and broader topical ?relatedness?
(?Mixed Relatedness?, 252 noun pairs) (Agirre etal., 2009).
Performance on these benchmarks isSpearman correlation between the aggregate hu-man judgements and pairwise cosine distances ofword vectors in a VSM.The results in Figure 1 show that the Depen-dency VSM substantially outperforms the Docu-ment VSM when predicting human judgements ofstrict attributional (categorial) similarity (?Simi-larity?
as opposed to ?Relatedness?)
for concretenouns.
Conversely the Document VSM is compet-87Figure 2: The performance of three phrase representations for predicting the behavioral phrasal similar-ity scores from Mitchell and Lapata (2010).
The highest correlation is 0.5033 and uses 25 Documentdimensions, 600 Dependency dimensions and the addition composition function.itive for less concrete word types, and for judge-ments of broader topical relatedness.4.2 Judgements of Phrase SimilarityWe also evaluated our system on behavioral dataof phrase similarity judgements gathered from 18human informants.
The adjective-noun phrasepairs are divided into 3 groups: high, mediumand low similarity (Mitchell and Lapata, 2010).For each pair of phrases, informants rated phrasesimilarity on a Likert scale of 1-7.
There are 36phrase pairs in each of the three groups for a to-tal of 108 phrase pairs.
Not all of the phrases oc-curred frequently enough in our corpus to pass ourthresholds, and so were omitted from our analy-sis.
In several cases we also used pluralizationsof the test phrases (e.g.
?dark eyes?)
where thesingular form was not found in our VSM.
Afterthese changes we were left with 28, 24 and 28in the high, medium and low groups respectively.In total we have 80 observed vectors for the 108phrase pairs.
These adjective-noun phrases wereincluded in the list of targets, so their statisticswere gathered in the same way as for single words.This does not impact results for composed vectors,as all of the single words in the phrases do appearin our VSMs.
A full list of the phrase pairs can befound in Mitchell and Lapata (2010).To evaluate, we used three different representa-tions of phrases.
For phrase pairs that passed ourthresholds, we can test the similarity of observedrepresentations by comparing the VSM represen-tation of the phrase (no composition function).For all 108 phrase pairs we can test the composedphrase representations, derived by applying addi-tion and dilation operations to word vectors.
Mul-tiplication is not used as SVD representations in-clude negative values, and so the product of twonegative values would be positive.Addition is the element-wise sum of two se-mantic feature vectors saddi = sadji +snouni , wheresnouni , sadji , and saddi are the ith element of thenoun, adjective, and predicted phrase vectors, re-spectively.
Dilation of two semantic feature vec-tors sadj and snoun is calculated by first decom-posing the noun into a component parallel to theadjective (x) and a component perpendicular tothe adjective (y) so that snoun = x + y. Dilationthen enhances the adjective component by multi-plying it by a scalar (?
): sdilate = ?x+y.
This canbe viewed as taking the representation of the noun,and up-weighting the elements it shares with theadjective, which is coherent with the notion of co-composition (Pustejovsky, 1995).
Previous work(Mitchell and Lapata, 2010) tuned the ?
parame-ter (?
= 16.7).
We use that value here, thoughfurther optimization might increase performance.For our evaluation we calculated the cosine dis-tance between pairs of phrases in the three dif-ferent representation spaces: observed, additionand dilation.
Results for a range of dimension-ality settings appear in Figure 2.
In the observedspace, we maximized performance when we in-88cluded all 1000 of the Document and 350 Depen-dency dimensions.
For consistency the y axis inFigure 2 extends only to 100 Document dimen-sions: changes beyond 100 dimensions for ob-served vectors were minimal.
By design, SVDwill tend to use lower dimensions to represent thestrongest signals in the input statistics, which typ-ically originate in the types of targets that are mostfrequent ?
in this case single words.
We have ob-served that less frequent and noisier counts, asmight be found for many phrases, are displacedto the higher dimensions.
Consistent with this ob-servation, maximum performance occurs using ahigh number of dimensions (correlation of 0.37 tohuman judgements of phrase similarity).Interestingly, using the single word vectors topredict the phrase vectors via the addition functiongives the best correlation of any of the represen-tations, outperforming even the observed phraserepresentations.
When using 25 Document di-mensions and 600 Dependency dimensions thecorrelation is 0.52, compared to the best per-formance of 0.51 using Dependency dimensionsonly.
We speculate that the advantage of com-posed vectors over observed vectors is due tosparseness and resulting noise/variance in the ob-served phrase vectors, as phrases are necessarilyless frequent than their constituent words.The dilation composition function performsslightly worse than addition, but shows best per-formance at the same point as addition.
Here, thehighest correlation (0.46) is substantially lowerthan that attained by addition, and uses 25 dimen-sions of the Document, and 600 dimensions of theDependency VSM.To summarize, without documents, {observed,addition and dilation} phrase vectors have maxi-mal correlations {0.37, 0.51 and 0.46}.
With doc-uments, {observed, addition and dilation} phrasevectors have maximal correlations {0.37, 0.52 and0.50}.
Our results using the addition function(0.52) outperform the results in two previous stud-ies (Mitchell and Lapata, 2010; Blacoe and Lap-ata, 2012): (0.46 and 0.48 respectively).
This isevidence that a VSM built from a larger corpus,and with both Document and Dependency infor-mation can yield superior results.4.3 Composed vs Observed Phrase VectorsNext we tested how well our representations andsemantic composition functions could predict theobserved vector statistics for phrases from thevectors of their component words.
Again, weexplored addition and dilation composition func-tions.
For testing we have 13, 575 vectors forwhich both the adjective and noun passed ourthresholds.
We predicted a composed phrase vec-tor using the statistics of the single words andone of the two composition functions (additionor dilation).
We then sorted the list of observedphrase vectors by their distance to the composedphrase vector and recorded the position of thecorresponding observed vector in the list.
Fromthis we calculated percentile rank, the percent ofphrases that are further from the predicted vec-tor than the observed vector.
Percentile rank is:100 ?
(1 ?
?rank/N) where ?rank is the aver-age position of the correct observed vector in thesorted list and N = 13, 575 is the size of the list.Figure 3 shows the changes in percentile rankin response to varying dimensions of Documentsand Dependencies for the addition function.
Di-lation results are not shown, but the pattern ofperformance is very similar.
In general, whenone includes more Document dimensions, the per-centile rank increases.
For both the dilation andaddition composition functions the peak perfor-mance is with 750 Dependency dimensions and1000 Document dimensions.
Dilation?s peak per-formance is 97.87; addition peaks at 98.03 per-centile rank.
As in Section 4.2, we see that theaccurate representation of phrases requires higherSVD dimensions.To evaluate when composition fails, we ex-amined the cases where the percentile rank was< 25%.
Amongst these words we found an over-representation of operational adjectives like ?bet-ter?
and ?more?.
As observed previously, it ispossible that such adjectives could be better rep-resented with a matrix or function (Socher et al2012; Baroni and Zamparelli, 2010).
Composi-tion may also be failing when the adjective-nounphrase is non-compositional (e.g.
lazy susan); fil-tering such phrases could improve performance.4.4 Brain Activity DataHere we explore the relationship between the neu-ral activity observed when a person reads a phrase,89100 250 500 750 10009393.59494.59595.59696.59797.598Number of Dependency DimensionsPercentileRankPercentile Rank for Varing Doc.
and Dep.
Dimensions (Addition)0 Doc Dims25501005007501000Figure 3: The percentile rank of observed phrasevectors compared to vectors created using the ad-dition composition function.and our predicted composed VSM for that phrase.We collected brain activity data using Magnetoen-cephalography (MEG).
MEG is a brain imagingmethod with much higher temporal resolution (1ms) than fMRI (?2 sec).
Since words are natu-rally read at a rate of about 2 per second, MEG is abetter candidate for capturing the fast dynamics ofsemantic composition in the brain.
Some previouswork has explored adjective-noun composition inthe brain (Chang et al 2009), but used fMRI andcorpus statistics based only on co-occurrence with5 hand-selected verbs.Our MEG data was collected while 9 partici-pants viewed 38 phrases, each repeated 20 times(randomly interleaved).
The stimulus nouns werechosen because previous research had shown themto be decodable from MEG recordings, and the ad-jectives were selected to modulate their most de-codable semantic properties (e.g.
edibility, ma-nipulability) (Sudre et al 2012).
The 8 adjec-tives selected are (?big?, ?small?, ?ferocious?,?gentle?, ?light?, ?heavy?, ?rotten?, ?tasty?
), andthe 6 nouns are (?dog?, ?bear?, ?tomato?, ?car-rot?, ?hammer?, ?shovel?).
The words ?big?
and?small?
are paired with every noun, ?ferocious?and ?gentle?
with animals, ?light?
and ?heavy?with tools and ?rotten?
and ?tasty?
with foods.We also included the words ?the?
and the word?thing?
as semantically neutral fillers, to presenteach of the words in a condition without seman-tic modulation.
In total there are 38 phrases (e.g.
?rotten carrot?, ?big hammer?
).In the MEG experiment, the adjective andpaired noun were each shown for 500ms, with a300ms interval between them, and there were 3Figure 4: Results for predicting composed phrasevectors (addition [4a] and dilation [4b]) fromMEG recordings.
Results shown are the aver-age over 9 subjects viewing 38 adjective-nounphrases.
This is the one task on which dilationoutperforms addition.
(a) Addition composition function results.
(b) Dilation composition function results.seconds in total time between the onset of subse-quent phrases.
Data was preprocessed to maxi-mize the signal/noise ratio as is common practice?
see Gross et al (2012).
The 20 repeated trialsfor each phrase were averaged together to createone average brain image per phrase.To determine if the recorded MEG data can beused to predict our composed vector space rep-resentations, we devised the following classifica-tion framework.2 The training data is comprisedof the averaged MEG signal for each of the 38phrases for one subject, and the labels are the 38phrases.
We use our VSMs and composition func-tions to form a mapping of the 38 phrases to com-2Predicting brain activity from VSM representations isalso possible, but provides additional challenges, as parts ofthe observed brain activity are not driven by semantics.90posed semantic feature vectors w ?
{s1 .
.
.
sm}.The mapping allows us to use Zero Shot Learn-ing (Palatucci et al 2009) to predict novel phrases(not seen during training) from a MEG record-ing.
This is a particularly attractive characteris-tic for the task of predicting words, as there aremany words and many more phrases in the En-glish language, and one cannot hope to collectMEG recordings for all of them.Formally, let us define the semantic represen-tation of a phrase w as semantic feature vector~sw = {s1...sm}, where the semantic space hasdimensionm that varies depending on the numberof Document and/or Dependency dimensions weinclude.
We utilize the mapping w ?
{s1 .
.
.
sm}to train m independent functions f1(X) ?s?1, .
.
.
, fm(X) ?
s?m where s?
represents thevalue of a predicted composed semantic feature.We combine the output of f1 .
.
.
fm to create thefinal predicted semantic vector ~s?
= {s?1 .
.
.
s?m}.We use cosine distance to quantify the distance be-tween true and predicted semantic vectors.To measure performance we use the 2 vs. 2 test.For each test we withhold two phrases and trainregressors on the remaining 36.
We use the re-gressors f and MEG data from the two held outphrases to create two predicted semantic vectors.We then choose the assignment of predicted se-mantic vectors (~s?i and ~s?j) to true semantic vec-tors (~si and ~sj) that minimizes the sum of cosinedistances.
If we choose the correct assignment(~s?i 7?
~si and ~s?j 7?
~sj) we mark the test as cor-rect.
2 vs. 2 accuracy is the number of 2 vs. 2tests with correct assignments divided by the totalnumber of tests.
There are (38 choose 2) = 703distinct 2 vs. 2 tests, and we evaluate on the subsetfor which neither the adjective nor noun are shared(540 pairs).
Chance performance is 0.50.For each f we trained a regressor with L2penalty.
We tune the regularization parame-ter with leave-one-out-cross-validation on trainingdata.
We train regressors using the first 800 ms ofMEG signal after the noun stimulus appears, whenwe assume semantic composition is taking place.Results appear in Figure 4.
The best perfor-mance (2 vs. 2 accuracy of 0.9440) is achievedwith dilation, 800 dimensions of Dependenciesand zero Document dimensions.
When we usethe addition composition function, optimal per-formance is 0.9212, at 600 Dependency and zeroDocument dimensions.
Note, however, that theparameter search here was much coarser that inSections 4.2 and 4.3, due to the computation re-quired.
We used a finer grid around the peaks inperformance for addition and dilation and foundminimal improvement (?0.5%) with the additionof a small number of Document dimensions.It is intriguing that this neurosemantic task isthe only task for which dilation outperforms addi-tion.
All other composition tasks explored in thisstudy were concerned with matching composedword vectors to observed or composed word vec-tors, whereas here we are interested in matchingcomposed word vectors to observed brain activity.Perhaps the brain works in a manner more akin tothe emphasis of elements as modeled by dilation,rather than a summing of features.
Further workis required to fully understand this phenomenon,but this is surely a thought-provoking result.35 ConclusionWe have performed a systematic study of comple-mentarity of topical (Document) and type (Depen-dency) features in Vector Space Model (VSM) forsemantic composition of adjective-noun phrases.To the best of our knowledge, this is one of thefirst such studies of this kind.
Through experi-ments on multiple real world benchmark datasets,we demonstrated the benefit of combining topic-and type-based features in a VSM.
Additionally,we introduced a novel task of predicting vec-tor representations of composed phrases from thebrain activity of human subjects reading thosephrases.
We exploited a large syntactically parsedcorpus to build our VSM models, and make thempublicly available.
We hope that the findings andresources from this paper will serve to inform fu-ture work on VSMs and semantic composition.AcknowledgmentWe are thankful to the anonymous reviewers for their con-structive comments.
We thank CMUs Parallel Data Labo-ratory (PDL) for making the OpenCloud cluster available,Justin Betteridge (CMU) for his help with parsing the corpus,and Yahoo!
for providing the M45 cluster.
This research hasbeen supported in part by DARPA (under contract numberFA8750-13-2-0005), NIH (NICHD award 1R01HD075328-01), Keck Foundation (DT123107), NSF (IIS0835797), andGoogle.
Any opinions, findings, conclusions and recommen-dations expressed in this paper are the authors and do notnecessarily reflect those of the sponsors.3No pun intended.91ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, Jana Kraval-ova, and Marius Pas.
2009.
A study on similarity andrelatedness using distributional and WordNet-based ap-proaches.
Proceedings of NAACL-HLT 2009.Abdulrahman Almuhareb and Massimo Poesio.
2004.Attribute-based and value-based clustering: An evalua-tion.
In Proceedings of EMNLP, pages 158?165.Marco Baroni and Alessandro Lenci.
2010.
Distributionalmemory: A general framework for corpus-based seman-tics.
Computational Linguistics, 36(4):673?721.Marco Baroni and Roberto Zamparelli.
2010.
Nouns arevectors, adjectives are matrices: Representing adjective-noun constructions in semantic space.
In Proceedingsof the 2010 Conference on Empirical Methods in Natu-ral Language Processing, pages 1183?1193.
Associationfor Computational Linguistics.W F Battig and W E Montague.
1969.
Category Normsfor Verbal Items in 56 Categories: A Replication and Ex-tension of the Connecticut Category Norms.
Journal ofExperimental Psychology Monographs, 80(3):1?46.William Blacoe and Mirella Lapata.
2012.
A Comparison ofVector-based Representations for Semantic Composition.In Proceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Computa-tional Natural Language Learning, pages 546?556, JejuIsland, Korea.David M Blei, Andrew Y Ng, and Michael I Jordan.
2003.Latent Dirichlet Allocation.
Journal of Machine LearningResearch, 3(4-5):993?1022.Jamie Callan and Mark Hoy.
2009.
The ClueWeb09 Dataset.http://boston.lti.cs.cmu.edu/Data/clueweb09/.Kai-min Chang, Vladimir L. Cherkassky, Tom M Mitchell,and Marcel Adam Just.
2009.
Quantitative modeling ofthe neural representation of adjective-noun phrases to ac-count for fMRI activation.
In Proceedings of the AnnualMeeting of the ACL and the 4th IJCNLP of the AFNLP,pages 638?646.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, EhudRivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin.2002.
Placing search in context: the concept revisited.ACM Transactions on Information Systems, 20(1):116?131.Thomas L Griffiths, Mark Steyvers, David M Blei, andJoshua B Tenenbaum.
2005.
Integrating topics and syn-tax.
Advances in neural information processing systems,17.Joachim Gross, Sylvain Baillet, Gareth R. Barnes,Richard N. Henson, Arjan Hillebrand, Ole Jensen, KarimJerbi, Vladimir Litvak, Burkhard Maess, Robert Oost-enveld, Lauri Parkkonen, Jason R. Taylor, Virginie vanWassenhove, Michael Wibral, and Jan-Mathijs Schoffe-len.
2012.
Good-practice for conducting and reportingMEG research.
NeuroImage, October.J Hall, J Nilsson, J Nivre, G Eryigit, B Megyesi, M Nilsson,and M Saers.
2007.
Single Malt or Blended?
A Studyin Multilingual Parser Optimization.
In Proceedings ofthe CoNLL Shared Task Session of EMNLPCoNLL 2007,volume s. 19-33, pages 933?939.
Association for Compu-tational Linguistics.Nancy Ide and Keith Suderman.
2006.
The American Na-tional Corpus First Release.
Proceedings of the Fifth Lan-guage Resources and Evaluation Conference (LREC).Eric Jones, Travis Oliphant, Pearu Peterson, and others.2001.
SciPy: Open source scientific tools for Python.George Karypis.
2003.
CLUTO: A Clustering Toolkit.Technical Report 02-017, Department of Computer Sci-ence, University of Minnesota.T Landauer and S Dumais.
1997.
A solution to Plato?s prob-lem: the latent semantic analysis theory of acquisition, in-duction, and representation of knowledge.
PsychologicalReview, 104(2):211?240.R B Lehoucq, D C Sorensen, and C Yang.
1998.
Arpackusers?
guide: Solution of large scale eigenvalue problemswith implicitly restarted Arnoldi methods.
SIAM.Dekang Lin and Xiaoyun Wu.
2009.
Phrase clustering fordiscriminative learning.
In Proceedings of the ACL.Dekang Lin.
1998.
Automatic Retrieval and Clustering ofSimilar Words.
In COLING-ACL, pages 768?774.K Lund and C Burgess.
1996.
Producing high-dimensionalsemantic spaces from lexical co-occurrence.
BehaviorResearch Methods, Instruments, and Computers, 28:203?208.George A Miller, Richard Beckwith, Christiane Fellbaum,Derek Gross, and Katherine Miller.
1990.
Introductionto WordNet: an on-line lexical database.
InternationalJournal of Lexicography, 3(4):235?244.Jeff Mitchell and Mirella Lapata.
2010.
Composition indistributional models of semantics.
Cognitive science,34(8):1388?429, November.Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012a.Comparing Abstract and Concrete Conceptual Represen-tations using Neurosemantic Decoding.
In NAACL Work-shop on Cognitive Modelling and Computational Linguis-tics.Brian Murphy, Partha Talukdar, and Tom Mitchell.
2012b.Selecting Corpus-Semantic Models for NeurolinguisticDecoding.
In First Joint Conference on Lexical and Com-putational Semantics (*SEM), pages 114?123, Montreal,Quebec, Canada.Brian Murphy, Partha Pratim Talukdar, and Tom Mitchell.2012c.
Learning Effective and Interpretable SemanticModels using Non-Negative Sparse Embedding.
In Inter-national Conference on Computational Linguistics (COL-ING 2012), Mumbai, India.S Pado?
and M Lapata.
2007.
Dependency-based construc-tion of semantic space models.
Computational Linguis-tics, 33(2):161?199.92Mark Palatucci, Geoffrey Hinton, Dean Pomerleau, andTom M Mitchell.
2009.
Zero-Shot Learning with Se-mantic Output Codes.
Advances in Neural InformationProcessing Systems, 22:1410?1418.James Pustejovsky.
1995.
The Generative Lexicon.
MITPress, Cambridge.Reinhard Rapp.
2003.
Word Sense Discovery Based onSense Descriptor Dissimilarity.
Proceedings of the NinthMachine Translation Summit, pp:315?322.Herbert Rubenstein and John B. Goodenough.
1965.
Con-textual correlates of synonymy.
Communications of theACM, 8(10):627?633, October.Mehrnoosh Sadrzadeh and Edward Grefenstette.
2011.
ACompositional Distributional Semantics Two ConcreteConstructions and some Experimental Evaluations.
Lec-ture Notes in Computer Science, 7052:35?47.Magnus Sahlgren.
2006.
The Word-Space Model: Using dis-tributional analysis to represent syntagmatic and paradig-matic relations between words in high-dimensional vectorspaces.
Dissertation, Stockholm University.Richard Socher, Brody Huval, Christopher D. Manning,and Andrew Y. Ng.
2012.
Semantic Compositionalitythrough Recursive Matrix-Vector Spaces.
In Conferenceon Empirical Methods in Natural Language Processingand Computational Natural Language Learning.Gustavo Sudre, Dean Pomerleau, Mark Palatucci, Leila We-hbe, Alona Fyshe, Riitta Salmelin, and Tom Mitchell.2012.
Tracking Neural Coding of Perceptual and Seman-tic Features of Concrete Nouns.
NeuroImage, 62(1):463?451, May.Peter D Turney.
2012.
Domain and Function : A Dual-SpaceModel of Semantic Relations and Compositions.
Journalof Artificial Intelligence Research, 44:533?585.93
