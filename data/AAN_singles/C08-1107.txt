Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 849?856Manchester, August 2008Learning Entailment Rules for Unary TemplatesIdan SzpektorDepartment of Computer ScienceBar-Ilan UniversityRamat Gan, Israelszpekti@macs.biu.ac.ilIdo DaganDepartment of Computer ScienceBar-Ilan UniversityRamat Gan, Israeldagan@macs.biu.ac.ilAbstractMost work on unsupervised entailmentrule acquisition focused on rules betweentemplates with two variables, ignoringunary rules - entailment rules betweentemplates with a single variable.
In this pa-per we investigate two approaches for un-supervised learning of such rules and com-pare the proposed methods with a binaryrule learning method.
The results showthat the learned unary rule-sets outperformthe binary rule-set.
In addition, a noveldirectional similarity measure for learningentailment, termed Balanced-Inclusion, isthe best performing measure.1 IntroductionIn many NLP applications, such as Question An-swering (QA) and Information Extraction (IE), itis crucial to recognize whether a specific targetmeaning is inferred from a text.
For example, aQA system has to deduce that ?SCO sued IBM?
isinferred from ?SCO won a lawsuit against IBM?to answer ?Whom did SCO sue??.
This type ofreasoning has been identified as a core semanticinference paradigm by the generic Textual Entail-ment framework (Giampiccolo et al, 2007).An important type of knowledge needed forsuch inference is entailment rules.
An entailmentrule specifies a directional inference relation be-tween two templates, text patterns with variables,such as ?X win lawsuit against Y ?
X sue Y ?.Applying this rule by matching ?X win lawsuitagainst Y ?
in the above text allows a QA system toc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.infer ?X sue Y ?
and identify ?IBM?, Y ?s instantia-tion, as the answer for the above question.
Entail-ment rules capture linguistic and world-knowledgeinferences and are used as an important buildingblock within different applications, e.g.
(Romanoet al, 2006).One reason for the limited performance ofgeneric semantic inference systems is the lack ofbroad-scale knowledge-bases of entailment rules(in analog to lexical resources such as WordNet).Supervised learning of broad coverage rule-sets isan arduous task.
This sparked intensive researchon unsupervised acquisition of entailment rules(and similarly paraphrases) e.g.
(Lin and Pantel,2001; Szpektor et al, 2004; Sekine, 2005).Most unsupervised entailment rule acquisitionmethods learn binary rules, rules between tem-plates with two variables, ignoring unary rules,rules between unary templates (templates withonly one variable).
However, a predicate quite of-ten appears in the text with just a single variable(e.g.
intransitive verbs or passives), where infer-ence requires unary rules, e.g.
?X take a nap?Xsleep?
(further motivations in Section 3.1).In this paper we focus on unsupervised learn-ing of unary entailment rules.
Two learning ap-proaches are proposed.
In our main approach,rules are learned by measuring how similar thevariable instantiations of two templates in a corpusare.
In addition to adapting state-of-the-art similar-ity measures for unary rule learning, we propose anew measure, termed Balanced-Inclusion, whichbalances the notion of directionality in entailmentwith the common notion of symmetric semanticsimilarity.
In a second approach, unary rules arederived from binary rules learned by state-of-the-art binary rule learning methods.We tested the various unsupervised unary rule849learning methods, as well as a binary rule learn-ing method, on a test set derived from a standardIE benchmark.
This provides the first comparisonbetween the performance of unary and binary rule-sets.
Several results rise from our evaluation: (a)while most work on unsupervised learning ignoredunary rules, all tested unary methods outperformedthe binary method; (b) it is better to learn unaryrules directly than to derive them from a binaryrule-base; (c) our proposed Balanced-Inclusionmeasure outperformed all other tested methods interms of F1 measure.
Moreover, only Balanced-Inclusion improved F1 score over a baseline infer-ence that does not use entailment rules at all .2 BackgroundThis section reviews relevant distributional simi-larity measures, both symmetric and directional,which were applied for either lexical similarity orunsupervised entailment rule learning.Distributional similarity measures follow theDistributional Hypothesis, which states that wordsthat occur in the same contexts tend to have similarmeanings (Harris, 1954).
Various measures wereproposed in the literature for assessing such simi-larity between two words, u and v. Given a word q,its set of features Fqand feature weights wq(f) forf ?
Fq, a common symmetric similarity measureis Lin similarity (Lin, 1998a):Lin(u, v) =?f?Fu?Fv[wu(f) + wv(f)]?f?Fuwu(f) +?f?Fvwv(f)where the weight of each feature is the pointwisemutual information (pmi) between the word andthe feature: wq(f) = log[Pr(f |q)Pr(f)].Weeds and Weir (2003) proposed to measure thesymmetric similarity between two words by av-eraging two directional (asymmetric) scores: thecoverage of each word?s features by the other.
Thecoverage of u by v is measured by:Cover(u, v) =?f?Fu?Fvwu(f)?f?Fuwu(f)The average can be arithmetic or harmonic:WeedsA(u, v) =12[Cover(u, v) + Cover(v, u)]WeedsH(u, v) =2 ?
Cover(u, v) ?
Cover(v, u)Cover(u, v) + Cover(v, u)Weeds et al also used pmi for feature weights.Binary rule learning algorithms adopted suchlexical similarity approaches for learning rules be-tween templates, where the features of each tem-plate are its variable instantiations in a corpus,such as {X=?SCO?, Y =?IBM?}
for the examplein Section 1.
Some works focused on learningrules from comparable corpora, containing com-parable documents such as different news articlesfrom the same date on the same topic (Barzilayand Lee, 2003; Ibrahim et al, 2003).
Such corporaare highly informative for identifying variations ofthe same meaning, since, typically, when variableinstantiations are shared across comparable docu-ments the same predicates are described.
However,it is hard to collect broad-scale comparable cor-pora, as the majority of texts are non-comparable.A complementary approach is learning from theabundant regular, non-comparable, corpora.
Yet,in such corpora it is harder to recognize varia-tions of the same predicate.
The DIRT algorithm(Lin and Pantel, 2001) learns non-directional bi-nary rules for templates that are paths in a depen-dency parse-tree between two noun variables Xand Y .
The similarity between two templates t andt?is the geometric average:DIRT (t, t?)
=?Linx(t, t?)
?
Liny(t, t?
)where Linxis the Lin similarity between X?s in-stantiations of t and X?s instantiations of t?ina corpus (equivalently for Liny).
Some workstake the combination of the two variable instantia-tions in each template occurrence as a single com-plex feature, e.g.
{X-Y =?SCO-IBM?
}, and com-pare between these complex features of t and t?
(Ravichandran and Hovy, 2002; Szpektor et al,2004; Sekine, 2005).Directional Measures Most rule learning meth-ods apply a symmetric similarity measure betweentwo templates, viewing them as paraphrasing eachother.
However, entailment is in general a direc-tional relation.
For example, ?X acquire Y ?X own Y ?
and ?countersuit against X ?
lawsuitagainst X?.
(Weeds and Weir, 2003) propose a directionalmeasure for learning hyponymy between twowords, ?l?
r?, by giving more weight to the cov-erage of the features of l by r (with ?
>12):WeedsD(l, r)=?Cover(l, r)+(1??
)Cover(r, l)When ?=1, this measure degenerates intoCover(l, r), termed Precision(l, r).
With850Precision(l, r) we obtain a ?soft?
version ofthe inclusion hypothesis presented in (Geffet andDagan, 2005), which expects l to entail r if the?important?
features of l appear also in r.Similarly, the LEDIR algorithm (Bhagat et al,2007) identifies the entailment direction betweentwo binary templates, l and r, which participatein a relation learned by (the symmetric) DIRT, bymeasuring the proportion of instantiations of l thatare covered by the instantiations of r.As far as we know, only (Shinyama et al, 2002)and (Pekar, 2006) learn rules between unary tem-plates.
However, (Shinyama et al, 2002) relieson comparable corpora for identifying paraphrasesand simply takes any two templates from compa-rable sentences that share a named entity instan-tiation to be paraphrases.
Such approach is notfeasible for non-comparable corpora where statis-tical measurement is required.
(Pekar, 2006) learnsrules only between templates related by local dis-course (information from different documents isignored).
In addition, their template structure islimited to only verbs and their direct syntactic ar-guments, which may yield incorrect rules, e.g.
forlight verbs (see Section 5.2).
To overcome this lim-itation, we use a more expressive template struc-ture.3 Learning Unary Entailment Rules3.1 MotivationsMost unsupervised rule learning algorithms fo-cused on learning binary entailment rules.
How-ever, using binary rules for inference is not enough.First, a predicate that can have multiple argumentsmay still occur with only one of its arguments.For example, in ?The acquisition of TCA was suc-cessful?, ?TCA?
is the only argument of ?acqui-sition?.
Second, some predicate expressions areunary by nature.
For example, modifiers, such as?the elected X?, or intransitive verbs.
In addition,it appears more tractable to learn all variations foreach argument of a predicate separately than tolearn them for combinations of argument pairs.For these reasons, it seems that unary rule learn-ing should be addressed in addition to binary rulelearning.
We are further motivated by the fact thatsome (mostly supervised) works in IE found learn-ing unary templates useful for recognizing relevantnamed entities (Riloff, 1996; Sudo et al, 2003;Shinyama and Sekine, 2006), though they did notattempt to learn generic knowledge bases of entail-ment rules.This paper investigates acquisition of unary en-tailment rules from regular non-comparable cor-pora.
We first describe the structure of unarytemplates and then explore two conceivable ap-proaches for learning unary rules.
The first ap-proach directly assesses the relation between twogiven templates based on the similarity of their in-stantiations in the corpus.
The second approach,which was also mentioned in (Iftene and Balahur-Dobrescu, 2007), derives unary rules from learnedbinary rules.3.2 Unary Template StructureTo learn unary rules we first need to define theirstructure.
In this paper we work at the syntac-tic representation level.
Texts are represented bydependency parse trees (using the Minipar parser(Lin, 1998b)) and templates by parse sub-trees.Given a dependency parse tree, any sub-tree canbe a candidate template, setting some of its nodesas variables (Sudo et al, 2003).
However, the num-ber of possible templates is exponential in the sizeof the sentence.
In the binary rule learning litera-ture, the main solution for exhaustively learning allrules between any pair of templates in a given cor-pus is to restrict the structure of templates.
Typi-cally, a template is restricted to be a path in a parsetree between two variable nodes (Lin and Pantel,2001; Ibrahim et al, 2003).Following this approach, we chose the structureof unary templates to be paths as well, where oneend of the path is the template?s variable.
How-ever, paths with one variable have more expressivepower than paths between two variables, since thecombination of two unary paths may generate abinary template that is not a path.
For example,the combination of ?X call indictable?
and ?call Yindictable?
is the template ?X call Y indictable?,which is not a path between X and Y .For every noun node v in a parsed sentence, wegenerate templates with v as a variable as follows:1.
Traverse the path from v towards the root ofthe parse tree.
Whenever a candidate pred-icate is encountered (any noun, adjective orverb) the path from that node to v is taken asa template.
We stop when the first verb orclause boundary (e.g.
a relative clause) is en-countered, which typically represent the syn-tactic boundary of a specific predicate.8512.
To enable templates with control verbs andlight verbs, e.g.
?X help preventing?, ?Xmake noise?, whenever a verb is encoun-tered we generate templates that are paths be-tween v and the verb?s modifiers, either ob-jects, prepositional complements or infiniteor gerund verb forms (paths ending at stopwords, e.g.
pronouns, are not generated).3.
To capture noun modifiers that act as predi-cates, e.g.
?the losingX?, we extract templatepaths between v and each of its modifiers,nouns or adjectives, that are derived from averb.
We use the Catvar database to identifyverb derivations (Habash and Dorr, 2003).As an example for the procedure, the templatesextracted from the sentence ?The losing partyplayed it safe?
with ?party?
as the variable are:?losing X?, ?X play?
and ?X play safe?.3.3 Direct Learning of Unary RulesWe applied the lexical similarity measures pre-sented in Section 2 for unary rule learning.
Eachargument instantiation of template t in the corpusis taken as a feature f , and the pmi between t andf is used for the feature?s weight.
We first adaptedDIRT for unary templates (unary-DIRT, apply-ing Lin-similarity to the single feature vector), aswell as its output filtering by LEDIR.
The variousWeeds measures were also applied1: symmetricarithmetic average, symmetric harmonic average,weighted arithmetic average and Precision.After initial analysis, we found that given a righthand side template r, symmetric measures suchas Lin (in DIRT) generally tend to prefer (scorehigher) relations ?l, r?
in which l and r are relatedbut do not necessarily participate in an entailmentor equivalence relation, e.g.
the wrong rule ?kill X?
injure X?.On the other hand, directional measures such asWeeds Precision tend to prefer directional rules inwhich the entailing template is infrequent.
If an in-frequent template has common instantiations withanother template, the coverage of its features istypically high, whether or not an entailment rela-tion exists between the two templates.
This behav-ior generates high-score incorrect rules.Based on this analysis, we propose a newmeasure that balances the two behaviors, termed1We applied the best performing parameter values pre-sented in (Bhagat et al, 2007) and (Weeds and Weir, 2003).Balanced-Inclusion (BInc).
BInc identifies entail-ing templates based on a directional measure butpenalizes infrequent templates using a symmetricmeasure:BInc(l, r) =?Lin(l, r) ?
Precision(l, r)3.4 Deriving Unary Rules From Binary RulesAn alternative way to learn unary rules is to firstlearn binary entailment rules and then derive unaryrules from them.
We derive unary rules from agiven binary rule-base in two steps.
First, for eachbinary rule, we generate all possible unary rulesthat are part of that rule (each unary template isextracted following the same procedure describedin Section 3.2).
For example, from ?X find solu-tion to Y ?
X solve Y ?
we generate the unaryrules ?X find?
X solve?, ?X find solution?
Xsolve?, ?solution to Y ?
solve Y ?
and ?find solu-tion to Y ?
solve Y ?.
The score of each generatedrule is set to be the score of the original binary rule.The same unary rule can be derived from dif-ferent binary rules.
For example, ?hire Y ?
em-ploy Y ?
is derived both from ?X hire Y ?
X em-ploy Y ?
and ?hire Y for Z ?
employ Y for Z?,having a different score from each original binaryrule.
The second step of the algorithm aggregatesthe different scores yielded for each derived ruleto produce the final rule score.
Three aggregationfunctions were tested: sum (Derived-Sum), aver-age (Derived-Avg) and maximum (Derived-Max).4 Experimental SetupWe want to evaluate learned unary and binary rulebases by their utility for NLP applications throughassessing the validity of inferences that are per-formed in practice using the rule base.To perform such experiments, we need a test-set of seed templates, which correspond to a set oftarget predicates, and a corpus annotated with allargument mentions of each predicate.
The evalu-ation assesses the correctness of all argument ex-tractions, which are obtained by matching in thecorpus either the seed templates or templates thatentail them according to the rule-base (the lattercorresponds to rule-application).Following (Szpektor et al, 2008), we found theACE 2005 event training set2useful for this pur-pose.
This standard IE dataset includes 33 types ofevent predicates such as Injure, Sue and Divorce.2http://projects.ldc.upenn.edu/ace/852All event mentions are annotated in the corpus, in-cluding the instantiated arguments of the predicate.ACE guidelines specify for each event its possiblearguments, each associated with a semantic role.For instance, some of the Injure event argumentsare Agent, Victim and Time.To utilize the ACE dataset for evaluating entail-ment rule applications, we manually representedeach ACE event predicate by unary seed templates.For example, the seed templates for Injure are ?Ainjure?, ?injure V ?
and ?injure in T ?.
We mappedeach event role annotation to the correspondingseed template variable, e.g.
?Agent?
to A and?Victim?
to V in the above example.
Templatesare matched using a syntactic matcher that han-dles simple morpho-syntactic phenomena, as in(Szpektor and Dagan, 2007).
A rule applicationis considered correct if the matched argument isannotated by the corresponding ACE role.For testing binary rule-bases, we automaticallygenerated binary seed templates from any twounary seeds that share the same predicate.
For ex-ample, for Injure the binary seeds ?A injure V ?, ?Ainjure in T ?
and ?injure V in T ?
were automaticallygenerated from the above unary seeds.We performed two adaptations to the ACEdataset to fit it better to our evaluation needs.
First,our evaluation aims at assessing the correctness ofinferring a specific target semantic meaning, whichis denoted by a specific predicate, using rules.Thus, four events that correspond ambiguously tomultiple distinct predicates were ignored.
For in-stance, the Transfer-Money event refers to both do-nating and lending money, and thus annotations ofthis event cannot be mapped to a specific seed tem-plate.
We also omitted 3 events with less than 10mentions, and were left with 26 events (6380 argu-ment mentions).Additionally, we regard all entailing mentionsunder the textual entailment definition as correct.However, event mentions are annotated as correctin ACE only if they explicitly describe the targetevent.
For instance, a Divorce mention does en-tail a preceding marriage event but it does not ex-plicitly describe it, and thus it is not annotated asa Marry event.
To better utilize the ACE dataset,we considered for a target event the annotations ofother events that entail it as being correct as well.We note that each argument was considered sep-arately.
For example, we marked a mention of adivorced person as entailing the marriage of thatperson, but did not consider the place and time ofthe divorce act to be those of the marriage .5 Results and AnalysisWe implemented the unary rule learning algo-rithms described in Section 3 and the binary DIRTalgorithm (Lin and Pantel, 2001).
We executedeach method over the Reuters RCV1 corpus3,learning for each template r in the corpus the top100 rules in which r is entailed by another tem-plate l, ?l?
r?.
All rules were learned in canonicalform (Szpektor and Dagan, 2007).
The rule-baselearned by binary DIRT was taken as the input forderiving unary rules from binary rules.The performance of each acquired rule-base wasmeasured for each ACE event.
We measured thepercentage of correct argument mentions extractedout of all correct argument mentions annotated forthe event (recall) and out of all argument mentionsextracted for the event (precision).
We also mea-sured F1, their harmonic average, and report macroaverage Recall, Precision and F1 over the 26 eventtypes.No threshold setting mechanism is suggested inthe literature for the scores of the different algo-rithms, especially since rules for different righthand side templates have different score ranges.Thus, we follow common evaluation practice (Linand Pantel, 2001; Geffet and Dagan, 2005) and testeach learned rule-set by taking the top K rules foreach seed template, whereK ranges from 0 to 100.WhenK=0, no rules are used and mentions are ex-tracted only by direct matching of seed templates.Our rule application setting provides a rathersimplistic IE system (for example, no named entityrecognition or approximate template matching).
Itis thus useful for comparing different rule-bases,though the absolute extraction figures do not re-flect the full potential of the rules.
In Secion 5.2we analyze the full-system?s errors to isolate therules?
contribution to overall system performance.5.1 ResultsIn this section we focus on the best performingvariations of each algorithm type: binary DIRT,unary DIRT, unary Weeds Harmonic, BInc andDerived-Avg.
We omitted the results of methodsthat were clearly inferior to others: (a) WeedsA,WeedsD and Weeds-Precision did not increase3http://about.reuters.com/researchandstandards/corpus/853Recall over not using rules because rules with in-frequent templates scored highest and arithmeticaveraging could not balance well these high scores;(b) out of the methods for deriving unary rulesfrom binary rule-bases, Derived-Avg performedbest; (c) filtering with (the directional) LEDIR didnot improve the performance of unary DIRT.Figure 1 presents Recall, Precision and F1 of themethods for different cutoff points.
First, we ob-serve that even when matching only the seed tem-plates (K=0), unary seeds outperform the binaryseeds in terms of both Precision and Recall.
Thissurprising behavior is consistent through all rulecutoff points: all unary learning algorithms per-form better than binary DIRT in all parameters.The inferior behavior of binary DIRT is analyzedin Section 5.2.The graphs show that symmetric unary ap-proaches substantially increase recall, but dramati-cally decrease precision already at the top 10 rules.As a result, F1 only decreases for these methods.Lin similarity (DIRT) and Weeds-Harmonic showsimilar behaviors.
They consistently outperformDerived-Avg.
One reason for this is that incorrectunary rules may be derived even from correct bi-nary rules.
For example, from ?X gain seat onY ?
elect X to Y ?
the incorrect unary rule ?Xgain?
electX?
is also generated.
This problem isless frequent when unary rules are directly scoredbased on their corpus statistics.The directional measure of BInc yields a moreaccurate rule-base, as can be seen by the muchslower precision reduction rate compared to theother algorithms.
As a result, it is the only algo-rithm that improves over the F1 baseline of K=0,with the best cutoff point at K=20.
BInc?s re-call increases moderately compared to other unarylearning approaches, but it is still substantially bet-ter than not using rules (a relative recall increase of50% already at K=10).
We found that many of thecorrect mentions missed by BInc but identified byother methods are due to occasional extractions ofincorrect frequent rules, such as partial templates(see Section 5.2).
This is reflected in the very lowprecision of the other methods.
On the other hand,some correct rules were only learned by BInc, e.g.
?countersuit againstX?X sue?
and ?X take wife?
X marry?.When only one argument is annotated for a spe-cific event mention (28% of ACE predicate men-tions, which account for 15% of all annotated ar-Figure 1: Average Precision, Recall and F1 at dif-ferent top K rule cutoff points.guments), binary rules either miss that mention, orextract both the correct argument and another in-correct one.
To neutralize this bias, we also testedthe various methods only on event mentions an-notated with two or more arguments and obtainedsimilar results to those presented for all mentions.This further emphasizes the general advantage ofusing unary rules over binary rules.8545.2 AnalysisBinary-DIRT We analyzed incorrect rules bothfor binary-DIRT and BInc by randomly sampling,for each algorithm, 200 rules that extracted incor-rect mentions.
We manually classified each rule ?l?
r?
as either: (a) Correct - the rule is valid insome contexts of the event but extracted some in-correct mentions; (b) Partial Template - l is only apart of a correct template that entails r. For exam-ple, learning ?X decide?
X meet?
instead of ?Xdecide to meet ?
X meet?
; (e) Incorrect - otherincorrect rules, e.g.
?charge X ?
convict X?.Table 1 summarizes the analysis and demon-strates two problems of binary-DIRT.
First, rela-tive to BInc, it tends to learn incorrect rules forhigh frequency templates, and therefore extractedmany more incorrect mentions for the same num-ber of incorrect rules.
Second, a large percentageof incorrect mentions extracted are due to partialtemplates at the rule left-hand-side.
Such rules areleaned because many binary templates have a morecomplex structure than paths between arguments.As explained in Section 3.2 the unary templatestructure we use is more expressive, enabling tolearn the correct rules.
For example, BInc learned?take Y into custody ?
arrest Y ?
while binary-DIRT learned ?X take Y ?
X arrest Y ?.System Level Analysis We manually analyzedthe reasons for false positives (incorrect extrac-tions) and false negatives (missed extractions) ofBInc, at its best performing cutoff point (K=20),by sampling 200 extractions of each type.From the false positives analysis (Table 2) wesee that 39% of the errors are due to incorrect rules.The main reasons for learning such rules are thosediscussed in Section 3.3: (a) related templates thatare not entailing; (b) infrequent templates.
Alllearning methods suffer from these issues.
As wasshown by our results, BInc provides a first step to-wards reducing these problems.
Yet, these issuesrequire further research.Apart from incorrectly learned rules, incorrecttemplate matching (e.g.
due to parse errors) andcontext mismatch contribute together 46% of theerrors.
Context mismatches occur when the entail-ing template is matched in inappropriate contexts.For example, ?slam X ?
attack X?
should not beapplied when X is a ball, only when it is a person.The rule-set net effect on system precision is betterestimated by removing these errors and fixing theannotation errors, which yields 72% precision.Binary DIRT Balanced InclusionCorrect 16 (70) 38 (91)Partial Template 27 (2665) 6 (81)Incorrect 157 (2584) 156 (787)Total 200 (5319) 200 (959)Table 1: Rule type distribution of a sample of 200rules that extracted incorrect mentions.
The corre-sponding numbers of incorrect mentions extractedby the sampled rules is shown in parentheses.Reason % mentionsIncorrect Rule learned 39.0Context mismatch 27.0Match error 19.0Annotation problem 15.0Table 2: Distribution of reasons for false positives(incorrect argument extractions) by BInc at K=20.Reason % mentionsRule not learned 61.5Match error 25.0Discourse analysis needed 12.0Argument is predicative 1.5Table 3: Distribution of reasons for false negatives(missed argument mentions) by BInc at K=20.Table 3 presents the analysis of false negatives.First, we note that 12% of the arguments cannotbe extracted by rules alone, due to necessary dis-course analysis.
Thus, a recall upper bound for en-tailment rules is 88%.
Many missed extractions aredue to rules that were not learned (61.5%).
How-ever, 25% of the mentions were missed because ofincorrect syntactic matching of correctly learnedrules.
By assuming correct matches in these caseswe isolate the recall of the rule-set (along with theseeds), which yields 39% recall.6 ConclusionsWe presented two approaches for unsupervised ac-quisition of unary entailment rules from regular(non-comparable) corpora.
In the first approach,rules are directly learned based on distributionalsimilarity measures.
The second approach de-rives unary rules from a given rule-base of binaryrules.
Under the first approach we proposed anovel directional measure for scoring entailmentrules, termed Balanced-Inclusion.We tested the different approaches utilizing astandard IE test-set and compared them to binaryrule learning.
Our results suggest the advantage oflearning unary rules: (a) unary rule-bases perform855better than binary rules; (b) it is better to directlylearn unary rules than to derive them from binaryrule-bases.
In addition, the Balanced-Inclusionmeasure outperformed all other tested methods.In future work, we plan to explore additionalunary template structures and similarity scores,and to improve rule application utilizing contextmatching methods such as (Szpektor et al, 2008).AcknowledgementsThis work was partially supported by ISF grant1095/05, the IST Programme of the EuropeanCommunity under the PASCAL Network of Ex-cellence IST-2002-506778 and the NEGEV project(www.negev-initiative.org).ReferencesBarzilay, Regina and Lillian Lee.
2003.
Learn-ing to paraphrase: An unsupervised approach us-ing multiple-sequence alignment.
In Proceedings ofHLT-NAACL.Bhagat, Rahul, Patrick Pantel, and Eduard Hovy.
2007.Ledir: An unsupervised algorithm for learning di-rectionality of inference rules.
In Proceedings ofEMNLP.Geffet, Maayan and Ido Dagan.
2005.
The distribu-tional inclusion hypotheses and lexical entailment.In Proceedings of ACL.Giampiccolo, Danilo, Bernardo Magnini, Ido Dagan,and Bill Dolan.
2007.
The third pascal recogniz-ing textual entailment challenge.
In Proceedings ofWTEP.Habash, Nizar and Bonnie Dorr.
2003.
A categorialvariation database for english.
In Proceedings ofNACL.Harris, Z.
1954.
Distributional structure.
Word,10(23):146?162.Ibrahim, Ali, Boris Katz, and Jimmy Lin.
2003.
Ex-tracting structural paraphrases from aligned mono-lingual corpora.
In Proceedings of IWP.Iftene, Adrian and Alexandra Balahur-Dobrescu.
2007.Hypothesis transformation and semantic variabilityrules used in recognizing textual entailment.
In Pro-ceedings of WTEP.Lin, Dekang and Patrick Pantel.
2001.
Discovery ofinference rules for question answering.
In Natu-ral Language Engineering, volume 7(4), pages 343?360.Lin, Dekang.
1998a.
Automatic retrieval and cluster-ing of similar words.
In Proceedings of COLING-ACL.Lin, Dekang.
1998b.
Dependency-based evaluation ofminipar.
In Proceedings of the Workshop on Evalu-ation of Parsing Systems at LREC.Pekar, Viktor.
2006.
Acquisition of verb entailmentfrom text.
In Proceedings of NAACL.Ravichandran, Deepak and Eduard Hovy.
2002.
Learn-ing surface text patterns for a question answeringsystem.
In Proceedings of ACL.Riloff, Ellen.
1996.
Automatically generating extrac-tion patterns from untagged text.
In AAAI/IAAI, Vol.2, pages 1044?1049.Romano, Lorenza, Milen Kouylekov, Idan Szpektor,Ido Dagan, and Alberto Lavelli.
2006.
Investigat-ing a generic paraphrase-based approach for relationextraction.
In Proceedings of EACL.Sekine, Satoshi.
2005.
Automatic paraphrase discov-ery based on context and keywords between ne pairs.In Proceedings of IWP.Shinyama, Yusuke and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted rela-tion discovery.
In Proceedings of HLT-NAACL.Shinyama, Yusuke, Satoshi Sekine, Sudo Kiyoshi, andRalph Grishman.
2002.
Automatic paraphrase ac-quisition from news articles.
In Proceedings of HLT.Sudo, Kiyoshi, Satoshi Sekine, and Ralph Grishman.2003.
An improved extraction pattern representationmodel for automatic ie pattern acquisition.
In Pro-ceedings of ACL.Szpektor, Idan and Ido Dagan.
2007.
Learning canon-ical forms of entailment rules.
In Proceedings ofRANLP.Szpektor, Idan, Hristo Tanev, Ido Dagan, and Bonaven-tura Coppola.
2004.
Scaling web-based acquisitionof entailment relations.
In Proceedings of EMNLP.Szpektor, Idan, Ido Dagan, Roy Bar Haim, and JacobGoldberger.
2008.
Contextual preferences.
In Pro-ceedings of ACL.Weeds, Julie and David Weir.
2003.
A general frame-work for distributional similarity.
In Proceedings ofEMNLP.856
