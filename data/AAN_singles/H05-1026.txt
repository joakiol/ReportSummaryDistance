Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 201?208, Vancouver, October 2005. c?2005 Association for Computational LinguisticsTraining Neural Network Language ModelsOn Very Large Corpora ?Holger Schwenk and Jean-Luc GauvainLIMSI-CNRSBP 133, 91436 Orsay cedex, FRANCEschwenk,gauvain@limsi.frAbstractDuring the last years there has been grow-ing interest in using neural networks forlanguage modeling.
In contrast to the wellknown back-off n-gram language models,the neural network approach attempts toovercome the data sparseness problem byperforming the estimation in a continuousspace.
This type of language model wasmostly used for tasks for which only avery limited amount of in-domain trainingdata is available.In this paper we present new algorithms totrain a neural network language model onvery large text corpora.
This makes pos-sible the use of the approach in domainswhere several hundreds of millions wordsof texts are available.
The neural networklanguage model is evaluated in a state-of-the-art real-time continuous speech recog-nizer for French Broadcast News.
Worderror reductions of 0.5% absolute are re-ported using only a very limited amountof additional processing time.1 IntroductionLanguage models play an important role in manyapplications like character and speech recognition,machine translation and information retrieval.
Sev-eral approaches have been developed during the last?This work was partially financed by the European Commis-sion under the FP6 Integrated Project TC-STAR.decades like n-gram back-off word models (Katz,1987), class models (Brown et al, 1992), structuredlanguage models (Chelba and Jelinek, 2000) or max-imum entropy language models (Rosenfeld, 1996).To the best of our knowledge word and class n-gramback-off language models are still the dominant ap-proach, at least in applications like large vocabularycontinuous speech recognition or statistical machinetranslation.
In many publications it has been re-ported that modified Kneser-Ney smoothing (Chenand Goodman, 1999) achieves the best results.
Allthe reference back-off language models (LM) de-scribed in this paper are build with this technique,using the SRI LM toolkit (Stolcke, 2002).The field of natural language processing has re-cently seen some changes by the introduction of newstatistical techniques that are motivated by success-ful approaches from the machine learning commu-nity, in particular continuous space LMs using neu-ral networks (Bengio and Ducharme, 2001; Bengioet al, 2003; Schwenk and Gauvain, 2002; Schwenkand Gauvain, 2004; Emami and Jelinek, 2004), Ran-dom Forest LMs (Xu and Jelinek, 2004) and Ran-dom cluster LMs (Emami and Jelinek, 2005).
Usu-ally new approaches are first verified on small tasksusing a limited amount of LM training data.
Forinstance, experiments have been performed usingthe Brown corpus (1.1M words), parts of the Wall-street journal corpus (19M words) or transcriptionsof acoustic training data (up to 22M words).
It ismuch more challenging to compare the new statis-tical techniques to carefully optimized back-off LMtrained on large amounts of data (several hundredmillions words).
Training may be difficult and very201time consuming and the algorithms used with sev-eral tens of millions examples may be impracticablefor larger amounts.
Training back-off LMs on largeamounts of data is not a problem, as long as power-ful machines with enough memory are available inorder to calculate the word statistics.
Practice hasalso shown that back-off LMs seem to perform verywell when large amounts of training data are avail-able and it is not clear that the above mentioned newapproaches are still of benefit in this situation.In this paper we compare the neural networklanguage model to n-gram model with modifiedKneser-Ney smoothing using LM training corporaof up to 600M words.
New algorithms are pre-sented to effectively train the neural network on suchamounts of data and the necessary capacity is ana-lyzed.
The LMs are evaluated in a real-time state-of-the-art speech recognizer for French BroadcastNews.
Word error reductions of up to 0.5% abso-lute are reported.2 Architecture of the neural network LMThe basic idea of the neural network LM is to projectthe word indices onto a continuous space and to usea probability estimator operating on this space (Ben-gio and Ducharme, 2001; Bengio et al, 2003).
Sincethe resulting probability functions are smooth func-tions of the word representation, better generaliza-tion to unknown n-grams can be expected.
A neuralnetwork can be used to simultaneously learn the pro-jection of the words onto the continuous space andto estimate the n-gram probabilities.
This is still an-gram approach, but the LM posterior probabilitiesare ?interpolated?
for any possible context of lengthn-1 instead of backing-off to shorter contexts.The architecture of the neural network n-gramLM is shown in Figure 1.
A standard fully-connected multi-layer perceptron is used.
Theinputs to the neural network are the indices ofthe n?1 previous words in the vocabulary hj =wj?n+1, ..., wj?2, wj?1 and the outputs are the pos-terior probabilities of all words of the vocabulary:P (wj = i|hj) ?i ?
[1, N ] (1)where N is the size of the vocabulary.
The inputuses the so-called 1-of-n coding, i.e., the i-th wordof the vocabulary is coded by setting the i-th ele-ment of the vector to 1 and all the other elements toprojectionlayer hiddenlayeroutputlayerinputprojectionssharedcontinuousrepresentation: representation:indices in wordlistLM probabilitiesdiscrete for all wordsprobability estimationNeural NetworkNwj?1 PHNP (wj=1|hj)wj?n+1wj?n+2P (wj=i|hj)P (wj=N|hj)P dimensional vectorsckoiMVdjp1 =pN =pi =Figure 1: Architecture of the neural networklanguage model.
hj denotes the contextwj?n+1, ..., wj?1.
P is the size of one projec-tion and H and N is the size of the hidden andoutput layer respectively.
When shortlists are usedthe size of the output layer is much smaller then thesize of the vocabulary.0.
The i-th line of the N ?P dimensional projectionmatrix corresponds to the continuous representationof the i-th word.
Let us denote ck these projections,dj the hidden layer activities, oi the outputs, pi theirsoftmax normalization, and mjl, bj , vij and ki thehidden and output layer weights and the correspond-ing biases.
Using these notations the neural networkperforms the following operations:dj = tanh(?lmjl cl + bj)(2)oi =?jvij dj + ki (3)pi = eoi /N?k=1eok (4)The value of the output neuron pi corresponds di-rectly to the probability P (wj = i|hj).
Training isperformed with the standard back-propagation algo-rithm minimizing the following error function:E =N?i=1ti log pi + ?
(?jlm2jl +?ijv2ij) (5)where ti denotes the desired output, i.e., the proba-bility should be 1.0 for the next word in the training202sentence and 0.0 for all the other ones.
The first partof this equation is the cross-entropy between the out-put and the target probability distributions, and thesecond part is a regularization term that aims to pre-vent the neural network from overfitting the trainingdata (weight decay).
The parameter ?
has to be de-termined experimentally.It can be shown that the outputs of a neural net-work trained in this manner converge to the posteriorprobabilities.
Therefore, the neural network directlyminimizes the perplexity on the training data.
Notealso that the gradient is back-propagated through theprojection-layer, which means that the neural net-work learns the projection of the words onto the con-tinuous space that is best for the probability estima-tion task.
The complexity to calculate one probabil-ity with this basic version of the neural network LMis quite high:O = (n?
1)?
P ?H + H + H ?N + N (6)where P is the size of one projection and H and N isthe size of the hidden and output layer respectively.Usual values are n=4, P=50 to 200, H=400 to 1000and N=40k to 200k.
The complexity is dominatedby the large size of the output layer.
In this paper theimprovements described in (Schwenk, 2004) havebeen used:1.
Lattice rescoring: speech recognition is donewith a standard back-off LM and a word latticeis generated.
The neural network LM is thenused to rescore the lattice.2.
Shortlists: the neural network is only used topredict the LM probabilities of a subset of thewhole vocabulary.3.
Regrouping: all LM probabilities needed forone lattice are collected and sorted.
By thesemeans all LM probability requests with thesame context ht lead to only one forward passthrough the neural network.4.
Block mode: several examples are propagatedat once through the neural network, allowingthe use of faster matrix/matrix operations.5.
CPU optimization: machine specific BLASlibraries are used for fast matrix and vector op-erations.The idea behind shortlists is to use the neuralnetwork only to predict the s most frequent words,s ?
|V |, reducing by these means drastically thecomplexity.
All words of the word list are still con-sidered at the input of the neural network.
The LMprobabilities of words in the shortlist (P?N ) are cal-culated by the neural network and the LM probabil-ities of the remaining words (P?B) are obtained froma standard 4-gram back-off LM:P?
(wt|ht) ={P?N (wt|ht)PS(ht) if wt ?
shortlistP?B(wt|ht) else(7)PS(ht) =?w?shortlist(ht)P?B(w|ht) (8)It can be considered that the neural network redis-tributes the probability mass of all the words in theshortlist.
This probability mass is precalculated andstored in the data structures of the back-off LM.
Aback-off technique is used if the probability mass fora requested input context is not directly available.Normally, the output of a speech recognition sys-tem is the most likely word sequence given theacoustic signal, but it is often advantageous to pre-serve more information for subsequent processingsteps.
This is usually done by generating a lattice,a graph of possible solutions where each arc cor-responds to a hypothesized word with its acousticand language model scores.
In the context of thiswork LIMSI?s standard large vocabulary continuousspeech recognition decoder is used to generate lat-tices using a n-gram back-off LM.
These lattices arethen processed by a separate tool and all the LMprobabilities on the arcs are replaced by those calcu-lated by the neural network LM.
During this latticerescoring LM probabilities with the same context htare often requested several times on potentially dif-ferent nodes in the lattice.
Collecting and regroupingall these calls prevents multiple forward passes sinceall LM predictions for the same context are immedi-ately available at the output.Further improvements can be obtained by prop-agating several examples at once though the net-work, also known as bunch mode (Bilmes et al,1997; Schwenk, 2004).
In comparison to equation 2and 3, this results in using matrix/matrix instead ofmatrix/vector operations which can be aggressivelyoptimized on current CPU architectures.
The Intel203Math Kernel Library was used.1 Bunch mode is alsoused for training the neural network.
Training of atypical network with a hidden layer with 500 nodesand a shortlist of length 2000 (about 1M parameters)take less than one hour for one epoch through fourmillion examples on a standard PC.3 Application to Speech RecognitionIn this paper the neural network LM is evaluatedin a real-time speech recognizer for French Broad-cast News.
This is a very challenging task sincethe incorporation of the neural network LM intothe speech recognizer must be very effective dueto the time constraints.
The speech recognizer it-self runs in 0.95xRT2 and the neural network in lessthan 0.05xRT.
The compute platform is an Intel Pen-tium 4 extreme (3.2GHz, 4GB RAM) running Fe-dora Core 2 with hyper-threading.The acoustic model uses tied-state position-dependent triphones trained on about 190 hours ofBroadcast News data.
The speech features consistof 39 cepstral parameters derived from a Mel fre-quency spectrum estimated on the 0-8kHz band (or0-3.8kHz for telephone data) every 10ms.
Thesecepstral coefficients are normalized on a segmentcluster basis using cepstral mean removal and vari-ance normalization.
The feature vectors are linearlytransformed (MLLT) to better fit the diagonal co-variance Gaussians used for acoustic modeling.Decoding is performed in two passes.
The firstfast pass generates an initial hypothesis, followedby acoustic model adaptation (CMLLR and MLLR)and a second decode pass using the adapted mod-els.
Each pass generates a word lattice which is ex-panded with a 4-gram LM.
The best solution is thenextracted using pronunciation probabilities and con-sensus decoding.
Both passes use very tight prun-ing thresholds, especially for the first pass, and fastGaussian computation based on Gaussian short lists.For the final decoding pass, the acoustic modelsinclude 23k position-dependent triphones with 12ktied states, obtained using a divisive decision treebased clustering algorithm with a 35 base phone set.1http://www.intel.com/software/products/mkl/2In speech recognition, processing time is measured in mul-tiples of the length of the speech signal, the real time factorxRT.
For a speech signal of 2h, a processing time of 0.5xRTcorresponds to 1h of calculation.The system is described in more detail in (Gauvainet al, 2005).The neural network LM is used in the last passto rescore the lattices.
A short-list of length 8192was used in order to fulfill the constraints on the pro-cessing time (the complexity of the neural networkto calculate a LM probability is almost linear withthe length of the short-list).
This gives a coverage ofabout 85% when rescoring the lattices, i.e.
the per-centage of LM requests that are actually performedby the neural network.3.1 Language model training dataThe following resources have been used for lan-guage modeling:?
Transcriptions of the acoustic training data(4.0M words)?
Commercial transcriptions (88.5M words)?
Newspaper texts (508M words)?
WEB data (13.6M words)First a language model was built for each cor-pus using modified Kneser-Ney smoothing as imple-mented in the SRI LM toolkit (Stolcke, 2002).
Theindividual LMs were then interpolated and mergedtogether.
An EM procedure was used to determinethe coefficients that minimize the perplexity on thedevelopment data.
Table 1 summarizes the charac-teristics of the individual text corpora.corpus #words Perpl.
Coeffs.Acoustic transcr.
4M 107.4 0.43Commercial transcr.
88.5M 137.8 0.14Newspaper texts 508M 103.0 0.35WEB texts 13.6M 136.7 0.08All interpolated 614M 70.2 -Table 1: Characteristics of the text corpora (numberof words, perplexity on the development corpus andinterpolation coefficients)Although the detailed transcriptions of the audiodata represent only a small fraction of the availabledata, they get an interpolation coefficient of 0.43.This shows clearly that they are the most appropriatetext source for the task.
The commercial transcripts,204the newspaper and WEB texts reflect less well thespeaking style of broadcast news, but this is to someextent counterbalanced by the large amount of data.One could say that these texts are helpful to learnthe general grammar of the language.
The word listincludes 65301 words and the OOV rate is 0.95% ona development set of 158k words.3.2 Training on in-domain data onlyFollowing the above discussion, it seems natural tofirst train a neural network LM on the transcrip-tions of the acoustic data only.
The architectureof the neural network is as follows: a continuousword representation of dimension 50, one hiddenlayer with 500 neurons and an output layer limitedto the 8192 most frequent words.
This results in3.2M parameters for the continuous representationof the words and about 4.2M parameters for the sec-ond part of the neural network that estimates theprobabilities.
The network is trained using standardstochastic back-propagation.3 The learning rate wasset to 0.005 with an exponential decay and the regu-larization term is weighted with 0.00003.
Note thatfast training of neural networks with more than 4Mparameters on 4M examples is already a challenge.The same fast algorithms as described in (Schwenk,2004) were used.
Apparent convergence is obtainedafter about 40 epochs though the training data, eachone taking 2h40 on standard PC equipped with twoIntel Xeon 2.8GHz CPUs.The neural network LM alone achieves a perplex-ity of 103.0 which is only a 4% relative reductionwith respect to the back-off LM (107.4, see Table 1).If this neural network LM is interpolated with theback-off LM trained on the whole training set theperplexity decreases from 70.2 to 67.6.
Despite thissmall improvements in perplexity a notable word er-ror reduction was obtained from 14.24% to 14.02%,with the lattice rescoring taking less than 0.05xRT.In the following sections, it is shown that larger im-provements can be obtained by training the neuralnetwork on more data.3.3 Adding selected dataTraining the neural network LM with stochasticback-propagation on all the available text corpora3The weights are updated after each example.would take quite a long time.
The estimated timefor one training epoch with the 88M words of com-mercial transcriptions is 58h, and more than 12 daysif all the 508M words of newspaper texts were used.This is of course not very practicable.
One solutionto this problem is to select a subset of the data thatseems to be most useful for the task.
This was doneby selecting six month of the commercial transcrip-tions that minimize the perplexity on the develop-ment set.
This gives a total of 22M words and thetraining time is about 14h per epoch.One can ask if the capacity of the neural networkshould be augmented in order to deal with the in-creased number of examples.
Experiments with hid-den layer sizes from 400 to 1000 neurons have beenperformed (see Table 2).size 400 500 600 1000?Tr.
time 11h20 13h50 16h15 11+16hPx alone 100.5 100.1 99.5 94.5interpol.
68.3 68.3 68.2 68.0Werr 13.99% 13.97% 13.96% 13.92%?
Interpolation of networks with 400 and 600hidden units.Table 2: Performance for a neural network LM andtraining time per epoch as a function of the size ofthe hidden layer (fixed 6 months subset of commer-cial transcripts).Although there is a small decrease in perplexityand word error when increasing the dimension of thehidden layer, this is at the expense of a higher pro-cessing time.
The training and recognition time arein fact almost linear to the size of the hidden layer.An alternative approach to augment the capacity ofthe neural network is to modify the dimension of thecontinuous representation of the words (in the range50 to 150).
The idea behind this is that the proba-bility estimation may be easier in a higher dimen-sional space (instead of augmenting the capacity ofthe non-linear probability estimator itself).
This issimilar in spirit to the theory behind support vectormachines (Vapnik, 1998).Increasing the dimension of the projection layerhas several advantages as can be seen from the Fig-ure 2.
First, the perplexity and word error ratesare lower than those obtained when the size of the20590951001051101151200  10  20  30  40  50PerplexityEpochsdim 50dim 60dim 70dim 100dim 120dim 150Figure 2: Perplexity in function of the size of thecontinuous word representation (500 hidden units,fixed 6 months subset of commercial transcripts).hidden layer is increased.
Second, convergence isfaster: the best result is obtained after about 15epochs while up to 40 are needed with large hiddenlayers.
Finally, increasing the size of the continu-ous word representation has only a small effect onthe training and recognition complexity of the neu-ral network4 since most of the calculation is doneto propagate and learn the connections between thehidden and the output layer (see equation 6).
Thebest result was obtained with a 120 dimensionalcontinuous word representation.
The perplexity is67.9 after interpolation with the back-off LM andthe word error rate is 13.88%.3.4 Training on all available dataIn this section an algorithm is proposed for trainingthe neural network on arbitrary large training cor-pora.
The basic idea is quite simple: instead ofperforming several epochs over the whole trainingdata, a different small random subset is used at eachepoch.
This procedure has several advantages:?
There is no limit on the amount of training data,?
After some epochs, it is likely that all the train-ing examples have been seen at least once,?
Changing the examples after each epoch addsnoise to the training procedure.
This potentiallyincreases the generalization performance.This algorithm is summarized in figure 4.
Theparameters of this algorithm are the size of the ran-dom subsets that are used at each epoch.
We chose414h20 for P=120 and H=500.808590951001051101151200  5  10  15  20  25  30  35  40  45  50PerplexityEpochs6 month fix1% resampled5% resampled10% resampled20% resampledFigure 3: Perplexity when resampling different ran-dom subsets of the commercial transcriptions.
(wordrepresentation of dimension 120, 500 hidden units)to always use the full corpus of transcriptions of theacoustic data since this is the most appropriate datafor the task.
Experiments with different random sub-sets of the commercial transcriptions and the news-paper texts have been performed (see Figure 3 and5).
In all cases the same neural network architecturewas used, i.e a 120 dimensional continuous wordrepresentation and 500 hidden units.
Some experi-ments with larger hidden units showed basically thesame convergence behavior.
The learning rate wasagain set to 0.005, but with a slower exponential de-cay.First of all it can be seen from Figure 3 that theresults are better when using random subsets insteadof a fixed selection of 6 months, although each ran-dom subset is actually smaller (for instance a total of12.5M examples for a subset of 10%).
Best resultswere obtained when taking 10% of the commercial+ Train network for one epochRepeatSelect training data:?
Use all acoustic transcriptions (4M words)?
Extract random subset of examples  from the large corpora?
Shuffle data(performing weight updates after each example)+ Test performance on development dataUntil convergenceFigure 4: Training algorithm for large corpora206Back-off LM Neural Network LMTraining data [#words] 600M 4M 22M 92.5M?
600M?Training time [h/epoch] - 2h40 14h 9h40 12h 3 ?
12hPerplexity (NN LM alone) - 103.0 97.5 84.0 80.0 76.5Perplexity (interpolated LMs) 70.2 67.6 67.9 66.7 66.5 65.9Word error rate (interpolated LMs) 14.24% 14.02% 13.88% 13.81% 13.75% 13.61%?
By resampling different random parts at the beginning of each epoch.Table 3: Comparison of the back-off and the neural network LM using different amounts of training data.The perplexities are given for the neural network LM alone and interpolated with the back-off LM trainedon all the data.
The last column corresponds to three interpolated neural network LMs.transcriptions.
The perplexity is 66.7 after interpo-lation with the back-off LM and the word error rateis 13.81% (see summary in Table 3).
Larger sub-sets of the commercial transcriptions lead to slowertraining, but don?t give better results.Encouraged by these results, we also included the508M words of newspaper texts in the training data.The size of the random subsets were chosen in orderto use between 4 and 9M words of each corpus.
Fig-ure 5 summarizes the results.
There seems to be noobvious benefit from resampling large subsets of theindividual corpora.
We choose to resample 10% ofthe commercial transcriptions and 1% of the news-paper texts.808590951001051100  5  10  15  20  25  30  35  40  45  50PerplexityEpochs6 month transcription fix10% transcriptions5% transcr + 1% journal5% transcr + 2% journal10% transcr + 1% journal10% transcr + 2% journalFigure 5: Perplexity when resampling different ran-dom subsets of the commercial transcriptions andthe newspaper texts.Table 3 summarizes the results of the differentneural network LMs.
It can be clearly seen that theperplexity of the neural network LM alone decreasessignificantly with the amount of training data used.The perplexity after interpolation with the back-offLM changes only by a small amount, but there is anotable improvement in word error rate.
This is an-other experimental evidence that the perplexity of aLM is not directly related to the word error rate.The best neural network LM achieves a word er-ror reduction of 0.5% absolute with respect to thecarefully tuned back-off LM (14.24% ?
13.75%).The additional processing time needed to rescore thelattices is less than 0.05xRT.
This is a significant im-provement, in particular for a fast real-time continu-ous speech recognition system.
When more process-ing time is available a word error rate of 13.61% canbe achieved by interpolating three neural networkstogether (in 0.14xRT).3.5 Using a better speech recognizerThe experimental results have also been validatedusing a second speech recognizer running in about7xRT.
This systems differs from the real-time recog-nizer by a larger 200k word-list, additional acousticmodel adaptation passes and less pruning.
Detailsare described in (Gauvain et al, 2005).
The word er-ror rate of the reference system using a back-off LMis 10.74%.
This can be reduced to 10.51% using aneural network LM trained on the fine transcriptionsonly and to 10.20% when the neural network LMis trained on all data using the described resamplingapproach.
Lattice rescoring takes about 0.2xRT.4 Conclusions and future workNeural network language models are becoming aserious alternative to the widely used back-off lan-guage models.
Consistent improvements in perplex-ity and word error rate have been reported (Bengioet al, 2003; Schwenk and Gauvain, 2004; Schwenkand Gauvain, 2005; Emami and Jelinek, 2004).
Inthese works, the amount of training data was how-207ever limited to a maximum of 20M words due to thehigh complexity of the training algorithm.In this paper new techniques have been describedto train neural network language models on largeamounts of text corpora (up to 600M words).
Theevaluation with a state-of-the-art speech recognitionsystem for French Broadcast News showed a signif-icant word error reduction of 0.5% absolute.
Theneural network LMs is incorporated into the speechrecognizer by rescoring lattices.
This is done in lessthan 0.05xRT.Several extensions of the learning algorithm it-self are promising.
We are in particular interestedin smarter ways to select different subsets from thelarge corpus at each epoch (instead of a randomchoice).
One possibility would be to use activelearning, i.e.
focusing on examples that are mostuseful to decrease the perplexity.
One could alsoimagine to associate a probability to each trainingexample and to use these probabilities to weight therandom sampling.
These probabilities would be up-dated after each epoch.
This is similar to boostingtechniques (Freund, 1995) which build sequentiallyclassifiers that focus on examples wrongly classifiedby the preceding one.5 AcknowledgmentThe authors would like to thank Yoshua Bengio forfruitful discussions and helpful comments.
The au-thors would like to recognize the contributions ofG.
Adda, M. Adda and L. Lamel for their involve-ment in the development of the speech recognitionsystems on top of which this work is based.ReferencesYoshua Bengio and Rejean Ducharme.
2001.
A neuralprobabilistic language model.
In NIPS, volume 13.Yoshua Bengio, Rejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Research,3(2):1137?1155.Jeff Bilmes, Krste Asanovic, Chee whye Chin, and JimDemmel.
1997.
Using phipac to speed error back-propagation learning.
In ICASSP, pages V:4153?4156.Peter F. Brown, Vincent J. Della Pietra, Peter V. deSouza,Jenifer C. Lai, and Robert L. Mercer.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18(4):467?470.Ciprian Chelba and Frederick Jelinek.
2000.
Structuredlanguage modeling.
Computer Speech & Language,13(4):283?332.Stanley F. Chen and Joshua T. Goodman.
1999.
Anempirical study of smoothing techniques for languagemodeling.
Computer Speech & Language, 13(4):359?394.Ahmad Emami and Frederick Jelinek.
2004.
Exact train-ing of a neural syntactic language model.
In ICASSP,pages I:245?248.Ahmad Emami and Frederick Jelinek.
2005.
Randomclusterings for language modeling.
In ICASSP, pagesI:581?584.Yoav Freund.
1995.
Boosting a weak learning al-gorithm by majority.
Information and Computation,121(2):256?285.Jean-Luc Gauvain, Gilles Adda, Martine Adda-Decker,Alexandre Allauzen, Veronique Gendner, Lori Lamel,and Holger Schwenk.
2005.
Where are we in tran-scribing BN french?
In Eurospeech.Slava M. Katz.
1987.
Estimation of probabilities fromsparse data for the language model component ofa speech recognizer.
IEEE Transactions on ASSP,35(3):400?401.Ronald Rosenfeld.
1996.
A maximum entropy approachto adaptive statistical language modeling.
ComputerSpeech & Language, 10(3):187?228.Holger Schwenk and Jean-Luc Gauvain.
2002.
Connec-tionist language modeling for large vocabulary contin-uous speech recognition.
In ICASSP, pages I: 765?768.Holger Schwenk and Jean-Luc Gauvain.
2004.
Neu-ral network language models for conversational speechrecognition.
In ICSLP, pages 1215?1218.Holger Schwenk and Jean-Luc Gauvain.
2005.
Build-ing continuous space language models for transcribingeuropean languages.
In Eurospeech.Holger Schwenk.
2004.
Efficient training of large neu-ral networks for language modeling.
In IJCNN, pages3059?3062.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In ICSLP, pages II: 901?904.Vladimir Vapnik.
1998.
Statistical Learning Theory.Wiley, New York.Peng Xu and Frederick Jelinek.
2004.
Random forest inlanguage modeling.
In EMNLP, pages 325?332.208
