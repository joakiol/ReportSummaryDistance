Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 249?252,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPIntroduction of a new paraphrase generation toolbased on Monte-Carlo samplingJonathan Chevelu1,2Thomas Lavergne Yves Lepage1Thierry Moudenc2(1) GREYC, universit?
de Caen Basse-Normandie(2) Orange Labs; 2, avenue Pierre Marzin, 22307 Lannion{jonathan.chevelu,thierry.moudenc}@orange-ftgroup.com,thomas.lavergne@reveurs.org, yves.lepage@info.unicaen.frAbstractWe propose a new specifically designedmethod for paraphrase generation basedon Monte-Carlo sampling and show howthis algorithm is suitable for its task.Moreover, the basic algorithm presentedhere leaves a lot of opportunities for fu-ture improvement.
In particular, our algo-rithm does not constraint the scoring func-tion in opposite to Viterbi based decoders.It is now possible to use some global fea-tures in paraphrase scoring functions.
Thisalgorithm opens new outlooks for para-phrase generation and other natural lan-guage processing applications like statis-tical machine translation.1 IntroductionA paraphrase generation system is a programwhich, given a source sentence, produces a differ-ent sentence with almost the same meaning.Paraphrase generation is useful in applicationsto choose between different forms to keep themost appropriate one.
For instance, automaticsummary can be seen as a particular paraphrasingtask (Barzilay and Lee, 2003) with the aim of se-lecting the shortest paraphrase.Paraphrases can also be used to improve natu-ral language processing (NLP) systems.
(Callison-Burch et al, 2006) improved machine translationsby augmenting the coverage of patterns that canbe translated.
Similarly, (Sekine, 2005) improvedinformation retrieval based on pattern recognitionby introducing paraphrase generation.In order to produce paraphrases, a promisingapproach is to see the paraphrase generation prob-lem as a translation problem, where the target lan-guage is the same as the source language (Quirk etal., 2004; Bannard and Callison-Burch, 2005).A problem that has drawn less attention is thegeneration step which corresponds to the decodingstep in SMT.
Most paraphrase generation tools usesome standard SMT decoding algorithms (Quirk etal., 2004) or some off-the-shelf decoding tools likeMOSES (Koehn et al, 2007).
The goal of a de-coder is to find the best path in the lattice producedfrom a paraphrase table.
This is basically achievedby using dynamic programming and especially theViterbi algorithm associated with beam searching.However decoding algorithms were designedfor translation, not for paraphrase generation.
Al-though left-to-right decoding is justified for trans-lation, it may not be necessary for paraphrasegeneration.
A paraphrase generation tool usuallystarts with a sentence which may be very similar tosome potential solution.
In other words, there is noneed to "translate" all of the sentences.
Moreover,decoding may not be suitable for non-contiguoustransformation rules.In addition, dynamic programming imposes anincremental scoring function to evaluate the qual-ity of each hypothesis.
For instance, it cannot cap-ture some scattered syntactical dependencies.
Im-proving on this major issue is a key point to im-prove paraphrase generation systems.This paper first presents an alternative to decod-ing that is based on transformation rule applicationin section 2.
In section 3 we propose a paraphrasegeneration method for this paradigm based on analgorithm used in two-player games.
Section 4briefly explain experimental context and its asso-ciated protocol for evaluation of the proposed sys-tem.
We compare the proposed algorithm with abaseline system in section 5.
Finally, in section 6,we point to future research tracks to improve para-phrase generation tools.2 Statistical paraphrase generation usingtransformation rulesThe paraphrase generation problem can be seen asan exploration problem.
We seek the best para-phrase according to a scoring function in a space249to search by applying successive transformations.This space is composed of states connected by ac-tions.
An action is a transformation rule with aplace where it applies in the sentence.
States are asentence with a set of possible actions.
Applyingan action in a given state consists in transformingthe sentence of the state and removing all rules thatare no more applicable.
In our framework, eachstate, except the root, can be a final state.
Thisis modelised by adding a stop rule as a particularaction.
We impose the constraint that any trans-formed part of the source sentence cannot be trans-formed anymore.This paradigm is more approriate for paraphrasegeneration than the standard SMT approach in re-spect to several points: there is no need for left-to-right decoding because a transformation can beapplied anywhere without order; there is no needto transform the whole of a sentence because eachstate is a final state; there is no need to keep theidentity transformation for each phrase in the para-phrase table; the only domain knowledge neededis a generative model and a scoring function forfinal states; it is possible to mix different genera-tive models because a statistical paraphrase table,an analogical solver and a paraphrase memory forinstance; there is no constraint on the scoring func-tion because it only scores final states.Note that the branching factor with a paraphrasetable can be around thousand actions per stateswhich makes the generation problem a difficultcomputational problem.
Hence we need an effi-cient generation algorithm.3 Monte-Carlo based ParaphraseGenerationUCT (Kocsis and Szepesv?ri, 2006) (Upper Con-fidence bound applied to Tree) is a Monte-Carloplanning algorithm that have some interestingproperties: it grows the search tree non-uniformlyand favours the most promising sequences, with-out pruning branch; it can deal with high branch-ing factor; it is an any-time algorithm and returnsbest solution found so far when interrupted; it doesnot require expert domain knowledge to evaluatestates.
These properties make it ideally suited forgames with high branching factor and for whichthere is no strong evaluation function.For the same reasons, this algorithm sounds in-teresting for paraphrase generation.
In particular,it does not put constraint on the scoring function.We propose a variation of the UCT algorithm forparaphrase generation named MCPG for Monte-Carlo based Paraphrase Generation.The main part of the algorithm is the samplingstep.
An episode of this step is a sequence of statesand actions, s1, a1, s2, a2, .
.
.
, sT, from the rootstate to a final state.
During an episode construc-tion, there are two ways to select the action aitoperfom from a state si.If the current state was already explored in aprevious episode, the action is selected accord-ing to a compromise between exploration and ex-ploitation.
This compromise is computed usingthe UCB-Tunned formula (Auer et al, 2001) as-sociated with the RAVE heuristic (Gelly and Sil-ver, 2007).
If the current state is explored forthe first time, its score is estimated using Monte-Carlo sampling.
In other word, to complete theepisode, the actions ai, ai+1, .
.
.
, aT?1, aTare se-lected randomly until a stop rule is drawn.At the end of each episode, a reward is com-puted for the final state sTusing a scoring func-tion and the value of each (state, action) pair of theepisode is updated.
Then, the algorithm computesan other episode with the new values.Periodically, the sampling step is stopped andthe best action at the root state is selected.
Thisaction is then definitely applied and a samplingis restarted from the new root state.
The actionsequence is built incrementally and selected af-ter being enough sampled.
For our experiments,we have chosen to stop sampling regularly after afixed amount ?
of episodes.Our main adaptation of the original algorithmis in the (state, action) value updating procedure.Since the goal of the algorithm is to maximise ascoring function, we use the maximum reachablescore from a state as value instead of the score ex-pectation.
This algorithm suits the paradigm pro-posed for paraphrase generation.4 Experimental contextThis section describes the experimental contextand the methodology followed to evaluate our sta-tistical paraphrase generation tool.4.1 DataFor the experiment reported in section 5, we useone of the largest, multi-lingual, freely availablealigned corpus, Europarl (Koehn, 2005).
It con-sists of European parliament debates.
We choose250French as the language for paraphrases and En-glish as the pivot language.
For this pair of lan-guages, the corpus consists of 1, 487, 459 Frenchsentences aligned with 1, 461, 429 English sen-tences.
Note that the sentences in this corpusare long, with an average length of 30 words perFrench sentence and 27.1 for English.
We ran-domly extracted 100 French sentences as a testcorpus.4.2 Language model and paraphrase tableParaphrase generation tools based on SMT meth-ods need a language model and a paraphrase table.Both are computed on a training corpus.The language models we use are n-gram lan-guage models with back-off.
We use SRILM (Stol-cke, 2002) with its default parameters for this pur-pose.
The length of the n-grams is five.To build a paraphrase table, we use the con-struction method via a pivot language proposedin (Bannard and Callison-Burch, 2005).Three heuristics are used to prune the para-phrase table.
The first heuristic prunes any entryin the paraphrase table composed of tokens with aprobability lower than a threshold .
The second,called pruning pivot heuristic, consists in deletingall pivot clusters larger than a threshold ?
.
Thelast heuristic keeps only the ?
most probable para-phrases for each source phrase in the final para-phrase table.
For this study, we empirically fix = 10?5, ?
= 200 and ?
= 10.4.3 Evaluation ProtocolWe developed a dedicated website to allow the hu-man judges with some flexibility in workplacesand evaluation periods.
We retain the principle ofthe two-step evaluation, common in the machinetranslation domain and already used for para-phrase evaluation (Bannard and Callison-Burch,2005).The question asked to the human evaluator forthe syntactic task is: Is the following sentence ingood French?
The question asked to the humanevaluator for the semantic task is: Do the followingtwo sentences express the same thing?In our experiments, each paraphrase was evalu-ated by two native French evaluators.5 Comparison with a SMT decoderIn order to validate our algorithm for paraphrasegeneration, we compare it with an off-the-shelfSMT decoder.We use the MOSES decoder (Koehn et al, 2007)as a baseline.
The MOSES scoring function isset by four weighting factors ?
?, ?LM, ?D, ?W.Conventionally, these four weights are adjustedduring a tuning step on a training corpus.
Thetuning step is inappropriate for paraphrase becausethere is no such tuning corpus available.
We em-pirically set ?
?= 1, ?LM= 1, ?D= 10 and?W= 0.
Hence, the scoring function (or rewardfunction for MCPG) is equivalent to:R(f?|f, I) = p(f?)?
?
(f |f?, I)where f and f?are the source and target sen-tences, I a segmentation in phrases of f , p(f?
)the language model score and ?
(f |f?, I) =?i?Ip(fi|f?i) the paraphrase table score.The MCPG algorithm needs two parameters.One is the number of episodes ?
done before se-lecting the best action at root state.
The other isk, an equivalence parameter which balances theexploration/exploitation compromise (Auer et al,2001).
We empirically set ?
= 1, 000, 000 andk = 1, 000.For our algorithm, note that identity paraphraseprobabilities are biased: for each phrase it isequal to the probability of the most probable para-phrase.
Moreover, as the source sentence is thebest meaning preserved "paraphrase", a sentencecannot have a better score.
Hence, we use aslightly different scoring function:R(f?|f, I) = min????p(f?
)p(f)?i?Ifi6=f?ip(fi|f?i)p(fi|fi), 1???
?Note that for this model, there is no need to knowthe identity transformations probability for un-changed part of the sentence.Results are presented in Table 1.
The Kappastatistics associated with the results are 0.84, 0.64and 0.59 which are usually considered as a "per-fect", "substantial" and "moderate" agreement.Results are close to evaluations from the base-line system.
The main differences are from Kappastatistics which are lower for the MOSES systemevaluation.
Judges changed between the two ex-periments.
We may wonder whether an evaluationwith only two judges is reliable.
This points to theambiguity of any paraphrase definition.251System MOSES MCPGWell formed (Kappa) 64%(0.57) 63%(0.84)Meaning preserved (Kappa) 58%(0.48) 55%(0.64)Well formed and meaning preserved (Kappa) 50%(0.54) 49%(0.59)Table 1: Results of paraphrases evaluation for 100 sentences in French using English as the pivot lan-guage.
Comparison between the baseline system MOSES and our algorithm MCPG.By doing this experiment, we have shown thatour algorithm with a biased paraphrase table isstate-of-the-art to generate paraphrases.6 Conclusions and further researchIn this paper, we have proposed a differentparadigm and a new algorithm in NLP fieldadapted for statistical paraphrases generation.This method, based on large graph exploration byMonte-Carlo sampling, produces results compa-rable with state-of-the-art paraphrase generationtools based on SMT decoders.The algorithm structure is flexible and genericenough to easily work with discontinous patterns.It is also possible to mix various transformationmethods to increase paraphrase variability.The rate of ill-formed paraphrase is high at37%.
The result analysis suggests an involvementof the non-preservation of the original meaningwhen a paraphrase is evaluated ill-formed.
Al-though the mesure is not statistically significantbecause the test corpus is too small, the same trendis also observed in other experiments.
Improv-ing on the language model issue is a key point toimprove paraphrase generation systems.
Our al-gorithm can work with unconstraint scoring func-tions, in particular, there is no need for the scor-ing function to be incremental as for Viterbi baseddecoders.
We are working to add, in the scoringfunction, a linguistic knowledge based analyzer tosolve this problem.Because MCPG is based on a different paradigm,its output scores cannot be directly compared toMOSES scores.
In order to prove the optimisa-tion qualities of MCPG versus state-of-the-art de-coders, we are transforming our paraphrase gener-ation tool into a translation tool.ReferencesP.
Auer, N. Cesa-Bianchi, and C. Gentile.
2001.
Adap-tive and self-confident on-line learning algorithms.Machine Learning.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In AnnualMeeting of ACL, pages 597?604, Morristown, NJ,USA.
Association for Computational Linguistics.Regina Barzilay and Lillian Lee.
2003.
Learn-ing to paraphrase: An unsupervised approach us-ing multiple-sequence alignment.
In HLT-NAACL2003: Main Proceedings, pages 16?23.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine transla-tion using paraphrases.
In HLT-NAACL 2006: MainProceedings, pages 17?24, Morristown, NJ, USA.Association for Computational Linguistics.Sylvain Gelly and David Silver.
2007.
Combining on-line and offline knowledge in UCT.
In 24th Interna-tional Conference on Machine Learning (ICML?07),pages 273?280, June.Levente Kocsis and Csaba Szepesv?ri.
2006.
Banditbased monte-carlo planning.
In 17th European Con-ference on Machine Learning, (ECML?06), pages282?293, September.Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,Christopher Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-jar, Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Annual Meeting of ACL, Demonstra-tion Session, pages 177?180, June.Philipp Koehn.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedingsof MT Summit.Chris Quirk, Chris Brockett, and Bill Dolan.
2004.Monolingual machine translation for paraphrasegeneration.
In Dekang Lin and Dekai Wu, edi-tors, the 2004 Conference on Empirical Methodsin Natural Language Processing, pages 142?149.,Barcelona, Spain, 25-26 July.
Association for Com-putational Linguistics.Satoshi Sekine.
2005.
Automatic paraphrase discov-ery based on context and keywords between ne pairs.In Proceedings of International Workshop on Para-phrase (IWP2005).Andreas Stolcke.
2002.
Srilm ?
an extensible languagemodeling toolkit.
In Proceedings of InternationalConference on Spoken Language Processing.252
