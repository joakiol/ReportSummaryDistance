Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1723?1732,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsDeceptive Answer Prediction with User Preference GraphFangtao Li?, Yang Gao?, Shuchang Zhou?
?, Xiance Si?, and Decheng Dai?
?Google Research, Mountain View?State Key Laboratory of Computer Architecture, Institute of Computing Technology, CAS{lifangtao,georgezhou,sxc,decheng}@google.com?Department of Computer Science and Technology, Tsinghua Universitygao young@163.comAbstractIn Community question answering (QA)sites, malicious users may provide decep-tive answers to promote their products orservices.
It is important to identify and fil-ter out these deceptive answers.
In thispaper, we first solve this problem withthe traditional supervised learning meth-ods.
Two kinds of features, including tex-tual and contextual features, are investi-gated for this task.
We further proposeto exploit the user relationships to identifythe deceptive answers, based on the hy-pothesis that similar users will have simi-lar behaviors to post deceptive or authenticanswers.
To measure the user similarity,we propose a new user preference graphbased on the answer preference expressedby users, such as ?helpful?
voting and?best answer?
selection.
The user prefer-ence graph is incorporated into traditionalsupervised learning framework with thegraph regularization technique.
The ex-periment results demonstrate that the userpreference graph can indeed help improvethe performance of deceptive answer pre-diction.1 IntroductionCurrently, Community QA sites, such as Yahoo!Answers1 and WikiAnswers2, have become one ofthe most important information acquisition meth-ods.
In addition to the general-purpose web searchengines, the Community QA sites have emerged aspopular, and often effective, means of informationseeking on the web.
By posting questions for otherparticipants to answer, users can obtain answersto their specific questions.
The Community QA1http://answers.yahoo.com2http://wiki.answers.comsites are growing rapidly in popularity.
Currentlythere are hundreds of millions of answers and mil-lions of questions accumulated on the CommunityQA sites.
These resources of past questions andanswers are proving to be a valuable knowledgebase.
From the Community QA sites, users can di-rectly get the answers to meet some specific infor-mation need, rather than browse the list of returneddocuments to find the answers.
Hence, in recentyears, knowledge mining in Community QA siteshas become a popular topic in the field of artifi-cial intelligence (Adamic et al, 2008; Wei et al,2011).However, some answers may be deceptive.
Inthe Community QA sites, there are millions ofusers each day.
As the answers can guide theuser?s behavior, some malicious users are moti-vated to give deceptive answers to promote theirproducts or services.
For example, if someoneasks for recommendations about restaurants in theCommunity QA site, the malicious user may post adeceptive answer to promote the target restaurant.Indeed, because of lucrative financial rewards, inseveral Community QA sites, some business own-ers provide incentives for users to post deceptiveanswers for product promotion.There are at least two major problems that thedeceptive answers cause.
On the user side, thedeceptive answers are misleading to users.
Ifthe users rely on the deceptive answers, they willmake the wrong decisions.
Or even worse, the pro-moted link may lead to illegitimate products.
Onthe Community QA side, the deceptive answerswill hurt the health of the Community QA sites.
ACommunity QA site without control of deceptiveanswers could only benefit spammers but couldnot help askers at all.
If the asker was cheated bythe provided answers, he will not trust and visitthis site again.
Therefore, it is a fundamental taskto predict and filter out the deceptive answers.In this paper, we propose to predict deceptive1723answer, which is defined as the answer, whose pur-pose is not only to answer the question, but alsoto promote the authors?
self-interest.
In the firststep, we consider the deceptive answer predictionas a general binary-classification task.
We extracttwo types of features: one is textual features fromanswer content, including unigram/bigram, URL,phone number, email, and answer length; the otheris contextual features from the answer context, in-cluding the relevance between answer and the cor-responding question, the author of the answer, an-swer evaluation from other users and duplicationwith other answers.
We further investigate the userrelationship for deceptive answer prediction.
Weassume that similar users tend to have similar be-haviors, i.e.
posting deceptive answers or post-ing authentic answers.
To measure the user rela-tionship, we propose a new user preference graph,which is constructed based on the answer evalu-ation expressed by users, such as ?helpful?
vot-ing and ?best answer?
selection.
The user prefer-ence graph is incorporated into traditional super-vised learning framework with graph regulariza-tion, which can make answers, from users withsame preference, tend to have the same category(deceptive or authentic).
The experiment resultsdemonstrate that the user preference graph can fur-ther help improve the performance for deceptiveanswer prediction.2 Related WorkIn the past few years, it has become a popular taskto mine knowledge from the Community QA sites.Various studies, including retrieving the accumu-lated question-answer pairs to find the related an-swer for a new question, finding the expert in aspecific domain, summarizing single or multipleanswers to provide a concise result, are conductedin the Community QA sites (Jeon et al, 2005;Adamic et al, 2008; Liu et al, 2008; Song etal., 2008; Si et al, 2010a; Figueroa and Atkin-son, 2011).
However, an important issue whichhas been neglected so far is the detection of decep-tive answers.
If the acquired question-answer cor-pus contains many deceptive answers, it would bemeaningless to perform further knowledge miningtasks.
Therefore, as the first step, we need to pre-dict and filter out the deceptive answers.
Amongprevious work, answer quality prediction (Song etal., 2010; Harper et al, 2008; Shah and Pomer-antz, 2010; Ishikawa et al, 2010) is most related tothe deceptive answer prediction task.
But these arestill significant differences between two tasks.
An-swer quality prediction measures the overall qual-ity of the answers, which refers to the accuracy,readability, completeness of the answer.
Whilethe deceptive answer prediction aims to predict ifthe main purpose of the provided answer is onlyto answer the specific question, or includes theuser?s self-interest to promote something.
Someof the previous work (Song et al, 2010; Ishikawaet al, 2010; Bian et al, 2009) views the ?bestanswer?
as high quality answers, which are se-lected by the askers in the Community QA sites.However, the deceptive answer may be selected ashigh-quality answer by the spammer, or becausethe general users are mislead.
Meanwhile, someanswers from non-native speakers may have lin-guistic errors, which are low-quality answers, butare still authentic answers.
Our experiments alsoshow that answer quality prediction is much dif-ferent from deceptive answer prediction.Previous QA studies also analyze the user graphto investigate the user relationship (Jurczyk andAgichtein, 2007; Liu et al, 2011).
They mainlyconstruct the user graph with asker-answerer rela-tionship to estimate the expertise score in Commu-nity QA sites.
They assume the answerer is moreknowledgeable than the asker.
However, we don?tcare which user is more knowledgeable, but aremore likely to know if two users are both spam-mers or authentic users.
In this paper, we pro-pose a novel user preference graph based on theirpreference towards the target answers.
We assumethat the spammers may collaboratively promotethe target deceptive answers, while the authen-tic users may generally promote the authentic an-swers and demote the deceptive answers.
The userpreference graph is constructed based on their an-swer evaluation, such as ?helpful?
voting or ?bestanswer?
selection.3 Proposed FeaturesWe first view the deceptive answer prediction as abinary-classification problem.
Two kinds of fea-tures, including textual features and contextualfeatures, are described as follows:3.1 Textual FeaturesWe first aim to predict the deceptive answer by an-alyzing the answer content.
Several textual fea-tures are extracted from the answer content:3.1.1 Unigrams and BigramsThe most common type of feature for text classi-fication is the bag-of-word.
We use an effective1724feature selection method ?2 (Yang and Pedersen,1997) to select the top 200 unigrams and bigramsas features.
The top ten unigrams related to decep-tive answers are shown on Table 1.
We can see thatthese words are related to the intent for promotion.professional service advice addresssite telephone therapy recommendhospital expertTable 1: Top 10 Deceptive Related Unigrams3.1.2 URL FeaturesSome malicious users may promote their productsby linking a URL.
We find that URL is good indi-cator for deceptive answers.
However, some URLsmay provide the references for the authentic an-swers.
For example, if you ask the weather inmountain view, someone may just post the linkto ?http://www.weather.com/?.
Therefore, besidesthe existence of URL, we also use the followingURL features:1).
Length of the URLs: we observe that thelonger urls are more likely to be spam.2).
PageRank Score: We employ the PageRank(Page et al, 1999) score of each URL as popularityscore.3.1.3 Phone Numbers and EmailsThere are a lot of contact information mentionedin the Community QA sites, such as phone num-bers and email addresses, which are very likely tobe deceptive, as good answers are found to be lesslikely to refer to phone numbers or email addressesthan the malicious ones.
We extract the number ofoccurrences of email and phone numbers as fea-tures.3.1.4 LengthWe have also observed some interesting patternsabout the length of answer.
Deceptive ones tendto be longer than authentic ones.
This can be ex-plained as the deceptive answers may be well pre-pared to promote the target.
We also employ thenumber of words and sentences in the answer asfeatures.3.2 Contextual FeaturesBesides the answer textual features, we further in-vestigate various features from the context of thetarget answer:3.2.1 Question Answer RelevanceThe main characteristic of answer in CommunityQA site is that the answer is provided to answerthe corresponding question.
We can use the corre-sponding question as one of the context features bymeasuring the relevance between the answer andthe question.
We employ three different modelsfor Question-Answer relevance:Vector Space ModelEach answer or question is viewed as a wordvector.
Given a question q and the answer a, ourvector model uses weighted word counts(e.g.TF-IDF) as well as the cosine similarity (q ?
a) oftheir word vectors as relevant function (Salton andMcGill, 1986).
However, vector model only con-sider the exact word match, which is a big prob-lem, especially when the question and answer aregenerally short compared to the document.
For ex-ample, Barack Obama and the president of the USare the same person.
But the vector model wouldindicate them to be different.
To remedy the word-mismatch problem, we also look for the relevancemodels in higher semantic levels.Translation ModelA translation model is a mathematical model inwhich the language translation is modeled in a sta-tistical way.
The probability of translating a sourcesentence (as answer here) into target sentence (asquestion here) is obtained by aligning the wordsto maximize the product of all the word probabil-ities.
We train a translation model (Brown et al,1990; Och and Ney, 2003) using the CommunityQA data, with the question as the target language,and the corresponding best answer as the sourcelanguage.
With translation model, we can com-pute the translation score for new question and an-swer.Topic ModelTo reduce the false negatives of word mismatchin vector model, we also use the topic models toextend matching to semantic topic level.
The topicmodel, such as Latent Dirichlet Allocation (LDA)(Blei et al, 2003), considers a collection of doc-uments with K latent topics, where K is muchsmaller than the number of words.
In essence,LDA maps information from the word dimen-sion to a semantic topic dimension, to address theshortcomings of the vector model.3.2.2 User Profile FeaturesWe extract several user?s activity statistics to con-struct the user profile features, including the level1725of the user in the Community QA site, the numberof questions asked by this user, the number of an-swers provided by this user, and the best answerratio of this user.3.2.3 User Authority ScoreMotivated by expert finding task (Jurczyk andAgichtein, 2007; Si et al, 2010a; Li et al, 2011),the second type of author related feature is author-ity score, which denotes the expertise score of thisuser.
To compute the authority score, we first con-struct a directed user graph with the user interac-tions in the community.
The nodes of the graphrepresent users.
An edge between two users in-dicates a contribution from one user to the other.Specifically, on a Q&A site, an edge from A toB is established when user B answered a questionasked by A, which shows user B is more likely tobe an expert than A.
The weight of an edge indi-cates the number of interactions.
We compute theuser?s authority score (AS) based on the link anal-ysis algorithm PageRank:AS(ui) =1?
dN + d?uj?M(ui)AS(uj)L(uj)(1)where u1, .
.
.
, uN are the users in the collection,N is the total number of users, M(ui) is the setof users whose answers are provided by user ui,L(ui) is the number of users who answer ui?squestions, d is a damping factor, which is set as0.85.
The authority score can be computed itera-tively with random initial values.3.2.4 Robot FeaturesThe third type of author related feature is used fordetecting whether the author is a robot, which arescripts crafted by malicious users to automaticallypost answers.
We observe that the distributions ofthe answer-posting time are very different betweengeneral user and robot.
For example, some robotsmay make posts continuously and mechanically,hence the time increment may be smaller that hu-man users who would need time to think and pro-cess between two posts.
Based on this observa-tion, we design an time sequence feature for robotdetection.
For each author, we can get a list oftime points to post answers, T = {t0, t1, ..., tn},where ti is the time point when posting the ith an-swer.
We first convert the time sequence T to timeinterval sequence ?T = {?t0,?t1, ...,?tn?1},where ?ti = ti+1 ?
ti.
Based on the intervalsequences for all users, we then construct a ma-trix Xm?b whose rows correspond to users andcolumns correspond to interval histogram withpredefined range.
We can use each row vector astime sequence pattern to detect robot.
To reducethe noise and sparse problem, we use the dimen-sion reduction techniques to extract the latent se-mantic features with Singular Value Decomposi-tion (SVD) (Deerwester et al, 1990; Kim et al,2006).3.2.5 Evaluation from Other UsersIn the Community QA sites, other users can ex-press their opinions or evaluations on the answer.For example, the asker can choose one of the an-swers as best answer.
We use a bool feature to de-note if this answer is selected as the best answer.In addition, other users can label each answer as?helpful?
or ?not helpful?.
We also use this helpfulevaluation by other users as the contextual feature,which is defined as the ratio between the numberof ?helpful?
votes and the number of total votes.3.2.6 Duplication with Other AnswersThe malicious user may post the pre-written prod-uct promotion documents to many answers, or justchange the product name.
We also compute thesimilarity between different answers.
If the twoanswers are totally same, but the question is differ-ent, these answer is potentially as a deceptive an-swer.
Here, we don?t want to measure the semanticsimilarity between two answers, but just measureif two answers are similar to the word level, there-fore, we apply BleuScore (Papineni et al, 2002),which is a standard metric in machine translationfor measuring the overlap between n-grams of twotext fragments r and c. The duplication score ofeach answer is the maximum BleuScore comparedto all other answers.4 Deceptive Answer Prediction with UserPreference GraphBesides the textual and contextual features, wealso investigate the user relationship for decep-tive answer prediction.
We assume that similarusers tend to perform similar behaviors (postingdeceptive answers or posting authentic answers).In this section, we first show how to compute theuser similarity (user preference graph construc-tion), and then introduce how to employ the userrelationship for deceptive answer prediction.4.1 User Preference Graph ConstructionIn this section, we propose a new user graph to de-scribe the relationship among users.
Figure 1 (a)shows the general process in a question answering1726QuestionAnswer1Answer2Best Answeru1u2u3u4u5u6(a) Question Answering (b) User Preference Relation (c) User Preference GraphFigure 1: User Preference Graph Constructionthread.
The asker, i.e.
u1, asks a question.
Then,there will be several answers to answer this ques-tion from other users, for example, answerers u2and u3.
After the answers are provides, users canalso vote each answer as ?helpful?
or ?not help-ful?
to show their evaluation towards the answer .For example, users u4, u5 vote the first answer as?not helpful?, and user u6 votes the second answeras ?helpful?.
Finally, the asker will select one an-swer as the best answer among all answers.
Forexample, the asker u1 selects the first answer asthe ?best answer?.To mine the relationship among users, previousstudies mainly focus on the asker-answerer rela-tionship (Jurczyk and Agichtein, 2007; Liu et al,2011).
They assume the answerer is more knowl-edgeable than the asker.
Based on this assump-tion, they can extract the expert in the commu-nity, as discussed in Section 3.2.3.
However, wedon?t care which user is more knowledgeable, butare more interested in whether two users are bothmalicious users or authentic users.
Here, we pro-pose a new user graph based on the user prefer-ence.
The preference is defined based on the an-swer evaluation.
If two users show same pref-erence towards the target answer, they will havethe user-preference relationship.
We mainly usetwo kinds of information: ?helpful?
evaluation and?best answer?
selection.
If two users give same?helpful?
or ?not helpful?
to the target answer, weview these two users have same user preference.For example, user u4 and user u5 both give ?nothelpful?
evaluation towards the first answer, wecan say that they have same user preference.
Be-sides the real ?helpful?
evaluation, we also assumethe author of the answer gives the ?helpful?
evalu-ation to his or her own answer.
Then if user u6 give?helpful?
evaluation to the second answer, we willview user u6 has same preference as user u3, whois the author of the second answer.
We also can ex-tract the user preference with ?best answer?
selec-tion.
If the asker selects the ?best answer?
amongall answers, we will view that the asker has samepreference as the author of the ?best answer?.
Forexample, we will view user u1 and user u2 havesame preference.Based on the two above assumptions, we canextract three user preference relationships (withsame preference) from the question answering ex-ample in Figure 1 (a): u4 ?
u5, u3 ?
u6, u1 ?
u2,as shown in Figure1 (b).
After extracting all userpreference relationships, we can construct the userpreference graph as shown in Figure 1 (c).
Eachnode represents a user.
If two users have the userpreference relationship, there will be an edge be-tween them.
The edge weight is the number ofuser preference relationships.In the Community QA sites, the spammersmainly promote their target products by promotingthe deceptive answers.
The spammers can collab-oratively make the deceptive answers look good,by voting them as high-quality answer, or select-ing them as ?best answer?.
However, the authen-tic users generally have their own judgements tothe good and bad answers.
Therefore, the evalu-ation towards the answer reflects the relationshipamong users.
Although there maybe noisy rela-tionship, for example, an authentic user may becheated, and selects the deceptive answer as ?bestanswer?, we hope the overall user preference rela-tion can perform better results than previous userinteraction graph for this task.17274.2 Incorporating User Preference GraphTo use the user graph, we can just compute thefeature value from the graph, and add it into thesupervised method as the features introduced inSection 3.
Here, we propose a new technique toemploy the user preference graph.
We utilize thegraph regularizer (Zhang et al, 2006; Lu et al,2010) to constrain the supervised parameter learn-ing.
We will introduce this technique based ona commonly used model f(?
), the linear weightmodel, where the function value is determined bylinear combination of the input features:f(xi) = wT ?
xi =?kwk ?
xik (2)where xi is a K dimension feature vector for theith answer, the parameter value wk captures theeffect of the kth feature in predicting the deceptiveanswer.
The best parameters w?
can be found byminimizing the following objective function:?1(w) =?iL(wTxi, yi) + ?
?
|w|2F (3)where L(wTxi, yi) is a loss function that mea-sures discrepancy between the predicted labelwT ?
xi and the true label yi, where yi ?{+1,?1}.
The common used loss functions in-clude L(p, y) = (p?y)2 (least square), L(p, y) =ln (1 + exp (?py)) (logistic regression).
For sim-plicity, here we use the least square loss function.|w|2F =?k w2k is a regularization term definedin terms of the Frobenius norm of the parametervector w and plays the role of penalizing overlycomplex models in order to avoid fitting.We want to incorporate the user preference re-lationship into the supervised learning framework.The hypothesis is that similar users tend to havesimilar behaviors, i.e.
posting deceptive answersor authentic answers.
Here, we employ the userpreference graph to denote the user relationship.Based on this intuition, we propose to incorporatethe user graph into the linear weight model withgraph regularization.
The new objective functionis changed as:?2(w) =?iL(wTxi, yi) + ?
?
|w|2F +?
?ui,uj?Nu?x?Aui ,y?Aujwui,uj (f(x)?
f(y))2 (4)where Nu is the set of neighboring user pairs inuser preference graph, i.e, the user pairs with samepreference.
Aui is the set of all answers posted byuser ui.
wui,uj is the weight of edge between uiand uj in user preference graph.
In the above ob-jective function, we impose a user graph regular-ization term?
?ui,uj?Nu?x?Aui ,y?Aujwui,uj (f(x)?
f(y))2to minimize the answer authenticity differenceamong users with same preference.
This regu-larization term smoothes the labels on the graphstructure, where adjacent users with same prefer-ence tend to post answers with same label.5 Experiments5.1 Experiment Setting5.1.1 Dataset ConstructionIn this paper, we employ the Confucius (Si etal., 2010b) data to construct the deceptive an-swer dataset.
Confucius is a community questionanswering site, developed by Google.
We firstcrawled about 10 million question threads withina time range.
Among these data, we further sam-ple a small data set, and ask three trained annota-tors to manually label the answer as deceptive ornot.
If two or more people annotate the answer asdeceptive, we will extract this answer as a decep-tive answer.
In total, 12446 answers are markedas deceptive answers.
Similarly, we also manu-ally annotate 12446 authentic answers.
Finally,we get 24892 answers with deceptive and authen-tic labels as our dataset.
With our labeled data,we employ supervised methods to predict decep-tive answers.
We conduct 5-fold cross-validationfor experiments.
The larger question threads datais employed for feature learning, such as transla-tion model, and topic model training.5.1.2 Evaluation MetricsThe evaluation metrics are precision, recall andF -score for authentic answer category and de-ceptive answer category: precision = Sp?ScSp ,recall = Sp?ScSc , and F = 2?precision?recallprecision+recall , whereSc is the set of gold-standard positive instances forthe target category, Sp is the set of predicted re-sults.
We also use the accuracy as one metric,which is computed as the number of answers pre-dicted correctly, divided by the number of total an-swers.1728Deceptive Answer Authentic Answer OverallPrec.
Rec.
F-Score Prec.
Rec.
F-Score Acc.Random 0.50 0.50 0.50 0.50 0.50 0.50 0.50Unigram/Bigram (UB) 0.61 0.71 0.66 0.66 0.55 0.60 0.63URL 0.93 0.26 0.40 0.57 0.98 0.72 0.62Phone/Mail 0.94 0.15 0.25 0.53 0.99 0.70 0.57Length 0.56 0.91 0.69 0.76 0.28 0.41 0.60All Textual Features 0.64 0.67 0.66 0.66 0.63 0.64 0.65QA Relevance 0.66 0.57 0.61 0.62 0.71 0.66 0.64User Profile 0.62 0.53 0.57 0.59 0.67 0.63 0.60User Authority 0.54 0.80 0.65 0.62 0.33 0.43 0.56Robot 0.66 0.62 0.64 0.61 0.66 0.64 0.64Answer Evaluation 0.55 0.53 0.54 0.55 0.57 0.56 0.55Answer Duplication 0.69 0.71 0.70 0.70 0.68 0.69 0.69All Contextual Feature 0.78 0.74 0.76 0.75 0.79 0.77 0.77Textutal + Contextual 0.80 0.82 0.81 0.82 0.79 0.80 0.81Table 2: Results With Textual and Contextual Features5.2 Results with Textual and ContextualFeaturesWe tried several different classifiers, includingSVM, ME and the linear weight models with leastsquare and logistic regression.
We find that theycan achieve similar results.
For simplicity, the lin-ear weight with least square is employed in ourexperiment.
Table 2 shows the experiment results.For textual features, it achieves much better re-sult with unigram/bigram features than the ran-dom guess.
This is very different from the an-swer quality prediction task.
The previous stud-ies (Jeon et al, 2006; Song et al, 2010) find thatthe word features can?t improve the performanceon answer quality prediction.
However, from Ta-ble 1, we can see that the word features can pro-vide some weak signals for deceptive answer pre-diction, for example, words ?recommend?, ?ad-dress?, ?professional?
express some kinds of pro-motion intent.
Besides unigram and bigram, themost effective textual feature is URL.
The phoneand email features perform similar results withURL.
The observation of length feature for decep-tive answer prediction is very different from previ-ous answer quality prediction.
For answer qualityprediction, length is an effective feature, for exam-ple, long-length provides very strong signals forhigh-quality answer (Shah and Pomerantz, 2010;Song et al, 2010).
However, for deceptive answerprediction, we find that the long answers are morepotential to be deceptive.
This is because most ofdeceptive answers are well prepared for productpromotion.
They will write detailed answers to at-tract user?s attention and promote their products.Finally, with all textual features, the experimentachieves the best result, 0.65 in accuracy.For contextual features, we can see that, themost effective contextual feature is answer dupli-cation.
The malicious users may copy the pre-pared deceptive answers or just simply edit the tar-get name to answer different questions.
Question-answer relevance and robot are the second mostuseful single features for deceptive answer predic-tion.
The main characteristics of the CommunityQA sites is to accumulate the answers for the tar-get questions.
Therefore, all the answers should berelevant to the question.
If the answer is not rel-evant to the corresponding question, this answeris more likely to be deceptive.
Robot is one ofmain sources for deceptive answers.
It automat-ically post the deceptive answers to target ques-tions.
Here, we formulate the time series as in-terval sequence.
The experiment result shows thatthe robot indeed has his own posting behavior pat-terns.
The user profile feature also can contributea lot to deceptive answer prediction.
Among theuser profile features, the user level in the Com-munity QA site is a good indicator.
The othertwo contextual features, including user authorityand answer evaluation, provide limited improve-ment.
We find the following reasons: First, somemalicious users post answers to various questionsfor product promotion, but don?t ask any question.From Equation 1, when iteratively computing the1729Deceptive Answer Authentic Answer OverallPrec.
Rec.
F-Score Prec.
Rec.
F-Score Acc.Interaction Graph as Feature 0.80 0.82 0.81 0.82 0.79 0.80 0.81Interaction Graph as Regularizer 0.80 0.83 0.82 0.82 0.80 0.81 0.82Preference Graph as Feature 0.79 0.83 0.81 0.82 0.78 0.80 0.81Preference Graph as Regularizer 0.83 0.86 0.85 0.85 0.83 0.84 0.85Table 3: Results With User Preference Graphfinal scores, the authority scores for these mali-cious users will be accumulated to large values.Therefore, it is hard to distinguish whether thehigh authority score represents real expert or mali-cious user.
Second, the ?best answer?
is not a goodsignal for deceptive answer prediction.
This maybe selected by malicious users, or the authenticasker was misled, and chose the deceptive answeras ?best answer?.
This also demonstrates that thedeceptive answer prediction is very different fromthe answer quality prediction.
When combiningall the contextual features, it can achieve the over-all accuracy 0.77, which is much better than thetextual features.
Finally, with all the textual andcontextual features, we achieve the overall result,0.81 in accuracy.5.3 Results with User Preference GraphTable 3 shows the results with user preferencegraph.
We compare with several baselines.
Inter-action graph is constructed by the asker-answererrelationship introduced in Section 3.2.3.
Whenusing the user graph as feature, we compute theauthority score for each user with PageRank asshown in Equation 1.
We also incorporating theinteraction graph with a regularizer as shown inEquation 4.
Note that we didn?t consider the edgedirection when using interaction graph as a regu-larizer.
From the table, we can see that when in-corporating user preference graph as a feature, itcan?t achieve a better result than the interactiongraph.
The reason is similar as the interactiongraph.
The higher authority score may boostedby other spammer, and can?t be a good indica-tor to distinguish deceptive and authentic answers.When we incorporate the user preference graphas a regularizer, it can achieve about 4% furtherimprovement, which demonstrates that the userevaluation towards answers, such as ?helpful?
vot-ing and ?best answer?
selection, is a good signalto generate user relationship for deceptive answerprediction, and the graph regularization is an ef-fective technique to incorporate the user prefer-ence graph.
We also analyze the parameter sen-10?5 10?4 10?3 10?2 10?1 1000.760.780.80.820.840.860.88AccuracyGeneral supervised methodInteraction Graph as RegularizerPreference Graph as RegularizerFigure 2: Results with different values of ?sitivity.
?
is the tradeoff weight for graph regular-ization term.
Figure 2 shows the results with dif-ferent values of ?.
We can see that when ?
rangesfrom 10?4 ?
10?2, the deceptive answer predic-tion can achieve best results.6 Conclusions and Future WorkIn this paper, we discuss the deceptive answerprediction task in Community QA sites.
Withthe manually labeled data set, we first predict thedeceptive answers with traditional classificationmethod.
Two types of features, including textualfeatures and contextual features, are extracted andanalyzed.
We also introduce a new user prefer-ence graph, constructed based on the user evalua-tions towards the target answer, such as ?helpful?voting and ?best answer?
selection.
A graph reg-ularization method is proposed to incorporate theuser preference graph for deceptive answer predic-tion.
The experiments are conducted to discussthe effects of different features.
The experimentresults also show that the method with user pref-erence graph can achieve more accurate results fordeceptive answer prediction.In the future work, it is interesting to incorpo-rate more features into deceptive answer predic-tion.
It is also important to predict the deceptivequestion threads, which are posted and answeredboth by malicious users for product promotion.Malicious user group detection is also an impor-tant task in the future.1730ReferencesLada A. Adamic, Jun Zhang, Eytan Bakshy, andMark S. Ackerman.
2008.
Knowledge sharingand yahoo answers: everyone knows something.
InProceedings of the 17th international conference onWorld Wide Web, WWW ?08, pages 665?674, NewYork, NY, USA.
ACM.Jiang Bian, Yandong Liu, Ding Zhou, EugeneAgichtein, and Hongyuan Zha.
2009.
Learning torecognize reliable users and content in social mediawith coupled mutual reinforcement.
In Proceedingsof the 18th international conference on World wideweb, WWW ?09, pages 51?60, NY, USA.
ACM.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
J. Mach.
Learn.Res., 3:993?1022, March.Peter F. Brown, John Cocke, Stephen A. Della Pietra,Vincent J. Della Pietra, Fredrick Jelinek, John D.Lafferty, Robert L. Mercer, and Paul S. Roossin.1990.
A statistical approach to machine translation.Comput.
Linguist., 16:79?85, June.S.
Deerwester, S.T.
Dumais, G.W.
Furnas, T.K.
Lan-dauer, and R. Harshman.
1990.
Indexing by latentsemantic analysis.
Journal of the American societyfor information science, 41(6):391?407.A.
Figueroa and J. Atkinson.
2011.
Maximum entropycontext models for ranking biographical answers toopen-domain definition questions.
In Twenty-FifthAAAI Conference on Artificial Intelligence.F.
Maxwell Harper, Daphne Raban, Sheizaf Rafaeli,and Joseph A. Konstan.
2008.
Predictors of answerquality in online q&a sites.
In Proceedings of thetwenty-sixth annual SIGCHI conference on Humanfactors in computing systems, CHI ?08, pages 865?874, New York, NY, USA.
ACM.Daisuke Ishikawa, Tetsuya Sakai, and Noriko Kando,2010.
Overview of the NTCIR-8 Community QA Pi-lot Task (Part I): The Test Collection and the Task,pages 421?432.
Number Part I.Jiwoon Jeon, W. Bruce Croft, and Joon Ho Lee.
2005.Finding similar questions in large question and an-swer archives.
In Proceedings of the 14th ACMCIKM conference, 05, pages 84?90, NY, USA.ACM.J.
Jeon, W.B.
Croft, J.H.
Lee, and S. Park.
2006.
Aframework to predict the quality of answers withnon-textual features.
In Proceedings of the 29th an-nual international ACM SIGIR conference on Re-search and development in information retrieval,pages 228?235.
ACM.P.
Jurczyk and E. Agichtein.
2007.
Discovering au-thorities in question answer communities by usinglink analysis.
In Proceedings of the sixteenth ACMCIKM conference, pages 919?922.
ACM.H.
Kim, P. Howland, and H. Park.
2006.
Dimensionreduction in text classification with support vectormachines.
Journal of Machine Learning Research,6(1):37.Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan Zhu.2011.
Learning to identify review spam.
In Pro-ceedings of the Twenty-Second international jointconference on Artificial Intelligence-Volume VolumeThree, pages 2488?2493.
AAAI Press.Yuanjie Liu, Shasha Li, Yunbo Cao, Chin-Yew Lin,Dingyi Han, and Yong Yu.
2008.
Understand-ing and summarizing answers in community-basedquestion answering services.
In Proceedings of the22nd International Conference on ComputationalLinguistics - Volume 1, COLING ?08, pages 497?504, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Jing Liu, Young-In Song, and Chin-Yew Lin.
2011.Competition-based user expertise score estimation.In Proceedings of the 34th international ACM SI-GIR conference on Research and development in In-formation Retrieval, pages 425?434.
ACM.Yue Lu, Panayiotis Tsaparas, Alexandros Ntoulas, andLivia Polanyi.
2010.
Exploiting social context forreview quality prediction.
In Proceedings of the19th international conference on World wide web,pages 691?700.
ACM.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Comput.
Linguist., 29:19?51, March.Lawrence Page, Sergey Brin, Rajeev Motwani, andTerry Winograd.
1999.
The pagerank citation rank-ing: Bringing order to the web.
Technical Report1999-66, Stanford InfoLab, November.
SIDL-WP-1999-0120.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Com-putational Linguistics, ACL ?02, pages 311?318,Stroudsburg, PA, USA.
ACL.Gerard Salton and Michael J. McGill.
1986.
Intro-duction to Modern Information Retrieval.
McGraw-Hill, Inc., New York, NY, USA.Chirag Shah and Jefferey Pomerantz.
2010.
Evaluat-ing and predicting answer quality in community qa.In Proceedings of the 33rd international ACM SIGIRconference on Research and development in infor-mation retrieval, SIGIR ?10, pages 411?418, NewYork, NY, USA.
ACM.X.
Si, Z. Gyongyi, and E. Y. Chang.
2010a.
Scal-able mining of topic-dependent user reputation forimproving user generated content search quality.
InGoogle Technical Report.1731Xiance Si, Edward Y. Chang, Zolta?n Gyo?ngyi, andMaosong Sun.
2010b.
Confucius and its intelli-gent disciples: integrating social with search.
Proc.VLDB Endow., 3:1505?1516, September.Young-In Song, Chin-Yew Lin, Yunbo Cao, and Hae-Chang Rim.
2008.
Question utility: a novel staticranking of question search.
In Proceedings of the23rd national conference on Artificial intelligence- Volume 2, AAAI?08, pages 1231?1236.
AAAIPress.Y.I.
Song, J. Liu, T. Sakai, X.J.
Wang, G. Feng, Y. Cao,H.
Suzuki, and C.Y.
Lin.
2010.
Microsoft researchasia with redmond at the ntcir-8 community qa pilottask.
In Proceedings of NTCIR.Wei Wei, Gao Cong, Xiaoli Li, See-Kiong Ng, andGuohui Li.
2011.
Integrating community questionand answer archives.
In AAAI.Y.
Yang and J.O.
Pedersen.
1997.
A compara-tive study on feature selection in text categoriza-tion.
In MACHINE LEARNING-INTERNATIONALWORKSHOP THEN CONFERENCE-, pages 412?420.
MORGAN KAUFMANN PUBLISHERS.Tong Zhang, Alexandrin Popescul, and Byron Dom.2006.
Linear prediction models with graph regu-larization for web-page categorization.
In Proceed-ings of the 12th ACM SIGKDD international con-ference on Knowledge discovery and data mining,pages 821?826.
ACM.1732
