Proceedings of NAACL HLT 2007, Companion Volume, pages 21?24,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsTranslation Model Pruning via Usage Statisticsfor Statistical Machine TranslationMatthias Eck, Stephan Vogel, and Alex WaibelInterACT ResearchCarnegie Mellon University, Pittsburgh, USAmatteck@cs.cmu.edu, vogel+@cs.cmu.edu, ahw@cs.cmu.eduAbstractWe describe a new pruning approach toremove phrase pairs from translation mod-els of statistical machine translation sys-tems.
The approach applies the originaltranslation system to a large amount of textand calculates usage statistics for thephrase pairs.
Using these statistics the rele-vance of each phrase pair can be estimated.The approach is tested against a strongbaseline based on previous work and showssignificant improvements.1 IntroductionA relatively new device for translation systems aresmall portable devices like cell phones, PDAs andhandheld game consoles.
The idea here is to have alightweight and convenient translation device e.g.for tourists that can be easily carried.
Other appli-cations include medical, relief, and military scenar-ios.Preferably such a device will offer speech-to-speech translation for both (or multiple) translationdirections.
These devices have been researched andare starting to become commercially available (e.g.Isotani et al, 2003).
The main challenges here arethe severe restrictions regarding both memory andcomputing power on such a small portable device.1.1 Statistical Machine TranslationGenerally statistical machine translation systemshave recently outperformed other translation ap-proaches so it seems natural to also apply them inthese scenarios.A main component of every statistical machinetranslation system is the translation model.
Thetranslation model assigns translation probabilitiesto phrase1 pairs of source and target phrases ex-tracted from a parallel bilingual text.
These phrasepairs are applied during the decoding process andtheir target sides are combined to form the finaltranslation.
A variety of algorithms to extractphrase pairs has been proposed.
(e.g.
Och and Ney,2000 and Vogel, 2005).Our proposed approach now tries to removephrase pairs, which have little influence on the fi-nal translation performance, from a translation sys-tem (pruning of the translation model2).
The goalis to reduce the number of phrase pairs and in turnthe memory requirement of the whole translationsystem, while not impacting the translation per-formance too heavily.The approach does not depend on the actual al-gorithm used to extract the phrase pairs and can beapplied to every imaginable method that assignsprobabilities to phrase pairs.
We assume that thephrase pairs were pre-extracted before decoding.
(in contrast to the proposed approaches to ?onlinephrase extraction?
(Zhang and Vogel, 2005; Calli-son-Burch et al, 2005)).The task now is to remove enough pre-extractedphrase pairs in order to accommodate the possiblystrict memory limitations of a portable devicewhile restricting performance degradation as muchas possible.We will not specifically address the computingpower limitations of the portable devices in thispaper.1A ?phrase?
here can also refer to a single word.2Small language models are also desirable and the approachescould be applied as well but this was not investigated yet.212 Previous workPrevious work mainly introduced two natural ideasto prune phrase pairs.
Both are for example di-rectly available in the Pharaoh decoder (Koehn,2004).Probability thresholdA very simple way to prune phrase pairs from atranslation model is to use a probability thresholdand remove all pairs for which the translationprobability is below the threshold.
The reasoningfor this is that it is very unlikely that a translationwith a very low probability will be chosen (overanother translation candidate with a higher prob-ability).Translation variety thresholdAnother way to prune phrase pairs is to impose alimit on the number of translation candidates for acertain phrase.
That means the pruned translationmodel can only have equal or fewer possible trans-lations for a given source phrase than the thresh-old.
This is accomplished by sorting the phrasepairs for each source phrase according to theirprobability and eliminating low probability onesuntil the threshold is reached.3 Pruning via Usage StatisticsThe approach presented here uses a different ideainspired by the Optimal Brain Damage algorithmfor neural networks (Le Cun et al, 1990).The Optimal Brain Damage algorithm for neuralnetworks computes a saliency for each networkelement.
The saliency is the relevance for the per-formance of the network.
In each pruning step theelement with the smallest saliency is removed, andthe network is re-trained and all saliencies are re-calculated etc.We can analogously view each phrase pair in thetranslation system as such a network element.
Thequestion is of course how to calculate the relevancefor the performance for each phrase pair.A simple approximation was already done in theprevious work using a probability or varietythreshold.
Here the relevance is estimated using thephrase pair probability or the phrase pair rank asrelevance indicators.But these are not the only factors that influencethe final selection of a phrase pair and most ofthese factors are not established during the trainingand phrase extraction process.
Especially the fol-lowing two additional factors play a major role inthe importance of a phrase pair.Frequency of the source phraseWe can clearly say that a phrase pair with a verycommon source phrase will be much more impor-tant than a phrase pair where the source phrase oc-curs only very rarely.Actual use of the phrase-pairBut even phrase-pairs with very common sourcephrases might not be used for the final translationhypothesis.
It is for example possible that it is partof a longer phrase pair that gets a higher probabil-ity so that the shorter phrase pair is not used.Generally there are a lot of different factors influ-encing the estimated importance of a phrase pairand it seems hard to consider every influence sepa-rately.
Hence the proposed idea does not use acombination of features to estimate the phrase pairimportance.
Instead the idea is to just apply thetranslation system to a large amount of text and seehow often a phrase pair is actually used (i.e.
influ-ences the translation performance).
If the translatedtext is large enough this will give a good statisticsof the relevance of this respective phrase pair.
Thisleads to the following algorithm:AlgorithmTranslate a large amount of (in-domain) data withthe translation system (tuned on a development set)and collect the following two statistics for eachphrase pair in the translation model.?
c(phrase pair) = Count how often a phrase pairwas considered during decoding (i.e.
wasadded to the translation lattice)?
u(phrase pair) = Count how often a phrase pairwas used in the final translation (i.e.
in thechosen path through the lattice).The overall score for a phrase pair with simplesmoothing (+1) is calculated as:[ ] [ ]1)(*)1)(log()(pair phrasepair phrasepair phrase++=ucscoreWe use the logarithm function to limit the influ-ence of the c value.
The u value is more importantas this measures how often a phrase was actuallyused in a translation hypothesis.
This scoring func-22tion was empirically found after experimentingwith a variety of possible scoring terms.The phrase pairs can then be sorted according tothis score and the top n phrase pairs can be selectedfor a smaller phrase translation model.4 Data and Experiments4.1 Experimental Setup & BaselineTranslation systemThe translation system that was used for the ex-periments is a state-of-the-art statistical machinetranslation system (Eck et al 2006).
The systemuses a phrase extraction method described in Vogel(2005) and a 6-gram language model.Training and testing dataThe training data for all experiments consisted ofthe BTEC corpus (Takezawa et al, 2002) with162,318 lines of parallel Japanese-English text.
Alltranslations were done from Japanese to English.The language model was trained on the Englishpart of the training data.The test set from the evaluation campaign ofIWSLT 2004 (Akiba et al, 2004) was used as test-ing data.
This data consists of 500 lines of tourismdata.
16 reference translations to English wereavailable.Extracted phrasesPhrase pairs for n-grams up to length 10 were ex-tracted (with low frequency thresholds for highern-grams).
This gave 4,684,044 phrase pairs(273,459 distinct source phrases).
The baselinescore using all phrase pairs was 59.11 (BLEU,Papineni et al, 2002) with a 95% confidence inter-val of [57.13, 61.09].Baseline pruningThe approaches presented in previous work servedas a baseline.
The probability threshold was testedfor 8 values (0 (no pruning), 0.0001, 0.0005, 0.001,0.005, 0.01, 0.05, 0.1) while the variety thresholdtested for 14 values (1, 2, 3, 4, 5, 6, 8, 10, 15, 20,50, 100, 200, 500 (no pruning in this case)) and allcombinations thereof.
The final translation scoresfor different settings are very fluctuating.
For thatreason we defined the baseline score for each pos-sible size as the best score that was reached withequal or less phrase pairs than the given size in anyof the tested combinations.4.2 Results forPruning via Usage StatisticsFor the proposed approach ?Pruning via UsageStatistics?, the translation system was applied tothe 162,318 lines of Japanese training data.As explained in section 3 it was now counted foreach phrase pair how often it occurred in a transla-tion lattice and how often it was used for the finaltranslation.
The phrase pairs were then sorted ac-cording to their relevance estimation and the top nphrase pairs were chosen for different values of n.The pruned phrase table was then used to translatethe IWSLT 2004 test set.
Table 1 shows the resultscomparing the baseline scores with the results us-ing the described pruning.
Figure 1 illustrates thescores.
The plateaus in the baseline graph are dueto the baseline definition as stated above.BLEU scores# of PhrasePairs (n)BaselinePruningRelative scoreimprovement100,000 - 0.4735 -200,000 0.3162 0.5008 58.38%300,000 0.4235 0.5154 21.70%400,000 0.4743 0.5241 10.50%500,000 0.4743 0.5269 11.09%600,000 0.4890 0.5359 9.59%800,000 0.5194 0.5394 3.85%1,000,000 0.5355 0.5442 1.62%1,500,000 0.5413 0.5523 2.03%2,000,000 0.5630 0.5749 2.11%3,000,000 0.5778 0.5798 0.35%4,000,000 0.5855 0.5865 0.17%4,684,044 0.5911 0.5911 0.00%Table 1: BLEU scores at different levels of pruning(Baseline: Best score with equal or less phrasepairs)For more than 1 million phrase pairs the differ-ences are not very pronounced.
However the trans-lation score for the proposed pruning algorithm isstill not significantly lower than the 59.11 score at2 million phrase pairs while the baseline dropsslightly faster.
For less than 1 million phrase pairsthe differences become much more pronouncedwith relative improvements of up to 58% at200,000 phrase pairs.
It is interesting to note thatthe improved pruning removes infrequent source23phrases and to a lesser extent source vocabularyeven for larger numbers of phrase pairs.0.300.350.400.450.500.550.600 1,000,000 2,000,000 3,000,000 4,000,000phrase pairsBLEUscoreBaseline PruningFigure 1: Pruning and baseline comparison5 Conclusions and Future WorkThe proposed pruning algorithm is able to outper-form a strong baseline based on previously intro-duced threshold pruning ideas.
Over 50% of phrasepairs can be pruned without a significant loss ofperformance.
Even for very low memory situationsthe improved pruning remains a viable optionwhile the baseline pruning performance dropsheavily.One idea to improve this new pruning approachis to exchange the used count with the count of thephrase occurring in the best path of the lattice ac-cording to a scoring metric.
This would requirehaving a reference translation available to be ableto tell which path is the actual best one (metric-best path).
It would be interesting to compare theperformance if the statistics is done using the met-ric-best path on a smaller amount of data to theperformance if the statistics is done using themodel-best path on a larger amount (as there is noreference translation necessary).The Optimal Brain Damage algorithm recalcu-lates the saliency after removing each networkelement.
It could also be beneficial to sequentiallyprune the phrase pairs and always re-calculate thestatistics after removing a certain number of phrasepairs.6 AcknowledgementsThis work was partly supported by the US DARPAunder the programs GALE and TRANSTAC.7 ReferencesYasuhiro Akiba, Marcello Federico, Noriko Kando,Hiromi Nakaiwa, Michael Paul, and Jun'ichi Tsujii}.2004.
Overview of the IWSLT04 Evaluation Cam-paign.
Proceedings of IWSLT 2004, Kyoto, Japan.Chris Callison-Burch, Colin Bannard, and Josh Schroe-der.
2005.
Scaling Phrase-Based Statistical MachineTranslation to Larger Corpora and Longer Phrases.Proceedings of ACL 2005, Ann Arbor, MI, USA.Yann Le Cun, John S. Denker, and Sara A. Solla.
1990.Optimal brain damage.
In Advances in Neural In-formation Processing Systems 2, pages 598-605.Morgan Kaufmann, 1990.Matthias Eck, Ian Lane, Nguyen Bach, Sanjika He-wavitharana, Muntsin Kolss, Bing Zhao, Almut SiljaHildebrand, Stephan Vogel, and Alex Waibel.
2006.The UKA/CMU Statistical Machine Translation Sys-tem for IWSLT 2006.
Proceedings of IWSLT 2006,Kyoto, Japan.Ryosuke Isotani, Kyoshi Yamabana, Shinichi Ando,Ken Hanazawa, Shin-ya Ishikawa and Ken.ichi Iso.2003.
Speech-to-speech translation software onPDAs for travel conversation.
NEC research & de-velopment, Tokyo, Japan.Philipp Koehn.
2004.
A Beam Search Decoder for Sta-tistical Machine Translation Models.
Proceedings ofAMTA 2004, Baltimore, MD, USA.Franz Josef Och and Hermann Ney, 2000.
Improvedstatistical alignment models, Proceedings of ACL2000, Hongkong, China.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
Proceedings ofACL 2002, Philadelphia, PA, USA.Toshiyuki Takezawa, Eiichiro Sumita, Fumiaki Sugaya,Hirofumi Yamamoto, and Seiichi Yamamoto.
2002.Toward a Broad-coverage Bilingual Corpus forSpeech Translation of Travel Conversation in theReal World.
Proceedings of LREC 2002, Las Palmas,Spain.Stephan Vogel.
2005.
PESA: Phrase Pair Extraction asSentence Splitting.
Proceedings of MTSummit X,Phuket, Thailand.Ying Zhang and Stephan Vogel.
2005.
An EfficientPhrase-to-Phrase Alignment Model for ArbitrarilyLong Phrases and Large Corpora.
Proceedings ofEAMT 2005, Budapest, Hungary.24
