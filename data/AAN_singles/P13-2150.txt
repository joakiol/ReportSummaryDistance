Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 866?872,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsExplicit and Implicit Syntactic Features for Text ClassificationMatt Post1 and Shane Bergsma1,21Human Language Technology Center of Excellence2Center for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MDAbstractSyntactic features are useful for manytext classification tasks.
Among these,tree kernels (Collins and Duffy, 2001)have been perhaps the most robust andeffective syntactic tool, appealing fortheir empirical success, but also be-cause they do not require an answerto the difficult question of which treefeatures to use for a given task.
Wecompare tree kernels to different ex-plicit sets of tree features on five diversetasks, and find that explicit features of-ten perform as well as tree kernels onaccuracy and always in orders of mag-nitude less time, and with smaller mod-els.
Since explicit features are easy togenerate and use (with publicly avail-able tools), we suggest they should al-ways be included as baseline compar-isons in tree kernel method evaluations.1 IntroductionFeatures computed over parse trees are use-ful for a range of discriminative tasks, in-cluding authorship attribution (Baayen et al1996), parse reranking (Collins and Duffy,2002), language modeling (Cherry and Quirk,2008), and native-language detection (Wongand Dras, 2011).
A major distinction amongthese uses of syntax is how the features are rep-resented.
The implicit approach uses treekernels (Collins and Duffy, 2001), which makepredictions with inner products between treepairs.
These products can be computed effi-ciently with a dynamic program that producesweighted counts of all the shared tree frag-ments between a pair of trees, essentially in-corporating all fragments without representingany of them explicitly.
Tree kernel approacheshave been applied successfully in many areasof NLP (Collins and Duffy, 2002; Moschitti,2004; Pighin and Moschitti, 2009).Tree kernels were inspired in part by ideasfrom Data-Oriented Parsing (Scha, 1990; Bod,1993), which was in turn motivated by uncer-tainty about which fragments to include in agrammar.
However, manual and automaticapproaches to inducing tree fragments haverecently been found to be useful in an ex-plicit approach to text classification, whichemploys specific tree fragments as features instandard classifiers (Post, 2011; Wong andDras, 2011; Swanson and Charniak, 2012).These feature sets necessarily represent only asmall subset of all possible tree patterns, leav-ing open the question of what further gainsmight be had from the unusued fragments.Somewhat surprisingly, explicit and implicitsyntactic features have been explored largelyindependently.
Here, we compare them on arange of classification tasks: (1,2) grammati-cal classification (is a sentence written by a hu-man?
), (3) question classification (what typeof answer is sought by this question?
), and(4,5) native language prediction (what is thenative language of a text?s author?
).Our main contribution is to show that an ex-plicit syntactic feature set performs as well orbetter than tree kernels on each tested task,and in orders of magnitude less time.
Sinceexplicit features are simple to generate (withpublicly available tools) and flexible to use, werecommend they be included as baseline com-parisons in tree kernel method evaluations.2 Experimental setupWe used the following feature sets:N-grams All unigrams and bigrams.11Experiments with trigrams did not show any im-866CFG rules Counts of depth-one context-free grammar (CFG) productions obtainedfrom the Berkeley parser (Petrov et al 2006).C&J features The parse-tree rerankingfeature set of Charniak and Johnson (2005),extracted from the Berkeley parse trees.TSG features We also parsed with aBayesian tree substitution grammar (Post andGildea, 2009, TSG)2 and extracted fragmentcounts from Viterbi derivations.We build classifiers with Liblinear3 (Fanet al 2008).
We divided each dataset intotraining, dev, and test sets.
We then trainedan L2-regularized L1-loss support vector ma-chine (-s 3) with a bias parameter of 1 (-B 1),optimizing the regularization parameter (-c)on the dev set over the range {0.0001 .
.
.
100}by multiples of 10.
The best model was thenused to classify the test set.
A sentence lengthfeature was included for every sentence.For tree kernels, we used SVM-light-TK4(Moschitti, 2004; Moschitti, 2006) with thedefault settings (-t 5 -D 1 -L 0.4),5 whichalso solves an L2-regularized L1-loss SVM op-timization problem.
We tuned the regulariza-tion parameter (-c) on the dev set in the samemanner as described above, providing 4 GB ofmemory to the kernel cache (-m 4000).6 Weused subset tree kernels, which compute thesimilarity between two trees by implicitly enu-merating all possible fragments of the trees (incontrast with subtree kernels, where all frag-ments fully extend to the leaves).3 TasksTable 1 summarizes our datasets.3.1 Coarse grammatical classificationOur first comparison is coarse grammaticalclassification, where the goal is to distin-guish between human-written sentences and?pseudo-negative?
sentences sampled from atrigram language model constructed from in-provement.2github.com/mjpost/dptsg3www.csie.ntu.edu.tw/~cjlin/liblinear/4disi.unitn.it/moschitti/Tree-Kernel.htm5Optimizing SVM-TK?s decay parameter (-L) didnot improve test-set accuracy, but did increase trainingtime (squaring the number of hyperparameter combi-nations to evaluate), so we stuck with the default.6Increased from the default of 40 MB, which halvesthe running time.train dev testCoarse grammaticality (BLLIP)sentences 100,000 6,000 6,000Fine grammaticality (PTB)sentences 79,664 3,978 3,840Question classification (TREC-10)sentences 4,907 545 500Native language (ICLE; 7 languages)documents 490 105 175sentences 17,715 3,968 6,777Native language (ACL; 5 languages)documents 987 195 185sentences 146,257 28,139 28,403Table 1: Datasets.system accuracy CPU timeChance 50.0 -N-gram 68.4 minutesCFG 86.3 minutesTSG 89.8 minutesC&J 92.9 an hourSVM-TK 91.0 a weekTable 2: Coarse grammaticality.
CPU time isfor classifier setup, training, and testing.domain data (Okanohara and Tsujii, 2007).Cherry and Quirk (2008) first applied syn-tax to this task, learning weighted parametersfor a CFG with a latent SVM.
Post (2011)found further improvements with fragment-based representations (TSGs and C&J) with aregular SVM.
Here, we compare their resultsto kernel methods.
We repeat Post?s experi-ments on the BLLIP dataset,7 using his exactdata splits (Table 2).
To our knowledge, treekernels have not been applied to this task.3.2 Fine grammatical classificationReal-world grammaticality judgments requiremuch finer-grained distinctions than thecoarse ones of the previous section (for exam-ple, marking dropped determiners or wrongverb inflections).
For this task, we too pos-itive examples from all sentences of sections2?21 of the WSJ portion of the Penn Tree-bank (Marcus et al 1993).
Negative exam-ples were created by inserting one or two errors7LDC Catalog No.
LDC2000T43867system accuracy CPU timeWong & Dras 60.6 -Chance 50.0 -N-gram 61.4 minutesCFG 64.5 minutesTSG 67.0 minutesC&J 71.9 an hourSVM-TK 67.8 weeksTable 3: Fine-grained classification accuracy(the Wong and Dras (2010) score is the highestscore from the last column of their Table 3).system accuracy CPU timePighin & Moschitti 86.6 -Bigram 73.2 secondsCFG 90.0 secondsTSG 85.6 secondsC&J 89.6 minutesSVM-TK 87.7 twenty min.Table 4: Question classification (6 classes).into the parse trees from the positive data us-ing GenERRate (Foster and Andersen, 2009).An example sentence pair is But the ballplay-ers disagree[ing], where the negative exam-ple incorrectly inflects the verb.
Wong andDras (2010) reported good results with parserstrained separately on the positive and negativesides of the training data and classifiers builtfrom comparisons between the CFG produc-tions of those parsers.
We obtained their datasplits (described as NoisyWSJ in their paper)and repeat their experiments here (Table 3).3.3 Question ClassificationWe look next at question classification (QC).Li and Roth (2002) introduced the TREC-10dataset,8 a set of questions paired with labelsthat categorize the question by the type of an-swer it seeks.
The labels are organized hi-erarchically into six (coarse) top-level labelsand fifty (fine) refinements.
An example ques-tion from the ENTY/animal category is Whatwas the first domesticated bird?.
Table 4 con-tains results predicting just the coarse labels.We compare to Pighin and Moschitti (2009),and also repeat their experiments, finding aslightly better result for them.8cogcomp.cs.illinois.edu/Data/QA/QC/system sent.
voting wholeWong & Dras - - 80.0Style 42.0 75.3 86.8CFG 39.5 73.2 83.7TSG 38.7 72.1 83.2C&J 42.9 76.3 86.3SVM-TK 40.7 69.5 -Style 42.5 65.3 83.7CFG 39.2 52.6 86.3TSG 40.4 56.8 84.7C&J 49.2 66.3 81.1SVM-TK 42.1 52.6 -Table 5: Accuracy on ICLE (7 languages, top)and ACL (five, bottom) datasets at the sen-tence and document levels.
All documentswere signature-stylized (?3.4).We also experimented with the refined ver-sion of the task, where we directly predict oneof the fifty refined categories, and found nearlyidentical relative results, with the best explicitfeature set (CFG) returning an accuracy of83.6% (in seconds), and the tree kernel system69.8% (in an hour).
For reference, Zhang andLee (2003) report 80.2% accuracy when train-ing on the full training set (5,500 examples)with an SVM and bag-of-words features.93.4 Native language identificationNative language identification (NLI) is thetask of determining a text?s author?s nativelanguage.
This is usually cast as a document-level task, since there are often not enoughcues to identify native languages at smallergranularities.
As such, this task presents achallenge to tree kernels, which are defined atthe level of a single parse tree and have no ob-vious document-level extension.
Table 5 there-fore presents three evaluations: (a) sentence-level accuracy, and document-level accuracyfrom (b) sentence-level voting and (c) direct,whole-document classification.We perform these experiments on twodatasets.
In order to mitigate topic bias10 andother problems that have been reported with9Pighin and Moschitti (2009) did not report resultson this version of the task.10E.g., when we train with all words, the keyword?Japanese?
is a strong indicator for Japanese authors,while ?Arabic?
is a strong indicator for English ones.868the ICLE dataset (Tetreault et al 2012),11 wepreprocessed each dataset into two signature-stylized versions by replacing all words not in astopword list.12 The first version replaces non-stopwords with word classes computed fromsurface-form signatures,13 and the second withPOS tags.14 N-gram features are then takenfrom both stylized versions of the corpus.Restricting the feature representation to betopic-independent is standard-practice in sty-lometric tasks like authorship attribution, gen-der identification, and native-language identi-fication (Mosteller and Wallace, 1984; Koppelet al 2003; Tomokiyo and Jones, 2001).3.4.1 ICLE v.2The first dataset is a seven-language subsetof the International Corpus of Learner En-glish, Version 2 (ICLE) (Granger et al 2009),which contains 3.7 million words of Englishdocuments written by people with sixteen dif-ferent native languages.
Table 1 containsscores, including one reported by Wong andDras (2011), who used the CFG and C&J fea-tures, and whose data splits we mirror.153.4.2 ACL Anthology NetworkWe also experimented with native languageclassification on scientific documents usinga version of the ACL Anthology Network(Radev et al 2009, AAN) annotated for ex-periments in stylemetric tasks, including anative/non-native author judgment (Bergsmaet al 2012).
For NLI, we further anno-tated this dataset in a semi-automatic fash-ion for the five most-common native languagesof ACL authors in our training era: English,Japanese, German, Chinese, and French.
Theannotation heuristics, designed to favor pre-cision over recall, provided annotations for1,959 of 8,483 papers (23%) in the 2001?2009AAN.1611Including prompts, characters, and special tokensthat correlate strongly with particular outcomes.12The stopword list contains the set of 524 SMART-system stopwords used by Tomokiyo and Jones (2001),plus punctuation and Latin abbreviations.13For example, suffix and capitalization.14Via CRFTagger (Phan, 2006).15Tetreault et alreported accuracies up to 90.1 in across-validation setting that isn?t directly comparable.16Details and data at old-site.clsp.jhu.edu/~sbergsma/Stylo/.607080901000 0.01 0.1 1 10 100 1,000accuracytraining time (thousands of seconds)size CFG CFG TSG TSG TSG+ TSG+ C&J C&J SVM-TK SVM-TK uSVM-TK USVM-TK100 7 62.6 6 61.0 8 73.1 407 72.8 13 62.9 27 62.7300 7 68.0 6 65.0 8 77.9 412 77.5 46 70.8 174 70.91000 7 73.3 6 70.9 9 78.4 433 82.2 227 77.1 1475 77.43000 9 75.8 7 77.5 12 82.3 465 87.1 1034 81.4 4394 81.210000 13 80.8 11 82.5 32 85.2 708 89.9 8984 85.5 6691 85.330000 37 83.5 29 85.8 108 87.7 1276 92.7 72859 88.8 7789 87.8100000 133 86.3 85 89.1 406 89.8 3152 93.0 873969 91.0 8488 89.0CFGTSGC&JSVM-TKuSVM-TKuSVM-TK USVM-TK1010.35 62.72628.84 70.97264.65 77.425447.47 81.229298.76 85.345938.05 87.848570.46 89.0OLD VALUESFigure 1: Train ng time (1000 econds) vs.test accuracy for coarse grammaticality, plot-ting test scores from models trained on 100,300, 1k, 3k, 10k, 30k, and 100k instances.4 DiscussionSyntactic features improve upon the n-grambaseline for all tasks except whole-documentclassification for ICLE.
Tree kernels are oftenamong the best, but always trail (by ordersof magnitude) when runtime is considered.Constructing the multi-class SVM-TK modelsfor the NLI tasks in particular was computa-tionally burdensome, requiring cpu-months oftime.
The C&J features are similarly often thebest, but incur a runtime cost due to the largemodels.
CFG and TSG features balance per-formance, model size, and runtime.
We nowcompare these approaches in more depth.4.1 Training time versus accuracyTree kernel training is quadratic in the size ofthe training data, and its empirical slownessis known.
It is informative to examine learn-ing curves to see how the time-accuracy trade-offs extrapolate.
We compared models trainedon the first 100, 300, 1k, 3k, 10k, 30k, and100k data points of the coarse grammaticalitydataset, split evenly between positive and neg-ative examples (Figure 1).
SVM-TK improvesover the TSG and CFG models in the limit,but at an extraordinary cost in training time:100k training examples is already pushing thebounds of practicality for tree kernel learning,and generating curve?s next point would re-quire several months of time.
Kernel methodsalso produce large models that result in slowtest-time performance, a problem dubbed the?curse of kernelization?
(Wang et al 2010).Approximate kernel methods designed toscale to large datasets address this (Severyn869and Moschitti, 2010).
We investigated theuSVM-TK toolkit,17 which enables tuning thetradeoff between training time and accuracy.While faster than SVM-TK, its performancewas never better than explicit methods alongboth dimensions (time and accuracy).4.2 OverfittingOverfitting is also a problem for kernel meth-ods.
The best models often had a huge numberof support vectors, achieving near-perfect ac-curacy on the training set but making manyerrors on the dev.
and test sets.
On the ICLEtask, close to 75% of all the training exam-ples were used as support vectors.
We foundonly half as many support vectors used for theexplicit representations, implying less error(Vapnik, 1998), and saw much lower variancebetween training and testing performance.4.3 Which fragments?Our findings support the observations ofCumby and Roth (2003), who point out thatkernels introduce a large number of irrelevantfeatures that may be especially harmful insmall-data settings, and that, when possible, itis often better to have a set of explicit, relevantfeatures.
In other words, it is better to havethe right features than all of them.
Tree ker-nels provide a robust, efficiently-computablemeasure of comparison, but they also skirt thedifficult question, Which fragments?So what are the ?right?
features?
Table 6)presents an intuitive list from the coarse gram-maticality task: phenomena such as balancedparenthetical phrases and quotations are asso-ciated with grammaticality, while small, flat,abstract rules indicate samples from the n-gram model.
Similar intuitive results hold forthe other tasks.
The immediate interpretabil-ity of the explicit formalisms is another ad-vantage, although recent work has shown thatweights on the implicit features can also be ob-tained after a kind of linearization of the treekernel (Pighin and Moschitti, 2009).Ultimately, which features matter is task-dependent, and skirting the question is ad-vantageous in many settings.
But it is alsoencouraging that methods for selecting frag-ments and other tree features work so well,17disi.unitn.it/~severyn/code.html(TOP (S ?
S , ?
NP (VP (VBZ says) ADVP) .
))(FRAG (X SYM) VP .
)(PRN (-LRB- -LRB-) S (-RRB- -RRB-))(PRN (-LRB- -LRB-) NP (-RRB- -RRB-))(S NP VP .
)(NP (NP DT CD (NN %)) PP)(NP DT)(PP (IN of))(TOP (NP NP PP PP .
))(NP DT JJ NNS)Table 6: The highest- and lowest-weightedTSG features (coarse grammaticality).yielding quick, light-weight models that con-trast with the heavy machinery of tree kernels.5 ConclusionTree kernels provide a robust measure of com-parison between trees, effectively making useof all fragments.
We have shown that forsome tasks, it is sufficient (and advantageous)to instead use an explicitly-represented subsetof them.
In addition to their flexibility andinterpetability, explicit syntactic features of-ten outperformed tree kernels in accuracy, andeven where they did not, the cost was multipleorders of magnitude increase in both trainingand testing time.
These results were consistentacross a range of task types, dataset sizes, andclassification arities (binary and multiclass).There are a number of important caveats.We explored a range of data settings, butthere are many others where tree kernels havebeen proven useful, such as parse tree rerank-ing (Collins and Duffy, 2002; Shen and Joshi,2003), sentence subjectivity (Suzuki et al2004), pronoun resolution (Yang et al 2006),relation extraction (Culotta and Sorensen,2004), machine translation evaluation (Liuand Gildea, 2005), predicate-argument recog-nition, and semantic role labeling (Pighin andMoschitti, 2009).
There are also tree ker-nel variations such as dependency tree kernels(Culotta and Sorensen, 2004) and shallow se-mantic tree kernels (Moschitti et al 2007).These variables provide a rich environment forfuture work; in the meantime, we take these re-sults as compelling motivation for the contin-ued development of explicit syntactic features(both manual and automatically induced), andsuggest that such features should be part ofthe baseline systems on applicable discrimina-tive NLP tasks.870ReferencesHarald Baayen, Hans Van Halteren, and FionaTweedie.
1996.
Outside the cave of shadows:Using syntactic annotation to enhance author-ship attribution.
Literary and Linguistic Com-puting, 11(3):121.Shane Bergsma, Matt Post, and David Yarowsky.2012.
Stylometric analysis of scientific arti-cles.
In Proc.
of NAACL-HLT, pages 327?337,Montre?al, Canada, June.
Association for Com-putational Linguistics.Rens Bod.
1993.
Using an annotated corpus as astochastic grammar.
In Proc.
of ACL, Colum-bus, Ohio, USA, June.Eugene Charniak and Mark Johnson.
2005.Coarse-to-fine n-best parsing and MaxEnt dis-criminative reranking.
In Proc.
of ACL, pages173?180, Ann Arbor, Michigan, USA, June.Colin Cherry and Chris Quirk.
2008.
Discrimi-native, syntactic language modeling through la-tent SVMs.
In Proc.
of AMTA, Waikiki, Hawaii,USA, October.Michael Collins and Nigel Duffy.
2001.
Convolu-tion kernels for natural language.
In Proc.
ofNIPS.Michael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: kernelsover discrete structures, and the voted percep-tron.
In Proc.
of ACL, pages 173?180, Philadel-phia, Pennsylvania, USA, July.Aron Culotta and Jeffrey Sorensen.
2004.
Depen-dency tree kernels for relation extraction.
InProc.
of ACL, pages 423?429.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh,Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIB-LINEAR: A library for large linear classification.Journal of Machine Learning Research, 9:1871?1874.Jennifer Foster and ?istein E. Andersen.
2009.GenERRate: Generating errors for use in gram-matical error detection.
In Proceedings of thefourth workshop on innovative use of NLP forbuilding educational applications, pages 82?90.Sylviane Granger, Estelle Dagneaux, Fanny Me-unier, and Magali Paquot.
2009.
The Inter-national Corpus of Learner English.
Version 2.Handbook and CD-Rom.Moshe Koppel, Shlomo Argamon, and Anat RachelShimoni.
2003.
Automatically categorizingwritten texts by author gender.
Literary andLinguistic Computing, 17(4):401?412.Xin Li and Dan Roth.
2002.
Learning questionclassifiers.
In Proc.
of COLING, pages 1?7.Ding Liu and Daniel Gildea.
2005.
Syntactic fea-tures for evaluation of machine translation.
InProceedings of the ACL Workshop on Intrinsicand Extrinsic Evaluation Measures for MachineTranslation and/or Summarization, pages 25?32.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large an-notated corpus of English: The Penn Treebank.Computational Linguistics, 19(2):330.Alessandro Moschitti, Silvia Quarteroni, RobertoBasili, and Suresh Manandhar.
2007.
Exploit-ing syntactic and shallow semantic kernels forquestion answer classification.
In Proc.
of ACL,pages 776?783, Prague, Czech Republic, June.Alessandro Moschitti.
2004.
A study on convo-lution kernels for shallow semantic parsing.
InProc.
of ACL.Alessandro Moschitti.
2006.
Making tree kernelspractical for natural language learning.
In Proc.of EACL, volume 6, pages 113?120.Frederick Mosteller and David L. Wallace.
1984.Applied Bayesian and Classical Inference: TheCase of the Federalist Papers.
Springer-Verlag.Daisuke Okanohara and Jun?ichi Tsujii.
2007.A discriminative language model with pseudo-negative samples.
In Proc.
of ACL, Prague,Czech Republic, June.Slav Petrov, Leon Barrett, Romain Thibaux, andDan Klein.
2006.
Learning accurate, compact,and interpretable tree annotation.
In Proc.
ofACL, Sydney, Australia, July.Xuan-Hieu Phan.
2006.
CRFTagger: CRF En-glish POS Tagger.
crftagger.sourceforge.net.Daniele Pighin and Alessandro Moschitti.
2009.Reverse engineering of tree kernel feature spaces.In Proc.
of EMNLP, pages 111?120, Singapore,August.Matt Post and Daniel Gildea.
2009.
Bayesianlearning of a tree substitution grammar.
InProc.
of ACL (short paper track), Suntec, Sin-gapore, August.Matt Post.
2011.
Judging grammaticality withtree substitution grammar derivations.
In Proc.of ACL, Portland, Oregon, USA, June.Dragomir R. Radev, Pradeep Muthukrishnan, andVahed Qazvinian.
2009.
The ACL anthologynetwork corpus.
In Proc.
of ACL Workshop onNatural Language Processing and InformationRetrieval for Digital Libraries, pages 54?61.Remko Scha.
1990.
Taaltheorie en taaltechnologie;competence en performance.
In R. de Kort andG.L.J.
Leerdam, editors, Computertoepassingenin de neerlandistiek, pages 7?22, Almere, theNetherlands.
De Vereniging.871Aliaksei Severyn and Alessandro Moschitti.
2010.Large-scale support vector learning with struc-tural kernels.
In Proc.
of ECML/PKDD, pages229?244.Libin Shen and Aravind K. Joshi.
2003.
An SVM-based voting algorithm with application to parsereranking.
In Proc.
of CoNLL, pages 9?16.Jun Suzuki, Hideki Isozaki, and Eisaku Maeda.2004.
Convolution kernels with feature selectionfor natural language processing tasks.
In Proc.of ACL, pages 119?126.Benjamin Swanson and Eugene Charniak.
2012.Native language detection with tree substitu-tion grammars.
In Proc.
of ACL (short papers),pages 193?197, Jeju Island, Korea, July.Joel Tetreault, Daniel Blanchard, Aoife Cahill, andMartin Chodorow.
2012.
Native tongues, lostand found: Resources and empirical evaluationsin native language identification.
In Proc.
ofCOLING, pages 2585?2602, Mumbai, India, De-cember.Laura Mayfield Tomokiyo and Rosie Jones.
2001.You?re not from ?round here, are you?
NaiveBayes detection of non-native utterances.
InProc.
of NAACL.Vladimir N. Vapnik.
1998.
Statistical LearningTheory.
John Wiley & Sons.Zhuang Wang, Koby Crammer, and SlobodanVucetic.
2010.
Multi-class pegasos on a bud-get.
In ICML, pages 1143?1150.Sze-Meng Jojo Wong and Mark Dras.
2010.
Parserfeatures for sentence grammaticality classifica-tion.
In Proceedings of the Australasian Lan-guage Technology Association Workshop, Mel-bourne, Australia, December.Sze-Meng Jojo Wong and Mark Dras.
2011.
Ex-ploiting parse structures for native languageidentification.
In Proc.
of EMNLP, pages 1600?1610, Edinburgh, Scotland, UK., July.Xiaofeng Yang, Jian Su, and Chew Lim Tan.2006.
Kernel-based pronoun resolution withstructured syntactic knowledge.
In Proc.
ofColing-ACL, pages 41?48.Dell Zhang and Wee Sun Lee.
2003.
Questionclassification using support vector machines.
InProceedings of the 26th annual internationalACM SIGIR conference on Research and de-velopment in informaion retrieval, SIGIR ?03,pages 26?32, New York, NY, USA.
ACM.872
