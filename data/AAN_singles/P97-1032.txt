Comparing a Linguistic and a Stochastic TaggerChr i s ter  Samuelsson  At ro  Vout i la inenLucent Technologies Research Unit  for Mult i l ingu~l Language Techno logyBell Laborator ies  P.O.
Box 4600 Mounta in  Ave, Room 2D-339 FIN-00014 University of Helsinki.Murray Hill, NJ 07974, USA F in landchrist er?research, bell-labs, tom Afro.
Vout ilainen?Helsinki.
FIAbst ractConcerning different approaches to auto-matic PoS tagging: EngCG-2, a constraint-based morphological tagger, is compared ina double-blind test with a state-of-the-artstatistical tagger on a common disambigua-tion task using a common tag set.
The ex-periments how that for the same amountof remaining ambiguity, the error rate ofthe statistical tagger is one order of mag-nitude greater than that of the rule-basedone.
The two related issues of primingeffects compromising the results and dis-agreement between human annotators arealso addressed.1 In t roduct ionThere are currently two main methods for auto-matic part-of-speech tagging.
The prevailing oneuses essentially statistical language models automat-ically derived from usually hand-annotated corpora.These corpus-based models can be represented e.g.as collocational matrices (Garside et al (eds.)
1987:Church 1988), Hidden Markov models (cf.
Cuttinget al 1992), local rules (e.g.
Hindle 1989) and neu-ral networks (e.g.
Schmid 1994).
Taggers using thesestatistical language models are generally reported toassign the correct and unique tag to 95-97% of wordsin running text.
using tag sets ranging from somedozens to about 130 tags.The less popular approach is based on hand-codedlinguistic rules.
Pioneering work was done in the1960"s (e.g.
Greene and Rubin 1971).
Recently, newinterest in the linguistic approach as been showne.g.
in the work of (Karlsson 1990: Voutilainen etal.
1992; Oflazer and Kuru6z 1994: Chanod andTapanainen 1995: Karlsson et al (eds.)
1995; Vouti-lainen 1995).
The first serious linguistic competitorto data-driven statistical taggers is the English Con-straint Grammar parser.
EngCG (cf.
Voutilainen etal.
1992; Karlsson et al (eds.)
1995).
The taggerconsists of the following sequentially applied mod-ules:1.
Tokenisation2.
Morphological analysis(a) Lexical component(b) Rule-based guesser for unknown words3.
Resolution of morphological mbiguitiesThe tagger uses a two-level morphological nal-yser with a large lexicon and a morphologicaldescription that introduces about 180 differentambiguity-forming morphological nalyses, as a re-sult of which each word gets 1.7-2.2 different analy-ses on an average.
Morphological analyses are as-signed to unknown words with an accurate rule-based 'guesser'.
The morphological disambiguatoruses constraint rules that discard illegitimate mor-phological analyses on the basis of local or globalcontext conditions.
The rules can be grouped asordered subgrammars: e.g.
heuristic subgrammar 2can be applied for resolving ambiguities left pendingby the more "careful' subgrammar 1.Older versions of EngCG (using about 1,150 con-straints) are reported (~butilainen etal.
1992; Vouti-lainen and HeikkiUi 1994; Tapanainen and Vouti-lainen 1994; Voutilainen 1995) to assign a correctanalysis to about 99.7% of all words while each wordin the output retains 1.04-1.09 alternative analyseson an average, i.e.
some of the ambiguities remait~unresolved.These results have been seriously questioned.
Onedoubt concerns the notion 'correct analysis".
Forexample Church (1992) argues that linguists whomanually perform the tagging task using the double-blind method disagree about the correct analysis inat least 3% of all words even after they have nego-tiated about the initial disagreements.
If this werethe case, reporting accuracies above this 97% "upperbound' would make no sense.However, Voutilainen and J~rvinen (1995) empir-ically show that an interjudge agreement virtuallyof 1()0% is possible, at least with the EngCG tag setif not with the original Brown Corpus tag set.
Thisconsistent applicability of the EngCG tag set is ex-plained by characterising it as grammatically ratherthan semantically motivated.246Another main reservation about the EngCG fig-ures is the suspicion that, perhaps partly due to thesomewhat underspecific nature of the EngCG tagset, it must be so easy to disambiguate hat also astatistical tagger using the EngCG tags would reachat least as good results.
This argument will be ex-amined in this paper.
It will be empirically shown(i) that the EngCG tag set is about as difficult for aprobabilistic tagger as more generally used tag setsand (ii) that the EngCG disambiguator has a clearlysmaller error rate than the probabilistic tagger whena similar (small) amount of ambiguity is permittedin the output.A state-of-the-art statistical tagger is trained ona corpus of over 350,000 words hand-annotated withEngCG tags.
then both taggers (a new versionknown as En~CG-21 with 3,600 constraints as fivesubgrammars-, and a statistical tagger) are appliedto the same held-out benchmark corpus of 55,000words, and their performances are compared.
Theresults disconfirm the suspected 'easiness' of theEngCG tag set: the statistical tagger's performancefigures are no better than is the case with betterknown tag sets.Two caveats are in order.
What we are not ad-dressing in this paper is the work load required formaking a rule-based or a data-driven tagger.
Therules in EngCG certainly took a considerable effortto write, and though at the present state of knowl-edge rules could be written and tested with less ef-fort, it may well be the case that a tagger with anaccuracy of 95-97% can be produced with less effortby using data-driven techniques.
3Another caveat is that EngCG alone does not re-solve all ambiguities, o it cannot be compared to atypical statistical tagger if full disambiguation is re-quired.
However, "~butilainen (1995) has shown thatEngCG combined with a syntactic parser producesmorphologically unambiguous output with an accu-racy of 99.3%, a figure clearly better than that of thestatistical tagger in the experiments below (however.the test data was not the same).Before examining the statistical tagger, two prac-tical points are addressed: the annotation of tile cor-pora used.
and the modification of the EngCG tagset for use in a statistical tagger.1An online version of EngCG-2 can be found at,ht tp://www.ling.helsinki.fi/"avoutila/engcg-2.ht ml.
:The first three subgrammars are generally highly re-liable and almost all of the total grammar developmenttime was spent on them: the last two contain ratherrough heuristic onstraints.3However, for an interesting experiment suggestingotherwise, see (Chanod and Tapanainen 1995).2 P reparat ion  of  Corpus  Resources2.1 Annotat ion  of training corpusThe stochastic tagger was trained on a sample of357,000 words from the Brown University Corpusof Present-Day English (Francis and Ku6era 1982)that was annotated using the EngCG tags.
The cor-pus was first analysed with the EngCG lexical anal-yser, and then it was fully disambiguated and, whennecessary, corrected by a human expert.
This an-notation took place a few years ago.
Since then, ithas been used in the development of new EngCGconstraints (the present version, EngCG-2, containsabout 3,600 constraints): new constraints were ap-plied to the training corpus, and whenever a readingmarked as correct was discarded, either the analysisin the corpus, or the constraint i self, was corrected.In this way, the tagging quality of the corpus wascontinuously improved.2.2 Annotat ion  of benchmark corpusOur comparisons use a held-out benchmark corpusof about 55,000 words of journalistic, scientific andmanual texts, i.e., no ,training effects are expectedfor either system.
The benchmark corpus was an-notated by first applying the preprocessor and mor-phological aaalyser, but not the morphological dis-ambiguator, to the text.
This morphologically am-biguous text was then independently and fully dis-ambiguated by two experts whose task was also todetect any errors potentially produced by the pre-viously applied components.
They worked indepen-dently, consulting written documentation f the tagset when necessary.
Then these manually disam-biguated versions were automatically compared witheach other.
At this stage, about 99.3% of all anal-yses were identical.
When the differences were col-lectiyely examined, virtually all were agreed to bedue to clerical mistakes.
Only in the analysis of 21words, different (meaning-level) interpretations per-sisted, and even here both judges agreed the ambigu-ity to be genuine.
One of these two corpus versionswas modified to represent he consensus, and this"consensus corpus' was used as a benchmark in theevaluations.As explained in Voutilainen and J/irvinen (1995).this high agreement rate is due to two main factors.Firstly, distinctions based on some kind of vague se-mantics are avoided, which is not always case withbetter known tag sets.
Secondly.
the adopted analy-sis of most of the constructions where humans tendto be uncertain is documented as a collection of tagapplication principles in the form of a grammar-inn's manual (for further details, cf.
Voutilainen andJ/irvinen 1995).Tile corpus-annotation procedure allows us t.o per-form a text-book statistical hypothesis test.
Lettile null hypothesis be that any two human eval-uators will necessarily disagree in at least 3% of247the cases.
Under this assumption, the probabilityof an observed disagreement of less than 2.88% isless than 5%.
This can be seen as follows: Forthe relative frequency of disagreement, fn, we havet - .
- - .
.
-that f .
is approximately --, N(p, ~/~) ,  where pis the actual disagreement probability and n is thenumber of trials, i.e., the corpus size.
This meansfn -P  v/- ff that P(( ~ < z) ~ ~(x) where ?b is thestandard normal distribution function.
This in turnmeans thatP ( f , < p + z P~ - p-----~) ) ,~ ~ ( z )Here n is 55,000 and ~(-1.645) = 0.05.
Under thenull hypothesis, p is at least 3% and thus:.
/O.O3.0.97P( f .
< o.o3- 1.64%/-g,o-g6 ) -= P(A  <__ 0.0288) < 0.05We can thus discard the null hypothesis at signifi-cance level 5% if the observed isagreement is lessthan 2.88%.
It was in fact 0.7% before error cor-.21)  rection, and virtually zero ( ~  after negotia-tion.
This means that we can actually discard thehypotheses that the human evaluators in averagedisagree in at least 0.8% of the cases before errorcorrection, and in at least 0.1% of the cases afternegotiations, at significance level 5%.2.3 Tag set conversionThe EugCG morphological analyser's output for-mally differs from most tagged corpora; consider thefollowing 5-ways ambiguous analysis of "'walk":walkwalk <SV> <SVO> V SUBJUNCTIVE VFINwalk <SV> <SVO> V IMP VFINwalk <SV> <SVG> V INFwalk <SV> <SVO> V PRES -SG3 VFINwalk N NOM SGStatistical taggers usually employ single tags toindicate analyses (e.g.
"'NN" for "'N NOM SG").Therefore a simple conversion program was made forproducing the following kind of output, where eachreading is represented as a single tag:walk V-SUBJUNCTIVE V-IMP V-INFV-PRES-BASE N-NOM-SGThe conversion program reduces the multipartEngCG tags into a set of 80 word tags and 17 punc-tuation tags (see Appendix) that retain the centrallinguistic characteristics of the original EngCG tagset.A reduced version of the benchmark corpus wasprepared with this conversion program for the sta-tistical tagger's use.
Also EngCG's output was con-verted into this format to enable direct comparisonwith the statistical tagger.8 The Stat ist ical  TaggerThe statistical tagger used in the experiments i aclassical trigram-based HMM decoder of the kinddescribed in e.g.
(Church 1988), (DeRose 1988) andnumerous other articles.
Following conventional no-tation, e.g.
(Rabiner 1989, pp.
272-274) and (Krennand Samuelsson 1996, pp.
42-46), the tagger recur-sively calculates the ~, 3, 7 and 6 variables for eachword string position t = 1 .
.
.
.
.
T and each possibles ta te  4 s i  : i = 1 , .
.
.
,n :a,(i) = P(W<,;S,  = si).
'3,(i) = P(W>,  IS, = s~)7t{i) ---&(i) =HereWW5tW>tSstP(W; & = si)P(&=s i IW)  = P(W)~,(i).
3,(i)r6y~o~,(i) .
3,(i)i=lmax P(S<t- l ,  S= = si; W<,)S<,_t= l/V1 = w lq , .
.
.
,  ~VT = Wkr- -  ~'VI = wk~ , .
.
.
, Wt  = wk ,"- l~Vt+l = wk,+ t, ?
?
.
,  I 'VT = Wkr-= S1 = si~ .
.
.
.
.
S t  = s i ,where St = si is the event of the tth word beingemitted from state si and Wt = wk, is the event ofthe tth word being the particular word w~, that wasactually observed in the word string.Note that for t = 1 .
.
.
.
.
T -1  ; i , j -  l .
.
.
.
.
nat+~(j)3,(0 = ~ 3,+1(j) "Pij .aj~,+~j= lwhere pij = P(St+I = sj I St = si) are the transi-tion probabilities, encoding the tag N-gram proba-bilities, anda jk  == P(Wt=wkIS ,=s j )  = P(Wt=w~l , \ ' t=z j )4The N- I  th-order  HMM corresponding to an N-gramtagger  is encoded as a first-order HMM,  where each s tatecor responds  to a sequence of ,V-I tags, i.e., for a t r igramtagger,  each s tate  corresponds to a tag pair.248are the lexical probabilities.
Here X, is the randomvariable of assigning a tag to the tth word and xj isthe last tag of the tag sequence ncoded as state sj.Note that si # sj need not imply zi # zj.More precisely, the tagger employs the converselexical probabilitiesP(Xt = zj I Wt = w,) ajka~ k = P(X, = zj) P(W, = wk)This results in slight variants a', fl', 7' and 6' of theoriginal quantities:~,( i )  6,(i) '= = I - \ [  P (Wu = o4(i ) 6;(i) .=1~,(i) r- H P(W~ =w~=) /3;(i) u=t+land thus Vi, t7~(i) = a;(i) ./3;(i) =ka ; ( i )  ./3;(i1i=1~,(i) .~,(i)and Vt~e, ( i )  ./3t(i)i=1= 7t(0argmax6;(i) = argmax6t(i)l< i<n l< i<nThe rationale behind this is to facilitate estimat-ing the model parameters from sparse data.
In moredetail, it is easy to estimate P(tag I word) for a pre-viously unseen word by backing off to statistics de-rived from words that end with the same sequenceof letters (or based on other surface cues), whereasdirectly estimating P(word I tag) is more difficult.This is particularly useful for languages with a richinflectional and derivational morphology, but alsofor English: for example, the suffix "-tion" is astrong indicator that the word in question is a noun;the suffix "-able" that it is an adjective.More technically, the lexicon is organised as areverse-suffix tree, and smoothing the probability es-timates is accomplished by blending the distributionat the current node of the tree with that of higher-level nodes, corresponding to (shorter) suffixes of thecurrent word (suffix).
The scheme also incorporatesprobability distributions for the set of capitalizedwords, the set of all-caps words and the set of in-frequent words, all of which are used to improve theestimates for unknown words.
Employing a smallamount of back-off smoothing also for the knownwords is useful to reduce lexical tag omissions.
Em-pirically, looking two branching points up the treefor known words, and all the way up to the rootfor unknown words, proved optimal.
The methodfor blending the distributions applies equally well tosmoothing the transition probabilities pij, i.e., thetag N-gram probabilities, and both the scheme andits application to these two tasks are described in de-tail in (Samuelsson 1996), where it was also shownto compare favourably to (deleted) interpolation, see(Jelinek and Mercer 1980), even when the back-offweights of the latter were optimal.The 6 variables enable finding the most probablestate sequence under the HMM, from which the mostlikely assignment of tags to words can be directly es-tablished.
This is the normal modus operandi of anHMM decoder.
Using the 7 variables, we can calcu-late the probability of being in state si at string po-sition t, and thus having emitted wk, from this state,conditional on the entire word string.
By summingover all states that would assign the same tag to thisword, the individual probability of each tag being as-signed to any particular input word, conditional onthe entire word string, can be calculated:P(X, = z i lW)  == Z P(S,=sj t W) = E 7,(J)8 j : r j=r  i $ j : r j  =~'=This allows retaining multiple tags for each word bysimply discarding only low-probability tags; thosewhose probabilities are below some threshold value.Of course, the most probable tag is never discarded,even if its probability happens to be less than thethreshold value.
By varying the threshold, we canperform a recall-precision, or error-rate-ambiguity,tradeoff.
A similar strategy is adopted in (de Mar-cken 1990).4 Exper imentsThe statistical tagger was trained on 357,000 wordsfrom the Brown corpus (Francis and Ku~era 1982),reannotated using the EngCG annotation scheme(see above).
In a first set of experiments, a 35,000word subset of this corpus was set aside and used toevaluate the tagger's performance when trained onsuccessively arger portions of the remaining 322,000words.
The learning curve, showing the error rate al-ter full disambiguation as a function of the amountof training data used, see Figure 1, has levelled off at322,000 words, indicating that little is to be gainedfrom further training.
We also note that the ab-solute value of the error rate is 3.51% - -  a typi-cal state-of-the-art figure.
Here, previously unseenwords contribute 1.08% to the total error rate, whilethe contribution from lexical tag omissions is 0.08%95% confidence intervals for the error rates wouldrange from + 0.30% for 30,000 words to + 0.20~c at322.000 words.The tagger was then trained on the entire setof 357,000 words and confronted with the separate55,000-word benchmark corpus, and run both in full2498v6.~ 5~ 4~ 3o 210Learning curve,I I I I I I0 50 I00 150 200 250 300Train ing set (kWords)Figure 1: Learning curve for the statistical taggeron the Brown corpus.Ambiguity(Tags/word)1.0001.0121.0251.0261.0351.0381.0481.0511.0591.0651.0701.0781.093Error rate (%)Statistical Tagger EngCG(~) (7)4.72 4.684.203.75(3.72)(3.48)3.40(3.20)3.14(2.99)2.87(2.80)2.692.550.430.290.150.120.10Table h Error-rate-ambiguity tradeoff or both tag-gets on the benchmark corpus.
Parenthesized num-bers are interpolated.and partial disambiguation mode.
Table 1 showsthe error rate as a function of remaining ambiguity(tags/word) both for the statistical tagger, and forthe EngCG-2 tagger.
The error rate for full disana-biguation using the 6 variables is 4.72% and usingthe 7 variables is 4.68%, both -4-0.18% with confi-dence degree 95%.
Note that the optimal tag se-quence obtained using the 7 variables need not equalthe optimal tag sequence obtained using the 6 vari-ables.
In fact, the former sequence may be assignedzero probability by the HMM, namely if one of itsstate transitions has zero probability.Previously unseen words account for 2.01%, andlexical tag omissions for 0.15% of the total error rate.These two error sources are together exactly 1.00%higher on the benchmark corpus than on the Browncorpus, and account for almost the entire differencein error rate.
They stem from using less completelexical information sources, and are most likely theeffect of a larger vocabulary overlap between the testand training portions of the Brown corpus than be-tween the Brown and benchmark corpora.The ratio between the error rates of the two tag-gets with the same amount of remaining ambiguityranges from 8.6 at 1.026 tags/word to 28,0 at 1.070tags/word.
The error rate of the statistical taggercan be further decreased, at the price of increasedremaining ambiguity, see Figure 2.
In the limit ofretaining all possible tags, the residual error rate isentirely due to lexical tag omissions, i.e., it is 0.15%,with in average 14.24 tags per word.
The reasonthat this figure is so high is that the unknown words,which comprise 10% of the corpus, are assigned allpossible tags as they are backed off all the way tothe root of the reverse-suffix tree.5v 432O0Error - ra te -ambigu i ty  trade-offi !
i l i l iI I I I i I r-2 4 6 8 i0 12 14Remain ing ambigui ty  (Tags/Word)Figure 2: Error-rate-ambiguity tradeoff or the sta-tistical tagger on the benchmark corpus.5 D iscuss ionRecently voiced scepticisms concerning the superiorEngCG tagging results boil down to the following:?
The reported results are due to the simplicityof the tag set employed by the EngCG system.?
The reported results are an effect of tradinghigh ambiguity resolution for lower error rate.?
The results are an effect of so-called primingof the huraan annotators when preparing thetest corpora, compromising the integrity of theexperimental evaluations.In the current article, these points of criticismwere investigated.
A state-of-the-art statisticaltagger, capable of performing error-rate-ambiguitytradeoff, was trained on a 357,000-word portion ofthe Brown corpus reannotated with the EngCG tagset, and both taggers were evaluated using a sep-arate 55,000-word benchmark corpus new to both250systems.
This benchmark corpus was independentlydisambiguated by two linguists, without access tothe results of the automatic taggers.
The initialdifferences between the linguists' outputs (0.7% ofall words) were jointly examined by the linguists;practically all of them turned out to be clerical er-rors (rather than the product of genuine differenceof opinion).In the experiments, the performance of theEngCG-2 tagger was radically better than that ofthe statistical tagger: at ambiguity levels commonto both systems, the error rate of the statistical tag-ger was 8.6 to 28 times higher than that of EngCG-2.
We conclude that neither the tag set used byEngCG-2, nor the error-rate-ambiguity tradeoff, norany priming effects can possibly explain the observeddifference in performance.Instead we must conclude that the lexical and con-textual information sources at the disposal of theEngCG system are superior.
Investigating this em-pirically by granting the statistical tagger access tothe same information sources as those available inthe Constraint Grammar framework constitutes fu-ture work.AcknowledgementsThough Voutilainen is the main author of theEngCG-2 tagger, the development of the systemhas benefited from several other contributions too.Fred Karlsson proposed the Constraint Grammarframework in the late 1980s.
Juha Heikkil?
andTimo J~irvinen contributed with their work on En-glish morphology and lexicon.
Kimmo Koskenniemiwrote the software for morphological nalysis.
PasiTapanainen has written various implementations ofthe CG parser, including the recent CG-2 parser(Tapanainen 1996).The quality of the investigation and presentationwas boosted by a number of suggestions to improve-ments and (often sceptical) comments from numer-ous ACL reviewers and UPenn associates, in partic-ular from Mark Liberman.ReferencesJ-P Chanod and P. Tapanainen.
1995.
TaggingFrench: comparing a statistical and a constraint-based method.
In Procs.
7th Conference of theEuropean Chapter of the Association for Compu-tational Lingaistics, pp.
149-157, ACL, 1995.K.
W. Church.
1988.
"'A Stochastic Parts Programand Noun Phrase Parser for Unrestricted Text.
".In Procs.
2nd Conference on Applied Natural Lan-guage Processing, pp.
136-143, ACL, 1988.K.
Church.
1992.
Current Practice in Part ofSpeech Tagging and Suggestions for the Future.
inSimmons (ed.
), Sbornik praci: In Honor of HenryKu6era.
Michigan Slavic Studies, 1992.D.
Cutting, J. Kupiec, J. Pedersen and P. Sibun.1992.
A Practical Part-of-Speech Tagger.
InProcs.
3rd Conference on Applied Natural Lan-guage Processing, pp.
133-140, ACL, 1992.S.
J. DeRose.
1988.
"Grammatical CategoryDisambiguation by Statistical Optimization".
InComputational Linguistics 14(1), pp.
31-39, ACL,1988.N.
W. Francis and H. Ku~era.
1982.
Fre-quency Analysis of English Usage, Houghton Mif-flin, Boston, 1982.R.
Garside, G. Leech and G. Sampson (eds.).
1987.The Computational Analysis of English.
Londonand New York: Longman, 1987.B.
Greene and G. Rubin.
1971.
Automatic gram-matical tagging of English.
Brown University,Providence, 1971.D.
Hindle.
1989.
Acquiring disambiguation rulesfrom text.
In Procs.
27th Annual Meeting of theAssociation for Computational Linguistics, pp.118-125, ACL, 1989.F.
Jelinek and R. L. Mercer.
1980.
"InterpolatedEstimation of Markov Source Paramenters fromSparse Data".
Pattern Recognition in Practice:381-397.
North Holland, 1980.F.
Karlsson.
1990.
Constraint Grammar as aFramework for Parsing Running Text.
In Procs.CoLing'90.
In Procs.
14th International Confer-ence on Computational Linguistics, ICCL, 1990.F.
Karlsson, A. Voutilainen, J. Heikkilii and A.Anttila (eds.).
1995.
Constraint Grammar.
ALanguage-Independent System for Parsing Unre-stricted Tezt.
Berlin and New York: Mouton deGruyter, 1995.B.
Krenn and C. Samuelsson.
The Linguist'sGuide to Statistics.
Version of April 23, 1996.http ://coli.
uni-sb, de/~christ er.C.
G. de Marcken.
1990.
"Parsing the LOB Cor-pus".
In Procs.
28th Annual Meeting of the As-sociation for Computational Linguistics, pp.
243-251, ACL, 1990.K.
Oflazer and I. KuruSz.
1994.
Tagging andmorphological disambiguation f Turkish text.
InProcs.
4th Conference on Applied Natural La1~-guage Processing.
ACL.
1994.L.
R. Rabiner.
1989.
"A Tutorial on Hid-den Markov Models and Selected Applicationsin Speech Recognition".
In Readings in SpeechRecognition, pp.
267-296.
Alex Waibel and Kai-Fu Lee (eds), Morgan I<aufmann, 1990.G.
Sampson.
1995.
English for the Computer, Ox-ford University Press.
1995.251C.
Samuelsson.
1996.
"Handling Sparse Data bySuccessive Abstraction".
In Procs.
16th Interna-tional Conference on Computational Linguistics,pp.
895-900, ICCL, 1996.H.
Schmid.
1994.
Part-of-speech tagging with neu-ral networks.
In Procs.
15th International Confer-ence on Computational Linguistics, pp.
172-176,ICCL, 1994.P.
Tapanainen.
1996.
The Constraint GrammarParser CG-2.
Publ.
27, Dept.
General Linguistics,University of Helsinki, 1996.P.
Tapanainen and A. Voutilainen.
1994.
Taggingaccurately - don't guess if you know.
In Procs.
4thConference on Applied Natural Language Process-ing, ACL, 1994.A.
Voutilainen.
1995.
"A syntax-based part ofspeech analyser".
In Procs.
7th Conference of theEuropean Chapter of the Association for Compu-tational Linguistics, pp.
157-164, ACL, 1995.A.
Voutilainen and J. Heikkil~.
1994.
An Englishconstraint grammar (EngCG): a surface-syntacticparser of English.
In Fries, Tottie and Schneider(eds.
), Creating and using English language cor-pora, Rodopi, 1994.A.
Voutilainen, J. Heikkil~ and A. Anttila.
1992.Constraint Grammar of English.
A Performance-Oriented Introduction.
Publ.
21, Dept.
GeneralLinguistics, University of Helsinki, 1992.A.
Voutilainen and T. J~irvinen.
"Specifying ashal-low grammatical representation for parsing pur-poses".
In Procs.
7th Conference of the Euro-pean Chapter of the Association for Computa-tional Linguistics, pp.
210-214, ACL, 1995.252Append ix :  Reduced  EngCG tag  setINGPunctuation tags: BE-IMP N-GEN-SG/PL'~colon BE-INF N-GEN-PL@comma BE-ING N-GEN-SG:~d~h BE-PAST-BASE N-NOM-SG/PL~dotdot BE-PAST-WAS N-NOM-PL@dquote BE-PRES-AM N-NOM-SG@exclamation BE-PRES-ARE NEG@fuUstop BE-PRES-IS NUM-CARD@lparen  BE-SUBJUNCTIVE NUM-FRA-PL@rparen CC NUM-FRA-SG@rparen CCX NUM-ORD@rparen CS PREP@rparen DET-SG/PL PRON@lquote DET-SG PRON-ACC@rquote DET-WH PRON-CMP@slash DO-EN PRON-DEM-PL@newlines DO-IMP PRON-DEM-SG@question DO-INF PRON-GEN@semicolon DO-ING PRON-INTERRWord tags: DO-PAST PRON-NOM-SG/PLA-ABS DO-PRES-BASE PRON-NOM-PLA-CMP DO-PRES-SG3 PRON-NOM-SGA-SUP DO-SUBJUNCTIVE PRON-RELABBR-GEN-SG/PL EN PRON-SUPABBR-GEN-PL HAVE-EN PRON-WHABBR-GEN-SG HAVE-IMP V-AUXMODABBR-NOM-SG/PL HAVE-INF V-IMPABBR-NOM-PL HAVE-ING V-INFABBR-NOM-SG HAVE-PAST V-PASTADV-ABS HAVE-PRES-BASE V-PRES-BASEADV-CMP HAVE-PRES-SG3 V-PRES-SG1ADV-SUP HAVE-SUBJUNCTIVE V-PRES-SG2ADV-WH I V-PRES-SG3BE-EN INFMARK V-SUBJUNCTIVE253
