Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 44?52,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsDiscrepancy Between Automatic and Manual Evaluation of SummariesShamima Mithun, Leila Kosseim, and Prasad PereraConcordia UniversityDepartment of Computer Science and Software EngineeringMontreal, Quebec, Canada{s mithun, kosseim, p perer}@encs.concordia.caAbstractToday, automatic evaluation metrics such asROUGE have become the de-facto mode ofevaluating an automatic summarization sys-tem.
However, based on the DUC and the TACevaluation results, (Conroy and Schlesinger,2008; Dang and Owczarzak, 2008) showedthat the performance gap between human-generated summaries and system-generatedsummaries is clearly visible in manual eval-uations but is often not reflected in automatedevaluations using ROUGE scores.
In this pa-per, we present our own experiments in com-paring the results of manual evaluations ver-sus automatic evaluations using our own textsummarizer: BlogSum.
We have evaluatedBlogSum-generated summary content usingROUGE and compared the results with theoriginal candidate list (OList).
The t-test re-sults showed that there is no significant differ-ence between BlogSum-generated summariesand OList summaries.
However, two man-ual evaluations for content using two differentdatasets show that BlogSum performed signif-icantly better than OList.
A manual evaluationof summary coherence also shows that Blog-Sum performs significantly better than OList.These results agree with previous work andshow the need for a better automated sum-mary evaluation metric rather than the stan-dard ROUGE metric.1 IntroductionToday, any NLP task must be accompanied by awell-accepted evaluation scheme.
This is why, forthe last 15 years, to evaluate automated summariza-tion systems, sets of evaluation data (corpora, topics,.
.
. )
and baselines have been established in text sum-marization competitions such as TREC1, DUC2, andTAC3.
Although evaluation is essential to verify thequality of a summary or to compare different sum-marization approaches, the evaluation criteria usedare by no means universally accepted (Das and Mar-tins, 2007).
Summary evaluation is a difficult taskbecause no ideal summary is available for a set ofinput documents.
In addition, it is also difficult tocompare different summaries and establish a base-line because of the absence of standard human orautomatic summary evaluation metrics.
On the otherhand, manual evaluation is very expensive.
Accord-ing to (Lin, 2004), large scale manual evaluations ofall participants?
summaries in the DUC 2003 confer-ence would require over 3000 hours of human effortsto evaluate summary content and linguistic qualities.The goal of this paper is to show that the literatureand our own work empirically point out the need fora better automated summary evaluation metric ratherthan the standard ROUGE metric4 (Lin, 2004).2 Current Evaluation SchemesThe available summary evaluation techniques canbe divided into two categories: manual and auto-matic.
To do a manual evaluation, human experts as-sess different qualities of the system generated sum-maries.
On the other hand, for an automatic eval-1Text REtrieval Conference: http://trec.nist.gov2Document Understanding Conference: http://duc.nist.gov3Text Analysis Conference: http://www.nist.gov/tac4http://berouge.com/default.aspx44uation, tools are used to compare the system gen-erated summaries with human generated gold stan-dard summaries or reference summaries.
Althoughthey are faster to perform and result in consistentevaluations, automatic evaluations can only addresssuperficial concepts such as n-grams matching, be-cause many required qualities such as coherence andgrammaticality cannot be measured automatically.As a result, human judges are often called for toevaluate or cross check the quality of the summaries,but in many cases human judges have different opin-ions.
Hence inter-annotator agreement is often com-puted as well.The quality of a summary is assessed mostly on itscontent and linguistic quality (Louis and Nenkova,2008).
Content evaluation of a query-based sum-mary is performed based on the relevance with thetopic and the question and the inclusion of importantcontents from the input documents.
The linguisticquality of a summary is evaluated manually based onhow it structures and presents the contents.
Mainly,subjective evaluation is done to assess the linguis-tic quality of an automatically generated summary.Grammaticality, non-redundancy, referential clarity,focus, structure and coherence are commonly usedfactors considered to evaluate the linguistic quality.A study by (Das and Martins, 2007) shows that eval-uating the content of a summary is more difficultcompared to evaluating its linguistic quality.There exist different measures to evaluate anoutput summary.
The most commonly used metricsare recall, precision, F-measure, Pyramid score,and ROUGE/BE.Automatic versus Manual EvaluationBased on an analysis of the 2005-2007 DUC data,(Conroy and Schlesinger, 2008) showed that theROUGE evaluation and a human evaluation can sig-nificantly vary due to the fact that ROUGE ignoreslinguistic quality of summaries, which has a huge in-fluence in human evaluation.
(Dang and Owczarzak,2008) also pointed out that automatic evaluation israther different than the one based on manual assess-ment.
They explained this the following way: ?auto-matic metrics, based on string matching, are unableto appreciate a summary that uses different phrasesthan the reference text, even if such a summary isperfectly fine by human standards?.To evaluate both opinionated and news articlebased summarization approaches, previously men-tioned evaluation metrics such as ROUGE or Pyra-mid are used.
Shared evaluation tasks such asDUC and TAC competitions also use these methodsto evaluate participants?
summary.
Table 1 showsTable 1: Human and Automatic System Performance atVarious TAC CompetitionsModel (Human) AutomaticPyr.
Resp.
Pyr.
Resp.2010 Upd.
0.78 4.76 0.30 2.562009 Upd.
0.68 8.83 0.26 4.142008 Upd.
0.66 4.62 0.26 2.322008 Opi.
0.44 Unk.
0.10 1.31the evaluation results of automatic systems?
averageperformance at the TAC 2008 to 2010 conferencesusing the pyramid score (Pyr.)
and responsiveness(Resp.).
In this evaluation, the pyramid score wasused to calculate the content relevance and the re-sponsiveness of a summary was used to judge theoverall quality or usefulness of the summary, con-sidering both the information content and linguisticquality.
These two criteria were evaluated manually.The pyramid score was calculated out of 1 and theresponsiveness measures were calculated on a scaleof 1 to 5 (1, being the worst).
However, in 2009,responsiveness was calculated on a scale of 10.
Ta-ble 1 also shows a comparison between automaticsystems and human participants (model).
In Table1, the first 3 rows show the evaluation results of theTAC Update Summarization (Upd.)
initial summarygeneration task (which were generated for news arti-cles) and the last row shows the evaluation results ofthe TAC 2008 Opinion Summarization track (Opi.
)where summaries were generated from blogs.
FromTable 1, we can see that in both criteria, automaticsystems are weaker than humans.
(Note that in thetable, Unk.
refers to unknown.
)Interestingly, in an automatic evaluation, often,not only is there no significant gap between modelsand systems, but in many cases, automatic systemsscored higher than some human models.Table 2 shows the performance of human (H.)and automated systems (S.) (participants) using au-tomated and manual evaluation in the TAC 2008 up-45Table 2: Automated vs. Manual Evaluation at TAC 2008Automated ManualR-2 R-SU4 Pyr.
Ling.
Resp.H.
Mean 0.12 0.15 0.66 4.79 4.62S.
Mean 0.08 0.12 0.26 2.33 2.32H.
Best 0.13 0.17 0.85 4.91 4.79S.
Best 0.11 0.14 0.36 3.25 2.29date summarization track.
In the table, R-2 and R-SU4 refer to ROUGE-2 and ROUGE-SU4 and Pyr.,Ling., and Resp.
refer to Pyramid, linguistic, andresponsiveness, respectively.
A t-test of statisticalsignificance applied to the data in Table 2 shows thatthere is no significant difference between human andparticipants in automated evaluation but that there isa significant performance difference between themin the manual evaluation.These findings indicate that ROUGE is not themost effective tool to evaluate summaries.
Our ownexperiments described below arrive at the same con-clusion.3 BlogSumWe have designed an extractive query-based summ-rizer called BlogSum.
In BlogSum, we have devel-oped our own sentence extractor to retrieve the ini-tial list of candidate sentences (we called it OList)based on question similarity, topic similarity, andsubjectivity scores.
Given a set of initial candidatesentences, BlogSum generates summaries using dis-course relations within a schema-based framework.Details of BlogSum is outside the scope of this pa-per.
For details, please see (Mithun and Kosseim,2011).4 Evaluation of BlogSumBlogSum-generated summaries have been evaluatedfor content and linguistic quality, specifically dis-course coherence.
The evaluation of the content wasdone both automatically and manually and the evalu-ation of the coherence was done manually.
Our eval-uation results also reflect the discrepancy betweenautomatic and manual evaluation schemes of sum-maries described above.In our evaluation, BlogSum-generated summarieswere compared with the original candidate list gen-erated by our approach without the discourse re-ordering (OList).
However, we have validated ouroriginal candidate list with a publicly available sen-tence ranker.
Specifically, we have conducted an ex-periment to verify whether MEAD-generated sum-maries (Radev et al, 2004), a widely used publiclyavailable summarizer5, were better than our candi-date list (OList).
In this evaluation, we have gener-ated summaries using MEAD with centroid, querytitle, and query narrative features.
In MEAD, querytitle and query narrative features are implementedusing cosine similarity based on the tf-idf value.
Inthis evaluation, we used the TAC 2008 opinion sum-marization dataset (described later in this section)and summaries were evaluated using the ROUGE-2and ROUGE-SU4 scores.
Table 3 shows the resultsof the automatic evaluation using ROUGE based onsummary content.Table 3: Automatic Evaluation of MEAD based on Sum-mary Content on TAC 2008System R-2 (F) R-SU4 (F)MEAD 0.0407 0.0642Average 0.0690 0.0860OList 0.1020 0.1070Table 3 shows that MEAD-generated summariesachieved weaker ROUGE scores compared to thatof our candidate list (OList).
The table also showsthat MEAD performs weaker than the average per-formance of the participants of TAC 2008 (Average).We suspect that these poor results are due to sev-eral reasons.
First, in MEAD, we cannot use opin-ionated terms or polarity information as a sentenceselection feature.
On the other hand, most of thesummarizers, which deal with opinionated texts, useopinionated terms and polarity information for thispurpose.
In addition, in this experiment, for some ofthe TAC 2008 questions, MEAD was unable to cre-ate any summary.
This evaluation results promptedus to develop our own candidate sentence selector.5MEAD: http://www.summarization.com/mead464.1 Evaluation of Content4.1.1 Automatic Evaluation of ContentFirst, we have automatically evaluated the sum-maries generated by our approach for content.
Asa baseline, we used the original ranked list of can-didate sentences (OList), and compared them to thefinal summaries (BlogSum).
We have used the datafrom the TAC 2008 opinion summarization track forthe evaluation.The dataset consists of 50 questions on 28 topics;on each topic one or two questions are asked and 9 to39 relevant documents are given.
For each question,one summary was generated by OList and one byBlogSum and the maximum summary length was re-stricted to 250 words.
This length was chosen causein the DUC conference from 2005 to 2007, in themain summarization task, the summary length was250 words.
In addition, (Conroy and Schlesinger,2008) also created summaries of length 250 wordsin their participation in the TAC 2008 opinion sum-marization task and performed well.
(Conroy andSchlesinger, 2008) also pointed out that if the sum-maries were too long this adversely affected theirscores.
Moreover, according to the same authorsshorter summaries are easier to read.
Based on theseobservations, we have restricted the maximum sum-mary length to 250 words.
However, in the TAC2008 opinion summarization track, the allowablesummary length is very long (the number of non-whitespace characters in the summary must not ex-ceed 7000 times the number of questions for the tar-get of the summary).
In this experiment, we usedthe ROUGE metric using answer nuggets (providedby TAC), which had been created to evaluate par-ticipants?
summaries at TAC, as gold standard sum-maries.
F-scores are calculated for BlogSum andOList using ROUGE-2 and ROUGE-SU4.
In thisexperiment, ROUGE scores are also calculated forall 36 submissions in the TAC 2008 opinion sum-marization track.The evaluation results are shown in Table 4.
Notethat in the table Rank refers to the rank of the systemcompared to the other 36 systems.Table 4 shows that BlogSum achieved a better F-Measure (F) for ROUGE-2 (R-2) and ROUGE-SU4(R-SU4) compared to OList.
From the results, wecan see that BlogSum gained 18% and 16% in F-Table 4: Automatic Evaluation of BlogSum based onSummary Content on TAC 2008System R-2 (F) R-SU4 (F) RankBest 0.130 0.139 1BlogSum 0.125 0.128 3OList 0.102 0.107 10Average 0.069 0.086 N/AMeasure over OList using ROUGE-2 and ROUGE-SU4, respectively.Compared to the other systems that participated tothe TAC 2008 opinion summarization track, Blog-Sum performed very competitively; it ranked thirdand its F-Measure score difference from the best sys-tem is very small.
Both BlogSum and OList per-formed better than the average systems.However, a further analysis of the results ofTable 4 shows that there is no significant differ-ence between BlogSum-generated summaries andOList summaries using the t-test with a p-valueof 0.228 and 0.464 for ROUGE-2 and ROUGE-SU4, respectively.
This is inline with (Conroy andSchlesinger, 2008; Dang and Owczarzak, 2008) whoshowed that the performance gap between human-generated summaries and system-generated sum-maries at DUC and TAC is clearly visible in a man-ual evaluation, but is often not reflected in automatedevaluations using ROUGE scores.
Based on thesefindings, we suspected that there might be a perfor-mance difference between BlogSum-generated sum-maries and OList which is not reflected in ROUGEscores.
To verify our suspicion, we have conductedmanual evaluations for content.4.1.2 Manual Evaluation of Content using theBlog DatasetWe have conducted two manual evaluations usingtwo different datasets to better quantify BlogSum-generated summary content.Corpora and Experimental DesignIn the first evaluation, we have again used the TAC2008 opinion summarization track data.
For eachquestion, one summary was generated by OList andone by BlogSum and the maximum summary lengthwas again restricted to 250 words.
To evaluate47content, 3 participants manually rated 50 summariesfrom OList and 50 summaries from BlogSum usinga blind evaluation.
These summaries were ratedon a likert scale of 1 to 5 where 1 refers to ?verypoor?
and 5 refers to ?very good?.
Evaluators ratedeach summary with respect to the question forwhich it was generated and against the referencesummary.
In this experiment, we have used theanswer nuggets provided by TAC as the referencesummary, which had been created to evaluateparticipants?
summaries at TAC.
Annotators wereasked to evaluate summaries based on their contentwithout considering their linguistic qualities.ResultsIn this evaluation, we have calculated the averagescores of all 3 annotators?
ratings to a particularquestion to compute the score of BlogSum for aparticular question.
Table 5 shows the performancecomparison between BlogSum and OList.
The re-sults show that 58% of the time BlogSum summarieswere rated better than OList summaries which im-plies that 58% of the time, our approach has im-proved the question relevance compared to that ofthe original candidate list (OList).Table 5: Comparison of OList and BlogSum based on theManual Evaluation of Summary Content on TAC 2008Comparison %BlogSum Score > OList Score 58%BlogSum Score = OList Score 30%BlogSum Score < OList Score 12%Table 6 shows the performance of BlogSum ver-sus OList on each likert scale; where ?
shows thedifference in performance.
Table 6 demonstratesthat 52% of the times, BlogSum summaries wererated as ?very good?
or ?good?, 26% of the timesthey were rated as ?barely acceptable?
and 22% ofthe times they were rated as ?poor?
or ?very poor?.From Table 6, we can also see that BlogSum out-performed OList in the scale of ?very good?
and?good?
by 8% and 22%, respectively; and improvedthe performance in ?barely acceptable?, ?poor?, and?very poor?
categories by 12%, 8%, and 10%, re-spectively.In this evaluation, we have also calculatedTable 6: Manual Evaluation of BlogSum and OList basedon Summary Content on TAC 2008Category OList BlogSum ?Very Good 6% 14% 8%Good 16% 38% 22%Barely Acceptable 38% 26% -12%Poor 26% 18% -8%Very Poor 14% 4% -10%whether there is any performance gap between Blog-Sum and OList.
The t-test results show that in a two-tailed test, BlogSum performed significantly betterthan OList with a p-value of 0.00281.Whenever human performance is computed bymore than one person, it is important to computeinter-annotator agreement.
This ensures that theagreement between annotators did not simply occurby chance.
In this experiment, we have also cal-culated the inter-annotator agreement using Cohen?skappa coefficient to verify the annotation subjectiv-ity.
We have found that the average pair-wise inter-annotator agreement is moderate according to (Lan-dis and Koch, 1977) with the kappa-value of 0.58.4.1.3 Manual Evaluation of Content using theReview DatasetWe have conducted a second evaluation usingthe OpinRank dataset6 and (Jindal and Liu, 2008)?sdataset to evaluate BlogSum-generated summarycontent.Corpora and Experimental DesignIn this second evaluation, we have used a subset ofthe OpinRank dataset and (Jindal and Liu, 2008)?sdataset.
The OpinRank dataset contains reviews oncars and hotels collected from Tripadvisor (about259,000 reviews) and Edmunds (about 42,230 re-views).
The OpinRank dataset contains 42,230 re-views on cars for different model-years and 259,000reviews on different hotels in 10 different cities.
Forthis dataset, we created a total of 21 questions in-cluding 12 reason questions and 9 suggestions.
Foreach question, 1500 to 2500 reviews were provided6OpinRank Dataset: http://kavita-ganesan.com/entity-ranking-data48as input documents to create the summary.
(Jindal and Liu, 2008)?s dataset consists of 905comparison and 4985 non-comparison sentences.Four human annotators labeled these data manually.This dataset consists of reviews, forum, and news ar-ticles on different topics from different sources.
Wehave created 9 comparison questions for this dataset.For each question, 700 to 1900 reviews were pro-vided as input documents to create the summary.For each question, one summary was generatedby OList and one by BlogSum and the maximumsummary length was restricted to 250 words again.To evaluate question relevance, 3 participantsmanually rated 30 summaries from OList and 30summaries from BlogSum using a blind evaluation.These summaries were again rated on a likert scaleof 1 to 5.
Evaluators rated each summary withrespect to the question for which it was generated.ResultsTable 7 shows the performance comparison betweenBlogSum and OList.
The results show that 67% ofthe time BlogSum summaries were rated better thanOList summaries.
The table also shows that 30%of the time both approaches performed equally welland 3% of the time BlogSum was weaker than OList.Table 7: Comparison of OList and BlogSum based on theManual Evaluation of Summary Content on the ReviewDatasetComparison %BlogSum Score > OList Score 67%BlogSum Score = OList Score 30%BlogSum Score < OList Score 3%Table 8 demonstrates that 44% of the time Blog-Sum summaries were rated as ?very good?, 33% ofthe time rated as ?good?, 13% of the time they wererated as ?barely acceptable?
and 10% of the timethey were rated as ?poor?
or ?very poor?.
From Ta-ble 8, we can also see that BlogSum outperformedOList in the scale of ?very good?
by 34% and im-proved the performance in ?poor?
and ?very poor?categories by 23% and 10%, respectively.In this evaluation, we have also calculatedwhether there is any performance gap between Blog-Table 8: Manual Evaluation of BlogSum and OList basedon Summary Content on the Review DatasetCategory OList BlogSum ?Very Good 10% 44% 34%Good 37% 33% -4%Barely Acceptable 10% 13% 3%Poor 23% 0% -23%Very Poor 20% 10% -10%Sum and OList.
The t-test results show that in a two-tailed test, BlogSum performed significantly verybetter than OList with a p-value of 0.00236.
In ad-dition, the average pair-wise inter-annotator agree-ment is substantial according to (Landis and Koch,1977) with the kappa-value of 0.77.4.1.4 AnalysisIn both manual evaluation for content, BlogSumperformed significantly better than OList.
We cansee that even though there was not any signifi-cant performance gap between BlogSum and OList-generated summaries in the automatic evaluation ofSection 4.1.1, both manual evaluations show thatBlogSum and OList-generated summaries signifi-cantly vary at the content level.
For content, our re-sults support (Conroy and Schlesinger, 2008; Dangand Owczarzak, 2008)?s findings and points out fora better automated summary evaluation tool.4.2 Evaluation of Linguistic QualityOur next experiments were geared at evaluating thelinguistic quality of our summaries.4.2.1 Automatic Evaluation of LinguisticQualityTo test the linguistic qualities, we did not usean automatic evaluation because (Blair-Goldensohnand McKeown, 2006) found that the ordering of con-tent within the summaries is an aspect which is notevaluated by ROUGE.
Moreover, in the TAC 2008opinion summarization track, on each topic, answersnippets were provided which had been used as sum-marization content units (SCUs) in pyramid evalua-tion to evaluate TAC 2008 participants summariesbut no complete summaries is provided to which wecan compare BlogSum-generated summaries for co-49herence.
As a result, we only performed two man-ual evaluations using two different datasets again tosee whether BlogSum performs significantly betterthan OList for linguistic qualities too.
The pos-itive results of the next experiments will ensurethat BlogSum-generated summaries are really sig-nificantly better than OList summaries.4.2.2 Manual Evaluation of DiscourseCoherence using the Blog DatasetIn this evaluation, we have again used the TAC2008 opinion summarization track data.
For eachquestion, one summary was generated by OList andone by BlogSum and the maximum summary lengthwas restricted to 250 words again.
Four participantsmanually rated 50 summaries from OList and 50summaries from BlogSum for coherence.
Thesesummaries were again rated on a likert scale of 1 to5.ResultsTo compute the score of BlogSum for a particularquestion, we calculated the average scores of all an-notators?
ratings to that question.
Table 9 showsthe performance comparison between BlogSum andOList.
We can see that 52% of the time BlogSumTable 9: Comparison of OList and BlogSum based on theManual Evaluation of Discourse Coherence on TAC 2008Comparison %BlogSum Score > OList Score 52%BlogSum Score = OList Score 30%BlogSum Score < OList Score 18%summaries were rated better than OList summaries;30% of the time both performed equally well; and18% of the time BlogSum was weaker than OList.This means that 52% of the time, our approach hasimproved the coherence compared to that of theoriginal candidate list (OList).From Table 10, we can see that BlogSum outper-formed OList in the scale of ?very good?
and ?good?by 16% and 8%, respectively; and improved the per-formance in ?barely acceptable?
and ?poor?
cate-gories by 12% and 14%, respectively.The t-test results show that in a two-tailed test,BlogSum performed significantly better than OListTable 10: Manual Evaluation of BlogSum and OListbased on Discourse Coherence on TAC 2008Category OList BlogSum ?Very Good 8% 24% 16%Good 22% 30% 8%Barely Acceptable 36% 24% -12%Poor 22% 8% -14%Very Poor 12% 14% 2%with a p-value of 0.0223.
In addition, the averagepair-wise inter-annotator agreement is substantialaccording to with the kappa-value of 0.76.4.2.3 Manual Evaluation of DiscourseCoherence using the Review DatasetIn this evaluation, we have again used the Opin-Rank dataset and (Jindal and Liu, 2008)?s datasetto conduct the second evaluation of content.
Inthis evaluation, for each question, one summarywas generated by OList and one by BlogSum andthe maximum summary length was restricted to250 words.
Three participants manually rated 30summaries from OList and 30 summaries fromBlogSum for coherence.ResultsTo compute the score of BlogSum for a particularquestion, we calculated the average scores of all an-notators?
ratings to that question.
Table 11 showsthe performance comparison between BlogSum andOList.
We can see that 57% of the time BlogSumTable 11: Comparison of OList and BlogSum based onthe Manual Evaluation of Discourse Coherence on theReview DatasetComparison %BlogSum Score > OList Score 57%BlogSum Score = OList Score 20%BlogSum Score < OList Score 23%summaries were rated better than OList summaries;20% of the time both performed equally well; and23% of the time BlogSum was weaker than OList.50Table 12: Manual Evaluation of BlogSum and OListbased on Discourse Coherence on the Review DatasetCategory OList BlogSum ?Very Good 13% 23% 10%Good 27% 43% 16%Barely Acceptable 27% 17% -10%Poor 10% 10% 0%Very Poor 23% 7% -16%From Table 12, we can see that BlogSum outper-formed OList in the scale of ?very good?
and ?good?by 10% and 16%, respectively; and improved theperformance in ?barely acceptable?
and ?very poor?categories by 10% and 16%, respectively.We have also evaluated if the difference in perfor-mance between BlogSum and OList was statisticallysignificant.
The t-test results show that in a two-tailed test, BlogSum performed significantly betterthan OList with a p-value of 0.0371.In this experiment, we also calculated the inter-annotator agreement using Cohen?s kappa coeffi-cient.
We have found that the average pair-wiseinter-annotator agreement is substantial according to(Landis and Koch, 1977) with the kappa-value of0.74.The results of both manual evaluations of dis-course coherence also show that BlogSum performssignificantly better than OList.5 ConclusionBased on the DUC and TAC evaluation re-sults, (Conroy and Schlesinger, 2008; Dang andOwczarzak, 2008) showed that the performance gapbetween human-generated summaries and system-generated summaries, which is clearly visible in themanual evaluation, is often not reflected in auto-mated evaluations using ROUGE scores.
In ourcontent evaluation, we have used the automatedmeasure ROUGE (ROUGE-2 & ROUGE-SU4) andthe t-test results showed that there was no signif-icant difference between BlogSum-generated sum-maries and OList summaries with a p-value of 0.228and 0.464 for ROUGE-2 and ROUGE-SU4, respec-tively.
We suspected that there might be a perfor-mance difference between BlogSum-generated sum-maries and OList which is not reflected in ROUGEscores.
To verify our suspicion, we have conductedtwo manual evaluations for content using two dif-ferent datasets.
The t-test results for both datasetsshow that in a two-tailed test, BlogSum performedsignificantly better than OList with a p-value of0.00281 and 0.00236.
Manual evaluations of co-herence also show that BlogSum performs signifi-cantly better than OList.
Even though there was nosignificant performance gap between BlogSum andOList-generated summaries in the automatic evalua-tion, the manual evaluation results clearly show thatBlogSum-generated summaries are better than OListsignificantly.
Our results supports (Conroy andSchlesinger, 2008; Dang and Owczarzak, 2008)?sfindings and points out for a better automated sum-mary evaluation tool.AcknowledgementThe authors would like to thank the anonymous ref-erees for their valuable comments on a previous ver-sion of the paper.This work was financially supported by NSERC.ReferencesAnnie Louis and Ani Nenkova.
2008.
Automatic Sum-mary Evaluation without Human Models.
Proceedingsof the First Text Analysis Conference (TAC 2008),Gaithersburg, Maryland (USA), November.Chin-Y.
Lin.
2004.
ROUGE: A Package for Auto-matic Evaluation of Summaries.
Text SummarizationBranches Out: Proceedings of the ACL-04 Workshop,pages 74?81, Barcelona, Spain, July.Dipanjan Das and Andre F. T. Martins.
2007.
ASurvey on Automatic Text Summarization.
Availablefrom: http://www.cs.cmu.edu/?
nasmith/LS2/ das-martins.07.pdf, Literature Survey for the Languageand Statistics II course at Carnegie Mellon University.Dragomir Radev et al 2004.
MEAD -A Platform forMultidocument Multilingual Text Summarization.
Pro-ceedings of the the 4th International Conference onLanguage Resources and Evaluation, pages 1?4, Lis-bon, Portugal.Hoa T. Dang and Karolina Owczarzak.
2008.
Overviewof the TAC 2008 Update Summarization Task.
Pro-ceedings of the Text Analysis Conference, Gaithers-burg, Maryland (USA), November.John M. Conroy and Judith D. Schlesinger.
2008.CLASSY and TAC 2008 Metrics.
Proceedings of the51Text Analysis Conference, Gaithersburg, Maryland(USA), November.John M. Conroy and Hoa T. Dang.
2008.
Mind the Gap:Dangers of Divorcing Evaluations of Summary Con-tent from Linguistic Quality.
Proceedings of the the22nd International Conference on Computational Lin-guistics Coling, pages 145?152, Manchester, UK.Nitin Jindal and Bing Liu.
2006.
Identifying Compar-ative Sentences in Text Documents.
SIGIR?06 Pro-ceedings of the 29th Annual International ACM SIGIRConference on Research and Development in Informa-tion Retrieval, pages 244?251, Seattle, Washington,USA, August.Richard J. Landis and Gary G. Koch.
1977.
A One-wayComponents of Variance Model for Categorical Data.Journal of Biometrics, 33(1):671?679.Sasha Blair-Goldensohn and Kathleen McKeown.
2006.Integrating Rhetorical-Semantic Relation Models forQuery-Focused Summarization.
Proceedings of theDocument Understanding Conference (DUC) Work-shop at NAACL-HLT 2006, New York, USA, June.Shamima Mithun and Leila Kosseim.
2011.
DiscourseStructures to Reduce Discourse Incoherence in BlogSummarization.
Proceedings of Recent Advances inNatural Language Processing, pages 479?486, Hissar,Bulgaria, September.52
