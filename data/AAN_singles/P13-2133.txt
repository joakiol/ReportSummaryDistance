Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 759?764,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsContext Vector Disambiguation for Bilingual Lexicon Extraction fromComparable CorporaDhouha BouamorCEA, LIST, Vision andContent Engineering Laboratory,91191 Gif-sur-Yvette CEDEXFrancedhouha.bouamor@cea.frNasredine SemmarCEA, LIST, Vision and ContentEngineering Laboratory,91191 Gif-sur-YvetteCEDEX Francenasredine.semmar@cea.frPierre ZweigenbaumLIMSI-CNRS,F-91403 Orsay CEDEXFrancepz@limsi.frAbstractThis paper presents an approach that ex-tends the standard approach used for bilin-gual lexicon extraction from comparablecorpora.
We focus on the unresolved prob-lem of polysemous words revealed by thebilingual dictionary and introduce a use ofa Word Sense Disambiguation process thataims at improving the adequacy of con-text vectors.
On two specialized French-English comparable corpora, empirical ex-perimental results show that our methodimproves the results obtained by two state-of-the-art approaches.1 IntroductionOver the years, bilingual lexicon extraction fromcomparable corpora has attracted a wealth of re-search works (Fung, 1998; Rapp, 1995; Chiaoand Zweigenbaum, 2003).
The basic assumptionbehind most studies is a distributional hypothe-sis (Harris, 1954), which states that words with asimilar meaning are likely to appear in similar con-texts across languages.
The so-called standard ap-proach to bilingual lexicon extraction from com-parable corpora is based on the characterizationand comparison of context vectors of source andtarget words.
Each element in the context vectorof a source or target word represents its associa-tion with a word which occurs within a windowof N words.
To enable the comparison of sourceand target vectors, words in the source vectors aretranslated into the target language using an exist-ing bilingual dictionary.The core of the standard approach is the bilin-gual dictionary.
Its use is problematic when a wordhas several translations, whether they are synony-mous or polysemous.
For instance, the Frenchword action can be translated into English asshare, stock, lawsuit or deed.
In such cases, itis difficult to identify in flat resources like bilin-gual dictionaries which translations are most rel-evant.
The standard approach considers all avail-able translations and gives them the same impor-tance in the resulting translated context vectors in-dependently of the domain of interest and wordambiguity.
Thus, in the financial domain, trans-lating action into deed or lawsuit would introducenoise in context vectors.In this paper, we present a novel approach thataddresses the word polysemy problem neglectedin the standard approach.
We introduce a WordSense Disambiguation (WSD) process that iden-tifies the translations of polysemous words thatare more likely to give the best representation ofcontext vectors in the target language.
For thispurpose, we employ five WordNet-based semanticsimilarity and relatedness measures and use a datafusion method that merges the results obtained byeach measure.
We test our approach on two spe-cialized French-English comparable corpora (fi-nancial and medical) and report improved resultscompared to two state-of-the-art approaches.2 Related WorkMost previous works addressing the task of bilin-gual lexicon extraction from comparable corporaare based on the standard approach.
In order toimprove the results of this approach, recent re-searches based on the assumption that more thecontext vectors are representative, better is thebilingual lexicon extraction were conducted.
Inthese works, additional linguistic resources suchas specialized dictionaries (Chiao and Zweigen-baum, 2002) or transliterated words (Prochassonet al 2009) were combined with the bilingual dic-759tionary to translate context vectors.
Few workshave however focused on the ambiguity problemrevealed by the seed bilingual dictionary.
(Hazemand Morin, 2012) propose a method that filters theentries of the bilingual dictionary on the base ofa POS-Tagging and a domain relevance measurecriteria but no improvements have been demon-strated.
Gaussier et al(2004) attempted to solvethe problem of word ambiguities in the source andtarget languages.
They investigated a number oftechniques including canonical correlation analy-sis and multilingual probabilistic latent semanticanalysis.
The best results, with an improvement ofthe F-Measure (+0.02 at Top20) were reported fora mixed method.
Recently, (Morin and Prochas-son, 2011) proceed as the standard approach butweigh the different translations according to theirfrequency in the target corpus.
Here, we propose amethod that differs from Gaussier et al(2004) inthis way: If they focus on words ambiguities onsource and target languages, we thought that itwould be sufficient to disambiguate only trans-lated source context vectors.3 Context Vector Disambiguation3.1 Semantic similarity measuresA large number of WSD techniques were pro-posed in the literature.
The most widely used onesare those that compute semantic similarity1 withthe help of WordNet.
WordNet has been used inmany tasks relying on word-based similarity, in-cluding document (Hwang et al 2011) and im-age (Cho et al 2007; Choi et al 2012) retrievalsystems.
In this work, we use it to derive a se-mantic similarity between lexical units within thesame context vector.
To the best of our knowledge,this is the first application of WordNet to bilinguallexicon extraction from comparable corpora.Among semantic similarity measures usingWordNet, we distinguish: (1) measures based onpath length which simply counts the distance be-tween two words in the WordNet taxonomy, (2)measures relying on information content in whicha semantically annotated corpus is needed to com-pute frequencies of words to be compared and (3)the ones using gloss overlap which are designedto compute semantic relatedness.
In this work,we use five similarity measures and comparetheir performances.
These measures include three1For consiseness, we often use ?semantic similarity?
torefer collectively to both similarity and relatedness.path-based semantic similarity measures denotedPATH,WUP (Wu and Palmer, 1994) and LEA-COCK (Leacock and Chodorow, 1998).
PATH isa baseline that is equal to the inverse of the short-est path between two words.
WUP finds the depthof the least common subsumer of the words, andscales that by the sum of the depths of individualwords.
The depth of a word is its distance to theroot node.
LEACOCK finds the shortest path be-tween two words, and scales that by the maximumpath length found in the is?a hierarchy in whichthey occur.
Path length measures have the advan-tage of being independent of corpus statistics, andtherefor uninfluenced by sparse data.Since semantic relatedness is considered to bemore general than semantic similarity, we alsouse two relatedness measures: LESK (Banerjeeand Pedersen, 2002) and VECTOR (Patwardhan,2003).
LESK finds overlaps between the glossesof word pairs, as well as words?
hyponyms.
VEC-TOR creates a co-occurrence matrix for each glosstoken.
Each gloss is then represented as a vectorthat averages token co-occurrences.3.2 Disambiguation processOnce translated into the target language, the con-text vectors disambiguation process intervenes.This process operates locally on each context vec-tor and aims at finding the most prominent trans-lations of polysemous words.
For this purpose,we use monosemic words as a seed set of dis-ambiguated words to infer the polysemous word?stranslations senses.
We hypothesize that a word ismonosemic if it is associated to only one entry inthe bilingual dictionary.
We checked this assump-tion by probing monosemic entries of the bilingualdictionary against WordNet and found that 95% ofthe entries are monosemic in both resources.
Ac-cording to the above-described semantic similaritymeasures, a similarity value SimV alue is derivedbetween all the translations provided for each pol-ysemous word by the bilingual dictionary and allmonosemic words appearing within the same con-text vector.
In practice, since a word can belong tomore than one synset2 in WordNet, the semanticsimilarity between two words w1 and w2 is definedas the maximum of SimV alue between the synsetor the synsets that include the synsets(w1) and2a group of a synonymous words in WordNet760synsets(w2) according to the following equation:SemSim(w1, w2) = max{SimV alue(s1, s2);(s1, s2) ?
synsets(w1)?
synsets(w2)} (1)Then, to identify the most prominent transla-tions of each polysemous unit wp, an average sim-ilarity is computed for each translation wjp of wp:Ave Sim(wjp) =1NNXi=1SemSim(wi, wjp) (2)where N is the total number of monosemic wordsin each context vector and SemSim is the simi-larity value of wjp and the ith monosemic word.Hence, according to average similarity valuesAve Sim(wjp), we obtain for each polysemousword wp an ordered list of translations w1p .
.
.
wnp .4 Experiments and Results4.1 Resources and Experimental SetupWe conducted our experiments on two French-English comparable corpora specialized on thecorporate finance and the breast cancer sub-domains.
Both corpora were extracted fromWikipedia3.
We consider the domain topic inthe source language (for instance cancer du sein[breast cancer]) as a query to Wikipedia andextract all its sub-topics (i.e., sub-categories inWikipedia) to construct a domain-specific cate-gories tree.
Then we collected all articles belong-ing to one of these categories and used inter-language links to build the comparable corpus.Both corpora have been normalized through thefollowing linguistic preprocessing steps: tokeni-sation, part-of-speech tagging, lemmatisation andfunction words removal.
The resulting corpora4sizes as well as their polysemy rate PR are givenin Table 1.
The polysemy rate indicates how muchwords in the comparable corpora are associatedto more than one translation in the seed bilingualdictionary.
The dictionary consists of an in-housebilingual dictionary which contains about 120,000entries belonging to the general language with anaverage of 7 translations per entry.In bilingual terminology extraction from com-parable corpora, a reference list is required toevaluate the performance of the alignment.
Suchlists are often composed of about 100 single3http://dumps.wikimedia.org/4Comparable corpora will be shared publiclyCorpus French English PRCorporate finance 402.486 756.840 41%Breast cancer 396.524 524.805 47%Table 1: Comparable corpora sizes in term ofwords and polysemy rates (PR) associated to eachcorpusterms (Hazem and Morin, 2012; Chiao andZweigenbaum, 2002).
Here, we created two ref-erence lists5 for the corporate finance and thebreast cancer sub-domains.
The first list is com-posed of 125 single terms extracted from the glos-sary of bilingual micro-finance terms6.
The secondlist contains 79 terms extracted from the French-English MESH and the UMLS thesauri7.
Notethat reference terms pairs appear more than fivetimes in each part of both comparable corpora.Three other parameters need to be set up,namely the window size, the association measureand the similarity measure.
We followed (Larocheand Langlais, 2010) to define these parame-ters.
They carried out a complete study of theinfluence of these parameters on the bilingualalignment.
The context vectors were defined bycomputing the Discounted Log-Odds Ratio (equa-tion 3) between words occurring in the same con-text window of size 7.Odds-Ratiodisc = log (O11 +12 )(O22 + 12 )(O12 + 12 )(O21 + 12 )(3)where Oij are the cells of the 2 ?
2 contingencymatrix of a token s co-occurring with the term Swithin a given window size.
As similarity mea-sure, we chose to use the cosine measure.4.2 Results of bilingual lexicon extractionTo evaluate the performance of our approach, weused both the standard approach (SA) and the ap-proach proposed by (Morin and Prochasson, 2011)(henceforth MP11) as baselines.
The experimentswere performed with respect to the five semanticsimilarity measures described in section 3.1.
Eachmeasure provides, for each polysemous word, aranked list of translations.
A question that ariseshere is whether we should introduce only the top-ranked translation into the context vector or con-sider a larger number of translations, mainly whena translation list contains synonyms.
For this5Reference lists will be shared publicly6http://www.microfinance.lu/en/7http://www.nlm.nih.gov/761a)CorporateFinance Method WN-T1 WN-T2 WN-T3 WN-T4 WN-T5 WN-T6 WN-T7Standard Approach (SA) 0.172MP11 0.336SinglemeasureWUP 0.241 0.284 0.301 0.275 0.258 0.215 0.224PATH 0.250 0.284 0.301 0.284 0.258 0.215 0.215LEACOCK 0.250 0.293 0.301 0.275 0.275 0.241 0.232LESK 0.272 0.293 0.293 0.275 0.258 0.250 0.215VECTOR 0.267 0.310 0.284 0.284 0.232 0.232 0.232CONDORCETMerge 0.362 0.379 0.353 0.362 0.336 0.275 0.267b)BreastCancerMethod WN-T1 WN-T2 WN-T3 WN-T4 WN-T5 WN-T6 WN-T7Standard Approach (SA) 0.493MP11 0.553SinglemeasureWUP 0.481 0.566 0.566 0.542 0.554 0.542 0.554PATH 0.542 0.542 0.554 0.566 0.578 0.554 0.554LEACOCK 0.506 0.578 0.554 0.566 0.542 0.554 0.542LESK 0.469 0.542 0.542 0.590 0.554 0.554 0.542VECTOR 0.518 0.566 0.530 0.566 0.542 0.566 0.554CONDORCETMerge 0.566 0.614 0.600 0.590 0.600 0.578 0.578Table 2: F-Measure at Top20 for the two domains; MP11 = (Morin and Prochasson, 2011).
In eachcolumn, italics shows best single similarity measure, bold shows best result.
Underline shows best resultoverall.reason, we take into account in our experimentsdifferent numbers of translations, noted WN-Ti,ranging from the pivot translation (i = 1) to theseventh word in the translation list.
This choice ismotivated by the fact that words in both corporahave on average 7 translations in the bilingual dic-tionary.
Both baseline systems use all translationsassociated to each entry in the bilingual dictionary.The only difference is that in MP11 translationsare weighted according to their frequency in thetarget corpus.The results of different works focusing on bilin-gual lexicon extraction from comparable corporaare evaluated on the number of correct candidatesfound in the first N first candidates output by thealignment process (the TopN ).
Here, we use theTop20 F-measure as evaluation metric.
The resultsobtained for the corporate finance corpus are pre-sented in Table 2a.
The first notable observation isthat disambiguating context vectors using seman-tic similarity measures outperforms the SA.
Thehighest F-measure is reported by VECTOR.
Us-ing the top two words (WN-T2) in context vec-tors increases the F-measure from 0.172 to 0.310.However, compared to MP11, no improvementis achieved.
Concerning the breast cancer cor-pus, Table 2b shows improvements in most casesover both the SA and MP11.
The maximum F-measure was obtained by LESK when for eachpolysemous word up to four translations (WN-T4)are considered in context vectors.
This methodachieves an improvement of respectively +0.097and +0.037% over SA and MP11.Each of the tested 5 semantic similarity mea-sures provides a different view of how to rankthe translations of a given test word.
Combiningthe obtained ranked lists should reinforce the con-fidence in consensus translations, while decreas-ing the confidence in non-consensus translations.We have therefore tested their combination.
Forthis, we used a voting method, and chose one inthe Condorcet family the Condorcet data fusionmethod.
This method was widely used to combinedocument retrieval results from information re-trieval systems (Montague and Aslam, 2002; Nu-ray and Can, 2006).
It is a single-winner electionmethod that ranks the candidates in order of pref-erence.
It is a pairwise voting, i.e.
it compares ev-ery possible pair of candidates to decide the pref-erence of them.
A matrix can be used to presentthe competition process.
Every candidate appearsin the matrix as a row and a column as well.
Ifthere are m candidates, then we need m2 elementsin the matrix in total.
Initially 0 is written to all theelements.
If di is preferred to dj , then we add 1 tothe element at row i and column j (aij).
The pro-762cess is repeated until all the ballots are processed.For every element aij , if aij > m/2 , then dibeats dj ; if aij < m/2, then dj beats di; other-wise (aij = m/2), there is a draw between di anddj .
The total score of each candidate is quantifiedby summing the raw scores it obtains in all pair-wise competitions.
Finally the ranking is achiev-able based on the total scores calculated.Here, we view the ranking of the extraction re-sults from different similarity measures as a spe-cial instance of the voting problem where theTop20 extraction results correspond to candidatesand different semantic similarity measures are thevoters.
The combination method referred to asCONDORCETMerge outperformed all the others(see Tables 2a and 2b): (1) individual measures,(2) SA, and (3) MP11.
Even though the two cor-pora are fairly different (subject and polysemyrate), the optimal results are obtained when con-sidering up to two most similar translations in con-text vectors.
This behavior shows that the fusionmethod is robust to domain change.
The additionof supplementary translations, which are probablynoisy in the given domain, degrades the overall re-sults.
The F-measure gains with respect to SA are+0.207 for corporate finance and +0.121 for thebreast cancer corpus.
More interestingly, our ap-proach outperforms MP11, showing that the roleof disambiguation is more important than that offeature weighting.5 ConclusionWe presented in this paper a novel method thatextends the standard approach used for bilinguallexicon extraction.
This method disambiguatespolysemous words in context vectors by selectingonly the most relevant translations.
Five seman-tic similarity and relatedness measures were usedfor this purpose.
Experiments conducted on twospecialized comparable corpora indicate that thecombination of similarity metrics leads to a betterperformance than two state-of-the-art approaches.This shows that the ambiguity present in special-ized comparable corpora hampers bilingual lexi-con extraction, and that methods such as the oneintroduced here are needed.
The obtained resultsare very encouraging and can be improved in anumber of ways.
First, we plan to mine muchlarger specialized comparable corpora and focuson their quality (Li and Gaussier, 2010).
We alsoplan to test our method on bilingual lexicon extrac-tion from general-domain corpora, where ambigu-ity is generally higher and disambiguation meth-ods should be all the more needed.ReferencesSatanjeev Banerjee and Ted Pedersen.
2002.
Anadapted lesk algorithm for word sense disambigua-tion using wordnet.
In Proceedings of the Third In-ternational Conference on Computational Linguis-tics and Intelligent Text Processing, CICLing ?02,pages 136?145, London, UK, UK.
Springer-Verlag.Yun-Chuang Chiao and Pierre Zweigenbaum.
2002.Looking for candidate translational equivalents inspecialized, comparable corpora.
In Proceedings ofthe 19th international conference on Computationallinguistics - Volume 2, COLING ?02, pages 1?5.
As-sociation for Computational Linguistics.Yun-Chuang Chiao and Pierre Zweigenbaum.
2003.The effect of a general lexicon in corpus-based iden-tification of french-english medical word transla-tions.
In Proceedings Medical Informatics Europe,volume 95 of Studies in Health Technology and In-formatics, pages 397?402, Amsterdam.Miyoung Cho, Chang Choi, Hanil Kim, Jungpil Shin,and PanKoo Kim.
2007.
Efficient image retrievalusing conceptualization of annotated images.
Lec-ture Notes in Computer Science, pages 426?433.Springer.Dongjin Choi, Jungin Kim, Hayoung Kim, Myungg-won Hwang, and Pankoo Kim.
2012.
A method forenhancing image retrieval based on annotation usingmodified wup similarity in wordnet.
In Proceed-ings of the 11th WSEAS international conferenceon Artificial Intelligence, Knowledge Engineeringand Data Bases, AIKED?12, pages 83?87, StevensPoint, Wisconsin, USA.
World Scientific and Engi-neering Academy and Society (WSEAS).Pascale Fung.
1998.
A statistical view on bilinguallexicon extraction: From parallel corpora to non-parallel corpora.
In Parallel Text Processing, pages1?17.
Springer.E?ric Gaussier, Jean-Michel Renders, Irina Matveeva,Cyril Goutte, and Herve?
De?jean.
2004.
A geometricview on bilingual lexicon extraction from compara-ble corpora.
In ACL, pages 526?533.Z.S.
Harris.
1954.
Distributional structure.
Word.Amir Hazem and Emmanuel Morin.
2012.
Adap-tive dictionary for bilingual lexicon extraction fromcomparable corpora.
In Proceedings, 8th interna-tional conference on Language Resources and Eval-uation (LREC), Istanbul, Turkey, May.Myunggwon Hwang, Chang Choi, and Pankoo Kim.2011.
Automatic enrichment of semantic relation763network and its application to word sense disam-biguation.
IEEE Transactions on Knowledge andData Engineering, 23:845?858.Audrey Laroche and Philippe Langlais.
2010.
Re-visiting context-based projection methods for term-translation spotting in comparable corpora.
In 23rdInternational Conference on Computational Lin-guistics (Coling 2010), pages 617?625, Beijing,China, Aug.Claudia Leacock and Martin Chodorow, 1998.
Com-bining local context and WordNet similarity for wordsense identification, pages 305?332.
In C.
Fellbaum(Ed.
), MIT Press.Bo Li and E?ric Gaussier.
2010.
Improving corpuscomparability for bilingual lexicon extraction fromcomparable corpora.
In 23rd International Confer-ence on Computational Linguistics (Coling 2010),Beijing, China, Aug.Mark Montague and Javed A. Aslam.
2002.
Con-dorcet fusion for improved retrieval.
In Proceedingsof the eleventh international conference on Informa-tion and knowledge management, CIKM ?02, pages538?548, New York, NY, USA.
ACM.Emmanuel Morin and Emmanuel Prochasson.
2011.Bilingual lexicon extraction from comparable cor-pora enhanced with parallel corpora.
In Proceed-ings, 4th Workshop on Building and Using Compa-rable Corpora (BUCC), page 27?34, Portland, Ore-gon, USA.Rabia Nuray and Fazli Can.
2006.
Automatic rankingof information retrieval systems using data fusion.Inf.
Process.
Manage., 42(3):595?614, May.Siddharth Patwardhan.
2003.
Incorporating Dictio-nary and Corpus Information into a Context VectorMeasure of Semantic Relatedness.
Master?s thesis,University of Minnesota, Duluth, August.Emmanuel Prochasson, Emmanuel Morin, and KyoKageura.
2009.
Anchor points for bilingual lexi-con extraction from small comparable corpora.
InProceedings, 12th Conference on Machine Transla-tion Summit (MT Summit XII), page 284?291, Ot-tawa, Ontario, Canada.Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proceedings of the 33rd an-nual meeting on Association for Computational Lin-guistics, ACL ?95, pages 320?322.
Association forComputational Linguistics.Zhibiao Wu and Martha Palmer.
1994.
Verbs seman-tics and lexical selection.
In Proceedings of the 32ndannual meeting on Association for ComputationalLinguistics, ACL ?94, pages 133?138.
Associationfor Computational Linguistics.764
