Acquiring the Meaning of Discourse MarkersBen HutchinsonSchool of InformaticsUniversity of EdinburghB.Hutchinson@sms.ed.ac.ukAbstractThis paper applies machine learning techniques toacquiring aspects of the meaning of discourse mark-ers.
Three subtasks of acquiring the meaning of adiscourse marker are considered: learning its polar-ity, veridicality, and type (i.e.
causal, temporal oradditive).
Accuracy of over 90% is achieved for allthree tasks, well above the baselines.1 IntroductionThis paper is concerned with automatically acquir-ing the meaning of discourse markers.
By con-sidering the distributions of individual tokens ofdiscourse markers, we classify discourse markersalong three dimensions upon which there is substan-tial agreement in the literature: polarity, veridical-ity and type.
This approach of classifying linguistictypes by the distribution of linguistic tokens makesthis research similar in spirit to that of Baldwin andBond (2003) and Stevenson and Merlo (1999).Discourse markers signal relations between dis-course units.
As such, discourse markers play animportant role in the parsing of natural languagediscourse (Forbes et al, 2001; Marcu, 2000), andtheir correspondence with discourse relations canbe exploited for the unsupervised learning of dis-course relations (Marcu and Echihabi, 2002).
Inaddition, generating natural language discourse re-quires the appropriate selection and placement ofdiscourse markers (Moser and Moore, 1995; Groteand Stede, 1998).
It follows that a detailed accountof the semantics and pragmatics of discourse mark-ers would be a useful resource for natural languageprocessing.Rather than looking at the finer subtleties inmeaning of particular discourse markers (e.g.
Best-gen et al (2003)), this paper aims at a broad scaleclassification of a subclass of discourse markers:structural connectives.
This breadth of coverageis of particular importance for discourse parsing,where a wide range of linguistic realisations must becatered for.
This work can be seen as orthogonal tothat of Di Eugenio et al (1997), which addresses theproblem of learning if and where discourse markersshould be generated.Unfortunately, the manual classification of largenumbers of discourse markers has proven to be adifficult task, and no complete classification yet ex-ists.
For example, Knott (1996) presents a list ofaround 350 discourse markers, but his taxonomicclassification, perhaps the largest classification inthe literature, accounts for only around 150 of these.A general method of automatically classifying dis-course markers would therefore be of great utility,both for English and for languages with fewer man-ually created resources.
This paper constitutes astep in that direction.
It attempts to classify dis-course markers whose classes are already known,and this allows the classifier to be evaluated empiri-cally.The proposed task of learning automatically themeaning of discourse markers raises several ques-tions which we hope to answer:Q1.
Difficulty How hard is it to acquire the mean-ing of discourse markers?
Are some aspects ofmeaning harder to acquire than others?Q2.
Choice of features What features are usefulfor acquiring the meaning of discourse mark-ers?
Does the optimal choice of features de-pend on the aspect of meaning being learnt?Q3.
Classifiers Which machine learning algo-rithms work best for this task?
Can the rightchoice of empirical features make the classifi-cation problems linearly separable?Q4.
Evidence Can corpus evidence be found forthe existing classifications of discourse mark-ers?
Is there empirical evidence for a separateclass of TEMPORAL markers?We proceed by first introducing the classes of dis-course markers that we use in our experiments.
Sec-tion 3 discusses the database of discourse markersused as our corpus.
In Section 4 we describe our ex-periments, including choice of features.
The resultsare presented in Section 5.
Finally, we conclude anddiscuss future work in Section 6.2 Discourse markersDiscourse markers are lexical items (possibly multi-word) that signal relations between propositions,events or speech acts.
Examples of discourse mark-ers are given in Tables 1, 2 and 3.
In this paperwe will focus on a subclass of discourse markersknown as structural connectives.
These markers,even though they may be multiword expressions,function syntactically as if they were coordinatingor subordinating conjunctions (Webber et al, 2003).The literature contains many different classi-fications of discourse markers, drawing upon awide range of evidence including textual co-hesion (Halliday and Hasan, 1976), hypotacticconjunctions (Martin, 1992), cognitive plausibil-ity (Sanders et al, 1992), substitutability (Knott,1996), and psycholinguistic experiments (Louw-erse, 2001).
Nevertheless there is also considerableagreement.
Three dimensions of classification thatrecur, albeit under a variety of names, are polarity,veridicality and type.
We now discuss each of thesein turn.2.1 PolarityMany discourse markers signal a concession, a con-trast or the denial of an expectation.
These mark-ers have been described as having the feature polar-ity=NEG-POL.
An example is given in (1).
(1) Suzy?s part-time, but she does more workthan the rest of us put together.
(Taken fromKnott (1996, p. 185))This sentence is true if and only if Suzy both is part-time and does more work than the rest of them puttogether.
In addition, it has the additional effect ofsignalling that the fact Suzy does more work is sur-prising ?
it denies an expectation.
A similar effectcan be obtained by using the connective and andadding more context, as in (2)(2) Suzy?s efficiency is astounding.
She?spart-time, and she does more work than therest of us put together.The difference is that although it is possible forand to co-occur with a negative polarity discourserelation, it need not.
Discourse markers like and aresaid to have the feature polarity=POS-POL.
1 On1An alternative view is that discourse markers like and areunderspecified with respect to polarity (Knott, 1996).
In thisthe other hand, a NEG-POL discourse marker likebut always co-occurs with a negative polarity dis-course relation.The gold standard classes of POS-POL and NEG-POL discourse markers used in the learning exper-iments are shown in Table 1.
The gold standardsfor all three experiments were compiled by consult-ing a range of previous classifications (Knott, 1996;Knott and Dale, 1994; Louwerse, 2001).
2POS-POL NEG-POLafter, and, as, as soon as,because, before, consideringthat, ever since, for, given that,if, in case, in order that, in that,insofar as, now, now that, onthe grounds that, once, seeingas, since, so, so that, the in-stant, the moment, then, to theextent that, when, wheneveralthough,but, even if,even though,even when,only if, onlywhen, or, orelse, though,unless, until,whereas, yetTable 1: Discourse markers used in the polarity ex-periment2.2 VeridicalityA discourse relation is veridical if it implies thetruth of both its arguments (Asher and Lascarides,2003), otherwise it is not.
For example, in (3) it isnot necessarily true either that David can stay up orthat he promises, or will promise, to be quiet.
Forthis reason we will say if has the feature veridical-ity=NON-VERIDICAL.
(3) David can stay up if he promises to be quiet.The disjunctive discourse marker or is also NON-VERIDICAL, because it does not imply that bothof its arguments are true.
On the other hand, anddoes imply this, and so has the feature veridical-ity=VERIDICAL.The VERIDICAL and NON-VERIDICAL discoursemarkers used in the learning experiments are shownin Table 2.
Note that the polarity and veridicalityare independent, for example even if is both NEG-POL and NON-VERIDICAL.2.3 TypeDiscourse markers like because signal a CAUSALrelation, for example in (4).account, discourse markers have positive polarity only if theycan never be paraphrased using a discourse marker with nega-tive polarity.
Interpreted in these terms, our experiment aims todistinguish negative polarity discourse markers from all others.2An effort was made to exclude discourse markers whoseclassification could be contentious, as well as ones whichshowed ambiguity across classes.
Some level of judgement wastherefore exercised by the author.VERIDICAL NON-VERIDICALafter, although, and, as, as soonas, because, but, consideringthat, even though, even when,ever since, for, given that, in or-der that, in that, insofar as, now,now that, on the grounds that,once, only when, seeing as,since, so, so that, the instant,the moment, then, though, tothe extent that, until, when,whenever, whereas, while, yetassumingthat, even if,if, if ever, ifonly, in case,on conditionthat, on theassumptionthat, only if,or, or else,supposingthat, unlessTable 2: Discourse markers used in the veridicalityexperiment(4) The tension in the boardroom rose sharplybecause the chairman arrived.As a result, because has the featuretype=CAUSAL.
Other discourse markers thatexpress a temporal relation, such as after, havethe feature type=TEMPORAL.
Just as a POS-POLdiscourse marker can occur with a negative polaritydiscourse relation, the context can also supply acausal relation even when a TEMPORAL discoursemarker is used, as in (5).
(5) The tension in the boardroom rose sharplyafter the chairman arrived.If the relation a discourse marker signals is nei-ther CAUSAL or TEMPORAL it has the featuretype=ADDITIVE.The need for a distinct class of TEMPORAL dis-course relations is disputed in the literature.
Onthe one hand, it has been suggested that TEMPO-RAL relations are a subclass of ADDITIVE ones onthe grounds that the temporal reference inherentin the marking of tense and aspect ?more or less?fixes the temporal ordering of events (Sanders et al,1992).
This contrasts with arguments that resolv-ing discourse relations and temporal order occur asdistinct but inter-related processes (Lascarides andAsher, 1993).
On the other hand, several of the dis-course markers we count as TEMPORAL, such as assoon as, might be described as CAUSAL (Oberlan-der and Knott, 1995).
One of the results of the ex-periments described below is that corpus evidencesuggests ADDITIVE, TEMPORAL and CAUSAL dis-course markers have distinct distributions.The ADDITIVE, TEMPORAL and CAUSAL dis-course markers used in the learning experiments areshown in Table 3.
These features are independentof the previous ones, for example even though isCAUSAL, VERIDICAL and NEG-POL.ADDITIVE TEMPORAL CAUSALand, but,whereasafter, assoon as,before,eversince,now, nowthat, once,until,when,wheneveralthough, because,even though, for, giventhat, if, if ever, in case,on condition that, onthe assumption that,on the grounds that,provided that, provid-ing that, so, so that,supposing that, though,unlessTable 3: Discourse markers used in the type exper-iment3 CorpusThe data for the experiments comes from adatabase of sentences collected automatically fromthe British National Corpus and the world wideweb (Hutchinson, 2004).
The database contains ex-ample sentences for each of 140 discourse structuralconnectives.Many discourse markers have surface forms withother usages, e.g.
before in the phrase before noon.The following procedure was therefore used to se-lect sentences for inclusion in the database.
First,sentences containing a string matching the sur-face form of a structural connective were extracted.These sentences were then parsed using a statisticalparser (Charniak, 2000).
Potential structural con-nectives were then classified on the basis of theirsyntactic context, in particular their proximity to Snodes.
Figure 1 shows example syntactic contextswhich were used to identify discourse markers.
(S ...) (CC and) (S...)(SBAR (IN after) (S...))(PP (IN after) (S...))(PP (VBN given) (SBAR (IN that) (S...)))(NP (DT the) (NN moment) (SBAR...))(ADVP (RB as) (RB long)(SBAR (IN as) (S...)))(PP (IN in) (SBAR (IN that) (S...)))Figure 1: Identifying structural connectivesIt is because structural connectives are easy toidentify in this manner that the experiments use onlythis subclass of discourse markers.
Due to bothparser errors, and the fact that the syntactic heuris-tics are not foolproof, the database contains noise.Manual analysis of a sample of 500 sentences re-vealed about 12% of sentences do not contain thediscourse marker they are supposed to.Of the discourse markers used in the experiments,their frequencies in the database ranged from 270for the instant to 331,701 for and.
The mean num-ber of instances was 32,770, while the median was4,948.4 ExperimentsThis section presents three machine learning ex-periments into automatically classifying discoursemarkers according to their polarity, veridicalityand type.
We begin in Section 4.1 by describingthe features we extract for each discourse markertoken.
Then in Section 4.2 we describe the differ-ent classifiers we use.
The results are presented inSection 4.3.4.1 Features usedWe only used structural connectives in the experi-ments.
This meant that the clauses linked syntacti-cally were also related at the discourse level (Web-ber et al, 2003).
Two types of features were ex-tracted from the conjoined clauses.
Firstly, we usedlexical co-occurrences with words of various partsof speech.
Secondly, we used a range of linguisti-cally motivated syntactic, semantic, and discoursefeatures.4.1.1 Lexical co-occurrencesLexical co-occurrences have previously been shownto be useful for discourse level learning tasks (La-pata and Lascarides, 2004; Marcu and Echihabi,2002).
For each discourse marker, the words occur-ring in their superordinate (main) and subordinateclauses were recorded,3 along with their parts ofspeech.
We manually clustered the Penn Treebankparts of speech together to obtain coarser grainedsyntactic categories, as shown in Table 4.We then lemmatised each word and excluded alllemmas with a frequency of less than 1000 per mil-lion in the BNC.
Finally, words were attached a pre-fix of either SUB or SUPER according to whetherthey occurred in the sub- or superordinate clauselinked by the marker.
This distinguished, for exam-ple, between occurrences of then in the antecedent(subordinate) and consequent (main) clauses linkedby if.We also recorded the presence of other discoursemarkers in the two clauses, as these had previously3For coordinating conjunctions, the left clause was taken tobe superordinate/main clause, the right, the subordinate clause.New label Penn Treebank labelsvb vb vbd vbg vbn vbp vbznn nn nns nnpjj jj jjr jjsrb rb rbr rbsaux aux auxg mdprp prp prp$in inTable 4: Clustering of POS labelsbeen found to be useful on a related classificationtask (Hutchinson, 2003).
The discourse markersused for this are based on the list of 350 markersgiven by Knott (1996), and include multiword ex-pressions.
Due to the sparser nature of discoursemarkers, compared to verbs for example, no fre-quency cutoffs were used.4.1.2 Linguistically motivated featuresThese included a range of one and two dimensionalfeatures representing more abstract linguistic infor-mation, and were extracted through automatic anal-ysis of the parse trees.One dimensional featuresTwo one dimensional features recorded the locationof discourse markers.
POSITION indicated whethera discourse marker occurred between the clauses itlinked, or before both of them.
It thus relates toinformation structuring.
EMBEDDING indicated thelevel of embedding, in number of clauses, of the dis-course marker beneath the sentence?s highest levelclause.
We were interested to see if some types ofdiscourse relations are more often deeply embed-ded.The remaining features recorded the presence oflinguistic features that are localised to a particu-lar clause.
Like the lexical co-occurrence features,these were indexed by the clause they occurred in:either SUPER or SUB.We expected negation to correlate with nega-tive polarity discourse markers, and approximatednegation using four features.
NEG-SUBJ and NEG-VERB indicated the presence of subject negation(e.g.
nothing) or verbal negation (e.g.
n?t).
We alsorecorded the occurrence of a set of negative polar-ity items (NPI), such as any and ever.
The featuresNPI-AND-NEG and NPI-WO-NEG indicated whetheran NPI occurred in a clause with or without verbalor subject negation.Eventualities can be placed or ordered in time us-ing not just discourse markers but also temporal ex-pressions.
The feature TEMPEX recorded the num-ber of temporal expressions in each clause, as re-turned by a temporal expression tagger (Mani andWilson, 2000).If the main verb was an inflection of to be or to dowe recorded this using the features BE and DO.
Ourmotivation was to capture any correlation of theseverbs with states and events respectively.If the final verb was a modal auxiliary, this el-lipsis was evidence of strong cohesion in the text(Halliday and Hasan, 1976).
We recorded this withthe feature VP-ELLIPSIS.
Pronouns also indicate co-hesion, and have been shown to correlate with sub-jectivity (Bestgen et al, 2003).
A class of featuresPRONOUNS represented pronouns, with  denot-ing either 1st person, 2nd person, or 3rd person ani-mate, inanimate or plural.The syntactic structure of each clause was cap-tured using two features, one finer grained and onecoarser grained.
STRUCTURAL-SKELETON identi-fied the major constituents under the S or VP nodes,e.g.
a simple double object construction gives ?NPVB NP NP?.
ARGS identified whether the clausecontained an (overt) object, an (overt) subject, orboth, or neither.The overall size of a clause was represented us-ing four features.
WORDS, NPS and PPS recordedthe numbers of words, NPs and PPs in a clause (notcounting embedded clauses).
The feature CLAUSEScounted the number of clauses embedded beneath aclause.Two dimensional featuresThese features all recorded combinations of linguis-tic features across the two clauses linked by thediscourse marker.
For example the MOOD featurewould take the value  DECL,IMP  for the sentenceJohn is coming, but don?t tell anyone!These features were all determined automaticallyby analysing the auxiliary verbs and the main verbs?POS tags.
The features and the possible values foreach clause were as follows: MODALITY: one ofFUTURE, ABILITY or NULL; MOOD: one of DECL,IMP or INTERR; PERFECT: either YES or NO; PRO-GRESSIVE: either YES or NO; TENSE: either PASTor PRESENT.4.2 Classifier architecturesTwo different classifiers, based on local and globalmethods of comparison, were used in the experi-ments.
The first, 1 Nearest Neighbour (1NN), is aninstance based classifier which assigns each markerto the same class as that of the marker nearest toit.
For this, three different distance metrics wereexplored.
The first metric was the Euclidean dis-tance function  , shown in (6), applied to proba-bility distributions.	 fffiflffi  (6)The second, !
"# , is a smoothed variant ofthe information theoretic Kullback-Leibner diver-gence (Lee, 2001, with $%'&)(+*	, ).
Its definitionis given in (7).!
"#ff-	 ./1032	4ff$-ffiff65798:fi;$/<ff(7)The third metric, =6>1?@?
@A , is a B -test weighted adap-tion of the Jaccard coefficient (Curran and Moens,2002).
In it basic form, the Jaccard coefficient is es-sentially a measure of how much two distributionsoverlap.
The B -test variant weights co-occurrencesby the strength of their collocation, using the fol-lowing function:CBDCFE/CFEffGfiH/CFE</ffI/CFE<ffThis is then used define the weighted version ofthe Jaccard coefficient, as shown in (8).
The wordsassociated with distributionsand  are indicatedby CJ and CLK , respectively.=>1?M?AONPRQTSCBDCJffUCBMCLK NP>V/CBMCJUCBDCLKff (8)!
"# and =6>1?
@?Ahad previously been found tobe the best metrics for other tasks involving lexi-cal similarity.
 is included to indicate what canbe achieved using a somewhat naive metric.The second classifier used, Naive Bayes, takesthe overall distribution of each class into account.
Itessentially defines a decision boundary in the formof a curved hyperplane.
The Weka implementa-tion (Witten and Frank, 2000) was used for the ex-periments, with 10-fold cross-validation.4.3 ResultsWe began by comparing the performance ofthe 1NN classifier using the various lexical co-occurrence features against the gold standards.
Theresults using all lexical co-occurrences are shownAll POS Best single POS BestTask Baseline  !W# =6>1?@?
A  !
"# =6>1?@?
A subsetpolarity 67.4 74.4 72.1 74.4 76.7 (rb) 83.7 (rb) 76.7 (rb) 83.7 Xveridicality 73.5 81.6 85.7 75.5 83.7 (nn) 91.8 (vb) 87.8 (vb) 91.8 Ytype 58.1 74.2 64.5 81.8 74.2 (in) 74.2 (rb) 77.4 (jj) 87.8 Z[Using \^]`_ and either rb or DMs+rb.
a Using both \^]`_ and vb, and bdcfegehcfiUjfk and vb+in.
l Using \^]`_ and vb+aux+inTable 5: Results using the 1NN classifier on lexical co-occurrencesFeature Positively correlated discourse marker co-occurrencesPOS-POL though m , but m , althoughm , assuming that mNEG-POL otherwise n , still m , in truth n , still n , after that m , in this way m , granted that m , incontrast m , by then n , in the event nVERIDICAL obviouslyn , now n , even n , indeed m , once more m , considering that m , even after m ,once more n , at first sight mNON-VERIDICAL or m , no doubt m , in turn m , then m , by all means m , before then nADDITIVE also n , in addition n , still n , only n , at the same time n , clearly n , naturally n ,now n , of course nTEMPORAL back m , once more m , like m , and m , once more n , which was why m , (D(D(CAUSAL again m ,altogether n ,back n ,finally n , also m , thereby n , at once n , while m ,clearly m , (D(D(Table 6: Most informative discourse marker co-occurrences in the super- ( o ) and subordinate ( p ) clausesin Table 5.
The baseline was obtained by assigningdiscourse markers to the largest class, i.e.
with themost types.
The best results obtained using just asingle POS class are also shown.
The results acrossthe different metrics suggest that adverbs and verbsare the best single predictors of polarity and veridi-cality, respectively.We next applied the 1NN classifier to co-occurrences with discourse markers.
The results areshown in Table 7.
The results show that for eachtask 1NN with the weighted Jaccard coefficient per-forms at least as well as the other three classifiers.1NN with metric: NaiveTask  !
"# =>1?M?
A Bayespolarity 74.4 81.4 81.4 81.4veridicality 83.7 79.6 83.7 73.5type 74.2 80.1 80.1 58.1Table 7: Results using co-occurrences with DMsWe also compared using the following combina-tions of different parts of speech: vb + aux, vb + in,vb + rb, nn + prp, vb + nn + prp, vb + aux + rb, vb +aux + in, vb + aux + nn + prp, nn + prp + in, DMs +rb, DMs + vb and DMs + rb + vb.
The best resultsobtained using all combinations tried are shown inthe last column of Table 5.
For DMs + rb, DMs + vband DMs + rb + vb we also tried weighting the co-occurrences so that the sums of the co-occurrenceswith each of verbs, adverbs and discourse markerswere equal.
However this did not lead to any betterresults.One property that distinguishes =6>1?M?Afrom theother metrics is that it weights features the strengthof their collocation.
We were therefore interestedto see which co-occurrences were most informa-tive.
Using Weka?s feature selection utility, weranked discourse marker co-occurrences by their in-formation gain when predicting polarity, veridical-ity and type.
The most informative co-occurrencesare listed in Table 6.
For example, if also occurs inthe subordinate clause then the discourse marker ismore likely to be ADDITIVE.The 1NN and Naive Bayes classifiers were thenapplied to co-occurrences with just the DMs thatwere most informative for each task.
The results,shown in Table 8, indicate that the performance of1NN drops when we restrict ourselves to this subset.4 However Naive Bayes outperforms all previous1NN classifiers.Base- 1NN with: NaiveTask line  !
"# Bayespolarity 67.4 72.1 69.8 90.7veridicality 73.5 85.7 77.6 91.8type 58.1 67.7 58.1 93.5Table 8: Results using most informative DMs4The bdcfege k metric is omitted because it essentially alreadyhas its own method of factoring in informativity.Feature Positively correlated featuresPOS-POL No significantly informative predictors correlated positivelyNEG-POL NEG-VERBAL m , NEG-SUBJ m , ARGS=NONE m , MODALITY=  ABILITY,ABILITY VERIDICAL VERB=BE m , WORDS n , WORDS m , MODALITY=  NULL,NULL NON-VERID TEMPEX m , PRONOUNJfqhrshtm, PRONOUNJfqhrshtnADDITIVE WORDS n , WORDS m , CLAUSES n , MODALITY=  ABILITY,FUTURE  ,MODALITY=  ABILITY,ABILITY  , NPS n , MODALITY=  FUTURE,FUTURE  ,MOOD=  DECLARATIVE,DECLARATIVE TEMPORAL EMBEDDING=7, PRONOUNJuqvrsgtffwXxEzym, MOOD=  INTERROGATIVE,DECLARATIVE CAUSAL NEG-SUBJ n , NEG-VERBAL n , NPI-WO-NEG n , NPI-AND-NEG n ,MODALITY=  NULL,FUTURE Table 9: The most informative linguistically motivated predictors for each class.
The indices o and pindicate that a one dimensional feature belongs to the superordinate or subordinate clause, respectively.Weka?s feature selection utility was also appliedto all the linguistically motivated features describedin Section 4.1.2.
The most informative features areshown in Table 9.
Naive Bayes was then appliedusing both all the linguistically motivated features,and just the most informative ones.
The results areshown in Table 10.All MostTask Baseline features informativepolarity 67.4 74.4 72.1veridicality 73.5 77.6 79.6type 58.1 64.5 77.4Table 10: Naive Bayes and linguistic features5 DiscussionThe results demonstrate that discourse markers canbe classified along three different dimensions withan accuracy of over 90%.
The best classifiersused a global algorithm (Naive Bayes), with co-occurrences with a subset of discourse markers asfeatures.
The success of Naive Bayes shows thatwith the right choice of features the classificationtask is highly separable.
The high degree of accu-racy attained on the type task suggests that there isempirical evidence for a distinct class of TEMPO-RAL markers.The results also provide empirical evidence forthe correlation between certain linguistic featuresand types of discourse relation.
Here we restrictourselves to making just five observations.
Firstly,verbs and adverbs are the most informative parts ofspeech when classifying discourse markers.
Thisis presumably because of their close relation tothe main predicate of the clause.
Secondly, Ta-ble 6 shows that the discourse marker DM in thestructure X, but/though/although Y DM Z is morelikely to be signalling a positive polarity discourserelation between Y and Z than a negative po-larity one.
This suggests that a negative polar-ity discourse relation is less likely to be embed-ded directly beneath another negative polarity dis-course relation.
Thirdly, negation correlates withthe main clause of NEG-POL discourse markers,and it also correlates with subordinate clause ofCAUSAL ones.
Fourthly, NON-VERIDICAL corre-lates with second person pronouns, suggesting that awriter/speaker is less likely to make assertions aboutthe reader/listener than about other entities.
Lastly,the best results with knowledge poor features, i.e.lexical co-occurrences, were better than those withlinguistically sophisticated ones.
It may be that thesophisticated features are predictive of only certainsubclasses of the classes we used, e.g.
hypotheticals,or signallers of contrast.6 Conclusions and future workWe have proposed corpus-based techniques for clas-sifying discourse markers along three dimensions:polarity, veridicality and type.
For these tasks wewere able to classify with accuracy rates of 90.7%,91.8% and 93.5% respectively.
These equate to er-ror reduction rates of 71.5%, 69.1% and 84.5% fromthe baseline error rates.
In addition, we determinedwhich features were most informative for the differ-ent classification tasks.In future work we aim to extend our work in twodirections.
Firstly, we will consider finer-grainedclassification tasks, such as learning whether acausal discourse marker introduces a cause or a con-sequence, e.g.
distinguishing because from so.
Sec-ondly, we would like to see how far our results canbe extended to include adverbial discourse markers,such as instead or for example, by using just fea-tures of the clauses they occur in.AcknowledgementsI would like to thank Mirella Lapata, Alex Las-carides, Bonnie Webber, and the three anonymousreviewers for their comments on drafts of this pa-per.
This research was supported by EPSRC GrantGR/R40036/01 and a University of Sydney Travel-ling Scholarship.ReferencesNicholas Asher and Alex Lascarides.
2003.
Logics ofConversation.
Cambridge University Press.Timothy Baldwin and Francis Bond.
2003.
Learning thecountability of English nouns from corpus data.
InProceedings of ACL 2003, pages 463?470.Yves Bestgen, Liesbeth Degand, and Wilbert Spooren.2003.
On the use of automatic techniques to deter-mine the semantics of connectives in large newspapercorpora: An exploratory study.
In Proceedings of theMAD?03 workshop on Multidisciplinary Approachesto Discourse, October.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the First Conference of theNorth American Chapter of the Association for Com-putational Linguistics (NAACL-2000), Seattle, Wash-ington, USA.James R. Curran and M. Moens.
2002.
Improvements inautomatic thesaurus extraction.
In Proceedings of theWorkshop on Unsupervised Lexical Acquisition, pages59?67, Philadelphia, PA, USA.Barbara Di Eugenio, Johanna D. Moore, and MassimoPaolucci.
1997.
Learning features that predict cueusage.
In Proceedings of the 35th Conference of theAssociation for Computational Linguistics (ACL97),Madrid, Spain, July.Katherine Forbes, Eleni Miltsakaki, Rashmi Prasad,Anoop Sarkar, Aravind Joshi, and Bonnie Webber.2001.
D-LTAG system?discourse parsing with a lex-icalised tree adjoining grammar.
In Proceedings of theESSLI 2001 Workshop on Information Structure, Dis-course Structure, and Discourse Semantics, Helsinki,Finland.Brigitte Grote and Manfred Stede.
1998.
Discoursemarker choice in sentence planning.
In Eduard Hovy,editor, Proceedings of the Ninth International Work-shop on Natural Language Generation, pages 128?137.
Association for Computational Linguistics, NewBrunswick, New Jersey.M.
Halliday and R. Hasan.
1976.
Cohesion in English.Longman.Ben Hutchinson.
2003.
Automatic classification of dis-course markers by their co-occurrences.
In Proceed-ings of the ESSLLI 2003 workshop on Discourse Par-ticles: Meaning and Implementation, Vienna, Austria.Ben Hutchinson.
2004.
Mining the web for discoursemarkers.
In Proceedings of the Fourth InternationalConference on Language Resources and Evaluation(LREC 2004), Lisbon, Portugal.Alistair Knott and Robert Dale.
1994.
Using linguisticphenomena to motivate a set of coherence relations.Discourse Processes, 18(1):35?62.Alistair Knott.
1996.
A data-driven methodology formotivating a set of coherence relations.
Ph.D. thesis,University of Edinburgh.Mirella Lapata and Alex Lascarides.
2004.
Inferringsentence-internal temporal relations.
In In Proceed-ings of the Human Language Technology Confer-ence and the North American Chapter of the Associ-ation for Computational Linguistics Annual Meeting,Boston, MA.Alex Lascarides and Nicholas Asher.
1993.
Temporalinterpretation, discourse relations and common senseentailment.
Linguistics and Philosophy, 16(5):437?493.Lillian Lee.
2001.
On the effectiveness of the skew di-vergence for statistical language analysis.
ArtificialIntelligence and Statistics, pages 65?72.Max M Louwerse.
2001.
An analytic and cognitive pa-rameterization of coherence relations.
Cognitive Lin-guistics, 12(3):291?315.Inderjeet Mani and George Wilson.
2000.
Robust tem-poral processing of news.
In Proceedings of the38th Annual Meeting of the Association for Compu-tational Linguistics (ACL 2000), pages 69?76, NewBrunswick, New Jersey.Daniel Marcu and Abdessamad Echihabi.
2002.
Anunsupervised approach to recognizing discourse rela-tions.
In Proceedings of the 40th Annual Meeting ofthe Association for Computational Linguistics (ACL-2002), Philadelphia, PA.Daniel Marcu.
2000.
The Theory and Practice of Dis-course Parsing and Summarization.
The MIT Press.Jim Martin.
1992.
English Text: System and Structure.Benjamin, Amsterdam.M.
Moser and J. Moore.
1995.
Using discourse analy-sis and automatic text generation to study discoursecue usage.
In Proceedings of the AAAI 1995 SpringSymposium on Empirical Methods in Discourse Inter-pretation and Generation, pages 92?98.Jon Oberlander and Alistair Knott.
1995.
Issues incue phrase implicature.
In Proceedings of the AAAISpring Symposium on Empirical Methods in Dis-course Interpretation and Generation.Ted J. M. Sanders, W. P. M. Spooren, and L. G. M. No-ordman.
1992.
Towards a taxonomy of coherence re-lations.
Discourse Processes, 15:1?35.Suzanne Stevenson and Paola Merlo.
1999.
Automaticverb classification using distributions of grammaticalfeatures.
In Proceedings of the 9th Conference of theEuropean Chapter of the ACL, pages 45?52, Bergen,Norway.Bonnie Webber, Matthew Stone, Aravind Joshi, and Al-istair Knott.
2003.
Anaphora and discourse structure.Computational Linguistics, 29(4):545?588.Ian H. Witten and Eibe Frank.
2000.
Data Mining:Practical machine learning tools with Java implemen-tations.
Morgan Kaufmann, San Francisco.
