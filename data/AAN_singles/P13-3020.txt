Proceedings of the ACL Student Research Workshop, pages 136?141,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsHigh-quality Training Data Selection using Latent Topicsfor Graph-based Semi-supervised LearningAkiko EriguchiOchanomizu University2-1-1 Otsuka Bunkyo-ku Tokyo, Japang0920506@is.ocha.ac.jpIchiro KobayashiOchanomizu University2-1-1 Otsuka Bunkyo-ku Tokyo, Japankoba@is.ocha.ac.jpAbstractIn a multi-class document categorizationusing graph-based semi-supervised learn-ing (GBSSL), it is essential to constructa proper graph expressing the relationamong nodes and to use a reasonable cat-egorization algorithm.
Furthermore, it isalso important to provide high-quality cor-rect data as training data.
In this con-text, we propose a method to construct asimilarity graph by employing both sur-face information and latent informationto express similarity between nodes anda method to select high-quality trainingdata for GBSSL by means of the PageR-ank algorithm.
Experimenting on Reuters-21578 corpus, we have confirmed that ourproposed methods work well for raisingthe accuracy of a multi-class documentcategorization.1 IntroductionGraph-based semi-supervised learning (GBSSL)algorithm is known as a useful and promising tech-nique in natural language processings.
It has beenwidely used for solving many document catego-rization problems (Zhu and Ghahramani, 2002;Zhu et al 2003; Subramanya and Bilmes, 2008).A good accuracy of GBSSL depends on successin dealing with three crucial issues: graph con-struction, selection of high-quality training data,and categorization algorithm.
We particularly fo-cus on the former two issues in our study.In a graph-based categorization of documents,a graph is constructed based on a certain relationbetween nodes (i.e.
documents).
It is similar-ity that is often used to express the relation be-tween nodes in a graph.
We think of two types ofsimilarity: the one is between surface informationobtained by document vector (Salton and McGill,1983) and the other is between latent informationobtained by word probabilistic distribution (LatentDirichlet Allocation (Blei et al 2003)).
Here,we propose a method.
We use both surface in-formation and latent information at the ratio of(1 ?
?)
: ?
(0 ?
?
?
1) to construct a similaritygraph for GBSSL, and we investigate the optimal?
for raising the accuracy in GBSSL.In selecting high-quality training data, it is im-portant to take two aspects of data into consider-ation: quantity and quality.
The more the train-ing data are, the better the accuracy becomes.
Wedo not always, however, have a large quantity oftraining data.
In such a case, the quality of train-ing data is generally a key for better accuracy.
It isrequired to assess the quality of training data ex-actly.
Now, we propose another method.
We usethe PageRank algorithm (Brin and Page, 1998) toselect high-quality data, which have a high cen-trality in a similarity graph of training data (i.e.labeled data) in each category.We apply our methods to solving the problemof a multi-class document categorization.
We in-troduce PRBEP (precision recall break even point)as a measure which is popular in the area of infor-mation retrieval.
We evaluate the results of exper-iments for each category and for the whole cat-egory.
We confirm that the way of selecting thehigh-quality training data from data on a similar-ity graph based on both surface information andlatent information is superior to that of selectingfrom a graph based on just surface information orlatent information.2 Related studiesGraph-based semi-supervised learning has re-cently been studied so much and applied to manyapplications (Subramanya and Bilmes, 2008; Sub-ramanya and Bilmes, 2009; Subramanya et al2010; Dipanjan and Petrov, 2011; Dipanjan andSmith, 2012; Whitney and Sarkar, 2012).136Subramanya and Bilmes (2008; 2009) have pro-posed a soft-clustering method using GBSSL andhave shown that their own method is better thanthe other main clustering methods of those days.Subramanya et al(2010) have also applied theirmethod to solve the problem of tagging and haveshown that it is useful.
Dipanjan and Petrov(2011) have applied a graph-based label propa-gation method to solve the problem of part-of-speech tagging.
They have shown that their pro-posed method exceeds a state-of-the-art baselineof those days.
Dipanjan and Smith (2012) havealso applied GBSSL to construct compact natu-ral language lexicons.
To achieve compactness,they used the characteristics of a graph.
Whitneyand Sarkar (2012) have proposed the bootstrap-ping learning method in which a graph propaga-tion algorithm is adopted.There are two main issues in GBSSL: the oneis the way of constructing a graph to propagate la-bels, and the other is the way of propagating la-bels.
It is essential to construct a good graph inGBSSL (Zhu, 2005).
On the one hand, graph con-struction is a key to success of any GBSSL.
Onthe other hand, as for semi-supervised learning, itis quite important to select better training data (i.e.labeled data), because the effect of learning willbe changed by the data we select as training data.Considering the above mentioned, in our study,we focus on the way of selecting training data soas to be well propagated in a graph.
We use thePageRank algorithm to select high-quality train-ing data and evaluate how our proposed methodinfluences the way of document categorization.3 Text classification based on a graphThe details of our proposed GBSSL method ina multi-class document categorization are as fol-lows.3.1 Graph constructionIn our study, we use a weighted undirected graphG = (V,E) whose node and edge represent a doc-ument and the similarity between nodes, respec-tively.
Similarity is regarded as weight.
V and Erepresent nodes and edges in a graph, respectively.A graph G can be represented as an adjacency ma-trix, and wij ?
W represents the similarity be-tween nodes i and j.
In particular, in the case ofGBSSL method, the similarity between nodes areformed as wij = sim(xi,xj)?
(j ?
K(i)).
K(i)is a set of i?s k-nearest neighbors, and ?
(z) is 1 ifz is true, otherwise 0.3.2 Similarity in a graphGenerally speaking, when we construct a graphto represent some relation among documents, co-sine similarity (simcos) of document vectors isadopted as a similarity measure based on surfaceinformation.
In our study, we add the similarity(simJS) based on latent information and the simi-larity (simcos) based on surface information in theproportion of ?
: (1 ?
?
)(0 ?
?
?
1).
We definethe sum of simJS and simcos as simnodes (see,Eq.
(1)).In Eq.
(1), P and Q represent the latent topicdistributions of documents S and T , respectively.We use Latent Dirichlet Allocation (LDA) (Bleiet al 2003) to estimate the latent topic distribu-tion of a document, and we use a measure Jensen-Shannon divergence (DJS) for the similarity be-tween topic distributions.
Incidentally, simJS inEq (1) is expressed by Eq.
(2).simnodes(S, T ) ?
?
?
simJS(P,Q)+(1?
?)
?
simcos(tfidf(S), tfidf(T )) (1)simJS(P,Q) ?
1?DJS(P,Q) (2)3.3 Selection of training dataWe use the graph-based document summarizationmethods (Erkan and Radev, 2004; Kitajima andKobayashi, 2012) in order to select high-qualitytraining data.
Erkan and Radev (2004) proposed amulti-document summarization method using thePageRank algorithm (Brin and Page, 1998) to ex-tract important sentences.
They showed that itis useful to extract the important sentences whichhave higher PageRank scores in a similarity graphof sentences.
Then, Kitajima and Kobayashi(2012) have expanded the idea of Erkan andRadev?s.
They introduced latent information toextract important sentences.
They call their ownmethod TopicRank.We adopt TopicRank method in our study.
In or-der to get high-quality training data, we first con-struct a similarity graph of training data in eachcategory, and then compute a TopicRank score foreach training datum in every category graph.
Weemploy the data with a high TopicRank score astraining data in GBSSL.In TopicRank method, Kitajima and Kobayashi(2012) regard a sentence as a node in a graph on137surface information and latent information.
TheTopicRank score of each sentence is computed byEq.
(3).
Each sentence is ranked by its TopicRankscore.
In Eq.
(3), d indicates a damping factor.We, however, deal with documents, so we replacea sentence with a document (i.e.
sentences) as anode in a graph.
In Eq.
(3), N indicates total num-ber of documents, adj[u] indicates the adjoiningnodes of document u.r(u) = d?v?adj[u]simnodes(u, v)?z?adj[v]simnodes(z, v)r(u)+1?
dN (3)3.4 Label propagationWe use the label propagation method (Zhu et al2003; Zhou et al 2004) in order to categorize doc-uments.
It is one of graph-based semi-supervisedlearnings.
It estimates the value of label basedon the assumption that the nodes linked to eachother in a graph should belong to the same cate-gory.
Here, W indicates an adjacency matrix.
lindicates the number of training data among all nnodes in a graph.
The estimation values f for nnodes are obtained as the solution (Eq.
(6)) of thefollowing objective function of an optimal prob-lem (Eq.
(4)).
The first term in Eq.
(4) expressesthe deviation between an estimation value and acorrect value of training data.
The second term inEq.
(4) expresses the difference between the esti-mation values of the nodes which are next to an-other in the adjacency graph.
?
(> 0) is a param-eter balancing both of the terms.
Eq.
(4) is trans-formed into Eq.
(5) by means ofL.
L(?
D?W )is called the Laplacian matrix.
D is a diagonal ma-trix, each diagonal element of which is equal to thesum of elements inW ?s each row (or column).J(f) =l?i=1(y(i) ?
f (i))2+?
?i<jw(i,j)(f (i) ?
f (j))2 (4)= ||y ?
f ||22 + ?fTLf (5)f = (I + ?L)?1y (6)4 Experiment4.1 Experimental settingsWe use Reuters-21578 corpus data set1 collectedfrom the Reuters newswire in 1987 as target doc-uments for a multi-class document categorization.It consists of English news articles (classified into135 categories).
We use the ?ModApte?
split toget training documents (i.e.
labeled data) andtest documents (i.e.
unlabeled data), extract doc-uments which have only its title and text body,and apply the stemming and the stop-word re-moval processes to the documents.
Then, follow-ing the experimental settings of Subramanya andBilmes (2008)2 , we use 10 most frequent cate-gories out of the 135 potential topic categories:earn, acq, grain, wheat, money-fx, crude, trade,interest, ship, and corn.
We apply the one-versus-the-rest method to give a category label to eachtest document.
Labels are given when the estima-tion values of each document label exceed each ofthe predefined thresholds.We prepare 11 data sets.
Each data set consistsof 3299 common test data and 20 training data.We use 11 kinds of categories of training data:the above mentioned 10 categories and a category(other) which indicates 125 categories except 10categories.
The categories of 20 training data arerandomly chosen only if one of the 11 categoriesis chosen at least once.Selecting high-quality training data, we use theGibbs sampling for latent topic estimation in LDA.The number of iteration is 200.
The number of la-tent topics in the target documents is decided byaveraging 10 trials of estimation with perplexity(see, Eq.
(7)).
Here, N is the number of all wordsin the target documents.
wmn is the n-th word inthe m-th document.
?
is an occurrence probabilityof the latent topics for the documents.
?
is an oc-currence probability of the words for every latenttopic.P (w) = exp(?
1N?mnlog(?z?mz?zwmn)) (7)In each category, a similarity graph is con-structed for the TopicRank method.
The numberof nodes (i.e.
|Vcategory|) in a graph corresponds to1http://www.daviddlewis.com/resources/testcollections/reuters21578/2Our data sets lack any tags and information excluding atitle and a text body.
Therefore, we cannot directlycompare with Subramanya and Bilmes?
results.138the total number of training data in each category,and the number of edges is E = (|Vcategory| ?|Vcategory|).
So, the graph is a complete graph.The parameter ?
in Eq (1) is varied from 0.0 to1.0 every 0.1.
We regard the average of TopicRankscores after 5 trials as the TopicRank score of eachdocument.
The number of training data in eachcategory is decided in each target data set.
Weadopt training data with a higher TopicRank scorefrom the top up to the predefined number.In label propagation, we construct another kindof similarity graph.
The number of nodes in agraph is |Vl+u| = n(= 3319), and the similar-ity between nodes is based on only surface infor-mation (in the case of ?
= 0 in Eq.
(1)).
Theparameter k in the k-nearest neighbors method isk ?
{2, 10, 50, 100, 250, 500, 1000, 2000, n}, theparameter ?
in the label propagation method, is?
?
{1, 0.1, 0.01, 1e ?
4, 1e ?
8}.
Using one ofthe 11 data sets, we decide a pair of optimal pa-rameters (k, ?)
for each category.
We categorizethe remaining 10 data sets by means of the decidedparameters.
Then, we obtain the value of precisionrecall break even point (PRBEP) and the averageof PRBEP in each category.
The value of PRBEPis that of precision or recall at the time when theformer is equal to the latter.
It is often used asan index to measure the ability of information re-trieval.4.2 ResultTable 1 shows a pair of the optimal parameters(k, ?)
in each category corresponding to the valueof ?
ranging from 0.0 to 1.0 every 0.1.
Figuresfrom 1 to 10 show the experimental results in us-ing these parameters in each category.
The hori-zontal axis indicates the value of ?
and the verti-cal axis indicates the value of PRBEP.
Each figureshows the average of PRBEP in each category af-ter 10 trials for each ?.
Fig.
11 shows how therelative ratio of PRBEP changes corresponding toeach ?
in each category, when we let the PRBEPat ?
= 0 an index 100.
Fig.
12 shows the macroaverage of PRBEP after 10 trials in the whole cat-egory corresponding to each ?.
Error bars indicatethe standard deviations.In all figures, the case at ?
= 0 means that onlysurface information is used for selecting the train-ing data.
The case at ?
= 1 means that only latentinformation is used.
The other cases at ?
6= 0 or 1mean that both latent information and surface in-formation are mixed at the ratio of ?
: (1 ?
?
)(0 < ?
< 1).First, we tell about Fig.
1-10.
On the one hand,in Fig.
4, 5, 6, 8, 10, the PRBEPs at ?
6= 0 aregreater than that at ?
= 0, although the PRBEP at?
= 1 is less than that at ?
= 0 in Fig.
4.
Onthe other hand, in Fig.
2, 7, the PRBEPs at ?
6= 0are less than that at ?
= 0.
In Fig.
1, 3, 9, thePRBEPs at ?
6= 0 fluctuate widely or narrowlyaround that at ?
= 0.
In addition, the PRBEPs at?
= 0 range from 7.7 to 74.3 and those at ?
= 1range from 8.0 to 72.6 in all 10 figures.
It is hardto find significant correlation between PRBEP and?.Secondly, in Fig.
11, some curves show an in-creasing trend and others show a decreasing trend.At best, the maximum value is three times as largeas that at ?
= 0.
At worst, the minimum is one-fifth.
Indexes at ?
6= 0 are greater than or equal toan index 100 at ?
= 0 in most categories.Finally, in Fig.
12, the local maximums are46.2, 46.9, 45.0 respectively at ?
= 0.2, 0.6, 0.9.The maximum is 46.9 at ?
= 0.6.
The mini-mum value of the macro average is 35.8 at ?
= 0,though the macro average at ?
= 1 is 43.4.
Hence,the maximum macro average is greater than that at?
= 1 by 3.5% and still greater than that at ?
= 0by 11.1%.
The macro average at ?
= 1 is greaterthan that at ?
= 0 by 7.6%.
Furthermore, themacro average increases monotonically from 35.8to 46.2 as ?
increases from 0.0 to 0.2.
When ?
ismore than 0.2, the macro averages fluctuate withinthe range from 40.3 to 46.9.
It follows that themacro average values at 0.1 ?
?
?
1 are greaterthan that at ?
= 0.
What is more important, themacro averages at ?
= 0.2, 0.4, 0.6, 0.7, 0.9 aregreater than that at ?
= 1 and of course greaterthan that at ?
= 0.5 DiscussionLooking at each Fig.
1-10, each optimal ?
atwhich PRBEP is the maximum is different and notuniform in respective categories.
So, we cannotsimply tell a specific ratio of balancing both infor-mation (i.e.
surface information and latent infor-mation) which gives the best accuracy.From a total point of view, however, we can seea definite trend or relationship.
In Fig.
11, wecan see the upward tendency of PREBP in half ofcategories.
Indexes of the PRBEP at ?
?
0.1 aregreater than or equal to 100 in most categories.139Table 1: the optimal parameters (k, ?)
for each categoryCategory\?
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0earn (500, 1) (50, 1) (1000, 1) (1000, 1) (50, 1) (50, 1) (50, 1) (50, 1) (50, 1) (50, 1) (50, 1)acq (100, 0.01) (100, 0.01) (100, 0.01) (2, 1) (100, 0.01) (100, 0.01) (100, 1e-8) (100, 1e-8) (100, 1e-8) (100, 1e-8) (100, 1e-8)money-fx (250, 0.01) (100, 1e-8) (10, 1e-4) (100, 1e-8) (2, 0.1) (2, 0.1) (2, 1e-8) (250, 1e-8) (2, 0.1) (2, 1e-8) (250, 1e-8)grain (250, 0.1) (2000, 1e-4) (100, 1) (250, 0.1) (100, 1) (50, 1) (250, 1) (50, 1) (50, 1) (50, 1) (100, 1)crude (50, 0.1) (2, 1) (250, 0.01) (50, 1e-8) (10, 0.01) (250, 0.01) (250, 0.01) (250, 1e-8) (10, 0.01) (250, 0.01) (250, 0.01)trade (2, 1) (10, 0.1) (50, 0.01) (10, 1e-8) (10, 1e-8) (10, 1e-8) (50, 1e-8) (10, 1e-8) (10, 1e-4) (10, 0.1) (10, 0.1)interest (10, 1) (50, 1e-8) (50, 1e-8) (10, 1) (2, 0.1) (250, 1e-8) (250, 0.01) (250, 0.01) (2, 1) (2, 0.1) (500, 1e-8)ship (3318, 1) (50, 1) (50, 1) (250, 0.1) (50, 0.1) (50, 0.1) (50, 1e-8) (50, 1e-8) (100, 0.1) (100, 0.1) (50, 0.01)wheat (500, 1e-8) (500, 1e-8) (250, 1e-8) (500, 1e-8) (500, 0.01) (1000, 0.01) (500, 1e-8) (250, 1e-8) (250, 1e-8) (250, 1e-8) (250, 1e-8)corn (10, 1e-8) (100, 1e-8) (250, 1e-8) (10, 1e-8) (250, 1e-8) (250, 1e-4) (500, 1e-8) (100, 1e-8) (250, 1e-8) (50, 0.01) (250, 1e-4)Figure 1: earn Figure 2: acq Figure 3: money-fxFigure 4: grain Figure 5: crude Figure 6: tradeFigure 7: interest Figure 8: ship Figure 9: wheatFigure 10: corn Figure 11: Relative value Figure 12: Macro average140The macro average of the whole category is shownin Fig.
12.
Regarding the macro average at ?
= 0as a baseline, the macro average at ?
= 1 is greaterthan that at ?
= 0 by 7.6% and still more, the max-imum at ?
= 0.6 is greater by 11.1%.
Besides,five macro averages at 0.1 ?
?
?
1 are greaterthan that at ?
= 1.
Therefore, we can say thatusing latent information gives a higher accuracythan using only surface information and that usingboth information gives a higher accuracy than us-ing only latent information.
So, if a proper ?
isdecided, we will get a better accuracy.6 ConclusionWe have proposed methods to construct a sim-ilarity graph based on both surface informationand latent information and to select high-qualitytraining data for GBSSL.
Through experiments,we have found that using both information givesa better accuracy than using either only surfaceinformation or only latent information.
We usedthe PageRank algorithm in the selection of high-quality training data.
In this condition, we haveconfirmed that our proposed methods are usefulfor raising the accuracy of a multi-class documentcategorization using GBSSL in the whole cate-gory.Our future work is as follows.
We will verifyin other data corpus sets that the selection of high-quality training data with both information givesa better accuracy and that the optimal ?
is around0.6.
We will revise the way of setting a pair of theoptimal parameters (k, ?)
and use latent informa-tion in the process of label propagation.ReferencesDavid M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent dirichlet alcation.
Journal of Ma-chine Learning Research.Sergey Brin and Lawrence Page.
1998.
The Anatomyof a Large-scale Hypertextual Web Search Engine.Computer Networks and ISDN Systems, pages 107?117.Das Dipanjan and Noah A. Smith.
2012.
Graph-basedlexicon expansion with sparsity-inducing penalties.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL HLT ?12, pages 677?687.Das Dipanjan and Slav Petrov.
2011.
UnsupervisedPart-of-Speech Tagging with Bilingual Graph-BasedProjections.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies - Vol.
1,pages 600?609.Gu?nes?
Erkan and Dragomir R. Radev.
2004.
LexRank:Graph-based Lexical Centrality as Salience in TextSummarization.
Journal of Artificial IntelligenceResearch 22, pages 457-479.Gu?nes?
Erkan.
2006.
Language Model-Based Docu-ment Clustering Using Random Walks.
Associationfor Computational Linguistics, pages 479?486.Risa Kitajima and Ichiro Kobayashi.
2012.
Multiple-document Summarization baed on a Graph con-structed based on Latent Information.
In Proceed-ings of ARG Web intelligence and interaction, 2012-WI2-1-21.Gerard Salton and Michael J. McGill.
1983.
Intro-duction to Modern Information Retrieval.
McGraw-Hill.Amarnag Subramanya and Jeff Bilmes.
2008.
Soft-Supervised Learning for Text Classification.
In Pro-ceedings of the 2008 Conference on Empirical Meth-ods in Natural Language Processing, pages 1090?1099.Amarnag Subramanya and Jeff Bilmes.
2009.
En-tropic graph regularization in non-parametric semi-supervised classification.
In Proceedings of NIPS.Amarnag Subramanya, Slav Petrov and FernandoPereira.
2010.
Efficient graph-based semi-supervised learning of structured tagging models.In Proceedings of the 2010 Conference on Empiri-cal Methods in Natural Language Processing, pages167?176.Dengyong Zhou, Oliver Bousquet, Thomas Navin Lal,Jason Weston, and Bernhard Scho?lkopf.
2004.Learning with Local and Global Consistency.
InNIPS 16.Xiaojin Zhu and Zoubin Ghahramani.
2002.
Learningfrom Labeled and Unlabeled Data with Label Prop-agation.
Technical report, Carnegie Mellon Univer-sity.Xiaojin Zhu, Zoubin Ghahramani, and John Lafferty.2003.
Semi-Supervised Learning Using GaussianFields and Harmonic Functions.
In Proceedigns ofthe International Conference on Machine Learning(ICML).Xiaojin Zhu.
2005.
Semi-Supervised Learning withGraphs.
PhD thesis, Carnegie Mellon University.Max Whitney and Anoop Sarkar.
2012.
Bootstrappingvia Graph Propagation.
The 50th Annual Meeting ofthe Association for Computational Linguistics.141
