Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 689?694, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsUoS: A Graph-Based System for Graded Word Sense InductionDavid Hope, Bill KellerUniversity of SussexCognitive and Language Processing Systems GroupBrighton, Sussex, UKdavehope@gmail.com, billk@sussex.ac.ukAbstractThis paper presents UoS, a graph-based WordSense Induction system which attempts tofind all applicable senses of a target wordgiven its context, grading each sense accord-ing to its suitability to the context.
Sensesof a target word are induced through use ofa non-parameterised, linear-time clustering al-gorithm that returns maximal quasi-stronglyconnected components of a target word graphin which vertex pairs are assigned to the samecluster if either vertex has the highest edgeweight to the other.
UoS participated inSemEval-2013 Task 13: Word Sense Induc-tion for Graded and Non-Graded Senses.
Twosystem were submitted; both systems returnedresults comparable with those of the best per-forming systems.1 IntroductionWord Sense Induction (WSI) is the task of automat-ically discovering word senses from text.
In princi-ple, WSI avoids reliance on a pre-defined sense in-ventory.1 Whereas the related task of Word SenseDisambiguation (WSD) can only assign pre-definedsenses to words on the basis of context, WSI fol-lows the dictum that ?The meaning of a word is itsuse in the language.?
(Wittgenstein, 1953) to dis-cover senses through examination of context of usein large text corpora.
WSI, therefore, may be applied1In practice, evaluation of a WSI system requires the use ofa gold standard sense inventory such as WordNet (Miller et al1990) or OntoNotes (Hovy et al 2006).to discover new, rare, or domain specific senses;senses undefined in existing sense inventories.2Previous WSI evaluations (Agirre and Soroa,2007; Manandhar et al 2010) have approachedsense induction in terms of finding the single mostsalient sense of a target word given its context.However, as shown in Erk and McCarthy (2009), agraded notion of sense may be more applicable, asmultiple senses of the target word may be perceivedby readers.
The SemEval-2013 WSI evaluation de-scribed in this paper is designed to explore the possi-bility of finding all perceived senses of a target wordin a single contextual instance.
The aim for partici-pants in the task is therefore to design a system thatwill induce a set of graded (weighted) senses of atarget word in a particular context.The paper is organised as follows: Section 2 in-troduces SemEval-2013 Task 13: Word Sense In-duction for Graded and Non-Graded Senses; Sec-tion 3 presents UoS, the system that participated inthe task; Section 4 reports evaluation results, show-ing that UoS returns scores comparable with thoseof the best performing systems.2 SemEval-2013 Task 132.1 AimThe aim for participants in SemEval-2013 Task 13:Word Sense Induction for Graded and Non-GradedSenses is to construct a system that will: (1) inducethe senses of a given set of target words and (2), labeleach test set context (instance) of a target word with2Surveys of WSI and WSD approaches are found in Navigli(2009) and Navigli (2012).689all applicable target word senses.
Candidate sensesare drawn from the WordNet 3.1 sense inventory.Systems must therefore return a set of graded sensesfor each target word in a particular context, where anumeric weight signifies (grades) each sense?s appli-cability to the context.
A non-graded sense is simplythe highest graded (weighted) sense out of all gradedsenses.2.2 Test SetThe test set consists of 4806 instances of 50 targetwords: 20 verbs (1901 instances), 20 nouns (1908),and 10 adjectives (997).3 Instances are extractedfrom the Open American National Corpus, being amix of both written and spoken contexts of targetwords.4 Only 542 instances are assigned more thanone sense by annotators, thus have graded senses.This figure somewhat detracts from the task?s aim asjust 11.62% of the test set can be assigned gradedsenses.2.3 Evaluation MeasuresSystems are evaluated in two ways: (1) in a WSDtask and (2), a clustering task.
In the first evalu-ation, systems are assessed by their ability to cor-rectly identify which WordNet 3.1 senses of the tar-get word are applicable in a given instance, and toquantify, and so, rank, senses according to their levelof applicability.
The supervised evaluation methodof previous SemEval WSI tasks (Agirre and Soroa,2007; Manandhar et al 2010) is applied to map in-duced senses to WordNet 3.1 senses, with the map-ping function of Jurgens (2012) used to account forthe applicability weights.
Three evaluation metricsare used -?
Jaccard Index: measures the overlap betweengold standard senses and those returned by aWSI system.?
Positionally-Weighted Kendall?s Tau: measuresthe ability of a system to rank senses by theirapplicability.3Stated as 4664 instances on the task website.
Note that thefigure of 4806 is for the revised test set.4http://www.americannationalcorpus.org/OANC/index.html.?
Weighted Normalized Discounted CumulativeGain (NDCG): measures the agreement in ap-plicability ratings, accounting for both theranking and difference in weights assigned tosenses.In the second evaluation, similarity between a partic-ipant?s clustering solution and that of the gold stan-dard set of senses is measured using two metrics -?
Fuzzy Normalised Mutual Information (NMI):extends the method of Lancichinetti et al(2009) to compute NMI between overlapping(fuzzy) clusters.
Fuzzy NMI measures thealignment of system and gold standard sensesindependently of the cluster sizes, so returns ameasure of how well a WSI system would per-form regardless of the sense distribution in acorpus.?
Fuzzy B-Cubed: adapts the overlapping B-Cubed measure defined in Amigo?
et al(2009)to the fuzzy clustering setting.
As an item-based, rather than cluster-based, measure,Fuzzy B-Cubed is sensitive to cluster size skew,thus captures the expected performance of aWSI system on a new corpus where the sensedistribution is the same.3 The UoS SystemThe UoS system uses a graph-based model of wordco-occurrence to induce target word senses as fol-lows:3.1 Constructing a Target Word GraphA graph G = (V,E) is constructed for each tar-get word.
V is a set of vertices and E ?
V ?
Va set of edges.
Each vertex v ?
V represents aword found in a dependency relation with the tar-get word.
Words are extracted from the dependency-parsed version of ukWaC (Ferraresi et al 2008).
Inthis evaluation V consists of the 300 highest rankeddependency relation words.5 Words are ranked us-ing the Normalised Pointwise Mutual Information5|V | = 300 was found to return the best results on the trialset over the range |V | = [100, 200, 300, ..., 1000].690(NPMI) measure (Bouma, 2009)6, defined for twowords w1, w2 as:NPMI(w1, w2) =(log p(w1,w2)p(w1) p(w2))?log p(w1, w2).
(1)An edge (vi, vj) ?
E is a pair of vertices.
An edgerepresents a symmetrical relationship between ver-tices vi and vj ; here, that words wi and wj co-occurin ukWaC contexts.
Each edge (vi, vj) is assigneda weight w(vi, vj) to quantify the significance ofwi, wj co-occurrence, the weight being the value re-turned by NPMI(wi, wj).3.2 Clustering the Target Word GraphA clustering algorithm is applied to the target wordgraph, partitioning it to a set of clusters.
Eachset of words in a cluster is taken to represent asense of the target word.
The clustering algorithmapplied is MaxMax, a non-parameterised, linear-time algorithm shown to return good results in pre-vious WSI evaluations (Hope and Keller, 2013).MaxMax transforms the weighted, undirected targetword graph G into an unweighted, directed graphG?, where edge direction in G?
indicates a maximalaffinity relationship between two vertices.
A ver-tex vi is said to have maximal affinity to a vertexvj if the edge weight w(vi, vj) is maximal amongstthe weights of all edges incident on vi.
Clusters areidentified by finding root vertices of quasi-stronglyconnected (QSC) subgraphs in G?
(Thulasiramanand Swamy, 1992).
A directed subgraph is said tobe QSC if, for any vertices vi and vj , there is a rootvertex vk (not necessarily distinct from vi and vj)with a directed path from vk to vi and a directed pathfrom vk to vj .73.3 Merging ClustersMaxMax tends to generate many fine-grained senseclusters.
Clusters are therefore merged using twomeasures: cohesion and separation (Tan et al6Application of the Log Likelihood Ratio measure (Dun-ning, 1993) returned the same set of words.
Though not re-quired here, NPMI has the useful properties that: if w1 and w2always co-occur NPMI = 1; if w1 and w2 are distributed as ex-pected under independence NPMI = 0, and if w1 and w2 neveroccur together, NPMI = ?1.7MaxMax is described in detail in Hope and Keller (2013).2006).
The cohesion of a cluster Ci is defined as:cohesion(Ci) =?x?Ci,y?Ciw(x, y)|Ci|.
(2)Separation between two clusters Ci, Cj is definedas:separation(Ci, Cj) = 1????
?x?Ci,y?Cjw(x, y)|Ci| ?
|Cj |???
.
(3)Cluster pairs with high cohesion and low separationare merged, the intuition being that words in suchpairs will retain a relatively high degree of semanticsimilarity.
High cohesion is defined as greater thanaverage cohesion.
Low separation is defined as a re-ciprocal relationship between two clusters: if a clus-ter Ci has the lowest separation to a cluster Cj (outof all clusters) and Cj the lowest separation to Ci,then the two (high cohesion) clusters are merged.83.4 Assigning Graded Word Senses to TargetWordsEach test instance is labelled with graded senses ofthe target word.
A score is computed for the test in-stance and each target word cluster as the reciprocalof the separation measure, where Ci is the set of con-tent words in the instance (nouns, verbs, adjectives,and adverbs, minus the target word itself) and Cj ,the words in the cluster.
The cluster with the lowestseparation score is taken to be the most salient senseof the target word, with all other positive separationscores taken to be perceived, graded senses of thetarget word in that particular instance.4 Evaluation ResultsTwo sets of results were submitted.
The first, UoS(top 3), returns the three highest scoring senses foreach instance; the second, UoS (# WN senses), re-turns the n = number of target word senses in Word-Net 3.1 most cohesive clusters, as defined by Equa-tion (2).Results for the seven participating WSI systemsare reported in Tables 1 and 2.
The ten baselines,provided by the organisers of the task, are -8The average number of WordNet 3.1 senses for targetwords is 8.58.
MaxMax returns an average of 59.54 clusters fortarget words; merging results in an average of 21.86 clusters.691System/Baseline Jaccard IndexF-ScorePositionally Weighted TauF-ScoreWeighted NDCGF-ScoreUoS (top 3) 0.232 0.625 0.374AI-KU (r5-a1000) 0.244 0.642 0.332AI-KU 0.197 0.620 0.387Unimelb (50k) 0.213 0.620 0.371Unimelb (5p) 0.218 0.614 0.365UoS (# WN senses) 0.192 0.596 0.315AI-KU (a1000) 0.197 0.606 0.215Most Frequent Sense 0.552 0.560 0.718Senses Eq.
Weighted 0.149 0.787 0.436Senses, Avg.
Weight 0.187 0.613 0.499One sense 0.192 0.609 0.2881 of 2 random senses 0.220 0.627 0.2871 of 3 random senses 0.244 0.633 0.2871 of n random senses 0.290 0.638 0.2861 sense per instance 0.000 0.945 0.000SemCor, MFS 0.455 0.465 0.339SemCor, All Senses 0.149 0.559 0.489Table 1: Results for the WSD evaluation: all instances.?
SemCor, Most Frequent Sense (MFS): labelseach instance with the MFS in SemCor.9?
SemCor, All Senses: labels each instance withall SemCor senses, weighting each accordingto its frequency in SemCor.?
1 sense per instance: labels each instance witha unique induced sense, equivalent to the 1cluster per instance baseline of the SemEval-2010 WSI task (Manandhar et al 2010).?
One sense: labels each instance with the sameinduced sense, equivalent to the MFS baselineof the SemEval-2010 WSI task.?
Most Frequent Sense: labels each instance withthe sense that is most frequently selected by an-notators for all target word instances.?
Senses Avg.Weighted: labels each instance withall senses.
Each sense is scored according to itsaverage applicability rating from the gold stan-dard labelling.?
Senses Eq.
Weighted: labels each instance withall senses, equally weighted.9http://www.cse.unt.edu/?rada/downloads.html#semcor.?
1 of 2 random senses: labels each instance withone of two randomly selected induced senses.?
1 of 3 random senses: labels each instance withone of three randomly selected induced senses.?
1 of n random senses: labels each instance withone of n randomly selected induced senses,where n is the number of senses for the targetword in WordNet 3.1.10As noted by the task?s organisers11, the SemCorscores are the fairest baselines for participating sys-tems to compare against as they have no knowledgeof the test set sense distribution; the other baselinesare more challenging as they have knowledge of thetest set sense distribution and annotator grading.4.1 Summary Analysis of Evaluation ResultsGiven the number of evaluation metrics (16 in totalon the task website), individual analysis of systemresults per metric is beyond the scope of this paper.However, a ranking of systems may be obtained bytaking a summed ranked score; that is, by adding10For the random senses baselines, induced senses aremapped to WordNet 3.1 senses using the mapping proceduredescribed in Agirre and Soroa (2007).
The mapping is providedby the task organisers.11http://www.cs.york.ac.uk/semeval-2013/task13/index.php?id=results692System/Baseline Fuzzy NMI Fuzzy B-CubedPrecisionFuzzy B-CubedRecallFuzzy B-CubedF-ScoreUnimelb (50k) 0.060 0.524 0.447 0.483Unimelb (5p) 0.056 0.470 0.449 0.459AI-KU 0.065 0.838 0.254 0.390AI-KU (r5-a1000) 0.039 0.502 0.409 0.451UoS (top 3) 0.045 0.479 0.420 0.448UoS (# WN senses) 0.047 0.988 0.112 0.201AI-KU (a1000) 0.035 0.905 0.194 0.320One sense 0.000 0.989 0.455 0.6231 of 2 random senses 0.028 0.495 0.456 0.4741 of 3 random senses 0.018 0.329 0.455 0.3821 of n random senses 0.016 0.168 0.451 0.2451 sense per instance 0.071 0.000 0.000 0.000Table 2: Results for the cluster-based evaluation: all instances.up each system?s rankings over all evaluation met-rics.
The summed ranking finds that UoS (top 3)is placed first.
If the WSD and cluster-based eval-uations are considered separately, then UoS (top 3)is ranked, respectively, first and fourth.
However,this result is countered by the relatively poor per-formance of UoS (# WN senses), being ranked fifthoverall.
Considering baselines, UoS (top 3) equalsor surpasses the SemCor baseline scores 67% of thetime, and 54% for the more challenging baselines;UoS (# WN senses) scores, respectively, 50% and44%.All instances results were supplemented withsingle-sense (non-graded) and multi-sense (graded)splits at a later date.12 These results show (again,using a ranked score) that for single-sense instances,AI-KU is the best performing system, with UoS (top3) placed fifth, and UoS (# WN senses) last.
BothUoS (top 3) and UoS (# WN senses) surpass theSemCor MFS baseline, with UoS (top 3) surpassingor equalling the harder baselines 79% of the time,and UoS (# WN senses) 68% of the time.
For multi-sense instances, AI-KU is, again, the best perform-ing system, with UoS (# WN senses) placed sec-ond and UoS (top 3) sixth.
UoS (top 3) surpassesor equals the SemCor baseline scores 67% of thetime; UoS (# WN senses) 83% of the time.
UoS(top3) passes/equals, the harder baselines 63% of thetime, with UoS (# WN senses) doing so 67% of thetime.
These results are somewhat confounding as12http://www.cs.york.ac.uk/semeval-2013/task13/index.php?id=results (4/4/2013)one would expect a system that performs well in themain set of results (all instances), as UoS (top 3)does, to do so in at least one of the single-sense /multi-sense splits: this is clearly not the case.
In-deed, the results suggest that UoS (# WN senses),found to perform poorly over all instances, is bettersuited to the task?s aim of finding graded senses.5 ConclusionThis paper presented UoS, a graph-based WSI sys-tem that participated in SemEval-2013 Task 13:Word Sense Induction for Graded and Non-GradedSenses.
UoS applied the MaxMax clustering algo-rithm to find a set of sense clusters in a target wordgraph.
The number of clusters was found automati-cally through identification of root vertices of max-imal quasi-strongly connected subgraphs.
Evalua-tion results showed the UoS (top 3) system to bethe best performing system (all instances), if a sim-ple ranking over all evaluation measures is applied.The second system, UoS (# WN senses), performedpoorly, being ranked fifth out of the seven participat-ing WSI systems.
Note, however, that the number ofevaluation metrics applied, and the wide variabilityin each system?s performances over different met-rics and different splits of instance types, make itdifficult to judge exactly which system is the bestperforming.
Future research therefore aims to carryout a detailed analysis of the results and to assesswhether the measures applied in the evaluation ade-quately reflect the performance of WSI systems.693ReferencesEneko Agirre and Aitor Soroa.
2007.
SemEval-2007 Task 02: Evaluating Word Sense Inductionand Discrimination Systems.
In Proceedings of the4th International Workshop on Semantic Evaluations,pages 7?12.
Association for Computational Linguis-tics.
Prague, Czech Republic.Enrique Amigo?, Julio Gonzalo, Javier Artiles, and FelisaVerdejo.
2009.
A Comparison of Extrinsic Cluster-ing Evaluation Metrics Based on Formal Constraints.Information Retrieval, 12(4):461?486.Gerlof Bouma.
2009.
Normalized (Pointwise) MutualInformation in Collocation Extraction.
Proceedings ofGSCL, pages 31?40.T.
Dunning.
1993.
Accurate Methods for the Statistics ofSurprise and Coincidence.
Computational Linguistics,19(1):61?74.Katrin Erk and Diana McCarthy.
2009.
Graded WordSense Assignment.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 1, pages 440?449, Singapore.
As-sociation for Computational Linguistics.Adriano Ferraresi, Eros Zanchetta, Marco Baroni, andSilvia Bernardini.
2008.
Introducing and evaluatingukWaC, a very large web-derived corpus of english.In Proceedings of the 4th Web as Corpus Workshop(WAC-4) Can we beat Google, pages 47?54.
Mar-rakech, Morocco.David Hope and Bill Keller.
2013.
MaxMax: A Graph-Based Soft Clustering Algorithm Applied to WordSense Induction.
In A. Gelbukh, editor, CICLing2013, Part I, LNCS 7816, pages 368?381.
Springer-Verlag Berlin Heidelberg.
to appear.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:the 90% Solution.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL, pages57?60.
Association for Computational Linguistics.David Jurgens.
2012.
An Evaluation of Graded SenseDisambiguation Using Word Sense Induction.
Pro-ceedings of *SEM First Joint Conference on Lexi-cal and Computational Semantics, 2012.
Associationfor Computational Linguistics, pages 189?198.
Mon-treal,Canada.Andrea Lancichinetti, Santo Fortunato, and Ja?nosKerte?sz.
2009.
Detecting the Overlapping and Hier-archical Community Structure in Complex Networks.New Journal of Physics, 11(3):033015.Suresh Manandhar, Ioannis P. Klapaftis, Dmitriy Dli-gach, and Sameer S. Pradhan.
2010.
SemEval-2010Task 14: Word Sense Induction and Disambiguation.In Proceedings of the 5th International Workshop onSemantic Evaluation, pages 63?68.
Association forComputational Linguistics.
Uppsala, Sweden.George A. Miller, Richard Beckwith, Christiane Fell-baum, Derek Gross, and Katherine J. Miller.
1990.
In-troduction to WordNet: An On-line Lexical Database.International Journal of Lexicography, 3(4):235.Roberto Navigli.
2009.
Word Sense Disambiguation: ASurvey.
ACM Computing Surveys (CSUR), 41(2):10.Roberto Navigli.
2012.
A Quick Tour of Word SenseDisambiguation, Induction and Related Approaches.In SOFSEM 2012: Theory and Practice of ComputerScience, volume 7147 of Lecture Notes in ComputerScience, pages 115?129.
Springer Berlin / Heidelberg.Pang-Ning Tan, Michael Steinbach, and Vipin Kumar.2006.
Introduction to Data Mining.
Pearson AddisonWesley.K.
Thulasiraman and N.S.
Swamy.
1992.
Graphs: The-ory and Algorithms.
Wiley.Ludwig Wittgenstein.
1953.
Philosophical Investiga-tions.
Blackwell.694
