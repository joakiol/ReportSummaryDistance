Proceedings of NAACL-HLT 2013, pages 1?11,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsModel With Minimal Translation Units, But Decode With PhrasesNadir Durrani?University of Edinburghdnadir@inf.ed.ac.ukAlexander Fraser Helmut SchmidUniversity of Stuttgartfraser,schmid@ims.uni-stuttgart.deAbstractN-gram-based models co-exist with theirphrase-based counterparts as an alternativeSMT framework.
Both techniques have prosand cons.
While the N-gram-based frame-work provides a better model that capturesboth source and target contexts and avoidsspurious phrasal segmentation, the ability tomemorize and produce larger translation unitsgives an edge to the phrase-based systems dur-ing decoding, in terms of better search per-formance and superior selection of transla-tion units.
In this paper we combine N-gram-based modeling with phrase-based decoding,and obtain the benefits of both approaches.Our experiments show that using this combi-nation not only improves the search accuracyof the N-gram model but that it also improvesthe BLEU scores.
Our system outperformsstate-of-the-art phrase-based systems (Mosesand Phrasal) and N-gram-based systems bya significant margin on German, French andSpanish to English translation tasks.1 IntroductionStatistical Machine Translation advanced fromword-based models (Brown et al 1993) towardsmore sophisticated models that take contextual in-formation into account.
Phrase-based (Och andNey, 2004; Koehn et al 2003) and N-gram-based(Marin?o et al 2006) models are two instances ofsuch frameworks.
While the two models have somecommon properties, they are substantially different.
?Much of the work presented here was carried out while thefirst author was at the University of Stuttgart.Phrase-based systems employ a simple and effec-tive machinery by learning larger chunks of trans-lation called phrases1.
Memorizing larger units en-ables the phrase-based model to learn local depen-dencies such as short reorderings, idioms, insertionsand deletions, etc.
The model however, has the fol-lowing drawbacks: i) it makes independence as-sumptions over phrases ignoring the contextual in-formation outside of phrases ii) it has issues han-dling long-distance reordering iii) it has the spuriousphrasal segmentation problem which allows multi-ple derivations of a bilingual sentence pair havingdifferent model scores for each segmentation.Modeling with minimal translation units helps ad-dress some of these issues.
The N-gram-based SMTframework is based on tuples.
Tuples are mini-mal translation units composed of source and targetcepts2.
N-gram-based models are Markov modelsover sequences of tuples (Marin?o et al 2006; Cregoand Marin?o, 2006) or operations encapsulating tu-ples (Durrani et al 2011).
This mechanism has sev-eral useful properties.
Firstly, no phrasal indepen-dence assumption is made.
The model has accessto both source and target context outside of phrases.Secondly the model learns a unique derivation of abilingual sentence given its alignment, thus avoidingthe spurious segmentation problem.Using minimal translation units, however, resultsin a higher number of search errors because of i)1A phrase-pair in PBSMT is a sequence of source and targetwords that is translation of each other, and is not necessarily alinguistic constituent.
Phrases are built by combining minimaltranslation units and ordering information.2A cept is a group of words in one language that is translatedas a minimal unit in one specific context (Brown et al 1993).1poor translation selection, ii) inaccurate future-costestimates and iii) incorrect early pruning of hypothe-ses that would produce better model scores if al-lowed to continue.
In order to deal with theseproblems, search is carried out only on a graphof pre-calculated orderings, and ad-hoc reorderinglimits are imposed to constrain the search space(Crego et al 2005; Crego and Marin?o, 2006), ora higher beam size is used in decoding (Durraniet al 2011).
The ability to memorize and pro-duce larger translation chunks during decoding, onthe other hand, gives a distinct advantage to thephrase-based system during search.
Phrase-basedsystems i) have access to uncommon translations,ii) do not require higher beam sizes, iii) have moreaccurate future-cost estimates because of the avail-ability of phrase-internal language model contextbefore search is started.
To illustrate this considerthe German-English phrase-pair ?scho?
ein Tor ?scored a goal?, composed from the tuples (cept-pairs) ?scho?
?
scored?, ?ein ?
a?
and ?Tor ?
goal?.It is likely that the N-gram system does not havethe tuple ?scho?
?
scored?
in its n-best translationoptions because ?scored?
is an uncommon transla-tion for ?scho??
outside the sports domain.
Even if?scho?
?
scored?
is hypothesized, it will be rankedquite low in the stack until ?ein?
and ?Tor?
are gen-erated in the next steps.
A higher beam is requiredto prevent it from getting pruned.
Phrase-based sys-tems, on the other hand, are likely to have access tothe phrasal unit ?scho?
ein Tor ?
scored a goal?
andcan generate it in a single step.
Moreover, a more ac-curate future-cost estimate can be computed becauseof the available context internal to the phrase.In this work, we extend the N-gram model, basedon operation sequences (Durrani et al 2011), touse phrases during decoding.
The main idea is tostudy whether a combination of modeling with min-imal translation units and using phrasal informationduring decoding helps to solve the above-mentionedproblems.The remainder of this paper is organized as fol-lows.
The next two sections review phrase-basedand N-gram-based SMT.
Section 2 provides a com-parison of phrase-based and N-gram-based SMT.Section 3 summarizes the operation sequence model(OSM), the main baseline for this work.
Section4 analyzes the search problem when decoding withFigure 1: Different Segmentations of a Bilingual Sen-tence Pairminimal units.
Section 5 discusses how informationavailable in phrases can be used to improve searchperformance.
Section 6 presents the results of thiswork.
We conducted experiments on the German-to-English and French-to-English translation tasks andfound that using phrases in decoding improves bothsearch accuracy and BLEU scores.
Finally we com-pare our system with two state-of-the-art phrase-based systems (Moses and Phrasal) and two state-of-the-art N-gram-based systems (Ncode and OSM)on standard translation tasks.2 Previous WorkPhrase-based and N-gram-based SMT are alter-native frameworks for string-to-string translation.Phrase-based SMT segments a bilingual sentencepair into phrases that are continuous sequences ofwords (Och and Ney, 2004; Koehn et al 2003)or discontinuous sequences of words (Galley andManning, 2010).
These phrases are then reorderedthrough a lexicalized reordering model that takesinto account the orientation of a phrase with respectto its previous phrase (Tillmann and Zhang, 2005)or block of phrases (Galley and Manning, 2008).There are several drawbacks of the phrase-basedmodel.
Firstly it makes an independence assump-tion over phrases, according to which phrases aretranslated independently of each other, thus ignor-ing the contextual information outside of the phrasalboundary.
This problem is corrected by the monolin-gual language model that takes context into account.But often the language model cannot compensate forthe dispreference of the translation model for non-local dependencies.
The second problem is that themodel is unaware of the actual phrasal segmentationof a sentence during training.
It therefore learns allpossible ways of segmenting a bilingual sentence.Different segmentations of a bilingual sentence re-2sult in different probability scores for the translationand reordering models, causing spurious ambiguityin the model.
See Figure 1.
In the first segmentation,the model learns the lexical and reordering proba-bilities of the phrases ?sie wu?rden ?
they would?and ?gegen ihre kampagne abstimmen ?
vote againstyour campaign?.
In the second segmentation, themodel learns the lexical and reordering probabilitiesof the phrases ?sie ?
they?
?wu?rden ?
would?, ?ab-stimmen ?
vote?, ?gegen ihre kampagne ?
againstyour campaign?.
Both segmentations result in dif-ferent translation and reordering scores.
This kindof ambiguity in the model subsequently results inthe presence of many different equivalent segmen-tations in the search space.
Also note that the twosegmentations contain different information.
Fromthe first segmentation the model learns the depen-dency between the verb ?abstimmen ?
vote?
and thephrase ?gegen ihre kampagne ?
against your cam-paign?.
The second segmentation allows the modelto capture the reordering of the complex verb pred-icate ?wu?rden ?
would?
and ?abstimmen ?
vote?
bylearning that the verb ?abstimmen ?
vote?
is discon-tinuous with respect to the auxiliary.
This informa-tion cannot be captured in the first segmentation be-cause of the phrasal independence assumption andstiff phrasal boundaries.
The model loses one of thedependencies depending upon which segmentationit chooses during decoding.N-gram-based SMT is an instance of a jointmodel that generates source and target strings to-gether in bilingual translation units called tuples.Tuples are essentially phrases but they are atomicunits that cannot be decomposed any further.
Thiscondition of atomicity results in a unique segmen-tation of the bilingual sentence pair given its align-ments.
The model does not make any phrasal inde-pendence assumption and generates a tuple by look-ing at a context of n ?
1 previous tuples (or opera-tions).
This allows the N-gram model to model allthe dependencies through a single derivation.The main drawback of N-gram-based SMT is itspoor search mechanism which is inherent from us-ing minimal translation units during search.
Decod-ing with tuples has problems with a high numberof search errors caused by lower translation cover-age, inaccurate future-cost estimation and pruningof correct hypotheses (see Section 4.2 for details).Crego and Marin?o (2006) proposed a way to couplereordering and search through POS-based rewriterules.
These rules are learned during training whenunits with crossing alignments are unfolded throughsource linearization to form minimal tuples.
For ex-ample, in Figure 1, the N-gram-based MT will lin-earize the word sequence ?gegen ihre kampagne ab-stimmen?
to ?abstimmen gegen ihre kampagne?, sothat it is in the same order as the English words.It also learns a POS-rule ?IN PRP NN VB ?
VBIN PRP NN?.
The POS-based rewrite rules serveto precompute the orderings that are hypothesizedduring decoding.
Coupling reordering and searchallows the N-gram model to arrange hypotheses in2m stacks (for an m word source sentence), eachcontaining hypotheses that cover exactly the sameforeign words.
This removes the need for future-cost estimation3.
Secondly, memorizing POS-basedrules enables phrase-based like reordering, howeverwithout lexical selection.
There are three drawbacksof this approach.
Firstly, lexical generation and re-ordering are decoupled.
Search is only performed ona small number of reorderings, pre-calculated usingthe source side and completely ignoring the target-side.
And lastly, the POS-based rules face data spar-sity problems especially in the case of long distancereorderings.Durrani et al(2011) recently addressed theseproblems by proposing an operation sequence N-gram model which strongly couples translation andreordering, hypothesizes all possible reorderingsand does not require POS-based rules.
Represent-ing bilingual sentences as a sequence of operationsenables them to memorize phrases and lexical re-ordering triggers like PBSMT.
However, using min-imal units during decoding and searching over allpossible reorderings means that hypotheses can nolonger be arranged in 2m stacks.
The problem ofinaccurate future-cost estimates resurfaces resultingin more search errors.
A higher beam size of 500 istherefore used to produce translation units in com-parison to phrase-based systems.
This, however,still does not eliminate all search errors.
This pa-per shows that using phrases instead of cepts in de-3Using m stacks with future-cost estimation is a more effi-cient solution but is not used ?due to the complexity of accu-rately computing these estimations in the N-gram architecture?
(Crego et al 2011).3coding improves the search accuracy and translationquality.
It also shows that using some phrasal in-formation in cept-based decoding captures some ofthese improvements.3 Operation Sequence ModelThe N-gram model with integrated reordering mod-els a sequence of operations obtained through thetransformation of a bilingual sentence pair.
An op-eration can either be to i) generate a sequence ofsource and target words, ii) to insert a gap as a place-holder for skipped words, iii) or to jump forward andbackward in a sentence to translate words discon-tinuously.
The translate operation Generate(X,Y)encapsulates the translation tuple (X,Y).
It gener-ates source and target translations simultaneously4.This is similar to N-gram-based SMT except thatthe tuples in the N-gram-based model are generatedmonotonically, whereas in this case lexical genera-tion and reordering information is strongly coupledin an operation sequence.Consider the phrase pair:The model memorizes itthrough the sequence:Generate(Wie, What is)?
Gap?
Generate (Sie,your)?
Jump Back (1)?Generate (heissen, name)Let O = o1, .
.
.
, oj?1 be a sequence of opera-tions as hypothesized by the translator to generatethe bilingual sentence pair ?F,E?
with an alignmentfunction A.
The translation model is defined as:p(F,E,A) = p(oJ1 ) =J?j=1p(oj |oj?n+1, ..., oj?1)where n indicates the amount of context used.
Thetranslation model is implemented as an N-grammodel of operations using SRILM-Toolkit (Stol-cke, 2002) with Kneser-Ney smoothing.
A 9-grammodel is used.
Several count-based features such asgap and open gap penalties and distance-based fea-tures such as gap-width and reordering distance areadded to the model, along with the lexical weightingand length penalty features in a standard log-linearframework (Durrani et al 2011).4The generation is carried out in the order of the target lan-guage E.4 Search4.1 Overview of Decoding FrameworkThe decoding framework used in the operation se-quence model is based on Pharaoh (Koehn, 2004a).The decoder uses beam search to build up the trans-lation from left to right.
The hypotheses are ar-ranged in m stacks such that stack i maintains hy-potheses that have already translated i many foreignwords.
The ultimate goal is to find the best scor-ing hypothesis, that has translated all the words inthe foreign sentence.
The overall process can beroughly divided into the following steps: i) extrac-tion of translation units ii) future-cost estimation, iii)hypothesis extension iv) recombination and pruning.During the hypothesis extension each extractedphrase is translated into a sequence of operations.The reordering operations (gaps and jumps) are gen-erated by looking at the position of the translator,the last foreign word generated etc.
(Refer to Algo-rithm 1 in Durrani et al(2011)).
The probability ofan operation depends on the n ?
1 previous opera-tions.
The model backs-off to the smaller n-gramsof operations if the full history is unknown.
We useKneser-Ney smoothing to handle back-off5.4.2 Drawbacks of Cept-based DecodingOne of the main drawbacks of the operation se-quence model is that it has a more difficult searchproblem than the phrase-based model.
The opera-tion model, although based on minimal translationunits, can learn larger translation chunks by mem-orizing a sequence of operations.
However, usingcepts during decoding has the following drawbacks:i) the cept-based decoder does not have access toall the translation units that a phrase-based decoderuses as part of a larger phrase.
ii) it requires a higherbeam size to prevent early pruning of better hypothe-ses that lead toward higher model scores when al-lowed to continue and iii) it uses worse future-costestimates than the phrase-based decoder.Recall the example from the last section.
Forthe cept-based decoder to generate the same phrasaltranslation, it requires three separate tuple transla-tions ?Wie ?
what is?, ?Sie ?
your?
and ?hei?en ?name?.
Here we are faced with three challenges.5We also tried Witten-Bell and Good Turing methods of dis-counting and found Kneser-Ney smoothing to produce the bestresults.4Translation Coverage: The first problem is thatthe N-gram model does not have the same cov-erage of translation options.
The English cepts?what is?, ?your?
and ?name?
are not good candi-date translations for the German cepts ?Wie?, ?Sie?and ?hei?en?, respectively.
When extracting tupletranslations for these cepts from the Europarl datafor our system, the tuple ?Wie ?
what is?
is ranked124th, ?hei?en ?
name?
is ranked 56th, and ?Sie ?your?
is ranked 9th in the list of n-best translationcandidates.
Typically only the 20 best translationoptions are used, to reduce the decoding time, andsuch phrasal units with less frequent cept transla-tions are never hypothesized in the N-gram-basedsystems.
The phrase-based system on the other handcan extract the phrase ?Wie hei?en Sie ?
what isyour name?
even if it is observed only once dur-ing training.
A similar problem is also reported inCosta-jussa` et al(2007).
When trying to repro-duce the sentences in the n-best translation outputof the phrase-based system, the N-gram-based sys-tem was only able to produce 37.5% of the sen-tences in the Spanish-to-English and 37.2% in theEnglish-to-Spanish translation tasks.
In compar-ison the phrase-based system was able to repro-duce 57.5% and 48.6% of the sentences in the n-best translation output of the Spanish-to-English andEnglish-to-Spanish N-gram-based systems.Larger Beam Size: A related problem is that ahigher beam size is required in cept-based decod-ing to prevent uncommon translations from gettingpruned.
The phrase-based system can generate thephrase-pair ?Wie hei?en Sie ?
what is your name?in a single step placing it directly into the stack threewords to the right.
The cept-based decoder generatesthis phrase in three stacks with the tuple translations?Wie ?
What is?, ?Sie ?
your?
and ?hei?en ?
name?.A very large stack size is required during decodingto prevent the pruning of ?Wie ?
What is?
which isranked quite low in the stack until the tuple ?Sie ?your?
is hypothesized in the next stack.
Costa-jussa`et al(2007) reports a significant drop in the perfor-mance of N-gram-based SMT when a beam size of10 is used instead of 50 in their experiments.
For the(cept-based) operation sequence model, Durrani etal.
(2011) required a stack size of 500.
In compari-son, the translation quality achieved by phrase-basedSMT remains the same when varying the beam sizebetween 5 and 50.Future-Cost Estimation: A third problem iscaused by inaccurate future-cost estimation.
Usingphrases helps phrase-based SMT to better estimatethe future language model cost because of the largercontext available, and allows the decoder to capturelocal (phrase-internal) reorderings in the future cost.In comparison the future cost for tuples is mostly un-igram probabilities.
The future-cost estimate for thephrase pair ?Wie hei?en Sie ?
What is your name?is estimated by calculating the cost of each feature.The language model cost, for example, is estimatedin the phrase-based system as follows:plm = p(What)?
p(is|What)?
p(your|What is)?
p(name|What is your)The cost of the direct phrase translation probabil-ity, one of the features used in the phrase translationmodel, is estimated as:ptm = p(What is your name|Wie hei?en Sie)Phrase-based SMT is aware during the prepro-cessing step that the words ?Wie hei?en Sie?
maybe translated as a phrase.
This is helpful for estimat-ing a more accurate future cost because the phrase-internal context is already available.
The same is nottrue for the operation sequence model, to which onlyminimal units are available.
The operation modeldoes not have the information that ?Wie hei?en Sie?may be translated as a phrase during decoding.
Thefuture-cost estimate available to the operation modelfor the span covering ?Wie hei?en Sie?
will have un-igram probabilities for both the translation and lan-guage model:plm = p(What)?
p(is|What)?
p(your)?
p(name)ptm = p(Generate(Wie, What is))?
p(Generate(hei?en,name))?
p(Generate(Sie, your))Thus the future-cost estimate in the operationmodel is much worse than that of the phrase-basedmodel.
The poor future-cost estimation leads tosearch errors, causing a drop in the translation qual-ity.
A more accurate future-cost estimate for thetranslation model cost would be:5ptm = p(Generate(Wie,What is))?
p(Insert Gap|C)?
p(Generate(Sie,your)|C)?
p(Jump Back(1)|C)p(Generate(hei?en,name)|C)where C is the context, i.e., the n?1 previously gen-erated operations.
The future-cost estimates com-puted in this manner are much more accurate be-cause they not only consider context, but also takethe reordering operations into account.5 N-gram Model with Phrase-basedDecodingIn the last section we discussed the disadvantages ofusing cepts during search in a left-to-right decodingframework.
We now define a method to empiricallystudy the mentioned drawbacks and whether usinginformation available in phrase-pairs during decod-ing can help improve search accuracy and translationquality.5.1 TrainingWe extended the training steps in Durrani et al(2011) to extract a phrase lexicon from the paral-lel data.
We extract all phrase pairs of length 6 andbelow, that are consistent (Och et al 1999) withthe word alignments.
Only continuous phrases asused in a traditional phrase-based system are ex-tracted thus allowing only inside-out (Wu, 1997)type of alignments.
The future cost of each fea-ture component used in the log-linear model is cal-culated.
The operation sequence required to hypoth-esize each phrase is generated and its future cost iscalculated.
The future costs of other features suchas language models, lexicalized probability features,etc.
are also estimated.
The estimates of the count-based reordering penalties (gap penalty and opengap penalty) and the distance-based features (gap-width and reordering distance) could not be esti-mated previously with cepts but are available whenusing phrases.5.2 DecodingWe extended the decoder developed by Durrani et al(2011) and tried three ideas.
In our primary experi-ments we enabled the decoder to use phrases insteadof cepts.
This allows the decoder to i) use phrase-internal context when computing the future-cost es-timates, ii) hypothesize translation options not avail-able to the cept-based decoder iii) cover multiplesource words in a single step subsequently improv-ing translation coverage and search.
Note that us-ing phrases instead of cepts during decoding, doesnot reintroduce the spurious phrasal segmentationproblem as is present in the phrase-based system,because the model is built on minimal units whichavoids segmentation ambiguity.
Different compo-sitions of the same phrasal unit lead to exactly thesame model score.
We therefore do not create anyalternative compositions of the same phrasal unitduring decoding.
This option is not available inphrase-based decoding, because an alternative com-position may lead towards a better model score.In our secondary set of experiments, we usedcept-based decoding but modified the decoder touse information available from the phrases extractedfor the test sentences.
Firstly, we used future-costestimates from the extracted phrases (see systemcept.500.fc in Table1).
This however, leads to in-consistency in the cases where the future cost is es-timated from some phrasal unit that cannot be gen-erated through the available cept translations.
Forexample, say the best cost to cover the sequence?Wie hei?en Sie?
is given by the phrase ?What isyour name?.
The 20-best translation options in cept-based system, however, do not have tuples ?Wie ?What?
and ?hei?en ?
name?.
To remove this dis-crepancy, we add all such tuples that are used inthe extracted phrases, to the list of extracted cepts(system cept.500.fc.t).
We also studied how muchgain we obtain by only adding tuples from phrasesand using cept-based future-cost estimates (systemcept.500.t).5.3 Evaluation MethodTo evaluate our modifications we apply a simplestrategy.
We hold the model constant and changethe search to use the baseline decoder, which usesminimal translation units, or the modified decodersthat use phrasal information during decoding.
Themodel parameters are optimized by running MERT(minimum error rate training) for the baseline de-coder on the dev set.
After we have the optimizedweights, we run the baseline decoder and our mod-ifications on the test.
Note that because all the de-coding runs use the same feature vector, the model6stays constant, only search changes.
This allows usto compare different decoding runs, obtained usingthe same parameters, but different search strategies,in terms of model scores.
We compute a search ac-curacy and translation quality for each run.Search accuracy is computed by comparing trans-lation hypotheses from the different decoding runs.We form a collection of the best scoring hypothesesby traversing through all the runs and selecting thesentences with highest model score.
For each inputsentence we select a single best scoring hypothesis.The best scoring hypothesis can be contributed fromseveral runs.
In this case all these runs will be givena credit for that particular sentence when computingthe search accuracy.
The search accuracy of a decod-ing run is defined as the percentage of hypothesesthat were contributed from this run, when forming alist of best scoring hypotheses.
For example, for atest set of 1000 sentences, the accuracy of a decod-ing run would be 30% if it was able to produce thebest scoring hypothesis for 300 sentences in the testset.
Translation quality is measured through BLEU(Papineni et al 2002).6 Experimental SetupWe initially experimented with two language pairs:German-to-English (G-E) and French-to-English (F-E).
We trained our system and the baseline sys-tems on most of the data6 made available for thetranslation task of the Fourth Workshop on Statis-tical Machine Translation.7 We used 1M bilin-gual sentences, for the estimation of the transla-tion model and 2M sentences from the monolingualcorpus (news commentary) which also contains theEnglish part of the bilingual corpus.
Word align-ments are obtained by running GIZA++ (Och andNey, 2003) with the grow-diag-final-and (Koehn etal., 2005) symmetrization heuristic.
We follow thetraining steps described in Durrani et al(2011), con-sisting of i) post-processing the alignments to re-move discontinuous and unaligned target cepts, ii)conversion of bilingual alignments into operationsequences, iii) estimation of the n-gram languagemodels.6We did not use all the available data due to scalability is-sues.
The scores reported are therefore well below those ob-tained by the systems submitted to the WMT evaluation series.7http://www.statmt.org/wmt09/translation-task.html6.1 Search Accuracy ResultsWe divided our evaluation into two halves.
Inthe first half we carried out experiments to mea-sure search accuracy and translation quality ofour decoders against the baseline cept-based OSM(cept.500) that uses minimal translation units with astack size of 500.
We used the version of the cept-based OSM system that does not allow discontinu-ous8 source cepts.
To increase the speed of the sys-tem we used a hard reordering limit of 159, in thebaseline decoder and our modifications, disallowingjumps that are beyond 15 words from the first opengap.
For each extracted cept or phrase 10-best trans-lation options are extracted.Using phrases in search reduces the decodingspeed.
In order to make a fair comparison, both thephrase-based and the baseline cept-based decodersshould be allowed to run for the same amount oftime.
We therefore reduced the stack size in thephrase-based decoder so that it runs in the sameamount of time as the cept-based decoder.
We foundthat using a stack size of 20010 for the phrase-baseddecoder was comparable in speed to using a stack-size of 500 in the cept-based decoding.We first tuned the baseline on dev11 to obtain anoptimized weight vector.
We then ran the baselineand our decoders as discussed in Section 5.2 on thedev-test.
Then we repeated this experiment by tun-ing the weights with our phrase-based decoder (us-ing a stack size of 100) and ran all the decoders againusing the new weights.Table 1 shows the average search accuracies andBLEU scores of the two experiments.
Using phrasesduring decoding in the G-E experiments resultedin a statistically significant12 0.69 BLEU pointsgain comparing our best system phrase.200 with thebaseline system cept.500.
We mark a result as sig-8Discontinuous source-side units did not lead to any im-provements in (Durrani et al 2011) and increased the decodingtimes by multiple folds.
We also found these to be less useful.9Imposing a hard reordering limit significantly reduced thedecoding time and also slightly increased the BLEU scores.10Higher stack sizes leads to improvement in model scoresfor both German-English and French-English and slight im-provement of BLEU in the case of the former.11We used news-dev2009a as dev and news-dev2009b as dev-test and tuned the weights with Z-MERT (Zaidan, 2009).12We use bootstrap resampling (Koehn, 2004b) to test ourresults against the baseline result.7System German FrenchAcc.
BLEU Acc.
BLEUBaseline System cept.stack-sizecept.50 25.95% 19.50 42.10% 21.44cept.100 30.04% 19.79 47.32% 21.70cept.200 35.17% 19.98 51.47% 21.82cept.500 41.56% 20.14 54.93% 21.87Our Cept-based Decoderscept.500.fc 48.44% 20.52* 54.73% 21.86cept.500.t 52.24% 20.34 67.95% 22.00cept.500.fc.t 61.81% 20.53* 67.76% 21.96Our Phrase-based Decodersphrase.50 58.88% 20.58* 80.83% 22.04phrase.100 69.85% 20.73* 88.34% 22.13phrase.200 79.71% 20.83* 92.93% 22.17*Table 1: Search Accuracies (Acc.)
and BLEU scores ofthe Baseline and Our Decoders with different Stack Sizes(fc = Future Cost Estimated from Phrases, t = Cept Trans-lation Options enriched from Phrases)nificant if the improvement shown by our decoderover the baseline decoder (cept.500) is significant atthe p ?
0.05 level, in both the runs.
All the out-puts that show statistically significant improvementsover the baseline decoder (cept.500) in Table 1 aremarked with an asterisk.The search accuracy of our best system(phrase.200), in G-E experiments is roughly80%, which means that 80% of the times thephrase-based decoder (using stack size 200) wasable to produce the same model score or a bettermodel score than the cept-based decoders (usinga stack size of 500).
Our F-E experiments alsoshowed improvements in BLEU and model scores.The search accuracy of our best system phrase.200is roughly 93% as compared with 55% in thebaseline decoder (cept.500) giving a BLEU pointgain of +0.30 over the baseline.Our modifications to the cept-based decoder alsoshowed improvements.
We found that extendingthe cept translation table (cept.500.t) using phraseshelps both in G-E and F-E experiments by extend-ing the list of n-best translation options by 18% and18.30% respectively.
Using future costs estimatedfrom phrases (cept.500.fc) improved both search ac-curacy and BLEU scores in G-E experiments, butdoes not lead to any improvements in the F-E ex-periments, as both BLEU and model scores dropslightly.
We looked at a few examples where themodel score dropped and found that in these cases,the best scoring hypotheses are ranked very low ear-lier in the decoding and make their way to the topgradually in subsequent steps.
A slight difference inthe future-cost estimate prunes these hypotheses inone or the other decoder.
We found future cost tobe more critical in G-E than F-E experiments.
Thiscan be explained by the fact that more reordering isrequired in G-E and it is necessary to account for thereordering operations and jump-based features (gap-based penalties, reordering distance and gap-width)in the future-cost estimation.
F-E on the other handis largely monotonic except for a few short distancereorderings such as flipping noun and adjective.6.2 Comparison with other Baseline SystemsIn the second half of our evaluation we comparedour best system phrase.200 with the baseline sys-tem cept.500, and other state-of-the-art phrase-basedand N-gram-based systems on German-to-English,French-to-English, and Spanish-to-English tasks13.We used the official evaluation data (news-test sets)from the Statistical Machine Translation Workshops2009-2011 for all three language pairs (German,Spanish and French).
The feature weights for all thesystems are tuned using the dev set news-dev2009a.We separately tune the baseline system (cept.500)and the phrase-based system (phrase.200) and do nothold the lambda vector constant like before.Baseline Systems: We also compared our systemwith i) Moses (Koehn et al 2007), ii) Phrasal14 (Ceret al 2010), and iii) Ncode (Crego et al 2011).We used the default stack sizes of 100 forMoses15, 200 for Phrasal, 25 for Ncode (with 2mstacks).
A 5-gram English language model is used.Both phrase-based systems use 20-best phrases fortranslation, Ncode uses 25-best tuple translations.The training and test data for Ncode was tagged us-ing TreeTagger (Schmid, 1994).
All the baselinesystems used lexicalized reordering model.
A hardreordering limit16 of 6 words is used as a default in13We did not include the results of Spanish in the previoussection due to space limitations but these are similar to those ofthe French-to-English translation task.14Phrasal provides two extensions to Moses: i) hierarchicalreordering model (Galley and Manning, 2008) and ii) discon-tinuous phrases (Galley and Manning, 2010).15Using stacks sizes from 200?1000 did not improve results.16We tried to increase the distortion limit in the baseline sys-8both the baseline phrase-based systems.
Amongstthe other defaults we retained the hard source gappenalty of 15 and a target gap penalty of 7 in Phrasal.We provide Moses and Ncode with the same post-edited alignments17 from which we removed target-side discontinuities.
We feed the original alignmentsto Phrasal because of its ability to learn discontinu-ous source and target phrases.
All the systems useMERT for the optimization of the weight vector.Ms Pd Nc C500 P200German-to-EnglishMT09 18.73* 19.00* 18.37* 19.06* 19.66MT10 18.58* 18.96* 18.64* 19.12* 19.70MT11 17.38* 17.58* 17.49* 17.87* 18.19French-to-EnglishMT09 24.61* 24.73* 24.28* 24.94* 25.27MT10 23.69* 23.09* 23.96 23.90* 24.25MT11 25.17* 25.55* 24.92* 25.40* 25.92Spanish-to-EnglishMT09 24.38* 24.63 24.72 24.48* 24.72MT10 25.55* 25.66* 25.87 25.68* 26.10MT11 25.72* 26.17* 26.36* 26.48 26.67Table 2: Comparison on 3-Test Sets ?
Ms = Moses, Pd= Phrasal (Discontinuous Phrases), Nc = Ncode, C500 =Cept.500, P200 = Phrase.200Table 2 compares the performance of our phrase-based decoder against the baselines.
Our systemshows an improvement over all the baseline systemsfor the G-E pair, in 11 out of 12 cases in the F-Epair and in 8 out of 12 cases in the S-E languagepair.
We mark a baseline with ?*?
to indicate thatour decoder shows an improvement over this base-line result which is significant at the p ?
0.05 level.7 Conclusion and Future WorkWe proposed a combination of using a model basedon minimal units and decoding with phrases.
Mod-eling with minimal units enables us to learn localand non-local dependencies in a unified manner andavoid spurious segmentation ambiguities.
However,using minimal units also in the search presents asignificant challenge because of the poor transla-tion coverage, inaccurate future-cost estimates andtems to 15 (in G-E experiments) as used in our systems but theresults dropped significantly in case of Moses and slightly forPhrasal so we used the default limits for both decoders.17Using post-processed alignments gave slightly better re-sults than the original alignments for these baseline systems.Details are omitted due to space limitation.the pruning of the correct hypotheses.
Phrase-basedSMT on the other hand overcomes these drawbacksby using larger translation chunks during search.However, the drawback of the phrase-based model isthe phrasal independence assumption, spurious am-biguity in segmentation and a weak mechanism tohandle non-local reorderings.
We showed that com-bining a model based on minimal units with phrase-based decoding can improve both search accuracyand translation quality.
We also showed that thephrasal information can be indirectly used in cept-based decoding with improved results.
We testedour system against the state-of-the-art phrase-basedand N-gram-based systems, for German-to-English,French-to-English, and Spanish-to-English for threestandard test sets.
Our system showed statisticallysignificant improvements over all the baseline sys-tems in most of the cases.
We have shown the bene-fits of using phrase-based search with a model basedon minimal units.
In future work, we would like tostudy whether a phrase-based system like Moses orPhrasal can profit from an OSM-style or N-gram-style feature.
Feng et al(2010) previously showedthat adding a linearized source-side language modelin a phrase-based system helped.
It would alsobe interesting to study whether the insight of us-ing minimal units for modeling and phrase-basedsearch would hold for hierarchical SMT.
Vaswani etal.
(2011) recently showed that a Markov model overthe derivation history of minimal rules can obtain thesame translation quality as using grammars formedwith composed rules.AcknowledgmentsWe would like to thank the anonymous reviewersfor their helpful feedback and suggestions.
NadirDurrani and Alexander Fraser were funded byDeutsche Forschungsgemeinschaft grant Models ofMorphosyntax for Statistical Machine Translation.Nadir Durrani was partially funded by the EuropeanUnion Seventh Framework Programme (FP7/2007-2013) under grant agreement n ?
287658.
HelmutSchmid was supported by Deutsche Forschungsge-meinschaft grant SFB 732.
This work was sup-ported in part by the IST Programme of the Eu-ropean Community, under the PASCAL2 Networkof Excellence, IST-2007-216886.
This publicationonly reflects the authors?
views.9ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and R. L. Mercer.
1993.
The Mathematicsof Statistical Machine Translation: Parameter Estima-tion.
Computational Linguistics, 19(2):263?311.Daniel Cer, Michel Galley, Daniel Jurafsky, and Christo-pher D. Manning.
2010.
Phrasal: A Statistical Ma-chine Translation Toolkit for Exploring New modelFeatures.
In Proceedings of the NAACL HLT 2010Demonstration Session, pages 9?12, Los Angeles,California, June.Marta R. Costa-jussa`, Josep M. Crego, David Vilar,Jose?
A.R.
Fonollosa, Jose?
B. Marin?o, and HermannNey.
2007.
Analysis and System Combination ofPhrase- and N-Gram-Based Statistical Machine Trans-lation Systems.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics;Companion Volume, Short Papers, pages 137?140,Rochester, New York, April.Josep M. Crego and Jose?
B. Marin?o.
2006.
ImprovingStatistical MT by Coupling Reordering and Decoding.Machine Translation, 20(3):199?215.Josep M. Crego, Jose?
B. Marin?o, and Adria` de Gispert.2005.
Reordered Search and Unfolding Tuples for N-Gram-Based SMT.
In Proceedings of the 10th Ma-chine Translation Summit (MT Summit X), pages 283?289, Phuket, Thailand.Josep M. Crego, Franc?ois Yvon, and Jose?
B. Marin?o.2011.
Ncode: an Open Source Bilingual N-gram SMTToolkit.
The Prague Bulletin of Mathematical Lin-guistics, (96):49?58.Nadir Durrani, Helmut Schmid, and Alexander Fraser.2011.
A Joint Sequence Translation Model with Inte-grated Reordering.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies, pages 1045?1054, Portland, Oregon, USA, June.Minwei Feng, Arne Mauser, and Hermann Ney.
2010.A Source-side Decoding Sequence Model for Statisti-cal Machine Translation.
In Conference of the Associ-ation for Machine Translation in the Americas 2010,Denver, Colorado, USA, October.Michel Galley and Christopher D. Manning.
2008.
ASimple and Effective Hierarchical Phrase ReorderingModel.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Processing,pages 848?856, Honolulu, Hawaii, October.Michel Galley and Christopher D. Manning.
2010.
Ac-curate Non-Hierarchical Phrase-Based Translation.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 966?974, Los Angeles, California, June.
Association forComputational Linguistics.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proceedingsof HLT-NAACL, pages 127?133, Edmonton, Canada.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh System Descriptionfor the 2005 IWSLT Speech Translation Evaluation.
InInternational Workshop on Spoken Language Transla-tion 2005.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL 2007Demonstrations, Prague, Czech Republic.Philipp Koehn.
2004a.
Pharaoh: A Beam Search De-coder for Phrase-Based Statistical Machine Transla-tion Models.
In AMTA, pages 115?124.Philipp Koehn.
2004b.
Statistical Significance Testsfor Machine Translation Evaluation.
In Dekang Linand Dekai Wu, editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain, July.Jose?
B. Marin?o, Rafael E. Banchs, Josep M. Crego, Adria`de Gispert, Patrik Lambert, Jose?
A. R. Fonollosa, andMarta R. Costa-jussa`.
2006.
N-gram-Based MachineTranslation.
Computational Linguistics, 32(4):527?549.Franz J. Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Models.Computational Linguistics, 29(1):19?51.Franz J. Och and Hermann Ney.
2004.
The AlignmentTemplate Approach to Statistical Machine Translation.Computational Linguistics, 30(1):417?449.Franz J. Och, Christoph Tillmann, and Hermann Ney.1999.
Improved alignment models for statistical ma-chine translation.
In Joint SIGDAT Conf.
on Empiri-cal Methods in Natural Language Processing and VeryLarge Corpora, pages 20?28, University of Maryland,College Park, MD.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Computa-tional Linguistics, ACL ?02, pages 311?318, Morris-town, NJ, USA.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In International Conferenceon New Methods in Language Processing, pages 44?49, Manchester, UK.10Andreas Stolcke.
2002.
SRILM - An Extensible Lan-guage Modeling Toolkit.
In Intl.
Conf.
Spoken Lan-guage Processing, Denver, Colorado.Christoph Tillmann and Tong Zhang.
2005.
A Local-ized Prediction Model for Statistical Machine Transla-tion.
In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL?05),pages 557?564, Ann Arbor, Michigan, June.Ashish Vaswani, Haitao Mi, Liang Huang, and DavidChiang.
2011.
Rule Markov Models for Fast Tree-to-String Translation.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 856?864,Portland, Oregon, USA, June.Dekai Wu.
1997.
Stochastic Inversion TransductionGrammars and Bilingual Parsing of Parallel Corpora.Computational Linguistics, 23(3):377?403.Omar F. Zaidan.
2009.
Z-MERT: A Fully ConfigurableOpen Source Tool for Minimum Error Rate Trainingof Machine Translation Systems.
The Prague Bulletinof Mathematical Linguistics, 91:79?88.11
