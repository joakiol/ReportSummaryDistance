Feature Selection for Trainable Multilingual Broadcast News SegmentationDavid D. Palmer, Marc Reichman, Elyes YaichVirage Advanced Technology Group300 Unicorn ParkWoburn, MA 01801{dpalmer,mreichman,eyaich}@virage.comAbstractIndexing and retrieving broadcast news storieswithin a large collection requires automatic de-tection of story boundaries.
This video newsstory segmentation can use a wide range of au-dio, language, video, and image features.
Inthis paper, we investigate the correlation be-tween automatically-derived multimodal fea-tures and story boundaries in seven differentbroadcast news sources in three languages.
Weidentify several features that are important forall seven sources analyzed, and we discuss thecontributions of other features that are impor-tant for a subset of the seven sources.1 IntroductionIndexing and retrieving stories within a large collectionof video requires automatic detection of story bound-aries, and video story segmentation is an essential steptoward providing the means for finding, linking, summa-rizing, and visualizing related parts of multimedia col-lections.
In many cases, previous story segmentation re-search has focused on single stream analysis techniques,utilizing only one of the information sources present innews broadcasts: natural language, audio, image, andvideo (see, for example, (Furht et al, 1995),(Fiscus andDoddington, 2002),(Greiff et al, 2001), (O?Connor etal., 2001)).
Some segmentation research has includedmultimodal approaches that were capable of combin-ing features from multiple information sources (Boykinand Merlino, 1999),(Hauptmann and Witbrock, 1998).
While this work was a significant improvement oversingle-stream approaches, they were rarely applied tonon-English sources without closed captioning.Previous work on story segmentation has identifiedmany features useful for finding story boundaries, butfeature selection is often model-dependent and does notaccount for the differences between broadcast sources.Specific features useful for video story segmentation varywidely from one source to the next, and the degree towhich each feature is useful also varies across sourcesand even from one broadcast to the next within a sin-gle source.
This variety suggests the need for trainabletechniques in which salient source-specific features canbe automatically learned from a set of training data.
Thisdata-driven approach is especially important in multilin-gual video processing, where native speakers may not beavailable to develop segmentation models for every lan-guage.The goal of this paper is to provide a model-independent investigation of the correlation between awide range of multimedia features and news story bound-aries, in order to aid the development of improved seg-mentation algorithms.
Our work seeks to complementrecent work in model-dependent feature selection, suchas (Hsu and Chang, 2003), without making assumptionsabout the dependencies between features.The feature analysis we describe in this paper consistedof several steps.
First, we created a data set for our exper-iments by capturing and digitally encoding a set of newsbroadcasts from seven video news sources in three lan-guages.
A native speaker manually labelled the storyand commercial boundaries in each broadcast; we de-scribe the data in Section 2.
We ran several state-of-the-art audio and video analysis software packages on eachrecorded broadcasts to extract time-stamped multimediametadata, and we defined a set of possible segmentationfeatures based on the metadata values produced by theanalysis software; we describe the software analysis andfeature extraction in Section 3.
Finally, we analyzed thepatterns of occurrence of the features with respect to storyand commercial boundaries in all the news broadcasts;the results of our analysis are described in Section 4.2 DataThe data for our segmentation research consists of aset of news broadcasts recorded directly from a satel-lite dish between September 2002 and February 2003.The data set contains roughly equal amounts (8-12 hours)of news broadcasts from seven sources in three lan-guages: Aljazeera (Arabic), BBC America (UK English),China Central TV (Mandarin Chinese), CNN HeadlineNews (US English), CNN International (US/UK En-glish), Fox News (US English), and Newsworld Interna-tional (US/UK English).Each broadcast was manually segmented with the la-bels ?story?
and ?commercial?
by one annotator and ver-ified by a second, at least one of whom was a nativespeaker of the broadcast language.
We found that a verygood segmentation is possible by a non-native speakerbased solely on video and acoustic cues, but a nativespeaker is required to verify story boundaries that re-quire language knowledge, such as a single-shot videosequence of several stories read by a news anchor with-out pausing.
The definition of ?story?
in our experi-ments corresponds with the Topic Detection and Track-ing definition: a segment of a news broadcast with a co-herent news focus, containing at least two independent,declarative clauses (LDC, 1999).
The segments withinbroadcasts briefly summarizing several stories were notassigned a ?story?
label, nor were anchor introductions,signoffs, banter, and teasers for upcoming stories.
Eachindividual story within blocks of contiguous stories waslabeled ?story.?
A sequence of contiguous commercialswas annotated with a single ?commercial?
label with asingle pair of boundaries for the entire block.Table 1 shows the details of our experimental data set.The first two columns show the broadcast source and thelanguage.
The next two columns show the total num-ber of hours and the number of hours labeled ?story?
foreach source.
It is interesting to note that the percentageof broadcast time devoted to news stories varies widelyby source, from 62% for CNN Headline News to 90% forCNN International.
Similarly, the average story lengthvaries widely, as shown in the final column of Table 1,from 52 seconds per story for CNN Headline News to 171seconds per story for Fox News.
These large differencesare extremely important when modeling the distributionsof stories (and commercials) within news broadcasts fromvarious sources.3 Feature extractionIn order to analyze audio and video events that are rele-vant to story segmentation, we encoded the news broad-casts described in Section 2 as MPEG files, then automat-ically processed the files using a range of media analysissoftware components.
The software components repre-Total Story Story Ave.Source Lang.
Hours Hours Count LengthALJ Ara 10:37 6:56 279 89 sBBC Eng 8:09 6:09 215 103 sCCTV Chi 9:05 7:14 235 111 sCNNH Eng 11:42 7:18 505 52 sCNNI Eng 10:14 9:13 299 111 sFox Eng 13:13 9:14 194 171 sNWI Eng 8:33 6:12 198 113 sTable 1: Data sources (Broadcast source, language, totalhours, hours of stories, number of stories, average storylength).sented state-of-the-art technology for a range of audio,language, image, and video processing applications.The audio and video analysis produced time-stampedmetadata such as ?face Chuck Roberts detected attime=2:38?
and ?speaker Bill Clinton identified betweenstart=12:56 and end=16:28.?
From the raw metadata wecreated a set of features that have previously been used instory segmentation work, as well as some novel featuresthat have not been used in previous published work.
Thesoftware components and resulting features are describedin the following sections.3.1 Audio and language processingA great deal of the information in a news broadcast iscontained in the raw acoustic portion of the news signal.Much of the information is contained in the spoken audio,both in the characteristics of the human speech signal andin the sequence of words spoken.
This information canalso take the form of non-spoken audio events, such asmusic, background noise, or even periods of silence.
Weran the following audio and language processing compo-nents on each of the data sources described in Section 2.Audio type classification segments and labels the au-dio signal based on a set of acoustic models: speech,music, breath, lip smack, and silence.
Speaker identi-fication models the speech-specific acoustic characteris-tics of the audio and seeks to identify speakers from alibrary of known speakers.
Automatic speech recogni-tion (ASR) provides an automatic transcript of the spo-ken words.
Topic classification labels segments of theASR output according to predefined categories.
The au-dio processing software components listed above are de-scribed in detail in (Makhoul et al, 2000).
Closed cap-tioning is a human-generated transcript of the spokenwords that is often embedded in a broadcast video sig-nal.Story segmentation features automatically extractedfrom audio and language processing components were:speech segment, music segment, breath, lip smack, si-lence segment, topic classification segment, closed cap-tioning segment, speaker ID segment, and speaker IDchange.
In addition we analyzed the ASR word se-quences in all broadcasts to automatically derive a set ofsource-dependent cue phrase n-gram features.
To de-termine cue n-grams, we extracted all relatively frequentunigrams, bigrams, and trigrams from the training dataand compared the likelihood of observing each n-gramnear a story boundary vs. elsewhere in the data.
Cuen-gram phrases were deemed to be those that were sig-nificantly more likely near the start of a story.3.2 Video and image processingThe majority of the bandwidth in a video broadcast sig-nal is devoted to video content, and this content is a richsource of information about news stories.
The composi-tion of individual frames of the video can be analyzed todetermine whether specific persons or items are shown,and the sequence of video frames can be analyzed to de-termine a pattern of image movement.
We ran the fol-lowing image and video processing components on eachof the data sources described in Section 2.Face identification detects human faces in the imageand compares the face to a library of known faces.
Colorscreen detection analyzes the frame to determine if it islikely to be primarily a single shade, like black or blue.Logo detection searches the video frame for logos in alibrary of known logos.
Shot change classification de-tects several categories of shot changes within a sequenceof video frames.Story segmentation features automatically extractedfrom image and video processing components were: an-chor face ID, blue screen detection, black screen detec-tion, logo detection, fast scene cut detection, slow scenetransition detection, gradual scene transient detection,and scene fade-to-black detection.3.3 Feature analysis methodologyEach feature in our experiments took the form of a binaryresponse to a question that related the presence of rawtime-stamped metadata within a window of time aroundeach story and commercial boundary, e.g., ?Did an an-chor face detection occur within 5 seconds of a storyboundary??
For processing components that producemetadata with an explicit duration (such as a speakerID segment), we defined separate features for the startand end of the segment plus a feature for whether themetadata segment ?persisted?
throughout the time win-dow around the boundary.
For example, a speaker IDsegment that begins at t=12 and ends at t=35 would re-sult in a true value for the feature ?Speaker ID segmentpersists,?
for a time window of 5 seconds around a storyboundary at t=20.For each binary feature, we calculated the maximumlikelihood (ML) probability of observing the feature neara story boundary.
For example, if there were 100 sto-ries, and the anchor face detection feature was true for 50of the stories, then p(anchor|story) = 50/100 = 0.5.We similarly calculated the ML probabilities of an an-chor face detection near a commercial boundary, outsideof both story and commercial, and inside a story but out-ide the window of time near the boundary.Useful features for segmentation in general are thosewhich occur primarily near only one type of bound-ary, which would result in a large relative magnitudedifference between these four probabilities.
Ideal fea-tures, f , for story segmentation would be those for whichp(f |story) is much larger than the other values.
For ourexperiments we identified features for which there wasat least an order of magnitude spread in the observationprobabilities across categories.4 ResultsThe overarching goal of our analysis was to identify mul-timedia events for each source that could be used to dis-tinguish stories from commercials and other non-storysegments in the broadcast.
The results of our feature se-lection experiments revealed several features that wereimportant for all seven sources we analyzed, as well asother features that were important for certain sources butnot others.
In this section we discuss our results.Table 2 shows the selected features for each of thebroadcast sources.
The first two columns show the nameand type of each feature, as defined in Section 3, withstart, end, and persist for durational metadata features,where relevant.
The cells in the remaining columns showa ?+?
if the feature was automatically selected for the cor-responding source; the cells are empty if the feature wasnot selected.
The cell contains ?n/a?
if the feature was notavailable for the source; this was the case for the English-language TDT topic classificationOf the hundreds of features we analyzed, only 14 wereselected for at least one of the broadcast sources.
Theselected features varied greatly by source, with some fea-tures being used by only one or two of the seven sources.There are only three features that were selected for eachof the seven sources: music segment persist, video fadestart, and cue n-gram detected.
Two other features,broadcaster logo detected and blue screen detected, wereselected for all but one of the sources.
One interesting re-sult is that these features selected for all or most sourcescome from all four information sources: audio, language,image, and video.The significance of the selected features also varied bysources.
For example, the blue screen detected featurewas selected for all but one source; this feature thus hasa much higher probability of occurring at certain pointsFeature Type ALJ BBC CCTV CNNH CNNI FOX NWICue n-gram detected language + + + + + + +Closed captioning start language + +TDT topic start language n/a n/a +Music segment start audio + +Music segment persist audio + + + + + + +Breath audio + +Speaker change audio + +Anchor face detected image + +Blue screen detected image + + + + + +Black screen detected image + + + +Broadcaster logo detected image + + + + +Video transition start video + + +Video transient start video + +Video fade start video + + + + + + +Table 2: Features automatically selected for each of the seven sources.than others.
For two sources (ALJ and CNN), the pres-ence of a blue screen is much more likely to occur duringa commercial.
For NWI it is most likely to occur at thestart of a story, and for BBC it is most likely to occuroutside stories and commercials.
For CCTV it is equallylikely in commercials and at the start of stories.
For noneof the sources is the blue screen likely to occur within astory.One of the most important features for all sevensources is the cue n-gram detected feature derived fromthe automatic speech recognition output.
Interestingly,the n-grams that indicate story boundaries were ex-tremely source-dependent, with almost no overlap in thelists of words derived across sources.
Table 3 shows someexamples of the highest-ranked n-grams from each of thesources (Arabic and Mandarin n-grams are shown manu-ally translated into English).Source Top n-gram featureAljazeera here is a reportBBC America hello and welcomeCCTV Chinese news broadcastCNN Headline News stories we?re followingCNN International world news I?mFox News exclusiveNewsworld International hello everybody I?mTable 3: Top n-gram feature derived for each source.ReferencesS.
Boykin and A. Merlino, ?Improving BroadcastNews Segmentation Processing,?
Proceedings of IEEEMulti-media Systems, Florence, Italy, June 7-11, 1999.J.
G. Fiscus and G. R. Doddington, ?Topic detection andtracking evaluation overview,?
In J. Allan, editor, TopicDetection and Tracking: Event-based Information Or-ganization, pages 17-31.
Kluwer Academic Publishers,Boston, 2002.B.
Furht, S. Smoliar, and H. Zhang, Video and ImageProcessing in Multimedia Systems, Kluwer AcademicPublishers, 1995.W.
Greiff, A. Morgan, R. Fish, M. Richards, A.Kundu, ?Fine-Grained Hidden Markov Modeling forBroadcast-News Story Segmentation,?
Proceeding ofFirst International Conference on Human LanguageTechnology Research (HLT 2001).A.
Hauptmann and M. Witbrock, ?Story Segmentationand Detection of Commercials in Broadcast NewsVideo,?
Advances in Digital Libraries Conference(ADL?98), Santa Barbara, CA, April 22 - 24, 1998.W.
Hsu, S. Chang, ?A Statistical Framework for FusingMid-level Perceptual Features in News Story Segmen-tation,?
IEEE International Conference on Multimediaand Expo (ICME) 2003.Linguistic Data Consortium, TDT2 Seg-mentation Annotation Guide, 1999.http://www.ldc.upenn.edu/Projects/TDT2/J.
Makhoul, F. Kubala, T. Leek, D. Liu, L. Nguyen,R.Schwartz, and A. Srivastava, ?Speech and LanguageTechnologies for Audio Indexing and Retrieval,?
inProceedings of the IEEE, vol.
88, no.
8, pp.
1338-1353,2000.N.
O?Connor; C. Czirjek; S. Deasy; S. Marlow; N. Mur-phy; A. Smeaton, ?News Story Segmentation in theFischlar Video Indexing System,?
Proceedings of IEEEInternational Conference on Image Processing (ICIP-2001), Thessaloniki Greece.
