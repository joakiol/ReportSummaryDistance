Variant Transduction: A Method for Rapid Development ofInteractive Spoken InterfacesHiyan Alshawi and Shona DouglasAT&T Labs Research180 Park AvenueFlorham Park, NJ 07932, USAfhiyan,shonag@research.att.comAbstractWe describe an approach (\vari-ant transduction") aimed at reduc-ing the eort and skill involvedin building spoken language inter-faces.
Applications are createdby specifying a relatively small setof example utterance-action pairsgrouped into contexts.
No interme-diate semantic representations areinvolved in the specication, andthe conrmation requests used inthe dialog are constructed automat-ically.
These properties of vari-ant transduction arise from combin-ing techniques for paraphrase gen-eration, classication, and example-matching.
We describe how a spo-ken dialog system is constructedwith this approach and also providesome experimental results on vary-ing the number of examples used tobuild a particular application.1 IntroductionDeveloping non-trivial interactive spoken lan-guage applications currently requires signi-cant eort, often several person-months.
Amajor part of this eort is aimed at copingwith variation in the spoken language inputby users.
One approach to handling varia-tion is to write a large natural language gram-mar manually and hope that its coverage issucient for multiple applications (Dowdinget al, 1994).
Another approach is to cre-ate a simulation of the intended system (typ-ically with a human in the loop) and thenrecord users interacting with the simulation.The recordings are then transcribed and an-notated with semantic information relating tothe domain; the transcriptions and annota-tions can then be used to create a statisticalunderstanding model (Miller et al, 1998) orused as guidance for manual grammar devel-opment (Aust et al, 1995).Building mixed initiative spoken languagesystems currently usually involves the designof semantic representations specic to the ap-plication domain.
These representations areused to pass data between the language pro-cessing components: understanding, dialog,conrmation generation, and response gener-ation.
However, such representations tend tobe domain-specic, and this makes it dicultto port to new domains or to use machinelearning techniques without extensive hand-labeling of data with the semantic represen-tations.
Furthermore, the use of intermediatesemantic representations still requires a naltransduction step from the intermediate rep-resentation to the action format expected bythe application back-end (e.g.
SQL databasequery or procedure call).For situations when the eort and exper-tise available to build an application is small,the methods mentioned above are impracti-cal, and highly directed dialog systems withlittle allowance for language variability areconstructed.In this paper, we describe an approach toconstructing interactive spoken language ap-plications aimed at alleviating these prob-lems.
We rst outline the characteristics ofthe method (section 2) and what needs tobe provided by the application builder (sec-tion 3).
In section 4 and section 5 we ex-plain variant expansion and the operation ofthe system at runtime, and in section 6 wedescribe how conrmation requests are pro-duced by the system.
In section 7 we givesome initial experimental results on varyingthe number of examples used to construct acall-routing application.2 Characteristics of our approachThe goal of the approach discussed in this pa-per (which we refer to as \variant transduc-tion") is to avoid the eort and specializedexpertise used to build current research pro-totypes, while allowing more natural spokeninput than is handled by spoken dialog sys-tems built using current commercial practice.This led us to adopt the following constraints: Applications are constructed using a rel-atively small number of example inputs(no grammar development or extensivedata collection). No intermediate semantic representa-tions are needed.
Instead, manipulationsare performed on word strings and on ac-tion strings that are nal (back-end) ap-plication calls. Conrmation queries posed by the sys-tem to the user are constructed automat-ically from the examples, without the useof a separate generation component. Dialog control should be simple to spec-ify for simple applications, while allowingtheexibility of delegating this controlto another module (e.g.
an \intelligent"back-end agent) for more complex appli-cations.We have constructed two telephone-basedapplications using this method, an applica-tion to access email and a call-routing appli-cation.
These two applications were chosento gain experience with the method becausethey have dierent usage characteristics andback-end complexity.
For the e-mail accesssystem, usage is typically habitual, and thesystem's mapping of user utterances to back-end actions needs to take into account dy-namic aspects of the current email session.For the call-routing application, the back-endcalls executed by the system are relativelysimple, but users may only encounter the sys-tem once, and the system's initial prompt isnot intended to constrain the rst input spo-ken by the user.3 Constructing an application withexample-action contextsAn interactive spoken language applicationconstructed with the variant transductionmethod consists of a set of contexts.
Eachcontext provides the mapping between userinputs and application actions that are mean-ingful in a particular stage of interaction be-tween the user and system.
For example thee-mail reader application includes contexts forlogging in and for navigating a mail folder.The actual contexts that are used at run-time are created through a four step process:1.
The application developer species (asmall number of) triples he; a; ci wheree is a natural language string (a typicaluser input), a is an application action(back-end application API call).
For in-stance, the string read the message fromJohn might be paired with the API callmailAgent.getWithSender("jsmith@att.com").The third element of a triple, c, is anexpression identifying another (or thesame) context, specically, the contextthe system will transition to if e is theclosest match to the user's input.2.
The set of triples for each context is ex-panded by the system into a larger setof triples.
The additional triples are ofthe form hv; a0; ci where v is a \variant"of example e (as explained in section 4below), and a0is an \adapted" version ofthe action a.3.
During an actual user session, the set oftriples for a context may optionally beexpanded further to take into accountthe dynamic aspects of a particular ses-sion.
For example, in the mail access ap-plication, the set of names available forrecognition is increased to include thosepresent as senders in the user's currentmail folder.4.
A speech recognition language model iscompiled from the expanded set of ex-amples.
We currently use a languagemodel that accepts any sequence of sub-strings of the examples, optionally sepa-rated by ller words, as well as sequencesof digits.
(For a small number of exam-ples, a statistical N-gram model is inef-fective because of low N-gram counts.)
Adetailed account of the recognition lan-guage model techniques used in the sys-tem is beyond the scope of this paper.In the current implementation, actions aresequences of statements in the Java language.Constructors can be called to create new ob-jects (e.g.
a mail session object) which can beassigned to variables and referenced in otheractions.
The context interpreter loads the re-quired classes and evaluates methods dynam-ically as needed.
It is thus possible for anapplication developer to build a spoken inter-face to their target API without introducingany new Java classes.
The system could eas-ily be adapted to use action strings from otherinterpreted languages.A key property of the process describedabove is that the application developer needsto know only the back-end API and English(or some other natural language).4 Variant compilationDierent expansion methods can be used inthe second step to produce variants v of anexample e. In the simplest case, v may bea paraphrase of e. Such paraphrase vari-ants are used in the experiments in section 7,where domain-independent \carrier" phrasesare used to create variants.
For example, thephrase I'd like to (among others) is used as apossible alternative for the phrase I want to.The context compiler includes an English-to-English paraphrase generator, so the applica-tion developer is not involved in the expan-sion process, relieving her of the burden ofhandling this type of language variation.
Weare also experimenting with other forms ofvariation, including those arising from lexical-semantic relations, user-specic customiza-tion, and those variants uttered by users dur-ing eld trials of a system.When v is a paraphrase of e, the adaptedaction a0is the same string as a.
In the moregeneral case, the meaning of variant v is dif-ferent from that of e, and the system attempts(not always correctly) to construct a0so thatit reects this dierence in meaning.
For ex-ample, including the variant show the messagefrom Bill Wilson of an example read the mes-sage from John, involves modifying the ac-tion mailAgent.getWithSender("jsmith@att.com")to mailAgent.getWithSender("wwilson@att.com").We currently adopt a simple approach tothe process of mapping language string vari-ants to their corresponding target actionstring variants.
The process requires theavailability of a \token mapping" t betweenthese two string domains, or data or heuristicsfromwhich such a mapping can be learned au-tomatically.
Examples of the token mappingare names to email addresses as illustrated inthe example above, name to identier pairs ina database system, \soundex" phonetic stringspelling in directory applications, and a bilin-gual dictionary in a translation application.The process proceeds as follows:1.
Compute a set of lexical mappings be-tween the variant v and example e. Thisis currently performed by aligning thetwo string in such a way as that the align-ment minimizes the (weighted) edit dis-tance between them (Wagner and Fis-cher, 1974).2.
The token mapping t is used to mapsubstitution pairs identied by the align-ment (hread; showi and hJohn, Bill Wil-soni in the example above) to corre-sponding substitution pairs in the actionstring.
In general this will result in asmaller set of substitution strings sincenot all word strings will be present inthe domain of t. (In the example, this re-sults in the single pair hjsmith@att.com,wwilson@att.comi.)3.
The action substitution pairs are appliedto a to produce a0.4.
The resulting action a0is checked for(syntactic) well-formedness in the actionstring domain; the variant v is rejected ifa0is ill-formed.5 Input interpretationWhen an example-action context is activeduring an interaction with a user, two com-ponents (in addition to the speech recognitionlanguage model) are compiled from the con-text in order to map the user inputs into theappropriate (possibly adapted) action:Classier A classier is built with trainingpairs hv; ai where v is a variant of anexample e for which the example actionpair he; ai is a member of the unexpandedpairs in the context.
Note that the clas-sier is not trained on pairs with adaptedexamples a0since the set of adaptedactions may be too large for accurateclassication (with standard classica-tion techniques).
The classiers typicallyuse text features such as N-grams ap-pearing in the training data.
In our ex-periments, we have used dierent classi-ers, including BoosTexter (Schapire andSinger, 2000), and a classier based onPhi-correlation statistics for the text fea-tures (see Alshawi and Douglas (2000)for our earlier application of Phi statis-tics in learning machine translation mod-els from examples).
Other classierssuch as decision trees (Quinlan, 1993) orsupport vector machines (Vapnik, 1995)could be used instead.Matcher The matcher can compute a dis-tortion mapping and associated distancebetween the output s of the speech rec-ognizer and a variant v. Various match-ers can be used such as those suggestedin example-based approaches to machinetranslation (Sumita and Iida, 1995).
Sofar we have used a weighted string editdistance matcher and experimented withdierent substitution weights includingones based on measures of statistical sim-ilarity between words such as the onedescribed by Pereira et al (1993).
Theoutput of the matcher is a real number(the distance) and a distortion mappingrepresented as a sequence of edit opera-tions (Wagner and Fischer, 1974).Using these two components, the methodfor mapping the user's utterance to an exe-cutable action is as follows:1.
The language model derived from con-text c is activated in the speech recog-nizer.2.
The speech recognizer produces a strings from the user's utterance.3.
The classier for c is applied to s to pro-duce an unadapted action a.4.
The matcher is applied pairwise to com-pare s with each variant vaderived froma triple he; a; c0i in the unexpanded ver-sion of c.5.
The triple hv; a0; c0i for which v pro-duces the smallest distance is selectedand passed along with e to the dialog con-troller.The relationship between the input s, vari-ant v, example e, and actions a and a0isdepicted in Figure 1.
In the gure, f isthe mapping between examples and actionsin the unexpanded context; r is the relationbetween examples and variants; and g is thesearch mapping implemented by the classier-matcher.
The role of e0is related to conrma-tions as explained in the following section.6 Conrmation and dialog controlDialog control is straightforward as the readermight expect, except for two aspects de-scribed in this section: (i) evaluation of next-context expressions, and (ii) generation ofp (prompt): say a mailreader commands (words spoken): now show me messages from Billv (variant): show the message from Bill Wilsone (example): read the message from Johna (associated action): mailAgent.getWithSender("jsmith@att.com")a0(adapted action): mailAgent.getWithSender("wwilson@att.com")e0(adapted example): read the message from Bill WilsonFigure 2: ExampleFigure 1: Variant Transduction mappingsconrmation requests based on the examplesin the context and the user's input.As noted in section 3 the third element cof each triple he; a; ci in a context is an ex-pression that evaluates to the name of thenext context (dialog state) that the systemwill transition to if the triple is selected.
Forsimple applications, c can simply always bean identier for a context, i.e.
the dialog statetransition network is specied explicitly in ad-vance in the triples by the application devel-oper.For more complex applications, next con-text expressions c may be calls that evalu-ate to context identiers.
In our implemen-tation, these calls can be Java methods ex-ecuted on objects known to the action in-terpreter.
They may thus be calls on theback-end application system, which is appro-priate for cases when the back-end has stateinformation relevant to what should happennext (e.g.
if it is an \intelligent agent").
Itmight also be a call to component that imple-ments a dialog strategy learning method (e.g.Levin and Pieraccini (1997)), though we havenot yet tried such methods in conjunctionwith the present system.A conrmation request of the form do youmean e0is constructed for each variant-actionpair (v; a0) of an example-action pair (e; a).The string e0is constructed by rst comput-ing a submapping h0of the mapping h rep-resenting the distortion between e and v. h0is derived from h by removing those edit op-erations which were not involved in mappingthe action a to the adapted action a0.
(Thematcher is used to compute h except whenthe process of deriving (v; a0) from (e; a) al-ready includes an explicit representation of hand t(h).
)The restricted mapping h0is used instead ofh to construct e0in order to avoid misleadingthe user about the extent to which the ap-plication action is being adapted.
Thus if hincludes the substitution w !
w0but t(w) isnot a substring of a then this edit operation isnot included in h0.
This way, e0includes w un-changed, so that the conrmation asked of theuser does not carry the implication that thechange w !
w0is taken into account in theaction a0to be executed by the system.
Forinstance, in the example in Figure 2, the word\now" in the user's input does not correspondto any part of the adapted action, and is notincluded in the conrmation string.
In prac-tice, the conrmation string e0is computedat the same time that the variant-action pair(v; a0) is derived from the original examplepair (e; a).The dialogow of control proceeds as fol-lows:1.
The active context c is set to a distin-guished initial context c0indicated bythe application developer.2.
A prompt associated with the current ac-tive context c is played to the user usinga speech synthesiser or by playing an au-dio le.
For this purpose the applicationdeveloper provides a text string (or audiole) for each context in the application.3.
The user's utterance is interpreted as ex-plained in the previous section to pro-duce the triple hv; a0; c0i.4.
A match distance d is computed as thesum of the distance computed for thematcher between s and v and the dis-tance computed by the matcher betweenv and e (where e is the example fromwhich v was derived).5.
If d is smaller than a preset threshold, itis assumed that no conrmation is neces-sary and the next three steps are skipped.6.
The system asks the user do you mean:e0.
If the user responds positively thenproceed to the next step, otherwise re-turn to step 2.7.
The action a0is executed, and any stringoutput it produces is read to the userwith the speech synthesizer.8.
The active context is set to the result ofevaluating the expression c0.9.
Return to step 2.Figure 2 gives an example showing thestrings involved in a dialog turn.
Handlingthe user's verbal response to the conrmationis done with a built-in yes-no context.The generation of conrmation requestsrequires no work by the application de-veloper.
Our approach thus providesan even more extreme version of auto-matic conrmation generation than that usedby Chu-Carroll and Carpenter (1999) whereonly a small eort is required by the devel-oper.
In both cases, the benets of care-fully crafted conrmation requests are beingtraded for rapid application development.7 ExperimentsAn important question relating to our methodis the eect of the number of examples onsystem interpretation accuracy.
To measurethis eect, we chose the operator services callrouting task described by Gorin et al (1997).We chose this task because a reasonably largedata set was available in the form of actualrecordings of thousands of real customers call-ing AT&T's operators, together with tran-scriptions and manual labeling of the de-sired call destination.
More specically, wemeasure the call routing accuracy for uncon-strained caller responses to the initial contextprompt AT&T.
How may I help you?.
An-other advantage of this task was that bench-mark call routing accuracy gures were avail-able for systems built with the full data set(Gorin et al, 1997; Schapire and Singer,2000).
We have not yet measured interpreta-tion accuracy for the structurally more com-plex e-mail access application.In this experiment, the responses to Howmay I help you?
are \routed" to fteen des-tinations, where routing means handing othe call to another system or human operator,or moving to another example-action contextthat will interact further with the user to elicitfurther information so that a subtask (such asmaking a collect call) can be completed.
Thusthe actions in the initial context are simplythe destinations, i.e.
a = a0, and the matcheris only used to compute e0.The fteen destinations include a destina-tion \other" which is treated specially in thatit is also taken to be the destination when thesystem rejects the user's input, for examplebecause the condence in the output of thespeech recognizer is too low.
Following previ-ous work on this task, cited above, we presentthe results for each experimental condition asan ROC curve plotting the routing accuracy(on non-rejected utterances) as a function ofthe false rejection rate (the percentage of thesamples incorrectly rejected); a classicationby the system of \other" is considered equiv-alent to rejection.The dataset consists of 8,844 utterances ofwhich 1000 were held out for testing.
We referto the remaining 7,884 utterances as the \fulltraining dataset".In the experiments, we vary two conditions:Input uncertainty The input string to theinterpretation component is either a hu-man transcription of the spoken utter-ance or the output of a speech recog-nizer.
The acoustic model used for au-tomatic speech recognition was a gen-eral telephone speech HHM model in allcases.
(For the full dataset, better re-sults can be achieved by an application-specic acoustic model, as presented byGorin et al (1997) and conrmed by ourresults below.
)Size of example set We select progres-sively larger subsets of examples fromthe full training set, as well as showingresults for the full training set itself.
Wewish to approximate the situation wherean application developer uses typicalexamples for the initial context withoutknowing the distribution of call types.We therefore select k utterances for eachdestination, with k set to 3, 5, and 10,respectively.
This selection is random,except for the provision that utterancesappearing more than once are preferred,to approximate the notion of a typicalutterance.
The selected examples areexpanded by the addition of variants, asdescribed earlier.
For each value of k,the results shown are for the median ofthree runs.Figure 3 shows the routing accuracy ROCcurves for transcribed input for k = 3; 5; 10and for the full training dataset.
These re-sults for transcribed input were obtained withBoosTexter (Schapire and Singer, 2000) as theclassier module in our system because wehave observed that BoosTexter generally out-performs our Phi classier (mentioned earlier)for text input.Figure 4 shows the corresponding four ROCcurves for recognition output, and an ad-ditional fth graph (the top one) showingthe improvement that is obtained with a do-main specic acoustic model coupled with atrigram language model.
These results forrecognition output were obtained with thePhi classier module rather than BoosTex-ter; the Phi classier performance is generallythe same as, or slightly better than, Boos-Texter when applied to recognition output.The language models used in the experimentsfor Figure 4 are derived from the examplesets for k = 3; 5; 10 (lower three graphs) andfor the full training set (upper two graphs),respectively.
As described earlier, the lan-guage model for restricted numbers of exam-ples is an unweighted one that recognizes se-quences of substrings of the examples.
For thefull training set, statistical N-gram languagemodels are used (N=3 for the top graph andN=2 for the second to top) since there is suf-cient data in the full training set for suchlanguage models to be eective.0 10 20 30 40 50 60 70 800102030405060708090100False rejection %%Correctactionsfull training set10 examples/action + variants5 examples/action + variants3 examples/action + variantsFigure 3: Routing accuracy for transcribedutterancesComparing the two gures, it can be seenthat the performance shortfall from usingsmall numbers of examples compared to thefull training set is greater when speech recog-0 10 20 30 40 50 60 70 800102030405060708090100False rejection %%Correctactionsfull training set,  trigrams, domain acousticsfull training set, bigrams10 examples/action + variants, subsequences5 examples/action + variants, subsequences3 examples/action + variants, subsequencesFigure 4: Routing accuracy for speech recog-nition outputnition errors are included.
This suggests thatit might be advantageous to use the examplesto adapt a general statistical language model.There also seem to be diminishing returns ask is increased from 3 to 5 to 10.
A likelyexplanation is that expansion of examples byvariants is progressively less eective as thesize of the unexpanded set is increased.
Thisis to be expected since additional real exam-ples presumably are more faithful to the taskthan articially generated variants.8 Concluding remarksWe have described an approach to construct-ing interactive spoken interfaces.
The ap-proach is aimed at shifting the burden of han-dling linguistic variation for new applicationsfrom the application developer (or data col-lection lab) to the underlying spoken languageunderstanding technology itself.
Applicationsare specied in terms of a relatively smallnumber of examples, while the mapping be-tween the inputs that users speak, variantsof the examples, and application actions, arehandled by the system.
In this approach, weavoid the use of intermediate semantic rep-resentations, making it possible to developgeneral approaches to linguistic variation anddialog responses in terms of word-string toword-string transformations.
Conrmationrequests used in the dialog are computed au-tomatically from variants in a way intended tominimize misleading the user about the appli-cation actions to be executed by the system.The quantitative results we have pre-sented indicate that a surprisingly small num-ber of training examples can provide use-ful performance in a call routing application.These results suggest that, even at its cur-rent early stage of development, the vari-ant transduction approach is a viable optionfor constructing spoken language applicationsrapidly without specialized expertise.
Thismay be appropriate, for example, for boot-strapping data collection, as well as for situa-tions (e.g.
small businesses) for which devel-opment of a full-blown system would be toocostly.
When a full dataset is available, themethod can provide similar performance tocurrent techniques while reducing the level ofskill necessary to build new applications.ReferencesH.
Alshawi and S. Douglas.
2000.
Learningdependency transduction models from unan-notated examples.
Philosophical Transactionsof the Royal Society (Series A: Mathematical,Physical and Engineering Sciences), 358:1357{1372, April.H.
Aust, M. Oerder, F. Seide, and V. Steinbiss.1995.
The Philips automatic train timetableinformation system.
Speech Communication,17:249{262.Jennifer Chu-Carroll and Bob Carpenter.
1999.Vector-based natural language call routing.Computational Linguistic, 25(3):361{388.J.
Dowding, J. M. Gawron, D. Appelt, J. Bear,L.
Cherny, R. Moore, and D. Moran.
1994.Gemini: A Natural Language System ForSpoken-Language Understanding.
In Proc.ARPA Human Language Technology Workshop'93, pages 43{48, Princeton, NJ.A.L.
Gorin, G. Riccardi, and J.H.
Wright.
1997.How may I help you?
Speech Communication,23(1-2):113{127.E.
Levin and R. Pieraccini.
1997.
A stochas-tic model of computer-human interaction forlearning dialogue strategies.
In Proceedings ofEUROSPEECH97, pages 1883{1886, Rhodes,Greece.Scott Miller, Michael Crystal, Heidi Fox, LanceRamshaw, Richard Schwartz, Rebecca Stone,Ralph Weischedel, and the Annotation Group.1998.
Algorithms that learn to extract informa-tion { BBN: description of the SIFT system asused for MUC-7.
In Proceedings of the SeventhMessage Understanding Conference (MUC-7),Fairfax, VA. Morgan Kaufmann.F.
Pereira, N. Tishby, and L. Lee.
1993.
Distribu-tional clustering of english words.
In Proceed-ings of the 31st meeting of the Association forComputational Linguistics, pages 183{190.J.R.
Quinlan.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann, San Mateo, CA.Robert E. Schapire and Yoram Singer.
2000.BoosTexter: A Boosting-based System forText Categorization.
Machine Learning,39(2/3):135{168.Eiichiro Sumita and Hitoshi Iida.
1995.
Het-erogeneous computing for example-based trans-lation of spoken language.
In Proceedings ofthe 6thInternational Conference on Theoreticaland Methodological Issues in Machine Transla-tion, pages 273{286, Leuven, Belgium.V.N.
Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer, New York.Robert A. Wagner and Michael J. Fischer.1974.
The String-to-String Correction Prob-lem.
Journal of the Association for ComputingMachinery, 21(1):168{173, January.
