Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 99?107,Paris, October 2009. c?2009 Association for Computational LinguisticsCo-Parsing with Competitive ModelsLidia KhmylkoNatural Language Systems GroupUniversity of Hamburg, Germanykhmylko@informatik.uni-hamburg.deKilian A. FothsmartSpeed GmbH & Co. KGHamburg, Germanykilian.foth@smartspeed.comWolfgang MenzelNatural Language Systems GroupUniversity of Hamburg, Germanymenzel@informatik.uni-hamburg.deAbstractWe present an asymmetric approach toa run-time combination of two parserswhere one component serves as a predic-tor to the other one.
Predictions are inte-grated by means of weighted constraintsand therefore are subject to preferentialdecisions.
Previously, the same architec-ture has been successfully used with pre-dictors providing partial or inferior infor-mation about the parsing problem.
It hasnow been applied to a situation where thepredictor produces exactly the same typeof information at a fully competitive qual-ity level.
Results show that the combinedsystem outperforms its individual compo-nents, even though their performance inisolation is already fairly high.1 IntroductionMachine learning techniques for automatically ac-quiring processing models from a data collec-tion and traditional methods of eliciting linguisticknowledge from human experts are usually con-sidered as two alternative roadmaps towards nat-ural language processing solutions.
Since the re-sulting components exhibit quite different perfor-mance characteristics with respect to coverage, ro-bustness and output quality, they might be able toprovide some kind of complementary information,which could even lead to a notable degree of syn-ergy between them when combined within a singlesystem solution.For the task of dependency parsing, the highpotential for such a synergy has indeed beendemonstrated already (e.g.
Zeman and Z?abokrtsky?
(2005), Foth and Menzel (2006)).A popular approach for combining alterna-tive decision procedures is voting (Zeman andZ?abokrtsky?, 2005).
It makes use of a symmet-ric architecture, where a meta component choosesfrom among the available candidate hypotheses bymeans of a (weighted) voting scheme.
Such an ap-proach not only requires the target structures of allcomponents to be of the same kind, but in case ofcomplex structures like parse trees also requiressophisticated decision procedures which are ableto select the optimal hypotheses with respect to ad-ditional global constraints (e.g.
the tree property).Since this optimization problem has to be solvedby the individual parser anyhow, an asymmetricarchitecture suggests itself as an alternative.In asymmetric architectures, a master compo-nent, i.e.
a full fledged parser, is solely in charge ofdeciding on the target structure, whilst the others(so called helper or predictor components) provideadditional evidence which is integrated into theglobal decision by suitable means.
Such a schemehas been extensively investigated for the WeightedConstraint Dependency Grammar, WCDG (Foth,2006).
External evidence from the predictor com-ponents is integrated by means of constraints,which check for compatibility between a localstructure and a prediction, and penalize this hy-pothesis in case of a conflict.
So far, however,all the additional information sources which havebeen considered in this research differed consider-ably from the master component: They either fo-cused on particular aspects of the parsing problem(e.g.
POS tagging, chunking, PP attachment), orused a simplified scheme for structural annotation(e.g.
projective instead of non-projective trees).This paper takes one step further by investigat-ing the same architecture under the additional con-dition that (1) the helper component provides the99very same kind of target structure as the master,and (2) the quality levels of each of the compo-nents in isolation are considered.As a helper component MSTParser (McDon-ald, 2006), a state-of-the-art dependency parser fornon-projective structures based on a discrimina-tive learning paradigm, is considered.
The accu-racy of MSTParser differs insignificatly from thatof WCDG with all the previously used helper com-ponents active.Section two introduces WCDG with a specialemphasis on the soft integration of external ev-idence while section three describes MSTParserwhich is used as a new predictor component.
Sinceparsing results for these systems have been re-ported in quite different experimental settings wefirst evaluate them under comparable conditionsand provide the results of using MSTParser as aguiding predictor for WCDG in section four anddiscuss whether the expected synergies have re-ally materialized.
Section five concentrates on acomparative error analysis.2 WCDGThe formalism of a Constraint Dependency Gram-mar was first introduced by Maruyama (1990)and suggests modeling natural language with thehelp of constraints.
Schro?der (2002) has extendedthe approach to Weighted Constraint DependencyGrammar, WCDG, where weights are used to fur-ther disambiguate between competing structuralalternatives.
A WCDG models natural languageas labeled dependency trees and is entirely declar-ative.
It has no derivation rules ?
instead, con-straints license well-formed tree structures.
Thereference implementation of WCDG for the Ger-man language used for the experiments describedbelow contains about 1, 000 manually compiledconstraints.1Every constraint of the WCDG carries a weight,also referred to as a penalty, in the interval fromzero to one, a lower value of the weight re-flects its greater importance.
Constraints havingzero weights are referred to as hard and are usedfor prohibitive rules.
Constraints with a weightgreater than zero, also called defeasible, may ex-press universal principles or vague preferences forlanguage phenomena.1Freely available from http://nats-www.informatik.uni-hamburg.de/view/CDG/DownloadPageAttempts have been made to compute theweights of a WCDG automatically by observingwhich weight vectors perform best on a given cor-pus, but the computations did not bring any sig-nificant improvements to the manually assignedscored (Schro?der et al, 2001).
Empirically, theabsolute values of defeasible constraints usuallydo not matter greatly as long as the relative impor-tance of the rules remains preserved so that typicalconstructions are preferred, but seldom variationsare also allowed.
Thus, the values of weights ofthe WCDG constraints have to be determined bythe grammar writer experimentally.If a set of dependency edges in a parse found bythe system violates any of the constraints, it is reg-istered as a constraint violation between the struc-ture and the rules of the language.
The score of ananalysis is the product of all the weights for con-straint violations occurring in the structure.
It be-comes possible to differentiate between the qual-ity of different parse results: the analysis with ahigher score is considered preferable.
Although,under these conditions, an analysis having only afew grave conflicts may be preferred by the systemagainst another one with a great number of smallerconstraint violations, but it ensures that an analysiswhich violates any of the hard constraints alwaysreceives the lowest possible score.The parsing problem is being treated in theWCDG system as a Constraint Satisfaction Prob-lem.
While a complete search is intractable forsuch a problem, transformation-based solutionmethods provide a reliable heuristic alternative.Starting with an initial guess about the optimaltree, changes of labels, subordinations, or lexi-cal variants are applied, with constraint violationsused as a control mechanism guiding the transfor-mation process (Foth et al, 2000).A transformation-based search cannot guaran-tee to find the best solution to the constraint sat-isfaction problem.
Compared to the resource re-quirements of a complete search, however, it is notonly more efficient, but can also be interrupted atany time.
Even if interrupted, it will always returnan analysis, together with a list of constraint viola-tions it was not able to remove.
The algorithm ter-minates on its own if no violated constraints witha weight above a predefined threshold remain.
Al-ternatively, a timeout condition can be imposed.The same kind of constraints that describegrammar rules, can also be used as an interface100to external predictor components.
Thus, the for-malism turned out to be flexible enough to incor-porate other sources of knowledge into the de-cision process on the optimal structural interpre-tation.
Foth and Menzel (2006) have reportedabout five additional statistical components thathave been successfully integrated into WCDG:POS tagger, chunker, supertagger, PP attacher anda shift-reduce oracle parser.
They have also shownthat the accuracy improves if multiple compo-nents interact and consistent predictions no longercan be guaranteed.
Even thought previously in-tegrated predictor components have an accuracythat is mostly ?
with the exception of the tag-ger ?
below that of the parser itself, WCDG notonly avoids error propagation successfully, it alsoimproves its results consistently with each compo-nent added.3 MSTParserMSTParser (McDonald, 2006) is a state-of-the-artlanguage independent data-driven parser.
It pro-cesses the input in two separate stages.
In the first,the dependency structure is determined, labeling isapplied to it successively in the second.
The rea-sons of its efficiency lie in the successful combi-nation of discriminative learning with graph-basedsolution methods for the parsing problem.In this edge-factored graph-based model, eachedge of the dependency graph is assigned a real-valued score by its linear model.
The score of thegraph is defined as the sum of its edge scores.If a scoring function for edges is known, theparsing problem becomes equivalent to finding thehighest scoring directed spanning tree in the com-plete graph over the given sentence, and the cor-rect parse can be obtained by searching the spaceof valid dependency graphs for a tree with a max-imum score.This formalism allows to find efficient solutionsfor both projective and non-projective trees.
Whenonly features over single edges are taken into ac-count, the complexity falls to O(n2) (McDonaldet al, 2005).Not only a single edge, but also adjacent edgesmay be included into the scoring function.
As aresult, intractability problems arise for the non-projective algorithm, but an efficient approximatealgorithm based on exhaustive search is providedfor this case (McDonald et al, 2006).
This algo-rithm was also used for our experiments.2The parsing model of MSTParser has the advan-tage that it can be trained globally and eventuallybe applied with an exact inference algorithm.
Onthe other hand, the parser has only limited accessto the history of parsing decisions.
To avoid com-plexity problems, the scores (and the feature rep-resentations) are restricted to a single edge or ad-jacent edges.
Outsourcing labeling into a separatestage comes at the price of not being able to com-bine knowledge about the label and the structure itis attached to.
Such combined evidence, however,might be helpful for some disambiguation prob-lems.4 Guiding WCDG by Predictions ofMSTParserMSTParser predictions are integrated into the de-cision procedure of WCDG by means of two ad-ditional constraints, which monitor each depen-dency hypothesis for being in accord with the pre-diction and penalize it if a mismatch has beenfound.
One of the constraints checks the attach-ment point being the same, while the other takescare of the dependency label.To properly adjust the weights of these con-straints, it has to be determined how valuable theinformation of the predictor is relative to the infor-mation already present in the system.
This grada-tion is needed to establish a balance between theinfluence of the grammar and the predictor.
Ac-cording to the scoring principles of WCDG, a lowweight strongly deprecates all deviations from theprediction, thus forcing the system to follow themalmost without exception.
Higher weights, on theother hand, enable the grammar to override a pre-diction.
This, however, also means that predic-tions have less guiding effect of the transformationprocess.
Typically for WCDG, the best suitableweights have to be tuned on development data.To determine the best constraint weights theWCDG grammar has been extended with threeadditional constraints similar to those used forthe shift-reduce predictor in the previous experi-ments (Foth, 2006).
Two of them advise WCDGon the structural information available from theMSTParser result and one fetches the edge labelpredicted.As a result of these experiments, the optimum2MSTParser is freely available from http://sourceforge.net/projects/mstparser101weight for the attachment predictions has been ad-justed to 0.75.
Compared to a weight of 0.9 forthe shift-reduce parser, this is a rather strong in-fluence, which also reflects the differences in thereliability of these two information sources.
Witha weight of 0.9, the integration of the label predic-tions is considerably weaker, which is consistentwith their lower degree of accuracy.EvaluationThe most common general measures for the qual-ity of dependency trees are structural accuracythat points out the percentage of words correctlyattached to their head word, and labeled accuracywhich is the ratio of the correctly attached wordswhich also have the correct label.
Still, it is dif-ficult to directly compare the results reported fordifferent parsers, as the evaluation results are in-fluenced by the data used during the experiment,the domain of the data, and different annotationguidelines.
Moreover, the particular kind of POSinformation might be relevant, which either can beobtained from the manual annotations or be pro-vided by a real tagger.
Even such a conditionas the treatment of punctuation has not yet be-come a standard.
Following the evaluation proce-dure in the CoNLL-X shared task (Buchholz andMarsi, 2006), we will not include punctuation intothe performance measures, as was done in previ-ous WCDG experiments (Foth and Menzel, 2006).The source of POS tagging information will needto be specified in each individual case.All the evaluations were performed on a thou-sand sentences (18, 602 ?
19, 601) from theNEGRA treebank, the same data set that was pre-viously used in the performance evaluations ofWCDG, e.g.
in (Foth, 2006).
The NEGRAtreebank is a collection of newspaper articles; inthe original, it stores phrase structure annotations.These have been automatically translated into de-pendency trees and then manually corrected tobring them in accord with the annotation guide-lines of WCDG.
The major difference consistsin a different treatment of non-projectivity, whereWCDG only allows non-projectivity in the attach-ment of verbal arguments, relative clauses and co-ordinations, i.e., the cases where it helps to de-crease ambiguity.
Furthermore, corrections wereapplied when the annotations of NEGRA itselfturned out to be inconsistent (usually in connec-tion with co-ordinated or elliptical structures, ad-verbs and subclauses).Unfortunately, these manually corrected datawere only available for a small part (3, 000 sen-tences) of the NEGRA corpus, which is notsufficient for training MSTParser on WCDG-conforming tree structures.
Previous evaluationsof the MSTParser have used much larger train-ing sets.
E.g., during the CoNLL-X sharedtask 39,216 sentences from the TIGER Treebank(Brants et al, 2002) were used.Therefore, we used 20, 000 sentences from theonline archive of www.heise.de as an alterna-tive training set.
They have been manually an-notated according to the WCDG guidelines (andare referred to heiseticker in the following)3.The texts in this corpus are all from roughly thesame domain as in NEGRA, and although verymany technical terms and proper nouns are used,the sentences have only a slightly longer meanlength compared to the NEGRA corpus.Using POS tags from the gold annotations,MSTParser achieves 90.5% structural and 87.5%labeled accuracy on the aforementioned NEGRAtest set (Table 1).
Even a model trained on theinconsistent NEGRA data excluding the test setreaches state-of-the-art 90.5 and 87.3% for struc-tural and labeled accuracy respectively, despitethe obvious mismatch between training and testdata.
This performance is almost the same as the90.4%/87.3% reported on the TIGER data duringthe CoNLL-X 2006 shared task.Experiment structural labeledMSTParser-h 90.5 87.5MSTParser-N 90.5 87.3MSTParser(CoNLL-X) 90.4 87.3WCDG + MST 92.9 91.3WCDG + MST + 5P 93.3 92.0Table 1: Structural/labeled accuracy results withPOS tagging from the gold standard.
WCDG?
no statistical enhancements used.
MSTParser-h ?
MSTParser trained on the heiseticker.MSTParser-N ?
MSTParser trained on NEGRA.5P ?
with all five statistical predictors of WCDG.As is to be expected, if a real POS tagger is usedin the experiments with MSTParser, the accuracyis reduced quite expectedly by approximately one3The heiseticker dependency treebank is underpreparation and will be available soon.102percent to 89.5%/86.0% (Table 2 (B)).
All the re-sults obtained with a real POS tagger are summa-rized in Table 2.
For comparison, under the sameevaluation conditions, the performance of WCDGwith different predictors is summarized in Table 2(A).Experiment structural labeled(A) WCDG 88.0 86.0CP 88.6 86.5PP 89.4 87.3ST 90.8 89.2SR 90.0 88.4PP+SR 90.2 88.6ST+SR 91.0 89.4ST+PP 90.8 89.25P 91.3 90.0(B) MSTParser 89.5 86.0(C) WCDG + MST 92.0 90.5PP 92.0 90.6CP 92.1 90.6SR 92.2 90.6ST 92.4 90.9CP+SR 92.3 90.7CP+ST 92.6 91.0ST+SR 92.9 91.4PP+CP+ST 92.6 91.1PP+ST+SR 92.8 91.3CP+ST+SR 92.9 91.45P 92.9 91.4Table 2: Structural/labeled accuracy results witha real POS tagger.
(A) WCDG experiments withdifferent statistical enhancements (B) MSTParserexperiment with a real POS tagger.
(C) Com-bined experiments of WCDG and MSTParser withother statistical enhancements of WCDG.
CP ?chunker, ST ?
supertagger, PP ?
prepositionalattacher, SR ?
shift-reduce oracle parser, 5P ?POS + CP + PP + ST + SR.The combined experiments in which MSTParserwas used as a predictor for WCDG have achievedhigher accuracy than each of the combined com-ponents in isolation: the structural accuracy risesto 92.0% while the labeled accuracy also gets overthe 90%-boundary (WCDG + MST experiment inTable 2 (C)) .Finally, the MSTParser predictor was evaluatedin combination with the other predictors avail-able for WCDG.
The results of the experimentsare shown in Table 2 (C).
Every combination ofMSTParser with other predictors (first four exper-iments) improves the accuracy.
The increase ishighest (0.4%) for the combination with the su-pertagger.
This confirms earlier experiments withWCDG, in which the supertagger also contributedthe largest gains.The experimental results again confirm thatWCDG is a reliable platform for information in-tegration.
Although the use of multiple predictorsdoes not lead to an accumulation of the individualimprovements, the performance of predictor com-binations is always higher that using them sepa-rately.
A maximum performance of 92.9%/91.4%is reached with all the six available predictors ac-tive.
For comparison, the same experiment withPOS tags from the gold standard has achieved evenbetter results of 93.3%/92.0% (Table 1).Unfortunately, the PP attacher brings accuracyreductions when it is working parallel to the shift-reduce predictor (experiment PP + CP + SR in Ta-ble 2 (C)).
This effect has already been observedin the experiments that combined the two alone(experiment PP + SR in Table 2 (A)).
When MSTwas combined with the PP attacher (experimentPP in Table 2 (C)), the increase of the performancewas also below a tenth of a percent.
The possiblereasons why the use of an additional informationsource does not improve the performance in thiscase may be the disadvantages of the PP attachercompared to a full parser.5 Error AnalysisA very useful property of WCDG is that it not onlycan be used as a parser, but also as a diagnostictool for dependency structures.
Applied to a givendependency tree, any constraint violation reportedby the constraint solver indicates an inconsistencybetween the structure and the WCDG constraintgrammar.Among the most frequent hard constraint vio-lations found in the MSTParser results are doublesubjects, double objects and direct objects in pas-sive, projectivity violations, conjunctions withouta clause as well as subordinate clause without con-junction.These findings are in line with the analysis of103McDonald and Nivre (2007).
For example, theerrors in distinguishing noun complements of theverb may be due to the fact that MSTParser ismore precise for longer dependency arcs and hasno access to the parsing history.In absolute figures, MSTParser commits 1509attachment errors of which 902 are corrected byWCDG.
On the other hand, WCDG adds another542 errors of its own, so that the final result stillcontains 1149 errors.For most labels, accuracy of the predictor com-bination is higher than in each of the parsersalone.
A particularly large gain has been observedfor coordinated elements (KON and CJ), subor-dinate (NEB) and relative (REL) clauses, indi-rect accusative objects (OBJA), genitive modifiers(GMOD) and apposition (APP).
Table 3) summa-rizes the values of structural precision, the ratio ofthe number of correct attachment of a given labelto the number of all the predictions for that labelmade by the parser, and label recall, the ratio be-tween the number of correct labeling decisions anddesired labeling.In this respect, the increase in the structural pre-cision of the PP attachment seems worth men-tioning.
MSTParser attaches 79.3% of PPs cor-rectly on the used test set.
Although MSTParserdoes not use any special PP-attachment resolu-tion mechanisms, it is comparable with the re-sult of WCDG combined with the PP attacher thatachieves 78.7% structural precision for PP edges.If MSTParser is trained on NEGRA exclud-ing the test set ?
the rest of NEGRA lackingconsistence mentioned above ?
it performs evenbetter, attaching 80.4% of PP-s correctly.
Thus,MSTParser as a statistical parser trained on a fullcorpus becomes a strong competitor for a PP at-tacher that has been trained on restricted four-tuples input.As for the errors in the MSTParser output thatare most often corrected in the hybrid experiment,this happens for both the structural precision andlabel recall of most verb complements, such as di-rect and indirect objects, or clausal objects as wellas for subordinate and relative clauses for suchsubordinate clauses.It even comes to one case in which the synergytook place in spite of the incorrect predictions.
Al-though MSTParser has predicted possessive modi-fiers more seldom than WCDG alone (the label re-call of MSTParser for possessive modification was(1) (2) (3)Label p r p r p rDET 98.4 99.3 98.7 99.5 99.3 99.5PN 97.4 97.4 98.0 98.0 98.0 98.7PP 67.6 98.1 78.3 97.4 80.1 98.5ADV 76.6 94.7 79.4 95.4 82.2 97.2SUBJ 94.0 90.9 91.3 86.4 95.8 94.0ATTR 95.2 95.8 97.7 98.2 98.3 98.4S 89.2 90.1 89.3 90.5 90.5 91.0AUX 95.9 94.2 98.6 97.8 98.7 97.6OBJA 87.9 83.9 83.8 72.5 92.5 88.7APP 85.1 88.5 88.9 90.9 90.9 94.0KON 78.9 88.1 78.9 88.3 86.0 89.2CJ 85.6 86.5 90.9 91.4 93.0 93.5GMOD 90.7 90.7 89.0 85.3 96.3 95.8KONJ 88.6 91.9 91.9 95.7 95.1 95.7PRED 90.3 75.0 85.4 60.4 91.7 76.4NEB 68.9 82.8 73.0 66.4 79.5 90.2REL 64.8 77.9 59.0 77.0 68.9 86.9Table 3: Per label structural precision (p,%) and label recal (r, %) in comparison forthe experiments with the real POS tagger (1)WCDG, (2) MSTParser, (3) WCDG combinedwith MSTParserover 5% below that of WCDG) its structural pre-cision and label recall in the combined experimentare by around 6% greater than WCDG result.Cases in which WCDG performs worse withthe predictor than its predictor alone can hardly befound.
Still, one may observe many cases in whichthe predictor has a negative influence on the per-formance of WCDG, such as for different kinds ofobjects (indirect objects, object clauses and infini-tive objects) and parenthetic matrix clauses.
Forall, the result of MSTParser was below that ofthe baseline WCDG with only the POS tagger ac-tive.
Same can be said about the labeled accu-racy for split verb prefixes and nominal time ex-pressions.
This worsening effect can be attributedto the lower values of the WCDG constraints forthe corresponding labels and edges than for theMSTParser predictor.
Thus, the search could notfind a decision scoring better than that when theMSTParser prediction has been followed.Around 15% of the sentences in the test set are104not projective.
The accuracy of MSTParser on theprojective sentences of the test set is higher thanthat on the non-projective sentences by more than3 percent (Table 4), although these values can-not be compared directly as the mean length ofnon-projective sentences is longer (25.0 vs. 15.3words).Experiment Non-proj.
Proj.MSTParser (POS) 88.2 91.7WCDG (POS) 87.2 90.2WCDG (POS + SR) 88.7 92.2WCDG (POS + MST) 91.3 93.6Table 4: Structural accuracy, (%), for differentparsing runs for non-projective vs. projective sen-tences.MSTParser generally tends to find many morenon-projective edges than the data has, while theprecision remains restricted.
The number of non-projective edges was determined by counting howoften an edge crosses some other edge.
Thus, ifa non-projective edge crossed three other edgesthe number of non-projective edges equals three.For MSTParser experiments with a real POS tag-ger (MSTParser POS-experiment in Table 5), thenon-projective edge recall, the ratio of the non-projective edges found in the experiment to thecorresponding value in the gold standard, is at23% and non-projective edge precision, the ratioof the correctly found non-projective edges to allnon-projective edges found, is also only 36% (sec-ond column in Table 5).Edges SentencesExperiment r p r pMSTParser (POS) 23 36 35 44WCDG (POS) 37 53 51 63WCDG (POS + SR) 41 47 57 55WCDG (POS + MST) 48 53 61 61Table 5: Recall (r, %) and precision (p, %) of thenon-projective edges and sentences for differentparsing runs.Precision and recall of non-projective sentencesis a less rigid measure.
If at least one edge-crossing is correctly identified in a non-projectivesentence, it is added to the correctly identifiednon-projective sentences, even if the identifiededge-crossing is not the one annotated in the goldstandard and the ratios are calculated respectively(right column of Table 5).
Under these relaxedconditions, MSTParser correctly identifies slightlyless than a half of the non-projective sentencesand over a third of non-projective edges.
In fact,WCDG under the same conditions (WCDG POS-experiment in Table 5) has a non-projective sen-tence precision of 63% and a non-projective edgeprecision of 53%.
Still, WCDG misses a consid-erable amount of non-projectivities.
More impor-tantly, as the present shift-reduce predictor has notbeen designed for non-projective parsing, its in-clusion reduces the non-projective sentence andedge precision of WCDG ?
to 55% and 47% re-spectively ?
WCDG (POS+SR) in Table 5.The expected benefits for the non-projectivesentences have not yet been observed to the fullextent.
The precision of the combined system tofind non-projective sentences and edges remainedlimited by the performance that WCDG was ableto achieve alone (WCDG (POS+MST) in Table 5).While MSTParser in many cases predicts non-projectivity correctly WCDG is seldom capable ofaccepting this external evidence.
On the contrary,WCDG often accepts an incorrect projective solu-tion of the predictor instead of relying on its owncues.
In its interaction with external predictorsWCDG should typically decide about the alterna-tives.6 Related WorkSo far, approaches to hybrid parsing have beenmainly based on the idea of a post-hoc selec-tion which can be carried out for either completeparses, or individual constituents and dependencyedges, respectively.
The selection component it-self can be based on heuristics, like a majorityvote.
Alternatively, a second-level classifier istrained to decide which component to trust underwhich conditions and therefore the approach is of-ten referred to as classifier stacking.In a series of experiments, Henderson and Brill(1999) combined three constituency-based parsersby a selection mechanism for either complete pars-ing results (parser switching) or individual con-stituents (parse hybridization), using both a heuris-tic decision rule as well as a na?
?ve Bayesian clas-sifier in each case.
Among the heuristics consid-ered were majority votes for constituents and a105similarity-based measure for complete trees.
Testson Penn Treebank data showed a clear improve-ment of the combined results over the best individ-ual parser.
Constituent selection outperformed thecomplete parse selection scheme, and Bayesian se-lection was slightly superior.Instead of coupling different data-driven parserswhich all provide comparable analyses for com-plete sentences, Rupp et al (2000) combined dif-ferently elaborated structural descriptions (namelychunks and phrase structure trees) obtained bydata-driven components with the output of aHPSG-parser.
Driven by the requirements of theparticular application (speech-to-speech transla-tion), the focus was not only on parse selection,but also on combining incomplete results.
How-ever, no quantitative evaluation of the results hasbeen published.Zeman and Z?abokrtsky?
(2005) applied the se-lection idea to dependency structures and extendedit by using more context features.
They com-bined seven different parsers for Czech, amongthem also a system based on a manually com-piled rule set.
Some of the individual parsers hada fairly poor performance, but even a simple vot-ing scheme on single edges contributed a signifi-cant improvement while the best results have beenobtained for a combination that did not includethe worst components.
Alternatively the authorsexperimented with a trained selection componentwhich not only had access to the alternative localparsing results, but also to their structural context.Neither a memory-based approach nor a modelbased on decision trees did result in further gains.In two separate experiments, Sagae and Lavie(2006) combined a number of dependency andconstituent parsers, respectively.
They created anew weighted search space from the results ofthe individual component parsers using differentweighting schemes for the candidates.
They thenreparsed this search space and found a consistentimprovement for the dependency structures, butnot for the constituent-based ones.While all these approaches attempt to integratethe available evidence at parse time, Nivre andMcDonald (2008) pursued an alternative architec-ture, where integration is achieved already at train-ing time.
They combined the two state-of-the-art data-driven dependency parsers, MaltParser(Nivre et al, 2006) and MSTParser (McDonald etal., 2006), by integrating the features of each of theclassifiers into the parsing model of the other oneat training time.
Since the two parsers are basedon quite different model types (namely a history-based vs. a structure-based one), they exhibit aremarkable complementary behavior (McDonaldand Nivre, 2007).
Accordingly, significant mutualbenefits have been observed.
Note, however, thatone of the major benefits of MaltParser, its incre-mental left-to-right processing, is sacrificed undersuch a combination scheme.Martins et al (2008) use stacked learning toovercome the restriction to the single-edge fea-tures in both MaltParser and MSTParser.
Theysuggest an architecture with two layers, where theoutput of a standard parser in the first level pro-vides new features for a parser in the subsequentlevel.
During the training phase, the second parserlearns to correct mistakes made by the first one.
Itallows to involve higher-order predicted edges tosimulate non-local features in the second parser.The results are competitive with McDonald andNivre (2007) while O(n2) runtime of the spanningtree algorithm is preserved.7 ConclusionIntegrating MSTParser as a full predictor withWCDG is beneficial for both of them.
Since thesesystems take their decisions based on completelydifferent sources of knowledge, combining bothhelps avoid many mistakes each of them commitsin isolation.
Altogether, with a real POS tagger, anaccuracy level of 92.9%/91.3% has been reached(the last row in Table 2 (C)), which is higher thanwhat any of the parsers achieved alone.
With POStagging from the gold standard, the accuracy hasbeen at 93.3%/92.0% (the last row in Table 1).
Tothe knowledge of the authors, these accuracy val-ues are also better than any previous parsing re-sults on the NEGRA test set.WCDG can profit from the combination notonly with ancillary predictors for specific parsingsubtasks, but also with another full parser.
Thisresult was achieved even though the second parseris very similar to WCDG with respect to both therichness and the accuracy of its target structures.The probable reason lies in the considerable dif-ference in the error profiles of both systems as re-gards specific linguistic phenomena.
WCDG wasalso used as a diagnostic tool for the errors ofMSTParser.Possibly, a higher degree of synergy could be106achieved if a stronger coupling of the compo-nents were established by also using the scores ofMSTParser as additional information for WCDG,reflecting the intuitive notion of preference orplausibility of the predictions.
This could be donefor the optimal parse tree alone as well as for thecomplete hypothesis space.
Alternatively, the out-put of MSTParser can be used as a initial statefor the transformation procedure of WCDG.
Viceversa, MSTParser could be enriched with addi-tional features based on the output of WCDG, sim-ilar to the feature-based integration of data-drivenparsers evaluated by Nivre and McDonald (2008).At the moment, the integration constraints treatsall attachment and label predictions as being uni-formly reliable.
To individualize them with re-spect to their type or origin could not only makethe system sensitive to qualitative differences be-tween predictions (for instance, with respect todifferent labels).
It would also allow the parserto accommodate multiple oracle predictors and tocarefully distinguish between typical configura-tions in which one prediction should be preferredover an alternative one.
MaltParser (Nivre et al,2006) is certainly a good candidate for carryingout such experiments.ReferencesSabine Brants, Stefanie Dipper, Silvia Hansen, Wolf-gang Lezius, and George Smith.
2002.
The TIGERtreebank.
In: Proceedings of the First Workshop onTreebanks and Linguistic Theories (TLT), pages 24?41.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProc.
CoNLL, pages 149 ?
164.Kilian A. Foth and Wolfgang Menzel.
2006.
Hybridparsing: using probabilistic models as predictors fora symbolic parser.
In Proc.
21st Int.
Conference onComputational Linguistics and ACL-44, pages 321?328.Kilian A. Foth, Wolfgang Menzel, and Ingo Schro?der.2000.
A Transformation-based Parsing Techniquewith Anytime Properties.
In 4th Int.
Workshop onParsing Technologies, IWPT-2000, pages 89 ?
100.Kilian A. Foth.
2006.
Hybrid Methods of Natural Lan-guage Analysis.
Doctoral thesis, Hamburg Univer-sity.John C. Henderson and Eric Brill.
1999.
Exploitingdiversity in natural language processing: Combiningparsers.
In Proceedings 4th Conference on Empiri-cal Methods in Natural Language Processing, pages187?194.Andre?
F. T. Martins, Dipanjan Das, Noah A. Smith, andEric P. Xing.
2008.
Stacking Dependency Parsers.In Proc.
of the 2008 Conf.
on Empirical Methods inNatural Language Processing, pages 157 ?
166.Hiroshi Maruyama.
1990.
Structural disambiguationwith constraint propagation.
In Proc.
28th AnnualMeeting of the ACL (ACL-90), pages 31?38.Ryan McDonald and Joakim Nivre.
2007.
Character-izing the errors of data-driven dependency parsingmodels.
In Proc.
EMNLP-CoNLL, pages 122 ?
131.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Hajic?.
2005.
Non-projective dependencyparsing using spanning tree algorithms.
In Proc.HLT/EMNLP, pages 523 ?
530.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency analysis with a two-stage discriminative parser.
In Proc.
CoNLL, pages216 ?
220.Ryan McDonald.
2006.
Discriminative Learning andSpanning Tree Algorithms for Dependency Parsing.PhD dissertation, University of Pennsylvania.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proc.
ACL-08: HLT, pages 950?958.Joakim Nivre, Johan Hall, Jens Nilsson, Gu?lsenEryig?it, and Svetoslav Marinov.
2006.
Labelledpseudo-projective dependency parsing with supportvector machines.
In Proc.
CoNLL-2006, pages 221?225.Christopher G. Rupp, Jo?rg Spilker, Martin Klarner, andKarsten L. Worm.
2000.
Combining analyses fromvarious parsers.
In Wolfgang Wahlster, editor, Verb-mobil: Foundations of Speech-to-Speech Transla-tion, pages 311?320.
Springer-Verlag, Berlin etc.Kenji Sagae and Alon Lavie.
2006.
Parser combi-nations by reparsing.
In Proc.
HLT/NAACL, pages129?132.Ingo Schro?der.
2002.
Natural Language Parsing withGraded Constraints.
Ph.D. thesis, Dept.
of Com-puter Science, University of Hamburg, Germany.Ingo Schro?der, Horia F. Pop, Wolfgang Menzel, andKilian A. Foth.
2001.
Learning grammar weightsusing genetic algorithms.
In Proc.
EuroconferenceRecent Advances in Natural Language Processing,pages 235 ?
239.Daniel Zeman and Zdene?k Z?abokrtsky?.
2005.
Improv-ing parsing accuracy by combining diverse depen-dency parsers.
In Proc.
9th International Workshopon Parsing Technologies (IWPT-2005), pages 171?178, Vancouver, B.C.107
