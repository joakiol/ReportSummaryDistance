Proceedings of the Fourteenth Conference on Computational Natural Language Learning, pages 234?242,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsInspecting the Structural Biases of Dependency Parsing Algorithms ?Yoav Goldberg and Michael ElhadadBen Gurion University of the NegevDepartment of Computer SciencePOB 653 Be?er Sheva, 84105, Israelyoavg|elhadad@cs.bgu.ac.ilAbstractWe propose the notion of a structural biasinherent in a parsing system with respectto the language it is aiming to parse.
Thisstructural bias characterizes the behaviourof a parsing system in terms of structuresit tends to under- and over- produce.
Wepropose a Boosting-based method for un-covering some of the structural bias inher-ent in parsing systems.
We then applyour method to four English dependencyparsers (an Arc-Eager and Arc-Standardtransition-based parsers, and first- andsecond-order graph-based parsers).
Weshow that all four parsers are biased withrespect to the kind of annotation they aretrained to parse.
We present a detailedanalysis of the biases that highlights spe-cific differences and commonalities be-tween the parsing systems, and improvesour understanding of their strengths andweaknesses.1 IntroductionDependency Parsing, the task of inferring a depen-dency structure over an input sentence, has gaineda lot of research attention in the last couple ofyears, due in part to to the two CoNLL sharedtasks (Nivre et al, 2007; Buchholz and Marsi,2006) in which various dependency parsing algo-rithms were compared on various data sets.
As aresult of this research effort, we have a choice ofseveral robust, efficient and accurate parsing algo-rithms.
?We would like to thank Reut Tsarfaty for comments anddiscussions that helped us improve this paper.
This work issupported in part by the Lynn and William Frankel Center forComputer Science.These different parsing systems achieve com-parable scores, yet produce qualitatively differentparses.
Sagae and Lavie (2006) demonstrated thata simple combination scheme of the outputs of dif-ferent parsers can obtain substantially improvedaccuracies.
Nivre and McDonald (2008) explorea parser stacking approach in which the output ofone parser is fed as an input to a different kind ofparser.
The stacking approach also produces moreaccurate parses.However, while we know how to produce accu-rate parsers and how to blend and stack their out-puts, little effort was directed toward understand-ing the behavior of different parsing systems interms of structures they produce and errors theymake.
Question such as which linguistic phenom-ena are hard for parser Y?
and what kinds of er-rors are common for parser Z?, as well as the moreambitious which parsing approach is most suitableto parse language X?, remain largely unanswered.The current work aims to fill this gap by propos-ing a methodology to identify systematic biases invarious parsing models and proposing and initialanalysis of such biases.McDonald and Nivre (2007) analyze the dif-ference between graph-based and transition-basedparsers (specifically the MALT and MST parsers)by comparing the different kinds of errors made byboth parsers.
They focus on single edge errors, andlearn that MST is better for longer dependencyarcs while MALT is better on short dependencyarcs, that MALT is better than MST in predict-ing edges further from the root and vice-versa, thatMALT has a slight advantage when predicting theparents of nouns and pronouns, and that MST isbetter at all other word categories.
They also con-clude that the greedy MALT Parser suffer from er-ror propagation more than the globally optimized234MST Parser.In what follows, we complement their work bysuggesting a different methodology of analysis ofparsers behaviour.
Our methodology is based onthe notion of structural bias of parsers, further ex-plained in Section 2.
Instead of comparing twoparsing systems in terms of the errors they pro-duce, our analysis compares the output of a pars-ing system with a collection of gold-parsed trees,and searches for common structures which are pre-dicted by the parser more often than they appear inthe gold-trees or vice-versa.
These kinds of struc-tures represent the bias of the parsing systems, andby analyzing them we can gain important insightsinto the strengths, weaknesses and inner workingof the parser.In Section 2.2 we propose a Boosting-basedalgorithm for uncovering these structural biases.Then, in Section 3 we go on to apply our analysismethodology to four parsing systems for English:two transition-based systems and two graph-basedsystems (Sections 4 and 5).
The analysis showsthat the different parsing systems indeed possessdifferent biases.
Furthermore, the analysis high-lights the differences and commonalities amongthe different parsers, and sheds some more lighton the specific behaviours of each system.Recent work by Dickinson (2010), publishedconcurrently with this one, aims to identify depen-dency errors in automatically parsed corpora byinspecting grammatical rules which appear in theautomatically parsed corpora and do not fit wellwith the grammar learned from a manually anno-tated treebank.
While Dickinson?s main concern iswith automatic identification of errors rather thancharacterizing parsers behaviour, we feel that hiswork shares many intuitions with this one: auto-matic parsers fail in predictable ways, those wayscan be analyzed, and this analysis should be car-ried out on structures which are larger than singleedges, and by inspecting trends rather than indi-vidual decisions.2 Structural BiasLanguage is a highly structured phenomena, andsentences exhibit structure on many levels.
Forexample, in English sentences adjectives appearbefore nouns, subjects tend to appear before theirverb, and syntactic trees show a tendency towardright-branching structures.11As noted by (Owen Rambow, 2010), there is little sensein talking about the structure of a language without referringDifferent combinations of languages and anno-tation strategies exhibit different structural prefer-ences: under a specific combination of languageand annotation strategy some structures are morefrequent than others, some structures are illegaland some are very rare.We argue that parsers also exhibit such struc-tural preferences in the parses they produce.
Thesepreferences stem from various parser design deci-sions.
Some of the preferences, such as projectiv-ity, are due to explicit design decisions and lie atthe core of some parsing algorithms.
Other pref-erences are more implicit, and are due to specificinteractions between the parsing mechanism, thefeature function, the statistical mechanism and thetraining data.Ideally, we would like the structural preferencesof a parser trained on a given sample to reflect thegeneral preferences of the language.
However, aswe demonstrate in Section 3, that it is usually notthe case.We propose the notion of structural bias forquantifying the differences in structural prefer-ences between a parsing system and the languageit is aiming to parse.
The structural bias of aparser with respect to a language is composed ofthe structures that tend to occur more often in theparser?s output than in the language, and vice-versa.Structural biases are related to but different thancommon errors.
Parser X makes many PP at-tachment errors is a claim about a common error.Parser X tends to produce low attachment for PPswhile the language tends to have high attachmentis a claim about structural bias, which is related toparser errors.
Parser X can never produce struc-ture Y is a claim about a structural preference ofa parser, which may or may not be related to itserror patterns.Structural bias is a vast and vague concept.
Inorder to give a more concrete definition, we posethe following question:Assuming we are given two parses of the samesentence.
Can we tell, by looking at the parses andwithout knowing the correct parse, which parserproduced which parse?Any predictor which can help in answering thisquestion is an indicator of a structural bias.to a specific annotation scheme.
In what follow, we assume afixed annotation strategy is chosen.235Definition: structural bias between sets of treesGiven two sets of parse trees, A and B, over thesame sentences, a structural bias between thesesets is the collection of all predictors which canhelp us decide, for a tree t, whether it belongs toA or to B.The structural bias between a parsing systemand an annotated corpus is then the structural biasbetween the corpus and the output of the parseron the sentences in the corpus.
Note that thisdefinition adheres to the error vs. bias distinctiongiven above.Under this task-based definition, uncoveringstructural biases between two sets of trees amountsto finding good predictors for discriminating be-tween parses coming from these two sets of trees.In what follows, we present a rich class of struc-tural predictors, and an algorithm for efficientlysearching this predictor class for good predictors.2.1 Representing StructureA dependency representation of sentences in-cludes words and dependency relations betweenthem (one word is the ROOT of the sentence, andeach other word has a single word as its parent).Whenever possible, we would like to equate wordswith their part-of-speech tags, to facilitate gener-alization.
However, in some cases the exact iden-tity of the word may be of interest.
When ana-lyzing a language with a relatively fixed word or-der, such as English, we are also interested in thelinear order between words.
This includes the di-rection between a parent and its dependent (doesthe parent appear before or after the dependent inthe sentence?
), as well as the order among severaldependents of the same parent.
The length of a de-pendency relation (distance in words between theparent and dependent) may also be structurally in-teresting.2In order to capture this kind of information, wetake a structural element of a dependency tree tobe any connected subtree, coupled with informa-tion about the incoming edge to the root of thesubtree.
Examples of such structural elements aregiven in Figure 1.
This class of predictors is notcomplete ?
it does not directly encode, for in-stance, information about the number of siblings2Relations can also be labeled, and labeling fit naturallyin our representation.
However, we find the commonly usedset of edge labels for English to be lacking, and didn?t includeedge labels in the current analysis.
(a) JJ3(b) NN VB IN/with2Figure 1: Structural Elements Examples.
(a) is an adjectivewith a parent 3 words to its right.
(b) is a verb whose parentis on the left, it has a noun dependent on its left, and a prepo-sition dependent 2 words to its right.
The lexical item of thepreposition is with.
The lexical items and distance to parentare optional, while all other information is required.
Thereis also no information about other dependents a given wordmay have.a node has or the location of the structure relativeto the root of the tree.
However, we feel it doescapture a good deal of linguistic phenomena, andprovide a fine balance between expressiveness andtractability.The class of predictors we consider is the set ofall structural elements.
We seek to find structuralelements which appear in many trees of set A butin few trees of set B, or vice versa.2.2 Boosting Algorithm with SubtreeFeaturesThe number of possible predictors is exponentialin the size of each tree, and an exhaustive search isimpractical.
Instead, we solve the search problemusing a Boosting algorithm for tree classificationusing subtree features.
The details of the algo-rithm and its efficient implementation are given in(Kudo and Matsumoto, 2004).
We briefly describethe main idea behind the algorithm.The Boosting algorithm with subtree featuresgets as input two parse sets with labeled, orderedtrees.
The output of the algorithm is a set of sub-trees ti and their weights wi.
These weighted sub-trees define a linear classifier over trees f(T ) =?ti?Twi, where f(T ) > 0 for trees in set A andf(T ) < 0 for trees in set B.The algorithm works in rounds.
Initially, allinput trees are given a uniform weight.
At eachround, the algorithm seeks a subtree t with a max-imum gain, that is the subtree that classifies cor-rectly the subset of trees with the highest cumu-lative weight.
Then, it re-weights the input trees,so that misclassified trees get higher weights.
Itcontinues to repeatedly seek maximum gain sub-trees, taking into account the tree weights in thegain calculation, and re-weighting the trees aftereach iteration.
The same subtree can be selectedin different iterations.Kudo and Matsumoto (2004) present an effec-236(a) JJ?d:3(b) VB?NN?
IN?w:with d:2Figure 2: Encoding Structural Elements as Ordered Trees.These are the tree encodings of the structural elements in Fig-ure 1.
Direction to parent is encoded in the node name, whilethe optional lexical item and distance to parent are encodedas daughters.tive branch-and-bound technique for efficientlysearching for the maximum gain tree at eachround.
The reader is referred to their paper for thedetails.Structural elements as subtrees The boostingalgorithm works on labeled, ordered trees.
Suchtrees are different than dependency trees in thatthey contain information about nodes, but notabout edges.
We use a simple transformation toencode dependency trees and structural elementsas labeled, ordered trees.
The transformationworks by concatenating the edge-to-parent infor-mation to the node?s label for mandatory informa-tion, and adding edge-to-parent information as aspecial child node for optional information.
Figure2 presents the tree-encoded versions of the struc-tural elements in Figure 1.
We treat the direction-to-parent and POS tag as required information,while the distance to parent and lexical item areoptional.2.3 Structural Bias PredictorsThe output of the boosting algorithm is a set ofweighted subtrees.
These subtrees are good can-didates for structural bias predictors.
However,some of the subtrees may be a result of over-fittingthe training data, while the weights are tuned tobe used as part of a linear classifier.
In our ap-plication, we disregard the boosting weights, andinstead rank the predictors based on their numberof occurrences in a validation set.
We seek predic-tors which appear many times in one tree-set butfew times in the other tree-set on both the train-ing and the validation sets.
Manual inspection ofthese predictors highlights the structural bias be-tween the two sets.
We demonstrate such an anal-ysis for several English dependency parsers below.In addition, the precision of the learned Boost-ing classifier on the validation set can serve as ametric for measuring the amount of structural biasbetween two sets of parses.
A high classificationaccuracy means more structural bias between thetwo sets, while an accuracy of 50% or lower meansthat, at least under our class of predictors, the setsare structurally indistinguishable.3 Biases in Dependency Parsers3.1 Experimental SetupIn what follows, we analyze and compare thestructural biases of 4 parsers, with respect to a de-pendency representation of English.Syntactic representation The dependency tree-bank we use is a conversion of the English WSJtreebank (Marcus et al, 1993) to dependencystructure using the procedure described in (Jo-hansson and Nugues, 2007).
We use the Mel?c?ukencoding of coordination structure, in which thefirst conjunct is the head of the coordination struc-ture, the coordinating conjunction depends on thehead, and the second conjunct depend on the coor-dinating conjunction (Johansson, 2008).Data Sections 15-18 were used for training theparsers3.
The first 4,000 sentences from sections10-11 were used to train the Boosting algorithmand find structural predictors candidates.
Sec-tions 4-7 were used as a validation set for rankingthe structural predictors.
In all experiments, weused the gold-standard POS tags.
We binned thedistance-to-parent values to 1,2,3,4-5,6-8 and 9+.Parsers For graph-based parsers, we usedthe projective first-order (MST1) and second-order (MST2) variants of the freely availableMST parser4 (McDonald et al, 2005; McDon-ald and Pereira, 2006).
For the transition-basedparsers, we used the arc-eager (ARCE) variant ofthe freely available MALT parser5 (Nivre et al,2006), and our own implementation of an arc-standard parser (ARCS) as described in (Huang etal., 2009).
The unlabeled attachment accuracies ofthe four parsers are presented in Table 1.Procedure For each parser, we train a boostingclassifier to distinguish between the gold-standardtrees and the parses produced for them by the3Most work on parsing English uses a much larger train-ing set.
We chose to use a smaller set for convenience.
Train-ing the parsers is much faster, and we can get ample test datawithout resorting to jackknifing techniques.
As can be seenin Table 1, the resulting parsers are still accurate.4http://sourceforge.net/projects/mstparser/5http://maltparser.org/237MST1 MST2 ARCE ARCS88.8 89.8 87.6 87.4Table 1: Unlabeled accuracies of the analyzed parsersParser Train Accuracy Val AccuracyMST1 65.4 57.8MST2 62.8 56.6ARCE 69.2 65.3ARCS 65.1 60.1Table 2: Distinguishing parser output from gold-trees basedon structural informationparser.
We remove from the training and valida-tion sets all the sentences which the parser got100% correct.
We then apply the models to thevalidation set.
We rank the learned predictorsbased on their appearances in gold- and parser-produced trees in the train and validation sets, andinspect the highest ranking predictors.Training the boosting algorithm was done us-ing the bact6 toolkit.
We ran 400 iterations ofboosting, resulting in between 100 and 250 dis-tinct subtrees in each model.
Of these, the top 40to 60 ranked subtrees in each model were good in-dicators of structural bias.
Our wrapping code isavailable online7 in order to ease the applicationof the method to other parsers and languages.3.2 Quantitative AnalysisWe begin by comparing the accuracies of theboosting models trained to distinguish the pars-ing results of the various parsers from the Englishtreebank.
Table 2 lists the accuracies on both thetraining and validation sets.The boosting method is effective in findingstructural predictors.
All parsers output is dis-tinguishable from English trees based on struc-tural information alone.
The ArcEager variant ofMALT is the most biased with respect to English.The transition-based parsers are more structurallybiased than the graph-based ones.We now turn to analyze the specific structuralbiases of the parsing systems.
For each systemwe present some prominent structures which areunder-produced by the system (these structuresappear in the language more often than they areproduce by the parser) and some structures whichare over-produced by the system (these structures6http://chasen.org/?taku/software/bact/7http://www.cs.bgu.ac.il/?yoavg/software/are produced by the parser more often than theyappear in the language).8 Specifically, we manu-ally inspected the predictors where the ratio be-tween language and parser was high, ranked byabsolute number of occurrences.4 Transition-based ParsersWe analyze two transition-based parsers (Nivre,2008).
The parsers differ in the transition sys-tems they adopt.
The ARCE system makesuse of a transition system with four transitions:LEFT,RIGHT,SHIFT,REDUCE.
The semantics ofthis transition system is described in (Nivre,2004).
The ARCS system adopts an alterna-tive transition system, with three transitions: AT-TACHL,ATTACHR,SHIFT.
The semantics of thesystem is described in (Huang et al, 2009).
Themain difference between the systems is that theARCE system makes attachments as early as pos-sible, while the ARCS system should not attach aparent to its dependent until the dependent has ac-quired all its own dependents.4.1 Biases of the Arc-Eager SystemOver-produced structures The over-producedstructures of ARCE with respect to English areoverwhelmingly dominated by spurious ROOT at-tachments.The structures ROOT??
, ROOT?DT,ROOT?WP are produced almost 300 times bythe parser, yet never appear in the language.
Thestructures ROOT??
, ROOT?WRB , ROOT?JJappear 14 times in the language and are producedhundreds of time by the parser.
Another interest-ing case is ROOT ?
?9+ NN , produced 180 times bythe parser and appearing 7 times in the language.As indicated by the distance marking (9+), nounsare allowed to be heads of sentences, but then theyusually appear close to the beginning, a fact whichis not captured by the parsing system.
Other, lessclear-cut cases, are ROOT as the parent of IN,NN, NNS or NNP.
Such structures do appear inthe language, but are 2-5 times more common inthe parser.A different ROOT attachment bias is capturedbyROOT VBZ VBD and ROOT VBD VBD ,8One can think of over- and under- produced structuresin terms of the precision and recall metrics: over-producedstructures have low precision, while under-produced struc-tures have low recall.238appearing 3 times in the language and producedover a 100 times by the parser.It is well known that the ROOT attachment ac-curacies of transition-based systems is lower thanthat of graph-based system.
Now we can refinethis observation: the ARCE parsing system failsto capture the fact that some categories are morelikely to be attached to ROOT than others.
It alsofails to capture the constraint that sentences usu-ally have only one main verb.Another related class of biases are captured bythe structures?VBD ?
?9+ VBD,?VBD ?
?5?7 VBDand ROOT?VBZ?VBZ which are produced bythe parser twice as many times as they appearin the language.
When confronted with embed-ded sentences, the parser has a strong tendency ofmarking the first verb as the head of the secondone.The pattern ?
?+9 IN suggests that the parserprefers high attachment for PPs.
The patternDT?NN9+captures the bias of the parsertoward associating NPs with the preceding verbrather than the next one, even if this preceding verbis far away.Under-produced structures We now turn toARCE?s under-produced structures.
These includethe structures IN/that?
, MD?
, VBD?
(each 4times more frequent in the language than in theparser) and VBP?
(twice more frequent in thelanguage).
MD and that usually have their par-ents to the left.
However, in some constructionsthis is not the case, and the parser has a hard timelearning these constructions.The structure ?$?RB appearing 20 times inthe language and 4 times in the parser, reflects avery specific construction (?$ 1.5 up from $ 1.2?
).These constructions pop up as under-produced byall the parsers we analyze.The structures ?
?1 RB?IN and ?RB?JJ ap-pear twice as often in the language.
Thesestem from constructions such as ?not/RB unex-pected/JJ?, ?backed away/RB from/IN?, ?pushedback/RB in/IN?, and are hard for the parser.Lastly, the structure JJ?NN?NNS?, deviatesfrom the the ?standard?
NP construction, and issomewhat hard for the parser (39 times parser, 67in language).
However, we will see below that thissame construction is even harder for other parsers.4.2 Biases of the Arc-Standard SystemOver-produced structures The over-producedstructures of ARCS do not show the spuriousROOT attachment ambiguity of ARCE.
They doinclude ROOT?IN, appearing twice as often inthe parser output than in the language.The patterns ROOT?VBZ?
?9+, , ?VBP?
?9+,, ?VBD?
?9+VBD and ?VB?VBD all reflectthe parser?s tendency for right-branching struc-ture, and its inability to capture the verb-hierarchyin the sentence correctly, with a clear preferencefor earlier verbs as parents of later verbs.Similarly, ?
?9+NNP and ?
?9+NNS indicate a ten-dency to attach NPs to a parent on their left (as anobject) rather than to their right (as a subject) evenwhen the left candidate-parent is far away.Finally, WRB MD VB , produced48 times by the parser and twice by the language,is the projective parser?s way of annotating thecorrect non-projective structure in which the wh-adverb is dependent on the verb.Under-produced structures of ARCS in-clude two structures WRB VBN andWRB VB , which are usually part ofnon-projective structures, and are thus almostnever produced by the projective parser.Other under-produced structures include appos-itive NPs:?
IN NN ?
?
(e.g., ?by Merill , the nation?s largest firm , ?
), andthe structure NN DT NN , which canstand for apposition (?a journalist, the first jour-nalist to .
.
.
?)
or phrases such as ?30 %/NN amonth?.TO usually has its parent on its left.
When thisis not the case (when it is a part of a quantifier,such as ?x to y %?, or due to fronting: ?Due toX, we did Y?
), the parser is having a hard time toadapt and is under-producing this structure.Similar to the other parsers, ARCS also under-produces NPs with the structure JJ?NN?
?1 NN,and the structure?$?RB.Finally, the parser under-produces the con-junctive structures ?NN?CC?NN?IN and?IN?CC?IN.2395 Graph-based ParsersWe analyze the behaviour of two graph-basedparsers (McDonald, 2006).
Both parsers performexhaustive search over all projective parse trees,using a dynamic programming algorithm.
Theydiffer in the factorizations they employ to makethe search tractable.
The first-order model em-ploys a single-edge factorization, in which eachedge is scored independently of all other edges.The second-order model employs a two-edge fac-torization, in which scores are assigned to pairsof adjacent edges rather than to a single edge at atime.5.1 Biases of First-order MST ParserOver-produced structures of MST1 include:?
IN NN NN ?
IN NNP NN?
IN NNP NNS ?
IN NN VBZ/Dwhere the parsers fails to capture the factthat prepositions only have one dependent.Similarly, in the pattern: ?CC NN NNSthe parser fails to capture that only one phraseshould attach to the coordinator, and the patternsNN NN VBZ NNS NNS VBPhighlight the parser?s failing to capture thatverbs have only one object.In the structure ROOT WRB VBD , pro-duced by the parser 15 times more than it appearsin the language, the parser fails to capture the factthat verbs modified by wh-adverbs are not likelyto head a sentence.All of these over-produced structures are fineexamples of cases where MST1 fails due to itsedge-factorization assumption.We now turn to analyzing the structures under-produced by MST1.Under-produced structures The non-projective structuresWRB VBN1and WRB VB1clearly cannot be produced by the projectiveparser, yet they appear over 100 times in thelanguage.The structure WRB?VBD?VBD which isrepresented in the language five times more thanin the parser, complements the over-produced casein which a verb modified by a wh-adverb heads thesentence.IN/that?, which was under-produced byARCE is under-produced here also, but less sothan in ARCE.
?$?RB is also under-producedby the parser.The structure CC?
?1 , usually due to conjunc-tions such as either, nor, but is produced 29 timesby the parser and appear 54 times in the language.An interesting under-produced structure is?NN IN CC NN .
This structure reflectsthe fact that the parser is having a hard time coor-dinating ?heavy?
NPs, where the head nouns aremodified by PPs.
This bias is probably a resultof the ?in-between pos-tag?
feature, which listsall the pos-tags between the head and dependent.This feature was shown to be important to theparser?s overall performance, but probably fails itin this case.The construction ?
?6?8JJ, where the adjectivefunctions as an adverb (e.g., ?he survived Xunscathed?
or ?to impose Y corporate-wise?
)is also under-produced by the parser, as wellas IN NN in which the prepositionfunctions as a determiner/quantifier (?at least?,?between?, ?more than?
).Finally, MST1 is under-producing NPs withsomewhat ?irregular?
structures: JJ?NN?NNSor JJ?NN?NNS (?common stock purchase war-rants?, ?cardiac bypass patients?
), or JJ?JJ?
(?agood many short-sellers?, ?West German insur-ance giant?
)5.2 Biases of Second-order MST ParserOver-produced structures by MST2 are differ-ent than those of MST1.
The less-extreme edgefactorization of the second-order parser success-fully prevents the structures where a verb has twoobjects or a preposition has two dependents.One over-produced structure,NNS JJ NNP ?
?, produced10 times by the parser and never in the language,is due to one very specific construction, ?bondsdue Nov 30 , 1992 ,?
where the second commashould attach higher up the tree.240Another over-produced structure involvesthe internal structure of proper names:NNP NNP NNP NNP (the ?correct?
analysismore often makes the last NNP head of all theothers).More interesting are: ?
?1 CC?VBD and?
?1 CC?NN?IN .
These capture the parser?s in-ability to capture the symmetry of coordinatingconjunctions.Under-produced structures of MST2 are over-all very similar to the under-produced structures ofMST1.The structure CC?
?1 which is under-produced byMST1 is no longer under-produced by MST2.
Allthe other under-produced structures of MST1 reap-pear here as well.In addition, MST2 under-produces the struc-tures ROOT?NNP?.
(it tends not to trust NNPsas the head of sentences) and??6?8TO?
?1 V B (wherethe parser is having trouble attaching TO correctlyto its parent when they are separated by a lot ofsentential material).6 DiscussionWe showed that each of the four parsing systemsis structurally biased with respect to the Englishtraining corpus in a noticeable way: we were ableto learn a classifier that can tell, based on structuralevidence, if a parse came from a parsing systemor from the training corpus, with various successrates.
More importantly, the classifier?s models areinterpretable.
By analyzing the predictors inducedby the classifier for each parsing system, we un-covered some of the biases of these systems.Some of these biases (e.g., that transition-basedsystem have lower ROOT-attachment accuracies)were already known.
Yet, our analysis refines thisknowledge and demonstrates that in the Arc-Eagersystem a large part of this inaccuracy is not dueto finding the incorrect root among valid ambigu-ous candidates, but rather due to many illegal rootattachments, or due to illegal structures where asentence is analyzed to have two main verbs.
Incontrast, the Arc-Standard system does not sharethis spurious root attachment behaviour, and itslow root accuracies are due to incorrectly choos-ing among the valid candidates.
A related bias ofthe Arc-Standard system is its tendency to chooseearlier appearing verbs as parents of later occur-ring verbs.Some constructions were hard for all the parsingmodels.
For example, While not discussed in theanalysis above, all parsers had biased structurescontaining discourse level punctuation elements(some commas, quotes and dashes) ?
we stronglybelieve parsing systems could benefit from specialtreatment of such markers.The NP construction (JJ?NN?NNS?)
ap-peared in the analyses of all the parsers, yet wereeasier for the transition-based parsers than for thegraph-based ones.
Other NP constructions (dis-cussed above) were hard only for the graph-basedparsers.One specific construction involving the dollarsign and an adverb appeared in all the parsers,and may deserve a special treatment.
Simi-larly, different parsers have different ?soft spots?
(e.g., ?backed away from?, ?not unexpected?
forARCE, ?at least?
for MST1, TO?
for ARCS, etc.
)which may also benefit from special treatments.It is well known that the first-order edge-factorization of the MST1 parser is too strong.Our analysis reveals some specific cases wherethis assumptions indeed breaks down.
Thesecases do not appear in the second-order factoriza-tion.
Yet we show that the second-order modelunder-produces the same structures as the first-order model, and that both models have specificproblems in dealing with coordination structures,specifically coordination of NPs containing PPs.We hypothesize that this bias is due to the ?pos-in-between?
features used in the MST Parser.Regarding coordination, the analysis revealsthat different parsers show different biases with re-spect to coordination structures.7 Conclusions and Future WorkWe presented the notion of structural bias ?
spe-cific structures that are systematically over- orunder- represented in one set of parse trees relativeto another set of parse trees ?
and argue that differ-ent parsing systems exhibit different structural bi-ases in the parses they produced due to various ex-plicit and implicit decisions in parser design.
Wepresented a method for uncovering some of thisstructural bias, and effectively used it to demon-strate that parsers are indeed biased with respectto the corpus they are trained on, and that differ-ent parsers show different biases.
We then ana-lyzed the biases of four dependency parsing sys-tems with respect to an English treebank.
We ar-241gue that by studying the structural biases of pars-ing systems we can gain a better understanding onwhere dependency parsers fail, and how they dif-fer from each other.
This understanding can in turnlead us toward designing better parsing systems.We feel that the current study is just the tip ofthe iceberg with respect to the analysis of struc-tural bias.
Any parsing system for any languageand annotation scheme can benefit from such anal-ysis.ReferencesSabine Buchholz and Erwin Marsi.
2006.
CoNLL-Xshared task on multilingual dependency parsing.
InProc.
of CoNLL.Markus Dickinson.
2010.
Detecting errors inautomatically-parsed dependency relations.
In Proc.of ACL.Liang Huang, Wenbin Jiang, and Qun Liu.
2009.Bilingually-constrained (monolingual) shift-reduceparsing.
In Proc of EMNLP.Richard Johansson and Pierre Nugues.
2007.
Ex-tended constituent-to-dependency conversion for en-glish.
In Proc of NODALIDA.Richard Johansson.
2008.
Dependency-based Seman-tic Analysis of Natural-language Text.
Ph.D. thesis,Lund University.Taku Kudo and Yuji Matsumoto.
2004.
A Boost-ing Algorithm for Classification of Semi-StructuredText.
In Proceedings of EMNLP.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarchinkiewicz.
1993.
Building a large annotatedcorpus of English: The penn treebank.
Computa-tional Linguistics, 19:313?330.Ryan McDonald and Joakim Nivre.
2007.
Character-izing the errors of data-driven dependency parsingmodels.
In Proc.
of EMNLP.Ryan McDonald and Fernando Pereira.
2006.
Onlinelearning of approximate dependency parsing algo-rithms.
In Proc of EACL.Ryan McDonald, Koby Crammer, and FernandoPereira.
2005.
Online large-margin training of de-pendency parsers.
In Proc of ACL.Ryan McDonald.
2006.
Discriminative Training andSpanning Tree Algorithms for Dependency Parsing.Ph.D.
thesis, University of Pennsylvania.Joakim Nivre and Ryan McDonald.
2008.
Integrat-ing graph-based and transition-based dependencyparsers.
In Proceedings of ACL, pages 950?958.Joakim Nivre, Johan Hall, and Jens Nillson.
2006.MaltParser: A data-driven parser-generator for de-pendency parsing.
In Proc.
of LREC.Joakim Nivre, Johan Hall, Sandra Ku?bler, Ryan Mc-donald, Jens Nilsson, Sebastian Riedel, and DenizYuret.
2007.
The CoNLL 2007 shared task on de-pendency parsing.
In Proc.
of EMNLP-CoNLL.Joakim Nivre.
2004.
Incrementality in determinis-tic dependency parsing.
In Incremental Parsing:Bringing Engineering and Cognition Together, ACL-Workshop.Joakim Nivre.
2008.
Algorithms for deterministic in-cremental dependency parsing.
Computational Lin-guistics, 34(4), December.Owen Rambow.
2010.
The Simple Truth about De-pendency and Phrase Structure Representations: AnOpinion Piece.
In Proceedings of NAACL.Kenji Sagae and Alon Lavie.
2006.
Parser combina-tion by reparsing.
In Proceedings of HLT-NAACL,pages 129?133.242
