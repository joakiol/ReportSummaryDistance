Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 87?91,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsQuery log analysis with GALATEAS LangLogMarco Trevisan and Luca DiniCELItrevisan@celi.itdini@celi.itEduard BarbuUniversita` di Trentoeduard.barbu@unitn.itIgor BarsantiGonetworki.barsanti@gonetwork.itNikolaos LagosXerox Research Centre EuropeNikolaos.Lagos@xrce.xerox.comFre?de?rique Segond and Mathieu RhulmannObjet Directfsegond@objetdirect.commruhlmann@objetdirect.comEd ValdBridgeman Art Libraryed.vald@bridgemanart.co.ukAbstractThis article describes GALATEASLangLog, a system performing Search LogAnalysis.
LangLog illustrates how NLPtechnologies can be a powerful supporttool for market research even when thesource of information is a collection ofqueries each one consisting of few words.We push the standard Search Log Analysisforward taking into account the semanticsof the queries.
The main innovation ofLangLog is the implementation of twohighly customizable components thatcluster and classify the queries in the log.1 IntroductionTransaction logs become increasingly importantfor studying the user interaction with systemslikeWeb Searching Engines, Digital Libraries, In-tranet Servers and others (Jansen, 2006).
Var-ious service providers keep log files recordingthe user interaction with the searching engines.Transaction logs are useful to understand the usersearch strategy but also to improve query sugges-tions (Wen and Zhang, 2003) and to enhancethe retrieval quality of search engines (Joachims,2002).
The process of analyzing the transactionlogs to understand the user behaviour and to as-sess the system performance is known as Transac-tion Log Analysis (TLA).
Transaction Log Anal-ysis is concerned with the analysis of both brows-ing and searching activity inside a website.
Theanalysis of transaction logs that focuses on searchactivity only is known as Search Log Analysis(SLA).
According to Jansen (2008) both TLAand SLA have three stages: data collection, datapreparation and data analysis.
In the data collec-tion stage one collects data describing the userinteraction with the system.
Data preparation isthe process of loading the collected data in a re-lational database.
The data loaded in the databasegives a transaction log representation independentof the particular log syntax.
In the final stagethe data prepared at the previous step is analyzed.One may notice that the traditional three levelslog analyses give a syntactic view of the infor-mation in the logs.
Counting terms, measuringthe logical complexity of queries or the simpleprocedures that associate queries with the ses-sions in no way accesses the semantics of queries.LangLog system addreses the semantic problemperforming clustering and classification for realquery logs.
Clustering the queries in the logs al-lows the identification of meaningful groups ofqueries.
Classifying the queries according to arelevant list of categories permits the assessmentof how well the searching engine meets the userneeds.
In addition the LangLog system addressproblems like automatic language identification,Name Entity Recognition, and automatic querytranslation.
The rest of the paper is organizedas follows: the next section briefly reviews somesystems performing SLA.
Then we present thedata sources the architecture and the analysis pro-cess of the LangLog system.
The conclusion sec-tion concludes the article summarizing the workand presenting some new possible enhancementsof the LangLog.872 Related workThe information in the log files is useful in manyways, but its extraction raises many challengesand issues.
Facca and Lanzi (2005) offer a sur-vey of the topic.
There are several commercialsystems to extract and analyze this information,such as Adobe web analytics1, SAS Web Analyt-ics2, Infor Epiphany3, IBM SPSS4.
These prod-ucts are often part of a customer relation manage-ment (CRM) system.
None of those showcasesinclude any form of linguistic processing.
On theother hand, Web queries have been the subjectof linguistic analysis, to improve the performanceof information retrieval systems.
For example, astudy (Monz and de Rijke, 2002) experimentedwith shallow morphological analysis, another (Liet al 2006) analyzed queries to remove spellingmistakes.
These works encourage our belief thatlinguistic analysis could be beneficial for Web loganalysis systems.3 Data sourcesLangLog requires the following information fromthe Web logs: the time of the interaction, thequery, click-through information and possiblymore.
LangLog processes log files which con-form to the W3C extended log format.
No otherformats are supported.
The system prototype isbased on query logs spanning one month of inter-actions recorded at the Bridgeman Art Library5.Bridgeman Art library contains a large repositoryof images coming from 8000 collections and rep-resenting more than 29.000 artists.4 AnalysesLangLog organizes the search log data into unitscalled queries and hits.
In a typical search-ing scenario a user submits a query to the con-tent provider?s site-searching engine and clickson some (or none) of the search results.
Fromnow on we will refer to a clicked item as a hit,and we will refer to the text typed by the user asthe query.
This information alone is valuable tothe content provider because it allows to discover1http://www.omniture.com/en/products/analytics2http://www.sas.com/solutions/webanalytics/index.html3http://www.infor.com4http://www-01.ibm.com/software/analytics/spss/5http://www.bridgemanart.comwhich queries were served with results that satis-fied the user, and which queries were not.LangLog extracts queries and hits from the logfiles, and performs the following analyses on thequeries:?
language identification?
tokenization and lemmatization?
named entity recognition?
classification?
cluster analysisLanguage information may help the contentprovider decide whether to translate the contentinto new languages.Lemmatization is especially important in lan-guages like German and Italian that have a richmorphology.
Frequency statistics of keywordshelp understand what users want, but they are bi-ased towards items associated with words withlesser ortographic and morpho-syntactic varia-tion.
For example, two thousand queries for?trousers?, one thousand queries for ?handbag?and another thousand queries for ?handbags?means that handbags are twice as popular astrousers, although statistics based on raw wordswould say otherwise.Named entities extraction helps the contentprovider for the same reasons lemmatization does.Named entities are especially important becausethey identify real-world items that the contentprovider can relate to, while lemmas less often doso.
The name entities and the most important con-cepts can be linked afterwards with resources likeWikipedia which offer a rich specification of theirproperties.Both classification and clustering allow thecontent provider to understand what kind of theusers look for and how this information is targetedby means of queries.Classification consists of classifying queriesinto categories drawn from a classificationschema.
When the schema used to classifyis different from the schema used in the con-tent provider?s website, classification may providehints as to what kind of queries are not matchedby items in the website.
In a similar way, clusteranalysis can be used to identify new market seg-ments or new trends in the user?s behaviour.
Clus-88ter analysis provide more flexybility than classifi-cation, but the information it produces is less pre-cise.
Many trials and errors may be necessary be-fore finding interesting results.
One hopes that thefinal clustering solution will give insights into thepatterns of users?
searches.
For example an on-line book store may discover that one cluster con-tains many software-related terms, altough noneof those terms is popular enough to be noticeablein the statistics.5 ArchitectureLangLog consists of three subsystems: log ac-quisition, log analysis, log disclosure.
Periodi-cally the log acquisition subsystem gathers newdata which it passes to the log analyses compo-nent.
The results of the analyses are then availablethrough the log disclosure subsystem.Log acquisition deals with the acquisition andnormalization and anonymization of the data con-tained in the content provider?s log files.
Thedata flows from the content provider?s servers toLangLog?s central database.
This process is car-ried out by a series of Pentaho Data Integration6procedures.Log analysis deals with the anaysis of the data.The analyses proper are executed by NLP systemsprovided by third parties and accessible as Webservices.
LangLog uses NLP Web services forlanguage identification, morpho-syntactic analy-sis, named entity recognition, classification andclustering.
The analyses are stored in the databasealong with the original data.Log disclosure is actually a collection of inde-pendent systems that allow the content providersto access their information and the analyses.
Logdisclosure systems are also concerned with accesscontrol and protection of privacy.
The contentprovider can access the output of LangLog usingAWStats, QlikView, or JPivot.?
AWStats7 is a widely used log analysis sys-tem for websites.
The logs gathered from thewebsites are parsed by AWStats, which gen-erates a complete report about visitors, vis-its duration, visitor?s countries and other datato disclose useful information about the visi-tor?s behavior.6http://kettle.pentaho.com7http://awstats.sourceforge.net?
QlikView8 is a business intelligence (BI)platform.
A BI platform provides histori-cal, current, and predictive views of busi-ness operations.
Usually such tools are usedby companies to have a clear view of theirbusiness over time.
In LangLog, QlickViewdoes not display sales or costs evolution overtime.
Instead, it displays queries on the con-tent provider?s website over time.
A dash-board with many elements (input selections,tables, charts, etc.)
provides a wide range oftools to visualize the data.?
JPivot9 is a front-end for Mondrian.
Mon-drian10 is an Online Analytical Processing(OLAP) engine, a system capable of han-dling and analyzing large quantities of data.JPivot allows the user to explore the outputof LangLog, by slicing the data along manydimensions.
JPivot allows the user to displaycharts, export results to Microsoft Excel orCSV, and use custom OLAP MDX queries.Log analysis deals with the anaysis of the data.The analyses proper are executed by NLP systemsprovided by third parties and accessible as Webservices.
LangLog uses NLP Web services forlanguage identification, morpho-syntactic analy-sis, named entity recognition, classification andclustering.
The analyses are stored in the databasealong with the original data.5.1 Language IdentificationThe system uses a language identification sys-tem (Bosca and Dini, 2010) which offers languageidentification for English, French, Italian, Span-ish, Polish and German.
The system uses fourdifferent strategies:?
N-gram character models: uses the distancebetween the character based models of theinput and of a reference corpus for the lan-guage (Wikipedia).?
Word frequency: looks up the frequency ofthe words in the query with respect to a ref-erence corpus for the language.?
Function words: searches for particleshighly connoting a specific language (suchas prepositions, conjunctions).8http://www.qlikview.com9http://jpivot.sourceforge.net10http://mondrian.pentaho.com89?
Prior knowledge: provides a default guessbased on a set of hypothesis and heuristicslike region/browser language.5.2 LemmatizationTo perform lemmatization, Langlog uses general-purpose morpho-syntactic analysers based on theXerox Incremental Parser (XIP), a deep robustsyntactic parser (Ait-Mokhtar et al 2002).
Thesystem has been adapted with domain-specificpart of speech disambiguation grammar rules, ac-cording to the results a linguistic study of the de-velopment corpus.5.3 Named entity recognitionLangLog uses the Xerox named entity recogni-tion web service (Brun and Ehrmann, 2009) forEnglish and French.
XIP includes also a namedentity detection component, based on a combina-tion of lexical information and hand-crafted con-textual rules.
For example, the named entityrecognition system was adapted to handle titlesof portraits, which were frequent in our dataset.While for other NLP tasks LangLog uses the samesystem for every content provider, named entityrecognition is a task that produces better analyseswhen it is tailored to the domain of the content.Because LangLog uses a NER Web service, it iseasy to replace the default NER system with a dif-ferent one.
So if the content provider is interestedin the development of a NER system tailored fora specific domain, LangLog can accomodate this.5.4 ClusteringWe developed two clustering systems: one per-forms hierarchical clustering, another performssoft clustering.?
CLUTO: the hierarchical clustering systemrelies on CLUTO411, a clustering toolkit.To understand the main ideas CLUTO isbased on one might consult Zhao andKarypis (2002).
The clustering process pro-ceeds as follows.
First, the set of queries tobe clustered is partitioned in k groups wherek is the number of desired clusters.
To doso, the system uses a partitional clusteringalgorithm which finds the k-way clusteringsolution making repeated bisections.
Then11http://glaros.dtc.umn.edu/gkhome/views/clutothe system arranges the clusters in a hierar-chy by successively merging the most similarclusters in a tree.?
MALLET: the soft clustering system wedeveloped relies on MALLET (McCallum,2002), a Latent Dirichlet Allocation (LDA)toolkit (Steyvers and Griffiths, 2007).Our MALLET-based system considers thateach query is a document and builds a topicmodel describing the documents.
The result-ing topics are the clusters.
Each query is as-sociated with each topic according to a cer-tain strenght.
Unlike the system based onCLUTO, this system produces soft clusters,i.e.
each query may belong to more than onecluster.5.5 ClassificationLangLog allows the same query to be classifiedmany times using different classification schemasand different classification strategies.
The resultof the classification of an input query is always amap that assigns each category a weight, wherethe higher the weight, the more likely the querybelongs to the category.
If NER performs bet-ter when tailored to a specific domain, classifi-cation is a task that is hardly useful without anycustomization.
We need a different classificationschema for each content provider.
We developedtwo classification system: an unsupervised sys-tem and a supervised one.?
Unsupervised: this system does not requireany training data nor any domain-specificcorpus.
The output weight of each categoryis computed as the cosine similarity betweenthe vector models of the most representa-tive Wikipedia article for the category andthe collection of Wikipedia articles most rel-evant to the input query.
Our evaluation inthe KDD-Cup 2005 dataset results in 19.14precision and 22.22 F-measure.
For com-parison, the state of the art in the competi-tion achieved a 46.1 F-measure.
Our systemcould not achieve a similar score because itis unsupervised, and therefore it cannot makeuse of the KDD-Cup training dataset.
In ad-dition, it uses only the query to perform clas-sification, whereas KDD-Cup systems werealso able to access the result sets associatedto the queries.90?
Supervised: this system is based on theWeka framework.
Therefore it can use anymachine learning algorithm implemented inWeka.
It uses features derived from thequeries and from Bridgeman metadata.
Wetrained a Naive Bayes classifier on a set of15.000 queries annotated with 55 categoriesand hits and obtained a F-measure of 0.26.The results obtained for the classificationare encouraging but not yet at the level ofthe state of the art.
The main reason forthis is the use of only in-house meta-data inthe feature computation.
In the future wewill improve both components by providingthem with features from large resources likeWikipedia or exploiting the results returnedby Web Searching engines.6 DemonstrationOur demonstration presents:?
The setting of our case study: the BridgemanArt Library website, a typical user search,and what is recorded in the log file.?
The conceptual model of the results of theanalyses: search episodes, queries, lemmas,named entities, classification, clustering.?
The data flow across the parts of the system,from content provider?s servers to the front-end through databases, NLP Web servicesand data marts.?
The result of the analyses via QlikView.7 ConclusionIn this paper we presented the LangLog system,a customizable system for analyzing query logs.The LangLog performs language identification,lemmatization, NER, classification and clusteringfor query logs.
We tested the LangLog system onqueries in Bridgeman Library Art.
In the futurewe will test the system on query logs in differ-ent domains (e.g.
pharmaceutical, hardware andsoftware, etc.)
thus increasing the coverage andthe significance of the results.
Moreover we willincorporate in our system the session informationwhich should increase the precision of both clus-tering and classification components.ReferencesSalah Ait-Mokhtar, Jean-Pierre Chanod and ClaudeRoux 2002.
Robustness Beyond Shallowness: In-cremental Deep Parsing.
Journal of Natural Lan-guage Engineering 8, 2-3, 121-144.Alessio Bosca and Luca Dini.
2010.
Language Identi-fication Strategies for Cross Language InformationRetrieval.
CLEF 2010 Working Notes.C.
Brun and M. Ehrmann.
2007.
Adaptation ofa Named Entity Recognition System for the ES-TER 2 Evaluation Campaign.
In proceedings ofthe IEEE International Conference on Natural Lan-guage Processing and Knowledge Engineering.F.
M. Facca and P. L. Lanzi.
2005.
Mining interestingknowledge from weblogs: a survey.
Data Knowl.Eng.
53(3):225241.Jansen, B. J.
2006.
Search log analysis: What is it;what?s been done; how to do it.
Library and Infor-mation Science Research 28(3):407-432.Jansen, B. J.
2008.
The methodology of search loganalysis.
In B. J. Jansen, A. Spink and I. Taksa (eds)Handbook of Web log analysis 100-123.
Hershey,PA: IGI.Joachims T. 2002.
Optimizing search engines us-ing clickthrough data.
In proceedings of the 8thACM SIGKDD international conference on Knowl-edge discovery and data mining 133-142.M.
Li, Y. Zhang, M. Zhu, and M. Zhou.
2006.
Ex-ploring distributional similarity based models forquery spelling correction.
In proceedings of In ACL06: the 21st International Conference on Computa-tional Linguistics and the 44th annual meeting ofthe ACL 10251032, 2006.Andrew Kachites McCallum.
2002.
MAL-LET: A Machine Learning for Language Toolkit.http://mallet.cs.umass.edu.C.
Monz and M. de Rijke.
2002.
Shallow Morpholog-ical Analysis in Monolingual Information Retrievalfor Dutch, German and Italian.
In Proceedings ofCLEF 2001.
SpringerM.
Steyvers and T. Griffiths.
2007.
ProbabilisticTopic Models.
In T. Landauer, D McNamara, S.Dennis and W. Kintsch (eds), Handbook of LatentSemantic Analysis, Psychology Press.J.
R. Wen and H.J.
Zhang 2003.
Query Clusteringin the Web Context.
In Wu, Xiong and Shekhar(eds) Information Retrieval and Clustering 195-226.
Kluwer Academic Publishers.Y.
Zhao and G. Karypis.
2002.
Evaluation of hierar-chical clustering algorithms for document datasets.In proceedings of the ACM Conference on Informa-tion and Knowledge Management.91
