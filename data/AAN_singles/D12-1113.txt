Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1234?1244, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsLearning-based Multi-Sieve Co-reference Resolution with Knowledge?Lev RatinovGoogle Inc.?ratinov@google.comDan RothUniversity of Illinois at Urbana-Champaigndanr@illinois.eduAbstractWe explore the interplay of knowledge andstructure in co-reference resolution.
To injectknowledge, we use a state-of-the-art systemwhich cross-links (or ?grounds?)
expressionsin free text to Wikipedia.
We explore waysof using the resulting grounding to boost theperformance of a state-of-the-art co-referenceresolution system.
To maximize the utility ofthe injected knowledge, we deploy a learning-based multi-sieve approach and develop novelentity-based features.
Our end system outper-forms the state-of-the-art baseline by 2 B3 F1points on non-transcript portion of the ACE2004 dataset.1 IntroductionCo-reference resolution is the task of grouping men-tions to entities.
For example, consider the text snip-pet in Fig.
11.
The correct output groups the men-tions {m1,m2,m5} to one entity while leaving m3?We thank Nicholas Rizzolo and Kai Wei Chang for theirinvaluable help with modifying the baseline co-reference sys-tem.
We thank the anonymous EMNLP reviewers for con-structive comments.
This research was supported by the ArmyResearch Laboratory (ARL) under agreement W911NF-09-2-0053 and by the Defense Advanced Research Projects Agency(DARPA) Machine Reading Program under Air Force ResearchLaboratory (AFRL) prime contract no.
FA8750-09-C-0181.Any opinions, findings, conclusions or recommendations arethose of the authors and do not necessarily reflect the view ofthe ARL, DARPA, AFRL, or the US government.?
The majority of this work was done while the first authorwas at the University of Illinois.1Throughout this paper, curly brackets {} denote the extentand square brackets [] denote the head.
?After the {[vessel]}m1 suffered a catastrophic torpedodetonation, {[Kursk]}m2 sank in the waters of {[BarentsSea]}m3 with all hands lost.
Though rescue attempts wereoffered by a nearby {Norwegian [ship]}m4 , Russia declinedinitial rescue offers, and all 118 sailors and officers aboard{[Kursk]}m5 perished.
?Figure 1: Example illustrating the challenges in co-referenceresolution.and m4 as singletons.
Resolving co-reference is fun-damental for understanding natural language.
Forexample in Fig.
1, to infer that Kusrk has suffereda torpedo detonation, we have to understand that{[vessel]}m1 refers to {[Kursk]}m2.This inference is typically trivial for humans, butproves extremely challenging for state-of-the-art co-reference resolution systems.
We believe that it isworld knowledge that gives people the ability to un-derstand text with such ease.
A human reader can in-fer that since Kursk sank, it must be a vessel and ves-sels which suffer catastrophic torpedo detonationscan sink.
Moreover, some readers might just knowthat Kursk is a Russian submarine named after thecity of Kursk, where the largest tank battle in his-tory took place in 1943.
In this work we are usingWikipedia as a source of encyclopedic knowledge.The key contributions of this work are:(1) Using Wikipedia to assign a set of knowledgeattributes to mentions in a context-sensitive way.For example, for the text in Fig.
1, our system as-signs to the mention ?Kursk?
the nationalities: Rus-sian, Soviet and the attributes ship, incident, subma-rine, shipwreck (as opposed to city or battle).
Weare using a publicly available system for context-1234sensitive disambiguation to Wikipedia.
We thenextract attributes from the cross-linked Wikipediapages (described in Sec.
3.1), assign these attributesto the document mentions (Sec.
3.2) and developknowledge-rich compatibility metric between men-tions (Sec.
3.3)2.
(2) Integrating the strength of rule-based systemssuch as (Haghighi and Klein, 2009; Raghunathan etal., 2010) into a machine learning framework.
Weare using a multi-sieve approach (Raghunathan etal., 2010), which splits pairwise ?co-reference?
vs.?non-coreference?
decisions to different types andattempts to make the easy decisions first (Goldbergand Elhadad, 2010).
Our multi-sieve approach isdifferent from (Raghunathan et al 2010) in sev-eral respects: (a) our sieves are machine-learningclassifiers, (b) the same pair of mentions can fallinto multiple sieves, (c) later sieves can overridethe decisions made by earlier sieves, allowing to re-cover from errors as additional evidence becomesavailable.
In our running example, the decisionof whether {[vessel]}m1 refers to {[Kursk]}m2 ismade before the decision of whether {[vessel]}m1refers to {Norwegian [ship]}m4 since decisions inthe same sentence are believed to be easier thancross-sentence ones.
We describe our learning-based multi-sieve approach in Sec.
4.
(3) A novel approach for entity-based features.
Assieves of classifiers are applied, our system attemptsto model entities and share the attributes between thementions belonging to the same entity.
Once the de-cision that {[vessel]}m1 and {[Kursk]}m2 co-refer ismade, we want the two mentions to share the Rus-sian nationality.
This allows us to avoid erroneouslylinking {[vessel]}m1 to {Norwegian [ship]}m4 de-spite vessel and ship being synonyms in Word-Net.
However, in this work we allow the sieves tomake conflicting decisions on the same pair of men-tions.
Hence, obtaining entities and their attributesby straightforward transitive closure of co-referencepredictions is impossible.
We describe our approachfor leveraging possibly contradicting predictions inSec.
5.
(4) By adding word-knowledge features and us-2The extracted attributes and the related re-sources are available for public download athttp://cogcomp.cs.illinois.edu/Data/Ace2004CorefWikiAttributes.zipInput: document d; mentions M = {m1, .
.
.
,mN}1) For each mi ?
M , assign it a Wikipedia page pi in acontext-sensitive way (pi may be null).- If pi 6= null: extract knowledge attributes from pi andassign to m.- Else extract knowledge attributes directly from m vianoun-phrase parsing techniques (Vadas and Curran, 2008).3) Let Q = {(mi,mj)}i6=j , be the queue of mentionpairs approximately sorted by ?easy-first?
(Goldberg andElhadad, 2010).4) Let G be a partial clustering graph.5) While Q is not empty- Extract a pair p = (mi,mj) from Q.- Using the knowledge attributes of mi and mj as well asthe structure of G, classify whether p is co-referent.- Update G with the classification decision.6) Construct an end clustering from G.Figure 2: High-level system architecture.ing learning-based multi-sieve approach, we im-prove the performance of the state-of-the-art systemof (Bengtson and Roth, 2008) by 3 MUC, 2 B3 and2 CEAF F1 points on the non-transcript portion ofthe ACE 2004 dataset.
We report our experimen-tal results in Sec.
6 and conclude with discussion inSec.
7.We conclude the introduction by giving a high-level overview of our system in Fig.
2.2 Baseline SystemIn this work, we are using the state-of-the-art sys-tem of (Bengtson and Roth, 2008), which relieson a pairwise scoring function pc to assign an or-dered pair of mentions a probability that they arecoreferential.
It uses a rich set of features includ-ing: string edit distance, gender match, whether thementions appear in the same sentence, whether theheads are synonyms in WordNet etc.
The functionpc is modeled using regularized averaged percep-tron for a tuned number of training rounds, learn-ing rate and margin.
For the end system, we keepthese parameters intact, our only modifications willbe adding knowledge-rich features and adding inter-mediate classification sieves to the training and theinference, which we will discuss in the followingsections.At inference time, given a document d and apairwise co-reference scoring function pc, (Bengt-son and Roth, 2008) generate a graph Gd accord-1235ing to the Best-Link decision model (Ng and Cardie,2002) as follows.
For each mention m in docu-ment d, let Bm be the set of mentions appearingbefore m in d. Let a be the highest scoring an-tecedent: a = argmaxb?Bm(pc(b,m)).
We will addthe edge (a,m) to Gd if pc(a,m) predicts the pair tobe co-referent with a confidence exceeding a chosenthreshold, then we take the transitive closure3.The properties of the Best-Link inference are il-lustrated in Fig.
3.
At this stage, we ask the readerto ignore the knowledge attributes at the bottom ofthe figure.
Let us assume that the pairwise classi-fier labeled the mentions (m2,m5) co-referent be-cause they have identical surface form; mentions(m1,m4) are co-referred because the heads are syn-onyms in WordNet.
Let us assume that since m1and m2 appear in the same sentence, the pairwiseclassifier managed to leverage the dependency parsetree to correctly co-ref the pair (m1,m2).
The tran-sitive closure would correctly link (m1,m5) despitethe incorrect prediction of the pairwise classifier on(m1,m5), and would incorrectly link m4 with allother mentions because of the incorrect pairwiseprediction on (m1,m4) and despite the correct pre-dictions on (m2,m4) and (m4,m5).Figure 3: A sample output of a pairwise co-reference classifier.The full edges represent a co-ref prediction and the empty edgesrepresent a non-coref prediction.
A set of knowledge attributesfor selected mentions is shown as well.3 Wikipedia as KnowledgeIn this section we describe our methodology for us-ing Wikipedia as a knowledge resource.
In Sec.
3.1we cover the process of knowledge extraction from3We use Platt Scaling while (Bengtson and Roth, 2008) usedthe raw output value of pc.Wikipedia pages.
We describe how to inject thisknowledge into mentions in Sec.
3.2.
The bottompart of Fig.
3 illustrates the knowledge attributes oursystem injects to two sample mentions at this stage.Finally, in Sec.
3.3 we describe a compatibility met-ric our system learns over the injected knowledge.3.1 Wikipedia Knowledge AttributesOur goal in this section is to extract from Wikipediapages a compact and highly-accurate set of knowl-edge attributes, which nevertheless possesses dis-criminative power for co-reference4.
We concentrateon three types of knowledge attributes: fine-grainedsemantic categories, gender information and nation-ality where applicable.Each Wikipedia page is assigned a set of cat-egories.
There are over 100K categories inWikipedia, many are extremely fine-grained andcontain very few pages.
The value of the Wikipediacategory structure for knowledge acquisition haslong been noticed in several influential works, suchas (Suchanek et al 2007; Nastase and Strube, 2008)to name a few.
However, while the recall of theabove resources is excellent, we found their preci-sion insufficient for our purposes.
We have imple-mented a simple high-precision low-recall heuris-tic for extracting the head words of Wikipedia cat-egories as follows.We noticed that Wikipedia categories have a sim-ple structure of either <noun-phrase> or <noun-phrase><relation-token><noun-phrase>, wherein the second case the category information is al-ways on the left.
Therefore, we first remove thetext succeeding a set of carefully chosen relation to-kens5.
With this heuristic ?Recipients of the GoldMedal of the Royal Astronomical Society?
becomesjust ?Recipients?
; ?Populated places in Africa?
be-comes ?places?
; however ?Institute for AdvancedStudy faculty?
becomes ?Institute?
(rather than?faculty?).
At the second step, we apply the Illi-nois POS tagger and keep only the tokens labeled asNNS.
This step allows us to exclude singular nounsincorrectly identified as heads, such as ?Institute?above.
To further reduce the noise in the category4We justify the reasons for our choice of high-precision low-recall knowledge extraction in Sec.
3.2.5The selected set was: {of, in, with, from, ?,?, at, who,which, for, and, by}1236extraction, we also remove all rare category tokenswhich appeared in less than 100 titles ending up with2088 fine-grained entity types.
We manually mappopular fine-grained categories to coarser-grainedones, more consistent with ACE entity typing.
Asample of the mapping is shown in the table below:Fine-grained Coarse-graineddepartments, organizations, banks, .
.
.
ORGvenues, trails, areas, buildings, .
.
.
LOCcountries, towns, villages, .
.
.
GPEchurches, highways, schools, .
.
.
FACILITYManual inspection of the extracted category key-words has led us to believe that this heuristicachieves a higher precision at a considerable lossof recall when compared to the more sophisticatedapproach of (Nastase and Strube, 2008), whichcorrectly identifies ?faculty?
as the head of ?Insti-tute for Advanced Study faculty?, but incorrectlyidentifies ?statistical organizations?
as the head of?Presidents of statistical organizations?
in abouthalf the titles containing the category6.We assign gender to the titles using the follow-ing simple heuristic.
The first paragraph of eachWikipedia article provides a very brief summaryof the entity in focus.
If the first paragraph of aWikipedia page contains the pronoun ?she?, but not?he?, the article is considered to be about a female(and vice-versa).
However, when the page is as-signed a non-person-related fine-grained NE type(e.g.
school) and at the same time is not assigneda person-related fine-grained NE type (e.g.
novel-ist), we mark the page as inanimate regardless of thepresence of he/she pronouns.
The nationality is as-signed by matching the tokens in the original (un-processed) categories of the Wikipedia page to a listof countries.
We assign nationality not only to theWikipedia titles, but also to single tokens.
For eachtoken, we track the list of titles it appears in, and ifthe union of the nationalities assigned to the titles itappears in is less than 7, we mark the token compat-ible with these nationalities.
This allows us to iden-tify Ivan Lebedev as Russian and Ronen Ben-Zoharas Israeli, even though Wikipedia may not containpages for these specific people.6 (Nastase and Strube, 2008) analyze a set of categories Sassigned to Wikipedia page p jointly, hence the same categoryexpression can be interpreted differently, depending on S.3.2 Injecting Knowledge AttributesOnce we have extracted the knowledge attributes ofWikipedia pages, we need to inject them into thementions.
(Rahman and Ng, 2011) used YAGO forsimilar purposes, but noticed that knowledge injec-tion is often noisy.
Therefore they used YAGO onlyfor mention pairs where one mention was an NEof type PER/LOC/ORG and the other was a com-mon noun.
This implies that all MISC NEs werediscarded, and all NE-NE pairs were discarded aswell.
We also note that (Rahman and Ng, 2011)reports low utility of FrameNet-based features.
Infact, when incrementally added to other features incluster-ranking model the FrameNet-based featuressometimes led to performance drops.
This observa-tion has motivated our choice of high-precision low-recall heuristic in Sec.
3.1 and will motivate us toadd features conservatively when building attributecompatibility metric in Sec.
3.3.Additionally, while (Rahman and Ng, 2011) usesthe union of all possible meanings a mention mayhave in Wikipedia, we deploy GLOW (Ratinov etal., 2011)7, a context-sensitive system for disam-biguation to Wikipedia.
Using context-sensitive dis-ambiguation to Wikipedia as well as high-precisionset of knowledge attributes allows us to inject theknowledge to more mention pairs when comparedto (Rahman and Ng, 2011).
Our exact heuristic forinjecting knowledge attributes to mentions is as fol-lows:Named Entities with Wikipedia DisambiguationIf the mention head is an NE matched to a Wikipediapage p by GLOW, we import all the knowledge at-tributes from p. GLOW allows us to map ?EphraimSneh?
to http://en.wikipedia.org/wiki/Efraim Snehand to assign it the Israeli nationality, male gender,and the fine-grained entity types: {member, politi-cian, person, minister, alumnus, physician, gen-eral}.Head and Extent KeywordsIf the mention head is not mapped to Wikipedia byGLOW and the head contains keywords which ap-pear in the list of 2088 fine-grained entity types,then the rightmost such keyword is added to the listof mention knowledge attributes.
If the head does7Available at: http://cogcomp.cs.illinois.edu/page/software_view/Wikifier1237not contain any entity-type keywords but the extentdoes, we add the rightmost such keyword of the ex-tent.
In both cases, we apply the heuristic of re-moving clauses starting with a select set of punctua-tions, prepositions and pronouns, annotating what isleft with POS tagger and restricting to noun tokensonly8.
This allows us to inject knowledge to men-tions unmapped to Wikipedia, such as: ?
{currentCycle World publisher [Larry Little]}?, which is as-signed the attribute publisher but not world or cy-cle.
Likewise, ?
{[Joseph Conrad Parkhurst], whofounded the motorcycle magazine Cycle World in1962 }?, is not assigned the attribute magazine,since the text following ?who?
is discarded.3.3 Learning Attributes CompatibilityIn the previous section we have assigned knowledgeattributes to the mentions.
Some of this information,such as gender and coarse-grained entity types arealso modeled in the baseline system of (Bengtsonand Roth, 2008).
Our goal is to build a compatibilitymetric on top of this redundant, yet often inconsis-tent information.The majority of the features we are using arestraightforward, such as: (1) whether the two men-tions mapped to the same Wikipedia page, (2)gender agreement (both Wikipedia and dictionary-based), (3) nationality agreement (here we measureonly whether the sets intersect, since mentions canhave multiple nationalities in the real world), (4)coarse-grained entity type match, etc.The only non-trivial feature is measuring com-patibility between sets of fine-grained entity types,which we describe below.
Let us assume that men-tion m1 was assigned the set of fine-grained entitytypes S1 and the mention m2 was assigned the setof fine-grained entity types S2.
We record whetherS1 and S2 share elements.
If they do, than, in addi-tion to the Boolean feature, the list of the shared el-ements also appears as a list of discrete features.
Wedo the same for the most similar and most dissimilarelements of S1 and S2 (along with their discretizedsimilarity score) according to a WordNet-based sim-ilarity metric of (Do et al 2009).
The reason for ex-plicitly listing the shared, the most similar and dis-8This heuristic is similar to the one we used for extractingWikipedia category headwords and seems to be a reasonablebaseline for parsing noun structures (Vadas and Curran, 2008).similar elements is that the WordNet similarity doesnot always correspond to co-reference compatibil-ity.
For example, the pair (company, rival) has alow similarity score according to WordNet, but char-acterizes co-referent mentions.
On the other hand,the pair (city, region) has a high WordNet simi-larity score, but characterizes non-coreferent men-tions.
We want to allow our system to ?memorize?the discrepancy between the WordNet similarity andco-reference compatibility of specific pairs.We also note that we generate a set of selectedconjunctive features, most notably of fine-grainedcategories with NER predictions.
The reason isthat the pair of mentions ?
(Microsoft, Google)?
arenot co-referent despite the fact that they both havethe company attribute.
On the other hand ?
(Mi-crosoft, Redmond-based company)?
is a co-referentpair.
To capture this difference, we generate thefeature ORG-ORG&&share attribute for the firstpair, and ORG-O&&share attribute for the secondpair9.
These features are also used in conjunctionwith string edit distance.
Therefore, if our systemsees two named entities which share the same fine-grained type but have a large string edit distance, itwill label the pair as non-coref.4 Learning-based Multi-Sieve AproachState-of-the-art machine-learning co-ref systems,e.g.
(Bengtson and Roth, 2008; Rahman and Ng,2011) train a single model for predicting co-reference of all mention pairs.
However, rule-basedsystems, e.g.
(Haghighi and Klein, 2009; Raghu-nathan et al 2010) characterize mention pairs bydiscourse structure and linguistic properties and ap-ply rules in a prescribed order (high-precision rulesfirst).
Somewhat surprisingly, such hybrid approachof applying rules on top of structures produced bystatistical tools (such as dependency parse trees) per-forms better than pure machine-learning approach10.In this work, we attempt to integrate the strengthof linguistically motivated rule-based systems withthe robustness of a machine learning approach.
Westarted with a hypothesis that different types of men-9The head of ?Redmond-based company?
is ?company?,which is not a named entity, and is marked O.10(Raghunathan et al 2010) recorded the best result onCoNLL 2011 shared task.1238tion pairs may require a different co-ref model.
Forexample, consider the text below:Queen Rania of Jordan , Egypt?s [Suzanne Mubarak]m1 andothers were using their charisma and influence to campaignfor equality of the sexes.
[Mubarak]m2 , wife of EgyptianPresident [Hosni Mubarak]m3 , and one of the conferenceorganizers, said they must find ways to .
.
.There is a subtle difference between mention pairs(m1,m2) and (m2,m3).
One of the differences ispurely structural.
The first pair appears in differentsentences, while the second pair ?
in the same sen-tence.
It turns out that string edit distance feature be-tween two named entities has different ?semantics?depending on whether the two mentions appear inthe same sentence.
The reason is that to avoid redun-dancy, humans refer to the same entity differentlywithin the sentence, preferring titles, nicknames andpronouns.
Therefore, when a similar-looking namedentities appear in the same sentence, they are ac-tually likely to refer to different entities.
On theother hand, in the sentence ?Reggie Jackson, nick-named Mr. October .
.
.
?
we have to rely heavily onsentence structure rather than string edit distance tomake the correct co-ref prediction.Trained on Sieve-specificSieve All Data TrainingAllSentencePairs 61.37 67.46ClosestNonProDiffSent 60.71 63.33NonProSameSentence 62.97 63.80NerMentionsDiffSent 86.44 87.12SameSentenceOneNer 64.10 68.88Adjacent 71.00 78.80SameSenBothNer 75.30 73.75Nested 76.11 79.00Table 1: F1 performance on co-referent mention pairs by sievetype when trained with all data versus sieve-specific data only.Our second intuition is that ?easy-first?
inferenceis necessary to effectively leverage knowledge.
Forexample, in Fig.
3, our goal is to link vessel toKursk and assign it the Russian/Soviet nationalityprior to applying the pairwise co-reference classi-fier on (vessel, Norwegian ship).
Therefore, ourgoal is to apply the pairwise classifier on pairs inprescribed order and to propagate the knowledgeacross mentions.
The ordering should be such that(a) maximum amount of information is injected atearly stages (b) the precision at the early stages is ashigh as possible (Raghunathan et al 2010).
Hence,we divide the mention pairs as follows:Nested: are pairs such as ?
{{[city]m1} of [Jerusalem]m2}?where the extent of one of the mentions contains the extent ofthe other.
For some mentions, the extent is the entire clause, sowe also added a requirement that mention heads are at most 7tokens apart.
Intuitively, it is the easiest case of co-reference.There are 5,804 training samples and 992 testing samples, outof which 208 are co-referent.SameSenBothNer: are pairs of named entities which appearin the same sentence.
We already saw an example for thiscase involving [Mubarak]m2 and [Hosni Mubarak]m3.
Thereare 13,041 training samples and 1,746 testing samples, out ofwhich 86 are co-referent.Adjacent: are pairs of mentions which appear closest to eachother on the dependency tree.
We note that most of the nestedpairs are also adjacent.
There are training 5,872 samples and895 testing samples, out of which 219 are co-referent.SameSentenceOneNer: are pairs which appear in the samesentence and exactly one of the mentions is a named entity, andthe other is not a pronoun.
Typical pairs are ?Israel-country?,as opposed to ?Bill Clinton - reporter?.
This type of pairs isfairly difficult, but our hope is to use encyclopedic knowledgeto boost the performance.
There are 15,715 training samplesand 2,635 testing samples, out of which 207 are co-referent.NerMentionsDiffSent: are pairs of mentions in different sen-tences, both of which are named entities.
There are 189,807training samples and 24,342 testing samples, out of which 1,628are co-referent.NonProSameSentence: are pairs in the same sentence, whereboth mentions are non-pronouns.
This sieve includes all thepairs in the SameSentenceOneNer sieve.
Typical pairs are?city-capital?
and ?reporter-celebrity?.
There are 33,895training samples and 5,393 testing samples, out of which 336are co-referent..ClosestNonProDiffSent: are pairs of mentions in different sen-tences with no other mentions between the two.
3,707 train-ing samples and 488 testing samples, out of which 38 are co-referent.AllSentencePairs: All mention pairs within same sentence.There are 49,953 training samples and 7,809 testing samples,out of which 846 are co-referent.TopSieve: The set of mention pairs classified by the baselinesystem.
525,398 training samples and 85,358 testing samples,out of which 1,387 are co-referent.In Tab.
1 we compare the performance at eachsieve in two scenarios11.
First, we train with the en-tire 525,398 training samples, and then we train on11The data is described in Sec.
6.1.1239whatever training data is available for the specificsieve12.
We were surprised to see that the F1 on thenested mentions, when trained on the 5,804 sieve-specific samples improves to 79.00 versus 76.11when trained on the 525,398 top sieve samples.There are several things to note when interpretingthe results in Tab 1.
First, the sheer ratio of positiveto negative samples fluctuates drastically.
For exam-ple, 208 out of the 992 testing samples at the nestedsieve are positive, while there are only 86 positivesamples out of 1,746 testing samples in the Same-SenBothNer sieve.
It seems unreasonable to use thesame model for inference at both sieves.
Second, thedata for intermediate sieves is not always a subset ofthe top sieve.
The reason is that top sieve extractsa positive instance only for the closest co-referentmentions, while sieves such as AllSentencePairs ex-tract samples for all co-referent pairs which appearin the same sentence.
Third, while our division tosieves may resemble witchcraft, it is motivated bythe intuition that mentions appearing close to oneanother are easier instances of co-ref as well as lin-guistic insights of (Raghunathan et al 2010).5 Entity-Based FeaturesIn this section we describe our approach for build-ing entity-based features.
Let {C1, C2, .
.
.
CN} bethe set of sieve-specific classifiers.
In our case, C1 isthe nested mention pairs classifier, C2 is the Same-SenBothNer classifier, and C9 is the top sieve clas-sifier.
We design entity-based features so that thesubsequent sieves ?see?
the decisions of the previ-ous sieves and use entity-based features based on theintermediate clustering.
However, unlike (Raghu-nathan et al 2010), we allow the subsequent sievesto change the decisions made by the lower sieves(since additional information becomes available).5.1 Intermediate Clustering Features (IC)Let Ri(m) be the set of all mentions which, whenpaired with the mention m, form valid sample pairsfor sieve i. E.g.
in our running example of Fig.
1,12We report pairwise performance on mention pairs becauseit is the more natural metric for the intermediate sieves.
Wereport only performance on co-referent pairs, because for manysieves, such as the top sieve, 99% of the mention pairs are non-coreferent, hence the baseline of labeling all samples as non-coreferent would result in 99% accuracy.
We are interested in amore challenging baseline, the co-referent pairs.R2([Kursk]m2) = {[Barents Sea]m3}, since bothm1 and m2 are NEs and appear in the same sen-tence.
Let R+i (m) be the set of all mentions whichwere labeled as co-referent to the mention m by theclassifier Ci (including m, which is co-referent toitself).
We define R?i (m) similarly.
We denote theunion of mentions co-refed to m during inferenceup to sieve i as E+i (m) = ?i?1j=1R+j (m).
Similarly,E?i (m) = ?i?1j=1R?j (m).
Using these definitionswe can introduce entity-based prediction featureswhich allow subsequent sieves to use informationaggregated from previous sieves:ICRi (mj ,mk) =???
?1 mj ?
R?i?1(mk)+1 mj ?
R+i?1(mk)0 OtherwiseICEi (mj ,mk) =???
?1 mj ?
E?i?1(mk)+1 mj ?
E+i?1(mk)0 OtherwiseICRi stores the pairwise prediction history, thuswhen classifying a pair (mj ,mk) at sieve i, aclassifier can see the predictions of all the previoussieves applicable on that pair.
ICEi stores thetransitive closures of the sieve-specific predictions.We note that both ICRi and ICEi can have the values+1 and -1 active at the same time if intermediatesieve classifiers generated conflicting predictions.However, a classifier at sieve i will use as featuresboth ICR1 ,.
.
.
ICRi?1 and ICE1 ,.
.
.
ICEi?1, thus itwill know the lowest sieve at which the conflictingevidence occurs.
The classifier at sieve i alsouses set identity, set containment, set overlap andother set comparison features between E+/?i?1 (mj)and E+/?i?1 (mk).
We check whether the sets havesymmetric difference, whether the size of theintersection between the two sets is at least halfthe size of the smallest set etc.
We also generatesubtypes of set comparison features when restrictingthe elements to NE-mentions and non-pronominalmentions (e.g ?what percent of named entities dothe sets have in common??
).5.2 Surface Form Compatibility (SFC)The intermediate clustering features do not allow usto generalize predictions from pairs of mentions topairs of surface strings.
For example, if we havethree mentions: {[vessel]m1 , [Kursk]m2 , [Kursk]m5},then the prediction on the pair (m1,m2) will not be1240(B)aseline (B)+Knowledge (B)+Predictions (B)+Knowledge+PredictionsTopSieve 66.58 69.08 68.77 70.43AllSentencePairs 67.46 71.79 69.59 73.50ClosestNonProDiffSent 63.33 65.62 65.57 70.76NonProSameSentence 63.80 69.62 67.03 71.11NerMentionsDiffSent 87.12 88.23 88.68 89.07SameSentenceOneNer 68.88 70.58 67.89 73.17Adjacent 78.80 81.32 80.00 81.79SameSenBothNer 73.75 80.50 77.21 80.98Nested 79.00 83.59 80.65 83.37Table 2: Utility of knowledge and prediction features (F1 on co-referent mention pairs) by inference sieves.
Both knowledge andentity-based features significantly and independently improve the performance for all sieves.
The goal of entity-based features isto propagate knowledge effectively, thus it is encouraging that the combination of entity-based and knowledge features performedsignificantly better than any of the approaches individually at the top sieve.used for the prediction on the pair (m1,m5), eventhough in both pairs we are asking whether Kurskcan be referred to as vessel.
The surface form com-patibility features mirror the intermediate clusteringfeatures, but relax mention IDs and replace themby surface forms.
Similarly to intermediate cluster-ing features, both +1 and -1 values can be active atthe same time.
We also generate subtypes of set-comparison features for NE-mentions and optionallystemmed non-pronominal mentions.
For example,in a text discussing President Clinton and PresidentPutin, some instances of the surface from presidentwill refer to Putin but not Clinton and vice-versa.Therefore, both for (Putin, president) and for (Clin-ton, president), the surface from compatibility willbe +1 and -1 simultaneously.
This indicates to thesystem that Putin can be referred to as president, butpresident can refer to other entities in the documentas well.6 Experiments and Results6.1 DataWe use the official ACE 2004 English trainingdata (NIST, 2004).
We started with the data splitused in (Culotta et al 2007), which used 336 doc-uments for training and 107 documents for testing.We note that ACE data contains both newswire textand transcripts.
In this work, we are using NLP toolssuch as POS tagger, named entity recognizer, shal-low parser, and a disambiguation to Wikipedia sys-tem to inject expressive features into a co-referencesystem.Unfortunately, current state-of-the-art NLP toolsdo not work well on transcribed text.
Therefore, wediscard all the transcripts.
Our criteria was simple.The ACE annotators have marked the named enti-ties both in newswire and in the transcripts.
We keptonly those documents which contained named en-tities (according to manual ACE annotation) and atleast 1/3 of the named entities started with a capitalletter.
After this pre-processing step, we were leftwith 275 out of the original 336 training documents,and 42 out of the 107 testing documents.For the experiments throughout this paper, fol-lowing Culotta et al(Culotta et al 2007) and muchother work, to make experiments more compara-ble across systems, we assume that perfect mentionboundaries and mention type labels are given.
How-ever, we do not use the gold named entity types suchas person/location/facility etc.
available in the data.In all experiments we automatically split words andsentences, and annotate the text with part-of-speechtags, named entities and cross-link concepts fromthe text to Wikipedia using publicly available tools.6.2 Ablation StudyIn Tab.
2 we report the pairwise F1 scores on co-referent mention pairs broken down by sieve andusing different components.
This allows us to see,for example, that adding only the knowledge at-tributes improved the performance at NonProSame-Sentence sieve from 63.80 to 69.62.
We have or-dered the sieves according to our initial intuition of?easy first?.
We were surprised to see that co-ref res-olution for named entities in the same sentence washarder than cross-sentence (73.75 vs. 87.12 base-124175767778798081820.5 0.9 0.999 1E-6 1E-9 1E-12 1E-15 0F1- MUCConfidence threshold for a positive predictionBaselineKnowledge&PredictionsKnowledgeOnlyPredictionsOnly81.58282.58383.58484.58585.50.5 0.9 0.999 1E-6 1E-9 1E-12 1E-15 0F1- B3Confidence threshold for a positive predictionBaselineKnowledge&PredictionsKnowledgeOnlyPredictionsOnly73747576777879800.5 0.9 0.999 1E-6 1E-9 1E-12 1E-15 0F1- CEAFConfidence threshold for a positive predictionBaselineKnowledge&PredictionsKnowledgeOnlyPredictionsOnlyFigure 4: End performance for various systems.line F1).
We were also surprised to see that resolv-ing all mention pairs within sentence when includ-ing pronouns was easier than resolving pairs whereboth mentions were non-pronouns (67.46 vs. 63.80baseline F1).We note that conceptually, the nested(B)+Predictions sieve should be identical tothe baseline.
However, in practice, the surfaceform compatibility (SFC) features are generatedfor the nested sieve as well.
Given two mentionsm1 and m2, the SFC features capture how manysurface forms E+(m1) and E+(m2) share.
At thenested sieve, E+(m) and R+(m) are just m, whichis identical to string comparison features alreadyexisting in the baseline system.
While the SFCfeatures do not add new information, they influencethe weight the features get (essentially leading toa different regularization), which in turn leads toslightly different results.6.3 End system performanceRecall that the Best-Link algorithm applies transi-tive closure on the graph generated by thresholdingthe pairwise co-reference scoring function pc.
Thelower the threshold on the positive prediction, thelower is the precision and the higher is the recall.
InFig.
4 we compare the end clustering quality acrossa variety of thresholds and for various system fla-vors using three metrics: MUC (Vilain et al 1995),B3 (Bagga and Baldwin, 1998) and CEAF (Luo,2005)13.
The purpose of this comparison is to seethe impact of the knowledge and the prediction fea-tures on the final output and to see whether the per-formance gains are due to (mis-)tuning of one ofthe systems or are they consistent across a varietyof thresholds.The end performance of the baseline systemon our training/testing split peaks at around 78.39MUC, 83.03 B3 and 77.52 CEAF, which is higher(e.g.
3 B3 F1 points) than the originally reportedresult on the entire dataset (which includes the tran-scripts).
This is expected, since well-formed text iseasier to process than transcripts.
We note that ourbaseline is a state-of-the art system which recordedthe highest B3 and BLANC scores at CoNLL 2011shared task and took the third place overall.
Fig.
4shows a minimum improvement of 3 MUC, 2 B3and 1.25 CEAF F1 points across all thresholds whencomparing the baseline to our end system.
Surpris-ingly, the knowledge features outperformed predic-tion features on pairwise, MUC and B3 metrics, butnot on the CEAF metric.
This shows that pairwiseperformance is not always indicative of cluster-levelperformance for all metrics.7 Conclusions and Related WorkTo illustrate the strengths of our approach, let usconsider the following text:Another terminal was made available in {[Jiangxi]m1}, an{inland [province]m2}.
.
.
.
The previous situation wherebylarge amount of goods for {Jiangxi [province]m3} had tobe re-shipped through Guangzhou and Shanghai will bechanged completely.The baseline system assigns each mention to aseparate cluster.
The pairs (m1,m2) and (m1,m3)13In the interest of space, we refer the reader to the literaturefor details about the different metrics.1242are misclassified because the baseline classifier doesnot know that Jiangxi is a province and the preposi-tion an before m2 is interpreted to mean it is a pre-viously unmentioned entity.
The pair (m2,m3) ismisclassified because identical heads have differentmodifiers, as in (big province, small province).
Ourend system first co-refs (m1,m2) at the AllSameSen-tence sieve due to the knowledge features, and thenco-refs (m1,m3) at the top sieve due to surface formcompatibility features indicating that province wasobserved to refer to Jiangxi in the document.
Thetransitivity of Best-Link takes care of (m2,m3).However, our approach has multiple limitations.Entity-based features currently do not propagateknowledge attributes directly, but through aggregat-ing pairwise predictions at knowledge-infused inter-mediate sieves.
We rely on gold mention bound-aries and exhaustive gold co-reference annotation.This prevented us from applying our approach tothe Ontonotes dataset where singleton clusters andco-referent nested mentions are removed.
There-fore the gold annotation for training several sievesof our scheme is missing (e.g.
nested mentions).Another limitation is our somewhat preliminary di-vision to sieves.
(Vilalta and Rish, 2003) have ex-perimented with approaches for automatic decom-position of data to subclasses and learning multiplemodels to improve data separability.
We hope thatsimilar approach would be useful for co-referenceresolution.
Ideally, we want to make ?simple de-cisions?
first, similarly to what was done in (Gold-berg and Elhadad, 2010) for dependency parsing,and model clustering as a structured problem, sim-ilarly to (Joachims et al 2009; Wick et al 2011).However, our experience with multi-sieve approachwith classifiers suggests that a single model wouldnot perform well for both lower sieves with littleentity-based information and higher sieves with a lotof entity-based features.
Addressing the aforemen-tioned challenges is a subject for future work.There has been an increasing interest inknowledge-rich co-reference resolution (Ponzettoand Strube, 2006; Haghighi and Klein, 2010; Rah-man and Ng, 2011).
We use Wikipedia differentlyfrom (Ponzetto and Strube, 2006) who focus onusing WikiRelate, a Wikipedia-based relatednessmetric (Strube and Ponzetto, 2006).
(Rahman andNg, 2011) used the union of all possible inter-pretations a mention may have in YAGO, whichmeans that Michael Jordan could be co-refed bothto a scientist and basketball player in the samedocument.
Additionally, (Rahman and Ng, 2011)use exact word matching, relying on YAGO?s abilityto extract a comprehensive set of facts offline14.
Weare the first to use context-sensitive disambiguationto Wikipedia, which received a lot of attention re-cently (Bunescu and Pasca, 2006; Cucerzan, 2007;Mihalcea and Csomai, 2007; Milne and Witten,2008; Ratinov et al 2011).
We extract context-sensitive, high-precision knowledge attributes fromWikipedia pages and apply (among other features)WordNet similarity metric on pairs of knowledgeattributes to determine attribute compatibility.We have integrated the strengths of rule-basedsystems such as (Haghighi and Klein, 2009; Raghu-nathan et al 2010) into a multi-sieve machine learn-ing framework.
We show that training sieve-specificmodels significantly increases the performance onmost intermediate sievesieves.We develop a novel approach for entity-based in-ference.
Unlike (Rahman and Ng, 2011) who con-struct entities left-to-right, and similarly to (Raghu-nathan et al 2010) we resolve easy instances of co-ref to reduce error propagation in entity-based fea-tures.
Unlike (Raghunathan et al 2010), we al-low later stages of inference to change the decisionsmade at lower stages as additional entity-based evi-dence becomes available.By adding word-knowledge features and refin-ing the inference, we improve the performance of astate-of-the-art system of (Bengtson and Roth, 2008)by 3 MUC, 2 B3 and 2 CEAF F1 points on the non-transcript portion of the ACE 2004 dataset.ReferencesA.
Bagga and B. Baldwin.
1998.
Algorithms for scoringcoreference chains.
In MUC7.E.
Bengtson and D. Roth.
2008.
Understanding the valueof features for coreference resolution.
In EMNLP.14YAGO uses WordNet to expand its set of facts.
For ex-ample, if Martha Stewart is assigned the meaning personalityfrom category head words analysis, YAGO adds the meaningcelebrity because personality is a direct hyponym of celebrity inWordNet.
However, this is done offline in a context-insensitiveway, which is inherently limited.1243R.
C. Bunescu and M. Pasca.
2006.
Using encyclope-dic knowledge for named entity disambiguation.
InEACL.S.
Cucerzan.
2007.
Large-scale named entity dis-ambiguation based on Wikipedia data.
In EMNLP-CoNLL.A.
Culotta, M. Wick, R. Hall, and A. McCallum.
2007.First-order probabilistic models for coreference reso-lution.
In HLT/NAACL, pages 81?88.Q.
Do, D. Roth, M. Sammons, Y. Tu, and V. Vydiswaran.2009.
Robust, light-weight approaches to computelexical similarity.
Technical report, University of Illi-nois at Urbana-Champaign.A.
Fader, S. Soderland, and O. Etzioni.
2009.
Scalingwikipedia-based named entity disambiguation to arbi-trary web text.
In WikiAI (IJCAI workshop).Y.
Goldberg and M. Elhadad.
2010.
An efficient algo-rithm for easy-first non-directional dependency pars-ing.
In NAACL.A.
Haghighi and D. Klein.
2009.
Simple coreferenceresolution with rich syntactic and semantic features.In EMNLP.A.
Haghighi and D. Klein.
2010.
Coreference resolutionin a modular, entity-centered model.
In HLT-ACL.
As-sociation for Computational Linguistics.T.
Joachims, T. Hofmann, Y. Yue, and C. Yu.
2009.Predicting structured objects with support vector ma-chines.
Communications of the ACM, Research High-light, 52(11):97?104, November.X.
Luo.
2005.
On coreference resolution performancemetrics.
In HLT.R.
Mihalcea and A. Csomai.
2007.
Wikify!
: linking doc-uments to encyclopedic knowledge.
In CIKM.D.
Milne and I. H. Witten.
2008.
Learning to link withwikipedia.
In CIKM.V.
Nastase and M. Strube.
2008.
Decoding wikipediacategories for knowledge acquisition.
In AAAI.V.
Ng and C. Cardie.
2002.
Improving machine learningapproaches to coreference resolution.
In ACL.NIST.
2004.
The ace evaluation plan.www.nist.gov/speech/tests/ace/index.htm.S.
P. Ponzetto and M. Strube.
2006.
Exploiting semanticrole labeling, wordnet and wikipedia for coreferenceresolution.
In HLT-ACL.K.
Raghunathan, H. Lee, S. Rangarajan, N. Chambers,M.
Surdeanu, D. Jurafsky, and C. Manning.
2010.A multi-pass sieve for coreference resolution.
InEMNLP.A.
Rahman and V. Ng.
2011.
Coreference resolutionwith world knowledge.
In HLT-ACL.L.
Ratinov, D. Downey, M. Anderson, and D. Roth.2011.
Local and global algorithms for disambiguationto wikipedia.
In ACL.M.
Strube and S. P. Ponzetto.
2006.
WikiRelate!
Com-puting Semantic Relatedness Using Wikipedia.
InProceedings of the Twenty-First National Conferenceon Artificial Intelligence, July.F.
M. Suchanek, G. Kasneci, and G. Weikum.
2007.Yago: A core of semantic knowledge.
In WWW.D.
Vadas and J. R. Curran.
2008.
Parsing noun phrasestructure with CCG.
In ACL.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreferencescoring scheme.
In MUC6, pages 45?52.R.
Vilalta and I. Rish.
2003.
A decomposition of classesvia clustering to explain and improve naive bayes.
InECML.M.
Wick, K. Rohanimanesh, K. Bellare, A. Culotta, andA.
McCallum.
2011.
Samplerank: Training factorgraphs with atomic gradients.
In ICML.1244
