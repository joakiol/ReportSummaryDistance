Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 37?40,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsBeyond Log-Linear Models:Boosted Minimum Error Rate Training for N-best Re-rankingKevin Duh?Dept.
of Electrical EngineeringUniversity of WashingtonSeattle, WA 98195kevinduh@u.washington.eduKatrin KirchhoffDept.
of Electrical EngineeringUniversity of WashingtonSeattle, WA 98195katrin@ee.washington.eduAbstractCurrent re-ranking algorithms for machinetranslation rely on log-linear models, whichhave the potential problem of underfitting thetraining data.
We present BoostedMERT, anovel boosting algorithm that uses MinimumError Rate Training (MERT) as a weak learnerand builds a re-ranker far more expressive thanlog-linear models.
BoostedMERT is easy toimplement, inherits the efficient optimizationproperties of MERT, and can quickly boost theBLEU score on N-best re-ranking tasks.
Inthis paper, we describe the general algorithmand present preliminary results on the IWSLT2007 Arabic-English task.1 IntroductionN-best list re-ranking is an important component inmany complex natural language processing applica-tions (e.g.
machine translation, speech recognition,parsing).
Re-ranking the N-best lists generated froma 1st-pass decoder can be an effective approach be-cause (a) additional knowledge (features) can be in-corporated, and (b) the search space is smaller (i.e.choose 1 out of N hypotheses).Despite these theoretical advantages, we have of-ten observed little gains in re-ranking machine trans-lation (MT) N-best lists in practice.
It has oftenbeen observed that N-best list rescoring only yieldsa moderate improvement over the first-pass outputalthough the potential improvement as measured bythe oracle-best hypothesis for each sentence is much?Work supported by an NSF Graduate Research Fellowship.higher.
This shows that hypothesis features are ei-ther not discriminative enough, or that the rerankingmodel is too weakThis performance gap can be mainly attributed totwo problems: optimization error and modeling er-ror (see Figure 1).1 Much work has focused on de-veloping better algorithms to tackle the optimizationproblem (e.g.
MERT (Och, 2003)), since MT eval-uation metrics such as BLEU and PER are riddledwith local minima and are difficult to differentiatewith respect to re-ranker parameters.
These opti-mization algorithms are based on the popular log-linear model, which chooses the English translatione of a foreign sentence f by the rule:argmaxe p(e|f) ?
argmaxe?Kk=1 ?k?k(e, f)where ?k(e, f) and ?k are the K features andweights, respectively, and the argmax is over all hy-potheses in the N-best list.We believe that standard algorithms such asMERT already achieve low optimization error (thisis based on experience where many random re-startsof MERT give little gains); instead the score gap ismainly due to modeling errors.
Standard MT sys-tems use a small set of features (i.e.
K ?
10) basedon language/translation models.2 Log-linear mod-els on such few features are simply not expressiveenough to achieve the oracle score, regardless ofhow well the weights {?k} are optimized.1Note that we are focusing on closing the gap to the oraclescore on the training set (or the development set); if we werefocusing on the test set, there would be an additional term, thegeneralization error.2In this work, we do not consider systems which utilize alarge smorgasbord of features, e.g.
(Och and others, 2004).37BLEU=.40, achieved by re-ranking with MERTBLEU=.56, achieved byselecting oracle hypothesesModeling problem:Log-linear model insufficient?Optimization problem:Stuck in local optimum?Figure 1: Both modeling and optimization problems in-crease the (training set) BLEU score gap between MERTre-ranking and oracle hypotheses.
We believe that themodeling problem is more serious for log-linear modelsof around 10 features and focus on it in this work.To truly achieve the benefits of re-ranking in MT,one must go beyond the log-linear model.
The re-ranker should not be a mere dot product operation,but a more dynamic and complex decision makerthat exploits the structure of the N-best re-rankingproblem.We present BoostedMERT, a general frameworkfor learning such complex re-rankers using standardMERT as a building block.
BoostedMERT is easy toimplement, inherits MERT?s efficient optimizationprocedure, and more effectively boosts the trainingscore.
We describe the algorithm in Section 2, reportexperiment results in Section 3, and end with relatedwork and future directions (Sections 4, 5).2 BoostedMERTThe idea for BoostedMERT follows the boostingphilosophy of combining several weak classifiersto create a strong overall classifier (Schapire andSinger, 1999).
In the classification case, boostingmaintains a distribution over each training sample:the distribution is increased for samples that are in-correctly classified and decreased otherwise.
In eachboosting iteration, a weak learner is trained to opti-mize on the weighted sample distribution, attempt-ing to correct the mistakes made in the previous iter-ation.
The final classifier is a weighted combinationof weak learners.
This simple procedure is very ef-fective in reducing training and generalization error.In BoostedMERT, we maintain a sample distribu-tion di, i = 1 .
.
.M over the M N-best lists.3 In3As such, it differs from RankBoost, a boosting-based rank-ing algorithm in information retrieval (Freund et al, 2003).
Ifeach boosting iteration t, MERT is called as as sub-procedure to find the best feature weights ?t on di.4The sample weight for an N-best list is increased ifthe currently selected hypothesis is far from the ora-cle score, and decreased otherwise.
Here, the oraclehypothesis for each N-best list is defined as the hy-pothesis with the best sentence-level BLEU.
The fi-nal ranker is a combination of (weak) MERT rankeroutputs.Algorithm 1 presents more detailed pseudocode.We use the following notation: Let {xi} representthe set of M training N-best lists, i = 1 .
.
.M .
EachN-best list xi contains N feature vectors (for N hy-potheses).
Each feature vector is of dimension K,which is the same dimension as the number of fea-ture weights ?
obtained by MERT.
Let {bi} be theset of BLEU statistics for each hypothesis in {xi},which is used to train MERT or to compute BLEUscores for each hypothesis or oracle.Algorithm 1 BoostedMERTInput: N-best lists {xi}, BLEU scores {bi}Input: Initialize sample distribution di uniformlyInput: Initialize y0 = [0], a constant zero vectorOutput: Overall Ranker: fT1: for t = 1 to T do2: Weak ranker: ?t = MERT({xi},{bi},di)3:4: if (t ?
2): {yt?1} = PRED(f t?1, {xi})5: {yt} = PRED(?t, {xi})6: ?t = MERT([yt?1; yt],{bi})7: Overall ranker: f t = yt?1 + ?tyt8:9: for i = 1 to M do10: ai = [BLEU of hypothesis selected by f t]divided by [BLEU of oracle hypothesis]11: di = exp(?ai)/normalizer12: end for13: end forapplied on MT, RankBoost would maintain a weight for eachpair of hypotheses and would optimize a pairwise ranking met-ric, which is quite dissimilar to BLEU.4This is done by scaling each BLEU statistic, e.g.
n-gramprecision, reference length, by the appropriate sample weightsbefore computing corpus-level BLEU.
Alternatively, one couldsample (with replacement) the N-best lists using the distribu-tion and use the resulting stochastic sample as input to an un-modified MERT procedure.38The pseudocode can be divided into 3 sections:1.
Line 2 finds the best log-linear feature weightson distribution di.
MERT is invoked as a weaklearner, so this step is computationally efficientfor optimizing MT-specific metrics.2.
Lines 4-7 create an overall ranker by combin-ing the outputs of the previous overall rankerf t?1 and current weak ranker ?t.
PRED is ageneral function that takes a ranker and a MN-best lists and generates a set of M N -dimoutput vector y representing the predicted re-ciprocal rank.
Specifically, suppose a 3-best listand a ranker predicts ranks (1,3,2) for the 1st,2nd, and 3rd hypotheses, respectively.
Theny = (1/1,1/3,1/2) = (1,0.3,0.5).5Finally, using a 1-dimensional MERT, thescalar parameter ?t is optimized by maximiz-ing the BLEU of the hypothesis chosen byyt?1+?tyt.
This is analogous to the line searchstep in boosting for classification (Mason et al,2000).3.
Lines 9-11 update the sample distribution disuch that N-best lists with low accuracies aiare given higher emphasis in the next iteration.The per-list accuracy ai is defined as the ratio ofselected vs. oracle BLEU, but other measuresare possible: e.g.
ratio of ranks, difference ofBLEU.The final classifier fT can be seen as a voting pro-cedure among multiple log-linear models generatedby MERT.
The weighted vote for hypotheses in anN-best list xi is represented by the N-dimensionalvector: y?
=?Tt=1 ?tyt =?Tt=1 ?t PRED(?t,xi).We choose the hypothesis with the maximum valuein y?Finally, we stress that the above algorithmis an novel extension of boosting to re-rankingproblems.
There are many open questions andone can not always find a direct analog betweenboosting for classification and boosting for rank-ing.
For instance, the distribution update scheme5There are other ways to define a ranking output that areworth exploring.
For example, a hard argmax definition wouldbe (1,0,0); a probabilistic definition derived from the dot prod-uct values can also be used.
It is the definition of PRED thatintroduces non-linearities in BoostedMERT.of Lines 9-11 is recursive in the classificationcase (i.e.
di = di ?
exp(LossOfWeakLearner)),but due to the non-decompositional properties ofargmax in re-ranking, we have a non-recursiveequation based on the overall learner (di =exp(LossOfOverallLearner)).
This has deep impli-cations on the dynamics of boosting, e.g.
the distri-bution may stay constant in the non-recursive equa-tion, if the new weak ranker gets a small ?.3 ExperimentsThe experiments are done on the IWSLT 2007Arabic-to-English task (clean text condition).
Weused a standard phrase-based statistical MT system(Kirchhoff and Yang, 2007) to generated N-best lists(N=2000) on Development4, Development5,and Evaluation sub-sets.
Development4 isused as the Train set; N-best lists that have the samesentence-level BLEU statistics for all hypotheses arefiltered since they are not important in impactingtraining.
Development5 is used as Dev set (inparticular, for selecting the number of iterations inboosting), and Evaluation (Eval) is the blinddataset for final ranker comparison.
Nine featuresare used in re-ranking.We compare MERT vs. BoostedMERT.
MERT israndomly re-started 30 times, and BoostedMERT isrun for 30 iterations, which makes for a relativelyfair comparison.
MERT usually does not improveits Train BLEU score, even with many random re-starts (again, this suggests that optimization erroris low).
Table 1 shows the results, with Boosted-MERT outperforming MERT 42.0 vs. 41.2 BLEUon Eval.
BoostedMERT has the potential to achieve43.7 BLEU, if a better method for selecting optimaliterations can be devised.It should be noted that the Train scores achievedby both MERT and BoostedMERT is still far fromthe oracle (around 56).
We found empirically thatBoostedMERT is somewhat sensitive to the size (M )of the Train set.
For small Train sets, BoostedMERTcan improve the training score quite drastically; forthe current Train set as well as other larger ones, theimprovement per iteration is much slower.
We planto investigate this in future work.39MERT BOOST ?Train, Best BLEU 40.3 41.0 0.7Dev, Best BLEU 24.0 25.0 1.0Eval, Best BLEU 41.2 43.7 2.5Eval, Selected BLEU 41.2 42.0 0.8Table 1: The first three rows show the BLEU score forTrain, Dev, and Eval from 30 iterations of BoostedMERTor 30 random re-restarts of MERT.
The last row showsthe actual BLEU on Eval when selecting the numberof boosting iterations based on Dev.
Last column in-dicates absolute improvements.
BoostedMERT outper-forms MERT by 0.8 points on Eval.4 Related WorkVarious methods are used to optimize log-linearmodels in re-ranking (Shen et al, 2004; Venugopalet al, 2005; Smith and Eisner, 2006).
Althoughthis line of work is worthwhile, we believe moregain is possible if we go beyond log-linear models.For example, Shen?s method (2004) produces large-margins but observed little gains in performance.Our BoostedMERT should not be confused withother boosting algorithms such as (Collins and Koo,2005; Kudo et al, 2005).
These algorithms arecalled boosting because they iteratively choose fea-tures (weak learners) and optimize the weights forthe boost/exponential loss.
They do not, however,maintain a distribution over N-best lists.The idea of maintaining a distribution over N-best lists is novel.
To the best of our knowledge,the most similar algorithm is AdaRank (Xu and Li,2007), developed for document ranking in informa-tion retrieval.
Our main difference lies in Lines 4-7in Algorithm 1: AdaRank proposes a simple closedform solution for ?
and combines only weak fea-tures, not full learners (as in MERT).
We have alsoimplemented AdaRank but it gave inferior results.It should be noted that the theoretical trainingbounds derived in the AdaRank paper is relevantto BoostedMERT.
Similar to standard boosting, thisbound shows that the training score can be improvedexponentially in the number of iterations.
However,we found that the conditions for which this bound isapplicable is rarely satisfied in our experiments.66The explanation for this is beyond the scope of this paper;the basic reason is that our weak rankers (MERT) are not weakin practice, so that successive iterations get diminishing returns.5 ConclusionsWe argue that log-linear models often underfit thetraining data in MT re-ranking, and that this is thereason we observe a large gap between re-ranker andoracle scores.
Our solution, BoostedMERT, createsa highly-expressive ranker by voting among multipleMERT rankers.Although BoostedMERT improves over MERT,more work at both the theoretical and algorithmiclevels is needed to demonstrate even larger gains.For example, while standard boosting for classifica-tion can exponentially reduce training error in thenumber of iterations under mild assumptions, theseassumptions are frequently not satisfied in the algo-rithm we described.
We intend to further explorethe idea of boosting on N-best lists, drawing inspi-rations from the large body of work on boosting forclassification whenever possible.ReferencesM.
Collins and T. Koo.
2005.
Discriminative rerankingfor natural langauge parsing.
Computational Linguis-tics, 31(1).Y.
Freund, R. Iyer, R.E.
Schapire, and Y.
Singer.
2003.An efficient boosting algorithm for combining prefer-ences.
Journal of Machine Learning Research, 4.K.
Kirchhoff and M. Yang.
2007.
The UW machinetranslation system for IWSLT 2007.
In IWSLT.T.
Kudo, J. Suzuki, and H. Isozaki.
2005.
Boosting-based parse reranking with subtree features.
In ACL.L.
Mason, J. Baxter, P. Bartless, and M. Frean.
2000.Boosting as gradient descent.
In NIPS.F.J.
Och et al 2004.
A smorgasbord of features for sta-tistical machine translation.
In HLT/NAACL.F.J.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In ACL.R.
E. Schapire and Y.
Singer.
1999.
Improved boostingalgorithms using confidence-rated predictions.
Ma-chine Learning, 37(3).L.
Shen, A. Sarkar, and F.J. Och.
2004.
Discriminativereranking for machine translation.
In HLT-NAACL.D.
Smith and J. Eisner.
2006.
Minimum risk anneal-ing for training log-linear models.
In Proc.
of COL-ING/ACL Companion Volume.A.
Venugopal, A. Zollmann, and A. Waibel.
2005.
Train-ing and evaluating error minimization rules for SMT.In ACL Workshop on Building/Using Parallel Texts.J.
Xu and H. Li.
2007.
AdaRank: A boosting algorithmfor information retrieval.
In SIGIR.40
