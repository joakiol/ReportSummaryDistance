Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 657?664Manchester, August 2008A Discriminative Alignment Model for Abbreviation RecognitionNaoaki Okazaki?okazaki@is.s.u-tokyo.ac.jpSophia Ananiadou?sophia.ananiadou@manchester.ac.ukJun?ichi Tsujii?
?tsujii@is.s.u-tokyo.ac.jp?Graduate School of InformationScience and TechnologyUniversity of Tokyo7-3-1 Hongo, Bunkyo-kuTokyo 113-8656, Japan?School of Computer Science,University of ManchesterNational Centre for Text Mining (NaCTeM)Manchester Interdisciplinary Biocentre131 Princess Street, Manchester M1 7DN, UKAbstractThis paper presents a discriminative align-ment model for extracting abbreviationsand their full forms appearing in actualtext.
The task of abbreviation recognitionis formalized as a sequential alignmentproblem, which finds the optimal align-ment (origins of abbreviation letters) be-tween two strings (abbreviation and fullform).
We design a large amount of fine-grained features that directly express theevents where letters produce or do not pro-duce abbreviations.
We obtain the optimalcombination of features on an aligned ab-breviation corpus by using the maximumentropy framework.
The experimental re-sults show the usefulness of the alignmentmodel and corpus for improving abbrevia-tion recognition.1 IntroductionAbbreviations present two major challenges in nat-ural language processing: term variation and am-biguity.
Abbreviations substitute for expandedterms (e.g., dynamic programming) through theuse of shortened term-forms (e.g., DP).
At thesame time, the abbreviation DP appearing alone intext is ambiguous, in that it may refer to differentconcepts, e.g., data processing, dirichlet process,differential probability.
Associating abbreviationsand their full forms is useful for various applica-tions including named entity recognition, informa-tion retrieval, and question answering.c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.The task of abbreviation recognition, in whichabbreviations and their expanded forms appearingin actual text are extracted, addresses the term vari-ation problem caused by the increase in the num-ber of abbreviations (Chang and Sch?utze, 2006).Furthermore, abbreviation recognition is also cru-cial for disambiguating abbreviations (Pakhomov,2002; Gaudan et al, 2005; Yu et al, 2006), pro-viding sense inventories (lists of abbreviation def-initions), training corpora (context information offull forms), and local definitions of abbreviations.Hence, abbreviation recognition plays a key role inabbreviation management.Numerous researchers have proposed a varietyof heuristics for recognizing abbreviation defini-tions, e.g., the use of initials, capitalizations, syl-lable boundaries, stop words, lengths of abbrevia-tions, and co-occurrence statistics (Park and Byrd,2001; Wren and Garner, 2002; Liu and Fried-man, 2003; Okazaki and Ananiadou, 2006; Zhouet al, 2006; Jain et al, 2007).
Schwartz andHearst (2003) implemented a simple algorithm thatfinds the shortest expression containing all alpha-numerical letters of an abbreviation.
Adar (2004)presented four scoring rules to choose the mostlikely expanded form in multiple candidates.
Aoand Takagi (2005) designed more detailed condi-tions for accepting or discarding candidates of ab-breviation definitions.However, these studies have limitations in dis-covering an optimal combination of heuristic rulesfrom manual observations of a corpus.
For exam-ple, when expressions transcrip:tion factor 1 andthyroid transcription factor 1 are full-form can-didates for the abbreviation TTF-11, an algorithmshould choose the latter expression over the shorter1In this paper, we use straight and::::wavy underlines to rep-resent correct and incorrect origins of abbreviation letters.657expression (former).
Previous studies hardly han-dle abbreviation definitions where full forms (e.g.,water activity) shuffle their abbreviation letters(e.g., AW).
It is also difficult to reject ?negative?definitions in a text; for example, an algorithmshould not extract an abbreviation definition fromthe text, ?the replicon encodes a large:::replic:ationprotein (RepA),?
since RepA provides a descrip-tion of the protein rather than an abbreviation.In order to acquire the optimal rules fromthe corpora, several researchers applied machinelearning methods.
Chang and Sch?utze (2006) ap-plied logistic regression to combine nine features.Nadeau and Turney (2005) also designed seven-teen features to classify candidates of abbrevia-tion definitions into positive or negative instancesby using the Support Vector Machine (SVM).Notwithstanding, contrary to our expectations, themachine-learning approach could not report betterresults than those with hand-crafted rules.We identify the major problem in the previ-ous machine-learning approach: these studies didnot model associations between abbreviation let-ters and their origins, but focused only on indirectfeatures such as the number of abbreviation lettersthat appear at the head of a full form.
This wasprobably because the training corpus did not in-clude annotations on the exact origins of abbrevia-tion letters but only pairs of abbreviations and fullforms.
It was thus difficult to design effective fea-tures for abbreviation recognition and to reuse theknowledge obtained from the training processes.In this paper, we formalize the task of abbrevi-ation recognition as a sequential alignment prob-lem, which finds the optimal alignment (origins ofabbreviation letters) between two strings (abbrevi-ation and full form).
We design a large amountof features that directly express the events whereletters produce or do not produce abbreviations.Preparing an aligned abbreviation corpus, we ob-tain the optimal combination of the features by us-ing the maximum entropy framework (Berger etal., 1996).
We report the remarkable improve-ments and conclude this paper.2 Proposed method2.1 Abbreviation alignment modelWe express a sentence x as a sequence of letters(x1, ..., xL), and an abbreviation candidate y in thesentence as a sequence of letters (y1, ..., yM).
Wedefine a letter mapping a = (i, j) to indicate thatthe abbreviation letter yjis produced by the letterin the full form xi.
A null mapping a = (i, 0) indi-cates that the letter in the sentence xiis unused toform the abbreviation.
Similarly, a null mappinga = (0, j) indicates that the abbreviation letter yjdoes not originate from any letter in x.
We de-fine a(x)and a(y)in order to represent the first andsecond elements of the letter mapping a.
In otherwords, a(x)and a(y)are equal to i and j respec-tively, when a = (i, j).
Finally, an abbreviationalignment a is defined as a sequence of letter map-pings, a = (a1, ..., aT), where T represents thenumber of mappings in the alignment.Let us consider the following example sentence:We investigate the effect of thyroid tran-scription factor 1 (TTF-1).This sentence contains an abbreviation candidateTTF-1 in parentheses2.
Figure 1 illustrates thecorrect alignment a (bottom line) and its two-dimensional representation for the example sen-tence3; the abbreviation letters ?t,?
?t,?
?f,?
?-,?
and?1?
originate from x30, x38, x52, nowhere (nullmapping), and x59respectively.We directly model the conditional probability ofthe alignment a, given x and y, using the maxi-mum entropy framework (Berger et al, 1996),P (a|x,y) =exp {?
?
F (a,x,y)}?a?C(x,y)exp {?
?
F (a,x,y)}.
(1)In Formula 1, F = {f1, ..., fK} is a global featurevector whose elements present K feature func-tions, ?
= {?1, ..., ?K} denotes a weight vectorfor the feature functions, and C(x,y) yields a setof possible alignments for the given x and y. Weobtain the following decision rule to choose themost probable alignment a?
for given x and y,a?
= argmaxa?C(x,y)P (a|x,y).
(2)Note that a set of possible alignments C(x,y)always includes a negative alignment whose ele-ments are filled with null-mappings (refer to Sec-tion 2.3 for further detail).
This allows the formulato withdraw the abbreviation candidate y when anyexpression in x is unlikely to be a definition.2Refer to Section 3.1 for text makers for abbreviations.3We ignore non-alphabetical letters in abbreviations.658We investigate the effect of thyroid transcription factor 1 (TTF-1) ...x:a~   ~  ~    ~~  ~  ~ ~       ~        ~    ~  ~   ~ ~8 words 4 lettersmin(|y|+5, 2|y|)<NIL> T  T  F  -   1  <SF>y:0 0 0 0 0 0 0 0 0123591 13 16 2122 25 28 30 38 47 52 55 59 6110123456t =ija =a2 3 4 5 6 7 8 9 10 11 12 13 14Null outsideOther positions Abbreviation Null inside Associate insidea = ((9,0), (13,0), (16,0), (21,0), (22, 0), (25,0), (28,0), (30,1), (38,2), (47,0), (52,3), (55,0), (59,5), (61,6))Figure 1: The correct alignment for the example sentence and its two dimensional representation.2.2 FeaturesThe main advantage of the discriminative align-ment model is its ability to incorporate a widerange of non-independent features.
Inspiredby feature engineering for Conditional RandomFields (CRFs) (Lafferty et al, 2001), we designtwo kinds of features: unigram (state) features de-fined on each letter mapping, and bigram (tran-sition) features defined on each pair of adjacentletter mappings.
Given a triplet, a, x, and y, aglobal feature function fk(a,x,y) ?
F sums upthe boolean values (0 or 1) of the correspondinglocal feature gk(a,x,y, t) at t ?
{1, ..., T},fk(a,x,y) =T?t=1gk(a,x,y, t).
(3)In other words, fk(a,x,y) counts the number oftimes the local feature is fired in the alignment a.A unigram feature corresponds to the observa-tion at xiand yjassociated by a mapping at=(i, j).
A unigram feature encodes the conditionwhere the letter in the full form xiis chosen orunchosen for producing the abbreviation letter yj.For example, we may infer from the letter mappingat a8= (30, 1) in Figure 1, that x30is mapped toy1because: x30is at the head of the word, y1is acapital letter, and both x30and y1are at the headof the word and abbreviation.Bigram features, combining two observations atasand at(1 ?
s < t ?
T ), are useful in capturingthe common characteristics shared by an abbrevi-ation definition.
For instance, we may presume inFigure 1 that the head letters in the full form mightbe selectively used for producing the abbreviation,based on the observations at a8= (30, 1) anda9= (38, 2).
In order to focus on the conditionsfor consecutive non-null mappings, we choose theprevious position s for the given t.s =???t?
1(at(y)= 0 ?
?u : au(y)= 0)max1?u<t{u | au(y)6= 0}(otherwise)(4)Formula 4 prefers the non-null mapping that is themost adjacent to t over the previous mapping (t ?1).
In Figure 1, transitions a9?a11and a11?a13exist for this reason.In this study, we express unigram and bi-gram features with atomic functions (Table 1)that encode observation events of xat(x), yat(y),at, xas(x)?xat(x), and yas(y)?yat(y).
Atomicfunctions x ctype, y ctype, x position, andy position present common heuristics used byprevious studies.
The function x word examinesthe existence of stop words (e.g., the, of, in) toprevent them from producing abbreviation letters.We also include x pos (part-of-speech of the word)since a number of full forms are noun phrases.Functions x diff , x diff wd, and y diff are de-signed specifically for bigram features, receivingtwo positions s and t in their arguments.
Thefunction x diff mainly deals with abbreviation def-initions that include consecutive letters of theirfull forms, e.g., amplifier (AMP).
The function659Function Return valuex ctype?
(a,x, t) xat(x)+?is {U (uppercase), L (lowercase), D (digit), W (whitespace), S (symbol) } letterx position?
(a,x, t) xat(x)+?is at the {H (head), T (tail), S (syllable head), I (inner), W (whitespace) } of the wordx char?
(a,x, t) The lower-cased letter of xat(x)+?x word?
(a,x, t) The lower-cased word (offset position ?)
containing the letter xat(x)x pos?
(a,x, t) The part-of-speech code of the word (offset position ?)
containing the letter xat(x)y ctype(a,y, t) yat(y)is {N (NIL) U (uppercase), L (lowercase), D (digit), S (symbol) } lettery position(a,y, t) yat(y)is at the {N (NIL) H (head), T (tail), I (inner)} of the wordy char(a,y, t) The lower-cased letter of yat(y)a state(a,y, t) {SKIP (at(y)= 0),MATCH (1 ?
at(y)?
|y|),ABBR (at(y)= |y|+ 1)}x diff(a,x, s, t) (at(x)?
as(x)) if letters xat(x)and xas(x)are in the same word, NONE otherwisex diff wd(a,x, s, t) The number of words between xat(x)and xas(x)y diff(a,y, s, t) (at(y)?
as(y))Table 1: Atomic functions to encode observation events in x and yCombination Rulesunigram(t) xy unigram(t)?
{a state(t)}xy unigram(t) x unigram(t)?
y unigram(t)?
(x unigram(t)?
y unigram(t))x unigram(t) x state0(t)?
x state?1(t)?
x state1(t)?
(x state?1(t)?
x state0(t))?
(x state0(t)?
x state1(t))y unigram(t){y ctype(t), y position(t), y ctype(t)y position(t)}x state?
(t){x ctype?
(t), x position?
(t), x char?
(t), x word?
(t), x pos?
(t), x ctype?
(t)x position?
(t),x position?
(t)x pos?
(t), x pos?
(t)x ctype?
(t), x ctype?
(t)x position?
(t)x pos?
(t)}bigram(s, t) xy bigram(s, t)?
{a state(s)a state(t)}xy bigram(s, t)(x state0(s)?
x state0(t)?
trans(s, t))?
(y unigram(s)?
y unigram(t)?
trans(s, t))?
(x state0(s)?
y unigram(s)?
x state0(t)?
y unigram(t)?
trans(s, t))trans(s, t){x diff(s, t), x diff wd(s, t), y diff(s, t)}Table 2: Generation rules for unigram and bigram features.x diff wd measures the distance of two words.The function y diff models the ordering of abbre-viation letters; this function always returns non-negative values if the abbreviation contains lettersin the same order as in its full form.We express unigram and bigram features withthe atomic functions.
For example, Formula 5 de-fines a unigram feature for the event where the cap-ital letter in a full-form word xat(x)produces theidentical abbreviation letter yat(y).gk(a,x,y, t) =??????????
?1 x ctype0(a,x, t) = U?
y ctype(a,y, t) = U?
a state(a,y, t) = MATCH0 (otherwise)(5)For notation simplicity, we rewrite this booleanfunction as (arguments a, x, and y are omitted),1{x ctype0(t)y ctype(t)a state(t)=U;U;MATCH}.
(6)In this formula, 1{v=v?
}is an indicator function thatequals 1 when v = v?
and 0 otherwise.
The termv presents a generation rule for a feature, i.e., acombination rule of atomic functions.Table 2 displays the complete list of gener-ation rules for unigram and bigram features4,unigram(t) and bigram(s, t).
For each genera-tion rule in unigram(t) and bigram(s, t), we de-fine boolean functions that test the possible valuesyielded by the corresponding atomic function(s).2.3 Alignment candidatesFormula 1 requires a sum over the possible align-ments, which amounts to 2LMfor a sentence (Lletters) with an abbreviation (M letters).
It isunrealistic to compute the partition factor of theformula directly; therefore, the factor has beencomputed by dynamic programing (McCallum etal., 2005; Blunsom and Cohn, 2006; Shimbo andHara, 2007) or approximated by the n-best list ofhighly probable alignments (Och and Ney, 2002;Liu et al, 2005).
Fortunately, we can prune align-ments that are unlikely to present full forms, by in-troducing the natural assumptions for abbreviationdefinitions:4In Table 2, a set of curly brackets {} denotes a list (array)rather than a mathematical set.
Operators ?
and ?
presentconcatenation and Cartesian product of lists.
For instance,when A = {a, b} and B = {c, d}, A?B = {a, b, c, d} andA?B = {ac, ad, bc, bd}.660investigate the effect of thyroid transcription factor 10   0  0    00  0  0 0       0        0    0  0   00   0  0    00  0  0 0       1        2    3  0   50   0  0    00  0  0 1       2        0    3  0   50   0  0    00  0  0 1       0        2    3  0   50   0  0    00  0  0 2       1        0    3  0   50   0  0    00  0  0 2       0        1    3  0   50   0  0    00  0  3 0       0        1    0  2   50   0  0    00  0  3 0       1        2    0  0   50   0  0    00  0  3 0       1        0    0  2   5.   .
.
..  .
.
.
.
.
.
.
..   .
.
..  .
.
.
.
.
.
.
..   .
.
..  .
.
.
.
.
.
.
.x: ~   ~  ~    ~~  ~  ~ ~       ~        ~    ~  ~   ~min(|y|+5, 2|y|) = 8 words, (|y| = 4; y = "TTF-1")94 13 16 2122 25 28 30 38 47 52 55 59ia =ShffleShffleShffleShffleShffle#0#1#2#3#4#5#6#7#8Figure 2: A part of the possible alignments for theabbreviation TTF-1 in the example sentence.1.
A full form may appear min(m + 5, 2m)words before its abbreviation in the same sen-tence, where m is the number of alphanu-meric letters in the abbreviation (Park andByrd, 2001).2.
Every alphanumeric letter in an abbreviationmust be associated with the identical (case-insensitive) letter in its full form.3.
An abbreviation letter must not originate frommultiple letters in its full form; a full-form let-ter must not produce multiple letters.4.
Words in a full form may be shuffled at mostd times, so that all alphanumeric letters in thecorresponding abbreviation appear in the re-arranged full form in the same order.
We de-fine a shuffle operation as removing a seriesof word(s) from a full form, and inserting theremoved word(s) to another position.5.
A full form does not necessarily exist in thetext span defined by assumption 1.Due to the space limitation, we do not describethe algorithm for obtaining possible alignmentsthat are compatible with these assumptions.
Al-ternatively, Figure 2 illustrates a part of possiblealignments C(x,y) for the example sentence.
Thealignment #2 represents the correct definition forthe abbreviation TTF-1.
We always include thenegative alignment (e.g., #0) where no abbrevia-tion letters are associated with any letters in x.The alignments #4?8 interpret the generationprocess of the abbreviation by shuffling the wordsin x.
For example, the alignment #6 moves theword ?of?
to the position between ?factor?
and?1?.
Shuffled alignments cover abbreviation defini-tions such as receptor of estrogen (ER) and wateractivity (AW).
We call the parameter d, distortionparameter, which controls the acceptable level ofreordering (distortion) for the abbreviation letters.2.4 Parameter estimationParameter estimation for the abbreviationalignment model is essentially the same asfor general maximum entropy models.
Givena training set that consists of N instances,((a(1),x(1),y(1)), ..., (a(N),x(N),y(N))), wemaximize the log-likelihood of the conditionalprobability distribution by using the maximuma posterior (MAP) estimation.
In order to avoidoverfitting, we regularize the log-likelihood witheither the L1or L2norm of the weight vector ?,L1=N?n=1logP (a(n)|x(n),y(n))?||?||1?1, (7)L2=N?n=1logP (a(n)|x(n),y(n))?||?||222?22.
(8)In these formulas, ?1and ?2are regularization pa-rameters for the L1and L2norms.
Formulas 7and 8 are maximized by the Orthant-Wise Limited-memory Quasi-Newton (OW-LQN) method (An-drew and Gao, 2007) and the Limited-memoryBFGS (L-BFGS) method (Nocedal, 1980)5.3 Experiments3.1 Aligned abbreviation corpusThe Medstract Gold Standard Corpus (Pustejovskyet al, 2002) was widely used for evaluating abbre-viation recognition methods (Schwartz and Hearst,2003; Adar, 2004).
However, we cannot usethis corpus for training the abbreviation alignmentmodel, since it lacks annotations on the origins ofabbreviation letters.
In addition, the size of thecorpus is insufficient for a supervised machine-learning method.Therefore, we built our training corpus with1,000 scientific abstracts that were randomly cho-sen from the MEDLINE database.
Although thealignment model is independent of linguistic pat-terns for abbreviation definitions, in the corpus wefound only three abbreviation definitions that weredescribed without parentheses.
Hence, we em-ployed parenthetical expressions, full-form ?(?
ab-breviation ?
)?, to locate possible abbreviation def-initions (Wren and Garner, 2002).
In order to ex-clude parentheses inserting clauses into passages,5We used Classias for parameter estimation:http://www.chokkan.org/software/classias/661we consider the inner expression of parentheses asan abbreviation candidate, only if the expressionconsists of two words at most, the length of the ex-pression is between two to ten characters, the ex-pression contains at least an alphabetic letter, andthe first character is alphanumeric.We asked a human annotator to assign refer-ence abbreviation alignments for 1,420 parentheti-cal expressions (instances) in the corpus.
If a par-enthetical expression did not introduce an abbre-viation, e.g., ?...
received treatment at 24 months(RRMS),?
the corresponding instance would havea negative alignment (as #0 in Figure 2).
Eventu-ally, our aligned corpus consisted of 864 (60.8%)abbreviation definitions (with positive alignments)and 556 (39.2%) other usages of parentheses (withnegative alignments).
Note that the log-likelihoodin Formula 7 or 8 increases only if the probabilisticmodel predicts the reference alignments, regard-less of whether they are positive or negative.3.2 Baseline systemsWe prepared five state-of-the-art systems of ab-breviation recognition as baselines: Schwartzand Hearst?s method (SH) (Schwartz and Hearst,2003), SaRAD (Adar, 2004), ALICE (Ao andTakagi, 2005), Chang and Sch?utze?s method(CS) (Chang and Sch?utze, 2006), and Nadeau andTurney?s method (NT) (Nadeau and Turney, 2005).We utilized the implementations available on theWeb for SH6, CS78, and ALICE9, and we repro-duced SaRAD and NT, based on their papers.Our implementation of NT consists of a classi-fier that discriminates between positive (true) andnegative (false) full forms, using all of the featurefunctions presented in the original paper.
Althoughthe original paper presented heuristics for gener-ating full-form candidates, we replaced the candi-date generator with the function C(x,y), so thatthe classifier and our alignment model can receivethe same set of full-form candidates.
The classi-fier of the NT system was modeled by the LIB-SVM implementation10with Radial Basis Func-6Abbreviation Definition Recognition Software:http://biotext.berkeley.edu/software.html7Biomedical Abbreviation Server:http://abbreviation.stanford.edu/8We applied a score cutoff of 0.14.9Abbreviation LIfter using Corpus-based Extraction:http://uvdb3.hgc.jp/ALICE/ALICE index.html10LIBSVM ?
A Library for Support Vector Machines:http://www.csie.ntu.edu.tw/?cjlin/libsvm/System P R F1Schwartz & Hearst (SH) .978 .940 .959SaRAD .891 .919 .905ALICE .961 .920 .940Chang & Sch?utze (CS) .942 .900 .921Nadeau & Turney (NT) .954 .871 .910Proposed (d = 0; L1) .973 .969 .971Proposed (d = 0; L2) .964 .968 .966Proposed (d = 1; L1) .960 .981 .971Proposed (d = 1; L2) .957 .976 .967Table 3: Performance on our corpus.tion (RBF) kernel11.
If multiple full-form can-didates for an abbreviation are classified as posi-tives, we choose the candidate that yields the high-est probability estimate.3.3 ResultsWe trained and evaluated the proposed method onour corpus by performing 10-fold cross valida-tion12.
Our corpus includes 13 out of 864 (1.5%)abbreviation definitions in which the abbreviationletters are shuffled.
Thus, we have examined twodifferent distortion parameters, d = 0, 1 in thisexperiment.
The average numbers of candidatesproduced by the candidate generator C(x,y) perinstance were 8.46 (d = 0) and 69.1 (d = 1), re-spectively.
The alignment model was trained in areasonable execution time13, ca.
5 minutes (d = 0)and 1.5 hours (d = 1).Table 3 reports the precision (P), recall (R), andF1 score (F1) on the basis of the number of cor-rect abbreviation definitions recognized by eachsystem.
The proposed method achieved the bestF1 score (0.971) of all systems.
The inclusion ofdistorted abbreviations (d = 1) gained the high-est recall (0.981 with L1regularization).
Base-line systems with refined heuristics (SaRAD andALICE) could not outperform the simplest sys-tem (SH).
The previous approaches with machinelearning (CS and NT) were roughly comparable torule-based methods.We also evaluated the alignment model on theMedstract Gold Standard development corpus toexamine the adaptability of the alignment modeltrained with our corpus (Table 4).
Since the origi-11We tuned kernel parameters C = 128 and ?
= 2.0 byusing the grid-search tool in the LIBSVM distribution.12We determined the regularization parameters as ?1= 3and ?2= 3 after testing {0.1, 0.2, 0.3, 0.5, 1, 2, 3, 5, 10} forthe regularization parameters.
The difference between thehighest and lowest F1 scores was 1.8%.13On Intel Dual-Core Xeon 5160/3GHz CPU, excludingtime for feature generation and data input/output.662System P R F1Schwartz & Hearst (SH) .942 .891 .916SaRAD .909 .859 .884ALICE .960 .945 .953Chang & Sch?utze (CS) .858 .852 .855Nadeau & Turney (NT) .889 .875 .882Proposed (d = 1; L1) .976 .945 .960Table 4: Performance on Medstract corpus.# Atomic function(s) F1(1) x position + x ctype .905(2) (1) + x char + y char .885(3) (1) + x word + x pos .941(4) (1) + x diff + x diff wd + y diff .959(5) (1) + y position + y ctype .964(6) All atomic functions .966Table 5: Effect of atomic functions (d = 0; L2).nal version of the Medstract corpus includes anno-tation errors, we used the version revised by Aoand Takagi (2005).
For this reason, the perfor-mance of ALICE might be over-estimated in thisevaluation; ALICE delivered much better resultsthan Schwartz & Hearst?s method on this corpus.The abbreviation alignment model trained withour corpus (d = 1; L1) outperformed the baselinesystems for all evaluation metrics.
It is notable thatthe model could recognize abbreviation definitionswith shuffled letters, e.g., transfer of single embryo(SET) and inorganic phosphate (PI), without anymanual tuning for this corpus.
In some false cases,the alignment model yielded incorrect probabilityestimates.
For example, the probabilities of thealignments prepubertal bipolarity, bi:polarity, andnon-definition (negative) for the abbreviation BPwere computed as 3.4%, 89.6%, and 6.7%, respec-tively; but the first expression prepubertal bipolar-ity is the correct definition for the abbreviation.Table 5 shows F1 scores of the proposed methodtrained with different sets of atomic functions.
Thebaseline setting (1), which built features only withx position and x ctype functions, gained a 0.905F1 score; further, adding more atomic functionsgenerally improves the score.
However, the x charand y char functions decreased the performancesince the alignment model was prone to overfit tothe training data, relying on the existence of spe-cific letters in the training instances.
Interestingly,the model was flexible enough to achieve a highperformance with four atomic functions (5).Table 6 demonstrates the ability for our ap-proach to obtain effective features; the table showsthe top 10 (out of 850,009) features with high# Feature ?1 U: x position0=H;y ctype0=U;y position0=H/M 1.73702 B: y position0=I/y position0=I/x diff=1/M-M 1.34703 U: x ctype?1=L;x ctype0=L/S 0.963424 B: x ctype0=L/x ctype0=L/x diff wd=0/M-M 0.940095 U: x position0=I;x char1=?t?/S 0.916456 U: x position0=H;x pos0=NN;y ctype0=U/M 0.867867 U: x ctype?1=S;xctype0=L;M 0.864748 B: x char0=?o?/x ctype0=L/y diff=0/M-S 0.712629 U: x char?1=?o?
;x ctype0=L/M 0.6976410 B: x position0=H/x ctype0=U/y diff=1/M-M 0.66418Table 6: Top ten features with high weights.weights assigned by the MAP estimation with L1regularization.
A unigram and bigram featureshave prefixes ?U:?
and ?B:?
respectively; a featureexpresses conditions at s (bigram features only),conditions at t, and mapping status (match or skip)separated by ?/?
symbols.
For example, the #1 fea-ture associates a letter at the head of a full-formword with the uppercase letter at the head of itsabbreviation.
The #4 feature is difficult to obtainfrom manual observations, i.e., the bigram featuresuggests the production of two abbreviation lettersfrom two lowercase letters in the same word.4 ConclusionWe have presented a novel approach for recogniz-ing abbreviation definitions.
The task of abbrevi-ation recognition was successfully formalized asa sequential alignment problem.
We developedan aligned abbreviation corpus, and obtained fine-grained features that express the events whereina full forum produces an abbreviation letter.
Theexperimental results showed remarkable improve-ments and usefulness of the alignment approachfor abbreviation recognition.
We expect the use-fullness of the discriminative model for buildingan comprehensible abbreviation dictionary.Future work would be to model cases in whicha full form yields non-identical letters (e.g., ?one??
?1?
and ?deficient?
?
?-?
), and to demonstratethis approach with more generic linguistic patterns(e.g., aka, abbreviated as, etc.).
We also plan toexplore a method for training a model with an un-aligned abbreviation corpus, estimating the align-ments simultaneously from the corpus.AcknowledgmentsThis work was partially supported by Grant-in-Aidfor Scientific Research on Priority Areas (MEXT,Japan), and Solution-Oriented Research for Sci-ence and Technology (JST, Japan).663ReferencesAdar, Eytan.
2004.
SaRAD: A simple and robustabbreviation dictionary.
Bioinformatics, 20(4):527?533.Andrew, Galen and Jianfeng Gao.
2007.
Scalable train-ing of L1-regularized log-linear models.
In Proceed-ings of the 24th International Conference on Ma-chine Learning (ICML 2007), pages 33?40.Ao, Hiroko and Toshihisa Takagi.
2005.
ALICE: Analgorithm to extract abbreviations from MEDLINE.Journal of the American Medical Informatics Asso-ciation, 12(5):576?586.Berger, Adam L., Vincent J. Della Pietra, and StephenA.
Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Computa-tional Linguistics, 22(1):39?71.Blunsom, Phil and Trevor Cohn.
2006.
Discrimina-tive word alignment with conditional random fields.In Proceedings of the 21st International Conferenceon Computational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics (Coling-ACL 2006), pages 65?72.Chang, Jeffrey T. and Hinrich Sch?utze.
2006.
Abbre-viations in biomedical text.
In Ananiadou, Sophiaand John McNaught, editors, Text Mining for Biol-ogy and Biomedicine, pages 99?119.
Artech House,Inc.Gaudan, Sylvain, Harald Kirsch, and Dietrich Rebholz-Schuhmann.
2005.
Resolving abbreviations to theirsenses in Medline.
Bioinformatics, 21(18):3658?3664.Jain, Alpa, Silviu Cucerzan, and Saliha Azzam.
2007.Acronym-expansion recognition and ranking on theweb.
In Proceedings of the IEEE International Con-ference on Information Reuse and Integration (IRI2007), pages 209?214.Lafferty, John, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Proba-bilistic models for segmenting and labeling sequencedata.
In Proceedings of the 18th International Con-ference on Machine Learning (ICML 2001), pages282?289.Liu, Hongfang and Carol Friedman.
2003.
Mining ter-minological knowledge in large biomedical corpora.In the 8th Pacific Symposium on Biocomputing (PSB2003), pages 415?426.Liu, Yang, Qun Liu, and Shouxun Lin.
2005.
Log-linear models for word alignment.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics (ACL 2005), pages 459?466.McCallum, Andrew, Kedar Bellare, and FernandoPereira.
2005.
A conditional random field fordiscriminatively-trained finite-state string edit dis-tance.
In Proceedings of the 21st Conference on Un-certainty in Artificial Intelligence (UAI 2005), pages388?395.Nadeau, David and Peter D. Turney.
2005.
A super-vised learning approach to acronym identification.In the 8th Canadian Conference on Artificial Intel-ligence (AI?2005) (LNAI 3501), page 10 pages.Nocedal, Jorge.
1980.
Updating quasi-newton matriceswith limited storage.
Mathematics of Computation,35(151):773?782.Och, Franz Josef and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proceedings of the 40thAnnual Meeting on Association for ComputationalLinguistics (ACL 2002), pages 295?302.Okazaki, Naoaki and Sophia Ananiadou.
2006.
Build-ing an abbreviation dictionary using a term recogni-tion approach.
Bioinformatics, 22(24):3089?3095.Pakhomov, Serguei.
2002.
Semi-supervised maximumentropy based approach to acronym and abbreviationnormalization in medical texts.
In Proceedings ofthe 40th Annual Meeting on Association for Compu-tational Linguistics (ACL 2002), pages 160?167.Park, Youngja and Roy J. Byrd.
2001.
Hybrid text min-ing for finding abbreviations and their definitions.
InProceedings of the 2001 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2001), pages 126?133.Pustejovsky, James, Jos?e Casta?no, Roser Saur?
?, AnnaRumshinsky, Jason Zhang, and Wei Luo.
2002.Medstract: creating large-scale information serversfor biomedical libraries.
In Proceedings of the ACL-02 workshop on Natural language processing in thebiomedical domain, pages 85?92.Schwartz, Ariel S. and Marti A. Hearst.
2003.
A simplealgorithm for identifying abbreviation definitions inbiomedical text.
In the 8th Pacific Symposium onBiocomputing (PSB 2003), pages 451?462.Shimbo, Masashi and Kazuo Hara.
2007.
A dis-criminative learning model for coordinate conjunc-tions.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL 2007), pages 610?619.Wren, Jonathan D. and Harold R. Garner.
2002.Heuristics for identification of acronym-definitionpatterns within text: towards an automated construc-tion of comprehensive acronym-definition dictionar-ies.
Methods of Information in Medicine, 41(5):426?434.Yu, Hong, Won Kim, Vasileios Hatzivassiloglou, andJohn Wilbur.
2006.
A large scale, corpus-basedapproach for automatically disambiguating biomedi-cal abbreviations.
ACM Transactions on InformationSystems (TOIS), 24(3):380?404.Zhou, Wei, Vetle I. Torvik, and Neil R. Smalheiser.2006.
ADAM: another database of abbreviations inMEDLINE.
Bioinformatics, 22(22):2813?2818.664
