First Joint Conference on Lexical and Computational Semantics (*SEM), pages 347?355,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsSemEval-2012 Task 1: English Lexical SimplificationLucia SpeciaDepartment of Computer ScienceUniversity of SheffieldL.Specia@sheffield.ac.ukSujay Kumar JauharResearch Group in Computational LinguisticsUniversity of WolverhamptonSujay.KumarJauhar@wlv.ac.ukRada MihalceaDepartment of Computer Science and EngineeringUniversity of North Texasrada@cs.unt.eduAbstractWe describe the English Lexical Simplifica-tion task at SemEval-2012.
This is the firsttime such a shared task has been organizedand its goal is to provide a framework for theevaluation of systems for lexical simplificationand foster research on context-aware lexicalsimplification approaches.
The task requiresthat annotators and systems rank a number ofalternative substitutes ?
all deemed adequate ?for a target word in context, according to how?simple?
these substitutes are.
The notion ofsimplicity is biased towards non-native speak-ers of English.
Out of nine participating sys-tems, the best scoring ones combine context-dependent and context-independent informa-tion, with the strongest individual contributiongiven by the frequency of the substitute re-gardless of its context.1 IntroductionLexical Simplification is a subtask of Text Simpli-fication (Siddharthan, 2006) concerned with replac-ing words or short phrases by simpler variants in acontext aware fashion (generally synonyms), whichcan be understood by a wider range of readers.
Itgenerally envisages a certain human target audiencethat may find it difficult or impossible to understandcomplex words or phrases, e.g., children, peoplewith poor literacy levels or cognitive disabilities, orsecond language learners.
It is similar in many re-spects to the task of Lexical Substitution (McCarthyand Navigli, 2007) in that it involves determiningadequate substitutes in context, but in this case onthe basis of a predefined criterion: simplicity.A common pipeline for a Lexical Simplificationsystem includes at least three major components: (i)complexity analysis: selection of words or phrasesin a text that are considered complex for the readerand/or task at hand; (ii) substitute lookup: searchfor adequate replacement words or phrases deemedcomplex in context, e.g., taking synonyms (withthe same sense) from a thesaurus or finding similarwords/phrases in a corpus using distributional simi-larity metrics; and (iii) context-based ranking: rank-ing of substitutes according to how simple they areto the reader/task at hand.As an example take the sentence: ?Hitler com-mitted terrible atrocities during the second WorldWar.?
The system would first identify complexwords, e.g.
atrocities, then search for substitutesthat might adequately replace it.
A thesaurus lookupwould yield the following synonyms: abomination,cruelty, enormity and violation, but enormity shouldbe dropped as it does not fit the context appropri-ately.
Finally, the system would determine the sim-plest of these substitutes, e.g., cruelty, and use itto replace the complex word, yielding the sentence:?Hitler committed terrible cruelties during the sec-ond World War.
?.Different from other subtasks of Text Simplifica-tion like Syntactic Simplification, which have beenrelatively well studied, Lexical Simplification hasreceived less attention.
Although a few recent at-tempts explicitly address dependency on context (deBelder et al, 2010; Yatskar et al, 2010; Biran et al,2011; Specia, 2010), most approaches are context-independent (Candido et al, 2009; Devlin and Tait,1998).
In addition, a general deeper understanding347of the problem is yet to be gained.
As a first attemptto address this problem in the shape of a shared task,the English Simplification task at SemEval-2012 fo-cuses on the third component, which we believe isthe core of the Lexical Simplification problem.The SemEval-2012 shared task on English Lexi-cal Simplification has been conceived with the fol-lowing main purposes: advancing the state-of-the-art Lexical Simplification approaches, and provid-ing a common framework for evaluation of LexicalSimplification systems for participants and other re-searchers interested in the field.
Another central mo-tive of such a shared task is to bring awareness to thegeneral vagueness associated with the notion of lex-ical simplicity.
Our hypothesis is that in addition tothe notion of a target application/reader, the notionof simplicity is highly context-dependent.
In otherwords, given the same list of substitutes for a giventarget word with the same sense, we expect differentorderings of these substitutes in different contexts.We hope that participation in this shared task willhelp discover some underlying traits of lexical sim-plicity and furthermore shed some light on how thismay be leveraged in future work.2 Task definitionGiven a short context, a target word in English,and several substitutes for the target word that aredeemed adequate for that context, the goal of theEnglish Simplification task at SemEval-2012 is torank these substitutes according to how ?simple?they are, allowing ties.
Simple words/phrases areloosely defined as those which can be understood bya wide range of people, including those with low lit-eracy levels or some cognitive disability, children,and non-native speakers of English.
In particular,the data provided as part of the task is annotated byfluent but non-native speakers of English.The task thus essentially involves comparingwords or phrases and determining their order ofcomplexity.
By ranking the candidates, as opposedto categorizing them into specific labels (simple,moderate, complex, etc.
), we avoid the need for afixed number of categories and for more subjectivejudgments.
Also ranking enables a more natural andintuitive way for humans (and systems) to performannotations by preventing them from treating eachindividual case in isolation, as opposed to relativeto each other.
However, the inherent subjectivityintroduced by ranking entails higher disagreementamong human annotators, and more complexity forsystems to tackle.3 Corpus compilationThe trial and test corpora were created from the cor-pus of SemEval-2007 shared task on Lexical Sub-stitution (McCarthy and Navigli, 2007).
This de-cision was motivated by the similarity between thetwo tasks.
Moreover the existing corpus provided anadequate solution given time and cost constraints forour corpus creation.
Given existing contexts with theoriginal target word replaced by a placeholder andthe lists of substitutes (including the target word),annotators (and systems) are required to rank substi-tutes in order of simplicity for each context.3.1 SemEval-2007 - LS corpusThe corpus from the shared task on Lexical Substi-tution (LS) at SemEval-2007 is a selection of sen-tences, or contexts, extracted from the English Inter-net Corpus of English (Sharoff, 2006).
It containssamples of English texts crawled from the web.This selection makes up the dataset of a total of2, 010 contexts which are divided into Trial and Testsets, consisting of 300 and 1710 contexts respec-tively.
It covers a total of 201 (mostly polysemous)target words, including nouns, verbs, adjectives andadverbs, and each of the target words is shown in10 different contexts.
Annotators had been asked tosuggest up to three different substitutes (words orshort phrases) for each of the target words withintheir contexts.
The substitutes were lemmatized un-less it was deemed that the lemmatization would al-ter the meaning of the substitute.
Annotators wereall native English speakers and each annotated theentire dataset.
Here is an example of a context forthe target word ?bright?
:<lexelt item="bright.a"><instance id="1"><context>During the siege, GeorgeRobertson had appointed Shuja-ul-Mulk,who was a <head>bright</head> boyonly 12 years old and the youngest surviv-ing son of Aman-ul-Mulk, as the ruler ofChitral.</context>348</instance> ... </lexelt>The gold-standard document contains each targetword along with a ranked list of its possible substi-tutes, e.g., for the context above, three annotatorssuggested ?intelligent?
and ?clever?
as substitutesfor ?bright?, while only one annotator came up with?smart?
:bright.a 1:: intelligent 3; clever 3; smart 1;3.2 SemEval-2012 Lexical SimplificationcorpusGiven the list of contexts and each respective listof substitutes we asked annotators to rank substi-tutes for each individual context in ascending orderof complexity.
Since the notion of textual simplic-ity varies from individual to individual, we carefullychose a group of annotators in an attempt to cap-ture as much of a common notion of simplicity aspossible.
For practical reasons, we selected annota-tors with high proficiency levels in English as sec-ond language learners - all with a university first de-gree in different subjects.The Trial dataset was annotated by four peoplewhile the Test dataset was annotated by five peo-ple.
In both cases each annotator tagged the com-plete dataset.Inter-annotator agreement was computed using anadaptation of the kappa index with pairwise rankcomparisons (Callison-Burch et al, 2011).
This isalso the primary evaluation metric for participatingsystems in the shared task, and it is covered in moredetail in Section 4.The inter-annotator agreement was computed foreach pair of annotators and averaged over all possi-ble pairs for a final agreement score.
On the Trialdataset, a kappa index of 0.386 was found, whilefor the Test dataset, a kappa index of 0.398 wasfound.
It may be noted that certain annotators dis-agreed considerably with all others.
For example,on the Test set, if annotations from one judge are re-moved, the average inter-annotator agreement risesto 0.443.
While these scores are apparently low, thehighly subjective nature of the annotation task mustbe taken into account.
According to the referencevalues for other tasks, this level of agreement is con-sidered ?moderate?
(Callison-Burch et al, 2011).It is interesting to note that higher inter-annotatoragreement scores were achieved between annota-tors with similar language and/or educational back-grounds.
The highest of any pairwise annotatoragreement (0.52) was achieved between annotatorsof identical language and educational background,as well as very similar levels of English proficiency.High agreement scores were also achieved betweenannotators with first languages belonging to thesame language family.Finally, it is also worth noticing that this agree-ment metric is highly sensitive to small differencesin annotation, thus leading to overly pessimisticscores.
A brief analysis reveals that annotators oftenagree on clusters of simplicity and the source of thedisagreement comes from the rankings within theseclusters.Finally, the gold-standard annotations for theTrial and Test datasets ?
against which systems areto be evaluated ?
were generated by averaging theannotations from all annotators.
This was donecontext by context where each substitution was at-tributed a score based upon the average of the rank-ings it was ascribed.
The substitutions were thensorted in ascending order of scores, i.e., lowest score(highest average ranking) first.
Tied scores weregrouped together to form a single rank.
For exam-ple, assume that for a certain context, four annota-tors provided rankings as given below, where multi-ple candidates between {} indicate ties:Annotator 1: {clear} {light} {bright} {lumi-nous} {well-lit}Annotator 2: {well-lit} {clear} {light}{bright} {luminous}Annotator 3: {clear} {bright} {light} {lumi-nous} {well-lit}Annotator 4: {bright} {well-lit} {luminous}{clear} {light}Thus the word ?clear?, having been ranked 1st,2nd, 1st and 4th by each of the annotators respec-tively is given an averaged ranking score of 2.
Sim-ilarly ?light?
= 3.25, ?bright?
= 2.5, ?luminous?
=4 and ?well-lit?
= 3.25.
Consequently the gold-standard ranking for this context is:Gold: {clear} {bright} {light, well-lit} {lumi-nous}3493.3 Context-dependencyAs mentioned in Section 1, one of our hypothe-ses was that the notion of simplicity is context-dependent.
In other words, that the ordering of sub-stitutes for different occurrences of a target wordwith a given sense is highly dependent on the con-texts in which such a target word appears.
In orderto verify this hypothesis quantitatively, we furtheranalyzed the gold-standard annotations of the Trialand Test datasets.
We assume that identical lists ofsubstitutes for different occurrences of a given tar-get word ensure that such a target word has the samesense in all these occurrences.
For every target word,we then generate all pairs of contexts containing theexact same initial list of substitutes and check theproportion of these contexts for which human an-notators ranked the substitutes differently.
We alsocheck for cases where only the top-ranked substituteis different.
The numbers obtained are shown in Ta-ble 1.Trial Test1) # context pairs 1350 76952) # 1) with same list 60 2423) # 2) with different rankings 24 1394) # 2) with different top substitute 19 38Table 1: Analysis on the context-dependency of the no-tion of simplicity.Although the proportion of pairs of contexts withthe same list of substitutes is very low (less than5%), it is likely that there are many other occur-rences of a target word with the same sense andslightly different lists of substitutes.
Further man-ual inspection is necessary to determine the actualnumbers.
Nevertheless, from the observed sampleit is possible to conclude that humans will, in fact,rank the same set of words (with the same sense)differently depending on the context (on an averagein 40-57% of the instances).4 Evaluation metricNo standard metric has yet been defined for eval-uating Lexical Simplification systems.
Evaluatingsuch systems is a challenging problem due to theaforementioned subjectivity of the task.
Since thisis a ranking task, rank correlation metrics are desir-able.
However, metrics such as Spearman?s RankCorrelation are not reliable on the limited number ofdata points available for comparison on each rank-ing (note that the nature of the problem enforces acontext-by-context ranking, as opposed to a globalscore), Other metrics for localized, pairwise rankcorrelation, such as Kendall?s Tau, disregard ties, ?which are important for our purposes ?
and are thusnot suitable.The main evaluation metric proposed for thisshared task is in fact a measure of inter-annotatoragreement, which is used for both contrasting twohuman annotators (Section 3.2) and contrasting asystem output to the average of human annotationsthat together forms the gold-standard.Out metric is based on the kappa index (Cohen,1960) which in spite of many criticisms is widelyused for its simplicity and adaptability for differentapplications.
The generalized form of the kappa in-dex is?
=P (A)?
P (E)1?
P (E)where P (A) denotes the proportion of times twoannotators agree and P (E) gives the probability ofagreement by chance between them.In order to apply the kappa index for a rankingtask, we follow the method proposed by (Callison-Burch et al, 2011) for measuring agreement overjudgments of translation quality.
This method de-fines P (A) and P (E) in such a way that it nowcounts agreement whenever annotators concur uponthe order of pairwise ranks.
Thus, if one annotatorranked two given words 1 and 3, and the second an-notator ranked them 3 and 7 respectively, they arestill in agreement.
Formally, assume that two anno-tators A1 and A2 rank two instance a and b. ThenP (A) = the proportion of times A1 and A2 agreeon a ranking, where an occurrence of agreement iscounted whenever rank(a < b) or rank(a = b) orrank(a > b).P (E) (the likelihood that annotators A1 and A2agree by chance) is based upon the probability thatboth of them assign the same ranking order to a andb.
Given that the probability of getting rank(a <b) by any annotator is P (a < b), the probabilitythat both annotators get rank(a < b) is P (a < b)2(agreement is achieved when A1 assigns a < b bychance and A2 also assigns a < b).
Similarly, the350probability of chance agreement for rank(a = b)and rank(a > b) are P (a = b)2 and P (a > b)2respectively.
Thus:P (E) = P (a < b)2 + P (a = b)2 + P (a > b)2However, the counts of rank(a < b) andrank(a > b) are inextricably linked, since for anyparticular case of a1 < b1, it follows that b1 >a1, and thus the two counts must be incrementedequally.
Therefore, over the entire space of rankedpairs, the probabilities remain exactly the same.
Inessence, after counting for P (a = b), the remainingprobability mass is equally split between P (a < b)and P (a > b).
Therefore:P (a < b) = P (a > b) =1?
P (a = b)2Kappa is calculated for every pair of ranked itemsfor a given context, and then averaged to get an over-all kappa score:?
=|N |?n=1Pn(A)?
Pn(E)1?
Pn(E)|N |where N is the total number of contexts, and Pn(A)and Pn(E) are calculated based on counts extractedfrom the data on the particular context n.The functioning of this evaluation metric is illus-trated by the following example:Context: During the siege, George Robert-son had appointed Shuja-ul-Mulk, who was a_____ boy only 12 years old and the youngestsurviving son of Aman-ul-Mulk, as the rulerof Chitral.Gold: {intelligent} {clever} {smart} {bright}System: {intelligent} {bright} {clever,smart}Out of the 6 distinct unordered pairs of lexicalitems, system and gold agreed 3 times.
Conse-quently, Pn(A) = 36 .
In addition, count(a =b) = 1.
Thus, Pn(a = b) = 112 .
Which gives aP (E) = 4196 and the final kappa score for this partic-ular context of 0.13.The statistical significance of the results from twosystems A and B is measured using the methodof Approximate Randomization, which has beenshown to be a robust approach for several NLP tasks(Noreen, 1989).
The randomization is run 1, 000times and if the p-value is ?
0.05 the difference be-tween systems A and B is asserted as being statisti-cally significance.5 BaselinesWe defined three baseline lexical simplification sys-tems for this task, as follows.L-Sub Gold: This baseline uses the gold-standardannotations from the Lexical Substitution cor-pus of SemEval-2007 as is.
In other words, theranking is based on the goodness of fit of sub-stitutes for a context, as judged by human anno-tators.
This method also serves to show that theLexical Substitution and Lexical Simplificationtasks are indeed different.Random: This baseline provides a randomized or-der of the substitutes for every context.
Theprocess of randomization is such that is allowsthe occurrence of ties.Simple Freq.
: This simple frequency baseline usesthe frequency of the substitutes as extractedfrom the Google Web 1T Corpus (Brants andFranz, 2006) to rank candidate substituteswithin each context.The results in Table 2 show that the ?L-Sub Gold?and ?Random?
baselines perform very poorly onboth Trial and Test sets.
In particular, the reason forthe poor scores for ?L-Sub Gold?
can be attributedto the fact that it yields many ties, whereas the gold-standard presents almost no ties.
Our kappa met-ric tends to penalize system outputs with too manyties, since the probability of agreement by chance isprimarily computed on the basis of the number ofties present in the two rankings being compared (seeSection 4).The ?Simple Freq.?
baseline, on the other hand,performs very strongly, in spite of its simplistic ap-proach, which is entirely agnostic to context.
In factit surpasses the average inter-annotator agreementon both Trial and Test datasets.
Indeed, the scores onthe Test set approach the best inter-annotator agree-ment scores between any two annotators.351Trial TestL-Sub Gold 0.050 0.106Random 0.016 0.012Simple Freq.
0.397 0.471Table 2: Baseline kappa scores on trial and test sets6 Results and Discussion6.1 ParticipantsFive sites submitted one or more systems to the task,totaling nine systems:ANNLOR-lmbing: This system (Ligozat et al,2012) relies on language models probabili-ties, and builds on the principle of the Sim-ple Frequency baseline.
While the baselineuses Google n-grams to rank substitutes, thisapproach uses Microsoft Web n-grams in thesame way.
Additionally characteristics, suchas the contexts of each term to be substituted,were integrated into the system.
Microsoft WebN-gram Service was used to obtain log likeli-hood probabilities for text units, composed ofthe lexical item and 4 words to the left and rightfrom the surrounding context.ANNLOR-simple: The system (Ligozat et al,2012) is based on Simple English Wikipediafrequencies, with the motivation that the lan-guage used in this version of Wikipedia istargeted towards people who are not first-language English speakers.
Word n-grams (n =1-3) and their frequencies were extracted fromthis corpus using the Text-NSP Perl moduleand a ranking of the possible substitutes of atarget word according to these frequencies indescending order was produced.EMNLPCPH-ORD1: The system performs a se-ries of pairwise comparisons between candi-dates.
A binary classifier is learned purposeusing the Trial dataset and artificial unlabeleddata extracted based on Wordnet and a corpusin a semi-supervised fashion.
A co-trainingprocedure that lets each classifier increase theother classifier?s training set with selected in-stances from the unlabeled dataset is used.
Thefeatures include word and character n-gramprobabilities of candidates and contexts usingweb corpora, distributional differences of can-didate in a corpus of ?easy?
sentences and acorpus of normal sentences, syntactic complex-ity of documents that are similar to the givencontext, candidate length, and letter-wise rec-ognizability of candidate as measured by a tri-gram LM.
The first feature sets for co-trainingcombines the syntactic complexity, charactertrigram LM and basic word length features, re-sulting in 29 features against the remaining 21.EMNLPCPH-ORD2: This is a variant of theEMNLPCPH-ORD1 system where the first fea-ture set pools all syntactic complexity fea-tures and Wikipedia-based features (28 fea-tures) against all the remaining 22 features inthe second group.SB-mmSystem: The approach (Amoia and Ro-manelli, 2012) builds on the baseline defini-tion of simplicity using word frequencies butattempt at defining a more linguistically mo-tivated notion of simplicity based on lexicalsemantics considerations.
It adopts differentstrategies depending on the syntactic complex-ity of the substitute.
For one-word substitutesor common collocations, the system uses itsfrequency from Wordnet as a metric.
In thecase of multi-words substitutes the system uses?relevance?
rules that apply (de)compositionalsemantic criteria and attempts to identify aunique content word in the substitute that mightbetter approximate the whole expression.
Theexpression is then assigned the frequency asso-ciated to this content word for the ranking.
Af-ter POS tagging and sense disambiguating allsubstitutes, hand-written rules are used to de-compose the meaning of a complex phrase andidentify the most relevant word conveying thesemantics of the whole.UNT-SimpRank: The system (Sinha, 2012) usesexternal resources, including the Simple En-glish Wikipedia corpus, a set of Spoken En-glish dialogues, transcribed into machine read-able form, WordNet, and unigram frequencies(Google Web1T data).
SimpRank scores eachsubstitute by a sum of its unigram frequency, its352frequency in the Simple English Wikipedia, itsfrequency in the spoken corpus, the inverse ofits length, and the number of senses the sub-stitute has in WordNet.
For a given context,the substitutes are then reverse-ranked based ontheir simplicity scores.UNT-SimpRankLight: This is a variant of Sim-pRank which does not use unigram frequen-cies.
The goal of this system is to checkwhether a memory and time-intensive and non-free resource such as the Web1T corpus makesa difference over other free and lightweight re-sources.UNT-SaLSA: The only resource SaLSA dependson is the Web1T data, and in particular only3-grams from this corpus.
It leverages the con-text provided with the dataset by replacing thetarget placeholder one by one with each of thesubstitutes and their inflections thus buildingsets of 3-grams for each substitute in a giveninstance.
The score of any substitute is then thesum of the 3-gram frequencies of all the gener-ated 3-grams for that substitute.UOW-SHEF-SimpLex: The system (Jauhar andSpecia, 2012) uses a linear weighted rankingfunction composed of three features to pro-duce a ranking.
These include a context sen-sitive n-gram frequency model, a bag-of-wordsmodel and a feature composed of simplicityoriented psycholinguistic features.
These threefeatures are combined using an SVM rankerthat is trained and tuned on the Trial dataset.6.2 Pairwise kappaThe official task results and the ranking of the sys-tems are shown in Table 3.Firstly, it is worthwhile to note that all the topranking systems include features that use frequencyas a surrogate measure for lexical simplicity.
Thisindicates a very high correlation between distribu-tional frequency of a given word and its perceivedcomplexity level.
Additionally, the top two systemsinvolve context-dependent and context-independentfeatures, thus supporting our hypothesis of the com-posite nature of the lexical simplification problem.Rank Team - System Kappa1 UOW-SHEF-SimpLex 0.4962UNT-SimpRank 0.471Baseline-Simple Freq.
0.471ANNLOR-simple 0.4653 UNT-SimpRankL 0.4494 EMNLPCPH-ORD1 0.4055 EMNLPCPH-ORD2 0.3936 SB-mmSystem 0.2897 ANNLOR-lmbing 0.1998 Baseline-L-Sub Gold 0.1069 Baseline-Random 0.01310 UNT-SaLSA -0.082Table 3: Official results and ranking according to the pair-wise kappa metric.
Systems are ranked together when thedifference in their kappa score is not statistically signifi-cant.Few of the systems opted to use some form ofsupervised learning for the task, due to the limitednumber of training examples given.
As pointed outby some participants who checked learning curvesfor their systems, the performance is likely to im-prove with larger training sets.
Without enoughtraining data, context agnostic approaches such asthe ?Simple Freq.?
baseline become very hard tobeat.We speculate that the reason why the effects ofcontext-aware approaches are somewhat mitigated isbecause of the isolated setup of the shared task.
Inpractice, humans produce language at an even levelof complexity, i.e.
consistently simple, or consis-tently complex.
In the shared task?s setup, systemsare expected to simplify a single target word in acontext, ignoring the possibility that sometimes sim-ple words may not be contextually associated withcomplex surrounding words.
This not only explainswhy context-aware approaches are less successfulthan was originally expected, but also gives a reasonfor the good performance of context-agnostic sys-tems.6.3 Recall and top-rankAs previously noted, the primary evaluation met-ric is very susceptible to penalize slight changes,making it overly pessimistic about systems?
perfor-mance.
Hence, while it may be an efficient way tocompare and rank systems within the framework of353a shared task, it may be unnecessarily devaluing thepractical viability of approaches.
We performed twopost hoc evaluations that assess system output froma practical point of view.
We check how well thetop-ranked substitute, i.e., the simplest substitute ac-cording to a given system (which is most likely tobe used in a real simplification task) compares to thetop-ranked candidate from the gold standard.
This isreported in the TRnk column of Table 4: the percent-age of contexts in which the intersection between thesimplest substitute set from a system?s output andthe gold standard contained at least one element.We note that while ties are virtually inexistent in thegold standard data, ties in the system output can af-fect this metric: a system that naively predicts allsubstitutes as the simplest (i.e., a single tie includ-ing all candidates) will score 100% in this metric.We also measured the ?recall-at-n" values for 1 ?n ?
3, which gives the ratio of candidates from thetop n substitute sets to those from the gold-standard.For a given n, we only consider contexts that haveat least n+1 candidates in the gold-standard (so thatthere is some ranking to be done).
Table 4 shows theresults of this additional analysis.Team - System TRnk n=1 n=2 n=3UOW-SHEF-SimpLex 0.602 0.575 0.689 0.769UNT-SimpRank 0.585 0.559 0.681 0.760Baseline-Simple Freq.
0.585 0.559 0.681 0.760ANNLOR-simple 0.564 0.538 0.674 0.768UNT-SimpRankL 0.567 0.541 0.674 0.753EMNLPCPH-ORD1 0.539 0.513 0.645 0.727EMNLPCPH-ORD2 0.530 0.503 0.637 0.722SB-mmSystem 0.477 0.452 0.632 0.748ANNLOR-lmbing 0.336 0.316 0.494 0.647Baseline-L-Sub Gold 0.454 0.427 0.667 0.959Baseline-Random 0.340 0.321 0.612 0.825UNT-SaLSA 0.146 0.137 0.364 0.532Table 4: Additional results according to the top-rank(TRnk) and recall-at-n metrics.These evaluation metrics favour systems that pro-duce many ties.
Consequently the baselines ?L-SubGold" and ?Random" yield overly high scores forrecall-at-n for n=2 and n= 3.
Nevertheless the restof the results are by and large consistent with therankings from the kappa metric.The results for recall-at-2, e.g., show that mostsystems, on average 70% of the time, are able tofind the simplest 2 substitute sets that correspondto the gold standard.
This indicates that most ap-proaches are reasonably good at distinguishing verysimple substitutes from very complex ones, and thatthe top few substitutes will most often produce ef-fective simplifications.These results correspond to our experience fromthe comparison of human annotators, who are easilyable to form clusters of simplicity with high agree-ment, but who strongly disagree (based on personalbiases towards perceptions of lexical simplicity) onthe internal rankings of these clusters.7 ConclusionsWe have presented the organization and findings ofthe first English Lexical Simplification shared task.This was a first attempt at garnering interest in theNLP community for research focused on the lexicalaspects of Text Simplification.Our analysis has shown that there is a very strongrelation between distributional frequency of wordsand their perceived simplicity.
The best systems onthe shared task were those that relied on this asso-ciation, and integrated both context-dependent andcontext-independent features.
Further analysis re-vealed that while context-dependent features are im-portant in principle, their applied efficacy is some-what lessened due to the setup of the shared task,which treats simplification as an isolated problem.Future work would involve evaluating the im-portance of context for lexical simplification in thescope of a simultaneous simplification to all thewords in a context.
In addition, the annotation ofthe gold-standard datasets could be re-done takinginto consideration some of the features that are nowknown to have clearly influenced the large varianceobserved in the rankings of different annotators,such as their background language and the educa-tion level.
One option would be to select annotatorsthat conform a specific instantiation of these fea-tures.
This should result in a higher inter-annotatoragreement and hence a simpler task for simplifica-tion systems.AcknowledgmentsWe would like to thank the annotators for their hardwork in delivering the corpus on time.354ReferencesMarilisa Amoia and Massimo Romanelli.
2012.
SB-mmSystem: Using Decompositional Semantics forLexical Simplification.
In English Lexical Simplifica-tion.
Proceedings of the 6th International Workshopon Semantic Evaluation (SemEval 2012), Montreal,Canada.Or Biran, Samuel Brody, and Noemie Elhadad.
2011.Putting it simply: a context-aware approach to lexi-cal simplification.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies, pages 496?501,Portland, Oregon.Thorsten Brants and Alex Franz.
2006.
The google web1t 5-gram corpus version 1.1.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011 work-shop on statistical machine translation.
In Proceedingsof the Sixth Workshop on Statistical Machine Transla-tion, pages 22?64, Edinburgh, Scotland.Arnaldo Candido, Jr., Erick Maziero, Caroline Gasperin,Thiago A. S. Pardo, Lucia Specia, and Sandra M.Aluisio.
2009.
Supporting the adaptation of texts forpoor literacy readers: a text simplification editor forBrazilian Portuguese.
In Proceedings of the FourthWorkshop on Innovative Use of NLP for Building Ed-ucational Applications, pages 34?42, Boulder, Col-orado.J Cohen.
1960.
A coefficient of agreement for nominalscales.
Educational and Psychological Measurement,20:37?46, April.Jan de Belder, Koen Deschacht, and Marie-FrancineMoens.
2010.
Lexical simplification.
In Proceedingsof Itec2010: 1st International Conference on Inter-disciplinary Research on Technology, Education andCommunication, Kortrijk, Belgium.Siobhan Devlin and John Tait.
1998.
The use of a psy-cholinguistic database in the simplification of text foraphasic readers.
Linguistic Databases, pages 161?173.Sujay Kumar Jauhar and Lucia Specia.
2012.
UOW-SHEF: SimpLex - Lexical Simplicity Ranking basedon Contextual and Psycholinguistic Features.
In En-glish Lexical Simplification.
Proceedings of the 6thInternational Workshop on Semantic Evaluation (Se-mEval 2012), Montreal, Canada.Anne-Laure Ligozat, Cyril Grouin, Anne Garcia-Fernandez, and Delphine Bernhard.
2012.
ANNLOR:A Naive Notation-system for Lexical Outputs Rank-ing.
In English Lexical Simplification.
Proceedings ofthe 6th International Workshop on Semantic Evalua-tion (SemEval 2012), Montreal, Canada.Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
In Pro-ceedings of the 4th International Workshop on Seman-tic Evaluations (SemEval-2007), Prague, Czech Re-public, pages 48?53.E.
Noreen.
1989.
Computer-intensive methods for test-ing hypotheses.
New York: Wiley.Serge Sharoff.
2006.
Open-source corpora: Using thenet to fish for linguistic data.
International Journal ofCorpus Linguistics, 11(4):435?462.Advaith Siddharthan.
2006.
Syntactic simplification andtext cohesion.
Research on Language and Computa-tion, 4:77?109.Ravi Sinha.
2012.
UNT-SimpRank: Systems for Lex-ical Simplification Ranking.
In English Lexical Sim-plification.
Proceedings of the 6th International Work-shop on Semantic Evaluation (SemEval 2012), Mon-treal, Canada.Lucia Specia.
2010.
Translating from complex to simpli-fied sentences.
In Proceedings of the 9th internationalconference on Computational Processing of the Por-tuguese Language, PROPOR?10, pages 30?39, Berlin,Heidelberg.
Springer-Verlag.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of simplic-ity: unsupervised extraction of lexical simplificationsfrom Wikipedia.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, pages 365?368, Los Angeles, California.355
