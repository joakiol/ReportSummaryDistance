Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 156?159,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsLeveraging POMDPs trained with User Simulations andRule-based Dialogue Management in a Spoken Dialogue SystemSebastian Varges, Silvia Quarteroni, Giuseppe Riccardi, Alexei V. Ivanov, Pierluigi RobertiDepartment of Information Engineering and Computer ScienceUniversity of Trento38050 Povo di Trento, Italy{varges|silviaq|riccardi|ivanov|roberti}@disi.unitn.itAbstractWe have developed a complete spoken di-alogue framework that includes rule-basedand trainable dialogue managers, speechrecognition, spoken language understand-ing and generation modules, and a com-prehensive web visualization interface.We present a spoken dialogue systembased on Reinforcement Learning thatgoes beyond standard rule based modelsand computes on-line decisions of the bestdialogue moves.
Bridging the gap betweenhandcrafted (e.g.
rule-based) and adap-tive (e.g.
based on Partially ObservableMarkov Decision Processes - POMDP) di-alogue models, this prototype is able tolearn high rewarding policies in a numberof dialogue situations.1 Reinforcement Learning in DialogueMachine Learning techniques, and particularlyReinforcement Learning (RL), have recently re-ceived great interest in research on dialogue man-agement (DM) (Levin et al, 2000; Williams andYoung, 2006).
A major motivation for this choiceis to improve robustness in the face of uncertaintydue for example to speech recognition errors.
Asecond important motivation is to improve adap-tivity w.r.t.
different user behaviour and applica-tion/recognition environments.The RL approach is attractive because it offers astatistical model representing the dynamics of theinteraction between system and user.
This con-trasts with the supervised learning approach wheresystem behaviour is learnt based on a fixed cor-pus.
However, exploration of the range of dialoguemanagement strategies requires a simulation en-vironment that includes a simulated user (Schatz-mann et al, 2006) in order to avoid the prohibitivecost of using human subjects.We demonstrate various parameters that influ-ence the learnt dialogue management policy byusing pre-trained policies (section 5).
The appli-cation domain is a tourist information system foraccommodation and events in the local area.
Thedomain of the trained DMs is identical to that of arule-based DM that was used by human users (sec-tion 4), allowing us to compare the two directly.2 POMDP demonstration systemThe POMDP DM implemented in this work isshown in figure 1: at each turn at time t, the incom-ingN user act hypotheses an,u split the state spaceSt to represent the complete set of interpretationsfrom the start state (N=2).
A belief update is per-formed resulting in a probability assigned to eachstate.
The resulting ranked state space is used asa basis for action selection.
In our current imple-mentation, belief update is based on probabilisticuser responses that include SLU confidences.
Ac-tion selection to determine system action am,s isbased on the best state (m is a counter for actionsin action set A).
In each turn, the system uses an-greedy action selection strategy to decide prob-abilistically if to exploit the policy or explore anyother action at random.
(An alternative would besoftmax, for example.)
At the end of each dia-logue/session a reward is assigned and policy en-tries are added or updated for each state-actionpair involved.
These pairs are stored in tabularform.
We perform Monte Carlo updating similarto (Levin et al, 2000):Qt(s, a) = R(s, a)/n+Qt?1 ?
(n ?
1)/n (1)where n is the number of sessions, R the rewardand Q the estimate of the state-action value.At the beginning of each dialogue, a user goalUG (a set of concept-value pairs) is generated ran-domly and passed to a user simulator.
The usersimulator takes UG and the current dialogue con-text to produce plausible SLU hypotheses.
These156Turn?t1?POLICY:??
Q(s1,a1)?s1,a1?s1,a2?s2,a1?s3,a4?St1?a1,s?a2,s?a3,s?a4,s?policy?lookup?policy??update?Ut1?a1,u?final?state?reward?computation?Turn?t2?
Turn?tn?state?space?Ut2?state?space?
St2??s1,1?
?s1,2??s2,2?a2,u?a1,s?
a2,s?a3,s?a4,s?an,s?
an,s?an,s?an,s?state?space?
Stn?user?goal?start?state?s1,n?Q(s1,a2)?Q(s2,a1)?Q(s3,a4)?UG?s3,n?s2,n?s4,n?s5,n?s6,n?Figure 1: POMDP Dialogue Managerare a subset of the concept-value pairs in UG alongwith a confidence estimate bootstrapped from asmall corpus of 74 in-domain dialogs.
We assumethat the user ?runs out of patience?
after 15 turnsand ends the call.The system visualizes POMDP-related infor-mation live for the ongoing dialogue (figure 2).The visualization tool shows the internal represen-tation of the dialogue manager including the theN best dialogue states after each user utteranceand the reranking of the action set.
At the endof each dialogue session, the reward and the pol-icy updates are shown, i.e.
new or updated stateentries and action values.
Moreover, the systemgenerates a plot that relates the current dialogue?sreward to the reward of previous dialogues.3 User SimulationTo conduct thousands of simulated dialogues, theDM needs to deal with heterogeneous but plau-sible user input.
We designed a User Simulator(US) which bootstraps likely user behaviors start-ing from a small corpus of 74 in-domain dialogs,acquired using a rule-based version of the system(section 4).
The role of the US is to simulatethe output of the SLU module to the DM duringthe whole interaction, fully replacing the ASR andSLU modules.
This differs from other user sim-ulation approaches where n-gram models of userdialog acts are represented.For each simulated dialogue, one or more usergoals are randomly selected from a list of possibleuser goals stored in a database table.
A goal is rep-resented as the set of concept-value pairs defininga task.
Simulation of the user?s behaviour happensin two stages.
First, a user model, i.e.
a modelof the user?s intentions at the current stage of thedialogue, is created.
This is done by mining theprevious system move to obtain the concepts re-quired by the DM and their corresponding values(if any) from the current user goal.
Then, the out-put of the user model is passed to an error modelthat simulates the ?noisy channel?
recognition er-rors based on statistics from the dialogue corpus.Errors produce perturbations of concept values aswell as phenomena such as noInput, noMatch andhangUp.
If the latter phenomena occur, they aredirectly propagated to the DM; otherwise, plau-sible confidences (based on the dialogue corpus)are attached to concept-value pairs.
The probabil-ity of a given concept-value observation at timet + 1 given the system move at time t, as,t, andthe session user goal gu, called P (ot+1|as,t, gu),is obtained by combining the outputs of the errormodel and the user model:P (ot+1|au,t+1) ?
P (au,t+1|as,t, gu)where au,t+1 is the true user action.
Finally,concept-value pairs are combined in an SLU hy-pothesis and, as in the regular SLU module, a cu-mulative utterance-level confidence is computed,determining the rank of each of the N hypothesesoutput to the DM.4 Rule-based Dialogue ManagementA rule-based DM was developed as a meaning-ful comparison to the trained DM, to obtain train-ing data from human-system interaction for theUS, and to understand the properties of the do-main.
Rule-based dialog management works intwo stages: retrieving and preprocessing facts (tu-ples) taken from a dialogue state database, andinferencing over those facts to generate a systemresponse.
We distinguish between the ?contextmodel?
of the first phase ?
essentially allowingmore recent values for a concept to override lessrecent ones ?
and the ?dialog move engine?
of thesecond phase.
In the second stage, acceptor rulesmatch SLU results to dialogue context, for ex-ample perceived user concepts to open questions.This may result in the decision to verify the ap-plication parameter in question, and the action isverbalized by language generation rules.
If theparameter is accepted, application dependent task157Figure 2: A screenshot of the online visualization tool.
Left: user goal (top), evolving ranked state space(bottom).
Center: per state action distribution at turn ti.
Right: consequent reward computation (top) andpolicy updates (bottom).
See video at http://www.youtube.com/watch?v=69QR0tKKhCw.158Figure 3: Left Pane: overview of a selection of dialogues in our visualization tool.
Right Pane: visual-ization of a system opening prompt followed by the user?s activity request.
All distinct SLU hypotheses(concept-value combinations) deriving from ASR are ranked based on concept-level confidence (2 in thisturn).rules determine the next parameter to be acquired,resulting in the generation of an appropriate re-quest.
See (Varges et al, 2008) for more details.5 Visualization ToolIn addition to the POMDP-related visualizationtool (figure 2), we developed another web-baseddialogue tool for both rule-based and POMDP sys-tem that displays ongoing and past dialogue ut-terances, semantic interpretation confidences anddistributions of confidences for incoming user acts(see dialogue logs in figure 3).Users are able to talk with several systems(via SIP phone connection to the dialogue systemserver) and see their dialogues in the visualizationtool.
They are able to compare the rule-basedsystem, a randomly exploring learner that has notbeen trained yet, and several systems that use vari-ous pre-trained policies.
The web tool is availableat http://cicerone.dit.unitn.it/DialogStatistics/.AcknowledgmentsThis work was partially supported by the Euro-pean Commission Marie Curie Excellence Grantfor the ADAMACH project (contract No.
022593)and by LUNA STREP project (contract No.33549).ReferencesE.
Levin, R. Pieraccini, and W. Eckert.
2000.
Astochastic model of human-machine interaction forlearning dialog strategies.
IEEE Transactions onSpeech and Audio Processing, 8(1).J.
Schatzmann, K. Weilhammer, M. Stuttle, andS.
Young.
2006.
A Survey of Statistical User Sim-ulation Techniques for Reinforcement-Learning ofDialogue Management Strategies.
Knowledge En-gineering Review, 21(2):97?126.Sebastian Varges, Giuseppe Riccardi, and Silvia Quar-teroni.
2008.
Persistent information state in a data-centric architecture.
In Proc.
9th SIGdial Workhopon Discourse and Dialogue, Columbus, Ohio.J.
D. Williams and S. Young.
2006.
Partially Ob-servable Markov Decision Processes for Spoken Di-alog Systems.
Computer Speech and Language,21(2):393?422.159
