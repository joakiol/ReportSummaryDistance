Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 744?753, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsDomain Adaptation for Coreference Resolution: An Adaptive EnsembleApproachJian Bo Yang??
?, Qi Mao?, Qiao Liang Xiang?, Ivor W. Tsang?,Kian Ming A.
Chai?, Hai Leong Chieu??
School of Computer Engineering, Nanyang Technological University, Singapore?
Electrical and Computer Engineering Department, Duke University, USA?
DSO National Laboratories, Singaporejianbo.yang@duke.edu, {qmao1,qlxiang,ivortsang}@ntu.edu.sg,{ckianmin,chaileon}@dso.org.sgAbstractWe propose an adaptive ensemble method toadapt coreference resolution across domains.This method has three features: (1) it can op-timize for any user-specified objective mea-sure; (2) it can make document-specific pre-diction rather than rely on a fixed base modelor a fixed set of base models; (3) it can auto-matically adjust the active ensemble membersduring prediction.
With simplification, thismethod can be used in the traditional within-domain case, while still retaining the abovefeatures.
To the best of our knowledge, thiswork is the first to both (i) develop a domainadaptation algorithm for the coreference reso-lution problem and (ii) have the above featuresas an ensemble method.
Empirically, we showthe benefits of (i) on the six domains of theACE 2005 data set in domain adaptation set-ting, and of (ii) on both the MUC-6 and theACE 2005 data sets in within-domain setting.1 IntroductionCoreference resolution is a fundamental componentof natural language processing (NLP) and has beenwidely applied in other NLP tasks (Stoyanov et al2010).
It gathers together noun phrases (mentions)that refer to the same real-world entity (Ng andCardie, 2002).
In the past decade, several corefer-ence resolution systems have been proposed, e.g.,(Ng and Cardie, 2002), (Denis and Baldridge, 2007)and (Stoyanov et al2010).
All of these focus onthe within-domain case ?
to use the labeled doc-uments from a domain to predict on the unlabeled?The work is done during postdoc in NTU, Singapore.documents in the same domain.
However, in prac-tice, there is usually limited labeled data in a specificdomain of interest, while there may be plenty of la-beled data in other related domains.
Effective use ofdata from the other domains for predicting in the do-main of interest is therefore an important strategy inNLP.
This is called domain adaptation, and, in thiscontext, the former domains is called the source do-mains, while the latter domain is called the targetdomain (Blitzer et al2006; Jiang and Zhai, 2007).Based on the type of the knowledge to be trans-ferred to the target domain, domain adaptation learn-ing can be categorized as instance-based method,feature-based method, parameter-based method orrelational-knowledge-based method (Pan and Yang,2010).
Previously, domain adaptation learning hasbeen successfully used in other NLP tasks such asrelation extraction (Jiang, 2009) and POS tagging(Jiang and Zhai, 2007), semantic detection (Tan etal., 2008), name entity recognition (Guo et al2009)and entity type classification (Jiang and Zhai, 2007).However, to the best of our knowledge, it has yet tobe explored for coreference resolution.In this paper, we propose an adaptive ensemblemethod to adapt coreference resolution across do-mains.
This proposed method can be categorizedas both feature-based and parameter-based domainadaptation learning methods.
It has three main steps:ensemble creation, cross-domain knowledge learn-ing and decision inference.
The first step createsthe ensemble by collecting a set of base models,which can be any individual methods with variousfeatures/instances/parameters settings.
The secondstep analyzes the collected base models from vari-744ous domains and learns the cross-domain knowledgebetween each target domain and the source domain.The third step infers the final decision in the targetdomain based on all ensemble results.In addition to domain adaptation, the proposedadaptive ensemble method has the following fea-tures that are absent in the other ensemble methods.First, it can optimize any user-specified objectivemeasure without using a separate development set.Second, it can provide document-specific predictioninstead of relying on a fixed base model or a fixedset of base models for all documents.
Third, it canautomatically adjust the active ensemble membersin decision inference so that underperforming basemodels are filtered out.
The proposed method canalso be used in the traditional within-domain prob-lem with some simplifications.We conduct experiments for coreference resolu-tion under both the within-domain setting and thedomain-adaptation setting.
In the within-domainsetting, we compare the proposed adaptive ensemblemethod with the mention-pair methods and other en-semble methods on the MUC-6 and ACE 2005 cor-pora.
The results show that the proposed adaptiveensemble method consistently outperforms thesebaselines.
In the domain adaptation setting, we usethe ACE 2005 corpora to create six domain adap-tation tasks to evaluate the effectiveness of our do-main adaptation learning.
The results show that ourmethod outperforms baselines that do not use do-main adaptation.The paper is organized as follows.
Section 2 re-views some existing ensemble methods for coref-erence resolution.
Section 3 presents the proposedadaptive ensemble method for domain adaptationproblems.
Section 4 presents a special case ofthe proposed method for the within-domain setting.Section 5 presents the experiments under both thewithin-domain and the domain adaptation settings.We conclude and discuss future work in Section 6.2 Existing Ensemble MethodsMany ensemble methods have been proposed in themachine learning literature, e.g., bagging (Breiman,1996), boosting (Freund and Schapire, 1996), ran-dom forest (Breiman, 2001) and mixture models(Bishop, 2007).
Some of them have been success-fully used in coreference resolution (Pang and Fan,2009; Munson et al2005; Rahman and Ng, 2011a).However, these methods only focus on the within-domain setting.All these methods comprise of two steps: ensem-ble creation and decision inference.
Ng and Cardie(2003) and Vemulapalli et al2009) applied thebagging and boosting techniques on the documentsto create the ensemble.
Recently, Rahman and Ng(2011a) further enriched the ensemble by consider-ing various feature sets and learning models.
Specif-ically, three types of feature sets (conventional, lex-ical and combined) and three learning algorithms(mention-pair model, mention-ranking model andthe clustering-ranking model) are employed.
In de-cision inference, these methods used voting or av-eraging to get the final prediction.
Rahman and Ng(2011a) proposed four voting strategies for predic-tion: applying best Per-NP-Type model, antecedent-based voting, cluster-based voting and weightedclustering-based voting.
Although their approachesachieved promising results in their end-to-end sys-tems, these do not consider the user-specific perfor-mance measure during the ensemble learning.Another branch of ensemble methods uses modelselection (Munson et al2005; Ng, 2005), simi-lar to the conventional model selection method forgeneric parameter-tuning.
The method of (Munsonet al2005) first collects a large family of base mod-els.
Then, a separate tuning set with ground truthis used to evaluate each base model?s performance.Finally, an iterative approach is used to select thebest performed base models to form the ensemble.Like other methods, this method uses the averagestrategy in decision inference.
Similarly, the methodof (Ng, 2005) ranks base models according to theirperformance on separate tuning set, and then usesthe highest-ranked base model for predicting on testdocuments.
These methods require a separate set oflabeled documents to assess the generalization per-formance.3 Adaptive Ensemble MethodIn this section, we give our adaptive ensemblemethod for domain adaptation for coreference res-olution.
We first introduce some notations.For a corpus of N documents, document Di745is the ith document, and it contains ni men-tions mi = (m1i , .
.
.
,mnii ) with the ordering ofeach mention as they appear in the document.The index set of all mention pairs in Di isEi = {(a, b) | 1 ?
a < b ?
ni}.
The transpose ofvector x is x?.
The performance measure functionfor document D is ?
(g(D); f(D)), where g(D) andf(D) represent the coreference ground-truth andprediction by model f on document D respectively.In coreference resolution, typical performance mea-sure functions include MUC (Vilain et al1995),Rand index (Rand, 1971), B-CUBED (Bagga andBaldwin, 1998) and CEAF (Luo, 2005).
In this pa-per, ?
can either be used as part of an objective func-tion in learning or as an evaluation measure for as-sessing the performance of a coreference system.We consider the typical domain adaptation prob-lem, which has one target domain t and p (p ?
1)source domains s1, .
.
.
, sp.
The target domaincontains N (t) labeled documents and M unla-beled documents, while source domains containN (s1), .
.
.
, N (sp) labeled documents.
Unlabeleddata in the source domains are not used.
We useD(v)i for the ith document in domain v.3.1 Ensemble CreationMention-pair methods have been widely-used forcoreference resolution due to their efficiency andeffectiveness, and they have often been taken asbase models in ensemble learning (Rahman and Ng,2011a; Munson et al2005).
We adopt a similar ap-proach by using the standard mention-pair method(Soon et al2001; Ng and Cardie, 2002) with var-ious parameters to form the ensemble, though ourframework can incorporate other coreference meth-ods in the ensemble.
Mention-pair methods usu-ally comprise of two steps.
The first step classifiesevery mention pair into either coreference or non-coreference with a confidence between 0 and 1.
Thesecond step partitions the set of mentions into clus-ters based on the confidence values, where mentionsin each cluster are presumed to be the same under-lying entity.Classification We use Soon?s approach (Soon etal., 2001) to select a portion of mention pairs to traina binary classifier because this has better generaliza-tion (Soon et al2001).
The positive mention pairsare the anaphoric mentionmbi (b = 2, .
.
.
, ni) pairedwith its closest antecedent mention mai (a < b),while the negative mention pairs are the mentionmbi paired with each of the intervening mentionsma+1i ,ma+2i , .
.
.
,mb?1i .
Following (Rahman andNg, 2011a), our binary classifier is SVM with theregularization parameter C. The classifier is trainedwith the software Liblinear (Fan et al2008), whichis also used to give probabilistic binary predictions.Clustering We adopt closest-first clustering (Soonet al2001) and best-first clustering (Ng and Cardie,2002) to determine whether a mention pair is coref-erent.
For each mention, the closest-first method(or best-first method) links it to the the closest (orthe best) preceding mention if the confidence value(obtained from the first step) of this mention pair isabove a specified threshold t.Features For each mention pair, we use thed = 39 features proposed by Rahman and Ng(2011b) to represent it.
These features can be ex-tracted using the Reconcile software (Stoyanov etal., 2010).
We use ?
?a,b ?
Rd to represent the fea-tures of a mention pair (ma,mb).
With this featureset, we found that the linear kernel is insufficient tofit the training data.
However, using an rbf kernelwould be too computationally expensive.
Hence, weaugment ?
?a,b with a d?-dimensional feature vector[?1 ?
?
?
?d?]
to give a new feature vector?a,b = [?
?a,b ?1 ?
?
?
?d?
], (1)where the d?
augmented features [?1 ?
?
?
?d?]
are de-termined by?j = exp(???
?a,b ?
cj?2d),?j = 1, .
.
.
, d?.
(2)Herein, c1, .
.
.
, cd?
are the d?
centroids of therandomly-selected subset C from all labeled men-tion pairs {?
?a,b | (a, b) ?
E1, .
.
.
, EN}.
In our ex-periments, we use the k-means algorithm to obtainthe centroids of C.Ensemble For domain v, we create a domain-specified ensemble F (v) = {f1, .
.
.
, f ?}
of ?
basemodels by including the closest-first and best-firstmention-pair methods with the differentC and t val-ues.
If multiple domains are provided, we gather all746the domain-specific ensembles into a grand ensem-ble F = F (s1) ?
?
?
?
F (sp) ?
F (t).3.2 Cross-domain Knowledge LearningGenerally, the feature distributions are different indifferent domains.
Therefore, effective domainadaptation requires using some knowledge of cross-domain similarity.
We now propose an approachto learn the parametric-distances between the doc-uments in source and target domains to characterizethis cross-domain knowledge.Distances between documents A document Di isrepresented by the sum of its new mention-pair fea-tures (Yu and Joachims, 2009; Finley and Joachims,2005):?
(Di) =?(a,b)?Ei?a,b.
(3)The distance between a source labeled documentD(su)i in domain su and a target labeled documentD(t)j is parameterized asDist(D(su)i ,D(t)j ;?)
= ???
(D(su)i ,D(t)j ), (4)where vector ?
?
Rd+d?
is to be learned, and vec-tor function?
(D(su)i ,D(t)j ) ?
Rd+d?
is the Euclideandistance vector between two documents given by?
(D(su)i ,D(t)j ) = (?
(D(su)i )?
?
(D(t)j ))?
(?
(D(su)i )?
?
(D(t)j )).
(5)The operator ?
is the element-wise product.
Dis-tance (4) is actually the Mahanalobis distance (Yangand Jin, 2006) with the scaling of features:(?
(D(su)i )?
?
(D(t)j ))?W (?
(D(su)i )?
?
(D(t)j )),where W is a diagonal matrix with diagonal entries?.
MatrixW is diagonal to reduce computation costand to increase statistical confidence in estimationwhen there is limited target labeled data (as is typi-cally the case in domain adaptation).That ?
is the vector of diagonal entries in W re-quires that each entry in ?
is non-negative.
If the lthentry of ?
is non-zero, then the lth feature in ?a,bcontribute towards (4).
To ensure that at least B fea-tures are used, we also constrain that each entry in ?is not more than unity and that 1??
?
B.Matching best base models For each labeled doc-ument D(v)j in domain v, we identify the best per-forming base model f (v)?j in F (v) withf (v)?j = arg maxf?F(v)?
(g(D(v)j ); f(D(v)j )), (6)where ?(?
; ?)
is the the performance objective func-tion to be instantiated in Section 3.3.Then, for each source domain su and documentD(t)j in the target domain, we find the set I(D(t)j ; su)of the documents in domain su that have the samebest performing base model as that for D(t)j :I(D(t)j ; su) = {D(su)i | f(su)?i = f(t)?j ,i = 1, .
.
.
, N (su)}.
(7)The key idea in I(D(t)j ; su) is to select documentsin a source domain su that are similar to documentD(t)j in the sense that they have the same best per-forming base model under a specific ?.
This ensuresthat optimization step (to be described next) is tar-geted towards ?
and not confounded by documentpairs that should be disimilar anyway.Optimization We determine the vector?
by mini-mizing the parametric distance (4) between all targetlabeled documents and their corresponding sourcelabeled document identified in the previous step.That is,min??
?N(t)?j=1?D(su)i ?I(D(t)j ;su)?
(D(su)i ,D(t)j ).
(8)The solution ?
to this linear programming problemcan be regarded as the cross-domain knowledge be-tween source domain su and the target domain t. Re-peating for every source domain su, u = 1, .
.
.
, p,gives the cross-domain knowledge between everysource domain and the target domain.The above three-steps procedure selects the effec-tive features for each pair of source and target do-mains.
Generally, the results of feature selectionvary for different pairs of source and target domains,due to the diversities of the feature distributions indifferent domains.7473.3 Decision InferenceAfter ensemble creation and cross-domain knowl-edge learning, we need to provide the coreferenceresult on an unseen document in the target domainbased on the results of all the members in F .
Un-like the previous methods using the voting/averageor their variants (Pang and Fan, 2009; Munson etal., 2005; Rahman and Ng, 2011a), we propose thefollowing nearest neighbor based approach.Given the grand ensemble F and all labeled doc-uments, the task is to predict on the target unlabeleddocument D(t)j , j = 1, .
.
.
,M .
The idea of the pro-posed method is to first find the k most similar docu-ments N (D(t)j ) from all labeled documents for doc-ument D(t)j .
Then, we choose the base model thatperforms best on the documents in N (D(t)j ) as themethod f (t)?j for document D(t)j .Firstly, we employ the parametric-distance (4) tomeasure the similarity between any labeled docu-mentD(v)i ,?v, i, from all source and target domains,and the target unlabeled document D(t)j .
Here, thecross-domain knowledge ?
in (4) has already beendetermined by the optimization (8) in Section 3.2.Secondly, based on the computed distance values,we select k nearest neighbor documents for the tar-get unlabeled document D(t)j from all labeled doc-uments D(v)i ,?v, i.
These k nearest neighbor docu-ments for document D(t)j make up the set N (D(t)j ).Thirdly, the optimal base model for the unlabeleddocument D(t)j prediction is chosen byf (t)?j = arg maxDp?N (D(t)j ), f?F?
(g(Dp); f(Dp)).
(9)We can instantiate the performance objective func-tion ?(g(?
); f(?))
in expressions (6) and (9) to beany coreference resolution measures, such as MUC,Rand index, B-CUBED and CEAF.
We have notknown of other (ensemble) coreference resolutionmethods that optimize for these measures.
This ab-sence is possibly due to their complex discrete andnon-convex properties.3.4 DiscussionThe above proposed adaptive ensemble approach in-corporates the domain adaptation knowledge during(a) the identification of similar documents betweendifferent domains and (b) the determination of ac-tive ensemble members.
Beside these, it has the fol-lowing features over other (ensemble) coreferencemethods: (i) It can optimize any user-specified ob-jective measure via (6) and (9).
An intuitive rec-ommendation is to directly optimize for an objectivefunction that matches the evaluation measure.
(ii)It can make document-specific decisions, as expres-sions (4) and (9) deal with each testing documentseparately.
(iii) The prediction on the testing docu-ment D(t)j is not based on all members in F but onlyon the active ensemble members N (D(t)j ).
This canfilter out some potentially unsuitable base modelsfor document D(t)j .
Moreover, the active ensemblemembers N (D(t)j ) is dynamically adjusted for eachtest document.For computational cost, the majority is by ensem-ble creation, since a large number of base modelsare usually used.
This is common among all ensem-ble methods.
In contrast, the costs in (4) and (9)are trivial as both are at the document level.
Thecost of generating centroids in (2) can also be highif the size of C is more than ten thousand, but thisis still negligible compared to the cost of ensemblecreation.4 Special Case: Within-domain SettingThe adaptive ensemble method presented in Sec-tion 3 is for the domain adaptation setting.
How-ever, it is possible to simplify it for the special caseof within-domain setting.
In the within-domain set-ting, the adaptive ensemble method only has ensem-ble creation and decision inference steps.In the ensemble creation step, we still use theclosest-first and best-first mention-pair methodswith various parameters to create the ensemble.
Un-like the domain adaptation setting, here we can onlyuse the labeled documents in the target domain tocreate the ensemble F (t).
Therefore, the size of en-semble here is reduced by p times compared to thedomain adaptation setting.In the decision inference step, we directly use theEuclidean distance ?
(D(t)i ,D(t)j ) in (5) for the la-beled documentD(t)i , i = 1, .
.
.
, N (t) and unlabeleddocument D(t)j , j = 1, .
.
.
,M .
Based on these dis-748tance values, we similarly select k nearest neighbordocumentsN (D(t)j ) for documentD(t)j , and then de-termine the final method f (t)?j for document D(t)j by(9) but with F replaced by F (t).5 ExperimentsWe test the proposed adaptive method and sev-eral baselines under both the within-domain andthe domain adaptation settings on the MUC-6 andACE 2005 corpora.
MUC-6 contains 60 docu-ments.
ACE 2005 contains 599 documents fromsix different domains: Newswire (NW), BroadcastNews (BN), Broadcast Conversations (BC), Web-blog (WL), Usenet (UN), and Conversational Tele-phone Speech (CTS).
In all our experiments, we usetwo popular performance measures, B-CUBED F-measure (Bagga and Baldwin, 1998) and CEAF F-measure (Luo, 2005) 1, to evaluate the coreferenceresolution result.
Since the focus of the paper is toinvestigate the effectiveness of coreference resolu-tion methods, we use the gold standard mentions inall experiments.For the proposed method, the ensemble F (v) inevery domain v has 208 members totally.
Theyare created by the closest-first and the best-firstmention-pair methods using SVM trained with pa-rameter C taking valuesC ?
[0.001, 0.01, 0.1, 1, 10, 100, 1000, 1000] (10)and using clustering with the threshold parameters ttaking valuest ?
[0.2, 0.25, 0.3, 0.34, 0.38, 0.4, 0.42, 0.44,0.46, 0.48, 0.5, 0.6, 0.7].
(11)The size of the selected subset C is fixed to 2000,and the number of centroids is determined bythe validation procedure from four possible values[10, 20, 30, 40].
We use k-means algorithm to com-pute the centroids.
Due to the randomness of sub-set C and k-means algorithm, we run the proposedmethod 5 times and report the average results.
Forthe number of nearest neighbor k, we report threeresults, each for k ?
{1, 3, 5}.1More exactly, we use the widely used ?3-CEAF F-measure.Table 1: The settings in the experiments under within-domain setting on MUC-6 and ACE 2005 corpora.
N (t)and M (t) and Total are the numbers of training, testingand all documents respectively.Domain N (t) M (t) TotalMUC-6 30 30 60BC 48 12 60BN 181 45 226CTS 31 8 39NW 85 21 106UN 39 10 49WL 95 24 1195.1 Within-domain SettingWe conduct the experiment under the within-domainsetting on seven tasks, with the per-domain settingshown in Table 1.
The validation set is created byfurther splitting training data into validation train-ing and validation testing sets with the ratio of N(t)M(t) ,where N (t) and M (t) are given in Table 1.
In thisexperiment, we attempt to study the following threethings.
First, we investigate whether the proposedensemble method is better than the tuned mention-pair methods and other ensemble methods.
Second,we investigate the optimal number of active ensem-ble members.
Third, we investigate the impact to theperformance of the coreference system, when differ-ent objective measures are used with different eval-uation measures.For the proposed ensemble method, we experi-mented with nearest neighbor set of sizes k = 1, 3, 5paired with objective function ?
in (9) set to RandIndex, CEAF or B-CUBED.
For baselines, the fol-lowing four are used:?
Two mention-pair baselines.
Two baselines arethe closest-first and the best-first mention-pairmethods with the tuned parameters C and t. Inthe tuning process, the ranges of C and t arespecified in (10) and (11) respectively.
Thesetwo mention-pair methods are named as Sc andSb for short.?
Two existing ensemble baselines.
The othertwo baselines are the ensemble methods us-ing the voting procedure in decision inference.749Table 2: B-CUBED F-measure results by all methods under within-domain setting on MUC-6 and ACE 2005 corpora.Baselines ?
= Rand ?
= CEAF ?
= B-CUBEDSc Sb Em Ec k=1 3 5 k=1 3 5 k=1 3 5MUC-6 66.1 66.1 61.9 57.1 67.6 67.3 68.5 65.2 64.1 65.5 68.7 66.7 67.5BC 64.1 65.1 34.2 24.8 65.5 65.4 65.7 65.9 65.5 62.9 66.5 66.1 66.0BN 75.9 74.8 57.7 48.0 75.7 75.1 74.9 76.3 75.9 75.3 76.4 76.3 76.7CTS 71.0 65.1 39.6 31.5 70.6 69.3 68.3 71.3 69.9 70.4 71.7 70.6 69.1NW 74.6 74.4 45.6 34.1 74.3 74.8 72.9 73.2 71.4 70.1 75.0 74.6 73.7UN 69.5 70.2 44.1 27.4 70.4 69.9 69.3 69.6 67.6 66.0 70.3 71.4 70.3WL 73.8 75.4 69.8 58.5 75.5 74.6 73.9 75.5 73.0 73.4 76.2 75.5 75.6Average 70.7 70.2 50.4 40.2 71.4 70.9 70.5 71.0 69.6 69.1 72.1 71.6 71.3These two baselines use the same ensemble asthe proposed method for fair comparison.
Indecision inference, these two baselines use themention-based voting and cluster-based votingrespectively, as proposed in (Rahman and Ng,2011a).
In these two baselines, all membersin the ensemble participate the voting process.These two ensemble baselines are named as Emand Ec for short.Tables 2 and 3 show the experiment results usingB-CUBED and CEAF as the evaluation measuresrespectively.
The best result for each of the seventasks is highlighted in bold.
The last rows of the ta-bles show the average performance value among allseven tasks.From the results, we observe that the proposed en-semble method with objective function matching theevaluation measure and with k = 1 generally per-forms best among all methods and all tasks.
Surpris-ingly, the common ensemble method with mention-based voting Em and cluster-based voting Ec strate-gies do not perform well.
The plausible reason isthe current ensemble may incorporate some bad basemodels due to inappropriate C and t values, whichwould undermine the voting result.
Nevertheless, itis difficult to judge the quality of the ensemble mem-bers in advance.
Therefore, this validates the impor-tance of choosing an active set of ensemble membersin decision inference.
The better performance of theproposed method over the mention-pair baselines Scand Sb is probably because of the document-specificdecision.
This is reasonable, as different base mod-els in the ensemble would be good at predictingthe different documents.
For the proposed ensem-ble method with various configurations, we observeusing an objective function that matches the evalu-ation measures is generally better.
An exception isthe MUC-6 and BN tasks in CEAF F-measure.
Wealso observe that the ensemble method with k = 1is generally better than that with the larger k, exceptthe BN and UN tasks in B-CUBED F-measure.
Thissuggests that the fewer the active ensemble membersthe better the generalization performance.
Follow-ing (Rahman and Ng, 2011a), we also conduct theStudent?s t-test, and the results show that the pro-posed method with the objective function matchingthe evaluation measure and with k = 1 is signifi-cantly better than the best baseline.
In contrast, thetwo baseline ensemble methods that use voting aresignificantly worse than the best baseline.
The sig-nificance level 0.05.5.2 Domain-adaptation SettingWe employ ACE 2005 corpora to simulate the do-main adaptation settings in experiments.
Specifi-cally, we create six domain adaptation tasks, BC,BN, CTS, NW, UN, WL in total.
Each task has onetarget domain and five source domains.
For exam-ple, in the task UN, the target domain is UN whilethe other five source domains are BC, BN, CTS, NWand WL.
The number of labeled documents in eachdomain is as the same as in Table 1, except whenthat domain is the target domain, in which case weuse only five labeled documents.
The number of test750Table 3: CEAF F-measure results by all methods under within-domain setting on MUC-6 and ACE 2005 corpora.Baselines ?
= Rand ?
= B-CUBED ?
= CEAFSc Sb Em Ec k=1 3 5 k=1 3 5 k=1 3 5MUC-6 62.6 62.5 62.7 57.5 62.0 60.6 61.0 64.5 62.7 63.8 63.1 58.7 59.2BC 58.8 56.5 36.6 26.6 56.7 57.1 57.0 58.3 58.8 57.2 59.3 59.2 58.4BN 67.9 66.5 55.1 44.7 69.4 69.4 69.9 69.8 70.2 69.6 69.5 69.0 68.7CTS 61.0 60.7 38.6 31.5 67.1 66.9 63.6 68.1 68.4 68.2 68.5 67.6 67.7NW 66.9 66.4 41.1 31.2 68.4 68.0 64.6 69.2 68.4 66.4 69.3 66.1 66.7UN 62.5 63.5 46.2 28.9 62.9 61.8 60.9 62.2 63.7 62.9 63.9 61.5 60.4WL 69.7 70.3 63.5 54.3 70.7 70.2 72.5 71.5 71.4 72.3 72.4 69.4 70.0Average 64.2 63.8 49.1 39.2 65.3 64.9 64.2 66.2 66.2 65.8 66.6 64.5 64.5(or unlabeled) documents in the target document isalso the same as in Table 1.
The validation set iscreated similarly as in the experiment under within-domain setting.For the proposed ensemble method, we heuristi-cally determine the parameter B in ?
to be the num-ber of non-zero elements in ?, where?
=N(t)?j=1?D(su)i ?I(D(t)j ;su)?
(D(su)i ,D(t)j ).Making use of the conclusion in the experimentsfor the within-domain setting, we fix the optimizedmeasure to be the final performance measure in (9).We compare with the following five baselines.?
Two mention-pair baselines in within-domainsetting.
Two baselines are same as Sc and Sb inthe experiments under within-domain settings,except that the labeled training documents arereduced to 5.?
Three proposed adaptive ensemble methodswithout cross-domain knowledge learning.These three baselines uses neighborhood sizesk = 1, 3, 5 with the grand ensemble F ratherthan the target domain ensemble F (t).
In an-other words, these three baselines are the sameas the proposed method, but with ?
= 1.Tables 4 and 5 show the experimental results inthe domain adaptation settings using B-CUBED andCEAF as the final performance measures respec-tively.
From the results, we can see that the pro-posed method with cross-domain knowledge gener-ally outperforms all the five baselines.
Among them,the best proposed domain adaptation method on av-erage outperforms the best of Sc, Sb by 7.2% for B-CUBED F-measure and 3% for CEAF F-measure.The grand-ensemble baselines are also significantlybetter than the within-domain baselines.
These re-sults clearly illustrate the usefulness of making useof the labeled documents in the source domains.
Forthe comparison between the proposed method withand without cross-domain knowledge learning, alltasks, except UN task in CEAF F-measure, showthe superiority of the proposed method with cross-domain knowledge learning.
Among them, excepttasks BN and CTS in B-CUBED F-measure, the per-formance gains are among 1%?3% for all tasks inboth measures.
These results verify the necessityof cross-domain knowledge learning.
For the com-parison of the proposed method with different k,unlike the results in the within-domain setting, theresults here show that choosing optimal k is task-dependent.
The reason of this observation is notclear yet.
It is plausible due to the increased uncer-tainties from multiple domains.6 Conclusions and Future WorkIn this paper, we proposed an adaptive ensem-ble method for coreference resolution under bothwithin-domain and domain adaptation settings.
Thekey advantage of the proposed method is incor-751Table 4: B-CUBED F-measure results by all methods under domain adaptation setting on ACE 2005 corpora, with ?set to B-CUBED.
The within-domain and grand ensemble methods are the baselines.Within-domain Grand ensemble Domain-adaptationSc Sb k=1 3 5 k=1 3 5BC 58.0 65.1 65.0 67.1 67.0 67.5 68.2 67.7BN 72.7 73.8 75.0 75.3 75.0 75.3 75.4 74.3CTS 63.2 62.1 65.7 64.8 64.0 64.1 65.8 65.8NW 54.9 54.6 73.6 73.1 74.2 73.0 74.4 74.7UN 66.5 42.7 67.2 68.2 68.9 69.7 68.7 68.2WL 68.6 73.2 73.0 72.6 73.4 74.8 74.5 73.6Average 64.0 61.9 69.9 70.2 70.4 70.7 71.2 70.7Table 5: CEAF F-measure results by all methods under domain adaptation setting on ACE 2005 corpora, with ?
setto CEAF.
The within-domain and grand ensemble methods are the baselines.Within-domain Grand ensemble Domain-adaptationSc Sb k=1 3 5 k=1 3 5BC 55.7 43.7 56.9 57.6 57.3 58.5 58.8 57.2BN 65.8 67.2 65.9 64.1 65.8 63.9 62.7 67.2CTS 56.0 51.0 56.6 54.6 53.7 58.6 57.4 55.3NW 52.7 55.0 66.4 64.1 63.8 69.4 66.7 66.8UN 64.0 39.1 63.6 63.7 64.4 64.3 62.9 62.7WL 70.3 64.2 68.1 67.8 70.2 67.3 69.6 72.0Average 60.7 53.4 62.9 62.0 62.5 63.7 63.0 63.5porating the cross-domain knowledge to aid coref-erence resolution learning.
This is useful whenthe labeled coreference labels are scarce.
We alsodemonstrate that the proposed adaptive ensemblemethod can be readily applied to conventional coref-erence tasks without cross-domain knowledge learn-ing.
Compared with existing ensemble methods, theproposed method is simultaneously endowed withthe following three distinctive features: optimizingany user-specified performance measure, making thedocument-specific prediction and automatically ad-justing the active ensemble members.
In the exper-iments under both within-domain settings and do-main adaptation settings, the results evidence theeffectiveness of the proposed cross-domain knowl-edge learning method, and also demonstrate the su-periority of the proposed adaptive ensemble methodover other baselines.Currently, the proposed method relies on somelimited target annotations.
It would be interestingto consider the pure unsupervised tasks that have noany target annotations.
Besides, to develop somebetter ways for document-level representation, e.g.,incorporating the domain knowledge, also deservesour attentions.
Similarly, to extend the diagonal Ma-halanobis matrix to the general covariance matrix isalso desirable.
Last but not least, to find a more sys-tematical way to determine the optimal k in the pro-posed method is also our possible future work.AcknowledgmentsThis work is supported by DSO grantDSOCL10021.ReferencesAmit Bagga and Breck Baldwin.
1998.
Entity-basedcross-document coreferencing using the vector space752model.
In Proceedings of the 36th Annual Meetingof the Association for Computational Linguistics and17th International Conference on Computational Lin-guistics - Volume 1, ACL?98, pages 79?85.Christopher M. Bishop.
2007.
Pattern Recognition andMachine Learning (Information Science and Statis-tics).
Springer, 1st ed.
2006. corr.
2nd printing edition,October.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proceedings of EMNLP, pages120?128.Leo Breiman.
1996.
Bagging predictors.
MachineLearning, 24(2):123?140, August.Leo Breiman.
2001.
Random forests.
Machine Learn-ing, 45(1):5?32, October.Pascal Denis and Jason Baldridge.
2007.
Joint determi-nation of anaphoricity and coreference resolution us-ing integer programming.
In Proc HLT, pages 236?243, Rochester, New York, April.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-RuiWang, and Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research, 9:1871?1874.Thomas Finley and Thorsten Joachims.
2005.
Super-vised clustering with support vector machines.
InProc.
ICML.Yoav Freund and Robert E. Schapire.
1996.
Experimentswith a New Boosting Algorithm.
In Proc.
ICML,pages 148?156.Honglei Guo, Huijia Zhu, Zhili Guo, Xiaoxun Zhang,Xian Wu, and Zhong Su.
2009.
Domain adapta-tion with latent semantic association for named entityrecognition.
NAACL ?09, pages 281?289.Jing Jiang and ChengXiang Zhai.
2007.
Instance weight-ing for domain adaptation in NLP.
In Proc.
ACL,pages 264?271, Prague, Czech Republic, June.Jing Jiang.
2009.
Multi-task transfer learning forweakly-supervised relation extraction.
In Proceedingsof the Joint Conference of the 47th Annual Meetingof the ACL and the 4th International Joint Conferenceon Natural Language Processing of the AFNLP, pages1012?1020, Suntec, Singapore, August.
Associationfor Computational Linguistics.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of the conference onHuman Language Technology and Empirical Methodsin Natural Language Processing, HLT ?05, pages 25?32.Art Munson, Claire Cardie, and Rich Caruana.
2005.Optimizing to arbitrary NLP metrics using ensembleselection.
In Proc HLT and EMNLP, pages 539?546.Vincent Ng and Claire Cardie.
2002.
Improving ma-chine learning approaches to coreference resolution.In Proc.
ACL, pages 104?111.Vincent Ng and Claire Cardie.
2003.
Weakly supervisednatural language learning without redundant views.
InProc.
HLT-NAACL.Vincent Ng.
2005.
Machine learning for coreference res-olution: From local classification to global ranking.
InProceedings of the ACL, pages 157?164.Sinno Jialin Pan and Qiang Yang.
2010.
A survey ontransfer learning.
IEEE Transactions on Knowledgeand Data Engineering, 22(10):1345?1359, October.Wenbo Pang and Xiaozhong Fan.
2009.
Chinese coref-erence resolution with ensemble learning.
In Proc.PACIIA, pages 236?243.Altaf Rahman and Vincent Ng.
2011a.
Ensemble-based coreference resolution.
In Proceedings of IJ-CAI, pages 1884?1889.Altaf Rahman and Vincent Ng.
2011b.
Narrowing themodeling gap: A cluster-ranking approach to corefer-ence resolution.
JAIR, 1:469?52.William M. Rand.
1971.
Objective criteria for the eval-uation of clustering methods.
Journal of the AmericanStatistical Association, 66(336):pp.
846?850.W.
M. Soon, H. T. Ng, and D. C. Y. Lim.
2001.
A ma-chine learning approach to coreference resolution ofnoun phrases.
Computational Linguistics,, pages 521?544.Veselin Stoyanov, Claire Cardie, Nathan Gilbert, EllenRiloff, David Buttler, and David Hysom.
2010.
Coref-erence resolution with reconcile.
In Proc.
ACL, pages156?161.Songbo Tan, Yuefen Wang, Gaowei Wu, and XueqiCheng.
2008.
Using unlabeled data to handle domain-transfer problem of semantic detection.
In Proceed-ings of the 2008 ACM symposium on Applied comput-ing, SAC ?08, pages 896?903.S.
Vemulapalli, X. Luo, J.F.Pitrelli, and I. Zitouni.
2009.classifier combination applied to coreference resolu-tion.
In NAACL HLT Student Rsearch Workshop.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceed-ings of the 6th conference on Message understanding,MUC6 ?95, pages 45?52.Liu Yang and Rong Jin.
2006.
Distance Metric Learning:A Comprehensive Survey.
Technical report, Depart-ment of Computer Science and Engineering, MichiganState University.Chun-Nam John Yu and Thorsten Joachims.
2009.Learning structural SVMs with latent variables.
InProc.
ICML, pages 1169?1176, New York, NY, USA.753
