Using Conditional Random Fields to Predict Pitch Accents inConversational SpeechMichelle L. GregoryLinguistics DepartmentUniversity at BuffaloBuffalo, NY 14260mgregory@buffalo.eduYasemin AltunDepartment of Computer ScienceBrown UniversityProvidence, RI 02912altun@cs.brown.eduAbstractThe detection of prosodic characteristics is an im-portant aspect of both speech synthesis and speechrecognition.
Correct placement of pitch accents aidsin more natural sounding speech, while automaticdetection of accents can contribute to better word-level recognition and better textual understanding.In this paper we investigate probabilistic, contex-tual, and phonological factors that influence pitchaccent placement in natural, conversational speechin a sequence labeling setting.
We introduce Con-ditional Random Fields (CRFs) to pitch accent pre-diction task in order to incorporate these factors ef-ficiently in a sequence model.
We demonstrate theusefulness and the incremental effect of these fac-tors in a sequence model by performing experimentson hand labeled data from the Switchboard Corpus.Our model outperforms the baseline and previousmodels of pitch accent prediction on the Switch-board Corpus.1 IntroductionThe suprasegmental features of speech relay criticalinformation in conversation.
Yet, one of the ma-jor roadblocks to natural sounding speech synthe-sis has been the identification and implementationof prosodic characteristics.
The difficulty with thistask lies in the fact that prosodic cues are never ab-solute; they are relative to individual speakers, gen-der, dialect, discourse context, local context, phono-logical environment, and many other factors.
This isespecially true of pitch accent, the acoustic cues thatmake one word more prominent than others in anutterance.
For example, a word with a fundamen-tal frequency (f0) of 120 Hz would likely be quiteprominent in a male speaker, but not for a typical fe-male speaker.
Likewise, the accent on the utterance?Jon?s leaving.?
is critical in determining whetherit is the answer to the question ?Who is leaving??
(?JON?s leaving.?)
or ?What is Jon doing??
(?Jon?sLEAVING.?).
Accurate pitch accent prediction liesin the successful combination of as many of the con-textual variables as possible.
Syntactic informationsuch as part of speech has proven to be a success-ful predictor of accentuation (Hirschberg, 1993; Panand Hirschberg, 2001).
In general, function wordsare not accented, while content words are.
Vari-ous measures of a word?s informativeness, such asthe information content (IC) of a word (Pan andMcKeown, 1999) and its collocational strength in agiven context (Pan and Hirschberg, 2001) have alsoproven to be useful models of pitch accent.
How-ever, in open topic conversational speech, accent isvery unpredictable.
Part of speech and the infor-mativeness of a word do not capture all aspects ofaccentuation, as we see in this example taken fromSwitchboard, where a function word gets accented(accented words are in uppercase):I, I have STRONG OBJECTIONS to THAT.Accent is also influenced by aspects of rhythmand timing.
The length of words, in both numberof phones and normalized duration, affect its likeli-hood of being accented.
Additionally, whether theimmediately surrounding words bear pitch accentalso affect the likelihood of accentuation.
In otherwords, a word that might typically be accented maybe unaccented because the surrounding words alsobear pitch accent.
Phrase boundaries seem to playa role in accentuation as well.
The first word of in-tonational phrases (IP) is less likely to be accentedwhile the last word of an IP tends be accented.
Inshort, accented words within the same IP are not in-dependent of each other.Previous work on pitch accent prediction, how-ever, neglected the dependency between labels.
Dif-ferent machine learning techniques, such as deci-sion trees (Hirschberg, 1993), rule induction sys-tems (Pan and McKeown, 1999), bagging (Sun,2002), boosting (Sun, 2002) have been used in ascenario where the accent of each word is pre-dicted independently.
One exception to this lineof research is the use of Hidden Markov Models(HMM) for pitch accent prediction (Pan and McK-eown, 1999; Conkie et al, 1999).
Pan and McKe-own (1999) demonstrate the effectiveness of a se-quence model over a rule induction system, RIP-PER, that treats each label independently by show-ing that HMMs outperform RIPPER when the samevariables are used.Until recently, HMMs were the predominant for-malism to model label sequences.
However, theyhave two major shortcomings.
They are trainednon-discriminatively using maximum likelihood es-timation to model the joint probability of the ob-servation and label sequences.
Also, they requirequestionable independence assumptions to achieveefficient inference and learning.
Therefore, vari-ables used in Hidden Markov models of pitch ac-cent prediction have been very limited, e.g.
part ofspeech and frequency (Pan and McKeown, 1999).Discriminative learning methods, such as MaximumEntropy Markov Models (McCallum et al, 2000),Projection Based Markov Models (Punyakanok andRoth, 2000), Conditional Random Fields (Laffertyet al, 2001), Sequence AdaBoost (Altun et al,2003a), Sequence Perceptron (Collins, 2002), Hid-den Markov Support Vector Machines (Altun etal., 2003b) and Maximum-Margin Markov Net-works (Taskar et al, 2004), overcome the limita-tions of HMMs.
Among these methods, CRFs isthe most common technique used in NLP and hasbeen successfully applied to Part-of-Speech Tag-ging (Lafferty et al, 2001), Named-Entity Recog-nition (Collins, 2002) and shallow parsing (Sha andPereira, 2003; McCallum, 2003).The goal of this study is to better identify whichwords in a string of text will bear pitch accent.Our contribution is two-fold: employing new pre-dictors and utilizing a discriminative model.
Wecombine the advantages of probabilistic, syntactic,and phonological predictors with the advantages ofmodeling pitch accent in a sequence labeling settingusing CRFs (Lafferty et al, 2001).The rest of the paper is organized as follows: InSection 2, we introduce CRFs.
Then, we describeour corpus and the variables in Section 3 and Sec-tion 4.
We present the experimental setup and reportresults in Section 5.
Finally, we discuss our results(Section 6) and conclude (Section 7).2 Conditional Random FieldsCRFs can be considered as a generalization of lo-gistic regression to label sequences.
They definea conditional probability distribution of a label se-quence y given an observation sequence x.
In thispaper, x = (x1, x2, .
.
.
, xn) denotes a sentence oflength n and y = (y1, y2, .
.
.
, yn) denotes the la-bel sequence corresponding to x.
In pitch accentprediction, xt is a word and yt is a binary label de-noting whether xt is accented or not.CRFs specify a linear discriminative function Fparameterized by ?
over a feature representation ofthe observation and label sequence ?(x,y).
Themodel is assumed to be stationary, thus the featurerepresentation can be partitioned with respect to po-sitions t in the sequence and linearly combined withrespect to the importance of each feature ?k, de-noted by ?k.
Then the discriminative function canbe stated as in Equation 1:F (x,y; ?)
=?t??,?t(x,y)?
(1)Then, the conditional probability is given byp(y|x; ?)
= 1Z(x,?
)F (x,y; ?)
(2)where Z(x,?)
= ?y?
F (x, y?
; ?)
is a normaliza-tion constant which is computed by summing overall possible label sequences y?
of the observation se-quence x.We extract two types of features from a sequencepair:1.
Current label and information about the obser-vation sequence, such as part-of-speech tag ofa word that is within a window centered at theword currently labeled, e.g.
Is the current wordpitch accented and the part-of-speech tag ofthe previous word=Noun?2.
Current label and the neighbors of that label,i.e.
features that capture the inter-label depen-dencies, e.g.
Is the current word pitch accentedand the previous word not accented?Since CRFs condition on the observation se-quence, they can efficiently employ feature repre-sentations that incorporate overlapping features, i.e.multiple interacting features or long-range depen-dencies of the observations, as opposed to HMMswhich generate observation sequences.In this paper, we limit ourselves to 1-orderMarkov model features to encode inter-label de-pendencies.
The information used to encode theobservation-label dependencies is explained in de-tail in Section 4.In CRFs, the objective function is the log-loss ofthe model with ?
parameters with respect to a train-ing set D. This function is defined as the negativesum of the conditional probabilities of each traininglabel sequence yi, given the observation sequencexi, where D ?
{(xi,yi) : i = 1, .
.
.
,m}.
CRFs areknown to overfit, especially with noisy data if notregularized.
To overcome this problem, we penalizethe objective function by adding a Gaussian prior(a term proportional to the squared norm ||?||2) assuggested in (Johnson et al, 1999).
Then the lossfunction is given as:L(?
;D) = ?m?ilog p(yi|xi; ?)
+12c||?||2= ?m?iF (xi,yi; ?)
+ logZ(xi,?
)+ 12c||?||2 (3)where c is a constant.Lafferty et al (2001), proposed a modificationof improved iterative scaling for parameter estima-tion in CRFs.
However, gradient-based methodshave often found to be more efficient for minimizingEquation 3 (Minka, 2001; Sha and Pereira, 2003).In this paper, we use the conjugate gradient descentmethod to optimize the above objective function.The gradients are computed as in Equation 4:?
?L =m?i?tEp[?t(xi,y)] ?
?t(xi,yi)+ c?
(4)where the expectation is with respect to all possi-ble label sequences of the observation sequence xiand can be computed using the forward backwardalgorithm.Given an observation sequence x, the best labelsequence is given by:y?
= arg maxyF (x,y; ??)
(5)where ??
is the parameter vector that minimizesL(?;D).
The best label sequence can be identifiedby performing the Viterbi algorithm.3 CorpusThe data for this study were taken from the Switch-board Corpus (Godfrey et al, 1992), which con-sists of 2430 telephone conversations between adultspeakers (approximately 2.4 million words).
Partic-ipants were both male and female and representedall major dialects of American English.
We used aportion of this corpus that was phonetically hand-transcribed (Greenberg et al, 1996) and segmentedinto speech boundaries at turn boundaries or pausesof more than 500 ms on both sides.
Fragments con-tained seven words on average.
Additionally, eachword was coded for probabilistic and contextualinformation, such as word frequency, conditionalprobabilities, the rate of speech, and the canonicalpronunciation (Fosler-Lussier and Morgan, 1999).The dataset used in all analysis in this study con-sists of only the first hour of the database, comprisedof 1,824 utterances with 13,190 words.
These utter-ances were hand coded for pitch accent and intona-tional phrase brakes.3.1 Pitch Accent CodingThe utterances were hand labeled for accents andboundaries according to the Tilt Intonational Model(Taylor, 2000).
This model is characterized by aseries of intonational events: accents and bound-aries.
Labelers were instructed to use duration, am-plitude, pausing information, and changes in f0 toidentify events.
In general, labelers followed the ba-sic conventions of EToBI for coding (Taylor, 2000).However, the Tilt coding scheme was simplified.Accents were coded as either major or minor (andsome rare level accents) and breaks were either ris-ing or falling.
Agreement for the Tilt coding wasreported at 86%.
The CU coding also used a simpli-fied EToBI coding scheme, with accent types con-flated and only major breaks coded.
Accent andbreak coding pair-wise agreement was between 85-95% between coders, with a kappa ?
of 71%-74%where ?
is the difference between expected agree-ment and actual agreement.4 VariablesThe label we were predicting was a binary distinc-tion of accented or not.
The variables we used forprediction fall into three main categories: syntac-tic, probabilistic variables, which include word fre-quency and collocation measures, and phonologicalvariables, which capture aspects of rhythm and tim-ing that affect accentuation.4.1 Syntactic variablesThe only syntactic category we used was a four-way classification for hand-generated part of speech(POS): Function, Noun, Verb, Other, where Otherincludes all adjectives and adverbs1 .
Table 1 givesthe percentage of accented and unaccented items byPOS.1We also tested a categorization of 14 distinct part of speechclasses, but the results did not improve, so we only report on thefour-way classification.Accented UnaccentedFunction 21% 79%Verb 59% 41%Noun 30% 70%Other 49% 51%Table 1: Percentage of accented and unaccenteditems by POS.Variable Definition ExampleUnigram log p(wi) and, IBigram log p(wi|wi?1) roughing itRev Bigram log p(wi|wi+1) rid ofJoint log p(wi?1, wi) and IRev Joint log p(wi, wi+1) and ITable 2: Definition of probabilistic variables.4.2 Probabilistic variablesFollowing a line of research that incorporates theinformation content of a word as well as collo-cation measures (Pan and McKeown, 1999; Panand Hirschberg, 2001) we have included a numberof probabilistic variables.
The probabilistic vari-ables we used were the unigram frequency, the pre-dictability of a word given the preceding word (bi-gram), the predictability of a word given the follow-ing word (reverse bigram), the joint probability of aword with the preceding (joint), and the joint prob-ability of a word with the following word (reversejoint).
Table 2 provides the definition for these,as well as high probability examples from the cor-pus (the emphasized word being the current target).Note all probabilistic variables were in log scale.The values for these probabilities were obtainedusing the entire 2.4 million words of SWBD2.
Table3 presents the Spearman?s rank correlation coeffi-cient between the probabilistic measures and accent(Conover, 1980).
These values indicate the strongcorrelation of accents to the probabilistic variables.As the probability increases, the chance of an accentdecreases.
Note that all values are significant at thep < .001 level.We also created a combined part of speech andunigram frequency variable in order to have a vari-able that corresponds to the variable used in (Pan2Our current implementation of CRF only takes categoricalvariables, thus for the experiments, all probabilistic variableswere binned into 5 equal categories.
We also tried more binsand produced similar results, so we only report on the 5-binnedcategories.
We computed correlations between pitch accent andthe original 5 variables as well as the binned variables and theyare very similar.Variables Spearman?s ?Unigram -.451Bigram -.309Reverse Bigram -.383Joint -.207Reverse joint -.265Table 3: Spearman?s correlation values for the prob-abilistic measures.and McKeown, 1999).4.3 Phonological variablesThe last category of predictors, phonological vari-ables, concern aspects of rhythm and timing of anutterance.
We have two main sources for these vari-ables: those that can be computed solely from astring of text (textual), and those that require somesort of acoustic information (acoustic).
Sun (2002)demonstrated that the number of phones in a syl-lable, the number of syllables in a word, and theposition of a word in a sentence are useful predic-tors of which syllables get accented.
While Sun wasconcerned with predicting accented syllables, someof the same variables apply to word level targets aswell.
For our textual phonological features, we in-cluded the number of syllables in a word and thenumber of phones (both in citation form as well astranscribed form).
Instead of position in a sentence,we used the position of the word in an utterancesince the fragments do not necessarily correspondto sentences in the database we used.
We also madeuse of the utterance length.
Below is the list of ourtextual features:?
Number of canonical syllables?
Number of canonical phones?
Number of transcribed phones?
The length of the utterance in number of words?
The position of the word in the utteranceThe main purpose of this study is to better pre-dict which words in a string of text receive accent.So far, all of our predictors are ones easily com-puted from a string of text.
However, we have in-cluded a few variables that affect the likelihood ofa word being accented that require some acousticdata.
To the best of our knowledge, these featureshave not been used in acoustic models of pitch ac-cent prediction.
These features include the durationof the word, speech rate, and following intonationalphrase boundaries.
Given the nature of the SWBDcorpus, there are many disfluencies.
Thus, we alsoFeature ?2 Sigcanonical syllables 1636 p < .001canonical phones 2430 p < .001transcribed phones 2741 p < .001utt length 80 p < .005utt position 295 p < .001duration 3073 p < .001speech rate 101 p < .001following pause 27 p < .001foll filled pause 328 p < .001foll IP boundary 1047 p < .001Table 4: Significance of phonological features onpitch accent prediction.included following pauses and filled pauses as pre-dictors.
Below is the list of our acoustic features:?
Log of duration in milliseconds normalizedby number of canonical phones binned into 5equal categories.?
Log Speech Rate; calculated on strings ofspeech bounded on either side by pauses of300 ms or greater and binned into 5 equal cat-egories.?
Following pause; a binary distinction ofwhether a word is followed by a period of si-lence or not.?
Following filled pause; a binary distinction ofwhether a word was followed by a filled pause(uh, um) or not.?
Following IP boundaryTable 4 indicates that each of these features sig-nificantly affect the presence of pitch accent.
Whilecertainly all of these variables are not independentof on another, using CRFs, one can incorporate allof these variables into the pitch accent predictionmodel with the advantage of making use of the de-pendencies among the labels.4.4 Surrounding InformationSun (2002) has shown that the values immediatelypreceding and following the target are good predic-tors for the value of the target.
We also experi-mented with the effects of the surrounding valuesby varying the window size of the observation-labelfeature extraction described in Section 2.
When thewindow size is 1, only values of the word that is la-belled are incorporated in the model.
When the win-dow size is 3, the values of the previous and the fol-lowing words as well as the current word are incor-porated in the model.
Window size 5 captures thevalues of the current word, the two previous wordsand the two following words.5 Experiments and ResultsAll experiments were run using 10 fold cross-validation.
We used Viterbi decoding to find themost likely sequence and report the performance interms of label accuracy.
We ran all experiments withvarying window sizes (w ?
{1, 3, 5}).
The baselinewhich simply assigns the most common label, un-accented, achieves 60.53 ?
1.50%.Previous research has demonstrated that part ofspeech and frequency, or a combination of thesetwo, are very reliable predictors of pitch accent.Thus, to test the worthiness of using a CRF model,the first experiment we ran was a comparison of anHMM to a CRF using just the combination of part ofspeech and unigram.
The HMM score (referred asHMM:POS, Unigram in Table 5) was 68.62 ?
1.78,while the CRF model (referred as CRF:POS, Uni-gram in Table 5) performed significantly better at72.56 ?
1.86.
Note that Pan and McKeown (1999)reported 74% accuracy with their HMM model.The difference is due to the different corpora usedin each case.
While they also used spontaneousspeech, it was a limited domain in the sense thatit was speech from discharge orders from doctorsat one medical facility.
The SWDB corpus is opendomain conversational speech.In order to capture some aspects of the IC andcollocational strength of a word, in the second ex-periment we ran part of speech plus all of the prob-abilistic variables (referred as CRF:POS, Prob inTable 5).
The model accuracy was 73.94%, thusimproved over the model using POS and unigramvalues by 1.38%.In the third experiment we wanted to know if TTSapplications that made use of purely textual inputcould be aided by the addition of timing and rhythmvariables that can be gleaned from a text string.Thus, we included the textual features described inSection 4.3 in addition to the probabilistic and syn-tactic features (referred as CRF:POS, Prob, Txt inTable 5).
The accuracy was improved by 1.73%.For the final experiment, we added the acousticvariable, resulting in the use of all the variables de-scribed in Section 4 (referred as CRF:All in Table5).
We get about 0.5% increase in accuracy, 76.1%with a window of size w = 1.Using larger windows resulted in minor increasesin the performance of the model, as summarized inTable 5.
Our best accuracy was 76.36% using allfeatures in a w = 5 window size.Model:Variables w = 1 w = 3 w = 5Baseline 60.53HMM: POS,Unigram 68.62CRF: POS, Unigram 72.56CRF: POS, Prob 73.94 74.19 74.51CRF: POS, Prob, Txt 75.67 75.74 75.89CRF: All 76.1 76.23 76.36Table 5: Test accuracy of pitch accent prediction onSWDB using various variables and window sizes.6 DiscussionPitch accent prediction is a difficult task, in that, thenumber of different speakers, topics, utterance frag-ments and disfluent production of the SWBD corpusonly increase this difficulty.
The fact that 21% ofthe function words are accented indicates that mod-els of pitch accent that mostly rely on part of speechand unigram frequency would not fair well with thiscorpus.
We have presented a model of pitch accentthat captures some of the other factors that influenceaccentuation.
In addition to adding more probabilis-tic variables and phonological factors, we have useda sequence model that captures the interdependenceof accents within a phrase.Given the distinct natures of corpora used, it isdifficult to compare these results with earlier mod-els.
However, in experiment 1 (HMM: POS, Uni-gram vs CRF: POS, Unigram) we have shown thata CRF model achieves a better performance than anHMM model using the same features.
However,the real strength of CRFs comes from their abilityto incorporate different sources of information effi-ciently, as is demonstrated in our experiments.We did not test directly the probabilistic measures(or collocation measures) that have been used beforefor this task, namely information content (IC) (Panand McKeown, 1999) and mutual information (Panand Hirschberg, 2001).
However, the measures wehave used encompass similar information.
For ex-ample, IC is only the additive inverse of our unigrammeasure:IC(w) = ?
log p(w) (6)Rather than using mutual information as a measureof collocational strength, we used unigram, bigramand joint probabilities.
A model that includes bothjoint probability and the unigram probabilities of wiand wi?1 is comparable to one that includes mutualinformation.Just as the likelihood of a word being accentedis influenced by a following silence or IP bound-ary, the collocational strength of the target wordwith the following word (captured by reverse bi-gram and reverse joint) is also a factor.
With theuse of POS, unigram, and all bigram and joint prob-abilities, we have shown that (a) CRFs outperformHMMs, and (b) our probabilistic variables increaseaccuracy from a model that include POS + unigram(73.94% compared to 72.56%).For tasks in which pitch accent is predicted solelybased on a string of text, without the addition ofacoustic data, we have shown that adding aspectsof rhythm and timing aids in the identification ofaccent targets.
We used the number of words inan utterance, where in the utterance a word falls,how long in both number of syllables and numberof phones all affect accentuation.
The addition ofthese variables improved the model by nearly 2%.These results suggest that Accent prediction modelsthat only make use of textual information could beimproved with the addition of these variables.While not trying to provide a complete modelof accentuation from acoustic information, in thisstudy we tested a few acoustic variables that havenot yet been tested.
The nature of the SWBD cor-pus allowed us to investigate the role of disfluenciesand widely variable durations and speech rate on ac-centuation.
Especially speech rate, duration and sur-rounding silence are good predictors of pitch accent.The addition of these predictors only slightly im-proved the model (about .5%).
Acoustic features arevery sensitive to individual speakers.
In the corpus,there are many different speakers of varying agesand dialects.
These variables might become moreuseful if one controls for individual speaker differ-ences.
To really test the usefulness of these vari-ables, one would have to combine them with acous-tic features that have been demonstrated to be goodpredictors of pitch accent (Sun, 2002; Conkie et al,1999; Wightman et al, 2000).7 ConclusionWe used CRFs with new measures of collocationalstrength and new phonological factors that captureaspects of rhythm and timing to model pitch accentprediction.
CRFs have the theoretical advantage ofincorporating all these factors in a principled and ef-ficient way.
We demonstrated that CRFs outperformHMMs also experimentally.
We also demonstratedthe usefulness of some new probabilistic variablesand phonological variables.
Our results mainly haveimplications for the textual prediction of accents inTTS applications, but might also be useful in au-tomatic speech recognition tasks such as automatictranscription of multi-speaker meetings.
In the nearfuture we would like to incorporate reliable acousticinformation, controlling for individual speaker dif-ference and also apply different discriminative se-quence labeling techniques to pitch accent predic-tion task.8 AcknowledgementsThis work was partially funded by CAREER award#IIS 9733067 IGERT.
We would also like to thankMark Johnson for the idea of this project, Dan Ju-rafsky, Alan Bell, Cynthia Girand, and Jason Bre-nier for their helpful comments and help with thedatabase.ReferencesY.
Altun, T. Hofmann, and M. Johnson.
2003a.Discriminative learning for label sequences viaboosting.
In Proc.
of Advances in Neural Infor-mation Processing Systems.Y.
Altun, I. Tsochantaridis, and T. Hofmann.
2003b.Hidden markov support vector machines.
InProc.
of 20th International Conference on Ma-chine Learning.M.
Collins.
2002.
Discriminative training meth-ods for Hidden Markov Models: Theory and ex-periments with perceptron algorithms.
In Proc.of Empirical Methods of Natural Language Pro-cessing.A.
Conkie, G. Riccardi, and R. Rose.
1999.Prosody recognition from speech utterances us-ing acoustic and linguistic based models ofprosodic events.
In Proc.
of EUROSPEECH?99.W.
J. Conover.
1980.
Practical NonparametricStatistics.
Wiley, New York, 2nd edition.E.
Fosler-Lussier and N. Morgan.
1999.
Effects ofspeaking rate and word frequency on conversa-tional pronunci ations.
In Speech Communica-tion.J.
Godfrey, E. Holliman, and J. McDaniel.
1992.SWITCHBOARD: Telephone speech corpus forresearch and develo pment.
In Proc.
of the Inter-national Conference on Acoustics, Speech, andSignal Processing.S.
Greenberg, D. Ellis, and J. Hollenback.
1996.
In-sights into spoken language gleaned from pho-netic transcripti on of the Switchboard corpus.In Proc.
of International Conference on SpokenLanguage Processsing.J.
Hirschberg.
1993.
Pitch accent in context: Pre-dicting intonational prominence from text.
Artifi-cial Intelligence, 63(1-2):305?340.M.
Johnson, S. Geman, S. Canon, Z. Chi, andS.
Riezler.
1999.
Estimators for stochasticunification-based grammars.
In Proc.
of ACL?99Association for Computational Linguistics.J.
Lafferty, A. McCallum, and F. Pereira.
2001.Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
InProc.
of 18th International Conference on Ma-chine Learning.A.
McCallum, D. Freitag, and F. Pereira.
2000.Maximum Entropy Markov Models for Infor-mation Extraction and Segmentation.
In Proc.of 17th International Conference on MachineLearning.A.
McCallum.
2003.
Efficiently inducing featuresof Conditional Random Fields.
In Proc.
of Un-certainty in Articifical Intelligence.T.
Minka.
2001.
Algorithms for maximum-likelihood logistic regression.
Technical report,CMU, Department of Statistics, TR 758.S.
Pan and J. Hirschberg.
2001.
Modeling localcontext for pitch accent prediction.
In Proc.
ofACL?01, Association for Computational Linguis-tics.S.
Pan and K. McKeown.
1999.
Word informa-tiveness and automatic pitch accent modeling.In Proc.
of the Joint SIGDAT Conference onEMNLP and VLC.V.
Punyakanok and D. Roth.
2000.
The use ofclassifiers in sequential inference.
In Proc.
ofAdvances in Neural Information Processing Sys-tems.F.
Sha and F. Pereira.
2003.
Shallow parsing withconditional random fields.
In Proc.
of HumanLanguage Technology.Xuejing Sun.
2002.
Pitch accent prediction usingensemble machine learning.
In Proc.
of the In-ternational Conference on Spoken Language Pro-cessing.B.
Taskar, C. Guestrin, and D. Koller.
2004.
Max-margin markov networks.
In Proc.
of Advancesin Neural Information Processing Systems.P.
Taylor.
2000.
Analysis and synthesis of intona-tion using the Tilt model.
Journal of the Acousti-cal Society of America.C.
W. Wightman, A. K. Syrdal, G. Stemmer,A.
Conkie, and M. Beutnagel.
2000.
Percep-tually Based Automatic Prosody Labeling andProsodically Enriched Unit Selection ImproveConcatenative Text-To-Speech Synthesis.
vol-ume 2, pages 71?74.
