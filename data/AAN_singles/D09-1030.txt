Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 286?295,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPFast, Cheap, and Creative: Evaluating Translation QualityUsing Amazon?s Mechanical TurkChris Callison-BurchCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, Marylandccb cs jhu eduAbstractManual evaluation of translation quality isgenerally thought to be excessively timeconsuming and expensive.
We explore afast and inexpensive way of doing it usingAmazon?s Mechanical Turk to pay smallsums to a large number of non-expert an-notators.
For $10 we redundantly recre-ate judgments from a WMT08 transla-tion task.
We find that when combinednon-expert judgments have a high-level ofagreement with the existing gold-standardjudgments of machine translation quality,and correlate more strongly with expertjudgments than Bleu does.
We go on toshow that Mechanical Turk can be used tocalculate human-mediated translation editrate (HTER), to conduct reading compre-hension experiments with machine trans-lation, and to create high quality referencetranslations.1 IntroductionConventional wisdom holds that manual evalua-tion of machine translation is too time-consumingand expensive to conduct.
Instead, researchersroutinely use automatic metrics like Bleu (Pap-ineni et al, 2002) as the sole evidence of im-provement to translation quality.
Automatic met-rics have been criticized for a variety of reasons(Babych and Hartley, 2004; Callison-Burch et al,2006; Chiang et al, 2008), and it is clear thatthey only loosely approximate human judgments.Therefore, having people evaluate translation out-put would be preferable, if it were more practical.In this paper we demonstrate that the manualevaluation of translation quality is not as expensiveor as time consuming as generally thought.
Weuse Amazon?s Mechanical Turk, an online labormarket that is designed to pay people small sumsof money to complete human intelligence tests ?tasks that are difficult for computers but easy forpeople.
We show that:?
Non-expert annotators produce judgmentsthat are very similar to experts and that havea stronger correlation than Bleu.?
Mechanical Turk can be used for complextasks like human-mediated translation editrate (HTER) and creating multiple referencetranslations.?
Evaluating translation quality through read-ing comprehension, which is rarely done, canbe easily accomplished through creative useof Mechanical Turk.2 Related workSnow et al (2008) examined the accuracy of la-bels created using Mechanical Turk for a varietyof natural language processing tasks.
These tasksincluded word sense disambiguation, word simi-larity, textual entailment, and temporal orderingof events, but not machine translation.
Snow etal.
measured the quality of non-expert annotationsby comparing them against labels that had beenpreviously created by expert annotators.
They re-port inter-annotator agreement between expert andnon-expert annotators, and show that the averageof many non-experts converges on performance ofa single expert for many of their tasks.Although it is not common for manual evalu-ation results to be reported in conference papers,several large-scale manual evaluations of machinetranslation quality take place annually.
These in-clude public forums like the NIST MT Evalu-ation Workshop, IWSLT and WMT, as well asthe project-specific Go/No Go evaluations for theDARPA GALE program.
Various types of humanjudgments are used.
NIST collects 5-point fluencyand adequacy scores (LDC, 2005), IWSLT and286WMT collect relative rankings (Callison-Burch etal., 2008; Paul, 2006), and DARPA evaluates us-ing HTER (Snover et al, 2006).
The details ofthese are provided later in the paper.
Public eval-uation campaigns provide a ready source of gold-standard data that non-expert annotations can becompared to.3 Mechanical TurkAmazon describes its Mechanical Turk web ser-vice1as artificial artificial intelligence.
The nameand tag line refer to a historical hoax from the 18thcentury where an automaton appeared to be able tobeat human opponents at chess using a clockworkmechanism, but was, in fact, controlled by a per-son hiding inside the machine.
The MechanicalTurk web site provides a way to pay people smallamounts of money to perform tasks that are sim-ple for humans but difficult for computers.
Exam-ples of these Human Intelligence Tasks (or HITs)range from labeling images to moderating blogcomments to providing feedback on relevance ofresults for a search query.Anyone with an Amazon account can eithersubmit HITs or work on HITs that were submit-ted by others.
Workers are sometimes referred toas ?Turkers?
and people designing the HITs are?Requesters.?
Requesters can specify the amountthat they will pay for each item that is completed.Payments are frequently as low as $0.01.
Turkersare free to select whichever HITs interest them.Amazon provides three mechanisms to help en-sure quality: First, Requesters can have each HITbe completed by multiple Turkers, which allowshigher quality labels to be selected, for instance,by taking the majority label.
Second, the Re-quester can require that all workers meet a particu-lar set of qualications, such as sufficient accuracyon a small test set or a minimum percentage ofpreviously accepted submissions.
Finally, the Re-quester has the option of rejecting the work of in-dividual workers, in which case they are not paid.The level of good-faith participation by Turkersis surprisingly high, given the generally small na-ture of the payment.2For complex undertakingslike creating data for NLP tasks, Turkers do not1http://www.mturk.com/2For an analysis of the demographics of Turk-ers and why they participate, see: http://behind-the-enemy-lines.blogspot.com/2008/03/mechanical-turk-demographics.htmlhave a specialized background in the subject, sothere is an obvious tradeoff between hiring indi-viduals from this non-expert labor pool and seek-ing out annotators who have a particular expertise.4 Experts versus non-expertsWe use Mechanical Turk as an inexpensive wayof evaluating machine translation.
In this section,we measure the level of agreement between ex-pert and non-expert judgments of translation qual-ity.
To do so, we recreate an existing set of gold-standard judgments of machine translation qualitytaken from the Workshop on Statistical MachineTranslation (WMT), which conducts an annuallarge-scale human evaluation of machine transla-tion quality.
The experts who produced the gold-standard judgments are computational linguistswho develop machine translation systems.We recreated all judgments from the WMT08German-English News translation task.
The out-put of the 11 different machine translation systemsthat participated in this task was scored by rankingtranslated sentences relative to each other.
To col-lect judgements, we reproduced the WMT08 webinterface in Mechanical Turk and provided theseinstructions:Evaluate machine translation quality Rank each transla-tion from Best to Worst relative to the other choices (ties areallowed).
If you do not know the source language then youcan read the reference translation, which was created by aprofessional human translator.The web interface displaced 5 different machinetranslations of the same source sentence, and hadradio buttons to rate them.Turkers were paid a grand total of $9.75 tocomplete nearly 1,000 HITs.
These HITs ex-actly replicated the 200 screens worth of expertjudgments that were collected for the WMT08German-English News translation task, with eachscreen being completed by five different Turkers.The Turkers were shown a source sentence, a ref-erence translation, and translations from five MTsystems.
They were asked to rank the translationsrelative to each other, assigning scores from bestto worst and allowing ties.We evaluate non-expert Turker judges by mea-suring their inter-annotator agreement with theWMT08 expert judges, and by comparing the cor-relation coefficient across the rankings of the ma-chine translation systems produced by the two setsof judges.287Sentence-level ranking(2360 itemscompared)Choose 1 2 3 4 5run 1run 2run 3run 4run 5run 6run 7run 8run 9run 10Expert-ExpertAgreement(56 itemscompared)Constituent-level ranking(4960 itemscompared)Chooserun 1run 2run 3run 4run 5run 6run 7run 8run 9run 10Expert-ExpertAgreement(339 itemscompared)Constituent-yes/no (4702itemscompared)Chooserun 1run 2run 3run 4run 5run 6run 7run 8run 9run 10Expert-ExpertAgreement(294 itemscompared)44.0% 51.6% 55.0% 56.8% 57.6%  perl analyze.perl sentence-ranking-Batch_25317_result.csv | cat - ../wmt08-human-judgments.csv.expert_annotator_id | grep German-English | grep -v Europarl | grep ",RANK,"| perl ../process_judgements.pl39.5% 51.9% 53.8% 57.0%42.5% 51.6% 54.3% 57.5%39.0% 53.9% 56.7% 57.6%40.6% 51.9% 56.6% 57.1%43.4% 49.0% 55.0% 56.2%41.9% 52.7% 55.6% 57.6%40.0% 47.9% 55.1% 56.6%40.5% 49.6% 56.8% 57.6%41.6% 47.3% 54.2% 58.4%41.3% 50.7% 55.3% 57.2% 57.6%41.1% 50.9% 54.9% 56.8% 57.8% <--weighted with non-experts40.9% 50.6% 53.9% 57.1% 57.8% <--weighted with non-experts (only against same size choose)41.5% 44.6% 47.9% 51.4% 53.0% <--- without weighting57.8% 57.8% 57.8% 57.8% 57.8%<--reported in WMT08 papercat ../wmt08-human-judgments.csv | grep German-English | grep -v Europarl | grep ",RANK," | perl ../process_judgements.pl1 2 3 4 555.8% 54.8% 55.1% 53.2% 53.4% perl analyze.perl constituent-ranking-Batch_25553_result.csv.itemIDs_fixed | cat - ../wmt08-human-judgments.csv.expert_annotator_id | grep German-English | grep -v Europarl | grep ",CONSTITUENT," | perl ../process_judgements.pl55.8% 54.8% 55.1% 53.2% 53.4%64.0% 64.0% 64.0% 64.0% 64.0%  ../wmt08-human-judgments.csv | grep German-English | grep -v Europarl | grep ",CONSTITUENT," | perl ../process_judgements.pl1 2 3 4 567.5% perl analyze-yes-no.perl constituent-yes-no-Batch_40756_result.csv.itemIDs_fixed | cat - ../wmt08-human-judgments.csv.expert_annotator_id | grep German-English | grep -v Europarl | grep ",CONSTITUENT_ACCEPT," | perl ../process_judgements.pl67.5%68.0% 68.0% 68.0% 68.0% 68.0% cat ../wmt08-human-judgments.csv | grep German-English | grep -v Europarl | grep ",CONSTITUENT_ACCEPT," | perl ../process_judgements.pl40%45%50%55%60%1 2 3 4 5Non-expert agreement with expertsNumber of non-experts votingUnweightedWeighted by non-expertsWeighted by expertsExpert v. Expert AgreementFigure 1: Agreement on ranking translatedsentences increases as more non-experts vote.Weighting non-experts?
votes based on agreementwith either experts or other non-expert increasesit up further.
Five weighted non-experts reach thetop line agreement between experts.Combining ranked judgments Each item is re-dundantly judged by five non-experts.
We wouldlike to combine of their judgments into a singlejudgment.
Combining ranked judgments it is morecomplicated than taking simple majority vote.
Weuse techniques from preference voting, in whichvoters rank a group of candidates in order of pref-erence.
To create an ordering from the the ranksassigned to the systems by multiple Turkers, weuse Schulze?s method (Schulze, 2003).
It is guar-anteed to correctly pick the winner that is pre-ferred pairwise over the other candidates.
It fur-ther allows a complete ranking of candidates to beconstructed, making it a suitable method for com-bining ranked judgments.Figure 1 shows the effect of combining non-experts judgments on their agreement with ex-perts.
Agreement is measured by examining eachpair of translated sentence and counting when twoannotators both indicated that A > B, A < B,or A = B.
Chance agreement is13.
The top lineindicates the inter-annotator agreement betweenWMT08 expert annotators, who agreed with eachother 58% of the time.
When we have only a sin-gle non-expert annotator?s judgment for each item,the agreement with experts is only 41%.
As weincrease the number of non-experts to five, theiragreement with experts improves to 53%, if theirTurker num HITs agreementA88B8YGWNRCDEA19P0XAPEMYVA4AB1UOP54VJ302AHGV18N1TW0U0A17TQVV46QFOPRA2KOG6YHXOH6NCA35JEX1CTHJ33QA38NBJV89Z0LZXAYMF2DS636OWVA2CGL84Z8W7SBSA37CX0VEG3TG23A1036VFVDH85DYA2Y4KSQZJTFGGNA35Y70MA34AFQ6A2XUF8U8TMOZ15AA0KYO0DZS6WIA339F49S46I0ECA20YJNU6YX5DMSA1XH9D9A4NHG4A1A6PCJWCD65GUA3VK3AVKZ6WE90A1LZZD7EC0CUR5AY1636IGKCLHYACPRBLNDZYK62A2MLF2MEMTX888AMXXRYDC28BY8A2AJJ7A2VLJR8VAVIKREHTNVTR5AP914VPXZEVUWAANG4NITPGM9AA3JG7O1DF5P7TIA3IP9RZLIV3UVJA2IINUWLHXF4L7A1O9BHBJKVI5PSA1EVIR5BCW38EGA18B5QL328E02AA16YNOXG4YX7W0AZSX83KOOREJ0AYWWEG65YVGCIAS65HZZ5T2B0UA3MBIJON2PHBNCA37JQWRRV7IS37A2HBWOM2DGNTCFA24ZRABMVOF68VA1FM4EOZWGF6VVA19X9J18Q19NGYAYUEYT9TZJDGUAN7O7OLS9ND8KA8H56XB9K7DB5A3FGNLFDOSH8W7A36EOEBVBUTO85A2U5YY5E2KPTP2A2SB2BW2W4T4WWA20J9VGWRY6KCPA1OCUUHOWY8B52A1MBZ6LM7JZKP7A1E8EDB7UZK38DAUAZYGJWM6JLKA523O8LQK0C6EA3V6V5LSC4PN8HA3OHQRF1MDQ99BA3MVGVHE2HCEROA3LSSQO6UWSKFRA3DOUTFZ2DG53LA2WLPO1VYCAEQFA2LYIHZLRYIKPQA2HNP1YL1IBFMUA2CGQY6JZGGLYPA2B8DTCHI9U902A2A4OZEX98MVZSA25ZA4IBIEZGNCA235PZXMLBGA4QA1T4TJIPCOQOGPA1SXPREFGUADVYA1MI7U9VJHRIPKA1KS77MYC4GGZ3A1JQN7G6S8158GA169X9KQI7BMA3A15A3FUISZDPI4A14LPCJ1O1773B190 0.3232142970 0.571075466 0.2412060339 0.2955390339 0.343685338 0.3996023934 0.5892473130 0.4160583925 0.3269841325 0.3002754820 0.37517 0.6088888915 0.3425925913 0.4384236513 0.4497041411 0.3892215611 0.610 0.3975155310 0.5723684210 0.59 0.607594949 0.666666678 0.611510798 0.673469398 0.619834717 0.432203397 0.446153856 0.606837616 0.448598136 0.529411766 0.632653066 0.608333336 0.521367526 0.591836736 0.55 0.509433965 0.559055124 0.605263164 0.677083334 0.584 0.632183914 0.510416674 0.545454554 0.556603774 0.505154644 0.469387763 0.681318683 0.627906983 0.658536593 0.618421053 0.592105263 0.526315793 0.653846153 0.592105263 0.557142863 0.552631583 0.585714292 0.651515152 0.6252 0.573529412 0.651515152 0.621212122 0.590909092 0.602941182 0.621212122 0.636363642 0.545454552 0.651515152 0.637681162 0.647058822 0.575757582 0.590909092 0.606060612 0.651515152 0.705128212 0.545454552 0.666666672 0.679487182 0.644736842 0.6323529400.20.40.60.80 50 100 150 200Agreement of individual Turkers v # HITSNumber of HITs completedTurker Num HITs AgreementAHHIWLEVWDLLLA2T20OFLM0TLDDA3A5BBI7THNYAMA3CS8NKS64RQEKA189OYGOEA9SE1ATFFJKQ1CSJRSATZVHF3D2AH3UA2XIZGKT97P8OQA2LWJW1HUBILFOA38UZKXYE10BN7AAGC1LGVUHGFMA23V9FCSERQFF4AVWJPWUEE3U4QA3L4VHAI7BMHC5A13M4NKIOXIIYLA3FV4AOL7N1JBSA1XH9D9A4NHG4A2HNP1YL1IBFMUA2S44DG2KJIR39A15ENPH82MTLJFAIG9DJSRWGIY0A1RXZQVZXHYGB9A3UQJGXMMOUVJQAIQ1I6ODSIO56A1CC4Z6E608UHRA2VO7B7FXDHM47AH52LWLXYY5KTAB4WMS5WKULFPAUAZYGJWM6JLKA3J51GPLVUTEVTA2B032AIP4I6HHA2A4OZEX98MVZSA3R5BMMS0J77E4A2V2IA1P5MWP9HA2O7MKGTGDB2O5A1HUS55RZ2IO4QAG1TKYUKS97EVA3U6F7Q77IZXIOAVUVW19FTPZX6A3QKCDFWGN3QKLA3KEBTLZFJVKBGA2HNJBV33OV5EZA1IER46T87OQSHA1HAIFNOPYYZIJA1FU5C6US2L0J0ARXKQFSEKTYIDA3TY1HADH82YAPA3RN8UHUA2H8YDA3NPGGTTZXNZUL300 0.39726734200 0.68883878157 0.63821138151 0.721513141 0.73357106104 0.6644606292 0.6344322383 0.774 0.5742222267 0.7031924148 0.5663924837 0.7354570636 0.7094499336 0.72873931 0.725038429 0.6466666728 0.7078651722 0.7279151918 0.5538461515 0.7136929513 0.5794183413 0.671945712 0.6939252311 0.6963855411 0.5970149310 0.614634159 0.609137069 0.719072168 0.662198398 0.607427068 0.678756488 0.708661427 0.623036657 0.695187176 0.729658796 0.643478264 0.686746994 0.693251533 0.668789813 0.660493833 0.680124223 0.681528663 0.73 0.674050633 0.679245282 0.677631582 0.677631582 0.698717952 0.6710526300.20.40.60.80 75 150 225 300Agreement of Individuals on Yes NoNumber of HITs completedFigure 2: The agreement of individual Turkerswith the experts.
The most prolific Turker per-formed barely above chance, indicating randomclicking.
This suggests that users who contributemore tend to have lower quality.votes are counted equally.Weighting votes Not all Turkers are createdequal.
The quality of their works varies.
Fig-ure 2 shows the agreement of individual Turkerswith expert annotators, plotted against the num-ber of HITs they completed.
The figure showsthat their agreement varies considerably, and thatTurker who completed the most judgments wasamong the worst performing.To avoid letting careless annotators drag downresults, we experimented with weighted voting.We weighted votes in two ways:?
Votes were weighted by measuring agree-ment with experts on the 10 initial judgmentsmade.
This would be equivalent to givingTurkers a pretest on gold standard data andthen calibrating their contribution based onhow well they performed.?
Votes were weighted based on how often oneTurker agreed with the rest of the Turkersover the whole data set.
This does not re-quire any gold standard calibration data.
Itgoes beyond simple voting, because it looksat a Turker?s performance over the entire set,rather than on an item-by-item basis.Figure 1 shows that these weighting mechanismsperform similarly well.
For this task, derivingweights from agreement with other non-expertsis as effective as deriving weights from experts.Moreover, by weighting the votes of five Turkers,288E-all E0 E0.o E1 E1.o E2 E2.o E3 E3.o E4 E4.oE-allE0E0.oE1E1.oE2E2.oE3E3.oE4E4.oN.ew.1.0N.ew.1.1N.ew.1.2N.ew.1.3N.ew.1.4N.ew.1.5N.ew.1.6N.ew.1.7N.ew.1.8N.ew.1.9N.ew.2.0N.ew.2.1N.ew.2.2N.ew.2.3N.ew.2.4N.ew.2.5N.ew.2.6N.ew.2.7N.ew.2.8N.ew.2.9N.ew.3.0N.ew.3.1N.ew.3.2N.ew.3.3N.ew.3.4N.ew.3.5N.ew.3.6N.ew.3.7N.ew.3.8N.ew.3.9N.ew.4.0N.ew.4.1N.ew.4.2N.ew.4.3N.ew.4.4N.ew.4.5N.ew.4.6N.ew.4.7N.ew.4.8N.ew.4.9N.ew.5.0N.nw.1.0N.nw.1.1N.nw.1.2N.nw.1.3N.nw.1.4N.nw.1.5N.nw.1.6N.nw.1.7N.nw.1.8N.nw.1.9N.nw.2.0N.nw.2.1N.nw.2.2N.nw.2.3N.nw.2.4N.nw.2.5N.nw.2.6N.nw.2.7N.nw.2.8N.nw.2.9N.nw.3.0N.nw.3.1N.nw.3.2N.nw.3.3N.nw.3.4N.nw.3.5N.nw.3.6N.nw.3.7N.nw.3.8N.nw.3.9N.nw.4.0N.nw.4.1N.nw.4.2N.nw.4.3N.nw.4.4N.nw.4.5N.nw.4.6N.nw.4.7N.nw.4.8N.nw.4.9N.nw.5.0N.uw.1.0N.uw.1.1N.uw.1.2N.uw.1.3N.uw.1.4N.uw.1.5N.uw.1.6N.uw.1.7N.uw.1.8N.uw.1.9N.uw.2.0N.uw.2.1N.uw.2.2N.uw.2.3N.uw.2.4N.uw.2.5N.uw.2.6N.uw.2.7N.uw.2.8N.uw.2.9N.uw.3.0N.uw.3.1N.uw.3.2N.uw.3.3N.uw.3.4N.uw.3.5N.uw.3.6N.uw.3.7N.uw.3.8N.uw.3.9N.uw.4.0N.uw.4.1N.uw.4.2N.uw.4.3N.uw.4.4N.uw.4.5N.uw.4.6N.uw.4.7N.uw.4.8N.uw.4.9N.uw.5.0bleu1 0.957 0.989 0.879 0.993 0.893 0.989 0.864 0.989 0.857 0.9790.957 1 0.921 0.929 0.932 0.857 0.957 0.786 0.975 0.779 0.9640.989 0.921 1 0.854 0.996 0.893 0.979 0.871 0.975 0.889 0.9680.879 0.929 0.854 1 0.857 0.693 0.900 0.739 0.904 0.654 0.8860.993 0.932 0.996 0.857 1 0.896 0.982 0.861 0.979 0.896 0.9640.893 0.857 0.893 0.693 0.896 1 0.846 0.768 0.907 0.829 0.9180.989 0.957 0.979 0.900 0.982 0.846 1 0.868 0.975 0.839 0.9610.864 0.786 0.871 0.739 0.861 0.768 0.868 1 0.836 0.671 0.8570.989 0.975 0.975 0.904 0.979 0.907 0.975 0.836 1 0.846 0.9890.857 0.779 0.889 0.654 0.896 0.829 0.839 0.671 0.846 1 0.8180.979 0.964 0.968 0.886 0.964 0.918 0.961 0.857 0.989 0.818 10.800 0.746 0.793 0.757 0.811 0.711 0.821 0.707 0.793 0.689 0.7570.700 0.639 0.721 0.625 0.739 0.725 0.714 0.732 0.679 0.636 0.6750.789 0.725 0.811 0.814 0.807 0.689 0.789 0.679 0.789 0.664 0.7820.779 0.664 0.829 0.668 0.825 0.757 0.779 0.739 0.757 0.779 0.7640.743 0.671 0.789 0.714 0.782 0.671 0.761 0.786 0.732 0.725 0.7570.871 0.904 0.850 0.832 0.861 0.825 0.886 0.746 0.879 0.721 0.8860.875 0.800 0.893 0.800 0.889 0.800 0.871 0.893 0.857 0.711 0.8750.664 0.604 0.668 0.679 0.682 0.554 0.693 0.732 0.654 0.564 0.6430.900 0.893 0.904 0.871 0.911 0.771 0.936 0.854 0.900 0.811 0.8890.786 0.739 0.793 0.657 0.814 0.846 0.754 0.564 0.804 0.850 0.782 0.76790909090.804 0.743 0.829 0.771 0.825 0.786 0.793 0.704 0.825 0.736 0.821 0.08535920280.807 0.771 0.825 0.832 0.839 0.707 0.825 0.696 0.811 0.757 0.7860.668 0.582 0.696 0.621 0.693 0.671 0.646 0.654 0.679 0.636 0.7000.818 0.793 0.836 0.832 0.832 0.775 0.821 0.832 0.836 0.668 0.8540.786 0.700 0.800 0.739 0.804 0.707 0.796 0.761 0.768 0.643 0.7610.704 0.600 0.757 0.704 0.739 0.604 0.718 0.668 0.696 0.682 0.7040.696 0.675 0.725 0.721 0.732 0.689 0.714 0.643 0.707 0.661 0.7180.825 0.732 0.861 0.768 0.854 0.711 0.850 0.779 0.804 0.761 0.8040.821 0.729 0.836 0.764 0.829 0.711 0.804 0.779 0.818 0.718 0.8290.807 0.729 0.846 0.739 0.850 0.804 0.793 0.671 0.811 0.811 0.800 0.75460.836 0.746 0.861 0.739 0.868 0.796 0.821 0.718 0.836 0.829 0.821 0.06734808930.832 0.761 0.864 0.729 0.875 0.825 0.821 0.675 0.832 0.879 0.8180.775 0.704 0.811 0.729 0.814 0.764 0.775 0.704 0.786 0.764 0.7750.714 0.661 0.757 0.711 0.754 0.732 0.711 0.632 0.739 0.714 0.7430.796 0.704 0.832 0.682 0.839 0.789 0.800 0.754 0.771 0.764 0.7710.739 0.618 0.779 0.639 0.782 0.700 0.736 0.679 0.711 0.729 0.7110.721 0.636 0.771 0.707 0.761 0.664 0.729 0.757 0.729 0.711 0.7390.764 0.700 0.804 0.739 0.800 0.696 0.782 0.779 0.764 0.739 0.7750.743 0.693 0.779 0.743 0.782 0.743 0.739 0.625 0.764 0.736 0.7540.800 0.700 0.829 0.729 0.832 0.704 0.811 0.682 0.779 0.775 0.764 0.75590.743 0.654 0.796 0.704 0.793 0.707 0.754 0.664 0.736 0.761 0.732 0.05624079130.718 0.632 0.771 0.675 0.768 0.700 0.729 0.661 0.714 0.746 0.7180.843 0.775 0.875 0.757 0.886 0.814 0.843 0.700 0.843 0.875 0.8210.786 0.689 0.836 0.721 0.832 0.729 0.796 0.686 0.768 0.793 0.7640.714 0.611 0.768 0.661 0.764 0.675 0.725 0.732 0.696 0.711 0.7070.739 0.661 0.782 0.711 0.786 0.711 0.743 0.632 0.743 0.761 0.7290.775 0.707 0.804 0.743 0.807 0.682 0.800 0.654 0.761 0.757 0.7540.754 0.693 0.804 0.743 0.796 0.736 0.750 0.618 0.768 0.779 0.7710.796 0.711 0.850 0.746 0.836 0.746 0.796 0.750 0.800 0.800 0.8140.739 0.636 0.793 0.671 0.789 0.707 0.750 0.700 0.721 0.746 0.725 0.74680.05740485550.743 0.654 0.796 0.704 0.793 0.707 0.754 0.664 0.736 0.761 0.732 0.73127272730.546 0.504 0.571 0.546 0.564 0.539 0.568 0.425 0.543 0.536 0.5680.771 0.682 0.814 0.721 0.804 0.689 0.793 0.711 0.768 0.764 0.7640.900 0.875 0.889 0.814 0.882 0.889 0.868 0.904 0.914 0.679 0.9460.686 0.711 0.661 0.739 0.686 0.643 0.718 0.596 0.704 0.557 0.6890.736 0.721 0.768 0.789 0.754 0.700 0.750 0.718 0.757 0.629 0.7930.854 0.832 0.868 0.868 0.864 0.786 0.864 0.782 0.879 0.729 0.8750.871 0.825 0.864 0.789 0.861 0.721 0.889 0.782 0.861 0.775 0.8640.746 0.686 0.736 0.654 0.764 0.682 0.739 0.532 0.743 0.782 0.6860.807 0.746 0.796 0.775 0.814 0.696 0.811 0.679 0.779 0.643 0.7570.704 0.661 0.704 0.711 0.707 0.564 0.754 0.579 0.693 0.632 0.668 0.73626363640.821 0.761 0.857 0.804 0.854 0.754 0.839 0.725 0.825 0.771 0.814 0.10712478170.782 0.704 0.825 0.725 0.821 0.754 0.786 0.686 0.789 0.800 0.7860.829 0.746 0.839 0.786 0.850 0.689 0.839 0.696 0.807 0.761 0.7860.596 0.500 0.657 0.618 0.646 0.550 0.618 0.664 0.600 0.604 0.5960.800 0.768 0.839 0.857 0.821 0.718 0.811 0.743 0.825 0.707 0.8360.714 0.679 0.750 0.729 0.761 0.679 0.739 0.604 0.714 0.729 0.7040.689 0.629 0.682 0.604 0.707 0.704 0.679 0.489 0.686 0.671 0.6540.743 0.671 0.775 0.689 0.779 0.729 0.754 0.829 0.729 0.650 0.7430.811 0.732 0.854 0.796 0.839 0.679 0.839 0.775 0.789 0.732 0.8040.800 0.718 0.811 0.729 0.814 0.768 0.775 0.668 0.786 0.675 0.793 0.73962727270.846 0.754 0.850 0.750 0.861 0.779 0.829 0.782 0.832 0.725 0.821 0.07943549220.739 0.646 0.775 0.679 0.779 0.675 0.757 0.679 0.721 0.736 0.7180.700 0.607 0.743 0.661 0.739 0.675 0.700 0.718 0.707 0.696 0.7110.764 0.671 0.782 0.704 0.789 0.714 0.761 0.746 0.739 0.636 0.7430.864 0.839 0.879 0.864 0.889 0.779 0.882 0.746 0.864 0.775 0.8500.768 0.686 0.793 0.732 0.800 0.661 0.793 0.779 0.746 0.704 0.7430.757 0.664 0.782 0.736 0.771 0.639 0.775 0.764 0.739 0.625 0.7540.836 0.757 0.846 0.789 0.854 0.707 0.846 0.768 0.825 0.757 0.8110.729 0.707 0.754 0.761 0.757 0.704 0.736 0.564 0.754 0.718 0.7500.832 0.761 0.861 0.768 0.868 0.779 0.836 0.725 0.836 0.832 0.818 0.75933636360.825 0.711 0.879 0.725 0.864 0.775 0.814 0.793 0.811 0.807 0.829 0.06550793460.718 0.636 0.768 0.711 0.761 0.664 0.739 0.714 0.714 0.689 0.7180.786 0.707 0.836 0.750 0.821 0.704 0.804 0.757 0.775 0.761 0.7960.764 0.664 0.818 0.693 0.807 0.714 0.775 0.761 0.750 0.761 0.7680.693 0.611 0.750 0.704 0.739 0.632 0.707 0.639 0.693 0.707 0.7000.789 0.729 0.825 0.779 0.832 0.714 0.807 0.668 0.782 0.779 0.7640.725 0.625 0.786 0.689 0.771 0.664 0.736 0.721 0.711 0.725 0.7320.814 0.732 0.861 0.746 0.854 0.761 0.825 0.739 0.811 0.821 0.8140.768 0.675 0.818 0.700 0.814 0.739 0.779 0.725 0.761 0.775 0.7610.743 0.643 0.800 0.650 0.793 0.736 0.754 0.704 0.729 0.775 0.739 0.75021818180.05663538330.739 0.646 0.793 0.682 0.789 0.736 0.739 0.682 0.739 0.761 0.739 0.73136363640.811 0.711 0.846 0.746 0.839 0.714 0.821 0.739 0.800 0.775 0.7960.729 0.679 0.736 0.761 0.725 0.625 0.750 0.657 0.711 0.507 0.7210.739 0.643 0.761 0.700 0.757 0.586 0.775 0.846 0.707 0.625 0.7140.879 0.832 0.861 0.786 0.879 0.854 0.839 0.711 0.879 0.739 0.8570.682 0.625 0.725 0.700 0.711 0.668 0.679 0.693 0.714 0.661 0.7320.711 0.650 0.757 0.671 0.732 0.764 0.675 0.604 0.736 0.679 0.7790.854 0.754 0.875 0.796 0.864 0.739 0.839 0.879 0.836 0.679 0.8500.800 0.736 0.836 0.782 0.821 0.746 0.796 0.707 0.829 0.761 0.8140.886 0.829 0.907 0.804 0.893 0.775 0.893 0.832 0.893 0.850 0.8960.764 0.696 0.800 0.718 0.789 0.782 0.732 0.750 0.779 0.689 0.814 0.76254545450.825 0.750 0.868 0.757 0.861 0.804 0.811 0.682 0.836 0.832 0.832 0.07847697450.879 0.804 0.911 0.793 0.907 0.793 0.871 0.764 0.857 0.839 0.8710.771 0.689 0.818 0.743 0.814 0.707 0.786 0.661 0.768 0.779 0.7570.804 0.686 0.854 0.693 0.843 0.764 0.800 0.814 0.779 0.757 0.8000.761 0.671 0.807 0.704 0.804 0.739 0.764 0.704 0.764 0.768 0.7610.764 0.711 0.796 0.768 0.789 0.721 0.754 0.739 0.789 0.718 0.8040.879 0.804 0.907 0.800 0.914 0.811 0.868 0.736 0.875 0.879 0.8610.804 0.711 0.846 0.786 0.832 0.646 0.821 0.750 0.786 0.761 0.7930.843 0.768 0.886 0.821 0.879 0.721 0.854 0.768 0.836 0.821 0.8360.714 0.625 0.761 0.718 0.754 0.643 0.711 0.704 0.718 0.696 0.725 0.78460.832 0.789 0.854 0.793 0.861 0.804 0.814 0.757 0.850 0.804 0.854 0.06392989280.807 0.775 0.818 0.829 0.825 0.668 0.832 0.661 0.807 0.757 0.7960.729 0.675 0.754 0.761 0.757 0.632 0.761 0.650 0.721 0.650 0.7110.746 0.625 0.796 0.679 0.786 0.671 0.746 0.782 0.714 0.686 0.7390.771 0.650 0.811 0.679 0.800 0.704 0.761 0.800 0.739 0.682 0.7710.886 0.825 0.914 0.825 0.918 0.829 0.882 0.757 0.893 0.864 0.8790.779 0.696 0.821 0.761 0.814 0.679 0.793 0.793 0.775 0.746 0.7820.857 0.775 0.889 0.761 0.896 0.821 0.846 0.718 0.854 0.868 0.8390.875 0.775 0.918 0.779 0.911 0.800 0.864 0.768 0.861 0.861 0.8640.754 0.675 0.789 0.704 0.796 0.729 0.757 0.704 0.757 0.761 0.746 0.78190.789 0.664 0.839 0.671 0.829 0.768 0.771 0.775 0.775 0.771 0.793 0.06987182890.829 0.736 0.875 0.761 0.864 0.746 0.846 0.818 0.811 0.779 0.8210.832 0.729 0.861 0.771 0.850 0.725 0.821 0.782 0.825 0.750 0.8320.871 0.771 0.914 0.782 0.907 0.775 0.871 0.775 0.850 0.850 0.8540.850 0.754 0.896 0.754 0.889 0.814 0.839 0.739 0.846 0.854 0.8460.725 0.621 0.775 0.721 0.761 0.618 0.736 0.686 0.714 0.693 0.7210.832 0.743 0.879 0.757 0.871 0.771 0.843 0.768 0.825 0.825 0.8250.796 0.693 0.843 0.718 0.832 0.711 0.811 0.768 0.775 0.775 0.7890.775 0.671 0.804 0.707 0.811 0.704 0.764 0.629 0.757 0.764 0.7430.818 0.707 0.850 0.725 0.843 0.714 0.818 0.768 0.796 0.775 0.807 0.78528181820.06220270680.818 0.725 0.864 0.732 0.857 0.757 0.829 0.750 0.804 0.818 0.811 0.79681818180.321 0.328 0.292 0.353 0.263 0.149 0.374 0.353 0.306 0.135 0.349Choose1 2 3 4 5unweighted 0.7625454545 0.7846 0.7819 0.7852818182 0.7968181818non-expert weights0.7362636364 0.7625454545 0.7593363636 0.7502181818 0.7313636364expert weights 0.7679090909 0.7546 0.7559 0.7468 0.73136363641 2 3 4 5unweightedexpertweightsnon-expertexpert-expertBLEU0.7625454545 0.7846 0.7819 0.7852818182 0.79681818180.7679090909 0.7546 0.7559 0.7468 0.73136363640.7362636364 0.7625454545 0.7593363636 0.7502181818 0.73136363640.778 0.778 0.778 0.778 0.7780.293 0.293 0.293 0.293 0.2930.20.30.40.50.60.70.80.91 2 3 4 5Correlation with Expert ranking of systemsUnweighted Non-expert weightsExpert-Expert BLEU00.2250.450.6750.9unweightedexpertweightsnon-expertexpert-expertBLEUSpearman?s correlationFigure 3: Correlation with experts?
ranking of sys-tems.
All of the different ways of combining thenon-expert judgments perform at the upper boundof expert-expert correlation.
All correlate morestrongly than Bleu.we are able to achieve the same rate of agreementwith experts as they achieve with each other.Correlation when ranking systems In addi-tion to measuring agreement with experts atthe sentence-level, we also compare non-expertsystem-level rankings with experts.
FollowingCallison-Burch et al (2008), we assigned a scoreto each of the 11 MT systems based on how of-ten its translations were judged to be better than orequal to any other system.
These scores were usedto rank systems and we measured Spearman?s ?against the system-level ranking produced by ex-perts.Figure 3 shows how well the non-expert rank-ings correlate with expert rankings.
An up-per bound is indicated by the expert-expert bar.This was created using a five-fold cross valida-tion where we used 20% of the expert judgmentsto rank the systems and measured the correlationagainst the rankings produced by the other 80%of the judgments.
This gave a ?
of 0.78.
All waysof combining the non-expert judgments resulted innearly identical correlation, and all produced cor-relation within the range of with what we wouldexperts to.The rankings produced using Mechanical Turkhad a much stronger correlation with the WMT08expert rankings than the Blue score did.
It shouldbe noted that the WMT08 data set does not havemultiple reference translations.
If multiple ref-erences were used that Bleu would likely havestronger correlation.
However, it is clear that thecost of hiring professional translators to createmultiple references for the 2000 sentence test setwould be much greater than the $10 cost of col-lecting manual judgments on Mechanical Turk.5 Feasibility of more complex evaluationsIn this section we report on a number of cre-ative uses of Mechanical Turk to do more so-phisticated tasks.
We give evidence that Turkerscan create high quality translations for some lan-guages, which would make creating multiple ref-erence translations for Bleu less costly than usingprofessional translators.
We report on experimentsevaluating translation quality with HTER and withreading comprehension tests.5.1 Creating multiple reference translationsIn addition to evaluating machine translation qual-ity, we also investigated the possibility of usingMechanical Turk to create additional referencetranslations for use with automatic metrics likeBleu.
Before trying this, we were skeptical thatTurkers would have sufficient language skills toproduce translations.
Our translation HIT had thefollowing instructions:Translate these sentences Your task is to translate 10 sen-tences into English.
Please make sure that your Englishtranslation:?
Is faithful to the original in both meaning and style?
Is grammatical, fluent, and natural-sounding English?
Does not add or delete information from the originaltext?
Does not contain any spelling errorsWhen creating your translation, please:?
Do not use any machine translation systems?
You may look up a word on wordreference.com if youdo not know its translationAfterwards, we?ll ask you a few quick questions about yourlanguage abilities.We solicited translations for 50 sentences inFrench, German, Spanish, Chinese and Urdu, anddesigned the HIT so that five Turkers would trans-late each sentence.Filtering machine translation Upon inspectingthe Turker?s translations it became clear that manyhad ignored the instructions, and had simply cut-and-paste machine translation rather then translat-ing the text themselves.
We therefore set up a sec-ond HIT to filter these out.
After receiving the289Topline MTurk MT Topline MTurk MTSpanishGermanFrenchChineseUrdu0.54 0.50 0.41 Spanish STDDEV 0.07 0.03 0.020.54 0.48 0.33 German STDDEV 0.07 0.02 0.020.54 0.33 0.25 French STDDEV 0.07 0.02 0.010.54 0.52 0.24 Chinese STDDEV 0.07 0.02 0.000.32 0.21 0.14 Urdu STDDEV 0.02 0.02 0.0100.100.200.300.400.500.60Spanish German French ChineseBleu scores of professional translators, Mechanical Turk, and MT1 LDC translator v. other LDC translators Mechanical Turk v. other LDC MT v. other LDC00.100.200.300.400.500.60UrduFigure 4: Bleu scores quantifying the quality of Turkers?
translations.
The chart shows the average Bleuscore when one LDC translator is compared against the other 10 translators (or the other 2 translators inthe case of Urdu).
This gives an upper bound on the expected quality.
The Turkers?
translation qualityfalls within a standard deviation of LDC translators for Spanish, German and Chinese.
For all languages,Turkers produce significantly better translations than an online machine translation system.translations, we had a second group of Turkersclean the results.Detect machine translation Please use two online machinetranslation systems to translate the text into English, and thencopy-and-paste the translations into the boxes below.
Finally,look at a list of translations below and click on the ones thatlook like they came from the online translation services.We automatically excluded Turkers whose transla-tions were flagged 30% of the time or more.Quality of Turkers?
translations Our 50 sen-tence test sets were selected so that we could com-pare the translations created by Turkers to transla-tions commissioned by the Linguistics Data Con-sortium.
For the Chinese, French, Spanish, andGerman translations we used the the Multiple-Translation Chinese Corpus.3This corpus has11 reference human translations for each Chinesesource sentence.
We had bilingual graduate stu-dents translate the first 50 English sentences ofthat corpus into French, German and Spanish, sothat we could re-use the multiple English referencetranslations.
The Urdu sentences were taken fromthe NIST MT Eval 2008 Urdu-English Test Set4which includes three distinct English translationsfor every Urdu source sentence.Figure 4 shows the Turker?s translation qualityin terms of the Bleu metric.
To establish an upperbound on expected quality, we determined what3LDC catalog number LDC2002T014LDC catalog number LDC2009E11the Bleu score would be for a professional trans-lator when measured against other professionals.We calculated a Bleu score for each of the 11LDC translators using the other 10 translators asthe reference set.
The average Bleu score forLDC2002T01 was 0.54, with a standard deviationof 0.07.
The average Bleu for the Urdu test set islower because it has fewer reference translations.To measure the Turkers?
translation quality, werandomly selected translations of each sentencefrom Turkers who passed the Detect MT HIT, andcompared them against the same sets of 10 ref-erence translations that the LDC translators werecompared against.
We randomly sampled theTurkers 10 times, and calculated averages andstandard deviations for each source language.
Fig-ure 4 the Bleu scores for the Turkers?
translationsof Spanish, German and Chinese are within therange of the LDC translators.
For all languages,the quality is significantly higher than an onlinemachine translation system.
We used Yahoo?s Ba-belfish for Spanish, German, French and Chinese,5was likely and Babylon for Urdu.Demographics We collected demographic in-formation about the Turkers who completed thetranslation task.
We asked how long they had spo-ken the source language, how long they had spo-5We also compared against Google Translate, but ex-cluded the results since its average Bleu score was better thanthe LDC translators, likely because the test data was used totrain Google?s statistical system.290SpanishNative lang English (7 people), Spanish (2), English-Spanish bilingual, Portuguese English, HindiCountry USA (7 people), Mexico (3), Brazil, USA (2)Spanish level 30+ years (2 people), 15 years (2), 6 years, 2 years (2), whole life (4) 18 years, 4 yearsEnglish level 15 years (3), whole life (9) whole life , 15 yearsGermanNative lang German (3), Turkish (2), Italian, Danish, English, Norwegian, Hindi Marathi, Tamil, Hindi, EnglishCountry Germany (3), USA, Italy, China, Denmark, Turkey, Norway, India USA (2), India (2)German level 20 years (2), 10 years (3), 5 years (2), 2 years, whole life (3) 10 years, 1 year (2)English level 20+ years (4), 10-20 years (5) whole life whole life (2), 15-20 years (2)FrenchNative lang English (9 people), Portuguese, Hindi English (2)Country USA (6), Israel, Singapore, UK, Brazil, India USA (2)French level 20+ years (4 people), 8-12 years (4), 5 years (2), 2 years 10 years, 1 years, 6 yearsEnglish level whole life (9), 20 years, 15 years whole life (2),ChineseNative lang Hindi (2) English (3) Hindi, Marathi, TamilCountry India (2) India (3), USA (3)Chinese level 2 years, 1 year 3 years, 2 years, noneEnglish level 18 years, 20+ years 16 years, whole life (2)UrduNative lang Urdu (6 people) Tamil (2), Hindi, TeluguCountry Pakistan (3), Bahrain, India, Saudi Arabia India (4)Urdu level whole life (6 people) 2 years, 1 year, never (2)English level 20+ years (5), 15 years (2), 10 years 10+ years (5), 5 yearsTable 1: Self-reported demographic information from Turkers who completed the translation HIT.
Thestatistics on the left are for people who appeared to do the task honestly.
The statistics on the right arefor people who appeared to be using MT (marked as using it 20% or more in the Detect MT HIT).ken English, what their native language was, andwhere they lived.
Table 1 gives their replies.Cost and speed We paid Turkers $0.10 to trans-late each sentence, and $0.006 to detect whether asentence was machine translated.
The cost is lowenough that we could create a multiple referenceset quite cheaply; it would cost less than $1,000 tocreate 4 reference translations for 2000 sentences.The time it took for the 250 translations to becompleted for each language varied.
It took lessthan 4 hours for Spanish, 20 hours for French, 22.5hours for German, 2 days for Chinese, and nearly4 days for Urdu.5.2 HTERHuman-mediated translation edit rate (HTER)is the official evaluation metric of the DARPAGALE program.
The evaluation is conducted an-nually by the Linguistics Data Consortium, andit is used to determine whether the teams partic-ipating the program have met that year?s bench-marks.
These evaluations are used as a ?Go / NoGo?
determinant of whether teams will continueto receive funding.
Thus, each team have a strongincentive to get as good a result as possible underthe metric.Each of the three GALE teams encompassesmultiple sites and each has a collection of ma-chine translation systems.
A general strategy em-ployed by all teams is to perform system combi-nation over these systems to produce a synthetictranslation that is better than the sum of its parts(Matusov et al, 2006; Rosti et al, 2007).
The con-tribution of each component system is weightedby the expectation that it will produce good out-put.
To our knowledge, none of the teams performtheir own HTER evaluations in order to set theseweights.We evaluated the feasibility of using Mechan-ical Turk to perform HTER.
We simplified theofficial GALE post-editing guidelines (NIST andLDC, 2007).
We provided these instructions:Edit Machine Translation Your task is to edit the machinetranslation making as few changes as possible so that itmatches the meaning of the human translation and is goodEnglish.
Please follow these guidelines:?
Change the machine translation so that it has the samemeaning as the human translation.?
Make the machine translation into intelligible English.?
Use as few edits as possible.?
Do not insert or delete punctuation simply to followtraditional rules about what is ?proper.??
Please do not copy-and-paste the human translationinto the machine translation.291Number of editorsSystem 0 1 2 3 4 5google.fr-en .44 .29 .24 .22 .20 .19google.de-en .48 .34 .30 .28 .25 .24rbmt5.de-en .53 .41 .33 .28 .27 .25geneva.de-en .65 .56 .50 .48 .45 .45tromble.de-en .77 .75 .74 .73 .71 .70Table 2: HTER scores for five MT systems.
Theedit rate decreases as the number of editors in-creases from zero (where HTER is simply the TERscore between the MT output and the referencetranslation) and five.We displayed 10 sentences from a news article.
Inone column was the reference English translation,in the other column were text boxes containingthe MT output to be edited.
To minimize the editrate, we collected edits from five different Turkersfor every machine translated segment.
We verifiedthese with a second HIT were we prompted Turk-ers to:Judge edited translations First, read the reference humantranslation.
After that judge the edited machine translationusing two criteria:?
Does the edited translation have the same meaning asthe reference human translation??
Is it acceptable English?
Some small errors are OK, solong as its still understandable.For the final score, we choose the edited segmentwhich passed the criteria and which minimized theedit distance to the unedited machine translationoutput.
If none of the five edits was deemed to beacceptable, then we used the edit distance betweenthe MT and the reference.Setup We evaluated five machine translationsystems using HTER.
These systems were se-lected from WMT09 (Callison-Burch et al, 2009).We wanted a spread in quality, so we took the toptwo and bottom two systems from the German-English task, and the top system from the French-English task (which significantly outperformedeverything else).
Based on the results of theWMT09 evaluation we would expect the see thefollowing ranking from the least edits to the mostedits: google.fr-en, google.de-en, rbmt5.de-en,geneva.de-en and tromble.de-en.Results Table 2 gives the HTER scores for thefive systems.
Their ranking is as predicted, indi-cating that the editing is working as expected.
Thetable reports averaged scores when the five anno-tators are subsampled.
This gives a sense of howmuch each additional editor is able to minimizethe score for each system.
The difference betweenthe TER score with zero editors, and the HTERfive editors is greatest for the rmbt5 system, whichhas a delta of .29 and is smallest for jhu-tromblewith .07.5.3 Reading comprehensionOne interesting technique for evaluating machinetranslation quality is through reading comprehen-sion questions about automatically translated text.The quality of machine translation systems can bequantified based on how many questions are an-swered correctly.Jones et al (2005) evaluated translation qualityusing a reading comprehension test the DefenseLanguage Proficiency Test (DLPT), which is ad-ministered to military translators.
The DLPT con-tains a collection of foreign articles of varying lev-els of difficulties, and a set of short answer ques-tions.
Jones et alused the Arabic DLPT to do astudy of machine translation quality, by automat-ically translating the Arabic documents into En-glish and seeing how many human subjects couldsuccessfully pass the exam.The advantage of this type of evaluation is thatthe results have a natural interpretation.
They indi-cate how understandable the output of a machinetranslation system is better than Bleu does, andbetter than other manual evaluation like the rela-tive ranking.
Despite this advantage, evaluatingMT through reading comprehension hasn?t caughton, due to the difficulty of administering it and dueto the fact that the DLPT or similar tests are notpublicly available.We conducted a reading comprehension evalua-tion using Mechanical Turk.
Instead of simply ad-ministering the test on Mechanical Turk, we usedit for all aspects from test creation to answer grad-ing.
Our procedure was as follows:Test creation We posted human translations offoreign news articles, and ask Tukers to write threequestions and provide sample answers.
We gavesimple instructions on what qualifies as a goodreading comprehension question.Reading comprehension test Please read the short news-paper article, and then write three reading comprehensionquestions about it, giving sample answers for each of yourquestions.
Good reading comprehension questions:292?
Ask about why something happened or why someonedid something.?
Ask about relationships between people or things.?
Should be answerable in a few words.Poor reading comprehension questions:?
Ask about numbers or dates.?
Only require a yes/no answer.Question selection We posted the questions foreach article back to Mechanical Turk, and askedother Turkers to vote on whether each questionwas a good and to indicate if it was redundant withany other questions in the set.
We sorted questionsto maximize the votes and minimized redundan-cies using a simple perl script, which discardedquestions below a threshold, and eliminated all re-dundancies.Taking the test We posted machine translatedversions of the foreign articles along with thequestions, and had Turkers answer them.
We en-sured that no one would see multiple translationsof the same article.Answer questions about a machine translated text You willanswer questions about an article that has been automat-ically translated from another language into English.
Thetranslation contains many errors, but the goal is to see howunderstandable it is.
Please do your best to guess at the rightanswers to the questions.
Please:?
Read through the automatically translated article.?
Answer the questions listed below, using just a fewwords.?
Give your best guess at the answers, even if the trans-lation is hard to understand.?
Don?t use any other information to answer the ques-tions.Grading the answers We aggregated theanswers and used Mechanical Turk to gradethem.
We showed the human translation of thearticle, one question, the sample answer, anddisplayed all answers to it.
After the Turkersgraded the answers, we calculated the percentageof questions that were answered correctly for eachsystem.Turkers created 90 questions for 10 articles, whichwere subsequently filtered down to 47 good ques-tions, ranging from 3?6 questions per article.
25Turkers answered questions about each translatedarticle.
To avoid them answering the questionsmultiple times, we randomly selected which sys-tem?s translation was shown to them.
Each sys-tem?s translation was displayed an average of 5System % Correct Answersreference 0.94google.fr-en 0.85google.de-en 0.80rbmt5.de-en 0.77geneva.de-en 0.63jhu-tromble.de-en 0.50Table 3: The results of evaluating the MT outputusing a reading comprehension testtimes per article.
As a control, we had three Turk-ers answer the reading comprehension questionsusing the reference translation.Table 3 gives the percent of questions that werecorrectly answered using each of the different sys-tems?
outputs and using the reference translation.The ranking is exactly what we would expect,based on the HTER scores and on the human eval-uation of the systems in WMT09.
This againhelps to validate that the reading comprehensionmethodology.
The scores are more interpretablethan Blue scores and than the WMT09 relativerankings, since it gives an indication of how un-derstandable the MT output is.Appendix A shows some sample questions andanswers for an article.6 ConclusionsMechanical Turk is an inexpensive way of gather-ing human judgments and annotations for a widevariety of tasks.
In this paper we demonstratethat it is feasible to perform manual evaluationsof machine translation quality using the web ser-vice.
The low cost of the non-expert labor foundon Mechanical Turk is cheap enough to collect re-dundant annotations, which can be utilized to en-sure translation quality.
By combining the judg-ments of many non-experts we are able to achievethe equivalent quality of experts.The suggests that manual evaluation of trans-lation quality could be straightforwardly done tovalidate performance improvements reported inconference papers, or even for mundane taskslike tracking incremental system updates.
Thischallenges the conventional wisdom which haslong held that automatic metrics must be usedsince manual evaluation is too costly and time-consuming.We have shown that Mechanical Turk can beused creatively to produce quite interesting things.293We showed how a reading comprehension testcould be created, administered, and graded, withonly very minimal intervention.We believe that it is feasible to use MechanicalTurk for a wide variety of other machine translatedtasks like creating word alignments for sentencepairs, verifying the accuracy of document- andsentence-alignments, performing non-simulatedactive learning experiments for statistical machinetranslation, even collecting training data for lowresource languages like Urdu.The cost of using Mechanical Turk is lowenough that we might consider attemptingquixotic things like human-in-the-loop minimumerror rate training (Zaidan and Callison-Burch,2009), or doubling the amount of training dataavailable for Urdu.AcknowledgmentsThis research was supported by the EuroMatrix-Plus project funded by the European Commission,and by the US National Science Foundation undergrant IIS-0713448.
The views and findings are theauthor?s alone.A Example reading comprehensionquestionsActress Heather Locklear arrested for driving under theinfluence of drugsThe actress Heather Locklear, Amanda on the popular se-ries Melrose Place, was arrested this weekend in Santa Bar-bara (California) after driving under the influence of drugs.
Awitness saw her performing inappropriate maneuvers whiletrying to take her car out of a parking space in Montecito, asrevealed to People magazine by a spokesman for the Califor-nian Highway Police.
The witness stated that around 4.30pmMs.
Locklear ?hit the accelerator very roughly, making ex-cessive noise and trying to take the car out from the park-ing space with abrupt back and forth maneuvers.
While re-versing, she passed several times in front of his sunglasses.
?Shortly after, the witness, who at first, apparently had not rec-ognized the actress, saw Ms. Locklear stopping in a nearbystreet and leaving the vehicle.It was this person who alerted the emergency services, be-cause ?he was concerned about Ms. Locklear?s life.?
Whenthe patrol arrived, the police found the actress sitting insideher car, which was partially blocking the road.
?She seemedconfused,?
so the policemen took her to a specialized centrefor drugs and alcohol and submitted her a test.
According to aspokesman for the police, the actress was cooperative and ex-cessive alcohol was ruled out from the beginning, even if ?asthe officers initially observed, we believe Ms. Locklear wasunder the influences drugs.?
Ms. Locklear was arrested undersuspicion of driving under the influence of some - unspecifiedsubstance, and imprisoned in the local jail at 7.00pm, to be re-leased some hours later.
Two months ago, Ms. Locklear wasreleased from a specialist clinic in Arizona where she wastreated after an episode of anxiety and depression.4 questions were selected?
Why did the bystander call emergency services?He was concerned for Ms. Locklear?s life.?
Why was Heather Locklear arrested in Santa Barbara?Because she was driving under the influence of drugs?
Where did the witness see her acting abnormally?Pulling out of parking in Montecito?
Where was Ms. Locklear two months ago?She was at a specialist clinic in Arizona.5 questions were excluded as being redundant?
What was Heather Locklear arrested for?Driving under the influence of drugs?
Where was she taken for testing?A specialized centre for drugs and alcohol?
Why was Heather Locklear arrested?She was arested on suspicion of driving under the in-fluence of drugs.?
Why did the policemen lead her to a specialized centrefor drugs and alcoholBecause she seemed confused.?
For what was she cured for two months ago?She was cured for anxiety and depression.Answers to Where was Ms. Locklear two months ago?that were judged to be correct:Arizona hospital for treatment of depression; at a treat-mend clinic in Arizona; in the Arizona clinic being treatedfor nervous breakdown; a clinic in Arizona; Arizona, un-der treatment for depression; She was a patient in a clinicin Arizona undergoing treatment for anxiety and depression;In an Arizona mental health facility ; A clinic in Arizona.
;In a clinic being treated for anxiety and depression.
; at anArizona clinicThese answers were judged to be incorrect: Locklearwas retired in Arizona; Arizona; Arizona; in Arizona;Ms.Locklaer were laid off after a treatment out of the clinicin Arizona.ReferencesBogdan Babych and Anthony Hartley.
2004.
Extend-ing the Bleu MT evaluation method with frequencyweightings.
In Proceedings of ACL.Chris Callison-Burch, Miles Osborne, and PhilippKoehn.
2006.
Re-evaluating the role of Bleu in ma-chine translation research.
In Proceedings of EACL.Chris Callison-Burch, Cameron Fordyce, PhilippKoehn, Christof Monz, and Josh Schroeder.
2008.Further meta-evaluation of machine translation.
InProceedings of the Third Workshop on StatisticalMachine Translation (WMT08).Chris Callison-Burch, Philipp Koehn, Christof Monz,and Josh Schroeder.
2009.
Findings of the 2009Workshop on Statistical Machine Translation.
InProceedings of the Fourth Workshop on StatisticalMachine Translation (WMT09), March.David Chiang, Steve DeNeefe, Yee Seng Chan, andHwee Tou Ng.
2008.
Decomposability of trans-lation metrics for improved evaluation and efficientalgorithms.
In Proceedings of EMNLP.294Douglas Jones, Wade Shen, Neil Granoien, MarthaHerzog, and Clifford Weinstein.
2005.
Measuringtranslation quality by testing English speakers witha new defense language proficiency test for Arabic.In Proceedings of the 2005 International Conferenceon Intelligence Analysis.LDC.
2005.
Linguistic data annotation specification:Assessment of fluency and adequacy in translations.Revision 1.5.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing consensus translation for multiplemachine translation systems using enhanced hypoth-esis alignment.
In Proceedings of EACL.NIST and LDC.
2007.
Post editing guidelines forGALE machine translation evaluation.
Guidelinesdeveloped by the National Institute of Standards andTechnology (NIST), and the Linguistic Data Consor-tium (LDC).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: A method for automaticevaluation of machine translation.
In Proceedingsof ACL.Michael Paul.
2006.
Overview of the IWSLT 2006evaluation campaign.
In Proceedings of Interna-tional Workshop on Spoken Language Translation.Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,Spyros Matsoukas, Richard Schwartz, and BonnieDorr.
2007.
Combining outputs from multiplemachine translation systems.
In Proceedings ofHLT/NAACL.Markus Schulze.
2003.
A new monotonic and clone-independent single-winner election method.
VotingMatters, (17), October.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of AMTA.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast - but is itgood?
Evaluating non-expert annotations for naturallanguage tasks.
In Proceedings of EMNLP.Omar F. Zaidan and Chris Callison-Burch.
2009.
Fea-sibility of human-in-the-loop minimum error ratetraining.
In Proceedings of EMNLP.295
