Rapid Prototyping of Robust Language Understanding Modulesfor Spoken Dialogue Systems?Yuichiro Fukubayashi, ?Kazunori Komatani, ?Mikio Nakano,?Kotaro Funakoshi, ?Hiroshi Tsujino, ?Tetsuya Ogata, ?Hiroshi G. Okuno?Graduate School of Informatics, Kyoto UniversityYoshida-Hommachi, Sakyo, Kyoto606-8501, Japan{fukubaya,komatani}@kuis.kyoto-u.ac.jp{ogata,okuno}@kuis.kyoto-u.ac.jp?Honda Research Institute Japan Co., Ltd.8-1 Honcho, Wako, Saitama351-0188, Japannakano@jp.honda-ri.com{funakoshi,tsujino}@jp.honda-ri.comAbstractLanguage understanding (LU) modules forspoken dialogue systems in the early phasesof their development need to be (i) easyto construct and (ii) robust against vari-ous expressions.
Conventional methods ofLU are not suitable for new domains, be-cause they take a great deal of effort tomake rules or transcribe and annotate a suf-ficient corpus for training.
In our method,the weightings of the Weighted Finite StateTransducer (WFST) are designed on twolevels and simpler than those for conven-tional WFST-based methods.
Therefore,our method needs much fewer training data,which enables rapid prototyping of LU mod-ules.
We evaluated our method in two dif-ferent domains.
The results revealed that ourmethod outperformed baseline methods withless than one hundred utterances as trainingdata, which can be reasonably prepared fornew domains.
This shows that our methodis appropriate for rapid prototyping of LUmodules.1 IntroductionThe language understanding (LU) of spoken dia-logue systems in the early phases of their devel-opment should be trained with a small amount ofdata in their construction.
This is because largeamounts of annotated data are not available in theearly phases.
It takes a great deal of effort and timeto transcribe and provide correct LU results to aFigure 1: Relationship between our method and con-ventional methodslarge amount of data.
The LU should also be robust,i.e., it should be accurate even if some automaticspeech recognition (ASR) errors are contained in itsinput.
A robust LU module is also helpful when col-lecting dialogue data for the system because it sup-presses incorrect LU and unwanted behaviors.
Wedeveloped a method of rapidly prototyping LU mod-ules that is easy to construct and robust against var-ious expressions.
It makes LU modules in the earlyphases easier to develop.Several methods of implementing an LU mod-ule in spoken dialogue systems have been proposed.Using grammar-based ASR is one of the simplest.Although its ASR output can easily be transformedinto concepts based on grammar rules, complicatedgrammars are required to understand the user?s ut-terances in various expressions.
It takes a great dealof effort to the system developer.
Extracting con-210Figure 2: Example of WFST for LUcepts from user utterances by keyword spotting orheuristic rules has also been proposed (Seneff, 1992)where utterances can be transformed into conceptswithout major modifications to the rules.
However,numerous complicated rules similarly need to bemanually prepared.
Unfortunately, neither methodis robust against ASR errors.To cope with these problems, corpus-based (Su-doh and Tsukada, 2005; He and Young, 2005) andWeighted Finite State Transducer (WFST)-basedmethods (Potamianos and Kuo, 2000; Wutiwi-watchai and Furui, 2004) have been proposed as LUmodules for spoken dialogue systems.
Since thesemethods extract concepts using stochastic analy-sis, they do not need numerous complicated rules.These, however, require a great deal of training datato implement the module and are not suitable forconstructing new domains.Here, we present a new WFST-based LU modulethat has two main features.1.
A statistical language model (SLM) for ASRand a WFST for parsing that are automaticallygenerated from the domain grammar descrip-tion.2.
Since the weighting for the WFST is simplerthan that in conventional methods, it requiresfewer training data than conventional weight-ing schemes.Our method accomplishes robust LU with less ef-fort using SLM-based ASR and WFST parsing.
Fig-ure 1 outlines the relationships between our methodand conventional schemes.
Since rule- or grammar-based approaches do not require a large amount ofdata, they take less effort than stochastic techniques.However, they are not robust against ASR errors.Stochastic approaches, on the contrary, take a greatdeal of effort to collect data but are robust againstASR errors.
Our method is an intermediate approachthat lies between these.
That is, it is more robust thanrule- or grammar-based approaches and takes lesseffort than stochastic techniques.
This characteristicmakes it easier to rapidly prototype LU modules fora new domain and helps development in the earlyphases.2 Related Work and WFST-basedApproachA Finite State Transducer (FST)-based LU is ex-plained here, which accepts ASR output as its in-put.
Figure 2 shows an example of the FST for avideo recording reservation domain.
The input, ?,means that a transition with no input is permitted atthe state transition.
In this example, the LU mod-ule returns the concept [month=2, day=22] for theutterance ?It is February twenty second please?.Here, a FILLER transition in which any word is ac-cepted is appropriately allowed between phrases.
InFigure 2, ?F?
represents 0 or more FILLER tran-sitions.
A FILLER transition from the start to theend is inserted to reject unreliable utterances.
ThisFILLER transition enables us to ignore unnecessarywords listed in the example utterances in Table 1.The FILLER transition helps to suppress the inser-tion of incorrect concepts into LU results.However, many output sequences are obtained forone utterance due to the FILLER transitions, be-cause the utterance can be parsed with several paths.We used a WFST to select the most appropriatepath from several output sequences.
The path withthe highest cumulative weight, w, is selected in a211Table 2: Many LU results for input ?It is February twenty second please?LU output LU result wIt is February twenty second please month=2, day=22 2.0It is FILLER twenty second please day=22 1.0It is FILLER twenty second FILLER day=22 1.0FILLER FILLER FILLER FILLER FILLER FILLER n/a 0Table 1: Examples of utterances with FILLERsASR outputWell, it is February twenty second pleaseIt is uhm, February twenty second pleaseIt is February, twe-, twenty second pleaseIt is February twenty second please, OK?
(LU result = [month=2, day=22])WFST-based LU.
In the example in Table 2, theconcept [month=2, day=22] has been selected, be-cause its cumulative weight, w, is 2.0, which is thehighest.The weightings of conventional WFST-based ap-proaches used an n-gram of concepts (Potamianosand Kuo, 2000) and that of word-concept pairs (Wu-tiwiwatchai and Furui, 2004).
They obtained then-grams from several thousands of annotated ut-terances.
However, it takes a great deal of ef-fort to transcribe and annotate a large corpus.
Ourmethod enables prototype LU modules to be rapidlyconstructed that are robust against various expres-sions with SLM-based ASR and WFST-based pars-ing.
The SLM and WFST are generated automat-ically from a domain grammar description in ourtoolkit.
We need fewer data to train WFST, becauseits weightings are simpler than those in conventionalmethods.
Therefore, it is easy to develop an LUmodule for a new domain with our method.3 Domain Grammar DescriptionA developer defines grammars, slots, and conceptsin a domain in an XML file.
This description en-ables an SLM for ASR and parsing WFST to be au-tomatically generated.
Therefore, a developer canconstruct an LU module rapidly with our method.Figure 3 shows an example of a descrip-tion.
A definition of a slot is described inkeyphrase-class tags and its keyphrases and...<keyphrase-class name="month">...<keyphrase><orth>February</orth><sem>2</sem></keyphrase>...</keyphrase-class>...<action type="specify-attribute"><sentence> {It is} [*month] *day [please]</sentence></action>Figure 3: Example of a grammar descriptionthe values are in keyphrase tags.
The month isdefined as a slot in this figure.
February and 2 aredefined as one of the phrases and values for the slotmonth.
A grammar is described in a sequence ofterminal and non-terminal symbols.
A non-terminalsymbol represents a class of keyphrases, which isdefined in keyphrase-class.
It begins with anasterisk ?*?
in a grammar description in sentencetags.
Symbols that can be skipped are enclosedby brackets [].
The FILLER transition describedin Section 2 is inserted between the symbols un-less they are enclosed in brackets [] or braces {}.Braces are used to avoid FILLER transitions frombeing inserted.
For example, the grammar in Figure3 accepts ?It is February twenty second please.?
and?It is twenty second, OK?
?, but rejects ?It is Febru-ary.?
and ?It, uhm, is February twenty second.
?.A WFST for parsing can be automatically gener-ated from this XML file.
The WFST in Figure 2 isgenerated from the definition in Figure 3.
Moreover,we can generate example sentences from the gram-mar description.
The SLM for the speech recognizeris generated with our method by using many exam-ple sentences generated from the defined grammar.2124 Weighting for ASR Outputs on TwoLevelsWe define weights on two levels for a WFST.
Thefirst is a weighting for ASR outputs, which is set toselect paths that are reliable at a surface word level.The second is a weighting for concepts, which isused to select paths that are reliable on a conceptlevel.
The weighting for concepts reflects correct-ness at a more abstract level than the surface wordlevel.
The weighting for ASR outputs consists oftwo categories: a weighting for ASR N-best outputsand one for accepted words.
We will describe thedefinitions of these weightings in the following sub-sections.4.1 Weighting for ASR N-Best OutputsThe N-best outputs of ASR are used for an input ofa WFST.
Weights are assigned to each sentence inASR N-best outputs.
Larger weights are given tomore reliable sentences, whose ranks in ASR N-bestare higher.
We define this preference aswis =e?
?scorei?Nj e?
?scorej,where wis is a weight for the i-th sentence in ASRN-best outputs, ?
is a coefficient for smoothing, andscorei is the log-scaled score of the i-th ASR out-put.
This weighting reflects the reliability of theASR output.
We set ?
to 0.025 in this study aftera preliminary experiment.4.2 Weighting for Accepted WordsWeights are assigned to word sequences that havebeen accepted by the WFST.
Larger weights aregiven to more reliable sequences of ASR outputs atthe surface word level.
Generally, longer sequenceshaving more words that are not fillers and more re-liable ASR outputs are preferred.
We define thesepreferences as the weights:1.
word(const.
): ww = 1.0,2. word(#phone): ww = l(W ), and3.
word(CM): ww = CM(W ) ?
?w.The word(const.)
gives a constant weight toall accepted words.
This means that sequenceswith more words are simply preferred.
Theword(#phone) takes the length of each acceptedword into consideration.
This length is measured byits number of phonemes, which are normalized bythat of the longest word in the vocabulary.
The nor-malized values are denoted as l(W ) (0 < l(W ) ?1).
By adopting word(#phone), the length of se-quences is represented more accurately.
We alsotake the reliability of the accepted words into ac-count as word(CM).
This uses confidence measures(Lee et al, 2004) for a word, W , in ASR outputs,which are denoted as CM(W ).
The ?w is the thresh-old for determining whether word W is accepted ornot.
The ww obtains a negative value for an unreli-able word W when CM(W ) is lower than ?w.
Thisrepresents a preference for longer and more reliablesequences.4.3 Weighting for ConceptsIn addition to the ASR level, weights on a conceptlevel are also assigned.
The concepts are obtainedfrom the parsing results by the WFST, and containseveral words.
Weights for concepts are defined byusing the measures of all words contained in a con-cept.We prepared three kinds of weights for the con-cepts:1.
cpt(const.
): wc = 1.0,2. cpt(avg):wc =?W (CM(W ) ?
?c)#W , and3.
cpt(#pCM(avg)):wc =?W (CM(W ) ?
l(W ) ?
?c)#W ,where W is a set of accepted words, W , in the corre-sponding concept, and #W is the number of wordsin W .The cpt(const.)
represents a preference forsequences with more concepts.
The cpt(avg)is defined as the weight by using the CM(W )of each word contained in the concept.
Thecpt(#pCM(avg)) represents a preference for longerand reliable sequences with more concepts.
The ?cis the threshold for the acceptance of a concept.213Table 3: Examples of weightings when parameter set is: word(CM) and cpt(#pCM(avg))ASR onput No, it is February twenty secondLU output FILLER it is February twenty secondCM(W ) 0.3 0.7 0.6 0.9 1.0 0.9l(W ) 0.3 0.2 0.2 0.9 0.6 0.5Concept - - - month=2 day=22word - 0.7 ?
?w 0.6 ?
?w 0.9 ?
?w 1.0 ?
?w 0.9 ?
?wcpt - - - (0.9 ?
0.9 ?
?c)/1 (1.0 ?
0.6 ?
?c + 0.9 ?
0.5 ?
?c)/2'&$%Reference From June third pleaseASR output From June third uhm FIT please LU resultCM(W ) 0.771 0.978 0.757 0.152 0.525 0.741LU reference From June third FILLER FILLER FILLER month:6, day:3Our method From June third FILLER FILLER FILLER month:6, day:3Keyword spotting From June third FILLER FIT please month:6, day:3, car:FIT(?FIT?
is the name of a car.
)Figure 4: Example of LU with WFST4.4 Calculating Cumulative Weight andTrainingThe LU results are selected based on the weightedsum of the three weights in Subsection 4.3 aswi = wis + ?w?ww + ?c?wcThe LU module selects an output sequence withthe highest cumulative weight, wi, for 1 ?
i ?
N .Let us explain how to calculate cumulative weightwi by using the example specified in Table 3.
Here,word(CM) and cpt(#pCM(avg)) are selected as pa-rameters.
The sum of weights in this table for ac-cepted words is ?w(4.1 ?
5?w), when the input se-quence is ?No, it is February twenty second.
?.The sum of weights for concepts is ?c(1.335 ?
2?c)because the weight for ?month=2?
is ?c(0.81 ?
?c)and the weight for ?day=22?
is ?c(0.525 ?
?c).Therefore, cumulative weight wi for this input se-quence is wis + ?w(4.1 ?
5?w) + ?c(1.335 ?
2?c).In the training phase, various combinations of pa-rameters are tested, i.e., which weightings are usedfor each of ASR output and concept level, such asN = 1 or 10, coefficient ?w,c = 1.0 or 0, and thresh-old ?w,c = 0 to 0.9 at intervals of 0.1, on the train-ing data.
The coefficient ?w,c = 0 means that acorresponding weight is not added.
The optimal pa-rameter settings are obtained after testing the variouscombinations of parameters.
They make the concepterror rate (CER) minimum for a training data set.We calculated the CER in the following equation:CER = (S +D + I)/N , where N is the number ofconcepts in a reference, and S, D, and I correspondto the number of substitution, deletion, and insertionerrors.Figure 4 shows an example of LU with ourmethod, where it rejects misrecognized concept[car:FIT], which cannot be rejected by keywordspotting.5 Experiments and Evaluation5.1 Experimental ConditionsWe discussed our experimental investigation into theeffects of weightings in Section 4.
The user utter-ance in our experiment was first recognized by ASR.Then, the i-th sentence of ASR output was input toWFST for 1 ?
i ?
N , and the LU result for thehighest cumulative weight, wi, was obtained.We used 4186 utterances in the video recordingreservation domain (video domain), which consistedof eight different dialogues with a total of 25 differ-ent speakers.
We also used 3364 utterances in therent-a-car reservation domain (rent-a-car domain) of214eight different dialogues with 23 different speakers.We used Julius 1 as a speech recognizer with anSLM.
The language model was prepared by usingexample sentences generated from the grammars ofboth domains.
We used 10000 example sentences inthe video and 40000 in the rent-a-car domain.
Thenumber of the generated sentences was determinedempirically.
The vocabulary size was 209 in thevideo and 891 in the rent-a-car domain.
The averageASR accuracy was 83.9% in the video and 65.7%in the rent-a-car domain.
The grammar in the videodomain included phrases for dates, times, channels,commands.
That of the rent-a-car domain includedphrases for dates, times, locations, car classes, op-tions, and commands.
The WFST parsing mod-ule was implemented by using the MIT FST toolkit(Hetherington, 2004).5.2 Performance of WFST-based LUWe evaluated our method in the two domains: videoand rent-a-car.
We compared the CER on test data,which was calculated by using the optimal settingsfor both domains.
We evaluated the results with 4-fold cross validation.
The number of utterances fortraining was 3139 (=4186*(3/4)) in the video and2523 (=3364*(3/4)) in the rent-a-car domain.The baseline method was simple keyword spot-ting because we assumed a condition where a largeamount of training data was not available.
Thismethod extracts as many keyphrases as possiblefrom ASR output without taking speech recogni-tion errors and grammatical rules into consideration.Both grammar-based and SLM-based ASR outputsare used for input in keyword spotting (denoted as?Grammar & spotting?
and ?SLM & spotting?
inTable 4).
The grammar for grammar-based ASRwas automatically generated by the domain descrip-tion file.
The accuracy of grammar-based ASR was66.3% in the video and 43.2% in the rent-a-car do-main.Table 4 lists the CERs for both methods.
In key-word spotting with SLM-based ASR, the CERs wereimproved by 5.2 points in the video and by 22.2points in the rent-a-car domain compared with thosewith grammar-based ASR.
This is because SLM-based ASR is more robust against fillers and un-1http://julius.sourceforge.jp/Table 4: Concept error rates (CERs) in each domainDomain Grammar &spotting SLM &spotting OurmethodVideo 22.1 16.9 13.5Rent-a-car 51.1 28.9 22.0known words than grammar-based ASR.
The CERwas improved by 3.4 and 6.9 points by optimalweightings for WFST.
Table 5 lists the optimal pa-rameters in both domains.
The ?c = 0 in the videodomain means that weights for concepts were notused.
This result shows that optimal parameters de-pend on the domain for the system, and these needto be adapted for each domain.5.3 Performance According to Training DataWe also investigated the relationship between thesize of the training data for our method and the CER.In this experiment, we calculated the CER in thetest data by increasing the number of utterances fortraining.
We also evaluated the results by 4-foldcross validation.Figures 5 and 6 show that our method outper-formed the baseline methods by about 80 utterancesin the video domain and about 30 utterances inthe rent-a-car domain.
These results mean that ourmethod can effectively be used to rapidly prototypeLU modules.
This is because it can achieve robustLU with fewer training data compared with conven-tional WFST-based methods, which need over sev-eral thousand sentences for training.6 ConclusionWe developed a method of rapidly prototyping ro-bust LU modules for spoken language understand-ing.
An SLM for a speech recognizer and a WFSTfor parsing were automatically generated from a do-main grammar description.
We defined two kindsof weightings for the WFST at the word and con-cept levels.
These two kinds of weightings werecalculated by ASR outputs.
This made it possi-ble to create an LU module for a new domain withless effort because the weighting scheme was rel-atively simpler than those of conventional methods.The optimal parameters could be selected with fewertraining data in both domains.
Our experiment re-215Table 5: Optimal parameters in each domainDomain N ?w ww ?c wcVideo 1 1.0 word(const.)
0 -Rent-a-car 10 1.0 word(CM)-0.0 1.0 cpt(#pCM(avg))-0.805101520253035403000 1000 500 250 100 50 10CER#utt.
for trainingGrammar-based ASR & keyword spottingSLM-based ASR & keyword spottingOur methodFigure 5: CER in video domain05101520253050553000 1000 500 250 100 50 10CER#utt.
for trainingGrammar-based ASR & keyword spottingSLM-based ASR & keyword spottingOur methodFigure 6: CER in rent-a-car domainvealed that the CER could be improved compared tothe baseline by training optimal parameters with asmall amount of training data, which could be rea-sonably prepared for new domains.
This means thatour method is appropriate for rapidly prototypingLU modules.
Our method should help developersof spoken dialogue systems in the early phases ofdevelopment.
We intend to evaluate our method onother domains, such as database searches and ques-tion answering in future work.AcknowledgmentsWe are grateful to Dr. Toshihiko Ito and Ms. YukaNagano of Hokkaido University for constructing therent-a-car domain system.ReferencesYulan He and Steve Young.
2005.
Spoken LanguageUnderstanding using the Hidden Vector State Model.Speech Communication, 48(3-4):262?275.Lee Hetherington.
2004.
The MIT finite-state trans-ducer toolkit for speech and language processing.
InProc.
6th International Conference on Spoken Lan-guage Processing (INTERSPEECH-2004 ICSLP).Akinobu Lee, Kiyohiro Shikano, and Tatsuya Kawahara.2004.
Real-time word confidence scoring using lo-cal posterior probabilities on tree trellis search.
InProc.
2004 IEEE International Conference on Acous-tics, Speech, and Signal Processing (ICASSP 2004),volume 1, pages 793?796.Alexandors Potamianos and Hong-Kwang J. Kuo.
2000.Statistical recursive finite state machine parsingfor speech understanding.
In Proc.
6th Interna-tional Conference on Spoken Language Processing(INTERSPEECH-2000 ICSLP), pages 510?513.Stephanie Seneff.
1992.
TINA: A natural language sys-tem for spoken language applications.
ComputationalLinguistics, 18(1):61?86.Katsuhito Sudoh and Hajime Tsukada.
2005.
Tightly in-tegrated spoken language understanding using word-to-concept translation.
In Proc.
9th European Con-ference on Speech Communication and Technology(INTERSPEECH-2005 Eurospeech), pages 429?432.Chai Wutiwiwatchai and Sadaoki Furui.
2004.
Hybridstatistical and structural semantic modeling for Thaimulti-stage spoken language understanding.
In Proc.HLT-NAACL Workshop on Spoken Language Under-standing for Conversational Systems and Higher LevelLinguistic Information for Speech Processing, pages2?9.216
