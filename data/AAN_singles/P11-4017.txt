Proceedings of the ACL-HLT 2011 System Demonstrations, pages 97?102,Portland, Oregon, USA, 21 June 2011. c?2011 Association for Computational LinguisticsWikipedia Revision Toolkit: Efficiently Accessing Wikipedia?s Edit HistoryOliver Ferschke, Torsten Zesch, and Iryna GurevychUbiquitous Knowledge Processing LabComputer Science Department, Technische Universita?t DarmstadtHochschulstrasse 10, D-64289 Darmstadt, Germanyhttp://www.ukp.tu-darmstadt.deAbstractWe present an open-source toolkit whichallows (i) to reconstruct past states ofWikipedia, and (ii) to efficiently access theedit history of Wikipedia articles.
Recon-structing past states of Wikipedia is a pre-requisite for reproducing previous experimen-tal work based on Wikipedia.
Beyond that,the edit history of Wikipedia articles has beenshown to be a valuable knowledge source forNLP, but access is severely impeded by thelack of efficient tools for managing the hugeamount of provided data.
By using a dedi-cated storage format, our toolkit massively de-creases the data volume to less than 2% ofthe original size, and at the same time pro-vides an easy-to-use interface to access the re-vision data.
The language-independent designallows to process any language represented inWikipedia.
We expect this work to consolidateNLP research using Wikipedia in general, andto foster research making use of the knowl-edge encoded in Wikipedia?s edit history.1 IntroductionIn the last decade, the free encyclopedia Wikipediahas become one of the most valuable and com-prehensive knowledge sources in Natural LanguageProcessing.
It has been used for numerous NLPtasks, e.g.
word sense disambiguation, semantic re-latedness measures, or text categorization.
A de-tailed survey on usages of Wikipedia in NLP can befound in (Medelyan et al, 2009).The majority of Wikipedia-based NLP algorithmsworks on single snapshots of Wikipedia, which arepublished by the Wikimedia Foundation as XMLdumps at irregular intervals.1 Such a snapshot onlyrepresents the state of Wikipedia at a certain fixedpoint in time, while Wikipedia actually is a dynamicresource that is constantly changed by its millions ofeditors.
This rapid change is bound to have an influ-ence on the performance of NLP algorithms usingWikipedia data.
However, the exact consequencesare largely unknown, as only very few papers havesystematically analyzed this influence (Zesch andGurevych, 2010).
This is mainly due to older snap-shots becoming unavailable, as there is no officialbackup server.
As a consequence, older experimen-tal results cannot be reproduced anymore.In this paper, we present a toolkit that solvesboth issues by reconstructing a certain past state ofWikipedia from its edit history, which is offered bythe Wikimedia Foundation in form of a databasedump.
Section 3 gives a more detailed overview ofthe reconstruction process.Besides reconstructing past states of Wikipedia,the revision history data also constitutes a novelknowledge source for NLP algorithms.
The se-quence of article edits can be used as training datafor data-driven NLP algorithms, such as vandalismdetection (Chin et al, 2010), text summarization(Nelken and Yamangil, 2008), sentence compres-sion (Yamangil and Nelken, 2008), unsupervisedextraction of lexical simplifications (Yatskar et al,2010), the expansion of textual entailment corpora(Zanzotto and Pennacchiotti, 2010), or assesing thetrustworthiness of Wikipedia articles (Zeng et al,2006).1http://download.wikimedia.org/97However, efficient access to this new resourcehas been limited by the immense size of the data.The revisions for all articles in the current EnglishWikipedia sum up to over 5 terabytes of text.
Con-sequently, most of the above mentioned previouswork only regarded small samples of the availabledata.
However, using more data usually leads to bet-ter results, or how Church and Mercer (1993) putit ?more data are better data?.
Thus, in Section 4,we present a tool to efficiently access Wikipedia?sedit history.
It provides an easy-to-use API for pro-grammatically accessing the revision data and re-duces the required storage space to less than 2% ofits original size.
Both tools are publicly availableon Google Code (http://jwpl.googlecode.com) as open source software under the LGPL v3.2 Related WorkTo our knowledge, there are currently only two alter-natives to programmatically access Wikipedia?s re-vision history.One possibility is to manually parse the originalXML revision dump.
However, due to the huge sizeof these dumps, efficient, random access is infeasi-ble with this approach.Another possibility is using the MediaWiki API2,a web service which directly accesses live data fromthe Wikipedia website.
However, using a web ser-vice entails that the desired revision for every singlearticle has to be requested from the service, trans-ferred over the Internet and then stored locally inan appropriate format.
Access to all revisions ofall Wikipedia articles for a large-scale analysis isinfeasible with this method because it is stronglyconstricted by the data transfer speed over the In-ternet.
Even though it is possible to bypass this bot-tleneck by setting up a local Wikipedia mirror, theMediaWiki API can only provide full text revisions,which results in very large amounts of data to betransferred.Better suited for tasks of this kind are APIsthat utilize databases for storing and accessing theWikipedia data.
However, current database-drivenWikipedia APIs do not support access to article re-visions.
That is why we decided to extend an es-tablished API with the ability to efficiently access2http://www.mediawiki.org/wiki/APIWikipedia?s edit history.
Two established WikipediaAPIs have been considered for this purpose.Wikipedia Miner3 (Milne and Witten, 2009) isan open source toolkit which provides access toWikipedia with the help of a preprocessed database.It represents articles, categories and redirects as Javaclasses and provides access to the article content ei-ther as MediaWiki markup or as plain text.
Thetoolkit mainly focuses on Wikipedia?s structure, thecontained concepts, and semantic relations, but itmakes little use of the textual content within the ar-ticles.
Even though it was developed to work lan-guage independently, it focuses mainly on the En-glish Wikipedia.Another open source API for accessing Wikipediadata from a preprocessed database is JWPL4 (Zeschet al, 2008).
Like Wikipedia Miner, it also rep-resents the content and structure of Wikipedia asJava objects.
In addition to that, JWPL contains aMediaWiki markup parser to further analyze the ar-ticle contents to make available fine-grained infor-mation like e.g.
article sections, info-boxes, or firstparagraphs.
Furthermore, it was explicitly designedto work with all language versions of Wikipedia.We have chosen to extend JWPL with our revi-sion toolkit, as it has better support for accessing ar-ticle contents, natively supports multiple languages,and seems to have a larger and more active developercommunity.
In the following section, we present theparts of the toolkit which reconstruct past states ofWikipedia, while in section 4, we describe tools al-lowing to efficiently access Wikipedia?s edit history.3 Reconstructing Past States of WikipediaAccess to arbitrary past states of Wikipedia is re-quired to (i) evaluate the performance of Wikipedia-based NLP algorithms over time, and (ii) to repro-duce Wikipedia-based research results.
For this rea-son, we have developed a tool called TimeMachine,which addresses both of these issues by making useof the revision dump provided by the WikimediaFoundation.
By iterating over all articles in the re-vision dump and extracting the desired revision ofeach article, it is possible to recover the state ofWikipedia at an earlier point in time.3http://wikipedia-miner.sourceforge.net4http://jwpl.googlecode.com98Property Description Example Valuelanguage The Wikipedia language version englishmainCategory Title of the main category of theWikipedia language version usedCategoriesdisambiguationCategory Title of the disambiguation category ofthe Wikipedia language version usedDisambiguationfromTimestamp Timestamp of the first snapshot to beextracted20090101130000toTimestamp Timestamp of the last snapshot to be ex-tracted20091231130000each Interval between snapshots in days 30removeInputFilesAfterProcessing Remove source files [true/false] falsemetaHistoryFile Path to the revision dump PATH/pages-meta-history.xml.bz2pageLinksFile Path to the page-to-page link records PATH/pagelinks.sql.gzcategoryLinksFile Path to the category membershiprecordsPATH/categorylinks.sql.gzoutputDirectory Output directory PATH/outdir/Table 1: Configuration of the TimeMachineThe TimeMachine is controlled by a single con-figuration file, which allows (i) to restore individualWikipedia snapshots or (ii) to generate whole snap-shot series.
Table 1 gives an overview of the con-figuration parameters.
The first three properties setthe environment for the specific language version ofWikipedia.
The two timestamps define the start andend time of the snapshot series, while the intervalbetween the snapshots in the series is set by the pa-rameter each.
In the example, the TimeMachine re-covers 13 snapshots between Jan 01, 2009 at 01.00p.m and and Dec 31, 2009 at 01.00 p.m at an inter-val of 30 days.
In order to recover a single snap-shot, the two timestamps have simply to be set tothe same value, while the parameter ?each?
has noeffect.
The option removeInputFilesAfterProcessingspecifies whether to delete the source files after pro-cessing has finished.
The final four properties definethe paths to the source files and the output directory.The output of the TimeMachine is a set of eleventext files for each snapshot, which can directly beimported into an empty JWPL database.
It can beaccessed with the JWPL API in the same way assnapshots created using JWPL itself.Issue of Deleted Articles The past snapshot ofWikipedia created by our toolkit is identical to thestate of Wikipedia at that time with the exception ofarticles that have been deleted meanwhile.
Articlesmight be deleted only by Wikipedia administratorsif they are subject to copyright violations, vandal-ism, spam or other conditions that violate Wikipediapolicies.
As a consequence, they are removed fromthe public view along with all their revision infor-mation, which makes it impossible to recover themfrom any future publicly available dump.5 Eventhough about five thousand pages are deleted everyday, only a small percentage of those pages actuallycorresponds to meaningful articles.
Most of the af-fected pages are newly created duplicates of alreadyexisting articles or spam articles.4 Efficient Access to RevisionsEven though article revisions are available from theofficial Wikipedia revision dumps, accessing this in-formation on a large scale is still a difficult task.This is due to two main problems.
First, the revi-sion dump contains all revisions as full text.
Thisresults in a massive amount of data and makes struc-tured access very hard.
Second, there is no efficientAPI available so far for accessing article revisionson a large scale.Thus, we have developed a tool calledRevisionMachine, which solves these issues.First, we describe our solution to the storage prob-lem.
Second, we present several use cases of theRevisionMachine, and show how the API simplifiesexperimental setups.5http://en.wikipedia.org/wiki/Wikipedia:DEL994.1 Revision StorageAs each revision of a Wikipedia article stores thefull article text, the revision history obviously con-tains a lot of redundant data.
The RevisionMachinemakes use of this fact and utilizes a dedicated stor-age format which stores a revision only by meansof the changes that have been made to the previousrevision.
For this purpose, we have tested existingdiff libraries, like Javaxdelta6 or java-diff7, whichcalculate the differences between two texts.
How-ever, both their runtime and the size of the result-ing output was not feasible for the given size of thedata.
Therefore, we have developed our own diffalgorithm, which is based on a longest common sub-string search and constitutes the foundation for ourrevision storage format.The processing of two subsequent revisions canbe divided into four steps:?
First, the RevisionMachine searches for allcommon substrings with a user-defined mini-mal length.?
Then, the revisions are divided into blocks ofequal length.
Corresponding blocks of bothrevisions are then compared.
If a block iscontained in one of the common substrings,it can be marked as unchanged.
Otherwise,we have to categorize the kind of changethat occurred in this block.
We differenti-ate between five possible actions: Insert,Delete, Replace, Cut and Paste8.
Thisinformation is stored in each block and is lateron used to encode the revision.?
In the next step, the current revision is repre-sented by means of a sequence of actions per-formed on the previous revision.For example, in the adjacent revision pairr1 : This is the very first sentence!r2 : This is the second sentencer2 can be encoded asREPLACE 12 10 ?second?DELETE 31 16http://javaxdelta.sourceforge.net/7http://www.incava.org/projects/java/java-diff8Cut and Paste operations always occur pairwise.
In ad-dition to the other operations, they can make use of an additionaltemporary storage register to save the text that is being moved.?
Finally, the string representation of this ac-tion sequence is compressed and stored in thedatabase.With this approach, we achieve to reduce the de-mand for disk space for a recent English Wikipediadump containing all article revisions from 5470 GBto only 96 GB, i.e.
by 98%.
The compressed data isstored in a MySQL database, which provides sophis-ticated indexing mechanisms for high-performanceaccess to the data.Obviously, storing only the changes instead ofthe full text of each revision trades in speed forspace.
Accessing a certain revision now requires re-constructing the text of the revision from a list ofchanges.
As articles often have several thousand re-visions, this might take too long.
Thus, in order tospeed up the recovery of the revision text, every n-threvision is stored as a full revision.
A low value ofn decreases the time needed to access a certain re-vision, but increases the demand for storage space.We have found n = 1000 to yield a good trade-off9.This parameter, among a few other possibilities tofine-tune the process, can be set in a graphical userinterface provided with the RevisionMachine.4.2 Revision AccessAfter the converted revisions have been stored inthe revision database, it can either be used stand-alone or combined with the JWPL data and ac-cessed via the standard JWPL API.
The latter op-tion makes it possible to combine the possibilitiesof the RevisionMachine with other components likethe JWPL parser for the MediaWiki syntax.In order to set up the RevisionMachine, it is onlynecessary to provide the configuration details for thedatabase connection (see Listing 1).
Upon first ac-cess, the database user has to have write permissionon the database, as indexes have to be created.
Forlater use, read permission is sufficient.
Access to theRevisionMachine is achieved via two API objects.The RevisionIterator allows to iterate over all revi-sions in Wikipedia.
The RevisionAPI grants accessto the revisions of individual articles.
In addition to9If hard disk space is no limiting factor, the parameter can beset to 1 to avoid the compression of the revisions and maximizethe performance.100/ / S e t up d a t a b a s e c o n n e c t i o nDatabaseConfiguration db = new DatabaseConfiguration ( ) ;db .setDatabase ( ?
dbname ? )
;db .setHost ( ?
hos tname ? )
;db .setUser ( ?
username ? )
;db .setPassword ( ?pwd? )
;db .setLanguage (Language .english ) ;/ / C r e a t e API o b j e c t sWikipedia wiki = WikiConnectionUtils .getWikipediaConnection (db ) ;RevisionIterator revIt = new RevisionIterator (db ) ;RevisionApi revApi = new RevisionApi (db ) ;Listing 1: Setting up the RevisionMachinethat, the Wikipedia object provides access to JWPLfunctionalities.In the following, we describe three use cases ofthe RevisionMachine API, which demonstrate howit is easily integrated into experimental setups.Processing all article revisions in WikipediaThe first use case focuses on the utilization of thecomplete set of article revisions in a Wikipedia snap-shot.
Listing 2 shows how to iterate over all revi-sions.
Thereby, the iterator ensures that successiverevisions always correspond to adjacent revisions ofa single article in chronological order.
The start ofa new article can easily be detected by checking thetimestamp and the article id.
This approach is es-pecially useful for applications in statistical naturallanguage processing, where large amounts of train-ing data are a vital asset.Processing revisions of individual articles Thesecond use case shows how the RevisionMachinecan be used to access the edit history of a specificarticle.
The example in Listing 3 illustrates how allrevisions for the article Automobile can be retrievedby first performing a page query with the JWPL APIand then retrieving all revision timestamps for thispage, which can finally be used to access the revi-sion objects.Accessing the meta data of a revision The thirduse case illustrates the access to the meta data of in-dividual revisions.
The meta data includes the nameor IP of the contributor, the additional user commentfor the revision and a flag that identifies a revision asminor or major.
Listing 4 shows how the number ofedits and unique contributors can be used to indicatethe level of edit activity for an article.5 ConclusionsIn this paper, we presented an open-source toolkitwhich extends JWPL, an API for accessingWikipedia, with the ability to reconstruct past statesof Wikipedia, and to efficiently access the edit his-tory of Wikipedia articles.Reconstructing past states of Wikipedia is aprerequisite for reproducing previous experimen-tal work based on Wikipedia, and is also a re-quirement for the creation of time-based series ofWikipedia snapshots and for assessing the influenceof Wikipedia growth on NLP algorithms.
Further-more, Wikipedia?s edit history has been shown to bea valuable knowledge source for NLP, which is hardto access because of the lack of efficient tools formanaging the huge amount of revision data.
By uti-lizing a dedicated storage format for the revisions,our toolkit massively decreases the amount of datato be stored.
At the same time, it provides an easy-to-use interface to access the revision data.We expect this work to consolidate NLP re-search using Wikipedia in general, and to fosterresearch making use of the knowledge encoded inWikipedia?s edit history.
The toolkit will be madeavailable as part of JWPL, and can be obtained fromthe project?s website at Google Code.
(http://jwpl.googlecode.com)AcknowledgmentsThis work has been supported by the Volkswagen Foun-dation as part of the Lichtenberg-Professorship Programunder grant No.
I/82806, and by the Hessian researchexcellence program ?Landes-Offensive zur EntwicklungWissenschaftlich-o?konomischer Exzellenz?
(LOEWE) aspart of the research center ?Digital Humanities?.
Wewould also like to thank Simon Kulessa for designing andimplementing the foundations of the RevisionMachine.101/ / I t e r a t e ove r a l l r e v i s i o n s o f a l l a r t i c l e sw h i l e (revIt .hasNext ( ) ) {Revision rev = revIt .next ( )rev .getTimestamp ( ) ;rev .getArticleID ( ) ;/ / p r o c e s s r e v i s i o n .
.
.
}Listing 2: Iteration over all revisions of all articles/ / Get a r t i c l e wi th t i t l e ?
Automobi le ?Page article = wiki .getPage ( ?
Automobi le ? )
;i n t id = article .getPageId ( ) ;/ / Get a l l r e v i s i o n s f o r t h e a r t i c l eCollection<Timestamp> revisionTimeStamps = revApi .getRevisionTimestamps (id ) ;f o r (Timestamp t :revisionTimeStamps ) {Revision rev = revApi .getRevision (id , t ) ;/ / p r o c e s s r e v i s i o n .
.
.
}Listing 3: Accessing the revisions of a specific article/ / Meta d a t a p r o v i d e d by t h e Rev i s ionAPIStringBuffer s = new StringBuffer ( ) ;s .append ( ?
The a r t i c l e has ?+revApi .getNumberOfRevisions (pageId ) +?
r e v i s i o n s .\ n ? )
;s .append ( ?
I t has ?+revApi .getNumberOfUniqueContributors (pageId ) +?
un iq ue c o n t r i b u t o r s .\ n ? )
;s .append (revApi .getNumberOfUniqueContributors (pageId , t r u e ) + ?
a r e r e g i s t e r e d u s e r s .\ n ? )
;/ / Meta d a t a p r o v i d e d by t h e R e v i s i o n o b j e c ts .append ( (rev .isMinor ( ) ?
?
Minor ?
: ?
Major ? )
+?
r e v i s i o n by : ?+rev .getContributorID ( ) ) ;s .append ( ?\nComment : ?+rev .getComment ( ) ) ;Listing 4: Accessing the meta data of a revisionReferencesSi-Chi Chin, W. Nick Street, Padmini Srinivasan, andDavid Eichmann.
2010.
Detecting wikipedia vandal-ism with active learning and statistical language mod-els.
In Proceedings of the 4th workshop on Informa-tion credibility, WICOW ?10, pages 3?10.Kenneth W. Church and Robert L. Mercer.
1993.
Intro-duction to the special issue on computational linguis-tics using large corpora.
Computational Linguistics,19(1):1?24.Olena Medelyan, David Milne, Catherine Legg, andIan H. Witten.
2009.
Mining meaning from wikipedia.Int.
J. Hum.-Comput.
Stud., 67:716?754, September.D.
Milne and I. H. Witten.
2009.
An open-source toolkitfor mining Wikipedia.
In Proc.
New Zealand Com-puter Science Research Student Conf., volume 9.Rani Nelken and Elif Yamangil.
2008.
Miningwikipedia?s article revision history for training com-putational linguistics algorithms.
In Proceedings ofthe AAAI Workshop on Wikipedia and Artificial Intel-ligence: An Evolving Synergy (WikiAI), WikiAI08.Elif Yamangil and Rani Nelken.
2008.
Mining wikipediarevision histories for improving sentence compres-sion.
In Proceedings of ACL-08: HLT, Short Papers,pages 137?140, Columbus, Ohio, June.
Associationfor Computational Linguistics.Mark Yatskar, Bo Pang, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2010.
For the sake of simplic-ity: unsupervised extraction of lexical simplificationsfrom wikipedia.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Linguis-tics, HLT ?10, pages 365?368.Fabio Massimo Zanzotto and Marco Pennacchiotti.2010.
Expanding textual entailment corpora fromwikipedia using co-training.
In Proceedings of theCOLING-Workshop on The People?s Web Meets NLP:Collaboratively Constructed Semantic Resources.Honglei Zeng, Maher Alhossaini, Li Ding, Richard Fikes,and Deborah L. McGuinness.
2006.
Computing trustfrom revision history.
In Proceedings of the 2006 In-ternational Conference on Privacy, Security and Trust.Torsten Zesch and Iryna Gurevych.
2010.
The more thebetter?
Assessing the influence of wikipedia?s growthon semantic relatedness measures.
In Proceedings ofthe Conference on Language Resources and Evalua-tion (LREC), Valletta, Malta.Torsten Zesch, Christof Mueller, and Iryna Gurevych.2008.
Extracting Lexical Semantic Knowledge fromWikipedia and Wiktionary.
In Proceedings of theConference on Language Resources and Evaluation(LREC).102
