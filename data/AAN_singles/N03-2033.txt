Automatically Predicting Information Quality in News DocumentsRong TangSchool of InformationScience and PolicyUniversity at Albany135 Western AvenueAlbany, NY 12222tangr@albany.eduKwong Bor NgGraduate School ofLibrary and InformationStudies, QueensCollege, CUNY.New York, NY 11367kbng@qc.eduTomek StrzalkowskiILS InstituteUniverity at Albany1400 Washington AveAlbany, NY 12222tomek@albany.eduPaul B. KantorSchool of CommunicationInformation and LibraryStudiesRutgers UniversityNew Brunswick, NJ 08901kan-tor@scils.rutgers.eduAbstractWe report here empirical results of a series ofstudies aimed at automatically predicting in-formation quality in news documents.
Multipleresearch methods and data analysis techniquesenabled a good level of machine prediction ofinformation quality.
Procedures regarding userexperiments and statistical analysis are de-scribed.12IntroductionAs a part of a large-scale multi-institutional projectHITIQA (High-quality Interactive Question Answer-ing), we worked on developing an extended model forclassifying information by quality, in addition to, and asan extension of the traditional notion of relevance.
Theproject involves Computer and Information Scienceresearchers from University at Albany and Rutgers Uni-versity.
Our serving clientele are intelligent analysts,and the documents that we targeted were news articles.Research ApproachThe term ?Quality?
is defined by International Organi-zation of Standards (1986) as ?the totality ofcharacteristics of an entity that bear on its ability tosatisfy stated and implied need?
(Standard 8402, 3.1).Among numerous study on classification of informationquality, Wang and Strong (1996) proposed four dimen-sions of qualities as detailed in Table 1: intrinsic,contextual, representational, and accessibility.Categories ElementsIntrinsic IQ Accuracy, Objectivity, Believ-ability, ReputationAccessibility IQ Accessibility, SecurityContextual IQ Relevancy, Value-added,Timeliness, Completeness,Amount of InformationRepresenta-tional IQInterpretability, Ease of Under-standing, Concise Representa-tion, Consistent RepresentationTable 1.
Information Quality Dimensions (Source:Strong, Lee, Wang, 1997, p.39)Empirical attempts to assess quality have primarilyfocused on counting hyperlinks in a networked envi-ronment.
Representative studies include the work byAmento and his colleagues (Amento, Terveen, & Hills,2000), Price and Hersh (1999), and Zhu and Gauch(2000).
However, as a whole, previous studies wereonly able to produce algorithmic measures for Webdocuments based on link counts and with a limitednumber of quality aspects such as popularity.
Our ap-proach is to record actual users?
quality assessments ofnews articles and conduct advanced statistical models ofassociation between users?
quality scoring and occur-rence and prevalence of certain textual features.3 Methodology and ResultsMultiple research methods were used.
Firstly, we con-ducted focus-group sessions to elicit key quality aspectsfrom news analysts.
Secondly, we performed expertsand students quality judgment experimental sessions.Thirdly, we identified a set of textual features, ran pro-grams to generate counts of the features, and performedstatistical analysis to establish the correlation betweenfeatures and users?
quality ratings.Two focus group sessions were conducted duringMarch and April of 2002.
Participants included journal-ism faculty members, professional editors, and a num-ber of journalists from a local newspaper Albany TimesUnion.
Nine information quality criteria were consid-ered to be salient to the context of news analysis: Accu-racy, Source reliability, Objectivity, Depth, Authorcredibility, Readability, Conciseness, GrammaticallyCorrectness, and Multiple Viewpoints.A computerized quality judgment system that incor-porated the nine quality aspects was developed.
Onethousand medium-sized (100 to 2500 words) news arti-cles were selected from the TREC collection (Voorhees,2001) with 25 relevant documents each from five TRECQ&A topics.We recruited expert and student participants forjudgment experiments.
Expert sessions were performedfirst and ten documents judged by experts were selectedand used as the training and testing material for the stu-dent participants.
The entire judgment experiment pe-riod ran from May to August of 2002.
As a result, eachof the 1,000 documents was rated twice, by two differ-ent judges, one at Albany, and one at Rutgers.There were high inter-judge agreements between Al-bany and Rutgers.
Figure 1 is the normality plot of thedifference between scores assigned by Rutgers?
judgesand Albany?s judges on the variable of ?accuracy,?
witha mean almost equals to zero (with range from ?
9 to +9).
The curves of the other eight quality variables aresimilar to the one below, indicating a very insignificantdisagreement in judgments.Observed Value86420-2-4-6-8ExpectedNormal Value86420-2-4-6-8Figure 1.
Normality Plot of differences in qualityjudgments on the aspect of ?Accuracy?Principle component analysis (PCA) revealed thesame two components from Albany data as from Rut-gers data.
As shown in Figure 2, one component (thelower one) consists of ?credibility?, ?source reliability?,?accuracy?, ?multi-view?, ?depth?, and ?objectivity.
?The second component (the upper one) consists of?grammar?, ?readability?, and ?verbose and concise-ness?.
Together they explain 58% of the variance.Component 11.0.50.0-.5-1.0Component21.0.50.0-.5-1.0a_multiva_grammaa_verbosa_readaba_credita_deptha_objecta_sourcea_accuraFigure 2.
PCA of Judgment data, in rotated space.Rotation method: Oblimin with Kaiser Normaliza-tion.
Rotation converged in 5 iterations.We recoded users?
scores 1 to 5 as low and scores 6to 10 as high.
We split the 1,000 documents into twohalves by random selection.
In our training round thefirst half was used to estimate the parameters that wouldgive best discriminant and logistic regression functions.In our testing round, we applied the functions to theother half to predict the quality criteria of the docu-ments.DiscriminantAnalysis Cor-rect-RateLogisticRegressionCorrect-RateAccuracy 75.8% 75.9%Source Reliability 67.8% 68.5%Objectivity 70.6% 73.8%Depth 77.4% 77.9%Author Credibility 69.3% 71.7%Readability 81.3% 83.0%Conciseness 70.5% 70.9%Grammar 74.9% 75.1%Multi-view 82.1% 82.2%Table 2.
Performance of prediction (based on split-half training and testing) by two methodsWe then employed stepwise discriminant analysis toselect the dominant predictive variables from a range of104 textual features.
These features included elementsof punctuations, special symbols, length of documentsegments, upper case, quotations, key terms, POS, andentities.
Our further analysis suggested that certain textfeatures are highly correlated with each of the nine as-pects.Quality As-pectsTextual Feature Pearsoncorrelation(2 tails)Accuracy Personal Pronoun 0.0002Source  Distinct organization 0.0048Objectivity Pronoun 0.0001Depth Document length 0.0000AuthorCredibilityDate unit, e.g.
day,week0.0000Readability Closing parenthesis 0.0099Conciseness Subordinating prepo-sition or conjunction0.0003Multi-view Past tense verb 0.0000GrammaticalcorrectnessAverage length ofparagraph in words0.0016Table 3.
Highly correlated textual features andquality aspectsAt this point, we are able to produce good predictionof several aspects of information quality, includingDepth, Objectivity, Multi-view, and Readability.
Theprediction testing and training for the remaining qualityaspects are currently in progress.
Tables 4 and 5 illus-trate the results of training versus testing classificationfor the criteria of  ?objectivity?
and ?depth,?
with rat-ings grouped into high and low categories.Predicted GroupMembershipObjectivityLow HighLow 58.7% 41.3%TrainingCases   Original High 12.7% 87.3%Low 45.5% 54.5%TestingCasesOriginalHigh 23.5% 76.5%Table 4.
Classification result of ?objectivity.
?75.5% of training cases correctly classified, 63.5%of testing cases correctly classifiedPredicted GroupMembershipDepthLow HighLow 64.5% 35.5% TrainingCasesOriginalHigh 11.9% 88.1%Low 51.0% 49.0% TestingCasesOriginalHigh 22.6% 75.4%Table 5.
Classification result of ?depth.?
74.5% oftraining cases correctly classified, 61.6% of testingcases correctly classified4 SummaryIn this study, we were able to identify important qualitycriteria relevant to intelligent analysts?
work and wewere also able to generate automatic quality metrics ofnews documents using users?
quality judgments.
Ournext step is to apply our machine prediction method toproduce measures of a new set of documents and haveusers to verify and modify machines?
scoring.
We hopethat through this, we can collect new data to test ourquality metrics and to further improve its?
performance.AcknowledgementThis paper is based on work supported by the AdvancedResearch and Development Activity (ARDA)?s Ad-vanced Question Answering for Intelligence(AQUAINT) Program under contract number 2002-H790400-000.ReferencesAmendo, B., Terveen, L., & Hill, W. (2000).
Does ?au-thority?
mean quality?
Predicting expert quality rat-ings of Web documents.
Proceedings of the Twenty-Third Annual International ACM SIGIR Conferenceon Research and Development in Information Re-trieval, 296-303.Price, S. L., & Hersh, W. R. (1999).
Filtering Webpages for quality indicators: An empirical approachto finding high quality consumer health informationon the World Wide Web.
Proceedings of the AMIA1999 Annual Symposium.
911-915.Voorhees, E. (2001).
Overview of TREC 2001.
In E.Voorhees (ed.)
NIST Special Publication 500-250:The Tenth Text REtrieval Conference, pp.
1 ?
15.Washington, D.C.Strong, D., Lee, Y., & Wang, R. Y.
(1997).
10 potholesin the road to information quality.
IEEE Computer,30(8), 38-46.Wang, R. Y., & Strong, D. M. (1996).
Beyond accu-racy: What data quality means to data consumers.Journal of Management Information Systems, 12(4),5-34.Zhu, X., & Gauch, S. (2000).
Incorporating quality met-rics in centralized/distributed information retrieval onthe World Wide Web.
Proceedings of the Twenty-Third Annual International ACM SIGIR Conferenceon Research and Development in Information Re-trieval, 288-295.
