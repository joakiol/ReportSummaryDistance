Inducing a Semantically Annotated Lexiconvia EM-Based ClusteringMats  RoothSte fan  R iez le rDet le f  P rescherG lenn  Car ro l lF ranz  Bei lInstitut ffir Maschinelle SprachverarbeitungUniversity of Stuttgart,  GermanyAbstractWe present a technique for automatic inductionof slot annotations for subcategorization frames,based on induction of hidden classes in the EMframework of statistical estimation.
The modelsare empirically evalutated by a general decisiontest.
Induction of slot labeling for subcategoriza-tion frames is accomplished by a further applica-tion of EM, and applied experimentally onframeobservations derived from parsing large corpora.We outline an interpretation of the learned rep-resentations a theoretical-linguistic decomposi-tional lexical entries.1 IntroductionAn important challenge in computational lin-guistics concerns the construction of large-scalecomputational lexicons for the numerous natu-ral languages where very large samples of lan-guage use are now available.
Resnik (1993) ini-tiated research into the automatic acquisitionof semantic selectional restrictions.
Ribas (1994)presented an approach which takes into accountthe syntactic position of the elements whose se-mantic relation is to be acquired.
However, thoseand most of the following approaches require asa prerequisite a fixed taxonomy of semantic rela-tions.
This is a problem because (i) entailmenthierarchies are presently available for few lan-guages, and (ii) we regard it as an open ques-tion whether and to what degree xisting designsfor lexical hierarchies are appropriate for repre-senting lexical meaning.
Both of these consid-erations uggest he relevance of inductive andexperimental pproaches to the construction oflexicons with semantic information.This paper presents a method for automaticinduction of semantically annotated subcatego-rization frames from unannotated corpora.
Weuse a statistical subcat-induction system whichestimates probability distributions and corpusfrequencies for pairs of a head and a subcatframe (Carroll and Rooth, 1998).
The statisticalparser can also collect frequencies for the nomi-nal fillers of slots in a subcat frame.
The induc-tion of labels for slots in a frame is based uponestimation of a probability distribution over tu-ples consisting of a class label, a selecting head,a grammatical relation, and a filler head.
Theclass label is treated as hidden data in the EM-framework for statistical estimation.2 EM-Based C lus ter ingIn our clustering approach, classes are deriveddirectly from distributional data--a sample ofpairs of verbs and nouns, gathered by pars-ing an unannotated corpus and extracting thefillers of grammatical relations.
Semantic lassescorresponding to such pairs are viewed as hid-den variables or unobserved data in the contextof maximum likelihood estimation from incom-plete data via the EM algorithm.
This approachallows us to work in a mathematically well-defined framework of statistical inference, i.e.,standard monotonicity and convergence r sultsfor the EM algorithm extend to our method.The two main tasks of EM-based clustering arei) the induction of a smooth probability modelon the data, and ii) the automatic discovery ofclass-structure in the data.
Both of these aspectsare respected in our application of lexicon in-duction.
The basic ideas of our EM-based clus-tering approach were presented in Rooth (Ms).Our approach constrasts with the merely heuris-tic and empirical justification of similarity-basedapproaches to clustering (Dagan et al, to ap-pear) for which so far no clear probabilisticinterpretation has been given.
The probabilitymodel we use can be found earlier in Pereiraet al (1993).
However, in contrast o this ap-104Class  17PROB 0.02650.04370 .03020 .03440 .03370 .03290 .02570 .01960 .01770 .01690 .01560 .013410 .01290 .01200 .01020 .00990 .00990 .00880 .00880 .00800 .0078increase.as:si nc rease .aso :ofa l l .as :spay .aso :oreduce.aso:or i se .as :sexceed.aso:oexceed.aso:saf fec t .aso :ogrow.as :sinc lude.aso:sreach .aso :sdecl ine.as:slose.aso:oact .aso :simprove .aso :oinc lude .aso :ocut .aso :oshow.aso :ovary .as :so~~ ~ .~.~ ~ o ~ .
~": : : : : : : : : : : : :  : : :  : : :  : : " : ' : :  : : .?
?
?
?
?
?
s ?
?
?
?
s s ?
s ?
?
?
?
?
?
?
??
?
s ?
?
?
s ?
s ?
s s s s s ??
?
?
?
?
?
?
?
s ?
?
?
?
?
?
?
s ?
?
?
?
?
?
?
?
?
?
?
??
?
s ?
?
?
?
?
?
?
?
?
s ?
?
?
?
s ?
?
o ?
?
?
??
?
s ?
?
s ?
?
?
?
?
?
?
?
?
?
s s ?
?
?
s ?s s ?
?
?
?
s ?
?
?
?
s ?
s ?
?
?
?
?
?
s ?
?
s s?
?
?
?
?
?
?
s s s ?
?
?
?
?
s ?
?
s ?
s s ?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?
s ?
?
s ?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
s ?
?
?
?
?
??
?
?
?
?
?
s ?
?
?
?
?
?
?
s s ?
?
??
?
?
?
s ?
?
?
?
?
?
?
?
s ?
?
s s ?
?
?
?
??
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
s ?
??
s ?
?
s s ?
?
?
?
s ?
s ?
s ?
?
?
?
s ?
?
?
?
?
s ?
?
?1: '11 :1  .
.
.
.
.
.
.
.
.
.
?
?
?
?
?
?
?
?
?
?
?
?
s ?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
?Figure 1: Classproach, our statistical inference method for clus-tering is formalized clearly as an EM-algorithm.Approaches to probabilistic lustering similar toours were presented recently in Saul and Pereira(1997) and Hofmann and Puzicha (1998).
Therealso EM-algorithms for similar probability mod-els have been derived, but applied only to sim-pler tasks not involving a combination of EM-based clustering models as in our lexicon induc-tion experiment.
For further applications of ourclustering model see Rooth et al (1998).We seek to derive a joint distribution of verb-noun pairs from a large sample of pairs of verbsv E V and nouns n E N. The key idea is to viewv and n as conditioned on a hidden class c E C,where the classes are given no prior interpreta-tion.
The semantically smoothed probability ofa pair (v, n) is defined to be:p(v,n) = ~~p(c ,v ,n )= ~-'\]p(c)p(vJc)p(nJc)cEC cECThe joint distribution p(c,v,n) is defined byp(c, v, n) = p(c)p(vlc)p(n\[c ).
Note that by con-struction, conditioning of v and n on each otheris solely made through the classes c.In the framework of the EM algorithm(Dempster et al, 1977), we can formalize clus-tering as an estimation problem for a latent class(LC) model as follows.
We are given: (i) a sam-ple space y of observed, incomplete data, corre-17: scalar changesponding to pairs from VxN,  (ii) a sample spaceX of unobserved, complete data, correspondingto triples from CxYxg,  (iii) a set X(y)  = {x EX \[ x = (c, y), c E C} of complete data relatedto the observation y, (iv) a complete-data speci-fication pe(x), corresponding to the joint proba-bility p(c, v, n) over C x V x N, with parameter-vector 0 : (0c, Ovc, OncJc E C, v e V, n E N), (v)an incomplete data specification Po(Y) which isrelated to the complete-data specification as themarginal probability Po(Y) -- ~~X(y)po(x).
"The EM algorithm is directed at finding avalue 0 of 0 that maximizes the incomplete-data log-likelihood function L as a func-tion of 0 for a given sample y ,  i.e., 0 =arg max L(O) where L(O) = lnl-IyP0(y ).0As prescribed by the EM algorithm, the pa-rameters of L(e) are estimated indirectly by pro-ceeding iteratively in terms of complete-data es-timation for the auxiliary function Q(0;0(t)),which is the conditional expectation of thecomplete-data log-likelihood lnps(x) given theobserved ata y and the current fit of the pa-rameter values 0 (t) (E-step).
This auxiliary func-tion is iteratively maximized as a function ofO (M-step), where each iteration is defined bythe map O(t+l) = M(O(t) = argmax Q(O; 0 (t))0Note that our application is an instance of theEM-algorithm for context-free models (Baum et105Class  5PROB 0 .04120 .05420 .03400 .02990 .02870 .02640 .02130 .02070 .01670 .01480 .01410 .01330 .01210 .01100 .01060 .01040 .00940 .00920.00890.00830 .0083~g ?~ggo o (Dggggo cD o o~ggggg~gg~Sgggggggg~g~ .D m~k.as :s  Q ?
?
..... :11111: :  11 :th ink ,as :s  ?
?
?
?
?
?
?
?
?
?
?shake .aso :s  ?
?
?
?
?
?
?
?
?
?
?
?
?smi le .as :s  ?
?
..... 1:  : 11:1 :1 : : .rep ly .as :s  ?
?shrug  .
.
.
.
.
: : : : : : : : : ?
: :wonder .as :s  ?
?
?
?
?
?
?
?
?fee l .aso :s  ?
?
?
?
?
?
?
?
?take .aso :s  ?
?
?
?
.... :1111.  :11  :watch .aso :s  ?
?
?
?
?
?
?
?
?
?
?ask.aso:s  ?
?
?
?
?
?
?
?
?
?
?
?
?
?te l l .aso :s  ?
?
?
?
?
?
?
?
?
?
?
?
?look.as:s ?
?
?
?
?
?
?
?
?
?
?~ ive .~so:s  ?
?
?
?
?
?
?
?
?
?
?hear .aso :s  ?
?
?
?
?
?
?
?
?
?grin.as:s ?
?
?
?
?
?
?
?
?
?
?
?answer .as :s  ?
?
?
?
?
?
?
?
?
?_ .~  o ~ .
.~  ~: : : ' ' : : : : .
: : : : : :?
?
?
?
?
?
Q ?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?1111:11: :1 .1 :11 :?
~ ?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
?
: ' : ' : ' : : : : : . '
: : :?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
??
?
?
?
?
?
t ?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
??
?
?
?
?
?
?
?
?
?
?
?Figure 2: Class 5: communicative actional., 1970),  from which the following particular-ily simple reestimation formulae can be derived.Let x = (c, y)  for fixed c and y. ThenM(Ovc)  = Evetv)?g Po( lY)Eypo( ly) 'M(On~) = F'vcY?{n}P?
(xiy)Eyp0( ly) 'E po( ly)lYlprobabilistic context-free grammar of (Carrolland Rooth, 1998) gave for the British NationalCorpus (117 million words).e67o55Intuitively, the conditional expectation of thenumber of times a particular v, n, or c choiceis made during the derivation is prorated by theconditionally expected total number of times achoice of the same kind is made.
As shown byBaum et al (1970), these expectations can becalculated efficiently using dynamic program-ming techniques.
Every such maximization stepincreases the log-likelihood function L, and a se-quence of re-estimates eventually converges to a(local) maximum of L.In the following, we will present some exam-ples of induced clusters.
Input to the clusteringalgorithm was a training corpus of 1280715 to-kens (608850 types) of verb-noun pairs partici-pating in the grammatical relations of intransi-tive and transitive verbs and their subject- andobject-fillers.
The data were gathered from themaximal-probability parses the head-lexicalizedFigure 3: Evaluation of pseudo-disambiguationFig.
2 shows an induced semantic class out ofa model with 35 classes.
At the top are listed the20 most probable nouns in the p(nl5 ) distribu-tion and their probabilities, and at left are the 30most probable verbs in the p(vn5) distribution.
5is the class index.
Those verb-noun pairs whichwere seen in the training data appear with a dotin the class matrix.
Verbs with suffix .
as  : s in-dicate the subject slot of an active intransitive.Similarily .
ass  : s denotes the subject slot of anactive transitive, and .
ass  : o denotes the objectslot of an active transitive.
Thus v in the abovediscussion actually consists of a combination ofa verb with a subcat frame slot as  : s ,  ass  : s ,or ass  : o.
Induced classes often have a basisin lexical semantics; class 5 can be interpreted106as clustering agents, denoted by proper names,"man", and "woman", together with verbs denot-ing communicative action.
Fig.
1 shows a clus-ter involving verbs of scalar change and thingswhich can move along scales.
Fig.
5 can be in-terpreted as involving different dispositions andmodes of their execution.3 Eva luat ion  of  C lus ter ing  Mode ls3.1 Pseudo-Disambiguat ionWe evaluated our clustering models on a pseudo-disambiguation task similar to that performedin Pereira et al (1993), but differing in detail.The task is to judge which of two verbs v andv ~ is more likely to take a given noun n as itsargument where the pair (v, n) has been cut outof the original corpus and the pair (v ~, n) is con-structed by pairing n with a randomly chosenverb v ~ such that the combination (v ~, n) is com-pletely unseen.
Thus this test evaluates how wellthe models generalize over unseen verbs.The data for this test were built as follows.We constructed an evaluation corpus of (v, n, v ~)triples by randomly cutting a test corpus of 3000(v, n) pairs out of the original corpus of 1280712tokens, leaving a training corpus of 1178698 to-kens.
Each noun n in the test corpus was com-bined with a verb v ~ which was randomly cho-sen according to its frequency such that the pair(v ~, n) did appear neither in the training nor inthe test corpus.
However, the elements v, v ~, andn were required to be part of the training corpus.Furthermore, we restricted the verbs and nounsin the evalutation corpus to the ones which oc-cured at least 30 times and at most 3000 timeswith some verb-functor v in the training cor-pus.
The resulting 1337 evaluation triples wereused to evaluate a sequence of clustering modelstrained from the training corpus.The clustering models we evaluated were?
parametrized in starting values of the trainingalgorithm, in the number of classes of the model,and in the number of iteration steps, resultingin a sequence of 3 ?
10 x 6 models.
Startingfrom a lower bound of 50 % random choice, ac-curacy was calculated as the number of timesthe model decided for p(nlv) > p(nlv' ) out of allchoices made.
Fig.
3 shows the evaluation resultsfor models trained with 50 iterations, averagedover starting values, and plotted against classcardinality.
Different starting values had an ef-76Figure 4: Evaluation on smoothing taskfect of + 2 % on the performance of the test.We obtained a value of about 80 % accuracy formodels between 25 and 100 classes.
Models withmore than 100 classes show a small but stableoverfitting effect.3.2 Smoothing PowerA second experiment addressed the smoothingpower of the model by counting the number of(v, n) pairs in the set V x N of all possible combi-nations of verbs and nouns which received a pos-itive joint probability by the model.
The V x N-space for the above clustering models includedabout 425 million (v, n) combinations; we ap-proximated the smoothing size of a model byrandomly sampling 1000 pairs from V x N andreturning the percentage of positively assignedpairs in the random sample.
Fig.
4 plots thesmoothing results for the above models againstthe number of classes.
Starting values had an in-fluence of -+ 1% on performance.
Given the pro-portion of the number of types in the trainingcorpus to the V ?
N-space, without clusteringwe have a smoothing power of 0.14 % whereasfor example a model with 50 classes and 50 it-erations has a smoothing power of about 93 %.Corresponding to the maximum likelihoodparadigm, the number of training iterations hada decreasing effect on the smoothing perfor-mance whereas the accuracy of the pseudo-disambiguation was increasing in the number ofiterations.
We found a number of 50 iterationsto be a good compromise in this trade-off.4 Lex icon  Induct ion  Based  onLatent  C lassesThe goal of the following experiment was to de-rive a lexicon of several hundred intransitive andtransitive verbs with subcat slots labeled withlatent classes.1074.1 Probabil ist ic Labeling with LatentClasses us ing EM-est imat ionTo induce latent classes for the subject slot ofa fixed intransitive verb the following statisti-cal inference step was performed.
Given a la-tent class model PLC(') for verb-noun pairs, anda sample n l , .
.
.
,aM of subjects for a fixed in-transitive verb, we calculate the probability ofan arbitrary subject n E N by:p(n)  =  _,P(C)PLc(nlc).cEC cCCThe estimation of the parameter-vector 0 =(Oclc E C) can be formalized in the EM frame-work by viewing p(n) or p(c, n) as a function of0 for fixed PLC(.).
The re-estimation formulaeresulting from the incomplete data estimationfor these probability functions have the follow-ing form (f(n) is the frequency of n in the sam-ple of subjects of the fixed verb):M(Oc) = EneN f(n)po(cln)E, elv f (?%)A similar EM induction process can be appliedalso to pairs of nouns, thus enabling induction oflatent semantic annotations for transitive verbframes.
Given a LC model PLC(') for verb-nounpairs, and a sample (nl,n2)l,..., (nl,n2)M ofnoun arguments (ni subjects, and n2 direct ob-jects) for a fixed transitive verb, we calculate theprobability of its noun argument pairs by:p(7%1, ?%2) = Ec,,c  c p(cl, c2, ?%1, ?%2)---- E c1 ,c2 6C P ( C1' C2 )PLC (?% 11cl )pLc (7%21c~)Again, estimation of the parameter-vector0 = (0clc210,c2 E C) can be formalizedin an EM framework by viewing p(nl,n2) orp(cl,c2,nl,n2) as a function of 0 for fixedPLC(.).
The re-estimation formulae resultingfrom this incomplete data estimation problemhave the following simple form (f(nz, n2) is thefrequency of (n!, n2) in the sample of noun ar-gument pairs of the fixed verb):M(Od~2) = Enl,n2eN f(7%1, n2)po(cl, c21nl, n2)Enl,   N Y(7%1, ?%2)Note that the class distributions p(c) andp(cl,C2) for intransitive and transitive modelscan be computed also for verbs unseen in theLC model.blush 5 0.982975 snarl 5 0.962094constance 3christina 3willie 2.99737ronni 2claudia 2gabriel 2maggie 2bathsheba 2sarah 2girl 1.9977mandeville 2jinkwa 2man 1.99859scott 1.99761omalley 1.99755shamlou 1angalo 1corbett 1southgate 1ace 1Figure 6: Lexicon entries: blush, snarlincrease 17 0.923698number 134.147demand 30.7322pressure 30.5844temperature 25.9691cost 23.9431proportion 23.8699size 22.8108rate 20.9593level 20.7651price 17.9996Figure 7: Scalar motion increase.4.2 Lexicon Induct ion Exper imentExperiments used a model with 35 classes.
Frommaximal probability parses for the British Na-tional Corpus derived with a statistical parser(Carroll and Rooth, 1998), we extracted fre-quency tables for intransitve verb/subject pairsand transitive verb/subject/object triples.
The500 most frequent verbs were selected for slotlabeling.
Fig.
6 shows two verbs v for whichthe most probable class label is 5, a classwhich we earlier described as communicative ac-tion, together with the estimated frequencies off(n)po(cln ) for those ten nouns n for which thisestimated frequency is highest.Fig.
7 shows corresponding data for an intran-sitive scalar motion sense of increase.Fig.
8 shows the intransitive verbs which take17 as the most probable label.
Intuitively, theverbs are semantically coherent.
When com-pared to Levin (1993)'s 48 top-level verb classes,we found an agreement of our classification withher class of "verbs of changes of state" except forthe last three verbs in the list in Fig.
8 which issorted by probability of the class label.Similar results for German intransitive scalarmotion verbs are shown in Fig.
9.
The datafor these experiments were extracted from themaximal-probability parses of a 4.1 million word108Class  8PROB 0 .0369 o o o  o o o o o o o o o oo ~ o  ~ 0 ~o o o o o o  o o o o o o o  o o o o0.05390 .04690 .04390 .03830 .02700 .02550 .01920 .01890 .01790 .01620 .01500 .01400 .01380 .01090 .01090 .00970 .00920.0091requ i re .aso :oshow,aso :oneed ,aso :oinvo lve .aso :oproduce .aso :ooccur .as :scause .aso :scause .aso :oa f fec t .aso :srequ i re .aso :smean.aso :osuggest .aso :oproduce .aso :sdemand.aso :oreduce .aso :sre f lec t .aso :oinvo lve .aso :sundergo .aso ;o: : : :1111111:!O  ?
?
?
: : : : : : : : : : : : : :: : :1 : .
.
.
:  " .
.
::1 .1 .1111"11  :: : : "  : .
.
??
?
?
?
?
?
?
$ ?
$ ??
?
?
?
?
?
?
?
: :1 .11  :1 : '1?
?
?
?
?
?
?
?
?
?
?
?Figure 5: Class 8: dispositions0.9779920.9480990.9236980.9083780.8773380.8760830.8034790.6724090.583314decreasedoubleincreasedeclinerisesoarfallslowdiminish0.5607270.4765240.428420.3655860.3653740.2927160.2801830.238182dropgrowvaryimproveclimbflowcutmount0.741467 ansteigen0.720221 steigen0.693922 absinken0.656021 sinken0.438486 schrumpfen0.375039 zuriickgehen0.316081 anwachsen0.215156 stagnieren0.160317 wachsen0.154633 hinzukommen(go up)(rise)(sink)(go down)(shrink)(decrease)(increase)(stagnate)(grow)(be added)Figure 8: Scalar motion verbscorpus of German subordinate clauses, yielding418290 tokens (318086 types) of pairs of verbsor adjectives and nouns.
The lexicalized proba-bilistic grammar for German used is describedin Beil et al (1999).
We compared the Ger-man example of scalar motion verbs to the lin-guistic classification of verbs given by Schuh-macher (1986) and found an agreement of ourclassification with the class of "einfache An-derungsverben" (simple verbs of change) exceptfor the verbs anwachsen (increase) and stag-nieren(stagnate) which were not classified thereat all.Fig.
i0 shows the most  probable pair of classesfor increase as a transitive verb, together withestimated frequencies for the head filler pair.Note  that the object label 17 is the class foundwith intransitive scalar mot ion  verbs; this cor-respondence is exploited in the next section.Figure 9: German intransitive scalar motionverbsincrease (8, 17) 0.3097650development - pressurefat - riskcommunication - awarenesssupplementation - concentrationincrease- number2.30552.118072.042271.989181.80559Figure 10: Transitive increase with estimatedfrequencies for filler pairs.5 L ingu is t ic  In terpreta t ionIn some linguistic accounts, multi-place verbsare decomposed into representations involv-ing (at least) one predicate or relationper argument.
For instance, the transitivecausative/inchoative verb increase, is composedof an actor/causative verb combining with a109VP / ~  VPANP vl  NP V1 NP V lVP  V VP  V VP  VANP V NP V NP Vincrease Riz R.,v ^ increase,vVPNP VIRlr A increase~vFigure 11: First tree: linguistic lexical entry fortransitive verb increase.
Second: correspondinglexical entry with induced classes as relationalconstants.
Third: indexed open class root addedas conjunct in transitive scalar motion increase.Fourth: induced entry for related intransitive in-crease.one-place predicate in the structure on the left inFig.
11.
Linguistically, such representations aremotivated by argument alternations (diathesis),case linking and deep word order, language ac-quistion, scope ambiguity, by the desire to repre-sent aspects of lexical meaning, and by the factthat in some languages, the postulated ecom-posed representations are overt, with each primi-tive predicate corresponding to a morpheme.
Forreferences and recent discussion of this kind oftheory see Hale and Keyser (1993) and Kural(1996).We will sketch an understanding of the lexi-cal representations i duced by latent-class label-ing in terms of the linguistic theories mentionedabove, aiming at an interpretation which com-bines computational leaxnability, linguistic mo-tivation, and denotational-semantic adequacy.The basic idea is that latent classes are compu-tational models of the atomic relation symbolsoccurring in lexical-semantic representations.
Aa first implementation, consider replacing the re-lation symbols in the first tree in Fig.
11 withrelation symbols derived from the latent class la-beling.
In the second tree in Fig 11, R17 and R8are relation symbols with indices derived fromthe labeling procedure of Sect.
4.
Such represen-tations can be semantically interpreted in stan-dard ways, for instance by interpreting relationsymbols as denoting relations between eventsand individuals.Such representations are semantically inad-equate for reasons given in philosophical cri-tiques of decomposed linguistic representations;see Fodor (1998) for recent discussion.
A lex-icon' estimated in the above way has as manyprimitive relations as there are latent classes.
Weguess there should be a few hundred classes in anapproximately complete lexicon (which wouldhave to be estimated from a corpus of hun-dreds of millions of words or more).
Fodor's ar-guments, which axe based on the very limited de-gree of genuine interdefinability of lexical itemsand on Putnam's arguments for contextual de-termination oflexical meaning, indicate that thenumber of basic concepts has the order of mag-nitude of the lexicon itself.
More concretely, alexicon constructed along the above principleswould identify verbs which are labelled with thesame latent classes; for instance it might identifythe representations of grab and touch.For these reasons, a semantically adequatelexicon must include additional relational con-stants.
We meet this requirement in a simpleway, by including as a conjunct a unique con-stant derived from the open-class root, as inthe third tree in Fig.
11.
We introduce index-ing of the open class root (copied from the classindex) in order that homophony of open classroots not result in common conjuncts in seman-tic representations--for instance, we don't wantthe two senses of decline exemplified in declinethe proposal and decline five percent o have ancommon entailment represented by a commonconjunct.
This indexing method works as longas the labeling process produces different latentclass labels for the different senses.The last tree in Fig.
11 is the learned represen-tation for the scalar motion sense of the intran-sitive verb increase.
In our approach, learningthe argument alternation (diathesis) relating thetransitive increase (in its scalar motion sense)to the intransitive increase (in its scalar motionsense) amounts to learning representations witha common component R17 A increase17.
In thiscase, this is achieved.6 Conc lus ionWe have proposed a procedure which mapsobservations of subcategorization frames withtheir complement fillers to structured lexicalentries.
We believe the method is scientificallyinteresting, practically useful, and flexible be-cause:1.
The algorithms and implementation are ef-ficient enough to map a corpus of a hundredmillion words to a lexicon.1102.
The model and induction algorithm havefoundations in the theory of parameter-ized families of probability distributionsand statistical estimation.
As exemplifiedin the paper, learning, disambiguation, andevaluation can be given simple, motivatedformulations.3.
The derived lexical representations are lin-guistically interpretable.
This suggests thepossibility of large-scale modeling and ob-servational experiments bearing on ques-tions arising in linguistic theories of the lex-icon.4.
Because a simple probabilistic model isused, the induced lexical entries could beincorporated in lexicalized syntax-basedprobabilistic language models, in particularin head-lexicalized models.
This providesfor potential application in many areas.5.
The method is applicable to any naturallanguage where text samples of sufficientsize, computational morphology, and a ro-bust parser capable of extracting subcate-gorization frames with their fillers are avail-able.ReferencesLeonard E. Baum, Ted Petrie, George Soules,and Norman Weiss.
1970.
A maximiza-tion technique occuring in the statisticalanalysis of probabilistic functions of Markovchains.
The Annals of Mathematical Statis-tics, 41(1):164-171.Franz Beil, Glenn Carroll, Detlef Prescher, Ste-fan Riezler, and Mats Rooth.
1999.
Inside-outside estimation of a lexicalized PCFG forGerman.
In Proceedings of the 37th AnnualMeeting of the A CL, Maryland.Glenn Carroll and Mats Rooth.
1998.
Valenceinduction with a head-lexicalized PCFG.
InProceedings of EMNLP-3, Granada.Ido Dagan, Lillian Lee, and Fernando Pereira.to appear.
Similarity-based models of wordcooccurence probabilities.
Machine Learning.A.
P. Dempster, N. M. Laird, and D. B. Rubin.1977.
Maximum likelihood from incompletedata via the EM algorithm.
Journal of theRoyal Statistical Society, 39(B):1-38.Jerry A. Fodor.
1998.
Concepts : Where Cogni-tire Science Went Wrong.
Oxford CognitiveScience Series, Oxford.K.
Hale and S.J.
Keyser.
1993.
Argument struc-ture and the lexical expression of syntactic re-lations.
In K. Hale and S.J.
Keyser, editors,The View from Building 20.
MIT Press, Cam-bridge, MA.Thomas Hofmann and Jan Puzicha.
1998.
Un-supervised learning from dyadic data.
Tech-nical Report TR-98-042, International Com-puter Science Insitute, Berkeley, CA.Murat Kural.
1996.
Verb Incorporation and El-ementary Predicates.
Ph.D. thesis, Universityof California, Los Angeles.Beth Levin.
1993.
English Verb Classesand Alternations.
A Preliminary Investiga-tion.
The University of Chicago Press,Chicago/London.Fernando Pereira, Naftali Tishby, and LillianLee.
1993.
Distributional clustering of en-glish words.
In Proceedings of the 31th AnnualMeeting of the A CL, Columbus, Ohio.Philip Resnik.
1993.
Selection and information:A class-bases approach to lexical relationships.Ph.D.
thesis, University of Pennsylvania, CISDepartment.Francecso Ribas.
1994.
An experiment on learn-ing appropriate selectional restrictions from aparsed corpus.
In Proceedings of COLING-9~,Kyoto, Japan.Mats Rooth, Stefan Riezler, Detlef Prescher,Glenn Carroll, and Franz Beil.
1998.
EM-based clustering for NLP applications.
InInducing Lexicons with the EM Algorithm.AIMS Report 4(3), Institut fiir MaschinelleSprachverarbeitung, Universit~t Stuttgart.Mats Rooth.
Ms. Two-dimensional c usters ingrammatical relations.
In Symposium on Rep-resentation and Acquisition of Lexical Knowl-edge: Polysemy, Ambiguity, and Generativity.AAAI 1995 Spring Symposium Series, Stan-ford University.Lawrence K. Saul and Fernando Pereira.
1997.Aggregate and mixed-order Markov modelsfor statistical anguage processing.
In Pro-ceedings of EMNLP-2.Helmut Schuhmacher.
1986.
Verben in Feldern.Valenzw5rterbuch zur Syntax und Semantikdeutscher Verben.
de Gruyter, Berlin.111
