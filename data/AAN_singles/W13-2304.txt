Proceedings of the 7th Linguistic Annotation Workshop & Interoperability with Discourse, pages 28?32,Sofia, Bulgaria, August 8-9, 2013. c?2013 Association for Computational LinguisticsInfluence of preprocessing on dependency syntax annotation: speed andagreementArne Skj?rholtDepartment of Informatics, University of Osloarnskj@ifi.uio.noAbstractWhen creating a new resource, prepro-cessing the source texts before annotationis both ubiquitous and obvious.
How thepreprocessing affects the annotation effortfor various tasks is for the most part anopen question, however.
In this paper,we study the effects of preprocessing onthe annotation of dependency corpora andhow annotation speed varies as a functionof the quality of three different parsers andcompare with the speed obtained whenstarting from a least-processed baseline.We also present preliminary results con-cerning the effects on agreement based ona small subset of sentences that have beendoubly-annotated.11 IntroductionIt is commonly accepted wisdom in treebankingthat it is preferable to preprocess data before PoSand syntax annotation, rather than having annota-tors work from raw text.
However, the impact ofpreprocessing is not well studied and factors suchas the lower bound on performance for preprocess-ing to be useful and the return on investment ofincreased performance are largely unknown.Corpora and applications based on dependencysyntax have become increasingly popular in recentyears, and many new corpora are being created.
Inthis work we investigate the task of syntactic anno-tation based on dependency grammar, and how an-notation speed and inter-annotator agreement areinfluenced by parser performance.
Our study isperformed in the context of the annotation effortcurrently under way at the national library of Nor-way, tasked with creating a freely available syn-tactically annotated corpus of Norwegian.
It is thefirst widely available such corpus.1Code and data used to obtain these results is available athttps://github.com/arnsholt/law7-annotation1.1 Related workThe Penn Treebank project (Marcus et al 1993)had annotators correct automatically parsed andPoS-tagged data, and they report that correctingrather than annotating from scratch is massivelyhelpful in the PoS annotation task (from scratchtook twice as long and increased error rate and dis-agreement by 50%), but unfortunately there is nosuch comparison for the syntactic bracketing task.The task of PoS annotation has been studied fur-ther by Fort and Sagot (2010), who establish thelower bound on tagger accuracy to be in the rangeof 60?80% for the preprocessing to be useful.For the task of syntactic bracketing, Chiou etal.
(2001) investigated some facets of the prob-lem while developing the Penn Chinese treebankand found that when using a parser with a labelledF1 = 76.04, the time spent correcting is 58% of thetime spent on unassisted annotation, and a furtherimproved parser (F1 = 82.14) reduces the time to50% of that used by unassisted annotation.2 Experimental protocolIn this section we outline the key methodologicalchoices made for our experiments.
First we dis-cuss what timing data we collect and the texts an-notated, before describing the preprocessors used.Environment For our experiments, four differ-ent texts were chosen for annotation: two fromthe Aftenposten (AP 06 & AP 08), and two fromDagbladet (DB 12 & DB 13), both daily news-papers.
Key statistics for the four texts are givenin Table 1.
The annotation effort uses the TREDtool2, originally created for the Prague Depen-dency Treebank project.
It is easily extended, andthus we used these facilities to collect the timingdata.
To minimise interference with the annota-tors, we simply recorded the time a sentence wasshown on screen and accounted for outliers causedby breaks and interruptions in the analysis.The annotation work is done by two annotators,Odin and Thor.
Both are trained linguists, and2http://ufal.mff.cuni.cz/tred/28Text n ?
sAP 06 373 17.0 10.8AP 08 525 16.5 9.11DB 12 808 12.1 8.47DB 13 648 14.6 9.15Total 2354 34223 tokensTable 1: Statistics of the annotated texts.
n num-ber of sentences, ?
mean length, s length standarddeviation.are full-time employees of the National Librarytasked with annotating the corpus.
The only ad-ditional instruction given to the annotators in con-junction with the experiment was that they try toclose the TRED program when they know that theywere going away for a long time, in order to min-imise the number of outliers.
The actual annota-tion proceeded as normal according to the anno-tation guidelines3.
Thor annotated AP 08 and DB13, while Odin annotated AP 06 and DB 12 as wellas the first 400 sentences of DB 13 for the purposesof measuring annotator agreement.Preprocessing In our experiments, we considerthree different statistical parsers as preprocessorsand compare these to a minimally preprocessedbaseline.
Unfortunately, it was impossible to gettiming data for completely unannotated data, asTRED requires its input to be a dependency tree.For this reason our minimal preprocessing, we callit the caterpillar strategy, is attaching each word tothe previous word, labelled with the most frequentdependency relation.Of the three statistical parsers, one is traineddirectly on already annotated Norwegian data re-leased by the treebank project (version 0.2) andthe other two are cross-lingual parsers trained onconverted Swedish and Danish data using the tech-niques described in Skj?rholt and ?vrelid (2012).In brief, this technique involves mapping the PoSand dependency relation tagsets of the source cor-pora into the corresponding tagsets of the targetrepresentation, and applying structural transfor-mations to bring the syntactic analyses into asclose a correspondence as possible with the tar-get analyses.
It was also shown that for lan-guages as closely related as Norwegian, Danishand Swedish, not delexicalising, contrary to the3Distributed with the corpus at:http://www.nb.no/Tilbud/Forske/Spraakbanken/Tilgjengelege-ressursar/TekstressursarParser UAS LASBaseline 30.8% 3.86%Danish 69.9% 46.7%Swedish 77.7% 68.1%Norwegian 86.6% 83.5%Table 2: Parser performance.
Labelled (LAS) andunlabelled (UAS) attachment scores.standard procedure in cross-lingual parsing (S?-gaard, 2011; Zeman and Resnik, 2008), yields anon-negligible boost in performance.All three parsers are trained using MaltParser(Nivre et al 2007) using the liblinear learner andthe nivreeager parsing algorithm with default set-tings.
The Norwegian parser is trained on the first90% of the version 0.2 release of the Norwegiandependency treebank with the remaining 10% heldout for evaluation, while the cross-lingual parsersare trained on the training sets of Talbanken05(Nivre et al 2006) and the Danish DependencyTreebank (Kromann, 2003) as distributed for theCoNLL-X shared task.
The parser trained onSwedish data is lexicalised, while the one trainedon Danish used a delexicalised corpus.The performance of the four different prepro-cessing strategies is summarised in Table 2.
Thenumbers are mostly in line with those reportedin Skj?rholt and ?vrelid (2012), with a drop ofa few percentage points in both LAS and UASfor all parsers, except for a gain of more than 5points LAS for the Danish parser, due to the fixedrelation labels.
There are three reasons for thedifferences: First of all, the test corpus is differ-ent; Skj?rholt and ?vrelid (2012) used the ver-sion 0.1 release of the Norwegian corpus, whilewe use version 0.2.
Secondly, TRED requires thatits input trees only have a single child of the rootnode, while MaltParser will attach unconnectedsubgraphs to the root node if the graph producedafter consuming the whole input isn?t connected.Finally, TRED validates dependency relation la-bels strictly, which revealed a few bugs in theconversion script for the Danish data.
A post-processing script corrects the invalid relations andattaches multiple children of the root node to themost appropriate child of the root.The texts given to the annotators were an amal-gam of the outputs of the four parsers, suchthat each block of ten sentences comes from thesame parser.
Each chunk was randomly assigned290501001502002502 4 6 8 10 12 14 16 18 20Mediantime(s)Sentence lengthCaterpillarDanishNorwegianSwedishFigure 1: Median annotation time, Odin.to a parser, in such a way that 5 chunks wereparsed with the baseline strategy and the remain-ing chunks were evenly distributed between theremaining three parsers.
This strategy ensuresas even a distribution between parsers as possi-ble, while keeping the annotators blind to parserassignments.
We avoid the annotators knowingwhich parser was used, as this could subcon-ciously bias their behaviour.3 ResultsSpeed To compare the different parsers as pre-processors for annotation, we need to apply a sum-mary statistic across the times for each annotator,binned by sentence length.
We use the median,which is highly resistant to outliers and concep-tually simpler than strategies for outlier elimina-tion4.
Furthermore, to ensure large enough bins,we only consider sentences of length 20 or less.Figure 1 shows the evolution of annotation timeas a function of sentence length for Odin forall four parsers, and Figure 2 the correspondinggraphs for Thor.
It is clear that, although Odinconsistently uses less time to annotate sentencesthan Thor, the different parsers are ranked identi-cally, and the relative speed-up of the higher qual-ity parsers is similar for both annotators.Agreement To measure agreement we study theLAS and UAS we get from comparing Odin andThor?s annotations.
Artstein and Poesio (2008) ar-gue strongly in favour of using a chance-corrected4Nor does it assume normality, which would be inappro-priate for timing data, unlike most outlier detection methods.01002003004005006007002 4 6 8 10 12 14 16 18 20Mediantime(s)Sentence lengthCaterpillarDanishNorwegianSwedishFigure 2: Median annotation time, Thor.Parser n UAS LASBaseline 10 99.1% 99.1%Danish 130 96.3% 94.0%Swedish 110 96.1% 94.4%Norwegian 150 96.8% 95.3%Table 3: Annotator agreement.
n sentences, unla-belled (UAS) and labelled (LAS) attachment.measure of agreement, but the measures theypresent are applicable to categorical data, notstructured data such as syntactic data.
Thus, sim-ple agreement measures are the standard measuresin syntax (Hajic?, 2004; Miltsakaki et al 2004;Maamouri et al 2008).
As mentioned in Sec-tion 2, only 400 sentences were doubly annotated.Ideally, we would have liked to have all the textsdoubly annotated, but external constraints on theannotation effort limited us to the set at hand.Table 3 shows the unlabelled and unlabelled ac-curacies on the doubly annotated dataset, alongwith the number of sentences in each dataset.
Dueto the random distribution of sentences, only a sin-gle baseline chunk was in the first 400 sentences,making it hard to draw conclusions on the qualityobtained with that strategy.
The imbalance is lesssevere for the other parsers, but the Norwegian setis still almost 50% larger than the Swedish one.The agreement on the baseline set is quite surpris-ing, with only a single token out of 115 receivingdifferent heads and all tokens having the same de-pendency relation.
Unlabelled agreement is lowerby about three percentage points on the three re-maining datasets, with no real variation in terms30of parser performance, and labelled agreement issomewhat lower again, indicating some level ofdisagreement over dependency relations.4 AnalysisOur results are clearest for the question of howtime used to annotate is affected by preprocess-ing quality.
The Danish parser halves the time re-quired to annotate sentences compared to the base-line; already an important gain.
The Norwegianparser cuts the time in half again, with the Swedishparser between the two.
Based on the learningcurves in Skj?rholt and ?vrelid (2012), a parserwith performance equivalent to the Danish parser(70% UAS) can be obtained with about 50 anno-tated sentences, and the 80% UAS of the Swedishparser is reachable with about 200 sentences.Given the limited amount of data available forour study of agreement, it is hard to make solidconclusions, but it does appear that head selec-tion is virtually unchanged by parser performance,while there may be some increase in agreementon dependency relation labels, from 96.0% withthe Danish parser, to 96.5% and 97.1% with theSwedish and Norwegian parsers.
Agreement is ex-tremely high for both heads and labels on the datapreprocessed with the baseline parser, but basedon 10 sentences, it is impossible to say whetherthis is a fluke or a reasonable approximation of thevalue we would get with a larger sample.The unchanged agreement score suggests thatthe annotators are not unduly influenced by a bet-ter parser.
An increase in agreement would not bean unambiguously positive result though; a pos-itive interpretation would be that the annotators?work is closer to the Platonic ideal of a correctanalysis of the corpus, but a less charitable inter-pretation is that the annotators are more biased bythe parser.
Furthermore, the very high agreementfor the baseline parser is potentially worrying ifthe result remains unchanged by a larger sample.This would indicate that in order to get the bestquality annotation, it is necessary to start froma virtually unprocessed corpus, which would re-quire four times as much time as using a 90% UASparser for preprocessing, based on our data.5 ConclusionsGiven the time-consuming nature of linguistic an-notation, higher annotation speed is an obviousgood for any annotation project as long as theannotation quality doesn?t degrade unacceptably.Based on the results obtained in our study, it isclear that the speed-up to be had from a good de-pendency parser is important, to the extent thatwhen annotating it is a very bad idea to not useone.
Further, based on the learning curves pre-sented in Skj?rholt and ?vrelid (2012), it seemsthat parser adaptation with a view to preprocess-ing for annotation is primarily useful in the ear-liest stages of an annotation effort as the learn-ing curves show that once 100 sentences are anno-tated, a parser trained on that data will already becompetitive with a cross-lingual parser for Norwe-gian.
Other languages may require more data, butthe amount required is most likely on the same or-der of magnitude.
If same-language data are avail-able, a parser trained on that may last longer.As regards annotator agreement, our resultsshow that head selection as measured by unla-belled accuracy is unchanged by parser accuracy.Agreement as measured by labelled accuracy in-creases somewhat with increased parser perfor-mance, which indicates that agreement on labelsincreases with parser performance.
The agreementresults for our baseline parser are extremely high,but given that we only have ten sentences to com-pare, it is impossible to say if this is a real differ-ence between the baseline and the other parsers.5.1 Future workThere are a number of things, particularly relat-ing to annotator agreement we would like to in-vestigate further.
Chief of these is the lack of achance corrected agreement measure for depen-dency syntax.
As mentioned previously, no suchmeasure has been formulated as most agreementmeasures are most naturally expressed in terms ofcategorical assignments, which is a bad fit for syn-tax.
However, it should be possible to create anagreement measure suitable for syntax.We would also like to perform a deeper study ofthe effects of preprocessing on agreement using aproper measure of agreement.
The results for ourbaseline strategy are based on extremely little data,and thus it is hard to draw any solid conclusions.We would also like to see if different groups of an-notators are influenced differently by the parsers.Our annotators were both trained linguists, and itwould be interesting to see if using lay annotatorsor undergraduate linguistics students changes theagreement scores.31ReferencesRon Artstein and Massimo Poesio.
2008.
Inter-CoderAgreement for Computational Linguistics.
Compu-tational Linguistics, 34(4):555?596.Fu-Dong Chiou, David Chiang, and Martha Palmer.2001.
Facilitating Treebank Annotation Using a Sta-tistical Parser.
In Proceedings of the first interna-tional conference on Human language technologyresearch, pages 1?4.Kar?n Fort and Beno?t Sagot.
2010.
Influence of Pre-annotation on POS-tagged Corpus Development.
InNianwen Xue and Massimo Poesio, editors, Pro-ceedings of the fourth linguistic annotation work-shop, pages 56?63, Stroudsburg.
Association forComputational Linguistics.Jan Hajic?.
2004.
Complex Corpus Annotation: ThePrague Dependency Treebank.
Jazykovedn?
?stavL?.
?t?ra, SAV.Matthias Trautner Kromann.
2003.
The Danish De-pendency Treebank and the DTAG Treebank Tool.In Proceedings of the 2nd Workshop on Treebanksand Linguistic Theories, pages 217?220.
V?xj?
Uni-versity Press.Mohamed Maamouri, Ann Bies, and Seth Kulick.2008.
Enhancing the Arabic Treebank : A Collabo-rative Effort toward New Annotation Guidelines.
InProceedings of the 6th International Conference onLanguage Resources and Evaluation, pages 3192?3196.
European Language Resources Association.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Eleni Miltsakaki, Rashmi Prasad, Aravind Joshi, andBonnie Webber.
2004.
The Penn Discourse Tree-bank.
In Proceedings of the 4th International Con-ference on Language Resources and Evaluation,pages 2237?2240.
European Language ResourcesAssociation.Joakim Nivre, Jens Nilsson, and Johan Hall.
2006.
Tal-banken05 : A Swedish Treebank with Phrase Struc-ture and Dependency Annotation.
In Proceedings ofthe fifth international conference on Language Re-sources and Evaluation.Joakim Nivre, Jens Nilsson, Johan Hall, AtanasChanev, G?ls?en Eryig?it, Sandra K?bler, SvetoslavMarinov, and Erwin Marsi.
2007.
MaltParser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Arne Skj?rholt and Lilja ?vrelid.
2012.
Impactof treebank characteristics on cross-lingual parseradaptation.
In Iris Hendrickx, Sandra K?bler, andKiril Simov, editors, Proceedings of the 11th inter-national workshop on treebanks and linguistic theo-ries, pages 187?198, Lisbon.
Edi?
?es Colibri.Anders S?gaard.
2011.
Data point selection for cross-language adaptation of dependency parsers.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics, pages 682?686.Daniel Zeman and Philip Resnik.
2008.
Cross-Language Parser Adaptation between Related Lan-guages.
In Anil Kumar Singh, editor, Proceedingsof the IJCNLP-08 Workshop on NLP for Less Priv-ileged Languages, pages 35?42, Hyderabad, India.Asian Federation of Natural Language Processing.32
