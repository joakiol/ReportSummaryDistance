High-Performance Bilingual Text Alignment UsingStatistical and Dictionary InformationMasahiko Haruno Takefumi YamazakiNTT Communication Science Labs.1-2356 Take Yokosuka-ShiKanagawa 238-03, Japanharuno@nttkb, ntt .jp yamazaki?nttkb, ntt .jpAbst ractThis paper describes an accurate androbust text alignment system for struc-turally different languages.
Amongstructurally different languages uch asJapanese and English, there is a limitationon the amount of word correspondencesthat can be statistically acquired.
Theproposed method makes use of two kindsof word correspondences in aligning bilin-gual texts.
One is a bilingual dictionary ofgeneral use.
The other is the word corre-spondences that are statistically acquiredin the alignment process.
Our methodgradually determines sentence pairs (an-chors) that correspond to each other by re-laxing parameters.
The method, by com-bining two kinds of word correspondences,achieves adequate word correspondencesfor complete alignment.
As a result, textsof various length and of various genresin structurally different languages can bealigned with high precision.
Experimen-tal results show our system outperformsconventional methods for various kinds ofJapanese-English texts.1 In t roduct ionCorpus-based approaches based on bilingual textsare promising for various applications(i.e., lexicalknowledge xtraction (Kupiec, 1993; Matsumoto etal., 1993; Smadja et al, 1996; Dagan and Church,1994; Kumano and Hirakawa, 1994; Haruno et al,1996), machine translation (Brown and others, 1993;Sato and Nagao, 1990; Kaji et al, 1992) and infor-mation retrieval (Sato, 1992)).
Most of these worksassume voluminous aligned corpora.Many methods have been proposed to align bilin-gual corpora.
One of the major approaches i  basedon the statistics of simple features uch as sentencelength in words (Brown and others, 1991) or incharacters (Gale and Church, 1993).
These tech-niques are widely used because they can be imple-mented in an efficient and simple way through dy-namic programing.
However, their main targets arerigid translations that are almost literal translations.In addition, the texts being aligned were structurallysimilar European languages (i.e., English-French,English-German).The simple-feature based approaches don't workin flexible translations for structurally different lan-guages uch as Japanese and English, mainly for thefollowing two reasons.
One is the difference in thecharacter types of the two languages.
Japanese hasthree types of characters (Hiragana, Katakana, andKanji), each of which has different amounts of in-formation.
In contrast, English has only one typeof characters.
The other is the grammatical andrhetorical difference of the two languages.
First, thesystems of functional (closed) words are quite differ-ent from language to language.
Japanese has a quitedifferent system of closed words, which greatly influ-ence the length of simple features.
Second, due torhetorical difference, the number of multiple match(i.e., 1-2, 1-3, 2-1 and so on) is more than that amongEuropean languages.
Thus, it is impossible in gen-eral to apply the simple-feature based methods toJapanese-English translations.One alternative alignment method is the lexicon-based approach that makes use of the word-correspondence knowledge of the two languages.
(Church, 1993) employed n-grams hared by two lan-guages.
His method is also effective for Japanese-English computer manuals both containing lots ofthe same alphabetic technical terms.
However,the method cannot be applied to general transla-tions in structurally different languages.
(Kay andRoscheisen, 1993) proposed a relaxation method toiteratively align bilingual texts using the word cor-respondences acquired during the alignment pro-cess.
Although the method works well among Euro-pean languages, the method does not work in align-ing structurally different languages.
In Japanese-English translations, the method does not captureenough word correspondences to permit alignment.As a result, it can align only some of the two texts.This is mainly because the syntax and rhetoric are131greatly differ in the two languages even in literaltranslations.
The number of confident word cor-respondences of words is not enough for completealignment.
Thus, the problem cannot be addressedas long as the method relies only on statistics.
Othermethods in the lexicon-based approach embed lex-ical knowledge into stochastic models (Wu, 1994;Chen, 1993), but these methods were tested usingrigid translations.To tackle the problem, we describe in thispaper a text alignment system that uses bothstatistics and bilingual dictionaries at the sametime.
Bilingual dictionaries are now widelyavailable on-line due to advances in CD-ROMtechnologies.
For example, English-Spanish,English-French, English-German, English-Japanese,Japanese-French, Japanese-Chinese and other dic-tionaries are now commercially available.
It is rea-sonable to make use of these dictionaries in bilingualtext alignment.
The pros and cons of statistics andonline dictionaries are discussed below.
They showthat statistics and on-line dictionaries are comple-mentary in terms of bilingual text alignment.Statistics Mer i t  Statistics is robust in the sensethat it can extract context-dependent usageof words and that it works well even if wordsegmentation 1 is not correct.Statistics Demer i t  The amount of word corre-spondences acquired by statistics is not enoughfor complete alignment.D ict ionar ies  Mer i t  They can contain the infor-mation about words that appear only once inthe corpus.Dictionaries Demer i t  They cannot capturecontext-dependent keywords in the corpus andare weak against incorrect word segmentation.Entries in the dictionaries differ from author toauthor and are not always the same as those inthe corpus.Our system iteratively aligns sentences by usingstatistical and on-line dictionary word correspon-dences.
The characteristics of the system are as fol-lows.?
The system performs well and is robust for var-ious lengths (especially short) and various gen-res of texts.?
The system is very economical because it as-sumes only online-dictionaries of general useand doesn't require the labor-intensive con-struction of domain-specific dictionaries.?
The system is extendable by registering statis-tically acquired word correspondences into userdictionaries.1In Japanese, there are no explicit delimiters betweenwords.
The first task for alignment is , therefore, todivide the text stream into words.We will treat hereafter Japanese-English transla-tions although the proposed method is language in-dependent.The construction of the paper is as follows.
First,Section 2 offers an overview of our alignment system.Section 3 describes the entire alignment algorithmin detail.
Section 4 reports experimental resultsfor various kinds of Japanese-English texts includingnewspaper editorials, scientific papers and critiqueson economics.
The evaluation is performed fromtwo points of view: precision-recall of alignment andword correspondences acquired during alignment.Section 5 concerns related works and Section 6 con-cludes the paper.2 System Overv iewJapanese text word seg~=~oa& pos  taggingEnglish textWord Correspondences............................................................... :word anchor correspondence unting & etting \]1I AUgnmentResult IFigure 1: Overview of the Alignment SystemFigure 1 overviews our alignment system.
Theinput to the system is a pair of Japanese and En-glish texts, one the translation of the other.
First,sentence boundaries are found in both texts usingfinite state transducers.
The texts are then part-of-speech (POS) tagged and separated into origi-nal form words z.
Original forms of English wordsare determined by 80 rules using the POS infor-mation.
From the word sequences, we extract onlynouns, adjectives, adverbs verbs and unknown words(only in Japanese) because Japanese and Englishclosed words are different and impede text align-ment.
These pre-processing operation can be easilyimplemented with regular expressions.2We use in this phase the JUMAN morphologicalanalyzing system (Kurohashi et al, 1994) for taggingJapanese texts and Brill's transformation-based tagger(Brill, 1992; Brill, 1994) for tagging English texts (JU-MAN: ftp://ftp.aist-nara.ac.jp/pub/nlp/tools/juman/Brih ftp://ftp.cs.jhu.edu/pub/brill).
We would like tothank all people concerned for providing us with thetools.132The initial state of the algorithm is a set of al-ready known anchors (sentence pairs).
These are de-termined by article boundaries, section boundariesand paragraph boundaries.
In the most general case,initial anchors are only the first and final sentencepairs of both texts as depicted in Figure 2.
Pos-sible sentence correspondences are determined fromthe anchors.
Intuitively, the number of possible cor-respondences for a sentence is small near anchors,while large between the anchors.
In this phase, themost important point is that each set of possiblesentence correspondences should include the correctcorrespondence.The main task of the system is to find anchorsfrom the possible sentence correspondences by us-ing two kinds of word correspondences: statisticalword correspondences and word correspondences asheld in a bilingual dictionary 3.
By using both cor-respondences, the sentence pair whose correspon-dences exceeds a pre-defined threshold is judged asan anchor.
These newly found anchors make wordcorrespondences more precise in the subsequent ses-sion.
By repeating this anchor setting process withthreshold reduction, sentence correspondences aregradually determined from confident pairs to non-confident pairs.
The gradualism of the algorithmmakes it robust because anchor-setting errors in thelast stage of the algorithm have little effect on over-all performance.
The output of the algorithm is thealignment result (a sequence of anchors) and wordcorrespondences as by-products.English EnglishJapanese JapaneseInitial State \[EaglishFigure 2: Alignment ProcessSAdding to the bilingual dictionary of general use,users can reuse their own dictionaries created in previouss e s s i o n s .3 A lgor i thms3.1 Stat ist ics  UsedIn this section, we describe the statistics used todecide word correspondences.
From many similar-ity metrics applicable to the task, we choose mu-tual information and t-score because the relaxationof parameters can be controlled in a sophisticatedmanner.
Mutual information represents the similar-ity on the occurrence distribution and t-score rep-resents the confidence of the similarity.
These twoparameters permit more effective relaxation than thesingle parameter used in conventional methods(Kayand Roscheisen, 1993).Our basic data structure is the alignable sen-tence matrix (ASM) and the anchor matrix (AM).ASM represents possible sentence correspondencesand consists of ones and zeros.
A one in ASM in-dicates the intersection of the column and row con-stitutes a possible sentence correspondence.
On thecontrary, AM is introduced to represent how a sen-tence pair is supported by word correspondences.The i-j Element of AM indicates how many timesthe corresponding words appear in the i-j sentencepair.
As alignment proceeds, the number of ones inASM reduces, while the elements of AM increase.Let pi be a sentence set comprising the ithJapanese sentence and its possible English corre-spondences as depicted in Figure 3.
For example, P2is the set comprising Jsentence2, Esentence2 andEsentencej, which means Jsentence2 has the pos-sibility of aligning with Esentence2 or Esentencej.The pis can be directly derived from ASM.exP2P3Jsentence I ?
EsentencelJsentence 2 Esentence2Jsentence 3 Esentence3?
?
, ?
?
?
, ?
?
?
?
, ?
?
, ?
, , , ?
?
?
,PM Jsentence  Esentence NFigure 3: Possible Sentence CorrespondencesWe introduce the contingency matrix (Fung andChurch, 1994) to evaluate the similarity of word oc-currences.
Consider the contingency matrix shownTable 1, between Japanese word wjp n and Englishword Weng.
The contingency matrix shows: (a) thenumber of pis in which both wjp, and w~ng werefound, (b) the number of pis in which just w~.g wasfound, (c) the number of pis in which just wjp, was133found, (d) the number of pis in which neither wordwas found.
Note here that pis overlap each otherand w~,~ 9 may be double counted in the contingencymatrix.
We count each w~,,~ only once, even if itoccurs more than twice in pls.\] WjpnWeng I a bI c dTable 1: Contingency MatrixIf Wjpn and weng are good translations of one an-other, a should be large, and b and c should be small.In contrast, if the two are not good translations ofeach other, a should be small, and b and c shouldbe large.
To make this argument more precise, weintroduce mutual information:log prob(wjpn, Weng)prob( w p. )prob( won9 )The probabilities are:a+c a+cprob(wjpn) - a T b + c W d - Ya+b a+bpr ob( w eng ) - a+b+c+d - Ma aprob( wjpn ,Weng ) -- a+b+c+d-  MUnfortunately, mutual information is not reliablewhen the number of occurrences i small.
Manywords occur just once which weakens the statisticsapproach.
In order to avoid this, we employ t-score,defined below, where M is the number of Japanesesentences.
Insignificant mutual information valuesare filtered out by thresholding t-score.
For exam-ple, t-scores above 1.65 are significant at the p >0.95 confidence level.t ~ prob(wjpn, Weng) - prob(wjpn)prob(weng)~/-~prob( wjpn ,Weng )3.2 Basic Alignment AlgorithmOur basic algorithm is an iterative adjustment of theAnchor Matrix (AM) using the Alignable SentenceMatrix (ASM).
Given an ASM, mutual informationand t-score are computed for all word pairs in possi-ble sentence correspondences.
A word combinationexceeding a predefined threshold is judged as a wordcorrespondence.
In order to find new anchors, wecombine these statistical word correspondences withthe word correspondences in a bilingual dictionary.Each element of AM, which represents a sentencepair, is updated by adding the number of word cor-respondences in the sentence pair.
A sentence paircontaining more than a predefined number of corre-sponding words is determined to be a new anchor.The detailed algorithm is as follows.3.2.1 Constructing Initial ASMThis step constructs the initial ASM.
If the textscontain M and N sentences respectively, the ASMis an M x N matrix.
First, we decide a set of an-chors using article boundaries, section boundariesand so on.
In the most general case, initial anchorsare the first and last sentences of both texts as de-picted in Figure 2.
Next, possible sentence corre-spondences are generated.
Intuitively, true corre-spondences are close to the diagonal linking the twoanchors.
We construct he initial ASM using sucha function that pairs sentences near the middle ofthe two anchors with as many as O(~/~) (L is thenumber of sentences existing between two anchors)sentences in the other text because the maximumdeviation can be stochastically modeled as O(~rL)(Kay and Roscheisen, 1993).
The initial ASM haslittle effect on the alignment performance so long asit contains all correct sentence correspondences.3.2.2 Constructing AMThis step constructs an AM when given an ASMand a bilingual dictionary.
Let thigh, tlow, Ihigh andIzow be two thresholds for t-score and two thresholdsfor mutual information, respectively.
Let ANC bethe minimal number of corresponding words for asentence pair to be judged as an anchor.First, mutual information and t-score are com-puted for all word pairs appearing in a possible sen-tence correspondence in ASM.
We use hereafter theword correspondences whose mutual information ex-ceeds Itow and whose t-score exceeds ttow.
For allpossible sentence correspondences Jsentencei andEsentencej (any pair in ASM), the following op-erations are applied in order.1.
If the following three conditions hold, add 3to the i-j element of AM.
(1) Jsentencei andEsentencej contain a bilingual dictionary wordcorrespondence (wjpn and w,ng).
(2) w~na doesnot occur in any other English sentence thatis a possible translation of Jsentencei.
(3)Jsentencei and Esentencej do not cross anysentence pair that has more than ANC wordcorrespondences.2.
If the following three conditions hold, add 3to the i-j element of AM.
(1) Jsentencei andEsentencej contain a stochastic word corre-spondence (wjpn and w~na) that has mutualinformation Ihig h and whose t-score exceedsthigh.
(2) w~g does not occur in any otherEnglish sentence that is a possible translationof Jsentencei.
(3) Jsentencei and Esentencejdo not cross any sentence pair that has morethan ANC word correspondences.3.
If the following three conditions hold, add 1to the i-j element of AM.
(1) Jsentencei andEsentencej contain a stochastic word corre-spondence (wjp~ and we~g) that has mutual134information Itoto and whose t-score exceedsttow.
(2) w~na does not occur in any otherEnglish sentence that is a possible translationof Jsentencei.
(3) Jsentencei and Esentencejdoes not cross any sentence pair that has morethan ANC word correspondences.The first operation deals with word correspon-dences in the bilingual dictionary.
The second op-eration deals with stochastic word correspondenceswhich are highly confident and in many cases involvedomain specific keywords.
These word correspon-dences are given the value of 3.
The third operationis introduced because the number of highly confi-dent corresponding words are too small to align allsentences.
Although word correspondences acquiredby this step are sometimes false translations of eachother, they play a crucial role mainly in the finaliterations phase.
They are given one point.3.2.3 Adjusting ASMThis step adjusts ASM using the AM constructedby the above operations.
The sentence pairs thathave at least ANC word correspondences are deter-mined to be new anchors.
By using the new set ofanchors, a new ASM is constructed using the samemethod as used for initial ASM construction.Our algorithm implements a kind of relaxation bygradually reducing flow, Izow and ANC,  which en-ables us to find confident sentence correspondencesfirst.
As a result, our method is more robust thandynamic programing techniques against he shortageof word-correspondence knowledge.4 Exper imenta l  Resu l tsIn this section, we report he result of experimentson aligning sentences in bilingual texts and on sta-tistically acquired word correspondences.
The textsfor the experiment varied in length and genres assummarized in Table 2.
Texts 1 and 2 are editorialstaken from 'Yomiuri Shinbun' and its English ver-sion 'Daily Yomiuri'.
This data was distributed elec-trically via a WWW server 4.
The first two texts clar-ify the systems's performance on shorter texts.
Text3 is an essay on economics taken from a quarterlypublication of The International House of Japan.Text 4 is a scientific survey on brain science takenfrom 'Scientific American' and its Japanese version'Nikkei Science '5.
Jpn  and Eng in Table2 representthe number of sentences in the Japanese and Englishtexts respectively.
The remaining table entries how4The Yomiuri data canbe obtained from www.yomiuri.co.jp.
We would like tothank Yomiuri Shinbun Co. for permitting us to use thedata.~We obtained the data from paper version of the mag-azine by using OCR.
We would like to thank Nikkei Sci-ence Co. for permitting us to use the data.categories of matches by manual alignment and in-dicate the difficulty of the task.Our evaluation focuses on much smaller texts thanthose used in other study(Brown and others, 1993;Gale and Church, 1993; Wu, 1994; Fung, 1995; Kayand Roscheisen, 1993) because our main targets arewell-separated articles.
However, our method willwork on larger and noisy sets too, by using wordanchors rather than using sentence boundaries assegment boundaries.
In such a case, the methodconstructing initial ASM needs to be modified.We briefly report here the computation time ofour method.
Let us consider Text 4 as an exam-ple.
After 15 seconds for full preprocessing, thefirst iteration took 25 seconds with tto~ = 1.55 andIzow = 1.8.
The rest of the algorithm took 20 sec-onds in all.
This experiment was performed on aSPARC Station 20 Model tIS21.
From the result,we may safely say that our method can be appliedto voluminous corpora.4.1 Sentence AlignmentTable 3 shows the performance on sentence align-ments for the texts in Table 2.
Combined, Statis-tics and D ic t ionary  represent the methods usingboth statistics and dictionary, only statistics andonly dictionary, respectively.
Both Combined  andDictionary use a CD-ROM version of a Japanese-English dictionary containing 40 thousands entries.Statistics repeats the iteration by using statisticalcorresponding words only.
This is identical to Kay'smethod (Kay and Roscheisen, 1993) except for thestatistics used.
D ic t ionary  performs the iterationof the algorithm by using corresponding words ofthe bilingual dictionary.
This delineates the cover-age of the dictionary.
The parameter setting usedfor each method was the optimum as determined byempirical tests.In Table 3, PRECISION delineates how many ofthe aligned pairs are correct and RECALL delineateshow many of the manual alignments we includedin systems output.
Unlike conventional sentence-chunk based evaluations, our result is measured onthe sentence-sentence basis.
Let us consider a 3-1matching.
Although conventional evaluations canmake only one error from the chunk, three errorsmay arise by our evaluation.
Note that our evalua-tion is more strict than the conventional one, espe-cially for difficult texts, because they contain morecomplex matches.For Text 1 and Text 2, both the combinedmethod and the dictionary method perform muchbetter than the statistical method.
This is ob-viously because statistics cannot capture word-correspondences in the case of short texts.Text 3 is easy to align in terms of both the com-plexity of the alignment and the vocabularies used.All methods performed well on this text.For Text 4, Combined  and Stat ist ics  perform1351 Root out guns at all costs 26 28 24 2 0 02 Economy \]acing last hurdle 36 41 25 7 2 03 Pacific Asia in the Post-Cold-War World 134 124 114 0 10 04 Visualizing the Mind 225 214 186 6 15 1Table 2: Test TextsII CombinedText  PRECISION I RECALL1 96.4% 96.3%2 95.3% 93.1%3 96.5% 97.1%4 91.6% 93.8%StatisticsPRECISION RECALL65.0% 48.5%61.3% 49.6%87.3% 85.1%82.2% 79.3%D ic t ionaryPRECISION RECALL89.3% 88.9%87.2% 75.1%86.3% 88.2%74.3% 63.8%Table 3: Result of Sentence Alignmentmuch better than D ic t ionary .
The reason for this isthat Text 4 concerns brain science and the bilingualdictionaries of general use did not contain domainspecific keywords.
On the other hand, the combinedand statistical methods well capture the keywordsas described in the next section.
Note here thatCombined  performs better than Stat is t ics  in thecase of longer texts, too.
There is clearly a limitationin the amount of word correspondences that can becaptured by statistics.
In summary, the performanceof Combined  is better than either Stat ist ics  orD ic t ionary  for all texts, regardless of text lengthand the domain.correspondences were not used.Although these word correspondences are very ef-fective for sentence alignment ask, they are unsat-isfactory when regarded as a bilingual dictionary.For example, ' 7 7 Y ~' ~ ~ ~n.MR I ' in Japaneseis the translation of 'functional MRI'.
In Table 4, thecorrespondence of these compound nouns was cap-tured only in their constituent level.
(Haruno et al,1996) proposes an efficient n-gram based method toextract bilingual collocations from sentence alignedbilingual corpora.5 Re la ted  Work4.2 Word CorrespondenceIn this section, we will demonstrate how well the pro-posed method captured omain specific word corre-spondences by using Text 4 as an example.
Table 4shows the word correspondences that have high mu-tual information.
These are typical keywords con-cerning the non-invasive approach to human brainanalysis.
For example, NMR, MEG, PET, CT, MRIand functional MRI are devices for measuring brainactivity from outside the head.
These technicalterms are the subjects of the text and are essentialfor alignment.
However, none of them have theirown entry in the bilingual dictionary, which wouldstrongly obstruct he dictionary method.It is interesting to note that the correct Japanesetranslation of 'MEG' is ' ~{i~i~\]'.
The Japanese mor-phological analyzer we used does not contain an en-try for ' ~i~i\[~' and split it into a sequence of threecharacters ' ~ ' , '  ~ '  and ' \[\]'.
Our system skillfullycombined ' ~i' and ' \[ \] '  with 'MEG', as a result ofstatistical acquisition.
These word correspondencesgreatly improved the performance for Text 4.
Thus,the statistical method well captures the domain spe-cific keywords that are not included in general-usebilingual dictionaries.
The dictionary method wouldyield false alignments if statistically acquired wordSentence alignment between Japanese and Englishwas first explored by Sato and Murao (Murao, 1991).They found (character or word) length-based ap-proaches were not appropriate due to the structuraldifference of the two languages.
They devised adynamic programming method based on the num-ber of corresponding words in a hand-crafted bilin-gual dictionary.
Although some results were promis-ing, the method's performance strongly depended onthe domain of the texts and the dictionary entries.
(Utsuro et al, 1994) introduced a statistical post-processing step to tackle the problem.
He first ap-plied Sato's method and extracted statistical wordcorrespondences from the result of the first path.Sato's method was then reiterated using both the ac-quired word correspondences and the hand-crafteddictionary.
His method involves the following twoproblems.
First, unless the hand-crafted ictionarycontains domain specific key words, the first pathyields false alignment, which in turn leads to falsestatistical correspondences.
Because it is impossiblein general to cover key words in all domains, it isinevitable that statistics and hand-crafted bilingualdictionaries must be used at the same time.136\[ Engl ish Mutua l  InFormation I J apanese~)T.,t.~4"-NMB.PET~5N5N5recordin~rea~recordin~3.683.51neuron 3.51fi lm 3.51~lucose 3.51incrense 3.~1MEG 3.51resolution 3.43electrical 3.43group 3.393.39electrical 3.39~:enerate 3.32provide 3.33MEG 3.33noun 3.17NMB.
3.17functional 3.17equipment 3.17organcompoundwaterradioactivePETspatialsuchmetabol ismverbscientistwnterwatermappin |takeuniversitythoushtcompoundlabeltaskradioactivityvisualnouns i |na lpresentI) 7"/L,~Z 4 .& time ~xY dan~6~ea.ut oradiogrsphyabilityCTauditorymentalMRICT,bMR !3.153.103.103.103.10:}.103.103.063.042.9E2.982.982.922.922.922.902,822,822,822.772.772.772.772.722.692.692.672.632.632.192.051.8Table 4: Statistically Acquired KeywordsThe proposed method involves iterative alignmentwhich simultaneously uses both statistics and abilingual dictionary.Second, their score function is not reliable espe-cially when the number of corresponding words con-tained in corresponding sentences is small.
Theirmethod selects a matching type (such as 1-1, 1-2and 2-1) according to the number of word correspon-dences per contents word.
However, in many cases,there are a few word translations in a set of corre-sponding sentences.
Thus, it is essential to decidesentence alignment on the sentence-sentence basis.Our iterative approach decides sentence alignmentlevel by level by counting the word correspondencesbetween a Japanese and an English sentence.
(Fung and Church, 1994; Fung, 1995) proposedmethods to find Chinese-English word correspon-dences without aligning parallel texts.
Their mo-tivation is that structurally different languages suchas Chinese-English and Japanese-English are diffi-cult to align in general.
Their methods bypassedaligning sentences and directly acquired word cor-respondences.
Although their approaches are ro-bust for noisy corpora and do not require any in-formation source, aligned sentences are necessaryfor higher level applications uch as well-grainedtranslation template acquisition (Matsumoto et as.,1993; Smadja et al, 1996; Haruno et al, 1996)and example-based translation (Sato and Nagao,1990).
Our method performs accurate alignment forsuch use by combining the detailed word correspon-dences: statistically acquired word correspondencesand those from a bilingual dictionary of general use.
(Church, 1993) proposed char_align that makesuse of n-grams shared by two languages.
Thiskind of matching techniques will be helpful in ourdictionary-based approach in the following situation:Entries of a bilingual dictionary do not completelymatch the word in the corpus but partially do.
Byusing the matching technique, we can make the mostof the information compiled in bilingual dictionaries.6 Conc lus ionWe have described a text alignment method forstructurally different languages.
Our iterativemethod uses two kinds of word correspondences atthe same time: word correspondences acquired bystatistics and those of a bilingual dictionary.
Bycombining these two types of word correspondences,the method covers both domain specific keywordsnot included in the dictionary and the infrequentwords not detected by statistics.
As a result, ourmethod outperforms conventional methods for textsof different lengths and different domains.AcknowledgementWe would like to thank Pascale Fung and Takehito Ut-suro for helpful comments and discussions.ReferencesEric Brill.
1992.
A simple rule-based part of speechtagger.
In Proc.
Third Con/erence on Apolied NaturalLanguage Processing, pages 152-155.Eric Brill.
1994.
Some advances in transformation-basedpart of speech tagging.
In Proc.
1Pth AAAI, pages722-727.P F Brown et al 1991.
Aligning sentences in parallelcorpora.
In the 29th Annual Meeting of ACL, pages169-176.P F Brown et al 1993.
The mathematics of statisti-cal machine translation.
Computational Linguistics,19(2):263-311, June.137S F Chen.
1993.
Aligning sentences in bilingual corporausing lexical information.
In the 31st Annual Meetingof ACL, pages 9-16.K W Church.
1993.
Char_align: A program for align-ing parallel texts at the character level.
In the 31stAnnual Meeting of ACL, pages 1-8.Ido Dagan and Ken Church.
1994.
Termight: identifyingand translating technical terminology.
In Proc.
FourthConference on Apolied Natural Language Processing,pages 34-40.Pascale Fung and K W Church.
1994.
K-vec: A newapproach for aligning parallel texts.
In Proc.
15thCOLING, pages 1096-1102.Pascale Fung.
1995.
A pattern matching method forfinding noun and proper nouns translations from noisyparallel corpora.
In Proc.
33rd ACL, pages 236-243.W A Gale and K W Church.
1993.
A program for align-ing sentences in bilingual corpora.
ComputationalLinguistics, 19(1):75-102, March.Masahiko Haruno, Satoru Ikehara, and Takefumi Ya-mazaki.
1996.
Learning Bilingual Collocations byWord-Level Sorting,.
In Proc.
16th COLING.Hiroyuki Kaji, Yuuko Kida, and Yasutsugu Morimoto.1992.
Learning translation templates from bilingaultext.
In Proc.
14th COLING, pages 672-678.Martin Kay and Martin Roscheisen.
1993.
Text-translation alignment.
Computational Linguistics,19(1):121-142, March.Akira Kumano and Hideki Hirakawa.
1994.
Building anMT dictionary from parallel texts based on linguisiticand statistical information.
In Proc.
15th COLING,pages 76-81.Julian Kupiec.
1993.
An algorithm for finding nounphrase correspondences in bilingual corpora.
In the31st Annual Meeting of A CL, pages 17-22.Sadao Kurohashi, Toshihisa Nakamura, Yuji Mat-sumoto, and Makoto Nagao.
1994.
Improvements ofJapanese morphological nalyzer juman.
In Proc.
In-ternational Workshop on Sharable Natural LanguageResources, pages 22-28.Yuji Matsumoto, Hiroyuki Ishimoto, and Takehito Ut-suro.
1993.
Structural matching of parallel texts.
Inthe 31st Annual Meeting of ACL, pages 23-30.H.
Murao.
1991.
Studies on bilingual text alignment.Bachelor Thesis, Kyoto University (in Japanese).Satoshi Sato and Makoto Nagao.
1990.
Toward memory-based translation.
In Proc.
13th COLING, pages 247-252.Satoshi Sato.
1992.
CTM: an example-based translationaid system.
In Proc.
l$th COLING, pages 1259-1263.Frank Smadja, Kathleen McKeown, and Vasileios Hatzi-vassiloglou.
1996.
Translating collocations for bilin-gual lexicons: A statistical approach.
ComputationalLinguistics, 22(1):1-38, March.Takehito Utsuro, Hiroshi Ikeda Masaya Yamane, YujiMatsumoto, and Makoto Nagao.
1994.
Bilingual textmatching using bilingual dictionary and statistics.
InProc.
15th COLING, pages 1076-1082.Dekai Wu.
1994.
Aligning a parallel English-Chinesecorpus statistically with lexical criteria.
In the 3AndAnnual Meeting of ACL, pages 80-87.138
