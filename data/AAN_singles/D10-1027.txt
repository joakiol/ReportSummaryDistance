Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 273?283,MIT, Massachusetts, USA, 9-11 October 2010. c?2010 Association for Computational LinguisticsEfficient Incremental Decoding for Tree-to-String TranslationLiang Huang 11Information Sciences InstituteUniversity of Southern California4676 Admiralty Way, Suite 1001Marina del Rey, CA 90292, USA{lhuang,haitaomi}@isi.eduHaitao Mi 2,12Key Lab.
of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, Chinahtmi@ict.ac.cnAbstractSyntax-based translation models should inprinciple be efficient with polynomially-sizedsearch space, but in practice they are oftenembarassingly slow, partly due to the costof language model integration.
In this paperwe borrow from phrase-based decoding theidea to generate a translation incrementallyleft-to-right, and show that for tree-to-stringmodels, with a clever encoding of deriva-tion history, this method runs in average-case polynomial-time in theory, and linear-time with beam search in practice (whereasphrase-based decoding is exponential-time intheory and quadratic-time in practice).
Exper-iments show that, with comparable translationquality, our tree-to-string system (in Python)can run more than 30 times faster than thephrase-based system Moses (in C++).1 IntroductionMost efforts in statistical machine translation so farare variants of either phrase-based or syntax-basedmodels.
From a theoretical point of view, phrase-based models are neither expressive nor efficient:they typically allow arbitrary permutations and re-sort to language models to decide the best order.
Intheory, this process can be reduced to the TravelingSalesman Problem and thus requires an exponential-time algorithm (Knight, 1999).
In practice, the de-coder has to employ beam search to make it tractable(Koehn, 2004).
However, even beam search runs inquadratic-time in general (see Sec.
2), unless a smalldistortion limit (say, d=5) further restricts the possi-ble set of reorderings to those local ones by rulingout any long-distance reorderings that have a ?jump?in theory in practicephrase-based exponential quadratictree-to-string polynomial linearTable 1: [main result] Time complexity of our incremen-tal tree-to-string decoding compared with phrase-based.In practice means ?approximate search with beams.
?longer than d. This has been the standard prac-tice with phrase-based models (Koehn et al, 2007),which fails to capture important long-distance re-orderings like SVO-to-SOV.Syntax-based models, on the other hand, usesyntactic information to restrict reorderings toa computationally-tractable and linguistically-motivated subset, for example those generated bysynchronous context-free grammars (Wu, 1997;Chiang, 2007).
In theory the advantage seems quiteobvious: we can now express global reorderings(like SVO-to-VSO) in polynomial-time (as opposedto exponential in phrase-based).
But unfortunately,this polynomial complexity is super-linear (beinggenerally cubic-time or worse), which is slow inpractice.
Furthermore, language model integrationbecomes more expensive here since the decoder nowhas to maintain target-language boundary words atboth ends of a subtranslation (Huang and Chiang,2007), whereas a phrase-based decoder only needsto do this at one end since the translation is alwaysgrowing left-to-right.
As a result, syntax-basedmodels are often embarassingly slower than theirphrase-based counterparts, preventing them frombecoming widely useful.Can we combine the merits of both approaches?While other authors have explored the possibilities273of enhancing phrase-based decoding with syntax-aware reordering (Galley and Manning, 2008), weare more interested in the other direction, i.e., cansyntax-based models learn from phrase-based de-coding, so that they still model global reordering, butin an efficient (preferably linear-time) fashion?Watanabe et al (2006) is an early attempt inthis direction: they design a phrase-based-style de-coder for the hierarchical phrase-based model (Chi-ang, 2007).
However, this algorithm even with thebeam search still runs in quadratic-time in prac-tice.
Furthermore, their approach requires grammartransformation that converts the original grammarinto an equivalent binary-branching Greibach Nor-mal Form, which is not always feasible in practice.We take a fresh look on this problem and turn ourfocus to one particular syntax-based paradigm, tree-to-string translation (Liu et al, 2006; Huang et al,2006), since this is the simplest and fastest amongsyntax-based approaches.
We develop an incremen-tal dynamic programming algorithm and make thefollowing contributions:?
we show that, unlike previous work, our in-cremental decoding algorithm runs in average-case polynomial-time in theory for tree-to-string models, and the beam search version runsin linear-time in practice (see Table 1);?
large-scale experiments on a tree-to-string sys-tem confirm that, with comparable translationquality, our incremental decoder (in Python)can run more than 30 times faster than thephrase-based system Moses (in C++) (Koehnet al, 2007);?
furthermore, on the same tree-to-string system,incremental decoding is slightly faster than thestandard cube pruning method at the same levelof translation quality;?
this is also the first linear-time incremental de-coder that performs global reordering.We will first briefly review phrase-based decod-ing in this section, which inspires our incrementalalgorithm in the next section.2 Background: Phrase-based DecodingWe will use the following running example fromChinese to English to explain both phrase-based andsyntax-based decoding throughout this paper:0 Bu`sh??
1Bushyu?
2withSha?lo?ng 3Sharonju?x?
?ng 4holdle-ed5 hu?`ta?n 6meeting?Bush held talks with Sharon?2.1 Basic Dynamic Programming AlgorithmPhrase-based decoders generate partial target-language outputs in left-to-right order in the formof hypotheses (Koehn, 2004).
Each hypothesis hasa coverage vector capturing the source-languagewords translated so far, and can be extended into alonger hypothesis by a phrase-pair translating an un-covered segment.
This process can be formalized asa deductive system.
For example, the following de-duction step grows a hypothesis by the phrase-pair?yu?
Sha?lo?ng, with Sharon?
covering Chinese span[1-3]:(?
??
?6) : (w, ?Bush held talks?)(???3???)
: (w?, ?Bush held talks with Sharon?)
(1)where a ?
in the coverage vector indicates the sourceword at this position is ?covered?
and where w andw?
= w+c+d are the weights of the two hypotheses,respectively, with c being the cost of the phrase-pair,and d being the distortion cost.
To compute d wealso need to maintain the ending position of the lastphrase (the 3 and 6 in the coverage vector).To add a bigram model, we split each ?LM itemabove into a series of +LM items; each +LM itemhas the form (v,a ) where a is the last word of thehypothesis.
Thus a +LM version of (1) might be:(?
??
?6,talks ) : (w, ?Bush held talks?)(???3??
?,Sharon ) : (w?, ?Bush held talks with Sharon?
)where the score of the resulting +LM itemw?
= w + c + d?
logPlm(with | talk)now includes a combination cost due to the bigramsformed when applying the phrase-pair.
The com-plexity of this dynamic programming algorithm forg-gram decoding is O(2nn2|V |g?1) where n is thesentence length and |V | is the English vocabularysize (Huang and Chiang, 2007).2741 2 3 4 5Figure 1: Beam search in phrase-based decoding expandsthe hypotheses in the current bin (#2) into longer ones.VPPPPyu?x1:NPVPVVju?x??ngASlex2:NP?
held x2 with x1Figure 2: Tree-to-string rule r3 for reordering.2.2 Beam Search in PracticeTo make the exponential algorithm practical, beamsearch is the standard approximate search method(Koehn, 2004).
Here we group +LM items into nbins, with each bin Bi hosting at most b items thatcover exactly i Chinese words (see Figure 1).
Thecomplexity becomes O(n2b) because there are a to-tal of O(nb) items in all bins, and to expand eachitem we need to scan the whole coverage vector,which costs O(n).
This quadratic complexity is stilltoo slow in practice and we often set a small distor-tion limit of dmax (say, 5) so that no jumps longerthan dmax are allowed.
This method reduces thecomplexity to O(nbdmax) but fails to capture long-distance reorderings (Galley and Manning, 2008).3 Incremental Decoding for Tree-to-StringTranslationWe will first briefly review tree-to-string translationparadigm and then develop an incremental decodingalgorithm for it inspired by phrase-based decoding.3.1 Tree-to-string TranslationA typical tree-to-string system (Liu et al, 2006;Huang et al, 2006) performs translation in twosteps: parsing and decoding.
A parser first parses thesource language input into a 1-best tree T , and thedecoder then searches for the best derivation (a se-(a) Bu`sh??
[yu?
Sha?lo?ng ]1 [ju?x?
?ng le hu?`ta?n ]2?
1-best parser(b) IP@?NP@1Bu`sh??VP@2PP@2.1Pyu?NP@2.1.2Sha?lo?ngVP@2.2VVju?x?
?ngASleNP@2.2.3hu?`ta?nr1 ?
(c) NP@1Bu`sh??VP@2PP@2.1Pyu?NP@2.1.2Sha?lo?ngVP@2.2VVju?x?
?ngASleNP@2.2.3hu?`ta?nr2 ?
r3 ?
(d) Bush held NP@2.2.3hu?`ta?nwith NP@2.1.2Sha?lo?ngr4 ?
r5 ?
(e) Bush [held talks]2 [with Sharon]1Figure 3: An example derivation of tree-to-string trans-lation (much simplified from Mi et al (2008)).
Shadedregions denote parts of the tree that matches the rule.quence of translation steps) d?
that converts sourcetree T into a target-language string.Figure 3 shows how this process works.
The Chi-nese sentence (a) is first parsed into tree (b), whichwill be converted into an English string in 5 steps.First, at the root node, we apply rule r1 preservingthe top-level word-order(r1) IP (x1:NP x2:VP) ?
x1 x2which results in two unfinished subtrees, NP@1 andVP@2 in (c).
Here X@?
denotes a tree node of la-bel X at tree address ?
(Shieber et al, 1995).
(Theroot node has address ?, and the first child of node ?has address ?.1, etc.)
Then rule r2 grabs the Bu`sh?
?subtree and transliterate it into the English word275in theory in practicephrase* O(2nn2 ?
|V |g?1) O(n2b)tree-to-str O(nc ?
|V |4(g?1)) O(ncb2)this work* O(nk log2(cr) ?
|V |g?1) O(ncb)Table 2: Summary of time complexities of various algo-rithms.
b is the beam width, V is the English vocabulary,and c is the number of translation rules per node.
As aspecial case, phrase-based decoding with distortion limitdmax is O(nbdmax).
*: incremental decoding algorithms.?Bush?.
Similarly, rule r3 shown in Figure 2 is ap-plied to the VP subtree, which swaps the two NPs,yielding the situation in (d).
Finally two phrasalrules r4 and r5 translate the two remaining NPs andfinish the translation.In this framework, decoding without languagemodel (?LM decoding) is simply a linear-timedepth-first search with memoization (Huang et al,2006), since a tree of n words is also of sizeO(n) and we visit every node only once.
Addinga language model, however, slows it down signifi-cantly because we now have to keep track of target-language boundary words, but unlike the phrase-based case in Section 2, here we have to rememberboth sides the leftmost and the rightmost boundarywords: each node is now split into +LM items like(?
a ?
b) where ?
is a tree node, and a and b are leftand right English boundary words.
For example, abigram +LM item for node VP@2 might be(VP@2 held ?
Sharon).This is also the case with other syntax-based modelslike Hiero or GHKM: language model integrationoverhead is the most significant factor that causessyntax-based decoding to be slow (Chiang, 2007).
Intheory +LM decoding is O(nc|V |4(g?1)), where Vdenotes English vocabulary (Huang, 2007).
In prac-tice we have to resort to beam search again: at eachnode we would only allow top-b +LM items.
Withbeam search, tree-to-string decoding with an inte-grated language model runs in time O(ncb2), whereb is the size of the beam at each node, and c is (max-imum) number of translation rules matched at eachnode (Huang, 2007).
See Table 2 for a summary.3.2 Incremental DecodingCan we borrow the idea of phrase-based decoding,so that we also grow the hypothesis strictly left-to-right, and only need to maintain the rightmostboundary words?The key intuition is to adapt the coverage-vectoridea from phrase-based decoding to tree-to-stringdecoding.
Basically, a coverage-vector keeps trackof which Chinese spans have already been translatedand which have not.
Similarly, here we might needa ?tree coverage-vector?
that indicates which sub-trees have already been translated and which havenot.
But unlike in phrase-based decoding, we cannot simply choose any arbitrary uncovered subtreefor the next step, since rules already dictate whichsubtree to visit next.
In other words what we needhere is not really a tree coverage vector, but more ofa derivation history.We develop this intuition into an agenda repre-sented as a stack.
Since tree-to-string decoding is atop-down depth-first search, we can simulate this re-cursion with a stack of active rules, i.e., rules that arenot completed yet.
For example we can simulate thederivation in Figure 3 as follows.
At the root nodeIP@?, we choose rule r1, and push its English-sideto the stack, with variables replaced by matched treenodes, here x1 for NP@1 and x2 for VP@2.
So wehave the following stacks = [ NP@1 VP@2],where the dot  indicates the next symbol to processin the English word-order.
Since node NP@1 is thefirst in the English word-order, we expand it first,and push rule r2 rooted at NP to the stack:[ NP@1 VP@2 ] [ Bush].Since the symbol right after the dot in the top rule isa word, we immediately grab it, and append it to thecurrent hypothesis, which results in the new stack[ NP@1 VP@2 ] [Bush  ].Now the top rule on the stack has finished (dot is atthe end), so we trigger a ?pop?
operation which popsthe top rule and advances the dot in the second-to-top rule, denoting that NP@1 is now completed:[NP@1  VP@2].276stack hypothesis[<s>  IP@?
</s>] <s>p [<s>  IP@?
</s>] [ NP@1 VP@2] <s>p [<s>  IP@?
</s>] [ NP@1 VP@2] [ Bush] <s>s [<s>  IP@?
</s>] [ NP@1 VP@2] [Bush  ] <s> Bushc [<s>  IP@?
</s>] [NP@1  VP@2] <s> Bushp [<s>  IP@?
</s>] [NP@1  VP@2] [ held NP@2.2.3 with NP@2.1.2] <s> Bushs [<s>  IP@?
</s>] [NP@1  VP@2] [held  NP@2.2.3 with NP@2.1.2] <s> Bush heldp [<s>  IP@?
</s>] [NP@1  VP@2] [held  NP@2.2.3 with NP@2.1.2] [ talks] <s> Bush helds [<s>  IP@?
</s>] [NP@1  VP@2] [held  NP@2.2.3 with NP@2.1.2] [talks  ] <s> Bush held talksc [<s>  IP@?
</s>] [NP@1  VP@2] [held NP@2.2.3  with NP@2.1.2] <s> Bush held talkss [<s>  IP@?
</s>] [NP@1  VP@2] [held NP@2.2.3 with  NP@2.1.2] <s> Bush held talks withp [<s>  IP@?
</s>] [NP@1  VP@2] [held NP@2.2.3 with  NP@2.1.2] [ Sharon] <s> Bush held talks withs [<s>  IP@?
</s>] [NP@1  VP@2] [held NP@2.2.3 with  NP@2.1.2] [Sharon ] <s> Bush held talks with Sharonc [<s>  IP@?
</s>] [NP@1  VP@2] [held NP@2.2.3 with NP@2.1.2 ] <s> Bush held talks with Sharonc [<s>  IP@?
</s>] [NP@1 VP@2 ] <s> Bush held talks with Sharonc [<s> IP@?
 </s>] <s> Bush held talks with Sharons [<s> IP@?
</s> ] <s> Bush held talks with Sharon </s>Figure 4: Simulation of tree-to-string derivation in Figure 3 in the incremental decoding algorithm.
Actions: p, predict;s, scan; c, complete (see Figure 5).Item ?
: ?s, ??
: w; ?
: step, s: stack, ?
: hypothesis, w: weightEquivalence ?
: ?s, ??
?
?
: ?s?, ???
iff.
s = s?
and lastg?1(?)
= lastg?1(??
)Axiom 0 : ?
[<s>g?1  ?
</s>], <s>g?1?
: 0Predict?
: ?...
[?
 ?
?
], ??
: w?
+ |C(r)| : ?...
[?
 ?
?]
[ f(?,E(r))], ??
: w + c(r) match(?, C(r))Scan?
: ?...
[?
 e ?
], ??
: w?
: ?...
[?
e  ?
], ?e?
: w ?
log Pr(e | lastg?1(?))Complete?
: ?...
[?
 ?
?]
[?], ??
: w?
: ?...
[?
?
 ?
], ??
: wGoal |T | : ?
[<s>g?1 ?
</s>], ?</s>?
: wFigure 5: Deductive system for the incremental tree-to-string decoding algorithm.
Function lastg?1(?)
returns therightmost g ?
1 words (for g-gram LM), and match(?, C(r)) tests matching of rule r against the subtree rooted atnode ?.
C(r) and E(r) are the Chinese and English sides of rule r, and function f(?,E(r)) = [xi 7?
?.var(i)]E(r)replaces each variable xi on the English side of the rule with the descendant node ?.var(i) under ?
that matches xi.277The next step is to expand VP@2, and we use rule r3and push its English-side ?VP ?
held x2 with x1?onto the stack, again with variables replaced bymatched nodes:[NP@1  VP@2] [ held NP@2.2.3 with NP@2.1.2]Note that this is a reordering rule, and the stack al-ways follows the English word order because wegenerate hypothesis incrementally left-to-right.
Fig-ure 4 works out the full example.We formalize this algorithm in Figure 5.
Eachitem ?s, ??
consists of a stack s and a hypothesis?.
Similar to phrase-based dynamic programming,only the last g?1 words of ?
are part of the signaturefor decoding with g-gram LM.
Each stack is a list ofdotted rules, i.e., rules with dot positions indictingprogress, in the style of Earley (1970).
We call thelast (rightmost) rule on the stack the top rule, whichis the rule being processed currently.
The symbol af-ter the dot in the top rule is called the next symbol,since it is the symbol to expand or process next.
De-pending on the next symbol a, we can perform oneof the three actions:?
if a is a node ?, we perform a Predict actionwhich expands ?
using a rule r that can pattern-match the subtree rooted at ?
; we push r is tothe stack, with the dot at the beginning;?
if a is an English word, we perform a Scan ac-tion which immediately adds it to the currenthypothesis, advancing the dot by one position;?
if the dot is at the end of the top rule, weperform a Complete action which simply popsstack and advance the dot in the new top rule.3.3 Polynomial Time ComplexityUnlike phrase-based models, we show herethat incremental decoding runs in average-casepolynomial-time for tree-to-string systems.Lemma 1.
For an input sentence of n words andits parse tree of depth d, the worst-case complex-ity of our algorithm is f(n, d) = c(cr)d|V |g?1 =O((cr)dng?1), assuming relevant English vocabu-lary |V | = O(n), and where constants c, r and g arethe maximum number of rules matching each treenode, the maximum arity of a rule, and the language-model order, respectively.Proof.
The time complexity depends (in part) on thenumber of all possible stacks for a tree of depth d. Astack is a list of rules covering a path from the rootnode to one of the leaf nodes in the following form:R1?
??
?[...
?1...]R2?
??
?[...
?2...] ...Rs?
??
?[...
?s...],where ?1 = ?
is the root node and ?s is a leaf node,with stack depth s ?
d. Each rule Ri(i > 1) ex-pands node ?i?1, and thus has c choices by the defi-nition of grammar constant c. Furthermore, each rulein the stack is actually a dotted-rule, i.e., it is associ-ated with a dot position ranging from 0 to r, where ris the arity of the rule (length of English side of therule).
So the total number of stacks is O((cr)d).Besides the stack, each state also maintains (g?1)rightmost words of the hypothesis as the languagemodel signature, which amounts to O(|V |g?1).
Sothe total number of states is O((cr)d|V |g?1).
Fol-lowing previous work (Chiang, 2007), we assumea constant number of English translations for eachforeign word in the input sentence, so |V | = O(n).And as mentioned above, for each state, there are cpossible expansions, so the overall time complexityis f(n, d) = c(cr)d|V |g?1 = O((cr)dng?1).We do average-case analysis below because thetree depth (height) for a sentence of n words is arandom variable: in the worst-case it can be linear inn (degenerated into a linear-chain), but we assumethis adversarial situation does not happen frequently,and the average tree depth is O(log n).Theorem 1.
Assume for each n, the depth of aparse tree of n words, notated dn, distributes nor-mally with logarithmic mean and variance, i.e.,dn ?
N (?n, ?2n), where ?n = O(logn) and ?2n =O(logn), then the average-case complexity of thealgorithm is h(n) = O(nk log2(cr)+g?1) for constantk, thus polynomial in n.Proof.
From Lemma 1 and the definition of average-case complexity, we haveh(n) = Edn?N (?n,?2n)[f(n, dn)],where Ex?D[?]
denotes the expectation with respect278to the random variable x in distribution D.h(n) = Edn?N (?n,?2n)[f(n, dn)]= Edn?N (?n,?2n)[O((cr)dnng?1)],= O(ng?1Edn?N (?n,?2n)[(cr)dn ]),= O(ng?1Edn?N (?n,?2n)[exp(dn log(cr))]) (2)Since dn ?
N (?n, ?2n) is a normal distribution,dn log(cr) ?
N (?
?, ?
?2) is also a normal distribu-tion, where ??
= ?n log(cr) and ??
= ?n log(cr).Therefore exp(dn log(cr)) is a log-normal distribu-tion, and by the property of log-normal distribution,its expectation is exp (??
+ ??2/2).
So we haveEdn?N (?n,?2/2)[exp(dn log(cr))]= exp (??
+ ?
?2/2)= exp (?n log(cr) + ?2n log2(cr)/2)= exp (O(log n) log(cr) + O(log n) log2(cr)/2)= exp (O(log n) log2(cr))?
exp (k(log n) log2(cr)), for some constant k= exp (log nk log2(cr))= nk log2(cr).
(3)Plug it back to Equation (2), and we have theaverage-case complexityEdn [f(n, dn)] ?
O(ng?1nk log2(cr))= O(nk log2(cr)+g?1).
(4)Since k, c, r and g are constants, the average-casecomplexity is polynomial in sentence length n.The assumption dn ?
N (O(logn), O(logn))will be empirically verified in Section 5.3.4 Linear-time Beam SearchThough polynomial complexity is a desirable prop-erty in theory, the degree of the polynomial,O(log cr) might still be too high in practice, depend-ing on the translation grammar.
To make it linear-time, we apply the beam search idea from phrase-based again.
And once again, the only question todecide is the choice of ?binning?
: how to assign eachitem to a particular bin, depending on their progress?While the number of Chinese words covered is anatural progress indicator for phrase-based, it doesnot work for tree-to-string because, among the threeactions, only scanning grows the hypothesis.
Theprediction and completion actions do not make realprogress in terms of words, though they do makeprogress on the tree.
So we devise a novel progressindicator natural for tree-to-string translation: thenumber of tree nodes covered so far.
Initially thatnumber is zero, and in a prediction step which ex-pands node ?
using rule r, the number increments by|C(r)|, the size of the Chinese-side treelet of r. Forexample, a prediction step using rule r3 in Figure 2to expand VP@2 will increase the tree-node count by|C(r3)| = 6, since there are six tree nodes in thatrule (not counting leaf nodes or variables).Scanning and completion do not make progressin this definition since there is no new tree nodecovered.
In fact, since both of them are determin-istic operations, they are treated as ?closure?
op-erators in the real implementation, which meansthat after a prediction, we always do as many scan-ning/completion steps as possible until the symbolafter the dot is another node, where we have to waitfor the next prediction step.This method has |T | = O(n) bins where |T | isthe size of the parse tree, and each bin holds b items.Each item can expand to c new items, so the overallcomplexity of this beam search is O(ncb), which islinear in sentence length.4 Related WorkThe work of Watanabe et al (2006) is closest inspirit to ours: they also design an incremental decod-ing algorithm, but for the hierarchical phrase-basedsystem (Chiang, 2007) instead.
While we leave de-tailed comparison and theoretical analysis to a futurework, here we point out some obvious differences:1. due to the difference in the underlying trans-lation models, their algorithm runs in O(n2b)time with beam search in practice while oursis linear.
This is because each prediction stepnow has O(n) choices, since they need to ex-pand nodes like VP[1, 6] as:VP[1,6] ?
PP[1, i] VP[i, 6],where the midpoint i in general has O(n)choices (just like in CKY).
In other words, theirgrammar constant c becomes O(n).2. different binning criteria: we use the number oftree nodes covered, while they stick to the orig-279inal phrase-based idea of number of Chinesewords translated;3. as a result, their framework requires gram-mar transformation into the binary-branchingGreibach Normal Form (which is not alwayspossible) so that the resulting grammar alwayscontain at least one Chinese word in each rulein order for a prediction step to always makeprogress.
Our framework, by contrast, workswith any grammar.Besides, there are some other efforts less closelyrelated to ours.
As mentioned in Section 1, whilewe focus on enhancing syntax-based decoding withphrase-based ideas, other authors have explored thereverse, but also interesting, direction of enhancingphrase-based decoding with syntax-aware reorder-ing.
For example Galley and Manning (2008) pro-pose a shift-reduce style method to allow hiearar-chical non-local reorderings in a phrase-based de-coder.
While this approach is certainly better thanpure phrase-based reordering, it remains quadraticin run-time with beam search.Within syntax-based paradigms, cube pruning(Chiang, 2007; Huang and Chiang, 2007) has be-come the standard method to speed up +LM de-coding, which has been shown by many authors tobe highly effective; we will be comparing our incre-mental decoder with a baseline decoder using cubepruning in Section 5.
It is also important to notethat cube pruning and incremental decoding are notmutually exclusive, rather, they could potentially becombined to further speed up decoding.
We leavethis point to future work.Multipass coarse-to-fine decoding is another pop-ular idea (Venugopal et al, 2007; Zhang and Gildea,2008; Dyer and Resnik, 2010).
In particular, Dyerand Resnik (2010) uses a two-pass approach, wheretheir first-pass, ?LM decoding is also incrementaland polynomial-time (in the style of Earley (1970)algorithm), but their second-pass, +LM decoding isstill bottom-up CKY with cube pruning.5 ExperimentsTo test the merits of our incremental decoder weconduct large-scale experiments on a state-of-the-arttree-to-string system, and compare it with the stan-dard phrase-based system of Moses.
Furturemore wealso compare our incremental decoder with the stan-dard cube pruning approach on the same tree-to-string decoder.5.1 Data and System PreparationOur training corpus consists of 1.5M sentence pairswith about 38M/32M words in Chinese/English, re-spectively.
We first word-align them by GIZA++ andthen parse the Chinese sentences using the Berke-ley parser (Petrov and Klein, 2007), then applythe GHKM algorithm (Galley et al, 2004) to ex-tract tree-to-string translation rules.
We use SRILMToolkit (Stolcke, 2002) to train a trigram languagemodel with modified Kneser-Ney smoothing on thetarget side of training corpus.
At decoding time,we again parse the input sentences into trees, andconvert them into translation forest by rule pattern-matching (Mi et al, 2008).We use the newswire portion of 2006 NIST MTEvaluation test set (616 sentences) as our develop-ment set and the newswire portion of 2008 NISTMT Evaluation test set (691 sentences) as our testset.
We evaluate the translation quality using theBLEU-4 metric, which is calculated by the scriptmteval-v13a.pl with its default setting which is case-insensitive matching of n-grams.
We use the stan-dard minimum error-rate training (Och, 2003) totune the feature weights to maximize the system?sBLEU score on development set.We first verify the assumptions we made in Sec-tion 3.3 in order to prove the theorem that tree depth(as a random variable) is normally-distributed withO(logn) mean and variance.
Qualitatively, we veri-fied that for most n, tree depth d(n) does look like anormal distribution.
Quantitatively, Figure 6 showsthat average tree height correlates extremely wellwith 3.5 log n, while tree height variance is boundedby 5.5 log n.5.2 Comparison with Cube pruningWe implemented our incremental decoding algo-rithm in Python, and test its performance on the de-velopment set.
We first compare it with the stan-dard cube pruning approach (also implemented inPython) on the same tree-to-string system.1 Fig-1Our implementation of cube pruning follows (Chiang,2007; Huang and Chiang, 2007) where besides a beam size bof unique +LM items, there is also a hard limit (of 1000) on the2800123450  10  20  30  40  50  60  70AverageDecodingTime(Secs)Sentence Lengthincrementalcube pruning29.529.629.729.829.93030.10  0.2  0.4  0.6  0.8  1  1.2  1.4BLEUScoreAvg Decoding Time (secs per sentence)incrementalcube pruning(a) decoding time against sentence length (b) BLEU score against decoding timeFigure 7: Comparison with cube pruning.
The scatter plot in (a) confirms that our incremental decoding scales linearlywith sentence length, while cube pruning super-linearly (b = 50 for both).
The comparison in (b) shows that at thesame level of translation quality, incremental decoding is slightly faster than cube pruning, especially at smaller beams.05101520250  10  20  30  40  50TreeDepthd(n)Sentence Length (n)Avg DepthVariance3.5 log nFigure 6: Mean and variance of tree depth vs. sentencelength.
The mean depth clearly scales with 3.5 log n, andthe variance is bounded by 5.5 log n.ure 7(a) is a scatter plot of decoding times versussentence length (using beam b = 50 for both sys-tems), where we confirm that our incremental de-coder scales linearly, while cube pruning has a slighttendency of superlinearity.
Figure 7(b) is a side-by-side comparison of decoding speed versus transla-tion quality (in BLEU scores), using various beamsizes for both systems (b=10?70 for cube pruning,and b=10?110 for incremental).
We can see that in-cremental decoding is slightly faster than cube prun-ing at the same levels of translation quality, and thedifference is more pronounced at smaller beams: fornumber of (non-unique) pops from priority queues.05101520253035400  10  20  30  40  50  60  70AverageDecodingTime(Secs)Sentence LengthM +?M 10M 6M 0t2sFigure 8: Comparison of our incremental tree-to-stringdecoder with Moses in terms of speed.
Moses is shownwith various distortion limits (0, 6, 10, +?
; optimal: 10).example, at the lowest levels of translation quality(BLEU scores around 29.5), incremental decodingtakes only 0.12 seconds, which is about 4 times asfast as cube pruning.
We stress again that cube prun-ing and incremental decoding are not mutually ex-clusive, and rather they could potentially be com-bined to further speed up decoding.5.3 Comparison with MosesWe also compare with the standard phrase-basedsystem of Moses (Koehn et al, 2007), with stan-dard settings except for the ttable limit, which we setto 100.
Figure 8 compares our incremental decoder281system/decoder BLEU timeMoses (optimal dmax=10) 29.41 10.8tree-to-str: cube pruning (b=10) 29.51 0.65tree-to-str: cube pruning (b=20) 29.96 0.96tree-to-str: incremental (b=10) 29.54 0.32tree-to-str: incremental (b=50) 29.96 0.77Table 3: Final BLEU score and speed results on the testdata (691 sentences), compared with Moses and cubepruning.
Time is in seconds per sentence, including pars-ing time (0.21s) for the two tree-to-string decoders.with Moses at various distortion limits (dmax=0, 6,10, and +?).
Consistent with the theoretical anal-ysis in Section 2, Moses with no distortion limit(dmax = +?)
scale quadratically, and monotonedecoding (dmax = 0) scale linearly.
We use MERTto tune the best weights for each distortion limit, anddmax = 10 performs the best on our dev set.Table 3 reports the final results in terms of BLEUscore and speed on the test set.
Our linear-timeincremental decoder with the small beam of sizeb = 10 achieves a BLEU score of 29.54, compara-ble to Moses with the optimal distortion limit of 10(BLEU score 29.41).
But our decoding (includingsource-language parsing) only takes 0.32 seconds asentences, which is more than 30 times faster thanMoses.
With a larger beam of b = 50 our BLEUscore increases to 29.96, which is a half BLEU pointbetter than Moses, but still about 15 times faster.6 ConclusionWe have presented an incremental dynamic pro-gramming algorithm for tree-to-string translationwhich resembles phrase-based based decoding.
Thisalgorithm is the first incremental algorithm that runsin polynomial-time in theory, and linear-time inpractice with beam search.
Large-scale experimentson a state-of-the-art tree-to-string decoder confirmedthat, with a comparable (or better) translation qual-ity, it can run more than 30 times faster than thephrase-based system of Moses, even though ours isin Python while Moses in C++.
We also showed thatit is slightly faster (and scale better) than the popularcube pruning technique.
For future work we wouldlike to apply this algorithm to forest-based transla-tion and hierarchical system by pruning the first-pass?LM forest.
We would also combine cube pruningwith our incremental algorithm, and study its perfor-mance with higher-order language models.AcknowledgementsWe would like to thank David Chiang, KevinKnight, and Jonanthan Graehl for discussions andthe anonymous reviewers for comments.
In partic-ular, we are indebted to the reviewer who pointedout a crucial mistake in Theorem 1 and its proofin the submission.
This research was supported inpart by DARPA, under contract HR0011-06-C-0022under subcontract to BBN Technologies, and underDOI-NBC Grant N10AP20031, and in part by theNational Natural Science Foundation of China, Con-tracts 60736014 and 90920004.ReferencesDavid Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?208.Chris Dyer and Philip Resnik.
2010.
Context-free re-ordering, finite-state translation.
In Proceedings ofNAACL.Jay Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 13(2):94?102.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of EMNLP 2008.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of HLT-NAACL, pages 273?280.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Fast decoding with integrated language models.In Proceedings of ACL, Prague, Czech Rep., June.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of AMTA, Boston,MA, August.Liang Huang.
2007.
Binarization, synchronous bina-rization, and target-side binarization.
In Proc.
NAACLWorkshop on Syntax and Structure in Statistical Trans-lation.Kevin Knight.
1999.
Decoding complexity in word-replacement translation models.
Computational Lin-guistics, 25(4):607?615.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Constantin,and E. Herbst.
2007.
Moses: Open source toolkit forstatistical machine translation.
In Proceedings of ACL:demonstration sesion.282Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In Proceedings of AMTA, pages 115?124.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of COLING-ACL, pages 609?616.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL: HLT,Columbus, OH.Franz Joseph Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL,pages 160?167.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of HLT-NAACL.Stuart Shieber, Yves Schabes, and Fernando Pereira.1995.
Principles and implementation of deductiveparsing.
Journal of Logic Programming, 24:3?36.Andreas Stolcke.
2002.
Srilm - an extensible lan-guage modeling toolkit.
In Proceedings of ICSLP, vol-ume 30, pages 901?904.Ashish Venugopal, Andreas Zollmann, and Stephen Vo-gel.
2007.
An efficient two-pass approach tosynchronous-CFG driven statistical MT.
In Proceed-ings of HLT-NAACL.Taro Watanabe, Hajime Tsukuda, and Hideki Isozaki.2006.
Left-to-right target generation for hierarchicalphrase-based translation.
In Proceedings of COLING-ACL.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?404.Hao Zhang and Daniel Gildea.
2008.
Efficient multi-pass decoding for synchronous context free grammars.In Proceedings of ACL.283
