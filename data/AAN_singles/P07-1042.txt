Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 328?335,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Symbolic Approach to Near-Deterministic Surface Realisation using TreeAdjoining GrammarClaire GardentCNRS/LORIANancy, Franceclaire.gardent@loria.frEric KowINRIA/LORIA/UHPNancy, Franceeric.kow@loria.frAbstractSurface realisers divide into those used ingeneration (NLG geared realisers) and thosemirroring the parsing process (Reversible re-alisers).
While the first rely on grammars noteasily usable for parsing, it is unclear howthe second type of realisers could be param-eterised to yield from among the set of pos-sible paraphrases, the paraphrase appropri-ate to a given generation context.
In this pa-per, we present a surface realiser which com-bines a reversible grammar (used for pars-ing and doing semantic construction) with asymbolic means of selecting paraphrases.1 IntroductionIn generation, the surface realisation task consists inmapping a semantic representation into a grammati-cal sentence.Depending on their use, on their degree of non-determinism and on the type of grammar they as-sume, existing surface realisers can be divided intotwo main categories namely, NLG (Natural Lan-guage Generation) geared realisers and reversiblerealisers.NLG geared realisers are meant as modules in afull-blown generation system and as such, they areconstrained to be deterministic: a generation systemmust output exactly one text, no less, no more.
In or-der to ensure this determinism, NLG geared realisersgenerally rely on theories of grammar which sys-tematically link form to function such as systemicfunctional grammar (SFG, (Matthiessen and Bate-man, 1991)) and, to a lesser extent, Meaning TextTheory (MTT, (Mel?cuk, 1988)).
In these theories, asentence is associated not just with a semantic rep-resentation but with a semantic representation en-riched with additional syntactic, pragmatic and/ordiscourse information.
This additional informationis then used to constrain the realiser output.1 Onedrawback of these NLG geared realisers however, isthat the grammar used is not usually reversible i.e.,cannot be used both for parsing and for generation.Given the time and expertise involved in developinga grammar, this is a non-trivial drawback.Reversible realisers on the other hand, are meantto mirror the parsing process.
They are used on agrammar developed for parsing and equipped with acompositional semantics.
Given a string and sucha grammar, a parser will assign the input stringall the semantic representations associated with thatstring by the grammar.
Conversely, given a seman-tic representation and the same grammar, a realiserwill assign the input semantics all the strings as-sociated with that semantics by the grammar.
Insuch approaches, non-determinism is usually han-dled by statistical filtering: treebank induced prob-abilities are used to select from among the possibleparaphrases, the most probable one.
Since the mostprobable paraphrase is not necessarily the most ap-propriate one in a given context, it is unclear how-ever, how such realisers could be integrated into ageneration system.In this paper, we present a surface realiser which1On the other hand, one of our reviewers noted that ?de-terminism?
often comes more from defaults when input con-straints are not supplied.
One might see these realisers as beingless deterministic than advertised; however, the point is that itis possible to supply the constraints that ensure determinism.328combines reversibility with a symbolic approach todeterminism.
The grammar used is fully reversible(it is used for parsing) and the realisation algorithmcan be constrained by the input so as to ensure aunique output conforming to the requirement of agiven (generation) context.
We show both that thegrammar used has a good paraphrastic power (itis designed in such a way that grammatical para-phrases are assigned the same semantic representa-tions) and that the realisation algorithm can be usedeither to generate all the grammatical paraphrases ofa given input or just one provided the input is ade-quately constrained.The paper is structured as follows.
Section 2 in-troduces the grammar used namely, a Feature BasedLexicalised Tree Adjoining Grammar enriched witha compositional semantics.
Importantly, this gram-mar is compiled from a more abstract specification(a so-called ?meta-grammar?)
and as we shall see, itis this feature which permits a natural and system-atic coupling of semantic literals with syntactic an-notations.
Section 3 defines the surface realisationalgorithm used to generate sentences from semanticformulae.
This algorithm is non-deterministic andproduces all paraphrases associated by the gram-mar with the input semantics.
We then go on toshow (section 4) how this algorithm can be usedon a semantic input enriched with syntactic or moreabstract control annotations and further, how theseannotations can be used to select from among theset of admissible paraphrases precisely these whichobey the constraints expressed in the added annota-tions.
Section 5 reports on a quantitative evaluationbased on the use of a core tree adjoining grammarfor French.
The evaluation gives an indication of theparaphrasing power of the grammar used as well assome evidence of the deterministic nature of the re-aliser.
Section 6 relates the proposed approach toexisting work and section 7 concludes with pointersfor further research.2 The grammarWe use a unification based version of LTAG namely,Feature-based TAG.
A Feature-based TAG (FTAG,(Vijay-Shanker and Joshi, 1988)) consists of a setof (auxiliary or initial) elementary trees and of twotree composition operations: substitution and ad-junction.
Initial trees are trees whose leaves are la-belled with substitution nodes (marked with a dow-narrow) or terminal categories.
Auxiliary trees aredistinguished by a foot node (marked with a star)whose category must be the same as that of the rootnode.
Substitution inserts a tree onto a substitutionnode of some other tree while adjunction inserts anauxiliary tree into a tree.
In an FTAG, the tree nodesare furthermore decorated with two feature struc-tures (called top and bottom) which are unified dur-ing derivation as follows.
On substitution, the topof the substitution node is unified with the top of theroot node of the tree being substituted in.
On adjunc-tion, the top of the root of the auxiliary tree is uni-fied with the top of the node where adjunction takesplace; and the bottom features of the foot node areunified with the bottom features of this node.
At theend of a derivation, the top and bottom of all nodesin the derived tree are unified.To associate semantic representations with natu-ral language expressions, the FTAG is modified asproposed in (Gardent and Kallmeyer, 2003).NPjJohnname(j,john)SNP?s VPrVrunsrun(r,s)VPxoften VP*often(x)?
name(j,john), run(r,j), often(r)Figure 1: Flat Semantics for ?John often runs?Each elementary tree is associated with a flat se-mantic representation.
For instance, in Figure 1,2the trees for John, runs and often are associated withthe semantics name(j,john), run(r,s) and often(x) re-spectively.Importantly, the arguments of a semantic functorare represented by unification variables which occurboth in the semantic representation of this functorand on some nodes of the associated syntactic tree.For instance in Figure 1, the semantic index s oc-curring in the semantic representation of runs alsooccurs on the subject substitution node of the asso-ciated elementary tree.2Cx/Cx abbreviate a node with category C and a top/bottomfeature structure including the feature-value pair { index : x}.329The value of semantic arguments is determined bythe unifications resulting from adjunction and sub-stitution.
For instance, the semantic index s in thetree for runs is unified during substitution with thesemantic indices labelling the root nodes of the treefor John.
As a result, the semantics of John oftenruns is(1) {name(j,john),run(r,j),often(r)}The grammar used describes a core fragment ofFrench and contains around 6 000 elementary trees.It covers some 35 basic subcategorisation framesand for each of these frames, the set of argument re-distributions (active, passive, middle, neuter, reflex-ivisation, impersonal, passive impersonal) and of ar-gument realisations (cliticisation, extraction, omis-sion, permutations, etc.)
possible for this frame.
Asa result, it captures most grammatical paraphrasesthat is, paraphrases due to diverging argument real-isations or to different meaning preserving alterna-tion (e.g., active/passive or clefted/non-clefted sen-tence).3 The surface realiser, GenIThe basic surface realisation algorithm used is a bot-tom up, tabular realisation algorithm (Kay, 1996)optimised for TAGs.
It follows a three step strat-egy which can be summarised as follows.
Given anempty agenda, an empty chart and an input seman-tics ?
:Lexical selection.
Select all elementary treeswhose semantics subsumes (part of) ?.
Storethese trees in the agenda.
Auxiliary treesdevoid of substitution nodes are stored in aseparate agenda called the auxiliary agenda.Substitution phase.
Retrieve a tree from theagenda, add it to the chart and try to combine itby substitution with trees present in the chart.Add any resulting derived tree to the agenda.Stop when the agenda is empty.Adjunction phase.
Move the chart trees to theagenda and the auxiliary agenda trees to thechart.
Retrieve a tree from the agenda, add itto the chart and try to combine it by adjunctionwith trees present in the chart.
Add any result-ing derived tree to the agenda.
Stop when theagenda is empty.When processing stops, the yield of any syntacti-cally complete tree whose semantics is ?
yields anoutput i.e., a sentence.The workings of this algorithm can be illustratedby the following example.
Suppose that the input se-mantics is (1).
In a first step (lexical selection), theelementary trees selected are the ones for John, runs,often.
Their semantics subsumes part of the input se-mantics.
The trees for John and runs are placed onthe agenda, the one for often is placed on the auxil-iary agenda.The second step (the substitution phase) consistsin systematically exploring the possibility of com-bining two trees by substitution.
Here, the tree forJohn is substituted into the one for runs, and the re-sulting derived tree for John runs is placed on theagenda.
Trees on the agenda are processed one byone in this fashion.
When the agenda is empty, in-dicating that all combinations have been tried, weprepare for the next phase.All items containing an empty substitution nodeare erased from the chart (here, the tree anchored byruns).
The agenda is then reinitialised to the contentof the chart and the chart to the content of the aux-iliary agenda (here often).
The adjunction phaseproceeds much like the previous phase, except thatnow all possible adjunctions are performed.
Whenthe agenda is empty once more, the items in the chartwhose semantics matches the input semantics are se-lected, and their strings printed out, yielding in thiscase the sentence John often runs.4 Paraphrase selectionThe surface realisation algorithm just sketched isnon-deterministic.
Given a semantic formula, itmight produce several outputs.
For instance, giventhe appropriate grammar for French, the input in (2a)will generate the set of paraphrases partly given in(2b-2k).
(2) a. lj :jean(j) la:aime(e,j,m) lm:marie(m)b. Jean aime Mariec.
Marie est aime?e par Jeand.
C?est Jean qui aime Mariee.
C?est Jean par qui Marie est aime?ef.
C?est par Jean qu?est aime?e Marieg.
C?est Jean dont est aime?e Marieh.
C?est Jean dont Marie est aime?ei.
C?est Marie qui est aime?e par Jean330j.
C?est Marie qu?aime Jeank.
C?est Marie que Jean aimeTo select from among all possible paraphrases ofa given input, exactly one paraphrase, NLG gearedrealisers use symbolic information to encode syn-tactic, stylistic or pragmatic constraints on the out-put.
Thus for instance, both REALPRO (Lavoie andRambow, 1997) and SURGE (Elhadad and Robin,1999) assume that the input associates semantic lit-erals with low level syntactic and lexical informa-tion mostly leaving the realiser to just handle in-flection, word order, insertion of grammatical wordsand agreement.
Similarly, KPML (Matthiessen andBateman, 1991) assumes access to ideational, inter-personal and textual information which roughly cor-responds to semantic, mood/voice, theme/rheme andfocus/ground information.In what follows, we first show that the semanticinput assumed by the realiser sketched in the previ-ous section can be systematically enriched with syn-tactic information so as to ensure determinism.
Wethen indicate how the satisfiability of this enrichedinput could be controlled.4.1 At most one realisationIn the realisation algorithm sketched in Section 3,non-determinism stems from lexical ambiguity:3 foreach (combination of) literal(s) l in the input thereusually is more than one TAG elementary tree whosesemantics subsumes l. Thus each (combination of)literal(s) in the input selects a set of elementarytrees and the realiser output is the set of combi-nations of selected lexical trees which are licensedby the grammar operations (substitution and adjunc-tion) and whose semantics is the input.One way to enforce determinism consists in en-suring that each literal in the input selects exactlyone elementary tree.
For instance, suppose we wantto generate (2b), repeated here as (3a), rather than3Given two TAG trees, there might also be several waysof combining them thereby inducing more non-determinism.However in practice we found that most of this non-determinism is due either to over-generation (cases where thegrammar is not sufficiently constrained and allows for one treeto adjoin to another tree in several places) or to spurious deriva-tion (distinct derivations with identical semantics).
The few re-maining cases that are linguistically correct are due to varyingmodifier positions and could be constrained by a sophisticatedfeature decorations in the elementary tree.any of the paraphrases listed in (2c-2k).
Intuitively,the syntactic constraints to be expressed are thosegiven in (3b).
(3) a. Jean aime Marieb.
Canonical Nominal Subject, Active verb form,Canonical Nominal Objectc.
lj :jean(j) la:aime(e,j,m) lm:marie(m)The question is how precisely to formulate theseconstraints, how to associate them with the seman-tic input assumed in Section 3 and how to ensurethat the constraints used do enforce uniqueness ofselection (i.e., that for each input literal, exactly oneelementary tree is selected)?
To answer this, we relyon a feature of the grammar used, namely that eachelementary tree is associated with a linguisticallymeaningful unique identifier.The reason for this is that the grammar is com-piled from a higher level description where tree frag-ments are first encapsulated into so-called classesand then explicitly combined (by inheritance, con-junction and disjunction) to produce the grammarelementary trees (cf.
(Crabbe?
and Duchier, 2004)).More generally, each elementary tree in the gram-mar is associated with the set of classes used to pro-duce that tree and importantly, this set of classes(we will call this the tree identifier) provides a dis-tinguishing description (a unique identifier) for thattree: a tree is defined by a specific combination ofclasses and conversely, a specific combination ofclasses yields a unique tree.4 Thus the set of classesassociated by the compilation process with a givenelementary tree can be used to uniquely identify thattree.Given this, surface realisation is constrained asfollows.1.
Each tree identifier Id(tree) is mapped into asimplified set of tree properties TPt.
Thereare two reasons for this simplification.
First,some classes are irrelevant.
For instance, theclass used to enforce subject-verb agreementis needed to ensure this agreement but doesnot help in selecting among competing trees.Second, a given class C can be defined to be4This is not absolutely true as a tree identifier only reflectspart of the compilation process.
In practice, they are few ex-ceptions though so that distinct trees whose tree identifiers areidentical can be manually distinguished.331equivalent to the combination of other classesC1 .
.
.
Cn and consequently a tree identifiercontaining C,C1 .
.
.
Cn can be reduced to in-clude either C or C1 .
.
.
Cn.2.
Each literal li in the input is associated with atree property set TPi (i.e., the input we gener-ate from is enriched with syntactic information)3.
During realisation, for each literal/tree propertypair ?li : TPi?
in the enriched input semantics,lexical selection is constrained to retrieve onlythose trees (i) whose semantics subsumes li and(ii) whose tree properties are TPiSince each literal is associated with a (simpli-fied) tree identifier and each tree identifier uniquelyidentifies an elementary tree, realisation produces atmost one realisation.Examples 4a-4c illustrates the kind of constraintsused by the realiser.
(4) a. lj :jean(j)/ProperNamela:aime(e,j,m)/[CanonicalNominalSubject,ActiveVerbForm, CanonicalNominalObject]lm:marie(m)/ProperNameJean aime Marie* Jean est aime?
de Marieb.
lc:le(c)/Detlc:chien(c)/Nounld:dort(e1,c)/RelativeSubjectlr:ronfle(e2,c)/CanonicalSubjectLe chien qui dort ronfle* Le chien qui ronfle dortc.
lj :jean(j)/ProperNamelp:promise(e1,j,m,e2)/[CanonicalNominalSubject,ActiveVerbForm, CompletiveObject]lm:marie(m)/ProperNamele2:partir(e2,j)/InfinitivalVerbJean promet a` marie de partir* Jean promet a` marie qu?il partira4.2 At least one realisationFor a realiser to be usable by a generation system,there must be some means to ensure that its inputis satisfiable i.e., that it can be realised.
How canthis be done without actually carrying out realisationi.e., without checking that the input is satisfiable?Existing realisers indicate two types of answers tothat dilemma.A first possibility would be to draw on (Yang etal., 1991)?s proposal and compute the enriched in-put based on the traversal of a systemic network.More specifically, one possibility would be to con-sider a systemic network such as NIGEL, precom-pile all the functional features associated with eachpossible traversal of the network, map them onto thecorresponding tree properties and use the resultingset of tree properties to ensure the satisfiability ofthe enriched input.Another option would be to check the wellformedness of the input at some level of the linguis-tic theory on which the realiser is based.
Thus forinstance, REALPRO assumes as input a well formeddeep syntactic structure (DSyntS) as defined byMeaning Text Theory (MTT) and similarly, SURGEtakes as input a functional description (FD) which inessence is an underspecified grammatical structurewithin the SURGE grammar.
In both cases, thereis no guarantee that the input be satisfiable sinceall the other levels of the linguistic theory must beverified for this to be true.
In MTT, the DSyntSmust first be mapped onto a surface syntactic struc-ture and then successively onto the other levels ofthe theory while in SURGE, the input FD can be re-alised only if it provides consistent information fora complete top-down traversal of the grammar rightdown to the lexical level.
In short, in both cases, thewell formedness of the input can be checked withrespect to some criteria (e.g., well formedness of adeep syntactic structure in MTT, well formedness ofa FD in SURGE) but this well formedness does notguarantee satisfiability.
Nonetheless this basic wellformedness check is important as it provides someguidance as to what an acceptable input to the re-aliser should look like.We adopt a similar strategy and resort to the no-tion of polarity neutral input to control the wellformedness of the enriched input.
The proposaldraws on ideas from (Koller and Striegnitz, 2002;Gardent and Kow, 2005) and aims to determinewhether for a given input (a set of TAG elemen-tary trees whose semantics equate the input seman-tics), syntactic requirements and resources cancelout.
More specifically, the aim is to determinewhether given the input set of elementary trees, eachsubstitution and each adjunction requirement is sat-isfied by exactly one elementary tree of the appro-priate syntactic category and semantic index.332Roughly,5 the technique consists in (automati-cally) associating with each elementary tree a po-larity signature reflecting its substitution/adjunctionrequirements and resources and in computing thegrand polarity of each possible combination of treescovering the input semantics.
Each such combina-tion whose total polarity is non-null is then filteredout (not considered for realisation) as it cannot pos-sibly lead to a valid derivation (either a requirementcannot be satisfied or a resource cannot be used).In the context of a generation system, polaritychecking can be used to check the satisfiability of theinput or more interestingly, to correct an ill formedinput i.e., an input which can be detected as beingunsatisfiable.To check a given input, it suffices to compute itspolarity count.
If it is non-null, the input is unsatis-fiable and should be revised.
This is not very usefulhowever, as the enriched input ensures determinismand thereby make realisation very easy, indeed al-most as easy as polarity checking.More interestingly, polarity checking can be usedto suggest ways of fixing an ill formed input.
In sucha case, the enriched input is stripped of its controlannotations, realisation proceeds on the basis of thissimplified input and polarity checking is used to pre-select all polarity neutral combinations of elemen-tary trees.
A closest match (i.e.
the polarity neutralcombination with the greatest number of control an-notations in common with the ill formed input) tothe ill formed input is then proposed as a probablysatisfiable alternative.5 EvaluationTo evaluate both the paraphrastic power of the re-aliser and the impact of the control annotations onnon-determinism, we used a graduated test-suitewhich was built by (i) parsing a set of sentences, (ii)selecting the correct meaning representations fromthe parser output and (iii) generating from thesemeaning representations.
The gradation in the testsuite complexity was obtained by partitioning theinput into sentences containing one, two or three fi-nite verbs and by choosing cases allowing for differ-ent paraphrasing patterns.
More specifically, the test5Lack of space prevents us from giving much details here.We refer the reader to (Koller and Striegnitz, 2002; Gardent andKow, 2005) for more details.suite includes cases involving the following types ofparaphrases:?
Grammatical variations in the realisations ofthe arguments (cleft, cliticisation, question, rel-ativisation, subject-inversion, etc.)
or of theverb (active/passive, impersonal)?
Variations in the realisation of modifiers (e.g.,relative clause vs adjective, predicative vs non-predicative adjective)?
Variations in the position of modifiers (e.g.,pre- vs post-nominal adjective)?
Variations licensed by a morpho-derivationallink (e.g., to arrive/arrival)On a test set of 80 cases, the paraphrastic levelvaries between 1 and over 50 with an average of18 paraphrases per input (taking 36 as upper cutoff point in the paraphrases count).
Figure 5 givesa more detailed description of the distribution ofthe paraphrastic variation.
In essence, 42% of thesentences with one finite verb accept 1 to 3 para-phrases (cases of intransitive verbs), 44% accept 4to 28 paraphrases (verbs of arity 2) and 13% yieldmore than 29 paraphrases (ditransitives).
For sen-tences containing two finite verbs, the ratio is 5%for 1 to 3 paraphrases, 36% for 4 to 14 paraphrasesand 59% for more than 14 paraphrases.
Finally, sen-tences containing 3 finite verbs all accept more than29 paraphrases.Two things are worth noting here.
First, the para-phrase figures might seem low wrt to e.g., work by(Velldal and Oepen, 2006) which mentions severalthousand outputs for one given input and an averagenumber of realisations per input varying between85.7 and 102.2.
Admittedly, the French grammarwe are using has a much more limited coverage thanthe ERG (the grammar used by (Velldal and Oepen,2006)) and it is possible that its paraphrastic poweris lower.
However, the counts we give only takeinto account valid paraphrases of the input.
In otherwords, overgeneration and spurious derivations areexcluded from the toll.
This does not seem to be thecase in (Velldal and Oepen, 2006)?s approach wherethe count seems to include all sentences associatedby the grammar with the input semantics.Second, although the test set may seem small it isimportant to keep in mind that it represents 80 inputs333with distinct grammatical and paraphrastic proper-ties.
In effect, these 80 test cases yields 1 528 dis-tinct well-formed sentences.
This figure comparesfavourably with the size of the largest regression testsuite used by a symbolic NLG realiser namely, theSURGE test suite which contains 500 input eachcorresponding to a single sentence.
It also comparesreasonably with other more recent evaluations (Call-away, 2003; Langkilde-Geary, 2002) which derivetheir input data from the Penn Treebank by trans-forming each sentence tree into a format suitable forthe realiser (Callaway, 2003).
For these approaches,the test set size varies between roughly 1 000 andalmost 3 000 sentences.
But again, it is worth stress-ing that these evaluations aim at assessing coverageand correctness (does the realiser find the sentenceused to derive the input by parsing it?)
rather thanthe paraphrastic power of the grammar.
They fail toprovide a systematic assessment of how many dis-tinct grammatical paraphrases are associated witheach given input.To verify the claim that tree properties can be usedto ensure determinism (cf.
footnote 4), we startedby eliminating from the output all ill-formed sen-tences.
We then automatically associated each well-formed output with its set of tree properties.
Finally,for each input semantics, we did a systematic pair-wise comparison of the tree property sets associatedwith the input realisations and we checked whetherfor any given input, there were two (or more) dis-tinct paraphrases whose tree properties were thesame.
We found that such cases represented slightlyover 2% of the total number of (input,realisations)pairs.
Closer investigation of the faulty data indi-cates two main reasons for non-determinism namely,trees with alternating order of arguments and deriva-tions with distinct modifier adjunctions.
Both casescan be handled by modifying the grammar in sucha way that those differences are reflected in the treeproperties.6 Related workThe approach presented here combines a reversiblegrammar realiser with a symbolic approach to para-phrase selection.
We now compare it to existing sur-faces realisers.NLG geared realisers.
Prominent generalpurpose NLG geared realisers include REALPRO,SURGE, KPML, NITROGEN and HALOGEN.
Fur-thermore, HALOGEN has been shown to achievebroad coverage and high quality output on a set of 2400 input automatically derived from the Penn tree-bank.The main difference between these and thepresent approach is that our approach is based on areversible grammar whilst NLG geared realisers arenot.
This has several important consequences.First, it means that one and the same grammar andlexicon can be used both for parsing and for gener-ation.
Given the complexity involved in developingsuch resources, this is an important feature.Second, as demonstrated in the Redwood LingoTreebank, reversibility makes it easy to rapidly cre-ate very large evaluation suites: it suffices to parse aset of sentences and select from the parser output thecorrect semantics.
In contrast, NLG geared realis-ers either work on evaluation sets of restricted size(500 input for SURGE, 210 for KPML) or requirethe time expensive implementation of a preprocessortransforming e.g., Penn Treebank trees into a formatsuitable for the realisers.
For instance, (Callaway,2003) reports that the implementation of such a pro-cessor for SURGE was the most time consuming partof the evaluation with the resulting component con-taining 4000 lines of code and 900 rules.Third, a reversible grammar can be exploited tosupport not only realisation but also its reverse,namely semantic construction.
Indeed, reversibilityis ensured through a compositional semantics that is,through a tight coupling between syntax and seman-tics.
In contrast, NLG geared realisers often haveto reconstruct this association in rather ad hoc ways.Thus for instance, (Yang et al, 1991) resorts to ad334hoc ?mapping tables?
to associate substitution nodeswith semantic indices and ?fr-nodes?
to constrainadjunction to the correct nodes.
More generally, thelack of a clearly defined compositional semantics inNLG geared realisers makes it difficult to see howthe grammar they use could be exploited to also sup-port semantic construction.Fourth, the grammar can be used both to gener-ate and to detect paraphrases.
It could be used forinstance, in combination with the parser and the se-mantic construction module described in (Gardentand Parmentier, 2005), to support textual entailmentrecognition or answer detection in question answer-ing.Reversible realisers.
The realiser presented herediffers in mainly two ways from existing reversiblerealisers such as (White, 2004)?s CCG system orthe HPSG ERG based realiser (Carroll and Oepen,2005).First, it permits a symbolic selection of the out-put paraphrase.
In contrast, existing reversible re-alisers use statistical information to select from theproduced output the most plausible paraphrase.Second, particular attention has been paid to thetreatment of paraphrases in the grammar.
Recallthat TAG elementary trees are grouped into familiesand further, that the specific TAG we use is com-piled from a highly factorised description.
We relyon these features to associate one and the same se-mantic to large sets of trees denoting semanticallyequivalent but syntactically distinct configurations(cf.
(Gardent, 2006)).7 ConclusionThe realiser presented here, GENI, exploits a gram-mar which is produced semi-automatically by com-piling a high level grammar description into a TreeAdjoining Grammar.
We have argued that a side-effect of this compilation process ?
namely, the as-sociation with each elementary tree of a set of treeproperties ?
can be used to constrain the realiseroutput.
The resulting system combines the advan-tages of two orthogonal approaches.
From the re-versible approach, it takes the reusability, the abilityto rapidly create very large test suites and the capac-ity to both generate and detect paraphrases.
Fromthe NLG geared paradigm, it takes the ability tosymbolically constrain the realiser output to a givengeneration context.GENI is free (GPL) software and is available athttp://trac.loria.fr/?geni.ReferencesCharles B. Callaway.
2003.
Evaluating coverage for large sym-bolic NLG grammars.
In 18th IJCAI, pages 811?817, Aug.J.
Carroll and S. Oepen.
2005.
High efficiency realization for awide-coverage unification grammar.
2nd IJCNLP.B.
Crabbe?
and D. Duchier.
2004.
Metagrammar redux.
InCSLP, Copenhagen.M.
Elhadad and J. Robin.
1999.
SURGE: a comprehensiveplug-in syntactic realization component for text generation.Computational Linguistics.C.
Gardent and L. Kallmeyer.
2003.
Semantic construction inFTAG.
In 10th EACL, Budapest, Hungary.C.
Gardent and E. Kow.
2005.
Generating and selecting gram-matical paraphrases.
ENLG, Aug.C.
Gardent and Y. Parmentier.
2005.
Large scale semantic con-struction for Tree Adjoining Grammars.
LACL05.C.
Gardent.
2006.
Integration d?une dimension semantiquedans les grammaires d?arbres adjoints.
TALN.M.
Kay.
1996.
Chart Generation.
In 34th ACL, pages 200?204,Santa Cruz, California.A.
Koller and K. Striegnitz.
2002.
Generation as dependencyparsing.
In 40th ACL, Philadelphia.I.
Langkilde-Geary.
2002.
An empirical verification of cover-age and correctness for a general-purpose sentence genera-tor.
In Proceedings of the INLG.B.
Lavoie and O. Rambow.
1997.
RealPro?a fast, portablesentence realizer.
ANLP?97.C.
Matthiessen and J.A.
Bateman.
1991.
Text generationand systemic-functional linguistics: experiences from En-glish and Japanese.
Frances Pinter Publishers and St. Mar-tin?s Press, London and New York.I.A.
Mel?cuk.
1988.
Dependency Syntax: Theorie and Prac-tice.
State University Press of New York.Erik Velldal and Stephan Oepen.
2006.
Statistical ranking intactical generation.
In EMNLP, Sydney, Australia.K.
Vijay-Shanker and AK Joshi.
1988.
Feature StructuresBased Tree Adjoining Grammars.
Proceedings of the 12thconference on Computational linguistics, 55:v2.M.
White.
2004.
Reining in CCG chart realization.
In INLG,pages 182?191.G.
Yang, K. McKoy, and K. Vijay-Shanker.
1991.
From func-tional specification to syntactic structure.
Computational In-telligence, 7:207?219.335
