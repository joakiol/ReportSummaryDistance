Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 920?927,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Language-Independent Unsupervised Modelfor Morphological SegmentationVera DembergSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9LW, GBv.demberg@sms.ed.ac.ukAbstractMorphological segmentation has beenshown to be beneficial to a range of NLPtasks such as machine translation, speechrecognition, speech synthesis and infor-mation retrieval.
Recently, a number ofapproaches to unsupervised morphologicalsegmentation have been proposed.
Thispaper describes an algorithm that drawsfrom previous approaches and combinesthem into a simple model for morpholog-ical segmentation that outperforms otherapproaches on English and German, andalso yields good results on agglutinativelanguages such as Finnish and Turkish.We also propose a method for detectingvariation within stems in an unsupervisedfashion.
The segmentation quality reachedwith the new algorithm is good enough toimprove grapheme-to-phoneme conversion.1 IntroductionMorphological segmentation has been shown to bebeneficial to a number of NLP tasks such as ma-chine translation (Goldwater and McClosky, 2005),speech recognition (Kurimo et al, 2006), informa-tion retrieval (Monz and de Rijke, 2002) and ques-tion answering.
Segmenting a word into meaning-bearing units is particularly interesting for morpho-logically complex languages where words can becomposed of several morphemes through inflection,derivation and composition.
Data sparseness forsuch languages can be significantly decreased whenwords are decomposed morphologically.
There ex-ist a number of rule-based morphological segmen-tation systems for a range of languages.
However,expert knowledge and labour are expensive, and theanalyzers must be updated on a regular basis in or-der to cope with language change (the emergence ofnew words and their inflections).
One might arguethat unsupervised algorithms are not an interestingoption from the engineering point of view, becauserule-based systems usually lead to better results.However, segmentations from an unsupervised algo-rithm that is language-independent are ?cheap?, be-cause the only resource needed is unannotated text.If such an unsupervised system reaches a perfor-mance level that is good enough to help another task,it can constitute an attractive additional component.Recently, a number of approaches to unsupervisedmorphological segmentation have been proposed.These algorithms autonomously discover morphemesegmentations in unannotated text corpora.
Here wedescribe a modification of one such unsupervised al-gorithm, RePortS (Keshava and Pitler, 2006).
TheRePortS algorithm performed best on English in arecent competition on unsupervised morphologicalsegmentation (Kurimo et al, 2006), but had very lowrecall on morphologically more complex languageslike German, Finnish or Turkish.
We add a newstep designed to achieve higher recall on morpho-logically complex languages and propose a methodfor identifying related stems that underwent regularnon-concatenative morphological processes such asumlauting or ablauting, as well as morphological al-ternations along morpheme boundaries.The paper is structured as follows: Section9202 discusses the relationship between language-dependency and the level of supervision of a learn-ing algorithm.
We then give an outline of the mainsteps of the RePortS algorithm in section 3 and ex-plain the modifications to the original algorithm insection 4.
Section 5 compares results for differentlanguages, quantifies the gains from the modifica-tions on the algorithm and evaluates the algorithmon a grapheme-to-phoneme conversion task.
We fi-nally summarize our results in section 6.2 Previous WorkThe world?s languages can be classified accordingto their morphology into isolating languages (littleor no morphology, e.g.
Chinese), agglutinative lan-guages (where a word can be decomposed into alarge number of morphemes, e.g.
Turkish) and in-flectional languages (morphemes are fused together,e.g.
Latin).Phenomena that are difficult to cope with formany of the unsupervised algorithms are non-concatenative processes such as vowel harmoniza-tion, ablauting and umlauting, or modifications atthe boundaries of morphemes, as well as infixation(e.g.
in Tagalog: sulat ?write?, s-um-ulat ?wrote?, s-in-ulat ?was written?
), circumfixation (e.g.
in Ger-man: mach-en ?do?, ge-mach-t ?done?
), the Ara-bic broken plural or reduplications (e.g.
in Pinge-lapese: mejr ?to sleep?, mejmejr ?sleeping?, mejme-jmejr ?still sleeping?).
For words that are subject toone of the above processes it is not trivial to automat-ically group related words and detect regular trans-formational patterns.A range of automated algorithms for morpholog-ical analysis cope with concatenative phenomena,and base their mechanics on statistics about hypoth-esized stems and affixes.
These approaches can befurther categorized into ones that use conditionalentropy between letters to detect segment bound-aries (Harris, 1955; Hafer and Weiss, 1974; De?jean,1998; Monson et al, 2004; Bernhard, 2006; Ke-shava and Pitler, 2006; Bordag, 2006), approachesthat use minimal description length and thereby min-imize the size of the lexicon as measured in en-tries and links between the entries to constitute aword form (Goldsmith, 2001; Creutz and Lagus,2006).
These two types of approaches very closelytie the orthographic form of the word to the mor-phemes.
They are thus not well-suited for copingwith stem changes or modifications at the edges ofmorphemes.
Only very few approaches have ad-dressed word internal variations (Yarowski and Wi-centowski, 2000; Neuvel and Fulop, 2002).A popular and effective approach for detecting in-flectional paradigms and filter affix lists is to clustertogether affixes or regular transformational patternsthat occur with the same stem (Monson et al, 2004;Goldsmith, 2001; Gaussier, 1999; Schone and Juraf-sky, 2000; Yarowski and Wicentowski, 2000; Neu-vel and Fulop, 2002; Jacquemin, 1997).
We drawfrom this idea of clustering in order to detect ortho-graphic variants of stems; see Section 4.3.A few approaches also take into account syntac-tic and semantic information from the context theword occurs (Schone and Jurafsky, 2000; Bordag,2006; Yarowski and Wicentowski, 2000; Jacquemin,1997).
Exploiting semantic and syntactic informa-tion is very attractive because it adds an additionaldimension, but these approaches have to cope withmore severe data sparseness issues than approachesthat emphasize word-internal cues, and they canbe computationally expensive, especially when theyuse LSA.The original RePortS algorithm assumes mor-phology to be concatenative, and specializes on pre-fixation and suffixation, like most of the above ap-proaches, which were developed and implementedfor English (Goldsmith, 2001; Schone and Jurafsky,2000; Neuvel and Fulop, 2002; Yarowski and Wi-centowski, 2000; Gaussier, 1999).
However, manylanguages are morphologically more complex.
Forexample in German, an algorithm also needs to copewith compounding, and in Turkish words can bevery long and complex.
We therefore extended theoriginal RePortS algorithm to be better adapted tocomplex morphology and suggest a method for cop-ing with stem variation.
These modifications ren-der the algorithm more language-independent andthereby make it attractive for applying to other lan-guages as well.3 The RePortS AlgorithmOn English, the RePortS algorithm clearly out-performed all other systems in Morpho Challenge92120051 (Kurimo et al, 2006), obtaining an F-measureof 76.8% (76.2% prec., 77.4% recall).
The next bestsystem obtained an F-score of 69%.
However, thealgorithm does not perform as well on other lan-guages (Turkish, Finnish, German) due to low re-call (see (Keshava and Pitler, 2006) and (Demberg,2006), p. 47).There are three main steps in the algorithm.
First,the data is structured in two trees, which provide thebasis for efficient calculation of transitional proba-bilities of a letter given its context.
The second stepis the affix acquisition step, during which a set ofmorphemes is identified from the corpus data.
Thethird step uses these morphemes to segment words.3.1 Data StructureThe data is stored in two trees, the forward tree andthe backward tree.
Branches correspond to letters,and nodes are annotated with the total corpus fre-quency of the letter sequence from the root of thetree up to the node.
During the affix identificationprocess, the forward tree is used for discovering suf-fixes by calculating the probability of seeing a cer-tain letter given the previous letters of the word.
Thebackward tree is used to determine the probabilityof a letter given the following letters of a word inorder to find prefixes.
If the transitional probabil-ity is high, the word should not be split, whereaslow probability is a good indicator of a morphemeboundary.
In such a tree, stems tend to stay togetherin long unary branches, while the branching factor ishigh in places where morpheme boundaries occur.The underlying idea of exploiting ?Letter Succes-sor Variety?
was first proposed in (Harris, 1955), andhas since been used in a number of morphemic seg-mentation algorithms (Hafer and Weiss, 1974; Bern-hard, 2006; Bordag, 2006).3.2 Finding AffixesThe second step is concerned with finding good af-fixes.
The procedure is quite simple and can be di-vided into two subtasks.
(1) generating all possibleaffixes and (2) validating them.
The validation stepis necessary to exclude bad affix candidates (e.g.
let-ter sequences that occur together frequently such assch, spr or ch in German or sh, th, qu in English).1www.cis.hut.fi/morphochallenge2005/An affix is validated if all three criteria are satisfiedfor at least 5% of its occurrences:1.
The substring that remains after peeling off anaffix is also a word in the lexicon.2.
The transitional probability between thesecond-last and the last stem letter is ?
1.3.
The transitional probability of the affix letternext to the stem is <1 (tolerance 0.02).Finally, all affixes that are concatenations of two ormore other suffixes (e.g., -ungen can be split up in-ung and -en in German) are removed.
This step re-turns two lists of morphological segments.
The pre-fix list contains prefixes as well as stems that usuallyoccur at the beginning of words, while the suffix listcontains suffixes and stems that occur at the end ofwords.
In the remainder of the paper, we will referto the content of these lists as ?prefixes?
and ?suf-fixes?, although they also include stems.
There areseveral assumptions encoded in this procedure thatare specific to English, and cause recall to be low forother languages: 1) all stems are valid words in thelexicon; 2) affixes occur at the beginning or end ofwords only; and 3) affixation does not change stems.In section 4, we propose ways of relaxing these as-sumptions to make this step less language-specific.3.3 Segmenting WordsThe final step is the complete segmentation of wordsgiven the list of affixes acquired in the previous step.The original RePortS algorithm uses a very simplemethod that peels off the most probable suffix thathas a transitional probability smaller than 1, until nomore affixes match or until less than half of the wordremains.
This last condition is problematic since itdoes not scale up well to languages with complexmorphology.
The same peeling-off process is exe-cuted for prefixes.Although this method is better than using aheuristic such as ?always peel off the longest pos-sible affix?, because it takes into account probablesites of fractures in words, it is not sensitive tothe affix context or the morphotactics of the lan-guage.
Typical mistakes that arise from this con-dition are that inflectional suffixes, which can onlyoccur word-finally, might be split off in the middleof a word after previously having peeled off a num-ber of other suffixes.9224 Modifications and Extensions4.1 Morpheme AcquisitionWhen we ran the original algorithm on a Germandata set, no suffixes were validated but reasonableprefix lists were found.
The algorithm works finefor English suffixes ?
why does it fail on German?The algorithm?s failure to detect German suffixes iscaused by the invalid assumption that a stem mustbe a word in the corpus.
German verb stems donot occur on their own (except for certain impera-tive forms).
After stripping off the suffix of the verbabholst ?fetch?, the remaining string abhol cannot befound in the lexicon.
However, words like abholen,abholt, abhole or Abholung are part of the corpus.The same problem also occurs for German nouns.Therefore, this first condition of the affix acqui-sition step needs to be replaced.
We therefore intro-duced an additional step for building an intermediatestem candidate list into the affix acquisition process.The first condition is replaced by a condition thatchecks whether a stem is in the stem candidate list.This new stem candidate acquisition procedure com-prises three steps:Step 1: Creation of stem candidate listAll substrings that satisfy conditions 2 and 3 butnot condition 1, are stored together with the set ofaffixes they occur with.
This process is similar tothe idea of registering signatures (Goldsmith, 2001;Neuvel and Fulop, 2002).
For example, let us as-sume our corpus contains the words Auffu?hrender,Auffu?hrung, auffu?hrt and Auffu?hrlaune but not thestem itself, since auffu?hr ?act?
is not a valid Ger-man word.
Conditions 2 and 3 are met, becausethe transitional probability between auffu?hr and thenext letter is low (there are a lot of different pos-sible continuations) and the transitional probabilityP (r|auffu?h) ?
1.
The stem candidate auffu?hr is thenstored together with the suffix candidates {ender,ung, en, t, laune}.Step 2: Ranking candidate stemsThere are two types of affix candidates: type-1 affixcandidates are words that are contained in the database as full words (those are due to compounding);type-2 affix candidates are inflectional and deriva-tional suffixes.
When ranking the stem candidates,we take into account the number of type-1 affix can-didates and the average frequency of tpye-2 affixFigure 1: Determining the threshold for validatingthe best candidates from the stem candidate list.candidates.The first condition has very good precision, sim-ilar to the original method.
The morphemes foundwith this method are predominantly stem forms thatoccur in compounding or derivation (Komposition-ssta?mme and Derivationssta?mme).
The second con-dition enables us to differentiate between stems thatoccur with common suffixes (and therefore havehigh average frequencies), and pseudostems suchas runtersch whose affix list contains many non-morphemes (e.g.
lucken, iebt, aute).
These non-morphemes are very rare since they are not gener-ated by a regular process.Step 3: PruningAll stem candidates that occur less than three timesare removed from the list.
The remaining stem can-didates are ordered according to the average fre-quency of their non-word suffixes.
This criterionputs the high quality stem candidates (that occurwith very common suffixes) to the top of the list.In order to obtain a high-precision stem list, it isnecessary to cut the list of candidates at some point.The threshold for this is determined by the data: wechoose the point at which the function of list-rankvs.
score changes steepness (see Figure 1).
Thisvisual change of steepness corresponds to the pointwhere potential stems found get more noisy becausethe strings with which they occur are not commonaffixes.
We found the performance of the result-ing morphological system to be quite stable (?1%f-score) for any cutting point on the slope between20% and 50% of the list (for the German data setranks 4000 and 12000), but importantly before thefunction tails off.
The threshold was also robustacross the other languages and data sets.9234.2 Morphological SegmentationAs discussed in section 3.3, the original implemen-tation of the algorithm iteratively chops off the mostprobable affixes at both edges of the word withouttaking into account the context of the affix.
In mor-phologically complex languages, this context-blindapproach often leads to suboptimal results, and alsoallows segmentations that are morphotactically im-possible, such as inflectional suffixes in the middleof words.
Another risk is that the letter sequence thatis left after removing potential prefixes and suffixesfrom both ends is not a proper stem itself but just asingle letter or vowel-less letter-sequence.These problems can be solved by using a bi-gramlanguage model to capture the morphotactic proper-ties of a particular language.
Instead of simply peel-ing off the most probable affixes from both ends ofthe word, all possible segmentations of the word aregenerated and ranked using the language model.
Theprobabilities for the language model are learnt froma set of words that were segmented with the origi-nal simple approach.
This bootstrapping allows usto ensure that the approach remains fully unsuper-vised.
At the beginning and end of each word, anedge marker ?#?
is attached to the word.
The modelcan then also acquire probabilities about which af-fixes occur most often at the edges of words.Table 2 shows that filtering the segmentation re-sults with the n-gram language model caused a sig-nificant improvement on the overall F-score for mostlanguages, and led to significant changes in pre-cision and recall.
Whereas the original segmen-tation yielded balanced precision and recall (both68%), the new filtering boosts precision to over73%, with 64% recall.
Which method is preferable(i.e.
whether precision or recall is more important)is task-dependent.In future work, we plan to draw on (Creutz andLagus, 2006), who use a HMM with morphemic cat-egories to impose morphotactic constraints.
In suchan approach, each element from the affix list is as-signed with a certain probability to the underlyingcategories of ?stem?, ?prefix?
or ?suffix?, depend-ing on the left and right perplexity of morphemes, aswell as morpheme length and frequency.
The tran-sitional probabilities from one category to the nextmodel the morphotactic rules of a language, whichcan thus be learnt automatically.4.3 Learning Stem VariationStem variation through ablauting and umlauting(an English example is run?ran) is an interest-ing problem that cannot be captured by the algo-rithm outlined above, as variations take place withinthe morphemes.
Stem variations can be context-dependent and do not constitute a morpheme inthemselves.
German umlauting and ablauting leadsto data sparseness problems in morphological seg-mentation and affix acquisition.
One problem is thataffixes which usually cause ablauting or umlautingare very difficult to find.
Typically, ablauted or um-lauted stems are only seen with a very small numberof different affixes, which means that the affix setsof such stems are divided into several unrelated sub-sets, causing the stem to be pruned from the stemcandidate list.
Secondly, ablauting and umlautinglead to low transitional probabilities at the positionsin stems where these phenomena occur.
Considerfor example the affix set for the stem candidate bock-spr, which contains the pseudoaffixes ung, u?nge andingen.
The morphemes sprung, spru?ng and sprin-gen are derived from the root spring ?to jump?.
Inthe segmentation step this low transitional probabil-ity thus leads to oversegmentation.We therefore investigated whether we can learnthese regular stem variations automatically.
A sim-ple way to acquire the stem variations is to look atthe suffix clusters which are calculated during thestem-acquisition step.
When looking at the sets ofsubstrings that are clustered together by having thesame prefix, we found that they are often inflectionsof one another, because lexicalized compounds areused frequently in different inflectional variants.
Forexample, we find Trainingssprung as well as Train-ingsspru?nge in the corpus.
The affix list of the stemcandidate trainings thus contains the words sprungand spru?nge.
Edit distance can then be used tofind differences between all words in a certain affixlist.
Pairs with small edit distances are stored andranked by frequency.
Regular transformation rules(e.g.
ablauting and umlauting, u ?
u?..e) occur atthe top of the list and are automatically accepted asrules (see Table 1).
This method allows us to notonly find the relation between two words in the lex-icon (Sprung and Spru?nge) but also to automaticallylearn rules that can be applied to unknown words tocheck whether their variant is a word in the lexicon.924freq.
diff.
examples1682 a a?..e sack-sa?cke, brach-bra?che, stark-sta?rke344 a a?
sahen-sa?hen, garten-ga?rten321 u u?..e flug-flu?ge, bund-bu?nde289 a?
a..s vertra?ge-vertrages, pa?sse-passes189 o o?..e chor-cho?re, strom-stro?me, ?ro?hre-rohr175 t en setzt-setzen, bringt-bringen168 a u laden-luden, *damm-dumm160 ?
ss la?
?t-la?sst, mi?brauch-missbrauch[.
.
.
]136 a en firma-firmen, thema-themen[.
.
.
]2 ?
g *flie?en-fliegen, *la?t-lagt2 um o *studiums-studiosTable 1: Excerpts from the stem variation detectionalgorithm results.
Morphologically unrelated wordpairs are marked with an asterisk.We integrated information about stem variationfrom the regular stem transformation rules (thosewith the highest frequencies) into the segmentationstep by creating equivalence sets of letters.
For ex-ample, the rule u ?
u?..e generates an equivalenceset {u?, u}.
These two letters then count as the sameletter when calculating transitional probabilities.
Weevaluated the benefit of integrating stem variationinformation for German on the German CELEXdata set, and achieved an improvement of 2% inrecall, without any loss in precision (F-measure:69.4%, Precision: 68.1%, Recall: 70.8%; values forRePortS-stems).
For better comparability to othersystems and languages, results reported in the nextsection refer to the system version that does not in-corporate stem variation.5 EvaluationFor evaluating the different versions of the algorithmon English, Turkish and Finnish, we used the train-ing and test sets from MorphoChallenge to enablecomparison with other systems.
Performance of thealgorithm on German was evaluated on 244k manu-ally annotated words from CELEX because Germanwas not included in the MorphoChallenge data.Table 2 shows that the introduction of the stemcandidate acquisition step led to much higher recallon German, Finnish and Turkish, but caused somelosses in precision.
For English, adding both com-ponents did not have a large effect on either preci-sion or recall.
This means that this component iswell behaved, i.e.
it improves performance on lan-guages where the intermediate stem-acquisition stepLang.
alg.version F-Meas.
Prec.
RecallEng1 original 76.8% 76.2% 77.4%stems 67.6% 62.9% 73.1%n-gram seg.
75.1% 74.4% 75.9%Ger2 original 59.2% 71.1% 50.7%stems 68.4% 68.1% 68.6%n-gram seg.
68.9% 73.7% 64.6%Tur1 original 54.2% 72.9% 43.1%stems 61.8% 65.9% 58.2%n-gram seg.
64.2% 65.2% 63.3%Fin1 original 47.1% 84.5% 32.6%stems 56.6% 74.1% 45.8%n-gram seg.
58.9% 76.1% 48.1%max-split* 61.3% 66.3% 56.9%Table 2: Performance of the algorithm with the mod-ifications on different languages.1MorphoChallenge Data, 2German CELEXis needed, but does not impair results on other lan-guages.
Recall for Finnish is still very low.
It can beimproved (at the expense of precision) by selectingthe analysis with the largest number of segments inthe segmentation step.
The results for this heuris-tic was only evaluated on a smaller test set (ca.
700wds), hence marked with an asterisk in Table 2.The algorithm is very efficient: When trained onthe 240m tokens of the German TAZ corpus, it takesup less than 1 GB of memory.
The training phasetakes approx.
5 min on a 2.4GHz machine, and thesegmentation of the 250k test words takes 3 min forthe version that does the simple segmentation andabout 8 min for the version that generates all possi-ble segmentations and uses the language model.5.1 Comparison to other systemsThis modified version of the algorithm performs sec-ond best for English (after original RePortS) andranks third for Turkish (after Bernhards algorithmwith 65.3% F-measure and Morfessor-Categories-MAP with 70.7%).
On German, our method sig-nificantly outperformed the other unsupervised al-gorithms, see Table 3.
While most of the systemscompared here were developed for languages otherthan German, (Bordag, 2006) describes a system ini-tially built for German.
When trained on the ?Pro-jekt Deutscher Wortschatz?
corpus which comprises24 million sentences, it achieves an F-score of 61%(precision 60%, recall 62%2) when evaluated on thefull CELEX corpus.2Data from personal communication.925morphology F-Meas.
Prec.
RecallSMOR-disamb2 83.6% 87.1% 80.4%ETI 79.5% 75.4% 84.1%SMOR-disamb1 71.8% 95.4% 57.6%RePortS-lm 68.8% 73.7% 64.6%RePortS-stems 68.4% 68.1% 68.6%best Bernhard 63.5% 64.9% 62.1%Bordag 61.4% 60.6% 62.3%orig.
RePortS 59.2% 71.1% 50.7%best Morfessor 1.0 52.6% 70.9% 41.8%Table 3: Evaluating rule-based and data-based sys-tems for morphological segmentation with respect toCELEX manual morphological annotation.Rule-based systems are currently the most com-mon approach to morphological decomposition andperform better at segmenting words than state-of-the-art unsupervised algorithms (see Table 3 for per-formance of state-of-the-art rule-based systems eval-uated on the same data).
Both the ETI3 and theSMOR (Schmid et al, 2004) systems rely on a largelexicon and a set of rules.
The SMOR system re-turns a set of analyses that can be disambiguated indifferent ways.
For details refer to pp.
29?33 in(Demberg, 2006).5.2 Evaluation on Grapheme-to-PhonemeConversionMorphological segmentation is not of value in itself?
the question is whether it can help improve resultson an application.
Performance improvements dueto morphological information have been reported forexample in MT, information retrieval, and speechrecognition.
For the latter task, morphological seg-mentations from the unsupervised systems presentedhere have been shown to improve accuracy (Kurimoet al, 2006).Another motivation for evaluating the system ona task rather than on manually annotated data isthat linguistically motivated morphological segmen-tation is not necessarily the best possible segmenta-tion for a certain task.
Evaluation against a manu-ally annotated corpus prefers segmentations that areclosest to linguistically motivated analyses.
Further-more, it might be important for a certain task tofind a particular type of morpheme boundaries (e.g.boundaries between stems), but for another task it3Eloquent Technology, Inc. (ETI) TTS system.www.mindspring.com/?ssshp/ssshp_cd/ss_eloq.htmmorphology F-Meas.
(CELEX) PER (dt)CELEX 100% 2.64%ETI 79.5% 2.78%SMOR-disamb2 83.0% 3.00%SMOR-disamb1 71.8% 3.28%RePortS-lm 68.8% 3.45%no morphology 3.63%orig.
RePortS 59.2% 3.83%Bernhard 63.5% 3.88%RePortS-stem 68.4% 3.98%Morfessor 1.0 52.6% 4.10%Bordag 64.1% 4.38%Table 4: F-measure for evaluation on manually an-notated CELEX and phoneme error rate (PER) fromg2p conversion using a decision tree (dt).might be very important to find boundaries betweenstems and suffixes.
The standard evaluation proce-dure does not differentiate between the types of mis-takes made.
Finally, only evaluation on a task canprovide information as to whether high precision orhigh recall is more important, therefore, the decisionas to which version of the algorithm should be cho-sen can only be taken given a specific task.For these reasons we decided to evaluate the seg-mentation from the new versions of the RePortS al-gorithm on a German grapheme-to-phoneme (g2p)conversion task.
The evaluation on this task is moti-vated by the fact that (Demberg, 2007) showed thatgood-quality morphological preprocessing can im-prove g2p conversion results.
We here compare theeffect of using our system?s segmentations to a rangeof different morphological segmentations from othersystems.
We ran each of the rule-based systems(ETI, SMOR-disamb1, SMOR-disamb2) and theunsupervised algorithms (original RePortS, Bern-hard, Morfessor 1.0, Bordag) on the CELEX dataset and retrained our decision tree (an implementa-tion based on (Lucassen and Mercer, 1984)) on thedifferent morphological segmentations.Table 4 shows the F-score of the different systemswhen evaluated on the manually annotated CELEXdata (full data set) and the phoneme error rate (PER)for the g2p conversion algorithm when annotatedwith morphological boundaries (smaller test set,since the decision tree is a supervised method andneeds training data).
As we can see from the results,the distribution of precision and recall (see Table 3)has an important impact on the conversion quality:the RePortS version with higher precision signifi-926cantly outperforms the other version on the task, al-though their F-measures are almost identical.
Re-markably, the RePortS version that uses the filter-ing step is the only unsupervised system that beatsthe no-morphology baseline (p < 0.0001).
Whileall other unsupervised systems tested here make thesystem perform worse than it would without mor-phological information, this new version improvesaccuracy on g2p conversion.6 ConclusionsA significant improvement in F-score was achievedby three simple modifications to the RePortS al-gorithm: generating an intermediary high-precisionstem candidate list, using a language model to dis-ambiguate between alternative segmentations, andlearning patterns for regular stem variation, whichcan then also be exploited for segmentation.
Thesemodifications improved results on four different lan-guages considered: English, German, Turkish andFinnish, and achieved the best results reported so farfor an unsupervised system for morphological seg-mentation on German.
We showed that the new ver-sion of the algorithm is the only unsupervised sys-tem among the systems evaluated here that achievessufficient quality to improve transcription perfor-mance on a grapheme-to-phoneme conversion task.AcknowledgmentsI would like to thank Emily Pitler and Samarth Ke-shava for making available the code of the RePortSalgorithm, and Stefan Bordag and Delphine Bern-hard for running their algorithms on the Germandata.
Many thanks also to Matti Varjokallio for eval-uating the data on the MorphoChallenge test setsfor Finnish, Turkish and English.
Furthermore, Iam very grateful to Christoph Zwirello and GregorMo?hler for training the decision tree on the new mor-phological segmentation.
I also want to thank FrankKeller and the ACL reviewers for valuable and in-sightful comments.ReferencesDelphine Bernhard.
2006.
Unsupervised morphological seg-mentation based on segment predictability and word seg-ments alignment.
In Proceedings of 2nd Pascal ChallengesWorkshop, pages 19?24, Venice, Italy.Stefan Bordag.
2006.
Two-step approach to unsupervised mor-pheme segmentation.
In Proceedings of 2nd Pascal Chal-lenges Workshop, pages 25?29, Venice, Italy.Mathias Creutz and Krista Lagus.
2006.
Unsupervised modelsfor morpheme segmentation and morphology learning.
InACM Transaction on Speech and Language Processing.H.
De?jean.
1998.
Morphemes as necessary concepts for struc-tures: Discovery from untagged corpora.
In Workshop onparadigms and Grounding in Natural Language Learning,pages 295?299, Adelaide, Australia.Vera Demberg.
2006.
Letter-to-phoneme conversion for a Ger-man TTS-System.
Master?s thesis.
IMS, Univ.
of Stuttgart.Vera Demberg.
2007.
Phonological constraints and morpho-logical preprocessing for grapheme-to-phoneme conversion.In Proc.
of ACL-2007.Eric Gaussier.
1999.
Unsupervised learning of derivationalmorphology from inflectional lexicons.
In ACL ?99 Work-shop Proceedings, University of Maryland.CELEX German Linguistic User Guide, 1995.
Center for Lex-ical Information.
Max-Planck-Institut for Psycholinguistics,Nijmegen.John Goldsmith.
2001.
Unsupervised learning of the mor-phology of a natural language.
computational Linguistics,27(2):153?198, June.S.
Goldwater and D. McClosky.
2005.
Improving statistical mtthrough morphological analysis.
In Proc.
of EMNLP.Margaret A. Hafer and Stephen F. Weiss.
1974.
Word segmen-tation by letter successor varieties.
Information Storage andRetrieval 10, pages 371?385.Zellig Harris.
1955.
From phoneme to morpheme.
Language31, pages 190?222.Christian Jacquemin.
1997.
Guessing morphology from termsand corpora.
In Research and Development in InformationRetrieval, pages 156?165.S.
Keshava and E. Pitler.
2006.
A simpler, intuitive approachto morpheme induction.
In Proceedings of 2nd Pascal Chal-lenges Workshop, pages 31?35, Venice, Italy.M.
Kurimo, M. Creutz, M. Varjokallio, E. Arisoy, and M. Sar-aclar.
2006.
Unsupervsied segmentation of words into mor-phemes ?
Challenge 2005: An introduction and evaluationreport.
In Proc.
of 2nd Pascal Challenges Workshop, Italy.J.
Lucassen and R. Mercer.
1984.
An information theoreticapproach to the automatic determination of phonemic base-forms.
In ICASSP 9.C.
Monson, A. Lavie, J. Carbonell, and L. Levin.
2004.
Un-supervised induction of natural language morphology inflec-tion classes.
In Proceedings of the Seventh Meeting of ACL-SIGPHON, pages 52?61, Barcelona, Spain.C.
Monz and M. de Rijke.
2002.
Shallow morphological analy-sis in monolingual information retrieval for Dutch, German,and Italian.
In Proceedings CLEF 2001, LNCS 2406.Sylvain Neuvel and Sean Fulop.
2002.
Unsupervised learningof morphology without morphemes.
In Proc.
of the Wshp onMorphological and Phonological Learning, ACL Pub.Helmut Schmid, Arne Fitschen, and Ulrich Heid.
2004.SMOR: A German computational morphology coveringderivation, composition and inflection.
In Proc.
of LREC.Patrick Schone and Daniel Jurafsky.
2000.
Knowledge-freeinduction of morphology using latent semantic analysis.
InProc.
of CoNLL-2000 and LLL-2000, Lisbon, Portugal.Tageszeitung (TAZ) Corpus.
Contrapress Media GmbH.https://www.taz.de/pt/.etc/nf/dvd.David Yarowski and Richard Wicentowski.
2000.
Minimallysupervised morphological analysis by multimodal align-ment.
In Proceedings of ACL 2000, Hong Kong.927
