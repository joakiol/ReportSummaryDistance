Proceedings of the EACL 2009 Demonstrations Session, pages 65?68,Athens, Greece, 3 April 2009. c?2009 Association for Computational LinguisticsA Mobile Health and Fitness Companion Demonstrator?Olov Sta?hl1 Bjo?rn Gamba?ck1,2 Markku Turunen3 Jaakko Hakulinen31ICE / Userware 2Dpt.
Computer & Information Science 3Dpt.
Computer SciencesSwedish Inst.
of Computer Science Norwegian Univ.
of Science and Technology Univ.
of TampereKista, Sweden Trondheim, Norway Tampere, Finland{olovs,gamback}@sics.se gamback@idi.ntnu.no {mturunen,jh}@cs.uta.fiAbstractMultimodal conversational spoken dia-logues using physical and virtual agentsprovide a potential interface to motivateand support users in the domain of healthand fitness.
The paper presents a multi-modal conversational Companion systemfocused on health and fitness, which hasboth a stationary and a mobile component.1 IntroductionSpoken dialogue systems have traditionally fo-cused on task-oriented dialogues, such as mak-ing flight bookings or providing public transporttimetables.
In emerging areas, such as domain-oriented dialogues (Dybkjaer et al, 2004), the in-teraction with the system, typically modelled as aconversation with a virtual anthropomorphic char-acter, can be the main motivation for the interac-tion.
Recent research has coined the term ?Com-panions?
to describe embodied multimodal con-versational agents having a long lasting interactionhistory with their users (Wilks, 2007).Such a conversational Companion within theHealth and Fitness (H&F) domain helps its usersto a healthier lifestyle.
An H&F Companion hasquite different motivations for use than traditionaltask-based spoken dialogue systems.
Instead ofhelping with a single, well-defined task, it trulyaims to be a Companion to the user, providingsocial support in everyday activities.
The systemshould thus be a peer rather than act as an expertsystem in health-related issues.
It is important tostress that it is the Companion concept which iscentral, rather than the fitness area as such.
Thusit is not of vital importance that the system shouldbe a first-rate fitness coach, but it is essential that it?The work was funded by the European Commis-sion?s IST priority through the project COMPANIONS(www.companions-project.org).Figure 1: H&F Companion Architectureshould be able to take a persistent part in the user?slife, that is, that it should be able to follow the userin all the user?s activities.
This means that theCompanion must have mobile capabilities.
Notnecessarily self-mobile (as a robot), but allowingthe user to bring the system with her, like a hand-bag or a pair of shoes ?
or as a mobile phone.The paper describes such a Health and FitnessCompanion.
It has a stationary (?home?)
compo-nent accounting for the main part of the user in-teraction and a mobile component which followsthe users in actual exercise activities.
Section 2outlines the overall system and its two basic com-ponents, and Section 3 details the implementation.Section 4 discusses some related work, while Sec-tion 5 describes the demonstrator set-up and plansfor future work.2 The Health and Fitness CompanionThe overall system architecture of the Health andFitness Companion is shown in Figure 1.
Thesystem components communicate with each otherover a regular mobile phone network.
The homesystem provides an exercise plan to the mobile partand in return gets the results of the performed ex-ercises from the mobile component.65Figure 2: Home Companion interface2.1 The Home H&F CompanionThe home part of the H&F Companion gathers in-formation from the user and makes suggestionstargeted for a healthy lifestyle on a daily basis.The Companion communicates with the user intwo main dialogue phases: a planning phase wherethe system talks about the coming day with theuser and a reporting phase where the user?s actualactivities are assessed with reference to what wasagreed on earlier.
The Companion can discuss thefollowing topics: travelling to work, getting lunch,activities to be performed before dinner, gettingdinner, and activities to be performed after din-ner.
It knows activities such as playing football,squash, or badminton; going to the gym or shop-ping; and watching television or reading a book.The Home Companion interface, illustratedin Figure 2, uses a Nabaztag/tag WLAN rabbit(www.nabaztag.com) as the physical agent in-terface.
The Nabaztag rabbit provides audio out-put and push-to-talk speech input, moves its ears,and operates four coloured lights to signal its sta-tus.
In the current implementation, the user needsto push the button located on top of the Nabaz-tag/tag device in order to speak to it.
As an alter-native, the system supports external micro-phonesand loudspeakers.
The user is also able to pick atopic by showing RFID tags (embedded in picturecards or other objects).The following dialogue example demonstratesthe interaction with the Companion.C1: Do you have anything arranged for today?U1: I?m playing SquashC2: Is that before dinner?U2: YesC3: Okay.
Are you going to get the train to work?U3: YesC4: Right.
Could you take in a packed lunch to work?U4: Yes, that would be fineFigure 3: The Mobile Companion GUI(Later on in the evening)C6: Welcome back!
How did your game of Squash go?U5: The game was cancelled, so I went to the gymC7: Okay.
Did you take the train to work?U6: Yes, I didC8: Right.
Did you take in a packed lunch?U7: No.
I was in a rush so I went to the cafeteria2.2 The Mobile H&F CompanionThe mobile part of the H&F Companion runs on amobile handset (e.g., a PDA), and is used duringphysical exercise (e.g., while running or walking)to track the distance, pace, duration, and caloriesburned.
The data gathered during an exercise isstored in the device?s record store, and can be usedto compare the results to previous runs.The user interface of the Mobile Companionconsists of a single screen showing an image of aNabaztag rabbit along with some text areas wherevarious exercise and device status information isdisplayed (Figure 3).
The rabbit image is intendedto give users a sense of communicating with thesame Companion, no matter if they are using thehome or mobile system.
To further the feeling ofpersistence, the home and mobile parts of the H&FCompanion also use the same TTS voice.When the mobile Companion is started, it asksthe user whether it should connect to the home sys-tem and download the current plan.
Such a planconsists of various tasks (e.g., shopping or exer-cise tasks) that the user should try to achieve dur-ing the day, and is generated by the home systemduring a session with the user.
If the user choosesto download the plan the Companion summarizesthe content of the plan for the user, excluding alltasks that do not involve some kind of exercise ac-tivity.
The Companion then suggests a suitabletask based on time of day and the user?s currentlocation.
If the user chooses not to download theplan, or rejects the suggested exercise(s), the Com-panion instead asks the user to suggest an exercise.66Once an exercise has been agreed upon, theCompanion asks the user to start the exercise andwill then track the progress (distances travelled,time, pace and calories burned) using a built-inGPS receiver.
While exercising, the user can askthe Companion to play music or to give reports onhow the user is doing.
After the exercise, the Com-panion will summarize the result and up-load it tothe Home system so it can be referred to later on.3 H&F Companion ImplementationThis section details the actual implementation ofthe Health and Fitness Companion, in terms of itstwo components (the home and mobile parts).3.1 Home Companion ImplementationThe Home Companion is implemented on topof Jaspis, a generic agent-based architecture de-signed for adaptive spoken dialogue systems (Tu-runen et al, 2005).
The base architectureis extended to support interaction with virtualand physical Companions, in particular with theNabaztag/tag device.For speech inputs and outputs, the Home Com-panion uses LoquendoTMASR and TTS compo-nents.
ASR grammars are in ?Speech Recogni-tion Grammar Specification?
(W3C) format andinclude semantic tags in ?Semantic Interpreta-tion for Speech Recognition (SISR) Version 1.0?
(W3C) format.
Domain specific grammars werederived from a WoZ corpus.
The grammars aredynamically selected according to the current di-alogue state.
Grammars can be precompiled forefficiency or compiled at run-time when dynamicgrammar generation takes place in certain situa-tions.
The current system vocabulary consists ofabout 1400 words and a total of 900 CFG grammarrules in 60 grammars.
Statistical language modelsfor the system are presently being implemented.Language understanding relies heavily on SISRinformation: given the current dialogue state, theinput is parsed into a logical notation compati-ble with the planning implemented in a CognitiveModel.
Additionally, a reduced set of DAMSL(Core and Allen, 1997) tags is used to mark func-tional dialogue acts using rule-based reasoning.Language generation is implemented as a com-bination of canned utterances and tree adjoininggrammar-based structures.
The starting point forgeneration is predicate-form descriptions providedby the dialogue manager.
Further details andcontextual information are retrieved from the di-alogue history and the user model.
Finally, SSML(Speech Synthesis Markup Language) 1.0 tags areused for controlling the Loquendo synthesizer.Dialogue management is based on close-cooperation of the Dialogue Manager and the Cog-nitive Manager.
The Cognitive Manager modelsthe domain, i.e., knows what to recommend to theuser, what to ask from the user, and what kindof feedback to provide on domain level issues.In contrast, the Dialogue Manager focuses on in-teraction level phenomena, such as confirmations,turn taking, and initiative management.The physical agent interface is implementedin jNabServer software to handle communicationwith Nabaztag/tags, that is, Wi-Fi enabled roboticrabbits.
A Nabaztag/tag device can handle vari-ous forms of interaction, from voice to touch (but-ton press), and from RFID ?sniffing?
to ear move-ments.
It can respond by moving its ears, or bydisplaying or changing the colour of its four LEDlights.
The rabbit can also play sounds such asmusic, synthesized speech, and other audio.3.2 Mobile Companion ImplementationThe Mobile Companion runs on Windows Mobile-based devices, such as the Fujitsu Siemens PocketLOOX T830.
The system is made up of two pro-grams, both running on the mobile device: a Javamidlet controls the main application logic (exer-cise tracking, dialogue management, etc.)
as wellas the graphical user interface; and a C++-basedspeech server that performs TTS and ASR func-tions on request by the Java midlet, such as load-ing grammar files or voices.The midlet is made up of Java manager classesthat provide basic services (event dispatching,GPS input, audio play-back, TTS and ASR, etc.
).However, the main application logic and the GUIare implemented using scripts in the Hecl script-ing language (www.hecl.org).
The script filesare read from the device?s file system and evalu-ated in a script interpreter created by the midletwhen started.
The scripts have access to a num-ber of commands, allowing them to initiate TTSand ASR operations, etc.
Furthermore, eventsproduced by the Java code are dispatched to thescripts, such as the user?s current GPS position,GUI interactions (e.g., stylus interaction and but-ton presses), and voice input.
Scripts are also usedto control the dialogue with the user.67The speech server is based on the LoquendoEmbedded ASR (speaker-independent) and TTSsoftware.1 The Mobile Companion uses SRGS 1.0grammars that are pre-compiled before being in-stalled on the mobile device.
The current systemvocabulary consists of about 100 words in 10 dy-namically selected grammars.4 Related WorkAs pointed out in the introduction, it is not the aimof the Health and Fitness Companion system to bea full-fledged fitness coach.
There are several ex-amples of commercial systems that aim to do that,e.g., miCoach (www.micoach.com) from Adi-das and NIKE+ (www.nike.com/nikeplus).MOPET (Buttussi and Chittaro, 2008) is aPDA-based personal trainer system supportingoutdoor fitness activities.
MOPET is similar toa Companion in that it tries to build a relation-ship with the user, but there is no real dialoguebetween the user and the system and it does notsupport speech input or output.
Neither doesMPTrain/TripleBeat (Oliver and Flores-Mangas,2006; de Oliveira and Oliver, 2008), a system thatruns on a mobile phone and aims to help usersto more easily achieve their exercise goals.
Thisis done by selecting music indicating the desiredpace and different ways to enhance user motiva-tion, but without an agent user interface model.InCA (Kadous and Sammut, 2004) is a spokenlanguage-based distributed personal assistant con-versational character with a 3D avatar and facialanimation.
Similar to the Mobile Companion, thearchitecture is made up of a GUI client running ona PDA and a speech server, but the InCA serverruns as a back-end system, while the Companionutilizes a stand-alone speech server.5 Demonstration and Future WorkThe demonstration will consist of two sequentialinteractions with the H&F Companion.
First, theuser and the home system will agree on a plan,consisting of various tasks that the user should tryto achieve during the day.
Then the mobile systemwill download the plan, and the user will have adialogue with the Companion, concerning the se-lection of a suitable exercise activity, which theuser will pretend to carry out.1As described in ?Loquendo embedded technologies:Text to speech and automatic speech recognition.
?www.loquendo.com/en/brochure/Embedded.pdfPlans for future work include extending the mo-bile platform with various sensors, for example, apulse sensor that gives the Companion informa-tion about the user?s pulse while exercising, whichcan be used to provide feedback such as tellingthe user to speed up or slow down.
We are also in-terested in using sensors to allow users to providegesture-like input, in addition to the voice and but-ton/screen click input available today.Another modification we are considering is tounify the two dialogue management solutions cur-rently used by the home and the mobile compo-nents into one.
This would cause the Companionto ?behave?
more consistently in its two shapes,and make future extensions of the dialogue and theCompanion behaviour easier to manage.ReferencesFabio Buttussi and Luca Chittaro.
2008.
MOPET:A context-aware and user-adaptive wearable sys-tem for fitness training.
Artificial Intelligence inMedicine, 42(2):153?163.Mark G. Core and James F. Allen.
1997.
Coding di-alogs with the DAMSL annotation scheme.
In AAAIFall Symposium on Communicative Action in Hu-mans and Machines, pages 28?35, Cambridge, Mas-sachusetts.Laila Dybkjaer, Niels Ole Bernsen, and WolfgangMinker.
2004.
Evaluation and usability of multi-modal spoken language dialogue systems.
SpeechCommunication, 43(1-2):33?54.Mohammed Waleed Kadous and Claude Sammut.2004.
InCa: A mobile conversational agent.
In Pro-ceedings of the 8th Pacific Rim International Con-ference on Artificial Intelligence, pages 644?653,Auckland, New Zealand.Rodrigo de Oliveira and Nuria Oliver.
2008.
Triple-Beat: Enhancing exercise performance with persua-sion.
In Proceedings of 10th International Con-ference, on Mobile Human-Computer Interaction,pages 255?264, Amsterdam, the Netherlands.
ACM.Nuria Oliver and Fernando Flores-Mangas.
2006.MPTrain: A mobile, music and physiology-basedpersonal trainer.
In Proceedings of 8th InternationalConference, on Mobile Human-Computer Interac-tion, pages 21?28, Espoo, Finland.
ACM.Markku Turunen, Jaakko Hakulinen, Kari-JoukoRa?iha?, Esa-Pekka Salonen, Anssi Kainulainen, andPerttu Prusi.
2005.
An architecture and applica-tions for speech-based accessibility systems.
IBMSystems Journal, 44(3):485?504.Yorick Wilks.
2007.
Is there progress on talking sensi-bly to machines?
Science, 318(9):927?928.68
