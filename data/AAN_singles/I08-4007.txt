Stochastic Dependency Parsing Based on A* Admissible SearchBor-shen LinNational Taiwan University of Science and Technology / No.
43, Keelung Road,Section 4, Taipei, Taiwanbslin@cs.ntust.edu.twAbstractDependency parsing has gained attention innatural language understanding because therepresentation of dependency tree is simple,compact and direct such that robust partialunderstanding and task portability can beachieved more easily.
However, many de-pendency parsers make hard decisions withlocal information while selecting amongthe next parse states.
As a consequence,though the obtained dependency trees aregood in some sense, the N-best output isnot guaranteed to be globally optimal ingeneral.In this paper, a stochastic dependency pars-ing scheme based on A* admissible searchis formally presented.
By well representingthe parse state and appropriately designingthe cost and heuristic functions, depend-ency parsing can be modeled as an A*search problem, and solved with a genericalgorithm of state space search.
Whenevaluated on the Chinese Tree Bank, thisparser can obtain 85.99% dependency ac-curacy at 68.39% sentence accuracy, and14.62% node ratio for dynamic heuristic.This parser can output N-best dependencytrees, and integrate the semantic processinginto the search process easily.1 IntroductionConstituency grammar has long been the main wayfor describing the sentence structure of natural lan-guage for decades.
The phrase structure of sen-tences can be analyzed by such parsing algorithmsas Earley or CYK algorithms (Allen, 1995; Juraf-sky and Martin 2001).
To parse sentences withconstituency grammar, a set of grammar rules writ-ten in linguistics is indispensable, while a corpusof tree bank annotated manually is also necessaryif stochastic parsing scheme is adopted.
In addition,the many non-terminal or phrasal nodes make itsophisticated to further interpret or disambiguatethe parse trees since deep linguistic knowledge isoften required.
All these factors make languageunderstanding very difficult, labor-intensive andnot easy to be ported to various tasks.Dependency grammar, on the other hand, describesthe syntactic and semantic relationships amonglexical terms directly with binary head-modifierlinks.
Such representation is very simple, compact,Figure 1.
An example of dependency parsingwith unlabeled dependency tree.?
with??
report?
type?
well??
yesterday?
send??
boss?with??yesterday?type?well?of??report?
?boss?send?to(Send to the boss the report typed well yesterday.)?
to?
of45Sixth SIGHAN Workshop on Chinese Language Processingdirect, and therefore helpful for simplifying theprocess of language understanding and increasingtask portability.
Figure 1 shows an example of de-pendency parsing for the Chinese sentence ??????????????
(meaning ?send to theboss the report typed well yesterday?).
Accordingto the binary head-modifier links, it is pretty easyto transform the dependency tree into the predicate?send(to:boss,object:report(typed(yesterday,well)))?for further interpretation or interaction, since thesemantic gap between them is slight and the map-pings are thus quite direct.
Furthermore, robustpartial understanding can be achieved easily whenfull parse is not obtainable, and measured preciselyby simply counting the correct attachments on thedependency tree (Ohno et al, 2004).
All these willnot be so simple provided that conventional con-stituency grammar is used.In the dependency parsing paradigm, several de-terministic, stochastic or machine-learning-basedparsing algorithms have been proposed (Eisner,1996; Covington 2001; Kudo and Matsumoto,2002; Yamada and Matsumoto, 2003; Nivre 2003;Nivre and Scholz, 2004; Chen et al, 2005).
Manyof them make hard decisions with local informa-tion while selecting among next parse states.
As aconsequence, though the obtained dependencytrees are good in some sense, the N-best output isnot guaranteed to be globally optimal in general(Klein and Manning, 2003).On the other hand, A* search that guarantees opti-mality has been applied to many areas including AI.Klein and Manning (2003) proposed to use A*search in PCFG parsing.
Dienes et al depictedprimarily the idea of applying A* search to de-pendency parsing, but there is not yet formalevaluation results and discussions based on theliteratures we have (Dienes et al, 2003).In this paper, a stochastic dependency parsingscheme based on A* admissible search is formallypresented.
By well representing the parse state andappropriately designing the cost and heuristic func-tions, dependency parsing can be modeled as anA* search problem, and solved with a generic al-gorithm of state space search.
This parser has beentested on the Chinese Tree Bank (Chen et al,1999), and 85.99% dependency accuracy at68.39% sentence accuracy can be obtained.
Amongthree types of proposed admissible heuristics, dy-namic heuristic can achieve the highest efficiencywith node ratio 14.62%.
This parser can output N-best dependency trees, and integrate the semanticprocessing into the search process easily.2 Fundamentals of A* SearchSearch is an important issue in AI area, since thesolutions of many problems could be automated ifthey could be modeled as search problems on statespace (Russell and Norvig, 2003).
A well knownexample is the traveling salesperson problem, asshown in the example of Figure 2.
In this sectionbasic constituents of A* search will be depicted.2.1 State RepresentationState representation is the first step for modelingthe problem domain.
It involves not only designingthe data structure of search state but indicating theway a state can be uniquely identified.
This is defi-nitely not a trivial issue, and depends on the howthe problem is defined.
In Figure 2, for example,search state cannot be represented simply with thecurrent city, because traveling salesperson problemrequires every city has to be visited exactly once.The two nodes of city E on level 2 in Figure 2,with paths A-B-E and A-C-E respectively, aretherefore regarded as different search states, andgenerate their successor states individually.
Thenode E with the path A-C-E, for example, can gen-erate successor B, but the one with the path A-B-Ecannot due to reentry of city B.
In other words, thesearch state here is the path, including all citiesvisited so far, instead of the current city.
However,if the problem is changed as finding the shortestpath from city A to city F, the two paths A-B-Eand A-C-E can then be merged into a single nodeof city E with shorter path tracked.
In such case,the search state will then be the current city insteadof the path.ABCDEFB CC D E E FFigure 2.
An example of state space search fortraveling salesperson problem frominitial city A.BLevel 0Level 1Level 2A46Sixth SIGHAN Workshop on Chinese Language ProcessingIn addition, for state representation three issuesneed to be further addressed.?
Initial state: what the initial state is.?
State transition: how successor states are gen-erated from the current state.?
Goal state: to judge whether the goal state isachieved or not.In the above example in Figure 2, the initial state isthe path containing city A only.
The state transitionis to visit next cities from the current city under theconstraint of no reentry.
The goal states are anypaths that depart from city A and pass every cityexactly once.2.2 State Space SearchWith the state well represented, two data structuresare utilized to guide the search.?
An open list (a priority queue, denoted as openin Figure 3) used for tracking those states notyet spanned.?
A closed list (denoted as closed in Figure 3)used for tracking those states visited so far toavoid the reentry of the same states.Then, a generic algorithm for state space search,with pseudo codes in object-oriented style shownin Figure 3, is performed to find the goal states.The initial state is first inserted into the queue, andan iterative search procedure (denoted as search()in Figure 3) is then called.
For each iteration in thesearch procedure, the search state is popped fromthe queue and inspected one by one.
If it is the goalstate, the procedure terminates and returns the goalstate.
Otherwise the successors of the current stateare generated, and inserted into the queue indi-vidually according to the priority provided not yetvisited.
The search procedure could be called mul-tiple times if more than one goal states are desired.Note that the algorithm in Figure 3 is genericenough to be adapted for various search strategies,including depth first search, breadth first search,best first search, algorithm A, algorithm A* and soon, by simply providing different evaluation func-tion f(n) of search state n for prioritizing the queue.For depth first search, for example, those stateswith the highest depth will have higher priority andbe inserted at the front of the queue, while forbreadth first search at the back.2.3 Optimality and EfficiencyIf the evaluation function f(n) satisfiesf(n) = g(n) + h(n),where g(n) is the real cost from the initial stateto the current state n and h(n) is the heuristic esti-mate of the cost from the current state n to the goalstate, such type of algorithm is called algorithm A.If further, the constraint of admissibility for h(n)holds, i.e.,h(n)?
h*(n),where h*(n) is the true minimum cost from cur-rent state n to the goal state, then optimality can beguaranteed, or equivalently, the goal state obtainedfirst will give minimum cost.
Such type of algo-rithm is called algorithm A*.
It can be proved thatfor algorithm A*, the closer the heuristic h(n) is tothe true minimum cost, h*(n), the higher the searchefficiency is.3 A*-based Dependency ParserIn this section, how dependency parsing is mod-eled as an A* search problem will be depicted indetail.3.1 FormulationFirst, let W = {W1, W2, ?,Wn} denote the wordlist (sentence) to be parsed, where Wi denotes the i-th word in the list.
And, each word Wi is expressedin the form,open.add(initial);goal = search();search() {while(true) {state = open.pop();if(state.isGoal()) return state;successors = state.getSuccessors();for(successor in successors) {if(!closed.contains(successor)) {closed.add(successor);open.add(successor);}}}}Figure 3.
Generic algorithm for state space search.47Sixth SIGHAN Workshop on Chinese Language ProcessingW = (w, t) (1)where w is the lexical term and t is the part-of-speech tag.
A dependency database can first bebuilt by extracting the dependency links amongwords from a corpus of tree bank.
Given the de-pendency database, a directed dependency graph Gindicating valid links for the word list W can beconstructed, as shown in the example of Figure 4.The direction of the link here indicates the direc-tion of modification.
Link W3 ?
W2 in Figure 4,for example, means W3 can modify (or be attachedto) W2.
The dependency graph G will be used fordirecting the state transition during search.3.2 Representation of Parse StateSince the goal of dependency parsing is to find thedependency tree, the search state can therefore berepresented as the dependency tree constructed sofar (denoted as T).
Besides, a set containing thosewords not yet attached to the dependency tree(denoted as C, meaning ?to be consumed?)
can begenerated dynamically by excluding those wordson the dependency tree T from word list W. Thethree issues depicted in Section 2.1 are thenaddressed as follows.?
Initial state: the dependency tree T is emptyand the set C contains all words in list W.?
State transition: a word is extracted from C andattached onto some node on the dependencytree T, and a successor state T?and C?is thengenerated.
Whether an attachment is valid ornot is determined according to the dependencygraph G, under the constraints of uniqueness(any word has at most one head) and projectiv-ity1 (Covington 2001, Nivre 2003).1 The projective constraint for new attachment is im-posed by those attachments already on the dependencytree.
Each attachment already on the tree forms a non-?
Goal state: the set C is empty while all wordsin W are attached onto the dependency tree T.With the parse state represented well, the genericalgorithm for state space search depicted in Section2.2 can then be performed to find the N-best de-pendency trees.
A partial search space based onthe dependency graph G in Figure 4 is displayed inFigure 5, in which R denotes the root node of de-pendency tree.
As shown in Figure 5, for eachsearch state (denoted by the ovals enclosed withdouble-lines), only a link in form of Wj ?
R or Wj?
Wi is actually tracked Through tracing thesearch tree back from the current state, all links canbe obtained, and the overall dependency tree T canbe constructed accordingly.
For the search state W3?
W2 in Figure 5, for example, the links W1?
R,W2?
W1 and W3 ?
W2 are obtained through backtracing, which can then construct the dependencytree, W3 ?
W2 ?
W1 ?
R, as can be seen on theleft-hand side of Figure 5.
This is similar to thecase of traveling salesperson problem in Figure 2,in which only the current city is tracked for eachstate, but the path that identifies the search stateuniquely can be obtained by back tracing.crossing region between its head and modifier to con-strain new attachments for those words located withinthat region.W1 W2W3W6W5W4Figure 4.
Example dependency graph.W7W3?W2W2?
RW1W2W3W1W7?W3W6?W1RRTW4,W5,W6,W7C NullR?W3W1W1?
RFigure 5.
A partial search space where somesearch states are associated with theirdependency trees with dashed lines.W3?W1W2 W2 W6W2?W148Sixth SIGHAN Workshop on Chinese Language Processing3.3 Cost FunctionFor given word list W, the dependency tree T withthe highest probability P(T) is desired, whereP(T) = {??P(Wl?R)}?{?
?P(Wj?Wi)} (2)The first term corresponds to those attachments onthe root node of dependency tree (i.e.
headlesswords), while the second term corresponds to theother attachments.
In A* search, the optimal goalstate obtained is guaranteed to give the minimalcost.
So, if the cost function, g(n), is defined as theminus logarithm of P(T),g(n)?
-log (P(T))=?
(?log(P(Wl?R))) +?(-log(P(Wj?Wi)))??
?step-cost (3)then the A* search algorithm that minimizes thecost g(n) will eventually maximizes the probabilityof the dependency tree, P(T).
Here the minus loga-rithms of the probabilities in Equation (3), ?log(P(Wl?
R)) and ?log(P(Wj?Wi)), can be re-garded as the step cost for each attachment (or link)accumulated during search.
Furthermore, sinceeach word is expressed with a lexical term and apart-of-speech tag as shown in Equation (1), theprobability for each link can be depicted as,P(Wl?
R) = P(Wl | R) = P(tl|R)?P(wl|tl) (4)P(Wj?Wi) = P(Wj | Wi) = P(tj|ti)?P(wj|tj) (5)assuming the word list W is generated by theBaysian networks in Figure 6(a) and 6(b) respec-tively.
Note that here the probability for a linkP(Wj ?
Wi) involves not only the lexical terms wjand wi but also the part-of-speech tags tj and ti, andis denoted as link bigram.
Such formulation can begeneralized to high order link n-grams.
Figure 6(c),for example, displays the Baysian network for thelink Wk ?
Wj conditioned on Wj?
Wi, based onwhich the link trigram is defined asP(Wk?Wj | Wj?Wi) = P(Wk | Wi, Wj)= P(tk | ti, tj)?P(wk | tk).
(6)When comparing Figure 6(c) with Figure 6(d), theBaysian networks for link trigram P(Wk | Wi, Wj)and conventional linear trigram P(Wi+2 | Wi, Wi+1)respectively, it could be found that link n-gram isflexible for long-distance stochastic dependencies,though the two topologies look similar.
Undoubt-edly, the Baysian networks in Figure 6 appear toosimple to model the real statistics precisely.
In thelink Wj?
Wi, for example, Wj might depend on notonly its parent Wi but the children of Wi (Eisner,1996).
Also, the direction (sgn(i-j)) and the dis-tance (|i-j|) of modification between Wi and Wjmight be important (Ohno et al, 2004).
All thesefactors could be taken into account by simply in-cluding the minus logarithms of the correspondingprobabilities into the step cost.3.4 Heuristic FunctionIn A* search, the evaluation function f(n) consistsof g(n), the real cost from the initial state to thecurrent state n, and h(n), the cost estimated heuris-tically from the current state n to the goal state.Now for dependency parsing, g(n) defined inEquation (3) is the accumulated cost for those at-tachments on current dependency tree T, while h(n)is the predicted cost of the attachments for thewords in C which have not yet been attached.Since h(n)?
h*(n) has to hold for admissibility, itis necessary to estimate h(n) conservatively enoughso as not to exceed the true minimum cost h*(n).To achieve higher search efficiency, however, it ispreferred to estimate h(n) more aggressively suchthat h(n) can be as close to h*(n) as possible.Therefore, in this paper, h(n) is estimated with theminimum of minus logarithms of link n-grams foreach word in C, with different levels of constraintsdescribed below.wjtj tiFigure 6.
Baysian networks of (a) link unigram(b) link bigram (c) link trigram (d)linear trigram.
(a)tk tjwiti(c)wiwjwk(d)ti+2P(Wj | Wi)wltl RP(Wl | R)P(Wk | Wi , Wj)ti+1 tiwi+2 wi+1 wiP(Wi+2|Wi , Wi+1)(b)49Sixth SIGHAN Workshop on Chinese Language Processing?
Global heuristic: the minimum is static, and canbe calculated in advance before parsing anysentence by considering all link n-grams foreach word.?
Local heuristic: the minimum is calculated ac-cording to the dependency graph G of currentparsing sentence by considering all possiblelink n-grams with respect to G.?
Dynamic heuristic: the minimum is calculateddynamically according to current dependencytree T during parsing by considering only pro-jective link n-grams with respect to G for cur-rent dependency tree T.By virtue of taking the minimum of minus loga-rithms of link n-grams, admissibility can be guar-anteed for these heuristics.
The latter with stricterconstraints on finding the minimum can give moreprecise estimate of cost (with higher h(n)) than theformer, and is thus expected to be more efficient,as will be discussed in the next section.4 ExperimentsThe stochastic dependency parsing scheme pro-posed in this paper has been first tested on the Chi-nese Tree Bank 3.0 licensed by Academia Sinica,Taiwan, with six sets of data collected from differ-ent sources (Chen et al, 1999).
Head-modifierlinks (with lexical term and part-of-speech tag) canbe extracted from the tree bank since it is producedby a head-driven parser (Chen, 1996).
In our ex-periments, One set, named as oral.check, contain-ing 4156 sentences manually transcribed from dia-logue database is used to train the dependency da-tabase, link statistics including conditional prob-abilities, link unigrams and bigrams as shown inEquation (4) and (5), and so on.
Another set,named as ev.check, containing 1797 sentences intext books of elementary school is used to test theA*-based dependency parser.
Note here the train-ing corpus and testing corpus are of different do-mains, and each word in test sentences is tran-scribed into (w, t) format with lexical term w andassociated part-of-speech tag t.4.1 Coverage RateThe number of occurrences and the coverage ratefor the conditional probability P(wj|tj), link uni-gram P(tj) and link bigram P(tj|ti) respectively areshown in Table 1.
As can be seen in this table, thecoverage rate for P(wj|tj) is as low as 50.8%, sincethe training and testing domains are quite different.The coverage rates for link unigram and bigram,however, can be up to 94.06% and 84.88% respec-tively.
This implies that, the link probabilities canbe estimated more appropriately and contributemore in finding the dependency trees.
To handlethe issue of data sparseness, in the following ex-periments a simple n-gram backoff mechanism isutilized for smoothing.No.
of occurrences P(wj|tj) P(tj) P(tj|ti)Training corpus 8137 120 3383Testing corpus 2791 101 1468Overlaps 1418 95 1246Coverage rate 50.80% 94.06% 84.88%Table 1.
Coverage rates for link statistics.4.2 Parsing AccuracyThe experiment settings for dependency parsingare depicted as below.
The first, denoted as BASE,is the baseline setting, while the others describe thesearch conditions applied to the baseline incremen-tally.
The heuristic h(n) used here is the dynamicheuristic.?
BASE: baseline with the cost defined in Equa-tion (3), (4) and (5).?
RP: root penalty for every root attachment Wl?
R is included into the step cost.?
DIR: the statistics for the direction of modifica-tion, P(D|Wj ?
Wi)=P(D|ti, tj), is included intothe step cost where D?
sgn(j?i).?
NA: the statistics for the number of attachments(or valence), P(Ni|Wi)=P(Ni|ti), is included intothe step cost and updated incrementally forevery new attachment onto Wi.
Ni is the currentnumber of words attached to Wi.?
REJ: the obtained dependency trees are in-spected one by one, and rejected if any of theconditions occurs: (a) the modifiers for con-junction word (e.g.
??
?, meaning ?and?)
be-long to different part-of-speech categories (in-correct meaning) (b) illegal use of ???(mean-ing?of?)
as leaf node (incomplete meaning).The baseline setting with Equation (3), (4) and (5)is based on the Baysian networks depicted in Fig-50Sixth SIGHAN Workshop on Chinese Language Processingure 6(a) and 6(b).
Finer statistics could be appliedin the settings RP, DIR and NA.
The setting RP,for example, can restrain the number of headlesswords.
The setting DIR can take into account thecost due to the direction of modification, since it infact matters2, but the link probability P(Wj ?
Wi)in Equation (5) cannot differentiate between thetwo directions.
The distance of modification, |j-i|,might matter too, but is not considered here due toquite limited amount of training data.
The settingNA can include the cost due to the number of at-tachments.
Verbs, for example, often require moreattachments, and may produce lower cost forhigher number of attachments.
Here for simplifica-tion, the statistics of P(Ni|ti) are gathered only forNi = 0,1,2, and >=3, respectively.
In addition tousing finer statistics, it is also feasible to use se-mantic or lexical rules to reject the dependencytrees with incorrect or incomplete meanings.
In thesetting REJ, two rules are utilized.
One is, the con-junction word should have modifiers in the samepart-of-speech category, while the other is, theword??
?must have at least one modifier.No.
ofAccurateSentencesSA DABASE 867 48.25% 77.42%+ RP 1039 57.82% 80.74%+ DIR 1102 61.32% 82.72%+ NA 1211 67.39% 85.21%CrossDomain+ REJ 1229 68.39% 85.99%BASE 1109 61.71% 85.93%+ RP 1314 73.12% 89.91%+ DIR 1340 74.57% 90.66%+ NA 1476 82.16% 92.81%WithinDomain+ REJ 1484 82.58% 93.20%Table 2.
Parsing accuracy for various settings.The parsing performance can be measured withsentence accuracy (SA) and dependency accuracy(DA).
Table 2 shows the experimental results forthe above settings.
The results for within-domaintest by using the set ev.check for both training andtesting are also listed here for comparison.
It canbe found in this table that, the performance ofcross-domain test is not so good for the baseline2 In the Chinese phrase??(in)???
(front)?, for example,??
?is the head while???
?is the modifier, and???
?always modifies??
?backwards.setting, but can be persistently improved whenfiner statistics are applied.
Rejection of incorrect orincomplete dependency trees is also helpful (REJ),though very few semantic or lexical constraints areutilized here.
When the constraints of all settingsare applied, 85.99% dependency accuracy can beobtained at 68.39% sentence accuracy.4.3 N-best OutputNote that due to data sparseness and the limitationof simplified Baysian networks, some parsing er-rors are intrinsic and difficult to avoid.
Figure 7shows the parsing result for a clause ????????
(meaning ?at the time for eating dinner?).
Theincorrect parsing result on the right-hand side(meaning ?to eat the time of dinner?)
is syntacti-cally correct and inevitable in fact, since the linkprobabilities (P(tj) or P(tj|ti)) dominate over theconditional probabilities (P(wj|tj)), as illustrated inSection 4.1.
Such problem could possibly be alle-viated to some extent if deep semantic constraints(e.g.
a transitive verb may require a subject and anobject) or lexical constraints (e.g.
some adjectivesmay modify only specific nouns) could be utilizedfor rejection while reprocessing the N-best output.Table 3 shows the experimental results of N-bestoutput for the setting REJ.
In Table 3 it can be seenthat higher sentence accuracy, 80.08%, for top-5output can be achieved, which implies large spacefor improvement in N-best processing.Top 1 Top 2 Top 3 Top 4 Top 5SA 68.39% 75.24% 78.02% 79.41% 80.08%Table 3.
Sentence accuracy for N-best output.?
(VC31)??
(Nad)?
(DE)??
(Naa)?
(VC31)??
(Nad)?
(DE)??
(Naa)Figure 7.
The parsing result for the clause????????
(time for eating dinner).correct dependency tree incorrect resulttimeofeatdinnertimeofdinnereat51Sixth SIGHAN Workshop on Chinese Language Processing4.4 Search EfficiencyThe search efficiency for each test sentence can bemeasured heuristically by the order of complexity,defined asC = lognN (7)where n is number of words in the test sentenceand N is the total number of the search nodes forthat sentence.
Cave is then used to denote the aver-age complexity over all test sentences.
In addition,Nall is the total number of search nodes for all testsentences, and can produce the node ratio RN whennormalized.
The experimental results for differentheuristics depicted in Section 3.4 are shown in Ta-ble 4.
As can be observed in this table, muchhigher search efficiency can be obtained for moreprecise heuristic estimate, but with compatible top1 sentence accuracy.
For dynamic heuristic withreal-time estimate of the cost according to the cur-rent dependency tree, the highest efficiency can beobtained at 2.38 average complexity and 14.62%node ratio, respectively.Heuristic SA Cave Nall RNNone 68.84% 2.91 4748268 100%Global 68.34% 2.82 3417766 66.29%Local 68.50% 2.39 841716 17.73%Dynamic 68.39% 2.38 694193 14.62%Table 4.
Search efficiency for different heuristics.5 ConclusionsWe have presented a stochastic dependency pars-ing scheme based on A* admissible search, andverified its parsing accuracy and search efficiencyon the Chinese Tree Bank 3.0.
85.99% dependencyaccuracy at 68.39% sentence accuracy can be ob-tained under cross-domain test.
Among three typesof admissible heuristics proposed, dynamic heuris-tic can achieve the highest efficiency with noderatio 14.62%.
This parser can output N-best de-pendency trees, and reprocess them flexibly withmore semantic constraints so as to achieve higherparsing accuracy.
This parsing scheme is the basisfor natural language understanding in our researchproject on dialogue systems for mission delegationtasks.
We plan to perform comparative studies withother non-stochastic approaches, and evaluate ourapproach on the shared task of dependency parsing.ReferencesAllen James, 1995.
Natural Language Understanding,The Benjamin/Cummings Publishing Company.Chen K. J.
1996.
A Model for Robust Chinese Parser,Computational Linguistics and Chinese LanguageProcessing, Vol.
1, no.
1, pp.
183-205.Chen K. J., et al 1999.
The CKIP Chinese Treebank:Guidelines for Annotation, ATALA Workshop ?Treebanks, Paris, pp.
85-96.Chen Yuchang, Asahara Masayuki and Matsumoto Yuji.2005.
Machine Learning-based Dependency Ana-lyzer for Chinese, ICCC-2005.Covington Michael A.
2001.
A Fundamental Algorithmfor Dependency Parsing, Proc.
of the 39th AnnualACM Southeast Conference, pp.
95-102.Dienes Peter, Koller Alexander and Kuhlmann Macro.2003.
Statistical A* Dependency Parsing, Proc.Workshop on Prospects and Advances in the Syn-tax/Semantics Interface.Eisner Jason M. 1996.
Three Probabilistic Models forDependency Parsing: An Exploration, Proc.
COL-ING, pp.
340-345.Jurafsky Daniel and Martin James H. 2001.
Speech andLanguage Processing, Prentice Hall, NJ.Klein Dan and Manning Christopher D. 2003.
A* Pars-ing: Fast Exact Viterbi Parse Selection, Proc.NAACL/HLT.Klein Dan and Manning Christopher D. 2003.
Fast Ex-act Inference with a Factored Model for NaturalLanguage Parsing, Proc.
Advances in Neural Infor-mation Processing Systems.Kudo Taku and Matsumoto Yuji.
2002.
Japanese De-pendency Analysis using Cascaded Chunking, Proc.CONLL.Nivre Joakim, 2003, An Efficient Algorithm for Projec-tive Dependency Parsing, Proc.
International Work-shop on Parsing Technologies (IWPT).Nivre Joakim and Scholz Mario.
2004.
DeterministicDependency Parsing of English Text, Proc.
COLING.Ohno Tomohiro, et al 2004.
Robust Dependency Pars-ing of Spontaneous Japanese Speech and Its Evalua-tion, Proc.
ICSLP.Russell S. and Norvig P. 2003.
Artificial Intelligence: AModern Approach, Prentice Hall.Yamada Hiroyasu and Matsumoto Yuji.
2003.
Statisti-cal Dependency Analysis with Support Vector Ma-chines, Proc.
IWPT52Sixth SIGHAN Workshop on Chinese Language Processing
