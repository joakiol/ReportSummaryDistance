Proceedings of the 6th Workshop on Building and Using Comparable Corpora, pages 1?10,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsCross-lingual WSD for Translation Extractionfrom Comparable CorporaMarianna ApidianakiLIMSI-CNRSRue John Von NeumannBP 133, 91403Orsay Cedex, Francemarianna@limsi.frNikola Ljubes?ic?Dept.
of Information SciencesUniversity of ZagrebIvana Luc?ic?a 3, HR-10000Zagreb, Croatianljubesi@ffzg.hrDarja Fis?erDepartment of TranslationUniversity of LjubljanaAs?kerc?eva 2, SI-1000Ljubljana, Sloveniadarja.fiser@ff.uni-lj.siAbstractWe propose a data-driven approach to en-hance translation extraction from compa-rable corpora.
Instead of resorting to anexternal dictionary, we translate sourcevector features by using a cross-lingualWord Sense Disambiguation method.
Thecandidate senses for a feature correspondto sense clusters of its translations in aparallel corpus and the context used fordisambiguation consists of the vector thatcontains the feature.
The translationsfound in the disambiguation output con-vey the sense of the features in the sourcevector, while the use of translation clusterspermits to expand their translation withseveral variants.
As a consequence, thetranslated vectors are less noisy and richer,and allow for the extraction of higher qual-ity lexicons compared to simpler methods.1 IntroductionLarge-scale comparable corpora are available inmany language pairs and are viewed as a sourceof valuable information for multilingual applica-tions.
Identifying translation correspondences inthis type of corpora permits to construct bilinguallexicons for low-resourced languages, and to com-plement and reduce the sparseness of existing re-sources (Munteanu and Marcu, 2005; Snover etal., 2008).
The main assumption behind transla-tion extraction from comparable corpora is that asource word and its translation appear in similarcontexts (Fung, 1998; Rapp, 1999).
So, in orderto identify a translation correspondence betweenthe two languages, the contexts of the source wordand the candidate translation have to be compared.For this comparison to take place, the same vectorspace has to be produced, which means that thevectors of the one language have to be translatedin the other language.
This generally assumes theavailability of a bilingual dictionary which mighthowever not be the case for some language pairsand domains.
Moreover, the classic way in whicha dictionary is put into use, which consists in trans-lating vector features by their first translation inthe dictionary, neglects semantics.
We expect thata method capable of identifying the correct senseof the features and translating them accordinglycould contribute to producing cleaner vectors andto extracting higher quality lexicons.In this paper, we show how source vectorscan be translated into the target language by across-lingual Word Sense Disambiguation (WSD)method which exploits the output of data-drivenWord Sense Induction (WSI) (Apidianaki, 2009),and demonstrate how feature disambiguation en-hances the quality of the translations extractedfrom the comparable corpus.
This study extendsour previous work on the topic (Apidianaki et al2012) by applying the proposed methods to a com-parable corpus of general language (built fromWikipedia) and optimizing various parameters thataffect the quality of the extracted translations.
Weexpect the disambiguation to have a beneficial im-pact on the results given that polysemy is a fre-quent phenomenon in a general, mixed-domaincorpus.
Our experiments are carried out on theEnglish-Slovene language pair but as the methodsare totally data-driven, the approach can be easilyapplied to other languages.The paper is organized as follows: In the nextsection, we present some related work on bilin-gual lexicon extraction from comparable corpora.Section 3 presents the data used in our experimentsand Section 4 provides details on the approach andthe experimental setup.
In Section 5, we report anddiscuss the obtained results before concluding andpresenting some directions for future work.12 Related workThe traditional approch to translation extractionfrom comparable corpora and most of its exten-sions (Fung, 1998; Rapp, 1999; Shao and Ng,2004; Otero, 2007; Yu and Tsujii, 2009; Marsiand Krahmer, 2010) presuppose the availabilityof a bilingual lexicon for translating source vec-tors into the target language.
A translation can-didate is generally considered as correct if it isan appropriate translation for at least one senseof the source word in the dictionary, which of-ten corresponds to its most frequent sense.
Analternative consists in considering all translationsprovided for a word in the dictionary but weight-ing them by their frequency in the target lan-guage (Prochasson et al 2009; Hazem and Morin,2012).
The high quality of the exploited hand-crafted resources, combined to the skewed distri-bution of the translations corresponding to differ-ent word senses, often lead to satisfying results.Nevertheless, the applicability of the methods islimited to languages and domains where bilingualresources are available.
Moreover, by promotingthe most frequent sense/translation, this approachneglects polysemy.
We believe that feature dis-ambiguation can lead to the production of cleanervectors and, consequently, to higher quality re-sults.The need to bypass pre-existing dictionarieshas been addressed by Koehn and Knight (2002)who built the initial seed dictionary automatically,based on identical spelling features between En-glish and German.
Cognate detection has alsobeen used by Saralegi et al(2008) for extract-ing word translations from English-Basque com-parable corpora.
The cognate and seed lexiconapproaches have been successfully combined byFis?er and Ljubes?ic?
(2011) who showed that the re-sults with an automatically created seed lexicon,based on language similarity, can be as good aswith a pre-existing dictionary.
But all these ap-proaches work on closely-related languages andcannot be used as successfully for language pairswith little lexical overlap, such as English andSlovene, which is the case in this experiment.Regarding the translation of the source vectors,we use contextual information to disambiguatetheir features and translate them using clustersof semantically similar translations in the targetlanguage.
A similar idea has been implementedby Kaji (2003) who performed sense-based wordclustering to extract sets of synonymous transla-tions from comparable corpora with the help of abilingual dictionary.Using translation clusters permits to expandfeature translation and to suggest multiple seman-tically correct translations.
A similar approach hasbeen adopted by De?jean et al(2005) who expandvector translation by using a bilingual thesaurusinstead of a lexicon.
In contrast to their work, themethod proposed here does not rely on any exter-nal knowledge source to determine word sensesor translation equivalents, and is thus fully data-driven and language independent.3 Resources3.1 Comparable corpusThe comparable corpus from which the bilin-gual lexicon will be extracted is a collection ofEnglish (EN) and Slovene (SL) texts extractedfrom Wikipedia.
The February 2013 dumps ofWikipedia articles were downloaded and cleanedfor both languages after which the English cor-pus was tokenized, part-of-speech (PoS) taggedand lemmatized with the TreeTagger (Schmid,1994).
The same pre-processing was applied to theSlovene corpus with the ToTaLe analyzer (Erjavecet al 2010) which uses the TnT tagger (Brants,2000) and was trained on MultextEast corpora.The Wikipedia corpus contains about 1.5 billiontokens for English and almost 24 million tokensfor Slovene.In previous work, we applied our approach to aspecialized comparable corpus from the health do-main (Apidianaki et al 2012).
The results wereencouraging, showing how translation clusteringand vector disambiguation help to improve thequality of the translations extracted from the com-parable corpus.
We believe that the positive im-pact of this approach will be more significant onlexicon extraction from a general language com-parable corpus, in which polysemy is more promi-nent.3.2 Parallel corpusThe parallel corpus used for clustering and wordsense induction consists of the Slovene-Englishparts of Europarl (release v6) (Koehn, 2005) andof JRC-Acquis (Steinberger et al 2006) andamounts to approximately 35M words per lan-guage.
A number of pre-processing steps are ap-plied to the corpus prior to sense induction, such2Figure 1: Translation extraction from comparable corpora using cross-lingual WSI and WSD.as elimination of sentence pairs with a great dif-ference in length, lemmatization and PoS taggingwith the TreeTagger (for English) and ToTaLe (forSlovene) (Erjavec et al 2010).
Next, the cor-pus is word-aligned with GIZA++ (Och and Ney,2003) and two bilingual lexicons are extracted,one for each translation direction (EN?SL/SL?EN).
To clean the lexicons from noisy alignments,the translations are filtered on the basis of theiralignment score and PoS, keeping only transla-tions that pertain to the same grammatical cate-gory as the source word.
We retain only intersect-ing alignments and use for clustering translationsthat translate a source word more than 10 timesin the training corpus.
This threshold reducesdata sparseness issues that affect the clusteringand eliminates erroneous word alignments.
Thefiltered EN-SL lexicon contains entries for 6,384nouns, 2,447 adjectives and 1,814 verbs havingmore than three translations in the training corpus.The parallel corpus, which contains EU texts, ismore specialized than the comparable corpus builtfrom Wikipedia.
This is not the ideal scenario forthis experiment; domain adaptation is importantfor the type of semantic processing we want to ap-ply as there might be a shift in the senses present inthe two corpora.
However, as EU texts often con-tain a lot of general vocabulary, we expect that thisdiscrepancy will not strongly affect the quality ofthe results.3.3 Gold standardWe evaluate the quality of the bilingual lexiconsextracted from the comparable corpus by compar-ing them to a gold standard lexicon, which wasbuilt from the aligned English (Fellbaum, 1998)and Slovene wordnets (Fis?er and Sagot, 2008).
Weextracted all English synsets from the Base Con-cept sets that belong to the Factotum domain andcontain literals with polysemy levels 1-5 and theirSlovene equivalents which have been validated bya lexicographer.
Of 1,589 such synsets, 200 wererandomly selected and used as a gold standard forautomatic evaluation of the method proposed inthis paper.4 Experimental setup4.1 Overview of the methodFigure 1 gives an overview of the way informa-tion mined from the parallel training corpus is ex-ploited for discovering translations of source (En-glish) words in the comparable corpus.
The par-allel corpus serves to extract an English-Sloveneseed lexicon and source language context vec-tors (Par vectors) for the Slovene translations ofEnglish words.
These vectors form the input tothe Word Sense Induction (WSI) method whichgroups the translations of an English word intoclusters.The clusters of semantically related Slovenetranslations constitute the candidate senses which,together with the Par vectors, are used for dis-ambiguating and translating the vectors extractedfrom the source (English) side of the comparablecorpus (Comp source).
The translated vectors arethen compared to the ones extracted from the tar-get language (Slovene) side of the comparable cor-pus (Comp target) and the best translations are se-lected, for a list of unknown words.
All steps ofthe proposed method illustrated in Figure 1 willbe detailed in the following sections.4.2 Translation clusteringThe translations of the English words in the lex-icon built as described in 3.2 are clustered ac-cording to their semantic proximity using a cross-lingual Word Sense Induction method (Apidi-anaki, 2008).
For each translation Ti of a wordw, a vector is built from the content word co-3Language POS Source word Slovene sense clustersEN?SLNounssphere{krogla} (geometrical shape){sfera, podroc?je} (area)address{obravnava, res?evanje, obravnavanje} (dealing with){naslov} (postal address)portion{kos} (piece){obrok, porcija} (serving){delez?}
(share)figure{s?tevilka, podatek, znesek} (amount){slika} (image){osebnost} (person)Verbsseal{tesniti} (to be water-/airtight){zapreti, zapec?atiti} (to close an envelope or some other container)weigh{pretehtati} (consider possibilities){tehtati, stehtati} (check weight)educate{pouc?iti} (give information){izobraz?evati, izobraziti} (give education)consume{potros?iti} (spend money/goods){uz?ivati, zauz?iti} (eat/drink)Adjsmature{zrel, odrasel} (adult){zorjen, zrel} (ripe)minor{nepomemben} (not very important){mladoleten, majhen} (under 18 years old)juvenile{nedorasel} (not adult/biologically mature yet){mladoleten, mladoletnis?ki} (not 18/legally adult yet)remote{odmaknjen, odroc?en} (far away and not easily accessible){oddaljen daljinski} (controlled from a distance (e.g.
remote control))Table 1: Entries from the English-Slovene sense cluster inventory.occurrences of w in the parallel sentences where itis translated by Ti.
Let N be the number of featuresretained for each Ti from the corresponding sourcecontexts.
Each feature Fj (1 ?
j ?
N) receives atotal weight with a translation Ti, tw(Fj,Ti), de-fined as the product of the feature?s global weight,gw(Fj), and its local weight with that translation,lw(Fj,Ti).
The global weight of a feature Fj is afunction of the number Ni of translations (Ti?s) towhich Fj is related, and of the probabilities (pi j)that Fj co-occurs with instances of w translated byeach of the Ti?s:gw(Fj) = 1?
?Ti pi j log(pi j)Ni(1)Each pi j is computed as the ratio of the co-occurrence frequency of Fj with w when translatedas Ti to the total number of features seen with Ti:pi j =cooc frequency(Fj,Ti)N(2)The local weight lw(Fj,Ti) between Fj and Ti di-rectly depends on their co-occurrence frequency:lw(Fj,Ti) = log(cooc frequency(Fj,Ti)) (3)The pairwise similarity of the translations is cal-culated using the Weighted Jaccard Coefficient(Grefenstette, 1994).WJ(Tm,Tn) =?
j min(tw(Tm,Fj), tw(Tn,Fj))?
j max(tw(Tm,Fj), tw(Tn,Fj))(4)The similarity score of each translation pair iscompared to a threshold locally defined for each wusing an iterative procedure.
The threshold (T ) fora word w is initially set to the mean of the scores(above 0) of its translation pairs.
The set of trans-lation pairs of w is then divided into two sets (G1and G2) according to whether they exceed, or areinferior to, the threshold.
The average of scores ofthe translation pairs in each set is computed (m1and m2) and a new threshold is calculated that isthe average of m1 and m2 (T = (m1+m2)/2).
Thenew threshold serves to separate again the transla-tion pairs into two sets, a new threshold is calcu-lated and the procedure is repeated until conver-gence.The semantically similar translations of w aregrouped into clusters.
Translation pairs with ascore above the threshold form initial clusters that4might be further enriched provided that there existadditional strongly related translations.
Cluster-ing stops when all translations of w are clusteredand all their relations have been checked.
An im-portant feature of the algorithm is that it performssoft clustering, so translations can be found in dif-ferent clusters.
The final clusters are characterizedby global connectivity, i.e.
all their elements arelinked by pertinent relations.Table 1 gives examples of clusters obtained forEnglish words of different PoS with clear sensedistinctions in the parallel corpus.
For each En-glish word, we provide the obtained clusters ofSlovene translations including a description of thesense described by each cluster.
For instance, thetranslations for the adjective minor from the train-ing corpus (nepomemben, mladoleten and majhen)are grouped into two clusters describing its twosenses: {nepomemben} - ?not very important?and {mladoleten, majhen} - ?under 18 years old?.The resulting cluster inventory contains 13,352clusters in total, for 8,892 words.
2,585 of thewords (1,518 nouns, 554 verbs and 513 adjectives)have more than one cluster.In the next section, we explain how the clus-ters and the corresponding translation vectors areused for disambiguating the source language vec-tors extracted from the comparable corpus.4.3 Cross-lingual vector comparison4.3.1 Vector buildingWe build context vectors in the two languages fornouns occurring at least 50 times in the compa-rable corpus.
The frequency threshold is impor-tant for the lexicon extraction approach to producegood results.
As features we use three contentwords to the left and to the right of the retainednouns, stopping at the sentence boundary, withouttaking into account their position.
Log-likelihoodis used to calculate feature weights.In the reported experiments we focus on the1,000 strongest features.
A portion of these fea-tures is disambiguated for each headword, de-pending on the availability of clustering informa-tion.
We observed that disambiguating a smalleramount of features yielded similar results and in-cluding additional features did not improve the re-sults.4.3.2 Vector translation and disambiguationTranslation correspondences between the two lan-guages of the comparable corpus are identified bycomparing the source language vectors, built asdescribed in Section 4.3.1, to the ones of the candi-date translations.
This comparison serves to quan-tify the similarity of the source and target wordsrepresented by the vectors and the highest rankedpairs are retained.For the comparison to take place, the sourcevectors have to be translated in the target language.In most previous work, the vectors were translatedusing external seed dictionaries: the first transla-tion proposed for a word in the dictionary wasused to translate all instances of the word in thevectors irrespective of their sense.
Here, we re-place the external dictionary with the output ofa data-driven cross-lingual WSD method (Apidi-anaki, 2009) which renders the method knowledgelight and adaptable to other language pairs.The translation clusters obtained during WSI(cf.
Section 4.2) describe the senses of the En-glish words in the parallel corpus.
We exploit thissense inventory for disambiguating the features inthe English vectors extracted from the comparablecorpus.
More precisely, we ask the WSD methodto select among the available clusters the one thatcorrectly translates in Slovene the sense of the En-glish features in the vectors built from the compa-rable corpus.
The selection is performed by com-paring information from the context of a feature,which corresponds to the rest of the vector wherethe feature appears, to the source language vectorsof the translations which served to their cluster-ing.
Inside the vectors, the features are orderedaccording to their score, calculated as described inSection 4.3.1.
Feature weights filter out the weakfeatures, i.e.
features with a score below the ex-perimentally set threshold of 0.01.
The retainedfeatures are then considered as a bag of words.On the clusters?
side, the information used fordisambiguation is found in the source languagevectors that revealed the similarity of the transla-tions.
If common features (CFs) exist between thecontext of a feature and the vectors of the transla-tions in a cluster, a score is calculated correspond-ing to the mean of the weights of the CFs with theclustered translations, where weights correspondto the total weights (tw?s) computed between fea-tures and translations during WSI.
In formula 5,CFj is the set of CFs and NCF is the number oftranslations Ti characterized by a CF.wsd score =?NCFi=1 ?
j w(Ti,CFj)NCF ?
|CFj|(5)5PoS Feature Assigned Cluster MFTNounsparty {oseba, stran, pogodbenica, stranka} strankamatter {zadeva, vpras?anje} zadevaVerbssettle {urediti, res?iti, res?evati} res?itifollow {upos?tevati, spremljati, slediti} sleditiAdjsalternative {nadomesten, alternativen} alternativeninvolved {vkljuc?en, vpleten} vkljuc?enTable 2: Disambiguation results.The cluster that receives the highest score is se-lected and assigned to the feature as a sense tag.The features are also tagged with their most fre-quent translation (MFT) in the parallel corpus,which sometimes already exists in the cluster se-lected during WSD.In Table 2, we present examples of disam-biguated features of different PoS from the vec-tor of the word transition.
The context used fordisambiguation consists of the other strong fea-tures in the vector and the cluster that best de-scribes the sense of the features in this contextis selected.
In the last column, we provide theMFT of the feature in the parallel corpus.
In theexamples shown here the MFT translation alreadyexists in the cluster selected by the WSD methodbut this is not always the case.
As we will showin the Evaluation section, the configuration wherethe MFT from the cluster assigned during disam-biguation is selected (called CLMFT) gives betterresults than MFT, which shows that the MFT inthe selected cluster is not always the most frequentalignment for the word in the parallel corpus.
Fur-thermore, the clusters provide supplementary ma-terial (i.e.
multiple semantically correct transla-tions) for comparing the vectors in the target lan-guage and improving the baseline results.
Still,MFT remains a very powerful heuristic due to theskewed distribution of word senses and transla-tions.4.4 Vector comparisonThe translation clusters proposed during WSD forthe features in the vectors built from the sourceside of the comparable corpus serve to translate thevectors in the target language.
In our experiments,we compare three different ways of translating thesource language features.1.
by keeping the most frequent transla-tion/alignment of the feature in the parallelcorpus (MFT);2. by keeping the most frequent translation fromthe cluster assigned to the feature during dis-ambiguation (CLMFT); and3.
by using the same cluster as in the second ap-proach, but producing features for all transla-tions in the cluster with the same weight (CL).The first approach (MFT) serves as the base-line since, instead of the sense clustering andWSD results, it just uses the most frequentsense/alignment heuristic.
In the first batch of ex-periments, we noticed that the results of the CL andCLMFT approaches heavily depend on the part-of-speech of the features.
So, we divided the CL andCLMFT approaches into three sub-approaches:1. translate only nouns, verbs or adjectives withthe clusters and other features with the MFTapproach (CLMFT N, CLMFT V, CLMFT A);2. translate nouns and adjectives with the clus-ters and verbs with the MFT approach(CLMFT NA); and3.
translate nouns and verbs with the clus-ters and adjectives with the MFT approach(CLMFT NV).The distance between the translated source andthe target-language vectors is computed by theDice metric.
By comparing the translated sourcevectors to the target language ones, we obtain aranked list of candidate translations for each goldstandard entry.5 Evaluation5.1 MetricsThe final result of our method consists in rankedlists of translation candidates for gold standard en-tries.
We evaluate this output by the mean recipro-cal rank (MRR) measure which takes into account6the rank of the first good translation found for eachentry.
Formally, MRR is defined asMRR =1|Q||Q|?i=11ranki(6)where |Q| is the length of the query, i.e.
the num-ber of gold standard entries we compute transla-tion candidates for, and ranki is the position of thefirst correct translation in the candidate list.5.2 ResultsTable 4 shows the translation extraction resultsfor different configurations.
The MFT score isused as the baseline.
We observe that disam-biguating all features in the vectors (CL) yieldslower results than the baseline compared to se-lecting only the most frequent translation from thecluster which slightly outperforms the MFT base-line.
In the CLMFT N, CLMFT NA, CLMFT NVconfigurations we disambiguate noun features,nouns and adjectives, and nouns and verbs, respec-tively, and translate words of other PoS using theMFT.
In CLMFT N, for instance, nouns are dis-ambiguated while verbs and adjectives are trans-lated by the word to which they were most fre-quently aligned in the parallel corpus.
The threeconfigurations where nouns are disambiguated(CLMFT N, CLMFT NA, CLMFT NV) give betterresults compared to those addressing verbs or ad-jectives alone.
Interestingly, disambiguating onlyadjectives gives worse results than disambiguatingonly verbs, but the combination of nouns and ad-jectives outperforms the combination of nouns andverbs.In CLMFT, features of all PoS are disambiguatedbut we only keep the most frequent translation inthe cluster and ignore the other translations.
Thissetting gives much better results than CL, wherethe whole cluster is used, which highlights twofacts: first, that disambiguation is beneficial fortranslation extraction and, second, that the noisepresent in the automatically built clusters harmsthe quality of the translations extracted from thecomparable corpus.
The better score obtained forCLMFT compared to MFT also shows that, in manycases, the most frequent translation in the clusterdoes not coincide with the most frequent align-ment of the word in the parallel corpus.
So, disam-biguation helps to select a more appropriate trans-lation than the MFT approach.
This improvementcompared to the baseline shows again that WSD isMRRMFT 0.0685CLMFT 0.0807CL 0.0434CLMFT N 0.0817CLMFT A 0.07CLMFT V 0.0714CLMFT NA 0.0842CLMFT NV 0.08048Table 3: Results of the experiment.MRR diff p-valueMFT CLMFT 0.0122 0.1830MFT CL 0.0251 0.0410CLMFT CL 0.0373 0.0120MFT CLMFT NA 0.0157 0.4296MFT CLMFT NV 0.0120 0.5195Table 4: Comparison of different configurations.useful in this setting.In Table 4, the results for different configura-tions are compared.
The statistical significance ofthe difference in the results was calculated by ap-proximate randomization (1,000 repetitions).
Weobserve that the differences between the CL andMFT configurations and the CL and CLMFT ones,are statistically significant.
This confirms that tak-ing most frequent translations, disambiguated ornot, works better than exploiting all the informa-tion in the clusters.
The remainder of the dif-ferences in the results are not statistically signif-icant.
One could wonder why the p-values are thathigh in case of the MFT setting on one side andCLMFT NA and CLMFT NV settings on the otherside although the differences in the results are notthat high.
The most probable explanation is thatthere is a low intersection in correct results anderrors.
Because of that, flipping the results be-tween the two systems ?
as performed in approx-imate randomization ?
often generates differenceshigher than the initial difference on the original re-sults.5.3 Qualitative analysisManual evaluation of the results shows that theprocedure can deal with concrete words much bet-ter than with abstract ones.
For example, the cor-rect translation of the headword enquiry is thethird highest-ranked translation.
The results are7also much better with monosemous and domain-specific terms (e.g.
the correct translation for cat-aclysm is the top-ranking candidate).
On the otherhand, general and polysemous expressions thatcan appear in a wide range of contexts are a muchtougher nut to crack.
For example, the correcttranslation candidate for word role, which can beused in a variety of contexts as well as metaphor-ically, is in the tenth position, whereas no correcttranslation was found for transition.
However, itmust be noted that even if the correct translation isnot found in the results, the output of our methodis in most cases a very coherent and solid descrip-tion of the semantic field of the headword in ques-tion.
This means that the list can still be useful forlexicographers to illicit the correct translation thatis missing, or organize the vocabulary in terms oftheir relational-semantic principles.We have also performed an error analysis incases where the correct translation could not befound among the candidates, which consisted ofchecking the 30 strongest disambiguated featuresof an erroneously translated headword.
We ob-served cases where the strongest features in thevectors are either very abstract and generic or tooheterogeneous for our method to be able to per-form well.
This was the case with the headwordscharacterisation, antecedent and thread.
In caseswhere the strongest features represented the con-cept clearly but the correct translation was notfound, we examined cluster, WSD and MFT qual-ity, as suggested by the parallel corpus.
The mainsource of errors in these cases is the noise in theclusters which is often due to pre-processing er-rors, especially in the event of multi-word expres-sions.
It seems that clustering is also problematicfor abstract or generic words, where senses mightbe lumped together.
The WSD step, on the otherhand, does not seem to introduce noise to the pro-cedure as it is correct in almost all the cases wehave examined.6 Discussion and conclusionWe have shown how cross-lingual WSD can beapplied to bilingual lexicon extraction from com-parable corpora.
The disambiguation of sourcelanguage features using translation clusters con-stitutes the main contribution of this work andpresents several advantages.
First, the method per-forms disambiguation by using sense descriptionsderived from the data, which clearly differentiatesour method from the approaches based on externallexicons and extends its applicability to resource-poor languages.
The translation clusters acquiredthrough WSI serve to disambiguate the features inthe source language context vectors and to pro-duce less noisy translated vectors.
An additionaladvantage is that the sense clusters often containmore than one translation and, therefore, providesupplementary material for the comparison of thevectors in the target language.The results show that data-driven semantic anal-ysis can help to circumvent the need for an exter-nal seed dictionary, traditionally considered as aprerequisite for translation extraction from paral-lel corpora.
Moreover, it is clear that disambiguat-ing the vectors improves the quality of the ex-tracted lexicons and manages to beat the simpler,but yet powerful, most frequent translation heuris-tic.
These encouraging results pave the way to-wards pure data-driven methods for bilingual lex-icon extraction.
This knowledge-light approachcan be applied to languages and domains that donot dispose of large-scale seed dictionaries but forwhich parallel corpora are available.An avenue that we intend to explore in futurework is to extract translations corresponding todifferent senses of the headwords.
Up to now,research on translation extraction has most of-ten aimed the identification of one good trans-lation for a source word in the comparable cor-pus.
This has also been the case because mostworks have focused on identifying translationsfor specialized terms that do not convey differ-ent senses.
However, words in a general lan-guage corpus like Wikipedia can be polysemousand it is important to identify translations corre-sponding to their different senses.
Moreover, pol-ysemy makes the translation extraction proceduremore difficult, as features corresponding to differ-ent senses are mingled in the same vector.
A wayto discover translations corresponding to differentword senses would be to apply a monolingual WSImethod on the source side of the comparable cor-pus which would group the closely related usagesof the headwords together, and to then build vec-tors for each usage group hopefully describing adistinct sense.
Using the generated sets of vectorsseparately will allow to extract translations corre-sponding to different senses of the source words.8ReferencesMarianna Apidianaki, Nikola Ljubes?ic?, and DarjaFis?er.
2012.
Disambiguating vectors for bilin-gual lexicon extraction from comparable corpora.In Eighth Language Technologies Conference, pages10?15, Ljubljana, Slovenia.Marianna Apidianaki.
2008.
Translation-orientedsense induction based on parallel corpora.
In Pro-ceedings of the 6th International Conference onLanguage Resources and Evaluation (LREC-08),pages 3269?3275, Marrakech, Morocco.Marianna Apidianaki.
2009.
Data-driven SemanticAnalysis for Multilingual WSD and Lexical Selec-tion in Translation.
In Proceedings of the 12thConference of the European Chapter of the Asso-ciation for Computational Linguistics (EACL-09),pages 77?85, Athens, Greece.Thorsten Brants.
2000.
TnT - A Statistical Part-of-Speech Tagger.
In Proceedings of the Sixth AppliedNatural Language Processing (ANLP-2000), Seat-tle, WA.Herve?
De?jean, Eric Gaussier, Jean-Michel Renders,and Fatiha Sadat.
2005.
Automatic processing ofmultilingual medical terminology: applications tothesaurus enrichment and cross-language informa-tion retrieval.
Artificial Intelligence in Medicine,33(2):111?124, February.Tomaz?
Erjavec, Darja Fis?er, Simon Krek, and NinaLedinek.
2010.
The JOS Linguistically TaggedCorpus of Slovene.
In Proceedings of the SeventhInternational Conference on Language Resourcesand Evaluation (LREC?10), Valletta, Malta.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cambridge,MA.Darja Fis?er and Nikola Ljubes?ic?.
2011.
Bilingual lexi-con extraction from comparable corpora for closelyrelated languages.
In Proceedings of the Inter-national Conference Recent Advances in NaturalLanguage Processing 2011, pages 125?131, Hissar,Bulgaria.
RANLP 2011 Organising Committee.Darja Fis?er and Beno?
?t Sagot.
2008.
Combining mul-tiple resources to build reliable wordnets.
In TSD2008 - Text Speech and Dialogue, Lecture Notes inComputer Science, Brno, Czech Republic.
Springer.Pascale Fung.
1998.
Machine translation and the in-formation soup, third conference of the associationfor machine translation in the americas, amta ?98,langhorne, pa, usa, october 28-31, 1998, proceed-ings.
In AMTA, volume 1529 of Lecture Notes inComputer Science.
Springer.Gregory Grefenstette.
1994.
Explorations in Auto-matic Thesaurus Discovery.
Kluwer Academic Pub-lishers, Norwell, MA.Amir Hazem and Emmanuel Morin.
2012.
Ica forbilingual lexicon extraction from comparable cor-pora.
In Proceedings of the 5th Workshop on Build-ing and Using Comparable Corpora (BUCC), Istan-bul, Turkey.Hiroyuki Kaji.
2003.
Word sense acquisition frombilingual comparable corpora.
In HLT-NAACL.Philipp Koehn and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
InIn Proceedings of ACL Workshop on UnsupervisedLexical Acquisition, pages 9?16.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofMT Summit X, pages 79?86, Phuket, Thailand.Erwin Marsi and Emiel Krahmer.
2010.
Automaticanalysis of semantic similarity in comparable textthrough syntactic tree matching.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics (Coling 2010), pages 752?760,Beijing, China, August.
Coling 2010 OrganizingCommittee.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving Machine Translation Performance by Ex-ploiting Non-Parallel Corpora.
Computational Lin-guistics, 31(4):477?504.Franz Josef Och and Hermann Ney.
2003.
A System-atic Comparison of Various Statistical AlignmentModels.
Computational Linguistics, 29(1):19?51.Pablo Gamallo Otero.
2007.
Learning bilingual lexi-cons from comparable english and spanish corpora.In Proceedings of MT Summit XI, pages 191?198.Emmanuel Prochasson, Emmanuel Morin, and KyoKageura.
2009.
Anchor points for bilingual lexi-con extraction from small comparable corpora.
InMachine Translation Summit 2009, page 8.Reinhard Rapp.
1999.
Automatic identification ofword translations from unrelated english and germancorpora.
In Proceedings of the 37th Annual Meet-ing of the Association for Computational Linguis-tics, pages 519?526, College Park, Maryland, USA,June.
Association for Computational Linguistics.Xabier Saralegi, In?aki San Vicente, and Antton Gur-rutxaga.
2008.
Automatic extraction of bilingualterms from comparable corpora in a popular sci-ence domain.
In Proceedings of the Building andusing Comparable Corpora workshop, 6th Interna-tional Conference on Language Resources and Eval-uations (LREC), Marrakech, Morocco.Helmut Schmid.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In Proceedingsof the International Conference on New Methodsin Language Processing, pages 44?49, Manchester,UK.9Li Shao and Hwee Tou Ng.
2004.
Mining newword translations from comparable corpora.
In Pro-ceedings of Coling 2004, pages 618?624, Geneva,Switzerland, Aug 23?Aug 27.
COLING.Matthew G. Snover, Bonnie J. Dorr, and Richard M.Schwartz.
2008.
Language and translation modeladaptation using comparable corpora.
In EMNLP,pages 857?866.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Toma Erjavec, and Dan Tufi.
2006.The jrc-acquis: A multilingual aligned parallel cor-pus with 20+ languages.
In In Proceedings of the 5thInternational Conference on Language Resourcesand Evaluation (LREC?2006), pages 2142?2147.Kun Yu and Junichi Tsujii.
2009.
Extracting bilin-gual dictionary from comparable corpora with de-pendency heterogeneity.
In Proceedings of HumanLanguage Technologies: The 2009 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, Companion Vol-ume: Short Papers, pages 121?124, Boulder, Col-orado, June.
Association for Computational Linguis-tics.10
