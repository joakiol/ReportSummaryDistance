Proceedings of SIGDIAL 2009: the 10th Annual Meeting of the Special Interest Group in Discourse and Dialogue, pages 11?20,Queen Mary University of London, September 2009. c?2009 Association for Computational LinguisticsCan I finish?
Learning when to respond to incremental interpretationresults in interactive dialogueDavid DeVault and Kenji Sagae and David TraumUSC Institute for Creative Technologies13274 Fiji WayMarina del Rey, CA 90292{devault,sagae,traum}@ict.usc.eduAbstractWe investigate novel approaches to re-sponsive overlap behaviors in dialoguesystems, opening possibilities for systemsto interrupt, acknowledge or complete auser?s utterance while it is still in progress.Our specific contributions are a method fordetermining when a system has reached apoint of maximal understanding of an on-going user utterance, and a prototype im-plementation that shows how systems canuse this ability to strategically initiate sys-tem completions of user utterances.
Morebroadly, this framework facilitates the im-plementation of a range of overlap behav-iors that are common in human dialogue,but have been largely absent in dialoguesystems.1 IntroductionHuman spoken dialogue is highly interactive, in-cluding feedback on the speech of others whilethe speech is progressing (so-called ?backchan-nels?
(Yngve, 1970)), monitoring of addresseesand other listener feedback (Nakano et al, 2003),fluent turn-taking with little or no delays (Sacks etal., 1974), and overlaps of various sorts, includingcollaborative completions, repetitions and othergrounding moves, and interruptions.
Interrup-tions can be either to advance the new speaker?sgoals (which may not be related to interpreting theother?s speech) or in order to prevent the speakerfrom finishing, which again can be for various rea-sons.
Few of these behaviors can be replicated bycurrent spoken dialogue systems.
Most of thesebehaviors require first an ability to perform in-cremental interpretation, and second, an ability topredict the final meaning of the utterance.Incremental interpretation enables more rapidresponse, since most of the utterance can be inter-preted before utterance completion (Skantze andSchlangen, 2009).
It also enables giving earlyfeedback (e.g., head nods and shakes, facial ex-pressions, gaze shifts, and verbal backchannels) tosignal how well things are being perceived, under-stood, and evaluated (Allwood et al, 1992).For some responsive behaviors, one must go be-yond incremental interpretation and predict someaspects of the full utterance before it has beencompleted.
For behaviors such as comply-ing with the evocative function (Allwood, 1995)or intended perlocutionary effect (Sadek, 1991),grounding by demonstrating (Clark and Schaefer,1987), or interrupting to avoid having the utter-ance be completed, one must predict the semanticcontent of the full utterance from a partial prefixfragment.
For other behaviors, such as timing areply to have little or no gap, grounding by sayingthe same thing at the same time (called ?chanting?by Hansen et al (1996)), performing collaborativecompletions (Clark and Wilkes-Gibbs, 1986), orsome corrections, it is important not only to pre-dict the meaning, but also the form of the remain-ing part of the utterance.We have begun to explore these issues in thecontext of the dialogue behavior of virtual human(Rickel and Johnson, 1999) or embodied conver-sational agent (Cassell et al, 2000) characters formultiparty negotiation role-playing (Traum et al,2008b).
In these kinds of systems, human-like be-havior is a goal, since the purpose is to allow a userto practice this kind of dialogue with the virtualhumans in training for real negotiation dialogues.The more realistic the characters?
dialogue behav-ior is, the more kinds of negotiation situations canbe adequately trained for.
We discuss these sys-11tems further in Section 2.In Sagae et al (2009), we presented our first re-sults at prediction of semantic content from partialspeech recognition hypotheses, looking at lengthof the speech hypothesis as a general indicator ofsemantic accuracy in understanding.
We summa-rize this previous work in Section 3.In the current paper, we incorporate additionalfeatures of real-time incremental interpretation todevelop a more nuanced prediction model that canaccurately identify moments of maximal under-standing within individual spoken utterances (Sec-tion 4).
We demonstrate the value of this newability using a prototype implementation that col-laboratively completes user utterances when thesystem becomes confident about how the utter-ance will end (Section 5).
We believe such pre-dictive models will be more broadly useful in im-plementing responsive overlap behaviors such asrapid grounding using completions, confirmationrequests, or paraphrasing, as well as other kinds ofinterruptions and multi-modal displays.
We con-clude and discuss future work in Section 6.2 Domain settingThe case study we present in this paper is takenfrom the SASO-EN scenario (Hartholt et al, 2008;Traum et al, 2008b).
This scenario is designedto allow a trainee to practice multi-party negoti-ation skills by engaging in face to face negotia-tion with virtual humans.
The scenario involvesa negotiation about the possible re-location of amedical clinic in an Iraqi village.
A human traineeplays the role of a US Army captain, and there aretwo virtual humans that he negotiates with: DoctorPerez, the head of the NGO clinic, and a local vil-lage elder, al-Hassan.
The doctor?s main objectiveis to treat patients.
The elder?s main objective is tosupport his village.
The captain?s main objectiveis to move the clinic out of the marketplace, ide-ally to the US base.
Figure 1 shows the doctor andelder in the midst of a negotiation, from the per-spective of the trainee.
Figure A-1 in the appendixshows a sample dialogue from this domain.The system has a fairly typical set of pro-cessing components for virtual humans or dia-logue systems, including ASR (mapping speechto words), NLU (mapping from words to semanticframes), dialogue interpretation and management(handling context, dialogue acts, reference and de-ciding what content to express), NLG (mappingFigure 1: SASO-EN negotiation in the cafe: Dr.Perez (left) looking at Elder al-Hassan.266666664mood : declarativesem :2666664type : eventagent : captain?
kirkevent : delivertheme : power ?
generatormodal :?possibility : can?speech?
act :?type : offer?3777775377777775Figure 2: AVM utterance representation.frames to words), non-verbal generation, and syn-thesis and realization.
The doctor and elder usethe same ASR and NLU components, but have dif-ferent modules for the other processing, includingdifferent models of context and goals, and differ-ent output generators.
In this paper, we will oftenrefer to the characters with various terms, includ-ing ?virtual humans?, ?agents?, or ?the system?.In this paper, we are focusing on the NLUcomponent, looking at incremental interpretationbased on partial speech recognition results, andthe potential for using this information to changethe dialogue strategy where warranted, and pro-vide responses before waiting for the final speechresult.
The NLU output representation is anattribute-value matrix (AVM), where the attributesand values represent semantic information thatis linked to a domain-specific ontology and taskmodel (Hartholt et al, 2008).
Figure 2 shows anexample representation, for an utterance such as?we can provide you with power generators?.
TheAVMs are linearized, using a path-value notation,as shown in Figure 3.To develop and test the new incremen-tal/prediction models, we are using a corpus of12<s>.mood declarative<s>.sem.type event<s>.sem.agent captain-kirk<s>.sem.event deliver<s>.sem.theme power-generator<s>.sem.modal.possibility can<s>.sem.speechact.type offerFigure 3: Example NLU frame.utterances collected from people playing the roleof captain and negotiating with the virtual doctorand elder.
In contrast with Figure A-1, whichis a dialogue with one of the system designerswho knows the domain well, dialogues with naiveusers are generally longer, and often have a fairlyhigh word error rate (average 0.54), with manyout of domain utterances.
The system is robust tothese kinds of problems, both in terms of the NLUapproach (Leuski and Traum, 2008; Sagae et al,2009) as well as the dialogue strategies (Traumet al, 2008a).
This is accomplished in part byapproximating the meaning of utterances.
Forexample, the frame in Figure 3 is also returned foran utterance of we are prepared to give you guysgenerators for electricity downtown as well as theASR output for this utterance, we up apparentlygive you guys generators for a letter city don town.3 Predicting interpretations from partialrecognition hypothesesOur NLU module, mxNLU (Sagae et al, 2009), isbased on maximum entropy classification (Bergeret al, 1996), where we treat entire individualframes as classes, and extract input features fromASR.
The training data for mxNLU is a corpusof approximately 3,500 utterances, each annotatedwith the appropriate frame.
These utterances werecollected from user sessions with the system, andthe corresponding frames were assigned manually.Out-of-domain utterances (about 15% of all utter-ances in our corpus) could not be mapped to con-cepts in our ontology and task model, and wereassigned a ?garbage?
frame.
For each utterancein our corpus, we have both a manual transcrip-tion and the output of ASR, although only ASRis used by mxNLU (both at training and at run-time).
Each training instance for mxNLU consistsof a frame, paired with a set of features that rep-resent the ASR output for user utterances.
Thespecific features used by the classifier are: eachword in the input string (bag-of-words representa-tion of the input), each bigram (pairs of consec-utive words), each pair of any two words in theinput, and the number of words in the input string.In the 3,500-utterance training set, there are 136unique frames (135 that correspond to the seman-tics of different utterances in the domain, plus oneframe for out-of-domain utterances).1 The NLUtask is then framed as a multiclass classificationapproach with 136 classes, and about 3,500 train-ing examples.Although mxNLU produces entire frames asoutput, we evaluate NLU performance by look-ing at precision and recall of the attribute-valuepairs (or frame elements) that compose frames.Precision represents the portion of frame elementsproduced by mxNLU that were correct, and re-call represents the portion of frame elements inthe gold-standard annotations that were proposedby mxNLU.
By using precision and recall offrame elements, we take into account that certainframes are more similar than others and also al-low more meaningful comparative evaluation withNLU modules that construct a frame from sub-elements or for cases when the actual frame is notin the training set.
The precision and recall offrame elements produced by mxNLU using com-plete ASR output are 0.78 and 0.74, respectively,for an F-score (harmonic mean of precision andrecall) of 0.76.3.1 NLU with partial ASR resultsThe simplest way to perform NLU of partial ASRresults is simply to process the partial utterancesusing the NLU module trained on complete ASRoutput.
However, better results may be obtainedby training separate NLU models for analysis ofpartial utterances of different lengths.
To trainthese separate NLU models, we first ran the au-dio of the utterances in the training data throughour ASR module, recording all partial results foreach utterance.
Then, to train a model to ana-lyze partial utterances containing N words, weused only partial utterances in the training set con-taining N words (unless the entire utterance con-tained less than N words, in which case we sim-ply used the complete utterance).
In some cases,multiple partial ASR results for a single utterance1In a separate development set of 350 utterances, anno-tated in the same way as the training set, we found no framesthat had not appeared in the training set.13010203040506070801 2 3 4 5 6 7 8 9 10 allLength n (words)F-scoreTrained on all dataTrained on partials up tolength nTrained on partials up tolength n + contextFigure 4: F-score for three NLU models on partialASR results up to N words.contained the same number of words, and we usedthe last partial result with the appropriate numberof words.2 We trained ten separate partial NLUmodels for N varying from one to ten.Figure 4 shows the F-score for frames obtainedby processing partial ASR results up to length Nusing three variants of mxNLU.
The dashed line isour baseline NLU model, trained on complete ut-terances only, and the solid line shows the resultsobtained with length-specific NLU models.
Thedotted line shows results for length-specific mod-els that also use features that capture aspects of di-alogue context.
In these experiments, we used uni-gram and bigram word features extracted from themost recent system utterance to represent context,but found that these context features did not im-prove NLU performance.
Our final NLU approachfor partial ASR hypotheses is then to train separatemodels for specific lengths, using hypotheses ofthat length during training (solid line in figure 4).4 How well is the system understanding?In this section, we present a strategy that usesmachine learning to more closely characterize theperformance of a maximum entropy based incre-mental NLU module, such as the mxNLU mod-ule described in Section 3.
Our aim is to iden-tify strategic points in time, as a specific utteranceis occurring, when the system might react withconfidence that the interpretation will not signif-2At run-time, this can be closely approximated by takingthe partial utterance immediately preceding the first partialutterance of length N + 1.Utterance time (ms)NLU F?score(empty)(empty)allelderelder doyouelder toyoudelder doyouagreeelder doyouagree toelder doyouagree to movetheelder doyouagree to movetheelder doyouagree to movetheclinic toelder doyouagree to movetheclinic downelder doyouagree to movetheclinic downtownelder doyouagree to movetheclinic downtown200 400 600 800 10001200140016001800200022002400260028000.00.10.20.30.40.50.60.70.80.9Partial ASR resultFigure 5: Incremental interpretation of a user ut-terance.icantly improve during the rest of the utterance.This reaction could take several forms, includingproviding feedback, or, as described in Section 5an agent might use this information to opportunis-tically choose to initiate a completion of a user?sutterance.4.1 Motivating exampleFigure 5 illustrates the incremental output ofmxNLU as a user asks, elder do you agree to movethe clinic downtown?
Our ASR processes cap-tured audio in 200ms chunks.
The figure showsthe partial ASR results after the ASR has pro-cessed each 200ms of audio, along with the F-14score achieved by mxNLU on each of these par-tials.
Note that the NLU F-score fluctuates some-what as the ASR revises its incremental hypothe-ses about the user utterance, but generally in-creases over time.For the purpose of initiating an overlapping re-sponse to a user utterance such as this one, theagent needs to be able (in the right circumstances)to make an assessment that it has already under-stood the utterance ?well enough?, based on thepartial ASR results that are currently available.
Wehave implemented a specific approach to this as-sessment which views an utterance as understood?well enough?
if the agent would not understandthe utterance any better than it currently does evenif it were to wait for the user to finish their utter-ance (and for the ASR to finish interpreting thecomplete utterance).Concretely, Figure 5 shows that after the entire2800ms utterance has been processed by the ASR,mxNLU achieves an F-score of 0.91.
However,in fact, mxNLU already achieves this maximal F-score at the moment it interprets the partial ASRresult elder do you agree to move the at 1800ms.The agent therefore could, in principle, initiate anoverlapping response at 1800ms without sacrific-ing any accuracy in its understanding of the user?sutterance.Of course the agent does not automatically re-alize that it has achieved a maximal F-score at1800ms.
To enable the agent to make this assess-ment, we have trained a classifier, which we callMAXF, that can be invoked for any specific par-tial ASR result, and which uses various features ofthe ASR result and the current mxNLU output toestimate whether the NLU F-score for the currentpartial ASR result is at least as high as the mxNLUF-score would be if the agent were to wait for theentire utterance.4.2 Machine learning setupTo facilitate the construction of our MAXF clas-sifier, we identified a range of potentially usefulfeatures that the agent could use at run-time to as-sess its confidence in mxNLU?s output for a givenpartial ASR result.
These features are exempli-fied in the appendix in Figure A-2, and include:K, the number of partial results that have been re-ceived from the ASR; N , the length (in words) ofthe current partial ASR result; Entropy, the en-tropy in the probability distribution mxNLU as-signs to alternative output frames (lower entropycorresponds to a more focused distribution); Pmax,the probability mxNLU assigns to the most prob-able output frame; NLU, the most probable outputframe (represented for convenience as fI , whereI is an integer index corresponding to a specificcomplete frame).
We also define MAXF (GOLD),a boolean value giving the ground truth aboutwhether mxNLU?s F-score for this partial is atleast as high as mxNLU?s F-score for the final par-tial for the same utterance.
In the example, notethat MAXF (GOLD) is true for each partial wheremxNLU?s F-score (F (K)) is ?
0.91, the valueachieved for the final partial (elder do you agree tomove the clinic downtown).
Of course, the actualF-score F (K) is not available at run-time, and socannot serve as an input feature for the classifier.Our general aim, then, is to train a classifier,MAXF, whose output predicts the value of MAXF(GOLD) as a function of the input features.
Tocreate a data set for training and evaluating thisclassifier, we observed and recorded the values ofthese features for the 6068 partial ASR results ina corpus of ASR output for 449 actual user utter-ances.3We chose to train a decision tree using Weka?sJ48 training algorithm (Witten and Frank, 2005).4To assess the trained model?s performance, we car-ried out a 10-fold cross-validation on our data set.5We present our results in the next section.4.3 ResultsWe will present results for a trained decisiontree model that reflects a specific precision/recalltradeoff.
In particular, given our aim to enablean agent to sometimes initiate overlapping speech,while minimizing the chance of making a wrongassumption about the user?s meaning, we selecteda model with high precision at the expense oflower recall.
Various precision/recall tradeoffs arepossible in this framework; the choice of a spe-cific tradeoff is likely to be system and domain-dependent and motivated by specific design goals.We evaluate our model using several featureswhich are exemplified in the appendix in Fig-ure A-3.
These include MAXF (PREDICTED),the trained MAXF classifier?s output (TRUE or3This corpus was not part of the training data for mxNLU.4Of course, other classification models could be used.5All the partial ASR results for a given utterance wereconstrained to lie within the same fold, to avoid training andtesting on the same utterance.15FALSE) for each partial; KMAXF, the first par-tial number for which MAXF (PREDICTED) isTRUE; ?F (K) = F (K) ?
F (Kfinal), the ?loss?in F-score associated with interpreting partial Krather than the final partialKfinal for the utterance;T (K), the remaining length (in seconds) in theuser utterance at each partial.We begin with a high level summary of thetrained MAXF model?s performance, before dis-cussing more specific impacts of interest in the di-alogue system.
We found that our trained modelpredicts that MAXF = TRUE for at least onepartial in 79.2% of the utterances in our cor-pus.
For the remaining utterances, the trainedmodel predicts MAXF = FALSE for all partials.The precision/recall/F-score of the trained MAXFmodel are 0.88/0.52/0.65 respectively.
The highprecision means that 88% of the time that themodel predicts that F-score is maximized at a spe-cific partial, it really is.
On the other hand, thelower recall means that only 52% of the time thatF-score is in fact maximized at a given partial doesthe model predict that it is.For the 79.2% of utterances for which thetrained model predicts MAXF = TRUE at somepoint, Figure 6 shows the amount of time in sec-onds, T (KMAXF), that remains in the user utter-ance at the time partialKMAXF becomes availablefrom the ASR.
The mean value is 1.6 seconds; asthe figure shows, the time remaining varies from 0to nearly 8 seconds per utterance.
This representsa substantial amount of time that an agent coulduse strategically, for example by immediately ini-tiating overlapping speech (perhaps in an attemptto improve communication efficiency), or by ex-ploiting this time to plan an optimal response tothe user?s utterance.However, it is also important to understand thecost associated with interpreting partial KMAXFrather than waiting to interpret the final ASR resultKfinal for the utterance.
We therefore analyzedthe distribution in ?F (KMAXF) = F (KMAXF)?F (Kfinal).
This value is at least 0.0 if mxNLU?soutput for partial KMAXF is no worse than its out-put for Kfinal (as intended).
The distribution isgiven in Figure 7.
As the figure shows, 62.35% ofthe time (the median case), there is no differencein F-score associated with interpreting KMAXFrather than Kfinal.
10.67% of the time, there isa loss of -1, which corresponds to a completelyincorrect frame at KMAXF but a completely cor-Utterance time remaining (seconds)Frequency0 2 4 6 80102030Figure 6: Distribution of T (KMAXF).
?F (KMAXF) range Percent ofutterances-1 10.67%(?1, 0) 17.13%0 62.35%(0, 1) 7.30%1 2.52%mean(?F (KMAXF)) -0.1484median(?F (KMAXF)) 0.0000Figure 7: The distribution in ?F (KMAXF), the?loss?
associated with interpreting partial KMAXFrather than Kfinal.rect frame at Kfinal.
The converse also happens2.52% of the time: mxNLU?s output frame is com-pletely correct at the early partial but completelyincorrect at the final partial.
The remaining casesare mixed.
While the median is no change in F-score, the mean case is a loss in F-score of -0.1484.This is the mean penalty in NLU performance thatcould be paid in exchange for the potential gain incommunication efficiency suggested by Figure 6.5 Prototype implementationTo illustrate one use of the techniques described inthe previous sections, we have implemented a pro-totype module that performs user utterance com-pletion.
This allows an agent to jump in during auser?s utterance, and say a completion of the utter-ance before it is finished, at a point when the agent16thinks it understands what the user means.
Thistype of completion is often encountered in human-human dialogue, and may be used, for example,for grounding or for bringing the other party?s turnto a conclusion.We have equipped one of our virtual humans,Doctor Perez, with an ability to perform comple-tions as follows.
The first step is for the agent torecognize when it understands what the user wantsto say.
As discussed in Sections 3 and 4, this oftenhappens before the user has completed the utter-ance.
NLU is performed on partial ASR hypothe-ses as they become available, and MAXF decideswhether the agent?s understanding of the currentpartial hypothesis is likely to improve given moretime.
Once MAXF indicates that the agent?s un-derstanding is likely to be already maximized forthe utterance, we take the current partial ASR hy-pothesis and attempt to generate text to complete itin a way that is fluent and agrees with the meaningof the utterance the user has in mind.The generation of the surface text for comple-tions takes advantage of the manual transcriptionsin the corpus of utterances used to train the NLUmodule.
For each frame that the agent under-stands, our training set contains several user utter-ances that correspond to the meaning in that frame.At the point where the agent is ready to formu-late a completion, mxNLU has already predicteda frame for the user?s utterance (even though it isstill incomplete).
We then consider only the setof known utterances that correspond to that frameas possible sources of completions.
As a simpledistance metric, we compute the word error rate(WER) between the current partial hypothesis forthe user?s utterance and a prefix of each of theseknown utterances.
In our prototype, these prefixeshave the same length as the current partial ASRhypothesis.
We then select the utterance whoseprefix has the lowest WER against the current par-tial ASR hypothesis.
As a final step, we look inthe prefix of our selected utterance for the last oc-currence of the last word in the partial ASR, andif such a word is found, we take the remainder ofthe utterance as the agent?s completion.
Consid-ering only the set of utterances that correspond tothe frame predicted by mxNLUmakes it likely thatthe completion will have the appropriate meaning.Since the completion is a suffix of a transcript of aprevious user utterance, and this suffix follows thelast word uttered by the user, it is likely to form afluent completion of the user?s partial utterance.For 356 utterances in our development set, theprocedure above generated completions contain-ing at least one word for 177 utterances.
We listbelow four representative examples of user utter-ance completions generated by Doctor Perez:1.
Partial utterance: we need toPredicted completion: move your clinicActual user completion: move the clinic2.
Partial utterance: I have ordersPredicted completion: to move you and thisclinicActual user completion: to help you in mov-ing the clinic to a new location3.
Partial utterance: the marketPredicted completion: is not safeActual user completion: is not a safer loca-tion4.
Partial utterance: we can alsoPredicted completion: give you medical sup-pliesActual user completion: build you a wellAlthough agent completions for user utterancesare often acceptably accurate (examples 1-3), oc-casionally they are incorrect (example 4).
Thefrequency of appropriate completions depends onthe accuracy of mxNLU and the chosen preci-sion/recall tradeoff in MAXF.
Finally, althoughthe agent has the ability to generate these com-pletions, clearly it should not complete the user?sutterance at every opportunity.
Determining a pol-icy that results in natural behavior with respect tothe frequency of completions for different types ofagents is a topic under current investigation.6 Summary and future workWe have presented a framework for interpretationof partial ASR hypotheses of user utterances, andhigh-precision identification of points within userutterances where the system already understandsthe intended meaning.
Our initial implementa-tion of an utterance completion ability for a vir-tual human serves to illustrate the capabilities ofthis framework, but only scratches the surface ofthe new range of dialogue behaviors and strategiesit allows.Immediate future work includes the design ofpolicies for completions and interruptions that re-17sult in natural conversational behavior.
Other ap-plications of this work include the generation ofparaphrases that can be used for grounding, in ad-dition to extra-linguistic behavior during user ut-terances, such as head nods and head shakes.AcknowledgmentsThe project or effort described here has been spon-sored by the U.S. Army Research, Development,and Engineering Command (RDECOM).
State-ments and opinions expressed do not necessarilyreflect the position or the policy of the UnitedStates Government, and no official endorsementshould be inferred.
We would also like to thankAnton Leuski for facilitating the use of incremen-tal speech results, and David Schlangen and theICT dialogue group, for helpful discussions.ReferencesJens Allwood, Joakim Nivre, and Elisabeth Ahlsen.1992.
On the semantics and pragmatics of linguisticfeedback.
Journal of Semantics, 9.Jens Allwood.
1995.
An activity based approach topragmatics.
Technical Report (GPTL) 75, Gothen-burg Papers in Theoretical Linguistics, University ofG?teborg.Adam L. Berger, Stephen D. Della Pietra, and VincentJ.
D. Della Pietra.
1996.
A maximum entropy ap-proach to natural language processing.
Computa-tional Linguistics, 22(1):39?71.Justine Cassell, Joseph Sullivan, Scott Prevost, andElizabeth Churchill, editors.
2000.
Embodied Con-versational Agents.
MIT Press, Cambridge, MA.Herbert H. Clark and Edward F. Schaefer.
1987.
Col-laborating on contributions to conversation.
Lan-guage and Cognitive Processes, 2:1?23.Herbert H. Clark and DeannaWilkes-Gibbs.
1986.
Re-ferring as a collaborative process.
Cognition, 22:1?39.
Also appears as Chapter 4 in (Clark, 1992).Herbert H. Clark.
1992.
Arenas of Language Use.University of Chicago Press.B.
Hansen, D. Novick, and S. Sutton.
1996.
Preventionand repair of breakdowns in a simple task domain.In Proceedings of the AAAI-96 Workshop on De-tecting, Repairing, and Preventing Human-MachineMiscommunication, pages 5?12.A.
Hartholt, T. Russ, D. Traum, E. Hovy, and S. Robin-son.
2008.
A common ground for virtual humans:Using an ontology in a natural language orientedvirtual human architecture.
In Language Resourcesand Evaluation Conference (LREC), May.A.
Leuski and D. Traum.
2008.
A statistical approachfor text processing in virtual humans.
In 26th ArmyScience Conference.Yukiko I. Nakano, Gabe Reinstein, Tom Stocky, andJustine Cassell.
2003.
Towards a model of face-to-face grounding.
In ACL, pages 553?561.Jeff Rickel and W. Lewis Johnson.
1999.
Virtual hu-mans for team training in virtual reality.
In Proceed-ings of the Ninth International Conference on Artifi-cial Intelligence in Education, pages 578?585.
IOSPress.H.
Sacks, E. A. Schegloff, and G. Jefferson.
1974.A simplest systematics for the organization of turn-taking for conversation.
Language, 50:696?735.M.
D. Sadek.
1991.
Dialogue acts are rationalplans.
In Proceedings of the ESCA/ETR workshopon multi-modal dialogue.K.
Sagae, G. Christian, D. DeVault, and D. R. Traum.2009.
Towards natural language understanding ofpartial speech recognition results in dialogue sys-tems.
In Short Paper Proceedings of NAACL HLT.Gabriel Skantze and David Schlangen.
2009.
Incre-mental dialogue processing in a micro-domain.
InProceedings of EACL 2009, pages 745?753.D.
Traum, W. Swartout, J. Gratch, and S. Marsella.2008a.
A virtual human dialogue model for non-team interaction.
In L. Dybkjaer and W. Minker,editors, Recent Trends in Discourse and Dialogue.Springer.D.
R. Traum, S. Marsella, J. Gratch, J. Lee, andA.
Hartholt.
2008b.
Multi-party, multi-issue, multi-strategy negotiation for multi-modal virtual agents.In Helmut Prendinger, James C. Lester, and MitsuruIshizuka, editors, IVA, volume 5208 of Lecture Notesin Computer Science, pages 117?130.
Springer.I.
H. Witten and E. Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques.
Mor-gan Kaufmann.Victor H. Yngve.
1970.
On getting a word in edgewise.In Papers from the Sixth Regional Meeting, pages567?78.
Chicago Linguistic Society.18A Appendix1 C Hello Doctor Perez.2 D Hello captain.3 E Hello captain.4 C Thank you for meeting me.5 E How may I help you?6 C I have orders to move this clinic to a camp near the US base.7 E We have many matters to attend to.8 C I understand, but it is imperative that we move the clinic out of this area.9 E This town needs a clinic.10 D We can?t take sides.11 C Would you be willing to move downtown?12 E We would need to improve water access in the downtown area, captain.13 C We can dig a well for you.14 D Captain, we need medical supplies in order to run the clinic downtown.15 C We can deliver medical supplies downtown, Doctor.16 E We need to address the lack of power downtown.17 C We can provide you with power generators.18 E Very well captain, I agree to have the clinic downtown.19 E Doctor, I think you should run the clinic downtown.20 D Elder, the clinic downtown should be in an acceptable condition beforewe move.21 E I can renovate the downtown clinic, Doctor.22 D OK, I agree to run the clinic downtown, captain.23 C Excellent.24 D I must go now.25 E I must attend to other matters.26 C Goodbye.26 D Goodbye.26 E Farewell, sir.Figure A-1: Successful negotiation dialogue between C, a captain (human trainee), D, a doctor (virtualhuman), and E, a village elder (virtual human).19MAXF model training featuresPartial ASR result F (K) K N Entropy Pmax NLU MAXF (GOLD)(empty) 0.00 1 0 2.96 0.48 f82 FALSE(empty) 0.00 2 0 2.96 0.48 f82 FALSEall 0.00 3 1 0.82 0.76 f72 FALSEelder 0.00 4 1 0.08 0.98 f39 FALSEelder do you 0.83 5 3 1.50 0.40 f68 FALSEelder to you d 0.50 6 3 1.31 0.75 f69 FALSEelder do you agree 0.83 7 4 1.84 0.35 f68 FALSEelder do you agree to 0.83 8 5 1.40 0.61 f68 FALSEelder do you agree to move the 0.91 9 7 0.94 0.49 f10 TRUEelder do you agree to move the 0.91 10 7 0.94 0.49 f10 TRUEelder do you agree to move the clinic to 0.83 11 9 1.10 0.58 f68 FALSEelder do you agree to move the clinic down 0.83 12 9 1.14 0.66 f68 FALSEelder do you agree to move the clinic downtown 0.91 13 9 0.50 0.89 f10 TRUEelder do you agree to move the clinic downtown 0.91 14 9 0.50 0.89 f10 TRUEFigure A-2: Features used to train the MAXF model.MAXF model evaluation featuresK F (K) ?F (K) T (K) MAXF (PREDICTED)1 0.00 -0.91 2.6 FALSE2 0.00 -0.91 2.4 FALSE3 0.00 -0.91 2.2 FALSE4 0.00 -0.91 2.0 FALSE5 0.83 -0.08 1.8 FALSE6 0.50 -0.41 1.6 FALSE7 0.83 -0.08 1.4 FALSE8 0.83 -0.08 1.2 FALSE9 (= KMAXF) 0.91 0.00 (=?F (KMAXF)) 1.0 TRUE10 0.91 0.00 0.8 TRUE11 0.83 -0.08 0.6 FALSE12 0.83 -0.08 0.4 FALSE13 0.91 0.00 0.2 TRUE14 0.91 0.00 0.0 TRUEFigure A-3: Features used to evaluate the MAXF model.20
