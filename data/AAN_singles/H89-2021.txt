Evaluating spoken language interactionAlexander I. Rudnicky, Michelle Sakamoto, and Joseph H. PolifroniSchool of Computer Science, Carnegie Mellon UniversityPittsburgh, PA 15213AbstractTo study the spoken language interface in the context of acomplex problem-solving task, a group of users were askedto perform a spreadsheet task, alternating voice andkeyboard input.
A total of 40 tasks were performed by eachparticipant, the first thirty in a group (over several days), theremaining ones a month later.
The voice spreadsheetprogram used in this study was extensively instrumented toprovide detailed information about the components of theinteraction.
These data, as well as analysis of theparticipants's utterances and recognizer output, provide afairly detailed picture of spoken language interaction.Although task completion by voice took longer than bykeyboard, analysis hows that users would be able to per-form the spreadsheet task faster by voice, if two key criteriacould be met: recognition occurs in real-time, and the errorrate is sufficiently low.
This initial experience with a spokenlanguage system also allows us to identify several metrics,beyond those traditionally associated with speech recog-nition, that can be used to characterize system performance.IntroductionThe ability to communicate by speech is known toenhance the quality of communication, asreflected inshorter problem-solving times and general user satis-faction \[2\].
Recent advances in speech recognitiontechnology \[4\] have made it possible to build "spokenlanguage" systems that create the opportunity for inter-acting naturally with computers.
Spoken languagesystems combine a number of desirable properties.Recognition of continuous speech allows users to use anatural speech style.
Speaker independence allowscasual users to easily use the system and eliminatestraining as well as its associated problems (such asdrift).
Large vocabularies make it possible to createhabitable languages for complex applications.
Finally,a natural anguage processing capability allows theuser to express him or herself using familiar locutions.While the recognition technology base that makesspoken language systems possible is rapidly maturing,there is no corresponding understanding of how suchsystems hould be designed or what capabilities userswill expect o have available.
It is intuitively apparentthat speech will be suited for some functions (e.g., dataentry) but unsuited for others (e.g., drawing).
Wewould also expect hat users will be willing to toleratesome level of recognition error, but do not know whatthis is or how it would be affected by the nature of thetask being performed or by the error recovery facilitiesprovided by the system.Meaningful exploration of such issues is difficultwithout some baseline understanding of how humansinteract with a spoken language system.
To providesuch a baseline, we implemented a spoken languagesystem using currently available technology and usedit to study humans performing a series of simple tasks.We chose to work with a spreadsheet program sincethe spreadsheet supports a wide range of activities,from simple data entry to complex problem solving.
Itis also a widely used program, with a large ex-perienced user population to draw on.
We chose toexamine performance over an extended series of tasksbecause we believe that regular use will be charac-teristic of spoken language applications.The voice spreadsheet systemThe voice spreadsheet (henceforth "vsc") consistsof the uNIx-based spreadsheet program sc interfacedto a recognizer embodying the SPHINX technologydescribed in \[4\].
Additional description of vsc isavailable elsewhere \[6\], as is a description of thespreadsheet language \[9\].The recognition component of the voice spreadsheetmakes use of two pieces of special-purpose hardware:a signal processing unit (the USA) and a search ac-celerator BEAM.
See \[1\] for fuller descriptions of theseunits.
The recognition code is embedded in thespreadsheet program, so that the complete system runsas a single process.150Table 1: Comparison of recognizer performance for on-line and read speechword utterancesTest Set utts words accuracy correctReference (read speech)Live Session (complete)Live Session (clean speech)Live Session (read version)9940636636649114861389138993.792.794.994.072.778.985.582.8To train the phonetic models used in the recognizer,we combined several different databases, all recordedat Carnegie Mellon using the same microphone asused for the spreadsheet s udy (a close-talking Sen-nheiser HMD-414).
The training speech consisted of:calculator sentences (1997 utterances), a (general)spreadsheet database (1819 utterances), and a task-specific database for financial data (196 utterances).
Atotal of 4012 utterances was thus included in the train-ing set.
Table 1 provides ome performance data thatcharacterize system performance.The basic recognition performance ("Reference"), astested on speech collected at the same time as thetraining data, is about what might be expected giventhe known performance characteristics of the SPI-mqxsystem (specifically, 94% word accuracy for theperplexity 60 version of the Resource Managementtask).The Table also presents recognition performance forspeech collected in the user study described below("Live Session").
The "complete" version shows sys-tem performance over 4 sessions representing 4 dif-ferent alkers and chosen from about he mid-point ofthe initial 30 task series (details below).
Note that thisset includes utterances that contain various spon-taneous peech phenomena that cannot be handled cor-rectly by the current system.
The "clean speech" setincludes only those utterances that both contain no in-terjected material (e.g., audible non-speech) and thatare grammatical.
Performance on this set is quitegood, and there is no evidence that mere "spontaneity"leads to poorer recognition performance.
We canverify this equivalence more concretely by comparingread and spontaneous speech produced by the sametalkers.
To do this, we asked the four participantswhose speech comprised the spontaneous test sets toreturn and record read versions of their spontaneousutterances, using scripts taken from our transcriptions.As can be seen in the Table, performance is com-parable for read and live speechl.Given that this pattern of results can be shown togeneralize to other tasks (and there is no reason tobelieve that they would not), the implications of thisexperiment are highly significant: A system trained onread speech will not substantially degrade in accuracywhen presented with spontaneous speech provided thatcertain other characteristics, uch as speech rate, willbe comparable.
Note that this only applies to thoseutterances that are comparable to read speech insofaras they are grammatical and contain no extraneousacoustic events.
The system will still need to deal withthese phenomena.
This result is encouraging for thoseapproaches to spontaneous speech \[10\] that deal withsuch speech in terms of accounting for extraneousevents and interpreting agrammatical utterances.
Ifthese problems can be solved in a satisfactory manner,then we can comfortably expect spontaneous spokenlanguage system performance tobe comparable tosys-tem performance evaluated on read speech.A study of spoken language system usageTo understand how users approach a voice-drivensystem and how they develop strategies for dealingwith this type of interface, we had a group of usersperform a series of more or less comparable task overan extended period of time and monitored various1The slightly better performance with Live speech might seemcounter-intuitive.
Examination of specific errors in the Read vers ionindicates that one of the speakers read her raated~l at a distinctlyslower pace than she spoke it spontaneously (we estimate 34%slower).
The bulk of the excess errors can be accounted for by thisinterpretation.
For example, many of the errors are splits, charac-terist ic of slow speech.151aspects of system and user performance over thisperiod.MethodWe were interested in not only how a casual userapproaches a spoken language system, but also howhis or her skill in using the system develops over time.Accordingly, we had a total of 8 participants completea series of 40 spreadsheet tasks.The task chosen for this study was the entry of per-sonal financial data from written descriptions ofvarious items in a fictitious person's monthly finances.An attempt was made to make each version of the taskcomparable in the amount of information it containedand in the number of complex arithmetic operationsrequired.
On the average, each task required entering38 pieces of financial information, an average of 6 ofthese entries required arithmetic operations such as ad-dition and multiplication.
Movement within theworksheet, although generally following a top to bot-tom order, skipped around, forcing the user to makearbitrary movements, including off-screen movements.Users were presented with preformatted worksheetscontaining appropriate headings for each of the itemsthey would have to enter.
In addition, each relevantcell location was given a label that would allow theuser to access it using symbolic movement instructions(as defined in \[9\]).The information to be entered was presented onseparate sheets of paper, one entry to a sheet, con-mined in a binder positioned to the side of the worksta-tion.
This was done to insure that all users dealt withthe information i  a sequential manner and would fol-low a predetermined movement sequence within theworksheet.
To aid the user, the bottom of each sheetgave the category heading for the information to beentered and, if existing, a symbolic label for the cellinto which the information was to be entered.PROCEDURE AND DESIGN.
All participants per-formed 40 tasks.
The first 30 tasks were completed ina block, over several days.
The last ten were com-pleted after an interval of about one month.
The pur-pose of the latter was to determine the extent o whichusers remembered their initial extended experiencewith the voice spreadsheet and to what degree thisretest would reflect the performance gains realizedover the course of the original block of sessions.
Sincewe were interested in studying a spoken language sys-tem in an environment that realistically reflects the set-tings in which such a system might eventually be used,we made no special attempt to locate the experiment ina benign environment or to control the existing one.The workstation was located in an open laboratory andwas not surrounded by any special enclosure.At the beginning of each session, each participantwas given a standard-format typing test to determinetheir facility with the keyboard.
The typing testrevealed two categories of participant, ouch typists (3people) with a mean typing rate of 63 words perminute (wpm) and "hunt and peck" typists (5 people),with a mean typing rate of 31 wpm.
Task modality(whether speech or typing) alternated over the courseof the experiment, each successive task being carriedout in a different modality.
To control for order andtask-version effects the initial modality and the se-quence of tasks (first-to-last vs  last-to-firs0 was variedto produce all possible combinations (four).
Twopeople were assigned to each combination.The participants were informally solicited from theuniversity community through personal contact andbulletin board announcements.
There were 3 womenand 5 men, ranging in age from 18 to 26 (mean of 22).With the exception of one person who was ofEnglish/Korean origin, all participants were nativespeakers of English.
All had previous experience withspreadsheets, anaverage of 2.3 years (range 0.75 to 5),though current usage ranged from daily to "severaltimes a year".
None of the participants reported anyprevious experience with speech recognition systems(though one had previously seen a SPHINX demonstra-tion).ResultsThe data collected in this study consisted of detailedtimings of the various stages of interaction as well asthe actual speech uttered over the course of systeminteraction.
The analyses presented in this section arebased on the first 30 sessions completed by the 8 par-ticipants.152Recognition performance and languagehabitabilityTo analyze recognizer performance we captured andstored each utterance spoken as well as the cor-responding recognition string produced by the system.All utterances were listened to and an exact lexicaltranscription produced.
The transcription conventionsare described more fully in \[8\], but suffice it to notethat in addition to task-relevant speech, we coded avariety of spontaneous speech phenomena, includingspeech and non-speech interjections, as well as inter-rupted words and similar phenomena.The analyses reported here are based on a total of12507 recorded and transcribed utterances, comprising43901 tokens.
We can use these data to answer avariety of questions about speech produced in a com-plex problem-solving environment.
Recognition per-formance data are presented in Figure 1.
The valuesplotted represent he error rate averaged across alleight subjects.Figure 1: Mean utterance accuracy across tasks~- 50~o3(10?
EXACT SENTENCE ERROR RATE IA SEMANTIC SENTENCE ERROR RATE IGRAMMATICAL ERROR RATE I2 3 4 7 12 20SCRIPT NUMBERThe top line in Figure 1 shows exact utterance ac-curacy, calculated over all utterances in the corpus,including system firings for extraneous noise andabandoned (i.e., user interrupted) utterances.
It doesnot include begin-end etector failures (which producea zero-length utterance), of which there were on theaverage 10% per session.
Exact accuracy correspondsto utterance accuracy as conventionally reported forspeech recognition systems using the NBS scoring al-gorithm \[5\].
The general trend of recognition perfor-mance over time is improvement, though the improve-ment appears to be fairly gradual.
The improvementindicates that users are sufficiently aware of whatmight improve system performance tomodify their be-havior accordingly.
On the other hand, the amount ofcontrol they have over it appears to be limited.The next line down shows semantic accuracy, cal-culated by determining, for each utterance, no matterwhat its content, whether the correct action was takenby the system 2.
Semantic accuracy, relative to exactaccuracy, represents he added performance that can berealized by the parsing and understanding componentsof an SLS.
In the present case, the added performanceresults from the 'silent' influence of the word-pairgrammar which is part of the recognizer.
Thus, gram-matical constraints are enforced not through, say, ex-plicit identification and reanalysis of out-of-languageutterances, but implicitly, through the word-pair gram-mar.
The spread between semantic and exact accuracydefines the contribution of higher-level process and isa parameter that can be used to track the performanceof "higher-lever' components of a spoken languagesystem.The line at the bottom of the graph showsgrammaticality error.
Grammaticality is determinedby first eliminating all non-speech events from thetranscribed corpus then passing these filtered ut-terances through the parsing component of the spread-sheet system.
Grammaticality provides a dynamicmeasure of the coverage provided by the system tasklanguage (on the assumption that the user's task lan-guage volves with experience) and is one indicator ofwhether the language is sufficient for carrying out thetask in question.The grammaticality function can be used to track anumber of system attributes.
For example, its valueover the period that covers the user's initial experiencewith a system indicate the degree to which the im-2For example, the user might say "LET' S GO DOWN FIVE",which lies outside the system language.
Nevertheless, because ofgrammatical constraints, the system might force this utterance into"DOWN FIVE", which happens to be grammatically acceptable andwhich also happens to cany out the desired action.
From the taskpoint of view, this recognition is correct; from the recognition pointof view it is, of course, wrong.153plemented language covers utterances produced by theinexperienced user and provides one measure of howsuccessfully the system designers have anticipated thespeech language that users intuitively select for thetask.
Examined over time, the grammaticality functionindicates the speed with which users modify theirspeech language for the task to reflect he constraintsimposed by the implementation a d how well theymanage to stay within it.
Measurement of gram-maticality after some time away from the system in-dicates how well the task language can be retained andis an indication of its appropriateness for the task.
Webelieve that grammaticality s an important componentof a composite metric for the language habitability ofan SLS and can provide a meaningful basis for com-paring different SLS interfaces to a particularapplication 3.Examining the curves for the present system wefind, unsurprisingly, that vsc is rather primitive in itsability to compensate for poor recognition perfor-mance, as evidenced by how close the semantic ac-curacy line is to the exact accuracy line.
On the otherhand, it appears to cover user language quite well, withonly an average of 2.9% grammaticality error 4.
In alllikelihood, this indicates that users found it quite easyto stay within the confines of the task, which in turnmay not be surprising iven its simplicity.SPONTANEOUS SPEECH PHENOMENA.
When aspoken language system is exposed to speechgenerated in a natural setting a variety of acousticevents appear that contribute to performance degrada-tion.
Spontaneous speech events can be placed intoone of three categories: lexical, extra.lexical, andnon-lexical, depending on whether the item is part ofthe system lexicon, a recognizable word that is not partof the lexicon, or some other event, such as a breathnoise.
These categories, as well as the procedure fortheir transcription, are described in greater detail in\[8\].
Table 2 lists the most common on-lexical eventsencountered in our corpus.
The number of events isgiven, as well as their incidence in terms of words inSSystem habitability, on the other hand, has to be based on acombination of language habitability, robustness with respect ospontaneous speech phenomena, nd system responsiveness.4Bear in mind that this percentage includes intentionalagrammaticality with respect o the task, such as expressions ofannoyance or interaction with other humans.the corpus.
Given the nature of the task:, it is notsurprising to find, for example, that a large number ofpaper rustles intrudes into the speech stream.
Non-lexical events were transcribed in 893 of the 12507utterances used for this analysis (7.14% of all ut-terances).Figure 2 show the proportion of transcribed ut-terances that contain extraneous material (such as theitems in Table 2).
This function was generated bycalculating rammaticality with both non-lexical andextra-lexical tokens included in the transcription.
Asis apparent, he incidence of extraneous events teadilydecreases over sessions.
Users apparently realize theharmful effects of such events and work to eliminatethem (conversely, the user does not appear to haveabsolute control over such events, otherwise thedecrease would have been much steeper).
The top linein the graphs hows utterance error rate, the percent ofutterances that are incorrectly recognizer and thereforelead to an unintended action; it includes errors due toboth the presence of unanticipated vents and to moreconventional failures of recognition.
The similarity inthe shape of the two functions uggests that speechrecognition accuracy is fairly constant across essions,major variations being accounted for by changes inambience (as tracked by the lower curve).Figure 2: Incidence of non-lexical events1~,50zuJo0:=o3C20?
EXACT SENTENCE ERROR RATE I?
GRAMMATICAL ERROR RATE WITH ++ ISCRIPTWhile existing statistical modeling techniques canbe used to deal with the most common events (such aspaper rustles) in a satisfactory manner (as shown by154Table 2: Frequency and incidence of (some) non-lexical spontaneous speech tokens.1.332 585 ++RUSTLE+ 0.009 4 ++PHONE-R ING+0.469 206 ++BREATH+ 0.009 4 ++NOISE+0.098 43 ++MUMBLE+ 0.009 4 ++DOOR-SLAM+0.041 18 ++SNIFF+ 0.009 4 ++CLEARING-THROAT+0.029 13 ++BACKGROUND-NOISE+ 0.009 4 ++BACKGROUND-VOICES+0.025 Ii ++MOUTH-NOISE+ 0.005 2 ++SNEEZE+0.022 i0 ++COUGH+ 0.002 I ++SIGH+0.013 6 ++YAWN+ 0.002 1 ++PING+0.011 5 ++GIGGLE+ 0.002 1 ++BACKGROUND-LAUGH+Note: Thetoken.first column given the percentage and the second column the actual number of tokens for the given non-lexical\[10\]), more general techniques will need to bedeveloped to account for low-frequency or otherwiseunexpected events.
A spoken language system shouldbe capable of accurately identifying novel events anddispose of them in appropriate ways.The time it takes to do thingsOf particular interest in the evaluation of a speechinterface is the potential advantages that speech offersover alternate input modalities, in particular thekeyboard.
On the simplest terms, a demonstration thata given modality provides a time advantage is a stronga priori argument that this modality is more desirablethan another.Q11001000~ 8007OO5OO4OOFigure 3: Total task completion rime$'%ARo LTo understand whether and how speech input 3oopresents an advantage, we examined the times, bothI I I I I Iaggregate and specific, that it took users to perform the 2oo 2 s 4 7 12 20task we gave them.
scmPr NUMBERAGGREGATE TASK TIMES.
The total time it takes toperform a task is a good indication of how effectivelyit can be carried out in a particular fashion.
Figure 3shows the mean total time it took users to perform thespreadsheet tasks.
As can be seen, keyboard entry isfaster.
Moreover, the time taken to perform a task bykeyboard improves steadily over time.
The com-parable speech time, while improving for a time,seems to asymptote a level above that of keyboardinput.
Since the tasks being performed are essentially(and over individuals, exactly) the same, we must inferthat the lack of improvement is due in some fashion tothe nature of the speech interface.The reasons for this become clearer if we examinein greater detail where the time goes.
The presentimplementation incurs substantial mounts of systemoverhead that at least in principle could be eliminatedthrough suitable modifications.
Currently, sizabledelays are introduced by the need to initialize therecognizer (about 200 ms), to log experimental data(about 600 ms), and by the two times real-time perfor-mance of the recognizer.
What would happen if weeliminate this overhead?If we replot the data by subtracting these times, butretaining the time taken to speak an utterance, we findthat the difference between speech and keyboard isreduced, though not eliminated (see Figure 4).
Thisresult underlines the probable importance of designingtightly-coupled spoken language systems for which theexcess time necessary for entering information byspeech as been reduced to a value comparable to that155found for keyboard input.
In a personal workstationenvironment this would essentially have to be nil, andwe believe represents a minimum requirement for suc-cessful speech-based applications that support goal-directed behavior.There is an additional penalty imposed on speech inthe current system--recognition error.
In terms of thetask, the only valid inputs are those for which the ut-terance is correctly recognized.
If an input is incor-rect, it has to be repeated.
We can get an idea of howfast the task could actually be performed ff we dis-count the total task time by the error rate.
That is, if atask is presently carried out in 10 min, but exhibits a25% utterance rror, then the task could actually havebeen carried in 7.5 min, had we been using a systemcapable of providing 100% utterance recognition.Figure 4 compares total task time corrected by thisprocedure.
If we do this, we find that the amount oftime taken to carry out the task by voice is actuallyfaster than by keyboard.Finally, we can ask what level of recognition perfor-mance is necessary for speech to equal keyboard input.Given that the mean task time over 15 sessions forkeyboard is 448 ms and that the mean task time for the"real-time" adjustment is 528 ms, then we can estimatethat a 15% error rate (a halving of the current rate) willproduce equivalent task completion times for speechand keyboard.
We believe that this goal is achievablein the near term.The above speculations are, of course, exercises inarithmetic and cannot take the place of an actualdemonstration.
We are currently working towards thegoals of creating a true real-time implementation ofour system and on improving system accuracy.TIME FOR INDIVIDUAL ACTIONS.
The tasks wehave chosen are very simple in nature and can bedecomposed into a small number of action classes (see\[9\]).
Our detailed logging procedure allows us to ex-amine the times taken to perform different classes ofactions in the spreadsheet task.
In the followinganalysis, we will concentrate on the three classes thatallow the user to perform the two major actions neces-sary for task completion, movement to a cell locationand entry of numeric data.Movement actions.
Examination of the movementdata shows that users adopt very different strategiesFigure 4: Adjusted total task completion time~ 1100 b-- I " VOICE " -7  i I A KEYBOARD /600;.=1""-....- .
.
.
.
.
.
.
_400 " t .
O. ,,0 ~ i ~ L3OO200 I I I !
!
!2 3 4 7 12 20SCRIPT NUMBERfor moving about the spreadsheet, depending onwhether they are using keyboard input or speech input.As Figure 5 shows, when in typing mode users relyheavily on relative motion (the "arrow" keys on theirkeyboard).
In contrast, users use symbolic and ab-solute movements in about the same proportion whenin speech mode.
A detailed iscussion of the reasonsfor this shift are beyond the scope of this paper.Briefly stated, the strategy shift can be traced to thepresence of a system response delay in the voice con-dition.
Delays affect he perceived relative cost of thetwo movement actions, making absolute and symbolicmovements more attractive.
A more thorough presen-tation, with additional experimental data, can be foundin \[7\].Figure 6 shows the total time taken by movementinstructions within each modality.
Surprisingly, voicemovement commands take less overall time thanmovement commands in keyboard mode, at least in-itially.
As the user refines his or her task skills, totalkeyboard movement time overtakes the voice time.Voice time initially also improves, but eventually ap-pears to asymptote, very likely because of a floor im-posed by the combination of system response andrecognition accuracy.
These data appear to support, at156252015100KEYBOARDFigure 5: Movement action counts, by classI -  4Oz8z ~ 35IJJ i, I \ [ \ ]  REL MOVEVOICEMODAUTYthe very least, the assertion that otal movement time iscomparable for the two modalities and that spreadsheetmovement can be carried out with comparable f-ficiency by voice and by keyboard.
Of course, con-temporary workstations make available alternate op-tions for movement.
The hand-operated mouse is oneexample, which might prove to be more efficient forsome classes of movement.
A controlled comparisonof speech and mouse movement would be of great in-terest, but lies beyond the scope of the current study.Figure 6: Total time for movement actions8 4ooj~z_ 350300._125O2OO150IO0 I I I I I !2 3 4 7 12 20SCRIPT NUMBERNumber Entry.
The input time data for numberentry (or more properly numeric expression entry,since the task could require the entry of arithmetic ex-pressions) clearly show that speech is superior in termsof time.
As seen in Figure 7 (which shows the medianinput time for entry commands) the advantage is ap-parent from the beginning and continues to be main-tained over successive r petitions of the task.Figure 7: Median numeric input imeEi.g5 2500E LU I-- z Lt.IVOICE I,~, KEYBOARD I1 2 3 4 7 12 20SCRIPTNUMBERThe advantage for speech entry can be due to anumber of reasons.
First, it may be faster to say anumber than to type it (a digit-string entry experiment\[3\] shows that he break-even point occurs between 3and 5 digits).
Second, when working from paper notes(a probably situation for this task in real life), users donot need to shift their attention from paper to keyboardto screen when speaking a number.
They would haveto do so if they were typing, particularly if they arehunt-and-peck typists.
Data supporting this interpreta-tion can be found in \[3\].Of course, we should not lose sight of the fact thatthe current implementation produces longer total tasktimes for speech than for keyboard and that his systemcannot show an overall advantage for speech input.Nevertheless, it clearly demonstrates that componentoperations can be at least as fast and in some casesfaster than keyboard input.
These characteristics willonly be observed in the complete system when systemresponse and recognition accuracy attain criticallevels.157DiscussionThe results obtained in this study provide a valuableinsight into the potential advantages of spoken lan-guages ystems and allow us to identify those aspectsof system design whose improvement is critical to theusability of such systems.
Furthermore, this study laysout a framework for the evaluation of SLS perfor-mance, identifying a number of useful diagnosticmetrics.System characteristicsAlthough we found that total task time was greaterfor speech input than for keyboard, this was not due toany intrinsic deficit for voice input.
In fact, if weexamine the component actions performed by the user,we find that they could be completed faster by voicethan by typing.
The failure of the speech mode toachieve greater throughput can be attributed to twoshortcomings ofour spoken language system.A time penalty is imposed by our current implemen-tation, which processes peech at about 2 times real-time and incorporates a substantial overhead.
Thepenalty is reflected not only in longer task times, butalso in changes to user strategies.
Fortunately, real-time performance can be achieved with a suitable im-plementation and sufficient hardware resources.
Weare currently reimplementing our system on a multi-processor computer and expect to achieve sub-real-time performance in the near future.While speed is a tractable problem, low accuracy isless so.
We can expect o improve utterance recog-nition on the order of 10% if we properly model ex-traneous events, but even if we do so, recognition per-formance may still be at a level that significantly inter-feres with task performance.
Judging from Figure 4, itmay be sufficient to provide a moderate improvementin recognition accuracy, which together with real-timerecognition would be sufficient to allow a spoken lan-guage system to perform at a level equivalent to akeyboard system.Evaluation methodologyThe present study also provides a strong basis forthe development of exact evaluation techniques forspoken language systems.The results of this study make it appar~mt that ut-terances are the key unit of analysis for SLS perfor-mance evaluation.
The success or failure, of a par-ticular transaction depends on whether the system cor-rectly interprets the user's intention, as expressed bythat utterance.
Utterance misinterpretation impactsone of the critical measures of task efficiency, the timeit takes to complete a task.
Word accuracy, while auseful metric, cannot be used to accurately charac-terize system performance.We have described three utterance-level metrics thatwe believe are necessary for a full characterization fSLS performance.Exact accuracy tracks the performance of thespeech recognition component and reflects both theability to identify words and the ability to deal withcertain classes of extraneous non-lexical events.
Exactaccuracy is therefore a measure of "raw" recognitionpower.Semantic accuracy tracks the performance of thesystem as a whole and is the actual determiner oftransaction success.
The contribution of higher-levelprocessing is defined by the spread between the exactand semantic accuracy curves.
But note that the mar-ginal contribution of such processing is also a functionof exact accuracy.
As the latter improves, the formerwill improve only insofar as it provides an improve-ment over the existing recognition performance.Grammatical accuracy specifies the utterancerejection rate for the parsing component of the system.In the case of the present system, a rejection is simplyany transcription that cannot be parsed.
In the case of amore sophisticated system (for example, one that iscapable of engaging the user in a clarification dialogueor interpreting agrammatical utterances), defininggrammaticality may be more difficult but should noton principle be impossible.
Grammatical accuracyalso reflects the habitability of a system, inasfar as itallows the user to express his or her task-relevant in-tentions in a natural manner.
In any case, trackinggrammatical ccuracy allows the evaluation of howwell the system embodies the language necessary fortask performance by a given user population.
Gram-matical accuracy, measured over time as in the presentstudy can also provide insight into how easy a systemlanguage is to learn and how adequate it is for a givenrange of activities.
Measurements aken after an158elapsed interval, as in the current paradigm, canprovide an indication of how well a user remembersthe language constraints imposed by a SLS and canthus reflect he quality of its design.The metrics presented above can be used to describesystem performance in ways that are useful for under-standing the characteristics of a particular spoken lan-guage system.
As such, they would be of limited in-terest to those not directly involved in spoken lan-guage research.
In a larger arena, SLSs will be com-peting with other interface technologies and the basesfor comparison will be universally applicable medics,such as task completion time and ease of use.
Thechallenge is to build systems that can compete suc-cessfully on those terms.AcknowledgmentsA number of people have contnbuted to the workdescribed in this paper.
We would like to thank RobertBrennan who did the initial implementation f the voicespreadsheet program and Takeema Hoy who produced thebulk of the transcriptions u ed in our performance analyses.The research described in this paper was sponsored by theDefense Advanced Research Projects Agency (DOD), ArpaOrder No.
5167, monitored by SPAWAR under contractN00039-85-C-0163.
The views and conclusions containedin this document are those of the authors and should not beinterpreted as representing the official policies, either ex-pressed or implied, of the Defense Advanced ResearchProjects Agency or the US Government.References1.
Bisiani, R., Anantharaman, T., and Butcher, L. .BEAM: An accelerator for speech recognition.Proceedings of the IEEE International Conference onAcoustics, Speech, and Signal Processing, 1989.2.
Chapanis, A. Interactive Human Communication:Some lessons learned from laboratory experiments.
InShackel, B., Ed., Man-Computer Interaction: HumanFactors Aspects of Computers and People, Sijthoffand Noordhoff, Rockville, Md, 1981, pp.
65-114.3.
Hauptmann, A.H. and Rudnicky, A.I.
A com-parison speech versus typed input.
Submitted for pub-lication.4.
Lee, K.-F. Automatic Speech Recognition: TheDevelopment ofthe SPHINX System.
KluwerAcademic Publishers, Boston, 1989.5.
PaUett, D.S.
Benchmark tests for DARPA Resourcemanagement database performance evaluations.
InProceedings oflCASSP, IEEE, 1989, pp.
536-539.6.
Rudnicky, A.I.
The design of voice-driven i ter-faces.
In Proceedings of the DARPA Workshop onSpoken Language Systems, Morgan Kaufman, 1989,pp.
120-124.7.
Rudnicky, A.I.
System response delay and userstrategy selection in a spreadsheet task.
Submitted forpublication.8.
Rudnicky, A.I.
and Sakamoto, M.H.
Transcriptionconventions for spoken language research.
Tech.Rept.
CMU-CS-89-194, Carnegie Mellon UniversitySchool of Computer Science, 1989.9.
Rudnicky, A.I., Polifroni, J.H., Thayer, E.H., andBrennan, R.A. "Interactive problem solving withspeech".
Journal of the Acoustical Society of America84 (1988), $213(A).10.
Ward, W.H.
Modelling Non-Verbal Sounds forSpeech Recognition.
In Proceedings of the DARPAworkshop on spoken language systems, Morgan Kauf-man, 1989.159
