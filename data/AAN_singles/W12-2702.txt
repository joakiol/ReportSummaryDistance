NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model?
On the Future of Language Modeling for HLT, pages 11?19,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsLarge, Pruned or Continuous Space Language Models on a GPUfor Statistical Machine TranslationHolger Schwenk, Anthony Rousseau and Mohammed AttikLIUM, University of Le Mans72085 Le Mans cedex 9, FRANCEHolger.Schwenk@lium.univ-lemans.frAbstractLanguage models play an important role inlarge vocabulary speech recognition and sta-tistical machine translation systems.
Thedominant approach since several decades areback-off language models.
Some years ago,there was a clear tendency to build huge lan-guage models trained on hundreds of billionsof words.
Lately, this tendency has changedand recent works concentrate on data selec-tion.
Continuous space methods are a verycompetitive approach, but they have a highcomputational complexity and are not yet inwidespread use.
This paper presents an ex-perimental comparison of all these approacheson a large statistical machine translation task.We also describe an open-source implemen-tation to train and use continuous space lan-guage models (CSLM) for such large tasks.We describe an efficient implementation of theCSLM using graphical processing units fromNvidia.
By these means, we are able to trainan CSLM on more than 500 million words in20 hours.
This CSLM provides an improve-ment of up to 1.8 BLEU points with respect tothe best back-off language model that we wereable to build.1 IntroductionLanguage models are used to estimate the proba-bility of a sequence of words.
They are an impor-tant module in many areas of natural language pro-cessing, in particular large vocabulary speech recog-nition (LVCSR) and statistical machine translation(SMT).
The goal of LVCSR is to convert a speechsignal x into a sequence of words w. This is usuallyapproached with the following fundamental equa-tion:w?
= argmaxwP (w|x)= argmaxwP (x|w)P (w) (1)In SMT, we are faced with a sequence of words ein the source language and we are looking for itsbest translation f into the target language.
Again,we apply Bayes rule to introduce a language model:f?
= argmaxfP (f |e)= argmaxfP (e|f)P (f) (2)Although we use a language model to evaluate theprobability of the produced sequence of words, wand f respectively, we argue that the task of the lan-guage model is not exactly the same for both ap-plications.
In LVCSR, the LM must choose amonga large number of possible segmentations of thephoneme sequence into words, given the pronuncia-tion lexicon.
It is also the only component that helpsto select among homonyms, i.e.
words that are pro-nounced in the same way, but that are written dif-ferently and which have usually different meanings(e.g.
ate/eight or build/billed).
In SMT, on the otherhand, the LM has the responsibility to chose the besttranslation of a source word given the context.
Moreimportantly, the LM is a key component which hasto sort out good and bad word reorderings.
Thisis known to be a very difficult issue when translat-ing from or into languages like Chinese, Japanese orGerman.
In LVCSR, the word order is given by thetime-synchronous processing of the speech signal.Finally, the LM helps to deal with gender, number,11etc accordance of morphologically rich languages,when used in an LVCSR as well as an SMT system.Overall, one can say that the semantic level seemsto be more important for language modeling in SMTthan LVCSR.
In both applications, so called back-offn-gram language models are the de facto standardsince several decades.
They were first introducedin the eighties, followed by intensive research onsmoothing methods.
An extensive comparison canbe found in (Chen and Goodman, 1999).
Modified-Kneser Ney smoothing seems to be the best perform-ing method and it is this approach that is almost ex-clusively used today.Some years ago, there was a clear tendency inSMT to use huge LMs trained on hundreds on bil-lions (1011) of words (Brants et al, 2007).
The au-thors report continuous improvement of the trans-lation quality with increasing size of the LM train-ing data, but these models require a large cluster totrain and to perform inference using distributed stor-age.
Therefore, several approaches were proposedto limit the storage size of large LMs, for instance(Federico and Cettolo, 2007; Talbot and Osborne,2007; Heafield, 2011).1.1 Continuous space language modelsThe main drawback of back-off n-gram languagemodels is the fact that the probabilities are estimatedin a discrete space.
This prevents any kind of inter-polation in order to estimate the LM probability ofan n-gram which was not observed in the trainingdata.
In order to attack this problem, it was pro-posed to project the words into a continuous spaceand to perform the estimation task in this space.
Theprojection as well as the estimation can be jointlyperformed by a multi-layer neural network (Bengioand Ducharme, 2001; Bengio et al, 2003).
The ba-sic architecture of this approach is shown in figure 1.A standard fully-connected multi-layer per-ceptron is used.
The inputs to the neuralnetwork are the indices of the n?1 pre-vious words in the vocabulary hj=wj?n+1,.
.
.
, wj?2, wj?1 and the outputs are the posteriorprobabilities of all words of the vocabulary:P (wj = i|hj) ?i ?
[1, N ] (3)where N is the size of the vocabulary.
The inputuses the so-called 1-of-n coding, i.e., the ith word ofprojectionlayer hiddenlayeroutputlayerinputprojectionssharedcontinuousrepresentation: representation:indices in wordlistLM probabilitiesdiscretefor all wordsprobability estimationNeural NetworkNwj?1 PHNP (wj=1|hj)wj?n+1wj?n+2P (wj=i|hj)P (wj=N|hj)P dimensional vectorsckoiMVdjp1 =pN =pi =Figure 1: Architecture of the continuous space LM.
hjdenotes the context wj?n+1, .
.
.
, wj?1.
P is the size ofone projection andH ,N is the size of the hidden and out-put layer respectively.
When short-lists are used the sizeof the output layer is much smaller then the size of thevocabulary.the vocabulary is coded by setting the ith element ofthe vector to 1 and all the other elements to 0.
Theith line of the N ?
P dimensional projection matrixcorresponds to the continuous representation of theith word.
Let us denote cl these projections, dj thehidden layer activities, oi the outputs, pi their soft-max normalization, and mjl, bj , vij and ki the hid-den and output layer weights and the correspondingbiases.
Using these notations, the neural networkperforms the following operations:dj = tanh(?lmjl cl + bj)(4)oi =?jvij dj + ki (5)pi = eoi /N?r=1eor (6)The value of the output neuron pi is used as the prob-ability P (wj = i|hj).
Training is performed withthe standard back-propagation algorithm minimiz-ing the following error function:E =N?i=1ti log pi + ???
?jlm2jl +?ijv2ij??
(7)where ti denotes the desired output, i.e., the proba-12bility should be 1.0 for the next word in the trainingsentence and 0.0 for all the other ones.
The first partof this equation is the cross-entropy between the out-put and the target probability distributions, and thesecond part is a regularization term that aims to pre-vent the neural network from overfitting the trainingdata (weight decay).
The parameter ?
has to be de-termined experimentally.The CSLM has a much higher complexity than aback-off LM, in particular because of the high di-mension of the output layer.
Therefore, it was pro-posed to limit the size of the output layer to the mostfrequent words, the other ones being predicted bya standard back-off LM (Schwenk, 2004).
All thewords are still considered at the input layer.It is important to note that the CSLM is still ann-gram approach, but the notion of backing-off toshorter contexts does not exist any more.
The modelcan provide probability estimates for any possiblen-gram.
It also has the advantage that the complex-ity only slightly increases for longer context win-dows, while it is generally considered to be unfea-sible to train back-off LMs on billions of words fororders larger than 5.The CSLM was very successfully applied to largevocabulary speech recognition.
It is usually used torescore lattices and improvements of the word er-ror rate of about one point were consistently ob-served for many languages and domains, for in-stance (Schwenk and Gauvain, 2002; Schwenk,2007; Park et al, 2010; Liu et al, 2011; Lamel etal., 2011).
More recently, the CSLM was also suc-cessfully applied to statistical machine translation(Schwenk et al, 2006; Schwenk and Este`ve, 2008;Schwenk, 2010; Le et al, 2010)During the last years, several extensions were pro-posed in the literature, for instance:?
Mikolov and his colleagues are working onthe use of recurrent neural networks instead ofmulti-layer feed-forward architecture (Mikolovet al, 2010; Mikolov et al, 2011).?
A simplified calculation of the short-list prob-ability mass and the addition of an adaptationlayer (Park et al, 2010; Liu et al, 2011)?
the so-called SOUL architecture which allowsto cover all the words at the output layer insteadof using a short-list (Le et al, 2011a; Le et al,2011b), based on work by (Morin and Bengio,2005; Mnih and Hinton, 2008).?
alternative sampling in large corpora (Xu et al,2011)Despite significant and consistent gains inLVCSR and SMT, CSLMs are not yet in widespreaduse.
Possible reasons for this could be the large com-putational complexity which requires flexible andcarefully tuned software so that the models can bebuild and used in an efficient manner.In this paper we provide a detailed comparison ofthe current most promising language modeling tech-niques for SMT: huge back-off LMs that integrateall available data, LMs trained on data selected withrespect to its relevance to the task by a recently pro-posed method (Moore and Lewis, 2010), and a newvery efficient implementation of the CSLM whichintegrates data selection.2 Continuous space LM toolkitFree software to train and use CSLM was proposedin (Schwenk, 2010).
The first version of this toolkitprovided no support for short lists or other means totrain CSLMs with large output vocabularies.
There-fore, it was not possible to use it for LVCSR andlarge SMT tasks.
We extended our tool with fullsupport for short lists during training and inference.Short lists are implemented as proposed in (Park etal., 2010), i.e.
we add one extra output neuron forall words that are not in the short list.
This prob-ability mass is learned by the neural network fromthe training data.
However, we do not use this prob-ability mass to renormalize the output distribution,we simply assume that it is sufficiently close to theprobability mass reserved by the back-off LM forwords that are not in the short list.
In summary, dur-ing inference words in the short-list are predicted bythe neural network and all the other ones by a stan-dard back-off LM.
No renormalization is performed.We have performed some comparative experimentswith renormalization during inference and we couldnot observe significant differences.
The toolkit sup-ports LMs in the SRILM format, an interface to thepopular KENLM is planed.132.1 Parallel processingThe computational power of general purpose pro-cessors like those build by Intel or AMD has con-stantly increased during the last decades and opti-mized libraries are available to take advantage of themulti-core capabilities of modern CPUs.
Our CSLMtoolkit fully supports parallel processing based onIntel?s MKL library.1 Figure 2 shows the time usedto train a large neural network on 1M examples.
Wetrained a 7-gram CSLM with a projection layer ofsize 320, two hidden layers of size 1024 and 512 re-spectively, and an output layer of dimension 16384(short-list).
We compared two hardware architec-tures:?
a top-end PC with one Intel Core i7 3930K pro-cessor (3.2 GHz, 6 cores).?
a typical server with two Intel Xeon X5675 pro-cessors (2?
3.06 GHz, 6 cores each).We did not expect a linear increase of the speedwith the number of threads run in parallel, but nev-ertheless, there is a clear benefit of using multiplecores: processing is about 6 times faster when run-ning on 12 cores instead of a single one.
The Core i73930K processor is actually slightly faster than theXeon X5675, but we are limited to 6 cores since itcan not interact with a second processor.2.2 Running on a GPUIn parallel to the development efforts for fast generalpurpose CPUs, dedicated hardware has been devel-oped in order to satisfy the computational needs ofrealistic 3D graphics in high resolutions, so calledgraphical processing units (GPU).
Recently, it wasrealized that this computational power can be infact used for scientific computing, e.g.
in chem-istry, molecular physics, earth quake simulations,weather forecasts, etc.
A key factor was the avail-ability of libraries and toolkits to simplify the pro-gramming of GPU cards, for instance the CUDAtoolkit of Nvidia.2 The machine learning commu-nity has started to use GPU computing and severaltoolkits are available to train generic networks.
Wehave also added support for Nvidia GPU cards to the1http://software.intel.com/en-us/articles/intel-mkl2http://developer.nvidia.com/cuda-downloads100200400800160032000  2  4  6  8  10  12time insecnumber of CPU coresIntel Xeon X5675Intel Core7 3690KNvidia Tesla M2090Nvidia GTX 580Figure 2: Time to train on 1M examples on various hard-ware architectures (the speed is shown in log scale).CSLM toolkit.
Timing experiments were performedwith two types of GPU cards:?
a Nvidia GTX 580 GPU card with 3 GB ofmemory.
It has 512 cores running at 1.54 GHz.?
a Nvidia Tesla M2090 card with 6 GB of mem-ory.
It has 512 cores running at 1.3 GHz.As can be seen from figure 2, for these networksizes the GTX 580 is about 3 times faster than twoIntel X5675 processors (12 cores).
This speed-upis smaller than the ones observed in other studies torun machine learning tasks on a GPU, probably be-cause of the large number of parameters which re-quire many accesses to the GPU memory.
For syn-thetic benchmarks, all the code and data often fitsinto the fast shared memory of the GPU card.
Weare continuing our work to improve the speed of ourtoolkit on GPU cards.
The Tesla M2090 is a little bitslower than the GTX 580 due to the lower core fre-quency.
However, it has a much better support fordouble precision floating point calculations whichcould be quite useful when training large neural net-works.3 Experimental ResultsIn this work, we present comparative results for var-ious LMs when integrated into a large-scale SMTsystem to translate from Arabic into English.
We usethe popular Moses toolkit to build the SMT system(Koehn et al, 2007).
As in our previous works, theCSLM is used to rescore 1000-best lists.
The sen-tence probability calculated by the CSLM is added14AFP APW NYT XIN LTW WPB CNAold avrg recent old avrg recent old avrg recent old avrg recent all all allUsing all the data:Words 151M 547M 371M 385M 547M 444M 786M 543M 364M 105M 147M 144M 313M 20M 39MPerplex 167.7 141.0 138.6 192.7 170.3 163.4 234.1 203.5 197.1 162.9 126.4 121.8 170.3 269.3 266.5After data selection:Words36M 77M 96M 62M 77M 89M 110M 54M 44M 23M 35M 38M 69M 6M 7M23% 26% 26% 16% 14% 20% 14% 10% 12% 22% 24% 26% 22% 30% 18%Perplex 160.9 135.0 131.6 185.3 153.2 151.1 201.2 173.6 169.5 159.6 123.4 117.7 153.1 263.9 253.2Table 1: Perplexities on the development data (news wire genre) of the individual sub-corpora in the LDC Gigawordcorpus, before and after data selection by the method of (Moore and Lewis, 2010).as 15th feature function and the coefficients of allthe feature functions are optimized by MERT.
TheCSLM toolkit includes scripts to perform this task.3.1 Baseline systemsThe Arabic/English SMT system was trained on par-allel and monolingual data similar to those avail-able in the well known NIST OpenMT evaluations.About 151M words of bitexts are available fromLDC out of which we selected 41M words to buildthe translation model.
The English side of all thebitexts was used to train the target language model.In addition, we used the LDC Gigaword corpusversion 5 (LDC2011T07).
It contains about 4.9 bil-lion words coming from various news sources (AFPand XIN news agencies, New York Times, etc) forthe period 1994 until 2010.
All corpus sizes aregiven after tokenization.For development and tuning, we used theOpenMT 2009 data set which contains 1313 sen-tences.
The corresponding data from 2008 was usedas internal test set.
We report separate results for thenews wire part (586 sentence, 24k words) and theweb part (727 sentences, 24k words) since we wantto compare the performance of the various LMs forformal and more informal language.
Four referencetranslations were available.
Case and punctuationwere preserved for scoring.It is well known that it is better to build LMs onthe individual sources and to interpolate them, in-stead of building one LM on all the concatenateddata.
The interpolation coefficients are tuned by op-timizing the perplexity on the development corpususing an EM procedure.
We split the huge Giga-word corpora AFP, APW, NYT and XIN into threeparts according to the time period (old, average andrecent).
This gives in total 15 sub-corpora.
The sizesand the perplexities are given in Table 1.
The inter-polated 4-gram LM of these 15 corpora has a per-plexity of 87 on the news part.If we add the English side of all the bitexts, theperplexity can be lowered to 71.1.
All the observedn-grams were preserved, e.g.
the cut-off for n-gramcounts was set to 1 for all orders.
This gives us anhuge LM with 1.4 billion 4-grams, 548M 3-gramsand 83M bigrams which requires more 26 GBytesto be stored on disk.
This LM is loaded into mem-ory by the Moses decoder.
This takes more than 10minutes and requires about 70 GB of memory.Moses supports memory mapped LMs, likeIRSTLM or KENLM, but this was not explored inthis study.
We call this LM ?big LM?.
We believethat it could be considered as a very strong base-line for a back-off LM.
We did not attempt to buildhigher order back-off LM given the size require-ments.
For comparison, we also build a small LMwhich was trained on the English part of the bitextsand the recent XIN corpus only.
It has a perplexityof 78.9 and occupies 2 GB on disk (see table 2).3.2 Data selectionWe have reimplemented the method of Moore andLewis (2010) to select the most appropriate LM databased on the difference between the sentence-wiseentropy of an in-domain and out-of domain LM.In our experiments, we have observed exactly thesame behavior than reported by the authors: the per-plexity decreases when less, but more appropriate151701801902002102202302400  10  20  30  40  50  60  70  80  90  100PerplexityPercentage of corpusAFPAPWNYTXINFigure 3: Decrease in perplexity when selecting data withthe method proposed in (Moore and Lewis, 2010).data is used, reaching a minimum using about 20%of the data only.
The improvement in the perplexitycan reach 20% relative.
Figure3 shows the perplex-ity for some corpora in function of the size of theselected data.
Detailed statistics for all corpora aregiven in Table 1 for the news genre.Unfortunately, these improvements in perplexityalmost vanish when we interpolate the individuallanguage models: the perplexity is 86.6 instead of87.0 when all the data from the Gigaword corpus isused.
This LM achieves the same BLEU score onthe development data, and there is a small improve-ment of 0.24 BLEU on the test set (Table 2).
Never-theless, the last LM has the advantage of being muchsmaller: 7.2 instead of 25 GBytes.
We have also per-formed the data selection on the concatenated textsof 4.9 billion words.
In this case, we do observe andecrease of the perplexity with respect to a modeltrained on all the concatenated data, but overall, theperplexity is higher than with an interpolated LM (asexpected).Px BLEULM Dev Size Dev TestSmall 78.9 2.0 GB 56.89 49.66Big 71.1 26 GB 58.66 50.75Giga 87.0 25.0 GB 57.08 50.08GigaSel 86.6 7.2 GB 57.03 50.32Table 2: Comparison of several 4-gram back-off lan-guage models.
See text for explanation of the models.3.3 Continuous space language modelsThe CSLM was trained on all the availabledata using the resampling algorithm described in(Schwenk, 2007).
At each epoch we randomly re-sampled about 15M examples.
We build only oneCSLM resampling simultaneously in all the corpora.The short list was set to 16k ?
this covers about 92%of the n-gram requests.
Since it is very easy to uselarge context windows with an CSLM, we trainedright away 7-grams.
We provide a comparison ofdifferent context lengths later in this section.
Thenetworks were trained for 20 epochs.
This can bedone in about 64 hours on a server with two IntelX5675 processors and in 20 hours on a GPU card.This CSLM achieves a perplexity of 62.9, to becompared to 71.1 for the big back-off LM.
This is arelative improvement of more than 11%, but actuallywe can do better.
If we train the CSLM on the smallcorpus only, i.e.
the English side of the bitexts andthe recent part of the XIN corpus, we achieve a per-plexity of 61.9 (see table 3).
This clearly indicatesthat it is better to focus the CSLM on relevant data.Random resampling is a possibility to train a neu-ral network on very large corpora, but it does notguarantee that all the examples are used.
Even ifwe resampled different examples at each epoch, wewould process at most 300M different examples (20epochs times 15M examples).
There is no reason tobelieve that we randomly select examples which areappropriate to the task (note, however, that the re-sampling coefficients are different for the individualLM Corpus Sent.
Perplexsize select.
on DevBack-off 4-gram LM:Small 196M no 78.9Big 5057M no 71.1CSLM 7-gram:big 5057M no 62.9Small 196M no 61.9Small 92M yes 60.96x Giga 425M yes.
57.910x Giga 553M yes.
56.9Table 3: Perplexity on the development data (news genre)for back-off and continuous space language models.16Small LM Huge LM CSLMGenre 4-gram back-off 7-gramNews 49.66 50.75 52.28Web / 35.17 36.53Table 4: BLEU scores on the test set for the translationfrom Arabic into English for various language models.corpora similar to the coefficients of an interpolatedback-off LM).
Therefore, we propose to use the dataselection method of Moore and Lewis (2010) to con-centrate the training of the CSLM on the most in-formative examples.
Instead of sampling randomlyn-grams in all the corpora, we do this in the selecteddata by the method of (Moore and Lewis, 2010).
Bythese means, it is more likely that we train the CSLMon relevant data.
Note that this has no impact on thetraining speed since the amount of resampled data isnot changed.The results for this method are summarized in Ta-ble 3.
In a first experiment, we used the selected partof the recent XIN corpus only.
This reduces the per-plexity to 60.9.
In addition, if we use the six or tenmost important Gigaword corpora, we achieve a per-plexity of 57.9 and 56.9 respectively.
This is 10%better than resampling blindly in all the data (62.9?
56.9).
Overall, the 7-gram CSLM improves theperplexity by 20% relative with respect to the huge4-gram back-off LM (71.1?
56.9).Finally, we used our best CSLM to rescore then-best lists of the Arabic/English SMT system.
Thebaseline BLEU score on the test set, news genre, is49.66 with the small LM.
This increases to 50.75with the big LM.
It was actually necessary to openthe beam of the Moses decoder in order to observesuch an improvement.
The large beam had no effectwhen the small LM was used.
This is a very strongbaseline to improve upon.
Nevertheless, this resultis further improved by the CSLM to 52.28, i.e.
asignificant gain of 1.8 BLEU.
We observe similarbehavior for the WEB genre.All our networks have two hidden layers sincewe have observed that this slightly improves perfor-mance with respect to the standard architecture withonly one hidden layer.
This is a first step towardsso-called deep neural networks (Bengio, 2007), butwe have not yet explored this systematically.Order: 4-gram 5-gram 6-gram 7-gramPx Dev: 63.9 59.5 57.6 56.9BLEU Dev: 59.76 60.11 60.29 60.26BLEU Test: 51.91 51.85 52.23 52.28Table 5: Perplexity on the development data (news genre)and BLEU scores of the continuous space language mod-els in function of the context size.In an 1000-best list for 586 sentences, we have atotal of 14M requests for 7-grams out of which morethan 13.5M were processed by the CSLM, e.g.
theshort list hit rate is almost 95%.
This resulted in only2670 forward passes through the network.
At eachpass, we collected in average 5350 probabilities atthe output layer.
The processing takes only a coupleof minutes on a server with two Xeon X5675 CPUs.One can of course argue that it is not correctto compare 4-gram and 7-gram language models.However, building 5-gram or higher order back-offLMs on 5 billion words is computationally very ex-pensive, in particular with respect to memory usage.For comparison, we also trained lower order CSLMmodels.
It can be clearly seen from Table 5 that theCSLM can take advantage of longer contexts, butit already achieves a significant improvement in theBLEU score at the same LM order (BLEU on thetest data: 50.75?
51.91).The CSLM is very space efficient: a saved net-work occupies about 600M on disk in function ofthe network architecture, in particular in function ofthe size of the continuous projection.
Loading takesonly a couple of seconds.
During training, 1 GByteof main memory is sufficient.
The memory require-ment during n-best rescoring essentially depends onthe back-off LM that is eventually charged to dealwith out-off short-list words.
Figure 4 shows someexample translations.4 ConclusionThis paper presented a comparison of several pop-ular techniques to build language models for sta-tistical machine translation systems: huge back-offmodels trained on billions of words, data selectionof most relevant examples and a highly efficient im-plementation of continuous space methods.Huge LMs perform well, but their storage mayrequire important computational resources ?
in our17?????
????
???
??
????
???
????
????
??????
?????
????
?????
????
????
??
??????
?????
????
???
?????
?????
?
?????
??????
???????
???
???
????
??????.?????
??????????
????
?Back-off LM:The minister inspected the sub-committee integrated combat marinepollution with oil, which includes the latest equipment lose face marine pollution andchemical plant in the port specializing in monitoring the quality of the crude oil supplierand with the most modern technological devices.CSLM: The minister inspected the integrated sub-committee to combat marine pollutionwith oil, which includes the latest equipment deal with marine pollution and inspect thechemical plant in the port specializing in monitoring the quality of the crude oil supplier,with the most modern technological devices.Google: The minister also inspected the sub-center for integrated control of marinepollution with oil, which includes the latest equipment on the face of marine pollution andchemical plant loses port specialist in quality control of crude oil and supplied????
???????
????
????????
??????
??
???????
?Back-off LM:Pyongyang is to respect its commitments to end nuclear program.CSLM: Pyongyang promised to respect its commitments to end the nuclear program.Google: Pyongyang is to respect its obligations to end nuclear program..
????
????
??
????
???
???
?
??????
??????
????
?
??
??
?Back-off LM: The Taliban militants in kidnappings in the country over the past two years.CSLM: Taliban militants have carried out kidnappings in the country repeatedly duringthe past two years.Google:The Taliban kidnappings in the country frequently over the past two years.Figure 4: Example translations when using the huge back-off and the continuous space LM.
For comparison we alsoprovide the output of Google Translate.case, 26 GB on disk and 70 GB of main memory fora model trained on 5 billions words.
The data selec-tion method proposed in (Moore and Lewis, 2010)is very effective at the corpus level, but the observedgains almost vanish after interpolation.
However,the storage requirement can be divided by four.The main contributions of this paper are sev-eral improvements of the continuous space languagemodel.
We have shown that data selection is veryuseful to improve the resampling of training datain large corpora.
Our best model achieves a per-plexity reduction of 20% relative with respect tothe best back-off LM we were able to build.
Thisgives an improvement of up to 1.8 BLEU points in avery competitive Arabic/English statistical machinetranslation system.We have also presented a very efficient imple-mentation of the CSLM.
The tool can take advan-tage of modern multi-core or multi-processor com-puters.
We also support graphical extension cardslike the Nvidia 3D graphic cards.
By these means,we are able to train a CSLM on 500M words inabout 20 hours.
This tool is freely available.3 Bythese means we hope to make large-scale continu-ous space language modeling available to a largercommunity.3http://www-lium.univ-lemans.fr/ ?cslm18AcknowledgmentsThis work has been partially funded by the FrenchGovernment under the project COSMAT (ANR-09-CORD-004) and the European Commission underthe project FP7 EuromatrixPlus.ReferencesYoshua Bengio and Rejean Ducharme.
2001.
A neu-ral probabilistic language model.
In NIPS, volume 13,pages 932?938.Yoshua Bengio, Rejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
JMLR, 3(2):1137?1155.Yoshua Bengio.
2007. learning deep architectures forAI.
Technical report, University of Montre?al.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In EMNLP, pages 858?867.Stanley F. Chen and Joshua T. Goodman.
1999.
Anempirical study of smoothing techniques for languagemodeling.
Computer Speech & Language, 13(4):359?394.Marcello Federico and Maura Cettolo.
2007.
Efficienthandling of n-gram language models for statistical ma-chine translation.
In Second Workshop on SMT, pages88?95.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Sixth Workshop on SMT,pages 187?197.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL,demonstration session.L.
Lamel, J.-L. Gauvain, V.-B.
Le, I. Oparin, , andS.
Meng.
2011.
Improved models for mandarinspeech-to-text transcription.
In ICASSP, pages 4660?4663.H.S.
Le, A. Allauzen, G. Wisniewski, and F. Yvon.
2010.Training continuous space language models: Somepractical issues.
In EMNLP, pages 778?788.H.S.
Le, I. Oparin, A. Allauzen, J-L. Gauvain, andF.
Yvon.
2011a.
Structured output layer neural net-work language model.
In ICASSP, pages 5524?5527.H.S.
Le, I. Oparin, A. Messaoudi, A. Allauzen, J-L. Gau-vain, and F. Yvon.
2011b.
Large vocabulary SOULneural network language models.
In Interspeech.X.
Liu, M. J. F. Gales, and P. C. Woodland.
2011.
Im-proving LVCSR system combination using neural net-work language model cross adaptation.
In Interspeech,pages 2857?2860.Toma?s?
Mikolov, Martin Karafia?t, Luka?s?
Burget, JanC?ernocky?, and Sanjeev Khudanpur.
2010.
Recurrentneural network based language model.
In Interspeech,pages 1045?1048.T.
Mikolov, S. Kombrink, L. Burget, J.H.
Cernocky, andS.
Khudanpur.
2011.
Extensions of recurrent neuralnetwork language model.
In ICASSP, pages 5528?5531.Andriy Mnih and Geoffrey Hinton.
2008.
A scalablehierarchical distributed language model.
In NIPS.Robert C. Moore and William Lewis.
2010.
Intelligentselection of language model training data.
In ACL,pages 220?224.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InProceedings of the Tenth International Workshop onArtificial Intelligence and Statistics.Junho Park, Xunying Liu, Mark J. F. Gales, and Phil C.Woodland.
2010.
Improved neural network based lan-guage modelling and adaptation.
In Interspeech, pages1041?1044.Holger Schwenk and Yannick Este`ve.
2008.
Data selec-tion and smoothing in an open-source system for the2008 NIST machine translation evaluation.
In Inter-speech, pages 2727?2730.Holger Schwenk and Jean-Luc Gauvain.
2002.
Connec-tionist language modeling for large vocabulary contin-uous speech recognition.
In ICASSP, pages I: 765?768.Holger Schwenk, Daniel De?chelotte, and Jean-Luc Gau-vain.
2006.
Continuous space language models forstatistical machine translation.
In Proceedings of theCOLING/ACL 2006 Main Conference Poster Sessions,pages 723?730.Holger Schwenk.
2004.
Efficient training of large neu-ral networks for language modeling.
In IJCNN, pages3059?3062.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech and Language, 21:492?518.Holger Schwenk.
2010.
Continuous space languagemodels for statistical machine translation.
The PragueBulletin of Mathematical Linguistics, (93):137?146.David Talbot and Miles Osborne.
2007.
Smoothedbloom filter language models: Tera-scale lms on thecheap.
In EMNLP, pages 468?476.Puyang Xu, Asela Gunawardana, and Sanjeev Khudan-pur.
2011.
Efficient subsampling for training complexlanguage models.
In EMNLP, pages 1128?1136.19
