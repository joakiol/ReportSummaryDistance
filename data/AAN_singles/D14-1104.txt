Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 968?973,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsImportance weighting and unsupervised domain adaptationof POS taggers: a negative resultBarbara Plank, Anders Johannsen and Anders S?gaardCenter for Language TechnologyUniversity of Copenhagen, DenmarkNjalsgade 140, DK-2300 Copenhagen Sbplank@cst.dk,ajohannsen@hum.ku.dk,soegaard@hum.ku.dkAbstractImportance weighting is a generalizationof various statistical bias correction tech-niques.
While our labeled data in NLP isheavily biased, importance weighting hasseen only few applications in NLP, most ofthem relying on a small amount of labeledtarget data.
The publication bias towardreporting positive results makes it hard tosay whether researchers have tried.
Thispaper presents a negative result on unsu-pervised domain adaptation for POS tag-ging.
In this setup, we only have unlabeleddata and thus only indirect access to thebias in emission and transition probabili-ties.
Moreover, most errors in POS tag-ging are due to unseen words, and there,importance weighting cannot help.
Wepresent experiments with a wide variety ofweight functions, quantilizations, as wellas with randomly generated weights, tosupport these claims.1 IntroductionMany NLP tasks rely on the availability of anno-tated data.
The majority of annotated data, how-ever, is sampled from newswire corpora.
Theperformance of NLP systems, e.g., part-of-speech(POS) tagger, parsers, relation extraction sys-tems, etc., drops significantly when they are ap-plied to data that departs from newswire conven-tions.
So while we can extract information, trans-late and summarize newswire in major languageswith some success, we are much less successfulprocessing microblogs, chat, weblogs, answers,emails or literature in a robust way.
The main rea-sons for the drops in accuracy have been attributedto factors such as previously unseen words and bi-grams, missing punctuation and capitalization, aswell as differences in the marginal distribution ofdata (Blitzer et al., 2006; McClosky et al., 2008;S?gaard and Haulrich, 2011).The move from one domain to another (from asource to a new target domain), say from newspa-per articles to weblogs, results in a sample selec-tion bias.
Our training data is now biased, sinceit is sampled from a related, but nevertheless dif-ferent distribution.
The problem of automaticallyadjusting the model induced from source to a dif-ferent target is referred to as domain adaptation.Some researchers have studied domain adap-tation scenarios, where small samples of labeleddata have been assumed to be available for thetarget domains.
This is usually an unrealistic as-sumption, since even for major languages, smallsamples are only available from a limited numberof domains, and in this work we focus on unsuper-vised domain adaptation, assuming only unlabeledtarget data is available.Jiang and Zhai (2007), Foster et al.
(2010; Plankand Moschitti (2013) and S?gaard and Haulrich(2011) have previously tried to use importanceweighting to correct sample bias in NLP.
Im-portance weighting means assigning a weightto each training instance, reflecting its impor-tance for modeling the target distribution.
Im-portance weighting is a generalization over post-stratification (Smith, 1991) and importance sam-pling (Smith et al., 1997) and can be used to cor-rect bias in the labeled data.Out of the four papers mentioned, only S?gaardand Haulrich (2011) and Plank and Moschitti(2013) considered an unsupervised domain adap-tation scenario, obtaining mixed results.
Thesetwo papers assume covariate shift (Shimodaira,2000), i.e., that there is only a bias in the marginaldistribution of the training data.
Under this as-sumption, we can correct the bias by applying aweight functionPt(x)Ps(x)to our training data points(labeled sentences) and learn from the weighteddata.
Of course this weight function cannot be968computed in general, but we can approximate itin different ways.In POS tagging, we typically factorize se-quences into emission and transition probabilities.Importance weighting can change emission prob-abilities and transition probabilities by assigningweights to sentences.
For instance, if our corpusconsisted of three sequences: 1) a/A b/A, 2) a/Ab/B, and 3) a/A b/B, then P (B|A) = 2/3.
If se-quences two and three were down-weighted to 0.5,then P (B|A) = 1/2.However, this paper argues that importanceweighting cannot help adapting POS taggers tonew domains using only unlabeled target data.
Wepresent three sources of evidence: (a) negativeresults with the most obvious weight functionsacross various English datasets, (b) negative re-sults with randomly sampled weights, as well as(c) an analysis of annotated data indicating thatthere is little variation in emission and transitionprobabilities across the various domains.2 Related workMost prior work on importance weighting use adomain classifier, i.e., train a classifier to discrimi-nate between source and target instances (S?gaardand Haulrich, 2011; Plank and Moschitti, 2013)(y ?
{s, t}).
For instance, S?gaard and Haulrich(2011) train a n-gram text classifier and Plankand Moschitti (2013) a tree-kernel based clas-sifier on relation extraction instances.
In thesestudies,?P (t|x) is used as an approximation ofPt(x)Ps(x), following Zadrozny (2004).
In ?3, we fol-low the approach of S?gaard and Haulrich (2011),but consider a wider range of weight functions.Others have proposed to use kernel mean match-ing (Huang et al., 2007) or minimizing KL-divergence (Sugiyama et al., 2007).Jiang and Zhai (2007) use importance weight-ing to select a subsample of the source data bysubsequently setting the weight of all selected datapoints to 1, and 0 otherwise.
However, they doso by relying on a sequential model trained onlabeled target data.
Our results indicate that thecovariate shift assumption fails to hold for cross-domain POS tagging.
While the marginal distri-butions obviously do differ (since we can tell do-mains apart without POS analysis), this is mostlikely not the only difference.
This might explainthe positive results obtained by Jiang and Zhai(2007).
We will come back to this in ?4.Cortes et al.
(2010) show that importanceweighting potentially leads to over-fitting, but pro-pose to use quantiles to obtain more robust weightfunctions.
The idea is to rank all weights and ob-tain q quantiles.
If a data point x is weighted byw, and w lies in the ith quantile of the ranking(i ?
q), x is weighted by the average weight ofdata points in the ith quantile.The weighted structured perceptron (?3) used inthe experiments below was recently used for a dif-ferent problem, namely for correcting for bias inannotations (Plank et al., 2014).l l l ll l l l l l l l l l l l l l l l0 5 10 15 209293949596979899 l wsjanswersreviewsemailsweblogsnewsgroupsFigure 1: Training epochs vs tagging accuracy forthe baseline model on the dev data.3 Experiments3.1 DataWe use the data made available in the SANCL2012 Shared Task (Petrov and McDonald, 2012).The training data is the OntoNotes 4.0 releaseof the Wall Street Journal section of the PennTreebank, while the target domain evaluation datacomes from various sources, incl.
Yahoo Answers,user reviews, emails, weblogs and newsgroups.For each target domain, we have both developmentand test data.3.2 ModelIn the weighted perceptron (Cavallanti et al.,2006), we make the learning rate dependent on thecurrent instance xn, using the following update:wi+1?
wi+ ?n?(yn?
sign(wi?
xn))xn(1)where ?nis the weight associated with xn.
SeeHuang et al.
(2007) for similar notation.We extend this idea straightforwardly to thestructured perceptron (Collins, 2002), for which969System Answers Newsgroups Reviews Avg Emails Weblogs WSJOur system 91.08 91.57 91.59 91.41 87.97 92.19 97.32SANCL12-2nd 90.99 92.32 90.65 91.32 ?
?
97.76SANCL12-best 91.79 93.81 93.11 92.90 ?
?
97.29SANCL12-last 88.24 89.70 88.15 88.70 ?
?
95.14FLORS basic 91.17 92.41 92.25 88.67 91.37 97.11 91.94Table 1: Tagging accuracies and comparison to prior work on the SANCL test sets (fine-grained POS).we use an in-house implementation.
We usecommonly used features, i.e., w,w?1, w?2,w+1, w+2, digit, hyphen, capitalization, pre-/suffix features, and Brown word clusters.
Themodel seems robust with respect to numberof training epochs, cf.
Figure 1.
Thereforewe fix the number of epochs to five and usethis setting in all our experiments.
Our codeis available at: https://bitbucket.org/bplank/importance-weighting-exp.3.3 Importance weightingIn our first set of experiments, we follow S?gaardand Haulrich (2011) in using document classifiersto obtain weights for the source instances.
Wetrain a text classifier that discriminates the twodomains (source and target).
For each sentencein the source and target domain (the unlabeledtext that comes with the SANCL data), we markwhether it comes from the source or target do-main and train a binary classifier (logistic regres-sion) to discriminate between the two.
For ev-ery sentence in the source we obtain its probabil-ity for the target domain by doing 5-fold cross-validation.
While S?gaard and Haulrich (2011)use only token-based features (word n-grams ?3), we here exploit a variety of features: wordtoken n-grams, and two generalizations: usingBrown clusters (estimated from the union of the5 target domains), and Wiktionary tags (if a wordhas multiple tags, we assign it the union of tags assingle tag; OOV words are marked as such).The distributions of weights can be seen in theupper half of Figure 2.3.3.1 ResultsTable 1 shows that our baseline model achievesstate-of-the-art performance compared toSANCL (Petrov and McDonald, 2012)1andFLORS (Schnabel and Sch?utze, 2014).
Ourresults align well with the second best POStagger in the SANCL 2012 Shared Task.
Note1https://sites.google.com/site/sancl2012/home/shared-task/resultsFigure 2: Histogram of different weight functions.that the best tagger in the shared task explicitlyused normalization and various other heuristicsto achieve better performance.
In the rest of thepaper, we use the universal tag set part of theSANCL data (Petrov et al., 2012).Figure 3 presents our results on developmentdata for different importance weighting setups.None of the above weight functions lead to signifi-cant improvements on any of the datasets.
We alsotried scaling and binning the weights, as suggestedby Cortes et al.
(2010), but results kept fluctuatingaround baseline performance, with no significantimprovements.3.4 Random weightingObviously, weight functions based on documentclassifiers may simply not characterize the rele-vant properties of the instances and hence lead tobad re-weighting of the data.
We consider threerandom sampling strategies, namely sampling ran-dom uniforms, random exponentials, and random970Figure 3: Results on development data for different weight functions, i.e., document classifiers trainedon a) raw tokens; b) tokens replaced by Wiktionary tags; c) tokens replaced by Brown cluster ids.
Theweight was the raw pt(y|x) value, no scaling, no quantiles.
Replacing only open-class tokens for b) andc) gave similar or lower performance.Zipfians and ran 500 samples for each.
For theseexperiments, we estimate significance cut-off lev-els of tagging accuracies using the approximaterandomization test.
To find the cut-off levels,we randomly replace labels with gold labels untilthe achieved accuracy significantly improves overthe baseline for more than 50% of the samples.For each accuracy level, 50 random samples weretaken.lllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0 200 40093.293.694.0answersIndexl randomexpzipflllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0 200 40094.294.494.694.8reviewsIndexTAlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0 200 40093.493.8emailsIndexTAlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0 200 40094.494.895.2weblogsIndexTAlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll0 200 40094.294.695.0newsgroupsIndexTA0 200 40093.093.493.8answersIndex0 200 40094.294.6reviewsIndexTA0 200 40093.293.694.0emailsIndexTA0 200 40094.294.695.095.4weblogsIndexTA0 200 40094.094.494.8newsgroupsIndexTA0 50 100 15092.593.093.594.0answers0 50 100 15093.594.094.5reviewsTA0 50 100 15092.593.093.594.0emailsTA0 50 100 15094.094.595.0weblogsTA0 50 100 15093.594.094.595.0newsgroupsTAFigure 4: Random weight functions (500 runseach) on test sets.
Solid line is the baseline per-formance, while the dashed line is the p-value cut-off.
From top: random, exponential and Zipfianweighting.
All runs fall below the cut-off.3.4.1 ResultsThe dashed lines in Figure 4 show the p-value cut-offs for positive results.
We see that most randomweightings of data lead to slight drops in perfor-mance or are around baseline performance, and noweightings lead to significant improvements.
Ran-dom uniforms seem slightly better than exponen-tials and Zipfians.domain (tokens) avg tag ambiguity OOV KL ?type tokenwsj (train/test: 731k/39k) 1.09 1.41 11.5 0.0006 0.99answers (28k) 1.09 1.22 27.7 0.048 0.77reviews (28k) 1.07 1.19 29.5 0.040 0.82emails (28k) 1.07 1.19 29.9 0.027 0.92weblogs (20k) 1.05 1.11 22.1 0.010 0.96newsgroups (20k) 1.05 1.14 23.1 0.011 0.96Table 2: Relevant statistics for our analysis (?4)on the test sets: average tag ambiguity, out-of-vocabulary rate, and KL-divergence and Pearsoncorrelation coefficient (?)
on POS bigrams.4 AnalysisSome differences between the gold-annotatedsource domain data and the gold-annotated tar-get data used for evaluation are presented in Ta-ble 2.
One important observation is the low ambi-guity of word forms in the data.
This makes theroom for improvement with importance weight-ing smaller.
Moreover, the KL divergencies overPOS bigrams are also very low.
This tells us thattransition probabilities are also relatively constantacross domains, again suggesting limited room forimprovement for importance weighting.Compared to this, we see much bigger differ-ences in OOV rates.
OOV rates do seem to explainmost of the performance drop across domains.In order to verify this, we implemented a ver-sion of our structured perceptron tagger with type-constrained inference (T?ackstr?om et al., 2013).This technique only improves performance on un-seen words, but nevertheless we saw significantimprovements across all five domains (cf.
Ta-ble 3).
This suggests that unseen words are amore important problem than the marginal distri-bution of data for unsupervised domain adaptationof POS taggers.971ans rev email webl newsgbase 93.41 94.44 93.54 94.81 94.55+type constr.
94.09?
94.85?
94.31?
95.99?
95.97?p-val cut-off 93.90 94.85 94.10 95.3 95.10Table 3: Results on the test sets by adding Wik-tionary type constraints.
?=p-value < 0.001.We also tried Jiang and Zhai?s subset selectiontechnique (?3.1 in Jiang and Zhai (2007)), whichassumes labeled training material for the targetdomain.
However, we did not see any improve-ments.
A possible explanation for these differentfindings might be the following.
Jiang and Zhai(2007) use labeled target data to learn their weight-ing model, i.e., in a supervised domain adaptationscenario.
This potentially leads to very differentweight functions.
For example, let the source do-main be 100 instances of a/A b/B and 100 in-stances of b/B b/B, and the target domain be 100instances of a/B a/B.
Note that a domain classi-fier would favor the first 100 sentences, but in anHMM model induced from the labeled target data,things look very different.
If we apply Laplacesmoothing, the probability of a/A b/B accord-ing to the target domain HMM model would be?
8.9e?7, and the probability of b/B b/B wouldbe ?
9e?5.
Note also that this set-up does not as-sume covariate shift.5 Conclusions and Future WorkImportance weighting, a generalization of variousstatistical bias correction techniques, can poten-tially correct bias in our labeled training data, butthis paper presented a negative result about impor-tance weighting for unsupervised domain adapta-tion of POS taggers.
We first presented exper-iments with a wide variety of weight functions,quantilizations, as well as with randomly gener-ated weights, none of which lead to significant im-provements.
Our analysis indicates that most er-rors in POS tagging are due to unseen words, andwhat remains seem to not be captured adequatelyby unsupervised weight functions.For future work we plan to extend this work tofurther weight functions, data sets and NLP tasks.AcknowledgementsThis research is funded by the ERC Starting GrantLOWLANDS No.
313695.ReferencesJohn Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP.Giovanni Cavallanti, Nicol`o Cesa-Bianchi, and Clau-dio Gentile.
2006.
Tracking the best hyperplanewith a simple budget perceptron.
In COLT.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In EMNLP.Corinna Cortes, Yishay Mansour, and Mehryar Mohri.2010.
Learning bounds for importance weighting.In NIPS.George Foster, Cyril Goutte, and Roland Kuhn.
2010.Discriminative instance weighting for domain adap-tation in statistical machine translation.
In EMNLP.Jiayuan Huang, Alexander Smola, Arthur Gretton,Karsten Borgwardt, and Bernhard Sch?olkopf.
2007.Correcting sample bias by unlabeled data.
In NIPS.Jing Jiang and ChengXiang Zhai.
2007.
Instanceweighting for domain adaptation in NLP.
In ACL.David McClosky, Eugene Charniak, and Mark John-son.
2008.
When is self-training effective for pars-ing?
In COLING.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 Shared Task on Parsing the Web.
In Notesof the First Workshop on Syntactic Analysis of Non-Canonical Language (SANCL).Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.A universal part-of-speech tagset.
In LREC.Barbara Plank and Alessandro Moschitti.
2013.
Em-bedding semantic similarity in tree kernels for do-main adaptation of relation extraction.
In ACL.Barbara Plank, Dirk Hovy, and Anders S?gaard.
2014.Learning part-of-speech taggers with inter-annotatoragreement loss.
In EACL.Tobias Schnabel and Hinrich Sch?utze.
2014.
Flors:Fast and simple domain adaptation for part-of-speech tagging.
TACL, 2:15?16.Hidetoshi Shimodaira.
2000.
Improving predictive in-ference under covariate shift by weighting the log-likelihood function.
Journal of Statistical Planningand Inference, 90:227?244.Peter Smith, Mansoor Shafi, and Hongsheng Gao.1997.
Quick simulation: A review of importancesampling techniques in communications systems.IEEE Journal on Selected Areas in Communica-tions, 15(4):597?613.T.M.F.
Smith.
1991.
Post-stratification.
The Statisti-cian, 40:315?323.972Anders S?gaard and Martin Haulrich.
2011.Sentence-level instance-weighting for graph-basedand transition-based dependency parsing.
In IWPT.Masashi Sugiyama, Shinichi Nakajima, HisashiKashima, Paul von B?unau, and Motoaki Kawanabe.2007.
Direct importance estimation with model se-lection and its application to covariate shift adapta-tion.
In NIPS.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, RyanMcDonald, and Joakim Nivre.
2013.
Token andtype constraints for cross-lingual part-of-speech tag-ging.
TACL, 1:1?12.Bianca Zadrozny.
2004.
Learning and evaluating clas-sifiers under sample selection bias.
In ICML.973
