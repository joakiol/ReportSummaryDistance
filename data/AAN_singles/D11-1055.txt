Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 594?604,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsPredicting a Scientific Community?s Response to an ArticleDani Yogatama Michael Heilman Brendan O?Connor Chris DyerSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USA{dyogatama,mheilman,brenocon,cdyer}@cs.cmu.eduBryan R. RoutledgeTepper School of BusinessCarnegie Mellon UniversityPittsburgh, PA 15213, USAroutledge@cmu.eduNoah A. SmithSchool of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAnasmith@cs.cmu.eduAbstractWe consider the problem of predicting mea-surable responses to scientific articles basedprimarily on their text content.
Specif-ically, we consider papers in two fields(economics and computational linguistics)and make predictions about downloads andwithin-community citations.
Our approach isbased on generalized linear models, allowinginterpretability; a novel extension that cap-tures first-order temporal effects is also pre-sented.
We demonstrate that text featuressignificantly improve accuracy of predictionsover metadata features like authors, topicalcategories, and publication venues.1 IntroductionWritten communication is an essential componentof the complex social phenomenon of science.
Assuch, natural language processing is well-positionedto provide tools for understanding the scientific pro-cess, by analyzing the textual artifacts (papers, pro-ceedings, etc.)
that it produces.
This paper is aboutmodeling collections of scientific documents to un-derstand how their textual content relates to how ascientific community responds to them.
While pastwork has often focused on citation structure (Borneret al, 2003; Qazvinian and Radev, 2008), our em-phasis is on the text content, following Ramage etal.
(2010) and Gerrish and Blei (2010).Instead of task-independent exploratory data anal-ysis (e.g., topic modeling) or multi-document sum-marization, we consider supervised models of thecollective response of a scientific community to apublished article.
There are many measures of im-pact of a scientific paper; ours come from directmeasurements of the number of downloads (froman established website where prominent economistspost papers before formal publication) and citations(within a fixed scientific community).
We adopt adiscriminative approach based on generalized lin-ear models that can make use of any text or meta-data features, and show that simple lexical fea-tures offer substantial power in modeling out-of-sample response and in forecasting response for fu-ture articles.
Realistic forecasting evaluations re-quire methodological care beyond the usual bestpractices of train/test separation, and we elucidatethese issues.In addition, we introduce a new regularizationtechnique that leverages the intuition that the rela-tionship between observable features and responseshould evolve smoothly over time.
This regularizerallows the learner to rely more strongly on more re-cent evidence, while taking into account a long his-tory of training data.
Our time series-inspired regu-larizer is computationally efficient in learning and isa significant advance over earlier text-driven fore-casting models that ignore the time variable alto-gether (Kogan et al, 2009; Joshi et al, 2010).We evaluate our approaches in two novel experi-mental settings: predicting downloads of economicsarticles and predicting citation of papers at ACLconferences.
Our approaches substantially outper-594015004 9log(# downloads)# docs.025000 18# citations# docs.Figure 1: Left: the distribution of log download countsfor papers in the NBER dataset one year after post-ing.
Right: the distribution of within-dataset citations ofACL papers within three years of publication (outliers ex-cluded for readability).form text-ignorant baselines on ground-truth predic-tions.
Our time series models permit flexibility infeatures and offer a novel and perhaps more inter-pretable view of the data than summary statistics.2 DataWe make use of two collections of scientific litera-ture, one from the economics domain, and the otherfrom computational linguistics and natural languageprocessing.
Statistics are summarized in Table 1.2.1 NBEROur first dataset consists of research papers in eco-nomics from the National Bureau of EconomicResearch (NBER) from 1999 to 2009 (http://www.nber.org).
Approximately 1,000 researcheconomists are affiliated with the NBER.
NewNBER working papers are posted to the websiteweekly.
The papers are not yet peer-reviewed, butgiven the prominence of many economists affiliatedwith the NBER, many of the papers are widely read.Text from the abstracts of the papers and relatedmetadata are publicly available.
Full text is availableto subscribers (universities typically have access).The NBER provided us with download statisticsfor these papers.
For each paper, we computedthe total number of downloads in the first year af-ter each paper?s posting.1 The download counts arelog-normally distributed, as shown in Figure 1, andso our regression models (?3) minimize squared er-rors in the log space.
Our download logs begin in1For the vast majority of papers, most of the downloads oc-cur soon after the paper?s posting.
We explored different mea-sures with different download windows (two years, for exam-ple) with broadly similar results.
We leave a more detailed anal-ysis of the time series patterns of downloads to future work.Dataset # Docs.
Avg.
#WordsResponseNBER 8,814 155 # downloads in firstyear (mean 761)ACL 4,026 3,966 at least 1 citation infirst 3 years?
(54% no)Table 1: Descriptive statistics about the datasets.1999.
We use the 8,814 papers from 1999?2009 pe-riod (there are 16,334 papers in the full dataset dat-ing back to 1985).
We only use text from the ab-stracts, since we were able to obtain full texts forjust a portion of the papers, and since the OCR ofthe full texts we do have is very noisy.2.2 ACLOur second dataset consists of research papersfrom the Association for Computational Linguis-tics (ACL) from 1980 to 2006 (Radev et al, 2009a;Radev et al, 2009b).
We have the full texts for pa-pers (OCR output) as well as structured citation data.There are 15,689 papers in the whole dataset.
Forthe citation prediction task, we include conferencepapers from ACL, EACL, HLT, and NAACL.2 Weremove journal papers, since they are characteristi-cally different from conference papers, as well asworkshop papers.
We do include short papers, in-teractive demo session papers, and student researchpapers that are included in the companion volumesfor these conferences (such papers are cited less thanfull papers, but many are still cited).
The resultingdataset contains 4,026 papers.
The number of pa-pers in each year varies because not all conferencesare annual.We look at citations in the three-year window fol-lowing publication, excluding self-citations and onlyconsidering citations from papers within these con-ferences.
Figure 1 shows a histogram; note thatmany papers (54%) are not cited at all, and the dis-tribution of citations per paper is neither normal norlog-normal.
We organize the papers into two classes:those with zero citations and those with non-zero ci-tations in the three-year window.2EMNLP is a relatively recent conference, and, in this col-lection, complete data for its papers postdate the end of the lasttraining period, so we chose to exclude it from our dataset.5953 ModelOur forecasting approach is based on generalizedlinear models for regression and classification.
Themodels are trained with an `2-penalty, often calleda ?ridge?
model (Hoerl and Kennard, 1970).3 Forthe NBER data, where (log) number of downloads isnearly a continuous measure, we use linear regres-sion.
For the ACL data, where response is the bi-nary cited-or-not variable we use logistic regression,often referred to as a ?maximum entropy?
model(Berger et al, 1996) or a log-linear model.
Webriefly review the class of models.
Then, we de-scribe a time series model appropriate for time seriesdata.3.1 Generalized Linear ModelsConsider a model that predicts a response y given avector input x = ?x1, .
.
.
, xd?
?
Rd.
Our modelsare linear functions of x and parameterized by thevector ?.
Given a corpus of M document features,X , and responses Y , we estimate:??
= argmin?
R(?)
+ L(?,X, Y ) (1)where L is a model-dependent loss function and Ris a regularization penalty to encourage models withsmall weight vectors.
We describe models and lossfunctions first and then turn to regularization.For the NBER data, the (log) number of down-loads is continuous, and so we use least-squareslinear regression model.
The loss function is thesum of the squared errors for the M documents inour training data: L(?,X, Y ) = ?Mi=1(yi ?
y?i)2,where the prediction rule for new documents is:y?
=?dj=0 ?jxj .
Probabilistically, this equates to anassumption that ?>x is the mean of a normal (i.e.,Gaussian) distribution from which random variabley is drawn.For the ACL data, we predict y from a discreteset C (specifically, the binary set of zero citations ormore than zero citations), and we use logistic regres-sion.
This model assumes that for the ith traininginput xi, the output yi is drawn according to:p(yi | xi) =(exp?>c xi) /(?c?
?C exp?>c?xi)3Preliminary experiments found no consistent benefit from`1 (?lasso?)
models, though we note that `1-regularization leadsto sparse, compact models that may be more interpretable.where there is a feature vector ?c for each classc ?
C. Under this interpretation, parameter esti-mation is maximum a posteriori inference for ?,and R(?)
is a log-prior for the weights.
The lossfunction is the negative log likelihood for the Mdocuments: L(?,X, Y ) = ?
?Mi=1 log p(yi | xi).The prediction rule for a new document is: y?
=argmaxc?C?dj=0 ?c,jxj .
Generalized linear mod-els and penalized regression are well-studied withan extensive literature (Mccullagh and Nelder, 1989;Hastie et al, 2009).
We leave other types of mod-els, such as Poisson (Cameron and Trivedi, 1998)or ordinal (McCullagh, 1980) regression models, tofuture work.3.2 Ridge RegressionWith large numbers of features, regularization iscrucial to avoid overfitting.
In ridge regression (Ho-erl and Kennard, 1970), a standard method to whichwe compare the time series regularization discussedin ?3.3, the penalty R(?)
is proportional to the `2-norm of ?:R(?)
= ???
?2 = ?
?j ?2jwhere ?
is a regularization hyperparameter that istuned on development data or by cross-validation.4This penalty pushes many ?j close (but not com-pletely) to zero.
In practice, we multiply the penaltyby the number of examples M to facilitate tuning of?.The ridge linear regression model can be inter-preted probabilistically as each coefficient ?j isdrawn i.i.d.
from a normal distribution with mean0 and variance 2?
?1.3.3 Time Series RegularizationA simple way to capture temporal variation is to con-join traditional features with a time variable.
Here,we divide the dataset into T time steps (years).
In thenew representation, the feature space expands fromRd to RT?d.
For a document published at year t, theelements of x are non-zero only for those featuresthat correspond to year-t; that is xt?,j = 0 for allt?
6= t.4The linear regression has a bias ?0 that is always active.The logistic regression also has an unpenalized bias ?c,0 foreach class c. This weight is not regularized.596Estimating this model with the new features usingthe `2-penalty would be effectively estimating sepa-rate models for each year under the assumption thateach ?t,j is independent; even for features that dif-fered only temporally (e.g., ?t,j and ?t+1,j).In this work, we apply time series regularizationto GLMs, enabling models that have coefficients thatchange over time but prefer gradual changes acrosstime steps.
Boyd and Vandenberghe (2004, ?6.3) de-scribe a general version of this sort of regularizer.To our knowledge, such regularizers have not previ-ously been applied to temporal modeling of text.The time series regularization penalty becomes:R(?)
= ?T?t=1d?j=1?2t,j+?
?T?t=2d?j=0(?t,j ?
?t?1,j)2It includes a standard `2-penalty on the coefficients,and a penalty for differences between coefficientsfor adjacent time steps to induce smooth changes.5Similar to the previous model, in practice, we mul-tiply the regularization constant ?
by MT to facili-tate tuning of ?
for datasets with different numbersof examples M and numbers of time steps T .
Thenew parameter, ?, controls the smoothness of the es-timated coefficients.
Setting ?
to zero imposes nopenalty for time-variation in the coefficients and re-sults in independent ridge regressions at each timestep.
Also, when the number of examples is con-stant across time steps, setting a large ?
parameter(?
?
?)
results in a single ridge regression over allyears since it imposes ?t,j = ?t+1,j for all t ?
T .The partial derivative is:?R/?
?t,j = 2?
?t,j+ 1{t > 1}2??
(?t,j ?
?t?1,j)+ 1{t < T}2??
(?t,j ?
?t+1,j)This time series regularization can be applied moregenerally, not just to linear and logistic regression.With either ridge regularization or this time se-ries regularization scheme, Eq.
1 is an unconstrainedconvex optimization problem for the linear models5Our implementation of the time series regularizer does notpenalize the magnitude of the weight for the bias feature (as inridge regression).
It does, however, penalize the difference inthe bias weight between time steps (as with other features).
?1?2?3?TY1Y2Y3YT...X1X2X3XT?,?Figure 2: Time series regression as a graphical model;the variables Xt and Yt are the sets of feature vectorsand response variables from documents dated t.we describe here.
There exist a number of optimiza-tion procedures for it; we use the L-BFGS quasi-Newton algorithm (Liu and Nocedal, 1989).Probabilistic InterpretationWe can interpret the time series regularization prob-abilistically as follows.
Let the coefficient for thejth feature over time be ?j = ?
?1,j , ?2,j , ..., ?T,j?.
?j are draws from a multivariate normal distribu-tion with a tridiagonal precision matrix ?
?1 = ?
?RT?T :?
= ???????
?1 + ?
??
0 0 .
.
.??
1 + 2?
??
0 .
.
.0 ??
1 + 2?
??
.
.
.0 0 ??
1 + 2?
.
.
.... ... ... ... .
.
.??????
?The form of R(?)
follows from noting:?2 log p(?j ;?, ?)
= ?>j ?
?j + constantThe squared difference between adjacent time stepscomes from the off-diagonal entries in the preci-sion matrix.6 Figure 2 shows a graphical represen-tation of the time series regularization in our model.Its Markov chain structure corresponds to the off-diagonals.There is a rich literature on time series analysis(Box et al, 2008; Hamilton, 1994).
The prior dis-tribution over the sequence ?
?1,j , .
.
.
, ?T,j?
that ourregularizer posits is closely linked to a first-order au-toregressive process, AR(1).6Consistent with the previous section, we assume that pa-rameters for different features, ?j and ?k, are independent.597NBER ACLResponse log(#downloads+1) 1{#citations > 0}GLM type normal / squared-loss logistic / log-lossMetric 1 mean absolute error accuracyMetric 2 Kendall?s ?
Kendall?s ?Table 2: Summary of the setup for the NBER downloadand ACL citation prediction experiments.4 FeaturesNBER metadata features?
Authors?
last names.
We treat each name as a bi-nary feature.
If a paper has multiple authors, allauthors are used and they have equal weights re-gardless of their ordering.?
NBER program(s).7 There are 19 major re-search programs at the NBER (e.g., MonetaryEconomics, Health Economics, etc.
).ACL metadata features?
Authors?
last names as binary features.?
Conference venues.
We use first letter of the ACLanthology paper ID, which correlates with its con-ference venue (e.g., P for the ACL main confer-ence, H for the HLT conference, etc.
).8Text features?
Binary indicator features for the presence of eachunigram, bigram, and trigram.
For the NBERdata, we have separate features for titles and ab-stracts.
For the ACL data, we have separate fea-tures for titles and full texts.
We pruned text fea-tures by document frequency (details in ?5).?
Log transformed word counts.
We include fea-tures for the numbers of words in the title and theabstract (NBER) or the full text (ACL).7Almost all NBER papers are tagged with one or more pro-grams (we assign untagged papers a ?null?
tag).
The completelist of NBER programs can be found at http://www.nber.org/programs8Papers in the ACL dataset have a tag which shows whichworkshop, conference, or journal they appeared in.
However,sometimes a conference is jointly held with another confer-ence, such that meta information in the dataset is different eventhough the conference is the same.
For this reason, we simplyuse the first letter of the paper ID.5 ExperimentsFor each of the datasets in ?2, we test our modelsfor two tasks: forecasting about future papers (i.e.,making predictions about papers that appeared af-ter a training dataset) and modeling held-out papersfrom the past (i.e., making predictions within thesame time period as the training dataset, on held-outexamples).For the NBER dataset, the task is to predict thenumber of downloads a paper will receive in its firstyear after publication.
For the ACL dataset, the taskis to predict whether a paper will be cited at all (byanother ACL paper in our dataset) within the firstthree years after its publication.
To our knowledge,clean, reliable citation counts are not available forthe NBER dataset; nor are download statistics avail-able for the ACL dataset.
Table 2 summarizes thevariables of interest, model types, and evaluationmetrics for the tasks.5.1 ExtrapolationThe lag between a paper?s publication and when itsoutcome (download or citation count) can be ob-served poses a unique methodological challenge.Consider predicting the number of downloads overg future time steps.
If t is the time of forecasting,we can observe the texts of all articles published be-fore t. However, any article published in the interval[t ?
g, t] is too recent for the outcome measurementof y to be taken.
We refer to the interval [t?
g, t] asthe ?forecast gap?.
Since recent articles are some-times the most relevant predictions at t, we do notwant to ignore them.
Consider a paper at time stept?, t?g < t?
< t. To extrapolate its number of down-loads, we consider the observed number in [t?, t], andthen estimate the ratio r of downloads that occur inthe first t?t?
time steps, against the first g time steps,using the fully observed portion of the training data.We then scale the observed downloads during [t?, t]by r?1 to extrapolate.
The same method is used toextrapolate citation counts.In preliminary experiments, we observed that ex-trapolating responses for papers in the forecast gapled to better performance in general.
For example,for the ridge regressions trained on all past yearswith the full feature set, the error dropped from 262to 259 when using extrapolation compared to with-598out extrapolation.
Also, the extrapolated downloadcounts were quite close to the true values (which wehave but do not use because of the forecast gap): forexample, the mean absolute error of the extrapolatedresponses was 99 when extrapolated based on themedian of the fully observed portion of the trainingdata (measured monthly).5.2 Forecasting NBER DownloadsIn our first set of experiments, we predict the numberof downloads of an NBER paper within one year ofits publication.We compare four approaches for predictingdownloads.
The first is a baseline that simply usesthe median of the log of the training and develop-ment data as the prediction.
The second and thirduse GLMs with ridge regression-style regularization(?3.2), trained on all past years (?all years?)
and onthe single most recent past year (?one year?
), respec-tively.
The last model (?time series?)
is a GLM withtime series regularization (?3.3).We divided papers by year.
Figure 3 illustrates theexperimental setup.
We held out a random 20% ofpapers for each year from 1999?2007 as a test set forthe task of modeling the past.
To define the featureset and tune hyperparameters, we used the remain-ing 80% of papers from 1999?2005 as our trainingdata and the remaining papers in 2006 as our devel-opment data.
After pruning,9 we have 37,251 to-tal features, of which 2,549 are metadata features.When tuning hyperparameters, we simulated the ex-istence of a forecast gap by using extrapolated re-sponses for papers in the last year of the trainingdata instead of their true responses.
We considered?
?
5{2,1,...,?5,?6}, and ?
?
5{3,2,...,?1,?2} and se-lected those that led to the best performance on thedevelopment set.We then used the selected feature set and hyperpa-rameters to test the forecasting and modeling capa-bilities of each model.
For forecasting, we predictednumbers of downloads of papers in 2008 and 2009.We used the baseline median, ridge regression, andtime series regularization models trained on papersin 1999?2007 and 1999?2008, respectively.
Wetreated the last year of the training data (2007 and9For NBER, text features appearing in less than 0.1% ormore than 99.9% of the training documents were removed.
ForACL, the thresholds were 2% and 98%.trainingmodeling test (unused)gap dev.trr taitan tagtraining gap testtrr tai tam taoddddddmodeling test (unused)oaelaeoaelaetraining gap testtrr tamdddmodeling testoaelaetao tarNBER ExperimentsACL Experimentstrainingmodeling test (unused)dev.toa tro ta dddoaelaes(u)p)v.(uu)p)vtrainingmodeling test (unused)gaptoa ta taltadddoaelaetrainingmodeling test (unused)gaptoa tal tatandddoaelaetrainingmodeling testtoa tadddoaelaegaptagtantesttaitesttagtesttangaptrr as(u)p)v.(uu)p)vFigure 3: An illustration of how the datasets were seg-mented for the experiments.
Portions of data for whichwe report results are shaded.
Time spans are not to scale.2008, respectively) as a forecast gap, since we wouldnot have observed complete responses of papers inthese years when forecasting.
For the ?one year?models, we trained ridge regressions only on themost recent past year, using papers in 2007 and2008, respectively, as training data.10 To test theadditive benefit of text features, we trained modelswith just metadata features (NBER programs andauthors, denoted ?Meta?)
and with both metadata10Papers from the most recent past year in a training set haveincomplete responses, so the models were trained on extrapo-lated responses for that year.
For the NBER development setfrom 2005, a ridge regression on just 2004 papers (for whichextrapolation is needed) outperformed a regression on just 2003(for which extrapolation is not needed), 278 to 367 mean abso-lute error.
For the ACL development set from 2001, a regressionon just 2000 (for which extrapolation is needed) led to slightlylower performance (59% versus 61%) than a regression on just1998 (for which extrapolation is not needed), probably due tothe relatively small number of conferences and papers in 2000.For consistency with the other models and with the NBER ex-periments, we evaluated regressions on the most recent (extrap-olated) year in our ACL experiments.599Features Model Modeling Forecasting1999?07 2008 2009?
median 333 371 397Meta one year 279 354 375Meta all years 303 334 378Meta time series 279 353 375Full one year 271 346 351Full all years 265 ?300 339Full time series ?
?245 ?321 ?332Table 3: Mean absolute errors for the NBER downloadpredictions.
???
indicates statistical significance betweentime series models using metadata features and the fullfeature set.
???
indicates statistical significance betweenthe time series and ridge regression models using the fullfeature set (Wilcoxon signed-rank test, p < 0.01).and text features (denoted ?Full?
).To evaluate the modeling capabilities, we trainedthe ridge regression and time series regularizationmodels on papers from 1999?2008 and predicted thenumbers of downloads of held-out papers in 1999?2007.
For comparison, we also trained ridge regres-sion models on each individual year (?one year?
)and predicted the numbers of downloads of the held-out papers in the corresponding year.Table 3 shows mean absolute errors for eachmethod on both forecasting test splits, and mean ab-solute errors averaged across papers over nine mod-eling test splits.
For interpretability, we report pre-dictions in terms of download counts, though themodels were trained with log counts (?2.1).
The re-sults show that even a simple n-gram representationof text contains a valuable, learnable signal that ispredictive of future downloads.
While the time se-ries model did not significantly outperform ridge re-gression at predicting future downloads, it did resultin significantly better performance for modeling pa-pers in the past.5.3 Forecasting ACL CitationsWe now turn to the problem of predicting citationlevels.
Recall that here we aim to predict whetheran ACL paper will be cited within our dataset withinthree years.
Our experimental setup (Figure 3) issimilar to the setup for the NBER dataset, exceptthat we use logistic regression to model the discretecited-or-not response variable.
We also make thesimplifying assumption that all citations occur at theend of each year.
Therefore, the forecast gap is onlyFeat.
Model Modeling Forecasting1980?03 2004 2005 2006?
majority 55 56 60 50Meta one year 61 56 54 62Meta all years 65 58 53 60Meta time series 66 56 53 56Full one year 69 70 64 67Full all years 67 69 70 70Full time series 70 ?69 ?70 ?72Table 4: Classification accuracy (%) for predictingwhether ACL papers will be cited within three years.
??
?indicates statistical significance between time series mod-els using metadata features and the full feature set (bi-nomial sign test, p < 0.01).
With the full feature set,differences between the time series and ridge (all years)models are not statistically significant at the 0.01 level,but for the modeling task p is estimated at 0.026, and forthe 2006 forecasting task, p is estimated at 0.050.two years (we have observed complete citations inthe test year).After feature pruning, there were 30,760 total fea-tures, of which 1,694 are metadata features.
Weconsidered ?
?
5{2,1,???
,?8,?9} (?Full?)
and ?
?5{2,1,???
,?11,?12} (?Meta?
); and ?
?
5{6,5,???
,0,?1}(both ?Full?
and ?Meta?
), selecting the best valuesusing the development data.Again, we compare four methods: a baseline ofalways predicting the most frequent class in thetraining data, ?all years?
and ?one year?
logistic re-gression models, and a logistic regression with thetime series regularizer.For the forecasting task, we used papers in 2004,2005, and 2006 as test sets.
As the training sets forthe ?all years?
and time series models, we used pa-pers from 1980 up to the last year before each testset, with the last two years extrapolated.
As thetraining sets for the ?one year?
models, we used pa-pers from the year immediately before the test set,with extrapolated responses.To evaluate modeling capabilities, we predictedcitation levels of held-out papers in 1980?2003.
Weused the ?all years?
and time series models trainedon 1980?2005.
We trained ?one year?
models sepa-rately for each year and predicted downloads for theheld-out papers in that year.Table 4 shows classification accuracy for eachmodel on the test data for both the forecasting andmodeling tasks.
It is again clear that adding text sig-600nificantly improved the performance of the model.Also, the time series regression model shows asmall, though not statistically significant, gain formodeling whether past papers will be cited?as wellas similarly small gains on two of the three forecast-ing test years.5.4 RankingWe can also use the models for ranking to help de-cide which papers are expected to have the greatestimpact.
With rankings, we can use the same metricboth for download and citation predictions.
For theNBER data, we ranked test-set papers based on thepredicted numbers of downloads and computed thecorrelation to the actual numbers of downloads.
Forthe ACL data, we ranked papers based on the prob-ability of being cited (within the next three years)and computed the correlation to the actual numbersof citations.11To measure ranking models?
ranking quality, weused Kendall?s ?
, a nonparametric statistic that mea-sures the similarity of two different orderings overthe same set of items.
Here, the items are scien-tific papers and the two metrics are the gold stan-dard numbers of downloads (or citations) and modelpredictions for the numbers of downloads, or cita-tion probabilities.
If q is the chance that a randomlydrawn pair of items will be ranked in the same wayby the two metrics, then ?
= 2(q ?
0.5).Table 5 shows Kendall?s ?
for each model for theforecasting tasks (i.e., prediction of future citationsor downloads) in both datasets.
As in the previousexperiments, we see small benefits for the time se-ries regression model on most held-out data splits?and larger benefits for including text features alongwith metadata features.6 AnalysisAn advantage of the time series regularized regres-sion model is its interpretability.
Inspecting featurecoefficients in the model allows us to identify trendsand changes of interests over time within a scientificcommunity.11Here, we use models of responses to individual papers forranking (i.e., in a pointwise ranking scheme).
Time series reg-ularization could also be applied to ranking models that modelpairwise preferences to optimize metrics like Kendall?s ?
di-rectly, as discussed by Joachims (2002).Feat.
Model NBER ACL?08 ?09 ?04 ?05 ?06Meta one year .29 .22 .17 .08 .16Meta all years .31 .22 .15 .12 .21Meta time series .29 .22 .14 .10 .17Full one year .35 .31 .44 .39 .33Full all years .43 .37 .42 .43 .40Full time series .43 .38 .47 .44 .43Table 5: Kendall?s ?
rank correlation for future predictionmodels on both datasets.2000 2005 2010?0.0040.004unemployment_rate2000 2005 2010inflation_ratetime seriesall yearsone yearFigure 4: Coefficients for two NBER bigram features.First, we illustrate the difference between the timeseries and the other models in Figure 4, for NBERmodels?
weights for unemployment rate and infla-tion rate appearing in a paper?s abstract.
The year-to-year weights of ?one year?
models fluctuate sub-stantially, and the ?all years?
model is necessar-ily constant, but the time series regularizer gives asmooth trajectory.6.1 TrendsPrevious work has examined the flow of ideasas trends in word and phrase frequencies, as inthe Google Books Ngram Viewer (Michel et al,2011).12 Topic models have been used extensively toexplore trends in low-dimensional spaces (Blei andLafferty, 2006; Wang et al, 2008; Wang and McCal-lum, 2006; Ahmed and Xing, 2010).
By contrast,our approach allows us to examine trends in the im-pact of text related to specific observation variables:the coefficient trendline for a feature illustrates itsassociation with measurements of scholarly impact(citation and download frequency).Text frequencies can be quite different from thediscriminative weights our model assigns to fea-tures.
Figure 5 illustrates the ?t,j trends in the ACLtime series model for some selected terms that oc-12http://ngrams.googlelabs.com6011980 1990 2000?0.0100.0000.010discoursegenerationgrammarsmachine_translationparsingsemanticsTime series coef.1980 1990 20000.00000.0010Term frequenciesFigure 5: Feature trends: model coefficients vs. term fre-quencies over time in the ACL corpus.
Term freq.
is thefraction of tokens (or bigrams for m.t.)
that year, that arethe term, averaged over a centered five-year window.cur frequently in conference session titles.
On theright are term frequencies (with smoothing, sinceyear-to-year frequencies are bumpy).
Most termsdecline over time.
On the left, by contrast, are theweights learned by our time series model.
Theytell a very different story: for example, parsing hasshown a definite increase in interest, while interestin grammars (e.g., formalisms) has declined some-what.
These trends have face validity, giving cre-dence to our analysis; they also broadly agree withHall et al (2008).6.2 AuthorsThe regression method also allows analysis of authorinfluence, since we fit a coefficient for each of theauthors in the ACL dataset.
Figure 6(a) addressesthe following question: do prolific authors get citedmore often, even after accounting for the content oftheir papers?13 The effect is present but relativelysmall according to our model: the total number ofpapers co-authored by an author has a weak corre-lation to the author?s citation prediction coefficient(?
= 0.16).Next, does the model provide more informationthan the simple citation probability of an author?Figure 6(b) compares coefficients to an author?s pa-pers?
probability of being cited.
Since we did notprune author features, there are many authors with13More precisely: if a prolific author and a non-prolific au-thor write a paper, does the prolific author?s paper have a higherprobability of being cited than the non-prolific author?s, allother things being equal?llllllllllllllllllllll lllllllllllllllllllllll llll lll lllllllllllllllllll lllllll lllllllll ll llllll lll llllllllllllllllllllllllllll l llllllllllllllllllllllllllllllllllllllllllllllllllllllllllll lllllllllllllllllll llll llllllllllllllll l llllllllllllllllllllllllllll llllllllllll0 10 20 30 40?0.0020.0010.004Doc.
frequency?
(a) Doc.
freq.
vs. coef.0.0 0.4 0.8?0.0020.0010.004lllllll lllll lllllllllllllllllllllllll llllllllllllllllllllllllll lllll lllllll lllllllllllllllllllllllllllll llll llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll llllllllllllllllllllllllllllllllll lllllllCitation proportion?
(b) Citation prop.
vs. coef.Figure 6: Analysis of author citation coefficients.
Everypoint is one ACL author, and the vertical axis shows thecitation coefficient, compared to (a) the number of docu-ments co-authored by the author; and (b) the proportionof an author?s papers that are cited within three years.The vertical bar is the macro-averaged citation propor-tion across authors, 41%.only a few papers, resulting in unsmoothed proba-bilities of 0, 0.5, 1, etc.
(these correspond to the ver-tical ?bands?
in the plot).
By contrast, the `2-penaltyof the model naturally assigned coefficients close tozero for such authors if it is justified.In general, the simple probability agrees with thecoefficient, but there are differences.
The semanticsof the regression imply we are measuring the rela-tive citation probability of an author, controlling fortext and venue effects.
If an author has a high cita-tion prediction coefficient but a low citation proba-bility, that implies the author has better-cited workthan would be expected according to the n-grams inhis or her papers.
We have omitted names of au-thors from the figure for clarity and confidentiality,but high outlier authors tend to be well-known re-searchers in the ACL community.
Obviously, sincethe prediction model is not perfect, it is not possibleto completely verify this hypothesis, but we feel thisanalysis is reasonably suggestive.7 Related WorkPrevious work on modeling scientific literaturemostly focused on citation graphs (Borner et al,2003; Qazvinian and Radev, 2008).
Some re-searchers, e.g., Erosheva et al (2004), have usedtext content.
Most of these are based on topic mod-els: Gerrish and Blei (2010) measure scholarly im-pact, Hall et al (2008) study the ?history of ideas?,602and Ramage et al (2010) rank universities based onscholarly output using topic models.Download rates and citation prediction were twoof the main tasks in the KDD Cup 2003 (McGovernet al, 2003; Brank and Leskovec, 2003).
Bethardand Jurafsky (2010) considered the problem slightlydifferently and proposed an information retrieval ap-proach to citation prediction.
Our approach is novelin that we formulate the problem as a forecastingtask and we seek to predict future impact of articles.Linear regression with text features has been usedto predict financial risk (Kogan et al, 2009) andmovie revenues (Joshi et al, 2010).
While the fore-casts in those papers are similar to ours, those au-thors did not consider a forecast gap or allowing theparameters of the model to vary over time.Our time series regularization is closely relatedto the fused lasso (Tibshirani et al, 2005).
It pe-nalizes a loss function by the `1-norm of the co-efficients and their differences.
The `1-penalty fordifferences between coefficients encourages sparsityin the differences.
We use the `2-norm to inducesmooth changes across time steps.8 ConclusionsWe presented a statistical approach to predicting ascientific community?s response to an article, basedon its textual content.
To improve the interpretabilityof the linear model, we developed a novel time seriesregularizer that encourages gradual changes acrosstime steps.
Our experiments showed that text fea-tures significantly improve accuracy of predictionsover baseline models, and we found that the featureweights learned with the time series regularizer re-flect important trends in the literature.AcknowledgementsWe thank the National Bureau of Economic Re-search for providing the NBER dataset for thisresearch, Fallaw Sowell for helpful discussions,and three anonymous reviewers for comments onan earlier draft of this paper.
This research wassupported by the Intelligence Advanced ResearchProjects Activity under grant number N10PC20222and TeraGrid resources provided by the PittsburghSupercomputing Center under grant number TG-DBS110003.ReferencesA.
Ahmed and E. P. Xing.
2010.
Timeline: A dy-namic hierarchical Dirichlet process model for recov-ering birth/death and evolution of topics in text stream.In Proc.
of UAI.A.
L. Berger, V. J. Della Pietra, and S. A. DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational Linguistics,22(1):39?71.S.
Bethard and D. Jurafsky.
2010. Who should I cite?Learning literature search models from citation behav-ior.
In Proc.
of CIKM.D.
Blei and J. Lafferty.
2006.
Dynamic topic models.
InProc.
of ICML.K.
Borner, C. Chen, and K. Boyack.
2003.
Visualiz-ing knowledge domains.
In B. Cronin, editor, AnnualReview of Information Science and Technology, vol-ume 37, pages 179?255.
Information Today, Inc.G.
Box, G. M. Jenkins, and G. Reinsel.
2008.
Time Se-ries Analysis: Forecasting and Control.
Wiley Seriesin Probability and Statistics.S.
Boyd and L. Vandenberghe.
2004.
Convex Optimiza-tion.
Cambridge University Press.J.
Brank and J. Leskovec.
2003.
The download estima-tion task on KDD Cup 2003.
SIGKDD Explorations,5(2):160?162.A.
Cameron and P. Trivedi.
1998.
Regression Analysis ofCount Data.
Cambridge University Press.E.
Erosheva, S. Fienberg, and J. Lafferty.
2004.
Mixedmembership models of scientific publications.
InProc.
of PNAS.S.
Gerrish and D. M. Blei.
2010.
A language-basedapproach to measuring scholarly impact.
In Proc.
ofICML.D.
Hall, D. Jurafsky, and C. D. Manning.
2008.
Studyingthe history of ideas using topic models.
In Proc.
ofEMNLP.J.
D. Hamilton.
1994.
Time Series Analysis.
PrincetonUniversity Press.T.
Hastie, R. Tibshirani, and J. Friedman.
2009.
The Ele-ments of Statistical Learning: Data Mining, Inference,and Prediction.
Springer.A.
E. Hoerl and R. W. Kennard.
1970.
Ridge regression:Biased estimation for nonorthogonal problems.
Tech-nometrics, 12(1):55?67.T.
Joachims.
2002.
Optimizing search engines usingclickthrough data.
In Proc.
of KDD.M.
Joshi, D. Das, K. Gimpel, and N. A. Smith.
2010.Movie reviews and revenues: An experiment in textregression.
In Proc.
of HLT-NAACL.S.
Kogan, D. Levin, B. R. Routledge, J. S. Sagi, and N. A.Smith.
2009.
Predicting risk from financial reportswith regression.
In Proc.
of HLT-NAACL.603D.
C. Liu and J. Nocedal.
1989.
On the limited memoryBFGSmethod for large scale optimization.
Mathemat-ical Programming B, 45(3):503?528.P.
Mccullagh and A. J. Nelder.
1989.
Generalized LinearModels.
London: Chapman & Hall.P.
McCullagh.
1980.
Regression models for ordinal data.Journal of the Royal Statistical Society B, 42(2):109?142.A.
McGovern, L. Friedland, M. Hay, B. Gallagher,A.
Fast, J. Neville, and D. Jensen.
2003.
Exploit-ing relational structure to understand publication pat-terns in high-energy physics.
SIGKDD Explorations,5(2):165?172.J.
Michel, Y. Shen, A. Aiden, A. Veres, M. Gray, TheGoogle Books Team, J. Pickett, D. Hoiberg, D. Clancy,P.
Norvig, J. Orwant, S. Pinker, M. Nowak, andE.
Aiden.
2011.
Quantitative analysis of culture usingmillions of digitized books.
Science, 331(6014):176?182.V.
Qazvinian and D. R. Radev.
2008.
Scientific papersummarization using citation summary networks.
InProc.
of COLING.D.
R. Radev, M. T. Joseph, B. Gibson, and P. Muthukrish-nan.
2009a.
A bibliometric and network analysis ofthe field of computational linguistics.
Journal of theAmerican Society for Information Science and Tech-nology.D.
R. Radev, P. Muthukrishnan, and V. Qazvinian.
2009b.The ACL anthology network corpus.
In Proc.
of ACLWorkshop on Natural Language Processing and Infor-mation Retrieval for Digital Libraries.D.
Ramage, C. D. Manning, and D. A. McFarland.
2010.Which universities lead and lag?
Toward universityrankings based on scholarly output.
In Proc.
of NIPSWorkshop on Computational Social Science and theWisdom of the Crowds.R.
Tibshirani, M. Saunders, S. Rosset, J. Zhu, andK.
Knight.
2005.
Sparsity and smoothness via thefused lasso.
Journal of the Royal Statistical Society B,67(1):91?108.X.
Wang and A. McCallum.
2006.
Topics over time: Anon-Markov continuous-time model of topical trends.In Proc.
of KDD.C.
Wang, D. Blei, and D. Heckerman.
2008.
Continuoustime dynamic topic models.
In Proc.
of UAI.604
