Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 163?172,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsA Non-Monotonic Arc-Eager Transition System for Dependency ParsingMatthew HonnibalDepartment of ComputingMacquarie UniversitySydney, Australiamatthew.honnibal@mq.edu.edu.auYoav GoldbergDepartment of Computer ScienceBar Ilan UniversityRamat Gan, Israelyoav.goldberg@gmail.comMark JohnsonDepartment of ComputingMacquarie UniversitySydney, Australiamark.johnson@mq.edu.edu.auAbstractPrevious incremental parsers have usedmonotonic state transitions.
However,transitions can be made to revise previousdecisions quite naturally, based on furtherinformation.We show that a simple adjustment to theArc-Eager transition system to relax itsmonotonicity constraints can improve ac-curacy, so long as the training data in-cludes examples of mistakes for the non-monotonic transitions to repair.
We eval-uate the change in the context of a state-of-the-art system, and obtain a statisticallysignificant improvement (p < 0.001) onthe English evaluation and 5/10 of theCoNLL languages.1 IntroductionHistorically, monotonicity has played an im-portant role in transition-based parsing systems.Non-monotonic systems, including the one pre-sented here, typically redundantly generate multi-ple derivations for each syntactic analysis, leadingto spurious ambiguity (Steedman, 2000).
Early,pre-statistical work on transition-based parsingsuch as Abney and Johnson (1991) implicitly as-sumed that the parser searches the entire spaceof possible derivations.
The presence of spuri-ous ambiguity causes this search space to be a di-rected graph rather than a tree, which considerablycomplicates the search, so spurious ambiguity wasavoided whenever possible.However, we claim that non-monotonicity andspurious ambiguity are not disadvantages in amodern statistical parsing system such as ours.Modern statistical models have much larger searchspaces because almost all possible analyses are al-lowed, and a numerical score (say, a probabilitydistribution) is used to distinguish better analy-ses from worse ones.
These search spaces are solarge that we cannot exhaustively search them, soinstead we use the scores associated with partialanalyses to guide a search that explores only a mi-nuscule fraction of the space (In our case we usegreedy decoding, but even a beam search only ex-plores a small fraction of the exponentially-manypossible analyses).In fact, as we show here the additional redun-dant pathways between search states that non-monotonicity generates can be advantageous be-cause they allow the parser to ?correct?
an ear-lier parsing move and provide an opportunity torecover from formerly ?fatal?
mistakes.
Infor-mally, non-monotonicity provides ?many paths upthe mountain?
in the hope of making it easier tofind at least one.We demonstrate this by modifying the Arc-Eager transition system (Nivre, 2003; Nivre et al2004) to allow a limited capability for non-monotonic transitions.
The system normally em-ploys two deterministic constraints that limit theparser to actions consistent with the previous his-tory.
We remove these, and update the transitionsso that conflicts are resolved in favour of the latestprediction.The non-monotonic behaviour provides an im-provement of up to 0.2% accuracy over the cur-rent state-of-the-art in greedy parsing.
It is pos-sible to implement the greedy parser we de-scribe very efficiently: our implementation, whichcan be found at http://www.github.com/syllog1sm/redshift, parses over 500 sen-tences a second on commodity hardware.1632 The Arc-Eager Transition SystemIn transition-based parsing, a parser consists of astate (or a configuration) which is manipulated bya set of actions.
An action is applied to a stateand results in a new state.
The parsing processconcludes when the parser reaches a final state, atwhich the parse tree is read from the state.
A par-ticular set of states and actions yield a transition-system.
Our starting point in this paper is the pop-ular Arc-Eager transition system, described in de-tail by Nivre (2008).The state of the arc-eager system is composedof a stack, a buffer and a set of arcs.
The stack andthe buffer hold the words of a sentence, and the setof arcs represent derived dependency relations.We use a notation in which the stack items areindicated by Si, with S0 being the top of the stack,S1 the item previous to it and so on.
Similarly,buffer items are indicated as Bi, with B0 beingthe first item on the buffer.
The arcs are of theform (h, l,m), indicating a dependency in whichthe word m modifies the word h with label l.In the initial configuration the stack is empty,and the buffer contains the words of the sentencefollowed by an artificial ROOT token, as sug-gested by Ballesteros and Nivre (2013).
In the fi-nal configuration the buffer is empty and the stackcontains the ROOT token.There are four parsing actions (Shift, Left-Arc,Right-Arc and Reduce, abbreviated as S,L,R,D re-spectively) that manipulate stack and buffer items.The Shift action pops the first item from the bufferand pushes it on the stack (the Shift action has anatural precondition that the buffer is not empty,as well as a precondition that ROOT can only bepushed to an empty stack).
The Right-Arc actionis similar to the Shift action, but it also adds adependency arc (S0, B0), with the current top ofthe stack as the head of the newly pushed item(the Right action has an additional preconditionthat the stack is not empty).1 The Left-Arc actionadds a dependency arc (B0, S0) with the first itemin the buffer as the head of the top of the stack,and pops the stack (with a precondition that thestack and buffer are not empty, and that S0 is notassigned a head yet).
Finally, the Reduce actionpops the stack, with a precondition that the stackis not empty and that S0 is already assigned a head.1For labelled dependency parsing, the Right-Arc andLeft-Arc actions are parameterized by a label L such that theaction RightL adds an arc (S0, L,B0), similarly for LeftL.2.1 MonotonictyThe preconditions of the Left-Arc and Reduce ac-tions ensure that every word is assigned exactlyone head, resulting in a well-formed parse tree.The single head constraint is enforced by ensur-ing that once an action has been performed, sub-sequent actions must be consistent with it.
We re-fer to this consistency as the monotonicity of thesystem.Due to monotonicity, there is a natural pair-ing between the Right-Arc and Reduce actionsand the Shift and Left-Arc actions: a word whichis pushed into the stack by Right-Arc must bepopped using Reduce, and a word which is pushedby Shift action must be popped using Left-Arc.
Asa consequence of this pairing, a Right-Arc movedetermines that the head of the pushed token mustbe to its left, while a Shift moves determines ahead to its right.
Crucially, the decision whetherto Right-Arc or Shift is often taken in a state ofmissing information regarding the continuation ofthe sentence, forcing an incorrect head assignmenton a subsequent move.Consider a sentence pair such as (a)?I saw Jackand Jill?
/ (b)?I saw Jack and Jill fall?.
In (a), ?Jackand Jill?
is the NP object of ?saw?, while in (b) it isa subject of the embedded verb ?fall?.
The mono-tonic arc-eager parser has to decide on an analysisas soon as it sees ?saw?
on the top of the stack and?Jack?
at the front of the buffer, without access tothe disambiguating verb ?fall?.In what follows, we suggest a non-monotonicvariant of the Arc-Eager transition system, allow-ing the parser to recover from the incorrect headassignments which are forced by an incorrect res-olution of a Shift/Right-Arc ambiguity.3 The Non-Monotonic Arc-Eager SystemThe Arc-Eager transition system (Nivre et al2004) has four moves.
Two of them create depen-dencies, two push a word from the buffer to thestack, and two remove an item from the stack:Push PopAdds dependency Right-Arc Left-ArcNo new dependency Shift ReduceEvery word in the sentence is pushed once andpopped once; and every word must have exactlyone head.
This creates two pairings, along the di-agonals: (S, L) and (R, D).
Either the push moveadds the head or the pop move does, but not bothand not neither.164I saw Jack and Jill fall RS L S R R D R D L R D L1 2 3 4 5 6 7 8 9 10 11 12I saw Jack and Jill fall R2 4 572 579Figure 1: State before and after a non-monotonic Left-Arc.
At move 9, fall is the first word of the buffer (markedwith an arrow), and saw and Jack are on the stack (circled).The arc created at move 4 was incorrect (in red).
Arcs arelabelled with the move that created them.
After move 9 (thelower state), the non-monotonic Left-Arc move has replacedthe incorrect dependency with a correct Left-Arc (in green).Thus in the Arc-Eager system the first move de-termines the corresponding second move.
In ournon-monotonic system the second move can over-write an attachment made by the first.
This changemakes the transition system non-monotonic, be-cause if the model decides on an incongruent pair-ing we will have to either undo or add a depen-dency, depending on whether we correct a priorRight-Arc, or a prior Shift.3.1 Non-monotonic Left-ArcFigure 1 shows a before-and-after view of a non-monotonic transition.
The sequence below thewords shows the transition history.
The words thatare circled in the upper and lower line are on thestack before and after the transition, respectively.The arrow shows the start of the buffer, and arcsare labelled with the move that added them.The parser began correctly by Shifting I andLeft-Arcing it to saw, which was then also Shifted.The mistake, made at Move 4, was to Right-ArcJack instead of Shifting it.The difficulty of this kind of a decision for anincremental parser is fundamental.
The leftwardcontext does not constrain the decision, and an ar-bitrary amount of text could separate Jack fromfall.
Eye-tracking experiments show that humansoften perform a saccade while reading such exam-ples (Frazier and Rayner, 1982).In moves 5-8 the parser correctly builds the restof the NP, and arrives at fall.
The monotonicityconstraints would force an incorrect analysis, hav-ing fall modify Jack or saw, or having saw modifyfall as an embedded verb with no arguments.I saw Jack and Jill RS L S S R D R D R D D L1 2 3 4 5 6 7 8 9 10 11 12I saw Jack and Jill R2 572 579Figure 2: State before and after a non-monotonic Reduce.After making the wrong push move at 4, at move 11 the parserhas Jack on the stack (circled), with only the dummy ROOTtoken left in the buffer.
A monotonic parser must determinis-tically Left-Arc Jack here to preserve the previous decision,despite the current state.
We remove this constraint, and in-stead assume that when the model selects Reduce for a head-less item, it is reversing the previous Shift/Right decision.
Weadd the appropriate arc, assigning the label that scored high-est when the Shift/Right decision was made.We allow Left-Arcs to ?clobber?
edges set byRight-Arcs if the model recommends it.
The pre-vious edge is deleted, and the Left-Arc proceedsas normal.
The effect of this is exactly as if themodel had correctly chosen Shift at move 4.
Wesimply give the model a second chance to makethe correct choice.3.2 Non-monotonic ReduceThe upper arcs in Figure 2 show a state resultingfrom the opposite error.
The parser has ShiftedJack instead of Right-Arcing it.
After building theNP the buffer is exhausted, except for the ROOTtoken, which is used to wrongly Left-Arc Jack asthe sentence?s head word.Instead of letting the previous choice lock us into the pair (Shift, Left-Arc), we let the later deci-sion reverse it to (Right-Arc, Reduce), if the parserhas predicted Reduce in spite of the signal from itsprevious decision.
In the context shown in Figure2, the correctness of the Reduce move is quite pre-dictable, once the choice is made available.When the Shift/Right-Arc decision is reversed,we add an arc between the top of the stack (S0)and the word preceding it (S1).
This is the arc thatwould have been created had the parser chosen toRight-Arc when it chose to Shift.
Since our idea isto reverse this mistake, we select the Right-Arc la-bel that the model scored most highly at that time.22An alternative approach to label assignment is to parame-terize the Reduce action with a label, similar to the Right-Arcand Left-Arc actions, and let that label override the previ-ously predicted label.
This would allow the parser to con-165To summarize, our Non-Monotnonic Arc-Eager system differs from the monotonicArc-Eager system by:?
Changing the Left-Arc action by removingthe precondition that S0 does not have a head,and updating the dependency arcs such previ-ously derived arcs having S0 as a dependentare removed from the arcs set.?
Changing the Reduce action by removing theprecondition that S0 has a head, and updatingthe dependency arcs such that if S0 does nothave a head, S1 is assigned as the head of S0.4 Why have two push moves?We have argued above that it is better to trust thesecond decision that the model makes, rather thanusing the first decision to determine the second.If this is the case, is the first decision entirely re-dundant?
Instead of defining how pop moves cancorrect Shift/Right-Arc mistakes, we could insteadeliminate the ambiguity.
There are two possibili-ties: Shift every token, and create all Right-Arcsvia Reduce; or Right-Arc every token, and replacethem with Left-Arcs where necessary.Preliminary experiments on the developmentdata revealed a problem with these approaches.
Inmany cases the decision whether to Shift or Right-Arc is quite clear, and its result provides usefulconditioning context to later decisions.
The in-formation that determined those decisions is neverlost, but saving all of the difficulty for later is nota very good structured prediction strategy.As an example of the problem, if the Shift moveis eliminated, about half of the Right-Arcs createdwill be spurious.
All of these arcs will be assignedlabels making important features uselessly noisy.In the other approach, we avoid creating spuriousarcs, but the model does not predict whether S0 isattached to S1, or what the label would be, and wemiss useful features.The non-monotonic transition system we pro-pose does not have these problems.
The modellearns to make Shift vs. Right-Arc decisions asnormal, and conditions on them ?
but withoutcommitting to them.dition its label decision on the new context, which was suf-ficiently surprising to change its move prediction.
For effi-ciency and simplicity reasons, we chose instead to trust thelabel the model proposed when the reduced token was ini-tially pushed into the stack.
This requires an extra vector oflabels to be stored during parsing.5 Dynamic OraclesAn essential component when training atransition-based parser is an oracle which,given a gold-standard tree, dictates the sequenceof moves a parser should make in order to deriveit.
Traditionally, these oracles are defined as func-tions from trees to sequences, mapping a gold treeto a single sequence of actions deriving it, evenif more than one sequence of actions derives thegold tree.
We call such oracles static.
Recently,Goldberg and Nivre (2012) introduced the conceptof a dynamic oracle, and presented a concrete ora-cle for the arc-eager system.
Instead of mappinga gold tree to a sequence of actions, the dynamicoracle maps a ?configuration, gold tree?
pair to aset of optimal transitions.
More concretely, thedynamic oracle presented in Goldberg and Nivre(2012) maps ?action, configuration, tree?
tuplesto an integer, indicating the number of gold arcsin tree that can be derived from configurationby some sequence of actions, but could not bederived after applying action to the configuration.There are two advantages to this.
First, theability to label any configuration, rather than onlythose along a single path to the gold-standardderivation, allows much better training data to begenerated.
States come with realistic histories, in-cluding errors ?
a critical point for the currentwork.
Second, the oracle accounts for spuriousambiguity correctly, as it will label multiple ac-tions as correct if the optimal parses resulting fromthem are equally accurate.In preliminary experiments in which we trainedthe parser using the static oracle but allowed thenon-monotonic repair operations during parsing,we found that the the repair moves yielded no im-provement.
This is because the static oracle doesnot generate any examples of the repair moves dur-ing training, causing the parser to rarely predictthem in test time.
We will first describe the Arc-Eager dynamic oracle, and then define dynamicoracles for the non-monotonic transition systemswe present.5.1 Monotonic Arc-Eager Dynamic OracleWe now briefly describe the dynamic oracle for thearc-eager system.
For more details, see Goldbergand Nivre (2012).
The oracle is computed by rea-soning about the arcs which are reachable from agiven state, and counting the number of gold arcswhich will no longer be reachable after applying a166given transition at a given state.
3The reasoning is based on the observations thatin the arc-eager system, new arcs (h, l,m) can bederived iff the following conditions hold:(a) There is no existing arc (h?, l?,m) such thath?
6= h, and (b) Either both h and m are on thebuffer, or one of them is on the buffer and the otheris on the stack.
In other words:(a) once a word acquires a head (in a Left-Arc orRight-Arc transition) it loses the ability to acquireany other head.
(b) once a word is moved from the buffer to thestack (Shift or Right-Arc) it loses the ability to ac-quire heads that are currently on the stack, as wellas dependents that are currently on the stack andare not yet assigned a head.4(c) once a word is removed from the stack (Left-Arc or Reduce) it loses the ability to acquire anydependents on the buffer.Based on these observations, Goldberg and Nivre(2012) present an oracle C(a, c, t) for the mono-tonic arc-eager system, computing the number ofarcs in the gold tree t that are reachable from aparser?s configuration c and are no longer reach-able from the configuration a(c) resulting from theapplication of action a to configuration c.5.2 Non-monotonic Dynamic OraclesGiven the oracle C(a, c, t) for the monotonic sys-tem, we adapt it to a non-monotonic variant byconsidering the changes from the monotonic to thenon-monotonic system, and adding ?
terms ac-cordingly.
We define three novel oracles: CNML,CNMD and CNML+D for systems with a non-monotonic Left-Arc, Reduce or both.CNML(a, c, t) = C(a, c, t) +?NML(a, c, t)CNMD(a, c, t) = C(a, c, t) +?NMD(a, c, t)CNML+D(a, c, t) = C(a, c, t) +?NML(a, c, t)+?NMD(a, c, t)The terms ?NML and ?NMD reflect the scoreadjustments that need to be done to the arc-eageroracle due to the changes of the Left-Arc and Re-duce actions, respectively, and are detailed below.3The correctness of the oracle is based on a property ofthe arc-eager system, stating that if a set of arcs which can beextended to a projective tree can be individually derived froma given configuration, then a projective tree containing all ofthe arcs in the set is also derivable from the same configura-tion.
This same property holds also for the non-monotonicvariants we propose.4The condition that the words on the stack are not yet as-signed a head is missing from (Goldberg and Nivre, 2012)Changes due to non-monotonic Left-Arc:?
?NML(RIGHTARC, c, t): The cost of Right-Arc is decreased by 1 if the gold head of B0 ison the buffer (because B0 can still acquire itscorrect head later with a Left-Arc action).
Itis increased by 1 for any word w on the stacksuch that B0 is the gold parent of w and wis assigned a head already (in the monotonicoracle, this cost was taken care of when theword was assigned an incorrect head.
In thenon-monotonic variant, this cost is delayed).?
?NML(REDUCE, c, t): The cost of Reduce isincreased by 1 if the gold head of S0 is on thebuffer, because removing S0 from the stackprecludes it from acquiring its correct headlater on with a Left-Arc action.
(This cost ispaid for in the monotonic version when S0acquired its incorrect head).?
?NML(LEFTARC, c, t): The cost of Left-Arc is increased by 1 if S0 is already assignedto its gold parent.
(This situation is blockedby a precondition in the monotonic case).The cost is also increased if S0 is assignedto a non-gold parent, and the gold parent isin the buffer, but not B0.
(As a future non-monotonic Left-Arc is prevented from settingthe correct head.)?
?NML(SHIFT, c, gold): The cost of Shift isincreased by 1 for any word w on the stacksuch that B0 is the gold parent of w and w isassigned a head already.
(As in Right-Arc, inthe monotonic oracle, this cost was taken careof when w was assigned an incorrect head.
)Changes due to non-monotonic Reduce:?
?NMD(SHIFT, c, gold): The cost of Shift isdecreased by 1 if the gold head of B0 is S0(Because this arc can be added later on witha non-monotonic Reduce action).?
?NMD(LEFTARC, c, gold): The cost ofLeft-Arc is increased by 1 if S0 is not as-signed a head, and the gold head of S0 isS1 (Because this precludes adding the correctarc with a Reduce of S0 later).?
?NMD(REDUCE, c, gold) = 0.
While it mayseem that a change to the cost of a Reduce ac-tion is required, in fact the costs of the mono-tonic system hold here, as the head of S0 is167predetermined to be S1.
The needed adjust-ments are taken care of in Left-Arc and Shiftactions.5?
?NMD(RIGHTARC, c, gold) = 06 Applying the Oracles in TrainingOnce the dynamic-oracles for the non-monotonicsystem are defined, we could in principle just plugthem in the perceptron-based training proceduredescribed in Goldberg and Nivre (2012).
How-ever, a tacit assumption of the dynamic-oracles isthat all paths to recovering a given arc are treatedequally.
This assumption may be sub-optimalfor the purpose of training a parser for a non-monotonic system.In Section 4, we explained why removing theambiguity between Shift and Right-Arcs alto-gether was an inferior strategy.
Failing to discrim-inate between arcs reachable by monotonic andnon-monotonic paths does just that, so this oracledid not perform well in preliminary experimentson the development data.Instead, we want to learn a model that will offerits best prediction of Shift vs. Right-Arc, whichwe expect to usually be correct.
However, in thosecases where the model does make the wrong de-cision, it should have the ability to later over-turnthat decision, by having an unconstrained choiceof Reduce vs. Left-Arc.In order to correct for that, we don?t use thenon-monotonic oracles directly when training theparser, but instead train the parser using both themonotonic and non-monotonic oracles simultane-ously by combining their judgements: while wealways prefer zero-cost non-monotonic actions tomonotonic-actions with non-zero cost, if the non-monotonic oracle assigns several actions a zero-cost, we prefer to follow those actions that are alsoassigned a zero-cost by the monotonic oracle, asthese actions lead to the best outcome without re-lying on a non-monotonic (repair) operation downthe road.7 ExperimentsWe base our experiments on the parser describedby Goldberg and Nivre (2012).
We began by im-plementing their baseline system, a standard Arc-Eager parser using an averaged Perceptron learnerand the extended feature set described by Zhang5If using a labeled reduce transition, the label assignmentcosts should be handled here.Stanford MALTW S W SUnlabelled AttachmentBaseline (G&N-12) 91.2 42.0 90.9 39.7NM L 91.4 43.1 91.0 40.1NM D 91.4 42.8 91.1 41.2NM L+D 91.6 43.3 91.3 41.5Labelled AttachmentBaseline (G&N-12) 88.7 31.8 89.7 36.6NM L 89.0 32.5 89.8 36.9NM D 88.9 32.3 89.9 37.7NM L+D 89.1 32.7 90.0 37.9Table 1: Development results on WSJ 22.
Both non-monotonic transitions bring small improvements in per-token(W) and whole sentence (S) accuracy, and the improvementsare additive.and Nivre (2011).
We follow Goldberg and Nivre(2012) in training all models for 15 iterations, andshuffling the sentences before each iteration.Because the sentence ordering affects themodel?s accuracy, all results are averaged fromscores produced using 20 different random seeds.The seed determines how the sentences are shuf-fled before each iteration, as well as when to fol-low an optimal action and when to follow a non-optimal action during training.
The Wilcoxonsigned-rank test was used for significance testing.A train/dev/test split of 02-21/22/23 of the PennTreebank WSJ (Marcus et al 1993) was used forall models.
The data was converted into Stan-ford dependencies (de Marneffe et al 2006) withcopula-as-head and the original PTB noun-phrasebracketing.
We also evaluate our models on de-pendencies created by the PENN2MALT tool, toassist comparison with previous results.
Automat-ically assigned POS tags were used during training,to match the test data more closely.
6 We also eval-uate the non-monotonic transitions on the CoNLL2007 multi-lingual data.8 Results and analysisTable 1 shows the effect of the non-monotonictransitions on labelled and unlabelled attachmentscore on the development data.
All results are av-erages from 20 models trained with different ran-dom seeds, as the ordering of the sentences at eachiteration of the Perceptron algorithm has an effecton the system?s accuracy.
The two non-monotonictransitions each bring small but statistically signif-icant improvements that are additive when com-bined in the NM L+D system.
The result is stable6We thank Yue Zhang for supplying the POS-tagged filesused in the Zhang and Nivre (2011) experiments.168across both dependency encoding schemes.Frequency analysis.
Recall that there are two popmoves available: Left-Arc and Reduce.
The Left-Arc is considered non-monotonic if the top of thestack has a head specified, and the Reduce moveis considered non-monotonic if it does not.
Howoften does the parser select monotonic and non-monotonic pop moves, and how often is its deci-sion correct?In Table 2, the True Positive column shows howoften non-monotonic transitions were used to addgold standard dependencies.
The False Positivecolumn shows how often they were used incor-rectly.
The False Negative column shows howoften the parser missed a correct non-monotonictransition, and the True Negative column showshow often the monotonic alternative was correctlypreferred (e.g.
the parser correctly chose mono-tonic Reduce in place of non-monotonic Left-Arc).
Punctuation dependencies were excluded.The current system has high precision but lowrecall for repair operations, as they are relativelyrare in the gold-standard.
While we alreadysee improvements in accuracy, the upper boundachievable by the non-monotonic operations ishigher, and we hope to approach it in the futureusing improved learning techniques.Linguistic analysis.
To examine what construc-tions were being corrected, we looked at the fre-quencies of the labels being introduced by thenon-monotonic moves.
We found that there weretwo constructions being commonly repaired, anda long-tail of miscellaneous cases.The most frequent repair involved the mark la-bel.
This is assigned to conjunctions introducingsubordinate clauses.
For instance, in the sentenceResults were released after the market closed, theStanford scheme attaches after to closed.
Theparser is misled into greedily attaching after to re-leased here, as that would be correct if after were apreposition, as in Results were released after mid-night.
This construction was repaired 33 times, 13where the initial decision was mark, and 21 timesthe other way around.
The other commonly re-paired construction involved greedily attaching anobject that was actually the subject of a comple-ment clause, e.g.
NCNB corp. reported net incomedoubled.
These were repaired 19 times.WSJ evaluation.
Table 3 shows the final testresults.
While still lagging behind search basedparsers, we push the boundaries of what can beTP FP TN FNLeft-Arc 60 14 18,466 285Reduce 52 26 14,066 250Total 112 40 32,532 535Table 2: True/False positive/negative rates for the predic-tion of the non-monotonic transitions.
The non-monotonictransitions add correct dependencies 112 times, and produceworse parses 40 times.
535 opportunities for non-monotonictransitions were missed.System O Stanford Penn2MaltLAS UAS LAS UASK&C 10 n3 ?
?
?
93.00Z&N 11 nk 91.9 93.5 91.8 92.9G&N 12 n 88.72 90.96 ?
?Baseline(G&N-12) n 88.7 90.9 88.7 90.6NM L+D n 88.9 91.1 88.9 91.0Table 3: WSJ 23 test results, with comparison against thestate-of-the-art systems from the literature of different run-times.
K&C 10=Koo and Collins (2010); Z&N 11=Zhangand Nivre (2011); G&N 12=Goldberg and Nivre (2012).achieved with a purely greedy system, with a sta-tistically significant improvement over G&N 12.CoNLL 2007 evaluation.
Table 4 shows the ef-fect of the non-monotonic transitions across theten languages in the CoNLL 2007 data sets.
Statis-tically significant improvements in accuracy wereobserved for five of the ten languages.
The accu-racy improvement on Hungarian and Arabic didnot meet our significance threshold.
The non-monotonic transitions did not decrease accuracysignificantly on any of the languages.9 Related WorkOne can view our non-monotonic parsing systemas adding ?repair?
operations to a greedy, deter-ministic parser, allowing it to undo previous de-cisions and thus mitigating the effect of incorrectparsing decisions due to uncertain future, whichis inherent in greedy left-to-right transition-basedparsers.
Several approaches have been taken to ad-dress this problem, including:Post-processing Repairs (Attardi and Ciaramita,2007; Hall and Nova?k, 2005; Inokuchi and Ya-maoka, 2012) Closely related to stacking, this lineof work attempts to train classifiers to repair at-tachment mistakes after a parse is proposed bya parser by changing head attachment decisions.The present work differs from these by incorporat-ing the repair process into the transition system.Stacking (Nivre and McDonald, 2008; Martinset al 2008), in which a second-stage parser runsover the sentence using the predictions of the firstparser as features.
In contrast our parser works in169System AR BASQ CAT CHI CZ ENG GR HUN ITA TURBaseline 83.4 76.2 91.5 82.3 78.8 87.9 81.2 77.6 83.8 78.0NM L+D 83.6 76.1 91.5 82.7 80.1 88.4 81.8 77.9 84.1 78.0Table 4: Multi-lingual evaluation.
Accuracy improved on Chinese, Czech, English, Greek and Italian (p < 0.001), trendedupward on Arabic and Hungarian (p < 0.005), and was unchanged on Basque, Catalan and Turkish (p > 0.4).a single, left-to-right pass over the sentence.Non-directional Parsing The EasyFirst parserof Goldberg and Elhadad (2010) tackles similarforms of ambiguities by dropping the Shift actionaltogether, and processing the sentence in an easy-to-hard bottom-up order instead of left-to-right,resulting in a greedy but non-directional parser.The indeterminate processing order increases theparser?s runtime from O(n) to O(n log n).
In con-trast, our parser processes the sentence incremen-tally, and runs in a linear time.Beam Search An obvious approach to tacklingambiguities is to forgo the greedy nature of theparser and instead to adopt a beam search (Zhangand Clark, 2008; Zhang and Nivre, 2011) or adynamic programming (Huang and Sagae, 2010;Kuhlmann et al 2011) approach.
While these ap-proaches are very successful in producing high-accuracy parsers, we here explore what can beachieved in a strictly deterministic system, whichresults in much faster and incremental parsing al-gorithms.
The use of non-monotonic transitions inbeam-search parser is an interesting topic for fu-ture work.10 Conclusion and future workWe began this paper with the observation thatbecause the Arc-Eager transition system (Nivreet al 2004) attaches a word to its governor ei-ther when the word is pushed onto the stack orwhen it is popped off the stack, monotonicity (plusthe ?tree constraint?
that a word has exactly onegovernor) implies that a word?s push-move de-termines its associated pop-move.
In this paperwe suggest relaxing the monotonicity constraintto permit the pop-move to alter existing attach-ments if appropriate, thus breaking the 1-to-1 cor-respondence between push-moves and pop-moves.This permits the parser to correct some early in-correct attachment decisions later in the parsingprocess.
Adding additional transitions means thatin general there are multiple transition sequencesthat generate any given syntactic analysis, i.e., ournon-monotonic transition system generates spuri-ous ambiguities (note that the Arc-Eager transitionsystem on its own generates spurious ambiguities).As we explained in the paper, with the greedy de-coding used here additional spurious ambiguity isnot necessarily a draw-back.The conventional training procedure fortransition-based parsers uses a ?static?
oraclebased on ?gold?
parses that never predicts anon-monotonic transition, so it is clearly notappropriate here.
Instead, we use the incrementalerror-based training procedure involving a ?dy-namic?
oracle proposed by Goldberg and Nivre(2012), where the parser is trained to predict thetransition that will produce the best-possible anal-ysis from its current configuration.
We explainedhow to modify the Goldberg and Nivre oracle soit predicts the optimal moves, either monotonic ornon-monotonic, from any configuration, and usethis to train an averaged perceptron model.When evaluated on the standard WSJ trainingand test sets we obtained a UAS of 91.1%, whichis a 0.2% improvement over the already state-of-the-art baseline of 90.9% that is obtained with theerror-based training procedure of Goldberg andNivre (2012).
On the CoNLL 2007 datasets, ac-curacy improved significantly on 5/10 languages,and did not decline significantly on any of them.Looking to the future, we believe that it wouldbe interesting to investigate whether adding non-monotonic transitions is beneficial in other parsingsystems as well, including systems that target for-malisms other than dependency grammars.
As weobserved in the paper, the spurious ambiguity thatnon-monotonic moves introduce may well be anadvantage in a statistical parser with an enormousstate-space because it provides multiple pathwaysto the correct analysis (of which we hope at leastone is navigable).We investigated a very simple kind of non-monotonic transition here, but of course it?s pos-sible to design transition systems with many moretransitions, including transitions that are explicitlydesigned to ?repair?
characteristic parser errors.
Itmight even be possible to automatically identifythe most useful repair transitions and incorporatethem into the parser.170AcknowledgmentsThe authors would like to thank the anony-mous reviewers for their valuable comments.This research was supported under the Aus-tralian Research Council?s Discovery Projectsfunding scheme (project numbers DP110102506and DP110102593).ReferencesStephen Abney and Mark Johnson.
1991.
Mem-ory requirements and local ambiguities of pars-ing strategies.
Journal of Psycholinguistic Re-search, 20(3):233?250.Giuseppe Attardi and Massimiliano Ciaramita.2007.
Tree revision learning for dependencyparsing.
In Human Language Technologies2007: The Conference of the North AmericanChapter of the Association for ComputationalLinguistics; Proceedings of the Main Confer-ence, pages 388?395.
Association for Compu-tational Linguistics, Rochester, New York.Miguel Ballesteros and Joakim Nivre.
2013.
Go-ing to the roots of dependency parsing.
Compu-tational Linguistics.
39:1.Marie-Catherine de Marneffe, Bill MacCartney,and Christopher D. Manning.
2006.
Generatingtyped dependency parses from phrase structureparses.
In Proceedings of the 5th InternationalConference on Language Resources and Evalu-ation (LREC).Lyn Frazier and Keith Rayner.
1982.
Making andcorrecting errors during sentence comprehen-sion: Eye movements in the analysis of struc-turally ambiguous sentences.
Cognitive Psy-chology, 14(2):178?210.Yoav Goldberg and Michael Elhadad.
2010.
Anefficient algorithm for easy-first non-directionaldependency parsing.
In Human Language Tech-nologies: The 2010 Annual Conference of theNorth American Chapter of the Associationfor Computational Linguistics (NAACL HLT),pages 742?750.Yoav Goldberg and Joakim Nivre.
2012.
A dy-namic oracle for arc-eager dependency parsing.In Proceedings of the 24th International Con-ference on Computational Linguistics (Coling2012).
Association for Computational Linguis-tics, Mumbai, India.Keith Hall and Vaclav Nova?k.
2005.
Correctivemodeling for non-projective dependency pars-ing.
In Proceedings of the 9th InternationalWorkshop on Parsing Technologies (IWPT),pages 42?52.Liang Huang and Kenji Sagae.
2010.
Dynamicprogramming for linear-time incremental pars-ing.
In Proceedings of the 48th Annual Meetingof the Association for Computational Linguis-tics (ACL), pages 1077?1086.Akihiro Inokuchi and Ayumu Yamaoka.
2012.Mining rules for rewriting states in a transition-based dependency parser for English.
In Pro-ceedings of COLING 2012, pages 1275?1290.The COLING 2012 Organizing Committee,Mumbai, India.Terry Koo and Michael Collins.
2010.
Efficientthird-order dependency parsers.
In Proceedingsof the 48th Annual Meeting of the Associationfor Computational Linguistics (ACL), pages 1?11.Marco Kuhlmann, Carlos Go?mez-Rodr?
?guez, andGiorgio Satta.
2011.
Dynamic program-ming algorithms for transition-based depen-dency parsers.
In Proceedings of the 49th An-nual Meeting of the Association for Computa-tional Linguistics: Human Language Technolo-gies - Volume 1, HLT ?11, pages 673?682.
Asso-ciation for Computational Linguistics, Strouds-burg, PA, USA.Mitchell P. Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1993.
Building alarge annotated corpus of English: The PennTreebank.
Computational Linguistics, 19:313?330.Andre?
Filipe Martins, Dipanjan Das, Noah A.Smith, and Eric P. Xing.
2008.
Stacking de-pendency parsers.
In Proceedings of the Con-ference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 157?166.Joakim Nivre.
2003.
An efficient algorithm forprojective dependency parsing.
In Proceedingsof the 8th International Workshop on ParsingTechnologies (IWPT), pages 149?160.Joakim Nivre.
2008.
Algorithms for determinis-tic incremental dependency parsing.
Computa-tional Linguistics, 34:513?553.Joakim Nivre, Johan Hall, and Jens Nilsson.2004.
Memory-based dependency parsing.
In171Hwee Tou Ng and Ellen Riloff, editors, HLT-NAACL 2004 Workshop: Eighth Conferenceon Computational Natural Language Learn-ing (CoNLL-2004), pages 49?56.
Associationfor Computational Linguistics, Boston, Mas-sachusetts, USA.Joakim Nivre and Ryan McDonald.
2008.
In-tegrating graph-based and transition-based de-pendency parsers.
In Proceedings of the 46thAnnual Meeting of the Association for Compu-tational Linguistics (ACL), pages 950?958.Mark Steedman.
2000.
The Syntactic Process.MIT Press, Cambridge, MA.Yue Zhang and Stephen Clark.
2008.
A tale of twoparsers: Investigating and combining graph-based and transition-based dependency parsing.In Proceedings of the 2008 Conference on Em-pirical Methods in Natural Language Process-ing, pages 562?571.
Association for Computa-tional Linguistics, Honolulu, Hawaii.Yue Zhang and Joakim Nivre.
2011.
Transition-based dependency parsing with rich non-localfeatures.
In Proceedings of the 49th AnnualMeeting of the Association for ComputationalLinguistics: Human Language Technologies,pages 188?193.
Association for ComputationalLinguistics, Portland, Oregon, USA.172
