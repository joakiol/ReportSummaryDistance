Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1245?1254, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsJoint Learning for Coreference Resolution with Markov LogicYang Song1, Jing Jiang2, Wayne Xin Zhao3, Sujian Li1, Houfeng Wang11Key Laboratory of Computational Linguistics (Peking University) Ministry of Education,China2School of Information Systems, Singapore Management University, Singapore3School of Electronics Engineering and Computer Science, Peking University, China{ysong, lisujian, wanghf}@pku.edu.cn, jingjiang@smu.edu.sg, batmanfly@gmail.comAbstractPairwise coreference resolution models mustmerge pairwise coreference decisions to gen-erate final outputs.
Traditional merging meth-ods adopt different strategies such as the best-first method and enforcing the transitivity con-straint, but most of these methods are usedindependently of the pairwise learning meth-ods as an isolated inference procedure at theend.
We propose a joint learning model whichcombines pairwise classification and mentionclustering with Markov logic.
Experimen-tal results show that our joint learning sys-tem outperforms independent learning sys-tems.
Our system gives a better performancethan all the learning-based systems from theCoNLL-2011 shared task on the same dataset.Compared with the best system from CoNLL-2011, which employs a rule-based method,our system shows competitive performance.1 IntroductionThe task of noun phrase coreference resolution is todetermine which mentions in a text refer to the samereal-world entity.
Many methods have been pro-posed for this problem.
Among them the mention-pair model (McCarthy and Lehnert, 1995) is one ofthe most influential ones and can achieve the state-of-the-art performance (Bengtson and Roth, 2008).The mention-pair model splits the task into threeparts: mention detection, pairwise classification andmention clustering.
Mention detection aims to iden-tify anaphoric noun phrases, including proper nouns,common noun phrases and pronouns.
Pairwise clas-sification takes a pair of detected anaphoric nounphrase candidates and determines whether they re-fer to the same entity.
Because these classificationdecisions are local, they do not guarantee that can-didate mentions are partitioned into clusters.
There-fore a mention clustering step is needed to resolveconflicts and generate the final mention clusters.Much work has been done following the mention-pair model (Soon et al 2001; Ng and Cardie, 2002).In most work, pairwise classification and mentionclustering are done sequentially.
A major weak-ness of this approach is that pairwise classificationconsiders only local information, which may not besufficient to make correct decisions.
One way toaddress this weakness is to jointly learn the pair-wise classification model and the mention cluster-ing model.
This idea has been explored to someextent by McCallum and Wellner (2005) using con-ditional undirected graphical models and by Finleyand Joachims (2005) using an SVM-based super-vised clustering method.In this paper, we study how to use a differentlearning framework, Markov logic (Richardson andDomingos, 2006), to learn a joint model for bothpairwise classification and mention clustering un-der the mention-pair model.
We choose Markovlogic because of its appealing properties.
Markovlogic is based on first-order logic, which makesthe learned models readily interpretable by humans.Moreover, joint learning is natural under the Markovlogic framework, with local pairwise classificationand global mention clustering both formulated asweighted first-order clauses.
In fact, Markov logichas been previously used by Poon and Domingos(2008) for coreference resolution and achieved good1245results, but it was used for unsupervised coreferenceresolution and the method was based on a differentmodel, the entity-mention model.More specifically, to combine mention cluster-ing with pairwise classification, we adopt the com-monly used strategies (such as best-first clusteringand transitivity constraint), and formulate them asfirst-order logic formulas under the Markov logicframework.
Best-first clustering has been previouslystudied by Ng and Cardie (2002) and Bengtson andRoth (2008) and found to be effective.
Transitivityconstraint has been applied to coreference resolutionby Klenner (2007) and Finkel and Manning (2008),and also achieved good performance.We evaluate Markov logic-based method on thedataset from CoNLL-2011 shared task.
Our ex-periment results demonstrate the advantage of jointlearning of pairwise classification and mention clus-tering over independent learning.
We examinebest-first clustering and transitivity constraint in ourmethods, and find that both are very useful for coref-erence resolution.
Compared with the state of theart, our method outperforms a baseline that repre-sents a typical system using the mention-pair model.Our method is also better than all learning systemsfrom the CoNLL-2011 shared task based on the re-ported performance.
Even with the top system fromCoNLL-2011, our performance is still competitive.In the rest of this paper, we first describe a stan-dard pairwise coreference resolution system in Sec-tion 2.
We then present our Markov logic model forpairwise coreference resolution in Section 3.
Exper-imental results are given in Section 4.
Finally wediscuss related work in Section 5 and conclude inSection 6.2 Standard Pairwise CoreferenceResolutionIn this section, we describe standard learning-basedframework for pairwise coreference resolution.
Themajor steps include mention detection, pairwiseclassification and mention clustering.2.1 Mention DetectionFor mention detection, traditional methods includelearning-based and rule-based methods.
Which kindof method to choose depends on specific dataset.
Inthis paper, we first consider all the noun phrasesin the given text as candidate mentions.
With-out gold standard mention boundaries, we use awell-known preprocessing tool from Stanford?s NLPgroup1 to extract noun phrases.
After obtaining allthe extracted noun phrases, we also use a rule-basedmethod to remove some erroneous candidates basedon previous studies (e.g.
Lee et al(2011), Uryupinaet al(2011)).
Some examples of these erroneouscandidates include stop words (e.g.
uh, hmm), webaddresses (e.g.
http://www.google.com),numbers (e.g.
$9,000) and pleonastic ?it?
pronouns.2.2 Pairwise ClassificationFor pairwise classification, traditional learning-based methods usually adopt a classification modelsuch as maximum entropy models and support vec-tor machines.
Training instances (i.e.
positive andnegative mention pairs) are constructed from knowncoreference chains, and features are defined to rep-resent these instances.In this paper, we build a baseline system that usesmaximum entropy models as the classification algo-rithm.
For generation of training instances, we fol-low the method of Bengtson and Roth (2008).
Foreach predicted mention m, we generate a positivemention pair between m and its closest precedingantecedent, and negative mention pairs by pairing mwith each of its preceding predicted mentions whichare not coreferential with m. To avoid having toomany negative instances, we impose a maximumsentence distance between the two mentions whenconstructing mention pairs.
This is based on the in-tuition that for each anaphoric mention, its preced-ing antecedent should appear quite near it, and mostcoreferential mention pairs which have a long sen-tence distance can be resolved using string match-ing.
During the testing phase, we generate men-tion pairs for each mention candidate with each ofits preceding mention candidates and use the learnedmodel to make coreference decisions for these men-tion pairs.
We also impose the sentence distanceconstraint and use string matching for mention pairswith a sentence distance exceeding the threshold.1http://nlp.stanford.edu/software/corenlp.shtml12462.3 Mention ClusteringAfter obtaining the coreferential results for all men-tion pairs, some clustering method should be used togenerate the final output.
One strategy is the single-link method, which links all the mention pairs thathave a prediction probability higher than a thresholdvalue.
Two other alternative methods are the best-first clustering method and clustering with the tran-sitivity constraint.
Best-first clustering means thatfor each candidate mention m, we select the bestone from all its preceding candidate mentions basedon the prediction probabilities.
A threshold valueis given to filter out those mention pairs that have alow probability to be coreferential.
Transitivity con-straint means that if a and b are coreferential andb and c are coreferential, then a and c must also becoreferential.
Previous work has found that best-firstclustering and transitivity constraint-based cluster-ing are better than the single-link method.
Finallywe remove all the singleton mentions.3 Markov Logic for Pairwise CoreferenceResolutionIn this section, we present our method for jointlearning of pairwise classification and mention clus-tering using Markov logic.
For mention detection,training instance generation and postprocessing, ourmethod follows the same procedures as described inSection 2.
In what follows, we will first describethe basic Markov logic networks (MLN) framework,and then introduce the first-order logic formulas weuse in our MLN including local formulas and globalformulas which perform pairwise classification andmention clustering respectively.
Through this way,these two isolated parts are combined together, andjoint learning and inference can be performed in asingle framework.
Finally we present inference andparameter learning methods.3.1 Markov Logic NetworksMarkov logic networks combine Markov networkswith first-order logic (Richardson and Domingos,2006; Riedel, 2008).
A Markov logic network con-sists of a set of first-order clauses (which we will re-fer to as formulas in the rest of the paper) just like infirst-order logic.
However, different from first-orderlogic where a formula represents a hard constraint,in an MLN, these constraints are softened and theycan be violated with some penalty.
An MLN Mis therefore a set of weighted formulas {(?i, wi)}i,where ?i is a first order formula andwi is the penalty(the formula?s weight).
These weighted formulasdefine a probability distribution over sets of groundatoms or so-called possible worlds.
Let y denote apossible world, then we define p(y) as follows:p(y) = 1Zexp(?
(?i,wi)?Mwi?c?Cn?if?ic (y)).
(1)Here each c is a binding of free variables in ?i toconstants.
Each f?ic represents a binary feature func-tion that returns 1 if the ground formula we get byreplacing the free variables in ?i with the constantsin c under the given possible world y is true, and 0otherwise.
n?i denotes the number of free variablesof a formula ?i.
Cn?i is the set of all bindings for thefree variables in ?i.
Z is a normalization constant.This distribution corresponds to a Markov networkwhere nodes represent ground atoms and factors rep-resent ground formulas.Each formula consists of a set of first-order predi-cates, logical connectors and variables.
Take the fol-lowing formula as one example:(?i, wi) : headMatch(a, b)?
(a ?= b) ?
coref (a, b).The formula above indicates that if two differentcandidate mentions a and b have the same headword, then they are coreferential.
Here a and b arevariables which can represent any candidate men-tion, headMatch and coref are observed predicateand hidden predicate respectively.
An observedpredicate is one whose value is known from the ob-servations when its free variables are assigned someconstants.
A hidden predicate is one whose value isnot known from the observations.
From this exam-ple, we can see that headMatch is an observed pred-icate because we can check whether two candidatementions have the same head word.
coref is a hid-den predicate because this is something we wouldlike to predict.3.2 FormulasWe use two kinds of formulas for pairwise classi-fication and mention clustering, respectively.
For1247describing the attributes ofmimentionType(i,t) mi has mention type NAM(named entities), NOM(nominal) or PRO(pronouns).entityType(i,e) mi has entity type PERSON, ORG, GPE or UN...genderType(i,g) mi has gender type MALE, FEMALE, NEUTRAL or UN.numberType(i,n) mi has number type SINGULAR, PLURAL or UN.hasHead(i,h) mi has head word h, here h can represent all possible head words.firstMention(i) mi is the first mention in its sentence.reflexive(i) mi is reflexive.possessive(i) mi is possessive.definite(i) mi is definite noun phrase.indefinite(i) mi is indefinite noun phrase.demonstrative(i) mi is demonstrative.describing the attributes of relations betweenmj andmimentionDistance(j,i,m) Distance between mj and mi in mentions.sentenceDistance(j,i,s) Distance between mj and mi in sentences.bothMatch(j,i,b) Gender and number of both mj and mi match: AGREE YES, AGREE NOand AGREE UN).closestMatch(j,i,c) mj is the first agreement in number and gender when looking backwardfrom mi: CAGREE YES, CAGREE NO and CAGREE UN.exactStrMatch(j,i) Exact strings match between mj and mi.pronounStrMatch(j,i) Both are pronouns and their strings match.nopronounStrMatch(j,i) Both are not pronouns and their strings match.properStrMatch(j,i) Both are proper names and their strings match.headMatch(j,i) Head word strings match between mj and mi.subStrMatch(j,i) Sub-word strings match between mj and mi.animacyMatch(j,i) Animacy types match between mj and mi.nested(j,i) mj/i is included in mi/j .c command(j,i) mj/i C-Commands mi/j .sameSpeaker(j,i) mj and mi have the same speaker.entityTypeMatch(j,i) Entity types match between mj and mi.alias(j,i) mj/i is an alias of mi/j .srlMatch(j,i) mj and mi have the same semantic role.verbMatch(j,i) mj and mi have semantic role for the same verb.Table 1: Observed predicates.pairwise classification, because the decisions are lo-cal, we use a set of local formulas.
For mentionclustering, we use global formulas to implementbest-first clustering or transitivity constraint.
Wenaturally combine pairwise classification with men-tion clustering via local and global formulas in theMarkov logic framework, which is the essence of?joint learning?
in our work.3.2.1 Local FormulasA local formula relates any observed predicates toexactly one hidden predicate.
For our problem, wedefine a list of observed predicates to describe theproperties of individual candidate mentions and therelations between two candidate mentions, shown inTable 1.
For our problem, we have only one hiddenpredicate, i.e.
coref.
Most of our local formulas arefrom existing work (e.g.
Soon et al(2001), Ng andCardie (2002), Sapena et al(2011)).
They are listedin Table 2, where the symbol ?+?
indicates that forevery value of the variable preceding ?+?
there is aseparate weight for the corresponding formula.3.2.2 Global FormulasGlobal formulas are designed to add global con-straints for hidden predicates.
Since in our problemthere is only one hidden predicate, i.e.
coref, ourglobal formulas incorporate correlations among dif-ferent ground atoms of the coref predicates.
Next wewill show the best-first and transitivity global con-straints.
Note that we treat them as hard constraintsso we do not set any weights for these global formu-las.1248Lexical FeaturesmentionType(j,t1+) ?
mentionType(i,t2+) ?
exactStrMatch(j,i) ?
j ?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
pronounStrMatch (j,i) ?
j ?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
properStrMatch(j,i) ?
j ?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
nopronounStrMatch(j,i) ?
j ?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
headMatch(j,i) ?
j ?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
subStrMatch(j,i) ?
j ?= i ?
coref(j,i)hasHead(j,h1+) ?
hasHead(i,h2+) ?
j ?= i ?
coref(j,i)Grammatical FeaturesmentionType(j,t1+) ?
mentionType(i,t2+) ?
genderType(j,g1+) ?
genderType(i,g2+) ?
j ?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
numberType(j,n1+) ?
numberType(i,n2+) ?
j ?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
bothMatch(j,i,b+) ?
j ?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
closestMatch(j,i,c+) ?
j ?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
animacyMatch(j,i) ?
j ?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
nested(j,i) ?
j ?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
c command(j,i) ?
j ?= i ?
coref(j,i)(mentionType(j,t1+) ?
mentionType(i,t2+)) ?
j ?= i ?
coref(j,i)(reflexive(j) ?
reflexive(i)) ?
j ?= i ?
coref(j,i)(possessive(j) ?
possessive(i)) ?
j ?= i ?
coref(j,i)(definite(j) ?
definite(i)) ?
j ?= i ?
coref(j,i)(indefinite(j) ?
indefinite(i)) ?
j ?= i ?
coref(j,i)(demonstrative(j) ?
demonstrative(i)) ?
j ?= i ?
coref(j,i)Distance and position FeaturesmentionType(j,t1+) ?
mentionType(i,t2+) ?
sentenceDistance(j,i,s+) ?
j ?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
mentionDistance (j,i,m+) ?
j ?= i ?
coref(j,i)(firstMention(j) ?
firstMention(i)) ?
j ?= i ?
coref(j,i)Semantic FeaturesmentionType(j,t1+) ?
mentionType(i,t2+) ?
alias(j,i) ?
j ?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
sameSpeaker(j,i) ?
j?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
entityTypeMatch(j,i) ?
j ?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
srlMatch(j,i) ?
j ?= i ?
coref(j,i)mentionType(j,t1+) ?
mentionType(i,t2+) ?
verbMatch(j,i) ?
j ?= i ?
coref(j,i)(entityType(j,e1+) ?
entityType(i,e2+)) ?
j ?= i ?
coref(j,i)Table 2: Local Formulas.Best-First constraint:coref(j, i) ?
?coref(k, i) ?j, k < i(k ?= j) (2)Here we assume that coref(j,i) returns true if can-didate mentions j and i are coreferential and falseotherwise.
Therefore for each candidate mention i,we should only select at most one candidate mentionj to return true for the predicate coref(j,i) from all itspreceding candidate mentions.Transitivity constraint:coref(j, k)?coref(k, i)?j < k < i ?
coref(j, i) (3)coref(j, k)?coref(j, i)?j < k < i ?
coref(k, i) (4)coref(j, i)?coref(k, i)?j < k < i ?
coref(j, k) (5)With the transitivity constraint, it means for givenmentions j, k and i, if any two pairs of them arecoreferential, then the third pair of them should bealso coreferential.We use best-first clustering and transitivity con-straint in our joint learning model respectively.
De-tailed comparisons between them will be shown inSection 4.3.3 InferenceWe use MAP inference which is implemented by In-teger Linear Programming (ILP).
Its objective is tomaximize a posteriori probability as follows.
Herewe use x to represent all the observed ground atomsand y to represent the hidden ground atoms.
For-mally, we havey?
= argmaxyp(y|x) ?
argmaxys(y, x),wheres(y, x) =?
(?i,wi)?Mwi?c?Cn?if?ic (y, x).
(6)Each hidden ground atom can only takes a value ofeither 0 or 1.
And global formulas should be satis-fied as hard constraints when inferring the best y?.
So1249the problem can be easily solved using ILP.
Detailedintroduction about transforming groundMarkov net-works in Markov logic into an ILP problem can befound in (Riedel, 2008).3.4 Parameter LearningFor parameter learning, we employ the onlinelearner MIRA (Crammer and Singer, 2003), whichestablishes a large margin between the score of thegold solution and all wrong solutions to learn theweights.
This is achieved by solving the quadraticprogram as followsmin ?
wt ?wt?1 ?
.
(7)s.t.
s(yi, xi)?
s(y?, xi) ?
L(yi, y?)?y?
?= yi, (yi, xi) ?
DHere D = {(yi, xi)}Ni=1 represents N training in-stances (each instance represents one single docu-ment in the dataset) and t represents the number ofiterations.
In our problem, we adopt 1-best MIRA,which means that in each iteration we try to find wtwhich can guarantee the difference between the rightsolution yi and the best solution y?
(i.e.
the one withthe highest score s(y?, xi), equivalent to y?
in Section3.3)) is at least as big as the loss L(yi, y?
), whilechanging wt?1 as little as possible.
The number offalse ground atoms of coref predicate is selected asloss function in our experiments.
Hard global con-straints (i.e.
best-first clustering or transitivity con-straint) must be satisfied when inferring the best y?in each iteration, which can make learned weightsmore effective.4 ExperimentsIn this section, we will first describe the dataset andevaluation metrics we use.
We will then present theeffect of our joint learning method, and finally dis-cuss the comparison with the state of the art.4.1 Data SetWe use the dataset from the CoNLL-2011 sharedtask, ?Modeling Unrestricted Coreference inOntoNotes?
(Pradhan et al 2011)2.
It uses the En-glish portion of the OntoNotes v4.0 corpus.
Thereare three important differences between OntoNotes2http://conll.cemantix.org/2011/and another well-known coreference dataset fromACE.
First, OntoNotes does not label any singletonentity cluster, which has only one reference in thetext.
Second, only identity coreference is tagged inOntoNotes, but not appositives or predicate nomi-natives.
Third, ACE only considers mentions whichbelong to ACE entity types, whereas OntoNotesconsiders more entity types.
The shared task is toautomatically identify both entity coreference andevent coreference, although we only focus on entitycoreference in this paper.
We don?t assume thatgold standard mention boundaries are given.
So wedevelop a heuristic method for mention detection.See details in Section 2.1.The training set consists of 1674 documents fromnewswire, magazine articles, broadcast news, broad-cast conversations and webpages, and the develop-ment set consists of 202 documents from the samesource.
For training set, there are 101264 mentionsfrom 26612 entities.
And for development set, thereare 14291 mentions from 3752 entities (Pradhan etal., 2011).4.2 Evaluation MetricsWe use the same evaluation metrics as used inCoNLL-2011.
Specifically, for mention detection,we use precision, recall and the F-measure.
A men-tion is considered to be correct only if it matchesthe exact same span of characters in the annotationkey.
For coreference resolution, MUC (Vilain et al1995), B-CUBED (Bagga and Baldwin, 1998) andCEAF-E (Luo, 2005) are used for evaluation.
Theunweighted average F score of them is used to com-pare different systems.4.3 The Effect of Joint LearningTo assess the performance of our method, we set upseveral variations of our system to compare with thejoint learning system.
The MLN-Local system usesonly the local formulas described in Table 2 with-out any global constraints under the MLN frame-work.
By default, the MLN-Local system uses thesingle-link method to generate clustering results.The MLN-Local+BF system replaces the single-linkmethod with best-first clustering to infer mentionclustering results after learning the weights for allthe local formulas.
The MLN-Local+Trans sys-tem replaces the best-first clustering with transitivity1250System Mention Detection MUC B-cube CEAF AvgR P F R P F R P F R P F FMLN-Local 62.52 74.75 68.09 56.07 65.55 60.44 65.67 72.95 69.12 45.55 37.19 40.95 56.84MLN-Local+BF 65.74 73.2 69.27 56.79 64.08 60.22 65.71 74.18 69.69 47.29 40.53 43.65 57.85MLN-Local+Trans 68.49 70.32 69.40 57.16 60.98 59.01 66.97 72.90 69.81 46.96 43.34 45.08 57.97MLN-Joint(BF) 64.36 75.25 69.38 55.47 66.95 60.67 64.14 77.75 70.29 50.47 39.85 44.53 58.50MLN-Joint(Trans) 64.46 75.37 69.49 55.48 67.15 60.76 64.00 78.11 70.36 50.63 39.84 44.60 58.57Table 3: Comparison between different MLN-based systems, using 10-fold cross validation on the training dataset.constraint.
The MLN-Joint system is a joint modelfor both pairwise classification and mention cluster-ing.
It can combine either best-first clustering or en-forcing transitivity constraint with pairwise classifi-cation, and we denote these two variants of MLN-Joint as MLN-Joint(BF) and MLN-Joint(Trans) re-spectively.To compare the performance of the various sys-tems above, we use 10-fold cross validation onthe training dataset.
We empirically find that ourmethod has a fast convergence rate, to learn theMLN model, we set the number of iterations to be10.The performance of these compared systems isshown in Table 3.
To provide some context forthe performance of this task, we report the medianaverage F-score of the official results of CoNLL-2011, which is 50.12 (Pradhan et al 2011).
We cansee that MLN-Local achieves an average F-score of56.84, which is well above the median score.
Whenadding best-first or transitivity constraint whichis independent of pairwise classification, MLN-Local+BF and MLN-Local+Trans achieve better re-sults of 57.85 and 57.97.
Most of all, we can seethat the joint learning model (MLN-Joint(BF) orMLN-Joint(Trans)) significantly outperforms inde-pendent learning model (MLN-Local+BF or MLN-Local+Trans) no matter whether best-first clusteringor transitivity constraint is used (based on a paired 2-tailed t-test with p < 0.05) with the score of 58.50or 58.57, which shows the effectiveness of our pro-posed joint learning method.Best-first clustering and transitivity constraintare very useful in Markov logic framework, andboth MLN-Local and MLN-Joint benefit from them.For MLN-Joint, these two clustering methods re-sult in similar performance.
But actually, transi-tivity is harder than best-first, because it signifi-cantly increases the number of formulas for con-straints and slows down the learning process.
Inour experiments, we find that MLN-Joint(Trans)3 ismuch slower than MLN-Joint(BF).
Overall, MLN-Joint(BF) has a good trade-off between effectivenessand efficiency.4.4 Comparison with the State of the ArtIn order to compare our method with the state-of-the-art systems, we consider the following systems.We implemented a traditional pairwise coreferencesystem using Maximum Entropy as the base classi-fier and best-first clustering to link the results.
Weused the same set of local features in MLN-Joint.We refer to this system as MaxEnt+BF.
To replacebest-first clustering with transitivity constraint, wehave another system named as MaxEnt+Trans.
Wealso consider the best 3 systems from CoNLL-2011shared task.
Chang?s system uses ILP to performbest-first clustering after training a pairwise corefer-ence model.
Sapena?s system uses a relaxation label-ing method to iteratively perform function optimiza-tion for labeling each mention?s entity after learningthe weights for features under a C4.5 learner.
Lee?ssystem is a purely rule-based one.
They use a batteryof sieves by precision (from highest to lowest) to it-eratively choose antecedent for each mention.
Theyobtained the highest score in CoNLL-2011.Table 4 shows the comparisons of our system withthe state-of-the-art systems on the development setof CoNLL-2011.
From the results, we can see thatour joint learning systems are obviously better than3For MLN-Joint(Trans), not all training instances can belearnt in a reasonable amount of time, so we set up a time outthreshold of 100 seconds.
If the model cannot response in 100seconds for some training instance, we remove it from the train-ing set.1251System Mention Detection MUC B-cube CEAF AvgR P F R P F R P F R P F FMLN-Joint(BF) 67.33 72.94 70.02 58.03 64.05 60.89 67.11 73.88 70.33 47.6 41.92 44.58 58.60MLN-Joint(Trans) 67.28 72.88 69.97 58.00 64.10 60.90 67.12 74.13 70.45 47.70 41.96 44.65 58.67MaxEnt+BF 60.54 76.64 67.64 52.20 68.52 59.26 60.85 80.15 69.18 51.6 37.05 43.13 57.19MaxEnt+Trans 61.36 76.11 67.94 51.46 68.40 58.73 59.79 81.69 69.04 53.03 37.84 44.17 57.31Lee?s System - - - 57.50 59.10 58.30 71.00 69.20 70.10 48.10 46.50 47.30 58.60Sapena?s System 92.45 27.34 42.20 54.53 62.25 58.13 63.72 73.83 68.40 47.20 40.01 43.31 56.61Chang?s System - - 64.69 - - 55.8 - - 69.29 - - 43.96 56.35Table 4: Comparisons with state-of-the-art systems on the development dataset.MaxEnt+BF and MaxEnt+Trans.
They also out-perform the learning-based systems of Sapena et al(2011) and Chang et al(2011), and perform com-petitively with Lee?s system (Lee et al 2011).
Notethat Lee?s system is purely rule-based, while ourmethods are developed in a theoretically sound way,i.e., Markov logic framework.5 Related WorkSupervised noun phrase coreference resolution hasbeen extensively studied.
Besides the mention-pairmodel, two other commonly used models are theentity-mention model (Luo et al 2004; Yang et al2008) and ranking models (Denis and Baldridge,2008; Rahman and Ng, 2009).
Interested readerscan refer to the literature review by Ng (2010).Under the mention-pair model, Klenner (2007)and Finkel and Manning (2008) applied Integer Lin-ear Programming (ILP) to enforce transitivity on thepairwise classification results.
Chang et al(2011)used the same ILP technique to incorporate best-firstclustering and generate the mention clusters.
In allthese studies, however, mention clustering is com-bined with pairwise classification only at the infer-ence stage but not at the learning stage.To perform joint learning of pairwise classifi-cation and mention clustering, in (McCallum andWellner, 2005), each mention pair corresponds toa binary variable indicating whether the two men-tions are coreferential, and the dependence betweenthese variables is modeled by conditional undirectedgraphical models.
Finley and Joachims (2005) pro-posed a general SVM-based framework for super-vised clustering that learns item-pair similarity mea-sures, and applied the framework to noun phrasecoreference resolution.
In our work, we take a differ-ent approach and apply Markov logic.
As we haveshown in Section 3, given the flexibility of Markovlogic, it is straightforward to perform joint learningof pairwise classification and mention clustering.In recent years, Markov logic has been widelyused in natural language processing problems (Poonand Domingos, 2009; Yoshikawa et al 2009; Cheand Liu, 2010).
For coreference resolution, the mostnotable one is unsupervised coreference resolutionby Poon and Domingos (2008).
Poon and Domin-gos (2008) followed the entity-mention model whilewe follow the mention-pair model, which are quitedifferent approaches.
To seek good performance inan unsupervised way, Poon and Domingos (2008)highly rely on two important strong indicators:appositives and predicate nominatives.
However,OntoNotes corpus (state-of-art NLP data collection)on coreference layer for CoNLL-2011 has excludedthese two conditions of annotations (appositives andpredicate nominatives) from their judging guide-lines.
Compared with it, our methods are more ap-plicable for real dataset.
Huang et al(2009) usedMarkov logic to predict coreference probabilitiesfor mention pairs followed by correlation cluster-ing to generate the final results.
Although they alsoperform joint learning, at the inference stage, theystill make pairwise coreference decisions and clus-ter mentions sequentially.
Unlike their method, Weformulate the two steps into a single framework.Besides combining pairwise classification andmention clustering, there has also been some workthat jointly performs mention detection and coref-erence resolution.
Daume?
and Marcu (2005) de-veloped such a model based on the Learning as1252Search Optimization (LaSO) framework.
Rahmanand Ng (2009) proposed to learn a cluster-rankerfor discourse-new mention detection jointly withcoreference resolution.
Denis and Baldridge (2007)adopted an Integer Linear Programming (ILP) for-mulation for coreference resolution which modelsanaphoricity and coreference as a joint task.6 ConclusionIn this paper we present a joint learning method withMarkov logic which naturally combines pairwiseclassification and mention clustering.
Experimentalresults show that the joint learning method signifi-cantly outperforms baseline methods.
Our methodis also better than all the learning-based systems inCoNLL-2011 and reaches the same level of perfor-mance with the best system.In the future we will try to design more globalconstraints and explore deeper relations betweentraining instances generation and mention cluster-ing.
We will also attempt to introduce more predi-cates and transform structure learning techniques forMLN into coreference problems.AcknowledgmentsPart of the work was done when the first authorwas a visiting student in the Singapore Manage-ment University.
And this work was partially sup-ported by the National High Technology Researchand Development Program of China(863 Program)(No.2012AA011101), the National Natural ScienceFoundation of China (No.91024009, No.60973053,No.90920011), and the Specialized Research Fundfor the Doctoral Program of Higher Education ofChina (Grant No.
20090001110047).ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithms forscoring coreference chains.
In The First InternationalConference on Language Resources and EvaluationWorkshop on Linguistics Coreference, pages 563?566.Eric Bengtson and Dan Roth.
2008.
Understandingthe value of features for coreference resolution.
InEMNLP.K.
Chang, R. Samdani, A. Rozovskaya, N. Rizzolo,M.
Sammons, and D. Roth.
2011.
Inference pro-tocols for coreference resolution.
In CoNLL SharedTask, pages 40?44, Portland, Oregon, USA.
Associa-tion for Computational Linguistics.Wanxiang Che and Ting Liu.
2010.
Jointly modelingwsd and srl with markov logic.
In Chu-Ren Huangand Dan Jurafsky, editors, COLING, pages 161?169.Tsinghua University Press.Koby Crammer and Yoram Singer.
2003.
Ultraconserva-tive online algorithms for multiclass problems.
Jour-nal of Machine Learning Research, 3:951?991.III Hal Daume?
and Daniel Marcu.
2005.
A large-scaleexploration of effective global features for a joint en-tity detection and tracking model.
In HLT ?05: Pro-ceedings of the conference on Human Language Tech-nology and Empirical Methods in Natural LanguageProcessing, pages 97?104, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Pascal Denis and Jason Baldridge.
2007.
Joint determi-nation of anaphoricity and coreference resolution us-ing integer programming.
In Human Language Tech-nologies 2007: The Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics; Proceedings of the Main Conference, pages236?243, Rochester, New York, April.
Association forComputational Linguistics.Pascal Denis and Jason Baldridge.
2008.
Specializedmodels and ranking for coreference resolution.
InEMNLP, pages 660?669.Jenny Rose Finkel and Christopher D. Manning.
2008.Enforcing transitivity in coreference resolution.
InACL (Short Papers), pages 45?48.
The Association forComputer Linguistics.T.
Finley and T. Joachims.
2005.
Supervised clusteringwith support vector machines.
In International Con-ference on Machine Learning (ICML), pages 217?224.Shujian Huang, Yabing Zhang, Junsheng Zhou, and Jia-jun Chen.
2009.
Coreference resolution using markovlogic networks.
In Proceedings of Computational Lin-guistics and Intelligent Text Processing: 10th Interna-tional Conference, CICLing 2009.M.
Klenner.
2007.
Enforcing consistency on coreferencesets.
In RANLP.Heeyoung Lee, Yves Peirsman, Angel Chang, NathanaelChambers, Mihai Surdeanu, and Dan Jurafsky.
2011.Stanford?s multi-pass sieve coreference resolution sys-tem at the conll-2011 shared task.
In Proceedings ofthe Fifteenth Conference on Computational NaturalLanguage Learning: Shared Task, pages 28?34, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, A Kamb-hatla, and Salim Roukos.
2004.
A mention-synchronous coreference resolution algorithm basedon the bell tree.
In Proc.
of the ACL, pages 135?142.1253Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proc.
of HLT/EMNLP, pages 25?32.Andrew McCallum and Ben Wellner.
2005.
Conditionalmodels of identity uncertainty with application to nouncoreference.
In Advances in Neural Information Pro-cessing Systems, pages 905?912.
MIT Press.J.
McCarthy and W. Lehnert.
1995.
Using decisiontrees for coreference resolution.
In Proceedings of the14th International Joint Conference on Artificial Intel-ligence.Vincent Ng and Claire Cardie.
2002.
Improving machinelearning approaches to coreference resolution.
In Pro-ceedings of the ACL, pages 104?111.Vincent Ng.
2010.
Supervised noun phrase coreferenceresearch: The first fifteen years.
In ACL, pages 1396?1411.
The Association for Computer Linguistics.Hoifung Poon and Pedro Domingos.
2008.
Joint unsu-pervised coreference resolution with markov logic.
InEMNLP, pages 650?659.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In EMNLP, pages 1?10.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
Conll-2011 shared task: Modeling unrestrictedcoreference in ontonotes.
In Proceedings of the Fif-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 1?27, Portland,Oregon, USA, June.
Association for ComputationalLinguistics.Altaf Rahman and Vincent Ng.
2009.
Supervised modelsfor coreference resolution.
In Proceedings of EMNLP,pages 968?977.Matthew Richardson and Pedro Domingos.
2006.Markov logic networks.
Machine Learning, 62(1-2):107?136.Sebastian Riedel.
2008.
Improving the accuracy and ef-ficiency of map inference for markov logic.
In UAI,pages 468?475.
AUAI Press.Emili Sapena, Llu?
?s Padro?, and Jordi Turmo.
2011.
Re-laxcor participation in conll shared task on coreferenceresolution.
In Proceedings of the Fifteenth Confer-ence on Computational Natural Language Learning:Shared Task, pages 35?39, Portland, Oregon, USA,June.
Association for Computational Linguistics.Wee Meng Soon, Hwee Tou Ng, and Chung Yong Lim.2001.
A machine learning approach to coreferenceresolution of noun phrases.
Computational Linguis-tics, 27(4):521?544.Olga Uryupina, Sriparna Saha, Asif Ekbal, and MassimoPoesio.
2011.
Multi-metric optimization for coref-erence: The unitn / iitp / essex submission to the 2011conll shared task.
In Proceedings of the Fifteenth Con-ference on Computational Natural Language Learn-ing: Shared Task, pages 61?65, Portland, Oregon,USA, June.
Association for Computational Linguis-tics.Marc B. Vilain, John D. Burger, John S. Aberdeen, Den-nis Connolly, and Lynette Hirschman.
1995.
Amodel-theoretic coreference scoring scheme.
In MUC, pages45?52.Xiaofeng Yang, Jian Su, Jun Lang, Chew Lim Tan, TingLiu, and Sheng Li.
2008.
An entity-mention model forcoreference resolution with inductive logic program-ming.
In ACL, pages 843?851.
The Association forComputer Linguistics.Katsumasa Yoshikawa, Sebastian Riedel, Masayuki Asa-hara, and Yuji Matsumoto.
2009.
Jointly identifyingtemporal relations with markov logic.
In ACL/AFNLP,pages 405?413.
The Association for Computer Lin-guistics.1254
