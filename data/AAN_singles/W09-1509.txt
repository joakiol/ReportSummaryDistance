Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 47?55,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsWeb Service Integration for Next Generation LocalisationDavid Lewis, Stephen Curran,Kevin Feeney, Zohar Etzioni,John KeeneyAndy Way Reinhard Sch?lerCentre for Next Generation LocalisationKnowledge and Data EngineeringGroupSchool of Computing Centre for LocalisationResearchTrinity College Dublin, Ireland Dublin City University, IrelandUniversity of Limerick,Ireland{Dave.Lewis|Stephen.curran|Kevin.Feeney|etzioniz|John.Keeney}@cs.tcd.ieaway@computing.dcu.ieReinhard.Schaler@ul.ieAbstractDevelopments in Natural Language Processing technol-ogies promise a variety of benefits to the localizationindustry, both in its current form in performing bulkenterprise-based localization and in the future in sup-porting personalized web-based localization on increa-singly user-generated content.
As an increasing varietyof natural language processing services become availa-ble, it is vital that the localization industry employs theflexible software integration techniques that will enableit to make best use of these technologies.
To date how-ever, the localization industry has been slow reap thebenefits of modern integration technologies such as webservice integration and orchestration.
Based on recentintegration experiences, we examine how the localiza-tion industry can best exploit web-based integrationtechnologies in developing new services and exploringnew business models?
IntroductionResearch and development of natural languageprocessing technologies are leading to a variety ofadvances in areas such as text analytics and ma-chine translation that have a range of commercialapplications.
The Localization Industry in particu-lar, is strategically well placed to make good use ofthese advances as it faces the challenge of localiz-ing accelerating volumes of digital content that isbeing targeted at increasingly global markets ofthis content.
It needs to exploit the benefits of NLPtechnologies to reduce the cost of translation andminimise the time to market of this digital content.Furthermore, where the localization industry bestlearns how to efficiently and flexibly employ  NLPtechnologies in the localization of digital content itwill be ideally placed to develop new services andexploit new business opportunities offered by theWWW.
In particular, today?s localization tech-niques are not able to keep pace with the WWW?sability to dynamically compose and personalizeexisting content and to support rapid developmentof large volumes of user generated content.
Tomeet this challenge, localization processes musteffectively employ NLP to move from manuallycentered, professional batch activities to highlyautomated, highly participative continuous activi-ties.
To do this, the technologies of the WWWneed to be employed to dynamically combine NLPtechnologies and leverage different levels of hu-man linguistic abilities and knowledge to best ac-complish the task at hand.In this paper we examine how this vision, whichwe term Next Generation Localization, can be sup-ported by current web-based, service-orientedsoftware integration techniques such as web ser-vice integration and orchestration.
Based on recentintegration experience we review the current issuesin using open interoperability standards and webservices to the integration of commercial localiza-tion platforms and NLP software.
We then describesome generic definitions for NLP web services andhow these provide flexibility in developing newlocalization service compositions.
Finally, we out-line the major software integration challenges fac-ing the localization industry and describe howthese are being addressed at Ireland?s Centre forNext Generation Localization (CNGL).47?
Next Generation LocalizationTraditional localization technologies andworkflows are no longer able to cope with the es-calating growth in volume.
Traditional localizationmethods are not adequate to manage, localize andpersonalize unpredictable, on-line, multilingual,digital content.
Machine Translation (MT) needs tobe integrated into translation and post-editingworkflows together with human translators.
Novelmachine-learning-based language technologies canautomatically provide metadata annotations (la-bels) to localization input in order to automate lo-calization standardization and management.Figure 1: Example use of Web Service Orchestration ina Localisation WorkflowFor Next Generation Localisation to beachieved, the individual components need to beinteroperable and easily reconfigurable.
The com-plexity of the resulting systems poses substantialsoftware engineering challenges and crucially re-quires detailed user requirement studies, technicaland user interface standards, as well as support forrapid prototyping and formative evaluation earlyon in the software lifecycle.
Blueprints for an in-dustrial environment for Next Generation Localisa-tion, which we term a Localisation Factory, areneeded to guide the development of localisationservices systems integrating advanced language,digital content and localisation management tech-nologies.
However, in order to successfullyachieve the goal of technical interoperability theseservices crucially needs to be supplemented bystandardised localisation processes and workflowsfor the Localisation Factory.
Figure 1 gives anoverview of a typical localisation workflow, thatwould be used for translating the content such asthe use manual for a product, into multiple lan-guages for different target markets.
Typically thisinvolves segmenting the content into sentences,looking up previously translated sentences from aTranslation Memory (MT), before passing untrans-lated segments to a Machine Translation (TM) ser-vice to generate further candidate translations.Next, the job is passed to professional translators,who can accept automated translations or providetheir own translations.
Current practice in perform-ing such workflows uses localisation platformssuch as SDL?s Idiom WorldServer to integrateTranslation Memory databases, Machine Transla-tion packages and the routing of jobs to translatorswho typically work remotely under the manage-ment of a localisation service provision agency.The localization industry has already underta-ken a number of separate standardization activitiesto support interoperability between different locali-sation applications.
The Localisation IndustryStandards Association (LISA ?
www.lisa.org) hasdeveloped various localisation standards:?
Translation Memory Exchange (TMX) for ex-changing TM database content.
Many TM toolproviders have implemented support for TMXin their products.?
Term Base eXchange (TBX): XML Terminol-ogy Exchange Standard.
An XML linkingstandard, called Term Link, is also being in-vestigated.?
Segmentation Rules eXchange (SRX), for ex-changing the rule by which content is original-ly segmented.
There has been very little sup-port to date for SRX because segmentation isthe main component that distinguished TMtools.
Segmentation has direct consequencesfor the level of reuse of a TM.
A TM's value issignificantly reduced without the segmentationrules that were used to build it.?
Global information management Metrics eX-change (GMX): A partially populated familyof standards of globalization and localization-related metricsThe Organization for the Advancement of Struc-tured Information Standards (OASIS ?
www.oasis-open.org), which produces e-business standardshas had a number of initiatives:?
XML Localisation Interchange File Format(XLIFF):  XLIFF is the most common openstandard for the exchange of localisable con-48tent and localisation process information be-tween tools in a workflow.
Many tool provid-ers have implemented support for XLIFF intheir products.?
Trans-WS  for automating the translation andlocalization process as a Web service.
Therehas not been much adoption of this standard.Work on the development and maintenance ofthe standard seems to be at a stand-still.?
Open Architecture for XML Authoring andLocalization: A recently started group lookingat linking many existing localisation standardsThe W3C, which develops many web stan-dards, has an Internationalisation Activity(www.w3.org/International) working on enablingthe use Web technologies with different languages,scripts, and cultures.
Specific standardisation in-cludes the Internationalisation Tag Set to supportinternationalisation of XML Schema/DTDs.To date, therefore, standard localisation proc-esses and workflows addressing common interop-erability issues have not yet been widely adopted.Outside of proprietary scenarios, digital publishersand service providers cannot integrate their proc-esses and technologies and cannot provide inde-pendent performance measures.
This implies lostbusiness opportunities for many and missed oppor-tunities for significant performance improvementfor most of the stakeholders.
We now examinehow web services may help improve this situation.?
Service Oriented Localization Integra-tionThe Centre for Next Generation Localisation[cngl] is developing a number of systems in orderto investigate the issues that arise in integratingcentralized workflows with community-basedvalue creation.
It aims to make full use of Service-Oriented Architecture [erl].
This advocatessoftware integration through well definedfunctional interfaces that can be invoked remotely,typically using the Web?s HTTP protocol withinput and output parameters encoded in XML.
TheW3C have standardized an XML format, The WebService Description Language (WSDL), fordescribing and exchanging such servicedefinitions.
Web services can be composed intomore complicated applications using explicitcontrol and data flow models that can be directlyexecuted by workflow engines.
This allows newworkflow applications to be defined declarativelyand immediately executed, thus greatly reducingthe integration costs of developing new workflowsand increasing the flexibility to modify existingones.
Such web-service based service compositionis known as Web Service Orchestration.
OASIShas standardized web service orchestrationlanguage called the Business Process ExecutionLanguage (BPEL), which has resulted in thedevelopment of several commercial executionplatform and BPEL workflow definition tools,which support workflow definition through drag-and drop interfaces.
In CNGL, web services andweb service orchestration are used  for integratingcomponents and operating workflows betweenpotential partners in the commercial localizationvalue chain.
This provides a high degree offlexibility in integrating the different languagetechnologies and localization products intodifferent workflow configurations for the project,while avoiding reliance on any single proprietaryplatform.
As an initial exploration of this space asystem integration trial was undertaken.
The use ofBPEL for integrating NLP software has previouslybeen used in the LanguageGrid project, but is apurely in support of academic research integration.Our work aimed flexibility instantiate commerciallocalisation workflow using NLP softwarewrapped in services that are orchestrated usingBPEL, while, as indicated in Figure 1, stillintegrating with commercial localisation workflowtools.
This exploration also included extending thehuman element of the localisation workflow bysoliciting translations from a body of volunteertranslators.
This is seen as more appropriate if therequired translation is not time constrained and itoften forms part of a customer relationshipstrategy.
Quality management may requireinvolvement of volunteer post-editors, andincomplete or poor translations may ultimately stillneed to be referred to professional translators.Thus our workflows can be configured to oper-ate in parallel to provide alternative translations.
Inthe professional localization workflow, after theMT stage, the candidate translation would be re-turned to the SDL Worldserver platform via whichprofessional translators and post-editors are able tocomplete the task.
In the crowd-sourcing variation,this manual step is instead performed by passingthe job to a similar application implemented as a49plug-in to the Drupal collaborative content man-agement system.Our implementation uses the XLIFF format as astandard for encapsulating the various transforma-tions that happen to a resource as it passes throughthe localisation process.
It should be noted, how-ever, that support for XLIFF is partial at best inmost localisation tools.
Where the standard is sup-ported, there are often different, specific flavoursused, and embedded elements within the XLIFFcan be lost as the resource passes through variousstages in the process.
Another problem with in-corporating current tools in our service-orientedframework is that some of them, such as IBM?sUIMA, are designed to function in a batch mode ?which does not map cleanly to services.
Neverthe-less, despite a range of practical problems, it wasin general possible to engineer service front-endsfor most of these tools so that they can be inte-grated into a composable service infrastructure.
Inthe following section we proceed to detail the de-sign of the generic web services we defined for thissystem and discuss the option undertaken in theirimplementation.3.1 Web Service DefinitionsThe OASIS TWS working group remains theonly real attempt to define web-services to supportthe localization process.
However, TWS has a li-mited scope.
Rather than aiming to support thedynamic composition of language services intoflexible localization workflows, it concentrates onsupporting the negotiation of ?jobs?
between ser-vice providers.
It is primarily intended to supportthe efficient out-sourcing of localization and trans-lation jobs and it does not address the compositionof language-services to form automatedworkflows.Therefore, in order to deploy web-services tosupport such composition, there is little standardi-sation to rely on.
Thus, a first step in addressingthe problem is to design a set of web-services andtheir interfaces suitable for the task.
In designingthese services, it is worthwhile to recall the generalgoals of service-oriented architectures; the servicesshould be designed to be as flexible and general aspossible and they should neither be tightly coupledto one another, nor to the overall system whichthey are part of.
Furthermore, in keeping with thegeneral trends in service designs [foster], variabili-ty in service behavior should generally be sup-ported through the passed data-structures ratherthan through different function signatures.Bearing these design goals in mind, we can be-gin to analyse the basic requirements of localisa-tion with a view to translating these requirementsinto concrete service definitions.
However, in or-der to further simplify this task, we adopt certainassumptions about the data-formats that will bedeployed.
Firstly, we assume that UTF-8 is theuniversal character encoding scheme in use acrossour services.
Secondly, we assume that XLIFF isemployed as the standard format for exchanginglocalization data between different parts of the lo-calisation process.XLIFF is primarily focused on describing a re-source in terms of source segments and target seg-ments.
Essentially, it assumes the following mod-el: a localization job can be divided up into a set oftranslatable resources.
Each of these resources isrepresented as an XLIFF file.
Each resource canbe further sub-divided into a sequence of translata-ble segments (which may be defined by an SRXconfiguration).
Each of these source segments canbe associated with a number of target segments,which represent the source segment translated intoa target language.
Finally, XLIFF also supportsthe association of various pieces of meta-data witheach resource or with the various elements intowhich the resource is sub-divided.This simple basic structure allows us to define avery simple set of general web-services, each ofwhich serves to transform the XLIFF in some way.These three basic classes of services transform theXLIFF inputs in the following ways:1.
Addition of target segments.2.
Sorting of target candidates3.
Addition of meta-data.Thus, we adopt these service-types as the set ofbasic, general service interfaces that our serviceswill implement.
They allow us to apply a widerange of useful language-technology processes tolocalization content through an extremely simpleset of service interfaces.
To give some examplesof how concrete services map onto these basic in-terfaces:?
A machine translation service is a manifesta-tion of type 1.
It adds translations, as targetsegments, for  source segments  in the XLIFFfile50?
A translation memory leveraging service is,similarly, implemented as a service of type 1.It can be considered as a special case of atranslation service.?
Our basic service-design supports the applica-tion of multiple TM and MT services to eachXLIFF file, potentially producing multipletranslation candidates for each source segment.There are various situations where there is aneed to order these candidates ?
for example tochoose which one will actually be used in thefinal translation, or to present a sorted list to ahuman user to allow them to most convenient-ly select the candidate that is most likely to beselected by them.
These services can be im-plemented using the common type 2 interface.?
A wide range of text analytics service can beimplemented as services of type 3.
For exam-ple, domain identification, language identifica-tion and various tagging services are all instan-tiations of this type.Although these service types are generic, in termsof the transformations that they apply to the XLIFFcontent, they may be very different in terms oftheir management and configuration.
Thus, it isneither possible nor desirable to devise genericmanagement interfaces ?
these interfaces need tobe tailored to the particular requirements of eachspecific service.
Thus, each service really consistsof two specifications ?
an implementation of thegeneric interface which allows the service to beeasily integrated as a standard component into aworkflow that transforms the XLIFF content, and aspecific interface that defines how the service canbe configured and managed.
The following sectionprovides several examples of specific services andtheir management interfaces.Although XLIFF provides significant support formanagement of the transformation of resources asthey proceed through the localisation workflow, itis not a universal solution.
It is an inherently re-source-oriented standard and it is thus not wellsuited for the aggregation of meta-data that hasbroader scope than that of the translatable resource.For example, in the course of a localisationworkflow, we may wish to store state informationrelating to the user, the project, the workflow itselfor various other entities that are not expressible asXLIFF resources.
Therefore, a service-orientedlocalization workflow has a need for a servicewhich allows the setting and retrieving of such me-ta-data.
The following section also includes a basicoutline of a service which can provide such func-tionality across the localization workflow.Finally, it should be pointed out that BPELdoes not provide a universal solution to the prob-lem of constructing workflows.
It is primarily de-signed to facilitate the orchestration of automatedweb-services and does not map well to humanprocesses.
This has been acknowledged in the pro-posed BPEL4People extension and the incorpora-tion of better support for human tasks is also a keymotivating factor for the development of theYAWL workflow specification language ?
a BPELalternative [vanderaalst].
To overcome this limita-tion, we have designed a general purpose servicewhich allows components to query the state of hu-man tasks within the workflow ?
this allowsworkflows to be responsive to the progress of hu-man tasks (e.g.
by redirecting a task that is takingtoo long).3.2 An MT Web ServiceAs part of our work within CNGL in the devel-opment of a Localisation Factory we have engi-neered a web service capable of leveraging transla-tions from multiple automated translation compo-nents.
The service operates by taking in an XLIFFdocument, iterating the segments of the documentand getting a translation from each of the transla-tion components for each segment.
These transla-tions are attached to the segment within the XLIFFand the service returns the final XLIFF documentback to the client.
The service can be configuredto use any permutation of the automated translationcomponents depending on the workflow in whichthe service finds itself operating.
Some translationcomponents may be inappropriate in a givenworkflow context and may be removed.
The ser-vice also allows for the weighting of translationscoming from different translation components sothat certain translations are preferred above others.The service implementation leverages transla-tion from two open web based translation systemsMicrosoft Live Translator [mslive] and Yahoo Ba-belfish [babelfish].
Microsoft Live Translator canbe accessed through a web service interface.
Ya-hoo Babelfish has no web service interface so get-ting back translations is implemented through ascreen-scraping technique on the HTML documentreturned.51The service also makes use of MaTrEx [ma-trex], a hybrid statistical/example-based machinetranslation system developed by our partner uni-versity Dublin City University.
MaTreX makes useof the open-source Moses decoder [moses].
Trans-lation models are created using MaTreX and arepassed to the Moses decoder which performs thattranslation from source to target language.
We tookthe Moses decoder and wrapped it in a web ser-vice.
The web service pipes segments for transla-tion to Moses which responds with translations.This translation model is produced based onaligned source and target corpora of content repre-sentative of the content passing through theworkflow.Finally we have taken a translation memoryproduct LanguageExchange from Alchemy, anindustrial partner within the project, and added thatto the list of automated translation componentsavailable to our service.
This allows any previoushuman translations to be leveraged during the au-tomated translation process.The service is engineered using BusinessProcess Execution Language (BPEL) to orchestratethe calling of the various translation componentsthat compose the service.
BPEL allows thosemanaging the service to easily compose a particu-lar configuration of the service.
Translation com-ponents can be easily added or removed from theservice.
The tool support around BPEL means thatthe user does not need a background in program-ming to  develop a particular configuration of thecomponents.One problem we encountered implementing theMT service as a wrapper around existing compo-nents was that they are unable to handle internalmarkup within the segments.
Segments passingthrough a localisation workflow are likely to con-tain markup to indicate particular formatting of thetext.
The machine translation components are onlyable to handle free text and the markup is not pre-served during translation.
Another problem en-countered in using free web services over the In-ternet was that implementations did not encouragevolume invocations, with source IP addresses re-questing high volumes being blacklisted.3.3 A Text Analytics  Web ServiceWe have implemented a generic text-categorization service to provide text-analytic sup-port for localization workflows.
It takes an XLIFFfile as input and produces an XLIFF file as output,transforming it by adding meta-data (a type 3transform).
The meta-data can be added either on afile-basis or on a segment basis, depending on therequirements of the workflow as expressed in theservice?s configuration.
The service provides asimple and generic XLIFF transformation as partof the localization workflow, while the manage-ment interface provides flexible configurability.The management interface is designed in orderto support multiple text analytic engines, each ofwhich can support multiple categorization schemaat once.
Our implementation uses two text en-gines, the open source TextCat package [textcat]and IBM?s Fragma software [fragma].
The follow-ing operations are provided by the service:Operation createSchema: The createSchemafunction creates a new categorisation schema basedon a provided set of training data, which can op-tionally be provided by an RSS feed for ongoingtraining data updates.Operation getEngines: This returns a list (en-coded in XML) of the categorisation engines thatare available to the Service.
This allows the clientto specify that a specific categorisation engine beused in subsequent requests.Operation viewSchema: This returns a list of thecategories contained within a schema (and the de-tails of the engine that was used to create it).Operation addData: This operation adds a pieceof training data to a categorisation schema - i.e.
itallows components to tell the service that a pieceof text has a known category of categoryID accord-ing to the schema with schemaID.Operation categorise: This provides a categorisa-tion of text provided as an XLIFF segment, accord-ing to a specified schema taken form the list sup-ported by the service.3.4 A Crowd-sourcing Web ServiceIn order to allow the localization workflow to in-corporate crowd-sourcing, by which we mean col-laborative input from a volunteer web-based user-community, we have designed and implemented aweb-service interface.
This interface is designed to52allow stages in the localization job to be handedoff to such a community.
From the point of viewof the workflow, the important thing is that thelocalisation requirements can be adequately speci-fied and that the status of the job can be ascer-tained by other elements in the workflow ?
allow-ing them to react to the progress (or lack thereof)in the task and, for example, to allow the job to beredirected to another process when it is not pro-gressing satisfactorily.Our service design is focused on supportingcrowd-sourcing, but it is intended to extend it tooffer general-purpose support for the integration ofhuman-tasks into a BPEL workflow.
It serves as atestbed and proof of concept for the developmentof a generic localization human task interface.
Theinitial specification has been derived from theTWS specification [tws], but incorporates severalimportant changes.
Firstly, it is greatly simplifiedby removing all the quote-related functions andreplacing them with the RequestJob and SubmitJobfunctions and combining all of the job controlfunctions into a single updateJob function andcombining the two job list functions into one.TWS, as a standard focused on support for lo-calization outsourcing ?
hence the concentration onnegotiating ?quotes?
between partners.
Our re-quirements are quite different ?
we cannot assumethat there is any price, or even any formal agree-ment which governs crowd-sourcing.
Indeed, ingeneral, a major problem with TWS which hin-dered its uptake is that it assumed a particularbusiness model ?
in practice localization jobs arenot so automated, nor so quick that automatedprice negotiation is a particularly desired feature.Such information can be incorporated into a JobDescription data structure, but a generic human-task interface should not assume any particularbusiness model ?
hence the significant changesbetween our API and that of TWS.
Nevertheless,there is much clear and well-structured thinkingcontained in the TWS standard ?
how best to de-scribe language pairs, jobs and various other com-monly referenced ideas in a localization workflow.By using TWS as a base, we can take advantage ofall of that work rather than designing our own da-ta-structures from scratch.
The main operation areas follows:Operation requestJob: The JobDescription inputparameter is an XML format which contains de-tails of the job that is being requested.
The returneddatatype is the details of the job that is offered bythe service.
These are not necessarily the same.
Forexample, the requested job might contain severallanguage pairs, but the returned description mightnot contain all of these language pairs as some ofthose requested might not be available in the ser-vice.
Generally, it can be assumed that the servicewill make its ?best effort?
to fulfill the require-ments and the returned data will be as close as itcan get to the requirements submitted.Operation submitJob: This operation works ex-actly as the one above, except for the fact that itsubmits the job to the service with the particularJobDescription required and receives back theJobDescription that will actually be carried out.Operation retrieveJobList: This accepts a Job-Description  input parameter, an XML formatwhich contains a ?filter?
on the various active jobs.The operation will return a list of all of the jobswhich match that specified in the JobdDescriptionargument.Operation updateJob: A JobDescription inputparameter is an XML format which contains a de-scription of the various changes to the job that arebeing requested.
The function will return a descrip-tion which details the new, updated state of the job(note that the service does not have to follow allthe requested changes and might ignore them).Operation retrieveJob:  A JobDescription inputparameter is an XML format which contains a ?fil-ter?
on the various jobs.
The operation returns aURI from which the client can retrieve the loca-lised content corresponding to the filters.Operation associateResource: This functions as-sociates a resource (TM / Glossary / etc) with aparticular job.
The returned value is the URI of theresource (which may be different than the passedResURI).
The types of resource supported willneed to be decided upon.?
Future Work: Translation QualityThe next challenge to applying these techniquesto workable industrial workflows is to fully ad-dress the metrology of such workflows.
The cur-rent approach does not support the instrumentationof web services to provide quality measurements.Further, such quality measures need to be providedin a way that is relevant to the quality of theworkflow as a whole and the business-driven keyperformance indicators which it aims to support.53However, the integration of translation qualitymetrics across different forms of workflow anddifferent industrial workflow components and lin-guistic technologies has been widely identified asrequiring considerable further investigation.
Eventhe most basic metric used in commercialworkflow, the word count against which transla-tion effort is estimated, is calculated differently bydifferent workflow systems.
This particular casehas already been addressed by LISA though itsproposal for Global information management Me-trics eXchange (GMX) [gmx].It is hardly surprising, therefore, that closing thegap between the metrics typically used by MT sys-tem developers and what is needed to support theuse of MT in commercial localization workflows islikely to be even more challenging.
For example,metrics such as BLEU [bleu] are well-understoodby MT developers used to participating in large-scale open MT evaluations such as NIST; a BLEUscore of 0.8 (say) means either that one?s MT sys-tem is extremely good, or that the task is quitesimple, or both, or even that there are a large num-ber of reference translations against which the sys-tem output is being compared.
On the other hand, ascore of 0.2 means that the quality is poor, thatthere is probably only one reference translationagainst which candidate translations are being eva-luated, or that the task is a very complex one.However, neither score means anything (much)to a potential user.
In the localization industry,Translation Memory is much more widely used,and there users and vendors use a different metric,namely fuzzy match score, i.e.
how closely a pre-viously translated source sentence matches the cur-rent input string.
Users typically ?know?
that ascore of around 70% fuzzy match is useful, whe-reas for a lower scored sentence it is likely to bequicker to translate this from scratch.One of our research goals in the CNGL is tobring these two communities closer together bydeveloping a translation quality metric that speaksto both sets of people, developers and users.
Onestep in the right direction might be the TranslationEdit Rate metric [ter], which measures the numberof editing commands (deletions, substitutions, andinsertions) that need to be carried out in order totransform the MT output into the reference transla-tion(s).
This is being quite widely used in the MTcommunity (cf.
the Global Autonomous LanguageExploitation (GALE) project) by MT developers,and speaks a language that users understand well.User studies will very much inform the directionsthat such research will take, but there are reasonsto believe that the gap can be bridged.Supposing then that such hurdles can be over-come, broadly speaking, the quality of a translationprocess might be dependent on multiple factors,each of which could be measured both intrinsicallyand extrinsically, including;?
Source and destination languages?
Content domain?
Diversity of vocabulary?
Repetitiveness of text?
Length and complexity of sentences?
Availability of relevant translation memories?
The cost and time incurred per translated wordOften control of quality of the translation processcan be impacted most directly by the quality of thehuman translators and the degree of control exertedover the source text.
Different levels of linguisticquality assurance may be undertaken and post-editors (who are often more experienced translatorsand therefore more expensive) are involved inhandling incomplete or missing translations.
How-ever, even in professional translation environ-ments, translation quality is regarded as relativelysubjective and exact measurement of the quality oftranslation is therefore problematic.?
ConclusionIn this paper we have discussed some the chal-lenges faced in taking a web service integrationand orchestration approach to the development ofnext generation localization workflows.
Based onour experiences of using these approaches to inte-grate both existing localization products and cut-ting edge research prototypes in MT , TA andcrowd-sourcing, new, innovative localisationworkflows can be rapidly assembled.
The maturityof the BPEL standard and the design of generalpurpose, reusable web service interfaces are key tothis success.Acknowledgments: This research is supportedby the Science Foundation Ireland (Grant07/CE/I1142) as part of the Centre for Next Gener-ation Localisation (www.cngl.ie) at Trinity CollegeDublin.54References[babelfish] Yahoo Babelfish Machine Translationhttp://babelfish.yahoo.com/ 6th Feb 2009[drupal] Drupal Content Management Systemhttp://www.drupal.org 6th Feb 2009[bleu] Kishore Papineni, Salim Roukos, Todd Ward andWei-Jing Zhu.
2002.
In 40th Annual Meeting of theAssociation for Computational Linguistics, Philadel-phia, PA., pp.311?318.
[bpel] Web Services Business Process Execution Lan-guage Version 2.0, OASIS Standard, 11 April 2007,Downloaded from http://docs.oasis-open.org/wsbpel/2.0/OS/wsbpel-v2.0-0S.html 6thFeb 2009[erl] Erl, Thomas, Service-oriented Architecture: Con-cepts, Technology, and Design.
Upper Saddle River:Prentice Hall  2005[foster] Foster, I., Parastatidis, S., Watson, P., andMckeown, M. 2008.
How do I model state?
: Let mecount the ways.
Commun.
ACM 51, 9 (Sep.
2008),34-41.
[fragma] Alexander Troussov, Mayo Takeuchi,D.J.McCloskey,http://atroussov.com/uploads/TSD2004_LangID_word_fragments.pdf 6th Feb 2009[gmx] Global Information Management Metrics Vo-lume (GMX-V) 1.0 Specification Version 1.0, 26February 2007, downloaded from http://www.xml-intl.com/docs/specification/GMX-V.html on 6th Feb2009[langexchange] Alchemy Language Exchangehttp://www.alchemysoftware.ie/products/alchemy_language_exchange.html 6th Feb 2009[matrex] MaTrEx Machine Translation - John Tinsley,Yanjun Ma, Sylwia Ozdowska, Andy Way.http://doras.dcu.ie/559/1/Tinsleyetal_WMT08.pdf[moses] Moses decoder http://www.statmt.org/moses/9th March 2009[mslive] Microsoft Live Translatorhttp://www.windowslivetranslator.com/ 6th Feb 2009[ter] Matt Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A Study ofTranslation Edit Rate with Targeted Human Annota-tion.
In Proceedings of the 7th Conference of the As-sociation for Machine Translation in the Americas,Cambridge, MA., pp.223?231.
[textcat] Java Text Categorisationhttp://textcat.sourceforge.net/ 6th Feb 2009[tbx] Termbase eXchange Formathttp://www.lisa.org/Term-Base-eXchange.32.0.html6th March 2009[tmx] Translation Memory eXchangehttp://www.lisa.org/Translation-Memory-e.34.0.html6th March 2009[tws] Translation Web Services Specification:http://www.oasis-open.org/committees/download.php/24350/trans-ws-spec-1.0.3.html[vanderaalst] Van Der Aalst, W.M.P.
Ter Hofstede,A.H.M.
?YAWL: Yet another workflow language?
In-formation Systems, Volume 30, Issue 4, June 2005,Pages 245-275[xliff] XML Localisation Interchange File Formathttp://docs.oasis-open.org/xliff/v1.2/os/xliff-core.html 6th March 200955
