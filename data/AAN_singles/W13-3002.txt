Proceedings of the 13th Meeting on the Mathematics of Language (MoL 13), pages 12?20,Sofia, Bulgaria, August 9, 2013. c?2013 Association for Computational LinguisticsOrder and Optionality: Minimalist Grammars with AdjunctionMeaghan FowlieUCLA LinguisticsLos Angeles, Californiamfowlie@ucla.eduAbstractAdjuncts are characteristically optional,but many, such as adverbs and adjectives,are strictly ordered.
In Minimalist Gram-mars (MGs), it is straightforward to ac-count for optionality or ordering, but notboth.
I present an extension of MGs, MGswith Adjunction, which accounts for op-tionality and ordering simply by keepingtrack of two pieces of information at once:the original category of the adjoined-tophrase, and the category of the adjunctmost recently adjoined.
By imposing apartial order on the categories, the Adjoinoperation can require that higher adjunctsprecede lower adjuncts, but not vice versa,deriving order.1 IntroductionThe behaviour of adverbs and adjectives has quali-ties of both ordinary selection and something else,something unique to that of modifiers.
This makesthem difficult to model.
Modifiers are generallyoptional and transparent to selection while argu-ments are required and driven by selection.
Inlanguages with relatively strict word order, argu-ments are strictly ordered, while modifiers may ormay not be.
In particular, (Cinque, 1999) proposesthat adverbs, functional heads, and descriptive ad-jectives are underlyingly uniformly ordered acrosslanguages and models them by ordinary Merge orselection.
Such a model captures only the orderingrestrictions on these morphemes; it fails to cap-ture their apparent optionality and transparencyto selection.
I propose a model of these orderedyet optional and transparent morphemes that intro-duces a function Adjoin which operates on pairsof categories: the original category of the modi-fied phrase together with the category of the mostrecently adjoined modifier.
This allows the deriva-tion to keep track of both the true head of thephrase and the place in the Cinque hierarchy ofthe modifier, preventing inverted modifier ordersin the absence of Move.2 Minimalist GrammarsI formulate my model as a variant of MinimalistGrammars (MGs), which are Stabler (1997)?s for-malisation of Chomsky?s (1995) notion of feature-driven derivations using the functions Merge andMove.
MGs are mildly context-sensitive, puttingthem in the right general class for human lan-guage grammars.
They are also simple and intu-itive to work with.
Another useful property is thatthe properties of well-formed derivations are eas-ily separated from the properties of derived struc-tures (Kobele et al 2007).
Minimalist Gram-mars have been proposed in a number of vari-ants, with the same set of well-formed derivations,such as the string-generating grammar in Keenan& Stabler (2003), the tree-generating grammarsin Stabler (1997) and Kobele et al2007), andthe multidominant graph-generating grammar inFowlie (2011).At the heart of each of these grammars is afunction that takes two derived structures and putsthem together, such as string concatenation ortree/graph building.
To make this presentation asgeneral as possible, I will simply call these func-tions Com.
I will give derived structures as stringsas (2003)?s grammar would generate them,1 butthis is just a place-holder for any derived structurethe grammar might be defined to generate.Definition 2.1.
A Minimalist Grammar is a five-tuple G = ?
?, sel, lic,Lex ,M?.
?
is a finite setof symbols called the alphabet.
sel?lic are finitesets of base features.
Let F={+f,-f,=X,X|f?1Keenan & Stabler?s grammar also incorporates an addi-tional element: lexical items are triples of string, features,and lexical status, which allows derivation of Spec-Head-Complement order.
I will leave this out for simplicity, as it isnot relevant here.12lic, X?
sel} be the features.
For  the empty string,Lex ?
?
?
{} ?
F ?
is the lexicon, and M is theset of operations Merge and Move.
The languageLG is the closure of Lex under M .
A set C ?
Fof designated features can be added; these are thetypes of complete sentences.Minimalist Grammars are feature-driven,meaning features of lexical items determinewhich operations can occur and when.
There aretwo disjoint finite sets of features, selectionalfeatures sel which drive the operation Mergeand licensing features lic which drive Move.Merge puts two derived structures together; Moveoperates on the already built structure.
Eachfeature has a positive and negative version, andthese features with their polarities make the setF from which the feature stacks for LexicalItems are drawn.
In the course of the derivationthe features will be checked, or deleted, by theoperations Merge and Move.Polarity?
Pos Negfor Merge =X X X?
selfor Move +f -f f?
licTable 1: FeaturesIn order for a derivation to succeed, LIs must bein the following form:=A =B+w +v=Y        ...X -f -g -h...!"#$%&'()X*'$+,-'$./0-1$23-2%)4$"#,'$35-00)#'-%%$')6&7$89$)4-'3#)#9-2%)-#)3$0$1#3)-3)&4)1"#$%&'()Y:))89-3)-3)#9$)1&;<0$;$2#:="19)3<$1-4-$')-3)$-#9$')6$'%$.)*=A,=B/)&')6&7$.)*+w,+v/:))>2)")#'".-#-&2"0)?@A"')4'";$5&'BC)#9$'$)-3);"D-;,;)&2$)3<$1-4-$'C)3&)#9$'$)5&,0.
)A$)"#);&3#)&2$)4$"#,'$)-2)#9-3)0-3#:)Figure 1: LI templateFor example, ?kick, =D=DV?
takes a comple-ment of category D, a specifier of category D, andis itself a V. ?which, =ND-wh?
takes an N as com-plement forming a D phrase, which will move be-cause of feature wh.Merge and Move are defined over expres-sions: sequences of pairs ?derived structure, fea-ture stack?.
The first pair in the sequence can bethought of as the ?main?
structure being built; theremaining are waiting to move.
An expression dis-plays feature f just in case that feature is the firstfeature in the feature stack of the first pair.An MG essentially works as follows: Merge is abinary operation driven by sel.
It takes two expres-sions and combines them into one just in case thefirst expression displays =X and the second dis-plays X for some X ?
sel.
Once the second ex-pression is selected, it may still have features re-maining; these are always negative licensing fea-tures and mean that the second structure is goingto move.
As such it is stored separately by thederivation.
When the matching positive licensingfeature comes up later in the derivation, the mov-ing structure is combined again.
This is Move.Move also carries the requirement that for eachf?lic there be at most one structure waitingto move.
This is the shortest move constraint(SMC).2Definition 2.2 (Merge).
For ?, ?
sequences ofnegative lic features, s, t derived structures:3Merge(?s, =X??
::moverss, ?t, X??
::moverst) ={(Com(s, t), ?)
:: moverss ?moverst if ?
= (s, ?)
:: (t, ?)
:: moverss ?moverst if ?
6= Definition 2.3 (Move).
For ?, ?, ?
sequencesof negative lic features, s, t derived structures,suppose ?!
?t, ??
?
movers such that ?
=-f?.
Then: Move(?s, +f??
::movers) ={?Com(s, t), ??
:: movers?
?t, ??
if ?
= ?s, ??
:: ?t, ??
:: movers?
?t, ??
if ?
6= In this article I will make use of annotatedderivation trees, which are trees describing thederivation.
In addition to the name of the func-tion, I (redundantly) include for clarity the derivedexpressions in the form of strings and features, andsometimes an explanation of why the function ap-plied.
For example, Figure 2 shows derivations(unannotated and annotated) of the wolf with fea-ture D.Mergethe:=ND wolf:NMergethe wolf:Dthe:=ND wolf:NFigure 2: Unannotated and annotated derivationtrees2The SMC is based on economy arguments in the linguis-tic literature (Chomsky, 1995), but it is also crucial for a typeof finiteness: the valid derivation trees of an MG form a regu-lar tree language (Kobele et al 2007).
The number of possi-ble movers must be finite for the automaton to be finite-state.The SMC could also be modified to allow up to a particular(finite) number of movers for each f?lic.3:: adds an element to a list; ?
appends two lists; ?
re-moves an element from a list.133 CartographyThe phenomena this model is designed to accountfor are modifiers and other apparently optionalprojections such as the following:(1) a.
The small ancient triangular green Irish paganmetal artifact was lost.b.
*The metal green small artifact was lost.Adjec-tivesc.
Frankly, John probably once usually arrivedearly.d.
*Usually, John early frankly once arrived prob-ably.
Adverbse.
[DP[DPzhethis[NumP[NumPyione[ClP[ClPzhiCL[NP[NPbi]]]pen]]]?this pen?
Functional projectionsThese three phenomena can all display option-ality, transparency to selection, and strict order-ing.
By transparency I mean that despite the inter-vening modifiers, properties of the selected headare relevant to selection.
For example, in a classi-fier language, the correct classifier selects a nouneven if adjectives intervene.The hypothesis that despite their optionalitythese projections are strictly ordered is part of syn-tactic cartography (Rizzi, 2004).
Cinque (1999,2010) in particular proposes a universal hierar-chy of functional heads that select adverbs in theirspecifiers, yielding an order on both the heads andthe adverbs.
He proposes a parallel hierarchy ofadjectives modifying nouns.
These hierarchies arevery deep.
The adverbs and functional heads in-corporate 30 heads and 30 adverbs.Cinque argues that the surprising univer-sality of adverb order calls for explanation.For example, Italian, English, Norwegian,Bosnian/Serbo-Croatian, Mandarin Chinese,and more show strong preferences for franklyto precede (un)fortunately.
These argumentscontinue for a great deal more adverbs.4(2) Italiana.
FrancamenteFranklyhohavepurtroppounfortunatelyunaapessimabadopinioneopiniondiofvoi.you?Frankly I unfortunately have a very bad opin-ion of you.?b.
*PurtroppoUnfortuatelyhohavefrancamentefranklyunaapessimabadopinioneopiniondiofvoi.you(3) Englisha.
Frankly, I unfortuately have a very bad opin-ion of you4Data from Cinque (1999)b.
?Unfortunately I frankly have a very bad opin-ion of you(4) Norwegiana.
PerPeterforlaterleaves[rerlig[honestlytalt]spoken][heldigvis][fortunately][nil][now]selskapet.the.party.
?Frankly, Peter is fortunately leaving the partynow.?b.
*PerPeterforlaterleaves[heldigvis][fortunately][rerlig[honestlytalt]spoken][nil][now]selskapet.the.party.
(5) Bosnian/Serbo-Croatiana.
lskreno,Frankly,jaInaialostunfortunatelyimamhavejakoverylosebadmisljenjeopinionoofvamayou.Frankly, I unfortunately have a very bad opin-ion of you.?b.
*Naialost,unfortunatelyjaIiskrenofranklyimamhavejakoverylosebadmisljenjeopinionoofvarna.you.
(6) Mandarin Chinesea.
laoshi-shuoFrankly,woIbuxingunfortunatelyduitotamenthemyouhavepian-jian.prejudice?Honestly I unfortunately have prejudiceagainst them.?b.
*buxingunfortunatelywoIlaoshi-shuoFranklyduitotamenthemyouhavepian-jian.prejudiceSupposing these hierarchies are indeed univer-sal, the grammar should account for it.
Moreover,in addition to strictly ordered adjuncts, ideally amodel of adjunction should account for unorderedadjuncts as well.
For example, English PPs areunordered:(7) a.
The alliance officer shot Kaeli in the cargohold with a gun.b.
The alliance officer shot Kaeli with a gun inthe cargo hold.It is not unusual to see this kind of asymme-try, where right adjuncts are unordered but left ad-juncts are ordered.4 Previous approaches to adjunctionThis section provides a brief overview of four ap-proaches to adjunction.
The first two are froma categorial grammar perspective and account forthe optionality and, more or less, transparency toselection; however, they are designed to model un-ordered adjuncts.
The other two are MG formal-14isations of the cartographic approach.
Since thecartographic approach takes adjuncts to be regu-lar selectors, unsurprisingly they account for or-der, but not easily for optionality or transparencyto selection.4.1 Categorial Grammar solutionsTo account for the optionality and transparency, acommon solution is for a modifier to combine withits modified phrase, and give the result the samecategory as the original phrase.
In traditional cate-gorial grammars, a nominal modifier has categoryN\N or N/N, meaning it combines with an N andthe result is an N.Similarly, in MGs, an X-modifier has features=XX: it selects an X and the resulting structure hascategory feature X.Merge*the bad big wolf:Dthe::=ND Merge*bad big wolf:Nbad::=NN Mergebig wolf:Nbig::=NN wolf::NFigure 3: Traditional MG derivation of *the badbig wolfWhat this approach cannot account for is order-ing.
This is because the category of the new phraseis the same regardless of the modifier?s place in thehierarchy.
That is, the very thing that accounts forthe optionality and the transparency of modifiers(that the category does not change) is what makesstrict ordering impossible.
Moreover, the modifieris not truly transparent to selection: the modifierin fact becomes the new head; it just happens toshare a category with the original head.
This canbe seen in tree-generating grammars such as Sta-bler (1997) (Figure 4).Merge?
big, =NN?
?wolf, N?<big wolfFigure 4: Derivation tree and derived bare tree.The < points to the head, big.4.1.1 Frey & Ga?rtnerFrey & Ga?rtner (2002) propose an improved ver-sion of the categorial grammar approach, onewhich keeps the modified element the head, giv-ing true transparency to selection.
They do this byasymmetric feature checking.To the basic MG formalism a third polarity isadded for sel, ?X.
This polarity drives the addedfunction Adjoin.
Adjoin behaves just like Mergeexcept that instead of cancelling both ?X and X,it cancels only ?X, leaving the original X intact.This allows the phrase to be selected or adjoinedto again by anything that selects or adjoins to X.This model accounts for optionality and true trans-parency: the modified element remains the head(Figure 4.1.1).Merge?big, ?N?
?wolf, N?>big wolfFigure 5: Frey & Ga?rtner: derivation tree and de-rived bare tree.
The > points to the head, wolf.Since this grammar is designed to model un-ordered modifiers, illicit orders are also derivable(Figure 6).Merge*the bad big wolf:Dthe::=ND Merge*bad big wolf:Nbad::?N Mergebig wolf:Nbig::?N wolf::NFigure 6: F & G derivation of *the bad big wolf4.2 Selectional approachA third approach is to treat adjuncts just like anyother selector.
This is the approach taken by syn-tactic cartography.
Such an approach accountsstraightforwardly for order, but not for optional-ity or transparency; this is unsurprising since thephenomena I am modelling share only ordering re-strictions with ordinary selection.The idea is to take the full hierarchy of modi-fiers and functional heads, and have each select theone below it; for example, big selects bad but notvice versa, and bad selects wolf.
However, herewe are left with the question of what to do whenbad is not present, and the phrase is just the bigwolf.
big does not select wolf.4.2.1 Silent, meaningless headsThe first solution is to give each modifier andfunctional head a silent, meaningless version thatserves only to tie the higher modifier to the lower.15For example, we add to the lexicon a silent, mean-ingless ?size?
modifier that goes where big andsmall and other LIs of category S go.?
?
the, =S D?
?
, =S D??
?
big, =G S?
?
, =G S??
?
bad, =N G?
?
, =N G??
?
wolf, N?This solution doubles substantial portions of thelexicon.
Doubling is not computationally signif-icant, but it does indicate a missing generalisa-tion: somehow, it just happens that each of thesemodifiers has a silent, meaningless doppelganger.Relatedly, the ordering facts are epiphenomenal.There is nothing forcing, say, D?s to always selectS?s.
There is no universal principle predicting thefairly robust cross-linguistic regularity.Moreover, normally when something silent is inthe derivation, we want to say it is contributingsomething semantically.
Here these morphemesare nothing more than a trick to hold the syntaxtogether.
Surely we can do better.4.2.2 Massive homophonyA second solution is for each morpheme in thehierarchy to have versions that select each levelbelow it.
For example, the has a version whichselects N directly, one that selects ?goodness?
ad-jectives like bad, one that selects ?size?
adjectiveslike big, and indeed one for each of the ten or solevels of adjectives.?
?the, =SD?
?the, =GD?
?the, =SD?
?the, =ND??
?big, =GS?
?big, =NatS?
?big, =NS??
?bad, =NatG?
?bad, =NG??
?Canadian, =NNat??
?wolf, N?This second solution lacks the strangeness ofsilent, meaningless elements, but computationallyit is far worse.
To compute this we simply useGauss?s formula for adding sequences of numbers,since an LI at level i in a hierarchy has i versions.For example, in the model above, the is at level4 (counting from 0), and there are 4 versions ofthe.
For a lexicon Lex without these duplicatedheads, and a language with k hierarchies of depthsli for each 1 ?
i ?
k, adding the duplicated headsincreases the size of the lexicon.
The increase isbounded below by a polynomial function of thedepths of the hierarchies as follows:5|Lex?| ?k?i=11/2(l2i + li) + |Lex|5 ProposalI propose a solution with three components: setsof categories defined to be adjuncts of particularcategories, a partial order on sel, and a new oper-ation Adjoin.
The sets of adjuncts I base on Sta-bler (2013).
The partial order models the hierar-chies of interest (e.g.
the Cinque hierarchy); Ad-join is designed to be sensitive to the order.Adjoin operates on pairs of selectional features.The first element is the category of the first thingthat was adjoined to, for example N. The secondelement is the category of the most recently ad-joined element, for example Adj3.
Adjoin is onlydefined if the new adjunct is higher in the hierar-chy than the last adjunct adjoined.I call these grammars Minimalist Grammarswith Adjunction (MGAs).Definition 5.1.
A Minimalist Grammar withAdjunction is a six-tupleG = ?
?, ?sel,?
?, ad, lic,Lex ,M?.
?
is a finiteset called the alphabet.
sel?lic are finite sets ofbase features, and ?sel,??
is a partial order.
LetF={+f,-f,=X,[X,Y]|f?
lic, X,Y ?
sel}.ad : sel?
P(sel) maps categories to theiradjuncts.
Lex ?
?
?
{} ?
F ?, and M is the setof operations Merge, Move, and Adjoin.
Thelanguage LG is the closure of Lex under M .
Aset C ?
sel of designated features can be added;{[c, x]|c ?
C, x ?
sel, x ?
c} are the types ofcomplete sentences.6The differences between MGs defined aboveand MGAs are: (1) in MGAs sel is partially or-dered; (2) in MGs the negative polarity for X ?sel is just X; in MGAs it is the pair [X,X]; (3)MGAs add a function: Adjoin; (4) MGAs definesome subsets of sel to be adjuncts of certain cate-gories; (5) Merge is redefined for the new featurepair polarity.
(Move remains unchanged.
)5I say ?bounded below?
because this formula calculatesthe increase to the lexicon assuming there is exactly one LI ateach level in the hierarchy.
If there are more, each LI at leveli of a hierarchy has i versions as well.6I have replaced all negative selectional features X withpairs [X,X].
This is for ease of defining Adjoin and the newMerge.
Equivalently, LIs can start with category features Xas in a traditional MG, and Adjoin can build pairs.
I chosethe formulation here because it halves the number of casesfor both Merge and Adjoin.16For ?A,??
a partial order, a, b ?
A are incom-parable, written a||b, iff a 6?
b and b 6?
a.To shorten the definition of Adjoin, I define afunction f adj which determines the output featuresunder Adjoin.
If the adjunct belongs to the hi-erarchy of adjuncts being tracked by the secondelement of the feature pair, that second elementchanges.
If not, the feature pair is unchanged.Definition 5.2.
For W, X, Y, Z ?
sel, W ?
ad(Y) :f adj([W, X], [Y, Z]) =?????
[Y, W] if W ?
Z[Y, Z] if W||Zundefined otherwiseNotice that if Z and W are incomparable, norecord is kept of the feature (W) of the adjunct.This is just like Frey & Ga?rtner?s asymmetric fea-ture checking, and derives adjuncts that are un-ordered with respect to each other.
In Definition5.3, I model languages like English in which gen-erally unordered adjuncts, like PPs, appear to theright, while ordered adjuncts, like adjectives, ap-pear to the left.
The rules could be easily modifiedfor different orderings.
See Section 6 for furtherdiscussion.Definition 5.3 (Adjoin).
For s, t derivedstructures, ?, ?
?
{?f|f ?
lic}?,?
?
{+f,= X|f ?
lic, X ?
sel}?
,W, X, Y, Z ?
sel, W ?
ad(Y),C = fadj([W, X], [Y, Z]):Adjoin(?s, [W, X]???
::mvrss,?t, [Y, Z]??
:: mvrst) =?????????????????????????????????????????
?Com(s, t), ?C?
:: mvrss ?mvrstif ?, ?
=  & W ?
Z?Com(t, s), ?C?
:: mvrss ?mvrstif ?, ?
=  & W||Z?s, ?C?
:: ?t, ??
:: mvrss ?mvrstif ?
= , ?
6=  & W 6< Z?t, ?C?
:: ?s, ??
:: mvrss ?mvrstif ?
6= , ?
=  & W 6< Z?, ?C?
:: ?s, ??
:: ?t, ??
:: mvrss ?mvrstif ?, ?
6=  & W 6< ZThe first case is for ordered adjuncts where nei-ther the adjunct nor the adjoined-to phrase willmove (encoded in empty ?, ?).
The second is thesame but for unordered adjuncts, which will ap-pear on the right.
The last three cases are for mov-ing adjunct, moving adjoined-to phrase, and bothmoving, respectively.
?
is a sequence of positivelicensing features, which allows adjuncts to takespecifiers.Merge needs a slight modification, to incorpo-rate the paired categories.
Notice that Merge isinterested only in the first element of the pair, the?real?
category.Definition 5.4 (Merge).
For ?, ?
?
F ?
, s, tderived structures, X, Y ?
sel:Merge(?s,=X??
::mvrss, ?t, [X, Y]??
::mvrst) ={(Com(s, t), ?)
:: mvrss ?mvrst if ?
= (s, ?)
:: (t, ?)
:: mvrss ?mvrst if ?
6= Move remains as in definition 2.3 above.5.1 ExamplesMGAs are most easily understood by example.This first example demonstrates straightforwardapplications of Adjoin that derive strictly-orderedprenominal adjectives.
The big bad wolf is deriv-able because the derivation remembers that an N-adjunct at level G in the hierarchy, ?bad, [G,G]?,adjoined to the noun.
It encodes this fact in thesecond element of the pair [N,G].
Big is then ableto adjoin because it too is an N-adjunct and it ishigher in the hierarchy than bad (S>G).
Finally,the can be defined to select wolf directly.Let sel = {D, G, M, N, P, C, T, V} and the partialorder ?
on sel be such that D ?
S ?
G ?
M ?
Nand C ?
T ?
Vadjuncts = {?N, {S, G, M, P, C}?
}Lex = {?bad, [G,G]?, ?big, [S,S]?, ?the,=N[D,D]?, ?wolf, [N,N]?, ?woods, [N,N]?,?in, =D[P,P]?
}Merge(the big bad wolf, [D,D])(the, =N[D,D]) Adjoin(big bad wolf, [N,S])(since S?G and S?ad(N))(big,[S,S]) Adjoin(bad wolf, [N,G])(since G?N and G?ad(N))(bad,[G,G]) (wolf,[N,N])Figure 7: Valid derivation of the big bad wolf*Bad big wolf, on the other hand, is not deriv-able without movement since the derivation re-members that big, which is at level S in the hierar-chy, has already been adjoined.
bad, being lowerin the hierarchy, cannot adjoin.17Adjoin*bad big wolf(since G < S)(bad, [G,G]) Adjoin(big wolf, [N,S])(since S?N and S?ad(N))(big, [S,S]) (wolf, [N,N])Figure 8: Invalid derivation of *bad big wolfThis next example shows a right adjunct, a PP,being adjoined to an NP.
Since P||N ?
that is, nohierarchical order is defined between N and P ?the PP adjoins to the right, but does not alter thecategory of the noun.Adjoin?wolf in the woods, [N, N]?since P ?ad(N) and P||NMerge?in the woods, [P, P]?
?in, = D[P, P]?
Merge?the woods, [D, D]?
?the, =N[D, D]?
?woods, [N, N]?
?wolf, [N, N]?Figure 9: Right adjunction6 Discussion and extensionsThis model captures both the strict ordering of themerge-only models and the optionality and trans-parency to selection of the categorial approaches.Cinque?s observation that there is a hierarchy offunctional heads and adverbs is modelled directlyby defining a hierarchy in the grammar itself.
Thestrict linear order falls out of the order imposed onthe selectional features and the definition of Ad-join: adjunction is only defined when the hierar-chy is respected.
Optionality is the result of thetransitivity of orders: intervening adjuncts are notnecessary for a higher one to be adjoined.
Trans-parency to selection is modelled by the pairing ofthe selectional features: the original category ofthe modified element is preserved, and Merge cansee only that feature.
The adjuncts are literally ig-nored.The cross-linguistic consistency of the ordersis accounted for by the claim that all human lan-guages have the same partial order on sel.
As such,it does not have to be learned, but rather comeswith the grammar.Computationally, this approach has an advan-tage over the merge-only model with homophonyas the latter increases the size of the lexicon bya polynomial function in the depths of the hierar-chies of adjuncts, but the former does not.6.1 Left and right adjunctsAs mentioned, I defined Adjoin to derive theasymmetry observed between left and right ad-juncts in many languages: left adjuncts such asadverbs and descriptive adjectives are strictly or-dered, while right adjuncts such as PPs and clausesare not.
This fact is derived by letting the presenceor absence of an ordering relation between the ad-junct and modified category determine which caseof Adjoin applies.
If there is an order, the usuallinear order will be calculated by Com, and theplace in the hierarchy is tracked.
Otherwise, thelinear order is switched, and there is asymmetricfeature checking.If this is not the effect desired, there are alterna-tives.
The simplest is to make the domain of thefunction ad sel ?
{right, left}, specifying the setsof right and left adjuncts.
This allows for muchmore flexibility, for good or ill.
It does not de-rive the asymmetry, but does allow ordered andunordered adjuncts to appear on the same side ofthe head, if such a pattern is desired.
This is anempirical question.6.2 Selection and adjunctsThis model allows LIs that are in the set of ad-juncts to be selected normally as arguments, sinceadjuncts have categories of their own.
For ex-ample, Red Ridinghood was small is derivable byallowing was to select ?small, [S,S]?
: ?was,=S[V,V]?.
This is an improvement over modelsthat do not give adjuncts categories of their own,such as Frey & Ga?rtner?s, but it is still lacking.
Inthis model, there will have to be massive dupli-cation in the lexicon so that was can select everyadjective: ?was, =S[V,V]?, ?was, =G[V,V]?etc.To solve this problem, we can take advantageof the function ad, and define was to select any-thing from a particular image under ad.
Such amodel expands the definition of Merge to operatenot only on categories, but also on sets of cate-gories.
The model would look something like this:Merge(?was, =ad(N)[V,V]?, ?small, [S,S]?
)is defined iff S?
ad(N)Because the set of features F is finite, allowingMerge to be defined over subsets of F does notchange the finite properties of MGs.
Merge couldin fact be allowed to be defined over any subset18of F .
I suggest this model because it is restricted:only sets that exist for other reasons already canbe quantified over.MGAs also allow adjuncts to select argumentsand license Move.
For example, a preposition canselect a complement before becoming an adjunctPP.
Moreover, a functional projection such as Fo-cus can Move a focused phrase into its specifierfrom the main tree, or Topic can Merge a specifier.The latter is a result of allowing positive polarityfeatures to follow the category pair.
Recall that intraditional MGs, an LI must be of the followingform for the derivation to succeed, where each piis a positive polarity feature, X, Y ?
sel and eachfi ?
lic:(= Y(p1p2...pn))X(-f1-f2...-fm)However, in MGAs, LIs of the following formare possible if the LI will Adjoin, the crucial dif-ference being the presence of pn+1...pk:(= Y(p1p2...pn))[X, Y](pn+1...pk)(-f1-f2...-fm)Figure 10 shows the end of a derivation in whichthe mover briefly is an adjunct, and so the licensor,the null Foc head.
Its positive licensing feature+foc moves to the front of the stack of the derivedstructure?s features.Suppose Foc ?
ad(T) and Foc ?
T.Move?briefly she spoke, [T,Foc]?Adjoin?she spoke, +foc[T,Foc]?, ?briefly, -foc?
?, [Foc,Foc]+foc?
Merge?she spoke, [T,T]?, ?briefly, -foc?Figure 10: Adjunct FocP with moved specifier.6.3 Adjuncts of adjunctsIn natural language, adjuncts can also be adjoinedto, for example as in the very bad wolf.
The func-tion ad maps single categories to their adjuncts,but it is not generally the case that, say, an adverb,can only adjoin to certain adjectives.
In order tocapture this fact without duplication in the lexi-con, Adjoin, like Merge, can be extended to allowsubsets of F .
Similarly to the Merge case, we canrestrict these subsets by requiring that they be theimage of a category under ad.
For example:?frankly, [Fr,Fr]?, ?unfortunately, [Fo,Fo]?, ?allegedly,[Al,Al]?, ?bad, [G,G]?, ?wolf, [N,N]??
LexFr ?
Fo ?
Al ?
V, S ?
G ?
N, Pad(N) = {S,G,P}ad(V) = ad(S) = ad(G)= {Fr,Fo,Al}Adjoin?unfortunately bad, [G,G]?
(since Fo||G and Fo?ad(G))?unfortunately, [Fo,Fo]?
?bad, [G,G]?Figure 11: Adjoining to an adjunctNotice however that we are still missing a gen-eralisation: S,G, and indeed all adjectives havethe same adjuncts.
Now, this can be modelled bycalling this set ad(ad(N)).
However, such a solu-tion assumes a special status for N over many othercategories such as G: why ad(ad(N)) rather thanad(ad(G))?
I would argue that such a status wouldreflect the reality of natural language.
We can seeN and V behaving in special ways: both are at thebottom of hierarchies, for example.
However, asfar as I am aware, no such status exists in anyMGs.
Formalising these observations is a matterfor further research.6.4 IslandhoodAdjuncts have another classic property: island-hood.
Movement is not possible out of certaintypes of adjuncts.
(8) a.
You left [because your ex showed up]Adjb.
*Who did you leave [because showedup]Adj?Any approach that keeps Adjoin separate fromMerge introduces the option of stipulating the Ad-junct Island Constraint (AIC), either as a separateconstraint on Adjoin, as Frey & Ga?rtner do, or bysimply not including moverss in the definition ofAdjoin, making the function undefined when theadjunct carries movers.
This is not very satisfy-ing, though: better perhaps would be to derive it,as Graf (2013) does.
On the other hand, perhapsnot all adjuncts are islands.
If beside is an ad-junct in (9), it is not an adjunct island.
(9) Who are you sitting [beside ]Adjunct?As always, islands must remain a matter for fur-ther research.7 ConclusionI have presented a model of adjunction that ac-counts for both the optionality and the strict or-19dering of many adjuncts.
MGAs accomplish thisby the simple expedience of keeping track of twopieces of information at once: the original cate-gory of the projecting phrase, and the category ofthe most recent adjunct to adjoin.
This allows Ad-join to be defined to only apply when the next ad-junct is not lower in a hierarchy than the last.
Atthe same time, Merge can see the original cate-gory, and ignores the adjunct?s category.I have also suggested some extensions of MGAsto more efficiently account for adjuncts as thesecond argument of Merge and Adjoin.
Theseinvolved quantification over categories, with theadded suggestion that the sets of categories inquestion be restricted by the sets of adjuncts al-ready defined.Future directions for this research include notonly matters internal to the model, such as howbest to model adjuncts of adjuncts, but alsolarger questions of the mathematical properties ofMGAs.
MGAs are weakly equivalent to MGs,since MGAs merely take existing ways to derivecertain strings and seek more efficient ways, whichcapture more generalisations.
If every adjunct inthe lexicon is replaced with the right set of selec-tors, Adjoin does not need to be used.
For exam-ple, the adjectives in the MGA lexicon used in theexamples in Section 5.1 can be replaced by the ad-jectives in either grammar from the selectional ap-proaches in Section 4.2, and the same string setcan be generated.Clearly MGs and MGAs are not strongly equiv-alent: the derivation trees differ in that MGAs havea function that is not present in MGs.Because the possible configurations of featuresremains finite, the derivation tree languages ofMGAs should prove to be regular, following Ko-bele et al2007)?s presentation: transition rules forAdjoin need merely be added.Also of interest are the subregular propertiesof the derivation tree language.
Although to myknowledge such notions as tierwise strictly local(Heinz et al 2011) have not yet been formallydefined for tree languages, I conjecture that inMGAs, Merge is tierwise strictly k-local, and Ad-join is strictly k-local.ReferencesNoam Chomsky.
1995.
The Minimalist Program.
MITPress, Cambridge, MA.Gugliemo Cinque.
1999.
Adverbs and functionalheads: a cross-linguistic perspective.
Oxford stud-ies in comparative syntax.
Oxford University Press,Oxford.Gugliemo Cinque.
2010.
The syntax of adjectives: acomparative study.
Linguistic Inquiry monographs.MIT Press, Cambridge, MA.Meaghan Fowlie.
2011.
Multidominant minimalistgrammars.
Master?s thesis, University of California,Los Angeles.Werner Frey and Hans-Martin Ga?rtner.
2002.
On thetreatment of scrambling and adjunction in minimal-ist grammars.
In Proceedings of the Conference onFormal Grammar (FGTrento), pages 41?52, Trento.Thomas Graf.
2013.
The price of freedom: Whyadjuncts are islands.
Slides of a talk given at theDeutsche Gesellschaft fu?r Sprachwissenschaft 2013,March 12?15, University of Potsdam, Potsdam, Ger-many.Jeffrey Heinz, Chetan Rawal, and Herbert Tanner.2011.
Tier-based strictly local constraints forphonology.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguis-tics, Portland, Oregon, USA, June.
Association forComputational Linguistics.Edward L. Keenan and Edward P. Stabler.
2003.
BareGrammar.
CSLI Publications, Stanford.Gregory M. Kobele, Christian Retore?, and Sylvain Sal-vati.
2007.
An automata-theoretic approach to min-imalism.
In J. Rogers and S. Kepser, editors, ModelTheoretic Syntax at ESSLLI ?07.
ESSLLI.Luigi Rizzi.
2004.
Locality and left periphery.
InAdriana Belletti, editor, Structures and Beyond:The Cartography of Syntactic Structures, volume 3,pages 223?251.
Oxford University Press, Oxford.Edward Stabler.
1997.
Derivational minimalism.
Log-ical Aspects of Computational Linguistics, pages68?95.Edward Stabler.
2013.
Bracketing paradoxes andopacity: Parsing late adjunction in copy construc-tions.
Talk given at UCLA Mathematical Linguis-tics Circle, April.20
