Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 309?313,Dublin, Ireland, August 23-24, 2014.IHS R&D Belarus: Cross-domain Extraction of Product Featuresusing Conditional Random FieldsMaryna ChernyshevichIHS Inc. / IHS Global Belarus131 Starovilenskaya St.220123, Minsk, BelarusMarina.Chernyshevich@ihs.comAbstractThis paper describes the aspect extractionsystem submitted by IHS R&D Belarusteam at the SemEval-2014 shared task re-lated to Aspect-Based Sentiment Analy-sis.
Our system is based on IHS Goldfirelinguistic processor and uses a rich set oflexical, syntactic and statistical featuresin CRF model.
We participated in twodomain-specific tasks ?
restaurants andlaptops ?
with the same system trained ona mixed corpus of reviews.
Among sub-missions of constrained systems from 28teams, our submission was ranked first inlaptop domain and fourth in restaurantdomain for the subtask A devoted to as-pect extraction.1 IntroductionWith a rapid growth of the blogs, forums, reviewsites and social networks, more and more peopleexpress their personal views about products onthe Internet in form of reviews, ratings, or rec-ommendations.
This is a great source of dataused by many researchers and commercial appli-cations that are focused on the sentiment analy-sis to determine customer opinions.Sentiment analysis can be done on document,sentence, and phrase level (Jagtap, V. S., Ka-rishma Pawar, 2013).
Earlier works were focusedmainly on the document (Turney, 2002; Pang,Lee and Vaithyanathan, 2002) and the sentencelevel (Kim and Hovy, 2004).
However, this in-formation can be insufficient for customers ?whoare seeking opinions on specific product features(aspects) such as design, battery life, or screen.This fine-grained classification is a topic of as-This work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.
org/licenses/by/4.0/pect-based sentiment analysis (Moghaddam andEster, 2012).Traditional approaches to aspect extraction arebased on frequently used nouns and noun phrases(Popescu and Etzioni, 2005; Blair-Goldensohn etal., 2008), exploiting opinions (Zhuang et al.,2006; Kobayashi, 2006), and supervised learning(Mukherjee and Liu, 2012).In this paper, we describe a system(IHS_RD_Belarus in official results) developedto participate in the international shared task or-ganized by the Conference on Semantic Evalua-tion Exercises (SemEval-2014) and focused onthe phrase-level sentiment classification, namelyaspect extraction (Pontiki et al., 2014).
An aspectterm means particular feature of a product or ser-vice used in opinion-bearing sentences (Myphone has amazing screen), as well as inneutral sentences (The screen brightnessautomatically adjusts).The organizers of SemEval-2014 task haveprovided a dataset of customer reviews with an-notated aspects of the target entities from twodomains: restaurants (3041 sentences) and lap-tops (3045 sentences).
The results were evaluat-ed separately in each domain.
Table 1 shows thedistribution of the provided data for each domaindataset, training and testing set, with number ofsentences and aspects.Laptops RestaurantsTrainingSentences 3045 3041Aspects 2358 3693TestingSentences 800 800Aspects 654 1134Table 1.
Distribution of the provided data.Many studies showed that sentiment analysisis very sensitive to the source domain (trainingcorpus domain) and performs poorly on datafrom other domain (Jakob and Gurevych, 2010).This restriction limits the applicability of in-309domain models to a wide domain diversity ofreviews.
One of the common approaches to de-velop a cross-domain system is training on amixture of labeled data from different domains(Aue and Gamon, 2005).
Cross-domain approachhas the advantage of better portability, but it suf-fers from lower accuracy compared to in-domainaspect extraction.
Our cross-domain system istrained on mixed training data, and the samemodel was used unchanged for classification ofboth domain-specific test datasets.2 System DescriptionAspect extraction may be considered as a se-quence labeling task because the product aspectsoccur at a sequence in a sentence (Liu, 2014).One of the state-of-the-art methods used for se-quence labeling is Conditional Random Fields(CRF) (Lafferty, 2001).
This method takes as aninput a sequence of tokens, calculates the proba-bilities of the various possible labelings andchooses the one with the maximum probability.We decided to deviate from Inside-Outside-Begin (IOB) scheme used by Jakob andGurevych (Jakob and Gurevych, 2010) and Li(Li et al., 2010) and introduced the followinglabels: FA for the attribute word preceding headword of a noun group; FH for the head word of anoun group; FPA for attribute word after headword of a noun group (Microsoft Office2003), and O for other non-aspect tokens.
Thefollowing is an example of our suggested tag-ging: I/O want/O to/O unplug/Othe/O external/FA keyboard/FH.Our experiments showed that the words usedin aspect terms are easier to recognize when theyare always tagged with the same tags.
For exam-ple, let?s consider the tagging of the word ?cam-era?
in the following cases: ?camera?
and ?com-pact camera?.
We propose the FH tag for bothexamples, while the IOB scheme assumes the FBtag for the first example and the FI tag for thesecond.2.1 Pre-processingTo facilitate feature generation for supervisedCRF learning, sentences were pre-processed withIHS Goldfire linguistic processor that performsthe following operations: slang and misspellingcorrection (?excelent?
?
"excellent" , ?amazin??
?amazing?, ?wouldnt?
?
?wouldn?t?
), part-of-speech tagging, parsing, noun phrase extrac-tion, semantic role labeling within expandedSubject-Action-Object (eSAO) relations(Todhunter et al., 2013), named entity recogni-tion, labeling for predictive question-answeringincluding rule-based sentiment analysis(Todhunter et al., 2014).In addition, we designed some simple rules todetect entity boundaries that take precedenceover CRF labeling.
For example, in the sentence?I run Final Cut Pro 7 and a fewother applications?, our boundary detectorrecognizes ?Final Cut Pro 7?
as an entityrepresented by a single token (Tkachenko andSimanovsky, 2012).2.2 FeaturesBelow we will describe the features used in CRFmodel to represent the current token, two previ-ous and two next tokens.Word features:?
Token feature represents a base form of atoken (word or entity) normalized by casefolding.
The vocabulary of terms is prettycompact within one domain, so this featurecan have considerable impact on terms ex-traction performance.?
Part of speech feature represents the part-of-speech tag of the current token withslight generalization, for example, the NNStag (plural noun) is mapped to NN (singu-lar noun).?
Named entity feature labels named entities,e.g., people, organizations, locations, etc.?
Semantic category denotes the presence ofthe token in manually crafted domain-independent word-lists ?
sets of words hav-ing a common semantic meaning ?
such asparameter (characteristics of object, e.g.,?durability?
), process (e.g., ?charging?
),sentiment-bearing word (e.g., ?problem?
),person (e.g., ?sister?
), doer of an action(someone or something that performs anaction, e.g., ?organizer?
), temporal word(date- or time-related words, e.g., ?Mon-day?
), nationality, word of reasoning (e.g.,?decision?, ?reason?
), etc.?
Semantic orientation (SO) score of tokenrepresents a low, mean or high SO score asseparate feature values (the thresholds weredetermined experimentally).
The SO of aword indicates the strength of its associa-tion with positive and negative reviews.We calculated SO of each word w usingPointwise Mutual Information (PMI)measures asSO (w) = PMI(w, pr) ?
PMI(w, nr),310where PMI is the amount of informationthat we acquire about the presence of theword in positive pr or negative reviews nr(Turney, 2002).
For the calculation of SOscore, we used rated reviews fromEpinions.com, Amazon.com and TripAdvi-sor.com.
To make corpus more precise, weincluded only 5-star reviews in our positivecorpus, and 1-star reviews in our negativecorpus.?
Frequency of token occurrence is repre-sented by five values ranging from veryfrequent to very rare words with an exper-imentally determined threshold.
The fre-quency was obtained by dividing the num-ber of reviews containing the token by thetotal number of reviews.
The reason of us-ing this as a feature is that people usuallycomment on the same product aspects andthe vocabulary that they use usually con-verges (Liu, 2012).?
Opinion target feature is a binary featurethat indicates whether a token is a part ofan item which opinions are expressed onand comes from the rule-based sentimentanalysis integrated in the predictive ques-tion-answering component of the IHSGoldfire linguistic processor.
Opinion tar-get can be a product feature as well as aproduct itself.Noun phrase features:?
Role of a token in a noun phrase: headword or attribute word.?
Noun phrase introduction feature marks alltokens of noun phrase beginning with pos-sessive pronoun, demonstrative pronoun,definite or indefinite article.?
Number of attributes with SO score higherthan the experimentally chosen threshold.This feature labels all words in a noungroup.
Our research showed that people of-ten use sentiment-bearing adjectives to de-scribe an aspect, e.g., ?My phone has agreat camera?.?
List feature was added to designate theavailability of list indicators (?and?
orcomma) in the noun group, e.g., ?Theleather carrying case, keyboardand mouse arrived in two days?.?
Leaves-up feature denotes the number ofof-phrases in a noun phrase before the to-ken under consideration.
For example, thetoken "battery" has one preceding of-phrase in the phrase "durability of battery".?
Leaves-down feature denotes the number ofof-phrases in a noun phrase after the tokenunder consideration.SAO features:?
Semantic label feature represents the roleof the token in eSAO relation: subject, ac-tion, adjective, object, preposition, indirectobject or adverb.?
SAO feature labels all words presented inan eSAO relation.
We used a set of eSAOpatterns to determine basic relations be-tween words.
To form a SAO pattern, eachnon-empty component of an eSAO relationwas mapped to an abstract value, e.g.,proper noun phrases to ?PNP?, commonnoun phrases to ?CNP?, predicates are leftin their canonical form.
For example, thesentence "The restaurant Tal of-fers authentic chongqing hot-pot."
is represented by the SAO pattern?PNP offer CNP?.
All words from eSAOare marked with the same SAO feature.2.3 Results and ExperimentsOur CRF model was trained on the mixed set of6086 sentences with annotated aspect terms(3045 from the laptop domain and 3041 from therestaurant domain).
The same model was appliedunchanged to the test dataset from laptop domain(800 sentences) and restaurant domain (800 sen-tences).
We evaluated our system using 5-foldcross-validation: in each of the five iterations ofthe cross-validation, we used 80% of the provid-ed training data for learning, and 20% for testing.laptops restaurantstraining set 0.707 0.7784development set 0.7214 0.7865test set 0.7455 0.7962baseline 0.3564 0.4715Table 2.
Performance on different datasets (F1-score).The Table 2 shows the model performance (F1-score) obtained on the training set (using 5-foldcross validation), on the development set (weused a part of the training set as developmentset), on the final test set and the baseline provid-ed by the task organizers.To evaluate the individual contribution of dif-ferent feature sets, we performed ablation exper-iment, presented in Table 3.
This test involvesremoving one of the following feature sets at atime: current token and its POS tag (TOK), com-binations with two previous and two next tokens311and their POS tags (CONT), named entity (NE),semantic category (SC), semantic orientation(SO), word frequency (WF), opinion target (OT),noun phrase related features (NP_F), and SAOpattern and semantic label (SAO_F).
Some fea-tures complement each other, so that despitesmall individual contribution, a cumulative im-provement is generally achieved by using themin a set.Dev set Test setlap rest lap restoverall 0.7214 0.7865 0.7455 0.7962-TOK 0.6642(-7.9%)0.7244(-7.9%)0.692(-7.2%)0.7445(-6.4%)-CONT 0.7101(-1.6%)0.77(-2.1%)0.7323(-1.8%)0.7811(-1.9%)-SC 0.6982(-3.3%)0.7854(-0.1%)0.7048(-5.8%)0.7864(-1.2%)-SO 0.709(-1.7%)0.7815(-0.6%)0.7442(-0.2%)0.7937(-0.3%)-OT 0.7026(-2.6%)0.7812(-0.7%)0.7381(-1%)0.7973(0.1%)-NP_F 0.717(-0.6%)0.777(-1.2%)0.7303(-2%)0.7801(-2%)-WF 0.716(-0.8%)0.788(0.2%)0.7399(-0.7%)0.7937(-0.3%)-SAO_F 0.7198(-0.2%)0.7854(-0.1%)0.7297(-2.1%)0.7981(0.2%)-NE 0.7191(-0.3%)0.7836(-0.4%)0.7444(-0.1%)0.7961(0)Table 3.
Ablation experiment (F1-score).The importance of a feature set is measured byF1-score on development and testing datasets forboth domains separately.Feature sets are listed in descending order oftheir impact on overall performance.
The analy-sis shows that the most important feature set isthe combination of Token and POS features.Other features contribute to the performance to asmaller degree.As can be seen, the relative influence of fea-tures on F1-score is similar on test and develop-ment sets, showing that our model effectivelyovercomes the overfitting problem.We conducted several experiments on thetraining data to prove the domain portability ofour CRF model.
The results are shown in Table 4.As can be seen, the training on single-domaindata improves the performance of in-domainclassification by about 2%, but lowers the per-formance of cross-domain classification by about40%.
The training on the mixed dataset demon-strates acceptable accuracy on both domain-specific test sets.TrainingdatasetResults onlaptops datasetResults onrestaurants datasetlaptops  0.7667 0.3778restaurants 0.2961 0.8223mixed  0.7455 0.7962Table 4.
Results of classification with differenttraining datasets (F1-score).2.4 Error Analysis and Further WorkThe error analysis showed three main errortypes: not recognized, excessively recognizedand partially recognized aspect terms (head wordis recognized correctly, e.g., ?separate RAMmemory?
instead of ?RAM memory?).
Whilefirst types are recall and precision errors respec-tively, partial aspect extraction yields both recalland precision errors.
A summary of the errors ontest dataset is presented in Table 5.laptops restaurantsnot recognized 68% 58%partiallyrecognized18% 30%excessivelyrecognized14% 12%Table 5.
Error types distribution.From Table 5, we can see that a major sourceof errors is related to not recognized aspectterms.
In the future, we would like to experimentwith additional techniques to overcome recallproblem, e.g., using dictionaries or concept tax-onomies and employ skip-chain CRF, proposedby Li et al.
(2010).
Further improvements can alsobe made by tuning parameters of CRF learning.To verify the cross-domain portability of thesystem, we are going to test it on a third domaintest dataset without including additional instanc-es in the training corpus, as proposed by Aue andGamon (2005).3 ConclusionIn this paper, we have presented a CRF-basedlearning technique applied to the aspect extrac-tion task.
We implemented rich set of lexical,syntactic and statistical features and showed thatour approach has good domain portability andperformance ranked first out of 28 participatingteams in the laptop domain and fourth in restau-rant domain.312ReferencesAnthony Aue and Michael Gamon.
2005.
Customiz-ing sentiment classifiers to new domains: a casestudy.
In Proceedings of Recent Advances in Natu-ral Language Processing, RANLP-2005.Sasha Blair-Goldensohn, Kerry Hannan, RyanMcDonald, Tyler Neylon, George A. Reis, and JeffReynar.
2008.
Building a sentiment summarizer forlocal service reviews.
In Proceedings of WWW-2008 workshop on NLP in the Information Explo-sion Era.V.
S. Jagtap and Karishma Pawar.
2012.
Analysis ofdifferent approaches to Sentence-Level SentimentClassification.
International Journal of ScientificEngineering and Technology, Volume 2, Issue 3.Niklas Jakob and Iryna Gurevych.
2010.
Extractingopinion targets in a single- and cross-domain set-ting with conditional random fields.
In Proceedingsof the 2010 Conference on Empirical Methods inNatural Language Processing, EMNLP?10.Soo-Min Kim and Eduard Hovy.
2004.
Determiningthe sentiment of opinions.
In Proceedings of In-terntional Conference on Computational Linguis-tics, COLING?04.Nozomi Kobayashi, Ryu Iida, Kentaro Inui, and YujiMatsumoto.
2006.
Opinion mining on the Web byextracting subject-attribute-value relations.
In Pro-ceedings of AAAI-CAAW ?06.John Lafferty, Andrew McCallum, and Fernando Pe-reira.
2001.
Conditional Random Fields: Probabil-istic Models for Segmenting and Labeling Se-quence Data.
In Proceedings of the Eighteenth In-ternational Conference on Machine Learning,ICML?01.Fangtao Li, Chao Han, Minlie Huang, Xiaoyan Zhu,Ying-Ju Xia, Shu Zhang, and Hao Yu.
2010.
Struc-ture-aware review mining and summarization.
InProceedings of the 23rd International Conferenceon Computational Linguistics, COLING?10.Bing Liu.
2012.
Sentiment Analysis and OpinionMining.Samaneh Moghaddam and Martin Ester.
2012.
As-pect-based opinion mining from online reviews.Tutorial at SIGIR Conference.Arjun Mukherjee and Bing Liu.
2012.
Aspect Extrac-tion through Semi-Supervised Modeling.
In Pro-ceedings of 50th Anunal Meeting of Association forComputational Linguistics, ACL?12.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
: sentiment classification usingmachine learning techniques.
In Proceedings ofConference on Empirical Methods in Natural Lan-guage Processing, EMNLP?02.Ana-Maria Popescu and Oren Etzioni.
2005.
Extract-ing product features and opinions from reviews.
InProceedings of Conference on Empirical Methodsin Natural Language Processing, EMNLP?05.Lawrence R. Rabiner.
1989.
A tutorial on hiddenMarkov models and selected applications in speechrecognition.
In Proceedings of the IEEE, 77(2): p.257-286.Maksim Tkachenko and Andrey Simanovsky.
2012.Named Entity Recognition: Exploring Features.
InProceedings of KONVENS?12.James Todhunter, Igor Sovpel and Dzi-anis Pastanohau.
System and method for automaticsemantic labeling of natural language texts.
U.S.Patent 8 583 422, November 12, 2013.James Todhunter, Igor Sovpel and Dzi-anis Pastanohau.
Question-answering system andmethod based on semantic labeling of text docu-ments and user questions.
U.S. Patent 8 666 730,September 16, 2014.Peter D. Turney.
2002.
Thumbs up or thumbs down?
:semantic orientation applied to unsupervised classi-fication of reviews.
In Proceedings of AnnualMeeting of the Association for Computational Lin-guistics, ACL?02.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of theHuman Language Technology Conference and theConference on Empirical Methods in Natural Lan-guage Processing, HLT/EMNLP?05.Lei Zhang and Bing Liu.
2014.
Aspect and EntityExtraction for Opinion Mining.
Data Mining andKnowledge Discovery for Big Data.Li Zhuang, Feng Jing, and Xiaoyan Zhu.
2006.
Moviereview mining and summarization.
In Proceedingsof ACM International Conference on Informationand Knowledge Management, CIKM?06.Maria Pontiki, Dimitrios Galanis, John Pavlopoulos,Haris Papageorgiou, Ion Androutsopoulos,and Suresh Manandhar.
2014.
SemEval-2014 Task4: Aspect Based Sentiment Analysis.
In Proceed-ings of the 8th International Workshop on Seman-tic Evaluation (SemEval 2014), Dublin, Ireland.313
