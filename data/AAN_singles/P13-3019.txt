Proceedings of the ACL Student Research Workshop, pages 130?135,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsA New Syntactic Metric for Evaluation of Machine TranslationMelania DumaDepartment of ComputerScienceUniversity of HamburgVogt-K?lln-Stra?e 3022527 Hamburgduma@informatik.uni-hamburg.deCristina VertanFaculty for Language,Literature and MediaUniversity of HamburgVon Melle Park 620146 Hamburgcristina.vertan@uni-hamburg.deWolfgang MenzelDepartment of ComputerScienceUniversity of HamburgVogt-K?lln-Stra?e 3022527 Hamburgmenzel@informatik.uni-hamburg.deAbstractMachine translation (MT) evaluation aims atmeasuring the quality of a candidatetranslation by comparing it with a referencetranslation.
This comparison can beperformed on multiple levels: lexical,syntactic or semantic.
In this  paper, wepropose a new syntactic metric for MTevaluation based on the comparison of thedependency structures of the reference andthe candidate translations.
The dependencystructures are obtained by means of aWeighted Constraints Dependency Grammarparser.
Based on  experiments performed onEnglish to German translations, we show thatthe new metric correlates well with humanjudgments at the system level.1 IntroductionResearch in automatic machine translation (MT)evaluation has the goal of developing a set ofcomputer-based methods that measure accuratelythe correctness of the output generated by a MTsystem.
However, this task is a difficult onemainly because there is no unique referenceoutput that can be used in the comparison withthe candidate translation.
One sentence can haveseveral correct translations.
Thus, it is difficult todecide if the deviation from an existing referencetranslation is a matter of style (the use ofsynonymous words, different syntax etc.)
or areal translation error.Most of the automatic evaluation metricsdeveloped so far are focused on the idea oflexical matching between the tokens of one ormore reference translations and the tokens of acandidate translation.
However, structuralsimilarity between a reference translation and acandidate one cannot be captured by lexicalfeatures.
Therefore, research in MT evaluationexperiences a gradual shift of focus from lexicalmetrics to structural ones, whether they aresyntactic or semantic or a combination of both.This paper introduces a new syntacticautomatic MT evaluation method.
At this stageof research the new metric is evaluatingtranslations from any source language intoGerman.
Given that a set of constraint-basedgrammar rules are available for that language,extensions to other target languages are anytimepossible.
The chosen tool for providing syntacticinformation for German is the WeightedConstraints Dependency Grammar (WCDG)parser (Menzel and Schr?der, 1998), which ispreferred over other parsers because of itsrobustness to ungrammatical input, as it is typicalfor MT output.
The rest of this paper is organizedas follows.
In Section 2 the state of the art in MTevaluation is presented, while in Section 3 thenew syntactic metric is described.
Theexperimental setup and results are presented inSection 4.
The last section deals with theconclusions and future work.2 State of the artAutomatic evaluation of MT systems relies onthe existence of at least one reference1 created bya human annotator.
Using an automatic methodof evaluation a score is computed, based on thesimilarity between the output of the MT systemand the reference.
This similarity can becomputed at different levels: lexical, syntactic orsemantic.
At the lexical level, the metricsdeveloped so far can be divided into two majorcategories: n-gram based and edit distance based.1 We will use the term reference for the referencetranslation and the term translation for the candidatetranslation.130Among the n-gram based metrics, one of themost popular methods of evaluation is BLEU(Papineni et al 2001).
It provides a score that iscomputed as the summed number of n-gramsshared by the references and the output, dividedby the total number of n-grams.
Lexical metricsthat use the edit distance are constructed usingthe Levenshtein distance applied at the wordlevel.
Among these metrics, WER (Niessen etal., 2000) is the one which is used morefrequently; it calculates the minimal number ofinsertion, substitutions and deletions needed totransform the candidate translation into areference.Metrics based on lexical matching suffer fromnot being able to consider the variationencountered in natural language.
Thus, theyreward a low score to an otherwise fluent andsyntactically correct candidate translation, if itdoes not share a certain number of words withthe set of references.
Because of this, majordisagreements between the scores assigned byBLEU and human judgments have been reportedin Koehn and Monz (2006) and Callison-Burchet al(2006).
Another disadvantage is that manyof them cannot be applied at the segment level,which is often needed in order to better assessthe quality of MT output and to determine whichimprovements should be made to the MT system.Because of these disadvantages there is anincreasing need for other approaches to MTevaluation that go beyond the lexical level of thephrases compared.In Liu and Gildea (2005),  three syntacticevaluation metrics are presented.
The first ofthese metrics, the Subtree Metric (SMT), isbased on determining the number of subtrees thatcan be found in both the candidate translationand the reference phrase structure trees.
Thesecond metric, which is a kernel-based subtreemetric, is defined as the maximum of the cosinemeasure between the MT output and the set ofreferences.
The third metric proposed computesthe number of matching n-grams between theheadword chains of the reference and thecandidate translation dependency trees obtainedusing the parser described in (Collins, 1999).The idea of syntactic similarity is furtherexploited in Owczarzak et al(2007) which usesa Lexical Functional Grammar (LFG) parser.The similarity between the translation and thereference is computed using the precision and therecall of the dependencies that illustrate the pairof sentences.
Furthermore, paraphrases are usedin order to improve the correlation with humanjudgments.
Another  set of syntactic metrics hasbeen  introduced in Gimenez (2008); some ofthem are based on analyzing different types oflinguistic information (i.e.
part-of-speech orlemma).3 A new syntactic automatic metricIn this section we introduce the  new syntacticmetric  which is based on constraint dependencyparsing.
In the first subsection, the WCDG parseris presented, together with the advantages ofusing this parser over the other ones available,while the second subsection provides a detaileddescription of the new metric.3.1 Weighted Constraint DependencyGrammar ParserOur research was performed using a dependencyparser.
We decided on this type of parserbecause, as opposed to constituent parsers, itoffers the possibility of better representing non-projective structures.
Moreover, it has beenshown in Kuebler and Prokic (2006) that, at leastin the case of German, the results achieved by adependency parser are more accurate than theones obtained when parsing using constituentparsers, and this is because dependency parserscan handle better long distance relations andcoordination.The goal of constraint dependencygrammars (CDG) is to create dependencystructures that represent a given phrase (Schr?deret al 2000) on parallel levels of analysis.
Arelation between two words in a sentence isrepresented using an edge, which connects theregent and the dependent.
Edges are annotatedusing labels in order to distinguish betweendifferent types of relations.
A constraint is madeup of a logical formula that describes propertiesof the tree.
One property, for example, that isalways enforced is that no word can have morethan one regent on any level at a time.
During theanalysis, each of the constraints is applied toevery edge or every pair of edges belonging tothe constructed dependency parse tree.
The mainadvantage of using constraint dependencygrammars over dependency grammars based ongenerative rules is that they can deal better withfree word order languages (Foth, 2004).Weighted Constraint Dependency Grammar(WCDG) (Menzel and Schr?der, 1998) assignsdifferent weights to the constraints of thegrammar.
Every constraint in WCDG is assigneda score which is a number between 0.0 and 1.0,131while the general score of a parse is calculated asthe product of all the scores of all the instancesof constraints that have not been satisfied.
Rulesthat have a score of 0 are called hard rules,meaning that they cannot be ignored, which isthe case of the one regent only rule mentionedearlier.
The advantage of using gradedconstraints, as opposed to crisp ones, stems fromthe fact that weights allow the parser to tolerateconstraint violations, which, in turn, makes theparser robust against ungrammaticality.
Theparser was evaluated using different types oftexts, and the results show that it has an accuracybetween 80% and 90% in computing correctdependency attachments depending on the typeof text (Foth et al 2004a).The benefit of using WCDG over other parsersis that it provides further information on a parse,like the general score of the parse and theconstraints that are violated by the final result.This information can be further explored in orderto perform an error analysis.
Moreover, becauseof the fact that the candidate translations aresometimes not well-formed, parsing themrepresents a challenge.
However, WCDG willalways provide a final result, in the form of adependency structure, even though it might havea low score due to the violated constraints.3.2 Description of the metricIn order to define a  new syntactic metric for MTevaluation, we have incorporated the WCDGparser in the process of evaluation.
Because theoutput of the WCDG parser is a dependency tree,we have looked into techniques of measuringhow similar two trees are.
Our aim was todetermine whether a tree similarity metricapplied on the two dependency parse trees wouldprove to be an efficient way of capturing thesimilarity between the reference and thetranslation.
Let us consider this example, inwhich the reference sentence is ?Die schwarzeKatze springt schnell auf den roten Stuhl.?
(engl.The black cat jumps quickly on the red chair)and the candidate translation is?Auf den rotenStuhl schnell springt die schwarze Katze?
(engl.On the red chair quickly jumps the red cat).
Eventhough the word order of the two segments isquite different, and the translation has anincorrect syntax, they roughly have the samemeaning.
We present in Figure 1 the dependencyparse trees obtained using WCDG for thesentences considered.
We can observe that thegeneral structure of the translation is similar tothat of the reference, the only difference beingthe reverse order between the left subtree and theright subtree.
The tree similarity measure that wechose to use was the All Common EmbeddedSubtrees (ACET) (Lin et al 2008) similarity.Given a tree T, an embedded subtree is obtainedby removing one or more nodes, except for theroot, from the tree T. The idea behind ACET isthat, the more substructures two trees share, themore similar they are.
Therefore, ACET isdefined as the number of common embeddedsubtrees shared between two trees.
The resultsreported in Lin et al(2008) show that ACEToutperforms tree edit distance (Zhang andShasha, 1989) in terms of efficiency.Figure 1.
Example of dependency parse trees forreference and candidate translationsIn our experiments, we have applied the ACETalgorithm, and computed the number of commonembedded subtrees between the dependencyparse trees of the hypothesis and the reference.Because of the additional information providedby the parsing, pre-processing of the output ofthe WCDG parser was necessary in order totransform the dependency tree into a general tree.We first removed the labels assigned to everyedge, but maintained the nodes and the left toright order between them.In the following, we will refer to the newproposed metric using CESM (CommonEmbedded Subtree Metric).
CESM wascomputed using the precision, the recall and theF-measure of the common embedded subtrees ofthe reference and the translation:132where treeref and treehyp represent thepreprocessed dependency trees of the referenceand the hypothesis translations.4 Experimental setup and evaluationIn order to determine how accurate CESM is incapturing the similarity between references andtranslations, we evaluated it at the system leveland at the segment level.
The evaluation wasconducted using data provided by the NAACL2012 WMT workshop (Callison-Burch et al2012).
The test data for the workshop consistedof 99 translated news articles in English,German, French, Spanish and Czech.At the system level, the initial German test setprovided at the workshop was filtered accordingto the length of segments.
This was done in orderto limit the time requirements of WCDG.
As aresult, 500 segments with a length between 50and 80 characters were extracted from theGerman reference file.
In the next step, wearbitrarily selected the outputs of 7 of the 15systems that were submitted for evaluation in theEnglish to German translation task: DFKI(Vilar, 2012), JHU (Ganitkevitch et al 2012),KIT (Niehues et al 2012), UK (Zeman, 2012)and three anonymized system outputs referred toas OnlineA, OnlineB, OnlineC.After this initial step of filtering the data, the 7systems were evaluated by calculating the CESMscore for every pair of reference and translationsegments corresponding to a system.
Theaverage scores obtained are depicted in Table 1.Evaluation of the metric at the system level wasperformed by measuring the correlation of theCESM metric with human judgments usingSpearman's rank correlation coefficient ?
:where n represents the number of MT systemsconsidered during evaluation, and di2 representsthe difference between the ranks, assigned to asystem, by the metric and the human judgments.The minimum value of ?
is -1, when there is nocorrelation between the two rankings, while themaximum value is 1, when the two rankingscorrelate perfectly (Callison-Burch et al 2012).In order to compute the ?
score, the scoresattributed to every system by CESM, wereconverted into ranks.
From the different rankingstrategies that were presented by the WMT12workshop, the standard ranking order waschosen.
The ?
rank correlation coefficient wascalculated as being ?
= 0.92, which shows thereis a strong correlation between the results ofCESM and the human judgments.
In order tobetter assess the quality of CESM, the test setwas also evaluated using NIST (Doddington,2002), which managed to obtain the same rankcorrelation coefficient of ?
= 0.92.No.
SystemnameCESMscoreNISTscore1 DFKI  0.069 4.77092 JHU 0.073 4.99043 KIT 0.090 5.13584 OnlineA 0.093 5.30395 OnlineB 0.091 5.30396 OnlineC 0.085 4.80227 UK 0.075 4.6579Table 1.
System level evaluation resultsThe first step in evaluating at the segment levelwas filtering the initial test set provided by theWMT12 workshop.
For this purpose, 2500reference and translation segments were selectedwith a length between 50 and 80 characters.
TheKendall tau rank correlation coefficient wascalculated in order to measure the correlationwith human judgments, where Kendall tau(Callison-Burch et al 2012) is defined as:In order to compute the value of Kendall tau, wedetermined the number of concordant pairs andthe number of discordant pairs of judgments.Similarly to the guideline followed during theWMT12 workshop (Callison-Burch et al 2012),we penalized ties given by CESM and ignoredties assigned by the human judgments.
Theobtained result was a correlation of 0.058.
As aterm of comparison, the highest correlation forsegment level reported in Callinson-Burch et al(2012) was 0.19 obtained by TerrorCat (Fishel etal., 2012) and the lowest was BlockErrCats(Popovic, 2012) with 0.040.
However, theseresults were obtained by evaluating on the entiretest set.
The rather low correlation result weobtained can be partially explained by the factthat only one judgment of a pair of reference andtranslation was taken into account.
It will be133interesting to see how the averaging of the ranksof a translation influences the correlationcoefficient.5 Conclusions and future workIn this paper, a new evaluation metric for MTwas introduced, which is based on thecomparison of dependency parse trees.
Thedependency trees were obtained using theWCDG German parser.
The reason why wechose this parser was that, due to its architecture,it is able to handle  ungrammatical andambiguous input data.
The experimentsconducted so far show that using the data madeavailable at the NAACL 2012 WMT workshop,CESM correlates well with the human judgmentsat the system level.
One of the futureexperiments that we intend to perform is toassess metric quality on the entire evaluation set.Moreover, we plan to compare CESM with othertree-based MT metrics.
Furthermore, theWMT12 workshop offers different rankingpossibilities, like the ones presented in Bojar etal (2011) and in Lopez (2012).
It will bedetermined how much are the segment levelevaluation results influenced by these rankingorders.One limitation of the proposed metric is that,at the moment it is restricted to translations fromany source language to German as a targetlanguage.
Because of this reason, we plan toextend the metric to other languages and see howwell it performs in different settings.
In furtherexperiments we also intend to test CESM usingstatistical based dependency  parsers, like theMalt Parser (Nivre et al 2007) and the MSTparser (McDonald et al 2006), in order todecide whether the choice of parser influencesthe performance of the metric.Another approach that we will explore forimproving CESM is to compare dependencyparse trees using the base form and the part-of-speech of the tokens, instead of the exact lexicalmatch.
We will try this approach in order toavoid penalizing lexical variation.The accuracy of CESM can be furtherincreased by the use of paraphrases, which canbe obtained by using a German thesaurus or alexical resource like GermaNet (Hamp andFeldweg, 1997).
Furthermore, a technique likethe one described in Owczarzak (2008) can beimplemented for generating domain specificparaphrases.
The results reported show that theuse of this kind of paraphrases in order toproduce new references has increased the BLEUscore, therefore this is an approach that will befurther investigated.AcknowledgmentsThis work was funded by the University ofHamburg Doctoral Fellowships in accordancewith the Hamburg Act for the Promotion ofYoung Researchers and Artists (HmbNFG), andthe EAMT Project ?Using Syntactic andSemantic Information in the Evaluation ofCorpus-based Machine Translation?.ReferenceO.
Bojar, M.
Ercegov?evi?, M Popel and O. Zaidan.2011.
A Grain of Salt for the WMT ManualEvaluation.
Proceedings of the Sixth Workshopon Statistical Machine Translation.C.
Callison-Burch, M. Osborne and P. Koehn.
2006.Re-evaluating the Role of Bleu in MachineTranslation Research.
Proceedings of EACL-2006.C.
Callison-Burch, P. Koehn, C. Monz, M. Post, R.Soricut and L. Specia.
2012.
Findings of the2012 Workshop on Statistical MachineTranslation.
Proceedings of WMT12.M.
J. Collins.
1999.
Head-driven StatisticalModels for Natural Language Parsing.
Ph.D.thesis, University of Pennsylvania.G.
Doddington.
2002.
Automatic Evaluation ofMachine Translation Quality Using N-gramCo-Occurrence Statistics.
Proceedings of the2nd International Conference on Human LanguageTechnology.K.
Foth.
2004.
Writing weighted constraints forlarge de-pendency grammars.
Recent Advancesin De-pendency Grammar, Workshop COLING2004.K.
Foth, M. Daum and W. Menzel.
2004a.
A broad-coverage parser for German based ondefeasible constraints.
KONVENS 2004,Beitr?ge zur 7, Konferenz zur Verarbeitungnat?rlicher Sprache, Wien.K.
Foth, M. Daum and W. Menzel.
2004b.Interactive grammar development withWCDG.
Proc.
of the 42nd Annual Meeting of theAssociation for Com-putational Linguistics.K.
Foth, T. By and W. Menzel.
2006.
Guiding acon-straint dependency parser with supertags.Proceedings of the 21st Int.
Conf.
onComputational Linguistics.134M.
Fishel, R. Sennrich, M. Popovic and O. Bojar.2012.
TerrorCat: a translation errorcategorization-based MT quality metric.Proceedings of the Seventh Workshop onStatistical Machine Translation.J.
Ganitkevitch, Y. Cao, J. Weese, M. Post and C.Callison-Burch.
2012.
Joshua 4.0: Packing,PRO, and paraphrases.
Proceedings of theSeventh Workshop on Statistical MachineTranslation.J.
Gimenez.
2008.
Empirical Machine Translationand its Evaluation.
Ph.
D. thesis.B.
Hamp and H. Feldweg.
1997.
GermaNet - aLexical-Semantic Net for German.
Proc.
ofACL workshop Automatic Information Extractionand Building of Lexical Semantic Resources forNLP Applications.P.
Koehn and C. Monz.
2006.
Manual andAutomatic Evaluation of Machine Translationbetween European Languages.
NAACL 2006Workshop on Statistical Machine Translation.P.
Koehn.
2010.
Statistical Machine Translation.Cambridge University Press.S.
K?bler and J. Prokic.
2006.
Why is GermanDependency Parsing more Reliable thanConstituent Parsing?.
Proceedings of the FifthInternational Work-shop on Treebanks andLinguistic Theories.Z.
Lin, H. Wang, S. McClean and C. Liu.
2008.
AllCommon Embedded Subtrees for MeasuringTree Similarity.
International Symposium onComputational Intelligence and Design.D.
Liu and D. Gildea.
2005.
Syntactic Features forEvaluation of Machine Translation.
ACL 2005Workshop on Intrinsic and Extrinsic EvaluationMeasures for Machine Translation and/orSummarization.A.
Lopez.
2012.
Putting human assessments ofmachine translation systems in order.Proceedings of the Seventh Workshop onStatistical Machine Translation.R.
McDonald, K. Lerman and F. Pereira.
2006.Multilingual Dependency Parsing with a Two-Stage Discriminative Parser.
Tenth Conferenceon Computational Natural Language Learning.W.
Menzel and I. Schr?der.
1998.
DecisionProcedures for Dependency Parsing UsingGraded Constraints.
Workshop On ProcessingOf Dependency-Based Grammars.J.
Niehues, Y. Zhang, M. Mediani, T. Herrmann, E.Cho and A. Waibel.
2012.
The karlsruhe instituteof technology translation systems for the WMT2012.
Proceedings of the Seventh Workshop onStatistical Machine Translation.S.
Niessen, F. J. Och, G. Leusch  and H. Ney.
2000.An Evaluation Tool for Machine Translation:Fast Evaluation for MT Research.
Proceedingsof the 2nd International Conference on LanguageResources and Evaluation (LREC).J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit, S.K?bler, S. Marinov  and E. Marsi.
2007.MaltParser: A language-independent systemfor data-driven dependency parsing.
NaturalLanguage Engineering.K.
Owczarzak, J. van Genabith and A.
Way.
2007.Dependency-based automatic evaluation formachine translation.
Proceedings of SSST,NAACL-HLT 2007 / AMTA Workshop on Syntaxand Structure in Statistical Translation.K.
Owczarzak.
2008.
A Novel Dependency-BasedEvaluation Metric for Machine Translation,Ph.D.
thesis.K.
Papineni, S. Roukos, T. Ward and W.-J.
Zhu.2001.
Bleu: a method for automatic evaluationof machine translation.
RC22176 (TechnicalReport), IBM T.J. Watson Research Center.M.
Popovic.
2012.
Class error rates for evaluationof machine translation output.
Proceedings ofthe Seventh Workshop on Statistical MachineTranslation.I.
Schr?der, W. Menzel, K. Foth and M. Schulz.
2000.Modeling dependency grammar withrestricted constraints.
Traitement Automatiquedes Langues.I.
Schr?der, H. Pop, W. Menzel and K. Foth.
2001.Learning grammar weights using geneticalgorithms.
Proceedings Euroconference RecentAdvances in Natural Language Processing.I.
Schr?der.
2002.
Natural Language Parsing withGraded Constraints.
Ph.D. thesis, Dept.
ofComputer Science, University of Hamburg.D.
Vilar.
2012.
DFKI?s SMT system for WMT2012.
Proceedings of the Seventh Workshop onStatistical Machine Translation.D.
Zeman.
2012.
Data issues of the multilingualtranslation matrix.
Proceedings of the SeventhWorkshop on Statistical Machine Translation.K.
Zhang and D. Shasha.
1989.
Simple fastalgorithms for the editing distance betweentrees and related problems.
SIAM Journal onComputing.135
