AN OVERVIEW OF DR-LINKAND ITSAPPROACH TO DOCUMENT F ILTERINGElizabeth D.Liddy 1, Woojin Paik I, Edmund S. Yu 2, Kenneth A. McVearry 31 School of Infcxmation StudiesSyracuse UniversitySyracuse, NY 132442 College of Engineering and Computer ScienceSyracuse UniversitySyracuse, NY 132443 Coherent Research, Inc.1 Adler DriveFEast Syracuse, NY 130571.
MOTIVATIONDR-LINK is an information retrieval system, complexin design and processing, with the potential forproviding significant advances inretrieval results due tothe range and richness of semantic representation doneby the various modules in the system.
By using a fullcontinuum of linguistic-conceptual processing, DR-LINK has the capability of producing documents whichprecisely match users' needs.
Each of DR-LINK's sixprocessing modules add to the conceptual enhancementof the document and query representation bymeans ofcontinual semantic enrichments to the text.
Richrepresentations are essential to meet the retrievalrequirements of complex information eeds and toreduce the ambiguities associated with keyword-basedretrieval.
To produce this enriched representation, thesystem uses lexical, syntactic, semantic, and discourselinguistic processing techniques for distilling fromdocuments and topic statements all the rich layers ofknowledge incorporated in their deceptively simpletextual surface and for producing a tex~ml representationwhich has been shaped by all these levels of linguisticprocessing.A vital aspect of our approach which is evidenced in thevarious emantic enrichments (e.g.
Subject Field Codes,proper noun categories, discourse components, concept-relation-concept triples, Conceptual Graphs) added to thebasic text, is the real attention paid to representation ata deeper than surface level.
That is, DR-LINK dealswith lexical entities via conceptually-based linguisticprocessing.
For example, complex nominals areinterpreted as meaningful multi-word constituentsbecause the combination of individual terms in complexnominals conveys quite different meanings than if theindividual constituents were interpreted separately.
Inaddition, verbs are represented by ease-frames so thatother lexical entities in the sentence which performparticular semantic roles in relation to the verb arerepresented according to these semantic roles.
Also, therich semantic data (e.g.
location, purpose, nationality)that are conveyed in the appositional phrases typicallyaccompanying proper nouns, are represented in such away that the semantic relations implicitly conveyed inthe appositions are explicitly available for more refinedrepresentation a d matching.2.
OVERVIEWDR-LINK's system architecture is modular in design,with six processing modules, each of which enhance thedocument and query representation in terms of continualsemantic enrichments o the text.
Briefly overviewed,the system's six modules function as follows:1.The Subiect Field Coder uses semantic wordknowledge to produce asummary-level topical vectorrepresentation of a document's contents that ismatched to a vector representation of a topicstatement in order to rank all documents for subject-based similarity to a query.
All of the documents withtheir Subject Field Code vectors are passed to:3582.
The Prover Noun Interpi~ter, which uses a variety ofknowledge bases and context heuristics to categorizeevery proper noun in the text.
The similaritybetween a query's proper noun requirements and eachdocument's Proper Noun Field is evaluated andcombined with the similarity value from the SubjectField Coder for a reranking of all documents inresponse to the query.
Those documents with amathematically determined potential for beingrelevant to the query are then passed to:3.
The Text Structurer.
which sub-divides a text intoits discourse-level segments in order to focus querymatching to the appropriate discourse component inresponse to particular types of information eeds.All of the structured texts, with the appropriatecomponents weighted, are passed to:4.
The Relation-Concert Detector.
whose purpose is toraise the level at which we do matching from a key-word or key-phrase l vel to a more concep~j~l levelby expanding terms in the topic statement to allterms which have been shown to be 'substitutable'for them.
Then, semantic relations between conceptsare recognized in both documents and topicstatements u ing separate handlers for the variousparts of speech.
This module produces concept-relation-concept triples which are passed to:5.
The Concemual Granh Generator which convertsthese triples into the CG formalism (Sowa, 1984), avariant of semantic networks in which arcs betweennodes are coded for relations.
The resultant CGs arepassed to:6.
The Concemual Gravh Marcher, which measures thedegree to which a particular topic statement CG andcandidate document CGs share a common structure,and does a final ranking of the documents.In combination, these six stages of processing producetextual representations that capture breadth and varietyof semantic knowledge.
However, since the ConceptualGraph generation and matching are so computationallyexpensive, we also take care to eliminate from furtherprocessing for each query, those documents which haveno likefihood of being relevant to a well-specified queryor query-profile.3.
DOCUMENT F ILTERING WITHINDR-L INKThe fact that information-intense governmentorganizations receive thousands of documents daily withonly a relatively small subset of them being ofpotential interest to any individual user suggests thatthe routing application of information retrieval can beapproached as a filtering process, with the types andoptimal number of filterings dependent on the desiredgranularity of filtering.
Our research demonstrates how afirst, rough-cut, purely content-based document filtercan be used to produce its appropriate preliminaryranking of an incoming flow of documents for eachuser.
Using the similarity values produced by the SFCFilter, later system modules further efine the rankingand perform f'mer levels of analysis and matching.The success of our filtering approach is attributable tothe representation scheme we use for all texts, bothdocuments and queries.
The Subject Field Codes (SFCs)are based on a culturally validated semantic odingscheme developed for use in Lon~man's Dictionary ofContemnorarv En21ish (LDOCE), a general purposedictionary.
Operationally, our system tags each word ina document with the appropriate SFC from thedictionary.
The within-document SFC frequencies arenormalized and each document is represented as afrequency-weighted, fixed-length vector of the SFCsoccurring in that document (see Figure 1).
For routing,queries are likewise represented asSFC vectors.
Thesystem matches each query SFC vector to the SFCvector of all incoming documents, which are thenranked on the basis of their vectors' similarity to thequery.
Those documents whose SFC vectors exceed apredetermined criterion of similarity to the query SFCvector can be displayed to the user immediately orpassed on to the Proper Noun Interpreter for furtherprocessing and a second-level r -ranking.The real merit of the SFC vectors is that hey representtexts at a more abstract, conceptual level than theindividual words in the natural language textsthemselves, thereby addressing the dual problems ofsynonymy and polysemy.
On the one hand, the use ofSFCs takes care of the "synonymous phrasing" problemby representing text at a level above the word-level bythe assignment of one SFC from amongst 124 possiblecodes to each word in the document.
This means that fffour synonymous terms were used within a text, oursystem would assign each of them the same SFC sincethey share a common domain which would be reflectedby their sharing a common SFC.
For example, severaldocuments hat discuss the effects of recent politicalmovements on legislation regarding civil rights wouldhave similar SFC vector epresentations even thoughthe vocabulary choices of the individual authors mightbe quite varied.
Even more importantly, if a user who isseeking documents on this same topic expresses her359A U. S. magistrate in Florida ordered Carlos Lehder Rivas, described as among the world~ leading cocainetraffickers, held without bond on 11 drug-smuggling counts.
Lehder, who was captured last week in Colombiaand immediately extradited to the U.S., pleaded innocent o the charges in federal court in Jacksonville.LAW .2667 SOCIOLOGY .1333BUSINESS .1333 ECONOMICS .0667DRUGS .1333 MILITARY .0667POLITICAL SCIENCE .1333 OCCUPATIONS .0667Fig.
1: Sample Wall Street Journal document and its SFC representationinformation eed in terms which do not match thevocabulary of any of the documents, her query will stillshow high similarity to these documents'representations because both the query's representationand the documents' representations are at the moreabstract, semantic-field level and the distribution ofSFCs on the vectors of the query and the relevantdocuments would be proportionately similar across theSFCs.The other problem with natural language as arepresentation alternative that has plagued its use ininformation retrieval is polysemy, the ability of asingle word to have multiple senses or meanings.
OurSFCoder uses psycholinguisfically-justified s nsedlsambiguation procedures (Liddy & Paik, 1992) toselect a single sense for each word.
Ambiguity is aserious problem, particularly in regard to the mostfrequently used lexical items.
According to Gentner(1981) the twenty most frequent nouns in English havean average of 7.3 senses each, while the twenty mostfrequent verbs have an average of 12.4 senses each.Since a particular word may function as more than onepart of speech and each word may also have more thanone sense, each of these entries and/or senses may beassigned ifferent SFCs.
This is a slight variant of thestandard isambiguation problem, which has shownitself to be nearly intractable for most NLPapplications, but which is successfully handled in DR-LINK, thereby allowing the system to producesemantically accurate SFC vectors.We based our computational approach to successfuldisambiguation current psycholinguistic researchfiterature which we interpret as suggesting that here arethree potential sources of influence on the humandisambignafion process: 1) local context, 2) domainknowledge, and 3) frequency data.
We havecomputationally approximated these three knowledgesources in our disambiguator.
The disambiguafionprocedures were tested by having the system select asingle SFC for each word.
These SFCs were comparedto the sense-selections made by an independent judge.The disambignation implementation selected the correctSFC 89% of the time.
This means that a word such as'drugs', which might refer to either medically prescribedremedies or illegal intoxicants that are traded on thestreet would be represented by different SFCs based onthe context in which it occurred.4.
PROCESSING IN THE SUBJECTFIELD CODERIn the Subject Field Coder, the following Stages ofprocessing are done:In Stage 1 processing, we run the documents andquery through a probabilistic part of speech tagger(Meteer et al 1991) in order to restrict candidate S1FCsof a word to those of the appropriate syntactic category.Stage 2 processing retrieves SFCs of each word'scorrect part of speech from the lexical database andassigns the SFCs.Stage 3 then uses an ordered set of sentence-levelcontext-heuristics to determine a word's correct SFC ifmultiple SFCs have been assigned to a word's differentsenses.
First, the SFCs attached to all words in asentence are evaluated to determine at the sentence l velwhether any words have only one SFC assigned to alltheir senses in LDOCE (unique-SFC), and; secondly,the SFCs which are assigned to more than three wordsin the sentence (frequent-SFC).Stage 4 scans the SFCs of each remaining word todetermine whether the unique-SFCs or frequent-SFCsdiscovered in Stage 3 occur amongst the multiple SFCsassigned by LDOCE to the anabignous word.
Thoseambiguous words which have no SFC in common withthe unique-SFCs or frequent-SFCs for that sentence arepassed on to the next stage.360Stage $ incorporates two global knowledge sources tocomplete the sense disambiguation task.
The primarysource is a correlation matrix which reflects tableestimates of SFC co-occunences within documents.
Thesecond source is the order in which the senses of a wordare listed in LDOCE which is based on frequency of usein the English language.
In Stage 5, each of theremaining ambiguous words is resolved a word at atime, accessing the matrix via the unique and mostfrequent-SFCs of the sentence.
The system evaluates thecorrelation coefficients between the unique and mostfrequent-SFCs of the sentence and the multiple SFCsassigned to the word being disambiguated to determinewhich of the multiple SFCs has the highest correlationwith a unique-SFC or fTequent-SFC.
The system thenselects that SFC as the unambiguous representation fthe sense of the word.Stage 6 processing produces a vector of SFCs andtheir fi'equencies for each document and for the query.Stage 7 normalizes the vectors of each text, and at:Stage 8, the document vectors are compared to thequery vector using a similarity measure.
A rankedlisting of the documents in decreasing order ofsimilarity is produced.The assignment of SFCs is fully automatic and doesnot require any human intervention.
In addition, thislevel of semantic representation f texts is efficient andhas been empirically tested as a reasonable approach forranking documents from a very large incoming flux ofdocuments.
For the 18th month TIPSTER evaluation,the use of this representation allowed the system toquickly rank 60 megabytes of text in the routingsituation that was tested.
All the later-determinedrelevant documents were within the top 37% of theranked ocuments produced by the SFC Module.A second level of lexical-semantic processing furtherimproves the performance of DR-LINK as a reasonabledocument filter.
That is, the Proper Noun Interpreter(Paik et al this volume) computes the similaritybetween a query's proper noun requirements and eachdocument's Proper Noun Field and combines this valuewith the similarity value produced by the SFCoder for areranking in relation to the query.
In the 18th monthtesting of our system, the results of this reranking basedon the SFC values and the Proper Noun values placedall the relevant documents within the top 28% of thedatabase.5.
DOCUMENT CLUSTERINGUS ING SUBJECT F IELD CODESThese summary-level semantic vector epresentations ofeach text's contents produced by the SFCOder have alsoproven useful as a means for dividing a database intoclusters of documents pertaining to the same subjectarea.
The SFC vectors are clustered using Ward'sagglomerative clustering algorithm (Ward, 1963) toform classes in the document d~mbase.
Ad hoe queriesare represented as SFC vectors and matched to thecentroid SFC vector of each cluster in the database.Clusters whose centroid SFC vector exhibit highsimilarity to the query SFC vector can then be browsedby users who do not have a fully specified query, butwho prefer to browse groups of documents whoseoptimum content they can only loosely define to thesystem (Liddy, Paik, & Woelfel, 1992).A qualitative analysis revealed that clustering SFCvectors using Ward's clustering algorithm resulted inmeaningful groupings of documents hat were similaracross concepts not directly encoded in SFCs.
Twoexamples: all of the documents about AIDS clusteredtogether, although AIDS is not in LDOCE.
Secondly,all of the documents about he hostages in Iran clusteredtogether even though proper nouns are not included inLDOCE and the word 'hostage' is tagged with the sameSFC as hundreds of other terms.
What the SFCrepresentation f documents accomplishes, is thatdocuments about he same or very similar topics haverelatively equal distributions of words with the sameSFCs and will therefore cluster together in meaningfulgroups.6.
CONCLUSIONOur implementation a d testings of the SFCOder as ameans for semantically representing the content oftexts, either for the purpose of ranking a document setaccording to likelihood of being relevant o anindividual query or for producing conceptually relatedclusters of documents for browsing are very promising.Particularly worthy of note is the observation that in alarge operational system, the ability to filter out anaverage of 72% of the incoming flux of millions ofdocuments will have a significant impact on anydocument detection system's performance with whichthis semantic-based document falter is combined.REFERENCESGentner, D. (1981).
Some interesting differences361between verbs and nouns.
Cognition and brain theory.4(2), 161-178.Liddy, E.D., McVearry, K.A., Paik, W., Yu, E.S.
&McKenna, M; (In press).
Development, implementation& testing of a discourse model for newspaper texts.
!Lroceedings of the Human Language TechnologyWorkshov.
Princeton, NJ: March, 1993.Liddy, E.D.
& Paik, W. (1992).
Statistically-guidedword sense disambiguation.
I  Proceedings of AAAIFall Svm_vosium Series: Probabilistie av_vroa~h~s toI ~ .
Menlo Park, CA: AAAI.Liddy, E.D., Paik, W. & Woelfel, J.K. (1992).
Use ofsubject field codes from a machine-read__able dictionaryfor automatic classification of documents.
Advances inClassification Research: Proceedin2s of the 3rd A$ISSIG/CR Classification Research Workshon.
Medford,NJ: Learned Information, Inc.Meteer, M., Schwartz, R. & Weischedel, R. (1991).POST: Using probabilities in language processing.Proceedings ofthe Twelfth International Conference onArtificial Intelfi~ence.
Sydney, Australia.Paik, W., Liddy, E.D., Yu, E.S.
& McKenna, M. (Inpress).
Interpretation f Proper Nouns for InformationRetrieval.
Proceedings of the Human LanguageTechnology Workshop.
Princeton, NJ: March, 1993.Sowa, J.
(1984).
Conceptual structures: Informationvrocessin~ in mind and machine.
Reading, MA:A ~dd_ison-Wesley.Ward, J.
(1963).
Hierarchical grouping to optimize anobjection function.
Journal of the American StatisticalAssociation.
58, p. 237-254.362
