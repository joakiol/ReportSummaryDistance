Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1491?1500,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA Recursive Recurrent Neural Networkfor Statistical Machine TranslationShujie Liu1, Nan Yang2, Mu Li1and Ming Zhou11Microsoft Research Asia, Beijing, China2University of Science and Technology of China, Hefei, Chinashujliu, v-nayang, muli, mingzhou@microsoft.comAbstractIn this paper, we propose a novel recursiverecurrent neural network (R2NN) to mod-el the end-to-end decoding process for s-tatistical machine translation.
R2NN is acombination of recursive neural networkand recurrent neural network, and in turnintegrates their respective capabilities: (1)new information can be used to generatethe next hidden state, like recurrent neu-ral networks, so that language model andtranslation model can be integrated natu-rally; (2) a tree structure can be built, asrecursive neural networks, so as to gener-ate the translation candidates in a bottomup manner.
A semi-supervised training ap-proach is proposed to train the parameter-s, and the phrase pair embedding is ex-plored to model translation confidence di-rectly.
Experiments on a Chinese to En-glish translation task show that our pro-posed R2NN can outperform the state-of-the-art baseline by about 1.5 points inBLEU.1 IntroductionDeep Neural Network (DNN), which essential-ly is a multi-layer neural network, has re-gainedmore and more attentions these years.
With theefficient training methods, such as (Hinton et al,2006), DNN is widely applied to speech and im-age processing, and has achieved breakthrough re-sults (Kavukcuoglu et al, 2010; Krizhevsky et al,2012; Dahl et al, 2012).Applying DNN to natural language processing(NLP), representation or embedding of words isusually learnt first.
Word embedding is a dense,low dimensional, real-valued vector.
Each dimen-sion of the vector represents a latent aspect ofthe word, and captures its syntactic and semanticproperties (Bengio et al, 2006).
Word embeddingis usually learnt from large amount of monolin-gual corpus at first, and then fine tuned for spe-cial distinct tasks.
Collobert et al (2011) proposea multi-task learning framework with DNN forvarious NLP tasks, including part-of-speech tag-ging, chunking, named entity recognition, and se-mantic role labelling.
Recurrent neural networksare leveraged to learn language model, and theykeep the history information circularly inside thenetwork for arbitrarily long time (Mikolov et al,2010).
Recursive neural networks, which have theability to generate a tree structured output, are ap-plied to natural language parsing (Socher et al,2011), and they are extended to recursive neuraltensor networks to explore the compositional as-pect of semantics (Socher et al, 2013).DNN is also introduced to Statistical MachineTranslation (SMT) to learn several componentsor features of conventional framework, includ-ing word alignment, language modelling, transla-tion modelling and distortion modelling.
Yang etal.
(2013) adapt and extend the CD-DNN-HMM(Dahl et al, 2012) method to HMM-based wordalignment model.
In their work, bilingual wordembedding is trained to capture lexical translationinformation, and surrounding words are utilized tomodel context information.
Auli et al (2013) pro-pose a joint language and translation model, basedon a recurrent neural network.
Their model pre-dicts a target word, with an unbounded history ofboth source and target words.
Liu et al (2013) pro-pose an additive neural network for SMT decod-ing.
Word embedding is used as the input to learntranslation confidence score, which is combinedwith commonly used features in the convention-al log-linear model.
For distortion modeling, Liet al (2013) use recursive auto encoders to makefull use of the entire merging phrase pairs, goingbeyond the boundary words with a maximum en-tropy classifier (Xiong et al, 2006).1491Different from the work mentioned above,which applies DNN to components of conven-tional SMT framework, in this paper, we proposea novel R2NN to model the end-to-end decod-ing process.
R2NN is a combination of recursiveneural network and recurrent neural network.
InR2NN, new information can be used to generatethe next hidden state, like recurrent neural net-works, and a tree structure can be built, as recur-sive neural networks.
To generate the translationcandidates in a commonly used bottom-up man-ner, recursive neural networks are naturally adopt-ed to build the tree structure.
In recursive neuralnetworks, all the representations of nodes are gen-erated based on their child nodes, and it is difficultto integrate additional global information, such aslanguage model and distortion model.
In order tointegrate these crucial information for better trans-lation prediction, we combine recurrent neural net-works into the recursive neural networks, so thatwe can use global information to generate the nexthidden state, and select the better translation can-didate.We propose a three-step semi-supervised train-ing approach to optimizing the parameters ofR2NN, which includes recursive auto-encodingfor unsupervised pre-training, supervised localtraining based on the derivation trees of forced de-coding, and supervised global training using ear-ly update strategy.
So as to model the transla-tion confidence for a translation phrase pair, weinitialize the phrase pair embedding by leveragingthe sparse features and recurrent neural network.The sparse features are phrase pairs in translationtable, and recurrent neural network is utilized tolearn a smoothed translation score with the sourceand target side information.
We conduct exper-iments on a Chinese-to-English translation taskto test our proposed methods, and we get about1.5 BLEU points improvement, compared with astate-of-the-art baseline system.The rest of this paper is organized as follows:Section 2 introduces related work on applyingDNN to SMT.
Our R2NN framework is introducedin detail in Section 3, followed by our three-stepsemi-supervised training approach in Section 4.Phrase pair embedding method using translationconfidence is elaborated in Section 5.
We intro-duce our conducted experiments in Section 6, andconclude our work in Section 7.2 Related WorkYang et al (2013) adapt and extend CD-DNN-HMM (Dahl et al, 2012) to word alignment.In their work, initial word embedding is firstlytrained with a huge mono-lingual corpus, then theword embedding is adapted and fine tuned bilin-gually in a context-depended DNN HMM frame-work.
Word embeddings capturing lexical trans-lation information and surrounding words model-ing context information are leveraged to improvethe word alignment performance.
Unfortunately,the better word alignment result generated by thismodel, cannot bring significant performance im-provement on a end-to-end SMT evaluation task.To improve the SMT performance directly, Auliet al (2013) extend the recurrent neural networklanguage model, in order to use both the sourceand target side information to scoring translationcandidates.
In their work, not only the target wordembedding is used as the input of the network, butalso the embedding of the source word, which isaligned to the current target word.
To tackle thelarge search space due to the weak independenceassumption, a lattice algorithm is proposed to re-rank the n-best translation candidates, generatedby a given SMT decoder.Liu et al (2013) propose an additive neural net-work for SMT decoding.
RNNLM (Mikolov et al,2010) is firstly used to generate the source and tar-get word embeddings, which are fed into a one-hidden-layer neural network to get a translationconfidence score.
Together with other common-ly used features, the translation confidence scoreis integrated into a conventional log-linear model.The parameters are optimized with developmen-t data set using mini-batch conjugate sub-gradientmethod and a regularized ranking loss.DNN is also brought into the distortion mod-eling.
Going beyond the previous work usingboundary words for distortion modeling in BTG-based SMT decoder, Li et al (2013) propose to ap-ply recursive auto-encoder to make full use of theentire merged blocks.
The recursive auto-encoderis trained with reordering examples extracted fromword-aligned bilingual sentences.
Given the rep-resentations of the smaller phrase pairs, recursiveauto-encoder can generate the representation ofthe parent phrase pair with a re-ordering confi-dence score.
The combination of reconstructionerror and re-ordering error is used to be the objec-tive function for the model training.14923 Our ModelIn this section, we leverage DNN to model theend-to-end SMT decoding process, using a novelrecursive recurrent neural network (R2NN), whichis different from the above mentioned work ap-plying DNN to components of conventional SMTframework.
R2NN is a combination of recur-sive neural network and recurrent neural network,which not only integrates the conventional glob-al features as input information for each combina-tion, but also generates the representation of theparent node for the future candidate generation.In this section, we briefly recall the recurren-t neural network and recursive neural network inSection 3.1 and 3.2, and then we elaborate ourR2NN in detail in Section 3.3.3.1 Recurrent Neural NetworkRecurrent neural network is usually used forsequence processing, such as language model(Mikolov et al, 2010).
Commonly used sequenceprocessing methods, such as Hidden MarkovModel (HMM) and n-gram language model, onlyuse a limited history for the prediction.
In HMM,the previous state is used as the history, and for n-gram language model (for example n equals to3), the history is the previous two words.
Recur-rent neural network is proposed to use unboundedhistory information, and it has recurrent connec-tions on hidden states, so that history informationcan be used circularly inside the network for arbi-trarily long time.??????1?????
?Figure 1: Recurrent neural networkAs shown in Figure 1, the network containsthree layers, an input layer, a hidden layer, and anoutput layer.
The input layer is a concatenation ofht?1and xt, where ht?1is a real-valued vec-tor, which is the history information from time 0to t?
1. xtis the embedding of the input word attime t .
Word embedding xtis integrated withprevious history ht?1to generate the current hid-den layer, which is a new history vector ht.
Basedon ht, we can predict the probability of the nextword, which forms the output layer yt.
The newhistory htis used for the future prediction, andupdated with new information from word embed-ding xtrecurrently.3.2 Recursive Neural NetworkIn addition to the sequential structure above, treestructure is also usually constructed in variousNLP tasks, such as parsing and SMT decoding.To generate a tree structure, recursive neural net-works are introduced for natural language parsing(Socher et al, 2011).
Similar with recurrent neuralnetworks, recursive neural networks can also useunbounded history information from the sub-treerooted at the current node.
The commonly usedbinary recursive neural networks generate the rep-resentation of the parent node, with the represen-tations of two child nodes as the input.?[?,?]
?[?,?]?[?,?]??[?,?
]Figure 2: Recursive neural networkAs shown in Figure 2, s[l,m]and s[m,n]arethe representations of the child nodes, and they areconcatenated into one vector to be the input of thenetwork.
s[l,n]is the generated representation ofthe parent node.
y[l,n]is the confidence score ofhow plausible the parent node should be created.l,m, n are the indexes of the string.
For example,for nature language parsing, s[l,n]is the represen-tation of the parent node, which could be a NP orV P node, and it is also the representation of thewhole sub-tree covering from l to n .3.3 Recursive Recurrent Neural NetworkWord embedding xtis integrated as new inputinformation in recurrent neural networks for eachprediction, but in recursive neural networks, no ad-ditional input information is used except the tworepresentation vectors of the child nodes.
How-ever, some global information , which cannot begenerated by the child representations, is crucial1493for SMT performance, such as language model s-core and distortion model score.
So as to integratesuch global information, and also keep the abilityto generate tree structure, we combine the recur-rent neural network and the recursive neural net-work to be a recursive recurrent neural network(R2NN).?[?,?]
?[?,?]?[?,?]?[?,?]
?[?,?]??[?,?]??[?,?
]Figure 3: Recursive recurrent neural networkAs shown in Figure 3, based on the recursivenetwork, we add three input vectors x[l,m]forchild node [l,m] , x[m,n]for child node [m,n] ,and x[l,n]for parent node [l, n] .
We call themrecurrent input vectors, since they are borrowedfrom recurrent neural networks.
The two recurrentinput vectors x[l,m]and x[m,n]are concatenat-ed as the input of the network, with the originalchild node representations s[l,m]and s[m,n].
Therecurrent input vector x[l,n]is concatenated withparent node representation s[l,n]to compute theconfidence score y[l,n].The input, hidden and output layers are calcu-lated as follows:x?
[l,n]= x[l,m]./ s[l,m]./ x[m,n]./ s[m,n](1)s[l,n]j= f(?ix?
[l,n]iwji) (2)y[l,n]=?j(s[l,n]./ x[l,n])jvj(3)where ./ is a concatenation operator in Equation1 and Equation 3, and f is a non-linear function,here we use HTanh function, which is definedas:HTanh(x) =?????
?1, x < ?1x, ?1 ?
x ?
11, x > 1(4)Figure 4 illustrates the R2NN architecture forSMT decoding.
For a source sentence ?laizi faguohe eluosi de?, we first split it into phrases ?laiz-i?, ?faguo he eluosi?
and ?de?.
We then checkwhether translation candidates can be found in thetranslation table for each span, together with thephrase pair embedding and recurrent input vec-tor (global features).
We call it the rule match-ing phase.
For a translation candidate of the s-pan node [l,m] , the black dots stand for the noderepresentation s[l,m], while the grey dots for re-current input vector x[l,m].
Given s[l,m]andx[l,m]for matched translation candidates, conven-tional CKY decoding process is performed usingR2NN.
R2NN can combine the translation pairsof child nodes, and generate the translation can-didates for parent nodes with their representationsand plausible scores.
Only the n-best translationcandidates are kept for upper combination, accord-ing to their plausible scores.??laizi?????
?faguo he eluosi?decoming from France and Russia NULLRule Match Rule Match Rule Matchcoming from France and RussiaR2NNcoming from France and RussiaR2NNFigure 4: R2NN for SMT decodingWe extract phrase pairs using the conventionalmethod (Och and Ney, 2004).
The commonly usedfeatures, such as translation score, language mod-el score and distortion score, are used as the recur-rent input vector x .
During decoding, recurrentinput vectors x for internal nodes are calculat-ed accordingly.
The difference between our modeland the conventional log-linear model includes:?
R2NN is not linear, while the conventionalmodel is a linear combination.?
Representations of phrase pairs are automat-ically learnt to optimize the translation per-formance, while features used in convention-al model are hand-crafted.?
History information of the derivation can berecorded in the representation of internal n-odes, while conventional model cannot.1494Liu et al (2013) apply DNN to SMT decoding,but not in a recursive manner.
A feature is learn-t via a one-hidden-layer neural network, and theembedding of words in the phrase pairs are usedas the input vector.
Our model generates the rep-resentation of a translation pair based on its childnodes.
Li et al (2013) also generate the repre-sentation of phrase pairs in a recursive way.
Intheir work, the representation is optimized to learna distortion model using recursive neural network,only based on the representation of the child n-odes.
Our R2NN is used to model the end-to-endtranslation process, with recurrent global informa-tion added.
We also explore phrase pair embed-ding method to model translation confidence di-rectly, which is introduced in Section 5.In the next two sections, we will answer the fol-lowing questions: (a) how to train the model, and(b) how to generate the initial representations oftranslation pairs.4 Model TrainingIn this section, we propose a three-step trainingmethod to train the parameters of our proposedR2NN, which includes unsupervised pre-trainingusing recursive auto-encoding, supervised localtraining on the derivation tree of forced decoding,and supervised global training using early updatetraining strategy.4.1 Unsupervised Pre-trainingWe adopt the Recursive Auto Encoding (RAE)(Socher et al, 2011) for our unsupervised pre-training.
The main idea of auto encoding is toinitialize the parameters of the neural network,by minimizing the information lost, which means,capturing as much information as possible in thehidden states from the input vector.As shown in Figure 5, RAE contains two part-s, an encoder with parameter W , and a decoderwith parameter W?.
Given the representations ofchild nodes s1and s2, the encoder generates therepresentation of parent node s .
With the parentnode representation s as the input vector, the de-coder reconstructs the representation of two childnodes s?1and s?2.
The loss function is defined asfollowing so as to minimize the information lost:LRAE(s1, s2) =12(??s1?
s?1??2+??s2?
s?2?
?2)(5)where ???
is the Euclidean norm.coming from France and Russia??laizi?????
?faguo he eluosicoming from France and Russiacoming from France and Russia?1 ?2??1?
?2???
?Figure 5: Recursive auto encoding for unsuper-vised pre-trainingThe training samples for RAE are phrase pairs{s1, s2} in translation table, where s1ands2can form a continuous partial sentence pair inthe training data.
When RAE training is done, on-ly the encoding model W will be fine tuned inthe future training phases.4.2 Supervised Local TrainingWe use contrastive divergence method to fine tunethe parameters W and V .
The loss functionis the commonly used ranking loss with a margin,and it is defined as follows:LSLT(W,V, s[l,n]) = max(0, 1?
y[l,n]oracle+ y[l,n]t)(6)where s[l,n]is the source span.
y[l,n]oracleisthe plausible score of a oracle translation result.y[l,n]tis the plausible score for the best transla-tion candidate given the model parameters W andV .
The loss function aims to learn a model whichassigns the good translation candidate (the oraclecandidate) higher score than the bad ones, with amargin 1.Translation candidates generated by forced de-coding (Wuebker et al, 2010) are used as ora-cle translations, which are the positive samples.Forced decoding performs sentence pair segmen-tation using the same translation system as decod-ing.
For each sentence pair in the training data,SMT decoder is applied to the source side, andany candidate which is not the partial sub-stringof the target sentence is removed from the n-bestlist during decoding.
From the forced decodingresult, we can get the ideal derivation tree in thedecoder?s search space, and extract positive/oracletranslation candidates.14954.3 Supervised Global TrainingThe supervised local training uses the n-odes/samples in the derivation tree of forced de-coding to update the model, and the trained modeltends to over-fit to local decisions.
In this subsec-tion, a supervised global training is proposed totune the model according to the final translationperformance of the whole source sentence.Actually, we can update the model from the rootof the decoding tree and perform back propaga-tion along the tree structure.
Due to the inexac-t search nature of SMT decoding, search errorsmay inevitably break theoretical properties, andthe final translation results may be not suitablefor model training.
To handle this problem, weuse early update strategy for the supervised glob-al training.
Early update is testified to be usefulfor SMT training with large scale features (Yu etal., 2013).
Instead of updating the model usingthe final translation results, early update approachoptimizes the model, when the oracle translationcandidate is pruned from the n-best list, meaningthat, the model is updated once it performs a unre-coverable mistake.
Back propagation is performedalong the tree structure, and the phrase pair em-beddings of the leaf nodess are updated.The loss function for supervised global trainingis defined as follows:LSGT(W,V, s[l,n]) = ?
log(?y[l,n]oracleexp (y[l,n]oracle)?t?nbestexp (y[l,n]t))(7)where y[l,n]oracleis the model score of a oracle trans-lation candidate for the span [l, n] .
Oracle transla-tion candidates are candidates get from forced de-coding.
If the span [l, n] is not the whole sourcesentence, there may be several oracle translationcandidates, otherwise, there is only one, which isexactly the target sentence.
There are much few-er training samples than those for supervised localtraining, and it is not suitable to use ranking lossfor global training any longer.
We use negativelog-likelihood to penalize all the other translationcandidates except the oracle ones, so as to leverageall the translation candidates as training samples.5 Phrase Pair EmbeddingThe next question is how to initialize the phrasepair embedding in the translation table, so as togenerate the leaf nodes of the derivation tree.There are more phrase pairs than mono-lingualwords, but bilingual corpus is much more difficultto acquire, compared with monolingual corpus.Embedding #Data #Entry #ParameterWord 1G 500K 20 ?
500KWord Pair 7M (500K)220 ?
(500K)2Phrase Pair 7M (500K)420 ?
(500K)4Table 1: The relationship between the size of train-ing data and the number of model parameters.
Thenumbers for word embedding is calculated on En-glish Giga-Word corpus version 3.
For word pairand phrase pair embedding, the numbers are cal-culated on IWSLT 2009 dialog training set.
Theword count of each side of phrase pairs is limitedto be 2.Table 1 shows the relationship between the sizeof training data and the number of model parame-ters.
For word embedding, the training size is 1Gbits, and we may have 500K terms.
For each ter-m, we have a vector with length 20 as parameters,so there are 20 ?
500K parameters totally.
Butfor source-target word pair, we may only have 7Mbilingual corpus for training (taking IWSLT dataset as an example), and there are 20 ?
(500K)2parameters to be tuned.
For phrase pairs, the sit-uation becomes even worse, especially when thelimitation of word count in phrase pairs is relaxed.It is very difficult to learn the phrase pair embed-ding brute-forcedly as word embedding is learnt(Mikolov et al, 2010; Collobert et al, 2011), s-ince we may not have enough training data.A simple approach to construct phrase pair em-bedding is to use the average of the embeddingsof the words in the phrase pair.
One problem isthat, word embedding may not be able to mod-el the translation relationship between source andtarget phrases at phrase level, since some phrasescannot be decomposed.
For example, the meaningof ?hot dog?
is not the composition of the mean-ings of the words ?hot?
and ?dog?.
In this section,we split the phrase pair embedding into two partsto model the translation confidence directly: trans-lation confidence with sparse features and trans-lation confidence with recurrent neural network.We first get two translation confidence vectors sep-arately using sparse features and recurrent neu-ral network, and then concatenate them to be thephrase pair embedding.
We call it translation con-fidence based phrase pair embedding (TCBPPE).14965.1 Translation Confidence with SparseFeaturesLarge scale feature training has drawn more at-tentions these years (Liang et al, 2006; Yu et al,2013).
Instead of integrating the sparse featuresdirectly into the log-linear model, we use them asthe input to learn a phrase pair embedding.
Forthe top 200,000 frequent translation pairs, each ofthem is a feature in itself, and a special feature isadded for all the infrequent ones.The one-hot representation vector is used as theinput, and a one-hidden-layer network generatesa confidence score.
To train the neural network,we add the confidence scores to the convention-al log-linear model as features.
Forced decodingis utilized to get positive samples, and contrastivedivergence is used for model training.
The neu-ral network is used to reduce the space dimensionof sparse features, and the hidden layer of the net-work is used as the phrase pair embedding.
Thelength of the hidden layer is empirically set to 20.5.2 Translation Confidence with RecurrentNeural Network?e???
?1 ??(??)???1??????
?Figure 6: Recurrent neural network for translationconfidenceWe use recurrent neural network to generate twosmoothed translation confidence scores based onsource and target word embeddings.
One is sourceto target translation confidence score and the otheris target to source.
These two confidence scoresare defined as:TS2T(s, t) =?ilog p(ei|ei?1, fai, hi) (8)TT2S(s, t) =?jlog p(fj|fj?1, ea?j, hj) (9)where, faiis the corresponding target wordaligned to ei, and it is similar for ea?j.p(ei|ei?1, fai, hi) is produced by a recurrent net-work as shown in Figure 6.
The recurrent neuralnetwork is trained with word aligned bilingual cor-pus, similar as (Auli et al, 2013).6 Experiments and ResultsIn this section, we conduct experiments to test ourmethod on a Chinese-to-English translation task.The evaluation method is the case insensitive IB-M BLEU-4 (Papineni et al, 2002).
Significanttesting is carried out using bootstrap re-samplingmethod proposed by (Koehn, 2004) with a 95%confidence level.6.1 Data Setting and BaselineThe data is from the IWSLT 2009 dialog task.The training data includes the BTEC and SLDBtraining data.
The training data contains 81k sen-tence pairs, 655K Chinese words and 806K En-glish words.
The language model is a 5-gram lan-guage model trained with the target sentences inthe training data.
The test set is development set9, and the development set comprises both devel-opment set 8 and the Chinese DIALOG set.The training data for monolingual word embed-ding is Giga-Word corpus version 3 for both Chi-nese and English.
Chinese training corpus con-tains 32M sentences and 1.1G words.
Englishtraining data contains 8M sentences and 247Mterms.
We only train the embedding for the top100,000 frequent words following (Collobert etal., 2011).
With the trained monolingual word em-bedding, we follow (Yang et al, 2013) to get thebilingual word embedding using the IWSLT bilin-gual training data.Our baseline decoder is an in-house implemen-tation of Bracketing Transduction Grammar (BT-G) (Wu, 1997) in CKY-style decoding with a lex-ical reordering model trained with maximum en-tropy (Xiong et al, 2006).
The features of thebaseline are commonly used features as standardBTG decoder, such as translation probabilities,lexical weights, language model, word penalty anddistortion probabilities.
All these commonly usedfeatures are used as recurrent input vector x inour R2NN.6.2 Translation ResultsAs we mentioned in Section 5, constructing phrasepair embeddings from word embeddings may benot suitable.
Here we conduct experiments to ver-1497ify it.
We first train the source and target word em-beddings separately using large monolingual data,following (Collobert et al, 2011).
Using monolin-gual word embedding as the initialization, we finetune them to get bilingual word embedding (Yanget al, 2013).The word embedding based phrase pair embed-ding (WEPPE) is defined as:Eppweb(s, t) =?iEwms(si) ./?jEwbs(sj)./?kEwmt(tk) ./?lEwbt(tl) (10)where ./ is a concatenation operator.
s andt are the source and target phrases.
Ewms(si) andEwmt(tk) are the monolingual word embeddings,and Ewbs(si) and Ewbt(tk) are the bilingualword embeddings.
Here the length of the wordembedding is also set to 20.
Therefore, the lengthof the phrase pair embedding is 20?
4 = 80 .We compare our phrase pair embedding meth-ods and our proposed R2NN with baseline system,in Table 2.
We can see that, our R2NN modelswith WEPPE and TCBPPE are both better than thebaseline system.
WEPPE cannot get significan-t improvement, while TCBPPE does, comparedwith the baseline result.
TCBPPE is much betterthan WEPPE.Setting Development TestBaseline 46.81 39.29WEPPE+R2NN 47.23 39.92TCBPPE+R2NN 48.70 ?
40.81 ?Table 2: Translation results of our proposed R2NNModel with two phrase embedding methods, com-pared with the baseline.
Setting ?WEPPE+R2NN?is the result with word embedding based phrasepair embedding and our R2NN Model, and?TCBPPE+R2NN?
is the result of translation con-fidence based phrase pair embedding and ourR2NN Model.
The results with ?
are significantlybetter than the baseline.Word embedding can model translation rela-tionship at word level, but it may not be power-ful to model the phrase pair respondents at phrasallevel, since the meaning of some phrases cannotbe decomposed into the meaning of words.
Andalso, translation task is difference from other NLPtasks, that, it is more important to model the trans-lation confidence directly (the confidence of onetarget phrase as a translation of the source phrase),and our TCBPPE is designed for such purpose.6.3 Effects of Global Recurrent Input VectorIn order to compare R2NN with recursive networkfor SMT decoding, we remove the recurrent inputvector in R2NN to test its effect, and the resultsare shown in Table 3.
Without the recurrent inputvectors, R2NN degenerates into recursive neuralnetwork (RNN).Setting Development TestWEPPE+R2NN 47.23 40.81WEPPE+RNN 37.62 33.29TCBPPE+R2NN 48.70 40.81TCBPPE+RNN 45.11 37.33Table 3: Experimental results to test the effects ofrecurrent input vector.
WEPPE /TCBPPE+RNNare the results removing recurrent input vectorswith WEPPE /TCBPPE.From Table 3 we can find that, the recurren-t input vector is essential to SMT performance.When we remove it from R2NN, WEPPE basedmethod drops about 10 BLEU points on devel-opment data and more than 6 BLEU points ontest data.
TCBPPE based method drops about 3BLEU points on both development and test datasets.
When we remove the recurrent input vectors,the representations of recursive network are gener-ated with the child nodes, and it does not integrateglobal information, such as language model anddistortion model, which are crucial to the perfor-mance of SMT.6.4 Sparse Features and Recurrent NetworkFeaturesTo test the contributions of sparse features and re-current network features, we first remove all therecurrent network features to train and test ourR2NN model, and then remove all the sparse fea-tures to test the contribution of recurrent networkfeatures.Setting Development TestTCBPPE+R2NN 48.70 40.81SF+R2NN 48.23 40.19RNN+R2NN 47.89 40.01Table 4: Experimental results to test the effects ofsparse features and recurrent network features.1498The results are shown in Table 6.4.
From theresults, we can find that, sparse features are moreeffective than the recurrent network features a lit-tle bit.
The sparse features can directly model thetranslation correspondence, and they may be moreeffective to rank the translation candidates, whilerecurrent neural network features are smoothedlexical translation confidence.7 Conclusion and Future WorkIn this paper, we propose a Recursive Recur-rent Neural Network(R2NN) to combine the re-current neural network and recursive neural net-work.
Our proposed R2NN cannot only inte-grate global input information during each com-bination, but also can generate the tree struc-ture in a recursive way.
We apply our model toSMT decoding, and propose a three-step semi-supervised training method.
In addition, we ex-plore phrase pair embedding method, which mod-els translation confidence directly.
We conduc-t experiments on a Chinese-to-English translationtask, and our method outperforms a state-of-the-art baseline about 1.5 points BLEU.From the experiments, we find that, phrase pairembedding is crucial to the performance of SMT.In the future, we will explore better methods forphrase pair embedding to model the translation e-quivalent between source and target phrases.
Wewill apply our proposed R2NN to other tree struc-ture learning tasks, such as natural language pars-ing.ReferencesMichael Auli, Michel Galley, Chris Quirk, and Geof-frey Zweig.
2013.
Joint language and translationmodeling with recurrent neural networks.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1044?1054, Seattle, Washington, USA, October.
Associa-tion for Computational Linguistics.Yoshua Bengio, Holger Schwenk, Jean-S?ebastienSen?ecal, Fr?ederic Morin, and Jean-Luc Gauvain.2006.
Neural probabilistic language models.
Inno-vations in Machine Learning, pages 137?186.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.George E Dahl, Dong Yu, Li Deng, and Alex Acero.2012.
Context-dependent pre-trained deep neuralnetworks for large-vocabulary speech recognition.Audio, Speech, and Language Processing, IEEETransactions on, 20(1):30?42.Geoffrey E Hinton, Simon Osindero, and Yee-WhyeTeh.
2006.
A fast learning algorithm for deep be-lief nets.
Neural computation, 18(7):1527?1554.Koray Kavukcuoglu, Pierre Sermanet, Y-Lan Boureau,Karol Gregor, Micha?el Mathieu, and Yann LeCun.2010.
Learning convolutional feature hierarchies forvisual recognition.
Advances in Neural InformationProcessing Systems, pages 1090?1098.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, pages 388?395.Alex Krizhevsky, Ilya Sutskever, and Geoff Hinton.2012.
Imagenet classification with deep convolu-tional neural networks.
In Advances in Neural Infor-mation Processing Systems 25, pages 1106?1114.Peng Li, Yang Liu, and Maosong Sun.
2013.
Recur-sive autoencoders for ITG-based translation.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 567?577, Seattle, Washington, USA, October.
Associa-tion for Computational Linguistics.Percy Liang, Alexandre Bouchard-C?ot?e, Dan Klein,and Ben Taskar.
2006.
An end-to-end discrimina-tive approach to machine translation.
In Proceed-ings of the 21st International Conference on Com-putational Linguistics and the 44th annual meetingof the Association for Computational Linguistics,pages 761?768.
Association for Computational Lin-guistics.Lemao Liu, Taro Watanabe, Eiichiro Sumita, andTiejun Zhao.
2013.
Additive neural networks for s-tatistical machine translation.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics, pages 791?801, Sofia, Bulgaria,August.
Association for Computational Linguistics.Tomas Mikolov, Martin Karafi?at, Lukas Burget, JanCernock`y, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In Pro-ceedings of the Annual Conference of Internation-al Speech Communication Association, pages 1045?1048.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational linguistics, 30(4):417?449.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic e-valuation of machine translation.
In Proceedings ofthe 40th annual meeting on association for compu-tational linguistics, pages 311?318.
Association forComputational Linguistics.1499Richard Socher, Cliff C Lin, Andrew Y Ng, andChristopher D Manning.
2011.
Parsing naturalscenes and natural language with recursive neuralnetworks.
In Proceedings of the 26th Internation-al Conference on Machine Learning (ICML), vol-ume 2, page 7.Richard Socher, John Bauer, and Christopher D Man-ning.
2013.
Parsing with compositional vectorgrammars.
In Proceedings of the 51st Annual Meet-ing of the Association for Computational Linguistic-s, volume 1, pages 455?465.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational linguistics, 23(3):377?403.Joern Wuebker, Arne Mauser, and Hermann Ney.2010.
Training phrase translation models withleaving-one-out.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, pages 475?484.
Association for Computa-tional Linguistics.Deyi Xiong, Qun Liu, and Shouxun Lin.
2006.
Max-imum entropy based phrase reordering model for s-tatistical machine translation.
In Proceedings of the43rd Annual Meeting of the Association for Compu-tational Linguistics, volume 44, page 521.Nan Yang, Shujie Liu, Mu Li, Ming Zhou, and NenghaiYu.
2013.
Word alignment modeling with contex-t dependent deep neural network.
In 51st AnnualMeeting of the Association for Computational Lin-guistics.Heng Yu, Liang Huang, Haitao Mi, and Kai Zhao.2013.
Max-violation perceptron and forced decod-ing for scalable MT training.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1112?1123, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.1500
