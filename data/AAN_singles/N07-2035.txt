Proceedings of NAACL HLT 2007, Companion Volume, pages 137?140,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsAnalysis and System Combination of Phrase- and N -gram-basedStatistical Machine Translation SystemsMarta R. Costa-jussa`1, Josep M. Crego1, David Vilar2Jose?
A. R. Fonollosa1, Jose?
B. Marin?o1 and Hermann Ney21TALP Research Center (UPC), Barcelona 08034, Spain{mruiz,jmcrego,adrian,canton}@gps.tsc.upc.edu2RWTH Aachen University, Aachen D-52056, Germany{vilar,ney}@i6.informatik.rwth-aachen.deAbstractIn the framework of the Tc-Star project,we analyze and propose a combination oftwo Statistical Machine Translation sys-tems: a phrase-based and an N -gram-basedone.
The exhaustive analysis includes acomparison of the translation models interms of efficiency (number of translationunits used in the search and computationaltime) and an examination of the errors ineach system?s output.
Additionally, wecombine both systems, showing accuracyimprovements.1 IntroductionStatistical machine translation (SMT) has evolvedfrom the initial word-based translation models tomore advanced models that take the context sur-rounding the words into account.
The so-calledphrase-based and N -gram-based models are two ex-amples of these approaches (Zens and Ney, 2004;Marin?o et al, 2006).In current state-of-the-art SMT systems, thephrase-based or the N -gram-based models are usu-ally the main features in a log-linear framework, rem-iniscent of the maximum entropy modeling approach.Two basic issues differentiate the N -gram-basedsystem from the phrase-based one: the training datais sequentially segmented into bilingual units; andthe probability of these units is estimated as a bilin-gual N -gram language model.
In the phrase-basedmodel, no monotonicity restriction is imposed on thesegmentation and the probabilities are normally es-timated simply by relative frequencies.This paper extends the analysis of both systemsperformed in (Crego et al, 2005a) by additionallyperforming a manual error analysis of both systems,which were the ones used by UPC and RWTH in thelast Tc-Star evaluation.Furthermore, we will propose a way to combineboth systems in order to improve the quality of trans-lations.Experiments combining several kinds of MT sys-tems have been presented in (Matusov et al, 2006),based only on the single best output of each system.Recently, a more straightforward approach of bothsystems has been performed in (Costa-jussa` et al,2006) which simply selects, for each sentence, one ofthe provided hypotheses.This paper is organized as follows.
In section 2,we briefly describe the phrase and the N -gram-basedbaseline systems.
In the next section we present theevaluation framework.
In Section 4 we report a struc-tural comparison performed for both systems and, af-terwards, in Section 5, we analyze the errors of bothsystems.
Finally, in the last two sections we rescoreand combine both systems, and the obtained resultsare discussed.2 Baseline Systems2.1 Phrase-based SystemThe basic idea of phrase-based translation is to seg-ment the given source sentence into units (here calledphrases), then translate each phrase and finally com-pose the target sentence from these phrase transla-tions.In order to train these phrase-based models, analignment between the source and target trainingsentences is found by using the standard IBM mod-els in both directions (source-to-target and target-to-source) and combining the two obtained align-ments.
Given this alignment an extraction of con-tiguous phrases is carried out, specifically we extractall phrases that fulfill the following restrictions: allsource (target) words within the phrase are alignedonly to target (source) words within the phrase.The probability of these phrases is normally esti-mated by relative frequencies, normally in both di-rections, which are then combined in a log-linear way.1372.2 N-gram-based SystemIn contrast with standard phrase-based approaches,the N -gram translation model uses tuples as bilin-gual units whose probabilities are estimated as anN -gram language model (Marin?o et al, 2006).
Thismodel approximates the joint probability betweenthe source and target languages by using N -grams.Given a word alignment, tuples define a uniqueand monotonic segmentation of each bilingual sen-tence, building up a much smaller set of unitsthan with phrases and allowing N -gram estimationto account for the history of the translation pro-cess (Marin?o et al, 2006).2.3 Feature functionsBoth baseline systems are combined in a log-linearway with several additional feature functions: a tar-get language model, a forward and a backward lex-icon model and a word bonus are common featuresfor both systems.
The phrase-based system also in-troduces a phrase bonus model.3 Evaluation frameworkThe translation models presented so far were the onesused by UPC and RWTH in the second evaluationcampaign of the Tc-Star project.
The goal of thisproject is to build a speech-to-speech translation sys-tem that can deal with real life data.The corpus consists of the official version of thespeeches held in the European Parliament PlenarySessions (EPPS), as available on the web page of theEuropean Parliament.
Table 1 shows some statistics.The following tools have been used for buildingboth systems: Word alignments were computed us-ing GIZA++ (Och, 2003), language models were es-timated using the SRILM toolkit (Stolcke, 2002), de-coding was carried out by the free available MARIEdecoder (Crego et al, 2005b) and the optimizationwas performed through an in-house implementationof the simplex method (Nelder and Mead, 1965).Spanish EnglishTrain Sentences 1.2MWords 32M 31MVocabulary 159K 111KDev Sentences 1 122 699Words 26K 21KTest Sentences 1 117 894Words 26K 26KTable 1: Statistics of the EPPS Corpora.4 Structural comparisonBoth approaches aim at improving accuracy by in-cluding word context in the model.
However, theimplementation of the models are quite different andmay produce variations in several aspects.Table 2 shows the effect on decoding time intro-duced through different settings of the beam size.Additionally, the number of available translationunits is shown, corresponding to number of avail-able phrases for the phrase-based system and 1gram,2gram and 3gram entries for the N -gram-based sys-tem.
Results are computed on the development set.Task Beam Time(s) Units50 2,677es?en 10 852 537k5 31150 2,689en?es 10 903 594k5 32950 1,264es?en 10 281 104k 288k 145k5 13850 1,508en?es 10 302 118k 355k 178k5 155Table 2: Impact on efficiency of the beam size in PB(top) and NB system (bottom).As it can be seen, the number of translation unitsis similar in both tasks for both systems (537k ?537k for Spanish to English and 594k ?
651k forEnglish to Spanish) while the time consumed in de-coding is clearly higher for the phrase-based system.This can be explained by the fact that in the phrase-based approach, the same translation can be hypoth-esized following several segmentations of the inputsentence, as phrases appear (and are collected) frommultiple segmentations of the training sentence pairs.In other words, the search graph seems to be over-populated under the phrase-based approach.Table 3 shows the effect on translation accuracyregarding the size of the beam in the search.
Resultsare computed on the test set for the phrase-basedand N -gram-based systems.Results of the N -gram-based system show that de-creasing the beam size produces a clear reductionof the accuracy results.
The phrase-based systemshows that accuracy results remain very similar un-der the different settings.
The reason is found onhow translation models are used in the search.
Inthe phrase-based approach, every partial hypothesis138Task Beam BLEU NIST mWER50 51.90 10.53 37.54es?en 10 51.93 10.54 37.495 51.87 10.55 37.4750 47.75 9.94 41.20en?es 10 47.77 9.96 41.095 47.86 10.00 40.7450 51.63 10.46 37.88es?en 10 51.50 10.45 37.835 51.39 10.45 37.8550 47.73 10.08 40.50en?es 10 46.82 9.97 41.045 45.59 9.83 41.04Table 3: Impact on accuracy of the beam size in PB(top) and NB system (bottom).is scored uncontextualized, hence, a single score isused for a given partial hypothesis (phrase).
In theN -gram-based approach, the model is intrinsicallycontextualized, which means that each partial hy-pothesis (tuple) depends on the preceding sequenceof tuples.
Thus, if a bad sequence of tuples (badscored) is composed of a good initial sequence (wellscored), it is placed on top of the first stacks (beam)and may cause the pruning of the rest of hypotheses.5 Error analysisIn order to better asses the quality and the differ-ences between the two systems, a human error anal-ysis was carried out.
The guidelines for this erroranalysis can be found in (Vilar et al, 2006).
Werandomly selected 100 sentences, which were evalu-ated by bilingual judges.This analysis reveals that both systems producethe same kind of errors in general.
However some dif-ferences were identified.
For the English to Spanishdirection the greatest problem is the correct genera-tion of the right tense for verbs, with around 20% ofall translation errors being of this kind.
Reorderingalso poses an important problem for both phrase andN-gram-based systems, with 18% or 15% (respec-tively) of the errors falling into this category.
Miss-ing words is also an important problem.
However,most of them (approximately two thirds for both sys-tems) are filler words (i.e.
words which do not con-vey meaning), that is, the meaning of the sentenceis preserved.
The most remarkable difference whencomparing both systems is that the N -gram basedsystem produces a relatively large amount of extrawords (approximately 10%), while for the phrase-based system, this is only a minor problem (2% ofthe errors).
In contrast the phrase-based system hasmore problems with incorrect translations, that iswords for which a human can find a correspondencein the source text, but the translation is incorrect.Similar conclusions can be drawn for the inverse di-rection.
The verb generating problem is not so acutein this translation direction due to the much simpli-fied morphology of English.
An important problemis the generation of the right preposition.The N -gram based system seems to be able to pro-duce more accurate translations (reflected by a lowerpercentage of translation errors).
However, it gener-ates too many additional (and incorrect words) inthe process.
The phrase-based system, in contrast,counteracts this effect by producing a more directcorrespondence with the words present in the sourcesentence at the cost of sometimes not being able tofind the exact translation.6 System Rescoring andCombinationIntegration of both output translations in the searchprocedure is a complex task.
Translation units ofboth models are quite different and generation his-tories pose severe implementation difficulties.
Wepropose a method for combining the two systems atthe level of N -best lists.Some features that are useful for SMT are too com-plex for including them directly in the search pro-cess.
A clear example are the features that requirethe entire target sentence to be evaluated, as this isnot compatible with the pruning and recombinationprocedures that are necessary for keeping the targetsentence generation process manageable.
A possiblesolution for this problem is to apply sentence levelre-ranking by using N -best lists.6.1 Rescoring CriteriaThe aim of the rescoring procedure is to choose thebest translation candidate out of a given set of Npossible translations.
In our approach this transla-tion candidates are produced independently by bothof the systems and then combined by a simple con-catenation1.
In order for the hypothesis to have acomparable set of scores, we perform an additional?cross-rescoring?
of the lists.Given an N -best list of the phrase-based (N -gram-based) system, we compute the cost of each targetsentence of this N -best list for the N -gram-based(phrase-based) system.
However this computationis not possible in all cases.
Table 4 shows the per-centage of target sentences that the N -gram-based1With removal of duplicates.139(phrase-based) system is able to produce given an N -best list of target sentences computed by the phrase-based (N -gram-based) system.
This percentage iscalculated on the development set.The vocabulary of phrases is bigger than the vo-cabulary of tuples, due to the fact that phrases areextracted from multiple segmentations of the train-ing sentence pairs.
Hence, the number of sentencesreproduced by the N -gram-based system is smallerthan the number of sentences reproduced by thephrase-based system.
Whenever a sentence can notbe reproduced by a given system, the cost of theworst sentence in the N -best list is assigned to it.Task N -best % NB % PBes?en 1000 37.5 57.5en?es 1000 37.2 48.6Table 4: Sentences (%) produced by each system.6.2 ResultsTable 5 shows results of the rescoring and systemcombination experiments on the test set.
The firsttwo rows include results of systems non-rescored andPB (NB) rescored by NB (PB).
The third row corre-sponds to the system combination.
Here, PB (NB)rescored by NB (PB) are simply merged and rankedby rescored score.System N -best BLEU NIST mWERSpanish-to-EnglishPB 1 51.90 10.54 37.50PB 1000 52.55 10.61 37.12NB 1 51.63 10.46 37.88NB 1000 52.25 10.55 37.43PB+NB 2 51.77 10.49 37.68PB+NB 2000 52.31 10.56 37.32English-to-SpanishPB 1 47.75 9.94 41.2PB 1000 48.46 10.13 39.98NB 1 47.73 10.09 40.50NB 1000 48.33 10.15 40.13PB+NB 2 48.26 10.05 40.61PB+NB 2000 48.54 10.16 40.00Table 5: Rescoring and system combination results.7 DiscussionThe structural comparison has shown on the onehand that the N -gram-based system outperformsthe phrase-based in terms of search time efficiencyby avoiding the overpopulation problem presentedin the phrase-based approach.
On the other handthe phrase-based system shows a better performancewhen decoding under a highly constrained search.A detailed error analysis has also been carried outin order to better determine the differences in per-formance of both systems.
The N -gram based sys-tem produced more accurate translations, but also alarger amount of extra (incorrect) words when com-pare to the phrase-based translation system.In section 6 we have presented a system combina-tion method using a rescoring feature for each SMTsystem, i.e.
the N -gram-based feature for the phrase-based system and vice-versa.
For both systems, con-sidering the feature of the opposite system leads toan improvement of BLEU score.ReferencesM.R.
Costa-jussa`, J.M.
Crego, A. de Gispert,P.
Lambert, M. Khalilov J.A.R.
Fonollosa, J.B.Marin?o, and R. Banchs.
2006.
Talp phrase-basedstatistical machine translation and talp systemcombination the iwslt 2006.
IWSLT06.J.
M. Crego, M. R. Costa-jussa`, J. Marin?o, and J. A.Fonollosa.
2005a.
N-gram-based versus phrase-based statistical machine translation.
IWSLT05,October.J.M.
Crego, J. Marin?o, and A. de Gispert.
2005b.An Ngram-based statistical machine translationdecoder.
ICSLP05, April.J.B.
Marin?o, R.E.
Banchs, J.M.
Crego, A. de Gis-pert, P. Lambert, J.A.R.
Fonollosa, and M.R.Costa-jussa`.
2006.
N-gram based machine trans-lation.
Computational Linguistics, 32(4):527?549.E.
Matusov, N. Ueffing, and H. Ney.
2006.
Com-puting consensus translation from multiple ma-chine translation systems using enhanced hypothe-ses alignment.
EACL06, pages 33?40.J.A.
Nelder and R. Mead.
1965.
A simplex methodfor function minimization.
The Computer Journal,7:308?313.F.J.
Och.
2003.
Giza++ software.
http://www-i6.informatik.rwth-aachen.de/?och/ soft-ware/giza++.html.A.
Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
Proc.
of the 7th Int.
Conf.
onSpoken Language Processing, ICSLP?02, Septem-ber.David Vilar, Jia Xu, Luis Fernando D?Haro, andHermann Ney.
2006.
Error Analysis of MachineTranslation Output.
In LREC06, pages 697?702,Genoa, Italy, May.Richard Zens and Hermann Ney.
2004.
Improve-ments in phrase-based statistical machine transla-tion.
In HLT04, pages 257?264, Boston, MA, May.140
