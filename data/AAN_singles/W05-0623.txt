Proceedings of the 9th Conference on Computational Natural Language Learning (CoNLL),pages 173?176, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsA Joint Model for Semantic Role LabelingAria HaghighiDept of Computer ScienceStanford UniversityStanford, CA, 94305aria42@stanford.eduKristina ToutanovaDept of Computer ScienceStanford UniversityStanford, CA, 94305kristina@cs.stanford.eduChristopher D. ManningDept of Computer ScienceStanford UniversityStanford, CA, 94305manning@cs.stanford.eduAbstractWe present a semantic role labeling sys-tem submitted to the closed track of theCoNLL-2005 shared task.
The system, in-troduced in (Toutanova et al, 2005), im-plements a joint model that captures de-pendencies among arguments of a predi-cate using log-linear models in a discrimi-native re-ranking framework.
We also de-scribe experiments aimed at increasing therobustness of the system in the presenceof syntactic parse errors.
Our final systemachieves F1-Measures of 76.68 and 78.45on the development and the WSJ portionof the test set, respectively.1 IntroductionIt is evident that there are strong statistical patternsin the syntactic realization and ordering of the argu-ments of verbs; for instance, if an active predicatehas an A0 argument it is very likely to come beforean A1 argument.
Our model aims to capture such de-pendencies among the labels of nodes in a syntacticparse tree.However, building such a model is computation-ally expensive.
Since the space of possible joint la-belings is exponential in the number of parse treenodes, a model cannot exhaustively consider theselabelings unless it makes strong independence as-sumptions.
To overcome this problem, we adopta discriminative re-ranking approach reminiscent of(Collins, 2000).
We use a local model, which la-bels arguments independently, to generate a smallernumber of likely joint labelings.
These candidate la-belings are in turn input to a joint model which canuse global features and re-score the candidates.
Boththe local and global re-ranking models are log-linear(maximum entropy) models.In the following sections, we briefly describe ourlocal and joint models and the system architecturefor combining them.
We list the features used by ourmodels, with an emphasis on new features, and com-pare the performance of a local and a joint model onthe CoNLL shared task.
We also study an approachto increasing the robustness of the semantic role la-beling system to syntactic parser errors, by consid-ering multiple parse trees generated by a statisticalparser.2 Local ModelsOur local model labels nodes in a parse tree inde-pendently.
We decompose the probability over la-bels (all argument labels plus NONE), into a productof the probability over ARG and NONE, and a prob-ability over argument labels given that a node is anARG.
This can be seen as chaining an identificationand a classification model.
The identification modelclassifies each phrase as either an argument or non-argument and our classification model labels eachpotential argument with a specific argument label.The two models use the same features.Previous research (Gildea and Jurafsky, 2002;Pradhan et al, 2004; Carreras and Ma`rquez, 2004)has identified many useful features for local iden-tification and classification.
Below we list the fea-tures and hand-picked conjunctions of features usedin our local models.
The ones denoted with asterisks(*) were not present in (Toutanova et al, 2005).
Al-though most of these features have been described inprevious work, some features, described in the nextsection, are ?
to our knowledge ?
novel.173?
Phrase-Type Syntactic category of node?
Predicate Lemma Stemmed target verb?
Path Sequence of phrase types between the predicate andnode, with ?, ?
to indicate direction?
Position Before or after predicate?
Voice Voice of predicate?
Head-Word of Phrase?
Head-POS POS tag of head word?
Sub-Cat CFG expansion of predicate?s parent?
First/Last Word?
Left/Right Sister Phrase-Type?
Left/Right Sister Head-Word/Head-POS?
Parent Phrase-Type?
Parent POS/Head-Word?
Ordinal Tree Distance Phrase-type concatenated with thelength of the Path feature?
Node-LCA Partial Path Path from the node to the lowestcommon ancestor of the predicate and the node?
PP Parent Head-Word If the parent of the node is a PP, theparent?s head-word?
PP NP Head-Word/Head-POS For a PP, retrieve the head-word /head-POS of its rightmost NP?
Temporal Keywords* Is the head of the node a temporalword e.g ?February?
or ?afternoon??
Missing subject* Is the predicate missing a subject inthe?standard?
location?
Projected path* Path from the maximal extended projectionof the predicate to the node?
Predicate Lemma & Path?
Predicate Lemma & Head-Word?
Predicate Lemma & Phrase-Type?
Voice & Position?
Predicate Lemma & PP Parent Head-Word?
Path & Missing subject*?
Projected path & Missing subject*2.1 Additional Local FeaturesWe found that a large source of errors for A0 and A1stemmed from cases such as those illustrated in Fig-ure 1, where arguments were dislocated by raisingor controlling verbs.
Here, the predicate, expected,does not have a subject in the typical position ?
in-dicated by the empty NP ?
since the auxiliary is hasraised the subject to its current position.
In order tocapture this class of examples, we use a binary fea-ture, Missing Subject, indicating whether the pred-icate is ?missing?
its subject, and use this feature inconjunction with the Path feature, so that we learntypical paths to raised subjects conditioned on theabsence of the subject in its typical position.In the particular case of Figure 1, there is an-other instance of an argument being quite far fromSPPPPNPi-A1aaa!!
!the trade gapVPPPPPis SPPPPNPi-A1-NONE-VPHHHexpected VPQQto widenFigure 1: Example of displaced argumentsits predicate.
The predicate widen shares the tradegap with expect as a A1 argument.
However, as ex-pect is a raising verb, widen?s subject is not in itstypical position either, and we should expect to findit in the same positions as expected?s subject.
Thisindicates it may be useful to use the path relative toexpected to find arguments for widen.
In general,to identify certain arguments of predicates embed-ded in auxiliary and infinitival VPs we expect it tobe helpful to take the path from the maximum ex-tended projection of the predicate ?
the highest VPin the chain of VP?s dominating the predicate.
Weintroduce a new path feature, Projected Path, whichtakes the path from the maximal extended projec-tion to an argument node.
This feature applies onlywhen the argument is not dominated by the maxi-mal projection, (e.g., direct objects).
These featuresalso handle other cases of discontinuous and non-local dependencies, such as those arising due to con-troller verbs.
For a local model, these new featuresand their conjunctions improved F1-Measure from73.80 to 74.52 on the development set.
Notably, theF1-Measure of A0 increased from 81.02 to 83.08.3 Joint ModelOur joint model, in contrast to the local model, col-lectively scores a labeling of all nodes in the parsetree.
The model is trained to re-rank a set of N likelylabelings according to the local model.
We find theexact top N consistent1 most likely local model la-belings using a simple dynamic program describedin (Toutanova et al, 2005).1A labeling is consistent if satisfies the constraint that argu-ment phrases do not overlap.174SNP1-A1Crude oil pricesVPVBD-VfellPP1-A3TOtoNP$27.80PP2-A4FROMfromNP$37.80NP2-AM-TMPyesterdayFigure 2: An example tree with semantic role annotations.Most of the features we use are described in moredetail in (Toutanova et al, 2005).
Here we brieflydescribe these features and introduce several newjoint features (denoted by *).
A labeling L of allnodes in the parse tree specifies a candidate argu-ment frame ?
the sequence of all nodes labeled witha non-NONE label according to L. The joint modelfeatures operate on candidate argument frames, andlook at the labels and internal features of the candi-date arguments.
We introduce them in the contextof the example in Figure 2.
The candidate argumentframe corresponding to the correct labeling for thetree is: [NP1-A1,VBD-V,PP1-A3,PP2-A4,NP2-AM-TMP].?
Core arguments label sequence: The sequenceof labels of core arguments concatenated withthe predicate voice.
Example: [voice:active:A1,V,A3,A4] A back-off feature which substitutesspecific argument labels with a generic argument(A) label is also included.?
Flattened core arguments label sequence*:Same as the previous but merging consecutiveequal labels.?
Core arguments label and annotated phrasetype sequence: The sequence of labels of corearguments together with annotated phrase types.Phrase types are annotated with the head word forPP nodes, and with the head POS tag for S and VPnodes.
Example: [voice:active: NP-A1,V,PP-to-A3,PP-from-A4].
A back-off to generic A labelsis also included.
Also a variant that adds the pred-icate stem.?
Repeated core argument labels with phrasetypes: Annotated phrase types for nodes withthe same core argument label.
This feature cap-tures, for example, the tendency of WHNP refer-ring phrases to occur as the second phrase havingthe same label as a preceding NP phrase.?
Repeated core argument labels with phrasetypes and sister/adjacency information*: Sim-ilar to the previous feature, but also indicateswhether all repeated arguments are sisters in theparse tree, or whether all repeated arguments areadjacent in terms of word spans.
These featurescan provide robustness to parser errors, making itmore likely to label adjacent phrases incorrectlysplit by the parser with the same label.4 Combining Local and Joint ModelsIt is useful to combine the joint model score witha local model score, because the local model hasbeen trained using all negative examples, whereasthe joint model has been trained only on likelyargument frames .
Our final score is given bya mixture of the local and joint model?s log-probabilities: scoreSRL(L|t) = ?
score`(L|t) +scoreJ(L|t), where score`(L|t) is the local score ofL, scoreJ(L|t) is the corresponding joint score, and?
is a tunable parameter.
We search among the topN candidate labelings proposed by the local model,for the labeling that maximizes the final score.5 Increasing Robustness to Parser ErrorsIt is apparent that role labeling is very sensitive to thecorrectness of the given parse tree.
If an argumentdoes not correspond to a constituent in a parse tree,our model will not be able to consider the correctphrase.One way to address this problem is to utilize alter-native parses.
Recent releases of the Charniak parser(Charniak, 2000) have included an option to providethe top k parses of a given sentence according tothe probability model of the parser.
We use thesealternative parses as follow: Suppose t1, .
.
.
, tk aretrees for sentence s with given probabilities P (ti|s)by the parser.
Then for a fixed predicate v, let Li175Precision Recall F?=1Development 77.66% 75.72% 76.68Test WSJ 79.54% 77.39% 78.45Test Brown 70.24% 65.37% 67.71Test WSJ+Brown 78.34% 75.78% 77.04Test WSJ Precision Recall F?=1Overall 79.54% 77.39% 78.45A0 88.32% 88.30% 88.31A1 78.61% 78.40% 78.51A2 72.55% 68.11% 70.26A3 73.08% 54.91% 62.71A4 77.42% 70.59% 73.85A5 100.00% 80.00% 88.89AM-ADV 58.20% 51.19% 54.47AM-CAU 63.93% 53.42% 58.21AM-DIR 52.56% 48.24% 50.31AM-DIS 76.56% 80.62% 78.54AM-EXT 73.68% 43.75% 54.90AM-LOC 61.52% 55.92% 58.59AM-MNR 58.33% 56.98% 57.65AM-MOD 97.85% 99.09% 98.47AM-NEG 97.41% 98.26% 97.84AM-PNC 49.50% 43.48% 46.30AM-PRD 100.00% 20.00% 33.33AM-REC 0.00% 0.00% 0.00AM-TMP 74.85% 67.34% 70.90R-A0 92.63% 89.73% 91.16R-A1 81.53% 82.05% 81.79R-A2 61.54% 50.00% 55.17R-A3 0.00% 0.00% 0.00R-A4 0.00% 0.00% 0.00R-AM-ADV 0.00% 0.00% 0.00R-AM-CAU 100.00% 50.00% 66.67R-AM-EXT 0.00% 0.00% 0.00R-AM-LOC 85.71% 57.14% 68.57R-AM-MNR 28.57% 33.33% 30.77R-AM-TMP 61.54% 76.92% 68.38V 97.32% 97.32% 97.32Table 1: Overall results (top) and detailed resultson the WSJ test (bottom) on the closed track of theCoNLL shared task.denote the best joint labeling of tree ti, with scorescoreSRL(Li|ti) according to our final joint model.Then we choose the labeling L which maximizes:arg maxi?{1,...,k}?
log P (ti|S) + scoreSRL(Li|ti) (1)Considering top k = 5 parse trees using this al-gorithm resulted in up to 0.4 absolute increase inF-Measure.
In future work, we plan to experimentwith better ways to combine information from mul-tiple parse trees.6 Experiments and ResultsFor our final results we used a joint model with ?
=1.5 (local model weight), ?
= 1 (parse tree log-probability weight) , N = 15 (candidate labelingsfrom the local model to consider) , and k = 5 (num-ber of alternative parses).
The whole training set forthe CoNLL-2005 task was used to train the mod-els.
It takes about 2 hours to train a local identifi-cation model, 40 minutes to train a local classifica-tion model, and 7 hours to train a joint re-rankingmodel.2In Table 1, we present our final development andtest results using this model.
The percentage ofperfectly labeled propositions for the three sets is55.11% (development), 56.52% (test), and 37.06%(Brown test).
The improvement achieved by thejoint model relative to the local model is about 2points absolute in F-Measure, similar to the im-provement when gold-standard syntactic parses areused (Toutanova et al, 2005).
The relative error re-duction is much lower for automatic parses, possi-bly due to a lower upper bound on performance.
Itis clear from the drop in performance from the WSJto Brown test set that our learned model?s featuresdo not generalize very well to related domains.ReferencesXavier Carreras and Llu?
?s Ma`rquez.
2004.
Introductionto the CoNLL-2004 shared task: Semantic role label-ing.
In Proceedings of CoNLL-2004.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of NAACL, pages 132?139.Michael Collins.
2000.
Discriminative reranking for nat-ural language parsing.
In Proceedings of ICML-2000.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesMartin, and Dan Jurafsky.
2004.
Shallow semanticparsing using support vector machines.
In Proceed-ings of HLT/NAACL-2004.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2005.
Joint learning improves semantic rolelabeling.
In Proceedings of ACL-2005.2On a 3.6GHz machine with 4GB of RAM.176
