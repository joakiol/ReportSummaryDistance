Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 407?413,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsComposition of Word RepresentationsImproves Semantic Role LabellingMichael Roth and Kristian WoodsendInstitute for Language, Cognition and ComputationSchool of Informatics, University of Edinburgh{mroth,kwoodsen}@inf.ed.ac.ukAbstractState-of-the-art semantic role labellingsystems require large annotated corpora toachieve full performance.
Unfortunately,such corpora are expensive to produce andoften do not generalize well across do-mains.
Even in domain, errors are oftenmade where syntactic information doesnot provide sufficient cues.
In this pa-per, we mitigate both of these problemsby employing distributional word repre-sentations gathered from unlabelled data.While straight-forward word representa-tions of predicates and arguments improveperformance, we show that further gainsare achieved by composing representa-tions that model the interaction betweenpredicate and argument, and capture fullargument spans.1 IntroductionThe goal of semantic role labelling (SRL) is todiscover the relations that hold between a pred-icate and its arguments in a given input sen-tence (e.g., ?who?
did ?what?
to ?whom?, ?when?,?where?, and ?how?).
This semantic knowl-edge at the predicate-argument level is requiredby inference-based NLP tasks in order to iden-tify meaning-preserving transformations, such asactive/passive, verb alternations and nominaliza-tions.
Several manually-build semantic resources,including FrameNet (Ruppenhofer et al., 2010)and PropBank (Palmer et al., 2005), have beendeveloped with the goal of documenting and pro-viding examples of such transformations and howthey preserve semantic role information.
Giventhat labelled corpora are inevitably restricted insize and coverage, and that syntactic cues are notby themselves unambiguous or sufficient, the suc-cess of systems that automatically provide corre-sponding analyses has been limited in practice.Recent work on SRL has explored approachesthat can leverage unlabelled data, following asemi-supervised (F?urstenau and Lapata, 2012;Titov and Klementiev, 2012) or unsupervisedlearning paradigm (Abend et al., 2009; Titov andKlementiev, 2011).
Unlabelled data provides ad-ditional statistical strength and can lead to moreconsistent models.
For instance, latent representa-tions of words can be computed, based on distri-butional similarity or language modelling, whichcan be used as additional features during tradi-tional supervised learning.
Although we wouldexpect that extra features would improve classifierperformance, this seems in part counter-intuitive.Just because one word has a specific representa-tion does not mean that it should be assigned aspecific argument label.
Instead, one would ex-pect a more complex interplay between predicate,argument and the context they appear in.In this paper, we investigate the impact of dis-tributional word representations for SRL.
Initially,we augment the feature space with word repre-sentations for a predicate and its argument head.Furthermore, we use a compositional approach tomodel a representation of the full argument, bycomposing a joint representation of all words inthe argument span, and we also investigate the in-teraction between predicate and argument, usinga compositional representation of the dependencypath.
We demonstrate the benefits of these com-positional features using a state-of-the-art seman-tic role labeller, which we evaluate on the Englishpart of the CoNLL-2009 data set.2 Related WorkResearch into using distributional informationin SRL dates back to Gildea and Jurafsky(2002), who used distributions over verb-objectco-occurrence clusters to improve coverage in ar-gument classification.
The distribution of a wordover these soft clusters assignments was added as407features to their classifier.
The SRL system byCroce et al.
(2010) combines argument clusteringbased on co-occurrence frequencies with a lan-guage model.
Collobert et al.
(2011) used dis-tributional word representations in a neural net-work model that can update representations dur-ing training.
Zapirain et al.
(2013) suggested dis-tributional information as a basis for a selectionalpreference model that can be used as a single addi-tional feature for classifying potential arguments.Most recently, Hermann et al.
(2014) used distri-butional word representations within pre-definedsyntactic contexts as input to a classifier whichlearns to distinguish different predicate senses.A complementary line of research explores therepresentation of sequence information.
Promi-nent examples are the works by Deschacht andMoens (2009) and Huang and Yates (2010) wholearned and applied Hidden Markov Models toassign state variables to words and word spans,which serve as supplementary features for classifi-cation.
One drawback of this approach is that statevariables are discrete and the number of states(i.e., their granularity) has to be chosen in advance.The popularity of distributional methods forword representation has been a motivation for de-veloping representations of larger constructionssuch as phrases and sentences, and there havebeen several proposals for computing the meaningof word combinations in vector spaces.
Mitchelland Lapata (2010) introduced a general frame-work where composition is formulated as a func-tion f of two vectors u and v. Depending onhow f is chosen, different composition modelsarise, the simplest being an additive model wheref(u, v) = u + v. To capture relational functions,Baroni and Zamparelli (2010) expanded on thisapproach by representing verbs, adjectives and ad-verbs by matrices which can modify the propertiesof nouns (represented by vectors).
Socher et al.
(2012) combined word representations with syn-tactic structure information, through a recursiveneural network that learns vector space represen-tations for multi-word phrases and sentences.
Anempirical comparison of these composition meth-ods was provided in (Blacoe and Lapata, 2012).In this work, we use type-based continuous rep-resentations of words to compose representationsof multiple word sequences and spans, which canthen be incorporated directly as features into SRLsystems.Distributional Feature ComputationArgument a ~aPredicate p ~pPredicate-argument Interaction ~a + ~pArgument Span w1.
.
.
wn?i~wiDependency Path from a to p ?w?path(a,p)~wTable 1: Features based on distributional wordrepresentations and additive composition.
Vector~w denotes the representation of word w.3 MethodFollowing the set-up of the CoNLL shared taskin 2009, we consider predicate-argument struc-tures that consist of a verbal or nominal pred-icate p and PropBank-labelled arguments ai?{a1.
.
.
an}, where each aicorresponds to the headword of the phrase that constitutes the respectiveargument.
Traditional semantic role labelling ap-proaches compute a set of applicable features oneach pair ?p, ai?, such as the observed lemma typeof a word and the grammatical relation to its head,that serve as indicators for a particular role label.The disadvantage of this approach lies in thefact that indicator features such as word andlemma type are often sparse in training data andhence do not generalize well across domains.
Incontrast, features based on distributional represen-tations (e.g., raw co-occurrence frequencies) canbe computed for every word, given that it occursin some unlabelled corpus.
In addition to this ob-vious advantage for out-of-domain settings, dis-tributional representations can provide a more ro-bust input signal to the classifier, for instance byprojecting a matrix of co-occurrence frequenciesto a lower-dimensional space.
We hence hypoth-esize that such features enable the model to be-come more robust out-of-domain, while providinghigher precision in-domain.Although simply including the components ofa word representation as features to a classifiercan lead to immediate improvements in SRL per-formance, this observation seems in part counter-intuitive.
Just because one word has a specificrepresentation does not mean that it should be as-signed a specific argument label.
In fact, onewould expect a more complex interplay betweenthe representation of an argument aiand the con-text it appears in.
To model aspects of this inter-play, we define an extended set of features that408further includes representations for the combina-tion of p and ai, the set of words in the depen-dency path between p and ai, and the set of wordsin the full span of ai.
We compute additive com-positional representations of multiple words, us-ing the simplest method of Mitchell and Lapata(2010) where the composed representation is theuniformly weighted sum of each single represen-tation.
Our full set of feature types based on distri-butional word representations is listed in Table 1.4 Experimental SetupWe evaluate the impact of different types of fea-tures by performing experiments on a benchmarkdataset for semantic role labelling.
To assess thegains of distributional representations realistically,we incorporate the features described in Section 3into a state-of-the-art SRL system.
The follow-ing paragraphs summarize the details of our ex-perimental setup.Semantic Role Labeller.
In all our experi-ments, we use the publicly available system byBj?orkelund et al.
(2010).1This system com-bines the first-ranked SRL system and the first-ranked syntactic parser in the CoNLL 2009 sharedtask for English (Bj?orkelund et al., 2009; Bohnet,2010).
To the best of our knowledge, thiscombination represents the current state-of-the-artfor semantic role labelling following the Prop-Bank/NomBank paradigm (Palmer et al., 2005;Meyers et al., 2004).
To re-train and evaluate mod-els with different feature sets, we use the sametraining, development and test sets as providedin the CoNLL shared task (Haji?c et al., 2009).Although the employed system features a fullsyntactic-semantic parsing pipeline, we only mod-ify the feature sets of the two components directlyrelated to the actual role labelling task, namely ar-gument identification and argument classification.Word Representations.
As a baseline, we sim-ply added as features the word representations ofthe predicate and argument head involved in aclassification decision (first two lines in Table 1).We experimented with a range of publicly avail-able sets of word representations, including em-beddings from various neural language models1http://code.google.com/p/mate-tools/2http://metaoptimize.com/projects/wordreprs/3http://ai.stanford.edu/%7eehhuang/4http://lebret.ch/words/5http://www.cis.upenn.edu/%7eungar/eigenwords/Development dims P R F1None ?
86.1 81.0 83.5Brown clusters2320 86.2 81.3 83.7Neural LM250 86.2 81.4 83.7Neural LM+Global350 86.2 81.4 83.7HLBL250 86.3 81.3 83.7H-PCA450 86.2 81.3 83.7Eigenwords550 86.2 81.3 83.6Table 2: Results on the CoNLL-2009 develop-ment set, using off-the-shelf word representationsfor predicates and argument as additional features.Performance numbers in percent.
(Mnih and Hinton, 2009; Collobert et al., 2011;Huang et al., 2012), eigenvectors (Dhillon et al.,2011), Brown clusters (Brown et al., 1992), andpost-processed co-occurrence counts (Lebret andCollobert, 2014).
Results on the development setfor various off-the-shelf representations are shownin Table 2.
The numbers reveal that any kind ofword representation can be employed to improveresults.
We choose to perform all follow-up exper-iments using the 50-dimensional embeddings in-duced by Turian et al.
(2010), using the method byCollobert et al., as they led to slightly better resultsin F1-score than other representations.
No signif-icant differences were observed, however, usingother types of representations or vector sizes.5 ResultsWe evaluate our proposed set of additional fea-tures on the CoNLL-2009 in-domain and out-of-domain test sets, using the aforementioned SRLsystem and word representations.
All results arecomputed using the system?s built-in preprocess-ing pipeline and re-trained models for argumentidentification and classification.
We report la-belled precision, recall and semantic F1-score ascomputed by the official scorer.The upper part of Table 3 shows SRL perfor-mance on the in-domain CoNLL-2009 test set,with and without (Original) additional featuresbased on distributional representations.
The re-sults reveal that any type of additional featurehelps to improve precision and recall in this setting(from 85.2% F1-score up to 85.5%), with signifi-cant gains for 4 of the 5 additional features (com-puted using a randomization test; cf.
Yeh, 2000).Interestingly, we find that the features do not seem409In-domain P R F1Original 87.4 83.1 85.2Original + Argument 87.6 83.3 85.4**Original + Predicate 87.4 83.2 85.2Original + Interaction 87.5 83.3 85.3**Original + Span 87.6 83.5 85.5**Original + Path 87.5 83.4 85.4**Original + All 87.6 83.4 85.5**Out-of-domain P R F1Original 76.9 71.7 74.2Original + Argument 77.4 71.9 74.5Original + Predicate 77.3 72.2 74.7*Original + Interaction 77.2 72.0 74.5Original + Span 77.3 72.3 74.7*Original + Path 77.2 72.3 74.7*Original + All 77.5 73.0 75.2**Table 3: Results on both CoNLL-2009 test sets.All numbers in percent.
Significant differencesfrom Original in terms of F1-score are marked byasterisks (* p<0.05, ** p<0.01).to have a cumulative effect here, as indicated bythe results with all features (+All, 85.5% F1).
Weconjecture that this is due to the high volume ofexisting in-domain training data, which rendersour full feature set redundant.
To test this conjec-ture, we further assess performance on the out-of-domain test set of the CoNLL-2009 shared task.The results for the out-of-domain experimentare summarized in the lower part of Table 3.We again observe that each single feature typeimproves classification, with absolute gains be-ing slightly higher than in the in-domain setting.More interestingly though, we find that the com-plete feature set boosts performance even further,achieving an overall gain in precision and recallof 0.6 and 1.3 percentage points, respectively.
Theresulting F1-score of 75.2 lies even higher than thetop score for this particular data set reported in theCoNLL shared task (Zhao et al., 2009; 74.6 F1).We next investigate the benefits of compo-sitional representations over features for singlewords by assessing their impact on the overall re-sult in an ablation study.
Table 4 shows resultsof ablation tests performed for the three composi-tional feature types Interaction, Span and Pathon the out-of-domain test set.
The results revealOut-of-domain P R F1Original 76.9 71.7 74.2Full (Original+All) 77.5 73.0 75.2Full ?Interaction 77.2 72.5 74.8Full ?Span 77.2 72.3 74.7Full ?Path 77.6 72.3 74.8Table 4: Results of an ablation study over featuresbased on compositional representations.
All num-bers in percent.a considerable loss in recall, indicating the impor-tance of including compositional word represen-tations and confirming our intuition that they canprovide additional gains over simple type-levelrepresentations.
In the next section, we discussthis result in more detail and provide examples ofimproved classification decisions.6 DiscussionAs a more detailed qualitative analysis, we exam-ined the impact of word representations on SRLperformance with respect to different argument la-bels and predicate types.
Results on the in-domaindata set, shown in the upper part of Table 5, sug-gest that most improvements in terms of preci-sion are gained for verbal predicates, while nom-inal predicates primarily benefit from higher re-call.
One reason for the latter observation mightbe that arguments of nominal predicates are gen-erally much harder to identify for the Originalmodel, as the cues provided by indicator featureson words and syntax are often inconclusive.
Forverbal predicates, the word representations mainlyprovide reinforcing signals to the classifier, im-proving its precision at a slight cost of recall.The results on the out-of-domain data set pro-vide more insights regarding the suitability ofword representations for generalization.
As shownin the lower half of Table 5, the additional featureson average have a positive impact on precision andrecall.
For verbal predicates, we observe only onecase, namely A0, in which improvements in recallcame with a decline in precision.
Regarding nomi-nal predicates, the trend is similar to what we haveseen in the in-domain setting, with most gains be-ing achieved in terms of recall.Apart from assessing quantitative effects, wefurther examined cases that directly show the qual-itative gains of the compositional features defined410Sentence with predicate and [gold argumentlabel] Original Features required for correction(1) He did not resent [theirA0] supervision A1 Interaction(2) [HeA1] is getting plenty of rest no label Interaction, Path(3) [HeA0] rose late and went down to have breakfast.
no label Path(4) He was able to sit [for hoursAM-TMP].
A2 Span(5) Because he had spoken [too softlyAM-MNR].
AM-TMP SpanTable 6: Example sentences in which distributional features compensated for errors made by Original.In-domain verbal nominalLabel P R P RA0 +0.4 +0.4 ?0.1 +2.4A1 +0.2 ?0.4 +0.6 +1.5A2 +1.7 ?1.5 ?
+2.5AM-ADV +0.8 +0.2 ?9.9 ?3.1AM-DIS +0.3 ?3.2 ?
?AM-LOC +0.8 +1.1 +0.6 +3.0AM-MNR ?0.5 ?1.2 +2.7 +0.3AM-TMP ?1.2 ?0.7 ?1.9 +3.3Out-of-domain verbal nominalLabel P R P RA0 ?0.9 +2.5 ?2.5 ?0.4A1 +1.7 +0.8 +1.0 +3.7A2 +1.4 +0.7 ?2.5 +3.2AM-ADV +5.6 +0.7 ?
?AM-DIS +7.3 ?
?
?AM-LOC +0.7 +2.4 ?
+15.0AM-MNR +6.4 +10.5 +9.7 +10.7AM-TMP +1.6 +1.8 ?6.7 +1.1Table 5: Differences in precision and recall perargument label and predicate word category.
Allnumbers represent absolute percentage points.in Section 3.
Table 6 lists examples from theout-of-domain data set that were misclassified bythe Original model but could be correctly pre-dicted using our enhanced feature set.
As illus-trated by Examples (1) and (2), the Interactionfeature seems to help recall by guiding classifica-tion decisions towards more meaningful and com-plete structures.Improvements using the Path feature can be ob-served in cases where nested syntactic structuresneed to be processed, as required in Example (2).In another instance, Example (3), the followingpath is predicted between argument and predicate:HeSBJ??
roseCOORD????andCONJ???wentOPRD???
toIM?
?have.Such cases are particularly problematic for theOriginal model because long and potentially er-roneous paths are sparse in the training data.Further gains in performance are achieved usingthe Span feature, which enables the model to bet-ter handle infrequent and out-of-vocabulary wordsoccurring in an argument span, including ?hours?and ?softly?
in Example (4) and (5), respectively.7 ConclusionsIn this paper, we proposed to enhance the featurespace of a state-of-the-art semantic role labeller byapplying and composing distributional word rep-resentations.
Our results indicate that combiningsuch features with standard syntactic cues leads tomore precise and more robust models, with sig-nificant improvements both in-domain and out-of-domain.
Ablation tests on an out-of-domain dataset have shown that gains in recall are mostly dueto features based on composed representations.Given the novelty of these features for SRL, webelieve that this insight is remarkable and deservesfurther investigation.
In future work, we plan toapply more sophisticated models of composition-ality to better represent predicate-argument struc-tures and to guide classification decisions towardsoutcomes that are semantically more plausible.We anticipate that this line of research will also beof interest for a range of related tasks beyond tra-ditional SRL, including predicate-argument struc-ture alignment (Roth and Frank, 2012) and im-plicit argument linking (Gerber and Chai, 2012).AcknowledgementsThis work has been supported by the FP7 Col-laborative Project S-CASE (Grant Agreement No610717) funded by the European Commission(Michael Roth), and by EPSRC (EP/K017845/1)in the framework of the CHIST-ERA READERSproject (Kristian Woodsend).411ReferencesOmri Abend, Roi Reichart, and Ari Rappoport.
2009.Unsupervised argument identification for semanticrole labeling.
In Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP, pages 28?36, Sun-tec, Singapore, August.Marco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, pages1183?1193, Cambridge, MA, October.
Associationfor Computational Linguistics.Anders Bj?orkelund, Love Hafdell, and Pierre Nugues.2009.
Multilingual semantic role labeling.
In Pro-ceedings of the Thirteenth Conference on Compu-tational Natural Language Learning: Shared Task,pages 43?48.
Association for Computational Lin-guistics.Anders Bj?orkelund, Bernd Bohnet, Love Hafdell, andPierre Nugues.
2010.
A high-performance syn-tactic and semantic dependency parser.
In Coling2010: Demonstration Volume, pages 33?36, Beijing,China, August.
Coling 2010 Organizing Committee.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for seman-tic composition.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning, pages 546?556, Jeju Island, Korea,July.
Association for Computational Linguistics.Bernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics (Coling 2010), pages 89?97, Bei-jing, China, August.Peter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-based n-gram models of natural language.Computational linguistics, 18(4):467?479.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Danilo Croce, Cristina Giannone, Paolo Annesi, andRoberto Basili.
2010.
Towards open-domain se-mantic role labeling.
In Proceedings of the 48th An-nual Meeting of the Association for ComputationalLinguistics, pages 237?246, Uppsala, Sweden, July.Association for Computational Linguistics.Koen Deschacht and Marie-Francine Moens.
2009.Semi-supervised semantic role labeling using theLatent Words Language Model.
In Proceedings ofthe 2009 Conference on Empirical Methods in Nat-ural Language Processing, pages 21?29, Singapore,August.Paramveer Dhillon, Dean P Foster, and Lyle H Ungar.2011.
Multi-view learning of word embeddings viaCCA.
In Advances in Neural Information Process-ing Systems, pages 199?207.Hagen F?urstenau and Mirella Lapata.
2012.
Semi-supervised semantic role labeling via structuralalignment.
Computational Linguistics, 38(1):135?171.Matthew Gerber and Joyce Chai.
2012.
Semantic RoleLabeling of Implicit Arguments for Nominal Predi-cates.
Computational Linguistics, 38(4):755?798.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational linguis-tics, 28(3):245?288.Jan Haji?c, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Ant`onia Mart?
?, Llu?
?sM`arquez, Adam Meyers, Joakim Nivre, SebastianPad?o, Jan?St?ep?anek, et al.
2009.
The conll-2009shared task: Syntactic and semantic dependenciesin multiple languages.
In Proceedings of the Thir-teenth Conference on Computational Natural Lan-guage Learning: Shared Task, pages 1?18.Karl Moritz Hermann, Dipanjan Das, Jason Weston,and Kuzman Ganchev.
2014.
Semantic frameidentification with distributed word representations.In Proceedings of the 52nd Annual Meeting of theAssociation for Computational Linguistics, pages1448?1458, Baltimore, Maryland, June.
Associationfor Computational Linguistics.Fei Huang and Alexander Yates.
2010.
Open-domainsemantic role labeling by modeling word spans.
InProceedings of the 48th Annual Meeting of the As-sociation for Computational Linguistics, pages 968?978, Uppsala, Sweden, July.Eric Huang, Richard Socher, Christopher Manning,and Andrew Ng.
2012.
Improving word represen-tations via global context and multiple word proto-types.
In Proceedings of the 50th Annual Meeting ofthe Association for Computational Linguistics (Vol-ume 1: Long Papers), pages 873?882, Jeju Island,Korea, July.R?emi Lebret and Ronan Collobert.
2014.
Word em-beddings through hellinger pca.
In Proceedings ofthe 14th Conference of the European Chapter of theAssociation for Computational Linguistics, pages482?490, Gothenburg, Sweden, April.
Associationfor Computational Linguistics.A.
Meyers, R. Reeves, C. Macleod, R. Szekely,V.
Zielinska, B.
Young, and R. Grishman.
2004.The nombank project: An interim report.
InA.
Meyers, editor, HLT-NAACL 2004 Workshop:Frontiers in Corpus Annotation, pages 24?31,Boston, Massachusetts, USA, May.412Jeff Mitchell and Mirella Lapata.
2010.
Compositionin Distributional Models of Semantics.
CognitiveScience, 34(8):1388?1429.Andriy Mnih and Geoffrey Hinton.
2009.
A scal-able hierarchical distributed language model.
In Ad-vances in Neural Information Processing Systems,volume 21, pages 1081?1088.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Michael Roth and Anette Frank.
2012.
Aligningpredicate argument structures in monolingual com-parable texts: A new corpus for a new task.
InProceedings of the First Joint Conference on Lexi-cal and Computational Semantics, pages 218?227,Montreal, Canada, June.Josef Ruppenhofer, Michael Ellsworth, Miriam R. L.Petruck, Christopher R. Johnson, and Jan Schef-fczyk.
2010.
FrameNet II: Extended Theory andPractice.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic composi-tionality through recursive matrix-vector spaces.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages1201?1211, Jeju Island, Korea, July.
Association forComputational Linguistics.Ivan Titov and Alexandre Klementiev.
2011.
Abayesian model for unsupervised semantic parsing.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 1445?1455, Port-land, Oregon, USA, June.Ivan Titov and Alexandre Klementiev.
2012.
Semi-supervised semantic role labeling: Approachingfrom an unsupervised perspective.
In Proceedingsof COLING 2012, pages 2635?2652, Mumbai, In-dia, December.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 384?394, Up-psala, Sweden, July.
Association for ComputationalLinguistics.Alexander Yeh.
2000.
More accurate tests forthe statistical significance of result differences.In Proceedings of the 18th International Confer-ence on Computational Linguistics, pages 947?953,Saarbr?ucken, Germany, August.Be?nat Zapirain, Eneko Agirre, Llu?
?s M`arquez, and Mi-hai Surdeanu.
2013.
Selectional preferences for se-mantic role classification.
Computational Linguis-tics, 39(3):631?663.Hai Zhao, Wenliang Chen, Jun?ichi Kazama, KiyotakaUchimoto, and Kentaro Torisawa.
2009.
Multi-lingual dependency learning: Exploiting rich fea-tures for tagging syntactic and semantic dependen-cies.
In Proceedings of the Thirteenth Confer-ence on Computational Natural Language Learning(CoNLL 2009): Shared Task, pages 61?66, Boulder,Colorado, USA, June.413
