AbstractCross-Language Multimedia Information RetrievalSharon Flankemotion, Inc.2600 Park Tower Dr., Vienna, VA 22180 USAsharon.flank@emotion.comSimple measures can achieve high-accuracycross-language r trieval in carefully chosenapplications.
Image retrieval is one of thoseapplications, with results ranging from 68%of human translator performance forGerman, to 100% for French.1 Introductioncontain strings of keywords.
Typical queriesare, as in most Web search applications, twoto three words in length.
At this point, all ofthe captions are in English.
eMotion hosts alarge database of images for sale and forlicensing, PictureQuest.
At least 10% ofPictureQuest's user base is outside theUnited States.
The tests were performed onthe PictureQuest database of approximately400,000 images.Information is increasingly global, and theneed to access it crosses language barriers.The topic of this paper, cross-languageinformation retrieval, concerns the automaticretrieval of text in one language via a queryin a different language.
A considerablebody of literature has grown up aroundcross-language information retrieval (e.g.Grefenstette 1998, TREC-7 1999).
Thereare two basic approaches.
Either the querycan be translated, or each entire documentcan be translated into the same language asthe query.
The accuracy of retrieval acrosslanguages, however, is generally not good.One of the weaknesses that plagues cross-language retrieval is that we do not have agood sense of who the users are, or how bestto interact with them.In this paper we describe a multimediaapplication for which cross-languageinformation retrieval works particularlywell.
eMotion, Inc. has developed a naturallanguage information retrieval applicationthat retrieves images, such as photographs,based on short textual descriptions orcaptions.
The captions are typically one tothree sentences, although they may alsoRecent Web utilization data for PictureQuestindicate that of the 10% of users fromoutside the United States, a significantportion come from Spanish-speaking,French-speaking, and German-speakingcountries.
It is expected that addingappropriate language interfaces and listingPictureQuest in foreign-language searchengines will dramatically increase non-English usage.The Cross-Language MultimediaRetrieval ApplicationThis paper offers several originalcontributions to the literature on cross-language information retrieval.
First, thechoice of application is novel, andsignificant because it simplifies the languageproblem enough to make it tractable.Because the objects retrieved are images andnot text, they are instantly comprehensibleto the user regardless of language issues.This fact makes it possible for users toperform a relevance assessment without heneed for any kind of translation.
Moreimportant, users themselves can selectobjects of interest, without recourse totranslation.
The images are, in fact,13associated with caption information, but,even in the monolingual system, few usersever even view the captions.
It should benoted that most of the images inPictureQuest are utilized for advertising andpublishing, rather than for newsapplications.
Users of history and newsphotos do tend to check the captions, andoften users in publishing will view thecaptions.
For advertising, however, what theimage itself conveys is far more importantthan the circumstances under which it wascreated.Another significant contribution of thispaper is the inclusion of a variety ofmachine translation systems.
None of thesystems tested is a high-end machinetranslation system: all are freely available onthe Web.Another key feature of this paper is thecareful selection of an accuracy measureappropriate to the circumstances of theapplication.
The standard measure, percentof monolingual performance achieved, isused, with a firm focus on precision.
In thisapplication, users are able to evaluate onlywhat they see, and generally have no ideawhat else is present in the collection.
As aresult, precision is of far more interest ocustomers than recall.
Recall is, however, ofinterest to image suppliers, and in any case itwould not be prudent to optimize forprecision without taking into account therecall tradeoff.The PictureQuest application avoids severalof the major stumbling blocks that stand inthe way of high-accuracy cross-languageretrieval.
Ballesteros and Croft (1997) noteseveral pitfalls common to cross-languageinformation retrieval:(1) The dictionary may not containspecialized vocabulary (particularlybilingual dictionaries).
(2) Dictionary translations are inherentlyambiguous and add extraneous termsto the query.
(3) Failure to translate multi-termconcepts as phrases reduceseffectiveness.In the PictureQuest application, these pitfallsare minimized because the queries are short,not paragraph-long descriptions as in TREC(see, e.g., Voorhees and Harman 1999).This would be a problem for a statisticalapproach, since the queries present littlecontext, but, since we are not relying oncontext (because reducing ambiguity is notour top priority) it makes our task simpler.Assuming that the translation program keepsmulti-term concepts intact, or at least that itpreserves the modifier-head structure, wecan successfully match phrases.
Thecaptions (i.e.
the documents o be retrieved)are mostly in sentences, and their phrasesare intact.
The phrase recognizer identifiesmeaningful phrases (e.g.
fire engine) andhandles them as a unit.
The pattern matcherrecognizes core noun phrases and makes itmore likely that hey will match correctly.Word choice can be a major issue as well forcross-language retrieval systems.
Someambiguity problems can be resolved throughthe use of a part-of-speech tagger on thecaptions.
As Resnik and Yarowsky (inpress) observe, part-of-speech taggingconsiderably reduces the word sensedisambiguation problem.
However, someambiguity remains.
For example, thedecision to translate a word as car,automobile, or vehicle, may dramaticallyaffect retrieval accuracy.
The PictureQuest14system uses a semantic net based onWordNet (Fellbaum 1998) to expand terms.Thus a query for car or automobile willretrieve ssentially identical results; vehiclewill be less accurate but will still retrievemany of the same images.
So while wordchoice may be a significant consideration fora system like that of Jang et al, 1999, itsimpact on PictureQuest is minimal.The use of WordNet as an aid to informationretrieval is controversial, and some studiesindicate it is more hindrance than help (e.g.Voorhees 1993, 1994, Smeaton, Kelledy andO'Donnell 1995).
WordNet uses extremelyfine-grained distinctions, which can interferewith precision even in monolingualinformation retrieval.
In a cross-languageapplication, the additional senses can addconfounding mistranslations.
If, on theother hand, WordNet expansion isconstrained, the correct ranslation may bemissed, lowering recall.
In the PictureQuestapplication, we have tuned WordNetexpansion levels and the correspondingweights attached to them so that WordNetserves to increase recall with minimalimpact on precision (Flank 2000).
Thistuned expansion appears to be beneficial inthe cross-language application as well.Gilarranz, Gonzalo and Verdejo (1997)point out that, for cross-languageinformation retrieval, some precision is lostin any case, and WordNet is more likely toenhance cross-linguistic than monolingualapplications.In fact, Smeaton and Quigley (1996)conclude that WordNet is indeed helpful inimage retrieval, in particular because imagecaptions are too short for statistical analysisto be useful.
This insight is what led us todevelop a proprietary image retrieval enginein the first place: fine-grained linguisticanalysis is more useful that a statisticalapproach in a caption averaging some thirtywords.
(Our typical captions are longer thanthose reported in Smeaton and Quigley1996).3 Translation MethodologyWe performed preliminary testing using twotranslation methodologies.
For the initialtests, we chose European languages: French,Spanish, and German.
Certainly this choicesimplifies the translation problem, but in ourcase it also reflects the most pressingbusiness need for translation.
For theFrench, Spanish, and German tests, we usedSystran as provided by AltaVista(Babelfish); we also tested several otherWeb translation programs.
We used nativespeakers to craft queries and then translatedthose queries either manually orautomatically and submitted them toPictureQuest.
The resulting image set wasevaluated for precision and, in a limitedfashion, for recall.The second translation methodologyemployed was direct dictionary translation,tested only for Spanish.
We used the samequeries for this test.
Using an on-lineSpanish-English dictionary, we selected, foreach word, the top (top-frequency)translation.
We then submitted this word-by-word translation to PictureQuest.
(Unlike AltaVista, this method spell-corrected letters entered without thenecessary diacritics.)
Evaluation proceededin the same manner.
The word-by-wordmethod introduces a weakness in phraserecognition: any phrase recognitioncapabilities in the retrieval system aredefeated if phrases are not retained in theinput.
We can assume that the non-English-speaking user will, however, recognizephrases in her or his own language, and look15them up as phrases where possible.
Thus wecan expect at least those multiword phrasesthat have a dictionary entry to be correctlyunderstood.
We still do lose the nounphrase recognition capabilities in theretrieval system, further confounded by thefact that in Spanish adjectives follow thenouns they modify.
In the hombre denegocios example in the data below, bothAltaVista and Langenscheidt correctlyidentify the phrase as multiword, andtranslate it as businessman rather than manof businesses.The use of phrase recognition has beenshown to be helpful, and, optimally, wewould like to include it.
Hull andGrefenstette 1996 showed the upper boundof the improvements possible by usinglexicalized phrases.
Every phrase thatappeared was added to the dictionary, andthat tactic did aid retrieval.
Both statisticalco-occurrence and syntactic phrases are alsopossible approaches.
Unfortunately, theextra-system approach we take here reliesheavily on the external machine translationto preserve phrases intact.
If AltaVista (or,in the case of Langenscheidt, he user)recognizes a phrase and translates it as aunit, the translation is better and retrieval islikely to be better.
If, however, thetranslation mistakenly misses a phrase,retrieval quality is likely to be worse.
As forcompositional noun phrases, if thetranslation preserves normal word order,then the PicmreQuest-internal oun phraserecognition will take effect.
That is, ifjeunefille translates as young girl, thenPictureQuest will understand that young isan adjective modifying girl.
In the moredifficult case, if the translation preserves thecorrect order in translating la selva africana,i.e.
the African jungle, then noun phraserecognition will work.
If, however, it comesout as the jungle African, then retrieval willbe worse.
In the architecture d scribed here,fixing this problem requires access to theinternals of the machine translation program.4 EvaluationEvaluating precision and recall on a largecorpus is a difficult task.
We used theevaluation methods detailed in Flank 1998.Precision was evaluated using a crossingmeasure, whereby any image ranked higherthan a better match was penalized.
Recallper se was measured only with respect o adefined subset of the images.
Rankingincorporates some recall measures into theprecision score, since images ranked too loware a recall problem, and images marked toohigh are a precision problem.
If there arethree good matches, and the third shows upas #4, the bogus #3 is a precision problem,and the too-low #4 is a recall problem.For evaluation of the overall cross-languageretrieval performance, we simply measuredthe ratio between the cross-language andmonolingual retrieval accuracy (C/M%).This is standard; see, for example, Jang et al1999.Table 1 illustrates the percentage ofmonolingual retrieval performance weachieved for the translation tests performed.In this instance, we take the precisionperformance of the human-translated queriesand normalize it to 100%, and adjust theother translation modalities relative to thehuman baseline.Language RawPrecision (%)French (Human) 80French 86(AltaVista)French 66(TransparentLanguage)C/M(%)1001008316Language RawPrecision (%)French (Intertran) 44Spanish (Human) 90Spanish 53(AltaVista)63 Spanish(LangenscheidtBilingualDictionary)German (Human) 80German 54(AltaVista)C/M(%)55100597010068Several other factors make the PictureQuestapplication a particularly good applicationfor machine translation technology.
Unlikedocument ranslation, there is no need tomatch every word in the description; usefulimages may be retrieved even if a word ortwo is lost.
There are no discourse issues atall: searches never use anaphora, and no onecares if the translated query sounds good ornot.In addition, the fact that the objects beingretrieved were images greatly simplified theendeavor.
Under normal circumstances,developing a user-friendly interface is amajor challenge.
Users with only limited (ornonexistent) reading knowledge of thelanguage of the documents need a way todetermine, first, which ones are useful, andsecond, what they say.
In the PictureQuestapplication, however, the retrieved assets areimages.
Users can instantly assess whichimages meet heir needs.In conclusion, it appears that simple on-linetranslation of queries can support effectivecross-language information retrieval, forcertain applications.
We showed how animage retrieval application eliminates omeof the problems of cross-language r trieval,and how carefully tuned WordNet expansionsimplifies word choice issues.
We used avariety of machine translation systems, noneof them high-end and all of them free, andnonetheless achieved commercially viableresults.5 Appendix: DataSource Example ScoreHuman men repairing road 100AV men repairing wagon 0Lang.
man repair oad 100Human woman wearing red 100shopping in storeAV woman dressed red buying 90 (2 ofin one tends 20 bad)Lang.
woman clothe red buy in wearingshop red is lost75 (5 of20 bad)Human cars driving on the 100highwayAV cars handling by the 80' (4 offreeway 20 bad)Lang.
cart handle for the 0expresswayHuman lions hunting in the 80 (1 of 5African forest bad)AV lions hunting in the 80 (1 of 5African forest bad)Lang.
lion hunt in thejungle 45 (11 ofgSt \] I 20 bad)~'~ .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
I~:~ i ~Human juggler using colorful balls 67 (1 of 3bad)AV juggler with using balls of 50 (4 of 8colors bad)Lang.
juggler by means of use (0; 1ball colour should bethere)17Source Example ScoreHuman blonde children playing 90(#3with marbles should be#1;remainderof top 20ok)AV blond children playing 90 (2 ofwith marbles 20 bad)Lang.
young fair play by means 50 (1 of 2of marble bad)Human buying powerAV spending power 45 (11 of20 bad)Lang.AVpurchasing power 100successful businessman i 60 (8 ofoffice 20 bad)Lang.
successful businessman i 6 (8 of 20office bad)Human mother and daughter 100 (butbaking bread in the kitchen no fullmatches)AV mother and daughter 30 (14 of\[horneando-removed\] 20 bad)bread in the kitchenLang.
mother and child bake 100 (butbread in the kitchen no fullmatches)Human old age and loneliness 100AV oldness and solitude 0Lang.
old age and loneliness 1005.1 SpanishHuman translations, tested on PictureQuest:90% (normalize to 100%)AltaVista: 53% (59% normalized)Langenscheidt, word-by-word: 63% (70%normalized)5.1.1 AltaVistaFor AltaVista, we left out the words thatAltaVista didn't translate.5.1.2 LangenscheidtLangenscheidt, word-by-word: 63% (70%normalized)For the Langenscheidt word-by-word, weused the bilingual dictionary to translateeach word separately as if we knew noEnglish at all, and always took the firsttranslation.
We made the followingadjustments:1.
Left out "una," since Langenscheidtmapped it to "unir" rather than to either a orone2.
Translated "e" as and instead of  e5.2 FrenchHuman translations, tested on PictureQuest:80%AltaVista: 86% (100% normalized)Transparent Language (freetranslation.com):66% (83% normalized)Intertran (www.intertran.net:2000): 44%(55% normalized)\[French examples originally drawn fromhttp ://humanities.uchicago.edu/ARTFL/projects/academie/1835.searchform.html:French-French\]Source : Example Score~,, ~ i!, ~ii~l! "
~:s~:: ~ ~'~  ~Human signs of the zodiac 100AV signs of the zodiac 100TrLang sign zodiaque 0IntrTranHuman\[signes\] any zodiacfish in water10030 (14 of 20bad)AV fish in water 30 (14 of 20bad)TrLang fish in water 30 (14 of 20bad)fish at water IntrTran 30 (14 of 20bad)18Source Example ScoreiHuman painful earaches lO0AV Painful earaches 100TrLang the painful ear evil 0the \[manx\] \[doreille\]' 0distressingto take a rabbit by theearsTo take a rabbit by theIntrTran,~ ~ ~ii ~HumanAV65 (7 of 20bad)65 (7 of 20bad) earsTrLang take a rabbit by the ears 65 (7 of 20bad)IntrTranHumancapture a bunny by theearscat which lives in wood80 (1 of 5bad)%~!,~:,.'
i~: ~'"45 (11 of 20bad)AV Cat which lives in wood 45 (11 of 20bad)TrLang cat that lives in wood 65 (7 of 20bad)cat thanksgiving lives atthe forestto leave a houseIntrTranHuman70 (6 of 20bad)60 (8 of 20bad)AV To leave a house 60 (8 of 20bad)TrLang to go out of a house 95 (1 of 20bad)IntrTran come out dune' dwelling 90 (18 of 20house bad)Human carpenter's tool 95 (1 of 20bad)AV Instrument of carpenter 100TrLang instrument of carpenter 100I IntrTran implement any carpenter 35 (13 of 20bad)Human to play the violin 100AV to play of the violin 100TrLang to play the violin 100IntrTran gamble any violin 0Human pleasures of the body 100Source Example ScoreAV Pleasures of the body 100100 TrLangIntrTranthe pleasures of the bodythe delight any bodyHuman a girl eats fruitAV a girl eats fruit 100TrLang a girl eats fruit 100IntrTran a girl am eating any fruit 65 (7 of 20bad)01005.3 GermanHuman translations, tested on PictureQuest:80% (100% normal ized)AltaVista 54% (68% normal ized)Source Example ScoreHuman boys golf course 95AV golf course 95Human artificial paradise 100AV artificial paradiese 0Human solar energy for automobiles 95AV solar energy for auto 95 ........................ ~, , ,~ :~,,~ .
~.~ ~ ~ ~; : .
, .
~<.~Human hiking through the forest 90AV migrations by the forest 0Human an elephant in a zoo 25(#17shouldbe #2)AV elephant in the zoo 100............... i!~ n = ~!~ ~ ~Human the synthesis of I00desoxyribonucleic acidAV the synthesis of the 0DesoxynribonukleinsaeureHuman black cars 100AV black auto 100Human playing together 60young together play19Source Example ScoreHuman women in blue 65AV Ladies in blue 75Human woman at work 65AV Ladies on work 406 AcknowledgementsI am grateful to Doug Oard for comments onan earlier version of  this paper.7 ReferencesBallesteros, Lisa, and W. Bruce Croft, 1997.
"PhrasalTranslation and Query Expansion Techniques forCross-Language Information Retrieval," in AAAISpring Symposium on Cross-Language Text andSpeech Retrieval, Stanford University, Palo Alto,California, March 24-26, 1997.Fellbaum, Christiane, ed., 1998.
WordNet: AnElectronic Lexical Database.
Cambridge, MA: MITPress.Flank, Sharon.
2000.
"Does WordNet ImproveMultimedia Information Retrieval?"
Working paper?Flank, Sharon.
1998?
"A Layered Approach to NLP-Based Information Retrieval," in Proceedings ofCOLING-ACL, 36th Annual Meeting of theAssociation for Computational Linguistics, Montreal,Canada, 10-14 August 1998.Gilarranz, Julio, Julio Gonzalo and Felisa Verdejo.1997.
"An Approach to Conceptual Text RetrievalUsing the EuroWordNet Multilingual SemanticDatabase," in AAAI Spring Symposium on Cross-Language Text and Speech Retrieval, StanfordUniversity, Palo Alto, California, March 24-26,1997.
(http://www.clis.umd.edu/dlrg/filter/sss/papers)Grefenstette, Gregory, ed., 1998.
Cross-LanguageInformation Retrieval.
Norwell, MA: Kluwer.Hull, David A. and Gregory Grefenstette, 1996.
"Experiments in Multilingual Information Retrieval,"m Proceedin s o the 19 th L ? "
g f nternational Conferenceon Research and Development in InformationRetrieval (SIGIR96) Zurich, Switzerland.Jang, Myung-Gil, Sung Hyon Myaeng, and SeYoung Park, 1999.
"Using Mutual Information toResolve Query Translation Ambiguities and QueryTerm Weighting," in Proceedings of 37 th AnnualMeeting of the Association for ComputationalLinguistics, College Park, Maryland.McCarley, J. Scott, 1999.
"Should We Translate theDocuments or the Queries in Cross-LanguageInformation Retrieval?
"Resnik, Philip and Yarowsky, David, in press.
"Distinguishing Systems and Distinguishing Sense:New Evaluation Methods for Word SenseDisambiguation," Natural Language Engineering.Smeaton, Alan F., F. Kelledy and R. O'Donnell,1995.
"TREC-4 Experiments at Dublin CityUniversity: Thresholding Posting Lists, QueryExpansion with WordNet and POS Tagging ofSpanish," in Donna K. Harman (ed.)
NIST SpecialPublication 500-236: The Fourth Text REtrievalConference (TREC-4), Gaithersburg, MD, USA:Department of Commerce, National Institute ofStandards and Technology.
(http://trec.nist.gov/pubs/trec4/t4_proceedings.html)Smeaton, Alan F. and I. Quigley, 1996.
"Experimentson Using Semantic Distances Between Words inImage Caption Retrieval," in Proceedings of the 19 thInternational Conference on Research andDevelopment in Information Retrieval (SIGIR96)Zurich, Switzerland.Voorhees, Ellen M. 1994.
"Query Expansion UsingLexical-Semantic Relations," in Proceedings of the17 th International ACM SIGIR Conference onResearch and Development in Information Retrieval,pp.
61-70.Voorhees, Ellen M. 1993.
"Using WordNet toDisambiguate Word Senses for Text Retrieval," inProceedings of the 16 th International ACM SIGIRConference on Research and Development inInformation Retrieval, pp.
171-180.Voorhees, Ellen M. and Donna K. Harman, editors,1999?
The 7 th Text Retrieval Conference (TREC- 7).20
