Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1627?1637,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsUnsupervised Morphology Induction Using Word EmbeddingsRadu SoricutGoogle Inc.rsoricut@google.comFranz Och?Human Longevity Inc.och@humanlongevity.comAbstractWe present a language agnostic, unsupervisedmethod for inducing morphological transfor-mations between words.
The method re-lies on certain regularities manifest in high-dimensional vector spaces.
We show that thismethod is capable of discovering a wide rangeof morphological rules, which in turn are usedto build morphological analyzers.
We evaluatethis method across six different languages andnine datasets, and show significant improve-ments across all languages.1 IntroductionWord representations obtained via neural net-works (Bengio et al, 2003; Socher et al, 2011a)or specialized models (Mikolov et al, 2013a) havebeen used to address various natural language pro-cessing tasks (Mnih et al, 2009; Huang et al, 2014;Bansal et al, 2014).
These vector representationscapture various syntactic and semantic propertiesof natural language (Mikolov et al, 2013b).
Inmany instances, natural language uses a small setof concepts to render a much larger set of mean-ing variations via morphology.
We show in this pa-per that morphological transformations can be cap-tured by exploiting regularities present in word-representations as the ones trained using the Skip-Gram model (Mikolov et al, 2013a).In contrast to previous approaches that com-bine morphology with vector-based word represen-tations (Luong et al, 2013; Botha and Blunsom,2014), we do not rely on an external morpholog-ical analyzer, such as Morfessor (Creutz and La-?Work done at Google, now at Human Longevity Inc.gus, 2007).
Instead, our method automatically in-duces morphological rules and transformations, rep-resented as vectors in the same embedding space.At the heart of our method is the SkipGrammodel described in (Mikolov et al, 2013a).
We fur-ther exploit the observations made by Mikolov etal (2013b), and further studied by (Levy and Gold-berg, 2014; Pennington et al, 2014), regarding theregularities exhibited by such embedding spaces.These regularities have been shown to allow infer-ences of certain types (e.g., king is to man whatqueen is to woman).
Such regularities also hold forcertain morphological relations (e.g., car is to carswhat dog is to dogs).
In this paper, we show that onecan exploit these regularities to model, in a princi-pled way, prefix- and suffix-based morphology.
Themain contributions of this paper are as follows:1. provides a method by which morphologicalrules are learned in an unsupervised, language-agnostic fashion;2. provides a mechanism for applying these rulesto known words (e.g., boldly is analyzed asbold+ly, while only is not);3. provides a mechanism for applying these rulesto rare and unseen words;We show that this method improves state-of-the-artperformance on a word-similarity rating task usingstandard datasets.
We also quantify the impact of ourmorphology treatment when using large amounts oftraining data (tens/hundreds of billions of words).The technique we describe is capable of induc-ing transformations that cover both typical, regu-lar morphological rules, such as adding suffix ed1627to verbs in English, as well as exceptions to suchrules, such as the fact that pluralization of wordsthat end in y require substituting it with ies.
Be-cause each such transformation is represented in thehigh-dimensional embedding space, it therefore cap-tures the semantics of the change.
Consequently,it allows us to build vector representations for anyunseen word for which a morphological analysis isfound, therefore covering an unbounded (albeit in-complete) vocabulary.Our empirical evaluations show that thislanguage-agnostic technique is capable of learningmorphological transformations across variouslanguage families.
We present results for English,German, French, Spanish, Romanian, Arabic,and Uzbek.
The results indicate that the inducedmorphological analysis deals successfully withsophisticated morphological variations.2 Previous WorkMany recent proposals in the literature use word-representations as the basic units for tacklingsentence-level tasks such as language model-ing (Mnih and Hinton, 2007; Mikolov and Zweig,2012), paraphrase detection (Socher et al, 2011a),sentiment analysis (Socher et al, 2011b), discrimi-native parsing (Collobert, 2011), as well as similartasks involving larger units such as documents (Glo-rot et al, 2011; Huang et al, 2012; Le and Mikolov,2014).
The main advantage offered by these tech-niques is that they can be both trained in an unsu-pervised manner, and also tuned using supervised la-bels.
However, most of these approaches treat wordsas units, and fail to account for phenomena involv-ing the relationship between various morphologicalforms that affect word semantics, especially for rareor unseen words.Previous attempts at dealing with sub-word unitsand their compositionality have looked at explicitly-engineered features such as stems, cases, POS, etc.,and used models such as factored NLMs (Alexan-drescu and Kirchhoff, 2006) to obtain representa-tions for unseen words, or compositional distribu-tional semantic models (Lazaridou et al, 2013) toderive representations for morphologically-inflectedwords, based on the composing morphemes.
A morerecent trend has seen proposals that deal with mor-phology using vector-space representations (Luonget al, 2013; Botha and Blunsom, 2014).
Given wordmorphemes (affixes, roots), a neural-network archi-tecture (recursive neural networks in the work ofLuong et al(2013), log-bilinear models in the caseof Botha and Blunsom (2014)), is used to obtainembedding representations for existing morphemes,and also to combine them into (possibly novel) em-bedding representations for words that may not havebeen seen at training time.Common to these proposals is the fact that themorphological analysis of words is treated as anexternal, preprocessing-style step.
This step isdone using off-the-shelf analyzers such as Morfes-sor (Creutz and Lagus, 2007).
As a result, the mor-phological analysis happens within a different modelcompared to the model in which the resulting mor-phemes are consequently used.
In contrast, the workpresented here uses the same vector-space embed-ding to achieve both the morphological analysis ofwords and to compute their representation.
As aconsequence, the morphological analysis can be jus-tified in terms of the relationship between the result-ing representation and other words that exhibit sim-ilar morphological properties.3 Morphology Induction using EmbeddingSpacesThe method we present induces morphologicaltransformations supported by evidence in terms ofregularities within a word-embedding space.
We de-scribe in this section the algorithm used to inducesuch transformations.3.1 Morphological TransformationsWe consider two main transformation types, namelyprefix and suffix substitutions.
Other transformationtypes can also be considered, but we restrict the fo-cus of this work to morphological phenomena thatcan be modeled via prefixes and suffixes.We provide first a high-level description of our al-gorithm, followed by details regarding the individualsteps.
The following steps are applied to monolin-gual training data over a finite vocabulary V :1.
Extract candidate prefix/suffix rules from V2.
Train embedding space En?
Rnfor all wordsin V16283.
Evaluate quality of candidate rules in En4.
Generate lexicalized morphological transfor-mationsWe provide more detailed descriptions next.Extract candidate rules from VStarting from (w1, w2) ?
V2, the algorithmextracts all possible prefix and suffix substitu-tions from w1to w2, up to a specified size1.We denote such substitutions using triplets ofthe form type:from:to.
For instance, tripletsuffix:ed:ing denotes the substitution of suf-fix ed with suffix ing; this substitution is supportedby many word pairs in an English vocabulary, e.g.
(bored, boring), (stopped, stopping), etc.
We callthese triplets candidate rules, because they form thebasis of an extended set from which the algorithmextracts morphological rules.At this stage, the candidate rules set contains bothrules that reflect true morphology phenomena, e.g.suffix:s: (replace suffix s with the null suf-fix, extracted from (stops, stop), (weds, wed), etc.
),or prefix:un: (replace prefix un with the nullprefix, from (undone, done), etc.
), but also rulesthat simply reflect surface-level coincidences, e.g.prefix:S: (delete S at the beginning of a word,from (Scream, cream), (Scope, cope), etc.
).Train embedding spaceUsing a large monolingual corpus, we train aword-embedding space Enof dimensionality n forall words in V using the SkipGram model (Mikolovet al, 2013a).
For the experiments reported inthis paper, we used our own implementation of thismodel (which varies only slightly from the publicly-available word2vec implementation2).Evaluate quality of candidate rulesThe extracted candidate rules set is evaluated byusing, for each proposed rule r, its support set:Sr= {(w1, w2) ?
V2|w1r??
w2}The notation w1r?
w2means that rule r applies toword w1(e.g., for rule suffix:ed:ing, word w11A maximum size of 6 is used in our experiments.2At code.google.com/p/word2vec.rule hit rate Example ?dwsuffix:er:o 0.8 ?dVotersuffix:ton: 1.1 ?dGaletonprefix:S: 1.6 ?dSDKprefix::in 28.8 ?dcompetentsuffix:ly: 32.1 ?dofficiallyprefix::re 37.0 ?dsitedprefix:un:re 39.0 ?dunmadesuffix:st:sm 52.5 ?degoistsuffix:ted:te 54.9 ?dimitatedsuffix:ed:ing 68.1 ?dprocuredsuffix:y:ies 69.6 ?dfoundrysuffix:t:ts 73.0 ?dpugilistsuffix:sed:zed 80.1 ?dserialisedTable 1: Candidate rules evaluated in En.ends with suffix ed), and the result of applying therule to word w1is word w2.
To speed up computa-tion, we downsample the sets Srto a large-enoughnumber of word pairs (1000 has been used in the ex-periments in this paper).We define a generic evaluation functionEvFoverpaired couples in Sr?Sr, using a function F : Rn?Rn?
R, as follows:EvF((w1, w2), (w,w?))
= FE(w2, w1+ ?
dw) (1)(w1, w2), (w,w?)
?
Sr, ?
dw= w??
wWord-pair combinations in Sr?Srare evaluated us-ing Eq.
1 to assess the meaning-preservation prop-erty of rule r. We use as FEfunction rankE,the cosine-similarity rank function in En.
We canquantitatively measure the assertion ?car is to carswhat dog is to dogs?, as rankE(cars, car+?ddog).We use a single threshold t0rankto capture meaningpreservation (all the experiments in this paper uset0rank= 100): for each proposed rule r, we com-pute a hit rate based on the number of times Eq.
1scores above t0rank, over the number of times it hasbeen evaluated.
In Table 1 we present some of thesecandidate rules and their hit rate.We note that rules that are non-meaning?preserving receive low hit rates, while rules that aremorphological in nature, such as suffix:ed:ing(verb change from past/participle to present-continuous) and suffix:y:ies (pluralization ofy?ending nouns), receive high hit rates.1629w1w2rank cosine transformationcreate created 0 0.58 suffix::d:?dethronecreate creates 0 0.65 suffix:te:tes:?evaluatecreate creates 1 0.62 suffix::s:?contradictcreated create 0 0.65 suffix:ed:e?erodedcreation create 0 0.52 suffix:ion:e:?communicationcreation created 0 0.54 suffix:ion:ed:?disruptionrecreations recreate 2 0.59 suffix:ions:e:?translationsrecreations recreating 1 0.53 suffix:ions:ing:?constructionsrecreations Recreations 81 0.64 prefix:r:R:?remediationTable 2: Examples of lexicalized morphological transformations evaluated in Enusing rank and cosine.Generate lexicalized morphologicaltransformationsThe results in Table 1 indicate the need for cre-ating lexicalized transformations.
For instance, rulesuffix:ly: (drop suffix ly, a perfectly reason-able morphological transformation in English) isevaluated to have a hit rate of 32.1%.
While suchtransformations are desirable, we want to avoid ap-plying them when firing without yielding meaning-preserving results (the rest of 67.9%), e.g., for word-pair (only, on).
We therefore create lexicalized trans-formations by restricting the rule application to thevocabulary subset of V which passes the meaning-preservation criterion.The algorithm also computes best direction vec-tors ?dwfor each rule support set Sr.
It greedilyselects a direction vector ?dw0that explains (basedon Equation 1) the most pairs in Sr. After subsetSw0ris computed for direction vector ?dw0, it ap-plies recursively on set Sr?
Sw0r.
This yields a newbest direction vector ?dw1, and so on.
The recursionstops when it finds a direction vector ?dwkthat ex-plains less than a predefined number of words (weused 10 in all the experiments from this paper).We consider multiple direction vectors ?dwibecause of the possibly-ambiguous nature of amorphological transformation.
Consider rulesuffix::s, which can be applied to the nounwalk to yield plural-noun walks; this case is mod-eled with a transformation like walk + ?dinvention,since ?dinvention=inventions?invention is a direc-tion that our procedure deems to explain well nounpluralization; it can also be applied to the verb walkto yield the 3rd-person singular form of the verb, inwhich case it is modeled as walk + ?denlist, since?denlist=enlists?enlist is a direction that our pro-cedure deems to explain well 3rd-person singularverb forms.
In that sense, our algorithm goes beyondproposing simple surface-level morphemes, with di-rection vectors encoding well-defined semantics forour morphological analysis.Lexicalized rules enhanced with direction vectorsare called morphological transformations.
For eachmorphological transformation, we evaluate againhow well it passes a proximity test in Enfor thewords it applies to.
As evaluation criteria, we usetwo instances of Eq 1, with FEinstantiated to rankEand cosineE, respectively.
We apply more stringentcriteria in this second pass, using thresholds on theresulting rank (trank) and cosine (tcosine) values toindicate meaning preservation (we used trank= 30and tcosine= 0.5 in all the experiments in this pa-per).
We present in Table 2 a sample of the re-sults of this procedure.
For instance, word createcan be transformed to creates using two differenttransformations: suffix:te:tes:?evaluateand suffix::s:?contradict, passing themeaning-preservation criteria with rank=0, co-sine=0.65, and rank=1, cosine=0.62, respectively.Lexicalized morphological transformations overa vocabulary V have a graph-based interpretation:words represent nodes, transformations representedges in a labeled, weighted, cyclic, directed multi-graph (weights are (r, c) pairs, rank and cosinevalues; multiple direction vectors create multipleedges between two nodes; cycles may exist, see1630Figure 1: A few strongly connected components of a GVMorphgraph for English.e.g.
created?create?created in Table 2).
Weuse the notation GVMorphto denote such a graph.GVMorphusually contains many strongly connectedcomponents, with components representing familiesof morphological variations.
As an illustration, wepresent in Figure 1 a few strongly connected compo-nents obtained for an English embedding space (forillustration purposes, we show only a maximum of 2directed edges between any two nodes in this multi-graph, even though more may exist).3.2 Inducing 1-to-1 Morphological MappingsThe induced graph GVMorphencodes a lot of infor-mation about words and how they relate to eachother.
For some applications, however, we wantto normalize away morphological diversity by map-ping to a canonical surface form.
This amounts toselecting, from among all the candidate morpholog-ical transformations generated, specific 1-to-1 map-pings.
In graph terms, this means building a labeled,weighted, acyclic, directed graph DVMorphstarting1631Figure 2: A part of a DVMorphgraph, with the morpho-logical family for the normal-form created.from GVMorph, using the nodes from GVMorphand re-taining only edges that meet certain criteria.For the experiments presented in Section 4, webuild a directed graph DVMorphas follows:1. edge w1(r,c)?
w2in GVMorphis considered onlyif count(w1) ?
count(w2) in V ;2. if multiple such edges exist, chose the one withminimal rank r;3. if multiple such edges still exist, chose the onewith the maximal cosine c.The interpretation we give is word-normalization: anormalization of w to w?is guaranteed to be mean-ing preserving (using the direction-vector seman-tics), and to a more frequent form.
A snippet of theresulting graph DVMorphis presented in Figure 2.One notable aspect of this normalization pro-cedure is that these are not ?traditional?
morpho-logical mappings, with morphology-inflected wordsmapped to their linguistic roots.
Rather, our methodproduces morphological mappings that favor fre-quency over linguistic normalization.
An exam-ple of this can be seen in Figure 2, where the rootform create is morphologically-explained by map-ping it to the form created.
This choice is purelybased on our desire to favor the accuracy of theword-representations for the normal forms; differ-ent choices regarding how this pruning procedureis performed lead to different normalization proce-dures, including some that are more linguistically-motivated (e.g., length-based).3.3 Morphological Transformations for Rareand Unknown WordsFor some count threshold C, we define VC= {w ?V |C ?
count(w)}.
The method we presented up tothis point induces a morphology graph DVCMorphthatcan be used to perform morphological analysis forany words in VC.
We analyze the rest of the wordswe may encounter (i.e., rare words and OOVs) bymapping them directly to nodes in DVCMorph.We extract such mappings from DVCMorphusingall the sequences of edges that start at nodes in thegraph and end in a normal-form (i.e., nodes that haveout-degree 0).
The result is a set of rule sequencesdenoted RS.
A count cutoff on the rule sequencecounts is used, since low-count sequences tend tobe less reliable (in the experiments reported in thispaper we use a cutoff of 50).
We also denote with Rthe set of all edges in DMorph.
Using sets RS and R,we mapw 6?
VCto a nodew??
DVCMorph, as follows:1. for rule-sequences s ?
RS from highest-to-lowest count, if ws?
w?and w??
DVCMorph,then s is the morphological analysis for w;2. if no s is found, do breadth-first search inDVCMorphusing r ?
R, up to a predefined3depthd; for k ?
d, word w?with wr1...rk??
w?
?DVCMorphand the highest count in VCis the mor-phological analysis for w.For example, this procedure uses the RS sequences=prefix : un : , suffix : ness :  to performthe OOV morphological analysis unassertivenesss??assertive.
We perform an in-depth analysis ofthe performance of this procedure in Section 4.2.4 Empirical ResultsIn this section, we evaluate the performance of theprocedure described in Section 3.
Our evaluationsaim at answering several empirical questions: how3We use d=1 in the experiments reported in Section 4.2.1632Lang |Tokens| |V | |GVMorph| |DVMorph|EN 1.1b 1.2m 780k 75,823DE 1.2b 2.9m 3.7m 169,017FR 1.5b 1.2m 1.8m 92,145ES 566m 941k 2.2m 82,379RO 1.7b 963k 3.8m 141,642AR 453m 624k 2.4m 114,246UZ 850m 2.0m 5.6m 194,717Table 3: Statistics regarding the size of the training dataand the induced morphology graphs.well does our method capture morphology, and howdoes it compare with previous approaches that useword-representations for morphology?
How welldoes this method handle OOVs?
How does the im-pact of morphology analysis change with trainingdata size?
We provide both qualitative and quanti-tative answers for each of these questions next.4.1 Quality of Morphological AnalysisWe first evaluate the impact of our morphologi-cal analysis on a standard word-similarity ratingtask.
The task measures word-level understand-ing by comparing the correlation between human-produced similarity ratings for word pairs, e.g.
(in-traspecific, interspecies), with those produced by analgorithm.
For the experiments reported here, wetrain SkipGram models4using a dimensionality ofn = 500.
We denote a system using only Skip-Gram model embeddings as SG.
To evaluate the im-pact of our method, we perform morphological anal-ysis for words below a count threshold C. For aword w ?
DVCMorph, we simply use the SkipGramvector-representation; for a word w 6?
DVCMorph, weuse as word-representation its mapping in DVCMorph;we denote such a system SG+Morph.
For both SGand SG+Morph systems, we compute the similarityof word-pairs using the cosine distance between thevector-representations.DataWe train both the SG and SG+Morph models fromscratch, for all languages considered.
For English,4Additional settings include a window-size of 5 and negativesampling set to 5.
Unseen words receive a zero-vector embed-ding and a cosine score of 0.we use the Wikipedia data (Shaoul and Westbury,2010).
For German, French, and Spanish, we usethe monolingual data released as part of the WMT-2013 shared task (Bojar et al, 2013).
For Arabicwe use the Arabic GigaWord corpus (Parker et al,2011).
For Romanian and Uzbek, we use collectionsof News harvested from the web and cleaned (boiler-plate removed, formatting removed, encoding madeconsistent, etc.).
All SkipGram models are trainedusing a count cutoff of 5 (all words with count lessthan the cutoff are ignored).
Table 3 presents statis-tics on the data and vocabulary size, as well as thesize of the induced morphology graphs.
These num-bers illustrate the richness of the morphological phe-nomena present in languages such as German, Ro-manian, Arabic, and Uzbek, compared to English.As test sets, we use standard, publicly-availableword-similarity datasets.
Most relevant for our ap-proach is the Stanford English Rare-Word (RW)dataset (Luong et al, 2013), consisting of 2034word pairs with a higher degree of English morphol-ogy compared to other word-similarity datasets.
Wealso use for English the WS353 (Finkelstein et al,2002) and RG65 datasets (Rubenstein and Goode-nough, 1965).
For German, we use the Gur350 andZG222 datasets (Zesch and Gurevych, 2006).
ForFrench we use the RG65 French version (Joubarneand Inkpen, 2011); for Spanish, Romanian, and Ara-bic we use their respective versions of WS353 (Has-san and Mihalcea, 2009).ResultsWe present in Table 4 the results obtained across6 language pairs and 9 datasets, using a countthreshold for SG+Morph of C = 100.
We alsoinclude the results obtained by two previously-proposed methods, LSM2013 (Luong et al, 2013)and BB2014 (Botha and Blunsom, 2014), whichshare some of the characteristics of our method.Even in the absence of any morphological treat-ment, our word representations are better than pre-viously used ones.
For instance, LSM2013 usesexactly the same EN Wikipedia (Shaoul and West-bury, 2010) training data, and achieves 26.8 and 34.4Spearman ?
correlation on RW, with and withoutmorphological treatment, respectively.
The wordrepresentations we train yield a ?
of 35.8 for SG,and a ?
of 41.8 for SG+Morph (+7.4 improve-1633Spearman ?Language EN DE FR ES RO ARTestset RW WS RG Gur ZG RG WS WS WSSystemLSM2013 w/o morph 26.8 62.6 62.8 - - - - - -LSM2013 w/ morph 34.4 64.6 65.5 - - - - - -BB2014 w/o morph 18.0 32.0 47.0 36.0 6.0 33.0 26.0 - -BB2014 w/ morph 30.0 40.0 41.0 56.0 25.0 45.0 28.0 - -SG 35.8 71.2 75.1 62.4 16.6 63.6 36.5 51.7 37.1SG+Morph 41.8 71.2 75.1 64.1 21.5 67.3 47.3 53.1 43.1# pairs 2034 353 65 350 222 65 353 353 353Table 4: Performance of previously proposed methods, compared to SG and SG+Morph trained on Wiki1b.
LSM2013uses exactly the same training data for EN, whereas BB2014 uses the same training data for DE, FR, ES.ment under the morphology condition).
The mor-phological treatment used by LSM2013 also has asmall effect on the words present in the EnglishWS and RG sets; our method does not propose anyseparate morphological treatment for the words inthese datasets, since all of them have been observedmore than our C = 100 threshold in the trainingdata (therefore have reliable representations).
TheSG word-representations for all the other languages(German, French, Spanish, Romanian, and Arabic)also perform well on this task, with much higherSpearman scores obtained by SG compared with thepreviously-reported scores.The results in Table 4 also show that our mor-phology treatment provides consistent gains acrossall languages considered.
For morphologically-richlanguages, all datasets reflect the impact of mor-phology treatment.
We observe significant gains be-tween the performance of the SG and SG+Morphsystems, on top of the high correlation numbers ofthe SG system.
For German, the relatively smallincrease we observe is due to the fact the Germannoun-compounds are not covered by our morpholog-ical treatment.
For French, Spanish, Romanian, andArabic, the gains by the SG+Morph support the con-clusion that our method, while completely language-agnostic, handles well the variety of morphologicalphenomena present in these languages.4.2 Quality of Morphological Analysis forUnknown/Rare WordsIn this section, we quantify the accuracy of the mor-phological treatment for OOVs presented in Sec-tion 3.3.
We assume that the statistics for unseenwords (with respect to their morphological make-up) are similar with the statistics for low-frequencywords.
Therefore, for some relatively-low counts Land H , the set V[L,H)= VL?
VHis a good proxyfor the population of OOV words that we see at run-time.
We evaluate OOV morphology as follows:1.
Run the procedure for morphology inductionon VL, resulting in DVLMorph;2.
Run the procedure for morphology inductionon VH, resulting in DVHMorph;3.
Apply OOV morphology using DVHMorphforeach w ?
V[L,H]; evaluate resulting w ?
w?against reference w ?
w?reffrom DVLMorph, asnormal-form(w?)
?
normal-form(w?ref).To make the analysis more revealing, we split the en-tries in V[L,H)in two: type T1 entries are those thathave in-degree > 0 in DVLMorph(i.e., words that havea morphological mapping in the reference graph);type T2 entries are those that have 0 in-degree inDVLMorph(i.e., words with no morphological mappingin the reference, e.g., proper-nouns in English).
Notethat the T1/T2 distinction reflects a recall/precisiontrade-off: T1-words should be morphologically an-alyzed, while T2-words should not; a method thatover-analyses has poor performance on T2, whileone that under-analyses performs poorly on T1.We use the same datasets as the ones presentedin Section 4.1, see Table 3.
The results for all thelanguages are shown in Table 6, with all rows using1634EN (RW testset) DE (RG testset)|Unmapped| Spearman ?
|Unmapped| Spearman ?Wiki1b News120b Wiki1b News120b WMT2b News20b WMT2b News20bSG 80 177 35.8 44.7 0 20 62.4 62.1SG+Morph 1 0 41.8 52.0 0 0 64.1 69.1Table 5: Comparison between models SG and SG+Morph at different training-data sizes.|V[1000,2000)| AccuracyLang T1 T2 T1 T2EN 3421 10617 89.7% 89.6%DE 10778 21234 90.8% 93.1%FR 6435 9807 90.3% 90.4%ES 5724 7412 91.1% 90.3%RO 11905 9254 86.5% 85.3%AR 7913 5202 92.4% 69.0%UZ 11772 9027 81.3% 84.1%Table 6: Accuracy of Rare&OOV analysis.the same setup.
Count L = 1000 was chosen suchthat DVLMorphis reliable enough to be used as refer-ence.
The accuracy results are consistently high (inthe 80-90% range) for both T1- and T2-words, evenfor morphologically-rich languages such as Uzbek.These results indicate that our method does well atboth identifying a morphological analysis when ap-propriate, as well as not proposing one when not jus-tified, and therefore provides accurate morphologyanalysis for rare and OOV words.4.3 Morphology and Training Data SizeWe also evaluate the impact of our morphology anal-ysis under a regime with substantially more trainingdata.
To this end, we use large collections of En-glish and German News, harvested from the web andcleaned (boiler-plate removed, formatting removed,encoding made consistent).
Statistics regarding theresulting vocabularies and the induced morphologyare presented in Table 7 (vocabulary cutoffs of 400for EN and 50 for DE).
We present results usingthe word-similarity task using the same StanfordRare-Word (RW) dataset for EN and RG dataset forDE, compared against the setup using only 1-2 bil-lion training tokens.
For SG+Morph, we use countthresholds of 3000 for EN and 100 for DE.
The re-sults are given in Table 5.
For English, a 100x in-Lang |Tokens| |V | |GVMorph| |DVMorph|EN 120b 1.0m 2.9m 98,268DE 20b 1.8m 6.7m 351,980Table 7: Statistics for large training-data sizes.crease in the training data for EN brings a 10-pointincrease in Spearman ?
(from 35.8 to 44.7, and from41.8 to 52.0).
The morphological analysis providessubstantial gains at either level of training-data size:6 points in ?
for Wiki1b (from 35.8 to 41.8), and 7.3points for News120b EN (from 44.7 to 52.0).
ForGerman, the increase in training-data size does notbring visible improvements (perhaps due the highvocabulary cutoff), but the morphological treatmenthas a large impact under the large training-data con-dition (7 points for News20b DE, from 62.1 to 69.1).5 Conclusions and Future WorkWe have presented an unsupervised method for mor-phology induction.
The method derives a morpho-logical analyzer from scratch, and only requires amonolingual corpus for training, with no additionalknowledge of the language.
Our evaluation showsthat this method performs well across a large va-riety of language families, and we present here re-sults that improve on current state-of-the-art for themorphologically-rich Stanford Rare-word dataset.We acknowledge that certain languages exhibitphenomena (such as word-compounds in German)that require a more focused approach for solvingthem.
But techniques like the ones presented herehave the potential to exploit vector-based word rep-resentations successfully to address such phenom-ena as well.1635ReferencesAndrei Alexandrescu and Katrin Kirchhoff.
2006.
Fac-tored neural language models.
In Human LanguageTechnology Conference of the North American Chap-ter of the Association of Computational Linguistics.Mohit Bansal, Kevin Gimpel, and Karen Livescu.
2014.Tailoring continuous word representations for depen-dency parsing.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Linguis-tics, ACL 2014, June 22-27, Baltimore, MD, USA, Vol-ume 2: Short Papers, pages 809?815.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Research,3:1137?1155.Ond?rej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, Philipp Koehn,Christof Monz, Matt Post, Radu Soricut, and LuciaSpecia.
2013.
Findings of the 2013 Workshop onStatistical Machine Translation.
In Proceedings of theEighth Workshop on Statistical Machine Translation,pages 1?44, Sofia, Bulgaria, August.
Association forComputational Linguistics.Jan A. Botha and Phil Blunsom.
2014.
Compositionalmorphology for word representations and languagemodelling.
CoRR.Ronan Collobert.
2011.
Deep learning for efficient dis-criminative parsing.
In Proceedings of the FourteenthInternational Conference on Artificial Intelligence andStatistics, pages 224?232.Mathias Creutz and Krista Lagus.
2007.
Unsupervisedmodels for morpheme segmentation and morphologylearning.
TSLP, 4(1).Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and EytanRuppin.
2002.
Placing search in context: the conceptrevisited.
ACM Trans.
Inf.
Syst., 20(1):116?131.Xavier Glorot, Antoine Bordes, and Yoshua Bengio.2011.
Domain adaptation for large-scale sentimentclassification: A deep learning approach.
In Proceed-ings of the 28th International Conference on MachineLearning, pages 513?520.Samer Hassan and Rada Mihalcea.
2009.
Cross-lingualsemantic relatedness using encyclopedic knowledge.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing, pages1192?1201.Eric H. Huang, Richard Socher, Christopher D. Manning,and Andrew Y. Ng.
2012.
Improving word representa-tions via global context and multiple word prototypes.In Proceedings of the 50th Annual Meeting of the Asso-ciation for Computational Linguistics, pages 873?882.Fei Huang, Arun Ahuja, Doug Downey, Yi Yang, YuhongGuo, and Alexander Yates.
2014.
Learning repre-sentations for weakly supervised natural language pro-cessing tasks.
Computational Linguistics, 40(1):85?120.Colette Joubarne and Diana Inkpen.
2011.
Comparisonof semantic similarity for different languages using thegoogle n-gram corpus and second-order co-occurrencemeasures.
In Advances in Artificial Intelligence - 24thCanadian Conference on Artificial Intelligence, pages216?221.Angeliki Lazaridou, Marco Marelli, Roberto Zamparelli,and Marco Baroni.
2013.
Compositionally derivedrepresentations of morphologically complex words indistributional semantics.
In Proceedings of the 51stAnnual Meeting of the Association for ComputationalLinguistics, pages 1517?1526.Quoc V. Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
CoRR,abs/1405.4053.Omer Levy and Yoav Goldberg.
2014.
Dependency-based word embeddings.
In Proceedings of the 52ndAnnual Meeting of the Association for ComputationalLinguistics, ACL 2014, June 22-27, 2014, Baltimore,MD, USA, Volume 2: Short Papers, pages 302?308.Minh-Thang Luong, Richard Socher, and Christopher D.Manning.
2013.
Better word representations with re-cursive neural networks for morphology.
In CoNLL,Sofia, Bulgaria.Tomas Mikolov and Geoffrey Zweig.
2012.
Context de-pendent recurrent neural network language model.
InIEEE Spoken Language Technology Workshop (SLT),pages 234?239.Tomas Mikolov, Kai Chen, Greg Corrado, and Jeff Dean.2013a.
Efficient estimation of word representations invector space.
CoRR, abs/1301.3781.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In Human Language Technolo-gies: Conference of the North American Chapter of theAssociation of Computational Linguistics, pages 746?751.Andriy Mnih and Geoffrey E. Hinton.
2007.
Three newgraphical models for statistical language modelling.
InMachine Learning, Proceedings of the Twenty-FourthInternational Conference, pages 641?648.Andriy Mnih, Zhang Yuecheng, and Geoffrey E. Hin-ton.
2009.
Improving a statistical language modelthrough non-linear prediction.
Neurocomputing, 72(7-9):1414?1418.Robert Parker, David Graff, Ke Chen, Junbo Kong, andKazuaki Maedaet.
2011.
Arabic gigaword fifthedition ldc2011t11.
In Linguistic Data Consortium,Philadelphia.1636Jeffrey Pennington, Richard Socher, and Christopher D.Manning.
2014.
Glove: Global vectors for word rep-resentation.
In Proceedings of EMNLP.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Communicationsof the ACM, 8(10):627633.Cyrus Shaoul and Chris Westbury.
2010.
The Westburylab Wikipedia corpus.Richard Socher, Eric H. Huang, Jeffrey Pennington, An-drew Y. Ng, and Christopher D. Manning.
2011a.
Dy-namic pooling and unfolding recursive autoencodersfor paraphrase detection.
In 25th Annual Confer-ence on Neural Information Processing Systems, pages801?809.Richard Socher, Jeffrey Pennington, Eric H. Huang, An-drew Y. Ng, and Christopher D. Manning.
2011b.Semi-supervised recursive autoencoders for predictingsentiment distributions.
In Proceedings of the 2011Conference on Empirical Methods in Natural Lan-guage Processing, pages 151?161.Torsten Zesch and Iryna Gurevych.
2006.
Automaticallycreating datasets for measures of semantic relatedness.In Workshop on Linguistic Distances, pages 16?24.1637
