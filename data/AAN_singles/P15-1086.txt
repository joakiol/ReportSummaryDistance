Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 889?898,Beijing, China, July 26-31, 2015. c?2015 Association for Computational LinguisticsDeep Questions without Deep UnderstandingIgor Labutov Sumit Basu Lucy VanderwendeCornell University Microsoft Research Microsoft Research124 Hoy Road One Microsoft Way One Microsoft WayIthaca, NY Redmond, WA Redmond, WAiil4@cornell.edu sumitb@microsoft.com lucyv@microsoft.comAbstractWe develop an approach for generatingdeep (i.e, high-level) comprehensionquestions from novel text that bypassesthe myriad challenges of creating a full se-mantic representation.
We do this by de-composing the task into an ontology-crowd-relevance workflow, consisting offirst representing the original text in alow-dimensional ontology, then crowd-sourcing candidate question templatesaligned with that space, and finally rank-ing potentially relevant templates for anovel region of text.
If ontological labelsare not available, we infer them from thetext.
We demonstrate the effectiveness ofthis method on a corpus of articles fromWikipedia alongside human judgments,and find that we can generate relevantdeep questions with a precision of over85% while maintaining a recall of 70%.1 IntroductionQuestions are a fundamental tool for teachers inassessing the understanding of their students.Writing good questions, though, is hard work, andharder still when the questions need to be deep(i.e., high-level) rather than factoid-oriented.These deep questions are the sort of open-endedqueries that require deep thinking and recall ratherthan a rote response, that span significant amountsof content rather than a single sentence.
Unsur-prisingly, it is these deep questions that have thegreatest educational value (Anderson, 1975; An-dre, 1979; McMillan, 2001).
They are thus a keyassessment mechanism for a spectrum of onlineeducational options, from MOOCs to interactivetutoring systems.
As such, the problem of auto-matic question generation has long been of inter-est to the online education community (Mitkovand Ha, 2003; Schwartz, 2004), both as a meansof providing self-assessments directly to studentsand as a tool to help teachers with question author-ing.
Much work to date has focused on questionsbased on a single sentence of the text (Becker etal., 2012; Lindberg et al, 2013; Mazidi and Niel-sen, 2014), and the ideal of creating deep, concep-tual questions has remained elusive.
In this work,we hope to take a significant step towards thischallenge by approaching the problem in a some-what unconventional way.Figure 1: Overview of our ontology-crowd-rele-vance approach.While one might expect the natural path to gener-ating deep questions to involve first extracting asemantic representation of the entire text, thestate-of-the-art in this area is at too early a stageto achieve such a representation effectively.
Ra-ther we take a step back from full understanding,and instead propose an ontology-crowd-relevanceworkflow for generating high-level questions,shown in Figure 1.
This involves 1) decomposinga text into a meaningful, intermediate, low-dimen-sional ontology, 2) soliciting high-level templatesfrom the crowd, aligned with this intermediaterepresentation, and 3) for a target text segment, re-trieving a subset of the collected templates based889on its ontological categories and then rankingthese questions by estimating the relevance ofeach to the text at hand.In this work, we apply the proposed workflowto the Wikipedia corpus.
For our ontology, we usea Cartesian product of article categories (derivedfrom Freebase) and article section names (directlyfrom Wikipedia) as the intermediate representa-tion (e.g.
category: Person, section: Early life),henceforth referred to as category-section pairs.We use these pairs to prompt our crowd workersto create relevant templates; for instance, (Person,Early Life) might lead a worker to generate thequestion ?Who were the key influences on <Per-son> in their childhood?
?, a good example of thesort of deep question that can?t be answered froma single sentence in the article.
We also developclassifiers for inferring these categories when ex-plicit or matching labels are not available.
Givena database of such category-section-specific ques-tion templates, we then train a binary classifierthat can estimate the relevance of each to a newdocument.
We hypothesize that the resultingranked questions will be both high-level and rele-vant, without requiring full machine understand-ing of the text ?
in other words, deep questionswithout deep understanding.In the sections that follow, we detail the variouscomponents of this method and describe the ex-periments showing their efficacy at generatinghigh-quality questions.
We begin by motivatingour choice of ontology and demonstrating its cov-erage properties (Section 3).
We then describe ourcrowdsourcing methodology for soliciting ques-tions and question-article relevance judgments(Section 4), and outline our model for determiningthe relevance of these questions to new text (Sec-tion 5).
After this we describe the two datasets thatwe construct for the evaluation of our approachand present quantitative results (Section 6) as wellas examples of our output and an error analysis(Section 7) before concluding (Section 8).2 Related WorkWe consider three aspects of past research in au-tomatic question generation: work that focuses onthe grammaticality of natural language questiongeneration, work that focuses on the semanticquality of generated questions, i.e.
the ?what toask about?
rather than ?how to ask it,?
and finallywork that builds a semantic representation of textin order to generate higher-level questions.Approaches focusing on the grammaticality ofquestion generation date back to the AU-TOQUEST system (Wolfe, 1976), which exam-ined the generation of Wh-questions from singlesentences.
Later systems addressing the same goalinclude methods that use transformation rules(Mitkov and Ha, 2003), template-based genera-tion (Chen et al, 2009; Curto et al, 2011) andovergenerate-and-rank methods (Heilman andSmith, 2010a).
Another approach has been to cre-ate fill-in-the-blank questions from single sen-tences to ensure grammaticality (Agarwal et al2011, Becker et al 2012).More relevant to our direction is work on thesemantic aspect of question generation, which hasbecome a more active research area in the pastseveral years.
Several authors (Mazidi and Niel-sen 2014; Linberg et al 2013) generate questionsaccording to the semantic role patterns extractedfrom the source sentence.
Becker et al (2012) alsoleverage semantic role labeling within a sentencein a supervised setting.
We hope to continue inthis direction of semantic focus, but extend the ca-pabilities of question generation to include open-ended questions that go far beyond the scope of asingle sentence.Other work has taken on the challenge ofdeeper questions by attempting to build a seman-tic representation of arbitrary text.
This has in-cluded work using concept maps over keywords(Olney et al 2012) and minimal recursion seman-tics (Yao 2010) to reason over concepts in the text.While the work of (Olney et al 2012) is impres-sive in its possibilities, the range of the types ofquestions that can be generated is restricted by arelatively specific set of relations (e.g.
Is-A, Part-Of) captured in the ontology of the domain (biol-ogy textbook).
Mannem et al (2010) observe aswe have that "capturing the exact true meaning ofa paragraph is beyond the reach of current NLPsystems;" thus, in their system for Shared Task A(for paragraph-level questions (Rus et al 2010))they make use of predicate argument structuresalong with semantic role labeling.
However, thegeneration of these questions is restricted to thefirst sentence of the paragraph.
Though motivatedby the same noble impulses of these authors toachieve higher-level questions, our hope is that wecan bypass the challenges and constraints of se-mantic parsing and generate deep questions via amore holistic approach.8903 An Ontology of Categories and Sec-tionsThe key insight of our approach is that we can lev-erage an easily interpretable (for crowd workers),low-dimensional ontology for text segments in or-der to crowdsource a set of high-level, reusabletemplates that generalize well to many docu-ments.
The choice of this representation muststrike a balance between domain coverage and thecrowdsourcing effort required to obtain that cov-erage.
Inasmuch as Wikipedia is deemed to havebroad coverage of human knowledge, we can es-timate domain coverage by measuring what frac-tion of that corpus is covered by the proposed rep-resentation.
In our work, we have developed a cat-egory-section ontology using annotations fromFreebase and Wikipedia (English), and now de-scribe its structure and coverage in detail.For the high-level categories, we make use ofthe Freebase ?notable type?
for each Wikipediaarticle.
In contrast to the noisy default Wikipediacategories, the Freebase ?notable types?
provide aclean high-level encapsulation of the topic or en-tity discussed in a Wikipedia article.
As we wishto maximize coverage, we compute the histogramby type and take the 300 most common onesacross Wikipedia.
We further merge these intoeight broad categories to reduce crowdsourcingeffort: Person, Location, Event, Organization,Art, Science, Health, and Religion.
These eightcategories cover 78% of Wikipedia articles (seeFigure 2a); the mapping between Freebase typesand our categories will be made available as partof our corpus (see Section 8).To achieve greater specificity of questionswithin the articles, we make use of Wikipedia sec-tions, which offer a high-level segmentation of thecontent.
The Cartesian product of our categoriesfrom above and the most common Wikipedia sec-tion titles (per category) then yield an interpreta-ble, low-dimensional representation of the article.For instance, the set of category-section pairs foran article about Albert Einstein contains (Person,Early_life), (Person, Awards), and (Person, Polit-ical_views) as well as several others.For each category, the section titles that occurmost frequently represent central themes in arti-cles belonging to that category.
We therefore hy-pothesize that question templates authored forsuch high-coverage titles are likely to generalizeto a large number of articles in that category.
Ta-ble 1 below shows the four most frequent sectionsfor each of our eight categories.Person Location Organiza-tionArtEarly life History History PlotCareer Geography Geography ReceptionPers.
life Economy Academics HistoryBiography Demo-graphicsDemo-graphicsProductionScience Event Health ReligionDescript.
Background Treatment EtymologyTaxonomy Aftermath Diagnosis IcongraphyHistory Battle Causes WorshipDistributn.
Prelude History MythologyTable 1: Most frequent section titles by category.As the crowdsourcing effort is directly propor-tional to the size of the ontology, our goal is toselect the smallest set of pairs that will providesufficient coverage.
As with categories, the cut-Figure 2: Coverage properties of our category-section representation: (a) fraction of  Wikipediaarticles covered by the top j most common Freebase types, grouped by our eight higher-levelcategories.
(b) Average fraction of sections covered per document if only the top k most frequentsections are used; each line represents one of our eight categories.891off for the number of sections used for each cate-gory is guided by the trade-off between coverageand crowdsourcing costs.
Figure 2b plots the av-erage fraction of an article covered by the top ksections from each category.
We found that thetop 50 sections cover 30% to 55% of the sectionsof an individual article (on average) across ourcategories.
This implies that by only crowdsourc-ing question templates for those 50 sections percategory, we would be able to ask questions abouta third to a half of the sections of any article.Of course, if we were to limit ourselves to onlysegments with these labels at runtime, we wouldcompletely miss many articles as well as texts out-side of Wikipedia.
To extend our reach, we alsodevelop the means for category and section infer-ence from raw text in Section 5 below, for casesin which ontological labels are either not availableor are not contained within our limited set.4 Crowdsourcing MethodologyWe designed a two-stage crowdsourcing pipelineto 1) collect templates targeted to a set of cate-gory-section pairs and 2) obtain binary relevancejudgments for the generated templates in relationto a set of article segments (for Wikipedia, theseare simply sections) that match in category-sec-tion labels.
We recruit Mechanical Turk workersfor both stages of the pipeline, filtering for work-ers from the United States due to native Englishproficiency.
A total of 307 unique workers partic-ipated in the two tasks combined (78 and 229workers for the generation and ratings tasks re-spectively).Figure 3: Prompt for the generation task for thecategory-section pair (Person, Legacy).4.1 Question generation taskFollowing the coverage analysis above, we selectthe 50 most frequent sections for the top two cat-egories, Person and Location, yielding 100 cate-gory-section pairs.
As these two categories covernearly 50% of all articles on Wikipedia, we be-lieve that they suffice in demonstrating the effec-tiveness of the proposed methodology.
For eachcategory-section pair, we instructed 10 (median)workers to generate a question regarding a hypo-thetical entity belonging to the target with theprompt in Figure 3.
Additional instructions and aninteractive tutorial were pre-administered, guid-ing the workers to formulate appropriately deepquestions, i.e.
questions that are likely to general-ize to many articles, while avoiding factoid ques-tions like ?When was X born?
?In total, 995 question templates were added toour question database using this methodology(only 0.5% of all generated questions were exactrepeats of existing questions).
We confirm in sec-tion 4.2 that workers were able to formulate deep,interesting and relevant questions whose answersspanned more than a single sentence and that gen-eralized to many articles using this prompt.In earlier pilots, we tried an alternative promptwhich also presented the text of a specific articlesegment.
In Figure 4, we show the average scopeand relevance of questions generated by workersunder both prompt conditions.
As the figuredemonstrates, the alternative prompt showingspecific article text resulted in questions that gen-eralized less well (workers?
questions were foundto be relevant to fewer articles), likely because thedetails in the text distracted the workers fromthinking broadly about the domain.
These ques-tions also had a smaller scope on average, i.e., an-swers to these questions were contained in shorterspans in the text.
The differences in scope and rel-evance between the two prompt designs were bothsignificant (p-values: 0.006 and 4.5e-11 respec-tively, via two-sided Welch?s t-tests).Figure 4: Average relevance and scope ofworker-generated questions versus how theworkers were prompted.8924.2 Question relevance rating taskFor our 100 category-section pairs, 4 (median) ar-ticle segments within reasonable length for a Me-chanical Turk task (200-1000 tokens) were drawnat random from the Wikipedia corpus; this re-sulted in a set of 513 article segments.
Eachworker was then presented with one of these seg-ments alongside at most 10 questions from thequestion template database matching in category-section; templates were converted into questionsby filling in the article-specific entity extractedfrom the title.
Workers were requested to rate eachquestion along three dimensions: relevance, qual-ity, and scope, as detailed below.
Quality andscope ratings were only requested when theworker determined the question to be relevant.?
Relevance: 1 (not relevant) ?
4 (relevant)Does the article answer the question??
Quality: 1 (poor) ?
4 (excellent)Is this question well-written??
Scope: 1 (single-sentence) ?
4 (multi-sen-tence/paragraph)How long is the answer to this question?A median of 3 raters provided an independentjudgment for each question-article pair.
The meanrelevance, quality and scope ratings across the 995questions were 2.3 (sd=0.83), 3.5 (sd=.65) and 2.6(sd=1.0) respectively.
Note that the sample sizesfor scope and quality were smaller, 774 and 778respectively, as quality/scope judgments were notgathered for questions deemed irrelevant.
We notethat 80% of the relevant crowd-sourced questionshad a median scope rating larger than 1 sentence,and 23% had a median scope rating of 4, definedas ?the answer to this question can be found inmany sentences and paragraphs,?
correspondingto the maximum attainable scope rating.
Note thatwhile in this work, we have only used the scopejudgments to report summary statistics about thegenerated questions, in future work these ratingscould be used to build a scope classifier to filterout questions targeting short spans of text.As described in Section 5.2, the relevance judg-ments are converted to binary relevance ratingsfor training the relevance classifier (we considerrelevance ratings {1, 2} as ?not relevant?
and {3,4} as ?relevant?).
In terms of agreement betweenraters for these binary relevance labels, we ob-tained a Fleiss?
Kappa of 0.33, indicating fairagreement.5 ModelThere are two key models to our system: the firstis for category and section inference of a novel ar-ticle segment, which allows us to infer the keys toour question database when explicit labels are notavailable.
The second is for question relevanceprediction, which lets us decide which questiontemplates from the database?s store for that cate-gory-section actually apply to the text at hand.5.1 Category/section inferenceBoth category and section inference were cast asstandard text-classification problems.
Categoryinference is performed on the whole article, whilesection inference is performed on the individualarticle segments (i.e., sections).
We trained indi-vidual logistic regression classifiers for the eightcategories and the 50 top section types for eachone (a total of 400) using the default L2 regulari-zation parameter in LIBLINEAR (Fan, 2008).
Forsection inference, a total of 736,947 article seg-ments were sampled from Wikipedia (June 2014snapshot), each belonging to one of the 400 sec-tion types and within the same length bounds fromSection 4.2 (200-1000 tokens).
For category infer-ence, we sampled a total of 86,348 articles with atleast 10 sentences and belonging to one of oureight categories.In both cases, a binary dataset was constructedfor a one-against-all evaluation, where the nega-tive instances were sampled randomly from thenegative categories or sections (there was an av-erage 17% and 32% positive skew in the sectionand category datasets, respectively).
Basic tf-idffeatures (using a vocabulary of 200,000 aftereliminating stopwords) were used in both textclassification tasks.
Applying the category/sectioninference to held-out portions of the dataset (30%for each category/section) resulted in balanced ac-curacies of 83%/95% respectively, which gave usconfidence in the inference.
Keep in mind that thisis not a strict bound on our question generationperformance, since the inferred category/section,while not matching the label perfectly, could stillbe sufficiently close to produce relevant questions(for instance, we could misrecognize ?Childhood?as ?Early Life?).
We explore the ramifications ofthis in our end-to-end experiments in Section 6.5.2 Relevance ClassificationWe also cast the problem of question/article rele-vance prediction as one of binary classification,where we map a question-article pair to a rele-vance score; as such our features had to combine893aspects of both the question and the article.
Ourcore approach was to use a vector of the compo-nent-wise Euclidean distances between individualfeatures of the question and article segment, i.e.,the ith feature vector component ??
is given by??
= (??
?
??
)2, where ??
and ??
are the compo-nents of the question and article feature vectors.For the feature representation, we utilized a con-catenation of continuous embedding features: 300features from a Word2Vec embedding (Mikolov,2013) and 200,000 tfidf features (as with cate-gory/section classification above).As question templates are typically short,though, we found that this representation aloneperformed poorly.
As a result, we augmented thevector by concatenating additional distance fea-tures between the target article segment and onespecific instance of an entire article for which thequestion applied.
This augmenting article was se-lected at random from all those for which the tem-plate was judged to be relevant.
The resulting fea-ture vector was thus doubled in length, where thefirst ?
distances were between the question tem-plate and the target segment, and the next ?
werebetween the augmenting article and the target seg-ment.
Note that the augmenting article segmentswere removed from the training/test sets.To train this classifier, we assumed that wewould be able to acquire at least ?
positive rele-vance labels for each question template, i.e., ?
ar-ticle segments judged to be relevant to each tem-plate for inclusion in the training set.
We explorethe effect of increasing values of ?, from 0 (whereno relevance labels are available) to 3 (referred toas conditions T0..T3 in Figure 5).
We then trainedand evaluated the relevance classifier, a single lo-gistic regression model using LIBLINEAR withdefault L2 regularization, using 10-fold cross-val-idation on DATASET I (see Section 6).Figure 5 depicts a series of ROC curves sum-marizing the performance of our template rele-vance classifier on unseen article segments.
Asexpected, we see increasing performance with in-creasing ?.
However, the benefit drops off after 3instances (i.e., T4 is only marginally better thanT3).
While the character of the curves is modest,keep in mind we are already filtering questions byretrieving them from the database for the inferredcategory-section (which by itself gives us a preci-sion of .74 ?
see green bars in Figure 6); this ROCrepresents the ?lift?
achieved by further filteringthe questions with our relevance classifier, result-ing in far higher precision (.85 to .95 ?
see bluebars in Figure 6).Figure 5: ROC curves for the task of question-to-article relevance prediction.
Tn means that n pos-itively labeled article segments were availablefor each question template during training.6 Experiments and ResultsIn this section, we describe the datasets used fortraining the relevance classifier in Section 5.2(DATASET I) as well as for end-to-end perfor-mance on unlabeled text segments (DATASET II).We then evaluate the performance on this seconddataset under three settings: first, when the cate-gory and section are known, second, when thoselabels are unavailable, and third, when neither thelabels nor the relevance classifier are available.6.1 DATASET I: for the Relevance ClassifierThe first dataset (DATASET I) was intended fortraining and evaluating the relevance classifier,and for this we assumed the category and sectionlabels were known.
As such, judgments were col-lected only for questions templates authored for agiven article?s actual category and section labels.After filtering out annotations from unreliableworkers (based on their pre-test results) as well asthose with inter-annotator agreement below 60%,we were left with a set of 995 rated questions,spanning across two categories (Person and Loca-tion) and 50 sections per category (100 category-section pairs total).
This corresponded to a total of4439 relevance tuples (label, question, article)where label is a binary relevance rating aggre-gated via majority vote across multiple raters.
Therelevance labels were skewed towards the positive(relevant) class with 63% relevant instances.This is of course a mostly unrealistic data set-ting for applications of question generation(known category and section labels), but greatly894useful in developing and evaluating the relevanceclassifier; we thus used this dataset only for thatpurpose (see Section 5.2 and Figure 5).6.2 DATASET II: for End-to-End EvaluationFor an end-to-end evaluation we need to examinesituations where the category and section labelsare not available and we must rely on inferenceinstead.
As this is the more typical use case for ourmethod, it is critical to understand how the perfor-mance will be affected.
For DATASET II, then, wefirst sampled articles from the Wikipedia corpusat random (satisfying the constraints described inSection 3) and then performed category and sec-tion inference on the article segments.
The cate-gory c with the highest posterior probability waschosen as the inferred category, while all sectiontypes ??
with a posterior probability greater than0.6 were considered as sources for templates.Only articles whose inferred category was Personor Location were considered, but given the noisein inference there was no guarantee that the truelabels were of these categories.
We continued thisprocess until we retrieved a total of 12 articles.
Foreach article segment in these 12, we drew a ran-dom subset of at most 20 question templates fromour database matching the inferred category andsection(s), then ordered them by their estimatedrelevance for presentation to judges.We then solicited an additional 62 MechanicalTurk workers to a rating task set up according tothe same protocol as for DATASET I.
After aggre-gation and filtering in the same way, the seconddataset contained a total 256 (label, question, ar-ticle) relevance tuples, skewed towards the posi-tive class with 72% relevant instances.6.3 Information Retrieval?based EvaluationAs our end-to-end task is framed as the retrievalof a set of relevant questions for a given articlesegment, we can measure performance in terms ofan information retrieval-based metric.
Consider auser who supplies an article segment (the ?query?in IR terms) for which she wants to generate aquiz: the system then presents a ranked list of re-trieved questions, ordered according to their esti-mated relevance to the article.
As she makes herway down this ranked list of questions, adding aquestion at a time to the quiz (set Q), the behaviorof the precision and recall (with respect to rele-vance to the article segment) of the questions inQ, summarizes the performance of the retrievalsystem (i.e.
the Precision-Recall (PR) curve(Manning, 2008)).
We summarize the perfor-mance of our system by averaging the individualarticle segments?
PR curves (linearly interpolated)from DATASET II, and present the average preci-sion over bins of recall values in Figure 6.
Weconsider the following experimental conditions:?
Known category/section, using relevanceclassifier (red): This is the case in which theactual category and section labels of the queryarticle are known, and only the questions thatmatch exactly in category and section are con-sidered for relevance classification (i.e.
addedto Q if found relevant by the classifier).
Recallis computed with respect to the total numberof relevant questions in DATASET II, includingthose corresponding to sections different fromthe section label of the article.?
Inferred category/section, using relevanceclassifier (blue): This is the expected usecase, where the category/section labels are notknown.
Questions matching in category andsection(s) to the inferred category and sectionof each article are considered and ranked in Qby their score from the relevance classifier.Recall is computed with respect to the totalnumber of relevant questions in DATASET II.?
Inferred category/section, ignoring rele-vance classifier (green): This is a baselinewhere we only use category/section inferenceand then retrieve questions from the databasewithout filtering: all questions that match ininferred category and section(s) of the articleare added to Q in a random ranking order,without performing relevance classification.As we examine Figure 6, it is important to pointout a subtlety in our choice to calculate recall ofthe known category/section condition (red bars)with respect to the set of all relevant questions,including those that are matched to sections dif-ferent from the original (labeled) sections.
Whilethis condition by construction does not have ac-cess to questions of any other section, the result-ing limitation in recall underlines the importanceof performing section inference: without infer-ence, we achieve a recall of no greater than 0.4.As we had hypothesized, while the labels of thesections play an instrumental role in instructingthe crowd to generate relevant questions, the re-sulting questions often tend to be relevant to con-tent found under different but semantically relatedsections as well.
Leveraging the available ques-tions of these related sections (by performing in-ference) boosts recall at the expense of only asmall degree of precision (blue bars).
If we forgorelevance classification entirely, we get a constantprecision of 0.74 (green bars) as mentioned in895Section 5.2; it is clear that the relevance classifierresults in a significant advantage.While there is a slight drop in precision whenusing inference, this is at least partly due to theconstraints that were imposed during data-collec-tion and relevance classifier training, i.e., all pairsof articles and questions belonged to the same cat-egory and section.
While this constraint made thecrowdsourcing methodology proposed in thiswork tractable, it also prevented the inclusion oftraining examples for sections that could poten-tially be inferred at test time.
One possible ap-proach to remedy this would be sample from arti-cle segments that are similar in text (in terms ofour distance metric) as opposed to only segmentsexactly matching in category and section.Figure 6: Precision-recall results for the end-to-end experiment, grouped in bins of recall ranges.7 Examples and Error AnalysisIn Table 2 we show a set of sample retrieved ques-tions and the corresponding correctness of the rel-evance classifier?s decision with respect to thejudgment labels; examining the errors yields someinteresting insights.
Consider the false positiveexample shown in row 8, where the category cor-rectly inferred as Location, but section title wasinferred as Transportation instead of Services.This mismatch resulted in the following templateauthored for (Location, Transportation) being re-trieved: "What geographic factors influence thepreferred transport methods in <entity>?"
To therelevance classifier, this particular template (con-taining the word ?transport?)
appears to be rele-vant on the surface level to the text of an articlesegment about schedules (Services) at a railwaystation.
However, as this template never appearedto judges in the context of a Services segment ?
asection that differs considerably in theme from theinferred section (Transportation) ?
the relevanceclassifier unsurprisingly makes the wrong call.TruesectionInferredsectionRe-sultGeneratedQuestionHon-oursLaterLifeTPWhat accomplishmentscharacterized the later ca-reer of Colin Cowdrey?ActingCareerTelevi-sionTPHow did Corbin Bern-stein?s television careerevolve over time?RouteDe-scrip-tionGeogra-phyTPWhat are some unique ge-ographic features ofPuerto Rico Highway 10?Athlet-icsAthletics TNHow much significance dopeople of DeMartha Cath-olic High School place onathletics?RouteDe-scrip-tionGeogra-phyTNHow does the geographyof Puerto Rico Highway10 impact its resources?WorkRecep-tionFNWhat type of reaction didThornton Dial receive?ActingCareerLaterCareerFPWhat were the most im-portant events in the latercareer of Corbin Berstein?Ser-vicesTranspor-tationFPWhat geographic factorsinfluence the preferredtransport methods in Wey-mouth Railway Station?LaterCareerLegacy FPHow has Freddy Mitch-ell?s legacy shaped currentevents?Table 2: Examples of retrieved questions.
TP, TN,FP, FN stand for true/false positive/negative withrespect to the relevance classification.In considering additional sources of relevanceclassification errors, recall that we employ a sin-gle relevant article segment for the purpose ofaugmenting a template?s feature representation.
Inthe case of the false negative example (row 6 inTable 2), the sensitivity of the classifier to the par-ticular augmenting article used is apparent.
Uponinspecting the target article segment (article:Thornton Dial, section: Work), and the augment-ing article segment (article: Syed Masood, section:Reception), it?s clear that the inferred section Re-ception is a reasonable title for the Work sectionof the article on Thornton Dial, making the ques-tion ?What type of reaction did Thornton Dial re-ceive??
a relevant question to the target article (asreflected in the human judgment).
However, alt-hough both segments generally talk about ?recep-tion,?
the language across the two segments is dis-tinct: the critical reception of Thornton Dial thevisual artist is described in a different way fromthe reception of Syed Masood the actor, resultingin little overlap in surface text, and as a result therelevance classifier falsely rejects the question.896Reasonable substitutions for inferred sectionscan also lead to false positives, as in row 9, for thearticle Freddy Mitchell.
In this case, while Legacy(the inferred section) is a believable substitute forthe true label of Later Career, in this case the ar-ticle segment did not discuss his legacy.
However,there was a good match between the augmentingarticle for this template and the section.
We hy-pothesize that in both this and the previous exam-ples a broader sample of augmenting article seg-ments for each category/section is likely to be ef-fective at mitigating these types of errors.8 ConclusionWe have presented an approach for generating rel-evant, deep questions that are broad in scope andapply to a wide range of documents, all withoutconstructing a detailed semantic representation ofthe text.
Our three primary contributions are 1)our insight that a low-dimensional ontologicaldocument representation can be used as an inter-mediary for retrieving and generalizing high-levelquestion templates to new documents, 2) an effi-cient crowdsourcing scheme for soliciting suchtemplates and relevance judgments (of templatesto article) from the crowd in order to train a rele-vance classification model, and 3) using cate-gory/section inference and relevance prediction toretrieve and rank relevant deep questions for newtext segments.
Note that the approach and work-flow presented here constitute a general frame-work that could potentially be useful in other lan-guage generation applications.
For example, asimilar setup could be used for high-level summa-rization, where question templates would be re-placed with ?summary snippets.
?Finally, to encourage the community to furtherexplore this approach as well as to compare it withothers, we are releasing all of our data (categorymappings, generated templates, and relevancejudgments) at http://research.microsoft.com/~su-mitb/questiongeneration .ReferencesManish Agarwal, Rakshit Shah, and Prashanth Man-nem.
2011.
Automatic Question Generation UsingDiscourse Cues.
In Proceedings of the 6th Work-shop on Innovative Use of NLP for Building Educa-tional Applications.Richard C. Anderson and W. Barry Biddle.
1975.
OnAsking People Questions About What they areReading.
Psychology of Learning and Motivation.9:90-132.Thomas Andre.
1979.
Does Answering Higher-levelQuestions while Reading Facilitate ProductiveLearning?
Review of Educational Research 49(2):280-318.Lee Becker, Sumit Basu, and Lucy Vanderwende.2012.
Mind the Gap: Learning to Choose Gaps forQuestion Generation.
In Proceedings of the 2012Conference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies.Wei Chen, Gregory Aist, and Jack Mostow.
2009.
Gen-erating Questions Automatically from InformationalText.
In S. Craig & S. Dicheva (Ed.
), Proceedingsof the 2nd Workshop on Question Generation.S?rgio Curto, Ana Cristina Mendes, and Luisa Coheur.2011.
Exploring Linguistically-rich Patterns forQuestion Generation.
In Proceedings of theUCNLG+Eval: Language Generation and Evalua-tion Workshop.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, Chih-Jen Lin.
2008.
LIBLINEAR: A li-brary for large linear classification.
Journal of Ma-chine Learning Research 9: 1871-1874.Michael Heilman and Noah Smith.
2010.
Good Ques-tion!
Statistical Ranking for Question Generation.
InProceedings of NAACL/HLT.David Lindberg, Fred Popowich, John Nesbit, and PhilWinne.
2013.
Generating Natural Language Ques-tions to Support Learning On-line.
In Proceedingsof the 14th European Workshop on Natural Lan-guage Generation.Prashanth Mannem, Rashmi Prasad, and AravindJoshi.
2010.
Question generation from paragraphs atUPenn: QGSTEC system description.
In Proceed-ings of the Third Workshop on Question Generation.Christopher D. Manning, Prabhakar Raghavan, andHinrich Schutze.
2008.
Introduction to InformationRetrieval.
Cambridge: Cambridge university pressKaren Mazidi and Rodney D. Nielsen.
2014.
LinguisticConsiderations in Automatic Question Generation.In Proceedings of ACL.James H. McMillan.
2001.
Secondary Teachers' Class-room Assessment and Grading Practices."
Educa-tional Measurement: Issues and Practice 20(1): 20-32.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S.Corrado, and Jeff Dean.
2013.
Distributed Repre-sentations of Words and Phrases and their Compo-sitionality.
In Proceedings of Advances in NeuralInformation Processing Systems.Ruslan Mitkov and Le An Ha.
2003.
Computer-AidedGeneration of Multiple-Choice Tests.
In Proceed-897ings of the HLT-NAACL 2003 Workshop on Build-ing Educational Applications Using Natural Lan-guage Processing.Andrew M. Olney, Arthur C. Graesser, and Natalie K.Person.
2012.
Question Generation from ConceptMaps.
Dialogue & Discourse 3(2): 75-99.Daniel Ramage, David Hall, Ramesh Nallapati, andChristopher D. Manning.
2009.
Labeled LDA: ASupervised Topic Model for Credit Attribution inMulti-labeled Corpora.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing.Vasile Rus, Brendan Wyse, Paul Piwek, Mihai Lintean,Svetlana Stoyanchev, and Cristian Moldovan.
2010.Overview of The First Question Generation SharedTask Evaluation Challenge.
In Proceedings of theThird Workshop on Question Generation.Lee Schwartz, Takako Aikawa, and Michel Pahud.2004.
Dynamic Language Learning Tools.
In Pro-ceedings of STIL/ICALL Symposium on ComputerAssisted Learning.John H. Wolfe.
1976.
Automatic Question Generationfrom Text - an Aid to Independent Study.
In Pro-ceedings of ACM SIGCSE-SIGCUE Joint Sympo-sium on Computer Science Education.Xuchen Yao and Yi Zhang.
2010.
Question generationwith minimal recursion semantics.
In Proceedingsof QG2010: The Third Workshop on Question Gen-eration.898
