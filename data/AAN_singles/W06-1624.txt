Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 199?207,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Weakly Supervised Learning Approachfor Spoken Language UnderstandingWei-Lin Wu, Ru-Zhan Lu, Jian-Yong Duan,Hui Liu, Feng Gao, Yu-Quan ChenDepartment of Computer Science and EngineeringShanghai Jiao Tong UniversityShanghai, 200030, P. R. China{wu-wl,lu-rz,duan-jy,liuhui,gaofeng,chen-yq}@cs.sjtu.edu.cnAbstractIn this paper, we present a weakly super-vised learning approach for spoken lan-guage understanding in domain-specificdialogue systems.
We model the task ofspoken language understanding as a suc-cessive classification problem.
The firstclassifier (topic classifier) is used to iden-tify the topic of an input utterance.
Withthe restriction of the recognized targettopic, the second classifier (semanticclassifier) is trained to extract the corre-sponding slot-value pairs.
It is mainlydata-driven and requires only minimallyannotated corpus for training whilst re-taining the understanding robustness anddeepness for spoken language.
Most im-portantly, it allows the employment ofweakly supervised strategies for trainingthe two classifiers.
We first apply thetraining strategy of combining activelearning and self-training (Tur et al,2005) for topic classifier.
Also, we pro-pose a practical method for bootstrappingthe topic-dependent semantic classifiersfrom a small amount of labeled sentences.Experiments have been conducted in thecontext of Chinese public transportationinformation inquiry domain.
The experi-mental results demonstrate the effective-ness of our proposed SLU frameworkand show the possibility to reduce humanlabeling efforts significantly.1 IntroductionSpoken Language Understanding (SLU) is one ofthe key components in spoken dialogue systems.Its task is to identify the user?s goal and extractfrom the input utterance the information neededto complete the query.
Traditionally, there aremainly two mainstreams in the SLU researches:knowledge-based approaches, which are basedon robust parsing or template matching tech-niques (Sneff, 1992; Dowding et al, 1993; Wardand Issar, 1994); and data-driven approaches,which are generally based on stochastic models(Pieraccini and Levin, 1993; Miller et al, 1995).Both approaches have their drawbacks, however.The former approach is cost-expensive to de-velop since its grammar development is time-consuming, laboursome and requires linguisticskills.
It is also strictly domain-dependent andhence difficult to be adapted to new domains.
Onthe other hand, although addressing such draw-backs associated with knowledge-based ap-proaches, the latter approach often suffers thedata sparseness problem and hence needs a fullyannotated corpus in order to reliably estimate anaccurate model.
More recently, some new varia-tion methods are proposed through certain trade-offs, such as the semi-automatically grammarlearning approach (Wang and Acero, 2001) andHidden Vector State (HVS) model (He andYoung, 2005).
The two methods require onlyminimally annotated data (only the semanticframes are annotated).This paper proposes a novel weakly super-vised spoken language understanding approach.Our SLU framework mainly includes two suc-cessive classifiers: topic classifier and semanticclassifier.
The main advantage of the proposedapproach is that it is mainly data-driven and re-quires only minimally annotated corpus for train-ing whilst retaining the understanding robustnessand deepness for spoken language.
In particular,the two classifiers are trained using weakly su-pervised strategies: the former one is trainedthrough the combination of active learning andself-training (Tur et al, 2005), and the latter one199is trained using a practical bootstrapping tech-nique.2 The System ArchitectureThe semantic representation of an applicationdomain is usually defined in terms of thesemantic frame, which contains a frame typerepresenting the topic of the input sentence, andsome slots representing the constraints the querygoal has to satisfy.
Then, the goal of the SLUsystem is to translate an input utterance into asemantic frame.
Besides the two key components,i.e., topic classifier and semantic classifier, oursystem also contains a preprocessor and a slot-value merger.
Figure 1 illustrates the overallsystem architecture.
It also describes the wholeSLU procedure using an example sentence.PreprocessorPlease tell me how can Igo from the people'ssquare to the bund by busTopicclassificationSemanticclassificationSlot-value mergerPlease tell me how canI go from [location]1 to[location]2 by [bus]Please tell me how canI go from [location]1 to[location]2 by [bus]FRAME:  ShowRouteFRAME:  ShowRoute[location]1:  ShowRoute.[route].
[origin][location]2:  ShowRoute.[route].
[destination][bus]: ShowRoute.[route].
[transport_type]FRAME: ShowRouteSLOTS: [route].
[origin] = the people's square[route].
[destination] = the bund[route].
[transport_type] = busInconsistentslot-valuesFigure 1: The System architecture12.1 The PreprocessorUsually, the preprocessor is to look for the sub-strings in a sentence that correspond to a seman-tic class or matching a regular expression and toreplace them with the class label, e.g., ?HuashanRoad?
and ?1954?
are replaced with two classlabels [road_name] and [number] respectively.
Inour system, the preprocessor can recognize morecomplex word sequences, e.g., ?1954 HuashanRoad?
can be recognized as [address] throughmatching a rule like ?
[address] ?
[number][road_name]?.
The preprocessor is implementedwith a local chart parser, which is a variation ofthe robust parser introduced in (Wang, 1999).The robust local parser can skip noise words inthe sentence, which ensures that the system hasthe low level robustness.
For example, ?1954 ofthe Huashan Road)?
can also be recognized as1 Because the length is limited, in this paper we only illus-trate all the example sentences in English, which are Chi-nese sentences, in fact.
[address] by skipping the words ?of the?.
How-ever, the robust local parser possibly skips thewords in the sentence by mistake and producesan incorrect class label.
To avoid this side-effect,this local parser exploits an embedded decisiontree for pruning, of which the details can be seenin (Wu et al, 2005).
According to our experience,it is fairly easy for a general developer with goodunderstanding of the application to author thesmall grammar used by the local chart parser andannotate the training cases for the embedded de-cision tree.
The work can be finished in severalhours.2.2 Topic ClassificationGiven the representation of semantic frame, topicclassification can be regarded as identifying theframe type.
It is suited to be dealt using patternrecognition techniques.
The application of statis-tical pattern techniques to topic classification canimprove the robustness of the whole understand-ing system.
Also, in our system, topic classifica-tion can greatly reduce the search space andhence improve the performance of subsequentsemantic classification.
For example, the totalnumber of slots into which the concept [location]can be filled in all topics is 33 and the corre-sponding maximum number of slots in a singletopic is decreased to 10.Many statistical pattern recognition techniqueshave been applied to similar tasks, such as Na?veBayes, N-Gram and Support Vector Machines(SVMs) (Wang et al, 2002).
According to theliterature (Wang et al, 2002) and our experi-ments, the SVMs showed the best performanceamong many other statistical classifiers.
Also, ithas been showed that active learning can be ef-fectively applied to the SVMs (Schohn and Cohn,2000; Tong and Koller, 2000).
Therefore, wechoose the SVMs as the topic classifier.
We re-sorted to the LIBSVM toolkit (Chang and Lin,2001) to construct the SVMs for our experiments.Following the practice in (Wang et al, 2002), theSVMs use a binary valued features vector.
If thesimplest feature (Chinese character) is used, eachquery is converted into a feature vector1 | |, , chch ch ch=< >JJKJJK ?
( | |chJJK  is the total number ofChinese characters occur in the corpus) with bi-nary valued elements: 1 if a given Chinese char-acter is in this input sentence or 0 otherwise.
Dueto the existence of the preprocessor, we can alsoinclude semantic class labels (e.g., [location]) asfeatures for topic classification.
Intuitively, theclass label features are more informative than the200Chinese character features.
At the same time,including class labels as features can also relievethe data sparseness problem.2.3 Topic-dependent Semantic Classifica-tionThe job of semantic classification is to assign theconcepts with the most likely slots.
It can also bemodeled as a classification problem since thenumber of possible slot names for each conceptis limited.
Let?s consider the example sentence inFigure 1.
After the preprocessing and topic clas-sification, we get the preprocessed result ?Pleasetell me how can I go from [location]1 to [loca-tion]2 by [bus]??
and the topic ShowRoute.
Wehave to work out which slots are to be filled withthe values such as [location]2.
The first clue isthe surrounding literal context.
Intuitively, wecan infer that it is a [destination] since a [destina-tion] indicator ?to?
is before it.
If [location]1 hasalready been recognized as a [origin], it is an-other clue to imply that  [location]2  is a [destination].
Since initially the slot context is not avail-able, the slot context is only employed for thesemantic re-classification, which will be de-scribed in latter section.To learn the topic-dependent semantic classi-fiers, the training sentences need to be annotatedagainst the semantic frame.
Our annotating sce-nario is relatively simple and can be performedby general developers.
For example, for the sen-tence ?Please tell me how can I go from the peo-ple?s square to the bund by bus?
?, the annotatedresults are like the following:The corresponding slot names can be automati-cally extracted from the domain model.
A do-main model is usually a hierarchical structure ofthe relevant concepts in the application domain.For every occurrence of a concept in the domainmodel graph, we list all the concept names alongthe path from the root to its occurrence positionand regard their concatenation as a slot name.Thus, the slot name is not flat since it inherits thehierarchy from the domain model.With provision of the annotated data, we cancollect all the literal and slot context features re-lated to each concept.
The examples of featuresfor the concept [location] are illustrated as fol-lows:(1) to within the ?3 windows(2) from _ to(3) ShowRoute.[route].
[origin] within the 2?windowsThe former two are literal context features.
Fea-ture (1) is a context-word that tends to indicateShowRoute.[route].[destination].
Feature (2) is acollocation that checks for the pattern ?from?and ?to?
immediately before and after the con-cept [location] respectively, and tends to indicateShowRoute.[route].[origin].
The third one is aslot context feature, which tends to imply thetarget concept [location] is of type Show-Route.[route].[destination].
In nature, these fea-tures are equivalent to the rules in the semanticgrammar used by the robust rule-based parser.For example, the feature (2) has the same func-tion as the semantic rule ?
[origin] ?
from [loca-tion] to?.
The advantage of our approach is thatwe can automatically learn the semantic ?rules?from the training data rather than manually au-thoring them.
Also, the learned ?rules?
are intrin-sically robust since they may involves gaps, forexample, feature (1) allows skipping some noisewords between ?to?
and [location].The next problem is how to apply these fea-tures when predicting a new case since the activefeatures for a new case may make opposite pre-dictions.
One simple and effective strategy isemployed by the decision list (Rivest, 1987), i.e.,always applying the strongest features.
In a deci-sion list, all the features are sorted in order ofdescending confidence.
When a new target con-cept is classified, the classifier runs down the listand compares the features against the contexts ofthe target concept.
The first matched feature isapplied to make a predication.
Obviously, how tomeasure the confidence of features is a very im-portant issue for the decision list.
We use themetric described in (Yarowsky, 1994; Golding,1995).
Provided that 1( | ) 0P s f >  for all i :( ) max ( | )iiconfidence f P s f=                 (1)This value measures the extent to which the con-text is unambiguously correlated with one par-ticular slot is .2.4 Slot-value merging and semantic re-classificationThe slot-value merger is to combine the slotsassigned to the concepts in an input sentence.Another simultaneous task of the slot-valuemerger is to check the consistency among theidentified slot-values.
Since the topic-dependentclassifiers corresponding to different conceptsFRAME: ShowRouteSlots:   [route].[origin].[location].
( the people?s square)[route].[destination].[location].
(the bund)[route].[transport_type].[by_bus].
(bus)201are training and running independently, it possi-bly results in inconsistent predictions.
Consider-ing the preprocessed word sequence ?Please tellme how can I go from [location]1 to [location]2by [bus]?
, they are semantically clashed if [loca-tion]1 and [location]2 are both classified asShowRoute.[route].[origin].
To relieve this prob-lem, we can use the semantic classifier based onthe slot context feature.
We apply the contextfeatures like, for example, ?Show-Route.[route].
[origin] within the k?
windows?,which tends to imply Show-Route.[route].[destination].
The literal contextsreflect the local lexical semantic dependency.The slot contexts, however, are good at capturingthe long distance dependency.
Therefore, whenthe slot-value merger finds that two or more slot-value pairs clash, it first anchors the one with thehighest confidence.
Then, it extracts the slot con-texts for the other concepts and passes them tothe semantic classification module for re-classification.
If the re- classification results stillclash, the dialog system can involve the user inan interactive dialog for clarity.The idea of semantic classification and re-classification can be understood as follows: itfirst finds the concept or slot islands (like partialparsing) and then links them together.
Thismechanism is well-suited for SLU since the spo-ken utterance usually consists of several phrasesand noises (restart, repeats and filled pauses, etc)are most often between them (Ward and Issar,1994).
Especially, this phenomena and the out-of-order structures are very frequent in the spo-ken Chinese utterances.3 Weakly Supervised Training of theTopic Classifier and Topic-dependentSemantic ClassifiersAs stated before, to train the classifiers for topicidentification and slot-filling, we need to labeleach sentence in the training set against the se-mantic frame.
Although this annotating scenariois relatively minimal, the labeling process is stilltime-consuming and costly.
Meanwhile unla-beled sentences are relatively easy to collect.Therefore, to reduce the cost of labeling trainingutterances, we employ weakly supervised tech-niques for training the topic and semantic classi-fiers.The weakly supervised training of the twoclassifiers is successive.
Assume that a smallamount of seed sentences are manually labeledagainst the semantic frame.
We first exploit thelabeled frame types (e.g.
ShowRoute) of theseed sentences to train a topic classifier throughthe combination of active learning and self-training.
The resulting topic classifier is used tolabel the remaining training sentences with thecorresponding topic, which are not selected byactive learning.
Then, we use all the sentencesannotated against the semantic frame (includingthe seed sentences and sentences labeled by ac-tive learning) and the remaining trainingsentences labeled the topic to train the semanticclassifiers using a practical bootstrapping tech-nique.3.1 Combining Active Learning and Self-training for Topic ClassificationWe employ the strategy of combining activelearning and self-training for training the topicclassifier, which was firstly proposed in (Tur etal., 2005) and applied to a similar task.One way to reduce the number of labeling ex-amples is active learning, which have been ap-plied in many domains (McCallum and Nigam,1998; Tang et al, 2002; Tur et al, 2005).
Usu-ally, the classifier is trained by randomly sam-pling the training examples.
However, in activelearning, the classifier is trained by selectivelysampling the training examples (Cohn et al,1994).
The basic idea is that the most informa-tive ones are selected from the unlabeled exam-ples for a human to label.
That is to say, thisstrategy tries to always select the examples,which will have the largest improvement on per-formance, and hence minimizes the human label-ing effort whilst keeping performance (Tur et al,2005).
According to the strategy of determiningthe informative level of an example, the activelearning approaches can be divided into twocategories: uncertainty-based and committee-based.
Here, we employ the uncertainty-basedstrategy for selective sampling.
It is assumed thata small amount of labeled examples is initiallyavailable, which is used to train a basic classifier.Then the classifier is applied to the unannotatedexamples.
Typically the most unconfident exam-ples are selected for a human to label and thenadded to the training set.
The classifier is re-trained and the procedure is repeated until thesystem performance converges.Another alternative for reducing human label-ing effort is self-training.
In self-training, an ini-tial classifier is built using a small amount ofannotated examples.
The classifier is then used tolabel the unannotated training examples.
Theexamples with classification confidence scores202over a certain threshold, together with their pre-dicted labels, are added to the training set to re-train the classifier.
This procedure repeated untilthe system performance converges.These two strategies are complementary andhence can be combined.
The combination strat-egy is quite straightforward for pool-based train-ing.
At each iteration, the current classifier isapplied to the examples in the current pool.
Themost unconfident examples in the pool are se-lected by active learning and labeled by a human.The remaining examples in the pool are auto-matically labeled by the current classifier.
Then,these two parts of labeled examples are bothadded into the training set and used for retrainingthe classifier.
Since the LIBSVM toolkit pro-vides the class probability, we directly use theclass probability as the confidence score.
Ourdynamic pool-based (the pool size is n ) algo-rithm of combining active learning and self-training for training the topic classifier is as fol-lows:1.
Given a small amount of human-labeledtraining settS  ( n  sentences) and a largeramount of unlabeled set uS , build the initialclassifier using tS .2.
While labelers/ sentences are available(a) Get n  unlabeled sentences from uS(b) Apply the current classifier to n  unla-beled sentences(c) Select m  examples which are most in-formative to the current classifier andmanually label the selected m  exam-ples(d) Add the m  human-labeled examplesand the remaining n m?
machine-labeled examples to the training settS(e) Train a new classifier on all labeled ex-amples3.2 Bootstrapping the Topic-dependentSemantic ClassifiersBootstrapping refers to a problem of inducing aclassifier given a small set of labeled data and alarge set of unlabeled data (Abney, 2002).
It hasbeen applied to problems such as word-sensedisambiguation (Yarowsky, 1995), web-pageclassification (Blum and Mitchell, 1998), named-entity recognition (Collins and Singer, 1999) andautomatic construction of semantic lexicon(Thelen and Riloff, 2003).
The key to the boot-strapping methods is to exploit the redundancy inthe unlabeled data (Collins and Singer, 1999).Thus, many language processing problems canbe dealt using the bootstrapping methods sincelanguage is highly redundant (Yarowsky, 1995).The semantic classification problem here alsoexhibits the redundancy.
In the example ?Pleasetell me how can I go from [location]1 to [loca-tion]2 by [bus]?
?, there are multiple literal con-text features which all indicate that [location]1 isof type ShowRoute.[route].
[origin], such as:(1) from within the ?1 windows;(2) from _ to ;(3) to within the +1 windows.If the [location]2 has already be recognized asShowRoute.[route].
[destination], thus the slotcontext feature ?ShowRoute.[route].
[origin]within the 2?
windows?
is also a strong evi-dence that [location]1 is of type Show-Route.[route].[origin].
That is to say, the literalcontext and slot context features above effec-tively overdetermine the slot of a concept in theinput sentence.
Especially, the literal and slotcontext features can be seen as two natural?views?
of an example from the respective of?Co-Training?
(Blum and Mitchell, 1998).
Ourbootstrapping algorithm exploits the property ofredundancy to incrementally identify the featuresfor assigning slots of a concept, given a few an-notated seed sentences.The bootstrapping algorithm is performed oneach topic iT  ( 1 i n?
?
, n  is the number oftopic) as follows:1.
For each concept jC  in iT  (1 j m?
?
, m isthe number of concepts appears in the sen-tences of topic iT ):(1.1) Build the two initial classifiers based onliteral and slot context features respec-tively using a small amount of labeledseed sentences.
(1.2) Apply the current classifier based on theliteral context feature to the remainingunlabeled concepts in the training sen-tences belong to topic iT .
Keep thoseclassified slots with confidence scoreabove a certain threshold (In this paper,the threshold is fixed on 0.5).2.
Check the consistency of the classified slotsin each sentence.
If some slots in a sentenceclashed, take the one with the highest confi-dence score among them and leave the othersunlabeled.3.
For each concept jC in iT , apply the currentclassifier based on the slot context to the re-sidual unlabeled concepts.
Keep those classi-203fied slots with confidence score above a cer-tain threshold.
Repeat Step 3.4.
Augment the new classified cases into thetraining set and retrain the two classifiersbased on literal and slot context features re-spectively.5.
If new slots are classified from the trainingdata, return to step 2.
Otherwise, repeat 2-5to label training data and keep all new classi-fied slots regardless of the confidence score.Train the two final semantic classifiers basedon the literal and context features respec-tively using the new labeled training data.4 Experiments and Results4.1 Data Collection and Experimental Set-tingOur experiments were carried out in the contextof Chinese public transportation information in-quiry domain.
We collected two kinds of corpusfor our domain using different ways.
Firstly, anatural language corpus was collected through aspecific website which simulated a dialog system.The user can conduct some mixed-initiative con-versational dialogues with it by typing Chinesequeries.
Then we collected 2,286 natural lan-guage utterances through this way.
It was dividedinto two parts: the training set contained 1,800sentences (TR), and the test set contained 486sentences (TS1).
Also, a spoken language corpuswas collected through the deployment of a pre-liminary version of telephone-based dialog sys-tem, of which the speech recognizer is based onthe speaker-independent Chinese dictation sys-tem of IBM ViaVoice Telephony and the SLUcomponent is a robust rule-based parser.
Thespoken utterances corpus contained 363 spokenutterances.
Then we obtained two test set fromthis corpus: one consisted of the recognized text(TS2); the other consisted of the correspondingtranscription (TS3).
The Chinese character errorrate and concept error rate of TS2 are 35.6% and41.1% respectively.
We defined ten types oftopic for our domain: ListStop, ShowFare,ShowRoute, ShowRouteTime, etc.
The firstcorpus covers all the ten topic types and the sec-ond corpus only covers four topic types.
The to-tal number of Chinese characters appear in thedata set is 923.
All the sentences were annotatedagainst the semantic frame.
In our experiments,the topic classifier and semantic classifiers weretrained on the natural language training set (TR)and tested on three test sets (TS1, TS2 and TS3).The performance of topic classification andsemantic classification are measured in terms oftopic error rate and slot error rate respectively.Topic performance is measured by comparingthe topic of a sentence predicated by the topicclassifier with the reference topic.
The slot errorrate is measured by counting the insertion, dele-tion and substitution errors between the slotsgenerated by our system and these in the refer-ence annotation.4.2 Supervised Training ExperimentsFirstly, in order to validate the effectiveness ofour proposed SLU system using successivelearners, we compared our system with a rule-based robust semantic parser.
The parsing algo-rithm of this parser is same as the local chartparser used by the preprocessor.
The handcraftedgrammar for this semantic parser took a linguis-tic expert one month to develop, which consistsof 798 rules (except the lexical rules for namedentities such as [loc_name]).
In our SLU system,we first use the SVMs to identify the topic andthen apply the semantic classifier (decision list)related to the identified topic to assign the slotsto the concepts.
The SVMs used the augmentedbinary features (923 Chinese characters and 20semantic class labels).
A general developer inde-pendently annotated the TR set against the se-mantic frame, which took only four days.Through feature extraction from the TR set andfeature pruning, we obtained 2,259 literal contextfeatures and 369 slot context features for 20kinds of concepts in our domain.
Table 1 Showsthat our SLU method has better performancethan the rule-based robust parser in both topicclassification and slot identification.
Due to thehigh concept error rate of recognized utterances,the performance of semantic classification on theTS2 is relatively poor.
However, if consideringonly the correctly identified concepts on TS2, theslot error rate is 9.2%.
Note that, since the TS2(recognized speech) covers only four types oftopic but TS1 (typed utterance) covers ten topics,the topic error on the TS2 (recognized speech) islower than that on TS1.Table 1 also compares our system with thetwo-stage classification with the reversed order.Another alternative for our system is to reversethe two main processing stages, i.e., finding theroles for the concepts prior to identifying thetopic.
For instance, in the example sentence inFig.1, the concept (e.g., [location]) in the pre-processed sequence is first recognized as slots(e.g., [route].
[origin]) before topic classification.204Therefore, the slots like [route].
[origin] can beincluded as features for topic classification,which is deeper than the concepts like [location]and potential to achieve improvement on per-formance of topic classification.
This strategywas adopted in some previous works (He andYoung, 2003; Wutiwiwatchai and Furui, 2003).However, the results indicate that, at least in ourtwo-stage classification formwork, the strategyof identifying the topic before assigning the slotsto the concepts is more optimal.
According toour error analysis, the unsatisfied performance ofthe reversed two-stage classification system canbe explained as follows:  (1) Since the semanticclassification is performed on all topics, thesearch space is much bigger and the ambiguitiesincrease.
This deteriorates the performance ofsemantic classification.
(2) In the case that theslots and Chinese characters are included as fea-tures, the topic classifier relies heavily on the slotfeatures.
Then, the errors of semantic classifica-tion have serious negative effect on the topicclassification.Table 1: Performance comparsion of the rule-based robust semantic parser, the reversed two-stage classification system and our SLU systems(TER: Topic Error Rate; SER: Slot Error Rate;DL: Decision List)TS1 TS2 TS3TER(%)SER( %)TER(%)SER( %)TER(%)SER( %)Rule-based se-mantic parser 6.8  11.6 4.1  47.9 3.0 5.4Reversed two-stage classifica-tion system4.9 11.1 3.6 47.4 2.5 4.9Our system 2.9   8.4 2.2   45.6 1.4  4.64.3 Weakly Supervised TrainingExperiments4.3.1 Active Learning and Self-training Ex-periments for Topic ClassificationIn order to evaluate the performance of activelearning and self-training, we compared threesampling strategies: random sampling, activelearning only, active learning and self-training.At each iteration of pool-based active learningand self-training, we get 200 sentences (i.e., thepool size is set as 200) and select 50 most uncon-fident sentences from them for manually labelingand exploit the remaining sentences using self-training.
All the experiments were repeated tentimes with different randomly selected seed sen-tences and the results were averaged.
Figure 1plots the learning curves of three strategiestrained on TR and tested on the TS1 set.
It is evi-dent that active learning significantly reduces theneed for labeled data.
For instance, it requires1600 examples if they are randomly chosen toachieve a topic error rate of 3.2% on TS1, butonly 600 actively selected examples, a saving of62.5%.
The strategy of combing active learningand self-training can further improve the per-formance of topic classification compared withactive learning only with the same amount oflabeled data.2.00%3.00%4.00%5.00%6.00%7.00%8.00%9.00%10.00%0 200 400 600 800 1000 1200 1400 1600 1800 2000Number of labeled sentencesTopicerrorrateRandomActive LearningActive Learning &Self-traingFigure 2: Learning curves using different sam-pling strategies.We also evaluated the performance of topicclassification using active learning and self-training with the pool size of 200 on the threetest sets.
Table 2 shows that active learning andself-training with the pool size of 200 achievesalmost the same performance on three test sets asrandom sampling, but requires only 33.3% data.Table 2: The topic error rate using active learn-ing and self-training with pool size of 200 on thethree test sets (AL: Active Learning)TS1 (%)TS2(%)TS3(%)LabeledSent.
(#)Random 2.9 2.2 1.4 1,800AL 3.2 2.5 1.7 600AL & self-training 2.9 2.5 1.4 6004.3.2 Bootstrapping Experiments for Se-mantic ClassificationAs stated before, the bootstrapping procedurebegins with a small amount of sentences anno-tated against the semantic frame, which is theinitial seed sentence or annotated by active learn-ing, and the remaining training sentences, thetopics of which are machine-labeled by the re-sulting topic classifier.
For example, in the205weakly supervised training scenario with thepool size of 200, the active learning and self-training procedure ran 8 iterations.
At each itera-tion, 50 sentences were selected by active learn-ing.
So the total number of labeled sentences is600.
We compared our bootstrapping methodswith supervised training for semantic classifica-tion.
We tried two bootstrapping methods: usingonly the literal context features (Bootstrapping 1)and using the literal and slot context features(Bootstrapping 2).
If the step 4 of the bootstrap-ping algorithm in Section 3.2 is canceled, thenew bootstrapping variation corresponds toBootstrapping 2.
Also, we repeated the experi-ments ten times with different labeled sentencesand the results were averaged.
Figure 3 plots thelearning curves of bootstrapping and supervisedtraining with different number of labeled sen-tences on the TS1 set.
The results indicate thatbootstrapping methods can effectively make useof the unlabeled data to improve the semanticclassification performance.
In particular, thelearning curve of bootstrapping 1 achieves moresignificant improvement than the curve of boot-strapping 2.
It can be explained as follows: in-cluding the slot context features further increasesthe redundancy of data and hence corrects theinitial misclassified cases by the semantic classi-fier using only literal context features or providesnew cases.6.00%8.00%10.00%12.00%14.00%16.00%18.00%20.00%0 100 200 300 400 500 600 700Number of labeled sentencesSloterrorrateSupervisedtrainingBootstrapping 1Bootstrapping 2Figure 3: Learning curves of bootstrapping meth-ods for semantic classification on TS1.Finally, we compared two SLU systemsthrough weakly supervised and supervisedtraining respectively.
The supervised one wastrained using all the annotated sentences in TR(1800 sentences).
In the weakly supervisedtraining scenario (the pool size is still 200), Thetopic classifier and semantic classifiers were bothtrained using only 600 labeled sentences.
Table 3shows that the weakly supervised scenarioachieves comparable performance to the super-vised one, but requires only 33.3% labeled data.Table 3: Performance comparison of two SLUsystems through weakly supervised and super-vised training on the three test sets (TER: TopicError Rate; SER: Slot Error Rate)TS1 TS2 TS3TER(%)SER(%)TER(%)SER(%)TER(%)SER(%)Supervised 2.9  8.4 2.2   45.6 1.4  4.6WeaklySupervised 2.9 9.7 2.5 44.8 1.4 5.75 Conclusion and Future workWe have presented a new SLU framework usingtwo successive classifiers.
The proposed frame-work exhibits the advantages as follows.z It has good robustness on processing spokenlanguage: (1) The preprocessor provides thelow level robustness.
(2) It inherits the ro-bustness of topic classification using statis-tical pattern recognition techniques.
It canalso make use of topic classification toguide slot filling.
(3) The strategy of firstfinding the concepts or slot islands and thenlinking them is suited for processing spokenlanguage.z It also keeps the understanding deepness: (1)The class of semantic classification is theslot name, which inherits the hierarchy fromthe domain model.
(2) The semantic re-classification mechanism ensures the consis-tency among the identified slot-value pairs.z It is mainly data-driven and requires onlyminimally annotated corpus for training.Most importantly, our proposed SLUframework allows the employment ofweakly supervised strategies for training thetwo classifiers, which can reduce the cost ofannotating labeled sentences.The future work includes further evaluation ofour approach in other application domains andlanguages.
We also plan to integrate this under-standing system into a whole dialog system.Then, high level knowledges, such as the dialogcontext, can also be included as the features oftopic and semantic classifiers.
Moreover, cur-rently, the topics are manually defined throughexamination of the example sentences by human.Then, it is worthwhile to investigate how to ap-propriately define topics and the probability of206exploiting the sentence clustering techniques tofacilitate the topic (frame) designment.6 AcknowledgementsThe authors would like to thank three anony-mous reviewers for their careful reading andhelpful suggestions.
This work is supported byNational Natural Science Foundation of China(NSFC, No.
60496326) and 863 project of China(No.
2001AA114210-11).ReferencesS.
Abney.
2002.
Bootstrapping.
In Proc.
of ACL, pp.360-367, Philadelphia, PA.A.
Blum and T. Mitchell.
1998.
Combining labeledand unlabeled data with co-training.
In Proc.
ofCOLT, Madison, WI.C.
Chang and C. Lin.
2001.
LIBSVM: a library forsupport vector machines.
Software available athttp://www.csie.ntu.edu.tw/~cjlin/libsvm.D.
Cohn, L. Atlas and R. Ladner.
1994.
Improvinggeneralization with active learning.
MachineLearning 15, pp.201-221.M.
Collins and Y. Singer.1999.
Unsupervised modelsfor named entity classification.
In Proc.
of EMNLP.J.
Dowding, J. M. Gawron, D. Appelt, J.
Bear, L.Cherny, R. Moore, and D. Moran.
1993.
GEMINI:A natural language system for spoken-languageunderstanding.
In Proc.
of  ACL, Columbus, Ohio,pp.
54-61.R.
Golding.
1995.
A Bayesian Hybrid Method forContext-sensitive Spelling Correction.
In Proc.
3rdWorkshop on Very Large Corpora, Boston, MA.Y.
He and S.J.
Young.
2003.
A Data-Driven SpokenLanguage Understanding System.
IEEE Workshopon Automatic Speech Recognition and Understand-ing, US Virgin Islands.Y.
He and S. Young.
2005.
Semantic Processing us-ing the Hidden Vector State Model.
ComputerSpeech and Language 19(1): 85-106.A.
McCallum and K. Nigam.1998.
Employing EMand pool-based active learning for text classifica-tion.
In Proc.
of  ICML.S.
Miller, R. Bobrow, R. Ingria, and R. Schwartz.1994.
Hidden Understanding Models of NaturalLanguage.
In Proc.
of ACL, pp.
25-32.R.
Pieraccini and E. Levin.
1993.
A learning ap-proach to natural language understanding.
NATO-ASI, New Advances & Trends in Speech Recogni-tion and Coding, Springer-Verlag, Bubion, Spain.R.
L. Rivest.
1987.
Learning decision lists.
MachineLearning, 2(3):229--246, 1987.S.
Seneff.
1992.
TINA: A natural language system forspoken language applications.
Computational Lin-guistics, vol.
18, no.
1., pp.
61-86.G.
Schohn and D. Cohn.
2000.
Less Is More: ActiveLearning with Support Vector Machines.
In Proc.of ICML, pp.
839-846.M.
Tang, X. Luo, S. Roukos.2002.
Active learning forstatistical natural language parsing.
In Proc.
ofACL, Philadelphia, Pennsylvania.M.
Thelen and E. Riloff.
2002.
A BootstrappingMethod for Learning Semantic Lexicons using Ex-traction Pattern Contexts.
In Proc.
of EMNLP?02.S.
Tong and D. Koller.
2000.
Support Vector MachineActive Learning with Applications to Text Classifi-cation.
In Proc.
of ICML, pp.
999-1006.G.
Tur, D. Hakkani-T?r, Robert E. Schapire.
Combin-ing Active and Semi-Supervised Learning for Spo-ken Language Understanding.
Speech Communi-cation, Vol.
45, No.
2, pp.
171-186, 2005.Y.
Wang.
1999.
A Robust Parser for Spoken Lan-guage Understanding.
In Proc.
of EUROSPEECH.Budapest, Hungary.Y.
Wang and A Acero.
2001.Grammar learning forspoken language understanding.
In Proc.
of ASRUWorkshop, Madonna di Campiglio, Italy.Y.
Wang, A. Acero, C. Chelba, B. Frey and L. Wong.2002.
Combination of Statistical and Rule-basedApproaches for Spoken Language Understanding,In ICSLP.
Denver, Colorado.W.
Ward and S. Issar.
1994.
Recent Improvements inthe CMU Spoken Language Understanding System.In Proc.
of ARPA Workshop on HLT, March, 1994.W Wu, J Duan, R Lu, F Gao.
2005.
Embedded ma-chine learning systems for Robust Spoken Lan-guage Parsing.
In Proc.
of IEEE NLP-KE, Wuhan,China.C.
Wutiwiwatchai and S. Furui.
2003.
Combination ofFinite State Automata and Neural Network forSpoken Language Understanding.
In Proc.
of EU-ROSPEECH2003, Geneva, Switzerland.D.
Yarowsky.
1994.
Decision Lists for Lexical Ambi-guity Resolution: Application to Accent Restora-tion in Spanish and French.
In Proc.
of ACL 1994,pp.
88-95.207
