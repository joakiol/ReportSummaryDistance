Syllable-based Phonetic transcription by Maximum Likelihood MethodsR.A.SharmanMP167, IBM(UK) Labs Ltd, Hursley Park, Winchester SO21 2JN, UKIntroductionThe transcription of orthographic words intophonetic symbols is one the principal steps of atext-to-speech system\[l\].
In such a system asuitable phonetic pronunciation must be supplied,without human intervention, for every word inthe text.
No dictionary, however large, willcontain all words, let alne proper names,technical terms and other textual itemscommonly found in unrestricted texts.Consequently, an automatic transcriptioncomponents is usually considered essential.ttand-written rule sets, defining the transcriptionof a letter in its context o some sound, view theprocess as that of parsing with a context-sensitive grammar.
This approach totranscription has been challenged more recentlyby a variety of methods uch as Neural nets\[2\],Perceptrons\[3\], Markov Models\[4\], andDecision Trees\[5\].
Some approaches have usedadditional information such as prefixes andsuffixes\[8\], syllable boundaries\[3\], sometimescombined with the use of parts-of-speech toassist in the disambiguation of multiplepronunciations.
In the phonetic transcription ofproper names special techniques can beemployed to improve accuracy\[9\] such asdetecting the language of origin of the name andusing different spelling to sound rules.
Eachmethod has its own advantages anddisadvantages in terms of computational speed,complexity and cost.
However, none of thesemethods by itself is completely adequate.The present method uses the two-stepconversion process described elsewhere\[i,3\] inwhich the structure of the word plays a centralrole.
First the orthographic word is divided intoits syllables, and secondly the syllable sequence isconverted to a phonetic string.
This not onlyaccords with linguistic intuition, but it alsoallows the two processes to be handled bydifferent techniques, choosing the techniquemost suited to each step.
The question ofwhether a moq)hological or ,~yllabicdecomposition f the word might produce betterresults is not thrther analysed here.
(For thepresent study data was available for syllables andnot for morphs, so in the sense the comparisoncould not be carried out by the techniquesproposed).
The effects of other factors, such aspart-of speech tagging, domain-dependentinformation, and other information sources, wereignored, although these could be useful inpractical systems.The technique proposed for syllabification isbased on the principle of Hidden MarkovModelling, well known in speech recognition\[7\].This presupposes the existence of some trainingmaterial containing words in both theirorthographic and syllabic tbrm.
Using this data amodel of syllable sequences can be designed andtrained to identit~y s llable boundaries.
Once themost likely syllable division of the word has beentbund the phonetic transcription can be producedby a variety of direct ranscription methods, suchas the one used here based on Decision Trees\[5\].The training of such a method presupposes theexistence of some training data containing wordsin both their syllabic and phonetic forms.
Usingthe latter data a Decision Tree can be trained totranscribe syllables in context into phonesequences.
The advantage of using decisiontrees is that they not only learn general rules, butalso capture idiosyncratic special casesautomatically.
The resulting process shouldperform transcription with high accuracy.Such a two-stage approach as been shown toyield improvements\[3\] but only where perfectsyllabification information is available,consequently a reliable syllabification techniqueis required.
The remainder of this paper1279discusses only the syllabification process indetail, since the decision tree methodology iswell described elsewhere\[5\], whereas thesyllabification algorithm proposed is novel.
Anexperiment using a very large set of word-syllable-pronunciation strings was used to trainthe two models, and then tests performed todetermine the accuracy of the resultingtranscription.A Maximum Likelihood Model ofsyllabificationThe purpose of this step is to make explicit thehidden syllable boundaries in the observedwords.
These often, but not always coincidewith the molphological boundaries of theconstituent parts of each word.
However, so asnot to confuse the question of the derivation of aword from its roots, prefixes and suffixes, withthe question of the pronunciation of the word insmall discrete sections of vowels and consonants,the term molphology is not used here.
Strictlyspeaking the term syllable might be moreaccurately applied only after transcription tophonemes.
However, we shall use it here toapply to such pronunciation units describedorthographically.
The purpose of such analysis isto obtain information which will be used by thephonetic transcription stage to make betterjudgements on the pronunciation of consonantand vowel clusters in particular.For example, the consonant cluster ph in theword loophole might be pronounced /f /  byanalogy with the same cluster in the wordtelephone.
However, it might also bepronounced as /ph/ by analogy with the samecluster in the word tophat.
The deciding factoris where the syllable boundary lies in the word.The most plausible structure for the wordtelephone is tele tphone, or possibly,te+le+phone, and for tophat is top+hat.
So apossible syllable structure for the word loopholemight be loop+hole, or alternatively loo-tphole,or maybe looph ~ole.
The syllable model needsto determine what the true, but unobserved,syllable sequence is, given only the observedevidence of the orthographic characters.
Thiscan be modelled as a decoding problem in whicha hidden sequence of states (syllables) gives riseto an observed sequence of symbols (letters).We need to discover the underlying sequence ofstates which gave rise to the observations.
Thecomplexity arises since the states andobservations do not align in a simple way\[l 1\].Syllable models of a similar type have beenproposed for prosody\[12\] but not fortranscription, whereas direct models oftranscription have been attempted\[4\].Let a orthographic word, W, be defined as asequence of letters, w 1,w 2 ..... %.
Let a syllabicword, S, be defined as a sequence of syllables,s~,s 2..... s,,.
The observed letter sequence, IV,then arises from some hidden sequence ofsyllables, S, with conditional probability I'(WIS),There are a finite number of such syllablesequences, of which the one given bymax P(WIN) where the maximisation is takenover all possible syllable sequences, is themaximum likelihood solution, and intuitively, themost plausible analysis.
By the well-know Bayestheorem, it is possible to rewrite this expressionas:max \[ P(WI S)\] = max 1_t'(5'1W)P(S) Iv-  "1s L p(nO JIn this equation it is interesting to interpret theP(SIW) as a probability distribution capturingthe facts of syllable division, while the P(S) is adifferent distribution capturing the facts ofsyllable sequences.
The latter model thuscontains information such as which syllablesform prefixes and suffixes, while the formercaptures ome of the facts of word constructionin the usage of the language.
Note that the termP(W), which models the sequence of letters, isnot required in the maximisation process, since itis not a function of S. Given the existence ofthese two distributions there is, in principle, awell-understood method of estimating theparameters, and performing the decoding\[7\].The estimation is provably capable of finding alocal optimum\[13\], and is thus dependent onfinding good initial conditions to train from.
Inthis application the initial conditions are providedby supervised training data obtained from adictionary.1280A variety of expansions of the terms t'(SIW) and1'(S) can be derived, depending on thecomputational cost which is acceptable, and theamount of training data available.
There is thus afamily of models of increasing complexity whichcan be used in a methodical way to obtain bettermodelling, and thus more accurate processingThe function P(SIW) can be simply modelled as#tl lwhich has the wdue 0 eveuwhere , except whens, = w:.. .
,  % tbr any j k, when it has the value 1.This simply says that each syllable is spelled thesame way as the letters which compose it.
Thispoints the way to a more sophisticated model ofsyllabification which incorporates spellingchanges at syllable boundaries, but this will notbe attempted here.
Another application of theapproach might be in a model of inflexional orderivational morphology where spelling changesarc observed at morph boundaries.The function P(S) can be modelled most simplyas a bi-gram distribution, where theapproximation is made that;:'(s,l.,', ..... s, :'(s, ls, ,)i \] i 2Such a simple model can capture manyinteresting effects of syllable placements adjacentto other syllables, and adjacent to boundaries.However, it would not be expected that subtleeffects of syllabification due to longer rangeeffects, if they exist, could be captured this way.An efficient computational scheme forsyllabificationOne complication exists before either the Viterbidecoding algorithm\[7\] for determining thedesired syllable sequence, or the Forward-Backward parameter estimation algorithm\[7\] canbe used.
This is due to the combinatorialexplosion of state sequences due to the fact thatpotential syllables may overlap the same lettersequences, as shown in the example above withthe word telephone.
1'his leads to the decodingand training algorithms becoming O(n 3) ratherthan O(n 2) in computational complexity, asusual tbr this type of problem.
The difficulty canbe overcome by the use of a technique fromcontext-fi-ee parsing\[14\], namely the use of asubstring table.
The method will be brieflydescribed.A word of length, n, can contain n ~/2substrings, any of which may potentially besyllables of the word.
Using the method oftabular layout familiar from the Cocke-Kasami-Younger (CKY) parsing algorithm, thesesubstrings can be conveniently represented as atriangular table, 71, ~-: {1~,} (see diagram below).Where the table contains a non-zero element heindex number of a unique syllable can be tbund.
'\['he first step in parsing the word is to generateall the possible substrings and check them againsta (able of possible syllables.
Even \[br longwords with 20 or 30 letters, this is not aprohibitive calculation.
If the letter string isidentified as a possible syllable then the uniqueidenti~ing number of the syllable can be enteredinto the table.
(note: dots used as abbreviat ion inh igher nodes for simplicity)tele I elep I lepltel I ele I lep.
.
.
.
4 .
.
.
.
.te I el I let t e I It e I- -  4 - -  ,?
L 'A  ?
ke p h o n eThe computat ional  structure used for f inding the syllable sequenceThe bigram sequence model can now becalculated by the following algorithm, which isan adaptation of the fi~miliar CKY algorithm:for each letter w\[i\], i=l,...,nfor each starting syllable positiont\[i,j\],j=l,...,n+l-ifor each ending syllable positiont\[i+j-l,k\],k=l ..... n-i-jlet x-t\[i,j\] and y-t\[i+j-1\]1281compute P(s(y)\[s(x))In this way it is possible to calculate all thepossible syllable sequences which apply to thegiven word without being overwhelmed by asearch for all possible syllable sequences.A methodology for constructing asyllabifierThe following methodology can be used to builda practical implementation of the techniqueoutlined above:.2.3.Collect a list of possible syllables.From the observed data of orthographic-syllabic word pairs, construct an initialestimate of P(M)= l-IP(milmi_~).
This isthe bi-gram model of syllable sequences.Using another list of words, not present inthe initial training data, use the Forward-Backward algorithm to improve the estimatesof the bi-gram model.
(This step is optionalif the original data is sufficiently large, sincethe hand annotated text may be superior tothe maximum likelihood solution generatedby the Forward-Backward algorithm.
)To decode a given orthographic word into itsunderlying syllable sequence, first construct atable of the possible syllables in the manner givenabove.
Use the variant of the parsing algorithmdescribed above to obtain a value for the mostlikely syllable sequence which could have givenrise to the observed spelling in a way consistentwith the Viterbi algorithm for strict HMM's.Training and testing the modelA large collection of words was obtained forwhich orthography, syllable boundaries andpronunciations were available\[l 1\], ultimatelyfrom a machine readable dictionary, the CollinsEnglish Dictionary.
As described\[ll\] theoriginal data was extracted from a type-settingtape in which the words were listed in the usualforms with abbreviations, run-ons, and othertypographical devices.
These were firstregularised by a combination of human andprogrammed conversioned so that no difficultieswere encountered in the current experiment.The word entries were then divided into trainingdata (220,000 words) and test data (5,000words) by randomly extracting words.
It wasobserved that the 220,000 words in the trainingtext were composed of sequences of syllablestaken from a set of 27,000 unique syllable types.An initial estimate of the syllable bi-gram modelcan be directly computed by observation.
Thisinitial model was able to decode the training datawith 96% accuracy and the test data with 89%accuracy.
This indicates the requirement for asmoothing technique to generalise theparameters of the bi-gram syllable model.
Suchsmoothing may reduce the accuracy of the modelon the training data, but should improve it on thetest data.A further 100,000 words, not previously seen inthe dictionary, were obtained from a corpus of100 million words of Newspaper articles(available on published CD ROM from theGuardian and Independent newspapers).Numeric items, tbrmatting words, and othertextual items not suitable for this test wereomitted.
Assuming that no new syllable typesare required to model this data, the trainingprocedure described above was used to adapt heinitial statistics obtained by direct inspection.The performance of the model on the trainingtext was 94% and on the test data 92%.
Thisindicates that some generalisation had occurredwhich made the model less specific to the initialtraining text, but more robust on the test text.The affect of this syllable model on the overallpronunciation system is as follows: The basicdecision tree transcription system when workingdirectly from orthography to phonemes has aword correct accuracy of 86% on training textand 78% on test data.
(the result for trainingdata is not 100% as expected because ofsmoothing and other generalisations in thedecision tree construction process).
With the useof syllables as marked, and a new decision treegrown on the syllable marked training data, theoverall system has a word accuracy rate of 92%on the training data and 89% on the test data.1282ConclusionsA method of determining syllable boundaries hasbeen shown.
The method can be improved bythe use of a tri-syllable model and by the use ofmore training data.
Other extensions could beexplored quite easily.
The method oes not findnew syllable types.
For this some type ofunsupervised clustering method is required.
Themethod leaves unsolved the treatment of unusualor idiosyncratic textual conventions, notations,and numeric information.
It seems that rule-based techniques will still be needed.While the more serious question still to beanswered for TTS systems lie elsewhere, forexample in prosody\[10\], the inability of systemsto perform transcription with high accuracymakes this still an open question.
The problemof transcription is also of interest in SpeechRecognition\[6\] where there is a need to generatephonetic baseforms of words which are includedin the recognisers' vocabulary.
In this case thework required to generate a pronouncingdictionary fbr a large vocabulary in a newdomain, including many technical terms and newjargon not previously seen, calls tbr anautomatic, rather than manual technique.In the wider context he method applied here isanother example of self-organising methodsapplied to Natural Language Processing.
Whilethese methods have found a fundamental place inspeech processing (for example, speechrecognition) they have yet to be seriouslyadopted for language processing.
It is apossibility that many more specific tasks inlanguage processing may be amenable totreatment by self-organising methods, with aconsequent improvement in the reliability andease of replication of the NLP systems whichincorporate hem.References1.
J.Allen, MS.Hunnicutt and D.Klatt, From\]'ext to Speech, Cambridge UniversityPress,Cambridge, 1987.2.
S.M.Lucas and R.i.Damper, Syntacticneural networks .fi~r bi-directional text-phonetics, tP 127-141 in Talking Machines,ed G.Bailly and C.Benoit, North Holland,19911.3.
W.A.Ainsworth and NP.Warren,Appfication of Mu#ilayer Perceptrons inText-m-Speech Synthesis Systems, pp 256-288 of Neural Networks fi~r Vision, Speechcmd Natural Language, ed D.J.Myers andC.Nightingale, Chapman Hall, 1992.4.
S.Parfitt and R.ASharman, A hi-directionalmodel of Eng, lish t'ronunciation, pp 801-804, Proceedings of EuroSpeech 91, Genoa,199l.5.
L.R.Bahl, P.V.
deSouza,P.S.Gopalakrishnan, D.Nahamoo andM.A.
Picheny, (~ontext-dependent Modellingof Phones in Contitmous Speech usingDecision \]'rees, IEEE ICASSP 1992.6.
F.Jelinek, et al, \]he develotmlent of a large-vocabuklty discrete wotff SpeechRecognition system, 11,51,21,; Trans, Speechand Signal Processing, 1985."7.
l,.Rabiner, 7'u/orial on ltidden MarkovModels' and Selected Atplications in SpeechRecognition, Proc IEEE vol 77, no 2.pp257-286, 1989.8.
S.R.Hertz, J.Kadin, and K.J.Karplus, \]hel)elta Rule l.)evelopment System fi~r SpeechSynthesis from Text, Proc IEEE vol 73 no11.
pp 1589-1601, 1985.9.
K.Church, P/vnouncing proper names,ACL Chicago, 1985.10.
R.Collier, H.C.Van Lecuwen andL.F.Willems, Speech Synthesis Today and7@norrow, l'hilips Journal of Research andDevelopment, vol 47 no 1, pp 15-34, 1992.II.
S.G.Lawrence and G.Kaye, Alignment ofphonemes with their cot7"e,pondingorthoL, raphy, Computer Speech andLanguage vol 1, pp 153-165, 1986.12.
MGiustiniani, A.Falaschi and P.Pierucci,Automatic inference of a Syllabic ProsodicModel Eurospeech pp 197-200, 1991.13.
B.Merialdo, On the locality of the l;orward-Backward Algorithm, IEEE Transactions onSpeech and Audio Processing, pp 255-257vol 1 no.
2, April 1993.14.
A.VAho and J.D.Ullman, "lhe theory ofpatwing; #'anslalion and compiling,Prentice-Hall, 1972.12~3
