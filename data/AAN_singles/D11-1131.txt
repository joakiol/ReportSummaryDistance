Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1416?1425,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsReducing Grounded Learning Tasks to Grammatical InferenceBenjamin B?rschingerDepartment of ComputingMacquarie UniversitySydney, Australiabenjamin.borschinger@mq.edu.auBevan K. JonesSchool of InformaticsUniversity of EdinburghEdinburgh, UKb.k.jones@sms.ed.ac.ukMark JohnsonDepartment of ComputingMacquarie UniversitySydney, Australiamark.johnson@mq.edu.auAbstractIt is often assumed that ?grounded?
learningtasks are beyond the scope of grammatical in-ference techniques.
In this paper, we showthat the grounded task of learning a seman-tic parser from ambiguous training data as dis-cussed in Kim and Mooney (2010) can be re-duced to a Probabilistic Context-Free Gram-mar learning task in a way that gives stateof the art results.
We further show that ad-ditionally letting our model learn the lan-guage?s canonical word order improves itsperformance and leads to the highest seman-tic parsing f-scores previously reported in theliterature.11 IntroductionOne of the most fundamental ideas about languageis that we use it to express our thoughts.
Learning anatural language, then, amounts to (at least) learninga mapping between the things we utter and the thingswe think, and can therefore be seen as the task oflearning a semantic parser, i.e.
something that mapsnatural language expressions such as sentences intomeaning representations such as logical forms.
Ob-viously, this learning can neither take place in a fullysupervised nor in a fully unsupervised fashion: thelearner does not ?hear?
the meanings of the sentencesshe observes, but she is also not treating them asmerely meaningless strings.
Rather, it seems plau-sible to assume that she uses extra-linguistic context1The source code used for our experiments and the evalua-tion is available as supplementary material for this article.to assign certain meanings to the linguistic input sheis confronted with.In this sense, learning a semantic parser seemsto go beyond the well-studied task of unsupervisedgrammar induction.
It involves not only learninga grammar for the form-side of language, i.e.
lan-guage expressions such as sentences, but also the?grounding?
of this structure in meaning represen-tations.
It requires going beyond the mere linguisticinput to incorporate, for example, perceptual infor-mation that provides a clue to the meaning of the ob-served forms.
Essentially, it seems as if ?grounded?learning tasks like this require dealing with twodifferent kinds of information, the purely formal(phonemic) and meaningful (semantic) aspects oflanguage.
Grammatical inference seems to be lim-ited to dealing with one level of formal information(Chang and Maia, 2001).
For this reason, probably,approaches to the task of learning a semantic parseremploy a variety of sophisticated and task-specifictechniques that go beyond (but often elaborate on)the techniques used for grammatical inference (Luet al, 2008; Chen and Mooney, 2008; Liang et al,2009; Kim and Mooney, 2010; Chen et al, 2010).In this paper, we show that one can reduce thetask of learning a semantic parser to a ProbabilisticContext Free Grammar (PCFG) learning task, andmore generally, that grounded learning tasks are notin principle beyond the scope of grammatical infer-ence techniques.
In particular, we show how to for-mulate the task of learning a semantic parser as dis-cussed by Chen, Kim and Mooney (2008, 2010) asthe task of learning a PCFG from strings.
Our modeldoes not only constitute a proof of concept that this1416reduction is possible for certain cases, it also yieldshighly competitive results.2By reducing the problem to the well understoodPCFG formalism, it also becomes easy to considerextensions, leading to our second contribution.
Wedemonstrate that a slight modification to our modelso that it also learns the language?s canonical wordorder improves its performance even beyond the bestresults previously reported in the literature.
Thislanguage-independent and linguistically well moti-vated elaboration allows the model to learn a globalfact about the language?s syntax, its canonical wordorder.Our contribution is two-fold.
We provide an illus-tration of how to reduce grounded learning tasks togrammatical inference.
Secondly, we show that ex-tending the model so that it can learn linguisticallywell motivated generalizations such as the canonicalword order can lead to better results.The structure of the paper is as follows.
First wegive a short overview of the previous work by Chen,Kim and Mooney and describe their dataset.
Then,we show how to reduce the parsing task addressedby them to a PCFG-learning task.
Finally, we ex-plain how to let our model additionally learn the lan-guage?s canonical word order.2 Previous Work by Chen, Kim andMooneyIn a series of recent papers, Chen, Kim and Mooneyapproach the task of learning a semantic parser fromambiguous training data (Chen and Mooney, 2008;Kim and Mooney, 2010; Chen et al, 2010).
Thisgoes beyond previous work on semantic parsingsuch as Lu et al (2008) or Zettlemoyer and Collins(2005) which rely on unambiguous training datawhere every sentence is paired only with its mean-ing.
In contrast, Chen, Kim and Mooney allowtheir training examples to exhibit the kind of uncer-tainty about sentence meanings human learners arelikely to have to deal with by allowing for sentencesto be associated with a set of candidate-meanings,2It has been pointed out to us by one reviewer that the taskwe address falls short of what is often called ?grounded learn-ing?.
We acknowledge that semantic parsing constitutes a verylimited kind of grounded learning but want to point out that thetask has been introduced as an instance of grounded learning inthe previous literature such as Chen and Mooney (2008).and the correct meaning might not even be in thisset.
They create the training data by first collect-ing humanly generated written language commentson four different RoboCup games.
The commentsare recorded with a time-stamp and then associatedwith all game events automatically extracted fromthe games which occured up to five seconds beforethe comment was made.
This leads to an ambigu-ous pairing of comments with candidate meaningsthat can be considered similar to the "linguistic in-put in the context of a rich, relevant, perceptual en-vironment" to which real language learners prob-ably have access (Chen and Mooney, 2008).
Forevaluation purposes, they manually create a gold-standard which contains unambiguous natural lan-guage comment / event pairs.
Due to the factthat some comments refer to events not detectedby their extraction-algorithm, not every natural lan-guage sentence has a gold matching meaning repre-sentation.
In addition to the inherent ambiguity ofthe training examples, the learner therefore has tosomehow deal with those examples which only have?wrong?
meanings associated with them.Datasets exist for both Korean and English, eachcomprising training and gold data for four games.3Some details about this data are given in Table 1,such as the number of examples, their average am-biguity and the number of misleading examples.For the following short discussion of previous ap-proaches, we mainly focus on Kim and Mooney(2010).
This is the most recent publication and re-ports the highest scores.2.1 The parsing taskLearning a semantic parser from the ambiguous datais, in fact, just one of three tasks discussed by Kimand Mooney (2010), henceforth KM.
In addition toparsing, they discuss matching and natural languagegeneration.
We are ignoring the generation task aswe are currently only interested in the parsing prob-lem, and we treat the matching task, picking the cor-rect meaning from the set of candidates, merely asa byproduct of parsing, rather than as a completelyseparate task: parsing implicitly requires the modelto disambiguate the data it is learning from.3The datasets are freely available at http://www.cs.utexas.edu/~ml/clamp/sportscasting/.
We re-trieved the data used here on March 29th, 2011.1417Number of comments Ambiguity# Training # Training withGold Match# Training withcorrect MR# Gold Noise Avg.
# of MRsEnglish datasettotal 1872 1492 1360 1539 0.2735 2.20Korean datasettotal 1914 1763 1733 1763 0.0946 2.39Table 1: Statistics for the Korean and the English datasets.
The numbers are basically identical to those reported inChen et al (2010) except for minimal differences in the number of training examples (we give one more for everyEnglish training set, and one more for the 2004 Korean training set).
In addition, our calculation of the averagesentential ambiguity (Avg.
# of MRs) differs because we assume that mutiple occurences of the same event in acontext do not add to the overall ambiguity, and our calculation of the noise (fraction of training examples withoutthe correct meaning in their context) takes into account that there are training examples which do not have their goldmeaning associated with them in the training data and is therefore slightly higher than the one reported in Chen et al(2010).KM?s model builds on previous work by Lu et al(2008) and is a generative model which defines ajoint probability distribution over natural languagesentences (NLs), meaning representations (MRs)and hybrid trees.
The NLs are the natural languagecomments to the games, the MRs are simple log-ical formulae describing game events and playingthe role of sentence meanings, and a hybrid tree isa tree structure that represents the correspondencebetween a sentence and its meaning.
More specif-ically, if some NL W has as its meaning an MRm, and m has been generated by a meaning gram-mar (MG) G, the hybrid tree corresponding to thepair ?W,m?
has as its internal nodes those rules ofG used in the derivation of m, and as its leaves thewords making up W.4 An example hybrid tree forthe pair ?THE PINK GOALIE PASSES THE BALL TOPINK11,pass(pink1,pink11)?
is given in Figure 1.Their model is trained by a variant of the Inside-Outside algorithm which deals with the hybrid treestructure and takes into account the ambiguity of thetraining examples.In addition to learning directly from the ambigu-ous training data, they also train a semantic parserin a supervised fashion on data that has been pre-viously disambiguated by their matching model.This slightly improves their system?s performance.Consequently, there are two scores for each of the4We use SMALL CAPS for words, sans serif for MRs andMR constituents (concepts), and italics for non-terminals andGrammars.SS?
pass PLAYER PLAYERPLAYERPLAYER?
pink11PINK11PASSES THE BALL TOPLAYERPLAYER?
pink1THE PINK GOALIEFigure 1: A hybrid tree for the sentence-meaningpair ?THE PINK GOALIE PASSES THE BALL TOPINK11,pass(pink1,pink11)?
.
The internal nodes cor-respond to the rules used to derive pass(pink1,pink11)from a given Meaning Grammar, and the leaves corre-spond to the words or substrings that make up the sen-tence.two languages (English and Korean) with whichwe compare our own model: those of the parserstrained directly from the ambiguous data, and thoseof the ?supervised?
parsers which constitute the cur-rent state of the art.
The details of their evaluationmethod are disccused in Section 3.3, and their scoresare given in Table 2, together with our own scores.3 Learning a Semantic Parser as aPCFG-learning problemGiven that one can effectively represent both a sen-tence?s form and its meaning in a hybrid tree, it is in-teresting to ask whether one can do with a structurethat can be learned by grammatical inference tech-1418niques from strings which incorporate the contextualinformation.
In this section, we show how to reducehybrid trees to such ?standard?
trees.
In effect, weshow via construction that ?grounded?
learning taskssuch as learning a semantic parser from semanticallyenriched and ambiguous data can be reduced to ?un-grounded?
tasks such as grammatical inference.Instead of taking the internal nodes of the treesgenerated by our model as corresponding to MGproduction rules, we take them to correspond to MRconstituents.
The MR pass(pink1,pink11), for exam-ple, has 4 constituents: the whole MR, the predicatepass, and the two arguments pink1 and pink11.
Fig-ure 2 gives the tree we assume instead of Figure 1for the sentence-meaning pair ?THE PINK GOALIEPASSES THE BALL TO PINK11,pass(pink1,pink11)?.Its root is assumed to correspond to the wholeMR and is labeled Spass(pink1,pink11).
The remain-ing three MR constituents correspond to the root?sdaughters which we label Phrasepink1, Phrasepassand Phrasepink11.
Generally speaking, we assume aspecial non-terminal Sm for every MR m generatedby the MG, and a special non-terminal Phrasecon foreach of the terminals of the MG (which loosely cor-respond to concepts).
This is only possible for MGswhich create a finite set of MRs, but the MG used byKim and Mooney (2010) obeys this restriction.5The tree?s terminals are the words that make upthe sentence, and we assume them to be dominatedby concept-specific pre-terminals Wordcon whichcorrespond to concept-specific probability distribu-tions over the language?s vocabulary.
Since eachPhrasecon may span multiple words, we give treesrooted in Phrasecon a left-recursive structure thatcorresponds to a unigram Markov-process.
Thisprocess generates an arbitrary sequence of wordssemantically related to con, dominated by the cor-responding pre-terminal Wordcon in our model, andwords not directly semantically related to con, dom-inated by a special word pre-terminal Word?.
Thesole further restriction is that every Phrasecon mustcontain at least one Wordcon.Trees like the one in Figure 2 can be generated bya Context-Free Grammar (CFG) which, in turn, canbe trained on strings to yield a PCFG which embod-5This grammar is given in the Appendix to Chen et al(2010) and generates a total of 2048 MRs.ies a semantic parser as will be discussed in Section3.3.
We now describe how to set up such a CFG in asystematic way and how to train it on the data usedby KM.3.1 Setting up the PCFGThe training data expresses information of two dif-ferent kinds ?
form and meaning.
Every training ex-ample consists of a natural language string (the for-mal information) and a set of candidate meaningsfor the string (the semantic information, its context),allowing for the possibility that none of the mean-ings in the context is the correct one.
In order tolearn from data like this within a grammatical in-ference framework, we have to encode the semanticinformation as part of the string.
Assigning a spe-cific MR m to a string corresponds, in our frame-work, to analyzing it as a tree with Sm as its root.A sentence?s context constrains which of the manypossible meanings might be expressed by the string.Thus the role played by the context is adequatelymodelled if we ensure that if a string W is associatedwith a context {m1,...,mn}, the model only considersthe possibilities that this string might be analyzed asSm1,...,Smn.There are 959 different contexts, i.e.
959 dif-ferent sets of MRs, in the English data set (984for the Korean data), and we therefore introduce959 new terminal symbols which play the role ofcontext-identifiers, for example C1 to C959.6 For-mally speaking, a context-identifier is a terminallike any other word of the language and we cantherefore prefix every comment in the training datawith the context-identifier standing for the set ofMRs associated with this comment, an idea takenfrom previous work such as Johnson et al (2010).Thus having incorporated the contextual informa-tion into the string, we go on to show how our modelmakes use of this information, considering the MRpass(pink1,pink11) as an example.
A formal de-scription of the model is given Figure 3.Assume that pass(pink1,pink11) is associatedwith only one training example and therefore occursonly in one specific context.
If the context-identifierintroduced for this context is C1, we require the6If we were to consider every possible context, we wouldhave to consider 22048 contexts because the MG generates 2048MRs.1419RootSpass(pink1,pink11)Phrasepink11PINK11PhrasepassWordpassTOPhXpassWord?BALLPhXpassWord?THEPhXpassWordpassPASSESPhrasepink1THE PINK GOALIEC76Figure 2: The tree-structure we propose instead of the Hybrid Tree structure used by (Kim and Mooney, 2010).
Thenon-terminal nodes do not correspond to MG productions, but to MR constituents.
The internal structure of thePhrasecon constituents, shown in full detail for Phrasepass, corresponds to a Markov process that generates the wordsthat make up the sentence.
The terminal C76 is a context-identifier that restricts the range of Sm non-terminals thatmight dominate the sentence and is only used during training, as described in Section 3.1.
The grammar that generatesthis trees is described in Figure 3.right-hand side of all rules with Spass(pink1,pink11) ontheir left-hand side to begin with C1.
More gener-ally, if an MR m occurs in the contexts associatedwith the context-identifiers CK,...,CL, we require theright-hand side of all rules with Sm on their left-handside to begin with exactly one of CK,...,CL.In this sense, the context-identifiers can be seenas providing the model with a top-down constraint?
if it encounters a context-identifier, it can onlytry analyses leading to MRs which are licensed bythis context-identifier.
On the other hand, the wordshave to be generated by concept-specific word-distributions, and the concepts that are present re-strict the range of possible Sm non-terminals whichmight dominate the whole string.
In this sense, thewords the model observes provide it with a bottom-up constraint ?
if it sees words which are semanti-cally related to certain concepts con1,...,conn, it hasto arrive at an MR which licenses the presence of thecorresponding Phraseconx non-terminals.
Of course,the model has to also learn which words are seman-tically related to which concepts.
To enable it to dothis, our grammar allows every Wordx non-terminalto be rewritten as every word of the language.Since there are sentences in the training data with-out the correct meaning in their context, we wantto give our model the possibility of not assigning toa sentence any of the MRs licensed by its context-identifier.
To do this, we employ another trick ofprevious work by Johnson et.
al and assume a spe-cial null meaning ?
to be present in every context.S?
may only span words generated by Word?, thelanguage-specific distribution for words not directlyrelated to any concept; this also has to be learned bythe model.As a last complication, we deal with the fact thatsyntactic constituents are linearized with respect toeach other.
For example, if an MR has 3 proper con-stituents (i.e.
excluding the MR itself), our grammarallows the corresponding 3 syntactic constituents ?which we might label Phrasepredicate, Phrasearg1and Phrasearg2 ?
to occur in any of the 6 possibleorders.
Therefore, we have an Sm rule for every con-text in which m occurs and for every possible orderof the proper constituents of m.A formally explicit description of the rule1420schemata used to generate the CFG is given in Fig-ure 3.7 Instantiating all those schemata leads to agrammar with 33,101 rules for the English data and30,731 rules for the Korean data.
The difference insize is due to differences in the size of the vocabu-lary and the different number of contexts in the datasets.These CFGs can now be trained on the trainingdata using the Inside-Outside algorithm (Lari andYoung, 1990).
After training, the resulting PCFGembodies a semantic parser in the sense that, witha slight modification we describe in section 3.3, itcan be used to parse a string into its meaning rep-resentation by determining the most likely syntacticanalysis and reading off the meaning assigned by ourmodel at the Sm-node.3.2 Possible objections to our reductionBefore we go on to discuss the details of trainingand evaluation of our model, we want to address anobjection that might seem tempting.
Isn?t our reduc-tion impractical and unrealistic as even a highly ab-stract model of language learning ?
after all, settingup the huge CFG requires knowledge about the vo-cabulary, the MG and all the complicated rules dis-cussed which, presumably, is more knowledge thanwe want to provide a language learner with, lest wetrivialize the task.
To this we reply firstly, that it istrue that our reduction only works for offline or batchgrounded learning tasks where all the data is avail-able to the model before the actual learning beginsso that it ?knows?
the words, the meanings and thecontexts present in the data.
This offline constraintis, however, true of all models which are trained byiterating multiple times over training data such asKM?s model.
Secondly, the intimidating CFG can inprinciple be reduced to a hand-full of intuitive prin-ciples and is easy to generate automatically.First of all, the many specific Sm-rewrite rules re-duce to the heuristic that every semantic constituentshould correspond to a syntactic constituent, and thefact that natural language expressions are linearly or-dered.
Note that our model does not contain knowl-edge about the specific word order of the language.7In our description, we use context-identifiers such as C1with a systematic ambiguity, letting them stand for the terminalsymbol representing a context and, in contexts such as m?C1,for the represented context itself.It simply allows for the constituents of an MR to oc-cur in every possible order which is a very unbiasedand empiricist assumption.
Of course, this leads tosome limited kind of ?implicit learning?
of word or-der in the sense that for every meaning and for everycontext, our model might (and in most cases will) as-sign different probabilities to the different rules forevery word order; so it can learn that certain specificMRs such as pass(pink1,pink11) are more often lin-earized in one way than in any other.
It cannot, how-ever, generalize this to other (or even unseen) MRs,i.e.
it does not learn a global fact about the language.In a way, it lacks the knowledge that there is such athing as word order, a point which we will elaborateon in Section 4.The many re-write rules for the pre-terminalWordxs are nothing but an explicit version of theassumption that every word the model encountersmight, in principle, be semantically related to everyconcept it knows.
Again, this seems to us to be areasonable assumption.Finally, the complicated looking set of rules forthe internal structure of Phrasexs corresponds toa simple unigram Markov-process for generatingstrings.
All in all, we do not see that we make anymore assumptions than other approaches; our for-mulation may make explicit how rich those assump-tions are but we have not qualitatively changed them.3.3 Training and EvaluationThe CFG described in the previous section is trainedon the same training data used by KM, except thatwe reduce it to strings (without changing the infor-mation present in the original data) by prefixing ev-ery sentence with a context-identifier.
For trainingwe run the Inside-Outside algorithm8 with uniforminitialization weights until convergence.
For En-glish, this results in an average number of 76 itera-tions for each fold, for Korean the average number ofiterations is 50.
To deal with the fact that the modelmight not observe certain meanings during training,we apply a simple smoothing technique by using aDirichlet prior of ?=0.1 on the rule probabilities.
Ineffect, this provides our system with a small numberof pseudo-observations for each rule which prevents8We use Mark Johnson?s freely available implementa-tion, available at http://web.science.mq.edu.au/~mjohnson/Software.htm.1421Root?
Sm m ?M ?
{?
}Sm ?
c Phrasep(m) c ?
C,m ?
c,m ?
Pred0(M)Sm ?
c {Phrasep(m), Phrasea1(m)} c ?
C,m ?
c,m ?
Pred1(M)Sm ?
c {Phrasep(m), Phrasea1(m), Phrasea2(m)} c ?
C,m ?
c,m ?
Pred2(M)S?
?
c Phrase?
c ?
CPhrase?
?Word?Phrase?
?
Phrase?Word?Phrasex ?Wordx x ?
TPhrasex ?
PhXxWordx x ?
TPhrasex ?
PhxWord?
x ?
TPhXx ?Wordr x ?
T, r ?
{x, ?
}PhXx ?
PhXxWordr x ?
T, r ?
{x, ?
}Phx ?
PhXxWordx x ?
TPhx ?
PhxWord?
x ?
TPhx ?Wordx x ?
TWordx ?
v x ?
T ?
{?
}, v ?
VFigure 3: The rule-schemata used to generate the NoWo-PCFG.
Root is the unique start-symbol, M is the set of allMRs present in the corpus, C is set the of all context-identifiers present in the corpus, T is the set of terminals of theMG, V is the vocabulary of the corpus.
Pred0(M) is the subset of all MRs in M of the form predicate, Pred1(M)is the subset of all MRs in M of the form predicate(arg1) and Pred2(M) is the subset of all MRs in M of the formpredicate(arg1,arg2).
p(m) is the predicate of the MR m, a1(m) is the first argument of the MR m, a2(m) is thesecond argument of the MR m. The rules expanding Phrasex ensure that it contains at least one Wordx.
A set on theright-hand side of a rule is shorthand for all possible orderings of the elements of the set.the automatic assignment of zero probability to rulesnot used during training.9For parsing, the resulting PCFG is slightly mod-ified by removing the context-identifiers.
This isdone because the task of a semantic parser is to es-tablish a mapping between NLs and MRs, irrespec-tive of contexts which were only used for learningthe parser and should not play a role in its final per-formance.
To do this, we add up the probability ofall rules which differ only in the context-identifierwhich can be thought of as marginalizing out the dif-ferent contexts, giving our first model which we callNoWo-PCFG.10Note that the context-deletion (and the simplesmoothing) enables NoWo-PCFG to parse sentencesinto meanings not present in the data it was trainedon which, in fact, happens.
For example, there are81 meanings in the training data for the first English9We experimented with ?=0.1, ?=0.5 and ?=1.0 and foundthat overall, 0.1 yields the best results.
We also tried jitteringthe initial rule weights during training and found that our re-sults are very robust and seem to be independent of a specificinitialization.10NoWo because this model, unlike the one described in Sec-tion 4, does not make explicit use of word order generalisa-tions.match that are not present in any of the other games?training data.
The PCFG trained on games 2, 3 and 4is still able to correctly assign 12 of those 81 mean-ings which it has not seen during the training phasewhich shows the effectiveness of the bottom-up con-straint.For evaluation, we employ 4-fold cross validationas described in detail in Chen and Mooney (2008)and used by KM: the model is trained on all possiblecombinations of 3 of the 4 games and is then usedto produce an MR for all sentences of the held-outgame for which there is a matching gold-standardmeaning.
For an NL W, our model produces an MRm by finding the most probable parse of W with theCKY algorithm and reading off m at the Sm-node.11An MR is considered correct if and only if it matchesthe gold-standard MR exactly; the final evaluationresult is averaged over all 4 folds.
Our evaluationresults for NoWo-PCFG are given in Table 2.
Allscores are reported in F-measure which is the har-monic mean of Precision and Recall.
In this specificcase, precision is the fraction of correct parses out11For parsing, we use Mark Johnson?s freely available CKYimplementation which can be downloaded at http://web.science.mq.edu.au/~mjohnson/Software.htm.1422English KoreanKM 0.742 0.764KM ?supervised?
0.810 0.808Chen et al (2010) 0.801 0.812NoWo-PCFG 0.742 0.718WO-PCFG 0.860 0.829Table 2: A summary of results for the parsing task, in F-measure.
We also show the results of Chen et al (2010),as given in Kim and Mooney (2010), which to our knowl-edge are the highest previously reported scores for Ko-rean.
WO-PCFG, described in Section 4 performs betterthan all previously reported models, but only slightly sofor Korean.of the total number of parses the model returns.
Re-call is the fraction of correct parses out of the totalnumber of test sentences.12NoWo-PCFG performs a little worse than KM?smodel.
Its scores are virtually identical for English(0.742) and worse for Korean (0.718 vs 0.764).
Weare not sure as to why our model performs worse onthe Korean data, but it might have to do with the factthat the Korean average ambiguity is higher than forthe English data.This shows that it is not only possible to re-duce the task of learning a semantic parser to stan-dard grammatical inference, but that this way of ap-proaching the problem yields comparable results.The remainder of the paper focuses on our secondmain point: that letting the model learn additionalkinds of information, such as the language?s canoni-cal word order, can further improve its performance.In order to do this we propose a model that learnsthe word order as well as the mapping from NLsto MRs, and compare its performance to that of theother models.4 Extending NoWo-PCFG to WO-PCFGWe already pointed out that our model considers ev-ery possible linear order of syntactic constituents.Our NoWo-PCFG model considers each of the pos-sible word orders for every meaning and context inisolation: it is unable to infer from the fact that mostmeanings it has observed are most likely to be ex-pressed with a certain word order that new meanings12Because our model parses every sentence, for it Recall andPrecision are identical and F-measure is identical to Accuracy.it will encounter are also more likely to be expressedwith this word order.
It seems, however, to be atleast a soft fact about languages that they do havea canonical word order that is more likely to be re-alized in its sentences than any other possible wordorder.
In order to test whether trying to learn thisorder helps our model, we modify the CFG used forNoWo-PCFG so it can learn word order generaliza-tions, and train it in the same way to yield anothersemantic parser, WO-PCFG.4.1 Setting up WO-PCFGFor every possible ordering of the constituents cor-responding to an MR, our grammar contains a rule.In NoWo-PCFG, these different rules all share thesame parent which prevents the model from learn-ing the probability of the different word orders cor-responding to the many rules.
A straight-forwardway to overcome this is to annotate every Sm nodewith the word order of its daughter.
We split everySm non-terminal in multiple Swo_m non-terminals,where wo ?
{v,sv,vs,svo,sov,osv,ovs,vso,vos} indi-cates the linear order of the constituents the non-terminal rewrites as.13This in itself does not yet alow our model to useword order as a means of generalization.
To modelthat whenever it encounters a specific example thatis indicative of a certain word order, this word or-der becomes slightly more probable for every otherexample as well, we have to make a further slightchange to the CFG which we now describe.
A for-mally explicit description of the necessary changeswhich we go on to describe is given in Figure 4.We introduce six new non-terminals, correspond-ing to the six possible word orders SVO, SOV, VSO,VOS, OSV and OVS and require every Swo_m non-terminal to be dominated by the non-terminal com-patible with its daughters linear order.
As an exam-ple, consider the two syntactic non-terminals cor-responding to the MR kick(pink1), Svs_kick(pink11)and Ssv_kick(pink11).
Whenever an example is suc-cessfully analyzed as Svs_kick(pink11), this shouldstrengthen our model?s expectation of encountering13We assume, somewhat simplifying, that an MR?s predicatecorresponds to a V(erb), its first argument corresponds to theS(ubject) and its second argument corresponds to the O(bject).These are purely formal categories that are not constrained tocorrespond to specific linguistic categories.1423Root?
wo wo ?WOwo?
Sx_m wo ?WO,x ?WOS, x ?
wo,m ?MSv_m ?
c Phrasep(m) c ?
C,m ?
c,m ?
Pred0(M)Sx_m ?
c {Phrasep(m), Phrasea1(m)} c ?
C,m ?
c,m ?
Pred1(M), x ?
{sv, vs}Sx_m ?
c {Phrasep(m), Phrasea1(m), Phrasea2(m)} c ?
C,m ?
c,m ?
Pred2(M), x ?WOSSv_?
?
c Phrase?
c ?
CFigure 4: In order to turn NoWo-PCFG described in Figure 3 into the WO-PCFG described in thetext, substitute the first five rule-schemata with the schemata given here.
WO is the set of wordorder non-terminals {SV O, SOV,OSV,OV S, V SO, V OS}, WOS is the set of word order annotations{v, sv, vs, svo, svo, ovs, osv, vso, vos}.
We take x ?
wo to mean that x is compatible with wo, where v is com-patible with all word orders, sv is compatible with SVO,SOV and OSV, and so on.
For rule-schemata 4 and 5, thechoice of x determines the order of the elements of the set on the right-hand side.
All other symbols have the samemeaning as explained in Figure 3.more examples where the verb precedes the sub-ject, i.e.
of the language being pre-dominantly VSO,VOS or OVS.
Therefore, we allow VSO, VOS andOVS to be rewritten as Svs_kick(pink11).
More gener-ally, every word order non-terminal can rewrite asany of the Swo_m non-terminals that are compatiblewith it.
Adding this additional layer of word orderabstraction leads to a grammar with 36,019 rules forEnglish and a grammar with 33,715 rules for Ko-rean.4.2 Evaluation of WO-PCFGTraining and evaluating WO-PCFG in exactly thesame way as the previous grammar gives an F-measure of 0.860 for English and an F-measure of0.829 for Korean.
Those scores are, to our knowl-edge, the highest scores previously reported for thisparsing task and establish our second main point:letting the model learn the language?s word order inaddition to learning the mapping from sentences toMR increases semantic parsing accuracy.14An intuitive explanation for the increase in perfor-mance is that by allowing the model to learn wordorder, we are providing it with a new dimensionalong which it can generalize.In this sense, we can look at our refinement asproviding the model with abstract linguistic knowl-edge, namely that languages tend to have a canon-14Liang et al (2009)?s model can be seen as capturing some-thing similar to our word order generalization with the help ofa Field Choice Model which primarily captures discourse co-herence and salience properties.
It differs, however, in that itcan only learn one generalization for each predicate type andno language wide generalization.ical word order.
The usefulness of this kind of in-formation is impressive ?
for English, it improvesthe accuracy of semantic parsing by almost 12% inF-measure and for Korean by 11.1%.
In addition,our model correctly learns that English?s predomi-nant word order is SVO and that Korean is predomi-nantly SOV, assigning by far the highest probabilityto the corresponding Root rewrite rule (0.91 for En-glish and 0.98 for Korean).
This kind of informationis useful in its own right and could, for example, beexploited by coupling word order with other linguis-tic properties, perhaps following Greenberg (1966)?simplicational universals.In this sense, the reduction of grounded learningproblems to grammatical inference does not onlymake possible the application of a wide variety oftools and insights developed over years of research,it might also make it easier to bring abstract (and notso abstract) linguistic knowledge to bear on thosetasks.The overall slightly worse performance of oursystem on Korean data might stem from the fact thatKorean, unlike English, has a rich morphology, andthat our model does not learn anything about mor-phology at all.
We plan on further investigating ef-fects like this in the future, as well as applying moreadvanced grammatical inference algorithms.5 Conclusion and Future WorkWe have shown that certain grounded learning taskssuch as learning a semantic parser from semanticallyenriched training data can be reduced to a gram-matical inference problem over strings.
This allows1424for the application of techniques and insights devel-oped for grammatical inference to grounded learn-ing tasks.
In addition, we have shown that lettingthe model learn the language?s canonical word or-der improves parsing performance, beyond the topscores previously reported, thus illustrating the use-fullnes of linguistic knowledge for tasks like this.In future research, we plan to address the limi-tation of our model to a finite set of meaning rep-resentations, in particular through the use of non-parametric Bayesian models such as the InfinitePCFG model of Liang et al (2007) and the Infi-nite Tree model of Finkel et al (2007); both allowfor a potentially infinite set of non-terminals, hencedirectly addressing this problem.
In addition, weare thinking about using an extension of the PCFGformalism that allows for some kind of ?feature-passing?
which could lead to much smaller and moregeneral grammars.ReferencesN.
C. Chang and T. V. Maia.
2001.
Grounded learningof grammatical constructions.
In 2001 AAAI SpringSymposium on Learning Grounded Representations.David L. Chen and Raymond J. Mooney.
2008.
Learningto sportscast: A test of grounded language acquisition.In Proceedings of the 25th International Conferenceon Machine Learning (ICML).David L. Chen, Joohyun Kim, and Raymond J. Mooney.2010.
Training a multilingual sportscaster: Using per-ceptual context to learn language.
Journal of ArtificialIntelligence Research, 37:397?435.Jenny R. Finkel, Trond Grenager, and Christopher D.Manning.
2007.
The infinite tree.
In Proceedingsof the 45th Annual Meeting of the Association of Com-putational Linguistics, pages 272?279.Joseph H. Greenberg.
1966.
Some universals of gram-mar with particular reference to the order of meaning-ful elements.
In Joseph H. Greenberg, editor, Univer-sals of Language, chapter 5, pages 73?113.
The MITPress, Cambridge, Massachusetts.Mark Johnson, Katherine Demuth, Michael Frank, andBevan Jones.
2010.
Synergies in learning wordsand their referents.
In J. Lafferty, C. K. I. Williams,J.
Shawe-Taylor, R.S.
Zemel, and A. Culotta, editors,Advances in Neural Information Processing Systems23, pages 1018?1026.Joohyun Kim and Raymond J. Mooney.
2010.
Genera-tive alignment and semantic parsing for learning fromambiguous supervision.
In Proceedings of the 23rd In-ternational Conference on Computational Linguistics(COLING 2010).K.
Lari and S.J.
Young.
1990.
The estimation of Stochas-tic Context-Free Grammars using the Inside-Outsidealgorithm.
Computer Speech and Language, 4(35-56).Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.2007.
The infinite PCFG using hierarchical Dirichletprocesses.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 688?697.Percy Liang, Michael I. Jordan, and Dan Klein.
2009.Learning semantic correspondences with less supervi-sion.
In Proceedings of the 47th Annual Meeting of theACL and the 4th IJCNLP of the AFNLP.Wei Lu, Hwee Tou Ng, Wee Sun Lee, and Luke S. Zettle-moyer.
2008.
A generative model for parsing natu-ral language to meaning representations.
In EmpiricalMethods in Natural Language Processing (EMNLP),pages 783?792.Luke S. Zettlemoyer and Michael Collins.
2005.
Learn-ing to map sentences to logical form: Structured clas-sification with probabilistic categorial grammars.
InProceedings of UAI 2005.1425
