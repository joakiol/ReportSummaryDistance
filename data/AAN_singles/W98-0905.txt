An Approach to the Automat ic  Acquisit ion of Phonotact icConstraintsAnja  Be lzSchool of Cognitive and Computing SciencesUniversity of SussexBrighton BN1 9QH, UKanj ab?cogs, susx.
ac.
ukAbstractThis paper describes a formal approach and apractical learning method for automatically ac-quiring phonotactic constraints encoded as fi-nite automata.
It is proposed that the use ofdifferent classes of syllables with class-specificintra-syl\]abic phonotactics results in a more ac-curate hypothesis of a language's phonologicalgrammar than the single syllable class tradi-tionally used.
Intra-syllabic constraints are en-coded as acyclic finite automata with input al-phabets of phonemic symbols.
These automatain turn form the transitions in cyclic finiteautomata that encode the inter-syllabic con-straints of word-level phonology.
A genetic al-gorithm is used to automatically construct finiteautomata from training sets of symbol strings.Results are reported for a set of German syl-lables and a set of Russian bisyllabic femininenouns .1 Background1.1 Phonotact ic  Descr ipt ionIn recent years, phonology - -  partly under theinfluence of computational models - -  has movedaway from procedural, rule-based approachestowards explicitly declarative statements of theconstraints that hold on possible phonologicalforms.
Such statements form sets of constraintsthat apply at a given level of description, andill-formedness i often defined as constraint vi-olation.Phonotactic descriptions state the constraintsthat hold for possible sequences of phonetic orphonemic features or symbols, usually at thelevel of the syllable (more rarely for onset, peakand coda separately).
Phonological words aredefined as sequences of at least one syllable.A phonotactic description is typically thoughtto be adequate only if it generalises beyond theset of phonological forms that exist in a lan-guage to a superset of possible forms that alsoincludes forms that could exist but do not.
Thisdistinction between non-existent but possibleforms on the one hand, and non-existent andimpossible forms on the other, is often describedin terms of accidental vs. systematic gaps (e.g.Carson-Berndsen (1993), Gibbon (1991), andoriginally Chomsky (1964)).Carson-Berndsen (1993) lists five encodingschemes for phonotactic description at the sylla-ble level found in the literature: templates thatmerely state the number of consonants permit-ted in the onset and coda, distribution matriceswith a separate matrix for each type of conso-nant duster, enhanced templates which add thenotion of phoneme classes, feature-based phono-tactic networks using feature bundles, natu-ral classes, variables, defaults and underspeci-fication, and phrase-structure ules which havethe same (potential) representative power asfeature-based phonotactic networks.In the approach presented here, finite stateautomata (FSA) encoding is preferred overthese other schemes, since they can an equiv-alently be represented by FSAs - -  what is de-scribed is always a regular language - -  in themost general sense of the term, and since finite-state machinery in itself does not have the dis-advantage of necessarily overgenerating forms(as do templates), or of excluding the possibil-ity of multi-tier description (as do most of theabove).1.2 FSAs and Phonotact ic  Descr ipt ionFSAs have been used to encode syllable phono-tactics e.g.
to reduce the search space for lexi-cal hypotheses (Carson-Berndsen, 1993) and todetect unknown words (Jusek et al, 1994) in35speech recognition, but are usually constructedin a painstaking manual process.
The aimof the research presented here is to develop acompletely automatic method for constructingphonotactic descriptions.
This requires a for-mal theoretical approach to the task as well as apractical automatic inference method.
The for-mer is outlined in the following section, whileSection 3 describes the genetic algorithm devel-oped for the latter.
The remainder of this sec-tion briefly summarises the standard FSA no-tation and definitions used in this paper as wellas some non-standard usages.Following (Hopcroft and Ullman, 1979, p.17),?
a deterministic finite-state automaton A is a 5-tuple (S, I ,  ~, so, F), in which S is a finite set ofstates, I is a finite input a alphabet, ~ is thestate transition function mapping S x I to S,so E S is the initial state, and F C S is the setof final states.
For every state s E S and everysymbol a E I, ~(s, a) is a state.
Ordinarily, theS-notation (sometimes ~) is also used for theinput of strings x E I*, such that ~ maps S ?
I*to S. The language L accepted by A, denotedL(A), is {xl~(s0 ,x) E F}.The transition function ~(s, a) is often repre-sented as a 2-dimensional state transition ma-trix, and (in contrast o most related researchwhich uses sets of production rules of the formSl --  as2) this matrix is used to represent FSAsin the learning method described in Section 3.The term FSA is taken to refer to n-leveltransducers, where the input alphabet consistseither of individual symbols a E I or of stringsx E I*.
Although only experiments for l-leveltransducers with labels a E I have been carriedout so far, the approach will be extended to thegeneral case, permitting multi-tier phonologicaldescription.
Another type of FSA is used whichcan be considered a further generalisation step,i.e.
FSAs where the transitions are themselvesFSAs.2 Formal  Approach  to  theAutomat ic  Acqu is i t ion  ofPhonotact i cs2.1 Syl lable ClassesThe phonological word is usually defined as asequence of syllables, in fact not taking thisgeneral approach would mean ignoring a ba-sic phonological regularity (the standard argu-meats in favour of the syllable are summarisede.g.
in Blevins (Blevins, 1994)).
Phonologicaldescription has, as a rule, described syllables interms of a single structure consisting of smallerunits of description (usually onset, peak andcoda) on which certain constraints hold, andwords as sequences of one or more occurrencesof this structure, on which by assumption ofurther constraints hold.
In many languages,however, word-initial and/or word-final conso-nant clusters differ from other consonant clus-ters with regard to (co-)occurrence onstraints.Goldsmith (1990, p. 107if) lists several exam-ples from different languages.
This has resultedin the use of the notion of extrasyllabicity toaccount for 'extra' consonantal segments at thebeginnings and the ends of words.
Similar prob-lems occur with regard to tonal and metricalregularities, where the first and/or the last vow-els in words are often referred to as 'extratonal'and/or 'extrametrical '1.There are two problems here.
The first isthat if a phonological theory assumes a sin-gle syllable class for a language and if the lan-guage has idiosyncratic word-initial and word-final phonotactics, then the set of possible wordsthat the theory hypothesises i  necessarily toolarge, and includes words that form systematic(rather than accidental) gaps in a language.The second problem is that if extrasyllabic-ity is used to reduce the first problem, thenthe resulting theory of syllable structure failsto account for everything that it is intended toaccount for, and is forced to integrate xtrasyl-labic material directly at the word level.Furthermore, it is likely that all languagesdisplay some phonological idiosyncracy at thebeginnings and/or ends of phonological words.For these reasons, it seems more practical tomake the general assumption that a word is ofthe form SISM*SF (where $1 stands for initialsyllable, SM for medial syllable, and SF for finalsyllable 2.
These basic syllable classes with dif-ferent associated sets of phonotactic constraintsenable the integration at the syllable level of seg-1E.g.
in the case of Kirundi, where words with aninitial vowel have no tone assigned to the first vowelby word-level phonology, and in Central Siberian Yup'ikwhere final syllables are never stressed (Goldsmith, 1990,p.
29 and p.179 respectively).2I am grateful to John Goldsmith for advice on thismatter.36Total number  of word forms: 408,603Uniquely occurring syllables:TOTAL: 9,851Initial: 3,851Medial: 3,858Final: 7, 119Monosyllabic words: 5,120Intersections:InitialMedialFinalMedial Final Mono2,626 1,478 1,3941,946 1,1873,877Initial n Medial n Final : 1092Initial I'1 Medial n Final Cl Mono : 728Hypothesised sets of possible German words:length S + S, SM*SF (Smono) CELEX1 9,851 5,120 5,1202 9.7 * 10 7 2.74 * l0 T 54,7543 9.55 * 1011 1.05 * 1011 105,0314 9.41 * 1015 4.08.1014 90,278Figure 1: German syllable statistics (fromCELEX).ments traditionally accounted for by extrasyl-labicity, and result in a more accurate hypothe-sis of a language's set of possible words.
Mono-syllabic words - -  often highly idiosyncratic 3 - -may have to be accounted for separately, by asyllable class Smono.Consider as an example the syllable statisticsfrom the German part of the lexical databaseCELEX (Baayen et al, 1995) shown in Fig-ure 1.
The statistics of set sizes and intersec-tions suggest hat 4 syllable classes are neededfor German (initial, medial, final and monosyl-lables).
Hypotheses for possible German wordsbased on a single syllable class (5 :+) would ar-rive at much larger word sets than a hypothesisbased on 4 syllable classes, and because it over-generates, the theory would not reflect some ofthe phonotactic onstraints that the statisticssuggest hold for German.In addition to word-initial and word-final po-ZDafydd Gibbon, personal communication.sition, syllables may have idiosyncratic phono-tactics as a result of tone and stress ef fects  4.It therefore seems natural to propose language-specific syllable class systems where eachclass has its own set of phonotactic con-straints (intra-syllabic onstraints) assigned toit.
Words can then be defined as sequencesof syllables, where language-specific 'syllable-tactics' (inter-syllabic constraints) constrain thepossible combinations of syllables from differ-ent classes, and hence the possible phonologicalforms of words .2.2 Syllabic SectionsFor an automatic method of constructingphonotactic descriptions, the syllable as a unitof description is problematic in that the meth-ods available for syllabification have recourse tomorphological knowledge, an underlying, moreabstract, underspecified level of description,and/or involve the notion of extrasyllabicity,and generally tend to require an amount ofprior knowledge of language-specific phonotac-tics that is unacceptable where the aim is todiscover these very constraints automatically.The main problems with syllabification arisefrom difficulties in assigning consonantal seg-ments to exactly one syllable, or drawing unam-biguous syllable boundaries between adjacentcodas and onsets.
Locating syllable peaks, ordividing lines between vocalic and consonantalsegments (distinguishable in the acoustic signal)is less problematic, and the approach to wordsegmentation proposed here involves utilisingthe relative ease with which peak boundariescan be located 6.
This requires the introduc-tion of the term syl labic sect ion  to describe agrouping of phonological segments consisting ofa peak and the consonantal material between itand either the preceding or the following peak.While the resulting sections are not syllables inthe traditional sense, they are syllabic in thatthey form single stress and tone-bearing units.4An example from Russian is that the vowel/o/onlyoccurs in the peak of stressed syllables.5Stress and tone effects are ignored in this paper, asthey are the subject of ongoing research.
*Ambiguous material such as glides on peak bound-aries poses no problem as long as it is consistentlygrouped either with the peak or with the surroundingconsonantal material.372.3 Learn ing  TaskPhonological words are thus analysed in termsof intra-syllabic and inter-syllabic onstraints asdescribed in Section 2.1, while the traditionalsyllable is replaced by syllabic sections for rea-sons outlined in the last section.In some languages (such as German, e.g.
inthe analysis from (Jusek et al, 1994) shown inFigure 3) only peak and coda constrain eachother, while in other languages (such as Rus-sia.n) only onset and peak are mutually con-strained (e.g.
Halle, (1971)).
The third possi-bility is that both types of constraints occur inthe same language.In order to allow for all three possibilities,the following approach is taken: each wordin a given training sample is scanned andsegmented in two ways, once by division beforethe peak and once after.
This two-way wordsegmentation results in the following twoanalyses:Initial Medial Final(or*) (VM+CM*)" (VF+cF ") +)In both cases, the initial and final sectionstogether contain three subsections that can beinterpreted as the onset, peak and coda of a tra-ditional syllable, which makes it possible to usethe same analysis to account for words of arbi-trary length, including monosyllables if appro-priate.
This approach also has the advantagethat it can incorporate constraints that crossthe boundaries of traditional syllables, such asassimilation phenomena.For a given training sample of words, in thefirst scan, all initial syllabic sections resultingfrom the word segmentation described above aregrouped together in data set D1, all final sec-tions in data set D3, and all remaining sections(regardless of how many result from each word)in D2.
The same process results in data setsD4-D6 for the second scan.
The learning taskis then to automatically construct an acyclicFSA on the basis of each data set, resulting insix automata A1-A6.
Two cyclic automata C1and C2 are then constructed (corresponding tothe two scans) that have the following structure,where A1 and A4 correspond to Ai, A2 and A5to AM and A3 and A6 to AF:A_MThe final result is a hypothesis of the (word-level) phonological grammar of a given lan-guage, based on a given training sample, en-coded by the intersection of C1 and C2 (i.e.
aword has to be accepted by both in order to beconsidered well-formed).
The present discussionis restricted to a basic syllable class system, butit is likely that descriptive accuracy can be fur-ther improved by extending this basic systemto include tone and stress effects.
This wouldof course result in more complex automata C1and C2 (trivially inferrable here).3 Learn ing  Method3.1 BackgroundThe  Grammat ica l  In fe rence  Prob lemGenerally, the problem considered here is thatof identifying a language L from a fixed finitesample D = (D+,D- ) ,  where D + C L andD-  NL  = 0 (D-  may be empty).
If D -  isempty, and D + is structurally complete withregard to L, the problem is not complex, andthere exist a number of reliable inference algo-rithms.
If D + is an arbitrary strict subset of L,the problem is less clearly defined.
Since any fi-nite sample is consistent with an infinite numberof languages, L cannot be identified uniquelyfrom D +.
"...the best we can hope to do is toinfer a grammar that will describe the stringsin D + and predict other strings that in somesense are of the same nature as those containedin D +'', (Fu and Booth, 1986, p.345).To constrain the set of possible languagesL, the inferred grammar is typically requiredto be as small as possible, in accordance witha more general principle of machine learningwhich holds that a solution should be the short-est or the most economical description consis-tent with all examples, as e.g.
suggested inMichalski (1983).
However, the problem of find-ing a minimal grammar consistent with a givensample D was shown to be NP-hard by Gold(1978).
Li & Vazirani (1988), Kearns ~ Valiant(1989) and Pitt & Warmuth (1993) have added38nonapproximability results of varying strength.In the special case where D contains ail stringsof symbols over a finite alphabet I of lengthshorter than k, a polynomial-time algorithm canbe found (Trakhtenbrot and Barzdin, 1973), butif even a small fraction of examples is missing,the problem is again NP-hard (Angluin, 1978).Genet ic  Search Given the nature of the in-ference problem, a search algorithm is the obvi-ous choice.
Genetic Algorithms (GAs) are par-ticularly suitable because search spaces tend tobe large, discontinuous, multimodal and high-dimensional.
The power of GAs as general-purpose search techniques derives partly fromtheir ability to efficiently and quickly searchlarge solution spaces, and from their robustnessand ability to approximate good solutions evenin the presence of discontinuity, multimodai-ity, noise and highdimensionality n the searchspace.
The most crucial difference to othergeneral-purpose arch and optimisation tech-niques is that GAs sample different areas of thesearch space simultaneously and are thereforeable to escape local optima, and to avoid poorsolution areas in the search space altogether.Re la ted  Research  A number of results havebeen reported for inference of regular andcontext-free grammars with evolutionary tech-niques, e.g.
by Zhou & Grefenstette (1986),Kammeyer & Belew (1996), Lucas (1994),Dupont (1994), Wyard (1989) and (1991).
Re-sults concerning the inference of stochasticgrammars with genetic algorithms have beendescribed by Schwehm & Ost (1995) and KellerLutz (1997a) and (1997b) describe.
Muchof this research bases inference on both nega-tive and positive examples, and no real linguis-tic data sets have been used.
Genotype repre-sentation is always based on sets of productionrules, and knowledge of the target grammar isoften utilised.
Of these, Zhou & Grefenstetteis the one approach directly comparable to thepresent method, and some comparative resultsare given in Section 4.3.2 The  Genet ic  A lgor i thmThe present algorithm 7 maintains a populationof individuals represented by genotypes of vary-rThe algorithm described here was developed in col-laboration with Berkan Eskikaya, School of Cognitiveand Computing Sciences, University of Sussex.ing length which are initialised to random genevalues and length.
In the iteration typical ofGAs, individuals are (1) evaluated accordingto the fitness function, (2) selected for repro-duction by a process that gives fitter individ-uals a higher chance at reproduction, (3) off-spring are created from two selected individu-als by crossover and mutation, and (4) weakerparents are replaced by fitter offspring.
Thesesteps are repeated until either the populationconverges (when the genotypes in the popula-tion have reached a degree of similarity beyondwhich further improvement is impossible), orthe nth generation is reached.The remainder of this section outlines the fit-ness function (corresponding to the evaluationfunction common to all search techniques), anddescribes how generalisation over the trainingset is achieved.
Full details of the GA can befound in Belz ~ Eskikaya (1998).F i tness  Eva luat ion  The fitness of automatais evaluated according to 3 fitness criteria thatassess (C1) consistency of the language coveredby the automaton with the data sample, (C2)smallness and (C3)generalisation to a super-set of the data sample.
For the evaluation ofC1, the number of strings in the data sam-ple that a given automaton parses are counted.Partial parsing of prefixes of strings is also re-warded, because the acquisition of whole stringsby the automata would otherwise be a matter ofchance.
Size (C2) is assessed in terms of num-ber of states, the reward being higher the fewerstates an FSA has.
This criterion serves as anadditional pressure on automata to have fewstates, although the number of states and, moreexplicitly, the number of transitions is alreadykept low by crossover and mutation.
General-isation (C3) is directly assessed only in termsof the size of the language covered by an au-tomaton, where the reward is higher the closerlanguage size is to a specified target size (ex-pressing a given degree of generalisation).When the goodness of a candidate solutionto a problem, or the fitness of an individual, ismost naturally expressed in terms of several cri-teria, the question arises how to combine thesecriteria into a single fitness value, or, alterna-tively, how to compare several individuals ac-cording to several criteria, in a way that accu-rately reflects the goodness of a candidate solu-39tion.
In the present context, trial runs showedthat the structural and functional properties ofsolution automata re very directly affected byeach of the three fitness criteria described above.Therefore, it was most natural to normalise thethree criteria to make up one third of the fit-ness value each, but to attach weights to themwhich can be manipulated (increased and de-creased) to affect the structural and functionalcharacteristics of resulting automata.Raising the weight on a fitness criterion (in-creasing its importance relative to the other cri-teria) has very predictable ffects, in that thecriterion with the highest weight is most reli-ab ly  satisfied.
Lowering the weight on C3 to-wards 0 has the result that language size be-comes unpredictable, while lowering the weighton C2 simply increases the average size of theresulting automata.
The weight on C1 tendsto have to be increased with increasing samplesize.Genera l isat ion There are two main parame-ters that influence the degree of generalisationa given population achieves: the fitness criteriaof size (C2) and degree of overgeneration (C3).C2 encourages automata to be as small as pos-sible, which - -  in the limit - -  leads to universalautomata that parse all strings x E I*.
This iscounterbalanced by C3 which limits the numberof strings not in the training set which automataare permitted to overgenerate.
To control thequality of generalisation, transitions that arenot used by any member of the training set areeliminated, because automata would otherwiseaccept arbitrary strings in addition to trainingset members to make up the required target lan-guage size.The overall effect is that a range of general-isation can be achieved over the training set,from precise training set coverage towards uni-versal automata, while meaningless overgenera-tion of strings is avoided.
When L(A) = train-ing set, only symbols a E I with identical distri-butions in the data set can be grouped togetheron the same transition between 2 states.
Asthe required degree of generalisation i creases,symbols with the most similar distributions aregrouped together first, followed by less similarones .Figure 2 shows an example of what effectscan be achieved in the limit.
The bottom dia-gram is part of the best automaton discoveredfor the second half of the German reduced syl-lable set, shown in Figure 4.
Here, the degree ofovergeneration was set to 1 (i.e.
L(A) = train-ing set), and the size criterion C2 had a smallweight.
This resulted in generalisation beingcompletely absent, i.e.
the automaton generatesonly nasal/consonant combinations that actu-ally occur in the data set.The top diagram in Figure 2 shows the effectof having a large weight on the size criterion,and increasing target language size.
The nasalswere consistently grouped together under thesecircumstances, because there is a higher de-gree of distributional similarity (in terms of thesets of phonemes that can follow) between m,n, N than between these and other phonemes.This achieves the effect that strings not in thedata set can be generated in a linguistically use-ful way, but also may have the side-effect thatrarer phoneme combinations (m\[p:f\], n\[ 'ts\] ,etc.)
are not be acquired, an effect that is de-scribed in (Belz, 1998).4 Resu l tsThis section summarises the results that havebeen achieved for complete presentation of datafor finite languages (Section 4.1), and incom-plete presentation of data for finite (Section 4.3)and infinite languages (Section 4.2).The last example (Section 4.3) illustrates howthe GA method can be used in conjunctionwith the formal approach described in Section 2to automatically discover word-level phonotac-tic descriptions from raw data sets of phonemestrings.4.1 German SyllablesHere, the aim was to discover an FSA thataccepts a known finite language precisely, sothat (following preliminary small-scale tests)the efficiency of the algorithm could be assessedagainst a medium-sized known language.
Thedata set was generated with the finite-state au-tomaton used by 3usek et al (1994) to representthe phonotactics of reduced German syllables(shown in Figure 3, double circles indicate fi-nal states).
This automaton accepts a languageof around 11,000 syllables, and because a train-ing set of this size is computationally infeasible,the automaton was divided in half (where thefirst half covers phonemes preceding the syllable40.... .-o m.N .?
.
@Figure 2: Effect of generalisation  phoneme groupings.peak, and the second half covers the remainder),and  the corresponding strings were used as sep-arate training sets (the training set for the firsthalf contained 127 strings, that for the second82).Results are summarised in Table 1, and thebest automaton that was found is shown in Fig-ure 4.
Algorithms based on Hopcroft & Ullman(1979, equivalence proofs pp.
26-27 and 22-23, minimisation algorithm p.70) to eliminatee-transitions, determinise and then minimisethe original manually constructed automatonshowed that the automatically discovered FSAsare the minimal deterministic s equivalents ofthe two halves of the original automaton, andtherefore represent optimal solutions.These results show that the GA is able to lo-cate optimal solutions (although it is not guar-anteed to find them), and that once an optimumhas been found, the population also convergeson it, rather than on another, suboptimal solu-tion.4.2 Some Non-linguistic ExamplesIn order to assess the performance of the GAon known infinite languages, and to compareit to another GA-based technique with similaraims, experiments were carried out for four pre-viously investigated learning tasks (Zhou andGrefenstette, 1986).
Task 1 was to discoverthe language (10)', Task 2 was 0"1"0"1", Task3 was "all strings with an even number of 0'sand an even number of l's", and Task 4 was"all strings such that the difference between thenumber of l's and 0's is 3 times n (where n isan integer)".
Zhou & Grefenstette used bothSThe 2-dimensional transition matrices that are usedas genotypes ncode only deterministic FSAs.Target found Z&GTask 'L' 'S' at generation TrialsBest Avg1 4 2 1 2 9802 7 161 23 29 10273 6 42 87 110 28204 4 10 69 90 2971Table 2: Results for non-linguistic examples.positive and negative xamples, and moreoverhad to "modify the training examples to makethe genetic algorithm 'understand' what a per-fect concept should be" (p.172).
The presentapproach used positive data only, consisting ofall strings up to a certain length L, resulting indatasets of varying size S. L was incrementedfrom 0 until the first value was found for which5 random runs produced the target automatonin under 200 generations.In all four cases, the task was to generaliseover the data set, by discovering the recursivenature of the target language from a small sub-set of examples.
Results are summarised in Ta-ble 2.
Zhou & Grefenstette measure the amountof time it takes for a target automaton to be dis-covered in terms of 'number of trials', but failto explain exactly what this refers to.
It canprobably be assumed to refer to the number ofgenerations, as this is the way genetic searchperformance is usually measured.
Given this in-terpretation, the present method outperformedthe earlier approach in all cases.
However, themain point is that the method described herediscovers the target FSAs without reference tonegative xamples or manipulation of the datasamples.
This second set of results shows that41FrontBackConsistency Overgeneration(Number of strings)Best 0AverageBestAverage127(100%)127(100%)82(100%)82(100%)States1 2.60 90 9.7Transitions Best foundat generation36 7834.0 9361 36061.5 780Table 1: Results for set of German reduced syllables.~'r.,.k.h.tL+.m.n.l+l.t L~lh~l.ch.~Lj+'d+r_ ~ ' -  f '~  fxS.P.T.K.m.n.N.Is.IISI.IL~\].ip~ -~ntLS.h" ~ ~ 3 I ea \~c.xT.~Figure 3: Manually constructed automaton for German reduced syllables.the GA can find optimal solutions for infinitelanguages.4.3 Russ ian  DataFor the third experiment the words in a dataset of 450 bisyllabic feminine Russian nounswere divided into sections in the way describedin Section 2.3.
This resulted in five learningsets D1-D5 (because the set of final consonantsis empty in this training set).
For each ofthe five learning sets, five automata were in-ferred that precisely generate the learning set.On the basis of these, the degree of generalisa-tion that could be expected given the trainingsets was estimated, and five automata A1-A5generalising to between 1.5 and 2.5 times thesize of their respective learning sets were thenevolved.
Finally, two automata C1 and C2 wereconstructed to encode inter-syllabic constraints,with labels A1-A5 representing the automatathat encode intra-syllabic onstraints:The resulting phonological grammar hypoth-esis (the intersection of the two automata en-coding inter-syllabic onstraints) accepted allwords from the original earning set of 450 Rus-sian bisyllabic feminine nouns, generalised to atotal set of words of around 10 times this size,and accepted ca.
85% of nouns from a testing setof 200 different such nouns.
Generalisation wasalmost always meaningful in that the greaterthe similarity between two phonemes (in termsof the phoneme sequences that can follow them),the higher was the hkelihood that they weregrouped together on the same transition.5 Conc lus ion  and  Fur ther  ResearchThis paper introduced a formal theoretical ap-proach to the automatic acquisition of phono-tactic constraints encoded as finite-state au-tomata, and described a genetic-search methodfor the construction of such automata.
Resultsshow that the method is reliably successful inconstructing FSAs that accurately cover train-ing samples and allow a range of generalisa-42)Figure 4: Best automatically discovered automata for German reduced syllables.tion over the learning samples.
The approachto phonotactic description involving several syl-lable classes that is proposed in this paper islikely to enable a more accurate account of pos-sible phonological forms in a language than ap-proaches that assume a single syllable class.
Fu-ture research will focus on developing word-levelphonotactic descriptions for larger datasets ofGerman and Russian words, and extending theapproach to descriptions incorporating tone andstress effects.ReferencesD.
Angluin.
1978.
On the complexity of min-imum inference of regular sets.
Informationand Control, 39:337-350.R.
H. Baayen, R. Piepenbrock, and L. Gulikers,editors.
1995.
The CELEX Lexical Database(CD-ROM).
Linguistic Data Consortium,University of Pennsylvania, Philadelphia, PA.A.
Belz and B. Eskikaya.
1998.
A genetic al-gorithm for finite-state automaton i duction.Cognitive Science Research Paper 487, Schoolof Cognitive and Computing Sciences, Uni-versity of Sussex.A.
Belz.
1998.
A few English words can helpimprove your Russian.
In Henri Prade, edi-tor, ECAI98: 13th European Conference onArtificial Iintelligence.
John Wiley & Sons.J.
Blevins.
1994.
The syllable in phonologicaltheory.J.
Carson-Berndsen.
1993.
An event-basedphonotactics for German.
Technical ReportASL-TR-29-92/UBI, Fakultaet fuer Linguis-tik und Literaturwissenschaft, University ofBielefeld.N.
Chomsky.
1964.
Current Issues in LinguisticTheory.
Mouton, The Hague.P.
Dupont.
1994.
Regular grammatical infer-ence from positive and negative samples bygenetic search: the GIG method.
In Gram-matical Inference and Applications, SecondInternational Colloquium, ICGI-94, Proceed-ings, Berlin.
Springer.K.
S. Fu and T. L. Booth.
1986.
Gram-matical inference: Introduction and sur-vey.
IEEE Transactions on Pattern Analysisand Machine Intelligence, PAMI-8:343-375.
(Reprinted from 1975.).D.
Gibbon.
1991.
Lexical signs and lexiconstructure: Phonology and prosody in theASL-lexicon.
Technical Report ASL-MEMO-20-91/UBI, University of Bielefeld.E.
M. Gold.
1978.
Complexity of automatonidentification from given data.
Informationand Control, 37:302-320.John A. Goldsmith.
1990.
Autosegmental ndMetrical Phonology.
Blackwell, Cambridge,Mass.M.
Halle.
1971.
The Sound Pattern of Russian.Mouton, The Hague.J.
E. Hopcroft and J. D. Ullman.
1979.
In-troduction to Automata Theory, Languages,and Computation.
Addison-Wesley, Reading,Mass.A.
Jusek, H. Rautenstrauch, G. A. Fink,F.
Kummertand G. Sagerer, J. Carson-Berndsen, and D. Gibbon.
1994.
Detektion43unbekannter Woerter mit Hilfe phonotaktis-cher Modelle.
In Mustererkennung 94, 16.DA GM-Symposium.T.
E. Kammeyer and R. K. Belew.
1996.Stochastic context-free grammar inductionwith a genetic algorithm using local search.Technical Report CS96-476, Cognitive Com-puter Science Rsearch Group, Computer Sci-ence and Engineering Department, Universityof California at San Diego.M.
Kearns and L. G. Valiant.
1989.
Crypto-graphic limitations on learning boolean for-mulae and finite automata.
In Proceedings ofthe 21st Annual ACM Symposium on The-ory of Computing, pages 433-444, New York.ACM.B.
Keller and R. Lutz.
1997a.
Evolving stochas-tic context-free grammars from examples us-ing a minimum description length principle.Nashville, Tennessee, July.
Paper presentedat the Workshop on Automata InductionGrammatical Inference and Language Acqui-sition, ICML-97.B.
Keller and R. Lutz.
1997b.
Learning SCFGsfl'om corpora using a genetic algorithm.
InICA NNGA 97.M.
Li and U. Vazirani.
1988.
On the learn-ability of finite automata.
In Proceedings ofthe 1988 Workshop on Computational Learn-ing Theory, pages 359-370, San Mateo, Ca.Morgan-Kaufmann.S.
Lucas.
1994.
Context-free grammar evolu-tion.
In First International Conference onEvolutionary Computing, pages 130-135.R.
S. Michalski.
1983.
A theory and method-ology of inductive learning.
In R. Michalski,K.
Carbonell, and T. Mitchell, editors, Ma-chine Learning: An Artificial Intelligence Ap-proach Vol.
1, pages 83-143.
Tioga, Palo Alto,CA.
Also published by Springer in 1994.L.
Pitt and M. K. Warmuth.
1993.
The mini-mum consistent DFA problem cannot be ap-proximated within any polynomial.
Journalof the Association for Computing Machinery,40(1):95-142.M.
Schwehm and A. Ost.
1995.
Inferenceof stochastic regular grammars by massivelyparallel genetic algorithms.
In Proceedingsof the Sixth International Conference on Ge-netic Algorithms, pages 520-527.
MorganKaufmann.B.
B. Trakhtenbrot and Ya.
Barzdin.
1973.
Fi-nite Automata.
North Holland, Amsterdam.P.
Wyard.
1989.
Representational issues forcontext free grammar induction using geneticalgorithms.
Technical report, Natural Lan-guage Group, Systems Research Division, BTLaboratories, Ipswich, UK.P.
Wyard.
1991.
Context free grammar induc-tion using genetic algorithms.
In Richard K.Belew and Lashon B. Booker, editors, Pro-ceedings of the Fourth International Confer-ence on Genetic Algorithms, pages 514-518,San Diego, CA.
Morgan Kaufmann.H.
Zhou and J. J. Grefenstette.
1986.
Induc-tion of finite automata by genetic algorithms.Proceedings of the 1986 International Confer-ence on Systems, Man and Cybernetics, pages170-174.44
