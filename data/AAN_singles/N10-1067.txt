Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 438?446,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsSome Empirical Evidence for Annotation Noise in a Benchmarked DatasetBeata Beigman KlebanovKellogg School of ManagementNorthwestern Universitybeata@northwestern.eduEyal BeigmanWashington University in St. Louisbeigman@wustl.eduAbstractA number of recent articles in computationallinguistics venues called for a closer exami-nation of the type of noise present in anno-tated datasets used for benchmarking (Rei-dsma and Carletta, 2008; Beigman Klebanovand Beigman, 2009).
In particular, BeigmanKlebanov and Beigman articulated a type ofnoise they call annotation noise and showedthat in worst case such noise can severelydegrade the generalization ability of a linearclassifier (Beigman and Beigman Klebanov,2009).
In this paper, we provide quantita-tive empirical evidence for the existence ofthis type of noise in a recently benchmarkeddataset.
The proposed methodology can beused to zero in on unreliable instances, facili-tating generation of cleaner gold standards forbenchmarking.1 IntroductionTraditionally, studies in computational linguisticsuse few trained annotators.
Lately this might bechanging, as inexpensive annotators are available inlarge numbers through projects like Amazon Me-chanical Turk or through online games where an-notations are produced as a by-product (Poesio etal., 2008; von Ahn, 2006), and, at least for certaintasks, the quality of multiple non-expert annotationsis close to that of a small number of experts (Snowet al, 2008; Callison-Burch, 2009).Apart from the reduced costs, mass annotation isa promising way to get detailed information aboutthe dataset, such as the level of difficulty of the dif-ference instances.
Such information is importantboth from the linguistic and from the machine learn-ing perspective, as the existence of a group of in-stances difficult enough to look like they have beenlabeled by random guesses can in the worst caseinduce the machine learner training on the datasetto misclassify a constant proportion of easy, non-controversial instances, as well as produce incor-rect comparative results in a benchmarking setting(Beigman Klebanov and Beigman, 2009; Beigmanand Beigman Klebanov, 2009) .In this article, we employ annotation generationmodels to estimate the types of instances in a multi-ply annotated dataset for a binary classification task.We provide the first quantitative empirical demon-stration, to our knowledge, of the existence of whatBeigman Klebanov and Beigman (2009) call ?anno-tation noise?
in a benchmarked dataset, that is, fora case where instances cannot be plausibly assignedto just two classes, and where instances in the thirdclass can be plausibly described as having been an-notated by flips of a nearly fair coin.
The ability toidentify such instances helps improve the gold stan-dard by eliminating them, and allows further empiri-cal investigation of their impact on machine learningfor the task in question.2 Generative models of annotationWe present a graphical model for the generation ofannotations.
The basic idea is that there are differenttypes of instances that induce different responsesfrom annotators.
Each instance may have a true la-bel of ?0?
or ?1?, however, the researcher?s accessto it is mediated by annotators who are guessing thetrue label by flipping a coin, where the bias of thecoin depends on the type of the instance.
The biasof the coin essentially models the difficulty of label-438ing the instance; coins biased close to 0 and 1 cor-respond to instances that are easy to classify; a faircoin represents instances that are very difficult if notimpossible to classify correctly with the given poolof annotators.
The model presented in Beigman Kle-banov and Beigman (2009) is a special case with 3types (A,B,C) where pA=0, pC=1 (easy cases), and0<pB<1 represents the hard cases, the harder thecloser pB is to 0.5.
Models used here are a type of la-tent class models (McCutcheon, 1987) widely usedin the Biometrics community (Espeland and Handel-man, 1989; Yang and Becker, 1997; Albert et al,2001; Albert and Dodd, 2004).The goal of modeling is to determine whethermore than two types of instances need to be postu-lated, to estimate how difficult each type is, and toidentify the troublemaking instances.The graphical model is presented in figure 1.
Weassume the dataset of size N is a mixture of k dif-ferent types of instances.
The proportion of types isgiven by ?
= (?1, .
.
.
, ?k), and coin biases for eachtype are given by p = (p1, .
.
.
, pk).
Each instance isannotated by n i.i.d coinflips, and random variablex ?
{0, .
.
.
, n} counts the number of ?1?s in the nannotations given to an instance.
Each instance be-longs to a type t ?
{1, ..., k}, characterized by a coinwith the probability pt of annotating with the label?1?.
Conditioned on t, the number of ?1?s in n an-notations has a binomial distribution with parameterpt: Pr(x = j|t) =(nj)pjt (1?
pt)n?j .x t ?
Nx t ?N a ?ppx a ?
NpFigure 1: A graphical model of annotation generation.The probability of observing j ?1?s out of n an-notations for an instance given ?
and p is thereforePr(x = j|?, p) =?kt=1 Pr(t|?)
?
Pr(x = j|t) ==(nj)?kt=1 ?tpjt (1 ?
pt)n?j .
The annotations arethus generated by a superposition of k binomials.3 Data3.1 Recognizing Textual Entailment -1For the experiments reported here we use the 800item test data of the first Recognizing Textual Entail-ment benchmark (RTE-1) from Dagan et al (2006).This task drew a lot of attention in the community,with a series of benchmarks in 2005-2007.The task is defined as follows: ?...
textual entail-ment is defined as a directional relationship betweenpairs of text expressions, denoted by T - the entail-ing ?Text?, and H - the entailed ?Hypothesis?.
Wesay that T entails H if the meaning of H can be in-ferred from the meaning of T, as would typically beinterpreted by people.
This somewhat informal defi-nition is based on (and assumes) common humanunderstanding of language as well as common back-ground knowledge?
(Dagan et al, 2006).
Furtherguidelines included an instruction to disregard tensedifferences, to accept cases where the inference is?very probable (but not completely certain)?
and toavoid cases where the inference ?has some positiveprobability that is not clearly very high.?
An exam-ple of a true entailment is the pair T-H: (T) CavernClub sessions paid the Beatles ?15 evenings and ?5lunchtime.
(H) The Beatles perform at Cavern.Although annotated by a small number of expertsfor the benchmark, the RTE-1 dataset has been latertransferred to a mass annotation framework by Snowet al (2008), who submitted simplified guidelinesto the Amazon Mechanical Turk workplace (hence-forth, AMT), collected 10 annotations per item fromthe total of 164 annotators, and showed that major-ity vote by Turkers agreed with expert annotation in89.7% of the cases.
We call the Snow et al (2008)Turker annotations SRTE dataset, and use it in sec-tion 6.
The instructions, followed by two examples,read: ?Please state whether the second sentence (theHypothesis) is implied by the information in the firstsentence (the Text), i.e., please state whether the Hy-pothesis can be determined to be true given that theText is true.
Assume that you do not know anythingabout the situation except what the Text itself says.Also, note that every part of the Hypothesis must beimplied by the Text in order for it to be true.?
Theguidelines for Turkers are somewhat different fromthe original, not mentioning the issue of highly prob-able though not certain inference or a special treat-439ment of tense mismatch between H and T, as well asdiscouraging reliance on background knowledge.Using Snow et al (2008) instructions, we col-lected 20 annotations for each of the 800 itemsthrough AMT from the total of 441 annotators.
Eachannotator did the minimum of 2 items, and waspaid $0.01 for 2 items, for the total annotator costof $80.
We used only annotators with prior AMTapproval rate of at least 95%, that is, only peoplewhose performance in previous tasks on AMT wasalmost always approved by the requester of the task.Our design is thus somewhat different from Snow etal.
(2008), as we paid more and selected annotatorswith a stake in their AMT reputation.3.2 Preparing the data for model fittingWe collected the annotations in two separate batchesof 10 annotations per item, using the same set of in-structions, incentives, and examples.
We hypothe-sized that controlling for these elements, we wouldget two random samples from the same distributionof Turkers, and hence will have two samples to makesure a model fitted on one sample generalized tothe other.
It turned out, however, that a 3-Binomialmodel with a good fit on one of the samples was re-jected with high probability for the other.1 Thus, onthe one hand, the variations between annotators ineach sample were not as high as to preclude a modelthat captures only instance variability from fittingwell; on the other hand, evidently, the two samplesdid not come from the same annotator distribution,but differed systematically due to factors we did notcontrol for.2 In order for our models not to inherit asystematic bias of any of the two samples, we mixedthe two samples, and constructed two sets, BRTEaand BRTEb, each with 10 annotations per item, byrandomly splitting the 20 answers per item into twogroups, allowing the same annotator to contributeto different groups on different instances.
Indeed,after the randomization, a model fitted for BRTEaproduced excellent generalization on BRTEb, as wewill see in section 4.2.1For details of the model fitting procedure, see section 4.2Such factors could be the hour and day of assignment, asthe composition of AMT?s global 24/7 workforce could differsystematically by day and hour.4 Fitting a model to BRTE dataUsing the model template presented in section 2, wesuccessively attempt to fit a model with k = 2, 3, .
.
.until a model with a good fit is found or no degreesof freedom are left.
For a given k, we fit the pa-rameters ?
and p using non-linear least squares trust-region method as implemented in the default versionof MATLAB?s lsqnonlin function.
We then use ?2 tomeasure goodness of fit; a model that cannot be re-jected with 95% confidence (p>0.05) would be con-sidered a good fit.
In all cases N=800, n=10, as weuse 10 annotations for each instance.4.1 Mixture of 2 BinomialsSuppose k=2, with types t0 and t1.
The best fit yieldsp0=0.237, p1=0.867, ?0=431800 , ?1=1-?0.
The model(shown in figure 2) is a poor fit, with ?2=73.66 wellabove the critical value of 14.07 for df=7, p=0.05.30204060801001201401600 1 2 3 4 5 6 7 8 9 10Number of label "1" annotationsNumber of instances ObservedPredictedFigure 2: Fitting the model B1+B2 to BRTEa data.
B1?B(10,0.237) on 431 instances, B2?
B(10,0.867) on 369instances.
The point (x,y) means that there are y in-stances given label ?1?
in exactly x out of 10 annotations.4.2 Model M: Mixture of 3 BinomialsSuppose now k=3.
The best fitting modelM=B1+B2+B3 is specified in figure 3; M fits thedata very well.
Assuming B1 and B3 reflect items3For degrees of freedom, we take the number of datapointsbeing fitted (11), take one degree of freedom off for knowing inadvance the total number of instances, and take off additional 3degrees of freedom for estimating p0, p1, and ?0 from the data.We are therefore left with 7 degrees of freedom in this case.440with uncontroversial labels ?0?
and ?1?, respec-tively, the model suggests that detecting ?0?
(no tex-tual entailment) is somewhat more difficult for non-experts than detecting ?1?
(there is textual entail-ment) in this dataset, with the rate of incorrect pre-dictions of about 20% and 10%, respectively.4 Themodel also predicts that 159800 ?
20% of the data aredifficult cases, with annotators flipping a close-to-a-fair coin (p=0.5487).0204060801001201400 1 2 3 4 5 6 7 8 9 10ObservedB1B2B3PredictedFigure 3: Fitting the model M=B1+B2+B3 to BRTEadata.
B1?
B(10,0.1978) on 343 instances, B2?B(10,0.5487) on 159 instances, B3?
B(10,0.8942) on298 instances.
The binomials are shown in grey lines.The model M fits with ?2=5.091; for df=5, this corre-sponds to p=0.4.We use the dataset BRTEb to test the model de-veloped on BRTEa.
The model fits with ?2=13.13,which, for df=10,5 corresponds to p=0.2154.We therefore conclude that, after eliminating sys-tematic differences between annotators, we were un-able to fit a model with two types of instances,whereas a model with three types of instances pro-vides a good fit both for the dataset on which it isestimated and for a new dataset.
This constitutesempirical evidence for the existence of a group ofinstances with near-random labels in this recently4We note that any conclusions from the model hold for theparticular 800 item dataset in question, and not for the task ofrecognizing textual entailment in general, as the dataset is notnecessarily a representative sample.
In fact, we know from Da-gan et al (2006) that these 800 items are not a random sam-ple, but rather what remained after some 400 instances were re-moved due to disagreements between expert annotators or dueto the judgment of one of organizers of the RTE-1 challenge.5No parameters are fitted using the BRTEb data.benchmarked dataset, at least for our pool of morethan 400 non-expert annotators.5 Could annotator heterogeneity providean alternative explanation?In the previous section, we established that instanceheterogeneity can explain the observations.
Wemight however ask whether a different model couldprovide a similarly fitting explanation.
Specifically,heterogeneity among annotators has been seen as amajor source of noise in the aggregate data and thereare several works attempting to separate high qual-ity annotators from low quality ones (Raykar et al,2009; Donmez et al, 2009; Sheng et al, 2008; Car-penter, 2008).
Could we explain the observed beha-vior with a model with only two types of instancesthat allows for annotator heterogeneity?In this section we construct such a model.
Weshow that this model entails an instance distribu-tion that is a superposition of two normal distribu-tions.
We subsequently show that the best fittingtwo-Gaussian model does not provide a good fit.We use a generation model similar to those in(Raykar et al, 2009; Carpenter, 2008) but withweaker parametric assumptions.
The graphicalmodel is given in figure 4.x t ?
Nx t ?N a ?ppx?Npt ?Figure 4: Annotation generation model with annotatorheterogeneity.We assume there are two types of instances t ?
{0, 1} with the proportions ?
= (?0, ?1).
The 2nprobabilities p = (pt1, .
.
.
, ptn) for t = 0, 1 cor-respond to coins drawn independently from somedistribution with parameter ?
= (?1, .
.
.
, ?n).
Wemake no assumption on the functional form apartfrom a positive probability to draw a value between0 and 1, this in particular is true for the beta distribu-tion used in (Raykar et al, 2009; Carpenter, 2008).As before, the number of ?1?s attributed to an in-stance of type t is a random variable x, determined441by independent flips of the n coins that correspondto the value of t. The marginal distribution of x is:Pr(x = j|?, ?)
==?t=0,1Pr(t|?)?[0,1]nPr(pt|?
)?Pr(x = j|pt, t, ?)dpt=?t=0,1?t?[0,1]nPr(pt|?)???|S|=j?i?Spti?i6?S(1?
pti)??
dptLet x1, .
.
.
, xN be the random variables correspond-ing to the number of ?1?s attributed to instances1, .
.
.
, N .
W.l.g we assume instances 1, .
.
.
, N ?
areall of type t0 (N ?
= ?0 ?
N ) and the rest of type t1.Since 0 ?
xj ?
n it follows that E(xj),Var(xj) <?
for j = 1, .
.
.
, N .
If for each instance the coin-flips are independent, we can think of this as a twostep process where we first draw the coins and thenflip them.
Thus, x1, .
.
.
, xN ?
are i.i.d and the cen-tral limit theorem implies that the average numberof ?1?s on t0 instances, namely the random variabley0 = 1N ?
?N ?j=1 xj has an approximately normal dis-tribution.6 Making the same argument for the distri-bution of y1 for instances of type t1, it follows thatthe number of ?1?s attributed to an instance of anytype y = y0 + y1 would have a distribution that is asuperposition of two Gaussians.The best least-squares fit of all two-Gaussianmodels to BRTEa data is produced by G=N1+N2,N1?
N (2.22, 1.73) on 418 instances, N2?N (9.07,1.41) on 382 instances; G is shown infigure 5.
G fits with ?2=36.77, much above the crit-ical value ?2=11.07 for df=5, p=0.05.
We can thusrule out annotator heterogeneity as the only expla-nation of the observed pattern of responses.6 Testing M on SRTE dataWe further test M on the annotations collected bySnow et al (2008) for the same 800 item dataset.While the instructions and the task were identical inBRTEa, BRTEb, and BRTE datasets, and in all cases6It can be shown that y0 ?
N (?, ?)
for ?
= n ?
EDist(?
)(p)and ?
=pVarDist(?
)(p) ?
n, using the expectation and varianceof the coin parameter for type t0 instances.
For example, for abeta distribution with parameters ?
and ?
these would be ?
=??+?
n and ?
=q???+?
n.0204060801001200 1 2 3 4 5 6 7 8 9 10ObservedPredictedFigure 5: Model G?s fit to BRTEa data, G= N1+N2, amixture of two Gaussians.each item was given 10 annotations, the incentivedesign was different (see section 3).Figure 6 shows that model M=B1+B2+B3 doesnot fit well, as SRTE dataset exhibits a rather diffe-rent distribution from both BRTE datasets.
In par-ticular, it is clear that had a model been fitted onSRTE data, the coin flipping probabilities for theclear types, B1 and B3, would have to be movedtowards 0.5; that is, an average annotator in SRTEdataset had worse ability to detect clear 0s and clear1s than an average BRTE annotator.
We note thatBRTEa and BRTEb agreed with expert annotationin 92.5% and 90.8% of the instances, respectively,both better than 89.7% in SRTE.7 Since we offeredsomewhat better incentives in BRTE, it is temptingto attribute the observed better quality of BRTE an-notations to the improved incentives, although it ispossible that some other uncontrolled AMT-relatedfactor is responsible for the difference between thedatasets, just as we found for our original two col-lected samples (see section 3.2).Supposing the main source of misfit is differencein incentives, we conjecture that the difference be-tween the 441 BRTE annotators and the 164 SRTEones is due to the existence in SRTE of unmotivated,or ?lazy?
annotators, that is, people who flipped thesame coin on every instance, no matter what type.Our hypothesis is that once an annotator is diligent(and motivated) enough to pay attention to the data,her annotations can be described by model M, butsome annotators are not sufficiently diligent.7Turker annotations were aggregated using majority vote, asin Snow et al (2008) section 4.3.4420204060801001201400 1 2 3 4 5 6 7 8 9 10BRTEaBRTEbObserved (SRTE)Predicted by MFigure 6: Model M?s fit to SRTE data.
BRTEa andBRTEb are shown in grey lines.In this model we assume there are three typesof instances as before, and two types of annotatorsa ?
{D,L}, for Diligent and Lazy, with their pro-portions in the population ?
= (?D, ?L).
The corre-sponding graphical model is shown in figure 7.x t ?
Nxt ?
Na ?ppx?Npt ?xt ?
Na ?
pcFigure 7: Annotation generation with diligent and lazyannotators.We assume that diligent annotators flip coins cor-responding to the types of instances, whereas lazyannotators always flip the same coin pL.Let nD and nL=n?nD be the number of diligentand lazy annotations given to a certain instance, thusPr(nD=r|?
)=(nr)?rD?n?rL , and the probability of ob-serving j label ?1?
annotations for an instance oftype t is given by:Pr(x = j|t, ?, p) =n?r=1[(nr)?rD?n?rL ??[?
(j1,j2)?S(rj1)pj1t (1?
pt)r?j1 ??(n?
rj2)pj2L (1?
pL)n?r?j2]]where S={(j1, j2):j1+j2=j; j1?r;j2?n-r}.
Finally,Pr(x=j|?, ?, p)=?kt=1 ?t Pr(x=j|t, ?, p).We assume that model M provides the values for?
and p for all diligent annotators, and estimate ?and pL, the proportion of the lazy annotators andthe coin they flip.
The best fitting model yields?=(0.79,0.21), and pL=0.74, predicting that aboutone-fifth of SRTE annotators are lazy.8 This modelfits with ?2=14.63, which is below the critical levelof ?2=15.51 for df=8,p=0.05, hence a hypothesisthat model M behavior for the diligent annotatorsand flipping a coin with bias 0.74 for the lazy onesgenerated the SRTE data cannot be rejected withhigh confidence.
We note that Carpenter (2008) ar-rived at a similar conclusion ?
that there are quitea few annotators making random guesses in SRTEdataset ?
by means of jointly estimating annotatoraccuracies.7 DiscussionTo summarize our findings: With systematic dif-ferences between annotators smoothed out, thereis evidence that non-expert annotators performingRTE task on RTE-1 test data tend to flip a close-to-fair coin on about 20% of instances, accordingto the best fitting model.9 This constitutes, to ourknowledge, the first empirical evidence for the ex-istence of the kind of noise termed annotation noisein Beigman Klebanov and Beigman (2009).
GivenBeigman Klebanov and Beigman (2009) warningagainst annotation noise in test data and their find-ing in Beigman and Beigman Klebanov (2009) thatannotation noise in training data can potentially dev-astate a linear classifier learning from the data, theimmediate usefulness of our result is that instancesof this difficult type can be identified, removed fromthe dataset before further benchmarking, and pos-8A more precise statement is that there are about one-fifthlazy potential annotators in the SRTE pool for any given item.It is possible that the length of stay of an annotator in the pool isnot independent of her diligence; for example, Callison-Burch(2009) found in his AMT experiments with tasks related to ma-chine translation that lazy annotators tended to stay longer anddo more annotations.9Beigman Klebanov and Beigman (2009) discuss the con-nection between noise models and inter-annotator agreement.443sibly used in a controlled fashion for subsequentstudies of the impact of annotation noise on specificlearning algorithms and feature spaces for this task.The current literature on generating benchmark-ing data from AMT annotations overwhelminglyconsiders annotator heterogeneity as the source ofobserved discrepancies, with instances falling intotwo classes only.
Our results suggest that, at least inRTE data, instance heterogeneity cannot be ignored.It also transpired that small variations in incen-tives (as between SRTE and BRTE), and even un-known factors possibly related to differences in thecomposition of AMT?s workforce can lead to sys-tematic differences in the resulting annotator pools,which results in annotations that are described bymodels with somewhat different parameter values.This can potentially limit the usefulness of our mainfinding, because it is not clear how reliable the iden-tification of hard cases is using any particular groupof Turkers.
While this is a valid concern in general,we show in section 7.1 that many items consistentlyfound to be hard by different groups of Turkers war-rant at least an additional examination, as they oftenrepresent borderline cases of highly or not-so-highlyprobable inferences, corruption of meaning by un-grammaticality, or difficulties related to the treat-ment of time references and background knowledge.Finally, our findings seem to be at odds with thefact that the 800 items analyzed here were left af-ter all items on which two experts disagreed and allitems that looked controversial to the arbiter wereremoved (see section 3).
One potential explanationis that things that are hard for Turkers are not nec-essarily hard for experts.
Yet it is possible that twoor three annotators, graduate students or faculty incomputational linguistics, are an especially homoge-nous and small pool of people to base gold standardannotations of the way things are ?typically inter-preted by people?
upon.
Furthermore, there is someevidence from additional expert re-annotations ofthis dataset that some controversies remain; we dis-cuss relation to expert annotations in section 7.2.7.1 Hard casesWe examine some of the instances that in all likeli-hood belong to the difficult type, according to Turk-ers.
We focus on items that received between 4 and7 class ?1?
annotations in SRTE and in each of ourtwo datasets (before randomization).
(1) T: Saudi Arabia, the biggest oil producer inthe world, was once a supporter of Osama binLaden and his associates who led attacks againstthe United States.
H: Saudi Arabia is theworld?s biggest oil exporter.
(2) T: Seiler was reported missing March 27 andwas found four days later in a marsh near hercampus apartment.
H: Abducted Audrey Seilerfound four days after missing.
(3) T: The spokesman for the rescue authorities,Linart Ohlin, said that the accident took placebetween 01:00 and dawn today, Friday (00:00GMT) in a disco behind the theatre, where ?hun-dreds?
of young people were present.
H: Thefire happened in the early hours of Friday morn-ing, and hundreds of young people were present.
(4) T: William Leonard Jennings sobbed loudly aswas charged with killing his 3-year-old son,Stephen, who was last seen alive on Dec.12,1962.
H: William Leonard Jennings killed his3-year-old son, Stephen.Labeling of examples 1-4 seems to hinge on theassessment of the likelihood of an alternative expla-nation.
Thus, it is possible that the biggest producerof oil is not the biggest exporter, because, for ex-ample, its internal consumption is much higher thanin the second-biggest producer.
In 2, abduction isa possible cause for being missing, but how rela-tively probable is it?
Similarly, fire is a kind of ac-cident, but can we infer that there was fire from areport about an accident?
In 4, could the man havesobbed because on top of loosing his son he wasalso being falsely accused of having killed him?
Ex-perts marked all five as true entailments, while manyTurkers had reservations.
(5) T: Bush returned to the White House late Satur-day while his running mate was off campaigningin the West.
H: Bush left the White House.
(6) T: De la Cruz?s family said he had gone to SaudiArabia a year ago to work as a driver after a longperiod of unemployment.
H: De la Cruz wasunemployed.
(7) T: Measurements by ground-based instrumentsaround the world have shown a decrease of upto 10 percent in sunlight from the late 1950s tothe early 1990s.
H: The world is about 10 percent darker than half a century ago.444In examples 5-7 time seems to be an issue.
If Bushreturned to White House, he must have left it before-hand, but does this count as entailment, or is the hy-pothesis referencing a time concurrent with the text,in which case T and H are in contradiction?
In 6,can H be seen as referring to some time more than ayear ago?
In 7, if the hypothesis is taken to be statedin mid- or late-2000s, the time of annotation, halfa century ago would reach to late 1950s, but it ispossible that further substantial reduction occurredbetween early 1990s mentioned in the text and mid2000s, amounting to much more than 10%.
Expertslabeled example 5 as false, 6 and 7 as true.
(8) T: On 2 February 1990, at the opening of Parlia-ment, he declared that apartheid had failed andthat the bans on political parties, including theANC, were to be lifted.
H: Apartheid in SouthAfrica was abolished in 1990.
(9) T: Kennedy had just won California?s Demo-cratic presidential primary when Sirhan shothim in Los Angeles on June 5, 1968.
H: Sirhankilled Kennedy.Labeling examples 8 and 9 (both true according tothe experts) requires knowledge about South Africanand American politics, respectively.
Was the ban onANC the only or the most important manifestationof apartheid?
Was abolishing apartheid merely anissue of declaring that it failed?
In 9, killing is a po-tential but not necessary outcome of shooting, so de-tails of Robert Kennedy?s case need to be known tothe annotator to render the case-specific judgment.
(10) T: The version for the PC has essentially thesame packaging as those for the big game con-soles, but players have been complaining thatit offers significantly less versatility when itcomes to swinging through New York.
H: Play-ers have been complaining that it sells signifi-cantly less versatility when it comes to swingingthrough New York.
(11) T: During his trip to the Middle East that tookthree days, Clinton made the first visit by anAmerican president to the Palestinian Territoriesand participated in a three-way meeting with Is-raeli Prime Minister Benjamin Netanyahu andPalestinian President Yasser Arafat.
H: Duringhis trip to the east of the Middle which lastedthree days, the Clinton to first visit to Ameri-can President to the occupied Palestinian terri-tories and participated in meeting tripartite co-operation with Israeli Prime Minister BenjaminNetanyahu and Palestinian President, YasserArafat.
(12) T: The ISM non-manufacturing index rose to64.8 in July from 59.9 in June.
H: The non-manufacturing index of the ISM raised 64.8 inJuly from 59.9 in June.
(13) T: Henryk Wieniawski, a Polish-born musician,was known for his special preference for resur-recting neglected or lost works for the violin.
H:Henryk Wieniawski was born in Polish.Examples 10-13 were labeled as false by experts,possibly betraying over-sensitivity to the failings oflanguage technology.
Sells is not an ideal substitu-tion for offers, but in a certain sense versatility issold as part of a product.
In 11-13, some Turkersfelt the hypothesis is not too bad a rendition of thetext or of its part, while experts seemed to hold MTto a higher standard.7.2 Turkers vs expertsModel M puts 159 items in the difficult type B2.While M is the best fitting model, it is possible tofind a model that still fits with p>0.05 but placesa smaller number of items in B2, in order to ob-tain a conservative estimate on the number of dif-ficult cases.
The model with B1?
B(10, 0.21) on373 items, B2?
B(10,0.563) on 110 items, B3?B(10,0.89) on 327 items still produces a fit withp>0.05, but going down to 100 instances in B2makes it impossible to find a good fit with a 3 typemodel.
There are therefore about 110 difficult casesby a conservative estimate.
Assuming there remain110 hard cases in the 800 item dataset for whicheven experts flip a fair coin, we expect about 55disagreements between the 800 item gold standardfrom RTE-1 and a replication by a new expert, oran agreement of 745800=93% on average.
This estimateis consistent with reports of 91% to 96% replicationaccuracy for the expert annotations on various sub-sets of the data by different groups of experts (seesection 2.3 in Dagan et al (2006)).AcknowledgmentsWe would like to thank the anonymous reviewers ofthis and the previous draft for helping us improve thepaper significantly.
We also thank Amar Cheema forhis advice on AMT.445ReferencesPaul Albert and Lori Dodd.
2004.
A Cautionary Note onthe Robustness of Latent Class Models for EstimatingDiagnostic Error without a Gold Standard.
Biometrics,60(2):427?435.Paul Albert, Lisa McShane, Joanna Shih, and The U.S.National Cancer Institute Bladder Tumor Marker Net-work.
2001.
Latent Class Modeling Approaches forAssessing Diagnostic Error without a Gold Standard:With Applications to p53 Immunohistochemical As-says in Bladder Tumors.
Biometrics, 57(2):610?619.Eyal Beigman and Beata Beigman Klebanov.
2009.Learning with Annotation Noise.
In Proceedings ofthe 47th Annual Meeting of the Association for Com-putational Linguistics, pages 280?287, Singapore.Beata Beigman Klebanov and Eyal Beigman.
2009.From Annotator Agreement to Noise Models.
Ac-cepted to Computational Linguistics.Chris Callison-Burch.
2009.
Fast, Cheap, and Cre-ative: Evaluating Translation Quality Using Amazon?sMechanical Turk.
In Proceedings of the EmpiricalMethods in Natural Language Processing Conference,pages 286?295, Singapore.Bob Carpenter.
2008.
Multilevel Bayesian Mod-els of Categorical Data Annotation.
Unpub-lished manuscript, last accessed 28 July 2009at lingpipe.files.wordpress.com/2009/01/anno-bayes-entities-09.pdf.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL Recognising Textual Entail-ment Challenge.
In J. Quin?onero Candela, I. Dagan,B.
Magnini, and F. d?Alche?-Buc, editors, MachineLearning Challenges, pages 177?190.
Springer.Pinar Donmez, Jaime Carbonell, and Jeff Schneider.2009.
Efficiently Learning and Accuracy of LabelingSources for Selective Sampling.
In Proceedings of the15th International Conference on Knowledge Discov-ery and Data Mining, pages 259?268, Paris, France.Mark Espeland and Stanley Handelman.
1989.
UsingClass Models to Characterize and Assess Relative Er-ror in Discrete Measurements.
Biometrics, 45(2):587?599.Allan McCutcheon.
1987.
Latent Class Analysis.
New-bury Park, CA, USA: Sage.Massimo Poesio, Udo Kruschwitz, and Jon Chamberlain.2008.
ANAWIKI: Creating Anaphorically AnnotatedResources through Web Cooperation.
In Proceedingsof the 6th International Conference on Language Re-sources and Evaluation, Marrakech, Morocco.Vikas Raykar, Shipeng Yu, Linda Zhao, Anna Jerebko,Charles Florin, Gerardo Hermosillo Valadez, Luca Bo-goni, and Linda Moy.
2009.
Supervised Learningfrom Multiple Experts: Whom to Trust when Every-one Lies a Bit.
In Proceedings of the 26th Annual In-ternational Conference on Machine Learning, pages889?896, Montreal, Canada.Dennis Reidsma and Jean Carletta.
2008.
ReliabilityMeasurement without Limits.
Computational Linguis-tics, 34(3):319?326.Victor Sheng, Foster Provost, and Panagiotis Ipeirotis.2008.
Get Another Label?
Improving Data Qualityand Data Mining Using Multiple, Noisy Labelers.
InProceedings of the 14th International Conference onKnowledge Discovery and Data Mining, pages 614?622, Las Vegas, Nevada, USA.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Ng.
2008.
Cheap and Fast - But is it Good?Evaluating Non-Expert Annotations for Natural Lan-guage Tasks.
In Proceedings of the Empirical Methodsin Natural Language Processing Conference, pages254?263, Honolulu, Hawaii.Luis von Ahn.
2006.
Games with a Purpose.
Computer,39(6):92?94.Ilsoon Yang and Mark Becker.
1997.
Latent Vari-able Modeling of Diagnostic Accuracy.
Biometrics,53(3):948?958.446
