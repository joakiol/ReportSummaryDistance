Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1891?1901,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsType-Aware Distantly Supervised Relation Extractionwith Linked ArgumentsMitchell Koch John Gilmer Stephen Soderland Daniel S. WeldDepartment of Computer Science & EngineeringUniversity of WashingtonSeattle, WA 98195, USA{mkoch,jgilme1,soderlan,weld}@cs.washington.eduAbstractDistant supervision has become the lead-ing method for training large-scale rela-tion extractors, with nearly universal adop-tion in recent TAC knowledge-base pop-ulation competitions.
However, there arestill many questions about the best wayto learn such extractors.
In this paper weinvestigate four orthogonal improvements:integrating named entity linking (NEL)and coreference resolution into argumentidentification for training and extraction,enforcing type constraints of linked argu-ments, and partitioning the model by rela-tion type signature.We evaluate sentential extraction perfor-mance on two datasets: the popular set ofNY Times articles partially annotated byHoffmann et al.
(2011) and a new dataset,called GORECO, that is comprehensivelyannotated for 48 common relations.
Wefind that using NEL for argument identi-fication boosts performance over the tra-ditional approach (named entity recogni-tion with string match), and there is furtherimprovement from using argument types.Our best system boosts precision by 44%and recall by 70%.1 IntroductionRelation extractors are commonly trained by dis-tant supervision (also known as knowledge-basedweak supervision (Hoffmann et al., 2011)), an au-tonomous technique that creates a labeled train-ing set by heuristically matching the contents of aknowledge base (KB) to mentions (substrings) ina textual corpus.
For example, if a KB containedthe ground tuple BornIn(Albert Einstein, Ulm) thenTrainingExtractionKBArgumentIdentificationMatchingTrain Extractor ExtractorArgumentIdentificationFigure 1: Distantly supervised extraction pipeline.a distant supervision system might label the sen-tence ?While [Einstein]1was born in [Ulm]2, hemoved to Munich at an early age.?
as a positivetraining instance of the BornIn relation.
Althoughdistant supervision is a simple idea and often cre-ates data with false positives, it has become ubiq-uitous; for example, all top-performing systems inrecent TAC-KBP slot filling competitions used themethod.Surprisingly, however, many aspects of distantsupervision are poorly studied.
In response weperform an extensive search of ways to improvedistant supervision and the extraction process, in-cluding using named entity linking (NEL) andcoreference to identify arguments for distant su-pervision and extraction, as well as using type con-straints and partitioning the trained model by rela-tion type signatures.The first step in the distant supervision processis argument identification (Figure 1) ?
findingtextual mentions referring to entities that might bein some relation.
Next comes matching, where KBfacts, e.g.
tuples such as R(e1, e2), are associatedwith sentences mentioning entities e1and e2inthe assumption that many of these sentences de-scribe the relation R. Most previous systems per-form these steps by first using named entity recog-nition (NER) to identify possible arguments andthen using a simple string match, but this crude1891approach misses many possible instances.
Sincethe separately-studied task of named entity linking(NEL) is precisely what is needed to perform dis-tant supervision, it is interesting to see if today?soptimized linkers lead to improved performancewhen used to train extractors.Coreference, the task of clustering mentionsthat describe the same entity, may also be use-ful for increasing the number of candidate argu-ments.
Consider the following variant of our pre-vious example: ?While [he]1was born in [Ulm]2,[Einstein]3moved to Munich at an early age.
?Since mentions 1 and 3 corefer, one could con-sider using either the pair ?1, 2?
or ?3, 2?
(or both)for training.
Intuitively, it seems that ?1, 2?
is morerepresentative of BornIn and might generalize bet-ter, so we consider the use of coreference at bothtraining and extraction time.Semantic relations often have selectional prefer-ences (also known as type signatures); for exam-ple, BornIn holds between people and locations.Therefore, it seems promising to include entitytypes, whether coarse or fine grained in the dis-tantly supervised relation extraction process.
Weconsider two ways of adding this information.
Byusing NEL to get linked entities, we can imposetype constraints on the relation extraction systemto only allow relations over appropriately typedmentions.
We also investigate using coarse typesfrom NER to learn separate models for differentrelation type signatures in order to make the mod-els more effective.In summary, this paper represents the followingcontributions:?
We explore several dimensions for improv-ing distantly supervised relation extraction,including better argument identification dur-ing training and extraction using both NELand coreference, partitioning the model byrelation type signatures, and enforcing typeconstraints of linked arguments as a post-processing step.
While some of these ideasmay seem straightforward, to our knowledgethey have not been systematically studied.And, as we show, they lead to dramatic im-provements.?
Since previous datasets are incapable of mea-suring an extractor?s true recall, we intro-duce GORECO, a new exhaustively-labeleddataset with gold annotations for sententialinstances of 48 relations across 128 newswiredocuments from the ACE 2004 corpus (Dod-dington et al., 2004).?
We demonstrate that NEL argument identifi-cation boosts both precision and recall, andusing type constraints with linked argumentsfurther boosts precision, yielding a 43% in-crease in precision and 27% boost to re-call.
Using coreference during training ar-gument identification gives an additional 7%improvement to precision and further boostsrecall by 9%.
Partitioning the model by rela-tion type signature offers further benefits, soour best system yields a total boost of 44% toprecision and 70% to recall.2 Distantly Supervised ExtractionAt a sentence-level, the goal for relation extrac-tion is to determine for each sentence, what factsare expressed.
We describe these as relation an-notations of the form s?R(m1,m2), where s isa sentence, R ?
R is a relation name, R is ourfinite set of target relations, and m1and m2aregrounded entity mentions of the form (s, t1, t2, e),where t1and t2delimit a text span in the sentence,and e is a grounded entity.2.1 TrainingDuring training, the contents of the KB are heuris-tically matched to the training corpus accordingto the distant supervision hypothesis: if a relationholds between two entities, any sentence contain-ing those two entities is likely to express that rela-tion.The training KB ?
contains fact tuples of formR(e1, e2), where R ?
R is a relation name, R isour finite set of target relations, and e1and e2areground entities.
The training text corpus ?
con-tains documents, which contain sentences.
Argu-ment identification is performed over the text cor-pus to get grounded mentionsm.
Then during sen-tential instance generation, sentential instances ofthe form (s,m1,m2) are generated representinga sentence with two grounded mentions.
At thispoint, these sentential instances can be matchedto the seed KB, yielding candidate relation anno-tations of the form s?R(m1,m2) by finding allrelations that hold over the entities in a sententialinstance.
These candidate relation annotations areall positive instances to use for training.
Negativeinstance generation is also performed, generating1892negative examples of the form s?NA(m1,m2) in-dicating that no relation holds between m1andm2.
There are several heuristics for generatingnegative instances, and the number of negative ex-amples and how they are treated can greatly affectperformance (Min et al., 2013).Because the distant supervision hypothesis of-ten does not hold, this training data is noisy.
Thata fact is in the KB does not imply that the sen-tence in question is expressing the relation.
Therehas been much work in combating noise in dis-tant supervision training data, but one of the mostsuccessful ideas is to train a multi-instance classi-fier which assumes at-least-one relation holds forpositive bags.
We use Hoffmann et al.
(2011)?sMULTIR system, which uses a probabilistic graph-ical model to jointly reason at the corpus-leveland sentence-level, handles overlapping relationsin the KB so that multiple relations can hold overan entity pair, and scales to large datasets.2.2 ExtractionThe trained relation extractor can assign a mostlikely relation and a confidence score to a senten-tial instance (s,m1,m2).
To get these sententialinstances, argument identification and sententialinstance generation are applied to new documents.Then the relation extractor potentially yields a re-lation annotation of the form s?R(m1,m2), orpotentially no relation.
At extraction time a men-tion m might have a NIL link if a correspond-ing ground entity was not found during argumentidentification (meaning the entity is not in the KB).The relation annotations have associated confi-dence scores, so a threshold can be chosen to onlyuse high-confidence relation annotations.3 Argument IdentificationAn important piece of relation extraction is deter-mining what can be an argument, and how to forma semantic representation of it.
We define an argu-ment identification function ArgIdent?
(D), whichtakes a document D, finds potential arguments,and links them to entities in ?
if possible, yield-ing m, a set of grounded mentions in D. Pre-vious relation extraction systems have based thison NER.
We evaluate NER-based argument iden-tification against argument identification based onNEL, as well as NEL with coreference.3.1 Named Entity RecognitionNamed entity recognition (NER) tags spans of to-kens with basic types such as PERSON, ORGANI-ZATION, LOCATION, and MISC.
This is a highaccuracy tagging task often performed using asequence classifier (Finkel et al., 2005).
Rela-tion extraction systems can base their argumentidentification on NER, by using NER to identifytext spans indicating entities and then find corre-sponding entities in the KB through exact stringmatch (Riedel et al., 2010).
Some downsides ofusing NER with exact string match for relation ex-traction is that it does not allow for overlappingmentions, it can only capture arguments with fullnames, and it can only capture arguments withtypes of the NER system, e.g., ?politician?
mightnot be captured.3.2 Named Entity LinkingNamed entity linking (NEL) is the task of ground-ing textual mentions to entities in a KB, such asWikipedia.
Thus ?named entity?
here, has a some-what broader definition than in NER ?
these areany entities in the KB, not just those expressedwith proper names.
Hachey et al.
(2013) definethree stages that NEL systems take to performthis task: extraction (mention detection), search(generating candidate KB entities for a mention),and disambiguation (selecting the best entity for amention).
There has been much work on the taskof NEL in recent years (Milne and Witten, 2008;Kulkarni et al., 2009; Ratinov et al., 2011; Chengand Roth, 2013).Our definition of a function ArgIdent(D) iscompletely served by an NEL system.
It canfind any entity in the KB, and those entities aregrounded.
Additionally, NEL can have overlap-ping mentions as well as support for abbreviatedmentions like ?Obama?, or acronyms like ?US?.NEL does not seek to capture anaphoric mentions,however.3.3 Coreference ResolutionCoreference resolution is the task of clusteringmentions of entities together, typically within asingle document.
Using coreference, we can findeven more mentions than NEL, since it can findpronouns and anaphoric mentions.
We seek to usecoreference to add additional arguments to thosefound by NEL, and we refer to this combined ar-gument identification method as NEL+Coref.
Tak-1893ing in arguments from NEL argument identifica-tion and coreference clusters, we ground the clus-ters by picking the most common grounded entityfrom NEL mentions that occur in a coreferencecluster.
A difficulty is that mentions from NELand coreference can have small differences in textspans, such as whether determiners are included.We try to assign each NEL argument to a corefer-ence cluster, first looking for an exact span match,then by looking for the shortest coreference men-tion that contains it.
If the coreference cluster al-ready has matched an NEL argument through ex-act span match that is different from the one foundby looking for the shortest containing coreferencemention, the new NEL argument is not added.This gives for each coreference cluster a possiblegrounding to an entity in the KB.
What is providedas final arguments for NEL+Coref argument iden-tification are, in order, grounded NEL arguments,grounded coreference arguments that do not over-lap with previous arguments, NIL arguments fromNEL that do not overlap with previous arguments,and NIL arguments from coreference that do notoverlap with previous arguments.4 Type-AwarenessRelations have expected types for each argument.Entity types, whether coarse-grained, such as fromNER, or fine-grained, such as from Freebase enti-ties, are an important source of information thatcan be useful for making decisions in relation ex-traction.
We bring type-awareness into the systemthrough partitioning the model, as well as by en-forcing type constraints on output relation annota-tions.Model Partitioning Instead of building a singlerelation extractor that can generate sentential in-stances and then relation annotations with argu-ments of any type, we can instead build separaterelation extractors for each possible coarse typesignature, e.g., (PERSON, PERSON), (PERSON, LO-CATION), etc., and combine the extractions fromthe extractor for each type signature.
This modi-fication allows each trained model to only handleinstances of specific types, and thus relations ofthat type signature, allowing each to do a better jobof choosing relations within the type signature.Type Constraints We can additionally reject re-lation annotations where the types of the argu-ments do not agree with the expected types of therelation.
That is, we only accept a relation annota-tion s?R(m1,m2) when EntityTypes(e1) ?
?16=?
and EntityTypes(e2)?
?26= ?, wherem1is linkedto e1, m2is linked to e2, EntityTypes provides theset of valid types for an entity, ?1is the set of al-lowed types for the first argument of target relationr, and ?2for the second argument.5 Evaluation SetupsRelation extraction is often evaluated from amacro-reading perspective (Mitchell et al., 2009),in which the extracted facts, R(e1, e2), are judgedtrue or false independent of any supporting sen-tence.
For these experiments, however, we take amicro-reading approach in order to strictly eval-uate whether a relation extractor is able to extractevery fact expressed by a sentence s?R(m1,m2).Micro-reading is more difficult, but it providesfully semantic information at the sentence anddocument level allowing detailed justifications,and, for our purposes, allows us to better under-stand the effects of our modifications.
In orderto fairly evaluate different systems, even those us-ing different methods of argument identification,we want to use gold evaluation data allowing forvarying mention types.
We additionally use Hoff-mann et al.
(2011)?s sentential evaluation as-is inorder to better compare with prior work.
For ourtraining corpus, we use the TAC-KBP 2009 (Mc-Namee and Dang, 2009) English newswire corpuscontaining one million documents with 27 millionsentences.5.1 Hoffmann et al.
Sentential EvaluationHoffmann et al.
(2011) generated their gold databy taking the union of sentential instances wheresome system being evaluated extracted a relationas well as the sentential instances matching ar-guments in the KB.
They took a random sampleof these sentential instances and manually labeledthem with either a single relation or NA.
Althoughthis process provides good coverage, since is issampled from extractions over a large corpus, itdoes not allow one to measure true recall.
Indeed,Hoffmann?s method significantly overestimates re-call, since the random sample is only over senten-tial instances where a program detected an extrac-tion or a KB match was found.
Furthermore, thistest set only contains sentential instances in whicharguments are marked using NER, which makesit impossible to determine if the use of NEL or1894coreference confers any benefit.Finally, it does not allow for the possibility thatthere may be multiple relations that should be ex-tracted for a pair of arguments.
For example, aCeoOf relation, and an EmployedBy relation mightboth be present for (Larry Page, Google).
To ad-dress these issues, we manually annotate a full setof documents with relation annotations.
Becausewe are evaluating changing various aspects of thedistant supervision process, we cannot use Riedelet al.
(2010)?s distant supervision data as-is as oth-ers did on the Hoffmann et al.
(2011) sententialevaluation.
Instead, we use the TAC-KBP data de-scribed above.5.2 GoReCo EvaluationIn order to allow for variations on mentions (NER,NEL, and coreference each has its own definitionof what a mention boundary should be), we wantgold relation annotations over coreference clus-ters broadly defined to allow mentions obtainedfrom NER and NEL, as well as gold coreferencementions.
So as long as a relation extraction sys-tem extracts a relation annotation s?R(m1,m2)where m1and m2are allowed options (based ontext spans), it will get credit for extracting therelation annotation.
We introduce the GORECO(gold relations and coreference) evaluation to sat-isfy these constraints.We start with an existing gold coreferencedataset, ACE 2004 (Doddington et al., 2004)newswire, consisting of 128 documents.
To getrelation annotations over coreference clusters, wedefine two human annotation tasks and use theBRAT (Stenetorp et al., 2012) tool for visualizationand relation and coreference annotations.Relation Annotation The annotator is pre-sented with a document with gold mentions indi-cated and asked to determine for each sentence,what facts involving target relations are expressedby the sentence.
They draw an arrow for each factand label it with the relation.
They also have theability to add mentions not present (ReAnn men-tions).Supplemental Coreference Mentions fromNER and NEL are displayed along with ACE andReAnn mentions from the previous task.
Theannotator draws coreference links from NER orNEL mentions to an ACE or ReAnn mention ifthey are coreferent.We randomly shuffle the 128 ACE 2004newswire documents and use 64 as a developmentset and 64 as a test set.
To complete annotationsof these documents, we only used one original hu-man annotator (hired using the oDesk crowdsourc-ing platform) and found mistakes by having otherscheck the work, as well as checking false positivesof relation extractors on the development set tofind patterns of annotation mistakes.
On average,there are 7 relation annotations per document.For the GORECO evaluation, we define ourtrain/test split (with the separate TAC-KBP corpusused for training) such that each has a different setof documents and entities, in order to evaluate howwell the system performs on unseen entities.
To dothis, we remove entities found in the gold evalua-tion set from the training KB.
(We do not removeentities for the Hoffmann et al.
(2011) evaluation,since they do not.)
We choose the threshold con-fidence score for each system using the develop-ment set to optimize for F1 and report results onthe test set.5.2.1 Target RelationsSince we use a different evaluation, we also seek tochoose a more comprehensive and interesting setof relations than prior work.
Riedel et al.
(2010),whose train and test data is also used by Hoff-mann et al.
(2011) and Surdeanu et al.
(2012), useFreebase properties under domains /people, /busi-ness, and /location.
Since /location relations suchas /location/location/contains dominate the results(and are relatively uninteresting in that they rarelychange), we do not use any /location relations, andinstead use the domains /people, /business, and/organization (Google, 2012).Since many Freebase properties are betweenan entity and a table instead of anotherentity, we also use joined relations, suchas /people/person/employment_history ./ /busi-ness/employment_tenure/company , in this caserepresenting employment.
We bring in an addi-tional 20 relations of this form, also under /person,/business, and /organization.
Additionally we useNELL (Carlson et al., 2010a) relations mapped toFreebase by Zhang et al.
(2012).We only include a relation in our set of targetrelations if both of its entity arguments are con-tained in the set of entities found via NER withexact string match or NEL over the training cor-pus.
We also remove inverse relations, since theyrepresent needless duplication.
This gives us a set1895R of 105 target relations based on joins and unionsof Freebase properties.
Of the 105 target relations,48 were used at least once in the GORECO data.6 Experiments and ResultsWe conduct experiments to determine how chang-ing distantly supervised relation extraction alongvarious dimensions affects performance.
We ex-amine the choice of argument identification dur-ing training and extraction, as well as the effectsof model type partitioning, and type constraints.We consider the space of all combinations of thesedimensions, but focus on specific combinationswhere we find improvements.6.1 Relation Extraction SetupWe use and modify Hoffmann et al.
(2011)?s sys-tem MULTIR to control our experiments and asa baseline.
For NER argument identification aswell as for the use of NER in the features, we useuse Stanford NER (Finkel et al., 2005).
For NELargument identification we use Wikipedia Minerwith the default threshold 0.5, and allowing re-peated mentions (Milne and Witten, 2008).
SinceWikipedia Miner does not support NIL links, weuse non-overlapping NER mentions as NIL links.For coreference, we use Stanford?s sieve-based de-terministic coreference system (Lee et al., 2013).For sentential instance generation, we take allpairs of non-overlapping arguments in a sentence(in either order).
If the arguments have KB links,we do not allow sentential instances where botharguments represent the same entity.
We use thesame lexical and syntactic features as MULTIR,based on the features of Mintz et al.
(2009).
Asrequired for features, we use Stanford CoreNLP?stokenizer, part of speech tagger (Toutanova et al.,2003), and dependency parser (de Marneffe andManning, 2008), and use the Charniak Johnsonconstiuent parser (Charniak and Johnson, 2005).For negative training generation, we take a simi-lar approach to Riedel et al.
(2010) and define apercentage parameter n of the number of nega-tive instances divided by the number of total in-stances.
Experimenting with n ?
{0, 20%, 80%},we find that n = 20% works best for our evalua-tions, optimizing for F1, although using 80% neg-ative training gives high precision at lower recall.We use frequency-based feature selection to elimi-nate features that appear less than 10 times, whichis helpful both for reducing overfitting as well as0.60 0.1 0.2 0.3 0.4 0.510.70.80.9Relative RecallPrecisionHoffmann et al.
(2011)NER+LTNERNER+LT+CTFigure 2: Methods evaluated in the context ofHoffmann et al.
(2011)?s sentential extractionevaluation.
NER: our NER baseline used fortraining and extraction, LT: use NEL for train-ing only, CT: use coreference for training only.
(NER+LT+CT means we use NER for extraction,and NEL+Coref for training.
)constraining memory usage.
Since the perceptronlearning of MULTIR is sensitive to instance order-ing, we perform 10 random shuffles and averagethe models.For model type partitioning, when training withNER, we ensure that the NER types match thecoarse relation type signatures.
For NEL, we at-tempt to use NER for coarse types of arguments,but if an NER type is not present, we map the Free-base type to its FIGER type (Ling and Weld, 2012)to its coarse type.
For type constraints, we useFreebase?s expected input and output types for re-lations.
For NIL links, we use the NER type ofPERSON, ORGANIZATION, or LOCATION, if avail-able, mapping it to appropriate Freebase types.6.2 NER BaselineAs a result of a larger training set, as well as modelaveraging, our baseline, which is otherwise equiv-alent to the methods of Hoffmann et al.
(2011)and uses their MULTIR system, has slightly higherprecision as shown in Figure 2, curve NER.
It isalso higher than that of Xu et al.
(2013), whoachieved higher performance than Hoffmann etal.
(2011); our baseline gets 89.9% precision and59.6% relative recall, while Xu et al.
(2013)?s sys-tem gets 84.6% precision and 56.1% relative re-call.
See Figure 3 and Table 1 for results onGORECO.6.3 NEL and Type ConstraintsOn GORECO, using NEL argument identificationincreases recall and gives higher precision over theentire curve.
We further find that filtering resultsusing type constraints gives a large boost in pre-1896cision at a small cost to recall.
Note the increasein performance from NER to NEL to NEL+TC inFigure 3a, as well as in Table 1.
Using NEL givesmore recall, since it is able to capture argumentsthat NER cannot, such as professions like ?pa-leontologist?.
The decrease in recall from typeconstraints comes from false positives in the typeconstraints process including from non-ideal links,e.g., ?paleontologist?
might get linked to the entityPaleontology, so will not have the type required forthe Profession relation.On the Hoffmann et al.
(2011) sentential evalu-ation, we were not able to use NEL argument iden-tification at extraction time, because the instancesin the test set are from NER argument identifica-tion.
We tried using NEL only at training timeand found that it got similar performance to usingNER (Figure 2, curve NER+LT).
Doing the sameon GORECO yielded slightly lower recall, becauseof the mismatch of features learned from NEL ar-guments (Figure 3b, curve NER+LT).6.4 NEL+Coref Argument IdentificationUsing NEL+Coref for both training and extrac-tion (see Table 1) introduces noise from argumentsnot encountered during training time, but usingNEL+Coref just for training results in a decreasein recall but similar precision (Figures 2 and 3b).We found using NEL+Coref at test time unhelp-ful for this dataset, because there were no exam-ples we could find where coreference could re-cover arguments that NEL could not.
There werethree true positives from NEL+Coref involvingpronouns in the GORECO development set, butthere were also proper name versions of the ar-guments nearby in the same sentences, makingcoreference unnecessary.
Additionally, corefer-ence brings in many mentions such as times like?Friday?
or ?1954?
that do not have correspondingKB matches during training time.
These sententialinstances have similar features to others involv-ing coreference mentions, and there are not neg-ative instances to weigh against these, since thesetypes do not appear in the training data.
Better fea-tures more suited to coreference mentions could behelpful here.At both training and extraction time, corefer-ence can cluster together mentions that can be con-sidered to be separate, such as in ?Brian Kain, a33-year-old accountant?, ?Brian Kain?
and ?ac-countant?
are coreferent in the gold ACE 2004770 10 20 30 40 50 60 70100.20.40.60.8True Positives CountPrecisionNERNEL+TCNELNER+TPNEL+TC+TP(a)770 10 20 30 40 50 60 70100.20.40.60.8True Positives CountPrecisionNERNER+LTNER+LT+CTNEL+TC+CT+TPNEL+TPNEL+TCNEL+TC+TPNEL+CTNEL+TC+CT(b)Figure 3: Precision versus true positives countcurves for different versions of the system evalu-ated on the GORECO test set, containing 470 goldinstances.
NER/NEL: argument identification usedin training and extraction, LT: use NEL for train-ing only, CT: use coreference for training only, TC:type constraints, TP: model type partitioning.dataset.
This means that type constraints willdisregard a Profession annotation between thesewhen it should not, because ?Brian Kain?
(whichwould have been a NIL link) gets the link of ?ac-countant?.
This effect contributes to the decreasein recall.6.5 Model Type PartitioningUsing type partitioning helps both NER and NELbased models as shown with the +TP curves inFigure 3).
Partitioning by type signature results ineach model being able to better choose relationsfor sentential instances of that type signature.
Inthe Partitioned columns of Table 1, removing typepartitioning from the best system (NEL training1897Single PartitionedR P F1 R P F1NER trainingNER extraction 7.9 21.8 11.6 11.3 21.0 14.7NEL extraction 8.5 21.4 12.2 9.8 19.7 13.1NEL trainingNER extraction 9.6 21.1 13.2 8.9 25.1 13.2NEL extraction 10.0 30.5 15.1 15.3 16.7 16.0NEL w/TC extraction 11.7 31.1 17.0 13.4 31.3 18.8NEL+Coref trainingNER extraction 9.4 19.2 12.6 6.8 28.3 11.0NEL extraction 12.1 27.5 16.8 11.1 21.6 14.6NEL w/TC extraction 12.8 33.3 18.5 12.1 34.1 17.9NEL+Coref extraction 10.6 20.4 14.0 10.0 12.9 11.3NEL+Coref w/TC extraction 9.4 22.7 13.3 7.9 19.1 11.1Table 1: Evaluation of different versions of the relation extraction system on the GORECO test set.
Fornearly all systems, partitioning the model by argument types boosts F1, as does using NEL at eithertraining or extraction time, and using coreference at training time with type constraints (w/TC) raises F1except with coreference at extraction time and when combined with type partitioning.and extraction, with type constraints, Partitioned)results in a decrease in F1 from 18.8% to 17.0%.Table 2 shows by-relation performance results forthe best system (curve NEL+TC+TP in Figure 3a).6.6 Other Dimensions ExploredWe also experimented with adding generalizedfeatures that replaced lexemes with WordNetclasses (Fellbaum, 1998), which had uneven re-sults.
We observed a small but consistent improve-ment on the NER baseline (11.6% F1 to 12.7%F1 on GORECO), but after introducing NEL argu-ment identification and partitioning, we no longerobserved the improvement.
For some relations,there was a small gain in recall that was offset bya loss in precision, but for others, the gain in recalloutweighed the loss of precision.We experimented with a negative instance feed-back loop that ran a trained extractor over thetraining corpus and tested whether each extrac-tion made was in fact a negative example.
Eventhough the training corpus contains one milliondocuments, this method only yielded a few thou-sand new negative instances due to the difficultyof being certain an extraction should be negative.A na?ve approach would simply ensure that bothentities participate in a relation in the KB; this istroublesome, because of KB incompleteness andbecause of type errors.
For example Freebase con-tains BornIn(Barack Obama, Honolulu), but our ex-tractor extracted BornIn(Barack Obama, Hawaii).To avoid labeling this true extraction as a nega-tive instance we have to be robust about locationsemantics.
We selected new negative instancesNA(e1, e2) from our initial extractor that had e1in the knowledge base, with e1participating as thefirst argument in the extracted relation but with-out e2as the second argument.
The results werepromising for some relations but overall inconclu-sive as identifying true negatives is quite difficult.Relation #Extractions #TP #FPNationality 50 11 38Profession 43 23 20EmployedBy 27 17 10Spouse 22 2 20LivedIn 6 4 2OrgInCitytown 4 3 1AthletePlaysForTeam 2 2 0OrgType 1 1 0Table 2: By-relation evaluation of the best system(NEL with type constraints and type partitioning)on the GORECO test set.
The true positives (TP)are the number of gold relations over coreferenceclusters that matched, so multiple extractions canmatch a single true positive.7 Related WorkThere has been much recent work on distantly su-pervised relation extraction.
Mintz et al.
(2009)use Freebase to train relation extractors overWikipedia without labeled data using multi-classlogistic regression and lexical and syntactic fea-tures.
Hoffmann et al.
(2011) use a probabilis-tic graphical model for multi-instance, multi-label1898learning and extract over newswire text usingFreebase relations.
Surdeanu et al.
(2012) take asimilar approach and use soft constraints and lo-gistic regression.
Riedel et al.
(2013) integrateopen information extraction with schema-based,proposing a universal schema approach, includingusing features based on latent types.
There hasalso been recent work on reducing noise in dis-tantly supervised relation extraction (Nguyen andMoschitti, 2011; Takamatsu et al., 2012; Roth etal., 2013; Ritter et al., 2013).
Xu et al.
(2013) andMin et al.
(2013) improve the quality of distant su-pervision training data by reducing false negativeexamples.Distant supervision is related to semi-supervised bootstrap learning work such asCarlson et al.
(2010b) and many others.
Note thatdistant supervision can be viewed as a subroutineof bootstrap learning; bootstrap learning cancontinue the process of distant supervision bytaking the new tuples found and then training onthose again, and repeating the process.There has also been work on performing NELand coreference jointly (Cucerzan, 2007; Ha-jishirzi et al., 2013), however these systems do notperform relation extraction.
Singh et al.
(2013)performs joint relation extraction, NER, and coref-erence in a fully-supervised manner.
They getslight improvement by adding coreference, but donot use NEL.
Ling and Weld (2013) extend MUL-TIR to find meronym relations in a biology text-book.
They get slight improvement over NER byusing coreference to pick the best mention of anentity in the sentence for the meronym relation attraining and extraction time.8 Conclusions and Future WorkGiven the growing importance of distant supervi-sion, a comprehensive understanding of its vari-ants is crucial.
While some of the optimizationswe propose may seem intuitive, they have not pre-viously been systematically explored.
Our experi-ments show that NEL, type constraints, and typepartitioning are extremely important in order tobest take advantage of the seed KB during trainingas well as known information at extraction time.Our best system results in a 44% increase in pre-cision, and a 70% increase in recall over our NERbaseline using GORECO.
While we were not ableto evaluate all our methods on Hoffmann et al.
(2011)?s sentential evaluation, our baseline per-forms significantly better than previous methods,especially in precision, and training-only modifi-cations perform similarly in both evaluations.Future work will explore the use of NEL in dis-tantly supervised relation extraction further, tun-ing a confidence parameter for the NEL system,and determining whether different confidence pa-rameters should be used for training and extrac-tion.
Another possible direction is interleavingNEL with relation extraction by using newly ex-tracted facts to try to improve NEL performance.We freely distribute GORECO a new gold stan-dard evaluation for relation extraction consistingof exhaustive annotations of the 128 documentsfrom ACE 2004 newswire for 48 relations.
Thesource code of our system, its output, as well asour gold data are available athttp://cs.uw.edu/homes/mkoch/re.AcknowledgementsWe thank Raphael Hoffmann, Luke Zettlemoyer,Mausam, Xiao Ling, Congle Zhang, HannanehHajishirzi, Leila Zilles, and the anonymous re-viewers for helpful feedback.
Additionally, wethank Anand Mohan and Graeme Britz for annota-tions and revisions of the GORECO dataset.
Thiswork was supported by Defense Advanced Re-search Projects Agency (DARPA) Machine Read-ing Program under Air Force Research Labora-tory (AFRL) prime contract no.
FA8750-09-C-0181, ONR grant N00014-12-1-0211, a gift fromGoogle, a grant from Vulcan, and the WRF / TJCable Professorship.
This material is based uponwork supported by the National Science Founda-tion Graduate Research Fellowship under GrantNo.
DGE-1256082.ReferencesAndrew Carlson, Justin Betteridge, Bryan Kisiel, BurrSettles, Estevam R. Hruschka Jr., and Tom M.Mitchell.
2010a.
Toward an architecture for never-ending language learning.
In Proceedings of theAAAI Conference on Artificial Intelligence (AAAI-10).Andrew Carlson, Justin Betteridge, Richard C. Wang,Estevam R. Hruschka, Jr., and Tom M. Mitchell.2010b.
Coupled semi-supervised learning for infor-mation extraction.
In Proceedings of the Third ACMInternational Conference on Web Search and DataMining, WSDM ?10, pages 101?110, New York,NY, USA.
ACM.1899Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,ACL ?05, pages 173?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Xiao Cheng and Dan Roth.
2013.
Relational inferencefor wikification.
In EMNLP.Silviu Cucerzan.
2007.
Large-scale named entity dis-ambiguation based on wikipedia data.
In EMNLP-CoNLL, pages 708?716.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The stanford typed dependencies rep-resentation.
In Coling 2008: Proceedings of theWorkshop on Cross-Framework and Cross-DomainParser Evaluation, CrossParser ?08, pages 1?8,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.George R. Doddington, Alexis Mitchell, Mark A. Przy-bocki, Lance A. Ramshaw, Stephanie Strassel, andRalph M. Weischedel.
2004.
The automatic contentextraction (ace) program-tasks, data, and evaluation.In LREC.Christiane Fellbaum, editor.
1998.
WordNet: an elec-tronic lexical database.
MIT Press.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,ACL ?05, pages 363?370, Stroudsburg, PA, USA.Association for Computational Linguistics.Google.
2012.
Freebase data dumps.https://developers.google.com/freebase/data.Ben Hachey, Will Radford, Joel Nothman, MatthewHonnibal, and James R. Curran.
2013.
Evalu-ating entity linking with wikipedia.
Artif.
Intell.,194:130?150, January.Hannaneh Hajishirzi, Leila Zilles, Daniel S. Weld, andLuke S. Zettlemoyer.
2013.
Joint coreference res-olution and named-entity linking with multi-passsieves.
In EMNLP, pages 289?299.
ACL.Raphael Hoffmann, Congle Zhang, Xiao Ling,Luke Zettlemoyer, and Daniel S. Weld.
2011.Knowledge-based weak supervision for informationextraction of overlapping relations.
In ACL-HLT,pages 541?550.Sayali Kulkarni, Amit Singh, Ganesh Ramakrishnan,and Soumen Chakrabarti.
2009.
Collective annota-tion of wikipedia entities in web text.
In Proceed-ings of the 15th ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining,KDD ?09, pages 457?466, New York, NY, USA.ACM.Heeyoung Lee, Angel Chang, Yves Peirsman,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2013.
Deterministic coreference resolu-tion based on entity-centric, precision-ranked rules.Comput.
Linguist., 39(4):885?916, December.Xiao Ling and Daniel S. Weld.
2012.
Fine-grainedentity recognition.
In Proceedings of the 26th Con-ference on Artificial Intelligence (AAAI).Xiao Ling and Daniel S. Weld.
2013.
Extractingmeronyms for a biology knowledge base using dis-tant supervision.
In Automated Knowledge BaseConstruction (AKBC) 2013: The 3rd Workshop onKnowledge Extraction at CIKM.Paul McNamee and Hoa Trang Dang.
2009.
Overviewof the tac 2009 knowledge base population track.
InText Analysis Conference (TAC), volume 17, pages111?113.David Milne and Ian H. Witten.
2008.
Learning to linkwith wikipedia.
In Proceedings of the 17th ACMConference on Information and Knowledge Man-agement, CIKM ?08, pages 509?518, New York,NY, USA.
ACM.Bonan Min, Ralph Grishman, Li Wan, Chang Wang,and David Gondek.
2013.
Distant supervision forrelation extraction with an incomplete knowledgebase.
In Proceedings of NAACL-HLT, pages 777?782.Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-rafsky.
2009.
Distant supervision for relation ex-traction without labeled data.
In Proceedings of the47th Annual Meeting of the Association for Compu-tational Linguistics (ACL-2009), pages 1003?1011.Tom M. Mitchell, Justin Betteridge, Andrew Carlson,Estevam Hruschka, and Richard Wang.
2009.
Pop-ulating the semantic web by macro-reading internettext.
In The Semantic Web-ISWC 2009, pages 998?1002.
Springer.Truc-Vien T. Nguyen and Alessandro Moschitti.
2011.End-to-end relation extraction using distant super-vision from external semantic repositories.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies: Short Papers - Volume 2, HLT?11, pages 277?282, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Lev Ratinov, Dan Roth, Doug Downey, and MikeAnderson.
2011.
Local and global algorithmsfor disambiguation to wikipedia.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies - Volume 1, HLT ?11, pages 1375?1384,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions with-out labeled text.
In ECML/PKDD (3), pages 148?163.1900Sebastian Riedel, Limin Yao, Benjamin M. Marlin, andAndrew McCallum.
2013.
Relation extraction withmatrix factorization and universal schemas.
In JointHuman Language Technology Conference/AnnualMeeting of the North American Chapter of the Asso-ciation for Computational Linguistics (HLT-NAACL?13), June.Alan Ritter, Luke Zettlemoyer, Mausam, and Oren Et-zioni.
2013.
Modeling missing data in distant super-vision for information extraction.
TACL, 1:367?378.Benjamin Roth, Tassilo Barth, Michael Wiegand, andDietrich Klakow.
2013.
A survey of noise reductionmethods for distant supervision.
In Proceedings ofthe 2013 Workshop on Automated Knowledge BaseConstruction, AKBC ?13, pages 73?78, New York,NY, USA.
ACM.Sameer Singh, Sebastian Riedel, Brian Martin, JiapingZheng, and Andrew McCallum.
2013.
Joint infer-ence of entities, relations, and coreference.
In CIKMWorkshop on Automated Knowledge Base Construc-tion (AKBC).Pontus Stenetorp, Sampo Pyysalo, Goran Topi?c,Tomoko Ohta, Sophia Ananiadou, and Jun?ichi Tsu-jii.
2012. brat: a Web-based Tool for NLP-AssistedText Annotation.
In Proceedings of the Demonstra-tions at the 13th Conference of the European Chap-ter of the Association for Computational Linguistics,Stroudsburg, PA, USA, April.
Association for Com-putational Linguistics.Mihai Surdeanu, Julie Tibshirani, Ramesh Nallapati,and Christopher D. Manning.
2012.
Multi-instancemulti-label learning for relation extraction.
In Pro-ceedings of the 2012 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning, pages 455?465.
Association for Computational Linguistics.Shingo Takamatsu, Issei Sato, and Hiroshi Nakagawa.2012.
Reducing wrong labels in distant supervi-sion for relation extraction.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics: Long Papers - Volume 1, ACL?12, pages 721?729, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology- Volume 1, NAACL ?03, pages 173?180, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Wei Xu, Zhao Le, Raphael Hoffmann, and Ralph Gr-ishman.
2013.
Filling knowledge base gaps for dis-tant supervision of relation extraction.
In Proceed-ings of the 2013 Conference of the Association forComputational Linguistics (ACL 2013), Sofia, Bul-garia, July.
Association for Computational Linguis-tics.Congle Zhang, Raphael Hoffmann, and Daniel S.Weld.
2012.
Ontological smoothing for relation ex-traction with minimal supervision.
In AAAI.1901
