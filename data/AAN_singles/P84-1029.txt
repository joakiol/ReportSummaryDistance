Prevent ing  False Inferences 1Aravind Joshi and Bonnie WebherDepartment of Computer and Information ScienceMoore School/D2University of PennsylvaniaPhiladelphia PA 19104Ralph M. Weischedel 2Department of Computer & Information SciencesUniversity of DelawareNewark DE 19716ABSTRACTI I n t roduct ionIn cooperative man-machine interaction, it is taken asnecessary that a system truthfully and informativelyrespond to a user's question.
It is not, however,sufficient.
In particular, if the system has reason tobelieve that its planned response nfight lead the user todraw an inference that it knows to be false, then itmust block it by nmdifying or adding to its response.The problem is that a system neither can nor shouldexplore all eonchtsions a user might possibly draw: itsreasoning must be constrained in some systematic andwell-motivated way.Such cooperative behavior was investigated in \[5\], inwhich a modification of Griee's Maxim of Quality isproposed:Grice's Maxim of Quality-Do not say what you believe to be false or for whichyou lack adequate vidence.Joshi's Revised Maxim of Quality -If you, the speaker, plan to say anything which mayimply for the hearer something that you believe to befalse, then provide further information to block it.This behavior was studied in the context of interpretingcertain definite noun phrases.
In this paper, weinvestigate this revised principle as applied to questionanswering.
In particular the goals of the researchdescribed here are to:I. characterize tractable cases in which thesystem as respondent (R) can anticipate thepossibility of the user/questioner (Q)drawing false conclusions from its responseand can hence alter or expand its responseso as to prevent it happening;2. develop a formal method for computing theprojected inferences that Q may draw froma particular response, identifying those1This work is partially supported by NSF Grants MCS81-07290, MCS 8.3-05221, and \[ST 83-11,100.2At present visiting the Department of Computer andInformation Science, University of Pennsylvania, Philadelphia, PA19104.factors whose presence or absence catalyzesthe inferences;3. enable the system to generate modificationsof its response that can defuse possible falseinferences and that \[nay provide additionaluseful information as well.Before we begin, it is important o see how this workdiffers from our related work on responding when thesystem notices a discrepancy between its beliefs andthose of its user \[7, 8, 9, 18\].
For example, if a user asks?
How many French students failed CSEI21 last term?'
,he shows that he .believes inter alia that the set ofFrench students is non-empty, that there is a courseCSEI21, and that it, was given last term.
If the systemsimply answers "None' ,  he will assume the systemconcurs w'ith these b~diefs ince the answer is consistentwith them.
Furthermore, he may conclude that Frenchstudents do r;'d.her well in a difficult course.
But thismay be a false conclusion if the system doesn't hold toall of those beliefs (e.g., it doesn't know of any Frenchstudents).
Thus while the system's assertion "NoFrench students failed CSEI21 last term" is true, it hasmisled the user (1) inlo believing it concurs with theuser's beliefs and (2) into drawing additional falseconclusions from its response.
3 The differences betweenthis related work and the current enterprise are that:1.
It is no_~t assumed in the current enterprisethat there is any overt indication that thedomain beliefs of the user are in any way atodds with those of the system.2.
In our related work, the user draws a falseconclusion from what is said because thepresuppositions of the response are not inaccord with the system's beliefs {following anice analysis in \[lO\]).
In the currententerpri.~e, the us~,r draws a false conclusionfrom what is said because the system'sresponse behavior is not in accord with theuser's expectations.
It.
may or may not also31t is a feature of Kaplan's CO-OP system \[7\] that it point~ outthe discrepancy by saying "| don't know of any French students ?134involve false domain beliefs that the systemattributes to the user.In this paper, we describe two kinds of falseconclusions we are attempting to block by modifyingotherwise true response:?
false conclusions drawn by standard defaultreasoning - i.e., by the user/listenerconcluding (incorrectly) that there is nothingspecial about this case?
false conclusions drawn in a task-orientedcontext on the basis of the user'sexpectations about the way a cooperativeexpert will respond.In Section II, we discuss examples of the first type,where the respondent (R) can reason that the questioner{Q) may inappropriately apply a default rule to the(true) information conveyed in R's response and hencedraw a false conclusion.
We characterize appropriateinformation for R to include in his response to block it.In Section HI, we describe xamples of the second type.Finally, in Section IV, we discuss our claim regardingthe primary constraint posed here on limiting R'sresponsibilities with respect to anticipating falseconclusions that Q may draw from its response: that is,it is only that part of R's knowledge base that isalready in focus (given the interaction up to that point,including R's formulating a direct answer to Q's query)that will be involved in anticipating the conclusionsthat Q may draw from R's response.H B lock ing  Potent ia l  M isapp l i cat ion  of  Defaul tRulesDefault reasoning is usually studied in the context of alogical system in its own right or an agent who reasonsabout the world from partial information and hencemay draw conclusions unsupported by traditional logic.However, one can also look at it in the context ofinteracting agents.
An agent's reasoning depends notonly on his perceptions of the world but also on theinformation he receives in interacting with other agents.This information is partial, in that another agentneither will nor can make everything explicit.
Knowingthis, the first agent (Q) will seek to derive informationimplicit in the interaction, in part by contrasting whatthe other agent (R) has made explicit with what Qassumes would have been made explicit, were somethingelse the case.
Because of this, R must be careful toforestall inappropriate derivations that Q might draw.The question is on what basis R should rea.~on that Qmay ~sume some piece of infotmati(>n (P) would havebeen made explicit in the interaction, were it the ease.One basis, we contend, is the likelihood that Q willapply some staudard efault rule of the type discussedby Reiter \[15\] if R doesn't make it explicite that therule is not applicable.
Reiter introduced the idea ofdefault rules in the stand-alone context of an agent orlogical system filling in its own partial information.Most standard default rules embody the sense that"given no reason to suspect otherwise, there's nothingspecial about the current case'.
For example, for a birdwhat would be special is that it can't fly - i.e., ?Mostbirds fly?.
Knowing only that Tweety is a bird and noreason to suspect otherwise, an agent may conclude bydefault that there's nothing special about Tweety andso he can fly.This kind of default reasoning can lead to falseconclusions in a stand-along situation, but also in aninteraction.
That is, in a question-answer interaction, ifthe respondent (l{) has reason for knowing or suspectingthat the situation goes counter to the standard efault,it seems to be common practice to convey thisinformation to the questioner (Q), to block hispote, tially a.ssuming the default.
To see this, considerthe following two examples.
(The first is very much likethe "Tweety" case above, while the second seems moregeneral.)A.
Example  1Suppose it's the case that most associate professors aretenured and most of them have Ph.Ds.
Consider thefollowing interchangeQ: Is Sam an ~sociate professor?R: Yes, but he doesn't have tenure.There are two thi,  gs to account for here: (1) Given theinformation w&s not requested, why did R include the"but" clause, and (2) why this clause and not anotherone?
We claim that the answer to the second questionhas to do with that part of R's knowledge base that iscurrently in focus.
This we discuss more in Section IV.In the meantime, we will just refer to this subset as?
RBc ".Assume RBc contains at least the followinginformation:(a) Sam is an associate professor.
(b) Most associate professors are tenured.
(c) Sam is not tenured.
(b) may be in RBc because the question of tenure maybe in context.
Based on RBc, R's direct response isclearly "Yes'.
This direct response however eouJd leadQ to conclude falsely, by default reasoning, that Sam istenured.
That is, R can reason that, given just (b) andhis planned response "Yes" (i.e., if (c) is not in Q'sknowledge base}, Q could infer by default reasoningthat Sam is tenured, which R knows with respect o!RBc is false.
Hence, R will modify that plannedresponse to block this false inference, as in the responseabove.In general, we can represent R's reasoning about Q'sreaction to a simple direct response ?Yes, B(a)' ,  givenQ believes "Most Bs F=, in terms of the followingdefault schema, using the notation introduced in \[15 I.135told{ILQ,l~(c)) k (Most x)\[B(x) = F(x)\]&-~h:,ld(R,Q,-~Flc)): M(F\[c})..__" .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.F(c)As in Reiter's discussion, "M(P)" means it is consistentto assume that P. In the associate professor example, Bcorresponds to the predicate "is an associate professor',F, to the predicate "has tenure',  and c, to Sam.
Usingsuch an inslantiated rule schema, R will recognize thatQ is likely to conclude F(c) - "Sam has tenure" - which.is false with rvspe(.t o RBc {and hence, with respect oall of R's knowledge base).
Thus R will modify hisdirect response so as to block this false conclusion.B.
Example  2Consider a user one of the mail systems on theDEC-20.
To exit from this system, a user who hasfinished reading all the messages he earlier specified canjust type a carriage return.
To exit under othercircumstances, the user must type QUIT.
Consider thefollowing interchange between a new user who hasfinished reading all his messages and either a mailsystem expert or the mail system itself.Q: How (In I get out of mail?R~ Since you h:tve read all your specified messages,you can just type a carriage return.
In all cases,you (':ill got ()lit by typing QHT.Here tile prohh,m is to account for all that part of R'sresponse beyond the simple truthful statement "Youcan type a carriage return.
"A general statement of this probh,m is a.s follows:Agent Q is in one situation (Sl) and wants to be inanother ($2).
There is a general procedure P forachieving $2 from any of several situations including Sl.There is a special prodecure P* (i.e., shorter, faster,simpler, etc.)
for achieving $2 frolu Sl.
Q doesn't knowhow to achieve $2, but R does (including proced,res Pand P*).
Q asks R how to achieve $2.If R knows.i~lat Q is in situation SI and truthfullyresponds to Q's request by simply telling him P*, Qmay falsely conclude that P* is a general procedure forachieving $2.
That is, as in the Tweety and Samexamples, if Q has no reason to suspect anything specialabout SI (such that P* only applies to it), then there isnothing special about it.
Therefore P* is adequate forachieving $2, whatever situation Q is in.
4 Later when Qtries to apply P* in a different situation to achieve $2,he may find that it doesn't work.
As a particularexaml)le of this, consider the mail case again.
In thisca .se~SI = Q has read all his messages$2 = Q is out of the mail systemP ~--- typing QUITP* - -  typing a carriage return~Lssume RBc contains at least the followinginforma.tion:(a) Sl(b) want(Q,S2)(c) ?s6S .
P(s) = S2(d) P*(S l )  = s2(e) S l6 r(f) simpler(P*,P)(g) VsE,~.
"-{s = SI) =* -~(P*ls) = $21where 17 is some set of states which includes SI and P(s)indicates action P applied to state S.Based on RBc, R's direct response would be "You canexit the mail system by typing carriage return' .
(It is&ssumed that an expert will always respond with the"best" procedure according to some metric, unle..~ heexplicitly indicates otherwise - of.
Section lIl, case 2}.However, this could lead Q to conclude falsely,-bydefault, something along tile lines of Vs .
P*(s) ---- $2.
5Thus R will modify his planned response to callattention to SI {in particular, how to recognize it) andthe limited applicability of P* to SI alone.
The othermodification to R's response ( ' In  all cages, you can getout by typing QUIT') ,  we would ascribe simply to R'sadhering to Grice's Alaxim o f  Quant i ty - "Make yourcontribution ,~s informative as is required for tilecurrent purposes of tile exchange" given R'sassumption of what is required of him in his role asexpert/teacher.HI B lock ing  Fa lse  Conc lus ions  in  Exper tI n teract ionsTile situations we are concerned with here are ones inwhich the system is explicitly tasked with providinghelp and expertise to the user.
In such circumstances,the user has a strong expectation that the system hasboth the experience and motivation to provide the mostappropriate help towards achieving the user's goals.
Theuser does not expect behavior like:Q: How can I get to Camden?R: You can't.As many studies have shown Ill, what an advice seeker(Q) expects is that an expert (R) will attempt torecognize what plan Q is attempting to follow in pursuitof what goal and respond to Q's question accordingly.Further studies \[11, 12, 13\] show that Q may alsoexpect that R will respond in terms of a better plan ifthe recognized one is either sub-optimal or unsuitablefor attaining Q's perceived goal.
Thus because of thisprinciple of "expert cooperative behavior', Q mayexpect a response to a more general question than theone he has actually asked.
That is, in asking an expert?
flow do 1 do X?"
or "Can I do X?
', Q is anticipating aresponse to "How can I achieve my goal?
"4Moreover if Q (falsely) believes that R doesn't know Q is in SI,Q will certainly assume that P* is a general procedure.
However,this isn't necessary to the default reasoning behavior we areinvestigating.5Clearly , this is only for some subset of states, onescorresponding to being in the mail system.136Con',id,.r a slud,.ut ((,~) :+skhig th,' foll,+,+i.g que+thm, near theend of the te rm.Q'.
Can I dr~q, C1~,-,77?Since it is already too late to drop a course, ti~e o~.
!y dire,'t answerthe ,x~*~rt (R) can give is "No'.
Of course, part of :,:, expert'sknowledge concerns the typical states users get into and thepossible actions that permit transitions between them.
Moreover itis al~o part of this expertise to infer such states from the currentstate of the inlrerac(.
ion, Q's query, some shared knowledge of Q'sgoals and Pxpectali ,ns and the shared assmnption that an expert isexpected to attend to these higher goals.
How the system shouldgo about in"erring these states is a difficult task that others areexami, iug \[2, 12, 13\].
We assume that such an inference has beenmade.
We al,~o assume for simplicity that the states are uniquelydet.ermined.
For example, we assume that the system has inferredthat Q i.,: in state Sb (student is doing badly in the course} andwants to be in a state Sg {student is in a position to do better inthis course or another one later), and that the a~tion a (dioppingthe course) will take him f:om Sb to Sg.Given this, the response in (2) may lead Q to draw someconclusiuns that I/.
knows to be false.
For example, R can reasonthat since a principle of cooperative behavior for an expert is totell Q the best way to go from Sb to Sg, Q is likely to concludefrom R's response that there is no way to go from Sb to Sg.
Thiscon+:lusion however would be false if R knows some other ways ofgoing from Sb to Sg.
To avoid potenlially misleading Q, R mustprovide additional information, such asR: No, bul you can take an incomplete and ask formore time to finish the work.As we noted earlier, an important question is how muchreasoning R should do to block fals~ conclusions on Q's part.Again.
we assume that R should only concern itself with those falseconclusions that Q is likely to draw that involve that part of R'sknowledge base currently in focus (RBc}, including of course thatsubset i t  nc~ds in order to answer the query in the first place.We will make this a little more precise by considering severalcases corresponding to the different states of R's knowledge basewith r~peet  to Sb, Sg.
and tran~iti,m~ between them.
Forconvenie,,.e, ~,: ~ill give an appropriate re~p~mse in terms of Sb,Sg and the actions.
Clearly, it should be given in terms ofdescriptions of ~lat,.s and actions understandable to Q.
(Moreover,by making further assumptions about Q's beliefs, R may be able tovalidly trim some of its respond.)1.
Suppose that it is possible to go from Sb to Sg bydropping the course aml that.
this is the only actionthat will take one from Sb to Sg.Sb SgIn this ca.se, the respon~ isR: Yes.
ct is t h~ only action that will takeyou fr,,m Sb to St.2.
Suppose that in addition to going from Sb to Sg bydropping the cour~,~o there is a better way, say ~, ofdoing so.e?
.jSb : SgIn this ca~e, the response is6"Betteruess" is yet another urea for future research.H: Yes, but there is a better action ,9 thatwill take you from Sb to Sg.3.
Suppose that dropping the course does not take youfrom Sb to St, but another action ~ will.
This is thesituation we considered in our earlier discussion.Sb SgIn this case the response isH: No, but there is an action ~ that willtake you from Sb to St.4.
Suppose that there is no action that will take one fromSb to Sg.Sb Sg ,/In this the rcspon~ isR: No.
There is no action that will take youfrom Sb to Sg.Of course, other situations are possible.
The point, however, isthat the additional information that R provides to prevent Q fromdrawing fa l~ conclusions is limited to just that part of R'sknowledge hase that R is focussed on in answering Q's query.IV  Const ra in ing  the  Renpondent ' s  Ob l igat ionsAs many people have observed - from studies across a range oflinguistic phenomena, including co-referring expressions \[3, 4, 16\],left dislocations \[14\], epitomizatkm \[17\], etc.
- a speaker (R)normally focuses on n particular part of its knowledge base.
Whathe focuses on dcpends in part oil (1) eoutext, (2} R's partialknf~wledge of Q's overall goals, as well as what Q knows already asa result of the interaction up to that point, and (3} Q's particularquery, etc.
The precise nature of how these various factors affectfocusing is complex and is receiving much attention \[3, 4, 16\].However, no matter how these various factors contribute tofocusing, we can certainly assume that H comes to focus on asubset of its knowledge base in order to provide a d i re r  answer toQ's query (at some level of inl,.rpretalion).
Let us call this subsetRBc for "R's current belief.~ .
Our claim is tlmt one importantconstraint on cooperative behavior is that it is determined b.v RBconly.
Clearly the i;ib~rmal.ion eeded for a direct response iscontained in RBc, a.~ is the information needed for many types ofhelpful responses.
In other words, RBc - -  that part of R'sknowledge base that R deeide~ to focus on in order to glve-a direct.response to Q's quer~ - also has the information needed togenerate several classes of h~Ipful responses.
The simplest ease ispresupposition failure \[7\], as in (he followingQ: l low many A's were given in (',IS 500 ?where Q presumes that CIS 500 was offered.
In trying toformulate a direct response, R will have to ascertain that CIS 500was offered.
If it was (Q's presumption is true}, then R can goahead and give a direct response.
If not, then R can indicate thatCIS 500 was not offered and thereby avoid misleading Q.
All ofthis is straightforward.
The point here is that the informationneeded to provide this extra response is already there in that partof R's knowledge base which R had to look up anyway in order totry to give the direct, response.In the above example, it is clear how the response can belocalized to RP, c. We would like to claim that this approach has awider applicability: that RBc alone is the basis for responses thatanticipate and attempt to block interactional defaults as well.Since RBc contains the information for a direct response, R canplan one (r}.
From r, R can reason whether it is possible for Q toinfer some conclusion (g) which R knows to be false because -~g isin RBe.
If so, then R should modify r so as to eliminate thispossibility.
The point is that the only false inferences that R willattempt to block are those whose falsity can be checked in RBc.137There may be other false inferences that Q may draw, whosefalsity cannot be deterntined solely with respect o RBc (althoughit might be possible with respect o R's entire knowledge base).While intuitively this may not seen enough of a constraint on theamount of anticipatory reasoning that Joshi's revised maximimposes on R, it does constrain things a lot by only considering a(relatively small) subset of knowledge base.
Factors such ascontext may further delimit S's responses, but they will all berelative to RBc.V Conclus ionThere are many gaps in the current work and several aspects notdiscussed here.
In particular,1.
We are developing a formMism for accommodating thesystem's reasoning based on a type of HOLDSpredicate whose two arguments are a proposition and astate; see \[6\].2.
We are working on more examples, especially moreproblematic cases in which, for example, a directanswer to Q's query would be myes m \[or the requestedprocedure} BUT a response to Q's higher goals wouldbe "no t or "no" plus a warning - e .g .
,Q: Can I buy a 50K savings bond?S: Yes, but you could get the same securityon other investments with higher returns.3.
We need to be more precise in specifying RBc, if we areto assume that all the information eeded to accountfor R's cooperative behevior is contained there.
Thismay in turn reflect on how the user's knowledge basemust be structured.4.
We need to be more precise in specifying how defaultrules play a role in causing R to modify his directresponse, in recognition of Q's likelihood of drawingwhat seems like a generalized "script" default - if thereis no reason to assume that there is anything specialabout the current case, don't.REFERENCES\[I\] Allen, J.Recognizing Intentions from Natural Language Utterances.In M. Brady (editor), Computational Models of Discourse,?
M1T Press, Cambridge MA, 1982.\[2\] Carberry, S.Tracking User Goals in an Information-SeekingEnvironment.In Proceedings of the National Conference on ArtificialIntelligence, pages 59-63.
AAAI, 1983.\[31 Groat, B.The Representation a d Use of Focus in DialogueUnderstanding.Technical Report 151, SRI International, Menlo Park CA,1977.14\] Grosz, B., Joshi, A.K.
& Weinstein, S.Providing a Unified Account of Definite Noun Phrases inDiscourse.In Proc.
?Mst Annual Medin9, pages 44-50.
Assoc.
forComputational Ling., Cambridge MA, June, 1983.15} Joshi, A.K.Mutual Beliefs in Question Answering Systems.In N. Smith leditor), Mutual Belief, .
Academic Press,New York, 1982.\[6\] Joshi, A., Webber, B.
& Wei~hedel, R.Living Up to Expectations: Computing Expert Responses.In Proceedings of AAAI-8~.
Austin TX, August, 1984?07\] Kaplan, J.Cooperative Responses from a Portable Natural LanguageDatabase Query System?In M. Brady (editor), Computational Models o\] Discourse,?
MIT Press, Cambridge MA, 1982.Isl Mays, E.Failures in natural anguage systems: application to databa~e query systems.In Proc.
First National Conference on ArtificialIntelligence (AAAI\].
Stanford CA, August, 1980.191 McCoy, K.Correcting Miseunceptions: What to S~y.In CH1'83 Conference Human Fhctors in ComputingSystems.
Cambridge MA, December, 1983.\[101 Nlercer, R. & Rosenberg, R.Generating Corrective Answers by ComputingPresuppositions of Answers, not of Questions.In Proceedings of the 1984 Con fere, ce, pages 16-19.Canadian Society for Computational Studies ofIntelligence, University of Western Ontario, London,Ontario, May.
1984.\[111 Pollack, M., Hirschberg, J. and Webber.
B.User Participation in the Reasoning Processes of ExpertSystems.In Proc.
AAA\[-8e.
CMU,  Pittsburgh PA, August, 1982?A longer version appears as Technical Report CIS-8~9,Dept.
of Computer and Information Science, Universityof Pennsylvania, July 1982.112\] Pollack, Martha E.Goal Inference in Expert S~lstesm.Technical Report MS-CIS-84-07, University ofPennsylvania, 1984.Doctoral dissertaion proposal.113\] Pollack, M.Good Answers to Bad Questions.In taeoc.
Canadian Sodettt for Computational Studies ofIntelligence (CSCSI\], Univ.
of Western Ontario,Waterloo, Canada, May, 1984.\[141 Prince, E.Topicalization, Focus Movement and Yiddish Movement: Apragmatic differentiation.In D. Alford et al (editor), Proceedin#s of the 7th AnnualAltering, pages 249-64.
Berkeley Linguistics Society,February, 1981.
(15\] Reiter, R.A Logic for Default Reasoning.Artificial lnteUigence 13:81-132, 1980.\[16\] Sidner, C,.
L.Focusing in the Comprehension f Definite Anaphora.In M. Brady (editor), Computational Models of Discourse,?
MIT Press, Cambrid~.e MA, 1982.117\] Ward, G.A Pragmatic Analysis of E~.,itomization: Topicalization it'stint.In Procredin9.~ of the Summer Aft.sting 198?.
LSA, CollegePark MD, Augu.~t, 1982.Also in Papers in Linguisti,:s 17.
(181 Webber, B.
& Mays, E.Varieties of User Misconceptions: Detection and Correction.In Proc.
IJCAI-8.
Karlsruhe, Germany, August, 1983.138
