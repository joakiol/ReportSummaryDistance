Evo lu t ion  o f  a Rap id ly  Learned Representat ion  for SpeechRamin Charles Nakisa and Kim PlunkettDepartment of Experimental PsychologyOxford UniversitySouth Parks RoadOxford OX1 3UD, UK{ramin.
nakisa,kim, plunkett }@psy.
ox.
ac.
ukAbstractNewly born infants are able to finelydiscriminate almost all human speechcontrasts and their phonemic categoryboundaries are initially identical, even forphonemes outside their target language.
Aconnectionist model is described which ac-counts for this ability.
The approach takenhas been to develop a model of innatelyguided learning in which an artificial neu-ral network (ANN) is stored in a "genome"which encodes its architecture and learn-ing rules.
The space of possible ANNs issearched with a genetic algorithm for net-works that can learn to discriminate hu-man speech sounds.
These networks per-form equally well having been trained onspeech spectra from any human languageso far tested (English, Cantonese, Swahili,Farsi, Czech, Hindi, Hungarian, Korean,Polish, Russian, Slovak, Spanish, Ukranianand Urdu).
Training the feature detec-tors requires exposure to just one minuteof speech in any of these languages.
Cate-gorisation of speech sounds based on thenetwork representations showed the hall-marks of categorical perception, as foundin human infants and adults.1 IntroductionPrecocious abilities in newborn infants are fre-quently taken as evidence for pre-specification f therepresentations that support those abilities.
Theprespecifications of these representations is innatelydetermined, presumably in the genome of the indi-vidual.
One such ability is that of newborn infantsto be universal listeners, able to discriminate speechcontrasts of all languages.
This is all the more re-markable since the low-pass filtered speech soundsthat foetuses hear in utero vary widely between dif-ferent languages.Eimas et al (1971) showed that 1-4 month oldinfants displayed categorical perception of the syl-l ab les /ba /and/pa / .
That is to say, infants carveup the phonetic space into a set of categories withsharp boundaries.
Variants of a phoneme, such as/b/ ,  are not discriminable, even though they dif-fer acoustically by the same amount as /p /  and/b /  (although see (guhl, 1993)).
More recent re-search has shown that the categories are univer-sal, so that English-learning infants can discriminatenon-native contrasts in Czech (Trehub, 1973), Hindi(Werker, Gilbert, Humphrey, ~ Tees, 1981), Nth-lakampx (Werker & Tees, 1984a), Spanish (Aslin,Pisoni, Hennessy, ~ Percy, 1981).and Zulu (Best,McRoberts, ~ Sithole, 1988).
This suggests that in-fants develop an initial representation f speech thatis universal and largely insensitive to the particularlanguage to which they are exposed.
The ability todiscriminate some non-native speech contrasts de-clines after the age of 10-12 months (Werker ~ Tees,1984a).Such rapid learning can be defined in terms ofa taxonomy developed in the field of animal be-haviour.
Mayr (1974) suggested that programs ofdevelopment form a continuum of flexibility in theirresponse to environmental stimulation.
He distin-guished between "open" and "closed" programs ofdevelopment.
"Closed" programs of developmentrely on environmental input to a relatively smalldegree, producing highly stereotyped behaviour.Precedents of rapidly learned "closed" developmentabound and are also termed "innately guided" learn-ing e.g.
imprinting in geese and ducks and song ac-quisition in birds (Marler, 1991).
"Open" programs,on the other hand, are responsive to a much broaderrange of stimulation and can produce a broaderrange of responses.
The presence of one type ofdevelopmental program does not preclude the exis-Nakisa ~ Plunkett 70 Evolution of Speech RepresentationsRamin Charles Nakisa and Kim Plunkett (1997) Evolution of a Rapidly Learned Representation for Speech.In T.M.
Ellison (ed.)
CoNLL977 Computational Natural Language Learning, ACL pp 70-79.
(~) 1997 Association for Computational Linguisticstence of the other, however.
Just because a ducklinghas imprinted on its mother does not mean that it isunable to learn to recognise new objects later in life.Similarly, the rapid learning of speech sounds by in-fants does not preclude later tuning of the speechrepresentation.
In fact, we would argue that it aidssuch development by ensuring that later language-specific fine-tuning of the representation does not en-counter local minima, which would be catastrophicfor linguistic development.To quote from a recent review (Jusczyk, 1992):Jusczyk and Bertoncini (1988) proposedthat the development of speech perceptionbe viewed as an innately guided learningprocess wherei,n the infant is primed in cer-tain ways to seek out some type of signalsas opposed to others.
The innate prewiringunderlying the infant's speech perceptionabilities allows for development to occurin one of several directions.
The natureof the input helps to select the directionthat development will take.
Thus, learningthe sound properties of the native languagetakes place rapidly because the system isinnately structured to be sensitive to corre-lations of certain distributional propertiesand not others.In order to make explicit what is meant by "in-nately guided learning" and "innate prewiring" wehave developed a connectionist model of innatelyguided learning.
The approach taken has been to en-code an artificial neural network (ANN) in a genomewhich stores its architecture and learning rules.
Thegenomic space of possible ANNs is searched for net-works that are well suited to the task of rapidlylearning to detect contrastive features of humanspeech sounds using unsupervised learning.
Impor-tantly, networks tart life with a completely random-ized set of connections and therefore have no repre-sentational knowledge about speech at the level ofindividual connections.
The network must thereforeuse its architecture and learning rules in combina-tion with auditory input to rapidly converge on arepresentation.The model attempts to explain how innate con-straints on a neural network could allow infants tobe sensitive to a wide range of features o soon afterbirth, and to develop the same initial features what-ever their target language.
It also exhibits otherfeatures typically associated with human speech per-ception, namely categorical perception and patternsof phoneme confusability similar to that of humans.The model does not account directly for the muchslower, roughly year-long process by which some fea-tural distinctions are lost.
It is possible that featuresare never lost and that units which represent infor-mation that is redundant in the target language areignored by higher level processing, as suggested byWerker and Tees (Werker 8z Tees, 1984b).2 Overv iew o f  the  Mode lThe goal of the model is to create a neural networkthat takes speech spectra as input and develops thesame representation f speech whatever the languageit is exposed to.
Furthermore we avoid hard-wiringthe connections in the network.
Rather, the net-work employs a set of unsupervised learning rulesthat converge on the same representation whateverthe initial set of connection strengths between neu-rons in the network.
It is important hat the learn-ing is unsupervised as the developing infant has noteaching signal as to the contrasts present in speech.In essence this model of early speech perception em-bodies Waddington's (1975) principle of epigenesis,or what Elman et al (1996) have more recently de-scribed as architectural/computational innateness.The approach we have taken is to encode the prop-erties of neural networks in a genome and to evolve,by a process called a genetic algorithm, a popula-tion 0f neural networks that respond in the appropri-ate way to speech spectra.
Initially, a population of50 genomes are randomly generated.
Each of thesenetworks is presented with speech spectra and wequantify how well its neuronal engram of speech en-codes the incoming signal.
This number is called the"fitness" of a network.
For the task of representingspeech sounds we want a network that is responsiveto the salient aspects of the speech signal, in par-ticular those necessary for identification of speechsegments.
A network that is good at representingspeech will encode tokens of the same acoustic seg-ment as similarly as possible and different segmentsas differently as possible.The initial population performs very poorly on thetask, but some networks perform better than others.Two parents are randomly selected from the popula-tion with a probability that increases with increas-ing fitness.
The parental genomes are spliced to-gether to form one child network that is then testedto find its fitness.
The child network then replacesthe network that has the lowest fitness in the pop-ulation.
Each gene also has a small chance of mu-tating to a new value after sexual reproduction, sothat new genes are constantly entering the collectivegene pool, otherwise the evolutionary process wouldsimply be a re-shuffling of genes present in the initialpopulation.
The process of parental selection, sexualNakisa 8J Plunkett 771 Evolution of Speech Representationsreproduction, mutation of the offspring and evalua-tion of the offspring is repeated for several thousandgenerations.
Genes that are useful for the task athand, as specified by the fitness function, increasein frequency in the population, while genes that arenot useful decline in frequency.
Within a few hun-dred generations the networks in the population de-velop representations that have a high fitness value,as illustrated in Figure 1.0.45OA0.05O~ 0.250.211.150.10.05 I I I I I I I I I "1~ ~0 ~?00 4~ ~0 ISO0 7~ ~0 9C0 10~C~'at ionFigure 1: Increase in the mean fitness of the popu-lation with increasing number of generations, wherea generation is defined as the production of one newnetwork.
Initially networks perform very poorly, butselection improves the population rapidly.Clearly, the encoding scheme used to store theproperties of neural networks critically affects howwell the networks may perform on any given task.The encoding scheme we have chosen is very flex-ible, storing information about the architecture ofa network and its learning properties.
Architecturedefines what neurons may be connected to other neu-rons, and this presupposes ome way of groupingneurons such that these gross patterns of connec-tivity can be defined.
For the purposes of definingnetwork architecture, therefore, the network is sub-divided into subnetworks.
The genome specifies howmany subnetworks there are, how many neurons arein each subnetwork what subnetworks are connectedto one another, and given that two subnetworks areconnected, what learning rule is used in connectionsbetween eurons in those subnetworks.3 Descr ip t ion  o f  the  Mode lThe model builds on previous connectionist models,particularly the broad class of models known as in-teractive activation with competition (IAC) modelsNakisa ~ Plunkett 72(see Grossberg (1978) for review).
An IAC networkconsists of a collection of processing units dividedinto several competitive pools.
Within pools thereare inhibitory connections and between pools thereare excitatory connections.
Connections are interac-tive because one pool interacts with other pools andin turn is affected by those pools.
Because of theseinteractions the activity of units in IAC networks de-velop over time, sometimes settling into steady pat-terns of activation.
Inhibitory connections within apool mean that one unit at a time dominates the oth-ers in a winner-take-all fashion.
The TRACE modelof speech perception is possibly the most successfuland best known example of such models (McClelland& Elman, 1986).Although similar to IAC networks, the models de-scribed here have three major modifications:Learn ing  Each network learns using n~any differ-ent, unsupervised learning rules.
These use onlylocal information, and so are biologically plau-sible.F lex ib le  Arch i tec ture  Every network is split intoa number of separate subnetworks.
This allowsexploration of different neuronal architectures,and it becomes possible to use different learn-ing rules to connect subnetworks.
Subnetworksdiffer in their "time-constants" i.e.
respond toinformation over different ime-scales.Genet ic  Se lect ion  Networks are evolved usinga technique called genetic connectionism(Chalmers, 1990).
Using a genetic algorithmallows great flexibility in the type of neural net-work that can be used.
All the attributes ofthe neural network can be simultaneously opti-mised rather than just the connections.
In thismodel the architecture, learning rules and time-constants are all optimised together.3.1 Genome Des ign and  Sexua lReproduct ionThe genome has been designed to have two chromo-somes tored as arrays of numbers.
One chromosomestores the attributes of each subnetwork, such as thenumber of units in the subnetwork, the subnetworktime constant and the indices of the other subnet-works to which the subnetwork projects.
The otherchromosome stores learning rules which are used tomodify connections between individual units.During sexual reproduction of two networks thetwo chromosomes from each parent are indepen-dently recombined.
In recombination, a point withina chromosome array is randomly chosen, and all theEvolution of Speech Representationsinformation up to that point is copied from the pa-ternal chromosome and the rest of the chromosomeis copied from the maternal chromosome creatinga hybrid chromosome with information from bothparents.
Clearly, the subnetwork and learning rulechromosomes must be the same length for sexualrecombination to occur, so not all pairs of parentscan reproduce.
Parents must be sexually compatiblei.e.
must have the same number of subnetworks andlearning rules.3.2 DynamicsThe dynamics of all units in the network are gov-erned by the first order equationda~ ~ s~n svn dt = 2-~wiJ a j -a~ (1)s,jWhere v,~ is the time constant for subnetwork n,a~ is the activity of the j th unit in subnetwork s, a~is the activity of the i th unit in subnetwork n, u~.~"is the synaptic strength between the j th  unit in sub-network s and the i th unit in subnetwork n. In otherwords, the rate of change in the activation of a unitis a weighted sum of the activity of the units whichare connected to the unit i, minus a decay term.
Ifthere is no input to the unit its activity dies awayexponentially with time constant r~.
The activity ofa unit will be steady when the activity of the unit isequal to its net input.
Activities were constrained tolie in the range 0.0, < a < 1.0.
Network activity forall the units was updated in a synchronous fashionwith a fixed time-step of 10 ms using a fourth or-der Runge-Kutta integration scheme adapted fromNumerical Recipes (Press, Flannery, Teukolsky, &Vetterling, 1988).3.3 Arch i tec tureArchitecture defines the gross pattern of connectiv-ity between groups of units.
The architecture has tobe stored in a "genome" to allow it to evolve with agenetic algorithm, and one very flexible method ofencoding the architecture is to create a subnetworkconnectivity matrix.
If there are n subnetworks inthe network, then the subnetwork connectivity ma-trix will be an n by n matrix.
The column num-ber indicates the subnetwork from which connectionsproject, and the row number indicates the subnet-works to which connections project.Complex architectures can be represented usinga subnetwork connectivity matrix.
The matrix al-lows diagonal elements to be non-zero, allowing asubnetwork to be fully connected to itself.
In ad-dition, the subnetwork connectivity matrix is usedto determine which learning rule will be used forthe connections between any pair of subnetworks.
Ifan element is zero there are no connections betweentwo subnetworks.
A positive integer element indi-cates that subnetworks are fully connected and thevalue of the integer specifies which one of the manylearning rules to use for that set of connections.
Asimple architecture is shown in Figure 2 alongsideits corresponding subnetwork connectivity matrix.3.4 Learn ing  RulesLearning rules are of the general form shown in equa-tion 2.
They are stored in the network genome ingroups of seven coefficients k0 to k6 following therepresentation used by Chalmers (1990).Awij = l(ko + klai + k2aj + k3aiaj +k4wij + k5aiwij + k6ajwij) (2)In Equation 2, wij is the change in synapticstrength between units j and i, l is the learning rate,ai is the activity of unit i, aj is the activity of unitj and wlj is the current synaptic strength betweenunits j and i.
The learning rate l is used to scaleweight changes to small values for each time stepto avoid undesirably rapid weight changes.
The co-efficients in this equation determine which learningrule is used.
For example, a Hebbian learning rulewould be represented in this scheme with k3 > 0 andk0 < 0and kl = k2 = k4 = k5 = k6 = 0.
Connec-tions between units using this learning rule wouldbe strengthened if both units were simultaneouslyactive.
A network has several earning rules in itsgenome stored as a set of these coefficients.
Weightvalues are clipped to avoid extremely large values de-veloping over long training periods.
The range usedwas -1.0 < wij < +1.0.3.5 Tra in ing  and  Eva luat ion  of  F i tnessNetworks were trained and evaluated using digi-tised speech files taken from the DARPA TIMITAcoustic-Phonetic Continuous Speech Corpus(TIMIT) as described in Garofolo et al (1990).All networks were constrained to have 64 inputunits because speech sounds were represented aspower spectra with 64 values.
This was an artificialconstraint imposed by the format of the spectra.The power spectra were calculated with the OGIspeech tools program MAKEDFT 1 (modified toproduce the correct output format) with a windowsize of 10 ms and with successive windows adjacentto one another.
For these simulations 8 outputsubnetworks were used to represent features because1Available from http://www.cse.ogi.edu.Nakisa ~ Plunkett 773 Evolution of Speech Representationsi j ,, s.b,,0 / : /{oooooo ........... oooooo)C =0 0 0 0 0 0 0 0 0 \0 0 0 0 1 0 0 3 0\]0 0 0 1 0 0 0 0 02 0 1 0 0 0 0 0 00 0 0 0 0 0 3 0 01 0 0 3 0 0 0 0 00 0 0 0 0 0 0 1 00 0 20  0 2 0 1 20 0 0 0 0 0 0 2 0?
$1 lbnet  I .
Sub  t2  .Figure 2: Example to lllu~ra~e the archltectur~ encoding scheme showing a network of 9 subnetworks and itscorresponding subnetwork connectivity matrix.
Subnetwork 1 and 2 are the input and output subnetworks,respectively.
Arrows represent sets of connections and the type of learning rule employed by those sets ofconnections.
There are three learning rules used; solid arrow (learning rule 1~, dashed arrow (learning rule 2)and dotted arrow (learning rule 3).
Some subnetworks are fully connected to themselves, uch as subnetwork8 (since C88 = 1), while others are information way-stations, uch as subnetwork 5 (C55 = 0).this is roughly the number claimed to be necessaryfor distinguishing all human speech sounds by somephoneticians (Jakobson ~ Waugh, 1979).All the connections, both within and between sub-networks, were initialised with random weights inthe range -1.0 to +1.0.
Networks were then exposedto a fixed number of different, randomly selectedtraining sentences (usually 30).
On each time-stepactivity was propagated through the network of sub-networks to produce a response activity on the out-put units.
All connections were then modified ac-cording to the learning rules specified in the genome.On the next time-step a new input pattern corre-sponding to the next time-slice of the speech signalwas presented and the process of activity propaga-tion and weight modification repeated.
The processof integrating activities and weight updates was re-peated until the network had worked its way throughall the time-slices of each sentence.In the testing phase activation was propagatedthrough the network without weight changes?
Theweights were frozen at the values they attained atthe end of the training phase.
Testing sentenceswere always different from training sentences?
Whena time-slice corresponded with the mid-point of aphoneme, as defined in the TIMIT phonologicaltranscription file, the output unit activities werestored alongside the correct identity of the phoneme.Network fitness was calculated using the stored out-put unit activities after the network had been ex-posed to all the testing sentences.
The fitness func-tion f wasNakisa ~ Plunkett 74N Nf = E i  E j= i+ l  dist(5~, 6~).
sN(N  - 1) (3)Where s = -4-1 if i and j are different phonemesand s = -1  if i and j are the identical phonemes,5~ and 5~ were the output unit activities at the mid-point of all N phonemes and dist was euclidean dis-tance.
This fitness function favoured networks thatrepresented occurrences ofthe same phoneme as sim-ilarly as possible and different phonemes as differ-ently as possible.
A perfect network would have allinstances of a given phoneme type mapping onto thesame point in the output unit space and differentphonemes as far apart as possible.
Note that con-stant output unit activities would result in a fitnessof 0.0.
An ideal learning rule would be able to findan appropriate set of weights whatever the initialstarting point in weight space.
Each network wastrained and tested three times from completely dif-ferent random initial weights on completely differentsentences.
This reduced random fitness variationscaused by the varying difficulty of training/testingsentences and the choice of initial weights.Evolution was carried out with a population of50 networks.
Genomes were initially generated withcertain limits on the variables.
All genomes had 16input subnetworks and 8 output subnetworks withtime constants randomly distributed in the range100 ms to 400 ms.
The input subnetworks had4 units each and the output subnetworks had 1unit each.
Each network started with 10 differentlearning rules with integer coefficients randomly dis-tributed in the range -2 to --t-2.
Subnetwork con-Evolution of Speech RepresentationsPInectivity matrices were generated with a probabil-ity of any element being non-zero of 0.3.
If an el-ement was non-zero, the learning rule used for theconnections between the subnetworks was randomlyselected from the 10 learning rules defined for thenetwork.
The networks were also constrained to befeed-forward, as shown in Figure 3.Featural Output0 Hz ~ 8 kHzSpectral InputFigure 3: Architectural constraints on the evolution-ary process.
The networks were all feed-forward,with no "hidden" units and a fixed number of in-put units (64) and output units (S).
Input unitswere grouped into subnets of 4 units each and eachinput unit carried information from one of the 64frequency values in the speech spectra ranging from0 to 8 kHz.4 Resul tsAll results shown are from the best network evolved(fitness=0.45) after it had been trained on 30 En-glish sentences corresponding to about 2 minutes ofcontinuous peech.
Figure 4 shows the response ofthis network to one of the T IMIT  testing sentences.From the response of the feature units to speechsounds (see Figure 4) it was clear that some unitswere switched off by fricatives, and some units wereswitched on by voicing, so both excitation and inhi-bition play an important part in the functioning ofthe feature detectors.
The feature unit responses didnot seem to correlate directly with any other stan-dard acoustic features (e.g.
nasal, compact, grave,flat etc.).
An analysis of the frequency response ofthe eight feature detectors (see Figure 5) showedthat each unit had excitatory projections from sev-eral frequency bands.
Generally, the frequency re-sponses were mutually exclusive so that each unitresponded to slightly different sounds, as one wouldexpect.F~t~umt s \[\] liFeature Unit 7 ili~i~ii::::::~!~Feature Unit 6 ~ii::~i~Feature Unit 4 ~ ~ii:.ii!Feature unit 3 | I ~  EF~t~Uni t2  ..... : - ------~ ..... ii::i::i::Eliii::ii ~ i i |Feature Unit 1 ::ii:::::i ~ ~iii::!
::i0 2 4Frequency / kHz: : !
: ~ : :  : : !
~ :Em ~!ii::;~i ~ i:1~: .
: .
: , : .
.
: .
~ .
.
.
.
, .
.~5: : : : : : : : : :)6 8Figure 5: Complex frequency response of all eightfeature units to pure tones.
Feature units 2 and 3receive strong excitatory inputs from low frequencies(below 4 kHz) and are therefore activated by voicing.4.1 Cross -L ingu ls t l c  Per fo rmanceIn order to determine the cross-linguistic perfor-mance of the "innate" features evolved on Englishspeech, sound files of the news in several anguageswere obtained from the Voice of America FTP site( f tp .voa .gov) .
Since phonological transcriptionfiles were not available for these files they could notbe used to test the network, because the times of thephoneme mid-points were unknown.
All the VOAbroadcast languages 2 were used as training files, andthe network was tested on 30 American English sen-tences found in the T IM IT  speech files.
The time-courses of development for four languages are shownin Figure 6.
Maximum fitness was reached aftertraining on any language for roughly 20 sentences(each lasting about 3 seconds).All of the human languages tested seemed to beequally effective for training the network to representEnglish speech sounds.
To see whether any soundscould be used for training, the network was trainedon white noise.
This resulted in slower learning anda lower fitness.
The fitness for a network trained onwhite noise never reached that of the same networktrained on human speech.
An even worse impedi-ment to learning was to train on low-pass filteredhuman speech.4.2 Categor ica l  Percept ionCategorical perception of some phonemes is a ro-bust phenomenon observed in both infants and2English, Cantonese, Swahili, Farsi, Czech, Hindi,Hungarian, Korean, Polish, Russian, Slovak, Spanish,Ukrdnian and Urdu.Nakisa 8d Plunkett 75 Evolution of Speech RepresentationsFeatureDetectorActivityi nputUnitActivityTIMITPhonesE8t- 0ETIMIT EWords.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Figure 4: Network response to the sentence "Agricultural products are unevenly distributed" (TIMIT speechfile test/dr3/fkms0/sxl40).
Input units are fed with sound spectra and activate the feature units.
Activityis shown as a greyscale (maximum activity is portrayed as black) with time on the horizontal axis.
Phoneand word start and end times as listed in TIMIT are shown in the bottom two panels.
This is the samenetwork as shown in Figure 5.0 5 10 15 20 25 30Nuntber d Traildng SentencesFigure 6: Network performance increases to its fi-nal value after presentation of just 20 sentences re-gardless of the language used to train the network.The six curves show the learning curves for a net-work tested on 30 sentences of English having beentrained on English, Cantonese, Swahili, Farsi, whitenoise and low-pass filtered English.adults.
We tested the network on a speech contin-uum ranging between two phonemes and calculatedthe change in the representation f the speech tokensalong this continuum.
Note that this model simplycreates a representation f speech on which identifi-cation judgements are based.
It does not identifyphonemes itself.
All that the model can provideis distances between its internal representations ofdifferent sounds.
Categorical perception can be ex-hibited by this network if the internal representationexhibits non-linear shifts with gradual changes in theinput i.e.
a small change in the input spectrum cancause a large change in the activity of the outputunits.Using a pair of real /~/  and /s /  spectra from amale speaker, a series of eleven spectra were createdwhich formed a linear continuum from a pure / .
f / toa pure /s / .
This was done by linearly interpolatingbetween the two spectra, so the second spectrumin the continuum was a linear sum of 0.9 times the/.f/ spectrum plus 0.1 times the /s /spect rum.
Thenext spectrum was a linear sum of 0.8 times the / .
f /spectrum plus 0.2 times the /s /spect rum,  and so onfor all nine intermediate spectra up to the pure /s / .Each of the eleven spectra in the continuum wereindividually fed into the input of a network that hadbeen trained on 30 sentences of continuous peech inNakisa ~ Plunkett 76 Evolution of Speech RepresentationsEnglish.
The output feature responses were storedfor each spectrum in the continuum.
The distancesof these feature vectors from the pure/ J~/and pure/ s /a re  shown in Figure 7.1.00.80.60.40.20.0=.
.
.
Il~h/Input SpectrumFigure 7: Response of the network to input on a / f /- / s /  continuum.
Circles show the distance from apure/ J~/and triangles how the distance from a pure/s/.Clearly, the distance of the pure/ j~/ f rom itself iszero, but moving along the continuum, the distancefrom the pure/.~/increases steadily until it reachesa maximum for the pure /s /  (distances were scaledsuch that the maximum distance was 1).
Figure 7shows that the representation is non-linear.
Thatis, linear variations in the input spectrum do notresult in linear changes in the activity of the featureunits.
Compared to the spectral representation ofthe /J~/- / s /  continuum, the network re-representsthe distances in the following ways:?
There is a discontinuity in the distances whichoccurs closer to the/ J~/ than the /s / .?
The distance from the representation f a pure/s / remains  mall for spectra that are a third ofthe way toward the pure/.~/.A classifier system using this representation wouldtherefore shift the boundary between the twophonemes toward If~ and be relatively insensitiveto spectral variations that occurred away from thisboundary.
These are the hallmarks of categoricalperception.4.3 Similarity Structure of theRepresentationA consequence of any representation is its effect onsimilarity judgements.
Miller and Nicely (1955) usedthis fact in an elegant experiment designed to inferthe manner in which humans identify sixteen En-glish consonants.
They asked subjects to identifyCV pairs where the consonant was one of the six-teen being tested and the vowel was /a I / ,  as in fa-ther.
By adding noise to the stimuli at a constantloudness and varying the loudness of the speechthey could control the signal to noise ratio of thestimuli and measure the number and type of errorsproduced.
Subjects produced a consistent patternof errors in which certain pairs of consonants weremore confusable than others.
For example, the fol-lowing pairs were highly confusable: m-n, f-0, v-~,p-t-k,  d-g, s-f ,  z- 5.
When clustered according toconfusability the consonants formed three groups:voiceless, voiced and nasal consonants.
Confusabil-ity was greatest within each group and smallest be-tween groups.Since our model did not classify phonemes it wasnot possible to create a phoneme confusability ma-trix using the same method as Miller and Nicely.However, it was possible to create a clustering dia-gram showing the similarity structure of the repre-sentations for each phoneme.
If given noisy input,phonemes whose representations are closest ogetherin the output space will be more easily confused thanphonemes that lie far apart.
Since a cluster analy-sis of many thousands of phoneme tokens would notbe clear, a centroid for each phoneme type was usedas the input to the cluster analysis.
Centroids werecalculated by storing the input and output represen-tations of phonemes in 1000 TIMIT sentences.
Clus-ter analyses for the spectral input representation a dthe featural output representation are shown in Fig-ure 8.
3From Figure 8 it is clear that the featural out-put representation broadly preserves the similaritystructure of the spectral input representation despitethe eight-fold compression in the number of units.In both the input and output representations thephonemes can be divided into three classes: frica-tives/affricates, vowels/semi-vowels, and other con-sonants.
Some phonemes are shifted between thesebroad categories in the output representation, e.g.t, 0 and f are moved into the fricative/affricate cat-egory.
The reason for this shift is that t occurs with3It should be noted that for stops, TIMIT transcribesclosures eparately from releases, so /p/ would be tran-scribed /pcl p/.
The results shown here are for the re-leases, hence their similarity to fricatives and affricates.Nakisa ~ Plunkett 77 Evolution of Speech Representationsa high token frequency, so by pulling it apart fromother frequently occurring, spectrally similar conso-nants, the fitness is increased.Both spectral and featural representations showeda high confusability for m-n, f-0, d-g, s-J ~, as foundin the Miller and Nicely experiments.
There werediscrepancies, however: the stops p-t-k were notparticularly similar in either the input or output rep-resentations due to an artifact of the representationsbeing snapshots at the mid-points of the stop release.In human categorisation experiments, phonemes arejudged on the basis of both the closure and the re-lease, which would greatly increase the similarity ofthe stops relative to other phonemes.
In the inputrepresentation, v-6 are fairly close together, butare pulled apart in the output representation.
Boththese phonemes had low token frequencies, o thisdifference may not be a result of random variation.In Figure 8 3 is not shown because it occurred veryinfrequently, but the centroids of z- 3 were very closetogether, as found by Miller and Nicely.Input RepresentationshIOutput RepresentationFigure 8: Similarity structure of the spectral andfeatural representations.
Labels are TIMIT ASCIIphonemic odes: dx-r, q-?, jh-d3, ch-~, zh-3, th-0,dh-0, em-rn, en-~, eng-~, nx-r, hh-h, hv-~, el-!, iy-il, ih-I, eh-e, ey-ej, aa-ct, ay-aj, ah-A, ao-o,oy-3j, uh-u, uw-m, ux-u, er- U ax-o, ix-i, axr-~,ax-h-o.5 DiscussionBy developing an appropriate architecture, time-constants and learning rules over many generations,the task of learning to represent speech sounds ismade more rapid over the course of development ofan individual network.
Evolution does all the hardwork and gives the network a developmental "leg-up".
However, having the correct innate architec-ture and learning rules is not sufficient for creatinggood representations.
Weights are not inherited be-tween generations so the network is dependent onthe environment for learning the correct representa-tion.
If deprived of sound input or fed acousticallyfiltered speech input, the model cannot form mean-ingful representations because ach network startslife with a random set of weights.
But given thesort of auditory input heard by an infant the modelrapidly creates the same set of universal features,whether or not it is in a noisy environment and what-ever the language it hears.We envisage that this method of creating a quickand dirty initial representation f sounds by innatelyguided learning is not specific to humans.
Clearly,humans and other animals have not been selectedfor their ability to discriminate he phonemes of En-glish.
But we would expect results similar to thosepresented here if the selection criterion were the abil-ity to discriminate a wide range of spectrally dis-similar sounds in the environment from only limitedexposure to their patterns of regularity e.g.
discrim-ination of the maternal call from other conspecificcalls, and the sound of predators from everyday en-vironmental noises.
It is therefore unsurprising thatanimals have been found, after suitable training, todiscriminate some phonemes in similar ways as dohumans (Kuhl & Miller, 1975).The advantages of innately guided learning overother self-organising networks are that it is muchfaster and is less dependent on the "correct" environ-mental statistics.
It also offers an account of how in-fants from different linguistic environments can comeup with the same featural representation so soon af-ter birth.
In this sense innately guided learning asimplemented in this model shows how genes and theenvironment could interact o ensure rapid develop-ment of a featural representation f speech on whichfurther linguistic development depends.6 AcknowledgementsRamin Nakisa was supported by a Training Fellow-ship from the Medical Research Council.
Furthersupport was provided by Research Project grantsfrom the EPSRC and ESRC to Kim Plunkett.Re ferencesAslin, R., Pisoni, D., Hennessy, B., ~ Perey, A.(1981).
Discrimination of voice-onset time byhuman infants: New findings and implicationsfor the effect of early experience.
Child Devel-opment, 52, 1135-1145.Nakisa 81 Plunkett 78 Evolution of Speech RepresentationsBest, C., MeRoberts, G., & Sithole, N. (1988).Examination of perceptual reorganization fornormative speech contrasts - Zulu click dis-crimination by English-speaking adults andinfants.
Journal of Experimental Psychol-ogy: Human journal = Perception and Per-formance, 1~(3), 345-360.Chalmers, D. (1990).
The evolution of learn-ing: An experiment in genetic onnectionism.In D. Touretzky, J. Elman, T. Sejnowski, &G. Hinton (Eds.
), Connectionist models: Pro-ceedings of the 1990 summer school (pp.
81-90).
Morgan Kaufmann Publishers, Inc.Eimas, P., Siqueland, E., Jusczyk, P., & Vigorito, J.(1971).
Speech perception in infants.
Science,171, 303-306.Elman, J., Bates, E., Johnson, M., Karmiloff-Smith,A., Parisi, D.; ~ Plunkett, K. (1996).
Rethink-ing innateness: A connectionist perspective ondevelopment.
Cambridge, Massachusetts: TheMIT Press.Garofolo, J., Lamel, L., Fisher, W., Fiscus, J., Pal-lett, D., ~ Dahlgren, N. (1990).
DARPATIMIT acoustic-phonetic continuous speechcorpus CD-ROM (Tech.
Rep. No.
NISTIR4930).
National Institute of Standards andTechnology, USA.Grossberg, S. (1978).
In L. Leeuwenburg ~ H. Buf-fart (Eds.
), Formal theories of visual percep-tion.
New York: Wiley.Jakobson, R., & Waugh, L. (1979).
The sound shapeof language.
Bloomington: Indiana UniversityPress.Jusczyk, P. (1992).
In C. Ferguson, L. Menn,C.
Stoel-Gammon (Eds.
), Phonological devel-opment: Models, research, implications (pp.17-64).
Timonium, Maryland 21094, USA:York Press, Inc.Jusczyk, P., & Bertoncini, J.
(1988).
Viewing the de-velopment of speech perception as an innatelyguided learning process.
Language and Speech,31, 217-238.Kuhl, P. (1993).
Developmental speech-perception- implications for models of language impair-ment.
Annals of the New York Academy ofSciences, 682 (2), 248-263.Kuhl, P., & Miller, J.
(1975).
Speech perceptionby the chinchilla: Voiced-voiceless distinctionin alveolar plosive consonants.
Science, 190,69-72.Marler, P. (1991).
Song-learning behaviour: Theinterface with neuroethology.
Trends in Neu-rosciences, 13 (5), 199-206.Mayr, E. (1974).
Behaviour programs and evo-lutionary strategies.
American Scientist, 62,650-659.McClelland, J. L., & Elman, J. L. (1986).
TheTRACE model of speech perception.
CognitivePsychology, 18, 1-86.Miller, G., & Nicely, P. (1955).
An analysis of per-ceptual confusions among some English con-sonants.
Journal of the Acoustical Society ofAmerica, 27, 338-352.Press, W., Flannery, B., Teukolsky, S., & Vetterling,W.
(1988).
Numerical recipes in C: The artof scientific computing.
Cambridge, England:Cambridge University Press.Trehub, S. (1973).
Infants' sensitivity to vowel andtonal contrasts.
Developmental Psychology, 9,91-96.Waddington, C. (1975).
The evolution of an evolu-tionist.
Ithaca, NY: Cornell University Press.Werker, J., Gilbert, J., Humphrey, K., ~ Tees,R.
(1981).
Developmental spects of cross-language speech perception.
Child Develop-ment, 52, 349-353.Werker, J., ~ Tees, R. (1984a).
Cross-languagespeech perception: Evidence for perceptual re-organisation during the first year of life.
InfantBehaviour and Development, 7 49-63.Werker, J., 8z Tees, R. (1984b).
Phonemic andphonetic factors in adult cross-language speechperception.
Journal of the Acoustical Societyof America, 75(6), 1866-1878.Nakisa 8J Plunkett 79 Evolution of Speech Representations
