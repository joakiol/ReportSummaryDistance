Proceedings of the 10th Conference on Parsing Technologies, pages 48?59,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsEfficiency in Unification-Based N -Best ParsingYi Zhang?, Stephan Oepen?, and John Carroll?
?Saarland University, Department of Computational Linguistics, and DFKI GmbH (Germany)?University of Oslo, Department of Informatics (Norway)?University of Sussex, Department of Informatics (UK)AbstractWe extend a recently proposed algorithm forn-best unpacking of parse forests to deal ef-ficiently with (a) Maximum Entropy (ME)parse selection models containing importantclasses of non-local features, and (b) forestsproduced by unification grammars contain-ing significant proportions of globally incon-sistent analyses.
The new algorithm empir-ically exhibits a linear relationship betweenprocessing time and the number of analysesunpacked at all degrees of ME feature non-locality; in addition, compared with agenda-driven best-first parsing and exhaustive pars-ing with post-hoc parse selection it leads toimproved parsing speed, coverage, and ac-curacy.
?1 Background?MotivationTechnology for natural language analysis using lin-guistically precise grammars has matured to a levelof coverage and efficiency that enables parsing oflarge amounts of running text.
Research groupsworking within grammatical frameworks like CCG(Clark & Curran, 2004), LFG (Riezler et al, 2002),and HPSG (Malouf & van Noord, 2004; Oepen,Flickinger, Toutanova, & Manning, 2004; Miyao,Ninomiya, & Tsujii, 2005) have successfully in-tegrated broad-coverage computational grammarswith sophisticated statistical parse selection models.The former delineate the space of possible analy-ses, while the latter provide a probability distribu-?The first author warmly acknowledges the guidance of hisPhD advisors, Valia Kordoni and Hans Uszkoreit.
We are grate-ful to Ulrich Callmeier, Berthold Crysmann, Dan Flickinger,and Erik Velldal for many discussions and their support.
Wethank Ron Kaplan, Martin Kay, and Bob Moore for provid-ing insightful information about related approaches, notably theXLE and CLE parsers.tion over competing hypotheses.
Parse selection ap-proaches for these frameworks often use discrimi-native Maximum Entropy (ME) models, where theprobability of each parse tree, given an input string,is estimated on the basis of select properties (calledfeatures) of the tree (Abney, 1997; Johnson, Ge-man, Canon, Chi, & Riezler, 1999).
Such features,in principle, are not restricted in their domain oflocality, and enable the parse selection process totake into account properties that extend beyond lo-cal contexts (i.e.
sub-trees of depth one).There is a trade-off in this set-up between the ac-curacy of the parse selection model, on the one hand,and the efficiency of the search for the best solu-tion(s), on the other hand.
Extending the context sizeof ME features, within the bounds of available train-ing data, enables increased parse selection accuracy.However, the interplay of the core parsing algo-rithm and the probabilistic ranking of alternate (sub-)hypotheses becomes considerably more complexand costly when the feature size exceeds the domainof locality (of depth-one trees) that is characteristicof phrase structure grammar-based formalisms.
Onecurrent line of research focuses on finding the bestbalance between parsing efficiency and parse selec-tion techniques of increasing complexity, aiming toidentify the most probable solution(s) with minimaleffort.This paper explores a range of techniques, com-bining a broad-coverage, high-efficiency HPSGparser with a series of parse selection models withvarying context size of features.
We sketch threegeneral scenarios for the integration: (a) a baselinesequential configuration, where all results are enu-merated first, and subsequently ranked; (b) an in-terleaved but approximative solution, performing agreedy search for an n-best list of results; and (c) atwo-phase approach, where a complete packed for-48est is created and combined with a specialized graphsearch procedure to selectively enumerate results in(globally) correct rank order.
Although conceptu-ally simple, the second technique has not previouslybeen evaluated for HPSG parsing (to the best of ourknowledge).
The last of these techniques, which wecall selective unpacking, was first proposed by Car-roll & Oepen (2005) in the context of chart-basedgeneration.
However, they only provide an accountof the algorithm for local ME properties and assertthat the technique should generalize to larger con-texts straightforwardly.
This paper describes thisgeneralization of selective unpacking, in its appli-cation to parsing, and demonstrates that the movefrom features that resemble a context-free domainof locality to features of, in principle, arbitrary con-text size can indeed be based on the same algorithm,but the required extensions are non-trivial.The structure of the paper is as follows.
Sec-tion 2 summarizes our formalism, grammars used,parse selection approach, and training and test data.Section 3 discusses the range of possibilities forstructuring the process of statistical, grammar-basedparsing, and Sections 4 to 6 describe our approachto efficient n-best parsing.
We present experimentalresults in Section 7, compare our approach to previ-ous ones (Section 8), and finally conclude.2 Overall Set-upWhile couched in the HPSG framework, the tech-niques explored here are applicable to the largerclass of unification-based grammar formalisms.
Wemake use of the DELPH-IN1 reference formalism,as implemented by a variety of systems, includingthe LKB (Copestake, 2002) and PET (Callmeier,2002).
For the experiments discussed here, weadapted the open-source PET parsing engine inconjunction with two publicly available grammars,the English Resource Grammar (ERG; Flickinger,2000) and the DFKI German Grammar (GG; Mu?ller& Kasper, 2000, Crysmann, 2005).
Our parse se-lection models were trained and evaluated on HPSGtreebanks that are distributed with these grammars.The following paragraphs summarize relevant prop-erties of the structures manipulated by the parser,1Deep Linguistic Processing with HPSG, an open-source repository of grammars and processing tools; see?http://www.delph-in.net/?.subjhhspecdet the lethesing nounn intr ledogthird sg fin verbv unerg lebarksFigure 1: Sample HPSG derivation tree for the sentence thedog barks.
Phrasal nodes are labeled with identifiers of gram-mar rules, and (pre-terminal) lexical nodes with class names fortypes of lexical entries.followed by relevant background on parse selection.Figure 1 shows an example ERG derivation tree.Internal tree nodes are labeled with identifiers ofgrammar rules, and leaves with lexical entries.
Thederivation tree provides complete information aboutthe actual HPSG analysis, in the sense that it can beviewed as a recipe for computing it.
Lexical entriesand grammar rules alike are ultimately just featurestructures, complex and highly-structured linguisticcategories.
When unified together in the configura-tion depicted by the derivation tree, the resulting fea-ture structure yields an HPSG sign, a detailed repre-sentation of the syntactic and semantic properties ofthe input string.
Just as the full derivation denotes afeature structure, so do its sub-trees, and for gram-mars like the ERG and GG each such structure willcontain hundreds of feature ?
value pairs.Because of the lexicalized nature of HPSG (andsimilar frameworks) our parsers search for well-formed derivations in a pure bottom-up fashion.Other than that, there are no hard-wired assumptionsabout the order of computation, i.e.
the specific pars-ing strategy.
Our basic set-up closely mimics that ofOepen & Carroll (2002), where edges indexed bysub-string positions in a chart represent the nodes ofthe tree, recording both a feature structure (as its cat-egory label) and the identity of the underlying lexi-cal entry or rule in the grammar.
Multiple edges de-rived for identical sub-strings can be ?packed?
into asingle chart entry in case their feature structures arecompatible, i.e.
stand in an equivalence or subsump-tion relation.
By virtue of having each edge keepback-pointers to its daughter edges?the immediatesub-nodes in the tree whose combination resulted in49the mother edge?the parse forest provides a com-plete and explicit encoding of all possible results in amaximally compact form.2 A simple unpacking pro-cedure is obtained from the cross-multiplication ofall local combinatorics, which is directly amenableto dynamic programming.Figure 2 shows a hypothetical forest (on the left),where sets of edges exhibiting local ambiguity havebeen packed into a single ?representative?
edge, viz.the one in each set with one or more incoming dom-inance arcs.
Confirming the findings of Oepen &Carroll (2002), in our experiments packing underfeature structure subsumption is much more effec-tive than packing under mere equivalence, i.e.
foreach pair of edges (over identical sub-strings) thatstand in a subsumption relation, a technique thatOepen & Carroll (2002) termed retro-active pack-ing ensures that the more general of the two edgesremains in the chart.
When packing under subsump-tion, however, some of the cross-product of localambiguities in the forest may not be globally con-sistent.
Assume for example that, in Figure 2, edges6 and 8 subsume 7 and 9 , respectively; combining7 and 9 into the same tree during unpacking can inprinciple fail.
Thus, unpacking effectively needs todeterministically replay unifications, but this extraexpense in our experience is negligible when com-pared to the decreased cost of constructing the for-est under subsumption.
In Section 3 we argue thatthis very property, in addition to increasing parsingefficiency, interacts beneficially with parse selectionand on-demand enumeration of results in rank order.Following (Johnson et al, 1999), a conditionalME model of the probabilities of trees {t1 .
.
.
tn}for a string s, and assuming a set of featurefunctions {f1 .
.
.
fm} with corresponding weights{?1 .
.
.
?m}, is defined as:p(ti|s) =exp?j ?jfj(ti)?nk=1 exp?j ?jfj(tk)(1)2This property of parse forests is not a prerequisite of thechart parsing framework.
The basic CKY procedure (Kasami,1965), for example, as well as many unification-based adapta-tions (e.g.
the Core Language Engine; Moore & Alshawi, 1992)merely record the local category of each edge, which is suffi-cient for the recognition task and simplifies the search.
How-ever, reading out complete trees from the chart, then, amountsto a limited form of search, going back to the rules of the gram-mar itself to (re-)discover decomposition relations among chartentries.Type Sample Features1 ?0 subjh hspec third sg fin verb?1 ?1 ?
subjh hspec third sg fin verb?1 ?0 hspec det the le sing noun?1 ?1 subjh hspec det the le sing noun?1 ?2 ?
subjh hspec det the le sing noun?2 ?0 subjh third sg fin verb?2 ?0 subjh hspce?2 ?1 subjh hspec det the le?2 ?1 subjh hspec sing noun?3 ?1 n intr le dog?3 ?2 det the le n intr le dog?3 ?3  det the le n intr le dog?Table 1: Examples of structural features extracted from thederivation tree in Figure 1.
The Type column indicates thetemplate corresponding to each sample feature; the integer thatstarts each feature indicates the degree of grandparenting (in thecase of type 1 and 2 features) or n-gram size (type 3 features).The symbols ?
and  denote the root of the tree and left pe-riphery of the yield, respectively.Feature functions fj can test for arbitrary structuralproperties of analyses ti, and their value typically isthe number of times a specific property is presentin ti.
Toutanova, Manning, Flickinger, & Oepen(2005) propose an inventory of features that per-form well in HPSG parse selection; currently we re-strict ourselves to the best-performing of these, ofthe form illustrated in Table 1, comprising depth-one sub-trees (or portions of these) with grammar-internal identifiers as node labels, plus optionallya chain of one or more dominating nodes (i.e.
lev-els of grandparents).
If a grandparents chain ispresent then the feature is non-local.
For expositorypurposes, Table 1 includes another feature type, n-grams over leaf nodes of the derivation; in Section 5below we speculate about the incorporation of these(and similar) features in our algorithm.3 Interleaving Parsing and RankingAt an abstract level, given a grammar and an associ-ated ME parse selection model, there are three basicways of combining them in order to find the single?best?
or small set of n-best results.The first way is a na?
?ve sequential set-up, in whichthe parser first enumerates the full set of analyses,computes a score for each using the model, and re-turns the highest-ranking n results.
For carefully501 ?
?2 3?|?4 3?2 ?
?5 6?|?5 7?4 ?
?8 6?|?8 7?|?9 6?|?9 7?6 ?
?10?|?11?Figure 2: Sample forest and sub-node decompositions: ovals in the forest (on the left) indicate packing of edges under subsump-tion, i.e.
edges 4 , 7 , 9 , and 11 are not in the chart proper.
During unpacking, there will be multiple ways of instantiating achart edge, each obtained from cross-multiplying alternate daughter sequences locally.
The elements of this cross-product we calldecomposition, and they are pivotal points both for stochastic scoring and dynamic programming in selective unpacking.
The tableon the right shows all non-leaf decompositions for our example packed forest: given two ways of decomposing 6 , there will bethree candidate ways of instantiating 2 and six for 4 , respectively, for a total of nine full trees.crafted grammars and inputs of average complexitythe approach can perform reasonably well.Another mode of operation is to organize theparser?s search according to an agenda (i.e.
priorityqueue) that assigns numeric scores to parsing moves(Erbach, 1991).
Each such move is an application ofthe fundamental rule of chart parsing, combining anactive and a passive edge, and the scores representthe expected ?figure of merit?
(Caraballo & Char-niak, 1998) of the resulting structure.
Assuming aparse selection model of the type sketched in Sec-tion 2, we can determine the agenda priority for aparsing move according to the (unnormalized) MEscore of the derivation (sub-)tree that would resultfrom its successful execution.
Note that, unlike inprobabilistic context-free grammars (PCFGs), MEscores of partial trees do not necessarily decrease asthe tree size increases; instead, the distribution offeature weights is in the range (??,+?
), centeredaround 0, where negative weights intuitively corre-spond to dis-preferred properties.This lack of monotonicity in the scores associatedwith sub-trees, on the one hand, is beneficial, in thatperforming a greedy best-first search becomes prac-tical: in contrast, with PCFGs and their monoton-ically decreasing probabilities on larger sub-trees,once the parser finds the first full tree the chart nec-essarily has been instantiated almost completely.
Onthe other hand, the same property prohibits the appli-cation of exact best-first techniques like A?
search,because there is no reliable future cost estimate; inthis respect, our set-up differs fundamentally fromthat of Klein & Manning (2003) and related PCFGparsing work.
Using the unnormalized sum of MEweights on a partial solution as its agenda score, ef-fectively, means that sub-trees with low scores ?sink?to the bottom of the agenda; highly-ranked partialconstituents, in turn, instigate the immediate cre-ation of larger structures, and ideally the bottom-upagenda-driven search will greedily steer the parsertowards full analyses with high scores.
Given itsheuristic nature, this procedure cannot guaranteethat its n-best list of results corresponds to the glob-ally correct rank order, but it may in practice comereasonably close to it.
While conceptually simple,greedy best-first search does not combine easily withambiguity packing in the chart: (a) at least whenpacking under subsumption, it is not obvious howto accurately compute the agenda score of packednodes, and (b) to the extent that the greedy searchavoids exploration of dis-preferred local ambigu-ity, the need for packing should be greatly reduced.Unfortunately, in scoring bottom-up parsing moves,ME features involving grandparenting are not ap-plicable, leading to a second potential source of re-duced parse selection accuracy.
In Section 7 below,we provide an empirical evaluation of both the na?
?vesequential and greedy best-first approaches.4 Selective UnpackingCarroll & Oepen (2005) observe that, at least forgrammars like the ERG, the construction of theparse forest can be very efficient (with observedpolynomial complexity), especially when packingedges under subsumption.
Their selective unpackingprocedure, originally proposed for the forest createdby a chart generator, aims to unpack the n-best set511 procedure selectively-unpack-edge(edge , n) ?2 results?
??
; i?
0;3 do4 hypothesis?
hypothesize-edge(edge , i); i?
i + 1;5 if (new?
instantiate-hypothesis(hypothesis)) then6 n?
n ?
1; results?
results ?
?new?
;7 while (hypothesis and n ?
1)8 return results;9 procedure hypothesize-edge(edge , i) ?10 if (edge.hypotheses[i]) return edge.hypotheses[i];11 if (i = 0) then12 for each (decomposition in decompose-edge(edge)) do13 daughters?
?
?
; indices?
?
?14 for each (edge in decomposition.rhs) do15 daughters?
daughters ?
?hypothesize-edge(edge, 0)?
;16 indices?
indices ?
?0?
;17 new-hypothesis(edge, decomposition, daughters, indices);18 if (hypothesis?
edge.agenda.pop()) then19 for each (indices in advance-indices(hypothesis.indices)) do20 if (indices ?
hypothesis.decomposition.indices) then continue21 daughters?
?
?
;22 for each (edge in hypothesis.decomposition.rhs) each (i in indices) do23 daughter?
hypothesize-edge(edge, i);24 if (not daughter) then daughters?
??
; break25 daughters?
daughters ?
?daughter?
;26 if (daughters) then new-hypothesis(edge, hypothesis.decomposition, daughters, indices)27 edge.hypotheses[i]?
hypothesis;28 return hypothesis;29 procedure new-hypothesis(edge , decomposition , daughters , indices) ?30 hypothesis?
new hypothesis(decomposition, daughters, indices);31 edge.agenda.insert(score-hypothesis(hypothesis), hypothesis);32 decomposition.indices?
decomposition.indices?
{indices};Figure 3: Selective unpacking procedure, enumerating the n best realizations for a top-level result edge from a packed forest.
Anauxiliary function decompose-edge() performs local cross-multiplication as shown in the examples in Figure 2.
Another utilityfunction not shown in pseudo-code is advance-indices(), a ?driver?
routine searching for alternate instantiations of daughter edges,e.g.
advance-indices(?0 2 1?)?
{?1 2 1?
?0 3 1?
?0 2 2?}.
Finally, instantiate-hypothesis() is the function that actually buildsresult trees, replaying the unifications of constructions from the grammar (as identified by chart edges) with the feature structuresof daughter constituents.of full trees from the forest, guaranteeing the glob-ally correct rank order according to the probabilitydistribution, with a minimal amount of search.
Thebasic algorithm is a specialized graph search throughthe forest, with local contexts of optimization corre-sponding to packed nodes.Each such node represents local combinatorics,and two key notions in the selective unpacking pro-cedure are the concepts of (a) decomposing an edgelocally into candidate ways of instantiating it, andof (b) nested contexts of local search for rankedhypotheses (i.e.
uninstantiated edges) about candi-date subtrees.
See Figure 2 for examples of the de-composition of edges.
Given one decomposition?i.e.
a vector of candidate daughters for a particu-lar rule?there can be multiple ways of instanti-ating each daughter: a parallel index vector ~I =?i0 .
.
.
in?
serves to keep track of ?vertical?
searchamong daughter hypotheses, where each index ijdenotes the i-th best instantiation (hypothesis) ofthe daughter at position j.
If we restrict ME fea-tures to a depth of one (i.e.
without grandparent-ing), then given the additive nature of ME scoreson complete derivations, it can be guaranteed thathypothesized trees including an edge e as an im-mediate daughter must use the best instantiation ofe in their own best instantiation.
Assuming a bi-nary rule, the corresponding hypothesis would usedaughter indices of ?0 0?.
The second-best instan-tiation, in turn, can be obtained from moving to thesecond-best hypothesis for one of the elements in the(right-hand side of the) decomposition, e.g.
indices52?0 1?
or ?1 0?
in the binary example.
Hypotheses areassociated with ME scores and ordered within eachnested context by means of a local priority queue(stored in the original representative edge, for con-venience).
Therefore, nested local optimizations re-sult in a top-down, breadth-first, exact n-best searchthrough the packed forest, while avoiding exhaustivecross-multiplication of packed nodes.Figure 3 shows the unchanged pseudo-code ofCarroll & Oepen (2005).
The main functionhypothesize-edge() controls both the ?horizontal?
and?vertical?
search, initializing the set of decompo-sitions and pushing initial hypotheses onto the lo-cal agenda when called on an edge for the firsttime (lines 11 ?
17).
For each call, the procedureretrieves the current next-best hypothesis from theagenda (line 18), generates new hypotheses by ad-vancing daughter indices (while skipping over con-figurations seen earlier) and calling itself recursivelyfor each new index (lines 19 ?
26), and, finally, ar-ranging for the resulting hypothesis to be cached forlater invocations on the same edge and i values (line27).
Note that unification (in instantiate-hypothesis())is only invoked on complete, top-level hypotheses,as our structural ME features can actually be eval-uated prior to building each full feature structure.However, as Carroll & Oepen (2005) suggest, theprocedure could be adapted to perform instantiationof sub-hypotheses within each local search, shouldadditional features require it.
For better efficiency,the instantiate-hypothesis() routine applies dynamicprogramming (i.e.
memoization) to intermediate re-sults.5 Generalizing the AlgorithmCarroll & Oepen (2005) offer no solution for selec-tive unpacking with larger context ME features.
Yet,both Toutanova et al (2005) and our own experi-ments (described in Section 7 below) suggest thatproperties of larger contexts and especially grand-parenting can greatly improve parse selection ac-curacy.
The following paragraphs outline how togeneralize the basic selective unpacking procedure,while retaining its key properties: exact n-best enu-meration with minimal search.
Our generalization ofthe algorithm distinguishes between ?upward?
con-texts, with grandparenting with dominating nodes asa representative feature type, and ?downward?
exten-sions, which we discuss for the example of lexicaln-gram features.A na?
?ve approach to selective unpacking withgrandparenting might be extending the cross-multiplication of local ambiguity to trees of morethan depth one.
However, with multiple levels ofgrandparenting this approach would greatly increasethe combinatorics to be explored, and it would posethe puzzle of overlapping local contexts of opti-mization.
Choices made among the alternates forone packed node would interact with other ambi-guity contexts in their internal nodes, rather thanmerely at the leaves of their decompositions.
How-ever, it is sufficient to keep the depth of decompo-sitions to minimal sub-trees and rather contextual-ize each decomposition as a whole.
Assuming oursample forest and set of decompositions from Fig-ure 2, let ?1 4 ?
: 6 ?
?10 ?
denote the decomposi-tion of node 6 in the context of 4 and 1 as itsimmediate parents.
When descending through theforest, hypothesize-edge() can, without significant ex-tra cost, maintain a vector ~P = ?pn .
.
.
p0?
of par-ents of the current node, for n-level grandparenting.For each packed node, the bookkeeping elements ofthe graph search procedure need to be contextual-ized on ~P , viz.
(a) the edge-local priority queue,(b) the record of index vectors hypothesized already,and (c) the cache of previous instantiations.
Assum-ing each is stored in an associative array, then allreferences to edge.agenda in the original procedurecan be replaced by edge.agenda[~P], and likewise forother slots.
With these extensions in place, the orig-inal control structure of nested, on-demand creationof hypotheses and dynamic programming of partialresults can be retained, and for each packed nodewith multiple parents ( 6 in our sample forest) therewill be parallel, contextualized partitions of opti-mization.
Thus, extra combinatorics introduced inthis generalized procedure are confined to only suchnodes, which (intuitively at least) appears to estab-lish the lower bound of added search needed?whilekeeping the algorithm non-approximative.
Section 7provides empirical data on the degradation of theprocedure in growing levels of grandparenting andthe number of n-best results to be extracted from theforest.Finally, we turn to enlarged feature contexts that53capture information from nodes below the elementsof a local decomposition.
Consider the exampleof feature type 3 in Table 1, n-grams (of vari-ous size) over properties of the yield of the parsetree.
For now we only consider lexical bi-grams.For an edge e dominating a sub-string of n words?wi .
.
.
wi+n?1?
there will be n?
1 bi-grams inter-nal to e, and two bi-grams that interact with wi?1and wi+n?which will be determined by the left-and right-adjacent edges to e in a complete tree.
Theinternal bi-grams are unproblematic, and we can as-sume that ME weights corresponding to these fea-tures have been included in the sum of weights as-sociated to e. Seeing that e may occur in multipletrees, with different sister edges, the selective un-packing procedure has to take this variation into ac-count when evaluating local contexts of optimiza-tion.Let xey denote an edge e, with x and y as thelexical types of its leftmost and rightmost daugh-ters, respectively.
Returning to our sample forest,assume lexicalizations ?
10?
and ?
11 ?
(each span-ning only one word), with ?
6= ?.
Obviously, whendecomposing 4 as ?8 6 ?, its ME score, in turn, willdepend on the choice made in the expansion of 6 :the sequences??
8?
?
6??and??
8?
?
6 ?
?will dif-fer in (at least) the scores associated with the bi-grams ????
vs.
????.
Accordingly, when evalu-ating candidate decompositions of 4 , the number ofhypotheses that need to be considered is doubled;as an immediate consequence, there can be up toeight distinct lexicalized variants for the decompo-sition 1 ?
?4 3 ?
further up in the tree.
It may lookas if combinatorics will cross-multiply throughoutthe tree?in the worst case returning us to an ex-ponential number of hypotheses?but this is fortu-nately not the case: regarding the external bi-gramsof 1 , node 6 no longer participates in its left- orrightmost periphery, so variation internal to 6 is nota multiplicative factor at this level.
This is essen-tially the observation of Langkilde (2000), and herbottom-up factoring of n-gram computation is eas-ily incorporated into our top-down selective unpack-ing control structure.
At the point where hypothesize-edge() invokes itself recursively (line 23 in Figure 3),its return value is now a set of lexicalized alternates,and hypothesis creation (in line 26) can take into ac-count the local cross-product of all such alternation.Including additional properties from non-local sub-trees (for example higher-order n-grams and headlexicalization) is a straightforward extension of thisscheme, replacing our per-edge left- and rightmostperiphery symbols with a generalized vector of ex-ternally relevant, internal properties.
In additionto traditional (head) lexicalization as we have justdiscussed it, such extended ?downward?
propertieson decompositions?percolated from daughters tomothers and cross-multiplied as appropriate?couldinclude metrics of constituent weight too, for exam-ple to enable the ME model to prefer ?balanced?
co-ordination structures.However, given that Toutanova et al (2005) ob-tain only marginally improved parse selection accu-racy from the inclusion of n-gram (and other lexical)ME features, we have left the implementation of lex-icalization and empirical evaluation for future work.6 Failure Caching and PropagationAs we pointed out at the end of Section 4, duringthe unpacking phase, unification is only replayed ininstantiate-hypothesis() on the top-level hypotheses.
Itis only at this step that inconsistencies in the localcombinatorics are discovered.
However, such a dis-covery can be used to improve the unpacking rou-tine by (a) avoiding further unification on hypothe-ses that have already failed to instantiate, (b) avoid-ing creating new hypotheses based on failed sub-hypotheses.
This requires some changes to the rou-tines instantiate-hypothesis() and hypothesize-edge(), aswell as an extra boolean marker for each hypothesis.The extended instantiate-hypothesis() starts bychecking whether the hypothesis is already markedas failed.
If it is not so marked, the routine recur-sively instantiates all sub-hypotheses.
Any failurewill again lead to instant return.
Otherwise, unifica-tion is used to create a new edge from the outcome ofthe sub-hypothesis instantiations.
If this unificationfails, the current hypothesis is marked.
Moreover,all its ancestor hypotheses are also marked (by re-cursively following the pointers to the direct parenthypotheses) as they are also guaranteed to fail.Correspondingly, hypothesize-edge() needs tocheck the instantiation failure marker to avoid re-turning hypotheses that are guaranteed to fail.
Ifa hypothesis coming out of the agenda is already54marked as failed, it will be used to create new hy-potheses (with advance-indices()), but dropped af-terward.
Subsequent hypotheses will be poppedfrom the agenda until either a hypothesis that is notmarked as failed is returned, or the agenda is empty.Moreover, hypothesize-edge() also needs to avoidcreating new hypotheses based on failed sub-hypotheses.
When a failed sub-hypothesis is found,the creation of the new hypothesis is skipped.
Butthe index vector ~I may not be simply discarded.Otherwise hypotheses based on advance-indices(~I)will not be reachable in the search.
On the otherhand, simply adding every advance-indices(~I) on tothe pending creation list is not efficient either in thecase where multiple sub-hypotheses fail.To solve the problem, we compute a failure vec-tor ~F = ?f0 .
.
.
fn?, where fj is 1 when the sub-hypothesis at position j is known as failed, and 0otherwise.
If a sub-hypothesis at position j is failedthen all the index vectors having value ij at posi-tion j must also fail.
By putting the result of ~I + ~Fon the pending creation list, we can safely skip thefailed rows of sub-hypotheses, while not losing thereachability of the others.
As an example, supposewe have a ternary index vector ?3 1 2?
for which anew hypothesis is to be created.
By checking the in-stantiation failure marker of the sub-hypotheses, wefind that the first and the third sub-hypotheses are al-ready marked.
The failure recording vector will thenbe ?1 0 1?.
By putting ?4 1 3?
= ?3 1 2?
+ ?1 0 1?on to the pending hypothesis creation list, the failedsub-hypotheses are skipped.We evaluate the effects of instantiation failurecaching and propagation below in Section 7.7 Empirical ResultsTo evaluate the performance of the selective unpack-ing algorithm, we carried out a series of empiricalevaluations with the ERG and GG, in combinationwith a modified version of the PET parser.
Whenrunning the ERG we used as our test set the JH4section of the LOGON treebank3, which contains1603 items with an average sentence length of 14.6words.
The remaining LOGON treebank (of around3The treebank is comprised of several booklets ofedited, instructional texts on backcountry activities in Nor-way.
The data is available from the LOGON web site at?http://www.emmtee.net?.Configuration GP Coverage Time (s)greedy best-first 0 91.6% 3889exhaustive unpacking 0 84.5% 4673selective unpacking0 94.3% 22451 94.3% 25292 94.3% 39643 94.2% 31994 94.2% 3502Table 2: Coverage on the ERG for different configurations, withfixed resource consumption limits (of 100k passive edges or 300seconds).
In all cases, up to ten ?best?
results were searched,and Coverage shows the percentage of inputs that succeed toparse within the available resource.
Time shows the end-to-endprocessing time for each batch.5 15 25 35String Length (Number of Input Tokens)0123456(s)(generated by [incr tsdb()] at 23-mar-2007 (12:44 h))?????????????????
gready best-first?
exhaustive unpacking?
selective unpacking?
forest creationFigure 4: Parsing times for different configurations using theERG, in all three cases searching for up to ten results, withoutthe use of grandparenting.8,000 items) was used in training the various MEparse disambiguation models.
For the experimentwith GG, we designated a 2825-item portion of theDFKI Verbmobil treebank4 for our tests, and trainedME models on the remaining 10,000 utterances.
Atonly 7.4 words, the average sentence length is muchshorter in the Verbmobil data.We ran seven different configurations of the parserwith different search strategies and (un-)packingmechanisms:?
Agenda driven greedy n-best parsing using theME score without grandparenting features; nolocal ambiguity packing;?
Local ambiguity packing with exhaustive un-packing, without grandparenting features;4The data in this treebank is taken from transcribed appoint-ment scheduling dialogues; see ?http://gg.dfki.de/?for further information on GG and its treebank.551 10 20 30 40 50 60 70 80 90 100Maximum Number of Trees to Unpack (n)0.000.020.040.060.080.10(s)?
?
?
?
??
???
?
?
??
?
???
?
?
??
?
???
?
??
?
????
??
??
?
?
?GP=0GP=1GP=2GP=3GP=4Figure 5: Mean times for selective unpacking of all test itemsfor n-best parsing with the ERG, for varying n and grandpar-enting (GP) levels?
Local ambiguity packing and selective unpack-ing for n-best parsing, with 0 through 4 levelsof grandparenting (GP) features.As a side-effect of differences in efficiency, someconfigurations could not complete parsing all sen-tences given reasonable memory constraints (whichwe set at a limit of 100k passive edges or 300 sec-onds processing time per item).
The overall cover-age and processing time of different configurationson JH4 are given in Table 2.The correlation between processing time and cov-erage is interesting.
However, it makes the efficiencycomparison difficult as parser behavior is not clearlydefined when the memory limit is exceeded.
To cir-cumvent this problem, in the following experimentswe average only over those 1362 utterances fromJH4 that complete parsing within the resource limitin all seven configurations.
Nevertheless, it mustbe noted that this restriction potentially reduces effi-ciency differences between configurations, as someof the more challenging inputs (which typically leadto the largest differences) are excluded.Figure 4 compares the processing time of differ-ent configurations.
The difference is much moresignificant for longer sentences (i.e.
with more than15 words).
If the parser unpacks exhaustively, thetime for unpacking grows with sentence length at aquickly increasing rate.
In such cases, the efficiencygain with ambiguity packing in the parsing phaseis mostly lost in the unpacking phase.
The graphshows that greedy best-first parsing without packingoutperforms exhaustive unpacking for sentences ofConfiguration Exact Match Top Tenrandom choice 11.34 43.06no grandparenting 52.52 68.38greedy best-first 51.79 69.48grandparenting[1] 56.83 85.33grandparenting[2] 56.55 84.14grandparenting[3] 56.37 84.14grandparenting[4] 56.28 84.51Table 3: Parse selection accuracy for various levels of grandpar-enting.
The exact match column shows the percentage of casesin which the correct tree, according to the treebank, was rankedhighest by the model; conversely, the top ten column indicateshow often the correct tree was among the ten top-ranking re-sults.less than 25 words.
With sentences longer than 25words, the packing mechanism helps the parser toovertake greedy best-first parsing, although the ex-haustive unpacking time also grows fast.With the selective unpacking algorithm presentedin the previous sections, unpacking time is reduced,and grows only slowly as sentence length increases.Unpacking up to ten results, when contrasted withthe timings for forest creation (i.e.
the first parsingphase) in Figure 4, adds a near-negligible extra costto the total time required for both phases.
Moreover,Figure 5 shows that with selective unpacking, as nis increased, unpacking time grows roughly linearlyfor all levels of grandparenting (albeit always withan initial delay in unpacking the first result).Table 4 summarizes a number of internal parsermeasurements using the ERG with different pack-ing/unpacking settings.
Besides the difference inprocessing time, we also see a significant differencein ?space?
between exhaustive and selective un-packing.
Also, the difference in ?unifications?
and?copies?
indicates that with our selective unpackingalgorithm, these expensive operations on typed fea-ture structures are significantly reduced.In return for increased processing time (andmarginal loss in coverage) when using grandparent-ing features, Table 3 shows some large improve-ments in parse selection accuracy (although the pic-ture is less clear-cut at higher-order levels of grand-parenting5).
A balance point between efficiency5The models were trained using the open-source TADM pack-age (Malouf, 2002), using default hyper-parameters for all con-figurations, viz.
a convergence threshold of 10?8, variance ofthe prior of 10?4, and frequency cut-off of 5.
It is likely that56Configuration GP Unifications Copies Space Unpack Total(#) (#) (kbyte) (s) (s)?
15greedy best-first 0 1845 527 2328 ?
0.12wordsexhaustive unpacking 0 2287 795 8907 0.01 0.12selective unpacking0 1912 589 8109 0.00 0.121 1913 589 8109 0.01 0.122 1914 589 8109 0.01 0.123 1914 589 8110 0.01 0.124 1914 589 8110 0.02 0.13> 15greedy best-first 0 25233 5602 24646 ?
1.66wordsexhaustive unpacking 0 39095 15685 80832 0.85 1.95selective unpacking0 17489 4422 33326 0.03 1.171 17493 4421 33318 0.05 1.212 17493 4421 33318 0.09 1.253 17495 4422 33321 0.13 1.274 17495 4422 33320 0.21 1.34Table 4: Contrasting the efficiency of various (un-)packing settings in use with ERG on short (top) and medium-length (bottom)inputs; in each configuration, up to ten trees are extracted.
Unification and Copies is the count of top-level FS operations, whereonly successful unifications require a subsequent copy (when creating a new edge).
Unpack and Total are unpacking and total parsetime, respectively.and accuracy can be made according to applicationneeds.Finally, we compare the processing time of theselective unpacking algorithm with and without in-stantiation failure caching and propagation (as de-scribed in Section 4 above).
The empirical resultsfor GG are summarized in Table 5, showing clearlythat the technique reduced unnecessary hypothesesand instantiation failures.
The design philosophy ofthe ERG and GG differ.
During the first, forest cre-ation phase, GG suppresses a number of features (inthe HPSG sense, not the ME sense) that can actuallyconstrain the combinatorics of edges.
This movemakes the packed forest more compact, but it im-plies that unification failures will be more frequentduring unpacking.
In a sense, GG thus moves partof the search for globally consistent derivations intothe second phase, and it is possible for the forest tocontain ?result?
trees that ultimately turn out to beincoherent.
Dynamic programming of instantiationfailures makes this approach tractable, while retain-ing the general breadth-first characteristic of the se-lective unpacking regime.further optimization of hyper-parameters for individual config-urations would moderately improve model performance, espe-cially for higher-order grandparenting levels with large numbersof features.8 DiscussionThe approach to n-best parsing described in this pa-per takes as its point of departure recent work of Car-roll & Oepen (2005), which describes an efficient al-gorithm for unpacking n-best trees from a forest pro-duced by a chart-based sentence generator and con-taining local ME properties with associated weights.In an almost contemporaneous study, but in the con-text of parsing with treebank grammars, Huang &Chiang (2005) develop a series of increasingly effi-cient algorithms for unpacking n-best results froma weighted hypergraph representing a parse forest.The algorithm of Carroll & Oepen (2005) and thefinal one of Huang & Chiang (2005) are essentiallyequivalent, and turn out to be reformulations of anapproach originally described by Jime?nez & Marzal(2000) (although expressed there only for grammarsin Chomsky Normal Form).In this paper we have considered ME propertiesthat extend beyond immediate dominance relations,extending up to 4 levels of grandparenting.
Pre-vious work has either assumed properties that arerestricted to the minimal parse fragments (i.e.
sub-trees of depth one) that make up the packed repre-sentation (Geman & Johnson, 2002), or has taken amore relaxed approach by allowing non-local prop-57Configuration Unifications Copies Hypotheses Space Unpack Total(#) (#) (#) (kbyte) (ms) (ms)greedy best-first 5980 1447 ?
9202 ?
400selective, no caching 5535 1523 1245 27188 70 410selective, with cache 4915 1522 382 27176 10 350Table 5: Efficiency effects of the instantiation failure caching and propagation with GG, without grandparenting.
All statistics areaverages over the 1941 items that complete within the resource bounds in all three configurations.
Unification, Copies, Unpack,and Total have the same interpretation as in Table 4, and Hypotheses is the average count of hypothesized sub-trees.erties but without addressing the problem of how toefficiently extract the top-ranked trees from a packedforest (Miyao & Tsujii, 2002).Probably the work closest in spirit to our approachis that of Malouf & van Noord (2004), who use anHPSG grammar comparable to the ERG and GG,non-local ME features, and a two-phase parse for-est creation and unpacking approach.
However, theirunpacking phase uses a beam search to find a good(single) candidate for the best parse; in contrast?for ME models containing the types of non-localfeatures that are most important for accurate parseselection?we avoid an approximative search and ef-ficiently identify exactly the n-best parses.When parsing with context free grammars, a (sin-gle) parse can be retrieved from a parse forest intime linear in the length of the input string (Bil-lot & Lang, 1989).
However, as discussed in Sec-tion 2, when parsing with a unification-based gram-mar and packing under feature structure subsump-tion, the cross-product of some local ambiguitiesmay not be globally consistent.
This means that ad-ditional unifications are required at unpacking time.In principle, when parsing with a pathological gram-mar with a high rate of failure, extracting a singleconsistent parse from the forest could take exponen-tial time (see Lang (1994) for a discussion of this is-sue with respect to Indexed Grammars).
In the caseof GG, a high rate of unification failure in unpackingis dramatically reduced by our instantiation failurecaching and propagation mechanism.9 Conclusions and Future WorkWe have described and evaluated an algorithm forefficiently computing the n-best analyses from aparse forest produced by a unification grammar, withrespect to a Maximum Entropy (ME) model con-taining two classes of non-local features.
The al-gorithm is efficient in that it empirically exhibits alinear relationship between processing time and thenumber of analyses unpacked, at all degrees of MEfeature non-locality.
It improves over previous workin providing the only exact procedure for retrievingn-best analyses from a packed forest that can dealwith features with extended domains of locality andwith forests created under subsumption.
Our algo-rithm applies dynamic programming to intermediateresults and local failures in unpacking alike.The experiments compared the new algorithmwith baseline systems representing other possibleapproaches to parsing with ME models: (a) a singlephase of agenda-driven parsing with on-line prun-ing based on intermediate ME scores, and (b) two-phase parsing with exhaustive unpacking and post-hoc ranking of complete trees.
The new approachshowed better speed, coverage, and accuracy thanthe baselines.Although we have dealt with the non-local MEfeatures that in previous work have been found to bethe most important for parse selection (i.e.
grand-parenting and n-grams), this does not exhaust thefull range of features that could possibly be useful.For example, it may be the case that accurately re-solving some kinds of ambiguities can only be donewith reference to particular parts?or combinationsof parts?of the HPSG feature structures represent-ing the analysis of a complete constituent.
To dealwith such cases we are currently designing an exten-sion to the algorithms described here which wouldadd a ?controlled?
beam search, in which the size ofthe beam was limited by the interval of score adjust-ments for ME features that could only be evaluatedonce the full linguistic structure became available.This approach would involve a constrained amountof extra search, but would still produce the exact n-best trees.58ReferencesAbney, S. P. (1997).
Stochastic attribute-value grammars.
Com-putational Linguistics, 23, 597 ?
618.Billot, S., & Lang, B.
(1989).
The structure of shared forestsin ambiguous parsing.
In Proceedings of the 27th Meetingof the Association for Computational Linguistics (pp.
143 ?151).
Vancouver, BC.Callmeier, U.
(2002).
Preprocessing and encoding techniquesin PET.
In S. Oepen, D. Flickinger, J. Tsujii, & H. Uszkor-eit (Eds.
), Collaborative language engineering.
A case studyin efficient grammar-based processing.
Stanford, CA: CSLIPublications.Caraballo, S. A., & Charniak, E. (1998).
New figures of meritfor best-first probabilistic chart parsing.
Computational Lin-guistics, 24(2), 275 ?
298.Carroll, J., & Oepen, S. (2005).
High-efficiency realization fora wide-coverage unification grammar.
In R. Dale & K. F.Wong (Eds.
), Proceedings of the 2nd International JointConference on Natural Language Processing (Vol.
3651, pp.165 ?
176).
Jeju, Korea: Springer.Clark, S., & Curran, J. R. (2004).
Parsing the WSJ using CCGand log-linear models.
In Proceedings of the 42nd Meetingof the Association for Computational Linguistics (pp.
104 ?111).
Barcelona, Spain.Copestake, A.
(2002).
Implementing typed feature structuregrammars.
Stanford, CA: CSLI Publications.Crysmann, B.
(2005).
Relative clause extraposition in German.An efficient and portable implementation.
Research on Lan-guage and Computation, 3(1), 61 ?
82.Erbach, G. (1991).
A flexible parser for a linguistic develop-ment environment.
In O. Herzog & C.-R. Rollinger (Eds.
),Text understanding in LILOG (pp.
74 ?
87).
Berlin, Ger-many: Springer.Flickinger, D. (2000).
On building a more efficient grammarby exploiting types.
Natural Language Engineering, 6 (1),15 ?
28.Geman, S., & Johnson, M. (2002).
Dynamic programming forparsing and estimation of stochastic unification-based gram-mars.
In Proceedings of the 40th Meeting of the Associationfor Computational Linguistics.
Philadelphia, PA.Huang, L., & Chiang, D. (2005).
Better k-best parsing.
InProceedings of the 9th International Workshop on ParsingTechnologies (pp.
53 ?
64).
Vancouver, Canada.Jime?nez, V. M., & Marzal, A.
(2000).
Computation of then best parse trees for weighted and stochastic context-freegrammars.
In Proceedings of the Joint International Work-shops on Advances in Pattern Recognition (pp.
183 ?
192).London, UK: Springer-Verlag.Johnson, M., Geman, S., Canon, S., Chi, Z., & Riezler, S.(1999).
Estimators for stochastic ?unification-based?
gram-mars.
In Proceedings of the 37th Meeting of the Associationfor Computational Linguistics (pp.
535 ?
541).
College Park,MD.Kasami, T. (1965).
An efficient recognition and syntax al-gorithm for context-free languages (Technical Report # 65-758).
Bedford, MA: Air Force Cambrige Research Labora-tory.Klein, D., & Manning, C. D. (2003).
A* parsing.
Fast exactViterbi parse selection.
In Proceedings of the 4th Confer-ence of the North American Chapter of the ACL.
Edmonton,Canada.Lang, B.
(1994).
Recognition can be harder than parsing.
Com-putational Intelligence, 10(4), 486 ?
494.Langkilde, I.
(2000).
Forest-based statistical sentence gener-ation.
In Proceedings of the 1st Conference of the NorthAmerican Chapter of the ACL.
Seattle, WA.Malouf, R. (2002).
A comparison of algorithms for maxi-mum entropy parameter estimation.
In Proceedings of the6th Conference on Natural Language Learning.
Taipei, Tai-wan.Malouf, R., & van Noord, G. (2004).
Wide coverage parsingwith stochastic attribute value grammars.
In Proceedings ofthe IJCNLP workshop Beyond Shallow Analysis.
Hainan,China.Miyao, Y., Ninomiya, T., & Tsujii, J.
(2005).
Corpus-orientedgrammar development for acquiring a Head-Driven PhraseStructure Grammar from the Penn Treebank.
In K.-Y.
Su,J.
Tsujii, J.-H. Lee, & O. Y. Kwong (Eds.
), Natural languageprocessing (Vol.
3248, pp.
684 ?
693).
Hainan Island, China.Miyao, Y., & Tsujii, J.
(2002).
Maximum entropy estimationfor feature forests.
In Proceedings of the Human LanguageTechnology Conference.
San Diego, CA.Moore, R. C., & Alshawi, H. (1992).
Syntactic and semanticprocessing.
In H. Alshawi (Ed.
), The Core Language Engine(pp.
129 ?
148).
Cambridge, MA: MIT Press.Mu?ller, S., & Kasper, W. (2000).
HPSG analysis of German.In W. Wahlster (Ed.
), Verbmobil.
Foundations of speech-to-speech translation (Artificial Intelligence ed., pp.
238 ?
253).Berlin, Germany: Springer.Oepen, S., & Carroll, J.
(2002).
Efficient parsing forunification-based grammars.
In S. Oepen, D. Flickinger,J.
Tsujii, & H. Uszkoreit (Eds.
), Collaborative language en-gineering.
A case study in efficient grammar-based process-ing (pp.
195 ?
225).
Stanford, CA: CSLI Publications.Oepen, S., Flickinger, D., Toutanova, K., & Manning, C. D.(2004).
LinGO Redwoods.
A rich and dynamic treebank forHPSG.
Journal of Research on Language and Computation,2(4), 575 ?
596.Riezler, S., King, T. H., Kaplan, R. M., Crouch, R., Maxwell III,J.
T., & Johnson, M. (2002).
Parsing the Wall Street Journalusing a Lexical-Functional Grammar and discriminative es-timation techniques.
In Proceedings of the 40th Meeting ofthe Association for Computational Linguistics.
Philadelphia,PA.Toutanova, K., Manning, C. D., Flickinger, D., & Oepen, S.(2005).
Stochastic HPSG parse selection using the Red-woods corpus.
Journal of Research on Language and Com-putation, 3(1), 83 ?
105.59
