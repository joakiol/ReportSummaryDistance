Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 354?357,Prague, June 2007. c?2007 Association for Computational LinguisticsUBC-UPC: Sequential SRL Using Selectional Preferences.An aproach with Maximum Entropy Markov ModelsBen?at Zapirain, Eneko AgirreIXA NLP GroupUniversity of the Basque CountryDonostia, Basque Country{benat.zapirain,e.agirre}@ehu.esLlu?
?s Ma`rquezTALP Research CenterTechnical University of CataloniaBarcelona, Catalonialluism@lsi.upc.eduAbstractWe present a sequential Semantic Role La-beling system that describes the taggingproblem as a Maximum Entropy MarkovModel.
The system uses full syntactic in-formation to select BIO-tokens from inputdata, and classifies them sequentially us-ing state-of-the-art features, with the addi-tion of Selectional Preference features.
Thesystem presented achieves competitive per-formance in the CoNLL-2005 shared taskdataset and it ranks first in the SRL subtaskof the Semeval-2007 task 17.1 IntroductionIn Semantic Role Labeling (SRL) the goal is to iden-tify word sequences or arguments accompanying thepredicate and assign them labels depending on theirsemantic relation.
In this task we disambiguate ar-gument structures in two ways: predicting VerbNet(Kipper et al, 2000) thematic roles and PropBank(Palmer et al, 2005) numbered arguments, as wellas adjunct arguments.In this paper we describe our system for the SRLsubtask of the Semeval2007 task 17.
It is based onthe architecture and features of the system named?model 2?
of (Surdeanu et al, forthcoming), but itintroduces two changes: we use Maximum Entropyfor learning instead of AdaBoost and we enlarge thefeature set with combined features and other seman-tic features.Traditionally, most of the features used in SRLare extracted from automatically generated syntac-tic and lexical annotations.
In this task, we also ex-periment with provided hand labeled semantic infor-mation for each verb occurrence such as the Prop-Bank predicate sense and the Levin class.
In addi-tion, we use automatically learnt Selectional Prefer-ences based on WordNet to generate a new kind ofsemantic based features.We participated in both the ?close?
and the ?open?tracks of Semeval2007 with the same system, mak-ing use, in the second case, of the larger CoNLL-2005 training set.2 System Description2.1 Data RepresentationIn order to make learning and labeling easier, wechange the input data representation by navigatingthrough provided syntactic structures and by extract-ing BIO-tokens from each of the propositions to beannotated as shown in (Surdeanu et al, forthcom-ing).
These sequential tokens are selected by ex-ploring the sentence spans or regions defined by theclause boundaries, and they are labeled with BIOtags depending on the location of the token: at thebeginning, inside, or outside of a verb argument.
Af-ter this data pre-processing step, we obtain a morecompact and easier to process data representation,making also impossible overlapping and embeddedargument predictions.2.2 Feature RepresentationApart from Selectional Preferences (cf.
Section 3)and those extracted from provided semantic infor-mation, most of the features we used are borrowedfrom the existing literature (Gildea and Jurafsky,2002; Xue and Palmer, 2004; Surdeanu et al, forth-coming).354On the verb predicate:?
Form; Lemma; POS tag; Chunk type and Typeof verb phrase; Verb voice; Binary flag indicat-ing if the verb is a start/end of a clause.?
Subcategorization, i.e., the phrase structure ruleexpanding the verb parent node.?
VerbNet class of the verb (in the ?close?
trackonly).On the focus constituent:?
Type; Head;?
First and last words and POS tags of the con-stituent.?
POS sequence.?
Bag-of-words of nouns, adjectives, and adverbsin the constituent.?
TOP sequence: right-hand side of the rule ex-panding the constituent node; 2/3/4-grams ofthe TOP sequence.?
Governing category as described in (Gildeaand Jurafsky, 2002).Context of the focus constituent:?
Previous and following words and POS tags ofthe constituent.?
The same features characterizing focus con-stituents are extracted for the two previous andfollowing tokens, provided they are inside theclause boundaries of the codified region.Relation between predicate and constituent:?
Relative position; Distance in words andchunks; Level of embedding with respect to theconstituent: in number of clauses.?
Binary position; if the argument is after or be-fore the predicate.?
Constituent path as described in (Gildea andJurafsky, 2002); All 3/4/5-grams of path con-stituents beginning at the verb predicate or end-ing at the constituent.?
Partial parsing path as described in (Carreraset al, 2004)); All 3/4/5-grams of path elementsbeginning at the verb predicate or ending at theconstituent.?
Syntactic frame as described by Xue andPalmer (2004)Combination Features?
Predicate and Phrase Type?
Predicate and binary position?
Head Word and Predicate?
Predicate and PropBank frame sense?
Predicate, PropBank frame sense, VerbNetclass (in the ?close?
track only)2.3 Maximum Entropy Markov ModelsMaximum Entropy Markov Models are a discrimi-native model for sequential tagging that models thelocal probability P (sn | sn?1, o), where o is thecontext of the observation.Given a MEMM, the most likely state sequence isthe one that maximizes the followingS = argmaxn?i=1P (si | si?1, o)Translating the problem to SRL, we haverole/argument labels connected to each state in thesequence (or proposition), and the observations arethe features extracted in these points (token fea-tures).
We get the most likely label sequence findingout the most likely state sequence (Viterbi).All the conditional probabilities are given by theMaximum Entropy classifier with a tunable Gaus-sian prior from the Mallet Toolkit1.Some restrictions are considered when we searchthe most likely sequence2:1.
No duplicate argument classes for A0-A5 andthematic roles.2.
If there is a R-X argument (reference), thenthere has to be a X argument before (refer-enced).3.
If there is a C-X argument (continuation), thenthere has to be a X argument before.4.
Before a I-X token, there has to be a B-X or I-Xtoken (because of the BIO encoding).5.
Given a predicate and its PropBank sense, onlysome arguments are allowed (e.g.
not all theverbs support A2 argument).6.
Given a predicate and its Verbnet class, onlysome thematic roles are allowed.3 Including Selectional PreferencesSelectional Preferences (SP) try to capture the factthat linguistic elements prefer arguments of a cer-tain semantic class, e.g.
a verb like ?eat?
prefers assubject edible things, and as subject animate entities,as in ?She was eating an apple?
They can be learnedfrom corpora, generalizing from the observed argu-ment heads (e.g.
?apple?, ?biscuit?, etc.)
into ab-stract classes (e.g.
edible things).
In our case we1http://mallet.cs.umass.edu2Restriction 5 applies to PropBank output.
Restriction 6 ap-plies to VerbNet output355follow (Agirre and Martinez, 2001) and use Word-Net (Fellbaum, 1998) as the generalization classes(the concept <food,nutrient>).The aim of using Selectional Preferences (SP) inSRL is to generalize from the argument heads inthe training instances into general word classes.
Intheory, using word classes might overcome the datasparseness problem for the head-based features, butat the cost of introducing some noise.More specifically, given a verb, we study the oc-currences of the target verb in a training corpus (e.g.the PropBank corpus), and learn a set of SPs foreach argument and adjunct of that verb.
For in-stance, given the verb ?kill?
we would have 2 SPsfor each argument type, and 4 SPs for some of theobserved adjuncts: kill A0, kill A1, kill AM-LOC, kill AM-MNR, kill AM-PNC and kill AM-TMP.Rather than coding the SPs directly as features,we code the predictions instead, i.e.
for each propo-sition in the training and testing set, we check theSPs for all the argument (and adjunct) headwords,and the SP which best fits the headword (see below)is the one that is selected.
We codify the predictedargument (or adjunct) label as features, and we insertthem among the corresponding argument features.For instance, let?s assume that the word ?railway?appears as the headword of a candidate argument of?kill?.
WordNet 1.6 yields the following hypernymsfor ?railway?
(from most general to most specific, weinclude the WordNet 1.6 concept numbers precededby their specifity level);1 00001740 1 000179542 00009457 2 059629763 00011937 3 059975924 03600463 4 060045805 03243979 5 060082366 03526208 6 060058397 03208595 7 029275998 03209020Note that we do not care about the sense ambigu-ity and the explosion of concepts that it carries.
Ouralgorithm will check each of the hypernyms of rail-way and match them with the concepts in the SPs of?kill?, giving preference to the most specific concept.In case that equally specific concepts match differentSPs, we will choose the SP that has the concept thatranks highest in the SP, and code the SP feature withthe label of the SP where the match succeeds.
In theexample, these are the most specific matches:AM-LOC Con:03243979 Level:5 Ranking:32A0 Con:06008236 Level:5 Ranking:209There is a tie in the level, so we choose the onewith the highest rank.
All in all, this means that ac-cording to the learnt SPs we would predict that ?rail-way?
is a location feature for ?kill?, and we wouldtherefore insert the ?SP:AM-LOC?
feature amongthe argument features.If ?railway?
appears as the headword of otherverbs, the predicted argument might be different.See for instance, the following verbs:destroy:A1 Con:03243979 Level:5 Ranking:43go:A0 Con:02927599 Level:7 Ranking:131go:A2 Con:02927599 Level:7 Ranking:721build:A1 Con:03209020 Level:8 Ranking:294Note that our training examples did not contain?railway?
as an argument of any of these verbs, butdue to the SPs we are able to code into a feature that?railway?
belongs to a concrete semantic class whichcontains conceptually similar headwords.We decided to code the prediction of the SPs,rather than the SPs themselves, in order to be morerobust to noise.There is a further subtlety with our SP system.
Inorder to label training and testing sets in similar con-ditions and avoid overfitting problems as much aspossible, we split the training set into five folds andtagged each one with SPs learnt from the other four.For extracting SP features from test set examples,we use SPs learnt in the whole training set.4 Experiments and ResultsWe participated in the ?close?
and the ?open?
trackswith the same classification model, but using dif-ferent training sets in each one.
In the close trackwe only use the provided training set, and in theopen, the CoNLL-2005 training set (without Verb-Net classes or thematic roles).Before our participation, we tested the system inthe CoNLL-2005 close track setting and it achievedcompetitive performance in comparison to the state-of-the-art results published in that challenge.4.1 Semeval2007 settingThe data provided in the close track consists of thepropositions of 50 different verb lemmas from Prop-Bank (sections 02-21).
The data for the CoNLL-2005 is also a subset of the PropBank data, but it356Track Label rank prec.
rec.
F1Close VerbNet 1st 85.31 82.08 83.66Close PropBank 1st 85.04 82.07 83.52Open PropBank 1st 84.51 82.24 83.36Table 1: Results in the SRL subtask of SemEval-2007 task 17includes all the propositions in sections 02-21 andno VerbNet classes nor thematic roles for learning.There is a total of 21 argument types for Prop-Bank and 47 roles for VerbNet, which amounts to21 ?
2 + 1 = 43 BIO-labels for PropBank predic-tions and 47 ?
2 + 1 = 95 for VerbNet.
We filteredthe less frequent (<5).We trained the Maximum Entropy classifiers with114,380 examples for the close track, and with828,811 for the open track.
We tuned the classifierby setting the Exponential Gaussian prior in 0.14.2 ResultsIn the close track we trained two classifiers, oneto label PropBank numbered arguments and a sec-ond to label VerbNet thematic roles.
Due to lackof time, we only trained the PropBank labels in theopen track.
Table 1 shows the results obtained in theSRL subtask.
We ranked first in all of them, out oftwo participants.4.3 DiscussionThe results indicate that in the close track the systemperforms similarly on both PropBank arguments andVerbNet roles.
The absence of VerbNet class-basedfeatures in the CoNLL-2005 training data couldcause the loss of performance in the open track.
Weplan to perform the experiment on VerbNet roles forthe open track to check the ability of the classifier togeneralize across verbs.Regarding the use of SP features, nowadays, wehave not obtained relevant improvements in the pre-dictions of the classifiers.
It is our first approach tothese kind of semantic features and there are moresophisticated but evident extraction variants whichwe are exploring.Although the general performance is very simi-lar without SP features, using them our system ob-tains better results in ARG3 core arguments and inthe most frequent adjuncts such as location (LOC),general-purpose (ADV) and temporal (TMP).We reproduced this improvements in experimentsrealized with CoNLL-2005 larger test sets.
In thatcase, we improved ARG3-ARG4 core arguments aswell as the mentioned adjuncts.
There were moreexamples to be classified and we get better overallperformance, but we need further experiments to bemore conclusive.5 ConclusionsWe have presented a sequential semantic role la-beling system for the Semeval-2007 task 17 (SRL).Based on Maximum Entropy Markov Models, it ob-tains competitive and promising results.
We alsohave introduced semantic features extracted fromSelectional Restrictions but we only have prelimi-nary evidence of their usefulness.AcknowledgementsWe thank David Martinez for kindly providing thesoftware that learnt the selectional preferences.
Thiswork has been partially funded by the Spanish ed-ucation ministry (KNOW).
Ben?at is supported by aPhD grant from the University of the Basque Coun-try.ReferencesE.
Agirre and D. Martinez.
2001.
Learning class-to-classselectional preferences.
In Proceedings of CoNLL-2001, Toulouse, France.X.
Carreras, L. Ma`rquez, and G. Chrupa?a.
2004.
Hi-erarchical recognition of propositional arguments withperceptrons.
In Proceedings of CoNLL 2004.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling ofsemantic roles.
Computational Linguistics , 28(3).K.
Kipper, Hoa Trang Dang, and M. Palmer.
2000.Class-based construction of a verb lexicon.
In Pro-ceedings of AAAI-2000 Seventeenth National Confer-ence on Artificial Intellingence, Austin, TX .M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics , 31(1).M.
Surdeanu, L. Ma`rquez, X. Carreras, and P.
Comas.(forthcoming).
Combination strategies for semanticrole labeling.
In Journal of Artificial Intelligence Re-search.N.
Xue and M. Palmer.
2004.
Calibrating features for se-mantic role labeling.
In Proceedings of EMNLP-2004 .357
