Coling 2010: Poster Volume, pages 1113?1121,Beijing, August 2010Controlled Natural Languages for Knowledge RepresentationRolf SchwitterCentre for Language TechnologyMacquarie UniversityRolf.Schwitter@mq.edu.auAbstractThis paper presents a survey of researchin controlled natural languages that can beused as high-level knowledge representa-tion languages.
Over the past 10 yearsor so, a number of machine-oriented con-trolled natural languages have emergedthat can be used as high-level interfacelanguages to various kinds of knowledgesystems.
These languages are relevant tothe area of computational linguistics sincethey have two very interesting properties:firstly, they look informal like natural lan-guages and are therefore easier to writeand understand by humans than formallanguages; secondly, they are preciselydefined subsets of natural languages andcan be translated automatically (and oftendeterministically) into a formal target lan-guage and then be used for automated rea-soning.
We present and compare the mostmature of these novel languages, showhow they can balance the disadvantagesof natural languages and formal languagesfor knowledge representation, and discusshow domain specialists can be supportedwriting specifications in controlled naturallanguage.1 IntroductionNatural languages are probably the most expres-sive knowledge representation languages that ex-ist; they are easy for humans to use and under-stand, and they are so powerful that they caneven serve as their own metalanguages.
Ironi-cally, it is just this expressive quality that makesnatural languages notoriously difficult for a com-puter to process and understand because a lot ofrelevant information is usually not stated explic-itly in an utterance but only implied by the hu-man author or speaker.
There exist ?
of course ?many useful resources and automated techniquesthat partly compensate for the lack of this back-ground knowledge, and there are many useful ap-plications that require only shallow processingof natural languages.
But there exist ?
withoutdoubt ?
many potential application scenarios thatwould benefit from deeper (axiom-based) knowl-edge that can be created and modified in a human-friendly way.Formal languages (Monin, 2003) have beensuggested and used as knowledge representationlanguages since they have a well-defined syntax,an unambiguous semantics and support automatedreasoning.
But these languages are often ratherdifficult for domain specialists to understand andcause a cognitive distance to the application do-main that is not inherent in natural language.One way to bridge the gap between a naturallanguage and a formal language is the use of acontrolled natural language (CNL) that can me-diate between these languages.
CNLs are engi-neered subsets of natural languages whose gram-mar and vocabulary have been restricted in a sys-tematic way in order to reduce both ambiguity andcomplexity of full natural languages.Traditionally, CNLs have been grouped intotwo broad categories: human-oriented CNLs andmachine-oriented CNLs (Huijsen, 1998).
Themain objective of human-oriented CNLs is to im-prove the readability and comprehensibility oftechnical documentation (e.g.
maintenance doc-1113umentation (ASD Simplified Technical English1)and to simplify and standardise human-humancommunication for specific purposes (e.g.
fortrade or for air traffic control (see (Pool, 2006)for an overview)).
The primary goal of machine-oriented CNLs is to improve the translatabilityof technical documents (e.g.
machine translation(Nyberg and Mitamura, 2000)) and the acquisi-tion, representation, and processing of knowledge(e.g.
for knowledge systems (Fuchs et al, 2008)and in particular for the Semantic Web (Schwitteret al, 2008)).Human- and machine-oriented CNLs have beendesigned with different goals in mind, and it is notsurprising that their coverage can be quite differ-ent.
O?Brien (2003) shows that there is not muchoverlap between the rule sets of CNLs in these twocategories nor among the rule sets within a cate-gory.
But since the structure of these CNLs is usu-ally simpler and more predictable than the struc-ture of full natural language, CNLs are in generaleasier for humans to understand and easier for acomputer to process.
An ideal CNL for knowl-edge representation should also be effortless towrite and expressive enough to describe the prob-lem at hand.In this paper, we will survey machine-orientedCNLs that can be used for knowledge represen-tation and can serve as high-level interface lan-guages to knowledge systems.
The rest of this pa-per is structured as follows: In Section 2, we intro-duce the most mature general-purpose CNLs anddiscuss the motivation for their design and inves-tigate their characteristics.
In Section 3, we dis-cuss some theoretical issues regarding the expres-sivity and complexity of CNLs.
Building on thesetheoretical considerations, we look in Section 4at a number of machine-oriented CNLs that havebeen developed specifically as interface languagesto the Semantic Web.
In Section 5, we discuss theimportance of supporting the writing process ofCNLs in an suitable way and compare three dif-ferent techniques.
In Section 6, we discuss differ-ent approaches that have been used to evaluate thewritability and understandability of CNLs, and fi-nally in Section 7, we present our conclusions.1http://www.asd-ste100.org/2 General-Purpose CNLsIn this section we focus on a number of machine-oriented CNLs that have been designed to serveas knowledge representation languages.
TheseCNLs are general-purpose languages in the sensethat they have not been developed for a spe-cific scenario or a particular application domain.These languages can be used where traditionalformal languages are used otherwise.
The aimof these languages is to equip domain specialistswith an expressive knowledge representation lan-guage that is on the one hand easy to learn, useand understand and on the other hand fully pro-cessable by a computer.2.1 Attempto Controlled English (ACE)ACE (Fuchs et al, 2008) is a CNL that cov-ers a well-defined subset of English that can betranslated unambiguously into first-order logicvia discourse representation structures (Kamp andReyle, 1993) and then be used for automated rea-soning.
ACE is defined by a small set of con-struction rules that describe its syntax and a smallset of interpretation rules that disambiguate con-structs that might appear ambiguous in full En-glish.
The vocabulary of ACE consists of pre-defined function words (e.g.
determiners, con-junctions, and pronouns), some predefined fixedphrases (e.g.
there is, it is false that), and con-tent words (nouns, proper names, verbs, adjec-tives, and adverbs).
ACE supports language con-structs such as:?
active and passive verbs (and modal verbs);?
strong negation (e.g.
no, does not) and weaknegation (e.g.
is is not provable that);?
subject and object relative clauses;?
declarative, interrogative, imperative andconditional sentences;?
various forms of anaphoric references tonoun phrases (e.g.
he, himself, the man, X).It is important to note that the meaning of wordsin ACE is not predefined; the user is expected todefine their meaning by ACE sentences or importthese definitions from an existing formal ontology.1114Here is a simple example of an ACE text togetherwith a question:Every company that buys at least threemachines gets a discount.
Six Swisscompanies each buy one machine.
AGerman company buys four machines.Who gets a discount?Note that ACE uses disambiguation markers(e.g.
each) on the surface level and mathematicalbackground knowledge about natural numbers inorder to answer the question above.
This mathe-matical knowledge is implemented as a set of Pro-log predicates which are executed during the proof(question answering process).ACE is supported by various tools2, amongthem a text editor that helps users to construct cor-rect ACE sentences with the help of hints and er-ror messages, a parser that translates ACE textsinto discourse representation structures, a para-phraser that reflects the interpretation of the ma-chine in CNL, and a Satchmo-style reasoning en-gine that can be used for consistency and redun-dancy checking as well as for question answering.Applications of ACE include software and hard-ware specifications, agent control, legal and med-ical regulations, and ontology construction.2.2 Processable English (PENG)PENG (White and Schwitter, 2009) is a CNL thatis similar to ACE but adopts a more light-weightapproach in the sense that it covers a smaller butfully tractable subset of English.
The languageprocessors of ACE and PENG are both basedon grammars that are written in a definite clausegrammar (DCG) notation.
These DCGs are en-hanced with feature structures and specifically de-signed to translate declarative and interrogativesentences into a first-order logic notation via dis-course representation structures.
In contrast to theoriginal version of ACE that uses the DCG di-rectly and resolves anaphoric references only aftera discourse representation structure has been con-structed, PENG transforms the DCG into a for-mat that can be processed by a top-down chartparser and resolves anaphoric references during2http://attempto.ifi.uzh.ch/site/tools/the parsing process while a discourse representa-tion structure is built up.
PENG has been designedfor an incremental parsing approach and was thefirst CNL that was supported by a predictive editor(Schwitter et al, 2003).
The PENG system pro-vides text- and menu-based writing support thatremoves some of the burden of learning and re-membering the constraints of the CNL from theuser and generates a paraphrase that clarifies theinterpretation for each sentence that the user en-ters.
PENG?s text editor dynamically enforcesthe grammatical restrictions of the CNL via look-ahead information while a text is written.
For eachword form that the user enters into the editor, a listof options is generated incrementally by the chartparser to inform the user about how the structureof the current sentence can be continued.
The syn-tactic restrictions ensure that the text follows therules of the CNL so that it can be translated un-ambiguously into the formal target language (first-order logic) and be processed by a theorem prover.In order to illustrate how PENG can be usedto reconstruct a problem in controlled natural lan-guage, we use an example from the TPTP problemlibrary3.
The problems in this library are usuallyused to test the capacity of automated reasoningtools and are translated manually by a human intothe formal target language.
For reasons of space,we use here one of the simpler problems of the li-brary; the puzzle PUZ012-1 below is also knownas ?The Mislabeled Boxes?
:There are three boxes a, b, and c on atable.
Each box contains apples or ba-nanas or oranges.
No two boxes con-tain the same thing.
Each box has a la-bel that says it contains apples or saysit contains bananas or says it containsoranges.
No box contains what it sayson its label.
The label on box a says?apples?.
The label on box b says ?or-anges?.
The label on box c says ?ba-nanas?.
You pick up box b and it con-tains apples.
What do the other twoboxes contain?In order to solve this puzzle by a computer,we have to reconstruct it and augment it with the3http://www.cs.miami.edu/?tptp/1115relevant background knowledge.
The main prob-lems that we face here for machine-processing arethe following ones: some of the constructions inthe problem description are ambiguous (e.g.
theantecedent for the personal pronoun it is opento two interpretations); the semantic relation be-tween some content words is not explicit (e.g.
therelation between the actual things in the box andthe names on the labels that describe these things);and some of the constructions are not relevant atall for the solution of the problem (e.g.
that thethree boxes are on the table).
Here is a possiblereconstruction of this puzzle in PENG:The label of the box a says APPLES.The label of the box b says ORANGES.The label of the box c says BANANAS.APPLES stands for apples.
ORANGESstands for oranges.
BANANAS standsfor bananas.
All apples are fruits.
Allbananas are fruits.
All oranges arefruits.
Each box contains the applesor contains the bananas or contains theoranges.
It is not the case that a boxcontains fruits and that the label of thebox says something that stands for thosefruits.
It is not the case that a box Xcontains fruits and that a box Y con-tains those fruits.
The box b containsthe apples.
What does the box a con-tain?
What does the box c contain?Note that this reconstruction makes informationthat is implicit or only assumed in the originalproblem description explicit in PENG.PENG has recently been used for the construc-tion of an interface to a situation awareness system(Baader et al, 2009) but the language can be usedfor similar applications to ACE.2.3 Computer Processable Language (CPL)CPL (Clark et al, 2010) is a controlled languagethat has been developed at Boeing Research andTechnology.
In contrast to ACE which applies asmall set of strict interpretation rules, and in con-trast to PENG, which relies on a predictive editor,the CPL interpreter directly resolves various typesof ambiguities using heuristic rules for preposi-tional phrase attachment, word sense disambigua-tion, semantic role labeling, compound noun in-terpretation, metonymy resolution, and other lan-guage processing activities.CPL accepts three types of sentences: groundfacts, questions, and rules.
In the case of groundfacts, a basic CPL sentence takes one of the fol-lowing three forms:?
There is|are NP?
NP verb [NP] [PP]*?
NP is|are passive-verb [by NP] [PP]*Verbs can include auxiliaries and particles, andnouns in noun phrases can be modified by othernouns, prepositional phrases, and adjectives.
Inthe case of questions, CPL accepts five forms; thetwo main forms are:?
What is NP??
Is it true that Sentence?In the case of rules, CPL accepts sentence pat-terns of the form:?
IF Sentence [AND Sentence]* THEN Sen-tence [AND Sentence]*Parsing of CPL is performed bottom-up withthe help of a broad coverage chart parser that usespreference for common word attachment patternsstored in a manually constructed database.
Dur-ing parsing, a simplified logical form is generatedfor basic sentences by rules that run in parallel tothe grammar rules.
There is no explicit quanti-fier scoping for these basic sentences and somedisambiguation decisions (e.g., word sense andsemantic relationships) are deferred and handledby the inference engine that makes a ?best guess?of word sense assignments using WordNet4.
Thelogical form is used to generate ground Knowl-edge Machine (KM) assertions.
KM5 is a frame-based language with first-order semantics.
TheKM interpreter employs a sophisticated machin-ery for reasoning, including reasoning about ac-tions using a situation calculus mechanism.
Rules4http://wordnet.princeton.edu/5http://userweb.cs.utexas.edu/users/mfkb/km.html1116are entered by the user who writes CPL sentenceswith the help of rule templates.
There exist seventemplates for this purpose: three of them createstandard logical implications and the rest describepreconditions and effects of actions.
Each CPLsentence is interpreted interactively.
The systemparaphrases its interpretation back to the user, al-lowing the user to spot and fix misinterpretations.Sentences that express states add facts to a sit-uation, and sentences that express actions trig-ger rules that update the situation, reflecting thechanges that the action has on the situation.
Theuser can ask questions about an emerging situationdirectly in CPL.While CPL relies on heuristics, CPL-Lite is aslimmed down version of CPL that can be in-terpreted deterministically in a similar fashion toPENG.
Each CPL-Lite sentence corresponds to asingle binary relation between two entities.
CPL-Lite distinguishes three types of relations: noun-like relations (e.g.
the age of <x> is <y>), verb-like relations (e.g.
<x> causes <y>), and pre-position-like relations (e.g.
<x> is during <y>).Interestingly, CPL-Lite has the same expressiv-ity as CPL, but CPL-Lite is more verbose andgrammatically more restricted.
For example, thefollowing two CPL sentences:1.
A man drives a car along a road for 1 hour.2.
The speed of the car is 30 km/h.can be expressed (or better reconstructed) in anunambiguous way in CPL-Lite:3.
A person drives a vehicle.4.
The path of the driving is a road.5.
The duration of the driving is 1 hour.6.
The speed of the driving is 30 km/h.Note that the user used here the noun person in-stead of man and vehicle instead of car during thisreconstruction process because only these wordswere available in the system?s ontology.CPL and CPL-Lite have been mainly used toencode general and domain specific common-sense knowledge and to allow knowledge engi-neers to pose queries in a comprehensible way.2.4 Other General-Purpose CNLsCommon Logic Controlled English (CLCE)6 is aproposal for a CNL ?
similar to ACE and PENG?
that has been designed as a human interface lan-guage for the ISO standard Common Logic (CL)7.However, CLCE itself is not part of this stan-dard but uses Common Logic semantics.
CLCEsupports full first-order logic with equality sup-plemented with an ontology for sets, sequences,and integers.
The primary syntactic restrictionsare the use of present tense verbs, singular nouns,and variables instead of pronouns.
Despite theselimitations, CLCE can express the kind of Englishused in software specifications, mathematics text-books, and definitions and axioms found in formalontologies.Formalized-English (Martin, 2002) is anotherproposal for a CNL that can be used as a gen-eral knowledge representation language.
This lan-guage has a relatively simple structure and is de-rived from a conventional knowledge represen-tation language.
Formalized-English contains anumber of formal-looking language elements andis therefore not a strict subset of standard English.3 Theoretical ConsiderationsDuring the design of a CNL one has to pay atten-tion to two important theoretical issues: the ex-pressive power of the envisaged language and itscomputational complexity.
E2V (Pratt-Hartmann,2003) is a CNL that mainly grew out of theoret-ical studies about the expressivity and complex-ity of natural language fragments.
E2V corre-sponds to the decidable two-variable fragment offirst-order logic (L2).
This fragment is interest-ing since it has the so-called finite model property.That means if a formula of L2 is satisfiable, then itis satisfiable in a finite model.
E2V includes deter-miners (every, no, a), nouns, transitive verbs, verbphrase negation, relative, reflexive, and personalpronouns.
Without any writing support it is diffi-cult to decide if a sentence is in E2V or not.
Forexample, one reading of sentence (7) is in E2V,the other one is not:6http://www.jfsowa.com/clce/specs.htm7ISO/IEC24707:200711177.
Every artist who employs a carpenter de-spises every beekeeper who admires him.On the syntactic level, E2V is a subset of ACEwith the exception that pronouns (e.g.
him) al-ways refer to the closest (acceptable) noun in thesyntax tree (e.g.
artist) and not to the closest (ac-ceptable) noun that occurs in the surface structure(e.g.
carpenter).
This is because the E2V inter-pretation relies on the two-variable fragment offirst-order logic.
Note that sentence (7) has thefollowing two possible representations (8 and 9)in first-order logic:8.
?x1 (artist(x1) & ?x2(carpenter(x2) & employ(x1,x2)) ->?x3 (beekeeper(x3) &admire(x3,x1) -> despise(x1,x3)))9.
?x1 ?x2 (artist(x1) &(carpenter(x2) & employ(x1,x2) ->?x3 (beekeeper(x3) &admire(x3,x2) -> despise(x1,x3)))Although there are three variables in the for-mula (8) that correspond to the three nouns insentence (7), the variables x2 and x3 never oc-cur free in the same sub-formula.
Therefore, thenumber of variables can be reduced by replacingx3 through x2.
This technique can not be appliedto the variables in formula (9).E2V has been extended in various ways (Pratt-Hartmann and Third, 2006) and one extension in-cludes counting determiners (e.g.
at least three,at most five, exactly four).
These determiners willnot in general translate into the two-variable frag-ment of first-order logic, but into the fragmentC2, which adds counting quantifiers to the two-variable fragment.
The satisfiability problem ofthis fragment is still decidable and its expressivityand computational complexity is similar to thosedescription logic languages that build the founda-tion of the Semantic Web.4 CNLs for the Semantic WebRecently, a number of CNLs have been developedthat can serve as front-end to those formal lan-guages that are used in the context of the SemanticWeb8.
These CNLs can be used by domain spe-cialists who prefer familiar natural language-likenotations over formal ones for authoring and ver-balising formal ontologies.ACE View (Kaljurand, 2007) is a CNL editorthat supports a defined subset of ACE that can beused as an alternative syntax for the Semantic Weblanguages OWL and SWRL.
ACE View integratestwo mappings: one from ACE to OWL/SWRLand one from OWL to ACE.
These mappings arenot bidirectional in a strict sense since the OWLto ACE mapping also covers OWL axioms andexpression types that the ACE to OWL mappingdoes not generate.Sydney OWL Syntax (SOS) (Cregan et al,2007) is a proposal for a CNL that builds uponPENG and provides a syntactically bidirectionalmapping to OWL-DL.
SOS is strictly bidirec-tional: each statement can be translated into OWLfunctional-style syntax and vice versa.
The bidi-rectional translation is achieved with the help of adefinite clause grammar that generates the targetnotation during the parsing process.
In contrast toACE, syntactic constructs of OWL are always car-ried over one-to-one to SOS.
Thus, semanticallyequivalent OWL statements that use different syn-tactical constructs are always mapped to differentSOS statements.Rabbit (Hart et al, 2008) is a CNL designed fora scenario where a domain expert and an ontologyengineer work together to build an ontology.
Theconstruction process is supported by a text-basedontology editor.
The editor accepts Rabbit sen-tences, helps to resolve possible syntax errors, andtranslates well-formed sentences into OWL.
Thesemantics of some Rabbit constructs is controver-sial (e.g.
exclusive interpretation of disjunction)and hard to align with the semantics of OWL.Lite Natural Language (Bernardi et al, 2007)is a CNL based on Categorial Grammar; it hasthe same expressivity as the description logic DL-Lite.
DL-Lite is a tractable fragment of OWLand has polynomial time complexity for the mainreasoning tasks.
DL-Lite is expressive enoughto capture relational databases and UML (UnifiedModeling Language) diagrams.8http://www.w3.org/TR/owl2-overview/1118CLOnE (Funk et al, 2007) is a CNL that isbuilt on top of the natural language processingframework GATE9.
CLOnE is a simple ontol-ogy authoring language that consists of elevensentence patterns which roughly correspond toeleven OWL axiom patterns.
It is unclear whetherCLOnE can be extended in a systematic way tocover larger fragments of OWL.The three controlled languages ACE, SOS, andRabbit are compared in more detail in Schwitter etal.
(2008).
There exist three other CNL researchstreams that are closely related to the SemanticWeb: CNLs for querying Semantic Web content(Bernstein and Kaufmann, 2006); CNLs for main-taining semantic wikis (Kuhn, 2009; Kuhn, 2010);and CNLs for describing rules and policies (DeCoi et al, 2009).5 Writing Support for CNLsWriting a specification in CNL is not an easy tasksince the author has to stick to the rules of the con-trolled language.
Writing in CNL is in essencea normative process that prescribes how humansshould use language to communicate effectivelywith a computer in order to achieve a particu-lar goal.
The challenge here is to develop in-terface techniques that make the writing processas unobtrusive and effortless as possible.
Threemain techniques have been suggested to supportthe writing process of CNLs: the use of error mes-sages, conceptual authoring, and predictive feed-back.Error messages seem to be the most obviousway to support the writing of a text in CNL, andmany CNLs (among them (Clark et al, 2010;Fuchs et al, 2008)) use this technique.
The useris supposed to learn and remember the restrictionsof the CNL and then to write the text followingthe memorised rules.
If the parsing process fails,then the CNL system tries to identify the causeof the error and provides one or more suggestionsfor how to fix the error.
The problem with thistechnique is that the input might be an unrestrictedsentence and a useful error message would requirein the worst case knowledge of the sort that isneeded for processing full natural language.9http://gate.ac.uk/Conceptual authoring (Power et al, 2009) isa technique that allows authors to edit a knowl-edge base on the semantic level by refining spe-cific categories and properties that occur in CNLsentences via a hierarchy of menu options.
Theselection of an option by the author results in anupdate of the underlying model and triggers thegeneration of a new sentence that can then be fur-ther refined.
This method relies on natural lan-guage generation techniques and makes the anal-ysis of CNL sentences unnecessary.
The problemwith this technique is that it does not allow the au-thor to specify new knowledge that is not alreadyencoded in the knowledge base; it is basically atechnique for knowledge authoring and visualiza-tion and does not provide an independent knowl-edge representation language.Predictive feedback (Schwitter et al, 2003;Kuhn and Schwitter, 2008) is a technique that in-forms the authors during the writing process aboutthe approved structures of the CNL.
This tech-nique relies on interfaces that are aware of thegrammar and can look-ahead within this grammar.Using this technique the author receives immedi-ate feedback while a text is written and cannotenter sentences that are not in the scope of thegrammar.
The grammar of the language PENGhas been designed from the beginning to be usedin a predictive editor and is processed by a chartparser that is able to generate the look-ahead in-formation.
The following example illustrates howa predictive editor works:?
A [ adjective | common noun ]?
A man [ verb | who | ?does not?
]?
A man works [ ?.?
| preposition | adverb ]In this example the look-ahead informationconsists of syntactic categories, word forms andpunctuation marks; all these elements are imple-mented as hypertext links.
Selecting a hypertextlink for a syntactic category displays approvedword forms and selecting a word form or a punc-tuation mark directly adds this element to the text.Kuhn (2010) shows in an number of experimentsthat predictive editors are easy for untrained usersto use and argues that predictive feedback is thebest way to support the writing process of CNLs.11196 Evaluating CNLsOver the past years, a number of different userexperiments have been designed to measure var-ious usability aspects of CNLs (see (Kuhn, 2010)for an introduction).
These experiments can begrouped into three different categories: task-basedexperiments, paraphrase-based experiments, andgraph-based experiments.In task-based experiments (for example, (Kauf-mann and Bernstein, 2007)), human subjects re-ceive a certain task that requires them to use aCNL as an interface language to a knowledge basetogether with a tool that potentially supports thewriting process.
These experiments test how easyor difficult it is to write in these controlled lan-guages using the given tool, but they do not testthe understandability of these languages.Paraphrase-based experiments (for example,(Hart et al, 2008)) aim to evaluate the understand-ability of a CNL in a tool-independent way.
Hu-man subjects receive a statement in CNL and achoice of paraphrases in full natural language, andthen have to select the correct paraphrase.
Theseexperiments scale well with the expressivity of theCNL but it is difficult to guarantee that the para-phrases are understood in the intended way.Graph-based experiments (for example,(Kuhn, 2010)) try to overcome the problems ofparaphrase-based experiments.
In order to test theunderstandability of CNLs and formal languages,a graph-based notation is used to describe asituation accompanied with statements in thelanguage to be tested.
The human subjects haveto decide which of these statements are true andwhich ones are false with respect to the situationillustrated by the graph notation.The reported results of these experiments in theliterature provide strong evidence that CNLs areeasier to write and easier to understand for domainspecialists than formal languages.7 ConclusionsIt is an exciting time to work on controlled naturallanguages.
In this paper, we surveyed a numberof machine-oriented controlled natural languagesthat can be used instead of formal languages forrepresenting knowledge.
These controlled nat-ural languages look like English but correspondto a formal target language.
Anyone who canread English has already the basic skills to under-stand these controlled natural languages.
Writinga specification in controlled natural language is abit harder: it requires that the author either learnsthe language in order to be able to stay withinits syntactic and semantic restrictions or that heuses an intelligent authoring tool that supports thewriting process and enforces the restrictions of thelanguage.Machine-oriented controlled natural languagescan be translated automatically (and often deter-ministically) into a formal target language (e.g.into full first-order logic or into a version of de-scription logics).
These languages can be usedto express the kind of information that occurs insoftware specifications, formal ontologies, busi-ness rules, and legal and medical regulations.In summary, an ideal machine-oriented con-trolled natural language should fulfill at least thefollowing requirements: (a) it should have a well-defined syntax and a precise semantics that is de-fined by an unambiguous mapping into a logic-based representation; (b) it should look as naturalas possible and be based on a subset of a certainnatural language; (c) it should be easy for humansto write and understand and easy for a machine toprocess; and (d) it should have the necessary ex-pressivity that is required to describe a problem inthe respective application domain.Of course these requirements can be in con-flict with each other and therefore careful com-promises need to be made when a new controllednatural language is designed.
This design processoffers many interesting research challenges for re-searchers in the area of computational linguisticsand artificial intelligence.
This research is drivenby the overall goal to close the gap between natu-ral and formal languages and to allow for true col-laboration between humans and machines in thenear future.AcknowledgmentsI would like to thank to three anonymous review-ers of Coling 2010 for their valuable feedback andto Robert Dale for comments and suggestions onprevious versions of this paper.1120ReferencesBaader, Franz, Andreas Bauer, Peter Baumgartner,Anne Cregan, Alfredo Gabaldon, Krystian Ji, KevinLee, Dave Rajaratnam and R. Schwitter.
2009.
ANovel Architecture for Situation Awareness Sys-tems, In: Proceedings of TABLEAUX 2009, LNAI5607, pp.
77?92.Bernardi, Raffaella, Diego Calvanese, and CamiloThorne.
2007.
Lite Natural Language.
In: Pro-ceedings of IWCS-7.Bernstein, Abraham and Esther Kaufmann.
2006.GINO ?
a guided input natural language ontologyeditor.
In: Proceedings of ISWC 2006, LNCS 4273,pp.
144?157.Clark, Peter, Phil Harrison, William R. Murray, andJohn Thompson.
2010 Naturalness vs. Predictabil-ity: A Key Debate in Controlled Languages.
In:Proceedings 2009 Workshop on Controlled NaturalLanguages (CNL?09).Cregan, Anne, Rolf Schwitter, and Thomas Meyer.2007.
Sydney OWL Syntax ?
towards a ControlledNatural Language Syntax for OWL 1.1.
In: Pro-ceedings of OWLED 2007, CEUR, vol.
258.De Coi, Juri L., Norbert E. Fuchs, Kaarel Kaljurand,Tobias Kuhn.
2009.
Controlled English for Rea-soning on the Semantic Web.
In: LNCS, vol.
5500,pp.
276?308.Fuchs, Norbert E., Kaarel Kaljurand, and Tobias Kuhn.2008.
Attempto Controlled English for KnowledgeRepresentation.
In: Reasoning Web, LNCS, vol.5224, pp.
104?124.Funk, Adam, Valentin Tablan, Kalina Bontcheva,Hamish Cunningham, Brian Davis, and SiegfriedHandschuh.
2007.
CLOnE: Controlled Languagefor Ontology Editing.
In: Proceedings of ISWC2007.Hart, Glen, Martina Johnson, and Catherine Dolbear.2008.
Rabbit: Developing a controlled natural lan-guage for authoring ontologies.
In: Proceedings ofESWC 2008, LNCS, vol.
5021, pp.
348?360.Huijsen, Willem-Olaf.
1998.
Controlled Language ?An Introduction.
In: Proceedings of CLAW 98, pp.1?15.Kaljurand, Kaarel.
2007.
Attempto Controlled En-glish as a Semantic Web Language.
PhD The-sis.
Faculty of Mathematics and Computer Science,University of Tartu.Kamp, Hans and Uwe Reyle.
1993.
From Discourseto Logic.
Kluwer, Dordrecht.Kaufmann, Esther and Abraham Bernstein.
2007.How Useful Are Natural Language Interfaces to theSemantic Web for Casual End-Users?
In: Proceed-ings of ISWC/ASWC 2007, NLCS, vol.
4825, pp.281?294.Kuhn, Tobias and Rolf Schwitter.
2008.
Writing Sup-port for Controlled Natural Languages.
In: Pro-ceedings of ALTA 2008, pp.
46?54.Kuhn, Tobias.
2009.
How controlled English can im-prove semantic wikis.
In: Proceedings of SemWiki2009, CEUR, vol.
464.Kuhn, Tobias.
2010.
Controlled English for Knowl-edge Representation.
Doctoral Thesis.
Faculty ofEconomics, Business Administration and Informa-tion Technology of the University of Zurich.Martin, Philippe.
2002.
Knowledge representationin CGLF, CGIF, KIF, Frame-CG and Formalized-English.
In: Proceedings of ICCS 2002, LNAI, vol.2393, pp.
77?91.Monin, Jean-Franc?ois.
2003.
Understanding FormalMethods.
Springer-Verlag, London.Nyberg, Eric H. and Teruko Mitamura.
2000.
TheKANTOO Machine Translation Environment.
In:Proceedings of AMTA 2000, LNCS, vol.
1934, pp.192?195.O?Brien, Sharon.
2003.
Controlling controlled english?
an analysis of several controlled language rulesets.
In: Proceedings of EAMT-CLAW 03, DublinCity University, Ireland, pp.
105?114.Pool, Jonathan.
2006.
Can Controlled LanguagesScale to the Web?
In: Proceedings of the 5th Int.Workshop on Controlled Language Applications.Power, Richard, Robert Stevens, Donia Scott, and AlanRector.
2009.
Editing OWL through generatedCNL.
In: Pre-Proceedings of the Workshop on CNL2009, CEUR, vol.
448.Pratt-Hartmann, Ian.
2003.
A two-variable fragmentof English.
In: Journal of Logic, Language and In-formation, 12(1), pp.
13?45.Pratt-Hartmann, Ian and Allan Third.
2006.
Morefragments of language: the case of ditransitiveverbs.
In: Notre Dame Journal of Formal Logic,47(2), pp.
151?177.Schwitter, Rolf, Anna Ljungberg, and David Hood.2003.
ECOLE ?
A Look-ahead Editor for a Con-trolled Language.
In: Proceedings of EAMT-CLAW03, pp.
141?150.Schwitter, Rolf, Kaarel Kaljurand, Anne Cregan,Catherine Dolbear, and Glen Hart.
2008.
Acomparison of three controlled natural languagesfor OWL 1.1.
In: Proceedings of OWLED 2008,CEUR, vol.
496.White, Colin and Rolf Schwitter.
2009.
An Update onPENG Light.
In: Proceedings of ALTA 2009, pp.80?88.1121
