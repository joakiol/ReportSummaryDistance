Forest-Based Statistical Sentence Generat ionI rene  Langk i ldeInformation Sciences Inst i tuteUniversity of Southern CaliforniaMarina del Rey CA 90292ilangkil?isi .eduAbst rac tThis paper presents a new approach to sta-tistical sentence generation in which Mterna-tive phrases are represented as packed sets oftrees, or forests, and then ranked statistically tochoose the best one.
This representation ffersadvantages in compactness and in the abilityto represent syntactic information.
It also fa-cilitates more efficient statistical ranking thana previous approach to statistical generation.An efficient ranking algorithm is described, to-gether with experimental results showing signif-icant improvements over simple enumeration ora lattice-based approach.1 I n t roduct ionLarge textual corpora offer the possibility ofa statistical approach to the task of sentencegeneration.
Like any large-scale NLP or AItask, the task of sentence generation requiresimmense amounts of knowledge.
The knowledgeneeded includes lexicons, grammars, ontologies,collocation lists, and morphological tables.
Ac-quiring and applying accurate, detailed knowl-edge of this breadth poses difficult problems.Knight and Hatzivassiloglou (1995) suggestedovercoming the knowledge acquisition bottle-neck in generation by tapping the informationinherent in textual corpora.
They performed ex-periments showing that automatically-acquired,corpus-based knowledge greatly reduced theneed for deep, hand-crafted knowledge.
Atthe same time, this approach to generation im-proved scalability and robustness, offering thepotential in the future for higher quality out-put.In their approach, K ~: H adapted techniquesused in speech recognition.
Corpus-based sta-tistical knowledge was applied to the generationprocess after encoding many alternative phras-ings into a structure called a lattice (see Fig-ure 1).
A lattice was able to represent largenumbers alternative phrases without requiringthe large amount of space that an explicitly enu-merated list of individual alternatives would re-quire.
The Mternative sentences in the latticewere then ranked according to a statistical lan-guage model, and the most likely sentence waschosen as output.
Since the number of phrasesthat needed be considered typically grew ex-ponentially with the length of the phrase, thelattice was usually too large for an exhaustivesearch, and instead an n-best algorithm wasused to heuristically narrow the search.The lattice-based method, though promising,had several drawbacks that will be discussedshortly.
This paper presents a different methodof statistical generation based on a forest struc-ture (a packed set of trees).
A forest is morecompact han a lattice, and it offers a hierar-chical organization that is conducive to repre-senting syntactic information.
Furthermore, itfacilitates dramatically more efficient statisticalranking, since constraints can be localized, andthe combinatorial explosion of possibilities thatneed be considered can be reduced.
In additionto describing the forest data structure we use,this paper presents a forest-based ranking algo-rithm, and reports experimental results on itsefficiency in both time and space.
It also favor-ably compares these results to the performanceof a lattice-based approach.2 Represent ing  Al ternat ive  Phrases2.1 Enumerated  lists and latticesThe task of sentence generation involves map-ping from an abstract representation f mean-ing or syntax to a linear ordering of words.Subtasks of generation usually include choosingcontent words, determining word order,170.g.~Figure 1: A lattice representing 576 different sen-tences, including "You may have to eat chicken","The chicken may have to be eaten by you", etc.171deciding when to insert function words, per-forming morphological inflections, and satis-fying agreement constraints, as well as othertasks.One way of leveraging corpus-based knowl-edge is to explicitly enumerate many alternatepossibilities and select the most likely accordingto a corpus-based statistical model.
Since manysubphrases and decisions will be common acrosspropose d sentences, a lattice is a more efficientway than one-by-one numeration to representthem.
A lattice is a graph where each arc is la-beled with a word.
A complete path from theleft-most node to right-most node through thelattice represents a possible sentence.
Multiplearcs leaving a particular node represent alter-nate paths.
A lattice thus allows structure tobe shared between sentences.
An example of alattice is shown in Figure 1.
This lattice encodes576 unique sentences.
In practice, a lattice mayrepresent many trillions of sentences.
Withouta compact representation for so many sentences,statistical generation would be much less feasi-ble.The lattice in Figure 1 illustrates severaltypes of decisions that need to be made in gen-eration.
For example, there is a choice be-tween the root words "chicken" and "poulet",the choice of whether to use singular or pluralforms of these words, the decision whether touse an article or not, and if so, which one--definite or indefinite.
There are also other wordchoice decisions uch as whether to use the aux-iliary verb "could", "might", or "may", andwhether to express the mode of eating with thepredicate "have to", "be obliged to", or "be re-quired to".
Finally, there is a choice betweenactive voice (bottom half of lattice), and pas-sive voice (top half).Inspection of the lattice reveals some un-avoidable duplication, however.
For example,the word "chicken" occurs four times, whilethe sublattice for the noun phrase contain-ing "chicken" is repeated twice.
So is theverb phrase headed by the auxiliaries "could","might", and "may".
Such repetition is com-mon in a lattice representation for text genera-tion, and has a negative impact on the efficiencyof the ranking algorithm because the same setof score calculations end up being made severaltimes.
Another drawback of the duplication isthat the representation consumes more storagespace than necessary.Yet another drawback of the lattice represen-tation is that the independence between manychoices cannot be fully exploited.
Stolcke et al(1997) noted that 55% of all word dependenciesoccur between adjacent words.
This means thatmost choices that must be made in non-adjacentparts of a sentence are independent.
For ex-ample, in Figure 1, the choice between "may","might", or "could" is independent of the choicebetween "a", "an" or "the" to precede "chicken"or "poulet".
Independence r duces the combi-nation of possibilities that must be considered,and allows some decisions to be made with-out taking into account he rest of the context.Even adjacent words are sometimes indepen-dent of each other, such as the words "tail" and"ate" in the sentence "The dog with the shorttail ate the bone".
A lattice does not offer anyway of representing which parts of a sentenceare independent of each other, and thus can-not take advantage of this independence.
Thisnegatively impacts both the amount of process-ing needed and the quality of the results.
Incontrast, a forest representation, which we willdiscuss shortly, does allow the independence tobe explicitly annotated.A final difficulty with using lattices is thatthe search space grows exponentially with thelength of the sentence(s), making an exhaustivesearch for the most likely sentence impracticalfor long sentences.
Heuristic-based searches of-fer only a poor approximation.
Any pruningthat is done renders the solution theoreticallyinadmissable, and in practice, frequently endsup pruning the mathematically optimal solu-tion.2.2 ForestsThese weaknesses of the lattice representationcan be overcome with a forest representation.
Ifwe assign a label to each unique arc and to eachgroup of arcs that occurs more than once in alattice, a lattice becomes a forest, and the prob-lems with duplication in a lattice are eliminated.The resulting structure can be represented as aset of context-free rewrite rules.
Such a forestneed not necessarily comply with a particulartheory of syntactic structure, but it can if onewishes.
It also need not be derived specificallyfrom a lattice, but can be generated irectly172from a semantic input.With a forest representation, it is quite nat-ural to incorporate syntactic information.
Syn-tactic information offers some potentially signif-icant advantages for statistical language model-ing.
However, this paper will not discuss tatis-tical modeling of syntax beyond making men-tion of it, leaving it instead for future work.
In-stead we focus on the nature of the forest repre-sentation itself and describe ageneral algorithmfor ranking alternative trees that can be usedwith any language model.A forest representation corresponding to thelattice in Figure 1 is shown in Figure 3.
Thisforest structure is an AND-OR graph, where theAND nodes represent sequences ofphrases, andthe OR nodes represent mutually exclusive al-ternate phrasings for a particular elative po-sition in the sentence.
For example, at the toplevel of the forest, node S.469 encodes the choicebetween active and passive voice versions of thesentence.
The active voice version is the leftchild node, labelled S.328, and the passive voiceversion is the right child node, S.358.
Thereare eight OR-nodes in the forest, correspondingto the eight distinct decisions mentioned earlierthat need to be made in deciding the best sen-tence to output.The nodes are uniquely numbered, so that re-peated references to the same node can be iden-tified as such.
In the forest diagram, only thefirst (left-most) reference to a node is drawncompletely.
Subsequent references only showthe node name written in italics.
This easesreadability and clarifies which portions of theforest actually need to have scores computedduring the ranking process.
Nodes N.275,NP.318, VP.225 and PRP.3 are repeated in theforest of Figure 3.S.469 ~ S.328S.469 ==~ S.358S.328 ~ PRP.3 VP.327PRP.3 ~ "you"VP.327 ==ez VP.248 NP.318S.358 ~ NP.318 VP.357NP.318 ~ NP.317NP.318 ~ N.275Figure 2: Internal representation f top nodes inforestFigure 2 illustrates how the forest is repre-sented internally, showing context-free r writerules for some of the top nodes in the forest.OR-nodes are indicated by the same label oc-curing more than once on the left-hand side of arule.
This sample of rules includes an exampleof multiple references to a node, namely nodeNP.318, which occurs on the right-hand side oftwo different rules.A generation forest differs from a parse forestin that a parse forest represents different pos-sible hierarchicM structures that cover a singlephrase.
Meanwhile a generation forest gener-ally represents one (or only a few) heirarchi-cal structures for a given phrase, but representsmany different phrases that generally expressthe same meaning.2.3 Prev ious work on packedgenerat ion treesThere has been previous work on developinga representation for a packed generation foreststructure.
Shemtov (1996) describes extensionsto a chart structure for generation originallypresented in (Kay, 1996) that is used to gen-erate multiple paraphrases from a semantic in-put.
A prominent aspect of the representationis the use of boolean vector expressions to asso-ciate each sub-forest with the portions of the in-put that it covers and to control the unification-based generation process.
A primary goM of therepresentation is to guarantee that each part ofthe semantic input is expressed once and onlyonce in each possible output phrase.In contrast, the packed forest in this paperkeeps the association between the semantic in-put and nodes in the forest separate from theforest representation itself.
(In our system,these mappings are maintained via an externalcache mechanism as described in (Langkilde andKnight, 1998)).
Once-and-only-once coverage ofthe semantic input is implicit, and is achievedby the process that maps from the input to aforest.3 Forest  rank ing  a lgor i thmThe algorithm proposed here for ranking sen-tences in a forest is a bottom-up dynamic pro-gramming algorithm.
It is analogous to achart parser, but performs an inverse compari-son.
Rather than comparing alternate syntacticstructures indexed to the same positions of an1737/t 71\ / \  ,-tit .~ ~=-~ \ / \ />  ,;-.
}i g - ~}?
~ -~- -~-~.Figure 3: A generation forest174input sentence, it compares alternate phrasescorresponding to the same semantic input.As in a probabilistic hart parser, the keyinsight of this algorithm is that the score foreach of the phrases represented by a particu-lar node in the forest can be decomposed intoa context-independent (i ternal) score, and acontext-dependent (external) score.
The inter-nal score, once computed, is stored with thephrase, while the external score is computed incombination with other sibling nodes.In general, the internal score for a phrase as-sociated with a node p can be defined recur-sively as:I(p) = 1-Ij=lJ I(cj) ?
E(cjJcontext(cl..Cj_l) )where I stands for the internal score, E the ex-ternal score, and cj for a child node of p. Thespecific formulation of I and E, and the pre-cise definition of the context depends on the lan-guage model being used.
As an example, in abigram model, I I=1 for leaf nodes, and E canbe expressed as:E = P(E i rstWord(e j ) lLastWord(c j_ l )  )Depending on the language model being used,a phrase will have a set of externally-relevantfeatures.
These features are the aspects of thephrase that contribute to the context-dependentscores of sibling phrases.
In the case of the bi-gram model, the features are the first and lastwords of the phrase.
In a trigram model it is thefirst and last two words.
In more elaborate lan-guage models, features might include elementssuch as head word, part-of-speech tag, constitu-tent category, etc.A crucial advantage of the forest-basedmethod is that at each node only the best in-ternally scoring phrase for each unique combi-nation of externally relevant features needs tobe maintained.
The rest can be pruned with-out sacrificing the guarantee of obtaining theoverall optimal solution.
This pruning reducesexponentially the total number of phrases thatneed to be considered.
In effect, the rankingIA bigram model is based on conditional probabil-ities, where the likelihood of each word in a phrase isassumed to depend on only the immediately previousword.
The likelihood of a whole phrase is the product ofthe conditional probabilities of each of the words in thephrase.VP.344 ~ VP.225 TO.341 VB.342 VBN.330225: 341: 342: 330:might havemay havecould havemight be requiredmay be requiredcould be requiredmight be havingmay be havingcould be havingmight be obligedmay be obligedcould be obligedto be eaten344:might ... eatenmay ... eatencould ... eatenFigure 4: Pruning phrases from a forest node,assuming a bigram modelalgorithm exploits the independence that existsbetween most disjunctions in the forest.To illustrate this, Figure 4 shows an exam-ple of how phrases in a node are pruned, as-suming a bigram model.
The rule for nodeVP.344 in the forest of Figure 3 is shown, to-gether with the phrases corresponding to eachof the child nodes.
If every possible com-bination of phrases is considered for the se-quence of nodes on the right-hand side, thereare three unique first words, namely "might","may" and "could", and only one unique finalword, "eaten".
Given that only the first andlast words of a phrase are externally relevantfeatures in a bigram model, only the three bestscoring phrases (out of the 12 total) need tobe maintained for node VP.344--one for eachunique first-word and last-word pair.
The othernine phrases can never be ranked higher, nomatter what constituents VP.344 later combineswith.Pseudocode for the ranking algorithm isshown below.
"Node" is assumed to bea record composed at least of an array ofchild nodes, "Node->c\[1..N\]," and best-rankedphrases, "Node->p\[1..M\]."
The function Con-catAndScore concatenates two strings together,and computes a new score for it based on theformula given above.
The function Prune guar-175antees that only the best phrase for each uniqueset of features values is maintained.
The coreloop in the algorithm considers the children ofthe node one-by-one, concatenating and scoringthe phrases of the first two children and prun-ing the results, before considering the phrasesof the third child, and concatenating them withthe intermediate results, and so on.
From thepseudocode, it can be seen that the complex-ity of the algorithm is dominated by the num-ber of phrases associated with a node (not thenumber of rules used to represent he forest,nor the number of children in a an AND node).More specifically, because of the pruning, it de-pends on the number of features associated withthe language model, and the average number ofunique combinations of feature values that areseen.
If f is the number of features, v the av-erage number of unique values seen in a nodefor each feature, and N the number of N bestbeing maintained for each unique set of fea-ture values (but not a cap on the number ofphrases), then the algorithm has the complex-ity O((vN) 2/) (assuming that children of ANDnodes are concatenated in pairs).
Note that f=2for the bigram model, and f=4 for the trigrammodel.In comparison, the complexity of an exhaus-RankForest(Node){if ( Leafp(Node)) LeafScore( Node);fo r j= l to J  {if ( not(ranked?
(Node->e\[j\])))RankForest(Node- > c\[j\]);}for m=l  to NumberOfPhrasesIn(Node->c\[1\])Node->p\[m\] = (Node->c\[1\])->p\[m\];k=ffpr j=2 to J {for m=l  to NumberOfPhrasesIn(Node)for n=l  to NumberOfPhrasesIn(Node->c\[j\])temp\[k++\] = ConcatAndScore(Node->p\[m\],(Node- >c\[j\])- >Pin\]);Prune( temp);for m=l  to NumberOfPhrasesIn(temp)Node->p\[m\] = (temp\[m\]);}tive search algorithm on a lattice is O((vN)~),where l is approximately the length of thelongest sentence in the lattice.
The forest-basedalgorithm thus offers an exponential reductionin complexity while still guaranteeing an opti-mal solution.
A capped N-best heuristic searchalgorithm on the other hand has complexityO(vN1).
However, as mentioned earlier, it typ-ically fails to find the optimal solution withlonger sentences.In conclusion, the tables in Figure 5 and Fig-ure 6 show experimental results comparing aforest representation to a lattice in terms of thetime and space used to rank sentences.
Theseresults were generated from 15 test set inputs,whose average sentence length ranged from 14to 36 words.
They were ranked using a bigrammodel.
The experiments were run on a SparcUltra 2 machine.
Note that the time results forthe lattice are not quite directly comparable tothose for a forest because they include overheadcosts for loading portions of a hash table.
It wasnot possible to obtain timing measurements forthe search algorithm alone.
We estimate thatroughly 80% of the time used in processing thelattice was used for search alone.
Instead, theresults in Figure 5 should be interpreted as acomparison between different kinds of systems.In that respect, it can be observed from Ta-ble 5 that the forest ranking program performsat least 3 or 4 seconds faster, and that the timeneeded does not grow linearly with the num-ber of paths being considered as it does withthe lattice program.
Instead it remains fairlyconstant.
This is consistent with the theoreti-cal result that the forest-based algorithm doesnot depend on sentence length, but only on thenumber of different alternatives being consid-ered at each position in the sentence.From Table 6 it can be observed that whenthere are a relatively moderate number of sen-tences being ranked, the forest and the latticeare fairly comparable in their space consump-tion.
The forest has a little extra overhead inrepresenting hierarchical structure.
However,the space requirements of a forest do not growlinearly with the number of paths, as do thoseof the lattice.
Thus, with very large numbersof paths, the forest offers significant savings inspace.The spike in the graphs deserves particular176comment.
Our current system for producing 2so~forests from semantic inputs generally producesOR-nodes with about two branches.
The par- 20o00oticular input that triggered the spike produceda forest where some high-level OR-nodes had amuch larger number of branches.
In a lattice, 150~oany increase in the number of branches expo-nentially increases the processing time and stor-age space requirements.
However, in the forest ,000oorepresentation, the increase is only polynomialwith the number of branches, and thus did not ~oproduce a spike.IO0  , ?
, ?
, ?
, ?
, ?
,90 "latt;,'e"" fo res t "  ---x---SO70605O403O2O10.
.
.
.
.
-x00000 .
.
.
.
.
.
.
.
.
.
.
10+06 10+08 10+10 10+12 10+14 10+16 le+18 lo+20Figure 5: Time required for the ranking pro-cess using a lattice versus a forest representa-tion.
The X-axis is the number of paths (logloscale), and the Y-axis is the time in seconds.4 Future  WorkThe forest representation a d ranking algorithmhave been implemented as part of the Nitro-gen generator system.
The results shown inthe previous section illustrate the time andspace advantages of the forest representationwhich make calculating the mathematically op-timal sentence in the forest feasible (particularlyfor longer sentences).
However, obtaining themathematically optimal sentence is only valu-able if the mathematical model itself providesa good fit.
Since a forest representation makesit possible to add syntactic information to themathematical model, the next question to askis whether such a model can provide a betterfit for natural English than the ngram modelswe have used previously.
In future work, weplan to modify the forests our system produces~orest"  -*-x- --10000 le+06 le+08 le+10 le+12 le+14 Ie+16 le+18 le+20Figure 6: Size of the data structure for a latticeversus a forest representation.
The X-axis is thenumber of paths (log~o scale), and the Y-axis isthe size in bytes.so they conform to the Penn Treebank corpus(Marcus et al, 1993) annotation style, and thendo experiments using models built with Tree-bank data.5 AcknowledgmentsSpecial thanks go to Kevin Knight, DanielMarcu, and the anonymous reviewers for theircomments.
This research was supported in partby NSF Award 9820291.Re ferencesM.
Kay.
1996.
Chart generation.
In Proc.
ACL.K.
Knight and V. Hatzivassiloglou.
1995.
Two-level, many-paths generation.
In Proc.
A CL.I.
Langkilde and K. Knight.
1998.
Generationthat exploits corpus-based statistical knowl-edge.
In Proc.
COLING-ACL.M.
Marcus, B. Santorini, and M. Marcinkiewicz.1993.
Building a large annotated corpus ofenglish: the Penn treebank.
ComputationalLinguistics, 19(2).H.
Shemtov.
1996.
Generation of paraphrasesfrom ambiguous logical forms.
In Coling'96.A.
Stolcke.
1997.
Linguistic knowledge andempirical methods in speech recognition.
AIMagazine, 18(4):25-31.177
