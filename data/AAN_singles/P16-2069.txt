Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 425?431,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsOne model, two languages: training bilingual parsers with harmonizedtreebanksDavid Vilares, Carlos G?omez-Rodr?
?guez and Miguel A. AlonsoGrupo LyS, Departamento de Computacio?n, Universidade da Corun?aCampus de A Corun?a s/n, 15071, A Corun?a, Spain{david.vilares, carlos.gomez, miguel.alonso}@udc.esAbstractWe introduce an approach to train lexical-ized parsers using bilingual corpora ob-tained by merging harmonized treebanksof different languages, producing parsersthat can analyze sentences in either ofthe learned languages, or even sentencesthat mix both.
We test the approachon the Universal Dependency Treebanks,training with MaltParser and MaltOpti-mizer.
The results show that these bilin-gual parsers are more than competitive, asmost combinations not only preserve accu-racy, but some even achieve significant im-provements over the corresponding mono-lingual parsers.
Preliminary experimentsalso show the approach to be promising ontexts with code-switching and when morelanguages are added.1 IntroductionThe need of frameworks for analyzing content indifferent languages has been discussed recently(Dang et al, 2014), and multilingual dependencyparsing is no stranger to this challenge.
Data-driven parsing models (Nivre, 2006) can be trainedfor any language, given enough annotated data.On languages where treebanks are not available,cross-lingual transfer can be used to train parsersfor a target language with data from one or moresource languages.
Data transfer approaches (e.g.Yarowsky et al (2001), Tiedemann (2014)) maplinguistic annotations across languages throughparallel corpora.
Instead, model transfer ap-proaches (e.g.
Naseem et al (2012)) rely on cross-linguistic syntactic regularities to learn aspects ofthe source language that help parse an unseen lan-guage, without parallel corpora.Model transfer approaches have benefitted fromthe development of multilingual resources thatharmonize annotations.
Petrov et al (2011)proposed a universal tagset, and McDonald etal.
(2011) employed it to transfer delexicalizedparsers (Zeman and Resnik, 2008).
More recently,several projects have presented treebank collec-tions of multiple languages with their annotationsstandardized at the syntactic level, including Ham-leDT (Zeman et al, 2012) and the Universal De-pendency Treebanks (McDonald et al, 2013).In this paper we also rely on these resources,but with a different goal: we use universal anno-tations to train bilingual dependency parsers thateffectively analyze unseen sentences in any ofthe learned languages.
Unlike delexicalized ap-proaches for model transfer, our parsers exploitlexical features.
The results are encouraging: ourexperiments show that, starting with a monolin-gual parser, we can ?teach?
it an additional lan-guage for free in terms of accuracy (i.e., withoutsignificant accuracy loss on the original language,in spite of learning a more complex task) in thevast majority of cases.2 Bilingual trainingUniversal Dependency Treebanks v2.0 (McDon-ald et al, 2013) is a set of CoNLL-formatted tree-banks for ten languages, annotated with commoncriteria.
They include two versions of PoS tags:universal tags (Petrov et al, 2011) in the CPOSTAGcolumn, and a refined annotation with treebank-specific information in the POSTAG column.
Someof the latter tags are not part of the core univer-sal set, and they can denote linguistic phenomenathat are language-specific, or phenomena that notall the corpora have annotated in the same way.To train monolingual parsers (our baseline), weused the official training-dev-set splits provided425with the corpora.
For the bilingual models, foreach pair of languages L1, L2; we simply mergedtheir training sets into a single file acting as a train-ing set for L1?L2, and we did the same for the de-velopment sets.
The test sets were not merged be-cause comparing the bilingual parsers to monolin-gual ones requires evaluating each bilingual parseron the two corresponding monolingual test sets.To build the models, we relied on MaltParser(Nivre et al, 2007).
Due to the large number oflanguage pairs that complicates manual optimiza-tion, and to ensure a fair comparison, we usedMaltOptimizer (Ballesteros and Nivre, 2012), anautomatic optimizer for MaltParser models.
Thissystem works in three phases: Phase 1 and 2choose a parsing algorithm by analyzing the train-ing set, and performing experiments with defaultfeatures.
Phase 3 tunes the feature model andalgorithm parameters.
We hypothesize that thebilingual models will learn a set of features thatfits both languages, and check this hypothesis byevaluating on the test sets.We propose two training configurations: (1) atreebank-dependent tags configuration where weinclude the information in the POSTAG columnand (2) a universal tags only configuration, wherewe do not use this information, relying only onthe CPOSTAG column.
Information that could bepresent in FEATS or LEMMA columns is not usedin any case.
This methodology plans to answertwo research questions: (1) can we train bilingualparsers with good accuracy by merging harmo-nized training sets?, and (2) is it essential that thetagsets for both languages are the same, or can westill get accuracy gains from fine-grained PoS tags(as in the monolingual case) even if some of themare treebank-specific?All models are freely available.13 EvaluationTo ensure a fair comparison between monolingualand bilingual models, we chose to optimize themodels from scratch with MaltOptimizer, expect-ing it to choose the parsing algorithm and featuremodel which is most likely to obtain good results.We observed that the selection of a bilingual pars-ing algorithm was not necessarily related with thealgorithms selected for the monolingual models.The system sometimes chose an algorithm for abilingual model that was not selected for any of1http://grupolys.org/software/PARSERS/the corresponding monolingual models.In view of this, and as it is known that differentparsing algorithms can be more or less competitivedepending on the language (Nivre, 2008), we rana control experiment to evaluate the models set-ting the same parsing algorithm for all cases, exe-cuting only phase 3 of MaltOptimizer.
We chosethe arc-eager parser for this experiment, as it wasthe algorithm that MaltOptimizer chose most fre-quently for the monolingual models in the previ-ous configuration.
The aim was to compare theaccuracy of the bilingual models with respect tothe monolingual ones, when there is no variationon the parsing algorithm between them.
The re-sults of this control experiment are not shown forspace reasons, but they were very similar to thoseof the original experiment.3.1 Results on the Universal TreebanksTable 1 compares the accuracy of bilingual modelsto that of monolingual ones, under the treebank-dependent tags configuration.
Each table cellshows the accuracy of a model, in terms of LASand UAS.
Cells in the diagonal correspond tomonolingual models (the baseline), with the celllocated at row i and column i representing the re-sult obtained by training a monolingual parser onthe training set of language Li, and evaluating iton the test set of the same language Li.
Each celloutside the diagonal (at row i and column j, withj 6= i) shows the results of training a bilingualmodel on the training set for Li?Lj, evaluated onthe test set of Li.As we can see, in a large majority of cases,bilingual parsers learn to parse two languages withno statistically significant accuracy loss with re-spect to the corresponding monolingual parsers(p < 0.05 with Bikel?s randomized parsing eval-uation comparator).
This happened in 74 out of90 cases when measuring UAS, or 69 out of 90 interms of LAS.
Therefore, in most cases where weare applying a parser to texts in a given language,adding a second language comes for free in termsof accuracy.More strikingly, there are many cases wherebilingual parsers outperform monolingual ones,even in this evaluation on purely monolingualdatasets.
In particular, there are 12 cases wherea bilingual parser obtains statistically significantgains in LAS over the monolingual baseline, and 9cases with significant gains in UAS.
This clearly426de en es fr id it ja ko pt-br svde78.27 78.01?77.82?77.83?77.84?78.10?77.86?77.94?78.13?78.60+84.03 84.08+83.82?83.55?
?83.85?84.12+83.88?83.63?83.87?84.38+en89.37+89.36 89.46+89.38+89.69++89.82++89.43+89.63++89.60++89.11?
?91.02+91.02 91.09+91.06+91.32++91.47++91.10+91.32++91.24+90.79?
?es80.85+81.08++80.60 80.95+81.16+80.92+81.41++81.49++79.96?81.26++85.17+85.27++84.75 85.15+85.00+85.13+85.52++85.39++84.70?85.42++fr79.01?79.39+79.36+79.29 79.61+79.34+79.16?79.36+79.09?79.66+84.17?84.49+84.56+84.47 84.32?84.41?84.34?84.72+83.98?84.84+id75.72?
?77.19?77.12?77.15?77.69 78.29+77.60?76.68??77.45?77.01??81.73??82.66?
?82.72?82.66?83.38 84.09+83.18?82.16??82.96?82.59??it82.62??83.17??83.12??83.10??83.74?
?84.40 84.62+84.79+83.70?84.55+86.14??86.46??86.78?86.69?86.73?
?87.54 87.48?87.46?87.39?87.23?ja76.53??76.24??76.61??76.32??75.18?
?77.05?77.46 76.89?76.69?76.89?83.77?83.89?84.26?84.05??83.08?
?83.97?84.34 83.65?83.97?84.17?ko86.13??88.30+87.91+88.49+85.86??88.72++87.14?
?87.83 86.75??88.68?90.61??92.16+92.00?92.35+90.19?
?92.55+91.89?92.12 91.39?
?92.39?pt-br84.83?85.06+84.99+84.97+85.10+85.43++84.95+85.12+84.88 85.25++87.18?87.19?87.27+87.17?87.35?87.68++87.13?87.35?87.39 87.43++sv81.71??82.01??82.03?81.92??82.34?82.63+82.81+82.94++82.19?82.4886.01??86.39?86.55?86.28?
?86.69?86.55?86.92+86.83?86.39?86.92Table 1: Performance on the Universal Dependency Treebanks test sets using the gold POSTAG information.
For each cell,its (row,column) pair indicates the language(s) with which the model was trained, with the row corresponding to the languagewhere it was evaluated.
?
++ ?
and ?
+ ?
indicate that the improvement in performance obtained by the bilingual model isstatistically significant or not, respectively.
?
- - ?
and ?
- ?
correspond to significant and not significant decreases in accuracy.de en es fr id it ja ko pt-br svde74.07 72.04?
?74.51+74.44+73.68?73.76?73.90?74.30+74.29+74.76++79.77 77.52?
?79.95+79.83+79.24?79.44?79.83+79.76?79.71?80.25+en88.46+88.35 88.65++88.39+88.61++88.68++88.65++88.61++88.65++88.50+90.35+90.27 90.54++90.26?90.47++90.53++90.49++90.43++90.55++90.43++es79.66??78.78?
?80.54 79.59??78.98??79.84??79.59??79.80??79.74??79.09??83.81??82.94?
?84.35 83.26??82.79??83.79??83.53??83.57?83.76??83.28?
?fr78.43+78.10?78.63+78.40 77.79?78.60+79.11+78.22?78.56+78.83+83.26?82.77?83.38?83.40 82.85?83.50+84.03+83.05?83.45+83.73+id74.46??74.65??77.09??76.23?
?78.31 77.86?77.10??75.58??76.90??78.34+80.87??80.21??82.81??81.78?
?83.81 83.52?82.68??81.20??82.50??83.83+it82.27??82.13??82.24??82.75??82.65?
?83.88 83.04??83.77?83.07??83.47?85.40??85.38??85.36??86.31??85.45?
?86.68 85.83??86.30?86.21??86.33?ja69.41??68.88??69.28??69.24??69.73??70.22?
?70.87 69.73??69.24??70.02?79.62??79.21??79.45??80.11??79.58??79.58?
?81.16 80.23?79.37??80.47??ko84.40??84.82??85.40??84.59??84.74??86.79?86.21?
?87.52 86.29??86.40??89.61??90.00??90.77??89.88??90.00??91.39?91.46?
?92.00 90.92??91.19??pt-br83.40?82.76??83.56?83.72?83.08?
?83.95+83.80?84.16++ 83.83 84.28++85.78?85.01??85.82?85.85?85.38?
?86.15+85.93?86.33+86.11 86.41++sv79.65??79.61??79.75??80.46?80.94+81.06+81.19+81.11+80.89?80.9384.14??84.42??84.46?
?84.88?85.14?85.51+85.29?85.14?85.05?85.32Table 2: Performance on the Universal Dependency Treebanks test sets using the gold CPOSTAG information.
The table islaid out like Table 1.surpasses the amount of significant gains to be ex-pected by chance, and applying the Benjamini-Hochberg procedure (Benjamini and Hochberg,1995) to correct for multiple comparisons with amaximum false discovery rate of 20% yields 8 sig-nificant improvements in LAS and UAS.
Therefore,it is clear that there is synergy between datasets:in some cases, adding annotated data in a differentlanguage to our training set can actually improvethe accuracy that we obtain in the original lan-guage.
This opens up interesting research poten-tial in using confidence criteria to select the datathat can help parsing in this way, akin to whatis done in self-training approaches (Chen et al,2008; Goutam and Ambati, 2011).Comparing the results by language, we note thatthe accuracy on the English and Spanish datasetsalmost always improves when adding a secondtreebank for training.
Other languages that tendto get improvements in this way are French andPortuguese.
There seems to be a rough trend to-wards the languages with the largest training cor-pora benefiting from adding a second language,and those with the smallest corpora (e.g.
Indone-sian, Italian or Japanese) suffering accuracy loss,likely because the training gets biased towards thesecond language.Training bilingual models containing a sig-nificant number of non-overlapping treebank-dependent tags tends to have a positive effect.
En-427glish and Spanish are two of the clearest exam-ples of this.
As shown in Table 3, which shows acomplete report of shared PoS tags for each pair oflanguages under the treebank-dependent tags con-figuration, English only shares 1 PoS tag with therest of the corpora under the said configuration,except for Swedish, with up to 5 tags in common;and the en-sv model is the only one suffering asignificant loss on the English test set.
Similar be-havior is observed on Spanish: sv (0), en (1), ja(10) and ko (12) are the four languages with fewestshared PoS tags, and those are the four that ob-tained a significant improvement on the Spanishevaluation; while with pt-br, with 15 shared PoStags, we lose accuracy.
The validity of this hy-pothesis is reinforced by an experiment where wedifferentiate the universal tags by language by ap-pending a language code to them (e.g.
EN NOUNfor an English noun).
An overall improvement wasobserved with respect to the bilingual parsers withnon-disjoint sets of features.de en es fr id it ja ko pt-br svde 16 1 14 14 14 13 10 12 14 0en 45 1 1 1 1 1 1 1 5es 24 14 14 13 10 12 15 0fr 14 14 13 10 12 14 0id 14 13 10 12 14 0it 13 10 12 13 0ja 763 10 10 0ko 20 12 0pt-br 15 0sv 25Table 3: Shared language-specific tags between pairs oflanguagesWhile all these experiments have been per-formed on sentences with gold PoS tags, prelim-inary experiments assuming predicted tags insteadshow analogous results: the absolute values ofLAS and UAS are slightly smaller across the board,but the behavior in relative terms is the same, andthe bilingual models that improved over the mono-lingual baseline in the gold experiments keep do-ing so under this setting.On the other hand, Table 2 shows the perfor-mance of the monolingual and bilingual modelsunder the universal tags only configuration.
Thebilingual parsers are also able to keep an ac-ceptable accuracy with respect to the monolin-gual models, but significant losses are much moreprevalent than under the treebank-dependent tagsconfiguration.Putting both tables together, our experimentsclearly suggest that not only treebank-specific tagsdo not impair the training of bilingual models, butthey are even beneficial, supporting the idea thatusing partially treebank-dependent tagsets helpsmultilingual parsing.
We hypothesize that thismay be because complementing the universal in-formation at the syntactic level with language-specific information at the lower levels (lexicaland morphological) may help the parser identifyspecific constructions of one language that wouldnot benefit from the knowledge learned from theother, preventing it from trying to exploit spuri-ous similarities between languages.
This explana-tion is coherent with work on delexicalized parsertransfer (Lynn et al, 2014) showing that better re-sults can be obtained using disparate languagesthan closely-related languages, as long as theyhave common syntactic constructions.
Thus, us-ing universal PoS tags to train multilingual parserscan be, surprisingly, counterproductive.3.2 Parsing code-switched sentencesOur bilingual parsers also show robustnesson texts exhibiting code-switching.
Unfortu-nately, there are no syntactically annotated code-switching corpora, so we could not perform a for-mal evaluation.
We did perform informal tests,by running the Spanish-English bilingual parserson some such sentences.
We observed that theywere able to parse the English and Spanish partsof the sentences much better than monolingualmodels.
This required training a bilingual tag-ger, which we did with the free distribution of theStanford tagger (Toutanova and Manning, 2000);merging the Spanish and English corpora to traina combined bilingual tagger.
Under the univer-sal tags only configuration, the multilingual tag-ger obtained 98.00% and 95.88% over the mono-lingual test sets.
Using treebank-dependent tagsinstead, it obtained 97.19% and 93.88% over themonolingual test sets.
Figure 1 shows an interest-ing example on how using bilingual parsers (andtaggers) affects the parsing accuracy.Table 4 shows the performance on a tiny code-switching treebank built on top of ten normalizedtweets.2This confirms that monolingual pipelinesperform poorly.
Using a bilingual tagger helps im-prove the performance, thanks to accurate tags forboth languages, but a bilingual parser is needed to2The code-switching treebank follows the Universal Tree-bank v2.0 annotations.
It can be obtained by asking any of theauthors.428Figure 1: Example with the en, es, en-es models.
Dotted lines represent incorrectly-parsed dependencies.
The correspondingEnglish sentence is: ?We are working hard on putting available the best products of Spain, thank you?Tagger Parser LAS UASen en 37.82 44.23es es 27.56 41.03en-es en 66.03 78.85en-es es 67.95 77.56en-es en-es 87.18 92.31Table 4: Performance on a code-switching treebank com-posed of 10 sentences.push both LAS and UAS up to state-of-the-art lev-els.3.3 Adding more languagesTo show that our approach works when morelanguages are added, we created a quadrilingualparser using the romanic languages and the finePoS tag set.
The results (LAS/UAS) on the mono-lingual sets were: 80.18/84.64 (es), 79.11/84.29(fr), 82.16/86.15 (it) and 84.45/86.80 (pt).
In allcases, the performance is almost equivalent to themonolingual parser.Noah?s ARK group (Ammar et al, 2016) hasshown that this idea can be also adapted to univer-sal parsing.
Our models are a collection of weightslearned from mixing harmonized treebanks, thataccurately analyze sentences in any of the learnedlanguages and where it is possible to take advan-tage of linguistic universals, but they are still de-pendent on language-specific word forms.
Instead,Ammar et al (2016) rely on multilingual wordclusters and multilingual word embeddings, learn-ing a universal representation.
They also supportincorporating language-specific information (e.g.PoS tags) to keep learning language-specific be-havior.
To address syntactic differences betweenlanguages (e.g.
noun-adjective vs adjective-nounstructure) they can inform the parser about the in-put language.4 Conclusions and future workTo our knowledge, this is the first attempt to trainpurely bilingual parsers to analyze sentences irre-spective of which of the two languages they arewritten in; as existing work on training a parseron two languages (Smith and Smith, 2004) fo-cused on using parallel corpora to transfer linguis-tic knowledge between languages.Our results reflect that bilingual parsers do notlose accuracy with respect to monolingual parserson their corresponding language, and can evenoutperform them, especially if fine-grained tagsare used.
This shows that, thanks to universal de-pendencies and shared syntactic structures acrossdifferent languages, using treebank-dependent tagsets is not a drawback, but even an advantage.The applications include parsing sentences ofdifferent languages with a single model, improv-ing the accuracy of monolingual parsing withtraining sets from other languages, and success-fully parsing sentences exhibiting code-switching.As future work, our approach could bene-fit from simple domain adaptation techniques(Daume?
III, 2009), to enrich the training set fora target language by incorporating data from asource language.5 AcknowledgmentsThis research is supported by the Ministeriode Econom?
?a y Competitividad (FFI2014-51978-C2).
David Vilares is funded by the Ministeriode Educacio?n, Cultura y Deporte (FPU13/01180).Carlos Go?mez-Rodr?
?guez is funded by an Opor-tunius program grant (Xunta de Galicia).
Wethank Marcos Garcia for helping with the code-switching treebank.
We also thank the reviewersfor their comments and suggestions.429ReferencesW.
Ammar, G. Mulcaire, M. Ballesteros, C. Dyer, andN.
A. Smith.
2016.
Many languages, one parser.arXiv preprint arXiv:1602.01595.M.
Ballesteros and J. Nivre.
2012.
MaltOptimizer:an optimization tool for MaltParser.
In Proceed-ings of the Demonstrations at the 13th Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 58?62.
Association forComputational Linguistics.Y.
Benjamini and Y. Hochberg.
1995.
Controllingthe false discovery rate: a practical and powerfulapproach to multiple testing.
Journal of the RoyalStatistical Society.
Series B (Methodological), pages289?300.W.
Chen, Y. Wu, and H. Isahara.
2008.
Learningreliable information for dependency parsing adap-tation.
In Proceedings of the 22Nd InternationalConference on Computational Linguistics - Volume1, COLING ?08, pages 113?120, Stroudsburg, PA,USA.
Association for Computational Linguistics.Y.
Dang, Y. Zhang, Paul J. Hu, S. A.
Brown, Y. Ku,J.
Wang, and H. Chen.
2014.
An integrated frame-work for analyzing multilingual content in Web 2.0social media.
Decision Support Systems, 61:126?135.H.
Daume?
III.
2009.
Frustratingly easy domain adap-tation.
arXiv preprint arXiv:0907.1815.R.
Goutam and B. R. Ambati.
2011.
Exploring selftraining for Hindi dependency parsing.
In Proceed-ings of 5th International Joint Conference on Natu-ral Language Processing, pages 1452?1456, ChiangMai, Thailand, November.
Asian Federation of Nat-ural Language Processing.T.
Lynn, J.
Foster, M. Dras, and L. Tounsi.
2014.Cross-lingual transfer parsing for low-resourced lan-guages: An Irish case study.
In CLTW 2014.
TheFirst Celtic Language Technology Workshop.
Pro-ceedings of the Workshop, pages 41?49, Dublin, Ire-land.R.
McDonald, S. Petrov, and K. Hall.
2011.
Multi-source transfer of delexicalized dependency parsers.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pages62?72.
Association for Computational Linguistics.R.
McDonald, J. Nivre, Y. Quirmbach-brundage,Y.
Goldberg, D. Das, K. Ganchev, K. Hall, S. Petrov,Hao Zhang, O. Ta?ckstro?m, C. Bedini, N. Castello?,and J. Lee.
2013.
Universal dependency annota-tion for multilingual parsing.
In Proceedings of the51st Annual Meeting of the Association for Com-putational Linguistics, pages 92?97.
Association forComputational Linguistics.T.
Naseem, R. Barzilay, and A. Globerson.
2012.
Se-lective sharing for multilingual dependency parsing.In Proceedings of the 50th AnnualMeeting of the As-sociation for Computational Linguistics, pages 629?637.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
Ku?bler, S. Marinov, and E. Marsi.
2007.
Malt-Parser: A language-independent system for data-driven dependency parsing.
Natural Language En-gineering, 13(2):95?135.J.
Nivre.
2006.
Two strategies for text parsing.In Mickael Suominen, Antti Arppe, Anu Airola,Orvoki Heina?ma?ki, Matti Miestamo, Urho Ma?a?tta?,Jussi Niemi, Kari K. Pitka?nen, and Kaius Sin-nema?ki, editors, A Man of Measure: Festschriftin Honour of Fred Karlsson on his 60th Birthday,pages 440?448.
A special supplement to SKY Jour-nal of Linguistics 19.J.
Nivre.
2008.
Algorithms for deterministic incremen-tal dependency parsing.
Computational Linguistics,34(4):513?553.S.
Petrov, D. Das, and R. McDonald.
2011.
Auniversal part-of-speech tagset.
arXiv preprintarXiv:1104.2086.D.
A. Smith and N. A. Smith.
2004.
Bilingual Parsingwith Factored Estimation: Using English to ParseKorean.
In Dekang Lin and Dekai Wu, editors, Pro-ceedings of EMNLP 2004, pages 49?56, Barcelona,Spain, July.
Association for Computational Linguis-tics.J.
Tiedemann.
2014.
Rediscovering annotation pro-jection for cross-lingual parser induction.
In Pro-ceedings of COLING 2014: 25th International Con-ference on Computational Linguistics, pages 1854?1864.K.
Toutanova and C. D Manning.
2000.
Enriching theknowledge sources used in a maximum entropy part-of-speech tagger.
In Proceedings of the 2000 JointSIGDAT conference on Empirical methods in nat-ural language processing and very large corpora:held in conjunction with the 38th Annual Meetingof the Association for Computational Linguistics-Volume 13, pages 63?70.D.
Yarowsky, G. Ngai, and R. Wicentowski.
2001.Inducing multilingual text analysis tools via robustprojection across aligned corpora.
In Proceedingsof the 1st International Conference on Human Lan-guage Technology Research, pages 1?8.
Associationfor Computational Linguistics.D.
Zeman and P. Resnik.
2008.
Cross-language parseradaptation between related languages.
In IJCNLP2008 Workshop on NLP for Less Privileged Lan-guages, pages 35?42, Hyderabad, India.
Interna-tional Institute of Information Technology.D.
Zeman, D. Marec?ek, M. Popel, L.
Ramasamy,J.
?Ste?pa?nek, Z.?Zabokrtsky?, and J. Hajic?.
2012.HamleDT: To Parse or Not to Parse?
In Nico-letta Calzolari (Conference Chair), Khalid Choukri,430Thierry Declerck, Mehmet Ug?ur Dog?an, BenteMaegaard, Joseph Mariani, Asuncion Moreno, JanOdijk, and Stelios Piperidis, editors, Proceedingsof the Eight International Conference on LanguageResources and Evaluation (LREC?12), Istanbul,Turkey, May.
European Language Resources Asso-ciation (ELRA).431
