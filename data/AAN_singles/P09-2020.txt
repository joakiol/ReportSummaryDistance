Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 77?80,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPA Syntactic and Lexical-Based Discourse SegmenterMilan TofiloskiSchool of Computing ScienceSimon Fraser UniversityBurnaby, BC, Canadamta45@sfu.caJulian BrookeDepartment of LinguisticsSimon Fraser UniversityBurnaby, BC, Canadajab18@sfu.caMaite TaboadaDepartment of LinguisticsSimon Fraser UniversityBurnaby, BC, Canadamtaboada@sfu.caAbstractWe present a syntactic and lexically baseddiscourse segmenter (SLSeg) that is de-signed to avoid the common problem ofover-segmenting text.
Segmentation is thefirst step in a discourse parser, a systemthat constructs discourse trees from el-ementary discourse units.
We compareSLSeg to a probabilistic segmenter, show-ing that a conservative approach increasesprecision at the expense of recall, while re-taining a high F-score across both formaland informal texts.1 Introduction?Discourse segmentation is the process of de-composing discourse into elementary discourseunits (EDUs), which may be simple sentences orclauses in a complex sentence, and from whichdiscourse trees are constructed.
In this sense, weare performing low-level discourse segmentation,as opposed to segmenting text into chunks or top-ics (e.g., Passonneau and Litman (1997)).
Sincesegmentation is the first stage of discourse parsing,quality discourse segments are critical to build-ing quality discourse representations (Soricut andMarcu, 2003).
Our objective is to construct a dis-course segmenter that is robust in handling bothformal (newswire) and informal (online reviews)texts, while minimizing the insertion of incorrectdiscourse boundaries.
Robustness is achieved byconstructing discourse segments in a principledway using syntactic and lexical information.Our approach employs a set of rules for insert-ing segment boundaries based on the syntax ofeach sentence.
The segment boundaries are thenfurther refined by using lexical information that?This work was supported by an NSERC Discovery Grant(261104-2008) to Maite Taboada.
We thank Angela Cooperand Morgan Mameni for their help with the reliability study.takes into consideration lexical cues, includingmulti-word expressions.
We also identify clausesthat are parsed as discourse segments, but are notin fact independent discourse units, and join themto the matrix clause.Most parsers can break down a sentence intoconstituent clauses, approaching the type of out-put that we need as input to a discourse parser.The segments produced by a parser, however, aretoo fine-grained for discourse purposes, breakingoff complement and other clauses that are not in adiscourse relation to any other segment.
For thisreason, we have implemented our own segmenter,utilizing the output of a standard parser.
The pur-pose of this paper is to describe our syntactic andlexical-based segmenter (SLSeg), demonstrate itsperformance against state-of-the-art systems, andmake it available to the wider community.2 Related WorkSoricut and Marcu (2003) construct a statisticaldiscourse segmenter as part of their sentence-leveldiscourse parser (SPADE), the only implemen-tation available for our comparison.
SPADE istrained on the RST Discourse Treebank (Carlsonet al, 2002).
The probabilities for segment bound-ary insertion are learned using lexical and syntac-tic features.
Subba and Di Eugenio (2007) useneural networks trained on RST-DT for discoursesegmentation.
They obtain an F-score of 84.41%(86.07% using a perfect parse), whereas SPADEachieved 83.1% and 84.7% respectively.Thanh et al (2004) construct a rule-basedsegmenter, employing manually annotated parsesfrom the Penn Treebank.
Our approach is con-ceptually similar, but we are only concerned withestablished discourse relations, i.e., we avoid po-tential same-unit relations by preserving NP con-stituency.773 Principles For Discourse SegmentationOur primary concern is to capture interesting dis-course relations, rather than all possible relations,i.e., capturing more specific relations such as Con-dition, Evidence or Purpose, rather than more gen-eral and less informative relations such as Elabo-ration or Joint, as defined in Rhetorical StructureTheory (Mann and Thompson, 1988).
By having astricter definition of an elementary discourse unit(EDU), this approach increases precision at the ex-pense of recall.Grammatical units that are candidates for dis-course segments are clauses and sentences.
Ourbasic principles for discourse segmentation followthe proposals in RST as to what a minimal unitof text is.
Many of our differences with Carl-son and Marcu (2001), who defined EDUs for theRST Discourse Treebank (Carlson et al, 2002),are due to the fact that we adhere closer to the orig-inal RST proposals (Mann and Thompson, 1988),which defined as ?spans?
adjunct clauses, ratherthan complement (subject and object) clauses.
Inparticular, we propose that complements of at-tributive and cognitive verbs (He said (that)..., Ithink (that)...) are not EDUs.
We preserve con-sistency by not breaking at direct speech (?X,?
hesaid.).
Reported and direct speech are certainlyimportant in discourse (Prasad et al, 2006); we donot believe, however, that they enter discourse re-lations of the type that RST attempts to capture.In general, adjunct, but not complement clausesare discourse units.
We require all discourse seg-ments to contain a verb.
Whenever a discourseboundary is inserted, the two newly created seg-ments must each contain a verb.
We segment coor-dinated clauses (but not coordinated VPs), adjunctclauses with either finite or non-finite verbs, andnon-restrictive relative clauses (marked by com-mas).
In all cases, the choice is motivated bywhether a discourse relation could hold betweenthe resulting segments.4 ImplementationThe core of the implementation involves the con-struction of 12 syntactically-based segmentationrules, along with a few lexical rules involving a listof stop phrases, discourse cue phrases and word-level parts of speech (POS) tags.
First, paragraphboundaries and sentence boundaries using NIST?ssentence segmenter1 are inserted.
Second, a sta-tistical parser applies POS tags and the sentence?ssyntactic tree is constructed.
Our syntactic rulesare executed at this stage.
Finally, lexical rules,as well as rules that consider the parts-of-speechfor individual words, are applied.
Segment bound-aries are removed from phrases with a syntacticstructure resembling independent clauses that ac-tually are used idiomatically, such as as it standsor if you will.
A list of phrasal discourse cues(e.g., as soon as, in order to) are used to insertboundaries not derivable from the parser?s output(phrases that begin with in order to... are tagged asPP rather than SBAR).
Segmentation is also per-formed within parentheticals (marked by paren-theses or hyphens).5 Data and Evaluation5.1 DataThe gold standard test set consists of 9 human-annotated texts.
The 9 documents include 3 textsfrom the RST literature2, 3 online product reviewsfrom Epinions.com, and 3 Wall Street Journal ar-ticles taken from the Penn Treebank.
The texts av-erage 21.2 sentences, with the longest text having43 sentences and the shortest having 6 sentences,for a total of 191 sentences and 340 discourse seg-ments in the 9 gold-standard texts.The texts were segmented by one of the au-thors following guidelines that were establishedfrom the project?s beginning and was used as thegold standard.
The annotator was not directly in-volved in the coding of the segmenter.
To ensurethe guidelines followed clear and sound principles,a reliability study was performed.
The guidelineswere given to two annotators, both graduate stu-dents in Linguistics, that had no direct knowledgeof the project.
They were asked to segment the 9texts used in the evaluation.Inter-annotator agreement across all three anno-tators using Kappa was .85, showing a high levelof agreement.
Using F-score, average agreementof the two annotators against the gold standard wasalso high at .86.
The few disagreements were pri-marily due to a lack of full understanding of theguidelines (e.g., the guidelines specify to break ad-junct clauses when they contain a verb, but oneof the annotators segmented prepositional phrases1http://duc.nist.gov/duc2004/software/duc2003.breakSent.tar.gz2Available from the RST website http://www.sfu.ca/rst/78Epinions Treebank Original RST Combined TotalSystem P R F P R F P R F P R FBaseline .22 .70 .33 .27 .89 .41 .26 .90 .41 .25 .80 .38SPADE (coarse) .59 .66 .63 .63 1.0 .77 .64 .76 .69 .61 .79 .69SPADE (original) .36 .67 .46 .37 1.0 .54 .38 .76 .50 .37 .77 .50Sundance .54 .56 .55 .53 .67 .59 .71 .47 .57 .56 .58 .57SLSeg (Charniak) .97 .66 .79 .89 .86 .87 .94 .76 .84 .93 .74 .83SLSeg (Stanford) .82 .74 .77 .82 .86 .84 .88 .71 .79 .83 .77 .80Table 1: Comparison of segmentersthat had a similar function to a full clause).
Withhigh inter-annotator agreement (and with any dis-agreements and errors resolved), we proceeded touse the co-author?s segmentations as the gold stan-dard.5.2 EvaluationThe evaluation uses standard precision, recall andF-score to compute correctly inserted segmentboundaries (we do not consider sentence bound-aries since that would inflate the scores).
Precisionis the number of boundaries in agreement with thegold standard.
Recall is the total number of bound-aries correct in the system?s output divided by thenumber of total boundaries in the gold standard.We compare the output of SLSeg to SPADE.Since SPADE is trained on RST-DT, it inserts seg-ment boundaries that are different from what ourannotation guidelines prescribe.
To provide a faircomparison, we implement a coarse version ofSPADE where segment boundaries prescribed bythe RST-DT guidelines, but not part of our seg-mentation guidelines, are manually removed.
Thisversion leads to increased precision while main-taining identical recall, thus improving F-score.In addition to SPADE, we also used the Sun-dance parser (Riloff and Phillips, 2004) in ourevaluation.
Sundance is a shallow parser whichprovides clause segmentation on top of a basicword-tagging and phrase-chunking system.
SinceSundance clauses are also too fine-grained for ourpurposes, we use a few simple rules to collapseclauses that are unlikely to meet our definition ofEDU.
The baseline segmenter in Table 1 insertssegment boundaries before and after all instancesof S, SBAR, SQ, SINV, SBARQ from the syntac-tic parse (text spans that represent full clauses ableto stand alone as sentential units).
Finally, twoparsers are compared for their effect on segmenta-tion quality: Charniak (Charniak, 2000) and Stan-ford (Klein and Manning, 2003).5.3 Qualitative ComparisonComparing the outputs of SLSeg and SPADE onthe Epinions.com texts illustrates key differencesbetween the two approaches.
[Luckily we bought the extended pro-tection plans from Lowe?s,] # [so weare waiting] [for Whirlpool to decide][if they want to do the costly repair] [orprovide us with a new machine].In this example, SLSeg inserts a single bound-ary (#) before the word so, whereas SPADE in-serts four boundaries (indicated by square brack-ets).
Our breaks err on the side of preserving se-mantic coherence, e.g., the segment for Whirlpoolto decide depends crucially on the adjacent seg-ments for its meaning.
In our opinion, the rela-tions between these segments are properly the do-main of a semantic, but not a discourse, parser.
Aclearer example that illustrates the pitfalls of fine-grained discourse segmenting is shown in the fol-lowing output from SPADE:[The thing] [that caught my attentionwas the fact] [that these fantasy novelswere marketed...]Because the segments are a restrictive relativeclause and a complement clause, respectively,SLSeg does not insert any segment boundaries.6 ResultsResults are shown in Table 1.
The combined in-formal and formal texts show SLSeg (using Char-niak?s parser) with high precision; however, ouroverall recall was lower than both SPADE and thebaseline.
The performance of SLSeg on the in-formal and formal texts is similar to our perfor-79mance overall: high precision, nearly identical re-call.
Our system outperforms all the other systemsin both precision and F-score, confirming our hy-pothesis that adapting an existing system wouldnot provide the high-quality discourse segmentswe require.The results of using the Stanford parser as analternative to the Charniak parser show that theperformance of our system is parser-independent.High F-score in the Treebank data can be at-tributed to the parsers having been trained on Tree-bank.
Since SPADE also utilizes the Charniakparser, the results are comparable.Additionally, we compared SLSeg and SPADEto the original RST segmentations of the threeRST texts taken from RST literature.
Performancewas similar to that of our own annotations, withSLSeg achieving an F-score of .79, and SPADEattaining .38.
This demonstrates that our approachto segmentation is more consistent with the origi-nal RST guidelines.7 DiscussionWe have shown that SLSeg, a conservative rule-based segmenter that inserts fewer discourseboundaries, leads to higher precision compared toa statistical segmenter.
This higher precision doesnot come at the expense of a significant loss inrecall, as evidenced by a higher F-score.
Unlikestatistical parsers, our system requires no trainingwhen porting to a new domain.All software and data are available3.
Thediscourse-related data includes: a list of clause-like phrases that are in fact discourse markers(e.g., if you will, mind you); a list of verbs usedin to-infinitival and if complement clauses thatshould not be treated as separate discourse seg-ments (e.g., decide in I decided to leave the carat home); a list of unambiguous lexical cues forsegment boundary insertion; and a list of attribu-tive/cognitive verbs (e.g., think, said) used to pre-vent segmentation of floating attributive clauses.Future work involves studying the robustness ofour discourse segments on other corpora, such asformal texts from the medical domain and otherinformal texts.
Also to be investigated is a quan-titative study of the effects of high-precision/low-recall vs. low-precision/high-recall segmenters onthe construction of discourse trees.
Besides its usein automatic discourse parsing, the system could3http://www.sfu.ca/?mtaboada/research/SLSeg.htmlassist manual annotators by providing a set of dis-course segments as starting point for manual an-notation of discourse relations.ReferencesLynn Carlson and Daniel Marcu.
2001.
DiscourseTagging Reference Manual.
ISI Technical ReportISI-TR-545.Lynn Carlson, Daniel Marcu and Mary E. Okurowski.2002.
RST Discourse Treebank.
Philadelphia, PA:Linguistic Data Consortium.Eugene Charniak.
2000.
A Maximum-Entropy In-spired Parser.
Proc.
of NAACL, pp.
132?139.
Seat-tle, WA.Barbara J. Grosz and Candace L. Sidner.
1986.
At-tention, Intentions, and the Structure of Discourse.Computational Linguistics, 12:175?204.Dan Klein and Christopher D. Manning.
2003.
FastExact Inference with a Factored Model for Natu-ral Language Parsing.
Advances in NIPS 15 (NIPS2002), Cambridge, MA: MIT Press, pp.
3?10.William C. Mann and Sandra A. Thompson.
1988.Rhetorical Structure Theory: Toward a FunctionalTheory of Text Organization.
Text, 8:243?281.Daniel Marcu.
2000.
The Theory and Practice ofDiscourse Parsing and Summarization.
MIT Press,Cambridge, MA.Rebecca J. Passonneau and Diane J. Litman.
1997.Discourse Segmentation by Human and AutomatedMeans.
Computational Linguistics, 23(1):103?139.Rashmi Prasad, Nikhil Dinesh, Alan Lee, AravindJoshi and Bonnie Webber.
2006.
Attribution and itsAnnotation in the Penn Discourse TreeBank.
Traite-ment Automatique des Langues, 47(2):43?63.Ellen Riloff and William Phillips.
2004.
An Introduc-tion to the Sundance and AutoSlog Systems.
Univer-sity of Utah Technical Report #UUCS-04-015.Radu Soricut and Daniel Marcu.
2003.
Sentence LevelDiscourse Parsing Using Syntactic and Lexical In-formation.
Proc.
of HLT-NAACL, pp.
149?156.
Ed-monton, Canada.Rajen Subba and Barbara Di Eugenio.
2007.
Auto-matic Discourse Segmentation Using Neural Net-works.
Proc.
of the 11th Workshop on the Se-mantics and Pragmatics of Dialogue, pp.
189?190.Rovereto, Italy.Huong Le Thanh, Geetha Abeysinghe, and ChristianHuyck.
2004.
Automated Discourse Segmentationby Syntactic Information and Cue Phrases.
Proc.
ofIASTED.
Innsbruck, Austria.80
