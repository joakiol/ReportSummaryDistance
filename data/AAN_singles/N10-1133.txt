Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 903?911,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsQuantifying the Limits and Success of Extractive Summarization SystemsAcross DomainsHakan Ceylan and Rada MihalceaDepartment of Computer ScienceUniversity of North TexasDenton, TX 76203{hakan,rada}@unt.eduUmut ?OzertemYahoo!
Labs701 First AvenueSunnyvale, CA 94089umut@yahoo-inc.comElena Lloret and Manuel PalomarDepartment ofSoftware and Computing SystemsUniversity of AlicanteSan Vicente del RaspeigAlicante 03690, Spain{elloret,mpalomar}@dlsi.ua.esAbstractThis paper analyzes the topic identificationstage of single-document automatic text sum-marization across four different domains, con-sisting of newswire, literary, scientific and le-gal documents.
We present a study that ex-plores the summary space of each domainvia an exhaustive search strategy, and findsthe probability density function (pdf) of theROUGE score distributions for each domain.We then use this pdf to calculate the per-centile rank of extractive summarization sys-tems.
Our results introduce a new way tojudge the success of automatic summarizationsystems and bring quantified explanations toquestions such as why it was so hard for thesystems to date to have a statistically signifi-cant improvement over the lead baseline in thenews domain.1 IntroductionTopic identification is the first stage of the gener-ally accepted three-phase model in automatic textsummarization, in which the goal is to identify themost important units in a document, i.e., phrases,sentences, or paragraphs (Hovy and Lin, 1999; Lin,1999; Sparck-Jones, 1999).
This stage is followedby the topic interpretation and summary generationsteps where the identified units are further processedto bring the summary into a coherent, human read-able abstract form.
The extractive summarizationsystems, however, only employ the topic identifi-cation stage, and simply output a ranked list of theunits according to a compression ratio criterion.
Ingeneral, for most systems sentences are the preferredunits in this stage, as they are the smallest grammat-ical units that can express a statement.Since the sentences in a document are reproducedverbatim in extractive summaries, it is theoreticallypossible to explore the search space of this problemthrough an enumeration of all possible extracts fora document.
Such an exploration would not onlyallow us to see how far we can go with extractivesummarization, but we would also be able to judgethe difficulty of the problem by looking at the dis-tribution of the evaluation scores for the generatedextracts.
Moreover, the high scoring extracts couldalso be used to train a machine learning algorithm.However, such an enumeration strategy has anexponential complexity as it requires all possiblesentence combinations of a document to be gener-ated, constrained by a given word or sentence length.Thus the problem quickly becomes impractical asthe number of sentences in a document increases andthe compression ratio decreases.
In this work, we tryto overcome this bottleneck by using a large clusterof computers, and decomposing the task into smallerproblems by using the given section boundaries or alinear text segmentation method.
As a result of thisexploration, we generate a probability density func-tion (pdf) of the ROUGE score (Lin, 2004) distri-butions for four different domains, which shows thedistribution of the evaluation scores for the gener-ated extracts, and allows us to assess the difficultyof each domain for extractive summarization.Furthermore, using these pdfs, we introduce anew success measure for extractive summarizationsystems.
Namely, given a system?s average scoreover a data set, we show how to calculate the per-903centile rank of this system from the correspondingpdf of the data set.
This allows us to see the trueimprovement a system achieves over another, suchas a baseline, and provides a standardized scoringscheme for systems performing on the same data set.2 Related WorkDespite the large amount of work in automatictext summarization, there are only a few studiesin the literature that employ an exhaustive searchstrategy to create extracts, which is mainly due tothe prohibitively large search space of the prob-lem.
Furthermore, the research regarding the align-ment of abstracts to original documents has showngreat variations across domains (Kupiec et al, 1995;Teufel and Moens, 1997; Marcu, 1999; Jing, 2002;Ceylan and Mihalcea, 2009), which indicates thatthe extractive summarization techniques are not ap-plicable to all domains at the same level.In order to automate the process of corpusconstruction for automatic summarization systems,(Marcu, 1999) used exhaustive search to generatethe best Extract from a given (Abstract, Text) tuple,where the best Extract contains a set of clauses fromText that have the highest similarity to the given Ab-stract.In addition, (Donaway et al, 2000) used exhaus-tive search to create all the sentence extracts oflength three starting with 15 TREC Documents, inorder to judge the performance of several summaryevaluation measures suggested in their paper.Finally, the study most similar to ours was doneby (Lin and Hovy, 2003), who used the articles withless than 30 sentences from the DUC 2001 data setto find oracle extracts of 100 and 150 (?5) words.These extracts were compared against one summarysource, selected as the one that gave the highestinter-human agreement.
Although it was concludedthat a 10% improvement was possible for extrac-tive summarization systems, which typically scorearound the lead baseline, there was no report on howdifficult it would be to achieve this improvement,which is the main objective of our paper.3 Description of the Data SetOur data set is composed of four different domains:newswire, literary, scientific and legal.
For all theDomain ?Dw ?Sw ?R ?C ?CwNewswire 641 101 84% 1 641Literary 4973 1148 77% 6 196Scientific 1989 160 92% 9 221Legal 3469 865 75% 18 192Table 1: Statistical properties of the data set.
?Dw, and?Sw represent the average number of words for each doc-ument and summary respectively; ?R indicates the av-erage compression ratio; and ?C and ?Cw represent theaverage number of sections for each document, and theaverage number of words for each section respectively.domains we used 50 documents and only one sum-mary for each document, except for newswire wherewe used two summaries per document.
For thenewswire domain, we selected the articles and theirsummaries from the DUC 2002 data set,1.
For theliterary domain, we obtained 10 novels that are lit-erature classics, and available online in text format.Further, we collected the corresponding summariesfor these novels from various websites such asCliffsNotes (www.cliffsnotes.com) and SparkNotes(www.sparknotes.com), which make available hu-man generated abstracts for literary works.
Thesesources give a summary for each chapter of thenovel, so each chapter can be treated as a sepa-rate document.
Thus we evaluate 50 chapters in to-tal.
For the scientific domain, we selected the ar-ticles from the medical journal Autoimmunity Re-views2 were selected, and their abstracts are usedas summaries.
Finally, for the legal domain, wegathered 50 law documents and their correspondingsummaries from the European Legislation Website,3which comprises four types of laws - Council Di-rectives, Acts, Communications, and Decisions overseveral topics, such as society, environment, educa-tion, economics and employment.Although all the summaries are human generatedabstracts for all the domains, it is worth mention-ing that the documents and their corresponding sum-maries exhibit a specific writing style for each do-main, in terms of the vocabulary used and the lengthof the sentences.
We list some of the statistical prop-erties of each domain in Table 1.1http://www-nlpir.nist.gov/projects/duc/data.html2http://www.elsevier.com/wps/product/cws home/6223563http://eur-lex.europa.eu/en/legis/index.htm9044 Experimental SetupAs mentioned in Section 1, an exhaustive searchalgorithm requires generating all possible sentencecombinations from a document, and evaluating eachone individually.
For example, using the values fromTable 1, and assuming 20 words per sentence, wefind that the search space for the news domain con-tains approximately(325)?
50 = 10, 068, 800 sum-maries.
The same calculation method for the sci-entific domain gives us(998)?
50 = 8.56 ?
1012summaries.
Obviously the search space gets muchbigger for the legal and literary domains due to theirlarger text size.In order to be able to cope with such a hugesearch space, the first thing we did was to modifythe ROUGE 1.5.54 Perl script by fixing the parame-ters to those used in the DUC experiments,5 and alsoby modifying the way it handles the input and outputto make it suitable for streaming on the cluster.The resulting script evaluates around 25-30 sum-maries per second on an Intel 2.33 GHz processor.Next, we streamed the resulting ROUGE script foreach (document, summary) pair on a large clusterof computers running on an Hadoop Map-Reduceframework.6 Based on the size of the search spacefor a (document, summary) pair, the number of com-puters allocated in the cluster ranged from just a fewto more than one thousand.Although the combination of a large cluster and afaster ROUGE is enough to handle most of the doc-uments in the news domain in just a few hours, asimple calculation shows that the problem is still im-practical for the other domains.
Hence for the scien-tific, legal, and literary domains, rather than consid-ering each document as a whole, we divide them intosections, and create extracts for each section suchthat the length of the extract is proportional to thelength of the section in the original document.
Forthe legal and scientific domains, we use the givensection boundaries (without considering the subsec-tions for scientific documents).
For the novels, wetreat each chapter as a single document (since eachchapter has its own summary), which is further di-vided into sections using a publicly available linear4http://berouge.com5-n 2 -x -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 06http://hadoop.apache.org/text segmentation algorithm by (Utiyama and Isa-hara, 2001).7 In all cases, we let the algorithm pickthe number of segments automatically.To evaluate the sections, we modified ROUGEfurther so that it applies the length constraint to theextracts only, not to the model summaries.
This isdue to the fact that we evaluate the extracts of eachsection individually against the whole model sum-mary, which is larger than the extract.
This way,we can get an overall ROUGE recall score for adocument extract, simply by summing up the re-call scores of each section extracts.
The precisionscore for the entire document can also be found byadding the weighted precision scores for each sec-tion, where the weight is proportional to the lengthof the section in the original document.
In our study,however, we only use recall scores.Note that, since for the legal, scientific, and lit-erary domains we consider each section of a doc-ument independently, we are not performing a trueexhaustive search for these domains, but rather solv-ing a suboptimal problem, as we divide the numberof words in the model summary to each section pro-portional to the section?s length.
However, we be-lieve that this is a fair assumption, as it has beenshown repeatedly in the past that text segmentationhelps improving the performance of text summariza-tion systems (yen Kan et al, 1998; Nakao, 2000;Mihalcea and Ceylan, 2007).5 Exhaustive Search AlgorithmLet Eik = Si1 , Si2 , ..., Sik be the ith extract thathas k sentences, and generated from a documentD with n sentences D = S1, S2, .
.
.
, Sn.
Further,let len(Sj) give the number of words in sentenceSj .
We enforce that Eik satisfies the following con-straints:len(Eik) = len(Si1) + .
.
.
+ len(Sik) ?
Llen(Eik?1) = len(Si1) + .
.
.
+ len(Sik?1) < Lwhere L is the length constraint on all the extractsof document D. We note that for any Eik , the or-der of the sentences in Eik?1 does not affect theROUGE scores, since only the last sentence may be7http://mastarpj.nict.go.jp/ mutiyama/software/textseg/textseg-1.211.tar.gz905chopped off due to the length constraint.8 Hence, westart generating sentence combinations(nr)in lexico-graphic order, for r = 1...n, and for each combina-tion Eik = Si1 , Si2 , ..., Sik where k > 1, we gener-ate additional extracts E?ik by successfully swappingSij with Sik for j = 1, ..., k?
1 and checking to seeif the above constraints are still satisfied.
Thereforefrom a combination with k sentences that satisfiesthe constraints, we might generate up to k ?
1 ad-ditional extracts.
Finally, we stop the process eitherwhen r = n and the last combination is generated,or we cannot find any extract that satisfies the con-straints for r.6 Generating pdfsOnce the extracts for a document are generated andevaluated, we go through each result and assign itsrecall score to a range, which we refer to as a bin.We use 1, 000 equally spaced bins between 0 and1.
As an example, a recall score of 0.46873 wouldbe assigned to the bin [0.468, 0.469].
By keepinga count for each bin, we are in fact building a his-togram of scores for the document.
Let this his-togram be h, and h[j] be the value in the jth bin ofthe histogram.
We then define the normalized his-togram h?
as:h?
[j] =N?Ni=1 h[j]h[j] (1)where N = 1, 000 is the number of bins in the his-togram.
Note that since the width of each bin is 1N ,the Riemann sum of the normalized histogram h?
isequal to 1, so h?
can be used as an approximationto the underlying pdf.
As an example, we show thehistogram h?
for the newswire document AP890323-0218 in Figure 1.We combine the normalized histograms of all thedocuments in a domain in order to find the pdf forthat domain.
This requires multiplying the valueof each bin in a document?s histogram, with allthe other possible combinations of bin values takenfrom each of the remaining histograms, and assign-ing the result to the average bin for each combina-8Note that we do not take the coherence of extracts into ac-count, i.e.
the sentences in an extract do not need to be sortedin order of their appearance in the original document.
We alsodo not change the position of the words in a sentence.051015202530354045500  100  200  300  400  500  600  700  800  900  1000"AP890323-0218.dat"Figure 1: The normalized histogram h?
of ROUGE-1 re-call scores for the newswire document AP890323-0218.tion.
This can be done iteratively by keeping a mov-ing average.
We illustrate this procedure in Algo-rithm 1, where K represents the number of docu-ments in a domain.Algorithm 1 Combine h?i?s for i = 1, .
.
.
,K to cre-ate hd, the histogram for domain d.1: hd := {}2: for i = 1 to N do3: hd[i] := h?1[i]4: end for5: for i = 2 to K do6: ht = {}7: for j = 1 to N do8: for k = 1 to N do9: a = round(((k ?
(i?
1)) + j)/i)10: ht[a] = ht[a] + (hd[k] ?
h?i[j])11: end for12: end for13: hd := ht14: end forThe resulting histogram hd, when normalized us-ing Equation 1, is an approximation to the pdf fordomain d. Furthermore, we used the round() func-tion in line 9, which rounds a number to the nearestinteger, as the bins are indexed by integers.
Notethat this rounding introduces an error, which is dis-tributed uniformly due to the nature of the round()function.
It is also possible to lower the affect of thiserror with higher resolutions (i.e.
larger number ofbins).
In Figure 2, we show a sample hd, obtainedby combining 10 documents from the newswire do-906024681012141618200  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1"newswire_10-ROUGE-1.dat"Figure 2: An example pdf obtained by combining 10 doc-ument histograms of ROUGE-1 recall scores from thenewswire domain.
The x-axis is normalized to [0,1].main.Recall from Section 4 that the documents inthe literary, legal, and scientific domains are di-vided into sections either by using the given sectionboundaries or by applying a text segmentation al-gorithm, and the extracts of each section are thenevaluated individually.
Hence for these domains, wefirst calculate the histogram of each section individ-ually, and then combine them to find the histogramof a document.
The combination procedure for thesection histograms is similar to Algorithm 1, exceptthat in this case we do not keep a moving average,but rather sum up the bins of the sections.
Notethat when bin i and j are added, the resulting val-ues should be expected to be half the times in bini + j, and half the times in i + j ?
1.7 Calculating Percentile RanksGiven a pdf for a domain, the success of a systemhaving a ROUGE recall score of S could be sim-ply measured by finding the area bounded by S.This gives us the percentile rank of the system inthe overall distribution.
Assuming 0 ?
S ?
1, letS?
= ?N ?S?, then the formula to calculate the per-centile rank can be simply given as:PR(S) =100NbS?i=1h?d[i] (2)ROUGE-1Domain ?
?
max minNewswire 39.39 0.87 65.70 20.20Literary 45.20 0.47 63.90 28.40Scientific 45.99 0.68 71.90 24.20Legal 72.82 0.28 82.40 62.80ROUGE-2Domain ?
?
max minNewswire 11.57 0.79 37.40 1.60Literary 5.41 0.34 16.90 1.80Scientific 10.98 0.60 33.30 1.30Legal 28.74 0.29 40.90 19.60ROUGE-SU4Domain ?
?
max minNewswire 15.33 0.69 38.10 6.40Literary 13.28 0.30 24.30 6.90Scientific 16.13 0.50 35.80 6.20Legal 35.63 0.25 45.70 28.70Table 2: Statistical properties of the pdfs8 ResultsThe ensemble distributions of ROUGE-1 recallscores per document are shown in Figure 3.
Theensemble distributions tell us that the performanceof the extracts, especially for the news and the sci-entific domains, are mostly uniform for each docu-ment.
This is due to the fact that documents in thesedomains, and their corresponding summaries, arewritten with a certain conventional style.
There ishowever a little scattering in the distributions of theliterary and the legal domains.
This is an expectedresult for the literary domain, as there is no specificsummarization style for these documents, but some-how surprising for the legal domain, where the effectis probably due to the different types of legal docu-ments in the data set.The pdf plots resulting from the ROUGE-1 recallscores are shown in Figure 4.9 In order to analyzethe pdf plots, and better understand their differences,Table 2 lists the mean (?)
and the standard deviation(?)
measures of the pdfs, as well as the average min-imum and maximum scores that an extractive sum-marization system can get for each domain.By looking at the pdf plots and the minimum andmaximum columns from Table 2, we notice that for9Similar pdfs are obtained for ROUGE-2 and ROUGE-SU4,even if at a different scale.90700.10.20.30.40.50.60.70.80.90  5  10  15  20  25  30  35  40  45  50"Ensemble-Newswire-50-ROUGE-1.dat"00.10.20.30.40.50.60.70.80.910  5  10  15  20  25  30  35  40  45  50"Literary-50-Ensemble-ROUGE-1.dat"0.10.20.30.40.50.60.70.80.910  5  10  15  20  25  30  35  40  45  50"Medical-50-Ensemble-ROUGE-1.dat"0.30.40.50.60.70.80.910  5  10  15  20  25  30  35  40  45  50"Legal-50-Ensemble-ROUGE-1.dat"Figure 3: ROUGE-1 recall score distributions per document for Newswire, Literary, Scientific and Legal Domains,respectively from left to right.051015202530354045500.36  0.38  0.4  0.42  0.44"Newswire-50-ROUGE-1.dat"01020304050607080900.4  0.41  0.42  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.5"Literary-50-ROUGE-1.dat"0102030405060700.4  0.41  0.42  0.43  0.44  0.45  0.46  0.47  0.48  0.49  0.5"Medical-50-ROUGE-1.dat"0204060801001201401600.7  0.72  0.74  0.76  0.78  0.8"Legal-50-ROUGE-1.dat"Figure 4: Probability Density Functions of ROUGE-1 recall scores for the Newswire, Literary, Scientific and LegalDomains, respectively from left to right.
The resolution of the x-axis is increased to 0.1.all the domains, the pdfs are long-tailed distribu-tions.
This immediately implies that most of theextracts in a summary space are clustered aroundthe mean, which means that for automatic summa-rization systems, it is very easy to get scores aroundthis range.
Furthermore, we can judge the hardnessof each domain by looking at the standard devia-tion values.
A lower standard deviation indicates asteeper curve, which implies that improving a sys-tem would be harder.
From the table, we can in-fer that the legal domain is the hardest while thenewswire is the easiest.Comparing Table 2 with the values in Table 1,we also notice that the compression ratio affects theperformance differently for each domain.
For ex-ample, although the scientific domain has the high-est compression ratio, it has a higher mean thanthe literary and the newswire domains for ROUGE-1 and ROUGE-SU4 recall scores.
This impliesthat although the abstracts of the medical journalsare highly compressed, they have a high overlapwith the document, probably caused by their writ-ing style.
This was in fact confirmed earlier by theexperiments in (Kupiec et al, 1995), where it wasfound out that for a data set of 188 scientific arti-cles, 79% of the sentences in the abstracts could beperfectly matched with the sentences in the corre-sponding documents.Next, we confirm our experiments by testing threedifferent extractive summarization systems on ourdata set.
The first system that we implement is calledRandom, and gives a random score between 1 and100 to each sentence in a document, and then se-lects the top scoring sentences.
The second system,Lead, implements the lead baseline method whichtakes the first k sentences of a document until thelength limit is reached.
Finally, the last system thatwe implement is TextRank, which uses a variation ofthe PageRank graph centrality algorithm in order toidentify the most important sentences in a document(Page et al, 1999; Erkan and Radev, 2004; Mihalceaand Tarau, 2004).
We selected TextRank as it has aperformance competitive with the top systems par-ticipating in DUC ?02 (Mihalcea and Tarau, 2004).We would also like to mention that for the literary,scientific, and legal domains, the systems apply thealgorithms for each section and each section is eval-uated independently, and their resulting recall scoresare summed up.
This is needed in order to be con-sistent with our exhaustive search experiments.The ROUGE recall scores of the three systems areshown in Table 3.
As expected, for the literary andlegal domains, the Random, and the Lead systemsscore around the mean.
This is due to the fact thatthe leading sentences for these two domains do notindicate any significance, hence the Lead system justbehaves like Random.
However for the scientific andnewswire domains, the leading sentences do have908ROUGE-1Domain Random Lead TextRankNewswire 39.13 45.63 44.43Literary 45.39 45.36 46.12Scientific 45.75 47.18 49.26Legal 73.04 72.42 74.82ROUGE-2Domain Random Lead TextRankNewswire 11.39 19.60 17.99Literary 5.33 5.41 5.92Scientific 10.73 12.07 12.76Legal 28.56 28.92 31.06ROUGE-SU4Domain Random Lead TextRankNewswire 15.07 21.58 20.46Literary 13.21 13.28 13.81Scientific 15.92 17.12 17.85Legal 35.41 35.55 37.64Table 3: ROUGE recall scores of the Lead baseline, Tex-tRank, and Random sentence selector across domainsimportance so the Lead system consistently outper-forms Random.
Furthermore, although TextRank isthe best system for the literary, scientific, and legaldomains, it gets outperformed by the Lead systemon the newswire domain.
This is also an expected re-sult as none of the single-document summarizationsystems were able to achieve a statistically signifi-cant improvement over the lead baseline in the previ-ous Document Understanding Conferences (DUC).The ROUGE scoring scheme does not tell us howmuch improvement a system achieved over another,or how far it is from the upper bound.
Since we nowhave access to the pdf of each domain in our data set,we can find this information simply by calculatingthe percentile rank of each system using the formulagiven in Equation 2.The percentile ranks of all three systems for eachdomain are shown in Table 4.
Notice how differentthe gap is between the scores of each system thistime, compared to the scores in Table 3.
For ex-ample, we see in Table 3 that TextRank on scientificdomain has only a 3.51 ROUGE-1 score improve-ment over a system that randomly selects sentencesto include in the extract.
However, Table 4 tells usthat this improvement is in fact 57.57%.From Table 4, we see that both TextRank andthe Lead system are in the 99.99% percentile ofROUGE-1Domain Random Lead TextRankNewswire %39.18 %99.99 %99.99Literary %62.89 %62.89 %97.90Scientific %42.30 %95.56 %99.87Legal %79.47 %16.19 %99.99ROUGE-2Domain Random Lead TextRankNewswire %39.57 %99.99 %99.99Literary %42.20 %54.32 %94.34Scientific %35.6 %96.03 %99.79Legal %36.68 %75.38 %99.99ROUGE-SU4Domain Random Lead TextRankNewswire %40.68 %99.99 %99.99Literary %46.39 %46.39 %96.84Scientific %36.37 %97.69 %99.94Legal %23.53 %42.00 %99.99Table 4: Percentile rankings of the Lead baseline, Tex-tRank, and Random sentence selector across domainsthe newswire domain although the systems have1.20, 1.61, and 1.12 difference in their ROUGE-1,ROUGE-2, and ROUGE-SU4 scores respectively.The high percentile for the Lead system explainswhy it was so hard to improve over these baseline inprevious evaluations on newswire data (e.g., see theevaluations from the Document Understanding Con-ferences).
Furthermore, we see from Table 2 that theupper bounds corresponding to these scores are 65.7,37.4, and 38.1 respectively, which are well aboveboth the TextRank and the Lead systems.
There-fore, the percentile rankings of the Lead and the Tex-tRank systems for this domain do not seem to giveus clues about how the two systems compare to eachother, nor about their actual distance from the up-per bounds.
There are two reasons for this: First,as we mentioned earlier, most of the summary spaceconsists of easy extracts, which make the distribu-tion long-tailed.10 Therefore even though we havequite a bit of systems achieving high scores, theirnumber is negligible compared to the millions of ex-tracts that are clustered around the mean.
Secondly,we need a higher resolution (i.e.
larger number ofbins) in constructing the pdfs in order to be able to10This also accounts for the fact that even though we mighthave two very close ROUGE scores that are not statistically sig-nificant, their percentile rankings might differ quite a bit.909see the difference more clearly between the two sys-tems.
Finally, when comparing two successful sys-tems using percentile ranks, we believe the use oferror reduction would be more beneficial.As a final note, we also randomly sampled ex-tracts from documents in the scientific and legal do-mains, but this time without considering the sectionboundaries and without performing any segmenta-tion.
We kept the number of samples for each doc-ument equal to the number of extracts we generatedfrom the same document using a divide-and-conquerapproach.
We evaluated the samples using ROUGE-1 recall scores, and obtained pdfs for each domainusing the same strategy discussed earlier in the pa-per.
The resulting pdfs, although they exhibit simi-lar characteristics, they have mean values (?)
around10% lower than the ones we listed in Table 2, whichsupports the findings from earlier research that seg-mentation is useful for text summarization.9 Conclusions and Future WorkIn this paper, we described a study that explores thesearch space of extractive summaries across four dif-ferent domains.
For the news domain we generatedall possible extracts of the given documents, andfor the literary, scientific, and legal domains we fol-lowed a divide-and-conquer approach by chunkingthe documents into sections, handled each sectionindependently, and combined the resulting scores atthe end.
We then used the distributions of the eval-uations scores to generate the probability densityfunctions (pdfs) for each domain.
Various statisticalproperties of these pdfs helped us asses the difficultyof each domain.
Finally, we introduced a new scor-ing scheme for automatic text summarization sys-tems that can be derived from the pdfs.
The newscheme calculates a percentile rank of the ROUGE-1 recall score of a system, which gives scores in therange [0-100].
This lets us see how far each sys-tem is from the upper bound, and thus make a bettercomparison among the systems.
The new scoringsystem showed us that while there is a 20.1% gapbetween the upper bound and the lead baseline forthe news domain, closing this gap is difficult, as thepercentile rank of the lead baseline system, 99.99%,indicates that the system is already very close to theupper bound.Furthermore, except for the literary domain, thepercentile rank of the TextRank system is also veryclose to the upperbound.
This result does not sug-gest that additional improvements cannot be madein these domains, but that making further improve-ments using only extractive summarization will beconsiderably difficult.
Moreover, in order to seethese future improvements, a higher resolution (i.e.larger number of bins) will be needed when con-structing the pdfs.In all our experiments we used the ROUGE(Lin, 2004) evaluation package and its ROUGE-1, ROUGE-2, and ROUGE-SU4 recall scores.
Wewould like to note that since ROUGE performs itsevaluations based on the n-gram overlap betweenthe peer and the model summary, it does not takeother summary quality metrics such as coherenceand cohesion into account.
However, our goal in thispaper was to analyze the topic-identification stageonly, which concentrates on selecting the right con-tent from the document to include in the summary,and the ROUGE scores were found to correlate wellwith the human judgments on assessing the contentoverlap of summaries.In the future, we would like to apply a similar ex-haustive search strategy, but this time with differ-ent compression ratios, in order to see the impactof compression ratios on the pdf of each domain.Furthermore, we would also like to analyze thehigh scoring extracts found by the exhaustive search,in terms of coherence, position and other features.Such an analysis would allow us to see whether theseextracts exhibit certain properties which could beused in training machine learning systems.AcknowledgmentsThe authors would like to thank the anonymous re-viewers of NAACL-HLT 2010 for their feedback.The work of the first author has been partly sup-ported by an award from Google, Inc.
The work ofthe fourth and fifth authors has been supported by anFPI grant (BES-2007-16268) from the Spanish Min-istry of Science and Innovation, under the projectTEXT-MESS (TIN2006-15265-C06-01) funded bythe Spanish Government, and the project PROME-TEO Desarrollo de Tcnicas Inteligentes e Interacti-vas de Minera de Textos (2009/119) from the Valen-cian Government.910ReferencesHakan Ceylan and Rada Mihalcea.
2009.
The decompo-sition of human-written book summaries.
In CICLing?09: Proceedings of the 10th International Conferenceon Computational Linguistics and Intelligent Text Pro-cessing, pages 582?593, Berlin, Heidelberg.
Springer-Verlag.Robert L. Donaway, Kevin W. Drummey, and Laura A.Mather.
2000.
A comparison of rankings producedby summarization evaluation measures.
In NAACL-ANLP 2000 Workshop on Automatic summarization,pages 69?78, Morristown, NJ, USA.
Association forComputational Linguistics.G.
Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based centrality as salience in text summariza-tion.
Journal of Artificial Intelligence Research, 22.Eduard H. Hovy and Chin Yew Lin.
1999.
Automatedtext summarization in summarist.
In Inderjeet Maniand Mark T. Maybury, editors, Advances in AutomaticText Summarization, pages 81?97.
MIT Press.Hongyan Jing.
2002.
Using hidden markov modeling todecompose human-written summaries.
Comput.
Lin-guist., 28(4):527?543.Julian Kupiec, Jan Pedersen, and Francine Chen.
1995.A trainable document summarizer.
In SIGIR ?95: Pro-ceedings of the 18th annual international ACM SI-GIR conference on Research and development in infor-mation retrieval, pages 68?73, New York, NY, USA.ACM.Chin-Yew Lin and Eduard Hovy.
2003.
The potentialand limitations of automatic sentence extraction forsummarization.
In Proceedings of the HLT-NAACL 03on Text summarization workshop, pages 73?80, Mor-ristown, NJ, USA.
Association for Computational Lin-guistics.Chin-Yew Lin.
1999.
Training a selection function forextraction.
In CIKM ?99: Proceedings of the eighthinternational conference on Information and knowl-edge management, pages 55?62, New York, NY, USA.ACM.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In Stan Szpakowicz Marie-Francine Moens, editor, Text Summarization BranchesOut: Proceedings of the ACL-04 Workshop, pages 74?81, Barcelona, Spain, July.
Association for Computa-tional Linguistics.Daniel Marcu.
1999.
The automatic construction oflarge-scale corpora for summarization research.
InSIGIR ?99: Proceedings of the 22nd annual interna-tional ACM SIGIR conference on Research and devel-opment in information retrieval, pages 137?144, NewYork, NY, USA.
ACM.Rada Mihalcea and Hakan Ceylan.
2007.
Explorations inautomatic book summarization.
In Proceedings of the2007 Joint Conference on Empirical Methods in Nat-ural Language Processing and Computational Natu-ral Language Learning (EMNLP-CoNLL), pages 380?389, Prague, Czech Republic, June.
Association forComputational Linguistics.Rada Mihalcea and Paul Tarau.
2004.
Textrank: Bring-ing order into texts.
In Conference on EmpiricalMethods in Natural Language Processing, Barcelona,Spain.Yoshio Nakao.
2000.
An algorithm for one-page sum-marization of a long text based on thematic hierarchydetection.
In ACL ?00: Proceedings of the 38th An-nual Meeting on Association for Computational Lin-guistics, pages 302?309, Morristown, NJ, USA.
Asso-ciation for Computational Linguistics.Lawrence Page, Sergey Brin, Rajeev Motwani, and TerryWinograd.
1999.
The pagerank citation ranking:Bringing order to the web.
Technical report, StanfordInfoLab.Karen Sparck-Jones.
1999.
Automatic summarising:Factors and directions.
In Inderjeet Mani and Mark T.Maybury, editors, Advances in Automatic Text Summa-rization, pages 1?13.
MIT Press.Simone Teufel and Marc Moens.
1997.
Sentence ex-traction as a classification task.
In Proceedings of theACL?97/EACL?97 Workshop on Intelligent ScallableText Summarization, Madrid, Spain, July.Masao Utiyama and Hitoshi Isahara.
2001.
A statisticalmodel for domain-independent text segmentation.
InIn Proceedings of the 9th Conference of the EuropeanChapter of the Association for Computational Linguis-tics, pages 491?498.Min yen Kan, Judith L. Klavans, and Kathleen R. McK-eown.
1998.
Linear segmentation and segment sig-nificance.
In In Proceedings of the 6th InternationalWorkshop of Very Large Corpora, pages 197?205.911
