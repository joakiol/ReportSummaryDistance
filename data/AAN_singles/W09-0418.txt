Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 105?109,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsNICT@WMT09:Model Adaptation and Transliteration for Spanish-English SMTMichael Paul, Andrew Finch and Eiichiro SumitaLanguage Translation GroupMASTAR ProjectNational Institute of Information and Communications TechnologyMichael.Paul@nict.go.jpAbstractThis paper describes the NICT statis-tical machine translation (SMT) systemused for the WMT 2009 Shared Task(WMT09) evaluation.
We participated inthe Spanish-English translation task.
Thefocus of this year?s participation was to in-vestigate model adaptation and transliter-ation techniques in order to improve thetranslation quality of the baseline phrase-based SMT system.1 IntroductionThis paper describes the NICT statistical machinetranslation (SMT) system used for the sharedtask of the Fourth Workshop on Statistical Ma-chine Translation.
We participated in the Spanish-English translation task under the ConstrainedCondition.
For the training of the SMT engines,we used two parallel Spanish-English corpora pro-vided by the organizers: the Europarl (EP) cor-pus (Koehn, 2005), which consists of 1.4M paral-lel sentences extracted from the proceedings of theEuropean Parliament, and the News Commentary(NC) corpus (Callison-Burch et al, 2008), whichconsists of 74K parallel sentences taken from ma-jor news outlets like BBC, Der Spiegel, and LeMonde.In order to adapt SMT systems to a specific do-main, recent research focuses on model adapta-tion techniques that adjust their parameters basedon information about the evaluation domain (Fos-ter and Kuhn, 2007; Finch and Sumita, 2008a).Statistical models can be trained on in-domainand out-of-domain data sets and combined atrun-time using probabilistic weighting betweendomain-specific statistical models.
As the officialWMT09 evaluation testset consists of documentstaken from the news domain, we applied statisticalmodel adaptation techniques to combine transla-tion models (tm), language models (lm) and dis-tortion models (dm) trained on (a) the in-domainNC corpus and (b) the out-of-domain EP corpus(cf.
Section 2).One major problem in the given translation taskwas the large amount of out-of-vocabulary (OOV)words, i.e., source language words that do not oc-cur in the training corpus.
For unknown words, notranslation entry is available in the statistical trans-lation model (phrase-table).
As a result, theseOOV words cannot be translated.
Dealing withlanguages with a rich morphology like Spanishand having a limited amount of bilingual resourcesmake this problem even more severe.There have been several efforts in dealing withOOV words to improve translation quality.
In ad-dition to parallel text corpora, external bilingualdictionaries can be exploited to reduce the OOVproblem (Okuma et al, 2007).
However, these ap-proaches depend on the coverage of the utilizedexternal dictionaries.Data sparseness problems due to inflectionalvariations were previously addressed by applyingword transformations using stemming or lemmati-zation (Popovic and Ney, 2005; Gupta and Fed-erico, 2006).
A tight integration of morpho-syntactic information into the translation modelwas proposed by (Koehn and Hoang, 2007) wherelemma and morphological information are trans-lated separately, and this information is combinedon the output side to generate the translation.However, these approaches still suffer from thedata sparseness problem, since lemmata and in-flectional forms never seen in the training corpuscannot be translated.In order to generate translations for unknownwords, previous approaches focused on translit-eration methods, where a sequence of charac-ters is mapped from one writing system into an-other.
For example, in order to translate names andtechnical terms, (Knight and Graehl, 1997) intro-duced a probabilistic model that replaces Japanese105katakana1 words with phonetically equivalent En-glish words.
More recently, (Finch and Sumita,2008b) proposed a transliteration method that isbased directly on techniques developed for phrase-based SMT, and transforms a character sequencefrom one language into another in a subword-level, character-based manner.
We extend this ap-proach by exploiting the phrase-table of the base-line SMT system to train a phrase-based translit-eration model that generates English translationsof Spanish OOV words as described in Section 3.The effects of the proposed techniques are investi-gated in detail in Section 4.2 Model AdaptationPhrase-based statistical machine translation en-gines use multiple statistical models to generate atranslation hypothesis in which (1) the translationmodel ensures that the source phrases and the se-lected target phrases are appropriate translations ofeach other, (2) the language model ensures that thetarget language is fluent, (3) the distortion modelcontrols the reordering of the input sentence, and(4) the word penalty ensures that the translationsdo not become too long or too short.
During de-coding, all model scores are weighted and com-bined to find the most likely translation hypothesisfor a given input sentence (Koehn et al, 2007).In order to adapt SMT systems to a specific do-main, separate statistical models can be trainedon parallel text corpora taken from the respec-tive domain (in-domain) and additional out-of-domain language resources.
The models are thencombined using mixture modeling (Hastie et al,2001), i.e., each model is weighted according toits fit with in-domain development data sets andthe linear combination of the respective scores isused to find the best translation hypothesis duringthe decoding of unseen input sentences.In this paper, the above model adaptation tech-nique is applied to combine the NC and the EPlanguage resources provided by the organizersfor the Spanish-English translation task.
As theWMT09 evaluation testset consists of documentstaken from the news domain, we used the NC cor-pus to train the in-domain models and the EP cor-pus to train the out-of-domain component models.Using mixture modeling, the above mentioned sta-tistical models are combined where each compo-nent model is optimized separately.
Weight opti-1A special syllabary alphabet used to write down foreignnames or loan words.mization is carried out using a simple grid-searchmethod.
At each point on the grid of weight pa-rameter values, the translation quality of the com-bined weighted component models is evaluated fordevelopment data sets taken from (a) the NC cor-pus and (b) from the EP corpus.3 TransliterationSource language input words that cannot be trans-lated by the standard phrase-based SMT mod-els are either left untranslated or simply removedfrom the translation output.
Common examplesare named entities such as personal names or tech-nical terms, but also include content words likecommon nouns or verbs that are not covered by thetraining data.
Such unknown occurrences couldbenefit from being transliterated into the MT sys-tem?s output during translation of orthographicallyrelated languages like Spanish and English.In this paper, we apply a phrase-based translit-eration approach similar to the one proposed in(Finch and Sumita, 2008b).
The transliterationmethod is based directly on techniques developedfor phrase-based SMT and treats the task of trans-forming a character sequence from one languageinto another as a character-level translation pro-cess.
In contrast to (Finch and Sumita, 2008b)where external dictionaries and inter-languagelinks in Wikipedia2 are utilized, the translitera-tion training examples used for the experiments inSection 4 are extracted directly from the phrase-table of the baseline SMT systems trained on theprovided data sets.
For each phrase-table entry,corresponding word pairs are identified accordingto a string similarity measure based on the edit-distance (Wagner, 1974) that is defined as the sumof the costs of insertion, deletion, and substitutionoperations required to map one character sequenceinto the other and can be calculated by a dynamicprogramming technique (Cormen et al, 1989).
Inorder to reduce noise in the training data, onlyword pairs whose word length and similarity areabove a pre-defined threshold are utilized for thetraining of the transliteration model.The obtained transliteration model is applied asa post-process filter to the SMT decoding process,i.e.. all source language words that could not betranslated using the SMT engine are replaced withthe corresponding transliterated word forms in or-der to obtain the final translation output.2http://www.wikipedia.org1064 ExperimentsThe effects of model adaptation and translitera-tion techniques were evaluated using the Spanish-English language resources summarized in Ta-ble 1.
In addition, the characteristics of this year?stestset are given in Table 2.
The sentence lengthis given as the average number of words per sen-tence.
The OOV word figures give the percentageof words in the evaluation data set that do not ap-pear in the NC/EP training data.
In order to get anidea how difficult the translation task may be, wealso calculated the language perplexity of the re-spective evaluation data sets according to 5-gramtarget language models trained on the NC/EP datasets.Concerning the development sets, the news-dev2009 data taken from the same news sourcesas the evaluation set of the shared task was usedfor the tuning of the SMT engines, and the de-vtest2006 data taken from the EP corpus was usedfor system parameter optimization.
For the evalua-tion of the proposed methods, we used the testsetsof the Second Workshop on SMT (nc-test2007 forNC and test2007 for EP).
All data sets were case-sensitive with punctuation marks tokenized.The numbers in Table 1 indicate that the char-acteristics of this year?s testset differ largely fromtestsets of previous evaluation campaigns.
TheNC devset (2,438/1,378 OOVs) contains twiceas many untranslatable Spanish words as theNC evalset (1,168/73 OOVs) and the EP devset(912/63 OOVs).
In addition, the high languageperplexity figures for this year?s testset show thatthe translation quality output for both baseline sys-tems is expected to be much lower than those forthe EP evaluation data sets.
In this paper, transla-tion quality is evaluated according to (1) the BLEUmetrics which calculates the geometric mean of n-gram precision by the system output with respectto reference translations (Papineni et al, 2002),and (2) the METEOR metrics that calculates uni-gram overlaps between translations (Banerjee andLavie, 2005).
Scores of both metrics range be-tween 0 (worst) and 1 (best) and are displayed inpercent figures.4.1 BaselineOur baseline system is a fairly typical phrase-based machine translation system (Finch andSumita, 2008a) built within the framework of afeature-based exponential model containing thefollowing features:Table 1: Language ResourcesCorpus Train Dev EvalNC Spanish sentences 74K 2,001 2,007words 2,048K 49,116 56,081vocab 61K 9,047 8,638length 27.6 24.5 27.9OOV (%) ?
5.2 / 2.9 1.4 / 0.9English sentences 74K 2,001 2,007words 1,795K 46,524 49,693vocab 47K 8,110 7,541length 24.2 23.2 24.8OOV (%) ?
5.2 / 2.9 1.2 / 0.9perplexity ?
349 / 381 348 / 458EP Spanish sentences 1,404K 1,861 2,000words 41,003K 50,216 61,293vocab 170K 7,422 8,251length 29.2 27.0 30.6OOV (%) ?
2.4 / 0.1 2.4 / 0.2English sentences 1,404K 1,861 2,000words 39,354K 48,663 59,145vocab 121K 5,869 6,428length 28.0 26.1 29.6OOV (%) ?
1.8 / 0.1 1.9 / 0.1perplexity ?
210 / 72 305 / 125Table 2: Testset 2009Corpus TestNC Spanish sentences 3,027words 80,591vocab 12,616length 26.6?
Source-target phrase translation probability?
Inverse phrase translation probability?
Source-target lexical weighting probability?
Inverse lexical weighting probability?
Phrase penalty?
Language model probability?
Lexical reordering probability?
Simple distance-based distortion model?
Word penaltyFor the training of the statistical models, stan-dard word alignment (GIZA++ (Och and Ney,2003)) and language modeling (SRILM (Stolcke,2002)) tools were used.
We used 5-gram lan-guage models trained with modified Knesser-Neysmoothing.
The language models were trained onthe target side of the provided training corpora.Minimum error rate training (MERT) with respectto BLEU score was used to tune the decoder?s pa-rameters, and performed using the technique pro-posed in (Och, 2003).
For the translation, the in-house multi-stack phrase-based decoder CleopA-TRa was used.The automatic evaluation scores of the baselinesystems trained on (a) only the NC corpus and (b)only on the EP corpus are summarized in Table 3.107Table 3: Baseline PerformanceNC Eval EP EvalBLEU METEOR BLEU METEORbaseline 17.56 40.52 33.00 56.504.2 Effects of Model AdaptationIn order to investigate the effect of model adapta-tion, each model component was optimized sep-arately using the method described in Section 2.Table 4 summarizes the automatic evaluation re-sults for various model combinations.
The combi-nation of NC and EP models using equal weightsachieves only a slight improvement for the NCtask (BLEU: +0.4%, METEOR: +0.4%), but alarge improvement for the EP task (BLEU: +1.0%,METEOR: +1.7%).
Weight optimization furtherimproves all translation tasks where the highestevaluation scores are achieved when the optimizedweights for all statistical models are used.
In total,model adaptation gains 1.1% and 1.3% in BLEUand 0.8% and 1.8% in METEOR for the NC andEP translation tasks, respectively.Table 4: Effects of Model Adaptationweight NC Eval EP Evaloptimization BLEU METEOR BLEU METEOR?
17.92 40.72 34.00 58.20tm 18.13 40.95 34.05 58.23tm+lm 18.25 41.23 34.12 58.22tm+dm 18.36 41.06 34.24 58.34tm+lm+dm 18.65 41.35 34.35 58.364.3 Effects of TransliterationIn order to investigate the effects of translitera-tion, we trained three different transliteration us-ing the phrase-table of the baseline systems trainedon (a) only the NC corpus, (b) only the EP cor-pus, and (c) on the merged corpus (NC+EP).
Theperformance of these phrase-based transliterationmodels is evaluated for 2000 randomly selectedtransliteration examples.
Table 5 summarizes theharacter-based automatic evaluation scores for theword error rate (WER) metrics, i.e., the edit dis-tance between the system output and the closestreference translation (Niessen et al, 2000), as wellas the BLEU and METEOR metrics.
The bestperformance is achieved when training examplesfrom both domains are exploit to transliterate un-known Spanish words into English.
Therefore, theNC+EP transliteration model was applied to thetranslation outputs of all mixture models describedin Section 4.2.The effects of the transliteration post-processare summarized in Table 6.
Transliteration consis-Table 5: Transliteration PerformanceTraining character-basedData WER BLEU METEORNC 13.10 83.62 86.74EP 11.76 85.93 87.89NC+EP 11.72 86.08 87.89tently improves the translation quality of all mix-ture models, although the gains obtained for theNC task (BLEU: +1.3%, METEOR: +1.3%) aremuch larger than those for the EP task (BLEU:+0.1%, METEOR: +0.2%) which is due to thelarger amount of untranslatable words in the NCevaluation data set.Table 6: Effects of Transliterationweight NC Eval EP Evaloptimization BLEU METEOR BLEU METEORtm 19.14 42.39 34.11 58.46tm+lm 19.46 42.65 34.16 58.44tm+dm 19.77 42.35 34.38 58.57tm+lm+dm 19.95 42.64 34.48 58.604.4 WMT09 Testset ResultsBased on the automatic evaluation results pre-sented in the previous sections, we selected theSMT engine based on the tm+lm+dm weights op-timized on the NC devset as the primary run forour testset run submission.
All other model weightcombinations were submitted as contrastive runs.The BLEU scores of these runs are listed in Ta-ble 7 and confirm the results obtained for theabove experiments, i.e., the best performing sys-tem is the one based on the mixture models us-ing separately optimized weights in combinationwith the transliteration of untranslatable Span-ish words using the phrase-based transliterationmodel trained on all available language resources.Table 7: Testset 2009 Performanceweight NC Eval EP Evaloptimization BLEU BLEUtm 21.07 20.81tm+lm 20.95 20.59tm+dm 21.45 21.32tm+lm+dm 21.67?
21.275 ConclusionThe work for this year?s shared task focused onthe task of effectively utilizing out-of-domain lan-guage resources and handling OOV words to im-prove translation quality.
Overall our experi-ments show that the incorporation of mixture mod-els and phrase-based transliteration techniqueslargely out-performed standard phrase-based SMTengines gaining a total of 2.4% in BLEU and 2.1%in METEOR for the news domain.108ReferencesS.
Banerjee and A. Lavie.
2005.
METEOR: An Auto-matic Metric for MT Evaluation with Improved Cor-relation with Human Judgments.
In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Eval-uation Measures for MT, pages 65?72, Ann Arbor,Michigan.C.
Callison-Burch, C. Fordyce, P. Koehn, C. Monz, andJ.
Schroeder.
2008.
Further Meta-Evaluation of Ma-chine Translation.
In Proceedings of the 3rd Work-shop on SMT, pages 70?106, Columbus, Ohio.H.
Cormen, C. Leiserson, and L. Rivest.
1989.
Intro-duction to Algorithms.
MIT Press.A.
Finch and E. Sumita.
2008a.
Dynamic Model Inter-polation for Statistical Machine Translation.
In Pro-ceedings of the 3rd Workshop on SMT, pages 208?215, Columbus, Ohio.A.
Finch and E. Sumita.
2008b.
Phrase-based Ma-chine Transliteration.
In Proceedings of the IJC-NLP, pages 13?18, Hyderabad, India.G.
Foster and R. Kuhn.
2007.
Mixture-Model Adapta-tion for SMT.
In Proceedings of the 2nd Workshopon SMT, pages 128?135, Prague, Czech Republic.D.
Gupta and M. Federico.
2006.
Exploiting WordTransformation in SMT from Spanish to English.
InProceedings of the EAMT, pages 75?80, Oslo, Nor-way.T.
Hastie, R. Tibshirani, and J. Friedman.
2001.
TheElements of Statistical Learning.
Springer, NewYork.K.
Knight and J. Graehl.
1997.
Machine Translitera-tion.
In Proceedings of the 35th ACL, pages 128?135, Madrid, Spain.P.
Koehn and H. Hoang.
2007.
Factored Transla-tion Models.
In Proceedings of the EMNLP-CoNLL,pages 868?876, Prague, Czech Republic.P.
Koehn, F.J. Och, and D. Marcu.
2007.
Statisti-cal Phrase-Based Translation.
In Proceedings of theHLT-NAACL, pages 127?133, Edmonton, Canada.P.
Koehn.
2005.
Europarl: A Parallel Corpus for Sta-tistical Machine Translation.
In Proceedings of theMT Summit X, pages 79?86, Phuket, Thailand.S.
Niessen, F.J. Och, G. Leusch, and H. Ney.
2000.An Evaluation Tool for Machine Translation: FastEvaluation for MT Research.
In Proc.
of the 2ndLREC, pages 39?45, Athens, Greece.F.J.
Och and H. Ney.
2003.
A Systematic Comparisonof Various Statistical Alignment Models.
Computa-tional Linguistics, 29(1):19?51.F.J.
Och.
2003.
Minimum Error Rate Training in Sta-tistical Machine Translation.
In Proceedings of the41st ACL, pages 160?167, Sapporo, Japan.H.
Okuma, H. Yamamoto, and E. Sumita.
2007.
In-troducing Translation Dictionary into phrase-basedSMT.
In Proceedings of MT Summit XI, pages 361?368, Copenhagen, Denmark.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a Method for Automatic Evaluation of Ma-chine Translation.
In Proceedings of the 40th ACL,pages 311?318, Philadelphia, USA.M.
Popovic and H. Ney.
2005.
Exploiting Phrasal Lex-ica and Additional Morpho-synatctic Language Re-sources for SMT with Scarce Training Data.
In Pro-ceedings of the EAMT, pages 212?218, Budapest,Hungary.A.
Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of ICSLP, pages901?904, Denver.R.W.
Wagner.
1974.
The string-to-string correctionproblem.
Journal of the ACM, 21(1):169?173.109
