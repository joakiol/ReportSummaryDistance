Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 325?335,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsReNoun: Fact Extraction for Nominal AttributesMohamed Yahya?Max Planck Institute for Informaticsmyahya@mpi-inf.mpg.deSteven Euijong Whang, Rahul Gupta, Alon HalevyGoogle Research{swhang,grahul,halevy}@google.comAbstractSearch engines are increasingly relying onlarge knowledge bases of facts to providedirect answers to users?
queries.
How-ever, the construction of these knowledgebases is largely manual and does not scaleto the long and heavy tail of facts.
Openinformation extraction tries to address thischallenge, but typically assumes that factsare expressed with verb phrases, and there-fore has had difficulty extracting facts fornoun-based relations.We describe ReNoun, an open informationextraction system that complements pre-vious efforts by focusing on nominal at-tributes and on the long tail.
ReNoun?s ap-proach is based on leveraging a large on-tology of noun attributes mined from a textcorpus and from user queries.
ReNouncreates a seed set of training data by us-ing specialized patterns and requiring thatthe facts mention an attribute in the ontol-ogy.
ReNoun then generalizes from thisseed set to produce a much larger set of ex-tractions that are then scored.
We describeexperiments that show that we extract factswith high precision and for attributes thatcannot be extracted with verb-based tech-niques.1 IntroductionOne of the major themes driving the current evo-lution of search engines is to make the searchexperience more efficient and mobile friendlyfor users by providing them concrete answers toqueries.
These answers, that apply to queriesabout entities that the search engine knows about(e.g., famous individuals, organizations or loca-tions) complement the links that the search en-?Work done during an internship at Google Research.gine typically returns (Sawant and Chakrabati,2013; Singhal, 2012; Yahya et al., 2012).
Tosupport such answers, the search engine main-tains a knowledge base that describes various at-tributes of an entity (e.g., (Nicolas Sarkozy,wife, Carla Bruni)).
Upon receiving a query,the search engine tries to recognize whether theanswer is in its knowledge base.For the most part, the aforementioned knowl-edge bases are constructed using manual tech-niques and carefully supervised information ex-traction algorithms.
As a result, they obtain highcoverage on head attributes, but low coverage ontail ones, such as those shown in Table 1.
For ex-ample, they may have the answer for the query?Sarkozy?s wife?, but not for ?Hollande?s ex-girlfriend?
or ?Google?s philanthropic arm?.
Inaddition to broadening the scope of query answer-ing, extending the coverage of the knowledge baseto long tail attributes can also facilitate providingWeb answers to the user.
Specifically, the searchengine can use lower-confidence facts to corrob-orate an answer that appears in text in one of thetop Web results and highlight them to the user.This paper describes ReNoun, an open-information extraction system that focuses on ex-tracting facts for long tail attributes.
The obser-vation underlying our approach is that attributesfrom the long tail are typically expressed as nouns,whereas most previous work on open-informationextraction (e.g., Mausam et al.
(2012)) extendtechniques for extracting attributes expressed inverb form.
Hence, the main contribution of ourwork is to develop an extraction system that com-plements previous efforts, focuses on nominal at-tributes and is effective for the long tail.
To thatend, ReNoun begins with a large but imperfect on-tology of nominal attributes that is extracted fromtext and the query stream (Gupta et al., 2014).ReNoun proceeds by using a small set of high-precision extractors that exploit the nominal na-325Attribute Fact Phrase Verb form seenlegal affairs (NPR, legal affairs NPR welcomed Nina Totenberg as 7correspondent correspondent, Nina Totenberg) its new legal affairs correspondent.economist (Princeton, economist, Princeton economist Paul Krugman 7Paul Krugman) was awarded the Nobel prize in 2008.ex-boyfriend (Trierweiler, ex-boyfriend, Trierweiler did not have any children 3Hollande with her ex-boyfriend Hollande.staff writer (The New Yorker, staff writer, Adam Gopnik is one of The New 3Adam Gopnik) Yorker?s best staff writers.Table 1: Examples of noun phrases as attributes, none which are part of a verb phrase.
Additionally, the first two attributes donot occur within a verb phrase in a large corpus (see ?
2 for details) in a setting where they can be associated with a triple.ture of the attributes to obtain a training set, andthen generalizes from the training set via distantsupervision to find a much larger set of extractionpatterns.
Finally, ReNoun scores extracted factsby considering how frequently their patterns ex-tract triples and the coherence of these patterns,i.e., whether they extract triples for semanticallysimilar attributes.
Our experiments demonstratethat ReNoun extracts a large body of high preci-sion facts, and that these facts are not extractedwith techniques based on verb phrases.2 PreliminariesThe goal of ReNoun is to extract triples of the form(S,A,O), where S is subject, A is the attribute, andO is the object.
In our setting, the attribute is al-ways a noun phrase.
We refer to the subject andobject as the arguments of the attribute.ReNoun takes as input a set of attributes, whichcan be collected using the methods described inGupta el al.
(2014), Lee et al.
(2012), and Pascaand van Durme (2007).
In this work, we use Biper-pedia (Gupta et al., 2014), which is an ontologyof nominal attributes automatically extracted fromWeb text and user queries.
For every attribute,Biperpedia supplies the Freebase (Bollacker et al.,2008) domain type (e.g., whether the attribute ap-plies to people, organizations or hotels).
Since theattributes themselves are the result of an extractionalgorithm, they may include false positives (i.e.,attributes that do not make sense).The focus of ReNoun is on attributes whose val-ues are concrete objects (e.g., wife, protege,chief-economist).
Other classes of attributesthat we do not consider in this work are (1) nu-meric (e.g., population, GDP) that are better ex-tracted from Web tables (Cafarella et al., 2008),and (2) vague (e.g., culture, economy) whosevalue is a narrative that would not fit the currentmode of query answering on search engines.We make the distinction between the fat headand long tail of attributes.
To define these two sets,we ordered the attributes in decreasing order of thenumber of occurrences in the corpus1.
We definedthe fat head to be the attributes until the point Nin the ordering such that the sum of the total num-ber of occurrences of attributes before N equaledthe number of total occurrences of the attributesafter N .
In our news corpus, the fat head included218 attributes (i.e., N = 218) and the long tailincluded 60K attributes.
Table 2 shows examplesfrom both.Fat headdaughter, headquarterspresident, spokesperson,Long tailchief economist, defender,philanthropic arm, protegeTable 2: Examples of fat head and long tail attributes.The output of ReNoun is a set of facts, whereeach fact could be generated by multiple extrac-tions.
We store the provenance of each extractionand the number of times each fact was extracted.Noun versus verb attributesReNoun?s goal is to extract facts for attributes ex-pressed as noun phrases.
A natural question iswhether we can exploit prior work on open in-formation extraction, which focused on extractingrelations expressed as verbs.
For example, if wecan extract facts for the attribute advised or isadvisor of, we can populate the noun attributeadvisor with the same facts.
In Section 7.2 wedemonstrate that this approach is limited for sev-eral reasons.First, attributes in knowledge bases are typicallyexpressed as noun phrases.
Table 3 shows that1The occurrences were weighted by the number of se-mantic classes they occur with in the ontology because manyclasses overlap.326Knowledge Base %Nouns %VerbsFreebase 97 3DBpedia 96 4Table 3: Percentage of attributes expressed as nouns phrasesamong the 100 attributes with the most facts.the vast majority of the attributes in both Freebaseand DBpedia (Auer et al., 2007) are expressed asnouns even for the fat head (and even more so forthe long tail).
Hence, if we extract the verb formof attributes we would need to translate them intonoun form, which would require us to solve theparaphrasing problem and introduce more sourcesof error (Madnani and Dorr, 2010).
Second, as wedig deeper into the long tail, attributes tend to beexpressed in text more in noun form rather thanverb form.
One of the reasons is that the attributenames tend to get longer and therefore unnaturalto express as verbs (e.g.
chief privacy officer, au-tomotive division).
Finally, there is often a sub-tle difference in meaning between verb forms andnoun forms of attributes.
For example, it is com-mon to see the phrase ?Obama advised Merkel onsaving the Euro,?
but that would not necessarilymean we want to say that Obama is an advisor ofAngela Merkel, in the common sense of advisor.Processed document corpusReNoun extracts facts from a large corpus of400M news articles.
We exploit rich synacticand linguistic cues, by processing these docu-ments with a natural language processing pipelinecomprising of ?
dependency parsing, noun phrasechunking, named entity recognition, coreferenceresolution, and entity resolution to Freebase.
Thechunker identifies nominal mentions in the textthat include our attributes of interest.
As discussedlater in the paper, we exploit the dependencyparse, coreference and entity resolution heavilyduring various stages of our pipeline.3 Overview of ReNounSince ReNoun aims at extracting triples for at-tributes not present in head-heavy knowledgebases, one key challenge is that we do not have anylabeled data (i.e.
known facts) for such attributes,especially in the long tail.
Therefore ReNoun hasan initial seed fact extraction step that automati-cally generates a small corpus of relatively preciseseed facts for all attributes, so that distant supervi-sion can be employed.
The second big challengeis to filter the noise from the resulting extractions.ReNoun?s extraction pipeline, shown in Fig-ure 1, is composed of four stages.Seed fact extraction: We begin by extracting asmall number of high-precision facts for our at-tributes.
For this step, we rely on manually spec-ified lexical patterns that are specifically tailoredfor noun phrases, but are general enough to be in-dependent of any specific attributes.
When apply-ing such patterns, we exploit coreference to makethe generated seed facts more precise by requiringthe attribute and object noun phrases of a seed factto refer to the same real-world entity.
This is elab-orated further in Section 4.Extraction pattern generation: Utilizing theseed facts, we use distant supervision (Mintz et al.,2009) to learn a set of dependency parse patternsthat are used to extract a lot more facts from thetext corpus.Candidate generation: We apply the learned de-pendency parse patterns from the previous stageto generate a much larger set of extractions.
Weaggregate all the extractions that give rise to thesame fact and store with them the provenance ofthe extraction.
The extractions generated here arecalled candidates because they are assigned scoresthat determine how they are used.
The applica-tion consuming an extraction can decide whetherto discard an extraction or use it, and in this casethe manner in which it is used, based on the scoreswe attach to it and the application?s precision re-quirements.Scoring: In the final stage, we score the facts, re-flecting our confidence in their correctness.
In-tuitively, we give a pattern a high score if it ex-tracts many facts that have semantically similar at-tributes, and then propagate this score to the factsextracted by the pattern (Section 6).4 Seed fact extractionSince we do not have facts, but only attributes, thefirst phase of ReNoun?s pipeline is to extract a setof high-precision seed facts that are used to trainmore general extraction patterns.
ReNoun extractsseed facts using a manually crafted set of extrac-tion rules (see Table 4).
However, the extractionrules and the application of these rules are tailoredto our task of extracting noun-based attributes.Specifically, when we apply an extraction ruleto generate a triple (S,A,O), we require that (1) Ais an attribute in our ontology, and (2) the value ofA and the object O corefer to the same real-world327		 		Figure 1: Extraction Pipeline: we begin with a set of high-precision extractors and use distant supervision to train otherextractors.
We then apply the new extractors and score the resulting triples based on the frequency and coherence of thepatterns that produce them.1.
the A of S, O ?
the CEO of Google, Larry Page2.
the A of S is O ?
the CEO of Google is Larry Page3.
O, S A ?
Larry Page, Google CEO4.
O, S?s A ?
Larry Page, Google?s CEO5.
O, [the] A of S ?
Larry Page, [the] CEO of Google6.
SAO ?
Google CEO Larry Page7.
S A, O ?
Google CEO, Larry Page8.
S?s A, O ?
Google?s CEO, Larry PageTable 4: High precision patterns used for seed fact extractionalong with an example of each.
Here, the object (O) and theattribute (A) corefer and the subject (S) is in close proxim-ity.
In all examples, the resulting fact is (Google, CEO,Larry Page).
Patterns are not attribute specific.entity.
For example, in Figure 2, CEO is in our on-tology and we can use a coreference resolver to in-fer that CEO and Larry Page refer to the same en-tity.
The use of coreference follows from the sim-ple observation that objects will often be referredto by nominals, many of which are our attributes ofinterest.
Since the sentence matches our sixth ex-traction rule, ReNoun extracts the triple (Google,CEO, Larry Page).Document:?
[Google]1[CEO]2[Larry Page]2started his term in 2011,when [he]2succeeded [Eric Schmidt]3.
[Schmidt]3hassince assumed the role of executive chairman of [thecompany]1.?
(a)Coreference clusters:# Phrases Freebase ID1 Google, the company /m/045c7b2 Larry Page, CEO, he /m/0gjpq3 Eric Schmidt, Schmidt /m/01gqf4(b)Figure 2: Coreference clusters: (a) a document annotatedwith coreference clusters; (b) a table showing each clusterwith the representative phrases in bold and the Freebase IDto which each cluster maps.We rely on a coreference resolver in the spirit ofHaghighi and Klein (2009).
The resolver clustersthe mentions of entities in a document so the ref-erences in each cluster are assumed to refer to thesame real-world entity.
The resolver also choosesfor each cluster a representative phrase, which is aproper noun or proper adjective (e.g., Canadian).Other phrases in the same cluster can be otherproper nouns or adjectives, common nouns likeCEO or pronouns like he in the example.
Eachcluster is possibly linked by an entity resolver toa Freebase entity using a unique Freebase ID.
Fig-ure 2(b) shows the coreference clusters from thesample document, with representative phrases inbold, along with the Freebase ID of each clus-ter.
Note that in our example the phrase execu-tive chairman, which is also in our ontology ofattributes, is not part of any coreference cluster.Therefore, the fact centered around this attributein the example will not be part of the seed extrac-tions, but could be extracted in the next phase.
Theresulting facts use Freebase IDs for the subject andobject (for readability, we will use entity namesin the rest of this work).
In summary, our seedextraction proceeds in two steps.
First, we findsentences with candidate attribute-object pairs thatcorefer and in which the attribute is in our ontol-ogy.
Second, we match these sentences against ourhand-crafted rules to generate the extractions.
InSection 7 we show that the precision of our seedfacts is 65% for fat head attributes and 80% forlong tail ones.5 Pattern and candidate fact generationIn this section we describe how ReNoun uses theseed facts to learn a much broader set of extrac-tion patterns.
ReNoun uses the learned patternsto extract many more candidate facts that are thenassigned scores reflecting their quality.5.1 Dependency patternsWe use the seed facts to learn patterns over de-pendency parses of text sentences.
A dependencyparse of a sentence is a directed graph whose ver-tices correspond to tokens labeled with the wordand the POS tag, and the edges are syntactic rela-tions between the corresponding tokens (de Marn-effe et al., 2006).
A dependency pattern is a sub-graph of a dependency parse where some wordshave been replaced by variables, but the POS tags328have been retained (called delexicalization).
A de-pendency pattern enables us to extract sentenceswith the same dependency parse as the sentencethat generated the pattern, modulo the delexical-ized words.
We note that one big benefit of usingdependency patterns is that they generalize well,as they ignore extra tokens in the sentence that donot belong to the dependency subgraph of interest.5.2 Generating dependency patternsThe procedure for dependency pattern generationis shown in Algorithm 1, and Figure 3 shows anexample of its application.
The input to the algo-rithm is the ontology of attributes, the seed facts(Section 4), and our processed text corpus (Sec-tion 2).Algorithm 1: Dependency pattern generationinput : Set of attributes A, Seed facts I , Corpus D.P := An empty set of dependency pattern-attribute pairs.foreach sentence s ?
D doforeach triple t = (S,A,O) found in s doif t ?
I thenG(s) = dependency parse of sP?= minimal subgraph of G(s)containing the head tokens of S, A and OP = Delexicalize(P?, S, A, O)P = P ?
{?P,A?
}return PAttributes: A ={executive chairman}Seed fact: I = {(Google, executive chairman, Eric Schmidt)}Sentence: s =?An executive chairman, like Eric Schmidt of Google, wields influenceover company operations.
?An/DETexecutive/NNchairman/NN detnnlike/INprep Schmidt/NNPpobj Eric/NNPnn of/INprep Google/NNPpobj(a)chairman/NN like/INprep Schmidt/NNPpobj of/INprep Google/NNPpobj(b){A/N} like/INprep {O/N}pobj of/INprep {S/N}pobj(c)Figure 3: Dependency pattern generation using seed facts,corresponding to Algorithm 1: (a) shows the input to the pro-cedure (dependency parse partially shown); (b) P?
; (c) P .The procedure iterates over the sentences in thecorpus, looking for matches between a sentences and a seed fact f .
A sentence s matches f ifs contains (i) the attribute in f , and (ii) phrases incoreference clusters that map to the same FreebaseIDs as the subject and object of f .
When a matchis found, we generate a pattern as follows.We denote by P?the minimal subgraph of thedependency parse of s containing the head tokensof the subject, attribute and object (Figure 3 (b)).We delexicalize the three vertices correspondingto the head tokens of the subject, attribute and ob-ject by variables indicating their roles.
The POStag associated with the attribute token is always anoun.
The subject and object are additionally al-lowed to have pronouns and adjectives associatedwith their tokens.
All POS tags corresponding tonouns are lifted to N, in order to match the vari-ous types of nouns.
We denote the resulting de-pendency pattern by P and add it to our output,associated with the matched attribute.
We notethat in principle, the vertex corresponding to thehead of the attribute does not need to be delexi-calized.
However, we do this to improve the ef-ficiency of pattern-matching, since we will oftenhave patterns for different attributes differing onlyat the attribute vertex.It is important to note that because of the man-ner in which the roles of subject and object wereassigned during seed fact extraction, the patternsReNoun generates clearly show which argumentwill take the role of the subject, and which willtake the role of the object.
This is in contrastto previous work such as Ollie (Mausam at al.,2012), where the assignment depends on the orderin which the arguments are expressed in the sen-tence from which the fact is being extracted.
Forexample, from the sentence ?Opel was describedas GM?s most successful subsidiary.?
and the seedfact (GM, subsidiary, Opel), the pattern thatReNoun generates will consistently extract factslike (BMW, subsidiary, Rolls-Royce), and notthe incorrect inverse, regardless of the relative or-dering of the two entities in the sentence.At this point we have dependency patterns ca-pable of generating more extractions for their seedfact attributes.
For efficient matching, we use theoutput of Algorithm 1 to generate a map from de-pendency patterns to their attributes with entrieslike that shown in Figure 4(a).
This way, a pat-tern match can be propagated to all its mapped at-tributes in one shot, as we explain in Section 5.3.Finally, we discard patterns that do not pass a sup-port threshold, where support is the number of dis-tinct seed facts from which a pattern could be gen-erated.329{A/N} like/INprep {O/N}pobj of/INprep {S/N}pobjattributes: {executive chairman, creative director, ...}(a)?An executive chairman, like Steve Chase of AOL, isresponsible for representing the company.??
(AOL, executive chairman, Steve Chase)(b)?A creative director, like will.i.am of 3D Systems, may alsobe referred to as chief creative officer.??
(3D Systems, creative director, will.i.am)(c)Figure 4: A dependency pattern and its use in extraction: (a)the pattern in our running example and the set of attributes towhich it applies; (b) and (c) sentences matching the patternand the resulting extractions.5.3 Applying the dependency patternsGiven the learned patterns, we can now generatenew extractions.
Each match of a pattern againstthe corpus will indicate the heads of the poten-tial subject, attribute and object.
The noun phraseheaded by the token matching the {A/N} vertex ischecked against the set of attributes to which thepattern is mapped.
If the noun phrase is foundamong these attributes, then a triple (S, A, O) isconstructed from the attribute and the Freebase en-tities to which the tokens corresponding to the Sand O nodes in the pattern are resolved.
This tripleis then emitted as an extraction along with the pat-tern that generated it.
Figure 4(b) and (c) show twosentences that match the dependency pattern in ourrunning example and the resulting extractions.Finally, we aggregate our extractions by theirgenerated facts.
For each fact f , we save the dis-tinct dependency patterns that yielded f and thetotal number of times it was found in the corpus.6 Scoring extracted factsIn this section we describe how we score the can-didate facts extracted by applying the dependencypatterns in Section 5.
Recall that a fact may beobtained from multiple extractions, and assigningscores to each fact (rather than each extraction) en-ables us to consider all extractions of a fact in ag-gregate.We score facts based on the patterns which ex-tract them.
Our scheme balances two character-istics of a pattern: its frequency and coherence.Pattern frequency is defined as the number of ex-has/VBZ {S/N}nsubj children/NNSdobj with/INprep {A/N}pobj {O/N}apposattributes: {ex-wife, boyfriend, ex-partner}frequency(P ) = 574, coherence(P ) = 0.429Example: ?Putin has two children with his ex-wife,Lyudmila.?
(a){A/N} {S/N}nn{O/N} nnattributes: {ex-wife, general manager, subsidiary,... }frequency(P ) = 52349038, coherence(P ) = 0.093Example: ?Chelsea F.C.
general manager Jos?e Mourinho...?
(b)Figure 5: (a) a coherent pattern extracting facts for semanti-cally similar attributes and (b) an incoherent pattern.tractions produced by the pattern.
Our first ob-servation is that patterns with a large number ofextractions are always able to produce correct ex-tractions (in addition to incorrect ones).
We alsoobserve that generic patterns produce more er-roneous facts compared to more targeted ones.To capture this, we introduce pattern coherence,which reflects how targeted a pattern is based onthe attributes to which it applies.
For example,we observed that if an extraction pattern yieldsfacts for the coherent set of attributes ex-wife,boyfriend, and ex-partner, then its output isconsistently good.
On the other hand, a patternthat yields facts for a less coherent set of attributesex-wife, general manager, and subsidiary ismore likely to produce noisy extractions.
Generic,more incoherent patterns are more sensitive tonoise in the linguistic annotation of a document.Figure 5 shows an example pattern for each case,along with its frequency and coherence.We capture coherence of attributes using word-vector representations of attributes that are cre-ated over large text corpora (Mikolov et al., 2013).The word-vector representation v(w) for a wordw (multi-word attributes can be preprocessed intosingle words) is computed in two steps.
First, thealgorithm counts the number of occurrences of aword w1that occurs within the text window cen-tered at w (typically a window of size 10), pro-ducing an intermediate vector that potentially hasa non-zero value for every word in the corpus.The intermediate vector is then mapped to a muchsmaller dimension (typically less than 1000) toproduce v(w).
As shown in (Mikolov et al., 2013),two words w1and w2for which the cosine dis-330tance between v(w1) and v(w2) is small tend tobe semantically similar.
Therefore, a pattern is co-herent if it applies to attributes deemed similar asper their word vectors.Given an extraction pattern P that extracts factsfor a set of attributes A, we define the coherenceof P to be the average pairwise coherence of all at-tributes inA, where the pairwise coherence of twoattributes a1and a2is the cosine distance betweenv(a1) and v(a2).Finally, we compute the score of a fact f bysumming the product of frequency and coherencefor each pattern of f as shown in Equation 1.S(f) =?P?Pat(f)frequency(P )?
coherence(P ) (1)7 Experimental EvaluationWe describe a set of experiments that validate thecontributions of ReNoun.
In Sections 7.2 and 7.3we validate our noun-centric approach: we showthat extractions based on verb phrases cannot yieldthe results of ReNoun and that NomBank, the re-source used by state of the art in semantic role-labeling for nouns, will not suffice either.
In Sec-tions 7.4-7.6 we evaluate the different componentsof ReNoun and its overall quality, and in Sec-tion 7.7 we discuss the cases in which ReNoun wasunable to extract any facts.7.1 SettingWe used the fat head (FH) and long tail (LT) at-tributes and annotated news corpus described inSection 2.
When evaluating facts, we used major-ity voting among three human judges, unless oth-erwise noted.
The judges were instructed to con-sider facts with inverted subjects and objects as in-correct.
For example, while (GM, subsidiary,Opel) is correct, its inverse is incorrect.7.2 Verb phrases are not enoughState-of-art open information extraction systemslike Ollie (Mausam at al., 2012) assume that a re-lation worth extracting is expressed somewhere inverb form.
We show this is not the case and jus-tify our noun-centric approach.
In this experimentwe compare ReNoun to a custom implementationof Ollie that uses the same corpus as ReNoun andsupports multi-word attributes.
While Ollie doestry to find relations expressed as nouns, its seedfacts are relations expressed as verbs.We randomly sampled each of FH and LT for100 attributes for which ReNoun extracts facts andReNoun Ollieflagship company -railway minister -legal affairs correspondent -spokesperson be spokesperson ofpresident-elect be president elect ofco-founder be co-founder ofTable 5: ReNoun attributes with and without a correspondingOllie relation.asked a judge to find potentially equivalent Ol-lie relations.
Note that we did not require thejudge to find exactly the same triple (thereby bias-ing the experiment towards finding more attributematches).
Furthermore, the judge was instructedthat a verb phrase like advised by should be con-sidered a match to the ReNoun attribute advisor.However, looking at the data, most facts involvingthe relation advised are not synonymous with theadvisor relation as we think of it (e.g., ?Obamaadvised Merkel on saving the Euro?).
This obser-vation suggests that there is an even more subtledifference between the meaning of verb expres-sions and noun-based expressions in text.
This ex-periment, therefore, gives an upper bound on thenumber of ReNoun attributes that Ollie can cover.For FH, not surprisingly, we could find matchesfor 99 of the 100 attributes.
However, for LT, only31 of the 100 attributes could be found, even underour permissive setting.
Most attributes that couldnot be matched were multi-word noun phrases.While in principle, one could use the Ollie patternsthat apply to the head of a multi-word attribute, wefound that we generate more interesting patternsfor specific multi-word attributes.
Table 5 showsexamples of attributes with and without verb map-pings in Ollie.We also compare in the other direction and esti-mate the portion of Ollie relations centered aroundnouns for which ReNoun fails to extract facts.
Forthis experiment, we randomly sampled 100 Ollierelations that contained common nouns whose ob-jects are concrete values, and looked for equivalentattributes in ReNoun extractions.
ReNoun extractsfacts for 48 of the Ollie relations.
Among the 52relations with no facts, 25 are not in Biperpedia(which means that ReNoun cannot extract facts forthem no matter what).
For the other 27 relations,ReNoun did not extract facts for the followingreasons.
First, some relations expressed actions,which cannot be expressed using nouns only, andare not considered attributes describing the subjectentity (e.g., citation of in ?Obama?s citation331of the Bible?).
Second, some relations have theobject (a common noun) embedded within them(e.g., have microphone in) and do not have cor-responding attributes that can be expressed us-ing nouns only.
The remaining relations eitherhave meaningless extractions or use common nounphrases as arguments.
ReNoun only uses propernouns (i.e., entities) for arguments because factswith common noun arguments are rarely interest-ing without more context.
We note that the major-ity of the 25 Ollie relations without correspondingBiperpedia attributes also fall into one of the threecategories above.7.3 Comparison against NomBankIn principle, the task of extracting noun-mediatedrelations can be compared to that of semantic rolelabeling (SRL) for nouns.
The task in SRL is toidentify a relation, expressed either through a verbor a noun, map it to a semantic frame, and mapthe arguments of the relation to the various roleswithin the frame.
State of the art SRL systems,such as that of Johansson and Nugues (2008), aretrained on NomBank (Meyers et al., 2004) forhandling nominal relations, which also means thatthey are limited by the knowledge it has.
We askeda judge to manually search NomBank for 100 at-tributes randomly drawn from each of FH and LTfor which ReNoun extracts facts.
For multi-wordattributes, we declare a match if its head word wasfound.
We were able to find 80 matches for theFH attributes and 42 for LT ones.
For example,we could not find entries for the noun attributescoach or linebacker (of a football team).
Thisresult is easy to explain by the fact that NomBankonly has 4700 attributes.In addition, for some nouns, the associatedframes do not allow for the extraction of triples.For example, all frames for the noun member spec-ify one argument only, so in the sentence ?Johnbecame a member of ACM?, the output relation is(ACM, member) instead of the desired triple (ACM,member, John).As we did with Ollie, we also looked at nounsfrom NomBank for which ReNoun does not ex-tract facts.
Out of a random sample of 100 Nom-Bank nouns, ReNoun did not extract facts for29 nouns (four of which are not in Biperpedia).The majority of the missed nouns cannot be usedby ReNoun because they either take single ar-guments (instead of two) or take either preposi-tional phrases or common nouns (instead of propernouns correponding to entities) as one their argu-ments.7.4 Quality of seed factsIn Section 4, we described our method for ex-tracting seed facts for our attributes.
Applyingthe method to our corpus resulted in 139M extrac-tions, which boiled down to about 680K uniquefacts covering 11319 attributes.
We sampled 100random facts from each of FH and LT, and ob-tained 65% precision for FH seed facts and 80%precision for LT ones.
This leads us to two obser-vations.First, the precision of seed facts for LT attributesis high, which makes them suitable for use asa building block in a distant supervision schemeto learn dependency parse patterns.
We are pri-marily interested in LT attributes, which earlierapproaches cannot deal with satisfactorily as wedemonstrated above.Second, LT attributes have higher precision thanFH attributes.
One reason is that multi-word at-tributes (which tend to be in LT) are sometimesincorrectly chunked, and only their head words arerecognized as attributes (which are more likely tobe in FH).
For example, in the phrase ?America?sGerman coach, Klinsmann?, the correct attributeis German coach (LT), but bad chunking may pro-duce the attribute coach (FH) with Germany as thesubject.
Another reason is that FH attributes arelikely to occur in speculative contexts where thepresence of the attribute is not always an asser-tion of a fact.
(While both FH and LT attributescan be subject to speculative contexts, we observethis more for FH than LT in our data.)
For ex-ample, before a person is a railway ministerof a country, there is little mention of her alongwith the attribute.
However, before a person iselected president, there is more media about hercandidacy.
Speculative contexts, combined withincorrect linguistic analysis of sentences, can re-sult in incorrect seed facts (e.g., from ?Republi-can favorite for US president, Mitt Romney, vis-ited Ohio?, we extract the incorrect seed fact (US,president, Mitt Romney)).7.5 Candidate generationUsing the seed facts, we ran our candidate gen-eration algorithm (Section 5).
In the first step ofthe algorithm we produced a total of about 2 mil-lion unique dependency patterns.
A third of these332patterns could extract values for exactly one at-tribute.
Manual inspection of these long tail pat-terns showed that they were either noise, or do notgeneralize.
We kept patterns supported by at least10 seed facts, yielding more than 30K patterns.We then applied the patterns to the corpus.
Theresult was over 460M extractions, aggregated intoabout 40M unique facts.
Of these, about 22M factswere for LT attributes, and 18M for FH.
We nowevaluate the quality of these facts.7.6 Scoring extracted factsIn Section 6, we presented a scheme for scoringfacts using pattern frequency and coherence.
Toshow its effectiveness we (i) compare it againstother scoring schemes, and (ii) show the qualityof the top-k facts produced using this scheme, forvarious k. To compute coherence, we generatedattribute word vectors using the word2vec2tooltrained on a dump of Wikipedia.First, we compare the quality of our scoringscheme (FREQ COH) with three other schemes asshown in Table 6.
The scheme FREQ is identicalto FREQ COH except that all coherences are setto 1.
PATTERN counts the number of distinct pat-terns that extract the fact while PATTERN COHsums the pattern coherences.
We generated a ran-dom sample of 252 FH and LT nouns with no en-tity disambiguation errors by the underlying nat-ural language processing pipeline.
The justifi-cation is that none of the schemes we considerhere capture such errors.
Accounting for sucherrors requires elaborate signals from the entitylinking system, which we leave for future work.For each scoring scheme, we computed the Spear-man?s rank correlation coefficient ?
between thescores and manual judgments (by three judges).
Alarger ?
indicates more correlation, and comput-ing ?
was statistically significant (p-value<0.01)for all schemes.Scheme Spearman?s ?FREQ 0.486FREQ COH 0.495PATTERN 0.265PATTERN COH 0.257Table 6: Scoring schemesFREQ and FREQ COH dominate, which showsthat considering the frequency with which patternsperform extraction helps.
The two schemes, how-ever, are very close to each other.
We observed2https://code.google.com/p/word2vec/FH LTk Precision #Attr Precision #Attr1021.00 8 1.00 501030.98 36 1.00 2941040.96 78 0.98 15481050.82 106 0.96 50931060.74 124 0.70 7821All 0.18 141 0.26 11178Table 7: Precision of random samples of the top-k scoringfacts, along with the attribute yield.that adding coherence helps when two facts havesimilar frequencies, but this effect is temperedwhen considering a large number of facts.Second, we evaluate the scoring of facts gener-ated by ReNoun by the precision of top-k resultsfor several values of k. In this evaluation, factswith disambiguation errors are counted as wrong.The particular context in which ReNoun is appliedwill determine where in the ordering to set thethreshold of facts to consider.
We compute pre-cision based on a sample of 50 randomly chosenfacts for each k. Table 7 shows the precision re-sults, along with the number of distinct attributes(#Attr) for which values are extracted at each k.As we can see, ReNoun is capable of generat-ing a large number of high quality facts (?70%precise at 1M), which our scoring method man-ages to successfully surface to the top.
The ma-jor sources of error were (i) incorrect dependencyparsing mainly due to errors in boilerplate text re-moval from news documents, (ii) incorrect coref-erence resolution of pronouns, (iii) incorrect entityresolution against Freebase, and (iv) cases wherea triple is not sufficient (e.g., ambassador whereboth arguments are countries.
)7.7 Missed extractionsWe analyze why ReNoun does not extract facts forcertain attributes.
For FH, we investigate all the 77attributes for which ReNoun is missing facts.
ForLT, there are about 50K attributes without corre-sponding facts, and we use a random sample of100 of those attributes.Cause FH LT ExampleVague 23 37 cultureNumeric 4 26 rainfallObject not KB entity 11 6 emailPlural 30 15 member firmsBad attribute / misspell 3 4 newsiesValue expected 6 12 nationalityTotal 77 100Table 8: Analysis of attributes with no extractions.333Table 8 shows the categorization of the missedattributes.
The first three categories are cases thatare currently outside the scope of ReNoun: vagueattributes whose values are long narratives, nu-meric attributes, and typed attributes (e.g., email)whose values are not modeled as Freebase enti-ties.
The next two categories are due to limitationsof the ontology, e.g., plural forms of attributes arenot always synonymized with singular forms andsome attributes are bad.
Finally, the ?Value ex-pected?
category contains the attributes for whichReNoun should have extracted values.
One reasonfor missing values is that the corpus itself does notcontain values of all attributes.
Another reason isthat some attributes are not verbalized in text.
Forexample, attributes like nationality are usuallynot explicitly stated when expressed in text.8 Related WorkOpen information extraction (OIE) was introducedby Banko et al.
(2007).
For a pair of noun phrases,their system, TEXTRUNNER, looks for the at-tribute (or more generally the relation) in the textbetween them and uses a classifier to judge thetrustworthiness of an extraction.
WOEparse(Wuand Weld, 2010) extends this by using dependencyparsing to connect the subject and object.
Bothsystems assume that the attribute is between itstwo arguments, an assumption that ReNoun dropssince it is not suitable for nominal attributes.Closest to our work are ReVerb (Fader et al.,2011) and Ollie (Mausam at al., 2012).
ReVerbuses POS tag patterns to locate verb relations andthen looks at noun phrases to the left and right forarguments.
Ollie uses the ReVerb extractions asits seeds to train patterns that can further extracttriples.
While Ollie?s patterns themselves are notlimited to verb relations (they also support nounrelations), the ReVerb seeds are limited to verbs,which makes Ollie?s coverage on noun relationsalso limited.
In comparison, ReNoun take a noun-centric approach and extracts many facts that donot exist in Ollie.cloClausIE (Del Corro and Gemulla, 2013) is anOIE framework that exploits knowledge about thegrammar of the English language to find clausesin a sentence using its dependency parse.
Theclauses are subsequently used to generate extrac-tions at multiple granularities, possibly with morethan triples.
While ClausIE comes with a prede-fined set of rules on how to extract facts from adependency parse, ReNoun learns such rules fromits seed facts.Finally, Nakashole et al.
(2014) and Mintz et al.
(2009) find additional facts for attributes that al-ready have facts in a knowledge base.
In contrast,ReNoun is an OIE framework whose goal is to findfacts for attributes without existing facts.9 ConclusionsWe described ReNoun, an open information ex-traction system for nominal attributes that focuseson the long tail.
The key to our approach is to startfrom a large ontology of nominal attributes and ap-ply noun-specific manual patterns on a large pre-processed corpus (via standard NLP components)to extract precise seed facts.
We then learn a set ofdependency patterns, which are used to generate amuch larger set of candidate facts.
We proposed ascoring function for filtering candidate facts basedon pattern frequency and coherence.
We demon-strated that the majority of long tail attributes inReNoun do not have corresponding verbs in Ol-lie.
Finally, our experiments show that our scor-ing function is effective in filtering candidate facts(top-1M facts are ?70% precise).In the future, we plan to extend ReNoun to ex-tract triples whose components are not limited toFreebase IDs.
As an example, extending ReNounto handle numerical or typed attributes would in-volve extending our extraction pattern learningto accommodate units (e.g., kilograms) and otherspecial data formats (e.g., addresses).AcknowledgmentsWe would like to thank Luna Dong, Anjuli Kan-nan, Tara McIntosh, and Fei Wu for many discus-sions about the paper.334ReferencesS?oren Auer, Christian Bizer, Georgi Kobilarov, JensLehmann, Richard Cyganiak, and Zachary G. Ives2007.
DBpedia: A Nucleus for a Web of Open Data.In Proceedings of the International Semantic WebConference.Michele Banko, Michael J. Cafarella, Stephen Soder-land, Matthew Broadhead, and Oren Etzioni 2007.Open Information Extraction from the Web.
In Pro-ceedings of the International Joint Conference onArtificial Intelligence .Kurt D. Bollacker, Colin Evans, Praveen Paritosh, TimSturge, and Jamie Taylor.
2008.
Freebase: a Col-laboratively Created Graph Database for StructuringHuman Knowledge.
In Proceedings of the Interna-tional Conference on Management of Data.Michael J. Cafarella, Alon Y. Halevy, Daisy Zhe Wang,Eugene Wu, and Yang Zhang 2008.
WebTables:Exploring the Power of Tables on the Web.
In Pro-ceedings of the VLDB Endowment.Luciano Del Corro and Rainer Gemulla.
2013.ClausIE: Clause-based Open Information Extrac-tion.
In Proceedings of the International World WideWeb Conference.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning 2006.
Generating TypedDependency Parses from Phrase Structure Parses.In Proceedings of Language Resources and Evalu-ation.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying Relations for Open InformationExtraction.
In Proceedings of Empirical Methods inNatural Language Processing.Rahul Gupta, Alon Halevy, Xuezhi Wang, StevenWhang, and Fei Wu.
2014.
Biperpedia: An On-tology for Search Applications.
In Proceedings ofthe VLDB Endowment.Aria Haghighi and Dan Klein 2009.
Simple Corefer-ence Resolution with Rich Syntactic and SemanticFeatures.
In Proceedings of Empirical Methods inNatural Language Processing.Richard Johansson and Pierre Nugues 2008.
The Ef-fect of Syntactic Representation on Semantic RoleLabeling.
In Proceedings of the International Con-ference on Computational Linguistics.Taesung Lee, Zhongyuan Wang, Haixun Wang, andSeung-won Hwang 2013.
Attribute Extraction andScoring: A Probabilistic Approach.
In Proceedingsof the International Conference on Data Engineer-ing .Mausam, Michael Schmitz, Stephen Soderland, RobertBart, and Oren Etzioni.
2012.
Open LanguageLearning for Information Extraction.
In Proceed-ings of Empirical Methods in Natural LanguageProcessing.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient Estimation of Word Repre-sentations in Vector Space.
arXiv.Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-rafsky.
2009.
Distant Supervision for Relation Ex-traction Without Labeled Data.
In Proceedings ofthe Association for Computational Linguistics.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004.
Annotating Noun Ar-gument Structure for NomBank.
In Proceedings ofLanguage Resources and Evaluation.Nitin Madnani and Bonnie J. Dorr.
2010.
Generat-ing Phrasal and Sentential Paraphrases: A Survey ofData-Driven Methods.
In Computational Linguis-tics 36(3).Ndapandula Nakashole, Martin Theobald, and GerhardWeikum.
2011.
Scalable Knowledge Harvestingwith High Precision and High Recall.
In Proceed-ings of Web Search and Data Mining.Marius Pasca.
2014.
Acquisition of Open-domainClasses via Intersective Semantics.
In Proceedingsof the International World Wide Web Conference.Marius Pasca and Benjamin Van Durme.
2007.
WhatYou Seek Is What You Get: Extraction of Class At-tributes from Query Logs.
In Proceedings of theInternational Joint Conference on Artificial Intelli-gence .Uma Sawant and Soumen Chakrabarti.
2013.
Learn-ing Joint Query Interpretation and Response Rank-ing.
In Proceedings of the International World WideWeb Conference.Amit Singhal.
2012 Introducing theKnowledge Graph: things, not stringshttp://googleblog.blogspot.com/2012/05/introducing-knowledge-graph-things-not.htmlFei Wu and Daniel S. Weld.
2010.
Open InformationExtraction Using Wikipedia.
In Proceedings of thethe Association for Computational Linguistics.Mohamed Yahya, Klaus Berberich, Shady Elbas-suoni, Maya Ramanath, Volker Tresp, and GerhardWeikum.
2012.
Natural Language Questions for theWeb of Data.
In Proceedings of Empirical Methodsin Natural Language Processing.335
