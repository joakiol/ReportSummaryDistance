Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1636?1647, Dublin, Ireland, August 23-29 2014.Learning to Summarise Related SentencesEmmanouil Tzouridis??Dep.
of Computer ScienceTU DarmstadtGermany{tzouridis,brefeld}@kma.informatik.tu-darmstadt.deJamal Abdul NasirDep.
of Computer ScienceLUMS LahorePakistanjamaln@lums.edu.pkUlf Brefeld???Inform.
Center f. EducationDIPF Frankfurt/MainGermanybrefeld@dipf.deAbstractWe cast multi-sentence compression as a structured prediction problem.
Related sentences arerepresented by a word graph so that summaries constitute paths in the graph (Filippova, 2010).We devise a parameterised shortest path algorithm that can be written as a generalised linearmodel in a joint space of word graphs and compressions.
We use a large-margin approach toadapt parameterised edge weights to the data such that the shortest path is identical to the desiredsummary.
Decoding during training is performed in polynomial time using loss augmented infer-ence.
Empirically, we compare our approach to the state-of-the-art in graph-based multi-sentencecompression and observe significant improvements of about 7% in ROUGE F-measure and 8%in BLEU score, respectively.1 IntroductionAutomatic text summarisation is one of oldest forms of natural language processing (Luhn, 1958; Bax-endale, 1958).
The goal is to extract the most important part of the content from either a single documentor a collection of documents (Mani, 2001; Roussinov and Chen, 2001; McKeown et al., 2005).Frequently, the information of interest is contained in only a part of a sentence or may be distributedacross parts of several sentences.
Identifying the content carrying part(s) constitutes an essential tech-nique not only for single- and multi-document extractive summarisation but also text simplification ingeneral.
Generating a simplified version of a text traditionally has many applications in question answer-ing (Hermjakob et al., 2002) and speech synthesis (Kaji et al., 2004).
Due to limited display sizes ofmobile devices, recent applications also deal with summarising/simplifying news articles, social media,emails, or websites (Corston-Oliver, 2001).Multi-sentence compression (MSC) unifies many of the mentioned characteristics and challenges andcan be seen as a key to text summarisation and simplification (Jing and McKeown, 2000).
The task inmulti-sentence compression is to map a collection of related sentences to a grammatical short sentencethat preserves the most important part of the content.
Sentence compression methods have been devisedusing manually crafted rules (Dorr et al., 2003), language models (Hori et al., 2003; Clarke and Lapata,2008), or syntactical representations (Barzilay and Lee, 2003; Galley and McKeown, 2007; Filippova andStrube, 2008).
Filippova (2010) introduces an elegant graph-based approach to multi-sentence compres-sion that simply relies on the words of the sentences and efficient dynamic programming.
Her approachimplements the observation that the frequency of words influences their appearance in human summaries(Nenkova et al., 2006).
Although being an intuitive rule that does work well in practice, frequency-basedstrategies often remain heuristic.In this paper we propose a structured learning-based approach to multi-sentence compression.
Inanalogy to Filippova (2010), related sentences are represented by a word graph (the input).
Words areidentified with vertices and directed edges connect adjacent words in at least one sentence, so that thesummarising sentence (the output) is contained as a path in the graph.
Generally, learning mappingsbetween complex structured and interdependent inputs and outputs challenges the standard model ofThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/1636learning a mapping from independently drawn instances to a small set of labels.
To capture the involveddependencies we represent input graphs G and output paths p jointly by a (possibly rich) feature repre-sentation ?
(G, p).
The goal is to find a linear function f(G, p) = ?>?
(G, p) in joint space such thatp = argminp?f(G, p?)
(1)is the desired summary for the collection G. Our approach can therefore be seen as translating the workby Filippova (2010) into the structured prediction framework (Tsochantaridis et al., 2005; Taskar et al.,2004).
Instead of applying heuristics, we adapt the decoding machinery to the data by parameterisinga shortest path algorithm.
The latter admits a representation as a generalised linear model in joint in-put output space.
We devise a structural support vector machine (SVM) (Tsochantaridis et al., 2005)to learn the shortest path in possibly high dimensional joint feature spaces and propose a generalised,loss-augmented decoding algorithm that is solved exactly by an integer linear program in polynomialtime.
Empirically, we evaluate the structural support vector machine on a real world news headlinesummarisation task.
Our experiments show that a very rudimentary set of five features already sufficesto significantly improve the state-of-the-art in graph-based multi-sentence compression.
We observe anincrease of 7% in in ROUGE F-measure and 8% in BLEU score, respectively.The remainder of the paper is organised as follows.
Section 2 reviews related work and Section 3introduces word graphs and shortest paths.
Our technical contribution is presented in Section 4.
Wereport on empirical results in Section 5 and Section 6 concludes.2 Related WorkThe goal of automatic text summarisation is to produce a summary of a given text (or text collection)that preserves the most important information (Luhn, 1958; Edmundson, 1969).
Summarisation systemsusually rely on clues or features that help to identify key elements such as the main topic of a document(Salton et al., 1994).
Such features may be extracted from sentences (e.g., the length of a sentence, itsposition in the text), words (e.g., frequency of a word, relative position in sentence) as well as from styleand structure elements (Kupiec et al., 1995; Teufel and Moens, 1997; Marcu, 1997).A special case of text summarisation is sentence compression; given a sentence, the task is to producea summary of the input that preserves the most important information and is grammatically correct (Jing,2000).
Sentence compression is thus relevant for many NLP tasks including question answering, machinetranslation, text simplification, speech synthesis applications and multi-sentence compression (e.g., Lin(2003)).Multi-sentence compression extends sentence compression to collections of related sentences that areto be summarised in a single output sentence.
Traditionally, contributions to multi-sentence compressionexploit linguistic properties based on lexical information and syntactic dependencies.
Dorr et al.
(2003)for instance propose a headline generation system based on linguistically-motivated, hand-crafted heuris-tics.
Barzilay and Lee (2003) study sentence compression with dependency trees.
The aligned trees arerepresented by a lattice from which a summary is extracted by an entropy-based criterion over all possi-ble traversals of the lattice.
Similarly, Barzilay and McKeown (2005) combine syntactic trees of similarsentences by a multi-sequence alignment candidate selection and summary generation.
Wan (2007) de-ploys a language model in combination with maximum spanning trees to rank candidate aggregationssatisfying grammatical constraints.
Hori et al.
(2003) propose a statistical model for automatic speechsummarisation without using parallel data or syntactic information.
Instead they focus on language mod-els to provide a scoring function and use dynamic programming for searching the compression with thehighest score.
Clarke and Lapata (2008) cast sentence compression as an optimisation problem.
Theyuse linguistically motivated constraints and integer linear programming to infer globally optimal com-pressions.Recently, graph-based approaches to multi-sentence compression have been proposed.
The underly-ing idea is that syntax may help to find important content.
Thus, instead of using hand-crafted rules,parsers, or language models, a simple and robust graph-based method can be used to generate reason-able summaries.
Graph-based multi-sentence compression approaches identify the summary with the1637shortest path in word graphs (Filippova, 2010).
Shortest paths of unweighted word graphs however donot necessarily lead to satisfying summaries.
As a remedy, Filippova (2010) introduces heuristic edgeweights based on normalised frequencies of the connected words.
Boudin and Morin (2013) propose anadditional re-ranking scheme to identify summarisations that contain key phrases.
The underlying ideais that particular key phrases give rise to certain topics and thus lead to more informative aggregations.In this paper, we parameterise the graph-based framework by Filippova (2010) such that the short-est path algorithm is adapted to labeled data at hand.
Adapting the dynamic programming to the datarenders the use of heuristics unnecessary.
Instead, word graphs and compressions are embedded in a(possibly high-dimensional) joint feature space where a generalised linear scoring function learns to sep-arate between compressions of different quality.
We develop a generalised, loss-augmented shortest pathalgorithm that is solved exactly by a (relaxed) integer linear program in polynomial time.3 Preliminaries3.1 Word GraphsIn a nutshell, word graphs represent collections of sentences efficiently in a graph by mapping identicalwords to a single vertex while the graph structure preserves the local neighbourhood of words.From a collection of related sentences a word graph is constructed as follows: Initially, every sentenceis augmented by a preceding start token ?S?
and a terminal end symbol ?E?
so that beginning and endof the sentences are preserved in the final graph.
Starting with the empty graph, sentences are addedone after another.
The first word of the first sentence is the auxiliary ?S?
that is converted into the firstvertex v?S?.
The second word of the first sentence also becomes a vertex v and the two vertices areconnected with a directed edge v?S??
v. The procedure continues with the third word and so on untilthe end symbol ?E?
is reached.
The other sentences are incorporated analogously.
A special case arisesif the graph already contains a vertex v that is identical to the word that is just to be added.
Insteadof adding a redundant vertex, the already existing vertex v is used and, if v 6= v?S?, connected to therespective predecessor as before.
In that case, the vertex v has an in-degree of (at least) two and is usedas the predecessor for the next word to be added.
The procedure continues until all n sentences areincorporated in the graph.Note that merging nodes to the same vertex requires an appropriate preprocessing of the sentences.Simple lower- or upper-case representations of words often suffice but more complex preprocessingschemas are also possible such as merging vertices carrying synonyms or words possessing small Word-Net distances (Miller, 1995; Fellbaum, 1998).
As word graphs are a condensed representation of theinput sentences, word graphs are also known as compression graphs.
The described construction givesus a directed graph G = (V, E), where V is the set of unique words in the sentences and E the set ofneighbouring words.
An exemplary word graph is shown in Figure 1.3.2 Shortest Path AlgorithmsGiven a directed weighted graph G = (V, E , cost) where V is the set of vertices and E ?
V ?
V the setof edges.
The function cost : (v, v?)
7?
<+assigns positive weights to every edge (v, v?)
?
E .
A pathp from a vertex vs?
V to a vertex ve?
V is a sequence of edges connecting vertices of G. We writeP(vs, ve) to denote the set of all possible paths starting in vsand terminating in ve.
The cost of a path isgiven by the sum of the weights of the edges on the path.The shortest path from a start vertex vs?
V to an end vertex ve?
V is defined as the path in G fromvsto vewith the lowest costs.
Introducing auxiliary binary variables p(v,v?
)indicating whether an edge(v, v?)
?
E lies on the path (pv,v?= 1) or not (pv,v?= 0) the shortest path can be computed by thefollowing optimisation problemp?= argminp?(v,v?
)?Epv,v?cost(v, v?)
s.t.
p ?
P(vs, ve).
(2)There exist many algorithms for computing shortest paths efficiently (Bellman, 1958; Ford, 1956; Dijk-stra, 1959).
Usually, these methods are based on dynamic programming or (relaxed) integer program-1638Figure 1: The word graph constructed from the sentences: ?Yahoo in rumoured $1.1bn bid to buy white-hot Tumblr?, ?Yahoo buys Tumblr as David Karp stays as CEO?, ?Yahoo to buy Tumblr for $1.1bn?.The shortest path is highlighted.ming, where an approximation of the exact quantity is iteratively updated until it converges to the correctsolution, which is achieved in polynomial time.
A prominent algorithm for computing the k-shortestpaths is Yen?s algorithm (Yen, 1971).
Intuitively, the approach recursively computes the second bestsolution by considering deviations from the shortest path, the third best solution from the previous twosolutions, and so on.
Figure 1 visualises the shortest path for the displayed compression graph.4 Learning to Summarise Related Sentences4.1 Problem SettingGiven a word graph G, we aim to find a ranking function f(G, p) that assigns the lowest score to thebest summary p?, that is, p?
!= argminpf(G, p).
Note that f is defined jointly on G and p to allow forexploiting dependencies between word graph and summary.
Our approach can thus be seen as an instanceof structured prediction models.
The quality of f is measured by the Hamming loss ?, ?
(p?, p?)
=12?
(vi,vj)?V[[p?ij6= p?ij]], that details differences between the best summary p?and the prediction p?,where [[z]] is the indicator function returning one if z is true and zero otherwise.
The generalisation erroris given byR[f ] =??
(p, argminp?f(G, p?
))dP (G, p)and approximated by its empirical counterpart?R[f ] =m?i=1?
(pi, argminp?f(Gi, p?
))(3)on a finite m-sample of pairs {(Gi, pi)}mi=1where Giis a word graph and pithe best summarising sen-tence.
However, minimising the empirical risk directly leads to an ill-posed optimisation problem as1639there generally exist many indistinguishable but equally well solutions realising an empirical loss ofzero.
We thus focus on the minimisation of the regularised empirical risk?Rreg[f ] = ?
(f) +m?i=1?
(pi, argminp?f(Gi, p?
)).The additive regularisation ?
(f) acts like a prior on f , e.g.
to enforce smooth solutions.
In the remainderwe use ?
(f) = ?f?2.4.2 RepresentationThe idea of our approach is as follows: We adapt the cost function of the graph to the training sample suchthat the shortest path of the compression graph is identical to the desired summary.
Recall the generalform of the cost function of Section 3.2.
Instead of a constant or hand-crafted function (Filippova, 2010),we deploy a linear mixture of features ?i, parameterised by ?,cost(v, v?)
=?i?i?i(v, v?)
= ?>?
(v, v?
).Features ?i(v, v?)
are drawn from adjacent vertices v, v?in the word graph to capture local dependenciesof the connecting edge.
Examples for feature functions are frequency-based counts or indicators such asPOS-transitions of the form ?234(v, v?)
= [[v is a noun?v?is a verb]].
Note that complex features usingthe context of the edge are straight forward by extending the feature representation to the input graph?
(v, v?,G).
The final feature vector is obtained by stacking-up all feature functions, that is, ?
(v, v?)
=(.
.
.
, ?i(v, v?
), .
.
.
)>.Using the parameterised costs in the computation of the shortest path in Equation (2) yields the fol-lowing objective function (ignoring the constraints for a moment) that can be rewritten as a generalisedlinear model in joint input output space?(vi,vj)?Vpij?>?
(vi, vj) = ?>???(vi,vj)?Vpij?
(vi, vj)???
??
???
(G,p)= ?>?
(G, p) = f(G, p)where the joint feature representation is given by?
(G, p) ????(vi,vj)?Vpij?
(vi, vj)?
?.Decoding the shortest path p?
for a fixed parameter vector ?
can now be computed byp?
= argminpf(G, p) s.t.
p ?
P(?S?, ?E?
)using standard shortest path algorithms (Yen, 1971).
In addition, reformulating the objective as a gener-alised linear model allows to adapt the parameters ?
to the data to identify shortest paths with summaries.4.3 OptimisationRecall that the goal of the optimisation is to find the ranking function f(G, p) that takes the smallest valuefor the best summary.
That is, for the i-th training instance (Gi, pi), we aim at fulfilling the constraints?>?
(Gi, p)?
?>?
(Gi, pi) > 0 (4)for all p ?
P(?S?, ?E?)\pi.
We extend the constraints in Equation (4) by a term that induces a marginbetween the best path piand all alternative paths.
A common technique is called margin-rescaling andimplies to scale the margin with the actual loss that is induced by decoding p?
instead of pi.
Thus,1640rescaling the margin by the loss implements the intuition that the confidence of rejecting a mistakenoutput is proportional to its error.
In the context of learning shortest paths, margin-rescaling gives us thefollowing constraints?>?
(Gi, p?)?
?>?
(Gi, pi) > ?
(pi, p?)?
?ifor all p ?
P(?S?, ?E?)\pi.
The non-negative ?i?
0 is a slack-variable that allows point-wise relaxationsof the margin.
Solving the equation for ?ishows that margin rescaling also effects the hinge loss thatnow augments the structural loss ?,`?
(Gi, pi, f) = max[minp?[?
(pi, p?)?
f(Gi, p?)
+ f(Gi, pi)]].The effective hinge loss upper bounds the structural loss ?
for every pair (Gi, pi) and trivially alsom?i=1`?
(Gi, pi, f) ?m?i=1?
(pi, argminp?f(Gi, p?))holds.
A max-margin approach to learning shortest paths therefore leads to the following optimisationproblem that is also known as structural support vector machine (Tsochantaridis et al., 2005)min?,??0??
?2+ Cm?i=1?is.t.
?i ?p?
?
P\pi: f(p?)?
f(pi) > ?
(pi, p?)?
?i.
(5)The parameter C trades-off margin maximisation and error minimisation and needs to be adjusted bythe user.
The above optimisation problem can be solved efficiently by cutting plane methods.
The ideabehind cutting planes is to instantiate only a minimal subset of the exponentially many constraints.
Thatis, for the i-th training example, we decode the shortest path p?
given our current model and consider twocases: (i) For p?
6= pithe prediction is erroneous and p?
is called the most strongly violated constraintas it realises the smallest function value and f(Gi, p?)
< f(Gi, p) holds for all p 6= p?.
Consequentially,the respective constraint of the above optimisation problem is instantiated and influences the subsequentiterations.
(ii) If instead the prediction is correct, that is p?
= pi, we need to verify that the runner-up p?
(2)fulfils the margin constraint.
If so, we proceed with the next training example, otherwise we instantiatethe corresponding constraint, analogously to case (i).
Luckily, we do not need to rely on an expensivetwo-best shortest path algorithm but can compute the most strongly violated constraint directly via thecost functionQ(p?)
= ?
(pi, p?)?
?>?
(Gi, p?)
+ ?>?
(Gi, pi) (6)that has to be maximised wrt p?.
The following proposition shows that we can equivalently solve a shortestpath problem for finding the maximiser of Q.Proposition 1.
The argmax p?
of Q in Equation (6) can be computed by minimising a shortest pathproblem with cost function cost(vi, vj) = pij+ ?>?
(vi, vj).Proof.
We treat the ground truth paths p as graphs and write V(p) for the set of nodes on the path andE(p) to denote the set of edges that lie on the path.
If, for instance, an element of the binary adjacencymatrix representing path p equals one, e.g., pij= 1, we write pi, pj?
V(p) and (pi, pj) ?
E(p).
First,note that the Hamming loss can be rewritten as?
(p, p?)
=?(pi,pj)?E(p)(1?
pijp?ij) .1641We havep?
= argmaxp??
(p, p?)
+ ?>?
(Gi, p)?
?>?
(Gi, p?
)= argmaxp??
(p, p?)?
?>?
(Gi, p?
)= argmaxp??(pi,pj)?E(p)(1?
pijp?ij)?
?>?
(Gi, p?
)= argmaxp???(pi,pj)?E(p)pijp?ij?
?>?
(Gi, p?
)= argminp??
(pi,pj)?E(p)pijp?ij+ ?>?
(Gi, p?
)= argminp??
(pi,pj)?E(p)pijp?ij+ ?>[?(xi,xj)?E(G)p?ij?
(vi, vj)]= argminp??
(vi,vj)?E(G)pijp?ij+ ?>[?(xi,xj)?E(G)p?ij?
(vi, vj)]= argminp??
(vi,vj)?E(G)[pij+ ?>?
(vi, vj)]p?ijThe output p?
is the shortest path with costs given by pij+ ?>?
(vi, vj).Using this result, the following lemma shows that we can compute the most strongly violated constraintdirectly by a linear program.Lemma 1.
The maximizer p?
of function Q in Equation (6) and thus the shortest path of Proposition 1can be computed in polynomial time by the following linear programminp?
?ij(pij+ ?>?
(vi, vj))p?ijsubject to the constraints?k ?
V(G)\{?S?, ?E?}
:?jp?kj??ip?ik?
0 ?
??jp?kj+?ip?ik?
0?jp??S?,j??ip?i,?S??
1 ?
??jp??S?,j+?ip?i,?S??
?1?ip?i,?E???jp??E?,j?
1 ?
??ip?i,?E?+?jp??E?,j?
?1?
(i, j) : p?ij?
G(i,j)?
?
(i, j) : p?ij?
{0, 1}.Proof.
For lack of space, we only motivate the constraints.
The first line of constraints guarantees thatevery inner node of the path has exactly as many incoming as outgoing edges, the second line forces thepath to start in v?S?and, analogously, the third line ensures that it terminates in v?E?.
The last line ofconstraints requires the edges of the path p?
to adhere to existing paths of G.4.4 ParallelisationUsing the result by Zinkevich et al.
(2011) the proposed approach can easily be distributed on severalmachines.
Note that cutting planes treat one input (G, p) at a time.
Thus, several models can be trainedindependently in parallel on disjoint subsets of the data.
A subsequent merging process aggregates themodels where each models impact is proportional to the amount of data it has been trained on.
Note thatthe described parallelisation can easily be realised by the MapReduce/Hadoop framework.
Processingtraining instances and updating local models is performed by (one or more) mappers while the mergeoperation is carried out by a reduce task.1642Table 1: Left: Collection of related sentences.
Right: Candidate compressions and number of votes.related sentencesWhite House: Hong Kong had ?plenty of time?
to stop Snowden live coverageEdward Snowden leaves reporters chasing shadows around an airportUS warns Moscow not to let Edward Snowden escape RussiaWikiLeaks forced to defend Ecuador as Edward Snowden seeks asylumSnowden is ?not on plane?
to Cubasummary #snowden seeks asylum 5snowden live coverage 5snowden escape russia 1edward snowden seeks asylum 3wikileaks forced to cuba 15 Empirical Results5.1 Data PreparationWe crawl RSS feeds of 6 major news sites and harvest news headlines of a predefined set of categoriesincluding sports, technology, and business.
The headlines are processed automatically by a spectral clus-tering.
The data is thus transformed into a fully connected graph where vertices correspond to headlinesand edges are weighted by the number of shared non-stopwords.
The clustering is performed for eachcategory on a daily basis.
Resulting clusters are headlines that belong (with high probability) to the sameevent and form our related input sentences.
Groups with less than five sentences are discarded.To identify the best summaries, we conduct a crowd sourcing experiment on Crowdflower1.
Everyannotator is given a group of related sentences together with 10 possible summaries generated by a 10-best Yen?s algorithm (Yen, 1971).
The task of the annotator is to pick the best summary or mark thecollection as inappropriate.
Each collection is labeled by at least 10 annotators.
The group is discardedif the majority of the annotators mark the group as inappropriate.
Otherwise, the three most frequentsummaries are extracted, ties are broken by the authors.
The most frequent summary is used as theground-truth annotation in the learning phase, the other two are used additionally in the evaluation.
Thedescribed process leaves us with 1024 sentences that are divided into 164 annotated groups of relatedsentences.
Table 1 shows an exemplary collection of related sentences (left) and a selection of summariestogether with the number of votes from the annotators.
The overall distribution of votes is displayed inFigure 2.
The figure shows the mean value per rank of all 164 normalised and sorted histograms.
Thebest summary receives on average 8% more votes than the runner-up (not shown).Figure 2: Distribution of annotations.5.2 Baselines and FeaturesWe compare our learning approach to graph-based sentence compression techniques proposed by Filip-pova (2010), Boudin and Morin (2013).
The two baselines construct word graphs as presented in Section3.1 and output the weighted shortest path.
Filippova (2010) uses a frequency-based heuristic for edgesweights and Boudin and Morin incorporate a keyphrase detection framework to re-rank summaries ac-cording to the number and importance of keyphrases found.
In addition, we also include an unweightedshortest path strategy which is a straight forward application of Yen?s algorithm (Yen, 1971) and triviallyreturns the shortest path in terms of the number of edges.
Additional straw men are a random (Random)input sentence and the shortest input sentence (Shortest).In our learning-based approach, every edge between vertices v and v?is associated with a featurevector.
Let w = #(v) the frequency of word v, w?the analogue for v?, e = #(v, v?)
the fre-quency of the edge, and n = |V| the number of vertices in the graph.
The feature vector ?
(v, v?)
of1http://crowdflower.com1643Table 2: ROUGE F-measure scorestraining set size22 35 48 61 74 87 100R1 Random 46.72 46.82 46.41 46.20 46.39 46.53 46.88Shortest 45.93 45.77 46.39 46.56 47.01 47.59 48.04Yen 45.14 44.47 45.12 45.13 45.63 46.14 46.39Filippova 52.70 52.94 52.16 52.02 52.22 52.45 51.81Boudin 52.72 53.12 53.43 53.52 53.10 52.81 52.35SVM 48.39 50.30 55.09 54.59 57.39 54.89 57.66R2 Random 30.43 30.63 30.56 30.31 30.38 30.64 31.09Shortest 27.65 27.43 27.90 27.93 28.64 29.47 30.10Yen 31.38 30.82 31.16 31.40 31.90 32.30 32.56Filippova 36.12 36.52 35.56 35.49 35.75 35.98 35.64Boudin 36.71 37.01 37.79 37.65 36.97 36.75 36.31SVM 33.64 35.40 40.46 40.68 43.44 40.45 43.58RW1.2 Random 35.91 35.97 35.74 35.58 35.80 35.93 36.07Shortest 34.47 34.29 34.77 34.85 35.32 35.9 36.16Yen 34.85 34.26 34.74 34.83 35.22 35.62 35.77Filippova 40.30 40.53 39.88 39.70 39.94 40.12 39.56Boudin 40.79 40.99 41.37 41.31 40.92 40.83 40.36SVM 37.94 39.06 42.61 42.33 44.63 42.90 45.00the edge v ?
v?consists of the normalised joint frequency ?1(w,w?)
=en, the maximal word fre-quency ?2(w,w?)
= max{wn,w?n}, the lexical relevance ?3(w,w?)
=2nw?w?w+w?, the normalised PMI?4(w,w?)
= (logew?w?
)/ ?
logen(Bouma, 2009), and ?5captures the average location of the phrase inthe input sentences (Turney, 2000),?5(w, w?)
=????
?1.0 : [0?
10]%0.4 : [10?
30]%0.8 : [30?
60]%0.6 : [60?
80]%1.0 : [80?
100]%.Note that ?i?
[0, 1] holds for 1 ?
i ?
5.
Also note that ?
denotes a rudimentary set of features.Elaborate representations could for instance also contain POS-tags or named entities.5.3 Experimental Setup and ResultsFor the news headline experiment, we draw m ?
{22, 35, 48, 61, 74, 87, 100} training instances withoutreplacement at random from the collected data.
The remaining instances are split randomly into equallysized holdout and test sets.
We perform model selection for adjusting the trade-off parameter of thesupport vector machine on the interval C ?
[2?10, 212].
We report average ROUGE F-measures (Lin,2004) and BLEU scores (Papineni et al., 2012) over 10 repetitions with distinct training, holdout, andtest sets.
In each repetition, all algorithms are trained and/or evaluated on identical data splits.ROUGE measures the concordance of system and human generated summaries by determining n-gram, word sequence, and word pair matches.
We use unigrams (R1), bigrams (R2), and the weightedlongest common subsequence (RW1.2) to evaluate compressions.
Note that R1 has been found to corre-late well with human evaluations based on various statistical metrics (Lin and Hovy, 2003).
Moreover,R1 and R2 emulate human pyramid and responsiveness scores (Owczarzak et al., 2012).Table 2 shows the resulting ROUGE scores for the news headline experiment.
Significant results aremarked in bold face according to a paired t-test using a significance level of 5%.
For small training sets,the structural support vector machine performs only slightly better than the unweighted application ofYen?s algorithm and is clearly outperformed by the unsupervised baselines.
However, the SVM improves1644Table 3: BLEU scorestraining set size22 35 48 61 74 87 100B1 Random 38.56 38.40 36.49 36.77 36.00 36.87 37.35Shortest 37.45 38.37 38.46 37.25 37.28 37.17 36.64Yen 29.46 28.3 29.39 29.98 29.99 31.20 30.64Filippova 44.26 43.29 44.66 44.57 45.21 43.10 43.52Boudin 44.00 42.54 44.75 44.39 44.80 43.22 43.96SVM 39.60 41.96 48.44 47.10 50.20 46.90 50.39B2 Random 34.85 34.80 33.12 33.48 32.65 33.77 34.12Shortest 33.34 34.27 34.54 33.39 33.39 33.43 33.45Yen 28.51 27.27 28.34 28.74 29.06 30.05 29.73Filippova 39.92 39.36 40.05 40.27 41.14 39.26 39.60Boudin 39.43 38.45 39.99 40.02 40.52 39.20 39.84SVM 36.37 38.63 45.31 44.15 47.40 43.75 47.44B3 Random 35.91 35.97 35.74 35.58 35.80 35.93 36.07Shortest 34.47 34.29 34.77 34.85 35.32 35.90 36.16Yen 27.85 26.64 27.61 27.84 28.38 29.34 28.93Filippova 36.07 35.88 35.86 36.42 37.37 35.55 35.97Boudin 35.05 34.39 35.40 35.76 36.17 34.93 35.85SVM 33.26 35.39 42.31 41.05 44.54 40.52 44.51continuously with increasing training set sizes and outperforms the baselines significantly for more than50 training examples.
The unsupervised baselines cannot utilise the valuable annotations of the data andremain constant.
For 100 training instances, we observe performance improvements of about 5-7% forall three ROUGE F-measures.The BLEU metric computes scores for individual segments, then averages these scores over the wholecorpus for a final score.
For our experiments we use BLEU-1, BLEU-2 and BLEU-3 to evaluate com-pressions.
Table 3 shows the corresponding results, significant results are again marked in bold faceaccording to a paired t-test with a significance level of 5%.
The table draws a similar picture than theprevious one.
The SVM continuously improves the performance with increasing training set sizes andbeats the baselines again at about 50 training examples significantly.
For 100 training instances, all threeBLEU scores are improved by about 7-8%, respectively.5.4 AnalysisThe Pearson correlation between BLEU scores per instance and the number of vertices is -0.1886.
Thenegative correlation implies that summarising larger word-graphs is more challenging.
A negative corre-lation of -0.1267 is also observed for the lexical diversity of the collection; diverse groups of sentencesare thus more difficult to summarise.
A possible remedy could be features that are not frequency-based,such as POS-transitions.
By contrast, the density of the graph given by |E|/|V|(|V| ?
1) shows a positivecorrelation of 0.1851.
The more dense a graph, the more edges interconnect vertices and there exist morepaths.
These paths however frequently pass through the same vertices and as a consequence the lexicaldiversity is low.
A positive correlation of graph density is therefore closely connected to a negativecorrelation of lexical diversity.6 ConclusionWe proposed to learn shortest paths in word graphs for multi-sentence compression.
A shortest pathalgorithm is parameterised and adapted to labeled data at hand using the structured prediction frame-work.
Word graphs and summaries are embedded in a joint feature space where a generalised linearscoring function learns to separate between compressions of different quality.
Decoding is performed1645by a generalised, loss-augmented shortest path algorithm that can be solved by an integer linear pro-gram in polynomial time.
Empirically, we observe that a very rudimentary set of five features suffices tosignificantly improve the state-of-the-art in graph-based multi-sentence compression.AcknowledgmentsJamal Abdul Nasir is supported by a grant from the Higher Education Commission, H-9 Islamabad,Pakistan.ReferencesR.
Barzilay and L. Lee.
2003.
Learning to Paraphrase: An Unsupervised Approach Using Multiple-SequenceAlignment, Proceedings of NAACL-HLT.R.
Barzilay and K. R. McKeown.
2005.
Sentence Fusion for Multidocument News Summarization, Comput.Linguist.
31(3), 297?328.P.
Baxendale 1958.
Machine-made index for technical literature - an experiment, IBM Journal of ResearchDevelopment, 2(4):354?361.R.
Bellman 1958.
On a routing problem, Quarterly of Applied Mathematics 16:87?90.F.
Boudin and E. Morin.
2013 Keyphrase Extraction for N-best reranking in multi-sentence compression, Pro-ceedigs of NAACL-HLTG.
Bouma.
2009.
Normalized (pointwise) Mutual information in collocation extraction, Proceedings of GSCL.J.
Clarke and M. Lapata.
2008.
Global inference for sentence compression: An integer linear programmingapproach, Journal of Artificial Intelligence Research, 31:399?429.S.
H. Corston-Oliver 2001.
Text compaction for display on very small screens, Proceedings of the NAACLWorkshop on Automatic Summarization.E.
W. Dijkstra 1959.
A note on two problems in connexion with graphs, Numerische Mathematik 1:269?271.B.
Dorr, D. Zajic, and R. Schwartz.
2003.
Hedge trimmer: A parse-and-trim approach to headline generation,Proceedings of the HLT-NAACL Workshop on Text Summarization.H.
P. Edmundson 1969.
New methods in automatic extracting, Journal of the ACM, 16(2):264?285.C.
Fellbaum (Ed.).
1998.
WordNet: An Electronic Lexical Database, Cambridge, MA: MIT Press.K.
Filippova.
2010.
Multi-sentence compression: Finding shortest paths in word graphs, Proceedings of COLING.K.
Filippova and M. Strube.
2008.
Dependency tree based sentence compression, Proceedings of INLG.J.
Ford, R. Lester 1956.
Network Flow Theory, Paper P-923, Santa Monica, California: RAND Corporation.M.
Galley and K. R. McKeown.
2007.
Lexicalized Markov grammars for sentence compression, Proceedings ofNAACL-HLTU.
Hermjakob, A. Echihabi, and D. Marcu.
2002.
Natural language based reformulation resource and wideexploitation for question answering, Proceedings of the Text Retrieval Conference.C.
Hori, S. Furui, R. Malkin, H. Yu, and A. Waibel.
2003.
A statistical approach to automatic speech summariza-tion, EURASIP Journal on Applied Signal Processing, 2:128?139.H.
Jing 2000.
Sentence reduction for automatic text summarization, Proc.
of ANLP.H.
Jing and K. McKeown.
2000.
Cut and paste based text summarization, Proc.
of NAACL.N.
Kaji, M. Okamoto, and S. Kurohashi,.
2004.
Paraphrasing predicates from written language to spokenlanguage using the web, Proceedings of HLT-NAACL.J.
Kupiec, J. Pedersen, and F. Chen 1995 A trainable document summarizer, Proceedings of SIGIR.1646C.
Lin.
2003.
Improving summarization performance by sentence compression - a pilot study, Proceedings of theInt.
Workshop on Information Retrieval with Asian Language.C.
Lin.
2004.
Rouge: A package for automatic evaluation of summaries, Proceedings of the ACL Workshop onText Summarization Branches Out.C.-Y.
Lin and E. H. Hovy.
2003.
Automatic Evaluation of Summaries Using N-gram Co-occurrence Statistics,Proceedings of HLT-NAACL.H.P.
Luhn.
1958.
The Automatic Creation of Literature Abstracts, IBM Journal of Research and Development2(2), 159?165.I.
Mani.
2001.
Automatic Summarization, Amsterdam, Philadelphia: John Benjamins.D.
Marcu.
1997 The Rhetorical Parsing of Natural Language Texts, Proceedings of ACL/EACL.K.
R. McKeown, J. Hirschberg, M. Galley, and S. Maskey.
2005.
From Text to Speech Summarization, Proceedingsof ICASSP.G.
A. Miller.
1995.
WordNet: a lexical database for English, Communications of the ACM Vol.
38, No.
11:39-41.A.
Nenkova, L. Vanderwende, and K. McKeown.
2006.
A compositional context sensitive multi-document sum-marizer: exploring the factors that influence summarization, Proceedings of SIGIR.K.
Owczarzak, J. M. Conroy, H. T. Dang, and A. Nenkova.
2012.
An assessment of the accuracy of automaticevaluation in summarization, Proceedings of the Workshop on Evaluation Metrics and System Comparison forAutomatic Summarization.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.
BLEU: a method for automatic evaluation of machinetranslation, Proceedings of ACL.D.
Roussinov and H. Chen.
2001 Information Navigation on the Web by Clustering and Summarizing QueryResults, Information Processing and Management, 37 (6), 789?816.G.
Salton, J. Allan, C. Buckley, and A. Singhal, 1994 Automatic Analysis, Theme Generation, and Summarizationof Machine-Readable Texts, Science 264(5164), 1421?1426.B.
Taskar and D. Klein and M. Collins and D. Koller and C. Manning.
2004.
Max-margin parsing, Proceedings ofEMNLP, 2004.S.
Teufel and M. Moens.
1997 sentence extraction as a classification task, Proceedings of the ACL/EACLWorkshop on Intelligent and Scalable Text Summarization.I.
Tsochantaridis, T. Joachims, T. Hofmann, and Y. Altun.
2005.
Large margin methods for structured andinterdependent output variables, JMLR, 6 (Sep):1453-1484.P.
D. Turney.
2000.
Learning algorithms for keyphrase extraction, Information Retrieval 2(4), 303?336.S.
Wan, R. Dale, M. Dras, C. Paris.
2007.
Global revision in summarisation : generating novel sentences withPrim?s algorithm, Proceedings of PACLING.J.
Y.
Yen.
1971.
Finding the k shortest loopless paths in a network, Management Science 17 (11): 712?716M.
Zinkevich, M. Weimer, A. Smola, and L. Li.
2011.
Parallelized stochastic gradient descent, Proceedings ofNIPS.1647
