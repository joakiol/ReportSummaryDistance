Automatically Identifying Computationally Relevant Typological FeaturesWilliam D. Lewis?Microsoft ResearchRedmond, WA 98052-6399wilewis@microsoft.comFei XiaUniversity of WashingtonSeattle, WA 98195fxia@u.washington.eduAbstractIn this paper we explore the potential for iden-tifying computationally relevant typological fea-tures from a multilingual corpus of language databuilt from readily available language data col-lected off the Web.
Our work builds on previousstructural projection work, where we extend thework of projection to building individual CFGsfor approximately 100 languages.
We then usethe CFGs to discover the values of typologicalparameters such as word order, the presence orabsence of definite and indefinite determiners,etc.
Our methods have the potential of beingextended to many more languages and parame-ters, and can have significant effects on currentresearch focused on tool and resource develop-ment for low-density languages and grammar in-duction from raw corpora.1 IntroductionThere is much recent interest in NLP in ?low-density?languages, languages that typically defy standard NLPmethodologies due to the absence or paucity of relevantdigital resources, such as treebanks, parallel corpora, ma-chine readable lexicons and grammars.
Even when re-sources such as raw or parallel corpora exist, they oftencannot be found of sufficient size to allow the use of stan-dard machine learning methods.
In some recent gram-mar induction and MT work (Haghighi and Klein, 2006;Quirk et al, 2005) it has been shown that even a smallamount of knowledge about a language, in the form ofgrammar fragments, treelets or prototypes, can go a longway in helping with the induction of a grammar from rawtext or with alignment of parallel corpora.In this paper we present a novel method for discov-ering knowledge about many of the world?s languagesby tapping readily available language data posted to theWeb.
Building upon our work on structural projectionsacross interlinearized text (Xia and Lewis, 2007), we de-scribe a means for automatically discovering a number ofcomputationally salient typological features, such as theexistence of particular constituents in a language (e.g.,?The work described in this document was done while Lewiswas faculty at the University of Washington.definite or indefinite determiners) or the canonical or-der of constituents (e.g., sentential word order, order ofconstituents in noun phrases).
This knowledge can thenbe used for subsequent grammar and tool developmentwork.
We demonstrate that given even a very small sam-ple of interlinearized data for a language, it is possible todiscover computationally relevant information about thelanguage, and because of the sheer volume and diversityof interlinear text on the Web, it is possible to do so forhundreds to thousands of the world?s languages.2 Background2.1 Web-Based Interlinear Data as ResourceIn linguistics, the practice of presenting language data ininterlinear form has a long history, going back at least tothe time of the structuralists.
Interlinear Glossed Text,or IGT, is often used to present data and analysis on alanguage that the reader may not know much about, andis frequently included in scholarly linguistic documents.The canonical form, an example of which is shown in (1),consists of three lines: a line for the language in question(often a sentence, which we will refer to here as the targetsentence), an English gloss line, and an English transla-tion.
(1) Rhoddodd yr athro lyfr i?r bachgen ddoegave-3sg the teacher book to-the boy yesterday?The teacher gave a book to the boy yesterday?
(Bailyn, 2001)The reader will note that many word forms are sharedbetween the gloss and translation lines, allowing for thealignment between these two lines as an intermediate stepin the alignment between the translation and the target.We use this fact to facilitate projections from the parsedEnglish data to the target language, and use the result-ing grammars to discover the values of the typologicalparameters that are the focus of this paper.We use ODIN, the Online Database of INterlinear text(http://www.csufresno.edu/odin), as our primary sourceof IGT data.
ODIN is the result of an effort to collectand database snippets of IGT contained in scholarly doc-uments posted to the Web (Lewis, 2006).
At the time ofthis writing, ODIN contains 41,581 instances of interlin-ear data for 944 languages.6852.2 The Structural Projection and CFG ExtractionAlgorithmsOur algorithm enriches the original IGT examples bybuilding phrase structures over the English data and thenprojects these onto the target language data via wordalignment.
The enrichment process has three steps: (1)parse the English translation using an English parser, (2)align the target sentence and the English translation us-ing the gloss line, and (3) project the phrase structuresonto the target sentence.
The specific details of the pro-jection algorithm are described in (Xia and Lewis, 2007).Given the projected phrase structures on target sentences,we then designed algorithms to extract context-free gram-mars (CFGs) for each of the languages by reading off thecontext-free rules from the projected target phrase struc-ture.
Identical rules are collapsed, and a frequency ofoccurrence is associated with each rule.
CFGs so gen-erated provide the target grammars we use for work oftypological discovery we describe here.Since the gloss line provides a means of associatingthe English translation with the target language, the pro-jections from the English translation effectively project?through?
the gloss line.
Any annotations associated theprojected words, such as POS tags, can be associated withwords and morphemes on the gloss line during the enrich-ment process and then can be projected onto the target.These tags are essential for answering some of the typo-logical questions, and are generally not provided by thelinguist.
This is especially important for associated par-ticular grammatical concepts, such as number or tense,with particular word categories, such as verb and noun.3 The IGT and English BiasesThe choice of the IGT as our source data type presentstwo causes for concern.
First, IGT is typically used bylinguists to illustrate linguistically interesting phenomenain a language.
A linguist often carefully chooses exam-ples from a language such that they are representative ofthe phenomena he or she wishes to discuss, and in no waycan they be seen as being randomly sampled from a ?cor-pus?
of day-to-day usage for the language.
It might beargued, then, that a corpus built over IGT suffers fromthis bias, what we call the IGT bias, and results generatedfrom IGT will be somewhat skewed.
Second, since weenrich IGT using a method of structural projection fromparses made to English translations, the language struc-tures and the grammars extracted from them might suf-fer from an English-centrism, what we call English bias:we cannot assume that all languages will have the sameor similar grammatical features or constructions that En-glish has, and by projecting structures from English, webias the structures we generate to the English source.
Thedegree to which we overcome these biases will demon-strate not only the success of our methodology, but alsothe viability of a corpus of IGT instances.4 Experimental Design4.1 The Typological ParametersLinguistic typology is the study of the classification oflanguages, where a typology is an organization of lan-guages by an enumerated list of logically possible types,most often identified by one or more structural features.1One of the most well known and well studied typolog-ical types, or parameters2, is that of word order, madefamous by Joseph Greenberg (Greenberg, 1963).
In thisseminal work, Greenberg identified six possible order-ings of Subjects, Objects, and Verbs in the world?s lan-guages, namely, SVO, SOV, VSO, VOS, OSV and OVS,and identified correlations between word order and otherconstituent orderings, such as the now well known ten-dency for SVO languages (e.g., English, Spanish) to haveprepositional ordering in adpositional phrases and SOV(e.g., Japanese, Korean) to have postpositional.We take inspiration from Greenberg?s work, and that ofsucceeding typologists (e.g.
(Comrie, 1989; Croft, 1990)).Using the linguistic typological literature as our base, weidentified a set of typological parameters which we feltcould have the most relevance to NLP, especially to taskswhich might require prototype or structural bootstraps.All of the parameters we identified enumerate variousconstituent orderings, or the presence or absence of par-ticular constituents.
The complete list of typological pa-rameters is shown in table 1.
There are two major cat-egories of parameters shown: (1) Constituent order pa-rameters, which are broken down into (a) word order and(b) morpheme order, and (2) constituent existence.
Foreach parameter, we enumerate the list of possible values(what typologists typically call types), which is generallya permutation of the possible orderings, constraining theset of possible answers to these values.
The value ndois reserved to indicate that a particular language exhibitsno dominant order for the parameter in question, that is,there is no default or canonical order for the language.The value nr, or not relevant, indicates that a primaryconstituent of the parameter does not exist in the languageand therefore no possible values for the parameter can ex-ist.
A good example of this can be seen for the DT+Nparameter: in some languages, definite and indefinite de-terminers may not exist, therefore making the parameterirrelevant.
In the specific case of determiners, we havethe Def and Indef parameters, which describe the pres-ence or absence of definite and/or indefinite determiners1See (Croft, 1990) for a thorough discussion of linguistictypology and lists of possible types.2The term typological parameter is in line with common us-age within the field of linguistic typology.686for any given language.
Since the parameters Def andIndef are strictly existence tests, their possible values areconstrained simply to Yes or No.4.2 Creating the Gold StandardsThe gold standards were created by examining grammarsand typological analyses for each language, and in somecases, consulting with native speakers or language ex-perts.
A principal target was the World Atlas of Lan-guage Structures, or WALS (Haspelmath et al, 2005),which contains a typology for hundreds of the world?slanguages.
For each of the parameters shown in Table 1,a WALS # is provided.
This was done for the convenienceof the reader, and refers to the specific section numbers inWALS that can be consulted for a detailed explanation ofthe parameter.
In some cases, WALS does not discussa particular parameter we used, in which case a WALSsection number is not provided (i.e., it is N/A).5 Finding the AnswersAs discussed, a typology consists of a parameter and alist of possible types, essentially the values this parame-ter may hold.
These values are usually not atomic, andcan be decomposed into their permuted elements, whichthemselves are types.
For instance, the word order param-eter is constrained by the types SVO, SOV, etc., whoseatoms are the types S for Subject, V for Verb, and Ofor Object.
When we talk about the order of words ina language, we are not talking about the order of certainwords, such as the constituents The teacher, read, and thebook in the sentence The teacher read the book, but ratherthe order of the types that each of these words maps to,S, V, and O.
Thus, examining individuals sentences of alanguage tell us little about the values for the typologicalparameters if the data is not annotated.The structural projections built over IGT provide theannotations for specific phrases, words or morphemesin the target language, and, where necessary, the struc-tural relationships between the annotations as expressedin a CFG.
There are three broad classes of algorithms forthis discovery process, which correspond directly to eachof the basic categories of parameters shown in Table 1.For the word order parameters, we use an algorithm thatdirectly examines the linear relationship of the relativetypes in the CFG.
For the DT+N variable, for instance,we look for the relative order of the POS tags DT and Nin the NP rules.
For the WOrder variable, we look forthe relative order NPs and Vs in the S (Sentence) and VPrules.
If a language has a dominant rule of S ?
NP VP,it is highly likely that the language is SVO or SOV, andwe can subsequently determine VO or OV by examiningthe VP rule: VP ?
V NP indicates VO and VP ?
NP Vindicates OV.Table 2: Functional Tags in the CFGsTag Meaning Parameters AffectedNP-SBJ Subject NP WOrder, V-OBJNP-OBJ Object NP WOrder, V-OBJNP-POSS Possessive NP Poss-NNP-XOBJ Oblique Object NP VP-OBJPP-XOBJ Oblique Object PP VP-OBJDT1 the DT-N, DefDT2 a,an DT-N, IndefDT3 this, that Dem-N, DefDT4 all other determiners Not usedDetermining morpheme order is somewhat simplifiedin that the CFGs do not have to be consulted, but rather agrammar consisting of possible morpheme orders, whichare derived from the tagged constituents on the gloss line.The source of the tags varies: POS tags, for instance, aregenerally not provided by the linguist, and thus must beprojected onto the target line from the English transla-tion.
Other tags, such as case, number, and tense/aspectare generally represented by the linguist but with a finergranularity than we need.
For example, the linguist willlist the specific case, such as NOM for Nominative orACC for Accusative, rather than just the label ?case?.
Weuse a table from (Lewis, 2006) that has the top 80 mor-pheme tags used by linguists to map the specific valuesto the case, number, and tense/aspect tags that we need.The existence parameters?in our study constrained toDefinite and Indefinite determiners?require us to test theexistence of particular POS annotations in the set of rel-evant CFG rules, and also to examine the specific map-pings of words between the gloss and translation lines.For instance, if there are no DT tags in any of the CFGrules for NPs, it is unlikely the language has definite orindefinite determiners.
This can specifically be confirmedby checking the transfer rules between the and a and con-stituents on the gloss line.
If either or both the or a mostlymap to NULL, then either or both may not exist in thelanguage.6 ExperimentsWe conducted two experiments to test the feasibility ofour methods.
For the first experiment, we built a goldstandard for each of the typological parameters shownin Table 1 for ten languages, namely Welsh, German,Yaqui, Mandarin Chinese, Hebrew, Hungarian, Icelandic,Japanese, Russian, and Spanish.
These languages werechosen for their typological diversity (e.g., word order),for the number of IGT instances available (all had a min-imum of fifty instances), and for the fact that some lan-guages were low-density (e.g., Welsh, Yaqui).
For thesecond experiment, we examined the WOrder parameterfor 97 languages.
The gold standard for this experimentwas copied directly from an electronic version of WALS.687Table 1: Computationally Salient Typological parameters (ndo=no dominant order, nr=not relevant)Label WALS # Description Possible ValuesWord OrderWOrder 330 Order of Words in a sentence SVO,SOV,VSO,VOS,OVS, OSV,ndo3V+OBJ 342 Order of the Verb, Object and Oblique Object (e.g., PP) VXO,VOX,OVX,OXV,XVO,XOV,ndoDT+N N/A Order of Nouns and Determiners (a, the) DT-N, N-DT, ndo, nrDem+N 358 Order of Nouns and Demonstrative Determiners (this, that) Dem-N, N-Dem, ndo, nrJJ+N 354 Order of Adjectives and Nouns JJ-N, N-JJ, ndoPRP$+N N/A Order of possessive pronouns and nouns PRP$-N, N-PRP$, ndo, nrPoss+N 350 Order of Possessive NPs and nouns NP-Poss, NP-Poss, ndo, nrP+NP 346 Order of Adpositions and Nouns P-NP, NP-P, ndoMorpheme OrderN+num 138 Order of Nouns and Number Inflections (Sing, Plur) N-num, num-N, ndoN+case 210 Order of Nouns and Case Inflections N-case, case-N, ndo, nrV+TA 282 Order of Verbs and Tense/Aspect Inflections V-TA, TA-V, ndo, nrExistence TestsDef 154 Do definite determiners exist?
Yes, NoIndef 158 Do indefinite determiners exist?
Yes, NoTable 3: Experiment 1 Results (Accuracy)WOrder VP DT Dem JJ PRP$ Poss P N N V Def Indef Avg+OBJ +N +N +N +N +N +NP +num +case +TAbasic CFG 0.8 0.5 0.8 0.8 1.0 0.8 0.6 0.9 0.7 0.8 0.8 1.0 0.9 0.800sum(CFG) 0.8 0.5 0.8 0.8 0.9 0.7 0.6 0.8 0.6 0.8 0.7 1.0 0.9 0.762CFG w/ func 0.9 0.6 0.8 0.9 1.0 0.8 0.7 0.9 0.7 0.8 0.8 1.0 0.9 0.831both 0.9 0.6 0.8 0.8 0.9 0.7 0.5 0.8 0.6 0.8 0.7 1.0 0.9 0.769Since the number of IGT instances varied greatly, from aminimum of 1 (Halkomelem, Hatam, Palauan, Itelmen)to a maximum of 795 (Japanese), as shown in the firstcolumn of Table 4, we were able to examine specificallythe correlation between the number of instances and oursystem?s performance (at least for this parameter).6.1 Experiment 1 - Results for 10 Languages, 14ParametersAs described, the grammars for any given language con-sist of a CFG and associated frequencies.
Our first in-tuition was that for any given word order parameter, themost frequent ordering, as expressed by the most frequentrule in which it appears, was likely the predominant pat-tern in the language.
Thus, for Hungarian, the order of theDT+N parameter is DT-N since the most frequent rule,namely NP ?
DT N, occurs much more frequently thanthe one rule with the opposing order, by a factor of 33 to1.
Our second intuition was based on the assumption thatnoise could cause an anomalous ordering to appear in themost frequent rule of a targeted type, especially when thenumber of IGT examples was limited.
We hypothesizedthat ?summing?
across a set of rules that contained the listof constituents we were interested in might give more ac-curate results, giving the predominant patterns a chanceto reveal themselves in the summation process.An examination of the types of rules in the CFGs andthe parameter values we needed to populate led us to con-sider enriching the annotations on the English side.
Forinstance, if a CFG contained the rule S ?
NP V, it is im-possible for us to tell whether the NP is a subject or anobject, a fact that is particularly relevant to the WOrderparameter.
We enriched the annotations with functionaltags, such as SBJ, OBJ, POSS, etc., which we assignedusing heuristics based on our knowledge of English, andwhich could then be projected onto the target.
The down-side of such an approach is that it increases the granular-ity of the grammar rules, which then could weaken thegeneralizations that might be relevant to particular typo-logical discoveries.
However, summing across such rulesmight alleviate some of this problem.
We also divided theEnglish determiners into four groups in order to distin-guish their different types, and projected the refined tagsonto the target.
The full set of functional tags we used areshown in Table 2, with the list of typological parametersthat were affected by the inclusion of each.4 The resultsfor the experiment are shown in Table 3.4It should be noted some ?summations?
were done to theCFGs in a preprocessing step, thus affecting all subsequent pro-cessing.
All variants of NN (NN, NNS, NNP) were collapsedinto N and all of VB (VB, VBD, VBZ, etc.)
into V. Unalignedwords and punctuation were also deleted and the affected rulescollapsed.688Table 4: Confusion Matrix for the Word Order TypesWord # of System Predictionorder languages SVO SOV VSO VOSSVO 46 32 8 0 6SOV 39 2 33 0 4VSO 11 2 2 3 4VOS 1 0 0 0 1Table 5: Word Order Accuracy for 97 languages# of IGT instances Average Accuracy100+ 100%40-99 99%10-39 79%5-9 65%3-4 44%1-2 14%6.2 Experiment 2 Results - Word Order for 97LanguagesThe second experiment sought to assign values for theWOrder parameter for 97 languages.
For this experiment,a CFG with functional tags was built for each language,and the WOrder algorithm was applied to each language?sCFG.
The confusion matrix in Table 4 shows the numberof correct and incorrect assignments.
SVO and SOV wereassigned correctly most of the time, whereas VSO pro-duced significant error.
This is mostly due to the smallersample sizes for VSO languages: of the 11 VSO lan-guages in our survey, over half had sample sizes less than10 IGT instances; of those with instance counts above 70(two languages), the answer was correct.6.3 Error AnalysisThere are four main types of errors that affected our sys-tem?s performance:?
Insufficient data ?
Accuracy of the parameters wasaffected by the amount of data available.
For theWOrder parameter, for instance, the number of in-stances is a good predictor of the confidence of thevalue returned.
The accuracy of the WOrder param-eter drops off geometrically as the number of in-stances approaches zero, as shown in Table 5.
How-ever, even with as few as 4-8 instances, one can ac-curately predict WOrder?s value more than half thetime.
For other parameters, the absence of crucialconstituents (e.g., Poss, PRP$) did not allow us togenerate a value.?
Skewed or inaccurate data ?
Depending on the num-ber of examples and source documents, results couldbe affected by the IGT bias.
For instance, althoughCantonese (YUH) is a strongly SVO language andODIN contains 73 IGT instances for the language,our system determined that Cantonese was VOS.This resulted from a large number of skewed exam-ples found in just one paper.?
Projection errors ?
In many cases, noise was intro-duced into the CFGs when the word aligner or pro-jction algorithm made mistakes, potentially intro-ducing unaligned constituents.
These were subse-quently collapsed out of the CFGs.
The absent con-stituents sometimes led to spurious results when theCFGs were later examined.?
Free constituent order ?
Some languages have freerconstituent order than others, making calculation ofparticular parametric values difficult.
For example,Jingulu (JIG) and German (GER) alternate betweenSVO and SOV.
In both cases, our grammars directedus to an order that was opposite our gold standard.7 Discussion7.1 DataIn examining Table 5, the reader might question why itis necessary to have 40 or more sentences of parsed lan-guage data in order to generalize the word order of a lan-guage with a high degree of confidence.
After all, anyonecould examine just one or two examples of parsed En-glish data to discern that English is SVO, and be nearlycertain to be right.
There are several factors involved.First, a typological parameter like WOrder is meant torepresent a canonical characteristic of the language; alllanguages exhibit varying degrees of flexibility in the or-dering of constituents, and discovering the canonical or-der of constituents requires accumulating enough data forthe pattern to emerge.
Some languages might requiremore instances of data to reach a generalization than oth-ers precisely because they might have freer word order.English has a more rigid word order than most, and thuswould require less data.Second, the data we are relying on is somewhatskewed, resulting from the IGT bias.
We have to collectsufficient amounts of data and from enough sources tocounteract any linguist-based biases introduced into thedata.
It is also the case that not all examples are fullsentences.
A linguist might be exploring the structure ofnoun phrases for instance, and not provide full sentences.Third, we are basing our analyses on projected struc-tures.
The word alignment and syntactic projections arenot perfect.
Consequently, the trees generated, and therules read off of them, may be incomplete or inaccurate.7.2 Relevance to NLPOur efforts described here were inspired by some re-cent work on low-density languages (Yarowksy andNgai, 2001; Maxwell and Hughes, 2006; Drabek andYarowsky, 2006).
Until fairly recently, almost all NLPwork was done on just a dozen or so languages, with the689vast majority of the world?s 6,000 languages being ig-nored.
This is understandable, since in order to do seri-ous NLP work, a certain threshold of corpus size mustbe achieved.
We provide a means for generating small,richly annotated corpora for hundreds of languages usingfreely available data found on the Web.
These corporacan then be used to generate other electronic resources,such as annotated corpora and associated NLP tools.The recent work of (Haghighi and Klein, 2006) and(Quirk et al, 2005) were also sources of inspiration.
Inthe former case, the authors showed that it is possible toimprove the results of grammar induction over raw cor-pora if one knows just a few facts about the target lan-guage.
The ?prototypes?
they describe are very similar tothe our constituent order parameters, and we see our workas an incremental step in applying grammar induction toraw corpora for a large number of languages.Quirk et al2005 demonstrates the success of usingfragments of a target language?s grammar, what they call?treelets?, to improve performance in phrasal translation.They show that knowing even a little bit about the syntaxof the target language can have significant effects on suc-cess of phrasal-based MT.
Our parameters are in someways similar to the treelets or grammar fragments builtby Quirk and colleagues and thus might be applicable tophrasal-based MT for a larger number of languages.Although the reader might question the utility of usingenriched IGT for discovering the values of typologicalparameters, since the ?one-off?
nature of these discover-ies might argue for using existing grammars (e.g., WALS)over harvesting and enriching IGT.
However, it is impor-tant to recognize that the parameters that we specify inthis paper are only a sample of the potential parametersthat might be recoverable from enriched IGT.
Further,because we are effectively building PCFGs for the lan-guages we target, it is possible to provide gradient valuesfor various parameters, such as the degree of word ordervariability in a language (e.g., SVO 90%, SOV 10%), thepotential for which we not explicitly explored in this pa-per.
In addition, IGT exists in one place, namely ODIN,for hundreds of languages, and the examples that are har-vested are also readily available for review (not alwaysthe case for grammars).8 ConclusionWe demonstrate a method for discovering interesting andcomputationally relevant typological features for hun-dreds of the world?s languages automatically using freelyavailable language data posted to the Web.
We demon-strate that confidence increases as the number of datapoints increases, overcoming the IGT and English biases.Inspired by work that uses prototypes and grammar frag-ments, we see the work we describe here as being quiterelevant to the growing body of work on languages whosedigital footprint is much smaller than the ten or so major-ity languages of the world.ReferencesJohn Frederick Bailyn.
2001.
Inversion, dislocation and option-ality in russian.
In Gerhild Zybatow, editor, Current Issuesin Formal Slavic Linguistics.B.
Comrie.
1989.
Language Universals and Linguistic Typol-ogy: Syntax and Morphology.
Blackwell, Oxford.William Croft.
1990.
Typology and Universals.
CambridgeUniversity Press, New York.Elliott Franco Drabek and David Yarowsky.
2006.
Inductionof fine-grained part-of-speech taggers via classifier combi-nation and crosslingual projection.
In Proceedings of COL-ING/ACL2006 Workshop on Frontiers in Linguistically An-notated Corpora.Joseph H. Greenberg.
1963.
Some universals of grammar withparticular reference to the order of meaningful elements.
InJoseph H. Greenberg, editor, Universals of Language, pages73?113.
MIT Press, Cambridge, Massachusetts.Aria Haghighi and Dan Klein.
2006.
Protoype-driven sequencemodels.
In Proceedings of HLT-NAACL, New York City, NY.Martin Haspelmath, Matthew S. Dryer, David Gil, and BernardComrie.
2005.
The World Atlas of Language Structures.Oxford University Press, Oxford, England.William D. Lewis.
2006.
ODIN: A Model for Adapting andEnriching Legacy Infrastructure.
In Proceedings of the e-Humanities Workshop, Amsterdam.
Held in cooperationwith e-Science 2006: 2nd IEEE International Conference one-Science and Grid Computing.Mike Maxwell and Baden Hughes.
2006.
Frontiers in linguisticannotation for lower-density languages.
In Proceedings ofCOLING/ACL2006 Workshop on Frontiers in LinguisticallyAnnotated Corpora.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
Depen-dency tree translation: Syntactically informed phrasal smt.In Proceedings of ACL 2005.Fei Xia and William D. Lewis.
2007.
Multilingual structuralprojection across interlinearized text.
In Proceedings of theNorth American Association of Computational Linguistics(NAACL) conference.David Yarowksy and Grace Ngai.
2001.
Inducing multilingualpos taggers and np bracketers via robust projection acrossaligned corpora.
In Proceedings of NAACL-2001, pages377?404.690
