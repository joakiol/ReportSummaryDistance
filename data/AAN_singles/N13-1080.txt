Proceedings of NAACL-HLT 2013, pages 685?690,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsWhat?s in a Domain?
Multi-Domain Learning for Multi-Attribute DataMahesh Joshi?
Mark Dredze?
William W. Cohen?
Carolyn P.
Rose???
School of Computer Science, Carnegie Mellon UniversityPittsburgh, PA, 15213, USA?
Human Language Technology Center of Excellence, Johns Hopkins UniversityBaltimore, MD, 21211, USAmaheshj@cs.cmu.edu,mdredze@cs.jhu.eduwcohen@cs.cmu.edu,cprose@cs.cmu.eduAbstractMulti-Domain learning assumes that a sin-gle metadata attribute is used in order to di-vide the data into so-called domains.
How-ever, real-world datasets often have multi-ple metadata attributes that can divide thedata into domains.
It is not always apparentwhich single attribute will lead to the best do-mains, and more than one attribute might im-pact classification.
We propose extensions totwo multi-domain learning techniques for ourmulti-attribute setting, enabling them to si-multaneously learn from several metadata at-tributes.
Experimentally, they outperform themulti-domain learning baseline, even when itselects the single ?best?
attribute.1 IntroductionMulti-Domain Learning (Evgeniou and Pontil,2004; Daume?
III, 2007; Dredze and Crammer, 2008;Finkel and Manning, 2009; Zhang and Yeung, 2010;Saha et al 2011) algorithms learn when training in-stances are spread across many domains, which im-pact model parameters.
These algorithms use exam-ples from each domain to learn a general model thatis also sensitive to individual domain differences.However, many data sets include a host of meta-data attributes, many of which can potentially definethe domains to use.
Consider the case of restaurantreviews, which can be categorized into domains cor-responding to the cuisine, location, price range, orseveral other factors.
For multi-domain learning, weshould use the metadata attribute most likely to char-acterize a domain: a change in vocabulary (i.e.
fea-tures) that most impacts the classification decision(Ben-David et al 2009).
This choice is not easy.First, we may not know which metadata attribute ismost likely to fit this role.
Perhaps the location mostimpacts the review language, but it could easily bethe price of the meal.
Second, multiple metadataattributes could impact the classification decision,and picking a single one might reduce classificationaccuracy.
Therefore, we seek multi-domain learn-ing algorithms which can simultaneously learn frommany types of domains (metadata attributes).We introduce the multi-attribute multi-domain(MAMD) learning problem, in which each learninginstance is associated with multiple metadata at-tributes, each of which may impact feature behavior.We present extensions to two popular multi-domainlearning algorithms, FEDA (Daume?
III, 2007) andMDR (Dredze et al 2009).
Rather than selectinga single domain division, our algorithms considerall attributes as possible distinctions and discoverchanges in features across attributes.
We evaluateour algorithms using two different data sets ?
a dataset of restaurant reviews (Chahuneau et al 2012),and a dataset of transcribed speech segments fromfloor debates in the United States Congress (Thomaset al 2006).
We demonstrate that multi-attribute al-gorithms improve over their multi-domain counter-parts, which can learn distinctions from only a singleattribute.2 MAMD LearningIn multi-domain learning, each instance x is drawnfrom a domain d with distribution x ?
Dd over avectors space RD and labeled with a domain spe-cific function fd with label y ?
{?1,+1} (for bi-nary classification).
In multi-attribute multi-domain685(MAMD) learning, we have M metadata attributes ina data set, where the mth metadata attribute has Kmpossible unique values which represent the domainsinduced by that metadata attribute.
Each instance xiis drawn from a distribution xi ?
Da specific to aset of attribute values Ai associated with each in-stance.
Additionally, each unique set of attributesindexes a function fA.1 Ai could contain a value foreach attribute, or no values for any attribute (whichwould index a domain-agnostic ?background?
distri-bution and labeling function).
Just as a domain canchange a feature?s probability and behavior, so caneach metadata attribute.Examples of data for MAMD learning abound.
Thecommonly used Amazon product reviews data set(Blitzer et al 2007) only includes product types, butthe original reviews can be attributed with author,product price, brand, and so on.
Additional exam-ples include congressional floor debate records (e.g.political party, speaker, bill) (Joshi et al 2012).
Inthis paper, we use restaurant reviews (Chahuneau etal., 2012), which have upto 20 metadata attributesthat define domains, and congressional floor de-bates, with two attributes that define domains.It is difficult to apply multi-domain learning algo-rithms when it is unclear which metadata attributeto choose for defining the ?domains?.
It is possiblethat there is a single ?best?
attribute to use for defin-ing domains, one that when used in multi-domainlearning will yield the best classifier.
To find thisattribute, one must rely on one?s intuition about theproblem,2 or perform an exhaustive empirical searchover all attributes using some validation set.
Boththese strategies can be brittle, because as the natureof data changes over time so may the ?best?
do-main distinction.
Additionally, multi-domain learn-ing was not designed to benefit from multiple helpfulattributes.We note here that Eisenstein et al(2011), as wellas Wang et al(2012), worked with a ?multifacetedtopic model?
using the framework of sparse addi-tive generative models (SAGE).
Both those modelscapture interactions between topics and multiple as-1Distributions and functions that share attributes could shareparameters.2Intuition is often critical for learning and in some cases canhelp, such as in the Amazon product reviews data set, whereproduct type clearly corresponds to domain.
However, for otherdata sets the choice may be less clear.pects, and can be adapted to the case of MAMD.
Whileour problem formulation has significant conceptualoverlap with the SAGE?like multifaceted topic mod-els framework, our proposed methods are motivatedfrom a fast online learning perspective.A naive approach for MAMD would be to treat ev-ery unique set of attributes as a domain, includingunique proper subsets of different attributes to ac-count for the case of missing attributes in some in-stances.3 However, introducing an exponential num-ber of domains requires a similar increase in train-ing data, clearly an infeasible requirement.
Instead,we develop multi-attribute extensions for two multi-domain learning algorithms, such that the increasein parameters is linear in the number of metadata at-tributes, and no special handling is required for thecase where some metadata attributes might be miss-ing from an instance.Multi-Attribute FEDA The key idea behindFEDA (Daume?
III, 2007) is to encode each domainusing its own parameters, one per feature.
FEDAmaps a feature vector x in RD to RD(K+1).
Thisprovides a separate parameter sub-space for everydomain k ?
1 .
.
.K, and also maintains a domain-agnostic shared sub-space.
Essentially, each featureis duplicated for every instance in the appropriatesub-space of RD(K+1) that corresponds to the in-stance?s domain.
We extend this idea to the MAMDsetting by using one parameter per attribute value.The original instance x ?
RD is now mapped intoRD(1+?mKm); a separate parameter for each at-tribute value and a shared set of parameters.
In ef-fect, for every metadata attribute a ?
Ai, the originalfeatures are copied into the appropriate sub-space.This grows linearly with the number of metadata at-tribute values, as opposed to exponentially in ournaive solution.
While this is still substantial growth,each instance retains the same feature sparsity as inthe original input space.
In this new setup, FEDA al-lows an instance to contribute towards learning theshared parameters, and the attribute-specific param-eters for all the attributes present on an instance.
Justlike multi-domain FEDA, any supervised learning al-gorithm can be applied to the transformed represen-tation.3While we used a similar setup for formulating our problem,we did not rule out the potential for factoring the distributions.686Multi-Attribute MDR We make a similar changeto MDR (Dredze et al 2009) to extend it forthe MAMD setting.
In the original formulation,Dredze et alused confidence-weighted (CW)learning (Dredze et al 2008) for learning sharedand domain-specific classifiers, which are combinedbased on the confidence scores associated with thefeature weights.
For training the MDR approaches ina multi-domain learning setup, they found that com-puting updates for the combined classifier and thenequally distributing them to the shared and domain-specific classifiers was the best strategy, although itapproximated the true objective that they aimed tooptimize.
In our multi-attribute setup confidence-weighted (CW) classifiers are learned for each of the?mKm attribute values in addition to a shared CWclassifier.
At classification time, a combined clas-sifier is computed for every instance.
However, in-stead of combining the shared classifier and a singledomain-specific classifier, we combine the sharedCW classifier and |Ai| different attribute value-specific CW classifiers associated with xi.
Thecombined classifier is found by minimizing the KL-divergence of the combined classifier with respect toeach of the underlying classifiers.4When learning the shared and domain-specificclassifiers, we follow the best result in Dredze etal.
and use the ?averaged update?
strategy (?7.3 inDredze et al, where updates are computed for thecombined classifier, and are then distributed to theshared and domain-specific classifiers.
MDR-U willindicate that the updates to the combined classifiersare uniformly distributed to the underlying sharedand domain-specific classifiers.Dredze et alalso used another scheme called?variance?
to distribute the combined update to theunderlying classifiers (?4, last paragraph in Dredzeet al Their idea was to give a lower portionof the update to the underlying classifier that hashigher variance (or in their terminology, ?less con-fidence?)
since it contributed less to the combinedclassifier.
We refer to this as MDR-V.
However, thisconflicts with the original CW intuition that featureswith higher variance (lower confidence) should re-ceive higher updates; since they are more in needof change.
Therefore, we implemented a modi-fied ?variance?
scheme, where the updates are dis-4We also tried the l2 distance method of Dredze et al(2009)but it gave consistently worse results.tributed to the underlying classifiers such that highervariance features receive the larger updates.
We re-fer to this as MDR-NV.
We observed significant im-provements with this modified scheme.3 ExperimentsTo evaluate our multi-attribute algorithms we con-sider two datasets.
First, we use two subsets of therestaurant reviews dataset (1,180,308 reviews) intro-duced by Chahuneau et al(2012) with the goal oflabeling reviews as positive or negative.
The firstsubset (50K-RND) randomly selects 50,000 reviewswhile the second (50K-BAL) is a class-balancedsample.
Following the approach of Blitzer et al(2007), scores above and below 3-stars indicatedpositive and negative reviews, while 3-star reviewswere discarded.
Second, we use the transcribed seg-ments of speech from the United States Congressfloor debates (Convote), introduced by Thomaset al(2006).
The binary classification task on thisdataset is that of predicting whether a given speechsegment supports or opposes a bill under discussionin the floor debate.In the WordSalad datasets, each restaurant re-view can have many metadata attributes, including aunique identifier, name (which may not be unique),address (we extract the zipcode), and type (Italian,Chinese, etc.).
We select the 20 most common meta-data attributes (excluding latitude, longitude, and theaverage rating).
5 In the Convote dataset, eachspeech segment is associated with the political partyaffiliation of the speaker (democrat, independent, orrepublican) and the speaker identifier (we use billidentifiers for creating folds in our 10-fold cross-validation setup).In addition to our new algorithms, we evalu-ate several baselines.
All methods use confidence-weighted (CW) learning (Crammer et al 2012).BASE A single classifier trained on all the data,and which ignores metadata attributes and uses uni-gram features.
For CW, we use the best-performingsetting from Dredze et al(2008) ?
the ?variance?algorithm, which computes approximate but closed?form updates, which also lead to faster learning.
Pa-rameters are tuned over a validation set within eachtraining fold.5Our method requires categorical metadata attributes, al-though real-valued attributes can be discretized.687metadata 1-META FEDA MDR-U MDR-V MDR-NV50K-RND NONE (BASE) 92.29 (?0.14)ALL (META) ?
92.69 (?0.10)CATEGORY ?
92.48 (?0.11) 92.47 (?0.10) ??
92.99 (?0.12) 91.16 (?0.16) ??
93.24 (?0.13)ZIPCODE 92.40 (?0.09) ?
92.73 (?0.09) ??
92.99 (?0.12) 91.19 (?0.20) ??
93.22 (?0.11)NEIGHBORHOOD 92.42 (?0.11) ?
92.65 (?0.13) ??
93.02 (?0.13) 91.17 (?0.21) ??
93.21 (?0.12)50K-BAL NONE (BASE) 89.95 (?0.10)ALL (META) ?
90.39 (?0.09)CATEGORY 90.09 (?0.11) ?
90.50 (?0.11) ?
90.60 (?0.11) 87.89 (?0.13) ??
91.33 (?0.08)ZIPCODE 89.97 (?0.12) ?
90.42 (?0.13) ?
90.56 (?0.09) 87.78 (?0.16) ??
91.30 (?0.10)ID ?
90.42 (?0.11) ??
90.64 (?0.11) ?
90.50 (?0.11) 87.78 (?0.25) ??
91.27 (?0.09)Table 1: Average accuracy (?
standard error) for the best three metadata attributes, when using a single attribute ata time.
Results that are numerically the best within a row are in bold.
Results significantly better than BASE aremarked with ?, and better than META are marked with ?.
Significance is measured using a two-tailed paired t-test with?
= 0.05.#attributes FEDA MDR-U MDR-V MDR-NV50K-RND MAMD ??
93.07 (?0.19) ??
93.12 (?0.11) 87.08 (?1.72) ??
93.19 (?0.12)1-ORCL ??
93.06 (?0.11) ??
93.17 (?0.11) 92.37 (?0.11) ??
93.39 (?0.12)1-TUNE ?
92.64 (?0.12) ?
92.81 (?0.16) 92.15 (?0.17) ??
93.07 (?0.14)1-MEAN ?
92.61 (?0.09) ?
92.59 (?0.10) 91.41 (?0.12) ?
92.58 (?0.10)50K-BAL MAMD ??
91.42 (?0.09) ??
91.06 (?0.04) 81.43 (?2.79) ??
91.40 (?0.08)1-ORCL ??
90.89 (?0.10) ??
90.87 (?0.11) 89.33 (?0.13) ??
91.45 (?0.07)1-TUNE ?
90.33 (?0.10) ??
90.70 (?0.14) 89.13 (?0.16) ??
91.26 (?0.08)1-MEAN ?
90.30 (?0.06) 89.92 (?0.07) 88.25 (?0.07) 90.06 (?0.08)Table 2: Average accuracy (?
standard error) using 10-fold cross-validation for methods that use all attributes, eitherdirectly (our proposed methods) or for selecting the ?best?
single attribute using one of the strategies described earlier.Formatting and significance symbols are the same as in Table 1.META Identical to BASE with a unique bias featureadded for each attribute value (Joshi et al 2012).1-META A special case of META where a uniquebias feature is added only for a single attribute.To use multi-domain learning directly, we couldselect a single attribute as the domain.
We considerseveral strategies for picking this attribute and eval-uate both FEDA and MDR in this setting.1-MEAN Choose an attribute randomly, equivalentto the expected (mean) error over all attributes.1-TUNE Select the best performing attribute on avalidation set.1-ORCL Select the best performing attribute onthe test set.
Though impossible in practice, this givesthe oracle upper bound on multi-domain learning.All experiments use ten-fold cross-validation.
Wereport the mean accuracy, along with standard error.4 ResultsTable 1 shows the results of single-attribute multi-domain learning methods for the WordSaladdatasets.
The table shows the three best-performingmetadata attributes (as decided by the highest accu-racy among all the methods across all 20 metadataattributes).
Clearly, several of the attributes can pro-vide meaningful domains, which demonstrates thatmethods that can select multiple attributes at onceare desirable.
We also see that our modification toMDR (MDR-NV) works the best.Table 3 shows the results of single-attribute multi-domain learning methods for the Convote dataset.The first observation to be made on this dataset isthat neither the PARTY, nor the SPEAKER attributeindividually achieve significant improvement overthe META baseline, which uses both these attributesas features.
This is in contrast with the results onthe WordSalad dataset, where some attributes bythemselves showed an improvement over the METAbaseline.
Thus, this dataset represents a more chal-lenging setup for our multi?attribute multi?domainlearning methods ?
they need to exploit the twoweak attributes simultaneously.We next demonstrate multi-attribute improve-ments over the multi-domain baselines (Tables 2and 4).
For WordSalad datasets, our exten-sions that can use all metadata attributes simul-taneously are consistently better than both the1-MEAN and the 1-TUNE strategies (except forthe case of the old variance scheme used by(Dredze et al 2009)).
For the skewed subset688metadata 1-META FEDA MDR-U MDR-V MDR-NVNONE (BASE) 67.08 (?1.74)ALL (META) ?
82.60 (?1.95)PARTY ?
78.81 (?1.47) ?
84.19 (?2.44) ?
83.23 (?2.48) ?
81.38 (?2.22) ?
83.92 (?2.31)SPEAKER ?
77.49 (?1.75) ?
82.88 (?2.43) ?
78.32 (?1.91) 62.43 (?2.20) ?
72.26 (?1.37)Table 3: Convote: Average accuracy (?
standard error) when using a single attribute at a time.
Results that arenumerically the best within a row are in bold.
Results significantly better than BASE are marked with ?, and betterthan META are marked with ?.
Significance is measured using a two-tailed paired t-test with ?
= 0.05.#attributes FEDA MDR-U MDR-V MDR-NVMAMD ??
85.71 (?2.74) ?
84.12 (?2.56) 50.44 (?1.78) ??
86.19 (?2.49)1-ORCL ?
84.77 (?2.47) ?
83.88 (?2.27) ?
81.38 (?2.22) ?
83.92 (?2.31)1-TUNE ?
84.19 (?2.44) ?
83.23 (?2.48) ?
81.38 (?2.22) ?
83.92 (?2.31)1-MEAN ?
83.53 (?2.40) ?
80.77 (?1.92) ?
71.91 (?1.82) ?
78.09 (?1.69)Table 4: Convote: Average accuracy (?
standard error) using 10-fold cross-validation for methods that use allattributes, either directly (our proposed methods) or for selecting the ?best?
single attribute using one of the strategiesdescribed earlier.
Formatting and significance symbols are the same as in Table 3.50K-RND, MAMD+FEDA is significantly better than1-TUNE+FEDA; MAMD+MDR-U is significantly bet-ter than 1-TUNE+MDR-U; MAMD+MDR-NV is notsignificantly different from 1-TUNE+MDR-U.
Forthe balanced subset 50K-BAL, a similar patternholds, except that MAMD+MDR-NV is significantlybetter than 1-TUNE+MDR-NV.
Clearly, our multi-attribute algorithms provide a benefit over existingapproaches.
Even with oracle knowledge of the testperformance using multi-domain learning, we canstill obtain improvements (FEDA and MDR-U in the50K-BAL set, and all the Convote results, exceptMDR-V).Although MAMD+MDR-NV is not significantly bet-ter than 1-TUNE+MDR-NV on the 50K-RND set,we found that in every single fold in our ten-fold cross-validation experiments, the ?best?
singlemetadata attribute decided using a validation set didnot match the best-performing single metadata at-tribute on the corresponding test set.
This showsthe potential instability of choosing a single best at-tribute.
Also, note that MDR-NV is a variant that wehave proposed in the current work, and in fact forthe earlier variant of MDR (MDR-U), as well as forFEDA, we do see significant improvements when us-ing all metadata attributes.
Furthermore, the compu-tational cost of evaluating every metadata attributeindependently to tune the single best metadata at-tribute can be high and often impractical.
Our ap-proach requires no such tuning.
Finally, observethat for FEDA, the 1-TUNE strategy is not signifi-cantly different from 1-MEAN, which just randomlypicks a single best metadata attribute.
For MDR-U,1-TUNE is significantly better than 1-MEAN on thebalanced subset 50K-BAL, but not on the skewedsubset 50K-RND.As mentioned earlier, the Convote dataset is achallenging setting for our methods due to the factthat no single attribute is strong enough to yield im-provements over the META baseline.
In this setting,both MAMD+FEDA and MAMD+MDR-NV achieve asignificant improvement over the META baseline,with MDR-NV being the best (though not signif-icantly better than FEDA).
Additionally, both ofthem are significantly better than their correspond-ing 1-TUNE strategies.
This result further supportsour claim that using multiple attributes in combi-nation for defining domains (even when any singleone of them is not particularly beneficial for multi?domain learning) is important.5 ConclusionsWe propose multi-attribute multi-domain learningmethods that can utilize multiple metadata attributessimultaneously for defining domains.
Using thesemethods, the definition of ?domains?
does not haveto be restricted to a single metadata attribute.
Ourmethods achieve a better performance on two multi-attribute datasets as compared to traditional multi-domain learning methods that are tuned to use a sin-gle ?best?
attribute.AcknowledgmentsThis research is supported by the Office of NavalResearch grant number N000141110221.689ReferencesShai Ben-David, John Blitzer, Koby Crammer, AlexKulesza, Fernando Pereira, and Jennifer WortmanVaughan.
2009.
A theory of learning from differentdomains.
Machine Learning.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, Bollywood, Boom-boxes and Blenders:Domain Adaptation for Sentiment Classification.
InProceedings of the 45th Annual Meeting of the Asso-ciation of Computational Linguistics, pages 440?447.Association for Computational Linguistics.Victor Chahuneau, Kevin Gimpel, Bryan R. Routledge,Lily Scherlis, and Noah A. Smith.
2012.
WordSalad: Relating Food Prices and Descriptions.
In Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing and Natural LanguageLearning (EMNLP 2012).Koby Crammer, Mark Dredze, and Fernando Pereira.2012.
Confidence-weighted linear classification fortext categorization.
Journal of Machine Learning Re-search (JMLR).Hal Daume?
III.
2007.
Frustratingly Easy Domain Adap-tation.
In Proceedings of the 45th Annual Meeting ofthe Association of Computational Linguistics, pages256?263.
Association for Computational Linguistics.Mark Dredze and Koby Crammer.
2008.
Online meth-ods for multi-domain learning and adaptation.
Pro-ceedings of the Conference on Empirical Methods inNatural Language Processing - EMNLP ?08.Mark Dredze, Koby Crammer, and Fernando Pereira.2008.
Confidence-weighted linear classification.
Pro-ceedings of the 25th international conference on Ma-chine learning - ICML ?08.Mark Dredze, Alex Kulesza, and Koby Crammer.
2009.Multi-domain learning by confidence-weighted pa-rameter combination.
Machine Learning, 79(1?2):123?149.Jacob Eisenstein, Amr Ahmed, and Eric P. Xing.
2011.Sparse Additive Generative Models of Text.
In Pro-ceedings of the 28th International Conference on Ma-chine Learning (ICML).Theodoros Evgeniou and Massimiliano Pontil.
2004.Regularized multi?task learning.
In Proceedings ofthe 2004 ACM SIGKDD international conference onKnowledge discovery and data mining - KDD ?04.Jenny R Finkel and Christopher D Manning.
2009.
Hier-archical Bayesian Domain Adaptation.
In Proceedingsof Human Language Technologies: The 2009 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 602?610.
Association for Computational Linguistics.Mahesh Joshi, Mark Dredze, William W. Cohen, and Car-olyn P. Rose?.
2012.
Multi-domain learning: When dodomains matter?
In Proceedings of EMNLP-CoNLL2012, pages 1302?1312.Avishek Saha, Piyush Rai, Hal Daume?
III, and SureshVenkatasubramanian.
2011.
Online learning of mul-tiple tasks and their relationships.
In Proceedings ofAISTATS 2011.Matt Thomas, Bo Pang, and Lillian Lee.
2006.
Getout the vote: Determining support or opposition fromCongressional floor-debate transcripts.
In Proceed-ings of EMNLP, pages 327?335.William Yang Wang, Elijah Mayfield, Suresh Naidu, andJeremiah Dittmar.
2012.
Historical Analysis of LegalOpinions with a Sparse Mixed-Effects Latent VariableModel.
In Proceedings of the 50th Annual Meetingof the Association for Computational Linguistics (ACL2012).Yu Zhang and Dit-Yan Yeung.
2010.
A Convex Formu-lation for Learning Task Relationships in Multi-TaskLearning.
In Proceedings of the Proceedings of theTwenty-Sixth Conference Annual Conference on Un-certainty in Artificial Intelligence (UAI-10).690
