Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 45?48,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsRethinking Grammatical Error Annotation and Evaluation with theAmazon Mechanical TurkJoel R. TetreaultEducational Testing ServicePrinceton, NJ, 08540, USAJTetreault@ets.orgElena FilatovaFordham UniversityBronx, NY, 10458, USAfilatova@fordham.eduMartin ChodorowHunter College of CUNYNew York, NY, USAmartin.chodorow@hunter.cuny.eduAbstractIn this paper we present results from two pi-lot studies which show that using the AmazonMechanical Turk for preposition error anno-tation is as effective as using trained raters,but at a fraction of the time and cost.
Basedon these results, we propose a new evaluationmethod which makes it feasible to comparetwo error detection systems tested on differentlearner data sets.1 IntroductionThe last few years have seen an explosion in the de-velopment of NLP tools to detect and correct errorsmade by learners of English as a Second Language(ESL).
While there has been considerable empha-sis placed on the system development aspect of thefield, with researchers tackling some of the tough-est ESL errors such as those involving articles (Hanet al, 2006) and prepositions (Gamon et al, 2008),(Felice and Pullman, 2009), there has been a woefullack of attention paid to developing best practices forannotation and evaluation.Annotation in the field of ESL error detection hastypically relied on just one trained rater, and thatrater?s judgments then become the gold standard forevaluating a system.
So it is very rare that inter-raterreliability is reported, although, in other NLP sub-fields, reporting reliability is the norm.
Time andcost are probably the two most important reasonswhy past work has relied on only one rater becauseusing multiple annotators on the same ESL textswould obviously increase both considerably.
This isespecially problematic for this field of research sincesome ESL errors, such as preposition usage, occur aterror rates as low as 10%.
This means that to collecta corpus of 1,000 preposition errors, an annotatorwould have to check over 10,000 prepositions.1(Tetreault and Chodorow, 2008b) challenged theview that using one rater is adequate by showingthat preposition usage errors actually do not havehigh inter-annotator reliability.
For example, trainedraters typically annotate preposition errors with akappa around 0.60.
This low rater reliability hasrepercussions for system evaluation: Their experi-ments showed that system precision could vary asmuch as 10% depending on which rater?s judgmentsthey used as the gold standard.
For some grammat-ical errors such as subject-verb agreement, whererules are clearly defined, it may be acceptable touse just one rater.
But for usage errors, the rulesare less clearly defined and two native speakers canhave very different judgments of what is acceptable.One way to address this is by aggregating a multi-tude of judgments for each preposition and treatingthis as the gold standard, however such a tactic hasbeen impractical due to time and cost limitations.While annotation is a problem in this field, com-paring one system to another has also been a majorissue.
To date, none of the preposition and articleerror detection systems in the literature have beenevaluated on the same corpus.
This is mostly due tothe fact that learner corpora are difficult to acquire(and then annotate), but also to the fact that they are1(Tetreault and Chodorow, 2008b) report that it would take80hrs for one of their trained raters to find and mark 1,000preposition errors.45usually proprietary and cannot be shared.
Examplesinclude the Cambridge Learners Corpus2 used in(Felice and Pullman, 2009), and TOEFL data, usedin (Tetreault and Chodorow, 2008a).
This makes itdifficult to compare systems since learner corporacan be quite different.
For example, the ?difficulty?of a corpus can be affected by the L1 of the writ-ers, the number of years they have been learning En-glish, their age, and also where they learn English (ina native-speaking country or a non-native speakingcountry).
In essence, learner corpora are not equal,so a system that performs at 50% precision in onecorpus may actually perform at 80% precision ona different one.
Such an inability to compare sys-tems makes it difficult for this NLP research area toprogress as quickly as it otherwise might.In this paper we show that the Amazon Mechani-cal Turk (AMT), a fast and cheap source of untrainedraters, can be used to alleviate several of the evalua-tion and annotation issues described above.
Specifi-cally we show:?
In terms of cost and time, AMT is an effec-tive alternative to trained raters on the tasks ofpreposition selection in well-formed text andpreposition error annotation in ESL text.?
With AMT, it is possible to efficiently collectmultiple judgments for a target construction.Given this, we propose a new method for evalu-ation that finally allows two systems to be com-pared to one another even if they are tested ondifferent corpora.2 Amazon Mechnical TurkAmazon provides a service called the Mechani-cal Turk which allows requesters (companies, re-searchers, etc.)
to post simple tasks (known as Hu-man Intelligence Tasks, or HITs) to the AMT web-site for untrained raters to perform for payments aslow as $0.01 in many cases (Sheng et al, 2008).Recently, AMT has been shown to be an effectivetool for annotation and evalatuation in NLP tasksranging from word similarity detection and emotiondetection (Snow et al, 2008) to Machine Transla-tion quality evaluation (Callison-Burch, 2009).
Inthese cases, a handful of untrained AMT workers2http://www.cambridge.org/elt(or Turkers) were found to be as effective as trainedraters, but with the advantage of being considerablyfaster and less expensive.
Given the success of us-ing AMT in other areas of NLP, we test whether wecan leverage it for our work in grammatical error de-tection, which is the focus of the pilot studies in thenext two sections.The presence of a gold standard in the above pa-pers is crucial.
In fact, the usability of AMT for textannotation has been demostrated in those studies byshowing that non-experts?
annotation converges tothe gold standard developed by expert annotators.However, in our work we concentrate on tasks wherethere is no single gold standard, either because thereare multiple prepositions that are acceptable in agiven context or because the conventions of preposi-tion usage simply do not conform to strict rules.3 Selection Task0.600.650.700.750.800.850.901 2 3 4 5 6 7 8 9 10KappaNumber of TurkersWriter vs. AMTRater 1 vs. AMTRater 2 vs. AMTFigure 1: Error Detection Task: Reliability of AMT as afunction of number of judgmentsTypically, an early step in developing a preposi-tion or article error detection system is to test thesystem on well-formed text written by native speak-ers to see how well the system can predict, or select,the writer?s preposition given the context aroundthe preposition.
(Tetreault and Chodorow, 2008b)showed that trained human raters can achieve veryhigh agreement (78%) on this task.
In their work, arater was shown a sentence with a target prepositionreplaced with a blank, and the rater was asked to se-lect the preposition that the writer may have used.We replicate this experiment not with trained ratersbut with the AMT to answer two research questions:1.
Can untrained raters be as effective as trained46raters?
2.
If so, how many raters does it take tomatch trained raters?In the experiment, a Turker was presented witha sentence from Microsoft?s Encarta encyclopedia,with one preposition in that sentence replaced witha blank.
There were 194 HITs (sentences) in all, andwe requested 10 Turker judgments per HIT.
SomeTurkers did only one HIT, while others completedmore than 100, though none did all 194.
The Turk-ers?
performance was analyzed by comparing theirresponses to those of two trained annotators and tothe Encarta writer?s preposition, which was consid-ered the gold standard in this task.
Comparing eachtrained annotator to the writer yielded a kappa of0.822 and 0.778, and the two raters had a kappa of0.742.
To determine how many Turker responseswould be required to match or exceed these levels ofreliability, we randomly selected samples of varioussizes from the sets of Turker responses for each sen-tence.
For example, when samples were of size N =4, four responses were randomly drawn from the setof ten responses that had been collected.
The prepo-sition that occurred most frequently in the samplewas used as the Turker response for that sentence.
Inthe case of a tie, a preposition was randomly drawnfrom those tied for most frequent.
For each samplesize, 100 samples were drawn and the mean valuesof agreement and kappa were calculated.
The reli-ability results presented in Table 1 show that, withjust three Turker responses, kappa with the writer(top line) is comparable to the values obtained fromthe trained annotators (around 0.8).
Most notable isthat with ten judgments, the reliability measures aremuch higher than those of the trained annotators.
34 Error Detection TaskWhile the previous results look quite encouraging,the task they are based on, preposition selection inwell-formed text, is quite different from, and lesschallenging than, the task that a system must per-form in detecting errors in learner writing.
To exam-ine the reliability of Turker preposition error judg-ments, we ran another experiment in which Turkerswere presented with a preposition highlighted in asentence taken from an ESL corpus, and were in-3We also experimented with 50 judgments per sentence, butagreement and kappa improved only negligibly.structed to judge its usage as either correct, incor-rect, or the context is too ungrammatical to makea judgment.
The set consisted of 152 prepositionsin total, and we requested 20 judgments per prepo-sition.
Previous work has shown this task to be adifficult one for trainer raters to attain high reliabil-ity.
For example, (Tetreault and Chodorow, 2008b)found kappa between two raters averaged 0.630.Because there is no gold standard for the er-ror detection task, kappa was used to compareTurker responses to those of three trained anno-tators.
Among the trained annotators, inter-kappaagreement ranged from 0.574 to 0.650, for a meankappa of 0.606.
In Figure 2, kappa is shown for thecomparisons of Turker responses to each annotatorfor samples of various sizes ranging from N = 1 toN = 18.
At sample size N = 13, the average kappa is0.608, virtually identical to the mean found amongthe trained annotators.0.400.450.500.550.600.651 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18KappaNumber of TurkersRater 1 vs. AMTRater 2 vs. AMTRater 3 vs. AMTMeanFigure 2: Error Detection Task: Reliability of AMT as afunction of number of judgments5 Rethinking EvaluationWe contend that the Amazon Mechanical Turk cannot only be used as an effective alternative annota-tion source, but can also be used to revamp evalu-ation since multiple judgments are now easily ac-quired.
Instead of treating the task of error detectionas a ?black or white?
distinction, where a preposi-tion is either correct or incorrect, cases of prepo-sition use can now be grouped into bins based onthe level of agreement of the Turkers.
For example,if 90% or more judge a preposition to be an error,47Task # of HITs Judgments/HIT Total Judgments Cost Total Cost # of Turkers Total TimeSelection 194 10 1,940 $0.02 $48.50 49 0.5 hoursError Detection 152 20 3,040 $0.02 $76.00 74 6 hoursTable 1: AMT Experiment Statisticsthe high agreement is strong evidence that this is aclear case of an error.
Conversely, agreement lev-els around 50% would indicate that the use of a par-ticular preposition is highly contentious, and, mostlikely, it should not be flagged by an automated er-ror detection system.The current standard method treats all cases ofpreposition usage equally, however, some are clearlyharder to annotate than others.
By breaking an eval-uation set into agreement bins, it should be possibleto separate the ?easy?
cases from the ?hard?
casesand report precision and recall results for the differ-ent levels of human agreement represented by differ-ent bins.
This method not only gives a clearer pic-ture of how a system is faring, but it also amelioratesthe problem of cross-system evaluation when twosystems are evaluated on different corpora.
If eachevaluation corpus is annotated by the same numberof Turkers and with the same annotation scheme, itwill now be possible to compare systems by sim-ply comparing their performance on each respectivebin.
The assumption here is that prepositions whichshow X% agreement in corpus A are of equivalentdifficulty to those that show X% agreement in cor-pus B.6 DiscussionIn this paper, we showed that the AMT is an ef-fective tool for annotating grammatical errors.
Ata fraction of the time and cost, it is possible toacquire high quality judgments from multiple un-trained raters without sacrificing reliability.
A sum-mary of the cost and time of the two experimentsdescribed here can be seen in Table 1.
In the task ofpreposition selection, only three Turkers are neededto match the reliability of two trained raters; in themore complicated task of error detection, up to 13Turkers are needed.
However, it should be notedthat these numbers can be viewed as upper bounds.The error annotation scheme that was used is a verysimple one.
We intend to experiment with differentguidelines and instructions, and to screen (Callison-Burch, 2009) and weight Turkers?
responses (Snowet al, 2008), in order to lower the number of Turk-ers required for this task.
Finally, we will look atother errors, such as articles, to determine howmanyTurkers are necessary for optimal annotation.AcknowledgmentsWe thank Sarah Ohls and Waverely VanWinkle fortheir annotation work, and Jennifer Foster and thetwo reviewers for their comments and feedback.ReferencesChris Callison-Burch.
2009.
Fast, cheap, and creative:Evaluating translation quality using Amazon?s Me-chanical Turk.
In EMNLP.Rachele De Felice and Stephen G. Pullman.
2009.
Auto-matic detection of preposition errors in learner writing.CALICO Journal, 26(3).Michael Gamon, Jianfeng Gao, Chris Brockett, AlexKlementiev, William B. Dolan, Dmitriy Belenko, andLucy Vanderwende.
2008.
Using contextual spellertechniques and language modeling for esl error cor-rection.
In Proceedings of IJCNLP, Hyderabad, India,January.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineering,12:115?129.Victor Sheng, Foster Provost, and Panagiotis Ipeirotis.2008.
Get another label?
Improving data quality anddata mining using multiple, noisy labelers.
In Pro-ceeding of ACM SIGKDD, Las Vegas, Nevada, USA.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Ng.
2008.
Cheap and fast ?
but is it good?evaluating non-expert annotations for natural languagetasks.
In EMNLP.Joel R. Tetreault and Martin Chodorow.
2008a.
The upsand downs of preposition error detection in ESL writ-ing.
In COLING.Joel Tetreault and Martin Chodorow.
2008b.
NativeJudgments of non-native usage: Experiments in prepo-sition error detection.
In COLING Workshop on Hu-man Judgments in Computational Linguistics.48
