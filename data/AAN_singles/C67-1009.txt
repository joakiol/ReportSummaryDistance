EXPERIMENTS WITH A POWERFUL PARSERbyMart in KAYThe Rand Corporation1700 Main StreetSANTA-MONICA - Cal i fornia 90406 - U.S.A.EXPERIMENTS WITH A POWERFUL PARSERThis paper describes a sophisticated syntactic-analysis program for the IBM 7040/44 computer anddiscusses some of the problems which it brings tolight.
Basically the program is a nondeterministicdevice which applies unrestricted rewriting rules toa family of symbol strings and delivers as output allthe strings that can be derived from members of theinitial family by means of the rules provided.
Asubsidiary mechanism deals with the relation of dom-inance, in the sense common in linguistics.
Thismakes it possible for rules to refer to complete orpartial syntactic structures, or P-markers, so thatthe program can be used at least to some extent fortransformational analys is .A program of this kind, which is intended for analy-sing natural languages, must be capable of operatingon a family of strings as a single unit because ofthe grammatical ambiguity of words.
Take, for ex-ample, the famous sentence "Time flies like an ar-row."
These five words are not, themselves, theprimary data on which a parsing program can be ex-pected to operate.
Instead, each word is replacedby one or more symbols representing the grammaticalcategories to which it belongs.
The assignments forthis example might be somewhat as follows:Wor d Grammatical categoryTimeflieslikeanarrowNoun, verb, adjectivePlural noun, 3rd person verbSingular noun, preposition,verbIndefinite articleS ingu lar  noun, adjective.Taking one category symbol for each word, it is pos-sible to form 30 different strings, preserving theorder of the original sentence.
These 30 stringsconstitute the family on which the program wouldoperate if set to analyze this sentence.The program is said to perform as a non-deterministicdevice because whenever two mutually incompatiblerules are applicable to the same string neither isgiven any priority; both are applied, and the result-ing strings developed independently.
Given thestring "A B C" and the rulesiA B- -~X YB C- '~Zthe program will therefore produce two new strings:X Y CA ZThe program contains no mechanism for guardingagainst sequences of rules which do not terminate.If the grammar contains the following rulesA B"~B AB A- -~A Band the string to be parsed contains either "A B y' or"B A," then the program will continue substitutingthese sub-strings for one another until the spaceavailable for intermediate results is exhausted.
Thismay hot seem to present any particularly severe pro-blem because a pair of rules such as these wouldnever appear in any properly constructed grammar.
But,as we shall shortly see, entirely plausible grammarscan be constructed for which this problem does arise.i.
THE FORM OF RULESIn order to get a general idea of the capabilitiesof the program, it will be useful first to cons iderthe notation used for presenting rules to it and theway this is interpreted by the machine.
In what fol-lows, we shall assume that the reader is familiarwith the terminology and usual conventions of phrase-structure and transformational grammar.
An exampleof the simplest kind of rewrite rule isVPRSG = PRES SG VERBThe Y'equals" sign is used in place of the more famil-iar arrow to separate the left and right-hand sidesof the rule.
The symbols on which the rules operateare words consisting of between one and six alphabeticcharacters.
The above rule will replace the symbol"VPRSG" by a string of three symbols "PRES SG VERB"whenever it occurs.
The following rule will invertthe order of the symbols "VERB" and "ING"VERB ING = ING VERBThe simplest way to represent a context free phrasestructure rule is as in the ' following example:NP AUX VP = SNotice that the normal order of the left and right-hand sides of the rule is reversed because the recog-nition process consists in rewriting strings as sin-g le  symbols; the rules must therefore take the formof reductions rather than productions.The program will accept phrase structure rules inthe form we have shown, but, in applying them, itwill not keep a record of the total sentence struc-ture to which they contribute.
In other words, itwill cause a new string to be constructed, but willnot relate this string in any way to the stringwhich was rewritten.
One way to cause this relation-ship to be preserved is to write the rule in the fol-lowing form :NP.I AUX.
2 VP.3 = S(I 2 3)The number following the symbols on the left,handside of the rule function very much like the numbersfrequently associated with structural indices intransformational rules.
When the\]eft-hand side o fthe rule is found to match a particular sub-string, ~the number associated with a given symbol in therule becomes a pointer to, or a temporary name for,that symbol.
With this interpretation, the left-hand side of the above rule can be read somewhat asfollows "Find an NP and call it i; Find an AUX fol-lowing this and call it 2; Find a VP following thisand call it 3.
"The numbers in parentheses after a symbol on the right-hand side of a rule are pointers to items-identifiedby the left-hand side, and which the new symbol mustdominate.
In the example, the symbol "S" is to dom-inate all the symbols mentioned on  the left-handside.A pointer may refer to a single symbol, as we :haveshown, or to a string of symbols.
The following ruleis equivalent to the one just described:NP.I AUX.
I VP.I = S(1)Furthermore, the string to which a pointer refersneed not be continuous.
Consider the followingexampleNP.
I  AUX VP .
I  -- S(1)3This will cause any  string I'NP AUX VP" to be re-written as "S", but the "S" will dominate only "NP" and"VP."
There will be no evidence of the intervening"AUX" in the ~na l  P-marker which will contain thefollowing phrase:S / \  .l~  VPConsider now the following pairs of rules:A.I B.2 C.I D.2 = P(1) Q(2)P.l Q.I = s(1)If these rules are applied to the string "A B C D"the following P-marker will be formed:/\/\oNotice that the first rule in the pair not only re-orders the symbols in the P-marker but forms twophrases simultaneously.A different way of using pointer numbers on the right-hand side can be illustrated by comparing the effectsof the following two rules:N. I  SG.1 V.2 SO.3 = NOUN(l) V(2) SG(3)N.I SG.I V.2 SG.2 = NOUN(l) 2What is required, we assume, is a context sensitivephrase structure rule which will rewrite "N SG" as"NOUN" in the environment before "V SG".
The firstrule achieves this effect but also introduces a new"J~" dominating the old one, and a new "SG".
Thesecond rule does what it really wanted: It constructsphrase labeled "NOUN" as required, and leaves thesymbols referred to by pointer number 2 unchanged.The context sensitive rule just considered is pre Isumably intended to insure that singular verbs haveonly singular subjects.
A second rule in which "SG"is replaced by "PL" would be required for plura~verbs.
But, ~nce  agreements of this kind may wellhave to be specified in other parts of the grammar,the situation might better be described by the fol-lowing three rules:SG.I = NUM(1)PL .
I  = NUM(!
)N.I NUM.
2 V.3 2 = NOUN(I 2) 3 2The first two rules introduce a node labeled "NUM'!into the structure above the singular and p lura lmorphemes.
The third rule checks for agreement andforms the subject noun phrase.
Pointer number 2 isassociated with the symbol "NUM" in the second placeon the left-hand side, and occurs by itself in thefourth place.
This means that the fourth symbolmatched by the rule must be "NUM," and also that itmust dominate exactly the same sub-tree as the second.In the example we are assuming that "NUM" governs asingle node which will be labeled either "SG" or "PL"and the rule will ensure that whichever of these isdominated by the first occurrence of "NUM" will alsobe dominated by the second occurrence.
Notice thatnoun and verb phrases could be formed simultaneouslyby the following rule:N.I NUM.
2 V.3 2 = NOUN(I 2) VERB(3 2)The symbols "ANY" and "NULL" are treated in a specialway by this program and should not occur in stringsto be analyzed.
The use of the symbol "NULL" isillustrated in the rule:4PPH = NULLThis will cause the symbol "PPH" to be deleted fromany string in which occurs.
The program is non-deterministic in its treatment of rules of this kind,as elsewhere, so that it zwill consider analyses inwhich the symbol is deleted, as well as any which canbe made by retaining it.
The symbol "NULL" is usedonly on the right-hand sides of rules.The symbol "ANY" is used only on the left-hand sidesof rules and has the property that the word implies,namely that it will match any symbol in a string.
Theuse of this special symbol is illustrated in the fol-lowing rule:5VERB.I  ANY.
I NP.I = VP(1)This will form a verb phrase from a verb and a nounphrase, With one intervening word or phrase, whosegrmmnatical category is irrelevant.Elements on the left-hand sides of rules can be spec-ified as optional by writing a dollar sign to the leftor right of the symbol as in the following rules:DET.I ADJ$.I NOUN.
I = NP(1)VERB.1 SANY.1 NP.I = VP(1)The first of these forms a noun phrase from a deter-miner and a noun, with or without an intervening ad-jective.
The second is a new version of a rulealready considered.
A verb phrase is formed from averb and a noun phrase, with or without an interveningword or phrase of some other type.Elements can also be specified as repeatable bywriting an asterisk against the symbol, as in thefollowing example:VERB.
i *NP.
i = VP(1)This says that a verb phrase may consist of a verbfollowed by one or more noun phrases.
It is oftenconvenient to be able to specify that a given elementmay occur zero or more times.
This is done in the ob-vious way by combining the dollar sign and the aster-isk as in the following rule:SDET.I *$ADJ.
I N.I *PP$.I = NP(1)According to this, a noun may constitute a nounphrase by itself.
However the noun may be preceededby a determiner and any number of adjectives, andfollowed by a prepositional phrase, and all of thesewill be embraced by the new noun phrase that is form-ed.
Notice that the asterisk and the dollar sign canbe placed before or after the symbol they refer to.The combination is often useful with symbol "ANY" inrules of the following kindsN.I NUM.2 *$ANY.3 V.4 2 = NOUN(I 2) 3 VERB(4 2)This is similar to an earlier example.
It combinesthe number morpheneme with a subject noun and with a6verb, provided that the two agree, and ~a~ows for--any number of other symbols to intervene.
The sym-bol "ANY" with an asterisk and a dollar s~g n cor Lresponds in this system to the so called variablesin the familiar notation of transformational gram-mar.
.,.
?Consider now the following rule:SCONJ.1 NP(S) .
I  = NP(1)This will form a noun phrase from a subordinatingconjunction followed bya  nou~phrase, providedthat this dominates only the "~ymbol "S." Any sym-bol on the left-hand side of the rule may be fol-lowed by an expression in parentheses specifyingthe string of characters that this symbol must di-rectly dominate.
This expression is constructedexactly like the left-hand sides of rules.
Inparticular, it may contain symbols followed by ex-pressions in parentheses.
The following rule willserve as an illustration of this, and of anothernew feature:NP($DET.I $*ANY.I ADJ(PRPRT.2) $*ANY.
3 N.4$ PP.5) 1 3 4 WH DEF 4 BE ADJ((2)) 5This rule calls for a noun phrase consisting of anoun, a preceding adjective which dominates a ~re -sent participle and, optionally, a number of otherelements.
This noun phrase is replaced by thedeterminer from the original noun phrase, if thereis one, the elements preceding the noun except forthe present participle, the noun itself, the sym-bol '~H," the symbol "DEF~" another Copy of thenoun, the symbol f~E~" the symbol "ADJ" dominatingexactly those elements originally dominated by'~RPRT" and, finally, any following prepositionalphrases the original noun phrase may have contained.The number "2" in double parentheses following "ADJ"on the right-hand side of this rule specifies thatthis symbol is to dominate, not the present par-ticiple itself, but the elements, if any, that itdominates.
This device turns out to have wideutility.Double parentheses can also be used following a sym-bol on the left-hand side of a rule, but with adifferent interpretation.
We have seen how singleparentheses are used to specify the strin~ in~ne-diately dominated by a given symbol.
DouSle'parantheses enclose a string which must be a pro-per analysis of the sub-tree dominated by the givensymbol.
A string is said to be a proper analysisof a sub-tree if each terminal symbol of the.sub-tree is dominated by some member of the string.
Asusual, a symbol is taken to dominate itself.
As anexample of this, consider the following rule:ART.I S((ART N.2 ANY*)).I 2 = DET(1) 2This rule applies to a string consisting of anarticle, a sentence, and a noun.
The sentence mustbe analysable, at some level, as an article fol-lowed by a noun, followed by at least one otherword or phrase.
The noun in the embeded sentence,and the sub-tree it dominates, must be exactlymatched by the noun corresponding to the last ele-ment on the left-hand side of the rule.
The initialarticle and the embeded sentence will be collectedas a phrase under the symbol "DET" and the finalnoun will be left unchanged.The principal facilities available for writing ruleshave now been exemplified.
Another kind of rule isalso available which has a left-hand side like thosealready described but no equal sign or right-handside.
However it will be in the best interests ofclarity to defer an explanation of how these rulesare interpreted.The user of the program may write rules in exactlythe form we have described or may add in format ionto control the order in which the rules are applied.This additional information takes the form of anexpression written before the rule and separatedfrom it by a comma.
This expression, in its turn,takes one of the following forms:n I ,nl/n 2 ,nl/n2/n 3 ,nl//n 3 ,n I in an integer which orde~ this rule relativeto the others.
Since the same integer can beassigned to more than one rule, the ordering ispartial.
Rules to which no number is explicitlyassigned are given the number 0 by the program.n 2 and nx, when present, are interpreted as fol- J lows: Egery symbol in the sub-string matched bythe left-hand side of the rule must have been pro-duced by a rule with number i, where ng) i~ nq.For these purposes the symbols in the 5riginalfamily of strings offerred for analysis are treat-ed as though they had been produced by a rule withnumber O.2.
PHRASE-STRUCTURE GRAMMARIt will be clear from what has been said alreadythat this program is an exceedingly powerful de-vice capable of operating on strings and trees ina wide variety of ways.
It would clearly be en-tirely adequate for analyzing sentences with acontext-free phrase-structure grammar.
~ut thisproblem has been solved before, and much more sim-ply.
We have seen how the notation can be used towrite context-sensitive rules, and we should there-fore expect the program to be able to analyze sen-tences with a context-sensitive grammar.
Howeverin the design of parsing algorithms, as elsewhere,context-sensitive grammars turn out to be surpris-ingly more complicated than context-free grammars.The problem that context-sensitive grammars posefor this program can be shown.~with a simple ex-ample.
I Consider the following in grammar:E (S) (2)B -,- F/ E J (4)D _,.
~G/A- } (5)l B/___.E (6)This grammar, though trivial, is well behaved inall important ways.
The language generated,though regular and unambigious, is infinite.ii am indebted for this example, as for otherideas too numerous to document individually, toSusumu Kuno of Harvard University.Furthermore, every rule is useful for some de-rivation.
Since the language generated is un-ambigious, thegrammar is necessarily cycle-free,in otherwords, it produces no derivation inwhich the same line occurs more than once.
Sup-pose, however, that the gr~nmar is used for analy-sis and is presented With the string"A D E" --not a sentence of the language.
The attempt toanalyze this string using rules of the grammar re-suits in a rewriting operation that begins asfollows and continues indefinitely:A D EA B E (by rule 3)A D E (by rule 6)A B E (by rule 3)etc .I t  would c lear ly  be poss ib le ,  in  p r inc ipa l ,  toequ ip  the  program wi th  a procedure  fo r  detect ingcyc les  o f  th i s  sor t ,  but  the  t imerequ i red  bysuch a procedure ,  and the  complex i ty  that  i twould in t roduce  in to  the  program as a who le ,  a resu f f i c ient  to  ru le  i t  out  o f  a l l  p rac t i ca l  con-s iderat ion .
I t  might  be argued that  the  s t r ingswhich have to  be ana lyzed  in  p ract i ca l  s i tuat ionscome from rea l  texts  and can be assumed to  besentences .
The problem of  d i s t ingu ish ing  sen-tences  from nonsentences  i s  o f  academic in teres t .But, in natural languages, the assignment of wordsto grammatical categories is notoriously ambigiousand for this problem to arise it is enough forsuitably ambigious words to come together in thesentence.
A sentence which would be accepted bythe above gram~nar, but which would also give riseto cycles in the analysis, might consist of wordswith the following grammatical categories:Word Grau~at ica l  CategoryI A2 B3 C, Ei0The program, as it stands, contains no mechanismwhich automatically guards against cycles.
How-ever, if the user knows where they are likely tooccur or discovers them as a result of his exper-ience with the program, he can include some specialrules in his grammar which will prevent them fromoccurring.
These rules, which we have alreadyeluded to, are formally similiar to all others ex-cept that they contain no equals sign and no right-hand side.
When a P-marker is found to contain astring which matches the left-hand side of one ofthese rules, the program arranges that, thence for-ward, no other rule shall be allowed to apply tothe whole string.
The cycle in this latest examplecould not occur if the grammar contained the rule:A B E3.
TRANSFORMATIONAL GRAMMARWe now come to the main concern of this paper whichis to discuss the extent to which the program wehave been describing can be made to function as atransformational analyzer.
The main purpose of theexamples that have been given is to show the greatpower of the program as a processor of symbolstrings.
The notion of dominance is provided for,but only in a rudimentary way.
It certainly couldnot be claimed that the program is a tree processorin any really workable sense.
Butgrammat ica ltransformations are operations on trees and our in-vestigation therefore must take the form of showingthat these operations can frequently, if not always,be mimicked by string rewriting rules.We shall take it that a transformational grammarconsists of a context-free or context-sensitivephrase-structure component and a set of transforma-tions ordered in some way.
To begin with, verylittle will be lost if we assume that the transfor-mational rules are simply ordered.Gonsider now the first transformation in the list.In general, this may be expected to introducephrases into the P-markers to which it applies whichcould not have been generated by thephrase-structurecomponent.
Let us now write some additional phrase-structure rules capable of generating these newphrases.
Let us insert these rules into the grammarimmediately following the first transformationalrule and establish the convention that, when theyiiare used in the analysis of the string, their out-put will be used only as input to the first trans-formation.
Now treat the second transformationalrule in the same way.
It also can be expected tocreate new kinds of phrase and phrase-structurerules can be written which would recognize these.It may be that some of the phrases formed by thesecond rule could also be~formed by the first, andin this case, it may be possible to move the ap-propriate rule from its position after the firsttransformation to a position after the second andto mark it as providing input only for these tworules.Notice that the rules we are proposing to constructwill not constitute what has sometimes been calleda surface ~rammar.
The phrases they describe cer-tainly do not belong to the base structure andmany of them may not be capable of surviving un-changed into the surface structure.
In generalthese rules describe phrases which can only havetransititory existence somewhere in the genera-tive process.
Notice also that in order to describethese phrases adequately it may sometimes be nec-essary to extend the notion of phrase structuregrammar somewhat.
Consider for example the fol-lowing transformation:X - A - B - Yi 2 3 4Adjoin 2 as right daughter of 3If we make the usual ass~pt ion that a rule isapplied repeatedly until no proper analyses of theP-marker remain which can be matched by its struc-tural index, then this transformation, and manyothers, may produce phrases of indefinitely manytypes.
Let us suppose that, before this trans-formation is applied for the first time, all pos-sible phrases that can be dominated by the symbol"B" are describable by context free phrase struc-ture rules of the following formB--~ ?2 I!~k12where the 4. are any strings.
The phrase struc-ture gramma~ needed to describe all the phrasesthat can exsist after the operation of this trans-formation must co~tain the following rules, ormore accurately rule schemata41B ~  .
t2  .
A* , .If . "
.=kWhere the asterisk indicates one or more repeti-tions of the symbol "A".
If the left and right-hand sides of these rules are reversed and theyare presented to the program in the proper nota-tion, then the transformation itself can be re-presented by the following pair of rules:B(*$ANY.
I *A.2) = 2 B+ I +BB+ B.1 +B = iSince there are no facilities for specifying~dom-inance relations among elements on the right-handsides of these rules, it is necessary to resort tosubterfuge.
The phrase dominated by the symbol"B" is reproduced in the output of this rule withcopies of the symbol "A" removed from the right-hand end and the remainder bounded by the symbols"B+" and'tFB ".
These symbols serve to delimit apart of the string which can only figure in thecomplete analysis of the sentence if it constitutesa phrase of type "B'.
The second rule removesthese boundry symbols from the phrase of type 'B"and, since no pointer is assigned to them, they willleave no trace in the final P-marker.
,~/Another, and perhaps more economical, way to writerecognition rules corresponding to this transforma-tion involves conflating the additional phrase-structure rules with the reverse of the transfor-mational rule itself to give rules of the follow-ing kind :=.
i *A.2 = 2 B+ i +B (i ~ i < n) 1B+B.
I+B= i13In fact, the elementary transformation for daugh-ter adjunct!on that we are providing for here ismoregenera l  than that often allowed by transfor-mational grammarians.
It is common to require thatif some element !
is adjoined as a daughter of an"other element b then b must have no daughters be-fore the transformatiolltakes place.Sister adjunct!on can  be  treated in an analogousmanner.
Consider the.
following transformation:X - A - B - Yi 2 3 4Adjoin 2 as right sister of 4.The phrases exsisting before this transformation iscarried out, and which have "B" as a constitutent,can be thought of as being described by a set ofrules as follows:a I ~ B ~I- - -~B ~2 a2 | |!
!a - - -~ B ~n nHere the a. are nonterminal symbols and the u. are1 ?
1 ?
strings, possibly null.
The grammar which descrlbesthe phrases existing after the operation of thistransformation must contain, in addition, the fol-lowing rules :a l ~ B  ~i A*a2T-~B ~2 A*!
!t a - - - -~B ~ A* n n ~The reverse transformation itself can now be re-presented  by  a set  o f  ru les  as  fo l lows :B. I  ~.
.
i  A* .2 ffi 2 B+ I +B lNotice that the strings referred toby  the symbols"X" and "Y" in both of the above examples areunchanged by the transformation and are therefore14not mentioned at all in the analysis rules.Experience shows that it is in fact rarely nec-essaryto  write separate rules for each =.. Inmost cases, a transformation of this kindlcould be "handled in the program with a rule of the follow-ing form:B.I ANY.
I A*.2 = 2 B+ i +BThis is one of a large number of cases in which ithas been found that the analysis rules can be mademore permissive than the original grammar suggestswithout introducing spurious structures and withoutseriously increasing the amount of time or spaceused by the program.While it is possible that transformational analy-sis can be done in an interesting way with a pro-gram of this sort there seems to be little hope offinding an algorithm for writing analysis rulescorresponding to a given transformational grammar.The following rule also involves sister adjunctionbut poses much more serious problems than the pre-vious example:X - A - Y - B - Zi 2 3 4 5Adjoin 2 a~ right sister of 4The problem here is that a variable "Y" intervenesbetween "A" and "B".
On the face of it, the analy-sis rule corresponding to this transformation wouldhave to be somewhat as follows:*$ANY.
I B.2 *A.3 -- 3 1 2And in principal the program could carry out arule of thfs kind.
However the first symbol on theleft-hand side of this rule will match any stringwhatsoever, so that, if the rule can be applied ~tall, it can be applied in a prodigious number ofways.
But, with real grammars, it usually turns outthat quite a lot can be said about the pa~t of thesentence covered by the variable "Y" so that analy-sis rules =an be written which are sufficientlYspecific to be practiable.
L~15iDeletions are notoriously troublesome in grammarsof any kind because theycan so easily give riseto cycles and undecidable problems.
Transforma-tional gran~arians require that lexical itemsshould only be deleted from a P-marker if thereis some other copy of the same item whichremains.This condition insures what they call the recover-ability of the transformation.
However, it is veryimportant to realize that recoverability, in thissense is a very weak condition.
The requirementis that, knowing that an item has been deletedfrom a certain position in the P-marker, it shouldbe possible to tell what that item was.
But thereis no requirement that a P-marker should containevidence that it was derived by means of a dele-tion transformation or of the places in it wheredeletions might have taken place.Deletions are more easy to cope with in certainsituations than others.
Consider for example thefollowing transformation:X - A - B - A - Yi 2 3 4 5Delete 4.The recoverability requirement is satisfied be-cause of the identity of the second and fourthelements in the structural index.
The corres-ponding rule for the program might be as follows:23/22, A.I B.2 = 1 2 1It is necessary to provide ordering informationwith a rule of this kind because it would otherwisebe capable of operating on its own output andcycling indefinately.
But  presumably this trans-formation can be carried out any number of t imesand the same therefore should be true of the cor-responding analysis rule.
Once again, experienceshows that the grammarian almost invariably knowsmore about the environment in which a deletiontakes place than is stated in the rule, and ifthis information is used carefully, analysis rulescan be written which do not lead to cycles.In principle the situation is even worse in rulesof the following kind:16X - A - Y - A - ZI 2 3 4 5Delete 4Here the third element is a variable which cancover any number of nodes in the P-marker.
Inanalysis we are therefore not only without in-formation about how many times the rule mayhave been applied but we know nothing aboutwhere to insert new copies of the symbol "A",except that they must be to the right of theexisting copy.The other commonly used elementary transforma-tions (substitutions and Chomsky-adjunction) donot present special problems.
The main outstand-ing difficulty comes from the fact that trans-formationalrules are ordered.
We have alreadysaid that the theory of transformational grammaris in the state of continual change and this isparticularly true of the part that concerns theordering of rules.
For this reason we have as-sumed that the rules are simply ordered in thehope that other possibilities will not be notablymore  difficult to deal with.
We shall also makethe assumption that transformational rules are allobligatory.Consider now the following grammarPhrase structurei.
S - -~A (D) B C2.
C - -~D ETransformationsi.
A - B - X1 2 30 2+1 3and suppose that the program is required toanalyze the string "A D B E".
Since, in genera-tion, the list of transformations is read fromtop to bottom it is reasonable to suppose thatin analysis it should be read from bottom to top.17We may take it that the analysis rule corres-ponding to the second transformation is some-what as follows:D.I B.2 = 2 1This, together with the two phrase-structurerules, is sufficient to give a complete analysisof the string with this underlying P-marker:But if this is an underlying P-marker, the secondtransformational rule could not possibly be usedto produce a derived structure from it becausethe first transformation, which according to ourassumption is obligatorY , can be applied to itgiving the following result:It is in fact not  sufficient to scan the list oftransformations from bottom to top because thisprocedure does not make allowance for the factthat the transformations are obligatory.
To re-gard transformations as optional which were in-tended to be obligatory isJin general to associatespurious base structures to some sentences.
Thesolution for the present gr~,mar is to use thefollowing set of analysis rules:I/0, B D2/1, D.I B.2 = 2 1 13/2, A B4/3, B' I  A.
2 = 2 1D.1 F.1 = c(1)A.I $D.I B.I C.I = S(1)The first and third rules contain, in effect, thestructural indices from the second and firsttransformations respectively.
The first rule saysthat no string i s  acceptable as a sentence whichcontains "B D" as a sub-string because to this it18would have beenpossible to'apply transformation2.
The second rule reverses the effect of trans-formation 2.
The third rule, excludes any P-marker existing at this stage with a proper analy-sis containing "A B" as a sub-string.
This isthe structural index of transformation i whichtherefore shouldhave been applied to any P-marker containing it.
The fourth rule reversesthe effect of transformation i and the remainingrules are the phrase-structure component of thegrammar.
Once again it turns out that  what maybe necessary in theory is only rarely needed inpractice.
Experience with t, his program is, sofar, very limited but no cases haveso  far beenfound in which incorrect analysis have resultedfrom omitting rules such as those numbered oneand three above.CONCLUSIONSIt requires skill to write rules for analyzingnatural sentences with the program described inthis paper.
A program can only properly be call-ed a transformational parser if it can work dir-ectly with the unedited rules of the transformation-al grammar.
But no algorithm is known, nor is itlikely that one will shortly be found, which willproduce from a transformational grammar a set ofcorresponding rules of the kind required by thisprogram.
It is not d~fficult to construct atransformational grammar for which no exactlycorresponding set of analysis rules can be writ-ten.
However, other programs have been writtenwhich, though they are still in many ways im-perfect, can more reasonably be called transfor-mational parsers.
What then are the advantagesof the present program?The current version of the program is written inALGOL and with very little regard for efficiency.But the basic algorithm is inherently a verygreatdeal more efficient than any of its competitors.The various interpretations of an ambiguous sen-tence, or a sentence which seems likely to beambiguous in the early stages of analysis, area l lworked on simultaneously.
A t  no stage canthe program be sa~d to be developing one inter-pretation of a sentence rather than another.
Iftwo interpretations differ only inseme small partof the P-marker, then only one complete P-marker19is stored with two versions of the ambiguouspart.
Work done on the unambiguous portion isdone only once for both interpretations.The program, though undoubtably very powerful,seems naive from the point of view of modernlinguistic theory.
The program embodies verylittle of what we know or believe to be trueabout the structure of natural languages.
Itmight well be said that a computer program foranalyzing natural languages is only interestingto the extent that it makes a claim about thebasic form of those languages.
But the programdescribed here is intended as a tool and not asa linguistic hypothesis.
There is much to belearned about natural language from ruminatingon the form of universal generative grammar andtrading counter-example for example.
But thereis also much to be learned from studying text asit actually occurs.
The small amount of workthat has so far been done with this program hasbeen sufficient to suggest strongly that a setof rules derived algorithmically from a trans-formational grammar is unlikely to be the mosteffective or the most revealing analytic device.20
