Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 176?184,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsGrading the Quality of Medical EvidenceBinod Gyawali, Thamar SolorioCoRAL LabDepartment of Computer and Information SciencesUniversity of Alabama at Birmingham, AL, USA{bgyawali,solorio}@cis.uab.eduYassine BenajibaClinical Decision Support Solutions DepartmentPhilips Research North America, Briarcliff Manor, NY, USAyassine.benajiba@philips.comAbstractEvidence Based Medicine (EBM) is the prac-tice of using the knowledge gained from thebest medical evidence to make decisions inthe effective care of patients.
This medi-cal evidence is extracted from medical docu-ments such as research papers.
The increas-ing number of available medical documentshas imposed a challenge to identify the ap-propriate evidence and to access the qualityof the evidence.
In this paper, we presentan approach for the automatic grading of ev-idence using the dataset provided by the 2011Australian Language Technology Association(ALTA) shared task competition.
With thefeature sets extracted from publication types,Medical Subject Headings (MeSH), title, andbody of the abstracts, we obtain a 73.77%grading accuracy with a stacking based ap-proach, a considerable improvement over pre-vious work.1 Introduction?Evidence Based Medicine (EBM) is the conscien-tious, explicit, and judicious use of current best evi-dence in making decisions about the care of individ-ual patients?
(Sackett et al, 1996).
EBM requires toidentify the best evidence, understand the method-ology and strength of the approaches reported inthe evidence, and bring relevant findings into clin-ical practice.
Davidoff et al (1995) express EBM interms of five related ideas.
Their ideas imply thatthe conclusions should be derived based on the bestevidence available, the clinical decisions should bemade based on the conclusions derived, and the per-formance of the clinical decisions should be evalu-ated constantly.
Thus, physicians practicing EBMshould be constantly aware of the new ideas andthe best methodologies available based on the mostrecent literature.
But the amount of clinical docu-ments available is increasing everyday.
For exam-ple, Pubmed, a service of the US National Library ofMedicine contains more than 21 million citations forbiomedical literature from MEDLINE, life sciencejournals, and online books (last updated on Decem-ber 7, 2011) 1.
The abundance of digital informa-tion makes difficult the task of evaluating the qualityof results presented and the significance of the con-clusions drawn.
Thus, it has become an importanttask to grade the quality of evidence so that the mostsignificant evidence is incorporated into the clinicalpractices.There are several scale systems available to grademedical evidence.
Some of them are: hierarchyof evidence proposed by Evans (2003), Grading ofRecommendations Assessment, Development, andEvaluation (GRADE) scale by GRADE (2004), andStrength of Recommendation Taxonomy (SORT)scale by Ebell et al (2004).
The SORT scale ad-dresses the quality, quantity, and consistency of evi-dence and proposes three levels of ratings: A, B, andC.
Grade A is recommended based on the consistent,good-quality patient-oriented evidence, grade B isbased on the inconsistent or limited-quality patient-oriented evidence, and grade C is based on consen-sus, disease-oriented evidence, usual practice, ex-pert opinion or case studies.1http://www.ncbi.nlm.nih.gov/books/NBK3827/176The Australasian Language Technology Associa-tion (ALTA) 2011 organized the shared task compe-tition2 to build an automatic evidence grading sys-tem for EBM based on the SORT grading scale.
Wecarry out our experiments using the data set providedfor the competition and compare the accuracy ofgrading the evidence by applying basic approachesand an ensemble (stacking) based approach of clas-sification.
We show that the later approach canachieve 73.77% of grading accuracy, a significantimprovement over the basic approaches.
We furtherextend our experiments to show that, using featuresets generated from the method and conclusion sec-tions of the abstracts helps to obtain higher accuracyin evidence grading than using a feature set gener-ated from the entire body of the abstracts.2 Related WorkTo the best of our knowledge, automatic evidencegrading based on a grading scale was initiated bySarker et al (2011).
Their work was based on theSORT scale to grade the evidence using the corpusdeveloped by Molla-Aliod (2010).
They showedthat using only publication types as features couldyield an accuracy of 68% while other informationlike publication types, journal names, publicationyears, and article titles could not significantly helpto improve the accuracy of the grading.
Molla-Aliodand Sarker (2011) worked on the evidence gradingproblem of 2011 ALTA shared task and achievedan accuracy of 62.84% using three sequential clas-sifiers, each trained by one of the following featuresets: word n-grams from the abstract, publicationtypes, and word n-grams from the title.
They ap-plied a three way classification approach where theinstances classified as A or C were removed fromthe test set and labeled as such, while instancesclassified as B were passed to the next classifier inthe pipeline.
They repeated this process until theyreached the end of three sequential classifiers.Most of the EBM related work is focused on ei-ther the identification of important statements fromthe medical abstracts or the classification of med-ical abstracts to facilitate the retrieval of impor-tant documents.
Work by Demner-Fushman et al(2006), Dawes et al (2007), Kim et al (2011) au-2http://www.alta.asn.au/events/sharedtask2011tomatically identify the key statements in the med-ical abstracts and classify them into different levelsthat are considered important for EBM practitionersin making decisions.
Kilicoglu et al (2009) workedon recognizing the clinically important medical ab-stracts using an ensemble learning method (stack-ing).
They used different combinations of featurevectors extracted from documents to classify the ev-idence into relevant or non relevant classes.
Theyapproached the problem as a binary classificationproblem without using any grading scales.Systematic Reviews (SRs) are very importantto support EBM.
Creating and updating SRs ishighly inefficient and needs to identify the best evi-dence.
Cohen et al (2010) used a binary classifica-tion system to identify the documents that are mostlikely to be included in creating and updating SRs.In this work, we grade the quality of evidencebased on the SORT scale, that is different from mostof the existing works related to classification of ab-stracts and identification of key statements of ab-stracts.
We work on the same problem as by Molla-Aliod and Sarker (2011) but, we undertake the prob-lem with a different approach and use different setsof features.3 DatasetWe use the data of 2011 ALTA shared task compe-tition that contains three different sets: training, de-velopment and test set.
The number of evidence in-stances present in each set is shown in Table 1.
Eachdata set consists of instances with grades A, B, or Cbased on the SORT scale.
The distribution of evi-dence grades is shown in Table 2.Data Set No.
of Evidence InstancesTraining Set 677Development Set 178Test Set 183Table 1: Evidence per data setThe evidence instances were obtained from thecorpus developed by Molla-Aliod and Santiago-Martinez (2011).
The corpus was generated basedon the question and the evidence based answer forthe question along with SOR grade obtained fromthe ?Clinical Inquiries?
section of the Journal of177Grades Trainingset (%)Developmentset (%)Test set(%)A 31.3 27.0 30.6B 45.9 44.9 48.6C 22.7 28.1 20.8Table 2: Evidence distribution per gradeFamily Practice (JFP).
A sample question from theJFP Clinical Inquiries section is ?How does smokingin the home affect children with asthma??.
Each ev-idence contains at least one or more publications de-pending upon from which publications the evidencewas generated.
Each publication is an XML file con-taining information such as abstract title, abstractbody, publication types, and MeSH terms.
Eachpublication is assigned at least one publication typeand zero or more MeSH terms.
The MeSH termsvocabulary 3 is developed and maintained by theNational Library of Medicine and is used in rep-resentation, indexing and retrieval of medical doc-uments.
Some of the medical document retrievalwork emphasizes the use of MeSH terms in the ef-ficient retrieval of documents (Trieschnigg et al,2009; Huang et al, 2011).
MeSH terms are alsoused in document summarization (Bhattacharya etal., 2011).Figure 1: Sample data fileEach data set contains an additional grade filewith the information related to the evidence in-stances, their grades, and the publications.
A sam-ple of the file is shown in Figure 1.
The first columncontains the evidence id, the second column containsthe grades A, B, or C of the evidence based on theSORT scale, and the remaining columns show thepublication id of each publication in the evidence.3http://www.nlm.nih.gov/meshThe problem in this task is to analyze the publica-tions in each evidence provided and classify theminto A, B or C.The dataset available for our research has ab-stracts in two different formats.
One of them con-tains abstracts divided into sections: background,objective, method, result, and conclusion.
The otherformat contains abstracts with all the information ina single block without any sections.
A sample of anabstract having only four sections in the given datais shown below:Objectives: To determine the effectiveness of a musclestrengthening program compared to a stretching program inwomen with fibromyalgia (FM).Methods: Sixty-eight women with FM were randomly as-signed to a 12 week, twice weekly exercise program consistingof either muscle strengthening or stretching.
Outcome measuresincluded muscle strength (main outcome variable), flexibility,weight, body fat, tender point count, and disease and symptomseverity scales.Results: No statistically significant differences betweengroups were found on independent t tests.
Paired t tests revealedtwice the number of significant improvements in the strengthen-ing group compared to the stretching group.
Effect size scoresindicated that the magnitude of change was generally greater inthe strengthening group than the stretching group.Conclusions: Patients with FM can engage in a speciallytailored muscle strengthening program and experience an im-provement in overall disease activity, without a significant exer-cise induced flare in pain.
Flexibility training alone also resultsin overall improvements, albeit of a lesser degree.In the abstract above, we see that the approachesapplied for the study are described in the methodsection, and the outcome and its effectiveness aredescribed in the conclusion section.4 Proposed MethodologyIn this paper we propose a system to identify thecorrect grade of an evidence given publications inthe evidence.
We deal with the problem of evi-dence grading as a classification problem.
In evi-dence grading, basic approaches have been shownto have poor performance.
Molla-Aliod and Sarker(2011) showed that a basic approach of using simplebag-of-word features and a Naive Bayes classifierachieved 45% accuracy and proposed a sequentialapproach to improve the accuracy at each step.
Ourpreliminary studies of applying the simple classifi-cation approach also showed similar results.
Here,we propose a stacking based approach (Wolpert,1781992) of evidence grading.
Stacking based approachbuilds a final classifier by combining the predictionsmade by multiple classifiers to improve the predic-tion accuracy.
It involves two steps.
In the first step,multiple base-level classifiers are trained with dif-ferent feature sets extracted from a dataset and theclassifiers are used to predict the classes of a sec-ond dataset.
Then, a higher level classifier is trainedusing the predictions made by the base-level clas-sifiers on the second dataset and used to predict theclasses of the actual test data.
In this approach, base-level classifiers are trained independent of each otherand allowed to predict the classes.
Based on thepredictions made by these base-level classifiers, thehigher level classifier learns from those predictionsand makes a new prediction that is the final class.Our stacking based approach of classification usesfive feature sets.
In the first step of classification, wetrain five classifiers using different feature sets perclassifier and use the classifiers to predict the gradesof the development dataset.
Thus, at the end of thefirst step, five different predictions on the develop-ment dataset are obtained.
In the second step, a newclassifier is trained using the grades predicted by thefive classifiers as features.
This new classifier is thenused to predict the grades of the test dataset.5 FeaturesWe extracted six sets of features from the publica-tions to perform our experiments.
They are as fol-lows:1.
Publication types2.
MeSH terms3.
Abstract title4.
Abstract body5.
Abstract method section6.
Abstract conclusion sectionFor feature set 1, we extracted 30 distinct publi-cation types from the training data.
For the MeSHterms feature set, we selected 452 unique MeSHterms extracted from the training data.
The publi-cations contained the descriptor name of the MeSHterms having an attribute ?majortopicyn?
with value?Y?
or ?N?.
As MeSH terms feature set, we selectedonly those MeSH term descriptor names having ma-jortopicyn=?Y?.We extracted the last four sets of features fromthe title, body, method, and conclusion sections ofthe abstracts.
Here, the body of an abstract meansthe whole content of the abstract, that includes back-ground, objective, method, result, and conclusionsections.
We applied some preprocessing steps togenerate these feature sets.
We also applied a featureselection technique to reduce the number of featuresand include only the high informative features fromthese feature sets.
The details about preprocess-ing and feature selection techniques are described inSection 6.We performed all the experiments on the basis ofevidence, i.e.
we created a single feature vector perevidence.
If an evidence contained more than onepublication, we generate its features as the union ofthe features extracted from all its publications.The grades of the evidence in the SORT scaleare based on the quality of evidence, basis of ex-periments, the methodologies used, and the types ofanalysis done.
Grades also depend upon the effec-tiveness of the approach used in the experiments.The method section of an abstract contains the in-formation related to the basis of the experiments,such as randomized controlled trails, systematic re-view, cohort studies, and the methods used in theirresearch.
The conclusion section of the abstractusually contains the assertion statements about howstrongly the experiment supports the claims.
Anal-ysis of the contents of abstracts shows that the in-formation needed for grading on SORT scale is typ-ically available in the method and conclusion sec-tions, more than in the other sections of the abstracts.Thus, we used the method and conclusion sectionsof the abstracts to generate two different feature setsso that only the features more likely to be importantin grading using the SORT rating would be included.Separating method and conclusion sections ofthe abstractsIn order to extract features from the method and con-clusion sections, we should separate them from thebody of abstracts, which is a challenging task forthose abstracts without section headers.
Of the to-tal number of abstracts, more than one-third of theabstracts do not contain the section headers.
In or-der to separate these sections, we used a very simpleapproach based on the number of sentences present179in the method and conclusion sections, and the bodyof the abstracts.
We used the following informationto separate the method and conclusion sections fromthese abstracts: i) Order of sections in the abstracts,ii) Average number of sentences in the method andconclusion sections of the abstracts having sections,and iii) Average number of sentences in the entirebody of the abstracts not having sections.
All the ab-stracts having section headers contained the sectionsin the same order: background, objective, method,result and conclusion.
From the available trainingdataset, we calculated:i.
The average number of sentences in the method(4.14) and conclusion (2.11) sections of the abstracts di-vided into sectionsii.
The average number of sentences (8.78) of the ab-stracts not having sectionsBased on these values, we fragmented the ab-stracts that do not have the section headers and sepa-rated the method and conclusion sections from them.Table 3 shows how the method and conclusion sec-tions of those abstracts were generated.
For exam-ple, the fourth row of the table says that, if an ab-stract without section headers has 6, 7 or 8 sentences(let it be n), then the 3rd, 4th and 5th sentences wereconsidered as the method section, and the nth sen-tence was considered as the conclusion section.Total sentences inAbstracts(n)Method Conclusion1 None 12 or 3 1 n4 or 5 2 and 3 n6 or 7 or 8 2, 3 and 4 nMore than 8 3, 4 and 5 n-1 and nTable 3: Selecting method and conclusion of the abstractshaving a single block6 Experiments and ResultsThis section describes the two sets of experimentsperformed to compare the performance of the stack-ing based approach and the effectiveness of the base-level classifiers used.
The first set of experimentswas done to provide a baseline comparison againstour stacking based approach.
The second set con-sists of five experiments to evaluate different con-figurations of stack based classifiers.
The basic ap-proach of classification implies the use of a singleclassifier trained by using a single feature vector.We applied preprocessing steps to generate fea-ture sets from the title, body, method and conclusionsections of the abstracts.
The preprocessing stepswere: detecting sentences using OpenNLP SentenceDetector4, stemming words in each sentence usingPorter Stemmer (Porter, 1980), changing the sen-tences into lower-case, and removing punctuationcharacters from the sentences.
After the preprocess-ing step, we generated features from the unigrams,bigrams and trigrams in each part.
We removedthose features from the feature sets that containedthe stopwords listed by Pubmed5 or contained anytoken having a length less than three characters.
Toremove the less informative features, we calculatedthe information gain of the features in the trainingdata using Weka (Hall et al, 2009) and selected onlythe top 500 high informative features for each fea-ture set.
We used the Weka SVM classifier for all theexperiments.
Based on the best result obtained af-ter a series of experiments run with different kernelfunctions and regularization parameters, we chosethe SVM classifier with a linear kernel and regular-ization parameter equals 1 for all the experiments.We used a binary weight for all the features.6.1 First set of experimentsIn the first set, we performed nine experiments usingthe basic classification approach and one experimentusing the stacking based approach.
The details ofthe experiments and the combinations of the featuresused in them are as shown in Table 4.The first six experiments in the table were imple-mented by applying a basic approach of classifica-tion and each using only a single set of features.
Ex-periments 7, 8, and 9 were similar to the first sixexperiments except, they used more than one set offeatures to create the feature vector.
Each feature inthe experiments 7, 8, and 9 encode the section of itsorigin.
For example, if feature abdomen is presentin method as well as conclusion sections, it is rep-resented as two distinct features conc abdomen andmethod abdomen.
In experiment 10, we applied4http://incubator.apache.org/opennlp5http://www.ncbi.nlm.nih.gov/books/NBK3827/table/pubmedhelp.T43/?report=objectonly)180the stacking approach of classification using fivebase-level classifiers.
The base-level classifiers inthis experiment are the basic classifiers used in ex-periments 1 to 5.Exp.No.Features used Exp.
type1.
Publication typesBasic approach2.
MeSH terms3.
Abstract title4.
Abstract method5.
Abstract conclusion6.
Abstract body7.Publication types,MeSH terms8.Publication types,MeSH terms,Abstract title,Abstract body9.Publication types,MeSH terms,Abstract title,Abstract method,Abstract conclusion10.Publication typesStacking basedapproachMeSH termsAbstract titleAbstract methodAbstract conclusionTable 4: Experiments to compare basic approaches to astacking based approachFigure 2 shows the results of the 10 experimentsdescribed in Table 4 in the same order, from 1st to10th place and the result of the experiment by Molla-Aliod and Sarker (2011).
The results show thatthe stacking based approach gives the highest ac-curacy (73.77%), outperforming all the basic ap-proaches applying any combination of feature sets.The stacking based approach outperforms the base-line of a single layered classification approach (Exp9) that uses all the five sets of features.
Molla-Aliodand Sarker (2011) showed that a simple approach ofusing a single classifier and bag-of-words featurescould not achieve a good accuracy (45.9%) and pro-posed a new approach of using a sequence of classi-fiers to achieve a better result.
Similar to their simpleapproach, our basic approaches could not achievegood results, but their performance is comparableto Molla-Aliod and Sarker (2011)?s baseline system.The result of our stacking based approach shows thatour approach has a better accuracy than the sequen-cial classification approach (62.84%) proposed byFigure 2: Comparison of accuracy of basic approaches toa stacking based approach.
X-axis shows the experimentsand Y-axis shows the accuracy of the experiments.
Thefirst nine experiments are based on the basic approachand the tenth experiment is based on the stacking basedapproach.Molla-Aliod and Sarker (2011).Our stacking based approach works on two lev-els.
In the first level, the base-level classifiers pre-dict the grades of the evidence.
In the next level,these predictions are used to train a new classifierthat learns from the predictions to identify the gradescorrectly.
Moreover, the five feature sets used in ourexperiments were unrelated to each other.
For ex-ample, the features present in MeSH headings weredifferent from the features used in publication types,and similarly, the features present in the method sec-tion of the abstract were different from the featurespresent in the conclusion section.
Each base-levelclassifier trained by one of these feature sets is spe-cialized in that particular feature set.
Thus, usingthe predictions made by these specialized base-levelclassifiers to train a higher level classifier helps tobetter predict the grades, this cannot be achieved bya single classifier trained by a set of features (Exp.1, 2, 3, 4, 5, 6), or a group of different feature sets(Exp.
7, 8, 9).6.2 Second set of experimentsIn the second set of experiments, we compared fiveexperiments performed varying the base-level clas-sifiers used in our stack based approach.
Experi-ments 1 and 2 were performed using a single base-level classifier, that means that the second classifieris trained on only one feature.
Experiments 3 and 4were performed by using four base-level classifiers,and experiment 5 was performed using five base-181level classifiers.
The 5th experiment in this set issame as the 10th experiment in the first set.
The de-tails about the feature sets used in each experimentare shown in Table 5.Exp.No.Features used No.
of Base levelclassifiers1.Publication types,1MeSH terms,Abstract title,Abstract body2.Publication types,1MeSH terms,Abstract title,Abstract method,Abstract conclusion3.Publication types4MeSH termsAbstract titleAbstract body4.Publication types4MeSH termsAbstract titleAbstract method,Abstract conclusion5.Publication types5MeSH termsAbstract titleAbstract methodAbstract conclusionTable 5: Experiments to compare stacking based ap-proachFigure 3 shows the accuracy of the five experi-ments shown in Table 5 in the same order.
It showsthat the accuracy of 1st and 2nd experiments is lowerthan the accuracy of 3rd, 4th, and 5th experiments.In these two experiments, a feature vector generatedfrom the prediction of a single base-level classifieris used to train the higher level classifier, that is notsufficient to make a correct decision.Experiments 3, 4, and 5 show a considerable im-provement in the accuracy of the grading.
Compar-ing the results of experiments 3 and 4, we see thatthe 4th experiment has higher accuracy than the 3rdone.
The difference between these experiments wasthe use of features from the method and conclusionsections of the abstracts in the 4th experiment, whileusing features from the entire body of abstracts inthe 3rd experiment.
The higher accuracy in the 4thexperiment shows that the method and conclusionsections of the experiment contain high informativetext that is important for evidence grading, whileFigure 3: Comparison of accuracy of the stacking basedapproaches.
X-axis shows the experiments and Y-axisshows the accuracy of the experiments.
1st and 2nd ex-periments use only one base-level classifier, 3rd and 4thexperiment are based on four base-level classifiers and5th one uses five base-level classifiers.the body of abstracts may contain some informationthat is not relevant to the task.
The same analysiscan also be inferred from the results of experiment8 and 9 in the first set of experiments.
The high-est accuracy obtained in the 5th experiment of apply-ing 5 base-level classifiers shows that identifying thesections of the abstracts containing high informativefeatures and using a sufficient number of base-levelclassifiers can help to achieve a good accuracy in ev-idence grading.7 Error AnalysisThe result obtained by the stacking based approach(5th experiment in Table 5) using five base-level clas-sifiers gave a higher error rate in predicting gradesA and C, compared to the error rate in predict-ing grade B.
Most of the error is the misclassifica-tion of A to C and vice versa.
One of the possi-ble reasons of this might be due to the use of thefeature set extracted from the conclusion section.Among the five base-level classifiers used in the ex-periment, the one trained by the features extractedfrom the conclusion sections has the lowest accu-racy (5th experiment in Figure 2).
We evaluated thetext contained in the conclusion section of the ab-stracts in our dataset.
The section mostly containsthe assertion statements having the words showingstrong positive/negative meanings.
Conclusion of Agrade evidence mostly contains the information thatstrongly asserts the claim (e.g.
emollient treatment182significantly reduced the high-potency topical cor-ticosteroid consumption in infants with AD), whilethat of C grade evidence is not strong enough to as-sert the claim (e.g.
PDL therapy should be consid-ered among the better established approaches in thetreatment of warts, although data from this trial sug-gest that this approach is probably not superior).
Itseems that the problem might be because of not pro-cessing the negations appropriately.
So, in order topreserve some negation information present in theconclusion sections, we performed another experi-ment by merging words no, not, nor with their suc-cessor word to create a single token from the twowords.
This approach still could not reduce the mis-classification.
Thus, the simple approach of extract-ing unigram, bigram, and trigram features from theconclusion section might not be sufficient and mightneed to include higher level analysis related to as-sertion/certainty of the statements to reduce the mis-classification of the evidence.Other possible reasons of the misclassificationof the evidence might be the imbalanced data set.Our dataset (Table 2) contains higher number of in-stances with grade B than those with grades A and C.Moreover, the number of publications per evidenceis not uniform, that ranges from 1 to 8 publicationsper evidence in the test data.
Analyzing the results,we found that misclassification of evidence havingonly one publication is higher than that of the evi-dence having more than one publication.
If an ev-idence contains only one publication, the featuresof the evidence extracted from a single publicationmight not be sufficient to accurately grade the evi-dence and might lead to misclassification.In order to evaluate the appropriateness of ourapproach in extracting the method and conclusionsections, we performed a manual inspection of ab-stracts.
We could not revise all the abstracts to ver-ify the approach.
Thus, we randomly selected 25abstracts without section headers from the test dataand viewed the content in them.
We found that theconclusion section was appropriately extracted in al-most all abstracts, while the selection of method sec-tion was partially effective.
Our approach was basedon the assumption that all the abstracts having manysentences have all the sections (background, objec-tive, method, result, and conclusion).
But we foundthat the abstracts do not follow the same format, andthe start sentence of the method section is not con-sistent.
Even a long abstract might sometimes startwith the method section, and sometimes the objec-tive section might not be present in the abstracts.This could lead to increase the error in our gradingsystem.8 ConclusionThis paper presents an approach of grading the med-ical evidence applying a stacking based classifierusing the features from publication types, MeSHterms, abstract body, and method, and conclusionsections of the abstracts.
The results show thatthis approach achieves an accuracy of 73.77%, thatis significantly better than the previously reportedwork.
Here, we present two findings: 1) We showthat the stacking based approach helps to obtain abetter result in evidence grading than the basic ap-proach of classification.
2) We also show that themethod and conclusion sections of the abstracts con-tain important information necessary for evidencegrading.
Using the feature sets generated from thesetwo sections helps to achieve a higher accuracy thanby using the feature set generated from the entirebody of the abstracts.In this work, all the information available in themethod and conclusion sections of the abstracts istreated with equal weight.
Evidence grading shouldnot depend upon specific disease names and syn-dromes, but should be based on how strong the factsare presented.
We would like to extend our ap-proach by removing the words describing specificdisease names, disease syndromes, and medications,and giving higher weight to the terms that describethe assertion of the statements.
In our current work,we apply a simple approach to extract the methodand conclusion sections from the abstracts not hav-ing sections.
Improving the approach by using a ma-chine learning algorithm that can more accuratelyextract the sections might help to increase the accu-racy of grading.
Including the information about thestrength of assertions made in the conclusion sec-tions could also help in boosting the accuracy.
Fu-ture work would also include testing the effective-ness of our approach on other diverse data sets hav-ing complex structures of the evidence, or on a dif-ferent grading scale.183ReferencesSanmitra Bhattacharya, Viet HaThuc, and Padmini Srini-vasan.
2011.
Mesh: a window into full text for doc-ument summarization.
Bioinformatics, 27(13):i120?i128.Aaron M. Cohen, Kyle Ambert, and Marian McDon-agh.
2010.
A Prospective Evaluation of an Au-tomated Classification System to Support Evidence-based Medicine and Systematic Review.
AMIA AnnuSymp Proc., 2010:121 ?
125.Frank Davidoff, Brian Haynes, Dave Sackett, andRichard Smith.
1995.
Evidence based medicine.BMJ, 310(6987):1085?1086, 4.Martin Dawes, Pierre Pluye, Laura Shea, Roland Grad,Arlene Greenberg, and Jian-Yun Nie.
2007.
The iden-tification of clinically important elements within med-ical journal abstracts: Patient-Population-Problem,Exposure-Intervention, Comparison, Outcome, Dura-tion and Results (PECODR).
Informatics in PrimaryCare, 15(1):9?16.Dina Demner-Fushman, Barbara Few, Susan E. Hauser,and George Thoma.
2006.
Automatically IdentifyingHealth Outcome Information in MEDLINE Records.Journal of the American Medical Informatics Associa-tion, 13(1):52 ?
60.M.
H. Ebell, J. Siwek, B. D. Weiss, S. H. Woolf, J. Sus-man, B. Ewigman, and M. Bowman.
2004.
Strengthof recommendation taxonomy (SORT): a patient-centered approach to grading evidence in the medi-cal literature.
American Family Physician, 69(3):548?56+.David Evans.
2003.
Hierarchy of evidence: a frame-work for ranking evidence evaluating healthcare inter-ventions.
Journal of Clinical Nursing, 12(1):77?84.GRADE.
2004.
Grading quality of evidence and strengthof recommendations.
BMJ, 328(7454):1490, 6.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.SIGKDD Explor.
Newsl., 11(1).Minlie Huang, Aurlie Nvol, and Zhiyong Lu.
2011.
Rec-ommending MeSH terms for annotating biomedicalarticles.
Journal of the American Medical Informat-ics Association, 18(5):660?667.Halil Kilicoglu, Dina Demner-Fushman, Thomas C Rind-flesch, Nancy L Wilczynski, and R Brian Haynes.2009.
Towards Automatic Recognition of Scientif-ically Rigorous Clinical Research Evidence.
Jour-nal of the American Medical Informatics Association,16(1):25?31.Su Kim, David Martinez, Lawrence Cavedon, and LarsYencken.
2011.
Automatic classification of sentencesto support Evidence Based Medicine.
BMC Bioinfor-matics, 12(Suppl 2):S5.Diego Molla-Aliod and Maria Elena Santiago-Martinez.2011.
Development of a Corpus for Evidence BasedMedicine Summarisation.
In Proceedings of theAustralasian Language Technology Association Work-shop.Diego Molla-Aliod and Abeed Sarker.
2011.
AutomaticGrading of Evidence: the 2011 ALTA Shared Task.In Proceedings of Australasian Language TechnologyAssociation Workshop, pages 4?8.Diego Molla-Aliod.
2010.
A Corpus for EvidenceBased Medicine Summarisation.
In Proceedings of theAustralasian Language Technology Association Work-shop, volume 8.MF Porter.
1980.
An algorithm for sufx stripping.
Pro-gram, 14(3):130?137.David L Sackett, William M C Rosenberg, J A Muir Gray,R Brian Haynes, and W Scott Richardson.
1996.
Ev-idence based medicine: what it is and what it isn?t.BMJ, 312(7023):71?72, 1.Abeed Sarker, Diego Molla-Aliod, and Cecile Paris.2011.
Towards automatic grading of evidence.
In Pro-ceedings of LOUHI 2011 Third International Work-shop on Health Document Text Mining and Informa-tion Analysis, pages 51?58.Dolf Trieschnigg, Piotr Pezik, Vivian Lee, Franciskade Jong, Wessel Kraaij, and Dietrich Rebholz-Schuhmann.
2009.
MeSH Up: effective MeSH textclassification for improved document retrieval.
Bioin-formatics, 25(11):1412?1418.David H. Wolpert.
1992.
Stacked generalization.
NeuralNetworks, 5(2):241 ?
259.184
