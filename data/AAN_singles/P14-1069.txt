Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 733?742,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsJoint POS Tagging and Transition-based Constituent Parsing in Chinesewith Non-local FeaturesZhiguo WangBrandeis UniversityWaltham, MA, USAzgwang@brandeis.eduNianwen XueBrandeis UniversityWaltham, MA, USAxuen@brandeis.eduAbstractWe propose three improvements to ad-dress the drawbacks of state-of-the-arttransition-based constituent parsers.
First,to resolve the error propagation problemof the traditional pipeline approach, weincorporate POS tagging into the syntac-tic parsing process.
Second, to allevi-ate the negative influence of size differ-ences among competing action sequences,we align parser states during beam-searchdecoding.
Third, to enhance the pow-er of parsing models, we enlarge the fea-ture set with non-local features and semi-supervised word cluster features.
Exper-imental results show that these modifica-tions improve parsing performance signif-icantly.
Evaluated on the Chinese Tree-Bank (CTB), our final performance reach-es 86.3% (F1) when trained on CTB 5.1,and 87.1% when trained on CTB 6.0, andthese results outperform all state-of-the-artparsers.1 IntroductionConstituent parsing is one of the most fundamen-tal tasks in Natural Language Processing (NLP).
Itseeks to uncover the underlying recursive phrasestructure of sentences.
Most of the state-of-the-art parsers are based on the PCFG paradigm andchart-based decoding algorithms (Collins, 1999;Charniak, 2000; Petrov et al, 2006).
Chart-basedparsers perform exhaustive search with dynam-ic programming, which contributes to their highaccuracy, but they also suffer from higher run-time complexity and can only exploit simple localstructural information.Transition-based constituent parsing (Sagae andLavie, 2005; Wang et al, 2006; Zhang and Clark,2009) is an attractive alternative.
It utilizes a se-ries of deterministic shift-reduce decisions to con-struct syntactic trees.
Therefore, it runs in lineartime and can take advantage of arbitrarily complexstructural features from already constructed sub-trees.
The downside is that they only search a tinyfraction of the whole space and are therefore com-monly considered to be less accurate than chart-based parsers.
Recent studies (Zhu et al, 2013;Zhang et al, 2013) show, however, that this ap-proach can also achieve the state-of-the-art perfor-mance with improved training procedures and theuse of additional source of information as features.However, there is still room for improvemen-t for these state-of-the-art transition-based con-stituent parsers.
First, POS tagging is typicallyperformed separately as a preliminary step, andPOS tagging errors will propagate to the parsingprocess.
This problem is especially severe for lan-guages where the POS tagging accuracy is rela-tively low, and this is the case for Chinese wherethere are fewer contextual clues that can be usedto inform the tagging process and some of thetagging decisions are actually influenced by thesyntactic structure of the sentence.
This createsa chicken and egg problem that needs to be ad-dressed when designing a parsing model.
Second,due to the existence of unary rules in constituen-t trees, competing candidate parses often have d-ifferent number of actions, and this increases thedisambiguation difficulty for the parsing model.Third, transition-based parsers have the freedomto define arbitrarily complex structural features,but this freedom has not fully been taken advan-tage of and most of the present approaches onlyuse simple structural features.In this paper, we address these drawbacks toimprove the transition-based constituent parsingfor Chinese.
First, we integrate POS tagging in-to the parsing process and jointly optimize thesetwo processes simultaneously.
Because non-localsyntactic information is now available to POS tag733determination, the accuracy of POS tagging im-proves, and this will in turn improve parsing ac-curacy.
Second, we propose a novel state align-ment strategy to align candidate parses with dif-ferent action sizes during beam-search decoding.With this strategy, parser states and their unaryextensions are put into the same beam, thereforethe parsing model could decide whether or notto use unary actions within local decision beam-s. Third, we take into account two groups ofcomplex structural features that have not beenpreviously used in transition-based parsing: non-local features (Charniak and Johnson, 2005) andsemi-supervised word cluster features (Koo et al,2008).
With the help of the non-local features,our transition-based parsing system outperform-s all previous single systems in Chinese.
Afterintegrating semi-supervised word cluster features,the parsing accuracy is further improved to 86.3%when trained on CTB 5.1 and 87.1% when trainedon CTB 6.0, and this is the best reported perfor-mance for Chinese.The remainder of this paper is organized as fol-lows: Section 2 introduces the standard transition-based constituent parsing approach.
Section 3describes our three improvements to standardtransition-based constituent parsing.
We discussand analyze the experimental results in Section 4.Section 5 discusses related work.
Finally, we con-clude this paper in Section 6.2 Transition-based Constituent ParsingThis section describes the transition-based con-stituent parsing model, which is the basis of Sec-tion 3 and the baseline model in Section 4.2.1 Transition-based Constituent ParsingModelA transition-based constituent parsing model is aquadruple C = (S, T, s0, St), where S is a set ofparser states (sometimes called configurations), Tis a finite set of actions, s0is an initialization func-tion to map each input sentence into a unique ini-tial state, and St?
S is a set of terminal states.Each action t ?
T is a transition function to tran-sit a state into a new state.
A parser state s ?
S isdefined as a tuple s = (?, ?
), where ?
is a stackwhich is maintained to hold partial subtrees thatare already constructed, and ?
is a queue which isused for storing word-POS pairs that remain un-processed.
In particular, the initial state has anB0,3c2,3w2A0,2b1,2w1a0,1w0sh,sh,rr-A,sh,rl-B(a)B0,3F2,3c2,3w2E0,2A0,2D1,2b1,2w1C0,1a0,1w0sh,ru-C,sh,ru-D,rr-A,ru-E,sh,ru-F,rl-B(b)Figure 1: Two constituent trees for an examplesentence w0w1w2with POS tags abc.
The cor-responding action sequences are given below, thespans of each nodes are annotated and the head n-odes are written with Bold font type.empty stack ?
and a queue ?
containing the entireinput sentence (word-POS pairs), and the terminalstates have an empty queue ?
and a stack ?
con-taining only one complete parse tree.
The task oftransition-based constituent parsing is to scan theinput POS-tagged sentence from left to right andperform a sequence of actions to transform the ini-tial state into a terminal state.In order to construct lexicalized constituen-t parse trees, we define the following actions forthe action set T according to (Sagae and Lavie,2005; Wang et al, 2006; Zhang and Clark, 2009):?
SHIFT (sh): remove the first word-POS pairfrom ?, and push it onto the top of ?;?
REDUCE-UNARY-X (ru-x): pop the topsubtree from ?, construct a new unary nodelabeled with X for the subtree, then push thenew subtree back onto ?.
The head of thenew subtree is inherited from its child;?
REDUCE-BINARY-{L/R}-X (rl/rr-x): popthe top two subtrees from ?, combine theminto a new tree with a node labeled with X,then push the new subtree back onto ?.
Theleft (L) and right (R) versions of the actionindicate whether the head of the new subtreeis inherited from its left or right child.With these actions, our parser can process treeswith unary and binary branches easily.
For exam-ple, in Figure 1, for the input sentence w0w1w2and its POS tags abc, our parser can construct t-wo parse trees using action sequences given belowthese trees.
However, parse trees in Treebanks of-ten contain an arbitrary number of branches.
To734Type Feature Templatesunigramsp0tc, p0wc, p1tc, p1wc, p2tcp2wc, p3tc, p3wc, q0wt, q1wtq2wt, q3wt, p0lwc, p0rwcp0uwc, p1lwc, p1rwc, p1uwcbigramsp0wp1w, p0wp1c, p0cp1w, p0cp1cp0wq0w, p0wq0t, p0cq0w, p0cq0tq0wq1w, q0wq1t, q0tq1w, q0tq1tp1wq0w, p1wq0t, p1cq0w, p1cq0ttrigramsp0cp1cp2c, p0wp1cp2c, p0cp1wq0tp0cp1cp2w, p0cp1cq0t, p0wp1cq0tp0cp1wq0t, p0cp1cq0wTable 1: Baseline features, where pirepresents theithsubtree in the stack ?
and qidenotes the ithitem in the queue ?.
w refers to the head lexicon,t refers to the head POS, and c refers to the con-stituent label.
piland pirrefer to the left and rightchild for a binary subtree pi, and piurefers to thechild of a unary subtree pi.process such trees, we employ binarization anddebinarization processes described in Zhang andClark (2009) to transform multi-branch trees intobinary-branch trees and restore the generated bi-nary trees back to their original forms.2.2 Modeling, Training and DecodingTo determine which action t ?
T should the parserperform at a state s ?
S, we use a linear model toscore each possible ?s, t?
combination:score(s, t) = ~w ?
?
(s, t) =?iwifi(s, t) (1)where ?
(s, t) is the feature function used for map-ping a state-action pair into a feature vector, and~w is the weight vector.
The score of a parser states is the sum of the scores for all state-action pairsin the transition path from the initial state to thecurrent state.
Table 1 lists the feature templatesused in our baseline parser, which is adopted fromZhang and Clark (2009).
To train the weight vec-tor ~w, we employ the averaged perceptron algo-rithm with early update (Collins and Roark, 2004).We employ the beam search decoding algorith-m (Zhang and Clark, 2009) to balance the trade-off between accuracy and efficiency.
Algorithm1 gives details of the process.
In the algorithm,we maintain a beam (sometimes called agenda)to keep k best states at each step.
The first beam0Algorithm 1 Beam-search Constituent ParsingInput: A POS-tagged sentence, beam size k.Output: A constituent parse tree.1: beam0?
{s0} .
initialization2: i?
0 .
step index3: loop4: P ?
{} .
a priority queue5: while beamiis not empty do6: s?
POP(beami)7: for all possible t ?
T do8: snew?
apply t to s9: score snewwith E.q (1)10: insert snewinto P11: beami+1?
k best states of P12: sbest?
best state in beami+113: if sbest?
Stthen14: return sbest15: i?
i+ 1is initialized with the initial state s0(line 1).
Atstep i, each of the k states in beamiis extendedby applying all possible actions (line 5-10).
Forall newly generated states, only the k best statesare preserved for beami+1(line 11).
The decod-ing process repeats until the highest scored state inbeami+1reaches a terminal state (line 12-14).3 Joint POS Tagging and Parsing withNon-local FeaturesTo address the drawbacks of the standardtransition-based constituent parsing model (de-scribed in Section 1), we propose a model to joint-ly solve POS tagging and constituent parsing withnon-local features.3.1 Joint POS Tagging and ParsingPOS tagging is often taken as a preliminary stepfor transition-based constituent parsing, thereforethe accuracy of POS tagging would greatly affec-t parsing performance.
In our experiment (de-scribed in Section 4.2), parsing accuracy woulddecrease by 8.5% in F1in Chinese parsing whenusing automatically generated POS tags instead ofgold-standard ones.
To tackle this issue, we inte-grate POS tagging into the transition-based con-stituent parsing process and jointly optimize thesetwo processes simultaneously.
Inspired from Ha-tori et al (2011), we modify the sh action by as-signing a POS tag for the word when it is shifted:?
SHIFT-X (sh-x): remove the first word from735?, assign POS tag X to the word and push itonto the top of ?.With such an action, POS tagging becomes a nat-ural part of transition-based parsing.
However,some feature templates in Table 1 become unavail-able, because POS tags for the look-ahead wordsare not specified yet under the joint framework.For example, for the template q0wt , the POS tagof the first word q0in the queue ?
is required, butit is not specified yet at the present state.To overcome the lack of look-ahead POS tags,we borrow the concept of delayed features origi-nally developed for dependency parsing (Hatori etal., 2011).
Features that require look-ahead POStags are defined as delayed features.
In these fea-tures, look-ahead POS tags are taken as variables.During parsing, delayed features are extracted andpassed from one state to the next state.
When ash-x action is performed, the look-ahead POStag of some delayed features is specified, there-fore these delayed features can be transformed in-to normal features (by replacing variable with thenewly specified POS tag).
The remaining delayedfeatures will be transformed similarly when theirlook-ahead POS tags are specified during the fol-lowing parsing steps.3.2 State AlignmentAssuming an input sentence contains n words, inorder to reach a terminal state, the initial state re-quires n sh-x actions to consume all words in ?,and n ?
1 rl/rr-x actions to construct a com-plete parse tree by consuming all the subtrees in?.
However, ru-x is a very special action.
It on-ly constructs a new unary node for the subtree ontop of ?, but does not consume any items in ?
or?.
As a result, the number of ru-x actions variesamong terminal states for the same sentence.
Forexample, the parse tree in Figure 1a contains noru-x action, while the parse tree for the same in-put sentence in Figure 1b contains four ru-x ac-tions.
This makes the lengths of complete actionsequences very different, and the parsing modelhas to disambiguate among terminal states withvarying action sizes.
Zhu et al (2013) proposed apadding method to align terminal states containingdifferent number of actions.
The idea is to appendsome IDLE actions to terminal states with shorteraction sequence, and make sure all terminal statescontain the same number of actions (including I-DLE actions).Algorithm 2 Beam-search with State AlignmentInput: A word-segmented sentence, beam size k.Output: A constituent parse tree.1: beam0?
{s0} .
initialization2: for i?
0 to 2n?
1 do .
n is sentence length3: P0?
{}, P1?
{} .
two priority queues4: while beamiis not empty do5: s?
POP(beami)6: for t ?
{sh-x,rl-x,rr-x} do7: snew?
apply t to s8: score snewwith E.q (1)9: insert snewinto P010: for all state s in P0do11: for all possible t ?
{ru-x} do12: snew?
apply t to s13: score snewwith E.q (1)14: insert snewinto P115: insert all states of P1into P016: beami+1?
k best states of P017: return the best state in beam2n?1We propose a novel method to align states dur-ing the parsing process instead of just aligning ter-minal states like Zhu et al (2013).
We classify allthe actions into two groups according to whetherthey consume items in ?
or ?.
sh-x, rl-x, andrr-x belong to consuming actions, and ru-x be-longs to non-consuming action.
Algorithm 2 givesthe details of our method.
It is based on the beamsearch decoding algorithm described in Algorith-m 1.
Different from Algorithm 1, Algorithm 2 isguaranteed to perform 2n?
1 parsing steps for aninput sentence containing n words (line 2), anddivides each parsing step into two parsing phas-es.
In the first phase (line 4-9), each of the k s-tates in beamiis extended by consuming action-s.
In the second phase (line 10-14), each of thenewly generated states is further extended by non-consuming actions.
Then, all these states extend-ed by both consuming and non-consuming action-s are considered together (line 15), and only thek highest-scored states are preserved for beami+1(line 16).
After these 2n ?
1 parsing steps, thehighest scored state in beam2n?1is returned asthe final result (line 17).
Figure 2 shows the statesaligning process for the two trees in Figure 1.
Wefind that our new method aligns states with theirru-x extensions in the same beam, therefore theparsing model could make decisions on whetherusing ru-x actions or not within local decision736s0a0,1b1,2A0,2c2,3B0,3T0C0,1b1,2D1,2A0,2E0,2c2,3F2,3B0,3T1beam0beam1beam2beam3beam4beam5Figure 2: State alignment for the two trees in Fig-ure 1, where s0is the initial state, T0and T1areterminal states corresponding to the two trees inFigure 1.
For clarity, we represent each state as arectangle with the label of top subtree in the stack?.
We also denote sh-x with?, ru-x with ?
or?, rl-x with?, and rr-x with?.beams.3.3 Feature ExtensionOne advantage of transition-based constituen-t parsing is that it is capable of incorporating ar-bitrarily complex structural features from the al-ready constructed subtrees in ?
and unprocessedwords in ?.
However, all the feature templatesgiven in Table 1 are just some simple structuralfeatures.
To further improve the performance ofour transition-based constituent parser, we con-sider two group of complex structural features:non-local features (Charniak and Johnson, 2005;Collins and Koo, 2005) and semi-supervised wordcluster features (Koo et al, 2008).Table 2 lists all the non-local features we wantto use.
These features have been proved very help-ful for constituent parsing (Charniak and Johnson,2005; Collins and Koo, 2005).
But almost all pre-vious work considered non-local features only inparse reranking frameworks.
Instead, we attemptto extract non-local features from newly construct-ed subtrees during the decoding process as theybecome incrementally available and score newlygenerated parser states with them.
One difficul-ty is that the subtrees built by our baseline pars-er are binary trees (only the complete parse treeis debinarized into its original multi-branch form),but most of the non-local features need to be ex-tracted from their original multi-branch forms.
Toresolve this conflict, we integrate the debinariza-tion process into the parsing process, i.e., when a(Collins and Koo, 2005) (Charniak and Johnson, 2005)Rules CoPar HeadTreeBigrams CoLenParGrandparent Rules RightBranchGrandparent Bigrams HeavyLexical Bigrams NeighboursTwo-level Rules NGramTreeTwo-level Bigrams HeadsTrigrams WprojHead-Modifiers WordTable 2: Non-local features for constituent pars-ing.new subtree is constructed during parsing, we de-binarize it immediately if it is not rooted with anintermediate node1.
The other subtrees for sub-sequent parsing steps will be built based on thesedebinarized subtrees.
After the modification, ourparser can extract non-local features incrementallyduring the parsing process.Semi-supervised word cluster features havebeen successfully applied to many NLP tasks(Miller et al, 2004; Koo et al, 2008; Zhu etal., 2013).
Here, we adopt such features for ourtransition-based constituent parser.
Given a large-scale unlabeled corpus (word segmentation shouldbe performed), we employ the Brown cluster al-gorithm (Liang, 2005) to cluster all words into abinary tree.
Within this binary tree, words ap-pear as leaves, left branches are labeled with 0 andright branches are labeled with 1.
Each word canbe uniquely identified by its path from the root,and represented as a bit-string.
By using variouslength of prefixes of the bit-string, we can produceword clusters of different granularities (Miller etal., 2004).
Inspired from Koo et al (2008), weemploy two types of word clusters: (1) taking 4bit-string prefixes of word clusters as replacementsof POS tags, and (2) taking 8 bit-string prefixes asreplacements of words.
Using these two types ofclusters, we construct semi-supervised word clus-ter features by mimicking the template structure ofthe original baseline features in Table 1.4 Experiment4.1 Experimental SettingWe conducted experiments on the Penn ChineseTreebank (CTB) version 5.1 (Xue et al, 2005):Articles 001-270 and 400-1151 were used as thetraining set, Articles 301-325 were used as thedevelopment set, and Articles 271-300 were used1Intermediate nodes are produced by binarization process.737as the test set.
Standard corpus preparation step-s were performed before our experiments: emp-ty nodes and functional tags were removed, andthe unary chains were collapsed to single unaryrules as Harper and Huang (2011).
To build wordclusters, we used the unlabeled Chinese Gigaword(LDC2003T09) and conducted Chinese word seg-mentation using a CRF-based segmenter.We used EVALB2tool to evaluate parsing per-formance.
The metrics include labeled precision(LP ), labeled recall (LR), bracketing F1and POStagging accuracy.
We set the beam size k to 16,which brings a good balance between efficiencyand accuracy.
We tuned the optimal number ofiterations of perceptron training algorithm on thedevelopment set.4.2 Pipeline Approach vs Joint POS Taggingand ParsingIn this subsection, we conducted some experi-ments to illustrate the drawbacks of the pipelineapproach and the advantages of our joint approach.We built three parsing systems: Pipeline-Goldsystem is our baseline parser (described in Sec-tion 2) taking gold-standard POS tags as input;Pipeline system is our baseline parser taking asinput POS tags automatically assigned by Stan-ford POS Tagger3; and JointParsing system isour joint POS tagging and transition-based pars-ing system described in subsection 3.1.
We trainedthese three systems on the training set and evalu-ated them on the development set.
The second,third and forth rows in Table 3 show the parsingperformances.
We can see that the parsing F1de-creased by about 8.5 percentage points in F1scorewhen using automatically assigned POS tags in-stead of gold-standard ones, and this shows thatthe pipeline approach is greatly affected by thequality of its preliminary POS tagging step.
Af-ter integrating the POS tagging step into the pars-ing process, our JointParsing system improved thePOS tagging accuracy to 94.8% and parsing F1to 85.8%, which are significantly better than thePipeline system.
Therefore, the joint parsing ap-proach is much more effective for transition-basedconstituent parsing.4.3 State Alignment EvaluationWe built two new systems to verify the effective-ness of our state alignment strategy proposed in2http://nlp.cs.nyu.edu/evalb/3http://nlp.stanford.edu/downloads/tagger.shtmlSystem LP LR F1POSPipeline-Gold 92.2 92.5 92.4 100Pipeline 83.9 83.8 83.8 93.0JointParsing 85.1 86.6 85.8 94.8Padding 85.4 86.4 85.9 94.8StateAlign 86.9 85.9 86.4 95.2Nonlocal 88.0 86.5 87.2 95.3Cluster 89.0 88.3 88.7 96.3Nonlocal&Cluster 89.4 88.7 89.1 96.2Table 3: Parsing performance on Chinese devel-opment set.Subsection 3.2.
The first system Padding extend-s our JointParsing system by aligning terminal s-tates with the padding strategy proposed in Zhu etal.
(2013), and the second system StateAlign ex-tends the JointParsing system with our state align-ment strategy.
The fifth and sixth rows of Table 3give the performances of these two systems.
Com-pared with the JointParsing system which does notemploy any alignment strategy, the Padding sys-tem only achieved a slight improvement on pars-ing F1score, but no improvement on POS tag-ging accuracy.
In contrast, our StateAlign systemachieved an improvement of 0.6% on parsing F1s-core and 0.4% on POS tagging accuracy.
All theseresults show us that our state alignment strategy ismore helpful for beam-search decoding.4.4 Feature Extension EvaluationIn this subsection, we examined the usefulnessof the new non-local features and the semi-supervised word cluster features described in Sub-section 3.3.
We built three new parsing system-s based on the StateAlign system: Nonlocal sys-tem extends the feature set of StateAlign systemwith non-local features, Cluster system extendsthe feature set with semi-supervised word clusterfeatures, and Nonlocal&Cluster system extend thefeature set with both groups of features.
Parsingperformances of the three systems are shown inthe last three rows of Table 3.
Compared with theStateAlign system which takes only the baselinefeatures, the non-local features improved parsingF1by 0.8%, while the semi-supervised word clus-ter features result in an improvement of 2.3% inparsing F1and an 1.1% improvement on POS tag-ging accuracy.
When integrating both groups offeatures, the final parsing F1reaches 89.1%.
Al-738Type System LP LR F1POSOur SystemsPipeline 80.0 80.3 80.1 94.0JointParsing 82.4 83.0 82.7 95.1Padding 82.7 83.6 83.2 95.1StateAlign 84.2 82.9 83.6 95.5Nonlocal 85.6 84.2 84.9 95.9Cluster 85.2 84.5 84.9 95.8Nonlocal&Cluster 86.6 85.9 86.3 96.0Single SystemsPetrov and Klein (2007) 81.9 84.8 83.3 -Zhu et al (2013) 82.1 84.3 83.2 -Reranking SystemsCharniak and Johnson (2005)?80.8 83.8 82.3 -Wang and Zong (2011) - - 85.7 -Semi-supervised Systems Zhu et al (2013) 84.4 86.8 85.6 -Table 4: Parsing performance on Chinese test set.
?Huang (2009) adapted the parse reranker to CTB5.l these results show that both the non-local fea-tures and the semi-supervised features are helpfulfor our transition-based constituent parser.4.5 Final Results on Test SetIn this subsection, we present the performances ofour systems on the CTB test set.
The correspond-ing results are listed in the top rows of Table 4.We can see that all these systems maintain a simi-lar relative relationship as they do on the develop-ment set, which shows the stability of our systems.To further illustrate the effectiveness of oursystems, we compare them with some state-of-the-art systems.
We group parsing systems intothree categories: single systems, reranking sys-tems and semi-supervised systems.
Our Pipeline,JointParsing, Padding, StateAlign and Nonlocalsystems belong to the category of single system-s, because they don?t utilize any extra process-ing steps or resources.
Our Cluster and Nonlo-cal&Cluster systems belong to semi-supervisedsystems, because both of them have employedsemi-supervised word cluster features.
The pars-ing performances of state-of-the-art systems areshown in the bottom rows of Table 4.
We can seethat the final F1of our Nonlocal system reached84.9%, and it outperforms state-of-the-art singlesystems by more than 1.6%.
As far as we know,this is the best result on the CTB test set acquiredby single systems.
Our Nonlocal&Cluster sys-tem further improved the parsing F1to 86.3%,and it outperforms all reranking systems and semi-supervised systems.
To our knowledge, this is theSystem F1Huang and Harper (2009) 85.2Nonlocal&Cluster 87.1Table 5: Parsing performance based on CTB 6.best reported performance in Chinese parsing.All previous experiments were conducted onCTB 5.
To check whether more labeled data canfurther improve our parsing system, we evaluat-ed our Nonlocal&Cluster system on the ChineseTreeBank version 6.0 (CTB6), which is a superset of CTB5 and contains more annotated data.We used the same development set and test setas CTB5, and took all the remaining data as thenew training set.
Table 5 shows the parsing per-formances on CTB6.
Our Nonlocal&Cluster sys-tem improved the final F1to 87.1%, which is 1.9%better than the state-of-the-art performance on CT-B6 (Huang and Harper, 2009).
Compared with it-s performance on CTB5 (in Table 4), our Nonlo-cal&Cluster system also got 0.8% improvemen-t. All these results show that our approach canbecome more powerful when given more labeledtraining data.4.6 Error AnalysisTo better understand the linguistic behavior ofour systems, we employed the berkeley-parser-analyser tool4(Kummerfeld et al, 2013) to cat-egorize the errors.
Table 6 presents the average4http://code.google.com/p/berkeley-parser-analyser/739SystemNPInt.Unary1-WordSpanCoordMod.AttachVerbArgsDiffLabelClauseAttachNounEdgeWorst 1.75 0.74 0.44 0.49 0.39 0.37 0.29 0.15 0.14PipelineJointParsingPaddingStateAlignNonlocalClusterNonlocal&ClusterBest 1.33 0.42 0.28 0.29 0.19 0.21 0.17 0.07 0.09Table 6: Parse errors on Chinese test set.
The shaded area of each bar indicates average number of thaterror type per sentence, and the completely full bar indicates the number in the Worst row.System VV?NN NN?VV DEC?DEG JJ?NN NR?NN DEG?DEC NN?NR NN?JJWorst 0.26 0.18 0.15 0.09 0.08 0.07 0.06 0.05PipelineJointParsingPaddingStateAlignNonlocalClusterNonlocal&ClusterBest 0.14 0.10 0.03 0.07 0.05 0.03 0.03 0.02Table 7: POS tagging error patterns on Chinese test set.
For each error pattern, the left hand side tag isthe gold-standard tag, and the right hand side is the wrongly assigned tag.number of errors for each error type by our pars-ing systems.
We can see that almost all the Worstnumbers are produced by the Pipeline system.
TheJointParsing system reduced errors of all typesproduced by the Pipeline system except for thecoordination error type (Coord).
The StateAlignsystem corrected a lot of the NP-internal errors(NP Int.).
The Nonlocal system and the Clustersystem produced similar numbers of errors for al-l error types.
The Nonlocal&Cluster system pro-duced the Best numbers for all the error types.
NP-internal errors are still the most frequent error typein our parsing systems.Table 7 presents the statistics of frequent POStagging error patterns.
We can see that JointPars-ing system disambiguates {VV, NN} and {DEC,DEG} better than Pipeline system, but cannot dealwith the NN?JJ pattern very well.
StateAlignsystem got better results in most of the patterns,but cannot disambiguate {NR, NN} well.
Non-local&Cluster system got the best results in dis-ambiguating the most ambiguous POS tag pairs of{VV, NN}, {DEC, DEG}, {JJ, NN} and {NN, N-R}.5 Related WorkJoint POS tagging with parsing is not a new idea.In PCFG-based parsing (Collins, 1999; Charniak,2000; Petrov et al, 2006), POS tagging is consid-ered as a natural step of parsing by employing lex-ical rules.
For transition-based parsing, Hatori etal.
(2011) proposed to integrate POS tagging withdependency parsing.
Our joint approach can beseen as an adaption of Hatori et al (2011)?s ap-proach for constituent parsing.
Zhang et al (2013)proposed a transition-based constituent parser toprocess an input sentence from the character level.However, manual annotation of the word-internalstructures need to be added to the original Tree-bank in order to train such a parser.Non-local features have been successfully usedfor constituent parsing (Charniak and Johnson,2005; Collins and Koo, 2005; Huang, 2008).However, almost all of the previous work use non-local features at the parse reranking stage.
Thereason is that the single-stage chart-based parsercannot use non-local structural features.
In con-trast, the transition-based parser can use arbitrari-ly complex structural features.
Therefore, we canconcisely utilize non-local features in a single-740stage parsing system.6 ConclusionIn this paper, we proposed three improvements totransition-based constituent parsing for Chinese.First, we incorporated POS tagging into transition-based constituent parsing to resolve the error prop-agation problem of the pipeline approach.
Second,we proposed a state alignment strategy to aligncompeting decision sequences that have differentnumber of actions.
Finally, we enhanced our pars-ing model by enlarging the feature set with non-local features and semi-supervised word clusterfeatures.
Experimental results show that all thesemethods improved the parsing performance sub-stantially, and the final performance of our parsingsystem outperformed all state-of-the-art systems.AcknowledgmentsWe thank three anonymous reviewers for theircogent comments.
This work is funded by theDAPRA via contract HR0011-11-C-0145 entitled/Linguistic Resources for Multilingual Process-ing0.
All opinions expressed here are those of theauthors and do not necessarily reflect the views ofDARPA.ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative r-eranking.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 173?180.
Association for Computational Lin-guistics.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st NorthAmerican chapter of the Association for Computa-tional Linguistics conference, pages 132?139.
Asso-ciation for Computational Linguistics.Michael Collins and Terry Koo.
2005.
Discriminativereranking for natural language parsing.
Computa-tional Linguistics, 31(1):25?70.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Pro-ceedings of the 42nd Meeting of the Association forComputational Linguistics (ACL?04), Main Volume,pages 111?118, Barcelona, Spain, July.Michael Collins.
1999.
HEAD-DRIVEN STATISTI-CAL MODELS FOR NATURAL LANGUAGE PARS-ING.
Ph.D. thesis, University of Pennsylvania.Mary Harper and Zhongqiang Huang.
2011.
Chinesestatistical parsing.
Handbook of Natural LanguageProcessing and Machine Translation.Jun Hatori, Takuya Matsuzaki, Yusuke Miyao, andJun?ichi Tsujii.
2011.
Incremental joint pos taggingand dependency parsing in chinese.
In Proceed-ings of 5th International Joint Conference on Nat-ural Language Processing, pages 1216?1224, Chi-ang Mai, Thailand, November.
Asian Federation ofNatural Language Processing.Zhongqiang Huang and Mary Harper.
2009.
Self-training pcfg grammars with latent annotationsacross languages.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing: Volume 2-Volume 2, pages 832?841.Association for Computational Linguistics.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In ACL, pages 586?594.Ling-Ya Huang.
2009.
Improve chinese parsing withmax-ent reranking parser.
Master Project Report,Brown University.Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In Proceedings of ACL-08: HLT, pages 595?603,Columbus, Ohio, June.
Association for Computa-tional Linguistics.Jonathan K. Kummerfeld, Daniel Tse, James R. Cur-ran, and Dan Klein.
2013.
An empirical examina-tion of challenges in chinese parsing.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics (Volume 2: Short Pa-pers), pages 98?103, Sofia, Bulgaria, August.
Asso-ciation for Computational Linguistics.Percy Liang.
2005.
Semi-supervised learning for nat-ural language.
Ph.D. thesis, Massachusetts Instituteof Technology.Scott Miller, Jethran Guinness, and Alex Zamanian.2004.
Name tagging with word clusters and discrim-inative training.
In HLT-NAACL, volume 4, pages337?342.
Citeseer.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In HLT-NAACL, pages404?411.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on Computation-al Linguistics and the 44th annual meeting of the As-sociation for Computational Linguistics, pages 433?440.
Association for Computational Linguistics.Kenji Sagae and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proceed-ings of the Ninth International Workshop on ParsingTechnology, pages 125?132.
Association for Com-putational Linguistics.741Zhiguo Wang and Chengqing Zong.
2011.
Parse r-eranking based on higher-order lexical dependen-cies.
In IJCNLP, pages 1251?1259.Mengqiu Wang, Kenji Sagae, and Teruko Mitamura.2006.
A fast, accurate deterministic parser for chi-nese.
In Proceedings of the 21st International Con-ference on Computational Linguistics and the 44thannual meeting of the Association for Computation-al Linguistics, pages 425?432.
Association for Com-putational Linguistics.Naiwen Xue, Fei Xia, Fu-Dong Chiou, and MarthaPalmer.
2005.
The penn chinese treebank: Phrasestructure annotation of a large corpus.
Natural lan-guage engineering, 11(2):207?238.Yue Zhang and Stephen Clark.
2009.
Transition-basedparsing of the chinese treebank using a global dis-criminative model.
In Proceedings of the 11th Inter-national Conference on Parsing Technologies, pages162?171.
Association for Computational Linguistic-s.Meishan Zhang, Yue Zhang, Wanxiang Che, and TingLiu.
2013.
Chinese parsing exploiting characters.In Proceedings of the 51st Annual Meeting of theAssociation for Computational Linguistics (Volume1: Long Papers), pages 125?134, Sofia, Bulgaria,August.
Association for Computational Linguistics.Muhua Zhu, Yue Zhang, Wenliang Chen, Min Zhang,and Jingbo Zhu.
2013.
Fast and accurate shift-reduce constituent parsing.
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages434?443, Sofia, Bulgaria, August.
Association forComputational Linguistics.742
