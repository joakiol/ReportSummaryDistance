Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1027?1037, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsEnsemble Semantics for Large-scale Unsupervised Relation ExtractionBonan Min1* Shuming Shi2 Ralph Grishman1 Chin-Yew Lin21New York University 2Microsoft Research AsiaNew York, NY, USA Beijing, China{min,grishman}@cs.nyu.edu {shumings,cyl}@microsoft.comAbstractDiscovering significant types of relationsfrom the web is challenging because of itsopen nature.
Unsupervised algorithms aredeveloped to extract relations from a cor-pus without knowing the relations in ad-vance, but most of them rely on taggingarguments of predefined types.
Recently,a new algorithm was proposed to jointlyextract relations and their argument se-mantic classes, taking a set of relation in-stances extracted by an open IE algorithmas input.
However, it cannot handle poly-semy of relation phrases and fails togroup many similar (?synonymous?)
rela-tion instances because of the sparseness offeatures.
In this paper, we present a novelunsupervised algorithm that provides amore general treatment of the polysemyand synonymy problems.
The algorithmincorporates various knowledge sourceswhich we will show to be very effectivefor unsupervised extraction.
Moreover, itexplicitly disambiguates polysemous rela-tion phrases and groups synonymousones.
While maintaining approximatelythe same precision, the algorithm achievessignificant improvement on recall com-pared to the previous method.
It is alsovery efficient.
Experiments on a real-world dataset show that it can handle 14.7million relation instances and extract avery large set of relations from the web.1 IntroductionRelation extraction aims at discovering semanticrelations between entities.
It is an important taskthat has many applications in answering factoidquestions, building knowledge bases and improv-ing search engine relevance.
The web has becomea massive potential source of such relations.
How-ever, its open nature brings an open-ended set ofrelation types.
To extract these relations, a systemshould not assume a fixed set of relation types, norrely on a fixed set of relation argument types.The past decade has seen some promising solu-tions, unsupervised relation extraction (URE) algo-rithms that extract relations from a corpus withoutknowing the relations in advance.
However, mostalgorithms (Hasegawa et al 2004, Shinyama andSekine, 2006, Chen et.
al, 2005) rely on taggingpredefined types of entities as relation arguments,and thus are not well-suited for the open domain.Recently, Kok and Domingos (2008) proposedSemantic Network Extractor (SNE), which gener-ates argument semantic classes and sets of synon-ymous relation phrases at the same time, thusavoiding the requirement of tagging relation argu-ments of predefined types.
However, SNE has 2limitations: 1) Following previous URE algo-rithms, it only uses features from the set of inputrelation instances for clustering.
Empirically wefound that it fails to group many relevant relationinstances.
These features, such as the surface formsof arguments and lexical sequences in between, arevery sparse in practice.
In contrast, there exist sev-eral well-known corpus-level semantic resourcesthat can be automatically derived from a sourcecorpus and are shown to be useful for generatingthe key elements of a relation: its 2 argument se-mantic classes and a set of synonymous phrases.For example, semantic classes can be derived froma source corpus with contextual distributional simi-larity and web table co-occurrences.
The ?synony-my?
1  problem for clustering relation instances* Work done during an internship at Microsoft Research Asia1027could potentially be better solved by adding theseresources.
2) SNE assumes that each entity or rela-tion phrase belongs to exactly one cluster, thus isnot able to effectively handle polysemy of relationphrases2.
An example of a polysemous phrase is bethe currency of  as in 2 triples <Euro, be the cur-rency of, Germany> and <authorship, be the cur-rency of, science>.
As the target corpus expandsfrom mostly news to the open web, polysemy be-comes more important as input covers a widerrange of domains.
In practice, around 22% (section3) of relation phrases are polysemous.
Failure tohandle these cases significantly limits its effective-ness.To move towards a more general treatment ofthe polysemy and synonymy problems, we present anovel algorithm WEBRE for open-domain large-scale unsupervised relation extraction without pre-defined relation or argument types.
The contribu-tions are:?
WEBRE incorporates a wide range of cor-pus-level semantic resources for improving rela-tion extraction.
The effectiveness of eachknowledge source and their combination are stud-ied and compared.
To the best of our knowledge, itis the first to combine and compare them for unsu-pervised relation extraction.?
WEBRE explicitly disambiguates polyse-mous relation phrases and groups synonymousphrases, thus fundamentally it avoids the limitationof previous methods.?
Experiments on the Clueweb09 dataset(lemurproject.org/clueweb09.php) show thatWEBRE is effective and efficient.
We present alarge-scale evaluation and show that WEBRE canextract a very large set of high-quality relations.Compared to the closest prior work, WEBRE sig-nificantly improves recall while maintaining thesame level of precision.
WEBRE is efficient.
Tothe best of our knowledge, it handles the largesttriple set to date (7-fold larger than largest previouseffort).
Taking 14.7 million triples as input, a com-plete run with one CPU core takes about a day.1 We use the term synonymy broadly as defined in Section 3.2 A cluster of relation phrases can, however, act as a whole asthe phrase cluster for 2 different relations in SNE.
However,this only accounts for 4.8% of the polysemous cases.2 Related WorkUnsupervised relation extraction (URE) algorithms(Hasegawa et al 2004; Chen et al 2005; Shinya-ma and Sekine, 2006) collect pairs of co-occurringentities as relation instances, extract features forinstances and then apply unsupervised clusteringtechniques to find the major relations of a corpus.These UREs rely on tagging a predefined set ofargument types, such as Person, Organization, andLocation, in advance.
Yao et al2011 learns fine-grained argument classes with generative models,but they share the similar requirement of taggingcoarse-grained argument types.
Most UREs use aquadratic clustering algorithm such as HierarchicalAgglomerate Clustering (Hasegawa et al 2004,Shinyama and Sekine, 2006), K-Means (Chen etal., 2005), or both (Rosenfeld and Feldman, 2007);thus they are not scalable to very large corpora.As the target domain shifts to the web, newmethods are proposed without requiring predefinedentity types.
Resolver (Yates and Etzioni, 2007)resolves objects and relation synonyms.
Kok andDomingos (2008) proposed Semantic Network Ex-tractor (SNE) to extract concepts and relations.Based on second-order Markov logic, SNE used abottom-up agglomerative clustering algorithm tojointly cluster relation phrases and argument enti-ties.
However, both Resolver and SNE requireeach entity and relation phrase to belong to exactlyone cluster.
This limits their ability to handle poly-semous relation phrases.
Moreover, SNE only usesfeatures in the input set of relation instances forclustering, thus it fails to group many relevant in-stances.
Resolver has the same sparseness problembut it is not affected as much as SNE because of itsdifferent goal (synonym resolution).As the preprocessing instance-detection step forthe problem studied in this paper, open IE extractsrelation instances (in the form of triples) from theopen domain (Etzioni et al 2004; Banko et al2007; Fader et al 2011; Wang et al2011).
Forefficiency, they only use shallow features.
Reverb(Fader et al 2011) is a state-of-the-art open do-main extractor that targets verb-centric relations,which have been shown in Banko and Etzioni(2008) to cover over 70% of open domain rela-tions.
Taking their output as input, algorithms havebeen proposed to resolve objects and relation syn-onyms (Resolver),  extract semantic networks1028(SNE), and map extracted relations into an existingontology (Soderland and Mandhani, 2007).Recent work shows that it is possible to con-struct semantic classes and sets of similar phrasesautomatically with data-driven approaches.
Forgenerating semantic classes, previous work appliesdistributional similarity (Pasca, 2007; Pantel et al2009), uses a few linguistic patterns (Pasca 2004;Sarmento et al 2007), makes use of structure inwebpages (Wang and Cohen 2007, 2009), or com-bines all of them (Shi et al 2010).
Pennacchiottiand Pantel (2009) combines several sources andfeatures.
To find similar phrases, there are 2 close-ly related tasks: paraphrase discovery and recog-nizing textual entailment.
Data-driven paraphrasediscovery methods (Lin and Pantel, 2001; Pascaand Dienes, 2005; Wu and Zhou, 2003; Sekine,2005) extends the idea of distributional similarityto phrases.
The Recognizing Textual Entailmentalgorithms (Berant et al2011) can also be used tofind related phrases since they find pairs of phrasesin which one entails the other.To efficiently cluster high-dimensional datasets,canopy clustering (McCallum et al 2000) uses acheap, approximate distance measure to divide da-ta into smaller subsets, and then cluster each subsetusing an exact distance measure.
It has been ap-plied to reference matching.
The second phase ofWEBRE applies the similar high-level idea of par-tition-then-cluster for speeding up relation cluster-ing.
We design a graph-based partitioningsubroutine that uses various types of evidence,such as shared hypernyms.3 Problem AnalysisThe basic input is a collection of relation instances(triples) of the form <ent1, ctx, ent2>.
For each tri-ple, ctx is a relational phrase expressing the rela-tion between the first argument ent1 and the secondargument ent2.
An example triple is <Obama, winin, NY>.
The triples can be generated by an openIE extractor such as TextRunner or Reverb.
Ourgoal is to automatically build a list of relations?
= {< ent1, ??
?, ent2 >} ?
3 < ?1,?,?2 >  where Pis the set of relation phrases, and ?1  and  ?2  aretwo argument classes.
Examples of triples and rela-tions R (as Type B) are shown in Figure 1.3 This approximately equal sign connects 2 possible represen-tations of a relation: as a set of triple instances or a triple with2 entity classes and a relation phrase class.The first problem is the polysemy of relationphrases, which means that a relation phrase ctx canexpress different relations in different triples.
Forexample, the meaning of be the currency of in thefollowing two triples is quite different: <Euro, bethe currency of, Germany> and <authorship, bethe currency of, science>.
It is more appropriate toassign these 2 triples to 2 relations ?a currency isthe currency of a country?
and ?a factor is im-portant in an area?
than to merge them into one.Formally, a relation phrase ctx is polysemous ifthere exist 2 different relations < ?1,?,?2 >  and< ?1?,?
?,?2?> where ???
?
?
?
??.
In the previ-ous example, be the currency of  is polysemousbecause it appears in 2 different relations.Polysemy of relation phrases is not uncommon.We generate clusters from a large sample of tripleswith the assistance of a soft clustering algorithm,and found that around 22% of relation phrases canbe put into at least 2 disjoint clusters that representdifferent relations.
More importantly, manual in-spection reveals that some common phrases arepolysemous.
For example, be part of can be putinto a relation ?a city is located in a country?
whenconnecting Cities to Countries, and another rela-tion ?a company is a subsidiary of a parent com-pany?
when connecting Companies to Companies.Failure to handle polysemous relation phrases fun-damentally limits the effectiveness of an algorithm.The WEBRE algorithm described later explicitlyhandles polysemy and synonymy of relationphrases in its first and second phase respectively.The second problem is the ?synonymy?
of rela-tion instances.
We use the term synonymy broadlyand we say 2 relation instances are synonymous ifthey express the same semantic relation betweenthe same pair of semantic classes.
For example,both <Euro, be the currency used in, Germany>and <Dinar, be legal tender in, Iraq> express therelation <Currencies, be currency of, Countries>.Solving this problem requires grouping synony-mous relation phrases and identifying argumentsemantic classes for the relation.Various knowledge sources can be derived fromthe source corpus for this purpose.
In this paper wepay special attention to incorporating various se-mantic resources for relation extraction.
We willshow that these semantic sources can significantlyimprove the coverage of extracted relations and the1029Figure 1.
Overview of the WEBRE algorithm (Illustrated with examples sampled from experiment results).
The tables and rec-tangles with a database sign show knowledge sources, shaded rectangles show the 2 phases, and the dotted shapes show the sys-tem output, a set of Type A relations and a set of Type B relations.
The orange arrows denote resources used in phase 1 and thegreen arrows show the resources used in phase 2.best performance is achieved when various re-sources are combined together.4 Mining Relations from the WebWe first describe relevant knowledge sources, andthen introduce the WEBRE algorithm, followed bya briefly analysis on its computational complexity.4.1 Knowledge SourcesEntity similarity graph We build two similaritygraphs for entities: a distributional similarity (DS)graph and a pattern-similarity (PS) graph.
The DSgraph is based on the distributional hypothesis(Harris, 1985), saying that terms sharing similarcontexts tend to be similar.
We use a text windowof size 4 as the context of a term, use PointwiseMutual Information (PMI) to weight context fea-tures, and use Jaccard similarity to measure thesimilarity of term vectors.
The PS graph is gener-ated by adopting both sentence lexical patterns andHTML tag patterns (Hearst, 1992; Kozareva et al2008; Zhang et al 2009; Shi et al 2010).
Twoterms (T) tend to be semantically similar if they co-occur in multiple patterns.
One example of sen-tence lexical patterns is (such as | including)T{,T}* (and|,|.).
HTML tag patterns include tables,dropdown boxes, etc.
In these two graphs, nodesare entities and the edge weights indicate entitysimilarity.
In all there are about 29.6 million nodesand 1.16 billion edges.Hypernymy graph Hypernymy relations arevery useful for finding semantically similar termpairs.
For example, we observed that a small cityin UK and another small city in Germany sharecommon hypernyms such as city, location, andplace.
Therefore the similarity between the twocities is large according to the hypernymy graph,while their similarity in the DS graph and the PSgraph may be very small.
Following existing work(Hearst, 1992, Pantel & Ravichandran 2004; Snowet al 2005; Talukdar et al 2008; Zhang et al2011), we adopt a list of lexical patterns to extracthypernyms.
The patterns include NP {,} (such as){NP,}* {and|or} NP, NP (is|are|was|were|being)(a|an|the) NP, etc.
The hypernymy graph is a bi-partite graph with two types of nodes: entity nodesand label (hypernym) nodes.
There is an edge (T,L) with weight w if L is a hypernym of entity Twith probability w. There are about 8.2 millionnodes and 42.4 million edges in the hypernymygraph.
In this paper, we use the terms hypernymand label interchangeably.Relation phrase similarity: To generate the pair-wise similarity graph for relation phrases with re-gard to the probability of expressing the samerelation, we apply a variant of the DIRT algorithm(Lin and Pantel, 2001).
Like DIRT, the paraphrasediscovery relies on the distributional hypothesis,but there are a few differences: 1) we use stemmedlexical sequences (relation phrases) instead of de-pendency paths as phrase candidates because of thevery large scale of the corpus.
2) We used ordered1030pairs of arguments as features of phrases whileDIRT uses them as independent features.
We em-pirically tested both feature schemes and foundthat using ordered pairs results in likely para-phrases but using independent features the resultcontains general inference rules4.4.2 WEBRE for Relation ExtractionWEBRE consists of two phases.
In the firstphase, a set of semantic classes are discovered andused as argument classes for each relation phrase.This results in a large collection of relations whosearguments are pairs of semantic classes and whichhave exactly one relation phrase.
We call theserelations the Type A relations.
An example Type Arelation is <{New York, London?
}, be locate in,{USA, England, ?}>.
During this phase, polyse-mous relation phrases are disambiguated andplaced into multiple Type A relations.
The secondphase is an efficient algorithm which groups simi-lar Type A relations together.
This step enrichesthe argument semantic classes and groups synon-ymous relation phrases to form relations with mul-tiple expressions, which we called Type Brelations.
Both Type A and Type B relations aresystem outputs since both are valuable resourcesfor downstream applications such as QA and WebSearch.
An overview of the algorithm is shown inFigure 1.
Here we first briefly describe a clusteringsubroutine that is used in both phases, and thendescribe the algorithm in detail.To handle polysemy of objects (e.g., entities orrelations) during the clustering procedure, a keybuilding block is an effective Multi-MembershipClustering algorithm (MMClustering).
For simplic-ity and effectiveness, we use a variant of Hierar-chical Agglomerative Clustering (HAC), in whichwe first cluster objects with HAC, and then reas-sign each object to additional clusters when itssimilarities with these clusters exceed a certainthreshold5.
In the remainder of this paper, we use{C} = MMClustering({object}, SimFunc, ?)
to rep-resent running MMClustering over a set of objects,4 For example, be part of  has ordered argument pairs <A, B>and <C, D>, and be not part of has ordered argument pairs<A, D> and <B, C>.
If arguments are used as independentfeatures, these two phrases shared the same set of features {A,B, C, D}.
However, they are inferential (complement relation-ship) rather than being similar phrases.5 This threshold should be slightly greater than the clusteringthreshold for HAC to avoid generating duplicated clusters.with threshold ?
to generate a list of clusters {C} ofthe objects, given the pairwise object similarityfunction SimFunc.
Our implementation uses HACwith average linkage since empirically it performswell.Discovering Type A Relations The first phaseof the relation extraction algorithm generates TypeA relations, which have exactly one relation phraseand two argument entity semantic classes.
For eachrelation phrase, we apply a clustering algorithm oneach of its two argument sets to generate argumentsemantic classes.
The Phase 1 algorithm processesrelation phrases one by one.
For each relationphrase ctx, step 4 clusters the set {ent1} usingMMClustering to find left-hand-side argument se-mantic classes {C1}.
Then for each cluster C in{C1}, it gathers the right-hand-side argumentswhich appeared in some triple whose left hand-side-side argument is in C, and puts them into{ent2?}.
Following this, it clusters {ent2?}
to findright-hand-side argument semantic classes.
Thisresults in pairs of semantic classes which are ar-guments of ctx.
Each relation phrase can appear inmultiple non-overlapping Type A relations.
Forexample, <Cities, be part of, Countries> and<Companies, be part of, Companies> are differentType A relations which share the same relationphrase be part of.
In the pseudo code, SimEntFuncis encoded in the entity similarity graphs.Algorithm Phase 1: Discovering Type A relationsInput:  set of triples T={<ent1, ctx, ent2>}entity similarity function SimEntFuncSimilarity threshold ?Output:  list of Type A relations {<C1, ctx, C2>}Steps:01.
For each relation phrase ctx02.
{ent1, ctx, ent2} = set of triples sharing ctx03.
{ent1} = set of ent1 in {ent1, ctx, ent2}04.
{C1} = MMClustering({ent1}, SimEntFunc, ?)05.
For each C in { C1}06.
{ent2?}
= set of ??
?2 ?.
?.
?< ??
?1, ??
?, ??
?2 > ??
?
??
?1 ?
?107.
{C2} = MMClustering({ent2?
}, SimEntFunc, ?)08.
For each C2 in {C2}09.
Add <C1, ctx, C2> into {<C1, ctx, C2>}10.
Return {<C1, ctx, C2>}Discovering Type B Relations  The goal ofphase 2 is to merge similar Type A relations, suchas <Cities, be locate in, Countries> and <Cities,be city of, Countries>, to produce Type B relations,which have a set of synonymous relation phrasesand more complete argument entity classes.
Thechallenge for this phase is to cluster a very large1031set of Type A relations, on which it is infeasible torun a clustering algorithm that does pairwise allpair comparison.
Therefore, we designed an evi-dence-based partition-then-cluster algorithm.The basic idea is to heuristically partition thelarge set of Type A relations into small subsets,and run clustering algorithms on each subset.
It isbased on the observation that most pairs of Type Arelations are not similar because of the sparsenessin the entity class and the relation semantic space.If there is little or no evidence showing that twoType A relations are similar, they can be put intodifferent partitions.
Once partitioned, the clusteringalgorithm only has to be run on each much smallersubset, thus computation complexity is reduced.The 2 types of evidence we used are sharedmembers and shared hypernyms of relation argu-ments.
For example, 2 Type A relationsr1=<Cities, be city of, Countries> and r2=<Cities,be locate in, Countries> share a pair of arguments<Tokyo, Japan>, and a pair of hypernyms <?city?,?country?>.
These pieces of evidence give us hintsthat they are likely to be similar.
As shown in thepseudo code, shared arguments and hypernyms areused as independent evidence to reduce sparseness.Algorithm Phase 2: Discovering Type B relationsInput:  Set of Type A relations {r}={<C1, ctx, C2>}Relation similarity function SimRelationFuncMap from entities to their hypernyms: Mentity2labelSimilarity threshold ?Edge weight threshold ?Variables G(V, E) = weighted graph in which V={r}Output:  Set of Type B relations {<C1, P, C2>}Steps:01.
{<ent, {r?
}>} = build  inverted index from argumentent to set of Type A relations {r?}
on {<C1, ctx, C2>}02 {<l, {r?
}>} = build  inverted index from hypernym lof arguments to set of Type A relations {r?}
on {<C1,ctx, C2>} with map Mentity2label03.
For each ent in {<ent, {r?}>}04.
For each pair of r1 and r2  s.t.
?1 ?
{??}
?
?2 ?
{??}05.
weight_edge(<r1, r2>) += weight (ent)06.
For each l in {<l, {r?}>}07.
For each pair of r1 and r2  s.t.
?1 ?
{??}
?
?2 ?
{??}08.
weight_edge(<r1, r2>) += weight (l)09.
For each edge <r1, r2> in G10.
If weight_edge(<r1, r2>) < ?11.
Remove edge <r1, r2> from G12.
{CC}= DFS(G)13.
For each connected component CC in {CC}14.
{<C1, ctx, C2>} = vertices in CC15.
{<C1?, P?, C2?>} = MMClustering({<C1, ctx, C2>},SimRelationFunc, ?)16.
Add {<C1?, P?, C2?>} into {<C1, P, C2>}17.
Return {<C1, P, C2>}Steps 1 and 2 build an inverted index from evi-dence to sets of Type A relations.
On the graph Gwhose vertices are Type A relations, steps 3 to 8set the value of edge weights based on the strengthof evidence that shows the end-points are related.The weight of evidence E is calculated as follows:??????(?)
=# ??????
??????
??
?????
?
???????
?
?max(# ???????
?
???????
??
)The idea behind this weighting scheme is similarto that of TF-IDF in that the weight of evidence ishigher if it appears more frequently and is less am-biguous (appeared in fewer semantic classes duringclustering of phase 1).
The weighting scheme isapplied to both shared arguments and labels.After collecting evidence, we prune (steps 9 to11) the edges with a weight less than a threshold ?to remove noise.
Then a Depth-First Search (DFS)is called on G to find all Connected ComponentsCC of the graph.
These CCs are the partitions oflikely-similar Type A relations.
We run MMClus-tering on each CC in {CC} and generate Type Brelations (step 13 to step 16).
The similarity of 2relations (SimRelationFunc) is defined as follows:???
(< ?1,?,?2 >, < ?1?,??,?2?
>)= ?0,     ??
???(?,??)
<  ?min????(?1,?1?
), ???(?2,?2?)?
,   ???
?4.3 Computational ComplexityWEBRE is very efficient since both phases de-compose the large-clustering task into much small-er clustering tasks over partitions.
Given n objectsfor clustering, a hierarchical agglomerative cluster-ing algorithm requires ?
(?2)  pairwise compari-sons.
Assuming the clustering task is split intosubtasks of size ?1, ?2, ?, ?
?, thus the computa-tional complexity is reduced to ?(?
?
?2?1 ).
Ideallyeach subtask has an equal size of ?/?, so the com-putational complexity is reduced to O(?2/?)
, afactor of ?
speed up.
In practice, the sizes of parti-tions are not equal.
Taking the partition sizes ob-served in the experiment with 0.2 million Type Arelations as input, the phase 2 algorithm achievesaround a 100-fold reduction in pairwise compari-sons compared to the agglomerative clustering al-gorithm.
The combination of phase 1 and phase 2achieves more than a 1000-fold reduction in pair-wise comparison, compared to running an agglom-erative clustering algorithm directly on 14.7million triples.
This reduction of computational1032complexity makes the unsupervised extraction ofrelations on a large dataset a reality.
In the experi-ments with 14.7 million triples as input, phase 1finished in 22 hours, and the phase 2 algorithmfinished in 4 hours with one CPU core.Furthermore, both phases can be run in parallelin a distributed computing environment becausedata is partitioned.
Therefore it is scalable and effi-cient for clustering a very large number of relationinstances from a large-scale corpus like the web.5 ExperimentData preparation We tested WEBRE on re-sources extracted from the English subset of theClueweb09 Dataset, which contains 503 millionwebpages.
For building knowledge resources, allwebpages are cleaned and then POS tagged andchunked with in-house tools.
We implemented thealgorithms described in section 4.1 to generate theknowledge sources, including a hypernym graph,two entity similarity graphs and a relation phrasesimilarity graph.We used Reverb Clueweb09 Extractions 1.1(downloaded from reverb.cs.washington.edu) asthe triple store (relation instances).
It is the com-plete extraction of Reverb over Clueweb09 afterfiltering low confidence and low frequency triples.It contains 14.7 million distinct triples with 3.3million entities and 1.3 million relation phrases.We choose it because 1) it is extracted by a state-of-the-art open IE extractor from the open-domain,and 2) to the best of our knowledge, it contains thelargest number of distinct triples extracted from theopen-domain and which is publicly available.Evaluation setup The evaluations are organized asfollows: we evaluate Type A relation extractionand Type B relation extraction separately, and thenwe compare WEBRE to its closest prior workSNE.
Since both phases are essentially clusteringalgorithms, we compare the output clusters withhuman labeled gold standards and report perfor-mance measures, following most previous worksuch as Kok and Domingos (2008) and Hasegawaet al(2004).
Three gold standards are created forevaluating Type A relations, Type B relations andthe comparison to SNE, respectively.
In the exper-iments, we set ?=0.6, ?=0.1 and ?=0.02 based ontrial runs on a small development set of 10k rela-tion instances.
We filtered out the Type A relationsand Type B relations which only contain 1 or 2triples since most of these relations are not differ-ent from a single relation instance and are not veryinteresting.
Overall, 0.2 million Type A relationsand 84,000 Type B relations are extracted.Evaluating Type A relations To understand theeffectiveness of knowledge sources, we run Phase1 multiple times taking entity similarity graphs(matrices) constructed with resources listed below:?
TS: Distributional similarity based on the triplestore.
For each triple <ent1, ctx, ent2>, featuresof ent1 are {ctx} and {ctx ent2}; features of ent2are {ctx} and {ent1 ctx}.
Features are weightedwith PMI.
Cosine is used as similarity measure.?
LABEL: The similarity between two entities iscomputed according to the percentage of tophypernyms they share.?
SIM: The similarity between two entities is thelinear combination of their similarity scores inthe distributional similarity graph and in thepattern similarity graph.?
SIM+LABEL SIM and LABEL are combined.Observing that SIM generates high quality butoverly fine-grained semantic classes, we modifythe entity clustering procedure to cluster argu-ment entities based on SIM first, and then fur-ther clustering the results based on LABEL.The outputs of these runs are pooled and mixedfor labeling.
We randomly sampled 60 relationphrases.
For each phrase, we select the 5 most fre-quent Type A relations from each run (4?5=206Type A relations in all).
For each relation phrase,we ask a human labeler to label the mixed pool ofType A relations that share the phrase: 1) The la-belers7 are asked to first determine the major se-mantic relation of each Type A relation, and thenlabel the triples as good, fair or bad based onwhether they express the major relation.
2) Thelabeler also reads all Type A relations and manual-ly merges the ones that express the same relation.These 2 steps are repeated for each phrase.
Afterlabeling, we create a gold standard GS1, whichcontains roughly 10,000 triples for 60 relationphrases.
On average, close to 200 triples are manu-6  Here 4 means the 4 methods (the bullet items above) ofcomputing similarity.7 4 human labelers perform the task.
A portion of the judg-ments were independently dual annotated; inter-annotatoragreement is 79%.
Moreover, each judgment is cross-checkedby at least one more annotator, further improving quality.1033ally labeled and clustered for each phrase.
Thiscreates a large data set for evaluation.We report micro-average of precision, recall andF1 on the 60 relation phrases for each method.
Pre-cision (P) and Recall (R) of a given relation phraseis defined as follows.
Here ??
and ???
represents aType A relation in the algorithm output and GS1,respectively.
We use t for triples and s(t) to repre-sent the score of the labeled triple t. s(t) is set to1.0, 0.5 or 0 for t labeled as good, fair and bad,respectively.?
=?
?
?(?)
????
???
|??|?
?, ?
=?
?
?(?)
????
???
?
?(??)
????????
?The results are in table 1.
Overall, LABEL per-forms 53% better than TS in F-measure, andSIM+LABEL performs the best, 8% better thanLABEL.
Applying a simple sign test shows bothdifferences are clearly significant (p<0.001).
Sur-prisingly, SIM, which uses the similarity matrixextracted from full text, has a F1 of 0.277, which islower than TS.
We also tried combining TS andLABEL but did not find encouraging performancecompared to SIM+LABEL.Algorithm Precision Recall F1TS 0.842 (0.886) 0.266 0.388LABEL 0.855 (0.870) 0.481 0.596SIM 0.755 (0.964) 0.178 0.277SIM+LABEL 0.843 (0.872) 0.540 0.643Table 1.
Phase 1 performance (averaged on multiple runs) ofthe 4 methods.
The highest performance numbers are in bold.
(The number in parenthesis is the micro-average when empty-result relation phrases are not considered for the method).Among the 4 methods, SIM has the highest preci-sion (0.964) when relation phrases for which itfails to generate any Type A relations are exclud-ed, but its recall is low.
Manual checking showsthat SIM tends to generate overly fine-grained ar-gument classes.
If fine-grained argument classes orextremely high-precision Type A relations are pre-ferred, SIM is a good choice.
LABEL performssignificantly better than TS, which shows that hy-pernymy information is very useful for finding ar-gument semantic classes.
However, it has coverageproblems in that the hypernym finding algorithmfailed to find any hypernym from the corpus forsome entities.
Following up, we found thatSIM+LABEL has similar precision and the highestrecall.
This shows that the combination of semanticspaces is very helpful.
The significant recall im-provement from TS to SIM+LABEL shows thatthe corpus-based knowledge resources significant-ly reduce the data sparseness, compared to usingfeatures extracted from the triple store only.
Theresult of the phase 1 algorithm with SIM+LABELis used as input for phase 2.Evaluating Type B relations The goal is 2-fold:1) to evaluate the phase 2 algorithm.
This involvescomparing system output to a gold standard con-structed by hand, and reporting performance; 2) toevaluate the quality of Type B relations.
For this,we will also report triple-level precision.We construct a gold standard GS28 for evaluat-ing Type B relations as follows: We randomlysampled 178 Type B relations, which contain 1547Type A relations and more than 100,000 triples.Since the number of triples is very large, it is in-feasible for labelers to manually cluster triples toconstruct a gold standard.
To report precision, weasked the labelers to label each Type A relationcontained in this Type B relation as good, fair orbad based on whether it expresses the same rela-tion.
For recall evaluation, we need to know howmany Type A relations are missing from each TypeB relation.
We provide the full data set of Type Arelations along with three additional resources: 1) atool which, given a Type A relation, returns aranked list of similar Type A relations based on thepairwise relation similarity metric in section 4, 2)DIRT paraphrase collection, 3) WordNet (Fell-baum, 1998) synsets.
The labelers are asked to findsimilar phrases by checking phrases which containsynonyms of the tokens in the query phrase.
Givena Type B relation, ideally we expect the labelers tofind all missing Type A relations using these re-sources.
We report precision (P) and recall (R) asfollows.
Here ??
and ???
represent Type B rela-tions in the algorithm output and GS2, respective-ly.
??
and ???
represent Type A relations.
?(??
)denotes the score of ??.
It is set to 1.0, 0.5 and 0for good, fair or bad respectively.?
=?
?
|??|??(??)
????????
?
|?
?|  ??????
?, ?
=?
?
|??|??(??)
????????
?
????
????
??????
?We also ask the labeler to label at most 50 ran-domly sampled triples from each Type B relation,and calculate triple-level precision as the ratio ofthe sum of scores of triples over the number of8 3 human labelers performed the task.
A portion of the judg-ments were independently dual annotated; inter-annotatoragreement is 73%.
Similar to labeling Type A relations, eachjudgment is cross-checked by at least one more annotator,further improving quality.1034Argument 1 Relation phrase Argument 2marijuana, caffeine, nicotine?
result in, be risk factor for, be major cause of?
insomnia, emphysema, breast cancer,?C# 2.0, php5, java, c++, ?
allow the use of, also use, introduce the concept of?
destructors, interfaces, template,?clinton, obama, mccain, ?
win, win in, take, be lead in,?
ca, dc, fl, nh, pa, va, ga, il, nc,?Table 3.
Sample Type B relations extracted.sampled triples.
We use ????
to represent the preci-sion calculated based on labeled triples.
Moreover,as we are interested in how many phrases arefound by our algorithm, we also include ??????
?,which is the recall of synonymous phrases.
Resultsare shown in Table 2.Interval P R (???????)
F1 ????
count[3, 5) 0.913 0.426 (0.026) 0.581 0.872 52149[5, 10) 0.834 0.514 (0.074) 0.636 0.863 21981[10, 20) 0.854 0.569 (0.066) 0.683 0.883 6277[20, 50) 0.899 0.675 (0.406) 0.771 0.894 2630[50, +?)
0.922 0.825 (0.594) 0.871 0.929 1089Overall 0.897 0.684 (0.324) 0.776 0.898 84126Table 2.
Performance for Type B relation extraction.
The firstcolumn shows the range of the maximum sizes of Type Arelations in the Type B relation.
The last column shows thenumber of Type B relations that are in this range.
The numberin parenthesis in the third column is the recall of phrases.The result shows that WEBRE can extract Type Brelations at high precision (both P and ????).
Theoverall recall is 0.684.
Table 2 also shows a trendthat if the maximum number of Type A relation inthe target Type B relation is larger, the recall isbetter.
This shows that the recall of Type B rela-tions depends on the amount of data available forthat relation.
Some examples of Type B relationsextracted are shown in Table 3.Comparison with SNE We compare WEBRE?sextracted Type B relations to the relations extract-ed by its closest prior work SNE9.
We found SNEis not able to handle the 14.7 million triples in aforeseeable amount of time, so we randomly sam-pled 1 million (1M) triples 10 and test both algo-rithms on this set.
We also filtered out resultclusters which have only 1 or 2 triples from bothsystem outputs.
For comparison purposes, we con-structed a gold standard GS3 as follows: randomlyselect 30 clusters from both system outputs, andthen find similar clusters from the other systemoutput, followed by manually refining the clusters9 Obtained from alchemy.cs.washington.edu/papers/kok0810 We found that SNE?s runtime on 1M triples varies fromseveral hours to over a week, depending on the parameters.The best performance is achieved with runtime of approxi-mately 3 days.
We also tried SNE with 2M triples, on whichmany runs take several days and show no sign of convergence.For fairness, the comparison was done on 1M triples.by merging similar ones and splitting non-coherentclusters.
GS3 contains 742 triples and 135 clusters.We report triple-level pairwise precision, recalland F1 for both algorithms against GS3, and reportresults in Table 4.
We fine-tuned SNE (using gridsearch, internal cross-validation, and coarse-to-fineparameter tuning), and report its best performance.Algorithm Precision Recall F1WEBRE 0.848 0.734 0.787SNE 0.850 0.080 0.146Table 4.
Pairwise precision/recall/F1 of WEBRE and SNE.Table 4 shows that WEBRE outperforms SNEsignificantly in pairwise recall while having similarprecision.
There are two reasons.
First, WEBREmakes use of several corpus-level semantic sourcesextracted from the corpus for clustering entitiesand phrases while SNE uses only features in thetriple store.
These semantic resources significantlyreduced data sparseness.
Examination of the outputshows that SNE is unable to group many triplesfrom the same generally-recognized fine-grainedrelations.
For example, SNE placed relation in-stances <Barbara, grow up in, Santa Fe> and<John, be raised mostly in, Santa Barbara> into 2different clusters because the arguments andphrases do not share features nor could be groupedby SNE?s mutual clustering.
In contrast, WEBREgroups them together.
Second, SNE assumes a re-lation phrase to be in exactly one cluster.
For ex-ample, SNE placed be part of in the phrase clusterbe city of and failed to place it in another cluster besubsidiary of.
This limits SNE?s ability to placingrelation instances with polysemous phrases intocorrect relation clusters.It should be emphasized that we use pairwiseprecision and recall in table 4 to be consistent withthe original SNE paper.
Pairwise metrics are muchmore sensitive than instance-level metrics, and pe-nalize recall exponentially in the worst case11 if analgorithm incorrectly splits a coherent cluster;therefore the absolute pairwise recall difference11 Pairwise precision and recall are calculated on all pairs thatare in the same cluster, thus are very sensitive.
For example, ifan algorithm incorrectly split a cluster of size N to a smallermain cluster of size N/2 and some constant-size clusters, pair-wise recall could drop to as much as ?
of its original value.1035should not be interpreted as the same as the in-stance-level recall reported in previous experi-ments.
On 1 million triples, WEBRE generates12179 triple clusters with an average size12 of 13while SNE generate 53270 clusters with an aver-age size 5.1.
In consequence, pairwise recall dropssignificantly.
Nonetheless, at above 80% pairwiseprecision, it demonstrates that WEBRE can groupmore related triples by adding rich semantics har-vested from the web and employing a more generaltreatment of polysemous relation phrases.
On 1Mtriples, WEBRE finished in 40 minutes, while therun time of SNE varies from 3 hours to a few days.6 ConclusionWe present a fully unsupervised algorithmWEBRE for large-scale open-domain relation ex-traction.
WEBRE explicitly handles polysemy rela-tions and achieves a significant improvement onrecall by incorporating rich corpus-based semanticresources.
Experiments on a large data set showthat it can extract a very large set of high-qualityrelations.AcknowledgementsSupported in part by the Intelligence AdvancedResearch Projects Activity (IARPA) via Air ForceResearch Laboratory (AFRL) contract numberFA8650-10-C-7058.
The U.S. Government is au-thorized to reproduce and distribute reprints forGovernmental purposes notwithstanding any copy-right annotation thereon.
The views and conclu-sions contained herein are those of the authors andshould not be interpreted as necessarily represent-ing the official policies or endorsements, eitherexpressed or implied, of IARPA, AFRL, or theU.S.
Government.ReferencesMichele Banko, Michael J. Cafarella, Stephen Soder-land, Matt Broadhead, and Oren Etzioni.
2007.
OpenInformation Extraction from the Web.
In Proceedingsof IJCAI 2007.12 The clusters which have only 1 or 2 triples are removed andnot counted here for both algorithms.Michele Banko and Oren Etzioni.
2008.
The TradeoffsBetween Open and Traditional Relation Extraction.In Proceedings of ACL 2008.Jonathan Berant, Ido Dagan and Jacob Goldberger.2011.
Global Learning of Typed Entailment Rules.
InProceedings of ACL 2011.Razvan Bunescu and Raymond J. Mooney.
2004.
Col-lective Information Extraction with Relational Mar-kov Networks.
In Proceedings of ACL 2004.Jinxiu Chen, Donghong Ji, Chew Lim Tan, ZhengyuNiu.
2005.
Unsupervised Feature Selection for Rela-tion Extraction.
In Proceedings of IJCNLP 2005.Oren Etzioni, Michael Cafarella, Doug Downey, Stan-ley Kok, Ana-Maria Popescu, Tal Shaked, StephenSoderland, Daniel S. Weld, and Alexander Yates.2004.
Web-scale information extraction inKnowItAll (preliminary results).
In Proceedings ofWWW 2004.Oren Etzioni, Michael Cafarella, Doug Downey,AnaMaria Popescu, Tal Shaked, Stephen Soderland,Daniel S. Weld and Alexander Yates.
2005.
Unsu-pervised named-entity extraction from the Web: AnExperimental Study.
In Artificial Intelligence,165(1):91-134.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying Relations for Open Information Ex-traction.
In Proceedings of EMNLP 2011.Christiane Fellbaum (Ed.).
1998.
WordNet: An Elec-tronic Lexical Database.
Cambridge, MA: MIT Press.Zelig S. Harris.
1985.
Distributional Structure.
The Phi-losophy of Linguistics.
New York: Oxford Uni-versity Press.Takaaki Hasegawa, Satoshi Sekine, Ralph Grishman .2004.Discovering Relations among Named Entitiesfrom Large Corpora.
In Proceedings of ACL 2004.Marti A. Hearst.
1992.
Automatic  Acquisition of  Hy-ponyms from Large Text Corpora.
In Proceedings ofCOLING 1992.Stanley Kok and Pedro Domingos.
2008.
ExtractingSemantic Networks from Text via Relational Cluster-ing.
In Proceedings of ECML 2008.Zornitsa Kozareva, Ellen Riloff, Eduard Hovy.
2008.Semantic Class Learning from the Web with Hypo-nym Pattern Linkage Graphs.
In Proceedings of ACL2008.Dekang Lin and Patrick Pantel.
2001.
DIRT ?
Discov-ery of Inference Rules from Text.
In Proceedings ofKDD 2001.Andrew McCallum, Kamal Nigam and Lyle Ungar.2000.
Efficient Clustering of High-Dimensional DataSets with Application to Reference Matching.
In Pro-ceedings of KDD 2000.Patrick Pantel, Eric Crestan, Arkady Borkovsky, Ana-Maria Popescu and Vishnu Vyas.
2009.
Web-ScaleDistributional Similarity and Entity Set Expansion.
InProceedings of EMNLP 2009.1036Patrick Pantel and Dekang Lin.
2002.
Discovering wordsenses from text.
In Proceedings of KDD2002.Patrick Pantel and Deepak Ravichandran.
2004.
Auto-matically Labeling Semantic Classes.
In Proceedingsof HLT/NAACL-2004.Marius Pasca.
2004.
Acquisition of Categorized NamedEntities for Web Search, In Proceedings of CIKM2004.Marius Pasca.
2007.
Weakly-supervised discovery ofnamed entities using web search queries.
In Proceed-ings of CIKM 2007.Marius Pasca and Peter Dienes.
2005.
Aligning needlesin a haystack: Paraphrase acquisition across the Web.In Proceedings of IJCNLP 2005.Marco Pennacchiotti and Patrick Pantel.
2009.
EntityExtraction via Ensemble Semantics.
In Proceedingsof EMNLP 2009.Benjamin Rosenfeld and Ronen Feldman.
2007.
Clus-tering for Unsupervised Relation Identification.
InProceedings of CIKM 2007.Luis Sarmento, Valentin Jijkoun, Maarten de Rijke andEugenio Oliveira.
2007.
?More like these?
: growingentity classes from seeds.
In Proceedings of CIKM2007.Satoshi Sekine.
2005.
Automatic paraphrase discoverybased on context and keywords between NE pairs.
InProceedings of the International Workshop on Para-phrasing, 2005.Shuming Shi, Huibin Zhang, Xiaojie Yuan, Ji-RongWen.
2010.
Corpus-based Semantic Class Mining:Distributional vs. Pattern-Based Approaches.
In Pro-ceedings of COLING 2010.Yusuke Shinyama, Satoshi Sekine.
2006.
PreemptiveInformation Extraction using Unrestricted RelationDiscovery, In Proceedings of NAACL 2006.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2005.Learning Syntactic Patterns for Automatic HypernymDiscovery.
In Proceedings of  In NIPS 17, 2005.Stephen Soderland and Bhushan Mandhani.
2007.
Mov-ing from Textual Relations to Ontologized Relations.In Proceedings of the 2007 AAAI Spring Symposiumon Machine Reading.Partha Pratim Talukdar, Joseph Reisinger, Marius Pasca,Deepak Ravichandran, Rahul Bhagat and FernandoPereira.
2008.
Weakly-Supervised Acquisition of La-beled Class Instances using Graph Random Walks.
InProceedings of EMNLP 2008.David Vickrey, Oscar Kipersztok and Daphne Koller.2010.
An Active Learning Approach to Finding Re-lated Terms.
In Proceedings of ACL 2010.Vishnu Vyas and Patrick Pantel.
2009.
SemiAutomaticEntity Set Refinement.
In Proceedings ofNAACL/HLT 2009.Vishnu Vyas, Patrick Pantel and Eric Crestan.
2009,Helping Editors Choose Better Seed Sets for EntitySet Expansion, In Proceedings of CIKM 2009.Richard C. Wang and William W. Cohen.
2007.
Lan-guage- Independent Set Expansion of Named EntitiesUsing the Web.
In Proceedings of ICDM 2007.Richard C. Wang and William W. Cohen.2009.
Automatic Set Instance Extraction using theWeb.
In Proceedings of ACL-IJCNLP 2009.Wei Wang, Romaric Besan?on and Olivier Ferret.
2011.Filtering and Clustering Relations for UnsupervisedInformation Extraction in Open Domain.
In Proceed-ings of CIKM 2011.Fei Wu and Daniel S. Weld.
2010.
Open informationextraction using Wikipedia.
In Proceedings of ACL2010.Hua Wu and Ming Zhou.
2003.
Synonymous colloca-tion extraction using translation information.
In Pro-ceedings of the ACL Workshop on MultiwordExpressions: Integrating Processing 2003.Limin Yao, Aria Haghighi, Sebastian Riedel, AndrewMcCallum.
2011.
Structured Relation Discovery Us-ing Generative Models.
In Proceedings of EMNLP2011.Alexander Yates and Oren Etzioni.
2007.
UnsupervisedResolution of Objects and Relations on the Web.
InProceedings of HLT-NAACL 2007.Fan Zhang, Shuming Shi, Jing Liu, Shuqi Sun, Chin-Yew Lin.
2011.
Nonlinear Evidence Fusion andPropagation for Hyponymy Relation Mining.
In Pro-ceedings of ACL 2011.Huibin Zhang, Mingjie Zhu, Shuming Shi, and Ji-RongWen.
2009.
Employing Topic Models for Pattern-based Semantic Class Discovery.
In Proceedings ofACL 2009.1037
