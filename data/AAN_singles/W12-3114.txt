Proceedings of the 7th Workshop on Statistical Machine Translation, pages 120?126,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsQuality Estimation:an experimental study using unsupervised similarity measuresErwan MoreauCNGL and Computational Linguistics GroupCentre for Computing and Language StudiesSchool of Computer Science and StatisticsTrinity College DublinDublin 2, Irelandmoreaue@cs.tcd.ieCarl VogelComputational Linguistics GroupCentre for Computing and Language StudiesSchool of Computer Science and StatisticsTrinity College DublinDublin 2, Irelandvogel@cs.tcd.ieAbstractWe present the approach we took for our par-ticipation to the WMT12 Quality EstimationShared Task: our main goal is to achieve rea-sonably good results without appeal to super-vised learning.
We have used various simi-larity measures and also an external resource(Google N -grams).
Details of results clarifythe interest of such an approach.1 IntroductionQuality Estimation (or Confidence Estimation)refers here to the task of evaluating the quality ofthe output produced by a Machine Translation (MT)system.
More precisely it consists in evaluating thequality of every individual sentence, in order (for in-stance) to decide whether a given sentence can bepublished as it is, should be post-edited, or is so badthat it should be manually re-translated.To our knowledge, most approaches so far (Spe-cia et al, 2009; Soricut and Echihabi, 2010; He etal., 2010; Specia et al, 2011) use several featurescombined together using supervised learning in or-der to predict quality scores.
These features be-long to two categories: black box features whichcan be extracted given only the input sentence andits translated version, and glass box features whichrely on various intermediate steps of the internal MTengine (thus require access to this internal data).For the features they studied, Specia et al (2009)have shown that black box features are informativeenough and glass box features do not significantlycontribute to the accuracy of the predicted scores.In this study, we use only black box features, andfurther, eschew supervised learning except in thebroadest sense.
Our method requires some refer-ence data, all taken to be equally good exemplarsof a positive reference category, against which theexperimental sentences are compared automatically.This is the extent of broader-sense supervision.
Themethod does not require a training set of items eachannotated by human experts with quality scores (ex-cept for the purpose of evaluation of course).Successful unsupervised learning averts risks ofthe alternative: supervised learning necessarilymakes the predicting system dependent on the an-notated training data, i.e.
less generic, and requiresa costly human evalution stage to obtain a reliablemodel.
Of course, our approach is likely not to per-form as well as supervised approaches: here the goalis to find a rather generic robust way to measurequality, not to achieve the best accuracy.
Neverthe-less, in the context of this Quality Evaluation Sharedtask (see (Callison-Burch et al, 2012) for a detaileddescription) we have also used supervised learningas a final stage, in order to submit results which canbe compared to other methods (see ?4).We investigate the use of various similarity mea-sures for evaluating the quality of machine translatedsentences.
These measures compare the sentenceto be evaluated against a reference text, providinga similarity score result.
The reference data is sup-posed to represent standard (well-formed) language,so that the score is expected to reflect how complex(source side) or how fluent (target side) the givensentence is.After presenting the similarity measures in sec-120tion 2, we will show in section 3 how they performindividually on the ranking task; finally we will ex-plain in section 4 how the results that we submittedwere obtained using supervised learning.2 ApproachOur method consists in trying to find the best mea-sure(s) to estimate the quality of machine translatedsentences, i.e.
the ones which show the highest cor-relation with the human annotators scores.
The mea-sures we have tested work always as follows.Given a sentence to evaluate (source or target),a score is computed by comparing the sentenceagainst a reference dataset (usually a big set of sen-tences).
This dataset is assumed to represent stan-dard and/or well-formed language.1 This score rep-resents either the quality (similarity measure) or thefaultiness (distance measure) of the sentence.
It isnot necessarily normalized, and in general cannot beinterpreted straightforwardly (for example like the 1to 5 scale used for this Shared Task, in which everyvalue 1, 2, 3, 4, 5 has a precise meaning).
In the con-text of the Shared task, this means that we focus onthe ?ranking?
evaluation measures provided ratherthan the ?scoring?
measures.
These scores are ratherintended to compare sentences relatively to one an-other: for instance, they can be used to discard theN% lowest quality sentences from post-editing.The main interest in such an approach is inavoiding dependence on costly-to-annotate trainingdata?correspondingly costly to obtain and whichrisk over-tuning the predicting system to the articu-lated features of the training items.
Our method stilldepends on the dataset used as reference, but thiskind of dependency is much less constraining, be-cause the reference dataset can be any text data.
Toobtain the best possible results, the reference datahas to be representative enough of what the eval-uated sentences should be (if they were of perfectquality), which implies that:?
a high coverage (common words or n-grams) ispreferable; this also means that the size of thisdataset is important;1We use this definition of ?reference?
in this article.
Pleasenotice that this differs from the sense ?human translation of asource sentence?, which is more common in the MT literature.?
the quality (grammaticality, language register,etc.)
must be very good: errors in the referencedata will infect the predicted scores.It is rather easy to use different reference datasetswith our approach (as opposed to obtain new humanscores and training a new model on this data), sincenowadays numerous textual resources are available(at least for the most common languages).2.1 Similarity measuresAll the measures we have used compare (in differentways) the n-grams of the tested sentence against thereference data (represented as a big bag of n-grams).There is a variety of parameters for each measure;here are the parameters which are common to all:Length of n-grams: from unigrams to 6-grams;Punctuation: with or without punctuation marks;Case sensitivity: binary;Sentence boundaries: binary signal of whetherspecial tokens should be added to mark the startand the end of sentences.2 This permits:?
that there is the same number of n-gramscontaining a token w, for every w in thesentence;?
to match n-grams starting/ending asentence only against n-grams whichstart/end a sentence.Most configurations of parameters presented in thispaper are empirical (i.e.
only the parameter set-tings which performed better during our tests wereretained).
Below are the main measures explored.32.1.1 Okapi BM25 similarity (TF-IDF)Term Frequency-Inverse Document Frequency(TF-IDF) is a widely used similarity measure inInformation Retrieval(IR).
It has also been shownto perform significantly better than only term fre-quency in tasks like matching coreferent namedentities (see e.g.
Cohen et al (2003)), which is2With trigrams, ?Hello World !?
(1 trigram) becomes?# # Hello World !
# #?
(5 trigrams).3One of the measures is not addressed in this paper for IPreasons (this measure obtained good results but was not best).121technically not very different from comparing sen-tences.
The general idea is to compare two docu-ments4 using their bags of n-grams representations,but weighting the frequency of every n-gram withthe IDF weight, which represents ?how meaning-ful?
the n-gram is over all documents based on itsinverse frequency (because the n-grams which arevery common are not very meaningful in general).There are several variants of TF-IDF compari-son measures.
The most recent ?Okapi BM25?
ver-sion was shown to perform better in general than theoriginal (more basic) definition (Jones et al, 2000).Moreover, there are different ways to actually com-bine the vectors together (e.g.
L1 or L2 distance).
Inthese experiments we have only used the Cosine dis-tance, with Okapi BM25 weights.
The weights arecomputed as usual (using the number of sentencescontaining X for any n-gram X), but are based onlyon the reference data.2.1.2 Multi-level matchingFor a given length N, ?simple matching?
is de-fined as follows: for every N -gram in the sentence,the score is incremented if this N -gram appears atleast once in the reference data.
The score is thenrelativized to the sentence N -gram length.
?Multi-level matching?
(MLM) is similar but withdifferent lengths of n-grams.
For (maximum) lengthN , the algorithm is as follows (for every n-gram):if the n-gram appears in the reference data the scoreis incremented; otherwise, for all n-grams of lengthN ?
1 in this n-gram, apply recursively the samemethod, but apply a penalty factor p (p < 1) tothe result.5 This is intended to overcome the bi-nary behaviour of the ?simple matching?.
This wayshort sentences can always be assigned a score, andmore importantly the score is smoothed accordingto the similarity of shorter n-grams (which is the be-haviour one wants to obtain intuitively).4In this case every sentence is compared against the refer-ence data; from an IR viewpoint, one can see the reference dataas the request and each sentence as one of the possible docu-ments.5This method is equivalent to computing the ?simple match-ing?
for different lengths N of N -grams, and then combine thescores sN in the following way: if sN < sN?1, then addp ?
(sN?1 ?
sN ) to the score, and so on.
However this ?ex-ternal?
combination of scores can not take into account some ofthe extensions (e.g.
weights).Two main variants have been tested.
The first oneconsists in using skip-grams.6 Different sizes andconfigurations were tested (combining skip-gramsand standard sequential n-grams), but none gavebetter results than using only sequential n-grams.The second variant consists in assigning a more fine-grained value, based on different parameters, insteadof always assigning 1 to the score when n-gram oc-curs in the reference data.
An optimal solution is notobvious, so we tried different strategies, as follows.Firstly, using the global frequency of the ngramin the reference data: intuitively, this could be in-terpreted as ?the more an n-gram appears (in thereference data), the more likely it is well-formed?.However there are obviously n-grams which appeara lot more than others (especially for short n-grams).This is why we also tried using the logarithm of thefrequency, in order to smooth discrepancies.Secondly, using the inverse frequency: this isthe opposite idea, thinking that the common n-grams are easy to translate, whereas the rare n-grams are harder.
Consequently, the critical partsof the sentence are the rare n-grams: assigning themmore weight focuses on these.
This works in bothcases (if the n-gram is actually translated correctlyor not), because the weight assigned to the n-gramis taken into account in the normalization factor.Finally, using the Inverse Document Frequency(IDF): this is a similar idea as the previous one, ex-cept that instead of considering the global frequencythe number of sentences containing the n-gram istaken into account.
In most cases (and in all casesfor long n-grams), this is very similar to the previ-ous option because the cases where an n-gram (atleast with n > 1) appears several times in the samesentence are not common.2.2 Resources used as reference dataThe reference data against which the sentencesare compared is crucial to the success of our ap-proach.
As the simplest option, we have used theEuroparl data on which the MT model was trained(source/target side for source/target sentences).
Sep-arately we tested a very different kind of data,namely the Google Books N -grams (Michel et al,6The true-false-true skip-grams in ?There isno such thing?
: There no, is such and no thing.1222011): it is no obstacle that the reference sentencesthemselves are unavailable, since our measures onlyneed the set of n-grams and possibly their frequency(Google Books N -gram data contains both).3 Individual measures onlyIn this section we study how our similarity measuresand the baseline features (when used individually)perform on the ranking task.
This evaluation canonly be done by means of DeltaAvg and Spearmancorrelation, since the values assigned to sentencesare not comparable to quality scores.
We have testednumerous combinations of parameters, but show be-low only the best ones (for every case).3.1 General observationsMethod Ref.
data DeltaAvg SpearmanMLM,1-4 Google, eng 0.26 0.22Baseline feature 1 0.29 0.29Baseline feature 2 0.29 0.29MLM,1-3,lf Google, spa 0.32 0.28Okapi,3,b EP, spa 0.33 0.27Baseline feature 8 0.33 0.32Okapi,2,b EP, eng 0.34 0.30Baseline feature 12 0.34 0.32Baseline feature 5 0.39 0.39MLM,1-5,b EP, spa 0.39 0.39MLM,1-5,b EP, eng 0.39 0.40Baseline feature 4 0.40 0.40Table 1: Best results by method and by resource on train-ing data.
b = sentence boundaries ; lf = log frequency(Google) ; EP = Europarl.Table 1 shows the best results that every methodachieved on the whole training data with differentresources, as well as the results of the best base-line features.7 Firstly, one can observe that the lan-guage model probability (baseline features 4 and 5)performs as good or slightly better than our bestmeasure.
Then the best measure is the one whichcombines different lengths of n-grams (multi-levelmatching, combining unigrams to 5-grams), fol-lowed by baseline feature 12 (percentage of bigrams7 Baseline 1,2: length of the source/target sentence;Baseline features 4,5: LM probability of source/target sentence;Baseline feature 8: average number of translations per sourceword with threshold 0.01, weighted by inverse frequency;Baseline feature 12: percentage of bigrams in quartile 4 of fre-quency of source words in a corpus of the source language.in quartile 4 of frequency), and then Okapi BM25applied to bigrams.
It is worth noticing that compar-ing either the source sentence or the target sentence(against the source/target training data) gives verysimilar results.
However, using Google Ngrams asreference data shows a significantly lower correla-tion.
Also using skip-grams or any of our ?fined-grained?
scoring techniques (see ?2.1.2) did not im-prove the correlation, even if in most cases thesewere as good as the standard version.3.2 Detailed analysis: how measures differEven when methods yield strongly correlated re-sults, differences can be significant.
For example,the correlation between the rankings obtained withthe two best methods (baseline 4 and MLM Eng.)
is0.53.
The methods do not make the same errors.8 Amethod may tend to make a lot of small errors, or onthe contrary, very few but big errors.0 20 40 60 80 100020406080% sentences within error rangerelative rankerror (%)Baseline feature 4MLM EP SpaMLM Google EngBaseline rankingFigure 1: Percentage of best segments within an errorrange.
For every measure, the X axis represents the sen-tences sorted by the difference between the predicted rankand the actual rank (?rank error?
), in such a way that forany (relative) number of sentences x, the y value repre-sents the maximum (relative) rank error for all prior sen-tences: for instance, 80% of the ranks predicted by thesethree measures are at most 40% from the actual rank.Let R and R?
be the actual and predicted ranks9of sentence, respectively.
Compute the difference8This motivates use of supervised learning (but see ?1).9It is worth noticing that ties are taken into account here: two123D = |R?R?|; then relativize to the total number ofsentences (the upper bound for D): D?
= D/N .D?
is the relative rank error.
On ascending sortby D?, the predicted ranks for the first sentencesare closest to their actual rank.
Taking the relativerank error D?j for the sentence at position Mj , oneknows that all ?lower?
sentences (?Mi, Mi ?
Mj)are more accurately assigned (D?i ?
D?j).
Thus, ifthe position is also relativized to the total numbersentences: M ?k = Mk/N , M?k is the proportion ofsentences for which the predicted rank is at worstD?k% from the real rank.
Figure 1 shows the percent-age of sentences withing a rank error range for threegood methods:10 the error distributions are surpris-ingly similar.
A baseline ranking is also represented,which shows the same if all sentences are assignedthe same rank (i.e.
all sentences are considered ofequal quality)11.We have also studied effects of some parameters:?
Taking punctuation into account helps a little;?
Ignoring case gives slightly better results;?
Sentences boundaries significantly improve theperformance;?
Most of the refinements of the local score (fre-quency, IDF, etc.)
do not perform better thanthe basic binary approach.4 Individual measures as featuresIn this section we explain how we obtained the sub-mitted results using supervised learning.4.1 ApproachWe have tested a wide range of regression algo-rithms in order to predict the scores, using theWeka12 toolkit (Hall et al, 2009).
All tests weresentences which are assigned the same score are given the samerank.
The ranking sum is preserved by assigning the averagerank; for instance if s1 > s2 = s3 > s4 the correspondingranks are 1, 2.5, 2.5, 4).10Some are not shown, because the curves were too close.11Remark: the plateaus are due to the ties in the actual ranks:there is one plateau for each score level.
This is not visible onthe predicted rankings because it is less likely that an impor-tant number of sentences have both the same actual rank andthe same predicted rank (whereas they all have the same ?pre-dicted?
rank in the baseline ranking, by definition).12www.cs.waikato.ac.nz/ml/weka ?
l.v., 04/2012.done using the whole training data in a 10 foldscross-validation setting.
The main methods were:?
Linear regression?
Pace regression (Wang and Witten, 2002)?
SVM for regression (Shevade et al, 2000)(SMOreg in Weka)?
Decision Trees for regression (Quinlan, 1992)(M5P in Weka)We have tested several combinations of featuresamong the features provided as baseline and ourmeasures.
The measures were primarily selectedon their individual performance (worst measureswere discarded).
However we also had to take thetime constraint into account, because some measuresrequire a fair amount of computing power and/ormemory and some were not finished early enough.Finally we have also tested several attributes selec-tion methods before applying the learning method,but they did not achieve a better performance.4.2 ResultsTable 2 shows the best results among the config-urations we have tested (expressed using the offi-cial evaluation measures, see (Callison-Burch et al,2012) for details).
These results were obtained usingthe default Weka parameters.In this table, the differ-ent features sets are abbreviated as follows:?
B: Baseline (17 features);?
M1: All measures scores (45 features);?
M2: Only scores obtained using the providedresources (33 features);?
L: Lengths (of source and target sentence, 2features).For every method, the best results were obtainedusing all possible features (baseline and our mea-sures).
The following results can also be observed:?
our measures increase the performance overuse of baseline features only (B+M1 vs. B);?
using an external resource (here Google n-grams) with some of our measures increases theperformance (B+M1 vs. B+M2);124Features Method DeltaAvg Spearman MAE RMSEB SVM 0.398 0.445 0.616 0.761B Pace Reg.
0.399 0.458 0.615 0.757L + M1 SVM 0.401 0.439 0.615 0.764L + M1 Lin.
Reg 0.408 0.441 0.610 0.757B Lin.
Reg.
0.408 0.461 0.614 0.754L + M1 M5P 0.409 0.441 0.610 0.757B + M2 SVM 0.409 0.447 0.605 0.753B + M2 Pace Reg.
0.417 0.466 0.603 0.744B + M2 M5P 0.419 0.472 0.601 0.746L + M1 Pace Reg.
0.426 0.454 0.603 0.751B + M2 Lin.
Reg.
0.428 0.481 0.598 0.740B M5P 0.434 0.487 0.586 0.729B + M1 SVM 0.444 0.489 0.585 0.734B + M1 Pace Reg.
0.453 0.505 0.584 0.724B + M1 Lin.
Reg.
0.456 0.507 0.583 0.724B + M1 M5P 0.457 0.508 0.583 0.724Table 2: Best results on 10-folds cross-validation on thetraining data (sorted by DeltaAvg score).?
the baseline features contribute positively to theperformance (B+M1 vs. L+M1);?
The M5P (Decision trees) method works bestin almost all cases (3 out of 4).Based on these training results, the two systemsthat we used to submit the test data scores were:?
TCD-M5P-resources-only, where scores werepredicted from a model trained using M5P onthe whole training data, taking only the base-line features (B) into account;?
TCD-M5P-all, where scores were predictedfrom a model trained using M5P on the wholetraining data, using all features (B+M1).The TCD-M5P-resources-only submissionranked 5th (among 17) in the ranking task, and5th among 19 (tied with two other systems) inthe scoring task (Callison-Burch et al, 2012).Unfortunately the TCD-M5P-all submission con-tained an error.13 Below are the official resultsfor TCD-M5P-resources-only and the correctedresults for TCD-M5P-all :13In four cases in which Google n-grams formed the refer-ence data, the scores were computed using the wrong language(Spanish instead of English) as the reference.
Since this erroroccured only for the test data (not the training data used to com-pute the model), it made the predictions totally meaningless.Submission DeltaAvg Spearman MAE RMSEresources-only 0.56 0.58 0.68 0.82all 0.54 0.54 0.70 0.84Contrary to previous observations using the train-ing data, these results show a better performancewithout our measures.
We think that this is mainlydue to the high variability of the results dependingon the data, and that the first experiments are moresignificant because cross-validation was used.5 ConclusionIn conclusion, we have shown that the robust ap-proach that we have presented can achieve good re-sults: the best DeltaAvg score reaches 0.40 on thetraining data, when the best supervised approach isat 0.45.
We think that this robust approach com-plements the more fine-grained approach with su-pervised learning: the former is useful in the caseswhere the cost to use the latter is prohibitive.Additionally, it is interesting to see that using ex-ternal data (here the Google N -grams) improves theperformance (when using supervised learning).
Asfuture work, we plan to investigate this questionmore precisely: when does the external data help?What are the differences between using the trainingdata (used to produce the MT engine) and anotherdataset?
How to select such an external data in orderto maximize the performance?
In our unsupervisedframework, is it possible to combine the score ob-tained with the external data with the score obtainedfrom the training data?
Similarly, can we combinescores obtained by comparing the source side andthe target side?AcknowledgmentsThis research is supported by Science FoundationIreland (Grant 07/CE/I1142) as part of the Centre forNext Generation Localisation (www.cngl.ie) fund-ing at Trinity College, University of Dublin.We thank the organizers who have accepted to applya bug-fix (wrong numbering of the sentences) in theofficial results, and for organizing the Shared task.References[Callison-Burch et al2012] Chris Callison-Burch,Philipp Koehn, Christof Monz, Matt Post, Radu125Soricut, and Lucia Specia.
2012.
Findings of the2012 workshop on statistical machine translation.In Proceedings of the Seventh Workshop on Statis-tical Machine Translation, Montreal, Canada, June.Association for Computational Linguistics.
[Cohen et al2003] W.W. Cohen, P. Ravikumar, and S.E.Fienberg.
2003.
A comparison of string distance met-rics for name-matching tasks.
In Proceedings of theIJCAI-2003 Workshop on Information Integration onthe Web (IIWeb-03), pages 73?78.
[Hall et al2009] M. Hall, E. Frank, G. Holmes,B.
Pfahringer, P. Reutemann, and I.H.
Witten.2009.
The weka data mining software: an update.ACM SIGKDD Explorations Newsletter, 11(1):10?18.
[He et al2010] Y.
He, Y. Ma, J. van Genabith, andA.
Way.
2010.
Bridging smt and tm with translationrecommendation.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 622?630.
Association for ComputationalLinguistics.
[Jones et al2000] Karen Sparck Jones, Steve Walker, andStephen E. Robertson.
2000.
A probabilistic modelof information retrieval: development and comparativeexperiments - parts 1 and 2.
Inf.
Process.
Manage.,36(6):779?840.
[Michel et al2011] J.B. Michel, Y.K.
Shen, A.P.
Aiden,A.
Veres, M.K.
Gray, J.P. Pickett, D. Hoiberg,D.
Clancy, P. Norvig, J. Orwant, et al 2011.
Quan-titative analysis of culture using millions of digitizedbooks.
science, 331(6014):176.
[Quinlan1992] J.R. Quinlan.
1992.
Learning with con-tinuous classes.
In Proceedings of the 5th Australianjoint Conference on Artificial Intelligence, pages 343?348.
Singapore.
[Shevade et al2000] S.K.
Shevade, SS Keerthi, C. Bhat-tacharyya, and K.R.K.
Murthy.
2000.
Improvementsto the smo algorithm for svm regression.
Neural Net-works, IEEE Transactions on, 11(5):1188?1193.
[Soricut and Echihabi2010] R. Soricut and A. Echihabi.2010.
Trustrank: Inducing trust in automatic trans-lations via ranking.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Linguis-tics, pages 612?621.
Association for ComputationalLinguistics.
[Specia et al2009] Lucia Specia, Marco Turchi, NicolaCancedda, Marc Dymetman, and Nello Cristianini.2009.
Estimating the sentence-level quality of ma-chine translation systems.
In Proceedings of the 13thConference of the European Association for MachineTranslation, pages 28?35.
[Specia et al2011] L. Specia, N. Hajlaoui, C. Hallett, andW.
Aziz.
2011.
Predicting machine translation ade-quacy.
In Machine Translation Summit XIII, Xiamen,China.
[Wang and Witten2002] Y. Wang and I.H.
Witten.
2002.Modeling for optimal probability prediction.
In Pro-ceedings of the Nineteenth International Conferenceon Machine Learning, pages 650?657.
Morgan Kauf-mann Publishers Inc.126
