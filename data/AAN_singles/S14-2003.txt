Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 17?26,Dublin, Ireland, August 23-24, 2014.SemEval-2014 Task 3: Cross-Level Semantic SimilarityDavid Jurgens, Mohammad Taher Pilehvar, and Roberto NavigliDepartment of Computer ScienceSapienza University of Rome{jurgens,pilehvar,navigli}@di.uniroma1.itAbstractThis paper introduces a new SemEvaltask on Cross-Level Semantic Similarity(CLSS), which measures the degree towhich the meaning of a larger linguisticitem, such as a paragraph, is captured bya smaller item, such as a sentence.
High-quality data sets were constructed for fourcomparison types using multi-stage an-notation procedures with a graded scaleof similarity.
Nineteen teams submitted38 systems.
Most systems surpassed thebaseline performance, with several attain-ing high performance for multiple com-parison types.
Further, our results showthat comparisons of semantic representa-tion increase performance beyond what ispossible with text alone.1 IntroductionGiven two linguistic items, semantic similaritymeasures the degree to which the two items havethe same meaning.
Semantic similarity is an es-sential component of many applications in Nat-ural Language Processing (NLP), and similaritymeasurements between all types of text as wellas between word senses lend themselves to a va-riety of NLP tasks such as information retrieval(Hliaoutakis et al., 2006) or paraphrasing (Glick-man and Dagan, 2003).Semantic similarity evaluations have largely fo-cused on comparing similar types of lexical items.Most recently, tasks in SemEval (Agirre et al.,2012) and *SEM (Agirre et al., 2013) have intro-duced benchmarks for measuring Semantic Tex-tual Similarity (STS) between similar-sized sen-tences and phrases.
Other data sets such as thatThis work is licensed under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/of Rubenstein and Goodenough (1965) measuresimilarity between word pairs, while the data setsof Navigli (2006) and Kilgarriff (2001) offer a bi-nary similar-dissimilar distinction between senses.Notably, all of these evaluations have focused oncomparisons between a single type, in contrast toapplication-based evaluations such as summariza-tion and compositionality which incorporate tex-tual items of different sizes, e.g., measuring thequality of a paragraph?s sentence summarization.Task 3 introduces a new evaluation where sim-ilarity is measured between items of differenttypes: paragraphs, sentences, phrases, words andsenses.
Given an item of the lexically-larger type,a system measures the degree to which the mean-ing of the larger item is captured in the smallertype, e.g., comparing a paragraph to a sentence.We refer to this task as Cross-Level Semantic Sim-ilarity (CLSS).
A major motivation of this taskis to produce semantic similarity systems that areable to compare all types of text, thereby free-ing downstream NLP applications from needing toconsider the type of text being compared.
Task 3enables assessing the extent to which the mean-ing of the sentence ?do u know where i can watchfree older movies online without download??
iscaptured in the phrase ?streaming vintage moviesfor free?, or how similar is ?circumscribe?
to thephrase ?beating around the bush.?
Furthermore,by incorporating comparisons of a variety of itemsizes, Task 3 unifies in a single task multiple ob-jectives from different areas of NLP such as para-phrasing, summarization, and compositionality.Because CLSS generalizes STS to items of dif-ferent types, successful CLSS systems can directlybe applied to all STS-based applications.
Fur-thermore, CLSS systems can be used in othersimilarity-based applications such as text simpli-fication (Specia et al., 2012), keyphrase iden-tification (Kim et al., 2010), lexical substitu-tion (McCarthy and Navigli, 2009), summariza-17tion (Sp?arck Jones, 2007), gloss-to-sense mapping(Pilehvar and Navigli, 2014b), and modeling thesemantics of multi-word expressions (Marelli etal., 2014) or polysemous words (Pilehvar and Nav-igli, 2014a).Task 3 was designed with three main objectives.First, the task should include multiple types ofcomparison in order to assess each type?s difficultyand whether specialized resources are needed foreach.
Second, the task should incorporate textfrom multiple domains and writing styles to en-sure that system performance is robust across texttypes.
Third, the similarity methods should be ableto operate at the sense level, thereby potentiallyuniting text- and sense-based similarity methodswithin a single framework.2 Task Description2.1 ObjectiveTask 3 is intended to serve as an initial task forevaluating the capabilities of systems at measuringall types of semantic similarity, independently ofthe size of the text.
To accomplish this objective,systems were presented with items from four com-parison types: (1) paragraph to sentence, (2) sen-tence to phrase, (3) phrase to word, and (4) word tosense.
Given a pair of items, a system must assessthe degree to which the meaning of the larger itemis captured in the smaller item.
WordNet 3.0 waschosen as the sense inventory (Fellbaum, 1998).2.2 Rating ScaleFollowing previous SemEval tasks (Agirre et al.,2012; Jurgens et al., 2012), Task 3 recognizes thattwo items?
similarity may fall within a range ofsimilarity values, rather than having a binary no-tion of similar or dissimilar.
Initially a six-point(0?5) scale similar to that used in the STS taskswas considered (Agirre et al., 2012); however, an-notators found difficulty in deciding between thelower-similarity options.
After multiple revisionsand feedback from a group of initial annotators,we developed a five-point Likert scale for rating apair?s similarity, shown in Table 1.1The scale was designed to systematically ordera broad range of semantic relations: synonymy,similarity, relatedness, topical association, and un-relatedness.
Because items are of different sizes,the highest rating is defined as very similar rather1Annotation materials along with all training and testdata are available on the task website http://alt.qcri.org/semeval2014/task3/.than identical to allow for some small loss in theoverall meaning.
Furthermore, although the scaleis designed as a Likert scale, annotators were givenflexibility when rating items to use values betweenthe defined points in the scale, indicating a blendof two relations.
Table 2 provides examples ofpairs for each scale rating for all four comparisontype.3 Task DataThough several data sets exist for STS and com-paring words and senses, no standard data set ex-ists for CLSS.
Therefore, we created a pilot dataset designed to test the capabilities of systems in avariety of settings.
The task data for all compar-isons but word-to-sense was created using a three-phase process.
First, items of all sizes were se-lected from publicly-available data sets.
Second,the selected items were used to produce a seconditem of the next-smaller level (e.g., a sentence in-spires a phrase).
Third, the pairs of items wereannotated for their similarity.
Because of the ex-pertise required for working with word senses, theword-to-sense data set was constructed by the or-ganizers using a separate but similar process.
Inthe training and test data, each comparison typehad 500 annotated examples, for a total of 2000pairs each for training and test.
We first describethe corpora used by Task 3 followed by the anno-tation process.
We then describe the constructionof the word-to-sense data set.3.1 CorporaTest and training data were constructed by draw-ing from multiple publicly-available corpora andthen manually generating a paired item for com-parison.
To achieve our second objective for thetask, the data sets used to create item pairs in-cluded texts from specific domains, social media,and text with idiomatic or slang language.
Table3 summarizes the corpora and their distributionacross the test and training sets for each compari-son type, with a high-level description of the genreof the data.
We briefly describe the corpora next.The WikiNews, Reuters 21578, and MicrosoftResearch (MSR) Paraphrase corpora are all drawnfrom newswire text, with WikiNews being au-thored by volunteer writers and the latter two cor-pora written by professionals.
Travel Guides wasdrawn from the Berlitz travel guides data in theOpen American National Corpus (Ide and Suder-man, 2004) and includes very verbose sentences184 ?
VerySimilarThe two items have very similar meanings and the most important ideas, concepts, or actions in the largertext are represented in the smaller text.
Some less important information may be missing, but the smallertext is a very good summary of the larger text.3 ?
SomewhatSimilarThe two items share many of the same important ideas, concepts, or actions, but include slightly differentdetails.
The smaller text may use similar but not identical concepts (e.g., car vs. vehicle), or may omit afew of the more important ideas present in the larger text.2 ?
Somewhatrelated but notsimilarThe two items have dissimilar meaning, but share concepts, ideas, and actions that are related.
The smallertext may use related but not necessarily similar concepts (window vs. house) but should still share someoverlapping concepts, ideas, or actions with the larger text.1 ?
SlightlyrelatedThe two items describe dissimilar concepts, ideas and actions, but may share some small details or domainin common and might be likely to be found together in a longer document on the same topic.0 ?
Unrelated The two items do not mean the same thing and are not on the same topic.Table 1: The five-point Likert scale used to rate the similarity of item pairs.
See Table 2 for examples.with many named entities.
Wikipedia Sciencewas drawn from articles tagged with the cate-gory Science on Wikipedia.
Food reviews weredrawn from the SNAP Amazon Fine Food Re-views data set (McAuley and Leskovec, 2013)and are customer-authored reviews for a variety offood items.
Fables were taken from a collection ofAesop?s Fables.
The Yahoo!
Answers corpus wasderived from the Yahoo!
Answers data set, whichis a collection of questions and answers from theCommunity Question Answering (CQA) site; thedata set is notable for having the highest degree ofungrammaticality in our test set.
SMT Europarlis a collection of texts from the English-languageproceedings of the European parliament (Koehn,2005); Europarl data was also used in the PPDBcorpus (Ganitkevitch et al., 2013), from whichphrases were extracted.
Wikipedia was used togenerate two phrase data sets from (1) extractingthe definitional portion of an article?s initial sen-tence, e.g., ?An [article name] is a [definition],?and (2) captions for an article?s images.
Webqueries were gathered from online sources of real-world queries.
Last, the first and second authorsgenerated slang and idiomatic phrases based onexpressions contained in Wiktionary.For all comparison types, the test data includedone genre that was not seen in the training datain order to test the generalizability of the systemson data from a novel domain.
In addition, weincluded a new type of challenge genre with Fa-bles; unlike other domains, the sentences pairedwith the fable paragraphs were potentially seman-tic interpretations of the intent of the fable, i.e.,the moral of the story.
These interpretations oftenhave little textual overlap with the fable itself andrequire a deeper interpretation of the paragraph?smeaning in order to make the correct similarityjudgment.Prior to the annotation process, all content wasfiltered to ensure its size and format matched thedesired text type.
By average, a paragraph in ourdataset consists of 3.8 sentences.
Typos and gram-matical mistakes in the community-produced con-tent were left unchanged.3.2 Annotation ProcessA two-phase process was used to produce the testand training data sets for all but word-to-sense.Phase 1 generates the item pairs from source textsand Phase 2 rates the pairs?
similarity.Phase 1 In this phase, annotators were shown thelarger text of a comparison type and then askedto produce the smaller text of the pair at a spec-ified similarity; for example an annotator may beshown a paragraph and asked to write a sentencethat is a ?3?
rating.
Annotators were instructed toleave the smaller text blank if they had difficultyunderstanding the larger text.The requested similarity ratings were balancedto create a uniform distribution of similarity val-ues.
Annotators were asked only to generate rat-ings of 1?4; pairs with a ?0?
rating were automat-ically created by pairing the larger item with ran-dom selections of text of the appropriate size fromthe same corpus.
The intent of Phase 1 is to pro-duce varied item pairs with an expected uniformdistribution of similarity values along the ratingscale.Four annotators participated in Phase 1 andwere paid a bulk rate of e110 for completing thework.
In addition to the four annotators, the firsttwo organizers also assisted in Phase 1: Both com-pleted items from the SCIENTIFIC genre and thefirst organizer produced 994 pairs, including all19PARAGRAPH TO SENTENCEParagraph: Teenagers take aerial shots of their neigh-bourhood using digital cameras sitting in old bottles whichare launched via kites - a common toy for children liv-ing in the favelas.
They then use GPS-enabled smart-phones to take pictures of specific danger points - such asrubbish heaps, which can become a breeding ground formosquitoes carrying dengue fever.Rating Sentence4 Students use their GPS-enabled cellphones totake birdview photographs of a land in orderto find specific danger points such as rubbishheaps.3 Teenagers are enthusiastic about taking aerialphotograph in order to study their neighbour-hood.2 Aerial photography is a great way to identifyterrestrial features that aren?t visible from theground level, such as lake contours or riverpaths.1 During the early days of digital SLRs, Canonwas pretty much the undisputed leader inCMOS image sensor technology.0 Syrian President Bashar al-Assad tells the USit will ?pay the price?
if it strikes against Syria.SENTENCE TO PHRASESentence: Schumacher was undoubtedly one of the verygreatest racing drivers there has ever been, a man who wasroutinely, on every lap, able to dance on a limit accessibleto almost no-one else.Rating Phrase4 the unparalleled greatness of Schumacher?sdriving abilities3 driving abilities2 formula one racing1 north-south highway0 orthodontic insurancePHRASE TO WORDPhrase: loss of air pressure in a tireRating Word4 flat-tire3 deflation2 wheel1 parking0 butterflyWORD TO SENSEWord: automobilenRating Sense4 car1n(a motor vehicle with four wheels; usuallypropelled by an internal combustion engine)3 vehicle1n(a conveyance that transports peopleor objects)2 bike1n(a motor vehicle with two wheels and astrong frame)1 highway1n(a major road for any form of motortransport)0 pen1n(a writing implement with a point fromwhich ink flows)Table 2: Example pairs and their ratings.those for the METAPHORIC genre, and those thatthe other annotators left blank.Phase 2 Here, the item pairs produced in Phase1 were rated for their similarity according to thescale described in Section 2.2.
An initial pilotstudy showed that crowdsourcing was only mod-erately effective for producing these ratings withhigh agreement.
Furthermore, the texts used inTask 3 came from a variety of genres, such asscientific domains, which some workers had dif-ficulty understanding.
While we note that crowd-sourcing has been used in prior STS tasks forgenerating similarity scores (Agirre et al., 2012;Agirre et al., 2013), both tasks?
efforts encoun-tered lower worker score correlations on some por-tions of the dataset (Diab, 2013), suggesting thatcrowdsourcing may not be reliable for judging thesimilarity of certain types of text.
See Section 3.5for additional details.Therefore, to ensure high quality, the first twoorganizers rated all items independently.
Becausethe sentence-to-phrase and phrase-to-word com-parisons contain slang and idiomatic language, athird American English mother tongue annotatorwas added for those data sets.
The third annotatorwas compensated e250 for their assistance.Annotators were allowed to make finer-graineddistinctions in similarity using multiples of 0.25.For all items, when any two annotators disagreedby one or more scale points, we performed anadjudication to determine the item?s rating in thegold standard.
The adjudication process revealedthat nearly all disagreements were due to annota-tor mistakes, e.g., where one annotator had over-looked a part of the text or had misunderstood thetext?s meaning.
The final similarity rating for anunadjudicated item was the average of its ratings.3.3 Word-to-SenseWord-to-sense comparison items were generatedin three phases.
To increase the diversity andchallenge of the data set, the word-to-sense wascreated for four types of words: (1) a word andits intended meaning are in WordNet, (2) a wordwas not in the WordNet vocabulary, e.g., the verb?zombify,?
(3) the word is in WordNet, but has anovel meaning that is not in WordNet, e.g., the ad-jective ?red?
referring to Communist, and (4) a setof challenge words where one of the word?s sensesand a second sense are directly connected by anedge in the WordNet network, but the two sensesare not always highly similar.20Paragraph-to-Sentence Sentence-to-Phrase Phrase-to-WordCorpus Genre Train Test Train Test Train TestWikiNews Newswire 15.0 10.0 9.2 6.0Reuters 21578 Newswire 20.2 15.0 5.0Travel Guides Travel 15.2 10.0 15.0 9.8Wikipedia Science Scientific ?
25.6 ?
14.8Food Reviews Review 19.6 20.0Fables Metaphoric 9.0 5.2Yahoo!
Answers CQA 21.0 14.2 17.6 17.4SMT Europarl Newswire 35.4 14.4MSR Paraphrase Newswire 10.0 10.0 8.8 6.0Idioms Idiomatic 12.8 12.6 20.0 20.0Slang Slang ?
15.0 ?
25.0PPDB Newswire 10.0 10.0Wikipedia Glosses Lexicographic 28.2 17.0Wikipedia Image Captions Descriptive 23.0 17.0Web Search Queries Search 5.0 5.0Table 3: Percentages of the training and test data per source corpus.In Phase 1, to select the first type of word,lemmas in WordNet were ranked by frequencyin Wikipedia; the ranking was divided into tenequally-sized groups, with words sampled evenlyfrom groups in order to control for word frequencyin the task data.
For the second type, words notpresent in WordNet were drawn from two sources:examining words in Wikipedia, which we referto as out-of-vocabulary (OOV), and slang words.For the third type, to identify words with a novelsense, we examined Wiktionary entries and chosenovel, salient senses that were distinct from thosein WordNet.
We refer to words with a novel mean-ing as out-of-sense (OOS).
Words of the fourthtype were chosen by hand.
The part-of-speech dis-tributions for all four types of items were balancedas 50% noun, 25% verb, 25% adjective.In Phase 2, each word was associated with aparticular WordNet sense for its intended mean-ing, or the closest available sense in WordNetfor OOV or OOS items.
To select a comparisonsense, we adopted a neighborhood search proce-dure: All synsets connected by at most three edgesin the WordNet semantic network were shown.Given a word and its neighborhood, the corre-sponding sense for the item pair was selected bymatching the sense with an intended similarity forthe pair, much like how text items were gener-ated in Phase 1.
The reason behind using thisneighborhood-based selection process was to min-imize the potential bias of consistently selectinglower-similarity items from those further away inthe WordNet semantic network.In Phase 3, given all word-sense pairs, annota-tors were shown the definitions associated with theintended meaning of the word and of the sense.Definitions were drawn from WordNet or fromWiktionary, if the word was OOV or OOS.
An-notators had access to the WordNet structure forthe compared sense in order to take into accountits parents and siblings.3.4 Trial DataThe trial data set was created using a separateprocess.
Source text was drawn from WikiNews;we selected the text for the larger item of eachlevel and then generated the text or sense of thesmaller.
A total of 156 items were produced.After, four fluent annotators independently ratedall items.
Inter-annotator agreement rates variedin 0.734?0.882, using Krippendorff?s ?
(Krippen-dorff, 2004) on the interval scale.3.5 Data Set DiscussionThe resulting annotation process produced a high-quality data set.
First, Table 4 shows the inter-annotator agreement (IAA) statistics for eachcomparison type on both the full and unadjudi-cated portions of the data set.
IAA was measuredusing Krippendorff?s ?
for interval data.
Becausethe disagreements that led to lower ?
in the fulldata were resolved via adjudication, the quality ofthe full data set is expected to be on par with thatof the unadjudicated data.
The annotation qualityfor Task 3 was further improved by manually ad-judicating all significant disagreements.In contrast, the data sets of current STS tasksaggregated data from annotators with moderatecorrelation with each other (Diab, 2013); STS-2012 (Agirre et al., 2012) saw inter-annotatorPearson correlations of 0.530?0.874 per data setand STS-2013 (Agirre et al., 2013) had average21Training TestData All Unadj.
All Unadj.Para.-to-Sent.
0.856 0.916 0.904 0.971Sent.-to-Phr.
0.773 0.913 0.766 0.980Phr.-to-Word 0.735 0.895 0.730 0.988Word-to-Sense 0.681 0.895 0.655 0.952Table 4: IAA rates for the task data.00.511.522.533.54100 200 300 400 500ScoringscaleParagraph-to-SentenceTrainingTest00.511.522.533.54100 200 300 400 500Sentence-to-PhraseTrainingTest00.511.522.533.54100 200 300 400 500ScoringscalePhrase-to-WordTrainingTest00.511.522.533.54100 200 300 400 500Word-to-SenseTrainingTestFigure 1: Similarity ratings distributions.inter-annotator correlations of 0.377?0.832.
How-ever, we note that Pearson correlation and Krip-pendorff?s ?
are not directly comparable (Artsteinand Poesio, 2008), as annotators?
scores may becorrelated, but completely disagree.Second, the two-phase construction processproduced values that were evenly distributedacross the rating scale, shown in Figure 1 as thedistribution of the values for all data sets.
How-ever, we note that this creation procedure was veryresource intensive and, therefore, semi-automatedor crowdsourcing-based approaches for produc-ing high-quality data will be needed to expandthe size of the data in future CLSS-based eval-uations.
Nevertheless, as a pilot task, the man-ual effort was essential for ensuring a rigorously-constructed data set for the initial evaluation.4 EvaluationParticipation The ultimate goal of Task 3 is toproduce systems that can measure similarity formultiple types of items.
Therefore, we stronglyencouraged participating teams to submit systemsthat were capable of generating similarity judg-ments for multiple comparison types.
However,to further the analysis, participants were also per-mitted to submit systems specialized to a singledomain.
Teams were allowed at most three systemsubmissions, regardless of the number of compar-ison types supported.Scoring Systems were required to provide sim-ilarity values for all items within a comparisontype.
Following prior STS evaluations, systemswere scored for each comparison type using Pear-son correlation.
Additionally, we include a secondscore using Spearman?s rank correlation, which isonly affected by differences in the ranking of itemsby similarity, rather than differences in the similar-ity values.
Pearson correlation was chosen as theofficial evaluation metric since the goal of the taskis to produce similar scores.
However, Spearman?srank correlation provides an important metric forassessing systems whose scores do not match hu-man scores but whose rankings might, e.g., string-similarity measures.
Ultimately, a global rankingwas produced by ordering systems by the sum oftheir Pearson correlation values for each of thefour comparison levels.Baselines The official baseline system wasbased on the Longest Common Substring (LCS),normalized by the length of items using themethod of Clough and Stevenson (2011).
Givena pair, the similarity is reported as the normalizedlength of the LCS.
In the case of word-to-sense,the LCS for a word-sense pair is measured be-tween the sense?s definition in WordNet and thedefinitions of each sense of the pair?s word, report-ing the maximal LCS.
Because OOV and slangwords are not in WordNet, the baseline reports theaverage similarity value of non-OOV items.
Base-line scores were made public after the evaluationperiod ended.Because LCS is a simple procedure, a secondbaseline based on Greedy String Tiling (GST)(Wise, 1996) was added after the evaluation pe-riod concluded.
Unlike LCS, GST better handlesthe transpositions of tokens across the two textsand can still report high similarity when encoun-tering reordered text.
The minimum match lengthfor GST was set to 6.5 ResultsNineteen teams submitted 38 systems.
Of thosesystems, 34 produced values for paragraph-to-sentence and sentence-to-phrase comparisons, 22for phrase-to-word, and 20 for word-to-sense.Two teams submitted revised scores for their sys-tems after the deadline but before the test set had22Team System Para-2-Sent Sent-2-Phr Phr-2-Word Word-2-Sense Official Rank Spearman RankMeerkat Mafia pairingWords?
0.794 0.704 0.457 0.389SimCompass run1 0.811 0.742 0.415 0.356 1 1ECNU run1 0.834 0.771 0.315 0.269 2 2UNAL-NLP run2 0.837 0.738 0.274 0.256 3 6SemantiKLUE run1 0.817 0.754 0.215 0.314 4 4UNAL-NLP run1 0.817 0.739 0.252 0.249 5 7UNIBA run2 0.784 0.734 0.255 0.180 6 8RTM-DCU run1?
0.845 0.750 0.305UNIBA run1 0.769 0.729 0.229 0.165 7 10UNIBA run3 0.769 0.729 0.229 0.165 8 11BUAP run1 0.805 0.714 0.162 0.201 9 13BUAP run2 0.805 0.714 0.142 0.194 10 9Meerkat Mafia pairingWords 0.794 0.704 -0.044 0.389 11 12HULTECH run1 0.693 0.665 0.254 0.150 12 16GST Baseline 0.728 0.662 0.146 0.185HULTECH run3 0.669 0.671 0.232 0.137 13 15RTM-DCU run2?
0.785 0.698 0.221RTM-DCU run3 0.780 0.677 0.208 14 17HULTECH run2 0.667 0.633 0.180 0.169 15 14RTM-DCU run1 0.786 0.666 0.171 16 18RTM-DCU run3?
0.786 0.663 0.171Meerkat Mafia SuperSaiyan 0.834 0.777 17 19Meerkat Mafia Hulk2 0.826 0.705 18 20RTM-DCU run2 0.747 0.588 0.164 19 22FBK-TR run3 0.759 0.702 20 23FBK-TR run1 0.751 0.685 21 24FBK-TR run2 0.770 0.648 22 25Duluth Duluth2 0.501 0.450 0.241 0.219 23 21AI-KU run1 0.732 0.680 24 26LCS Baseline 0.527 0.562 0.165 0.109UNAL-NLP run3 0.708 0.620 25 27AI-KU run2 0.698 0.617 26 28TCDSCSS run2 0.607 0.552 27 29JU-Evora run1 0.536 0.442 0.090 0.091 28 31TCDSCSS run1 0.575 0.541 29 30Duluth Duluth1 0.458 0.440 0.075 0.076 30 5Duluth Duluth3 0.455 0.426 0.075 0.079 31 3OPI run1 0.433 0.213 0.152 32 36SSMT run1 0.789 33 34DIT run1 0.785 34 32DIT run2 0.784 35 33UMCC DLSI SelSim run1 0.760 36 35UMCC DLSI SelSim run2 0.698 37 37UMCC DLSI Prob run1 0.023 38 38Table 5: Task results.
Systems marked with a ?
were submitted after the deadline but are positionedwhere they would have ranked.been released.
These systems were scored andnoted in the results but were not included in theofficial ranking.Table 5 shows the performance of the participat-ing systems across all the four comparison types interms of Pearson correlation.
The two right-mostcolumns show system rankings by Pearson (Offi-cial Rank) and Spearman?s ranks correlation.The SimCompass system attained first place,partially due to its superior performance onphrase-to-word comparisons, providing an im-provement of 0.10 over the second-best sys-tem.
The late-submitted version of the MeerkatMafia pairingWords?
system corrected a bug inthe phrase-to-word comparison, which ultimatelywould have attained first place due to large per-formance improvements over SimCompass onphrase-to-word and word-to-sense.
ENCU andUNAL-NLP systems rank respectively second andthird while the former being always in top-4 andthe latter being among the top-7 systems across thefour comparison types.
Most systems were ableto surpass the naive LCS baseline; however, themore sophisticated GST baseline (which accountsfor text transposition) outperforms two-thirds ofthe systems.
Importantly, both baselines perform23poorly on smaller text, highlighting the impor-tance of performing a semantic comparison, as op-posed to a string-based one.Within the individual comparison types, spe-cialized systems performed well for the largertext sizes.
In the paragraph-to-sentence type, therun1 system of UNAL-NLP provides the best of-ficial result, with the late RTM-DCU run1?
sys-tem surpassing its performance slightly.
MeerkatMafia provides the best performance in sentence-to-phrase with its SuperSaiyan system and thebest performances in phrase-to-word and word-to-sense with its late pairingWords?
system.Comparison-Type Analysis Performanceacross the comparison types varied considerably,with systems performing best on comparisonsbetween longer textual items.
As a general trend,both the baselines?
and systems?
performancestend to decrease with the size of lexical itemsin the comparison types.
A main contributingfactor to this is the reliance on textual similaritymeasures (such as the baselines), which performwell when two items?
may share content.
How-ever, as the items?
content becomes smaller, e.g.,a word or phrase, the textual similarity does notnecessarily provide a meaningful indication ofthe semantic similarity between the two.
Thisperformance discrepancy suggests that, in orderto perform well, CLSS systems must rely oncomparisons between semantic representationsrather than textual representations.
The twotop-performing systems on these smaller levels,Meerkat Mafia and SimCompass, used additionalresources beyond WordNet to expand a word orsense to its definition or to represent words withdistributional representations.Per-genre results and discussions Task 3 in-cludes multiple genres within the data set for eachcomparison type.
Figure 2 shows the correlationof each system for each of these genres, with sys-tems ordered left to right according to their officialranking in Table 5.
An interesting observation isthat a system?s official rank does not always matchthe rank from aggregating its correlations for eachgenre individually.
This difference suggests thatsome systems provided good similarity judgmentson individual genres, but their range of similarityvalues was not consistent between genres leadingto lower overall Pearson correlation.
For instance,in the phrase-to-word comparison type, the ag-gregated per-genre performance of Duluth-1 andDuluth-3 are among the best whereas their over-all Pearson performance puts these systems amongthe worst-performing ones in the comparison type.Among the genres, CQA, SLANG, and ID-IOMATIC prove to be the more difficult for sys-tems to interpret and judge.
These genres in-cluded misspelled, colloquial, or slang languagewhich required converting the text into semanticform in order to meaningfully compare it.
Fur-thermore, as expected, the METAPHORIC genrewas the most difficult, with no system perform-ing well; we view the METAPHORIC genre as anopen challenge for future systems to address wheninterpreting larger text.
On the other hand, SCI-ENTIFIC, TRAVEL, and NEWSWIRE tend to bethe easiest genres for paragraph-to-sentence andsentence-to-phrase.
All three genres tend to in-clude many named entities or highly-specific lan-guage, which are likely to be more preserved in themore-similar paired items.
Similarly, DESCRIP-TIVE and SEARCH genres were easiest in phrase-to-word, which also often featured specific wordsthat were preserved in highly-similar pairs.
Inthe case of word-to-sense, REGULAR proves to bethe least difficult genre.
Interestingly, in word-to-sense, most systems attained moderate perfor-mance for comparisons with words not in Word-Net (i.e., OOV) but had poor performance forslang words, which were also OOV.
This differ-ence suggests that systems could be improved withadditional semantic resources for slang.Spearman Rank Analysis Although the goal ofTask 3 is to have systems produce similarity judg-ments, some applications may benefit from simplyhaving a ranking of pairs, e.g., ranking summa-rizations by goodness.
The Spearman rank corre-lation measures the ability of systems to performsuch a ranking.
Surprisingly, with the Spearman-based ranking, the Duluth1 and Duluth3 systemsattain the third and fifth ranks ?
despite beingamong the lowest ranked with Pearson.
Both sys-tems were unsupervised and produced similarityvalues that did not correlate well with those ofhumans.
However, their Spearman ranks demon-strate the systems ability to correctly identify rela-tive similarity and suggests that such unsupervisedsystems could improve their Pearson correlationby using the training data to tune the range of sim-ilarity values to match those of humans.24012345UNAL-NLP-2ECNU-1Meerkat_Mafia-SSMeerkat_Mafia-HSemantiKLUE-1UNAL-NLP-1SimCompass-1BUAP-1BUAP-2Meerkat_Mafia-PWSSMT-1RTM-DCU-1DIT-1DIT-2UNIBA-2RTM-DCU-3FBK-TR-2UNIBA-1UNIBA-3FBK-TR-3FBK-TR-1RTM-DCU-2AI-KU-1UNAL-NLP-3AI-KU-2HULTECH-1HULTECH-3HULTECH-2TCDSCSS-2TCDSCSS-1JU-Evora-1Duluth-2Duluth-1Duluth-3(a)Paragraph-to-SentenceCorrelationsper genreCQAReviewTravelNewswireScientificMetaphoric012345Meerkat_Mafia-SSECNU-1UMCC_DLSI_SelSim-1SemantiKLUE-1SimCompass-1UNAL-NLP-1UNAL-NLP-2UNIBA-2UNIBA-1UNIBA-3BUAP-1BUAP-2Meerkat_Mafia-HMeerkat_Mafia-PWFBK-TR-3UMCC_DLSI_SelSim-2FBK-TR-1AI-KU-1RTM-DCU-3HULTECH-3RTM-DCU-1HULTECH-1FBK-TR-2HULTECH-2UNAL-NLP-3AI-KU-2RTM-DCU-2TCDSCSS-2TCDSCSS-1Duluth-2JU-Evora-1Duluth-1OPI-1Duluth-3(b)Sentence-to-PhraseCorrelationsper genreScientificCQAIdiomaticSlangTravelNewswire0123SimCompass-1ECNU-1UNAL-NLP-2UNIBA-2HULTECH-1UNAL-NLP-1Duluth-2HULTECH-3UNIBA-1UNIBA-3SemantiKLUE-1OPI-1RTM-DCU-3HULTECH-2RTM-DCU-1RTM-DCU-2BUAP-1BUAP-2JU-Evora-1Duluth-1Duluth-3Meerkat_Mafia-PW(c)Phrase-to-WordCorrelationsper genreSlangNewswireIdiomaticDescriptiveLexicographicSearch012Meerkat_Mafia-PWSimCompass-1SemantiKLUE-1ECNU-1UNAL-NLP-2UNAL-NLP-1Duluth-2BUAP-1BUAP-2UNIBA-2HULTECH-2UNIBA-1UNIBA-3OPI-1HULTECH-1HULTECH-3JU-Evora-1Duluth-3Duluth-1UMCC_DLSI_Prob-1(d)Word-to-SenseCorrelationsper genreOOSOOVregularregular-challengeSlangFigure 2: A stacked histogram for each system, showing its Pearson correlations for genre-specific por-tions of the gold-standard data, which may also be negative.6 ConclusionThis paper introduces a new similarity task, Cross-Level Semantic Similarity, for measuring the se-mantic similarity of lexical items of differentsizes.
Using a multi-phase annotation proce-dure, we have produced a high-quality data set of4000 items comprising of various genres, evenly-split between training and test with four types ofcomparison: paragraph-to-sentence, sentence-to-phrase, phrase-to-word, and word-to-sense.
Nine-teen teams submitted 38 systems, with most teamssurpassing the baseline system and several sys-tems achieving high performance in multiple typesof comparison.
However, a clear performancetrend emerged where systems perform well onlywhen the text itself is similar, rather than its under-lying meaning.
Nevertheless, the results of Task 3are highly encouraging and point to clear futureobjectives for developing CLSS systems that op-erate on more semantic representations rather thantext.
In future work on CLSS evaluation, we firstintend to develop scalable annotation methods toincrease the data sets.
Second, we plan to add newevaluations where systems are tested according totheir performance in an application related to eachcomparison-type, such as measuring the quality ofa paraphrase or summary.AcknowledgmentsWe would like to thank Tiziano Flati, Marc Franco Salvador,Maud Erhmann, and Andrea Moro for their help in preparingthe trial data; Gaby Ford, Chelsea Smith, and Eve Atkinsonfor their help in generating the training and test data; andAmy Templin for her help in generating and rating the train-ing and test data.The authors gratefully acknowledge thesupport of the ERC Starting Grant Multi-JEDI No.
259234.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre.
2012.
SemEval-2012 task 6: A pilot on semantictextual similarity.
In Proceedings of the 6th InternationalWorkshop on Semantic Evaluation (SemEval-2012), pages385?393, Montr?eal, Canada.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 Shared Task:Semantic textual similarity, including a pilot on typed-similarity.
In Proceedings of the Second Joint Confer-25ence on Lexical and Computational Semantics (*SEM),Atlanta, Georgia.Ron Artstein and Massimo Poesio.
2008.
Inter-coder agree-ment for computational linguistics.
Computational Lin-guistics, 34(4):555?596.Paul Clough and Mark Stevenson.
2011.
Developing a cor-pus of plagiarised short answers.
Language Resourcesand Evaluation, 45(1):5?24.Mona Diab.
2013.
Semantic textual similarity: past presentand future.
In Joint Symposium on Semantic Process-ing.
Keynote address.
http://jssp2013.fbk.eu/sites/jssp2013.fbk.eu/files/Mona.pdf.Christiane Fellbaum, editor.
1998.
WordNet: An ElectronicDatabase.
MIT Press, Cambridge, MA.Juri Ganitkevitch, Benjamin Van Durme, and Chris Callison-Burch.
2013.
PPDB: The paraphrase database.
In Pro-ceedings of the 2013 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics: Human Language Technologies (NAACL-HLT),pages 758?764, Atlanta, Georgia.Oren Glickman and Ido Dagan.
2003.
Acquiring lexicalparaphrases from a single corpus.
In Proceedings of theInternational Conference on Recent Advances in NaturalLanguage Processing (RANLP), pages 81?90, Borovets,Bulgaria.Angelos Hliaoutakis, Giannis Varelas, Epimenidis Voutsakis,Euripides GM Petrakis, and Evangelos Milios.
2006.Information retrieval by semantic similarity.
Interna-tional Journal on Semantic Web and Information Systems,2(3):55?73.Nancy Ide and K. Suderman.
2004.
The American Na-tional Corpus First Release.
In Proceedings of the 4thLanguage Resources and Evaluation Conference (LREC),pages 1681?1684, Lisbon, Portugal.David Jurgens, Saif Mohammad, Peter Turney, and KeithHolyoak.
2012.
SemEval-2012 Task 2: MeasuringDegrees of Relational Similarity.
In Proceedings ofthe 6th International Workshop on Semantic Evaluation(SemEval-2012), pages 356?364, Montr?eal, Canada.Adam Kilgarriff.
2001.
English lexical sample task de-scription.
In The Proceedings of the Second InternationalWorkshop on Evaluating Word Sense Disambiguation Sys-tems (SENSEVAL-2), pages 17?20, Toulouse, France.Su Nam Kim, Olena Medelyan, Min-Yen Kan, and Timo-thy Baldwin.
2010.
SemEval-2010 Task 5: AutomaticKeyphrase Extraction from Scientific Articles.
In Pro-ceedings of the 5th International Workshop on SemanticEvaluation (SemEval-2010), pages 21?26, Los Angeles,California.Philipp Koehn.
2005.
Europarl: A parallel corpus for sta-tistical machine translation.
In Proceedings of MachineTranslation Summit X, pages 79?86, Phuket, Thailand.Klaus Krippendorff.
2004.
Content Analysis: An Introduc-tion to Its Methodology.
Sage, Thousand Oaks, CA, sec-ond edition.Marco Marelli, Stefano Menini, Marco Baroni, Luisa Ben-tivogli, Raffaella Bernardi, and Roberto Zamparelli.
2014.SemEval-2014 Task 1: Evaluation of compositional dis-tributional semantic models on full sentences through se-mantic relatedness and textual entailment.
In Proceedingsof the 8th International Workshop on Semantic Evaluation(SemEval-2014), Dublin, Ireland.Julian John McAuley and Jure Leskovec.
2013.
From ama-teurs to connoisseurs: modeling the evolution of user ex-pertise through online reviews.
In Proceedings of the 22ndInternational Conference on World Wide Web (WWW),pages 897?908, Rio de Janeiro, Brazil.Diana McCarthy and Roberto Navigli.
2009.
The Englishlexical substitution task.
Language Resources and Evalu-ation, 43(2):139?159.Roberto Navigli.
2006.
Meaningful clustering of senseshelps boost Word Sense Disambiguation performance.
InProceedings of the 21st International Conference on Com-putational Linguistics and the 44th Annual Meeting ofthe Association for Computational Linguistics (COLING-ACL), pages 105?112, Sydney, Australia.Mohammad Taher Pilehvar and Roberto Navigli.
2014a.
Alarge-scale pseudoword-based evaluation framework forstate-of-the-art Word Sense Disambiguation.
Computa-tional Linguistics, 40(4).Mohammad Taher Pilehvar and Roberto Navigli.
2014b.A robust approach to aligning heterogeneous lexical re-sources.
In Proceedings of the 52nd Annual Meeting ofthe Association for Computational Linguistics, pages 468?478, Baltimore, USA.Herbert Rubenstein and John B. Goodenough.
1965.
Con-textual correlates of synonymy.
Communications of theACM, 8(10):627?633.Karen Sp?arck Jones.
2007.
Automatic summarising: Thestate of the art.
Information Processing and Management,43(6):1449?1481.Lucia Specia, Sujay Kumar Jauhar, and Rada Mihalcea.2012.
SemEval-2012 Task 1: English Lexical Simplifica-tion.
In Proceedings of the Sixth International Workshopon Semantic Evaluation (SemEval-2012), pages 347?355.Michael J.
Wise.
1996.
YAP3: Improved detection of simi-larities in computer program and other texts.
In Proceed-ings of the twenty-seventh SIGCSE technical symposiumon Computer science education, SIGCSE ?96, pages 130?134, Philadelphia, Pennsylvania, USA.26
