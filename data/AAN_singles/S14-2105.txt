Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 601?604,Dublin, Ireland, August 23-24, 2014.Swiss-Chocolate: Sentiment Detectionusing Sparse SVMs and Part-Of-Speech n-GramsMartin JaggiETH ZurichZ?urich, Switzerlandjaggi@inf.ethz.chFatih Uzdilli and Mark CieliebakZurich University of Applied SciencesWinterthur, Switzerland{ uzdi, ciel } @zhaw.chAbstractWe describe a classifier to predict themessage-level sentiment of English micro-blog messages from Twitter.
This pa-per describes the classifier submitted tothe SemEval-2014 competition (Task 9B).Our approach was to build up on the sys-tem of the last year?s winning approachby NRC Canada 2013 (Mohammad et al.,2013), with some modifications and addi-tions of features, and additional sentimentlexicons.
Furthermore, we used a sparse(`1-regularized) SVM, instead of the morecommonly used `2-regularization, result-ing in a very sparse linear classifier.1 IntroductionWith the immense growth of user generated textonline, the interest in automatic sentiment analy-sis of text has greatly increased recently in bothacademia and industry.In this paper, we describe our approach for amodified SVM based classifier for short text as inTwitter messages.
Our system has participated inthe SemEval-2014 Task 9 competition, ?SentimentAnalysis in Twitter, Subtask?B Message PolarityClassification?
(Rosenthal et al., 2014).
The goalis to classify a tweet (on the full message level)into the three classes positive, negative, and neu-tral.
An almost identical competition was alreadyrun in 2013.Our Results in the Competition.
Our approachwas ranked on the 8th place out of the 50 partici-pating submissions, with an F1-score of 67.54 onthe Twitter-2014 test set.
The 2014 winning teamobtained an average F1-score of 70.96.This work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/(The more detailed rankings of our approachwere 4th rank on the LiveJournal data, 5th on theSMS data (2013), 18th on Twitter-2013, and 16thon Twitter Sarcasm, see (Rosenthal et al., 2014)for full details and all results).Data.
In the competition, the tweets for trainingand development were only provided as tweet IDs.A fraction (10-15%) of the tweets were no longeravailable on twitter, which makes the results of thecompetition not fully comparable.
For testing, inaddition to last years data (tweets and SMS), newtweets and data from a surprise domain were pro-vided.
An overview of the data, which we wereable to download, is shown in Table 1.Table 1: Overview of the data we found availablefor training, development and testing.Dataset Total Positive Negative NeutralTrain (Tweets) 8224 3058 1210 3956Dev (Tweets) 1417 494 286 637Test: Twitter2014 1853 982 202 669Test: Twitter2013 3813 1572 601 1640Test: SMS2013 2093 492 394 1207Test: Tw2014Sarcasm 86 33 40 13Test: LiveJournal2014 1142 427 304 4112 Description of Our ApproachCompared to the previous NRC Canada 2013approach (Mohammad et al., 2013), our mainchanges are the following three: First we usesparse linear classifiers instead of classical denseones.
Secondly, we drop n-gram features com-pletely, in favor of what we call part-of-speechn-grams, which are n-grams where up to two to-kens are the original ones, and the rest of the to-kens is replaced by their corresponding POS tag(noun, verb, punctuation etc).
Third, we addedtwo new sentiment lexicons, containing numericalscores associated for all 3 classes (positive, neu-tral, negative), instead of just 2 as in classical po-601larity lexicons.
All changes are described in moredetail in Sections 4 and 3 below.Performance.
We tried to reproduce the sameclassifier as in (Mohammad et al., 2013) as a base-line for comparison.Trying to quantify our contributions, whenadding all our additional features and tricks de-scribed below, the score of our method increasesfrom the baseline of 63.25 to 64.81 (on the Twitter-2013 test set), which is a gain of 1.56 points in F1.Baseline Approach by NRC Canada 2013.Unfortunately our replica system of Mohammadet al.
(2013) only achieved an F1-score of 63.25 onthe Twitter-2013 test set, while their score in the2013 competition on the same test set was 69.02,nearly 6 points higher in F1.Part of this big difference might be explainedby the fact that the exact same training sets arenot available anymore.
Other possibly more im-portant differences are the SVM classifier variantused and class weighting (described in Section 4).Furthermore, we didn?t implement all features inthe exactly same way, see the more detailed de-scription in Section 3.1.2 below.
Although we hadthe impression that these changes individually hadonly a relatively minor effect, it might be that thechanges together with the different training set addup to the difference in score.3 FeaturesBefore we describe the linear classifier in Sec-tion 4, we detail the used features for each tweetmessage.
On average, we generated 843 featuresper tweet.
For comparison, the average in ourNRC Canada 2013 replica system was only 285.Most of the increase in features comes from thefact that we allowed for slightly longer n-grams(6 instead of 4), and substrings (length 6 insteadof 5).3.1 New Features3.1.1 Part of Speech n-gramsWe used the ArkTweetNLP structured predictionPOS tagger provided by Owoputi et al.
(2013)together with their provided standard model(model.20120919) suitable for twitter data.Part of speech n-grams are n-grams where upto two tokens are kept as the original ones, and allother tokens are replaced by their correspondingPOS tag (noun, verb, punctuation etc).
We gener-ated these modified n-grams for all possible posi-tions of the one or two original tokens within the npositions, for 3 ?
n ?
6.As features for a classifier, we found POS n-grams at least as useful (with some more robust-ness) as the n-grams themselves.
In our finalapproach, we dropped the use of n-grams com-pletely, and only used POS n-grams instead.
Theidea of replacing some of the tokens by their POStag is also investigated by Joshi and Penstein-Ros?e(2009), where the authors used n ?
3.3.1.2 Various Changes Compared to NRCCanada 2013?
We do not allow n-grams (or POS n-grams)to span over sentence boundaries.?
Substrings of length up to 6 (instead of 5).?
Substring features are weighted in-creasingly by their length (weights0.7 ?
{1.0, 1.1, 1.2, 1.4, 1.6, 1.9} for lengths3, 4, .
.
.
)?
Instead of the score itself, we used the sig-moid value s(t) = 1/(1+ e?t)) of each lexi-con score.
For each lexicon, the 4 scores werethe same as in (Mohammad et al., 2013), i.e.per tweet, we use the number of tokens ap-pearing in the lexicon, the sum and the maxof the scores, and the last non-zero score.We skipped some features from the baseline ap-proach (because their effect was not significant inour setting): Elongated words (number of wordswith one character repeated more than two times),and word clustering.
Also, we had a slightly sim-plified variant of how to use the lexicon scores.We didn?t count the lexicon scores separately peremotion (pos and neg), but only altogether.3.2 Existing FeaturesText Preprocessing.
A good tokenization seemsvery important for twitter data.
We used the pop-ular tokenizer ArkTweetNLP (Owoputi et al., 2013)which is suitable for tweets.
All text was trans-formed to lowercase (except for those features in(Mohammad et al., 2013) which use case infor-mation).
As usual, URLs were normalized tohttp://someurl and twitter user IDs to @someuser.We also employed the usual marking of negatedcontexts of a sentence as in (Pang et al., 2002),602using the list of negation words from ChristopherPotts?
sentiment tutorial1.4 ClassifierWe used a linear support vector machine (SVM)classifier, which is standard for text data.
The Lib-Linear package (Fan et al., 2008) was employed fortraining the multi-class classifier.Multi-Class Formulation, and Class Weights.We found significant performance changes de-pending on which type of multi-class SVM, andalso which regularizer (`1- or `2-norm) is used.For the multi-class variant, we found the one-against-all models to perform slightly better thanthe Crammer and Singer (2001) formulation.More importantly, since the 3 classes (posi-tive, negative and neutral) are quite unbalancedin size in the training set, it is crucial to set agood weight for each class in the SVM.
We used(4.52, 1.38, 1.80), which corresponds to the twicethe ratio of each class compared to the averageclass size.Sparse Linear Classifiers.
In our setting, an`1-regularized squared loss SVM (one-against-all)performed best (this is mode L1R L2LOSS SVC inLibLinear), despite the fact that `2-regularization isgenerally more commonly used in text applica-tions.
We used C = 0.055 for the regularizationparameter, and ?
= 0.003 as the optimization stop-ping criterion.
We did not employ any kernel, butalways used linear classifiers.Another benefit of the `1-regularization is thatthe resulting classifier is extremely sparse andcompact, which significantly accelerates the eval-uation of the classifier on large amounts of text,e.g.
for testing.
Our final classifier only uses1985 non-zero features (1427 unigram/substrings,and 558 other features, such as lexicon scores, n-grams, POS n-grams, as explained in the previousSection 3).As the resulting classifier is so small, it is alsorelatively easy to read and interpret.
We havemade our final classifier weights publicly availablefor download as a text file2.
Every line containsthe feature description followed by the 3 weightscorresponding to the 3 sentiment classes.1http://sentiment.christopherpotts.net/lingstruc.html2http://www.m8j.net/sentiment/Our final classifier was trained on 9641 tweets,which are all we could download from the IDsgiven in this years train and dev set.5 LexiconsA sentiment lexicon is a mapping from words (orn-grams) to an association score corresponding topositive or negative sentiment.
Such lists can beconstructed either from manually labeled data (su-pervised), or automatically labeled data (unsuper-vised) as for example tweets with a positive ornegative smiley.
We used the same set of lexiconsas in (Mohammad et al., 2013), with one addition:5.1 A Lexicon for 3-Class ClassificationOur main new addition was another type of lexi-con, which not only provides one score per word,but 3 of them, (being the association to positive,negative and neutral).
The idea here is to improveon the discrimination quality, especially for neu-tral text, and treat all 3 labels in this multi-classtask the same way, instead of just 2 as in the pre-vious approaches.Data.
We found it challenging to find gooddatasets to build such a lexicon.
We again used theSentiment140 corpus (Go et al., 2009) (containingtweets with positive or negative emoticons).
Usinga subset of 100k positive and 100k negative ones,we added a set of 100k arbitrary (hopefully neu-tral) tweets.
The neutral set was chosen randomlyfrom the thinknook.com dataset3of 1.5mio tweets(from which we ignored the provided labels, andcounted the tweets as neutral).We did the same with the movie reviews fromthe recent kaggle competition on annotated re-views from the rotten-tomatoes website4.
We au-tomatically built a lexicon from 100k texts in thisdataset, with the data balanced equally for thethree classes.Features Used in the Lexicon.
To construct thelexicon, we extracted the POS n-grams (as we de-scribed in Section 3.1.1 above) from all texts.
Incomparison, Mohammad et al.
(2013) used non-contiguous n-grams (unigram?unigram, unigram?bigram, and bigram?bigram pairs).
We only usedPOS n-grams with 2 tokens kept original, and the3http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/4http://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data603remaining ones replaced by their POS tag, with nranging from 3 to 6.Building the Lexicon.
While in (Mohammadet al., 2013), the score for each n-gram was com-puted using point-wise mutual information (PMI)with the labels, we trained a linear classifier onthe same labels instead.
The lexicon weights areset as the resulting classifier weights for our (POS)n-grams.
We used the same type of sparse SVMtrained with LibLinear, for 3 classes, as in the finalclassifier.Download of the Lexicons.
We built 4 lexiconsas described above.
Thanks to the sparsity ofthe linear weights from the SVM, they are againrelatively small, analogous to the final classifier.We also provide the lexicons for download as textfiles5.5.2 Existing LexiconsLexicons from Manually Labeled Data.
Weused the same 3 existing sentiment lexicons as in(Mohammad et al., 2013).
All lexicons give a sin-gle score for each word (if present in the lexicon).Those existing lexicons are: NRC Emotion Lexi-con (about 14k words), the MPQA Lexicon (about8k words), and the Bing Liu Lexicon (about 7kwords).Lexicons from Automatically Labeled Data.The NRC hashtag sentiment lexicon was gen-erated automatically from a set of 775k tweetscontaining a hashtag of a small predefined listof positive and negative hashtags (Mohammadet al., 2013).
Lexicon scores were trained viaPMI (point-wise mutual information).
Scores arenot only available for words, but also unigram?unigram, unigram?bigram, and bigram?bigrampairs (that can be non-contiguous in the text).The Sentiment140 lexicon (Go et al., 2009) wasgenerated automatically from a set of 1.6 milliontweets containing a positive or negative emoticon.This uses the same features and scoring as above.6 ConclusionWe have described an SVM classifier to detect thesentiment of short texts such as tweets.
Our sys-tem is built up on the approach of NRC Canada(Mohammad et al., 2013), with several modifica-tions and extensions (e.g.
sparse linear classifiers,5http://www.m8j.net/sentiment/POS-n-grams, new lexicons).
We have seen thatour system significantly improves the baseline ap-proach, achieving a gain of 1.56 points in F1 score.We participated in the SemEval-2014 competi-tion for Twitter polarity classification, and our sys-tem was among the top ten out of 50 submissions,with an F1-score of 67.54 on tweets.For future work, it would be interesting to in-corporate our improvements into the most recentversion of NRC Canada or similar systems, to seehow much one could gain there.ReferencesCrammer, K. and Singer, Y.
(2001).
On the Algo-rithmic Implementation of Multiclass Kernel-based Vector Machines.
JMLR, 2:265?292.Fan, R.-E., Chang, K.-W., Hsieh, C.-J., Wang, X.-R., and Lin, C.-J.
(2008).
LIBLINEAR: A Li-brary for Large Linear Classification.
JMLR,9:1871?1874.Go, A., Bhayani, R., and Huang, L. (2009).
Twit-ter Sentiment Classification using Distant Su-pervision.
Technical report, The Stanford Natu-ral Language Processing Group.Joshi, M. and Penstein-Ros?e, C. (2009).
General-izing dependency features for opinion mining.In Proceedings of the ACL-IJCNLP 2009 Con-ference Short Papers, p 313?316, Singapore.Association for Computational Linguistics.Mohammad, S. M., Kiritchenko, S., and Zhu, X.(2013).
NRC-Canada: Building the State-of-the-Art in Sentiment Analysis of Tweets.
InSemEval-2013 - Proceedings of the Interna-tional Workshop on Semantic Evaluation, pages321?327, Atlanta, Georgia, USA.Owoputi, O., O?Connor, B., Dyer, C., Gimpel,K., Schneider, N., and Smith, N. A.
(2013).Improved Part-Of-Speech Tagging for OnlineConversational Text with Word Clusters.
InProceedings of NAACL-HLT, pages 380?390.Pang, B., Lee, L., and Vaithyanathan, S. (2002).Thumbs up?
Sentiment Classification usingMachine Learning Techniques.
In ACL-02 con-ference, pages 79?86, Morristown, NJ, USA.Association for Computational Linguistics.Rosenthal, S., Ritter, A., Nakov, P., and Stoyanov,V.
(2014).
SemEval-2014 Task 9: SentimentAnalysis in Twitter.
In SemEval 2014 - Pro-ceedings of the Eighth International Workshopon Semantic Evaluation, Dublin, Ireland.604
