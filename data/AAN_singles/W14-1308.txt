Proceedings of the 5th Workshop on Language Analysis for Social Media (LASM) @ EACL 2014, pages 62?70,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsA Cascaded Approach for Social Media Text Normalization of TurkishDilara Toruno?gluDep.
of Computer Eng.Istanbul Technical UniversityIstanbul, Turkeytorunoglud@itu.edu.trG?uls?en Eryi?gitDep.
of Computer Eng.Istanbul Technical UniversityIstanbul, Turkeygulsen.cebiroglu@itu.edu.trAbstractText normalization is an indispensablestage for natural language processing ofsocial media data with available NLPtools.
We divide the normalization prob-lem into 7 categories, namely; letter casetransformation, replacement rules & lexi-con lookup, proper noun detection, deasci-ification, vowel restoration, accent nor-malization and spelling correction.
Wepropose a cascaded approach where eachill formed word passes from these 7 mod-ules and is investigated for possible trans-formations.
This paper presents the firstresults for the normalization of Turkishand tries to shed light on the different chal-lenges in this area.
We report a 40 per-centage points improvement over a lexiconlookup baseline and nearly 50 percentagepoints over available spelling correctors.1 IntroductionWith the increasing number of people using microblogging sites like Facebook and Twitter, socialmedia became an indefinite source for machinelearning area especially for natural language pro-cessing.
This service is highly attractive for infor-mation extraction, text mining and opinion min-ing purposes as the large volumes of data availableonline daily.
The language used in this platformdiffers severely from formally written text in that,people do not feel forced to write grammaticallycorrect sentences, generally write like they talk ortry to impress their thoughts within a limited num-ber of characters (such as in Twitter 140 charac-ters).
This results with a totally different languagethan the conventional languages.
The research ontext normalization of social media gained speedtowards the end of the last decade and as always,almost all of these elementary studies are con-ducted on the English language.
We know fromearlier research results that morphologically richlanguages such as Turkish differ severely from En-glish and the methods tailored for English do notfit for these languages.
It is the case for text nor-malization as well.Highly inflectional or agglutinative languagesshare the same characteristic that a unique lemmain these languages may have hundreds of possiblesurface forms.
This increases the data sparsity instatistical models.
For example, it?s pointed out inHakkani-T?ur et al.
(2000) that, it is due to Turk-ish language?s inflectional and derivational mor-phology that the number of distinct word formsis very large compared to English distinct wordsize (Table 1).
This large vocabulary size is thereason why the dictionary1lookup or similaritybased approaches are not suitable for this kind oflanguages.
And in addition to this, it is not aneasy task to collect manually annotated data whichcould cover all these surface forms and their re-lated mistakes for statistical approaches.Corpus Size Turkish English1M words 106,547 33,39810M words 417,775 97,734Table 1: Vocabulary sizes for two Turkish and En-glish corpora (Hakkani-T?ur et al., 2000)In this paper, we propose a cascaded approachfor the social text normalization (specifically forTweets) of Turkish language.
The approach isa combination of rule based and machine learn-ing components for different layers of normaliza-tion, namely; letter case transformation, replace-ment rules & lexicon lookup, proper noun detec-tion, deasciification, vowel restoration, accent nor-malization and spelling correction.
Following thework of Han and Baldwin (2011), we divided thework into two stages: ill formed word detection1For these languages, it is theoretically impossible to putevery possible surface form into a dictionary.62and candidate word generation.
Our contributionis: 1. a new normalization model which could beapplied to other morphologically rich languages aswell with appropriate NLP tools 2. the first re-sults and test data sets for the text normalizationof Turkish.The paper is structured as follows: Section 2and 3 give brief information about related workand morphologically rich languages, Section 4presents our normalization approach and Section5 the experimental setup, Section 6 gives our ex-perimental results and discussions and Section 7the conclusion.2 Related WorkAn important part of the previous studies havetaken the normalization task either as a lexi-con lookup (together with or without replacementrules) or as a statistical problem.
There also ex-ist many studies which use their combination.
Inthese studies, a lexicon lookup is firstly employedfor most common usage of slang words, abbrevi-ations etc.
and then a machine learning methodis employed for the rest.
Zhang et al.
(2013) usesreplacement rules and a graph based model in or-der to select the best rule combinations.
Wang andNg (2013) uses a beam search decoder.
Hassanand Menezes (2013) propose an unsupervised ap-proach which uses Random Walks on a contextualsimilarity bipartite graph constructed from n-gramsequences.
In Han and Baldwin (2011), word sim-ilarity and context is used during lexicon lookup.Cook and Stevenson (2009) uses an unsupervisednoisy channel model.
Clark and Araki (2011)makes dictionary lookup.
Liu et al.
(2012) usesa unified letter transformation to generate possi-ble ill formed words in order to use them in thetraining phase of a noisy channel model.
Eisen-stein (2013) analyzes phonological factors in so-cial media writing.Others, treating the normalization task as amachine translation (MT) problem which triesto translate from an ill formed language to aconventional one, form also another importantgroup.
For example the papers from Kaufmannand Kalita (2010), Pennell and Liu (2011), Aw etal.
(2006) and Beaufort et al.
(2010) may be col-lected under this group.
Since the emergence ofsocial media is very recent, only the latest stud-ies are focused on this area and the earlier onesgenerally work for the text normalization in TTS(text-to-speech), ASR (automatic speech recogni-tion) systems or SMS messages.
Social media nor-malization poses new challenges on top of these,for example Twitter statuses contains mentions(@user name), hashtags (#topic), variant numberof emoticons ( e.g.
:) :@ <3 @>? )
and spe-cial keywords (RT - retweet, DM - direct messageetc.
).Although very rare, there are also some stud-ies on languages other than English and theseare mostly for speech recognition and SMS mes-sages , e.g.
Panchapagesan et al.
(2004) for HindiTTS, Nguyen et al.
(2010) for Vietnamese TTS,Jia et al.
(2008) for Mandarin TTS, Khan andKarim (2012) for Urdu SMS.
To the best of ourknowledge, our study is the first attempt for thenormalization of social media data for morpholog-ically rich languages.3 Morphologically Rich LanguagesMorphologically rich languages such as Turkish,Finnish, Korean, Hebrew etc., pose significantchallenges for natural language processing tasks(Tsarfaty et al., 2013; Sarikaya et al., 2009).
Asstated previously, the highly productive morphol-ogy of these languages results in a very large num-ber of word forms from a given stem.
Table 2 listsonly a few (among hundreds of possible) surfaceforms for the Turkish stem ?ev?
(house).Surface form Englishev houseeve to the houseevde at the houseevdeki (which is) at the houseevdekiler those (who are) at the houseevdekilerde at those (who are)Table 2: Some surface forms for ?ev?
(house)Sarikaya et al.
(2009) list the emerging prob-lems as below:1. increase in dictionary size2.
poor language model probability estimation3.
higher out-of-vocabulary (OOV) rate4.
inflection gap for machine translation2That is why, the normalization methods pro-posed so far (adapting MT or language models or2Since, the number of possible word surface forms afterinflections is very high, the alignment and translation accura-cies in these languages are very badly affected.63lexicon lookup approaches) do not seem appropri-ate for the processing of morphologically rich lan-guages, as in our case for Turkish.4 The Proposed ArchitectureWe divide the normalization task into two parts:Ill-formed word detection and candidate genera-tion.
Figure 1 presents the architecture of the pro-posed normalization approach.
The following sub-sections provide the details for both of these twoparts and their components.Before sending the input into these stages, wefirst use our tokenizer specifically tailored forTwitter for splitting the tweets into meaningful to-kens.
Our tokenizer is actually the first step ofour normalization process since: 1.
It intelligentlysplits the wrongly written word-punctuation com-binations (e.g.
?a,b?
to [a , b]), while leaving ?Ah-met?den?
(from Ahmet) is left as it is since theapostrophe sign is used to append inflectional fea-tures to a proper noun.)
2.
It does special pro-cessing for emoticons and consecutive punctua-tion marks so that they still reside together afterthe tokenization (e.g.
:D or !!!!!
are output as theyoccur).Figure 1: Normalization architecture4.1 Ill-formed Word DetectionAs stated earlier, since it is not possible to use alexicon lookup table for morphologically rich lan-guages, we use a morphological analyzer (S?ahinet al., 2013) and an abbreviation list3and a list of1045 abbreviations for controlling in-vocabulary(IV) words (labeled with a +NC ?No Change?
la-bel for further use).
By this way, we filter all theout-of-vocabulary (OOV) words and transfer themto the candidate generation process.
Mentions(@user name), hashtags (#topic), emoticons (:D) ,vocatives (?ahahahaha?)
and keywords (?RT?)
arealso assumed to be OOV words since we want todetect these and tag them with special labels to belater used in higher-level NLP modules (e.g.
POStagging, syntactic analysis).4.2 Candidate GenerationIn the candidate generation part, we have sevencomponents (rule based or machine learning mod-els) which work sequentially.
The outputs of eachof these components are controlled by the morpho-logical analyzer and if the normalized form from acomponent becomes an IV word then the processis terminated and the output is labeled with a rele-vant tag (provided in Table 3).
Otherwise, the can-didate generation process continues with the nextcomponent over the original input (except for the?Letter Case Transformation?
and ?ReplacementRules & Lexicon Lookup?
components where theinput is replaced by the modified output althoughit is still not an IV word, (see Section 4.2.1 and4.2.2 for details).Label Component+NC No Change+LCT Letter Case Transformation+RR Replacement Rules & Lexicon Lookup+PND Proper Noun Detection+DA Deasciification+VR Vowel Restoration+AN Accent Normalization+NoN No Suggested NormalizationTable 3: Component Labels4.2.1 Letter Case TransformationAn OOV token, coming to this stage, may be inone of the 4 different forms: lowercase, UPPER-CASE, Proper Noun Case or miXEd CaSe.
Ifthe token is in lowercase and does not possessany specific punctuation marks for proper nouns(i.e.
?
(apostrophe) or .
(period)) , it is directly3obtained from TLA (Turkish Language Association)http://www.tdk.gov.tr/index.php?option=com_content&id=198:Kisaltmalar64transferred to the next stage without any change(e.g.
umuttan (from hope)).
If the token is inProper Noun Case (e.g.
Umut?tan), it is acceptedas a correct proper noun (even if it does not oc-cur within the morphological analyzer?s lexicon orwas previously detected as an OOV word), left un-touched (taking the label +NC) and excluded fromall future evaluations.For UPPERCASE, miXEd CaSe and lowercasewords, we convert them into Proper Noun Case ifthey either contain an apostrophe (which is usedin Turkish to separate inflectional suffixes from aproper noun) or a period (.)
which is used for-mally in Turkish to denote abbreviations.
Thesewords are labeled with a ?+LCT?
label after thenormalization.
If the word does not contain anyof these two marks, it is then converted into low-ercase form and processed by the morphologicalanalyzer as explained at the beginning of Sec-tion 4.2.
It should be noted that all words goingout from this component towards next stages aretransformed into lowercase from this point on.?ahmet?ten?
?
Proper Noun?AHMET?TEN?
?
Proper Noun?EACL.
?- Abbreviation4.2.2 Replacement Rules & Lexicon Look-upWhile normalizing the tweets, we have to dealwith the following problems:1.
Slang words2.
Character repetition in interjections3.
Twitter-specific words4.
Emo style writingWe created a slang word lexicon of 272 words.This lexicon contains entries as the following:?kib?
for ?kendine iyi bak?
(take care of your-self ), ?nbr?
for ?ne haber?
(what?s up).
The tokenswithin the lexicon are directly replaced with theirnormalized forms.Repetition of some characters within a word isa very common method to express exclamationin messages, such as in ?l?utfeeeennnn?
instead of?l?utfen?
(please), ?c?ooooooook?
instead of ?c?ok?
(very) and ?ayyyyy?
instead of ?ay?
(oh!).
We re-duce the repeated characters into a single characterin the case that the consecutive occurrence countis greater than 2.The usage of Twitter-specific words such ashashtags (?#topic?
), mentions (?
@user name?
),emoticons (?:)?
), vocatives (?hahahhah?,?h?o?o?o?o?o?)
and keywords (?RT?)
also causesa host of problems.
The recurring patterns invocatives are reduced into minimal forms duringthe normalization process, as for ?haha?
insteadof ?hahahhah?
and ?h?o?
instead of ?h?o?o?o?o?o?.Emo style writing, as in the example ?$eker4you?
instead of ?s?eker senin ic?in?
(sweety, it?sfor you), is another problematic field for the nor-malization task.
We created 35 replacement ruleswith regular expressions in order to automaticallycorrect or label the given input for Twitter-specificwords and Emo style writing.
Examples include?$ ?
s?
?, ? ?
e?, ?3 ?
e?
and ?!?
i?.Through these replacement rules, we are able tocorrect most instances of Emo style writing.Our regular expressions also label the followingtoken types by the given specific labels for futurereference:?
Mentions: Nicknames that referto users on Twitter are labeled as e.g.@mention[@dida]?
Hashtags: Hashtags that refer to trend-ing topics on Twitter are labeled as e.g.@hashtag[#geziparki]?
Vocatives: Vocatives are labeled as e.g.@vocative[hehe]?
Smileys: Emoticons are labeled as e.g.@smiley[:)]?
Twitter-specific Keywords: Keywords like?RT?, ?DM?, ?MT?, ?Reply?
etc.
are labeled ase.g.
@keyword[RT]Figure 2 shows the normalized version of atweet in informal Turkish that could be translatedlike ?
@dida what?s up, why don?t you call #of-fended :(?, before and after being processed by thiscomponent.
Although the word ?aram?on?
alsoneeds normalization as ?aram?yorsun?
(you don?tcall), this transformation is not realized within thecurrent component and applied later in the accentnormalization component given in Section 4.2.6.4.2.3 Proper Noun DetectionAs previously stated, all OOV words coming tothis stage are in lowercase.
In this component, ouraim is to detect proper nouns erroneously writtenin lowercase (such as ?ahmetten?
or ?ahmetden?
)and convert them to proper noun case with correctformatting (?Ahmet?ten?
for the aforementionedexamples).65@didanbr nedenaram?on#k?r?ld?m: (@mention[@dida] ne haber neden aram?on @hashtag[#k?r?ld?m] @smiley[: (]Figure 2: Normalization with Replacement Rules & Lexicon Look-upFor this purpose, we use proper name gazetteersfrom S?eker and Eryi?git (2012) together with anewly added organization gazetteer of 122 tokensin order to check whether a given word couldbe a proper noun.
Turkish proper nouns arevery frequently selected from common nouns suchas ?C?ic?ek?
(flower), ?S?eker?
(sugar) and ??Ipek?(silk).
Therefore, it is quite difficult to recog-nize such words as proper nouns when they arewritten in lowercase, as the task could not be ac-complished by just checking the existence of suchwords within the gazetteers.For our proper noun detection component, weuse the below strategy:1.
We reduce the size of the gazetteers by remov-ing all words with length ?
2 characters, or witha ratio value under our specified threshold (1.5).Ratio value is calculated, according to the formulagiven in Equation 1, considering the occurrencecounts from two big corpora, the METU-Sabanc?Treebank (Say et al., 2002) and the web corpusof Sak et al.
(2011).
Table 4 gives the counts forthree sample words.
One may observe from thetable that ?ahmet?
occured 40 times in proper caseand 20 times in lower case form within the twocorpora resulting in a ratio value of 2.0.
Since theratio value for ?umut?
is only 0.4 (which is un-der our threshold), this noun is removed from ourgazetteers so that it would not be transformed intoproper case in case it is found to occur in low-ercase form.
A similar case holds for the word?sa?glam?
(healthy).
Although it is a very frequentTurkish family name, it is observed in our corporamostly as a common noun with a ratio value of0.09.ratio(wn) =Occurence in Propercase(wn)Occurence in Lowercase(wn)(1)2.
We pass the tokens to a morphological an-alyzer for unknown words (S?ahin et al., 2013)and find possible lemmata as in the example be-low.
We then search for the longest possible stemwithin our gazetteers (e.g.
the longest stem for?ahmetten?
found within the name gazetteer isProper Case Lowercase Sense RatioSa?glam=9 sa?glam=100 healthy Ratio=0.09Umut=40 umut=100 hope Ratio=0.4Ahmet=40 ahmet=20 n/a Ratio=2.0Table 4: Example of Ratio Values?ahmet?
), and when a stem is found within thegazetteers, the initial letter of the stem is capital-ized and the inflectional suffixes after the stem areseparated by use of an apostrophe (?Ahmet?ten?
).If none of the possible stems is found within thegazetteers, the word is left as is and transferred tothe next stage in its original form.
?ahmet +Noun+A3sg+Pnon+Abl?
?ahmette +Noun+A3sg+Pnom+Loc?
?ahmetten +Noun+A3sg+Pnon+Nom?4.2.4 DeasciificationThe role of the deasciifier is the reconstruction ofTurkish-specific characters with diacritics (i.e.
?,?I, s?, ?o, c?, ?g, ?u) from their ASCII-compliant coun-terparts (i.e.
i, I, s, o, c, g, u).
Most users of so-cial media use asciified letters, which should becorrected in order to obtain valid Turkish words.The task is also not straightforward because of theambiguity potential in asciified forms, as betweenthe words ?yasa?
(law) and ?yas?a?
(live).
Forthis stage, we use the deasciifier of Y?uret (Y?uretand de la Maza, 2006) which implements theGPA algorithm (which itself is basically a decisiontree implementation) in order to produce the mostlikely deasciified form of the input.4.2.5 Vowel RestorationThere is a new trend of omitting vowels in typ-ing among the Turkish social media users, in or-der to reduce the message length.
In this stage, weprocess tokens written with consonants only (e.g.?svyrm?
), which is how vowel omission often hap-pens.
The aim of the vowel restoration is the gen-eration of the original word by adding vowels intothe appropriate places (e.g.
?svyrm?
to ?seviyo-rum?
(I love)).
We employed a vocalizer (Adal?66and Eryi?git, 2014) which uses CRFs for the con-struction of the most probable vocalized output.4.2.6 Accent NormalizationIn the social media platform, people generallywrite like they talk by transferring the pronouncedversions of the words directly to the written text.Eisenstein (2013) also discusses the situation forthe English case.
In the accent normalization mod-ule we are trying to normalize this kind of writingsinto proper forms.
Some examples are given be-low:?gidicem?
instead of ?gidece?gim?
(I?ll go)?geliyonmu??
instead of ?geliyor musun??
(Are you coming?
)In this component, we first try to detect the mostcommon verb accents (generally endings such as?-cem, -yom, -c?az?
etc.)
used in social media andthen uses regular expression rules in order to re-place these endings with their equivalent morpho-logical analysis.
One should note that since inmost of the morphologically rich languages, theverb also carries inflections related to the personagreement, we produce rules for catching all thepossible surface forms of these accents.Table 5 introduces some of these re-placement rules (column 1 and column 3).As a result, the word ?gidcem?
becomes?git+Verb+Pos+Fut+A1sg?4.
We then use amorphological generator and takes the cor-rected output (if any) ?gidece?gim?
(I?ll go) for?git+Verb+Pos+Fut+A1sg?5.We also have more complex replacement rulesin order to process more complex accent problems.To give an example, the proper form of the word?gidiyonmu?
is actually ?gidiyor musun?
(are yougoing) and in the formal form it is the questionenclitic (?mu?)
which takes the person agreement(?-sun?
2. person singular) where as in the accentform the person agreement appears before ?mu?
asa single letter ?gidiyonmu?.4Please note that, we also change the last letter of the stemaccording to the harmonization rules of Turkish: the last let-ters ?bcdg?
are changed to ?pc?tk?.5the morphological tags in the table stands for: +Pos:Positive, +Prog1: Present continuous tense, +A2sg: 2. per-son singular, +Fut: Future tense, +A1sg: 1. person singular,+A1pl: 1. person pluralAccent Correct Morph.endings endings Analysis+iyon +iyorsun +Verb+Pos+Prog1+A2sg+cem +ece?gim +Verb+Pos+Fut+A1sg+caz +aca?g?z +Verb+Pos+Fut+A1plTable 5: Accent Normalization ReplacementRules4.2.7 Spelling CorrectionAs the last component of our normalization ap-proach, we propose to use a high performancespelling corrector.
This spelling corrector shouldespecially give a high precision score rather thanrecall since the false positives have a very harm-ing effect on the normalization task by producingoutputs with a totally different meaning.
Unfortu-nately, we could not find such a corrector for Turk-ish.
We tested with an MsWord plugin and thespelling corrector of Zemberek (Ak?n and Ak?n,2007) and obtained a negative impact by usingboth.
We are planning to create such a spellingcorrector as future work.If an OOV word couldn?t still be normalized atthe end of the proposed iterative model (consisting7 components), it is labeled with a ?+NoN?
labeland left in its original input format.5 Experimental SetupIn this section we provide information about ourused data sets, our evaluation strategy and the usedmodels in the experiments.5.1 Data SetsTo test our success rates, we used a total of 1,200tweets aligned and normalized manually.
Themanual alignment is a one-to-many token align-ment task from the original input towards the nor-malized forms.
To give an example, the slang us-age ?kib?
will be aligned to 3 tokens (?kendineiyi bak?
(take care of yourself )) on the normal-ized tweet.
Although there are cases for many-to-one alignment (such as in ?cats,dogs?
), these arehandled in the tokenization stage before the nor-malization.
We used half of this data set as ourvalidation set during the development of our pro-posed components and reserved the remaining 600tweets (collected from a different time slot) as a to-tally unseen data set for using at the end.
Table 6provides some statistics over these data sets: thenumber of tweets, the number of tokens and the67Data Sets # Tweets # Tokens # OOVValidation Set 600 6,322 2,708Test Set 600 7,061 2,192Table 6: Description of the Data Setsnumber of OOV tokens.Besides the aforementioned datasets, we alsohad access to a much bigger Twitter data setconsisting of 4,049 manually normalized tweets(Eryi?git et al., 2013) (59,012 tokens in total).
Theonly difference of this data set is that the tweetsare not aligned on token level as in the previouslyintroduced data sets.
That is why, it is not possi-ble to use them for gold standard evaluation of oursystem.
But in order to be able to have an ideaabout the performance of the previous approachesregarding lexicon lookup, we decided to automat-ically align this set and create a baseline lexiconlookup model for comparison purposes.
(see thedetails in Section 5.3).5.2 Evaluation MethodWe evaluated our work both for ill formed worddetection and candidate generation separately.
Forill formed word detection, we provide precision(P), recall (R), f-measure (F) and accuracy (Acc.)scores.
For candidate generation, we provide onlythe accuracy scores (the number of correctly nor-malized tokens over the total number of detectedill formed words).5.3 Compared ModelsTo the best of our knowledge this study is thefirst attempt for the normalization of Turkish so-cial media data.
Since there are only spelling cor-rector systems available for the task we comparedthe proposed model with them.
In other words, wecompared 3 different models with our proposedsystem:Model 1 (MsWord) is the model where we use anapi for getting the MsWord Turkish spelling sug-gestions.
Although this is not a tool developed fornormalization purposes we wanted to see its suc-cess on our data sets.
We accepted the top bestsuggestion as the normalized version for the inputtokens.Model 2 (Zemberek) (Ak?n and Ak?n, 2007) is alsoan open source spelling corrector for Turkish.Model 3 (Lookup Table) is a model that we devel-oped with the aim of creating a baseline lookupapproach for comparison.
For this purpose, wefirst used GIZA++ (Och and Ney, 2000) in orderto automatically align the normalized tweets (us-ing the 4,049 tweets?
data set presented in Sec-tion 5.1) and created a lookup table with the pro-duced aligned token sequences.
We then used thislookup table to check for the existence of each illformed word and get its normalized counterpart.6 Experimental ResultsTable 7 and Table 8 gives the results of the illformed word detection for different systems forthe validation set and the test set consecutively.
Inthese experiments, we do not provide the results ofthe ?Lookup Table?
model since the ill formed de-tection part of it is exactly the same with our pro-posed model.
For MsWord and Zemberek we con-sidered each modified word as an ill formed worddetected by that system.
We can see from the ta-bles that our proposed model has an f-measure ofill formed word detection 0.78.
As it is explainedin Section 4.1, our ill formed word detection ap-proach is very straightforward and it uses only amorphological analyzer and an abbreviation listin order to detect OOV words.
Thus, one maywonder why the scores for the proposed modelare not very close to 1 although it outperformsall of its available rivals.
This is because, thereexists nearly 20% of the ill formed tokens whichare not suspended to our morphological filter al-though they are manually annotated as ill formedby human annotators.
This is certainly possiblefor morphologically rich languages since a wordsurface form may be the valid analysis of manystems.
The ill formed word ?c?al?s??c?m?
is a goodexample for this situation.
Although this wordwill be understood by most of the people as the illformed version of the word ?c?al?s?aca?g?m?
(I?m go-ing to work), it is considered by the morphologicalanalyzer as a valid Turkish word since althoughvery rare, it could also be the surface form ofthe word ?c?al?s??
with additional derivational andinflectional suffixes ?c?al?s?+?c?+m?
meaning ?myworker?.Systems P R F Acc.MsWord 0.25 0.59 0.35 0.58Zemberek 0.21 0.17 0.19 0.21Proposed Model 0.75 0.81 0.78 0.80Table 7: Ill Formed Word Detection EvaluationResults on Validation Set68Systems P R F Acc.MsWord 0.24 0.19 0.21 0.56Zemberek 0.11 0.29 0.20 0.11Proposed Model 0.71 0.72 0.71 0.86Table 8: Ill Formed Word Detection EvaluationResults on Test SetData Set Systems AccuracyMsWord 0.25Validation Set Zemberek 0.21Lookup Table 0.34Proposed Model 0.75MsWord 0.24Test Set Zemberek 0.11Lookup Table 0.31Proposed Model 0.71Table 9: Candidate Generation Results on DataSetsTable 9 gives the evaluation scores of each dif-ferent system for both the validation and test datasets.
Although the lookup model is very basic,one can observe from the table that it outperformsboth MsWord and Zemberek.
Our proposed iter-ative model obtains the highest scores (75% forvalidation and 71% for test sets) with a relativeimprovement of 40 percentage points over the lex-icon lookup baseline.7 ConclusionIn this paper we presented a cascaded normaliza-tion model for Turkish which could also be appliedto the morphologically rich languages with appro-priate NLP tools.
The model has two main parts:ill formed word detection and candidate word gen-eration consisting of 7 normalization stages (let-ter case transformation, replacement rules & lex-icon lookup, proper noun detection, deasciifica-tion, vowel restoration, accent normalization andspelling correction) executed sequentially one ontop of the other one.
We present the first and high-est results for Turkish text normalization6of so-cial media data with a 86% accuracy of ill formedword detection and 71% accuracy for candidateword generation.
A morphological analyzer isused for the detection of ill formed words.
Butwe believe the accuracy of this first detection stage6The produced test sets and the Web interface of theTurkish Normalizer is available via http://tools.nlp.itu.edu.tr(Eryi?git, 2014)may be improved by the addition of a lexiconlookup (before the morphological filter) consistingthe most frequent normalization cases extractedfrom manually normalized data if available.
Thus,as a future work we plan to extend our work bothon the ill formed word detection and on the cre-ation of a spelling corrector with social web datain focus.AcknowledgmentThis work is part of our ongoing research project?Parsing Turkish Web 2.0 Sentences?
supportedby ICT COST Action IC1207 TUBITAK 1001(grant no: 112E276).
The authors want to thankTurkcell Global Bilgi for sharing the manuallynormalized data of user comments from the Tele-com domain.
We also want to thank Ozan ArkanCan for his valuable discussions and helps duringthe data preparation.ReferencesK?ubra Adal?
and G?uls?en Eryi?git.
2014.
Vowel anddiacritic restoration for social media texts.
In 5thWorkshop on Language Analysis for Social Media(LASM) at EACL, Gothenburg, Sweden, April.
As-sociation for Computational Linguistics.Ahmet Afsin Ak?n and Mehmet D?undar Ak?n.
2007.Zemberek, an open source nlp framework for turkiclanguages.
Structure.AiTi Aw, Min Zhang, Juan Xiao, and Jian Su.
2006.A phrase-based statistical model for sms text nor-malization.
In Proc.
of the COLING/ACL onMain conference poster sessions, COLING-ACL?06, pages 33?40, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Richard Beaufort, Sophie Roekhaut, Louise-Am?elieCougnon, and C?edrick Fairon.
2010.
A hybridrule/model-based finite-state framework for normal-izing sms messages.
In Proc.
of the 48th AnnualMeeting of the Association for Computational Lin-guistics, ACL ?10, pages 770?779, Stroudsburg, PA,USA.
Association for Computational Linguistics.Eleanor Clark and Kenji Araki.
2011.
Text normal-ization in social media: progress, problems and ap-plications for a pre-processing system of casual en-glish.
Procedia-Social and Behavioral Sciences,27:2?11.Paul Cook and Suzanne Stevenson.
2009.
Anunsupervised model for text message normaliza-tion.
In Proc.
of the Workshop on ComputationalApproaches to Linguistic Creativity, CALC ?09,pages 71?78, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.69Jacob Eisenstein.
2013.
Phonological factors in socialmedia writing.
In Proc.
of the Workshop on Lan-guage Analysis in Social Media, pages 11?19, At-lanta, Georgia, June.
Association for ComputationalLinguistics.G?uls?en Eryi?git, Fatih Samet C?etin, Meltem Yan?k,Tanel Temel, and?Iyas C?ic?ekli.
2013.
Turksent:A sentiment annotation tool for social media.
InProc.
of the 7th Linguistic Annotation Workshopand Interoperability with Discourse, pages 131?134,Sofia, Bulgaria, August.
Association for Computa-tional Linguistics.G?uls?en Eryi?git.
2014.
ITU Turkish NLP web service.In Proc.
of the Demonstrations at the 14th Confer-ence of the European Chapter of the Associationfor Computational Linguistics (EACL), Gothenburg,Sweden, April.
Association for Computational Lin-guistics.Dilek Z. Hakkani-T?ur, Kemal Oflazer, and G?okhan T?ur.2000.
Statistical morphological disambiguation foragglutinative languages.
In Proc.
of the 18th confer-ence on Computational linguistics - Volume 1, COL-ING ?00, pages 285?291, Stroudsburg, PA, USA.Association for Computational Linguistics.Bo Han and Timothy Baldwin.
2011.
Lexical normali-sation of short text messages: Makn sens a #twitter.In Proc.
of the 49th ACL HLT, pages 368?378, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Hany Hassan and Arul Menezes.
2013.
Social text nor-malization using contextual graph random walks.
InProc.
of the 51st ACL, pages 1577?1586, Sofia, Bul-garia, August.
Association for Computational Lin-guistics.Yuxiang Jia, Dezhi Huang, Wu Liu, Shiwen Yu, andHaila Wang.
2008.
Text normalization in Mandarintext-to-speech system.
In ICASSP, pages 4693?4696.
IEEE.Max Kaufmann and Jugal Kalita.
2010.
Syntactic nor-malization of Twitter messages.
In Proc.
of the 8thInternational Conference on Natural Language Pro-cessing (ICON 2010), Chennai, India.
Macmillan In-dia.Osama A Khan and Asim Karim.
2012.
A rule-basedmodel for normalization of sms text.
In Tools withArtificial Intelligence (ICTAI), 2012 IEEE 24th In-ternational Conference on, volume 1, pages 634?641.
IEEE.Fei Liu, Fuliang Weng, and Xiao Jiang.
2012.
Abroad-coverage normalization system for social me-dia language.
In Proc.
of the 50th ACL, pages 1035?1044, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Thu-Trang Thi Nguyen, Thanh Thi Pham, and Do-DatTran.
2010.
A method for vietnamese text normal-ization to improve the quality of speech synthesis.In Proc.
of the 2010 Symposium on Information andCommunication Technology, SoICT ?10, pages 78?85, New York, NY, USA.
ACM.Franz Josef Och and Hermann Ney.
2000.
Giza++:Training of statistical translation models.K Panchapagesan, Partha Pratim Talukdar, N SridharKrishna, Kalika Bali, and AG Ramakrishnan.
2004.Hindi text normalization.
In Fifth InternationalConference on Knowledge Based Computer Systems(KBCS), pages 19?22.
Citeseer.Deana Pennell and Yang Liu.
2011.
A character-levelmachine translation approach for normalization ofsms abbreviations.
In IJCNLP, pages 974?982.Muhammet S?ahin, Umut Sulubacak, and G?uls?enEryi?git.
2013.
Redefinition of turkish morphologyusing flag diacritics.
In Proc.
of The Tenth Sym-posium on Natural Language Processing (SNLP-2013), Phuket, Thailand, October.Has?im Sak, Tunga G?ung?or, and Murat Sarac?lar.
2011.Resources for Turkish morphological processing.Lang.
Resour.
Eval., 45(2):249?261, May.Ruhi Sarikaya, Katrin Kirchhoff, Tanja Schultz, andDilek Hakkani-Tur.
2009.
Introduction to the spe-cial issue on processing morphologically rich lan-guages.
Trans.
Audio, Speech and Lang.
Proc.,17(5):861?862, July.Bilge Say, Deniz Zeyrek, Kemal Oflazer, and Umut?Ozge.
2002.
Development of a corpus and a tree-bank for present-day written Turkish.
In Proc.
of theEleventh International Conference of Turkish Lin-guistics, Famaguste, Cyprus, August.G?okhan Ak?n S?eker and G?uls?en Eryi?git.
2012.
Initialexplorations on using CRFs for Turkish named en-tity recognition.
In Proc.
of COLING 2012, Mum-bai, India, 8-15 December.Reut Tsarfaty, Djam?e Seddah, Sandra K?ubler, andJoakim Nivre.
2013.
Parsing morphologically richlanguages: Introduction to the special issue.
Com-putational Linguistics, 39(1):15?22.Pidong Wang and Hwee Tou Ng.
2013.
A beam-search decoder for normalization of social mediatext with application to machine translation.
InProc.
of NAACL-HLT, pages 471?481.Deniz Y?uret and Michael de la Maza.
2006.
Thegreedy prepend algorithm for decision list induc-tion.
In Proc.
of the 21st international conferenceon Computer and Information Sciences, ISCIS?06,pages 37?46, Berlin, Heidelberg.
Springer-Verlag.Congle Zhang, Tyler Baldwin, Howard Ho, BennyKimelfeld, and Yunyao Li.
2013.
Adaptive parser-centric text normalization.
In Proc.
of the 51st ACL,pages 1159?1168, Sofia, Bulgaria, August.
Associa-tion for Computational Linguistics.70
