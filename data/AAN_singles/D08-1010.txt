Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 89?97,Honolulu, October 2008. c?2008 Association for Computational LinguisticsMaximum Entropy based Rule Selection Model forSyntax-based Statistical Machine TranslationQun Liu1 and Zhongjun He1,2 and Yang Liu1 and Shouxun Lin11Key Laboratory of Intelligent Information ProcessingInstitute of Computing Technology, Chinese Academy of SciencesBeijing, 100190, China2Graduate University of Chinese Academy of SciencesBeijing, 100049, China{liuqun,zjhe,yliu,sxlin}@ict.ac.cnAbstractThis paper proposes a novel maximum en-tropy based rule selection (MERS) modelfor syntax-based statistical machine transla-tion (SMT).
The MERS model combines lo-cal contextual information around rules andinformation of sub-trees covered by variablesin rules.
Therefore, our model allows the de-coder to perform context-dependent rule se-lection during decoding.
We incorporate theMERS model into a state-of-the-art linguis-tically syntax-based SMT model, the tree-to-string alignment template model.
Experi-ments show that our approach achieves signif-icant improvements over the baseline system.1 IntroductionSyntax-based statistical machine translation (SMT)models (Liu et al, 2006; Galley et al, 2006; Huanget al, 2006) capture long distance reorderings by us-ing rules with structural and linguistical informationas translation knowledge.
Typically, a translationrule consists of a source-side and a target-side.
How-ever, the source-side of a rule usually correspondsto multiple target-sides in multiple rules.
Therefore,during decoding, the decoder should select a correcttarget-side for a source-side.
We call this rule selec-tion.Rule selection is of great importance to syntax-based SMT systems.
Comparing with word selec-tion in word-based SMT and phrase selection inphrase-based SMT, rule selection is more genericand important.
This is because that a rule not onlycontains terminals (words or phrases), but also con-NPDNPNPX 1DEGNPBNNX 2NNNPDNPNPX 1DEGNPBNNX 2NNX 1 X 2 levels X 2 standard of X 1Figure 1: Example of translation rulestains nonterminals and structural information.
Ter-minals indicate lexical translations, while nontermi-nals and structural information can capture short orlong distance reorderings.
See rules in Figure 1 forillustration.
These two rules share the same syntactictree on the source side.
However, on the target side,either the translations for terminals or the phrase re-orderings for nonterminals are quite different.
Dur-ing decoding, when a rule is selected and applied to asource text, both lexical translations (for terminals)and reorderings (for nonterminals) are determined.Therefore, rule selection affects both lexical transla-tion and phrase reordering.However, most of the current syntax-based sys-tems ignore contextual information when they se-lecting rules during decoding, especially the infor-mation of sub-trees covered by nonterminals.
Forexample, the information of X 1 and X 2 is notrecorded when the rules in Figure 1 extracted fromthe training examples in Figure 2.
This makes thedecoder hardly distinguish the two rules.
Intuitively,information of sub-trees covered by nonterminals aswell as contextual information of rules are believed89NPDNPX 1 :NP DEGNPBX 2 :NN NNNPDNPX 1 :NP DEGNPBX 2 :NN NNindustrial products manufacturing levels overall standard of the matchFigure 2: Training examples for rules in Figure 1to be helpful for rule selection.Recent research showed that contextual infor-mation can help perform word or phrase selec-tion.
Carpuat and Wu (2007b) and Chan etal.
(2007) showed improvents by integrating word-sense-disambiguation (WSD) system into a phrase-based (Koehn, 2004) and a hierarchical phrase-based (Chiang, 2005) SMT system, respectively.Similar to WSD, Carpuat and Wu (2007a) used con-textual information to solve the ambiguity prob-lem for phrases.
They integrated a phrase-sense-disambiguation (PSD) model into a phrase-basedSMT system and achieved improvements.In this paper, we propose a novel solution forrule selection for syntax-based SMT.
We use themaximum entropy approach to combine rich con-textual information around a rule and the informa-tion of sub-trees covered by nonterminals in a rule.For each ambiguous source-side of translation rules,a maximum entropy based rule selection (MERS)model is built.
Thus the MERS models can help thedecoder to perform a context-dependent rule selec-tion.Comparing with WSD (or PSD), there are someadvantages of our approach:?
Our approach resolves ambiguity for rules withmulti-level syntactic structure, while WSD re-solves ambiguity for strings that have no struc-tures;?
Our approach can help the decoder performboth lexical selection and phrase reorderings,while WSD can help the decoder only performlexical selection;?
Our method takes WSD as a special case, sincea rule may only consists of terminals.In our previous work (He et al, 2008), we re-ported improvements by integrating a MERS modelinto a formally syntax-based SMT model, the hier-archical phrase-based model (Chiang, 2005).
In thispaper, we incorporate the MERS model into a state-of-the-art linguistically syntax-based SMT model,the tree-to-string alignment template (TAT) model(Liu et al, 2006).
The basic differences are:?
The MERS model here combines rich informa-tion of source syntactic tree as features sincethe translation model is linguistically syntax-based.
He et al (2008) did not use this in-formation.?
In this paper, we build MERS models for allambiguous source-sides, including lexicalized(source-side which only contains terminals),partially lexicalized (source-side which con-tains both terminals and nonterminals), and un-lexicalized (source-side which only containsnonterminals).
He et al (2008) only builtMERS models for partially lexicalized source-sides.In the TAT model, a TAT can be considered as atranslation rule which describes correspondence be-tween source syntactic tree and target string.
TATcan capture linguistically motivated reorderings atshort or long distance.
Experiments show that byincorporating MERS model, the baseline systemachieves statistically significant improvement.This paper is organized as follows: Section 2reviews the TAT model; Section 3 introduces theMERS model and describes feature definitions; Sec-tion 4 demonstrates a method to incorporate theMERS model into the translation model; Section 5reports and analyzes experimental results; Section 6gives conclusions.2 Baseline SystemOur baseline system is Lynx (Liu et al, 2006),which is a linguistically syntax-based SMT system.For translating a source sentence fJ1 = f1...fj ...fJ ,Lynx firstly employs a parser to produce a sourcesyntactic tree T (fJ1 ), and then uses the sourcesyntactic tree as the input to search translations:90e?I1 = argmaxe?I1Pr(eI1|fJ1 )(1)= argmaxe?I1Pr(T (fJ1 )|fJ1 )Pr(eI1|T (fJ1 ))In doing this, Lynx uses tree-to-string alignmenttemplate to build relationship between source syn-tactic tree and target string.
A TAT is actually atranslation rule: the source-side is a parser tree withleaves consisting of words and nonterminals, thetarget-side is a target string consisting of words andnonterminals.TAT can be learned from word-aligned, source-parsed parallel corpus.
Figure 4 shows three typesof TATs extracted from the training example in Fig-ure 3: lexicalized (the left), partially lexicalized(the middle), unlexicalized (the right).
LexicalizedTAT contains only terminals, which is similar tophrase-to-phrase translation in phrase-based modelexcept that it is constrained by a syntactic tree on thesource-side.
Partially lexicalized TAT contains bothterminals and non-terminals, which can be used forboth lexical translation and phrase reordering.
Un-lexicalized TAT contains only nonterminals and canonly be used for phrase reordering.Lynx builds translation model in a log-linearframework (Och and Ney, 2002):P (eI1|T (fJ1 )) =(2)exp[?m ?mhm(eI1, T (fJ1 ))]?e?
exp[?m ?mhm(eI1, T (fJ1 ))]Following features are used:?
Translation probabilities: P (e?|T? )
and P (T?
|e?);?
Lexical weights: Pw(e?|T? )
and Pw(T?
|e?);?
TAT penalty: exp(1), which is analogous tophrase penalty in phrase-based model;?
Language model Plm(eI1);?
Word penalty I .In Lynx, rule selection mainly depends on trans-lation probabilities and lexical weights.
These fourscores describe how well a source tree links to a tar-get string, which are estimated on the training cor-pus according to occurrence times of e?
and T?
.
ThereIPNPBNN NN NNVPVV VPBVVThe incomes of city and village resident continued to growFigure 3: Word-aligned, source-parsed training example.NN NPBNNX 1NN NNNPBNNX 1NNX 2NNX 3city and village incomes of X 1 resident X 3 X 1 X 2Figure 4: TATs learned from the training example in Fig-ure 3.are no features in Lynx that can capture contextualinformation during decoding, except for the n-gramlanguage model which considers the left and rightneighboring n-1 target words.
But this informationit very limited.3 The Maximum Entropy based RuleSelection Model3.1 The modelIn this paper, we focus on using contextual infor-mation to help the TAT model perform context-dependent rule selection.
We consider the rule se-lection task as a multi-class classification task: fora source syntactic tree T?
, each corresponding targetstring e?
is a label.
Thus during decoding, when aTAT ?T?
, e???
is selected, T?
is classified into label e?
?,actually.A good way to solve the classification problem isthe maximum entropy approach:Prs(e?|T?
, T (Xk)) =(3)exp[?i ?ihi(e?, C(T?
), T (Xk))]?e?
?exp[?i ?ihi(e?
?, C(T?
), T (Xk))]91where T?
and e?
are the source tree and target string ofa TAT, respectively.
hi is a binary feature functionsand ?i is the feature weight of hi.
C(T? )
defines localcontextual information of T?
.
Xk is a nonterminal inthe source tree T?
, where k is an index.
T (Xk) is thesource sub-tree covered by Xk.The advantage of the MERS model is that it usesrich contextual information to compute posteriorprobability for e?
given T?
.
However, the transla-tion probabilities and lexical weights in Lynx ignorethese information.Note that for each ambiguous source tree, webuild a MERS model.
That means, if there areN source trees extracted from the training corpusare ambiguous (the source tree which correspondsto multiple translations), thus for each ambiguoussource tree Ti (i = 1, ..., N ), a MERS model Mi(i = 1, ..., N ) is built.
Since a source tree may cor-respond to several hundreds of target translations atmost, the feature space of a MERS model is not pro-hibitively large.
Thus the complexity for training aMERS model is low.3.2 Feature DefinitionLet ?T?
, e??
be a translation rule in the TAT model.We use f(T? )
to represent the source phrase coveredby T?
.
To build a MERS model for the source tree T?
,we explore various features listed below.1.
Lexical Features (LF)These features are defined on source words.Specifically, there are two kinds of lexical fea-tures: external features f?1 and f+1, whichare the source words immediately to the leftand right of f(T?
), respectively; internal fea-tures fL(T (Xk)) and fR(T (Xk)), which arethe left most and right most boundary words ofthe source phrase covered by T (Xk), respec-tively.See Figure 5 (a) for illustration.
Inthis example, f?1=t?
?ga?o, f+1=zh?`za`o,fL(T (X1))=go?ngye`, fR(T (X1))=cha?np??n.2.
Parts-of-speech (POS) Features (POSF)These features are the POS tags of the sourcewords defined in the lexical features: P?1,P+1, PL(T (Xk)), PR(T (Xk)) are the POStags of f?1, f+1, fL(T (Xk)), fR(T (Xk)), re-VPVVt?
?ga?oDNPX 1 :NPNNgo?ngye`NNcha?np?
?nDEGdeNPBNNzh?`za`o(a) Lexical FeaturesVPVVt?
?ga?oDNPX 1 :NPNNgo?ngye`NNcha?np?
?nDEGdeNPBNNzh?`za`o(b) POS FeaturesDNPX 1 :NP2 wordsDEGdeNPDNPX 1 :NP DEGde(c) Span Feature (d) Parent FeatureNPDNPX 1 :NP DEGdeNPB(e) Sibling FeatureFigure 5: Illustration of features of theMERSmodel.
Thesource tree of the TAT is ?
DNP(NP X 1 ) (DEG de)?.Gray nodes denote information included in the feature.92spectively.
POS tags can generalize over alltraining examples.Figure 5 (b) shows POS features.
P?1=VV,P+1=NN, PL(T (X1))=NN, PR(T (X1))=NN.3.
Span Features (SPF)These features are the length of the sourcephrase f(T (Xk)) covered by T (Xk).
In Liu?sTATmodel, the knowledge learned from a shortspan can be used for a larger span.
This is notreliable.
Thus we use span features to allow theMERS model to learn a preference for short orlarge span.In Figure 5 (c), the span of X 1 is 2.4.
Parent Feature (PF)The parent node of T?
in the parser tree of thesource sentence.
The same source sub-tree mayhave different parent nodes in different trainingexamples.
Therefore, this feature may provideinformation for distinguishing source sub-trees.Figure 5 (d) shows that the parent is a NP node.5.
Sibling Features (SBF)The siblings of the root of T?
.
This feature con-siders neighboring nodes which share the sameparent node.In Figure 5 (e), the source tree has one siblingnode NPB.Those features make use of rich informationaround a rule, including the contextual informationof a rule and the information of sub-trees coveredby nonterminals.
They are never used in Liu?s TATmodel.Figure 5 shows features for a partially lexicalizedsource tree.
Furthermore, we also build MERS mod-els for lexicalized and unlexicalized source trees.Note that for lexicalized tree, features do not includethe information of sub-trees since there is no nonter-minals.The features can be easily obtained by modify-ing the TAT extraction algorithm described in (Liuet al, 2006).
When a TAT is extracted from aword-aligned, source-parsed parallel sentence, wejust record the contextual features and the features ofthe sub-trees.
Then we use the toolkit implementedby Zhang (2004) to train MERS models for the am-biguous source syntactic trees separately.
We set theiteration number to 100 and Gaussian prior to 1.4 Integrating the MERS Models into theTranslation ModelWe integrate the MERS models into the TAT modelduring the translation of each source sentence.
Thusthe MERS models can help the decoder performcontext-dependent rule selection during decoding.For integration, we add two new features into thelog-linear translation model:?
Prs(e?|T?
, T (Xk)).
This feature is computed bythe MERS model according to equation (3),which gives a probability that the model select-ing a target-side e?
given an ambiguous source-side T?
, considering rich contextual informa-tion.?
Pap = exp(1).
During decoding, if a sourcetree has multiple translations, this feature is setto exp(1), otherwise it is set to exp(0).
Sincethe MERS models are only built for ambiguoussource trees, the first feature Prs(e?|T?
, T (Xk))for non-ambiguous source tree will be set to1.0.
Therefore, the decoder will prefer touse non-ambiguous TATs.
However, non-ambiguous TATs usually occur only once in thetraining corpus, which are not reliable.
Thuswe use this feature to reward ambiguous TATs.The advantage of our integration is that we neednot change the main decoding algorithm of Lynx.Furthermore, the weights of the new features can betrained together with other features of the translationmodel.5 Experiments5.1 CorpusWe carry out experiments on Chinese-to-Englishtranslation.
The training corpus is the FBIS cor-pus, which contains 239k sentence pairs with 6.9MChinese words and 8.9M English words.
For thelanguage model, we use SRI Language ModelingToolkit (Stolcke, 2002) with modified Kneser-Neysmoothing (Chen and Goodman, 1998) to train twotri-gram language models on the English portion of93No.
of No.
of No.
of ambiguousTypeTATs source trees source trees% ambiguousLexicalized 333,077 16,367 14,380 87.86Partially Lexicalized 342,767 38,497 28,397 73.76Unlexicalized 83,024 7,384 5,991 81.13Total 758,868 62,248 48,768 78.34Table 1: Statistical information of TATs filtered by test sets of NIST MT 2003 and 2005.SystemFeaturesP (e?|T? )
P (T?
|e?)
Pw(e?|T? )
Pw(T?
|e?)
lm1 lm2 TP WP Prs APLynx 0.210 0.016 0.081 0.051 0.171 0.013 -0.055 0.403 - -+MERS 0.031 0.008 0.020 0.080 0.152 0.014 0.027 0.270 0.194 0.207Table 2: Feature weights obtained by minimum error rate training on the development set.
The first 8 features are usedby Lynx.
TP=TAT penalty, WP=word penalty, AP=ambiguous TAT penalty.
Note that in fact, the positive weight forWP and AP indicate a reward.the training corpus and the Xinhua portion of the Gi-gaword corpus, respectively.
NIST MT 2002 test setis used as the development set.
NIST MT 2003 andNIST MT 2005 test sets are used as the test sets.The translation quality is evaluated by BLEU met-ric (Papineni et al, 2002), as calculated by mteval-v11b.pl with case-insensitive matching of n-grams,where n = 4.5.2 TrainingTo train the translation model, we first run GIZA++(Och and Ney, 2000) to obtain word alignment inboth translation directions.
Then the word alignmentis refined by performing ?grow-diag-final?
method(Koehn et al, 2003).
We use a Chinese parser de-veloped by Deyi Xiong (Xiong et al, 2005) to parsethe Chinese sentences of the training corpus.Our TAT extraction algorithm is similar to Liu etal.
(2006), except that we make some tiny modifica-tions to extract contextual features for MERS mod-els.
To extract TAT, we set the maximum height ofthe source sub-tree to h = 3, the maximum numberof direct descendants of a node of sub-tree to c = 5.See (Liu et al, 2006) for specific definitions of theseparameters.Table 1 shows statistical information of TATswhich are filtered by the two test sets.
For each type(lexicalized, partially lexicalized, unlexicalized) ofTATs, a great portion of the source trees are am-biguous.
The number of ambiguous source trees ac-counts for 78.34% of the total source trees.
This in-dicates that the TAT model faces serious rule selec-tion problem during decoding.5.3 ResultsWe use Lynx as the baseline system.
Then theMERS models are incorporated into Lynx, andthe system is called Lynx+MERS.
To run thedecoder, Lynx and Lynx+MERS share the samesettings: tatTable-limit=30, tatTable-threshold=0,stack-limit=100, stack-threshold=0.00001.
Themeanings of the pruning parameters are the same toLiu et al (2006).We perform minimum error rate training (Och,2003) to tune the feature weights for the log-linearmodel to maximize the systems?s BLEU score on thedevelopment set.
The weights are shown in Table 2.These weights are then used to run Lynx andLynx+MERS on the test sets.
Table 3 shows theresults.
Lynx obtains BLEU scores of 26.15 onNIST03 and 26.09 on NIST05.
Using all featuresdescribed in Section 3.2, Lynx+MERS finally ob-tains BLEU scores of 27.05 on NIST03 and 27.28on NIST05.
The absolute improvements is 0.90and 1.19, respectively.
Using the sign-test describedby Collins et al (2005), both improvements arestatistically significant at p < 0.01.
Moreover,Lynx+MERS also achieves higher n-gram preci-sions than Lynx.94Test Set System BLEU-4Individual n-gram precisions1 2 3 4NIST03Lynx 26.15 71.62 35.64 18.64 9.82+MERS 27.05 72.00 36.72 19.51 10.37NIST05Lynx 26.09 70.39 35.12 18.53 10.11+MERS 27.28 71.16 36.19 19.62 10.95Table 3: BLEU-4 scores (case-insensitive) on the test sets.5.4 AnalysisThe baseline system only uses four features forrule selection: the translation probabilities P (e?|T?
)and P (T?
|e?
); and the lexical weights Pw(e?|T? )
andPw(T?
|e?).
These features are estimated on the train-ing corpus by the maximum likelihood approach,which does not allow the decoder to perform a con-text dependent rule selection.
Although Lynx useslanguage model as feature, the n-gram languagemodel only considers the left and right n-1 neigh-boring target words.The MERS models combines rich contextual in-formation as features to help the decoder performrule selection.
Table 4 shows the effect of differentfeature sets.
We test two classes of feature sets: thesingle feature (the top four rows of Table 4) and thecombination of features (the bottom five rows of Ta-ble 4).
For the single feature set, the POS tags arethe most useful and stable features.
Using this fea-ture, Lynx+MERS achieves improvements on boththe test sets.
The reason is that POS tags can be gen-eralized over all training examples, which can alle-viate the data sparseness problem.Although we find that some single features mayhurt the BLEU score, they are useful in combina-tion of features.
This is because one of the strengthsof the maximum entropy model is that it can in-corporate various features to perform classification.Therefore, using all features defined in Section 3.2,we obtain statistically significant improvements (thelast row of Table 4).
In order to know how theMERS models improve translation quality, we in-spect the 1-best outputs of Lynx and Lynx+MERS.We find that the first way that theMERSmodels helpthe decoder is that they can perform better selectionfor words or phrases, similar to the effect of WSDor PSD.
This is because that lexicalized and partiallylexicalized TAT contains terminals.
Considering theFeature Sets NIST03 NIST05LF 26.12 26.32POSF 26.36 26.21PF 26.17 25.90SBF 26.47 26.08LF+POSF 26.61 26.59LF+POSF+SPF 26.70 26.44LF+POSF+PF 26.81 26.56LF+POSF+SBF 26.68 26.89LF+POSF+SPF+PF+SBF 27.05 27.28Table 4: BLEU-4 scores on different feature sets.following examples:?
Source:?
Reference: Malta is located in southern Eu-rope?
Lynx: Malta in southern Europe?
Lynx+MERS: Malta is located in southern Eu-ropeHere the Chinese word ?
?
is incor-rectly translated into ?in?
by the baseline system.Lynx+MERS produces the correct translation ?is lo-cated in?.
That is because, the MERS model consid-ers more contextual information for rule selection.In the MERS model, Prs(in| ) = 0.09, which issmaller than Prs(is located in| ) = 0.14.
There-fore, the MERS model prefers the translation ?is lo-cated in?.
Note that here the source tree (VV )is lexicalized, and the role of the MERS model isactually the same as WSD.The second way that the MERS models help thedecoder is that they can perform better phrase re-orderings.
Considering the following examples:95?
Source: [ ]1 [ ]2...?
Reference: According to its [developmentstrategy]2 [in the Chinese market]1 ...?
Lynx: Accordance with [the Chinese market]1[development strategy]2 ...?
Lynx+MERS: According to the [developmentstrategy]2 [in the Chinese market]1The syntactic tree of the Chinese phrase ??
is shown in Figure 6.
How-ever, there are two TATs which can be applied to thesource tree, as shown in Figure 7.
The baseline sys-tem selects the left TAT and produces a monotonetranslation of the subtrees ?X 1 :PP?
and ?X 2 :NPB?.However, Lynx+MERS uses the right TAT and per-forms correct phrase reordering by swapping the twosource phrases.
Here the source tree is partially lex-icalized, and both the contextual information andthe information of sub-trees covered by nontermi-nals are considered by the MERS model.6 ConclusionIn this paper, we propose a maximum entropy basedrule selection model for syntax-based SMT.
Weuse two kinds information as features: the local-contextual information of a rule, the information ofsub-trees matched by nonterminals in a rule.
Duringdecoding, these features allow the decoder to per-form a context-dependent rule selection.
However,this information is never used in most of the currentsyntax-based SMT models.The advantage of the MERS model is that it canhelp the decoder not only perform lexical selection,but also phrase reorderings.
We demonstrate oneway to incorporate the MERS models into a state-of-the-art linguistically syntax-based SMT model,the tree-to-string alignment model.
Experimentsshow that by incorporating the MERS models, thebaseline system achieves statistically significant im-provements.We find that rich contextual information can im-prove translation quality for a syntax-based SMTsystem.
In future, we will explore more sophisti-cated features for the MERS model.
Moreover, wewill test the performance of the MERS model onlarge scale corpus.NPDNPPP DEGNPBin Chinese market ofdevelopment strategyFigure 6: Syntactic tree of the source phrase ?
?.NPDNPPPX 1DEGNPBX 2NPDNPPPX 1DEGNPBX 2X 1 X 2 X 2 X 1Figure 7: TATs which can be used for the source phrase?
?.AcknowledgementsWe would like to thank Yajuan Lv for her valuablesuggestions.
This work was supported by the Na-tional Natural Science Foundation of China (NO.60573188 and 60736014), and the High TechnologyResearch and Development Program of China (NO.2006AA010108).ReferencesMarine Carpuat and Dekai Wu.
2007a.
How phrasesense disambiguation outperforms word sense disam-biguation for statistical machine translation.
In 11thConference on Theoretical and Methodological Issuesin Machine Translation, pages 43?52.Marine Carpuat and Dekai Wu.
2007b.
Improving sta-tistical machine translation using word sense disam-biguation.
In Proceedings of EMNLP-CoNLL 2007,pages 61?72.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word sense disambiguation improves statistical ma-chine translation.
In Proceedings of the 45th Annual96Meeting of the Association for Computational Linguis-tics, pages 33?40.Stanley F. Chen and Joshua Goodman.
1998.
An empir-ical study of smoothing techniques for language mod-eling.
Technical Report TR-10-98, Harvard UniversityCenter for Research in Computing Technology.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics, pages 263?270.M.
Collins, P. Koehn, and I. Kucerova.
2005.
Clause re-structuring for statistical machine translation.
In Proc.of ACL05, pages 531?540.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of COLING-ACL 2006, pages 961?968.Zhongjun He, Qun Liu, and Shouxun Lin.
2008.
Im-proving statistical machine translation using lexical-ized rule selection.
In Proceedings of the 22nd In-ternational Conference on Computational Linguistics(Coling 2008), pages 321?328.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of the 7th BiennialConference of the Association for Machine Translationin the Americas.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proceedings ofHLT-NAACL 2003, pages 127?133.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
In Proceedings of the Sixth Conference of theAssociation for Machine Translation in the Americas,pages 115?124.Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machine trans-lation.
In Proceedings of the 44th Annual Meeting ofthe Association for Computational Linguistics, pages609?616.Franz Josef Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics, pages 440?447.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics, pages 295?302.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics, pages 160?167.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics,pages 311?318.Andreas Stolcke.
2002.
Srilm ?
an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken language Processing,volume 2, pages 901?904.Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, andYueliang Qian.
2005.
Parsing the penn chinese tree-bank with semantic knowledge.
In Proceedings ofIJCNLP 2005, pages 70?81.Le Zhang.
2004.
Maximum entropy model-ing toolkit for python and c++.
available athttp://homepages.inf.ed.ac.uk/s0450736/maxent too-lkit.html.97
