Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 153?158,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsUnsupervised Word Alignment Using Frequency Constraint in PosteriorRegularized EMHidetaka Kamigaito1,2, Taro Watanabe2, Hiroya Takamura1, Manabu Okumura11Tokyo Institute of Technology, Precision and Intelligence Laboratory4259 Nagatsuta-cho Midori-ku Yokohama, Japan2National Institute of Information and Communication Technology3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, JapanAbstractGenerative word alignment models, suchas IBM Models, are restricted to one-to-many alignment, and cannot explicitlyrepresent many-to-many relationships ina bilingual text.
The problem is par-tially solved either by introducing heuris-tics or by agreement constraints such thattwo directional word alignments agreewith each other.
In this paper, we fo-cus on the posterior regularization frame-work (Ganchev et al., 2010) that can forcetwo directional word alignment modelsto agree with each other during train-ing, and propose new constraints that cantake into account the difference betweenfunction words and content words.
Ex-perimental results on French-to-Englishand Japanese-to-English alignment tasksshow statistically significant gains over theprevious posterior regularization baseline.We also observed gains in Japanese-to-English translation tasks, which prove theeffectiveness of our methods under gram-matically different language pairs.1 IntroductionWord alignment is an important component in sta-tistical machine translation (SMT).
For instancephrase-based SMT (Koehn et al., 2003) is basedon the concept of phrase pairs that are automat-ically extracted from bilingual data and rely onword alignment annotation.
Similarly, the modelfor hierarchical phrase-based SMT is built fromexhaustively extracted phrases that are, in turn,heavily reliant on word alignment.The Generative word alignment models, such asthe IBM Models (Brown et al., 1993) and HMM(Vogel et al., 1996), are popular methods for au-tomatically aligning bilingual texts, but are re-stricted to represent one-to-many correspondenceof each word.
To resolve this weakness, vari-ous symmetrization methods are proposed.
Ochand Ney (2003) and Koehn et al.
(2003) proposevarious heuristic methods to combine two direc-tional models to represent many-to-many relation-ships.
As an alternative to heuristic methods, fil-tering methods employ a threshold to control thetrade-off between precision and recall based ona score estimated from the posterior probabili-ties from two directional models.
Matusov et al.
(2004) proposed arithmetic means of two mod-els as a score for the filtering, whereas Liang etal.
(2006) reported better results using geometricmeans.
The joint training method (Liang et al.,2006) enforces agreement between two directionalmodels.
Posterior regularization (Ganchev et al.,2010) is an alternative agreement method whichdirectly encodes agreement during training.
DeN-ero and Macherey (2011) and Chang et al.
(2014)also enforce agreement during decoding.However, these agreement models do not takeinto account the difference in language pairs,which is crucial for linguistically different lan-guage pairs, such as Japanese and English: al-though content words may be aligned with eachother by introducing some agreement constraints,function words are difficult to align.We focus on the posterior regularization frame-work and improve upon the previous work byproposing new constraint functions that take intoaccount the difference in languages in terms ofcontent words and function words.
In particular,we differentiate between content words and func-tion words by frequency in bilingual data, follow-ing Setiawan et al.
(2007).Experimental results show that the proposedmethods achieved better alignment qualities on theFrench-English Hansard data and the Japanese-English Kyoto free translation task (KFTT) mea-sured by AER and F-measure.
In translation eval-uations, we achieved statistically significant gains153in BLEU scores in the NTCIR10.2 Statistical word alignment withposterior regularization frameworkGiven a bilingual sentence x = (xs,xt) where xsandxtdenote a source and target sentence, respec-tively, the bilingual sentence is aligned by a many-to-many alignment of y.
We represent posteriorprobabilities from two directional word alignmentmodels as??p?(?
?y |x) and??p?(?
?y |x) with each ar-row indicating a particular direction, and use ?
todenote the parameters of the models.
For instance,?
?y is a subset of y for the alignment from xstoxtunder the model of p(xt,?
?y |xs).
In the case ofIBM Model 1, the model is represented as follows:p(xt,?
?y |xs) =?i1|xs|+ 1pt(xti|xs??yi).
(1)where we define the index of xt, xsas i, j(1 ?i ?
|xt|, 1 ?
j ?
|xs|) and the posterior probabil-ity for the word pair (xti, xsj) is defined as follows:?
?p (i, j|x) =pt(xti|xsj)?j?pt(xti|xsj?).
(2)Herein, we assume that the posterior probabil-ity for wrong directional alignment is zero (i.e.,?
?p (?
?y |x) = 0).1Given the two directional mod-els, Ganchev et al.
defined a symmetric feature foreach target/source position pair, i, j as follows:?i,j(x,y) =??
?+1 (?
?y ?
y) ?
(?
?yi= j),?1 (?
?y ?
y) ?
(?
?yj= i),0 otherwise.
(3)The feature assigns 1 for the subset of word align-ment for?
?y , but assigns ?1 for?
?y .
As a result,if a word pair i, j is aligned with equal posteriorprobabilities in two directions, the expectation ofthe feature value will be zero.
Ganchev et al.
de-fined a joint model that combines two directionalmodels using arithmetic means:p?
(y|x) =12??p?
(y|x) +12??p?(y|x).
(4)Under the posterior regularization framework, weinstead use q that is derived by maximizing the fol-lowing posterior probability parametrized by ?
foreach bilingual data x as follows (Ganchev et al.,2010):q?
(y|x) =??p?(?
?y |x) +??p?(?
?y |x)2(5)?exp{??
?
?
(x,y)}Z1No alignment is represented by alignment into a specialtoken ?null?.=?
?q (?
?y |x)Z??q??p?
(x) +?
?q (?
?y |x)Z??q??p?
(x)2Z,Z =12(Z??q??p?+Z??q??p?),?
?q (?
?y |x) =1Z??q??p?(?
?y ,x)exp{??
?
?(x,y)},Z??q=???y??p?(?
?y ,x)exp{??
?
?(x,y)},?
?q (?
?y |x) =1Z??q??p?(?
?y ,x)exp{??
?
?
(x, y)},Z??q=???y??p?(?
?y ,x)exp{??
?
?
(x,y)},such that Eq?
[?i,j(x,y)] = 0.
In the E-step ofEM-algorithm, we employ q?
instead of p?
to ac-cumulate fractional counts for its use in the M-step.
?
is efficiently estimated by the gradient as-cent for each bilingual sentence x.
Note that pos-terior regularization is performed during parame-ter estimation, and not during testing.3 Posterior Regularization withFrequency ConstraintThe symmetric constraint method represented inEquation (3) assumes a strong one-to-one rela-tion for any word, and does not take into accountthe divergence in language pairs.
For linguisti-cally different language pairs, such as Japanese-English, content words may be easily aligned one-to-one, but function words are not always alignedtogether.
In addition, Japanese is a pro-drop lan-guage which can easily violate the symmetric con-straint when proper nouns in the English side haveto be aligned with a ?null?
word.
In addition, lowfrequency words may cause unreliable estimatesfor adjusting the weighing parameters ?.In order to solve the problem, we improveGanchev?s symmetric constraint so that it can con-sider the difference between content words andfunction words in each language.
In particular, wefollow the frequency-based idea of Setiawan et al.
(2007) that discriminates content words and func-tion words by their frequencies.
We propose con-straint features that take into account the differ-ence between content words and function words,determined by a frequency threshold.3.1 Mismatching constraintFirst, we propose a mismatching constraint thatpenalizes word alignment between content wordsand function words by decreasing the correspond-ing posterior probabilities.154The constraint is represented as f2c (function tocontent) constraint:?f2ci,j(x,y) = (6)????????????????
?+1 (?
?y ?
y) ?
(?
?yi= j) ?
((xti?
Ct?
xsj?
Fs)?(xti?
Ft?
xsj?
Cs)) ?
(?i,j(x,y) > 0),0 (?
?y ?
y) ?
(?
?yj= i) ?
((xti?
Ct?
xsj?
Fs)?(xti?
Ft?
xsj?
Cs)) ?
(?i,j(x,y) > 0),0 (?
?y ?
y) ?
(?
?yi= j) ?
((xti?
Ct?
xsj?
Fs)?(xti?
Ft?
xsj?
Cs)) ?
(?i,j(x,y) < 0),?1 (?
?y ?
y) ?
(?
?yj= i) ?
((xti?
Ct?
xsj?
Fs)?(xti?
Ft?
xsj?
Cs)) ?
(?i,j(x,y) < 0).where ?i,j(x,y) =??p?
(i, j|x) ???p?
(i, j|x) isthe difference in the posterior probabilities be-tween the source-to-target and the target-to-sourcealignment.
Csand Ctrepresent content words inthe source sentence and target sentence, respec-tively.
Similarly, Fsand Ftare function wordsin the source and target sentence, respectively.
In-tuitively, when there exists a mismatch in contentword and function word for a word pair (i, j), theconstraint function returns a non-zero value forthe model with the highest posterior probability.When coupled with the constraint such that the ex-pectation of the feature value is zero, the constraintfunction decreases the posterior probability of thehighest direction and discourages agreement witheach other.Note that when this constraint is not fired, wefall back to the constraint function in Equation (3)for each word pair.3.2 Matching constraintIn contrast to the mismatching constraint, oursecond constraint function rewards alignment forfunction to function word matching, namely f2f.The f2f constraint function is defined as follows:?f2fi,j(x,y) = (7)????????????????
?+1 (?
?y ?
y) ?
(?
?yi= j)?(xti?
Ft?
xsj?
Fs) ?
(?i,j(x,y) > 0),0 (?
?y ?
y) ?
(?
?yj= i)?(xti?
Ft?
xsj?
Fs) ?
(?i,j(x,y) > 0),0 (?
?y ?
y) ?
(?
?yi= j)?(xti?
Ft?
xsj?
Fs) ?
(?i,j(x,y) < 0),?1 (?
?y ?
y) ?
(?
?yj= i)?(xti?
Ft?
xsj?
Fs) ?
(?i,j(x,y) < 0).This constraint function returns a non-zero valuefor a word pair (i, j) when they are functionwords.
As a result, the pair of function wordsare encouraged to agree with each other, but notother pairs.
The content to content word matchingfunction c2c can be defined similarly by replac-ing Fsand Ftby Csand Ct, respectively.
Like-wise, the function to content word matching func-tion f2c is defined by considering the matchingof content words and function words in two lan-guages.
As noted in the mismatch function, whenno constraint is fired, we fall back to Eq (3) foreach word pair.4 Experiment4.1 Experimental SetupThe data sets used in our experiments are theFrench-English Hansard Corpus, and two data setsfor Japanese-English tasks: the Kyoto free trans-lation task (KFTT) and NTCIR10.
The HansardCorpus consists of parallel texts drawn from of-ficial records of the proceedings of the CanadianParliament.
The KFTT (Neubig, 2011) is derivedfrom Japanese Wikipedia articles related to Ky-oto, which is professionally translated into En-glish.
NTCIR10 comes from patent data employedin a machine translation shared task (Goto et al.,2013).
The statistics of these data are presented inTable 1.Sentences of over 40 words on both source andtarget sides are removed for training alignmentmodels.
We used a word alignment toolkit ci-cada2for training the IBM Model 4 with ourproposed methods.
Training is bootstrapped fromIBM Model 1, followed by HMM and IBM Model4.
When generating the final bidirectional wordalignment, we use a grow-diag-final heuristic forthe Japanese-English tasks and an intersectionheuristic in the French-English task, judged bypreliminary studies.Following Bisazza and Federico (2012), weautomatically decide the threshold for word fre-quency to discriminate between content words andfunction words.
Specifically, the threshold is de-termined by the ratio of highly frequent words.The threshold th is the maximum frequency thatsatisfies the following equation:?w?
(freq(w)>th)freq(w)?w?allfreq(w)> r. (8)Here, we empirically set r = 0.5 by preliminarystudies.
This method is based on the intuition thatcontent words and function words exist in a docu-ment at a constant rate.4.2 Word alignment evaluationWe measure the impact of our proposed meth-ods on the quality of word alignment measured2https://github.com/tarowatanabe/cicada155Table 1: The statistics of the data setshansard kftt NTCIR10French English Japanese English Japanese Englishtrain sentence 1.13M 329.88K 2.02Mword 23.3M 19.8M 6.08M 5.91M 53.4M 49.4Mvocabulary 78.1K 57.3K 114K 138K 114K 183Kdev sentence 1.17K 2Kword 26.8K 24.3K 73K 67.3Kvocabulary 4.51K 4.78K 4.38K 5.04Ktest WA sentence 447 582word 7.76K 7.02K 14.4K 12.6Kvocabulary 1,92K 1.69K 2.57K 2.65KTR sentence 1.16K 8.6Kword 28.5K 26.7K 334K 310Kvocabulary 4.91K 4.57K 10.4K 12.7KFigure 1: Precision Recall graph in HansardFrench-EnglishFigure 2: Precision Recall graph in KFTTFigure 3: AER in Hansard French-English Figure 4: AER in KFTT156Table 2: Results of word alignment evaluation with the heuristics-based method (GDF)KFTT Hansard (French-English)method precision recall AER F precision recall AER Fsymmetric 0.4595 0.5942 48.18 0.5182 0.7029 0.8816 7.29 0.7822f2f 0.4633 0.5997 47.73 0.5227 0.7042 0.8851 7.29 0.7844c2c 0.4606 0.5964 48.02 0.5198 0.7001 0.8816 7.34 0.7804f2c 0.4630 0.5998 47.74 0.5226 0.7037 0.8871 7.10 0.7848by AER and F-measure (Och and Ney, 2003).Since there exists no distinction for sure-possiblealignments in the KFTT data, we use only surealignment for our evaluation, both for the French-English and the Japanese-English tasks.
Table 2summarizes our results.The baseline method is symmetric constraint(Ganchev et al., 2010) shown in Table 2.
The num-bers in bold and in italics indicate the best scoreand the second best score, respectively.
The dif-ferences between f2f,f2c and baseline in KFTT arestatistically significant at p < 0.05 using the sign-test, but in hansard corpus, there exist no signifi-cant differences between the baseline and the pro-posed methods.
In terms of F-measure, it is clearthat the f2f method is the most effective methodin KFTT, and both f2f and f2c methods exceed theoriginal posterior regularized model of Ganchev etal.
(2010).We also compared these methods with filteringmethods (Liang et al., 2006), in addition to heuris-tic methods.
We plot precision/recall curves andAER by varying the threshold between 0.1 and0.9 with 0.1 increments.
From Figures, it can beseen that our proposed methods are superior tothe baseline in terms of both precision-recall andAER.4.3 Translation evaluationNext, we performed a translation evaluation, mea-sured by BLEU (Papineni et al., 2002).
Wecompared the grow-diag-final and filtering method(Liang et al., 2006) for creating phrase tables.The threshold for the filtering factor was set to0.1 which was the best setting in the word align-ment experiment in section 4.2 under KFTT.
Fromthe English side of the training data, we trained aword using the 5-gram model with SRILM (Stol-cke and others, 2002).
?Moses?
toolkit was usedas a decoder (Koehn et al., 2007) and the modelparameters were tuned by k-best MIRA (Cherryand Foster, 2012).
In order to avoid tuning insta-bility, we evaluated the average of five runs (Hop-kins and May, 2011).
The results are summarizedTable 3: Results of translation evaluationKFTT NTCIR10GDF Filtered GDF Filteredsymmetric 19.06 19.28 28.3 29.71f2f 19.15 19.17 28.36 29.74c2c 19.26 19.02 28.36 29.92f2c 18.91 19.20 28.36 29.67in Table 3.
Our proposed methods achieved largegains in NTCIR10 task with the filtered method,but observed no gain in the KFTT with the filteredmethod.
In NTCIR10 task with GDF, the gain inBLEU was smaller than that of KFTT.
We cal-culate p-values and the difference between sym-metric and c2c (the most effective proposed con-straint) are lower than 0.05 in kftt with GDF andNTCIR10 with filtered method.
There seems tobe no clear tendency in the improved alignmentqualities and the translation qualities, as shown innumerous previous studies (Ganchev et al., 2008).5 ConclusionIn this paper, we proposed new constraint func-tions under the posterior regularization frame-work.
Our constraint functions introduce afine-grained agreement constraint considering thefrequency of words, a assuming that the highfrequency words correspond to function wordswhereas the less frequent words may be treatedas content words, based on the previous work ofSetiawan et al.
(2007).
Experiments on wordalignment tasks showed better alignment quali-ties measured by F-measure and AER on both theHansard task and KFTT.
We also observed largegain in BLEU, 0.2 on average, when comparedwith the previous posterior regularization methodunder NTCIR10 task.As our future work, we will investigate moreprecise methods for deciding function words andcontent words for better alignment and translationqualities.157ReferencesArianna Bisazza and Marcello Federico.
2012.
Cuttingthe long tail: Hybrid language models for translationstyle adaptation.
In Proceedings of the 13th Confer-ence of the European Chapter of the Association forComputational Linguistics, pages 439?448.
Associ-ation for Computational Linguistics.Peter F Brown, Vincent J Della Pietra, Stephen A DellaPietra, and Robert L Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational linguistics, 19(2):263?311.Yin-Wen Chang, Alexander M. Rush, John DeNero,and Michael Collins.
2014.
A constrained viterbirelaxation for bidirectional word alignment.
In Pro-ceedings of the 52nd Annual Meeting of the Associa-tion for Computational Linguistics (Volume 1: LongPapers), pages 1481?1490, Baltimore, Maryland,June.
Association for Computational Linguistics.Colin Cherry and George Foster.
2012.
Batch tun-ing strategies for statistical machine translation.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 427?436.
Association for Computational Lin-guistics.John DeNero and Klaus Macherey.
2011.
Model-based aligner combination using dual decomposi-tion.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 420?429, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Kuzman Ganchev, Jo?ao V. Grac?a, and Ben Taskar.2008.
Better alignments = better translations?In Proceedings of ACL-08: HLT, pages 986?993,Columbus, Ohio, June.
Association for Computa-tional Linguistics.Kuzman Ganchev, Joao Grac?a, Jennifer Gillenwater,and Ben Taskar.
2010.
Posterior regularization forstructured latent variable models.
The Journal ofMachine Learning Research, 99:2001?2049.Isao Goto, Ka Po Chow, Bin Lu, Eiichiro Sumita, andBenjamin K Tsou.
2013.
Overview of the patentmachine translation task at the ntcir-10 workshop.In Proceedings of the 10th NTCIR Workshop Meet-ing on Evaluation of Information Access Technolo-gies: Information Retrieval, Question Answeringand Cross-Lingual Information Access, NTCIR-10.Mark Hopkins and Jonathan May.
2011.
Tuning asranking.
In Proceedings of the 2011 Conference onEmpirical Methods in Natural Language Process-ing, pages 1352?1362, Edinburgh, Scotland, UK.,July.
Association for Computational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 48?54.
Association for Computa-tional Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 177?180.
Association for Computational Lin-guistics.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of the HumanLanguage Technology Conference of the NAACL,Main Conference, pages 104?111, New York City,USA, June.
Association for Computational Linguis-tics.E.
Matusov, R. Zens, and H. Ney.
2004.
SymmetricWord Alignments for Statistical Machine Transla-tion.
In Proceedings of COLING 2004, pages 219?225, Geneva, Switzerland, August 23?27.Graham Neubig.
2011.
The Kyoto free translationtask.
http://www.phontron.com/kftt.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting on association for compu-tational linguistics, pages 311?318.
Association forComputational Linguistics.Hendra Setiawan, Min-Yen Kan, and Haizhou Li.2007.
Ordering phrases with function words.
InProceedings of the 45th annual meeting on associ-ation for computational linguistics, pages 712?719.Association for Computational Linguistics.Andreas Stolcke et al.
2002.
Srilm-an extensible lan-guage modeling toolkit.
In INTERSPEECH.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
Hmm-based word alignment in statisticaltranslation.
In Proceedings of the 16th conferenceon Computational linguistics-Volume 2, pages 836?841.
Association for Computational Linguistics.158
