Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 756?766,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsGrounded Semantic Parsing for Complex Knowledge ExtractionAnkur P. Parikh?School of Computer ScienceCarnegie Mellon Universityapparikh@cs.cmu.eduHoifung Poon Kristina ToutanovaMicrosoft ResearchRedmond, WA, USAhoifung,kristout@microsoft.comAbstractRecently, there has been increasing interest inlearning semantic parsers with indirect super-vision, but existing work focuses almost ex-clusively on question answering.
Separately,there have been active pursuits in leveragingdatabases for distant supervision in informa-tion extraction, yet such methods are oftenlimited to binary relations and none can han-dle nested events.
In this paper, we gener-alize distant supervision to complex knowl-edge extraction, by proposing the first ap-proach to learn a semantic parser for extract-ing nested event structures without annotatedexamples, using only a database of such com-plex events and unannotated text.
The key ideais to model the annotations as latent variables,and incorporate a prior that favors semanticparses containing known events.
Experimentson the GENIA event extraction dataset showthat our approach can learn from and extractcomplex biological pathway events.
More-over, when supplied with just five examplewords per event type, it becomes competitiveeven among supervised systems, outperform-ing 19 out of 24 teams that participated in theoriginal shared task.1 IntroductionThe goal of semantic parsing is to map text intoa complete and detailed meaning representation(Mooney, 2007).
Supervised approaches for learn-ing a semantic parser require annotated examples,?This research was conducted during the author?s intern-ship at Microsoft Research.InformationExtractionID TYPE CAUSE THEME TRIGGERT1 PROTEIN - - BCLT2 PROTEIN - - RFLATT3 PROTEIN - - IL-10E1 POS-REG T1 E2 stimulatesE2 NEG-REG T3 T2 inhibition(NEG-REG,BCL,RFLAT)(NEG-REG,IL-10,RFLAT)Complex Event (POS-REG,BCL,(NEG-REG,IL-10,RFLAT))GENIA Event AnnotationFigure 1: Given sentence ?BCL stimulates inhibitionof RFLAT by IL-10?, information extraction focuses onclassifying simple relations among entities (top), whereasideally we want to extract the complex event that capturesimportant contextual information (middle), as exempli-fied by the GENIA event annotation (bottom).which are expensive and time-consuming to acquire(Zelle and Mooney, 1993; Zettlemoyer and Collins,2005; Zettlemoyer and Collins, 2007).
As a re-sult, there has been rising interest in learning se-mantic parsers from indirect supervision.
Examplesinclude unsupervised approaches that leverage dis-tributional similarity by recursive clustering (Poonand Domingos, 2009; Poon and Domingos, 2010;Titov and Klementiev, 2011), semi-supervised ap-proaches that learn from dialog context (Artzi andZettlemoyer, 2011), grounded approaches that learnfrom annotated question-answer pairs (Clarke et al,2010; Liang et al, 2011) or virtual worlds (Chen andMooney, 2011; Artzi and Zettlemoyer, 2013).Such progress is exciting, but most applicationsfocus on question answering, where the semanticparser is used to convert natural-language questionsinto formal queries.
In contrast, complex knowl-edge extraction represents a relatively untapped ap-756(a) (b) (c)(POS-REG,BCL,(NEG-REG,IL-10,RFLAT))(NEG-REG,TP53,(POS -REG,BCL,IL-2))(POS-REG,AKT2,(POS -REG,IL-4,ERBB2))(NEG-REG,(POS-REG,BCL,IL-2),BRAF)??
BCL stimulates inhibition of RFLAT by IL-10.
The ability of IL-10 to block RFLAT requires BCL.stimulatesinhibitionIL-10BCLRFLATTHEMECAUSEPOS-REGCAUSETHEMENEG-REGrequiresBCLabilityRFLATTHEME CAUSEPOS-REGCAUSETHEMENEG-REGRAISEIL-10blockThe NEG-REGRAISINGNULLFigure 2: Grounded semantic parsing for complex knowledge extraction: (a) input database of complex events, withouttextual annotation; (b) event extraction as semantic parsing; (c) a complex sentence that requires RAISING.plication area for semantic parsing, with great po-tential.
Text with valuable information has beenundergoing rapid growth across scientific and busi-ness disciplines alike.
A prominent example isPubMed (www.ncbi.nlm.nih.gov/pubmed), whichcontains over 24 million biomedical research arti-cles and grows by over one million each year.
Re-search on information extraction abounds, but ittends to focus on classifying simple relations amongentities, so is incapable of extracting the preva-lent complex knowledge with nested event struc-tures.
Figure 1 illustrates this problem with anexample sentence ?BCL stimulates inhibition ofRFLAT by IL-10?.
Traditional information extrac-tion would be content with extracting two binaryrelation instances (NEG-REG,BCL,RFLAT) and(NEG-REG,IL-10,RFLAT), where NEG-REGrepresents a negative regulation (i.e., inhibition).However, the sentence also discloses important con-textual information, i.e., BCL regulates RFLAT bystimulating the inhibitive effect of IL-10, and like-wise the inhibition of RFLAT by IL-10 is con-trolled by BCL.
Such context-specific knowledgeis crucial in translational medicine: imagine a tar-geted therapy that tries to suppress RFLAT by in-ducing either BCL or IL-10, without taking intoaccount their interdependency.
As Figure 1 shows,this knowledge can be represented by events withnested structures (e.g., the THEME argument of E1is an event E2), as exemplified by the GENIA eventextraction dataset (Kim et al, 2009).Complex knowledge extraction can be naturallyframed as a semantic parsing problem, with theevent structure represented by a semantic parse; seeFigure 2.
However, annotating example sentencesis expensive and time-consuming.
GENIA is theonly corpus of its kind by far; its annotation tookyears and its scope is limited to the narrow domainof transcription in human blood cells.
In contrast,databases are usually available.
For example, dueto the central importance of biological pathways inunderstanding diseases and developing drug targets,there exist many pathway databases (Schaefer et al,2009; Kanehisa, 2002; Cerami et al, 2011).
Lim-ited by manual curation, they are incomplete and notup-to-date, thereby the need for automated extrac-tion.
But compared to question answering, knowl-edge extraction can derive more leverage from suchdatabases via distant supervision (Craven and Kum-lien, 1999; Mintz et al, 2009).
The key insight isthat databases can be used to automatically anno-tate sentences with a relation if the arguments of aknown instance co-occur in the sentence.
This learn-ing paradigm, however, has never been applied toextracting nested events.In this paper, we propose the first approach tolearn a semantic parser from a database of complexevents and unannotated text, by generalizing dis-tant supervision to complex knowledge extraction.The key idea is to recover the latent annotations viaEM, guided by a structured prior that favors seman-tic parses containing known events in the database,in the form of virtual evidence (Pearl, 1988; Subra-manya and Bilmes, 2007).
Experiments on the GE-NIA dataset demonstrate the promise of this direc-tion.
Our GUSPEE (GroUnded Semantic Parsingfor Event Extraction) system can successfully learnfrom and extract complex events, without requiring757textual annotations (Figure 2).
Moreover, after in-corporating prototype-driven learning using just fiveexample words for each event type, GUSPEE be-comes competitive even among supervised systems,outperforming 19 out of 24 teams that participatedin the GENIA event extraction shared task.
Withsignificant information loss (skipping event triggersand, most importantly, the nested event structures),it is possible to reduce GENIA events to binary re-lations so that existing distant-supervision methodsare applicable.
Yet even in such an evaluation tai-lored for existing methods, our system still outper-formed them by a wide margin.2 Related WorkExisting approaches for GENIA event extraction aresupervised methods that either used a carefully en-gineered classification pipeline (Bjorne et al, 2009;Quirk et al, 2011) or applied joint inference (Riedelet al, 2009; Poon and Vanderwende, 2010; Riedeland McCallum, 2011).
Poon and Vanderwende(2010) used a dependency-based formulation that re-sembled our semantic parsing one, but learned fromsupervised data.
Classification approaches first needto classify words into event triggers, where distantsupervision is not directly applicable.In distant supervision (Craven and Kumlien,1999; Mintz et al, 2009), if two entities are knownto have a binary relation in the database, their co-occurrence in a sentence justifies labeling the in-stance with the relation.
This assumption is oftenincorrect, and Riedel et al (2010) introduced latentvariables to model the uncertainty; the model waslater improved by Hoffmann et al (2011).
GUS-PEE generalizes this idea to structured predictionwhere the latent annotations are not simple classifi-cation decisions, but nested events.
Krishnamurthyand Mitchell (2012) and Reddy et al (2014) tookan important step toward this direction, by learninga semantic parser based on combinatorial categorialgrammar (CCG) from Freebase and web sentences.However, Krishnamurthy and Mitchell (2012) stilllearned from binary relations, using only simple sen-tences (of length ten or less).
Reddy et al (2014)learned from n-ary relations as well, yet their for-mulation only allows relations between entities, notrelations between relations.
Thus their approachcannot represent nested events, let alne extract-ing them.
And like Krishnamurthy and Mitchell(2012), Reddy et al (2014) focused on simple textand excluded sentences where entities were not de-pendency neighbors (i.e., not directly connected inthe ungrounded graph), as well as sentences with un-known entities.
While such restrictions do not im-pede parsing simple questions in their evaluation,their approach is not directly applicable to com-plex knowledge extraction.
Reschke et al (2014)also generalized distant supervision to n-ary rela-tions for extracting template-based events, but sim-ilar to Reddy et al (2014), they did not considernested events.Distant supervision can be viewed as a specialcase of the more general paradigm of groundedlearning from a database.
Clarke et al (2010) andLiang et al (2011) used the database to determineif a candidate semantic parse would yield the anno-tated answer, whereas distant supervision uses thedatabase to determine if a relation instance is con-tained therein.
Our GUSPEE system is inspiredby grounded unsupervised semantic parsing (GUSP)(Poon, 2013) and shares a similar semantic repre-sentation.
GUSP, like most grounded learning ap-proaches, applied to question answering and did notleverage distant supervision.
GUSPEE can thus beviewed as an extension of GUSP to leverage distantsupervision for complex knowledge extraction.Grounding in GUSPEE is materialized by virtualevidence favoring semantic structures that conformwith the database.
The idea of virtual evidence wasfirst introduced by Pearl (1988) and later applied inseveral applications such as Subramanya and Bilmes(2007).
Unlike in prior work, the virtual evidencein GUSPEE involves non-local factors (comparing asemantic parse with complex events in the database)and presents a major challenge to efficient learning.Existing semantic parsers often adopt highly ex-pressive formalisms such as CCG (Steedman, 2000).Such formalisms are extremely powerful, but alsodifficult to learn.
We instead adopted a dependency-based formalism (Poon and Domingos, 2009; Lianget al, 2011; Poon, 2013).
Moreover, following Poonand Domingos (2009), Krishnamurthy and Mitchell(2012), Poon (2013), we started with syntactic de-pendency parses, and focused on annotating nodesand edges with semantic states.7583 Grounded Semantic Parsing for EventExtractionWe use the GENIA event extraction task (Kim etal., 2009) as a representative example of complexknowledge extraction.
The goal is to identify biolog-ical events from text, including the trigger words andarguments (Figure 1, bottom).
There are nine eventtypes, including simple ones such as Expressionand Transcription that can only have one THEMEargument, Binding that can have more than oneTHEME argument, and regulations that can have bothTHEME and CAUSE arguments.
Protein annotationsare given as input.We formulate this task as semantic parsing andpresent our GUSPEE system (Figure 2).
The coreof GUSPEE is a tree HMM (Section 3.1), whichextracts events from a sentence by annotating itssyntactic dependency tree with event and argumentstates.
In training, GUSPEE takes as input unan-notated text and a database of complex events, andlearns the tree HMM using EM, guided by groundedlearning from the database via virtual evidence.3.1 Problem FormulationLet t be a syntactic dependency tree for a sentence,with nodes niand dependency edges di,j(njis achild of ni).
A semantic parse of t is an assign-ment z that maps each node to an event state andeach dependency to an argument state.
The semanticstate of a protein word is fixed to that protein annota-tion.
Basic event states are the nine event types andNULL (signifying a non-event, e.g., ?The?
in Figure2 (c)).
Basic argument states are THEME, CAUSE,and NULL.
Additional states will be introduced laterin Section 3.2 and 3.4.GUSPEE models z, t by a tree HMM:P?
(z, t) =?mPEMIT(tm|zm, ?
)?PTRANS(zm|zpi(m), ?
)where ?
are the emission and transition parameters,m ranges over the nodes and dependency edges,pi(nj) = di,jand pi(di,j) = ni.
Note that this formu-lation implicitly assumes a fixed underlying directedtree, while the words and dependencies may vary.Semantic parsing finds the most probable seman-tic assignment given the dependency tree:z?= argmaxzlogP?
(z|t) = argmaxzlogP?
(z, t)In training, GUSPEE takes as input a set of complexevents (database K) and syntactic dependency trees(unannotated text T ), and maximizes the likelihoodof T augmented by virtual evidence ?K(z).?
?= argmax?logP?
(T |K)= argmax??t?Tlog?zP?
(z, t) ?
?K(z)Virtual evidence is analogous to a Bayesian prior,but applies to variable states rather than model pa-rameters (Subramanya and Bilmes, 2007).3.2 Handling Syntax-Semantics MismatchFor simple sentences such as the one in Figure 2(b),the complex event can be represented by a seman-tic parse using only basic states.
In general, how-ever, syntax and semantics often diverge.
For ex-ample, in Figure 2(c), ?requires?
triggers the topPOS-NEG event that has a THEME argument trig-gered by ?block?, but ?ability?
stands in between thetwo; likewise for ?block?
and ?IL-10?.
Addition-ally, mismatch could stem from errors in the syntac-tic parse.
In such cases, the correct semantic parsecan no longer be represented by basic states alone.Following GUSP (Poon, 2013), we introduced a newargument state RAISING which, if assigned to a de-pendency, would require that the parent and child beassigned the same basic event state.
We also intro-duce a corresponding RAISE version for each non-null event state, to signify that the word derives itsbasic state from RAISING of a child.
RAISING isrelated to but not identical with type raising in CCGand other grammars.
For simplicity, we did not useother complex states explored in Poon (2013).3.3 Virtual Evidence for Grounded LearningGrounded learning in GUSPEE is attained by incor-porating the virtual evidence ?K(z), which favorsthe z?s containing known events in K and penal-izes those containing unknown events.
Intuitively,this can be accomplished by identifying events in zand comparing them with events in K. But this isnot robust as individual events and mentions may befragmental and incomplete.
Insisting on matchingan event in full would miss partial matches that stillconvey valuable supervision.
Proteins are given as759input and can be mapped to event arguments a pri-ori.
Matching sub-events with only one protein argu-ment would be too noisy without direct supervisionon triggers.
We thus consider matching minimumsub-events with two protein arguments.Specifically, we preprocessed complex eventsin K to identify minimum logical forms contain-ing two protein arguments from each complexevent, where arguments not directly leading toeither protein are skipped.
For example, thecomplex event in Figure 1 would generate threesub-events: (NEG-REG,IL-10,RFLAT),(POS-REG,BCL,(NEG-REG,-,RFLAT)),(POS-REG,BCL,(NEG-REG,IL-10,-)),where - signifies underspecification.
We denote theset of such sub-events as S(K).Likewise, given a semantic parse z, for every pro-tein pair in z, we would convert the minimum se-mantic parse subtree spanning the two proteins intothe canonical logical form and compare it with el-ements in S(K).
If the minimum subtree containsNULL, either in an event or argument state, it sig-nifies a non-event and would be ignored.
Other-wise, the canonical form is derived by collapsingRAISING states.
For example, in both Figure 2 (b)and (c), the minimum subtree spanning the proteinsIL-10 and RFLAT is converted into the same logi-cal form of (NEG-REG,IL-10,RFLAT).
We de-note the set of such logical forms as E(z).Formally, the virtual evidence in GUSPEE are:?K(z) = exp?e?E(z)?(e,K)where?
(e,K) ={?
: e ?
S(K)??
: e /?
S(K)In distant supervision, where z is simply a binaryrelation, it is trivial to evaluate ?K(z).
(In fact,the original distant supervision algorithm is exactlyequivalent to this form, with ?
= ?.)
In GUSPEE,however, z is a semantic parse and evaluating E(z)and ?
(e,K) involves a global factor that does notdecompose into local dependencies as the tree HMMP?
(z, t).
The naive way to compute the augmentedlikelihood (Section 3.1) is thus intractable.3.4 Efficient Learning with Virtual EvidenceTo render learning tractable, the key idea is to aug-ment the local event and argument states so that theycontain sufficient information for evaluating ?K(z).Specifically, the semantic state z(ni) needs to repre-sent not only the semantic assignment to ni(e.g., aNEG-REG event trigger), but also the set of (possi-bly incomplete) sub-events in the subtree under ni.We accomplished this by representing the semanticpaths from nito proteins in the subtree.
Forexample, in Figure 2 (b), the augmented state of ?in-hibition?
would be (NEG-REG?THEME?RFLAT,NEG-REG?CAUSE?IL-10).
To facilitatecanonicalization and sub-event comparison, apath containing NULL will be skipped, andRAISING will be collapsed.
E.g., in Fig-ure 2(c), the augmented state of ?ability?would become (NEG-REG?THEME?RFLAT,NEG-REG?CAUSE?IL-10).With these augmented states, ?K(z) decomposesinto local factors.
The proteins under niare knowna priori, as well as the children containing them.
Se-mantic paths from nito proteins can thus be com-puted by imposing consistency constraints for eachchild.
Namely, for child njthat contains protein p,the semantic path from nito p should result fromcombining z(ni), z(di,j), and the semantic pathfrom njto p. The minimum sub-events spanningtwo proteins under ni, if any, can be derived fromthe semantic paths in the augmented state.
Note thatif both proteins come from the same child nj, thepair needs not be considered at ni, as their minimumspanning sub-event, if any, would be under njandalready be factored in there.The number of augmented states isO(sp), and thenumber of sub-event evaluations isO(s?p2), where sis the number of distinct semantic paths, and p is thenumber of proteins in the subtree.
Below, we showhow s, p can be constrained to reasonable ranges tomake computation efficient.First, consider s. The number of semantic pathsis theoretically unbounded since a path can be ar-bitrarily long.
However, semantic paths containedin a database event are bounded in length and canbe precomputed from the database (the maximumin GENIA is four).
Longer paths can be repre-sented by a special dummy path signifying that they760would not match any database events.
Likewise, cer-tain sub-paths would not occur in database events.E.g., in GENIA, simple events cannot take eventsas arguments, so paths containing sub-paths suchas Expression ?
Transcription are also ille-gitimate and can be represented same as the above.We also notice that for regulation events with otherregulation events as arguments, the semantics canbe compressed into a single regulation event, e.g.,POS-REG?NEG-REG is semantically equivalentwith NEG-REG, as the collective effect of a posi-tive regulation on top of a negative one is negative.Therefore, when evaulating the semantic path fromnito a protein during dynamic programming, wewould collapse consecutive regulation events in thechild path, if any.
This further reduces the length ofsemantic paths to at most three (regulation - regula-tion - simple event - protein).Next, we notice that p is bounded to begin with,but it could be quite large.
When a sentence con-tains many proteins (i.e., large p), it often stemsfrom conjunction of proteins, as in ?TP53 regulatesmany downstream targets such as ABCB1, AFP,APC, ATF3, BAX?.
All proteins in the conjunctplay a similar role in their respective events, such asTHEME in the above example among ?ABCB1, AFP,APC, ATF3, BAX?, and so share the same semanticpaths.
Therefore, prior to learning, we preprocessedthe sentences to condense each conjunct into a sin-gle effective protein node.
We identified conjunctionby Stanford dependencies (conj ?).
In GENIA, thisreduces the maximum number of effective proteinnodes to two for the vast majority of sentences (over90%).
Both representation and evaluation are nowreasonably efficient.
To further speed up learning, inour experiments we only trained on sentences withat most two effective protein nodes, as this alreadyperformed quite well.
Training on GENIA took 1.5hours and semantic parsing of a sentence took lessthan a second (with one i7 core at 2.4 GHz).Unlike RAISING, the augmented states intro-duced in this section are specific to GENIA events.However, the rules to canonicalize states are generaland can potentially be adapted to other domains.
Analternative strategy to combat state explosion is byembedding the discrete states in a low-dimensionalvector space (Socher et al, 2013), which is a direc-tion for future research.3.5 FeaturesThe GUSPEE model uses log-linear models for theemission and transition probabilities and trains usingfeature-rich EM (Berg-Kirkpatrick et al, 2010).
Thefeatures are:Word emission I[lemma = l, zm= n];Dependency emission I[dependency = d, zm=e] where e /?
{NULL, RAISE};Transition I[zm= a, zpi(m)= b] where a, b /?
{NULL, RAISE}.To modulate the model complexity, GUSPEEimposes a standard L2prior on the weights, andincludes the following features with fixed weights:?
WNULL: apply to NULL states;?
WRAISE?P: apply to protein RAISING;?
WRAISE?E: apply to event RAISING.The advantage of a feature-rich representation isflexibility in feature engineering.
Here, we ex-cluded NULL and RAISE in dependency emissionand transition features, and regulated them sepa-rately to enable parameter tying for better general-ization.4 Experiments4.1 Evaluation on GENIA Event ExtractionIn principle, we can learn GUSPEE from any path-way database.
However, evaluation is challenging asthese databases do not contain textual annotations.Prior work on distant supervision resorted to sam-pling and annotating new extractions.
This is effec-tive for comparing among distant-supervision sys-tems, but it cannot be used to compare them withsupervised learning.
Moreover, as annotation is con-ducted by the authors or crowdsourcing, consistencyand quality are hard to control.We thus adopted a novel approach to evaluationby simulating a grounded learning scenario usingthe GENIA event extraction dataset (Kim et al,2009).
Specifically, we generated a set of complexevents from the annotations of training sentences asthe database.
The annotations were discarded after-wards and GUSPEE learned from the database andunannotated text alone.
The learned model was thenapplied to semantic parsing of test sentences andevaluated on event precision, recall, and F1.
This761Event Type Rec.
Prec.
F1Expression 50.8 41.9 45.9Transcription 18.3 14.0 15.9Catabolism 0 0 0Phosphorylation 36.2 43.6 39.5Localization 0 0 0Binding 24.0 42.6 30.7Regulation 2.5 5.0 3.3Positive regulation 11.4 21.4 14.9Negative regulation 4.4 16.4 6.9Total Event F1 19.1 29.4 23.2Table 1: GENIA event extraction results of GUSPEEevaluation methodology enables us to assess the trueaccuracy and compare head-to-head with supervisedmethods.GENIA contains 800 abstracts for training and150 for development.
It also has a test set, but itsannotation is not made public.
Therefore, we usedthe training set for grounded learning and develop-ment, and reserved the development set for testing.The majority events are Regulation (includingPositive regulation, Negative regulation).See Kim et al (2009) for details.
We processedall sentences using SPLAT (Quirk et al, 2012),to conduct tokenization, part-of-speech tagging,and constituency parsing.
We then postprocessedthe parses to obtain Stanford dependencies (deMarneffe et al, 2006).
During development on thetraining data, we found the following parameters(Section 3) to perform quite well and used them inall subsequent experiments: ?
= 20, WNULL= 4,WRAISE?P= 2, WRAISE?E= ?6, L2prior = 0.1.Interestingly, we found that encouraging proteinRAISING is beneficial, which probably stems fromthe fact that proteins are often separated from eventtriggers by noun modifiers, such as ?the BCL gene?,?IL-10 protein?.Table 1 shows GUSPEE?s results on GENIAevent extraction.
Note that this event-based eval-uation is rather stringent, as it considers an eventincorrect if one of its argument events is not com-pletely correct, thus an incorrect event will renderall its upstream events incorrect.
See Kim et al(2009) for details.
For comparison, Table 2 showsthe results of MSR11, a state-of-the-art supervisedsystem.
MSR11 also provides a upper bound for thesupervised version of GUSPEE, as the latter is muchless engineered.Event Type Rec.
Prec.
F1Expression 76.4 81.5 78.8Transcription 49.4 73.6 59.1Catabolism 65.6 80.0 74.4Phosphorylation 73.9 84.5 78.9Localization 74.6 75.8 75.2Binding 48.0 50.9 49.4Regulation 32.5 47.1 38.6Positive regulation 38.7 51.7 44.3Negative regulation 35.9 54.9 43.9Total Event F1 50.2 62.6 55.7Table 2: GENIA event extraction results of state-of-the-art supervised system MSR11 (Quirk et al, 2011).Not surprisingly, grounded learning with GUS-PEE still lags behind supervised learning.
MSR11used a rich set of features, including POS tags, linearand dependency n-grams, etc.
Also, it is expectedthat indirect supervision do not provide as effectivesignals as direct supervision.
However, the com-parison reveals a particularly interesting contrast.Event types such as Expression, Catabolism,Phosphorylation, and Localization are rela-tively easy, yet GUSPEE performed rather poorlyon them.
Simple events do not admit multiple ar-guments, so they appear less often in the virtual evi-dence, and grounded learning has difficulty learningthese event types, especially their triggers.
In lightof this, it?s actually remarkable that GUSPEE stilllearned a substantial portion of them.4.2 Prototype-Driven LearningWhile full-blown annotations are undoubtedly ex-pensive and time-consuming to generate, it is rathereasy for a domain expert to provide a few trig-ger words per event type, such as ?expression?,?expressed?
for Expression.
This motivates usto explore prototype-driven learning (Haghighi andKlein, 2006) in combination with grounded learn-ing.
Specifically, we simulated expert selection bypicking the top five most frequent trigger wordsfor each event type from training data.
We thenaugmented grounded learning in GUSPEE by in-corporating word emission features for each proto-type word and the corresponding event state, e.g.,I[lemma = express, zm= Expression].
Theweights are fixed to a large number (five in our case).Table 3 shows the results with prototypes, whichimproved substantially.
Not surprisingly, simple762Event Type Rec.
Prec.
F1Expression 55.3 88.3 68.0Transcription 50.0 39.1 43.9Catabolism 52.4 100.0 68.9Phosphorylation 61.7 82.9 70.7Localization 52.8 100.0 69.1Binding 20.2 92.7 33.2Regulation 24.1 64.0 35.0Positive regulation 17.4 63.8 27.4Negative regulation 8.4 52.8 14.5Total Event F1 27.9 72.2 40.2Table 3: GENIA event extraction results of GUSPEEwith five prototype words per event type0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0303540GENIA Event Extraction F1Incomplete DatabaseIncomplete TextFigure 3: GENIA Event F1 of GUSPEE with prototypes,using incomplete database or text.events such as Catabolism benefited the most fromprototypes, as they have fewer variations in trig-gers.
While the F1 score 40.2 still lags behindthe supervised state of the art, it would have beencompetitive compared to the 24 teams participat-ing in the original shared task, outperforming 19 ofthem (the top 5th system scored an F1 of 40.5, seewww.nactem.ac.uk/tsujii/GENIA/SharedTask/results/results-master.html, Task 1).4.3 Database-Text MismatchIn our simulation of grounded learning, every eventin the database is mentioned in some text and viceversa.
In practice, however, there is usually a mis-match between database and text: the unannotatedtext generally contains more facts than are alreadypopulated in the database; conversely, a databasefact may not be explicitly mentioned in the text.The GENIA dataset offers an excellent opportu-nity to study the robustness of grounded learningin light of such mismatch.
Specifically, we simu-0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0152025303540 GENIA Event Extraction F110% Database50% DatabaseFigure 4: GENIA Event F1 of GUSPEE with prototypes,using a fraction of database and increasing amount ofunannotated text.lated a grounded learning scenario with an incom-plete database by populating events from the anno-tations of a random fraction of training text, and thenlearning GUSPEE with this database and all trainingtext.
Likewise, we simulated a scenario with incom-plete text using the training event database in full,but only a fraction of unannotated text.Figure 3 shows the results of GUSPEE with proto-types as the fraction varies between 0.1 and 1, by av-eraging five random runs.
In both scenarios, the F1score degrades smoothly as the fraction gets smaller.Precision stays roughly the same while recall grad-ually degrades (curves not shown).
This shows thatGUSPEE is reasonably robust.
Not surprisingly, thedegradation is steeper with incomplete database thanwith incomplete text.To further investigate the effect of unannotatedtext, we also randomly sampled a fraction ofdatabase events for grounded supervision, and eval-uated GUSPEE with increasing amounts of unanno-tated text.
Figure 4 shows the results by averagingnine random runs.
The F1 increases steadily withadditional unannotated text, mainly due to rising re-call (curves not shown).
This suggests that GUSPEEcould potentially benefit from more unannotated textand is reasonably robust even when some text is notrelevant to the available events.
As expected, moregrounded supervision (50% vs. 10% database) ledto substantially better F1 and lower variation.4.4 Error AnalysisUpon manual inspection, we found that syntactic er-rors considerably affect performance.
Poon (2013)763introduced complex states such as Sinking andImplicit to combat syntax-semantics mismatch,which could also be incorporated into GUSPEE.Improving syntactic parsing, either separately byadapting to the biomedical domain, or jointly alongwith semantic parsing, is another important futuredirection.
GUSPEE achieved better precision thanrecall, especially when learning with prototypes, andmight benefit from augmenting prototypes by distri-butional similarity (Haghighi and Klein, 2006).4.5 Comparison with Existing DistantSupervision ApproachesExisting distant supervision approaches are not di-rectly applicable to extracting nested events.
How-ever, we can convert the extraction task into clas-sifying minimum sub-events between proteins, forwhich existing methods can be applied.
Specifically,we used binary sub-events in S(K) (Section 3.3) fordistant supervision, and evaluated on classifying testsentences.
This would enable an interesting compar-ison with GUSPEE, as the latter also derived indirectsupervision from S(K) alone.
Textual annotationsof triggers and nested event structures in GUSPEEoutput were ignored, and prototypes were not usedto enable a fair comparison.
For distant supervision,we used the state-of-the-art MultiR system (Hoff-mann et al, 2011) with standard lexical and syntac-tic features (Mintz et al, 2009).
MultiR can be usedfor supervised learning by fixing relations accordingto the sentence-level annotations, which provides asupervised upper bound.Table 4 shows the results.
GUSPEE outperformedMultiR by a wide margin, improving F1 by 24%.Surprisingly, GUSPEE even surpassed the super-vised upper bound of MultiR.
This suggests thatour semantic parsing formulation not only is supe-rior in representation power, but also facilitates bet-ter learning.
We also experimented with sharingparameters among related sub-events in a MultiR-like model, but it did not improve the performance.Upon close inspection, we found that MultiR mainlyscored on Binding events and failed almostly en-tirely on the more difficult Regulation events.GUSPEE was able to extract Regulation events,but incurred some precision errors.Method Rec.
Prec.
F1 (Class.
)MultiR 11.2 21.7 14.8MultiR (Super.)
12.1 24.4 16.2GUSPEE 22.9 15.3 18.4Table 4: Classification results on GENIA when events aresimplified to binary relations for distant supervision.5 SummaryWe generalize distant supervision to complexknowledge extraction and propose the first approachto learn a semantic parser from a database of nestedevents and unannotated text.
Experiments on GE-NIA event extraction showed that our GUSPEE sys-tem could learn from and extract such complexevents, and was competitive even among supervisedsystems after incorporating a few easily-obtainableprototype event trigger words.Future directions include: PubMed-scale path-way extraction; application to other domains; in-corporating additional complex states to addresssyntax-semantics mismatch; learning vector-spacerepresentations for complex states; joint syntactic-semantic parsing; incorporating reasoning and othersources of indirect supervision.ReferencesYoav Artzi and Luke Zettlemoyer.
2011.
Bootstrappingsemantic parsers from conversations.
In Proceedingsof the 2011 Conference on Empirical Methods in Nat-ural Language Processing.Yoav Artzi and Luke Zettlemoyer.
2013.
Weakly su-pervised learning of semantic parsers for mapping in-structions to actions.
Transactions of the Associationfor Computational Linguistics, 1:49?62.Taylor Berg-Kirkpatrick, John DeNero, and Dan Klein.2010.
Painless unsupervised learning with features.
InProceedings of Human Language Technologies: The2010 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics.Jari Bjorne, Juho Heimonen, Filip Ginter, Antti Airola,Tapio Pahikkala, and Tapio Salakoski.
2009.
Extract-ing complex biological events with rich graph-basedfeature sets.
In Proceedings of the BioNLP Workshop.Ethan G Cerami, Benjamin E Gross, Emek Demir, IgorRodchenkov,?Ozg?un Babur, Nadia Anwar, NikolausSchultz, Gary D Bader, and Chris Sander.
2011.
Path-way commons, a web resource for biological path-way data.
Nucleic acids research, 39(suppl 1):D685?D690.764David Chen and Ray Mooney.
2011.
Learning to in-terpret natural language navigation instructions fromobservations.
In Proceedings of the Twenty Sixth Na-tional Conference on Artificial Intelligence.James Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving semantic parsing fromworld?s response.
In Proceedings of the 2010 Con-ference on Natural Language Learning.M.
Craven and J. Kumlien.
1999.
Constructing biolog-ical knowledge bases by extracting information fromtext sources.
In Proceedings of the 7th InternationalConference on Intelligent Systems for Molecular Biol-ogy, pages 77?86.
AAAI Press.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProceedings of the Fifth International Conference onLanguage Resources and Evaluation, pages 449?454,Genoa, Italy.
ELRA.Aria Haghighi and Dan Klein.
2006.
Prototype-drivenlearning for sequence models.
In Proceedings of theForty Fourth Annual Meeting of the Association forComputational Linguistics.Raphael Hoffmann, Congle Zhang, Xiao Ling, LukeZettlemoyer, and Daniel S. Weld.
2011.
Knowledge-based weak supervision for information extraction ofoverlapping relations.
In Proceedings of the FortyNinth Annual Meeting of the Association for Compu-tational Linguistics.Minoru Kanehisa.
2002.
The kegg database.
Silico Sim-ulation of Biological Processes, 247:91?103.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Junichi Tsujii.
2009.
Overview ofBioNLP-09 Shared Task on event extraction.
In Pro-ceedings of the BioNLP Workshop.Jayant Krishnamurthy and Tom M. Mitchell.
2012.Weakly supervised training of semantic parsers.
InEMNLP-12.Percy Liang, Michael Jordan, and Dan Klein.
2011.Learning dependency-based compositional semantics.In Proceedings of the Forty Ninth Annual Meeting ofthe Association for Computational Linguistics.Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky.2009.
Distant supervision for relation extraction with-out labeled data.
In Proceedings of the Forty SeventhAnnual Meeting of the Association for ComputationalLinguistics.Raymond J. Mooney.
2007.
Learning for semantic pars-ing.
In Proceedings of the Eighth International Con-ference on Computational Linguistics and IntelligentText Processing, pages 311?324, Mexico City, Mex-ico.
Springer.J.
Pearl.
1988.
Probabilistic Reasoning in IntelligentSystems: Networks of Plausible Inference.
MorganKaufmann, San Francisco, CA.Hoifung Poon and Pedro Domingos.
2009.
Unsuper-vised semantic parsing.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing, pages 1?10, Singapore.
ACL.Hoifung Poon and Pedro Domingos.
2010.
Unsuper-vised ontological induction from text.
In Proceedingsof the Forty Eighth Annual Meeting of the Associationfor Computational Linguistics, pages 296?305, Upp-sala, Sweden.
ACL.Hoifung Poon and Lucy Vanderwende.
2010.
Joint infer-ence for knowledge extraction from biomedical liter-ature.
In Proceedings of Human Language Technolo-gies: The 2010 Annual Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics.Hoifung Poon.
2013.
Grounded unsupervised semanticparsing.
In Proceedings of the Fifty First Annual Meet-ing of the Association for Computational Linguistics.Chris Quirk, Pallavi Choudhury, Michael Gamon, andLucy Vanderwende.
2011.
Msr-nlp entry in bionlpshared task 2011.
In Proc.
BioNLP.Chris Quirk, Pallavi Choudhury, Jianfeng Gao, HisamiSuzuki, Kristina Toutanova, Michael Gamon, WentauYih, and Lucy Vanderwende.
2012.
MSR SPLAT, alanguage analysis toolkit.
In Proceedings of NAACLHLT Demonstration Session.Siva Reddy, Mirella Lapata, and Mark Steedman.
2014.Large-scale semantic parsing without question-answerpairs.
Transactions of the Association for Computa-tional Linguistics.Kevin Reschke, Martin Jankowiak, Mihai Surdeanu,Christopher D Manning, and Daniel Jurafsky.
2014.Event extraction using distant supervision.
In Lan-guage Resources and Evaluation Conference (LREC).Sebastian Riedel and Andrew McCallum.
2011.
Fast androbust joint models for biomedical event extraction.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing.Sebastian Riedel, Hong-Woo Chun, Toshihisa Takagi,and Jun?ichi Tsujii.
2009.
A markov logic approachto bio-molecular event extraction.
In Proc.
BioNLP.Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling relations and their mentions withoutlabeled text.
In Proceedings of the Sixteen EuropeanConference on Machine Learning.Carl F. Schaefer, Kira Anthony, Shiva Krupa, JeffreyBuchoff, Matthew Day, Timo Hannay, and Ken-neth H. Buetow.
2009.
PID: The pathway interactiondatabase.
Nucleic Acids Research, 37:674?679.765Richard Socher, John Bauer, Christopher D. Manning,and Andrew Y. Ng.
2013.
Parsing with composi-tional vector grammars.
In Proceedings of the FiftyFirst Annual Meeting of the Association for Computa-tional Linguistics.Mark Steedman.
2000.
The syntactic process, vol-ume 35.
MIT Press.Amarnag Subramanya and Jeff Bilmes.
2007.
Virtualevidence for training speech recognizers using par-tially labeled data.
In Proceedings of Human Lan-guage Technologies: The 2007 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics.Ivan Titov and Alexandre Klementiev.
2011.
A bayesianmodel for unsupervised semantic parsing.
In Proceed-ings of the Forty Ninth Annual Meeting of the Associ-ation for Computational Linguistics.John M. Zelle and Ray Mooney.
1993.
Learning seman-tic grammars with constructive inductive logic pro-gramming.
In Proceedings of the Eleventh NationalConference on Artificial Intelligence.Luke S. Zettlemoyer and Michael Collins.
2005.
Learn-ing to map sentences to logical form: Structured clas-sification with probabilistic categorial grammers.
InProceedings of the Twenty First Conference on Un-certainty in Artificial Intelligence, pages 658?666, Ed-inburgh, Scotland.
AUAI Press.Luke S. Zettlemoyer and Michael Collins.
2007.
Onlinelearning of relaxed ccg grammars for parsing to logicalform.
In Proceedings of the Joint Conference on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning.766
