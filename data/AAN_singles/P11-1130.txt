Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1298?1307,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsTranslating from Morphologically Complex Languages:A Paraphrase-Based ApproachPreslav NakovDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417nakov@comp.nus.edu.sgHwee Tou NgDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417nght@comp.nus.edu.sgAbstractWe propose a novel approach to translatingfrom a morphologically complex language.Unlike previous research, which has targetedword inflections and concatenations, we fo-cus on the pairwise relationship between mor-phologically related words, which we treat aspotential paraphrases and handle using para-phrasing techniques at the word, phrase, andsentence level.
An important advantage ofthis framework is that it can cope with deriva-tional morphology, which has so far remainedlargely beyond the capabilities of statisticalmachine translation systems.
Our experimentstranslating from Malay, whose morphology ismostly derivational, into English show signif-icant improvements over rivaling approachesbased on five automatic evaluation measures(for 320,000 sentence pairs; 9.5 million En-glish word tokens).1 IntroductionTraditionally, statistical machine translation (SMT)models have assumed that the word should be the ba-sic token-unit of translation, thus ignoring any word-internal morphological structure.
This assumptioncan be traced back to the first word-based models ofIBM (Brown et al, 1993), which were initially pro-posed for two languages with limited morphology:French and English.
While several significantly im-proved models have been developed since then, in-cluding phrase-based (Koehn et al, 2003), hierarchi-cal (Chiang, 2005), treelet (Quirk et al, 2005), andsyntactic (Galley et al, 2004) models, they all pre-served the assumption that words should be atomic.Ignoring morphology was fine as long as the mainresearch interest remained focused on languageswith limited (e.g., English, French, Spanish) or min-imal (e.g., Chinese) morphology.
Since the attentionshifted to languages like Arabic, however, the im-portance of morphology became obvious and sev-eral approaches to handle it have been proposed.Depending on the particular language of interest,researchers have paid attention to word inflectionsand clitics, e.g., for Arabic, Finnish, and Turkish,or to noun compounds, e.g., for German.
However,derivational morphology has not been specificallytargeted so far.In this paper, we propose a paraphrase-based ap-proach to translating from a morphologically com-plex language.
Unlike previous research, we focuson the pairwise relationship between morphologi-cally related wordforms, which we treat as poten-tial paraphrases, and which we handle using para-phrasing techniques at various levels: word, phrase,and sentence level.
An important advantage of thisframework is that it can cope with various kindsof morphological wordforms, including derivationalones.
We demonstrate its potential on Malay, whosemorphology is mostly derivational.The remainder of the paper is organized as fol-lows: Section 2 gives an overview of Malay mor-phology, Section 3 introduces our paraphrase-basedapproach to translating from morphologically com-plex languages, Section 4 describes our dataset andour experimental setup, Section 5 presents and anal-yses the results, and Section 6 compares our work toprevious research.
Finally, Section 7 concludes thepaper and suggests directions for future work.12982 Malay Morphology and SMTMalay is an Astronesian language, spoken by about180 million people.
It is official in Malaysia, In-donesia, Singapore, and Brunei, and has two majordialects, sometimes regarded as separate languages,which are mutually intelligible, but occasionally dif-fer in orthography/pronunciation and vocabulary:Bahasa Malaysia (lit.
?language of Malaysia?)
andBahasa Indonesia (lit.
?language of Indonesia?
).Malay is an agglutinative language with very richmorphology.
Unlike other agglutinative languagessuch as Finnish, Hungarian, and Turkish, whichare rich in both inflectional and derivational forms,Malay morphology is mostly derivational.
Inflec-tionally,1 Malay is very similar to Chinese: there isno grammatical gender, number, or tense, verbs arenot marked for person, etc.In Malay, new words can be formed by the fol-lowing three morphological processes:?
Affixation, i.e., attaching affixes, which are notwords themselves, to a word.
These can be pre-fixes (e.g., ajar/?teach?
?
pelajar/?student?
),suffixes (e.g., ajar ?
ajaran/?teachings?
), cir-cumfixes (e.g., ajar ?
pengajaran/?lesson?
),and infixes (e.g., gigi/?teeth?
?
gerigi/?toothedblade?).
Infixes only apply to a small numberof words and are not productive.?
Compounding, i.e., forming a new word byputting two or more existing words together.For example, kereta/?car?
+ api/?fire?
makekereta api and keretapi in Bahasa Indonesia andBahasa Malaysia, respectively, both meaning?train?.
As in English, Malay compounds arewritten separately, but some stable ones likekerjasama/?collaboration?
(from kerja/?work?and sama/?same?)
are concatenated.
Concate-nation is also required when a circumfix isapplied to a compound, e.g., ambil alih/?takeover?
(ambil/?take?
+ alih/?move?)
is con-catenated to form pengambilalihan/?takeover?when targeted by the circumfix peng-.
.
.-an.1Inflection is variation in the form of a word that is oblig-atory in some given grammatical context.
For example, plays,playing, played are all inflected forms of the verb play.
It doesnot yield a new word and cannot change the part of speech.?
Reduplication, i.e., word repetition.
InMalay, reduplication requires using a dash.
Itcan be full (e.g., pelajar-pelajar/?students?
),partial (e.g., adik-beradik/?siblings?, fromadik/?younger brother/sister?
), and rhythmic(e.g., gunung-ganang/?mountains?, from theword gunung/?mountain?
).Malay has very little inflectional morphology, Italso has some clitics2, which are not very frequentand are typically spelled concatenated to the preced-ing word.
For example, the politeness marker lahcan be added to the command duduk/?sit down?
toyield duduklah/?please, sit down?, and the pronounnya can attach to kereta to form keretanya/?his car?.Note that clitics are not affixes, and clitic attachmentis not a word derivation or a word inflection process.Taken together, affixation, compounding, redu-plication, and clitic attachment yield a rich vari-ety of wordforms, which cause data sparseness is-sues.
Moreover, the predominantly derivational na-ture of Malay morphology limits the applicabil-ity of standard techniques such as (1) removingsome/all of the source-language inflections, (2) seg-menting affixes from the root, and (3) clusteringwords with the same target translation.
For example,if pelajar/?student?
is an unknown word and lemma-tization/stemming reduces it to ajar/?teach?, wouldthis enable a good translation?
Similarly, would seg-menting3 pelajar as peN+ ajar, i.e., as ?person do-ing the action?
+ ?teach?, make it possible to gener-ate ?student?
(e.g., as opposed to ?teacher?)?
Finally,if affixes tend to change semantics so much, howlikely are we to find morphologically related word-forms that share the same translation?
Still, thereare many good reasons to believe that morphologi-cal processing should help SMT for Malay.Consider affixation, which can yield words withsimilar semantics that can use each other?s trans-lation options, e.g., diajar/?be taught (intransitive)?and diajarkan/?be taught (transitive)?.
However, thiscannot be predicted from the affix, e.g., compareminum/?drink (verb)?
?
minuman/?drink (noun)?
andmakan/?eat?
?
makanan/?food?.2A clitic is a morpheme that has the syntactic characteristicsof a word, but is phonologically bound to another word.
Forexample, ?s is a clitic in The Queen of England?s crown.3The prefix peN suffers a nasal replacement of thearchiphoneme N to become pel in pelajar.1299Looking at compounding, it is often the case thatthe semantics of a compound is a specialization ofthe semantics of its head, and thus the target lan-guage translations available for the head could be us-able to translate the whole compound, e.g., comparekerjasama/?collaboration?
and kerja/?work?.
Alter-natively, it might be useful to consider a segmentedversion of the compound, e.g., kerja sama.Reduplication, among other functions, expressesplural, e.g., pelajar-pelajar/?students?.
Note, how-ever, that it is not used when a quantity or a num-ber word is present, e.g., dua pelajar/?two students?and banyak pelajar/?many students?.
Thus, if we donot know how to translate pelajar-pelajar, it wouldbe reasonable to consider the translation options forpelajar since it could potentially contain among itstranslation options the plural ?students?.Finally, consider clitics.
In some cases, a cliticcould express a fine-grained distinction such as po-liteness, which might not be expressible in the targetlanguage; thus, it might be feasible to simply removeit.
In other cases, e.g., when it is a pronoun, it mightbe better to segment it out as a separate word.3 MethodWe propose a paraphrase-based approach to Malaymorphology, where we use paraphrases at three dif-ferent levels: word, phrase, and sentence level.First, we transform each development/testingMalay sentence into a word lattice, where we addsimplified word-level paraphrasing alternatives foreach morphologically complex word.
In the lattice,each alternative w?
of an original word w is assignedthe weight of Pr(w?|w), which is estimated usingpivoting over the English side of the training bi-text.
Then, we generate sentence-level paraphrasesof the training Malay sentences, in which exactlyone morphologically complex word is substituted bya simpler alternative.
Finally, we extract additionalMalay phrases from these sentences, which we useto augment the phrase table with additional transla-tion options to match the alternative wordforms inthe lattice.
We assign each such additional phrasep?
a probability maxp Pr(p?|p), where p is a Malayphrase that is found in the original training Malaytext.
The probability is calculated using phrase-levelpivoting over the English side of the training bi-text.3.1 Morphological AnalysisGiven a Malay word, we build a list of morpholog-ically simpler words that could be derived from it;we also generate alternative word segmentations:(a) words obtainable by affix strippinge.g., pelajaran ?
pelajar, ajaran, ajar(b) words that are part of a compound worde.g., kerjasama ?
kerja(c) words appearing on either side of a dashe.g., adik-beradik ?
adik, beradik(d) words without cliticse.g., keretanya ?
kereta(e) clitic-segmented word sequencese.g., keretanya ?
kereta nya(f) dash-segmented wordformse.g., aceh-nias ?
aceh - nias(g) combinations of the above.The list is built by reversing the basic morpho-logical processes in Malay: (a) addresses affixation,(b) handles compounding, (c) takes care of redu-plication, and (d) and (e) deal with clitics.
Strictlyspeaking, (f) does not necessarily model a morpho-logical process: it proposes an alternative tokeniza-tion, but this could make morphological sense too.Note that (g) could cause potential problems wheninteracting with (f), e.g., adik-beradik would be-come adik - beradik and then by (a) it would turninto adik - adik, which could cause the SMT sys-tem to generate two separate translations for the twoinstances of adik.
To prevent this, we forbid theapplication of (f) to reduplications.
Taking into ac-count that reduplications can be partial, we only al-low (f) if |LCS(l,r)|min(|l|,|r|) < 0.5, where l and r are thestrings to the left and to the right of the dash, re-spectively, LCS(x, y) is the longest common char-acter subsequence, not necessarily consecutive, ofthe strings x and y, and |x| is the length of the stringx.
For example, LCS(adik,beradik)=adik, and thus,the ratio is 1 (?
0.5) for adik-beradik.
Similarly,LCS(gunung,ganang)=gnng, and thus, the ratio is4/6=0.67 (?
0.5) for gunung-ganang.
However, foraceh-nias, it is 1/4=0.25, and thus (f) is applicable.1300As an illustration, here are the wordforms wegenerate for adik-beradiknya/?his siblings?
: adik,adik-beradiknya, adik-beradik nya, adik-beradik,beradiknya, beradik nya, adik nya, and beradik.And for berpelajaran/?is educated?, we build the list:berpelajaran, pelajaran, pelajar, ajaran, and ajar.Note that the lists do include the original word.To generate the above wordforms, we used twomorphological analyzers: a freely available Malaylemmatizer (Baldwin and Awab, 2006), and an in-house re-implementation of the Indonesian stemmerdescribed in (Adriani et al, 2007).
Note that thesetools?
objective is to return a single lemma/stem,e.g., they would return adik for adik-beradiknya, andajar for berpelajaran.
However, it was straightfor-ward to modify them to also output the above in-termediary wordforms, which the tools were gener-ating internally anyway when looking for the finallemma/stem.
Finally, since the two modified ana-lyzers had different strengths and weaknesses, wecombined their outputs to increase recall.3.2 Word-Level ParaphrasingWe perform word-level paraphrasing of the Malaysides of the development and the testing bi-texts.First, for each Malay word, we generate theabove-described list of morphologically simplerwords and alternative word segmentations; we thinkof the words in this list as word-level paraphrases.Then, for each development/testing Malay sentence,we generate a lattice encoding all possible para-phrasing options for each individual word.We further specify a weight for each arc.
We as-sign 1 to the original Malay word w, and Pr(w?|w)to each paraphrase w?
of w, where Pr(w?|w) is theprobability that w?
is a good paraphrase of w. Notethat multi-word paraphrases, e.g., resulting fromclitic segmentation, are encoded using a sequence ofarcs; in such cases, we assign Pr(w?|w) to the firstarc, and 1 to each subsequent arc.We calculate the probability Pr(w?|w) using thetraining Malay-English bi-text, which we align atthe word level using IBM model 4 (Brown et al,1993), and we observe which English words w andw?
are aligned to.
More precisely, we use pivoting toestimate the probability Pr(w?|w) as follows:Pr(w?|w) = ?i Pr(w?|w, ei)Pr(ei|w)Then, following (Callison-Burch et al, 2006; Wuand Wang, 2007), we make the simplifying assump-tion that w?
is conditionally independent of w givenei, thus obtaining the following expression:Pr(w?|w) =?i Pr(w?|ei)Pr(ei|w)We estimate the probability Pr(ei|w) directlyfrom the word-aligned training bi-text as follows:Pr(ei|w) = #(w,ei)Pj #(w,ej)where #(x, e) is the number of times the Malayword x is aligned to the English word e.Estimating Pr(w?|ei) cannot be done directlysince w?
might not be present on the Malay side ofthe training bi-text, e.g., because it is a multi-tokensequence generated by clitic segmentation.
Thus, wethink of w?
as a pseudoword that stands for the unionof all Malay words in the training bi-text that are re-ducible to w?
by our morphological analysis proce-dure.
So, we estimate Pr(w?|ei) as follows:Pr(w?|ei) = Pr({v : w?
?
forms(v)}|ei)where forms(x) is the set of the word-level para-phrases4 for the Malay word x.Since the training bi-text occurrences of the wordsthat are reducible to w?
are distinct, we can rewritethe above as follows:Pr(w?|ei) =?v:w?
?forms(v) Pr(v|ei)Finally, the probability Pr(v|ei) can be estimatedusing maximum likelihood:Pr(v|ei) = #(v,ei)Pu #(u,ei)3.3 Sentence-Level ParaphrasingIn order for the word-level paraphrases to work,there should be phrases in the phrase table that couldpotentially match them.
For some of the words, e.g.,the lemmata, there could already be such phrases,but for other transformations, e.g., clitic segmenta-tion, this is unlikely.
Thus, we need to augment thephrase table with additional translation options.One approach would be to modify the phrase ta-ble directly, e.g., by adding additional entries, whereone or more Malay words are replaced by their para-phrases.
This would be problematic since the phrasetranslation probabilities associated with these new4Note that our paraphrasing process is directed: the para-phrases are morphologically simpler than the original word.1301entries would be hard to estimate.
For example, theclitics, and even many of the intermediate morpho-logical forms, would not exist as individual words inthe training bi-text, which means that there would beno word alignments or lexical probabilities availablefor them.Another option would be to generate separateword alignments for the original training bi-text andfor a version of it where the source (Malay) sidehas been paraphrased.
Then, the two bi-texts andtheir word alignments would be concatenated andused to build a phrase table (Dyer, 2007; Dyer etal., 2008; Dyer, 2009).
This would solve the prob-lems with the word alignments and the phrase pairprobabilities estimations in a principled manner, butit would require choosing for each word only one ofthe paraphrases available to it, while we would pre-fer to have a way to allow all options.
Moreover, theparaphrased and the original versions of the corpuswould be given equal weights, which might not bedesirable.
Finally, since the two versions of the bi-text would be word-aligned separately, there wouldbe no interaction between them, which might leadto missed opportunities for improved alignments inboth parts of the bi-text (Nakov and Ng, 2009).We avoid the above issues by adopting a sentence-level paraphrasing approach.
Following the gen-eral framework proposed in (Nakov, 2008), we firstcreate multiple paraphrased versions of the source-side sentences of the training bi-text.
Then, eachparaphrased source sentence is paired with its orig-inal translation.
This augmented bi-text is word-aligned and a phrase table T ?
is built from it, whichis merged with a phrase table T for the original bi-text.
The merged table contains all phrase entriesfrom T , and the entries for the phrase pairs from T ?that are not in T .
Following Nakov and Ng (2009),we add up to three additional indicator features (tak-ing the values 0.5 and 1) to each entry in the mergedphrase table, showing whether the entry came from(1) T only, (2) T ?
only, or (3) both T and T ?.
We alsotry using the first one or two features only.
We setall feature weights using minimum error rate train-ing (Och, 2003), and we optimize their number (one,two, or three) on the development dataset.55In theory, we should re-normalize the probabilities; in prac-tice, this is not strictly required by the log-linear SMT model.Each of our paraphrased sentences differs from itsoriginal sentence by a single word, which preventscombinatorial explosions: on average, we generate14 paraphrased versions per input sentence.
It fur-ther ensures that the paraphrased parts of the sen-tences will not dominate the word alignments or thephrase pairs, and that there would be sufficient inter-action at word alignment time between the originalsentences and their paraphrased versions.3.4 Phrase-Level ParaphrasingWhile our sentence-level paraphrasing informs thedecoder about the origin of each phrase pair (orig-inal or paraphrased bi-text), it provides no indica-tion about how good the phrase pairs from the para-phrased bi-text are likely to be.Following Callison-Burch et al (2006), we fur-ther augment the phrase table with one additionalfeature whose value is 1 for the phrase pairs com-ing from the original bi-text, and maxp Pr(p?|p) forthe phrase pairs extracted from the paraphrased bi-text.
Here p is a Malay phrase from T , and p?
is aMalay phrase from T ?
that does not exist in T but isobtainable from p by substituting one or more wordsin p with their derivationally related forms generatedby morphological analysis.
The probability Pr(p?|p)is calculated using phrase-level pivoting through En-glish in the original phrase table T as follows (unlikeword-level pivoting, here ei is an English phrase):Pr(p?|p) = ?i Pr(p?|ei)Pr(ei|p)We estimate the probabilities Pr(ei|p) andPr(p?|ei) as we did for word-level pivoting, exceptthat this time we use the list of the phrase pairs ex-tracted from the original training bi-text, while be-fore we used IBM model 4 word alignments.
Whencalculating Pr(p?|ei), we think of p?
as the set of allpossible Malay phrases q in T that are reducible top?
by morphological analysis of the words they con-tain.
This can be rewritten as follows:Pr(p?|ei) =?q:p?
?par(q) Pr(q|ei)where par(q) is the set of all possible phrase-levelparaphrases for the Malay phrase q.The probability Pr(q|ei) is estimated using maxi-mum likelihood from the list of phrase pairs.
Thereis no combinatorial explosion here, since the phrasesare short and contain very few paraphrasable words.1302Number of sentence pairs 1K 2K 5K 10K 20K 40K 80K 160K 320KNumber of English words 30K 60K 151K 301K 602K 1.2M 2.4M 4.7M 9.5Mbaseline 23.81 27.43 31.53 33.69 36.68 38.49 40.53 41.80 43.02lemmatize all 22.67 26.20 29.68 31.53 33.91 35.64 37.17 38.58 39.68-1.14 -1.23 -1.85 -2.16 -2.77 -2.85 -3.36 -3.22 -3.34?noisier?
channel model (Dyer, 2007) 23.27 28.42 32.66 33.69 37.16 38.14 39.79 41.76 42.77-0.54 +0.99 +1.13 +0.00 +0.48 -0.35 -0.74 -0.04 -0.25lattice + sent-par (orig+lemma) 24.71 28.65 32.42 34.95 37.32 38.40 39.82 41.97 43.36+0.90 +1.22 +0.89 +1.26 +0.64 -0.09 -0.71 +0.17 +0.34lattice + sent-par 24.97 29.11 33.03 35.12 37.39 38.73 41.04 42.24 43.52+1.16 +1.68 +1.50 +1.43 +0.71 +0.24 +0.51 +0.44 +0.50lattice + sent-par + word-par 25.14 29.17 33.00 35.09 37.39 38.76 40.75 42.23 43.58+1.33 +1.74 +1.47 +1.40 +0.71 +0.27 +0.22 +0.43 +0.56lattice + sent-par + word-par + phrase-par 25.27 29.19 33.35 35.23 37.46 39.00 40.95 42.30 43.73+1.46 +1.76 +1.82 +1.54 +0.78 +0.51 +0.42 +0.50 +0.71Table 1: Evaluation results.
Shown are BLEU scores and improvements over the baseline (in %) for different numbersof training sentences.
Statistically significant improvements are in bold for p < 0.01 and in italic for p < 0.05.4 Experiments4.1 DataWe created our Malay-English training and develop-ment datasets from data that we downloaded fromthe Web and then sentence-aligned using variousheuristics.
Thus, we ended up with 350,003 trainingsentence pairs, including 10.4M English and 9.7MMalay word tokens.
We further downloaded 49.8Mword tokens of monolingual English text, which weused for language modeling.For testing, we used 1,420 sentences with 28.8KMalay word tokens, which were translated by threehuman translators, yielding translations of 32.8K,32.4K, and 32.9K English word tokens, respectively.For development, we used 2,000 sentence pairs of63.4K English and 58.5K Malay word tokens.4.2 General Experimental SetupFirst, we tokenized and lowercased all datasets:training, development, and testing.
We then builtdirected word-level alignments for the training bi-text for English?Malay and for Malay?Englishusing IBM model 4 (Brown et al, 1993), whichwe symmetrized using the intersect+grow heuristic(Och and Ney, 2003).
Next, we extracted phrase-level translation pairs of maximum length seven,which we scored and used to build a phrase tablewhere each phrase pair is associated with the fol-lowing five standard feature functions: forward andreverse phrase translation probabilities, forward andreverse lexicalized phrase translation probabilities,and phrase penalty.We trained a log-linear model using the followingstandard SMT feature functions: trigram languagemodel probability, word penalty, distance-based dis-tortion cost, and the five feature functions from thephrase table.
We set al weights on the developmentdataset by optimizing BLEU (Papineni et al, 2002)using minimum error rate training (Och, 2003), andwe plugged them in a beam search decoder (Koehnet al, 2007) to translate the Malay test sentences toEnglish.
Finally, we detokenized the output, and weevaluated it against the three reference translations.4.3 SystemsUsing the above general experimental setup, we im-plemented the following baseline systems:?
baseline.
This is the default system, which usesno morphological processing.?
lemmatize all.
This is the second baseline thatuses lemmatized versions of the Malay side ofthe training, development and testing datasets.?
?noisier?
channel model.6 This is the model ofDyer (2007).
It uses 0-1 weights in the latticeand only allows lemmata as alternative word-forms; it uses no sentence-level or phrase-levelparaphrases.6We also tried the word segmentation model of Dyer (2009)as implemented in the cdec decoder (Dyer et al, 2010), whichlearns word segmentation lattices from raw text in an unsu-pervised manner.
Unfortunately, it could not learn meaning-ful word segmentations for Malay, and thus we do not compareagainst it.
We believe this may be due to its focus on word seg-mentation, which is of limited use for Malay.1303sent.
system 1-gram 2-gram 3-gram 4-gram1k baseline 59.78 29.60 17.36 10.46paraphrases 62.23 31.19 18.53 11.352k baseline 64.20 33.46 20.41 12.92paraphrases 66.38 35.42 21.97 14.065k baseline 68.12 38.12 24.20 15.72paraphrases 70.41 40.13 25.71 17.0210k baseline 70.13 40.67 26.15 17.27paraphrases 72.04 42.28 27.55 18.3620k baseline 73.19 44.12 29.14 19.50paraphrases 73.28 44.43 29.77 20.3140k baseline 74.66 45.97 30.70 20.83paraphrases 75.47 46.54 31.09 21.1780k baseline 75.72 48.08 32.80 22.59paraphrases 76.03 48.47 33.20 23.00160k baseline 76.55 49.21 34.09 23.78paraphrases 77.14 49.89 34.57 24.06320k baseline 77.72 50.54 35.19 24.78paraphrases 78.03 51.24 35.99 25.42Table 2: Detailed BLEU n-gram precision scores: in%, for different numbers of training sentence pairs, forbaseline and lattice + sent-par + word-par + phrase-par.Our full morphological paraphrasing system islattice + sent-par + word-par + phrase-par.
Wealso experimented with some of its componentsturned off.
lattice + sent-par + word-par excludesthe additional feature from phrase-level paraphras-ing.
lattice + sent-par has all the morphologicallysimpler derived forms in the lattice during decod-ing, but their weights are uniformly set to 0 ratherthan obtained using pivoting from word alignments.Finally, in order to compare closely to the ?noisier?channel model, we further limited the morpholog-ical variants of lattice + sent-par in the lattice tolemmata only in lattice + sent-par (orig+lemma).5 Results and DiscussionThe experimental results are shown in Table 1.First, we can see that lemmatize all has a consis-tently disastrous effect on BLEU, which shows thatMalay morphology does indeed contain informationthat is important when translating to English.Second, Dyer (2007)?s ?noisier?
channel modelhelps for small datasets only.
It performs worse thanlattice + sent-par (orig+lemma), from which it dif-fers in the phrase table only; this confirms the im-portance of our sentence-level paraphrasing.Moving down to lattice + sent-par, we can seethat using multiple morphological wordforms in-stead of just lemmata has a consistently positive im-pact on BLEU for datasets of all sizes.Sent.
System BLEU NIST TER METEOR TESLA1k baseline 23.81 6.7013 64.50 49.26 1.6794paraphrases 25.27 6.9974 63.03 52.32 1.75792k baseline 27.43 7.3790 61.03 54.29 1.8718paraphrases 29.19 7.7306 59.37 57.32 2.00315k baseline 31.53 8.0992 57.12 59.09 2.1172paraphrases 33.35 8.4127 55.41 61.67 2.224010k baseline 33.69 8.5314 55.24 62.26 2.2656paraphrases 35.23 8.7564 53.60 63.97 2.363420k baseline 36.68 8.9604 52.56 64.67 2.3961paraphrases 37.46 9.0941 52.16 66.42 2.462140k baseline 38.49 9.3016 51.20 66.68 2.5166paraphrases 39.00 9.4184 50.68 67.60 2.560480k baseline 40.53 9.6047 49.88 68.77 2.6331paraphrases 40.95 9.6289 49.09 69.10 2.6628160k baseline 41.80 9.7479 48.97 69.59 2.6887paraphrases 42.30 9.8062 48.29 69.62 2.7049320k baseline 43.02 9.8974 47.44 70.23 2.7398paraphrases 43.73 9.9945 47.07 70.87 2.7856Table 3: Results for different evaluation measures: forbaseline and lattice + sent-par + word-par + phrase-par(in % for all measures except for NIST).Adding weights obtained using word-level piv-oting in lattice + sent-par + word-par helps abit more, and also using phrase-level paraphrasingweights yields even bigger further improvements forlattice + sent-par + word-par + phrase-par.Overall, our morphological paraphrases yield sta-tistically significant improvements (p < 0.01) inBLEU, according to Collins et al (2005)?s sign test,for bi-texts as large as 320,000 sentence pairs.A closer look at BLEU.
Table 2 shows detailedn-gram BLEU precision scores for n=1,2,3,4.
Oursystem outperforms the baseline on all precisionscores and for all numbers of training sentences.Other evaluation measures.
Table 3 reportsthe results for five evaluation measures: BLEUand NIST 11b, TER 0.7.25 (Snover et al, 2006),METEOR 1.0 (Lavie and Denkowski, 2009), andTESLA (Liu et al, 2010).
Our system consistentlyoutperforms the baseline for all measures.Example translations.
Table 4 shows two trans-lation examples.
In the first example, the redupli-cation bekalan-bekalan (?supplies?)
is an unknownword, and was left untranslated by the baseline sys-tem.
It was not a problem for our system though,which first paraphrased it as bekalan and then trans-lated it as supply.
Even though this is still wrong (weneed the plural supplies), it is arguably preferable topassing the word untranslated; it also allowed for abetter translation of the surrounding context.1304src : Mercy Relief telah menghantar 17 khemah khas bernilai $5,000 setiap satu yang boleh menampung kelas seramai 30pelajar, selain bekalan-bekalan lain seperti 500 khemah biasa, barang makanan dan ubat-ubatan untuk mangsa gempa Sichuan.ref1: Mercy Relief has sent 17 special tents valued at $5,000 each, that can accommodate a class of 30 students, includingother aid supplies such as 500 normal tents, food and medicine for the victims of Sichuan quake.base: mercy relief has sent 17 special tents worth $5,000 each class could accommodate a total of 30 students, besides otherbekalan-bekalan 500 tents as usual, foodstuff and medicines for sichuan quake relief.para: mercy relief has sent 17 special tents worth $5,000 each class could accommodate a total of 30 students, besides othersupply such as 500 tents, food and medicines for sichuan quake relief.src : Walaupun hidup susah, kami tetap berusaha untuk menjalani kehidupan seperti biasa.ref1: Even though life is difficult, we are still trying to go through life as usual.base: despite the hard life, we will always strive to undergo training as usual.para: despite the hard life, we will always strive to live normal.Table 4: Example translations.
For each example, we show a source sentence (src), one of the three referencetranslations (ref1), and the outputs of baseline (base) and of lattice + sent-par + word-par + phrase-par (para).In the second example, the baseline system trans-lated menjalani kehidupan (lit.
?go through life?
)as undergo training, because of a bad phrase pair,which was extracted from wrong word alignments.Note that the words menjalani (?go through?)
andkehidupan (?life/existence?)
are derivational formsof jalan (?go?)
and hidup (?life/living?
), respectively.Thus, in the paraphrasing system, they were in-volved in sentence-level paraphrasing, where thealignments were improved.
While the wrong phrasepair was still available, the system chose a better onefrom the paraphrased training bi-text.6 Related WorkMost research in SMT for a morphologically richsource language has focused on inflected forms ofthe same word.
The assumption is that they wouldhave similar semantics and thus could have the sametranslation.
Researchers have used stemming (Yangand Kirchhoff, 2006), lemmatization (Al-Onaizan etal., 1999; Goldwater and McClosky, 2005; Dyer,2007), or direct clustering (Talbot and Osborne,2006) to identify such groups of words and use themas equivalence classes or as possible alternatives intranslation.
Frameworks for the simultaneous use ofdifferent word-level representations have been pro-posed as well (Koehn and Hoang, 2007).A second important line of research has focusedon word segmentation, which is useful for languageslike German, which are rich in compound words thatare spelled concatenated (Koehn and Knight, 2003;Yang and Kirchhoff, 2006), or like Arabic, Turk-ish, Finnish, and, to a lesser extent, Spanish andItalian, where clitics often attach to the precedingword (Habash and Sadat, 2006).
For languages withmore or less regular inflectional morphology likeArabic or Turkish, another good idea is to segmentwords into morpheme sequences, e.g., prefix(es)-stem-suffix(es), which can be used instead of theoriginal words (Lee, 2004) or in addition to them.This can be achieved using a lattice input to thetranslation system (Dyer et al, 2008; Dyer, 2009).Unfortunately, none of these general lines of re-search suits Malay well, whose compounds arerarely concatenated, clitics are not so frequent, andmorphology is mostly derivational, and thus likelyto generate words whose semantics substantially dif-fers from the semantics of the original word.
There-fore, we cannot expect the existence of equivalenceclasses: it is only occasionally that two derivation-ally related wordforms would share the same tar-get language translation.
Thus, instead of look-ing for equivalence classes, we have focused on thepairwise relationship between derivationally relatedwordforms, which we treat as potential paraphrases.Our approach is an extension of the ?noisier?channel model of Dyer (2007).
He starts by generat-ing separate word alignments for the original train-ing bi-text and for a version of it where the sourceside has been lemmatized.
Then, the two bi-textsand their word alignments are concatenated and usedto build a phrase table.
Finally, the source sides ofthe development and the test datasets are convertedinto confusion networks where additional arcs areadded for word lemmata.
The arc weights are set to1 for the original wordforms and to 0 for the lem-mata.
In contrast, we provide multiple paraphras-ing alternatives for each morphologically complexword, including derivational forms that occupy in-termediary positions between the original wordform1305and its lemma.
Note that some of those paraphrasingalternatives are multi-word, and thus we use a latticeinstead of a confusion network.
Moreover, we givedifferent weights to the different alternatives ratherthen assigning them all 0.Second, our work is related to that of Dyer etal.
(2008), who use a lattice to add a single alter-native clitic-segmented version of the original wordfor Arabic.
However, we provide multiple alterna-tives.
We also include derivational forms in addi-tion to clitic-segmented ones, and we give differentweights to the different alternatives (instead of 0).Third, our work is also related to that of Dyer(2009), who uses a lattice to add multiple alterna-tive segmented versions of the original word for Ger-man, Hungarian, and Turkish.
However, we focuson derivational morphology rather than on cliticsand inflections, add derivational forms in additionto clitic-segmented ones, and use cross-lingual wordpivoting to estimate paraphrase probabilities.Finally, our work is related to that of Callison-Burch et al (2006), who use cross-lingual pivot-ing to generate phrase-level paraphrases with corre-sponding probabilities.
However, our paraphrasesare derived through morphological analysis; thus,we do not need corpora in additional languages.7 Conclusion and Future WorkWe have presented a novel approach to trans-lating from a morphologically complex language,which uses paraphrases and paraphrasing tech-niques at three different levels of translation: word-level, phrase-level, and sentence-level.
Our experi-ments translating from Malay, whose morphology ismostly derivational, into English have shown signif-icant improvements over rivaling approaches basedon several automatic evaluation measures.In future work, we want to improve the proba-bility estimations for our paraphrasing models.
Wealso want to experiment with other morphologicallycomplex languages and other SMT models.AcknowledgmentsThis work was supported by research grantPOD0713875.
We would like to thank the anony-mous reviewers for their detailed and constructivecomments, which have helped us improve the paper.ReferencesMirna Adriani, Jelita Asian, Bobby Nazief, S. M.M.Tahaghoghi, and Hugh E. Williams.
2007.
StemmingIndonesian: A confix-stripping approach.
ACM Trans-actions on Asian Language Information Processing,6:1?33.Yaser Al-Onaizan, Jan Curin, Michael Jahr, KevinKnight, John Lafferty, Dan Melamed, Franz-JosefOch, David Purdy, Noah A. Smith, and DavidYarowsky.
1999.
Statistical machine translation.Technical report, JHU Summer Workshop.Timothy Baldwin and Su?ad Awab.
2006.
Open sourcecorpus analysis tools for Malay.
In Proceedings of the5th International Conference on Language Resourcesand Evaluation, LREC ?06, pages 2212?2215.Peter F. Brown, Vincent J. Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: parameter esti-mation.
Computational Linguistics, 19(2):263?311.Chris Callison-Burch, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine translationusing paraphrases.
In Proceedings of the Human Lan-guage Technology Conference of the North AmericanChapter of the Association for Computational Linguis-tics, HLT-NAACL ?06, pages 17?24.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics, ACL ?05, pages 263?270.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause restructuring for statistical machinetranslation.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics,ACL ?05, pages 531?540.Christopher Dyer, Smaranda Muresan, and Philip Resnik.2008.
Generalizing word lattice translation.
In Pro-ceedings of the 46th Annual Meeting of the Associationfor Computational Linguistics, ACL ?08, pages 1012?1020.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec: Adecoder, alignment, and learning framework for finite-state and context-free translation models.
In Proceed-ings of the ACL 2010 System Demonstrations, ACL?10, pages 7?12.Christopher Dyer.
2007.
The ?noisier channel?
: trans-lation from morphologically complex languages.
InProceedings of the Second Workshop on StatisticalMachine Translation, WMT ?07, pages 207?211.Chris Dyer.
2009.
Using a maximum entropy model tobuild segmentation lattices for MT.
In Proceedingsof Human Language Technologies: The 2009 Annual1306Conference of the North American Chapter of the As-sociation for Computational Linguistics, NAACL ?09,pages 406?414.Michel Galley, Mark Hopkins, Kevin Knight, and DanielMarcu.
2004.
What?s in a translation rule?
In Pro-ceedings of the Human Language Technology Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, HLT-NAACL ?04,pages 273?280.Sharon Goldwater and David McClosky.
2005.
Improv-ing statistical MT through morphological analysis.
InProceedings of the Conference on Human LanguageTechnology and Empirical Methods in Natural Lan-guage Processing, HLT-EMNLP ?05, pages 676?683.Nizar Habash and Fatiha Sadat.
2006.
Arabic prepro-cessing schemes for statistical machine translation.
InProceedings of the Human Language Technology Con-ference of the North American Chapter of the Associ-ation for Computational Linguistics, Companion Vol-ume: Short Papers, HLT-NAACL ?06, pages 49?52.Philipp Koehn and Hieu Hoang.
2007.
Factored transla-tion models.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing, EMNLP-CoNLL ?07, pages 868?876.Philipp Koehn and Kevin Knight.
2003.
Empirical meth-ods for compound splitting.
In Proceedings of the 10thConference of the European Chapter of the Associa-tion for Computational Linguistics, EACL ?03, pages187?193.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the 2003 Conference of the North AmericanChapter of the Association for Computational Linguis-tics on Human Language Technology, NAACL ?03,pages 48?54.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Associationfor Computational Linguistics Companion Volume onDemo and Poster Sessions, ACL ?07, pages 177?180.Alon Lavie and Michael J. Denkowski.
2009.
The me-teor metric for automatic evaluation of machine trans-lation.
Machine Translation, 23:105?115.Young-Suk Lee.
2004.
Morphological analysis for sta-tistical machine translation.
In Proceedings of the Hu-man Language Technology Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, HLT-NAACL ?04, pages 57?60.Chang Liu, Daniel Dahlmeier, and Hwee Tou Ng.2010.
TESLA: Translation evaluation of sentenceswith linear-programming-based analysis.
In Proceed-ings of the Joint Fifth Workshop on Statistical MachineTranslation and MetricsMATR, WMT ?10, pages 354?359.Preslav Nakov and Hwee Tou Ng.
2009.
Improved statis-tical machine translation for resource-poor languagesusing related resource-rich languages.
In Proceedingsof the 2009 Conference on Empirical Methods in Nat-ural Language Processing, EMNLP ?09, pages 1358?1367.Preslav Nakov.
2008.
Improved statistical machinetranslation using monolingual paraphrases.
In Pro-ceedings of the 18th European Conference on ArtificialIntelligence, ECAI ?08, pages 338?342.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics, ACL ?03, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Computa-tional Linguistics, ACL ?02, pages 311?318.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informedphrasal SMT.
In Proceedings of the 43rd AnnualMeeting of the Association for Computational Linguis-tics, ACL ?05, pages 271?279.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of the Association for Machine Trans-lation in the Americas, AMTA ?06, pages 223?231.David Talbot and Miles Osborne.
2006.
Modelling lex-ical redundancy for machine translation.
In Proceed-ings of the 21st International Conference on Compu-tational Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics, COLING-ACL ?06, pages 969?976.Hua Wu and Haifeng Wang.
2007.
Pivot languageapproach for phrase-based statistical machine transla-tion.
Machine Translation, 21(3):165?181.Mei Yang and Katrin Kirchhoff.
2006.
Phrase-basedbackoff models for machine translation of highly in-flected languages.
In Proceedings of the EuropeanChapter of the Association for Computational Linguis-tics, EACL ?06, pages 41?48.1307
