AUGMENTED PHRASE STRUCTURE GRAMMARSGeorge E. HeidornComputer Sciences DepartmentIBM Thomas J. Watson Research CenterYorktown Heights, NYABSTRACTAugmented phrase structure grammarsconsist of phrase structure rules ~withembedded condit ions and structure-bui ld ingactions written in a special ly developedlanguage.
An attr ibute-value,record-or iented information structure is anintegral part of the theory.I.
INTRODUCTIONAn augmented phrase structure grammar(APSG) consists of a col lection of phrasestructure rules which are augmented byarbitrary condit ions and structure bui ldingactions.
This basic idea is not new, havingbeen used in syntax-directed compil ing \[e.g.I\] as well as in natural  language processing\[e.g.
2\], but what is new are thepart icular language in which these rules arewritten and the algor i thms that apply them.This brief paper is intended to serveas an introduct ion to augmented phrasestructure grammars.
First, the form of datastructure used is discussed, fol lowed bydiscussions of the analysis and synthesis oftext, i.e.
decoding and encoding.
(Although this session of the workshop isdevoted to natural language inout, thisbrief discussion of synthesis is includedbecause one of the important features ofAPSG is the consistent manner in which bothdecoding and encoding are specif ied.)
Thenthere is a section on implementat ions andapplications, fol lowed by concludingremarks.II.
DATA STRUCTUREThe data structure used by APSG is aform of semantic network, consist ing of"records" which are col lect ions ofattr ibute-value pairs.
Records represententities, either physical or abstract, suchdiverse things as vehicles, actions, wordsand verb phrases.
There are three dif ferentkinds of attr ibutes: relations, which haveas their values pointers to other records;properties, which have as their valueseither numbers or character strings; andindicators, which have bit string values andusual ly serve in a role similar to featuresin l inguistic terminology.A record that has a NAME attr ibute iscalled a "named record" and can be referredto by using the value of the NAME attr ibutein single quotes.
Named records are used tohold information that is re lat ivelypermanent, such as information aboutrelevant words and concepts, and are definedin the fol lowing manner (where theparentheses enclose structure-bui ld inginformation):SERVIC ( 'ACTIVITY' ,E,ES, ING,ED,TRANS,PS='VERB' ,XYZ=3)It is convenient to picture a record asa box enclosing a column of relation andproperty names on the left and a column ofcorresponding values on the right.Indicators which are present in the record(i.e.
have a non-zero value) are listed atthe bottom of the box.
The named record"SERVIC" defined above could be drawn as:I NAME "SERVIC" SUP "ACTIVITY" PS "VERB" XYZ 3|E,ES, ING,ED,TRANSDouble quotes enclose a character string,single quotes enclose the name of a namedrecord.
The values of the SUPerset and PS(part-of-speech) attr ibutes are real lypointers to the records "ACTIVITY" and"VERB" and could be drawn as directed linesto those other records if they were includedin the diagram.The named record "SERVIC" given herecould be considered to be a dict ionary entrystating that the VERB stem SERVIC can takeendings E, ES, ING and ED, the VERB SERVICis TRANSitive, and the concept SERVIC is anACTIVITY.
(When a named record name appearswithout the expl icit  mention of an attr ibutename, the SUPerset attr ibute is assumed.
)The XYZ attr ibute was included just toi l lustrate a numerical ly-valued property.Of course, the true meaning of any of thisinformation depends completely upon the wayit is used by the APSG rules.During decoding and encoding, recordscalled "segment records" are employed tohold information about segments of text.For example, the segment "are servicing"could be descr ibed by the record:I SUP "SERVIC" PRES,P3,PLUR,PROGwhich could be interpreted as saying that"are servicing" is the present, thirdperson, plural, progressive form of"service".
Similarly, the sentence "The bigmen are servic ing a truck."
could bedescribed by:ISUP "MAN"SUP "SERVIC" 1 ~S IZE  "BIG"AGENT iGOAL L I ~ ~PRES,PROG ~ S U P  "TRUCK"' ~INDEF,S INGwhere the indicators DEF and INDEF meandefinite and indefinite, respectively.
Thesentence "A truck is being serviced by thebig men."
could be described by exactly thesame record structure but with the addit ionof a PASSIVE indicator in the record on theleft.During a dialogue some records thatbegin as segment records may be kept tobecome part of longer term memory torepresent the entit ies (in the broadestsense of the term) that are being discussed.Segment records then might have pointersinto this longer term memory to showreferrents.
So, for example, the sentence"They are servic ing a truck."
might bedescribed by the same record structure shownabove if the referrent of "they" was knownto be a certain group of men who are big.III.
ANALYSIS OF TEXT (DECODING)Decoding is the process by which recordstructures of the sort just shown areconstructed from strings of text.
Themanner in which these records are to bebuilt is specif ied by APSG decoding rules.A decoding rule consists of a list of one ormore "segment types" (meta-symbols) on theleft of an arrow to indicate which types ofcontiguous segments must be present in orderfor a segment of the type on the right ofthe arrow to be formed.
Condit ions whichmust be satisf ied in order for the rule tobe appl icable may be stated in parentheseson the left side of the rule, andstructure-bui ld ing operations to beperformed when a new segment record iscreated are stated in parentheses on theright side.For i l lustrat ive purposes, some o f  therules which would be required to produce thesegment records shown in the previoussection will be discussed here.
Completeexamples are given in Reference 3.If the str ing "servicing" appeared inthe input, and the substr ing "servic" weredescribed by the VERBSTEM segment recordI SUP "SERVIC" Ithen the ruleVERBSTEM(ING*) I N G -->VERB(SUP(VERBSTEM,PRESTART)would form the VERB segment recordI SUP "SERVIC" 1.PRESPARTto describe the str ing "servicing",ident i fy ing it as the present part ic ipleform of service.
This rule says that if asegment of the str ing being decoded isdescribed as a VERBSTEM, and the associatedsegment record has a SUP attr ibute whichpoints to a named record which has an INGindicator (as the named record for "SERVIC"defined in the previous section would), andthis segment is fol lowed immediately by thecharacters "i", "n" and "g", then create aVERB segment record with the same SUP as theVERBSTEM and with a PRESPART indicator, todescribe the entire segment ("servicing" inthis case).Then the ruleVERB --> VERBPH(~VERB)would create aVERBPHrase segment recordwhich is a copy (4) of the VERB segmentrecord just shown.If the str ing "are"input were descr ibedrecordI SUP "BE" .PRES,P3,PLURthen the ruleappearing in theby the VERB segmentVERB('BE') VERBPH(PRESPART) -->VERBPH(PROG,FORM=FORM(VERB))would produce the new VERBPH segment recordI sup SERWC IPRES,P3,PLUR,PROG Ifrom the twojust shown, to describe thestr ing "are servicing".
This rule says thatif a segment of the str ing being decoded isdescribed as a VERB with a SUP of "BE', andit is fol lowed by a segment described as aVERBPH with a PRESPART indicator, thencreate a new VERBPH segment record which isa copy (automatical ly,  because the segmenttype is the same) of the VERBPH segmentrecord referred to on the left of the rule,but which as a PROGressive indicator and theFORM information from the VERB.
FORM wouldhave previously been defined as the name ofa group of indicators (i.e.
those having todo with tense, person and number).
S imi larrules can be used to recognize passives,perfects and modal construct ions.Cont inuing with the example, if thestr ing "the big men" were decoded to theNOUNPH segment recordI SUP "MAN"SIZE "BIG"DEF,PLURthen the ruleNOUNPH VERBPH(NUMB.EQ.NUMB(NOUNPH), SUBJECT)--> VERBPH(SUBJECT=NOUNPH,-NUMB,-PERS)would produce the new VERBPH segment record(the one on the left in this diagram)I SUP "SERVIC" ISUP "MAN" ISUBJECT ~ SIZE "BIG" PRES,PROG DEF,PLURfrom the previous VERBPH record, to descr ibethe str ing "the big men are servicing".
Itis important to real ize that the record onthe left in the above diagram is a segmentrecord that "covers" the entire str ing andthat the record shown on the right (which isthe same one from the previous diagram) Justserves as the value of its SUBJECTattr ibute.
The rule above says that if aNOUNPH is fol lowed by a VERBPH, and theNUMBer indicators of the VERBPH are the sameas the NUMBer indicators of the NOUNPH, andthe VERBPH does not already have a SUBJECTattribute, then create a new VERBPH segmentrecord which is a copy of the old one, giveit a SUBJECT attr ibute point ing to theNOUNPH record, and delete the NUMBer andIItt!iIiiIIiI!ItItiiPERson indicators.
Considering the subjectto be part of the verb phrase in this mannercan s impl i fy the handling of someconstruct ions involv ing inverted word order.If the string being decoded were "thebig men are servic ing a truck.
", a rulesimilar to the last one shown above could beused to pick up the direct object.
Then therule.
/VERBPH(SUBJECT,OBJECTI -TTRANS*IPASSIVE).--> SENT (~VERBPH)could be applied, which says if a VERBPHextending between two periods has a SUBJECTattr ibute and also either has an OBJECTattr ibute or does not need one because thereis no TRANSit ive indicator in the namedrecord pointed to by the SUP (i.e.
the verbis intransit ive) or because there is aPASSIVE indicator, then call it a SENTence.To get the record structure descr ib ingthis string into the form shown near the endof the previous section, one more rule wouldbe needed:SENT($ 'ACTION' ,~PASSIVE,SUBJECT)  -->SENT(AGENT=SUBJECT,GOAL=OBJECT,-SUBJECT,-OBJECT)This says that for a non-PASSIVE ACTIONSENTence that sti l l  has a SUBJECT attr ibute,set the AGENT and GOAL attr ibutes to thevalues of the SUBJECT and OBJECT attr ibutes,respectively, and then delete the SUBJECTand OBJECT attr ibutes from the record.
Thenotat ion $'ACTION" is read "in the set"ACTION'" and means that the named record"ACTION" must appear somewhere in theSUPerset chain of the current record.
Inthe previous section the named record"SERVIC" was def ined to have a SUP of"ACTIVITY'.
If the named record "ACTIVITY"were s imi lar ly def ined to have a SUP of"ACTION', the segment record underdiscussion here would satisfy the condit ion$'ACTION'.From the above examples it can be seenthat the condit ion speci f icat ions take theform of logical expressions involving thevalues of attr ibutes.
Each element in acondit ion speci f icat ion is basicaly of theform value.re lat ion.value,  but this is notobvious because there are several notat ionalshortcuts avai lable in the rule language.For example, "BE" is short forSUP.EG.
'BE ' ,PRESPART is short forPRESPART.NE.0,  and -~SUBJECT is short forSUBJECT.EQ.0.
The elements are combined byand's (commas) and or's (vertical bars).In most cases the attr ibute whose valueis being tested is to be found in thesegment record associated with theconstituent, but that is not always thecase.
For example, ING* tests the value ofthe ING indicator in the named recordpointed to by the SUP of the segment record,and could be written ING(SUP) orING(SUP).NE.0.
Another example isNUMB(NOUNPH) which was used to refer to thevalue of the NUMB indicators in the NOUNPHsegment in one of the rules above.From the examples it can also be seenthat creation speci f icat ions take the formof short procedures consist ing of s ta tementsfor sett ing the values of attr ibutes.
Eachelement in a creat ion speci f icat ion isbasical ly of the form attr ibute=value (where"=" means replacement),  but again this isnot obvious becuase of the notat ionalshortcuts used.
For example, SUP(VERBSTEM)is short for SUP=SUP(VERBSTEM),  PRESPART isshort for PRESPART:I  (note that this formhas a different meaning when it is used in acondit ion specif icat ion),  and -SUBJECT isshort for SUBJECT=0.In all of the examples here, theattr ibute whose value is set would be in thesegment record being built, but that neednot always be the case.
If, for example,there were some reason to want to give theAGENT record of an action an ABC attr ibuteequal to one more than the XYZ attr ibute ofthe concept record associated with thatact ion (i.e.
the named record pointed to byits SUP), the fo l lowing could be included inthe last rule shown:ABC(AGENT)=XYZ(SUP)+Iwhich can be read as "set the ABC attr ibuteof the AGENT of this record to the value ofthe XYZ attr ibute of the SUP of this recordplus I."
There is no limit to the nesting ofattr ibute names used in this manner.Although in the example rules givenhere the condit ions are primari ly syntactic,semantic constra ints  can be stated inexact ly the same manner.
Much of the recordbui lding shown here can be consideredsemantic (and somewhat case oriented).
Theimportant point, however, is that the kindof condit ion test ing and structure bui ld ingdone is at the discret ion of the person whowrites the rules.
Complete speci f icat ionsfor the APSG rule language are given inReference 3.The decoding a lgor i thm used with APSGis basical ly  that of a bottom-up,left-to-r ight,  paral le l -processing,syntax-d i rected compiler.
An important andnovel feature of this a lgor i thm is somethingcalled a "rule instance record", whichpr imari ly  mainta ins information abut thepotential  appl icab i l i ty  of a rule.
A ruleinstance record is in i t ia l ly  created for arule whenever a segment which can be thefirst const i tutent  of that rule becomesavai lable.
(A terminal  segment becomesavai lable by being obtained from the inputstream, and a non-terminal  segment becomesavai lable whenever a rule is applied.)
Thenthe rule instance record "waits" for asegment which can be the next const i tuent ofthe associated rule to become avai lable.When such a segment becomes available, therule instance record is "extended".
When arule instance record becomes complete (i.e.all of its const i tuents are avai lable),  theassociated rule is appl ied (i.e.
thesegment record speci f ied on the right isbuilt and made avai lable).
There may bemany rule instance records in existence fora part icular  rule at any point in time.Because of the parallel processingnature of the decoding algorithm, when asegment record is created to describe aportion of the input text it does not resultin the destruct ion of other recordsdescribing the same portion or parts of it.Local ambiguit ies caused by mult iple wordsenses, idioms and the like may result inmore than one segment record being createdto describe a part icular portion of thetext, but usual ly only one of them is ableto combine with its neighbors to become partof the analysis for an entire sentence.IV.
SYNTHESIS OF TEXT (ENCODING)Encoding is the process  by whichstrings of text are produced from recordstructures of the sort already shown.
Themanner in which this processing is to bedone is specif ied by APSG encoding rules.The right side of an encoding rule specif ieswhat segments a segment of the type on theleft s ide  is to be expanded into.Condit ions and structure-bui ld ing actionsare included in exactly the same manner asin decoding rules.The encoding a lgor i thm begins with asingle segment record and its associatedtype s ide-by-side on a stack.
At each cyclethrough the algorithm, the top pair isremoved from the stack and examined.
Ifthere is a rule that can be applied, itresults in new pairs being put on the top ofthe stack, according to its right hand side.Otherwise, either the character string valueof the NAME attr ibute of the SUP of thesegment record (e.g.
"servic") is put out,or the name of the segment type itself  (e.g.
"I") is put out.
Eventual ly the stackbecomes empty and the algor ithm terminates,having produced the desired output string.For example, if at some point thefol lowing pair were to come off the top ofthe stack:VERBPH I SUP "SERVIC" IPRES,P3,PLUR,PROGthe fol lowing encoding rule could beapplied:VERBPH(PROG) -->VERB('BE',FORM:FORM(VERBPH))VERB(-PROG,-FORM,PRESPART)result ing in the fol lowing two pairs beingput on the top of the stack:VERB ISUP "BE"PRES,P3,PLURVERBPH I SUPPRESPART'SERVIC" 1The above rule says that a VERBPH segmentwith a PROGressive indicator should beexpanded into a VERB segment with a SUP of"BE" and the same FORM indicators as theVERBPH, fol lowed by a new VERBPH segmentwhich begins as a copy (automatical ly) ofthe old one and then is modif ied by delet ingthe PROG and FORM indicators and setting thePRESPART indicator.When the VERB segment shown above comesoff the stack, a rule would be applied toput the string "are" into the output.
Then,after appl icat ion of a couple more rules,the top of the stack would have the fourpairsVERBSTEM \[ SUP "SERVIC" iI nullN nullG nullwhich would result in the string "servicing"being produced after four cycles of thealgorithm.
Complete encoding examples maybe found in Reference 3.V.
IMPLEMENTATIONS AND APPLICATIONSAs part of the original  work on APSG acomputer system called NLP (~atural Language~rocessor)  was developed in 1968.
This is aFORTRAN program for the IBM 360/370computers which wil l  accept as input namedrecord def init ions and decoding and encodingrules in exact ly the form shown in thispaper and then perfor m decoding and encodingof text \[3\].
A set of about 300 namedrecord def in i t ions and 800 rules was writtenfor NLP to implement a specif ic system(called NLPQ) which is capable of carryingon a dialogue in Engl ish about a simplequeuing problem and then producing a programin the GPSS s imulat ion language to solve theproblem \[3,4\].More recently a LISP implementat ion ofNLP has been done, which accepts exactly thesame input and does the same processing asthe FORTRAN version.
An interest ing featureof this new version is that the compilerpart, whose pr imary task is to translatecondit ion and creat ion speci f icat ions (i.e.the information in parentheses) into lambdaexpressions, is itself  wr i t ten as a set ofAPSG rules.
This work is part of a projectat IBM Research to develop a system whichwil l  produce appropr iate account ingappl icat ion programs after carrying on anatural  language dialogue with a businessmanabout his requirements.
APSG is also beingused in the development of a naturala laaguage query system for relat ional  databases and is being considered for use inother projects at IBM.
None of this recentwork has been documented yet.VI.
CONCLUDING REMARKSAPSG clearly has much in common withother current computat ional  l inguist ictheories, with the ideas of proceduralspeci f icat ion and arbitrary condit ions andst rucutre-bui ld ing actions being verypopular at this time.
It would seem to bemost similar to Woods" augmented transit ionnetworks (ATN) \[5\], especial ly  as used bySimmons \[6\].
Registers in the ATN modelcorrespond closely to attr ibutes of segmentrecords in APSG, and the semantic networkstructures of Simmons are very close to therecord structures of APSG.!!i|!!!p!|!I!!!D!
!Context-free phrasestructure  grammarshave been known to be inadequate fordescribing natural languages for many years,and context-sensitive phrase structuregrammars have not been found to be veryuseful, either.
Augmented phrase structuregrammars, however, appear to be able toexpress the facts of a natural language in avery concise and convenient manner, theyhave the power of computer programs, whilemaintaining the appearance of grammars.Although APSG was used successfully toimplement one fairly large system (NLPQ), itis too early to do a thorough appraisal ofits capabilities.
Through the extensive useanticipated in the next year however, itsstrengths and weaknesses should become moreapparent.ACKNOWLEDGEMENTSI am indebted to my former students at theNaval Postgraudate School for their effortson the original implementation andapplication, my colleagues at IBMResearch -- Martin Mikelsons, PeterSheridan, Irving Wladawsky and TedCodd -- for their interest, ideas and workon the current implementatons andapplications, and my wife, Beryl, for hertyping assistance and general helpfulness.REFERENCESI.
Balzer, R.M., and Farber, D.J.,"APAREL - a parse-request language,"COMM.
ACM 12, 11 (Nov. 1969), 624-631.2.
Thompson, F.B., Lockemann, P.C., Dostert,B., Deverill, R.S., "REL: a rapidlyextensible language system," In PROC.24th NAT'L CONF., ACM, NY, 1969, 399-417.3.
Heidorn, G.E., "Natural language inputsto a simulation programming system,"Technical Report NPS-55HD72101A, Nava lPostgraduate School, Monterey,California, Oct. 1972.4.
Heidorn, G.E., "English as a very highlevel language for simulationprogramming," Proc.
Symp.
on Very HighLevel Languages, SIGPLAN NOTICES 9,4(April 1974), 91-100.5.
Woods, W.A., "Transition network grammarsfor natural language analysis," COMM.ACM 13, 10 (Oct. 1970), 591-606.6.
Simmons, R.F., "Semantic networks: theircomputation and use for understandingEnglish sentences," in COMPUTER MODELS OFTHOUGHT AND LANGUAGE, R.C.
Schank andK.M.
Colby (Eds.
), W.H.
Freeman andCo., San Francisco, Calif., 1973, 63-113.!DIAGNOSIS AS A NOTION OF GRAMMARMitchel l  MarcusArt i f ic ia l  Intel l igence LaboratoryM.I.T.This paper will sketch an approach tonatural language parsing based on a newconception of what makes up a recognit iongrammar for syntactic analysis and how sucha grammar should be structured.
This theoryof syntactic analysis formalizes a notionvery much like the psychologist 's  notion of"perceptual strategies" \[Bever "70\] andmakes this formal ized notion - which will becalled the notion of wait -and-seediagnostics - a central and integral part ofa theory of what one knows about thestructure of language.
By recognit iongrammar, we mean here what a speaker of alanguage knows about that language thatallows him to assign grammatical  structureto the word strings that make up utterancesin that language.This theory of grammar is based on thehypothesis that every language user knows aspart of his recognit ion grammar a set ofhighly specif ic diagnostics that he uses todecide determinist ica l ly  what structure tobuild next at each point in the process ofparsing an utterance.
By determinist ica l i?I mean that once grammatical  structure isbuilt, it cannot be discarded in the normalcourse of the parsing process, i.e.
that no"backtracking" can take place unless thesentence is consciously perceived as being a"garden path".
This notion of grammar putsknowledge about control l ing the parsingprocess on an equal footing with knowledgeabout its possible outputs.To test this theory of grammar, aparser has been implemented that provides alanguage for writ ing grammars of this sort,and a grammar for Engl ish is current ly beingwritten that attempts to capture thewait -and-see diagnost ics needed to parseEngl ish within the constraints of thetheory.
The control  structure of the parserstrongly ref lects the assumptions the theorymakes about the structure of language, andthe discussion below wil l  use the structureof the parser as an example of theimpl icat ions of this theory for the parsingprocess.
The current grammar of Engl ish isdeep but not yet broad; this has al lowedinvest igat ion of the sorts of wait -and-seediagnost ics needed to handle complex Engl ishconstruct ions without a need to wait unti l  agrammar for the entire range of Engl ishconstruct ions could be written.
To givesome idea of the scope of the grammar, theparser is capable of handling sentenceslike:Do all the boys the l ibrarian gavebooks to want to read them?The men John wanted to be bel ieved by shothim yesterday.It should be ment ioned that certaingrammatical  phenomena are not handled at allby the present grammar, chief among themconjunct ion and certain important sorts oflexical ambiguity.
There is everyintention, however, of expanding the grammarto deal with them.Two ParadigmsTo explain exactly what the details ofthis wait -and-see (W&S) paradigm are, it isuseful to compare this notion with thecurrent prevai l ing parsing paradigm, which Iwil l  call the guess-and-then-backup (G&B)paradigm.
This paradigm is central to theparsers of both Terry' Winograd's SHRDLU\[Winograd "72\] and Bill Woods" LUNAR \[Woods"72\] systems.In a parser based on the G&B paradigm,various options are enumerated in theparser's grammar for the next possibleconst i tuent at any given point in the parseand these options are tested one at a timeagainst the input.
The parser assumestentat ively that one of these options iscorrect and then proceeds with this optionuntil  either the parse is completed or theoption fails, at which point the parsersimply backs up and tries the next optionenumerated in the parser's grammar.
This isthe paradigm of G&B: enumerate all options,pick one, and then (if it fails) backup andpick another.
While attempts have been madeto make this backup process clever,especial ly  in Winograd's  SHRDLU, it seemsthat it is very diff icult,  if not impossiblein general, to tell from the nature of thecul de sac exactly where the parser has goneastray.
In order to parse a sentence ofeven moderate complexity, there are not onebut many points at which a G&B parser mustmake guesses about what sort of structure toexpect next and at all of these points thecorrect hypothesis must be found before theparse can be successful ly  completed.Furthermore, the parser may proceedarb i t rar i ly  far ahead on any of thesehypotheses before discover ing that thehypothesis  was incorret, perhapsinval idat ing several other hypothesescontingent upon the first.
In essence, theG&B paradigm considers the grammar of anatural  language to be a t ree-structuredspace through which the parser must blindly,though perhaps cleverly, search to find acorrect parse.The W&S paradigm rejects the notion ofbackup as a standard control mechanism forparsing.
At each point in the parsingprocess, a W&S parser wil l  only buildgrammatical  structure it is sure it can use.The parser does this by determining, by atwo part process, which of the hypothesespossible at any given point of the parse iscorrect before attempting any of them.
Theparser first recognizes the specif ics i tuat ion it is in, determined both on thebasis of global expectat ions result ing fromwhatever structure it has parsed andabsorbed, and from features of lower levelsubstructures from a l ittle ahead in theinput to which internal structure can beassigned with certainty but whose functionis as yet undetermined.
Each such s i tuat ioncan be so defined that it restrains the setof possible hypotheses to at most two orthree.
If only one hypothesis is possible,a W&S parser wil l  take it as given,otherwise it wil l  proceed to the second step!!i!
!IIIIIIIIIIIiII!Iof the determinat ion process ,  to do a~if ferential  diagnosis to decide between thecompeting hypotheses.
For each dif ferentsituation, a W&S grammar includes a seriesof easi ly computed tests that decidesbetween the competing hypotheses.
The keyassumption of  the W&S paradigm, then?
i_gsthat the structure of  natural languageprovides enough and the right information t__~o~etermine exactly what too d__oo next at  eachpoint of  ~ Parse.
There is not suff ic ientroom here to discuss this assumption; thereader is invited to read \[Marcus ~74\],which discusses this assumption at length.Th___~e Parser Itself?
To  firm up this talk of "expectations',,"situations", and the like, it it useful tosee how these notions are real ized in theexist ing W&S parsing system.
Before we cando this, it wil l  be necessary to get anoverview of the structure and operat ion ofthe parser itself.A grammar in this system is made up ofpackets of pattern- invoked demons, whichwil l  be cal led modules.
(The notion ofpacket here derives from work by ScottFahlman \[Fahlman "73\].)
The parser itselfconsists of two levels, a group level and aclause level, and any packet of modules isintended to function at one level or theo~her.
Modules at group level are intendedto work on a buffer of words and word levelstructures and to eventual ly bu i ld  grouplevel structures, such as Noun Grouos (i.e.Noun Phrases up to the head noun) and VerbGrouPs (i.e.
the verb cluster up to themain verb), which are then put onto the endof a buffer of group level structures notyet absorbed by higher level processes.Modules at clause level are intended to workon these substructures and to assemble theminto clauses.
The group buffer and the wordbuffer can both grow up to somepredetermined length, on the order of 3, 4,or 5 structures.
Thus the modules at thelevel above needn't immediately use eachstructure as it comes into the buffer; butrather can let a small number of structures"pile up" and then examine these structuresbefore deciding how to use the first ofthem.
In this sense the modules at eachlevel have a l imited, sharply constra inedlook-ahead abil ity; they can wait and seewhat sort of environment surrounds asubstructure in the buffer below beforedeciding what the higher level funct ion ofthat substructure is.
(It should be notedthat the amount of look-ahead is constra inednot only by maximum buffer length but alsoby the restr ict ion that a module may accessonly the two substructures immediatelyfol lowing the one it is current ly trying touti l ize.
This constraint  is necessarybecause the substructure about to beut i l ized at any moment may not be the firstin the buffer, for various reasons.
)Every module consists of a pattern, apretest procedure, and a body to be executedif the pattern matches and the pretestsucceeds.
Each pattern consists of anordered list of sets of features.
Asstructures are built up by the parser, theyare label led with features, where a featureis any property of a structure that thegrammar wants to be visible at a glance toany module looking even casual ly at thatstructure.
(Structures can also haveregisters attached to them, carrying morespecial ized sorts of information; thecontents of a register are pr iv i leged inthat a module can access the contents of aregister only if it knows the name of thatregister.)
A module's  pattern matches if thefeature sets of the pattern are subsumed bythe feature sets of consecut ive structuresin the appropr iate buffer, with the matchstart ing at the effect ive beginning of thebuffer.Very few modules in any W&S grammar aealways active, wait ing to be tr iggered whentheir patterns match; a module is activeonly when a packet it is in has beenactivated, i.e.
added to the set ofpresent ly active packets.
Packets areact ivated or deact ivated by the parser atthe specif ic order of individual  modules;any module can add or remove packets fromthe set of active packets if it has reasonto do so.A pr ior i ty ordering of modules providessti l l  further control.
Every module isassigned a numerical  priority, creating apart ial  order ing on the active modules.
Atany time, only the h ighest -pr ior i t ied moduleof those whose patterns match wil l  beal lowed to run.
Thus, a special  purposemodule can edge out a general purpose moduleboth of whose patterns match in a givenenvironment,  or a module to handle somelast-resort  case can lurk low in a pool ofact ive modules, to serve as default only ifno h lgher -pr ior i t ied  module responds to asituation.F irmin~ Up The Notion Of  Si tuat ionThis, in brief, is the structure of theW&S parser; now we can turn to a discussionof how this structure ref lects thetheoret ica l  f ramework discussed above.
Letus begin by recast ing a statement madeabove: In decid ing what unique course ofact ion to take at any point in the parse,the parser first recognizes a speci f icwel l -def lned s i tuat ion on the basis of acombinat ion of global expectat ions and thespecif ic  features of lower levelsubstructures which are as yet unabsorbed.It should now become clear that what itmeans to have a global expectat ion is thatthe appropr iate packet is active in theparser, and that each module is i tself  thespecial ist  for the s i tuat ion that itspacket, pattern and pretest define.
Thegrammar act ivates and deact ivates packets toref lect its global expectat ions aboutsyntact ic structures that may be encounteredas a result of what it has seen so far.
(The parser might also act ivate packets onthe basis of what some higher level processin a natural  language understanding systemtells it to expect by way of d iscoursephenomena.)
These packets often ref lectrather large scale grammatical  expectat ions;for example, the fo l lowing are some packets
