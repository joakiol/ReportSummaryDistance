Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 73?80, Vancouver, October 2005. c?2005 Association for Computational LinguisticsA Discriminative Matching Approach to Word AlignmentBen Taskar Simon Lacoste-Julien Dan KleinComputer Science Division, EECS DepartmentUniversity of California, BerkeleyBerkeley, CA 94720AbstractWe present a discriminative, large-margin approach to feature-basedmatching for word alignment.
In thisframework, pairs of word tokens re-ceive a matching score, which is basedon features of that pair, including mea-sures of association between the words,distortion between their positions, sim-ilarity of the orthographic form, and soon.
Even with only 100 labeled train-ing examples and simple features whichincorporate counts from a large unla-beled corpus, we achieve AER perfor-mance close to IBM Model 4, in muchless time.
Including Model 4 predic-tions as features, we achieve a relativeAER reduction of 22% in over inter-sected Model 4 alignments.1 IntroductionThe standard approach to word alignment fromsentence-aligned bitexts has been to constructmodels which generate sentences of one lan-guage from the other, then fitting those genera-tive models with EM (Brown et al, 1990; Ochand Ney, 2003).
This approach has two primaryadvantages and two primary drawbacks.
In itsfavor, generative models of alignment are well-suited for use in a noisy-channel translation sys-tem.
In addition, they can be trained in an un-supervised fashion, though in practice they dorequire labeled validation alignments for tuningmodel hyper-parameters, such as null counts orsmoothing amounts, which are crucial to pro-ducing alignments of good quality.
A primarydrawback of the generative approach to align-ment is that, as in all generative models, explic-itly incorporating arbitrary features of the in-put is difficult.
For example, when consideringwhether to align two words in the IBM models(Brown et al, 1990), one cannot easily includeinformation about such features as orthographicsimilarity (for detecting cognates), presence ofthe pair in various dictionaries, similarity of thefrequency of the two words, choices made byother alignment systems on this sentence pair,and so on.
While clever models can implicitlycapture some of these information sources, ittakes considerable work, and can make the re-sulting models quite complex.
A second draw-back of generative translation models is that,since they are learned with EM, they requireextensive processing of large amounts of datato achieve good performance.
While tools likeGIZA++ (Och and Ney, 2003) do make it eas-ier to build on the long history of the generativeIBM approach, they also underscore how com-plex high-performance generative models can,and have, become.In this paper, we present a discriminative ap-proach to word alignment.
Word alignment iscast as a maximum weighted matching problem(Cormen et al, 1990) in which each pair of words(ej, fk) in a sentence pair (e, f) is associatedwith a score sjk(e, f) reflecting the desirabilityof the alignment of that pair.
The alignment73for the sentence pair is then the highest scoringmatching under some constraints, for examplethe requirement that matchings be one-to-one.This view of alignment as graph matching isnot, in itself, new: Melamed (2000) uses com-petitive linking to greedily construct matchingswhere the pair score is a measure of word-to-word association, and Matusov et al (2004)find exact maximum matchings where the pairscores come from the alignment posteriors ofgenerative models.
Tiedemann (2003) proposesincorporating a variety of word association?clues?
into a greedy linking algorithm.What we contribute here is a principled ap-proach for tractable and efficient learning of thealignment score sjk(e, f) as a function of ar-bitrary features of that token pair.
This con-tribution opens up the possibility of doing thekind of feature engineering for alignment thathas been so successful for other NLP tasks.
Wefirst present the algorithm for large margin es-timation of the scoring function.
We then showthat our method can achieve AER rates com-parable to unsymmetrized IBM Model 4, usingextremely little labeled data (as few as 100 sen-tences) and a simple feature set.
Remarkably,by including bi-directional IBM Model 4 predic-tions as features, we achieve an absolute AERof 5.4 on the English-French Hansards alignmenttask, a relative reduction of 22% in AER over in-tersected Model 4 alignments and, to our knowl-edge, the best AER result published on this task.2 AlgorithmWe model the alignment prediction task as amaximum weight bipartite matching problem,where nodes correspond to the words in thetwo sentences.
For simplicity, we assume herethat each word aligns to one or zero words inthe other sentence.
The edge weight sjkrepre-sents the degree to which word j in one sentencecan translate into the word k in the other sen-tence.
Our goal is to find an alignment thatmaximizes the sum of edge scores.
We representa matching using a set of binary variables yjkthat are set to 1 if word j is assigned to wordk in the other sentence, and 0 otherwise.
Thescore of an assignment is the sum of edge scores:s(y) =?jksjkyjk.
The maximum weight bi-partite matching problem, arg maxy?Y s(y), canbe solved using well known combinatorial algo-rithms or the following linear program:maxz?jksjkzjk(1)s.t.?jzjk?
1,?kzjk?
1, 0 ?
zjk?
1,where the continuous variables zjkcorrespond tothe binary variables yjk.
This LP is guaranteedto have integral (and hence optimal) solutionsfor any scoring function s(y) (Schrijver, 2003).Note that although the above LP can be used tocompute alignments, combinatorial algorithmsare generally more efficient.
However, we usethe LP to develop the learning algorithm below.For a sentence pair x, we denote positionpairs by xjkand their scores as sjk.
We letsjk= wf(xjk) for some user provided fea-ture mapping f and abbreviate wf(x,y) =?jkyjkwf(xjk).
We can include in the fea-ture vector the identity of the two words, theirrelative positions in their respective sentences,their part-of-speech tags, their string similarity(for detecting cognates), and so on.At this point, one can imagine estimating alinear matching model in multiple ways, includ-ing using conditional likelihood estimation, anaveraged perceptron update (see which match-ings are proposed and adjust the weights ac-cording to the difference between the guessedand target structures (Collins, 2002)), or inlarge-margin fashion.
Conditional likelihood es-timation using a log-linear model P (y | x) =1Zw(x)exp{wf(x,y)} requires summing over allmatchings to compute the normalization Zw(x),which is #P-complete (Valiant, 1979).
In ourexperiments, we therefore investigated the aver-aged perceptron in addition to the large-marginmethod outlined below.2.1 Large-margin estimationWe follow the large-margin formulation ofTaskar et al (2005a).
Our input is a set oftraining instances {(xi,yi)}mi=1, where each in-stance consists of a sentence pair xiand a target74alignment yi.
We would like to find parametersw that predict correct alignments on the train-ing data:yi= arg max?yi?Yiwf(xi, y?i), ?i,where Yiis the space of matchings appropriatefor the sentence pair i.In standard classification problems, we typi-cally measure the error of prediction, (yi, y?i),using the simple 0-1 loss.
In structured prob-lems, where we are jointly predicting multiplevariables, the loss is often more complex.
Whilethe F-measure is a natural loss function for thistask, we instead chose a sensible surrogate thatfits better in our framework: Hamming distancebetween yiand y?i, which simply counts thenumber of edges predicted incorrectly.We use an SVM-like hinge upper bound onthe loss (yi, y?i), given by max?yi?Yi[wfi(y?i) +i(y?i) ?
wfi(yi)], where i(y?i) = (yi, y?i), andfi(y?i) = f(xi, y?i).
Minimizing this upper boundencourages the true alignment yito be optimalwith respect to w for each instance i:min||w||??
?imax?yi?Yi[wfi(y?i) + i(y?i)] ?
wfi(yi),where ?
is a regularization parameter.In this form, the estimation problem is a mix-ture of continuous optimization over w and com-binatorial optimization over yi.
In order totransform it into a more standard optimizationproblem, we need a way to efficiently handle theloss-augmented inference, max?yi?Yi[wfi(y?i) +i(y?i)].
This optimization problem has pre-cisely the same form as the prediction prob-lem whose parameters we are trying to learn?
max?yi?Yiwfi(y?i) ?
but with an additionalterm corresponding to the loss function.
Our as-sumption that the loss function decomposes overthe edges is crucial to solving this problem.
Inparticular, we use weighted Hamming distance,which counts the number of variables in whicha candidate solution y?idiffers from the targetoutput yi, with different cost for false positives(c+) and false negatives (c-):i(y?i) =?jk[c-yi,jk(1 ?
y?i,jk) + c+y?i,jk(1 ?
yi,jk)]=?jkc-yi,jk+?jk[c+ ?
(c- + c+)yi,jk]y?i,jk.The loss-augmented matching problem can thenbe written as an LP similar to Equation 1 (with-out the constant term?jkc-yi,jk):maxz?jkzi,jk[wf(xi,jk) + c+ ?
(c- + c+)yi,jk]s.t.?jzi,jk?
1,?kzi,jk?
1, 0 ?
zi,jk?
1.Hence, without any approximations, we have acontinuous optimization problem instead of acombinatorial one:max?yi?Yiwfi(y?i)+i(y?i) = di+maxzi?Zi(wFi+ci)zi,where di=?jkc-yi,jkis the constant term, Fiis the appropriate matrix that has a column offeatures f(xi,jk) for each edge jk, ciis the vectorof the loss terms c+ ?
(c- + c+)yi,jkand finallyZi= {zi:?jzi,jk?
1,?kzi,jk?
1, 0 ?zi,jk?
1}.Plugging this LP back into our estimationproblem, we havemin||w||?
?maxz?Z?iwFizi+ cizi?
wFiyi, (2)where z = {z1, .
.
.
, zm}, Z = Z1?
.
.
.?Zm.
In-stead of the derivation in Taskar et al (2005a),which produces a joint convex optimizationproblem using Lagrangian duality, here wetackle the problem in its natural saddle-pointform.2.2 The extragradient methodFor saddle-point problems, a well-known solu-tion strategy is the extragradient method (Ko-rpelevich, 1976), which is closely related toprojected-gradient methods.The gradient of the objective in Equation 2is given by:?iFi(zi?
yi) (with respect to w)and Fiw + ci(with respect to each zi).
We de-note the Euclidean projection of a vector ontoZias PZi(v) = arg minu?Zi||v ?
u|| and pro-jection onto the ball ||w|| ?
?
as P?
(w) =?w/max(?, ||w||).75An iteration of the extragradient method con-sists of two very simple steps, prediction:w?t+1 = P?
(wt + ?k?iFi(yi?
zti));z?t+1i= PZi(zti+ ?k(Fiwt + ci));and correction:wt+1 = P?
(wt + ?k?iFi(yi?
z?t+1i));zt+1i= PZi(zti+ ?k(Fiw?t+1 + ci)),where ?kare appropriately chosen step sizes.The method is guaranteed to converge linearlyto a solution w?, z?
(Korpelevich, 1976; He andLiao, 2002; Taskar et al, 2005b).
Please seewww.cs.berkeley.edu/~taskar/extragradient.pdffor more details.The key subroutine of the algorithm is Eu-clidean projection onto the feasible sets Zi.
Incase of word alignment, Ziis the convex hull ofbipartite matchings and the problem reduces tothe much-studied minimum cost quadratic flowproblem (Bertsekas et al, 1997).
The projectionproblem PZi(z?i) is given byminz?jk12(z?i,jk?
zi,jk)2s.t.?jzi,jk?
1,?kzi,jk?
1, 0 ?
zi,jk?
1.We can now use a standard reduction of bipar-tite matching to min cost flow by introducing asource node connected to all the words in onesentence and a sink node connected to all thewords in the other sentence, using edges of ca-pacity 1 and cost 0.
The original edges jk havea quadratic cost 12(z?i,jk?
zi,jk)2 and capacity 1.Now the minimum cost flow from the source tothe sink computes projection of z?ionto ZiWeuse standard, publicly-available code for solvingthis problem (Guerriero and Tseng, 2002).3 ExperimentsWe applied this matching algorithm to word-level alignment using the English-FrenchHansards data from the 2003 NAACL sharedtask (Mihalcea and Pedersen, 2003).
Thiscorpus consists of 1.1M automatically alignedsentences, and comes with a validation set of 39sentence pairs and a test set of 447 sentences.The validation and test sentences have beenhand-aligned (see Och and Ney (2003)) and aremarked with both sure and possible alignments.Using these alignments, alignment error rate(AER) is calculated as:AER(A,S, P ) = 1 ?
|A ?
S| + |A ?
P ||A| + |S|Here, A is a set of proposed index pairs, S isthe sure gold pairs, and P is the possible goldpairs.
For example, in Figure 1, proposed align-ments are shown against gold alignments, withopen squares for sure alignments, rounded opensquares for possible alignments, and filled blacksquares for proposed alignments.Since our method is a supervised algorithm,we need labeled examples.
For the training data,we split the original test set into 100 trainingexamples and 347 test examples.
In all our ex-periments, we used a structured loss function(yi, y?i) that penalized false negatives 3 timesmore than false positives, where 3 was picked bytesting several values on the validation set.
In-stead of selecting a regularization parameter ?and running to convergence, we used early stop-ping as a cheap regularization method, by set-ting ?
to a very large value (10000) and runningthe algorithm for 500 iterations.
We selected astopping point using the validation set by simplypicking the best iteration on the validation set interms of AER (ignoring the initial ten iterations,which were very noisy in our experiments).
Allselected iterations turned out to be in the first50 iterations, as the algorithm converged fairlyrapidly.3.1 Features and ResultsVery broadly speaking, the classic IBM mod-els of word-level translation exploit four primarysources of knowledge and constraint: associationof words (all IBM models), competition betweenalignments (all models), zero- or first-order pref-erences of alignment positions (2,4+), and fer-tility (3+).
We model all of these in some way,76one of themajorobjectives oftheseconsultations is tomakesurethat therecoverybenefits all .leundelesgrandsobjectifsdelesconsultationsestdefaireensortequelarelanceprofitee?galementa`tous.one of themajorobjectives oftheseconsultations is tomakesurethat therecoverybenefits all .leundelesgrandsobjectifsdelesconsultationsestdefaireensortequelarelanceprofitee?galementa`tous.
(a) Dice only (b) Dice and Distanceone of themajorobjectives oftheseconsultations is tomakesurethat therecoverybenefits all .leundelesgrandsobjectifsdelesconsultationsestdefaireensortequelarelanceprofitee?galementa`tous.one of themajorobjectives oftheseconsultations is tomakesurethat therecoverybenefits all .leundelesgrandsobjectifsdelesconsultationsestdefaireensortequelarelanceprofitee?galementa`tous.
(c) Dice, Distance, Orthographic, and BothShort (d) All featuresFigure 1: Example alignments for each successive feature set.except fertility.1First, and, most importantly, we want to in-clude information about word association; trans-lation pairs are likely to co-occur together ina bitext.
This information can be captured,among many other ways, using a feature whose1In principle, we can model also model fertility, byallowing 0-k matches for each word rather than 0-1, andhaving bias features on each word.
However, we did notexplore this possibility.value is the Dice coefficient (Dice, 1945):Dice(e, f) = 2CEF (e, f)CE(e)CF(f)Here, CEand CFare counts of word occurrencesin each language, while CEFis the number ofco-occurrences of the two words.
With just thisfeature on a pair of word tokens (which dependsonly on their types), we can already make a stab77at word alignment, aligning, say, each Englishword with the French word (or null) with thehighest Dice value (see (Melamed, 2000)), sim-ply as a matching-free heuristic model.
WithDice counts taken from the 1.1M sentences, thisgives and AER of 38.7 with English as the tar-get, and 36.0 with French as the target (in linewith the numbers from Och and Ney (2003)).As observed in Melamed (2000), this use ofDice misses the crucial constraint of competi-tion: a candidate source word with high asso-ciation to a target word may be unavailable foralignment because some other target has an evenbetter affinity for that source word.
Melameduses competitive linking to incorporate this con-straint explicitly, while the IBM-style modelsget this effect via explaining-away effects in EMtraining.
We can get something much like thecombination of Dice and competitive linking byrunning with just one feature on each pair: theDice value of that pair?s words.2 With just aDice feature ?
meaning no learning is neededyet ?
we achieve an AER of 29.8, between theDice with competitive linking result of 34.0 andModel 1 of 25.9 given in Och and Ney (2003).An example of the alignment at this stage isshown in Figure 1(a).
Note that most errors lieoff the diagonal, for example the often-correctto-a` match.IBM Model 2, as usually implemented, addsthe preference of alignments to lie near the di-agonal.
Model 2 is driven by the product of aword-to-word measure and a (usually) Gaussiandistribution which penalizes distortion from thediagonal.
We can capture the same effect us-ing features which reference the relative posi-tions j and k of a pair (ej, fk).
In addition to aModel 2-style quadratic feature referencing rela-tive position, we threw in the following proxim-ity features: absolute difference in relative posi-tion abs(j/|e|?k/|f |), and the square and squareroot of this value.
In addition, we used a con-junction feature of the dice coefficient times theproximity.
Finally, we added a bias feature oneach edge, which acts as a threshold that allows2This isn?t quite competitive linking, because we usea non-greedy matching.in1978Americansdivorced1,122,000times .en1978,onaenregistre?1,122,000divorcessurlecontinent.in1978Americansdivorced1,122,000times .en1978,onaenregistre?1,122,000divorcessurlecontinent.
(a) (b)Figure 2: Example alignments showing the ef-fects of orthographic cognate features.
(a) Diceand Distance, (b) With Orthographic Features.sparser, higher precision alignments.
With thesefeatures, we got an AER of 15.5 (compare to 19.5for Model 2 in (Och and Ney, 2003)).
Note thatwe already have a capacity that Model 2 doesnot: we can learn a non-quadratic penalty withlinear mixtures of our various components ?
thisgives a similar effect to learning the variance ofthe Gaussian for Model 2, but is, at least inprinciple, more flexible.3 These features fix theto-a` error in Figure 1(a), giving the alignmentin Figure 1(b).On top of these features, we included otherkinds of information, such as word-similarityfeatures designed to capture cognate (and ex-act match) information.
We added a feature forexact match of words, exact match ignoring ac-cents, exact matching ignoring vowels, and frac-tion overlap of the longest common subsequence.Since these measures were only useful for longwords, we also added a feature which indicatesthat both words in a pair are short.
These or-thographic and other features improved AER to14.4.
The running example now has the align-ment in Figure 1(c), where one improvementmay be attributable to the short pair feature ?
ithas stopped proposing the-de, partially becausethe short pair feature downweights the score ofthat pair.
A clearer example of these featuresmaking a difference is shown in Figure 2, whereboth the exact-match and character overlap fea-3The learned response was in fact close to a Gaussian,but harsher near zero displacement.78tures are used.One source of constraint which our model stilldoes not explicitly capture is the first-order de-pendency between alignment positions, as in theHMM model (Vogel et al, 1996) and IBM mod-els 4+.
The the-le error in Figure 1(c) is symp-tomatic of this lack.
In particular, it is a slightlybetter pair according to the Dice value than thecorrect the-les.
However, the latter alignmenthas the advantage that major-grands follows it.To use this information source, we included afeature which gives the Dice value of the wordsfollowing the pair.4 We also added a word-frequency feature whose value is the absolutedifference in log rank of the words, discourag-ing very common words from translating to veryrare ones.
Finally, we threw in bilexical featuresof the pairs of top 5 non-punctuation words ineach language.5 This helped by removing spe-cific common errors like the residual tendencyfor French de to mistakenly align to English the(the two most common words).
The resultingmodel produces the alignment in Figure 1(d).It has sorted out the the-le / the-les confusion,and is also able to guess to-de, which is not themost common translation for either word, butwhich is supported by the good Dice value onthe following pair (make-faire).With all these features, we got a final AERof 10.7, broadly similar to the 8.9 or 9.7 AERsof unsymmetrized IBM Model 4 trained on thesame data that the Dice counts were takenfrom.6 Of course, symmetrizing Model 4 by in-tersecting alignments from both directions doesyield an improved AER of 6.9, so, while ourmodel does do surprisingly well with cheaply ob-tained count-based features, Model 4 does stilloutperform it so far.
However, our model can4It is important to note that while our matching algo-rithm has no first-order effects, the features can encodesuch effects in this way, or in better ways ?
e.g.
using asfeatures posteriors from the HMM model in the style ofMatusov et al (2004).5The number of such features which can be learneddepends on the number of training examples, and sincesome of our experiments used only a few dozen trainingexamples we did not make heavy use of this feature.6Note that the common word pair features affectedcommon errors and therefore had a particularly large im-pact on AER.Model AERDice (without matching) 38.7 / 36.0Model 4 (E-F, F-E, intersected) 8.9 / 9.7/ 6.9Discriminative MatchingDice Feature Only 29.8+ Distance Features 15.5+ Word Shape and Frequency 14.4+ Common Words and Next-Dice 10.7+ Model 4 Predictions 5.4Figure 3: AER on the Hansards task.also easily incorporate the predictions of Model4 as additional features.
We therefore addedthree new features for each edge: the predictionof Model 4 in the English-French direction, theprediction in the French-English direction, andthe intersection of the two predictions.
Withthese powerful new features, our AER droppeddramatically to 5.4, a 22% improvement over theintersected Model 4 performance.Another way of doing the parameter estima-tion for this matching task would have beento use an averaged perceptron method, as inCollins (2002).
In this method, we merely runour matching algorithm and update weightsbased on the difference between the predictedand target matchings.
However, the perfor-mance of the average perceptron learner on thesame feature set is much lower, only 8.1, noteven breaking the AER of its best single feature(the intersected Model 4 predictions).3.2 Scaling ExperimentsWe explored the scaling of our method by learn-ing on a larger training set, which we created byusing GIZA++ intersected bi-directional Model4 alignments for the unlabeled sentence pairs.We then took the first 5K sentence pairs fromthese 1.1M Model 4 alignments.
This gave usmore training data, albeit with noisier labels.On a 3.4GHz Intel Xeon CPU, GIZA++ took18 hours to align the 1.1M words, while ourmethod learned its weights in between 6 min-utes (100 training sentences) and three hours(5K sentences).794 ConclusionsWe have presented a novel discriminative, large-margin method for learning word-alignmentmodels on the basis of arbitrary features of wordpairs.
We have shown that our method is suit-able for the common situation where a moder-ate number of good, fairly general features mustbe balanced on the basis of a small amount oflabeled data.
It is also likely that the methodwill be useful in conjunction with a large labeledalignment corpus (should such a set be created).We presented features capturing a few separatesources of information, producing alignments onthe order of those given by unsymmetrized IBMModel 4 (using labeled training data of aboutthe size others have used to tune generativemodels).
In addition, when given bi-directionalModel 4 predictions as features, our methodprovides a 22% AER reduction over intersectedModel 4 predictions alone.
The resulting 5.4AER on the English-French Hansarks task is,to our knowledge, the best published AER fig-ure for this training scenario (though since weuse a subset of the test set, evaluations are notproblem-free).
Finally, our method scales tolarge numbers of training sentences and trainsin minutes rather than hours or days for thehigher-numbered IBM models, a particular ad-vantage when not using features derived fromthose slower models.ReferencesD.
P. Bertsekas, L. C. Polymenakos, and P. Tseng.
1997.An e-relaxation method for separable convex cost net-work flow problems.
SIAM J.
Optim., 7(3):853?870.P.
F. Brown, J. Cocke, S. A. Della Pietra, V. J. DellaPietra, F. Jelinek, J. D. Lafferty, R. L. Mercer, andP.
S. Roossin.
1990.
A statistical approach to machinetranslation.
Computational Linguistics, 16(2):79?85.M.
Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments withperceptron algorithms.
In Proc.
EMNLP.T.
H. Cormen, C. E. Leiserson, and R. L. Rivest.
1990.Introduction to Algorithms.
MIT Press, Cambridge,MA.L.
R. Dice.
1945.
Measures of the amount of ecologic as-sociation between species.
Journal of Ecology, 26:297?302.F.
Guerriero and P. Tseng.
2002.
Implementationand test of auction methods for solving generalizednetwork flow problems with separable convex cost.Journal of Optimization Theory and Applications,115(1):113?144, October.B.S.
He and L. Z. Liao.
2002.
Improvements of someprojection methods for monotone nonlinear variationalinequalities.
JOTA, 112:111:128.G.
M. Korpelevich.
1976.
The extragradient method forfinding saddle points and other problems.
Ekonomikai Matematicheskie Metody, 12:747:756.E.
Matusov, R. Zens, and H. Ney.
2004.
Symmetric wordalignments for statistical machine translation.
In Proc.of COLING 2004.I.
D. Melamed.
2000.
Models of translational equivalenceamong words.
Computational Linguistics, 26(2):221?249.R.
Mihalcea and T. Pedersen.
2003.
An evaluation ex-ercise for word alignment.
In Proceedings of the HLT-NAACL 2003 Workshop, Building and Using parallelTexts: Data Driven Machine Translation and Beyond,pages 1?6, Edmonton, Alberta, Canada.F.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?52.A.
Schrijver.
2003.
Combinatorial Optimization: Poly-hedra and Efficiency.
Springer.B.
Taskar, V. Chatalbashev, D. Koller, and C. Guestrin.2005a.
Learning structured prediction models: a largemargin approach.
In Proceedings of the InternationalConference on Machine Learning.B.
Taskar, S. Lacoste-Julien, and M. Jordan.
2005b.Structured prediction via the extragradient method.In Proceedings of Neural Information Processing Sys-tems.J.
Tiedemann.
2003.
Combining clues for word align-ment.
In Proceedings of EACL.L.
G. Valiant.
1979.
The complexity of computing thepermanent.
Theoretical Computer Science, 8:189?201.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-basedword alignment in statistical translation.
In COLING16, pages 836?841.80
