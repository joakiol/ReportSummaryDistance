Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 560?567,Sydney, July 2006. c?2006 Association for Computational LinguisticsLoss Minimization in Parse RerankingIvan TitovDepartment of Computer ScienceUniversity of Geneva24, rue Ge?ne?ral DufourCH-1211 Gene`ve 4, Switzerlandivan.titov@cui.unige.chJames HendersonSchool of InformaticsUniversity of Edinburgh2 Buccleuch PlaceEdinburgh EH8 9LW, United Kingdomjames.henderson@ed.ac.ukAbstractWe propose a general method for rerankerconstruction which targets choosing thecandidate with the least expected loss,rather than the most probable candidate.Different approaches to expected loss ap-proximation are considered, including es-timating from the probabilistic model usedto generate the candidates, estimatingfrom a discriminative model trained torerank the candidates, and learning to ap-proximate the expected loss.
The pro-posed methods are applied to the parsereranking task, with various baseline mod-els, achieving significant improvementboth over the probabilistic models and thediscriminative rerankers.
When a neuralnetwork parser is used as the probabilisticmodel and the Voted Perceptron algorithmwith data-defined kernels as the learningalgorithm, the loss minimization modelachieves 90.0% labeled constituents F1score on the standard WSJ parsing task.1 IntroductionThe reranking approach is widely used in pars-ing (Collins and Koo, 2005; Koo and Collins,2005; Henderson and Titov, 2005; Shen and Joshi,2003) as well as in other structured classifica-tion problems.
For structured classification tasks,where labels are complex and have an internalstructure of interdependency, the 0-1 loss consid-ered in classical formulation of classification al-gorithms is not a natural choice and different lossfunctions are normally employed.
To tackle thisproblem, several approaches have been proposedto accommodate loss functions in learning algo-rithms (Tsochantaridis et al, 2004; Taskar et al,2004; Henderson and Titov, 2005).
A very differ-ent use of loss functions was considered in the ar-eas of signal processing and machine translation,where direct minimization of expected loss (Min-imum Bayes Risk decoding) on word sequenceswas considered (Kumar and Byrne, 2004; Stol-cke et al, 1997).
The only attempt to use Mini-mum Bayes Risk (MBR) decoding in parsing wasmade in (Goodman, 1996), where a parsing al-gorithm for constituent recall minimization wasconstructed.
However, their approach is limitedto binarized PCFG models and, consequently, isnot applicable to state-of-the-art parsing meth-ods (Charniak and Johnson, 2005; Henderson,2004; Collins, 2000).
In this paper we considerseveral approaches to loss approximation on thebasis of a candidate list provided by a baselineprobabilistic model.The intuitive motivation for expected loss mini-mization can be seen from the following example.Consider the situation where there are a group ofseveral very similar candidates and one very dif-ferent candidate whose probability is just slightlylarger than the probability of any individual candi-date in the group, but much smaller than their totalprobability.
A method which chooses the maxi-mum probability candidate will choose this outliercandidate, which is correct if you are only inter-ested in getting the label exactly correct (i.e.
0-1loss), and you think the estimates are accurate.
Butif you are interested in a loss function where theloss is small when you choose a candidate whichis similar to the correct candidate, then it is betterto choose one of the candidates in the group.
Withthis choice the loss will only be large if the outlierturns out to be correct, while if the outlier is cho-sen then the loss will be large if any of the groupare correct.
In other words, the expected loss of560choosing a member of the group will be smallerthan that for the outlier.More formally, the Bayes risk of a model y =h(x) is defined asR(h) = Ex,y?
(y, h(x)), (1)where the expectation is taken over all the possi-ble inputs x and labels y and ?
(y, y?)
denotes aloss incurred by assigning x to y?
when the correctlabel is y.
We assume that the loss function pos-sesses values within the range from 0 to 1, whichis equivalent to the requirement that the loss func-tion is bounded in (Tsochantaridis et al, 2004).
Itfollows that an optimal reranker h?
is one whichchooses the label y that minimizes the expectedloss:h?
(x) = arg miny?
?G(x)?yP (y|x)?
(y, y?
), (2)where G(x) denotes a candidate list provided bya baseline probabilistic model for the input x.In this paper we propose different approaches toloss approximation.
We apply them to the parsereranking problem where the baseline probabilis-tic model is a neural network parser (Henderson,2003), and to parse reranking of candidates pro-vided by the (Collins, 1999) model.
The result-ing reranking method achieves very significant im-provement in the considered loss function and im-provement in most other standard measures of ac-curacy.In the following three sections we will discussthree approaches to learning such a classifier.
Thefirst two derive a classification criteria for use witha predefined probability model (the first genera-tive, the second discriminative).
The third de-fines a kernel for use with a classification methodfor minimizing loss.
All use previously proposedlearning algorithms and optimization criteria.2 Loss Approximation with aProbabilistic ModelIn this section we discuss approximating the ex-pected loss using probability estimates given bya baseline probabilistic model.
Use of probabil-ity estimates is not a serious limitation of thisapproach because in practice candidates are nor-mally provided by some probabilistic model andits probability estimates are used as additional fea-tures in the reranker (Collins and Koo, 2005; Shenand Joshi, 2003; Henderson and Titov, 2005).In order to estimate the expected loss on the ba-sis of a candidate list, we make the assumption thatthe total probability of the labels not in the can-didate list is sufficiently small that the difference?
(x, y?)
of expected loss between the labels in thecandidate list and the labels not in the candidatelist does not have an impact on the loss definedin (1):?
(x, y?)
=?y/?G(x) P (y|x)?
(y, y?
)?y/?G(x) P (y|x)?
(3)?y?G(x) P (y|x)?
(y, y?
)?y?G(x) P (y|x)This gives us the following approximation to theexpected loss for the label:l(x, y?)
=?y?G(x) P (y|x)?
(y, y?
)?y?G(x) P (y|x).
(4)For the reranking case, often the probabilisticmodel only estimates the joint probability P (x, y).However, neither this difference nor the denomi-nator in (4) affects the classification.
Thus, replac-ing the true probabilities with their estimates, wecan define the classifierh?
(x) = arg miny?
?G(x)?y?G(x)P (x, y|??)?
(y, y?
), (5)where ??
denotes the parameters of the probabilis-tic model learned from the training data.
This ap-proach for expected loss approximation was con-sidered in the context of word error rate minimiza-tion in speech recognition, see for example (Stol-cke et al, 1997).3 Estimating Expected Loss withDiscriminative ClassifiersIn this section we propose a method to improve onthe loss approximation used in (5) by constructingthe probability estimates using a trained discrimi-native classifier.
Special emphasis is placed on lin-ear classifiers with data-defined kernels for rerank-ing (Henderson and Titov, 2005), because they donot require any additional domain knowledge notalready encoded in the probabilistic model, andthey have demonstrated significant improvementover the baseline probabilistic model for the parsereranking task.
This kernel construction can bemotivated by the existence of a function whichmaps a linear function in the feature space of thekernel to probability estimates which are superiorto the estimates of the original probabilistic model.5613.1 Estimation with Fisher KernelsThe Fisher kernel for structured classificationis a trivial generalization of one of the bestknown data-defined kernels for binary classifica-tion (Jaakkola and Haussler, 1998).
The Fisherscore of an example input-label pair (x, y) is avector of partial derivatives of the log-likelihoodof the example with respect to the model parame-ters1:?FK??
(x, y) = (6)(logP (x, y|??
), ?logP (x,y|??)?
?1,..., ?logP (x,y|??)?
?l).This kernel defines a feature space which is appro-priate for estimating the discriminative probabilityin the candidate list in the form of a normalizedexponentialP (x, y)?y?
?G(x) P (x, y?)?
(7)exp(w?T ?FK??
(x, y))?y?
?G(x) exp(w?T ?FK??
(x, y?
))for some choice of the decision vector w = w?with the first component equal to one.It follows that it is natural to use an estimatorof the discriminative probability P (y|x) in expo-nential form and, therefore, the appropriate formof the loss minimizing classifier is the following:h?FK(x) = (8)arg miny?
?G(x)?y?G(x)exp(Aw?T ?FK??
(x, y?))?
(y, y?
),where w?
is learned during classifier training andthe scalar parameter A can be tuned on the devel-opment set.
From the construction of the Fisherkernel, it follows that the optimal value A is ex-pected to be close to inverse of the first componentof w?, 1/w?1.If an SVM is used to learn the classifier, thenthe form (7) is the same as that proposed by (Platt,1999), where it is proposed to use the logistic sig-moid of the SVM output as the probability estima-tor for binary classification problems.1The first component logP (x, y|??)
is not in the strictsense part of the Fisher score, but usually added to kernelfeatures in practice (Henderson and Titov, 2005).3.2 Estimation with TOP Kernels forRerankingThe TOP Reranking kernel was defined in (Hen-derson and Titov, 2005), as a generalization of theTOP kernel (Tsuda et al, 2002) proposed for bi-nary classification tasks.
The feature extractor forthe TOP reranking kernel is given by:?TK??
(x, y) = (9)(v(x, y, ??
), ?v(x, y, ??)?
?1,..., ?v(x, y, ??)?
?l),wherev(x, y, ??)
= log P (x, y|??)?
log?y??G(x)?
{y}P (x, y?|??
).The TOP reranking kernel has been demon-strated to perform better than the Fisher kernelfor the parse reranking task (Henderson and Titov,2005).
The construction of this kernel is moti-vated by the minimization of the classification er-ror of a linear classifier wT ???
(x, y).
This linearclassifier has been shown to converge, assumingestimation of the discriminative probability in thecandidate list can be in the form of the logistic sig-moid (Titov and Henderson, 2005):P (x, y)?y?
?G(x) P (x, y?)?
(10)11 + exp(?w?T ?TK??
(x, y))for some choice of the decision vector w = w?with the first component equal to one.
From thisfact, the form of the loss minimizing classifier fol-lows:h?TK(x) = (11)arg miny?
?G(x)?y?G(x)g(Aw?T ?TK??
(x, y?))?
(y, y?
),where g is the logistic sigmoid and the scalar pa-rameter A should be selected on the developmentset.
As for the Fisher kernel, the optimal value ofA should be close to 1/w?1.3.3 Estimates from Arbitrary ClassifiersAlthough in this paper we focus on approacheswhich do not require additional domain knowl-edge, the output of most classifiers can be usedto estimate the discriminative probability in equa-tion (7).
As mentioned above, the form of (7)562is appropriate for the SVM learning task witharbitrary kernels, as follows from (Platt, 1999).Also, for models which combine classifiers usingvotes (e.g.
the Voted Perceptron), the number ofvotes cast for each candidate can be used to de-fine this discriminative probability.
The discrim-inative probability of a candidate is simply thenumber of votes cast for that candidate normalizedacross candidates.
Intuitively, we can think of thismethod as treating the votes as a sample from thediscriminative distribution.4 Expected Loss LearningIn this section, another approach to loss approx-imation is proposed.
We consider learning a lin-ear classifier to choose the least loss candidate,and propose two constructions of data-defined losskernels which define different feature spaces forthe classification.
In addition to the kernel, thisapproach differs from the previous one in that theclassifier is assumed to be linear, rather than thenonlinear functions in equations (8) and (11).4.1 Loss KernelThe Loss Kernel feature extractor is composed ofthe logarithm of the loss estimated by the proba-bilistic model and its first derivatives with respectto each model parameter:?LK??
(x, y) = (12)(v(x, y, ??
), ?v(x, y, ??)?
?1,..., ?v(x, y, ??)?
?l),wherev(x, y, ??)
= log(?y?
?G(x)P (y?, x|??)?
(y?, y)).The motivation for this kernel is very similar tothat for the Fisher kernel for structured classifica-tion.
The feature space of the kernel guaranteesconvergence of an estimator for the expected lossif the estimator is in normalized exponential form.The standard Fisher kernel for structured classifi-cation is a special case of this Loss Kernel when?
(y, y?)
is 0-1 loss.4.2 Loss Logit KernelAs the Loss kernel was a generalization of theFisher kernel to arbitrary loss function, so the LossLogit Kernel is a generalization of the TOP kernelfor reranking.
The construction of the Loss LogitKernel, like the TOP kernel for reranking, can bemotivated by the minimization of the classificationerror of a linear classifier wT ?LLK??
(x, y), where?LLK??
(x, y) is the feature extractor of the kernelgiven by:?LLK??
(x, y) = (13)(v(x, y, ??
), ?v(x, y, ??)?
?1,..., ?v(x, y, ??)?
?l),wherev(x, y, ??)
= log(?y?
?G(x)P (y?|x, ??)(1??
(y?, y)))?log(?y?
?G(x)P (y?|x, ??)?
(y?, y)).5 Experimental EvaluationTo perform empirical evaluations of the proposedmethods, we considered the task of parsing thePenn Treebank Wall Street Journal corpus (Mar-cus et al, 1993).
First, we perform experimentswith SVM Struct (Tsochantaridis et al, 2004) asthe learner.
Since SVM Struct already uses theloss function during training to rescale the marginor slack variables, this learner allows us to test thehypothesis that loss functions are useful in pars-ing not only to define the optimization criteria butalso to define the classifier and to define the featurespace.
However, SVM Struct training for largescale parsing experiments is computationally ex-pensive2, so here we use only a small portion ofthe available training data to perform evaluationsof the different approaches.
In the other two setsof experiments, described below, we test our bestmodel on the standard Wall Street Journal parsingbenchmark (Collins, 1999) with the Voted Percep-tron algorithm as the learner.5.1 The Probabilistic Models of ParsingTo perform the experiments with data-defined ker-nels, we need to select a probabilistic model ofparsing.
Data-defined kernels can be applied toany kind of parameterized probabilistic model.For our first set of experiments, we chooseto use a publicly available neural network basedprobabilistic model of parsing (Henderson, 2003).2In (Shen and Joshi, 2003) it was proposed to use anensemble of SVMs trained the Wall Street Journal corpus,but the generalization performance of the resulting classifiermight be compromised in this approach.563This parsing model is a good candidate for our ex-periments because it achieves state-of-the-art re-sults on the standard Wall Street Journal (WSJ)parsing problem (Henderson, 2003), and data-defined kernels derived from this parsing modelhave recently been used with the Voted Percep-tron algorithm on the WSJ parsing task, achiev-ing a significant improvement in accuracy over theneural network parser alone (Henderson and Titov,2005).
This gives us a baseline which is hard tobeat, and allows us to compare results of our newapproaches with the results of the original data-defined kernels for reranking.The probabilistic model of parsing in (Hender-son, 2003) has two levels of parameterization.
Thefirst level of parameterization is in terms of ahistory-based generative probability model.
Theseparameters are estimated using a neural network,the weights of which form the second level of pa-rameterization.
This approach allows the prob-ability model to have an infinite number of pa-rameters; the neural network only estimates thebounded number of parameters which are relevantto a given partial parse.
We define data-definedkernels in terms of the second level of parameteri-zation (the network weights).For the last set of experiments, we used theprobabilistic model described in (Collins, 1999)(model 2), and the Tree Kernel (Collins and Duffy,2002).
However, in these experiments we onlyused the estimates from the discriminative classi-fier, so the details of the probabilistic model arenot relevant.5.2 Experiments with SVM StructBoth the neural network probabilistic model andthe kernel based classifiers were trained on sec-tion 0 (1,921 sentences, 40,930 words).
Section 24(1,346 sentences, 29,125 words) was used as thevalidation set during the neural network learningand for choosing parameters of the models.
Sec-tion 23 (2,416 sentences, 54,268 words) was usedfor the final testing of the models.We used a publicly available tagger (Ratna-parkhi, 1996) to provide the part-of-speech tagsfor each word in the sentence.
For each tag, thereis an unknown-word vocabulary item which isused for all those words which are not sufficientlyfrequent with that tag to be included individuallyin the vocabulary.
For these experiments, we onlyincluded a specific tag-word pair in the vocabu-R P F1 CMSSN 80.9 81.7 81.3 18.3TRK 81.1 82.4 81.7 18.2SSN-Estim 81.4 82.3 81.8 18.3LLK-Learn 81.2 82.4 81.8 17.6LK-Learn 81.5 82.2 81.8 17.8FK-Estim 81.4 82.6 82.0 18.3TRK-Estim 81.5 82.8 82.1 18.6Table 1: Percentage labeled constituent recall (R),precision (P), combination of both (F1) and per-centage complete match (CM) on the testing set.lary if it occurred at least 20 time in the trainingset, which (with tag-unknown-word pairs) led tothe very small vocabulary of 271 tag-word pairs.The same model was used both for choosing thelist of candidate parses and for the probabilisticmodel used for loss estimation and kernel featureextraction.
For training and testing of the kernelmodels, we provided a candidate list consisting ofthe top 20 parses found by the probabilistic model.For the testing set, selecting the candidate with anoracle results in an F1 score of 89.1%.We used the SVM Struct software pack-age (Tsochantaridis et al, 2004) to train the SVMfor all the approaches based on discriminativeclassifier learning, with slack rescaling and lin-ear slack penalty.
The loss function is defined as?
(y, y?)
= 1 ?
F1(y, y?
), where F1 denotes F1measure on bracketed constituents.
This loss wasused both for rescaling the slacks in the SVM andfor defining our classification models and kernels.We performed initial testing of the models onthe validation set and preselected the best modelfor each of the approaches before testing it onthe final testing set.
Standard measures of pars-ing accuracy, plus complete match accuracy, areshown in table 1.3 As the baselines, the table in-cludes the results of the standard TOP rerankingkernel (TRK) (Henderson and Titov, 2005) andthe baseline probabilistic model (SSN) (Hender-son, 2003).
SSN-Estim is the model using lossestimation on the basic probabilistic model, as ex-plained in section 2.
LLK-Learn and LK-Learn arethe models which define the kernel based on loss,using the Loss Logit Kernel (equation (13)) andthe Loss Kernel (equation (12)), respectively.
FK-Estim and TRK-Estim are the models which esti-3All our results are computed with the evalb pro-gram (Collins, 1999).564mate the loss with data-defined kernels, using theFisher Kernel (equation (8)) and the TOP Rerank-ing kernel (equation (11)), respectively.All our proposed models show better F1 accu-racy than the baseline probabilistic model SSN,and all these differences are statistically signifi-cant.4 The difference in F1 between TRK-Estimand FK-Estim is not statistically significant, butotherwise TRK-Estim demonstrates a statisticallysignificant improvement over all other models.
Itshould also be noted that exact match measures forTRK-Estim and SSN-Estim are not negatively af-fected, even though the F1 loss function was opti-mized.
It is important to point out that SSN-Estim,which improves significantly over SSN, does notrequire the learning of a discriminative classifier,and differs from the SSN only by use of the dif-ferent classification model (equation (5)), whichmeans that it is extremely easy to apply in prac-tice.One surprising aspect of these results is the fail-ure of LLK-Learn and LK-Learn to achieve im-provement over SSN-Estim.
This might be ex-plained by the difficulty of learning a linear ap-proximation to (4).
Under this explanation, theperformance of LLK-Learn and LK-Learn couldbe explained by the fact that the first component oftheir kernels is a monotonic function of the SSN-Estim estimation.
To test this hypothesis, we didan additional experiment where we removed thefirst component of Loss Logit Kernel (13) fromthe feature vector and performed learning.
Sur-prisingly, the model achieved virtually the sameresults, rather than the predicted worse perfor-mance.
This result might indicate that the LLK-Learn model still can be useful for different prob-lems where discriminative learning gives more ad-vantage over generative approaches.These experimental results demonstrate thatthe loss approximation reranking approaches pro-posed in this paper demonstrate significant im-provement over the baseline models, achievingabout the same relative error reduction as previ-ously achieved with data-defined kernels (Hender-son and Titov, 2005).
This improvement is despitethe fact that the loss function is already used in thedefinition of the training criteria for all the mod-els except SSN.
It is also interesting to note thatthe best result on the validation set for estimation4We measured significance of all the experiments in thispaper with the randomized significance test (Yeh, 2000).of the loss with data-defined kernels (12) and (13)was achieved when the parameter A is close to theinverse of the first component of the learned de-cision vector, which confirms the motivation forthese kernels.5.3 Experiments with Voted Perceptron andData-Defined KernelsThe above experiments with the SVM Structdemonstrate empirically the viability of our ap-proaches.
The aim of experiments on the entireWSJ is to test whether our approaches still achievesignificant improvement when more accurate gen-erative models are used, and also to show thatthey generalize well to learning methods differentfrom SVMs.
We perform experiments on the stan-dard WSJ parsing data using the standard split intotraining, validation and testing sets.
We replicatecompletely the setup of experiments in (Hender-son and Titov, 2005).
For a detailed description ofthe experiment setup, we refer the reader to (Hen-derson and Titov, 2005).
We only note here thatthe candidate list has 20 candidates, and, for thetesting set, selecting the candidate with an oracleresults in an F1 score of 95.4%.We selected the TRK-Estim approach for theseexperiments because it demonstrated the best re-sults in the previous set of experiments (5.2).
Wetrained the Voted Perceptron (VP) modificationdescribed in (Henderson and Titov, 2005) with theTOP Reranking kernel.
VP is not a linear classi-fier, so we were not able to use a classifier in theform (11).
Instead the normalized counts of votesgiven to the candidate parses were used as proba-bility estimates, as discussed in section 3.3.The resulting accuracies of this model are pre-sented in table 2, together with results of theTOP Reranking kernel VP (Henderson and Titov,2005) and the SSN probabilistic model (Hender-son, 2003).
Model TRK-Estim achieves signifi-cantly better results than the previously proposedmodels, which were evaluated in the same exper-imental setup.
Again, the relative error reductionis about the same as that of TRK.
The resultingsystem, consisting of the generative model andthe reranker, achieves results at the state-of-the-artlevel.
We believe that this method can be appliedto most parsing models to achieve a significant im-provement.565R P F1Henderson, 2003 88.8 89.5 89.1Henderson&Titov, 2005 89.1 90.1 89.6TRK-Estim 89.5 90.5 90.0Table 2: Percentage labeled constituent recall (R),precision (P), combination of both (F1) on the test-ing set.5.4 Experiments with Voted Perceptron andTree KernelIn this series of experiments we validate the state-ment in section 3.3, where we suggested that lossapproximation from a discriminative classifier isnot limited only to models with data-defined ker-nels.
We apply the same method as used inthe TRK-Estim model above to the Tree Ker-nel (Collins and Duffy, 2002), which we call theTK-Estim model.We replicated the parse reranking experimen-tal setup used for the evaluation of the Tree Ker-nel in (Collins and Duffy, 2002), where the can-didate list was provided by the generative proba-bilistic model (Collins, 1999) (model 2).
A list ofon average 29 candidates was used, with an oracleF1 score on the testing set of 95.0%.
We trainedVP using the same parameters for the Tree Ker-nel and probability feature weighting as describedin (Collins and Duffy, 2002).
A publicly avail-able efficient implementation of the Tree Kernelwas utilized to speed up computations (Moschitti,2004).
As in the previous section, votes of the per-ceptron were used to define the probability esti-mate used in the classifier.The results for the MBR decoding method (TK-Estim), defined in section 3.3, along with the stan-dard Tree Kernel VP results (Collins and Duffy,2002) (TK) and the probabilistic baseline (Collins,1999) (CO99) are presented in table 3.
The pro-posed model improves in F1 score over the stan-dard VP results.
Differences between all the mod-els are statistically significant.
The error reductionof TK-Estim is again about the same as the errorreduction of TK.
This improvement is achievedwithout adding any additional linguistic features.It is important to note that the model improvesin other accuracy measures as well.
We wouldexpect even better results with MBR-decoding iflarger n-best lists are used.
The n-best parsing al-gorithm (Huang and Chiang, 2005) can be used toefficiently produce candidate lists as large as 106R P F1?
CB 0C 2CCO99 88.1 88.3 88.2 1.06 64.0 85.1TK 88.6 88.9 88.7 0.99 66.5 86.3TK-Estim 89.0 89.5 89.2 0.91 66.6 87.4* F1 for previous models may have rounding errors.Table 3: Result on the testing set.
Percentage la-beled constituent recall (R), precision (P), combi-nation of both (F1), an average number of cross-ing brackets per sentence (CB), percentage of sen-tences with 0 and ?
2 crossing brackets (0C and2C, respectively).parse trees with the model of (Collins, 1999).6 ConclusionsThis paper considers methods for the estimation ofexpected loss for parse reranking tasks.
The pro-posed methods include estimation of the loss froma probabilistic model, estimation from a discrim-inative classifier, and learning of the loss using aspecialized kernel.
An empirical comparison ofthese approaches on parse reranking tasks is pre-sented.
Special emphasis is given to data-definedkernels for reranking, as they do not require theintroduction of any additional domain knowledgenot already encoded in the probabilistic model.The best approach, estimation of the loss on thebasis of a discriminative classifier, achieves verysignificant improvements over the baseline gener-ative probabilistic models and the discriminativeclassifier itself.
Though the largest improvement isdemonstrated in the measure which corresponds tothe considered loss functional, other measures ofaccuracy are also improved.
The proposed methodachieves 90.0% F1 score on the standard WallStreet Journal parsing task when the SSN neuralnetwork is used as the probabilistic model and VPwith a TOP Reranking kernel as the discriminativeclassifier.AcknowledgmentsWe would like to thank Michael Collins andTerry Koo for providing us their data and use-ful comments on experimental setup, and Alessan-dro Moschitti for providing us the source code forhis Tree Kernel implementation.
We also thankanonymous reviewers for their constructive com-ments.566ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminativereranking.
In Proc.
43rd Meeting of Association forComputational Linguistics, pages 173?180, Ann Ar-bor, MI.Michael Collins and Nigel Duffy.
2002.
New rank-ing algorithms for parsing and tagging: Kernelsover discrete structures and the voted perceptron.In Proc.
40th Meeting of Association for Computa-tional Linguistics, pages 263?270, Philadelphia, PA.Michael Collins and Terry Koo.
2005.
Discriminativereranking for natural language parsing.
Computa-tional Linguistics, 31(1):25?69.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia, PA.Michael Collins.
2000.
Discriminative reranking fornatural language parsing.
In Proc.
17th Int.
Conf.
onMachine Learning, pages 175?182, Stanford, CA.Joshua Goodman.
1996.
Parsing algorithms and meth-ods.
In Proc.
34th Meeting of the Association forComputational Linguistics, pages 177?183, SantaCruz, CA.James Henderson and Ivan Titov.
2005.
Data-definedkernels for parse reranking derived from probabilis-tic models.
In Proc.
43rd Meeting of Association forComputational Linguistics, Ann Arbor, MI.James Henderson.
2003.
Inducing history representa-tions for broad coverage statistical parsing.
In Proc.joint meeting of North American Chapter of the As-sociation for Computational Linguistics and the Hu-man Language Technology Conf., pages 103?110,Edmonton, Canada.James Henderson.
2004.
Discriminative training ofa neural network statistical parser.
In Proc.
42ndMeeting of Association for Computational Linguis-tics, Barcelona, Spain.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proc.
9th Int.
Workshop on Parsing Tech-nologies, Vancouver, Canada.Tommi S. Jaakkola and David Haussler.
1998.
Ex-ploiting generative models in discriminative classi-fiers.
Advances in Neural Information ProcessesSystems 11.Terry Koo and Michael Collins.
2005.
Hidden-variable models for discriminative reranking.
InProc.
Conf.
on Empirical Methods in Natural Lan-guage Processing, Vancouver, B.C., Canada.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine transla-tion.
In Proceedings of the Human Language Tech-nology Conference and Meeting of the North Amer-ican Chapter of the Association for ComputationalLinguistics, Boston, MA.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Alessandro Moschitti.
2004.
A study on convolutionalkernels for shallow semantic parsing.
In Proc.
42ndMeeting of the Association for Computational Lin-guistics, Barcelona, Spain.John C. Platt.
1999.
Probabilistic outputs for sup-port vector machines and comparision to regular-ized likelihood methods.
In A. Smola, P. Bartlett,B.
Scholkopf, and D. Schuurmans, editors, Ad-vances in Large Margin Classifiers, pages 61?74.MIT Press.Adwait Ratnaparkhi.
1996.
A maximum entropymodel for part-of-speech tagging.
In Proc.
Conf.
onEmpirical Methods in Natural Language Process-ing, pages 133?142, Univ.
of Pennsylvania, PA.Libin Shen and Aravind K. Joshi.
2003.
An SVMbased voting algorithm with application to parsereranking.
In Proc.
of the 7th Conf.
on Computa-tional Natural Language Learning, pages 9?16, Ed-monton, Canada.Andreas Stolcke, Yochai Konig, and Mitchel Wein-traub.
1997.
Explicit word error minimization inn-best list rescoring.
In Proc.
of 5th European Con-ference on Speech Communication and Technology,pages 163?165, Rhodes, Greece.Ben Taskar, Dan Klein, Michael Collins, DaphneKoller, and Christopher Manning.
2004.
Max-margin parsing.
In Proc.
Conf.
on Empirical Meth-ods in Natural Language Processing, Barcelona,Spain.Ivan Titov and James Henderson.
2005.
Deriving ker-nels from MLP probability estimators for large cate-gorization problems.
In International Joint Confer-ence on Neural Networks, Montreal, Canada.Ioannis Tsochantaridis, Thomas Hofmann, ThorstenJoachims, and Yasemin Altun.
2004.
Support vectormachine learning for interdependent and structuredoutput spaces.
In Proc.
21st Int.
Conf.
on MachineLearning, pages 823?830, Banff, Alberta, Canada.K.
Tsuda, M. Kawanabe, G. Ratsch, S. Sonnenburg,and K. Muller.
2002.
A new discriminative ker-nel from probabilistic models.
Neural Computation,14(10):2397?2414.Alexander Yeh.
2000.
More accurate tests for thestatistical significance of the result differences.
InProc.
17th International Conf.
on ComputationalLinguistics, pages 947?953, Saarbruken, Germany.567
