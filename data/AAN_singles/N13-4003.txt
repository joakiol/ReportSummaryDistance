Tutorials, NAACL-HLT 2013, pages 7?9,Atlanta, Georgia, June 9 2013. c?2013 Association for Computational LinguisticsTowards Reliability-Aware Entity Analytics andIntegration for Noisy Text at ScaleSameep Mehta, L. Venkata SubramaniamIBM Research Indiasameepmehta,lvsubram@in.ibm.com1 OutlineDue to easy to use apps (Facebook, Twitter, etc.
), higher Internet connectivity andalways on facility allowed by smart phones, the key characteristics of raw data arechanging.
This new data can be characterized by 4V?s - Volume, Velocity, Vari-ety and Veracity.
For example during a Football match, some people will Tweetabout goals, penalties, etc., while others may write longer blogs and further therewill be match reports filed in trusted online news media after the match.
Althoughthe sources may be varied, the data describes and provides multiple evidences forthe same event.
Such multiple evidences should be used to strengthen the beliefin the underlying physical event as the individual data points may have inherentuncertainty.
The uncertainty can arise from inconsistent, incomplete and ambigu-ous reports.
The uncertainty is also because the trust levels of the different sourcesvary and affect the overall reliability.
We will summarize various efforts to performreliability aware entity integration.The other problem in text analysis in such setting is posed by presence of noisein the text.
Since the text is produced in several informal settings such as email,blogs, tweet, SMS, chat and is inherently noisy and has several veracity issues.For example, missing punctuation and the use of non-standard words can oftenhinder standard natural language processing techniques such as part-of-speech tag-ging and parsing.
Further downstream applications such as entity extraction, entityresolution and entity completion have to explicitly handle noise in order to returnuseful results.
Often, depending on the application, noise can be modeled and itmay be possible to develop specific strategies to immunize the system from theeffects of noise and improve performance.
Also the aspect of reliability is key as alot of this data is ambiguous, incomplete, conflicting, untrustworthy and deceptive.The key goals of this tutorial are:71.
Draw the attention of researchers towards methods for doing entity analyticsand integration on data with 4V characteristics.2.
Differentiate between noise and uncertainty in such data.3.
Provide an in-depth discussion on handling noise in NLP based methods.4.
Finally, handling uncertainty through information fusion and integration.This tutorial builds on two earlier tutorials: NAACL 2010 tutorial on NoisyText and COMAD 2012 tutorial on Reliability Aware Data Fusion.
In parallelthe authors are also hosting a workshop on related topic ?Reliability Aware DataFusion?
at SIAM Data Mining Conference, 2013.2 Outline2.1 Data with 4V characteristics?
Define Volume, Velocity, Variety and Veracity and metrics to quantify them?
Information extraction on data with 4V characteristics2.2 Key technical challenges posed by the 4V dimensions and linguis-tics techniques to address them?
Analyzing streaming text?
Large scale distributed algorithms for NLP?
Integrating structured and unstructured data?
Noisy text analytics?
Reliability?
Use case: Generating single view of entity from social data2.3 Computing Reliability and Trust?
Computing source reliability?
Identifying Trust Worthy Messages?
Data fusion to improve reliability: Probabilistic data fusion, informationmeasures, evidential reasoning?
Use case: Event detection using social data, news and online sources83 Speaker BiosSameepMehta1 is researcher in Information Management Group at IBM ResearchIndia.
He received his M.S.
and Ph.D. from The Ohio State University, USA in2006.
He also holds an Adjunct Faculty position at the International Institute ofInformation Technology, New Delhi.
Sameep regularly advises MS and PhD stu-dents at University of Delhi and IIT Delhi.
He regularly delivers Tutorials at CO-MAD (2009, 2010 and 2011).
His current research interests include Data Mining,Business Analytics, Service Science, Text Mining, and Workforce Optimization.L.
Venkata Subramaniam2 manages the information management analyticsand solutions group at IBM Research India.
He received his PhD from IIT Delhiin 1999.
His research focuses on unstructured information management, statisticalnatural language processing, noisy text analytics, text and data mining, informationtheory, speech and image processing.
He often teaches and guides student thesis atIIT Delhi on these topics.
His tutorial titled Noisy Text Analytics was the secondlargest at NAACL-HLT 2010.
He co founded the AND (Analytics for Noisy Un-structured Text Data) workshop series and also co-chaired the first four workshops,2007-2010.
He was guest co-editor of two special issues on Noisy Text Analyticsin the International Journal of Document Analysis and Recognition in 2007 and2009.1http://in.linkedin.com/in/sameepmehta2https://sites.google.com/site/lvs004/9
