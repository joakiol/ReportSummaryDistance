Proceedings of NAACL HLT 2007, Companion Volume, pages 73?76,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsImplicitly Supervised Language Model Adaptation for MeetingTranscriptionDavid Huggins-DainesLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213dhuggins@cs.cmu.eduAlexander I. RudnickyLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213air@cs.cmu.eduAbstractWe describe the use of meeting metadata,acquired using a computerized meetingorganization and note-taking system, toimprove automatic transcription of meet-ings.
By applying a two-step languagemodel adaptation process based on notesand agenda items, we were able to re-duce perplexity by 9% and word error rateby 4% relative on a set of ten meetingsrecorded in-house.
This approach can beused to leverage other types of metadata.1 IntroductionAutomatic transcription of multi-party conversa-tions such as meetings is one of the most difficulttasks in automatic speech recognition.
In (Morganet al, 2003) it is described as an ?ASR-complete?problem, one that presents unique challenges for ev-ery component of a speech recognition system.Though much of the literature on meeting tran-scription has focused on the unique acoustic mod-eling and segmentation problems incurred by meet-ing transcription, language modeling for meetingsis an interesting problem as well.
Though meet-ing speech is spontaneous in nature, the vocabularyand phrasing in meetings can be very specializedand often highly technical.
Speaking style can varygreatly between speakers, and the discourse struc-ture of multi-party interaction gives rise to cross-speaker effects that are difficult to model with stan-dard N-gram models (Ji and Bilmes, 2004).Speech in meetings has one crucial advantageover many other transcription tasks, namely that itdoes not occur in isolation.
Meetings are scheduledand discussed in advance, often via e-mail.
Peopletake notes and create agendas for meetings, and of-ten read directly from electronic presentation mate-rials.
The structure of meetings can be exploited -topics can be segmented both temporally and acrossspeakers, and these shifting topics can be modeledas sub-languages.We examine the effect of leveraging one partic-ular type of external information, namely the writ-ten agendas and meeting minutes, and we demon-strate that, by using off-line language model adapta-tion techniques, these can significantly (p < 0.01)improve language modeling and speech recognitionaccuracy.
The language in the notes and agendas isvery similar to that used by the speakers, hence weconsider this to be a form of semi-supervised or im-plicitly supervised adaptation.2 CorpusThe SmartNotes system, described in (Banerjee andRudnicky, 2007) is a collaborative platform formeeting organization, recording, and note-taking.As part of our research into meeting segmentationand recognition, we have collected a series of 10 un-scripted meetings using SmartNotes.
These meet-ings themselves are approximately 30 minutes inlength (ranging from 1745 to 7208 words) with threeregular participants, and consist of discussions andreporting on our ongoing research.
The meetingsare structured around the agendas and action itemsconstructed through the SmartNotes interface.
The73agenda itself is largely constant from meeting tomeeting, while each meeting typically reviews dis-cusses the previous week?s action items.
Each par-ticipant is equipped with a laptop computer and anindividual headset microphone.Each meeting was manually transcribed and seg-mented for training and testing purposes.
The tran-scription includes speaker identification and timinginformation.
As part of the meeting, participants areencouraged to take notes and define action items.These are automatically collected on a server alongwith timestamp information.
In (Banerjee and Rud-nicky, 2007), it was shown that timestamped text ofthis kind is useful for topic segmentation of meet-ings.
In this work, we have not attempted to takeadvantage of the timing information, nor have weattempted to perform any topic segmentation.
Giventhe small quantity of text available from the notes,we feel that the type of static language model adap-tation presented here is most feasible when done atthe entire meeting level.
A cache language model(Kuhn and Mori, 1990) may be able to capture the(informally attested) locality effects between notesand speech.Since the notes are naturalistic text, often con-taining shorthand, abbreviations, numbers, punctu-ation, and so forth, we preprocess them by runningthem through the text-normalization component ofthe Festival1 speech synthesis system and extractingthe resulting string of individual words.
This yieldedan average of 252 words of adaptation data for eachof the 10 meetings.3 System DescriptionUnless otherwise noted, all language models eval-uated here are trigram models using Katz smooth-ing (Katz, 1987) and Good-Turing discounting.
Lin-ear interpolation of multiple source models was per-formed by maximizing the likelihood over a held-outset of adaptation data.For automatic transcription, our acoustic mod-els consist of 5000 tied triphone states (senones),each using a 64-component Gaussian mixture modelwith diagonal covariance matrices.
The input fea-tures consist of 13-dimensional MFCC features,delta, and delta-delta coefficients.
These models1http://www.festvox.org/Corpus # Words PerplexityFisher English 19902585 178.41Switchboard-I 2781951 215.52ICSI (75 Meetings) 710115 134.94Regular Meetings 266043 111.76Switchboard Cellular 253977 280.81CallHome English 211377 272.19NIST Meetings 136932 199.40CMU (ISL Meetings) 107235 292.86Scenario Meetings 36694 306.43Table 1: Source Corpora for Language Modelare trained on approximately 370 hours of speechdata, consisting of the ICSI meeting corpus (Mor-gan et al, 2003), the HUB-4 Broadcast News cor-pus, the NIST pilot meeting corpus, the WSJ CSR-0 and CSR-1 corpora,2 the CMU Arctic TTS cor-pora (Kominek and Black, 2004), and a corpus of 32hours of meetings previously recorded by our groupin 2004 and 2005.Our baseline language model is based on a linearinterpolation of source language models built fromconversational and meeting speech corpora, using aheld-out set of previously recorded ?scenario?
meet-ings.
These meetings are unscripted, but have a fixedtopic and structure, which is a fictitious scenario in-volving the hiring of new researchers.
The sourcelanguage models contain a total of 24 million wordsfrom nine different corpora, as detailed in Table 1.The ?Regular Meetings?
and ?Scenario Meetings?were collected in-house and consist of the same 32hours of meetings mentioned above, along with theremainder of the scenario meetings.
We used a vo-cabulary of 20795 words, consisting of all wordsfrom the locally recorded, ICSI, and NIST meetings,combined with the Switchboard-I vocabulary (withthe exception of words occurring less than 3 times).The Switchboard and Fisher models were pruned bydropping singleton trigrams.4 Interpolation and Vocabulary ClosureWe created one adapted language model for eachmeeting using a two-step process.
First, the sourcelanguage models were re-combined using linear in-terpolation to minimize perplexity on the set of notes2All corpora are available through http://ldc.upenn.edu/74Meeting Baseline Interpolated Closure04/17 90.05 85.96 84.4104/24 90.16 85.54 81.8805/02 94.27 89.24 89.1905/12 110.95 101.68 87.1305/18 85.78 81.50 78.0405/23 97.51 93.07 94.3906/02 109.70 104.49 101.9006/12 96.80 92.88 91.0506/16 93.93 87.71 79.1706/20 97.19 93.88 92.48Mean 96.57 91.59 (-5.04) 87.96 (-8.7)S.D.
8.61 7.21 (1.69) 7.40 (6.2)p n/a < 0.01 < 0.01Table 2: Adaptation Results: Perplexityfor each meeting.
Next, the vocabulary was ex-panded using the notes.
In order to accomplishthis, a trigram language model was trained from thenotes themselves and interpolated with the output ofthe previous step using a small, fixed interpolationweight ?
= 0.1.
It should be noted that this alsohas the effect of slightly boosting the probabilitiesof the N-grams that appear in the notes.
We felt thiswas useful because, though these probabilities arenot reliably estimated, it is likely that people will usemany of the same N-grams in the notes as in theirmeeting speech, particularly in the case of numbersand acronyms.
The results of interpolation and N-gram closure are shown in Table 2 in terms of test-set perplexity, and in Table 3 in terms of word errorrate.
Using a paired t-test over the 10 meetings, theimprovements in perplexity and accuracy are highlysignificant (p < 0.01).5 Topic Clustering and DimensionalityReductionIn examining the interpolation component of theadaptation method described above, we noticed thatthe in-house meetings and the ICSI meetings consis-tently took on the largest interpolation weights.
Thisis not surprising since both of these corpora are sim-ilar to the test meetings.
However, all of the sourcecorpora cover potentially relevant topics, and by in-terpolating the corpora as single units, we have noway to control the weights given to individual top-Meeting Baseline Interpolated Closure04/17 45.22 44.37 43.3404/24 47.35 46.43 45.2505/02 47.20 47.20 46.2805/12 49.74 48.02 46.0705/18 45.29 44.63 43.4405/23 43.68 43.00 42.8006/02 48.66 48.29 47.8506/12 45.68 45.90 45.2806/16 45.98 45.45 44.2906/20 47.03 46.73 46.68Mean 46.59 46.0 (-0.58) 45.13 (-1.46)S.D.
1.78 1.68 (0.54) 1.64 (1.0)p n/a < 0.01 < 0.01Table 3: Adaptation Results: Word Errorics within them.
Also, people may use different, butrelated, words in writing and speaking to describethe same topic, but we are unable to capture thesesemantic associations between the notes and speech.To investigate these issues, we conducted sev-eral brief experiments using a reduced training cor-pus consisting of 69 ICSI meetings.
We convertedthese to a vector-space representation using tf.idfscores and used a deterministic annealing algorithm(Rose, 1998) to create hard clusters of meetings,from each of which we trained a source model forlinear interpolation.
We compared these clusters torandom uniform partitions of the corpus.
The in-terpolation weights were trained on the notes, andthe models were tested on the meeting transcripts.Out-of-vocabulary words were not removed fromthe perplexity calculation.
The results (mean andstandard deviation over 10 meetings) are shown inTable 4.
For numbers of clusters between 2 and42, the annealing-based clusters significantly out-perform the random partition.
The perplexity with42 clusters is also significantly lower (p < 0.01)than the perplexity (256.5?
21.5) obtained by train-ing a separate source model for each meeting.To address the second issue of vocabulary mis-matches between notes and speech, we applied prob-abilistic latent semantic analysis (Hofmann, 1999)to the corpus, and used this to ?expand?
the vocab-ulary of the notes.
We trained a 32-factor PLSAmodel on the content words (we used a simple75# of Clusters Random Annealed2 546.5 ?
107.4 514.1 ?
97.94 462.2 ?
86.3 426.2 ?
73.98 397.7 ?
67.1 356.1 ?
54.942 281.6 ?
31.5 253.7 ?
22.9Table 4: Topic Clustering Results: PerplexityMeeting Baseline PLSA ?Boosted?04/17 105.49 104.59 104.8704/24 98.97 97.58 97.8005/02 105.61 104.15 104.4805/12 122.37 116.73 118.0405/18 98.55 94.92 95.1805/23 111.28 107.84 108.0306/02 125.31 121.49 121.6406/12 109.31 106.38 106.5506/16 106.86 103.27 104.2806/20 117.46 113.76 114.18Mean 110.12 107.07 107.50S.D.
8.64 7.84 7.93p n/a < 0.01 < 0.01Table 5: PLSA Results: Perplexityentropy-based pruning to identify these ?contentwords?)
from the ICSI meeting vocabulary.
To adaptthe language model, we used the ?folding-in?
proce-dure described in (Hofmann, 1999), running an iter-ation of EM over the notes to obtain an adapted un-igram distribution.
We then simply updated the uni-gram probabilities in the language model with thesenew values and renormalized.
While the results,shown in Table 5, show a statistically significant im-provement in perplexity, this adaptation method isis problematic, as it increases the probability massgiven to all the words in the PLSA model.
In subse-quent results, also shown in Table 5, we found thatsimply extracting these words from the original un-igram distribution and boosting their probabilitiesby the equivalent amount also reduces perplexityby nearly as much (though the difference from thePLSA model is statistically significant, p = 0.004).6 ConclusionsWe have shown that notes collected automaticallyfrom participants in a structured meeting situationcan be effectively used to improve language mod-eling for automatic meeting transcription.
Further-more, we have obtained some encouraging resultsin applying source clustering and dimensionality re-duction to make more effective use of this data.
Infuture work, we plan to exploit other sources ofmetadata such as e-mails, as well as the structure ofthe meetings themselves.7 AcknowledgementsThis research was supported by DARPA grant NGCH-D-03-0010.
The content of the information inthis publication does not necessarily reflect the po-sition or the policy of the US Government, and noofficial endorsement should be inferred.ReferencesS.
Banerjee and A. I. Rudnicky.
2007.
Segmenting meet-ings into agenda items by extracting implicit supervi-sion from human note-taking.
In Proceedings of the2007 International Conference on Intelligent User In-terfaces, January.Thomas Hofmann.
1999.
Probabilistic latent semanticanalysis.
In Proceedings of UAI?99, Stockholm.G.
Ji and J. Bilmes.
2004.
Multi-speaker language mod-eling.
In Proceedings of HLT-NAACL.S.
M. Katz.
1987.
Estimation of probabilities fromsparse data for the language model component of aspeech recognizer.
IEEE Transactions on Acoustics,Speech, and Signal Processing, 35(3):400?401.J.
Kominek and A.
Black.
2004.
The CMU Arctic speechdatabases.
In 5th ISCA Speech Synthesis Workshop,Pittsburgh.R.
Kuhn and R. De Mori.
1990.
A cache-based naturallanguage model for speech recognition.
IEEE Trans-actions on Pattern Analysis and Machine Intelligence,pages 570?583.N.
Morgan, D. Baron, S. Bhagat, R. Dhillon H. Carvey,J.
Edwards, D. Gelbart, A. Janin, A. Krupski, B. Pe-skin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters.2003.
Meetings about meetings: research at ICSI onspeech in multiparty conversation.
In Proceedings ofICASSP, Hong Kong, April.K.
Rose.
1998.
Deterministic annealing for clustering,compression, classification, regression, and related op-timization problems.
In Proceedings of the IEEE,pages 2210?2239.76
