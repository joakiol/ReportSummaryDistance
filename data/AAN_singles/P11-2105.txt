Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 598?602,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsHierarchical Text Classification with Latent ConceptsXipeng Qiu, Xuanjing Huang, Zhao Liu and Jinlong ZhouSchool of Computer Science, Fudan University{xpqiu,xjhuang}@fudan.edu.cn, {zliu.fd,abc9703}@gmail.comAbstractRecently, hierarchical text classification hasbecome an active research topic.
The essentialidea is that the descendant classes can sharethe information of the ancestor classes in apredefined taxonomy.
In this paper, we claimthat each class has several latent concepts andits subclasses share information with these d-ifferent concepts respectively.
Then, we pro-pose a variant Passive-Aggressive (PA) algo-rithm for hierarchical text classification withlatent concepts.
Experimental results showthat the performance of our algorithm is com-petitive with the recently proposed hierarchi-cal classification algorithms.1 IntroductionText classification is a crucial and well-provenmethod for organizing the collection of large scaledocuments.
The predefined categories are formedby different criterions, e.g.
?Entertainment?, ?Sport-s?
and ?Education?
in news classification, ?Junk E-mail?
and ?Ordinary Email?
in email classification.In the literature, many algorithms (Sebastiani, 2002;Yang and Liu, 1999; Yang and Pedersen, 1997) havebeen proposed, such as Support Vector Machines(SVM), k-Nearest Neighbor (kNN), Na?
?ve Bayes(NB) and so on.
Empirical evaluations have shownthat most of these methods are quite effective in tra-ditional text classification applications.In past serval years, hierarchical text classificationhas become an active research topic in database area(Koller and Sahami, 1997; Weigend et al, 1999)and machine learning area (Rousu et al, 2006; Caiand Hofmann, 2007).
Different with traditional clas-sification, the document collections are organizedas hierarchical class structure in many applicationfields: web taxonomies (i.e.
the Yahoo!
Directoryhttp://dir.yahoo.com/ and the Open Direc-tory Project (ODP) http://dmoz.org/), emailfolders and product catalogs.The approaches of hierarchical text classificationcan be divided in three ways: flat, local and globalapproaches.The flat approach is traditional multi-class classi-fication in flat fashion without hierarchical class in-formation, which only uses the classes in leaf nodesin taxonomy(Yang and Liu, 1999; Yang and Peder-sen, 1997; Qiu et al, 2011).The local approach proceeds in a top-down fash-ion, which firstly picks the most relevant categoriesof the top level and then recursively making thechoice among the low-level categories(Sun and Lim,2001; Liu et al, 2005).The global approach builds only one classifier todiscriminate all categories in a hierarchy(Cai andHofmann, 2004; Rousu et al, 2006; Miao and Qiu,2009; Qiu et al, 2009).
The essential idea of globalapproach is that the close classes have some com-mon underlying factors.
Especially, the descendan-t classes can share the characteristics of the ances-tor classes, which is similar with multi-task learn-ing(Caruana, 1997; Xue et al, 2007).Because the global hierarchical categorization canavoid the drawbacks about those high-level irrecov-erable error, it is more popular in the machine learn-ing domain.However, the taxonomy is defined artificially andis usually very difficult to organize for large scaletaxonomy.
The subclasses of the same parent classmay be dissimilar and can be grouped in differen-t concepts, so it bring great challenge to hierarchi-598SportsFootballBasketballSwimmingSurfingSportsWaterFootballBasketballSwimmingSurfingBall(a) (b)CollegeHighSchoolCollegeHighSchoolAcademyFigure 1: Example of latent nodes in taxonomycal classification.
For example, the ?Sports?
nodein a taxonomy have six subclasses (Fig.
1a), butthese subclass can be grouped into three unobserv-able concepts (Fig.
1b).
These concepts can showthe underlying factors more clearly.In this paper, we claim that each class may haveseveral latent concepts and its subclasses share in-formation with these different concepts respectively.Then we propose a variant Passive-Aggressive (PA)algorithm to maximizes the margins between latentpaths.The rest of the paper is organized as follows.
Sec-tion 2 describes the basic model of hierarchical clas-sification.
Then we propose our algorithm in section3.
Section 4 gives experimental analysis.
Section 5concludes the paper.2 Hierarchical Text ClassificationIn text classification, the documents are often rep-resented with vector space model (VSM) (Salton etal., 1975).
Following (Cai and Hofmann, 2007),we incorporate the hierarchical information in fea-ture representation.
The basic idea is that the notionof class attributes will allow generalization to takeplace across (similar) categories and not just acrosstraining examples belonging to the same category.Assuming that the categories is ?
=[?1, ?
?
?
, ?m], where m is the number of thecategories, which are organized in hierarchicalstructure, such as tree or DAG.Give a sample x with its class path in the taxono-my y, we define the feature is?
(x,y) = ?(y)?
x, (1)where ?
(y) = (?1(y), ?
?
?
, ?m(y))T ?
Rm and ?is the Kronecker product.We can define?i(y) ={ti if ?i ?
y0 otherwise , (2)where ti >= 0 is the attribute value for node v. Inthe simplest case, ti can be set to a constant, like 1.Thus, we can classify x with a score function,y?
= argmaxyF (w,?
(x,y)), (3)where w is the parameter of F (?
).3 Hierarchical Text Classification withLatent ConceptsIn this section, we first extent the Passive-Aggressive (PA) algorithm to the hierarchical clas-sification (HPA), then we modify it to incorporatelatent concepts (LHPA).3.1 Hierarchical Passive-Aggressive AlgorithmThe PA algorithm is an online learning algorithm,which aims to find the new weight vectorwt+1 to bethe solution to the following constrained optimiza-tion problem in round t.wt+1 = arg minw?Rn12||w ?wt||2 + C?s.t.
?
(w; (xt, yt)) <= ?
and ?
>= 0.
(4)where ?
(w; (xt, yt)) is the hinge-loss function and ?is slack variable.Since the hierarchical text classification is loss-sensitive based on the hierarchical structure.
Weneed discriminate the misclassification from ?near-ly correct?
to ?clearly incorrect?.
Here we use treeinduced error ?(y,y?
), which is the shortest pathconnecting the nodes yleaf and y?leaf .
yleaf repre-sents the leaf node in path y.Given a example (x,y), we look for the w tomaximize the separation margin ?
(w; (x,y)) be-tween the score of the correct path y and the closesterror path y?.?
(w; (x,y)) = wT?(x,y)?wT?
(x, y?
), (5)599where y?
= argmaxz ?=y wT?
(x, z) and ?
is a fea-ture function.Unlike the standard PA algorithm, which achievea margin of at least 1 as often as possible, we wishthe margin is related to tree induced error ?
(y, y?
).This loss is defined by the following function,?
(w; (x,y)) ={0, ?
(w; (x,y)) > ?
(y, y?)?
(y, y?)?
?
(w; (x,y)), otherwise (6)We abbreviate ?
(w; (x,y)) to ?.
If ?
= 0 then wtitself satisfies the constraint in Eq.
(4) and is clearlythe optimal solution.
We therefore concentrate onthe case where ?
> 0.First, we define the Lagrangian of the optimiza-tion problem in Eq.
(4) to be,L(w, ?, ?, ?)
= 12||w?wt||2+C?+?(???)???s.t.
?, ?
>= 0.
(7)where ?, ?
is a Lagrange multiplier.We set the gradient of Eq.
(7) respect to ?
to zero.?
+ ?
= C. (8)The gradient of w should be zero.w ?wt ?
?(?(x,y)?
?
(x, y?))
= 0 (9)Then we get,w = wt + ?(?(x,y)?
?
(x, y?)).
(10)Substitute Eq.
(8) and Eq.
(10) to objective func-tion Eq.
(7), we getL(?)
= ?12?2||?(x,y)?
?
(x, y?
)||2+ ?wt(?(x,y)?
?
(x, y?)))?
??
(y, y?)
(11)Differentiate Eq.
(11 with ?, and set it to zero, weget??
= ?
(y, y?)?wt(?(x,y)?
?
(x, y?)))||?(x,y)?
?
(x, y?
)||2 (12)From ?
+ ?
= C, we know that ?
< C, so??
= min(C, ?
(y, y?)?wt(?(x,y)?
?
(x, y?)))||?(x,y)?
?
(x, y?)||2).
(13)3.2 Hierarchical Passive-Aggressive Algorithmwith Latent ConceptsFor the hierarchical taxonomy ?
= (?1, ?
?
?
, ?c),we define that each class ?i has a set H?i =h1?i , ?
?
?
, hm?i with m latent concepts, which are un-observable.Given a label path y, it has a set of several latentpaths Hy.
For a latent path z ?
Hy, a functionProj(z) .= y is the projection from a latent path zto its corresponding path y.Then we can define the predict latent path h?
andthe most correct latent path h?:h?
= arg maxproj(z)?=ywT?
(x, z), (14)h?
= arg maxproj(z)=ywT?
(x, z).
(15)Similar to the above analysis of HPA, we re-definethe margin?
(w; (x,y) = wT?(x,h?)?
wT?
(x, h?
), (16)then we get the optimal update step?
?L = min(C,?
(wt; (x,y))||?(x,h?)?
?
(x, h?)||2).
(17)Finally, we get update strategy,w = wt + ??L(?(x,h?)?
?
(x, h?)).
(18)Our hierarchical passive-aggressive algorithmwith latent concepts (LHPA) is shown in Algorith-m 1.
In this paper, we use two latent concepts foreach class.4 Experiment4.1 DatasetsWe evaluate our proposed algorithm on two datasetswith hierarchical category structure.WIPO-alpha dataset The dataset1 consisted of the1372 training and 358 testing document com-prising the D section of the hierarchy.
Thenumber of nodes in the hierarchy was 188, withmaximum depth 3.
The dataset was processedinto bag-of-words representation with TF?IDF1World Intellectual Property Organization, http://www.wipo.int/classifications/en600input : training data set: (xn,yn), n = 1, ?
?
?
, N ,and parameters: C,Koutput: wInitialize: cw?
0,;for k = 0 ?
?
?K ?
1 dow0 ?
0 ;for t = 0 ?
?
?T ?
1 doget (xt,yt) from data set;predict h?,h?
;calculate ?
(w; (x,y)) and?
(yt, y?t);if ?
(w; (x,y)) ?
?
(yt, y?t) thencalculate ?
?L by Eq.
(17);update wt+1 by Eq.
(18).
;endendcw = cw +wT ;endw = cw/K ;Algorithm 1:Hierarchical PA algorithmwith la-tent conceptsweighting.
No word stemming or stop-wordremoval was performed.
This dataset is usedin (Rousu et al, 2006).LSHTC dataset The dataset2 has been constructedby crawling web pages that are found in theOpen Directory Project (ODP) and translatingthem into feature vectors (content vectors) andsplitting the set of Web pages into a training,a validation and a test set, per ODP category.Here, we use the dry-run dataset(task 1).4.2 Performance MeasurementMacro Precision, Macro Recall and Macro F1 arethe most widely used performance measurementsfor text classification problems nowadays.
Themacro strategy computes macro precision and re-call scores by averaging the precision/recall of eachcategory, which is preferred because the categoriesare usually unbalanced and give more challenges toclassifiers.
The Macro F1 score is computed usingthe standard formula applied to the macro-level pre-cision and recall scores.MacroF1 = P ?RP +R, (19)2Large Scale Hierarchical Text classification Pascal Chal-lenge, http://lshtc.iit.demokritos.grTable 1: Results on WIPO-alpha Dataset.?-?
means thatthe result is not available in the author?s paper.Accuracy F1 Precision Recall TIEPA 49.16 40.71 43.27 38.44 2.06HPA 50.84 40.26 43.23 37.67 1.92LHPA 51.96 41.84 45.56 38.69 1.87HSVM 23.8 - - - -HM3 35.0 - - - -Table 2: Results on LSHTC dry-run DatasetAccuracy F1 Precision Recall TIEPA 47.36 44.63 52.64 38.73 3.68HPA 46.88 43.78 51.26 38.2 3.73LHPA 48.39 46.26 53.82 40.56 3.43where P is the Macro Precision and R is the MacroRecall.
We also use tree induced error (TIE) in theexperiments.4.3 ResultsWe implement three algorithms3: PA(Flat PA), H-PA(Hierarchical PA) and LHPA(Hierarchical PAwith latent concepts).
The results are shown in Table1 and 2.
For WIPO-alpha dataset, we also comparedLHPA with two algorithms used in (Rousu et al,2006): HSVM and HM3.We can see that LHPA has better performancesthan the other methods.
From Table 2, we can seethat it is not always useful to incorporate the hierar-chical information.
Though the subclasses can shareinformation with their parent class, the shared infor-mation may be different for each subclass.
So weshould decompose the underlying factors into dif-ferent latent concepts.5 ConclusionIn this paper, we propose a variant Passive-Aggressive algorithm for hierarchical text classifi-cation with latent concepts.
In the future, we willinvestigate our method in the larger and more noisydata.AcknowledgmentsThis work was (partially) funded by NSFC (No.61003091 and No.
61073069), 973 Program (No.3Source codes are available in FudanNLP toolkit, http://code.google.com/p/fudannlp/6012010CB327906) and Shanghai Committee of Sci-ence and Technology(No.
10511500703).ReferencesL.
Cai and T. Hofmann.
2004.
Hierarchical documentcategorization with support vector machines.
In Pro-ceedings of CIKM.L.
Cai and T. Hofmann.
2007.
Exploiting known tax-onomies in learning overlapping concepts.
In Pro-ceedings of International Joint Conferences on Arti-ficial Intelligence.R.
Caruana.
1997.
Multi-task learning.
Machine Learn-ing, 28(1):41?75.D.
Koller and M Sahami.
1997.
Hierarchically classify-ing documents using very few words.
In Proceedingsof the International Conference on Machine Learning(ICML).T.Y.
Liu, Y. Yang, H. Wan, H.J.
Zeng, Z. Chen, and W.Y.Ma.
2005.
Support vector machines classificationwith a very large-scale taxonomy.
ACM SIGKDD Ex-plorations Newsletter, 7(1):43.Youdong Miao and Xipeng Qiu.
2009.
Hierarchicalcentroid-based classifier for large scale text classifica-tion.
In Large Scale Hierarchical Text classification(LSHTC) Pascal Challenge.Xipeng Qiu, Wenjun Gao, and Xuanjing Huang.
2009.Hierarchical multi-class text categorization with glob-al margin maximization.
In Proceedings of the ACL-IJCNLP 2009 Conference, pages 165?168, Suntec,Singapore, August.
Association for ComputationalLinguistics.Xipeng Qiu, Jinlong Zhou, and Xuanjing Huang.
2011.An effective feature selection method for text catego-rization.
In Proceedings of the 15th Pacific-Asia Con-ference on Knowledge Discovery and Data Mining.Juho Rousu, Craig Saunders, Sandor Szedmak, and JohnShawe-Taylor.
2006.
Kernel-based learning of hierar-chical multilabel classification models.
In Journal ofMachine Learning Research.G.
Salton, A. Wong, and CS Yang.
1975.
A vector spacemodel for automatic indexing.
Communications of theACM, 18(11):613?620.F.
Sebastiani.
2002.
Machine learning in automated textcategorization.
ACM computing surveys, 34(1):1?47.A.
Sun and E.-P Lim.
2001.
Hierarchical text classi-fication and evaluation.
In Proceedings of the IEEEInternational Conference on Data Mining.A.
Weigend, E. Wiener, and J Pedersen.
1999.
Exploit-ing hierarchy in text categorization.
In InformationRetrieval.Y.
Xue, X. Liao, L. Carin, and B. Krishnapuram.
2007.Multi-task learning for classification with dirichletprocess priors.
The Journal of Machine Learning Re-search, 8:63.Y.
Yang and X. Liu.
1999.
A re-examination of textcategorization methods.
In Proc.
of SIGIR.
ACMPressNew York, NY, USA.Y.
Yang and J.O.
Pedersen.
1997.
A comparative studyon feature selection in text categorization.
In Proc.
ofInt.
Conf.
on Mach.
Learn.
(ICML), volume 97.602
