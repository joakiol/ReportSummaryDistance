Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1049?1056Manchester, August 2008Chinese Dependency Parsing with Large Scale AutomaticallyConstructed Case StructuresKun YuGraduate School of Infor-matics,Kyoto University, Japankunyu@nlp.kuee.kyoto-u.ac.jpDaisuke KawaharaNational Institute of Informa-tion and CommunicationsTechnology, Japandk@nict.go.jpSadao KurohashiGraduate School of Infor-matics,Kyoto University, Japankuro@i.kyoto-u.ac.jpAbstractThis paper proposes an approach usinglarge scale case structures, which areautomatically constructed from both asmall tagged corpus and a large raw cor-pus, to improve Chinese dependencyparsing.
The case structure proposed inthis paper has two characteristics: (1) itrelaxes the predicate of a case structure tobe all types of words which behaves as ahead; (2) it is not categorized by semanticroles but marked by the neighboringmodifiers attached to a head.
Experimen-tal results based on Penn Chinese Tree-bank show the proposed approachachieved 87.26% on unlabeled attach-ment score, which significantly outper-formed the baseline parser without usingcase structures.1 IntroductionCase structures (i.e.
predicate-argument struc-tures) represent what arguments can be attachedto a predicate, which are very useful to recognizethe meaning of natural language text.
Research-ers have applied case structures to Japanese syn-tactic analysis and improved parsing accuracysuccessfully (Kawahara and Kurohashi, 2006(a);Abekawa and Okumura, 2006).
However, fewworks focused on using case structures in Chi-nese parsing.
Wu (2003) proposed an approachto learn the relations between verbs and nounsand applied these relations to a Chinese parser.Han et al (2004) presented a method to acquire?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.the sub-categorization of Chinese verbs and usedthem in a PCFG parser.Normally, case structures are categorized bysemantic roles for verbs.
For example, Kawaharaand Kurohashi (2006(b)) constructed Japanesecase structures which were marked by post posi-tions.
Wu (2003) classified the Chinese verb-noun relations as ?verb-object?
and ?modifier-head?.
In this paper, we propose a new type ofChinese case structure, which is different fromthose presented in previous work (Wu, 2003;Han et al, 2004; Kawahara and Kurohashi,2006(a); Abekawa and Okumura, 2006) in twoaspects:(1) It relaxes the predicate of a case structureto be all types of words which behaves as a head;(2) It is not categorized by semantic rolesbut marked by the neighboring modifiers at-tached to a head.
The sibling modification infor-mation remembers the parsing history of a headnode, which is useful to correct the parsing errorsuch as a verb ?
(see) is modified by two nouns??
(film) and ??
(introduction) as objects(see Figure 1).Figure 1.
Dependency trees of an example sen-tence (I see the introduction of a film).We automatically construct large scale casestructures from both a small tagged corpus and alarge raw corpus.
Then, we apply the large scalecase structures to a Chinese dependency parser toimprove parsing accuracy.The Chinese dependency parser using casestructures is evaluated by Penn Chinese Tree-bank 5.1 (Xue et al, 2002).
Results show that the1049automatically constructed case structures helpedincrease parsing accuracy by 2.13% significantly.The rest of this paper is organized as follows:Section 2 describes the proposed Chinese casestructure and the construction method in detail;Section 3 describes a Chinese dependency parserusing constructed case structures; Section 4 liststhe experimental results with a discussion in sec-tion 5; Related work is introduced in Section 6;Finally, Section 7 gives a brief conclusion andthe direction of future work.2 Chinese Case Structure and its Con-struction2.1 A New Type of Chinese Case StructureWe propose a new type of Chinese case structurein this paper, which is represented as the combi-nation of a case pattern and a case element (seeFigure 2).
Case element remembers the bi-lexicaldependency relation between all types of head-modifier pairs, which is also recognized in previ-ous work (Wu, 2003; Han et al, 2004; Kawaharaand Kurohashi, 2006(a); Abekawa and Okumura,2006).
Case pattern keeps the pos-tag sequenceof all the modifiers attached to a head to remem-ber the parsing history of a head node.Figure 2.
An example of constructed casestructure.2.2 Construction CorpusWe use 9,684 sentences from Penn ChineseTreebank 5.1 as the tagged corpus, and 7,338,028sentences written in simplified Chinese fromChinese Gigaword (Graff et al, 2005) as the rawcorpus for Chinese case structure construction.Before constructing case structures from theraw corpus, we need to get the syntactic analysisof it.
First, we do word segmentation and pos-tagging for the sentences in Chinese Gigawordby a Chinese morphological  analyzer(Nakagawa and Uchimoto, 2007).
Then aChinese deterministic syntactic analyzer (Yu etal., 2007) is used to parse the whole corpus.To guarantee the accuracy of constructed casestructures, we only use the sentences with lessthan k words from Chinese Gigaword.
It is basedon the assumption that parsing short sentences ismore accurate than parsing long sentences.
Theperformance of the deterministic parser used foranalyzing Chinese Gigaword (see Figure 3)shows smaller k ensures better parsing qualitybut suffers from lower sentence coverage.Referring to Figure 2, we set k as 30.Figure 3.
Performance of the deterministic parserwith different k on 1,800 sentences2.2.3 Case Pattern ConstructionA case pattern consists of a sequence of pos-tagsindicating the order of all the modifiers attachedto a head (see Figure 1), which can be repre-sented as following.>=< ??
rnnlmmi posposposposposposcp ],,...,[,],...,,[ 1111Here,lmm pospospos ],...,,[ 11?
means the pos-tagsequence of the modifiers attached to a headfrom the left side, andrnn pospospos ],,...,[ 11 ?
meansthe pos-tag sequence of the modifiers attached toa head from the right side.We use the 33 pos-tags defined in Penn Chi-nese Treebank (Xue et al, 2002) to describe acase pattern, and make following modifications:?
group common noun, proper noun andpronoun together and mark them as ?noun?;?
group predicative adjective and all theother verbs together and mark them as ?verb?;?
only regard comma, pause, colon andsemi-colon as punctuations and mark them as?punc?, and neglect other punctuations.2 UAS means unlabeled attachment score (Buchholz andMarsi, 2006).
The sentences used for this evaluation arefrom Penn Chinese Treebank with gold word segmentationand pos-tag.0 10 20 30 40 50 60 70 80 90 100 1102030405060708090100%kUASSentence Coverage1050?
group cardinal number and ordinal numbertogether and mark them as ?num?;?
keep the original definition for other pos-tags but label them by new tags, such as labeling?P?
as ?prep?
and labeling ?AD?
as ?adv?.The task of case pattern construction is to ex-tract cpi for each head from both the tagged cor-pus and the raw corpus.
As we will introducelater, the Chinese dependency parser using casestructures applies CKY algorithm for decoding.Thus the following substrings of cpi are also ex-tracted for each head as horizontal Markoviza-tion during case pattern construction.
],1[],,1[],,...,[,],...,,[ 1111njmkpospospospospospos rjjlkk??
?>< ??
],1[  ,],...,,[ 11 mkpospospos lkk ?
?>< ?
],1[  ,],,...,[ 11 njpospospos rjj ?
?>< ?2.4 Case Element ConstructionAs introduced in Section 2.1, a case elementkeeps the lexical preference between a head andits modifier.
Therefore, the task of case elementconstruction is to extract head-modifier pairsfrom both the tagged corpus and the raw corpus.Although only the sentences with less than k(k=30) words from Chinese Gigaword are usedas raw corpus to guarantee the accuracy, therestill exist some dependency relations with lowaccuracy in these short sentences because of thenon-perfect parsing quality.
Therefore, we applya head-modifier (HM) classifier to the parsedsentences from Chinese Gigaword to further ex-tract head-modifier pairs with high quality.
ThisHM classifier is based on SVM classification.Table 1 lists the features used in this classifier.Feature DescriptionPoshead/PosmodPos-tag pair of head and modifierDistance Distance between head and modifierHasComma If there exists comma between head and  modifier, set as 1; otherwise as 0HasColon If there exists colon between head and modifier, set as 1; otherwise as 0HasSemi If there exists semi-colon between head and modifier, set as 1; otherwise as 0Table 1.
Features for HM classifier.The HM classifier is trained on 3500 sentencesfrom Penn Chinese Treebank with gold-standardword segmentation and pos-tag.
All the sentencesare parsed by the same Chinese deterministicparser used for Chinese Gigaword analysis.
Thecorrect dependency relations created by theparser are looked as positive examples and theleft dependency relations are used as negativeexamples.
TinySVM 3  is selected as the SVMtoolkit.
A polynomial kernel is used and degreeis set as 2.
Tested on 346 sentences, which arefrom Penn Chinese Treebank and parsed by thesame deterministic parser with gold standardword segmentation and pos-tag, this HM classi-fier achieved 96.77% on precision and 46.35%on recall.3 A Chinese Dependency Parser UsingCase Structures3.1 Parsing ModelWe develop a lexicalized Chinese dependencyparser to use constructed case structures.
Thisparser gives a probability P(T|S) to each possibledependency tree T of an input sentenceS=w1,w2,?,wn (wi is a node representing a wordwith its pos-tag), and outputs the dependencytree T* that maximizes P(T|S) (see equation 1).CKY algorithm is used to decode the dependencytree from bottom to up.
)|(maxarg STPTT=?
(1)To use case structures, P(T|S) is divided intotwo parts (see equation 2): the probability of asentence S generating a root node wROOT, and theproduct of the probabilities of a node wi generat-ing a case structure CSi.
?=?=miiiROOT wCSPSwPSTP1)|()|()|(  (2)As introduced in Section 2, a case structureCSi is composed of a case pattern cpi and a caseelement cmi.
Thus),|()|()|,()|(iiiiiiiiiicpwcmPwcpPwcmcpPwCSP?==(3)A case element cmi consists of a set of de-pendencies {Dj}, in which each Dj is a tuple <wj,disj, commaj>.
Here wj means a modifier node,disj means the distance between wj and its head,and commaj means the number of commas be-tween wj and its head.
Assuming any Dj and Dkare independent of each other when they belongto the same case element, P(cmi|wi,cpi) can bewritten as),|,,(),|(),|(iijjjjiijjiiicpwcommadiswPcpwDPcpwcmP?
?==(4)3 http://chasen.org/~taku/software/TinySVM/1051Finally, P(wj,disj,commaj | wi,cpi) is divided as),,|,(),|(),|,,(ijijjiijiijjjcpwwcommadisPcpwwPcpwcommadiswP?=  (5)Maximum likelihood estimation is used to es-timate P(wROOT|S) on training data set with thesmoothing method used in (Collins, 1996).
Theestimation of P(cpi|wi), P(wj|wi,cpi), and P(disj,commaj |wi,wj,cpi) will be introduced in the fol-lowing subsections.3.2 Estimating P(cpi|wi) by Case PatternsThree steps are used to estimate P(cpi|wi) bymaximum likelihood estimation using the con-structed case patterns:?
Estimate P(cpi|wi) only by the case pat-terns from the tagged corpus and represent it as)|(?
iitagged wcpP ;?
Estimate P(cpi|wi) only by the case pat-terns from the raw corpus and represent it as)|(?
iiraw wcpP ;?
Estimate P(cpi|wi) by equation 6, in which?pattern is calculated by equation 7 to set properratio for the probabilities estimated by the casepatterns from different corpora.)|(?)1()|(?)|(?iirawpatterniitaggedpatterniiwcpPwcpPwcpP?
?+?= ??
(6)1+++++++=rawrawtaggedtaggedrawrawtaggedtaggedpattern ?????????
(7)In equation 7, ?tagged and ?raw mean the occur-rence of a lexicalized node wi=<lexi, posi> gener-ating cpi in the tagged or raw corpus, ?tagged and?raw mean the occurrence of a back-off nodewi=<posi> generating cpi in the tagged or rawcorpus.
To overcome the data sparseness prob-lem, we not only apply the smoothing methodused in (Collins, 1996) for a lexicalized head toback off it to its part-of-speech, but also assign avery small value to P(cpi|wi) when there is no cpimodifying wi in the constructed case patterns.3.3 Estimating P(wj|wi,cpi) and P(disj, com-maj |wi,wj,cpi) by Case ElementsTo estimate P(wj|wi,cpi) and P(disj, commaj|wi,wj,cpi) by maximum likelihood estimation, wealso use three steps:?
Estimate the two probabilities only by thecase elements from the tagged corpus and repre-sent them as ),|(?
iijtagged cpwwP  and),,|,(?
ijijjtagged cpwwcommadisP ;?
Estimate the two probabilities only by thecase elements from the raw corpus, and representthem as ),|(?
iijraw cpwwP  and),,|,(?
ijijjraw cpwwcommadisP ;?
Estimate P(wj|wi,cpi) and P(disj, commaj|wi,wj,cpi) by equation 8 and equation 9.),|(?)1(),|(?),|(?iijrawelementiijtaggedelementiijcpwwPcpwwPcpwwP??+?=??(8)),,|,(?)1(),,|,(?),,|,(?ijijjrawelementijijjtaggedelementijijjcpwwcommadisPcpwwcommadisPcpwwcommadisP??+?=??
(9)The smoothing method used in (Collins, 1996)is applied during estimation.Figure 4.
Parsing accuracy with different ?elementon development data set.In order to set proper ratio for the probabilitiesestimated by the case elements from differentcorpora, we use a parameter ?element in equation 8and 9.
The appropriate setting (?element =0.4) islearned by a development data set (see Figure 4).4 Evaluation Results4.1 Experimental SettingWe use Penn Chinese Treebank 5.1 as data set toevaluate the proposed approach.
9,684 sentencesfrom Section 001-270 and 400-931, which arealso used for constructing case structures, areused as training data.
346 sentences from Section271-300 are used as testing data.
334 sentencesfrom Section 301-325 are used as developmentdata.
Penn2Malt4 is used to transfer the phrasestructure of Penn Chinese Treebank to depend-ency structure.
Gold-standard word segmentationand pos-tag are applied in all the experiments.4 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0 1.186.086.587.087.588.088.5UAS(%)?element1052Unlabeled attachment score (UAS) (Buchholzand Marsi, 2006) is used as evaluation metric.Because of the difficulty of assigning correcthead to Chinese punctuation, we calculate UASonly on the dependency relations in which themodifier is not punctuation.4.2 ResultsThree parsers were evaluated in this experiment:?
?baseline?
: a parser not using case struc-tures, where P(T|S) is calculated by equation 11and P(wj|wi) and P(disj, commaj |wi,wj) are esti-mated by training data set only.????????=?=jinjijijjijROOTjinjiijjjROOTwwcommadisPwwPSwPwcommadiswPSwPSTP],,1[,],,1[,)),|,()|(()|()|,,()|()|((11)?
?w/ case elem?
: a parser only using caseelement, which also calculates P(T|S) by equa-tion 11 but estimates P(wj|wi) and P(disj, commaj|wi,wj) by constructed case elements.?
?proposed?
: the parser introduced in Sec-tion 3, which uses both case elements and casepatterns.The evaluation results on testing data set (seeTable 2) shows the proposed parser achieved87.26% on UAS, which was 2.13% higher thanthat of the baseline parser.
This improvement isregarded as statistically significant (McNemar?stest: p<0.0005).
Besides, Table 2 shows only us-ing case elements increased parsing accuracy by1.30%.
It means both case elements and case pat-terns gave help to parsing accuracy, and caseelements contributed more in the proposed ap-proach.Parsingmodel baseline w/ case elem proposedUAS (%) 85.13 86.43 (+1.30) 87.26 (+2.13)Table 2.
Parsing accuracy of different parsingmodels.Figure 5 and Figure 6 show the dependencytrees of two example sentences created by boththe baseline parser and the proposed parser.
InFigure 5, the baseline parser incorrectly assigned?
?/NN (signing) as the head of ?
?/NN (co-operation).
However, after using the case ele-ment ?
?/NN (project) ?
?
?/NN, the cor-rect head of ?
?/NN was found by the pro-posed parser.
Figure 6 shows the baseline parserrecognized ?
?/VV (opening) as the head of ?/P (as) incorrectly.
But in the proposed parser,the probability of ?
?/VV generating the casepattern ?
[prep, punc, prep]l?
was much lowerthan that of ?
?/VV  generating the case pattern?[prep]l?.
Therefore, the proposed parser rejectedthe incorrect dependency that?/P modified?
?/VV  and got the correct head of ?/P as ?
?/VV (show) successfully.5 Discussion5.1 Influence of the Number of Case Struc-tures on Parsing AccuracyDuring case structure construction, we only usedthe sentences with less than k (k=30) words fromChinese Gigaword as the raw corpus.
Enlarging kwill introduce more sentences from Chinese Gi-gaword and increase the number of case struc-tures.
Table 4 lists the number of case structuresand parsing accuracy of the proposed parser ontesting data set with different k5.
It shows enlarg-ing the number of case structures is a possibleway to increase parsing accuracy.
But simplysetting larger k did not help parsing, because itdecreased the parsing accuracy of Chinese Gi-gaword and consequently decreased the accuracyof constructed case structures.
Using good parseselection (Reichart and Rappoport, 2007; Yateset al, 2006) on the syntactic analysis of ChineseGigaword is a probable way to construct morecase structures without decreasing their accuracy.We will consider about it in the future.k 10 20 30 40# of Case Ele-ment (M) 0.66 1.14 1.81 2.75# of Case Pat-tern (M) 0.57 1.55 3.91 8.48UAS (%) 85.16 86.42 87.26 87.07Table 4.
Case structure number and parsingaccuracy with different k.5.2 Influence of the Case Structure Con-struction Corpus on Parsing AccuracyWe also evaluated the proposed parser on testingdata set using case structures constructed fromdifferent corpora.Results (see Table 5) show that parsing accu-racy was improved greatly only when using casestructures constructed from both the two corpora.The case structures constructed from either of a5 Considering about the time expense of case structure con-struction, we only did test for k ?40.1053single corpus only gave a little help to parsing.
Itis because among all the case structures usedduring testing (see Table 6), 19.57% case ele-ments were constructed from the tagged corpusonly and 54.18% case patterns were constructedfrom the raw corpus only.
The incorrect head-modifier pairs extracted from Chinese Gigawordis a possible reason for the fact that some caseelements only existing in the tagged corpus.
En-hancing good parse selection on Chinese Giga-word could improve the quality of extractedhead-modifier pairs and solve this problem.
Inaddition, the strict definition of case pattern is aprobable reason that makes more than half of thecase patterns only exist in the raw corpus and18.18% case patterns exist in neither of the twocorpora.
We will modify the representation ofcase pattern to make it more flexible to the num-ber of modifiers to resolve this issue in the future.Corpus Tagged Raw Tagged+RawUAS (%) 85.25 85.90 87.26Table 5.
Parsing accuracy with case structuresconstructed from different corpora.Corpus Tagged Raw Tagged+Raw None% of caseelement  19.57 8.95 68.07 3.41% of casepattern 0.03 54.18 27.61 18.18Table 6.
Ratio of case structures constructedfrom different corpora.Figure 5.
Dependency trees of an example sentence (The signing of China-US cooperation high techproject ?).
?/P ?
?/NN  ?/PU  ?/P  ??
?/NN  ?
?/VV ?/DEC  ??
?/NN  ?/AD  ?
?/VV  ...(a) dependency tree created by the baseline parser(b) dependency tree created by the proposed parser?/P ?
?/NN  ?/PU  ?/P  ??
?/NN  ?
?/VV  ?/DEC  ??
?/NN  ?/AD  ?
?/VV ...Figure 6.
Dependency trees of an example sentence (As introduced, the exhibition opening in thestadium will show?
).5.3 Parsing Performance with Real Pos-tagGold standard word segmentation and pos-tagare applied in previous experiments.
However,parsing accuracy will be affected by the incorrectword segmentation and pos-tag in the real appli-cations.
Currently, the best performance of Chi-nese word segmentation has achieved 99.20% onF-score, but the best accuracy of Chinese pos-tagging was 96.89% (Jin and Chen, 2008).Therefore, we think pos-tagging is more crucialfor applying parser in real task compared withword segmentation.
Considering about this, weevaluated the parsing models introduced in Sec-tion 4 with real pos-tag in this experiment.Parsing model baseline proposedUAS (%) 80.91 82.90 (+1.99)Table 7.
Parsing accuracy of different parsingmodels with real pos-tag.An HMM-based pos-tagger is used to get pos-tag for testing sentences with gold word segmen-tation.
The pos-tagger was trained on the sametraining data set described in Section 4.1 andachieved 93.70% F-score on testing data set.
Re-sults (see Table 7) show that even if with realpos-tags, the proposed parser still outperformedthe baseline parser significantly.
However, theresults in Table 7 indicate that incorrect pos-tagaffected the parsing accuracy of the proposedparser greatly.
Some researchers integrated pos-1054tagging into parsing and kept n-best pos-tags toreduce the effect of pos-tagging errors on parsingaccuracy (Cao et al, 2007).
We will also con-sider about this in our future work.6 Related WorkTo our current knowledge, there were few worksabout using case structures in Chinese parsing,except for the work of Wu (2003) and Han et al(2004).
Compared with them, our proposed ap-proach presents a new type of case structures forall kinds of head-modifier pairs, which not onlyrecognizes bi-lexical dependency but also re-members the parsing history of a head node.Parsing history has been used to improve pars-ing accuracy by many researchers (Yamada andMatsumoto, 2003; McDonald and Pereira, 2006).Yamada and Matsumoto (2003) showed thatkeeping a small amount of parsing history wasuseful to improve parsing performance in a shift-reduce parser.
McDonald and Pereira (2006) ex-panded their first-order spanning tree model to besecond-order by factoring the score of the treeinto the sum of adjacent edge pair scores.
In ourproposed approach, the case patterns rememberthe neighboring modifiers for a head node likeMcDonald and Pereira?s work.
But it keeps allthe parsing histories of a head, which is differentfrom only keeping adjacent two modifiers in(McDonald and Pereira, 2006).
Besides, to usethe parsing histories in CKY decoding, our ap-proach applies horizontal Markovization duringcase pattern construction.
In general, the successof using case patterns in Chinese parsing in hispaper proves again that keeping parsing history iscrucial to improve parsing performance, no mat-ter in which way and to which parsing model it isapplied.There were also some works that handled lexi-cal preference for Chinese parsing in other ways.For example, Cheng et al (2006) and Hall et al(2007) applied shift-reduce deterministic parsingto Chinese.
Sagae and Tsujii (2007) generalizedthe standard deterministic framework to prob-abilistic parsing by using a best-first search strat-egy.
In these works, lexical preferences wereintroduced as features for predicting parsing ac-tion.
Besides, Bikel and Chiang (2000) appliedtwo lexicalized parsing models developed forEnglish to Penn Chinese Treebank.
Wang et al(2005) proposed a completely lexicalized bot-tom-up generative parsing model to parse Chi-nese, in which a word-similarity-based smooth-ing was introduced to replace part-of-speechsmoothing.7 Conclusion and Future WorkThis paper proposes an approach to use largescale case structures, which are automaticallyconstructed from both a small tagged corpus andthe syntactic analysis of a large raw corpus, toimprove Chinese dependency parsing.
The pro-posed case structures not only recognize the lexi-cal preference between all types of head-modifierpairs, but also keep the parsing history of a headword.
Experimental results show the proposedapproach improved parsing accuracy signifi-cantly.
Besides, although we only apply the pro-posed approach to Chinese dependency parsingcurrently, the same idea could be adapted toother languages easily because it doesn?t use anylanguage specific knowledge.There are several future works under consid-eration, such as modifying the representation ofcase patterns to make it more robust, enhancinggood parse selection on the analysis of raw cor-pus, and integrating pos-tagging into parsingmodel.ReferencesT.Abekawa and M.Okumura.
2006.
Japanese De-pendency Parsing Using Co-occurrence Informa-tion and a Combination of Case Elements.
In Pro-ceedings of   the joint conference of theInternational Committee on Computational Lin-guistics and the Association for ComputationalLinguistics 2006. pp.
833-840.D.Bikel.
2004.
Intricacies of Collins?
Parsing Model.Computational Linguistics, 30(4): 479-511.D.Bikel and D.Chiang.
2000.
Two Statistical ParsingModels Applied to the Chinese Treebank.
In Pro-ceedings of the 2nd Chinese Language ProcessingWorkshop.
pp.
1-6.S.Buchholz and E.Marsi.
2006.
CoNLL-X SharedTask on Multilingual Dependency Parsing.
In Pro-ceedings of the 10th Conference on ComputationalNatural Language Learning.H.Cao et al.
2007.
Empirical Study on Parsing Chi-nese Based on Collins?
Model.
In Proceedings ofthe 10th Conference of the Pacific Association forComputational Linguisitcs.
pp.
113-119.Y.Cheng, M.Asahara and Y.Matsumoto.
2006.
Multi-lingual Dependency Parsing at NAIST.
In Proceed-ings of the 10th Conference on ComputationalNatural Language Learning.
pp.
191-195.1055M.Collins.
1996.
A New Statistical Parser Based onBigram Lexical Dependencies.
In Proceedings ofthe 34th Annual Meeting of the Association forComputational Linguistics.
pp.
184-191.M.Collins.
1999.
Head-Driven Statistical Models forNatural Language Parsing.
Ph.D Thesis.
Universityof Pennsylvania.D.Graff et al.
2005.
Chinese Gigaword Second Edi-tion.
Linguistic Data Consortium, Philadelphia.J.Hall et al 2007.
Single Malt or Blended?
A Study inMultilingual Parser Optimization.
In Proceedingsof the shared task at the Conference on Computa-tional Natural Language Learning 2007. pp.
933-939.X.Han et al.
2004.
Subcategorization Acquisition andEvaluation for Chinese Verbs.
In Proceedings ofthe 20th International Conference on Computa-tional Linguistics.G.Jin and X.Chen.
2008.
The Fourth InternationalChinese Language Processing Bakeoff: ChineseWord Segmentation, Named Entity Recognitionand Chinese Pos Tagging.
In Proceedings of the 6thSIGHAN Workshop on Chinese Language Process-ing.D.Kawahara and S.Kurohashi.
2006 (a).
A Fully-lexicalized Probabilistic Model for Japanese Syn-tactic and Case frame Analysis.
In Proceedings ofthe Human Language Technology conference -North American chapter of the Association forComputational Linguistics annual meeting 2006.pp.
176-183.D.Kawahara and S.Kurohashi.
2006 (b).
Case FrameCompilation from the Web Using High-performance Computing.
In Proceedings of the 5thInternational Conference on Language Resourcesand Evaluation.T.Kudo and Y.Matsumoto.
2002.
Japanese Depend-ency Analysis Using Cascaded Chunking.
In Pro-ceedings of the Conference on Natural LanguageLearning.
pp.
29-35.R.McDonald and F.Pereira.
2006.
Online Learning ofApproximate Dependency Parsing Algorithm.
InProceedings of the 11th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics.R.McDonald and J.Nivre.
2007.
Characterizing theErrors of Data-driven Dependency Parsing Models.In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing Confer-ence on Computational Natural Language Learn-ing 2007.T.Nakagawa and K.Uchimoto.
2007.
A Hybrid Ap-proach to Word Segmentation and POS Tagging.In Proceedings of the 45th Annual Meeting of theAssociation for Computational Linguistics.
pp.217-220.R.Reichart and A.Rappoport.
2007.
An EnsembleMethod for Selection of High Quality Parses.
InProceedings of the 45th Annual Meeting of the As-sociation for Computational Linguistics.
pp.
408-415.K.Sagae and J.Tsujii.
2007.
Dependency Parsing andDomain Adaptation with LR Models and ParserEnsembles.
In Proceedings of the shared task atthe Conference on Computational Natural Lan-guage Learning 2007. pp.
1044-1050.Q.Wang, D.Schuurmans, and D.Lin.
2005.
StrictlyLexical Dependency Parsing.
In Proceedings of the9th International Workshop on Parsing Technolo-gies.
pp.
152-159.A.Wu.
2003.
Learning Verb-Noun Relations to Im-prove Parsing.
In Proceedings of the 2nd SIGHANWorkshop on Chinese Language Processing.
pp.119-124.N.Xue, F.Chiou and M.Palmer.
2002.
Building aLarge-Scale Annotated Chinese Corpus.
In Pro-ceedings of the 18th International Conference onComputational Linguistics.N.Xue and M.Palmer.
2003.
Annotating the Proposi-tions in the Penn Chinese Treebank.
In Proceed-ings of the 2nd SIGHAN Workshop on Chinese Lan-guage Processing.N.Xue and M.Palmer.
2005.
Automatic SemanticRule Labeling for Chinese Verbs.
In Proceedingsof the 19th International Joint Conference on Artifi-cial Intelligence.H.Yamada and Y.Matsumoto.
2003.
Statistical De-pendency Analysis with Support Vector Machines.In Proceedings of the 7th International Workshopon Parsing Technologies.A.Yates, S.Schoenmackers, and O.Etzioni.
2006.
De-tecting Parser Errors Using Web-based SemanticFilters.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Process-ing.
pp.
27-34.J.You and K.Chen.
2004.
Automatic Semantic RoleAssignment for a Tree Structure.
In Proceedings ofthe 3rd SIGHAN Workshop on Chinese LanguageProcessing.K.Yu, S.Kurohashi, and H.Liu.
2007.
A Three-stepDeterministic Parser for Chinese Dependency Pars-ing.
In Proceedings of the Human Language Tech-nologies: the Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics 2007. pp.
201-204.1056
