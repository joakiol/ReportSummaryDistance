A resource for constructing customized test suitesfor molecular biology entity identification systemsK.
Bretonnel CohenCenter for Computational Pharmacology,University of Colorado School of Medicinekevin.cohen@uchsc.eduLorraine TanabeNational Center for BiotechnologyInformation, NLM, NIHtanabe@ncbi.nlm.nih.govShuhei KinoshitaFujitsu Ltd. Bio-IT Lab, and Center forComputational Pharmacology, University ofColorado School of Medicineshuhei.kinoshita@uchsc.eduLawrence HunterCenter for Computational Pharmacology,University of Colorado School of Medicinelarry.hunter@uchsc.eduAbstractThis paper describes a data source andmethodology for producing customized testsuites for molecular biology entityidentification systems.
The data consists of:(a) a set of gene names and symbols classifiedby a taxonomy of features that are relevant tothe performance of entity identificationsystems, and (b) a set of sententialenvironments into which names and symbolsare inserted to create test data and theassociated gold standard.
We illustrate theutility of test sets producible by thismethodology by applying it to five entityidentification systems and describing the errorpatterns uncovered by it, and investigaterelationships between performance on acustomized test suite generated from this dataand the performance of a system on twocorpora.1 IntroductionThis paper describes a methodology and data for thetesting of molecular biology entity identification (EI)systems by developers and end users.
Molecularbiology EI systems find names of genes and geneproducts in free text.
Several years?
publication historyhas established precision, recall, and F-score as the defacto standards for evaluating EI systems for molecularbiology texts at the publication stage and incompetitions like BioCreative(www.mitre.org/public/biocreative).
These measuresprovide important indices of a system?s overall outputquality.
What they do not provide is the detailed sort ofinformation about system performance that is useful forthe system developer who is attempting to assess thestrengths and weaknesses of a work in progress, nor dothey provide detailed information to the potentialconsumer who would like to compare two systemsagainst each other.
Hirschman and Mani (2003) pointout that different evaluation methods are useful atdifferent points in the software life-cycle.
In particular,what they refer to as feature-based evaluation via testsuites is useful at two points: in the development phase,and for acceptance testing.
We describe here amethodology and a set of data for constructingcustomized feature-based test suites for EI in themolecular biology domain.
The data consists of twosets.
One is a set of names and symbols of entities asthat term is most commonly understood in themolecular biology domain?genes and gene products.
(Sophisticated ontologies such as GENIA (Ohta et al2002) include other kinds of entities relevant tomolecular biology as well, such as cell lines.)
Thenames and symbols exemplify a wide range of thefeatures that characterize entities in this domain?casevariation, presence or absence of numbers, presence orabsence of hyphenation, etc.
The other is a set ofsentences that exemplify a range of sentential contextsin which the entities can appear, varying with respect toposition of the entity in the sentence (initial, medial, orfinal), presence of keywords like gene and protein,tokenization issues, etc.
Both the entities and thesentential contexts are classified in terms of a taxonomyof features that are relevant to this domain in particularand to natural language processing and EI in general.The methodology consists of generating customized testsuites that address specific performance issues bycombining sets of entities that have particularcharacteristics with sets of contexts that have particularcharacteristics.
Logical combination of subsets ofcharacteristics of entities and contexts allows thedeveloper to assess the effect of specific characteristicson performance, and allows the user to assessperformance of the system on types of inputs that are ofAssociation for Computational Linguistics.Linking Biological Literature, Ontologies and Databases, pp.
1-8.HLT-NAACL 2004 Workshop: Biolink 2004,particular interest to them.
For example, if thedeveloper or end-user wants to assess the ability of asystem to recognize gene symbols with a particularcombination of letter case, hyphenation, and presenceor absence of numerals, the data and associated codethat we provide can be used to generate a test suiteconsisting of symbols with and without thatcombination of features in a variety of sententialcontexts.Inspiration for this work comes on the one handfrom standard principles of software engineering andsoftware testing, and on the other hand from descriptivelinguistics (Harris 1951, Samarin 1967).
In Hirschmanand Mani?s taxonomy of evaluation techniques, ourmethodology is referred to as feature-based, in that it isbased on the principle of classifying the inputs to thesystem in terms of some set of features that are relevantto the application of interest.
It is designed to providethe developer or user with detailed information aboutthe performance of her EI system.
We apply it to fivemolecular biology EI and information extractionsystems: ABGene (Tanabe and Wilbur 2002a, Tanabeand Wilbur 2002b); KeX/PROPER (Fukuda et al1997); Yapex (Franz?n et al 2002); the stochastic POStagging-based system described in Cohen et al (insubmission); and the entity identification component ofOno et al?s information extraction system (Ono et al2001), and show how it gives detailed usefulinformation about each that is not apparent from thestandard metrics and that is not documented in the citedpublications.
(Since we are not interested in punishingsystem developers for graciously making their workavailable by pointing out their flaws, we do not refer tothe various systems by name in the remainder of thispaper.
)Software testing techniques can be grouped intostructured (Beizer 1990), heuristic (Kaner et al 2002),and random categories.
Testing an EI system byrunning it on a corpus of texts and calculating precision,recall, and F-score for the results falls into the categoryof random testing.
Random testing is a powerfultechnique, in that it is successful in finding bugs.
Whendone for the purpose of evaluation, as distinct fromtesting (see Hirschman and Thompson 1997 for thedistinction between the two, referred to there asperformance evaluation and diagnostic evaluation), italso is widely accepted as the relevant index ofperformance for publication.
However, its output lacksimportant information that is useful to a systemdeveloper (or consumer): it tells you how often thesystem failed, but not what it failed at; it tells you howoften the system succeeds, but not where its strengthsare.For the developer or the user, a structured test suiteoffers a number of advantages in answering these sortsof questions.
The utility of such test suites in generalsoftware testing is well-accepted.
Oepen et al (1998)lists a number of advantages of test suites vs.naturalistic corpora for testing natural languageprocessing software in particular:?
Control over test data: test suites allow for?focussed and fine-grained diagnosis of systemperformance?
(15).
This is important to thedeveloper who wants to know exactly whatproblems need to be fixed to improve performance,and to the end user who wants to know thatperformance is adequate on exactly the data thatthey are interested in.?
Systematic coverage: test suites can allow forsystematic evaluation of variations in a particularfeature of interest.
For example, the developermight want to evaluate how performance varies asa function of name length, or case, or the presenceor absence of hyphenation within gene symbols.The alternative to using a structured test suite is touse a corpus, and then search through it for therelevant inputs and hope that they are actuallyattested.?
Control of redundancy: while redundancy in acorpus is representative of actual redundancy ininputs, test suites allow for reduction ofredundancy when it obscures the situation, or forincreasing it when it is important to test handling ofa feature whose importance is greater than itsfrequency in naturally occurring data.
Forexample, names of genes that are similar to namesof inherited diseases might make up only a smallproportion of the gene names that occur in PubMedabstracts, but the user whose interests lie incurating OMIM might want to be able to assureherself that coverage of such names is adequate,beyond the level to which corpus data will allow.?
Inclusion of negative data: in the molecularbiology domain, a test suite can allow forsystematic evaluation of potential false positives.?
Coherent annotation: even the richest metadata israrely adequate or exactly appropriate for exactlythe questions that one wants to ask of a corpus.Generation of structured, feature-based test suitesobviates the necessity for searching throughcorpora for the entities and contexts of interest, andallows instead the structuring of contexts andlabeling of examples that is most useful to thedeveloper.The goal of this paper is to describe a methodology andpublicly available data set for constructing customizedand refinable test suites in the molecular biologydomain quickly and easily.
A crucial differencebetween similar work that simply documents adistributable test suite (e.g.
Oepen (1998) and Volk(1998)) and the work reported in this paper is that weare distributing not a static test suite, but rather data forgenerating test suites?data that is structured andclassified in such a way as to allow software developersand end users to easily generate test suites that arecustomized to their own assessment needs anddevelopment questions.
We build this methodologyand data on basic principles of software engineeringand of linguistic analysis.
The first such principleinvolves making use of the software testing notion ofthe catalogue.A catalogue is a list of test conditions, or qualitiesof particular test inputs (Marick 1997).
It correspondsto the features of feature-based testing, discussed inHirschman and Mani (2003) and to the schedule(Samarin 1967:108-112) of descriptive linguistictechnique.
For instance, a catalogue of test conditionsfor numbers might include:?
zero, non-zero, real, integer?
positive, negative, unsigned?
the smallest number representable in some datatype, language, or operating system; smaller thanthe smallest number representable?
the largest number representable; larger than thelargest number representableNote that the catalogue includes both ?clean?conditions and ?dirty?
ones.
This approach to softwaretesting has been highly successful, and indeed the best-selling book on software testing (Kaner et al 1999) canfairly be described as a collection of catalogues ofvarious types.The contributions of descriptive linguistics includeguiding our thinking about what the relevant features,conditions, or categories are for our domain of interest.In this domain, that will include the questions of whatfeatures may occur in names and what features mayoccur in sentences?particularly features in the one thatmight interact with features in the other.
Descriptivelinguistic methodology is described in detail in e.g.Harris (1951) and Samarin (1967); in the interests ofbrevity, we focus on the software engineeringperspective here, but the thought process is verysimilar.
The software engineering equivalent of thedescriptive linguist?s hypothesis is the fault model(Binder 1999)?an explicit hypothesis about a potentialsource of error based on ?relationships and componentsof the system under test?
(p. 1088).
For instance,knowing that some EI systems make use of POS taginformation, we might hypothesize that the presence ofsome parts of speech within a gene name might bemistaken for term boundaries (e.g.
the of in bag ofmarbles, LocusID 43038).
Catalogues are used todevelop a set of test cases that satisfies the variousqualities.
(They can also be used post-hoc to group theinputs in a random test bed into equivalence classes,although a strong motivation for using them in the firstplace is to obviate this sort of search-based post-hocanalysis.)
The size of the space of all possible testcases can be estimated from the Cartesian product of allcatalogues; the art of software testing (and linguisticfieldwork) consisting, then, of selecting the highest-yielding subset of this often enormous space that can berun and evaluated in the time available for testing.At least three kinds of catalogues are relevant totesting an EI system.
They fall into one of two verybroad categories: syntagmatic, having to do withcombinatory properties, and paradigmatic, having to dowith varieties of content.
The three kinds of cataloguesare:1.
A catalogue of environments in which gene namescan appear.
This is syntagmatic.2.
A catalogue of types of gene names.
This isparadigmatic.3.
A catalogue of false positives.
This is bothsyntagmatic and paradigmatic.The catalogue of environments would include, forexample, elements related to sentence position, such assentence-initial, sentence-medial, and sentence-final;elements related to list position, such as a single genename, a name in a comma-separated list, or a name in aconjoined noun phrase; and elements related totypographic context, such as location withinparentheses (or not), having attached punctuation (e.g.
asentence-final period) (or not), etc.
The catalogue oftypes of names would include, for example, names thatare common English words (or not); names that arewords versus ?names?
that are symbols; single-wordversus multi-word names; and so on.
The secondcategory also includes typographic features of genenames, e.g.
containing numbers (or not), consisting ofall caps (or not), etc.
We determined candidate featuresfor inclusion in the catalogues through standardstructuralist techniques such as examining public-domain databases containing information about genes,including FlyBase, LocusLink, and HUGO, and byexamining corpora of scientific writing about genes,and also by the software engineering techniques of?common sense, experience, suspicion, analysis, [and]experiment?
(Binder 1999).
The catalogues thensuggested the features by which we classified andvaried the entities and sentences in the data.General format of the dataThe entities and sentences are distributed in XMLformat and are available at a supplemental web site(compbio.uchsc.edu/Hunter_lab/testing_ei).
A plain-text version is also available.
A representative entity isillustrated in Figure 1 below, and a representativesentence is illustrated in Figure 2.
All data in thecurrent version is restricted to the ASCII character set.Test suite generationData sets are produced by selecting sets of entityfeatures and sets of sentential context features andinserting the entities into slots in the sentences.
Thiscan be accomplished with the user?s own tools, or usingapplications available at the supplemental web site.The provided applications produce two files: a filecontaining raw data for use as test inputs, and a filecontaining the corresponding gold standard data markedup in an SGML-like format.
For example, if the rawdata file contains the sentence ACOX2 polymorphismsmay be correlated with an increased risk of larynxcancer, then the gold standard file will contain thecorresponding sentence <gp>ACOX2</gp>polymorphisms may be correlated with an increasedrisk of larynx cancer.
Not all users will necessarilyagree on what counts as the ?right?
gold standard?seeOlsson et al (2002) and the BioCreative site for someof the issues.
Users can enforce their own notions ofcorrectness by using our data as input to their owngeneration code, or by post-processing the output of ourapplications.ID: 136name_vs_symbol: nlength: 3case: acontains_a_numeral: ycontains_Arabic_numeral: yArabic_numeral_position: fcontains_Roman_numeral:<several typographic features omitted>contains_punctuation: 1contains_hyphen: 1contains_forward_slash:<several punctuation-related features omitted>contains_function_word:function_word_position:contains_past_participle: 1past_participle_position: icontains_present_participle:present_participle_position:source_authority: HGNC ID: 2681 "Approved GeneName" fieldoriginal_form_in_source: death-associatedprotein 6data: death-associated protein 6Figure 1  A representative entry from the entity datafile.
A number of null-valued features are omitted forbrevity?see the full entry at the supplemental web site.The data field (last line of the figure) is what is outputby the generation software.ID: 25type: tptotal_number_of_names: 1list_context:position: Itypographic_context:appositive:source_id: PMID: 14702106source_type: titleoriginal_form_in_source: Stat-3 is requiredfor pulmonary homeostasis during hyperoxia.slots: <> is required for pulmonaryhomeostasis during hyperoxia.Figure 2  A representative entry from the sentencesfile.
Features and values are explained in section 2.2Feature set for sentential contexts below.
The slotsfield (last line of the figure) shows where an entitywould be inserted when generating test data.2   The taxonomy of features for entitiesand sentential contextsIn this section we describe the feature sets for entitiesand sentences, and motivate the inclusion of each,where not obvious.2.1   Feature set for entitiesConceptually, the features for describing name-inputsare separated into four categories:orthographic/typographic, morphosyntactic, source, andlexical.?
Orthographic/typographic features describe thepresence or absence of features on the level ofindividual characters, for example the case of letters,the presence or absence of punctuation marks, and thepresence or absence of numerals.?
Morphosyntactic features describe the presence orabsence of features on the level of the morpheme orword, such as the presence or absence of participles,the presence or absence of genitives, and the presenceor absence of function words.?
Source features are defined with reference to thesource of an input.
(It should be noted that in softwareengineering, as in Chomskyan theoretical linguistics,data need not be naturally-occurring to be useful;however, with the wealth of data available for genenames, there is no reason not to include naturalisticdata, and knowing its source may be useful, e.g.
inevaluating performance on FlyBase names, etc.
)Source features include source type, e.g.
literature,database, or invention; identifiers in a database;canonical form of the entity in the database; etc.?
Lexical features are defined with respect to therelationship between an input and some outside sourceof lexical information, for instance whether or not aninput is or contains a common English word.
This isalso the place to indicate whether or not an input ispresent in a resource such as LocusLink, whether or notit is on a particular stoplist, whether it is in-vocabularyor out-of-vocabulary for a particular language model,etc.The distinction between these three broadcategories of features is not always clear-cut.
Forexample, presence of numerals is anorthographic/typographic feature, and is alsomorphosyntactic when the numeral postmodifies anoun, e.g.
in heat shock protein 60.
Likewise, featuresmay be redundant?for example, the presence of aGreek letter in the square-bracket- or curly-bracket-enclosed formats, or the presence of an apostrophizedgenitive, are not independent of the presence of theassociated punctuation marks.
However, Booleanqueries over the separate feature sets let them bemanipulated and queried independently.
So, entitieswith names like A' can be selected independently ofnames like Parkinson?s disease.2.1.1   Orthographic/typographic featuresLength:  Length is defined in characters forsymbols and in whitespace-tokenized words for names.Case: This feature is defined in terms of fivepossible values: all-upper-case, all-lower-case, upper-case-initial-only, each-word-upper-case-initial (e.g.Pray For Elves), and mixed.
The fault modelmotivating this feature hypothesizes that taggers mayrely on case to recognize entities and may fail on somecombinations of cases with particular sententialpositions.
For example, one system performed well ongene symbols in general, except when the symbols arelower-case-initial and in sentence-initial position (e.g.p100 is abundantly expressed in liver?
(PMID1722209) and bif displays strong genetic interactionwith msn (PMID 12467587).Numeral-related features: A set of featuresencodes whether or not an entity contains a numeral,whether the numeral is Arabic or Roman, and thepositions of numerals within the entity (initial, medial,or final).
The motivation for this feature is thehypothesis that a system might be sensitive to thepresence or absence of numerals in entities.
Onesystem failed when the entity was a name (vs. asymbol), it contained a number, and the number was inthe right-most (vs. a medial) position in a word.
Itcorrectly tagged entities like glucose 6 phosphatedehydrogenase but missed the boundary on<gp>alcohol dehydrogenase</gp> 6.
This pattern wasspecific to numbers?letters in the same position arehandled correctly.Punctuation-related features: A set of featuresincludes whether an entity contains any punctuation, thecount of punctuation marks, and which marks they are(hyphen, apostrophe, etc.).
One system failed torecognize names (but typically not symbols) when theyincluded hyphens.
Another system had a very reliablepattern of failure involving apostrophes just in case theywere in genitives.Greek-letter-related features:  These featuresencode whether or not an entity contains a Greek letter,the position of the letter, and the format of the letter.
(This feature is an example of an orthographic featurewhich may be defined on a substring longer than acharacter, e.g.
beta.)
Two systems had problemsrecognizing gene names when they contained Greekletters in the PubMed Central format, i.e.
[beta]1integrin.2.1.2   Morphosyntactic featuresThe most salient morphosyntactic feature is whether anentity is a name or a symbol.
The fault modelmotivating this feature suggests that a system mightperform differently depending on whether an input is aname or a symbol.
The most extreme case of a systembeing sensitive to this feature was one system thatperformed very well on symbols but recognized nonames whatsoever.Features related to function words: a set offeatures encodes whether or not an entity contains afunction word, the number of function words in theentity, and their positions?for instance, the facts: thatscott of the antarctic (FlyBase ID FBgn0015538)contains two function words; that they are of and the;and that they are medial to the string.
This feature ismotivated by two fault models.
One posits that asystem might apply a stoplist to its input and thatprocessing of function words might therefore halt at anearly stage.
The other posits that a system mightemploy shallow parsing to find boundaries of entitiesand that the shallow parser might insert boundaries atthe locations of function words, causing some words tobe omitted from the entity.
One system always hadpartial hits on names that were multi-word unless eachword in it was upper-case-initial, or there was analphanumeric postmodifier (i.e.
a numeral, upper-casedsingleton letter, or Greek letter) at the right edge.Features related to inflectional morphology: aset of features encodes whether or not an entity containsnominal number or genitive morphology or verbalparticipial morphology, and the positions of the wordsin the entity that contain those morphemes, for instancethe facts that apoptosis antagonizing transcriptionfactor (HUGO ID 19235) contains a present participleand that the word that contains it is medial to the string.Features related to parts of speech: Futuredevelopment of the data will include features encodingthe parts of speech present in names.2.1.3   Source featuresSource or authority:  This feature encodes thesource of or authority cited for an entity.
For many ofthe entries in the current data, it is an identifier fromsome database.
For others, it is a website (e.g.www.flynome.org).
Other possible values include thePMID of a document in which it was observed.Original form in source:  Where there is a sourcefor the entity or for some canonical form of the entity,the original form is given.
This is not equivalent to the?official?
form, but rather is the exact form in which theentity occurs; it may even contain typographic errors(e.g.
the extraneous space in nima ?related kinase,LocusID 189769 (reported to the NCBI service desk).2.1.4   Lexical featuresThese might be better called lexicographic features.They can be encoded impressionistically, or can bedefined with respect to an external source, such asWordNet, the UMLS, or other lexical resources.
Theymay also be useful for encoding strictly localinformation, such as whether or not a gene was attestedin training data or whether it is present in a particularlanguage model or other local resource.
These featuresare allowed in the taxonomy but are not implemented inthe current data.
Our own use of the entity datasuggests that it should be, especially encoding ofwhether or not names include common English words.
(The presence of function words is already encoded.
)2.2   Feature set for sentential contextsIn many ways, this data is much harder to build andclassify than the names data, for at least two reasons.Many more features interact with each other, and assoon as a sentence contains more than one gene name,it contains more than one environment, and the numberof features for the sentence as a whole is multiplied, asare the interactions between them.
For this reason, wehave focussed our attention so far on sentencescontaining only a single gene name, although thecurrent version of the data does include a number ofmulti-name sentences.2.2.1   PositivityThe fundamental distinction in the feature set forsentences has to do with whether the sentence isintended to provide an environment in which genenames actually appear, or whether it is intended toprovide a non-trivial opportunity for false positives.True positive sentences contain some slot in whichentities from the names data can be inserted, e.g.
<>polymorphisms may be correlated with an increasedrisk of larynx cancer or <> interacts with <> and <>in the two-hybrid system.False positive sentences contain one or moretokens that are deliberately intended to posechallenging opportunities for false positives.
Certainlyany sentence which does not consist all and only of asingle gene name contains opportunities for falsepositives, but not all potential false positives are createdequal.
We include in the data set sentences that containtokens with orthographic and typographiccharacteristics that mimic the patterns commonly seenin gene names and symbols, e.g.
The aim of the presentstudy is to evaluate the impact on QoL?
where QoL isan abbreviation for quality of life.
We also includesentences that contain ?keywords?
that may often beassociated with genes, such as gene, protein, mutant,expression, etc., e.g.
Demonstration of antifreezeprotein activity in Antarctic lake bacteria.2.2.2   Features for TP sentencesNumber and positional features encode the total numberof slots in the sentence, and their positions.
The valuefor the position feature is a list whose values range overinitial, medial, and final.
For example, the sentence<> interacts with <> and <> in the two-hybrid systemhas the value I,M (initial and medial) for the positionfeature.Typographic context features encode issuesrelated to tokenization, specifically related topunctuation, for example if a slot has punctuation onthe left or right edge, and the identity of the punctuationmarks.List context features encode data about position inlists.
These include the type of list (coordination,asyndetic coordination, or complex coordination).The appositive feature is for the special case ofappositioned symbols or abbreviations and their fullnames or definitions, e.g.
The Arabidopsis INNER NOOUTER (INO) gene is essential for formation and?For the systems that we have tested with it, it has notrevealed problems that are independent of thetypographic context.
However, we expect it to be offuture use in testing systems for abbreviation expansionin this domain.Source features encode the identification and typeof the source for the sentence and its original form inthe source.
The source identifier is often a PubMed ID.It bears pointing out again that there is no a priorireason to use sentences with any naturally-occurring?source?
at all, as opposed to the products of thesoftware engineer?s imagination.
Our primary rationalefor using naturalistic sources at all for the sentence datahas more to do with convincing the user that some ofthe combinations of entity features and sententialfeatures that we claim to be worth generating actuallydo occur.
For instance, it might seem counterintuitivethat gene symbols or names would ever occur lower-case-initial in sentence initial position, but in fact wefound many instances of this phenomenon; or that amulti-word gene name would occur in text in all upper-case letters, but see the INNER NO OUTER exampleabove.Syntactic features encode the characteristics ofthe local environment.
Some are very lexical, such as:whether the following word is a keyword; whether thepreceding word is a species name.
Others are moreabstract, such as whether the preceding word is anarticle; whether the preceding word is an adjective;whether the preceding word is a conjunction; whetherthe preceding word is a preposition.
Interactions withthe list context features are complex.
The fault modelmotivating these features hypothesizes that POS contextand the presence of keywords might affect a system?sjudgments about the presence and boundaries of names.2.2.3   Features for FP sentencesMost features for FP sentences encode thecharacteristics that give the contents of the sentencetheir FP potential.
The keyword feature is a list ofkeywords present in the sentence, e.g.
gene, protein,expression, etc.
The typographic features featureencodes whether or not the FP potential comes fromorthographic or typographic features of some token inthe sentence, such as mixed case, containing hyphensand a number, etc.
The morphological features featureencodes whether or not the FP potential comes fromapparent morphology, such as words that end with aseor in.3   Testing the relationship betweenpredictions from performance on a testsuite and performance on a corpusPrecision and recall on data in a structured test suiteshould not be expected to predict precision and recallon a corpus, since there is no relation between theprevalence of features in the test suite and prevalence offeatures in the corpus.
However, we hypothesized thatperformance on an equivalence class of inputs in a testsuite might predict performance on the sameequivalence class in a corpus.
To test this hypothesis,we ran a number of test suites through one of thesystems and analyzed the results, looking for patterns oferrors.
The test suites were very simple, varying onlyentity length, case, hyphenation, and sentence position.Then we ran two corpora through the same system andexamined the output for the actual corpora to see if thepredictions based on the system?s behavior on the testsuite actually described performance on similar entitiesin the corpora.One corpus, which we refer to as PMC (since itwas sampled from PubMed Central), consists of 2417sentences sampled randomly from a set of 1000 full-text articles.
This corpus contains 3491 entities.
It isdescribed in Tanabe and Wilbur (2002b).
The secondcorpus was distributed as training data for theBioCreative competition.
It consists of 10,000sentences containing 11,851 entities and is described indetail at www.mitre.org/public/biocreative.
Eachcorpus is annotated for entities.The predictions based on the system?s performanceon the test suite data were:1.
The system will have low recall on entities thathave numerals in initial position, followed by adash, e.g.
825-Oak, 12-LOX, and 18-wheeler(/^\d+-/ in Perl).2.
The system will have low recall on names thatcontain stopwords, such as Pray For Elves and kenand barbie.3.
The system will have low recall on sentence-medial terms that begin with a capital letter, suchas Always Early.4.
The system will have low recall on three-character-long symbols.5.
The system will have good recall on (long) namesthat end with numerals.We then examined the system?s true positive, falsepositive, and false negative outputs from the twocorpora for outputs that belonged to the equivalenceclasses in 1-5.
Table 1 shows the results.BioCreativeTP FP FN P R1 12 57 17 .17 .412 0 1 38 0.0 0.04 556 278 512 .67 .525 284 251 72 .53 .80PubMed CentralTP FP FN P R1 8 10 0 .44 1.02 1 0 2 1.0 .334 163 64 188 .72 .465 108 54 46 .67 .70Table 1  Performance on two corpora for thepredictable categories  Numbers in the far left columnrefer to the predictions listed above.
Overallperformance on the corpora was: BioCreative P = .65,R = .68, and PMC P = .71, R = .62.For equivalence classes 1, 2, and 4, the predictionsmostly held.
Low recall was predicted, and actualrecall was .41, 0.0, .52, 1.0 (the one anomaly), .33, and.46 for these classes of names, versus overall recall of.68 on the BioCreative corpus and .62 on the PMCcorpus.
The prediction held for equivalence class 5, aswell; good recall was predicted, and actual recall was.80 and .70?higher than the overall recalls for the twocorpora.
The third prediction could not be evaluateddue to the normalization of case in the gold standards.These results suggest that a test suite can be a goodpredictor of performance on entities with particulartypographic characteristics.4   ConclusionWe do not advocate using this approach to replace thequantitative evaluation of EI systems by precision,recall, and F-measure.
Arguably, overall performanceon real corpora is the best evaluation metric for entityidentification, in which case the standard metrics arewell-suited to the task.
However, at specific points inthe software lifecycle, viz.
during development and atthe time of acceptance testing, the standard metrics donot provide the right kind of information.
We can,however, get at this information if we bear in mind twothings:1.
Entity identification systems are software, and assuch can be assessed by standard software testingtechniques.2.
Entity identification systems are in some senseinstantiations of hypotheses about linguisticstructure, and as such can be assessed by standardlinguistic ?field methods.
?This paper describes a methodology and a data set forutilizing the principles of software engineering andlinguistic analysis to generate test suites that answer theright kinds of questions for developers and for endusers.
Readers are invited to contribute their own data.AcknowledgmentsThe authors gratefully acknowledge support for thiswork from NIH/NIAAA grant U01-AA13524-02;comments from Andrew E. Dolbey on an earlierversion of this work; Philip V. Ogren for help withstochastic-POS-tagging-based system; the Center forComputational Pharmacology NLP reading group andthe anonymous reviewers for insightful comments onthe current version; and Fukuda et al, Ono et al, andFranz?n et al for generously making their systemspublicly available.ReferencesBeizer, Boris (1990).
Software testing techniques, 2nded.
Van Nostrand Reinhold.Binder, Robert V. (1999).
Testing object-orientedsystems: models, patterns, and tools.
Addison-Wesley.Cohen, K. Bretonnel; Philip V. Ogren; ShuheiKinoshita; and Lawrence Hunter (in submission).Entity identification in the molecular biology domainwith a stochastic POS tagger.
ISMB 2004.Cole, Ronald; Joseph Mariani; Hans Uszkoreit; AnnieZaenen; and Victor Zue (1997).
Survey of the state ofthe art in human language technology.
CambridgeUniversity Press.Franz?n, Kristofer; Gunnar Eriksson; Fredrik Olsson;Lars Asker; Per Lid?n; and Joakim C?ster (2002).Protein names and how to find them.
InternationalJournal of Medical Informatics 67(1-3):49-61.Fukuda, K.; T. Tsunoda; A. Tamura; and T. Takagi(1997).
Toward information extraction: identifyingprotein names from biological papers.
PacificSymposium on Biocomputing 1998, pp.
705-716.Harris, Zellig S. (1951).
Methods in structurallinguistics.
University of Chicago Press.Hirschman, Lynette; and Inderjeet Mani (2003).Evaluation.
In Mitkov (2003), pp.
415-429.Hirschman, Lynette; and Henry S. Thompson (1997).Overview of evaluation in speech and naturallanguage processing.
In Cole et al (1997), pp.
409-414.Kaner, Cem; Hung Quoc Nguyen; and Jack Falk(1999).
Testing computer software, 2nd ed.
JohnWiley & Sons.Kaner, Cem; James Bach; and Bret Pettichord (2002).Lessons learned in software testing: a context-drivenapproach.
John Wiley & Sons.Marick, Brian (1997).
The craft of software testing:subsystem testing including object-based and object-oriented testing.
Prentice Hall.Mitkov, Ruslan (2003).
The Oxford Handbook ofComputational Linguistics.
Oxford University Press.Nerbonne, John (1998).
Linguistic Databases.
CSLIPublications.Ohta, Tomoko; Yuka Tateisi; Jin-Dong Kim; HidekiMima; and Jun-ichi Tsujii (2002).
The GENIAcorpus: an annotated corpus in molecular biology.Proceedings of the Human Language TechnologyConference.Oepen, Stephan; Klaus Netter; and Judith Klein (1998).TSNLP ?
Test Suites for Natural LanguageProcessing.
In Nerbonne (1998), pp.
13-36.Olsson, Fredrik; Gunnar Eriksson; Kristofer Franz?n;Lars Asker; and Per Lid?n (2002).
Notions ofcorrectness when evaluating protein name taggers.Proceedings of the 19th International Conference onComputational Linguistics (COLING 2002), Taipei,Taiwan.Ono, Toshihide; Haretsugu Hishigaki; Akira Tanigami;and Toshihisa Takagi (2001).
Automated extractionof information on protein-protein interactions fromthe biological literature.
Bioinformatics 17(2):155-161.Samarin, William J.
(1967).
Field linguistics: a guideto linguistic field work.
Irvington.Tanabe, Lorraine; and W. John Wilbur (2002a).Tagging gene and protein names in biomedical text.Bioinformatics 18(8):1124-1132.Tanabe, Lorraine; and W. John Wilbur (2002b).Tagging gene and protein names in full text articles.Proceedings of the workshop on natural languageprocessing in the biomedical domain, pp.
9-13.Association for Computational Linguistics.Volk, Martin (1998).
Markup of a test suite withSGML.
In Nerbonne (1998), pp.
59-76.
