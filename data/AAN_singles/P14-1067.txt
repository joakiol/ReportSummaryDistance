Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 710?720,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsAdaptive Quality Estimation for Machine TranslationMarco Turchi(1)Antonios Anastasopoulos(3)Jos?e G. C. de Souza(1,2)Matteo Negri(1)(1)FBK - Fondazione Bruno Kessler, Via Sommarive 18, 38123 Trento, Italy(2)University of Trento, Italy(3)National Technical University of Athens, Greece{turchi,desouza,negri}@fbk.euanastasopoulos.ant@gmail.comAbstractThe automatic estimation of machinetranslation (MT) output quality is a hardtask in which the selection of the appro-priate algorithm and the most predictivefeatures over reasonably sized training setsplays a crucial role.
When moving fromcontrolled lab evaluations to real-life sce-narios the task becomes even harder.
Forcurrent MT quality estimation (QE) sys-tems, additional complexity comes fromthe difficulty to model user and domainchanges.
Indeed, the instability of the sys-tems with respect to data coming from dif-ferent distributions calls for adaptive so-lutions that react to new operating con-ditions.
To tackle this issue we proposean online framework for adaptive QE thattargets reactivity and robustness to userand domain changes.
Contrastive exper-iments in different testing conditions in-volving user and domain changes demon-strate the effectiveness of our approach.1 IntroductionAfter two decades of steady progress, researchin statistical machine translation (SMT) started tocross its path with translation industry with tan-gible mutual benefit.
On one side, SMT researchbrings to the industry improved output quality anda number of appealing solutions useful to increasetranslators?
productivity.
On the other side, themarket needs suggest concrete problems to solve,providing real-life scenarios to develop and eval-uate new ideas with rapid turnaround.
The evolu-tion of computer-assisted translation (CAT) envi-ronments is an evidence of this trend, shown bythe increasing interest towards the integration ofsuggestions obtained from MT engines with thosederived from translation memories (TMs).The possibility to speed up the translation pro-cess and reduce its costs by post-editing good-quality MT output raises interesting research chal-lenges.
Among others, these include decidingwhat to present as a suggestion, and how to do itin the most effective way.In recent years, these issues motivated researchon automatic QE, which addresses the problemof estimating the quality of a translated sentencegiven the source and without access to referencetranslations (Blatz et al, 2003; Specia et al, 2009;Mehdad et al, 2012).
Despite the substantialprogress done so far in the field and in success-ful evaluation campaigns (Callison-Burch et al,2012; Bojar et al, 2013), focusing on concretemarket needs makes possible to further define thescope of research on QE.
For instance, movingfrom controlled lab testing scenarios to real work-ing environments poses additional constraints interms of adaptability of the QE models to the vari-able conditions of a translation job.
Such variabil-ity is due to two main reasons:1.
The notion of MT output quality is highlysubjective (Koponen, 2012; Turchi et al,2013; Turchi and Negri, 2014).
Since thequality standards of individual users mayvary considerably (e.g.
according to theirknowledge of the source and target lan-guages), the estimates of a static QE modeltrained with data collected from a group ofpost-editors might not fit with the actualjudgements of a new user;2.
Each translation job has its own specifici-ties (domain, complexity of the source text,average target quality).
Since data from anew job may differ from those used to trainthe QE model, its estimates on the new in-stances might result to be biased or uninfor-mative.The ability of a system to self-adapt to the be-710haviour of specific users and domain changes isa facet of the QE problem that so far has beendisregarded.
To cope with these issues and dealwith the erratic conditions of real-world trans-lation workflows, we propose an adaptive ap-proach to QE that is sensitive and robust to dif-ferences between training and test data.
Along thisdirection, our main contribution is a framework inwhich QE models can be trained and can continu-ously evolve over time accounting for knowledgeacquired from post editors?
work.Our approach is based on the online learningparadigm and exploits a key difference betweensuch framework and the batch learning methodscurrently used.
On one side, the QE models ob-tained with batch methods are learned exclusivelyfrom a predefined set of training examples underthe assumption that they have similar characteris-tics with respect to the test data.
This makes themsuitable for controlled evaluation scenarios wheresuch condition holds.
On the other side, onlinelearning techniques are designed to learn in a step-wise manner (either from scratch, or by refining anexisting model) from new, unseen test instancesby taking advantage of external feedback.
Thismakes them suitable for real-life scenarios wherethe new instances to be labelled can considerablydiffer from the data used to train the QE model.To develop our approach, different online algo-rithms have been embedded in the backbone ofa QE system.
This required the adaptation of itsstandard batch learning workflow to:1.
Perform online feature extraction from asource?target pair (i.e.
one instance at a timeinstead of processing an entire training set);2.
Emit a prediction for the input instance;3.
Gather user feedback for the instance (i.e.calculating a ?true label?
based on theamount of user post-editions);4.
Send the true label back to the model to up-date its predictions for future instances.Focusing on the adaptability to user and domainchanges, we report the results of comparative ex-periments with two online algorithms and the stan-dard batch approach.
The evaluation is carried outby measuring the global error of each algorithmon test sets featuring different degrees of similar-ity with the data used for training.
Our resultsshow that the sensitivity of online QE models todifferent distributions of training and test instancesmakes them more suitable than batch methods forintegration in a CAT framework.Our adaptive QE infrastructure has been re-leased as open source.
Its C++ implementation isavailable at http://hlt.fbk.eu/technologies/aqet.2 Related workQE is generally cast as a supervised machinelearning task, where a model trained from a col-lection of (source, target, label) instances is usedto predict labels1for new, unseen test items (Spe-cia et al, 2010).In the last couple of years, research in the fieldreceived a strong boost by the shared tasks orga-nized within the WMT workshop on SMT,2whichis also the framework of our first experiment in?5.
Current approaches to the tasks proposed atWMT have mainly focused on three main direc-tions, namely: i) feature engineering, as in (Hard-meier et al, 2012; de Souza et al, 2013a; de Souzaet al, 2013b; Rubino et al, 2013b), ii) modellearning with a variety of classification and regres-sion algorithms, as in (Bicici, 2013; Beck et al,2013; Soricut et al, 2012), and iii) feature selec-tion as a way to overcome sparsity and overfittingissues, as in (Soricut et al, 2012).Being optimized to perform well on specificWMT sub-tasks and datasets, current systems re-flect variations along these directions but leave im-portant aspects of the QE problem still partiallyinvestigated or totally unexplored.3Among these,the necessity to model the diversity of human qual-ity judgements and correction strategies (Kopo-nen, 2012; Koponen et al, 2012) calls for solu-tions that: i) account for annotator-specific be-haviour, thus being capable of learning from inher-ently noisy datasets produced by multiple annota-tors, and ii) self-adapt to changes in data distribu-tion, learning from user feedback on new, unseentest items.1Possible label types include post-editing effort scores(e.g.
1-5 Likert scores indicating the estimated percentageof MT output that has to be corrected), HTER values (Snoveret al, 2006), and post-editing time (e.g.
seconds per word).2http://www.statmt.org/wmt13/3For a comprehensive overview of the QE approachesproposed so far we refer the reader to the WMT12 andWMT13 QE shared task reports (Callison-Burch et al, 2012;Bojar et al, 2013).711These interconnected issues are particularly rel-evant in the CAT framework, where translationjobs from different domains are routed to pro-fessional translators with different idiolect, back-ground and quality standards.The first aspect, modelling annotators?
individ-ual behaviour and interdependences, has been ad-dressed by Cohn and Specia (2013), who exploredmulti-task Gaussian Processes as a way to jointlylearn from the output of multiple annotations.
Thistechnique is suitable to cope with the unbalanceddistribution of training instances and yields bettermodels when heterogeneous training datasets areavailable.The second problem, the adaptability of QEmodels, has not been explored yet.
A commontrait of all current approaches, in fact, is the re-liance on batch learning techniques, which assumea ?static?
nature of the world where new unseeninstances that will be encountered will be similarto the training data.4However, similarly to trans-lation memories that incrementally store translatedsegments and evolve over time incorporating usersstyle and terminology, all components of a CATtool (the MT engine and the mechanisms to assignquality scores to the suggested translations) shouldtake advantage of translators feedback.On the MT system side, research on adaptiveapproaches tailored to interactive SMT and CATscenarios explored the online learning protocol(Littlestone, 1988) to improve various aspects ofthe decoding process (Cesa-Bianchi et al, 2008;Ortiz-Mart?
?nez et al, 2010; Mart?
?nez-G?omez etal., 2011; Mart?
?nez-G?omez et al, 2012; Mathuret al, 2013; Bertoldi et al, 2013).As regards QE models, our work represents thefirst investigation on incremental adaptation by ex-ploiting users feedback to provide targeted (sys-tem, user, or project specific) quality judgements.3 Online QE for CAT environmentsWhen operating with advanced CAT tools, transla-tors are presented with suggestions (either match-ing fragments from a translation memory or auto-matic translations produced by an MT system) foreach sentence of a source document.
Before beingapproved and published, translation suggestionsmay require different amounts of post-editing op-erations depending on their quality.4This assumption holds in the WMT evaluation scenario,but it is not necessarily valid in real operating conditions.Each post-edition brings a wealth of dynamicknowledge about the whole translation processand the involved actors.
For instance, adaptive QEcomponents could exploit information about thedistance between automatically assigned scoresand the quality standards of individual translators(inferred from the amount of their corrections) to?profile?
their behaviour.The online learning paradigm fits well with thisresearch objective.
In the online framework, dif-ferently from the batch mode, the learning al-gorithm sequentially processes an unknown se-quence of instances X = x1, x2, ..., xn, returninga prediction p(xi) as output at each step.
Differ-ences between p(xi) and the true label p?
(xi) ob-tained as feedback are used by the learner to refinethe next prediction p(xi+1).In our experiments on adaptive QE we aim topredict the quality of the suggested translationsin terms of HTER, which measures the minimumedit distance between the MT output and its man-ually post-edited version in the [0,1] interval.5Inthis scenario:?
The set of instances X is represented by(source, target) pairs;?
The prediction p(xi) is the automatically es-timated HTER score;?
The true label p?
(xi) is the actual HTER scorecalculated over the target and its post-edition.At each step of the process, the goal of the learneris to exploit user post-editions to reduce the differ-ence between the predicted HTER values and thetrue labels for the following (source, target) pairs.As depicted in Figure 1, this is done as follows:1.
At step i, an unlabelled (source, target) pairxiis sent to a feature extraction component.To this aim, we used an adapted version(Shah et al, 2014) of the open-source QuEst6tool (Specia et al, 2013).
The tool, which im-plements a large number of features proposedby participants in the WMT QE shared tasks,has been modified to process one sentence ata time as requested for integration in a CATenvironment;5Edit distance is calculated as the number of edits (wordinsertions, deletions, substitutions, and shifts) divided by thenumber of words in the reference.
Lower HTER values indi-cate better translations.6http://www.quest.dcs.shef.ac.uk/712Figure 1: Online QE workflow.
<src>, <trg> and <pe> respectively stand for the source sentence, thetarget translation and the post-edited target.2.
The extracted features are sent to an on-line regressor, which returns a QE predictionscore p(xi) in the [0,1] interval (set to 0 at thefirst round of the iteration);3.
Based on the post-edition done by the user,the true HTER label p?
(xi) is calculated bymeans of the TERCpp7open source tool;4.
The true label is sent back to the online al-gorithm for a stepwise model improvement.The updated model is then ready to processthe following instance xi+1.This new paradigm for QE makes it possibleto: i) let the QE system learn from one point ata time without complete re-training from scratch,ii) customize the predictions of an existing QEmodel with respect to a specific situation (post-editor or domain), or even iii) build a QE modelfrom scratch when training data is not available.For the sake of clarity it is worth observing that,at least in principle, a model built in a batch fash-ion could also be adapted to new test data.
For in-stance, this could be done by running periodic re-training routines once a certain amount of new la-belled instances has been collected (de facto mim-icking an online process).
Such periodic updates,however, would not represent a viable solution inthe CAT framework where post-editors?
work can-not be slowed by time-consuming procedures tore-train core system components from scratch.7goo.gl/nkh2rE4 Evaluation frameworkTo measure the adaptation capability of differentQE models, we experiment with a range of condi-tions defined by variable degrees of similarity be-tween training and test data.The degree of similarity depends on several fac-tors: the MT engine used, the domain of the docu-ments to be translated, and the post-editing style ofindividual translators.
In our experiments, the de-gree of similarity is measured in terms of ?HTER,which is computed as the absolute value of the dif-ference between the average HTER of the trainingand test sets.
Large values indicate a low simi-larity between training and test data and a morechallenging scenario for the learning algorithms.4.1 Experimental setupIn the range of possible evaluation scenarios, ourexperiments cover:?
One artificial setting (?5) obtained from theWMT12 QE shared task data, in which train-ing/test instances are arranged to reflect ho-mogeneous distributions of the HTER labels.?
Two settings obtained from data collectedwith a CAT tool in real working condi-tions, in which different facets of the adap-tive QE problem interact with each other.In the first (user change, ?6.1), train-ing and test data from the same domain areobtained from different users.
In the sec-713ond (user+domain change, ?6.2), train-ing and test data are obtained from differentusers and domains.For each setting, we compare an adaptive andan empty model against a system trained in batchmode.
The adaptive model is built on top of anexisting model created from the training data andexploits the new test instances to refine its predic-tions in a stepwise manner.
The empty model onlylearns from the test set, simulating the worst con-dition where training data is not available.
Thebatch model is built by learning only from thetraining data and is evaluated on the test set with-out exploiting information from the test instances.Each model is also compared against a commonbaseline for regression tasks, which is particularlyrelevant in settings featuring different data distri-butions between training and test sets.
This base-line (?
henceforth) is calculated by labelling eachinstance of the test set with the mean HTER scoreof the training set.
Previous works (Rubino et al,2013a) demonstrated that its results can be partic-ularly hard to beat.4.2 Performance indicator and feature setTo measure the adaptability of our model to agiven test set we compute the Mean Absolute Er-ror (MAE), a metric for regression problems alsoused in the WMT QE shared tasks.
The MAE isthe average of the absolute errors ei= |fi?
yi|,where fiis the prediction of the model and yiisthe true value for the ithinstance.As our focus is on the algorithmic aspect, in allexperiments we use the same feature set, whichconsists of the seventeen features proposed in(Specia et al, 2009).
This feature set, fully de-scribed in (Callison-Burch et al, 2012), takes intoaccount the complexity of the source sentence(e.g.
number of tokens, number of translations persource word) and the fluency of the target trans-lation (e.g.
language model probabilities).
Theresults of previous WMT QE shared tasks haveshown that these baseline features are particularlycompetitive in the regression task (with only fewsystems able to beat them at WMT12).4.3 Online algorithmsIn our experiments we evaluate two online algo-rithms, OnlineSVR (Parrella, 2007)8and Passive-8http://www2.imperial.ac.uk/?gmontana/onlinesvr.htmAggressive Perceptron (Crammer et al, 2006),9bycomparing their performance with a batch learningstrategy based on the Scikit-learn implementationof Support Vector Regression (SVR).10The choice of the OnlineSVR and Passive-Aggressive (OSVR and PA henceforth) is moti-vated by different considerations.
From a perfor-mance point of view, as an adaptation of -SVRwhich proved to be one of the top performing algo-rithms in the regression QE tasks at WMT, OSVRseems to be the best candidate.
For this reason,we use the online adaptation of -SVR proposedby (Ma et al, 2003).
The goal of OnlineSVR is tofind a way to add each new sample to one of threesets (support, empty, error) maintaining the con-sistency of a set of conditions known as Karush-Kuhn Tucker (KKT) conditions.
For each newpoint, OSVR starts a cycle where the samples aremoved across the three sets until the KKT condi-tions are verified and the new point is assigned toone of the sets.
If the point is identified as a sup-port vector, the parameters of the model are up-dated.
This allows OSVR to benefit from the pre-diction capability of -SVR in an online setting.From a practical point of view, providing thebest trade off between accuracy and computationaltime (He and Wang, 2012), PA represents a goodsolution to meet the demand of efficiency posedby the CAT framework.
For each instance i, afteremitting a prediction and receiving the true label,PA computes the -insensitive hinge loss function.If its value is larger than the tolerance parameter(), the weights of the model are updated as muchas the aggressiveness parameter C allows.
In con-trast with OSVR, which keeps track of the mostimportant points seen in the past (support vectors),the update of the weights is done without consid-ering the previously processed i-1 instances.
Al-though it makes PA faster than OSVR, this is ariskier strategy because it may lead the algorithmto change the model to adapt to outlier points.5 Experiments with WMT12 dataThe motivations for experiments with training andtest data featuring homogeneous label distribu-tions are twofold.
First, since in this artificial sce-nario adaptation capabilities are not required forthe QE component, batch methods operate in theideal conditions (as training and test are indepen-9https://code.google.com/p/sofia-ml/10http://scikit-learn.org/714WMT DatasetTrain Test?
?
Batch Adaptive EmptyHTER MAE MAE MAE Alg.
MAE Alg.200 754 0.39 13.7 13.2 13.2?OSVR 13.5?OSVR600 754 1.32 13.8 12.7 12.9?OSVR 13.5?OSVR1500 754 1.22 13.8 12.7 12.8?OSVR 13.5?OSVRTable 1: MAE of the best performing batch, adaptive and empty models on WMT12 data.
Training setsof different size and the test set have been arranged to reflect homogeneous label distributions.dent and identically distributed).
This makes pos-sible to obtain from batch models the best possibleperformance to compare with.
Second, this sce-nario provides the fairest conditions for such com-parison because, in principle, online algorithmsare not favoured by the possibility to learn fromthe diversity of the test instances.For our controlled experiments we use theWMT12 English-Spanish corpus, which consistsof 2,254 source-target pairs (1,832 for training,422 for test).
The HTER labels for our regressiontask are calculated from the post-edited versionand the target sentences provided in the dataset.To avoid biases in the label distribution, theWMT12 training and test data have been merged,shuffled, and eventually separated to generatethree training sets of different size (200, 600, and1500 instances), and one test set with 754 in-stances.
For each algorithm, the training sets areused for learning the QE models, optimizing pa-rameters (i.e.
C, , the kernel and its parame-ters for SVR and OSVR; tolerance and aggressive-ness for PA) through grid search in 10-fold cross-validation.Evaluation is carried out by measuring the per-formance of the batch (learning only from thetraining set), the adaptive (learning from the train-ing set and adapting to the test set), and the empty(learning from scratch from the test set) models interms of global MAE scores on the test set.Table 1 reports the results achieved by thebest performing algorithm for each type of model(batch, adaptive, empty).
As can be seen, closeMAE values show a similar behaviour for the threetypes of models.11With the same amount of train-ing data, the performance of the batch and theadaptive models (in this case always obtained withOSVR) is almost identical.
This demonstratesthat, as expected, the online algorithms do not take11Results marked with the ???
symbol are NOT statisti-cally significant compared to the corresponding batch model.The others are always statistically significant at p?0.005, cal-culated with approximate randomization (Yeh, 2000).advantage of test data with a label distribution sim-ilar to the training set.
All the models outper-form the baseline, even if the minimal differencesconfirm the competitiveness of such a simple ap-proach.Overall, these results bring some interesting in-dications about the behaviour of the different on-line algorithms.
First, the good results achievedby the empty models (less than one MAE pointseparates them from the best ones built on thelargest training set) suggest their high potentialwhen training data are not available.
Second,our results show that OSVR is always the bestperforming algorithm for the adaptive and emptymodels.
This suggests a lower capability of PA tolearn from instances similar to the training data.6 Experiments with CAT dataTo experiment with adaptive QE in more realis-tic conditions we used a CAT tool12to collecttwo datasets of (source, target, post edited tar-get) English-Italian tuples.The source sentences inthe datasets come from two documents from dif-ferent domains, respectively legal (L) and infor-mation technology (IT).
The L document, whichwas extracted from a European Parliament resolu-tion published on the EUR-Lex platform,13con-tains 164 sentences.
The IT document, which wastaken from a software user manual, contains 280sentences.
The source sentences were translatedwith two SMT systems built by training the Mosestoolkit (Koehn et al, 2007) on parallel data fromthe two domains (about 2M sentences for IT and1.5M for L).
Post-editions were collected fromeight professional translators (four for each docu-ment) operating with the CAT tool in real workingconditions.According to the way they are created, the twodatasets allow us to evaluate the adaptability ofdifferent QE models with respect to user changes12MateCat ?
http://www.matecat.com/13http://eur-lex.europa.eu/715user changeLegal DomainTrain Test?
?
Batch Adaptive EmptyHTER MAE MAE MAE Alg.
MAE Alg.rad cons 20.5 21.4 20.6 14.5 PA 12.5 OSVRcons rad 19.4 21.2 21.3 16.1 PA 11.3 OSVRsim1 sim2 3.3 14.7 12.2 12.6?OSVR 12.9?OSVRsim2 sim1 3.2 13.4 13.3 13.9?OSVR 15.2?OSVRIT DomainTrain Test?
?
Batch Adaptive EmptyHTER MAE MAE MAE Alg MAE Algcons rad 12.8 19.2 19.8 17.5?OSVR 16.6 OSVRrad cons 9.6 16.8 16.6 15.6 PA 15.5 OSVRsim2 sim1 3.3 14.7 14.4 15?OSVR 15.5?OSVRsim1 sim2 1.1 15 13.9 14.4?OSVR 16.1?OSVRTable 2: MAE of the best performing batch, adaptive and empty models on CAT data collected fromdifferent users in the same domain.within the same domain (?6.1), as well as user anddomain changes at the same time (?6.2).For each document D (L or IT), these two sce-narios are obtained by dividing D into two partsof equal size (80 instances for L and 140 for IT).The result is one training set and one test set foreach post-editor within the same domain.
For theuser change experiments, training and test setsare selected from different post-editors within thesame domain.
For the user+domain changeexperiments, training and test sets are selectedfrom different post-editors in different domains.On each combination of training and test sets,the batch, adaptive, and empty models are trainedand evaluated in terms of global MAE scores onthe test set.6.1 Dealing with user changesAmong the possible combinations of training andtest data from different post-editors in the samedomain, Table 2 refers to two opposite scenarios.For each domain, these respectively involve themost dissimilar and the most similar post-editorsaccording to the ?HTER.
Also in this case, foreach model (batch, adaptive and empty) we onlyreport the MAE of the best performing algorithm.The first scenario defines a challenging situationwhere two post-editors (rad and cons) are charac-terized by opposite behaviour.
As evidenced bythe high ?HTER values, one of them (rad) is themost ?radical?
post-editor (performing more cor-rections) while the other (cons) is the most ?con-servative?
one.
As shown in Table 2, global MAEscores for the online algorithms (both adaptive andempty) indicate their good adaptation capabilities.This is evident from the significant improvementsboth over the baseline (?)
and the batch models.Interestingly, the best results are always achievedby the empty models (with MAE reductions up to10 points when tested on rad in the L domain,and 3.2 points when tested on rad in the IT do-main).
These results (MAE reductions are alwaysstatistically significant) suggest that, when deal-ing with datasets with very different label distri-butions, the evident limitations of batch methodsare more easily overcome by learning from scratchfrom the feedback of a new post-editor.
This alsoholds when the amount of test points to learn fromis limited, as in the L domain where the test setcontains only 80 instances.
From the application-oriented perspective that motivates our work, con-sidering the high costs of acquiring large and rep-resentative QE training data, this is an importantfinding.The second scenario defines a less challeng-ing situation where the two post-editors (sim1 andsim2) are characterized by the most similar be-haviour (small ?HTER).
This scenario is closer tothe situation described in Section ?5.
Also in thiscase MAE results for the adaptive and empty mod-els are slightly worse, but not significantly, thanthose of the batch models and the baseline.
How-ever, considering the very small amount of ?unin-formative?
instances to learn from (especially forthe empty models), these lower results are not sur-prising.A closer look at the behaviour of the online al-gorithms in the two domains leads to other obser-vations.
First, OSVR always outperforms PA forthe empty models and when post-editors have sim-716user+domain changeTrain Test?
?
Batch Adaptive EmptyHTER MAE MAE MAE Alg MAE AlgL cons IT rad 24.5 26.4 27 18.2 OSVR 16.6 OSVRIT rad L cons 24.0 24.9 25.4 19.7 OSVR 12.5 OSVRL rad L cons 20.5 21.4 20.6 14.5 PA 12.5 OSVRL cons L rad 19.4 21.2 21.3 16.1 PA 11.3 OSVRIT cons L cons 13.5 17.3 17.5 15.7 OSVR 12.5 OSVRIT cons IT rad 12.8 19.2 19.8 17.5 OSVR 16.6 OSVRL cons IT cons 12.7 17.6 17.6 15.1 OSVR 15.5 OSVRIT rad IT cons 9.6 16.8 16.6 15.6 PA 15.5 OSVRIT cons L rad 8.3 12.3 13 10.7 OSVR 11.3 OSVRL rad IT rad 6.8 17 16.9 16.2 OSVR 16.6 OSVRL rad IT cons 5.0 15.4 16.2 14.7 OSVR 15.5 OSVRIT rad L rad 2.2 10.6 10.8 10.5 OSVR 11.3 OSVRTable 3: MAE of the best performing batch, adaptive and empty models on CAT data collected fromdifferent users and domains.ilar behaviour, which are situations where the al-gorithm does not have to quickly adapt or react tosudden changes.Second, PA seems to perform better for theadaptive models when the post-editors have sig-nificantly different behaviour and a quick adapta-tion to the incoming points is required.
This canbe motivated by the fact that PA relies on a simplerand less robust learning strategy that does not keeptrack of all the information coming from the previ-ously processed instances, and can easily modifyits weights taking into consideration the last seenpoint (see Section ?3).
For OSVR the addition ofnew points to the support set may have a limitedeffect on the whole model, in particular if the num-ber of points in the set is large.
This also resultsin a different processing time for the two algo-rithms.14For instance, in the empty configurationson IT data, OSVR devotes 6.0 ms per instance toupdate the model, while PA devotes 4.8ms, whichcomes at the cost of lower performance.6.2 Dealing with user and domain changesIn the last round of experiments we evaluate thereactivity of different online models to simultane-ous user and domain changes.
To this aim, ourQE models are created using a training set comingfrom one domain (L or IT), and then used to pre-dict the HTER labels for the test instances comingfrom the other domain (e.g.
training on L, testingon IT).Among the possible combinations of training14Their complexity depends on the number of features (f )and the number of previously seen instances (n).
While forPA it is linear in f, i.e.
O(f), for OSVR it is quadratic in n, i.e.O(n2*f).and test data, Table 3 refers to scenarios involv-ing the most conservative and radical post-editorsin each domain (previously identified with consand rad)15.
In the table, results are ordered ac-cording to the ?HTER computed between the se-lected post-editor in the training domain (e.g.
Lcons) and the selected post-editor in the test do-main (e.g.
IT rad).
For the sake of comparison,we also report (grey rows) the results of the ex-periments within the same domain presented in?6.1.
For each type of model (batch, adaptive andempty) we only show the MAE obtained by thebest performing algorithm.Intuitively, dealing with simultaneous user anddomain changes represents a more challengingproblem compared to the previous setting whereonly post-editors changes were considered.
Suchintuition is confirmed by the results of the adaptivemodels that outperform both the baseline (?)
andthe batch models even for low ?HTER values.
Al-though in these cases the distance between train-ing and test data is comparable to the experimentswith similar post-editors working in the same do-main (sim1 and sim2), here the predictive powerof the batch models seems in fact to be lower.
Thesame holds also for the empty models except intwo cases where the ?HTER is the smallest (2.2and 5.0).
This is a strong evidence of the fact that,in case of domain changes, online models can stilllearn from new test instances even if they have alabel distribution similar to the training set.When the distance between training and test in-creases, our results confirm our previous findings15For brevity, we omit the results for the other post-editorswhich, however, show similar trends with respect to the pre-vious experiments.717about the potential of the empty models.
The ob-served MAE reductions range in fact from 10.4to 12.9 points for the two combinations with thehighest ?HTER.From the algorithmic point of view, our resultsindicate that OSVR achieves the best performancefor all the combinations involving user and domainchanges.
This contrasts with the results of most ofthe combinations involving only user changes withpost-editors characterized by opposite behaviour(grey rows in Table 3).
However, it has to be re-marked that in the case of heterogeneous datasetsthe difference between the two algorithms is al-ways very high.
In our experiments, when PA out-performs OSVR, its MAE results are significantlylower and vice-versa (respectively up to 1.5 and1.7 MAE points).
This suggests that, although PAis potentially capable of achieving higher resultsand better adapt to the new test points, its instabil-ity makes it less reliable for practical use.As a final analysis of our results, we investi-gated how the performance of the different typesof models (batch, adaptive, empty) relates to thedistance between training and test sets.
To thisaim, we computed the Pearson correlation be-tween the ?HTER (column 3 in Table 3) and theMAE of each model (columns 5, 6 and 8), whichrespectively resulted in 0.9 for the batch, 0.63 forthe adaptive and -0.07 for the empty model.
Thesevalues confirm that batch models are heavily af-fected by the dissimilarity between training andtest data: large differences in the label distributionimply higher MAE results and vice-versa.
Thisis in line with our previous findings about batchmodels that, learning only from the training set,cannot leverage possible dissimilarities of the testset.
The lower correlation observed for the adap-tive models also confirms our intuitions: adaptingto the new test points, these models are in factmore robust to differences with the training data.As expected, the results of the empty models arecompletely uncorrelated with the ?HTER sincethey only use the test set.This analysis confirms that, even when dealingwith different domains, the similarity between thetraining and test data is one of the main factors thatshould drive the choice of the QE model.
Whenthis distance is minimal, batch models can be areasonable option, but when the gap between train-ing and test data increases, adaptive or empty mod-els are a preferable choice to achieve good results.7 ConclusionIn the CAT scenario, each translation job can beseen as a complex situation where the user (hispersonal style and background), the source doc-ument (the language and the domain) and the un-derlying technology (the translation memory andthe MT engine that generate translation sugges-tions) contribute to make the task unique.
So far,the adaptability to such specificities (a major chal-lenge for CAT technology) has been mainly sup-ported by the evolution of translation memories,which incrementally store translated segments in-corporating the user style.
The wide adoption oftranslation memories demonstrates the importanceof capitalizing on such information to increasetranslators productivity.While this lesson recently motivated researchon adaptive MT decoders that learn from user cor-rections, nothing has been done to develop adap-tive QE components.
In the first attempt to ad-dress this problem, we proposed the applicationof the online learning protocol to leverage usersfeedback and to tailor QE predictions to their qual-ity standards.
Besides highlighting the limitationsof current batch methods to adapt to user anddomain changes, we performed an application-oriented analysis of different online algorithms fo-cusing on specific aspects relevant to the CAT sce-nario.
Our results show that the wealth of dynamicknowledge brought by user corrections can be ex-ploited to refine in a stepwise fashion the qual-ity judgements in different testing conditions (userchanges as well as simultaneous user and domainchanges).As an additional contribution, to spark furtherresearch on this facet of the QE problem, our adap-tive QE infrastructure (integrating all the compo-nents and the algorithms described in this paper)has been released as open source.
Its C++ im-plementation is available at http://hlt.fbk.eu/technologies/aqet.AcknowledgementsThis work has been partially supported by the EC-funded project MateCat (ICT-2011.4.2-287688).ReferencesDaniel Beck, Kashif Shah, Trevor Cohn, and LuciaSpecia.
2013.
SHEF-Lite: When less is more fortranslation quality estimation.
In Proceedings of the7188thWorkshop on Statistical Machine Translation,Sofia, Bulgaria, August.Nicola Bertoldi, Mauro Cettolo, and Federico Mar-cello.
2013.
Cache-based Online Adaptationfor Machine Translation Enhanced Computer As-sisted Translation.
In Proceedings of the XIV Ma-chine Translation Summit, pages 1147?1162, Nice,France.Ergun Bicici.
2013.
Feature decay algorithms for fastdeployment of accurate statistical machine transla-tion systems.
In Proceedings of the 8thWorkshopon Statistical Machine Translation, Sofia, Bulgaria,August.John Blatz, Erin Fitzgerald, George Foster, SimonaGandrabur, Cyril Goutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2003.
Confidence Es-timation for Machine Translation.
Summer work-shop final report, JHU/CLSP.Ondrej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut, andLucia Specia.
2013.
Findings of the 2013 Workshopon Statistical Machine Translation.
In Proceedingsof the 8thWorkshop on Statistical Machine Transla-tion, WMT-2013, pages 1?44, Sofia, Bulgaria.Chris Callison-Burch, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012 Workshop on Statistical Ma-chine Translation.
In Proceedings of the 7thWork-shop on Statistical Machine Translation (WMT?12),pages 10?51, Montr?eal, Canada.Nicol`o Cesa-Bianchi, Gabriel Reverberi, and SandorSzedmak.
2008.
Online Learning Algorithms forComputer-Assisted Translation.
Deliverable D4.2,SMART: Statistical Multilingual Analysis for Re-trieval and Translation.Trevor Cohn and Lucia Specia.
2013.
ModellingAnnotator Bias with Multi-task Gaussian Processes:An Application to Machine Translation Quality Es-timation.
In Proceedings of the 51stAnnual Meet-ing of the Association for Computational Linguis-tics, ACL-2013, pages 32?42, Sofia, Bulgaria.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
OnlinePassive-Aggressive Algorithms.
J. Mach.
Learn.Res., 7:551?585, December.Jos?e G.C.
de Souza, Christian Buck, Marco Turchi, andMatteo Negri.
2013a.
FBK-UEdin participation tothe WMT13 quality estimation shared task.
In Pro-ceedings of the 8thWorkshop on Statistical MachineTranslation, Sofia, Bulgaria, August.Jos?e G.C.
de Souza, Miquel Espl`a-Gomis, MarcoTurchi, and Matteo Negri.
2013b.
Exploiting Quali-tative Information from Automatic Word Alignmentfor Cross-lingual NLP Tasks.
In Proceedings of the51stAnnual Meeting of the Association for Compu-tational Linguistics - Short Papers, pages 771?776,Sofia, Bulgaria.Christian Hardmeier, Joakim Nivre, and J?org Tiede-mann.
2012.
Tree Kernels for Machine Transla-tion Quality Estimation.
In Proceedings of the Sev-enth Workshop on Statistical Machine Translation(WMT?12), pages 109?113, Montr?eal, Canada.Zhengyan He and Houfeng Wang.
2012.
A Com-parison and Improvement of Online Learning Al-gorithms for Sequence Labeling.
In Proceedingsof the 24th International Conference on Compu-tational Linguistics (COLING 2012), pages 1147?1162, Mumbai, India.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45thAnnual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180.Maarit Koponen, Wilker Aziz, Luciana Ramos, andLucia Specia.
2012.
Post-editing Time as a Mea-sure of Cognitive Effort.
In Proceedings of theAMTA 2012 Workshop on Post-editing Technologyand Practice (WPTP 2012), San Diego, California.Maarit Koponen.
2012.
Comparing Human Percep-tions of Post-editing Effort with Post-editing Op-erations.
In Proceedings of the Seventh Workshopon Statistical Machine Translation, pages 181?190,Montr?eal, Canada.Nick Littlestone.
1988.
Learning Quickly when Irrel-evant Attributes Abound: A New Linear-ThresholdAlgorithm.
In Machine Learning, pages 285?318.Junshui Ma, James Theiler, and Simon Perkins.
2003.Accurate Online Support Vector Regression.
NeuralComputation, 15:2683?2703.Pascual Mart?
?nez-G?omez, Germ?an Sanchis-Trilles, andFrancisco Casacuberta.
2011.
Online Learning viaDynamic Reranking for Computer Assisted Transla-tion.
In Proceedings of the 12th international con-ference on Computational linguistics and intelligenttext processing - Volume Part II, CICLing?11.Pascual Mart?
?nez-G?omez, Germ?an Sanchis-Trilles, andFrancisco Casacuberta.
2012.
Online adaptationstrategies for statistical machine translation in post-editing scenarios.
Pattern Recognition, 45(9):3193?3203, September.Prashant Mathur, Mauro Cettolo, and Marcello Fed-erico.
2013.
Online Learning Approaches in Com-puter Assisted Translation.
In Proceedings of the8thWorkshop on Statistical Machine Translation,Sofia, Bulgaria.719Yashar Mehdad, Matteo Negri, and Marcello Federico.2012.
Match without a Referee: Evaluating MTAdequacy without Reference Translations.
In Pro-ceedings of the 7thWorkshop on Statistical MachineTranslation, pages 171?180, Montr?eal, Canada.Daniel Ortiz-Mart?
?nez, Ismael Garc?
?a-Varea, and Fran-cisco Casacuberta.
2010.
Online learning for in-teractive statistical machine translation.
In HumanLanguage Technologies: The 2010 Annual Confer-ence of the North American Chapter of the Associa-tion for Computational Linguistics, HLT ?10, pages546?554, Stroudsburg, PA, USA.Francesco Parrella.
2007.
Online support vector re-gression.
Master?s Thesis, Department of Informa-tion Science, University of Genoa, Italy.Raphael Rubino, Jos?e G.C.
de Souza, Jennifer Fos-ter, and Lucia Specia.
2013a.
Topic Models forTranslation Quality Estimation for Gisting Purposes.In Proceedings of the Machine Translation SummitXIV, Nice, France.Raphael Rubino, Antonio Toral, S Cort?es Va?
?llo, JunXie, Xiaofeng Wu, Stephen Doherty, and Qun Liu.2013b.
The CNGL-DCU-Prompsit translation sys-tems for WMT13.
In Proceedings of the 8thWork-shop on Statistical Machine Translation, pages 211?216, Sofia, Bulgaria.Kashif Shah, Marco Turchi, and Lucia Specia.
2014.An Efficient and User-friendly Tool for MachineTranslation Quality Estimation.
In Proceedings ofthe 9th International Conference on Language Re-sources and Evaluation, Reykjavik, Iceland.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Trans-lation in the Americas, pages 223?231, Cambridge,Massachusetts, USA.Radu Soricut, Nguyen Bach, and Ziyuan Wang.
2012.The SDL Language Weaver Systems in the WMT12Quality Estimation Shared Task.
In Proceedings ofthe 7thWorkshop on Statistical Machine Translation(WMT?12), pages 145?151, Montr?eal, Canada.Lucia Specia, Nicola Cancedda, Marc Dymetman,Marco Turchi, and Nello Cristianini.
2009.
Estimat-ing the sentence-level quality of machine translationsystems.
In Proceedings of the 13thAnnual Con-ference of the European Association for MachineTranslation (EAMT?09), pages 28?35, Barcelona,Spain.Lucia Specia, Dhwaj Raj, and Marco Turchi.
2010.Machine Translation Evaluation versus Quality Es-timation.
Machine translation, 24(1):39?50.Lucia Specia, Kashif Shah, Jos?e G.C.
de Souza, andTrevor Cohn.
2013.
QuEst - A Translation Qual-ity Estimation Framework.
In Proceedings of the51stAnnual Meeting of the Association for Compu-tational Linguistics: System Demonstrations, ACL-2013, pages 79?84, Sofia, Bulgaria.Marco Turchi and Matteo Negri.
2014.
Automatic An-notation of Machine Translation Datasets with Bi-nary Quality Judgements.
In Proceedings of the 9thInternational Conference on Language Resourcesand Evaluation, Reykjavik, Iceland.Marco Turchi, Matteo Negri, and Marcello Federico.2013.
Coping with the Subjectivity of HumanJudgements in MT Quality Estimation.
In Proceed-ings of the 8thWorkshop on Statistical MachineTranslation, pages 240?251, Sofia, Bulgaria.Alexander Yeh.
2000.
More Accurate Tests for theStatistical Significance of Result Differences.
InProceedings of the 18th conference on Computa-tional linguistics (COLING 2000) - Volume 2, pages947?953, Saarbrucken, Germany.720
