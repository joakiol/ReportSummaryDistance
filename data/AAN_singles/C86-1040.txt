The Ro le  o f  Invers ion  and  PP -F ront ingIn  Re la t ing  D iscourse  E lements :some implications for cognitive and computational models ofNatural Language ProcessingMark  V incent  LaPnl laThe Art i f ic ial  Intel l igence LaboratoryandThe Depar tment  of L inguist icsUniversity of Texas at Austin,Austin, Texas USA11 April 1986O.
Abstract1 This paper will explore and discuss the less obvious wayssyntactic structure is used to convey information and how thisinformation could be used by a natural language databasesystem as a heuristic to organize and search a discourse space.The primary concern of this paper will be to present ageneral theory of processing which capitalizes on theinformation provided by such non-SVO word orders asinversion, (wh) clcfting and prepositional phrase (PP) fronting.since it seems that these non-SVO structures are sensitive toNPs.
Thus every discourse representation is in some wayredundantly specified for at least one constituent.For example, the organization of the discourse representationfor the sentences, "In the forest stood a house" and "In thepark, Mary kissed John", are:Labcl: house, forestD.R.
: a house stood in the forestand1.
In t roduct ionEnglish at its simplest is an SVO, Subject Verb Object,language.
However it is not limited to SVO order.
Clefts,pseudo-clefts, inversion, topicalization, left dislocation andvarious types of fronting are instances of deviation from SVOorder.
Non-SVO orders are not exceptional or found only inobscure literary writing.
They abound throughout writing andspeech.
An interesting question is what use do these structuresdo in English, and how can a cognitive or computationaltheory use such information?Non-SVO word order helps the reader (or listener) toconstruct a discourse representation.
It is a heuristic devise forcreating coherent and cohesive representations of text and forsearching existing representations.
In other words, it is adevice for finding in long or medium term memory therelevant context, or discourse space, in which to embed andinterpret he sentence being processed.
It is a linguistic deviceused for changing the discourse focus (Sidncr 1978, 1983;Grosz 1978, 1981).
It is important o note that no particulardiscourse representation construction schema is assumed inthis claim but rather that this claim holds no matter whattype of construction algorithm is used.
Also, what is beingproposed here is not a linguistic rule for constructing discourserepresentation but rather a principle (heuristic) forconstructing them.
That is a principle for organizing andsearching discourse representations.As a sentence is processed a representation is made of it.This representation consists of a label, which is a (syntacticand semantic) parse 2 of the first constituent cncountercd,followed by a parse of the whole sentence.
Actually, one couldhave multiple labels that consisted of the NPs in the sentence,1This research was supported by the U.S. Army Research Officeunder contract DAAG29-84-K-0060.
Artificial IntelligenceLaboratory, Department of Computer Sciences.I would like to thank Professor Robert F. Simmons for hissupport, help and criticisms.2The exact representation f the text will be discussed later.
Fornow English Will be used to represent what eventually will be adiscourse representation f some sort.
Parse is being used hcre in ageneral sense to mean "give the structure of".
The "structure" ofcourse depends on the linguistic theory used to give the parses.Label: Mary, Johu, parkD.R.
: Mary kissed John in the parkTwo uses could be made of this system of "labels"?
The firstis to simply use the labels to index the text and to facilitatethe search through the text.
The second would be to use thelabels as nodes in a semantic network.
Thus in the discoursefragment, "A house stood in the forest.
Outside stood anangel."
the label "outside "3 could be related to either thelabel "house" or "forest" or both via a "location" arc.Due to tile lack of space the former (index) nsc will be tilefocus of this paper but note that it is indeed difficult toseparate these two uses since it is necessary to locate ttlematerial necessary to construct a discourse space, i.e.
even ifone were only using a very restricted procedure for creatingdiscourse representations one would still need to locateprevious discourse items in order to resolve anaphora, andtherefore create some sort of structured link from discourse todiscourse.In the sections that follow we will give linguistic motivationfor this analysis.
Due to the enormity of the urea and thelimitation of space, only inversion and PP-fronting will bediscussed.
In the final section we will present he findings of astudy done to see if this system of "labels" could be used tospeed the construction of discourse representations, specificallythe resolution of anaphora.2.
Invers ionIn Green's (1980) study of the discourse function of various"classes" of inversion, she assigns each instance of inversion aparticular function.
This approach, however, is bound to bcinadequate.
If one tries to compile a list of such functions forvarious syntactic fornls in a language, how does one know ifthe list is complete?
Every time a new function is discoveredfor a form one must add it to the list.
Another problem withthis approach is that one never knows if one has been specificenough or gcncral enough.
Being too specific in thecharacterization f such functions creates a very long list and3Or rather the relation "outside" that implies outside ofsomething.168can miss generalizations.
Being too general might hide tilecorrect function of a form in a language.
This section willreview and criticize a subset of her list of proposed hmctions.2.1.
Use 1inversion allows the listener first to identify tile object beingtalked about before assigning information to it, whether newor old.
This is most noticeable in the speech of sportscasters(Green; p 584):a. Underneath is Smithb.
IIigh in the, air to get the ball was Jim Brady.c.
Bringing the ball up is Marty Mcstemacher.d.
Back come the Kahoks with the ball.c.
And sitting down is Kcvin ,loses.In this case inversion allows the viewer to single out the playeron the TV screen before processing his name.
This allows theviewer to first pick out the player, makca  non-linguisticrepresentation i memory of that player and then add the(new) information given by the sportscaster: the player'sname.
If inw;rsion were not used the viewer would \[lave tofirst store the namc given by the sportscaster, idcntify theplayer, construct a representation f that play and his actions,retrieve the player's name and then assign that information totile representation created by the description of tile player'sactions.
This is a much more difficult and time consumingprocedure.
In this itmtancc, inversion hclI)s to cut down theamount of processing necessary to construct a representation.Therefore one eoukl hypothesize that upon hearing/readingthe first few elements of a sentence, the listener follows threesteps:1. if the sentence is SVO (and does not have any overtsignals to search for a previously mentioned item ofinformation,) construct a representation f tile sentenceand add it to the local discourse space.2.
Else search memory for the last mention of theitem under construction and add the "new" information,i.e.
what is in the predicate, to that local discoursespace.
Pointers are left pointing to both discoursespaces.3.
Otherwise, construct an entirely new discoursespace.Actually it is nnclear where the information should bedeposited.
For example, a house has been robbed.
The policeinvestigate.
They ~ksk questions concerning tile robbery.
Thenthe father of the household, when asked who they think couldhave done it, says: "As I said before, that boy John is athief.
""\['he police who do not have any previously knowledge ofJohn add this information to tile present discourse space.
Theyprobably note that the person who said it has reasons forbringing it to tile listener's attention that he had said thisbefore, llis daughter, however, who knows John but did notknow her father's opinion of him, adds to her mentalrepresentation labeled "John" this information.
She also addsthis utterance to the cun'ent discourse space, i,e.
"robbery ofhouse".
The man's wife on the other hand already knew abouther husband's opinion.
It would seem redundant for her to addthis information to her mental representation labeled "John".What she probably does is call up tile mental reprcsentation"John" and leave u pointer pointing to it from the currentdiscoursc space "robbery of house", thereby connecting thetwo representations.
So it would seem that things arc not ascut and dried as one would suppose.2.2.
Use 2Tile second use for inversion which Green cites is its abilityto conncct pieces of discourse together.
This is used frequentlyin journalism.
One can link and expand a 1)rcviouslymentioned proposition by anaphorically referencing it ill thegramlnatical subject slot, thereby smoothly linking the newinformation in the predicate to thc old, prcviously mentionedinformation.
She also notes that this connective function isuscd in literary texts or expository prose.
She finds thatinversion provides a concise form in which news writers canbegin a sentencc with old information.Ill the analysis being developed ill this section, tile onlydifference between tile sportscasters' speech and this"connective" function is that the "connecting" in thesportscasters' peech is done to an image rather than a(previously mentioned) linguistic concept.
Ill tim sportscastercase, the viewer ha.s to create a nonlinguistie representation inhis consciousness, i.e.
his discourse space.
Then when tile nameof the player is mentioned the viewer assigns the informationto the representation.
The "connective function" clme differsouly in the type of reprcsent~ation built.2.3.
Use 3in her third function, Green expounds upon the notion thatwhat is invcrted is not ncccssarily new information.
She showsthat inversion can be used to set a scene for an event or as ameans to locate actors in a story, e.g.
"Outside stood anAngel", "Ill a little white house lived two rabbits.
"Notice how similar this "function" is to the sportscasters'speech (and the newspaper examples).
Tile sportscaster sl)ecchuses inversion to identify tile player so that tile listener canmore easily identify the (new) information, i.e.
his name, withthe player.
The scene-setting, and literary connecting, functionof inversion identifies a locale in which to place the actors, thecharacters.
From a processing point of view these are the samethings.
Even Green notes tile similarities between thejonl'nMist hmction and the literary connecting function.13(b) Sprawled in the foreground is George169Price.
4"Ex.
13b, which is part of the description of anaccompanying picture, is very much like the newsexamples: it identifies a piece of the ~ anindividual.L by ~ him with reference to --mor__~especifically,, in the fore ro~ of--something takento be alre%.~_ ~ th__.~e ~ieture as a whole."
(p.588;underlining added)In all these cases a discourse representation can be createdthat uses as its label the first constituent of the inversion.
Inthe sportscasters' speech the representation of the sentence(s)would be linked to the image of the player and the(information) "living of the two rabbits" would be assigned tothe discourse representation labeled "white house".
This lastassignment might seem strange but suppose that the housewas previously identified as being in a large forest:Discourse  i.In a large fo res t  stood a house.In the house l ived two whi te  rabbits .This use of inversion does not seem to signal an extensivesearch of memory but rather seems to create a more localchain of association: there is a sense in which the secondsentence is an elaboration of the stored information about "ahouse".
Compare the short discourse structure above withDiscourse  2.In a large forest  stood ~ house.Two whi te  rabb i ts  l ived in the house.Intuitively, this discourse seems harder to process than theprevious one.
Finally compare these sentences with:Discourse  3.A house stood in a large forest.In the house l ived two whi te  rabbits .This last discourse seems ~ easy to process as Discourse 1(D1).
The theory under development here accounts for this.Discourse 2 (D2) is harder to process than D1 because whenprocessing D2 one must store the concept "two white rabbitslived..", in some manner, and then search for a previousmention of "house" in which to embed the information.
(Notice it is not the inversion that makes "the house"anaphoric but rather the use of the definite article.
5)4Green's numbering.
I will continue to use Green's numbering forher examples.
I will use a more coherent numbering system for myexamples.5I would still like to maintain that inversion is used as a signal ofanaphora.Inversion, in this case, makes the discourse asier to process.Discourse 3 is as easy to process as Discourse 1 because theinverted element, "a house", becomes the label and the"connecting phrase", the label, of the representation of thesecond sentence is "in the house".
An interesting observationis that Discourse 5 seems harder to process than D1, asexpected, but easier than D2.
The explanation for the latterobservation is not at first obvious.Discourse 5.A house stood in a large forest.Two white rabbits lived in the house.In the case of the above discourse (DS), "a house" is thelabel of the last representation built.
So even though theconnecting phrase "in the house" is not in initial position,which accounts for why this discourse is harder to processthan D1, there is a '"top level" item, i.e.
label, "a house", towhich the connecting phrase "in the house" can connect.However, in D2 not only is the connecting phrase "in thehouse" buried in the second sentence, i.e.
not in initialposition, the item to which it must connect is also buried.Extending this reasoning the .theory would predict that D3would be easiest o process, D1 and D4 the next hardest andD2 the hardest.
This claim is a strong claim about the internalstructure of discourse representations and could be falsifiedwith psychological experimentation.The intuitive sense in which a sentence is harder or easier toprocess is perhaps also related to the idea that the subject isan external argument which, participates in a predicaterelationship with the entire VP and not just the verb.
In thissense the object(s) of the verb are more "deeply embedded" inthe sentential predicate than the subject.
THus inversion, PP-fronting, etc., can be viewed as moving an embedded, orinternal argument, to a more external position, e.g.
adjunctposition.Not also that those verbs which appear in inverted sentencesseem to be ergative verbs.
That  is the deep structure of thesentence "Outside the house stood an angle" is probably \[S e\[VP stood an angle\] \[pp outside the house\] 6 (Helm 1085,personal communication).
This might help to explain thegreater "availability" of fronted material.An important point to note is that Green does not considerthe scene setting function and the literary connecting functionto be the same thing.
In the scene setting examples theinverted element is completely new information, where~s inthe literary connecting function this does not have to be thecase.
This is an important point for the theory in this paper aswell.
Crucially, the claim of this theory is not that theinverted element is old information but that it is the170important element with respect o embe~tding of information.When it is new information it sets up a context in which newinformation can be embedded, including the information in itsown predicate.
When it is old information it serves to find thecorrect context in which to embed the information in thepredicate.In all of the above cases, inversion is used to locate andidentify an (old) entity, an event in the sportscaster speech, a(previous) location, or all image, and give more (new)information about that entity, or create a context in which toembed information.3.
PP FrontingPP-fronting is used to provide a continuity, a cohesion, inthe text.
It provides a useful progression of labels to which toattach the accompanying information.
For example, an articleby Lawrence (1985) opens with a fronted prepositional phrasewhich provides a tinm setting, or relation on the (narrative)time line, for the activity in the sentences which follows7:\])iscourse 6.befol'c I was tall enou~ to ride on thecoaster ~ ,  I spent many pleasant hours per-suading my reluctant father to accompany me.
(p. 4)The PP also provides a way to link up the topic (theme) of tilearticle to the opening statement of the article.
The tlmmc isthe "new adult" Amusement Parks.
The article initialprepositional phrase picks out a particular item within anamusement park and associates the remembrances of thewriter to it.The next sentence also has a fronted PP.
This PP also linksthe next sentence to the article's main topic:Discourse 7.As an aficionado f amusement ~12~., 1 was over-joyed when our whole family finally flew toCalifornia to tackle Walt Disncy's extravaganza.
(p.4)The next paragraph starts out witb yet another frontedtemporal prepositional phrase, moving the time setting up tothe present:Discourse 8.More than two decades later, I'm still journeyingto parks.These first few examples of word order deviation all have thecharacteristic of giving the reading a temporal "focus" andorder in a series of events that occur over a number of years.In order for a discourse processor to understaml this text, it6This is roughly the structure.7The nnderlining is minewould have to have a place to start.
The logical starting placewould be with the label AMUSEMENT PARK, since this is inthe title of the piece.
Under this discourse representation labelit could build other representations.
The first representationthat it would build would be about the author since this is thefirst matrix NP of the first sentence, D6.
For the nextsentence, D7, it would already know what type of temporalrelation to assign to the proposition expressed in the matrixclause.
The sentence in the next paragraph is easily processedsince it advances the time of the preceding paragraph.
Ratherthan building a representation f items and attaching to this aset of properties, these fronted PPs build an abstractrepresentation of temporal items related by ttle time of eachitem.The general theme of the article is amusement parks.Ilowevcr, since the opening of the article is more a personalrecollection rather than expository, tile information that needsto be organized is not information about particular objects,i.e.
amusement parks, but rather episodes in the author's life.Each cpisode's temporal relation is specified by the frontedprepositional phrase.
In general this is the function of frontedprepositional phrases, the specification of relations.
This is awidely used tcchnique; used more than any other non-SVOpattern:The general hypothesis is that the first thing that oneencounters in a sentcncc is a link to preceding information,either explicit or implied.
The link provides the proper contextin which to build tile new representation.
It also provides themeans for quickly searching the discourse space.PP-fronting, like inversion, allows the reader to connect thecurrent phrase, or sentence, being processed to an appropriate,and most likely salient, antecedent.
For example Green (1980)comes to the same conclusion about inversions insportscasters' speech.
She notes that sportscasters useinversion when broadcasting play-by-play to identify tileplayer by his action and then name him:~ ~ .and then ~ it was Dave Bonko.Back come the I(ahoks with the ball.An(.._~l in comes nnmber 5.~1, and that will be MikeMatakitis.Into tile ~ for the New Trier West is Brenner.The reason she gives is that this is helpful to tile TV viewers,since they don't have scorecards identifying the players.
Shcgoes on to say that in this way the viewer can single out theplayer on the screen before receiving his name.
(This also givesthe sportscaster time to look up the player's name if nccd be.
)Further more she notes that sportscasters use this invertedstyle even when tile player arc well known or there numbersarc clearly visible.
This observation fits in nicely with themodal being built here.171Thus like inversion PP-fronting is used to help link, vialabels, i.e.
focused material, one discourse representation toanother.4.
The  Exper imentIn the above two sections we briefly motiw~ted anddeveloped an analysis of the organization of discourserepresentations.
Basically the analysis claimed that eachdiscourse representation, o matter how it is represented, i.e.what particular theory or formalism, were indexed via theirfocused NPs.
The analysis also claimed that non-SVO wordstructure was a signal to search through the labels to locatethe structure in which to embed the representation currentlybeing processed.There are two aspects of this analysis that we will focus onin this section: the creating of labels and the searching of thelabels.
The more complicated aspects of building andembedding, or relating, the strnctnrcs to one another will beiguored for the sake of exposition.A simple experiment was performed to explore thecomputational usefulness of the proposed labeling system.Three programs werc written in Symbolics \]?rolog.
Eachprogram processed a set of twenty-six sentences and createddiscourse representations.
To create the discourserepresentations the DRS construction algorithm found iuKamp (1986) was used.
Added to this were straightforwardrules for creating DRSs for locative prepositional phrases.
Thetask for each program was to resolve simple anaphora bysearching through the discourse representations for theantecedent.
A straightforward feature matching technique wasused to do this.
If one were trying to resolve the reference fora pronoun and a full NP then only the features of the lexiealitem, e.g.
masculine, singular, was matched.
If the referencefor a full NP was being resolved then the whole lexical itemwas search for.The first program only constructed discourse representations.it did not construct labels as well.
Thus whenever anaphoricresolution was called for by thc DRS Construction algorithm,this program had to search through thc cntire data basc untila match was found.
The second program created labels butthey were only searched when the sentences being processedhad non-SVO structure.
The third program created labels aswell but it only searched the labels.
That is the heuristicalways applied.Each DRS was a flat list.
Each label list was also flat.
Beforeeach run of the program the machine was cokl booted.The data was a list of 24 sentences.
The l~st sentencecontained the only fronted PP, which referred back to the firstsentence.
The results of this experiment are discussed in thenext section.1725.
Resu l t sIn pilot experiments tile DRS list was allowcd to havecomplex structure, In other words, the DRS list was a lists oflists (of lists, ete).
The label list on the other baad was flat,i.e.
a list of lists.
The processing in the case of the complexstructure was speeded up by a factor of 3, overall, when theheuristic was applied at all times (in the third program).
Inthe case of the second program only the processing of the non-SVO sentences, the last sentence in the corpus were sped up.However in subsequent experiments the DRS lists were allconverted into flat lists.
The effect was less dramatic butsignificant none the less.
Below is given two tables of thefigures for all 3 runs.
The first table is the time each programtook for processing the non-SVO sentence, i.e.
the lastsentence in the corpus and the time it took to process thecorpus overall 8,I Time (secs) I 'rime forTable i J of Non-SV0 \[ Ent i reI Sentence I Corpus.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.First  P rogram I \[No heur is t ics  \] .023 \] .66I I. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Second Program I IHeur is t ic  on \[ .0187 \[ .659Non-SVO \[ Istructure I I. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Third Program \[ \[Heur is t ic  at I .019 \] .54Al l  times.
J I8The times given iu table 1 are rounded to tile nearest thousandthof a second.
Ilowevcr all percentages were computed with originalmicrosecond numbers and then rounded.I g o f  ~peed'r~b\].e 2 I Up as  comparedI to  F?r~t Progra.lnF i r s t  P rogram I Speed up:No l leur i s t ; J .
cs  I O~ on NolI-SVOI Og overa l lSecond Program I Speed up:Ileurl~:lt,:lc on I 19% on Non-sVONolI-SVO I lg oveFa\ ] .
lstructure \]Th i rd  Progra,  i, \[ Speed up:HeurJ.~t:\].c ~t \] 16~ on No~I-.SVi}All timet~ \] 19% overs,  l\].It is obvious from tile above table thaC always searching thelabel list(s) is far superior to searching tile larger discoursestruetm'es.
I~ is also significant that whm, the heuristic(searching ~he label li,'.
;t) is only applied l.o Non-SVOstruetui'es~ i.e.
tile fast senl, enee of  the eo,'pl,s~ t;ll,'~b tilt., spee(1up is significaa,t.
(The difference between the second a,ld thirdprogram wil, h respect to I, hc last sentence, the non-.SVOsentence is not significant and i~; due to nlaehine rel;~t.edfactors.)
The,'efore it seems that it is worthwhile buih\[ing aseparate list of NPs and searching that list; for at; leastresolving a.naphora nd using it not only for the (linguistically)motivated non.4~VO structure search but all the time azs well.6, Conc lus lonWe have mo~ival, ed a heuristic device that consists ofcreating a llst of the NPs in each sentence.
This wa~'~motivated on linguistic grounds for non-SVO word or<let.
Itweal suggested that this list could facilitate in the constructiondiscourse ,'epresenl, ations and for resolving auaphora in acomputer program.
The latter of these two suggestions wasinvestigated.
It was found that iudeed a significant deere~sein processing time occurred.The first of the two above hypothesis was ,lOt empiricallyinvestigated.
One avenue of interesting research would be tosee if the information provided by non-SVO word order couldhelp in the construction of more complicated discourserei>reseatations and if such representations would help in are~mlike Question-Answering.A second avenue of research would be in psycholinguistics.Basically experiments could be set up to test the hypothesisthat non-SVO word order some how signals a search of thediscourse space.ReferencesSiduer, C. l"ocusing and Discourse.
Discourse \]b'occss, Oct-Dec 1983, pp.
10%130.Chafe, Wallace.
Language and Consciousness.
Language,1974, 50, l l  1-133.Chafe, Wallace.
Givenness, Con:restiveness, Definiteness,Subje.cts, Topics, and Point of View.
1,1 Charles l,ited.
), 5',tbject and 7bpic.
New York: Academic Press,1 !t76.Davidson, Alice.
Peculiar Pa~,~sives.
Language, March 1980,Green, Ceorgia M. Some Wherefores of English Inversions.Language, 1980, 51,~3), 582-602.Grosz, B .
.
Focusing and Description in Natural l,anguageI)ialogues.
In Josh:, A.
B. L. Wetter  and 1.
A.
Sag(F, ds.
), l';lements of Di,scmtrse \[Szderstanding.Cambridge: (Sunbridge University Press, 1981.
(\]rosz, t l .
.
Focusing in I)ialog.
In Tinlap-2: Th.corclicall.ssues in Natural Language 15"ocessing.
New York:ACM an<l ACI,, 1978.llavilangd, Susan t';.
and Ilerbert Clark.
WhaL's New?Acquiring new informal;ion im a process incomprehension.
Journal of l/erbal Learning and l/erbalBehavior, 1!t74, 13, 512-.,538.Ih!im, h'ene R. The 5'c.rnantics of Definite and b~definiteNoun Phrases.
Doe~ol'al dissertation, University ofMassachusel, ts at Ainherst, September 1982.Kemp, liens.
Situations in Discourse without Time orQuestions.
CSLI, 1986.
To appear in CSI,I Stalfford.Kemp, Hans.
Context, Thought and Communication.
In 7heProceedings from the Aristotelian ,5'ociety.
, 1985.Lawrence, A. Amusement Parks.
AAA, 1985, .l'rinee, Ellen F. A Comparison of Wh-Clcfts and It-Clefts inDiseonrse.
Language, 1978, Vol.
5"5~(4).Schank, Roger C. Dynamic Memory: a theory of remindingand learning in computers and people, CambridgeUniversity Press, 1982.Sidner, C. l,evels of Complexil, y in Discourse for Anaphoral)isambiguation and Speech Act Interpretation.
1,1Proceeding of the Fifth, International Joint Conferenceon Artificial Intelligence.
IJCAI, 1978.173
