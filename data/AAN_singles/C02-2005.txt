Scaled log likelihood ratios for the detection of abbreviations in textcorporaTibor KissSprachwissenschaftliches InstitutRuhr-Universit?t BochumD-44780 Bochumtibor@linguistics.ruhr-uni-bochum.deJan StrunkSprachwissenschaftliches InstitutRuhr-Universit?t BochumD-44780 Bochumstrunk@linguistics.ruhr-uni-bochum.deAbstractWe describe a language-independent, flexi-ble, and accurate method for the detection ofabbreviations in text corpora.
It is based onthe idea that an abbreviation can be viewedas a collocation, and can be identified by us-ing methods for collocation detection suchas the log likelihood ratio.
Although the loglikelihood ratio is known to show a good re-call, its precision is poor.
We employ scal-ing factors which lead to a strong improve-ment of precision.
Experiments with Englishand German corpora show that abbreviationscan be detected with high accuracy.IntroductionThe detection of abbreviations in a text corpusforms one of the initial steps in tokenization (cf.Liberman/Church 1992).
This is not a trivialtask, since a tokenizer is confronted with am-biguous tokens.
For English, e.g., Palmer/Hearst(1997:241) report that periods (?)
can be used asdecimal points, abbreviation marks, end-of-sentence marks, and as abbreviation marks at theend of a sentence.
In this paper, we will concen-trate on the classification of the period as eitheran abbreviation mark or a punctuation mark.
Weassume that an abbreviation can be viewed as acollocation consisting of the abbreviated worditself and the following ?.
In case of an abbrevia-tion, we expect the occurrence of ?
following theprevious ?word?
to be more likely than in a caseof an end-of-sentence punctation.
The startingpoint is the log likelihood ratio (log ?, Dunning1993).If the null hypothesis (H0) ?
as given in (1) ?expresses that the occurrence of a period is in-dependent of the preceeding word, the alterna-tive hypothesis (HA) in (2) assumes that the oc-currence of a period is not independent of theoccurrence of the word preceeding it.
(1) H0: P(?|w) = p = P(?|?w)(2) HA: P(?|w) = p1 ?
p2 = P(?|?w)The log ?
of the two hypotheses is given in (3).Its distribution is asymptotic to a ?2 distributionand can hence be used as a test statistic (Dun-ning 1993).
(3) ( ) ( )0log    - 2 log  AL HL H??
??
??
??
??
?=1 Problems for an unscaled log ?
approachAlthough log ?
identifies collocations muchbetter than competing approaches (Dunning1993) in terms of its recall, it suffers from itsrelatively poor precision rates.
As is reported inEvert et al (2000), log ?
is very likely to detectall collocations contained in a corpus, but asmore collocations are detected with decreasinglog ?, the number of wrongly classified itemsincreases.
The table in (4) is a sample from theWall Street Journal (1987).1 According to theasymptotic ?2 distribution all the pairs given in(4) count as candidates for abbreviations.
Someof the ?true?
abbreviations are either rankedlower than non-abbreviations or receive thesame log ?
values as non-abbreviations.
Candi-dates which should not be analyzed as abbrevia-tions are indicated in boldface.
(4) Candidates for abbreviations from WSJ1As distributed by ACL/DCI.
We have removed allannotations from the corpora before processing them.
(1987)Candidate C(w, ?)
C(w, ??)
log ?L.F 5 0 29.29N.H 5 0 29.29holiday 7 4 27.02direction  8 8 25.56ounces 4 0 23.43Vt 4 0 23.43debts 7 7 22.36Frankfurt 5 2 21.13U.N 3 0 17.57depositor 3 0 17.57In the present sample, the likelihood of a periodbeing dependent on the word preceeding itshould be 99.99 % if its log ?
is higher than7.88.2 But, as has been illustrated in (4), eventhis figure leads to a problematic classificationof the candidates, since many non-abbreviationsare wrongly classified as being abbreviations.This means that an unmodified log ?
approach tothe detection of abbreviations will produce manyerrors and thus cannot be employed.2 Scaling log likelihood ratiosSince a pure log ?
approach falsely classifiesmany non-abbreviations as being abbreviations,we use log ?
as a basic ranking which is scaledby several factors.
These factors have been ex-perimentally developed by measuring their ef-fect in terms of precision and recall on a trainingcorpus from WSJ.3 The result of the scalingoperation is a much more compact ranking ofthe true positives in the corpus.
The effect of thescaling methods on the data presented in (4) areillustrated in (5).By applying the scaling factors, the asymptoticrelation to the ?2 distribution cannot be retained.The threshold value of the classification is henceno longer determined by the ?2 distribution, butdetermined on the basis of the classificationresults derived from the training corpus.
Thescaling factors, once they have been determinedon the basis of the training corpus, have not beenmodified any further.
In this sense, the methoddescribed here can be characterized as a corpus-filter method, where a given corpus is used to2This is the corresponding ?2 value for a confidencedegree of 99.99 %.3The training corpus had a size of 6 MB.filter the initial results (cf.
Grefenstette1999:128f.).
(5) Result of applying scaling factorsCandidate log ?
S(log ?
)L.F 29.29 216.43N.H 29.29 216.43holiday 27.02 0.03direction  25.56 0.00ounces 23.43 3.17Vt 23.43 173.14debts 22.36 0.00Frankfurt 21.13 0.01U.N 17.57 17.57depositor 17.57 0.04In the present setting, applying the scaling fac-tors to the training corpus has led to to a thresh-old value of 1.0.
Hence, a value above 1.0 al-lows a classification of a given pair as an abbre-viation, while a value below that leads to anexclusion of the candidate.
An ordering of thecandidates from table (5) is given in (6), wherethe threshold is indicated through the dashedline.
(6) Ranking according to S(log ?
)Candidate log ?
S(log ?
)L.F 29.29 216.43N.H 29.29 216.43Vt 23.43 173.14Thurs 29.29 29.29U.N 17.57 17.57ounces 23.43 3.17depositor 17.57 0.04holiday 27.02 0.03Frankfurt 21.13 0.01direction  25.56 0.00debts 22.36 0.00As can be witnessed in (6), the scaling methodsare not perfect.
In particular, ounces is stillwrongly considered as an initial element of anabbreviation, poiting to a weakness of the ap-proach which will be discussed in section 5.3 The scaling factorsWe have employed three different scaling fac-tors, as given in (7), (8), and (9).4 Each scaling4The use of e as a base for scaling factors S1 and S2reflects that log ?
can also be expressed as HA beingelog ?/2 more likely than H0 (cf.
Manning/Sch?tzefactor is applied to the log ?
of a candidate pair.The weighting factors are formulated in such away that allows a tension between them (cf.section 3.4).
The effect of this tension is that anincrease following from one factor may be can-celled out or reduced by a decrease followingfrom the application of another factor, and viceversa.
(7) S1(log ?
): log ?
?
e C(word, ?
)/C(word, ??).
(8) S2(log ?
): log ?
( ) ( )( ) ( ),  - , ,  , ?+ ?iC word C wordC word C word.
(9) S3(log ?
): log ?
?
length of word1e .3.1 Ratio of occurrence: S1By employing scaling factor (7), the log ?
isadditionally weighted by the ranking which isdetermined by the occurrence of pairs of theform (word, ?)
in relation to pairs of the form(word, ??).
If events of the second type are ei-ther rare or at least lower than events of the firsttype, the scaling factor leads to an increase ofthe initial log ?
value.53.2 Relative difference: S2The second scaling factor is a variation of therelative difference.
Depending on the figures ofC(word, ?)
and C(word, ??
), its value can beeither positive, negative, or 0.
(10) If C(word, ?)
> C(word, ??
), 0 < S2 ?
1.
(11) If C(word, ?)
= C(word, ??
), S2 = 0.
(12) If C(word, ?)
< C(word, ??
), ?1 ?
S2 < 0.If C(word, ??)
= 0, S2 reaches a maximum of 1.Hence, S2 in general leads to a reduction of theinitial log ?
value.
S2 also has a significant effecton log ?
if the occurrence of word with ?
equalsthe occurrence of word without ?.
In this case, S2will be 0.
Since the log ?
values are multipliedwith each scaling factor, a value of 0 for S2 willlead to a value of 0 throughout.
Hence the pair(word, ?)
will be excluded from being an abbre-viation.
This move seems extremely plausible: if1999:172f.
).5If C(word, ??)
= 0, S1(log ?)
= log ?
?
eC(word,?
),reflecting an even higher likelihood that the pairshould actually count as an abbreviation.word occurs approximately the same time withand without a following ?, it is quite unlikelythat the pair (word, ?)
forms an abbreviation.6Similarly, the value of S2 will be negative if thenumber of occurrences of word without ?
ishigher than the number of occurrences of wordwith ?.
Again, the resulting decrease reflects thatthe pair (word, ?)
is even more unlikely to be anabbreviation.Both the relative difference (S2) and the ratio ofoccurrence (S1) allow a scaling that abstractsaway from the absolute figure of occurrence,which strongly influences log ?.73.3 Length of abbreviations: S3Scaling factor (9), finally, leads to a reduction oflog ?
depending on the length of the word whichpreceeds a period.
This scaling factor followsthe idea that an abbreviation is more likely to beshort.3.4 Interaction of scaling factorsAs was already mentioned, the scaling factorscan interact with each other.
Consequently, anincrease by a factor may be reduced by anotherone.
This can be illustrated with the pair (U.N,?)
in (6).
The application of the scaling factorsdoes not change the value as the initial log ?calculation.
(13) S1(U.N, ?)
= e3, S2(U.N, ?)
= 1,S3(U.N, ?)
= 31eSince the length of word actually equals itsoccurrence together with a ?, and since U.Nnever occurs without a trailing ?, S1 leads to anincrease by a factor of e3, which however is fullycompensated by the application of S3.6Obviously, this assumption is only valid if the abso-lute number of occurrence is not too small.7As an illustration, consider the pairs (outstanding,?)
and (Adm, ?).
The first pair occurs 260 times in ourtraining corpus, the second one 51 times.
While (out-standing, ??)
occurs 246 times, (Adm, ??)
neveroccurs.
Still, the log ?
value for (outstanding, ?)
is804.34, while the log ?
value for (Adm, ?)
is just289.38, reflecting a bias for absolute numbers ofoccurrence.4 ExperimentsThe scaling methods described in section 3 havebeen applied to test corpora from English (WallStreet Journal, WSJ) and German (Neue Z?rcherZeitung, NZZ).
The scaled log ?
was calculatedfor all pairs of the form (word, ?).
The test cor-pora were annotated in the following fashion: Ifthe value was higher than 1, the tag <A> wasassigned to the pair.
All other candidates weretagged as <S>.8 The automatically classifiedcorpora were compared with their hand-taggedreferences.
(14) Annotation for test corporaTag Interpretation<S> End-of-Sentence<A> Abbreviation<A><S> Abbreviation at end of sentenceWe have chosen two different types of test cor-pora: First, we have used two test corpora of anapproximate size of 2 and 6 MB, respectively.The WSJ corpus contained 19,776 candidates ofthe form (word, ?
); the NZZ corpus contained37,986 such pairs.
Second, we have tried to de-termine the sensitivity of the present approach todata sparseness.
Hence, the approach was ap-plied to ten individual articles from each WSJand NZZ.
For English, these articles containedbetween 7 and 26 candidate pairs, for Germanthe articles comprised between 16 and 52 pairs.The reference annotation allowed the determina-tion of a baseline which determines the percent-age of correctly classified end-of-sentence marksif each pair (word, ?)
is classified as an end-of-sentence mark.9 The baseline varies from corpusto corpus, depending on a variety of factors (cf.Palmer/Hearst 1997).
In the following tables, wehave reported two measures: first, the error rate,which is defined in (15), and second, the Fmeasure (cf.
van Rijsbergen 1979:174), which is8A tokenizer should treat pairs which have beenannotated with <A> as single tokens, while tokenswhich have been annotated with <S> should betreated as two separate tokens.
Three-dot-ellipses arecurrently not considered.
Also <A><S> tags are notconsidered in the experiments (cf.
section 5).9Following this baseline, we assume that correctlyclassified end-of-sentence marks count as true posi-tives in the evaluations.a weighted measure of precision and recall, asdefined in (16).10(15) Error rate11(   )  (   )(  )< > ?
< > + < > ?
< >C A S C S AC all candidates(16) F measure: ( )2+PRR P4.1 Results of first experimentThe results of the classification process for thelarger files are reported in table (17).
F(B) andF(S) are the F measure of the baseline, and thepresent approach, respectively.
E(B) is the errorrate of the baseline, and E(S) is the error rate ofthe scaled log ?
approach.
(17) Results of classification for large filesF(B) F(S) E(B) E(S)WSJ 81.11 99.57 31.78 0.59NZZ 95.05 99.71   9.44 0.29As (17) shows, the application of the scaled log?
leads to significant improvements for bothfiles.
In particular, the error rate has droppedfrom over 30 % to 0.6 % in the WSJ corpus.
Forboth files, the accuracy is beyond 99 %.4.2 Results of second experimentThe results of the second experiment are re-ported in table (18) for the articles from the WallStreet Journal, and in table (19) for the articlesfrom the Neue Z?rcher Zeitung.
The scaled log ?approach generally outperforms the baselineapproach.
This is reflected in the F measure aswell as in the error rate, which is reduced to athird.
For one article (WSJ_1) the present ap-proach actually performs below the baseline (cf.section 5).10Manning/Sch?tze (1999:269) criticize the use ofaccuracy and error if the number of true negatives ?C(<A> ?
<A>) in the present case ?
is large.
Sincethe number of true negatives is small here, accuracyand error escape this criticism.11C(<X> ?
<Y>) is the number of X which havebeen wrongly classified as Y.
In (16), P stands for theprecision, and R for the recall.
(18) Results of classification for single articlesfrom WSJF(B) F(S) E(B) E(S)WSJ_1  88.00    77.78   21.43  28.57WSJ_2  83.87 100.00    27.78    0.00WSJ_3  100.00 100.00   0.00  0.00WSJ_4  81.82  97.30   30.77  3.85WSJ_5  66.67  85.71   50.00  16.67WSJ_6  89.66  96.30   18.75  6.25WSJ_7  100.00  100.00   0.00  0.00WSJ_8  88.00  90.00   21.43  14.29WSJ_9  47.06  72.73   69.23  23.08WSJ_10  83.33  100.00   28.57  0.00?
82.84  91.98   26.80  9.27(19) Results of classification for single articlesfrom NZZF(B) F(S) E(B) E(S)NZZ_1  95.08  100.00   9.38  0.00NZZ_2  93.02  97.56    13.04    4.35NZZ_3  96.00  98.97   7.69  1.92NZZ_4  96.15  100.00   7.41  0.00NZZ_5  93.18  98.80   12.77  2.13NZZ_6  96.84  98.92   6.12  2.04NZZ_7  97.50  97.37   4.88  4.88NZZ_8  89.66  100.00   18.75  0.00NZZ_9  96.97  97.14   5.88  2.86NZZ_10  93.94  99.71   11.43  0.29?
94.83  98.18   9.73  1.82In general, the articles from NZZ containedfewer abbreviations, which is reflected in thecomparatively high baseline scores.
Still, thepresent approach is able to outperform the base-line approach.
Particularly noteworthy are thearticles NZZ_1, NZZ_4, and NZZ_8, where theerror rate is reduced to 0.
In general, the errorrate has been reduced to a fifth.5 Weaknesses and future stepsWe have noted in section 2 that the scaling fac-tors do not lead to a perfect classification.
Thisis particularly reflected in the application ofS(log ?)
to WSJ_1 and NZZ_7, which actuallyshow the same problem: In the training corpus,ounces was always followed by ?.
In WSJ_1, theword said was always followed by ?, and thisalso happened in NZZ_7 for kann.
Without theinclusion of additional metrics, non-abbreviations which exclusively occur at the endof sentences are wrongly classified.
The table in(20) illustrates, however, that the error rate forfalse negatives drops significantly if plausiblecorpus sizes are considered.
(20) False negatives (f.n.)
and corpus size<S> f.n.
= <S> ?
<A> Error %NZZ  34,400  81  0.23WSJ  13,492  56  0.41NZZ_7  39  2  5.12WSJ_1  11  4  36.36We have also ignored abbreviation occuring atthe end of the sentence.
The next step will be tointegrate methods for the detection of abbrevia-tions at the end of the sentence, e.g.
by integrat-ing additional phonotactic information, and alsoto cover the problematic cases reported above.ConclusionWe have presented an accurate and compara-tively simple method for the detection of abbre-viations which makes use of scaled log likeli-hood ratios.
Experiments have shown that themethod works well with large files and also withsmall samples with sparse data.
We expect fur-ther improvements once additional classificationschemata have been integrated.ReferencesDunning T. (1993)  Accurate methods for the statis-tics of surprise and coincidence.
ComputationalLinguistics, 19/1, pp.
61?74.Evert S., U. Heid and W. Lezius (2000) Methodenzum qualitativen Vergleich von Signifikanzma?enzur Kollokationsidentifikation.
ITG Fachbericht161, pp.
215?220.Grefenstette G. (1999) Tokenization.
?SyntacticWordclass Tagging?, H. van Halteren, ed., KluwerAcademic Publishers, pp.
117?133.Liberman M.Y.
and K.W.
Church (1992) Text analy-sis and word pronunciation in text-to-speech syn-thesis.
In ?Advances in Speech Signal Processing?,S.
Furui & M.M.
Sondhi, ed., M. Dekker Inc., pp.791?831.Manning, C.D.
and H. Sch?tze (1999) Foundations ofstatistical natural language processing.
The MITPress, Cambridge/London.Palmer D.D.
and M.A.
Hearst (1997) Adaptive multi-lingual sentence boundary disambiguation.
Com-putational Linguistics, 23/3, pp.
241?267.van Rijsbergen C.J.
(1979) Information Retrieval.Butterworths, London.
