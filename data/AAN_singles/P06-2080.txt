Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 619?626,Sydney, July 2006. c?2006 Association for Computational LinguisticsSemantic parsing with Structured SVM Ensemble Classification ModelsLe-Minh Nguyen, Akira Shimazu, and Xuan-Hieu PhanJapan Advanced Institute of Science and Technology (JAIST)Asahidai 1-1, Nomi, Ishikawa, 923-1292 Japan{nguyenml,shimazu,hieuxuan}@jaist.ac.jpAbstractWe present a learning framework for struc-tured support vector models in whichboosting and bagging methods are used toconstruct ensemble models.
We also pro-pose a selection method which is based ona switching model among a set of outputsof individual classifiers when dealing withnatural language parsing problems.
Theswitching model uses subtrees mined fromthe corpus and a boosting-based algorithmto select the most appropriate output.
Theapplication of the proposed framework onthe domain of semantic parsing shows ad-vantages in comparison with the originallarge margin methods.1 IntroductionNatural language semantic parsing is an interest-ing problem in NLP (Manning and Schutze, 1999)as it would very likely be part of any interestingNLP applications (Allen, 1995).
For example, thenecessary of semantic parsing for most of NLP ap-plication and the ability to map natural language toa formal query or command language is critical fordeveloping more user-friendly interfaces.Recent approaches have focused on using struc-tured prediction for dealing with syntactic parsing(B. Taskar et.
al., 2004) and text chunking prob-lems (Lafferty et al 2001).
For semantic pars-ing, Zettlemoyer and Collins (2005) proposed amethod for mapping a NL sentence to its logicalform by structured classification using a log-linearmodel that represents a distribution over syntac-tic and semantic analyses conditioned on the in-put sentence.
Taskar et al(B. Taskar et.
al.,2004) present a discriminative approach to pars-ing inspired by the large-margin criterion under-lying support vector machines in which the lossfunction is factorized analogous to the decodingprocess.
Tsochantaridis et al(Tsochantaridis etal., 2004) propose a large-margin models based onSVMs for structured prediction (SSVM) in gen-eral and apply it for syntactic parsing problem sothat the models can adapt to overlap features, ker-nels, and any loss functions.Following the successes of the SSVM algorithmto structured prediction, in this paper we exploitthe use of SSVM to the semantic parsing problemby modifying the loss function, feature representa-tion, maximization algorithm in the original algo-rithm for structured outputs (Tsochantaridis et al,2004).Beside that, forming committees or ensemblesof learned systems is known to improve accuracyand bagging and boosting are two popular ensem-ble methods that typically achieve better accuracythan a single classifier (Dietterich, 2000).
Thisleads to employing ensemble learning models forSSVM is worth to investigate.
The first problem offorming an ensemble learning for semantic pars-ing is how to obtain individual parsers with re-spect to the fact that each individual parser per-forms well enough as well as they make differenttypes of errors.
The second one is that of com-bining outputs from individual semantic parsers.The natural way is to use the majority voting strat-egy that the semantic tree with highest frequencyamong the outputs obtained by individual parsersis selected.
However, it is not sure that the ma-jority voting technique is effective for combiningcomplex outputs such as a logical form structure.Thus, a better combination method for semantictree output should be investigated.To deal with these problems, we proposed an619ensemble method which consists of learning andaveraging phases in which the learning phases areeither a boosting or a bagging model, and the av-eraging phase is based on a switching method onoutputs obtained from all individual SSVMs.
Forthe averaging phase, the switching model is usedsubtrees mined from the corpus and a boosting-based algorithm to select the most appropriate out-put.Applications of SSVM ensemble in the seman-tic parsing problem show that the proposed SSVMensemble is better than the SSVM in term of the F-measure score and accuracy measurements.The rest of this paper are organized as follows:Section 2 gives some background about the struc-tured support vector machine model for structuredpredictions and related works.
Section 3 proposesour ensemble method for structured SVMs on thesemantic parsing problem.
Section 4 shows exper-imental results and Section 5 discusses the advan-tage of our methods and describes future works.2 Backgrounds2.1 Related WorksZelle and Mooney initially proposed the empir-ically based method using a corpus of NL sen-tences and their formal representation for learn-ing by inductive logic programming (Zelle, 1996).Several extensions for mapping a NL sentence toits logical form have been addressed by (Tang,2003).
Transforming a natural language sentenceto a logical form was formulated as the task of de-termining a sequence of actions that would trans-form the given input sentence to a logical form(Tang, 2003).
The main problem is how to learn aset of rules from the corpus using the ILP method.The advantage of the ILP method is that we do notneed to design features for learning a set of rulesfrom corpus.
The disadvantage is that it is quitecomplex and slow to acquire parsers for mappingsentences to logical forms.
Kate et alpresenteda method (Kate et al, 2005) that used transfor-mation rules to transform NL sentences to logi-cal forms.
Those transformation rules are learntusing the corpus of sentences and their logicalforms.
This method is much simpler than the ILPmethod, while it can achieve comparable result onthe CLANG (Coach Language) and Query corpus.The transformation based method has the condi-tion that the formal language should be in the formof LR grammar.Ge and Mooney also presented a statisticalmethod (Ge and Mooney, 2005) by merging syn-tactic and semantic information.
Their methodrelaxed the condition in (Kate et al, 2005) andachieved a state-of the art performance on theCLANG and query database corpus.
However thedistinction of this method in comparison with themethod presented in (Kate et al, 2005) is that Geand Mooney require training data to have SAPTs,while the transforation based method only needsthe LR grammar for the formal language.The work proposed by (Zettlemoyer andCollins, 2005) that maps a NL sentence to its log-ical form by structured classification, using a log-linear model that represents a distribution oversyntactic and semantic analyses conditioned onthe input sentence.
This work is quite similar toour work in considering the structured classifica-tion problem.
The difference is that we used thekernel based method instead of a log-linear modelin order to utilize the advantage of handling a verylarge number of features by maximizing the mar-gin in the learning process.2.2 Structured Support Vector ModelsStructured classification is the problem of predict-ing y from x in the case where y has a meaningfulinternal structure.
Elements y ?
Y may be, for in-stance, sequences, strings, labelled trees, lattices,or graphs.The approach we pursue is to learn a dis-criminant function F : X ?
Y ?
R over <input, output > pairs from which we can derivea prediction by maximizing F over the responsevariable for a specific given input x.
Hence, thegeneral form of our hypotheses f isf(x;w) = argmaxy?YF (x; y;w)where w denotes a parameter vector.As the principle of the maximum-margin pre-sented in (Vapnik, 1998), in the structured clas-sification problem, (Tsochantaridis et al, 2004)proposed several maximum-margin optimizationproblems.For convenience, we define?
?i(y) ?
?
(xi, yi)?
?
(xi, y)where (xi,yi) is the training data.The hard-margin optimization problem is:SVM0 : minw12?w?2 (1)620?i,?y ?
Y \yi : ?w, ??i(y)?
> 0 (2)where ?w, ??i(y)?
is the linear combination offeature representation for input and output.The soft-margin criterion was proposed(Tsochantaridis et al, 2004) in order to allowerrors in the training set, by introducing slackvariables.SVM1 : min12?w?2 +Cnn?i=1?i,s.t.
?i, ?i ?
0(3)?i, ?y ?
Y \yi : ?w, ??i(y)?
?
1?
?i (4)Alternatively, using a quadratic term C2n?i?2i topenalize margin violations, we obtained SVM2.Here C > 0 is a constant that control the trade-off between training error minimization and mar-gin maximization.To deal with problems in which |Y | is verylarge, such as semantic parsing, (Tsochantaridis etal., 2004) proposed two approaches that general-ize the formulation SVM0 and SVM1 to the casesof arbitrary loss function.
The first approach is tore-scale the slack variables according to the lossincurred in each of the linear constraints.SVM?s : min???
?w,?12?w?2 +Cnn?i=1?i,s.t.
?i, ?i ?
0(5)?i,?y ?
Y \yi : ?w, ??i(y)?
?1?
?i?
(yi, y)(6)The second approach to include loss function is tore-scale the margin as a special case of the Ham-ming loss.
The margin constraints in this settingtake the following form:?i,?y ?
Y \yi : ?w, ??i(y)?
?
?
(yi, y)?
?i (7)This set of constraints yields an optimization prob-lem, namely SVM?m1 .2.3 Support Vector Machine LearningThe support vector learning algorithm aims to finda small set of active constraints that ensures a suf-ficiently accurate solution.
The detailed algorithm,as presented in (Tsochantaridis et al, 2004) can beapplied to all SVM formulations mentioned above.The only difference between them is the cost func-tion in the following optimization problems:SVM?s1 : H(y) ?
(1?
??
?i(y), w?)?
(yi, y)SVM?s2 : H(y) ?
(1?
??
?i(y), w?)??
(yi, y)SVM?m1 : H(y) ?
(?
(yi, y)?
??
?i(y), w?
)SVM?m2 : H(y) ?
(??
(yi, y)?
??
?i(y), w?
)Typically, the way to apply structured SVM is toimplement feature mapping ?
(x, y), the loss func-tion ?
(yi, y), as well as the maximization algo-rithm.
In the following section, we apply a struc-tured support vector machine to the problem of se-mantic parsing in which the mapping function, themaximization algorithm, and the loss function areintroduced.3 SSVM Ensemble for Semantic ParsingAlthough the bagging and boosting techniqueshave known to be effective for improving theperformance of syntactic parsing (Henderson andBrill, 2000), in this section we focus on our en-semble learning of SSVM for semantic parsingand propose a new effective switching model foreither bagging or boosting model.3.1 SSVM for Semantic ParsingAs discussed in (Tsochantaridis et al, 2004), themajor problem for using the SSVM is to imple-ment the feature mapping ?
(x, y), the loss func-tion ?
(yi, y), as well as the maximization algo-rithm.
For semantic parsing, we describe herethe method of structure representation, the featuremapping, the loss function, and the maximizationalgorithm.3.1.1 Structure representationA tree structure representation incorporatedwith semantic and syntactic information is namedsemantically augmented parse tree (SAPT) (Geand Mooney, 2005).
As defined in (Ge andMooney, 2005), in an SAPT, each internal node inthe parse tree is annotated with a semantic label.Figure 1 shows the SAPT for a simple sentence inthe CLANG domain.
The semantic labels whichare shown after dashes are concepts in the domain.Some concepts refer to predicates and take an or-dered list of arguments.
Concepts such as ?team?and ?unum?
might not have arguments.
A specialsemantic label, ?null?, is used for a node that doesnot correspond to any concept in the domain.3.1.2 Feature mappingFor semantic parsing, we can choose a mappingfunction to get a model that is isomorphic to aprobabilistic grammar in which each rule withinthe grammar consists of both a syntactic rule anda semantic rule.
Each node in a parse tree y for asentence x corresponds to a grammar rule gj witha score wj .621Figure 1: An Example of tree representation inSAPTAll valid parse trees y for a sentence x arescored by the sum of the wj of their nodes, and thefeature mapping ?
(x, y) is a history gram vectorcounting how often each grammar rule gj occursin the tree y.
Note that the grammar rules are lex-icalized.
The example shown in Figure 2 clearlyexplains the way features are mapped from an in-put sentence and a tree structure.3.1.3 Loss functionLet z and zi be two semantic tree outputs and|zi| and |zi| be the number of brackets in z andzi, respectively.
Let n be the number of commonbrackets in the two trees.
The loss function be-tween zi and z is computed as bellow.F ?
loss(zi, z) = 1?2?
n|zi|+ |z|(8)zero?
one(zi, z) ={1 if zi 6= z0 otherwise(9)3.1.4 Maximization algorithmNote that the learning function can be efficientlycomputed by finding a structure y ?
Y that max-imizes F (x, y;w)=?w, ??i(y)?
via a maximiza-tion algorithm.
Typically we used a variant ofFigure 2: Example of feature mapping using treerepresentationCYK maximization algorithm which is similar tothe one for the syntactic parsing problem (John-son,1999).
There are two phases in our maximiza-tion algorithm for semantic parsing.
The first isto use a variant of CYK algorithm to generate aSAPT tree.
The second phase then applies a deter-ministic algorithm to output a logical form.
Thescore of the maximization algorithm is the samewith the obtained value of the CYK algorithm.The procedure of generating a logical form us-ing a SAPT structure originally proposed by (Geand Mooney, 2005) and it is expressed as Algo-rithm 1.
It generates a logical form based on aknowledge database K for given input node N .The predicate argument knowledge, K, specifies,for each predicate, the semantic constraints on itsarguments.
Constraints are specified in terms ofthe concepts that can fill each argument, such asplayer(team, unum) and bowner(player).The GETsemanticHEAD determines which ofnode?s children is its semantic head based on theyhaving matching semantic labels.
For example, inFigure 1N3 is determined to be the semantic headof the sentence since it matches N8?s semantic la-bel.
ComposeMR assigns their meaning represen-tation (MR) to fill the arguments in the head?s MRto construct the complete MR for the node.
Figure1 shows an example of using BuildMR to generatea semantic tree to a logical form.622Input: The root node N of a SAPTPredicate knowledge KNotation: XMR is the MR of node XOutput: NMRBeginCi= the ith child node of NCh= GETsemanticHEAD(N )ChMR =BuildMR(Ch,K)for each other child Ci where i 6= h doCiMR =BuildMR(Ci,K)ComposeMR(ChMR ,CiMR ,K)endNMR=ChMREndAlgorithm 1: BuildMR(N,K): Computing a logicalform form an SAPT(Ge and Mooney, 2005)Input: S = (xi; yi; zi), i = 1, 2, ..., l in which xi is1the sentence and yi, zi is the pair of tree structure andits logical formOutput: SSVM model2repeat3for i = 1 to n do45SVM?s1 : H(y, z) ?
(1?
??
?i(y), w?)?
(zi, z)SVM?s2 : H(y, z) ?
(1?
??
?i(y), w?)??
(zi, z)SVM?m1 : H(y, z) ?
(?
(zi, z)?
??
?i(y), w?
)SVM?m2 : H(y, z) ?
(??
(zi, z)?
??
?i(y), w?
)compute < y?, z?
>= argmaxy,z?Y,Z H(Y,Z);6compute ?i = max{0,maxy,z?Si H(y, z)};7if H(y?, z?)
> ?i + ?
then8Si ?
Si ?
y?, z?
;9solving optimization with SVM;10end11end12until no Si has changed during iteration;13Algorithm 2: Algorithm of SSVM learning for se-mantic parsing.
The algorithm is based on the originalalgorithm(Tsochantaridis et al, 2004)3.1.5 SSVM learning for semantic parsingAs mentioned above, the proposed maximiza-tion algorithm includes two parts: the first is toparse the given input sentence to the SAPT treeand the second part (BuildMR) is to convert theSAPT tree to a logical form.
Here, the scoreof maximization algorithm is the same with thescore to generate a SAPT tree and the loss functionshould be the measurement based on two logicalform outputs.
Algorithm 2 shows our generationof SSVM learning for the semantic parsing prob-lem which the loss function is based on the scoreof two logical form outputs.3.2 SSVM Ensemble for semantic parsingThe structured SVM ensemble consists of a train-ing and a testing phase.
In the training phase, eachindividual SSVM is trained independently by itsown replicated training data set via a bootstrapmethod.
In the testing phase, a test example is ap-plied to all SSVMs simultaneously and a collec-tive decision is obtained based on an aggregationstrategy.3.2.1 Bagging for semantic parsingThe bagging method (Breiman, 1996) is sim-ply created K bootstrap with sampling m itemsfrom the training data of sentences and their logi-cal forms with replacement.
We then applied theSSVM learning in the K generated training datato create K semantic parser.
In the testing phase,a given input sentence is parsed by K semanticparsers and their outputs are applied a switchingmodel to obtain an output for the SSVM ensembleparser.3.2.2 Boosting for semantic parsingThe representative boosting algorithm is theAdaBoost algorithm (Schapire, 1999).
EachSSVM is trained using a different training set.Assuming that we have a training set TR =(xi; yi)|i = 1, 2, ..., l consisting of l samples andeach sample in the TR is assigned to have thesame value of weight p0(xi) = 1/l.
For trainingthe kth SSVM classifier, we build a set of trainingsamplesTRboostk = (xi; yi)|i = 1, 2, .., l?
that is ob-tained by selecting l?
(< l) samples among thewhole data set TR according to the weight valuepk?1(xi) at the (k-1)th iteration.
This trainingsamples is used for training the kth SSVM clas-sifier.
Then, we obtained the updated weight val-ues pk(xi) of the training samples as follows.
Theweight values of the incorrectly classified sam-ples are increased but the weight values of thecorrectly classified samples are decreased.
Thisshows that the samples which are hard to clas-sify are selected more frequently.
These updatedweight values will be used for building the train-ing samples TRboostk+1 = (xi; yi)|i = 1, 2, ..., l?of the (k+1)th SSVM classifier.
The sampling pro-cedure will be repeated until K training samplesset has been built for the Kth SSVM classifier.6233.2.3 The proposed SSVM ensemble modelWe construct a SSVM ensemble model by usingdifferent parameters for each individual SSVM to-gether with bagging and boosting models.
The pa-rameters we used here including the kernel func-tion and the loss function as well as features usedin a SSVM.
Let N and K be the number of dif-ferent parameters and individual semantic parsersin a SSVM ensemble, respectively.
The motiva-tion is to create individual parsers with respect tothe fact that each individual parser performs wellenough as well as they make different types oferrors.
We firstly create N ensemble models us-ing either boosting or bagging models to obtainN?K individual parsers.
We then select the top Tparsers so that their errors on the training data areminimized and in different types.
After formingan ensemble model of SSVMs, we need a processfor aggregating outputs of individual SSVM clas-sifiers.
Intuitively, a simplest way is to use a vot-ing method to select the output of a SSVM ensem-ble.
Instead, we propose a switching method usingsubtrees mining from the set of trees as follows.Let t1, t2, ..., tK be a set of candidate parse treesproduced by an ensemble of K parsers.
From theset of tree t1, t2, ..., tK we generated a set of train-ing data that maps a tree to a label +1 or -1, wherethe tree tj received the label +1 if it is an correctedoutput.
Otherwise tj received the label -1.
Weneed to define a learning function for classifying atree structure to two labels +1 and -1.For this problem, we can apply a boosting tech-nique presented in (Kudo and Matsumoto, 2004).The method is based on a generation of Adaboost(Schapire, 1999) in which subtrees mined from thetraining data are severed as weak decision stumpfunctions.The technique for mining these subtrees is pre-sented in (Zaki, 2002) which is an efficient methodfor mining a large corpus of trees.
Table 1 showsan example of mining subtrees on our corpus.
OneTable 1: Subtrees mined from the corpusFrequency Subtree20 (and(bowner)(bpos))4 (and(bowner)(bpos(right)))4 (bpos(circle(pt(playerour11))))15 (and(bpos)(not(bpos)))8 (and(bpos(penalty-areaour)))problem for using the boosting subtrees algorithm(BT) in our switching models is that we might ob-tain several outputs with label +1.
To solve this,we evaluate a score for each value +1 obtained bythe BT and select the output with the highest score.In the case of there is no tree output received thevalue +1, the output of the first individual semanticparser will be the value of our switching model.4 Experimental ResultsFor the purpose of testing our SSVM ensem-bles on semantic parsing, we used the CLANGcorpus which is the RoboCup Coach Language(www.robocup.org).
In the Coach Competition,teams of agents compete on a simulated soccerfield and receive advice from a team coach ina formal language.
The CLANG consists of 37non-terminal and 133 productions; the corpus forCLANG includes 300 sentences and their struc-tured representation in SAPT (Kate et al, 2005),then the logical form representations were builtfrom the trees.
Table 2 shows the statistic on theCLANG corpus.Table 2: Statistics on CLANG corpus.
The average lengthof an NL sentence in the CLANG corpus is 22.52 words.
Thisindicates that CLANG is the hard corpus.
The average lengthof the MRs is also large in the CLANG corpus.Statistic CLANGNo.of.
Examples 300Avg.
NL sentence length 22.5Avg.
MR length (tokens) 13.42No.
of non-terminals 16No.
of productions 102No.
of unique NL tokens 337Table 3: Training accuracy on CLANG corpusParameter Training Accuracylinear+F-loss(?s) 83.9%polynomial(d=2)+F-loss (?m) 90.1%polynomial(d=2)+F-loss(?s) 98.8%polynomial(d=2)+F-loss(?m) 90.2%RBF+F-loss(?s) 86.3%To create an ensemble learning with SSVM, weused the following parameters with the linear ker-nel, the polynomial kernel, and RBF kernel, re-spectively.
Table 3 shows that they obtained dif-ferent accuracies on the training corpus, and theiraccuracies are good enough to form a SSVM en-semble.
The parameters in Table 3 is used to formour proposed SSVM model.The following is the performance of theSSVM1, the boosting model, the bagging model,and the models with different parameters on the1The SSVM is obtained via http://svmlight.joachims.org/624CLANG corpus2.
Note that the numbers of in-dividual SSVMs in our ensemble models are setto 10 for boosting and bagging, and each individ-ual SSVM can be used the zero-one and F1 lossfunction.
In addition, we also compare the per-formance of the proposed ensemble SSVM mod-els and the conventional ensemble models to as-sert that our models are more effective in formingSSVM ensemble learning.We used the standard 10-fold cross validationtest for evaluating the methods.
To train a BTmodel for the switching phase in each fold test,we separated the training data into 10-folds.
Wekeep 9/10 for forming a SSVM ensemble, and1/10 for producing training data for the switch-ing model.
In addition, we mined a subset ofsubtrees in which a frequency of each subtree isgreater than 2, and used them as weak functionsfor the boosting tree model.
Note that in testingthe whole training data in each fold is formed aSSVM ensemble model to use the BT model esti-mated above for selecting outputs obtained by theSSVM ensemble.To evaluate the proposed methods in parsing NLsentences to logical form, we measured the num-ber of test sentences that produced complete log-ical forms, and the number of those logical formsthat were correct.
For CLANG, a logical form iscorrect if it exactly matches the correct representa-tion, up to reordering of the arguments of commu-tative operators.
We used the evaluation methodpresented in (Kate et al, 2005) as the formula be-low.precision = #correct?representation#completed?representationrecall = #correct?representation#sentencesTable 4 shows the results of SSVM, the SCSIS-SOR system (Ge and Mooney, 2005), and the SILTsystem (Kate et al, 2005) on the CLANG corpus,respectively.
It shows that SCSISSOR obtainedapproximately 89% precision and 72.3% recallwhile on the same corpus our best single SSVMmethod 3 achieved a recall (74.3%) and lower pre-cision (84.2%).
The SILT system achieved ap-proximately 83.9% precision and 51.3% recall 4which is lower than the best single SSVM.2We set N to 5 and K to 6 for the proposed SSVM.3The parameter for SSVM is the polynomial(d=2)+(?s)4Those figures for precision and recall described in(Kate et al, 2005) showed approximately this precision andrecall of their method in this paperTable 4: Experiment results with CLANG corpus.
EachSSVM ensemble consists of 10 individual SSVM.
SSVMbagging and SSVM boosting used the voting method.
P-SSVM boosting and P-SSVM bagging used the switchingmethod (BT) and voting method (VT).System Methods Precision Recall1 SSVM 84.2% 74.3%1 SCSISSOR 89.0% 72.3%1 SILT 83.9% 51.3%10 SSVM Bagging 85.7% 72.4%10 SSVM Boosting 85.7% 72.4%10 P-SSVM Boosting(BT) 88.4% 79.3%10 P-SSVM Bagging(BT) 86.5% 79.3%10 P-SSVM Boosting(VT) 86.5% 75.8%10 P-SSVM Bagging(VT) 84.6% 75.8%Table 4 also shows the performance of Bagging,Boosting, and the proposed SSVM ensemble mod-els with bagging and boosting models.
It is impor-tant to note that the switching model using a boost-ing tree method (BT) to learn the outputs of indi-vidual SSVMs within the SSVM ensemble model.It clearly indicates that our proposed ensem-ble method can enhance the performance of theSSVM model and the proposed methods are moreeffective than the conventional ensemble methodfor SSVM.
This was because the output of eachSSVM is complex (i.e a logical form) so it is notsure that the voting method can select a correctedoutput.
In other words, the boosting tree algo-rithms can utilize subtrees mined from the corpusto estimate the good weight values for subtrees,and then combines them to determine whether ornot a tree is selected.
In our opinion, with theboosting tree algorithm we can have a chance toobtain more accurate outputs.
These results in Ta-ble 4 effectively support for this evidence.Moreover, Table 4 depicts that the proposed en-semble method using different parameters for ei-ther bagging and boosting models can effectivelyimprove the performance of bagging and boost-ing in term of precision and recall.
This was be-cause the accuracy of each individual parser in themodel with different parameters is better than eachone in either the boosting or the bagging model.In addition, when performing SSVM on the testset, we might obtain some ?NULL?
outputs sincethe grammar generated by SSVM could not de-rive this sentence.
Forming a number of individualSSVMs to an ensemble model is the way to handlethis case, but it could make the numbers of com-pleted outputs and corrected outputs increase.
Ta-625ble 4 indicates that the proposed SSVM ensemblemodel obtained 88.4% precision and 79.3% recall.Therefore it shows substantially a better F1 scorein comparison with previous work on the CLANGcorpus.Summarily, our method achieved the best re-call result and a high precision on CLANG corpus.The proposed ensemble models outperformed theoriginal SSVM on CLANG corpus and its perfor-mances also is better than that of the best pub-lished result.5 ConclusionsThis paper presents a structured support vectormachine ensemble model for semantic parsingproblem by employing it on the corpus of sen-tences and their representation in logical form.We also draw a novel SSVM ensemble model inwhich the forming ensemble strategy is based on aselection method on various parameters of SSVM,and the aggregation method is based on a switch-ing model using subtrees mined from the outputsof a SSVM ensemble model.Experimental results show substantially that theproposed ensemble model is better than the con-ventional ensemble models for SSVM.
It can alsoeffectively improve the performance in term ofprecision and recall in comparison with previousworks.AcknowledgmentsThe work on this paper was supported by a Mon-bukagakusho 21st COE Program.ReferencesJ.
Allen.
1995.
Natural Language Understand-ing (2nd Edition).
Mento Park, CA: Benjam-ing/Cumming.L.
Breiman.
1996.
Bagging predictors.
MachineLearning 24, 123-140.T.G.
Dietterich.
2000.
An experimental compari-son of three methods for constructing ensemblesof decision trees: Bagging, boosting, and ran-domization.
Machine Learning 40 (2) 139-158.M.
Johnson 1999.
PCFG models of linguistic treerepresentation.
Computational Linguistics.R.
Ge and R.J. Mooney.
2005.
A Statistical Se-mantic Parser that Integrates Syntax and Seman-ics.
In proceedings of CONLL 2005.J.C.
Henderson and E. Brill 2000.
Bagging andBoosting a Treebank Parser.
In proceedingsANLP 2000: 34-41R.J.
Kate et al 2005.
Learning to Transform Nat-ural to Formal Languages.
Proceedings of AAAI2005, page 825-830.T.
Kudo, Y. Matsumoto.
A Boosting Algorithmfor Classification of Semi-Structured Text.
Inproceeding EMNLP 2004.J.
Lafferty, A. McCallum, and F. Pereira.
2001.Conditional random fields: Probabilistic modelsfor segmenting and labeling sequence data.
InProc.
of ICML 2001.D.C.
Manning and H. Schutze.
1999.
Founda-tion of Statistical Natural Language Processing.Cambridge, MA: MIT Press.L.S.
Zettlemoyer and M. Collins.
2005.
Learn-ing to Map Sentences to Logical Form: Struc-tured Classification with Probabilistic Catego-rial Grammars.
In Proceedings of UAI, pages825?830.I.
Tsochantaridis, T. Hofmann, T. Joachims, andY.
Altun.
2004.
Support Vector Machine Learn-ing for Interdependent and Structured OutputSpaces.
In proceedings ICML 2004.V.
Vapnik.
1995.
The Nature of Statistical Learn-ing Theory.
Springer, N.Y., 1995.L.R.
Tang.
2003.
Integrating Top-down andBottom-up Approaches in Inductive Logic Pro-gramming: Applications in Natural LanguageProcessing and Relation Data Mining.
Ph.D.Dissertation, University of Texas, Austin, TX,2003.B.
Taskar, D. Klein, M. Collins, D. Koller, andC.D.
Manning.
2004.
Max-Margin Parsing.
Inproceedings of EMNLP, 2004.R.E.
Schapire.
1999.
A brief introduction to boost-ing.
Proceedings of IJCAI 99M.J.
Zaki.
2002.
Efficiently Mining FrequentTrees in a Forest.
In proceedings 8th ACMSIGKDD 2002.J.M.
Zelle and R.J. Mooney.
1996.
Learningto parse database queries using inductive logicprogramming.
In Proceedings AAAI-96, 1050-1055.626
