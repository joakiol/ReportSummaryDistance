The Interaction of Knowledge Sourcesin Word Sense DisambiguationMark StevensonUniversity of SheffieldYorick Wilks*University of SheffieldWord sense disambiguation (WSD) is a computational linguistics task likely to benefit from thetradition of combining different knowledge sources in artificial in telligence research.
An importantstep in the exploration of this hypothesis i to determine which linguistic knowledge sources aremost useful and whether their combination leads to improved results.
We present a sense taggerwhich uses several knowledge sources.
Tested accuracy exceeds 94% on our evaluation corpus.Our system attempts to disambiguate all content words in running text rather than limitingitself to treating a restricted vocabulary of words.
It is argued that this approach is more likely toassist the creation of practical systems.1.
IntroductionWord sense disambiguation (WSD) is a problem long recognised in computationallinguistics (Yngve 1955) and there has been a recent resurgence of interest, includinga special issue of this journal devoted to the topic (Ide and V4ronis 1998).
Despite thisthere is still a considerable diversity of methods employed by researchers, as well asdifferences in the definition of the problems to be tackled.
The SENSEVAL evaluationframework (Kilgarriff 1998) was a DARPA-style competition designed to bring someconformity to the field of WSD, although it has yet to achieve that aim completely.
Themain sources of divergence are the choice of computational paradigm, the proportionof text words disambiguated, the granularity of the meanings assigned to them, andthe knowledge sources used.
We will discuss each in turn.Resnik and Yarowsky (1997) noted that, for the most part, part-of-speech tagging istackled using the noisy channel model, although transformation rules and grammatico-statistical methods have also had some success.
There has been far less consensusas to the best approach to WSD.
Currently, machine learning methods (Yarowsky1995; Rigau, Atserias, and Agirre 1997) and combinations of classifiers (McRoy 1992)have been popular.
This paper reports a WSD system employing elements of bothapproaches.Another source of difference in approach is the proportion of the vocabulary dis-ambiguated.
Some researchers have concentrated on producing WSD systems thatbase results on a limited number of words, for example Yarowsky (1995) and Schtitze(1992) who quoted results for 12 words, and a second group, including Leacock, Tow-ell, and Voorhees (1993) and Bruce and Wiebe (1994), who gave results for just one,namely interest.
But limiting the vocabulary on which a system is evaluated can havetwo serious drawbacks.
First, the words used were not chosen by frequency-basedsampling techniques and so we have no way of knowing whether or not they arespecial cases, a point emphasised by Kilgarriff (1997).
Secondly, there is no guarantee?
Department of Computer Science, 211 Regent Court, Portobello Street, Sheffield $1 4DP, UK(~) 2001 Association for Computational LinguisticsComputational Linguistics Volume 27, Number 3that the techniques employed will be applicable when a larger vocabulary is tackled.However it is likely that mark-up for a restricted vocabulary can be carried out morerapidly since the subject has to learn the possible senses of fewer words.Among the researchers mentioned above, one must distinguish between, on theone hand, supervised approaches that are inherently limited in performance to thewords over which they evaluate because of limited training data and, on the otherhand, approaches whose unsupervised learning methodology is applied to only smallnumbers of words for evaluation, but which could in principle have been used to tagall content words in a text.
Others, such as Harley and Glennon (1997) and ourselvesWilks and Stevenson (1998a, 1998b; Stevenson and Wilks 1999), have concentrated onapproaches that disambiguate allcontent words.
1In addition to avoiding the problemsinherent in restricted vocabulary systems, wide coverage systems are more likely tobe useful for NLP applications, as discussed by Wilks et al (1990).A third difference concerns the granularity of WSD attempted, which one canillustrate in terms of the two levels of semantic distinctions found in many dictionaries:homograph and sense (see Section 3.1).
Like Cowie, Guthrie, and Guthrie (1992), weshall give results at both levels, but it is worth pointing out that the targets of, say, workusing translation equivalents (e.g., Brown et al 1991; Gale, Church, and Yarowsky1992c; and see Section 2.3) and Roget categories (Yarowsky 1992; Masterman 1957)correspond broadly to the wider, homograph, distinctions.In this paper we attempt o show that the high level of results more typical ofsystems trained on many instances of a restricted vocabulary can also be obtainedby large vocabulary systems, and that the best results are to be obtained from anoptimization of a combination of types of lexical knowledge (see Section 2).1.1 Lexical Knowledge and WSDSyntactic, semantic, and pragmatic information are all potentially useful for WSD, ascan be demonstrated by considering the following sentences:(1)(2)(3)(4)John did not feel well.John tripped near the well.The bat slept.He bought a bat from the sports shop.The first two sentences contain the ambiguous word well; as an adjective in (1)where it is used in its "state of health" sense, and as a noun in (2), meaning "watersupply".
Since the two usages are different parts of speech they can be disambiguatedby this syntactic property.Sentence (3) contains the word bat, whose nominal readings are ambiguous be-tween the "creature" and "sports equipment" meanings.
Part-of-speech informationcannot disambiguate he senses ince both are nominal usages.
However, this sentencecan be disambiguated using semantic information, such as preference r strictions.
Theverb sleep prefers an animate subject and only the "creature" sense of bat is animate.So Sentence (3) can be effectively disambiguated by its semantic behaviour but not byits syntax.1 In this paper we define content words as nouns, verbs, adjectives, and adverbs, although others haveincluded other part-of-speech categories (Hirst 1995).322Stevenson and Wilks Interaction of Knowledge Sources in WSDA preference restriction will not disambiguate S ntence (4) since the direct objectpreference will be at least as general as physical object, and any restriction on the directobject slot of the verb sell would cover both senses.
The sentence can be disambiguatedon pragmatic grounds because it is far more likely that sports equipment will be boughtin a sports shop.
Thus pragmatic information can be used to disambiguate bat to its"sports equipment" sense.Each of these knowledge sources has been used for WSD and in Section 3 we de-scribe a method which performs rough-grained disambiguation using part-of-speechinformation.
Wilks (1975) describes a system which performs WSD using semanticinformation in the form of preference restrictions.
Lesk (1986) also used semantic in-formation for WSD in the form of textual definitions from dictionaries.
Pragmatic in-formation was used by Yarowsky (1992) whose approach relied upon statistical modelsof categories from Roget's Thesaurus (Chapman, 1977), a resource that had been usedin much earlier approaches to WSD such as Masterman (1957).The remainder of this paper is organised as follows: Section 2 reviews some sys-tems which have combined knowledge sources for WSD.
In Section 3 we discuss therelationship between semantic disambiguation a d part-of-speech tagging, reportingan experiment which quantifies the connection.
A general WSD system is presentedin Section 4.
In Section 5 we explain the strategy used to evaluate this system, and wereport the results in Section 6.2.
BackgroundA comprehensive r view of WSD is beyond the scope of this paper but may befound in Ide and V4ronis (1998).
Combining knowledge sources for WSD is not anew idea; in this section we will review some of the systems which have tried to dothat.2.1 McRoy's SystemEarly work on coarse-grained WSD based on combining knowledge sources was un-dertaken by McRoy (1992).
Her work was carried out without the use of machine-readable dictionaries (MRD), necessitating the manual creation of the complex set oflexicons this system requires.
There was a lexicon of 8,775 unique roots, a hierarchyof 1,000 concepts, and a set of 1,400 collocational patterns.
The collocational patternsare automatically extracted from a corpus of text in the same domain as the text beingdisambiguated and senses are manually assigned to each.
If the collocation occurs inthe text being disambiguated, then it is assumed that the words it contains are beingused in the same senses as were assigned manually.Disambiguation makes use of several knowledge sources: frequency information,syntactic tags, morphological information, semantic ontext (clusters), collocations andword associations, role-related expectations, and selectional restrictions.
The knowl-edge sources are combined by adding their results.
Each knowledge source assigns a(possibly negative) numeric value to each of the possible senses.
The numerical valuedepends upon the type of knowledge source.
Some knowledge sources have only twopossible values, for example the frequency information has one value for frequentsenses and another for infrequent ones.
The numerical values assigned for each weredetermined manually.
The selectional restrictions knowledge source assigns cores inthe range -10 to +10, with higher scores being assigned to senses that are more specific(according to the concept hierarchy).
Disambiguation is carried out by summing thescores from each knowledge source for all candidate senses and choosing the one withthe highest overall score.323Computational Linguistics Volume 27, Number 3In a sample of 25,000 words from the Wall Street Journal, the system covered 98% ofword-occurrences that were not proper nouns and were not abbreviated, emonstrat-ing the impressive coverage of the hand-crafted lexicons.
No quantitative evaluationof the disambiguation quality was carried out due to the difficulty in obtaining an-notated test data, a problem made more acute by the use of a custom-built exicon.In addition, comparison of system output against manually annotated text had yet tobecome a standard evaluation strategy in WSD research.2.2 The Cambridge Language Survey SystemThe Cambridge International Dictionary of English (CIDE) (Procter 1995) is a learners' dic-tionary which consists of definitions written using a 2,000 word controlled vocabulary.
(This lexicon is similar to LDOCE, which we use for experiments presented later in thispaper; it is described in Section 3.1.)
The senses in CIDE are grouped by guidewords,similar to homographs in LDOCE.
It was produced using a large corpus of Englishcreated by the Cambridge Language Survey (CLS).The CLS also produced a semantic tagger (Harley and Glennon 1997), a commer-cial product hat tags words in text with senses from their MRD.
The tagger consistsof four sub-taggers running in parallel, with their results being combined after allhave run.
The first tagger uses collocations derived from the CIDE example sentences.The second examines the subject codes for all words in a particular sentence and thenumber of matches with other words is calculated.
A part-of-speech tagger producedin-house by CUP is run over the text and high scores are assigned to senses thatagree with the syntactic tag assigned.
Finally, the selectional restrictions of verbs andadjectives are examined.
The results of these processes are combined using a simpleweighting scheme (similar to McRoy's; see Section 2.1).
This weighting scheme, in-spired by those used in computer chess programs, assigns each sub-process a weightin the range -100 to +100 before summing.
Unlike McRoy, this approach does not con-sider the specificity of a knowledge source in a particular instance but always assignsthe same overall weight to each.Harley and Glennon report 78% correct agging of all content words at the CIDEguideword level (which they equate to the LDOCE sense level) and 73% at the sub-sense level, as compared to a hand-tagged corpus of 4,000 words.2.3 Machine Learning applied to WSDAn early application of machine learning to the WSD problem was carried out byBrown et al (1991).
Several different disambiguation cues, such as first noun to theleft/right and second word to the left/right, were extracted from parallel text.
Trans-lation differences were used to define the senses, as this approach was used in anEnglish-French machine translation system.
The parallel text effectively provided su-pervised training examples for this algorithm.
Nadas et al (1991) used the flip-flopalgorithm to decide which of the cues was most important for each word by maxi-mizing mutual information scores between words.
Yarowsky (1996) used an extremelyrich features et by expanding this set with syntactic relations uch as subject-verb,verb-object and adjective-noun relations, part-of-speech n-grams and others.
The ap-proach was based on the hypothesis that words exhibited "one sense per collocation"(Yarowsky 1993).
A large corpus was examined to compute the probability of a partic-ular collocate occurring with a certain sense and the discriminatory power of each wascalculated using the log-likelihood ratio.
These ratios were used to create a decisionlist, with the most discriminating collocations being preferred.
This approach as thebenefit hat it does not combine the probabilities of the collocates, which are highlynon-independent knowledge sources.324Stevenson and Wilks Interaction of Knowledge Sources in WSDYarowsky (1993) also examined the discriminatory power of the individual knowl-edge sources.
It was found that each collocation indicated a particular sense with avery high degree of reliability, with the most successful--the first word to the left ofa noun--achieving 99% precision.
Yet collocates have limited applicability; althoughprecise, they can only be applied to a limited number of tokens.
Yarowsky (1995)dealt with this problem largely by producing an unsupervised learning algorithm thatgenerates probabilistic decision list models of word senses from seed collocates.
Thisalgorithm achieves 97% correct disambiguation.
I  these experiments Yarowsky dealsexclusively with binary sense distinctions and evaluates his highly effective algorithmson small samples of word tokens.Ng and Lee (1996) explored an approach to WSD in which a word is assignedthe sense of the most similar example already seen.
They describe this approach as"exemplar-based learning" although it is also known as k-nearest neighbor learning.Their system is known as LEXAS (LEXical Ambiguity-resolving System), a supervisedlearning approach which requires disambiguated training text.
LEXAS was based onPEBLS, a publically available xemplar-based learning algorithm.A set of features is extracted from disambiguated xample sentences, includingpart-of-speech information, morphological form, surrounding words, local collocates,and words in verb-object syntactic relations.
When a new, untagged, usage is encoun-tered, it is compared with each of the training examples and the distance from each iscalculated using a metric adopted from Cost and Salzberg (1993).
This is calculated asthe sum of the differences between each pair of features in the two vectors.
The differ-ences between two values vl and v2 is calculated according to (5), where C1,i representsthe number of training examples with value Vl that are classified with sense i in thetraining corpus, and C1 the number with value vl in any sense .
C2, i and C2 denotesimilar values and n denotes the total number of senses for the word under consider-ation.
The sense of the example with the minimum distance from the untagged usageis chosen: if there is more than one with the same distance, one is chosen at randomto break the tie.Cl,i C2,i I (5)6(Vl, V2) ~- C1 C2i=1Ng and Lee tested LEXAS on two separate data sets: one used previously in WSDresearch, the other a new, manually tagged, corpus.
The common data set was theinterest corpus constructed by Bruce and Wiebe (1994) consisting of 2,639 sentencesfrom the Wall Street Journal, each containing an occurrence of the noun interest.
Eachoccurrence is tagged with one of its six possible senses from LDOCE.
Evaluation iscarried out through 100 random trials, each trained on 1,769 sentences and tested onthe 600 remaining sentences.
The average accuracy was 87.4%, significantly higherthan the figure of 78% reported by Bruce and Wiebe.Further evaluation was carried out on a larger data set constructed by Ng andLee.
This consisted of 192,800 occurrences of the 121 nouns and 70 verbs that are "themost frequently occurring and ambiguous words in English" (Ng and Lee 1996, 44).The corpus was made up from the Brown Corpus (Ku~era nd Francis 1967) and theWall Street Journal Corpus and was tagged with the correct senses from WordNetby university undergraduates specializing in linguistics.
Before training, two subsetsof the corpus were put aside as test sets: the first (B?50) contains 7,119 occurrencesof the ambiguous words from the Brown Corpus, while the second (WSd6) contained14,139 from the Wall Street Journal Corpus.
LEXAS correctly disambiguated 54% ofwords in BCS0 and 68.6% in WSJ6.
Ng and Lee point out that both results are higherthan choosing the first, or most frequent, sense in each of the corpora.
The authors325Computational Linguistics Volume 27, Number 3Table 1Relative contribution of knowledge sources in LEXAS.Knowledge Source AccuracyCollocations 80.2%PoS and Morphology 77.2%Surrounding words 62.0%Verb-object 43.5%attribute the lower performance on the Brown Corpus to the wider variety of texttypes it contains.Ng and Lee attempted to determine the relative contribution of each knowledgesource.
This was carried out by re-running the data from the "interest" corpus throughthe learning algorithm, this time removing all but one set of features.
The results areshown in Table 1.
They found that the local collocations were the most useful knowl-edge source in their system.
However, it must be remembered that this experimentwas carried out on a data set consisting of a single word and may, therefore, not begeneralizable.2.4 DiscussionThis review has been extremely brief and has not covered large areas of research intoWSD.
For example, we have not discussed connectionist approaches, as used by Waltzand Pollack (1985), V6ronis and Ide (1990), Hirst (1987), and Cottrell (1984), However,we have attempted to discuss some of the approaches to combining diverse types oflinguistic knowledge for WSD and have concentrated on those which are related tothe techniques used in our own disambiguation system.Of central interest to our research is the relative contribution of the various knowl-edge sources which have been applied to the WSD problem.
Both Ng and Lee (1996)and Yarowsky (1993) reported some results in the area.
However, Ng and Lee reportedresults for only a single word and Yarowsky considers only words with two possiblesenses.
This paper is an attempt to increase the scope of this research by discussinga disambiguation algorithm which operates over all content words and combines avaried set of linguistic knowledge sources.
In addition, we examine the relative ffectof each knowledge source to gauge which are the most important, and under whatcircumstances.We first report an in-depth study of a particular knowledge source, namely part-of-speech tags.3.
Part of Speech and Word Senses3.1 LDOCEThe experiments described in this section use the Longman Dictionary of ContemporaryEnglish (LDOCE) (Procter 1978).
LDOCE is a learners' dictionary, designed for studentsof English, containing roughly 36,000 word types.
LDOCE was innovative in its useof a defining vocabulary of 2,000 words with which the definitions were written.
Ifa learner of English could master this small core then, it was assumed, they couldunderstand every entry in the dictionary.In LDOCE, the senses for each word type are grouped into homographs: ets ofsenses with related meanings.
For example, one of the homographs of bank means326Stevenson and Wilks Interaction of Knowledge Sources in WSDbank  1 n I land along the side of a river, lake, etc.
2 earth which is heaped up in afield or a garden, often making a border or division 3 a mass of snow, mud, clouds,etc.
: The banks of dark cloud promised a heavy storln 4 a slope made at bends in a road orrace-track, so that they are safer for cars to go round 5 SANDBANK: The Dogger Bankin the North Sea can be dangerous for shipsbank  2 v \[If~\] (of a car or aircraft) to move with one side higher than the other, esp.when making a turn - see also BANK UPbank  3 n 1 a row, esp.
of OARs in an ancient boat or KEYs on a TYPEWRITERbank  4 n I a place where money is kept and paid out on demand, and where relatedactivities go on - see picture at STREET 2 (usu.
in comb.)
a place where something isheld ready for use, esp.
ORGANIC product of human origin for medical use: Hospitalbloodbanks have saved many lives 3 (a person who keeps) a supply of money or piecesfor payment or use in a game of chance 4 break the bank to win all the money thatthe BANK4(3) has in a game of chancebank  5 v 1\[T1\] to put or keep (money) in a bank 2\[L9, esp.
with\] to keep one's money(esp.
in the stated bank): Where do you bank?Figure 1The entry for bank in LDOCE (slightly simplified for clarity).roughly "things piled up", with different senses distinguishing exactly what is piled(see Figure 1).
If the senses are sufficiently close together in meaning there will beonly one homograph for that word, which we then call monohomographic.
However, ifthe senses are far enough apart, as in the bank case, they will be grouped into separatehomographs,  which we call polyhomographic.As can be seen from the example ntry, each LDOCE homograph includes informa-tion about the part of speech with which the homograph is marked and that appliesto each of the senses within that homograph.
The vast majority of homographs inLDOCE are marked with a single part of speech; however, about 2% of word types inthe dictionary contain a homograph that is marked with more than one part of speech(e.g., noun or verb), meaning that either part of speech may apply.Although the granularity of the distinction between homographs in LDOCE israther coarse-grained, they are, as we noted at the beginning of this paper, an appro-priate level for many practical computational linguistic applications.
For example, bankin the sense of "financial institution" translates to banque in French, but when usedin the "edge of r iver" sense it translates as bord.
This level of semantic disambigua-tion is frequently sufficient for choosing the correct arget word in an English-to-FrenchMachine Translation system and is at a similar level of granularity to the sense distinc-tions explored by other researchers in WSD, for example Brown et al (1991), Yarowsky(1996), and McRoy (1992) (see Section 2).327Computational Linguistics Volume 27, Number 33.2 Using Part-of-Speech Information to Resolve SensesWe began by examining the potential usefulness of part-of-speech information forsense resolution.
It was found that 34% of the content-word types in LDOCE werepolysemous, and 12% polyhomographic.
(Polyhomographic words are necessarily pol-ysemous ince each homograph is a non-empty set of senses.)
If we assume that thepart of speech of each polyhomographic word in context is known, then 88% of wordtypes would be disambiguated to the homograph level.
(In other words, 88% do nothave two homographs with the same part of speech.)
Some words will be disam-biguated to the homograph level if they are used in a certain part of speech but notothers.
For example, beam has 3 homographs in LDOCE; the first two are marked asnouns while the third is marked as verb.
This word would be disambiguated if usedas a verb but not if used as a noun.
If we assume that every word of this type isassigned a part of speech which disambiguates it (i.e., verb in the case of beam), thenan additional 7% of words in LDOCE could, potentially, be disambiguated.
Therefore,up to 95% of word types in LDOCE can be disambiguated to the homograph levelby part-of-speech information alone.
However, these figures do not take into accounteither errors in part-of-speech tagging or the corpus distribution of tokens, since eachword type is counted exactly once.The next stage in our analysis was to attempt o disambiguate some texts us-ing the information obtained from part-of-speech tags.
We took five articles from theWall Street Journal, containing 391 polyhomographic content words.
These articles weremanually tagged with the most appropriate LDOCE homograph by one of the authors.The texts were then part-of-speech tagged using Brill's transformation-based l arningtagger (Brill, 1995).
The tags assigned by the Brill tagger were manually mapped ontothe simpler part-of-speech tag set used in LDOCE.
2 If a word has more than one ho-mograph with the same part of speech, then part-of-speech tags alone cannot alwaysidentify a single homograph; in such cases we chose the first sense listed in LDOCEsince this is the one which occurs most frequently.
3It was found that 87.4% of the polyhomographic content words were assignedthe correct homograph.
A baseline for this task can be calculated by computing thenumber of tokens that would be correctly disambiguated if the first homograph foreach was chosen regardless of part of speech.
78% of polyhomographic tokens werecorrectly disambiguated this way using this approach.These results show there is a clear advantage to be gained (over 42% reduction inerror rate) by using the very simple part-of-speech-based method described comparedwith simply choosing the first homograph.
However, we felt that it would be useful tocarry out some further analysis of the data.
To do this, it is useful to divide the polyho-mographic words into four classes, all based on the assumption that a part-of-speechtagger has been run over the text and that homographs which do not correspond tothe grammatical category assigned have been removed.Full disambiguation (by part of speech): If only a single homograph with thecorrect part of speech remains, that word has been fully disambiguatedby the tagger.2 The Brill tagger uses the 48-tag set from the Penn Tree Bank (Marcus, Santorini, and Marcinkiewicz1993), while LDOCE uses a set of 17 more general tags.
Brill's tagger has a reported error rate ofaround 3%, although we found that mapp ing  the Penn TreeBank tags used by Brill onto the simplerLDOCE tag set led to a lower error rate.3 In the 3rd Edition of LDOCE the publ ishers claim that the senses are indeed ordered by frequency,a l though they make no such claim in the 1st Edition used here.
However, Guo (1989) found evidencethat there is a correspondence b tween the order in which senses are listed and the frequency ofoccurrence in the 1st Edition.328Stevenson and Wilks Interaction of Knowledge Sources in WSDPartial disambiguation (by part of speech): If there is more than one possible ho-mograph with the correct part of speech but some have been removedfrom consideration, that word has been partially disambiguated by partof speech.No disambiguation (by part of speech): If all the homographs of a word havethe same part of speech, which is then assigned by the tagger, then nonecan be removed and no disambiguation has been carried out.Part-of-speech error: It is possible for the part-of-speech tagger to assign an incor-rect part of speech, leading to the correct homograph being removed fromconsideration.
It is worth mentioning that this situation has two possibleoutcomes: first, some homographs, with incorrect parts of speech, mayremain; or second, all homographs may have been removed from consid-eration.In Table 3 we show in the column labelled Count the number of words in ourfive articles which fall into each of the four categories.
The relative performance ofthe baseline method (choosing the first sense) compared to the reported algorithm(removing homographs using part-of-speech tags) are shown in the rightmost twocolumns.
The figures in brackets indicate the percentage of polyhomographic wordscorrectly disambiguated by each method on a per-class basis.
It can be seen that themajority of the polyhomographic words (297 of 342) fall into the "Full disambiguation"category, all of which are correctly disambiguated by the method reported here.
Whenno disambiguation is carried out, the algorithm described simply chooses the firstsense and so the results are the same for both methods.
The only condition underwhich choosing the first sense is more effective than using part-of-speech informationis when the part-of-speech tagger makes an error and all the homographs with thecorrect part of speech are removed from consideration.
In most cases this means thatthe correct homograph cannot be chosen; however, in a small number of cases, this isequivalent to choosing the most frequent sense, since if all possible homographs havebeen removed from consideration, the algorithm reverts to using the simpler heuristicof choosing the word's first homograph.
4Although this result may seem intuitively obvious, there have, we believe, been noother attempts to quantify the benefit o be gained from the application of a part-of-speech tagger in WSD (see Wilks and Stevenson 1998a).
The method escribed here iseffective in removing incorrect senses from consideration, thereby reducing the searchspace if combined with other WSD methods.In the experiments reported in this section we made use of the particular struc-ture of LDOCE, which assigns each sense to a homograph from which its part ofspeech information is inherited.
However, there is no reason to believe that the methodreported here is limited to lexicons with this structure.
In fact this approach canbe applied to any lexicon which assigns part-of-speech information to senses, al-though it would not always be possible to evaluate at the homograph level as wedo here.In the remainder of this paper we go on to describe a sense tagger that assignssenses from LDOCE using a combination of classifiers.
The set of senses consideredby the classifiers is first filtered using part-of-speech tags.4 An example ofthis situation is shown in the bottom row of Table 2.329Computational Linguistics Volume 27, Number 3Table 2Examples of the four word types introduced in Section 3.2.
The leftmost column indicates thefull set of homographs for the example words, with upper case indicating the correcthomograph.
The remaining columns how (respectively) the part-of-speech assigned by thetagger, the resulting set of senses after filtering, and the type of the word.All PoS After Word typeHomographs Tag taggingN, v, v n N Full disambiguationn, adj, V v V Full disambiguationn, V, v v V, v Partial disambiguationn, N, v n n, N Partial disambiguationN, n n N, n No disambiguationv, V v v, V No disambiguationN, v, v v v v PoS errorN, v, v adj N, v, v PoS errorTable 3Error analysis for the experiment on WSD by part of speech alone.Correctly disambiguated by:Word Type Count Baseline method PoS methodFull disambiguation 297 268 (90%) 297 (100%)Partial disambiguation 58 22 (38%) 32 (55%)No disambiguation 23 10 (43%) 10 (43%)Part-of-speech error 13 5 (38%) 3 (23%)All polyhomographic 391 305 (78%) 342 (87%)4.
A Sense Tagger which Combines Knowledge SourcesWe adopt a f ramework in which different knowledge sources are appl ied as separatemodules.
One type of module,  a filter, can be used to remove senses from considerationwhen a knowledge source identifies them as unlikely in context.
Another type can beused when a knowledge source provides evidence for a sense but cannot identifyit confidently; we call these partial taggers (in the spirit of McCarthy's notion of"partial information" \[McCarthy and Hayes, 1969\]).
The choice of whether to apply aknowledge source as either a filter or a partial tagger depends on whether it is likely torule out correct senses.
If a knowledge source is unlikely to reject the correct sense, thenit can be safely implemented as a filter; otherwise implementat ion as a partial taggerwould be more appropriate.
In addition, it is necessary to represent he context ofambiguous words so that this information can be used in the disambiguation process.In the system described here these modules are referred to as feature extractors.Our sense tagger is implemented within this modular  architecture, one whereeach module is a filter, partial tagger, or feature extractor.
The architecture of thesystem is represented in Figure 2.
This system currently incorporates a single fil-ter (par t -o f - speech  f i l te r ) ,  three partial taggers (s imulated anneal ing,  sub jec tcodes, se lec t iona l  res t r i c t ions )  and a single feature extractor (co l locat ion  ex-t rac tor ) .330Stevenson and Wilks Interaction of Knowledge Sources in WSD.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
p. .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.Figure 2Sense tagger architecture.331Computational Linguistics Volume 27, Number 34.1 PreprocessingBefore the filters or partial taggers are applied, the text is tokenized, lemmatized,split into sentences, and part-of-speech tagged, again using Brill's tagger.
A namedentity identifier is then run over the text to mark and categorize proper names, whichwill provide information for the selectional restrictions partial tagger (see Section 4.4).These preprocessing stages are carried out by modules from Sheffield University'sInformation Extraction system, LaSIE, and are described in more detail by Gaizauskaset al (1996).Our system disambiguates only the content words in the text, and the part-of-speech tags are used to decide which are content words.
There is no attempt to dis-ambiguate any of the words identified as part of a named entity.
These are excludedbecause they have already been analyzed semantically by means of the classificationadded by the named entity identifier (see Section 4.4).
Another eason for not attempt-ing WSD on named entities is that when words are used as names they are not beingused in any of the senses listed in a dictionary.
For example, Rose and May are namesbut there are no senses in LDOCE for this usage.
It may be possible to create a dummyentry in the set of LDOCE senses indicating that the word is being used as a name,but then the sense tagger would simply repeat work carried out by the named entityidentifier.4.2 Part-of-Speech filteringWe take the part-of-speech tags assigned by the Brill tagger and use a manually createdmapping to translate these to the corresponding LDOCE grammatical category (seeSection 3.2).
Any senses which do not correspond to the category returned are removedfrom consideration.
I  practice, the filtering is carried out at the same time as the lexicallookup phase and the senses whose grammatical categories do not correspond to thetag assigned are never attached to the ambiguous word.
There is also an option ofturning off filtering so that all senses are attached regardless of the part-of-speech tag.If none of the dictionary senses for a given word agree with the part-of-speech tagthen all are kept.It could be reasonably argued that removing senses is a dangerous strategy since,if the part-of-speech tagger made an error, the correct sense could be removed fromconsideration.
However, the experiments described in Section 3.2 indicate that part-of-speech information isunlikely to reject he correct sense and can be safely implementedas a filter.4.3 Optimizing Dictionary Definition OverlapLesk (1986) proposed that WSD could be carried out using an overlap count of contentwords in dictionary definitions as a measure of semantic loseness.
This method wouldtag all content words in a sentence with their senses from a dictionary that containstextual definitions.
However, it was found that the computations which would benecessary to test every combination of senses, even for a sentence of modest length,was prohibitive.The approach was made practical by Cowie, Guthrie, and Guthrie (1992) (seealso (Wilks, Slator, and Guthrie 1996)).
Rather than computing the overlap for allpossible combinations ofsenses, an approximate solution is identified by the simulatedannealing optimization algorithm (Metropolis et al 1953).
Although this algorithm isnot guaranteed to find the global solution to an optimization problem, it has beenshown to find solutions that are not significantly different from the optimal one (Presset al 1988).
Cowie et al used LDOCE for their implementation a d found it correctlydisambiguated 47% of words to the sense level and 72% to the homograph level332Stevenson and Wilks Interaction of Knowledge Sources in WSDZ(no semantic restriction)T, W, X, Y, 2, 4 ~(abstract)I,W ?
~ ~ Q ' Y ' 5(animate)S,E, 1,2,5 L,E, 6,7 G, 7 PV '~A,  O/,V H~OO,(solid) (liquid) (gas) (plant) (ani~{al) (~umaXn~\] N B,R D~K M,K F,R(movable (nonmovable (animal (ammal (human (humansolid) solid) male) female) male) female)Figure 3Bruce and Guthrie's hierarchy of LDOCE semantic odes.when compared with manually assigned senses.
The optimization must be carried outrelative to a function that evaluates the suitability of a particular choice of senses.
Inthe Cowie et al implementation this was done using a simple count of the numberof words (tokens) in common between all the definitions for a given choice of senses.However, this method prefers longer definitions, since they have more words thatcan contribute to the overlap, and short definitions or definitions by synonym arecorrespondingly penalized.
We addressed this problem by computing the overlap in adifferent way: instead of each word contributing one, we normalized its contributionby the number of words in the definition it came from.
In their implementation Cowieet al also added pragmatic odes to the overlap computation; however, we prefer tokeep different knowledge sources eparate and use this information in another partialtagger (see Section 4.5).
The Cowie et al implementation returned one sense for eachambiguous word in the sentence without any indication of the system's confidencein its choice, but we adapted the system to return a set of suggested senses for eachambiguous word in the sentence.4.4 Selectional PreferencesOur next partial tagger eturns the set of senses for each word that is licensed byselectional preferences (in the sense of Wilks 1975).
LDOCE senses are marked withselectional restrictions expressed by 36 semantic odes not ordered in a hierarchy.However, the codes are clearly not of equal evels of generality; for example, the code His used to represent all humans, while M represents human males.
Thus for a restrictionwith type H, we would want to allow words with the more specific semantic lass M tomeet it.
This can be computed if the semantic ategories are organized into a hierarchy.Then all categories subsumed by another category will be regarded as satisfying therestriction.
Bruce and Guthrie (1992) manually identified relations between the LDOCEsemantic lasses, grouping the codes into small sets with roughly the same meaningand attached escriptions; for example M, K are grouped as a pair described as "humanmale".
The hierarchy produced is shown in Figure 3.333Computational Linguistics Volume 27, Number 3Table 4Mapping of named entities onto LDOCE semantic odes.
The named entities can be mappedto any semantic ode within a particular node of the hierarchy since the disambiguationalgorithm treats all codes in the same node as equivalent.Named Entity Type LDOCE codePERSON H (= Human)ORGANIZATION T (= Abstract)LOCATION N (= Non-movable solid)DATE T (---- Abstract)TIME T (= Abstract)MONEY T (= Abstract)PERCENT T (---- Abstract)UNKNOWN Z (---- No  semantic restriction)The named entities identified as part of the preprocessing phase (Section 4.1) areused by this module, which requires first a mapping between the name types andLDOCE semantic odes, shown in Table 4.Any use of preferences for sense selection requires prior identification of the sitein the sentence where such a relationship holds.
Although prior identification was notdone by syntactic methods in Wilks (1975), it is often easiest o think of the relation-ships as specified in grammatical terms, e.g., as subject-verb, verb-object, adjective-noun etc.
We perform this step by means of a shallow syntactic analyzer (Stevenson1998) which finds the following grammatical relations: the subject, direct and indirectobject of each verb (if any), and the noun modified by an adjective.
Stevenson (1998)describes an evaluation of this system in which the relations identified were comparedwith those derived from Penn TreeBank parses (Marcus, Santorini, and Marcinkiewicz1993).
It was found that the parser achieved 51% precision and 69% recall.The preference resolution algorithm begins by examining a verb and the nounsit dominates.
Each sense of the verb applies a preference to those nouns such thatsome of their senses may be disallowed.
Some verb senses will disallow all senses fora particular noun it dominates and these senses of the verb are immediately rejected.This process leaves us with a set of verb senses that do not conflict with the nounsthat verb governs, and a set of noun senses licensed by at least one of those verbsenses.
For each noun, we then check whether it is modified by an adjective.
If it is,we reject any senses of the adjectives which do not agree with any of the remainingnoun senses.
This approach is rather conservative in that it does not reject a senseunless it is impossible for it to fit into the preference pattern of the sentence.In order to explain this process more fully we provide a walk-through explanationof the procedure applied to a toy example shown in Table 5.
It is assumed that thenamed-entity identifier has correctly identified John as a person and that the shallowparser has found the correct syntactic relations.
In order to make this example asstraightforward aspossible, we consider only the case in which the ambiguous wordshave few senses.
The disambiguation process operates by considering the relationsbetween the words in known grammatical relations, and before it begins we haveessentially a set of possible senses for each word related via their syntax.
This situationis represented by the topmost ree in Figure 4.Disambiguation is carried out by considering each verb sense in turn, beginningwith run(l).
As run is being used transitively, it places two restrictions on the sentence:first, the subject must satisfy the restriction human and the object abstract.
In this334Stevenson and Wilks Interaction of Knowledge Sources in WSDTable 5Sentence and lexicon for toy example of selectional preference resolution algorithm.Example sentence:John ran the hilly course.Sense Definition and Example RestrictionJohnran (1)ran (2)hilly (1)course (1)course (2)proper nameto control an organisation run IBMto move quickly by foot run a marathonundulating terrain hilly roadroute race courseprogramme of study physics coursetype:humansubject:human object:abstractsubject:human object:inanimatemodifies:nonmovable sol idtype:noumovable solidtype:abstractrun(l)restriction:human restriction:abstractJohn course(2){ run(1 ),run(2) }~ b j e c t - ~ b  IJohn { course(1),course(2) }fI adjective-noun~I{hilly(l)}run(2)restriction:human restriction:inanimateJohn course(I)type:nonmovable solidhilly(l)Figure 4Restriction resolution in toy example.example, John has been identified as a named entity and marked as human, so thesubject restriction is not broken.
Note that, if the restriction were broken, then theverb sense run(l) would be marked as incorrect by this partial tagger and no furtherattempt would be made to resolve its restrictions.
As this was not the case, we considerthe direct-object slot, which places the restriction abst rac t  on the noun which fills it.course(2) fulfils this criterion, course is modif ied by hilly which expects a noun of typenoumovable so l id .
However,  course(2) is marked abst rac t ,  which does not complywith this restriction.
Therefore, assuming that run is being used in its second senseleads to a situation in which there is no set of senses which comply with all therestrictions placed on them; therefore run(l) is not the correct sense of run and thepartial tagger marks this sense as wrong.
This situation is represented by the tree atthe bottom left of Figure 4.
The sense course(2) is not rejected at this point since it maybe found to be acceptable in the configuration of senses of another sense of run.The algorithm now assumes that run(2) is the correct sense.
This implies thatcourse(I) is the correct sense as it complies with the inanimate restriction that that verbsense places on the direct object.
As well as complying with the restriction imposedby run(2), course(I) also complies with the one imposed by hilly(i), since nonmovableso l id  is subsumed by inanimate.
Therefore, assuming that the senses run(2) and335Computational Linguistics Volume 27, Number 3course(I) are being used does not lead to any restrictions being broken and the algo-rithm marks these as correct.Before leaving this example it is worth discussing a few additional points.
Thesense course(2) is marked as incorrect because there is no sense of run with which aninterpretation of the sentence can be constructed using course(2).
If there were furthersenses of run in our example, and course(2) was found to be suitable for those extrasenses, then the algorithm would mark the second sense of course as correct.
There is,however, no condition under which run(l) could be considered as correct hrough theconsideration of further verb senses.
Also, although John and hilly are not ambiguous inthis example, they still participate in the disambiguation process.
In fact they are vitalto its success, as the correct senses could not have been identified without consideringthe restrictions placed by the adjective hilly.This partial tagger eturns, for all ambiguous noun, verb, and adjective occurrencesin the text, the set of senses which satisfy the preferences imposed on those words.Adverbs do not have any selectional preferences in LDOCE and so are ignored by thispartial tagger.4.5 Subject CodesOur final partial tagger is a re-implementation f the algorithm developed by Yarowsky(1992).
This algorithm is dependent upon a categorization of words in the lexiconinto subject areas--Yarowsky used the Roget large categories.
In LDOCE, primarypragmatic odes indicate the general topic of a text in which a sense is likely to beused.
For example, LN means "Linguistics and Grammar" and this code is assignedto some senses of words such as "ellipsis", "ablative", "bilingual" and "intransitive".Roget is a thesaurus, o each entry in the lexicon belongs to one of the large categories;but over half (56%) of the senses in LDOCE are not assigned a primary code.
Wetherefore created a dummy category, denoted by --,  used to indicate a sense whichis not associated with any specific subject area and this category is assigned to allsenses without a primary pragmatic ode.
These differences between the structuresof LDOCE and Roget meant that we had to adapt the original algorithm reported inYarowsky (1992).In Yarowsky's implementation, the correct subject category is estimated by apply-ing (6), which maximizes the sum of a Bayesian term (the fraction on the right) overall possible subject categories (SCat) for the ambiguous word over the words in itscontext (w).
A context of 50 words on either side of the ambiguous word is used.ARGMAX Pr( w\[ S Cat) Pr( SCat)scat ~ log Pr(w) (6)w e contextYarowsky assumed the prior probability of each subject category to be constant,so the value Pr(SCat) has no effect on the maximization in (6), and (7) was in effectbeing maximized.ARCMAX Pr (w\]SCat)SCat ~ log Pr(w) (7)w e contextBy including a general pragmatic ode to deal with the lack of coverage, we createdan extremely skewed distribution of codes across senses and Yarowsky's assumptionthat subject codes occur with equal probability is unlikely to be useful in this ap-plication.
We gained a rough estimate of the probability of each subject category bydetermining the proportion of senses in LDOCE to which it was assigned and apply-ing the maximum likelihood estimate.
It was found that results improved when the336Stevenson and Wilks Interaction of Knowledge Sources in WSDrough estimate of the likelihood of pragmatic odes was used.
This procedure gener-ates estimates based on counts of types and it is possible that this estimate could beimproved by counting tokens, although the problem of polysemy in the training datawould have to be overcome in some way.The algorithm relies upon the calculation of probabilities gained from corpus tatis-tics: Yarowsky used the Grolier's Encyclopaedia, which comprised a 10 million wordcorpus.
Our implementation used nearly 14 million words from the non-dialogueportion of the British National Corpus (Burnard 1995).
Yarowsky used smoothing pro-cedures to compensate for data sparseness in the training corpus (detailed in Gale,Church, and Yarowsky \[1992b\]), which we did not implement.
Instead, we attemptedto avoid this problem by considering only words which appeared at least 10 timesin the training contexts of a particular word.
A context model is created for eachpragmatic ode by examining 50 words on either side of any word in the corpus con-taining a sense marked with that code.
Disambiguation is carried out by examining thesame 100 word context window for an ambiguous word and comparing it against hemodels for each of its possible categories.
Further details may be found in Yarowsky(1992).Yarowsky reports 92% correct disambiguation ver 12 test words, with an averageof three possible Roget large categories.
However, LDOCE has a higher level of aver-age ambiguity and does not contain as complete a thesaural hierarchy as Roget, so wewould not expect such good results when the algorithm is adapted to LDOCE.
Con-sequently, we implemented the approach as a partial tagger.
The algorithm identifiesthe most likely pragmatic ode and returns the set of senses which are marked withthat code.
In LDOCE, several senses of a word may be marked with the same prag-matic code, so this partial tagger may return more than one sense for an ambiguousword.4.6 Collocation ExtractorThe final disambiguation module is the only feature-extractor in our system and isbased on collocations.
A set of 10 collocates are extracted for each ambiguous wordin the text: first word to the left, first word to the right, second word to the left,second word to the right, first noun to the left, first noun to the right, first verb tothe left, first verb to the right, first adjective to the left, and first adjective to theright.
Some of these types of collocation were also used by Brown et al (1991) andYarowsky (1993) (see Section 2.3).
All collocates are searched for within the sentencewhich contains the ambiguous word.
If some particular collocation does not exist foran ambiguous word, for example if it is the first or last word in a sentence, then anull value (NoColl) is stored instead.
Rather than storing the surface form of the co-occurrence, morphological roots are stored instead, as this allows for a smaller set ofcollocations, helping to cope with data sparseness.
The surface form of the ambiguousword is also extracted from the text and stored.
The extracted collocations and surfaceform combine to represent the context of each ambiguous word.4.7 Combining Disambiguation ModulesThe results from the disambiguation modules (filter, partial taggers, and feature x-tractor) are then presented to a machine learning algorithm to combine their results.The algorithm we chose was the TIMBL memory-based learning algorithm (Daelemanset al 1999).
Memory-based learning is another name for exemplar-based learning, asemployed by Ng and Lee (Section 2.3).
The TiMBL algorithm has already been used forvarious NLP tasks including part-of-speech tagging and PP-attachment (Daelemans etal.
1996; Zavrel, Daelemans, and Veenstra 1997).337Computational Linguistics Volume 27, Number 3Like PEBLS, which formed the core of Ng and Lee's LEXAS system, TiMBL classifiesnew examples by comparing them against previously seen cases.
The class of the mostsimilar example is assigned.
At the heart of this approach is the distance metric A(X, Y)which computes the similarity between instances X and Y.
This measure is calculatedusing the weighted overlap metric shown in (8), which calculates the total distance bycomputing the sum of the distance between each position in the feature vector.nA(X, Y) =- ~_, wi6(xi, yi) (8)i=1where:xl-yi if numeric, else ~ axi-  min~ ?5(xi, y i )  = i f  Xi = y i  (9)if xi # yiFrom (9) we can see that TiMBL treats numeric and symbolic features differently.For numeric features, the unweighted distance is computed as the difference betweenthe values for that feature in each instance, divided by the maximum possible dis-tance computed over all pairs of instances in the database.
5 For symbolic features, theunweighted istance is 0 if they are identical, and 1 otherwise.
For both numeric andsymbolic features, this distance is multiplied by the weight for the particular feature,based on the Gain Ratio measure introduced by Quinlan (1993).
This is a measure ofthe difference in uncertainty between the situations with and without knowledge ofthe value of that feature, as in (10).H(C) - ~-,v Pr(v) x H(CIv) (10)wi = H(v)Where C is the set of classifications, v ranges over all values of the feature i andH(C) is the entropy of the class labels.
Probabilities are estimated from frequencyof occurrence in the training data.
The numerator of this formula determines theknowledge about the distribution of classes that is added by knowing the value offeature i.
However, this measure can overestimate he value of features with largenumbers of possible values.
To compensate, it is divided by H(v), the entropy of thefeature values.Word senses are presented to TiMBL in a feature-vector representation, with eachsense which was not removed by the part of speech filter being represented by aseparate vector.
The vectors are formed from the following pieces of information inorder: headword, homograph number, sense number, rank of sense (the order of thesense in the lexicon), part of speech from lexicon, output from the three partial tag-gers (simulated annealing, subject codes, and selectional restrictions), sur-face form of headword from the text, the ten collocates, and an indicator of whetherthe sense is appropriate or not in the context (correct or incorrect).Figure 5 shows the feature vectors generated for the word influence in the contextshown.
The final value in the feature vector shows whether the sense is correct ornot in the particular context.
We can see that, in this case, there is one correct sense,influence_l_la, the definition of which is "power to gain an effect on the mind of5 An earlier version of this system (Stevenson and Wilks 1999) used TiMBL version 1.0 (Daelemans et al1998), which supports only symbolic features.338Stevenson and Wilks Interaction of Knowledge Sources in WSDContextRegard ing At lanta's new mil l ion dollar airport, the jury recommended "that when the new management  takecharge Jan. 1 the airport be operated in a manner  that will  el iminate political influences".Feature VectorsLearning features Truthinfluence 1 la 1 n influences 1 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate correctinfluence 1 lb 2 n influences 0 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrectinfluence 1 2 3 n influences 0 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrectinfluence 1 3 4 n influences 0 12.03 y NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrectinfluence 1 4 5 n influences 0 12.03 n NoColl manner NoColl eliminate NoCofl in NoColl political NoColl eliminate incorrectinfluence 1 5 6 n influences 0 12.03 n NoColl manner NoColl eliminate NoColl in NoCon political NoColl eliminate incorrectinfluence 1 6 7 n influences 0 12.03 n NoColl manner NoColl eliminate NoColl in NoColl political NoColl eliminate incorrectFigure 5Example feature-vector representation.or get results from, without asking or doing anything".
Features 10-19 are producedby the collocation extractor, and these are identical since each vector is taken fromthe same content.
Features 7-9 show the results of the partial taggers.
The first is theoutput from simulated annealing, the second the subject  code, and the third these lect iona l  res t r i c t ions .
All noun senses of influence share the same pragmaticcode (--), and consequently this partial tagger eturns the same score for each sense.A final point worth noting is that in LDOCE, influence has a verb sense which thepart-of-speech filter removed from consideration, and consequently this sense is notincluded in the feature-vector representation.The TiMBL algorithm is trained on tokens presented in this format.
When disam-biguating unannotated text, the algorithm is applied to data presented in the sameformat without the classification.
The unclassified vectors are then compared with allthe training examples, and it is assigned the class of the closest one.5.
Evaluation Strategy5.1 Evaluation CorpusThe evaluation of WSD algorithms has recently become a much-studied area.
Gale,Church, and Yarowsky (1992a), Resnik and Yarowsky (1997), and Melamed and Resnik(2000) each presented arguments for adopting various evaluation strategies, withResnik and Yarowsky's proposal directly influencing the set-up of SENSEVAL (Kil-garriff 1998).
At the heart of their proposals is the ability of human subjects to markup text with the phenomenon i question (WSD in this case) and evaluate the resultsof computation.
This linguistic phenomenon has proved to be far more elusive andcomplex than many others.
We have discussed this at length elsewhere (Wilks 1997)and will assume here that humans can mark up text for senses to a sufficient degree.Kilgarriff (1993) questioned the possibility of creating sense-tagged texts, claiming thetask to be impossible.
However, it should be borne in mind that no alternative hasyet been widely accepted and that Kilgarriff himself used the markup-and-test modelfor SENSEVAL.
In the following discussion we compare the evaluation methodologyadopted here with those proposed by others.339Computational Linguistics Volume 27, Number 3The standard evaluation procedure for WSD is to compare the output of the sys-tem against gold standard texts, but these are very labor-intensive to obtain; lexicalsemantic markup is generally considered to be a more difficult and time-consumingtask than part-of-speech markup (Fellbaum et al 1998).
Rather than expend a vastamount of effort on manual tagging we decided to combine two existing resources:SEMCOR (Landes, Leacock, and Tengi 1998), and SENSUS (Knight and Luk 1994).SEMCOR is a 200,000 word corpus with the content words manually tagged as partof the WordNet project.
The semantic tagging was carried out by trained lexicogra-phers under disciplined conditions that attempted to keep tagging inconsistencies toa minimum.
SENSUS is a large-scale ontology designed for machine-translation a dwas itself produced by merging the ontological hierarchies of WordNet, LDOCE (asderived by Bruce and Guthrie, see Section 4.4), and the Penman Upper Model (Bate-man et al, 1990) from ISI.
To facilitate the merging of these three resources to produceSENSUS, Knight and Luk were required to derive a mapping between the senses in thetwo lexical resources.
We used this mapping to translate the WordNet-tagged contentwords in SEMCOR to LDOCE tags.The mapping of senses is not one-to-one, and some WordNet synsets are mappedonto two or three LDOCE senses when WordNet does not distinguish between them.The mapping also contained significant gaps, chiefly words and senses not in thetranslation scheme.
SEMCOR contains 91,808 words tagged with WordNet synsets,6,071 of which are proper names, which we ignored, leaving 85,737 words whichcould potentially be translated.
The translation contains only 36,869 words taggedwith LDOCE senses; however, this is a reasonable size for an evaluation corpus for thetask, and it is several orders of magnitude larger than those used by other researchersworking in large vocabulary WSD, for example Cowie, Guthrie, and Guthrie (1992),Harley and Glennon (1997), and Mahesh et al (1997).
This corpus was also constructedwithout the excessive cost of additional hand-tagging and does not introduce any ofthe inconsistencies that can occur with a poorly controlled tagging strategy.Resnik and Yarowsky (1997) proposed to evaluate large vocabulary WSD systemsby choosing a set of test words and providing annotated test and training examplesfor just these words, allowing supervised and unsupervised algorithms to be testedon the same vocabulary.
This model was implemented in SENSEVAL (Kilgarriff 1998).However, for the evaluation of the system presented here, there would have beenno benefit from using this strategy since it still involves the manual tagging of largeamounts of data and this effort could be used to create a gold standard corpus inwhich all content words are disambiguated.
It is possible that some computationaltechniques may evaluate well over a small vocabulary but may not work for a largeset of words, and the evaluation strategy proposed by Resnik and Yarowsky will notdiscriminate between these cases.In our evaluation corpus, the most frequent ambiguous type is have, which appears604 times.
A large number of words (2407) occur only once, and nearly 95% have 25occurrences or less.
Table 6 shows the distribution of ambiguous types by number ofcorpus tokens.
It is worth noting that, as would be expected, the observed istributionis highly Zipfian (Zipf 1935).Differences in evaluation corpora makes comparison difficult.
However, some ideaof the difficulty of WSD can be gained by calculating properties of the evaluation cor-pus.
Gale, Church, and Yarowsky (1992a) suggest hat the lowest level of performancewhich can be reasonably expected from a WSD system is that achieved by assigningthe most likely sense in all cases.
Since the first sense in LDOCE is usually the mostfrequent, we calculate this baseline figure using a heuristic which assumes the firstsense is always correct.
This is the same baseline heuristic we used for the experiments340Stevenson and Wilks Interaction of Knowledge Sources in WSDTable 6Occurrence of ambiguous words in the evaluation corpus.Occurrence Range Count1-25 5488 (94.6%)26-50 202 (3.5%)51-75 67 (1.2%)76-100 21 (0.04%)100-604 26 (0.4%)reported in Section 3, although those were for the homograph level.
We applied thenaive heuristic of always choosing the first sense in our corpus and found that 30.9%of senses were correctly disambiguated.Another measure that gives insight into an evaluation corpus is to count the av-erage polysemy, i.e., the number of possible senses we can expect for each ambiguousword in the corpus.
The average polysemy is calculated by counting the sum of pos-sible senses for each ambiguous token and dividing by the number of tokens.
This isrepresented by (11), where w ranges over all ambiguous tokens in the corpus, S(w) isthe number of possible senses for word w, and N is the number of ambiguous tokens.The average polysemy for our evaluation corpus is 14.62.Average polysemy = ~w in text S( w) (11)NOur annotated corpus has the unusual property that more than one sense maybe marked as correct for a particular token.
This is an unavoidable side-effect of amapping between lexicon senses which is not one-to-one.
However, it does not implythat WSD is easier in this corpus than one in which only a single sense is markedfor each token, as can be shown from an imaginary example.
The worst case for aWSD algorithm is when each of the possible semantic tags for a given word occurswith equal frequency in a corpus, and so the prior probabilities exhibit a uniform,uninformative distribution.
Then a corpus with an average polysemy of 5, and 2 sensesmarked correct on each ambiguous token, will have a baseline not less than 40%.However, one with an average polysemy of 2, and only a single sense on each, willhave a baseline of at least 50%.
Test corpora in which each ambiguous token hasexactly two senses were used by Brown et al (1991), Yarowsky (1995) and others.Our system was tested using a technique known as 10-fold cross validation.
Thisprocess is carried out by splitting the available data into ten roughly equal subsets.One of the subsets is chosen as the test data and the TiMBL algorithm is trained on theremainder.
This is repeated ten times, so that each subset is used as test data exactlyonce, and results are averaged across all of the test runs.
This technique provides twoadvantages: first, the best use can be made of the available data, and secondly, thecomputed results are more statistically reliable than those obtained by simply settingaside a single portion of the data for testing.5.2 Evaluation MetricsThe choice of scoring metric is an important one in the evaluation of WSD algorithms.The most commonly used metric is the ratio of words for which the system has as-signed the correct sense compared to those which it attempted todisambiguate.
Resnikand Yarowsky (1997) dubbed this the exact match metric, which is usually expressed341Computational Linguistics Volume 27, Number 3as a percentage calculated according to the formula in (12).Exact match = Number of correctly assigned senses x 100% (12)Number of senses assignedResnik and Yarowsky criticize this metric because it assumes a WSD system com-mits to a particular sense.
They propose an alternative metric based on cross-entropythat compares the probabilities for each sense as assigned by a WSD system againstthose in the gold standard text.
The formula in (13) shows the method for computingthis metric, where the WSD system has processed N words and Pr(csi) is the proba-bility assigned to the correct sense of word i.N 1N ~ l?g2 Pr(csi) (13)i=1This evaluation metric may be useful for disambiguation systems that assign probabil-ities to each sense, such as those developed by Resnik and Yarowsky, since it providesmore information than the exact match metric.
However, for systems which simplychoose a single sense and do not measure confidence, it provides far less information.When a WSD assigns only one sense to a word and that sense is incorrect, hat word isscored as ~.
Consequently, the formula in (13) returns c~ if there is at least one wordin the test set for which the tagger assigns a zero probability to the correct sense.
ForWSD systems which assign exactly one sense to each word, this metric returns 0 ifall words are tagged correctly, and cx~ otherwise.
This metric is potentially very usefulfor the evaluation of WSD systems that return non-zero probabilities for each possiblesense; however, it is not useful for the metric presented in this paper and others thatare not based on probabilistic models.Melamed and Resnik (2000) propose a metric for scoring WSD output when theremay be more than one correct sense in the gold standard text, as with the evaluationcorpus we use.
They mention that when a WSD system returns more than one senseit is difficult to tell if they are intended to be disjunctive or conjunctive.
The scorefor a token is computed by dividing the number of correct senses identified by thealgorithm by the total it returns, making the metric equivalent to precision in infor-mation retrieval (van Rijsbergen 1979).
6For systems which return exactly one sensefor each word, this equates to scoring a token as 1 if the sense returned is correct, and0 otherwise.
For the evaluation of the system presented here, the metric proposed byMelamed and Resnik is then equivalent to the exact match metric.The exact match metric has the advantage of being widely used in the WSD lit-erature.
In our experiments he exact match figure is computed at the LDOCE senselevel, where the number of tokens correctly disambiguated to the sense level is di-vided by the number ambiguous at that level.
At the homograph level, the numbercorrectly disambiguated to the homograph is divided by the number which are poly-homographic.6.
PerformanceUsing the evaluation procedure described in the previous ection, it was found that thesystem correctly disambiguated 90% of the ambiguous instances to the fine-grainedsense level, and in excess of 94% to the homograph level.6 The metric operates lightly differently for systems that assign probabilities to senses,342Stevenson and Wilks Interaction of Knowledge Sources in WSDTable 7System results, baselines, and corpus characteristics.
Sense level results are calculated over allpolysemous words in the evaluation corpus while those reported for the homograph level arecalculated only over polyhomographic ones.Entire SubcorporaCorpus Noun Verb Adjective AdverbSense level Accuracy 90.37% 91.24% 88.38% 91.09% 70.61%Baseline 30.90% 34.56% 18.46% 25.76% 36.73%Tokens 36,774 26,091 6,465 3,310 908Types 5,804 4.041 1,021 1,006 125Average Polysemy 14.62 13.65 24.35 6.07 4.43Homograph level Accuracy 94.65% 94.63% 95.26% 96.89% 90.67%Baseline 71.24% 73.47% 60.72% 87.10% 86.87%Tokens 18,219 11,380 5,194 1,326 319Types 1,683 1,264 709 201 34Average Polysemy 2.52 2.32 2.81 2.95 3.13In order to analyze the effectiveness of our tagger in more detail, we split themain corpus into sub-corpora by grammatical category.
In other words, we createdfour individual sub-corpora containing the ambiguous words which had been part-of-speech tagged as nouns, verbs, adjectives, and adverbs.
The figures characterizingeach of these corpora are shown in Table 7.
The majority of the ambiguous wordswere nouns, with far fewer verbs and adjectives, and less than one thousand adverbs.The average polysemy for nouns, at both sense and homograph levels, is roughlythe same as the overall corpus average although it is noticably higher for verbs atthe sense level.
At the sense level the average polysemy figures are much lower foradjectives and adverbs.
This is because it is common for English words to act as eithera noun or a verb and, since these are the most polysemous grammatical categories,the average polysemy count becomes large due to the cumulative ffect of polysemyacross grammatical categories.
However, words that can act as adjectives or adverbsare unlikely to be nouns or verbs.
This, plus the fact that adjectives and adverbs aregenerally less polysemous in LDOCE, means that their average polysemy in text is farlower than it is for nouns or verbs.Table 7 shows the accuracy of our system over the four subcorpora.
We can seethat the tagger achieves higher results at the homograph level than the sense levelon each of the four subcorpora, which is consistent with the result over the wholecorpus.There is quite a difference in the tagger's results across the different subcorpora--91% for nouns and 70% for adverbs.
Perhaps the learning algorithm does not performas well on adverbs because that corpus is significantly smaller than the other three.This hypothesis was checked by testing our system on portions of each of the threesubcorpora that were roughly equal in size to the adverb subcorpus.
We found that thereduced data caused a slight loss of accuracy on each of the three subcorpora; how-ever, there was still a marked difference between the results for the adverb subcorpusand the other three.
Further analysis showed that the differences in performance overdifferent subcorpora seem linked to the behavior of different partial taggers whenused in combination.
In the following section we describe this behavior in more de-tail.343Computational Linguistics Volume 27, Number 36.1 Interaction of Knowledge SourcesIn order to gauge the contribution of each knowledge source separately, we imple-mented a set of simple disambiguation algorithms, each of which uses the outputfrom a single partial tagger.
Each algorithm takes the result of its partial tagger andchecks it against the disambiguated text to see if it is correct.
If the partial tagger eturnsmore than one sense, as do the simulated annealing, subject code and se lect iona lpreference taggers, the first sense is taken to break the tie.
For the partial tagger basedon Yarowsky's ubject-code algorithm, we choose the sense with the highest saliencyvalue.
If more than one sense has been assigned the maximum value, the tie is againbroken by choosing the first sense.
Therefore, each partial tagger eturns a single senseand the exact match metric is used to determine the proportion of tokens for whichthat tagger eturns the correct sense.
The part-of-speech filter is run before the partialtaggers make their decision and so they only consider the set of senses it did not re-move.
The results of each tagger, computed at both sense and homograph levels overthe evaluation corpus and four subcorpora, re shown in Table 7.We can see that the partial taggers that are most effective are those based on thesimulated annealing algorithm and Yarowsky's ubject code approach.
The success ofthese modules upports our decision to use existing disambiguation algorithms thathave already been developed rather than creating new ones.The most successful of the partial taggers is the one based on Yarowsky's algorithmfor modelling thesaural categories by wide contexts.
This consistently achieves over70% correct disambiguation a d seems particularly successful when disambiguatingadverbs (over 85% correct).
It is quite surprising that this algorithm is so successful foradverbs, since it would seem quite reasonable to expect an algorithm based on subjectcodes to be more successful on nouns and less so on modifiers uch as adjectives andadverbs.Yarowsky (1992) reports that his algorithm achieves 92% correct disambiguation,which is nearly 13% higher than achieved in our implementation.
However, Yarowskytested his implementation  a restricted vocabulary of 12 words, the majority of whichwere nouns, and used Roget large categories as senses.
The baseline performance forthis corpus is 66.5%, considerably higher than the 30.9% computed for the corpusused in our experiments.
Another possible reason for the difference in results is thefact that Yarowsky used smoothing algorithms to avoid problems with the probabilityestimates caused by data sparseness.
We did not employ these procedures and usedsimple corpus frequency counts when calculating the probabilities ( ee Section 4.5).
Itis not possible to say for sure that the differences between implementations did notlead to the differences in results, but it seems likely that the difference in the semanticgranularity of LDOCE subject codes and Roget categories was an important factor.The second partial tagger based on an existing approach is the one which usessimulated annealing to optimize the overlap of words shared by the dictionary defini-tions for a set of senses.
In Section 4.3 we noted that Cowie et al (1992) reported 47%correct disambiguation to the sense level using this technique, while in our adaptationover 17% more words are correctly disambiguated.
Our application filtered out senseswith the incorrect part of speech in addition to using a different method to calculateoverlap that takes account of short definitions.
It seems likely that these changes arethe source of the improved results.Our least successful partial tagger is the one based on selectional preferences.Although its overall result is slightly below the overall corpus baseline, it is very suc-cessful at disambiguating verbs.
This is consistent with the work of Resnik (1997), whoreported that many words do not have strong enough selectional restrictions to carryout WSD.
We expected preferences to be successful for adjectives as well, although344Stevenson and Wilks Interaction of Knowledge Sources in WSDTable 8Performance of individual partial taggers (at sense level).All Nouns Verbs Adjectives Adverbssimulated annealing (I) 65.24% 66.50% 67.51% 49.02% 50.61%selectional preferences (2) 44.85% 40.73% 75.80% 27.56% 0%subject codes (3) 79.41% 79.18% 72.75% 73.73% 85.50%this is not the case in our evaluation.
This is because the sense discrimination of ad-jectives is carried out after that for nouns in our algorithm (see Section 4.4), and theformer is hindered by the low results of the latter.
Adverbs cannot be disambiguatedby preference methods against LDOCE because it does not contain the appropriateinformation.Our analysis of the behavior of the individual partial taggers provides ome cluesto the behavior of the overall system, consisting of all taggers, on the different sub-corpora, as shown in Table 7.
The system performs to roughly the same level overthe noun, verb, and adjective sub-corpora with only a 3% difference between the bestand worst performance.
The system's worst performance is on the abverb sub-corpus,where it disambiguates only slightly more than 70% of tokens successfully.
This maybe due to the fact that only two partial taggers provide evidence for this grammaticalcategory.
However, the system still manages to disambiguate most of the adverbs to thehomograph level successfully, and this is probably because the part-of-speech filter hasruled out the incorrect homographs, not because the partial taggers performed well.One can legitimately wonder whether in fact the different knowledge sources forWSD are all ways of encoding the same semantic information, in a similar way thatone might suspect ransformation rules and statistics encode the same informationabout part-of-speech tag sequences in different formats.
However, the fact that an op-timized combination ofour partial taggers yields a significantly higher figure than anyone tagger operating independently, shows that they must be orthogonal informationsources.6.2 The overall value of the part-of-speech filterWe have already examined the usefulness of part-of-speech tags for semantic disam-biguation in Section 3.
However, we now want to know the effect it has within asystem consisting of several disambiguation modules.
It was found that accuracy atthe sense level reduced to 87.87% and to 93.36% at the homograph level when thefilter was removed.
Although the system's performance did not decrease by a largeamount, the part-of-speech filter brings the additional benefit of reducing the searchspace for the three partial taggers.
In addition, the fact that these results are not af-fected much by the removal of the part-of-speech filter, shows that the WSD modulesalone do a reasonable job of resolving part-of-speech ambiguity as a side-effect ofsemantic disambiguation.7.
ConclusionPreviously reported WSD systems that enjoyed a high level of accuracy have oftenoperated on restricted vocabularies and employed a single WSD methodology.
Thesemethods have often been pursued for sound reasons to do with evaluation, but havebeen limited in their applicability and also in their persuasiveness regarding the scal-345Computational Linguistics Volume 27, Number 3ability and interaction of the various WSD partial methods.
This paper reported asystem which disambiguated all content words in a text, as defined by a standardmachine readable dictionary, with a high degree of accuracy.Our evaluation shows that disambiguation can be carried out with more accurateresults when several knowledge sources are combined.
It remains unclear exactly whatit means to optimize the combination of modules within a learning system like T?MBL:we could, in further work, treat the part-of-speech tagger as a partial tagger and nota filter, and we could allow the system to learn some "optimal" weighting of allthe partial taggers.
It also remains an interesting question whether, because of theundoubted existence of novel senses in text, a sense tagger can ever reach the levelthat part-of-speech tagging has.
However, we believe we have shown that interestingcombinations of WSD methods on a substantial training corpus are possible, and thatthis can show, among other things, the relative independence of the types of semanticinformation expressed by the various forms of lexical input.AcknowledgmentsThe work described here was supported bythe European Union Language Engineeringproject ECRAN - Extraction of Content:Research at Near-market (LE-2110).
One ofthe authors was also supported by theEPSRC grant MALT (GR/M73521) whilewriting this paper.
We are grateful for thefeedback from many colleagues in Sheffield,especially Mark Hepple, and for thedetailed comments from the anonymousreviewers of an earlier version of this paper.Gillian Callaghan was extremely helpful inthe preparation of the final version of thispaper.
Any errors are our own.ReferencesBateman, John, Robert Kasper, Joharu~aMoore, and Richard Whimey.
1990.
Ageneral organization of knowledge fornatural language processing: thePENMAN upper model, Technical report,USC/Information Sciences Institute,Marina del Rey, CA.Brill, Eric.
1995.
Transformation-basederror-driven learning and naturallanguage processing: A case study in partof speech tagging.
ComputationalLinguistics, 21(4):543-566.Brown, Peter, Stephen Della Pietra, VincentDella Pietra, and Robert Mercer.
1991.Word sense disambiguation usingstatistical methods.
In Proceedings ofthe29th Meeting of the Association forComputational Linguistics (ACL-91),pages 264-270, Berkeley, CA.Bruce, Rebecca nd Louise Guthrie.
1992.Genus disambiguation: A study inweighted performance.
In Proceedings ofthe 14th International Conference onComputational Linguistics (COLING-92),pages 1187-1191, Nantes, France.Bruce, Rebecca nd Janyce Wiebe.
1994.Word-sense disambiguation usingdecomposable models.
In Proceedings ofthe32nd Annual Meeting of the Association forComputational Linguistics (ACL-94),pages 139-145, Las Cruces, New Mexico.Burnard, Lou.
1995.
Users Reference Guide forthe British National Corpus.
OxfordUniversity Computing Services.Chapman, R. L. 1977.
Roget's InternationalThesaurus Fourth Edition, Thomas Y.Crowell Company, New York, NY.Cost, Scott and Steven Salzberg.
1993.
Aweighted nearest neighbour algorithm forlearning with symbolic features.
MachineLearning, 10(1):57-78.Cottrell, Garrison.
1984.
A model of lexicalaccess of ambiguous words.
In Proceedingsof the National Conference on ArtificialIntelligence (AAAI-84), pages 61-67,Austin, TX.Cowie, Jim, Louise Guthrie, and JoeGuthrie.
1992.
Lexical disambiguationusing simulated annealing.
In Proceedingsof the 14th International Conference onComputational Linguistics (COLING-92),pages 359-365, Nantes, France.Daelemans, Walter, Jakub Zavrel, PeterBerck, and Steven Gillis.
1996.
MBT: Amemory-based part of speech taggergenerator.
In Proceedings ofthe FourthWorkshop on Very Large Corpora,pages 14-27, Copenhagen.Daelemans, Walter, Jakub Zavrel, Ko vander Sloot, and Antal van den Bosch.
1998.TiMBL: Tilburg memory based learnerversion 1.0.
Technical report, Universityof Tilburg Technical Report 98-03.Daelemans, Walter, Jakub Zavrel, Ko vander Sloot, and Antal van den Bosch.
1999.TiMBL: Tilburg memory based learner,version 2.0, reference guide.
Technical346Stevenson and Wilks Interaction of Knowledge Sources in WSDreport, University of Tilburg TechnicalReport 99-01.
Available from ht tp : / / i l k .kub.
nl/~ ilk/papers/ilk990 I. ps.Fellbaum, Christiane, Joachim Grabowski,Shari Landes, and A. Baumann.
1998.Matching words to senses in WordNet:Naive vs. expert differentiation of senses.In Christiane Fellbaum, editor, WordNet:An Electronic Lexical Database and SomeApplications.
MIT Press, Cambridge, MA.Gaizauskas, Robert, Takahiro Wakao, KevinHumphreys, Hamish Cunningham, andYorick Wilks.
1996.
Description of theLaSIE system as used for MUC-6.
InProceedings ofthe Sixth MessageUnderstanding Conference (MUC-6),pages 207-220, San Francisco, CA.Gale, William, Kenneth Church, and DavidYarowsky.
1992a.
Estimating upper andlower bounds on the performance ofword sense disambiguation programs.
InProceedings ofthe 30th Annual Meeting of theAssociation for Computational Linguistics(ACL-92), pages 249-256, Newark, DE.Gale, William, Kenneth Church, and DavidYarowsky.
1992b.
A method fordisambiguating word senses in a largecorpus.
Computers and the Humanities,26:415-439.Gale, William, Kenneth Church, and DavidYarowsky.
1992c.
One sense per discourse.In Proceedings ofthe DARPA Speech andNatural Language Workshop, pages 233-237,Harriman, NY.Guo, Cheng-Ming.
1989.
Constructing aMachine Tractable Dictionary fromLongman Dictionary of ContemporaryEnglish.
Technical Report MCCS-89-156,Computing Research Laboratory, NewMexico State University.Harley, Andrew and Dominic Glennon.1997.
Sense tagging in action: Combiningdifferent ests with additive weights.
InProceedings ofthe SIGLEX Workshop"Tagging Text with Lexical Semantics",pages 74-78, Washington, DC.Hirst, Graeme.
1987.
Semantic Interpretationand the Resolution of Ambiguity.
CambridgeUniversity Press, Cambridge, UK.Hirst, Graeme.
1995.
Near-synonymy andthe structure of lexical knowledge.
InAmerican Association for Artificial IntelligenceSpring Symposium on Lexicons, pages 51-56.Ide, Nancy and Jean V4ronis.
1998.Introduction to the special issue on wordsense disambiguation: The state of the art.Computational Linguistics, 24(1):1-40.Kilgarriff, Adam.
1993.
Dictionary wordsense distinctions: An enquiry into theirnature.
Computers and the Humanities,26:356-387.Kilgarriff, Adam.
1997.
Sample the lexicon.Technical Report ITRI-97-01, ITRI,University of Brighton.Kilgarriff, Adam.
1998.
SENSEVAL: AnExercise in Evaluating Word SenseDisambiguation Programs.
In Proceedingsof the First International Conference onLanguage Resources and Evaluation,pages 581-585, Granada, Spain.Knight, Kevin and Steve K. Luk.
1994.Building a large knowledge base formachine translation.
In Proceedings oftheAmerican Association for Arti~cialIntelligence Conference (AAAI-94),pages 185-109, Seattle, WA.Ku~era, Henri and Winthrop Francis.
1967.A Computational Analysis of Present-dayAmerican English.
Brown University Press,Providence, RI.Landes, Shari, Claudia Leacock, and RandeeTengi.
1998.
Building a semanticconcordance of English.
In C. Fellbaum,editor, WordNet: An Electronic LexicalDatabase and Some Applications.
MIT Press,Cambridge, MA.Leacock, Claudia, Geoffrey Towell, andEllen Voorhees.
1993.
Corpus-basedstatistical sense resolution.
In Proceedingsof the ARPA Human Language TechnologyWorkshop, pages 260-265, Plainsboro, NJ.Lesk, Michael.
1986.
Automatic sensedisambiguation using machine readabledictionaries: how to tell a pine cone froman ice cream cone.
In Proceedings ofACMSIGDOC Conference, pages 24-26, Toronto.Mahesh, Kavi, Sergei Nirenburg, StephenBeale, Evelyne Viegas, Victor Raskin, andBoyan Onyshkevych.
1997.
Word sensedisambiguation: Why have statistics whenwe have these numbers?
In Proceedings ofthe Seventh International Conference on The-oretical and Methodological Issues in MachineTranslation, pages 151-159, Sante Fe, NM.Marcus, Mitchell, Beatrice Santorini, andMary Marcinkiewicz.
1993.
Building alarge annotated corpus of English: ThePenn Tree Bank.
Computational Linguistics,19(2):313-330.Masterman, Margaret.
1957.
The thesaurusin syntax and semantics.
MechanicalTranslation, 4:1-2.McCarthy, J. and P. Hayes.
1969.
Somephilosophical problems from thestandpoint of artificial intelligence.
In B.Meltzer and D. Michie, editors, MachineIntelligence 4.
Edinburgh, EdinburghUniversity Press.
pages 463-502.McRoy, Susan.
1992.
Using multipleknowledge sources for word sensedisambiguation.
Computational Linguistics,18(1):1-30.347Computational Linguistics Volume 27, Number 3Melamed, Daniel and Philip Resnik.
2000.Evaluation of sense disambiguation givenhierarchical tag sets.
Computers and theHumanities, 34:1-2.Metropolis, Norbert, Anne Rosenbluth,Maya Rosenbluth, Andrew Teller, andEdward Teller.
1953.
Equation statecalculations by fast computing machines.Journal of Chemical Physics, 21:1087-1092.Nadas, Andrew, David Nahamoo, MichaelPicheny, and Jonathan Powell.
1991.
Aniterative "flip-flop" approximation of themost informative split in the constructionof decision trees.
In Proceedings ofthe IEEEInternational Conference on Acoustics, Speechand Signal Processing, pages 565-568,Toronto.Ng, Hwee and Hian Lee.
1996.
Integratingmultiple knowledge sources todisambiguate word sense: Anexemplar-based approach.
In Proceedingsof the 34th Meeting of the Association forComputational Linguistics (ACL-96),pages 40-47, Santa Cruz, CA.Press, William, Saul Teukolsky, WilliamVetterling, and Brian Flannery.
1988.Numerical Recipes in C: The Art of ScientificComputing.
Cambridge University Press,Cambridge.Procter, Paul, editor.
1978.
LongmanDictionary of Contemporary English.Longman Group, Essex, UK.Procter, Paul, editor.
1995.
CambridgeInternational Dictionary of English.Cambridge University Press, Cambridge.Quinlan, J.
1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann, San Mateo,CA.Resnik, Philip.
1997.
Selectional preferencesand word sense disambiguation.
IProceedings ofthe SIGLEX Workshop"Tagging Text with Lexical Semantics: What,why and how?
", pages 52-57, Washington,D.C.Resnik, Philip and David Yarowsky.
1997.
Aperspective on word sensedisambiguation techniques and theirevaluation.
In Proceedings ofthe SIGLEXWorkshop "Tagging Text with LexicalSemantics: What, why and how?
",pages 79-86, Washington, D.C.Rigau, German, Jordi Atserias, and EnekoAgirre.
1997.
Combining unsupervisedlexical knowledge methods for wordsense disambiguation.
I  35th Meeting ofthe Association for Computational Linguisticsand the Eighth Meeting of the EuropeanChapter of the Association for ComputationalLinguistics (ACL/EACL-97), pages 48-55,Madrid, Spain.Sch~tze, Hinrich.
1992.
Dimensions ofmeaning.
In Proceedings ofSupercomputing'92, pages 787-796, Minneapolis, MN.Stevenson, Mark.
1998.
Extracting syntacticrelations using heuristics.
In Proceedings ofthe European Summer School on Logic,Language and Information '98 StudentWorkshop, ages 248-256, Saarbri~cken,Germany.Stevenson, Mark and Yorick Wilks.
1999.Combining weak knowledge sources forsense disambiguation.
I  Proceedings oftheSixteenth International Joint Conference onArtificial Intelligence (IJCAI-99),pages 884-889, Stockholm, Sweden.van Rijsbergen, Keith.
1979.
InformationRetrieval.
Butterworths, London.V~ronis, Jean and Nancy Ide.
1990.
Wordsense disambiguation with very largeneural networks extracted from machinereadable dictionaries.
In Proceedings ofthe13th International Conference onComputational Linguistics (COLING-90),pages 389-394, Helsinki.Waltz, David and Jordan Pollack.
1985.Massively parallel parsing: A stronglyinteractive model of natural anguageinterpretation.
Cognitive Science, 9:51-74.Wilks, Yorick.
1975.
A preferentialpattern-seeking semantics for naturallanguage inference.
Artificial Intelligence,6:53-74.Wilks, Yorick.
1997.
Senses and Texts.Computers and the Humanities, 31:77-90.Wilks, Yorick, Dan Fass, Cheng-Ming Guo,James McDonald, Tony Plate, and BrianSlator.
1990.
Providing machine tractabledictionary tools.
Machine Translation,5:99-154.Wilks, Yorick, Brian Slator, and LouiseGuthrie.
1996.
Electric Words: Dictionaries,Computers and Meanings.
MIT Press,Cambridge, MA.Wilks, Yorick and Mark Stevenson.
1998a.The grammar of sense: Usingpart-of-speech tags as a first step insemantic disambiguation.
Journal ofNatural Language Engineering, 4(2):135-144.Wilks, Yorick and Mark Stevenson.
1998b.Optimizing combinations of knowledgesources for word sense disambiguation.In Proceedings ofthe 36th Meeting of theAssociation for Computational Linguistics(COLING-ACL-98), pages 1398-1402,Montreal.Yarowsky, David.
1992.
Word-sensedisambiguation using statistical models ofRoget's categories trained on largecorpora.
In Proceedings ofthe 14thInternational Conference on ComputationalLinguistics (COLING-92), pages 454-460,Nantes, France.348Stevenson and Wilks Interaction of Knowledge Sources in WSDYarowsky, David.
1993.
One sense percollocation.
In Proceedings ofthe ARPAHuman Language Technology Workshop,pages 266-271, Princeton, NJ.Yarowsky, David.
1995.
Unsupervised wordsense disambiguation rivaling supervisedmethods.
In Proceedings ofthe 33rd AnnualMeeting of the Association for ComputationalLinguistics (ACL-95), pages 189-196,Cambridge, MA.Yarowsky, David.
1996.
Homographdisambiguation i  text-to-speechsynthesis.
In J. Hirschberg, R. Sproat, andJ.
van Santen, editors, Progress in SpeechSynthesis.
Springer Verlag, New York, NY,pages 159-175.Yngve, Victor.
1995.
Syntax and the problemof multiple meaning.
In W. Locke and D.Booth, editors, Machine Translation ofLanguages.
Wiley, New York.Zavrel, Jakub, Walter Daelemans, and JornVeenstra.
1997.
Resolving PP-attachmentwith memory-based learning.
InProceedings ofthe Workshop onComputational Natural Language Learning(CoNLL '97), pages 136-144, Madrid.Zipf, Georg.
1935.
The Psycho-Biology ofLanguage.
Houghton Mifflin, Boston, MA.349
