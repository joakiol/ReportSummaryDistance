Annotating and Learning Event Durationsin TextFeng Pan?Bing, Microsoft CorporationRutu Mulkar-Mehta?
?Information Sciences Institute (ISI),University of Southern CaliforniaJerry R. Hobbs?Information Sciences Institute (ISI),University of Southern CaliforniaThis article presents our work on constructing a corpus of news articles in which eventsare annotated for estimated bounds on their duration, and automatically learning from thiscorpus.
We describe the annotation guidelines, the event classes we categorized to reduce grossdiscrepancies in inter-annotator judgments, and our use of normal distributions to model vagueand implicit temporal information and to measure inter-annotator agreement for these eventduration distributions.
We then show that machine learning techniques applied to this data canproduce coarse-grained event duration information automatically, considerably outperforming abaseline and approaching human performance.
The methods described here should be applicableto other kinds of vague but substantive information in texts.1.
IntroductionConsider the sentence from a news article:George W. Bush met with Vladimir Putin in Moscow.How long did the meeting last?
Our first inclination is to say we have no idea.
But infact we do have some idea.
We know the meeting lasted more than ten seconds andless than one year.
As we guess narrower and narrower bounds, our chances of beingcorrect go down, but if we are correct, the utility of the information goes up.
Just howaccurately can we make duration judgments like this?
How much agreement can weexpect among people?
Will it be possible to extract this kind of information from textautomatically??
Microsoft Corporation, 475 Brannan St., San Francisco, CA 94107, USA.E-mail: fengpan@microsoft.com.??
4676 Admiralty Way, Marina del Rey, CA 90292, USA.
E-mail: me@rutumulkar.com.?
4676 Admiralty Way, Marina del Rey, CA 90292, USA.
E-mail: hobbs@isi.edu.Submission received: 2 November 2006; revised submission received: 26 January 2011; accepted forpublication: 7 March 2011.?
2011 Association for Computational LinguisticsComputational Linguistics Volume 37, Number 4Sometimes we are explicitly told the duration of events, as in ?a five-day meeting?and ?I have lived here for three years.?
But more often, such phrases are missing, andpresent-day natural language applications simply have to proceed without them.There has been a great deal of work on formalizing temporal information (Allen1984; Moens and Steedman 1988; Zhou and Fikes 2002; Han and Lavie 2004; Hobbs andPan 2004) and on temporal anchoring and event ordering in text (Hitzeman, Moens, andGrover 1995; Mani and Wilson 2000; Filatova and Hovy 2001; Boguraev and Ando 2005;Mani et al 2006; Lapata and Lascarides 2006).
The uncertainty of temporal durationshas been recognized as one of the most significant issues for temporal reasoning (Allenand Ferguson 1994).
Chittaro and Montanari (2000) point out by way of example thatwe have to know how long a battery remains charged to decide when to replace it or topredict the effects of actions which refer to the battery charge as a precondition.Yet to our knowledge, there has been no serious published empirical effort to modeland learn the vague and implicit duration information in natural language, and toperform reasoning over this information.
Cyc has some fuzzy duration information,although it is not generally available; Rieger (1974) discusses the issue for less than apage; there has been work in fuzzy logic on representing and reasoning with imprecisedurations (Godo and Vila 1995; Fortemps 1997).
But none of these efforts make anattempt to collect human judgments on such durations or to extract them automaticallyfrom text.Nevertheless, people have little trouble exploiting temporal information implicitlyencoded in the descriptions of events, relying on their knowledge of the range of usualdurations of types of events.
This hitherto largely unexploited information is part of ourcommonsense knowledge.
We can estimate roughly how long events of different typeslast and roughly how long situations of various sorts persist.
We know that governmentpolicies typically last somewhere between one and ten years, and weather conditionsfairly reliably persist between three hours and one day.
We are often able to decidewhether two events overlap or are in sequence by accessing this information.
We knowthat if a war started yesterday, we can be pretty sure it is still going on today.
If ahurricane started last year, we can be sure it is over by now.This article describes an exploration into how this information can be capturedautomatically.
Our results can have a significant impact on computational linguisticsapplications like event anchoring and ordering in text (Mani and Schiffman 2007), eventcoreference (Bejan and Harabagiu 2010), question answering (Tao et al 2010; Harabagiuand Bejan 2005), and other intelligent systems that would benefit from such temporalcommonsense knowledge, for example, temporal reasoning (Zhou and Hripcsak 2007).Our goal is to be able to extract this implicit event duration information fromtext automatically, and to that end we first annotated the events in news articles withbounds on their durations.
The corpus that we have annotated currently contains all48 non-Wall-Street-Journal (non-WSJ) news articles (2,132 event instances), as well as10 WSJ articles (156 event instances), from the TimeBank corpus annotated in TimeML(Pustejovsky et al 2003).
The non-WSJ articles (mainly political and disaster news)include both print and broadcast news that are from a variety of news sources, suchas ABC, AP, CNN, and VOA.
All the annotated data have already been integrated intothe TimeBank corpus.1This article is organized as follows.
In Section 2 we describe our annotation guide-lines, including the annotation strategy and assumptions, and the representative event1 The annotated data are available at http://www.isi.edu/?hobbs/EventDuration/annotations.728Pan, Mulkar-Mehta, and Hobbs Annotating and Learning Event Durations in Textclasses we have categorized to minimize discrepant judgments between annotators.
Themethod for measuring inter-annotator agreement when the judgments are intervals ona scale is described in Section 3.
We will discuss how to integrate our event durationannotations to TimeML in Section 4.
In Section 5 we show that machine learning tech-niques applied to the annotated data considerably outperform a baseline and approachhuman performance.2.
Annotation Guidelines and Event ClassesEvery event to be annotated was already identified in the TimeBank corpus.
In ourproject, annotators were asked to provide lower and upper bounds on the durationof the event, and a judgment of level of confidence in those estimates on a scale from 1to 10.
An interface was built to facilitate the annotation.
Graphical output is displayedto enable us to visualize quickly the level of agreement among different annotators foreach event.
For example, Figure 1 shows the output of the annotations (three annotators)for the ?finished?
event in the sentence:After the victim, Linda Sanders, 35, had finished her cleaning and was waiting for her clothesto dry, ...Figure 1 shows that the first annotator believed that the event lasts for minutes whereasthe second annotator believed it could only last for several seconds.
The third annotatedthe event as ranging from a few seconds to a few minutes.
The confidence level of theannotators is generally subjective but as all three were higher than 5, it shows reasonableconfidence.
A logarithmic scale is used for the output (see Section 3.1 for details).2.1 Annotation InstructionsAnnotators were asked to make their judgments as intended readers of the article, usingwhatever world knowledge was relevant to an understanding of the article.
They wereasked to identify upper and lower bounds that would include 80% of the possible cases.For example, rainstorms of 10 seconds or of 40 days and 40 nights might occur, but theyare clearly anomalous and should be excluded.
There are two strategies for consideringthe range of possibilities:1.
Pick the most probable scenario, and annotate its upper and lower bounds.Figure 1Example annotation output by three annotators.729Computational Linguistics Volume 37, Number 42.
Pick the set of probable scenarios, and annotate the bounds of their upperand lower bounds.We deemed the second to be the preferred strategy.The judgments were to be made in context.
First of all, information in the syntacticenvironment needed to be considered before annotating.
For example, there is a differ-ence in the duration of the watching events in the phrases watch a movie and watch abird fly.Moreover, the events needed to be annotated in light of the information providedby the entire article.
This meant annotators were to read the entire article before startingto annotate.
One may learn in the last paragraph, for example, that the demonstrationevent mentioned in the first paragraph lasted for three days, and that information wasto be used for annotation.However, they were not to use knowledge of the future when annotating a historicalarticle.
For example, an article from the fall of 1990 may talk about the coming waragainst Iraq.
Today we know exactly how long that lasted.
But annotators were askedto try to put themselves in the shoes of the 1990 readers of that article, and maketheir judgments accordingly.
This was because we wanted people?s estimates of typicaldurations of events, rather than the exact durations.Annotation could be made easier and more consistent if coreferential and nearcorefer-ential descriptions of events were identified initially.
Annotators were asked to give thesame duration ranges for such cases.
For example, in the sentence during the demonstra-tion, people chanted antigovernment slogans, annotators were to give the same durationsfor the ?demonstration?
and ?chanted?
events.2.2 AnalysisWhen the articles were completely annotated by the three annotators, the results wereanalyzed and the differences were reconciled.
Differences in annotation could be due tothe differences in interpretations of the event; we found that the vast majority of radi-cally different judgments could be categorized into a relatively small number of classes,however.
Some of these correspond to aspectual features of events, which have beeninvestigated intensively (e.g., Vendler 1967; Dowty, 1979; Moens and Steedman 1988;Passonneau 1988; Giorgi and Pianesi 1997; Madden and Zwaan 2003; Smith 2005).
Wethen developed guidelines to make annotators aware of these cases and to guide them inmaking the judgments (see the next section).
There is a residual of gross discrepancies inannotators?
judgments that result from differences of opinion, for example, about howlong a government policy is typically in effect.
But the number of these discrepancieswas surprisingly small.These guidelines were then used in the annotation of a test set.
It was shown thatthe agreement in the test set was greater than the agreement obtained when anno-tations were performed without the guidelines.
(See Section 3.3 for the experimentalresults.
)2.3 Event ClassesAction vs. State: Actions involve change, such as those described by words like speaking,gave, and skyrocketed.
States involve things staying the same, such as being dead, being dry,730Pan, Mulkar-Mehta, and Hobbs Annotating and Learning Event Durations in Textand being at peace.
When we have an event in the passive tense, sometimes there is anambiguity about whether the event is a state or an action.
For example, inThree people were injured in the attack.does the word ?injured?
describe an action or a state?
This matters because they willhave different durations.
The state begins with the action and lasts until the victim ishealed.
In the sentence,Farkas was ordered home and retired.although retired usually indicates a state, it looks here more like the action by hiscompany of retiring Farkas.There are some general diagnostic tests to distinguish actions and states (Vendler1967; Dowty 1979); for example, action verbs are fine in the progressive form butprogressives of stative verbs are usually odd.
Another test can be applied to this specificcase: Imagine someone says the sentence after the action has ended but the state is stillpersisting.
Would they use the past or present tense?
In the ?injured?
example, it is clearwe would say ?Three people were injured in the attack,?
whereas we would say ?Threepeople are injured from the attack.?
Similarly, we would say ?Farkas was retired?
ratherthan ?Farkas is retired.
?Our annotation interface handles events of this type by allowing the annotators tospecify which interpretation they are giving.
If the annotator feels it?s too ambiguous todistinguish, annotations can be given for both interpretations.Aspectual Events: Some events are aspects of larger events, such as their start orfinish.
Although they may seem instantaneous, we believe they should be consideredto happen across some interval (i.e., the first or last sub-event of the larger event).
Forexample, inAfter the victim, Linda Sanders, 35, had finished her cleaning and was waiting for her clothesto dry,...the ?finished?
event should be considered as the last sub-event of the larger event (the?cleaning?
event), because it actually involves opening the door of the washer, takingout the clothes, closing the door, and so on.
All this takes time.
This interpretation willalso give us more information on typical durations than if we simply assume suchevents are instantaneous.In the following example:General Abacha?s supporters began a two-day rally in the capital.the gathering of people marks the ?beginning?
of the rally, and it generally takes timefor a crowd of people to get together to start a rally.Reporting Events: These are everywhere in the news.
They can be direct quotes,taking exactly as long as the sentence takes to read, or they can be summarizations oflong press conferences.
We need to distinguish different cases: Quoted Report: This is when the reported content is quoted.
The durationof the event should be the actual duration of the utterance of the quoted731Computational Linguistics Volume 37, Number 4content.
The duration can easily be verified by saying the sentence outloud and timing it.
For example, in?It looks as though they panicked,?
a detective said of the robbers.the saying probably took between 1 and 3 seconds; it?s very unlikely ittook more than 10 seconds. Unquoted Report: When the reporting description occurs without quotes,the report could be as short as the duration of the actual utterance of thereported content (lower bound), and as long as the duration of a briefingor press conference (upper bound).If the sentence is very short, then it?s likely that it is one complete sentence fromthe speaker?s remarks, and a short duration should be given; if it is a long, complexsentence, then it?s more likely to be a summary of a long discussion or press conference,and a longer duration should be given.
For example, considerThe police said it did not appear that anyone else was injured.A Brooklyn woman who was watching her clothes dry in a laundromat was killed Thursdayevening when two would-be robbers emptied their pistols into the store, the police said.If the first sentence were quoted text, it would be very much the same.
Hencethe duration of the ?said?
event should be short.
In the second sentence everythingthat the spokesperson (here the police) has said is compiled into a single sentenceby the reporter, and it is unlikely that the spokesperson said only a single sentencewith all this information.
Thus, it is reasonable to give longer duration to this?said?
event.Multiple Events: Many occurrences of verbs and other event descriptors refer tomultiple events, especially, but not exclusively, if the subject or object of the verb isplural.
For example, inIraq has destroyed its long-range missiles.both single (i.e., destroyed one missile) and aggregate (i.e., destroyed all missiles)events happened.
This was a significant source in disagreements in our first round ofannotation.
Because both judgments provide useful information, our current annotationinterface allows the annotator to specify the event as multiple, and give durations forboth the single and aggregate events.In the following example:Seventy-five million copies of the rifle have been built since it entered production inFebruary 1947.?built?
and ?production?
are both multiple events.
The annotators were asked togive durations for both the single (i.e., built/produce one rifle) and aggregate (i.e.,built/produce 75 million copies of the rifle) events.732Pan, Mulkar-Mehta, and Hobbs Annotating and Learning Event Durations in TextEvents Involving Negation: Negated events didn?t happen, so it may seem strange tospecify their duration.
But whenever negation is used, there is a certain class of eventswhose occurrence is being denied.
Annotators should consider this class, and make ajudgment about the likely duration of the events in it.
In addition, there is the intervalduring which the nonoccurrence of the events holds.
For example, inHe was willing to withdraw troops in exchange for guarantees that Israel would not beattacked.there is the typical amount of time of ?being attacked,?
namely, the duration of a singleattack, and a longer period of time of ?not being attacked.?
The first is probably fromseconds to minutes and the second from months to years.
Similarly to multiple events,annotators were asked to give durations for both the event negated and the negation ofthat event.Appearance Events.
Verbs like ?seem?
and ?appear?
usually indicate appearanceevents.
The duration of this kind of event depends on the duration of the validity oravailability of the evidence that causes one to have some impression at the time.
Suchan event begins when enough evidence has accumulated for one to make that guess orjudgment, and it ends when either the evidence is contradicted or certainty is achieved.For example, inIt appears that the destruction of this city in 2700 B.C.
was related to the eruption of thevolcano.the ?appears?
event lasts from when the archaeologist discovers enough evidence tomake the conjecture until the time the conjecture is refuted or confirmed.Positive Infinite Durations: These are states which continue essentially forever oncethey begin, for example,He is dead.Here the state continues for an infinite amount of time, and we allow this as a possibleannotation.3.
Inter-Annotator AgreementAlthough the graphical output of the annotations enables us to visualize quickly thelevel of agreement among different annotators for each event, a quantitative measure-ment of the agreement is needed.The kappa statistic (Krippendorff 1980; Siegel and Castellan 1988; Carletta 1996; DiEugenio and Glass 2004), which factors out the agreement that is expected by chance,has become the de facto standard to assess inter-annotator agreement.
It is computed asfollows:?
=P(A) ?
P(E)1 ?
P(E) (1)733Computational Linguistics Volume 37, Number 4P(A) is the observed agreement among the annotators, and P(E) is the expected agree-ment, which is the probability that the annotators agree by chance.In order to compute the kappa statistic for our task, we have to compute P(A) andP(E) first.
But those computations are not straightforward.P(A): What should count as agreement among annotators for our task?P(E): What is the probability that the annotators agree by chance for our task?3.1 What Should Count as Agreement?Determining what should count as agreement is not only important for assessing inter-annotator agreement, but is also crucial for later evaluation of machine learning ex-periments.
For example, for a given event with a known gold-standard duration rangefrom 1 hour to 4 hours, if a machine learning program outputs a duration of 3 hoursto 5 hours, how should we evaluate this result?We first need to decide what scale is most appropriate.
One possibility is just toconvert all the temporal units to seconds.
However, this would not correctly capture ourintuitions about the relative relations between duration ranges.
For example, the differ-ence between 1 second and 20 seconds is significant, whereas the difference between1 year 1 second and 1 year 20 seconds is negligible.
Consider the range from 1 yearto 5 years and the range from 1 second to 5 seconds.
The distance between 1 year and5 years in seconds would be much larger than that between 1 second and 5 seconds, butintuitively, they represent the same level of uncertainty.
In order to handle this problem,we use a logarithmic scale for our data.
After first converting from temporal units toseconds, we then take the natural logarithms of these values.
This use of a logarithmicscale also conforms to the idea of the importance of half orders of magnitude (HOM)(Hobbs 2000; Hobbs and Kreinovich 2001), which has been shown to have utility incommonsense reasoning and in several very different linguistic contexts.In the literature on the kappa statistic, most authors address only category data(either in nominal scales or ordinal scales); some can handle more general data, such asdata in interval scales or ratio scales (Krippendorff 1980; Carletta 1996).
However, noneof the techniques directly apply to our data, which involves a range of durations from alower bound to an upper bound.In fact, what coders annotate for a given event is not just a range, but a durationdistribution for the event, where the area between the lower bound and the upper boundcovers about 80% of the entire distribution area.
It is natural to assume that the mostlikely duration in such a distribution is the mean or average duration, and that thedistribution flattens out toward the upper and lower bounds.
Thus, we use the normalor Gaussian distribution to model the distribution of possible durations.In order to determine a normal distribution, we need to know two parameters:the mean and the standard deviation.
For our duration distributions with given lowerand upper bounds, the mean is the average of the bounds.
Under the assumptionthat the area between lower and upper bounds covers 80% of the entire distributionarea, the lower and upper bounds are each 1.28 standard deviations from the mean.Then the standard deviation can be computed using either the upper bound (Xupper) orthe lower bound (Xlower) as follows:?
=Xupper ?
?1.28=Xlower ?
?
?1.28 , where ?
=Xupper + Xlower2 (2)734Pan, Mulkar-Mehta, and Hobbs Annotating and Learning Event Durations in TextWith this data model, the agreement between two annotations can be defined asthe overlapping area between two normal distributions.2 The agreement among manyannotations is the average overlap of all the pairwise overlapping areas.
For example,for a given event, suppose the two annotations are:1.
Lower: 10 minutes; upper: 30 minutes2.
Lower: 10 minutes; upper 2 hoursAfter converting to seconds and to the natural logarithmic scale, they become:1.
Lower: 6.39692; upper: 7.495542.
Lower: 6.39692; upper: 8.88184We then compute their means and standard deviations:1.
?1 = 6.94623; ?1 = 0.428612.
?2 = 7.63938; ?2 = 0.96945The distributions and their overlap are then as in Figure 2.
The overlap or agreement(P(A)) is 0.508706.3.2 Expected AgreementWhat is the probability that the annotators agree by chance for our task?
The firstquick response to this question may be 0, if we consider all the possible durations from1 second to 1,000 years or even positive infinity.However, not all the durations are equally possible.
As in Krippendorff (1980) andSiegel and Castellan (1988), we assume there exists one global distribution for ourtask (i.e., the duration ranges for all the events), and ?chance?
annotations would beconsistent with this distribution.
Thus, the baseline will be an annotator who knows theglobal distribution and annotates in accordance with it, but does not read the specificarticle being annotated.
Therefore, we must compute the global distribution of thedurations, in particular, of their means and their widths.
This will be of interest notonly in determining expected agreement, but also in terms of what it says about thegenre of news articles and about fuzzy judgments in general.We first compute the distribution of the means of all the annotated durations.
Itshistogram is shown in Figure 3, where the horizontal axis represents the mean values inthe natural logarithmic scale and the vertical axis represents the number of annotateddurations with that mean.There are two peaks in this distribution.
One is from 5 to 7 in the natural logarithmicscale, which corresponds to about 1.5 minutes to 30 minutes.
The other is from 14 to 17in the natural logarithmic scale, which corresponds to about 8 days to 6 months.
Onecould speculate that this bimodal distribution is because daily newspapers report shortevents that happened the day before and place them in the context of larger trends.
Thelowest point between the two peaks occurs at 11, which roughly corresponds to one day.2 This idea is due to Hoa Trang Dang.735Computational Linguistics Volume 37, Number 4Figure 2Overlap of judgments of [10 minutes, 30 minutes] and [10 minutes, 2 hours].We also compute the distribution of the widths (i.e., Xupper ?
Xlower) of all theannotated durations, and its histogram is shown in Figure 4, where the horizontal axisrepresents the width in the natural logarithmic scale and the vertical axis represents thenumber of annotated durations with that width.The peak of this distribution occurs at 2.5 in the natural logarithmic scale.
Thisshows that for annotated durations, the most likely uncertainty factor from a mean oraverage duration is 3.5:Xupper?
=?Xlower= e1.25 = 3.5 (3)becauselog(Xupper) ?
log(?)
= log(Xupper? )
= 2.5/2 = 1.25 (4)This is the half orders of magnitude factor that Hobbs and Kreinovich (2001) argue givesthe optimal granularity; making something three to four times bigger changes the waywe interact with it.Because the global distribution is determined by these mean and width distribu-tions, we can then compute the expected agreement, that is, the probability that theannotators agree by chance, where the chance is based on this global distribution.
Two736Pan, Mulkar-Mehta, and Hobbs Annotating and Learning Event Durations in Textapproaches were used to approximate this probability, both of which use a normaldistribution to approximate the global distribution.The first approach is to compute a fixed global normal distribution with the meanas the mean of the mean distribution and the standard deviation as the mean standarddeviation (this can be straightforwardly computed from the width distribution).
Wethen compute the expected agreement by averaging all the agreement scores (overlaps)between this fixed distribution and each of the annotated duration distributions.The second approach is to generate 1,000 normal distributions whose means arerandomly generated from the mean distribution and standard deviations are randomlycomputed from the width distribution.
We then compute the expected agreement byaveraging all the agreement scores (overlaps) between these 1,000 random distributions.In a sense, both of these capture the way an annotator might annotate if he or shedid not read the article but only guessed on the basis of the global distribution.
As itturns out, the results of the two approaches of computing the expected agreement arevery close; they differ by less than 0.01: P(E)1 = 0.1439, P(E)2 = 0.1530.
We will use theresults of the second approach as the baseline in the next section.3.3 Inter-Annotator Agreement ExperimentsIn order to see how effective our guidelines are, we conducted experiments to comparethe inter-annotator agreement before and after annotators read the guidelines.The data for the evaluation was split into two sets.
The first set contained 13 articles(521 events, 1,563 annotated durations) which were all political and disaster newsstories from ABC, APW, CNN, PRI, and VOA.
The annotators annotated independentlyFigure 3Distribution of means of annotated durations.737Computational Linguistics Volume 37, Number 4Figure 4Distribution of widths of annotated durations.before reading the guidelines.
The annotators were only given short instructions on whatto annotate and one sample article with annotations.
The second set (test set) contained5 articles (125 events, 375 annotated durations) that were also political and disasternews stories from the same news sources.
The annotators annotated independently afterreading the guidelines.The comparison is shown in Figure 5.
Agreement is measured by the area of overlapin two distributions and is thus a number between 0 and 1.
The graphs show the answerto the question ?If we set the threshold for agreement at x, counting everything abovex as agreement, what is the percentage y of inter-annotator agreement??
The horizontalaxis represents the overlap thresholds, and the vertical axis represents the agreementpercentage, that is, the percentage of annotated durations that agree for given overlapthresholds.
There are three lines in the graph.
The top one (with circles) representsthe after-guidelines agreement; the middle one (with triangles) represents the before-guidelines agreement; and the lowest one (with squares) represents the expected orbaseline agreement.
This graph shows that, for example, if we define agreement to bea 10% overlap or better (an overlap threshold of 0.1), we can get 0.8 agreement afterreading the guidelines, 0.72 agreement before reading the guidelines, and 0.36 expectedagreement with only the knowledge of the global distribution.
From this graph, we cansee that our guidelines are indeed effective in improving the inter-annotator agreement.Table 1 shows more detailed experimental results.
For each overlap threshold, itshows the expected or baseline agreement, the before-guidelines agreement, and theafter-guidelines agreement, as well as the kappa statistic computed from the after-738Pan, Mulkar-Mehta, and Hobbs Annotating and Learning Event Durations in TextFigure 5Inter-annotator agreement: Expected, before-guidelines, and after-guidelines.guidelines agreement (P(A)) and the expected or baseline agreement (P(E)).
The agree-ment actually gets marginally worse when the agreement criteria is very stringent (i.e.,overlap ?
0.9), which indicates there really is no consensus at that level of agreement.The overall agreement is relatively low.
Thus in this article, we mainly focus on learn-ing coarse-grained event durations with much higher inter-annotator agreement.
SeeSections 5.2 and 5.3 for more details.Table 1Inter-annotator agreement with different overlap thresholds.Overlap Expected BeforeG.
AfterG.
KappaThreshold Agreement Agreement Agreement (AfterG.
A.
)0.1 0.36 0.72 0.80 0.690.2 0.28 0.59 0.70 0.580.3 0.22 0.52 0.67 0.580.4 0.17 0.43 0.58 0.490.5 0.12 0.31 0.45 0.380.6 0.08 0.22 0.35 0.290.7 0.05 0.15 0.23 0.190.8 0.02 0.10 0.10 0.080.9 0.01 0.07 0.04 0.031.0 0.00 0.05 0.03 0.03739Computational Linguistics Volume 37, Number 44.
Extending TimeML with Estimated Event DurationsThis section describes the event classes in TimeML and how we can integrate ourannotations of estimated event durations with them.
This can enrich the expressivenessof TimeML, and provide natural language applications that use TimeML with thisadditional implicit event duration information for temporal reasoning.4.1 TimeML and Its Event ClassesTimeML (Pustejovsky et al 2003) is a rich specification language for event and tem-poral expressions in natural language text.
Unlike most previous attempts at eventand temporal specification, TimeML separates the representation of event and temporalexpressions from the anchoring or ordering dependencies that may exist in a given text.TimeML includes four major data structures: EVENT, TIMEX3, SIGNAL, andLINK.
EVENT is a cover term for situations that happen or occur, and also thosepredicates describing states or circumstances in which something obtains or holds true.TIMEX3, which extends TIMEX2 (Ferro 2001), is used to mark up explicit temporalexpressions, such as time, dates, and durations.
SIGNAL is used to annotate sectionsof text, typically function words that indicate how temporal objects are related to eachother (e.g., ?when?, ?during?, ?before?).
The set of LINK tags encode various relationsthat exist between the temporal elements of a document, including three subtypes:TLINK (temporal links), SLINK (subordination links), and ALINK (aspectual links).Our event duration annotations can be integrated into the EVENT tag.
In TimeMLeach event belongs to one of the seven event classes, namely, reporting, perception,aspectual, I-action, I-state, state, and occurrence.
The TimeML annotation guidelines3give detailed descriptions for each of the classes:Reporting.
This class describes the action of a person or an organization declaringsomething, narrating an event, informing about an event, and so forth (e.g., say, report,tell, explain, state).Perception.
This class includes events involving the physical perception of anotherevent (e.g., see, watch, view, hear).Aspectual.
This class focuses on different facets of event history, that is, initiation,reinitiation, termination, culmination, continuation (e.g., begin, stop, finish, continue).I-Action.
An I-Action is an Intensional Action.
It introduces an event argument (whichmust be in the text explicitly) describing an intensional action or situation which doesnot necessarily actually happen but may be only desired or possible (e.g., attempt, try,promise).I-State.
This class of events is similar to the previous class.
It includes states that referto alternative possible worlds (e.g., believe, intend, want).State.
This class describes circumstances in which something obtains or holds true (e.g.,on board, kidnapped, peace).3 http://www.timeml.org/site/publications/timeMLdocs/annguide 1.2.pdf.740Pan, Mulkar-Mehta, and Hobbs Annotating and Learning Event Durations in TextOccurrence.
This class includes all the many other kinds of events describing somethingthat happens or occurs in the world (e.g., die, crash, build, sell).4.2 Integrating Event Duration AnnotationsOur event duration annotations can be integrated into TimeML by adding two moreattributes to the EVENT tag for the lower bound and upper bound duration annotations(e.g., ?lowerBoundDuration?
and ?upperBoundDuration?
attributes).To minimize changes to the existing TimeML specifications caused by the inte-gration, we can try to share as much as possible our event classes as described inSection 2.3 with the existing ones in TimeML.We can see that four event classes are shared with very similar definitions: report-ing, aspectual, state, and action/occurrence.
For the other three event classes that onlybelong to TimeML (perception, I-action, I-state), the I-action and perception classes canbe treated as special subclasses of the action/occurrence class, and the I-state class as aspecial subclass of the state class.There are still three classes that only belong to the event duration annotations (i.e.,multiple, negation, and positive infinite), however.
The positive infinite class can betreated as a special subclass of the state class with a special duration annotation forpositive infinity.Each multiple event has two annotations, one for single events and the other foraggregate events.
Because the single event is usually more likely to be encountered inmultiple documents, and thus the duration of the single event is usually more likelyto be shared and re-used, to simplify the specification we can take only the durationannotation of the single events for the multiple event class, and the single event canbe assigned with one of the seven TimeML event classes.
For example, the ?destroyed?event in the earlier example is assigned with the occurrence class in TimeBank.The events involving negation can be simplified similarly.
Because the eventnegated is usually more likely to be encountered in multiple documents, we can takeonly the duration annotation of the negated event for this class.4.3 Annotation Consistency EvaluationAfter the two corpora are integrated, it would be useful to evaluate how consistentthe temporal relationship annotations are originally in TimeBank and in the newlyintegrated event duration annotations.
Because not all the events in TimeBank areanchored exactly on a time line, for this consistency evaluation we have only evaluatedthe ?includes?
/ ?
is included?
TLINK relationship: If event A includes event B, theduration of event A should be no shorter than the duration of event B.
Because theevent duration annotation is a range, there are three possible relationships betweenthe two duration annotations for event A [a1, a2] and event B [b1, b2]:1.
A strictly includes B if a1 ?
b2 (strictly compatible)2.
A possibly includes B if a1 ?
b2, but a2 ?
b1(softly compatible)3.
A doesn?t include B if a2 < b1(incompatible)We call the case (i) strictly compatible, (ii) softly compatible, and (iii) incompatible.741Computational Linguistics Volume 37, Number 4For this consistency evaluation, one article was randomly picked from each newssource, and for each ?include?
TLINK relationship in the article, one of the threecompatibility labels is assigned based on their definitions.
The result shows that outof a total of 116 ?include?
relationships, 59.5% are strictly compatible, 19.8% are softlycompatible, and 20.7% are incompatible.
We can merge the first two categories ascompatible, which accounts for 79.3%.Most of the incompatible cases are due to different event interpretations and guide-lines for the two corpora.
For example, TimeBank bounds most of the I-State events tothe article time, whereas our annotations usually give them a much longer duration?for example, the event ?appear?
in ?Everyone appears to believe that somehow Cuba isgoing to change,?
and the event ?hope?
in ?The quarantine hopes to staunch the flow ofIraqi oil.
?Aspectual events are another class of events that cause many incompatible cases,including those where multiple interpretations are possible, for example, in ?This isquite an extraordinary story unfolding here,?
the event ?unfolding?
can be interpretedas either the start of the unfolding (TimeBank), or the entire process of the unfolding(duration annotation); Sometimes even when both corpora agree on the interpretationof the event, they may not agree on its duration, for example, in ?But with the task-force investigation just getting under way, officials have been careful not to draw any firmconclusions,?
it is clear that the ?getting?
event is the start of the investigation, but shouldit last momentarily (TimeBank) or for a couple of weeks (duration annotation)?
Despitethe difficulty with this event class, there exists some clear cases, for example, in ?A newEssex County task force began delving Thursday into the slayings of 14 black women over thelast five years in the Newark area,?
it is correct to bound ?began?
to Thursday, whereas theevent ?delving?
should last much longer.5.
Learning Event DurationsIt is highly unlikely that the relatively small amount of data we have annotated sofar could support an automatic classification task at this fine granularity.
But we haveidentified two classification tasks at a coarser granularity that we can hope to do wellon and that have some independent utility.
The first exploits the distribution shownin Figure 3, a bimodal distribution of events classifying them into those lasting lessthan a day and those lasting more than a day.
The second coarse-grained task is theapproximate identification of the temporal unit most likely to be used to describe theduration of the event.In Section 5.1 we describe the features that were used in the machine learning ex-periments.
Section 5.2 describes the experiment on classifying events into those lastingmore or less than a day.
Section 5.3 describes the experiment on identifying the mostappropriate temporal unit for the mean durations.5.1 FeaturesIn this section, we describe the lexical, syntactic, and semantic features that we consid-ered in learning event durations.5.1.1 Local Context.
For a given event, the local context features include a window of ntokens to its left and n tokens to its right, as well as the event itself, for n = {0, 1, 2, 3}.The best n determined via cross validation turned out to be 0, that is, the event itself742Pan, Mulkar-Mehta, and Hobbs Annotating and Learning Event Durations in Textwith no local context.
But we also present results for n = 2 in Section 5.2.3 to evaluatethe utility of local context.A token can be a word or a punctuation mark.
Punctuation marks are not removed,because they can be indicative features for learning event durations.
For example, thequotation mark is a good indication of quoted reporting events, and the duration ofsuch events most likely lasts for seconds or minutes, depending on the length of thequoted content.
However, there are also cases where quotation marks are used for otherpurposes, such as for titles of artistic works.For each token in the local context, including the event itself, three features areincluded: the original form of the token, its lemma (or root form), and its part-of-speech (POS) tag.
The lemma of the token is extracted from parse trees generatedby the CONTEX parser (Hermjakob and Mooney 1997), which includes rich contextinformation in parse trees, and the Brill tagger (Brill 1992) is used for POS tagging.The local context features extracted for the ?signed?
event in the sentence below isshown in Table 2 (with a window size n = 2).
The feature vector is [signed, sign, VBD,the, the, DT, plan, plan, NN, Friday, Friday, NNP, on, on, IN].The two presidents on Friday signed the plan.5.1.2 Syntactic Relations.
The information in the event?s syntactic environment is veryimportant in deciding the durations of events.
For example, there is a difference in thedurations of the ?watch?
events in the phrases ?watch a movie?
and ?watch a bird fly.
?For a given event, both the head of its subject and the head of its object are extractedfrom the parse trees generated by the CONTEX parser.
Similarly to the local contextfeatures, for both the subject head and the object head, their original form, lemma,and POS tags are extracted as features.
When there is no subject or object for an event,?NULL?
is used for the feature values.For the ?signed?
event in The two presidents on Friday signed the plan, the head ofits subject is ?presidents?
and the head of its object is ?plan.?
The extracted syntacticrelation features are shown in Table 3, and the feature vector is [presidents, president,NNS, plan, plan, NN].5.1.3 WordNet Hypernyms.
Events with the same hypernyms may have similar durations.For example, events ?ask?
and ?talk?
both have a direct WordNet (Miller et al 1990)hypernym of ?communicate,?
and most of the time they do have very similar durationsin the corpus.However, closely related events don?t always have the same direct hypernyms.For example, ?see?
has a direct hypernym of ?perceive,?
whereas for ?observe?
oneTable 2Local context features for the ?signed?
event with n = 2 in ?The two presidents on Friday signedthe plan.
?Features Original Lemma POSEvent signed sign VBD1token-after the the DT2token-after plan plan NN1token-before Friday Friday NNP2token-before on on IN743Computational Linguistics Volume 37, Number 4Table 3Syntactic relation features for the ?signed?
event in ?The two presidents on Friday signedthe plan.
?Features Original Lemma POSSubject presidents president NNSObject plan plan NNneeds to go two steps up through the hypernym hierarchy before reaching ?perceive.
?Correlation between events may be lost if only the direct hypernyms of the words areextracted.It is useful to extract the hypernyms not only for the event itself, but also for thesubject and object of the event.
For example, events related to a group of people or anorganization usually last longer than those involving individuals, and the hypernymscan help distinguish such concepts.
The direct hypernyms of nouns are not alwaysgeneral enough for this purpose, but a hypernym at too high a level can be too generalto be useful.
For our learning experiments, we use the first three levels of hypernymsfrom WordNet.Hypernyms are only used for the events and their subjects and objects, not for thelocal context words.
For each level of hypernyms in the hierarchy, it?s possible to havemore than one hypernym, for example, ?see?
has two direct hypernyms, ?perceive?
and?comprehend.?
For a given word, it may also have more than one sense in WordNet.
Insuch cases, as in Gildea and Jurafsky (2002), we only take the first sense of the wordand the first hypernym listed for each level of the hierarchy.
A word disambiguationmodule might improve the learning performance.
But because the features we need arethe hypernyms, not the word sense itself, even if the first word sense is not the correctone, its hypernyms can still be good enough in many cases.
For example, in one newsarticle, the word ?controller?
refers to an air traffic controller, which corresponds to thesecond sense in WordNet, but its first sense (business controller) has the same hypernymof ?person?
(three levels up) as the second sense (direct hypernym).
Because we take thefirst three levels of hypernyms, the correct hypernym is still extracted.When there are fewer than three levels of hypernyms for a given word, its hyper-nym on the previous level is used.
When there is no hypernym for a given word (e.g.,?go?
), the word itself will be used as its hypernyms.
Because WordNet only provideshypernyms for nouns and verbs, ?NULL?
is used for the feature values for a word thatis not a noun or a verb.For the ?signed?
event in The two presidents on Friday signed the plan, the extractedWordNet hypernym features for the event (?signed?
), its subject (?presidents?
), andits object (?plan?)
are shown in Table 4, and the feature vector is [write, communicate,interact, corporate executive, executive, administrator, idea, content, cognition].Table 4WordNet hypernym features for the event (?signed?
), its subject (?presidents?
), and its object(?plan?)
in The two presidents on Friday signed the plan.Feature 1-hyper 2-hyper 3-hyperEvent write communicate interactSubject corporate executive executive administratorObject idea content cognition744Pan, Mulkar-Mehta, and Hobbs Annotating and Learning Event Durations in Text5.2 Learning Coarse-Grained Event DurationsThe distribution of the means of the annotated durations in Figure 3 is bimodal, dividingthe events into those that take less than a day and those that take a day or more.
Thus,in our first machine learning experiment, we have tried to learn this coarse-grained eventduration information as a binary classification task.5.2.1 Inter-Annotator Agreement, Baseline, and Upper Bound.
Before evaluating the perfor-mance of different learning algorithms, we first assess the inter-annotator agreement,the baseline, and the upper bound for the learning task.Table 5 shows the inter-annotator agreement results among three annotators forbinary event durations.
The experiments were conducted on the same data sets as inSection 3.3.
Two kappa values are reported with different ways of measuring expectedagreement (P(E)), that is, whether or not the annotators have prior knowledge of theglobal distribution of the task, as described in Section 3.2.Human agreement is 0.877 and is a good estimate of the upper bound performancefor this binary classification task.
The baseline for the learning task is always takingthe most probable class.
Because 59.0% of the total data is ?long?
events, the baselineperformance is 59.0%.5.2.2 Data.
The original annotated data was translated into a binary classification.
Foreach event annotation, the most likely or mean duration was calculated by averagingthe logs of its lower and upper bound durations.
If its most likely or mean durationwas less than a day (about 11.4 in the natural logarithmic scale), it was assigned to the?short?
event class, otherwise it was assigned to the ?long?
event class.
(Note that theselabels are strictly a convenience and not an analysis of the meanings of ?short?
and?long.?
)We divided the total annotated non-WSJ data (2,132 event instances) into two datasets: a training data set with 1,705 event instances (about 80% of the total non-WSJdata) and a held-out test data set with 427 event instances (about 20% of the totalnon-WSJ data).
The WSJ data (156 event instances) was kept for further test purposes(see Section 5.2.5).5.2.3 Experimental Results (non-WSJ)Learning Algorithms.
Three supervised learning algorithms were evaluated for ourbinary classification task, namely, Support Vector Machines (SVM) (Vapnik 1995), NaiveBayes (NB) (Duda and Hart 1973), and Decision Trees (C4.5) (Quinlan 1993).
The Weka(Witten and Frank 2005) machine learning package was used for the implementation ofthese learning algorithms.
Linear kernel is used for SVM in our experiments.Table 5Inter-annotator agreement for binary event durations.P(A) P(E) Kappa0.877 With global distribution 0.528 0.740Without global distribution 0.500 0.755745Computational Linguistics Volume 37, Number 4Table 6Test performance of three algorithms.Class Algor.
Prec.
Recall F-ScoreShort SVM 0.707 0.606 0.653NB 0.567 0.768 0.652C4.5 0.571 0.600 0.585Long SVM 0.793 0.857 0.823NB 0.834 0.665 0.740C4.5 0.765 0.743 0.754Each event instance has a total of 18 feature values, as described in Section 5.1,for the event only condition, and 30 feature values for the local context condition, whenn = 2.
For SVM and C4.5, all features are converted into binary features (6,665 and 12,502features).Results.
Ten-fold cross validation was used to train the learning models, which werethen tested on the unseen held-out test set, and the performance (including the pre-cision, recall, and F-score4 for each class) of the three learning algorithms is shownin Table 6.
The significant measure is overall precision, and this is shown for thethree algorithms in Figure 6, together with human agreement (the upper bound of thelearning task) and the baseline.We can see that among the three learning algorithms, SVM achieves the best F-scorefor each class and also the best overall precision (76.6%).
Compared with the baseline(59.0%) and human agreement (87.7%), this level of performance is very encouraging,especially as the learning is from such limited training data.Feature Evaluation.
The best performing learning algorithm, SVM, was then usedto examine the utility of combinations of four different feature sets (i.e., event, localcontext, syntactic, and WordNet hypernym features).
The detailed comparison is shownin Table 7.5We can see that most of the performance comes from event word or phrase itself.A significant improvement above that is due to the addition of information about thesubject and object.
Local context does not help and in fact may hurt, and hypernyminformation also does not seem to help.
It is of interest that the most important infor-mation is that from the predicate and arguments describing the event, as our linguisticintuitions would lead us to expect.5.2.4 Learning Performance by Event Class.
It?s useful to compare the learning performancebetween different event classes, and see how they contribute to the overall learningperformance.
We would intuitively expect that some classes of events (e.g., reportingevents) are relatively easier to learn than others.4 F-score is computed as the harmonic mean of the precision and recall: F = (2?Prec?Rec)/(Prec+Rec).5 When all features are used, the results for event and context become the same, which indicates that thesyntactic and hypenym features dominate.746Pan, Mulkar-Mehta, and Hobbs Annotating and Learning Event Durations in TextFigure 6Overall test precision on non-WSJ data.Table 7Feature evaluation with different feature sets using SVM.Class Event Only (n = 0) Event Only Event + Syn(n = 0) + Syntactic + HyperPrec.
Rec.
F Prec.
Rec.
F Prec.
Rec.
FShort 0.742 0.465 0.571 0.758 0.587 0.662 0.707 0.606 0.653Long 0.748 0.908 0.821 0.792 0.893 0.839 0.793 0.857 0.823Overall Prec.
74.7% 78.2% 76.6%Local Context Context Context + Syn(n = 2) + Syntactic + HyperShort 0.672 0.568 0.615 0.710 0.600 0.650 0.707 0.606 0.653Long 0.774 0.842 0.806 0.791 0.860 0.824 0.793 0.857 0.823Overall Prec.
74.2% 76.6% 76.6%Table 8 shows the precision for each TimeML event class for the test set.
For eachevent class, it also includes the total number of event instances, how many of them areas short or long events based on their annotations in the corpus, and the number of errorinstances.Although there is too few data to draw firm conclusions, we can see that theevent classes with the highest precision are aspectual events (e.g., continue, start) andperception events (e.g., see, look), though they don?t contribute much to the overallperformance (total instances are only 14 and 9, respectively).
As we expected, reportingevents perform relatively better than most other event classes, but they don?t actuallycontribute much to the overall performance either (only 39 instances out of a totalof 427 instances).
The biggest event class is occurrence events (240 instances), and itsprecision is not much lower than the overall precision (73.3% vs. 76.5%).
The most747Computational Linguistics Volume 37, Number 4Table 8Learning performance for each event class.Event Class # Events # Short # Long # Error PrecisionAspectual 14 3 11 0 100%Perception 9 0 9 0 100%Reporting 39 38 1 6 84.6%I State 32 8 24 5 84.4%State 61 2 59 13 78.7%Occurrence 240 84 156 64 73.3%I Action 32 17 15 12 62.5%Total 427 152 275 100 76.6%Table 9Test performance on WSJ data.Class Prec.
Rec.
FShort 0.692 0.610 0.649Long 0.779 0.835 0.806Overall Prec.
75.0%difficult event class for learning their durations seems to be I-action events (e.g., try,insist).5.2.5 Test on WSJ Data.
Section 5.2.3 describes the experimental results with the learnedmodel trained and tested on data from the same genre, that is, non-WSJ articles.
In orderto evaluate whether the learned model can perform well on data from different newsgenres, we tested it on the unseen WSJ data (156 event instances).
The performance(including the precision, recall, and F-score for each class) is shown in Table 9.
The pre-cision (75.0%) is very close to the test performance on the non-WSJ data, and indicatesthe significant generalization capacity of the learned model.5.3 Learning the Most Likely Temporal UnitThese encouraging results prompted us to try to learn more fine-grained event durationinformation, namely, the most likely temporal units of event durations (cf.
Rieger?s[1974] ORDERHOURS, ORDERDAYS).For each original event annotation, we can obtain the most likely (mean) durationby averaging its lower and upper bound durations, and assigning it to one of sevenclasses?second, minute, hour, day, week, month, and year?based on the temporal unitof its most likely duration.However, human agreement on this more fine-grained task is low (44.4%).
This isunderstandable.
An annotation of [30 minutes, 1 hour] and [35 minutes, 2 hours] willnot match, even though the area of their overlap is 52.72%.66 A natural logarithmic scale is used for the overlap/agreement computation, as described in Section 3.1.748Pan, Mulkar-Mehta, and Hobbs Annotating and Learning Event Durations in TextTable 10Inter-annotator agreement for most likely temporal unit.P(A) P(E) Kappa0.798 With global distribution 0.151 0.762Without global distribution 0.143 0.764Based on this observation, instead of evaluating the exact agreement between an-notators, an ?approximate agreement?
is computed for the most likely temporal unit ofevents.
In ?approximate agreement,?
temporal units are considered to match if they arethe same temporal unit or an adjacent one.
For example, ?second?
and ?minute?
match,but ?minute?
and ?day?
do not.We conducted an experiment for learning this multi-classification task.
The samedata sets as in the binary classification task were used.
The only difference was that theclass for each instance was now labeled with one of the seven temporal unit classes.The baseline for this multi-classification task was always taking the temporal unitwhich with its two neighbors spans the greatest amount of data.
Because the ?week,??month,?
and ?year?
classes together take up the largest portion (51.5%) of the data,the baseline was always taking the ?month?
class, where both ?week?
and ?year?
werealso considered a match.
Table 10 shows the inter-annotator agreement results for themost likely temporal unit when using ?approximate agreement?.
Human agreement,the upper bound, for this task increases from 44.4% to 79.8%.Ten-fold cross validation was also used to train the learning models, which werethen tested on the unseen held-out test set.
The performance of the three algorithms isshown in Figure 7.
The best performing learning algorithm is again SVM with 67.9%test precision.
Compared with the baseline (51.5%) and human agreement (79.8%), thisagain is a very promising result, especially for a multi-classification task with suchlimited training data.
It is reasonable to expect that when more annotated data becomeavailable, the learning algorithm will achieve higher performance when learning thisand more fine-grained event duration information.Figure 7Overall test precisions for learning most likely temporal unit.749Computational Linguistics Volume 37, Number 4Although the coarse-grained duration information may look too coarse to be useful,computers have no idea at all whether a meeting event takes seconds or centuries, soeven coarse-grained estimates would give it a useful rough sense of how long eachevent may take.
More fine-grained duration information is definitely more desirable fortemporal reasoning tasks.
But coarse-grained durations to a level of temporal units canalready be very useful.
For example, when you want to know how long it takes to learnto use some new software, an answer of ?hours?
or ?months?
is often enough.6.
ConclusionIn the research described in this article, we have addressed a problem?extractinginformation about event durations encoded in event descriptions?that has heretoforereceived very little attention in the field.
It is information that can have a substantialimpact on applications where the temporal placement of events is important.
Moreover,it is representative of a set of problems?making use of the vague information intext?that has largely eluded empirical approaches in the past.
We have explicated thelinguistic categories of the phenomena that give rise to grossly discrepant judgmentsamong annotators, and give guidelines for resolving these discrepancies.
We have alsodescribed a method for measuring inter-annotator agreement when the judgments areintervals on a scale; this should extend from time to other scalar judgments.
Inter-annotator agreement is too low on fine-grained judgments.
However, for the coarse-grained judgments of more than or less than a day, and of approximate agreement ontemporal unit, human agreement is acceptably high.
For these cases, we have shownthat machine-learning techniques achieve encouraging results.This article has also provided the necessary foundation for modeling and learningour most fine-grained duration annotations that are intervals on a scale.
The durationinterval has been modeled using the normal distribution, and the difference betweentwo duration intervals are then the overlap between two distributions.
So in order tolearn this distribution, only two parameters need to be learned, namely, the mean andthe standard deviation of the normal distribution, and the cost function can be definedas a function of distribution overlaps.AcknowledgmentsThis work was supported by the AdvancedResearch and Development Activity(ARDA), now the Disruptive TechnologyOffice (DTO), under DOD/DOI/ARDAContract no.
NBCHC040027.
The authorshave profited from discussions with HoaTrang Dang, Donghui Feng, Kevin Knight,Inderjeet Mani, Daniel Marcu, JamesPustejovsky, Deepak Ravichandran, andNathan Sobo.ReferencesAllen, James F. 1984.
Towards a generaltheory of action and time.
ArtificialIntelligence, 23(2):123?154.Allen, James F. and George Ferguson.
1994.Actions and events in interval temporallogic.
Journal of Logic and Computation,4(5):531?579.Bejan, Cosmin Adrian and Sanda Harabagiu.2010.
Unsupervised event coreferenceresolution with rich linguistic features.
InProceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics,pages 1412?1422, Uppsala, Sweden.Boguraev, Branimir and Rie Kubota Ando.2005.
Timeml-compliant text analysis fortemporal reasoning.
In Proceedings of theInternational Joint Conference on ArtificialIntelligence, pages 997?1003, Edinburgh,Scotland, UK.Brill, Eric.
1992.
A simple rule-based partof speech tagger.
In Applied NaturalLanguage Processing, pages 152?155,Trento, Italy.Carletta, Jean.
1996.
Assessing agreement onclassification tasks: The kappa statistic.Computational Linguistics, 22:249?254.Chittaro, Luca and Angelo Montanari.
2000.Temporal representation and reasoning750Pan, Mulkar-Mehta, and Hobbs Annotating and Learning Event Durations in Textin artificial intelligence: Issues andapproaches.
Annals of Mathematicsand Artificial Intelligence, 28:47?106.Dowty, David.
1979.
Word Meaning andMontague Grammar.
Reidel, Dordrecht.Duda, R. O. and P. E. Hart.
1973.
PatternClassification and Scene Analysis.
John Wiley& Sons, New York.Di Eugenio, Barbara and Michael Glass.2004.
The kappa statistic: A second look.Computational Linguistics, 30(1):95?101.Ferro, Lisa.
2001.
Instruction manual forthe annotation of temporal expressions.MITRE Technical Report MTR01W0000046, June 2001.
The MITRECorporation, Mc Lean, VA.Filatova, Elena and Eduard Hovy.
2001.Assigning time-stamps to event-clauses.In Proceedings of the Workshop on Temporaland Spatial Information Processing -Volume 13, pages 13:1?13:8, Toulouse,France.Fortemps, Philippe.
1997.
Jobshopscheduling with imprecise durations:A fuzzy approach.
IEEE Transactions onFuzzy Systems, 5(1997), pages 557?569.Gildea, Daniel and Daniel Jurafsky.2002.
Automatic labeling of semanticroles.
Computational Linguistics,28(3):245?288.Giorgi, Alessandra and Fabio Pianesi.
1997.Tense and Aspect: From Semantics toMorphosyntax, Oxford University Press,Oxford Studies in Comparative Syntax,Oxford.Godo, Lluis and Llu?
?s Vila.
1995.
Possibilistictemporal reasoning based on fuzzytemporal constraints.
In Proceedings of theInternational Joint Conference on ArtificialIntelligence, pages 1916?1923, Montreal,Quebec, Canada.Han, Benjamin and Alon Lavie.
2004.A framework for resolution of time innatural language.
Transactions on AsianLanguage Information Processing SpecialIssue on Spatial and Temporal InformationProcessing, 3(1):11?32, March.Harabagiu, Sanda and Cosmin Adrian Bejan.2005.
Question answering based ontemporal inference.
In Proceedings of theAAAI-2005 Workshop on Inference forTextual Question Answering, pages 27?34,Pittsburg, PA.Hermjakob, Ulf and Raymond J. Mooney.1997.
Learning parse and translationdecisions from examples with richcontext.
In The 35th Annual Meeting of theAssociation for Computational Linguistics,pages 482?489, Madrid, Spain.Hitzeman, Janet, Marc Moens, and ClaireGrover.
1995.
Algorithms for analysingthe temporal structure of discourse.In Proceedings of the 7th Conference of theEuropean Chapter of the Association forComputational Linguistics, pages 253?260,Dublin, Ireland.Hobbs, Jerry R. 2000.
Half orders ofmagnitude.
In Proceedings of KR-2000Workshop on Semantic Approximation,Granularity, and Vagueness, pages 28?38,Breckenridge, CO.Hobbs, Jerry R. and Vladik Kreinovich.2001.
Optimal choice of granularity incommonsense estimation: Why half ordersof magnitude?
In Proceedings of Joint 9thInformation Fuzzy Systems Association WorldCongress and 20th North American FuzzyInformation Processing Society InternationalConference, July 2001, pages 1343?1348,Vancouver.Hobbs, Jerry R. and Feng Pan.
2004.
Anontology of time for the semantic web.ACM Transactions on Asian LanguageInformation Processing, 3:66?85.Krippendorff, Klaus.
1980.
Content Analysis:An Introduction to Its Methodology.
SagePublications, Beverly Hills, CA.Lapata, Mirella and Alex Lascarides.
2006.Learning sentence-internal temporalrelations.
Journal of AI Research, 27(1),pages 85?117.Madden, Carol J. and Rolf A. Zwaan.
2003.How does verb aspect constrain eventrepresentations?
Memory & Cognition,31:663?672.Mani, Inderjeet and Barry Schiffman.
2007.Temporally anchoring and ordering eventsin news.
Event Recognition in NaturalLanguage, John Benjamins.Mani, Inderjeet, Marc Verhagen, Ben Wellner,Chong Min Lee, and James Pustejovsky.2006.
Machine learning of temporalrelations.
In Proceedings of the JointConference of the International Committee onComputational Linguistics and the Associationfor Computational Liguistics, pages 17?18,Sydney, Australia.Mani, Inderjeet and George Wilson.2000.
Robust temporal processingof news.
In Proceedings of the 38thAnnual Meeting on Association forComputational Linguistics, pages 69?76,Stroudsburg, PA.Miller, George A., Richard Beckwith,Christiane Fellbaum, Derek Gross, andKatherine Miller.
1990.
Wordnet: Anon-line lexical database.
InternationalJournal of Lexicography, 3:235?244.751Computational Linguistics Volume 37, Number 4Moens, Marc and Mark Steedman.
1988.Temporal ontology and temporalreference.
Computational Linguistics,14:15?28.Passonneau, Rebecca J.
1988.
Acomputational model of the semanticsof tense and aspect.
ComputationalLinguistics, 14:44?60.Pustejovsky, James, Patrick Hanks, RoserSauri, Andrew See, Robert Gaizauskas,Andrea Setzer, Dragomir Radev,Beth Sundheim, David Day, Lisa Ferro,and Marcia Lazo.
2003.
The timebankcorpus.
In Proceedings of Corpus Linguistics,pages 647?656, Lancaster, UK.Quinlan, J. Ross.
1993.
C4.5: Programs forMachine Learning.
Morgan KaufmannPublishers Inc., San Francisco, CA.Rieger, Charles J.
1974.
Conceptualmemory: A theory and computer programfor processing and meaning content ofnatural language utterances.
StanfordArtificial Intelligence Laboratory Memo,Computer Science Department,Stanford University.Siegel, S. and N. J. Castellan.
1988.Nonparametric statistics for the behavioralsciences.
McGraw?Hill, Inc., 2nd edition,London, UK.Smith, Carlota.
2005.
Aspectual entities andtense in discourse.
Aspectual Inquiries,pages 223?238, Kluwer, Dordrecht.Tao, Cui, Harold R. Solbrig, Deepak K.Sharma, Wei-Qi Wei, Guergana K. Savova,and Christopher G. Chute.
2010.Time-oriented question answering fromclinical narratives sing semantic-webtechniques.
In Proceedings of the 9thInternational Semantic Web Conferenceon the Semantic Web - Volume Part II,pages 241?256, Berlin.Vapnik, Vladimir N. 1995.
The Nature ofStatistical Learning Theory.
Springer-Verlag,New York, NY.Vendler, Zeno.
1967.
Linguistics inphilosophy.
Aspectual Inquiries.
CornellUniversity Press, Ithaca, NY.Witten, I. H. and E. Frank.
2005.
DataMining: Practical Machine LearningTools and Techniques.
Morgan Kaufmann,2nd edition, San Francisco, CA.Zhou, Li and George Hripcsak.
2007.Temporal reasoning with medicaldata?a review with emphasis onmedical natural language processing.Journal of Biomedical Informatics,40(2):183?202.Zhou, Qing and Qing Zhou Richard Fikes.2002.
A reusable time ontology.
InProceedings of the AAAI Workshop onOntologies for the Semantic Web, theEighteenth National Conference on ArtificialIntelligence WS-02-11, Edmonton,Alberta, Canada.752
