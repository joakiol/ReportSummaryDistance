Proceedings of the BioNLP Workshop on Linking Natural Language Processing and Biology at HLT-NAACL 06, pages 73?80,New York City, June 2006. c?2006 Association for Computational LinguisticsExploring Text and Image Features to Classify Images in Bioscience Lit-eratureBarry Rafkind Minsuk Lee Shih-Fu Chang Hong YuDVMM Group Department of Health Sci-encesDVMM Group Department of Health Sci-encesColumbia University University of Wisconsin-MilwaukeeColumbia University University of Wisconsin-MilwaukeeNew York, NY 10027 Milwaukee, WI  53201 New York, NY 10027 Milwaukee, WI  53201Barryr@ee.columbia.eduMinsuk.Lee@gmail.comSfchang@ee.columbia.eduHong.Yu @uwm.eduAbstractA picture is worth a thousand words.Biomedical researchers tend to incorpo-rate a significant number of images (i.e.,figures or tables) in their publications toreport experimental results, to present re-search models, and to display examples ofbiomedical objects.
Unfortunately, thiswealth of information remains virtuallyinaccessible without automatic systems toorganize these images.
We explored su-pervised machine-learning systems usingSupport Vector Machines to automaticallyclassify images into six representativecategories based on text, image, and thefusion of both.
Our experiments show asignificant improvement in the average F-score of the fusion classifier (73.66%) ascompared with classifiers just based onimage (50.74%) or text features (68.54%).1 IntroductionA picture is worth a thousand words.
Biomedicalresearchers tend to incorporate a significant num-ber of figures and tables in their publications toreport experimental results, to present researchmodels, and to display examples of biomedicalobjects (e.g., cell, tissue, organ and other images).For example, we have found an average of 5.2 im-ages per biological article in the journal Proceed-ings of the National Academy of Sciences (PNAS).We discovered that 43% of the articles in themedical journal The Lancet contain biomedicalimages.
Physicians may want to access biomedicalimages reported in literature for the purpose ofclinical education or to assist clinical diagnoses.For example, a physician may want to obtain im-ages that illustrate the disease stage of infants withRetinopathy of Prematurity for the purpose ofclinical diagnosis, or to request a picture of ery-thema chronicum migrans, a spreading annularrash that appears at the site of tick-bite in Lymedisease.
Biologists may want to identify the ex-perimental results or images that support specificbiological phenomenon.
For example, Figure 1shows that a transplanted progeny of a single mul-tipotent stem cell can generate sebaceous glands.Organizing bioscience images is not a new task.Related work includes the building of domain-specific image databases.
For example, the ProteinData Bank (PDB) 1  (Sussman et al, 1998) stores3-D images of macromolecular structure data.WebPath 2  is a medical web-based resource thathas been created by physicians to include over4,700 gross and microscopic medical images.
Text-based image search systems like Google ignoreimage content.
The SLIF (Subcellular LocationImage Finder) system (Murphy et al, 2001; Kou etal., 2003) searches protein images reported in lit-erature.
Other work has explored joint text-imagefeatures in classifying protein subcellular locationimages (Murphy et al, 2004).
The existing sys-tems, however, have not explored approaches thatautomatically classify general bioscience imagesinto generic categories.1http://www.rcsb.org/pdb/2http://www-medlib.med.utah.edu/WebPath/webpath.html73Classifying images into generic categories is animportant task that can benefit many other naturallanguage processing and image processing tasks.For example, image retrieval and question answer-ing systems may return ?Image-of-Thing?
images(e.g., Figure 1), not the other types (e.g., Figure2~5), to illustrate erythema chronicum migrans.Biologists may examine ?Gel?
images (e.g., Figure2), rather than ?Model?
(e.g., Figure 4) to accessspecific biological evidence for molecular interac-tions.
Furthermore, a generic category may easethe task of identifying specific images that may besub-categories of the generic category.
For exam-ple, a biologist may want to obtain an image of aprotein structure prediction, which might be a sub-category of ?Model?
(Figure 4), rather than an im-age of x-ray crystallography that can be readilyobtained from the PDB database.This paper represents the first study that definesa generic bioscience image taxonomy, and ex-plores automatic image classification based on thefusion of text and image classifiers.Gel-Image consists of gel images such as Northern(for DNA), Southern (for RNA), and Western (forprotein).
Figure 2 shows an example.Graph consists of bar charts, column charts, linecharts, plots and other graphs that are drawn eitherby authors or by a computer (e.g., results of patchclamping).
Figure 3 shows an example.Image-of-Thing refers to images of cells, cellcomponents, tissues, organs, or species.
Figure 1shows an example.Mix refers to an image (e.g., Figure 5) that incor-porates two or more other categories of images.Model: A model may demonstrate a biologicalprocess, molecular docking, or an experimentaldesign.
We include as Model any structure (e.g.,chemical, molecular, or cellular) that is illustratedby a drawing.
We also include gene or protein se-quences and sequence alignments, as well as phy-logenetic trees in this category.
Figure 4 shows oneexample.Table refers to a set of data arranged in rows andcolumns.Table 1.
Bioscience Image Taxonomy2 Image TaxonomyWe downloaded from PubMed Central  a total of17,000 PNAS full-text articles (years 1995-2004),which contain a total of 88,225 images.
We manu-ally examined the images and defined an imagetaxonomy (as shown in Table 1) based on feedbackfrom physicians.
The categories were chosen tomaintain balance between coherence of content ineach category and the complexity of the taxonomy.For example, we keep images of biological objects(e.g., cells, tissues, organs etc) in one single cate-gory in this experiment to avoid over decomposi-tion of categories and insufficient data inindividual categories.
Therefore we stress princi-pled approaches for feature extraction and classi-fier design.
The same fusion classificationframework can be applied to cases where eachcategory is further refined to include subclasses.Figure 1.
Image of_Thing3  Figure 2.
Gel image4Figure 3.
Graph image5   Figure 4.
Model image6Figure 5.
Mix image73This image appears in the cover page of PNAS 102 (41):14477 ?
14936.4The image appears in the article (pmid=10318918)5The image appears in the article (pmid=15699337)6The image appears in the article (pmid=11504922)7The image appears in the article (pmid=15755809)743 Image ClassificationWe explored supervised machine-learning methodsto automatically classify images according to ourimage taxonomy (Table 1).
Since it is straightfor-ward to distinguish table separately by applyingsurface cues (e.g., ?Table?
and ?Figure?
), we havedecided to exclude it from our experiments.3.1 Support Vector MachinesWe explored supervised machine-learning systemsusing Support Vector Machines (SVMs) whichhave shown to out-perform many other supervisedmachine-learning systems for text categorizationtasks (Joachims, 1998).
We applied the freelyavailable machine learning MATLAB package TheSpider to train our SVM systems (Sable and Wes-ton, 2005; MATLAB).
The Spider implementsmany learning algorithms including a multi-classSVM classifier which was used to learn our dis-criminative classifiers as described below in sec-tion 3.4.A fundamental concept in SVM theory is theprojection of the original data into a high-dimensional space in which separating hyperplanescan be found.
Rather than actually doing this pro-jection, kernel functions are selected that effi-ciently compute the inner products between data inthe high-dimensional space.
Slack variables areintroduced to handle non-separable cases and thisrequires an upper bound variable, C.Our experiments considered three popular ker-nel function families over five different variantsand five different values of C. The kernel functionimplementations are explained in the softwaredocumentation.
We considered kernel functions inthe forms of polynomial, radial basis function, andGaussian.
The adjustable parameter for polynomialfunctions is the order of the polynomial.
For radialbasis function and Gaussian functions, sigma is theadjustable parameter.
A grid search was performedover the adjustable parameter for values 1 to 5 andfor values of C equal to [10^0, 10^1, 10^2, 10^3,10^4].3.2 Text FeaturesPrevious work in the context of newswire imageclassification show that text features in image cap-tions are efficient for image categorization (Sable,2000, 2002, 2003).
We hypothesize that imagecaptions provide certain lexical cues that effi-ciently represent image content.
For example, thewords ?diameter?, ?gene-expression?, ?histogram?,?lane?, ?model?, ?stained?, ?western?, etc arestrong indicators for image classes and thereforecan be used to classify an image into categories.The features we explored are bag-of-words and n-grams from the image captions after processing thecaption text by the Word Vector Tool (Wurst).3.3 Image FeaturesWe also investigated image features for the tasksof image classification.
We started with four typesof image features that include intensity histogramfeatures, edge-direction histogram features, edge-based axis features, and the number of 8-connectedregions in the binary-valued image obtained fromthresholding the intensity.The intensity histogram was created by quantiz-ing the gray-scale intensity values into the range 0-255 and then making a 256-bin histogram for thesevalues.
The histogram was then normalized by di-viding all values by the total sum.
For the purposeof entropy calculations, all zero values in the his-togram are set to one.
From this adjusted, normal-ized histogram, we calculated the total entropy asthe sum of the products of the entries with theirlogarithms.
Additionally, the mean, 2nd moment,and 3rd moment are derived.
The combination ofthe total entropy, mean, 2nd, and 3rd momentsconstitute a robust and concise representation ofthe image intensity.Edge-Direction Histogram (Jain and Vailaya,1996) features may help distinguish images withpredominantly straight lines such as those found ingraphs, diagrams, or charts from other images withmore variation in edge orientation.
The EDH be-gins by convolving the gray-scale image with both3x3 Sobel edge operators (Jain, 1989).
One opera-tor finds vertical gradients while the other findshorizontal gradients.
The inverse tangent of theratio of the vertical to horizontal gradient yieldscontinuous orientation values in the range of ?pi to+pi.
These values are subsequently converted intodegrees in the range of 0 to 179 degrees (we con-sider 180 and 0 degrees to be equal).
A histogramis counted over these 180 degrees.
Zero values inthe histogram are set to one in order to anticipateentropy calculations and then the modified histo-gram is normalized to sum to one.
Finally, the total75entropy, mean, 2nd and 3rd moments are extractedto summarize the EDH.The edge-based axis features are meant to helpidentify images containing graphs or charts.
First,Sobel edges are extracted above a sensitivitythreshold of 0.10 from the gray-scale image.
Thisyields a binary-valued intensity image with 1?soccurring in locations of all edges that exceed thethreshold and 0?s occurring otherwise.
Next, thevertical and horizontal sums of this intensity imageare taken yielding two vectors, one for each axis.Zero values are set to one to anticipate the entropycalculations.
Each vector is then normalized bydividing each element by its total sum.
Finally, wefind the total entropy, mean, 2nd , and 3rd mo-ments to represent each axis for a total of eight axisfeatures.The last image feature under consideration wasthe number of 8-connected regions in the binary-valued, thresholded Sobel edge image as describedabove for the axis features.
An 8-connected regionis a group of edge pixels for which each membertouches another member vertically, horizontally, ordiagonally in the eight adjacent pixel positions sur-rounding it.
The justification for this feature is thatthe number of solid regions in an image may helpseparate classes.A preliminary comparison of various combina-tions of these image features showed that the inten-sity histogram features used alone yielded the bestclassification accuracy of approximately 54% witha quadratic kernel SVM using an upper slack limitof C = 10^4.3.4 FusionWe integrated both image and text features for thepurpose of image classification.
Multi-class SVM?swere trained separately on the image features andthe text features.
A multi-class SVM attempts tolearn the boundaries of maximal margin in featurespace that distinguishes each class from the rest.Once the optimal image and text classifiers werefound, they were used to process a separate set ofimages in the fusion set.
We extracted the marginsfrom each data point to the boundary in featurespace.Thus, for a five-class classifier, each data pointwould have five associated margins.
To make afair comparison between the image-based classifierand the text-based classifier, the margins for eachdata point were normalized to have unit magnitude.So, the set of five margins for the image classifierconstitutes a vector that then gets normalized bydividing each element by its L2 norm.
The same isdone for the vector of margins taken from the textclassifier.
Finally, both normalized vectors areconcatenated to form a 10-dimensional fusion vec-tor.
To fuse the margin results from both classifi-ers, these normalized margins were used to trainanother multi-class SVM.A grid search through parameter space withcross validation identified near-optimal parametersettings for the SVM classifiers.
See Figure 6 forour system flowchart.Figure 6.
System Flow-chart3.5 Training, Fusion, and Testing DataWe randomly selected a subset of 554 figure im-ages from the total downloaded image pool.
Oneauthor of this paper is a biologist who annotatedfigures under five classes; namely, Gel_Image(102), Graph (179), Image_of_Thing (64), Mix(106), and Model (103).These images were split up such that for eachcategory, roughly a half was used for training, aquarter for fusion, and a quarter for testing (seeFigure 7).
The training set was used to train classi-76fiers for the image-based and text-based features.The fusion set was used to train a classifier on topof the results of the image-based and text-basedclassifiers.
The testing set was used to evaluate thefinal classification system.For each division of data, 10 folds were gener-ated.
Thus within the training and fusion data sets,there are 10 folds which each have a randomizedpartitioning into 90% for training and 10% for test-ing.
The testing data set did not need to be parti-tioned into folds since all of it was used to test thefinal classification system.
(See Figure 8).In the 10-fold cross-validation process, a classi-fier is trained on the training partition and thenmeasured for accuracy (or error rate) on the testingpartition.
Of the 10 resulting algorithms, the onewhich performs the best is chosen (or just onewhich ties for the best accuracy).Figure 7.
Image-set Divisions3.6 Evaluation MetricsWe report the widely used recall, precision, and F-score (also known as F-measure) as the evaluationmetrics for image classification.
Recall is the totalnumber of true positive predictions divided by thetotal number of true positives in the set (true pos +false neg).
Precision is the fraction of the numberof true positive predictions divided by the totalnumber of positive predictions (true pos + falsepos).
F-score is the harmonic mean of recall andprecision equal to (C. J. van Rijsbergen, 1979):( )recallprecisionrecallprecision +/**2Figure 8.
Partitioning Method for Training andFusion Datasets4 Experimental ResultsTable 2 shows the Confusion Matrix for the imagefeature classifier obtained from the testing part ofthe training data.
The actual categories are listedvertically and predicted categories are listed hori-zontally.
For instance, of 26 actual GEL images,18 were correctly classified as GEL, 4 were mis-classified as GRAPH, 2 as IMAGE_OF_THING, 0as MIX, and 2 as MODEL.Actual  Predicted CategoriesGel Graph Thing Mix ModelGel 18 4 2 0 2Graph 3 39 0 1 1Img_Thing 1 1 12 2 0Mix 4 17 0 3 3Model 8 13 0 1 3Table 2.
Confusion Matrix for Image Feature Clas-sifierA near-optimal parameter setting for the classi-fier based on image features alone used a polyno-mial kernel of order 2 and an upper slack limit of C= 10^4.
Table 3 shows the performance of imageclassification with image features.
True Positives,False Positives, False Negatives, Precision =TP/(TP+FP), Recall = TP/(TP+FN), and F-score =2 * Precision * Recall / (Precision + Recall).
Ac-cording to the F-score scores, this classifier doesbest on distinguishing IMAGE_OF_THING im-ages.
The overall accuracy = sum of true positives /total number of images = (18+39+12+3+3)/138 =75/138 =  54%.
This can be compared with thebaseline of (3+39+1+1)/138 = 32% if all images77were classified as the most popular category,GRAPH.
Clearly, the image-based classifier doesbest at recognizing IMAGE_OF_THING figures.Category TP FP FN Prec.
Recall FscoreGel 18 16 8 0.529 0.692 0.600Graph 39 35 5 0.527 0.886 0.661Img_Thing 12 2 4 0.857 0.750 0.800Mix 3 4 10 0.429 0.231 0.300Model 3 6 22 0.333 0.120 0.176Table 3.
Precision, Recall, F-score for Image Clas-sifierActual  Predicted CategoriesGel Graph Thing Mix ModelGel 22 2 0 2 0Graph 4 36 0 4 0Img_Thing 0 3 11 1 1Mix 3 9 1 12 2Model 3 5 0 3 14Table 4.
Confusion Matrix for Caption Text Clas-sifierCategory TP FP FN Prec Recall FscoreGel 22 10 4 0.688 0.845 0.758Graph 36 19 8 0.655 0.818 0.727Img_Thing 11 1 5 0.917 0.688 0.786Mix 12 10 15 0.545 0.444 0.489Model 14 3 11 0.824 0.560 0.667Table 5.
Precision, Recall, F-score for CaptionText ClassifierThe text-based classifier excels in finding GEL,GRAPH, and IMAGE_OF_THING images.
Itachieves an accuracy of (22+36+11+12+14)/138 =95/138 = 69%.A near-optimal parameter setting for the fusionclassifier based on both image features and textfeatures used a linear kernel with C = 10.
The cor-responding Confusion matrix follows in Table 6.Actual  Predicted CategoriesGel Graph Thing Mix ModelGel 23 0 0 3 0Graph 2 37 1 2 2Img_Thing 0 1 15 0 0Mix 2 7 1 14 3Model 3 5 0 4 13Table 6.
Confusion Matrix for Fusion ClassifierCategory TP FP FN Prec.
Recall FscoreGel 23 7 3 0.767 0.885 0.822Graph 37 13 7 0.740 0.841 0.787Img_Thing 15 2 1 0.882 0.938 0.909Mix 14 9 13 0.609 0.519 0.560Model 13 5 12 0.722 0.520 0.605Table 7.
Precision, Recall, F-score for FusionClassifierFrom Table 7, it is apparent that the fusion clas-sifier does best on IMAGE_OF_THING and alsoperforms well on GEL and GRAPH.
These aresubstantial improvements over the classifiers thatwere based on image or text feature alone.
AverageF-scores and accuracies are summarized below inTable 8.The overall accuracy for the fusion classifier =sum of true positives / total number of image =(23+37+15+14+13)/138 = 102/138 = 74%.
Thiscan be compared with the baseline of 44/138 =32% if all images were classified as the most popu-lar category, GRAPH.Classifier Average F-score  AccuracyImage 50.74% 54%CaptionText68.54% 69%Fusion 73.66% 74%Table 8.
Comparison of Average F-scores and Ac-curacy among all three Classifiers5 DiscussionIt is not surprising that the most difficult categoryto classify is Mix.
This was due to the fact that Miximages incorporate multiple categories of otherimage types.
Frequently, one other image type thatappears in a Mix image dominates the image fea-tures and leads to its misclassification as the otherimage type.
For example, Figure 9 shows that aMix image was misclassified as Gel_Image.This mistake is forgivable because the imagedoes contain sub-images of gel-images, eventhough the entire figure is actually a mix of gel-images and diagrams.
This type of result highlightsthe overlap between classifications and the diffi-culty in defining exclusive categories.For both misclassifications, it is not easy tostate exactly why they were classified wronglybased on their image or text features.
This lack of78intuitive understanding of discriminative behaviorof SVM classifiers is a valid criticism of the tech-nique.
Although generative machine learningmethods (such as Bayesian techniques or GraphicalModels) offer more intuitive models for explainingsuccess or failure, discriminative models like SVMare adopted here due to their higher performanceand ease of use.Figure 10 shows an example of a MIX figurethat was mislabeled by the image classifier asGRAPH and as GEL_IMAGE by the text classi-fier.
However, it was correctly labeled by the fu-sion classifier.
This example illustrates the value ofthe fusion classifier for being able to improve uponits component classifiers.6 ConclusionsFrom the comparisons in Table 8, we see that fus-ing the results of classifiers based on text and im-age features yields approximately 5%improvement over the text -based classifier alonewith respect to both average F-score and Accuracy.In fact, the F-score improved for all categories ex-cept for MODEL which experienced a 6% drop.The natural conclusion is that the fusion classifiercombines the classification performance from thetext and image classifiers in a complementary fash-ion that unites the strengths of both.7 Future WorkTo enhance the performance of the text features,one may restrict the vocabulary to functionally im-portant biological words.
For example, ?phos-phorylation?
and ?3-D?
are important words thatmight sufficiently separate ?protein function?
from?protein structure?.Further experimentation on a larger image setwould give us even greater confidence in our re-sults.
It would also expand the diversity withineach category, which would hopefully lead to bet-ter generalization performance of our classifiers.Other possible extensions of this work includeinvestigating different machine learning ap-proaches besides SVMs and other fusion methods.Additionally, different sets of image and text fea-tures can be explored as well as other taxonomies.Caption: ?The 2.6-kb HincII XhoI fragment con-taining approximately half of exon 4 and exon 5and 6 was subcloned between the Neo gene andthymidine kinase (Fig.
1 A).
The location of thegenomic probe used to screen for homologous re-combination is shown in Fig.
1 A. Gene Targetingin Embryonic Stem (ES) Cells and Generation ofMutant Mice.
Genomic DNA of resistant cloneswas digested with SacI and hybridized with the 30.9-kb KpnI SacI external probe (Fig.
1 A).
Chi-meric male offspring were bred to C57BL/6J fe-males and the agouti F1 offspring were tested fortransmission of the disrupted allele by Southernblot analysis of SacI-digested genomic DNA byusing the 3 external probe (Fig.
1 A and B).
A 360-bp region, including the first 134 bp of the 275-bpexon 4, was deleted and replaced with the PGKneocassette in the reverse orientation (Fig.
1 A).
Afterselection with G418 and gangciclovir, doubly re-sistant clones were screened for homologous re-combination by Southern blotting andhybridization with a 3 external probe (Fig.
1 A).Offspring were genotyped by Southern blotting ofgenomic tail DNA and hybridized with a 3 externalprobe (Fig.
1 B).
To confirm that HFE / mice donot express the HFE gene product, we performedNorthern blot analyses ?Figure 9.
Above, caption text and image of a MIXfigure mis-classified as GEL_IMAGE by the Fu-sion Classifier79?Conductance properties of store-operated channels inA431 cells.
(a) Store-operated channels in A431 cells,activated by the mixture of 100 mM BAPTA-AM and 1mM Tg in the bath solution, were recorded in c/a modewith 105 mM Ba2+ (Left), 105 mM Ca2+ (Center), and140 mM Na+ (Right) in the pipette solution at mem-brane potential as indicated.
(b) Fit to the unitary cur-rent-voltage relationship of store-operated channels withBa2+ (n = 46), Ca2+ (n = 4), Na+ (n = 3) yielded slopesingle-channel conductance of 1 pS for Ca2+ and Ba2+and 6 pS for Na+.
(c) Open channel probability of store-operated channels (NPomax30) expressed as a functionof membrane potential.
Data from six independent ex-periments in c/a mode with 105 mM Ba2+ as a currentcarrier were averaged at each membrane potential.
(band c) The average values are shown as mean ?
SEM,unless the size of the error bars is smaller than the sizeof the symbols.
?Figure 10.
Above, caption text and image of aMIX figure incorrectly labeled as GRAPH by Im-age Classifier and GEL_IMAGE by the Text Clas-sifierAcknowledgementsWe thank three anonymous reviewers for theirvaluable comments.
Hong Yu and Minsuk Lee ac-knowledge the support of JDRF 6-2005-835.ReferencesAnil K. Jain and A.
Vailaya., August 1996, Image re-trieval using color andshape.
Pattern Recognition, 29:1233?1244Anil K. Jain, Fundamentals of Digital Image Processing,Prentice Hall, 1989C.
J. van Rijsbergen.
Information Retrieval.
Butter-worths, London, second edition, 1979.Joachims T, 1998, Text categorization with support vec-tor machines: Learning with many relevant features.Presented at Proceedings of ECML-98, 10th Euro-pean Conference on Machine LearningKou, Z., W.W. Cohen and R.F.
Murphy.
2003.
Extract-ing Information from Text and Images for LocationProtemics, pp.
2-9.
In ACM SIGKDD Workshop onData Mining in Bioinformatics (BIOKDD).Murphy, R.F., M. Velliste, J. Yao, and P.G.
2001.Searching Online Journals for Fluorescence Micro-scope Images depicting Protein Subcellular LocationPatterns, pp.
119-128.
In IEEE International Sympo-sium on Bio-Informatics and Biomedical Engineering(BIBE).Murphy, R.F., Kou, Z., Hua, J., Joffe, M., and Cohen,W.
2004.
Extracting and structuring subcellular lo-cation information from on-line journal articles: thesubcellular location image finder.
In Proceedings ofthe IASTED International Conference on KnowledgeSharing and Collaborative Engineering (KSCE2004),St. Thomas, US Virgin Islands, pp.
109-114.Sable, C. and V. Hatzivassiloglou.
2000.
Text-basedapproaches for non-tropical image categorization.International Journal on Digital Libraries.
3:261-275.Sable, C., K. McKeown and K. Church.
2002.
NLPfound helpful (at least for one text categorizationtask).
In Proceedings of Empirical Methods in Natu-ral Language Processing (EMNLP).
Philadelphia, PASable, C. 2003.
Robust Statistical Techniques for theCategorization of Images Using Associated Text.
InComputer Science.
Columbia University, New York.Sussman J.L., Lin D., Jiang J., Manning N.O., PriluskyJ., Ritter O., Abola E.E.
(1998) Protein Data Bank(PDB): Database of Three-Dimensional Structural In-formation of Biological Macromolecules.
Acta Crys-tallogr D Biol Crystallogr 54:1078-1084MATLAB ?.
The Mathworks Inc.,http://www.mathworks.com/Weston, J., A. Elisseeff, G. BakIr, F. Sinz.
Jan. 26th,2005.
The SPIDER: object-orientated machine learn-ing library.
Version 6.
MATLAB Package.http://www.kyb.tuebingen.mpg.de/bs/people/spider/Wurst, M., Word Vector Tool, Univerist?t Dortmund,http://www-ai.cs.uni-dortmund.de/SOFTWARE/WVTOOL/index.html80
