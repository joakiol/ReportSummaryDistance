Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 714?724,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsMultitask Learning for Adaptive Quality Estimationof Automatically Transcribed UtterancesJos?e G. C. de Souza?
?, Hamed Zamani?, Matteo Negri?, Marco Turchi?, Daniele Falavigna?
?University of Trento, Italy?Fondazione Bruno Kessler, Italy?School of ECE, College of Engineering, University of Tehran, Iran{desouza, negri, turchi, falavi}@fbk.euh.zamani@ut.ac.irAbstractWe investigate the problem of predictingthe quality of automatic speech recognition(ASR) output under the following rigid con-straints: i) reference transcriptions are notavailable, ii) confidence information about thesystem that produced the transcriptions is notaccessible, and iii) training and test data comefrom multiple domains.
To cope with theseconstraints (typical of the constantly increas-ing amount of automatic transcriptions thatcan be found on the Web), we propose adomain-adaptive approach based on multitasklearning.
Different algorithms and strategiesare evaluated with English data coming fromfour domains, showing that the proposed ap-proach can cope with the limitations of previ-ously proposed single task learning methods.1 IntroductionThe variety of applications for large vocabularyspeech recognition technology (LVCSR) is rapidlygrowing.
For instance, automatic transcriptions arenow used, either as-is or as rough material to bechecked and corrected by humans, for captioningand subtitling DVD movies, Youtube videos, TVprograms and recordings in noisy environments suchas meetings and teleconferences.
To enable fur-ther integration in these and other scenarios, the im-provement of the core automatic speech recognition(ASR) technology should go hand in hand with thedevelopment of evaluation methods adequate to ad-dress new needs and constraints.
Indeed, the stan-dard evaluation protocol, based on computing theword error rate of transcription hypotheses againstreference transcripts,1is not always a viable solu-tion.In terms of needs, the aforementioned appli-cations call for efficient and replicable evaluationmethods suitable for real-time processing.
Whilethe availability of manually-created reference tran-scripts is a core ingredient for system development,tuning and lab testing, their use for on-field evalu-ation (i.e.
during the actual use) is impractical forobvious reasons (i.e.
the need of a quick response).In terms of constraints, the problem is that ASRtechnology is often used as a black-box, that is, with-out any knowledge of how the transcriptions are gen-erated.2This calls for techniques capable to esti-mate ASR output quality under the rigid constraintof having, as a basic source of information, only thespoken utterance (the acoustic signal) and the tran-scription itself.
Indeed, the invaluable informationprovided by current confidence estimation methods(e.g.
word posterior probabilities (Evermann andWoodland, 2000; Wessel et al, 2001), consensus de-coding (Mangu et al, 2000) and minimum Bayes-risk decoding (Xu et al, 2010)) is not accessiblewhen evaluating the output of an unknown system.1The word error rate (WER) is the minimum edit distancebetween an hypothesis and the reference transcription.
Editdistance is calculated as the number of edits (word insertions,deletions, substitutions) divided by the number of words in thereference.2For instance, as announced by Google, in 2012 about157 million YouTube videos in 10 languages already fea-tured captions generated by a black-box ASR system(source: http://techcrunch.com/2012/06/15/youtube-launches-auto-captions-in-spanish/).714To cope with these issues, Negri et al (2014)proposed a reference-free ASR quality estimation(QE) method capable to operate both in a glass-box(i.e.
having access to confidence information) andin a black-box fashion (i.e.
without any knowledgeabout the ASR system?s inner workings).
Accordingto the authors, despite the promising evaluation re-sults, the supervised learning approach adopted hasa main limitation: the degradation in performancewhen models are trained on non-homogeneous datathat comes from different domains, speakers, or sys-tems.
However, although empirical evidence of thislimitation is provided, the robustness of ASR QEsystems to the heterogeneity of training and test datais left as an open issue.Filling this gap, which is the goal of this paper,would be a significant step towards real-time ASRoutput evaluation, and its seamless integration in anumber of application frameworks.
Along this di-rection, we propose and evaluate a supervised do-main adaptation technique based on multitask learn-ing (Caruana, 1997).
Our approach aims to exploittraining data coming from different ?domains?
(ina broad sense, e.g.
different genres, speakers, top-ics, styles, etc.)
and to obtain ASR QE models thatare robust to differences with respect to the test data.Experiments are carried out with English data com-ing from four domains, and by comparing differentalgorithms and strategies.Overall, our contributions can be summarized asfollows:?
Multitask learning (MTL) is investigated forthe first time in the ASR QE scenario, as a wayto cope with the dissimilarity between trainingand test data coming from multiple domains.?
The QE problem is approached both as a re-gression (assignment of real-valued quality la-bels) and as a binary classification task (as-signment of ?good?/?bad?
labels according toa given, arbitrary WER threshold).
The lattertask is introduced as a preliminary study.?
Results are thoroughly analyzed, consideringboth the amount of training data coming fromthe different domains and the relative distancebetween their distributions.2 Related WorkIn the ASR field, most prior works that address thereference-free estimation of output quality fall intothe confidence estimation (CE) framework.
In thisframework, the reliability of a transcription is es-timated from the system?s standpoint, that is, as afunction of the process that generated the transcrip-tion (Sukkar and Lee, 1996; Evermann and Wood-land, 2000; Wessel et al, 2001; Sanchis et al, 2012;Seigel, 2013, inter alia).
In CE, the informationavailable to the estimator covers all the aspects ofthe decoding process (e.g.
word posterior probabili-ties, n-best lists, hypotheses density, language modelscores).
Although related to our problem, CE hencebuilds on a strong assumption (i.e.
the ASR systemis known), which does not hold in many situations.Quality estimation, instead, operates in the leastfavorable condition in which, besides the lack of ref-erences, the ASR system is regarded as a ?black-box?.
To our knowledge, the study proposed in (Ne-gri et al, 2014) is the most relevant related workalong this direction.
In their investigation, the au-thors run a set of experiments aimed to predict theWER of automatically transcribed utterances in dif-ferent testing conditions (by varying the distance be-tween training and test data), with different state-of-the-art learning algorithms (all for regression),and with different groups of features (the so called?black-box?
and ?glass-box?
feature groups).
Themajor problem emphasized in their analysis is thestrong dependency between QE models and the de-gree of homogeneity of training and test data.
Fromthe application perspective, this is a severe limita-tion since (as in any other supervised learning set-ting) the similarity of training and test sets is a strongrequirement that should be bypassed (possibly withminimal loss in performance).
This issue, which hasnot been addressed yet, is the starting point of ourinvestigation.Another aspect that so far has been disregardedconcerns the type of estimates that a model shouldreturn.
Indeed, while ASR QE has been explored asa regression task (i.e.
aiming to return real-valuedquality estimates), nothing has been done to ap-proach it as a classification problem (i.e.
assigningquality estimates chosen from two or more classes).In classification mode, we return explicit good/bad715labels based on a fixed, application-dependent qual-ity criterion defined a priori (a threshold set on train-ing data).
Since the way to present the quality esti-mates can have interesting effects on their practicaluse, the impact of the aforementioned learning prob-lem on a supervised classification setting is anotheraspect that deserves investigation and motivates ourwork.3 Multitask Learning for Adaptive ASRQuality EstimationThe problem of dealing with different distributionsbetween training and test data is broadly investi-gated by the machine learning community.
In par-ticular, approaches for dealing with domain driftare proposed within the scope of transfer learning,whose aim is to explore knowledge from one ormore source tasks (henceforth, we use the terms do-main and task interchangeably) and apply it to a tar-get task (Pan and Yang, 2010).
In this paper we usea transfer learning technique called multitask learn-ing (MTL), which explores domain-specific trainingsignals of related tasks to improve model generaliza-tion (Caruana, 1997).MTL is an inductive transfer method that assumesthat the tasks are related and share a certain struc-ture that allows knowledge transfer.
In early works,for instance, these shared structures are the hid-den layers of a neural network (Caruana, 1997).3The authors showed that MTL improves over learn-ing each task in isolation (called single task learn-ing, STL henceforth) for different problems.
Sev-eral approaches to MTL have been proposed andeach makes different assumptions about the struc-ture shared among the tasks.
In this work we ex-plore three different MTL algorithms that deal withtask relatedness in different ways.Before defining each one of the three approaches,we introduce some basic notation previously usedby Chen et al (2011).
In MTL there are K ?
Ntasks and each task k ?
[1,K] has mktraining in-stances {(x1, y1), .
.
.
, (xmk, ymk)}, with xi?
Rdwhere d is the number of features and yi?
R isthe output (the response variable or label).
For each3Another intuitive example of transferable knowledge is thefact that, for some domains, a fraction of the extracted featurescan show a correlated behavior.task, the input features and labels form two differentmatrices X(k)= [x1,(k), .
.
.
,xmk,(k)] and Y(k)=[y1,(k), .
.
.
, ymk,(k)], respectively.
The weights ofthe features for all tasks are represented by matrixW, where each column corresponds to a task andeach row corresponds to a feature.
The functionL(W,X,Y) is the loss function defined for eachalgorithm.
We work with two loss functions:?
Least squares (for regression), defined as(XT(k)Wk?Y(k))2, where k is the task identi-fier and Wkis the k-th column of W;?
Logistic Regression (for classification), definedas log(1 + exp(?Y(k)XT(k)Wk)).MTL Lasso.
This algorithm extends the idea ofthe Lasso (Tibshirani, 1996) to the MTL setting.
InMTL Lasso the `1-norm (the sum of the absolutevalues of the weights vector, given byd?i=1|wi|) isapplied to all the tasks at once (the ||W||1compo-nent in Eq.
1).
The ?
?
[0, 1] parameter controlsthe level of regularization applied to the model.
Inother words, the sparsity of the predicted model iscontrolled via ?
which weights the `1-norm acrossall tasks.minWK?k=1L(Wk,X(k),Y(k)) + ?||W||1(1)MTL L21.
This algorithm (Argyriou et al, 2007)learns a low-dimensional representation of the fea-tures across tasks, and induces sparsity on the fea-ture weights for all the tasks at the same time.
Thisis achieved through the use of a group regularizerthat penalizes the weights matrix W with the `2,1-norm (Eq.
2).
This norm is defined as ||W||2,1=d?i=1||Wi||2, where d is the number of features andWiis the i-th row of W. It is obtained by firstcomputing the 2-norm of each row in W (the fea-tures) and then computing the 1-norm over the re-sulting vector.
The 2-norm of a vector is given by||x||2=??ix2i.
The parameter ?
?
[0, 1] con-trols the regularization applied to the model.
MTLL21 assumes that all tasks share the same featurerepresentation.716minWK?k=1L(Wk,X(k),Y(k)) + ?||W||2,1(2)Robust MTL.
This algorithm does not assumethat all the tasks share the same feature representa-tion as the previous two algorithms do (Chen et al,2011).
Moreover, RMTL uses two different struc-tures: one for grouping related tasks to share knowl-edge; the other for identifying irrelevant tasks andkeeping them in a different group that does not shareinformation with the first one.
This is to cope withsituations in which, since tasks are not related, neg-ative transfer of information across tasks might oc-cur, thus harming the generalization of the model.The algorithm approximates task relatedness via alow-rank structure and identifies outlier tasks us-ing a group-sparse structure (column-sparse, at tasklevel).
RMTL minimizes the expression described in3.
It employs a non-negative linear combination ofthe trace norm (the task relatedness component L)and a column-sparse structure induced by the `1,2-norm (the outlier task detection component S).
If atask is an outlier it will have non-zero entries in S.minWK?k=1L(Wk,X(k),Y(k))+?l||L||?+?s||S||1,2(3)In Eq.
3 W is subject to L+S, where ||.||?is thetrace norm, given by the sum of the singular values?iof W, and ||S||1,2is the group regularizer thatinduces sparsity on the tasks.
It is obtained by firstcomputing the `1-norm over the columns of W andthen applying the `2-norm over the resulting vector.The ?land ?sparameters control the level of regu-larization of L and S, respectively.All the MTL algorithms presented in this sectionare linear, with different regularization terms.
WhileRMTL is only defined for regression, the other algo-rithms are defined for both regression and classifica-tion.4 Experimental SettingOur experiments aim to measure the capability ofMTL methods to learn across different domains.
Tothis aim, the algorithms4previously described are4In our experiments we used the implementations availablein the Malsar toolkit (Zhou et al, 2012)compared with the STL baseline, both in regressionand in binary classification.
Given a set of (signal,transcription, WER) tuples as training instances, ourtask is to label new unseen (signal, transcription)test pairs with a WER prediction (regression mod-els) or with a good/bad tag (classification models)depending on the quality of the transcription.In classification, the class boundary is defined apriori, according to an arbitrary threshold ?
set onthe WER of the instances: those with a WER ?
?will be considered as positive examples while theothers will be considered as negative examples.
Dif-ferent thresholds can be set to experiment with test-ing conditions that reflect a variety of application-oriented requirements.
We work at one extreme, inwhich a value of ?
close to zero (0.05) emphasizessystems?
ability to precisely identify high-qualitytranscriptions (those withWER ?
?
).
Any applica-tion that requires precise judgments to isolate high-quality ASR output can potentially benefit of suchoptimization (e.g.
data selection for acoustic model-ing using a QE-based active learning model).
Theinvestigation of other thresholding schemes, how-ever, is certainly an aspect that we want to explorein the future.The small value of ?
selected produces a skeweddistribution of classes, with a ratio of good to badlabels across the four domains of about 75% ?good?and 25% ?bad?.
To cope with this issue, we use asample weighting technique while training the clas-sification models (Veropoulos et al, 1999).
We as-sign a weight w to each of the training instances,computed as the inverse of its class frequency in thetraining set.
In other words, w is obtained by di-viding the total number of training samples by thenumber of training samples belonging to the class ofthe given utterance.4.1 DataOur datasets include English audio recordings fromfour different domains: broadcast news (henceforthNews), political speeches (Legal), weather reports(Weather) and talks of single speakers in the con-text of the TED talks (TED).
All datasets (see Ta-ble 1 for details) were used in past ASR evaluationcampaigns, and are provided with manual referencetranscriptions associated to each audio recording.717News Legal Weather TEDTotal dur.
(min) 150 338 108 340# running words 26,282 53,846 23,722 41,545# utterances 737 2,922 1,290 2,245# speakers 178 95 36 28Avg.
utt.
dur.
(s) 12.2 6.9 5.0 9.1WER 17.7 20.4 11.9 22.9Table 1: Some characteristics of the four domains.News.
We use the HUB45corpus, which contains104 hours of broadcasts from different television andradio networks.
We selected the 1999 test set of theDARPA Hub-4 evaluation, consisting of two record-ings acquired in TV studios and containing speechof professional speakers reading news.Legal.
This audio database6contains recordingsof European Parliament members speaking in ple-nary sessions, as well as recordings of interpreters(non-native speakers).
Speech is hence quite sponta-neous, and a relevant level of reverberation is presentdue to the usage of table-mounted microphones.
Thedata that we used for our experiments are both theEnglish EPPS development (dev06) and evaluation(eval07) sets of the 2007 TC-STAR ASR evaluationcampaign (Hamon et al, 2007).Weather.
This dataset is formed by recordings ofweather reports broadcasted by the BBC EnglishTV channel, and contains both national and localweather forecasts.
There are roughly 50 nativespeakers and the speech is delivered very quickly.Although the speakers are native and the recordingsare performed in a controlled environment, there aresome hesitations, grammar errors or lengthy formu-lations in the recordings which are corrected in thecaptions (which can thus be considered as loose ref-erence transcripts (Mohr et al, 2013)).TED.
This dataset contains audio recordings ofEnglish speakers (28 different talks) and was usedwithin the IWSLT 2013 evaluation campaign (Cet-tolo et al, 2013).
This domain presents large vari-ability of topics (hence a large, unconstrained vocab-ulary), presence of non-native speakers, and a rather5distributed by the Linguistic Data Consortium and avail-able at https://catalog.ldc.upenn.edu/docs/LDC2000S88/6http://catalog.elra.info/product_info.php?products_id=1032informal speaking style.Given their diverse nature, the four domainspresent a big challenge both for ASR and QE sys-tems.
From Table 1 it is possible to grasp severaldifferences among them.
One aspect that reflectssuch differences is the WER of the ASR system weused to transcribe the utterances (described in Sec-tion 4.2).
The lowest WER is for Weather, a do-main in which the speech is planned.
This is alsothe domain with the shortest average utterance dura-tion (5 sec.
), the lowest number of speakers (36) andthe lowest number of running words (23,722).
Thehigher WER achieved on the other domains is dueto the more challenging conditions posed by each ofthem.
TED and News include speeches about un-constrained topics, and their average utterance dura-tions tend to be longer than for the other two do-mains.
News is the shortest domain in durationand the smallest in number of utterances (150 min.for 737 utterances), but has the highest number ofspeakers.
This means that there are very few utter-ances for each speaker, in average, and that both theASR and the QE system must cope with the differ-ences in speech for all these subjects.
Legal presentsthe second largest number of speakers, both nativeand non-native, using a specific terminology on avaried number of topics.4.2 ASR SystemThe ASR engine used in our experiments makesuse of Hidden Markov Models (HMMs) of tri-phone units and of 4-gram back-off language mod-els (LMs).
HMMs are trained on domain-specificsets of audio data.
The HUB4 training corpus is re-leased with ?verbatim?
transcriptions of the audiosignals while, for the other three domains (i.e.
Le-gal, Weather and TED), training data have only as-sociated captions, which are not always exact tran-scriptions of the corresponding audio recordings.
Toextract audio segments with reliable transcriptionswe hence applied a lightly supervised training pro-cedure (Lamel et al, 2001).
This resulted in 67hours of recordings for the Weather domain, 144hours for TED, 164 hours for News and 100 hoursfor Legal.
For LM training, first, a general purposeLM is trained on the Gigaword text corpus (5th ed.
)(Parker et al, 2011) then, it is adapted to all do-mains, using domain specific text data.
Each auto-718matic transcription of the data presented in Table 1is generated with the corresponding word and timeboundaries that are aligned with the reference utter-ances.
This allows us to compute the utterance WERand the features for the various prediction models.4.3 FeaturesOur models are trained with the same 52 ?black-box?
features proposed by Negri et al (2014), whichcan be categorized in three groups: Signal, Hybridand Textual.
The first group aims to capture the diffi-culty to transcribe the input and is extracted by look-ing at the signal segment as a whole.
Hybrid featuresprovide a more fine-grained way to capture the tran-scription difficulty, by linking the signal to the out-put transcription.
Textual features aim to capture theplausibility/fluency of a transcription considering itssurface word information.4.4 Evaluation MetricsRegression.
Our regression models are evaluatedin terms of mean absolute error (MAE).
The MAE, astandard error measure for regression, is the averageof the absolute difference between the prediction y?iof a model and the gold standard response yifor allinstances in the test set.
As it is an error measure,lower values indicate better performance.Classification.
To handle the imbalanced classdistribution, and equally reward the correct classi-fication on both classes, our evaluation is carried outin terms of balanced accuracy (BA ?
the higher thebetter), which is computed as the average of the ac-curacies on the two classes (Brodersen et al, 2010).When the distribution of classes is balanced, BA isequal to the accuracy metric.4.5 BaselinesRegression.
We compare the MTL methodsagainst two baselines.
The first one, simple but oftenhard to beat for regression models, is computed bylabeling all the test instances with the Mean WERvalue calculated on the training set.
The secondbaseline is an STL algorithm trained on data fromthe target domain.
The algorithm that we used (STLElastic henceforth) is the elastic net (Zou and Hastie,2005).
Parameter estimation is performed with 5-fold cross-validation.Classification.
In this setting we also consider twobaselines.
The first one (Majority) is computed bylabeling all the test instances with the most frequentlabel in the training set and, by definition, corre-sponds to a score of 0.5 in terms of balanced accu-racy.
The second classification baseline is the logis-tic regression (STL LogReg henceforth), also knownas maximum entropy algorithm (Hastie et al, 2009).We perform parameter optimization for LogReg us-ing stratified 5-fold cross-validation in a randomizedsearch process (Bergstra and Bengio, 2012).For both STL baselines we selected algorithms7that induce linear models and use the same loss func-tions (least squares for regression and logistic re-gression for classification) of the MTL methods.5 Results and DiscussionTo mitigate the effect of having considerably differ-ent amounts of training data in the four domains,and equally weight their contribution to the learn-ing task, all our models (STL and MTL) are trainedusing the same number of instances from all the do-mains and, at most, half of the data available for thesmallest domain, News (i.e.
362 instances).
To ana-lyze performance variations with different amountsof data, we create subsets of the 362 instances, for10 different sizes ranging from 10% to 100% of theinstances for each domain.8We repeat this process30 times by randomly shuffling all the data avail-able for each domain.
For each of the resultinglearning curves, the plots in this section present theconfidence intervals9(at 95%) for the 30 differenttrain/test splits.In addition to the STL model trained only on in-domain data, we also experiment with an STL modeltrained on the concatenation of the training data ofall domains.
Its results are, on average, statisticallycomparable to, or worse than, STL in-domain forboth regression and classification.Regression.
Among the three MTL regression al-gorithms, RMTL achieves the best results in all our7We used the implementations available in Scikit-learn (Pe-dregosa et al, 2011).8That is, for instance, with 10% of training data from fourdomains, the total amount of instances is 144 (36*4).9The confidence intervals are used to show whether thereare statistically significant differences in performance amongthe models.719Figure 1: Learning curves for the regression models evaluated on the four domains.
The evaluation metric is MAE (?).tests.
This suggests that its capability to handle do-main divergence, thus avoiding negative transfer, isrequired to increase performance.
For the sake ofvisualization, in the plots in Figure 1 we hence omitthe curves of the other MTL methods, keeping onlythose of RMTL and the two baselines.As shown in the figure, for the Legal domain,RMTL results are better than those of both the base-lines (lower MAE) even with 30% of the data and,except in one case (40% of the data), the improve-ment over STL (always the stronger baseline) is sta-tistically significant.
For Weather and TED, the im-provement is less evident: more data are requiredto outperform the STL baseline (respectively 50%and 60%), the improvements are not always statisti-cally significant and, for TED, the MAE results con-verge to those of STL with 100% of the data.
For theNews domain RMTL?s performance is always com-parable to STL.
An interesting behavior can be ob-served in the Legal domain, in which the Mean base-line degrades as we add training data.
This suggeststhat, even internally to the domain, training and testlabels have very different distributions.
A smallerdegradation is observed for the STL model, whichimproves over the Mean baseline as it also uses theinformation captured by the features.
The two base-lines, however, assume that both training and testdata come from similar distributions.
Instead, bytaking advantage also of the knowledge transferredfrom the other domains, RMTL allows to cope withthe differences between training and test.Classification.
In this setting we compare theMTL algorithms (L21 and Lasso) with the STL (Lo-gReg) and Majority baselines.
As shown in Fig-ure 2, the two MTL models (which significantly out-perform the Majority baseline in all conditions) al-ways achieve a higher balanced accuracy than sin-gle task learning in three domains (TED, Legal andWeather).
In the Weather domain, the performanceimprovement over the STL baseline is always statis-tically significant when using from 20% to 100% ofthe training data.
For TED and Legal, MTL perfor-mance tends to converge to the results of STL whenthe models are trained on 100% of the data (around65% BA), with an improvement that remains statis-720Figure 2: Learning curves for the classification models evaluated on four domains for WER scores with threshold at0.05.
Evaluation is calculated with balanced accuracy (?
).tically significant only for TED.
For the News do-main, similar to the regression setting, the improve-ment of MTL over STL is less evident.
Indeed, onlyL21 outperforms the single task baseline but the dif-ference is not statistically significant.Our classification results can be explained takinginto consideration the distribution of positive andnegative instances in each domain.
Weather, forwhich MTL always outperforms STL, has the mostbalanced distribution (35% good and 65% bad).
Inthe other three domains, instead, the proportion ofnegative samples is always above 77%.
Although inthis penalized condition all algorithms are supportedby sample weighting, MTL seems to better exploitthis technique when the target domain is balanced.The challenging nature of the data we are using(described in Section 4.1) is corroborated by themoderate performance achieved by STL.
Althoughit is trained with in-domain data, the best STL clas-sification model (for the Legal domain) does not ex-ceed a BA of 66%.
In this difficult scenario, the use-fulness of MTL is demonstrated by its capability ofreaching the best performance of STL with smalleramounts of data in most of the cases (e.g.
30% ofthe data for the Legal domain).Domains divergence.
To further analyze the per-formance of MTL in regression and classification,following previous works on MTL and domainadaptation in computer vision (Costante et al, 2014;Samanta et al, 2014), we use maximum mean dis-crepancy (MMD) as a measure of divergence be-tween domains.
MMD is an effective way to com-pare two multivariate distributions p and q by mini-mizing the difference in Reproducing Kernel HilbertSpace (RKHS) between the means of the projecteddistributions (Gretton et al, 2012).
It is defined assupf?FEp[f(p)]?Eq[f(q)] where p and q are pointssampled i.i.d.
from two domains and f(.)
is a con-tinuous bounded function on p and q (usually a unitball function).
We measure the pairwise divergencesamong the domains described in Section 4.1 usingthe features extracted and a radial basis function ker-nel.
The divergences are presented in Figure 3.According to the pairwise MMD, the most di-721vergent pair is News-Weather, which is followedby News-Legal.
The distance between News andthe other domains indicates that, when it is used astarget, knowledge transfer from the other domainsmight be problematic.
In fact, looking at the re-sults obtained by classification and regression mod-els for News, we notice that none of the MTL meth-ods achieves significant improvements over the STLbaselines.
Furthermore, the RMTL regression learn-ing curve (Figure 1) for News shows that RMTL fol-lows the same curve of STL, meaning that it is ableto handle the high divergence between News and theother domains and hence, it learns mostly from in-domain data.In general, the divergence measurements betweenthe domains are relatively high (the values are closerto 1 than to 0).
This is not surprising given theintra- and inter-domain variability of speakers andtopics, the different conditions in which speech wasrecorded, and the WER differences across domains.However, the interesting aspect evidenced by themeasurements is that MMD allows to successfullyapproximate such domain differences (and, likely,other more implicit diversity indicators), thus beinga useful instrument to measure domain relatedness.Figure 3: Domains divergence given by MMD (0 meanssimilar and 1 means dissimilar).6 ConclusionWe presented a supervised approach to ASR qual-ity estimation aimed to cope with large differencesbetween training and test data.
To achieve robust-ness and adaptability to such differences, we ex-ploited the capability of multitask learning, whichallows QE models to make the best use of train-ing data coming from multiple domains by trans-ferring knowledge across them.
The MTL learn-ing paradigm was applied both in regression mode(WER prediction) and, in a preliminary inves-tigation, for binary classification (assignment of?good?/?bad?
quality labels).
In both settings, we ex-perimented with different amounts of English datacoming from four very diverse domains (differentgenres, speakers, topics, and styles).Our results indicate that MTL, which we used forthe first time in ASR QE10, is able to take advantageof data coming from such heterogeneous domainsand to significantly improve over single-task learn-ing baselines both in regression and in classification.Although the extent of the improvement depends onthe divergence between the domains (a major is-sue for any supervised learning task), our resultsshow that in the worst case MTL performance con-verges to the results of single-task learning.
Overall,by suggesting a way to overcome the main limita-tions of previous approaches, our study opens in-teresting research avenues towards reference-free,system-agnostic and real-time ASR output evalua-tion.AcknowledgmentsThe work of Hamed Zamani was supported by theFBK-HLT Summer Internship Program 2014.References[Argyriou et al2007] Andreas Argyriou, Theodoros Ev-geniou, and Massimiliano Pontil.
2007.
Multi-taskfeature learning.
In Advances in Neural InformationProcessing Systems 19, pages 41?48.
[Bergstra and Bengio2012] James Bergstra and YoshuaBengio.
2012.
Random Search for Hyper-ParameterOptimization.
Journal of Machine Learning Research,13:281?305.
[Brodersen et al2010] Kay Henning Brodersen,Cheng Soon Ong, Klaas Enno Stephan, and10Previous works (Cohn and Specia, 2013; C. de Souza etal., 2014b) successfully applied similar methods to QE for ma-chine translation (Specia et al, 2009; Mehdad et al, 2012; C. deSouza et al, 2014a; Turchi et al, 2014).722Joachim M. Buhmann.
2010.
The balanced ac-curacy and its posterior distribution.
In Proceedingsof the 2010 20th International Conference on PatternRecognition, ICPR ?10, pages 3121?3124.[C.
de Souza et al2014a] Jos?e G. C. de Souza, Jes?usGonz?alez-Rubio, Christian Buck, Marco Turchi, andMatteo Negri.
2014a.
FBK-UPV-UEdin participa-tion in the WMT14 Quality Estimation shared-task.
InProceedings of the Ninth Workshop on Statistical Ma-chine Translation, pages 322?328.[C.
de Souza et al2014b] Jos?e G. C. de Souza, MarcoTurchi, and Matteo Negri.
2014b.
Machine Transla-tion Quality Estimation Across Domains.
In Proceed-ings of the 25th International Conference on Compu-tational Linguistics, COLING ?14, pages 409?420.
[Caruana1997] Rich Caruana.
1997.
Multitask Learning.Machine learning, 28(28):41?75.
[Cettolo et al2013] Mauro Cettolo, Jan Niehues, Sebas-tian Stuker, Luisa Bentivogli, and Marcello Federico.2013.
Report on the 10th IWSLT Evaluation Cam-paign.
In Proceedings of the International Workshopon Spoken Language Translation.
[Chen et al2011] Jianhui Chen, Jiayu Zhou, and JiepingYe.
2011.
Integrating Low-rank and Group-sparseStructures for Robust Multi-task Learning.
In Pro-ceedings of the 17th ACM SIGKDD International Con-ference on Knowledge Discovery and Data Mining,KDD ?11, pages 42?50.
[Cohn and Specia2013] Trevor Cohn and Lucia Specia.2013.
Modelling Annotator Bias with Multi-taskGaussian Processes: An application to Machine Trans-lation Quality Estimation.
In Proceedings of the 51stAnnual Meeting of the Association for ComputationalLinguistics, pages 32?42.
[Costante et al2014] G. Costante, E. Bellocchio,P.
Valigi, and E. Ricci.
2014.
PersonalizingVision-based Gestural Interfaces for HRI with UAVs:a Transfer Learning Approach.
In Proceedings of the2014 International Conference on Intelligent Robotsand Systems, IROS ?14, pages 3319?3326.
[Evermann and Woodland2000] G. Evermann and P. C.Woodland.
2000.
Large vocabulary decoding andconfidence estimation using word posterior probabil-ities.
In Proceedings of the 2000 International Con-ference on Acoustics, Speech, and Signal Processing,ICASSP ?00, pages 1655?1658.
[Gretton et al2012] Arthur Gretton, Karsten M. Borg-wardt, Malte J. Rasch, Bernhard Scholkopf, andAlexander Smola.
2012.
A kernel two-sample test.Journal of Machine Learning Research, 13:723?773.
[Hamon et al2007] Olivier Hamon, Djamel Mostefa, andKhalid Choukri.
2007.
End-to-End Evaluation ofa Speech-to-Speech Translation System in TC-STAR.In Proceedings of Machine Translation Summit XI,pages 223?230.
[Hastie et al2009] Trevor Hastie, Robert Tibshirani, andJerome Friedman.
2009.
The Elements of StatisticalLearning.
[Lamel et al2001] Lori Lamel, Jean-Luc Gauvain, andGilles Adda.
2001.
Investigating lightly supervisedacoustic model training.
In Proceedings of the 2001International Conference on Acoustics, Speech, andSignal Processing, volume 1 of ICASSP ?01, pages477?480.
IEEE.
[Mangu et al2000] Lidia Mangu, Eric Brill, and AndreasStolcke.
2000.
Finding consensus in speech recog-nition: word error minimization and other applica-tions of confusion networks.
Computer Speech & Lan-guage, 14(4):373?400.
[Mehdad et al2012] Yashar Mehdad, Matteo Negri, andMarcello Federico.
2012.
Match without a Ref-eree: Evaluating MT Adequacy without ReferenceTranslations.
In Proceedings of the Seventh Work-shop on Statistical Machine Translation, pages 171?180, Montr?eal, Canada, June.
Association for Compu-tational Linguistics.
[Mohr et al2013] Christian Mohr, Christian Saam, KevinKilgour, Jonas Gehring, Sebastian St?uker, and AlexWaibel.
2013.
Slightly supervised adaptation ofacoustic models on captioned bbc weather forecasts.In Proceedings of the First Workshop on Speech, Lan-guage and Audio in Multimedia (SLAM), pages 32?36.
[Negri et al2014] Matteo Negri, Marco Turchi, Jos?e G.C.
de Souza, and Daniele Falavina.
2014.
Qual-ity Estimation for Automatic Speech Recognition.In Proceedings of the 25th International Conferenceon Computational Linguistics, COLING ?14, pages1813?1823.
[Pan and Yang2010] Sinno Jialin Pan and Qiang Yang.2010.
A Survey on Transfer Learning.
IEEETransactions on Knowledge and Data Engineering,22(10):1345?1359.
[Parker et al2011] Robert Parker, David Graff, JunboKong, Ke Chen, and Kazuaki Maeda.
2011.
Englishgigaword fifth edition, june.
Linguistic Data Consor-tium, LDC2011T07.
[Pedregosa et al2011] Fabian Pedregosa, Ga?el Varo-quaux, Alexandre Gramfort, Vincent Michel, BertrandThirion, Olivier Grisel, Mathieu Blondel, Peter Pret-tenhofer, Ron Weiss, Vincent Dubourg, Jake Van-derplas, Alexandre Passos, David Cournapeau, Math-ieu Brucher, Mathieu Perrot, and?Edouard Duchesnay.2011.
Scikit-learn : Machine Learning in Python.Journal of Machine Learning Research, 12:2825?2830.723[Samanta et al2014] Suranjana Samanta, Tirumarai A.Selvan, and Sukhendu Das.
2014.
Modeling Sequen-tial Domain Shift through Estimation of Optimal Sub-spaces for Categorization.
In Proceedings of the 2014British Machine Vision Conference.
[Sanchis et al2012] Alberto Sanchis, Alfons Juan, andEnrique Vidal.
2012.
A Word-Based Na?
?ve BayesClassifier for Confidence Estimation in Speech Recog-nition.
IEEE Transactions on Audio, Speech and Lan-guage Processing, 20(2):565?574.
[Seigel2013] Mathew Stephen Seigel.
2013.
Confi-dence Estimation for Automatic Speech RecognitionHypotheses.
Ph.D. thesis, University of Cambridge.
[Specia et al2009] Lucia Specia, Nicola Cancedda, MarcDymetman, Marco Turchi, and Nello Cristianini.2009.
Estimating the Sentence-Level Quality of Ma-chine Translation Systems.
In Proceedings of the 13thAnnual Conference of the EAMT, number May, pages28?35.
[Sukkar and Lee1996] Rafid A. Sukkar and Chin-HuiLee.
1996.
Vocabulary independent discriminative ut-terance verification for nonkeyword rejection in sub-word based speech recognition.
IEEE Transactions onAudio, Speech and Language Processing, 4(6):420?429.
[Tibshirani1996] Rob Tibshirani.
1996.
Regressionshrinkage and selection via the lasso.
Journal of theRoyal Statistical Society.
Series B (Methodological),58(1):267?288.
[Turchi et al2014] Marco Turchi, Antonios Anastasopou-los, Jos?e G. C. de Souza, and Matteo Negri.
2014.Adaptive Quality Estimation for Machine Translation.In Proceedings of the 52nd Annual Meeting of the As-sociation for Computational Linguistics (Volume 1:Long Papers), pages 710?720, Baltimore, Maryland,June.
Association for Computational Linguistics.
[Veropoulos et al1999] K. Veropoulos, C. Campbell, andN.
Cristianini.
1999.
Controlling the sensitivity ofsupport vector machines.
In Proceedings of the In-ternational Joint Conference on Artificial Intelligence,IJCAI ?99, pages 55?60.
[Wessel et al2001] Frank Wessel, Ralf Schluter, KlausMacherey, and Hermann Ney.
2001.
Confidence mea-sures for large vocabulary continuous speech recogni-tion.
IEEE Transactions on Audio, Speech and Lan-guage Processing, 9(3):288?298.
[Xu et al2010] Haihua Xu, Dan Povey, Lidia Mangu, andJie Zhu.
2010.
An improved consensus-like methodfor Minimum Bayes Risk decoding and lattice combi-nation.
In Proceedings of the 2010 International Con-ference on Acoustics Speech and Signal Processing,ICASSP ?10, pages 4938?4941.
[Zhou et al2012] Jiayu Zhou, Jianhui Chen, and JiepingYe.
2012.
MALSAR: Multi-tAsk Learning via Struc-turAl Regularization.
[Zou and Hastie2005] Hui Zou and Trevor Hastie.
2005.Regularization and variable selection via the elasticnet.
Journal of the Royal Statistical Society: SeriesB (Statistical Methodology), 67(2):301?320.724
