Proceedings of the Second Workshop on Metaphor in NLP, pages 18?26,Baltimore, MD, USA, 26 June 2014.c?2014 Association for Computational LinguisticsMetaphor Detection through Term RelevanceMarc SchulderSpoken Language SystemsSaarland UniversitySaarbr?ucken, Germanymarc.schulder@lsv.uni-saarland.deEduard HovyLanguage Technologies InstituteCarnegie Mellon UniversityPittsburth, PA, USAhovy@cs.cmu.eduAbstractMost computational approaches tometaphor detection try to leverage eitherconceptual metaphor mappings or selec-tional preferences.
Both require extensiveknowledge of the mappings/preferencesin question, as well as sufficient data forall involved conceptual domains.
Creatingthese resources is expensive and oftenlimits the scope of these systems.We propose a statistical approach tometaphor detection that utilizes the rarityof novel metaphors, marking words thatdo not match a text?s typical vocabularyas metaphor candidates.
No knowledgeof semantic concepts or the metaphor?ssource domain is required.We analyze the performance of thisapproach as a stand-alone classifier andas a feature in a machine learning model,reporting improvements in F1measureover a random baseline of 58% and 68%,respectively.
We also observe that, asa feature, it appears to be particularlyuseful when data is sparse, while its effectdiminishes as the amount of training dataincreases.1 IntroductionMetaphors are used to replace complicated or un-familiar ideas with familiar, yet unrelated conceptsthat share an important attribute with the intendedidea.
In NLP, detecting metaphors and other non-literal figures of speech is necessary to interprettheir meaning correctly.
As metaphors are a pro-ductive part of language, listing known examplesis not sufficient.
Most computational approachesto metaphor detection are based either on the the-ory of conceptual mappings (Lakoff and John-son, 1980) or that of preference violation (Wilks,1978).Lakoff and Johnson (1980) showed thatmetaphors have underlying mappings betweentwo conceptual domains: The figurative sourcedomain that the metaphor is taken from and theliteral target domain of the surrounding context inwhich it has to be interpreted.
Various metaphorscan be based on the same conceptual metaphormapping, e.g.
both ?The economy is a house ofcards?
and ?the stakes of our debates appearsmall?
match POLITICS IS A GAME.Another attribute of metaphors is that theyviolate semantic selectional preferences (Wilks,1978).
The theory of selectional preference ob-serves that verbs constrain their syntactic argu-ments by the semantic concepts they accept inthese positions.
Metaphors violate these con-straints, combining incompatible concepts.To make use of these theories, extensive knowl-edge of pairings (either mappings or preferences)and the involved conceptual domains is required.Especially in the case of conceptual mappings, thismakes it very difficult for automated systems toachieve appropriate coverage of metaphors.
Evenwhen limited to a single target domain, detectingall metaphors would require knowledge of manymetaphoric source domains to cover all relevantmappings (which themselves have to be known,too).
As a result of this, many systems attemptto achieve high precision for specific mappings,rather than provide general coverage.Many approaches (Gedigian et al., 2006; Krish-nakumaran and Zhu, 2007; Mohler et al., 2013;Tsvetkov et al., 2013, and more) make use of man-ually crafted knowledge bases such as WordNet orFrameNet to establish concept domains.
Other re-cent works establish domains via topic modeling(Shutova et al., 2010; Heintz et al., 2013), ad-hocclustering (Strzalkowski et al., 2013) or by usingsemantic similarity vectors (Hovy et al., 2013).We introduce term relevance as a measure forhow ?out of place?
a word is in a given con-18text.
Our hypothesis is that words will often beout of place because they are not meant literally,but rather metaphorically.
Term relevance is basedon term frequency measures for target domainsand mixed-domain data.
The advantage of thisapproach is that it only requires knowledge of atext?s literal target domain, but none about anysource domains or conceptual mappings.
As itdoes not require sentence structure information,it is also resistant to noisy data, allowing the useof large, uncurated corpora.
While some worksthat utilize domain-mappings circumvent the needfor pre-existing source data by generating it them-selves (Strzalkowski et al., 2013; Mohler et al.,2013), our approach is truly source-independent.We present a threshold classifier that uses termrelevance as its only metric for metaphor detec-tion.
In addition we evaluate the impact of termrelevance at different training sizes.Our contributions are:?
We present a measure for non-literalness thatonly requires data for the literal domain(s) ofa text.?
Our approach detects metaphors indepen-dently of their source domain.?
We report improvements for F1of 58%(stand-alone) and 68% (multi-feature) over arandom baseline.2 Term RelevanceWe hypothesize that novel metaphoric languageis marked by its unusualness in a given context.There will be a clash of domains, so the vocab-ulary will be noticeably different1.
Therefore, anunusual choice of words may indicate metaphoric-ity (or non-literalness, at the least).We measure this fact through a domain-specificterm relevance metric.
The metric consists oftwo features: Domain relevance, which measureswhether a term is typical for the literal target do-main of the text, and common relevance, whichindicates terms that are so commonly used acrossdomains that they have no discriminative power.If a term is not typical for a text?s domain (i.e.1Strongly conventionalized metaphors will not meet thisexpectation, as they have become part of the target domain?svocabulary.
Such metaphors can be easily detected by con-ventional means, such as knowledge bases.
Our concern istherefore focused on novel metaphors.has a low relevance), but is not very common ei-ther, it is considered a metaphor candidate.
Thiscan of course be extended to multiple literal do-mains (e.g.
a political speech on fishing regula-tions will have both governance and maritime vo-cabulary), in which case a word is only consideredas a metaphor if it is untypical for all domains in-volved.2.1 MetricWe base domain relevance on TF-IDF (term fre-quency inverse document frequency), which iscommonly used to measure the impact of a termon a particular document.
Terms with a great im-pact receive high scores, while low scores are as-signed to words that are either not frequent in thedocument or otherwise too frequent among otherdocuments.We adapt this method for domain relevance (dr)by treating all texts of a domain as a single ?doc-ument?.
This new term frequency inverse domainfrequency measures the impact of a term on thedomain.tfdom(t, d) =# of term t in domain d# of terms in domain d(1)idfdom(t) = log# of domains# of domains containing t(2)dr(t, d) = tfdom(t, d)?
idfdom(t) (3)To detect metaphors, we look for terms with lowscores in this feature.
However, due to the natureof TF-IDF, a low score might also indicate a wordthat is common among all domains.
To filter outsuch candidates, we use normalized document fre-quency as a common relevance indicator.cr(t) =# of documents containing t# of documents(4)In theory, we could also use domain frequencyto determine common relevance, as we alreadycompute it for domain relevance.
However, as thisreduces the feature?s granularity and otherwise be-haves the same (as long as domains are of equalsize), we keep regular document frequency.2.2 Generating DomainsWe need an adequate number of documents foreach domain of interest to compute domain rele-vance for it.
We require specific data for the literaldomain(s) of a text, but none for the metaphor?s19source domains.
This reduces the required num-ber of domain data sets significantly without rul-ing out any particular metaphor mappings.We extract domain-specific document collec-tions from a larger general corpus, using the key-word query search of Apache Lucene2, a softwarefor indexed databases.
The keywords of the querysearch are a set of seed terms that are consideredtypical literal terms for a domain.
They can bemanually chosen or extracted from sample data.For each domain we extract the 10,000 highestranking documents and use them as the domain?sdataset.Afterwards, all remaining documents are ran-domly assigned to equally sized pseudo-domaindatasets.
These pseudo-domains allow us to com-pute the inverse of the domain frequency for theTF-IDF without the effort of assigning all docu-ments to proper domains.
The document frequencyscore that will be used as common relevance is di-rectly computed on the documents of the completecorpus.3 DataWe make use of two different corpora.
The first isthe domain-independent corpus required for com-puting term relevance.
The second is an evalua-tion corpus for the governance domain on whichwe train and test our systems.Both corpora are preprocessed using NLTK(Loper and Bird, 2002)3.
After tokenization, stop-words and punctuation are removed, contractionsexpanded (e.g.
we?ve to we have) and numbersgeneralized (e.g.
1990?s to @?s).
The remainingwords are reduced to their stem to avoid data spar-sity due to morphological variation.In case of the domain corpus, we also removedgeneric web document contents, such as HTMLmark-up, JavaScript/CSS code blocks and similarboilerplate code4.3.1 Domain CorpusAs a basis for term relevance, we require a largecorpus that is domain-independent and ideally alsostyle-independent (i.e.
not a newspaper corpus or2http://lucene.apache.org/core/3http://nltk.org4Mark-up and boilerplate removal scripts adaptedfrom http://love-python.blogspot.com/2011/04/html-to-text-in-python.html andhttp://effbot.org/zone/re-sub.htmWikipedia).
The world wide web meets these re-quirements.
However, we cannot use public onlinesearch engines, such as Google or Bing, becausethey do not allow a complete overview of their in-dexed documents.
As we require this provide togenerate pseudo-domains and compute the inversedocument/domain frequencies, we use a precom-piled web corpus instead.ClueWeb095contains one billion web pages,half of which are English.
For reasons of process-ing time and data storage, we limited our experi-ments to a single segment (en0000), containing 3million documents.
The time and storage consid-erations apply to the generation of term relevancevalues during preprocessing, due to the require-ments of database indexing.
They do not affectthe actual metaphor detection process, therefore,we do not expect scalability to be an issue.
AsClueWeb09 is an unfiltered web corpus, spam fil-tering was required.
We removed 1.2 million spamdocuments using the Waterloo Spam Ranking forClueWeb096by Cormack et al.
(2011).3.2 Evaluation CorpusEvaluation of the two classifiers is done with a cor-pus of documents related to the concept of gov-ernance.
Texts were annotated for metaphoricphrases and phrases that are decidedly in-domain,as well as other factors (e.g.
affect) that we will notconcern ourselves with.
The focus of annotationwas to exhaustively mark metaphors, irrespectiveof their novelty, but avoid idioms and metonymy.The corpus is created as part of the MICS:Metaphor Interpretation in terms of Culturally-relevant Schemas project by the U.S. IntelligenceAdvanced Research Projects Activity (IARPA).We use a snapshot containing 2,510 English sen-tences, taken from 312 documents.
Of the 2,078sentences that contain metaphors, 72% containonly a single metaphoric phrase.
The corpus con-sists of around 48k tokens, 12% of which are partsof metaphors.
Removing stopwords and punctua-tion reduces it to 23k tokens and slightly skews thedistribution, resulting in 15% being metaphors.We divide the evaluation data into 80% devel-opment and 20% test data.
All reported results arebased on test data.
Where training data is requiredfor model training (see section 5), ten-fold crossvalidation is performed on the development set.5http://lemurproject.org/clueweb09/6http://plg.uwaterloo.ca/?gvcormac/clueweb09spam/20Subdomain Seed TermsExecutive administer rule govern leadLegislative pass law regulate debate parliamentJudicial judge hearing case rule casesentenceAdministr.
administer manage issue permitsanalyze study facilitate obstructEnforcement enforce allow permit require warmake mandate defeat overcomeEconomy budget tax spend plan financesElection vote campaign canvass elect defeatform party create platformAcceptance government distrust (de)legitimizeauthority reject oppose strike flagprotest pride salute march acceptTable 1: Manually selected seed terms for docu-ment search queries.
The 10k documents with thehighest relevance to the seeds are assigned to thesubdomain cluster.4 Basic ClassificationTo gain an impression of the differentiating powerof tf-idf in metaphor detection, we use a basicthreshold classifier (tc) that uses domain relevance(dr) and common relevance (cr) as its only fea-tures.
Given a word w, a target domain d and twothresholds ?
and ?
:tc(w, d) =??
?metaphor if dr(w, d) < ?and cr(w) < ?literal otherwise(5)In cases where a text has more than one literal do-main or multiple relevant subdomains are avail-able, a word is only declared a metaphor if it isnot considered literal for any of the (sub)domains.4.1 Seed TermsThe threshold classifier is evaluated using two dif-ferent sets of seed terms.
The first set is com-posed of 60 manually chosen terms7from eightgovernance subdomains.
These are shown in table1.
Each subdomain corpus consists of its 10,000highest ranking documents.
We do not subdi-vide the evaluation corpus into these subdomains.Rather, we assume that each sentence belongs to7Terms were chosen according to human understandingof typical terms for governance.
No optimization of the termchoices was performed thereafter.principl financi legisl congress crisicorpor famili middl compani futurcountri global negoti medicaid unitindustri promis polici constitut saveobama health creat capitalist housclinton nation dream american businuclear amend great medicar careeconom million feder recoveri jobcommun potenti polit freedom lawprosper energi elect program newTable 2: The fifty stems with the highest tf-idfscore in the gold data.
Used as seed terms for doc-ument search, generating a single governance do-main.
Stems are listed in no particular order.all eight subdomains8, so a word is only consid-ered a metaphor if it is non-literal for all of them.Preliminary experiments showed that this providesbetter performance than using a single domain cor-pus with more documents.As the first set of seeds is chosen without sta-tistical basis, the resulting clusters might miss im-portant aspects of the domain.
To ensure that ourevaluation is not influenced by this, we also in-troduce a second seed set, which is directly basedon the development data.
As we mentioned insection 3.2, sentences in the MICS corpus werenot only annotated for metaphoric phrases, butalso for such that are decidedly domain-relevant.For example in the sentence ?Our economy is thestrongest on earth?, economy is annotated as in-domain and strongest as metaphor.Based on these annotations, we divide the en-tire development data into three bags of words,one each for metaphor, in-domain and unmarkedwords.
We then compute TF-IDF values for thesebags, as we did for the domain clusters.
The fiftyterms9that score highest for the in-domain bag(i.e.
those that make the texts identifiable as gover-nance texts) are used as the second set of seeds (ta-ble 2).
It should be noted that while the seeds werebased on the evaluation corpus, the resulting termrelevance features were nevertheless computed us-ing clusters extracted from the web corpus.8As our evaluation corpus does not specify secondary do-mains for its texts (e.g.
fishery), we chose not to define anyfurther domains at this point.9Various sizes were tried for the seed set.
Using fiftyterms offered the best performance, being neither too specificnor watering down the cluster quality.
It is also close to thesize of our first seed set.21F1Prec RecRandom 0.222 0.142 0.500All Metaphor 0.249 0.142 1.000T-hold: Manual Seeds 0.350 0.276 0.478T-hold: 50-best Seeds 0.346 0.245 0.591Table 3: Summary of best performing settingsfor each threshold classifier model.
Bold num-bers indicate best performance; slanted bold num-bers: best threshold classifier recall.
All resultsare significantly different from the baselines withp < 0.01.4.2 EvaluationWe evaluate and optimize our systems for the F1metric.
In addition we provide precision and re-call.
Accuracy on the other hand proved an inap-propriate metric, as the prevalence of literal wordsin our data resulted in a heavy bias.
We eval-uate on a token-basis, as half of the metaphoricphrases consist of a single word and less than 15%are more than three words long (including stop-words, which are filtered out later).
Additionally,evaluating on a phrase-basis would have requiredgrouping non-metaphor sections into phrases of asimilar format.Based on dev set performance, we choose a do-main relevance threshold ?
= 0.02 and a commonrelevance threshold ?
= 0.1.
We provide a ran-dom baseline, as well as one that labels all wordsas metaphors, as they are the most frequently en-countered baselines in related works.
Results areshown in table 3.Both seed sets achieve similar F-scores, beatingthe baselines by between 39% and 58%, but theirprecision and recall performance differs notably.Both models are significantly better than the base-line and significantly different from one anotherwith p < 0.01.
Significance was computed for atwo-tailed t-test using sigf (Pad?o, 2006)10.Using manually chosen seed terms results in arecall rate that is slightly worse than chance, butit is made up by the highest precision.
The factthat this was achieved without expert knowledgeor term optimization is encouraging.The classifier using the fifty best governanceterms shows a stronger recall, most likely be-10http://www.nlpado.de/?sebastian/software/sigf.shtmlcause the seeds are directly based on the develop-ment data, resulting in a domain cluster that moreclosely resembles the evaluation corpus.
Preci-sion, on the other hand, is slightly below that of themanual seed classifier.
This might be an effect ofthe coarser granularity that a single domain scoreoffers, as opposed to eight subdomain scores.5 Multi-Feature ClassificationUsing term relevance as the only factor formetaphor detection is probably insufficient.Rather, we anticipate to use it either as a pre-filtering step or as a feature for a more complexmetaphor detection system.
To simulate the latter,we use an off-the-shelf machine learning classifierwith which we test how term relevance interactswith other typical word features, such as part ofspeech.
As we classify all words of a sentence, wetreat the task as a binary sequential labeling task.Preliminary tests were performed with HMM,CRF and SVM classifiers.
CRF performance wasthe most promising.
We use CRFsuite (Okazaki,2007)11, an implementation of conditional randomfields that supports continuous values via scalingfactors.
Training is performed on the developmentset using ten-fold cross validation.We present results for bigram models.
Larger n-grams were inspected, too, including models withlook-ahead functionality.
While they were slightlymore robust with regard to parameter changes,there was no improvement over the best bigrammodel.
Also, as metaphor processing still is a lowresource task for which sufficient training data ishard to come by, bigrams are the most accessibleand representative option.5.1 Training FeaturesWe experimented with different representationsfor the term relevance features.
As they are con-tinuous values, they could be used as continuousfeatures.
Alternatively, they could be representedas binary features, using a cut-off value as for ourthreshold classifier.
In the end, we chose a hy-brid approach where thresholds are used to createbinary features, but are also scaled according totheir score.
Thresholds were again determined onthe dev set and set to ?
= 0.02 and ?
= 0.79.Each domain receives an individual domain rel-evance feature.
There is only a single common rel-11http://www.chokkan.org/software/crfsuite/22F1Prec RecAll Metaphor 0.249 0.142 1.000T-hold: Manual Seeds 0.350 0.276 0.478CRF: Basic 0.187 0.706 0.108CRF: Rel 0.219 0.683 0.130CRF: PosLex 0.340 0.654 0.230CRF: PosLexRel 0.373 0.640 0.263Table 4: Summary of best performing settings foreach CRF model.
Bold numbers indicate best per-formance; slanted bold numbers: best CRF re-call.
All results are significantly different from thebaseline with p < 0.01.evance feature, as it is domain-independent.
Sur-prisingly, we found no noteworthy difference inperformance between the two seed sets (manualand 50-best).
Therefore we only report results forthe manual seeds.In addition to term relevance, we also providepart of speech (pos) and lexicographer sense (lex)as generic features.
The part of speech is auto-matically generated using NLTK?s Maximum En-tropy POS Tagger, which was trained on the PennTreebank.
To have a semantic feature to compareour relevance weights to, we include WordNet?slexicographer senses (Fellbaum, 1998), which arecoarse-grained semantic classes.
Where a wordhas more than one sense, the first was chosen.
If nosense exists for a word, the word is given a senseunknown placeholder value.5.2 Performance EvaluationPerformance of the CRF system (see table 4)seems slightly disappointing at first when com-pared to our threshold classifier.
The best-performing CRF beats the threshold classifier byonly two points of F-score, despite considerablyricher training input.
Precision and recall perfor-mance are reversed, i.e.
the CRF provides a higherprecision of 0.6, but only detects one out of fourmetaphor words.
All models provide stable resultsfor all folds, their standard deviation (about 0.01for F1) being almost equal to that of the baseline.All results are significantly different from thebaseline as well as from each other with p < 0.01,except for the precision scores of the three non-basic CRF models, which are significantly differ-ent from each other with p < 0.05.0 0.05 0.1 0.15 0.20.25 0.3 0.35 0.4200 400 600 800 1000 1200 1400 1600 1800 0 0.05 0.1 0.15 0.20.25 0.3 0.35 0.4F 1 Number of Training Sentences ModelsCRF BasicCRF PosLexThreshold + Relevance+ RelevanceBaselineFigure 1: Performance curves for various training data sizes.
Models with term relevance features (solidlines) outperform models without term relevance (dashed lines) at 1400 sentences.
1800 sentences rep-resent the entire training set.
Baseline (thin line) and best threshold classifier (dotted line) provided forreference.23Adding term relevance provides a consistentboost of 0.025 to the F-score.
This boost, however,is rather marginal in comparison to the one pro-vided by part of speech and lexicographer sense.A possible reason for this could be that the itemweights learned during training correspond tooclosely to our term relevance scores, thus makingthem obsolete when enough training data is pro-vided.
The next section explores this possibilityby comparing different amounts of training data.5.3 Training Size EvaluationWith 2000 metaphoric sentences, the dataset weused was already among the largest annotated cor-pora.
By reducing the amount of training data weevaluate whether term relevance is an efficient fea-ture when data is sparse.
To this end, we repeatour ten-fold cross validations, but withhold someof the folds from each training set.Figure 1 compares the performance of CRF fea-ture configurations with and without term rele-vance.
In both cases adding term relevance out-performs the standard configuration?s top perfor-mance with 400 sentences less, saving about aquarter of the training data.In figure 2 we also visualize the relative gainthat adding term relevance provides.
As one cansee, small datasets profit considerably more fromour metric.
Given only 200 sentences, the PosLexmodel receives 4.7 times the performance gainfrom term relevance it got at at maximum trainingsize.
The basic model has a factor of 6.8.
This sup-ports our assumption that term relevance is similarto the item weights learned during CRF training.As labeled training data is considerably more ex-pensive to create than corpora for term relevance,this is an encouraging observation.6 Related WorkFor a comprehensive review on computationalmetaphor detection, see Shutova (2010).
We limitour discussion to publications that were not cov-ered by the review.
While there are several papersevaluating on the same domain, direct comparisonproved to be difficult, as many works were eitherevaluated on a sentence level (which our data wasinappropriate for, as 80% of sentences containedmetaphors) or did not provide coverage informa-tion.
Another difference was that most evaluationswere performed on balanced datasets, while ourown data was naturally skewed for literal terms.Strzalkowski et al.
(2013) follow a related hy-pothesis, assuming that metaphors lack topical re-latedness to in-domain words while being syntac-tically connected to them.
Instead of using themetaphor candidate?s relevance to a target domaincorpus to judge relatedness, they circumvent the0 %25 %50 %75 %100 %125 %150 %200 400 600 800 1000 1200 1400 1600 18000 %25 %50 %75 %100 %125 %150 %Relative Gain Number of Training SentencesModelsCRF BasicCRF PosLexFigure 2: Relative performance gain of models obtained from addition of term relevance features.24need for pre-existing source data by generatingad-hoc collocation clusters and check whether thetwo highest ranked source clusters share vocab-ulary with the target domain.
Further factors intheir decision process are co-ocurrences in sur-rounding sentences and psycholinguistic image-ability scores (i.e.
how easy it is to form a men-tal picture of a word).
Evaluating on data in thegovernance domain, they achieve an accuracy of71% against an all metaphor baseline of 46%, butreport no precision or recall.Mohler et al.
(2013) and Heintz et al.
(2013)also evaluate on the governance domain.
Ratherthan detecting metaphors at a word-level, both de-tect whether sentences contain metaphors.
Mohleret al.
(2013) compare semantic signatures of sen-tences to signatures of known metaphors.
They,too, face a strong bias against the metaphor labeland show how this can influence the balance be-tween precision and recall.
Heintz et al.
(2013)classify sentences as containing metaphors if theircontent is related to both a target and source do-main.
They create clusters via topic modeling and,like us, use manually chosen seed terms to asso-ciate them with domains.
Unlike our approach,theirs also requires seeds of all relevant source do-mains.
They observe that identifying metaphors,even on a sentence level, is difficult even for ex-perienced annotators, as evidenced by an inter-annotator agreement of ?
= 0.48.Shutova et al.
(2010) use manually annotatedseed sentences to generate source and target do-main vocabularies via spectral clustering.
The re-sulting domain clusters are used for selectionalpreference induction in verb-noun relations.
Theyreport a high precision of 0.79, but have no data onrecall.
Target concepts appearing in similar lexico-syntactic contexts are mapped to the same sourceconcepts.
The resulting mappings are then used todetect metaphors.
This approach is notable for itscombination of distributional clustering and selec-tional preference induction.
Verbs and nouns areclustered into topics and linked through inductionof selectional preferences, from which metaphoricmappings are deduced.
Other works (S?eaghdha,2010; Ritter et al., 2010) use topic modeling to di-rectly induce selectional preferences, but have notyet been applied to metaphor detection.Hovy et al.
(2013) generalize semantic prefer-ence violations from verb-noun relations to anysyntactic relation and learn these in a supervisedmanner, using SVM and CRF models.
The CRFis not the overall best-performing system, butachieves the highest precision of 0.74 against anall-metaphor baseline of 0.49.
This is in linewith our own observations.
While they arguethat metaphor detection should eventually be per-formed on every word, their evaluation is limitedto a single expression per sentence.Our work is also related to that of Sporleder andLi (2009) and Li and Sporleder (2010), in whichthey detect idioms through their lack of seman-tic cohesiveness with their context.
Cohesivenessis measured via co-occurence of idiom candidateswith other parts of a text in web searches.
Theydo not make use of domains, basing their measureentirely on the lexical context instead.7 ConclusionWe have presented term relevance as a non-literalness indicator and its use for metaphor de-tection.
We showed that even on its own, term rel-evance clearly outperforms the baseline by 58%when detecting metaphors on a word basis.We also evaluated the utility of term relevanceas a feature in a larger system.
Results for thiswere mixed, as the general performance of oursystem, a sequential CRF classifier, was lowerthan anticipated.
However, tests on smaller train-ing sets suggest that term relevance can help whendata is sparse (as it often is for metaphor process-ing).
Also, precision was considerably higher forCRF, so it might be more useful for cases wherecoverage is of secondary importance.For future work we plan to reimplement theunderlying idea of term relevance with differentmeans.
Domain datasets could be generated viatopic modeling or other clustering means (Shutovaet al., 2010; Heintz et al., 2013) and should alsocover dynamically detected secondary target do-mains.
Instead of using TF-IDF, term relevancecan be modeled using semantic vector spaces(see Hovy et al.
(2013)).
While our preliminarytests showed better performance for CRF thanfor SVM, such a change in feature representationwould also justify a re-evaluation of our classifierchoice.
To avoid false positives (and thus improveprecision), we could generate ad-hoc source do-mains, like Strzalkowski et al.
(2013) or Shutovaet al.
(2010) do, to detect overlooked literal con-nections between source and target domain.25AcknowledgementsWe would like to thank the reviewers and proof-readers for their valuable input.This research effort was in part supported bythe German Academic Exchange Service (DAAD)scholarship program PROMOS with funds fromthe Federal Ministry of Education and Research(BMBF), awarded by the International Office ofSaarland University as well as by the IntelligenceAdvanced Research Projects Activity (IARPA)via Department of Defense US Army ResearchLaboratory contract number W911NF-12-C-0025.The U.S. Government is authorized to reproduceand distribute reprints for Governmental purposesnotwithstanding any copyright annotation thereon.Disclaimer: The views and conclusions con-tained herein are those of the authors and shouldnot be interpreted as necessarily representing theofficial policies or endorsements, either expressedor implied, of DAAD, BMBF, IARPA, DoD/ARLor the German or U.S. Government.ReferencesGordon V. Cormack, Mark D. Smucker, and CharlesL.
A. Clarke.
2011.
Efficient and effective spamfiltering and re-ranking for large web datasets.
In-formation retrieval, 14(5):441?465.Christiane Fellbaum.
1998. ed.
WordNet: an elec-tronic lexical database.
MIT Press, Cambridge MA,1:998.Matt Gedigian, John Bryant, Srini Narayanan, and Bra-nimir Ciric.
2006.
Catching metaphors.
In Pro-ceedings of the HLT/NAACL-06 Workshop on Scal-able Natural Language Understanding, pages 41?48.Ilana Heintz, Ryan Gabbard, Mahesh Srinivasan, DavidBarner, Donald S Black, Marjorie Freedman, andRalph Weischedel.
2013.
Automatic extraction oflinguistic metaphor with lda topic modeling.
Pro-ceedings of the ACL-13 Workshop on Metaphor,page 58.Dirk Hovy, Shashank Srivastava, Sujay Kumar Jauhar,Mrinmaya Sachan, Kartik Goyal, Huiying Li, Whit-ney Sanders, and Eduard Hovy.
2013.
Identifyingmetaphorical word use with tree kernels.
Proceed-ings of the ACL-13 Workshop on Metaphor, page 52.Saisuresh Krishnakumaran and Xiaojin Zhu.
2007.Hunting elusive metaphors using lexical resources.In Proceedings of the HLT/NAACL-07 Workshop onComputational Approaches to Figurative Language,pages 13?20.
Association for Computational Lin-guistics.George Lakoff and Mark Johnson.
1980.
Metaphorswe live by, volume 111.
University of ChicagoPress.Linlin Li and Caroline Sporleder.
2010.
Using gaus-sian mixture models to detect figurative language incontext.
In Proceedings of NAACL-10, pages 297?300.
Association for Computational Linguistics.Edward Loper and Steven Bird.
2002.
NLTK: Thenatural language toolkit.
In Proceedings of theCOLING/ACL-02 workshop on Interactive presen-tation sessions, pages 63?70.
Association for Com-putational Linguistics.Michael Mohler, David Bracewell, David Hinote, andMarc Tomlinson.
2013.
Semantic signatures forexample-based linguistic metaphor detection.
Pro-ceedings of the ACL-13 Workshop on Metaphor,page 27.Naoaki Okazaki, 2007.
CRFsuite: a fast implementa-tion of conditional random fields (CRFs).Sebastian Pad?o, 2006.
User?s guide to sigf: Signifi-cance testing by approximate randomisation.Alan Ritter, Oren Etzioni, et al.
2010.
A latent dirich-let allocation method for selectional preferences.
InACT-10, pages 424?434.
Association for Computa-tional Linguistics.Diarmuid?O S?eaghdha.
2010.
Latent variable modelsof selectional preference.
In Proceedings of ACL-10, pages 435?444.
Association for ComputationalLinguistics.Ekaterina Shutova, Lin Sun, and Anna Korhonen.2010.
Metaphor identification using verb and nounclustering.
In Proceedings of COLING-10, pages1002?1010.
Association for Computational Linguis-tics.Ekaterina Shutova.
2010.
Models of metaphor in NLP.In Proceedings of ACL-10, pages 688?697.
Associ-ation for Computational Linguistics.Caroline Sporleder and Linlin Li.
2009.
Unsupervisedrecognition of literal and non-literal use of idiomaticexpressions.
In Proceedings of EACL09, pages 754?762.
Association for Computational Linguistics.Tomek Strzalkowski, George Aaron Broadwell, SarahTaylor, Laurie Feldman, Boris Yamrom, SamiraShaikh, Ting Liu, Kit Cho, Umit Boz, Ignacio Cases,et al.
2013.
Robust extraction of metaphors fromnovel data.
Proceedings of the ACL-13 Workshopon Metaphor, page 67.Yulia Tsvetkov, Elena Mukomel, and Anatole Gersh-man.
2013.
Cross-lingual metaphor detection us-ing common semantic features.
Proceedings of theACL-13 Workshop on Metaphor, page 45.Yorick Wilks.
1978.
Making preferences more active.Artificial Intelligence, 11(3):197?223.26
