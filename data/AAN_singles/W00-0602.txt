Some Challenges of Developing Fully-Automated Systems forTaking Audio Comprehension ExamsDav id  D .
Pa lmerThe  MITRE Corporat ion202 Bur l ington RoadBedford,  MA 01730palmer@mitre, orgAbst ractAudio comprehension tests are designed to help eval-uate a listener's understanding of a spoken passageand are frequently a key component of languagecompetency exams.
Just as reading comprehensionexams are proving useful in evaluating text-basedlanguage processing technology, audio comprehen-sion exams can be used to evaluate spoken languageprocessing systems.
In this paper we discuss ome ofthe challenges of developing automated systems fortaking audio comprehension exams.1 In t roduct ionThere is currently interest in using reading compre-hension exams to evaluate natural anguage process-ing (NLP) systems.
Reading comprehension testsare designed to help evaluate a reader's understand-ing of a written passage and are thus an example ofa text-based language processing task.
Audio com-prehension tests, on the other hand, are designed tohelp evaluate a listener's understanding of a spokenpassage and are an example of a spoken languageprocessing task.
These tests are frequently a keycomponent of language competency exams, such asthe Test of English as a Foreign Language (TOEFL)in the United States.In this paper, we focus on some of the futurechallenges of developing fully-automated techniquesfor audio comprehension, i  which the system de-veloped processes the exam passages (and possiblyquestions) from the original audio source.
Audiocomprehension provides an excellent example of anund@rstanding-based evaluation paradigm for speechsystems, in which the emphasis is not solely on"getting all the words right" but rather on usingspeech recognition technology to automatically ac-complish a task with a human benchmark: answer-ing questions about a natural anguage story.
Thetraditional paradigm for spoken language processingtasks, such as audio comprehension, has consistedlargely of applying an existing text-based system tothe hypothesis words output by an automatic speechrecognition (ASR) system, ignoring the fact that in-formation is lost due to recognition errors when mov-ing from text to speech and the possibility that it canbe regained in part via word confidence prediction.We believe that successful approaches to audiocomprehension will tackle the speech problem di-rectly, by avoiding the use of features that are char-acteristic of written text and by explicitly addressingthe problem of speech recognition errors through theuse of smoothing techniques and word confidence in-formation.
Preliminary research in fully-automatedtechniques for reading comprehension, such as theDeep Read system developed by Hirschman et al(1999), has included many standard NLP com-ponents, such as part-of-speech tagging, corefer-ence/pronoun resolution, proper name finding, andmorphological nalysis (stemming).
While the tech-niques that are being developed for reading compre-hension provide a starting point, these techniquescannot be effectively applied to audio comprehensionexams directly, because of the nature of differencesbetween written and spoken language data.
In thispaper we address three specific challenges in devel-oping audio comprehension system:?
Fundamental differences between text-baseddata and spoken language data (Section 2)?
Identifying proper names in "noisy" data (Sec-tions 3)?
Dealing with out-of-vocabulary words (Sections4)In our discussion we will use examples takenfrom television and radio broadcast news, a "found"source of audio passages with a virtually unlimitedvocabulary and a wide range of opportunities foraudio comprehension e~aluation.
All ASR transcrip-tions we use will be actual output from a broadcastnews ASR system with a word error rate of 30%.2 Fundamenta l  Data  D i f fe rencesBeyond the obvious difference between a raw audiosignal and a written text, the type of data outputby a speech recognizer is fundamentally differentfrom text-based ata, even though they both con-sist primarily of words.
On one level, there are im-6portant orthographic differences, since spoken lan-guage transcriptions lack many features present inwritten language.
In addition, there is an inher-ent uncertainty in spoken language transcriptions,which almost always contain word errors.
The de-gree of uncertainty is variable, since the word errorrate (WER) of state-of-the-art speech recognizerscan range from very low (1-5%) to very high (40-50%), depending on the domain (e.g., digit recogni-tion vs. travel dialog vs. broadcast news vs. tele-phone conversations).To illustrate some of the important differences be-tween text-based and spoken language data, Figure 1shows three versions of a sentence from a 1997 CNNnews broadcast.
The first version is the sentenceas it would t~;pically be written.
The second ver-sion is the sentence as it would look as output froma "perfect" ASR system in which all spoken wordsare correctly transcribed (0% WER); we will discusscharacteristics of this version in Section 2.1.
Thethird version shows the actual output of a speechrecognizer with a word error rate of 30%; in addi-tion to the output hypothesis words, this version alsoshows word-level confidence scores.
We will discussthe problems created by the word errors in Section2.2.2.1 Or thograph ic  and Lexical  FeaturesConsider the following (written) questions that maybe asked about a spoken passage containing the ex-ample sentence in Figure 1, all of which can be easilyanswered by humans directly from the written pas-sage:Who has been seeking Mr. Reineck?Whom have German authorities been seeking?How long have German authorities been seekingMr.
Reineck?In comparing the "clean" transcription in Figure1 and the written questions above to the perfect(0% WER) transcription, there are several differ-ences that are immediately evident.
These differ-ences impact the tokenization of the data as well asthe lexical representation of words, which will af-fect the ability of an audio comprehension systemto relate words in a written question 1 to ASR tran-scriptions.Lack of  cap i ta l i zat ion and punctuat ion :  Inmany languages, including English, capitalizationand punctuation, such as periods, commas, and quo-tation marks, provide important information aboutsentence/utterance boundaries and the presence ofproper names (which we will discuss in Section 3).However, ASR output is usually caseless, such that1Spoken questions would first need to be transcribed bythe ASR system and will be addressed in Section 2.2.boundaries and names (e.g., "REINECK") are notas easy to identify as in written language.Most  abbrev ia t ions  are spel led out:  Related tothe lack of punctuation, ASR output does not usu-ally contain abbreviations ("MISTER" vs. "Mr.").Numbers  are spel led out:  Types of numbers inASR data are not as easy to recognize ("NINETEENNINETY TWO" vs. 1992).
Tokenization of num-bers is very different in ASR output, as a singlewritten token like "$163.75" that is easily recogniz-able as a monetary amount can result in a largenumber of ASR output words "ONE HUNDREDSIXTY THREE DOLLARS AND SEVENTY FIVECENTS," which is not immediately identified as asingle quantity.P resence  of  Disf luencies:  Though not presentin this example, spoken language frequently con-tains disfluencies, such as pause fillers ("UH","UM"), word fragments, and repetitions, that arenot present in written language.
For example, theperson reading the passage may actually say "MIS-TER REIN- UH REINECK," making successful pro-cessing of the output more difficult.Effective audio comprehension systems will needto normalize all text-based and spoken languagedata to address orthographic differences.2.2 Uncer ta in ty  in Speech Transcr ip t ionsOne of the primary factors that distinguishes text-based language processing tasks, such as read-ing comprehension, from spoken-language process-ing tasks, such as audio comprehension, is the uncer-tainty inherent in the word sequence output by thespeech recognizer.
The sequence of output words israrely the same as the actual spoken word sequence,due to word substitution, insertion, and deletionerrors.
This uncertainty is clear in the third ver-sion of the sentence in Figure 1; of the eleven spo-ken words, three (27.3%) of the corresponding ASRoutput words are incorrect (SINKING, IS, ARRIV-ING).
Audio comprehension systems that processthis "noisy" third version as if it contained the actualspoken words could not possibly answer any of thesample questions above correctly, since the most im-portant words (MISTER REINECK) are not presentin the output.Hirschman et al (1999) report initial results indeveloping a reading comprehension system using a"bag of words" approach, in which the sentences in apassage that are deemed most likely to contain theanswer are those with the maximum lexical over-lap with the question, without regard for word or-der within the sentence.
Recognition word errorswould obviously adversely affect such an approachapplied to audio comprehension; i  cases where thewords in the answer to the question Were misrecog-nized, the system would be incapable of answering7correctly.
In the case of spoken questions, an addi-tional layer of uncertainty is present since the recog-nizer may output different hypothesis words for thesame word in a question and in a spoken passage;for example, "Reineck" was also misrecognized else-where in the same news story as "RIGHT AT, ""RYAN AND," "RYAN EIGHT," "REINER," and"RUNNING AND.
"One of the possible ways to address this lexicaloverlap problem is to expand the set of candidatewords: rather than restricting processing to the sin-gle best recognizer hypothesis equence, we can al-low the top N hypothesis equences (known as the"N-best list").
In the example of Figure 1, if (SEEK-ING, MISTER, and REINECK) are alternative hy-potheses ~or the incorrect (SINKING, IS, and AR-RIVING) somewhere in the N-best list, the bag Ofwords approach would at least have a chance of an-swering the question correctly.While the bag of words approach is a simple tech-nique providing an initial baseline result, "deeper"understanding of reading (and audio) comprehen-sion passages will require modeling of the sequentialnature of the language.
Statistical anguage mod-eling, an essential component of most state-of-the-art speech recognition systems, seeks to estimatethe probability of the sequence of L spoken words,P(wl...WL).
The language modeling within the ASRsystem contributes to the output word sequence, butthe actual recognizer output is usually not the orig-inal sequence wl...WL, but instead a sequence of Mwords hl...hM, where M may not necessarily be thesame as L and where P(hl...hM) ~ P(Wl...WL).Systems processing ASR output data must there-fore effectively model the difference between theactual sequence Wl...WL and the hypothesized se-quence hl...hM.One way to account for word errors in the ASRoutput sequence hl...hM is by integrating word-levelconfidence scores into the model of the word se-quence.
This word-level confidence score, which is anumber between 0 and 1 produced by many currentautomatic speech recognition systems, is an estimateof the posterior probability that the word outputby an ASR system is actually correct.
As such,it provides us with important information aboutthe output transcription that can assist error detec-tion.
The third version of the sentence in Figure 1also includes the word confidence scores that wereproduced with the output word sequence.
In thisparticular example, the word confidence scores arean excellent indication of the presence of word er-rors, since the three word errors (SINKING, IS, andARRIVING) also have the three lowest confidencescores (.14, .09, and .21).
Unfortunately, thoughconfidence scores are a good indication of correct-ness, it is not always this straightforward to distin-guish the errors from correctly transcribed words.3 Robust  Name F ind ingExtracting entities such as proper names is an im-portant first step in many systems aimed at auto-matic language understanding, and identifying thesetypes of phrases is useful in many language un-derstanding tasks, such as coreference resolution,sentence chunking and parsing, and summariza-tion/gisting.
The targets of proper name find-ing, names of persons, locations, and organizations,are very often the answers to the common "W-questions" Who?
and Where?
A common definitionof the extended name finding task, known as the"named entity" task, also includes numeric phrases,such as dates, times, monetary amounts, and per-cents, which are often the answers to other com-mon questions When?
and How Much?
Identify-ing named entities in passages hould thus help inreading/audio comprehension.
In fact, Hirschman etal.
(1999) report that identifying named entities inreading comprehension passages and questions con-sistently improves the performance of their system,even when the name recognition has an accuracy aslow as 76.5%.
We would expect name recognitionto also be a very important component of any audiocomprehension system.Figure 2 shows an example of the importance ofnames in a news story.
This example again showsthree versions of a sentence from the news.
The firstversion shows the ASR output for a sentence.
Dueto the word errors "OUR STRAWS YEAR BEHINDIT", an audio comprehension system would be un-able to answer most simple questions uch as:Who is shown on a T-shirt with a sledgeham-mer?Where is JSrg Haider from?However, the second version in Figure 2 showsthat if we know that "OUR STRAWS" is a locationphrase and that "YEAR BEHIND IT" is a personphrase (albeit incorrectly transcribed), we could atleast know where in the passage to find the answerto the Who?
and Where?
questions, since the otherwords in the sentence are correctly transcribed.
Thisinformation could be used, for example, to consultother corresponding word sequences in the N-bestlist or word lattice in which the words "Austria'sJSrg Haider" may have been correctly transcribed.In this case "Haider" is an out-of-vocabulary wordand would not be present elsewhere in the N-bestlist; we will discuss this problem in Section 4.3.1 Name f inding techn iquesFinding names in text-based sources uch as news-paper and newswire documents has been a focus ofresearch for many years, and some systems have8reported performance approaching human perfor-mance (96-98%) on the named entity task.
Find-ing names in speech data is a very new topic of re-search, and most previous work has consisted of thedirect application of text-based systems to speechdata, with some minor adaptations.For the range of word error rates common formost large vocabulary ASR systems (< 30%), allthe named entity models we will describe in this sec-tion produce performance between 70-90%.
This iscomparable to or better than the accuracy (76.5%)of the named entity system that Hirschman et al(1999) report improves their reading comprehensionsystem.
However, there is significant room for im-provement of the speech data NE systems.
Previouswork has found that the absence of capitalizationand punctuation information in speech transcrip-tions results in a 2-3% decrease in name finding per-formance(Miller t al., 1999), and this degradation isgreater in the presence of word errors.
The decline inNE performance for text-based systems applied di-rectly to errorful speech data is roughly linear withincrease in WER, although the NE performance de-grades more slowly than the WER, i.e.
each recog-nition error does not result in an NE error.
One ofthe goals of work directly on speech understandingmodels hould be to improve this linear degradation.One example of a trainable text-based systemthat has been applied successfully to speech rec-ognizer output is described by Bikel et a/.
(1999).Each type of entity (person, location, etc.)
to berecognized is represented as a separate state in afinite-state machine.
A bigram language model istrained for each phrase type (i.e., for each state),and Viterbi-style decoding is then used to pro-duce the most likely sequence of phrase labels ina test utterance.
This model incorporates non-overlapping features about the words, such as punc-tuation and capitalization, in a bigram back-off tohandle infrequent or unobserved words.
Specifically,each word is deterministically assigned one of 14non-overlapping features (such as two-digit-number,contains-digit-and-period, capitalized-word, and all-capital-letters), and the back-off distribution de-pends on the assigned feature.
The approach hasresulted in high performance on many text-basedtasks, including English and Spanish newswire texts.Despite the fact that the original model relied heav-ily on text-based features uch as punctuation andcapitalization in the language model back-off, itgives good results on speech data without modify-ing anything but the training material (Miller et al,1999).A closely related statistical approach to named en-tity tagging specifically targeted at speech data wasdeveloped at Sheffield by Gotoh and Renals (2000).In their model, named entity tags are treated as cat-egories associated with words, effectively expandingthe vocabulary, e.g.
a word that might be both a per-son and a place name would be represented with twodifferent lexical items.
An n-gram language modelis trained on these augmented words, using a sin-gle model for joint word/tag dependence on the his-tory rather than the two components used in theBikel model and thus representing the class-to-classtransitions implicitly rather than explicitly.
A keydifference between the approaches i in the back-offmechanism, which resembles a class grammar for theSheffield system.
In addition, the Sheffield approachuses a causal decoding algorithm, unlike the Viterbialgorithm which delays decisions until an entire sen-tence has been observed, though this is not a restric-tion of the model.
The extended-vocabulary n:gramapproach as the advantage that it is well-suited tousing directly in the ASR search process.Palmer, Ostendorf, and Burger (1999; 2000) usea model similar to other probabilistic name findingmodels, with several important differences in themodel topology and the language modeling tech-nique used.
A key difference in their approach isthat infrequent data is handled using the class-basedsmoothing technique described in (Iyer and Osten-dorf, 1997) that, unlike the orthographic-feature-dependent back-off, allows for ambiguity of wordclasses.
They describe methods for incorporatinginformation from place and name word lists, as wellas simple part-of-speech labels, and thus account forthe fact that some words can be used in multipleclasses.
Their results for high error rates (28.2) areslightly better than the simple back-off, suggestingthat the POS smoothing technique is more robust oASR errors.
In addition to the robustness providedby the class-based smoothing, they also report ini-tial success in integrating word confidence scores intotheir model to further improve the robustness of thesystem to speech recognition errors.4 Out-of-Vocabulary WordsHistorically, the goal of automatic speech recogni-tion (ASR) has been to transcribe the sequence ofwords contained in an audio stream.
State-of-the-art speech recognition systems model this problemusing a probabilistic formulation in which the mostlikely sequence of words is produced given a sequenceof acoustic features derived from the raw utteranceaudio signal.
While this approach has been verysuccessful, the model has a serious limitation: it canonly produce output hypotheses from a finite list ofwords that the recognizer explicitly models.
Thislist of possible output words is known as the systemvocabulary, and any spoken word not contained inthe vocabulary is referred to as an out-of-vocabulary(OOV) word.
Every OOV word in the input utter-ance is guaranteed to result in one or more output9errors.As we discussed in Section 2.2, ASR output worderrors, especially from spoken names, will adverselyaffect audio comprehension performance.
However,the methods for dealing with errors that we dis-cussed in previous ections, such as using N-best listoutput, can only compensate for misrecognitions ofknown words.
Since OOV words will never appear inthe hypothesized N-best output, other methods arenecessary for accounting for their presence in the in-put audio stream.
Figures 1 and 2 both have exam-ples of words that were out-of-vocabulary (Reineck,Haider) for the particular ASR system.
Figure 3shows another example, in which several names areout-of-vocabulary (Brill, Salif, Keta, Nusa, Fateh,Ali-han)..Some examples of questions that might be askedabout this passage are:Which two musicians did Wally Brill discover?Where is vocalist Salif Keta from?Who got turned onto Keta and Ali-han's music?Clearly, these questions could not be answered i-rectly from the actual output due to the word errors.In fact, identifying likely proper names in the out-put, as we discussed in Section 3, would also be inad-equate, because the output word error "DECATUR"might be mistaken for the answer to a Where?
ques-tion, and "MISTER FUNG" might be mistaken forthe answer to a Who?
question.
An effective methodfor dealing with out-of-vocabulary words is thus nec-essary.4.1 Increasing ASR VocabularyOne approach to the OOV problem might be toincrease the vocabulary size of the ASR system.Speech recognition systems can have a range of vo-cabulary sizes, depending on the target domain, thegenerality required, as well as the availability of com-putational resources.
For example, many researchsystems designed for constrained environments, suchas real-time travel information dialog, use a vo-cabulary size as small as 1,000-5,000 words.
Onthe other hand, current research systems for uncon-strained tasks such as the transcription ofbroadcastnews programs frequently have vocabularies between25,000 and 64,000 words.
Increasing the vocabu-lary size of a speech recognition system can resultin lower error rates, in part by decreasing the per-centage of OOV words in the input utterance.
How-ever, systems with larger vocabularies require morememory and run slower than those with smallervocabularies.
Since practical ASR systems cannothave unlimited memory and computational require-ments, they naturally cannot have unlimited vocab-ulary sizes.10In addition to increased computational cost,adding words to a vocabulary increases the poten-tial confusability with other vocabulary words.
Infact, Rosenfeld (1995) reports that a vocabulary sizearound 64,000 is nearly optimal for processing readNorth American Business news, and that increas-ing the vocabulary size beyond this yields negligiblerecognition improvement at best.
The optimal vo-cabulary size is also domain dependent: a 64,000word vocabulary may not be necessary for travel di-alog but may be inadequate for directory assistance.Rosenfeld's analysis shows that increasing the sys-tem vocabulary size can help recognition rates formany common words while hurting for less commonwords.
Yet the less common words, such as newnames introduced as a result of national and inter-national events, usually contain more semantic in-formation about the utterance, and these errors aremuch more costly for language understanding ap-plications.
Since new words are constantly being in-troduced into common usage, it is impossible to everhave a complete vocabulary of all spoken words, andthe treatment of new lexical items is thus an essen-tial element of any system aiming to process naturallanguage.Hetherington (1995) conducts an extensive empir-ical study of the out-of-vocabulary problem in hisPhD thesis.
He presents a demonstration of themagnitude of the OOV problem for a wide rangeof multilingual natural anguage corpora and showsthat some tasks can require vocabularies larger than100,000 words to reduce the OOV rate below 1%.He shows that even an OOV rate of 1% results in15-20% of all utterances containing unknown words.He also produces experimental results of the effectthat unknown words have on speech recognition out-put, showing that, on the average, each OOV inputword results in 1.5 actual word errors.
Of the errorsresulting from OOV words, 20% of these word errorsresult from in-vocabulary words being misrecognizeddue to their proximity to an unknown word.
Thiswork demonstrates the need for OOV word handlingin any speech recognition system.4.2 Dynamic  Vocabular iesThe need for unlimited spoken language vocabularydespite a limited ASR vocabulary suggests an al-ternative view of large-vocabulary spoken languageprocessing, in which r~ither than trying to includeall possible words in the ASR vocabulary we in-stead develop techniques for dynamically adaptingthe overall audio comprehension system vocabularyusing lexical resources, without requiring a largerASR vocabulary and the problems this entails.Geutner et al (1998) describe a multi-pass de-coding approach targeted at reducing the out-of-vocabulary rates for heavily inflected language, suchas Serbo-Croatian, German, and Turkish.
Theirwork attempts to dynamically expand the effectivevocabulary size by adapting the recognition dictio-nary to each utterance.
In the first recognition pass,an utterance-specific vocabulary list is constructedfrom the word lattice.
They then use a techniquethey call "Hypothesis Driven Lexical Adaptation"to expand the vocabulary list by adding all wordsin a full dictionary that are sufficiently similar tothose in the utterance list, where "similarity" is de-termined by the morphology and phonetics of thewords.
An automatic process then creates a new ut-terance recognition vocabulary and language modelfrom the expanded vocabulary list , and a secondrecognition pass is performed using the expandedmodels.
Geutner et al report hat the lexical adap-tation methods result in a significant decrease of upto 55% in OOV rates for the inflected languages,and that this improvement in OOV rate results inan improvement in the recognition rate of 3-6% (ab-solute).Geutner's multi-pass approach requires vocabu-lary adaptation and re-recognition of each com-plete utterance.
The importance of name findingin audio comprehension suggests an alternative tothis approach that would allow more targeted re-recognition of partial utterances.
As the examplein Figure 2 showed, it is possible to determine thedata regions that contain the potential answers, evenwhen the words themselves are misrecognized.
Forwritten questions, phonetic information from thequestion and hypothesis words can be used to helprepair key misrecognitions.
For example, using pho-netic information, it is possible to relate "vocal-ist Salif Keta" in a question to "VOCALIST SELLTHE DECATUR" in the ASR output.
This infor-mation can be supplemented with external lexical re-sources, such as word lists for the appropriate ype ofproper name, to expand the set of possible hypothe-ses within the region.
Large lists of names are avail-able publicly that could be used for this purpose;for example, the U.S. Census publishes a ranked listof the most common surnames and first names inthe United States, most of which are OOV wordsfor current ASR systems.
Once a region in the ASRoutput is identified as an OOV person, the Censusdata could be used to correct he OOV errors.
Thiswould then allow the audio comprehension systemto answer more Who?
questions correctly.5 Conc lus ionsJust as research in reading comprehension can helpevaluate text-based NLP systems against a hu-man benchmark, audio comprehension can providea useful task for evaluating speech understand-ing systems.
Audio comprehension provides anunderstanding-based evaluation paradigm for speechsystems that encourages research on a useful spokenlanguage understanding application rather than on"getting all the words right."
The techniques devel-oped for audio comprehension promise to be widelyuseful in many language understanding area.Re ferencesD.
Bikel, R. Schwartz, R. Weischedel, "An Algo-rithm that Learns What's in a Name," MachineLearning, 34(1/3):211-231, 1999.P.
Geutner, M. Finke, P. Scheytt, "Adaptive Vocab-ularies for Transcribing Multilingual BroadcastNews," Proc.
International Conference on Acous-tic, Speech and Signal Processing, 1998.Y.
Gotoh, S. Renals, "Information Extraction FromBroadcast News,"Philosophical Transactions ofthe Royal Society, series A: Mathematical, Physi-cal and Engineering Sciences, vol.358, issue 1769,April 2000.I.L.
Hetherington, "A Characterization of theProblem of New, Out-of-Vocabulary Words inContinuous-Speech Recognition and Understand-ing," PhD Thesis, Massachusetts Institute ofTechnology, 1995.L.
Hirschman, M. Light, E. Breck, J. Burger,"Deep Read: A Reading Comprehension System,"Proc.
37th Annual Meeting fo the Association forComputational Linguistics (A CL99), pp.
325-332,1999.R.
Iyer and M. Ostendorf, "Transforming Out-of-Domain Estimates to Improve In-Domain Lan-guage Models," Proc.
European Conference onSpeech Comm.
and Tech., Vol.
4, pp.
1975-1978,1997.D.
Miller, R. Schwartz, R. Weischedel, R.Stone, "Named Entity Extraction from BroadcastNews," Proc.
DARPA Broadcast News Workshop,pp.
37-40, 1999.D.
Palmer, M. Ostendorf, and J. Burger, "RobustInformation Extraction from Spoken LanguageData," Proc.
European Conference on SpeechComm.
and Tech., pp.
1035-1038, 1999.D.
Palmer, M. Ostendorf, and J. Burger, "RobustInformation Extraction from Automatically Gen-erated Speech Transcriptions," Speech Communi-cation, in press, 2000.P.
Robinson, E. Brown, J. Burger, N. Chinchor, A.Douthat, L. Ferro, and L, Hirschman, "Overview:Information extraction from broadcast news,"Proc.
DARPA Broadcast News Workshop, pp.
27-30, 1999.R.
Rosenfeld, "Optimizing Lexical and N-gram Cov-erage Via Judicious Use of Linguistic Data."Proc.
European Conference on Speech Comm.
andTech., volume 2, pages 1763-1766, 1995.M.
Siu and H. Gish, "Evaluation of word confidencefor speech recognition systems," Computer Speech~4 Language, Vol.
13, No.
4, Oct 1999, pp.
299-31911German authorities have been seeking Mr. Reineck since 1992.GERMAN AUTHORITIES HAVE BEEN SEEKING MISTER REINECKSINCE NINETEEN NINETY TWOGERMAN(.74) AUTHORITIES(.90) HAVE(.79) BEEN(.82) SINKING(.14) IS(.09) ARRIVING(.21)SINCE(.60) NINETEEN(.90) NINETY(.95) TWO(.94)Figure 1: Example of text-based vs. spoken language differences: a written sentence and its ASR transcrip-tions (WER 0% and 30% with word confidence scores).THE SHIRTS SHOW OUR STRAWS YEAR BEHIND IT WITH A SLEDGEHAMMER AND A RACISTCAPTIONTHE SHIRTS SHOW \[location\] \[person\] WITH A SLEDGEHAMMER AND A RACIST CAPTIONThe T-shirts showed Austria's JSrg Haider with a sledgehammer and a racist captionFigure 2: Example showing the importance of names: ASR output (30% WER) for a sentence, the sameASR sentence with locations of proper names labeled, and the correct ranscription.
:i:Then a few years ago, Wally Brill got turned onto the music of West African vocalist Salif Keta and thehaunting sounds of the late Nusa Fateh Ali-han.IN A FEW YEARS AGO ALWAYS REAL GOT TURNED ON TO THE MUSIC OF WEST AFRICANVOCALIST SELL THE DECATUR AND THE HAUNTING SOUNDS OF THE LATE MISTER FUNG'SALLEYFigure 3: Example of ASR output showing numerous OOV name errors.1
