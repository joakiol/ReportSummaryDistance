Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 681?688Manchester, August 2008A Joint Information Model for n-best RankingPatrick PantelYahoo!
Inc.Santa Clara, CA 95054me@patrickpantel.comVishnu VyasUSC Information Sciences InstituteMarina del Rey, CAvishnu@isi.eduAbstractIn this paper, we present a method formodeling joint information when gene-rating n-best lists.
We apply the methodto a novel task of characterizing the simi-larity of a group of terms where only asmall set of many possible semanticproperties may be displayed to a user.We demonstrate that considering the re-sults jointly, by accounting for the infor-mation overlap between results, generatesbetter n-best lists than considering themindependently.
We propose an informa-tion theoretic objective function for mod-eling the joint information in an n-bestlist and show empirical evidence thathumans prefer the result sets produced byour joint model.
Our results show with95% confidence that the n-best lists gen-erated by our joint ranking model aresignificantly different from a baseline in-dependent model 50.0% ?
3.1% of thetime, out of which they are preferred76.6% ?
5.2% of the time.1 IntroductionRanking result sets is a pervasive problem in theNLP and IR communities, exemplified by key-word search engines such as Google (Brin andPage 1998), machine translation systems (Zhanget al 2006), and recommender systems (Sharda-nand and Maes 1995; Resnick and Varian 1997).Consider the lexical semantics task of explain-ing why a set of terms are similar: given a set ofterms and a large set of possible explanations for?
2008.
Licensed under the Creative Commons Attri-bution-Noncommercial-Share Alike 3.0 Unportedlicense (http://creativecommons.org/licenses/by-nc-sa/3.0/).
Some rights reserved.their similarity, one must choose only the best nexplanations to display to a user.
There are manyways to explain why terms are similar2; one wayis to list the semantic properties that are sharedby the terms.
For example, consider the follow-ing set of terms corresponding to fruit names:{apple, ume, pawpaw, quince}Example semantic properties that could beused to explain their similarity include: they areproducts, they can be eaten, they are solid (butnot they are companies, for example).
The list ofsuch semantic properties can be very large andsome are much more informative than others.
Forexample, the property can-be-eaten is muchmore informative of the similarity of {apple, ume,pawpaw, quince} than the property is-solid.
Us-ing a simple measure of association betweenproperties and queries, explained in detail later inthis paper, one can rank each property and obtainthe following three highest scoring properties forexplaining the similarity of these terms:{they are products, they can beimported, they can be exported}Even though can be imported and can be ex-ported are highly ranked explanations, takenjointly, once we know one the other does not of-fer much more information since most things thatcan be imported can also be exported.
In otherwords, there is a large overlap in informationbetween the two properties.
A more informativeset of explanations could be obtained by replac-ing one of these two properties with a propertythat scored lower but had less information over-lap with the others, for example:2 In (Vyas and Pantel 2008), we explore the task ofexplaining the similarity between terms in detail.
Inthis paper, we focus on the task of choosing the bestset of explanations given a set of candidates.681{they are products, they can beimported, they can be eaten}Even though, taken alone, the property can beeaten may not be as informative as can be ex-ported, it does indeed add more information tothe explanation set when considered jointly withthe other explanations.In this paper, we propose an information theo-retic objective function for modeling the jointinformation in an n-best list.
Derived using con-ditional self-information, we measure the amountof information that each property contributes to aquery.
Intuitively, when adding a new property toa result set, we should prefer a property that con-tributes the maximum amount of information tothe existing set.
In our experiments, we showempirical evidence that humans prefer our jointmodel?s result sets on the task of explaining whya set of terms are similar.The remainder of this paper is organized asfollows.
In the next section, we review relatedliterature and position our contribution withinthat landscape.
Section 3 presents the task of ex-plaining the similarity of a set of terms and de-scribes a method for generating candidate expla-nations from which we will apply our rankingmodel.
In Section 4, we formally define ourranking task and present our Joint InformationRanking model.
Experimental results are pre-sented in Section 5 and finally, we conclude witha discussion and future work.2 Related WorkThere are a vast number of applications ofranking and its importance to the commercialsuccess at companies such as Google and Yahoohave fueled a great deal of research in recentyears.
In this paper, we investigate one particularaspect of ranking, the importance of consideringthe results in an n-best list jointly because of theinformation overlap issues described in theintroduction, and one particular application,namely explaining why a set of terms are similar.Considering results jointly is not a new ideaand is very similar to the concept of diversity-based ranking introduced in the IR communityby Carbonell and Goldstein (1998).
In short, se-lecting an n-best list is a balancing act betweenmaximizing the relevance of the list and the in-formation novelty of its results.
One commonlyused approach is to define a measure of novel-ty/semantic similarity between documents and toapply heuristics to reduce the relevance score ofa result item (a hit) by a function of the similarityof this item to other results in the list (Carbonelland Goldstein 1998; Zhu et al 2007).
Anothercommon approach is to cluster result documentsaccording to their semantic similarity and presentclusters to users instead of individual documents(Hearst and Pedersen 1996; Leuski 2001; Liu andCroft 2004).
In this paper, we argue that the bal-ance between relevance and novelty can be cap-tured by a formal model that maximizes the jointinformation content of a result set.
Instead ofranking documents in an IR setting, we focus inthis paper on a new task of selecting the best se-mantic properties that describe the similarity of aset of query terms.By no means an exhaustive list, the mostcommonly cited ranking and scoring algorithmsare HITS (Kleinberg 1998) and PageRank (Pageet al 1998), which rank hyperlinked documentsusing the concepts of hubs and authorities.
Themost well-known keyword scoring methodswithin the IR community are the tf-idf (Saltonand McGill 1983) and pointwise mutual informa-tion (Church and Hanks 1989) measures, whichput more importance on matching keywords thatoccur frequently in a document relative to thetotal number of documents that contain the key-word (by normalizing term frequencies with in-verse document frequencies).
Various methodsincluding tf-idf have been comparatively eva-luated by Salton and Buckley (1987).
Creating n-best lists using the above algorithms produceresult sets where each result is considered inde-pendently.
In this paper, we investigate the utilityof considering the result sets jointly and compareour joint method to a pointwise mutual informa-tion model.Within the NLP community, n-best list rank-ing has been looked at carefully in parsing, ex-tractive summarization (Barzilay et al 1999;Hovy and Lin 1998), and machine translation(Zhang et al 2006), to name a few.
The problemof learning to rank a set of objects by combininga given collection of ranking functions usingboosting techniques is investigated in (Freund etal.
2003).
This rank boosting technique has beenused in re-ranking parsers (Collins and Koo2000; Charniak and Johnson 2005).
Such re-ranking approaches usually improve the likelih-ood of candidate results using extraneous fea-tures and, for example in parsing, the propertiesof the trees.
In this paper, we focus on a differ-ence task: the lexical semantics task of selectingthe best semantic properties that help explainwhy a set of query terms are similar.
Unlike inparsing and machine translation, we are not ulti-682mately looking for the best single result, but in-stead the n-best.Looking at commercial applications, there aremany examples showcasing the importance ofranking, for example Internet search engines likeGoogle and Yahoo (Brin and Page 1998).
Anoth-er application is online recommendation systemswhere suggestions must be ranked before beingpresented to a user (Shardanand and Maes 1995).Also, in online social networks such as Facebookand LinkedIn, new connections or communitiesare suggested to users by leveraging their socialconnections (Spretus, et al 2005).3 Explaining SimilaritySeveral applications, such as IR engines, returnthe n-best ranked results to a query.
Although weexpect our joint information model, presented inSection 4.2, to generalize to many ranking tasks,our focus in this paper is on the task of choosingthe n-best explanations that describe the similari-ty of a set of terms.
That is, given a set of terms,one must choose the best set of characterizationsof why the terms are similar, chosen from a largeset of possible explanations.Analyzing the different ways in which one canexplain/characterize the similarity between termsis beyond the scope of this paper3.
The types ofexplanations that we consider in this paper aresemantic properties that are shared by the terms.For example, consider the query terms {apple,ume, pawpaw, quince} presented in Section 1.An example set of properties that explains thesimilarity of these words might include {they areproducts, they can be imported, they can be ex-ported, they are tasty, they grow}.The range of possible semantic properties islarge.
For the above example, we may have of-fered many other properties like {they are enti-ties, they can be eaten, they have skin, they arewords, they can be roasted, they can be shipped,etc.}
Choosing a high quality concise set ofproperties is the goal of this paper.Our hypothesis is that considering items in aresult set jointly for ranking produces better re-sult sets than considering them independently.An important question then is: what is a utilityfunction for measuring a better result?
We pro-pose that a result set is considered better thananother if a person could more easily reconstructthe original query from it.
Or, in other words, aresult set is considered better than another if it3 This topic is the focus of (Vyas and Pantel 2008).reduces more the uncertainty of what the originalquery was.
Here, reducing the uncertainty meansmaking it easier for a human to understand theoriginal question (i.e., a good explanation shouldclarify the query).Formally, we define our ranking task as:Task Definition: Given a query Q = {q1, q2, ?,qm} and a set of candidate properties R = {r1,r2, ?, rk}, where q is a term and r is a property,find the set of properties R' = {r1, r2, ?, rn} thatmost reduces the uncertainty of Q, where n << k.Recall from Section 1 the example Q = {apple,ume, pawpaw, quince}.
The set of properties:{they are products, they can beimported, they can be eaten}is preferred over the set{they are products, they can beimported, they can be exported}since it reduces more the uncertainty of what theoriginal query is.
That is, if we hid the query{apple, ume, pawpaw, quince} from a person,the first set of properties would help more thatperson guess the query elements than the secondproperties.In Section 4, we describe two models for mea-suring this uncertainty reduction and in Section5.1, we describe an evaluation methodology forquantifying this reduction in uncertainty usinghuman judgments.3.1 Source of PropertiesWhat is the source of the semantic properties tobe used as explanations?
Following Lin (1998),we use syntactic dependencies between words tomodel their semantic properties.
The assumptionhere is that some grammatical relations, such assubject and object can often yield semanticproperties of terms.
For example, given enoughcorpus occurrences of a phrase like ?students eatmany apples?, then we can infer the propertiescan-be-eaten for apples and can-eat for students.Unfortunately, many grammatical relations donot specify semantic properties, such as mostconjunction relations for example.
In this paper,we use a combination of corpus statistics andmanual filters of grammatical relations (such asomitting conjunction relations) to uncovercandidate semantic properties, as described in thenext section.
With this method, we unfortunatelyuncover some non-semantic properties and fail touncover some correct semantic properties.683Improving the candidate lists of semanticproperties is grounds for further investigation.3.2 Extracting PropertiesGiven a set of similar terms, we look at theoverlapping syntactic dependencies between thewords in the set to form candidate semanticproperties.
Example properties extracted by oursystem (described below) for a random sample oftwo instances from a cluster of food, {apple,beef}, include4:shredded, sliced, lean, sour, de-licious, cooked, import, export,eat, cook, dice, taste, market,consume, slice, ...We obtain candidate properties by parsing alarge textual corpus with the Minipar parser (Lin1993)5.
For each word in the corpus, we extractall of its dependency links, forming a featurevector of syntactic dependencies.
For example,below is a sample of the feature vector for theword apple:adj-mod:gala, adj-mod:shredded,object-of:caramelize, object-of:eat,object-of:import, ...Intersecting apple?s feature vector with beef?s,we are left with the following candidateproperties:adj-mod:shredded, object-of:eat,object-of:import, ...In this paper, we omit the relation name of thesyntactic dependencies, and instead write:shredded, eat, import, ...This list of syntactic dependencies forms thecandidate properties for our ranking task definedin Section 3.In Section 4, we use corpus statistics overthese syntactic dependencies to find the mostinformative properties that explain the similarityof a set of terms.
Some syntactic dependenciesare not reliably descriptive of the similarity ofwords such as conjunctions and determiners.
Weomit these dependency links from our model.4 Ranking ModelsIn this section, we present our ranking models forchoosing the n-best results to a query accordingto our task definition from Section 3.
The models4 We omit the syntactic relations for readability.5 Section 5.1 describes the specific corpus and methodthat was used to obtain our reported results.are expected to generalize to many ranking tasks,however in this paper we focus solely on theproblem of choosing the best semantic propertiesthat describe the similarity of a set of terms.In the next section, we outline our baseline in-dependent model, which is based on a commonlyused ranking metric in lexical semantics for se-lecting the most informative properties of a term.Then in Section 4.2, we propose our new modelfor considering the properties jointly.4.1 EIIR: Expected Independent Informa-tion Ranking Model (Baseline Model)Recall the task definition from Section 3.
Findinga property r that most reduces the uncertainty ina query set Q can be modeled by measuring thestrength of association between r and Q.Following Pantel and Lin (2002), we usepointwise mutual information (pmi) to measurethe association strength between two events qand r, where q is a term in Q and r is syntacticdependency, as follows (Church and Hanks1989):( ) ( )( ) ( )NfqcNrwcNrqcFfWwrqpmi ???=?
?,,,log,  (4.1)where c(q,r) is the frequency of r in the featurevector of q (as defined in Section 3.2), W is theset of all words in our corpus, F is the set of allsyntactic dependencies in our corpus, andN = ( )?
??
?Ww Fffwc , is the total frequency count ofall features of all words.We estimate the association strength betweena property r and a set of terms Q by taking theexpected pmi between r and each term in Q as:( ) ( ) ( )?
?=QqrqpmiqPrQpmi ,,  (4.2)where P(q) is the probability of q in the corpus.Finally, the EIIR model chooses an n-best listby selecting the n properties from R that havehighest pmi(Q, r).4.2 JIR: Joint Information Ranking ModelThe hypothesis of this paper is that consideringitems in an n-best result set jointly for rankingproduces better result sets than considering themindependently, an example of which is shown inSection 1.Recall our task definition from Section 3: toselect an n-best list R' from R such that it mostreduces the uncertainty of Q.
Recall that for ex-plaining the similarity of terms, Q is the set of684query words to be explained and R is the set ofall properties shared by words in Q.
The abovetask of finding R' can be captured by the follow-ing objective function:( )RQIRRR?=??
?minarg  (4.3)where I(Q|R') is the amount of information in Qgiven R':6( ) ( ) ( )???
?=?QqRqIqPRQI  (4.4)where P(q) is the probability of term q in ourcorpus (defined in the Section 4.1) and I(q|R') isthe amount of information in q given R', which isdefined as the conditional self-informationbetween q and R' (Merhav and Feder 1998):( ) ( )( )( )( )RcRqcrrrqPrrrqIRqInn???=?==?
*,,log,...,,log,...,,2121(4.5)where c(q,R') is the frequency of all properties inR' occurring with word q and * represents allpossible terms in the corpus7.
We have:( ) ( )??
?=?RrrqcRqc ,,  and ( ) ( )????
?
?=?Rr QqrqcRc ,*,where c(q,r) is defined as in Section 4.1 and Q' isthe set of all words that have all the properties inR'.
Computing c(*,R') efficiently can be doneusing a reverse index from properties to terms.The Joint Information Ranking model (JIR) isthe objective function in Eq.
4.3.
We find a sub-optimal solution to Eq.
4.3 using a greedy algo-rithm by starting with an empty set R' and itera-tively adding one property r at a time into R' suchthat:( ) ( )???????
?=QqRRrrRqIqPr minarg  (4.6)The intuition behind this algorithm is as fol-lows: when choosing a property r to add to a par-tial result set, we should choose the r that contri-butes the maximum amount of information to theexisting set (where all properties are consideredjointly).6 Note that finding the set R' that minimizes theamount of information in Q given R' equates to find-ing the R' that reduces most the uncertainty in Q.7 Note that each property in R' is shared by q becauseof the way the candidate properties in R were con-structed (see Section 3.2).A brute force optimal solution to Eq.
4.3 in-volves computing I(Q|R') for all subsets R' of sizen of R. In future work, we will investigate heuris-tic search algorithms for finding better solutionsto Eq.
4.3, but our experimental results discussedin Section 5 show that our greedy solution to Eq.4.3 already yields significantly better n-best liststhan the baseline EIIR model.5 Experimental ResultsIn this section, we show empirical evidence thatconsidering items in an n-best result set jointlyfor ranking produces better result sets than con-sidering them independently.
We validate thisclaim by testing whether or not human judgesprefer the set of explanations generated by ourjoint model (JIR) over the independent model(EIIR).5.1 Experimental SetupWe trained the probabilities described in Section4 using corpus statistics extracted from theTREC-9 and TREC-2002 Aquaint collectionsconsisting of approximately 600 million words.We used the Minipar parser (Lin 1993) to ana-lyze each sentence and we collected the frequen-cy counts of the grammatical contexts output byMinipar and used them to compute the probabili-ty and pointwise mutual information values fromSections 4.1 and 4.2.
Given any set of words Qfrom the corpus, our joint and independent mod-els generate a ranked list of n-best explanations(i.e., properties) for the similarity of the words.Recall the example set Q = {apple, beef} fromSection 3.2.
Following Section 3.2, all grammat-ical contexts output by Minipar that both wordsshare form a candidate explanation set R for theirsimilarity.
For {apple, beef}, our systems found312 candidate explanations.
Applying the inde-pendent ranking model, EIIR, we obtain the fol-lowing top-5 best explanations, R':product, import of, export, banon, industryUsing the joint model, JIR, we obtain:export, product, eat, ban on,from menu5.2 Comparing Ranking ModelsIn order to obtain a representative set of similarterms as queries to our systems, we randomlychose 100 concepts from the CBC collection(Pantel and Lin 2002) consisting of 1628 clustersof nouns.
For each of these concepts, we ran-domly chose a set of cluster instances (nouns),685where the size of each set was randomly chosento consist of two or three noun (chosen to reducethe runtime of our algorithm).
For example, threeof our randomly sampled concepts were Music,Flowers, and Alcohol and below are the randominstances selected from these concepts:?
{concerto, quartet, Fifth Symphony}?
{daffodil, lily}?
{gin, alcohol, rum}Each of these three samples forms a query.Applying both our EIIR and JIR models, we gen-erated the top-5 explanations for each of the 100samples.
For example, below are the explana-tions returned for {daffodil, lily}:?
EIIR: bulb, bouquet of, yellow, pink, hybr-id?
JIR: flowering, bulb, bouquet of, hybrid,yellowTwo judges then independently annotated 500test cases using the following scheme.
For eachof the 100 samples, a judge is presented with thesample along with the top-1 explanation of bothsystems, randomly ordered for each sample suchthat the judge can never know which systemgenerated which explanation.
The judge thenmust make one of the following three choices:?
Explanation 1: The judge prefers the firstexplanation to the second.?
Explanation 2: The judge prefers thesecond explanation to the first.?
Equal: The judge cannot determine thatone explanation is better than the other.The judge is then presented with the top-2 ex-planations from each system, then the top-3, top-4, and finally the top-5 explanations, making theabove annotation decision each time.
Once thejudge has seen the top-5 explanations for thesample, the judge moves on to the next sampleand repeats this process until all 100 samples areannotated.
Allowing the judges to see the top-1,top-2, up to top-5 explanations allows us to laterinspect how our ranking algorithms perform ondifferent sizes of explanation sets.The above annotation task was performed in-dependently by two judges and the resultingagreement between the judges, using the Kappastatistic (Siegel and Castellan Jr. 1988), was ?
=0.60.
Table 1 lists the full confusion matrix onthe annotation task.
On just the annotations of thetop-5 explanations, the agreement was ?
= 0.73.Table 2 lists the Kappas for the different sizes ofexplanation sets.
It is more difficult for judges todetermine the quality of smaller explanation sets.For the above top-5 explanations for the query{daffodil, lily}, both judges preferred the JIRproperties since flowering was deemed more in-formative than pink given that we also know theproperty yellow.5.2.1 Evaluation ResultsTable 3 shows sample n-best lists generated byour system and Table 4 presents the results of theexperiment described in the previous section.Table 4 lists the preferences of the judges for then-best lists generated by the independent andjoint models, in terms of the percentage of sam-ples preferred by each judge on each model.
Wereport our results on both all 500 annotations andon the 100 annotations for the explanation sets ofsize n = 5.
Instead of using an adjudicator forresolving the two judges?
disagreements, weweighted each judge?s decision by 0.5.
We usedbootstrap resampling to obtain the 95% confi-dence intervals.The judges significantly preferred the jointmodel over the independent model.
Looking atall annotated explanation sets (varying n from 1to 5), the n-best lists from JIR were preferred39.7% of the time.
On the 50.0% ?
3.1% testcases where one list was preferred over another,the JIR lists were preferred overall 76.6% ?
5.2%of the time, with 95% confidence.
Cautionshould be taken when interpreting the results forn < 3 since the annotator agreement for these wasvery low.
However, as shown in Figure 1, humanpreference for the JIR model was higher at n ?
3.Table 2.
Inter-annotator agreement statistics overvarying explanation set sizes n.n AGREEMENT (%) KAPPA (?
)1 75.0 0.472 70.0 0.503 77.0 0.624 78.0 0.635 84.0 0.73Table 1.
Confusion matrix between the two judges onthe annotation task over all explanation set sizes(n = 1 ?
5).JIR EIIR EQUALJIR 153 2 48EIIR 11 33 19EQUAL 29 7 1986865.2.2 Discussion and Error AnalysisFigure 1 illustrates the annotated preferencesover varying sizes of explanation sets, for n ?
[1 .. 5].
Except in the case where only one expla-nation is returned, we see consistent preferencesbetween the judges.
Manual inspection of thesize 1 explanation sets showed that often oneproperty is not enough to understand the similari-ty of the query words.
For example, consider thefollowing two explanation sets: {sell} and{drink}.
If you did not know the original query Q,one list would not be much better than the otherin determining what the query was.
But, by add-ing one more property, we get: {sell, drink} and{drink, spike with}.
The second explanation listreduces much more the uncertainty that the queryconsists of alcoholic beverages, as you probablyguessed (the first list also reduces the uncertainty,but not as much as the second).
The above ex-ample is taken from our random sample list forthe query words {gin, alcohol, rum} ?
the expla-nation {drink, spike with} was generated usingthe JIR model.We manually inspected some of the samplequeries where both judges preferred the EIIR n-best list.
One such sample query was: {JerryFalwell, Jim Bakker, Pat Robertson}.
The n-bestlists returned by the JIR and EIIR models respec-tively were {televangelist, evangelist, Rev., tele-vision, founder} and {evangelist, television, Rev.,founder, religious}.
Both judges preferred theEIIR list because of the overlap in informationbetween televangelist and evangelist.
The prob-lem here in JIR was that the word televangelistwas very rare in the corpus and thus few termshad both the feature televangelist and evangelist.We would expect in a larger corpus to see a larg-er overlap with the two features, in which caseevangelist would not be chosen by the JIR model.As discussed in Section 2, considering resultsjointly is not a new idea and is very similar to theconcept of diversity-based ranking introduced inthe IR community by Carbonell and Goldstein(1998).
Their proposed technique, called maxim-al marginal relevance (MMR), forms the basis ofmost schemes used today and works as follows.Initially, each result item is scored independentlyof the others.
Then, the n-best list is selected byiteratively choosing the highest scoring resultand then discounting each remaining candidate?sscore by some function of the similarity (or in-formation gain) between that candidate and thecurrently selected members of the n-best list.
Inpractice, these heuristic-based algorithms are fastto compute and are used heavily by commercialIR engines.
The purpose of this paper is to inves-tigate a principled definition of diversity usingthe concept of maximal joint information.
Theobjective function proposed in Eq.
4.3 provides abasis for understanding diversity through the lensof information theory.
Although this paper fo-Table 3.
Five example n-best lists, drawn from our random sample described in Section 5.1, using the joint JIRmodel and the independent EIIR model (for n=5).Query (Q) JIR n-best (R') EIIR n-best (R'){gin, alcohol, rum} drink, spike with, sell, use, consume sell, drink, use, consume, buy{Temple University, Michigan State}political science at, professor at,director at, student at, attendprofessor at, professor, director at,student at, student{concerto, quartet, Fifth Symphony} Beethoven, his, play, write, performance his, play, write, performance, perform{ranch house, loft}offer, brick, sprawling, rambling,turn-of-the-centuryhis, live, her, buy, small{dysentery, tuberculosis} morbidity, die of, case, patient, suffer from die of, case, patient, case of, haveTable 4.
Percentage of test cases where the judgespreferred JIR vs. EIIR vs. they had no preference,computed over all explanation set sizes (n = 1 ?
5)vs. only the explanation sets of size n = 5.SYSTEM ALL (95% CONF?)
N=5 (95% CONF?
)JIR 39.7% ?
3.0% 43.7% ?
6.9%EIIR 10.4% ?
1.3% 10.1% ?
4.2%Equal 50.0% ?
3.1% 45.2% ?
6.9%?95% confidence intervals estimated using bootstrap resampling.Figure 1.
Percentage of human preference for eachmodel with varying sizes of explanation sets (n).00.20.40.60.811 2 3 4 5PreferenceNumber?of?explanations?
(n)Model?Preference?vs.
?Number?of?ExplanationsJIR?Preferred EIIR?Baseline?Preferred Equal?
(No?Preference)687cuses on the task of explaining the similarity ofterms, we plan in future work to apply our me-thod to an IR task in order to compare and con-trast our method with MMR.6 ConclusionThis paper investigates the problem of n-bestranking on the lexical semantics task of explain-ing/characterizing the similarity of a group ofterms where only a small set of many possiblesemantic properties may be displayed to a user.We propose that considering the results jointly,by accounting for the information overlap be-tween results, helps generate better n-best lists.We presented an information theoretic objectivefunction, called Joint Information Ranking, formodeling the joint information in an n-best list.On our lexical semantics task, empirical evidenceshows that humans significantly prefer JIR n-bestlists over a baseline model that considers the ex-planations independently.
Our results show thatthe n-best lists generated by the joint model arejudged to be significantly different from thosegenerated by the independent model 50.0% ?3.1% of the time, out of which they are preferred76.6% ?
5.2% of the time, with 95% confidence.In future work, we plan to investigate otherjoint models using latent semantic analysis tech-niques, and to investigate heuristic algorithms toboth optimize search efficiency and to better ap-proximate our JIR objective function.
Althoughapplied only to the task of characterizing the si-milarity of terms, it is our hope that the JIR mod-el will generalize well to many ranking tasks,from keyword search ranking, to recommenda-tion systems, to advertisement placements.ReferencesBarzilay, R.; McKeown, K.; and Elhadad, M. 1999.
InformationFusion in the Context of Multi-Document Summarization.
InProceedings of ACL-1999.
pp.
550-557.
College Park, MD.Brin, S. and Page, L. 1998.
The Anatomy of a Large-Scale Hyper-textual Web Search Engine.
Computer Networks and ISDN Sys-tems, 30:107-117.Carbonell, J. G. and Goldstein, J.
1998.
The Use of MMR, Diversi-ty-Based Reranking for Reordering Documents and ProducingSummaries.
In Proceedings of SIGIR-1998.
pp.
335-336.Charniak, E. and Johnson, M. 2005.
Coarse-to-fine n-best parsingand MaxEnt disciriminative reranking.
In Proceedings of ACL-2005.
pp.
173-180.
Ann Arbor, MI.Church, K. and Hanks, P. 1989.
Word association norms, mutualinformation, and lexicography.
In Proceedings of ACL-89.
pp.76-83.
Vancouver, Canada.Collins, M. and Koo, T. 2000.
Discriminative Reranking for Natu-ral Laguage Parsing.
In Proceedings ICML-2000.
pp.
175-182.Palo Alto, CAFreund, Y.; Iyer, R.; Schapier, E.R and Singer, Y.
2003.
An effi-cient boosting algorithm for combining preferences.
The Journalof Machine Learning Research, 4:933-969.Harris, Z.
1985.
Distributional structure.
In: Katz, J. J.
(ed.)
ThePhilosophy of Linguistics.
New York: Oxford University Press.pp.
26-47.Hearst, M. A. and Pedersen, J. O.
1996.
Reexamining the clusterhypothesis: Scatter/gather on retrieval results.
In Proceedings ofSIGIR-1996.
pp.
76-84.
Zurich, Switzerland.Hovy, E.H. and Lin, C.-Y.
1998.
Automated Text Summarization inSUMMARIST.
In M. Maybury and I. Mani (eds), Advances inAutomatic Text Summarization.
Cambridge, MIT Press.Kleinberg, J.
1998.
Authoritative sources in a hyperlinked environ-ment.
In Proceedings of the Ninth Annual ACM-SIAM Sympo-sium on Discrete Algorithms.
Pp.
668-677.
New York, NY.Leuski, A.
2001.
Evaluating document clustering for interactiveinformation retrieval.
In Proceedings of CIKM-2001.
pp.
33-40.Atlanta, GA.Lin, D. 1998.
Automatic retrieval and clustering of similar words.In Proceedings of COLING/ACL-98.
pp.
768-774.
Montreal,Canada.Lin, D. 1993.
Parsing Without OverGeneration.
In Proceedings ofACL-93.
pp.
112-120.
Columbus, OH.Liu, X. and Croft, W. B.
2004.
Cluster-based retrieval using lan-guage models.
In Proceedings of SIGIR-2004.
pp.
186-193.Sheffield, UK.Merhav, N. and Feder, M. 1998.
Universal Prediction.
IEEE Trans-actions on Information Theory, 44(6):2124-2147.Page, L.; Brin, S.; Motwani R.; Winograd, T. 1998.
The PageRankCitation Ranking: Bringing Order to the Web.
Stanford DigitalLibrary Technologies Project.Pantel, P. and Lin, D. 2002.
Discovering Word Senses from Text.In Proceedings of KDD-02.
pp.
613-619.
Edmonton, Canada.Resnick, P. and Varian, H. R. 1997.
Recommender Systems.
Com-munications of the ACM, 40(3):56-58.Salton, G. and Buckley, C. 1987.
Term Weighting Approaches inAutomatic Text Retrieval.
Technical Report:TR81-887, Ithaca,NY.Salton, G. and McGill, M. J.
1983.
Introduction to Modern Infor-mation Retrieval.
McGraw Hill.Shardanand, U. and Maes, P. 1995.
Social Information Filtering:Algorithms for Automating ?Word of Mouth?.
In Proceedingsof ACM CHI-1995.
pp.
210-217.
New York.Siegel, S. and Castellan Jr., N. J.
1988.
Nonparametric Statistics forthe Behavioral Sciences.
McGraw-Hill.Spretus, E.; Sahami, M.; and Buyukkokten, O.
2005.
EvaluatingSimilarity Measures: A Large-Scale Study in the Orkut SocialNetwork.
In Proceedings of SIGKDD-2005.
pp.
678-684.
Chi-cago, IL.Vyas, V. and Pantel, P. 2008.
Explaining Similarity of Terms.
InProceedings of COLING-2008.
Manchester, England.Zhu, X.; Goldberg, A.; Van Gael, J.; and Andrzejewski, D. 2007.Improving Diversity in Ranking using Absorbing RandomWalks.
In Proceedings of NAACL HLT 2007. pp.
97-104.Rochester, NY.Zhang, Y,; Callan, J.; and Minka, T. 2002.
Novelty and redundancydetection in adaptive filtering.
In Proceedings of SIGIR-2002.pp.
81-88.
Tampere, Finland.Zhang, Y.; Hildebrand, A. S.; and Vogel, S. 2006.
Distributed Lan-guage Modeling for N-best List Re-ranking.
In Proceedings ofEMNLP-2006.
Pp.
216-223.
Sydney, Australia.688
