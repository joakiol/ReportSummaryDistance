Proceedings of ACL-08: HLT, pages 479?487,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsIntensional Summaries as Cooperative Responses in Dialogue:Automation and EvaluationJoseph PolifroniNokia Research Center3 Cambridge CenterCambridge, MA 02142, USAjoseph.polifroni@nokia.comMarilyn WalkerDepartment of Computer ScienceUniversity of SheffieldSheffield, S1 4DP, United KingdomM.A.Walker@sheffield.ac.ukAbstractDespite its long history, and a great dealof research producing many useful algo-rithms and observations, research in coop-erative response generation has had littleimpact on the recent commercialization ofdialogue technologies, particularly withinthe spoken dialogue community.
We hy-pothesize that a particular type of cooper-ative response, intensional summaries, areeffective for when users are unfamiliar withthe domain.
We evaluate this hypothe-sis with two experiments with cruiser,a DS for in-car or mobile users to accessrestaurant information.
First, we comparecruiser with a baseline system-initiativeDS, and show that users prefer cruiser.Then, we experiment with four algorithmsfor constructing intensional summaries incruiser, and show that two summarytypes are equally effective: summaries thatmaximize domain coverage and summariesthat maximize utility with respect to auser model.1 IntroductionResearch in cooperative response generation hashad a long history and produced many usefulalgorithms and observations (Mays 1980; Pol-lack et al1982; Joshi et al, 1986; Kalita etal., 1986) inter alia.
However, it has had littleimpact on the recent commercialization of dia-logue technologies, particularly within the spo-ken dialogue community.
We believe that thislack of interest in cooperative response genera-tion arises from two limitations of previous work:(1) There has been relatively little empirical re-search showing that cooperative responses leadU1: Tell me about restaurants in London.SI1: What kind of cuisine are you interested in?C1: I know of 596 restaurants in London.
I knowof 3 inexpensive vegetarian restaurants and 14inexpensive Chinese restaurants.
I also knowof 4 inexpensive Greek restaurants.U2: Chinese.SI2: Do you have a price range in mind?C2: I know of 27 restaurants in London thatserve Chinese food.
There are 8 inexpen-sive Chinese restaurants in Chinatown and2 inexpensive Chinese restaurants in Hamp-stead/Kilburn.
I also know of 1 inexpensiveChinese restaurant in Soho.U3: How about a cheap one?SI3: What neighborhood would you like?C3: I know of 1 inexpensive Chinese restaurantin Hampstead/Kilburn with very good foodquality and 1 in Bayswater with good foodquality.
I also know of 2 in Chinatown withmedium food quality.Figure 1: Intensional summaries (C = cruiser) ascompared with a system initiative (SI) strategy inthe London restaurant domain.
U = Userto more natural, effective, or efficient dialogues(Litman et al1998; Demberg and Moore, 2006);and (2) Previous work has hand-crafted such re-sponses, or hand-annotated the database to sup-port them (Kaplan, 1984; Kalita et al, 1986;Cholvy, 1990; Polifroni et al, 2003; Benamara,2004), which has made it difficult to port andscale these algorithms.Moreover, we believe that there is an evengreater need today for cooperative response gen-eration.
Larger and more complex datasets aredaily being created on the Web, as information479is integrated across multiple sites and vendors.Many users will want to access this informationfrom a mobile device and will have little knowl-edge of the domain.
We hypothesize that theseusers will need cooperative responses that selectand generalize the information provided.In particular, we hypothesize that a partic-ular type of cooperative response, intensionalsummaries, when provided incrementally dur-ing a dialogue, are effective for large or com-plex domains, or when users are unfamiliarwith the domain.
These intensional summarieshave the ability to describe the data that formsthe knowledge base of the system, as well asrelationships among the components of thatdatabase.
We have implemented intensionalsummaries in cruiser (Cooperative ResponsesUsing Intensional Summaries of Entities and Re-lations), a DS for in-car or mobile users to accessrestaurant information (Becker et al2006; Wenget al2005; Weng et al2006).
Figure 1 contrastsour proposed intensional summary strategy withthe system initiative strategy used in many di-alogue systems (Walker et al, 2002; VXML,2007).Previous research on cooperative responseshas noted that summary strategies shouldvary according to the context (Sparck Jones,1993), and the interests and preferences of theuser (Gaasterland et al, 1992; Carenini andMoore, 2000; Demberg and Moore, 2006).A number of proposals have emphasized theimportance of making generalizations (Kaplan,1984; Kalita et al, 1986; Joshi et al, 1986).In this paper we explore different methods forconstructing intensional summaries and inves-tigate their effectiveness.
We present fullyautomated algorithms for constructing inten-sional summaries using knowledge discoverytechniques (Acar, 2005; Lesh and Mitzen-macher, 2004; Han et al, 1996), and decision-theoretic user models (Carenini and Moore,2000).We first explain in Sec.
2 our fully automated,domain-independent algorithm for constructingintensional summaries.
Then we evaluate ourintensional summary strategy with two experi-ments.
First, in Sec.
3, we test the hypothesisthat users prefer summary responses in dialoguesystems.
We also test a refinement of that hy-pothesis, i.e., that users prefer summary typeresponses when they are unfamiliar with a do-main.
We compare several versions of cruiserwith the system-initiative strategy, exemplifiedin Fig.
1, and show that users prefer cruiser.Then, in Sec.
4, we test four different algo-rithms for constructing intensional summaries,and show in Sec.
4.1 that two summary typesare equally effective: summaries that maximizedomain coverage and summaries that maximizeutility with respect to a user model.
We alsoshow in Sec.
4.2 that we can predict with 68%accuracy which summary type to use, a signifi-cant improvement over the majority class base-line of 47%.
We sum up in Sec.
5.2 Intensional SummariesThis section describes algorithms which result inthe four types of intensional summaries shown inFig.
2.
We first define intensional summaries asfollows.
Let D be a domain comprised of a set Rof database records {ri, ...rn}.
Each record con-sists of a set of attributes {Aj , ..., An}, with as-sociated values v: D(Ai)={vi,1, vi,2, ..., vi,n}.
Ina dialogue system, a constraint is a value intro-duced by a user with either an explicit or impliedassociated attribute.
A constraint c is a func-tion over records in D such that cj(R) returns arecord r if r ?
D and r : Ai = c. The set of alldialogue constraints {ci, ..., cn} is the context Cat any point in the dialogue.
The set of recordsR in D that satisfy C is the focal information:R is the extension of C in D. For example, theattribute cuisine in a restaurant domain has val-ues such as ?French?
or ?Italian?.
A user utter-ance instantiating a constraint on cuisine, e.g.,?I?m interested in Chinese food?, results in a setof records for restaurants serving Chinese food.Intensional summaries as shown in Fig.
2 aredescriptions of the focal information, that high-light particular subsets of the focal informationand make generalizations over these subsets.The algorithm for constructing intensionalsummaries takes as input the focal informationR, and consists of the following steps:?
Rank attributes in context C, using one of tworanking methods (Sec.
2.1);480Type Ranking #atts Clusters Scoring SummaryRef-SingRefiner 3 SinglevalueSize I know of 35 restaurants in London serving Indianfood.
All price ranges are represented.
Some of theneighborhoods represented are Mayfair, Soho, andChelsea.
Some of the nearby tube stations are GreenPark, South Kensington and Piccadilly Circus.Ref-AssocRefiner 2 Associative Size I know of 35 restaurants in London serving Indianfood.
There are 3 medium-priced restaurants in May-fair and 3 inexpensive ones in Soho.
There are also2 expensive ones in Chelsea.UM-SingUsermodel3 SinglevalueUtility I know of 35 restaurants in London serving Indianfood.
There are 6 with good food quality.
There arealso 12 inexpensive restaurants and 4 with good ser-vice quality.UM-AssocUsermodel2 Associative Utility I know of 35 restaurants in London serving Indianfood.
There are 4 medium-priced restaurants withgood food quality and 10 with medium food quality.There are also 4 that are inexpensive but have poorfood quality.Figure 2: Four intensional summary types for a task specifying restaurants with Indian cuisine in London.?
Select top-N attributes and construct clustersusing selected attributes (Sec.
2.2);?
Score and select top-N clusters (Sec.
2.3);?
Construct frames for generation, perform aggre-gation and generate responses.2.1 Attribute RankingWe explore two candidates for attribute ranking:User model and Refiner.User model: The first algorithm utilizesdecision-theoretic user models to provide an at-tribute ranking specific to each user (Careniniand Moore, 2000).
The database contains 596restaurants in London, with up to 19 attributesand their values.
To utilize a user model, wefirst elicit user ranked preferences for domainattributes.
Attributes that are unique acrossall entities, or missing for many entities, areautomatically excluded, leaving six attributes:cuisine, decor quality, food quality, price, ser-vice, and neighborhood.
These are ranked usingthe SMARTER procedure (Edwards and Bar-ron, 1994).
Rankings are converted to weights(w) for each attribute, with a formula whichguarantees that the weights sum to 1:wk =1KK?i=k1iwhere K equals the number of attributes in theranking.
The absolute rankings are used to se-lect attributes.
The weights are also used forcluster scoring in Sec.
2.3.
User model rankingis used to produce UM-Sing and UM-Associn Fig.
2.Refiner method: The second attributeranking method is based on the Refiner algo-rithm for summary construction (Polifroni et al,2003).
The Refiner returns values for every at-tribute in the focal information in frames or-dered by frequency.
If the counts for the top-N(typically, 4) values for a particular attribute,e.g., cuisine, exceeded M% (typically 80%) ofthe total counts for all values, then that at-tribute is selected.
For example, 82% of In-dian restaurants in the London database are inthe neighborhoods Mayfair, Soho, and Chelsea.Neighborhood would, therefore, be chosen as anattribute to speak about for Indian restaurants.The thresholds M and N in the original Refinerwere set a priori, so it was possible that no at-tribute met or exceeded the thresholds for a par-ticular subset of the data.
In addition, some en-tities could have many unknown values for someattributes.Thus, to insure that all user queries result insome summary response, we modify the Refiner481method to include a ranking function for at-tributes.
This function favors attributes thatcontain fewer unknown values but always re-turns a ranked set of attributes.
Refiner rankingis used to produce Ref-Sing and Ref-Assoc inFig.
2.2.2 Subset ClusteringBecause the focal information is typically toolarge to be enumerated, a second parameter at-tempts to find interesting clusters representingsubsets of the focal information to use for thecontent of intensional summaries.
We assumethat the coverage of the summary is important,i.e., the larger the cluster, the more general thesummary.The simplest algorithm for producing clustersutilizes a specified number of the top-ranked at-tributes to define a cluster.
Single attributes,as in the Ref-Sing and UM-Sing examples inFig.
2, typically produce large clusters.
Thusone algorithm uses the top three attributes toproduce clusters, defined by either a single value(e.g., UM-Sing) or by the set of values thatcomprise a significant portion of the total (e.g.,Ref-Sing).price_rangemedium inexpensivefood_quality food_qualitygood medium poor(4) (10) (4)Figure 3: A partial tree for Indian restaurants inLondon, using price range as the predictor variableand food quality as the dependent variable.
Thenumbers in parentheses are the size of the clustersdescribed by the path from the root.However, we hypothesize that more informa-tive and useful intensional summaries might beconstructed from clusters of discovered associ-ations between attributes.
For example, as-sociations between price and cuisine producesummaries such as There are 49 medium-pricedrestaurants that serve Italian cuisine.
We applyc4.5 decision tree induction to compute associ-ations among attributes (Kamber et al, 1997;Quinlan, 1993).
Each attribute in turn is desig-nated as the dependent variable, with other at-tributes used as predictors.
Thus, each branchin the tree represents a cluster described by theattribute/value pairs that predict the leaf node.Fig.
3 shows clusters of different sizes inducedfrom Indian restaurants in London.
The clustersize is determined by the number of attributesused in tree induction.
With two attributes, theaverage cluster size at the leaf node is 60.4, butdrops to 4.2 with three attributes.
Thus, we usetwo attributes to produce associative clusters, asshown in Fig.
2 (i.e., the Ref-Assoc and UM-Assoc responses), to favor larger clusters.2.3 Cluster ScoringThe final parameter scores the clusters.
Onescoring metric is based on cluster size.
Singleattributes produce large clusters, while associa-tion rules produce smaller clusters.The second scoring method selects clustersof high utility according to a user model.
Wefirst assign scalar values to the six ranked at-tributes (Sec.
2.1), using clustering methods asdescribed in (Polifroni et al, 2003) The weightsfrom the user model and the scalar values forthe attributes in the user model yield an overallutility U for a cluster h, similar to utilities ascalculated for individual entities (Edwards andBarron, 1994; Carenini and Moore, 2000):Uh =K?k=1wk(xhk)We use cluster size scoring with Refiner rank-ing and utility scoring with user model ranking.For conciseness, all intensional summaries arebased on the three highest scoring clusters.2.4 SummaryThe algorithms for attribute selection and clus-ter generation and scoring yield the four sum-mary types in Table 2.
Summary Ref-Sing isconstructed using (1) the Refiner attribute rank-ing; and (2) no association rules.
(The quanti-fier (e.g., some, many) is based on the cover-482age.)
Summary Ref-Assoc is constructed us-ing (1) the Refiner attribute ranking; and (2)association rules for clustering.
Summary UM-Sing is constructed using (1) a user model withranking as above; and (2) no association rules.Summary UM-Assoc is constructed using (1) auser model with ranking of price, food, cuisine,location, service, and decor; and (2) associationrules.3 Experiment OneThis experiment asks whether subjects preferintensional summaries to a baseline system-initiative strategy.
We compare two types of in-tensional summary responses from Fig.
2, Ref-Assoc and UM-Assoc to system-initiative.The 16 experimental subjects are asked to as-sume three personas, in random order, chosen totypify a range of user types, as in (Demberg andMoore, 2006).
Subjects were asked to read thedescriptions of each persona, which were avail-able for reference, via a link, throughout the ex-periment.The first persona is the Londoner, represent-ing someone who knows London and its restau-rants quite well.
The Londoner persona typi-cally knows the specific information s/he is look-ing for.
We predict that the system-initiativestrategy in Fig.
1 will be preferred by this per-sona, since our hypothesis is that users preferintensional summaries when they are unfamiliarwith the domain.The second persona is the Generic tourist(GT), who doesn?t know London well and doesnot have strong preferences when it comes toselecting a restaurant.
The GT may want tobrowse the domain, i.e.
to learn about the struc-ture of the domain and retrieve information byrecognition rather than specification (Belkin etal., 1994).
We hypothesize that the Ref-Assocstrategy in Fig.
2 will best fit the GT, since thecorresponding clusters have good domain cover-age.The third persona is the UM tourist (UMT).This persona may also want to browse thedatabase, since they are unfamiliar with Lon-don.
However, this user has expressed prefer-ences about restaurants through a previous in-teraction.
The UMT in our experiment is con-cerned with price and food quality (in that or-der), and prefers restaurants in Central London.After location, the UMT is most concerned withcuisine type.
The intensional summary labelledUm-Assoc in Fig.
2 is based on this user model,and is computed from discovered associationsamong preferred attributes.As each persona, subjects rate responses ona Likert scale from 1-7, for each of four dia-logues, each containing between three and fourquery/response pairs.
We do not allow tie votesamong the three choices.3.1 Experimental resultsThe primary hypothesis of this work is thatusers prefer summary responses in dialogue sys-tems, without reference to the context.
To testthis hypothesis, we first compare Londoner re-sponses (average rating 4.64) to the most highlyrated of the two intensional summaries (averagerating 5.29) for each query/response pair.
Thisdifference is significant (df = 263, p < .0001),confirming that over users prefer an intensionalsummary strategy to a system-initiative strat-egy.Table 1 shows ratings as a function of personaand response type.
Overall, subjects preferredthe responses tailored to their persona.
TheLondoner persona signifcantly preferred Lon-doner over UMT responses (df = 95, p < .05),but not more than GT responses.
This con-firms our hypothesis that users prefer incremen-tal summaries in dialogue systems.
Further,it disconfirms our refinement of that hypothe-sis, that users prefer summaries only when theyare unfamiliar with the domain.
The fact thatno difference was found between Londoner andGT responses indicates that GT responses con-tain information that is perceived as useful evenwhen users are familiar with the domain.The Generic Tourist persona also preferredthe GT responses, significantly more than theLondoner responses (df = 95, p < .05), butnot significantly more than the UMT responses.We had hypothesized that the optimal summarytype for users completely new to a domain woulddescribe attributes that have high coverage ofthe focal information.
This hypothesis is discon-firmed by these findings, that indicate that user483Response TypePersona London GT UMTLondon 5.02 4.55 4.32GT 4.14 4.67 4.39UM tourist 3.68 4.86 5.23Table 1: Ratings by persona assumed.
London =Londoner persona, GT = Generic tourist, UMT =User Model touristmodel information is helpful when constructingsummaries for any user interested in browsing.Finally, the UM Tourist persona overwhelm-ingly preferred UMT responses over Londonerresponses (df = 95, p < .0001).
However, UMTresponses were not significantly preferred to GTresponses.
This confirms our hypothesis thatusers prefer summary responses when they areunfamiliar with the domain, but disconfirmsthe hypothesis that users will prefer summariesbased on a user model.
The results for both theGeneric Tourist and the UM Tourist show thatboth types of intensional summaries contain use-ful information.4 Experiment TwoThe first experiment shows that users prefer in-tensional summaries; the purpose of the sec-ond experiment is to investigate what makes agood intensional summary.
We test the differentways of constructing such summaries describedin Sec.
2, and illustrated in Fig.
2.Experimental subjects were 18 students whoseuser models were collected as described inSec.
2.3.
For each user, the four summary typeswere constructed for eight tasks in the Londonrestaurant domain, where a task is defined by aquery instantiating a particular attribute/valuecombination in the domain (e.g., I?m interestedin restaurants in Soho).
The tasks were selectedto utilize a range of attributes.
The focal in-formation for four of the tasks (large set tasks)were larger than 100 entities, while the focal in-formation for the other four tasks were smallerthan 100 entities (small set tasks).
Each taskwas presented to the subject on its own webpage with the four intensional summaries pre-sented as text on the web page.
Each subjectwas asked to carefully read and rate each al-User model RefinerAssociation rules 3.4 2.9Single attributes 3.0 3.4User model RefinerSmall dataset 3.1 3.4Large dataset 3.2 2.9Table 2: User ratings showing the interaction be-tween clustering method, attribute ranking, anddataset size in summaries.ternative summary response on a Likert scaleof 1 .
.
.
5 in response to the statement, This re-sponse contains information I would find usefulwhen choosing a restaurant.
The subjects werealso asked to indicate which response they con-sidered the best and the worst, and to providefree-text comments about each response.4.1 Hypothesis Testing ResultsWe performed an analysis of variance with at-tribute ranking (user model vs. refiner), clus-tering method (association rules vs. single at-tributes), and set size (large vs. small) as in-dependent variables and user ratings as the de-pendent variable.
There was a main effect for setsize (df = 1, f = 6.7, p < .01), with summariesdescribing small datasets (3.3 average rating)rated higher than those for large datasets (3.1average rating).There was also a significant interaction be-tween attribute ranking and clustering method(df = 1, f = 26.8, p < .001).
Table 2 showsratings for the four summary types.
There areno differences between the two highest ratedsummaries: Ref-Sing (average 3.4) and UM-Assoc (average 3.4).
See Fig.
2.
This suggeststhat discovered associations provide useful con-tent for intensional summaries, but only for at-tributes ranked highly by the user model.In addition, there was another significant in-teraction between ranking method and setsize(df = 1, f = 11.7, p < .001).
The ratings at thebottom of Table 2 shows that overall, users ratesummaries of small datasets higher, but usersrate summaries higher for large datasets when auser model is used.
With small datasets, usersprefer summaries that don?t utilize user modelinformation.484We also calculate the average utility for eachresponse (Sec.
2.1) and find a strong correlationbetween the rating and its utility (p < .005).When considering this correlation, it is impor-tant to remember that utility can be calculatedfor all responses, and there are cases where theRefiner responses have high utility scores.4.2 Summary Type PredictionOur experimental data suggest that characteris-tics associated with the set of restaurants beingdescribed are important, as well as utility in-formation derived from application of a a usermodel.
The performance of a classifier in pre-dicting summary type will indicate if trends wediscovered among user judgements carry over toan automated means of selecting which responsetype to use in a given context.In a final experiment, for each task, we use thehighest rated summary as a class to be predictedusing C4.5 (Quinlan, 1993).
Thus we have 4classes: Ref-Sing, Ref-Assoc, UM-Sing, andUM-Assoc.
We derive two types of feature setsfrom the responses: features derived from eachuser model and features derived from attributesof the query/response pair itself.
The five fea-ture sets for the user model are:?
umInfo: 6 features for the rankings for each at-tribute for each user?s model, e.g.
a summarywhose user had rated food quality most highlywould receive a ?5?
for the feature food quality;?
avgUtility: 4 features representing an averageutility score for each alternative summary re-sponse, based on its clusters (Sec.
2.3).?
hiUtility: 4 features representing the highestutility score among the three clusters selectedfor each response;?
loUtility: 4 features representing the lowest util-ity score among the three clusters selected foreach response;?
allUtility: 12 features consisting of the high,low, and average utility scores from the previousthree feature sets.Three feature sets are derived from the queryand response pair:?
numRests: 4 features for the coverage of eachresponse.
For summary Ref-Assoc in Ta-ble 2, numRests is 43; for summary UM-Assoc, numrests is 53.;Sys Feature Sets Acc(%)S1 allUtility 47.1S2 task, numRests 51.5S3 allUtility,umInfo 62.3?S4 allUtility,umInfo,numRests,task 63.2?S5 avgUtility,umInfo,numRests,task 62.5?S6 hiUtility,umInfo,numRests,task 66.9?S7 hiUtility,umInfo,numRests,task,dataset68.4?S8 loUtility,umInfo,numRests,task 60.3?S9 hiUtility,umInfo 64.0?Table 3: Accuracy of feature sets for predicting pre-ferred summary type.
?
= p < .05 as compared tothe Baseline (S1)).?
task: A feature for the type of constraint usedto generate the focal information (e.g., cuisine,price range).?
dataset: A feature for the size of the focal in-formation subset (i.e., big, small), for valuesgreater and less than 100.Table 3 shows the relative strengths of the twotypes of features on classification accuracy.
Themajority class baseline (System S1) is 47.1%.The S2 system uses only features associatedwith the query/response pair, and its accuracy(51.5%) is not significantly higher than the base-line.
User model features perform better thanthe baseline (S3 in Table 3), and combiningfeatures from the query/response pair and theuser model significantly increases accuracy in allcases.
We experimented with using all the utilityscores (S4), as well as with using just the aver-age (S5), the high (S6), and the low (S8).
Thebest performance (68.4%)is for the (S7) systemcombination of features.The classification rules in Table 4 for the bestsystem (S7) suggests some bases for users?
deci-sions.
The first rule is very simple, simply stat-ing that, if the highest utility value of the Ref-Sing response is lower than a particular thresh-old, then use the UM-Assoc response.
In otherwords, if one of the two highest scoring responsetypes has a low utility, use the other.The second rule in Table 4 shows the effectthat the number of restaurants in the responsehas on summary choice.
In this rule, the Ref-Sing response is preferred when the highest util-485IF (HighestUtility: Ref-Sing) < 0.18THEN USE UM-Assoc----------------------------------------IF (HighestUtility: Ref-Assoc) > 0.18) &&(NumRestaurants: UM-Assoc < 400) &&(HighestUtility: UM-Assoc < .47)THEN USE Ref-Sing----------------------------------------IF (NumRestaurants: UM-Assoc < 400) &&(HighestUtility: UM-Assoc < .57) &&(HighestUtility: Ref-Assoc > .2)THEN USE Ref-AssocTable 4: Example classification rules from System 7in Table 3.ity value of that response is over a particularthreshold.The final rule in Table 4 predicts Ref-Assoc,the lowest overall scoring response type.
Whenthe number of restaurants accounted for byUM-Assoc, as well as the highest utility forthat response, are both below a certain thresh-old, and the highest utility for the Ref-Assocresponse is above a certain threshold, then useRef-Assoc.
The utility for any summary typeusing the Refiner method is usually lower thanthose using the user model, since overall utility isnot taken into account in summary construction.However, even low utility summaries may men-tion attributes the user finds important.
That,combined with higher coverage, could make thatsummary type preferable over one constructedto maximize user model utility.5 ConclusionWe first compared intensional summary coop-erative responses against a system initiative di-alogue strategy in cruiser.
Subjects assumedthree ?personas?, a native Londoner, a touristwho was interacting with the system for the firsttime (GT), or a tourist for which the systemhas a user model (UMT).
The personas weredesigned to reflect differing ends of the spectradefined by Belkin to characterize information-seeking strategies (Belkin et al, 1994).
Therewas a significant preference for intensional sum-maries across all personas, but especially whenthe personas were unfamiliar with the domain.This preference indicates that the benefits ofintensional summaries outweigh the increase inverbosity.We then tested four algorithms for summaryconstruction.
Results show that intensionalsummaries based on a user model with associa-tion rules, or on the Refiner method (Polifroni etal., 2003), are equally effective.
While (Dem-berg and Moore, 2006) found that their usermodel stepwise refinement (UMSR) method wassuperior to the Refiner method, they also foundmany situations (70 out of 190) in which theRefiner method was preferred.
Our experimentwas structured differently, but it suggests that,in certain circumstances, or within certain do-mains, users may wish to hear about choicesbased on an analysis of focal information, irre-spective of user preferences.Our intensional summary algorithms auto-matically construct summaries from a database,along with user models collected via a domain-independent method; thus we believe thatthe methods described here are domain-independent.
Furthermore, in tests to deter-mine whether a classifier can predict the bestsummary type to use in a given context, weachieved an accuracy of 68% as compared to amajority class baseline of 47%, using dialoguecontext features.
Both of these results pointhopefully towards a different way of automatingdialogue design, one based on a combination ofuser modelling and an analysis of contextual in-formation.
In future work we hope to test thesealgorithms in other domains, and show that in-tensional summaries can not only be automati-cally derived but also lead to reduced task timesand increased task success.ReferencesA.C.
Acar and A. Motro.
2005.
Intensional Encapsu-lations of Database Subsets via Genetic Program-ming.
Proc, 16th Int.
Conf.
on Database and Ex-pert Systems Applications.
Copenhagen.Tilman Becker, Nate Blaylock, Ciprian Gersten-berger, Ivana Kruijff-Korbayova?, Andreas Ko-rthauer, Manfred Pinkal, Michael Pitz, PeterPoller, and Jan Schehl.
Natural and intuitive mul-timodal dialogue for in-car applications: The sam-mie system.
In ECAI, pages 612?616, 2006.486N.
J. Belkin, C. Cool, A. Stein and U. Thiel.
1994.Cases, Scripts, and Information Seeking Strate-gies: On the Design of Interactive Information Re-trieval Systems.
Expert Systems and Applications,9(3):379?395.F.
Benamara.
2004.
Generating Intensional Answersin Intelligent Question Answering Systems.
Proc.3rd Int.
Conf.
on Natural Language GenerationINLG.G.
Carenini and J. Moore.
2000.
A Strategy for Gen-erating Evaluative Arguments.
Proc.
First Int?lConf.
on Natural Language Generation.
1307?1314.Brant Cheikes and Bonnie Webber.
Elements of acomputational model of cooperative response gen-eration.
In Proc.
Speech and Natural LanguageWorkshop, pages 216?220, Philadelphia, 1989.X.
Chen and Y-F. Wu.
2006.
Personalized Knowl-edge Discovery: Mining Novel Association Rulesfrom Text.
Proc., SIAM Conference on Data Min-ing.L.
Cholvy.
1990.
Answering Queries Addressedto a Rule Base.
Revue d?Intelligence Artificielle.1(1):79?98.V.
Demberg and J. Moore.
2006 Information Pre-sentation in Spoken Dialogue Systems.
Proc.
11thConf.
EACL..W. Edwards and F. Hutton Barron.
1994.
Smartsand smarter: Improved simple methods for mul-tiattribute utility measurement.
OrganizationalBehavior and Human Decision Processes.
60:306?325.T.
Gaasterland and P. Godfrey and J. Minker.
1992.An Overview of Cooperative Answering.
Journalof Intelligent Information Systems.
1(2):387?416.J.
Han, Y. Huang and N. Cercone.
1996.
IntelligentQuery Answering by Knowledge Discovery Tech-niques.
IEEE Transactions on Knowledge andData Engineering.
8(3):373?390.Aravind Joshi, Bonnie Webber, and Ralph M.Weischedel.
Living up to expectations: computingexpert responses.
In HLT ?86: Proceedings of theworkshop on Strategic computing natural language,pages 179?189, Morristown, NJ, USA, 1986.
Asso-ciation for Computational Linguistics.J.
Kalita and M.J. Colburn and G. McCalla.
1984.A response to the need for summary responses.COLING-84).
432?436.M.
Kamber, L. Winstone, W. Gong, S. Cheng andJ Han.
1997.
Generalization and decision treeinduction: efficient classification in data mining.Proc.
7th Int.
Workshop on Research Issues inData Engineering (RIDE ?97).
111?121.S.J.Kaplan.
1984.
Designing a Portable NaturalLanguage Database Query System.
ACM Trans-actions on Database Systems, 9(1):1?19.N.
Lesh and M. Mitzenmacher.
Interactive datasummarization: an example application.
Proc.,Working Conference on Advanced Visual Inter-faces.
Gallipoli, Italy.
pages 183?187.Diane J. Litman, Shimei Pan, and Marilyn A.Walker.
Evaluating response strategies in a web-based spoken dialogue agent.
In COLING-ACL,pages 780?786, 1998.J.
Polifroni, G. Chung, and S. Seneff.
2003.
Towardsthe Automatic Generation of Mixed-Initiative Di-alogue Systems from Web Content.
Proc.
Eu-rospeech.
2721?2724.E.
Mays.
Correcting misconceptions about databasestructure.
In Proceedings of the CSCSI ?80, 1980.Martha E. Pollack, Julia Hirschberg, and Bonnie L.Webber.
User participation in the reasoning pro-cesses of expert systems.
In AAAI, pages 358?361,1982.J.R.
Quinlan 1993.
C4.5: Programs for MachineLearning.
Morgan Kaufmann.
San Mateo, CA.K.
Sparck Jones.
1998.
Automatic summarising:factors and directions.
I. Mani and M. Maybury,eds.
Advances in Automatic Text Summarization.MIT Press.M.
Walker, A. Rudnicky, J. Aberdeen, E. Bratt, J.Garofolo, H. Hastie, A.
Le, B. Pellom, A. Potami-anos, R. Passonneau, R. Prasad, S. Roukos, G.Sanders, S. Seneff and D. Stallard.
2002.
DARPACommunicator Evaluation: Progress from 2000 to2001.
Proc, ICSLP 2002.F.
Weng, L. Cavedon, B. Raghunathan, D. Mirkovic,H.
Cheng, H. Schmidt, H. Bratt, R. Mishra,S.
Peters, L. Zhao, S. Upson, E. Shriberg, andC.
Bergmann.
Developing a conversational dia-logue system for cognitively overloaded drivers.
InProceedings, International Congress on IntelligentTransportation Systems, 2005.F.
Weng, S. Varges, B. Raghunathan, F. Ratiu,H.
Pon-Barry, B. Lathrop, Q. Zhang, T. Schei-deck, H. Bratt, K. Xu, M. Purver, R. Mishra,M.
Raya, S. Peters, Y. Meng, L. Cavedon, andL.
Shriberg.
Chat: A conversational helper forautomotive tasks.
In Proceedings, Interspeech: In-ternational Conference on Spoken Language Pro-cessing, 2006.Voxeo.
VoiceXML Development Guide.http://voicexml.org.487
