Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 76?84,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsParsing word clustersMarie Candito?
and Djam?
Seddah???
Alpage (Universit?
Paris 7/INRIA), 30 rue du ch?teau des rentiers 75013 Paris, France?
Universit?
Paris-Sorbonne, 28, rue Serpente, 75006 Paris, FranceAbstractWe present and discuss experiments in sta-tistical parsing of French, where terminalforms used during training and parsing arereplaced by more general symbols, particu-larly clusters of words obtained through un-supervised linear clustering.
We build on thework of Candito and Crabb?
(2009) who pro-posed to use clusters built over slightly coars-ened French inflected forms.
We investigatethe alternative method of building clustersover lemma/part-of-speech pairs, using a rawcorpus automatically tagged and lemmatized.We find that both methods lead to compara-ble improvement over the baseline (we ob-tain F1=86.20% and F1=86.21% respectively,compared to a baseline of F1=84.10%).
Yet,when we replace gold lemma/POS pairs withtheir corresponding cluster, we obtain an up-per bound (F1=87.80) that suggests room forimprovement for this technique, should tag-ging/lemmatisation performance increase forFrench.We also analyze the improvement in perfor-mance for both techniques with respect toword frequency.
We find that replacing wordforms with clusters improves attachment per-formance for words that are originally eitherunknown or low-frequency, since these wordsare replaced by cluster symbols that tend tohave higher frequencies.
Furthermore, clus-tering also helps significantly for medium tohigh frequency words, suggesting that trainingon word clusters leads to better probability es-timates for these words.1 IntroductionStatistical parsing techniques have dramatically im-proved over the last 15 years, yet lexical data sparse-ness remains a critical problem.
And the richer themorphology of a language, the sparser the treebank-driven lexicons will be for that language.Koo et al (2008) have proposed to use word clus-ters as features to improve graph-based statisticaldependency parsing for English and Czech.
Theirclusters are obtained using unsupervised clustering,which makes it possible to use a raw corpus con-taining several million words.
Candito and Crabb?
(2009) applied clustering to generative constituencyparsing for French.
They use a desinflection step thatremoves some inflection marks from word forms andthen replaces them with word clusters, resulting ina significant improvement in parsing performance.Clustering words seems useful as a way of address-ing the lexical data sparseness problem, since countson clusters are more reliable and lead to better prob-ability estimates.
Clustering also appears to addressthe mismatch of vocabularies between the originaltreebank and any external, potentially out-of-domaincorpus: clusters operate as an intermediary betweenwords from the treebank and words from the exter-nal corpus used to compute clusters.
Furthermore,parsing word clusters instead of word forms aug-ments the known vocabulary.However, depending on the clustering method,clusters are either not very reliable or are availableonly for very frequent words.
In order to parse wordclusters one needs to determine which word clustersare reliable enough to be beneficial, so the tuningof parameters such as cluster granularity and clusterreliability becomes very important.The aim of this paper is to give an in-depth studyof the "parsing word clusters" technique.
In particu-lar, starting from the Candito and Crabb?
(2009) ex-periments, we investigate the use of clustering lem-76mas instead of desinflected forms.
We also pro-vide an analysis of the performance gains obtainedwith respect to word frequency (frequent words, rarewords, unknown words).In the next section, we describe the French tree-bank used as the basis for all of our experiments.We describe in section 3 the statistical parser usedfor training and testing.
We then describe the desin-flection process used prior to unsupervised cluster-ing (section 4), and the Brown algorithm we use forunsupervised clustering (section ).
We describe ourexperiments and results in section 6, and provide adiscussion in section 7.
We then point out some re-lated work and conclude in section 9.2 French TreebankFor our experiments, we used the French Tree-bank (Abeill?
et al, 2003), which contains 12531sentences, 350931 tokens, from the newspaperLe Monde.
We used the treebank instantiation(hereafter FTB-UC) as first described in (Canditoand Crabb?, 2009), where :(i) the rich original annotation containing morpho-logical and functional information is mapped to asimpler phrase-structure treebank with a tagset of28 part-of-speech tags, and no functional annotation(ii) some compounds with regular syntax are brokendown into phrases containing several simple words(iii) the remaining sequences annotated as com-pound words in the FTB are merged into a singletoken, whose components are separated with anunderscoreFor all experiments in this paper (tagging andparsing) we used the same partition of the treebankas these authors : first 10% for test, next 10% fordev and the rest for training1.3 Berkeley ParserWe report here experiments using the BerkeleyPCFG parser with latent annotations (Petrov et al,2006), hereafter BKY, which is a constituent parserthat has been proven to perform well for French(Crabb?
and Candito, 2008; Seddah et al, 2009),1More precisely the partition is : first 1235 sentences fortest, next 1235 sentences for development, and remaining 9881sentences for training.though a little lower than a combination of a taggerplus the dependency-based MST parser (Candito etal., 2010).
Though PCFG-style parsers operate ontoo narrow a domain of locality, splitting symbolsaccording to structural and/or lexical properties isknown to help parsing (Klein and Manning., 2003).Following (Matsuzaki et al, 2005), the BKY algo-rithm uses EM to estimate probabilities on symbolsthat are automatically augmented with latent anno-tations, a process which can be viewed as symbolsplitting.
It iteratively evaluates each such split andmerges back the less beneficial ones.
Crabb?
andCandito (2008) show that some of the informationcarried by the latent annotations is lexical, since re-placing words by their gold part-of-speech tag leadsto worse results than the corresponding perfect tag-ging test, with words unchanged.
This is a clear in-dication that lexical distinctions are used, and perco-late up the parse tree via the latent annotations.We now describe how the BKY software handlesrare and unknown words, as this is pertinent to ourdiscussion in section 6.
P (w|tag) is calculated us-ing Bayes?
rule, as P (tag|w)P (w)/P (tag).
Rel-ative frequency estimates are used for words thatare sufficiently frequent.
For rare words (appear-ing less than 10 times in our settings), P (tag|w)is smoothed using the proportion of tokens in thesecond half of the training set that were not seen inthe first half, and that have this tag.
For unknownwords, words signatures are used: these are wordclasses determined by information such as the wordsuffix, whether the word is capitalized, whether itcontains digits, etc.
P (w|tag) is estimated withP (signature(w)|tag), and is also smoothed in thesame way rare words are.4 Morphological clusteringA first approach to word clustering is to clusterforms on a morphological basis.
In the case ofa relatively morphologically rich language such asFrench, this is an obvious way to reduce lexicalsparseness caused by inflection.
(Candito and Crabb?, 2009) proposed the use ofa desinflection method, without resorting to part-of-speech tagging.
We propose an alternate methodhere, which uses lemmas and part-of-speech tagsthat are output by a tagger/lemmatizer.
Because77counts on lemmas are more reliable, clusteringover lemmas presumably produces clusters that aremore reliable than those produced by clustering overdesinflected forms.
However, this approach doescreate a constraint in which automatically taggedand lemmatized text is required as input to theparser, leading to the introduction of tagging errors.Both morphological clustering methods make useof the Lefff lexicon (Sagot, 2010).
Before we de-scribe these two methods, we briefly give basic in-formation on French inflectional morphology and onthe Lefff.4.1 French inflection and the Lefff lexiconFrench nouns appear in singular and plural forms,and have an intrinsic gender.
The number and gen-der of a noun determines the number and gender ofdeterminers, adjectives, past participles that dependon it.
Hence in the general case, past participles andadjectives have four different forms.
The major in-flectional variation appears for finite verbs that varyfor tense, mood, person and number.
A regular verbmay correspond to more than 60 inflected forms ifall tenses and mood are included.
In practice, someforms occur very rarely, because some tense/moodpairs are rare, and further, in the case of newspa-per text for instance, the first and second persons arealso rare.
So for instance in the FTB-UC, there are33 different forms for the highly frequent verb andauxiliary avoir (to have), that appears 4557 times.The medium frequency verb donner (to give) occurs155 times, under 15 different forms.
In the wholetreebank, there are 27130 unique word forms, corre-sponding to 17570 lemmas.The Lefff is a freely available rich morphologi-cal and syntactic French lexicon (Sagot, 2010).
Itcontains 110, 477 lemmas (simple and compounds)and 536, 375 inflected forms.
The coverage on theFTB-UC is high : around 96% of the tokens, and80, 1% of the types are present in the Lefff (leavingout punctuation and numeric tokens, and ignoringcase differences).4.2 DesinflectionThe aim of the desinflection step is to reduce lex-ical data sparseness caused by inflection, withouthurting parsability and without committing oneselfas far as lexical ambiguity is concerned.
The ideais to leave unchanged the parser?s task in disam-biguating part-of-speech tags.
In that case, mor-phological clustering using lemmas is not an option,since lemma assignment presupposes POS disam-biguation.
Furthermore, useful information such asverb mood (which is needed to capture, for instance,that infinitive verbs have no overt subject or that par-ticipial clauses are sentence modifiers) is discardedduring lemmatization, though it is encoded in theFTB with different projections for finite verbs (pro-jecting sentences) versus non finite verbs (projectingVPpart or VPinf).The intuition of Candito and Crabb?
(2009) isthat other inflectional markers in French (gender andnumber for determiners, adjectives, pronouns andnouns, or tense and person for verbs) are not crucialfor inferring the correct phrase-structure projectionfor a given word.
Consequently, they proposed toachieve morphological clustering by desinflection,namely by removing unneeded inflectional markers,identified using the Lefff.
This lexicon-based tech-nique can be viewed as an intermediate method be-tween stemming and lemmatization.The desinflection process is as follows: for a to-ken t to desinflect, if it is known in the lexicon,then for each inflected lexical entry le of t, try toget a corresponding singular entry.
If correspond-ing singular entries exist for all such le and all havethe same form, then replace t by the correspond-ing form.
For instance for wt=entr?es (ambigu-ous between entrances and entered, fem, plural), thetwo lexical entries are [entr?es/N/fem/plu] and [en-tr?es/V/fem/plu/part/past]2 , each have a correspond-ing singular lexical entry, with form entr?e.The same process is used to map feminine formsto corresponding masculine forms.
This allows oneto change mang?e (eaten, fem, sing) into mang?
(eaten, masc, sing).
But for the form entr?e, am-biguous between N and Vpastpart entries, only theparticiple has a corresponding masculine entry (withform entr?).
In that case, in order to preserve theoriginal part-of-speech ambiguity, entr?e is not re-placed by entr?.
Finite verb forms, when unambigu-ous with other parts-of-speech, are mapped to sec-ond person plural present indicative correspondingforms.
This choice was made in order to avoid cre-2This is just an example and not the real Lefff format.78Dev set Overall Overall (-punct) Unseen (4.8)POS acc 97.38 96.99 91.95Lemma acc 98.20 97.93 92.52Joint acc 96.35 95.81 87.16Test set Overall Overall (-punct) Unseen (4.62)POS acc 97.68 97.34 90.52Lemma acc 98.36 98.12 91.54Joint acc 96.74 96.26 85.28Table 1: MORFETTE performance on the FTB-UC devand test sets (with and without punctuation)ating ambiguity: the second person plural forms endwith a very typical -ez suffix, and the resulting formis very unlikely ambiguous.
For the first token of asentence, if it is unknown in the lexicon, the algo-rithm tries to desinflect the corresponding lowercaseform.This desinflection process reduces the numberof distinct tokens in the FTB-UC training set from24110 to 18052.4.3 Part-of-speech tagging and lemmatizationIn order to assign morphological tags and lemmas towords we use a variation of the MORFETTE modeldescribed in (Chrupa?a et al, 2008).
It is a se-quence labeling model which combines the predic-tions of two classification models (one for morpho-logical tagging and one for lemmatization) at decod-ing time, using a beam search.While (Chrupa?a et al, 2008) use Maximum Entropytraining to learn PM and PL, we use the MORFETTEmodels described in (Seddah et al, 2010), that aretrained using the Averaged Sequence Perceptron al-gorithm (Freund and Schapire, 1999).
The two clas-sification models incorporate additional features cal-culated using the Lefff lexicon.Table 1 shows detailed results on dev set andtest set of the FTB-UC, when MORFETTE is trainedon the FTB-UC training set.
To the best of ourknowledge the parts-of-speech tagging performanceis state-of-the-art for French3 and the lemmatizationperformance has no comparable results.5 Unsupervised clustering3A pure MAXENT based tagger is described in (Denis andSagot, 2009), that also uses the Lefff, under the form of featuresfor the known categories of a word in the lexicon.
The authorsreport 97.70% of accuracy and 90.01% for unseen data.We use the Brown et al (1992) hard clustering al-gorithm, which has proven useful for various NLPtasks such as dependency parsing (Koo et al, 2008)and named entity recognition (Liang, 2005).
The al-gorithm to obtain C clusters is as follows: each ofthe C most frequent tokens of the corpus is assignedits own distinct cluster.
For the (C + 1)th most fre-quent token, create a (C + 1)th cluster.
Then foreach pair among the C + 1 resulting clusters, mergethe pair that minimizes the loss in the likelihood ofthe corpus, according to a bigram language modeldefined on the clusters.
Repeat this operation for the(C + 2)th most frequent token, etc.
The result is ahard clustering of words in the corpus into C distinctclusters, though the process can be continued to fur-ther merge pairs of clusters among the C clusters,ending with a single cluster for the entire vocabu-lary.
A binary tree hierarchy of merges for the Cclusters can be obtained by tracing the merging pro-cess, with each cluster identified by its path withinthis binary tree.
Clusters can thus be used at variouslevels of granularity.6 Experiments and results6.1 ClusteringFor the Brown clustering algorithm, we used PercyLiang?s code4, run on the L?Est R?publicain corpus,a 125 million word journalistic corpus, freely avail-able at CNRTL5.
The corpus was first tokenized andsegmented into sentences.
For compound words,the 240 most frequent compounds of the FTB-UCwere systematically recognized as one token.
Wetried out the two alternate morphological clusteringprocesses described in section 4 as a preprocessingstep before the unsupervised clustering algorithmwas run on the L?Est R?publicain corpus :(i) word forms were replaced by correspondingdesinflected form(ii) word forms were replaced by a concatenationof the part-of-speech tag and lemma obtained withMORFETTE6 .4http://www.eecs.berkeley.edu/ pliang/software5http://www.cnrtl.fr/corpus/estrepublicain6Because these experiments were first run with a versionof Morfette that was not yet optimized for lemmatization, wechose to overide the MORFETTE lemma when the Lefff lemmais available for a given form and part-of-speech tag pair sup-plied by Morfette.
Morfette?s current results (version 0.3.1) in79Name Terminal symbols Vocabulary size Terminal symbolsin training set in training set in dev/test setsBASELINE wf 24110 wfDFL Desinflected wf 18052 Desinflected wfDFL+CLUST>X Cluster1(desinflected wf) 1773 (X = 200) Cluster1(desinflected wf)GOLDCATLEMMA Gold POS+lemma 18654 Gold POS+lemmaAUTOCATLEMMA Gold POS+lemma 18654 Automatic POS+lemmaGOLDCATLEMMA+CLUST>X Cluster2(gold POS+lemma) 1298 (X = 200) Cluster2(gold POS+lemma)AUTOCATLEMMA+CLUST>X Cluster2(gold POS+lemma) 1298 (X = 200) Cluster2(automatic POS+lemma)Table 2: Types of terminal symbols used for training and parsingIn the first case we obtain clusters of desinflectedforms, whereas in the second case we obtain clustersof tag+lemma pairs.
Note that lemmas alone couldbe used, but as noted earlier, important syntacticinformation would be lost, particularly for verbmood.
We did try using clusters of lemmas, coupledwith a few suffixes to record the verb mood, but thisresulted in more or less the same performance asclusters of tag+lemma pairs.6.2 Berkeley parser settingsFor BKY we used Slav Petrov?s code, adapted forFrench by Crabb?
and Candito (2008) by modify-ing the suffixes used to classify unknown words.
Weuse the partition between training, development andtest sets introduced in section 2.
Note though thatthe BKY algorithm itself uses two sets of sentencesat training: a learning set and a smaller validationset for tuning model hyperparameters.
In all experi-ments in this paper, we used 2% of the training set asas a validation set, and 98% as a learning set.
Thisdiffers from (Candito and Crabb?, 2009), where thedev set was used as a validation set.6.3 ExperimentsWe then tested several settings differing only in theterminal symbols used in the training set, and in thedev and test sets.
We list these settings in table 2.
Forthe settings involving unsupervised linear clustering:DFL+CLUST>X: Each desinflected form df is re-placed by Cluster1(df) : if df occurred more thanX times in the L?Est R?publicain corpus, it is re-placed by its cluster id, otherwise, a special clus-ter UNKC is used.
Further, a _c suffix is added iflemmatization renders this step obsolete.the desinflected form starts with a capital letter, andadditional features are appended, capturing whetherthe form is all digits, ends with ant, or r, or ez (cf.this is the ending of the desinflected forms of unam-biguous finite verbs).
(Candito and Crabb?, 2009)showed that these additional features are needed be-cause clusters are noisy: linear context clusteringsometimes groups together items that belong to dif-ferent parts-of-speech.GOLDCATLEMMA+CLUST>X: The terminalform used is the gold part-of-speech concatenatedto the cluster id of the gold POS+lemma, or UNKCif that pair did not occur more than X times in theL?Est R?publicain corpus.AUTOCATLEMMA+CLUST>X: For thetraining set, the same setting as GOLD-CATLEMMA+CLUST>X is used.
But for thedev and test sets, predicted parts-of-speech andlemmas are used, as output by the MORFETTE tag-ger/lemmatizer: the terminal form is the predictedpart-of-speech concatenated with the cluster id ofthe predicted POS+lemma, or UNKC if that pairwas not frequent enough.For the CLUST>X experiments, we report resultswith X = 200.
We have found empirically thatvarying X between 20 and 700 has very little effecton performance gains, both for clustering of desin-flected forms and clustering of tag+lemma pairs.Also, all results are with a maximum number ofclusters set to 1000, and we found that limiting thenumber of clusters (by taking only a prefix of thecluster bit string) degrades results.6.4 Evaluation metricsWe evaluate parsing performance using labeled F-Measure (combining labeled precision and labeled80DEV SETTERMINAL SYMBOLS F1<40 F1 UAS Tagging Acc.BASELINE 86.06 83.81 89.23 96.44DFL 86.65 84.67 (+0.86) 89.86 96.52DFL+CLUST>200 87.57 85.53 (+1.72) 90.68 96.47AUTOCATLEMMA 86.77 84.52 (+0.71) 89.97 96.25AUTOCATLEMMA+CLUST>200 87.53 85.19 (+1.38) 90.39 96.78GOLDCATLEMMA 87.74 85.53 (+1.72) 91.42 98.49GOLDCATLEMMA+CLUST>200 88.83 86.52 (+2.71) 92.11 99.46TEST SETTERMINAL SYMBOLS F1<40 F1 UAS Tagging Acc.BASELINE 86.16 84.10 89.57 96.97DFL 87.13 85.07 (+0.93) 90.45 97.08DFL+CLUST>200 88.22 86.21 (+2.11) 90.96 96.98AUTOCATLEMMA 86.85 84.83 (+0.73) 90.30 96.58AUTOCATLEMMA+CLUST>200 87.99 86.20 (+2.10) 91.22 97.11GOLDCATLEMMA 88.16 85.90 (+1.80) 91.52 98.54GOLDCATLEMMA+CLUST>200 89.93 87.80 (+3.70) 92.83 99.41Table 3: Parsing performance on the dev set/test set when training and parsing make use of clustered terminal symbols.F1<40 is the F-Measure combining labeled precision and labeled recall for sentences of less than 40 words.
All othermetrics are for all sentences of the dev set/test set.
UAS = Unlabeled attachement score of converted constituencytrees into surface dependency trees.
All metrics ignore punctuation tokens.recall) both for sentences of less than 40 words, andfor all sentences7 .
We also use the unlabeled attach-ment score (UAS), obtained when converting theconstituency trees output by the BKY parsers intosurface dependency trees, using the conversion pro-cedure and software of (Candito et al, 2010)8.
Punc-tuation tokens are ignored in all metrics.7 DiscussionResults are shown in table 3.
Our hope was that us-ing lemmatization would improve overall accuracyof unsupervised clustering, hence leading to betterparsing performance.
However, results using bothmethods are comparable.7Note that often for statistical constituent parsing results aregiven for sentences of less than 40 words, whereas for depen-dency parsing, there is no such limitation.
The experiment DFLand DFL+CLUST>200 are reproduced from the previous work(Candito and Crabb?, 2009).
More precisely, this previous workreports F1 = 88.29 on the test set, but for sentences ?
40words, for a DFL+CLUST>20 experiment, and as previouslymentioned, the dev set was used as validation set for the BKYalgorithm.
We report now F1 = 88.22 for the same less-than-40-words sentences, leaving dev set unused at training time.8The conversion uses head propagation rules to find the headon the right-hand side of the CFG rules, first proposed for En-glish in (Magerman, 1995).
Hence the process is highly sensi-tive to part-of-speech tags.Table 3 shows that both morphological clusteringtechniques (DFL and AUTOCATLEMMA) slightlyimprove performance (+0.97 and +0.73 F1 over thebaseline for the test set)9.
In the case of AUTO-CATLEMMA, morphological ambiguity is totally ab-sent in training set: each terminal symbol is the goldPOS+lemma pair, and hence appears with a uniquepart-of-speech in the whole training set.
But at pars-ing time, the terminal symbols are the POS+lemmapairs predicted by MORFETTE, which are wrong forapproximately 3% of the tokens.
So when com-paring the impact on parsing of the two morpho-logical clustering techniques, it seems that the ad-vantage of lemmatization (a sounder morphologicalclustering compared to the desinflection process) iscounterbalanced by tagging errors that lead to wrongPOS+lemma pairs.
Indeed, it can be verified that9We have computed p-values for pairs of results, us-ing Dan Bikel?s statistical significance tester for evalb out-put (http://www.cis.upenn.edu/ dbikel/software.html).
All ex-periments have a p-value < 0.05 both for recall and preci-sion when compared to the baseline.
The differences betweenDFL and AUTOCATLEMMA, and between DFL+CLUST>200and AUTOCATLEMMA+CLUST>200 are not statistically sig-nificant (p ?
value > 0.2).
The gain obtained by addingthe unsupervised clustering is clearly significant (p ?
value >0.005), both when comparing AUTOCATLEMMA and AUTO-CATLEMMA+CLUST>200, and DFL and DFL+CLUST>200.81BASELINE DFL+CLUST>200 AUTOCATLEMMA+CLUST>200FREQUENCY RANGE #tokens in dev set UAS Tagging UAS Tagging UAS Taggingin original training setany 31733 (100%) 89.23 96.43 90.68 96.45 90.39 96.780 (original unseen) 1892 (5.96%) 84.78 82.56 88.79 89.22 88.64 91.170 < x ?
5 3376 (10.64%) 86.49 94.52 88.68 93.13 88.33 95.415 < x ?
10 1906 (6.01%) 90.35 96.59 91.50 95.02 91.55 96.1210 < x ?
20 2248 (7.08%) 89.55 96.71 91.37 95.42 90.57 95.9120 < x ?
50 3395 (10.70%) 91.87 96.35 92.40 95.96 91.72 95.94x ?
50 18916 (59.61%) 89.53 98.12 90.75 (+1.22) 98.12 90.56 (+1.03) 97.91Table 4: Tagging accuracy and UAS scores for words in the dev set, grouped by ranges of frequencies in the originaltraining set.when parsing the gold POS+lemma pairs (the nonrealistic GOLDCATLEMMA setting10), performanceis greatly improved (+1.80 F1 over the baseline).Replacing morphological clusters by their corre-sponding unsupervised clusters leads to a further im-provement, both for F1 score and for UAS.
But hereagain, using desinflection or tagging+lemmatisationleads to more or less the same improvement.
Butwhile the former method is unlikely improvable, thelatter method might reveal more effective if the per-formance of the tagging/lemmatisation phase im-proves.
The GOLDCATLEMMA+CLUST>200 ex-periment gives the upper bound performance : itleads to a +3.70F1 increase over the baseline, forthe test set.
In that case, the terminal symbols aremade of the perfect POS plus the cluster of the per-fect POS+lemma pair.
Very few such terminal sym-bols are unseen in training set, and all are unam-biguous with respect to part-of-speech (hence the99.46% tagging accuracy).In order to better understand the causes of im-provement, we have broken down the tagging accu-racy scores and the UAS scores according to variousranges of word frequencies.
For word forms in thedev set that occur x times in the original trainingset, for x in a certain range, we look at how manyare correctly tagged by the parsers, and how manyreceive the correct head when constituents are con-verted into surface dependencies.The results are shown in table 4.
Unseen wordsand rare words are much better handled (about +4points for UAS for unseen words, and +2 points10The GOLDCATLEMMA experiment leads to high taggingaccuracy, though not perfect, because of POS+lemma pairspresent in dev/test sets but missing in the training set.for forms appearing less than 5 times).
This issimply obtained because the majority of originalrare or unknowns are replaced by terminal symbols(either a cluster id or the UNKC token, plus suf-fixes) that are shared by many forms in the tree-bank, leading to higher counts.
This can be veri-fied by analyzing tagging accuracies and UAS scoresfor various frequency ranges for the modified termi-nal symbols : the symbols that replace the wordforms in the training set for DFL+CLUST>X andAUTOCATLEMMA+CLUST>X experiments.
Thisis shown in table 5.
It can be seen that the major-ity of the tokens have now high-frequency.
For in-stance for the DFL+CLUST>200 experiment, thereare only 0.09% terminal symbols in the dev set thatare unseen in training set, and 92.62% appear morethan 50 times.
The parsers do not perform verywell on low-frequency modified terminal symbols,but they are so few that it has little impact on theoverall performance.Hence, in our parsing word clusters experiments,there are almost no real unseen anymore, there areonly terminal symbols made of a cluster id or UNKC(plus suffixes).
More precisely, for instance about30% of the original unseen in the dev set, are re-placed by a UNKC* symbol, which means that 70%are replaced by a cluster-based symbol and are thus?connected?
to the known vocabulary.Interestingly, the improvement in performance isalso evident for words with high frequency in theoriginal treebank: for the forms appearing morethan 50 times in the original training set, the UASincreases by +1.22 with DFL+CLUST>200 and+1.03 with AUTOCATLEMMA+CLUST>200 (table4).
This means that despite any imperfections in the82DFL+CLUST>200 AUTOCATLEMMA+CLUST>200FREQUENCY RANGE percentage UAS Tagging percentage UAS Taggingin modified training set of dev set of dev setany 100 90.68 96.45 100 90.39 96.780 (effective unseen) 0.09 86.21 58.62 0.08 84.00 40.000 < x ?
5 0.45 88.19 86.81 0.32 70.30 70.305 < x ?
10 0.64 90.69 91.18 0.37 92.24 79.3110 < x ?
20 1.31 90.12 94.22 0.87 89.53 88.0920 < x ?
50 4.88 88.64 92.19 3.30 86.44 92.65x ?
50 92.62 90.81 96.83 95.07 90.60 97.21replaced by UNKC* 8.58 90.64 88.10 8.73 89.67 90.07Table 5: Tagging accuracy and UAS scores for modified terminal symbols in the dev set, grouped by ranges of fre-quencies in the modified training sets.
The ?replaced by UNKC*?
line corresponds to the case where the desinflectedform or the POS+lemma pair does not appear more than 200 times in the L?est R?publicain corpus.unsupervised Brown clustering, which uses very lo-cal information, the higher counts lead to better es-timates even for high-frequency words.8 Related workWe have already cited the previous work of Koo etal.
(2008) which has directly inspired ours.
Sagaeand Gordon (2009) explores the use of syntacticclustering to improve transition-based dependencyparsing for English : using an available 30 mil-lion word corpus parsed with a constituency parser,words are represented as vectors of paths within theobtained constituency parses.
Words are then clus-tered using a similarity metric between vectors ofsyntactic paths.
The clusters are used as features tohelp a transition-based dependency parser.
Note thatthe word representation for clustering is more com-plex (paths in parse trees), thus these authors haveto cluster a smaller vocabulary : the top 5000 mostfrequent words are clustered.Agirre et al (2008) use the same approach of re-placing words by more general symbols, but thesesymbols are semantic classes.
They test variousmethods to assign semantic classes (gold seman-tic class, most-frequent sense in sense-tagged data,or a fully unsupervised sense tagger).
Though themethod is very appealing, the reported improvementin parsing is rather small, especially for the fully un-supervised method.Versley and Rehbein (2009) cluster words accord-ing to linear context features, and use the clustersas features to boost discriminative German parsingfor unknown words.
Another approach to augmentthe known vocabulary for a generative probabilisticparser is the one pursued in (Goldberg et al, 2009).Within a plain PCFG, the lexical probabilities forwords that are rare or absent in the treebank aretaken from an external lexical probability distribu-tion, estimated using a lexicon and the Baulm-Welchtraining of an HMM tagger.
This is proven useful tobetter parse Hebrew.9 Conclusion and future workWe have provided a thorough study of the resultsof parsing word clusters for French.
We showedthat the clustering improves performance both forunseen and rare words and for medium- to high-frequency words.
For French, preprocessing wordswith desinflection or with tagging+lemmatisationlead to comparable results.
However, the methodusing POS tagging is expected to yield higher per-formance should a better tagger become available inthe future.One avenue for further improvement is to use aclustering technique that makes explicit use of syn-tactic or semantic similarity, instead of simple linearcontext sharing.
While the Brown clustering algo-rithm can be run on large raw corpus, it uses ex-tremely local information (bigrams).
The resultingclusters are thus necessarily noisy, and semantic orsyntactic clustering would certainly be more appro-priate.
Since resource-based semantic clustering isdifficult for French due to a lack of resources, clus-tering based on distributional syntactic similarity isa worthwhile technique to investigate in the future.83AcknowledgmentsThis work was supported by the ANR Sequoia(ANR-08-EMER-013).
We are grateful to ouranonymous reviewers for their comments and toGrzegorz Chrupala for helping us with Morfette.ReferencesAnne Abeill?, Lionel Cl?ment, and Fran?ois Toussenel,2003.
Building a Treebank for French.
Kluwer, Dor-drecht.Eneko Agirre, Timothy Baldwin, and David Martinez.2008.
Improving parsing and PP attachment perfor-mance with sense information.
In Proceedings ofACL-08: HLT, pages 317?325, Columbus, Ohio, June.Association for Computational Linguistics.Peter F. Brown, Vincent J. Della, Peter V. Desouza, Jen-nifer C. Lai, and Robert L. Mercer.
1992.
Class-basedn-gram models of natural language.
Computationallinguistics, 18(4):467?479.Marie Candito and Beno?t Crabb?.
2009.
Im-proving generative statistical parsing with semi-supervised word clustering.
In Proceedings of the11th International Conference on Parsing Technolo-gies (IWPT?09), pages 138?141, Paris, France, Octo-ber.
Association for Computational Linguistics.Marie Candito, Benoit Crabb?, and Pascal Denis.
2010.Statistical french dependency parsing : Treebankconversion and first results.
In Proceedings ofLREC?2010, Valletta, Malta.Grzegorz Chrupa?a, Georgiana Dinu, and Josef van Gen-abith.
2008.
Learning morphology with morfette.
InIn Proceedings of LREC 2008, Marrakech, Morocco.ELDA/ELRA.Benoit Crabb?
and Marie Candito.
2008.
Exp?riencesd?analyse syntaxique statistique du fran?ais.
In Actesde la 15?me Conf?rence sur le Traitement Automatiquedes Langues Naturelles (TALN?08), pages 45?54, Avi-gnon, France.Pascal Denis and Beno?t Sagot.
2009.
Coupling an anno-tated corpus and a morphosyntactic lexicon for state-of-the-art pos tagging with less human effort.
In Proc.of PACLIC, Hong Kong, China.Yoav Freund and Robert E. Schapire.
1999.
Large mar-gin classification using the perceptron algorithm.
Ma-chine learning, 37(3):277?296.Yoav Goldberg, Reut Tsarfaty, Meni Adler, and MichaelElhadad.
2009.
Enhancing unlexicalized parsing per-formance using a wide coverage lexicon, fuzzy tag-setmapping, and EM-HMM-based lexical probabilities.In Proc.
of EACL-09, pages 327?335, Athens, Greece.Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proceedings of the 41stMeeting of the Association for Computational Linguis-tics.Terry Koo, Xavier Carreras, and Michael Collins.
2008.Simple semi-supervised dependency parsing.
In Pro-ceedings of ACL-08, pages 595?603, Columbus, USA.Percy Liang.
2005.
Semi-supervised learning for naturallanguage.
In MIT Master?s thesis, Cambridge, USA.D.M.
Magerman.
1995.
Statistical decision-tree mod-els for parsing.
In Proc.
of ACL?95, pages 276?283,Morristown, NJ, USA.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic cfg with latent annotations.
InProceedings of the 43rd Annual Meeting of the Associ-ation for Computational Linguistics (ACL), pages 75?82.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc.
of ACL-06, Sydney,Australia.Kenji Sagae and Andrew S. Gordon.
2009.
Clusteringwords by syntactic similarity improves dependencyparsing of predicate-argument structures.
In Proceed-ings of the 11th International Conference on Pars-ing Technologies (IWPT?09), pages 192?201, Paris,France, October.
Association for Computational Lin-guistics.Beno?t Sagot.
2010.
The Lefff, a freely availableand large-coverage morphological and syntactic lexi-con for french.
In Proceedings of LREC?10, Valetta,Malta.Djam?
Seddah, Marie Candito, and Benoit Crabb?.
2009.Cross parser evaluation and tagset variation: A FrenchTreebank study.
In Proceedings of the 11th Interna-tion Conference on Parsing Technologies (IWPT?09),pages 150?161, Paris, France, October.
Associationfor Computational Linguistics.Djam?
Seddah, Grzegorz Chrupa?a, Ozlem Cetinoglu,Josef van Genabith, and Marie Candito.
2010.Lemmatization and statistical lexicalized parsing ofmorphologically-rich languages.
In Proceedings of theNAACL/HLT Workshop on Statistical Parsing of Mor-phologically Rich Languages (SPMRL 2010), Los An-geles, CA.Yannick Versley and Ines Rehbein.
2009.
Scalable dis-criminative parsing for german.
In Proceedings of the11th International Conference on Parsing Technolo-gies (IWPT?09), pages 134?137, Paris, France, Octo-ber.
Association for Computational Linguistics.84
