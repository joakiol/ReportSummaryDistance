Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 694?702,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsMultilingual Named Entity Recognition using Parallel Data and Metadatafrom WikipediaSungchul Kim?POSTECHPohang, South Koreasubright@postech.ac.krKristina ToutanovaMicrosoft ResearchRedmond, WA 98502kristout@microsoft.comHwanjo YuPOSTECHPohang, South Koreahwanjoyu@postech.ac.krAbstractIn this paper we propose a method to auto-matically label multi-lingual data with namedentity tags.
We build on prior work utiliz-ing Wikipedia metadata and show how to ef-fectively combine the weak annotations stem-ming from Wikipedia metadata with infor-mation obtained through English-foreign lan-guage parallel Wikipedia sentences.
The com-bination is achieved using a novel semi-CRFmodel for foreign sentence tagging in the con-text of a parallel English sentence.
The modeloutperforms both standard annotation projec-tion methods and methods based solely onWikipedia metadata.1 IntroductionNamed Entity Recognition (NER) is a frequentlyneeded technology in NLP applications.
State-of-the-art statistical models for NER typically requirea large amount of training data and linguistic exper-tise to be sufficiently accurate, which makes it nearlyimpossible to build high-accuracy models for a largenumber of languages.Recently, there have been two lines of work whichhave offered hope for creating NER analyzers inmany languages.
The first has been to devise analgorithm to tag foreign language entities usingmetadata from the semi-structured Wikipedia repos-itory: inter-wiki links, article categories, and cross-language links (Richman and Schone, 2008).
Thesecond has been to use parallel English-foreign lan-guage data, a high-quality NER tagger for English,and projected annotations for the foreign language(Yarowsky et al, 2001; Das and Petrov, 2011).
Par-allel data has also been used to improve existingmonolingual taggers or other analyzers in two lan-guages (Burkett et al, 2010a; Burkett et al, 2010b).
?This research was conducted during the author?s internshipat Microsoft ResearchThe goal of this work is to create high-accuracyNER annotated data for foreign languages.
Herewe combine elements of both Wikipedia metadata-based approaches and projection-based approaches,making use of parallel sentences extracted fromWikipedia.
We propose a statistical model whichcan combine the two types of information.
Simi-larly to the joint model of Burkett et al (2010a), ourmodel can incorporate both monolingual and bilin-gual features in a log-linear framework.
The advan-tage of our model is that it is much more efficientas it does not require summing over matchings ofsource and target entities.
It is a conditional modelfor target sentence annotation given an aligned En-glish source sentence, where the English sentence isused only as a source of features.
Exact inference isperformed using standard semi-markov CRF modelinference techniques (Sarawagi and Cohen, 2004).Our results show that the semi-CRF model im-proves on the performance of projection models bymore than 10 points in F-measure, and that we canachieve tagging F-measure of over 91 using a verysmall number of annotated sentence pairs.The paper is organized as follows: We firstdescribe the datasets and task setting in Section2.
Next, we present our two baseline methods:A Wikipedia metadata-based tagger and a cross-lingual projection tagger in Sections 3 and 4, re-spectively.
We present our direct semi-CRF taggingmodel in Section 5.2 Data and taskAs a case study, we focus on two very dif-ferent foreign languages: Korean and Bulgarian.The English and foreign language sentences thatcomprise our training and test data are extractedfrom Wikipedia (http://www.wikipedia.org).
Cur-rently there are more than 3.8 million articles inthe English Wikipedia, 125,000 in the BulgarianWikipedia, and 131,000 in the Korean Wikipedia.694Figure 1: A parallel sentence-pair showing gold-standard NE labels and word alignments.To create our dataset, we followed Smith et al(2010) to find parallel-foreign sentences using com-parable documents linked by inter-wiki links.
Theapproach uses a small amount of manually annotatedarticle-pairs to train a document-level CRF modelfor parallel sentence extraction.
A total of 13,410English-Bulgarian and 8,832 English-Korean sen-tence pairs were extracted.Of these, we manually annotated 91 English-Bulgarian and 79 English-Korean sentence pairswith source and target named entities as well asword-alignment links among named entities in thetwo languages.
Figure 1 illustrates a Bulgarian-English sentence pair with alignment.The named entity annotation scheme followed hasthe labels GPE (Geopolitical entity), PER (Person),ORG (Organization), and DATE.
It is based on theMUC-7 annotation guidelines, and GPE is synony-mous with Location.
The annotation process wasnot as rigorous as one might hope, due to lack of re-sources.
The English-Bulgarian and English-Koreandatasets were labeled by one annotator each and thenannotations on the English sentences were double-checked by the other annotator.
Disagreements wererare and were resolved after discussion.The task we evaluate on is tagging of foreign lan-guage sentences.
We measure performance by la-beled precision, recall, and F-measure.
We give par-tial credit if entities partially overlap on their span ofwords and match on their labels.Table 1 shows the total number of English,Bulgarian and Korean entities and the percent-age of entities that were manually aligned to anentity of the same type in the other language.The data sizes are fairly small as the data isLanguage Entities Aligned %English 342 93.9%Bulgarian 344 93.3%English 414 88.4%Korean 423 86.5%Table 1: English-Bulgarian and English-Korean datacharacteristics.used only to train models with very few coarse-grained features and for evaluation.
These datasetsare available at http://research.microsoft.com/en-us/people/kristout/nerwikidownload.aspx.As we can see, less than 100% of entities haveparallels in the other language.
This is due to twophenomena: one is that the parallel sentences some-times contain different amounts of information andone language might use more detail than the other.The other is that the same information might be ex-pressed using a named entity in one language, andusing a non-entity phrase in the other language (e.g.
?He is from Bulgaria?
versus ?He is Bulgarian?
).Both of these causes of divergence are much morecommon in the English-Korean dataset than in theEnglish-Bulgarian one.3 Wiki-based tagger: annotating sentencesbased on Wikipedia metadataWe followed the approach of Richman and Schone(2008) to derive named entity annotations of bothEnglish and foreign phrases in Wikipedia, usingWikipedia metadata.
The following sources of in-formation were used from Wikipedia: category an-notations on English documents, article links whichlink from phrases in an article to another article inthe same language, and interwiki links which link695Figure 2: Candidate NEs for the English and Bulgariansentences according to baseline taggers.from articles in one language to comparable (seman-tically equivalent) articles in the other language.
Inaddition to the Wikipedia-derived resources, the ap-proach requires a manually specified map from En-glish category key-phrases to NE tags, but does notrequire expert knowledge for any non-English lan-guage.
We implemented the main ideas of the ap-proach but some implementation details may differ.To tag English language phrases, we first derivednamed entity categorizations of English article titles,by assigning a tag based on the article?s categoryinformation.
The category-to-NE map used for theassignment is a small manually specified map fromphrases appearing in category titles to NE tags.
Forexample, if an article has categories ?People by?,?People from?, ?Surnames?
etc., it is classified asPER.
Looking at the example in Figure 1, the articlewith title ?Igor Tudor?
is classified as PER becauseone of its categories is ?Living people?.
The fullmap we use is taken from the paper (Richman andSchone, 2008).Using the article-level annotations and articlelinks we define a local English wiki-based taggerand a global English wiki-based tagger, which willbe described in detail next.Local EnglishWiki-based tagger.
This Wiki-basedtagger tags phrases in an English article based on thearticle links from these phrases to NE-tagged arti-cles.
For example, suppose that the phrase ?Split?
inthe article with title ?Igor Tudor?
is linked to the ar-ticle with title ?Split?, which is classified as GPE.Thus the local English Wiki-based tagger can tagthis phrase as GPE.
If, within the same article, thephrase ?Split?
occurs again, it can be tagged againeven if it is not linked to a tagged article (this isthe one sense per document assumption).
Addition-ally, the tagger tags English phrases as DATE if theymatch a set of manually specified regular expres-sions.
As a filter, phrases that do not contain a cap-italized word or a number are not tagged with NEtags.Global English Wiki-based tagger.
This taggertags phrases with NE tags if these phrases have everbeen linked to a categorized article (the most fre-quent label is used).
For example, if ?Split?
doesnot have a link anywhere in the current article, buthas been linked to the GPE-labeled article with ti-tle ?Split?
in another article, it will still be taggedas GPE.
We also apply a local+global Wiki-tagger,which tags entities according to the local Wiki-tagger and additionally tags any non-conflicting en-tities according to the global tagger.Local foreign Wiki-based tagger.
The idea is thesame as for the local English tagger, with the dif-ference that we first assign NE tags to foreign lan-guage articles by using the NE tags assigned to En-glish articles to which they are connected with inter-wiki links.
Because we do not have maps from cate-gory phrases to NE tags for foreign languages, usinginter-wiki links is a way to transfer this knowledgeto the foreign languages.
After we have categorizedforeign language articles we follow the same algo-rithm as for the local English Wiki-based tagger.
ForBulgarian we also filtered out entities based on cap-italization and numbers, but did not do that for Ko-rean as it has no concept of capitalization.Global foreign Wiki-based tagger The global andlocal+global taggers are analogous, using the cate-gorization of foreign articles as above.Figure 2 shows the tags assigned to English andBulgarian strings according to the local and globalWiki-based taggers.
The global Wiki-based tag-ger could assign multiple labels to the same string(corresponding to different senses in different oc-currences).
In case of multiple possible labels, themost frequent one is denoted by * in the Figure.
TheFigure also shows the results of the Stanford NERtagger for English (Finkel et al, 2005) (we used theMUC-7 classifier).Table 2 reports the performance of the local (LWiki-tagger), local+global (LG Wiki tagger) and theStanford tagger.
We can see that the local Wiki tag-gers have higher precision but lower recall than thelocal+global Wiki taggers.
The local+global taggers696Language L Wiki-tagger LG Wiki-tagger Stanford TaggerPrec Rec F1 Prec Rec F1 Prec Rec F1English 92.8 75.1 83.0 79.7 89.5 84.3 86.5 77.5 81.7Bulgarian 94.1 48.7 64.2 86.8 79.9 83.2English 92.6 75.6 83.2 84.1 86.7 85.4 82.2 71.9 76.7Korean 89.5 57.3 69.9 43.2 78.0 55.6Table 2: English-Bulgarian and English-Korean Wiki-based tagger performance.are overall best for English and Bulgarian.
The lo-cal tagger is best for Korean, as the precision sufferstoo much due to the global tagger.
This is perhapsdue in part to the absence of the capitalization filterfor Korean which improved precision for Bulgarianand English.
The Stanford tagger is worse than theWiki-based tagger, but it is different enough that itcontributes useful information to the task.4 Projection ModelFrom Table 2 we can see that the English Wiki-based taggers are better than the Bulgarian and Ko-rean ones, which is due to the abundance and com-pleteness of English data in Wikipedia.
In such cir-cumstances, previous research has shown that onecan project annotations from English to the moreresource-poor language (Yarowsky et al, 2001).Here we follow the approach of Feng et al (2004)to train a log-linear model for projection.Note that the Wiki-based taggers do not requiretraining data and can be applied to any sentencesfrom Wikipedia articles.
The projection model de-scribed in this section and the Semi-CRF modeldescribed in Section 5 are trained using annotateddata.
They can be applied to tag foreign sen-tences in English-foreign sentence pairs extractedfrom Wikipedia.The task of projection is re-cast as a ranking task,where for each source entity Si, we rank all possiblecandidate target entity spans Tj and select the bestspan as corresponding to this source entity.
Eachtarget span is labeled with the NE label of the corre-sponding source entity.
The probability distributionover target spans Tj for a given source entity Si isdefined as follows:p(Si|Tj) =exp(?f(Si, Tj))?j?
exp(?f(Si, T?j))where ?
is a parameter vector, and f(Si, Tj) is a fea-ture vector for the candidate entity pair.From this formulation we can see that a fixed setof English source entities Si is required as input.The model projects these entities to correspondingforeign entities.
We train and evaluate the projectionmodel using 10-fold cross-validation on the datasetfrom Table 1.
For training, we use the human-annotated gold English entities and the manually-specified entity alignments to derive correspondingtarget entities.
At test time we use the local+globalWiki-based tagger to define the English entities andwe don?t use the manually annotated alignments.4.1 FeaturesWe present the features for this model in a lot ofdetail since analogous feature types are also used inour final direct semi-CRF model.
The features aregrouped into four categories.Word alignment featuresWe exploit a feature set based on HMM word align-ments in both directions (Och and Ney, 2000).
Todefine the features we make use of the posterioralignment link probabilities as well as the mostlikely (Viterbi) alignments.
The posterior proba-bilities are the probabilities of links in both direc-tions given the source and target sentences: P (ai =j|s, t) and P (aj = i|s, t).If a source entity consists of positions i1, .
.
.
, imand a potential corresponding target entity consistsof positions j1, .
.
.
, jn, the word-alignment derivedfeatures are:?
Probability that each word from one of the en-tities is aligned to a word from the other entity,estimated as:?i?i1...im?j?j1...jn P (ai = j|s, t) We use ananalogous estimate for the probability in theother direction.697?
Sum of posterior probabilities of links fromwords inside one entity to words outside an-other entity?i?i1...im(1 ?
?j?j1...jn P (ai =j|s, t)).
Probabilities from the other HMM di-rection are estimated analogously.?
Indicator feature for whether the source andtarget entity can be extracted as a phrase pairaccording to the combined Viterbi alignments(grow-diag-final) and the standard phrase ex-traction heuristic (Koehn et al, 2003).Phonetic similarity featuresThese features measure the similarity between asource and target entity based on pronunciation.
Weutilize a transliteration model (Cherry and Suzuki,2009), trained from pairs of English person namesand corresponding foreign language names, ex-tracted from Wikipedia.
The transliteration modelcan return an n-best list of transliterations of a for-eign string, together with scores.
For example thetop 3 transliterations in English of the Bulgarianequivalent of ?Igor Tudor?
from Figure 1 are IgorTwoodor, Igor Twoodore, and Igore Twoodore.We estimate phonetic similarity between a sourceand target entity by computing Levenshtein andother distance metrics between the source entityand the closest transliteration of the target (out of a10-best list of transliterations).
We use normalizedand un-normalized Levenshtein distance.
Wealso use a BLEU-type measure which estimatescharacter n-gram overlap.Position/Length featuresThese report relative length and position of theEnglish and foreign entity following (Feng et al,2004).Wiki-based tagger featuresThese features look at the degree of match betweenthe source and target entities based on the tags as-signed to them by the local and global Wiki-taggersfor English and the foreign language, and by theStanford tagger for English.
These are indicator fea-tures separate for the different source-target taggercombinations, looking at whether the taggers agreein their assignments to the candidate entities.4.2 Model EvaluationWe evaluate the tagging F-measure for projec-tion models on the English-Bulgarian and English-Korean datasets.
10-fold cross-validation was usedto estimate model performance.
The foreign lan-guage NE F-measure is reported in Table 3.
The bestWiki-based tagger performance is shown on the lastline as a baseline (repeated from Table 2).We present a detailed evaluation of the model togain understanding of the strengths and limitationsof the projection approach and to motivate our directsemi-CRF model.
To give an estimate of the upperbound on performance for the projection model, wefirst present two oracles.
The goal of the oracles itto estimate the impact of two sources of error for theprojection model: the first is the error in detectingEnglish entities, and the second is the error in deter-mining the corresponding foreign entity for a givenEnglish entity.The first oracle ORACLE1 has access to the gold-standard English entities and gold-standard wordalignments among English and foreign words.
Foreach source entity, ORACLE1 selects the longest for-eign language sequence of words that could be ex-tracted in a phrase pair coupled with the source en-tity word sequence (according the standard phraseextraction heuristic (Koehn et al, 2003)), and labelsit with the label of the source entity.
Note that theword alignments do not uniquely identify the corre-sponding foreign phrase for each English phrase andsome error is possible due to this.
The performanceof this oracle is closely related to the percentage oflinked source-target entities reported in Table 1.
Thesecond oracle ORACLE2 provides the performanceof the projection model when gold-standard sourceentities are known, but the corresponding target en-tities still have to be determined by the projectionmodel (gold-standard alignments are not known).
Inother words, ORACLE2 is the projection model withall features, where in the test set we provide the goldstandard English entities as input.
The performanceof ORACLE2 is determined by the error in automaticword alignment and in determining phonetic corre-spondence.
As we can see the drop due to this erroris very large, especially on Korean, where perfor-mance drops from 90.0 to 81.9 F-measure.The next section in the Table presents the perfor-698Method English-Bulgarian English-KoreanPrec Rec F1 Prec Rec F1ORACLE1 98.3 92.9 95.5 95.5 85.1 90.0ORACLE2 96.7 86.3 91.2 90.5 74.7 81.9PM-WF 71.7 80.0 75.7 85.1 72.2 78.1PM+WF 73.6 81.3 77.2 87.6 74.9 80.8Wiki-tagger 86.8 79.9 83.2 89.5 57.3 69.9Table 3: English-Bulgarian and English-Korean Projection tagger performance.mance of non-oracle projection models, which donot have access to any manually labeled informa-tion.
The local+global Wiki-based tagger is used todefine English entities, and only automatically de-rived alignment information is used.
PM+WF is theprojection model using all features.
The line above,PM-WF represents the projection model withoutthe Wiki-tagger derived features, and is included toshow that the gain from using these features is sub-stantial.
The difference in accuracy between the pro-jection model and ORACLE2 is very large, and is dueto the error of the Wiki-based English taggers.
Thedrop for Bulgarian is so large that the best projec-tion model PM+WF does not reach the performanceof 83.2 achieved by the baseline Wiki-based tagger.When source entities are assigned with error for thislanguage pair, projecting entity annotations from thesource is not better than using the target Wiki-basedannotations directly.
For Korean while the trend inmodel performance is similar as oracle informationis removed, the projection model achieves substan-tially better performance (80.8 vs 69.9) due to themuch larger difference in performance between theEnglish and Korean Wiki-based taggers.The drawback of the projection model is that itdetermines target entities only by assigning the bestcandidate for each source entity.
It cannot create tar-get entities that do not correspond to source entities,it is not able to take into account multiple conflictingsource NE taggers as sources of information, and itdoes not make use of target sentence context and en-tity consistency constraints.
To address these short-comings we propose a direct semi-CRF model, de-scribed in the next section.5 Semi-CRF ModelSemi-Markov conditional random fields (semi-CRFs) are a generalization of CRFs.
They assign la-bels to segments of an input sequence x, rather thanto individual elements xi and features can be de-fined on complete segments.
We apply Semi-CRFsto learn a NE tagger for labeling foreign sentences inthe context of corresponding source sentences withexisting NE annotations.The semi-CRF defines a distribution over foreignsentence labeled segmentations (where the segmentsare named entities with their labels, or segments oflength one with label ?NONE?).
To formally definethe distribution, we introduce some notation follow-ing Sarawagi and Cohen (2005):Let s = ?s1, .
.
.
, sp?
denote a segmentation ofthe foreign sentence x, where a segment sj =?tj , uj , yj?
is determined by its start position tj , endposition uj , and label yj .
Features are defined onsegments and adjacent segment labels.
In our appli-cation, we only use features on segments.
The fea-tures on segments can also use information from thecorresponding English sentence e along with exter-nal annotations on the sentence pair A.The feature vector for each segment can be de-noted by F (j, s,x, e,A) and the weight vector forfeatures by w. The probability of a segmentation isthen defined as:P (s|x, e,A) =?j expw?F (j, s,x, e,A)Z(x, e,A)In the equation above Z represents a normalizersumming over valid segmentations.5.1 FeaturesWe use both boolean and real-valued features in thesemi-CRF model.
Example features and their val-ues are given in Table 4.
The features are the onesthat fire on the segment of length 1 containing theBulgarian equivalent of the word ?Split?
and la-beled with label GPE (tj=13,uj=13,yj=GPE), fromthe English-Bulgarian sentence pair in Figure 1.699The features look at the English and foreign sen-tence as well as external annotations A.
Note thatthe semi-CRF model formulation does not require afixed labeling of the English sentence.
Different andpossibly conflicting NE tags for candidate Englishand foreign sentence substrings according to theWiki-based taggers and the Stanford tagger are spec-ified as one type of external annotations (see Figure2).
Another annotation type is derived from HMM-based word alignments and the transliteration modeldescribed in Section 4.
They provide two kinds ofalignment links between English and foreign tokens:one based on the HMM-word alignments (poste-rior probability of the link in both directions), andanother based on different character-based distancemetrics between transliterations of foreign wordsand English words.
The transliteration model anddistance metrics were described in Section 4 as well.For the example Bulgarian correspondent of ?Split?in the figure, the English ?Split?
is linked to it ac-cording to both the forward and backward HMM,and according to two out of the three transliterationdistance measures.
A third annotation type is au-tomatically derived links between foreign candidateentity strings (sequences of tokens) and best corre-sponding English candidate entities.
The candidateEnglish entities are defined by the union of entitiesproposed by the Wiki-based taggers and the Stan-ford tagger.
Note that these English candidate en-tities can be overlapping and inconsistent withoutharming the model.
We link foreign candidate seg-ments with English candidate entities based on theprojection model described in Section 4 and trainedon the same data.
The projection model scores everysource-target entity pair and selects the best sourcefor each target candidate entity.
For our exampletarget segment, the corresponding source candidateentity is ?Split?, labeled GPE by the local+globalWiki-tagger and by the global Wiki-tagger.The features are grouped into three categories:Group 1.
Foreign Wiki-based tagger features.These features look at target segments and extractindicators of whether the label of the segment agreeswith the label assigned by the local, global, and/orlocal+global wiki tagger.
For the example segmentfrom the sentence in Figure 1, since neither the localnor global tagger have assigned a label GPE, the firstthree features have value zero.
In addition to tags onthe whole segment, we look at tag combinations forindividual words within the segment as well as twowords to the left and right outside the segment.
Inthe first section in Table 4 we can see several featuretypes and and their values for our example.Group 2.
Foreign surface-based features.
Thesefeatures look at orthographic properties of the wordsand distinguish several word types.
The types arebased on capitalization and also distinguish numbersand punctuation.
In addition, we make use of word-clusters generated by JCluster.
1We look at properties of the individual words aswell as the concatenation for all words in the seg-ment.
In addition, there are features for words twowords to the left and two words to the right outsidethe segment.
The second section in the Table showsseveral features of this type with their values.Group 3.
Label match between English andaligned foreign entities.
These features look atthe linked English segment for the candidate tar-get segment and compare the tags assigned to theEnglish segment by the different English taggers tothe candidate target label.
In addition to segment-level comparisons, they also look at tag assignmentsfor individual source tokens linked to the individualtarget tokens (by word alignment and transliterationlinks).
The last section in the Table contains samplefeatures with their values.
The feature SOURCE-E-WIKI-TAG-MATCH looks at whether the correspond-ing source entity has the same local+global Wiki-tagger assigned tag as the candidate target entity.The next two features look at the Stanford taggerand the global Wiki-tagger.
The real-valued fea-tures like SCORE-SOURCE-E-WIKI-TAG-MATCH re-turn the score of the matching between the sourceand target candidate entities (according to the pro-jection model), if the labels match.
In this way, moreconfident matchings can impact the target tags morethan less confident ones.5.2 Experimental resultsOur main results are listed in Table 5.
We perform10-fold cross-validation as in the projection experi-ments.
The best Wiki-based and projection modelsare listed as baselines at the bottom of the table.1Software distributed by Joshua Goodmanhttp://research.microsoft.com/en-us/downloads/0183a49d-c86c-4d80-aa0d-53c97ba7350a/default.aspx.700Method English-Bulgarian English-KoreanPrec Rec F1 Prec Rec F1MONO 86.7 79.4 82.9 89.1 57.1 69.6BI 90.1 83.3 86.6 88.6 79.8 84.0MONO-ALL 94.7 86.2 90.3 90.2 84.3 87.2BI-ALL-WT 95.7 87.6 91.5 92.4 87.6 89.9BI-ALL 96.4 89.4 92.8 94.7 87.9 91.2Wiki-tagger 86.8 79.9 83.2 89.5 57.3 69.9PM+WF 73.6 81.3 77.2 87.6 74.9 80.8Table 5: English-Bulgarian and English-Korean semi-CRF tagger performance.Feature Description Example ValueWIKI-TAG-MATCH 0WIKI-GLOBAL-TAG-MATCH 0WIKIGLOBAL-POSSIBLE-TAG 0WIKI-TAG&LABEL NONE&GPEWIKI-GLOBAL-TAG&LABEL NONE&GPEFIRST-WORD-CAP 1CONTAINS-NUMBER 0PREV-WORD-CAP 0WORD-TYPE&LABEL Xxxx&GPEWORD-CLUSTER& LABEL 101&GPESEGMENT-WORD-TYPE&LABEL Xxxx&GPESEGMENT-WORD-CLUSTER&LABEL Xxxx&GPESOURCE-E-WIKI-TAG-MATCH 1SOURCE-E-STANFORD-TAG-MATCH 0SOURCE-E-WIKI-GLOBAL-TAG-MATCH 1SOURCE-E-POSSIBLE-GLOBAL 1SOURCE-E-ALL-TAG-MATCH 0SOURCE-W-FWA-TAG & LABEL GPE & GPESOURCE-W-BWA-TAG & LABEL GPE & GPESCORE-SOURCE-E-WIKI-TAG-MATCH -0.009SCORE-SOURCE-E-GLOBAL-TAG-MATCH -0.009SCORE-SOURCE-E-STANFORD-TAG-MATCH -1Table 4: Features with example values.We look at performance using four sets of fea-tures: (i) Monolingual Wiki-tagger based, usingonly the features in Group 1 (MONO); (ii) Bilinguallabel match and Wiki-tagger based, using featuresin Groups 1 and 3 (BI); (iii) Monolingual all, us-ing features in Groups 1 and 2 (MONO-ALL), and(iv) Bilingual all, using all features (BI-ALL).
Ad-ditionally, we report performance of the full bilin-gual model with all features, but when English can-didate entities are generated only according to thelocal+global Wiki-taggger (BI-ALL-WT).The main results show that the full semi-CRFmodel greatly outperforms the baseline projectionand Wiki-taggers.
For Bulgarian, the F-measure ofthe full model is 92.8 compared to the best base-line result of 83.2.
For Korean, the F-measure of thesemi-CRF is 91.2, more than 10 points higher thanthe performance of the projection model.Within the semi-CRF model, the contribution ofEnglish sentence context was substantial, leading to2.5 point increase in F-measure for Bulgarian (92.8versus 90.3 F-measure), and 4.0 point increase forKorean (91.2 versus 87.2).The additional gain due to considering candidatesource entities generated from all English taggerswas 1.3 F-measure points for both language pairs(comparing models BI-ALL and BI-ALL-WT).If we restrict the semi-CRF to use only featuressimilar to the ones used by the projection model, westill obtain performance much better than that of theprojection model: comparing BI to the projectionmodel, we see gains of 9.4 points for Bulgarian, and4 points for Korean.
This is due to the fact that thesemi-CRF is able to relax the assumption of one-to-one correspondence between source and target enti-ties, and can effectively combine information frommultiple source and target taggers.We should note that the proposed method can onlytag foreign sentences in English-foreign sentencepairs.
The next step for this work is to train mono-lingual NE taggers for the foreign languages, whichcan work on text within or outside of Wikipedia.Preliminary results show performance of over 80 F-measure for such monolingual models.6 Related WorkAs discussed throughout the paper, our model buildsupon prior work on Wikipedia metadata-based NEtagging (Richman and Schone, 2008) and cross-lingual projection for named entities (Feng et al,2004).
Other interesting work on aligning namedentities in two languages is reported in (Huang andVogel, 2002; Moore, 2003).Our direct semi-CRF tagging approach is relatedto bilingual labeling models presented in previous701work (Burkett et al, 2010a; Smith and Smith, 2004;Snyder and Barzilay, 2008).
All of these modelsjointly label aligned source and target sentences.
Incontrast, our model is not concerned with taggingEnglish sentences but only tags foreign sentences inthe context of English sentences.
Compared to thejoint log-linear model of Burkett et al (2010a), oursemi-CRF approach does not require enumeration ofn-best candidates for the English sentence and is notlimited to n-best candidates for the foreign sentence.It enables the use of multiple unweighted and over-lapping entity annotations on the English sentence.7 ConclusionsIn this paper we showed that using resources fromWikipedia, it is possible to combine metadata-basedapproaches and projection-based approaches for in-ducing named entity annotations for foreign lan-guages.
We presented a direct semi-CRF taggingmodel for labeling foreign sentences in parallel sen-tence pairs, which outperformed projection by morethan 10 F-measure points for Bulgarian and Korean.ReferencesDavid Burkett, John Blitzer, and Dan Klein.
2010a.Joint parsing and alignment with weakly synchronizedgrammars.
In Proceedings of NAACL.David Burkett, Slav Petrov, John Blitzer, and DanKlein.
2010b.
Learning better monolingual modelswith unannotated bilingual text.
In Proceedings ofthe Fourteenth Conference on Computational NaturalLanguage Learning, pages 46?54, Uppsala, Sweden,July.
Association for Computational Linguistics.Colin Cherry and Hisami Suzuki.
2009.
Discrimina-tive substring decoding for transliteration.
In EMNLP,pages 1066?1075.Dipanjan Das and Slav Petrov.
2011.
Unsupervisedpart-of-speech tagging with bilingual graph-based pro-jections.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Linguistics:Human Language Technologies, pages 600?609, Port-land, Oregon, USA, June.
Association for Computa-tional Linguistics.Donghui Feng, Yajuan Lv, and Ming Zhou.
2004.
A newapproach for English-Chinese named entity alignment.In Proceedings of the Conference on Empirical Meth-ods in Natural Language Processing EMNLP, pages372?379.Jenny Finkel, Trond Grenager, and Christopher D. Man-ning.
2005.
Incorporating non-local information intoinformation extraction systems by gibbs sampling.
InACL.Fei Huang and Stephan Vogel.
2002.
Improved namedentity translation and bilingual named entity extrac-tion.
In ICMI.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In HLT-NAACL, pages 127?133.Robert C. Moore.
2003.
Learning translations of named-entity phrases from parallel corpora.
In EACL.Franz Josef Och and Hermann Ney.
2000.
Improved sta-tistical alignment models.
In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics.Alexander E. Richman and Patrick Schone.
2008.Mining wiki resources for multilingual named entityrecognition.
In ACL.Sunita Sarawagi and William W. Cohen.
2004.
Semi-markov conditional random fields for information ex-traction.
In In Advances in Neural Information Pro-cessing Systems 17, pages 1185?1192.Sunita Sarawagi and William W. Cohen.
2005.
Semi-markov conditional random fields for information ex-traction.
In In Advances in Neural Information Pro-cessing Systems 17 (NIPS 2004).David A. Smith and Noah A. Smith.
2004.
Bilin-gual parsing with factored estimation: using Englishto parse Korean.
In EMNLP.Jason R. Smith, Chris Quirk, and Kristina Toutanova.2010.
Extracting parallel sentences from compara-ble corpora using document level alignment.
In HLT,pages 403?411, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Benjamin Snyder and Regina Barzilay.
2008.
Crosslin-gual propagation for morphological analysis.
In AAAI.David Yarowsky, Grace Ngai, and Richard Wicentowski.2001.
Inducing multilingual text analysis tools via ro-bust projection across aligned corpora.
In HLT.702
