Retrieving Meaning-equivalent Sentencesfor Example-based Rough TranslationMitsuo Shimohata Eiichiro SumitaATR Spoken Language TranslationResearch Laboratoriesmitsuo.shimohata@atr.co.jpeiichiro.sumita@atr.co.jpYuji MatsumotoNara Institute ofScience and Technologymatsu@is.aist-nara.ac.jpAbstractExample-based machine translation (EBMT)is a promising translation method for speech-to-speech translation because of its robust-ness.
It retrieves example sentences similar tothe input and adjusts their translations to ob-tain the output.
However, it has problems inthat the performance degrades when input sen-tences are long and when the style of inputsand that of the example corpus are different.This paper proposes a method for retrieving?meaning-equivalent sentences?
to overcomethese two problems.
A meaning-equivalentsentence shares the main meaning with an in-put despite lacking some unimportant informa-tion.
The translations of meaning-equivalentsentences correspond to ?rough translations.
?The retrieval is based on content words, modal-ity, and tense.1 IntroductionSpeech-to-speech translation (S2ST) technologies con-sist of speech recognition, machine translation (MT), andspeech synthesis (Waibel, 1996; Wahlster, 2000; Ya-mamoto, 2000).
The MT part receives speech texts rec-ognized by a speech recognizer.
The nature of speechcauses difficulty in translation since the styles of speechare different from those of written text and are sometimesungrammatical (Lazzari, 2002).
Therefore, rule-basedMT cannot translate speech accurately compared with itsperformance for written-style text .Example-based MT (EBMT) is one of the corpus-based machine translation methods.
It retrieves examplessimilar to inputs and adjusts their translations to obtainthe output (Nagao, 1981).
EBMT is a promising methodfor S2ST in that it performs robust translation of ungram-matical sentences and requires far less manual work thanrule-based MT.However, there are two problems in applying EBMTto S2ST.
One is that the translation accuracy drasticallydrops as input sentences become long.
As the length ofa sentence becomes long, the number of retrieved similarsentences greatly decreases.
This often results in no out-put when translating long sentences.
The other problemarises due to the differences in style between input sen-tences and the example corpus.
It is difficult to acquirea large volume of natural speech data since it requiresmuch time and cost.
Therefore, we cannot avoid using acorpus with written-style text, which is different from thatof natural speech.
This style difference makes retrieval ofsimilar sentences difficult and degrades the performanceof EBMT.This paper proposes a method of retrieving sentenceswhose meaning is equivalent to input sentences to over-come the two problems.
A meaning-equivalent sentencemeans a sentence having the main meaning of an inputsentence despite lacking some unimportant information.Such a sentence can be more easily retrieved than a simi-lar sentence, and its translation is useful enough in S2ST.We call this translation strategy example-based ?roughtranslation.
?Retrieval of meaning-equivalent sentences is based oncontent words, modality, and tense.
This provides robust-ness against long inputs and in the differences in style be-tween the input and the example corpus.
This advantagedistinguishes our method from other translation methods.We describe the difficulties in S2ST in Section 2.
Then,we describe our purpose, features for retrieval, and re-trieval method for meaning-equivalent sentences in Sec-tion 3.
We report an experiment comparing our methodwith two other methods in Section 4.
The experimentdemonstrates the robustness of our method to length ofinput and the style differences between inputs and the ex-ample corpus.0204060801002-5 6-10 11-15 16-Number of SentencesSentence Length (Words)UntranslatedTranslatedFigure 1: Distribution of Untranslated Inputs by Length2 Difficulty in Example-based S2ST2.1 Translation Degradation by Input LengthA major problem with machine translation, regardless ofthe translation method, is that performance drops rapidlyas input sentences become longer.
For EBMT, the longerinput sentences become, the fewer similar example sen-tences exist in the example corpus.
Figure 1 showstranslation difficulty in long sentences in EBMT (Sumita,2001).
The EBMT system is given 591 test sentencesand returns translation result as translated/untranslated.Untranslated means that there exists no similar examplesentences for the input.
Although the EBMT is equippedwith a large example corpus (about 170K sentences), itoften failed to translate long inputs.2.2 Style Differences between Concise andConversationalThe performance of example-based S2ST greatly de-pends on the example corpus.
It is advantageous for anexample corpus to have a large volume and the samestyle as the input sentences.
A corpus of texts dictatedfrom conversational speech is favorable for S2ST.
Un-fortunately, it is very difficult to prepare such an exam-ple corpus since this task requires laborious work such asspeech recording and speech transcription.Therefore, we cannot avoid using a written-style cor-pus, such as phrasebooks, to prepare a sufficiently largevolume of examples.
Contained texts are almost gram-matical and rarely contain unnecessary words.
We callthe style used in such a corpus ?concise?
and the styleseen in conversational speech ?conversational.
?Table 1 shows the average numbers of words in con-cise (Takezawa et al, 2002) and conversational corpora(Takezawa, 1999).
Sentences in conversational style areabout 2.5 words longer than those in concise style in bothLanguageEnglish JapaneseConcise 5.4 6.2Conversational 7.9 8.9Table 1: Number of Words by SentencesLanguage ModelConcise ConversationalConcise 16.4 58.3Test Conversational 72.3 16.3Table 2: Cross PerplexityEnglish and Japanese.
This is because conversationalstyle sentences contain unnecessary words or subordinateclauses, which have the effects of assisting the listener?scomprehension and avoiding the possibility of giving thelistener a curt impression.Table 2 shows cross perplexity between concise andconversational corpora (Takezawa et al, 2002).
Perplex-ity is used as a metric for how well a language modelderived from a training set matches a test set (Jurafskyand Martin, 2000).
Cross perplexities between conciseand conversational corpora are much higher than the self-perplexity of either of the two styles.
This result alsoillustrates the great difference between the two styles.3 Meaning-equivalent SentenceExample-based S2ST has the difficulties described inSection 2 when it attempts to translate inputs exactly.Here, we set our translation goal to translating input sen-tences not exactly but roughly.
We assume that a roughtranslation is useful enough for S2ST, since unimportantinformation rarely disturbs the progress of dialogs andcan be recovered in the following dialog if needed.
Wecall this translation strategy ?rough translation.
?We propose ?meaning-equivalent sentence?
to carryout rough translation.
Meaning-equivalent sentences aredefined as follows:meaning-equivalent sentence(to an input sentence)A sentence that shares the main meaning withthe input sentence despite lacking some unim-portant information.
It does not contain infor-mation additional to that in the input sentence.Important information is subjectively recognizedmainly due to one of two reasons: (1) It can be surmisedfrom the general situation, or (2) It does not place a strongrestriction on the main information.Input Sentence Unimportant?1 Would you take a picture of me?
Yes2 Would you take a picture of this painting?
No3 Could you tell me a Chinese restaurant around here?
Yes4 Could you tell me a Chinese restaurant around here?
No5 My baggage was stolen from my room while I was out.
Yes6 Please change my room because the room next door is noisy.
YesFigure 2: Examples of Unimportant InformationFigure 2 shows examples of unimportant/important in-formation.
Information to be examined is written in bold.The information ?of me?
in (1) and ?around here?
in (3)can be surmised from the general situation, while the in-formation ?of this painting?
in (2) and ?Chinese?
wouldnot be surmised since it denotes a special object.
The sub-ordinate sentences in (4) and (5) are regarded as unimpor-tant since they have small significance and are omittable.3.1 Basic Idea of RetrievalThe retrieval of meaning-equivalent sentence depends oncontent words and basically does not depend on func-tional words.
Independence from functional words bringsrobustness to the difference in styles.However, functional words include important informa-tion for sentence meaning: the case relation of contentwords, modality, and tense.
Lack of case relation infor-mation is compensated by the nature of the restricted do-main.
A restricted domain, as a domain of S2ST, has arelatively small lexicon and meaning variety.
Therefore,if content words included in an input are given, their re-lation is almost determined in the domain.
Informationof modality and tense is extracted from functional wordsand utilized in classifying the meaning of a sentence (de-scribed in Section 3.2.2).This retrieval method is similar to information re-trieval in that content words are used as clues for retrieval(Frakes and Baeza-Yates, 1992).
However, our task hastwo difficulties: (1) Retrieval is carried out not by docu-ments but by single sentences.
This reduces the effective-ness of word frequencies.
(2) The differences in modalityand tense in sentences have to be considered since theyplay an important role in determining a sentence?s com-municative meaning.3.2 Features for Retrieval3.2.1 Content WordsWords categorized as either noun1, adjective, adverb,or verb are recognized as content words.
Interrogatives1Number and pronoun are included.Modality Cluestekudasai (auxiliary verb)Requestteitadakeru (auxiliary verb)shi-tai (expression)Desire te-hoshii (expression)negau (verb)ka (final particle)Questionne (final particle)nai (auxiliary verb or adjective)Negationmasen (auxiliary verb)Tense CluesPast ta (auxiliary verb)Table 3: Clues for Discriminating Modalities in Japaneseare also included.
Words such as particles, auxiliaryverbs, conjunctions, and interjections are recognized asfunctional words.We utilize a thesaurus to expand the coverage of theexample corpus.
We call the relation of two words thatare the same ?identical?
and words that are synonymousin the given thesaurus ?synonymous.
?3.2.2 Modality and TenseThe meaning of a sentence is discriminated by itsmodality and tense, since these factors obviously deter-mine meaning.
We defined two modality groups andone tense group by examining our corpus.
The modal-ity groups are (?request?, ?desire?, ?question?, ?confir-mation?, ?others?,) and (?negation?, ?others?.)
The tensegroup is (?past?, ?others?.)
These modalities and tenseare distinguished by surface clues, mainly by particlesand auxiliary verbs.
Table 3 shows a part of the cluesused for discriminating modalities in Japanese.
Sentenceshaving no clues are classified as others.
Figure 3 2 shows2Japanese content words are written in sans serif style andJapanese functional words in italic style.Modality &Sentence3 Tense4hoteru wo yoyaku shi tekudasai request(Will you reserve this hotel?
)hoteru wo yoyaku shi tai desire(I want to reserve this hotel.
)hoteru wo yoyaku shi mashi ta ka?
question(Did you reserve this hotel?)
pasthoteru wo yoyaku shi tei masen negation(I do not reserve this hotel.
)Figure 3: Sentences and their Modality and Tensesample sentences and their modality and tense.
Clues areunderlined.A speech act is a concept similar to modality in whichspeakers?
intentions are represented.
The two studies in-troduced information of the speech act in their S2ST sys-tems (Wahlster, 2000; Tanaka and Yokoo, 1999).
The twostudies and our method differ in the effect of speech actinformation.
Their effect of speech act information is sosmall that it is limited to generating the translation text.Translation texts are refined by selecting proper expres-sions according to the detected speakers?
intention.3.3 Retrieval and RankingSentences that satisfy the conditions below are recog-nized as meaning-equivalent sentences.1.
It is required to have the same modality and tense asthe input sentence.2.
All content words are included (identical or synony-mous) in the input sentence.
This means that the setof content words of a meaning-equivalent sentenceis a subset of the input.3.
At least one content word is included (identical) inthe input sentence.If more than one sentence is retrieved, we must rankthem to select the most similar one.
We introduce ?focusarea?
in the ranking process to select sentences that aremeaning-equivalent to the main sentence in complex sen-tences.
We set the focus area as the last N words from theword list of an input sentence.
N denotes the number ofcontent words in meaning-equivalent sentences.
This isbecause main sentences in complex sentences tend to beplaced at the end in Japanese.3Space characters are inserted into word boundaries inJapanese texts.4The value ?others?
in all modality/tense groups is omitted.Inputgaishutsu shi teiru aida ni,(While I was out),kaban wo nusuma re mashi ta(my baggage was stolen.
)Meaning-equivalent Sentencebaggu wo nusuma re ta(My bag was stolen).C1 nusumu5 1C2 ( kaban = baggu ) 1C3 - 0C4 - 0C5 wo, re, ta 3C6 suru, teiru, ni, masu 4Figure 4: Example of Conditions for RankingRetrieved sentences are ranked by the conditions de-scribed below.
Conditions are described in order of prior-ity.
If there is more than one sentence having the highestscore under these conditions, the most similar sentence isselected randomly.C1: # of identical words in focus area.C2: # of synonymous words in focus area.C3: # of identical words in non-focus area.C4: # of synonymous words in non-focus area.C5: # of common functional words.C6: # of different functional words.
(the fewer, the higher priority)Figure 4 shows an example of conditions for ranking.Content word in a focus area of input are underlined andfunctional words are written in italic.4 Experiment4.1 Test DataWe used a bilingual corpus of travel conversation, whichhas Japanese sentences and their English translations(Takezawa et al, 2002).
This corpus was sentence-aligned, and a morphological analysis was done on bothlanguages by our morphological analysis tools.
The bilin-gual corpus was divided into example data (Example) andtest data (Concise) by extracting test data randomly fromthe whole set of data.In addition to this, we used a conversational speechcorpus for another set of test data (Takezawa, 1999).
Thiscorpus contains dialogs between a traveler and a hotel5Words are converted to base form.0204060801001-5 6-10 11-15 16-0204060801001-5 6-10 11-15 16-0204060801001-5 6-10 11-15 16-Method-1 Method-2 ProposedConciseConversationalInput Length (Words)Accuracy(%)(Strict with Func. )
(Rough with Func.)
(Rough w/o Func.
)Input Length (Words) Input Length (Words)Accuracy(%)Accuracy(%)Figure 5: Results# of AverageCorpus Sentences LengthExample 92,397 7.4Concise 1,588 6.6Conversational 800 10.1Table 4: Statistics of the Corporareceptionist.
It tests the robustness in styles.
We call thistest corpus ?Conversational.
?We use sentences including more than one contentword among the three corpora.
The statistics of the threecorpora are shown in Table 4.The thesaurus used in the experiment was ?Kadokawa-Ruigo-Jisho?
(Ohno and Hamanishi, 1984).
Each wordhas semantic code consisting of three digits, that is, thisthesaurus has three hierarchies.
We defined ?synony-mous?
words as sharing exact semantic codes.4.2 Compared Retrieval MethodsWe use two example-based retrieval methods to show thecharacteristic of the proposed method.
The first method(Method-1) uses ?strict?
retrieval, which does not al-low missing words in input.
The method takes func-tional words into account on retrieval.
This method cor-responds to the conventional EBMT method.
The secondmethod (Method-2) uses ?rough?
retrieval, which doesallow missing words in input, but still takes functionalwords into account.4.3 Evaluation MethodologyEvaluation was carried out by judging whether retrievedsentences are meaning-equivalent to inputs.
It must benoted that inputs and retrieved sentences are both inJapanese.
We did not compare inputs and translations ofretrieved sentences, since translation accuracy is a matterof the example corpus and does not concern our method.The sentence with the highest score among retrievedsentences was taken and evaluated.
The sentences aremarked manually as meaning-equivalent or not by aJapanese native.
A meaning-equivalent sentence includesall important information in the input but may lack someunimportant information.4.4 ResultsFigure 5 shows the accuracy of the three methods withthe concise and conversational style data.
Accuracy isdefined as the ratio of the number of correctly equivalentsentences to that of total inputs.
Inputs are classified intofour types by their word length.The performance of Method-1 reflects the narrow cov-erage and style-dependency of conventional EBMT.
Thelonger input sentences become, the more steeply its per-formance degrades in both styles.
The method can re-trieve no similar sentence for inputs longer than elevenwords in conversational style.Method-2 adopts a ?rough?
strategy in retrieval.
Itattains higher accuracy than Method-1, especially withlonger inputs.
This indicates the robustness of the roughretrieval strategy to longer inputs.
However, the methodstill has an accuracy difference of about 15% between thetwo styles.The accuracy of the proposed method is better thanthat of Method-2, especially in conversational style.
Theaccuracy difference in longer inputs becomes smaller(about 4%) than that of Method-2.
This indicates the ro-bustness of the proposed method to the differences be-tween the two styles.5 Related Work5.1 EBMTThe rough translation proposed in this paper is a type ofEBMT (Sumita, 2001; Veale and Way, 1997; Carl, 1999;Brown, 2000).
The basic idea of EBMT is that sentencessimilar to the inputs are retrieved from an example corpusand their translations become the basis of outputs.Here, let us consider the difference between ourmethod and other EBMT methods by dividing similar-ity into a content-word part and a functional-word part.In the content-word part, our method and other EBMTmethods are almost the same.
Content words are im-portant information in a similarity measure process, andthesauri are utilized to extend lexical coverage.
In thefunctional-word part, our method is characterized by dis-regarding functional words, while other EBMT meth-ods still rely on them for the similarity measure.
In ourmethod, the lack of functional word information is com-pensated by the semantically narrow variety in S2ST do-mains and the use of information on modality and tense.Consequently, our method gains robustness to length andthe style differences between inputs and the example cor-pus.5.2 Translation MemoryTranslation memory (TM) is aimed at retrieving infor-mative translation example from example corpus.
TMand our method share the retrieval strategy of rough andwide coverage.
However, recall is more highly weightedthan precision in TM, while recall and precision shouldbe equally considered in our method.
To carry outwide coverage retrieval, TM relaxed various conditionson inputs: Preserving only mono-gram and bi-gram onwords/characters (Baldwin, 2001; Sato, 1992), remov-ing functional words (Kumano et al, 2002; Wakita et al,2000), and removing content words (Sumita and Tsut-sumi, 1988).
In our method, information on functionalwords is removed and that on modality and tense is in-troduced instead.
Information on word order is also re-moved while instead we preserve information on whethereach word is located in the focus area.6 ConclusionsIn this paper, we introduced the idea of meaning-equivalent sentences for robust example-based S2ST.Meaning-equivalent sentences have the same main mean-ing as the input despite lacking some unimportant infor-mation.
Translation of meaning-equivalent sentences cor-responds to rough translations, which aim not at exacttranslation with narrow coverage but at rough translationwith wide coverage.
For S2ST, we assume that this trans-lation strategy is sufficiently useful.Then, we described a method for retrieving meaning-equivalent sentences from an example corpus.
Retrievalis based on content words, modality, and tense.
Thisstrategy is feasible owing to the restricted domains, of-ten adopted in S2ST, which have relatively small varietyin lexicon and meaning.
An experiment demonstrated therobustness of our method to input length and the style dif-ferences between inputs and the example corpus.Most MT systems aim to achieve exact translation, butunfortunately they often output bad or no translation forlong conversational speeches.
The rough translation pro-posed in this paper achieves robustness in translation forsuch inputs.
This method compensates for the shortcom-ings of conventional MT and makes S2ST technologymore practical.AcknowledgementsThe research reported here was supported in part by acontract with the Telecommunications Advancement Or-ganization of Japan entitled, ?A study of speech dialoguetranslation technology based on a large corpus?.ReferencesT.
Baldwin.
2001.
Low-cost, high-performance transla-tion retrieval: Dumber is better.
In Proc.
of the 39thACL, pages 18?25.R.
D. Brown.
2000.
Automated generalization of trans-lation examples.
In Proc.
of the 18th COLING.M.
Carl.
1999.
Inducing translation templates forexample-based machine translation.
In Proc.
of the MTSummit VII, pages 250?258.W.
B. Frakes and R. Baeza-Yates, editors.
1992.
Infor-mation Retrieval Data Structures & Algorithms.
Pren-tice Hall.D.
Jurafsky and J. H. Martin, editors.
2000.
Speech andLanguage Processing.
Prentice Hall.T.
Kumano, I. Goto, H. Tanaka, N. Uratani, and T. Ehara.2002.
A translation aid system by retrieving bilingualnews database.
In System and Computers in Japan,pages 19?29.G.
Lazzari.
2002.
The V1 framework program in Eu-rope: Some thoughts about speech to speech trans-lation research.
In Proc.
of 40th ACL Workshop onSpeech-to-Speech Translation, pages 129?135.M.
Nagao.
1981.
A framework of a mechanical transla-tion between Japanese and English by analogy princi-ple.
In Artificial and Human Intelligence, pages 173?180.S.
Ohno and M. Hamanishi, editors.
1984.
Ruigo-Shin-Jiten.
Kadokawa.
(in Japanese).S.
Sato.
1992.
CTM: An example-based translation aidsystem.
In Proc.
of the 14th COLING, pages 1259?1263.E.
Sumita and Y. Tsutsumi.
1988.
A translation aidsystem using flexible text retrieval based on syntax-matching.
In TRL Research Report TR87-1019.
IBMTokyo Research Laboratory.E.
Sumita.
2001.
Example-based machine translationusing DP-matching between work sequences.
In Proc.of the ACL 2001 Workshop on Data-Driven Methodsin Machine Translation, pages 1?8.T.
Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, andS.
Yamamoto.
2002.
Toward a broad-coverage bilin-gual corpus for speech translation of travel conversa-tions in the real world.
In Proc.
of the 3rd LREC, pages147?152.T.
Takezawa.
1999.
Building a bilingual travel con-versation database for speech translation research.
InProc.
of the 2nd international workshop on East-Asianresources and evaluation conference on language re-sources and evaluation, pages 17?20.H.
Tanaka and A. Yokoo.
1999.
An efficient statisticalspeech act type tagging system for a speech translationsystems.
In Proc.
of the Association for ComputationalLinguistics, pages 381?388.T.
Veale and A.
Way.
1997.
Gaijin: A bootstrapping,template-driven approach to example-based MT.
InProc.
of the NeMNLP97.W.
Wahlster, editor.
2000.
Verbmobil: Foundations ofSpeech-to-Speech Translation.
Springer.Alex Waibel.
1996.
Interactive translation of conversa-tional speech.
IEEE Computer, 29(7):41?48.Y.
Wakita, K. Matsui, and Y. Sagisaka.
2000.
Finekeyword clustering using a thesaurus and example se-tences for speech translation.
In Proc.
of InternationalConference of Speech Language Processing, pages390?393.S.
Yamamoto.
2000.
Toward speech communicationsbeyond language barrier - research of spoken languagetranslation technologies at ATR -.
In Proc.
of ICSLP,volume 4, pages 406?411.
