Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 716?721,Dublin, Ireland, August 23-24, 2014.UMCC_DLSI_SemSim: Multilingual System for MeasuringSemantic Textual SimilarityAlexander Ch?vezH?ctor D?vilaDI, University of Matanzas, Cuba.
{alexander.chavez,hector.davila}@umcc.cuYoan Guti?rrezAntonio Fern?ndez-Orqu?nAndr?s MontoyoRafael Mu?ozDLSI, University of Alicante, Spain.
{ygutierrezmontoyo,rafael}@dlsi.ua.es,antonybr@yahoo.comAbstractIn this paper we describe thespecifications and results ofUMCC_DLSI system, which wasinvolved in Semeval-2014 addressing twosubtasks of Semantic Textual Similarity(STS, Task 10, for English and Spanish),and one subtask of Cross-Level SemanticSimilarity (Task 3).
As a supervisedsystem, it was provided by different typesof lexical and semantic features to train aclassifier which was used to decide thecorrect answers for distinct subtasks.These features were obtained applying theHungarian algorithm over a semanticnetwork to create semantic alignmentsamong words.
Regarding the Spanishsubtask of Task 10 two runs weresubmitted, where our Run2 was the bestranked with a general correlation of 0.807.However, for English subtask our best run(Run1 of our 3 runs) reached 16th place of38 of the official ranking, obtaining ageneral correlation of 0.682.
In terms ofTask 3, only addressing Paragraph toSentence subtask, our best run (Run1 of 2runs) obtained a correlation value of 0.760reaching 3rd place of 34.1 IntroductionMany applications of language processing rely onmeasures of proximity or remoteness of variouskinds of linguistic units (words, meanings,sentences, documents).
Thus, issues such asdisambiguation of meanings, detection of lexicalchains, establishing relationships betweendocuments, clustering, etc., require accuratesimilarity measures.The problem of formalizing and quantifying anintuitive notion of similarity has a long history inphilosophy, psychology, artificial intelligence,and through the years has followed many differentperspectives (Hirst, 2001).
Recent research in thefield of Computational Linguistics hasemphasized the perspective of semantic relationsbetween two lexemes in a lexical resource, or itsinverse, semantic distance.
The similarity ofsentences is a confidence score that reflects therelationship between the meanings of twosentences.
This similarity has been addressed inthe literature with terminologies such as affinity,proximity, distance, difference and divergence(Jenhani, et al., 2007).
The different applicationsof text similarity have been separated into a groupof similarity tasks: between two long texts, fordocument classification; between a short text witha long text, for Web search; and between two shorttexts, for paraphrase recognition, automaticmachine translation, etc.
(Han, et al., 2013).At present, the calculation of the similaritybetween texts has been tackled from differentpoints of views.
Some have opted for a singlemeasure to capture all the features of texts andother models have been trained with variousmeasures to take text features separately.
In thiswork, we addressed the combination of severalmeasures using a Supervised Machine Learning(SVM) approach.
Moreover, we intend tointroduce a new approach to calculate textualsimilarities using a knowledge-based system,which is based on a set of cases composed by avector with values of several measures.
We alsocombined both approaches.This work is licensed under a Creative CommonsAttribution 4.0 International Licence.
Page numbers andproceedings footer are added by the organisers.
Licencedetails: http://creativecommons.org/licenses/by/4.0/716After this introduction, the rest of the paper isorganized as follows.
Section 2 shows the Pre-processing stage.
Subsequently, in Section 3 weshow the different features used in our system.
InSection 4 we describe our knowledge-basedsystem.
Tasks and runs are provided in Section 5.Finally, the conclusions and further work can befound in Section 6.2 Pre-processingBelow are listed the pre-processing stepsperformed by our system.
In bold we emphasizesome cases which were used in different tasks.?
All brackets were removed.?
The abbreviations were expanded to theirrespective meanings.
It was applied using alist of the most common abbreviations inEnglish, with 819 and Spanish with 473.Phrases like ?The G8?
and ?The Group ofEight?
are detected as identical.?
Deletion of hyphen to identify wordsforms.
For example, ?well-studied?
wasreplaced by ?well studied?.
Example takenfrom line 13 of MSRpar corpus in test setof Semeval STS 2012 (Agirre, et al., 2012).?
The sentences were tokenized and POS-tagged using Freeling 3.0 (Padr?
andStanilovsky, 2012).?
All contractions were expanded.
Forexample: n't, 'mand 's.
In the case of 's wasreplaced with ?is?
or ?of?, ?Tom's bad?
to?Tom is bad?
and ?Tom's child?
by "Childof Tom".
(Only for English tasks).?
Punctuation marks were removed from thetokens except for the decimal point innumbers.?
Stop words were removed.
We used a listof the most common stop words.
(28 forEnglish and 48 for Spanish).?
The words were mapped to the mostcommon sense of WordNet 3.0.
(Only forSpanish task).?
A syntactic tree was built for everysentence using Freeling 3.0.1 The windows is the number of intermediate wordsbetween two words.2 Dataset of high quality English paragraphs containing overthree billion words and it is available inhttp://ebiquity.umbc.edu/resource/html/id/3513 Features ExtractionMeasures of semantic similarity have beentraditionally used between words or concepts, andmuch less between text segments, (i.e.
two ormore words).
The emphasis on word to wordsimilarity is probably due to the availability ofresources that specifically encode relationsbetween words or concepts (e.g.
WordNet)(Mihalcea, et al., 2006).
Following we describethe similarity measures used in this approach.3.1 Semantic Similarity of WordsA relatively large number of word to wordsimilarity metrics have previously been proposedin the literature, ranging from distance-orientedmeasures computed on semantic networks, tometrics based on models of distributionalsimilarity learned from large text collections(Mihalcea, et al., 2006).3.2 Corpus-based MeasuresCorpus-based measures of word semanticsimilarity try to identify the degree of similaritybetween words using information exclusivelyderived from large corpora (Mihalcea, et al.,2006).
We considered one metric named LatentSemantic Analysis (LSA) (Landauer, et al., 1998).Latent Semantic Analysis: The Latentsemantic analysis (LSA) is a corpus/documentbased measure proposed by Landauer in 1998.
InLSA term co-occurrences in a corpus are capturedby means of a dimensionality reduction operatedby singular value decomposition (SVD) on theterm-by-document matrix ?
representing thecorpus (Mihalcea, et al., 2006).
There is avariation of LSA called HAL (HyperspaceAnalog to Language) (Burgess, et al., 1998) thatis based on the co-occurrence of words in acommon context.
The variation consists ofcounting the number of occurrences in that twowords appear at n1 distance (called windows).For the co-occurrence matrix of words we tookas core the UMBC WebBase corpus2 (Han, et al.,2013), which is derived from the StanfordWebBase project3 .
For the calculation of HALmeasure we used the Cosine Similarity betweenthe vectors for each pair of words.3 Stanford WebBase 2001. http://bit.ly/WebBase.7173.3 Knowledge-based MeasuresThere are many measures developed to quantifythe degree of semantic relation between twowords senses using semantic networkinformation.
For example:Leacock & Chodorow Similarity: TheLeacock & Chodorow (LC) similarity isdetermined as follows:?????
= ?
log (??????2??)
(1)Where length is the length of the shortest pathbetween senses using node-counting and D is themaximum depth of the taxonomy (Leacock andChodorow, 1998)Wu and Palmer: The Wu and Palmersimilarity metric (Wup) measures the depth of twogiven senses in the WordNet taxonomy, and thedepth of the least common subsumer (LCS), andcombine them into a similarity score (Wu andPalmer, 1994):??????
=2??????(???)?????(?????1)+?????(????
?2)(2)Resnik: The Resnik similarity (Res) returnsthe information content (IC) of the LCS of twosenses:??????
= ??(???)
(3)Where IC is defined as:??(?)
= ?
log?(?)
(4)And P(c) is the probability of encountering aninstance of sense c in a large corpus (Resnik,1995) (Mihalcea, et al., 2006).Lin: The Lin similarity builds on Resnik?smeasure and adds a normalization factorconsisting of the information content of the twoinputs senses (Lin, 1998):??????
=2???(???)??(?????)+??(????
?2)(5)Jiang & Conrath: The Jiang and Conrathsimilarity (JC) is defined as follows (Jiang andConrath, 1997):?????
=1??(?????1)+??(?????2)?2???(???
)(6)PathLen: The PathLen similarity (Len)involves the path lengths between two senses inthe taxonomy (Pedersen, et al., 2004).4 Copyright (c) 2006 by Chris Parkinson, available inhttp://sourceforge.net/projects/simmetrics/???????
= ?
log ???????(????
?1, ????
?1)(7)Where ???????(????
?1, ????
?1)  is thenumber of edges in the shortest path between????
?1and ????
?2.Word Similarity: In order to calculate thesimilarity between two words (WS) we used thefollowing expression:??
(?1,?2) =???
?1 ?
??????
(?1)?2 ?
??????(?2)???
(?1, ?2)(8)Where  ???
(?1, ?2)  is one of the similaritymetrics at sense level previously described.3.4 Lexical FeaturesWe used a well-known lexical attributes similaritymeasures based on distances between strings.Dice-Similarity, Euclidean-Distance, Jaccard-Similarity, Jaro-Winkler, Levenstein Distance,Overlap-Coefficient, QGrams Distance, Smith-Waterman, Smith-Waterman-Gotoh, Smith-Waterman-Gotoh-Windowed-Affine.These metrics have been obtained from an API(Application Program Interface) SimMetricslibrary v1.5 for.NET4 2.0.3.5 Word Similarity ModelsWith the purpose of calculating the similaritybetween two words, we developed two modelsinvolving the previous word similarity metrics.These were defined as:Max Word Similarity: The Max WordSimilarity (MaxSim) is defined as follows:??????
(?1,?2) ={1              ????????????
(?1,?2) = 1???
(??????
(?1,?2), ??????
(?1,?2))(9)Where ??????????
(?1,?2) is the QGram-Distance between w1 and w2.Statistics and Weight Ratio: For calculatingthe weight ratio in this measure of similarity wasused WordNet 3.0 and it was defined in (10):?????????
(?1,?2) =(??????
(?1,?2) + (1??????
(?1,?2)))2(10)718Where ??????
(?1,?2) takes a value basedon the type of relationship between w1 and w2.The possible values are defined in Table 1.Value Relation between ?1 and ?210 Antonym.1 Synonym.2 Direct Hypernym, Similar_To orDerivationally Related Form.3 Two-links indirect Hypernym, Similar_Toor Derivationally Related Form.3 One word is often found in the gloss of theother.9 Otherwise.Table 1: Values of Weight Ratio.3.6 Sentence AlignmentIn the recognition of texts?
similarities, severalmethods of lexical alignment have been used andcan be appreciated by different point of views(Brockett, 2007) (Dagan, et al., 2005).
Glickman(2006) used the measurement of the overleapgrade between bags of words as a form ofsentence alignment.
Rada et al.
(2006) madereference to an all-for-all alignment, leaving openthe possibility when the same word of a sentenceis aligned with several sentences.
For this task, weused the Hungarian assignment algorithm as away to align two sentences (Kuhn, 1955).
Usingthat, the alignment cost between the sentences wasreduced.
To increase the semantic possibilities weused all word similarity metrics (including the twoword similarity models) as a function cost.3.7 N-Grams AlignmentUsing the Max Word Similarity model, wecalculated three features based on 2-gram, 3-gramand 4-gram alignment with the Hungarianalgorithm.4 Knowledge-based SystemFor similarity calculation between two phrases,we developed a knowledge-based system usingSemEval-2012, SemEval-2013 and SemEval-2014 training corpus (Task 10 and Task 1 for thelast one).
For each training pair of phrases weobtained a vector with all measures explainedabove.
Having it, we estimated the similarityvalue between two new phrases by applying theEuclidian distance between the new vector (madewith the sentence pair we want to estimate thesimilarity value) and each vector in the trainingcorpus.
Then, the value of the instance with minorEuclidian Distance was assigned to the new pairof phrases.5 Tasks and runsOur system participated in Sentence to Phrasesubtask of Task 3: ?Cross-Level SemanticSimilarity?
(Jurgens, et al., 2014) and in twosubtasks of Task 10: ?Multilingual SemanticTextual Similarity?
of SemEval-2014.
It isimportant to remark that our system, using SVMapproach, did not participate in Task 1:?Evaluation of compositional distributionalsemantic models on full sentences throughsemantic relatedness and textual entailment?, dueto deadline issues.
We compared our systemresults with the final ranking of Task 1 and wecould have reached the 6th place of the ranking forRelatedness Subtask with a 0.781 of correlationcoefficient, and the 9th place for EntailmentSubtask with an accuracy of 77.41%.Task10SpTask 10EnTask 3SentencetoPhraseFeatures/Runs 1 2 1 2 3 1 2PathLenAlign x  x x  x xResAlign x  x x  x xLcAlign x  x x  x xWupAlign x  x x  x xRes x  x x  x xLc x  x x  x xDiceSimilarity x x x x  x xEuclideanDistance x x x x  x xJaccardSimilarity x x x x  x xJaroWinkler x x x x  x xLevenstein x x x x  x xOverlap-Coefficientx x x x  x xQGramsDistance x x x x  x xSmithWaterman x x x x  x xSmithWatermanGotoh x x x x  x xSmithWatermanGotoh-WindowedAffinex x x x  x xBiGramAlingHungMax x  x x  x xTriGramAlingHungMax x  x x  x xTetraGramAlingHungMax x  x x  x xWordAlingHungStatWeigthRatio x  x x  x xSentenceLengthPhrase1 x  x x  x xSentenceLengthPhrase2 x  x x  x xTable 2: Features and runs.
Spanish (Sp) andEnglish (En).In Table 2 is important to remark thefollowing situations:?
In Task 10 Spanish (two runs), we used thetraining corpus of Task 10 English.719?
In Run2 of Task 10 English, the similarityscore was replaced for the knowledge-based system value if Euclidean Distanceof the most similar case was less than 0.30.?
Run3 of Task 10 English was a knowledge-based system.?
In Run1 of Sentence to Phrase of Task 3,we trained the SVM model using only thetraining corpus of this task.?
In Run2 of Sentence to Phrase of Task 3,we trained the SVM model using thetraining corpus of this task and the trainingcorpus of Task 10 English.6 ConclusionIn this paper we introduced a new framework forrecognizing Semantic Textual Similarity,involving feature extraction for SVM model and aknowledge-based system.
We analyzed differentways to estimate textual similarities applying thisframework.
We can see in Table 3 that all runsobtained encouraging results.
Our best run wasfirst position of the ranking for task 10 (Spanish)and other important positions were reached in theothers subtasks.
According to our participation,we used a SVM which works with featuresextracted from six different strategies: String-Based Similarity Measures, Semantic SimilarityMeasures, Lexical-Semantic Alignment,Statistical Similarity Measures and SemanticAlignment.
Finally, we can conclude that oursystem achieved important results and it is able tobe applied on different scenarios, such as task 10,task 3.1 and task 1.
See Table 3 and the beginningof Section 5.Subtask RunSemEval-2014PositionTask 10-SpanishRun1 4Run2 1Task 10-EnglishRun1 16Run2 18Run3 33Task-3Run1 3Run2 16Table 3: SemEval-2014 results.As further work, we plan to analyze the maindifferences between task 10 for Spanish andEnglish in order to homogenise both system?sresults.AcknowledgmentsThis research work has been partially funded bythe University of Alicante, GeneralitatValenciana, Spanish Government and theEuropean Commission through the projects,"Tratamiento inteligente de la informaci?n para laayuda a la toma de decisiones" (GRE12-44),ATTOS (TIN2012-38536-C03-03), LEGOLANG(TIN2012-31224), SAM (FP7-611312), FIRST(FP7-287607) and ACOMP/2013/067.ReferenceEneko Agirre, Mona Diab, Daniel Cer and AitorGonzalez-Agirre, 2012.
SemEval 2012 Task 6: APilot on Semantic Textual Similarity..
s.l., FirstJoin Conference on Lexical and ComputationalSemantic (*SEM), Montr?al, Canada.
2012., pp.385-393.Chris Brockett, 2007.
Aligning the RTE 2006 Corpus.Microsoft Research, p. 14.Curt Burgess, Kay Livesay and Kevin Lund, 1998.Explorations in Context Space: Words,Sentences, Discourse.
Discourse Processes, Issue25, pp.
211 - 257.Ido Dagan, Oren Glickman and Bernardo Magnini,2005.
The PASCAL Recognising TextualEntailment Challenge.
En: Proceedings of thePASCAL Challenges Workshop on RecognisingTextual Entailment.Oren Glickman, Ido Dagan and Moshe Koppel, 2006.A Lexical Alignment Model for ProbabilisticTextual Entailment.
In: Proceedings of the FirstInternational Conference on Machine LearningChallenges: Evaluating Predictive UncertaintyVisual Object Classification, and RecognizingTextual Entailment.
Southampton, UK: Springer-Verlag, pp.
287--298.Lushan Han et al., 2013.
UMBC_EBIQUITY-CORE:Semantic Textual Similarity Systems.
s.l., s.n.Alexander B. Hirst and Graeme, 2001.
Semanticdistance in WordNet: An experimental,application-oriented evaluation of five measures.Ilyes Jenhani, Nahla Ben Amor and Zi Elouedi, 2007.Information Affinity: A New Similarity Measurefor Possibilistic Uncertain Information.
En:Symbolic and Quantitative Approaches toReasoning with Uncertainty.
s.l.
:Springer BerlinHeidelberg, pp.
840-852.Jay Jiang and David Conrath, 1997.
Semanticsimilarity based on corpus statistics and lexicaltaxonomy.
s.l., Proceedings of the InternationalConference on Research in ComputationalLinguistics.David Jurgens, Mohammad Taher and RobertoNavigli, 2014.
SemEval-2014 Task 3: Cross-720Level Semantic Similarity.
Dublin, Ireland, InProceedings of the 8th International Workshop onSemantic Evaluation., pp.
23-24.Harold W. Kuhn, 1955.
The Hungarian Method for theassignment problem.
Naval Research LogisticsQuarterly.Thomas K. Landauer, Peter W. Foltz and DarrellLaham, 1998.
Introduction to latent semanticanalysis.
Discourse Processes, Issue 25, pp.
259-284.Claudia Leacock and Martin Chodorow, 1998.Combining local context and WordNet sensesimilarity for word sense identification.
s.l.
:s.n.Lin Dekang, 1998.
An information-theoretic definitionof similarity.
s.l., Proceedings of the InternationalConf.
on Machine Learning.Rada Mihalcea, Courtney Corley and CarloStrapparava, 2006.
Corpus-based andknowledge-based measures of text semanticsimilarity.
In: IN AAAI?06.
s.l.
:21st NationalConference on Artificial Intelligence, pp.
775--780.Lu?s Padr?
and Evgeny Stanilovsky, 2012.
FreeLing3.0: Towards Wider Multilinguality.
Istanbul,Turkey, Proceedings of the Language Resourcesand Evaluation Conference (LREC 2012) ELRA.Ted Pedersen, Siddharth Patwardhan and JasonMichelizzi, 2004.
WordNet::Similarity -Measuring the Relatedness of Concepts.American Association for Artificial Intelligence.Philip Resnik, 1995.
Using information content toevaluate semantic similarity.
s.l., Proceedings ofthe 14th International Joint Conference onArtificial Intelligence.Zhibiao Wu and Martha Palmer, 1994.
Verb semanticsand lexical selection.721
