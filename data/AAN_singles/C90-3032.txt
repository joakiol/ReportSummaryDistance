SYNTACTIC  NORMALIZAT ION OF  SPONTANEOUS SPEECH*Hagen Langer,  University of  Bielefeld, W-GermanyABSTRACTThis paper presents some techniques that provide astandard parsing system for the analysis of ill-formedutterances.
These techniques are feature generalizationand heuristically driven deletions.PROBLEMGenerally the development of grammars, formalismsand natural language processors is based on writtenlanguage data or, sometimes, not real data at all, butinvented 'example sentences'.
This holds for bothcomputational and general linguistics.
Thus manyparsing systems that work quite well for sentences likela.
and lb.
fail, if they get applied to the authenticdata in 2a.
and 2b.
:la.lb.2a.2b.die Grundform ist nicht eckigthe basic form is not angulardas blaue habe ich als Waage auf dem gr0nen llegenI have got the blue one lying upon theOAT greenOATOneDAT like a balancedie die Grund die Grundform sind is nich Is nleh eckigthe the basic the basic form are is not is not angulardas blaue hab ich ale Waage aul das gr0ne liegenI have got the blue one lying upon theACC greenAcconeACC like a balanceTo native recipients the utterances in 2. appear to bemore or less defective, but interpretable xpressions.Moreover, the interpretation of 2a.
or 2b.
might re-quire even less effort than, for instance, understandingan absolutely grammatical 'garden path sentence'.Since utterances like 2a.
and 2b.
occur quite fre-quently in spontaneous speech, an approach to parsingeveryday language has to provide techniques thatcover repairs, ungrammatical repetitions (2a.
), case-assignment violation (2b.
), agreement errors and otherphenomena that have been summarized under the label'iU-formed' in earlier research (Kwasny/Sondheimer"1 am indebted to Dafydd Gibbon, Hans Karlgren and HannesRieser for their comments on earlier drafts of this paper.
Thisresearch was supported by the Deutsche Forschungsgemein-schaft.
Some aspects are discussed in more detail in Langer1990.1981, Jensen et al 1983, Weischedel/Sondheimer1983, Lesmo/Torasso 1984, Kudo et al 1988).Though the present paper will adhere to this termi-nology, it should be emphasized that it is not pre-supposed that there are any general criteria preciseenough to tell us exactly whether some utterance is'ill-formed' relative to a natural language.
Let usassume, instead, that some utterance U is 'ill-formed(defective, irregular .
.
.
.  )
with respect to a grammarG' iff U is not a sentence of the language specified byG.
Since, for instance, repairs exhibit a high degreeof structural regularity (el.
Schegloff et al 1977,Lever 1983, Kindt/Laubenstein in preparation) onemight prefer to describe them within the grarxmaar andnot within some other domain (e.g.
within a pro-duction/perception model).
Therefore the concept 'ill--formed' is used as a relational term that always has tobe re-defined with respect o the given context.There have been two main directions in the priorresearch on ill-formedness.
The one direction hasfocussed on the problem of parsing ill-formed input inrestricted domain applications, such as naturallanguage interfaces to databases or robot assemblysystems (Lesmo/Torasso 1984, Self ridge 1986,Carbonell/Hayes 1.987).
Though the techniques de-veloped in that field seem to be quite adequate for theintended purposes, the results are not directly trans-ferable to the interpretation of spontaneous peech,since the restrictions affect not only the topicaldomains but also the linguistic phenomena under con-sideration: e.g.
the CASPAR parser (cf.
Carbonell/Hayes 1987) is restricted to a subset o f  inlperatives,Lesmo/Torasso (1984) achieve interpretations for ill-formed word order only at the price of neglectinglong distance dependencies tc.The other main direction has been the'relaxation'-approach (Kwasny/Sondheimer 1981,Weischedel/Sondheimer 1983).
The basic idea is torelax those grammatical constraints an input stringdoes not meet, if a parse would fail otherwise.
Themain problem of this approach is that relaxing con-straints (i.e.
ignoring them) makes a grammar lessprecise.
Thus, for instance, a noun phrase that lacksagreement in number is analysed as a noun phrasewithout number and it remains unexplicated how thisanalysis might support a further interpretation.
Surpris-ingly, none of these papers concentrates on real life180 1~q~ontaneous speech (most of them are explicitly eon~cerned with written man-machine communication).The present paper focusses the problem of norm-Mization, i.e.
how to define the relation between ill-,brined utterances (e.g.
2a.
and 2b.)
and their well-formed 'counterpa~s' (la.
and lb.).
A sentence is anadequate normalization of an ill-formed utterance, if itcorresponds m our intuitions about what the speakermight have intended to say.
This is, of course, notobservable, but a request for repetition (which typical-ly does not give rise to a literally repetition in case of~n utterance like 2a.)
might serve as a suitable test.In the present approach normalization is based on~olely syntactic heuristics, not because syntactic in-tbrmation is regarded to be sufficient, but as a startingpoint for further work.
Thus, the normalizationsachieved on the basis of these heuristics erve as de-hlult interpretations that have to be evaluated usingadditional intbrmation about the linguistic and situa-tional context.
The empirical background is a corpusof authentic German dialogues about block worlds thathas been recorded tbr the study of coherence pheno-mena (cf.
Forschergruppe Koh@enz \[ed.\] 1987).I will discuss three heuristics that are used in anexperimental normalization system, called NOBUGS(NOrmalisierungskomponente im Bielefelder Unifika-tionsbasierten Analysesystem f/Jr Gesprochene Sprachenormalization component of a Bie!efeM tmifica-tion-based ,;peech analysis system).
The core ofNOBUGS is a left-corner parser that interprets aGPSG-Iike formalism encoded in DCG notation.
Thegrammars used with NOBUGS are very restrictive andexclude everything that is beyond the bounds of writ-ten standard German.
But in combination with theheuristics I will discuss now the system is capable ofhandling a wider range of phenomena includingmorpho-syntactic deviations, explicit repair and un-grammatical repetitions.MORPHO-SYNTACTIC DEVIATIONSMorpho-syntactic deviations make up a considerableproportion of errors both in spoken and writtenGerman (German has a much more complex inflect-ional morphology than English).The basic principle of this approach to normaliza-tion is as follows:'Fry to find out which properties of a given inputstring make a parse fail and use the given grammat-ical knowledge to alter the input string minimally sothat it is as similar as l~ssible to its initial state butwithout he properties that caused the thilure.What is meant by that can easily be seen if we con-sider an example where the property that makes aparse fail is evident, e.g.
the string 'John sleep',which lacks tile NP-VP-agreement concerning personand mtmber that is required by the following rule:cat  = S eat  = NP cat = VPperson  = X~ ~ ease  = nom person  = X~num= X 2 person  = X~ num= X~num= X2This rule is not applicable to 'John sleep', since thereare no lexieal entries for 'John' and 'sleep', respec-tively, that have unifiable specifications for person andnumber, and this makes the whole parse fail.The strategy to account for strings like 'Johnsleep' consists of three steps:Step 1: Collect all lexical entries that match with thewords of the input string and generalize them by sub-stituting variables for their morpho-syntaetic specifi-cations (ease, number, gender etc.
).Step2: Parse the string using the generalized lexicalentries instead of tim completely specified entries.Step3: If the parse with generalized specifications isuccessful, the problem with the input string is mor-pho-syntactie (agreement error or ease-assignment vio-lation).
Collect all preterminal categories (most ofthem still contain variable morpho-syntactic specifica-tions) and try to unify them with full-specified lexicalentries.
At least one matching entry will belong tosome item different from the corresponding word inthe input string.
In that case replace the original wordby the matching item.
If there are many different setsof matching entries choose the one that requires theleast number of substitutions and output it as thedefault normalization (if there are many sets of match-ing entries that require the same least number of sub-stitutions the normalization is ambigous.
In that caseoutput all of them).Returning to our example string 'John sleep', letus assmne that the grammar consists just of the rulestated above and the following lexical entries:Jol)n:sleeF.sleeps:person = 3, num= so, cat = rip, case =nornperson = 3, num ,, pl.
cat = vpperson = 3, num= sg, cat = vpGeneralizing the lexical entries for the input string'John sleep' will produce two new entries:John:sleep:person = MAR 1, hum = VAR 2, cat = np,case = VAR 3person = VAR 4, num = VAR 5, cat = vpA parse using these entries will be successfld.
Theapplication of the rule unifies the variable specifica-2 181dons for nmnber and person and instantiates case no-minative in the NP.
The preterminal categories result-ing from the parse are:person = VARI person = VARInum= VAR2 num= VAR2cat =np cat = vpcase : nomThough the crucial specifications (person and num)are still variable the difference is now that there arethe same variables in both categories.
The (only) setof lexical entries that match with these preterminalcategories requires the replacement of 'sleep' by'sleeps' and thus 'John sleeps' is the normalization of'John sleep'.Note that this strategy is not, in principle, limitedto morpho-syntactic features.
It might be useful forphonological and semantic normalization, as well.EXPL ICZT REPAIRWhen people detect an error during an utterance theyoften try to correct it immediately.
This, in general,makes the utterance as a whole ungrammatical.
Thestructure of an utterance containing a self repair isoften:Left  context - reparandum - repair indicator - reparansright context.The reparandum is the part of the utterance that is tobe corrected by the reparans.
Typical repair indic-ators are interjections like 'uh no', 'nonsense', sorry'etc.
The following example from our corpus showsthat structure (note that the left context is empty in theoriginal German version):Den linken oh ~uatsch_ den roton stellst du links hinrel~arandum indicator reparans right contox~You ,put the !ef~ one eh nonsense the red one to the leftleft c. reparandum indicator reparans right contextA plausible normalization of this utterance would be'Den roten stellst du links hin' ('You put the red oneto the left3.
This normalization differs from the ori-ginal utterance in that the reparandum and the repairindicators have been deleted.
The strategy to coverthis type of repair is to scan the input string w~w v..w.until a repair indicator sequence w~w~?r..wj is found(1 < i < j < n).
If there is such an explicit signal,then there probably is something wrong immediatelybefore the repair sequence.
But it is not clear what thereparandum is.
Possibly the reparandum is just theword immediately before the repair indicator sequenceor a longer substring or even the whole substring w~wv..w~_ ~.
Which deletion of a substring WkWk+~...W jgives a grammatical sentence can only be decided bythe grammar.
Thus it is necessary to parse the resultsof the alternative deletions beginning with wl...w~.
2wj+t...w .
and incrementing the length of the deletedsuhstring until the parse succeeds.
If the deletion of asubstring wkw~+,..wj makes a parse successful and ifthere is no other deletion of a substring w~w~+l...wjsuch that k < 1 then wtw2...wk_~wj+~wi42...w n is thenormalization of the input string.If applied to the utterance 'You put the left one ehnonsense the red one to the left' the first deletiongives 'You put the left the red one to the left' whichis not accepted by the parer.
The second alternativetried ('You put the the red one to the left') fails, too.But the third attempt ('You put the red one to theleft') is accepted by the parser and thus considered asthe normalization of the original utterance.UNGRAMMATICAL REPETITIONSUngrammatical repetitions of single words or longerstretches occur quite frequently in spontaneous speech.As long as a sequence is repeated completely andwithout any alteration it is easy to detect the redun-dant duplication and remove it from the input string toget a normalized version.
The problem is with incom-plete repetitions and repetitions that introduce newlexical items:Some b locks  some red b locks  are smal l ,\ / \ /part  1 part  2Some red some blue b locks  are smal l .\ _ _ /  \ _ _  /part  I part  2The deletion of the substrings indicated as 'part 1' inthe utterances above, respectively, would yield asuitable normalization.
Utterances of this kind are inmany respects like the explicit repairs discussedabove, but they lack indicators.
Typically, part 2 issimilar to part 1 in that at least some words occur inboth substrings.
Moreover, part 1 and part 2 oftenbelong to the same category (e.g.
NP in the utterancesabove).
This similarity motivates the following heuris-tic:The input string wlw2...w ~ is scanned for two dif-ferent occurrences, ay w~ and wj (1 _< w I < w i <w,), of the same lexical item.
w~ and wj are per-mitted to differ in their inflectional properties, sincean unsuitable inflection of w~ might have been thereason to repeat it in proper inflexion as wj (e.g.
'He takes took a block').
If such a repetition is182 3fbund the substring beginning with the first occur-rence up to the word immediately before the secondoccurrence (i.e.
w~w~+,...wj.~) is parsed.
If the parseis~ succesful and yields some category C for thesubstring, the next step is to find a prefix ofwjwj+a...w, that belongs to the same category C.If such a prefix exists and wtw2 ...w~_twjwj+~...w, isaccepted as a grammatical sentence it is consideredto be the suitable normalization.Let us apply this strategy to the utterance 'Someblocks some red blocks are small'.
Scanning this inputstring from the left to the right will immediately findthe repeated lexical item 'some'.
The parse of thesubstring 'Some blocks' results in an NP and thus aprefix of 'some red blocks are small' is searched forwhich is also an NP.
Such a prefix is found (i.e.
'some red blocks') and therefore 'some red blocks aresmall' is tested if it is a grammatical sentence and,indeed, it is.RESULTS, CONCLUSIONS, FURTHER TASKSThe normalization strategies outlined in this papermake a /given standard parsing system applicable tocertain language phenomena that occur frequently inspontaneous speech, but deviate from the standards ofwritten language.
Additional rules, special grammarformalisms or fixed parsing algorithms are not requir-ed.If the parse succeeds, the analysis assigned to adeviating input is not only some partial structuredescription, but a well-formed sentence including itscomplete syntactic structure.Preliminary tests have shown that the normaliza-tions achieved by the strategies discussed in this paperare plausible default interpretations in most cases.
Badnormalizations result from the lack of phonological,semantic and world knowledge.
A typical example is'Take a red block oh no blue block' which gets in-correctly normalized into 'Take a red blue block', ifthe grarnmar accepts 'block' being specified by twodifferent color adjectives.
If it does not, trying thenext alternative according to the explicit-repair stra-tegy described above will yield the most plausibleresult 'Take a blue block'.
Another way to avoid thewrong normalization is to consult additional phonolog-ical infomlation about the input string.
It is veryprobable that there is a contrastive stress upon 'blue'in the input utterance.
Let us assume the rule: if thereis a word with contrastive stress in a reparanssequence then there must be a suitable word in thereparandmn sequence to which it is in contrast.
Thisimplies that 'red' must be part of the reparandum (andthus has to be deleted) and rules out the wrong norm-alization 'Take the red blue block'.
A further task willbe to find out how additional semantic and phonolog-ical intormation both in the grammar and in thenormalization strategies can be used to make thenormalization results more reliable.REFERENCESCarbonell, J.G./Hayes, P.J.
: Robust parsing usingmultiple construction-specific strategies.
In: Bolc,Leonard\[ed.\]: Natural language parsing systems.Berlin 1987. pp.
1-32.
(Springer series symboliccomputation - artificial intelligence).Forschergruppe Koh~irenz \[ed.\]: "n Gebitde oder was"- Daten zum Diskurs fiber Modellwelten.
KoLiBri-Arbeitsbericht 2.
Bielefeld 1987.Jensen, K./Heidorn, G.E./Miller, L.A./Ravin, Y.:Parse fitting and prose fixing: getting a hold onill-formedness.
In: AJCL 9 (1983), 147-160.Kindt, W./Laubenstein, U.: Reparaturen und Koordi-nationskonstruktionen.
KoLiBri-Arbeitsbericht 20.
(In preparation).Kudo, I./Koshino, H./Chung, M./Morimoto, T. :Schema method: A framework for correctinggrammatically ill-formed input.
In: COLING 1988,341-347.Kwasny, S.C./Sondheimer, N.K.
: Relaxationtechniques for parsing ill-formed input in naturallanguage understanding systems.
In: AJCL 7(1982), 99-108.Langer, H.: Syntaktische Normalisierung gesprochenerSpraehe.
KoLiBri-Arbeitsbericht 23.
Bielefeld1990.Lesmo, L./Torasso, P.: Interpreting syntactically ill-formed sentences.
In: COLING 1984, 534-539.Levelt, W.J.M.
: Monitoring and self-repair in speech.In: Cognition 14 (1983), 41-104.Schegloff, E.A./Jefferson, C./Sacks, H.: The prefer-ence for self-correction in the organization ofrepair in conversation.
In: Language 53 (1977),361-382.Selfridge, M.: Integrated processing produces robustunderstanding.
In: CL 12 (1983), 161-177.Weischedel, R.M./Sondheimer, N.K.
: Meta-Rules as abasis for processing ill-formed input.
In: AJCL 9(1983), 161-177.4 183
