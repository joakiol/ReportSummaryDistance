Learning Hebrew Roots: Machine Learning with Linguistic ConstraintsEzra DayaDept.
of Computer ScienceUniversity of Haifa31905 HaifaIsraeledaya@cs.haifa.ac.ilDan RothDept.
of Computer ScienceUniversity of IllinoisUrbana, IL 61801USAdanr@cs.uiuc.eduShuly WintnerDept.
of Computer ScienceUniversity of Haifa31905 HaifaIsraelshuly@cs.haifa.ac.ilAbstractThe morphology of Semitic languages is unique inthe sense that the major word-formation mechanismis an inherently non-concatenative process of inter-digitation, whereby two morphemes, a root and apattern, are interwoven.
Identifying the root of agiven word in a Semitic language is an importanttask, in some cases a crucial part of morphologicalanalysis.
It is also a non-trivial task, which manyhumans find challenging.
We present a machinelearning approach to the problem of extracting rootsof Hebrew words.
Given the large number of po-tential roots (thousands), we address the problem asone of combining several classifiers, each predict-ing the value of one of the root?s consonants.
Weshow that when these predictors are combined byenforcing some fairly simple linguistics constraints,high accuracy, which compares favorably with hu-man performance on this task, can be achieved.1 IntroductionThe standard account of word-formation processesin Semitic languages describes words as combina-tions of two morphemes: a root and a pattern.1 Theroot consists of consonants only, by default three(although longer roots are known), called radicals.The pattern is a combination of vowels and, possi-bly, consonants too, with ?slots?
into which the rootconsonants can be inserted.
Words are created byinterdigitating roots into patterns: the first radical isinserted into the first consonantal slot of the pattern,the second radical fills the second slot and the thirdfills the last slot.
See Shimron (2003) for a survey.Identifying the root of a given word is an im-portant task.
Although existing morphological an-alyzers for Hebrew only provide a lexeme (whichis a combination of a root and a pattern), for otherSemitic languages, notably Arabic, the root is anessential part of any morphological analysis sim-1An additional morpheme, vocalization, is used to abstractthe pattern further; for the present purposes, this distinction isirrelevant.ply because traditional dictionaries are organized byroot, rather than by lexeme.
Furthermore, roots areknown to carry some meaning, albeit vague.
We be-lieve that this information can be useful for compu-tational applications and are currently experiment-ing with the benefits of using root and pattern infor-mation for automating the construction of a Word-Net for Hebrew.We present a machine learning approach, aug-mented by limited linguistic knowledge, to the prob-lem of identifying the roots of Hebrew words.
Tothe best of our knowledge, this is the first appli-cation of machine learning to this problem.
Whilethere exist programs which can extract the root ofwords in Arabic (Beesley, 1998a; Beesley, 1998b)and Hebrew (Choueka, 1990), they are all depen-dent on labor intensive construction of large-scalelexicons which are components of full-scale mor-phological analyzers.
Note that Tim Bockwalter?sArabic morphological analyzer2 only uses ?wordstems ?
rather than root and pattern morphemes ?
toidentify lexical items.
(The information on root andpattern morphemes could be added to each stem en-try if this were desired.)?
The challenge of our workis to automate this process, avoiding the bottleneckof having to laboriously list the root and pattern ofeach lexeme in the language, and thereby gain in-sights that can be used for more detailed morpho-logical analysis of Semitic languages.As we show in section 2, identifying roots is anon-trivial problem even for humans, due to thecomplex nature of Hebrew derivational and inflec-tional morphology and the peculiarities of the He-brew orthography.
From a machine learning per-spective, this is an interesting test case of interac-tions among different yet interdependent classifiers.After presenting the data in section 3, we discuss asimple, baseline, learning approach (section 4) andthen propose two methods for combining the resultsof interdependent classifiers (section 5), one whichis purely statistical and one which incorporates lin-2http://www.qamus.org/morphology.htmguistic constraints, demonstrating the improvementof the hybrid approach.
We conclude with sugges-tions for future research.2 Linguistic backgroundIn this section we refer to Hebrew only, althoughmuch of the description is valid for other Semiticlanguages as well.
As an example of root-and-pattern morphology, consider the Hebrew rootsg.d.l, k.t.b and r.$.m and the patterns haCCaCa,hitCaCCut and miCCaC, where the ?C?s indicatethe slots.
When the roots combine with these pat-terns the resulting lexemes are hagdala, hitgadlut,migdal, haktaba, hitkatbut, miktab, har$ama, hi-tra$mut, mir$am, respectively.
After the root com-bines with the pattern, some morpho-phonologicalalternations take place, which may be non-trivial:for example, the hitCaCCut pattern triggers assimi-lation when the first consonant of the root is t or d :thus, d.r.$+hitCaCCut yields hiddar$ut.
The samepattern triggers metathesis when the first radical iss or $ : s.d.r+hitCaCCut yields histadrut rather thanthe expected *hitsadrut.
Semi-vowels such as w ory in the root are frequently combined with the vow-els of the pattern, so that q.w.m+haCCaCa yieldshaqama, etc.
Frequently, root consonants such as wor y are altogether missing from the resulting form.These matters are complicated further due to twosources: first, the standard Hebrew orthographyleaves most of the vowels unspecified.
It does notexplicate a and e vowels, does not distinguish be-tween o and u vowels and leaves many of the ivowels unspecified.
Furthermore, the single letterw is used both for the vowels o and u and for theconsonant v, whereas i is similarly used both forthe vowels i and for the consonant y.
On top ofthat, the script dictates that many particles, includ-ing four of the most frequent prepositions, the def-inite article, the coordinating conjunction and somesubordinating conjunctions all attach to the wordswhich immediately follow them.
Thus, a form suchas mhgr can be read as a lexeme (?immigrant?
), asm-hgr ?from Hagar?or even as m-h-gr ?from theforeigner?.
Note that there is no deterministic wayto tell whether the first m of the form is part of thepattern, the root or a prefixing particle (the preposi-tion m ?from?
).The Hebrew script has 22 letters, all of whichcan be considered consonants.
The number oftri-consonantal roots is thus theoretically boundedby 223, although several phonological constraintslimit this number to a much smaller value.
Forexample, while roots whose second and third radi-cals are identical abound in Semitic languages, rootswhose first and second radicals are identical are ex-tremely rare (see McCarthy (1981) for a theoreti-cal explanation).
To estimate the number of rootsin Hebrew we compiled a list of roots from twosources: a dictionary (Even-Shoshan, 1993) and theverb paradigm tables of Zdaqa (1974).
The union ofthese yields a list of 2152 roots.3While most Hebrew roots are regular, many be-long to weak paradigms, which means that root con-sonants undergo changes in some patterns.
Exam-ples include i or n as the first root consonant, w ori as the second, i as the third and roots whose sec-ond and third consonants are identical.
For example,consider the pattern hCCCh.
Regular roots such asp.s.q yield forms such as hpsqh.
However, the irreg-ular roots n.p.l, i.c.g, q.w.m and g.n.n in this patternyield the seemingly similar forms hplh, hcgh, hqmhand hgnh, respectively.
Note that in the first and sec-ond examples, the first radical (n or i ) is missing, inthe third the second radical (w) is omitted and inthe last example one of the two identical radicals isomitted.
Consequently, a form such as hC1C2h canhave any of the roots n.C1.C2, C1.w.C2, C1.i.C2,C1.C2.C2 and even, in some cases, i.C1.C2.While the Hebrew script is highly ambiguous,ambiguity is somewhat reduced for the task we con-sider here, as many of the possible lexemes of agiven form share the same root.
Still, in order to cor-rectly identify the root of a given word, context mustbe taken into consideration.
For example, the form$mnh has more than a dozen readings, includingthe adjective ?fat?
(feminine singular), which hasthe root $.m.n, and the verb ?count?, whose root ism.n.i, preceded by a subordinating conjunction.
Inthe experiments we describe below we ignore con-text completely, so our results are handicapped bydesign.3 Data and methodologyWe take a machine learning approach to the prob-lem of determining the root of a given word.
Fortraining and testing, a Hebrew linguist manuallytagged a corpus of 15,000 words (a set of newspa-per articles).
Of these, only 9752 were annotated;the reason for the gap is that some Hebrew words,mainly borrowed but also some frequent words suchas prepositions, do not have roots; we further elim-inated 168 roots with more than three consonantsand were left with 5242 annotated word types, ex-hibiting 1043 different roots.
Table 1 shows the dis-tribution of word types according to root ambiguity.3Only tri-consonantal roots are counted.
Ornan (2003) men-tions 3407 roots, whereas the number of roots in Arabic is esti-mated to be 10,000 (Darwish, 2002).Number of roots 1 2 3 4Number of words 4886 335 18 3Table 1: Root ambiguity in the corpusTable 2 provides the distribution of the roots ofthe 5242 word types in our corpus according to roottype, where Ci is the i-th radical (note that someroots may belong to more than one group).Paradigm Number PercentageC1 = i 414 7.90%C1 = w 28 0.53%C1 = n 419 7.99%C2 = i 297 5.66%C2 = w 517 9.86%C3 = h 18 0.19%C3 = i 677 12.92%C2 = C3 445 8.49%Regular 3061 58.41%Table 2: Distribution of root paradigmsAs assurance for statistical reliability, in all theexperiments discussed in the sequel (unless other-wise mentioned) we performed 10-fold cross valida-tion runs a for every classification task during evalu-ation.
We also divided the test corpus into two sets:a development set of 4800 words and a held-out setof 442 words.
Only the development set was usedfor parameter tuning.
A given example is a wordtype with all its (manually tagged) possible roots.In the experiments we describe below, our systemproduces one or more root candidates for each ex-ample.
For each example, we define tp as the num-ber of candidates correctly produced by the system;fp as the number of candidates which are not cor-rect roots; and fn as the number of correct roots thesystem did not produce.
As usual, we define recallastptp+fp and precision astptp+fn ; we then computef -measure for each example (with ?
= 0.5) and(macro-) average to obtain the system?s overall f -measure.To estimate the difficulty of this task, we askedsix human subjects to perform it.
Subjects wereasked to identify all the possible roots of all thewords in a list of 200 words (without context), ran-domly chosen from the test corpus.
All subjectswere computer science graduates, native Hebrewspeakers with no linguistic background.
The aver-age precision of humans on this task is 83.52%, andwith recall at 80.27%, f -measure is 81.86%.
Twomain reasons for the low performance of humansare the lack of context and the ambiguity of some ofthe weak paradigms.4 A machine learning approachTo establish a baseline, we first performed two ex-periments with simple, baseline classifiers.
In all theexperiments described in this paper we use SNoW(Roth, 1998) as the learning environment, with win-now as the update rule (using perceptron yieldedcomparable results).
SNoW is a multi-class clas-sifier that is specifically tailored for learning in do-mains in which the potential number of informationsources (features) taking part in decisions is verylarge, of which NLP is a principal example.
It worksby learning a sparse network of linear functionsover a pre-defined or incrementally learned featurespace.
SNoW has already been used successfullyas the learning vehicle in a large collection of nat-ural language related tasks, including POS tagging,shallow parsing, information extraction tasks, etc.,and compared favorably with other classifiers (Roth,1998; Punyakanok and Roth, 2001; Florian, 2002).Typically, SNoW is used as a classifier, and predictsusing a winner-take-all mechanism over the activa-tion values of the target classes.
However, in addi-tion to the prediction, it provides a reliable confi-dence level in the prediction, which enables its usein an inference algorithm that combines predictorsto produce a coherent inference.4.1 Feature typesAll the experiments we describe in this work sharethe same features and differ only in the target clas-sifiers.
The features that are used to characterize aword are both grammatical and statistical:?
Location of letters (e.g., the third letter of theword is b ).
We limit word length to 20, thusobtaining 440 features of this type (recall thethe size of the alphabet is 22).?
Bigrams of letters, independently of their loca-tion (e.g., the substring gd occurs in the word).This yields 484 features.?
Prefixes (e.g., the word is prefixed by k$h?when the?).
We have 292 features of this type,corresponding to 17 prefixes and sequencesthereof.?
Suffixes (e.g., the word ends with im, a pluralsuffix).
There are 26 such features.4.2 Direct predictionIn the first of the two experiments, referred to asExperiment A, we trained a classifier to learn rootsas a single unit.
The two obvious drawbacks ofthis approach are the large set of targets and thesparseness of the training data.
Of course, defin-ing a multi-class classification task with 2152 tar-gets, when only half of them are manifested in thetraining corpus, does not leave much hope for everlearning to identify the missing targets.In Experiment A, the macro-average precision often-fold cross validation runs of this classificationproblem is 45.72%; recall is 44.37%, yielding anf -score of 45.03%.
In order to demonstrate the in-adequacy of this method, we repeated the same ex-periment with a different organization of the train-ing data.
We chose 30 roots and collected all theiroccurrences in the corpus into a test file.
We thentrained the classifier on the remainder of the corpusand tested on the test file.
As expected, the accuracywas close to 0%,4.3 Decoupling the problemIn the second experiment, referred to as Experi-ment B, we separated the problem into three dif-ferent tasks.
We trained three classifiers to learneach of the root consonants in isolation and thencombined the results in the straight-forward way(a conjunction of the decisions of the three classi-fiers).
This is still a multi-class classification butthe number of targets in every classification task isonly 22 (the number of letters in the Hebrew al-phabet) and data sparseness is no longer a problem.As we show below, each classifier achieves muchbetter generalization, but the clear limitation of thismethod is that it completely ignores interdependen-cies between different targets: the decision on thefirst radical is completely independent of the deci-sion on the second and the third.We observed a difference between recognizingthe first and third radicals and recognizing the sec-ond one, as can be seen in table 3.
These results cor-respond well to our linguistic intuitions: the mostdifficult cases for humans are those in which thesecond radical is w or i, and those where the secondand the third consonants are identical.
Combiningthe three classifiers using logical conjunction yieldsan f -measure of 52.84%.
Here, repeating the sameexperiment with the organization of the corpus suchthat testing is done on unseen roots yielded 18.1%accuracy.To demonstrate the difficulty of the problem, weconducted yet another experiment.
Here, we trainedthe system as above but we tested it on differentwords whose roots were known to be in the trainingset.
The results of experiment A here were 46.35%,whereas experiment B was accurate in 57.66% ofC1 C2 C3 rootPrecision: 82.25 72.29 81.85 53.60Recall: 80.13 70.00 80.51 52.09f -measure: 81.17 71.13 81.18 52.84Table 3: Accuracy of SNoW?s identifying the cor-rect radicalthe cases.
Evidently, even when testing only onpreviously seen roots, both na?
?ve methods are un-successful (although method A here outperformsmethod B).5 Combining interdependent classifiersEvidently, simple combination of the results of thethree classifiers leaves much room for improve-ment.
Therefore we explore other ways for com-bining these results.
We can rely on the fact thatSNoW provides insight into the decisions of theclassifiers ?
it lists not only the selected target, butrather all candidates, with an associated confidencemeasure.
Apparently, the correct radical is chosenamong SNoW?s top-n candidates with high accu-racy, as the data in table 3 reveal.This observation calls for a different way of com-bining the results of the classifiers which takes intoaccount not only the first candidate but also others,along with their confidence scores.5.1 HMM combinationWe considered several ways, e.g., via HMMs, of ap-pealing to the sequential nature of the task (C1 fol-lowed by C2, followed by C3).
Not surprisingly, di-rect applications of HMMs are too weak to providesatisfactory results, as suggested by the followingdiscussion.
The approach we eventually opted forcombines the predictive power of a classifier to es-timate more accurate state probabilities.Given the sequential nature of the data and thefact that our classifier returns a distribution overthe possible outcomes for each radical, a naturalapproach is to combine SNoW?s outcomes via aMarkovian approach.
Variations of this approachare used in the context of several NLP problems,including POS tagging (Schu?tze and Singer, 1994),shallow parsing (Punyakanok and Roth, 2001) andnamed entity recognition (Tjong Kim Sang andDe Meulder, 2003).Formally, we assume that the confidence suppliedby the classifier is the probability of a state (radical,c) given the observation o (the word), P (c|o).
Thisinformation can be used in the HMM framework byapplying Bayes rule to computeP (o|c) = P (c|o)P (o)P (c) ,where P (o) and P (c) are the probabilities of ob-serving o and being at c, respectively.
That is,instead of estimating the observation probabilityP (o|c) directly from training data, we computeit from the classifiers?
output.
Omitting details(see Punyakanok and Roth (2001)), we can nowcombine the predictions of the classifiers by findingthe most likely root for a given observation, asr = argmaxP (c1c2c3|o, ?
)where ?
is a Markov model that, in this case, canbe easily learned from the supervised data.
Clearly,given the short root and the relatively small numberof values of ci that are supported by the outcomesof SNoW, there is no need to use dynamic program-ming here and a direct computation is possible.However, perhaps not surprisingly given the dif-ficulty of the problem, this model turns out to be toosimplistic.
In fact, performance deteriorated.
Weconjecture that the static probabilities (the model)are too biased and cause the system to abandon goodchoices obtained from SNoW in favor of worse can-didates whose global behavior is better.For example, the root &.b.d was correctly gen-erated by SNoW as the best candidate for the word&obdim, but since P (C3 = b|C2 = b), which is0.1, is higher than P (C3 = d|C2 = b), which is0.04, the root &.b.b was produced instead.
Note thatin the above example the root &.b.b cannot possiblybe the correct root of &obdim since no pattern inHebrew contains the letter d, which must thereforebe part of the root.
It is this kind of observations thatmotivate the addition of linguistic knowledge as avehicle for combining the results of the classifiers.An alternative approach, which we intend to investi-gate in the future, is the introduction of higher-levelclassifiers which take into account interactions be-tween the radicals (Punyakanok and Roth, 2001).5.2 Adding linguistic constraintsThe experiments discussed in section 4 are com-pletely devoid of linguistic knowledge.
In partic-ular, experiment B inherently assumes that any se-quence of three consonants can be the root of agiven word.
This is obviously not the case: withvery few exceptions, all radicals must be present inany inflected form (in fact, only w, i, n and in an ex-ceptional case l can be deleted when roots combinewith patterns).
We therefore trained the classifiersto consider as targets only letters that occurred inthe observed word, plus w, i, n and l, rather thanany of the alphabet letters.
The average number oftargets is now 7.2 for the first radical, 5.7 for thesecond and 5.2 for the third (compared to 22 each inthe previous setup).In this model, known as the sequential model(Even-Zohar and Roth, 2001), SNoW?s perfor-mance improved slightly, as can be seen in table 4(compare to table 3).
Combining the results inthe straight-forward way yields an f -measure of58.89%, a small improvement over the 52.84% per-formance of the basic method.
This new resultshould be considered baseline.
In what follows wealways employ the sequential model for training andtesting the classifiers, using the same constraints.However, we employ more linguistic knowledge fora more sophisticated combination of the classifiers.C1 C2 C3 rootPrecision: 83.06 72.52 83.88 59.83Recall: 80.88 70.20 82.50 57.98f -measure: 81.96 71.34 83.18 58.89Table 4: Accuracy of SNoW?s identifying the cor-rect radical, sequential model5.3 Combining classifiers using linguisticknowledgeSNoW provides a ranking on all possible roots.
Wenow describe the use of linguistic constraints to re-rank this list.
We implemented a function whichuses knowledge pertaining to word-formation pro-cesses in Hebrew in order to estimate the likeli-hood of a given candidate being the root of a givenword.
The function practically classifies the can-didate roots into one of three classes: good candi-dates, which are likely to be the root of the word;bad candidates, which are highly unlikely; and av-erage cases.The decision of the function is based on the ob-servation that when a root is regular it either occursin a word consecutively or with a single w or i be-tween any two of its radicals.
The scoring func-tion checks, given a root and a word, whether thisis the case.
Furthermore, the suffix of the word, af-ter matching the root, must be a valid Hebrew suffix(there is only a small number of such suffixes in He-brew).
If both conditions hold, the scoring functionreturns a high value.
Then, the function checks ifthe root is an unlikely candidate for the given word.For example, if the root is regular its consonantsmust occur in the word in the same order they occurin the root.
If this is not the case, the function re-turns a low value.
We also make use in this functionof our pre-compiled list of roots.
A root candidatewhich does not occur in the list is assigned the lowscore.
In all other cases, a middle value is returned.The actual values that the function returns werechosen empirically by counting the number of oc-currences of each class in the training data.
For ex-ample, ?good?
candidates make up 74.26% of thedata, hence the value the function returns for ?good?roots is set to 0.7426.
Similarly, the middle value isset to 0.2416 and the low ?
to 0.0155.As an example, consider hipltm, whose root isn.p.l (note that the first n is missing in this form).Here, the correct candidate will be assigned the mid-dle score while p.l.t and l.t.m will score high.In addition to the scoring function we imple-mented a simple edit distance function which re-turns, for a given root and a given word, the inverseof the edit distance between the two.
For exam-ple, for hipltm, the (correct) root n.p.l scores 1/4whereas p.l.t scores 1/3.We then run SNoW on the test data and rank theresults of the three classifiers globally, where theorder is determined by the product of the three dif-ferent classifiers.
This induces an order on roots,which are combinations of the decisions of threeindependent classifiers.
Each candidate root is as-signed three scores: the product of the confidencemeasures of the three classifiers; the result of thescoring function; and the inverse edit distance be-tween the candidate and the observed word.
Werank the candidates according to the product ofthe three scores (i.e., we give each score an equalweight in the final ranking).In order to determine which of the candidates toproduce for each example, we experimented withtwo methods.
First, the system produced the top-icandidates for a fixed value of i.
The results on thedevelopment set are given in table 5.i = 1 2 3 4Precision 82.02 46.17 32.81 25.19Recall 79.10 87.83 92.93 94.91f -measure 80.53 60.52 48.50 39.81Table 5: Performance of the system when producingtop-i candidates.Obviously, since most words have only one root,precision drops dramatically when the system pro-duces more than one candidate.
This calls for a bet-ter threshold, facilitating a non-fixed number of out-puts for each example.
We observed that in the ?dif-ficult?
examples, the top ranking candidates are as-signed close scores, whereas in the easier cases, thetop candidate is usually scored much higher than thenext one.
We therefore decided to produce all thosecandidates whose scores are not much lower thanthe score of the top ranking candidate.
The dropin the score, ?, was determined empirically on thedevelopment set.
The results are listed in table 6,where ?
varies from 0.1 to 1 (?
is actually computedon the log of the actual score, to avoid underflow).These results show that choosing ?
= 0.4 pro-duces the highest f -measure.
With this value for?, results for the held-out data are presented in ta-ble 7.
The results clearly demonstrate the addedbenefit of the linguistic knowledge.
In fact, our re-sults are slightly better than average human perfor-mance, which we recall as well.
Interestingly, evenwhen testing the system on a set of roots which donot occur in the training corpus (see section 4), weobtain an f -score of 65.60%.
This result demon-strates the robustness of our method.Held-out data HumansPrecision: 80.90 83.52Recall: 88.16 80.27f -measure: 84.38 81.86Table 7: Results: performance of the system onheld-out data.It must be noted that the scoring function aloneis not a function for extracting roots from Hebrewwords.
First, it only scores a given root candidateagainst a given word, rather than yield a root given aword.
While we could have used it exhaustively onall possible roots in this case, in a general setting ofa number of classifiers the number of classes mightbe too high for this solution to be practical.
Sec-ond, the function only produces three different val-ues; when given a number of candidate roots it mayreturn more than one root with the highest score.
Inthe extreme case, when called with all 223 potentialroots, it returns on the average more than 11 can-didates which score highest (and hence are rankedequally).Similarly, the additional linguistic knowledge isnot merely eliminating illegitimate roots from theranking produced by SNoW.
Using the linguisticconstraints encoded in the scoring function onlyto eliminate roots, while maintaining the rankingproposed by SNoW, yields much lower accuracy.Clearly, our linguistically motivated scoring doesmore than elimination, and actually re-ranks the?
= 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0Precision 81.81 80.97 79.93 78.86 77.31 75.48 73.71 71.80 69.98 67.90Recall 81.06 82.74 84.03 85.52 86.49 87.61 88.72 89.70 90.59 91.45f -measure 81.43 81.85 81.93 82.06 81.64 81.10 80.52 79.76 78.96 77.93Table 6: Performance of the system, producing candidates scoring no more than ?
below the top score.roots.
It is only the combination of the classifierswith the linguistically motivated scoring functionwhich boosts the performance on this task.5.4 Error analysisLooking at the questionnaires filled in by our sub-jects (section 3), it is obvious that humans haveproblems identifying the correct roots in two gen-eral cases: when the root paradigm is weak (i.e.,when the root is irregular) and when the word can beread in more than way and the subject chooses onlyone (presumably, the most prominent one).
Our sys-tem suffers from similar problems: first, its perfor-mance on the regular paradigms is far superior to itsoverall performance; second, it sometimes cannotdistinguish between several roots which are in prin-ciple possible, but only one of which happens to bethe correct one.To demonstrate the first point, we evaluated theperformance of the system on a different organiza-tion of the data.
We tested separately words whoseroots are all regular, vs. words all of whose roots areirregular.
We also tested words which have at leastone regular root (mixed).
The results are presentedin table 8, and clearly demonstrate the difficulty ofthe system on the weak paradigms, compared to al-most 95% on the easier, regular roots.Regular Irregular MixedNumber of words 2598 2019 2781Precision: 92.79 60.02 92.54Recall: 96.92 73.45 94.28f -measure: 94.81 66.06 93.40Table 8: Error analysis: performance of the systemon different cases.A more refined analysis reveals differences be-tween the various weak paradigms.
Table 9 lists f -measure for words whose roots are irregular, classi-fied by paradigm.
As can be seen, the system hasgreat difficulty in the cases of C2 = C3 and C3 = i.Finally, we took a closer look at some of the er-rors, and in particular at cases where the system pro-duces several roots where fewer (usually only one)are correct.
Such cases include, for example, theParadigm f -measureC1 = i 70.57C1 = n 71.97C2 = i/w 76.33C3 = i 58.00C2 = C3 47.42Table 9: Error analysis: the weak paradigmsword hkwtrt (?the title?
), whose root is the regu-lar k.t.r; but the system produces, in addition, alsow.t.r, mistaking the k to be a prefix.
This is the kindof errors which are most difficult to cope with.However, in many cases the system?s errors arerelatively easy to overcome.
Consider, for example,the word hmtndbim (?the volunteers?)
whose root isthe irregular n.d.b.
Our system produces as many asfive possible roots for this word: n.d.b, i.t.d, d.w.b,i.h.d, i.d.d.
Clearly some of these could be elimi-nated.
For example, i.t.d should not be produced,because if this were the root, nothing could explainthe presence of the b in the word; i.h.d should beexcluded because of the location of the h. Similarphenomena abound in the errors the system makes;they indicate that a more careful design of the scor-ing function can yield still better results, and this isthe direction we intend to pursue in the future.6 ConclusionsWe have shown that combining machine learningwith limited linguistic knowledge can produce state-of-the-art results on a difficult morphological task,the identification of roots of Hebrew words.
Ourbest result, over 80% precision, was obtained usingsimple classifiers for each of the root?s consonants,and then combining the outputs of the classifiers us-ing a linguistically motivated, yet extremely coarseand simplistic, scoring function.
This result is com-parable to average human performance on this task.This work can be improved in a variety of ways.We intend to spend more effort on feature engineer-ing.
As is well-known from other learning tasks,fine-tuning of the feature set can produce additionalaccuracy; we expect this to be the case in this task,too.
In particular, introducing features that capturecontextual information is likely to improve the re-sults.
Similarly, our scoring function is simplisticand we believe that it can be improved.
We also in-tend to improve the edit-distance function such thatthe cost of replacing characters reflect phonologicaland orthographic constraints (Kruskal, 1999).In another track, there are various other ways inwhich different inter-related classifiers can be com-bined.
Here we only used a simple multiplica-tion of the three classifiers?
confidence measures,which is then combined with the linguistically mo-tivated functions.
We intend to investigate more so-phisticated methods for this combination, includinghigher-order machine learning techniques.Finally, we plan to extend these results to morecomplex cases of learning tasks with a large num-ber of targets, in particular such tasks in which thetargets are structured.
We are currently working onsimilar experiments for Arabic root extraction.
An-other example is the case of morphological disam-biguation in languages with non-trivial morphology,which can be viewed as a POS tagging problem witha large number of tags on which structure can be im-posed using the various morphological and morpho-syntactic features that morphological analyzers pro-duce.
We intend to investigate this problem for He-brew in the future.AcknowledgmentsThis work was supported by The Caesarea EdmondBenjamin de Rothschild Foundation Institute for In-terdisciplinary Applications of Computer Science.Dan Roth is supported by NSF grants CAREER IIS-9984168, ITR IIS-0085836, and ITR-IIS 00-85980.We thank Meira Hess and Liron Ashkenazi for an-notating the corpus and Alon Lavie and Ido Daganfor useful comments.ReferencesKen Beesley.
1998a.
Arabic morphological analy-sis on the internet.
In Proceedings of the 6th In-ternational Conference and Exhibition on Multi-lingual Computing, Cambridge, April.Kenneth R. Beesley.
1998b.
Arabic morphologyusing only finite-state operations.
In MichaelRosner, editor, Proceedings of the Workshopon Computational Approaches to Semitic lan-guages, pages 50?57, Montreal, Quebec, August.COLING-ACL?98.Yaacov Choueka.
1990.
MLIM - a system for full,exact, on-line grammatical analysis of ModernHebrew.
In Yehuda Eizenberg, editor, Proceed-ings of the Annual Conference on Computers inEducation, page 63, Tel Aviv, April.
In Hebrew.Kareem Darwish.
2002.
Building a shallow Arabicmorphological analyzer in one day.
In Mike Ros-ner and Shuly Wintner, editors, ComputationalApproaches to Semitic Languages, an ACL?02Workshop, pages 47?54, Philadelphia, PA, July.Abraham Even-Shoshan.
1993.
HaMillon HaX-adash (The New Dictionary).
Kiryat Sefer,Jerusalem.
In Hebrew.Y.
Even-Zohar and Dan Roth.
2001.
A sequentialmodel for multi class classification.
In EMNLP-2001, the SIGDAT Conference on EmpiricalMethods in Natural Language Processing, pages10?19.Radu Florian.
2002.
Named entity recognition as ahouse of cards: Classifier stacking.
In Proceed-ings of CoNLL-2002, pages 175?178.
Taiwan.Joseph Kruskal.
1999.
An overview of se-quence comparison.
In David Sankoff and JosephKruskal, editors, Time Warps, String Edits andMacromolecules: The Theory and Practice of Se-quence Comparison, pages 1?44.
CSLI Publica-tions, Stanford, CA.
Reprint, with a foreword byJohn Nerbonne.John J. McCarthy.
1981.
A prosodic theory of non-concatenative morphology.
Linguistic Inquiry,12(3):373?418.Uzzi Ornan.
2003.
The Final Word.
University ofHaifa Press, Haifa, Israel.
In Hebrew.Vasin Punyakanok and Dan Roth.
2001.
The useof classifiers in sequential inference.
In NIPS-13; The 2000 Conference on Advances in NeuralInformation Processing Systems 13, pages 995?1001.
MIT Press.Dan Roth.
1998.
Learning to resolve natural lan-guage ambiguities: A unified approach.
In Pro-ceedings of AAAI-98 and IAAI-98, pages 806?813, Madison, Wisconsin.H.
Schu?tze and Y.
Singer.
1994.
Part-of-speech tag-ging using a variable memory markov model.
InProceedings of the 32nd Annual Meeting of theAssociation for Computational Linguistics.Joseph Shimron, editor.
2003.
Language Process-ing and Acquisition in Languages of Semitic,Root-Based, Morphology.
Number 28 in Lan-guage Acquisition and Language Disorders.
JohnBenjamins.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CoNLL-2003 sharedtask: Language-independent named entity recog-nition.
In Walter Daelemans and Miles Osborne,editors, Proceedings of CoNLL-2003, pages 142?147.
Edmonton, Canada.Yizxaq Zdaqa.
1974.
Luxot HaPoal (The Verb Ta-bles).
Kiryath Sepher, Jerusalem.
In Hebrew.
