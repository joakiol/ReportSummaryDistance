Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 593?601,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPVariational Decoding for Statistical Machine TranslationZhifei Li and Jason Eisner and Sanjeev KhudanpurDepartment of Computer Science and Center for Language and Speech ProcessingJohns Hopkins University, Baltimore, MD 21218, USAzhifei.work@gmail.com, jason@cs.jhu.edu, khudanpur@jhu.eduAbstractStatistical models in machine translationexhibit spurious ambiguity.
That is, theprobability of an output string is splitamong many distinct derivations (e.g.,trees or segmentations).
In principle, thegoodness of a string is measured by thetotal probability of its many derivations.However, finding the best string (e.g., dur-ing decoding) is then computationally in-tractable.
Therefore, most systems usea simple Viterbi approximation that mea-sures the goodness of a string using onlyits most probable derivation.
Instead,we develop a variational approximation,which considers all the derivations but stillallows tractable decoding.
Our particularvariational distributions are parameterizedas n-gram models.
We also analyticallyshow that interpolating these n-gram mod-els for different n is similar to minimum-risk decoding for BLEU (Tromble et al,2008).
Experiments show that our ap-proach improves the state of the art.1 IntroductionAmbiguity is a central issue in natural languageprocessing.
Many systems try to resolve ambigu-ities in the input, for example by tagging wordswith their senses or choosing a particular syntaxtree for a sentence.
These systems are designed torecover the values of interesting latent variables,such as word senses, syntax trees, or translations,given the observed input.However, some systems resolve too many ambi-guities.
They recover additional latent variables?so-called nuisance variables?that are not of in-terest to the user.1 For example, though machinetranslation (MT) seeks to output a string, typicalMT systems (Koehn et al, 2003; Chiang, 2007)1These nuisance variables may be annotated in trainingdata, but it is more common for them to be latent even there,i.e., there is no supervision as to their ?correct?
values.will also recover a particular derivation of that out-put string, which specifies a tree or segmentationand its alignment to the input string.
The compet-ing derivations of a string are interchangeable fora user who is only interested in the string itself, soa system that unnecessarily tries to choose amongthem is said to be resolving spurious ambiguity.Of course, the nuisance variables are importantcomponents of the system?s model.
For example,the translation process from one language to an-other language may follow some hidden tree trans-formation process, in a recursive fashion.
Manyfeatures of the model will crucially make referenceto such hidden structures or alignments.However, collapsing the resulting spuriousambiguity?i.e., marginalizing out the nuisancevariables?causes significant computational dif-ficulties.
The goodness of a possible MT out-put string should be measured by summing upthe probabilities of all its derivations.
Unfortu-nately, finding the best string is then computation-ally intractable (Sima?an, 1996; Casacuberta andHiguera, 2000).2 Therefore, most systems merelyidentify the single most probable derivation andreport the corresponding string.
This correspondsto a Viterbi approximation that measures the good-ness of an output string using only its most proba-ble derivation, ignoring all the others.In this paper, we propose a variational methodthat considers all the derivations but still allowstractable decoding.
Given an input string, the orig-inal system produces a probability distribution pover possible output strings and their derivations(nuisance variables).
Our method constructs a sec-ond distribution q ?
Q that approximates p as wellas possible, and then finds the best string accord-ing to q.
The last step is tractable because eachq ?
Q is defined (unlike p) without reference tonuisance variables.
Notice that q here does not ap-proximate the entire translation process, but only2May and Knight (2006) have successfully used tree-automaton determinization to exactly marginalize out someof the nuisance variables, obtaining a distribution over parsedtranslations.
However, they do not marginalize over theseparse trees to obtain a distribution over translation strings.593the distribution over output strings for a particularinput.
This is why it can be a fairly good approxi-mation even without using the nuisance variables.In practice, we approximate with several dif-ferent variational families Q, corresponding to n-gram (Markov) models of different orders.
Wegeometrically interpolate the resulting approxima-tions q with one another (and with the original dis-tribution p), justifying this interpolation as similarto the minimum-risk decoding for BLEU proposedby Tromble et al (2008).
Experiments show thatour approach improves the state of the art.The methods presented in this paper should beapplicable to collapsing spurious ambiguity forother tasks as well.
Such tasks include data-oriented parsing (DOP), applications of HiddenMarkov Models (HMMs) and mixture models, andother models with latent variables.
Indeed, ourmethods were inspired by past work on varia-tional decoding for DOP (Goodman, 1996) and forlatent-variable parsing (Matsuzaki et al, 2005).2 Background2.1 TerminologyIn MT, spurious ambiguity occurs both in regularphrase-based systems (e.g., Koehn et al (2003)),where different segmentations lead to the sametranslation string (Figure 1), and in syntax-basedsystems (e.g., Chiang (2007)), where differentderivation trees yield the same string (Figure 2).In the Hiero system (Chiang, 2007) we are us-ing, each string corresponds to about 115 distinctderivations on average.We use x to denote the input string, and D(x) toconsider the set of derivations then considered bythe system.
Each derivation d ?
D(x) yields sometranslation string y = Y(d) in the target language.We write D(x, y)def= {d ?
D(x) : Y(d) = y} todenote the set of all derivations that yield y. Thus,the set of translations permitted by the model isT(y)def= {y : D(x, y) 6= ?}
(or equivalently,T(y)def= {Y(d) : d ?
D(x)}).
We write y?
forthe translation string that is actually output.2.2 Maximum A Posterior (MAP) DecodingFor a given input sentence x, a decoding methodidentifies a particular ?best?
output string y?.
Themaximum a posteriori (MAP) decision rule isy?
= argmaxy?T(x)p(y | x) (1)machine translation software?
?
?
?
?
?machine translation software?
?
?
?
?
?Figure 1: Segmentation ambiguity in phrase-based MT: twodifferent segmentations lead to the same translation string.S->(?
?, machine) S->( ?
, translation) S->( ?
, software)S->(?
?, machine)?S->( ?
, software)S->(S0 S1, S0 S1)S->(S0 S1, S0 S1)S->(S0 ?
S1, S0 translation S1)Figure 2: Tree ambiguity in syntax-based MT: two derivationtrees yield the same translation string.
(An alternative decision rule, minimum Bayesrisk (MBR), will be discussed in Section 4.
)To obtain p(y | x) above, we need to marginal-ize over a nuisance variable, the derivation of y.Therefore, the MAP decision rule becomesy?
= argmaxy?T(x)?d?D(x,y)p(y, d | x) (2)where p(y, d | x) is typically derived from a log-linear model as follows,p(y, d | x) =e??s(x,y,d)Z(x)=e?
?s(x,y,d)?y,d e?
?s(x,y,d)(3)where ?
is a scaling factor to adjust the sharp-ness of the distribution, the score s(x, y, d) is alearned linear combination of features of the triple(x, y, d), and Z(x) is a normalization constant.Note that p(y, d | x) = 0 if y 6= Y(d).
Our deriva-tion set D(x) is encoded in polynomial space, us-ing a hypergraph or lattice.3 However, both |D(x)|and |T(x)| may be exponential in |x|.
Since themarginalization needs to be carried out for eachmember of T(x), the decoding problem of (2)turns out to be NP-hard,4 as shown by Sima?an(1996) for a similar problem.3A hypergraph is analogous to a parse forest (Huang andChiang, 2007).
(A finite-state lattice is a special case.)
It canbe used to encode exponentially many hypotheses generatedby a phrase-based MT system (e.g., Koehn et al (2003)) or asyntax-based MT system (e.g., Chiang (2007)).4Note that the marginalization for a particular y would betractable; it is used at training time in certain training objec-tive functions, e.g., maximizing the conditional likelihood ofa reference translation (Blunsom et al, 2008).5942.3 Viterbi ApproximationTo approximate the intractable decoding problemof (2), most MT systems (Koehn et al, 2003; Chi-ang, 2007) use a simple Viterbi approximation,y?
= argmaxy?T(x)pViterbi(y | x) (4)= argmaxy?T(x)maxd?D(x,y)p(y, d | x) (5)= Y(argmaxd?D(x)p(y, d | x))(6)Clearly, (5) replaces the sum in (2) with a max.In other words, it approximates the probability ofa translation string by the probability of its most-probable derivation.
(5) is found quickly via (6).The Viterbi approximation is simple and tractable,but it ignores most derivations.2.4 N-best Approximation (or Crunching)Another popular approximation enumerates the Nbest derivations in D(x), a set that we call ND(x).Modifying (2) to sum over only these derivationsis called crunching by May and Knight (2006):y?
= argmaxy?T(x)pcrunch(y | x) (7)= argmaxy?T(x)?d?D(x,y)?ND(x)p(y, d | x)3 Variational Approximate DecodingThe Viterbi and crunching methods above approx-imate the intractable decoding of (2) by ignor-ing most of the derivations.
In this section, wewill present a novel variational approximation,which considers all the derivations but still allowstractable decoding.3.1 Approximate InferenceThere are several popular approaches to approxi-mate inference when exact inference is intractable(Bishop, 2006).
Stochastic techniques such asMarkov Chain Monte Carlo are exact in the limitof infinite runtime, but tend to be too slow for largeproblems.
By contrast, deterministic variationalmethods (Jordan et al, 1999), including message-passing (Minka, 2005), are inexact but scale upwell.
They approximate the original intractabledistribution with one that factorizes better or hasa specific parametric form (e.g., Gaussian).In our work, we use a fast variational method.Variational methods generally work as follows.When exact inference under a complex model pis intractable, one can approximate the posteriorp(y | x) by a tractable model q(y), where q ?
Q ischosen to minimize some information loss such asthe KL divergence KL(p ?
q).
The simpler modelq can then act as a surrogate for p during inference.3.2 Variational Decoding for MTFor each input sentence x, we assume that a base-line MT system generates a hypergraph HG(x)that compactly encodes the derivation set D(x)along with a score for each d ?
D(x),5 which weinterpret as p(y, d | x) (or proportional to it).
Forany single y ?
T(x), it would be tractable usingHG(x) to compute p(y | x) =?d p(y, d | x).However, as mentioned, it is intractable to findargmaxy p(y | x) as required by the MAP de-coding (2), so we seek an approximate distributionq(y) ?
p(y | x).6For a fixed x, we seek a distribution q ?
Q thatminimizes the KL divergence from p to q (bothregarded as distributions over y):7q?
= argminq?QKL(p ?
q) (8)= argminq?Q?y?T(x)(p log p?
p log q) (9)= argmaxq?Q?y?T(x)p log q (10)So far, in order to approximate the intractableoptimization problem (2), we have defined an-other optimization problem (10).
If computingp(y | x) during decoding is computationally in-tractable, one might wonder if the optimizationproblem (10) is any simpler.
We will show this isthe case.
The trick is to parameterize q as a fac-torized distribution such that the estimation of q?and decoding using q?
are both tractable throughefficient dynamic programs.
In the next three sub-sections, we will discuss the parameterization, es-timation, and decoding, respectively.3.2.1 Parameterization of qIn (10), Q is a family of distributions.
If we se-lect a large family Q, we can allow more com-plex distributions, so that q?
will better approxi-mate p. If we select a smaller family Q, we can5The baseline system may return a pruned hypergraph,which has the effect of pruning D(x) and T(x) as well.6Following the convention in describing variational infer-ence, we write q(y) instead of q(y | x), even though q(y)always depends on x implicitly.7To avoid clutter, we denote p(y | x) by p, and q(y) by q.We drop p log p from (9) because it is constant with respectto q.
We then flip the sign and change argmin to argmax.595guarantee that q?
will have a simple form withmany conditional independencies, so that q?
(y)and y?
= argmaxy q?
(y) are easier to compute.Since each q(y) is a distribution over outputstrings, a natural choice for Q is the family ofn-gram models.
To obtain a small KL diver-gence (8), we should make n as large as possible.In fact, q?
?
p as n ?
?.
Of course, this lastpoint also means that our computation becomesintractable as n?
?.8 However, if p(y | x) is de-fined by a hypergraph HG(x) whose structure ex-plicitly incorporates an m-gram language model,both training and decoding will be efficient whenm ?
n. We will give algorithms for this case thatare linear in the size of HG(x).9Formally, each q ?
Q takes the formq(y) =?w?Wq(r(w) | h(w))cw(y) (11)where W is a set of n-gram types.
Each w ?W isan n-gram, which occurs cw(y) times in the stringy, and w may be divided into an (n ?
1)-gramprefix h(w) (the history) and a 1-gram suffix r(w)(the rightmost or current word).8Blunsom et al (2008) effectively do take n = ?, bymaintaining the whole translation string in the dynamic pro-gramming state.
They alleviate the computation cost some-how by using aggressive beam pruning, which might be sen-sible for their relatively small task (e.g., input sentences of< 10 words).
But, we are interested in improving the perfor-mance for a large-scale system, and thus their method is nota viable solution.
Moreover, we observe in our experimentsthat using a larger n does not improve much over n = 2.9A reviewer asks about the interaction with backed-offlanguage models.
The issue is that the most compact finite-state representations of these (Allauzen et al, 2003), whichexploit backoff structure, are not purely m-gram for anym.
They yield more compact hypergraphs (Li and Khudan-pur, 2008), but unfortunately those hypergraphs might not betreatable by Fig.
4?since where they back off to less than ann-gram, e is not informative enough for line 8 to find w.We sketch a method that works for any language modelgiven by a weighted FSA, L. The variational family Q canbe specified by any deterministic weighted FSA, Q, withweights parameterized by ?.
One seeks ?
to minimize (8).Intersect HG(x) with an ?unweighted?
version of Q inwhich all arcs have weight 1, so that Q does not preferany string to another.
By lifting weights into an expectationsemiring (Eisner, 2002), it is then possible to obtain expectedtransition counts in Q (where the expectation is taken underp), or other sufficient statistics needed to estimate ?.This takes only time O(|HG(x)|) when L is a left-to-rightrefinement of Q (meaning that any two prefix strings thatreach the same state in L also reach the same state in Q),for then intersecting L or HG(x) with Q does not split anystates.
That is the case when L and Q are respectively purem-gram and n-gram models withm ?
n, as assumed in (12)and Figure 4.
It is also the case when Q is a pure n-grammodel and L is constructed not to back off beyond n-grams;or when the variational family Q is defined by deliberatelytaking the FSA Q to have the same topology as L.The parameters that specify a particular q ?
Qare the (normalized) conditional probability distri-butions q(r(w) | h(w)).
We will now see how toestimate these parameters to approximate p(?
| x)for a given x at test time.3.2.2 Estimation of q?Note that the objective function (8)?
(10) asks us toapproximate p as closely as possible, without anyfurther smoothing.
(It is assumed that p is alreadysmoothed appropriately, having been constructedfrom channel and language models that were esti-mated with smoothing from finite training data.
)In fact, if p were the empirical distribution overstrings in a training corpus, then q?
of (10) is justthe maximum-likelihood n-gram model?whoseparameters, trivially, are just unsmoothed ratios ofthe n-gram and (n?1)-gram counts in the trainingcorpus.
That is, q?
(r(w) | h(w)) = c(w)c(h(w)) .Our actual job is exactly the same, except that pis specified not by a corpus but by the hypergraphHG(x).
The only change is that the n-gram countsc?
(w) are no longer integers from a corpus, but areexpected counts under p:10q?
(r(w) | h(w)) =c?(w)c?
(h(w))= (12)?y cw(y)p(y | x)?y ch(w)(y)p(y | x)=?y,d cw(y)p(y, d | x)?y,d ch(w)(y)p(y, d | x)Now, the question is how to efficiently compute(12) from the hypergraph HG(x).
To develop theintuition, we first present a brute-force algorithmin Figure 3.
The algorithm is brute-force sinceit first needs to unpack the hypergraph and enu-merate each possible derivation in the hypergraph(see line 1), which is computationally intractable.The algorithm then enumerates each n-gram and(n ?
1)-gram in y and accumulates its soft countinto the expected count, and finally obtains the pa-rameters of q?
by taking count ratios via (12).Figure 4 shows an efficient version that exploitsthe packed-forest structure of HG(x) in com-puting the expected counts.
Specifically, it firstruns the inside-outside procedure, which annotateseach node (say v) with both an inside weight ?
(v)and an outside weight ?(v).
The inside-outsidealso finds Z(x), the total weight of all derivations.With these weights, the algorithm then exploresthe hypergraph once more to collect the expected10One can prove (12) via Lagrange multipliers, with q?(?
|h) constrained to be a normalized distribution for each h.596Brute-Force-MLE(HG(x ))1 for y , d in HG(x)  each derivation2 forw in y  each n-gram type3  accumulate soft count4 c?
(w) + = cw(y) ?
p(y, d | x)5 c?
(h(w)) + = cw(y) ?
p(y, d | x)6 q?
?
MLE using formula (12)7 return q?Figure 3: Brute-force estimation of q?.Dynamic-Programming-MLE(HG(x ))1 run inside-outside on the hypergraph HG(x)2 for v in HG(x)  each node3 for e ?
B(v)  each incoming hyperedge4 ce ?
pe ?
?
(v)/Z(x)5 for u ?
T (e)  each antecedent node6 ce ?
ce ?
?
(u)7  accumulate soft count8 forw in e  each n-gram type9 c?
(w) + = cw(e) ?
ce10 c?
(h(w)) + = cw(e) ?
ce11 q?
?
MLE using formula (12)12 return q?Figure 4: Dynamic programming estimation of q?.
B(v) rep-resents the set of incoming hyperedges of node v; pe repre-sents the weight of the hyperedge e itself; T (e) representsthe set of antecedent nodes of hyperedge e. Please refer tothe text for the meanings of other notations.counts.
For each hyperedge (say e), it first gets theposterior weight ce (see lines 4-6).
Then, for eachn-gram type (say w), it increments the expectedcount by cw(e) ?
ce, where cw(e) is the number ofcopies of n-gram w that are added by hyperedgee, i.e., that appear in the yield of e but not in theyields of any of its antecedents u ?
T (e).While there may be exponentially many deriva-tions, the hypergraph data structure representsthem in polynomial space by allowing multiplederivations to share subderivations.
The algorithmof Figure 4 may be run over this packed forestin time O(|HG(x)|) where |HG(x)| is the hyper-graph?s size (number of hyperedges).3.2.3 Decoding with q?When translating x at runtime, the q?
constructedfrom HG(x) will be used as a surrogate for p dur-ing decoding.
We want its most probable string:y?
= argmaxyq?
(y) (13)Since q?
is an n-gram model, finding y?
is equiv-alent to a shortest-path problem in a certain graphwhose edges correspond to n-grams (weightedwith negative log-probabilities) and whose ver-tices correspond to (n?
1)-grams.However, because q?
only approximates p, y?
of(13) may be locally appropriate but globally inade-quate as a translation of x.
Observe, e.g., that an n-gram model q?
(y) will tend to favor short stringsy, regardless of the length of x.
Suppose x = lechat chasse la souris (?the cat chases the mouse?
)and q?
is a bigram approximation to p(y | x).
Pre-sumably q?
(the | START), q?
(mouse | the), andq?
(END | mouse) are all large in HG(x).
So themost probable string y?
under q?
may be simply?the mouse,?
which is short and has a high proba-bility but fails to cover x.Therefore, a better way of using q?
is to restrictthe search space to the original hypergraph, i.e.:y?
= argmaxy?T(x)q?
(y) (14)This ensures that y?
is a valid string in the origi-nal hypergraph HG(x), which will tend to rule outinadequate translations like ?the mouse.
?If our sole objective is to get a good approxi-mation to p(y | x), we should just use a singlen-gram model q?
whose order n is as large as pos-sible, given computational constraints.
This maybe regarded as favoring n-grams that are likely toappear in the reference translation (because theyare likely in the derivation forest).
However, in or-der to score well on the BLEU metric for MT eval-uation (Papineni et al, 2001), which gives partialcredit, we would also like to favor lower-order n-grams that are likely to appear in the reference,even if this means picking some less-likely high-order n-grams.
For this reason, it is useful to in-terpolate different orders of variational models,y?
= argmaxy?T(x)?n?n ?
log q?n(y) (15)where n may include the value of zero, in whichcase log q?0(y)def= |y|, corresponding to a conven-tional word penalty feature.
In the geometric inter-polation above, the weight ?n controls the relativeveto power of the n-gram approximation and canbe tuned using MERT (Och, 2003) or a minimumrisk procedure (Smith and Eisner, 2006).Lastly, note that Viterbi and variational approx-imation are different ways to approximate the ex-act probability p(y | x), and each of them haspros and cons.
Specifically, Viterbi approxima-tion uses the correct probability of one complete597derivation, but ignores most of the derivations inthe hypergraph.
In comparison, the variational ap-proximation considers all the derivations in the hy-pergraph, but uses only aggregate statistics of frag-ments of derivations.
Therefore, it is desirable tointerpolate further with the Viterbi approximationwhen choosing the final translation output:11y?
= argmaxy?T(x)?n?n ?
log q?n(y)+ ?v ?
log pViterbi(y | x) (16)where the first term corresponds to the interpolatedvariational decoding of (15) and the second termcorresponds to the Viterbi decoding of (4).12 As-suming ?v > 0, the second term penalizes transla-tions with no good derivation in the hypergraph.13For n ?
m, any of these decoders (14)?
(16) may be implemented efficiently by using then-gram variational approximations q?
to rescoreHG(x)?preserving its hypergraph topology, butmodifying the hyperedge weights.14 While theoriginal weights gave derivation d a score oflog p(d | x), the weights as modified for (16)will give d a score of?n ?n ?
log q?n(Y(d)) + ?v ?log p(d | x).
We then find the best-scoring deriva-tion and output its target yield; that is, we findargmaxy?T(x) via Y(argmaxd?D(x)).4 Variational vs. Min-Risk DecodingIn place of the MAP decoding, another commonlyused decision rule is minimum Bayes risk (MBR):y?
= argminyR(y) = argminy?y?l(y, y?)p(y?
| x)(17)11It would also be possible to interpolate with the N -bestapproximations (see Section 2.4), with some complications.12Zens and Ney (2006) use a similar decision rule as hereand they also use posterior n-gram probabilities as featurefunctions, but their model estimation and decoding are overan N -best, which is trivial in terms of computation.13Already at (14), we explicitly ruled out translations yhaving no derivation at all in the hypergraph.
However,suppose the hypergraph were very large (thanks to a largeor smoothed translation model and weak pruning).
Then(14)?s heuristic would fail to eliminate bad translations (?themouse?
), since nearly every string y ?
??
would be derivedas a translation with at least a tiny probability.
The ?soft?
ver-sion (16) solves this problem, since unlike the ?hard?
(14), itpenalizes translations that appear only weakly in the hyper-graph.
As an extreme case, translations not in the hypergraphat all are infinitely penalized (log pViterbi(y) = log 0 =??
), making it natural for the decoder not to consider them,i.e., to do only argmaxy?T(x) rather than argmaxy???
.14One might also want to use the q?n or smoothed versionsof them to rescore additional hypotheses, e.g., hypothesesproposed by other systems or by system combination.where l(y, y?)
represents the loss of y if the trueanswer is y?, and the risk of y is its expectedloss.15 Statistical decision theory shows MBR isoptimal if p(y?
| x) is the true distribution, whilein practice p(y?
| x) is given by a model at hand.We now observe that our variational decodingresembles the MBR decoding of Tromble et al(2008).
They use the following loss function, ofwhich a linear approximation to BLEU (Papineniet al, 2001) is a special case,l(y, y?)
= ?(?0|y|+?w?N?wcw(y)?w(y?))
(18)where w is an n-gram type, N is a set of n-gramtypes with n ?
[1, 4], cw(y) is the number of oc-currence of the n-gram w in y, and ?w(y?)
is anindicator function to check if y?
contains at leastone occurrence of w. With the above loss func-tion, Tromble et al (2008) derive the MBR rule16y?
= argmaxy(?0|y|+?w?N?wcw(y)g(w | x))(19)where g(w | x) is a specialized ?posterior?
proba-bility of the n-gram w, and is defined asg(w | x) =?y??w(y?)p(y?
| x) (20)Now, let us divide N , which contains n-gramtypes of different n, into several subsets Wn, eachof which contains only the n-grams with a givenlength n. We can now rewrite (19) as follows,y?
= argmaxy?n?n ?
gn(y | x) (21)by assuming ?w = ?|w| and,gn(y | x)={|y| if n = 0?w?Wn g(w | x)cw(y) if n > 0(22)Clearly, their rule (21) has a quite similar formto our rule (15), and we can relate (20) to (12) and(22) to (11).
This justifies the use of interpolationin Section 3.2.3.
However, there are several im-portant differences.
First, the n-gram ?posterior?of (20) is very expensive to compute.
In fact, it re-quires an intersection between each n-gram in thelattice and the lattice itself, as is done by Tromble15The MBR becomes the MAP decision rule of (1) if a so-called zero-one loss function is used: l(y, y?)
= 0 if y = y?
;otherwise l(y, y?)
= 1.16Note that Tromble et al (2008) only consider MBR for alattice without hidden structures, though their method can bein principle applied in a hypergraph with spurious ambiguity.598et al (2008).
In comparison, the optimal n-gramprobabilities of (12) can be computed using theinside-outside algorithm, once and for all.
Also,g(w | x) of (20) is not normalized over the historyof w, while q?
(r(w) | h(w)) of (12) is.
Lastly, thedefinition of the n-gram model is different.
Whilethe model (11) is a proper probabilistic model, thefunction of (22) is simply an approximation of theaverage n-gram precisions of y.A connection between variational decoding andminimum-risk decoding has been noted before(e.g., Matsuzaki et al (2005)), but the derivationabove makes the connection formal.DeNero et al (2009) concurrently developedan alternate to MBR, called consensus decoding,which is similar to ours in practice although moti-vated quite differently.5 Experimental ResultsWe report results using an open source MT toolkit,called Joshua (Li et al, 2009), which implementsHiero (Chiang, 2007).5.1 Experimental SetupWe work on a Chinese to English translation task.Our translation model was trained on about 1Mparallel sentence pairs (about 28M words in eachlanguage), which are sub-sampled from corporadistributed by LDC for the NIST MT evalua-tion using a sampling method based on the n-gram matches between training and test sets inthe foreign side.
We also used a 5-gram lan-guage model with modified Kneser-Ney smooth-ing (Chen and Goodman, 1998), trained on a dataset consisting of a 130M words in English Giga-word (LDC2007T07) and the English side of theparallel corpora.
We use GIZA++ (Och and Ney,2000), a suffix-array (Lopez, 2007), SRILM (Stol-cke, 2002), and risk-based deterministic annealing(Smith and Eisner, 2006)17 to obtain word align-ments, translation models, language models, andthe optimal weights for combining these models,respectively.
We use standard beam-pruning andcube-pruning parameter settings, following Chi-ang (2007), when generating the hypergraphs.The NIST MT?03 set is used to tune modelweights (e.g.
those of (16)) and the scaling factor17We have also experimented with MERT (Och, 2003), andfound that the deterministic annealing gave results that weremore consistent across runs and often better.Decoding scheme MT?04 MT?05Viterbi 35.4 32.6MBR (K=1000) 35.8 32.7Crunching (N=10000) 35.7 32.8Crunching+MBR (N=10000) 35.8 32.7Variational (1to4gram+wp+vt) 36.6 33.5Table 1: BLEU scores for Viterbi, Crunching, MBR, and vari-ational decoding.
All the systems improve significantly overthe Viterbi baseline (paired permutation test, p < 0.05).
Ineach column, we boldface the best result as well as all resultsthat are statistically indistinguishable from it.
In MBR, K isthe number of unique strings.
For Crunching and Crunch-ing+MBR, N represents the number of derivations.
On av-erage, each string has about 115 distinct derivations.
Thevariational method ?1to4gram+wp+vt?
is our full interpola-tion (16) of four variational n-gram models (?1to4gram?
), theViterbi baseline (?vt?
), and a word penalty feature (?wp?).?
of (3),18 and MT?04 and MT?05 are blind test-sets.
We will report results for lowercase BLEU-4,using the shortest reference translation in comput-ing brevity penalty.5.2 Main ResultsTable 1 presents the BLEU scores under Viterbi,crunching, MBR, and variational decoding.
Bothcrunching and MBR show slight significant im-provements over the Viterbi baseline; variationaldecoding gives a substantial improvement.The difference between MBR and Crunch-ing+MBR lies in how we approximate the distri-bution p(y?
| x) in (17).19 For MBR, we takep(y?
| x) to be proportional to pViterbi(y?
| x) if y?is among the K best distinct strings on that mea-sure, and 0 otherwise.
For Crunching+MBR, wetake p(y?
| x) to be proportional to pcrunch(y?
| x),which is based on the N best derivations.5.3 Results of Different Variational DecodingTable 2 presents the BLEU results under differentways in using the variational models, as discussedin Section 3.2.3.
As shown in Table 2a, decod-ing with a single variational n-gram model (VM)as per (14) improves the Viterbi baseline (exceptthe case with a unigram VM), though often notstatistically significant.
Moreover, a bigram (i.e.,?2gram?)
achieves the best BLEU scores amongthe four different orders of VMs.The interpolation between a VM and a wordpenalty feature (?wp?)
improves over the unigram18We found the BLEU scores are not very sensitive to ?,contrasting to the observations by Tromble et al (2008).19We also restrict T(x) to {y : p(y | x) > 0}, using thesame approximation for p(y | x) as we did for p(y?
| x).599(a) decoding with a single variational modelDecoding scheme MT?04 MT?05Viterbi 35.4 32.61gram 25.9 24.52gram 36.1 33.43gram 36.0?
33.14gram 35.8?
32.9(b) interpolation between a single variationalmodel and a word penalty feature1gram+wp 29.7 27.72gram+wp 35.5 32.63gram+wp 36.1?
33.14gram+wp 35.7?
32.8?
(c) interpolation of a single variational model, theViterbi model, and a word penalty feature1gram+wp+vt 35.6?
32.8?2gram+wp+vt 36.5?
33.5?3gram+wp+vt 35.8?
32.9?4gram+wp+vt 35.6?
32.8?
(d) interpolation of several n-gram VMs, theViterbi model, and a word penalty feature1to2gram+wp+vt 36.6?
33.6?1to3gram+wp+vt 36.6?
33.5?1to4gram+wp+vt 36.6?
33.5?Table 2: BLEU scores under different variational decodersdiscussed in Section 3.2.3.
A star ?
indicates a result that issignificantly better than Viterbi decoding (paired permutationtest, p < 0.05).
We boldface the best system and all systemsthat are not significantly worse than it.
The brevity penaltyBP in BLEU is always 1, meaning that on average y?
is noshorter than the reference translation, except for the ?1gram?systems in (a), which suffer from brevity penalties of 0.826and 0.831.VM dramatically, but does not improve higher-order VMs (Table 2b).
Adding the Viterbi fea-ture (?vt?)
into the interpolation further improvesthe lower-order models (Table 2c), and all the im-provements over the Viterbi baseline become sta-tistically significant.
At last, interpolation of sev-eral variational models does not yield much fur-ther improvement over the best previous model,but makes the results more stable (Table 2d).5.4 KL Divergence of Approximate ModelsWhile the BLEU scores reported show the prac-tical utility of the variational models, it is alsointeresting to measure how well each individualvariational model q(y) approximates the distribu-tion p(y | x).
Ideally, the quality of approxima-tion should be measured by the KL divergenceKL(p ?
q)def= H(p, q) ?
H(p), where the cross-entropy H(p, q)def= ?
?y p(y | x) log q(y), andMeasure H(p, ?)
Hd(p) H(p)bits/word q?1 q?2 q?3 q?4 ?MT?04 2.33 1.68 1.57 1.53 1.36 1.03MT?05 2.31 1.69 1.58 1.54 1.37 1.04Table 3: Cross-entropies H(p, q) achieved by various ap-proximations q.
The notation H denotes the sum of cross-entropies of all test sentences, divided by the total numberof test words.
A perfect approximation would achieve H(p),which we estimate using the true Hd(p) and a 10000-best list.the entropy H(p)def= ?
?y p(y | x) log p(y | x).Unfortunately H(p) (and hence KL = H(p, q) ?H(p)) is intractable to compute.
But, since H(p)is the same for all q, we can simply use H(p, q)to compare different models q.
Table 3 reports thecross-entropies H(p, q) for various models q.We also report the derivational entropyHd(p)def= ?
?d p(d | x) log p(d | x).20 From this,we obtain an estimate of H(p) by observing thatthe ?gap?
Hd(p) ?
H(p) equals Ep(y)[H(d | y)],which we estimate from our 10000-best list.Table 3 confirms that higher-order variationalmodels (drawn from a larger family Q) approxi-mate p better.
This is necessarily true, but it isinteresting to see that most of the improvement isobtained just by moving from a unigram to a bi-gram model.
Indeed, although Table 3 shows thatbetter approximations can be obtained by usinghigher-order models, the best BLEU score in Ta-bles 2a and 2c was obtained by the bigram model.After all, p cannot perfectly predict the referencetranslation anyway, hence may not be worth ap-proximating closely; but p may do a good jobof predicting bigrams of the reference translation,and the BLEU score rewards us for those.6 Conclusions and Future WorkWe have successfully applied the general varia-tional inference framework to a large-scale MTtask, to approximate the intractable problem ofMAP decoding in the presence of spurious am-biguity.
We also showed that interpolating vari-ational models with the Viterbi approximation cancompensate for poor approximations, and that in-terpolating them with one another can reduce theBayes risk and improve BLEU.
Our empirical re-sults improve the state of the art.20Both H(p, q) and Hd(p) involve an expectation over ex-ponentially many derivations, but they can be computed intime only linear in the size of HG(x) using an expectationsemiring (Eisner, 2002).
In particular, H(p, q) can be foundas ?
?d?D(x) p(d | x) log q(Y(d)).600Many interesting research directions remainopen.
To approximate the intractable MAP de-coding problem of (2), we can use different vari-ational distributions other than the n-gram modelof (11).
Interpolation with other models is alsointeresting, e.g., the constituent model in Zhangand Gildea (2008).
We might also attempt to min-imize KL(q ?
p) rather than KL(p ?
q), in orderto approximate the mode (which may be prefer-able since we care most about the 1-best transla-tion under p) rather than the mean of p (Minka,2005).
One could also augment our n-gram mod-els with non-local string features (Rosenfeld et al,2001) provided that the expectations of these fea-tures could be extracted from the hypergraph.Variational inference can also be exploited tosolve many other intractable problems in MT (e.g.,word/phrase alignment and system combination).Finally, our method can be used for tasks beyondMT.
For example, it can be used to approximatethe intractable MAP decoding inherent in systemsusing HMMs (e.g.
speech recognition).
It can alsobe used to approximate a context-free grammarwith a finite state automaton (Nederhof, 2005).ReferencesCyril Allauzen, Mehryar Mohri, and Brian Roark.2003.
Generalized algorithms for constructing sta-tistical language models.
In ACL, pages 40?47.Christopher M. Bishop.
2006.
Pattern recognition andmachine learning.
Springer.Phil Blunsom, Trevor Cohn, and Miles Osborne.
2008.A discriminative latent variable model for statisticalmachine translation.
In ACL, pages 200?208.Francisco Casacuberta and Colin De La Higuera.
2000.Computational complexity of problems on proba-bilistic grammars and transducers.
In ICGI, pages15?24.Stanley F. Chen and Joshua Goodman.
1998.
An em-pirical study of smoothing techniques for languagemodeling.
Technical report.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.John DeNero, David Chiang, and Kevin Knight.
2009.Fast consensus decoding over translation forests.
InACL-IJCNLP.Jason Eisner.
2002.
Parameter estimation for proba-bilistic finite-state transducers.
In ACL, pages 1?8.Joshua Goodman.
1996.
Efficient algorithms for pars-ing the DOP model.
In EMNLP, pages 143?152.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In ACL, pages 144?151.M.
I. Jordan, Z. Ghahramani, T. S. Jaakkola, and L. K.Saul.
1999.
An introduction to variational meth-ods for graphical models.
In Learning in GraphicalModels.
MIT press.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InNAACL, pages 48?54.Zhifei Li and Sanjeev Khudanpur.
2008.
A scalabledecoder for parsing-based machine translation withequivalent language model state maintenance.
InACL SSST, pages 10?18.Zhifei Li, Chris Callison-Burch, Chris Dyer, JuriGanitkevitch, Sanjeev Khudanpur, Lane Schwartz,Wren Thornton, Jonathan Weese, and Omar.
Zaidan.2009.
Joshua: An open source toolkit for parsing-based machine translation.
In WMT09, pages 135?139.Adam Lopez.
2007.
Hierarchical phrase-based trans-lation with suffix arrays.
In EMNLP-CoNLL, pages976?985.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic CFG with latent annotations.
InACL, pages 75?82.Jonathan May and Kevin Knight.
2006.
A better n-bestlist: practical determinization of weighted finite treeautomata.
In NAACL, pages 351?358.Tom Minka.
2005.
Divergence measures and messagepassing.
In Microsoft Research Technical Report(MSR-TR-2005-173).
Microsoft Research.Mark-Jan Nederhof.
2005.
A general technique totrain language models on language models.
Com-put.
Linguist., 31(2):173?186.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In ACL, pages 440?447.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL, pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automatic eval-uation of machine translation.
In ACL, pages 311?318.Roni Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.2001.
Whole-sentence exponential language mod-els: A vehicle for linguistic-statistical integration.Computer Speech and Language, 15(1).Khalil Sima?an.
1996.
Computational complexityof probabilistic disambiguation by means of tree-grammars.
In COLING, pages 1175?1180.David A. Smith and Jason Eisner.
2006.
Minimum riskannealing for training log-linear models.
In ACL,pages 787?794.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In ICSLP, pages 901?904.Roy Tromble, Shankar Kumar, Franz Och, and Wolf-gang Macherey.
2008.
Lattice Minimum Bayes-Risk decoding for statistical machine translation.
InEMNLP, pages 620?629.Richard Zens and Hermann Ney.
2006.
N-gram poste-rior probabilities for statistical machine translation.In WMT06, pages 72?77.Hao Zhang and Daniel Gildea.
2008.
Efficient multi-pass decoding for synchronous context free gram-mars.
In ACL, pages 209?217.601
