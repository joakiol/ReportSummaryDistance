Memory-based semantic role labeling:Optimizing features, algorithm, and outputAntal van den Bosch, Sander Canisius,Iris HendrickxILK / Computational LinguisticsTilburg University, P.O.
Box 90153,NL-5000 LE Tilburg, The Netherlands{Antal.vdnBosch,S.V.M.Canisius,I.H.E.Hendrickx}@uvt.nlWalter Daelemans,Erik Tjong Kim SangCNTS / Department of LinguisticsUniversity of Antwerp, Universiteitsplein 1,B-2610 Antwerpen, Belgium{Walter.Daelemans,Erik.TjongKimSang}@ua.ac.be1 IntroductionIn this paper we interpret the semantic role labeling prob-lem as a classification task, and apply memory-basedlearning to it in an approach similar to Buchholz et al(1999) and Buchholz (2002) for grammatical relation la-beling.
We apply feature selection and algorithm parame-ter optimization strategies to our learner.
In addition, weinvestigate the effect of two innovations: (i) the use ofsequences of classes as classification output, combinedwith a simple voting mechanism, and (ii) the use of iter-ative classifier stacking which takes as input the originalfeatures and a pattern of outputs of a first-stage classifier.Our claim is that both methods avoid errors in sequencesof predictions typically made by simple classifiers thatare unaware of their previous or subsequent decisions ina sequence.2 Data and FeaturesThe CoNLL-2004 shared task (Carreras and Ma`rquez,2004) supplied data sets for the semantic role labelingtask with several levels of annotation apart from the rolelabels to be predicted.
Central to our approach is thechoice to adopt the instance encoding analogous to Buch-holz et al (1999) to have our examples represent relationsbetween pairs of verbs and chunks.
That is, we transformthe semantic role labeling task to a classification task inwhich we decide for all pairs of verbs and chunks whetherthey stand in a semantic role relation.
Afterwards we con-sider all adjacent chunks to which the same role label isassigned by our classifier as belonging to the same argu-ment.
All results reported below use this task representa-tion.
Processing focuses on one verb at a time; verbs aretreated independently.We did not employ the provided Propbank data northe verb sense information available, nor did we use anyother external source of information.Apart from the provided words and the predicted PoStags, chunk labels, clause labels, and named-entity labels,provided beforehand, we have considered an additionalset of automatically derived features:1. attenuated words (Eisner, 1996), i.e.
wordformsoccurring below a frequency threshold (of 10) areconverted to a string capturing some of the originalword form?s features (capitalization, whether it con-tains numbers or a hyphen, or suffix letters);2. the distance between the candidate role word and theverb, measured in intervening words, chunks, NPchunks or VP chunks (negative if the word is to theleft, positive if it is to the right of the verb);3. preceding preposition: a feature containing the headword of the previous chunk if that was labeled aspreposition;4. passive main verb: a binary feature which is on ifthe main verb is used in a passive construction;5. current clause: a binary feature which is on if thecurrent word is in the same clause as the main verb;6. role pattern: the most frequently occurring role pat-tern of the main verb in the training data (containsthe order of V and A0-A5).For every target verb in every sentence, the data sup-plied to the learners contains instances for every headword of non-VP chunks and for all words in VP chunks,and all words in all chunks containing a target verb (i.e.,more instances than chunks, to account for the fact thatsome roles are contained within chunks).
Here is an ex-ample instance for the second chunk of the training data:expect -2 -1 0 morph-cap in INNN PP passive clause A0VA1 A1This instance contains 12 features: the verb (1), dis-tance to the verb measured in chunks (2), NP chunks (3)and VP chunks (4), attenuated words (5?6), PoS tags (7?8), a chunk tag (9), passive main verb (10), current clause(11) and role pattern (12).
The final item of the line isthe required output class.
Our choice of instance formatis only slightly harmful for performance: with a perfectclassifier we can still obtain a maximal F?=1 score of 99.1on the development data.3 ApproachIn this section we describe our approach to semantic rolelabeling.
The core part of our system is a memory-basedlearner.
During the development of the system we haveused feature selection and parameter optimization by it-erative deepening.
Additionally we have evaluated threeextensions of the basic memory-based learning method:class n-grams, i.e.
complex classes composed of se-quences of simple classes, iterative classifier stacking andautomatic output post-processing.3.1 Memory-based learningMemory-based learning is a supervised inductive algo-rithm for learning classification tasks based on the k-nn algorithm (Cover and Hart, 1967; Aha et al, 1991)with various extensions for dealing with nominal featuresand feature relevance weighting.
Memory-based learn-ing stores feature representations of training instances inmemory without abstraction and classifies new (test) in-stances by matching their feature representation to all in-stances in memory, finding the most similar instances.From these ?nearest neighbors?, the class of the test itemis extrapolated.
See Daelemans et al (2003) for a de-tailed description of the algorithms and metrics used inour experiments.
All memory-based learning experi-ments were done with the TiMBL software package1.In previous research, we have found that memory-based learning is rather sensitive to the chosen featuresand the particular setting of its algorithmic parameters(e.g.
the number of nearest neighbors taken into account,the function for extrapolation from the nearest neighbors,the feature relevance weighting method used, etc.).
In or-der to minimize the effects of this sensitivity, we have putmuch effort in trying to find the best set of features andthe optimal learner parameters for this particular task.3.2 Feature selectionWe have employed bi-directional hill-climbing (Caruanaand Freitag, 1994) for finding the features that were mostsuited for this task.
This wrapper approach starts with theempty set of features and evaluates the learner for everyindividual feature on the development set.
The featureassociated with the best performance is selected and theprocess is repeated for every pair of features that includesthe best feature.
For every next best set of features, the1We used TiMBL version 5.0, available freely for researchfrom http://ilk.uvt.nl.system evaluates each set that contains one extra featureor has one feature less.
This process is repeated until thelocal search does not lead to a performance gain.3.3 Parameter optimizationWe used iterative deepening (ID) as a heuristic way ofsearching for optimal algorithm parameters.
This tech-nique combines classifier wrapping (using the trainingmaterial internally to test experimental variants) (Kohaviand John, 1997) with progressive sampling of trainingmaterial (Provost et al, 1999).
We start with a large poolof experiments, each with a unique combination of algo-rithmic parameter settings.
Each settings combination isapplied to a small amount of training material and testedon a small held-out set alo taken from the training set.Only the best settings are kept; the others are removedfrom the pool of competing settings.
In subsequent itera-tions, this step is repeated, retaining the best-performingsettings, with an exponentially growing amount of train-ing and held-out data ?
until all training data is used orone best setting is left.
Selecting the best settings at eachstep is based on classification accuracy on the held-outdata; a simple one-dimensional clustering on the rankedlist of accuracies determines which group of settings isselected for the next iteration.3.4 Class n-gramsAlternative to predicting simple classes, sequential taskscan be rephrased as mappings from input examples tosequences of classes.
Instead of predicting just A1 inthe example given earlier, it is possible to predict a tri-gram of classes.
The second example in the training datawhich we used earlier, is now labeled with the trigramA1 A1 A1, indicating that the chunk in focus has an A1relation with the verb, along with its left and right neigh-bor chunks (which are all part of the same A1 argument).expect -2 -1 0 morph-cap inIN NN PP passive clause A0VA1A1 A1 A1Predicting class trigrams offers two potential benefits.First, the classifier is forced to predict ?legal?
sequencesof classes; this potentially fixes a problem with simpleclassifiers which are blind to their previous or subsequentsimple classifications in sequences, potentially resultingin impossible sequences such as A1 A0 A1.
Second,if the classifier predicts the trigrams example by exam-ple, it produces a sequence of overlapping trigrams whichmay contain information that can boost classification ac-curacy.
Effectively, each class is predicted three times, sothat a simple majority voting can be applied: we simplytake the middle prediction as the actual classification ofthe example unless the two other votes together suggestanother class label.Prec.
Recall F?=1 methoda 51.6% 51.9% 51.8 feature selectionb 57.3% 52.7% 54.9 parameter optimizationc 58.8% 54.2% 56.4 feature selectiond 59.5% 53.9% 56.5 parameter optimizatione 64.3% 54.2% 58.8 classifier stackingf 66.3% 56.3% 60.9 parameter optimizationg 66.5% 56.3% 60.9 feature selectionh 68.1% 56.8% 61.9 classifier stackingi 68.3% 57.5% 62.4 feature selectionj 68.9% 57.8% 62.9 classifier stackingk 69.1% 57.8% 63.0 classifier stacking50.6% 30.3% 37.9 baselineTable 1: Effects of cascaded feature selection, parameteroptimization and classifier stacking on the performancemeasured on the development data set.3.5 Iterative classifier stackingStacking (Wolpert, 1992) refers to a class of meta-learning systems that learn to correct errors made bylower-level classifiers.
We implement stacking by addinga windowed sequence of previous and subsequent outputclass labels to the original input features.
To generatethe training material, we copy these windowed (unigram)class labels into the input, excluding the focus class label(which is a perfect predictor of the output class).
To gen-erate test material, the output of the first-stage classifiertrained on the original data is used.Stacking can be repeated; an nth-stage classifier can bebuilt on the output of the n-1th-stage classifier.
We im-plemented this by replacing the class features in the inputof each nth-stage classifier by the output of the previousclassifier.3.6 Automatic output post-processingEven while employing n-gram output classes and clas-sifier stacking, we noticed that our learner made sys-tematic errors caused by the lack of broader (sentential)contextual information in the instances and the classes.The most obvious of these errors was having multiple in-stances of arguments A0-A5 in one sentence.
Althoughsentences with multiple A0-A3 arguments appear in thetraining data, they are quite rare (0.17%).
When thelearner assigns an A0 role to three different argumentsin a sentence, most likely at least two of these are wrong.In order to reflect this fact, we have restricted the sys-tem to outputting at most one phrase of type A0-A5.
Ifthe learner predicts multiple arguments then only the oneclosest to the main verb is kept.Features a-b c-d e-f g-h i-kwords -1?0 -2?1 -2?1 -2?1 -2?1PoS tags 0?1 0?1 0?1 -1?1 -1?1chunk tags 0 0?2 0?2 -1?1 -1?1NE tags ?
?
?
?
?output classes NA NA -3?3 -3?3 -3?3distances cNV cNVw cNVw Vw cNVmain verb + + + + +role pattern + + + + +passive verb + + + + +current clause + + + + +previous prep.
?
+ + + ?Total 12 18 24 23 24Table 2: Features used in the different runs mentionedin Table 1.
The numbers mentioned for words, part-of-speech tags, chunk tags, named entity tags and outputclasses show the position of the tokens with respect tothe focus token (0).
Distances are measured in chunks,NP chunks, VP chunks and words.
In all other table en-tries, + denotes selection and ?
omission.Parameters a b-c d-e f-kalgorithm IB1 IB1 IB1 IB1distance metric O M J Oswitching threshold NA 2 2 NAfeature weighting nw nw nw nwneighborhood size 1 15 19 1class weights Z ED1 ED1 ZTable 3: Parameters of the machines learner that wereused in the different runs mentioned in Table 1.
Moreinformation about the parameters and their values can befound in Daelemans et al (2003).4 ResultsWe started with a feature selection process with the fea-tures described in section 2.
This experiment used abasic k-nn classifier without feature weighting, a near-est neighborhood of size 1, attenuated words, and outputpost-processing.
We evaluated the effect of trigram out-put classes by performing an experiment with and with-out them.
The feature selection experiment without tri-gram output classes selected 10 features and obtained anF?=1 score of 46.3 on the development data set.
The ex-periment that made use of combined classes selected 12features and reached a score of 51.8.We decided to continue using trigram output classes.Subsequently, we optimized the parameters of our ma-chine learner based on the features in the second experi-ment and performed another feature selection experimentwith these parameters.
The performance effects can befound in Table 1 (rows b and c).
An additional parameteroptimization step did not have a substantial effect (Ta-ble 1, row d).After training a stacked classifier while using the out-put of the best first stage learner, performance wentup from 56.5 to 58.8.
Additional feature selectionand parameter optimization were useful at this level(F?=1=60.9, see Table 1).
Most of our other performancegain was obtained by a continued process of classifierstacking.
Parameter optimization did not result in im-proved performance when stacking more than one classi-fier.
Feature selection was useful for the third-stage clas-sifier but not for the next one.
Our final system obtainedan F?=1 score of 63.0 on the development data (Table 1)and 60.1 on the test set (Table 4).5 ConclusionWe have described a memory-based semantic role labeler.In the development of the system we have used featureselection through bi-directional hill-climbing and param-eter optimization through iterative deepening search.
Wehave evaluated n-gram output classes, classifier stackingand output post-processing, all of which increased per-formance.
An overview of the performance of the systemon the test data can be found in Table 4.AcknowledgementsSander Canisius, Iris Hendrickx, and Antal van denBosch are funded by NWO (Netherlands Organisation forScientific Research).
Erik Tjong Kim Sang is funded byIWT STWW as a researcher in the ATraNoS project.ReferencesD.
W. Aha, D. Kibler, and M. Albert.
1991.
Instance-based learning algorithms.
Machine Learning, 6:37?66.S.
Buchholz, J. Veenstra, and W. Daelemans.
1999.
Cas-caded grammatical relation assignment.
In EMNLP-VLC?99, the Joint SIGDAT Conference on EmpiricalMethods in Natural Language Processing and VeryLarge Corpora, June.S.
Buchholz.
2002.
Memory-Based Grammatical Rela-tion Finding.
PhD thesis, University of Tilburg.X.
Carreras and L. Ma`rquez.
2004.
Introduction to theconll-2004 shared task: Semantic role labe ling.
InProceedings of CoNLL-2004.
Boston, MA, USA.R.
Caruana and D. Freitag.
1994.
Greedy attribute se-lection.
In Proceedings of the Eleventh InternationalConference on Machine Learning, pages 28?36, NewBrunswick, NJ, USA.
Morgan Kaufman.T.
M. Cover and P. E. Hart.
1967.
Nearest neighborpattern classification.
Institute of Electrical and Elec-Precision Recall F?=1Overall 67.12% 54.46% 60.13A0 80.41% 70.18% 74.95A1 62.04% 59.67% 60.83A2 46.29% 35.85% 40.41A3 59.42% 27.33% 37.44A4 67.44% 58.00% 62.37A5 0.00% 0.00% 0.00AM-ADV 25.00% 4.56% 7.71AM-CAU 0.00% 0.00% 0.00AM-DIR 33.33% 12.00% 17.65AM-DIS 58.38% 50.70% 54.27AM-EXT 53.85% 50.00% 51.85AM-LOC 38.79% 19.74% 26.16AM-MNR 48.00% 18.82% 27.04AM-MOD 97.11% 89.61% 93.21AM-NEG 74.67% 88.19% 80.87AM-PNC 44.44% 4.71% 8.51AM-PRD 0.00% 0.00% 0.00AM-TMP 58.84% 32.53% 41.90R-A0 80.26% 76.73% 78.46R-A1 78.95% 42.86% 55.56R-A2 100.00% 22.22% 36.36R-A3 0.00% 0.00% 0.00R-AA 0.00% 0.00% 0.00R-AM-LOC 0.00% 0.00% 0.00R-AM-MNR 0.00% 0.00% 0.00R-AM-PNC 0.00% 0.00% 0.00R-AM-TMP 66.67% 14.29% 23.53V 97.93% 97.93% 97.93Table 4: The performance of our system measured on thetest data.tronics Engineers Transactions on Information Theory,13:21?27.W.
Daelemans, J. Zavrel, K. Van der Sloot, and A. Vanden Bosch.
2003.
TiMBL: Tilburg memory basedlearner, version 5.0, reference guide.
ILK Techni-cal Report 03-08, Tilburg University.
available fromhttp://ilk.uvt.nl/downloads/pub/papers/ilk.0308.ps.J.
Eisner.
1996.
An Empirical Comparison of Probabil-ity Models for Dependency Grammar.
Technical Re-port IRCS-96-11, Institute for Research in CognitiveScience, University of Pennsylvania.R.
Kohavi and G. John.
1997.
Wrappers for featuresubset selection.
Artificial Intelligence Journal, 97(1?2):273?324.F.
Provost, D. Jensen, and T. Oates.
1999.
Efficient pro-gressive sampling.
In Proceedings of the Fifth Interna-tional Conference on Knowledge Discovery and DataMining, pages 23?32.D.
H. Wolpert.
1992.
On overfitting avoidance as bias.Technical Report SFI TR 92-03-5001, The Santa FeInstitute.
