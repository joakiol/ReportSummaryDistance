Proceedings of NAACL HLT 2007, pages 196?203,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsUsing Wikipedia for Automatic Word Sense DisambiguationRada MihalceaDepartment of Computer ScienceUniversity of North Texasrada@cs.unt.eduAbstractThis paper describes a method for generat-ing sense-tagged data using Wikipedia asa source of sense annotations.
Throughword sense disambiguation experiments,we show that the Wikipedia-based senseannotations are reliable and can be used toconstruct accurate sense classifiers.1 IntroductionAmbiguity is inherent to human language.
In partic-ular, word sense ambiguity is prevalent in all naturallanguages, with a large number of the words in anygiven language carrying more than one meaning.For instance, the English noun plant can mean greenplant or factory; similarly the French word feuillecan mean leaf or paper.
The correct sense of an am-biguous word can be selected based on the contextwhere it occurs, and correspondingly the problem ofword sense disambiguation is defined as the task ofautomatically assigning the most appropriate mean-ing to a polysemous word within a given context.Among the various knowledge-based (Lesk,1986; Galley and McKeown, 2003; Navigli and Ve-lardi, 2005) and data-driven (Yarowsky, 1995; Ngand Lee, 1996; Pedersen, 2001) word sense dis-ambiguation methods that have been proposed todate, supervised systems have been constantly ob-served as leading to the highest performance.
Inthese systems, the sense disambiguation problemis formulated as a supervised learning task, whereeach sense-tagged occurrence of a particular wordis transformed into a feature vector which is thenused in an automatic learning process.
Despite theirhigh performance, these supervised systems have animportant drawback: their applicability is limited tothose few words for which sense tagged data is avail-able, and their accuracy is strongly connected to theamount of labeled data available at hand.To address the sense-tagged data bottleneck prob-lem, different methods have been proposed in thepast, with various degrees of success.
This includesthe automatic generation of sense-tagged data usingmonosemous relatives (Leacock et al, 1998; Mi-halcea and Moldovan, 1999; Agirre and Martinez,2004), automatically bootstrapped disambiguationpatterns (Yarowsky, 1995; Mihalcea, 2002), paral-lel texts as a way to point out word senses bear-ing different translations in a second language (Diaband Resnik, 2002; Ng et al, 2003; Diab, 2004),and the use of volunteer contributions over the Web(Chklovski and Mihalcea, 2002).In this paper, we investigate a new approach forbuilding sense tagged corpora using Wikipedia as asource of sense annotations.
Starting with the hy-perlinks available in Wikipedia, we show how wecan generate sense annotated corpora that can beused for building accurate and robust sense clas-sifiers.
Through word sense disambiguation ex-periments performed on the Wikipedia-based sensetagged corpus generated for a subset of the SENSE-VAL ambiguous words, we show that the Wikipediaannotations are reliable, and the quality of a sensetagging classifier built on this data set exceeds by alarge margin the accuracy of an informed baselinethat selects the most frequent word sense by default.The paper is organized as follows.
We first pro-196vide a brief overview of Wikipedia, and describe theview of Wikipedia as a sense tagged corpus.
We thenshow how the hyperlinks defined in this resourcecan be used to derive sense annotated corpora, andwe show how a word sense disambiguation systemcan be built on this dataset.
We present the resultsobtained in the word sense disambiguation experi-ments, and conclude with a discussion of the results.2 WikipediaWikipedia is a free online encyclopedia, represent-ing the outcome of a continuous collaborative effortof a large number of volunteer contributors.
Virtu-ally any Internet user can create or edit a Wikipediawebpage, and this ?freedom of contribution?
has apositive impact on both the quantity (fast-growingnumber of articles) and the quality (potential mis-takes are quickly corrected within the collaborativeenvironment) of this online resource.
Wikipedia edi-tions are available for more than 200 languages, witha number of entries varying from a few pages tomore than one million articles per language.1The basic entry in Wikipedia is an article (orpage), which defines and describes an entity or anevent, and consists of a hypertext document with hy-perlinks to other pages within or outside Wikipedia.The role of the hyperlinks is to guide the reader topages that provide additional information about theentities or events mentioned in an article.Each article in Wikipedia is uniquely referencedby an identifier, which consists of one or more wordsseparated by spaces or underscores, and occasion-ally a parenthetical explanation.
For example, thearticle for bar with the meaning of ?counter fordrinks?
has the unique identifier bar (counter).2The hyperlinks within Wikipedia are created us-ing these unique identifiers, together with an an-chor text that represents the surface form of the hy-perlink.
For instance, ?Henry Barnard, [[UnitedStates|American]] [[educationalist]], was born in[[Hartford, Connecticut]]?
is an example of a sen-tence in Wikipedia containing links to the articlesUnited States, educationalist, and Hartford, Con-1In the experiments reported in this paper, we use a down-load from March 2006 of the English Wikipedia, with approxi-mately 1 million articles, and more than 37 millions hyperlinks.2The unique identifier is also used to form the article URL,e.g.
http://en.wikipedia.org/wiki/Bar (counter)necticut.
If the surface form and the unique iden-tifier of an article coincide, then the surface formcan be turned directly into a hyperlink by placingdouble brackets around it (e.g.
[[educationalist]]).Alternatively, if the surface form should be hyper-linked to an article with a different unique identi-fier, e.g.
link the word American to the article onUnited States, then a piped link is used instead, as in[[United States|American]].One of the implications of the large number ofcontributors editing the Wikipedia articles is theoccasional lack of consistency with respect to theunique identifier used for a certain entity.
For in-stance, the concept of circuit (electric) is also re-ferred to as electronic circuit, integrated circuit,electric circuit, and others.
This has led to the so-called redirect pages, which consist of a redirectionhyperlink from an alternative name (e.g.
integratedcircuit) to the article actually containing the descrip-tion of the entity (e.g.
circuit (electric)).Finally, another structure that is particularly rel-evant to the work described in this paper is thedisambiguation page.
Disambiguation pages arespecifically created for ambiguous entities, and con-sist of links to articles defining the different mean-ings of the entity.
The unique identifier for a dis-ambiguation page typically consists of the paren-thetical explanation (disambiguation) attached tothe name of the ambiguous entity, as in e.g.
cir-cuit (disambiguation) which is the unique identifierfor the disambiguation page of the entity circuit.3 Wikipedia as a Sense Tagged CorpusA large number of the concepts mentioned inWikipedia are explicitly linked to their correspond-ing article through the use of links or piped links.Interestingly, these links can be regarded as senseannotations for the corresponding concepts, whichis a property particularly valuable for entities thatare ambiguous.
In fact, it is precisely this observa-tion that we rely on in order to generate sense taggedcorpora starting with the Wikipedia annotations.For example, ambiguous words such as e.g.
plant,bar, or chair are linked to different Wikipedia ar-ticles depending on their meaning in the contextwhere they occur.
Note that the links are manuallycreated by the Wikipedia users, which means thatthey are most of the time accurate and referencing197the correct article.
The following represent five ex-ample sentences for the ambiguous word bar, withtheir corresponding Wikipedia annotations (links):In 1834, Sumner was admitted to the [[bar(law)|bar]] at the age of twenty-three, and enteredprivate practice in Boston.It is danced in 3/4 time (like most waltzes), withthe couple turning approx.
180 degrees every [[bar(music)|bar]].Vehicles of this type may contain expensive au-dio players, televisions, video players, and [[bar(counter)|bar]]s, often with refrigerators.Jenga is a popular beer in the [[bar(establishment)|bar]]s of Thailand.This is a disturbance on the water surface of a riveror estuary, often cause by the presence of a [[bar(landform)|bar]] or dune on the riverbed.To derive sense annotations for a given ambigu-ous word, we use the links extracted for all the hy-perlinked Wikipedia occurrences of the given word,and map these annotations to word senses.
For in-stance, for the bar example above, we extract fivepossible annotations: bar (counter), bar (establish-ment), bar (landform), bar (law), and bar (music).Although Wikipedia provides the so-called dis-ambiguation pages that list the possible meanings ofa given word, we decided to use instead the anno-tations collected directly from the Wikipedia links.This decision is motivated by two main reasons.First, a large number of the occurrences of ambigu-ous words are not linked to the articles mentionedby the disambiguation page, but to related concepts.This can happen when the annotation is performedusing a concept that is similar, but not identical to theconcept defined.
For instance, the annotation for theword bar in the sentence ?The blues uses a rhyth-mic scheme of twelve 4/4 [[measure (music)|bars]]?is measure (music), which, although correct and di-rectly related to the meaning of bar (music), is notlisted in the disambiguation page for bar.Second, most likely due to the fact that Wikipediais still in its incipient phase, there are several in-consistencies that make it difficult to use the disam-biguation pages in an automatic system.
For exam-ple, for the word bar, the Wikipedia page with theidentifier bar is a disambiguation page, whereas forthe word paper, the page with the identifier papercontains a description of the meaning of paper as?material made of cellulose,?
and a different pagepaper (disambiguation) is defined as a disambigua-tion page.
Moreover, in other cases such as e.g.
theentries for the word organization, no disambiguationpage is defined; instead, the articles correspondingto different meanings of this word are connected bylinks labeled as ?alternative meanings.
?Therefore, rather than using the senses listed ina disambiguation page as the sense inventory fora given ambiguous word, we chose instead to col-lect all the annotations available for that word inthe Wikipedia pages, and then map these labels toa widely used sense inventory, namely WordNet.33.1 Building Sense Tagged CorporaStarting with a given ambiguous word, we derive asense-tagged corpus following three main steps:First, we extract all the paragraphs in Wikipediathat contain an occurrence of the ambiguous wordas part of a link or a piped link.
We select para-graphs based on the Wikipedia paragraph segmen-tation, which typically lists one paragraph per line.4To focus on the problem of word sense disambigua-tion, rather than named entity recognition, we ex-plicitly avoid named entities by considering onlythose word occurrences that are spelled with a lowercase.
Although this simple heuristic will also elim-inate examples where the word occurs at the begin-ning of a sentence (and therefore are spelled with anupper case), we decided nonetheless to not considerthese examples so as to avoid any possible errors.Next, we collect all the possible labels for thegiven ambiguous word by extracting the leftmostcomponent of the links.
For instance, in thepiped link [[musical notation|bar]], the label musi-cal notation is extracted.
In the case of simple links(e.g.
[[bar]]), the word itself can also play the roleof a valid label if the page it links to is not deter-mined as a disambiguation page.Finally, the labels are manually mapped to theircorresponding WordNet sense, and a sense tagged3Alternatively, the Wikipedia annotations could also playthe role of a sense inventory, without the mapping to WordNet.We chose however to perform this mapping for the purpose ofallowing evaluations using a widely used sense inventory.4The average length of a paragraph is 80 words.198Word sense Labels in Wikipedia Wikipedia definition WordNet definitionbar (establishment) bar (establishment), nightclub a retail establishment which serves a room or establishment wheregay club, pub alcoholic beverages alcoholic drinks are servedover a counterbar (counter) bar (counter) the counter from which drinks a counter where you can obtainare dispensed food or drinkbar (unit) bar (unit) a scientific unit of pressure a unit of pressure equal to a milliondynes per square centimeterbar (music) bar (music), measure music a period of music musical notation for a repeatingmusical notation pattern of musical beatsbar (law) bar association, bar law the community of persons engaged the body of individuals qualified tolaw society of upper canada in the practice of law practice law in a particularstate bar of california jurisdictionbar (landform) bar (landform) a type of beach behind which lies a submerged (or partly submerged)a lagoon ridge in a river or along a shorebar (metal) bar metal, pole (object) - a rigid piece of metal or woodbar (sports) gymnastics uneven bars, - a horizontal rod that serves as ahandle bar support for gymnasts as theyperform exercisesbar (solid) candy bar, chocolate bar - a block of solid substanceTable 1: Word senses for the word bar, based on annotation labels used in Wikipediacorpus is created.
This mapping process is very fast,as a relatively small number of labels is typicallyidentified for a given word.
For instance, for thedataset used in the experiments reported in Section5, an average of 20 labels per word was extracted.To ensure the correctness of this last step, forthe experiments reported in this paper we used twohuman annotators who independently mapped theWikipedia labels to their corresponding WordNetsense.
In case of disagreement, a consensus wasreached through adjudication by a third annotator.In a mapping agreement experiment performed onthe dataset from Section 5, an inter-annotator agree-ment of 91.1% was observed with a kappa statisticsof ?=87.1, indicating a high level of agreement.3.2 An ExampleAs an example, consider the ambiguous word bar,with 1,217 examples extracted from Wikipediawhere bar appeared as the rightmost component ofa piped link or as a word in a simple link.
Sincethe page with the identifier bar is a disambigua-tion page, all the examples containing the singlelink [[bar]] are removed, as the link does not re-move the ambiguity.
This process leaves us with1,108 examples, from which 40 different labels areextracted.
These labels are then manually mappedto nine senses in WordNet.
Figure 1 shows the la-bels extracted from the Wikipedia annotations forthe word bar, the corresponding WordNet definition,as well as the Wikipedia definition (when the sensewas defined in the Wikipedia disambiguation page).4 Word Sense DisambiguationProvided a set of sense-annotated examples for agiven ambiguous word, the task of a word sense dis-ambiguation system is to automatically learn a dis-ambiguation model that can predict the correct sensefor a new, previously unseen occurrence of the word.We use a word sense disambiguation system thatintegrates local and topical features within a ma-chine learning framework, similar to several of thetop-performing supervised word sense disambigua-tion systems participating in the recent SENSEVALevaluations (http://www.senseval.org).The disambiguation algorithm starts with a pre-processing step, where the text is tokenized and an-notated with part-of-speech tags.
Collocations areidentified using a sliding window approach, wherea collocation is defined as a sequence of words thatforms a compound concept defined in WordNet.Next, local and topical features are extracted fromthe context of the ambiguous word.
Specifically, weuse the current word and its part-of-speech, a localcontext of three words to the left and right of the am-biguous word, the parts-of-speech of the surround-ing words, the verb and noun before and after theambiguous words, and a global context implementedthrough sense-specific keywords determined as a listof at most five words occurring at least three times199in the contexts defining a certain word sense.This feature set is similar to the one used by (Ngand Lee, 1996), as well as by a number of state-of-the-art word sense disambiguation systems partici-pating in the SENSEVAL-2 and SENSEVAL-3 evalu-ations.
The features are integrated in a Naive Bayesclassifier, which was selected mainly for its perfor-mance in previous work showing that it can lead toa state-of-the-art disambiguation system given thefeatures we consider (Lee and Ng, 2002).5 Experiments and ResultsTo evaluate the quality of the sense annotations gen-erated using Wikipedia, we performed a word sensedisambiguation experiment on a subset of the am-biguous words used during the SENSEVAL-2 andSENSEVAL-3 evaluations.
Since the Wikipedia an-notations are focused on nouns (associated with theentities typically defined by Wikipedia), the senseannotations we generate and the word sense disam-biguation experiments are also focused on nouns.Starting with the 49 ambiguous nouns used duringthe SENSEVAL-2 (29) and SENSEVAL-3 (20) evalu-ations, we generated sense tagged corpora follow-ing the process outlined in Section 3.1.
We then re-moved all those words that have only one Wikipedialabel (e.g.
detention, which occurs 58 times, butappears as a single link [[detention]] in all the oc-currences), or which have several labels that are allmapped to the same WordNet sense (e.g.
church,which has 2,198 occurrences with several differ-ent labels such as Roman church, Christian church,Catholic church, which are all mapped to the mean-ing of church, Christian church as defined in Word-Net).
This resulted in a set of 30 words that havetheir Wikipedia annotations mapped to at least twosenses according to the WordNet sense inventory.Table 2 shows the disambiguation results usingthe word sense disambiguation system described inSection 4, using ten-fold cross-validation.
For eachword, the table also shows the number of senses, thetotal number of examples, and two baselines: a sim-ple informed baseline that selects the most frequentsense by default,5 and a more refined baseline that5Note that this baseline assumes the availability of a sensetagged corpus in order to determine the most frequent sense ofa word.
The baseline is therefore ?informed,?
as compared to arandom, ?uninformed?
sense selection.baselines word senseword #s #ex MFS LeskC disambig.argument 2 114 70.17% 73.63% 89.47%arm 3 291 61.85% 69.31% 84.87%atmosphere 3 773 54.33% 56.62% 71.66%bank 3 1074 97.20% 97.20% 97.20%bar 10 1108 47.38% 68.09% 83.12%chair 3 194 67.57% 65.78% 80.92%channel 5 366 51.09% 52.50% 71.85%circuit 4 327 85.32% 85.62% 87.15%degree 7 849 58.77% 73.05% 85.98%difference 2 24 75.00% 75.00% 75.00%disc 3 73 52.05% 52.05% 71.23%dyke 2 76 77.63% 82.00% 89.47%fatigue 3 123 66.66% 70.00% 93.22%grip 3 34 44.11% 77.00% 70.58%image 2 84 69.04% 74.50% 80.28%material 3 223 95.51% 95.51% 95.51%mouth 2 409 94.00% 94.00% 95.35%nature 2 392 98.72% 98.72% 98.21%paper 5 895 96.98% 96.98% 96.98%party 3 764 68.06% 68.28% 75.91%performance 2 271 95.20% 95.20% 95.20%plan 3 83 77.10% 81.00% 81.92%post 5 33 54.54% 62.50% 51.51%restraint 2 9 77.77% 77.77% 77.77%sense 2 183 95.10% 95.10% 95.10%shelter 2 17 94.11% 94.11% 94.11%sort 2 11 81.81% 90.90% 90.90%source 3 78 55.12% 81.00% 92.30%spade 3 46 60.86% 81.50% 80.43%stress 3 565 53.27% 54.28% 86.37%AVERAGE 3.31 316 72.58% 78.02% 84.65%Table 2: Word sense disambiguation results, in-cluding two baselines (MFS = most frequent sense;LeskC = Lesk-corpus) and the word sense disam-biguation system.
Number of senses (#s) and num-ber of examples (#ex) are also indicated.implements the corpus-based version of the Lesk al-gorithm (Kilgarriff and Rosenzweig, 2000).6 DiscussionOverall, the Wikipedia-based sense annotationswere found reliable, leading to accurate sense classi-fiers with an average relative error rate reduction of44% compared to the most frequent sense baseline,and 30% compared to the Lesk-corpus baseline.There were a few exceptions to this general trend.For instance, for some of the words for which onlya small number of examples could be collected fromWikipedia, e.g.
restraint or shelter, no accuracy im-provement was observed compared to the most fre-quent sense baseline.
Similarly, several words in the2007678808284860.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9  1Classifier accuracyFraction of dataWord sense disambiguation learning curveFigure 1: Learning curve on the Wikipedia data set.data set have highly skewed sense distributions, suchas e.g.
bank, which has a total number of 1,074 ex-amples out of which 1,044 examples pertain to themeaning of financial institution, or the word mate-rial with 213 out of 223 examples annotated withthe meaning of substance.One aspect that is particularly relevant for any su-pervised system is the learning rate with respect tothe amount of available data.
To determine the learn-ing curve, we measured the disambiguation accu-racy under the assumption that only a fraction of thedata were available.
We ran ten fold cross-validationexperiments using 10%, 20%, ..., 100% of the data,and averaged the results over all the words in thedata set.
The resulting learning curve is plotted inFigure 1.
Overall, the curve indicates a continuouslygrowing accuracy with increasingly larger amountsof data.
Although the learning pace slows down aftera certain number of examples (about 50% of the datacurrently available), the general trend of the curveseems to indicate that more data is likely to lead toincreased accuracy.
Given that Wikipedia is growingat a fast pace, the curve suggests that the accuracy ofthe word sense classifiers built on this data is likelyto increase for future versions of Wikipedia.Another aspect we were interested in was the cor-relation in terms of sense coverage with respect toother sense annotated data currently available.
Forthe set of 30 nouns in our data set, we collectedall the word senses that were defined in either theWikipedia-based sense-tagged corpus or in the SEN-SEVAL corpus.
We then determined the percentagecovered by each sense with respect to the entire dataset available for a given ambiguous word.
For in-stance, the noun chair appears in Wikipedia withsenses #1 (68.0%), #2 (31.9%), and #4(0.1%), andin SENSEVAL with senses #1 (87.7%), #2 (6.3%),and #3 (6.0%).
The senses that do not appear are in-dicated with a 0% coverage.
The correlation is thenmeasured between the relative sense frequencies ofall the words in our dataset, as observed in the twocorpora.
Using the Pearson (r) correlation factor, wefound an overall correlation of r = 0.51 between thesense distributions in the Wikipedia corpus and theSENSEVAL corpus, which indicates a medium cor-relation.
This correlation is much lower than theone observed between the sense distributions in thetraining data and in the test data in the SENSEVALcorpus, which was measured at a high r = 0.95.This suggests that the sense coverage in Wikipediafollows a different distribution than in SENSEVAL,mainly reflecting the difference between the gen-res of the two corpora: an online collection of en-cyclopedic pages as available from Wikipedia, ver-sus the manually balanced British National Cor-pus used in SENSEVAL.
It also suggests that usingthe Wikipedia-based sense tagged corpus to disam-biguate words in the SENSEVAL data or viceversawould require a change in the distribution of sensesas previously done in (Agirre and Martinez, 2004).baselines word senseDataset #s #ex MFS LeskC disambig.SENSEVAL 4.60 226 51.53% 58.33% 68.13%WIKIPEDIA 3.31 316 72.58% 78.02% 84.65%Table 3: Average number of senses and exam-ples, most frequent sense and Lesk-corpus baselines,and word sense disambiguation performance on theSENSEVAL and WIKIPEDIA datasets.Table 3 shows the characteristics of the SEN-SEVAL and the WIKIPEDIA datasets for the nounslisted in Table 2.
The table also shows the mostfrequent sense baseline, the Lesk-corpus baseline,as well as the accuracy figures obtained on eachdataset using the word sense disambiguation systemdescribed in Section 4.66As a side note, the accuracy obtained by our system on theSENSEVAL data is comparable to that of the best participatingsystems.
Using the output of the best systems: the JHUR sys-tem on the SENSEVAL-2 words, and the HLTS3 system on the201Overall the sense distinctions identified inWikipedia are fewer and typically coarser than thosefound in WordNet.
As shown in Table 3, for theset of ambiguous words listed in Table 2, an aver-age of 4.6 senses were used in the SENSEVAL an-notations, as compared to about 3.3 senses per wordfound in Wikipedia.
This is partly due to a differ-ent sense coverage and distribution in the Wikipediadata set (e.g.
the meaning of ambiance for the am-biguous word atmosphere does not appear at all inthe Wikipedia corpus, although it has the highest fre-quency in the SENSEVAL data), and partly due to thecoarser sense distinctions made in Wikipedia (e.g.Wikipedia does not make the distinction between theact of grasping and the actual hold for the noun grip,and occurrences of both of these meanings are anno-tated with the label grip (handle)).There are also cases when Wikipedia makes dif-ferent or finer sense distinctions than WordNet.
Forinstance, there are several Wikipedia annotations forimage as copy, but this meaning is not even definedin WordNet.
Similarly, Wikipedia makes the distinc-tion between dance performance and theatre perfor-mance, but both these meanings are listed under onesingle entry in WordNet (performance as public pre-sentation).
However, since at this stage we are map-ping the Wikipedia annotations to WordNet, thesedifferences in sense granularity are diminished.7 Related WorkIn word sense disambiguation, the line of work mostclosely related to ours consists of methods trying toaddress the sense-tagged data bottleneck problem.A first set of methods consists of algorithms thatgenerate sense annotated data using words semanti-cally related to a given ambiguous word (Leacock etal., 1998; Mihalcea and Moldovan, 1999; Agirre andMartinez, 2004).
Related non-ambiguous words,such as monosemous words or phrases from dictio-nary definitions, are used to automatically collectexamples from the Web.
These examples are thenturned into sense-tagged data by replacing the non-ambiguous words with their ambiguous equivalents.Another approach proposed in the past is based onthe idea that an ambiguous word tends to have dif-SENSEVAL-3 words, an average accuracy of 71.31% was mea-sured (the output of the systems participating in SENSEVAL ispublicly available from http://www.senseval.org).ferent translations in a second language (Resnik andYarowsky, 1999).
Starting with a collection of paral-lel texts, sense annotations were generated either forone word at a time (Ng et al, 2003; Diab, 2004), orfor all words in unrestricted text (Diab and Resnik,2002), and in both cases the systems trained on thesedata were found to be competitive with other wordsense disambiguation systems.The lack of sense-tagged corpora can also be cir-cumvented using bootstrapping algorithms, whichstart with a few annotated seeds and iteratively gen-erate a large set of disambiguation patterns.
Thismethod, initially proposed by (Yarowsky, 1995),was successfully evaluated in the context of theSENSEVAL framework (Mihalcea, 2002).Finally, in an effort related to the Wikipedia col-lection process, (Chklovski and Mihalcea, 2002)have implemented the Open Mind Word Expert sys-tem for collecting sense annotations from volunteercontributors over the Web.
The data generated usingthis method was then used by the systems participat-ing in several of the SENSEVAL-3 tasks.Notably, the method we propose has several ad-vantages over these previous methods.
First, ourmethod relies exclusively on monolingual data, thusavoiding the possible constraints imposed by meth-ods that require parallel texts, which may be difficultto find.
Second, the Wikipedia-based annotationsfollow a natural Zipfian sense distribution, unlike theequal distributions typically obtained with the meth-ods that rely on the use of monosemous relativesor bootstrapping methods.
Finally, the grow paceof Wikipedia is much faster than other more task-focused and possibly less-engaging activities suchas Open Mind Word Expert, and therefore has thepotential to lead to significantly higher coverage.With respect to the use of Wikipedia as a re-source for natural language processing tasks, thework that is most closely related to ours is per-haps the name entity disambiguation algorithm pro-posed in (Bunescu and Pasca, 2006), where an SVMkernel is trained on the entries found in Wikipediafor ambiguous named entities.
Other language pro-cessing tasks with recently proposed solutions re-lying on Wikipedia are co-reference resolution us-ing Wikipedia-based measures of word similarity(Strube and Ponzetto, 2006), enhanced text classi-fication using encyclopedic knowledge (Gabrilovich202and Markovitch, 2006), and the construction of com-parable corpora using the multilingual editions ofWikipedia (Adafre and de Rijke, 2006).8 ConclusionsIn this paper, we described an approach for us-ing Wikipedia as a source of sense annotations forword sense disambiguation.
Starting with the hy-perlinks available in Wikipedia, we showed how wecan generate a sense annotated corpus that can beused to train accurate sense classifiers.
Through ex-periments performed on a subset of the SENSEVALwords, we showed that the Wikipedia sense annota-tions can be used to build a word sense disambigua-tion system leading to a relative error rate reductionof 30?44% as compared to simpler baselines.Despite some limitations inherent to this approach(definitions and annotations in Wikipedia are avail-able almost exclusively for nouns, word and sensedistributions are sometime skewed, the annotationlabels are occasionally inconsistent), these limi-tations are overcome by the clear advantage thatcomes with the use of Wikipedia: large sense taggeddata for a large number of words at virtually no cost.We believe that this approach is particularlypromising for two main reasons.
First, the size ofWikipedia is growing at a steady pace, which conse-quently means that the size of the sense tagged cor-pora that can be generated based on this resourceis also continuously growing.
While techniques forsupervised word sense disambiguation have been re-peatedly criticized in the past for their limited cover-age, mainly due to the associated sense-tagged databottleneck, Wikipedia seems a promising resourcethat could provide the much needed solution for thisproblem.
Second, Wikipedia editions are availablefor many languages (currently about 200), whichmeans that this method can be used to generate sensetagged corpora and build accurate word sense clas-sifiers for a large number of languages.ReferencesS.
F. Adafre and M. de Rijke.
2006.
Finding similar sentencesacross multiple languages in wikipedia.
In Proceedings ofthe EACL Workshop on New Text, Trento, Italy.E.
Agirre and D. Martinez.
2004.
Unsupervised word sensedisambiguation based on automatically retrieved examples:The importance of bias.
In Proceedings of EMNLP 2004,Barcelona, Spain, July.R.
Bunescu and M. Pasca.
2006.
Using encyclopedic knowl-edge for named entity disambiguation.
In Proceedings ofEACL 2006, Trento, Italy.T.
Chklovski and R. Mihalcea.
2002.
Building a sense taggedcorpus with Open Mind Word Expert.
In Proceedings of theACL 2002 Workshop on ?Word Sense Disambiguation: Re-cent Successes and Future Directions?, Philadelphia, July.M.
Diab and P. Resnik.
2002.
An unsupervised method forword sense tagging using parallel corpora.
In Proceedingsof ACL 2002, Philadelphia.M.
Diab.
2004.
Relieving the data acquisition bottleneck inword sense disambiguation.
In Proceedings of ACL 2004,Barcelona, Spain.E.
Gabrilovich and S. Markovitch.
2006.
Overcoming the brit-tleness bottleneck using wikipedia: Enhancing text catego-rization with encyclopedic knowledge.
In Proceedings ofAAAI 2006, Boston.M.
Galley and K. McKeown.
2003.
Improving word sensedisambiguation in lexical chaining.
In Proceedings of IJCAI2003, Acapulco, Mexico.A.
Kilgarriff and R. Rosenzweig.
2000.
Framework and re-sults for English SENSEVAL.
Computers and the Humani-ties, 34:15?48.C.
Leacock, M. Chodorow, and G.A.
Miller.
1998.
Using cor-pus statistics and WordNet relations for sense identification.Computational Linguistics, 24(1):147?165.Y.K.
Lee and H.T.
Ng.
2002.
An empirical evaluation of knowl-edge sources and learning algorithms for word sense disam-biguation.
In Proceedings of EMNLP 2002, Philadelphia.M.E.
Lesk.
1986.
Automatic sense disambiguation using ma-chine readable dictionaries: How to tell a pine cone from anice cream cone.
In Proceedings of the SIGDOC Conference1986, Toronto, June.R.
Mihalcea and D.I.
Moldovan.
1999.
An automatic methodfor generating sense tagged corpora.
In Proceedings of AAAI1999, Orlando.R.
Mihalcea.
2002.
Bootstrapping large sense tagged corpora.In Proceedings of LREC 2002, Canary Islands, Spain.R.
Navigli and P. Velardi.
2005.
Structural semantic intercon-nections: a knowledge-based approach to word sense dis-ambiguation.
IEEE Transactions on Pattern Analysis andMachine Intelligence (PAMI), 27.H.T.
Ng and H.B.
Lee.
1996.
Integrating multiple knowledgesources to disambiguate word sense: An examplar-based ap-proach.
In Proceedings of ACL 1996, New Mexico.H.T.
Ng, B. Wang, and Y.S.
Chan.
2003.
Exploiting paralleltexts for word sense disambiguation: An empirical study.
InProceedings of ACL 2003, Sapporo, Japan.T.
Pedersen.
2001.
A decision tree of bigrams is an accuratepredictor of word sense.
In Proceedings of NAACL 2001,Pittsburgh.P.
Resnik and D. Yarowsky.
1999.
Distinguishing sys-tems and distinguishing senses: new evaluation methods forword sense disambiguation.
Natural Language Engineering,5(2):113?134.M.
Strube and S. P. Ponzetto.
2006.
Wikirelate!
computingsemantic relatedeness using Wikipedia.
In Proceedings ofAAAI 2006, Boston.D.
Yarowsky.
1995.
Unsupervised word sense disambiguationrivaling supervised methods.
In Proceedings of ACL 1995,Cambridge.203
