Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1097?1107,Denver, Colorado, May 31 ?
June 5, 2015. c?2015 Association for Computational LinguisticsChinese Event Coreference Resolution: An Unsupervised ProbabilisticModel Rivaling Supervised ResolversChen Chen and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{yzcchen,vince}@hlt.utdallas.eduAbstractRecent work has successfully leveraged thesemantic information extracted from lexi-cal knowledge bases such as WordNet andFrameNet to improve English event corefer-ence resolvers.
The lack of comparable re-sources in other languages, however, has madethe design of high-performance non-Englishevent coreference resolvers, particularly thoseemploying unsupervised models, very diffi-cult.
We propose a generative model for theunder-studied task of Chinese event corefer-ence resolution that rivals its supervised coun-terparts in performance when evaluated on theACE 2005 corpus.1 IntroductionEvent coreference resolution is the task of deter-mining which event mentions in a text refer to thesame real-world event.
Compared to entity corefer-ence, event coreference is not only much less stud-ied, but it is arguably more challenging.
Recall thatfor two event mentions to be coreferent, both theirtriggers (i.e., the words realizing the occurrence ofthe events) and their corresponding arguments (e.g.,the times, places, and people involved in them) haveto be compatible.
However, identifying potential ar-guments (which is typically performed by an entityextraction system), linking arguments to their eventmentions (which is typically performed by an eventextraction system), and determining the compatibil-ity between two event arguments (which is the jobof an entity coreference resolver), are all non-trivialtasks.
In other words, end-to-end event coreferenceresolution is complicated in part by the fact that anevent coreference resolver has to rely on the noisyoutputs produced by its upstream components in thestandard information extraction (IE) pipeline.In this paper, we examine Chinese event coref-erence resolution.
While English event coreferenceis under-investigated, Chinese event coreference ismuch less studied than English event coreference.In terms of task definition, there is no difference be-tween English and Chinese event coreference.
How-ever, the design of high-performance Chinese eventcoreference resolvers is complicated in part by thelack of large-scale lexical knowledge bases.
Re-cent work by Bejan and Harabagiu (2010; 2014) hasshown that the semantic information extracted fromWordNet (Fellbaum, 1998) and FrameNet (Bakeret al, 1998) significantly contributed to the perfor-mance of their English event coreference resolver.While the lack of comparable lexical knowledgebases in Chinese can be mitigated in part by the useof event coreference annotated data, we focus on achallenging version of the task --- unsupervised Chi-nese event coreference resolution.
Specifically, ourgoal is to learn an event coreference model withoutusing data annotated with event coreference links.When evaluated on the Chinese portion of the ACE2005 corpus, our unsupervised probabilistic modelfor event coreference resolution rivals its state-of-the-art supervised counterpart in performance.
This,together with the fact that its underlying genera-tive process is not language-dependent and does notrely on features extracted from lexical knowledgebases, potentially enables it to be applied to lan-guages where neither annotated data nor large-scale1097knowledge bases are available.Another feature of our model that deserves men-tion is that it performs joint event coreference res-olution and anaphoricity determination.
Anaphoric-ity determination, the task of determining whethera mention is anaphoric and hence needs to be re-solved, is an issue common to both entity and eventcoreference resolution.
However, determining theanaphoricity of an event mention is arguably moredifficult than determining the anaphoricity of a pro-noun.
The reason is that while there exist lexical andsyntactic cues that can be used to reliably identifypleonastic pronouns (Bergsma and Yarowsky, 2011),the lack of such cues in event mentions makes theidentification of anaphoric event mentions challeng-ing even in a supervised manner, let alne in an un-supervised manner.
Note that ignoring anaphoric-ity determination and having our model attempt toresolve every event mention is not a viable option,as only 24.4% of the Chinese event mentions in ourevaluation corpus (ACE 2005) are anaphoric.
Ourdecision to jointly model anaphoricity determinationand event coreference resolution was inspired by thedifficulty of designing a standalone system for deter-mining the anaphoricity of event mentions.2 Related WorkAlmost all existing approaches to event coreferenceare developed for English.
These approaches canbroadly be divided into three categories.Within-document coreference is the most pop-ularly investigated and arguably the most importantevent coreference task.
While early work in MUC(e.g., Humphreys et al (1997)) is limited to sev-eral scenarios, ACE takes a further step towards pro-cessing more fine-grained events.
Most ACE eventcoreference resolvers are supervised, training a pair-wise model to determine whether two event men-tions are coreferent (e.g., Ahn (2006)).Improvements to this standard approach includethe use of feature weighting to train a better model(McConky et al, 2012), and graph-based cluster-ing algorithms to produce event coreference clusters(Chen and Ji, 2009; Sangeetha and Arock, 2012).Chen et al (2011) train multiple classifiers to han-dle coreference between event mentions of differentsyntactic types (e.g., verb-noun coreference, noun-noun coreference) on the OntoNotes corpus (Prad-han et al, 2007).
However, OntoNotes is only par-tially annotated with event coreference links, andChen et al further make the simplifying assumptionthat event coreference chains are all and only thosecoreference chains that involve at least one verb.More recently, Cybulska and Vossen (2012) andGoyal et al (2013) have performed event corefer-ence using semantic relations (e.g., hyponymy rela-tions extracted fromWordNet) and distributional se-mantic information, respectively, on the IntelligenceCommunity (IC) corpus (Hovy et al, 2013).
The ICcorpus, which at the time of writing is not yet pub-licly available, is different from the MUC and ACEcorpora in that it is annotated with not only full eventcoreference relations but also partial event corefer-ence relations.
Partial coreference is a term coinedby Hovy et al to refer to event relations that exhibitsubtle deviation from the perfect identity of events(e.g., the subset relation, the membership relation).While all of the aforementioned work addresses thefull event coreference task, a two-stage approach isrecently proposed by Araki et al (2014) to identifysubevent relations from the IC corpus.Cross-document coreference is first investigatedby Bagga and Baldwin (1999), who represent aneventmention as a vector of its context words and de-termine whether two event mentions are coreferentbased on the cosine similarity of their vectors.
Be-jan and Harabagiu (2010; 2014) and Lee et al (2012)propose nonparametric models and a joint entity andevent coreference model respectively for within- andcross-document event coreference, evaluating theirmodels on the ECB corpus.
However, ECB "is an-notated mainly for cross-document coreference" andmany difficult cases of within-document coreferenceare not annotated (Liu et al, 2014).Naughton (2009) and Elkhlifi and Faiz (2009)have worked on sentence-level event coreference,where the goal is to determinewhether two sentencescontaining event mentions are coreferent.
Some-what unfortunately, simplifying assumptions have tobe made when a sentence containing multiple non-coreferent event mentions is encountered.Compared to English event coreference, there hasbeen much less work on Chinese event coreference.SinoCoreferencer (Chen and Ng, 2014), a publicly-available ACE-style within-document event corefer-1098ence resolver for Chinese that achieves state-of-the-art results, employs a supervised approach where aclassifier is trained to determine whether two eventmentions are coreferent.
We will compare our unsu-pervised model against this supervised resolver.3 ACE Event CoreferenceIn this section, we overview the ACE 2005 eventcoreference task, which is the version of the within-document event coreference task we focus on.The ACE 2005 event coreference task requiresthat an event coreference resolver perform corefer-ence on event mentions belonging to one of the ACEevent types.
More specifically, an event mention iscomposed of a trigger (i.e., the word realizing theevent's occurrence) and a set of arguments (i.e., theevent's participants).
Each event trigger has a typeand a subtype.
In ACE 2005, eight event types aredefined, which are further subcategorized into 33subtypes.
Each event argument has a semantic role.In ACE 2005, a set of argument roles is defined foreach event type.
That is, an event's type determineswhat roles its mentions' arguments can assume.
Notsurprisingly, two event mentions cannot be corefer-ent if their triggers have different subtypes or theyhave incompatible arguments (e.g., their dates or lo-cations are different).To better understand the ACE 2005 event corefer-ence task, consider the sentence in Figure 1, whichis taken from the ACE 2005 corpus.
This examplecontains three event mentions belonging to the ACEevent types.
Specifically, these three mentions aretriggered by the words ?
(leaving), ??
(assassi-nated) and??
(attack).
??
and??
have typeLife and subtype Die, whereas?
has type Move-ment and subtype Transport.
Note that ??and??
refer to the same real-world event and aretherefore coreferent.4 The Generative ModelIn this section, we present our generative model.4.1 NotationWe begin by introducing the notation that we use inthe rest of this paper.
We denote e to be the currentevent mention to be resolved (henceforth the activeevent mention).
C, the set of candidate antecedents???????????????
[?]?????[??]???
[??]??????????
?Shameri and his son were [assassinated] during morningrush hour when [leaving] home.
This [attack] once againdemonstrated the insurgents' ability.Figure 1: An excerpt from a Chinese document in theACE 2005 corpus with the corresponding English trans-lation.
The event mentions are bracketed.of e, contains all the event mentions preceding e inthe associated text as well as a dummy candidate an-tecedent d (to which e will be resolved if it is non-anaphoric).
Also, we define k to be the context sur-rounding e as well as every candidate antecedent c inC, and kcto be the context surrounding e and can-didate antecedent c. Moreover, we define l to be abinary variable indicating whether c is the correctantecedent of e. Finally, etand ctdenote e and c'srespective trigger words.4.2 TrainingOur model estimates P (e, k, c, l), the probability ofseeing (1) the active event mention e; (2) the con-text k surrounding e and its candidate antecedents;(3) a candidate antecedent c of e; and (4) l, a binaryvalue indicating whether c is e's correct antecedent.Since we estimate this probability from a raw, unan-notated corpus, we are effectively treating e, k, andc as observed data and l as hidden data.Owing to the presence of hidden data, we esti-mate the model parameters using the Expectation-Maximization (EM) algorithm (Dempster et al,1977).
Specifically, we use EM to iteratively esti-mate the model parameters from data in which eachevent mention is labeled with the probability that itcorefers with each of its candidate antecedents, andapply the resulting model to relabel each event men-tion with the probability that it corefers with each ofits candidate antecedents.
Below we describe the de-tails of the E-step and the M-step.4.2.1 E-StepThe goal of the E-step is to compute P (l=1|e, k, c),the probability that a candidate antecedent c is thecorrect antecedent of e given context k. Assumingthat exactly one of the e's candidate antecedents is1099its correct antecedent, we can rewrite P (l=1|e, k, c)as follows:P (l=1|e, k, c) =P (e, k, c, l=1)?c?
?CP (e, k, c?, l=1)(1)As we can see from Equation (1), to computeP (l=1|e, k, c), we need to compute P (e, k, c, l=1),which can be rewritten using Chain Rule:P (e, k, c, l=1) = P (e|k, c, l=1) ?
P (l=1|k, c)?
P (c|k) ?
P (k)(2)Next, given l = 1 (i.e., c is the antecedent of e),we assume that we can generate e from c withoutlooking at the context.
Using this assumption andapproximating e and c by their trigger words, we canrewrite P (e|k, c, l=1) as follows:P (e|k, c, l=1) ?
P (et|ct, l=1) (3)Moreover, we assume that (1) given e and c's con-text, the probability of c being the antecedent of eis not affected by the context of the other candidateantecedents; and (2) kcis sufficient for determiningwhether c is the antecedent of e. So,P (l=1|k, c) ?
P (l=1|kc, c) ?
P (l=1|kc) (4)Next, applying Bayes Rule to P (l=1|kc), we get:P (kc|l=1)P (l=1)P (kc|l=1)P (l=1) + P (kc|l=0)P (l=0)(5)Representing kcas a set of n features f1c, .
.
.
fncand assuming that each f icis conditionally indepen-dent given l, we can approximate Expression (5) as:?iP (fic|l=1)P (l=1)?iP (fic|l=1)P (l=1) +?iP (fic|l=0)P (l=0)(6)Given Equations (2), (3), (4) and (6), we canrewrite P (l=1|e, k, c) as follows:P (l=1|e, k, c) =P (e, k, c, l=1)?c?
?CP (e, k, c?, l=1)?P (et|ct, l=1) ?
?iP (fic|l=1)Zc?
P (c|k)?c?
?CP (et|c?t, l=1) ?
?iP (fic?|l=1)Zc??
P (c?|k)(7)whereZx=?iP (fix|l=1)P (l=1)+?iP (fix|l=0)P (l=0)(8)As we can see from Equation (7), our model hasfour groups of parameters, namely P (et|ct, l=1),P (fic=1|l), P (l) andP (c|k).
With these four groupsof parameters, we can apply Equation (7) to effi-ciently compute P (l=1|e, k, c).Two points deserve mention before we describeour M-step.
First, among the four groups of param-eters, P (et|ct, l=1) and P (f ic|l) are estimated in theM-step described below; P (l) is estimated in param-eter initialization and used throughout the EM itera-tions (details on parameter initialization appear afterthe M-step); and P (c|k) is computed heuristically.Intuitively, P (c|k) is the prior probability of a can-didate antecedent c given context k. The simplestway to model P (c|k) is to assume that every candi-date antecedent is equally likely given the context.
Inpractice, however, some candidate antecedents areimplausible given the context.
To identify such can-didate antecedents, we employ a simple heuristic,which considers a candidate antecedent implausibleif its event subtype is different from that of e. Con-sequently, we model P (c|k) as follows: if c is im-plausible, we set P (c|k) to 0 and distribute the prob-ability mass uniformly over all and only the plausi-ble candidate antecedents.
Since this heuristic is notapplicable to dummy candidates, we assume for sim-plicity that they are all plausible.Second, by including d as a dummy candidate an-tecedent for each e, we model anaphoricity determi-nation and event coreference in a joint fashion.
If themodel resolves e to d, it means that the model positse as non-anaphoric; on the other hand, if the modelresolves e to a non-dummy candidate antecedent c,it means that the model posits e as anaphoric andc as e's correct antecedent.
This joint modelingmethod has proven effective in earlier work on su-pervised entity coreference resolution (e.g., Rahmanand Ng (2009; 2011)).4.2.2 M-StepGiven P (l=1|e, k, c), the goal of the M-step isto (re)estimate two of the four groups of param-eters mentioned above, namely P (et|ct, l=1) andP (fic|l), using maximum likelihood estimation.1100Specifically, P (et|ct, l=1) is estimated as fol-lows:P (et|ct, l=1) =Count(et, ct, l=1) + ?Count(ct, l=1) + ?
?
|t|(9)where Count(ct, l=1) is the expected number oftimes c has trigger word ctwhen it is the antecedentof an event mention; and |t| is the number of possibletrigger words in the training data (we treat the "trig-ger word" of a dummy candidate antecedent as anunseen word).
Also, ?
is the Laplace smoothing pa-rameter, which we set to 1, andCount(et, ct, l=1) isthe expected number of times e has etas its triggerwhen its antecedent c has trigger ct.
Given triggerwords e?tand c?t, we compute Count(e?t, c?t, l=1) asfollows:Count(e?t, c?t, l=1) =?e,c:et=e?t,ct=c?tP (l=1|e, k, c)(10)The remaining group of parameters, P (f ic|l), canbe estimated in a similar fashion.To start the induction process, we initialize allparameters with uniform values.
Specifically,P (et|ct, l=1) is set to 1|t|, and P (l=1|kc) is set to0.5.
As noted before, P (l) is also initialized hereand used throughout the EM iterations.
Recall thatP (l=1) is the fraction of event pairs that are corefer-ent.
Since we assumed earlier that each event men-tion has exactly one (dummy or non-dummy) an-tecedent, P (l=1) can be computed as the number ofevent mentions divided by the total number of eventpairs.
After initialization, we iteratively run the E-step and the M-step until convergence.There is an important question we have not ad-dressed: what features f icshould we use to representcontext kc, which we need to estimate P (f ic|l)?
Wedefer the discussion of this question to Section 5.4.3 InferenceAfter training, we can apply the resulting model toresolve event mentions.
Given an event mention e,we determine its antecedent as follows:c?
= argmaxc?CP (l=1|e, k, c) (11)where C is the set of candidate antecedents of e. Inother words, we apply Equation (11) to each of e'scandidate antecedents, and select the one that yieldsthe largest probability.
If c is a non-dummy candi-date antecedent, we posit c as the antecedent of e;otherwise, we posit e as non-anaphoric.5 Context FeaturesAs mentioned at the end of Section 4.2.2, to fullyspecify our model, we need to describe the featuresficused to represent kc, which is needed to com-pute P (f ic|l).
Recall that kcencodes the contextsurrounding candidate antecedent c and active eventmention e. We represent kcusing six features that en-code the relationship between c and e, some of whichare motivated by previous work on supervised eventcoreference resolution (e.g., Chen and Ji (2009)).Below we describe these six features, which can bebroadly divided into three categories.5.1 Trigger-Based FeaturesWe employ two trigger-based features (Features 1and 2), both of which are binary-valued and are com-puted based on e's and c's triggers.Feature 1 encodes whether ctand et, the triggerwords of c and e, satisfy any of the following threeconditions:1. ctand etare lexically identical;2. ctand etcontain the same basic verb (BV) andtheir verb structures are compatible;3. the similarity between ctand etis greater thana certain threshold (which we set to 0.8 in ourexperiments).Intuitively, Feature 1 is a recall-enhancing feature:it encodes a condition whose satisfaction can helpdiscover many event coreference links.
However, itis not designed to be precision-oriented, as it is com-puted based solely on the triggers and not their sur-rounding contexts.
Below we explain conditions 2and 3 in more detail.Recall that condition 2 encodes our observationthat an event coreference relation may exist betweentwo non-identical trigger words having the same BVif their verb structures are compatible.
To under-stand this condition, let us explain the notion of BVsand how we determine the compatibility of two verbstructures.
A BV is a single-character Chinese verb,which is the building block of all Chinese verbs.1101Specifically, Li et al (2012) observe that, with a fewexceptions, a Chinese verb constructed out of a ba-sic verb bv possesses one of six main verb struc-tures: (1) bv (e.g., ?
(arrest)); (2) bv + verb (e.g.,??
(deliver), where bv is?
); (3) verb + bv (e.g.,??
(leave), where bv is?
); (4) bv + complemen-tation (e.g., ??
(enter), where bv is ?
); (5) bv +noun/adjective (e.g., ??
(shoot), where bv is?
);(6) noun/adjective + bv (e.g., ??
(slight wound),where bv is ?).
Now, assuming that t1and t2aretwo lexically different trigger words containing thesame BV (bv), we say that their verb structures (de-noted as vs1and vs2) are incompatible if one of thefollowing conditions is satisfied: (1) bv appears indifferent positions in t1and t2(e.g., ??
(shoot)and??
(leave), where bv is?
); (2) both vs1andvs2have bv + verb or verb + bv as their verb struc-ture (e.g.,??
(deliver) and??
(reach), where bvis?
); or (3) both vs1and vs2have noun/adjective+ bv or bv + noun/adjective as their verb structure(e.g.,??
(slight wound) and??
(severe wound),where bv is?).
Note that these three incompatibil-ity conditions encode our commonsense knowledgeof when two Chinese verbs having the same BV can-not have the same meaning.Next, we explain how we compute the similar-ity between two trigger words in condition 3.
Tocapture their semantic similarity, we first applyword2vec (Mikolov et al, 2013) to the Chinese Gi-gaword corpus (Parker et al, 2009) to obtain a vec-tor representation of each word and then compute thecosine similarity between the two word vectors.Feature 2, our second trigger-based feature, en-codes whether two nominal event mentions are in-compatible w.r.t.
number.
Specifically, its value isTrue if and only if (1) c and e are both nouns, and (2)one is singular and the other is plural.
Intuitively,this feature encodes a non-coreference condition.5.2 Argument-Based FeaturesWe employ three argument-based features (Fea-tures 3?5), all of which are binary-valued and arecomputed based on c's and e's arguments.Feature 3 encodes whether c and e possess two ar-guments that have the same semantic role but dif-ferent semantic classes.1 Intuitively, Feature 3 en-1The possible semantic classes are the ACE 2005 entitycodes a non-coreference condition: c and e cannotbe coreferent if such arguments exist.Feature 4 can be viewed as a generalized versionof Feature 3, encoding whether c and e possess twoarguments that have the same semantic role but arenot coreferent.Feature 5 encodes whether c and e possess twonamed entity (NE) arguments that both have Valueas their NE type but are lexically different.
Suchevent mentions have a good chance of being notcoreferent.5.3 Distance FeatureWe employ one distance feature (Feature 6) that en-codes how far c and e are apart from each other interms of the number of event mentions.
To reducedata sparseness during parameter estimation, how-ever, we quantize the distance as follows.
Let d bethe distance between the first event mention and thelast event mention in the document for which the dis-tance feature will be computed.
Note that the dis-tance between an arbitrary pair of event mentions inthis document will be between 0 and d. We dividethe interval [0,d] into four equal-sized regions, andset the value of the distance feature based on whichof the four bins it falls into.5.4 Features for Dummy CandidatesNow that we can compute the aforementioned sixfeatures for a non-dummy candidate antecedent, wenext specify how we compute these features for adummy candidate antecedent d of active event men-tion e. For Feature 1, we set the feature value of dto True, whereas for Features 2?5, we set the fea-ture value of d to False.
To understand why thesevalues are chosen, note that for each of these fea-tures the opposite value could be a strong indicatorof non-coreference, potentially causing the model tohave an overly strong bias against selecting d as theantecedent of e.Finally, to compute Feature 6, we assume that dis the zero-th event mention of the associated docu-ment, and then compute the distance feature in thesame way as described above.
By letting d be thezero-th event mention, we make the probability ofpicking d as the correct antecedent (the probability oftypes, i.e., Person,Organization,GPE, Facility, and Lo-cation.1102classifying e as non-anaphoric) depend on e's posi-tion in the associated text.
This makes sense becausein general, the probability of e being non-anaphorictends to be larger (smaller) when it appears earlier(later) in the document.6 Evaluation6.1 Experimental SetupDataset.
For evaluation, we conduct five-foldcross-validation experiments on the 633 Chinesedocuments of the ACE 2005 training corpus.
Statis-tics on the corpus are shown in Table 1.Evaluation measures.
We report results in termsof recall (R), precision (P), and F-score (F) using thecommonly-used coreference evaluation measuresgiven by the CoNLL scorer, namely the link-basedMUC scorer (Vilain et al, 1995), the mention-basedB3 scorer (Bagga and Baldwin, 1998), the entity-based version of the CEAF scorer (Luo, 2005), andthe Rand index-based BLANC scorer (Recasens andHovy, 2011), after singleton event mentions are re-moved from the coreference partitions produced byour resolver.
We use the latest version (version 8)of the CoNLL scorer2, which fixes a bug in pre-vious versions (Pradhan et al, 2014).
In addition,we report the CoNLL score (Pradhan et al, 2011),which is the unweighted average of the MUC, B3,and CEAF F-scores.Evaluation setting.
We perform an end-to-endevaluation, as it can more accurately reflect the per-formance of an event coreference resolver when it isused in practice.More specifically, to extract the event mentionsused in our evaluation, we employ SinoCorefer-encer3, which, as mentioned before, is an end-to-endACE-style Chinese IE system that achieves state-of-the-art event coreference results.
Specifically, theevent triggers needed to compute the trigger-basedcontext features are extracted using SinoCorefer-encer's event extraction subsystem.
The event sub-types needed to identify and filter out implausiblecandidate antecedents are also provided by its eventextraction subsystem.
The event arguments neededto compute the argument-based context features are2conll.github.io/reference-coreference-scorers/3Downloadable from http://www.hlt.utdallas.edu/~yzcchen/coreference/Documents 633Sentences 9,967Event mentions 3,333Event coreference chains 2,521Table 1: Statistics on the ACE 2005 Chinese corpus.first extracted and typed by its entity extraction sub-system, and then linked to their triggers by its eventextraction subsystem.
Finally, the entity coreferencelinks and the semantic roles needed to compute Fea-ture 4 are provided by its entity coreference sub-system and its event extraction subsystem, respec-tively.4 Details of each of these subsystems can befound in Chen and Ng (2014).6.2 ResultsWe employ two supervised resolvers as baseline sys-tems.
The first baseline employs rote learning, sim-ply positing two event mentions as coreferent if theircorresponding triggers are annotated as coreferent inthe training data.
The second baseline is SinoCoref-erencer, which has produced the best Chinese eventcoreference results to date on the ACE corpus.Row 1 of Table 2 shows the results of the base-line that employs rote learning.
As we can see, thisbaseline achieves a CoNLL score of 37.9.
Row 2shows the results of SinoCoreferencer.
It performssignificantly better than the rote-learning baselinew.r.t.
all five scoring measures5, achieving a CoNLLscore of 39.2.
Finally, row 3 shows the results of ourmodel.
Despite being unsupervised, it significantlyoutperforms the better baseline, SinoCoreferencer,w.r.t.
all five scoring measures, achieving a CoNLLscore of 41.5, which is 2.3 points higher than that ofSinoCoreferencer.
These results suggest that a gen-erative approach to unsupervised event coreferenceholds promise.6.3 Ablation ExperimentsRecall that in our model eight probability terms playa major role: P (et|ct), P (c|k), and P (f ic|l) for each4We employ only those semantic roles that can be reliablydetermined by SinoCoreferencer's event extraction subsystem,namely, Agent, Adjudicator, Defendant, Giver, Per-son, Place, Position, Organization, Origin, and Re-cipient.5All significance tests are paired t-tests, with p < 0.05.1103MUC B3 CEAFeBLANC CoNLLSystem R P F R P F R P F R P F FRote learning 42.6 36.4 39.3 41.4 32.3 36.3 37.0 39.7 38.3 27.4 20.0 23.1 37.9SinoCoreferencer 42.7 38.3 40.4 41.5 34.7 37.8 39.9 39.2 39.5 28.1 23.7 25.7 39.2Our model 43.1 42.4 42.8 41.4 39.1 40.2 40.7 42.6 41.6 27.5 26.4 26.9 41.5Table 2: Five-fold cross-validation event coreference results on the ACE 2005 corpus.of the six context features.
To investigate the con-tribution of each probability term to overall perfor-mance, we conduct ablation experiments.
Specifi-cally, in each ablation experiment, we remove ex-actly one term from the model and retrain it.Ablation results are shown in Table 3.
Each rowcontains the F-scores obtained via the five evaluationmeasures.
To facilitate comparison, the scores of themodel in which all eight probability terms are used isshown in row 1.
As we can see, Feature 1 is the mostuseful feature: its removal causes the CoNLL scoreto drop significantly by 5.3 points.
A closer exam-ination reveals that the drop in the CoNLL score iscaused by a significant drop in recall w.r.t.
all scor-ers.
Recall that this feature encodes the conditionsunder which two triggers are likely to be coreferent.It is perhaps not surprising that its removal causes asignificant drop in recall.The second most useful feature is P (c|k), whichplaces zero probability mass on candidate an-tecedents whose event subtypes are different fromthat of the active event mention.
Its removal causesthe CoNLL score to drop significantly by 1.6 points.The removal of each other feature resulted in a small,insignificant drop in the CoNLL score.6.4 Error AnalysisIt is somewhat surprising that our unsupervised eventcoreference model outperforms the better supervisedbaseline, SinoCoreferencer.
To understand why, weanalyze the errors made by the two resolvers.Our analysis proceeds as follows.
First, to gain in-sights into the differences between the two resolvers,we examine those candidate event mentions that arecorrectly handled by one model but not the other(Section 6.4.1).
Specifically, we consider a candi-date event mention e correctly handled if (1) e is acorrectly resolved anaphoric event mention; (2) e isan unresolved singleton event mention; or (3) e is anunresolved non-event mention (i.e., not a true eventSystem MUC B3 CEAFeBLANCCoNLLFull model 42.8 40.2 41.6 26.9 41.5?
P (et|ct) 42.9 39.8 40.9 26.9 41.2?
P (c|k) 41.2 38.6 39.8 24.9 39.9?
Feature 1 37.5 32.9 38.2 20.8 36.2?
Feature 2 42.5 39.9 41.4 26.6 41.3?
Feature 3 42.4 40.0 41.3 26.9 41.2?
Feature 4 42.5 40.1 41.7 27.0 41.4?
Feature 5 42.4 40.0 41.4 26.5 41.3?
Feature 6 42.3 39.6 40.9 26.8 40.9Table 3: Ablation results in terms of F-scores.mention).
Second, to understand how to improveevent coreference, we identify the major sources oferror made by both resolvers (Section 6.4.2).6.4.1 Common Sources of DisagreementThere are 323 candidate event mentions that are cor-rectly handled by one model but not the other in ourdataset.
Among these 323 cases, 205 (50 anaphoric +79 singletons + 76 non-event) are correctly handledby the unsupervised model, and 118 (42 anaphoric +52 singletons + 24 non-event) are correctly handledby SinoCoreferencer.From these numbers, we can see that the unsuper-vised model performs far better than SinoCorefer-encer in not resolving the singletons and the non-event mentions.
This is perhaps not surprisinggiven the unsupervised model's relatively stricterconditions on resolving a candidate event mention.Specifically, it is unlikely to posit two candidateevent mentions as coreferent unless (1) their trig-gers have a BV match or a large word2vec similar-ity value and (2) none of the non-coreference condi-tions are satisfied.
Overall, these results explain whythe unsupervised model has a much higher precisionthan SinoCoreferencer.Not only does the unsupervised model performmuch better in not resolving singletons and non-event mentions, but it is also slightly more accurate1104in resolving the anaphoric event mentions, which ul-timately enables it to achieve a higher recall thanSinoCoreferencer.
In particular, it correctly resolves50 anaphoric mentions that are incorrectly handledby SinoCoreferencer.
The successful resolution ofthese anaphoric mentions can be attributed largely toits use of BV and word2vec, neither of which is ex-ploited by SinoCoreferencer.
However, while a BVmatch or a high word2vec similarity value is a goodindicator of event coreference, they are by no meansperfect.
This partly explains why there are singletonsand non-event mentions that are correctly handled bySinoCoreferencer but not the unsupervised model.Despite the fact that SinoCoreferencer slightlylags behind the unsupervised model in resolv-ing anaphoric mentions, it correctly resolves 42anaphoric event mentions that are incorrectly han-dled by the unsupervised model.
These are casesthat cannot be handled simply by relying on BVmatch or word2vec similarity.
More specifically,two of the unique features of SinoCoreferencer areprimarily responsible for its successful resolution ofthese event mentions.
First, it learns coreferent trig-ger pairs from the training data.
These pairs provedto be useful for event coreference, as we saw fromthe competitive results provided by the rote-learningbaseline.
Second, unlike the unsupervised model,SinoCoreferencer can posit two event mentions ascoreferent without considering their triggers.
Morespecifically, SinoCoreferencer may posit two eventmentions as coreferent if the corresponding argu-ments of the two event mentions (i.e., argumentshaving the same role) are coreferent.
Neither of thesetwo recall-enhancing features of SinoCoreferenceris a precise indicator of event coreference.
In otherwords, employing themwidens the precision gap be-tween the two resolvers.6.4.2 Common Sources of ErrorNext, we discuss the major sources of error madeby both our unsupervised model and SinoCorefer-encer.
Broadly, the errors can be divided into twocategories, precision errors and recall errors.Precision errors arise primarily from erroneouscoreference links established between (1) one ormore candidate eventmentions that are not true eventmentions; (2) two event mentions with incompati-ble latent attributes such as Modality, Polarity,Genericity, and Tense, since these attributes arenot exploited by the two resolvers; (3) two eventmentions with incompatible arguments, since thesearguments fail to be extracted by the argument iden-tification component; (4) two mentions representingevents that occur at different times, since the eventmentions are not timestamped6; and (5) two eventmentions whose corresponding arguments are incor-rectly posited by the entity coreference subsystem ascoreferent.On the other hand, recall errors arise primarilyfrom missing coreference links attributed to (1) thetrigger identification component's failure to detectone or both of the triggers involved in an event coref-erence link; (2) the entity coreference subsystem'sfailure to establish the link(s) between the corre-sponding arguments of two coreferent event men-tions; (3) the lack of positive evidence of event coref-erence, such as BV match, high word2vec similar-ity, and coreferent arguments; and (4) the argumentidentification component's failure to extract one ormore arguments of an event mention.7 ConclusionsWe presented a generative model for the rela-tively under-studied task of unsupervised Chineseevent coreference resolution whose parameters werelearned using EM from an unannotated corpus.When evaluated on the ACE 2005 corpus, our modelsignificantly outperforms SinoCoreferencer, a state-of-the-art Chinese event coreference resolver.Since the performance of our resolver is limited inpart by the errors made by SinoCoreferencer's sub-systems, we plan to mitigate this problem by per-forming joint inference for entity coreference, eventextraction and event coreference in future work.AcknowledgmentsWe thank the three anonymous reviewers for theirdetailed and insightful comments on an earlier draftof this paper.
This work was supported in part byNSF Grants IIS-1147644 and IIS-1219142.6Not all of these errors can be fixed by exploiting the Tenseattribute, as Tense is only a crude approximation of time.
Forinstance, in the phrases ????
(arrested for the first time)and ????
(arrested again), the two occurrences of ??
(arrested) are associated with different timestamps despite thefact that they have the same Tense.1105ReferencesDavid Ahn.
2006.
The stages of event extraction.
InProceedings of the Workshop on Annotating and Rea-soning about Time and Events, pages 1--8.Jun Araki, Zhengzhong Liu, Eduard Hovy, and TerukoMitamura.
2014.
Detecting subevent structurefor event coreference resolution.
In Proceedings ofthe Ninth International Conference on Language Re-sources and Evaluation, pages 4553--4558.Amit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In Proceedings of theLinguistic Coreference Workshop at the First Interna-tional Conference on Language Resources and Evalu-ation, page 563--566.Amit Bagga and Breck Baldwin.
1999.
Cross-documentevent coreference: Annotation, experiments, and ob-servations.
In Proceedings of the ACL Workshop onCoreference and Its Applications, pages 1--9.Collin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceed-ings of the 36th Annual Meeting of the Association forComputational Linguistics and the 17th InternationalConference on Computational Linguistics, Volume 1,pages 86--90.Cosmin Bejan and Sanda Harabagiu.
2010.
Unsuper-vised event coreference resolution with rich linguisticfeatures.
In Proceedings of the 48th Annual Meeting ofthe Association for Computational Linguistics, pages1412--1422.Cosmin Bejan and Sanda Harabagiu.
2014.
Unsuper-vised event coreference resolution.
ComputationalLinguistics, 40(2):311--347.Shane Bergsma and David Yarowsky.
2011.
NADA:A robust system for non-referential pronoun detection.In Proceedings of the 8th Discourse Anaphora andAnaphor Resolution Colloquium, pages 12--23.Zheng Chen and Heng Ji.
2009.
Graph-based eventcoreference resolution.
In Proceedings of the 2009Workshop on Graph-based Methods for Natural Lan-guage Processing (TextGraphs-4), pages 54--57.Chen Chen andVincent Ng.
2014.
SinoCoreferencer: Anend-to-end Chinese event coreference resolver.
InPro-ceedings of the Ninth International Conference on Lan-guage Resources and Evaluation, pages 4532--4538.Bin Chen, Jian Su, Sinno Jialin Pan, and Chew Lim Tan.2011.
A unified event coreference resolution by inte-grating multiple resolvers.
In Proceedings of the 5thInternational Joint Conference on Natura LanguageProcessing, pages 102--110.Agata Cybulska and Piek Vossen.
2012.
Using se-mantic relations to solve event coreference in text.In Proceedings of the LREC Workshop on SemanticRelations-II Enhancing Resources and Applications(SemRel 2012), pages 60--67.Arthur P. Dempster, Nan M. Laird, and Donald B. Ru-bin.
1977.
Maximum likelihood from incomplete datavia the EM algorithm.
Journal of the Royal StatisticalSociety.
Series B (Methodological), 39:1--38.Aymen Elkhlifi and Rim Faiz.
2009.
Automatic anno-tation approach of events in news articles.
Interna-tional Journal of Computing and Information Sciences,7(1):40--50.Christiane Fellbaum.
1998.
WordNet: An Electronic Lex-ical Database.
MIT Press, Cambridge, MA.Kartik Goyal, Sujay Kumar Jauhar, Huiying Li, Mrin-maya Sachan, Shashank Srivastava, and Eduard Hovy.2013.
A structured distributional semantic model forevent co-reference.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Linguis-tics: Volume 2 (Short Papers), pages 467--473.Eduard Hovy, Teruko Mitamura, Felisa Verdejo, JunAraki, and Andrew Philpot.
2013.
Events are notsimple: Identity, non-identity, and quasi-identity.
InProceedings of the NAACL-HLT Workshop on Events:Definition, Detection, Coreference, and Representa-tion, pages 21--28.Kevin Humphreys, Robert Gaizauskas, and Saliha Az-zam.
1997.
Event coreference for information ex-traction.
In Proceedings of the ACL/EACL Workshopon Operational Factors in Practical, Robust AnaphoraResolution for Unrestricted Texts, pages 75--81.Heeyoung Lee, Marta Recasens, Angel Chang, MihaiSurdeanu, and Dan Jurafsky.
2012.
Joint entity andevent coreference resolution across documents.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages489--500.Peifeng Li, Guodong Zhou, Qiaoming Zhu, and LibinHou.
2012.
Employing compositional semanticsand discourse consistency in Chinese event extrac-tion.
In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language ProcessingandComputational Natural Language Learning, pages1006--1016.Zhengzhong Liu, Jun Araki, Eduard Hovy, and TerukoMitamura.
2014.
Supervised within-document eventcoreference using information propagation.
In Pro-ceedings of the Ninth International Conference on Lan-guage Resources and Evaluation, pages 4539--4544.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of Human LanguageTechnology Conference and Conference on EmpiricalMethods in Natural Language Processing, pages 25--32.1106Katie McConky, Rakesh Nagi, Moises Sudit, andWilliam Hughes.
2012.
Improving event co-referenceby context extraction and dynamic feature weighting.In Proceedings of the 2012 IEEE International Multi-Disciplinary Conference on Cognitive Methods in Sit-uation Awareness and Decision Support, pages 38--43.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositionality.In Proceedings of the Advances in Neural InformationProcessing Systems, pages 3111--3119.Martina Naughton.
2009.
Sentence Level Event Detec-tion and Coreference Resolution.
Ph.D. thesis, Na-tional University of Ireland, Dublin, Ireland.Robert Parker, David Graff, Ke Chen, Junbo Kong, andKazuaki Maeda.
2009.
Chinese Gigaword fourth edi-tion.
Linguistic Data Consortium, Philadelphia, PA.Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,Jessica MacBride, and Linnea Micciulla.
2007.
Unre-stricted coreference: Identifying entities and events inOntoNotes.
In Proceedings of the International Con-ference on Semantic Computing, pages 446--453.Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
CoNLL-2011 Shared Task: Modeling unre-stricted coreference in OntoNotes.
In Proceedings ofthe Fifteenth Conference on Computational NaturalLanguage Learning: Shared Task, pages 1--27.Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-uard Hovy, Vincent Ng, and Michael Strube.
2014.Scoring coreference partitions of predicted mentions:A reference implementation.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 2: Short Papers), pages30--35.Altaf Rahman and Vincent Ng.
2009.
Supervised mod-els for coreference resolution.
In Proceedings of the2009 Conference on Empirical Methods in NaturalLanguage Processing, pages 968--977.Altaf Rahman and Vincent Ng.
2011.
Narrowing themodeling gap: A cluster-ranking approach to corefer-ence resolution.
Journal of Artificial Intelligence Re-search, 40:469--521.Marta Recasens and Eduard Hovy.
2011.
BLANC:Implementing the Rand Index for coreference evalua-tion.
Natural Language Engineering, 17(4):485--510.S.
Sangeetha and Michael Arock.
2012.
Event corefer-ence resolution using mincut based graph clustering.In Proceedings of the Fourth International Workshopon Computer Networks & Communications, pages253--260.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceedingsof the SixthMessageUnderstanding Conference, pages45--52.1107
