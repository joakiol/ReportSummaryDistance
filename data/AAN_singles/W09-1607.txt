Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 46?52,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSentence Position revisited:A robust light-weight Update Summarization ?baseline?
AlgorithmRahul Katragaddarahul k@research.iiit.ac.inPrasad Pingalipvvpr@iiit.ac.inLanguage Technologies Research CenterIIIT HyderabadVasudeva Varmavv@iiit.ac.inAbstractIn this paper, we describe a sentence po-sition based summarizer that is built basedon a sentence position policy, created fromthe evaluation testbed of recent summariza-tion tasks at Document Understanding Con-ferences (DUC).
We show that the summa-rizer thus built is able to outperform most sys-tems participating in task focused summariza-tion evaluations at Text Analysis Conferences(TAC) 2008.
Our experiments also show thatsuch a method would perform better at pro-ducing short summaries (upto 100 words) thanlonger summaries.
Further, we discuss thebaselines traditionally used for summarizationevaluation and suggest the revival of an oldbaseline to suit the current summarization taskat TAC: the Update Summarization task.1 IntroductionDocument summarization received a lot of atten-tion since an early work by Luhn (1958).
Statis-tical information derived from word frequency anddistribution was used by the machine to computea relative measure of significance, first for individ-ual words and then for sentences.
Later, Edmund-son (1969) introduced four clues for identifying sig-nificant words (topics) in a text.
Among them titleand location are related to position methods, whilethe other two are presence of cue words and highfrequency content words.
Edmundson assigned pos-itive weights to sentences according to their ordinalposition in the text, giving more weight to the firstsentence in the first paragraph and last sentence inthe last paragraph.Position of a sentence in a document or the po-sition of a word in a sentence give good clues to-wards importance of the sentence or word respec-tively.
Such features are called locational features,and a sentence position feature deals with presenceof key sentences at specific locations in the text.Sentence Position has been well studied in summa-rization research since its inception, early in Ed-mundson?s work (1969).
Earlier, Baxendale (1958)investigated a sample of 200 paragraphs to deter-mine where the important words are most likely tobe found.
He concluded that in 85% of the para-graphs, the first sentence was a topic sentence and in7% of the paragraphs, the final one.Recent advances in machine learning have beenadapted to summarization problem through the yearsand locational features have been consistently usedto identify salience of a sentence.
Some represen-tative work in ?learning?
sentence extraction wouldinclude training a binary classifier (Kupiec et al,1995), training a Markov model (Conroy et al,2004), training a CRF (Shen et al, 2007), and learn-ing pairwise-ranking of sentences (Toutanova et al,2007).In recent years, at the Document Understand-ing Conferences (DUC1), Text Summarization re-search evolved through task focused evaluationsranging from ?generic single-document summariza-tion?
to ?query-focused multi-document summariza-tion (QFMDS)?.
The QFMDS task models the real-world complex question answering task wherein,given a topic and a set of 25 relevant documents, the1http://duc.nist.gov/46task is to synthesize a fluent, well-organized 250-word summary of the documents that answers thequestion(s) in the topic statement.
Recent focusin the community has been towards query-focusedupdate-summarization task at DUC and the TextAnalysis Conference (TAC2).
The update task was toproduce short (~100 words) multi-document updatesummaries of newswire articles under the assump-tion that the user has already read a set of earlierarticles.
The purpose of each update summary willbe to inform the reader of new information about aparticular topic.The rest of the paper is organized as follows.
InSection 2, we describe a Sub-optimal Position Pol-icy (SPP) based on Pyramid Annotated Data, thenwe derive a simple algorithm for summarizationbased on the SPP in Section 3, and show evaluationresults.
Next, in Section 4, we explain the currentbaselines and evaluation for Multi-Document Sum-marization and finally in Section 5, we discuss theneed for an older baseline in the current context ofthe short summary task of update summarization.2 Sub-Optimal Sentence Position PolicyGiven a large text collection and a way to approxi-mate the relevance for a reasonably large subset ofsentences, we could identify significant positionalattributes for the genre of the collection.
Our ex-periments are based on the work described in (Linand Hovy, 1997), whose experiments using the Ziff-Davis corpus gave great insights on the selectivepower of the position method.2.1 Sentence Position Yield and OptimalPosition Policy (OPP)Lin and Hovy (1997) provide an empirical validationfor the position hypothesis.
They describe a methodof deriving an Optimal Position Policy for a collec-tion of texts within a genre, as long as a small setof topic keywords is defined for each text.
They de-fined sentence yield (strength of relevance) of a sen-tence based on the mention of topic keywords in thesentence.The positional yield is defined as the average sen-tence yield for that position in the document.
They2http://www.nist.gov/tac/computed the yield of each sentence position in eachdocument by counting the number of different key-words contained in the respective sentence in eachdocument, and averaging over all documents.
AnOptimal Position Policy (OPP) is derived based onthe decreasing values of positional yield.Their experiments grounded on the assumptionthat abstract is an ideal representation of centraltopic(s) of a text.
For their evaluations, they usedthe abstract to compare whether the sentences foundbased on their Optimal Position Policy are indeed agood selection.
They used precision-recall measuresto establish those findings.At our disposal we had data from pyramid eval-uations that provided sentences and their mappingto any content units in the gold standard summaries.The annotations in the data provide a unique prop-erty that each sentence can derive for itself a scorefor relevance.2.2 DocumentsThere are a wide variety of document types acrossgenre.
In our case of newswire collection we haveidentified two primary types of documents: smalldocument and large document.
This distinction ismade based on the total sentences in the document.All documents that have the number of sentencesabove a threshold should be considered large.
Weexperimented on thresholds varying from 10 to 35sentences and figured out that documents?
distribu-tion into the two categories was acceptable whenthreshold-ed at 20 sentences.
This decision is alsowell supported by the fact that the last sentences ofa document were more important than the others inthe middle (Baxendale, 1958).Sentence Position Yield (SPY) is obtained sep-arately for both types of documents.
For a smalldocument, sentence positions have values from 1through 20.
Meanwhile, for a large document wecompute SPY for position 1 through 20, then the last15 sentences labeled 136 through 150 and ?any othersentence?
is labeled 100.
It can be seen in figure 3that sentences that do not come from leading or trail-ing part of large documents do not contribute muchcontent to the summaries.47Figure 1: A sample mapping of SCU annotation to source document sentences.
An excerpt from mapping of topicD0701A of DUC 2007 QF-MDS task.Figure 2: Sentence Position Yield for small documents.2.3 Pyramid DataSummary content units, referred as SCUs hereafter,are semantically motivated, sub-sentential units thatare variable in length but not bigger than a sententialclause.
SCUs emerge from annotation of a collec-tion of human summaries for the same input.
Theyare identified by noting information that is repeatedacross summaries, whether the repetition is as smallas a modifier of a noun phrase or as large as a clause.The weight an SCU obtains is directly proportionalto the number of reference summaries that supportthat piece of information.
The evaluation methodthat is based on overlapping SCUs in human andautomatic summaries is described in the Pyramidmethod (Nenkova et al, 2007).The University of Ottawa has organized the pyra-mid annotation data such that for some of the sen-tences in the original document collection (thosethat were picked by systems participating in pyra-mid evaluation), a list of corresponding content unitsis known (Copeck et al, 2006).
We used this data toidentify locations in a document from where mostsentences were being picked, and which of those lo-cations were being most content responsive to thequery.A sample of SCU mapping is shown in figure 1.Three sentences are seen in the figure among whichtwo have been annotated with system IDs and SCUweights wherever applicable.
The first sentence hasnot been picked by any of the summarizers partici-pating in Pyramid Evaluations, hence it is unknownif the sentence would have contributed to any SCU.The second sentence was picked by 8 summarizersand that sentence contributed to an SCU of weight3.
The third sentence in the example was pickedby one summarizer, however, it did not contributeto any SCU.
This example shows all the three typesof sentences available in the corpus: unknown sam-ples, positive samples and negative samples.For each SCU, a weight is associated in pyramidannotations.
Thus a sentential score could be de-fined as sum of weights of all the contributing SCUsof the sentence.
For an unknown sample and a neg-ative sample, sentential score is 0.
For example, inthe second sentence in figure 1 the score is 3, con-tributed by a single SCU.While the same for the firstand third sentences is 0.For each sentence position the sentential score isaveraged over all documents, which we call Sen-tence Position Yield.
SPY for small and large doc-uments is shown in figures 2 and 3.
Based on thesevalues for various positions, a simple Position Pol-48Figure 3: Sentence Position Yield for large documentsicy was framed as shown below.
A position policy isan ordered set consisting of elements in the order ofmost importance.
Within a subset, each sub-elementis equally important and treated likewise.
{s1, S1, {s2, S2, s3} , {S3, s4, s5, s6, s7, s8, s20} ,{S4, s9} .
.
.
}In the above position policy, sentences from smalldocuments and large documents are represented bysi and Sj respectively.The position policy described above provides anordering of ranked sentence positions based on avery accurate ?relevance?
annotations on sentences.However, there is a large subset of sentences that arenot annotated with either positive or negative rele-vance judgment.
Hence, the policy derived is basedon a high-precision low-recall corpus3 for sentencerelevance.
If all the sentences were annotated withsuch judgements, the policy could have been differ-ent.
For this reason we call the above derived policy,a Sub-optimal Position Policy (SPP).3 SPP as an algorithmThe goal of creating a position policy was to identifyits effectiveness as a summarization algorithm.
The3DUC 2005 and 2006 data has been used for learning theSPP.
In further experiments in section 3, DUC 2007 and TAC2008 data have been used as test data.above simple heuristic was easily incorporated as analgorithm based on simple scoring for each distinctset in the policy.
For instance, based on the policyabove, all s1 get the highest weight followed by nextbest weight to all S1 and so on.As it can be observed, only the first sentence ofeach document could end up comprising the sum-mary.
This is okay, till we don?t get redundant infor-mation in the summary.
Hence we also used a sim-ple unigram match based redundancy measure thatdoesn?t allow a sentence if it matches any of the al-ready selected sentences in at least 40% of contentwords in it.
We also dis-allow sentences greater than25 content words.We applied the above algorithm to generate multi-document summaries for various tasks.
We have ap-plied it to Query-Focused Multi-Document Summa-rization (QF-MDS) task of DUC 2007 and Query-Focused Update Summarization task of TAC 2008.3.1 Query-Focused Multi-DocumentSummarizationThe query-focused multi-document summarizationtask at DUC models the real world complex ques-tion answering task.
Given a topic and a set of 25relevant documents, this task is to synthesize a flu-ent, well-organized 250 word summary of the docu-ments that answers the question(s) in the topic state-49ment/narration.The summaries from the above algorithm for theQF-MDS were evaluated based on ROUGE met-rics (Lin, 2004).
The average4 recall scores are re-ported for ROUGE-2 and ROUGE-SU4 in Table 1.Also reported are the performance of the top per-forming system and the official baseline(s).
This al-gorithm performed worse than most systems partic-ipating in the task that year and performed better5than only the ?first x words?
baseline and 3 other sys-tems.system ROUGE-2 ROUGE-SU4?first x words?
baseline 0.06039 0.10507?generic?
baseline 0.09382 0.14641SPP algorithm 0.06913 0.12492system 15 (top system) 0.12448 0.17711Table 1: ROUGE 2, SU4 Recall scores for two base-lines, the SPP algorithm and a top performing systemat Query-Focused Multi-Document Summarization task,DUC 2007.3.2 Update Summarization TaskThe update summarization task is to produce short(~100 words) multi-document update summaries ofnewswire articles under the assumption that the userhas already read a set of earlier articles.
The initialdocument set is called cluster A and the next set ofarticles are called cluster B.
For cluster A, a query-focused multi-document summary is expected.
Thepurpose of each ?update summary?
(summary ofcluster B) will be to inform the reader of new in-formation about a particular topic.
Summaries fromthe above algorithm for the Query Focused Up-date Summarization task were evaluated based onROUGE metrics.
This algorithm performed surpris-ingly better at this task when compared to QF-MDS.The rouge scores suggest that this algorithm is wellabove the median for cluster A and among the top 5systems for cluster B.It must be noted that consistent performanceacross clusters (both A and B) shows the robustnessof the ?SPP algorithm?
at the update summarizationtask.
Also, it is evident that such an algorithm iscomputationally simple and light-weight.4Averaged over all the 45 topics of DUC 2007 dataset.5Better in a statistical sense, based on 95% confidence inter-vals of the two systems?
evaluation based on ROUGE-2.These surprisingly high scores on ROUGE met-rics prompted us to evaluate the summaries based onPyramid Evaluation (Nenkova et al, 2007).
Pyramidevaluation provides a more semantic approach toevaluation of content based on SCUs as discussed inSection 2.3.
The average6 modified pyramid scoresof cluster A and cluster B summaries is shown inTable 2, along with the average recall scores forROUGE-2, ROUGE-SU4 scores.
The pyramid eval-uation7 suggests that this algorithm performs betterthan all other automated systems at TAC 2008.
Ta-ble 3 shows the average performance (across clus-ters) of ?first x words?
baseline, SPP algorithm andtwo top performing systems (System ID=43 andID=11).
System 43 was adjudged best system basedon ROUGE metrics, and system 11 was top per-former based on pyramid evaluations at TAC 2008.ROUGE-2 ROUGE-SU4 pyramidcluster A 0.08987 0.1213 0.3432cluster B 0.09319 0.1283 0.3576Table 2: Cluster wise ROUGE 2, SU4 Recall scores andmodified Pyramid Scores for SPP algorithm at the UpdateSummarization task.3.3 DiscussionIt is interesting to observe that the algorithm thatperforms very poorly at QF-MDS, does very wellin the Update Summarization task.
A possible ex-planation for such behavior could be based on sum-mary length.
For a 250 word summary in the QF-MDS task, human summaries might provide a de-scriptive answer to the query that includes informa-tion nuggets accompanied by background informa-tion.
Indeed, it has been earlier reported that humansappreciate receiving more information than just theanswer to the query, whenever possible (Lin et al,2003; Bosma, 2005).Whereas, in the case of Update Summarizationtask the summary length is only 100 words.
In sucha short length humans need to trade-off between an-swer sentences and supporting sentences, and usu-ally answers are preferred.
And since our method6Averaged over all the 48 topics of TAC 2008 dataset.7Pyramid Annotation were done by a volunteer who alsovolunteered for annotations during DUC 2007.50system ROUGE-2 ROUGE-SU4 pyramid?first x words?
baseline 0.05896 0.09327 0.166SPP algorithm 0.09153 0.1245 0.3504System 43 (top in ROUGE) 0.10395 0.13646 0.289System 11 (top in pyramid) 0.08858 0.12484 0.336Table 3: Average ROUGE 2, SU4 Recall scores and modified Pyramid Scores for baseline, SPP algorithm and two topperforming systems at TAC 2008.identifies sentences that are known to be contribut-ing towards the needed answers, it performs betterat the shorter version of the task.Another possible explanation is that as a shortersummary length is required, the task of choosing themost important information becomes more difficultand no approach works well consistently.
Also, ithas often been noted that this baseline is indeed quitestrong for this genre, due to the journalistic conven-tion for putting the most important part of an articlein the initial paragraphs.4 Baselines in Summarization TasksOver the years, as summarization research followedtrends from generic single-document summariza-tion, to generic multi-document summarization, tofocused multi-document summarization there weretwo major baselines that stayed throughout the eval-uations.
Those two baselines are:1.
First N words of the document (or of the most re-cent document).2.
First sentence from each document in chronologicalorder until the length requirement is reached.The first baseline was in place ever since the firstevaluation of generic single document summariza-tion took place in DUC 2001.
For multi-documentsummarization, first N words of the most recentdocument (chronologically) was chosen as the base-line 1.
In the recent summarization evaluations atText Analysis Conference (TAC 2008), where up-date summarization was evaluated; baseline 1 stillpersists.
This baseline performs pretty poorly at con-tent evaluations based on all manual and automaticmetrics.
However, since it doesn?t disturb the orig-inal flow and ordering of a document, linguisticallythese summaries are the best.
Indeed it outperformsall the automated systems based on linguistic qualityevaluations.The second baseline had been used occasionallywith multi-document summarization from 2001 to2004 with both generic multi-document summariza-tion and focused multi-document summarization.
In2001 only one system significantly outperformed thebaseline 2 (Nenkova, 2005).
In 2003 QF-MDS how-ever, only one system outperformed the baseline 2above, while in 2004 at the same task, no systemsignificantly outperforms the baseline.
This baselineas can be seen, over the years has been pretty muchuntouched by systems based on content evaluation.However, the linguistic aspects of summary qualitywould be compromised in such a summary.Currently, for the Update Summarization task atTAC 2008, NIST?s baseline is the baseline 1 (?first xwords?
baseline).
And all systems (except one) per-form better than the baseline in all forms of contentevaluation.
Since the task is to generate 100 wordsummaries (short summaries), based on past experi-ences, there is no doubt that baseline 2 would per-form well.It is interesting to observe that baseline 2 is a closeapproximation to the ?SPP algorithm?
described inthis paper.
There are two main differences that wedraw between ?baseline 2?
and SPP algorithm.
First,?baseline 2?
picks only the first sentence in eachdocument, while ?SPP algorithm?
could pick othersentences in an order described by the position pol-icy.
Second, ?baseline 2?
puts no restriction on re-dundancy, thus due to journalistic conventions entiresummary might be comprised of the same ?informa-tion nuggets?, wasting the minimal real-estate avail-able (~100 words).
On the other hand, in our ?SPPalgorithm?
we consider a simple unigram-overlapmeasure to identify redundant information in sen-tence pairs that avoids redundant nuggets in the finalsummary.515 Discussion and ConclusionBaselines 1 and 2 mentioned above, could togetheract as a balancing mechanism to compare for lin-guistic quality and responsive content in the sum-mary.
The availability of a stronger content respon-sive summary as a baseline would enable steadyprogress in the field.
While all the linguisticallymotivated systems would compare themselves withbaseline 1, the summary content motivated systemswould compare with the stronger baseline 2 and getbetter than it.Over the years to come, the usage of ?baseline 1?doesn?t help in understanding whether there hasbeen significant improvement in the field.
This is be-cause almost every simple algorithm beats the base-line performance.
Having a better baseline, like theone based on the position hypothesis, would raisethe bar for systems participating in coming years,and tracking progress of the field over the years iseasier.In this paper, we derived a method to identify a?sub-optimal position policy?
based on pyramid an-notation data, that were previously unavailable.
Wealso distinguish small and large documents to obtainthe position policy.
We described the Sub-optimalSentence Position Policy (SPP) based on pyramidannotation data and implemented the SPP as an al-gorithm to show that a position policy thus formedis a good representative of the genre and thus per-forms way above median performance.
We furtherdescribe the baselines used in summarization evalu-ation and discuss the need to bring back baseline 2(or the ?SPP algorithm?)
as an official baseline forupdate summarization task.Ultimately, as Lin and Hovy (1997) suggest, theposition method can only take us certain distance.
Ithas a limited power of resolution (the sentence) andits limited method of identification (the position in atext).
Which is why we intend to use it as a baseline.Currently, as we can see the algorithm generates ageneric summary, it doesn?t consider the topic orquery to generate a query-focused summary.
In fu-ture we plan to extend the SPP algorithm with somebasic method for bringing in relevance.ReferencesP.
B. Baxendale.
1958.
Machine-made index for tech-nical literature ?
an experiment.
IBM Journal of Re-search and Development, 2(Non-topical Issue).Wauter Bosma.
2005.
Extending answers using dis-course structures.
In Horacio Saggion and J. L. Minel,editors, RANLP workshop on Crossing Barriers in Textsummarization Research, pages 2?9.
Incoma Ltd.John M. Conroy, Judith D. Schlesinger, Jade Goldstein,and Dianne P. O?leary.
2004.
Left-brain/right-brainmulti-document summarization.
In the proceedings ofDocument Understanding Conference (DUC) 2004.Terry Copeck, D Inkpen, Anna Kazantseva, A Kennedy,D Kipp, Vivi Nastase, and Stan Szpakowicz.
2006.Leveraging duc.
In proceedings of DUC 2006.H.
P. Edmundson.
1969.
New methods in automatic ex-tracting.
In Journal of the ACM, volume 16, pages264?285.
ACM.Julian Kupiec, Jan Pedersen, and Francine Chen.
1995.A trainable document summarizer.
In the proceedingsof ACM SIGIR?95, pages 68?73.
ACM.Chin-Yew Lin and Eduard Hovy.
1997.
Identifying top-ics by position.
In Proceedings of the fifth conferenceon Applied natural language processing, pages 283?290.
ACL.Jimmy Lin, Dennis Quan, Vineet Sinha, Karun Bakshi,David Huynh, Boris Katz, and David R. Karger.
2003.The role of context in question answering systems.
Inthe proceedings of CHI?04.
ACM.Chin-Yew Lin.
2004.
Rouge: A package for automaticevaluation of summaries.
In the proceedings of ACLWorkshop on Text Summarization Branches Out.
ACL.H.P.
Luhn.
1958.
The automatic creation of literature ab-stracts.
In IBM Journal of Research and Development,Vol.
2, No.
2, pp.
159-165, April 1958.Ani Nenkova, Rebecca Passonneau, and Kathleen McK-eown.
2007.
The pyramid method: Incorporating hu-man content selection variation in summarization eval-uation.
In ACM Trans.
Speech Lang.
Process., vol-ume 4, New York, NY, USA.
ACM.Ani Nenkova.
2005.
Automatic text summarization ofnewswire: Lessons learned from the document under-standing conference.
In Manuela M. Veloso and Sub-barao Kambhampati, editors, AAAI, pages 1436?1441.AAAI Press / The MIT Press.Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, and ZhengChen.
2007.
Document summarization using condi-tional random fields.
In the proceedings of IJCAI ?07.,pages 2862?2867.
IJCAI.Kristina Toutanova, Chris Brockett, Michael Gamon, Ja-gadeesh Jagarlamundi, Hisami Suzuki, and Lucy Van-derwende.
2007.
The pythy summarization system:Microsoft research at duc 2007.
In the proceedings ofDocument Understanding Conference 2007.52
