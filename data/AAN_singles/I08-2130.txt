Combining Context Features by Canonical Belief Network for ChinesePart-Of-Speech TaggingHongzhi Xu and Chunping LiSchool of Software, Tsinghua UniversityKey Laboratory for Information System Security, Ministry of Education Chinaxuhz05@mails.tsinghua.edu.cncli@tsinghua.edu.cnAbstractPart-Of-Speech(POS) tagging is the essen-tial basis of Natural language process-ing(NLP).
In this paper, we present an al-gorithm that combines a variety of contextfeatures, e.g.
the POS tags of the words nextto the word a that needs to be tagged and thecontext lexical information of a by Canoni-cal Belief Network to together determine thePOS tag of a.
Experiments on a Chinese cor-pus are conducted to compare our algorithmwith the standard HMM-based POS taggingand the POS tagging software ICTCLAS3.0.The experimental results show that our algo-rithm is more effective.1 IntroductionPart-Of-Speech(POS) tagging is the essential basisof Natural language processing(NLP).
It is the pro-cess in which each word is assigned to a correspond-ing POS tag that describes how this word be used ina sentence.
Typically, the tags can be syntactic cat-egories, such as noun, verb and so on.
For Chineselanguage, word segmentation must be done beforePOS tagging, because, different from English sen-tences, there is no distinct boundary such as whitespace to separate different words(Sun, 2001).
Also,Chinese word segmentation and POS tagging can bedone at the same time(Ng, 2004)(Wang, 2006).There are two main approaches for POS tagging:rule-based and statistical algorithms(Merialdo,1994).
Rule based POS tagging methods ex-tratct rules from training corpus and use theserules to tag new sentences(Brill, 1992)(Brill,1994).
Statistic-based algorithms based on BeliefNetwork(Murphy, 2001) such as Hidden-Markov-Model(HMM)(Cutting, 1992)(Thede, 1999), Lex-icalized HMM(Lee, 2000) and Maximal-Entropymodel(Ratnaparkhi, 1996) use the statistical infor-mation of a manually tagged corpus as backgroundknowledge to tag new sentences.
For example, theverb is mostly followed by a noun, an adverb ornothing, so if we are sure that a word a is a verb,we could say the word b following a has a largeprobability to be a noun.
This could be helpfulspecially when b has a lot of possible POS tags or itis an unknown word.Formally, this process relates to Pr(noun|verb),Pr(adverb|verb) and Pr(nothing|verb), that canbe estimated from the training corpus.
HMM-based tagging is mainly based on such statisticalinformation.
Lexicalized HMM tagging not onlyconsiders the POS tags information to determinewhether b is noun, adverb or nothing, but alsoconsiders the lexical information a itself.
Thatis, it considers the probabilities Pr(noun|a, verb),Pr(adverb|a, verb) and Pr(nothing|a, verb) forinstance.
Since combining more context informa-tion, Lexicalized HMM tagging gets a better perfor-mance(Lee, 2000).The main problem of Lexicalized HMM is thatit suffers from the data sparseness, so parametersmoothing is very important.
In this paper, wepresent a new algorithm that combines several con-text information, e.g.
the POS tags information andlexical information as features by Canonical BeliefNetwork(Turtle, 1991) to together determine the tag907of a new word.
The experiments show that our algo-rithm really performs well.
Here, we don?t exploreChinese word segmentation methods, and related in-formation can be found in(Sun, 2001).The rest of the paper is organized as follows.
Insection 2 and section 3, we describe the standardHMM-based tagging and Lexicalized HMM taggingrespectively which are relevant to our algorithm.
Insection 4, we describe the Belief Network as a pre-liminary.
In section 5, we present our algorithm thatis based on Canonical Belief Network.
Section 6 isthe experiments and their results.
In section 7, wehave the conclusion and the future work.2 Standard Hidden Markov ModelThe problem of POS tagging can be formally de-fined as: given an observation(sentence) w ={w1, w2, ..., wT } and a POS tag set TS ={t1, t2, ..., tM}, the task is to find a tag sequencet = {t1, t2, ..., tT }, where ti ?
TS, that is the mostpossible one to explain the observation.
That is tofind t to maximize the probability Pr(t|w).
It canbe rewritten by Bayesian rule as follows.Pr(t|w) = Pr(w|t)?
Pr(t)Pr(w)As for any sequence t, the probability Pr(w) is con-stant, we could ignore Pr(w).
For Pr(t), it can bedecomposed by the chain rule as follows.Pr(t) = Pr(t1, t2, ..., tT )= Pr(t1)?
Pr(t2|t1)?
Pr(t3|t1, t2)?...?
Pr(tT |t1, t2, ..., tT?1)Through this formula, we could find that the calcu-lation is impossible because of the combination ex-plosion of different POS tags.
Generally, we use an-gram especially n = 2 model to calculate Pr(t)approximately as follows.Pr(t) = Pr(t1|t0)?
Pr(t2|t1)?
Pr(t3|t2)?...?
Pr(tT |tT?1)where t0 is nothing.
For Pr(w|t), with an indepen-dent assumption, it can be calculated approximatelyas follows.Pr(w|t) = Pr(w1|t1)?
Pr(w2|t2)?
Pr(w3|t3)...?
Pr(wT |tT )Usually, the probability Pr(ti|ti?1) is called tran-sition probability, and Pr(wi|ti) is called the emis-sion probability.
They both can be estimated fromthe training set.
This means that the tag ti of wordwi is only determined by the tag ti?1 of word wi?1.So, we could find the best sequence through a for-ward(left to right) process.If we state all possible POS tags(stats) of eachword and connect all possible ti?1 with all possi-ble ti and each edge is weighted by Pr(ti|ti?1),we could get a Directed Acyclic Graph(DAG).
Thesearching process(decoding) that is involved in find-ing t that maximizes Pr(t|w) can be explained asfinding the path with the maximal probability.
Forthis sub task, Viterbi is an efficient algorithm thatcan be used(Allen, 1995).3 Lexicalized Hidden Markov ModelLexicalized HMM is an improvement to the stan-dard HMM.
It substitutes the probability Pr(ti|ti?1)with Pr(ti|ti?J,i?1, wi?L,i?1), and the probabilityPr(wi|ti) with Pr(wi|ti?K,i, wi?I,i?1).
In otherwords, the tag of word wi is determined by the tagsof the J words right before wi and L words right be-fore wi.
It uses more context information of wi todetermine its tag.However, it will suffer from the data sparse-ness especially when the values of J , L, K andI are large, which means it needs an explosivelylarger training corpus to get a reliable estimation ofthese parameters, and smoothing techniques must beadopted to mitigate the problem.
Back-off smooth-ing is used by Lexicalized HMM.
In the back-offmodel, if a n-gram occurs more than k times intraining corpus, then the estimation is used but dis-counted, or the estimation will use a shorter n-grame.g.
(n-1)-gram estimation as a back-off probabil-ity.
So, it is a recursive process to estimate a n-gramparameter.4 Belief NetworkBelief Network is a probabilistic graphical model,which is also a DAG in which nodes representrandom variables, and the arcs represent condi-tional independence assumptions.
For example, theprobability Pr(A,B) = Pr(A) ?
Pr(B|A) canbe depicted as Figure 1(a), and if we decompose908%$&D%$&E%$$%F GFigure 1: Some Belief Networks.Pr(A,B) = Pr(B) ?
Pr(A|B), it can be de-picted as Figure 1(b).
Similarly, the probabilityPr(A,B,C) = Pr(A)?
Pr(B|A)?
Pr(C|A,B)can be depicted as Figure 1(c).As we have analyzed above, such decompositionwould need us to estimate a large amount of pa-rameters.
In the belief network, a conditional in-dependence relationship can be stated as follows: anode is independent of its ancestors given its par-ents, where the ancestor/parent relationship is withrespect to some fixed topological ordering of thenodes.
For example, if we simplify the graph Figure1(c) to graph Figure 1(d), it is equivalent to the de-composition: Pr(A,B,C) = Pr(A)?Pr(B|A)?Pr(C|B), which is actually the same as that ofHMM.
More details about Belief Network can foundin(Murphy, 2001).5 Canonical Belief Network BasedPart-Of-Speech Tagging5.1 Canonical Belief NetworkCanonical Belief Network was proposed by Turtlein 1991(Turtle, 1991), and it was used in informa-tion retrieval tasks.
Four canonical forms are pre-sented to combine different features, that is and, or,wsum and sum to simplify the probability combi-nation further.
With the and relationship, it meansthat if a node in a DAG is true, then all of its parentsmust be true.
With the or relationship, it means thatif a node in a DAG is true, then at least one of its par-ents is true.
With the wsum relationship, it meansthat if a node in a DAG is true, it is determined by allof its parents and each parent has a different weight.With the sum relationship, it means that if a node ina DAG is true, it is determined by all of its parentsand each parent has an equal weight.For example, we want to evaluate the probabil-ity Pr(D|A) or Pr(D = true|A = true), and$&%'DQGD$&%'RUE$&%'ZVXPF$&%'VXPGFigure 2: Canonical Belief Networks forPr(A,B,C,D).node D has two parents B and C, we could usethe four canonical forms to evaluate Pr(D|A) asshown in Figure 2.
Suppose that Pr(B|A) = p1and Pr(C|A) = p2, with the four canonical formand, or, wsum and sum, we could get the follow-ing estimations respectively.Pand(D|A) = p1 ?
p2Por(D|A) = 1?
(1?
p1)?
(1?
p2)Pwsum(D|A) = w1p1 + w2p2Psum(D|A) = (p1 + p2)/2The standard Belief Network actually supposes thatall the relationships are and.
However, in real world,it is not the case.
For example, we want to evaluatethe probability that a person will use an umbrella,and there are two conditions that a person will useit: raining or a violent sunlight.
If we use the stan-dard Belief Network, it is impossible to display suchsituation, because it could not be raining and sunnyat the same time.
The or relationship could easilysolve this problem.5.2 Algorithm DescriptionDefinition: A feature is defined as the context in-formation of a tag/word, which can be POS tags,words or both.
For example, {Ti?J , ..., Ti?1} is afeature of tag ti, {Ti?J , ..., Ti} is a feature of wordwi, {Ti?J , ..., Ti?1,Wi?L, ...,Wi?1} is a feature oftag ti, {Ti?K , ..., Ti,Wi?I , ...,Wi?1} is a feature ofword wi.In our algorithm, we select 6 features for tag ti,and select 2 features for word wi, which are shownin Table 1.
We can see that f1t , f2t and f3t are actuallythe n-gram features used in HMM, f4t , f5t and f6t areactually features used by lexicalized HMM.We adopt the canonical form or to combine themas shown in Figure 3, and use the canonical form909Featuresf1t : Ti?3, Ti?2, Ti?1f2t : Ti?2, Ti?1ti f3t : Ti?1f4t : Ti?3, Ti?2, Ti?1, Wi?3, Wi?2, Wi?1f5t : Ti?2, Ti?1, Wi?2, Wi?1f6t : Ti?1, Wi?1wi f1w: Ti?1, Tif2w: TiTable 1: Features used for ti and wi.and to combine features of ti and wi.
Because wethink that the POS tag of a new word can be de-termined if any one of the features can give a highconfidence or implication of a certain POS tag.
Theprobabilities Pr(f it |ti?1), i = 1, ..., 6. are all 1,which means that all the features in the CanonicalBelief Network are considered to estimate the tagti of word wi when we have already estimated thetag ti?1 of word wi?1.
So, the transition probabilitycould be calculated as follows.ptransi?1,i = 1?6?j=1[1?
Pr(ti|f jt )]In the same way, the probabilities Pr(f iw|ti), i =1, 2. are all 1.
The emission probability could becalculated as follows.pomiti = 1?2?j=1[1?
Pr(wi|f jw)]Let?s return to the POS tagging problem whichneeds to find a tag sequence t that maximizes theprobability Pr(t|w), given a word sequence w de-fined in Section 2.
It is involved in evaluating twoprobabilities Pr(t) and Pr(w|t).
With the Canon-ical Belief Network we just defined, they could becalculated as follows.Pr(t) = ?Ti=1 ptransi?1,iPr(w|t) = ?Ti=1 pomitiPr(w, t) = Pr(t)?
Pr(w|t)The canonical form or would not suffer fromthe data sparseness even though it refers to 4-gram, because if a 4-gram feature(f1t for example)doesn?t appear in the training corpus, the probabilityIWRUIWIW IWIW IWWLWLRUIZZLIZD EWLFigure 3: Canonical Belief Networks used in our al-gorithm.Pr(ti|f1t ) is estimated as zero, which means the fea-ture contributes nothing to determine the probabilitythat word wi gets a tag ti, which is actually deter-mined by a lower n-grams.
Cases are the same for3-gram, 2-gram and so on.
In a special case, when a4-gram (f4t for example) appears in the training cor-pus and appears only once, the probability Pr(ti|f1t )will be 1, which means that the sentence or phrasewe need to tag may have appeared in the trainingcorpus, so we can tag the sentence or phrase withreference to the appeared sentence or phrase in thetraining corpus.
This is an intuitional comprehen-sion of our algorithm and its motivation.Decoding: The problem of using high n-gramis the combination explosion especially for highgrams.
For example, consider the feature , supposeone word has 3 possible tags on average, then wehave to evaluate 33 = 27 cases for f1t , further, dif-ferent features could get different combinations andthe number of combinations will be 272?92?32 =531441.
To solve the problem, we constrain all fea-tures to be consistent.
For example, the tag ti?1 offeature f1t must be same as that of feature f2t , f3t ,f4t , f5t and f6t at one combination.
The followingfeatures are not consistent, because the ti?1 in f1t isV BP , while the ti?1 in f4t is NN .f1t = JJ,NNS, V BPf4t = JJ,NNS,NN, little, boys, bookThis will decrease the total combination to 33 = 27.We use a greedy search scheme that is based on theclassic decoding algorithm Viterbi.
Suppose that theViterbi algorithm has reached the state ti?1, to cal-culate the best path from the start to ti, we only usethe tags on the best path from the start to ti?1 to cal-culate the probability.
This decreases the total com-910bination to 3(the number of possible tags of ti?1),which is the same as that of standard HMM.6 ExperimentsDataset: We conduct our experiments on a Chinesecorpus consisting of all news from January, 1998 ofPeople?s Daily, tagged with the tag set of PekingUniversity(PKU), which contains 46 POS tags1.
Forthe corpus, we randomly select 90% as the trainingset and the remaining 10% as the test set.
The cor-pus information is shown in Table 2, where unknownwords are the words that appear in test set but not intraining set.
The experiments are run on a machinewith 2.4GHZ CPU, and 1GB memory.Training set Test setWords 1021592 112321Sentences 163419 17777Unknow words 2713Table 2: Chinese corpus information.Unknown Words: In our experiments, we firststore all the words with their all possible POS tagsin a dictionary.
So, our algorithm gets all possibletags of a word through a dictionary.
As for the wordin the test set that doesn?t appear in the training set,we give the probability Pr(wi|f jw) value 1, with allj.
This processing is quite simple, however, it isenough to observe the relative performances of dif-ferent POS taggers.For Chinese word segmentation, we use the seg-mentation result of ICTCLAS3.02.
The segmenta-tion result is shown in Table 3.
Sen-Prec is the ratioof the sentences that are correctly segmented amongall sentences in the test set.Precision Recall F1 Sen-Prec0.9811 0.9832 0.9822 0.9340Table 3: Segmentation Result by ICTCLAS.Open Test: We compare the POS tagging per-formance of our algorithm with the standard HMM,1http://icl.pku.edu.cn/Introduction/corpustagging.htm2ICTCLAS3.0 is a commercial software developed by Insti-tute of Computing Technology, Chinese Academy of Science,that is used for Chinese word segmentation and POS tagging.and ICTCLAS3.0.
The experimental result is shownin Table 4.
Prec-Seg is the POS tagging precisionon the words that are correctly segmented.
Prec-Sen is the ratio of the sentences that are correctlytagged among all sentences in the test set.
Prec-Sen-Seg is the ratio of sentences that are correctly taggedamong the sentences that are correctly segmented.With the experiments, we can see that, our algo-rithm always gets the best performance.
The ICT-CLAS3.0 doesn?t perform very well.
However, thisis probably because of that the tag set used by ICT-CLAS3.0 is different from that of PKU.
Even thoughit provides a mapping scheme from their tags toPKU tags, they may be not totally consistent.
Thepublished POS tagging precision of ICTCLAS3.0 is94.63%, also our algorithm is a little better.
This hasproved that our algorithm is more effective for POStagging task.ICTCLAS HMM CBNPrecision 0.9096 0.9388 0.9465Recall 0.9115 0.9408 0.9485F1 0.9105 0.9398 0.9475Prec-Seg 0.9271 0.9569 0.9647Prec-Sen 0.6342 0.7404 0.7740Prec-Sen-Seg 0.6709 0.7927 0.8287Table 4: Open test comparison result on Chinesecorpus.Close Test: As we have analyzed above in Sec-tion 5.2 that our algorithm takes advantage of moreinformation in the training set.
When a sentence or aphrase appears in the training set, it will help a lot totag the new sentence correctly.
To test whether thiscase really happens, we conduct a new experimentthat is the same as the first one except that the testset is also added to the training set.
The experimen-tal result is shown in Table 5.
We can see that theperformance of our algorithm is greatly improved,while the HMM doesn?t improve much, which fur-ther proves our analysis.Even though our algorithm gives a satisfying per-formance, it may be able to be improved by adopt-ing smoothing techniques to take advantage of moreuseful features, e.g.
to make the probabilities suchas Pr(ti|f1t ), Pr(ti|f2t ) not be zero.
In addition, theadoption of techniques to deal with unknown words911ICTCLAS HMM CBNPrecision 0.9096 0.9407 0.9658Recall 0.9115 0.9427 0.9678F1 0.9105 0.9417 0.9668Prec-Seg 0.9271 0.9588 0.9843Prec-Sen 0.6342 0.7476 0.8584Prec-Sen-Seg 0.6709 0.8004 0.9191Table 5: Close test comparison result on Chinesecorpus.and techniques to combine with rules may also im-prove the performance of our algorithm.
If we havea larger training corpus, it may be better to removesome confusing features such as f3t and f2w, becausethey contain weak context information and this iswhy a higher n-gram model always performs betterthan a lower n-gram model when the training corpusis large enough.
However, this should be validatedfurther.7 Conclusion and Future WorkIn this paper, we present a novel algorithm thatcombines useful context features by Canonical Be-lief Network to together determine the tag of a newword.
The ?or?
node can allow us to use higher n-gram model although the training corpus may be notsufficient.
In other words, it can overcome the datasparseness problem and make use of more informa-tion from the training corpus.
We conduct experi-ments on a Chinese popular corpus to evaluate ouralgorithm, and the results have shown that it is pow-erful even in case that we don?t deal with the un-known words and smooth the parameters.We think that our algorithm could also be usedfor tagging English corpus.
In addition, we only ex-tract simple context information as features.
We be-lieve that there exists more useful features that canbe used to improve our algorithm.
For example, thesyntax analysis could be combined as a new fea-ture, because a POS sequence may be illegal eventhough it gets the maximal probability through ouralgorithm.
Yet, these will be our future work.Acknowledgement This work was supported byChinese 973 Research Project under grant No.2002CB312006.ReferencesAdwait Ratnaparkhi.
1996.
A Maximum Entropy Modelfor Part-Of-Speech Tagging.
In Proc.
of the Empiri-cal Methods in Natural Language Processing Confer-ence(EMNLP?96), 133-142.Bernard Merialdo.
1994.
Tagging English Text witha Probabilistic Model.
Computational Linguistics,20(2):155?172.Doug Cutting, Julian Kupied, Jan Pedersen and PenelopeSibun.
1992.
A Practical part-of-speech tagger.
InProceedings of the 3rd Conference on Applied NaturalLanguage Processing(ANLP?92), 133-140.Eric Brill.
1992.
A simple rule-based part of speech tag-ger.
In Proc.
of the 30th Conference on Applied Com-putational Linguistics(ACL?92), Trento, Italy, 112-116.Eric Brill.
1994.
Some Advances in Transformation-Based Part of Speech Tagging.
In Proc.
ofthe 12th National Conference on Artificial Intelli-gence(AAAI?94), 722-727.Howard Turtle and W. Bruce Croft.
1991.
Evaluation ofan Inference Network-Based Retrieval Model.
ACMTransactions on Information Systems, 9(3):187-222.Hwee Tou Ng and Jin Kiat Low.
2004.
Chinese Part-of-Speech Tagging: One-at-a-Time or All-at-Once?Word-Based or Character-Based?.
In Proc.
of the Em-pirical Methods in Natural Language Processing Con-ference(EMNLP?04).James Allen.
1995.
Natural Language Understanding.The Benjamin/Cummings Publishing Company.Kevin P. Murphy.
2001.
An introduction to graphicalmodels.
Technical report, Intel Research TechnicalReport.Maosong Sun and Jiayan Zou.
2001.
A critical appraisalof the research on Chinese word segmentation(In Chi-nese).
Contemporary Linguistics, 3(1):22-32.Mengqiu Wang and Yanxin Shi.
2006.
Using Part-of-Speech Reranking to Improve Chinese Word Segmen-tation.
In Proc.
of the 5th SIGHAN Workshop on Chi-nese Language Processing, 205-208.Sang-Zoo Lee, Jun-ichi Tsujii and Hae-Chang Rim.2000.
Lexicalized Hidden Markov Models for Part-of-Speech Tagging.
In Proc.
of 18th International Con-ference on Computational Linguistics(COLING?00),Saarbrucken, Germany, 481-487.Scott M. Thede and Mary P. Harper.
1999.
A Second-Order Hidden Markov Model for Part-of-Speech Tag-ging.
In Proc.
of the 37th Conference on AppliedComputational Linguistics(ACL?99), 175-182.912
