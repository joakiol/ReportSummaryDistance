Integrating a Lexical Database and a Training Collection for Text CategorizationJose Maria G6mez-Hidalgo, Manuel de Buenaga Rodrtguez{jmgomez,mbuenaga} @dia.ucm.esDepartamento deInform~ltica y Autom~tticaUniversidad Complutense de MadridAvda.
Complutense s/n, 28040 Madrid (Spain)AbstractAutomatic text categorization is a complexand useful task for manynatural languageprocessing applications.
Recent approaches totextcategorization focus more on algorithmsthan on resources involved in thisoperation.
Incontrast o this trend, we present an approachbased on the integration of widely availableresources aslexical databases and trainingcollections to overcome current limitationsofthe task.
Our approach ~makes use of Word-Net synonymy information toincrease evi-dence for bad trained categories.
When testinga direct categorization, a WordNet basedone, atraining algorithm, and our integrated ap-proach, the latter exhibitsa better perfomancethan any of the others.
Incidentally, WordNetbased approach perfomance is comparablewith the trainingapproach one.1 In t roduct ionText categorization (TC) is the classification ofdocu-ments with respect to a set of one or more pre-existingcategories.
TCis a hard and very useful operation fre-quently applied to the assignment of subject categoriesto documents, toroute and filter texts, or as a part ofnatural anguage processingsystems.In this paper we present an automatic TC approachbased on theuse of several inguistic resources.
Nowa-days, many resources like trainingcollections and lexi-cal databases have been successfully employed for textclassificationtasks \[Boguraev and Pustejovsky, 1996\],but always in an isolated way.
Thecurrent trend in theTC field is to pay more attention to algorithms thantoresources.
We believe that the key idea for the im-provement of text categorization is increasingtheamount of information a system makes use of,through the integration ofseveral resources.We have chosen the Information Retrieval vectorspace model for ourapproach.
Term weight vectors arecomputed for documents and categoriesemploying thelexical database WordNet and the training subset ofthe testcollection Reuters-22173.
We calculate theweight vectors for:1 This research issupported by the Spanish Commttee ofSctence andTechnology (CICYT TIC94-0187)._ A direct approach,_ a Wordnet based approach,_ a training collection approach,_ and finally, a technique for integrating WordNetand a training collection.Later, we compare document-category similarity bymeans of a cosine-basedfunction.
We have driven aseries of experiments on the test subset of Reuters-22173, which yields two conclusions.
First, the inte-grated approach performs better than any of the otherones, confirming thehypothesis that the more informeda text classification system is, thebetter it performs.Secondly, the lexical database oriented technique canrival with the training approach, avoiding the necessityofcost-expensive building of training collections forany domain andclassification task.2 Task  Descr ipt ionGiven a set of documents and a set of categories, thegoal of acategorization system is to decide whetherany document belongs to anycategory ornot.
The sys-tem makes use of the information contained in adocu-ment to compute a degree of pertainance of the docu-ment to each category.
Categories are usually subjectlabels likeart or military, but other categories like textgenres are also interesting\[Karlgren a d Cutting,1994\].
Documents can be news stories, e-mailmessages, reports, and so forth.The most widely used resource for TC is the trainingcollection.
Attaining collection is a set of manuallyclassified documents that allowsthe system to guessclues on how to classify new unseen documents.Thereare currently several TC test collections, fromwhich a training subset and a test subset can be ob-tained.
Forinstance, the huge TREC collection\[Harman, 1996\], OHSUMED \[Hersh etal, 1994\] andReuters-22173 \[Lewis, 1992\] have been collected forthistask.
We have selected Reuters because it has beenused in other work,facilitating the comparison of re-suits.Lexical databases have been rarely employed in TC,but severalapproaches ave demonstrated their useful-ness for term classification operations like word sensedisambiguation\[Resnik, 1995; Agirre and Rigau,1996\].
A lexical database is a referencesystem thataccumulates information on the lexical items of one o39several anguages In this view,machine readable dic-tionaries can also be regarded as primitive lexicaldata-bases.
Current lexical databases include WordNet\[Miller, 1995\], EDR\[Yokoi, 1995\] and Roget's The-saurus.
WordNet's large coverage and frequent utili-zation has led us touse it for our experiments.We organize our work depending on the kind andnumber ofresources involved.
First, a direct approachin which only the categoriesthemselves are the termsused in representation has been tested.
Secondly,WordNet by itself has been usedfor increasing thenumber of terms and so, the amount of predictingin-formation.
Thirdly, we have made use of the trainingsubset of Reuters toobtain the categories representa-tives.
Finally, we have employed both WordNet andReuters to get a betterrepresentation of undertramedcategories3 Integrat ing Resources in the VectorSpaceModelThe Vector Space Model (VSM) \[Salton and McGill,1983\] is a very suitableenvironment for expressing ourapproaches to TC: it is supported by many experiencesin textretrieval \[Lewis, 1992; Salton, 1989\];' it allowsthe seamless integratlonof multiple knowledge sourcesfor text classification, and it makes it easyto identifythe role of every knowledge source involved in theclassification operation In the nextsections we presenta straightforward adaptation of the VSM for TC, andtheway we use the chosen resources for calculatingseveral model elements.3.1 Vector SpaceModei for Text Catego-r izat ionThe bulk of the VSM for Information Retrieval (IR) isrepresenting naturallanguage xpressions as termweight vectors.
Each weight measures theimportanceof a term in a natural anguage xpression, which canbe adocument or a query.
Semantic loseness betweendocuments and queries is computed by the cosine ofthe anglebetween document and query vectors.Exploiting an obvious analogy between queries andcategories,the latters can be represented by termweight vectors Then, a category canbe assigned to adocument when the cosine similarity between themexceeds acertaln threshold, or when the category ishighly ranked.
In a closer look,and given three sets ofN terms, M documents and Lcategories, the weightvector for document j is (wdl.l,Wd2j ..... wdNl) and theweight vector for category k is (WC-lk, WC2k-,.
,WCNk).The similarity between document j and category k isobtained with the formula:8lm(dj,Ck)=Nwdv ?
WC,kt=lEwct=lTerm weights for document vectors can be computedmaking use of wellknown formulae based on termfrequency.
We use the following one from\[Salton,1989\]:M wd v = ~ " log2 ~-Where ~/ i s  the frequency of term t in documentj, anddfl is the-number of documents m which term : occursNow, only weights for category vectors are to be ob-tained.
Next we will show how to do it depending onthe resource used.3.2 Direct ApproachThis approach to TC makes no use of any resourceapart to the documents tobe classified It tests the in-tuition that the name of content-basedcategories is agood predictor for the occurrence of these categories.For instance, the occurrence of the word "barley" inadocument suggests that this one should be classifiedin the barley z category.
All the following examplesare taken from the Reuters categoryset and involvewords that actually occur in the documents, category.Wehave taken exactly the categories names, althoughclassification in moregeneral categories like strategtc-metal should rather relay on the occurrence of morespecificwords like '"gold" or "zinc.
"In this approach, the terms used for the representa-tion are justthe categories themselves.
The weight ofterm t m the vector forcategory j is 1 tf i = j and 0 inother cases.
Multiword categories imply the use ofmultiwordterms.
For example, the expression "balanceof payments" is considered as one term.
When catego-ries consist of several synonyms(like zron-steel), all ofthem are used in the representation.
Since the numberofcategories m Reuters is 135, and two of them arecomposite, these approachproduces 137-componentvectors .3.3 WordNet based ApproachLexical databases contain many kinds of information(concepts; synonymy andother lexical relations; hy-ponymy and other conceptual relations; etc.
),For in-stance, WordNet represents concepts as synonyms sets,or synsets.
We haveselected this synonymy informa-tion, performing a "categoryexpansion" simdar toquery expansion in IR.
For any category,the synset itbelongs to is selected, and any other term belonging toit is added to therepresentation.
This technique in-creases the amount of evidence used topredict categoryoccurrence.Unfortunately, the disambiguation of categorieswith respect oWordNet concepts is required.
We haveperformed this task manually, becausethe small num-ber of categories in the test collection made it afford-able.
We are currently designing algorithms for auto-mating this operation.After locating categories in WordNet, a term setcontaining allthe category's synonyms has been built.For the 135 categories used in thisstudy, we have pro-duced 368 terms.
Although some meaningless terms2 All the following examples are taken from the Reuterscategory set, andthey anvolve words that actually occur mthe documents40occur and could bedeleted, we have developed noautomatic riteria for this at the moment.Let us take a look to one example.
The fuel categoryhasdriven us to the addition of the terms"combustible" and "combustible material," since theybelong to the same synset in WordNet.
In general, thetermweight vector for category k Is 1 for every syno-nym of the category an0 for any other term.3.4 Training Col lect ion ApproachThe key asumption when using a training collection isthat a term often occurring within a category andrarely within others is a good predictorfor that cate-gory.
A set of predictors is typically computed fromterm tocategory co-ocurrence statistics, as a trainingstep.
The computation depends on the approach andalgorithmselected.
As Lewis \[1992\] has done before,we have replicated in the VSMearly Bayesian experi-ments that had reported good results.Terms are selected according to the number of timesthey occur withincategories.
Those terms which co-occur at least with the 1% and at mostwith the 10% ofthe categories are taken.
Among them, those 286 with-highest document frequency are selected.
We work theweights out in the same way as in documents vectors:= O~k "log2 ~-  WCtkWhere t~k is the number of times that term zoccurswithin documents assigned to category k, and cfiis thenumber of categories within term # occurs.
For exam-ple, aRer selecting and weighting categories, the high-frequency term" export" shows its largest weight forcategory trade, but it also shows large weights forgrain or wheat, andsmall weights for belgtan-francand wool.
A less frequent erm typically provides evi-dence for asmaller number of categories.
For example,"private" has a large weight only for acq (acquisition),and medium for earn (earnings) and trade.3.5 Integrat ing WordNet  and a Train-ingCol leet ionSeveral ways of integrating WordNet and Reuters haveoccurred to us.
Asensible one is to use concepts in-stead of terms as representatives.However, and al-though promising, Voorhees \[1993\] reported no im-provements with this idea.On the other side, we haverealized that the shortcomings in training canbe cor-rected using WordNet to provide better forecast of lowfrequencycategories.In general, we have linked WordNet weight vectorsto training weigth vectors.
First we have removedthose WordNet terms not ocurring in thetraining col-lection.
Then we have normahzed both WordNet vec-tors andtraining vectors to separately add up acrosseach category.
This way we have smoothed trainingweights (much larger than WordNetones), giving equalinfluence to each kind of term weight.
This tech-niqueresults in 461 term weights vectors, 185 comingfrom WordNet, and 286 fromtraining.
Weights forterms ocurring in both sets have beensummed.Examples of terms coming from training are"import" or"government," with high weights forhighly frequent categories, like acq.
Examplesof termscoming from WordNet are "petroleum" or" peanut,"with wezghts only for the correspondingcategoriescrude and groundnut respectively.We can clearly identify the role of each resource inthis TCapproach.
WordNet supplies information on thesemantic relatedness of termsand categories whentraining data is no longer available or reliable It di-rectly contributes with part of the terms used in thevector representation.
On the other side, the trainingcollection supplies terms for those categories that arebetter trained The problem of unavailabilityof trainingdata is then overcome through the use of an externresource.4 Eva luat ionEvaluation of TC and other text classification opera-tions exhibits greatheterogeneity.
Several metrics andtest collections have been used fordifferent approachesor works.
This results in a lack of comparabilityamong the approaches,forcing to replicate xperimentsfrom other researchers.
Trying to minimize this prob-lem, we havechosen a set of very extended metrics anda frequently used free testcollection for our work.
Themetrics are recall and precision, and the testcollectionis, as introduced before, Reuters-22173.
Before step-ping into the actual results, we provide acloser look tothese elements.4.1 Evaluat ion metricsThe VSM promotes recall and precision based evalua-tion, but there are several ways of calculating or evendefining them.
Wefocus on recall, being the discussionanalogous for precismn.
First,definition can be givenregarding categories or documents \[Larkey andCroft,1996\].
Second, computation can be done macro-averaging or micro-averaging \[Lewis, 1992\]._ Recall can be defined as the number of correctly as-signed documents to a category over the number ofdocuments to becorrectly assigned to the category.But a document-oriented definition is also possible:the number of correctly assigned categories toadocument over the number of correct categories tobe assigned to thedocument.
This later definition ismore coherent with the task, but theformer allowsto identify the most problematic categories._ Macro-averaging consists of computing recall andprecision for every item (document or category) inone of both previous ways, and averaging aRer it.Micro-averaging is adding up all numbers of cor-rectly assigned items, items assigned, and items tobe assigned, and calculate only one value of recalland precision.
When micro-averaging, no distinc-tion about document or category orientation can bemade.
Macro-averaging assigns equal weight toevery category, while micro-averaging is influencedby most frequent categories.Evaluation depends finally on the category assignmentstrategy: probabihty thresholding, k-per-doe assign-ment, etc.
Strategies define the way to produce re-call/precision tables.
For instance, if similarities arenormalized to the \[0,1\] interval, eleven levels of prob-41PATTERN-ID 6505 TRAINING-SET18-JUN-1987 11:44:27.20TOPICS: bop trade END-TOPICSPLACES: italy END-PLACESPEOPLE: END-PEOPLEORGS: END-ORGSEXCHANGES: END-EXCHANGESCOMPANIES: END-COMPANIESITALIAN BALANCE OF PAYMENTS IN DEFICIT IN MAYROME, June 18 - Italy's overall balance of payments showeda deficit of 3,211 bllllon izre in May compared with a surplusof 2,040 bil l ion in April, provxsional Bank of Italy figureshow.The May deflclt compares with a surplus of 1,555 billionlire an the corresponding month of 1986.For the flrst five months of 1987, the overall balance ofpayments showed a surplus of 299 bil l lon lire agalnst a deficitof 2,854 bil l lon in the corresponding 1986 perlod.REUTERability threshold can be set to0.0, 0.1, and so.
Whenthe system performs k-per-doe assignment, he valueof k is ranged from 1 to a reasonable maximum.Figure 1We must assign an unknown number of categories toeach document in Reuters.
So, the probabdity thresh-olding approach seems the most sensible one.
We havethen computed recall and precision for eleven ,levels ofthreshold, both macro and micro-averaging.
Whenmacro-averaging, we have used the category-orienteddefinition of recall and precision.
After that, we havecalculated averages of those eleven values in order toget single figures for comparison.4.2 The Test  Co l lec t ionThe Reuters-22173 collection consists of 22,173newswire articles from Reuters collected uring 1987.Documents in Reuters deal with financial topics, andwere classified in several sets of financial categoriesby personnel from Reuters Ltd. and Carnegie GroupInc.
Documents vary in length and number of catego-ries assigned, from 1 line to more than 50, and fromnone categories to more than 8.
There are five sets ofcategories: TOPICS, ORGANIZATIONS,EXCHANGES, PLACES, and PEOPLE.
As othersbefore, we have selected the 135TOPICS for our ex-periments.
An example of  news article classified inbop (balance of payments) and trade is shown in Fig-ure 1.
Some spurious formatting has been removedfrom it.eral partitions have been suggested for Reuters \[Lewis,1992\], among which ones we have opted for the mostgeneral and difficult one.
First 21,450news stories areused for training, and last 723 are kept for testing.
Wesummarize significant differences between test andtraining sets in Table 2.
These differences can bringnoise into categorization, because training relies onsimilarity between training and test documents.
Nev-ertheless, this 21,450/723 partition has been used be-fore \[Lewis, 1992; Hayes and Weinstein, 1990\] andinvolves the general case of documents with no cate-gories assigned.We have worked with raw data provided in theReuters distribution.
Control characters, numbers andseveral separators like"/" have been removed, andcategories different from the TOPICS set have beenignored.
For disambiguating categories with respect oWordNet senses, we first had to acquire their meaning,not always self-evident This task has been performedby direct examination of training documents.4.3 Resu l ts  and In terpretat ionThe results of our first series of experiments are sum-marized in Table 3.This table shows recall and preci-sion averages calculated both macro and micro-averaging for a threshold-based assignment strategy.Values for the integrated approach show some generaladvantage over WordNet and training approaches, butresults are not decisive.
Training results are compara-ble with those from Lewis \[1992\], and the WordNetapproach is roughly equivalent to the training one.Does Number0Words OcurrsDocAvgDoes with Number1+ Topics PercentTopics OcurrsDecAysSubcollectmnTralnn'l~21,4502,851,455127I 1,098 i5213,7560 64Test Total723 22,173140,922 2,992,377195 134566 11,66478 53896 14,6521 24 0 66Table 2.
Reuters-22173 stat~stlcsWhen a test collection is provided, it is customary todivide it into a training subset and a test subset.
Sev-Threshold Macro-averagingstrategy Recall i PrecisionDirect i 0 239302 0 242661WordNet 0 324899 0 306445Training 0 325586 0 188701i Integrated 0 373365 0220186Table 3.
Overall results from our exMwro-averagmgRecall Precision0.205849 0 2357750 260762 0 2983630365988 02757310418652 0296423)erlmentsOn one hand, the integrated approach shows a betterperformance than the WordNet one in general, al-though a problem of precision is detected when macro-averaging.
The influence of low precision training hasproduced this effect.
We are planning to strengthen42WordNet influence to overcome this problem.
On theother hand, the integrated approach reports better gen-eral performance than the training approach.As expected, WordNet and training both beat thedirect approach.
When comparing WordNet and train-ing approaches, we observe that the former producesbetter esults with categories of low frequency, whilethe latter performs better in highly frequent categories.However, both exhibit the same overall behaviour.Differences in categories are noticed by the fact thatmicro-averaging is influenced by highly frequent ele-ments, while macro-averaging depends on the resultsof many elements of low frequency.5 Related WorkText categorization has emerged as a very active fieldof research in the recent years.
Many studies havebeen conducted to test the accuracy of training meth-ods, although much less work has been developed inlexical database methods.
However, lexical databasesand especially WordNet have been often used for othertext classification tasks, like word sense disambigua-tion.Many different algorithms making use of a trainingcollection have been used for TC, including k-nearest-neighbor algorithms \[Masand et al, 1992\], Bayesianclassifiers \[Lewis, 1992\], learning algorithms based inrelevance feedback \[Lewis et al, 1996\] or in decisiontrees \[Apte t al., 1994\], or neural networks \[Wiener etal., 1995\].
Apart from Lewis \[1992\], the closest ap-proach to ours is the one from Larkey and Croft\[1996\], who combine k-nearest-neighbor, Bayesianindependent and relevance feedback classifiers,showing improvements over the separated approaches.Although they do not make use of several resources,their approach tends to increase the information avail-able to the system, in the spirit of our hypothesis.To our knowledge, lexical databases have been usedonly once in TC.
Hearst \[1994\] adapted a disambigua-tion algorithm by Yarowsky using WordNet to recog-nize category occurrences.
Categories are made ofWordNet erms, which is not the general case of stan-dard or user-defined categories.
It is a hard task toadapt WordNet subsets to pre-existing categories,especially when they are domain dependent.
Hearst'sapproach shows promising results confirmed by thefact that our WordNet -based approach performs atleast equally to a simple training approach.Lexical databases have been employed recently inword sense disarnbiguation.
For example, Agirre andRigan \[1996\] make use of a semantic distance thattakes into account structural factors in WordNet forachieving good results for this task.
Additionally,Resnik \[1995\] combines the use of WordNet and a textcollection for a definition of a distance for disambigu-ating noun groupings.
Although the text collection isnot a training collection (in the sense of a collection ofmanually labeled texts for a pre-defined text process-ing task), his approach can be regarded as the mostsimilar to ours in the disambiguation task.
Finally, Ngand Lee \[1996\] make use of several sources of infor-mation inside a training collection (neighborhood, partof speech, morphological form, etc.)
to get good re-sults in disambiguating unrestricted text.We can see, then, that combining resources in TC isa new and promising approach supported by previousresearch in this and other text classification operations.With more information extracted from WordNet andbetter training algorithms, automatic TC integratingseveral resources could compete with manual indexingin qua!ity, and beat it in cost and efficiency.6 Conclusions and Future WorkIn this paper, we have presented a multiple resourceapproach for TC.
This approach integrates the use of alexical database and a training collection in a vectorspace model for TC.
The technique is based on im-proving the language of representation constructionthrough the use of the lexical database, which over-comes training deficiencies.
We have tested our ap-proach against training algorithms and lexical databasealgorithms, reporting better esults than both of thesetechniques.
We have also acknowledged that a lexicaldatabase algorithm can rival training algorithms in realworld situations.Two main work lines are open: first, we have toconduct new series of experiments ocheck the lexicaldatabase and the combined approaches with othermore sophisticated training approaches; econd, wewill extend the multiple resource technique to othertext classification tasks, like text routing or relevancefeedback in text retrieval.References\[Agirre and Rigau, 1996\] E. Agirre and G. Rigau.Word sensedisambiguation using conceptual distance.In Proceedings of COLING, 1996.\[Apte et al, 1994\] C. Apte, F Damerau, and S.W.Weiss.Automated learning of decision rules for textcategorization.
ACMTransaetions m lnformatton Sys-tems, Vol.
12, No.
3, 1994.\[Boguraev and Pustejovsky, 1996\] B. Boguraev and J.Pustejovsky, J.(Eds.).
Corpus Processing for LexicalAcqutsltton.
The MIT Press, 1996.\[Harman, 1996\] D. Harman.
Overview of the ForthText RetrievalConference (TREC-4).
In Proceedingsof the Fourth Text RetrievalConference, 1996.\[Hayes and Weinstein, 1990\] P.J.
Hayes and S.P.
We-instein.CONSTRUE/TIS: a system for content-basedindexing of a database of newsstories.
In Proceedingsof the Second Annual Conference on lnnovattveApph-cattons of Arttfictal Intelhgence, 1990.\[Hearst, 1994\] M. Hearst.
Context and structure inautomatedfull-text mformatton access.
Ph D. Thesis,Computer Science Division,University of California atBerkeley, 1994.\[Hersh et al, 1994\] W. Hersh, C. Buckley, T.J. Leone,and D.Hlckman.
OHSUMED: an interactive retrievalevaluation and new large testcollection for research.
InProceedmgs of the ACM SIGIR, 1994.43\[Karlgren and Cutting, 1994\] J. Karlgren and D. Cut-ting.
Recogninzingtext genres with simple metricsusing discriminant analysis.
In Proceedings ofCOLING, 1994.\[Larkey and Croft, 1996\] L.S.
Larkey and W.B.
Croft.Combiningclassifiers in text categorization.
In Pro-ceedings of the ACMSIGIR, 1996.\[Lewis et al, 1996\] D.D.
Lewis, R.E.
Schapire, J.P.Callan, andR.
Papka.
Training algorithms for lineartext classifiers.
In Proceedings of the ACM SIGIR,1996.\[Lewis, 1992\] D.D.
Lewis.
Representation a d learn-ingin information retrieval.
Ph.
D. Thesis, Dept.
ofComputer and InformationScience, University of Mas-sachusetts, 1992.\[Masand et al, 1992\] B. Masand, G. Linoff, and D.Waltz.Classifying news stories using memory basedreasoning.
In Proceedingsofthe ACMSIGIR, 1992.\[Miller, 1995\] G. Miller.
WordNet: a lexical databasefor English.Communications f the ACM, Vol.
38, No.11, 1995.\[Ng and Lee, 1996\] H.T.
Ng and H.B.
Lee.
Integratingmultipleknowledge sources to disambiguate wordsense: an exemplar based approach.In Proceedings ofthe ACL, 1996.\[Resnik, 1995\] P. Resnik.
Disambiguating noungroupings with respectto WordNet senses.
In Pro-ceedings of the Third Workshop onVery Large Cor-pora, 1995.\[Salton and McOill, 1983\] G. Salton and M.J. McGill.lntroductionto modern information retrieval.
McGraw-Hill, 1983.\[Salton, 1989\] G. Salton.
Automating text processing:thetransformation, a alysis and retrieval of informa-tion bycomputer.
Addison-Wesley, 1989.\[Voorhees, 1993\] E.M. Voorhees.
Using WordNet todisambiguate wordsenses for text retrieval.
In Pro-ceedings of the ACM SIGIR,1993.\[Wiener et al, 1995\] E.D.
Wiener, J. Pedersen andA.S.
Weigend.A neural network approach to topicspotting.
In Proceedings oftheSDAIR, 1995.\[Yokoi, 1995\] T. Yokoi.
The EDR electronic diction-ary.
Communications of the ACM, Vol.
38, No.ll,1995.44
