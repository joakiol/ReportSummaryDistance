The (Non)Utility of Predicate-Argument Frequencies for PronounInterpretationAndrew Kehler?UC San Diegoakehler@ucsd.eduDouglas AppeltSRI Internationalappelt@ai.sri.comLara Taylor?UC San Diegolmtaylor@ucsd.eduAleksandr Simma?UC San Diegoasimma@ucsd.eduAbstractState-of-the-art pronoun interpretation sys-tems rely predominantly on morphosyntac-tic contextual features.
While the use ofdeep knowledge and inference to improvethese models would appear technically in-feasible, previous work has suggested thatpredicate-argument statistics mined fromnaturally-occurring data could provide auseful approximation to such knowledge.We test this idea in several system configu-rations, and conclude from our results andsubsequent error analysis that such statis-tics offer little or no predictive informationabove that provided by morphosyntax.1 IntroductionThe last several years has seen a number of worksthat use weight-based systems (trained either man-ually or via supervised learning) for pronoun in-terpretation, in addition to others that have ad-dressed the broader task of entity-level coreference(see Mitkov (2002) for a useful survey).
These sys-tems typically rely on a variety of morphosyntacticfactors that have been posited in the literature toaffect the interpretation of pronouns in naturally-occurring discourse, including gender and numberagreement, the distance between the pronoun andantecedent, the grammatical positions of the pro-noun and antecedent, and the linguistic form of theantecedent, among others.
A common refrain is thatthe performance of systems that rely on such fea-tures is plateauing, and that further progress will re-quire the use of world knowledge and inference (ibid.,Ch.
9, inter alia).
World knowledge, after all, wouldseem to play a role in determining that the referent ofit in example (1) is the entity denoted by his industryrather than Glendening?s initiative or the edge.
?Department of Linguistics.
?Department of Computer Science and Engineering.
(1) He worries that Glendening?s initiative couldpush his industry over the edge, forcing it toshift operations elsewhere.Of course, no well-suited knowledge base and accom-panying inference procedure exists that can deliversuch a capability robustly in an open domain.In lieu of this capability, previous authors havesuggested that what can be viewed as a more su-perficial form of semantic information ?
predicate-argument statistics mined from naturally-occurringdata ?
could be used to capture certain selectionalregularities.
For instance, such statistics might re-veal that forcing industry is a more likely verb-object combination in naturally-occurring data thanforcing initiative or forcing edge.
Assuming thatsuch statistics imply that industries are more likelyto be forced in the real world than are initiatives oredges, this information could be taken to establish apreference for his industry as the antecedent of it in(1).
While there will always be cases that require ar-bitrarily deep knowledge for their interpretation, theempirical question of how far one can go by relyingon this sort of selectional information remains.Our point of departure is the work of Lappinand Leass (1994, henceforth L&L) and Dagan etal.
(1995).
(See also Dagan and Itai (1990).)
L&Ldemonstrated with a system called RAP that a(manually-tuned) weight-based scheme for integrat-ing pronoun interpretation preferences can achievehigh performance on real data, in their case, 86%accuracy on a corpus of computer training manu-als.1 Dagan et al (1995) then developed a postpro-cessor based on predicate-argument statistics thatwas used to override RAP?s decision when it failedto express a clear preference between two or moreantecedents, which resulted in a modest rise in per-1Kennedy and Boguraev (1996, henceforth, K&B)adapted L&L?s algorithm to rely on far less syntac-tic analysis (noun phrase identification and rudimentarygrammatical role marking), with performance in the 75%range on mixed genres.formance (2.5%).2 Because RAP is symbolic, thetwo systems were necessarily coupled in a black-box manner.
They noted, however, that if one hada statistically-driven pronoun interpretation system,co-occurrence information could be modeled along-side morphosyntactic information:?A promising direction for future researchis the development of an empirically basedmodel for salience criteria analogous to theone that we constructed for lexical prefer-ence.
The integration of these models usinga probabilistic decision procedure will hope-fully yield an optimized integrated systemfor anaphora resolution.?
(p. 643)In this work we set out to evaluate Dagan et al?sproposal.
Indeed, the weight-combination scheme ofL&L is suggestive of a particular approach to super-vised learning ?
maximum entropy (MaxEnt) ?
inwhich such a system of weights is inferred from max-imum likelihood counts on annotated data.
UsingMaxEnt, we trained a system based on an optimizedset of morphosyntactic features and augmented itwith predicate-argument statistics in two scenarios:(i) one mimicking the Dagan et al postprocessor,and (ii) one in which the predicate-argument statis-tics were represented as features alongside the mor-phosyntactic features.
Our results and subsequenterror analysis suggest, however, that such statisticsoffer little or no predictive information above thatprovided by morphosyntax.2 Corpora UsedThe training and test data sets came from the news-paper and newswire segments of the Automatic Con-tent Extraction (ACE) program corpus.
The train-ing data contained 2773 annotated third-person pro-nouns, and the test data (the February 2002 evalua-tion set) contained 762 annotated third-person pro-nouns.
The performance statistics on the test datareported here are from the only time an evaluationwith this data was performed; progress during devel-opment was estimated solely via jackknifing on thetraining data.The annotated pronouns included only those thatwere ACE ?markables?, i.e., ones that referred to en-tities of the following types: Persons, Organiza-tions, GeoPoliticalEntities (politically defined2The difference amounted to 9 additional correct pre-dictions in a corpus of 360 examples.
They express a be-lief that the improvement is real, but acknowledge thatthey would need twice as many examples in their corpusto reach statistical significance.geographical regions, their governments, or theirpeople), Locations, and Facilities.
Thus, therewere pronouns in both the development and (pre-sumably) test sets for which there were no annota-tions.
As such, certain problems that real-world sys-tems face, such as non-referential (e.g., ?pleonastic?
)pronouns and pronouns that refer to eventualities,did not have to be dealt with.
(However, these pro-nouns were possible antecedents to other pronouns,and thus were sometimes mistakenly selected as thecorrect antecedent.)
Thus, our results are not nec-essarily comparable to those of a system that dealswith these difficulties (although previous work variesa fair bit on how their datasets were filtered in this re-gard).
Our main purpose here is to establish a state-of-the-art baseline with which to assess the contribu-tion of predicate-argument frequency information.3 Learning AlgorithmsWe implemented three pronoun interpretation sys-tems: a MaxEnt model, a Naive Bayes model, anda version of the Hobbs algorithm as a baseline.
Ourexperimentation was driven predominantly using theMaxEnt system using an n-fold jackknifing paradigm(n was typically three).
Naive Bayes was imple-mented toward the end of the project as a machinelearning baseline.
Both machine learning algorithmswere trained as binary coreference classifiers, thatis, the examples provided to them consisted of pair-ings of a pronoun and a possible antecedent phrase,along with a binary coreference outcome determinedfrom the annotated keys.
Thus, for a given pronounthere was one example generated for each possibleantecedent phrase.
So as to focus learning on onlythe coreferential phrase that is most likely to havebeen directly responsible for a given pronominaliza-tion, all coreferential phrases except the closest interms of Hobbs distance (discussed later) were elimi-nated before training.
Because we are ultimately in-terested in identifying the correct antecedent amonga list of possible ones, during testing the antecedentassigned the highest probability was chosen.These systems received as input the results ofSRI?s TextPro system, a chunk-style shallow parsercapable of recognizing low-level constituents (noungroups, verb groups, etc.).
No difficult attachmentsare attempted, and the results are errorful.
Therewas no human-annotated linguistic information inthe input.
The systems are described further below.Maximum Entropy Modeling As previously in-dicated, the weight-based scheme of L&L suggestsMaxEnt modeling (Berger et al, 1996) as a particu-larly natural choice for a machine learning approach.In MaxEnt, the parameters of an exponential modelof the following form are estimated:p(y|x) = e?i ?ifi(x,y)?y e?i ?ifi(x,y)The variable y represents the outcome (coreferenceor not) and x represents the context.
There isone value for each feature that predicts coreferencebehavior, represented by the parameters ?1, ..., ?n,which are Lagrange multipliers that constrain the ex-pected value of each feature in the model to be thevalues found in the distribution of the training data.
(The fi(x, y) are indicator functions which equal 1when the corresponding feature is present, and 0otherwise.)
The desired values for these parame-ters are obtained by maximizing the likelihood ofthe training data with respect to the model.3 Thus,whereas L&L?s RAP system uses an additive systemof weights that is trained manually, the MaxEnt sys-tem learns a multiplicative system of weights auto-matically.
One can view the MaxEnt system as yield-ing a probabilistic notion of antecedent salience: Thesalience value assigned to a potential antecedent ofa given pronoun is just the probability that Maxentassigns to the outcome of coreference.Naive Bayes In Naive Bayes modeling, a Bayesianprobability distribution is estimated under a strongassumption: that all of the features are conditionallyindependent given the target value.
Thus given nfeatures xi with respect to the context x, we have:p(y|x) = p(y)p(x|y)p(x) ?p(y) ?ni=1 p(xi|y)p(x)The context x is constant for each outcome y, so weonly need to find:argmaxy?
{0,1}p(y)n?i=1p(xi|y)For most natural language processing scenarios, in-cluding ours, this independence assumption is almostcertainly false.
Nonetheless, Naive Bayes modelsseem to work well in practice when used as classifiers.That is, the choice that receives the highest probabil-ity (relative to the other choices) is often the correctone even though the actual probabilities the modelgenerates may not be very good.
These models havethe advantage that they are efficiently trained; onlya single pass through the training data is necessary.3The results reported here were produced by using theimproved iterative scaling algorithm with binary-valued features.
We also experimented with real-valuedfeatures, with highly similar results on jackknifed data.Hobbs Algorithm We also implemented a versionof Hobbs?s (1978) well-known pronoun interpretationalgorithm as a baseline, in which no machine learningis involved.
His algorithm takes the syntactic repre-sentations of the sentences up to and including thecurrent sentence as input, and performs a search foran antecedent noun phrase on these trees.
Since ourshallow parsing system does not build full syntactictrees for the input, we developed a version that doesa simple search through the list of noun groups recog-nized.
In accordance with Hobbs?s search procedure,noun groups are searched in the following order: (i)in the current sentence from right-to-left, startingwith the first noun group to the left of the pronoun,(ii) in the previous sentence from left-to-right, (iii) intwo sentences prior from left-to-right, and (iv) in thecurrent sentence from left-to-right, starting with thefirst noun group to the right of the pronoun (for cat-aphora).
The first noun group that agrees with thepronoun with respect to number, gender, and personis chosen as the antecedent.4 FeaturesOur automatically trained systems employed a set ofhard constraints and soft features.
Hard con-straints are used to weed out potential antecedentsbefore they are sent to the machine learning algo-rithm.
There are only two such constraints, onebased on number agreement and one based on gen-der agreement.
Both are conservative in their appli-cation.
The soft features are used by the machinelearning algorithm.
After considerable experimenta-tion we settled on a set of forty such features, notincluding predicate-argument features that will bedescribed in Section 5.
These features fall into fivecategories, listed here with abbreviations that will beused in the tables given in Section 6:Gender Agreement (gend): Includes features totest a strict match of gender (e.g., a male pro-noun and male antecedent), as well as merecompatibility (e.g., a male pronoun with an an-tecedent of unknown gender).
These featuresare more liberal than the gender-based hard con-straint mentioned above.Number Agreement (num): Includes features totest a strict match of number (e.g., a singu-lar pronoun and singular antecedent), as wellas mere compatibility (e.g., a singular pro-noun with an antecedent of unknown number).These features are likewise more liberal than thenumber-based hard constraint mentioned above.Distance (dist): Includes features pertaining tothe distance between the pronoun and the po-tential antecedent.
Examples include the num-ber of sentences between them and the ?Hobbsdistance?, that is, the number of noun groupsthat Hobbs?s search algorithm has to skip be-fore the potential antecedent is found (Hobbs,1978; Ge et al, 1998).Grammatical Role (pos): Includes features per-taining to the syntactic position of the potentialantecedent.
Examples include whether the po-tential antecedent appears to be the subject orobject of a verb, and whether the potential an-tecedent is embedded in a prepositional phrase.Linguistic Form (lform): Includes features per-taining to the referential form of the potentialantecedent, e.g., whether it is a proper name,definite description, indefinite NP, or a pronoun.The values of these features ?
computed from oursystem?s errorful shallow constituent parses ?
com-prised the input to the learning algorithms, alongwith the outcome as indicated by the annotated key.5 Predicate-Argument FrequenciesWith a trained statistical model for pronoun inter-pretation in hand, we can now consider the use ofpredicate-argument statistics to improve it.
Con-sider sentence (1) again, repeated as (2).
(2) He worries that Glendening?s initiative couldpush his industry over the edge, forcing it toshift operations elsewhere.Suppose that our system selects the edge as the an-tecedent of it instead of his industry.
It turns outthat in a large corpus of shallowly-parsed data (par-ticularly the newswire subset of the TDT-2 corpus,see below), industr(y|ies) appears nine times as thehead of the object noun phrase of force (in its var-ious number/tense combinations), whereas edge(s)never does.4 So by collecting predicate-argument co-occurrence statistics, one could extract the ?knowl-edge?
that industries are (statistically speaking)more likely to be forced than edges are, and pos-sibly use this information to change the predictionof the statistical model.We utilized three types of predicate-argumentstatistics in our experiments: subject-verb, verb-object, and possessive-noun.
We processed the entire4Likewise, the subject-verb combination industr(y|ies)shift occurs three times in the corpus whereas edge(s)shift does not.newswire subset of the Topic Detection and Tracking(TDT-2) corpus with TextPro, which resulted in1,321,072 subject-verb relationships, 1,167,189 verb-object relationships, and 301,477 possessive-noun re-lationships.
Words were categorized by their lem-mas when available, and proper names for each ofthe ACE entity types were classified into respectiveclasses (i.e., proper person names all counted as in-stances of proper person).While counts were collected for a broad rangeof predicate-argument combinations, there were stillmany combinations that were only seen once ortwice, and certainly other possible combinations ex-ist that were not seen at all.
The distributionthat these statistics yield therefore needed to besmoothed.
We took two approaches to smooth-ing.
First, because Dagan et al used Good-Turingsmoothing in their experiments, we did likewise soas to replicate their work as closely as possible.
Sec-ond, we tried an approach based on the distributionalclustering method of Pereira et al (1993).
Thismethod yielded word classes that offered more ro-bust count approximations for their member words.However, both methods yielded similar results whenembedded in the larger system, and so we will reporton the results of using Good-Turing so as to remainmore directly comparable to Dagan et alThe smoothed predicate-argument statistics wereemployed in two ways.
First, we built a postpro-cessing filter modeled directly on Dagan et al?s sys-tem.
Their implementation made use of two equa-tions.
The first computes the frequency with whicha candidate head noun C is found with the predi-cate word A, normalized by the number of times Cis found alone, so as to not bias the statistic towardswords that are common in isolation:stat(C) = P (tuple(C,A) | C) = freq[tuple(C,A)]freq(C)The second equation then weighs the difference instatistical co-occurrence against the different saliencevalues assigned by the pronoun interpretation mod-ule for two competing candidates C1 and C2:ln(stat(C2)stat(C1))> K ?
[salience(C1)?
salience(C2)]The parameter K determines the threshold at whichstatistical preferences supersede salience preferences.In our implementation, the measure of salience issimply the probability of coreference assigned by thestatistical model.
Another parameter max sets athreshold for the maximum difference between thesalience values for the two candidates; any pair forwhich this difference exceeds max will not be con-sidered.
For each combination of feature sets thatwe evaluated (see Section 6), we performed addi-tional experiments to determine the optimal valuesof K and max.
To keep consistent with Dagan et al,statistics were not used (here or in the other MaxEntsystem) for potential antecedents that were them-selves pronominal.
To properly use statistics in suchcases, the system would need to have access to anantecedent for the pronoun that has a lexical head;neither model was given access to such information.In our second approach, we simply developed fea-tures that represent the magnitude of the predicate-argument statistics and utilized them during Max-Ent training along with the morphosyntactic featuresdescribed earlier.
The statistics were normalized bydividing them by the total counts for the head ofthe potential antecedent in the relevant predicate-argument configuration.5In certain respects these different system config-urations mirror questions about pronoun interpre-tation that linger in the theoretical and psycholin-guistics literature.
A result showing that the post-processing filter version works best might provideevidence, as has been suggested, that people pri-marily use morphosyntactic features to resolve pro-nouns, relying on semantic information only whenmore than one possibility remains active.
A resultshowing that the integrated version works best mightsuggest that semantic information is used in concertwith morphosyntactic information.
Finally, a resultshowing that neither version improves performancemight suggest that morphosyntactic information isthe dominant determinant of pronoun interpretation,and/or that any semantic information utilized is notobtained primarily from superficial cues.
The resultsare reported in the next section.6 ResultsOur final MaxEnt system used 40 features, whichwere categorized into five classes in Section 4.
Toget a sense for the relative contributions of each fea-ture type, we ran evaluations with all 25 (32) possiblecombinations of these five groups.
We first report re-sults on the held-out training data, and then providethe blind test results.
Table 1 provides the resultson the held-out sections of the training data dur-ing 3-fold jackknifing for a sample of five of these 32combinations.
The four rightmost columns representthe results from: (i) MaxEnt with no frequency fea-tures (MaxEnt), (ii) MaxEnt with frequency features5Experiments with unnormalized counts were also runon jackknifed data with similar results.included during training (MaxEnt-Features), (iii)MaxEnt with Dagan et al postprocessing (MaxEnt-Postprocessing), and (iv) Naive Bayes without fre-quencies.
Experiments with n-fold jackknifing forother values of n produced similar results.
Daganet al postprocessing was not attempted with NaiveBayes since the postprocessor makes crucial use ofthe probabilities the model assigns to competing an-tecedents, and as previously mentioned, the actualprobabilities assigned by Naive Bayes are not neces-sarily reliable due to the independence assumptionsit makes.The testing phase breaks ties with respect to theorder imposed by Hobbs?s algorithm.
In the case inwhich no features were used during ?training?
(seethe first row of Table 1, columns 2 and 5), the modelswill produce the same probability for each possibleantecedent.
Thus, these experiments reduce to usingthe Hobbs algorithm, which, performing at 68.23%accuracy,6 provides a nontrivial baseline.
As can beseen, adding groups of additional features incremen-tally improves performance, up to a final result of76.16% for MaxEnt using all morphosyntactic fea-tures, and a comparable 76.24% for Naive Bayes.In the end, the predicate-argument statistics pro-vided little if any value, used either as features dur-ing MaxEnt training or for Dagan et al postpro-cessing.
In the best-performing MaxEnt system con-figuration (see bottom row), the statistics improveperformance by less than 0.5%.
Interestingly, perfor-mance was hurt when only statistical features wereused (65.71% in MaxEnt-Features and 66.25% inMaxEnt-Postprocessing) as compared to none at all(68.23%).
Whereas the Hobbs algorithm ranks all ofthe potential antecedents when no features are used,it only breaks ties in the MaxEnt-Features systemthat remain after statistical features order the po-tential antecedents, and the MaxEnt-Postprocessorsystem uses statistics to rerank the Hobbs orderingbetween potential antecedents after the fact.
Thisreranking proved detrimental in both cases.Table 2 provides the final results of blind test eval-uation for the same five combinations of feature sets.The final result of the system without predicate-argument statistics was 75.72%, which is presumablyreasonable performance considering that the systemdoes not rely on fully-parsed input and lacks access6All results are reported here in terms of accuracy,that is, the number of pronouns correctly resolved dividedby the total number of pronouns read in.
Correctness isdefined with respect to anaphor-antecedent relationships:a chosen antecedent is correct if the ACE keys place thepronoun and antecedent in the same coreference class.Features MaxEnt MaxEnt-Features MaxEnt-Postprocessing Naive Bayesnone .6823 .6571 .6625 .6823num, gend .6870 .6863 .6841 .6859num, gend, dist .7274 .7386 .7461 .7313num, gend, dist, pos .7425 .7465 .7505 .7436num, gend, dist, pos, lform .7616 .7663 .7656 .7624Table 1: Results from jackknifing on training datato world knowledge.7 In this case, the integrated fea-ture system performed identically, whereas the post-processor system displayed a performance improve-ment of about 1% (a difference of 8 pronouns).The MaxEnt results on the test data suffered onlya minimal (and in a few cases, no) loss from thoseon the held-out data.
Overtraining appears to havebeen kept to a minimum; the generality of the fea-tures was perhaps responsible for this.8 The resultsfrom Naive Bayes generalized less well, exhibiting a2% decrement on the test evaluation.
The Hobbs al-gorithm, which is not trained, exhibited similar per-formance on both sets of data.7 Error AnalysisThere are a variety of possible reasons why thepredicate-argument statistics failed to markedly im-prove performance in each of the system configura-tions.
While it could be that such statistics are sim-ply not good predictors for pronoun interpretation,data sparsity in the collected predicate-argumentstatistics could also be to blame.We carried out an error analysis to gain further in-sight into this question.
To address the data-sparsityissue, we employed the technique used in Keller andLapata (2003, K&L) to get a more robust approx-imation of predicate-argument counts.9 We wrote7These performance results include 64 ?impossible?cases in which, due to misparsing, no correct antecedentswere provided to the model; hence 91.6% accuracy is thebest that could be achieved.
The results likewise includeerrors in which the model selected a bogus antecedentthat resulted from a misparse.8As such, informal post-hoc experiments with Gaus-sian smoothing (Chen and Rosenfeld, 2000) failed to im-prove performance.9K&L use this technique to obtain frequencies forpredicate-argument bigrams that were unseen in a givencorpus, showing that the massive size of the web out-weighs the noisy and unbalanced nature of searches per-formed on it to produce statistics that correlate well withcorpus data.
We are admittedly extending this reason-ing to relations between the heads of predicates and ar-guments without establishing that K&L?s technique sogeneralizes, but we nonetheless feel that it is sufficientfor the purpose of an exploratory error analysis.
The re-a script to collect the number of pages that theAltaVista search engine found for each predicate-argument combination and its variants per the fol-lowing schema, modeled directly after K&L:Subject-Verb: Search for occurrences of the com-binations N V where N is the singular or pluralform of the subject head noun and V is the in-finitive, singular or plural present, past, perfect,or gerund of the head verb.Verb-Object: Search for occurrences of the com-binations V Det N , where V and N are asabove for the verb and object head noun respec-tively, and Det is the determiner the, a(n), orthe empty string.Possessive-Noun: Search for occurrences of thecombinations Poss N , where Poss is the sin-gular or plural form of the possessive and N isthe singular or plural form of the noun.As in K&L, all searches were done as exact matches.The results for all of the different form combinationstotaled together comprised the unnormalized counts.We also computed normalized counts, in which theunnormalized count was divided by the total num-ber of pages AltaVista returned for the head of thecandidate antecedent, so that, as before, the countswould not unduly bias antecedents with head wordsthat occurred frequently in isolation.We created a list of those examples for which theMaxEnt model ?
trained with all 5 groups of themorphosyntactic features activated, but not any sta-tistical ones ?
made incorrect predictions during 3-fold jackknifing on the training data.
(We used held-out data so that our test data would remain blind.
)We then pared the list down to a reasonable sizefor manual analysis in a variety of ways.
First, ofcourse, only those examples that fall into one of thethree predicate-argument configurations with whichwe are concerned were included (most were).
Second,we filtered out the cases in which either the mostproximal correct antecedent (with proximity definedsults we received from this technique held few surprises.Features MaxEnt MaxEnt-Features MaxEnt-Postprocessing Naive Bayesnone .6877 .6496 .6627 .6877num, gend .6667 .6745 .6719 .6654num, gend, dist .7336 .7415 .7428 .7297num, gend, dist, pos .7441 .7507 .7520 .7441num, gend, dist, pos, lform .7572 .7572 .7677 .7415Table 2: Results of final blind test evaluationwith respect to the Hobbs algorithm?s search order)or the antecedent chosen by the model was a propername.
Because all proper names of each ACE typewere classed together in our experiments, statisticswould not make different predictions for two suchnames.
While statistics could differentiate betweena potential proper name antecedent and one headedby a common noun (and presumably did in our ex-periments), we could not use K&L?s method on thosecases unless we used the actual proper name insteadof the category in the search query ?
this would likelycreate an undue bias to the other antecedent; con-sider comparing counts for lawyer argued with thosefor Snodgrass argued.
Third, we filtered out cases inwhich either the chosen antecedent or most proximalcorrect antecedent was itself a pronominal, for thereasons given in Section 5.
Lastly, we eliminated asmall set of cases in which the chosen antecedent washeadless, as no predicate-argument statistics couldbe collected for such a case.These filters pared down the errors to a corpus of45 examples; in all cases the chosen antecedent andmost proximal correct antecedent were each headedby common nouns.
Upon manual inspection, a fur-ther subset of the cases were found to be caused byfactors irrelevant to the question at hand: 9 cases inwhich the antecedent chosen should have been ruledout as impossible (e.g., the collocation these days asthe antecedent of they), 5 cases in which either theannotated keys were incorrect or our mapping systemfailed to assign credit for a correct answer where itwas due, and 11 cases in which our shallow parsermisparsed either the chosen antecedent or the cor-rect antecedent.
This left a corpus of 20 cases toexamine using the K&L methodology.The preferences embodied by the statistics col-lected split these cases down the middle: in 10 casesthe correct antecedent had a higher normalized prob-ability than the chosen one, and in the other 10 casesthe opposite was true.10 To get a sense for the data,we consider two examples, the first being a case in10The unnormalized counts disagreed with the normal-ized ones in only one case; the unnormalized one favoredthe correct antecedent for that example.which predicate-argument statistics were definitive:(3) After the endowment was publicly excoriated forhaving the temerity to award some of its moneyto art that addressed changing views of genderand race, many institutions lost the will to showany art that was rambunctious or edgy.The MaxEnt model selected the temerity as the an-tecedent of its (salience value: 0.30), preferring itto the correct antecedent the endowment (saliencevalue: 0.10).
However, AltaVista found no occur-rences of temerity?s money or its variants on the web,and thus the unnormalized and normalized countswere 0.
On the other hand, endowment?s money andits variants had unnormalized and normalized statis-tics of 1583 and 1.47?
10?3 respectively.Example (4), on the other hand, is a case in whichthe statistics merely strengthened the bias to thewrong antecedent:(4) The dancers were joined by about 70 supportersas they marched around a fountain not far fromthe mayor?s office, chanting: ?Giuliani scaredof sex!
Who?s he going to censor next?
?The model preferred the supporters as the antecedentof they (salience value: 0.54) over the correct an-tecedent the dancers (salience value: 0.45).
Statis-tics support the same conclusion, with unnormalizedand normalized counts of 2283 and 1.18?
10?3 forsupporters marched and its variants, and of 334 and1.72?
10?4 for dancers marched and its variants.The analysis of this sample therefore suggests thatpredicate-argument statistics are unlikely to be ofmuch help when used in a model trained with a state-of-the-art set of morphosyntactic features, even ifrobust counts were available.
While the statisticalpreferences for our data sample were split down themiddle, it is important to understand that the casesfor which statistics hurt are potentially more damn-ing than those for which they helped.
In the cases inwhich statistics reinforced a wrong answer, no (rea-sonable) manipulation of statistical features or filterscan rescue the prediction.
On the other hand, forthe cases in which statistics could help, their suc-cessful use will depend on the existence of a formulathat can capture these cases without changing thepredictions for examples that the model currentlyclassifies correctly.
Although our informal analysisadmittedly has certain limits ?
the web counts wecollected are only approximations of true counts, andthe size of our manually-inspected corpus ended upbeing fairly small ?
our experience leads us to believethat predicate-argument statistics are a poor substi-tute for world knowledge, and more to the point, theydo not offer much predictive power to a state-of-the-art morphosyntactically-driven pronoun interpreta-tion system.
Indeed, crisp ?textbook?
examples suchas (3) appear to be empirically rare; the help pro-vided by statistics for several of the examples seemedto be more due to fortuity than the capturing of anactual world knowledge relationship.
Consider (5):(5) Chung, as part of a plea bargain deal withthe department, has claimed that then-DNC fi-nance director Richard Sullivan personally askedhim for a $125,000 donation in April 1995, thesources said.
Sullivan took the money despitehaving previously voiced suspicions that Chungwas acting as a conduit for illegal contributionsfrom Chinese business executives, they added.Our system selected Chinese business executives asthe antecedent of they (salience value: 0.34), over thecorrect the sources (salience value: 0.10).
Predicate-argument statistics support sources (normalized andunnormalized values of 29662 and 5.78?
10?4) overexecutives (2391 and 1.50?
10?4), and thus this ex-ample was classified as one of the 10 for which statis-tics helped.
In actuality, however, the correct an-tecedent is determined by unrelated factors, demon-strated by the fact that if the head nouns executivesand sources were switched in (5), the preferred an-tecedent would be the executives, contrary to whatpredicate-argument statistics would predict.8 ConclusionIn conclusion, our experimental results and erroranalysis suggest that predicate-argument statisticsoffer little predictive power to a pronoun interpre-tation system trained on a state-of-the-art set ofmorphosyntactic features.
On the one hand, it ap-pears that the distribution of pronouns in discourseallows for a system to correctly resolve a majorityof them using only morphosyntactic cues.
On theother hand, predicate-argument statistics appear toprovide a poor substitute for the world knowledgethat may be necessary to correctly interpret the re-maining cases.AcknowledgmentsThis work was supported by the ACE program(www.nist.gov/speech/tests/ACE/).ReferencesAdam Berger, Stephen A. Della Pietra, and Vin-cent J. Della Pietra.
1996.
A maximum entropyapproach to natural language processing.
Compu-tational Linguistics, 22(1):39?71.Stanley F. Chen and Ronard Rosenfeld.
2000.
Asurvey of smoothing techniques for ME models.IEEE Transactions on Speech and Audio Process-ing, 8(1):37?50.Ido Dagan and Alon Itai.
1990.
Automatic acquisi-tion of constraints for the resolution of anaphorareferences and syntactic ambiguities.
In Proceed-ings of the 13th International Conference on Com-putational Linguistics (COLING-90), pages 330?332.Ido Dagan, John Justenson, Shalom Lappin, Her-bert Leass, and Amnon Ribak.
1995.
Syntax andlexical statistics in anaphora resolution.
AppliedArtificial Intelligence, 9(6):633?644, Nov/Dec.Niyu Ge, John Hale, and Eugene Charniak.
1998.A statistical approach to anaphora resolution.
InProceedings of the Sixth Workshop on Very LargeCorpora, Montreal, Quebec.Jerry R. Hobbs.
1978.
Resolving pronoun references.Lingua, 44:311?338.Frank Keller and Mirella Lapata.
2003.
Usingthe web to obtain frequencies for unseen bigrams.Computational Linguistics, 29(3).Christopher Kennedy and Branimir Boguraev.
1996.Anaphora for everyone: Pronominal anaphora res-olution without a parser.
In Proceedings of the16th International Conference on ComputationalLinguistics (COLING-96).Shalom Lappin and Herbert Leass.
1994.
An algo-rithm for pronominal anaphora resolution.
Com-putational Linguistics, 20(4):535?561.Ruslan Mitkov.
2002.
Anaphora Resolution.
Long-man, London.Fernando Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of English words.In Proceedings of the 31st Annual Meeting of theAssociation for Computational Linguistics (ACL-93), pages 183?190.
