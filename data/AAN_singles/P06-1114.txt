Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 905?912,Sydney, July 2006. c?2006 Association for Computational LinguisticsMethods for Using Textual Entailment inOpen-Domain Question AnsweringSanda Harabagiu and Andrew HicklLanguage Computer Corporation1701 North Collins BoulevardRichardson, Texas 75080 USAsanda@languagecomputer.comAbstractWork on the semantics of questions hasargued that the relation between a ques-tion and its answer(s) can be cast in termsof logical entailment.
In this paper, wedemonstrate how computational systemsdesigned to recognize textual entailmentcan be used to enhance the accuracy ofcurrent open-domain automatic questionanswering (Q/A) systems.
In our experi-ments, we show that when textual entail-ment information is used to either filter orrank answers returned by a Q/A system,accuracy can be increased by as much as20% overall.1 IntroductionOpen-Domain Question Answering (Q/A) sys-tems return a textual expression, identified froma vast document collection, as a response to aquestion asked in natural language.
In the questfor producing accurate answers, the open-domainQ/A problem has been cast as: (1) a pipeline oflinguistic processes pertaining to the processingof questions, relevant passages and candidate an-swers, interconnected by several types of lexico-semantic feedback (cf.
(Harabagiu et al, 2001;Moldovan et al, 2002)); (2) a combination oflanguage processes that transform questions andcandidate answers in logic representations suchthat reasoning systems can select the correct an-swer based on their proofs (cf.
(Moldovan et al,2003)); (3) a noisy-channel model which selectsthe most likely answer to a question (cf.
(Echi-habi and Marcu, 2003)); or (4) a constraint sat-isfaction problem, where sets of auxiliary ques-tions are used to provide more information andbetter constrain the answers to individual ques-tions (cf.
(Prager et al, 2004)).
While different intheir approach, each of these frameworks seeks toapproximate the forms of semantic inference thatwill allow them to identify valid textual answers tonatural language questions.Recently, the task of automatically recog-nizing one form of semantic inference ?
tex-tual entailment ?
has received much attentionfrom groups participating in the 2005 and 2006PASCAL Recognizing Textual Entailment (RTE)Challenges (Dagan et al, 2005).
1As currently defined, the RTE task requires sys-tems to determine whether, given two text frag-ments, the meaning of one text could be reason-ably inferred, or textually entailed, from the mean-ing of the other text.
We believe that systems de-veloped specifically for this task can provide cur-rent question-answering systems with valuable se-mantic information that can be leveraged to iden-tify exact answers from ranked lists of candidateanswers.
By replacing the pairs of texts evaluatedin the RTE Challenge with combinations of ques-tions and candidate answers, we expect that textualentailment could provide yet another mechanismfor approximating the types of inference neededin order answer questions accurately.In this paper, we present three different methodsfor incorporating systems for textual entailmentinto the traditional Q/A architecture employed bymany current systems.
Our experimental resultsindicate that (even at their current level of per-formance) textual entailment systems can substan-tially improve the accuracy of Q/A, even when noother form of semantic inference is employed.The remainder of the paper is organized as fol-1http://www.pascal-network.org/Challenges/RTE905ProcessingQuestionModule(QP)PassageRetrievalModule(PR)Answer TypeExpected KeywordsModuleAnswerProcessing(AP)RankedListof AnswersTEXTUALENTAILMENTMethod 1TEXTUALENTAILMENTMethod 2List of QuestionsGenerationAUTO?QUABRanked List of ParagraphsTEXTUALENTAILMENTMethod 3Entailed QuestionsEntailed ParagraphsList of Entailed ParagraphsQuestionDocumentsAnswersAnswers?M1Answers?M2Answers?M3QUESTION ANSWERING SYSTEMFigure 1: Integrating Textual Entailment in Q/A.lows.
Section 2 describes the three methods ofusing textual entailment in open-domain questionanswering that we have identified, while Section 3presents the textual entailment system we haveused.
Section 4 details our experimental methodsand our evaluation results.
Finally, Section 5 pro-vides a discussion of our findings, and Section 6summarizes our conclusions.2 Integrating Textual Entailment inQuestion AnsweringIn this section, we describe three different meth-ods for integrating a textual entailment (TE) sys-tem into the architecture of an open-domain Q/Asystem.Work on the semantics of questions (Groe-nendijk, 1999; Lewis, 1988) has argued that theformal answerhood relation found between a ques-tion and a set of (correct) answers can be castin terms of logical entailment.
Under these ap-proaches (referred to as licensing by (Groenendijk,1999) and aboutness by (Lewis, 1988)), p is con-sidered to be an answer to a question ?q iff ?q logi-cally entails the set of worlds in which p is true(i.e.?p).
While the notion of textual entailment hasbeen defined far less rigorously than logical en-tailment, we believe that the recognition of textualentailment between a question and a set of candi-date answers ?
or between a question and ques-tions generated from answers ?
can enable Q/Asystems to identify correct answers with greaterprecision than current keyword- or pattern-basedtechniques.As illustrated in Figure 1, most open-domainQ/A systems generally consist of a sequence ofthree modules: (1) a question processing (QP)module; (2) a passage retrieval (PR) module; and(3) an answer processing (AP) module.
Questionsare first submitted to a QP module, which extractsa set of relevant keywords from the text of thequestion and identifies the question?s expected an-swer type (EAT).
Keywords ?
along with the ques-tion?s EAT ?
are then used by a PR module to re-trieve a ranked list of paragraphs which may con-tain answers to the question.
These paragraphs arethen sent to an AP module, which extracts an ex-act candidate answer from each passage and thenranks each candidate answer according to the like-lihood that it is a correct answer to the originalquestion.Method 1.
In Method 1, each of a ranked list ofanswers that do not meet the minimum conditionsfor TE are removed from consideration and thenre-ranked based on the entailment condence (areal-valued number ranging from 0 to 1) assignedby the TE system to each remaining example.
Thesystem then outputs a new set of ranked answerswhich do not contain any answers that are not en-tailed by the user?s question.Table 1 provides an example where Method 1could be used to make the right prediction for a setof answers.
Even though A1 was ranked in sixthposition, the identification of a high-confidencepositive entailment enabled it to be returned as the906top answer.
In contrast, the recognition of a neg-ative entailment for A2 caused this answer to bedropped from consideration altogether.Q1 : ?What did Peter Minuit buy for the equivalent of $24.00?
?Rank1 TE Rank2 Answer TextA1 6th YES(0.89)1st Everyone knows that, back in 1626, Peter Mi-nuit bought Manhattan from the Indians for $24worth of trinkets.A2 1st NO(0.81)?
In 1626, an enterprising Peter Minuit flaggeddown some passing locals, plied them withbeads, cloth and trinkets worth an estimated$24, and walked away with the whole island.Table 1: Re-ranking of answers by Method 1.Method 2.
Since AP is often a resource-intensive process for most Q/A systems, we ex-pect that TE information can be used to limit thenumber of passages considered during AP.
As il-lustrated in Method 2 in Figure 1, lists of passagesretrieved by a PR module can either be ranked (orfiltered) using TE information.
Once ranking iscomplete, answer extraction takes place only onthe set of entailed passages that the system consid-ers likely to contain a correct answer to the user?squestion.Method 3.
In previous work (Harabagiu et al,2005b), we have described techniques that can beused to automatically generate well-formed natu-ral language questions from the text of paragraphsretrieved by a PR module.
In our current system,sets of automatically-generated questions (AGQ)are created using a stand-alone AutoQUAB gen-eration module, which assembles question-answerpairs (known as QUABs) from the top-ranked pas-sages returned in response to a question.
Table 2lists some of the questions that this module hasproduced for the question Q2: ?How hot does theinside of an active volcano get?
?.Q2: ?How hot does the inside of an active volcano get?
?A2 Tamagawa University volcano expert Takeyo Kosaka said lava frag-ments belched out of the mountain on January 31 were as hot as 300degrees Fahrenheit.
The intense heat from a second eruption onTuesday forced rescue operations to stop after 90 minutes.
Becauseof the high temperatures, the bodies of only five of the volcano?sinitial victims were retrieved.Positive EntailmentAGQ1 What temperature were the lava fragments belched out of the moun-tain on January 31?AGQ2 How many degrees Fahrenheit were the lava fragments belched outof the mountain on January 31?Negative EntailmentAGQ3 When did rescue operations have to stop?AGQ4 How many bodies of the volcano?s initial victims were retrieved?Table 2: TE between AGQs and user question.Following (Groenendijk, 1999), we expect thatif a question ?q logically entails another question?q?, then some subset of the answers entailed by?q?
should also be interpreted as valid answers to?q.
By establishing TE between a question andAGQs derived from passages identified by the Q/Asystem for that question, we expect we can iden-tify a set of answer passages that contain correctanswers to the original question.
For example, inTable 2, we find that entailment between questionsindicates the correctness of a candidate answer:here, establishing that Q2 entails AGQ1 and AGQ2(but not AGQ3 or AGQ4) enables the system to se-lect A2 as the correct answer.When at least one of the AGQs generated bythe AutoQUAB module is entailed by the originalquestion, all AGQs that do not reach TE are fil-tered from consideration; remaining passages areassigned an entailment confidence score and aresent to the AP module in order to provide an ex-act answer to the question.
Following this pro-cess, candidate answers extracted from the APmodule were then re-associated with their AGQsand resubmitted to the TE system (as in Method1).
Question-answer pairs deemed to be posi-tive instances of entailment were then stored in adatabase and used as additional training data forthe AutoQUAB module.
When no AGQs werefound to be entailed by the original question, how-ever, passages were ranked according to their en-tailment confidence and sent to AP for further pro-cessing and validation.3 The Textual Entailment SystemProcessing textual entailment, or recognizingwhether the information expressed in a text can beinferred from the information expressed in anothertext, can be performed in four ways.
We can try to(1) derive linguistic information from the pair oftexts, and cast the inference recognition as a clas-sification problem; or (2) evaluate the probabilitythat an entailment can exist between the two texts;(3) represent the knowledge from the pair of textsin some representation language that can be asso-ciated with an inferential mechanism; or (4) usethe classical AI definition of entailment and buildmodels of the world in which the two texts are re-spectively true, and then check whether the modelsassociated with one text are included in the mod-els associated with the other text.
Although we be-lieve that each of these methods should be inves-tigated fully, we decided to focus only on the firstmethod, which allowed us to build the TE systemillustrated in Figure 2.Our TE system consists of (1) a Preprocess-ing Module, which derives linguistic knowledgefrom the text pair; (2) an Alignment Module, whichtakes advantage of the notions of lexical alignment907ClassifierYESNOTextualInput 2TextualInput 1Preprocessing TrainingCorporaFeaturesAlignmentDependencyFeaturesParaphraseFeaturesSemantic/PragmaticFeaturesCoreferenceCoreferenceNEAliasingConceptParaphrase AcquisitionWWWLexical AlignmentAlignment ModuleFeature ExtractionClassification ModuleLexico?SemanticPoS/ NERSynonyms/AntonymsNormalizationSyntacticSemanticTemporalParsingModality Detection Speech Act RecognitionPragmaticsFactivity Detection Belief RecognitionFigure 2: Textual Entailment Architecture.and textual paraphrases; and (3) a ClassicationModule, which uses a machine learning classifier(based on decision trees) to make an entailmentjudgment for each pair of texts.As described in (Hickl et al, 2006), the Prepro-cessing module is used to syntactically parse texts,identify the semantic dependencies of predicates,label named entities, normalize temporal and spa-tial expressions, resolve instances of coreference,and annotate predicates with polarity, tense, andmodality information.Following preprocessing, texts are sent toan Alignment Module which uses a MaximumEntropy-based classifier in order to estimate theprobability that pairs of constituents selected fromtexts encode corresponding information that couldbe used to inform an entailment judgment.
Thismodule assumes that since sets of entailing textsnecessarily predicate about the same set of indi-viduals or events, systems should be able to iden-tify elements from each text that convey similartypes of presuppositions.
Examples of predicatesand arguments aligned by this module are pre-sented in Figure 3.Pred: Pred:ArgM?LOCthe inside of an active volcanoan active volcanoHow hotthe mountainthe lava fragmentsOriginal QuestionAuto?QUABWhat temperatureget hotbe temperatureArg1Answer TypeArg1Figure 3: Alignment GraphAligned constituents are then used to extractsets of phrase-level alternations (or ?paraphrases?
)from the WWW that could be used to capture cor-respondences between texts longer than individualconstituents.
The top 8 candidate paraphrases fortwo of the aligned elements from Figure 3 are pre-sented in Table 3.Finally, the Classication Module employs aJudgment ParaphraseYES lava fragments in pyroclastic flows can reach 400 degreesYES an active volcano can get up to 2000 degreesNO an active volcano above you are slopes of 30 degreesYES the active volcano with steam reaching 80 degreesYES lava fragments such as cinders may still be as hot as 300 degreesNO lava is a liquid at high temperature: typically from 700 degreesTable 3: Phrase-Level Alternationsdecision tree classifier in order to determinewhether an entailment relationship exists for eachpair of texts.
This classifier is learned using fea-tures extracted from the previous modules, includ-ing features derived from (1) the (lexical) align-ment of the texts, (2) syntactic and semantic de-pendencies discovered in each text passage, (3)paraphrases derived from web documents, and (4)semantic and pragmatic annotations.
(A completelist of features can be found in Figure 4.)
Based onthese features, the classifier outputs both an entail-ment judgment (either yes or no) and a confidencevalue, which is used to rank answers or paragraphsin the architecture illustrated in Figure 1.3.1 Lexical AlignmentSeveral approaches to the RTE task have arguedthat the recognition of textual entailment can beenhanced when systems are able to identify ?or align ?
corresponding entities, predicates, orphrases found in a pair of texts.
In this section,we show that by using a machine learning-basedclassifier which combines lexico-semantic infor-mation from a wide range of sources, we are ableto accurately identify aligned constituents in pairsof texts with over 90% accuracy.We believe the alignment of corresponding en-tities can be cast as a classification problem whichuses lexico-semantic features in order to computean alignment probability p(a), which correspondsto the likelihood that a term selected from one textentails a term from another text.
We used con-stituency information from a chunk parser to de-compose the pair of texts into a set of disjoint seg-908ALIGNMENT FEATURES: These three features are derived from theresults of the lexical alignment classification.1 LONGEST COMMON STRING: This feature represents the longestcontiguous string common to both texts.2 UNALIGNED CHUNK: This feature represents the number ofchunks in one text that are not aligned with a chunk from the other3 LEXICAL ENTAILMENT PROBABILITY: This feature is defined in(Glickman and Dagan, 2005).DEPENDENCY FEATURES: These four features are computedfrom the PropBank-style annotations assigned by the semanticparser.1 ENTITY-ARG MATCH: This is a boolean feature which fires whenaligned entities were assigned the same argument role label.2 ENTITY-NEAR-ARG MATCH: This feature is collapsing the ar-guments Arg1 and Arg2 (as well as the ArgM subtypes) into singlecategories for the purpose of counting matches.3 PREDICATE-ARG MATCH: This boolean feature is flagged whenat least two aligned arguments have the same role.4 PREDICATE-NEAR-ARG MATCH: This feature is collapsing the ar-guments Arg1 and Arg2 (as well as the ArgM subtypes) into singlecategories for the purpose of counting matches.PARAPHRASE FEATURES: These three features are derived fromthe paraphrases acquired for each pair.1 SINGLE PATTERN MATCH: This is a boolean feature which firedwhen a paraphrase matched either of the texts.2 BOTH PATTERN MATCH: This is a boolean feature which firedwhen paraphrases matched both texts.3 CATEGORY MATCH: This is a boolean feature which fired whenparaphrases could be found from the same paraphrase cluster thatmatched both texts.SEMANTIC/PRAGMATIC FEATURES: These six features are ex-tracted by the preprocessing module.1 NAMED ENTITY CLASS: This feature has a different value foreach of the 150 named entity classes.2 TEMPORAL NORMALIZATION: This boolean feature is flaggedwhen the temporal expressions are normalized to the same ISO9000 equivalents.3 MODALITY MARKER: This boolean feature is flagged when thetwo texts use the same modal verbs.4 SPEECH-ACT: This boolean feature is flagged when the lexiconsindicate the same speech act in both texts.5 FACTIVITY MARKER: This boolean feature is flagged when thefactivity markers indicate either TRUE or FALSE in both texts simul-taneously.6 BELIEF MARKER: This boolean feature is set when the beliefmarkers indicate either TRUE or FALSE in both texts simultaneously.CONTRAST FEATURES: These six features are derived from theopposing information provided by antonymy relations or chains.1 NUMBER OF LEXICAL ANTONYMY RELATIONS: This featurecounts the number of antonyms from WordNet that are discoveredbetween the two texts.2 NUMBER OF ANTONYMY CHAINS: This feature counts the num-ber of antonymy chains that are discovered between the two texts.3 CHAIN LENGTH: This feature represents a vector with thelengths of the antonymy chains discovered between the two texts.4 NUMBER OF GLOSSES: This feature is a vector representing thenumber of Gloss relations used in each antonymy chain.5 NUMBER OF MORPHOLOGICAL CHANGES: This feature is a vectorrepresenting the number of Morphological-Derivation relations foundin each antonymy chain.6 NUMBER OF NODES WITH DEPENDENCIES: This feature is a vec-tor indexing the number of nodes in each antonymy chain that con-tain dependency relations.7 TRUTH-VALUE MISMATCH: This is a boolean feature which firedwhen two aligned predicates differed in any truth value.8 POLARITY MISMATCH: This is a boolean feature which firedwhen predicates were assigned opposite polarity values.Figure 4: Features Used in Classifying Entailmentments known as ?alignable chunks?.
Alignablechunks from one text (Ct) and the other text (Ch)are then assembled into an alignment matrix (Ct?Ch).
Each pair of chunks (p ?
Ct ?
Ch) is thensubmitted to a Maximum Entropy-based classi-fier which determines whether or not the pair ofchunks represents a case of lexical entailment.Three classes of features were used in theAlignment Classifier: (1) a set of statistical fea-tures (e.g.
cosine similarity), (2) a set of lexico-semantic features (including WordNet Similar-ity (Pedersen et al, 2004), named entity classequality, and part-of-speech equality), and (3) a setof string-based features (such as Levenshtein editdistance and morphological stem equality).As in (Hickl et al, 2006), we used a two-step approach to obtain sufficient training datafor the Alignment Classifier.
First, humans weretasked with annotating a total of 10,000 align-ment pairs (extracted from the 2006 PASCAL De-velopment Set) as either positive or negative in-stances of alignment.
These annotations were thenused to train a hillclimber that was used to anno-tate a larger set of 450,000 alignment pairs se-lected at random from the training corpora de-scribed in Section 3.3.
These machine-annotatedexamples were then used to train the MaximumEntropy-based classifier that was used in our TEsystem.
Table 4 presents results from TE?s linear-and Maximum Entropy-based Alignment Classi-fiers on a sample of 1000 alignment pairs selectedat random from the 2006 PASCAL Test Set.Classifier Training Set Precision Recall F-MeasureLinear 10K pairs 0.837 0.774 0.804Maximum Entropy 10K pairs 0.881 0.851 0.866Maximum Entropy 450K pairs 0.902 0.944 0.922Table 4: Performance of Alignment Classifier3.2 Paraphrase AcquisitionMuch recent work on automatic paraphras-ing (Barzilay and Lee, 2003) has used relativelysimple statistical techniques to identify text pas-sages that contain the same information from par-allel corpora.
Since sentence-level paraphrases aregenerally assumed to contain information aboutthe same event, these approaches have generallyassumed that all of the available paraphrases fora given sentence will include at least one pair ofentities which can be used to extract sets of para-phrases from text.The TE system uses a similar approach to gatherphrase-level alternations for each entailment pair.In our system, the two highest-confidence en-tity alignments returned by the Lexical Alignmentmodule were used to construct a query whichwas used to retrieve the top 500 documents fromGoogle, as well as all matching instances from ourtraining corpora described in Section 3.3.
Thismethod did not always extract true paraphrases ofeither texts.
In order increase the likelihood that909only true paraphrases were considered as phrase-level alternations for an example, extracted sen-tences were clustered using complete-link cluster-ing using a technique proposed in (Barzilay andLee, 2003).3.3 Creating New Sources of Training DataIn order to obtain more training data for our TEsystem, we extracted more than 200,000 examplesof textual entailment from large newswire corpora.Positive Examples.
Following an idea pro-posed in (Burger and Ferro, 2005), we created acorpus of approximately 101,000 textual entail-ment examples by pairing the headline and firstsentence from newswire documents.
In order toincrease the likelihood of including only positiveexamples, pairs were filtered that did not share anentity (or an NP) in common between the headlineand the first sentenceJudgment ExampleYES Text-1: Sydney newspapers made a secret deal not to reporton the fawning and spending during the city?s successful bidfor the 2000 Olympics, former Olympics Minister Bruce Bairdsaid today.Text-2: Papers Said To Protect Sydney BidYES Text-1: An IOC member expelled in the Olympic briberyscandal was consistently drunk as he checked out Stockholm?sbid for the 2004 Games and got so offensive that he wasthrown out of a dinner party, Swedish officials said.Text-2: Officials Say IOC Member Was DrunkTable 5: Positive ExamplesNegative Examples.
Two approaches wereused to gather negative examples for our trainingset.
First, we extracted 98,000 pairs of sequen-tial sentences that included mentions of the samenamed entity from a large newswire corpus.
Wealso extracted 21,000 pairs of sentences linked byconnectives such as even though, in contrast andbut.Judgment ExampleNO Text-1: One player losing a close friend is Japanese pitcherHideki Irabu, who was befriended by Wells during springtraining last year.Text-2: Irabu said he would take Wells out to dinner when theYankees visit Toronto.NO Text-1: According to the professor, present methods of clean-ing up oil slicks are extremely costly and are never completelyefficient.Text-2: In contrast, he stressed, Clean Mag has a 100 percentpollution retrieval rate, is low cost and can be recycled.Table 6: Negative Examples4 Experimental ResultsIn this section, we describe results from four setsof experiments designed to explore how textualentailment information can be used to enhance thequality of automatic Q/A systems.
We show thatby incorporating features from TE into a Q/A sys-tem which employs no other form of textual infer-ence, we can improve accuracy by more than 20%over a baseline.We conducted our evaluations on a set of500 factoid questions selected randomly fromquestions previously evaluated during the annualTREC Q/A evaluations.
2 Of these 500 questions,335 (67.0%) were automatically assigned an an-swer type from our system?s answer type hierar-chy ; the remaining 165 (33.0%) questions wereclassified as having an unknown answer type.
Inorder to provide a baseline for our experiments,we ran a version of our Q/A system, known asFERRET (Harabagiu et al, 2005a), that does notmake use of textual entailment information whenidentifying answers to questions.
Results from thisbaseline are presented in Table 7.Question Set Questions Correct Accuracy MRRKnown Answer Types 335 107 32.0% 0.3001Unknown Answer Types 265 81 30.6% 0.2987Table 7: Q/A Accuracy without TEThe performance of the TE system describedin Section 3 was first evaluated in the 2006 PAS-CAL RTE Challenge.
In this task, systems weretasked with determining whether the meaning ofa sentence (referred to as a hypothesis) could bereasonably inferred from the meaning of anothersentence (known as a text).
Four types of sen-tence pairs were evaluated in the 2006 RTE Chal-lenge, including: pairs derived from the output of(1) automatic question-answering (QA) systems,(2) information extraction systems (IE), (3) in-formation retrieval (IR) systems, and (4) multi-document summarization (SUM) systems.
The ac-curacy of our TE system across these four tasks ispresented in Table 8.Training DataDevelopment Set Additional CorporaNumber of Examples 800 201,000Task QA-test 0.5750 0.6950IE-test 0.6450 0.7300IR-test 0.6200 0.7450SUM-test 0.7700 0.8450Overall Accuracy 0.6525 0.7538Table 8: Accuracy on the 2006 RTE Test SetIn previous work (Hickl et al, 2006), we havefound that the type and amount of training dataavailable to our TE system significantly (p < 0.05)impacted its performance on the 2006 RTE TestSet.
When our system was trained on the trainingcorpora described in Section 3.3, the overall accu-racy of the system increased by more than 10%,2Text Retrieval Conference (http://trec.nist.gov)910from 65.25% to 75.38%.
In order to provide train-ing data that replicated the task of recognizing en-tailment between a question and an answer, we as-sembled a corpus of 5000 question-answer pairsselected from answers that our baseline Q/A sys-tem returned in response to a new set of 1000 ques-tions selected from the TREC test sets.
2500 posi-tive training examples were created from answersidentified by human annotators to be correct an-swers to a question, while 2500 negative exampleswere created by pairing questions with incorrectanswers returned by the Q/A system.After training our TE system on this corpus, weperformed the following four experiments:Method 1.
In the first experiment, the rankedlists of answers produced by the Q/A system weresubmitted to the TE system for validation.
Un-der this method, answers that were not entailedby the question were removed from consideration;the top-ranked entailed answer was then returnedas the system?s answer to the question.
Resultsfrom this method are presented in Table 9.Method 2.
In this experiment, entailment in-formation was used to rank passages returned bythe PR module.
After an initial relevance rank-ing was determined from the PR engine, the top50 passages were paired with the original questionand were submitted to the TE system.
Passageswere re-ranked using the entailment judgment andthe entailment confidence computed for each pairand then submitted to the AP module.
Featuresderived from the entailment confidence were thencombined with the keyword- and relation-basedfeatures described in (Harabagiu et al, 2005a) inorder to produce a final ranking of candidate an-swers.
Results from this method are presented inTable 9.Method 3.
In the third experiment, TE was usedto select AGQs that were entailed by the questionsubmitted to the Q/A system.
Here, AutoQUABwas used to generate questions for the top 50 can-didate answers identified by the system.
When atleast one of the top 50 AGQs were entailed bythe original question, the answer passage associ-ated with the top-ranked entailed question was re-turned as the answer.
When none of the top 50AGQs were entailed by the question, question-answer pairs were re-ranked based on the entail-ment confidence, and the top-ranked answer wasreturned.
Results for both of these conditions arepresented in Table 9.Hybrid Method.
Finally, we found that thebest results could be obtained by combining as-pects of each of these three strategies.
Under thisapproach, candidate answers were initially rankedusing features derived from entailment classifica-tions performed between (1) the original questionand each candidate answer and (2) the originalquestion and the AGQ generated from each can-didate answer.
Once a ranking was established,answers that were not judged to be entailed bythe question were also removed from final rank-ing.
Results from this hybrid method are providedin Table 9.Known EAT Unknown EATAcc MRR Acc MRRBaseline 32.0% 0.3001 30.6% 0.2978Method 1 44.1% 0.4114 39.5% 0.3833Method 2 52.4% 0.5558 42.7% 0.4135Method 3 41.5% 0.4257 37.5% 0.3575Hybrid 53.9% 0.5640 41.9% 0.4010Table 9: Q/A Performance with TE5 DiscussionThe experiments reported in this paper suggestthat current TE systems may be able to provideopen-domain Q/A systems with the forms of se-mantic inference needed to perform accurate an-swer validation.
While probabilistic or web-basedmethods for answer validation have been previ-ously explored in the literature (Magnini et al,2002), these approaches have modeled the rela-tionship between a question and a (correct) answerin terms of relevance and have not tried to approx-imate the deeper semantic phenomena that are in-volved in determining answerhood.Our work suggests that considerable gains inperformance can be obtained by incorporating TEduring both answer processing and passage re-trieval.
While best results were obtained usingthe Hybrid Method (which boosted performanceby nearly 28% for questions with known EATs),each of the individual methods managed to boostthe overall accuracy of the Q/A system by at least7%.
When TE was used to filter non-entailed an-swers from consideration (Method 1), the over-all accuracy of the Q/A system increased by 12%over the baseline (when an EAT could be iden-tified) and by nearly 9% (when no EAT couldbe identified).
In contrast, when entailment in-formation was used to rank passages and candi-date answers, performance increased by 22% and10% respectively.
Somewhat smaller performancegains were achieved when TE was used to select911amongst AGQs generated by our Q/A system?sAutoQUAB module (Method 3).
We expect thatby adding features to TE system specifically de-signed to account for the semantic contributionsof a question?s EAT, we may be able to boost theperformance of this method.6 ConclusionsIn this paper, we discussed three different waysthat a state-of-the-art textual entailment systemcould be used to enhance the performance of anopen-domain Q/A system.
We have shown thatwhen textual entailment information is used to ei-ther filter or rank candidate answers returned by aQ/A system, Q/A accuracy can be improved from32% to 52% (when an answer type can be de-tected) and from 30% to 40% (when no answertype can be detected).
We believe that these resultssuggest that current supervised machine learningapproaches to the recognition of textual entailmentmay provide open-domain Q/A systems with theinferential information needed to develop viableanswer validation systems.7 AcknowledgmentsThis material is based upon work funded in wholeor in part by the U.S. Government and any opin-ions, findings, conclusions, or recommendationsexpressed in this material are those of the authorsand do not necessarily reflect the views of the U.S.Government.ReferencesRegina Barzilay and Lillian Lee.
2003.
Learningto paraphrase: An unsupervised approach usingmultiple-sequence alignment.
In HLT-NAACL.John Burger and Lisa Ferro.
2005.
Generating an En-tailment Corpus from News Headlines.
In Proceed-ings of the ACL Workshop on Empirical Modeling ofSemantic Equivalence and Entailment, pages 49?54.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The PASCAL Recognizing Textual Entail-ment Challenge.
In Proceedings of the PASCALChallenges Workshop.Abdessamad Echihabi and Daniel Marcu.
2003.
Anoisy-channel approach to question answering.
InProceedings of the 41st Meeting of the Associationfor Computational Linguistics.Oren Glickman and Ido Dagan.
2005.
A ProbabilisticSetting and Lexical Co-occurrence Model for Tex-tual Entailment.
In Proceedings of the ACL Work-shop on Empirical Modeling of Semantic Equiva-lence and Entailment, Ann Arbor, USA.Jeroen Groenendijk.
1999.
The logic of interrogation:Classical version.
In Proceedings of the Ninth Se-mantics and Linguistics Theory Conference (SALTIX), Ithaca, NY.Sanda Harabagiu, Dan Moldovan, Marius Pasca, RadaMihalcea, Mihai Surdeanu, Razvan Bunsecu, Rox-ana Girju, Vasile Rus, and Paul Morarescu.
2001.The Role of Lexico-Semantic Feedback in Open-Domain Textual Question-Answering.
In Proceed-ings of the 39th Meeting of the Association for Com-putational Linguistics.S.
Harabagiu, D. Moldovan, C. Clark, M. Bowden,A.
Hickl, and P. Wang.
2005a.
Employing TwoQuestion Answering Systems in TREC 2005.
InProceedings of the Fourteenth Text REtrieval Con-ference.Sanda Harabagiu, Andrew Hickl, John Lehmann, andDan Moldovan.
2005b.
Experiments with Inter-active Question-Answering.
In Proceedings of the43rd Annual Meeting of the Association for Compu-tational Linguistics (ACL?05).Andrew Hickl, John Williams, Jeremy Bensley, KirkRoberts, Bryan Rink, and Ying Shi.
2006.
Rec-ognizing Textual Entailment with LCC?s Ground-hog System.
In Proceedings of the Second PASCALChallenges Workshop.David Lewis.
1988.
Relevant Implication.
Theoria,54(3):161?174.Bernardo Magnini, Matteo Negri, Roberto Prevete, andHristo Tanev.
2002.
Is it the right answer?
ex-ploiting web redundancy for answer validation.
InProceedings of the Fortieth Annual Meeting of theAssociation for Computational Linguistics (ACL),Philadelphia, PA.Dan Moldovan, Marius Pasca, Sanda Harabagiu, andMihai Surdeanu.
2002.
Performance Issues and Er-ror Analysis in an Open-Domain Question Answer-ing System.
In Proceedings of the 4Oth Meeting ofthe Association for Computational Linguistics.Dan Moldovan, Christine Clark, Sanda Harabagiu,and Steve Maiorano.
2003.
COGEX: A LogicProver for Question Answering.
In Proceedings ofHLT/NAACL-2003.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.WordNet::Similarity - Measuring the Relatedness ofConcepts.
In Proceedings of the Nineteenth Na-tional Conference on Artificial Intelligence (AAAI-04), San Jose, CA.John Prager, Jennifer Chu-Carroll, and KrzysztofCzuba.
2004.
Question answering using con-straint satisfaction: Qa-by-dossier-with-contraints.In Proceedings of the ACL-2004, pages 574?581,Barcelona, Spain, July.912
