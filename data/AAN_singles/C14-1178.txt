Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1886?1896, Dublin, Ireland, August 23-29 2014.Learning the Taxonomy of Function Words for ParsingDongchen Li, Xiantao Zhang, Dingsheng Luo and Xihong WuKey Laboratory of Machine Perception and IntelligenceSpeech and Hearing Research CenterPeking University, Beijing, China{lidc,zhangxt,dsluo,wxh}@cis.pku.edu.cnAbstractCompletely data-driven grammar training is prone to over-fitting.
Human-defined word classknowledge is useful to address this issue.
However, the manual word class taxonomy may beunreliable and irrational for statistical natural language processing, aside from its insufficientlinguistic phenomena coverage and domain adaptivity.
In this paper, a formalized representationof function word subcategorization is developed for parsing in an automatic manner.
The functionword classification representing intrinsic features of syntactic usages is used to supervise thegrammar induction, and the structure of the taxonomy is learned simultaneously.
The grammarlearning process is no longer a unilaterally supervised training by hierarchical knowledge, butan interactive process between the knowledge structure learning and the grammar training.
Theestablished taxonomy implies the stochastic significance of the diversified syntactic features.The experiments on both Penn Chinese Treebank and Tsinghua Treebank show that the proposedmethod improves parsing performance by 1.6% and 7.6% respectively over the baseline.1 IntroductionProbabilistic context-free grammar (PCFG) is widely used in the fields of speech recognition, machinetranslation, information retrieval, etc.
It takes the empirical rules and probabilities from a Treebank.However, due to the context-free assumption, PCFG does not always perform well (Klein and Man-ning, 2003).
For instance, it assumes adverbs, including temporal adverbs, degree adverbs and negationadverbs, to share the same distribution, whereas the distinction would provide useful indication for dis-ambiguating the syntactic structure of the context.It arose that the manual word classification in linguistic research was used to enrich PCFG and im-prove the performance.
However, from the point of view of statistical natural language processing, thereare some drawbacks for manual classification.
Firstly, Linguistic phenomena covered by the manualrefinement may be limited by the linguistic observations of human.
Secondly, the evidence of manualrefinement is often based on a particular corpus or specific sources of knowledge acquisition.
As a result,its adaptivity to different domains or genres may be insufficient.
As for function words, due to the ambi-guity and complexity in syntactic grammar, it is more difficult to develop formalized representation thanfor content words.
There are diversified standards for grammar refinement.
Consequently, the word clas-sification or category refinement can be conducted in distinct manners, while each of them is reasonablein some sense.
A delicate hierarchical classification inevitably involves in multiple dividing standards.However, the word sets under distinct dividing standards may be overlapping.
The problems come upthat how to choose the set of the multiple standards to cooperate to build the taxonomy, and how to de-cide the priority of each standard.
Regarding that the manual method is hard to overcome critical issues,manual taxonomy for function words may not be reliable for statistical natural language processing.This article attempts to address these issues in a data-driven manner.
we first manually construct acursory and flat classification of function words.
A hierarchical split-merge approach is employed toThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1886introduce our classification, and the PCFG training procedure is supervised to alleviate the over-fittingissue.
The priorities of the subcategorization standards are determined by the measurement of effec-tiveness for parsing in a greedy manner in the hierarchical classification.
And the hierarchical structureof the classification is learned by data-driven approach in the course of grammar induction, so as to fitthe practical usages in the Treebank.
Accordingly, the grammar learning process is no longer a unilat-erally supervised training by hierarchical knowledge, but an interactive process between the knowledgerepresentation induction and the grammar training.
That is, the grammar induction is supervised by theknowledge and the structure of the taxonomy is learned simultaneously.
These two processes are iteratedfor several rounds and the hierarchical structure of the function word taxonomy is constructed.
In eachround, the induced grammar could benefit from the optimized taxonomy during the learning process.
Thecategory split in the early rounds take more priorities than in the late ones.
Thus, the learned taxonomyimplies the stochastic significance of the series of the syntactic features.Experiments on Penn Chinese Treebank Fifth Edition (CTB5.0) (Xue et al., 2002) and Tsinghua Chi-nese Treebank (TCT) (Zhou, 2004) are performed.
The results show that the induced grammars withrefined conjunction categories gain parsing performance improvement by 1.6% on CTB and by 7.6% onTCT.
During the training process, a taxonomy of function words is learned, which reflects their practicalusages in the corpus.The rest of this paper is organized as follows.
We first review related work on category refinementfor parsing.
Then we describe our manually defined categories of function words in Section 3.
Thehierarchical state-split approach for introducing the the categories are presented in Section 4, and ourtaxonomy learning method is described in Section 5.
In Section 7, experimental comparison is conductedamong various methods on granularity choosing.
And conclusions of this research are drawn in lastsection.2 Related WorkA variety of techniques have been proposed to enrich PCFG in either manual (Klein and Manning, 2003;Zhang and Clark, 2011) or automatic (Petrov, 2009; Cohen et al., 2012) manner.2.1 Automatic Refinement of Function Words for ParsingOne way of grammar refinement is data-driven state-split methods (Matsuzaki et al., 2005; Prescher,2005).
The part-of-speech and syntactic tags in the grammar are automatically split to encode the kindsof linguistic distinctions exhibited in the Treebank.
The hierarchical state-split approach (Petrov et al.,2006) started from a bare-bones Treebank derived grammar, and iteratively refined it in a split-merge-smooth cycle with the EM-based parameter re-estimation.
It achieved state of the art accuracies for manylanguages including English, Chinese and German.One tag is usually heterogeneous, in the sense that its word set can be of multiple different types.Nevertheless, the automatic process tries to split the tags through a greedy data-driven manner, wheremultiple distinctive information is used simultaneously when dividing tags.
Thus the refined tags arenot intuitively interpretable.
Meanwhile, considering that the EM algorithm usually gets stuck at a sub-optimal configuration, this data-driven method suffers from the risk of over-fitting.
As shown in theirexperiments, there is little to be gained from splitting the closed part-of-speech classes (e.g.
DT, CC, IN)or the nonterminal ADJP.To alleviate the risk of over-fitting, we employ the human-defined knowledge to constrain the splittingprocess in this research.
Based on the state-split model, our approach aims to reach a compromisebetween manual and automatic refinement approaches.2.2 Manual Refinement of Function Words for ParsingThe other way to refine the annotation for training a parser is incorporating knowledge base.
Semanticknowledge of content words has been proved to be effective in alleviate the data sparsity.
Some re-searches utilized semantic knowledge in WordNet (Miller, 1995; Fellbaum, 1999) for English parsing(Fujita et al., 2010; Agirre et al., 2008), and Xiong et al.
(2005; Lin et al.
(2009) improved Chinese pars-1887ing by incorporating semantic knowledge in HowNet (Dong and Dong, 2003; Dong and Dong, 2006).While WordNet and Hownet contain word classification for content words, Li et al.
(2014b; Li et al.
(2014a) have focused on exploiting manual classification for conjunction in parsing.Klein and Manning (2003) examined the annotation in Penn English Treebank, manually split the ma-jority of the part-of-speech (POS) tags.
For the function words, they split the tag ?IN?
into subordinatingconjunctions, complementizers and prepositions, and appended?BE to all forms of ?be?
and?HAVE toall forms of ?have?.
Conjunction tags are also marked to indicate whether they were ?But?, ?but?
or?&?.
The experimental results showed that the split tags of function words surprisingly make much con-tribution to the overall improved parsing accuracy.
Levy and Manning (2003) transferred this work toPenn Chinese Treebank.
They found that, in some cases, certain adverbs such as ?however (,)?
and?especially (c??)?
preferred IP modification and could help disambiguate IP coordination from VPcoordination.
To capture this point, they marked those adverbs possessing an IP grandparent.
However,these manual refinement methods seems to split the tags in a rough way, which might account for a mod-est accuracy achieved.
Some existing work used heuristic rules to simply split the tags of function words(Klein and Manning, 2003; Levy and Manning, 2003).
They demonstrated that many function wordsstood out to be helpful in predicting the syntactic structure and syntactic label.3 Manual Tabular Subcategories of Function WordsWhen subcategorizing function words, in this section, we manually list various grammatical distinctionsthat are commonly made in traditional and generative grammar in a fairly flat taxonomy.
The grammartraining procedure learns by using our manual taxonomy as a starting point, and constructs a reasonableand subtle hierarchical strucutre based on the distribution of function words usages in the corpus.Based on some existing knowledge base (Xia, 2000; Xue et al., 2000; Zhu et al., 1995; Wang andYu, 2003) and previous research work (Li et al., 2014b), we investigate and summarize the usage offunction words, and come up with a hierarchical subcategories.
The taxonomy of the function words isrepresented in a tree structure, where each subcategory of a function word corresponds to a node in thetaxonomy, the nonterminals are subcategories and the terminals are words.For the convenience and consistence, our manual classification just gives a rough and broad taxonomy.It is labor-intensive and error-prone of classifying the function words manually to produce a consistentoutput.
Fine-grained hierarchical structure is not obligatory, but would be harmful if inappropriately clas-sified, as it may mislead the learning process.
To avoid this kind of risk, the elaboration is saved, ratherthan introducing unnecessary bias.
The learning process would perform the hierarchical classificationaccording to the distribution in the corpus.For instance, the distinction within conjunctions is intricate.
Conjunctions are the words that arecalled ?connective words?
in traditional Chinese grammar books.
In Penn Chinese Treebank, they aretagged as coordinating conjunctions (CC), subordinating conjunctions (CS), or adverbs (AD) accordingto their syntactic distribution.
CC conjoins two equivalent constituents (noun phrases, clauses, etc.
),each of which has approximately the same function as the whole construction.
CS precedes a subordi-nating clause, whereas conjunctive adverbs often appear in the main clause and pair with a subordinatingconjunction (e.g., if (XJ)/CS ... then (?)/AD).
However, in Chinese, it is often hard to tell the sub-ordinating clause from the main clause in the compound statement.
As a result, in the prospective oflinguistic computing, the confusion is that, CS and conjunctive adverbs both precedes the subordinatingclauses or main clauses, while CC connects two phrases or precedes the main clause.
In our scheme,we simply conflates the CC, CS and conjunctive adverbs together.
This result in a general ?conjunction?category, within which we just enumerate all the possible uses of the conjunctions.
As a result, the struc-ture of our human-defined taxonomy is fairly flat, as briefly shown in Figure 1 and Figure 2.
Our schemereleases our hands from the confusing situations, by leaving them to our data-driven method described inthe following section.
Figure 1 and Figure 2 abbreviate the manual classification and their correspondingexamples.Many prepositions in Chinese are evolved from verbs, thus the linguistic characteristics of preposi-tions are somewhat similar to verbs.
Therefore, this paper divides the preposition word set according to1888Subordinating Conjunction????????????????????????????????????????????
?Coordination: bothQProgression: not only?=Transition: although?,Preference: rather than?
?Cause: becauseduCondition????????????????????
?Assumption: ifXJUniversalization: whatever?
?Unnecessary Condition: sinceQ,Insufficient Condition: although=?Sufficient Condition: as long as?
?Necessary Condition: only if?kEquality: unless??...
...Figure 1: Abbreviated Hierarchical subcategories of subordinating conjunctions with examples.Adverb??????????????????????????????????????????????????????????
?Conjunctive Adverb?????????????????????????????????????????Transition??????
?Preference: would rather?XIn case: lest?
?Otherwise: or else?KHowever: but%Result??
?Therefore: so?
?Then, As a result: as soon?So that: so that?BProgression??????
?Furthermore: but also?In addition: moreover,Later: subsequently? As well: likewise?...Adjunct Adverb??
?Frequency Adverbs: for many times?gDegree Adverbs: very4?......Figure 2: Abbreviated Hierarchical subcategories of adverbs with examples.the types of their associated arguments: ?benefactive?, such as ??(for)?
and ??
(to)?, marks the ben-eficiary of an action; ?locative?, such as ?3(in)?, marks adverbials that indicate the place of the event;?direction?, such as ?
?(towards)?
and ?
d(from)?, marks adverbials that answer the questions ?fromwhere??
and ?to where??
; ?temporal?, such as ?3(on)?, marks temporal or aspectual adverbials thatanswer the question ?when?
?, and so on.4 Refining Grammar with Hierarchical Category RefinementIn this section, we choose the appropriate granularity in a data-driven manner based on the split-mergelearning method in Section 2.1.
Our approach first initializes the categories with the most general sub-categories in the taxonomy and then splits the categories through the hypernym-hyponym relation in the1889taxonomy.
Data-driven method is used to merge the overly refined subcategories.The top category in the taxonomy is used as the starting annotations of POS tags.
As we cannotpredict which layer should be the most adequate one, we try to avoid applying any priori restriction onthe refinement granularity, and start with the most general tags.With the hierarchical knowledge, it turns out to be a critical issue that which granularity should beused to refine the tags for parsing.
We intend to take neither too coarse subcategories nor too fine ones inthe hierarchical knowledge for parsing.
Instead, it would be our advantage to split the tags with the verygranularity where needed, rather than splitting them all to one specific granularity in the taxonomy.For example, ?Conjunctive Adverbs?
are divided into three subcategories in our taxonomy as shownin Figure 2.
The evidence for the refinement may occur in very rare case, and certainly some of thecontext of the different subcategories are quite the same.
Splitting symbols with the same context isnot only unnecessary, but potentially harmful, since it unreasonably fragments observations of othersymbols.behavior.In this paper, the hierarchical subcategory knowledge is used to refine grammars by supervising theautomatic hierarchical state-split approach.
In the split stage in each cycle, the function word subcategoryis split along the hierarchy of the knowledge, instead of being randomly split and classified automatically.In this way, we try to alleviate the over-fitting of the greedy data-driven approach, and a new set ofknowledge-related tags are generated.
In the following step, we retreat some of the split subcategories totheir more general layer according to its likelihood loss of merging them.
In this way, we try to avoid theexcessive refinement in our hierarchical knowledge without sufficient data support.There are two issues that we have to consider in this process: a) how to deal with the polysemouswords, and b) how to deal with the multi-branch situation other than binary branch in the taxonomy.Regarding to the polysemous words, they occur mostly in two situation for function words.
Some are thepolysemous words which can be taken as conjunctions or auxiliary words, while the others can be takenas preposition or adverbs.
Fortunately there is no ambiguity for a word given its POS tag, so we couldneglect this situation in the split process when training.
We demonstrated the solution for the multiplebranches in the Section 5.5 Learning the Taxonomy of Function WordsThere are multiple subcategorization criterions for building function word taxonomy, and it is diffi-culty for human to rank the ordering in the classification process.
This section represents the methodof learning the taxonomy of the function words in data-driven manner.
Based on the manual tabularclassification, the similar word classes are conflated to express the data distribution.The multiple branches in the taxonomy are intractable for the original split-merge method, because itsplits every category into two and merges half of them for efficiency.
If we follow this scheme in ourtraining process, it would be difficult to deal with the multi-branch situation in the taxonomy, becausehow to choose the first two to split among the multiple branches is another challenge.
It is an equallydifficult problem for us to binarize the taxonomy by hand comparing to directly choosing the granularity.It would be our advantage to binarize the taxonomy by a data-driven method.
For automatic binariza-tion, a straightforward approach is to measure the utility of traversing all the plausible ways of cutting allthe branches into two sets individually and use the best one.
Then we can deal with the divided two setsin the same manner recursively.
However, not only is this impractical, requiring an entire training phasefor each possible binarization scheme which is exponentially expensive, but it assumes the contributionsof multiple binarizations in different branches are independent.
In fact, extra sub-symbols may need tobe added to several nonterminals before they can cooperate to pass information along the parse tree.Therefore, we go in the opposite direction, and propose an extended version of split-merge learningto handle the multiple branches in the taxonomy.
That is, we split each state into all the subcategories inthe lower layer in the taxonomy even if it has multiple branches, train, and then measure for every twosibling subcategories in the same layer the loss in likelihood incurred when merging them.
If this loss issmall, the new division of these two subcategories does not carry enough useful information and can bemerged back.
Contrary to the gain in likelihood for splitting, the loss in likelihood for merging can be1890efficiently approximated (Petrov et al., 2006).More specifically, we assume transitivity in merging multiple subcategories in one layer.
Figure 3gives an illustration.
After the split stage, the category A has been split into subcategories A-1, A-2, ...to A-7.
Then we compute the loss in likelihood of the training data by merging back each pair of twosubcategories through A-1 to A-7.
If the loss is lower than a certain threshold1set for each round ofmerge, this pair of newly split subcategories will be merged.
We only show the sibling ones for brevity inthis example.
Assume the losses of merging these pairs (A-1, A-2), (A-2, A-3), (A-3, A-4) and (A-4, A-5) are below the threshold ?.
Thus, A-1, A-2, A-3, A-4 and A-5 are merged to X-1 due to the transitivityof the connected points, where X-1 is the automatically generated subcategory which contains the fiveconflated subcategories as its descendants.
At the meantime, A-6 and A-7 still remain.
This scheme is anapproximation because it merges subcategories that should be merged with the same subcategory.
But itwill leave the split of this instances to the next round when more evidence on interaction with other morerefined subcategories is given.
(a) Refined subcategories before the merge stage (b) Refined subcategories after the merge stageFigure 3: Illustration of merging the subcategories for multiple branches in the taxonomy.
Where ?
isa certain threshold below which this pair of subcategories will be merged, and X is the automaticallygenerated subcategory which contains the conflated subcategories as its descendants.After merging in each round, the hierarchical knowledge is reshaped to fit the practical usage in theTreebank.
The split-merge cycles allow us to progressively increase the complexity of the hierarchicalknowledge, and the more useful distinctions are represented as the higher level in the taxonomy, whichgives priority to the most useful distinctions in return by supervising the grammar induction.
Figure 4demonstrates the transformation of the hierarchical structure from the tabular classification.
Along thisroad, the training scheme is not a unilateral training, but an interactive process between the knowledgerepresentation learning and the grammar training.
Our learning process exerts a mutual effect to both theinduced grammar and the optimized structure of the hierarchical knowledge.
In this way, the set of di-viding standards are chosen iteratively according to their syntactic features.
The more effective divisionsare conducted in the early stages.
In the following stages, the divisions which interact with previousdivisions to give the most effective disambiguating information are adopted.
The final taxonomy arebuilt based on manual classification in data-driven approach, and the hierarchical structure are optimizedand rational in the perspective of actual data distribution.
Figure 4 illustrates a concrete instance of theprocedure of learning the taxonomy.
On one hand, this procedure provides a more rational hierarchicalsubcategorization structure according to data distribution.
On the other hand, the order of the divisioncriterions represents the priorities the grammar induction takes for each criterion.
The structure in thehigher levels of the taxonomy are determined by the dominant syntactic characteristics.
And the divisionin the later iterations are on the basis of minor distinctive characteristics.6 Experiments and Results6.1 Data SetWe present experimental results on both CTB5.0 (All traces and functional tags were stripped.)
and TCT.We ran experiments on CTB5.0 using the standard data allocation: files from CHTB 001.fid toCHTB 270.fid, and files from CHTB 400.fid to CHTB 1151.fid were used as training set.
The develop-ment set includes files from CHTB 301.fid to CHTB 325.fid, and the test set includes files CHTB 271.fid1In practice, instead of setting a predefined threshold for merging, we merge a specific number of the newly split subcate-gories.1891AA-7A-6A-5A-4A-3A-2A-1(a) First round of category splitAA-7A-6X-1X-1A-5A-4A-3A-2A-1(b) First round of category mergeAA-7A-6X-1A-5A-4A-3A-2A-1(c) Second round of category splitAA-7A-6X-1X-3X-2X-2A-2A-1X-3A-5A-4A-3(d) Second round of category mergeAA-7A-6X-1X-3A-5A-4A-3X-2A-2A-1(e) Third round of category splitAA-7A-6X-1X-3X-4A-3X-2A-2A-1X-4A-5A-4(f) Third round of category mergeFigure 4: Iteration of grammar induction and taxonomy structure learningto CHTB 300.fid.
Experiments on TCT use the data set as in CIPS-SIGHAN-ParsEval-2012 (Zhou,2012).
We have parsed on the segmented text in the Treebank, namely, no use of gold POS-tags, useof gold segmentations, and full-length sentences.
This is the same as for other 5 parsers in Table 1 forcomparison.
All the experiments were carried out after six cycles of split-merge.18926.2 Final ResultsThe final results are shown in Table 1.
Our final parsing performance is higher than both the manualannotation method (Levy and Manning, 2003) and the data-driven method (Petrov, 2009).Parser Precision Recall F1Levy(2003) 78.40 79.20 78.80Petrov(2009) 84.82 81.93 83.33Lin(2009) 86.00 83.10 84.50Qian(2012) 84.57 83.68 84.13Zhang(2013) 84.42 84.43 84.43This paper 86.55 83.41 84.95Table 1: Our final parsing performance compared with the best previous work on CTB5.0.On test set TCT, the method achieves the best precision, recall and F-measure in the CIPS-SIGHAN-ParsEval-2012 competition, and table 2 compares our results with the system of Beijing InformationScience and Technology University (BISTU) which got the second place in the competition.Parser Precision Recall F1BISTU 70.10 68.08 69.08This paper 76.81 76.66 76.74Table 2: Our final parsing performance compared with the best previous works on TCT.Given the manual labor required for generating the taxonomy (and in languages where there is ataxonomy, determining whether it is suitable), this first study focuses on a language where there is quitea bit of under- and over-specification in the Treebanks?
tag sets.
So this work is only implemented onChinese.
We regard it as future work to transfer this approach to other languages.6.3 AnalysisThe outline of constructing the taxonomy of function words are as follows.
Firstly, the function words aremanually subcategorized in a rough and cursory way.
When dealing with subcategories hard to resolvetheir relation of subordination, we simply treat them as siblings in the tree in a rather flat stricture, andleave the elaboration of exquisite clustering to the algorithms.
The data-driven approach in Section 4 au-tomatically choose the appropriate granularity of refinement for our grammar.
Moreover, the split-mergelearning for multiple branches in the hierarchical subcategories in Section 5 exploits the relationship be-tween the sibling nodes in the same layer, making use of the Treebank data to adjust and optimize thehierarchy.During the split-merge process, the hierarchical subcategories are learned to fit the data, which isa transformation of our manually defined hierarchy.
The transformed hierarchy is just the route mapof subcategories employed in our model.
As abbreviated in Figure 5 and Figure 6, many distinctionsbetween word sets of the subcategories have been exploited by our approach, and the learned taxonomyis interpretable.
For instance, It shows that the learned structure of the taxonomy is reasonable.6.4 Comparison with Previous WorkAlthough the taxonomy of function words are learned in the grammar training process, the grammar istrained on the Treebank in supervised manner.
Thus, this work is not directly relevant with unsupervisedgrammar induction literature (Headden III et al., 2009; Berant et al., 2007; Mare?cek and?Zabokrtsk`y,2014).1893Subordinating Conjunction????????????????????????????????????????????????????
?Coordination: bothQX{Progression: not only?=Transition: although?,X{Preference: rather than?
?Cause: becauseduCondition??????????????????????
?Universalization: ?
?Equality: ??X???????????????X????
?X{Assumption: XJSufficient Condition: ?
?Necessary Condition: ?kX{Unnecessary Condition: Q,Insufficient Condition: =?...Figure 5: Abbreviated automatically learned hierarchical subcategories of subordinating conjunctionswith examples.
Where ?X?
represents the automatically generated subcategory.Conjunctive Adverb?????????????????????????????????????????????Transition????????
?Preference: would rather?XX{In case: lest?
?Otherwise: or else?KHowever: but%Result????
?X{Therefore: so?
?So that: so that?BThen, As a result: as soon?Progression?????????X????
?X{Furthermore: but also?In addition: moreover,Later: subsequently? As well: likewise?...Figure 6: Abbreviated automatically learned hierarchical subcategories of adverbs with examples.Lin et al.
(2009) and Li et al.
(2014b) presented ideas of using either hierarchical semantic knowledgefrom HowNet for content words or grammar knowledge for subordinating conjunctions.
They introducedhierarchical subcategory knowledge in a different stage.
They split the original Treebank categoriesin split-merge process according to the data, and then find a method to map the subcategories to thenode in the taxonomy, and constrain their further splitting.
Comparing to their work, our approach ismore delicate, which is splitting the categories according to the knowledge, and learning the knowledgestructure according to data during the training course.
Lin et al.
(2009) incorporated semantic knowledgeof content words into the data-driven method.
It would be promising if this work stacks with the contentword knowledge.
However, the work with content word knowledge have to handle the polysemous wordsin the semantic taxonomy, so they split the categories according to the data, and then find a way to mapthe subcategories to the node in the taxonomy, and constrain their further splitting.
It is our goal to makethese two methods compatible with each other.Incorporating word formation knowledge achieved higher parsing accuracy according to Zhang and1894Clark (2011).
However, they ran their experiment on gold POS-tags and a different data set split, whichis different form the setup of work in Table 1 including this work.
They also presented their result onautomatically assigned POS-tags and the same data set split as in the work in Table 1 to facilitate theperformance comparison.
It gave F1score of 81.45% for sentences with less than 40 words and 78.3%for all sentences, significantly lower than Petrov and Klein (2007).Zhang et al.
(2013) exhaustively exploited character-level syntactic structures for words, and achieved84.43% on F1measure.
They placed more emphasis on the word-formation of content words, whichour model highlights the value of the function words.
The complementary intuitions make it possible tointegrate these approaches together in the future work.7 ConclusionThis paper presents an approach for inducing finer syntactic categories while learning the taxonomy forfunction words.
It used linguistic insight to guide the state-split process, and the hierarchical structurerepresenting syntactic features of function word usages was established during the grammar trainingprocess.
Empirical evidence has been provided that automatically subcategorizing function words con-tributes to high parsing performance.
The induced grammar supervised by the taxonomy outperformedpervious approaches, which benefited from both the knowledge and the data-driven method.
The pro-posed approach for learning the structure of the taxonomy could be generalized to construct semanticknowledge base.AcknowledgmentsThis work was supported in part by the National Basic Research Program of China (973 Program) undergrant 2013CB329304, the Research Special Fund for Public Welfare Industry of Health under grant201202001, the Key National Social Science Foundation of China under grant 12&ZD119, the NationalNatural Science Foundation of China under grant 91120001.ReferencesEneko Agirre, Timothy Baldwin, and David Martinez.
2008.
Improving parsing and pp attachment performancewith sense information.
Proceedings of ACL-08: HLT, pages 317?325.Jonathan Berant, Yaron Gross, Matan Mussel, Ben Sandbank, Eytan Ruppin, and Shimon Edelman.
2007.
Boost-ing unsupervised grammar induction by splitting complex sentences on function words.
In Proceedings of theBoston University Conference on Language Development.Shay B Cohen, Karl Stratos, Michael Collins, Dean P Foster, and Lyle Ungar.
2012.
Spectral learning of latent-variable pcfgs.
In Proceedings of the 50th annual meeting of the Association for Computational Linguistics:Long Papers-Volume 1, pages 223?231.
Association for Computational Linguistics.Zhendong Dong and Qiang Dong.
2003.
Hownet-a hybrid language and knowledge resource.
In Proceedingsof the international conference on natural language processing and knowledge engineering, pages 820?824.IEEE.Zhengdong Dong and Qiang Dong.
2006.
HowNet and the computation of meaning.
World Scientific PublishingCo.
Pte.
Ltd.Christiane Fellbaum.
1999.
WordNet.
Wiley Online Library.Sanae Fujita, Francis Bond, Stephan Oepen, and Takaaki Tanaka.
2010.
Exploiting semantic information for hpsgparse selection.
Research on language and computation, 8(1):1?22.William P Headden III, Mark Johnson, and David McClosky.
2009.
Improving unsupervised dependency parsingwith richer contexts and smoothing.
In Proceedings of Human Language Technologies: The 2009 AnnualConference of the North American Chapter of the Association for Computational Linguistics, pages 101?109.Association for Computational Linguistics.Dan Klein and Christopher D Manning.
2003.
Accurate unlexicalized parsing.
In Proceedings of the 41st annualmeeting on Association for Computational Linguistics-Volume 1, pages 423?430.
Association for ComputationalLinguistics.1895Roger Levy and Christopher D Manning.
2003.
Is it harder to parse chinese, or the chinese treebank?
InProceedings of the 41st annual meeting on Association for Computational Linguistics-Volume 1, pages 439?446.
Association for Computational Linguistics.Dongchen Li, Xiantao Zhang, and Xihong Wu.
2014a.
Improved parsing with taxonomy of conjunctions.
In 2014IEEE China Summit & International Conference on Signal and Information Processing.
IEEE.Dongchen Li, Xiantao Zhang, and Xihong Wu.
2014b.
Learning grammar with explicit annotations for subordi-nating conjunctions in chinese.
In Proceedings of the 52th annual meeting of the Association for ComputationalLinguistics Student Research Workshop.
Association for Computational Linguistics.Xiaojun Lin, Yang Fan, Meng Zhang, Xihong Wu, and Huisheng Chi.
2009.
Refining grammars for parsingwith hierarchical semantic knowledge.
In Proceedings of the 2009 conference on empirical methods in naturallanguage processing: Volume 3-Volume 3, pages 1298?1307.
Association for Computational Linguistics.David Mare?cek and Zden?ek?Zabokrtsk`y.
2014.
Dealing with function words in unsupervised dependency parsing.In Computational Linguistics and Intelligent Text Processing, pages 250?261.
Springer.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2005.
Probabilistic cfg with latent annotations.
In Pro-ceedings of the 43rd annual meeting on Association for Computational Linguistics, pages 75?82.
Associationfor Computational Linguistics.George A Miller.
1995.
Wordnet: a lexical database for english.
Communications of the ACM, 38(11):39?41.Slav Petrov and Dan Klein.
2007.
Improved inference for unlexicalized parsing.
In Human language technologies2007: the conference of the North American chapter of the Association for Computational Linguistics, pages404?411.Slav Petrov, Leon Barrett, Romain Thibaux, and Dan Klein.
2006.
Learning accurate, compact, and interpretabletree annotation.
In Proceedings of the 21st international conference on computational linguistics and the 44thannual meeting of the Association for Computational Linguistics, pages 433?440.
Association for Computa-tional Linguistics.Slav Orlinov Petrov.
2009.
Coarse-to-Fine natural language processing.
Ph.D. thesis, University of California.Detlef Prescher.
2005.
Inducing head-driven pcfgs with latent heads: Refining a tree-bank grammar for parsing.In Machine Learning: ECML 2005, pages 292?304.
Springer.Hui Wang and Shiwen Yu.
2003.
The semantic knowledge-base of contemporary chinese and its applicationsin wsd.
In Proceedings of the second SIGHAN workshop on Chinese language processing-Volume 17, pages112?118.
Association for Computational Linguistics.Fei Xia.
2000.
The part-of-speech tagging guidelines for the penn chinese treebank (3.0).
Technical report.Deyi Xiong, Shuanglong Li, Qun Liu, Shouxun Lin, and Yueliang Qian.
2005.
Parsing the penn chinese treebankwith semantic knowledge.
In Natural language processing?IJCNLP 2005, pages 70?81.
Springer.Nianwen Xue, Fei Xia, Shizhe Huang, and Anthony Kroch.
2000.
The bracketing guidelines for the penn chinesetreebank (3.0).
Technical report.Nianwen Xue, Fu-Dong Chiou, and Martha Palmer.
2002.
Building a large-scale annotated chinese corpus.
InProceedings of the 19th international conference on computational linguistics-Volume 1, pages 1?8.
Associationfor Computational Linguistics.Yue Zhang and Stephen Clark.
2011.
Syntactic processing using the generalized perceptron and beam search.Computational linguistics, 37(1):105?151.Meishan Zhang, Yue Zhang, Wanxiang Che, and Ting Liu.
2013.
Chinese parsing exploiting characters.
51stannual meeting of the Association for Computational Linguistics.Qiang Zhou.
2004.
Annotation scheme for chinese treebank.
Journal of Chinese information processing, 18(4):1?8.Qiang Zhou.
2012.
Evaluation report of the third chinese parsing evaluation: Cips-sighan-parseval-2012.
InProceedings of the second CIPS-SIGHAN joint conference on Chinese language processing, pages 159?167.Xuefeng Zhu, Shiwen Yu, and Hui Wang.
1995.
The development of contemporary chinese grammatical knowl-edge base and its applications.
International journal of asian language processing, 5(1,2):39?41.1896
