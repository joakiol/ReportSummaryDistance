Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 226?230, New York City, June 2006. c?2006 Association for Computational LinguisticsMulti-lingual Dependency Parsing with Incremental Integer LinearProgrammingSebastian Riedel and Ruket C?ak?c?
and Ivan Meza-RuizICCSSchool of InformaticsUniversity of EdinburghEdinburgh, EH8 9LW, UKS.R.Riedel,R.Cakici,I.V.Meza-Ruiz@sms.ed.ac.ukAbstractOur approach to dependency parsing isbased on the linear model of McDonaldet al(McDonald et al, 2005b).
Instead ofsolving the linear model using the Max-imum Spanning Tree algorithm we pro-pose an incremental Integer Linear Pro-gramming formulation of the problem thatallows us to enforce linguistic constraints.Our results show only marginal improve-ments over the non-constrained parser.
Inaddition to the fact that many parses didnot violate any constraints in the first placethis can be attributed to three reasons: 1)the next best solution that fulfils the con-straints yields equal or less accuracy, 2)noisy POS tags and 3) occasionally ourinference algorithm was too slow and de-coding timed out.1 IntroductionThis paper presents our submission for the CoNLL2006 shared task of multilingual dependency pars-ing.
Our parser is inspired by McDonald etal.
(2005a) which treats the task as the search for thehighest scoring Maximum Spanning Tree (MST) ina graph.
This framework is efficient for both pro-jective and non-projective parsing and provides anonline learning algorithm which combined with arich feature set creates state-of-the-art performanceacross multiple languages (McDonald and Pereira,2006).However, McDonald and Pereira (2006) mentionthe restrictive nature of this parsing algorithm.
Intheir original framework, features are only definedover single attachment decisions.
This leads to caseswhere basic linguistic constraints are not satisfied(e.g.
verbs with two subjects).
In this paper wepresent a novel way to implement the parsing al-gorithms for projective and non-projective parsingbased on a more generic incremental Integer LinearProgramming (ILP) approach.
This allows us to in-clude additional global constraints that can be usedto impose linguistic information.The rest of the paper is organised in the followingway.
First we give an overview of the Integer LinearProgramming model and how we trained its param-eters.
We then describe our feature and constraintsets for the 12 different languages of the task (Hajic?et al, 2004; Chen et al, 2003; Bo?hmova?
et al, 2003;Kromann, 2003; van der Beek et al, 2002; Brantset al, 2002; Kawata and Bartels, 2000; Afonso etal., 2002; Dz?eroski et al, 2006; Civit Torruella andMart??
Anton?
?n, 2002; Nilsson et al, 2005; Oflazer etal., 2003; Atalay et al, 2003).
Finally, our results arediscussed and error analyses for Chinese and Turk-ish are presented.2 ModelOur model is based on the linear model presented inMcDonald et al (2005a),s (x,y) =?
(i,j)?ys (i, j) =?w ?
f (i, j)(1)where x is a sentence, y a parse and s a score func-tion over sentence-parse pairs.
f (i, j) is a multidi-226mensional feature vector representation of the edgefrom token i to token j and w the correspondingweight vector.
Decoding in this model amounts tofinding the y for a given x that maximises s (x,y)y?
= argmaxys (x,y)and y contains no cycles, attaches exactly one headto each non-root token and no head to the root node.2.1 DecodingInstead of using the MST algorithm (McDonald etal., 2005b) to maximise equation 1, we present anequivalent ILP formulation of the problem.
An ad-vantage of a general purpose inference technique isthe addition of further linguistically motivated con-straints.
For instance, we can add constraints thatenforce that a verb can not have more than one sub-ject argument or that coordination arguments shouldhave compatible types.
Roth and Yih (2005) issimilarly motivated and uses ILP to deal with ad-ditional hard constraints in a Conditional RandomField model for Semantic Role Labelling.There are several explicit formulations of theMST problem as integer programs in the literature(Williams, 2002).
They are based on the concept ofeliminating subtours (cycles), cuts (disconnections)or requiring intervertex flows (paths).
However, inpractice these cause long solving times.
While thefirst two types yield an exponential number of con-straints, the latter one scales cubically but producesnon-fractional solutions in its relaxed version, caus-ing long runtime of the branch and bound algorithm.In practice solving models of this form did not con-verge after hours even for small sentences.To get around this problem we followed an incre-mental approach akin to Warme (1998).
Instead ofadding constraints that forbid all possible cycles inadvance (this would result in an exponential num-ber of constraints) we first solve the problem withoutany cycle constraints.
Only if the result contains cy-cles we add constraints that forbid these cycles andrun the solver again.
This process is repeated un-til no more violated constraints are found.
Figure 1shows this algorithm.Groetschel et al (1981) showed that such an ap-proach will converge after a polynomial number ofiterations with respect to the number of variables.1.
Solve IP Pi2.
Find violated constraints C in the solution of Pi3.
if C = ?
we are done4.
Pi+1 = Pi ?
C5.
i = i + 16. goto (1)Figure 1: Incremental Integer Linear ProgrammingIn practice, this technique showed fast convergence(less than 10 iterations) in most cases, yielding solv-ing times of less than 0.5 seconds.
However, forsome sentences in certain languages, such as Chi-nese or Swedish, an optimal solution could not befound after 500 iterations.In the following section we present the bjectivefunction, variables and linear constraints that makeup the Integer Linear Program.2.1.1 VariablesIn the implementation1 of McDonald et al(2005b) dependency labels are handled by findingthe best scoring label for a given token pair so thats (i, j) = max s (i, j, label)goes into Equation 1.
This is only exact as long as nofurther constraints are added.
Since our aim is to addconstraints our variables need to explicitly model la-bel decisions.
Therefore, we introduce binary vari-ablesli,j,label?i ?
0..n, j ?
1..n, label ?
bestb (i, j)where n is the number of tokens and the index 0represents the root token.
bestb (i, j) is the set of blabels with maximal s (i, j, label).
li,j,label equals 1if there is a dependency with the label label betweentoken i (head) and j (child), 0 otherwise.Furthermore, we introduce binary auxiliary vari-ablesdi,j?i ?
0..n, j ?
1..nrepresenting the existence of a dependency betweentokens i and j.
We connect these to the li,j,label vari-ables by a constraintdi,j =?labelli,j,label.1Note, however, that labelled parsing is not described in thepublication.2272.1.2 Objective FunctionGiven the above variables our objective functioncan be represented as?i,j?label?bestk(i,j)s (i, j, label) ?
li,j,labelwith a suitable k.2.1.3 Constraints Added in AdvanceOnly One Head In all our languages every tokenhas exactly one head.
This yields?i>0di,j = 1for non-root tokens j > 0 and?idi,0 = 0for the artificial root node.Typed Arity Constraints We might encounter so-lutions of the basic model that contain, for instance,verbs with two subjects.
To forbid these we simplyaugment our model with constraints such as?jli,j,subject ?
1for all verbs i in a sentence.2.1.4 Incremental ConstraintsNo Cycles If a solution contains one or more cy-cles C we add the following constraints to our IP:For every c ?
C we add?
(i,j)?cdi,j ?
|c| ?
1to forbid c.Coordination Argument Constraints In coordi-nation conjuncts have to be of compatible types.
Forexample, nouns can not coordinate with verbs.
Weimplemented this constraint by checking the parsesfor occurrences of incompatible arguments.
If wefind two arguments j, k for a conjunction i: di,j anddi,k and j is a noun and k is a verb then we adddi,j + di,k ?
1to forbid configurations in which both dependenciesare active.Projective Parsing In the incremental ILP frame-work projective parsing can be easily implementedby checking for crossing dependencies after each it-eration and forbidding them in the next.
If we seetwo dependencies that cross, di,j and dk,l, we addthe constraintdi,j + dk,l ?
1to prevent this in the next iteration.
This can alsobe used to prevent specific types of crossings.
Forinstance, in Dutch we could only allow crossing de-pendencies as long as none of the dependencies is a?Determiner?
relation.2.2 TrainingWe used single-best MIRA(Crammer and Singer,2003).For all experiments we used 10 training iter-ations and non-projective decoding.
Note that weused the original spanning tree algorithm for decod-ing during training as it was faster.3 System SummaryWe use four different feature sets.
The first fea-ture set, BASELINE, is taken from McDonald andPereira (2005b).
It uses the FORM and the POSTAGfields.
This set alo includes features that combinethe label and POS tag of head and child such as(Label, POSHead) and (Label, POSChild?1).
Forour Arabic and Japanese development sets we ob-tained the best results with this configuration.
Wealso use this configuration for Chinese, German andPortuguese because training with other configura-tions took too much time (more than 7 days).The BASELINE also uses pseudo-coarse-POS tag(1st character of the POSTAG) and pseudo-lemmatag (4 characters of the FORM when the lengthis more than 3).
For the next configuration wesubstitute these pseudo-tags by the CPOSTAG andLEMMA fields that were given in the data.
This con-figuration was used for Czech because for other con-figurations training could not be finished in time.The third feature set tries to exploit the genericFEATS field, which can contain a list features suchas case and gender.
A set of features per depen-dency is extracted using this information.
It con-sists of cross product of the features in FEATS.
Weused this configuration for Danish, Dutch, Spanish228and Turkish where it showed the best results duringdevelopment.The fourth feature set uses the triplet of la-bel, POS child and head as a feature such as(Label, POSHead, POSChild).
It also uses theCPOSTAG and LEMMA fields for the head.
Thisconfiguration is used for Slovene and Swedish datawhere it performed best during development.Finally, we add constraints for Chinese, Dutch,Japanese and Slovene.
In particular, arity constraintsto Chinese and Slovene, coordination and arity con-straints to Dutch, arity and selective projectivityconstraints for Japanese2.
For all experiments b wasset to 2.
We did not apply additional constraints toany other languages due to lack of time.4 ResultsOur results on the test set are shown in Table 1.Our results are well above the average for all lan-guages but Czech.
For Chinese we perform signif-icantly better than all other participants (p = 0.00)and we are in the top three entries for Dutch, Ger-man, Danish.
Although Dutch and Chinese are lan-guages were we included additional constraints, ourscores are not a result of these.
Table 2 compares theresult for the languages with additional constraints.Adding constraints only marginally helps to improvethe system (in the case of Slovene a bug in our im-plementation even degraded accuracy).
A more de-tailed explanation to this observation is given in thefollowing section.
A possible explanation for ourhigh accuracy in Chinese could be the fact that wewere not able to optimise the feature set on the de-velopment set (see the previous section).
Maybe thisprevented us from overfitting.
It should be noted thatwe did use non-projective parsing for Chinese, al-though the corpus was fully projective.
Our worstresults in comparison with other participants can beseen for Czech.
We attribute this to the reducedtraining set we had to use in order to produce amodel in time, even when using the original MSTalgorithm.2This is done in order to capture the fact that crossing de-pendencies in Japanese could only be introduced through dis-fluencies.4.1 ChineseFor Chinese the parser was augmented with a set ofconstraints that disallowed more than one argumentof the types head, goal, nominal, range, theme, rea-son, DUMMY, DUMMY1 and DUMMY2.By enforcing arity constraints we could either turnwrong labels/heads into right ones and improve ac-curacy or turn right labels/heads into wrong ones anddegrade accuracy.
For the test set the number of im-provements (36) was higher than the number of er-rors (22).
However, this margin was outweighed bya few sentences we could not properly process be-cause our inference method timed out.
Our overallimprovement was thus unimpressive 7 tokens.In the context of duplicate ?head?
dependencies(that is, dependencies labelled ?head?)
the num-ber of sentences where accuracy dropped far out-weighed the number of sentences where improve-ments could be gained.
Removing the arity con-straints on ?head?
labels therefore should improveour results.This shows the importance of good second bestdependencies.
If the dependency with the secondhighest score is the actual gold dependency and itsscore is close to the highest score, we are likely topick this dependency in the presence of additionalconstraints.
On the other hand, if the dependencywith the second highest score is not the gold one andits score is too high, we will probably include thisdependency in order to fulfil the constraints.There may be some further improvement to begained if we train our model using k-best MIRAwith k > 1 since it optimises weights with respectto the k best parses.4.2 TurkishThere is a considerable gap between the unlabelledand labelled results for Turkish.
And in terms of la-bels the POS type Noun gives the worst performancebecause many times a subject was classified as ob-ject or vice a versa.Case information in Turkish assigns argumentroles for nouns by marking different semantic roles.Many errors in the Turkish data might have beencaused by the fact that this information was not ad-equately used.
Instead of fine-tuning our feature setto Turkish we used the feature cross product as de-229Model AR CH CZ DA DU GE JP PO SL SP SW TUOURS 66.65 89.96 67.64 83.63 78.59 86.24 90.51 84.43 71.20 77.38 80.66 58.61AVG 59.94 78.32 67.17 78.31 70.73 78.58 85.86 80.63 65.16 73.53 76.44 55.95TOP 66.91 89.96 80.18 84.79 79.19 87.34 91.65 87.60 73.44 82.25 84.58 65.68Table 1: Labelled accuracy on the test sets.Constraints DU CH SL JAwith 3927 4464 3612 4526without 3928 4471 3563 4528Table 2: Number of tokens correctly classified withand without constraints.scribed in Section 3.
Some of the rather meaning-less combinations might have neutralised the effectof sensible ones.
We believe that using morpho-logical case information in a sound way would im-prove both the unlabelled and the labelled dependen-cies.
However, we have not performed a separate ex-periment to test if using the case information alonewould improve the system any better.
This could bethe focus of future work.5 ConclusionIn this work we presented a novel way of solving thelinear model of McDonald et al (2005a) for projec-tive and non-projective parsing based on an incre-mental ILP approach.
This allowed us to includeadditional linguistics constraints such as ?a verb canonly have one subject.
?Due to time constraints we applied additionalconstraints to only four languages.
For each onewe gained better results than the baseline withoutconstraints, however, this improvement was onlymarginal.
This can be attributed to 4 main rea-sons: Firstly, the next best solution that fulfils theconstraints was even worse (Chinese).
Secondly,noisy POS tags caused coordination constraints tofail (Dutch).
Thirdly, inference timed out (Chinese)and fourthly, constraints were not violated that oftenin the first place (Japanese).However, the effect of the first problem might bereduced by training with a higher k. The secondproblem could partly be overcome by using a bet-ter tagger or by a special treatment within the con-straint handling for word types which are likely tobe mistagged.
The third problem could be avoidableby adding constraints during the branch and boundalgorithm, avoiding the need to resolve the full prob-lem ?from scratch?
for every constraint added.
Withthese remedies significant improvements to the ac-curacy for some languages might be possible.6 AcknowledgementsWe would like to thank Beata Kouchnir, AbhishekArun and James Clarke for their help during thecourse of this project.ReferencesKoby Crammer and Yoram Singer.
2003.
Ultraconservativeonline algorithms for multiclass problems.
J. Mach.
Learn.Res., 3:951?991.M.
Groetschel, L. Lovasz, and A. Schrijver.
1981.
The ellipsoidmethod and its consequences in combinatorial optimization.Combinatorica, I:169?
197.R.
McDonald and F. Pereira.
2006.
Online learning of approx-imate dependency parsing algorithms.
In Proc.
of the 11thAnnual Meeting of the EACL.R.
McDonald, K. Crammer, and F. Pereira.
2005a.
Onlinelarge-margin training of dependency parsers.
In Proc.
of the43rd Annual Meeting of the ACL.Ryan McDonald, Fernando Pereira, Kiril Ribarov, and Jan Ha-jic.
2005b.
Non-projective dependency parsing using span-ning tree algorithms.
In Proceedings of HLT/EMNLP 2005,Vancouver, B.C., Canada.D.
Roth and W. Yih.
2005.
Integer linear programming in-ference for conditional random fields.
In Proc.
of the In-ternational Conference on Machine Learning (ICML), pages737?744.David Michael Warme.
1998.
Spanning Trees in Hypergraphswith Application to Steiner Trees.
Ph.D. thesis, University ofVirginia.Justin C. Williams.
2002.
A linear-size zero - one program-ming model for the minimum spanning tree problem in pla-nar graphs.
Networks, 39:53?60.230
