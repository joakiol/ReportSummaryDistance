Ambiguity Resolution in the DMTRANS PLUSHiroaki Kitano, Hideto Tomabechi, and Lori LevinAbstractWe present a cost-based (or energy-based) model of dis-ambiguation.
When a sentence is ambiguous, a parse withthe least cost is chosen from among multiple hypotheses.Each hypothesis i  assigned a cost which is added when:(1) a new instance is created to satisfy reference success,(2) links between instances are created or removed to sat-isfy constraints on concept sequences, and (3) a conceptnode with insufficient priming is used for further process-ing.
This method of ambiguity resolution is implemented inDMT~NS PLUS, which is a second generation bi-direetionalEnglish/Japanese machine translation system based on a mas-sively parallel spreading activation paradigm developed atthe Center for Machine Translation at Carnegie Mellon Uni-versity.Center for Machine TranslationCarnegie Mellon UniversityPittsburgh, PA 15213 U.S.A.access (DMA) paradigm of natural anguage process-ing.
Under the DMA paradigm, the mental state ofthe hearer is modelled by a massively parallel networkrepresenting memory.
Parsing is performed by pass-ing markers in the memory network.
In our model,the meaning of a sentence is viewed as modificationsmade to the memory network.
The meaning of a sen-tence in our model is definable as the difference in thememory network before and after understanding thesentence.2 L imitat ions of  Cur rent  Methodsof  Ambigu i ty  Reso lut ion1 Int roduct ionOne of the central issues in natural anguage under-standing research is ambiguity resolution.
Since manysentences are ambiguous out of context, techniques forambiguity resolution have been an important topic innatural language understanding.
In this paper, we de-scribe a model of ambiguity resolution implementedin DMTRANS PLUS, which is a next generation ma-chine translation system based on a massively parallelcomuputational paradigm.
In our model, ambiguitiesare resolved by evaluating the cost of each hypothe-sis; the hypothesis with the least cost will be selected.Costs are assigned when (1) a new instance is ere-ated to satisfy reference success, (2) links between in-stances are created or removed to satisfy constraintson concept sequences, and (3) a concept node withinsufficient priming is used for further processing.The underlying philosophy of the model is to viewparsing as a dynamic physical process in which onetrajectory is taken from among many other possiblepaths.
Thus our notion of the cost of the hypothesisa representation f the workload required to take thepath representing the hypothesis.
One other impor-tant idea is that our model employs the direct memory*E-mail address i hiroaki@a.nl.cs.cmu.edu.
Also with NECCorporation.Traditional syntactic parsers have been using attach-ment preferences and local syntactic and semantic on-straints for resolving lexical and structural ambiguities.
(\[17\], \[28\], \[2\], \[7\], \[26\], \[11\], \[5\]) However, thesemethods cannot select one interpretation from severalplausible interpretations because they do not incorpo-rate the discourse context of the sentences being parsed(\[81, \[4\]).Connectionist-type approaches as seen in \[18\], \[25\],and \[8\] essentially stick to semantic restrictions andassociations.
However, \[18\], \[25\], \[24\] only providelocal interactions, omitting interaction with contexLMoreover, difficulties regarding variable-binding andembedded sentences should be noticed.In \[8\], world knowledge is used through testing ref-erential success and other sequential tests.
However,this method oes not provide a uniform model of pars-ing: lexical ambiguities are resolved by marker passingand structural disambiguations are resolved by apply-ing separate sequential tests.An approach by \[15\] is similar to our model in thatboth precieve parsing as a physical process.
However,their model, along with most other models, fails tocapture discourse context.\[12\] uses marker passing as a method of contex-tual inference after a parse; however, no contextual in-formation is feed-backed during the sentential parsing(marker-passing is performed after a separate parsing- 72 -process providing multiple hypotheses of the parse).\[20\] is closer to our model in that marker-passingbased contextual inference is used during a sententialparse (i.e., an integrated processing of syntax, seman-tics and pragmatics at real-time); however the parsing(LFG, and ease-frame based) and contextual inferences(marker-passing) are not under an uniform architecture.Past generations of DMTRANS (\[19\], \[23\]) have notincorporated cost-based structural mbiguity resolutionschemes.3 Overview of DMTRANS PLUS3.1 Memory  Access  Pars ingDMTRANS PLUS is a second generation DMA systembased upon DMTRANS (\[19\]) with new methods of am-biguity resolution based on costs.Unlike most natural anguage systems, which arebased on the "Build-and-Store" model, our systememploys a "Recognize-and-Record" model (\[14\],\[19\],\[21\]).
Understanding of an input sentence (or speechinput in ~/iDMTRANS PLUS) is defined as changes madein a memory network.
Parsing and natural anguageunderstanding in these systems are considered to bememory-access processes, identifying existent knowl-edge in memory with the current input.
Sentencesare always parsed in context, i.e., through utilizingthe existing and (currently acquired) knowledge aboutthe world.
In other words, during parsing, relevantdiscourse ntities in memory are constantly being re-membered.The model behind DMTRANS PLUS is a simulationof such a process.
The memory network incorporatesknowledge from morphophonetics to discourse.
Eachnode represents a concept (Concept Class node; CC)or a sequence of concepts (Concept Sequence Classnode; CSC).CCs represent such knowledge as phones (i.e.
\[k\]),phonemes (i.e.
/k/), concepts (i.e.
*Hand-Gun,*Event, *Mtrans-Action), and plans (i.e.
*Pick-Up-Gun).
A hierarchy of Concept Class (CC) entitiesstores knowledge both declaratively and procedurelyas described in \[19\] and \[21\].
Lexieal entries are rep-resented as lexical nodes which are a kind of CC.Phoneme sequences are used only for ~DMTRANSPLUS, the speech-input version of DM'IRANS PLUS.CSCs represent sequences of concepts such asphoneme sequences (i.e.
</k//ed/i//g//il>), conceptsequences (i.e.
<*Conference *Goal-Role *Attend*Want>), and plan sequences (i.e.
<*Declare-Want-Attend *Listen-Instruction>).
The linguistic knowl-edge represented as CSCs can be low-level surfacespecific patterns uch as phrasal lexicon entries \[1\]or material at higher levels of abstration such as inMOP's \[16\].
However, CSCs should not be confusedwith 'discourse segments' \[6\].
In our model, infor-mation represented in discourse segments are distribu-tively incorporated in the memory network.During sentence processing we create concept in-stances (CI) correpsonding to CCs and concept se-quence instances (CSI) corresponding to CSCs.
Thisis a substantial improvement over past DMA research.Lack of instance creation and reference inpast researchwas a major obstacle to seriously modelling discoursephenomena.CIs and CSIs are connected through several types oflinks.
A guided marker passing scheme is employedfor inference on the memory network following meth-ods adopted in past DMA models.DMTRANS PLUS uses three markers for parsing:?
An Activation Marker (A-Marker) is createdwhen a concept is initially activated by a lexicalitem or as a result of concept refinement.
I  indi-cates which instance of a concept is the source ofactivation and contains relevant cost information.A-Markers are passed upward along is-a links inthe abstraction hierarchy.?
A Prediction marker (P-Marker) is passed alonga concept sequence to identify the linear orderof concepts in the sequence.
When an A-Markerreaches a node that has a P-Marker, the P-Markeris sent to the next element of the concept se-quence, thus predicting which node is to be acti-vated next.?
A Context marker (C-Marker) is placed on a nodewhich has contextual priming.Information about which instances originated acti-vations is carried by A-Markers.
The binding list ofinstances and their roles are held in P-Markers 1.The following is the algorithm used in DMTRANSPLUS parsing:Let Lex, Con, Elem, and Seq be a set of lexicalnodes, conceptual nodes, elements of concept se-quences, and concept sequences, respectively.Parse(~For each word w in S, do"Activate(w),For all i and j:if Active(Ni) A Ni E ConIMarker parsing spreading activation is our choice over eon-nectionist network precisely because of this reason.
Variable bind-ing (which cannot be easily handled in counectionist network) canbe trivially attained through structure (information) passing of A-Markers and P-Markers.- 73 -then do concurrently:Activate(isa(Ni)if Active(ej.N~) ^ Predicted(ej.Ni) A-~Last(ej.Ni)then Predict(ej+l.Ni)if Active(ej.Ni) A Predicted(ej.Ni) ^ Last(ej.Ni)then Accept(Ni), Activate(isa(Ni) )Predict(N)for all Ni E N do:if Ni E Con,then Pmark(Ni), Predict(isainv(Ni))if Ni E Elem,then Pmark(Ni), Predict(isainv(N i) )if Ni E Seq,then emark( eo.Ni), Predict(isainv(eo.Ni) )if N~ = NIL,then Stop.ActivateI ,--- instanceof(c)if i = ff thencreate inst( c ), A ddc ost, activate(c)elsefor each i E Ido concurrently:activate(c)Acceptif Constraints ~TAsstone( Constraints), Addcostactivate( isa( c ) )where Ni and ej.Ni denote a node in the memory net-work indexed by i and a j-th element of a node Ni,respectively.Active(N) is true iff a node or an element of a nodegets an A-Marker.Activate(N) sends A-Markers to nodes and elementsgiven in the argument.Predict(N) moves a P-Marker to the next element ofthe CSC.Predicted(N) is true iff a node or an element of a nodegets a P-Marker.Pmark(N) puts a P-Marker on a node or an elementgiven in the argument.Last(N) is true iff an element is the last element of theconcept sequence.Accept(N) creates an instance under N with links whichconnect he instance to other instances.isa(N) returns a list of nodes and elements which areconnected to the node in the argument by abstractionlinks.isainv(N) returns a list of nodes and elements whichare daughters of a node N.Some explanation would help understanding this al-gorithm:1.
Prediction.Initially all the first elements of concept sequences(CSC - Concept Sequence Class) are predicted byputting P-Markers on them.2.
Lexicai Access.A lexical node is activated by the input word.3.
Concept Activation.An A-Marker is created and sent to the correspond-ing CC (Concept Class) nodes.
A cost is added to theA-Marker if the CC is not C-Marked (i.e.
A C-Markeris not placed on it.).4.
Discourse Entity IdentificationA CI (Concept Instance) under the CC is searchedfor.I f  the CI exists, an A-Marker is propagated tohigher CC nodes.Else, a CI node is created under the CC, and anA-Marker is propagated to higher CC nodes.5.
Activation Propagation.An A-Marker is propagated upward in the absl~ac-tion hierarchy.6.
Sequential prediction.When an A-Marker reaches any P-Marked node (i.e.part of CSC), the P-Marker on the node is sent to thenext element of the concept sequence.7.
Contextual PrimingWhen an A-Marker reaches any Contextual Rootnode.
C-Makers are put on the contexual childrennodes designated by the root node.8.
Conceptual Relation Instautiation.When the last element of a concept sequence re-cieves an A-Marker, Constraints (world and dis-course knowledge) are checked for.A CSI is created under the CSC with packaginglinks to each CI.
This process is called concept refine-ment.
See \[19\].The memory network is modified by performinginferences tored in the root CSC which had the ac-cepted CSC attached to it.9.
Activation PropagationA-Marker is propagated from the CSC to highernodes.3.2 Memory  Network  Mod i f i ca t ionSeveral different incidents trigger the modification ofthe memory network during parsing:?
An individual concept is instantiated (i.e.
an in-stance is created) under a CC when the CC re-ceives an A-Marker and a CI (an instance that- 74 -was created by preceding utterances) is not exis-tent.
This instantiation is a creation of a specificdiscourse ntity which may be used as an existentinstance in the subsequent recognitions.A concept sequence instance is created under theaccepted CSC.
In other words, if a whole conceptsequence is accepted, we create an instance ofthe sequence instantiating it with the specific CIsthat were created by (or identified with) the spe-cific lexical inputs.
This newly created instanceis linked to the accepted CSC with a instance re-lation link and to the instances of the elements ofthe concept sequences by links labelled with theirroles given in the CSC.?
Links are created or removed in the CSI creationphase as a result of invoking inferences based onthe knowledge attached to CSCs.
For example,when the parser accepts the sentence I went tothe UMIST, an instance of I is created under theCC representing L Next, a CSI is created underPTRANS.
Since PTRANS entails that the agentis at the location, a location link must be createdbetween the discourse ntities I and UMIST.
Suchrevision of the memory network is conducted byinvoking knowledge attached to each CSC.Since modification of any part of the memory net-work requires ome workload, certain costs are addedto analyses which require such modifications.4 Cost -based  Approach  to theAmbigu i ty  Reso lu t ionAmbiguity resolution in DMTRANS PLUS is based onthe calculation of the cost of each parse.
Costs areattached to each parse during the parse process.Costs are attached when:1.
A CC with insufficient priming is activated,2.
A CI is created under CC, and3.
Constraints imposed on CSC are not satisfied ini-tially and links are created or removed to satisfythe constraint.Costs are attached to A-Markers when these oper-ations are taken because these operations modify thememory network and, hence, workloads are required.Cost information is then carried upward by A-Markers.The parse with the least cost will be chosen.The cost of each hypothesis are calculated by:n mCi = E cij + E constraintlk +biasij=o k=owhere Ci is a cost of the i-th hypothesis, cij is a costcarried by an A-Marker activating the j-th element ofthe CSC for the i-th hypothesis, constrainta is a costof assuming k-th constraint of the i-th hypothesis, andb/as~ represents lexical preference of the CSC for thei-th hypothesis.
This cost is assigned to each CSC andthe value of Ci is passed up by A-Markers if higher-level processing is performed.
At higher levels, eachcij may be a result of the sum of costs at lower-levels.It should be noted that this equation is very simi-lax to the activation function of most neural networksexcept for the fact our equation is a simple linear equa-tion which does not have threshold value.
In fact, ifwe only assume the addition of cost by priming at thelexical-level, our mechanism of ambiguity resolutionwould behave much like connectionist models with-out inhibition among syntactic nodes and excitationlinks from syntax to lexicon 2.
However, the majordifference between our approach and the connectionistapproach is the addition of costs for instance creationand constraint satisfaction.
We will show that thesefactors are especially important in resolving structuralambiguities.The following subsections describe three mecha-nisms that play a role in ambiguity resolution.
How-ever, we do not claim that these are the only mecha-nisms involved in the examples which follow s .4.1 Contextual PrimingIn our system, some CC nodes designated as Contex-tual Root Nodes have a list of thematically relevantnodes.
C-Markers are sent to these nodes as soon asa Contextual Root Node is activated.
Thus each sen-tence and/or each word might influence the interpre-tation of following sentences or words.
When a nodewith C-Marker is activated by receiving an A-Marker,the activation will be propagated with no cost.
Thus, aparse using such nodes would have no cost.
However,when a node without a C-Marker is activated, a smallcost is attached to the interpretation using that node.In \[19\] the discussion of C-Marker propagation con-centrated on the resolution of word-level ambiguities.However, C-Markers are also propagated toconceptual2We have not incorporated these factors primarily because struc-tured P-Markers can play the role of top-down priming; however,we may be incorporating these factors in the future.3For example, in one implementation f DMTRANS, we are us-ing time-delayed decaying activations which resolve ambiguity evenwhen two CI nodes are concurrently active.- 75 -class nodes, which can represent word-level, phrasal,or sentential knowledge.
Therefore, C-Markers canbe used for resolving phrasal-level and sentential-levelambiguities such as structural ambiguities.
For exam-ple, atama ga itai literally means, '(my) head hurts.
'This normally is identified with the concept sequencesassociated with the *have-a-symptom concept classnode, but if the preceding sentence is asita yakuinkaida ('There is a board of directors meeting tomorrow'),the *have-a-problem concept class node must be ac-tivated instead.
Contextual priming attained by C-Markers can also help resolve structural ambiguity insentences like did you read about the problem withthe students?
The cost of each parse will be deter-mined by whether eading with students or problemswith students is contextually activated.
(Of course,many other factors are involved in resolving this typeof ambiguity.
)Our model can incorporate ither C-Markers or aconnectionist-type competitive activation and inhibi-tion scheme for priming.
In the current implementa-tion, we use C-Markers for priming simply because C-Marker propagation is computationaUy less-expensivethan connectionist-type competitive activation and in-hibition schemes 4.
Although connectionist approachescan resolve certain types of lexical ambiguity, theyare computationally expensive unless we have mas-sively parallel computers.
C-Markers are a resonablecompromise because they are sent to semantically rel-evant concept nodes to attain contextual priming with-out computationally expensive competitive activationand inhibition methods.4.2 Reference to the Discourse EntityWhen a lexical node activates any CC node, a CI nodeunder the CC node is searched for (\[19\], \[21\]).
Thisactivity models reference to an already established dis-course entity \[27\] in the heater's mind.
If  such a CInode exists, the reference succeeds and this parse willbe attached with no cost.
However, if no such instanceis found, reference failure results.
If this happens, aninstantiation activity is performed creating a new in-stance with certain costs.
As a result, a parse usingnewly created instance node will be attached with somecost.For example, if a preceding discourse contained areference to a thesis, a CI node such as THESIS005would have been created.
Now if a new input sen-tence contains the word paper, CC nodes for THI/-'*This does not mean that our model can not incorporate a con-nectionist model.
The choice of C-Markers over the eonnectionistapproach is mostly due to computational cost.
As we will describelater, our model is capable of incorporating a connectionist approach.SIS and SHEET-OF-PAPER are activated.
This causes asearch for CI nodes under both CC nodes.
Since theCI node THESIS005 will be found, the reading wherepaper means thesis will not acquire a cost.
However,assuming that there is not a CI node corresponding toa sheet of paper, we will need to create a new one forthis reading, thus incurring a cost.We can also use reference to discourse ntities toresolve structural ambiguities.
In the sentence Wesent her papers, ff the preceding discourse mentionedYoshiko's papers, a specific CI node such as YOSHIKO-P/ff'ER003 representing Yoshiko's papers would havebeen created.
Therefore, during the processing of Wesent her papers, the reading which means we sent pa-pers to her needs to create a CI node representing pa-pers that we sent, incurring some cost for creating thatinstance node.
On the other hand, the reading whichmeans we sent Yoshiko's papers does not need to cre-ate an instance (because it was already created) so it iscostless.
Also, the reading that uses paper as a sheetof paper is costly as we have demonstrated above.4.3 ConstraintsConstraints are attached to each CSC.
These con-straints play important roles during disambiguation.Constraints define relations between instances whensentences or sentence fragments are accepted.
Whena constraint is satisfied, the parse is regarded as plau-sible.
On the other hand, the parse is less plausiblewhen the constraint is unsatisfied.
Whereas traditionalparsers imply reject a parse which does not satisfy agiven constraint, DMTRANS PLUS, builds or removeslinks between odes forcing them to satisfy constraints.A parse with such forced constraints will record anincreased cost and will be less preferred than parseswithout attached costs.The following example illustrates how this schemeresolves an ambiguity.
As an initial setting we as-sume that the memory network has instances of 'man'(MAN1) and 'hand-gun' (HAND-GUN1) connectedwith a PossEs relation (i.e.
link).
The input utteranceis" "Mary picked up an Uzzi.
Mary shot the man withthe hand-gun."
The second sentence is ambiguous inisolation and it is also ambiguious if it is not knownthat an Uzzi is a machine gun.
However, when it ispreceeded by the first sentence and ff the hearer knowsthat Uzzi is a machine gun, the ambiguity is drasticallyreduced.
DMTRANS PLUS hypothesizes and modelsthis disambiguation activity utilizing knowledge aboutworld through the cost recording mechanism describedabove.During the processing of the first sentence, DM-TRANS PLUS creates instances of 'Mary' and 'Uzzi'- 76 -and records them as active instances in memory (i.e.,MARY1 and UZZI1 are created).
In addition, alink between MARY1 and UZZI1 is created with thePOSSES relation label.
This link creation is invoked bytriggering side-effects (i.e., inferences) stored in theCSC representing the action of 'MARY1 picking upthe UZZII'.
We omit the details of marker passing(for A-, P-, and C-Markers) since it is described etailelsewhere (particulary in \[19\]).When the second sentence comes in, an instanceMARY1 already exists and, therefore, no cost ischarged for parsing 'Mary '5.
However, we now havethree relevant concept sequences (CSC's6):CSCI: (<agent> <shoot> <object>)CSC2: (<agent> <shoot> <object> <with> <instrument>)CSC3: (<person> <with> <instrument>)These sequences are activated when concepts inthe sequences are activated in order from below inthe abstraction hierarchy.
When the "man" comes in,recognition of CSC3:(<person> <with> <instrument>)starts.
When the whole sentence is received, we havetwo top-level CSCs (i.e., CSC1 and CSC2) accepted(all elements of the sequences recognized).
The ac-ceptance of CSC1 is performed through first acceptingCSC3 and then substituting CSC3 for <object>.When the concept sequences are satisfied, their con-straints are tested.
A constraint for CSC2 is (POSSES<agent> <instrument>) and a constraint for CSC3 (andCSCl, which uses CSC3) is (POSSES <person> <in-strument>).
Since 'MARY1 POSSESS HAND-GUNI'now has to be satisfied and there is no instance of thisin memory, we must create a POSSESS link betweenMARY1 and HAND-GUN1.
A certain cost, say 10,is associated with the creation of this link.
On theother hand, MAN1 POSSESS HAND-GUN1 is knownin memory because of an earlier sentence.
As a result,CSC3 is instantiated with no cost and an A-Markerfrom CSC3 is propagated upward to CSC1 with nocost.
Thus, the cost of instantiating CSC1 is 0 andthe cost of instantiating CSC2 is 10.
This way, theinterpretation with CSC 1 is favored by our system.sOl course, 'Mary' can be 'She'.
The method for handling thistype of pronoun reference was already reported in \[19\] and we donot discuss it here.6As we can see from this example ofCSC's, a concept sequencecan be normally regarded asa subcategorization list of a VP head.However, concept sequences are not restricted tosuch lists and areactually often at higher levels of abstraction representing MOP-likesequences.5 Discussion:5.1  G loba l  M in imaThe correct hypothesis in our model is the hypothe-sis with the least cost.
This corresponds to the notionof global minima in most connectionist li erature.
Onother hand, the hypothesis which has the least costwithin a local scope but does not have the least costwhen it is combined with global context is a localminimum.
The goal of our model is to find a globalminimum hypothesis in a given context.
This idea isadvantageous for discourse processing because a parsewhich may not be preferred in a local context mayyeild a least cost hypothesis n the global context.
Sim-ilarly, the least costing parse may turn out to be costlyat the end of processing due to some contexual infer-ence triggered by some higher context.One advantage of our system is that it is possible todefine global and local minima using massively paral-lel marking passing, which is computationally efficientand is more powerful in high-level processing involv-ing variable-binding, structure building, and constraintpropagations 7 than neural network models.
In addi-tion, our model is suitable for massively parallel archi-tectures which are now being researched by hardwaredesigners as next generation machines s.5.2  Psycho l ingu is t i c  Re levance  of  theMode lThe phenomenon f lexical ambiguity has been studiedby many psycholinguistic researchers including \[13\],\[3\], and \[17\].
These studies have identified contextualpriming as an important factor in ambiguity resolution.One psycholinguistic study that is particularlyrelevent o DMTRANS PLUS is Crain and Steedman\[4\], which argues for the principle of referential suc-cess.
Their experiments demonstrate that people preferthe interpretation which is most plausible and accessespreviously defined discourse ntities.
This psycholin-guistic claim and experimental result was incorporatedin our model by adding costs for instance creation andconstraint satisfaction.Another study relevent to our model is be the lex-ical preference theory by Ford, Bresnan and Kaplan\[5\].
Lexical preference theory assumes a preferenceorder among lexical entries of verbs which differ insubcategorization forprepositional phrases.
This typeof preference was incorporated as the bias term in ourcost equation.7Refer to \[22\] for details in this direction.SSee \[23\] and \[9\] for discussion.- 77  -Although we have presented a basic mechanism toincorporate these psyeholinguistic theories, well con-trolled psycholinguistic experiments will be necessaryto set values of each constant and to validate our modelpsycholinguistically.5.3 Reverse  CostIn our example in the previous section, if the firstsentence was Mary picked an S&W where the hearerknows that an S&W is a hand-gun, then an instanceof 'MARY POSSES HAND-GUNI' is asserted as truein the first sentence and no cost is incurred in the in-terpretation of the second sentence using CSC2.
Thismeans that the cost for both PP-attachements in Maryshot the man with the handgun are the same (no costin either cases) and the sentence remains ambiguous.This seems contrary to the fact that in Mary picked aS& W. She shot the man with the hand-gun, that naturalinterpretation (given that the hearer knows S&W is ahand-gun) seems to be that it was Mary that had thehand-gun ot the man.
Since our costs are only neg-atively charged, the fact that 'MARY1 POSSES S&W'is recorded in previous entence does not help the dis-ambiguation of the second sentence.In order to resolve ambiguities uch as this onewhich remain after our cost-assignment procedure hasapplies, we are currently working on a reverse costcharge scheme.
This scheme will retroactively in-crease or decrease the cost of parses based on otherevidence from the discourse context.
For example, thediscourse context might contain information that wouldmake it more plausible or less plausible for Mary to usea handgun.
We also plan to implement time-sensitivediminishing levels of charges to prefer facts recognizedin later utterances.5.4 Incorporat ion  o f  Connect ion is t  Mode lAs already mentioned, our model can incorporateconnectionist models of ambiguity resolution.
In aconnectionist network activation of one node trig-gers interactive excitation and inhibition among nodes.Nodes which get more activated will be primed morethan others.
When a parse uses these more activenodes, no cost will be added to the hypothesis.
Onthe other hand, hypotheses using less activated nodesshould be assigned higher costs.
There is nothingto prevent our model from integrating this idea, es-pecially for lexical ambiguity resolution.
The onlyreason that we do not implement a connectionist ap-proach at present is that the computational cost willbe emonomous on current computers.
Readers houldalso be aware that DMA is a guided marker passing al-gorithm in which markers are passed only along certainlinks whereas connectionist models allow spreadingof activation and inhibition virtually to any connectednodes.
We hope to integrate DMA and connectionistmodels on a real massively parallel computer and wishto demonstrate r al-time translation.
One other possi-bility is to integrate with a connectionist network forspeech recognition 9.
We expect, by integrating withconnectionist networks, to develop a uniform modelof cost-based processing.6 ConclusionWe have described the ambiguity resolution schemein DMTRANS PLUS.
Perhaps the central contributionof this paper to the field is that we have shown amethod of ambiguity resolution in a massively paral-lel marker passing paradigm.
Cost evaluation for eachparse through (1) reference and instance creation, (2)constraint satisfaction and (3) C-Markers are combinedinto the marker passing model.
We have also dicussedon the possibility to merge our model with connec-tionist models where they are applicable.
The guidingprinciple of our model, that parsing is a physical pro-tess of memory modification, was useful in derivingmechanisms described in this paper.
We expect furtherinvestigation along these lines to provide us insightsin many aspects of natural language processing.AcknowldgementsThe authors would like to thank members of the Centerfor Machine Translation for fruitful discussions.
Wewould especially like to thank Masaru Tomita, HitoshiIida, Jaime Carbonell, and Jay McClelland for theirencouragement.Appendix: ImplementationDMTRANS PLUS is implemented on IBM-RT's usingboth CMU-COMMONLISP and MULTILISP running onthe Mach distributed operating system at CMU.
Algo-rithms for structural disambiguation using cost attache-ment were added along with some other house-keepingfunctions to the original DMTRANS to implement DM-TRANS PLUS.
All capacities reported in this paper havebeen implemented except he schemes mentioned inthe sections 5.3 and 5.4 (i.e., negative costs, integra-tion of connectionist models).9Augmentation of the cost-basod model to the phonological levelhas already been impl~rnentod in \[10\].- 78 -References\[1\] Becket, J.D.
The phrasal lexicon.
In 'Theoretical Issues inNatural Language Processing', 1975.\[2\] Boguraev, B. K., et.
el., Three Papers on Parsing, TechnicalReport 17, Computer Laboratory, University of Cambridge,1982.\[3\] Cottrell, G., A Model of Lexical Access of Ambiguous Words, in'Lexical Ambiguity Resolution', S.Small, et.
eLI.
(eds), MorganKaufmann Publishers, 1988.\[4\] Crain, S. and Steex~an, M., On not being led up with guardenpath: the use of context by the psychological syntax processor,in 'Natural Language Parsing', 1985.\[5\] Ford, M., Bresnan, J. and Kaplan, R., A Competence-BasedTheory of Syntactic Closure, in 'The Mental Representation fGrammatical Relations', 1981.\[6\] Grosz, B. and Sidner, C. L., The Structure of Discourse Struc-ture, CSLI Report No.
CSLI-85-39, 1985.\[7\] Hays, P. J., On semantic neLs, frames and associations, in'Proceedings of IJCAI-77, 1977.\[8\] Hirst' G., Semantic Interpretation and the Resolution of Am-biguity, Cambridge University Press, 1987.\[9\] Kitano, H., Multilingual Information Retrieval Mechanism us-ing VLSI, in 'Proceedings of RIAO-88', 1988.\[10\] Kitano, H., et.
eL, Manuscript An Integrated Discourse Under-standing Model for an Interpreting Telephony under the DirectMemory Access Paradigm, Carnegie Mellon University, 1989.\[11\] Marcus, M. P., A theory of syntactic recognition for naturallanguage, MIT Press, 1980.\[12\] Norvig, P., Unified Theory of Inference for Text Understading,Ph.D.
Dissertation, University of California, Berkeley, 1987.\[13\] Prather, P. and Swinney, D., Lexical Processing andAmbigu.ity Resolution: An Autonomous Processing in an InteractiveBox, in 'Lealcal Ambiguity Resolution', S.Small, eL el.
(F_,ds),Morgan Kanfmann Publishers, 1988.\[14\] Riesbnck, C. and Martin, C., Direct Memory Access Parsing,YALEU/DCS/RR 354, 1985.\[15\] Selman, B. end Hint, G., Parsing as an Energy Minimize.tion Problem, in Genetic Algorithms and Simulated Annealing,Davis, L.
(Ed.
), Morgan Kanfmann Publishers, CA, 1987.\[16\] Schank, R., Dynamic Memory: A theory of learning in com.puters and people.
Cambridge University Press.
1982\[17\] Small, S., eL IlL (~ls.)
Lexical Ambiguity Resolution, MorganKanfmann Publishers, Inc., CA, 1988.\[18\] Small, S., et.
el.
TowardConnectionist Parsing, in Proceedingsof AAAI-82, 1982.\[19\] Tornabechi, H., Direct Memory Access Translation, in 'Pro-ceedings of the IJCAI-88', 1987.\[20\] Tcmabechi, H. and Tomita, M., The Integration of Unifwatlan-based Syntax/Semantics and Memory.based Pragmatics forReal-Time Understanding ofNoisy Continuous Speech Input,in 'Proceedings of the AAAI-88', 1988.\[21\] Tcsuabechi, H. and Tomita, M., Application of the DirectMemory Access paradigm to natural language interfaces toknowledge.based systems, in 'Proceedings of the COLING-88', 1988.\[22\] Tcrnabechi, H. and Tomita, M., Manuscript.
MASSIVELYPARALLEL CONSTRAINT PROPAGATION: Parsing withUnification.based Grammar without Unification.
CarnegieMellon University.\[23\] Tcmabechi, H., Mitamura, T., and Tomita, M., DIRECTMEM-ORY ACCESS TRANSLATION FOR SPEECH INPUT: A Mas-sively Parallel Network of Episodic~Thematic nd Phonolog.ical Memory, in 'Proceedings of the International Confer-ence un Fifth Generation Computer Systems 1988' (FGCS'88),1988.\[24\] Tonretzky, D. S., Connectionism and PP Attachment, in 'Pro-ceedings of the 1988 Connectionist Models Summer School,1988.\[25\] Waltz, D. L. and Pollack, J.
B., Massively Parallel Parsing: AStrongly Interactive Model of Natural Language Interpretation.Cognitive Science 9(I): 51-74, 1985.\[26\] Wmmer, E., The ATN and the Sausage Machine: Which oneis baloney?
Cognition, 8(2), June, 1980.\[27\] Webber, B. L., So what can we talk about now?, in 'Com-putational Models of Discourse', (Eds.
M. Brady and R.C.Berwick), MIT Press, 1983.\[28\] Wilks, Y.
A., Huang, X. and Fass, D., Syntax, preference andright attachment, in 'Proceedings of the UCAI-85, 1985.- 79  -
