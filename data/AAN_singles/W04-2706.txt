Deep Syntactic Annotation: Tectogrammatical Representation and BeyondPetr SgallCenter forComputational Linguisticssgall@ufal.mff.cuni.czJarmila Panevova?Institute of Formaland Applied Linguisticspanevova@ufal.mff.cuni.czEva Hajic?ova?Center forComputational Linguisticshajicova@ufal.mff.cuni.czAbstractThe requirements of the depth and precisionof annotation vary for different intended usesof the corpus but it has been commonly ac-cepted nowadays that the standard annotationsof surface structure are only the first steps ina more ambitious research program, aiming ata creation of advanced resources for most dif-ferent systems of natural language processingand for testing and further enrichment of lin-guistic and computational theories.
Amongthe several possible directions in which we be-lieve the standard annotation systems shouldgo (and in some cases already attempt to go)beyond the POS tagging or shallow syntacticannotations, the following four are character-ized in the present contribution: (i) predicate-argument representation of the underlying syn-tactic relations as basically corresponding to arooted tree that can be univocally linearized,(ii) the inclusion of the information structureusing very simple means (the left-to-right or-der of the nodes and three attribute values),(iii) relating this underlying structure (render-ing the ?linguistic meaning,?
i.e.
the semanti-cally relevant counterparts of the grammaticalmeans of expression) to certain central aspectsof referential semantics (reference assignmentand coreferential relations), and (iv) handlingof word sense disambiguation.
The first threeissues are documented in the present paper onthe basis of our experience with the devel-opment of the structure and scenario of thePrague Dependency Treebank which providesfor syntactico-semantic annotation of large textsegments from the Czech National Corpus andwhich is based on a solid theoretical frame-work.1 Introduction1It has been commonly accepted within the computationallinguistics community involved in corpus annotation thatpart-of-speech tagging and shallow syntactic annotationthough very progressive, important and useful tasks attheir time, are only the first steps in a more ambitiousresearch program, aiming at a creation of advanced re-sources for most different systems of natural languageprocessing and for testing and further enrichment of lin-guistic and computational theories.
On the basis of ourexperience with the development and implementation ofthe annotation scheme of the Prague Dependency Tree-bank (PDT, (Hajic?
et al, 2001a), (Bo?hmova?, 2004)), wewould like to indicate four directions, in which we be-lieve the standard annotation systems should be (and insome cases already attempt to be) extended to fulfill thepresent expectations.
We believe that we can offer usefulinsights in three of these, namely(i) an adequate and perspicuous way to represent theunderlying syntactic relations as basically corre-sponding to a rooted tree that can be equivocally lin-earized,(ii) with the inclusion of the information structure us-ing very simple means (the left-to-right order of thenodes and two indexes), and(iii) in relating this underlying structure (rendering the?linguistic meaning,?
i.e.
the semantically relevantcounterparts of the grammatical means of expres-sion) to certain central issues of referential seman-tics (reference assignment and coreferential rela-tions).1The research reported on in this paper has been supportedby the project of the Czech Ministry of Education LN00A063,and by the grants of the Grant Agency of the Czech RepublicNo.
405/03/0913 and No.
405/03/0377.These insights have been elaborated into annotationguidelines which now are being used (and checked) onthe basis of PDT, i.e.
of syntactico-semantic annotationsof large text segments from the Czech National Corpus,which allows for a reliable confirmation of the adequacyof the chosen theoretical framework and for its enrich-ment in individual details.
The fourth dimension we havein mind is that of handling word-sense disambiguation,for which the material of our annotated texts, the PDT,serves as a starting point.2 The Extensions2.1 The Structure: Deep Dependency and ValencyThe development of formal theories of grammar has doc-umented that when going beyond the shallow grammat-ical structure toward some kind of functional or seman-tic structure, two notions become of fundamental impor-tance: the notion of the head of the structure and the no-tion of valency (i.e.
of the requirements that the headsimpose on the structure they are heads of).
To adducejust some reflections of this tendency from Americanlinguistic scene, Fillmore?s ?case grammar?
(with verbframes) and his FrameNet ((Fillmore et al, 2003)), Bres-nan?s and Kaplans?s lexical functional grammar (withthe distinction between the constituent and the functionalstructure and with an interesting classification of func-tions) and Starosta?s ?lexicase grammar?
can serve asthe earliest examples.
To put it in terms of formal syn-tactic frameworks, the phrase structure models take onat least some traits of the dependency models of lan-guage; Robinson has shown that even though Fillmoreleaves the issue of formal representation open, the phrase-structure based sentence structure he proposes can be eas-ily and adequately transposed in terms of dependencygrammar.
Dependency account of sentence structure isdeeply rooted in European linguistic tradition and it is nowonder then that formal descriptions originating in Eu-rope are dependency-based (see Sgall, Kunze, Hellwig,Hudson, Mel?chuk).
We understand it as crucial to usesentence representations ?deep?
enough to be adequate asan input to a procedure of semantic(-pragmatic) interpre-tation (i.e.
representing function words and endings byindexes of node labels, restoring items which are deletedin the morphemic or phonemic forms of sentences anddistinguishing tens of kinds of syntactic relations), ratherthan to be satisfied with some kind of ?surface?
syntax.The above-mentioned development of formal frame-works toward an inclusion of valency in some way or an-other has found its reflection in the annotation scenariosthat aimed at going beyond the shallow structure of sen-tences.
An important support for annotation conceived inthis way can be found in schemes that are based on aninvestigation of the subcategorization of lexical units thatfunction as heads of complex structures, see.
Fillmore?sFRAMENET, the PropBank as a further stage of the de-velopment of the Penn Treebank (Palmer et al, 2001)and Levin?s verb classes (Levin, 1993) on which the LCSDatabase (Dorr, 2001) is based.
There are other systemsworking with some kind of ?deep syntactic?
annotation,e.g.
the broadly conceived Italian project carried out inPisa (N. Calzolari, A. Zampolli) or the Taiwanese projectMARVS; another related framework is presented by theGerman project NEGRA, basically surface oriented, withwhich the newly produced subcorpus TIGER containsmore information on lexical semantics.
Most work thathas already been carried out concerns subcategorizationframes (valency) of verbs but this restriction is not nec-essary: not only verbs but also nouns or adjectives andadverbs may have their ?frames?
or ?grids?.One of the first complex projects aimed at a deep (un-derlying) syntactic annotation of a large corpus is the al-ready mentioned Prague Dependency Treebank (Hajic?,1998); it is designed as a complex annotation of Czechtexts (taken from the Czech National Corpus); the under-lying syntactic dependency relations (called functors) arecaptured in the tectogrammatical tree structures (TGTS);see (Hajic?ova?, 2000).
The set of functors comprises53 valency types subclassified into (inner) participants(arguments) and (free) modifications (adjuncts).
Someof the free modifications are further subcategorized intomore subtle classes (constituting mainly the underlyingcounterparts, or meanings, of prepositions).Each verb entry in the lexicon is assigned a valencyframe specifying which type of participant or modifi-cation can be associated with the given verb; the va-lency frame also specifies which participant/modificationis obligatory and which is optional with the given verbentry (in the underlying representations of sentences),which of them is deletable on the surface, which may ormust function as a controller, and so on.
Also nouns andadjectives have their valency frames.The shape of TGTSs as well as the repertory and clas-sification of the types of modifications of the verbs isbased on the theoretical framework of the FunctionalGenerative Description, developed by the Prague re-search team of theoretical and computational linguis-tics as an alternative to Chomskyan transformationalgrammar (Sgall et al, 1986).
The first two arguments,though labeled by ?semantically descriptive?
tags ACTand PAT (Actor and Patient, respectively) correspondto the first and the second argument of a verb (cf.Tesnie`re?s (Tesnie`re, 1959) first and second actant), theother three arguments of the verb being then differen-tiated (in accordance with semantic considerations) asADDR(essee), ORIG(in) or EFF(ect); these five func-tors belong to the set of participants (arguments) andare distinguished from (free) modifications (adjuncts)such as LOC(ative), several types of directional and tem-poral (e.g.
TWHEN) modifications, APP(urtenance),R(e)STR(ictive attribute), DIFF(erence), PREC(edingcotext referred to), etc.
on the basis of two basic oper-ational criteria (Panevova?, 1974), (Panevova?, 1994):(i) can the given type of modification modify in princi-ple every verb?
(ii) can the given type of modification occur in theclause more than once?If the answers to (i) and (ii) are yes, then the modifica-tion is an adjunct, if not, then we face an argument.We assume that the cognitive roles can be determinedon the basis of combinations of the functors with the lex-ical meanings of individual verbs (or other words), e.g.the Actor of buy is the buyer, that of sell is the seller, theAddressee and the Patient of tell are the experiencer andthe object of the message, respectively.The valency dic-tionary created for and used during the annotation of thePrague Dependency Treebank, called PDT-VALLEX, isdescribed in (Hajic?
et al, 2003).
The relation betweenfunction and (morphological) form as used in the valencylexicon is described in (Hajic?
and Ures?ova?, 2003).An illustration of this framework is presented in Fig.
1.#48SENT&Gen;ADDRjen?ePRECtuzemsk?RSTRv?robceACTdodatPREDhlavaPAT ty iRSTRdenDIFFpozdTWHENFigure 1: A simplified TGTS of the Czech sentence Jenz?etuzemsky?
vy?robce dostal hlavy o c?tyr?i dny pozde?ji.
?How-ever, the domestic producer got the heads four days later.
?Let us adduce further two examples in which the func-tors are written in capitals in the otherwise strongly sim-plified representations, where most of the adjuncts are un-derstood as depending on nouns, whereas the other func-tors concern the syntactic relations to the verb.
Let usnote that with the verb arrive the above mentioned testdetermines the Directional as a (semantically) obligatoryitem that can be specified by the hearer according to thegiven context (basically, as here or there):(1) Jane changed her house from a shabby cottage intoa comfortable home.(1?)
Jane.ACT changed her.APP house.PATfrom-a-shabby.RSTR cottage.ORIGinto-a-comfortable.RSTR home.EFF.
(2) Yesterday Jim arrived by car.(2?)
Yesterday.TWHEN Jim.ACT arrived here.DIR3 by-car.MEANS.A formulation of an annotation scenario based on well-specified subcategorization criteria helps to compare dif-ferent schemes and to draw some conclusions from sucha comparison.
In (Hajic?ova?
and Kuc?erova?, 2002) the au-thors attempt to investigate how different frameworks an-notating some kind of deep (underlying) syntactic level(the LCS Data, PropBank and PDT) compare with eachother (having in mind also a more practical applica-tion, namely a machine translation project the modulesof which would be ?machine-learned?, using a procedurebased on syntactically annotated parallel corpora).
Weare convinced that such a pilot study may also contributeto the discussions on a possibility/impossibility of for-mulating a ?theory neutral?
syntactic annotation scheme.The idea of a theory neutral annotation scenario seemsto be an unrealistic goal: it is hardly possible to imag-ine a classification of such a complex subsystem of lan-guage as the syntactic relations are, without a well mo-tivated theoretical background; moreover, the languagesof the annotated texts are of different types, and the the-oretical frameworks the authors of the schemes are usedto work with differ in the ?depth?
or abstractness of theclassification of the syntactic relations.
However, the dif-ferent annotation schemes seem to be translatable if thedistinctions made in them are stated as explicitly as pos-sible, with the use of operational criteria, and supportedby larger sentential contexts.
The third condition is maderealistic by very large text corpora being available elec-tronically; making the first two conditions a realistic goalis fully in the hands of the designers of the schemes.2.2 Topic/Focus ArticulationAnother aspect of the sentence structure that has to betaken into account when going beyond the shallow struc-ture of sentences is the communicative function of thesentence, reflected in its information structure.
As hasbeen convincingly argued for during decades of linguisticdiscussions (see studies by Rooth, Steedman, and severalothers, and esp.
the argumentation in (Hajic?ova?
et al,1998)), the information structure of the sentence (topic-focus articulation, TFA in the sequel) is semantically rel-evant and as such belongs to the semantic structure of thesentence.
A typical declarative sentence expresses thatits focus holds about its topic, and this articulation hasits consequences for the truth conditions, especially forthe differences between meaning proper, presuppositionaand allegations (see (Hajic?ova?, 1993); (Hajic?ova?
et al,1998)).TFA often is understood to constitute a level of its own,but this is not necessary, and it would not be simple to de-termine the relationships between this level and the otherlayers of language structure.
In the Functional GenerativeDescription (Sgall et al, 1986), TFA is captured as one ofthe basic aspects of the underlying structure, namely asthe left-to-right dimension of the dependency tree, work-ing with the basic opposition of contextual boundness;the contextually bound (CB) nodes stand to the left of thenon-bound (NB) nodes, with the verb as the root of thetree being either contextually bound or non-bound.It should be noted that the opposition of NB/CB is thelinguistically patterned counterpart of the cognitive (andpre-systemic) opposition of ?given?
and ?new?
informa-tion.
Thus, e.g.
in (3) the pronoun him (being NB), infact constitutes the focus of the sentence.
(3) (We met a young pair.)
My older companion recog-nized only HIM.In the prototypical case, NB items belong to the focusof the sentence, and CB ones constitute its topic; sec-ondary cases concern items which are embedded moredeeply than to depend on the main verb of the sentence,cf.
the position of older in (3), which may be understoodas NB, although it belongs to the topic (being an adjunctof the CB noun companion).In the tectogrammatical structures of the PDT anno-tation scenario, we work with three values of the TFAattribute, namely t (contextually bound node), c (contex-tually bound contrastive node) and f (contextually non-bound node).
20,000 sentences of the PDT have al-ready been annotated in this way, and the consistency andagreement of the annotators is being evaluated.
It seemsto be a doable task to annotate and check the whole set ofTGTSs (i.e.
55,000 sentences) by the end of 2004.
Thismeans that by that time the whole set of 55,000 sentenceswill be annotated (and checked for consistency) on bothaspects of deep syntactic structure.
An algorithm the in-put of which are the TGTSs with their TFA values andthe output of which is the division of the whole sentencestructure into the (global) topic and the (global) focus isbeing formulated.2.3 CoreferenceThe inclusion into the annotation scheme of the two as-pects mentioned above in Sect.
2.1 and 2.2, namely thedeep syntactic relations and topic-focus articulation, con-siderably extends the scenario in a desirable way, towarda more complex representation of the meaning of the sen-tence.
The third aspect, the account of coreferential rela-tions, goes beyond linguistic meaning proper toward whatcan be called the sense of the utterance (Sgall, 1994).Two kinds of coreferential relations have to be distin-guished: grammatical coreference (i.e.
with verbs of con-trol, with reflexive pronouns, with verbal complementsand with relative pronouns) and textual (which may crosssentence boundaries), both endophoric and exophoric.Several annotation schemes have been reported at re-cent conferences (ACL, LREC) that attempt at a rep-resentation of coreference relations in continuous texts.As an example of an attempt to integrate the treatmentof anaphora into a complex deep syntactic scenario, wewould like to present here a brief sketch of the schemerealized in the Prague Dependency Treebank.
For thetime being, we are concerned with coreference relationsin their narrower sense, i.e.
not covering the so-calledbridging anaphora (for a possibility to cover also the lat-ter phenomenon, see (Bo?hmova?, 2004)).In the Prague Dependency Treebank, coreference isunderstood as an asymmetrical binary relation betweennodes of a TGTS (not necessarily the same TGTS), or,as the case may be, as a relation between a node andan entity that has no corresponding counterpart in theTGTS(s).
The node from which the coreferential linkleads, is called an anaphor, and the node, to which thelink leads, is called an antecedent.The present scenario of the PDT provides three coref-erential attributes: coref, cortype and corlemma.
The at-tribute coref contains the identifier of the antecedent; ifthere are more than one antecedents of one anaphor, theattribute coref includes a sequence of identifiers of therelevant antecedents; since every node of a TGTS has anidentifier of its own it is a simple programming task toselect the specific information on the antecedent.
The at-tribute cortype includes the information on the type ofcoreference (the possible values are gram for grammat-ical and text for textual coreference), or a sequence ofthe types of coreference, where each element of cortypecorresponds to an element of coref.
The attribute cor-lemma is used for cases of a coreference between a nodeand an entity that has no corresponding counterpart in theTGTS(s): for the time being, there are two possible val-ues of this attribute, namely segm in the case of a coref-erential link to a whole segment of the preceding text (notjust a sentence), and exoph in the case of an exophoricrelation.
Cases of reference difficult to be identified evenif the situation is taken into account are marked by theassignment of unsp as the lemma of the anaphor.
Thisdoes not mean that a decision is to be made between twoor more referents but that the reference cannot be fullyspecified even within a broader context.In order to facilitate the task of the annotators andto make the resulting structures more transparent andtelling, the coreference relations are captured by arrowsleading from the anaphor to the antecedent and the typesof coreference are distinguished by different colors of thearrows.
There are certain notational devices used in caseswhen the antecedent is not within the co-text (exophoriccoreference) or when the link should lead to a whole seg-ment rather than to a particular node.
If the anaphorcorefers to more than a single node or to a subtree, thelink leads to the closest preceding coreferring node (sub-tree).
If there is a possibility to choose between a link toan antecedent or to a postcedent, the link always leads tothe antecedent.#51SENT&Gen;ACTn jak?RSTRzemACTusn?st_seCOND?stavn?RSTRz?konPATpakTWHENtenPATt ?koMANNm nitPREDFigure 2: A TGTS of the sentence Pokud se ne?jaka?
zeme?usnese na u?stavn?
?m za?konu, pak se to te?z?ko me?n??.
?If acountry accepts a constitution law, then this is difficult tochange.
?The manual annotation is made user-friendly by a spe-cial module within the TRED editor (Hajic?
et al, 2001b)which is being used for all three subareas of annotation.In the case of coreference, an automatic pre-selection ofnodes relevant for annotation is used, making the processfaster.Until now, about 30,000 sentences have been annotatedas for the above types of coreference relations.
One of theadvantages of a corpus-based study of a language phe-nomenon is that the researchers become aware of sub-tleties and nuances that are not apparent.
For those whoattempt at a corpus annotation, of course, it is necessaryto collect a list of open questions which have a temporarysolution but which should be studied more intensivelyand to a greater detail in the future.Another issue the study of which is significant andcan be facilitated by an availability of a semantically an-notated corpus, is the question of a (finite) mechanismthe listener (reader) can use to identify the referents.
Ifthe backbone of such a mechanism is seen in the hierar-chy (partial ordering) of salience, then it can be under-stood that this hierarchy typically is modified by the flowof discourse in a way that was specified and illustratedby (Hajic?ova?, 1993), (Hajic?ova?
et al, in prep).
In theflow of a discourse, prototypically, a new discourse ref-erent emerges as corresponding to a lexical occurrencethat carries f; further occurrences carry t or c, their ref-erents being primarily determined by their degrees ofsalience, although the difference between the lowest de-grees of salience reduction, is not decisive.
It appears tobe possible to capture at least certain aspects of this hi-erarchy by some (still tentative) heuristic rules, which tieup the increase/decrease of salience with the position ofthe given item in the topic or in the focus of the given ut-terance.
It should also be remarked that there are certainpermanently salient referents, which may be referred toby items in topic (as ?given?
information) without havinga referentially identical antecedent in the discourse.
Wedenote them as carrying t or c, but perhaps it would bemore adequate to consider them as being always able tobe accommodated(i) by the utterance itself, as especially the indexicals(I, you, here, now, yesterday,.
.
.
),(ii) by the given culture (democracy, Paris, Shake-speare, don Quijote,.
.
.
), by universal human expe-rience (sun, sky), or(iii) by the general domain concerned (history, biol-ogy,...).Since every node in the PDT carries one of the TFAvalues (t, c or f) from which the appurtenance of thegiven item to the topic or focus of the whole sentencecan be determined, it will be possible to use the PDTdata and the above heuristics to start experiments withan automatic assignment of coreferential relations andcheck them against the data with the manual annotationof coreference.2.4 Lexical SemanticsThe design of the tectogrammatical representation is suchthat the nodes in the tectogrammatical tree structure rep-resent (almost) only the autosemantic words found in thewritten or spoken utterance they represent.
We believethat it is thus natural to start distinguishing word sensesonly at this level (and not on a lower level, such as surfacesyntax or linearized text).Moreover, there is a close relation between valencyand word senses.
We hypothesize that with a suitableset of dependency relations (both inner participants andfree modifications, see Sect.
2.1), there is only one va-lency frame per word sense (even though synonyms ornear synonyms might have different valency frames).
Theopposite is not true: there can be several word senses withan identical valency frame.Although in the detailed valency lexicon VALLEX(Lopatkova?, 2003), (Lopatkova?
et al, 2003) an attempthas originally been made to link the valency frames to(Czech) EuroWordNet (Pala and ?Sevec?ek, 1999) sensesto prove this point, this has been abandoned for the timebeing because of the idiosyncrasies in WordNet design,which does not allow to do so properly.We thus proceed independently with word sense an-notation based on the Czech version of WordNet.
Cur-rently, we have annotated 10,000 sentences with wordsenses, both nouns and verbs.
We are assessing now fur-ther directions in annotation; due to low inter-annotatoragreement, we will probably tend to annotate only over apreselected subset of the WordNet synsets.
An approachto building semantic lexicons that is more related to ourconcept of meaning representation is being prepared inthe meantime (Holub and Stran?a?k, 2003).3 ConclusionsUp to now, the framework has been checked on a largeamount of running text segments from the Czech Na-tional Corpus (as for the valency classification 55,000 ut-terances, as for the topic-focus structure 20,000 ones).
Inseveral cases, it was found that a more detailed classifi-cation is needed (e.g.
with the differentiation of the Gen-eral Actor vs.
Unspecified, cf.
the difference betweenOne can cook well with this oven and At this pub theycook well).
However, it has been confirmed that goodresults can be achieved with the chosen classification ofabout 40 valency types and of 15 other grammatical at-tribute types (such as (Semantic) Number, Tense, Modal-ities, etc., but also different values of Location, such asthose corresponding to the preferred functions of in, at,on, under, over, etc., or of Benefactive (positive vs. neg-ative), and so on).
It can be supposed that the core oflanguage corresponds to underlying sentence structuresand to their unmarked morphemic and phonemic coun-terparts.
The marked layers have to be described by spe-cific sets of rules, most of which concern irregularitiesof morphemics, including differences between the under-lying order of nodes and the surface (morphemic) wordorder, especially in cases in which the latter does not di-rectly meet the condition of projectivity (with no cross-ing of edges, cf.
the discontinuous constituents of otherframeworks).The prototypical varieties of sentence structure canthus be characterized by projective rooted trees, whichpoints to the possibility to describe the core of languagestructure on the basis of a maximally perspicuous pat-tern that comes close to patterns present in other domains(primitive logic, arithmetics, and so on) which are nor-mally mastered by children.
Structures of this kind arenot only appropriate for computer implementation, butthey also help understand the relative easiness of mas-tering the mother tongue, without a necessity to assumecomplex innate mechanisms specific for the language fac-ulty.ReferencesAlena Bo?hmova?.
2004.
Automatized Procedures inthe Process of Annotation of PDT.
Ph.D. the-sis, Charles University, Faculty of Mathemartics andPhysics, Prague.Bonnie Dorr.
2001.
The LCS Database.http://www.umiacs.umd.edu/ ?bonnie/LCS Database Documentation.html.Charles J. Fillmore, Christopher R. Robinson, andMiriam R. L. Petruck.
2003.
Background toFrameNet.
International Journal of Lexicography,16:235?250.Eva Hajic?ova?
and Ivona Kuc?erova?.
2002.
Argu-ment/valency structure in PropBank, LCS database andPrague Dependency Treebank: A comparative study.In Proceedings of LREC.Jan Hajic?
and Zden?ka Ures?ova?.
2003.
Linguistic Anno-tation: from Links to Cross-Layer Lexicons.
In Pro-ceedings of The Second Workshop on Treebanks andLinguistic Theories, volume 9 of Mathematical Mod-eling in Physics, Engineering and Cognitive Sciences,pages 69?80.
Va?xjo?
University Press, November 14?15, 2003.Jan Hajic?, Eva Hajic?ova?, Petr Pajas, Jarmila Panevova?,Petr Sgall, and Barbora Vidova?-Hladka?.
2001a.Prague Dependency Treebank 1.0 (Final ProductionLabel).
CDROM CAT: LDC2001T10, ISBN 1-58563-212-0.Jan Hajic?, Petr Pajas, and Barbora Hladka?.
2001b.The Prague Dependency Treebank: Annotation Struc-ture and Support.
In IRCS Workshop on LinguisticDatabases, pages 105?114, Philadelphia, PA, Dec. 11?13.Jan Hajic?, Alevtina Be?mova?, Petr Pajas, JarmilaPanevova?, Veronika ?Rezn?
?c?kova?, and Zden?ka Ures?ova?.2003.
PDT-VALLEX: Creating a Large-coverage Va-lency Lexicon for Treebank Annotation.
In JoakimNivre and Erhard Hinrichs, editors, 2nd InternationalWorkshop on Treebanks and Linguistic Theories, vol-ume 9 of Mathematical Modeling in Physics, Engi-neering and Cognitive Sciences, pages 57?68.
Va?xjo?University Press, Va?xjo?, Sweden, Nov. 14?15, 2003.Jan Hajic?.
1998.
Building a Syntactically Anno-tated Corpus: The Prague Dependency Treebank.
InEva Hajic?ova?, editor, Issues of Valency and Meaning.Studies in Honor of Jarmila Panevova?, pages 12?19.Prague Karolinum, Charles University Press.Eva Hajic?ova?, Barbara Partee, and Petr Sgall.
1998.Topic-focus articulation, tripartite structures, and se-mantic content.
Kluwer Academic Publishers, Ams-terdam, Netherlands.Eva Hajic?ova?, Jir???
Havelka, and Petr Sgall.
in prep.Topic and Focus, Anaphoric Relations and Degrees ofSalience.
In Prague Linguistic Circle Papers 5.
Ams-terdam/Philadelphia: John Benjamins.Eva Hajic?ova?.
1993.
Issues of Sentence Structure andDiscourse.
Charles University, Prague, Czech Repub-lic.Eva Hajic?ova?.
2000.
Dependency-Based Underlying-Structure Tagging of a Very Large Czech Corpus.In Special issue of TAL journal, Grammaires deDe?pendence / Dependency Grammars (ed.
Sylvian Ka-hane), pages 57?78.
Hermes.Martin Holub and Pavel Stran?a?k.
2003.
Approaches tobuilding semantic lexicons.
In WDS?03 Proceedingsof Contributed Papers, Part I, pages 173?178, Prague.MATFYZPRESS, Charles University.Beth Levin.
1993.
English Verb Classes and Alterna-tions: A Preliminary Investigation.
The University ofChicago Press.Marke?ta Lopatkova?, Zdene?k ?Zabokrtsky?, Karol?
?naSkwarska, and Va?clava Benes?ova?.
2003.
VALLEX1.0.
http://ckl.mff.cuni.cz/zabokrtsky/vallex/1.0.Marke?ta Lopatkova?.
2003.
Valency in the Prague De-pendency Treebank: Building the Valency Lexicon.Prague Bulletin of Mathematical Linguistics, 79?80:inprint.Karel Pala and Pavel ?Sevec?ek.
1999.
Czechwordnet.
http://www.fi.muni.cz/nlp/grants/ewn cz.ps.en.Martha Palmer, J Rosenzweig, and S Cotton.
2001.Automatic Predicate Argument Analysis of the PennTreeBank.
In J Allan, editor, Processdings of HLT2001, First Int.
Conference on Human Technology Re-search.
Morgan Kaufmann, San Francisco.Jarmila Panevova?.
1974.
On verbal Frames in FunctionalGenerative Description.
Prague Bulletin of Mathemat-ical Linguistics, 22:3?40.Jarmila Panevova?.
1994.
Valency Frames and the Mean-ing of the Sentence.
In Philip Luelsdorff, editor, ThePrague School of Structural and Functional Linguis-tics, pages 223?243.
John Benjamins, Amsterdam-Philadephia.Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?.
1986.The meaning of the sentence and its semantic andpragmatic aspects.
Reidel, Dordrecht.Petr Sgall.
1994.
Meaning, Reference and Discourse Pat-terns.
In Philip Luelsdorff, editor, The Prague Schoolof Structural and Functional Linguistics, pages 277?309.
John Benjamins, Amsterdam-Philadephia.Lucien Tesnie`re.
1959.
?Elements de Syntaxe Structurale.Klincksieck, Paris.
