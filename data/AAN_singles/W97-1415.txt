The CARTOON project : Towards Integration of Multimodal and LinguisticAnalysis for Cartographic ApplicationsMartin, J.C., Bfiffault, X., Gon~alves, M.R., Vapillon, J.LIMSI-CNRS, BP 133, 91403 Orsay Cedex, France, Email: (martin/xavier/goncalve/vap)@limsi.frReference in multimodal input Human-Computer Interaction has already been studied in several experimentsinvolving either simulated or implemented systems (Mignot and Carbonell 96, Huls and Bos 95) includingcartographic application (Siroux et al 95, Cheyer and Julia 95, Oviatt 96).
In this paper, we present our projectnamed CARTOON (CARTography and cOOperatioN between modalities): we describe the current prototype butalso how we plan to integrate tools providing linguistic analysis.1.
THE CURRENT PROTOTYPEThe current prototype enables to combinespeech recognition, mouse pointing andkeyboard to interact with a cartographicaldatabase (figure 1).
Several functions areavailable such as requesting information on thename or location of a building, the shortestitinerary and the distance between two points,or zooming in and out.
Several combinationsare possible such as :?
What is the name of this <pointing>building ??
What is this <pointing> ??
Where is the police station ??
Show me the hospital?
I want to go from here <pointing> to thehospital Figure 1: Events detected on the three modalities (speech,?
I am in front of the police station.
How can mouse, keyboard) are displayed in the lower window as aI go here <pointing> ?
function of time.
The recognized words were: '7 want to go",?
Show me how to go from here <pointing> "here", "here".
Two mouse clicks were also detected.
Theto here <pointing>.
system displayed the corresponding itinerary.Currently, there is no linguistic analysis.Events produced by the speech recognitionsystem (a Vecsys Datavox) are either words orsequences of words ("I_want_to_go").
Thereare 38 such speech events which arecharacterized by: the recognized word, thetime of utterance and the recognition score.The pointing gestures events are characterizedby an (x, y) position and the time of detection.The overall architecture is described in figure2 : events detected on the keyboard, mouse andspeech modalities (left-hand side) are time-stamped coherently by a modality server andthen integrated in the multimodal whichmerges them and activates the application.
Figure 2: current software and hardware architecture.The multimodal interface is based on a theoretical framework of ~ types of cooperation between modalities ~ thatwe initially presented in (Martin and B&oule 93) and that has been used by other French researchers in (Catinisand Caelen 95, Coutaz and Nigay 94).
Our framework proposes six basic types of cooperation betweenmodalities (either input or output):?
transfer : the result of one modality is used by another modality?
specialization : a modality is devoted to the transmission of the same type of information?
equivalence : choice between several modalities for the transmission of a given piece of information?
redundancy : the same piece of information is transmitted in several modalities?
complementarity : pieces of information regarding the same command are transmitted on several modalities?
concurrency : parallel transmission of independent information in several modalitiesThe CARTOON project 119The combination of modalities are described in a specification language that is based on the theoreticalframework.
Three criteria of fusion are available for redundancy and complementarity: emporal coincidence,sequence and structural completion.
The multimodal module uses Guided Propagation Networks (B&oule 1985)which provide what we call << multimodal recognition scores ~ incorporating the score provided by the speechrecognizer.
In the case of missing events, several commands may be activated with different recognition scores.The command with the highest score is selected by the system and may prompt he user for information if needed.More details on this multimodal framework and module can be found in (Martin et al 95).2.
Linguistic analysisThe multimodal module of the current prototype does not featureeither any syntactic or semantic analysis.
In the next prototype, thiswill be handled by a linguistic software engineering environmentdeveloped by the Language and Cognition group of LIMSI.
Thesyntactic analysis, based on a chart parser uses a LFG grammar forFrench, and the semantic analysis is based on conceptual graphs.
Achart is a graph whose nodes are positioned between the words of thesentence to be parsed (figure 3).
They contain two types of arcs:active arcs (which represent beginnings of a syntactic structure) andcompleted arcs (which represent a whole syntactic structure).
A greatinterest of the Chart is that it gives not only a trace of the rulesapplied while parsing a sentence, but also the possibility of analyzingin details the structures build at various levels which can be usefulfor interacting with the multimodal module.
The current grammarcontains about 225 rules of French simple sentences.
In addition tothese simple sentences, difficult problems are also handled: clitics,complex determiners, completives, various forms of questions,extraction and non limited dependancies, coordinations,comparatives.
Some extensions are currently under development:negation, support verbs, circonstant subordinate phrases, ellipses.More details on these linguistic tools can be found in (Vapillon et al97).
Figure 3: A chart resulting of the analysisof ~ l ara going in front of the Town Hall ~.3.
Towards integration of multimodal and linguistic analysisWorking with two specialized modules allows us to take advantages of each of them.
The linguistic moduleprovides a detailed linguistic analysis.
Both modules may work in parallel but have to exchange results.
Forinstance, the multimodal module may send to the linguistic module partial results (activation of multimodal unitsrepresenting hypothesis on the types of cooperation between modalities).
These pieces of information may allowthe linguistic module to drop early (or on the other way confirm some hypothesis).
Symmetrically, the results ofthe linguistic module (parts of charts and conceptual graphs) may be used by the multimodal module as events ofa higher level of abstraction.
Future directions also include extending estures to circling and trajectory gestureson a tactile screen, implementing dialog history and studying how the linguistic tools cop with spontaneousspoken language.
Finally, experimental studies need to be put into place to evaluate the cooperation betweenspeech and gestures that are used by the subjects when interacting with a map.B6roule, D. (1985).
A model of Adaptative Dynamic Associative Memory for speech processing.
Thesis.
In French.Bourdot, P. et al (1995) Management ofnon-standard devices for multimodal user interfaces under UNIX/XI 1.
In \[CMC\].Catinis, L.& Caelen, J.
(1995) Analyzing the multimodal behavior of users in a drawing task.
IHM'95.
In French.Cheyer, A.
& Julia, L. (1995) Multimodal maps: an agent-based approach.
In \[CMC 1995\].CMC (1995).
Proceedings of the International Conference on Cooperative Multimodal Communication (CMC'95).
Bunt, H,Beun, R.J. & Borghuis, T.
(FEds.).
Eindhoven, may 24-26.Coutaz, J.
& Nigay, L. (1994) The CARE properties in multimodal interfaces.
IHM'94.
In French.Huls, C. & Bos, E. (1995) Studies into full integration of language and action.
In \[CMC 1995\].Martin, J.C. & Bdroule, D. (1993) Types and goals of cooperation between modalities.
IHM'93.
In French.Martin, J.C. et al (1995) Towards adequate r presentation technologies for multimodal interfaces.
In \[CMC\], II : 207-222Mignot, C. & Carbonell, N. (1996).
Oral and gestural command : an empirical study.
TSI.
Vol 15, no 10, In French.Oviatt, S. (1996).
Multimodal interfaces for dynamic interactive maps.
Proceedings of CHI'96, april 13-18. pp 95-102.Siroux, J. et al (1995) Modeling and processing of the oral and tactile activities in the Georal tactile system.
In \[CMC\].Vapillon, J. et al (1997) An Object-Oriented Linguistic Engineenng Environment using LFG and CG.
ACL/EACL'97workshop "Computational Environments for Grammar Development and Linguistic Engineering", 1997, Madrid.
