Proceedings of the 2nd Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 26?34,Baltimore, Maryland, USA, June 22-27, 2014.c?2014 Association for Computational LinguisticsUnsupervised Techniques for Extracting and Clustering Complex Eventsin NewsDelia Rusu?Jo?zef Stefan Institute andJo?zef Stefan InternationalPostgraduate SchoolLjubljana, Sloveniadelia.rusu@ijs.siJames Hodson, Anthony KimballBloomberg LabsNew York, NY, USA{jhodson2,akimball2}@bloomberg.netAbstractStructured machine-readable representa-tions of news articles can radically changethe way we interact with information.
Onestep towards obtaining these representa-tions is event extraction - the identificationof event triggers and arguments in text.With previous approaches mainly focus-ing on classifying events into a small set ofpredefined types, we analyze unsupervisedtechniques for complex event extraction.In addition to extracting event mentions innews articles, we aim at obtaining a moregeneral representation by disambiguatingto concepts defined in knowledge bases.These concepts are further used as featuresin a clustering application.
Two evalua-tion settings highlight the advantages andshortcomings of the proposed approach.1 IntroductionEvent extraction is a key prerequisite for gener-ating structured, machine-readable representationsof natural language.
Such representations can aidvarious tasks like a) question answering, by en-abling systems to provide results for more com-plex queries, b) machine translation, by enhanc-ing different translation models or c) novelty de-tection, as a basis for computing geometric dis-tances or distributional similarities.
Event extrac-tion primarily requires identifying what has oc-curred and who or what was involved, as well asthe time interval of the occurrence.
Additionalinformation related to the event mention may in-clude its location.
Moreover, the event mentioncan also be labeled as belonging to a certain eventtype.
Generally speaking, the goal of event ex-traction is to identify the event trigger, i.e.
the?The work was carried out while the first author was anintern with Bloomberg Labs.words that most clearly define the event, and theevent arguments.
For example, the event mention{Hurricane Katrina struck the coast of New Or-leans in August 2005} belonging to the occurrenceof natural disasters type of events includes the lo-cation of the disaster - New Orleans and the timeof occurrence - August 2005.
The event trigger isthe verb struck while the other words represent thearguments of this event.
The generalized form ofthe event mention is {natural disaster occurred atlocation on date}.
Another similar event mentionis {Hurricane Katrina hit New Orleans}, havingthe generalized form {natural disaster occurredat location}.
Both event mentions can be gener-alized to {natural disaster occurred at location},with the first event mention providing additionaldetails regarding the date of the occurrence.Supervised approaches imply classifying ex-tracted event mentions according to predefinedevent types (Hong et al., 2011; Li et al., 2013).Lexical databases such as FrameNet (Baker etal., 1998), VerbNet (Schuler, 2005) or Prop-Bank (Kingsbury and Palmer, 2002) can serve astraining data.
However, the coverage of this datais still limited, especially for domain-specific ap-plications, and acquiring more labeled data can beexpensive.
Unsupervised approaches, on the otherhand, are usually used to extract large numbersof untyped events (Fader et al., 2011; Nakasholeet al., 2012; Alfonseca et al., 2013; Lewis andSteedman, 2013).
Despite the coverage of thesetechniques, some of the extracted events can suf-fer from reduced quality in terms of both precisionand recall.
Distant supervision aims at mitigatingthe disadvantages of both supervised and unsuper-vised techniques by leveraging events defined inknowledge bases (Mintz et al., 2009).In this work we investigate unsupervised tech-niques for extracting and clustering complexevents from news articles.
For clustering eventswe are using their generalized representation ob-26Event pattern Explanation Event mention{entity1, verb, entity2} an event having two named en-tities as arguments; verb modi-fiers are also included{Obama, apologized for prob-lems with, ACA rollout}{sub, verb}{sub, verb, obj}a sequence of inter-relatedevents having as arguments asubject and an object{Obama, apologized}{Obama, offered, fix}{sub, entity1, verb, obj,entity2}an event having a subject, anobject and two named entitiesas arguments{Hurricane Katrina, struck,coast, of New Orleans}Table 1: Examples of extracted events from text, where the event triggers are underlined and namedentities are marked in bold.tained by disambiguating events to concepts de-fined in knowledge bases.
We are primarily look-ing at Bloomberg news articles which have a par-ticular writing style: complicated sentence struc-tures and numerous dependencies between words.In such cases a first challenge is to correctly iden-tify the event trigger and all event arguments.Moreover, an event is described in news in dif-ferent ways.
Therefore, a second challenge isto capture the relations between event mentions.Thirdly, Bloomberg news mainly focuses on fi-nancial news reporting.
Lexical databases such asFrameNet are intended for the general domain anddo not cover most of the events described in finan-cial news.2 General ApproachWe propose the following pipeline for extractingand clustering complex events from news articles.Firstly, we identify events based on the outputof a dependency parser.
Parsers can capture de-pendencies between words belonging to differentclauses, enabling the detection of sequences ofinter-related events.
Section 3 describes two com-plementary approaches to event extraction whichleverage dependencies between verbs and short-est paths between entities.
Secondly, we obtainmore general representations of the events by an-notating them with concepts defined in (multilin-gual) knowledge bases (see Section 4).
We referto such generalized events as complex events.
Theknowledge base structure allows us to experimentwith different levels of generalization.
As a finalstep we apply a data-driven clustering algorithm togroup similar generalized events.
Clustering canbe seen as an alternative to labeling events withpredefined event types.
Details regarding the clus-tering approach can be found in Section 5.3 Event ExtractionMost of the previous unsupervised information ex-traction techniques have been developed for iden-tifying semantic relations (Fader et al., 2011;Nakashole et al., 2012; Lewis and Steedman,2013).
These approaches extract binary relationsfollowing the pattern {arg1, relation, arg2}.
Anexample of such a relation is {EBX Group Co.,founder, Eike Batista}, with the arguments of thefounder relation being EBX Group Co. and EikeBatista.
Similar to relations, events also have ar-guments such as named entities or time expres-sions (Li et al., 2013).
In addition to the argu-ments, events are also characterized by the pres-ence of an event trigger.
In this work we considerverbs as event triggers, and identify events follow-ing the pattern:{verb, arg1, arg2,...,argn},where arg1, arg2,...,argnis the list of event ar-guments.
Aside from named entities and time ex-pressions, we find additional valid argument can-didates to be the subject or object of the clause.Together with the verb we also include its mod-ifiers.
Table 1 lists a few examples of extractedevents.In order to extract the events, we use the out-put of a dependency parser.
Dependency parsinghas been widely used for relation and event ex-traction (Nakashole et al., 2012; Alfonseca et al.,2013; Lewis and Steedman, 2013).
There are vari-ous publicly-available tools providing dependencyparse at the sentence level.
We use the outputof ZPar (Zhang and Clark, 2011), which imple-ments an incremental parsing process with the de-27(a)Last week Obama apologized for .
.
.
and offered a fix telling insurers they do n?t have to cancel plans next yearNMODVMODSUB VMOD NMODOBJNMOD OBJ SUBVMODVMODVCVMODVMODOBJ NMODVMODVMODVMODROOTPERSON(b)Last week Obama apologized for .
.
.
and offered a fix telling insurers they do n?t have to cancel plans next yearVMODVC VMODVMODROOTVBD VBD VBG VBP VB VBFigure 1: (a) Example sentence with highlighted word dependencies and named entities.
(b) Examplesentence marked with dependencies between verbs.coding based on the Beam Search algorithm.
Theparser processes around 100 sentences per secondat above 90% F-score.The sentences that we are analyzing have arather complex structure, with numerous depen-dencies between words.
An example sentence ispresented in Figure 1 (a).
In this example there isa sequence of inter-related events which share thesame subject: {Obama apologized} and {Obamaoffered fix}.
Such events cannot be captured us-ing only simple pattern matching techniques likethe one implemented by REVERB (Fader et al.,2011).
Other relations that are hard to identify arethe lexically distant ones - this is the case with thedependence between the verb apologized and theverb offered.
Consequently, we consider the fol-lowing two complementary approaches to eventextraction, both of them based on the output of thedependency parser:1.
Identifying verbs (including verb modifiers)and their arguments,2.
Identifying shortest paths between entities.3.1 Identifying Verbs and Their ArgumentsIn order to identify inter-related events we extractdependency sub-trees for the verbs in the sentence.The verb sub-trees also allow us to extend the ar-gument list with missing arguments.
This is thecase of the event mention {Obama offered fix},where the subject Obama is missing.The example sentence in Figure 1 (b) containstwo verb sub-trees, the first one including thenodes apologized and offered and the second oneincluding the nodes telling, do, have and cancel.Once the sub-trees are identified, we can augmentthem with their corresponding arguments.
For de-termining the arguments we use the REVERB re-lation pattern:V |V P |VW?P,where V matches any verb, V P matches a verbfollowed by a preposition and VW?P matches averb followed by one or more nouns, adjectives,adverbs or pronouns and ending with a preposi-tion.3.2 Identifying Shortest Paths betweenEntitiesManual qualitative analysis of the events extractedusing the approach described in Subsection 3.1suggests that the verbs and arguments patternsdo not cover all the events that are of interest tous.
This is the case of events where two or morenamed entities are involved.
For example, for thesentence in Figure 1 (a) we identify the event men-tions {Obama apologized} and {Obama offeredfix} using verb and argument patterns, but we can-not identify the event mention {Obama apologizedfor problems with ACA rollout} which includestwo named entities: Obama and ACA (AffordableHealthcare Act).
We therefore expand our set ofextracted events by identifying the shortest pathconnecting all identified entities.
This is similarto the work of Bunescu and Mooney (2005) which28Obama apologized for the problems with the ACA rolloutSUB VMODPMODNMODPMODNMODPERSON AFFORDABLE HEALTH CARE ACTFigure 2: An event mention {Obama apologizedfor problems with ACA rollout} identified usingthe shortest path between entities approach.build shortest path dependency kernels for relationextraction, where the shortest path connects twonamed entities in text.We first use the Stanford Named Entity Recog-nizer (Finkel et al., 2005) to detect named entitiesand temporal expressions in the sentence.
Next,we determine the shortest path in the dependencytree linking these entities.
An example entity pat-tern discovered using this approach is shown inFigure 2.4 Event DisambiguationWe disambiguate the events by annotating eachword with WordNet (Fellbaum, 2005) super-senses and BabelNet (Navigli and Ponzetto, 2012)senses and hypernyms.
WordNet super-senses of-fer the highest level of generalization for events,followed by BabelNet hypernyms and BabelNetsenses.
The choice of annotating with Word-Net concepts is motivated by its wide usage asa knowledge base covering the common Englishvocabulary.
There are 41 WordNet super-senseclasses defined for nouns and verbs.
Table 2 de-picts example WordNet super-senses with a shortdescription.Previous work on annotating text with WordNetsuper-senses mainly used supervised techniques.Ciaramita and Altun (2006) propose a sequentiallabeling approach and train a discriminative Hid-den Markov Model.
Lacking labeled data we in-vestigate simple unsupervised techniques.
Firstly,we take into account the first sense heuristic whichchooses, from all the possible senses for a givenword, the sense which is most frequent in a givencorpus.
The first sense heuristic has been usedas a baseline in many evaluation settings, and itis hard to overcome for unsupervised disambigua-tion algorithms (Navigli, 2009).
Secondly, we usea kernel to compute the similarity between the sen-tence and the super-sense definition.
If x and y areSuper-sense Descriptioncommuni-cation.nouncommunicative processesand contentsquantity.noun quantities and units of mea-surepossession.noun possession and transfer ofpossessionpossession.verb buying, selling, owningmotion.verb walking, flying, swimmingstative.verb being, having, spatial rela-tionsTable 2: Example noun and verb super-sense la-bels and descriptions taken from WordNet.row vectors representing normalized counts of thewords in the sentence and the words in the super-sense definition, respectively, the kernel is definedas:k(x, y) =xyT?x?
?y?BabelNet is a multilingual knowledge base,mainly integrating concepts from WordNet andWikipedia.
The current version 2.0 contains 50languages.
We use the BabelNet 1.0.1 knowledgebase and API to disambiguate words.
As a start-ing point we consider the PageRank-based disam-biguation algorithm provided by the API, but fu-ture work should investigate other graph-based al-gorithms.5 Event ClusteringEvents are clustered based on the features theyhave in common.
We aim at obtaining clusters forthe two types of extracted events: verbs and theirarguments and shortest paths between entities inthe dependency tree.
The following two event pat-terns are considered for this experiment, for bothevent patterns: {sub, verb, obj} and {sub, verb,obj, entities}, where the verb and arguments canappear in the sentence in any order.
Each event isdescribed using a set of features.
These featuresare extracted for the arguments of each event: thesub, obj and entities.
The following feature com-binations are used for each argument in the eventargument list:?
WordNet super-senses,?
BabelNet senses,29?
BabelNet hypernyms,?
WordNet super-senses, BabelNet senses andhypernyms.For the WordNet experiments we include bothdisambiguation techniques - using the first senseheuristic and the kernel for determining the sim-ilarity between the sentence and the super-sensedefinition.
Similar to the WordNet disambigua-tion approach we generate vectors for each event,where a vector x includes normalized counts of theargument features for the specific event.
Thus wecan determine the similarity between two eventsusing the kernel defined in Section 4.The Chinese Whispers algorithm (Biemann,2006) presented in Algorithm 1 is used to clusterthe events.
We opted for this graph-clustering al-gorithm due to the fact that it is scalable and non-parametric.
The highest rank class in the neigh-borhood of a given event eiis the class of the eventmost similar to ei.Data: set of events EResult: class labels for events in Efor ei?
E do class(ei) = i;while not converged dorandomize order of events in E;for ei?
E doclass(ei) = highest ranked class inthe neighborhood of ei;endendAlgorithm 1: Chinese Whispers Algorithm.6 EvaluationWe evaluated the extracted events, as well as theclusters obtained for the disambiguated events.For each set of experiments we prepared a datasetby sampling Bloomberg news articles.As there is no benchmark dataset for the newsarticles that we are analyzing, we propose to eval-uate event extraction in terms of completeness.Clustering evaluation is done based on the modelitself, and for different feature combinations.
Inwhat follows we describe the evaluation setting inmore detail.6.1 Event Extraction EvaluationThe evaluation dataset consists of a sample of 23stories belonging to the MEDICARE topic, con-taining a total of 1088 sentences.
The event ex-traction algorithms yields 229 entity paths and 515verb and argument events.
Each event is assessedin terms of completeness; an event is deemed tobe complete if all event elements (the event triggerand the arguments) are correctly identified.
Weonly analyze two event patterns: {sub, verb, obj}and {sub, verb, obj, entities}, as events belong-ing to other patterns are rather noisy.
Two anno-tators independently rate each event with 1 if allevent elements are correctly identified, and 0 oth-erwise.
Note that incomplete events receive a 0score.
Cohen?s kappa coefficient (Cohen, 1960)of inter-annotator agreement for this experimentwas 0.70.
The entity path approach correctly iden-tified 78.6% of the entities while the verb argu-ments approach identified 69.1% of the events.Events obtained using entity paths tend to have ahigher number of arguments compared to the verbarguments approach; this explains the higher scoreobtained by this technique.6.2 Clustering EvaluationAs we do not know the cluster labels a priori, weopt for evaluating the clusters using the model it-self.
To this end, we use the Silhouette Coeffi-cient (Kaufman and Rousseeuw, 1990); we plan toinvestigate other clustering evaluation metrics infuture work.
The Silhouette Coefficient is definedfor each sample, and it incorporates two scores:s =b?
amax(a, b),where a is the mean distance between a sampleand all other points within the same class whereasb is the mean distance to all other points in thenext nearest class.
To determine the coefficient fora set of samples one needs to find the mean of thecoefficient for each sample.
A higher coefficientscore is associated with a model having better de-fined clusters.
The best clustering model will ob-tain a Silhouette coefficient of 1, while the worstone will obtain a -1 score.
Values close to 0 implyoverlapping clusters.
Negative values signify thatthe model assigned samples to the wrong cluster,as a different cluster is more similar.The evaluation dataset comprises 325 MEDI-CARE news articles and 16,450 sentences.
Inthis dataset we identify 7,491 verb and argumentevents and 2,046 shortest path events.
Table 3shows example events belonging to two event30Figure 3: Clustering evaluation results for verbs and arguments (left) and shortest paths between entities(right) events, using different feature combinations.clusters.
The first cluster is obtained by extract-ing verb argument events while the second clusteris composed of shortest entity path events.In Figure 3 we show clustering evaluation re-sults for the (a) verbs and arguments and (b) short-est paths between entities, using different featurecombinations.
As expected, the best results areobtained in the case of the WordNet super-senses,which are the most generic senses assigned to theevents.
There is less overlap among the BabelNetsenses and hypernyms, although results improveas more data is available.
The results also mark thedifference between the two types of events: verbsand arguments versus shortest paths between en-tities.
Events extracted using the entity path ap-proach tend to have a higher number of arguments,which in turn implies a richer set of features.
Thisexplains the higher scores obtained in the case ofshortest path events compared to verb argumentevents.7 Related WorkThe event extraction task have received a lot of at-tention in recent years, and numerous approaches,both supervised and unsupervised, have been pro-posed.
This section attempts to summarize themain findings.Supervised approaches.
These approachesclassify events based on a number of predefinedevent types.
A popular dataset is the NIST Au-tomatic Content Extraction (ACE) corpora (Dod-dington et al., 2004) which consists of labeledrelations and events in text.
State-of-the-art ap-proaches mainly use sequential pipelines to sep-arately identify the event trigger and the argu-ments (Hong et al., 2011).
More recently Liet al.
(2013) propose a joint framework whichconsiders event triggers and arguments together.Their model is based on structured perceptron withBeam Search.
In another line of work (Alfonsecaet al., 2013) events extracted in an unsupervisedmanner from the output of a dependency parserare the building blocks of a Noisy-OR model forheadline generation.
Tannier and Moriceau (2013)identify event threads in news, i.e.
a succession ofevents in a story, using a cascade of classifiers.Mintz et al.
(2009) propose a distant supervi-sion approach.
They use Freebase relations andfind sentences which contain entities appearing inthese relations.
From the sentences the authors ex-tract a number of textual features which are usedfor relation classification.
Dependency parsingfeatures are used to identify relations that are lex-ically distant.Unsupervised approaches.
Most unsuper-vised approaches have been tailored to identify-ing relations in text.
Fader et al.
(2011) extractrelations and their arguments based on part-of-speech patterns.
However, such patterns fail todetect lexically distant relations between words.Therefore, most state-of-the-art unsupervised ap-proaches also rely on sentence parsing.
For ex-ample, Lewis and Steedman (2013) extract cross-lingual semantic relations from the English andFrench parses of sentences.
Relational patterns ex-tracted from the sentence parse tree have also beengeneralized to syntactic-ontologic-lexical patternsusing a frequent itemset mining approach (Nakas-hole et al., 2012).
Poon and Domingos (2009)31Event Features{owners are being incen-tivized to drop their healthinsurance coverage}noun.personnoun.possession{analysts are not permit-ted receive compensationdirectly}noun.personnoun.possession{HHS General issued re-port in July 2013}noun.personnoun.groupnoun.time{lawmakers asked Kath-leen Sebelius to respond byDecember 6}noun.personnoun.timeTable 3: Example events belonging to two eventclusters.
Each event is assigned WordNet super-sense features.learn a semantic parser using Markov logic byconverting dependency trees into quasi-logicalforms which are clustered.DIRT (Lin and Pantel, 2001) is an unsuper-vised method for discovering inference rules fromtext.
The authors leverage the dependency parseof a sentence in order to extract indirect seman-tic relations of the form ?X relation Y ?
betweentwo words X and Y .
Inference rules such as ?Xrelation1Y ?
X relation2Y ?
are determinedbased on the similarity of the relations.ALICE (Banko and Etzioni, 2007) is a sys-tem that iteratively discovers concepts, relationsand their generalizations from the Web.
The sys-tem uses a data-driven approach to expand the coreconcepts defined by the WordNet lexical databasewith instances from its Web corpus.
These in-stances are identified by applying predefined ex-traction patterns.
The relations extracted usingTextRunner (Banko et al., 2007) are generalizedusing a clustering-based approach.Our aim is to identify events rather than any re-lation between two concepts.
We therefore pro-pose different extraction patterns based on the de-pendency parse of a sentence which allow us to de-tect event triggers and event arguments that can belexically distant.
Events are generalized by map-ping them to concepts from two different knowl-edge bases (WordNet and BabelNet), allowing usto experiment with multiple levels of generaliza-tion.8 Conclusions and Future WorkIn this work we investigated different unsuper-vised techniques for extracting and clusteringcomplex events from news articles.
As a firststep we proposed two complementary event ex-traction algorithms, based on identifying verbs andtheir arguments and shortest paths between enti-ties, respectively.
Next, we obtained more gen-eral representations of the event mentions by an-notating the event trigger and arguments with con-cepts from knowledge bases.
The generalized ar-guments were used as features for a clustering ap-proach, thus determining related events.As future work on the event extraction side,we plan to improve event quality by learning amodel for filtering out noisy events.
In the caseof event disambiguation we are looking into dif-ferent graph-based disambiguation algorithms toenhance concept annotations.AcknowledgmentsWe would like to thank Pierre Brunelle and Kon-stantine Arkoudas as well as the anonymous re-viewers for their helpful comments.
This workwas funded by Bloomberg LP and the ICT Pro-gramme of the EC under XLike (ICT-STREP-288342).References[Alfonseca et al.2013] Enrique Alfonseca, DanielePighin, and Guillermo Garrido.
2013.
Heady:News headline abstraction through event patternclustering.
In Proceedings of the 51st AnnualMeeting of the Association for ComputationalLinguistics, pages 1243?1253.
[Baker et al.1998] Collin F Baker, Charles J Fillmore,and John B Lowe.
1998.
The Berkeley FramenetProject.
In Proceedings of the 36th Annual Meet-ing of the Association for Computational Linguis-tics and 17th International Conference on Compu-tational Linguistics-Volume 1, pages 86?90.
Associ-ation for Computational Linguistics.
[Banko and Etzioni2007] Michele Banko and Oren Et-zioni.
2007.
Strategies for lifelong knowledge ex-traction from the web.
In Proceedings of the 4th in-ternational conference on Knowledge capture, pages95?102.
ACM.
[Banko et al.2007] Michele Banko, Michael J Ca-farella, Stephen Soderland, Matthew Broadhead,and Oren Etzioni.
2007.
Open information extrac-tion for the web.
In IJCAI, volume 7, pages 2670?2676.32[Biemann2006] Chris Biemann.
2006.
Chinese whis-pers: an efficient graph clustering algorithm and itsapplication to natural language processing problems.In Proceedings of the first workshop on graph basedmethods for natural language processing, pages 73?80.
Association for Computational Linguistics.
[Bunescu and Mooney2005] Razvan C Bunescu andRaymond J Mooney.
2005.
A shortest path depen-dency kernel for relation extraction.
In Proceedingsof the conference on Human Language Technologyand Empirical Methods in Natural Language Pro-cessing, pages 724?731.
Association for Computa-tional Linguistics.
[Ciaramita and Altun2006] Massimiliano Ciaramitaand Yasemin Altun.
2006.
Broad-coverage sensedisambiguation and information extraction with asupersense sequence tagger.
In Proceedings of the2006 Conference on Empirical Methods in NaturalLanguage Processing, pages 594?602.
Associationfor Computational Linguistics.
[Cohen1960] Jacob Cohen.
1960.
A coefficient ofagreement for nominal scales.
Educational and Psy-chological Measurement, 20(1):37?46.
[Doddington et al.2004] George R Doddington, AlexisMitchell, Mark A Przybocki, Lance A Ramshaw,Stephanie Strassel, and Ralph M Weischedel.
2004.The automatic content extraction (ace) program-tasks, data, and evaluation.
In LREC.
[Fader et al.2011] Anthony Fader, Stephen Soderland,and Oren Etzioni.
2011.
Identifying relations foropen information extraction.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 1535?1545.
Associationfor Computational Linguistics.
[Fellbaum2005] Christiane Fellbaum.
2005.
Wordnetand wordnets.
In Keith et al.
Brown, editor, Ency-clopedia of Language and Linguistics, pages 665?670.
Oxford: Elsevier, second edition.
[Finkel et al.2005] Jenny Rose Finkel, Trond Grenager,and Christopher Manning.
2005.
Incorporatingnon-local information into information extractionsystems by gibbs sampling.
In Proceedings of the43rd Annual Meeting on Association for Computa-tional Linguistics, pages 363?370.
Association forComputational Linguistics.
[Hong et al.2011] Yu Hong, Jianfeng Zhang, Bin Ma,Jianmin Yao, Guodong Zhou, and Qiaoming Zhu.2011.
Using cross-entity inference to improve eventextraction.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies-Volume 1,pages 1127?1136.
Association for ComputationalLinguistics.
[Kaufman and Rousseeuw1990] Leonard Kaufman andPeter J Rousseeuw.
1990.
Finding groups in data:an introduction to cluster analysis.
John Wiley &Sons.
[Kingsbury and Palmer2002] Paul Kingsbury andMartha Palmer.
2002.
From treebank to propbank.In Proceedings of the International Conference onLanguage Resources and Evaluation LREC.
[Lewis and Steedman2013] Mike Lewis and MarkSteedman.
2013.
Unsupervised induction of cross-lingual semantic relations.
In Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing EMNLP, pages 681?692.
[Li et al.2013] Qi Li, Heng Ji, and Liang Huang.
2013.Joint event extraction via structured prediction withglobal features.
In Proceedings of the 51st AnnualMeeting of the Association for Computational Lin-guistics.
[Lin and Pantel2001] Dekang Lin and Patrick Pantel.2001.
Dirt - discovery of inference rules from text.In Proceedings of the seventh ACM SIGKDD in-ternational conference on Knowledge discovery anddata mining, pages 323?328.
ACM.
[Mintz et al.2009] Mike Mintz, Steven Bills, RionSnow, and Dan Jurafsky.
2009.
Distant supervisionfor relation extraction without labeled data.
In Pro-ceedings of the Joint Conference of the 47th AnnualMeeting of the ACL and the 4th International JointConference on Natural Language Processing of theAFNLP: Volume 2, pages 1003?1011.
Associationfor Computational Linguistics.
[Nakashole et al.2012] Ndapandula Nakashole, Ger-hard Weikum, and Fabian Suchanek.
2012.
Patty: ataxonomy of relational patterns with semantic types.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learn-ing, pages 1135?1145.
Association for Computa-tional Linguistics.
[Navigli and Ponzetto2012] Roberto Navigli and Si-mone Paolo Ponzetto.
2012.
Babelnet: The auto-matic construction, evaluation and application of awide-coverage multilingual semantic network.
Arti-ficial Intelligence, 193:217?250.
[Navigli2009] Roberto Navigli.
2009.
Word sense dis-ambiguation: A survey.
ACM Computing Surveys(CSUR), 41(2):10.
[Poon and Domingos2009] Hoifung Poon and PedroDomingos.
2009.
Unsupervised semantic parsing.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume1-Volume 1, pages 1?10.
Association for Computa-tional Linguistics.
[Schuler2005] Karin Kipper Schuler.
2005.
Verbnet: Abroad-coverage, comprehensive verb lexicon.
[Tannier and Moriceau2013] Xavier Tannier andV?eronique Moriceau.
2013.
Building event threadsout of multiple news articles.
Proceedings of the2013 Conference on Empirical Methods in NaturalLanguage Processing EMNLP.33[Zhang and Clark2011] Yue Zhang and Stephen Clark.2011.
Syntactic processing using the generalizedperceptron and beam search.
Computational Lin-guistics, 37(1):105?151.34
