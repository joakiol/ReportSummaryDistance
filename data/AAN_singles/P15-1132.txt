Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1365?1374,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning Tag Embeddings and Tag-specificComposition Functions in Recursive Neural NetworkQiao Qian, Bo Tian, Minlie Huang, Yang Liu*, Xuan Zhu*, Xiaoyan ZhuState Key Lab.
of Intelligent Technology and Systems, National Lab.
for Information Scienceand Technology, Dept.
of Computer Science and Technology, Tsinghua University, Beijing 100084, PR China*Samsung R&D Institute Beijing, Chinaqianqiaodecember29@126.com , smxtianbo@gmail.comaihuang@tsinghua.edu.cn , yang.liu@samsung.comxuan.zhu@samsung.com , zxy-dcs@tsinghua.edu.cnAbstractRecursive neural network is one of themost successful deep learning modelsfor natural language processing due tothe compositional nature of text.
Themodel recursively composes the vectorof a parent phrase from those of childwords or phrases, with a key compo-nent named composition function.
Al-though a variety of composition func-tions have been proposed, the syntacticinformation has not been fully encodedin the composition process.
We pro-pose two models, Tag Guided RNN (TG-RNN for short) which chooses a compo-sition function according to the part-of-speech tag of a phrase, and Tag EmbeddedRNN/RNTN (TE-RNN/RNTN for short)which learns tag embeddings and thencombines tag and word embeddings to-gether.
In the fine-grained sentimentclassification, experiment results showthe proposed models obtain remarkableimprovement: TG-RNN/TE-RNN obtainremarkable improvement over baselines,TE-RNTN obtains the second best resultamong all the top performing models, andall the proposed models have much lessparameters/complexity than their counter-parts.1 IntroductionAmong a variety of deep learning models for nat-ural language processing, Recursive Neural Net-work (RNN) may be one of the most popular mod-els.
Thanks to the compositional nature of natu-ral text, recursive neural network utilizes the re-cursive structure of the input such as a phrase orsentence, and has shown to be very effective formany natural language processing tasks includingsemantic relationship classification (Socher et al,2012), syntactic parsing (Socher et al, 2013a),sentiment analysis (Socher et al, 2013b), and ma-chine translation (Li et al, 2013).The key component of RNN and its variants isthe composition function: how to compose thevector representation for a longer text from thevector of its child words or phrases.
For instance,as shown in Figure 2, the vector of ?is very inter-esting?
can be composed from the vector of the leftnode ?is?
and that of the right node ?very interest-ing?.
It?s worth to mention again, the compositionprocess is conducted with the syntactic structureof the text, making RNN more interpretable thanother deep learning models.so?max?so?max?so?max?so?max?so?max?...g?g?very interestingis  very interestingis very interestingFigure 1: The example process of vector composi-tion in RNN.
The vector of node ?very interesting?is composed from the vectors of node ?very?
andnode ?interesting?.
Similarly, the node ?is very in-teresting?
is composed from the phrase node ?veryinteresting?
and the word node ?is?
.There are various attempts to design the com-position function in RNN (or related models).
InRNN (Socher et al, 2011), a global matrix is usedto linearly combine the elements of vectors.
InRNTN (Socher et al, 2013b), a global tensor isused to compute the tensor products of dimen-sions to favor the association between different el-1365ements of the vectors.
Sometimes it is challeng-ing to find a single function to model the compo-sition process.
As an alternative, multiple com-position functions can be used.
For instance, inMV-RNN (Socher et al, 2012), different matricesis designed for different words though the modelis suffered from too much parameters.
In AdaMCRNN/RNTN (Dong et al, 2014), a fixed numberof composition functions is linearly combined andthe weight for each function is adaptively learned.In spite of the success of RNN and its variants,the syntactic knowledge of the text is not yet fullyemployed in these models.
Two ideas are moti-vated by the example shown in Figure 2: First,the composition function for the noun phrase ?themovie/NP?
should be different from that for theadjective phrase ?very interesting/ADJP?
since thetwo phrases are quite syntactically different.
Morespecifically to sentiment analysis, a noun phrase ismuch less likely to express sentiment than an ad-jective phrase.
There are two notable works men-tioned here: (Socher et al, 2013a) presented tocombine the parsing and composition processes,but the purpose is for parsing; (Hermann andBlunsom, 2013) designed composition functionsaccording to the combinatory rules and categoriesin CCG grammar, however, only marginal im-provement against Naive Bayes was reported.
Ourproposed model, tag guided RNN (TG-RNN), isdesigned to use the syntactic tag of the parentphrase to guide the composition process from thechild nodes.
As an example, we design a functionfor composing noun phrase (NP) and another onefor adjective phrase (ADJP).
This simple strategyobtains remarkable improvements against strongbaselines.is?/?VBZ?very?/?RB?very?interes?ng?/?ADJP?interes?ng?/?JJ?is?very?interes?ng?/?VP?the?/?DT?the?movie?/?NP?movie?/?NN?the?movie?is?very?interes?ng?/?S?Figure 2: The parse tree for sentence ?The movieis very interesting?
built by Stanford Parser.Second, when composing the adjective phrase?very interesting/ADJP?
from the left node?very/RB?
and the right node ?interesting/JJ?, theright node is obviously more important than theleft one.
Furthermore, the right node ?interest-ing/JJ?
apparently contributes more to sentimentexpression.
To address this issue, we proposeTag embedded RNN/RNTN (TE-RNN/RNTN), tolearn an embedding vector for each word/phrasetag, and concatenate the tag vector with theword/phrase vector as input to the compositionfunction.
For instance, we have tag vectors forDT,NN,RB,JJ,ADJP,NP, etc.
and the tag vectorsare then used in composing the parent?s vector.The proposed TE-RNTN obtain the second best re-sult among all the top performing models but withmuch less parameters and complexity.
To the bestof our knowledge, this is the first time that tag em-bedding is proposed.To summarize, the contributions of our work areas follows:?
We propose tag-guided composition func-tions in recursive neural network, TG-RNN.Tag-guided RNN allocates a compositionfunction for a phrase according to the part-of-speech tag of the phrase.?
We propose to learn embedding vectors forpart-of-speech tags of words/phrases, andintegrate the tag embeddings in RNN andRNTN respectively.
The two models, TE-RNN and TE-RNTN, can leverage the syn-tactic information of child nodes when gen-erating the vector of parent nodes.?
The proposed models are efficient and effec-tive.
The scale of the parameters is well con-trolled.
Experimental results on the StanfordSentiment Treebank corpus show the effec-tiveness of the models.
TE-RNTN obtainsthe second best result among all publicly re-ported approaches, but with much less pa-rameters and complexity.The rest of the paper is structured as follows: inSection 2, we survey related work.
In Section 3,we introduce the traditional recursive neural net-work as background.
We present our ideas in Sec-tion 4.
The experiments are introduced in Section5.
We summarize the work in Section 6.2 Related WorkDifferent kinds of representations are used insentiment analysis.
Traditionally, the bag-of-words representations are used for sentiment anal-ysis (Pang and Lee, 2008).
To exploit the rela-tionship between words, word co-occurrence (Tur-ney et al, 2010) and syntactic contexts (Pad?o1366and Lapata, 2007) are considered.
In order todistinguish antonyms with similar contexts, neu-ral word vectors (Bengio et al, 2003) are pro-posed and can be learnt in an unsupervised man-ner.
Word2vec (Mikolov et al, 2013a) introducesa simpler network structure making computationmore efficiently and makes billions of samplesfeasible for training.Semantic composition deals with representinga longer text from its shorter components, whichis extensively studied recently.
In many previ-ous works, a phrase vector is usually obtained byaverage (Landauer and Dumais, 1997), addition,element-wise multiplication (Mitchell and Lap-ata, 2008) or tensor product (Smolensky, 1990) ofword vectors.
In addition to using vector repre-sentations, matrices can also be used to representphrases and the composition process can be donethrough matrix multiplication (Rudolph and Gies-brecht, 2010; Yessenalina and Cardie, 2011).Recursive neural models utilize the recursivestructure (usually a parse tree) of a phrase or sen-tence for semantic composition.
In RecursiveNeural Network (Socher et al, 2011), the treewith the least reconstruction error is built and thevectors for interior nodes is composed by a globalmatrix.
Matrix-Vector Recursive Neural Network(MV-RNN) (Socher et al, 2012) assigns matri-ces for every words so that it could capture therelationship between two children.
In RecursiveNeural Tensor Networks (RNTN) (Socher et al,2013b), the composition process is performed ona parse tree in which every node is annotatedwith fine-grained sentiment labels, and a globaltensor is used for composition.
Adaptive Multi-Compositionality (Dong et al, 2014) uses multipleweighted composition matrices instead of sharinga single matrix.The employment of syntactic information inRNN is still in its infant.
In (Socher et al, 2013a),the part-of-speech tag of child nodes is consideredin combining the processes of both compositionand parsing.
The main purpose is for better pars-ing by employing RNN, but it is not designed forsentiment analysis.
In (Hermann and Blunsom,2013), the authors designed composition functionsaccording to the combinatory rules and categoriesin CCG grammar.
However, only marginal im-provement against Naive Bayes was reported.
Un-like (Hermann and Blunsom, 2013), our TG-RNNobtains remarkable improvements against strongbaselines, and we are the first to propose tag em-bedded RNTN which obtains the second best re-sult among all reported approaches.3 Background: Recursive Neural ModelsIn recursive neural models, the vector of a longertext (e.g., sentence) is composed from those of itsshorter components (e.g., words or phrases).
Tocompose a sentence vector through word/phrasevectors, a binary parse tree has to be built witha parser.
The leaf nodes represent words and in-terior nodes represent phrases.
Vectors of interiornodes are computed recursively by composition ofchild nodes?
vectors.
Specially, the root vector isregarded as the sentence representation.
The com-position process is shown in Figure 1.More formally, vector vi?
Rdfor node i iscalculated via:vi= f(g(vli, vri)) (1)where vliand vriare child vectors, g is a compo-sition function, and f is a nonlinearity function,usually tanh.
Different recursive neural modelsmainly differ in composition function.
For exam-ple, the composition function for RNN is as below:g(vli, vri) = W[vlivri]+ b (2)where W ?
Rd?2dis a composition matrix and bis a bias vector.
And the composition function forRNTN is as follows:g(vli, vri) =[vlivri]T[1:d][vlivri]+ W[vlivri]+ b (3)where W and b are defined in the previous modeland T[1:d]?
R2d?2d?dis the tensor that definesmultiple bilinear forms.The vectors are used as feature inputs to a soft-max classifier.
The posterior probability over classlabels on a node vector viis given byyi= softmax(Wsvi+ bs).
(4)The parameters in these models include theword table L, a composition matrix W in RNN,and W and T[1:d]in RNTN, and the classificationmatrix Wsfor the softmax classifier.13674 Incorporating Syntactic Knowledgeinto Recursive Neural ModelThe central idea of the paper is inspired by the factthat words/phrases of different part-of-speech tagsplay different roles in semantic composition.
Asdiscussed in the introduction, a noun phrase (e.g.,a movie/NP) may be composed different from averb phrase (e.g., love movie/VP).
Furthermore,when composing the phrase a movie/NP, the twochild words, a/DT and movie/NN, may play dif-ferent roles in the composition process.
Unfor-tunately, the previous RNN models neglect suchsyntactic information, though the models do em-ploy the parsing structure of a sentence.We have two approaches to improve the compo-sition process by leveraging tags on parent nodesand child nodes.
One approach is to use differentcomposition matrices for parent nodes with differ-ent tags so that the composition process could beguided by phrase type, for example, the matrix for?NP?
is different from that for ?VP?
.
The other ap-proach is to introduce ?tag embedding?
for wordsand phrases, for example, to learn tag vectors for?NP, VP, ADJP?, etc., and then integrate the tagvectors with the word/phrase vectors during thecomposition process.4.1 Tag Guided RNN (TG-RNN)We propose Tag Guided RNN (TG-RNN) to re-spect the tag of a parent phrase during the com-position process.
The model chooses a composi-tion function according to the part-of-speech tagof a phrase.
For example, ?the movie?
has tag NP,?very interesting?
has tag ADJP, the two phraseshave different composition matrices.More formally, we design composition func-tions g with a factor of the phrase tag of a parentnode.
The composition function becomesg(ti, vli, vri) = gti(vli, vri) = Wti[vlivri]+ bti(5)where tiis the phrase tag for node i, Wtiand btiare the parameters of function gti, as defined inEquation 2.
In other words, phrase nodes withvarious tags have their own composition functionssuch as gNP, gV P, and so on.
There are to-tally k composition function in this model wherek is the number of phrase tags.
When composingchild vectors, a function is chosen from the func-tion pool according to the tag of the parent node.The process is depicted in Figure 3.
We term thismodel Tag guided RNN, TG-RNN for short.so?max?so?max?so?max?so?max?so?max?...
?very / RB interesting / JJis / VBZ very interesting / ADJPis very interesting / VPgNP?
gADJP?
gVP?gNP?
gADJP?
gVP?......Figure 3: The vector of phrase ?very interesting?is composed with highlighted gADJPand ?is veryinteresting?
with gV P.But some tags have few occurrences in the cor-pus.
It is hard and meaningless to train compo-sition functions for those infrequent tags.
So wesimply choose top k frequent tags and train k com-position functions.
A common composition func-tion is shared across phrases with all infrequenttags.
The value of k depends on the size of thetraining set and the occurrences of each tag.
Spe-cially, when k = 0, the model is the same as thetraditional RNN.4.2 Tag Embedded RNN and RNTN(TE-RNN/RNTN)In this section, we propose tag embedded RNN(TE-RNN) and tag embedded RNTN (TE-RNTN)to respect the part-of-speech tags of child nodesduring composition.
As mentioned above, tags ofparent nodes have impact on composition.
How-ever, some phrases with the same tag should becomposed in different ways.
For example, ?is in-teresting?
and ?like swimming?
have the same tagVP.
But it is not reasonable to compose the twophrases using the previous model because the part-of-speech tags of their children are quite different.If we use different composition functions for chil-dren with different tags like TG-RNN, the numberof tag pairs will amount to as many as k?k, whichmakes the models infeasible due to too many pa-rameters.In order to capture the compositional effects ofthe tags of child nodes, an embedding et?
Rdeiscreated for every tag t, where deis the dimensionof tag vector.
The tag vector and phrase vector are1368concatenated during composition as illustrated inFigure 4.Formally, the phrase vector is composed by thefunctiong(vli, etli, vri, etri) = W??????vlietlivrietri?????
?+ b (6)where tliand triare tags of the left and the rightnodes respectively, etliand etriare tag vectors, andW ?
Rd?
(2de+2d)is the composition matrix.
Weterm this model Tag embedded RNN, TE-RNN forshort.so?max?so?max?...very / RB interesting / JJis / VBZ very interesting / ADJPis very interesting / VPg?g?so?max?so?max?so?max?Figure 4: RNN with tag embedding.
There is a tagembedding table, storing vectors for RB, JJ, andADJP, etc.
Then we compose the phrase vector?very interesting?
from the vectors for ?very?
and?interesting?, and the tag vectors for RB and JJ.Similarly, this idea can be applied to RecursiveNeural Tensor Network (Socher et al, 2013b).
InRNTN, the tag vector and the phrase vector canbe interweaved together through a tensor.
Morespecifically, the phrase vectors and tag vectors aremultiplied by the composed tensor.
The composi-tion function changes to the following:g(vli, etli, vri, etri)=??????vlietlivrietri??????T[1:d]??????vlietlivrietri?????
?+ W??????vlietlivrietri?????
?+ b(7)where the variables are similar to those defined inequation 3 and equation 7.
We term this modelTag embedded RNTN, TE-RNTN for short.The phrase vectors and tag vectors are used asinput to a softmax classifier, giving the posteriorprobability over labels viayi= softmax(Ws[vieti]+ bs) (8)4.3 Model TrainingLet yibe the target distribution for node i, y?ibethe predicted sentiment distribution.
Our goal is tominimize the cross-entropy error between yiandy?ifor all nodes.
The loss function is defined asfollows:E(?)
= ?
?i?jyjilog y?ij+ ?||?||2(9)where j is the label index, ?
is a L2-regularizationterm, and ?
is the parameter set.Similar to RNN, the parameters for our mod-els include word vector table L, the compositionmatrix W , and the sentiment classification matrixWs.
Besides, our models have some additional pa-rameters, as discussed below:TG-RNN: There are k composition matrices fortop k frequent tags.
They are defined as Wt?Rk?d?2d.
The original composition matrix W isfor all infrequent tags.
As a result, the parameterset of TG-RNN is ?
= (L,W,Wt,Ws).TE-RNN: The parameters include the tag em-bedding table E, which contains all the em-beddings for part-of-speech tags for words andphrases.
And the size of matrix W ?
Rd?
(2d+2de)and the softmax classifier Ws?
RN?(de+d).
Theparameter set of TE-RNN is ?
= (L,E,W,Ws).TE-RNTN: This model has one more tensorT ?
R(2d+2de)?
(2d+2de)?dthan TE-RNN.
The pa-rameter set of TE-RNTN is ?
= (L,E,W, T,Ws)5 Experiment5.1 Dataset and Experiment SettingWe evaluate our models on Stanford SentimentTreebank which contains fully labeled parse trees.It is built upon 10,662 reviews and each sentencehas sentiment labels on each node in the parsetree.
The sentiment label set is {0,1,2,3,4}, wherethe numbers mean very negative, negative, neu-tral, positive, and very positive, respectively.
Weuse standard split (train: 8,544 dev: 1,101, test:2,210) on the corpus in our experiments.
In addi-tion, we add the part-of-speech tag for each leafnode and phrase-type tag for each interior node1369using the latest version of Stanford Parser.
Be-cause the newer parser generated trees differentfrom those provided in the datasets, 74/11/11 re-views in train/dev/test datasets are ignored.
Af-ter removing the broken reviews, our dataset con-tains 10566 reviews (train: 8,470, dev: 1,090, test:2,199).The word vectors were pre-trained on an unla-beled corpus (about 100,000 movie reviews) byword2vec (Mikolov et al, 2013b) as initial val-ues and the other vectors is initialized by samplingfrom a uniform distribution U(?
?, ?)
where ?
is0.01 in our experiments.
The dimension of wordvectors is 25 for RNN models and 20 for RNTNmodels.
Tanh is chosen as the nonlinearity func-tion.
And after computing the output of node iwith vi= f(g(vli, vri)), we set vi=vi||vi||so thatthe resulting vector has a limited norm.
Back-propagation algorithm (Rumelhart et al, 1986)is used to compute gradients and we use mini-batch SGD with momentum as the optimizationmethod, implemented with Theano (Bastien et al,2012).
We trained all our models using stochas-tic gradient descent with a batch size of 30 exam-ples, momentum of 0.9, L2-regularization weightof 0.0001 and a constant learning rate of 0.005.5.2 System ComparisonWe compare our models with several methodswhich are evaluated on the Sentiment Treebankcorpus.
The baseline results are reported in (Donget al, 2014) and (Kim, 2014).We make comparison to the following base-lines:?
SVM.
A SVM model with bag-of-words rep-resentation (Pang and Lee, 2008).?
MNB/bi-MNB.
Multinomial Naive Bayesand its bigram variant, adopted from (Wangand Manning, 2012).?
RNN.
The first Recursive Neural Networkmodel proposed by (Socher et al, 2011).?
MV-RNN.
Matrix Vector Recursive NeuralNetwork (Socher et al, 2012) representseach word and phrase with a vector and a ma-trix.
As reported, this model suffers from toomany parameters.?
RNTN.
Recursive Neural Tenser Net-work (Socher et al, 2013b) employs a tensorMethod Fine-grained Pos./Neg.SVM 40.7 79.4MNB 41.0 81.8bi-MNB 41.9 83.1RNN 43.2 82.4MV-RNN 44.4 82.9RNTN 45.7 85.4AdaMC-RNN 45.8 87.1AdaMC-RNTN 46.7 88.5DRNN 49.8 87.7TG-RNN (ours) 47.0 86.3TE-RNN (ours) 48.0 86.8TE-RNTN (ours) 48.9 87.7CNN 48.0 88.1DCNN 48.5 86.8Para-Vec 48.7 87.8Table 1: Classification accuray.
Fine-grainedstands for 5-class prediction and Pos./Neg.
meansbinary prediction which ignores all neutral in-stances.
All the accuracy is at the sentence level(root).for composition function which could modelthe meaning of longer phrases and capturenegation rules.?
AdaMC.
Adaptive Multi-Compositionalityfor RNN and RNTN (Dong et al, 2014)trains more than one composition functionsand adaptively learns the weight for eachfunction.?
DCNN/CNN.
Dynamic Convolutional Neu-ral Network (Kalchbrenner et al, 2014) and asimple Convolutional Neural Network (Kim,2014), though these models are of differentgenres to RNN, we include them here for faircomparison since they are among top per-forming approaches on this task.?
Para-Vec.
A word2vec variant (Le andMikolov, 2014) that encodes paragraph in-formation into word embedding learning.
Asimple but very competitive model.?
DRNN.
Deep Recursive Neural Network (Ir-soy and Cardie, 2014) stacks multiple recur-sive layers.The comparative results are shown in Ta-ble 1.
As illustrated, TG-RNN outperformsRNN, RNTN, MV-RNN, AdMC-RNN/RNTN.1370Compared with RNN, the fine-grained accuracyand binary accuracy of TG-RNN is improved by3.8% and 3.9% respectively.
When compared withAdaMC-RNN, the accuracy of our method rises by1.2% on the fine-grained prediction.
The resultsshow that the syntactic knowledge does facilitatephrase vector composition in this task.As for TE-RNN/RNTN, the fine-grained accu-racy of TE-RNN is boosted by 4.8% comparedwith RNN and the accuracy of TE-RNTN by 3.2%compared with RNTN.
TE-RNTN also beat theAdaMC-RNTN by 2.2% on the fine-grained clas-sification task.
TE-RNN is comparable to CNNand DCNN, another line of models for this task.TE-RNTN is better than CNN, DCNN, and Para-Vec, which are the top performing approaches onthis task.
TE-RNTN is worse than DRNN, butthe complexity of DRNN is much higher than TE-RNTN, which will be discussed in the next sec-tion.
Furthermore, TE-RNN is also better thanTG-RNN.
This implies that learning the tag em-beddings for child nodes is more effective thansimply using the tags of parent phrases in com-position.Note that the fine-grained accuracy is moreconvincible and reliable to compare different ap-proaches due to the two facts: First, for the bi-nary classification task, some approaches train an-other binary classifier for positive/negative clas-sification while other approaches, like ours, di-rectly use the fine-grained classifier for this pur-pose.
Second, how the neutral instances are pro-cessed is quite tricky and the details are not re-ported in the literature.
In our work, we sim-ply remove neural instances from the test data be-fore the evaluation.
Let the 5-dimension vectory be the probabilities for each sentiment label ina test instance.
The prediction will be positive ifargmaxi,i ?=2yiis greater than 2, otherwise nega-tive, where i ?
{0, 1, 2, 3, 4} means very negative,negative, neutral, positive, very positive, respec-tively.5.3 Complexity AnalysisTo gain deeper understanding of the models pre-sented in Table 1, we discuss here about the pa-rameter scale of the RNN/RNTN models sincethe prediction power of neural network models ishighly correlated with the number of parameters.The analysis is presented in Table 2 (the opti-mal values are adopted from the cited papers).
Theparameters for the word table have the same sizen ?
d across all recursive neural models, where nis the number of words and d is the dimension ofword vector.
Therefore, we ignore this part but fo-cus on the parameters of composition functions,termed model size.
Our models, TG-RNN/TE-RNN, have much less parameters than RNTN andAdMC-RNN/RNTN, but have much better perfor-mance.
Although TE-RNTN is worse than DRNN,however, the parameters of DRNN are almost 9times of ours.
This indicates that DRNN is muchmore complex, which requires much more dataand time to train.
As a matter of a fact, our TE-RNTN only takes 20 epochs for training which is10 times less than DRNN.Method model size # of parametersRNN 2d21.8KRNTN 4d3108KAdaMC-RNN 2d2?
c 18.7KAdaMC-RNTN 4d3?
c 202KDRNN d ?
h ?
l+2h2?
l 451KTG-RNN (ours) 2d2?
(k + 1) 8.8KTE-RNN (ours) 2(d + de) ?
d 1.7KTE-RNTN (ours) 4(d + de)2?
d 54KTable 2: The model size.
d is the dimensionof word/phrase vectors (the optimal value is 30for RNN & RNTN, 25 for AdaMC-RNN, 15 forAdaMC-RNTN, 300 for DRNN).
For AdaMC, cis the number of composition functions (15 is theoptimal setting).
For DRNN, l and h is the numberof layers and the width for each layer (the optimalvalues l = 4, h = 174).
For our methods, k is thenumber of unshared composition matrices and dethe dimension of tag embedding, for the optimalsetting refer to Section 5.4.5.4 Parameter AnalysisWe have two key parameters to tune in our pro-posed models.
For TG-RNN, the number of com-position functions k is an important parameter,which corresponds to the number of distinct POStags of phrases.Let?s start from the corpus analysis.
As shownin Table 3, the corpus contains 215,154 phrasesbut the distribution of phrase tags is extremely im-balanced.
For example, the phrase tag ?NP?
ap-pears 60,239 times while ?NAC?
appears only 10times.
Hence, it is impossible to learn a composi-1371Phrase tag Frequency Phrase tag FrequencyNP 60,239 ADVP 1,140S 33,138 PRN 976VP 26,956 FARG 792PP 14,979 UCP 362ADJP 7,912 SSINV 266SBAR 5,308 others 1,102Table 3: The distribution of phrase-type tags in thetraining data.
The top 6 frequency tags cover morethan 95% phrases.tion function for the infrequent phrase tags.Each of the top k frequent phrase tags corre-sponds to a unique composition function, while allthe other phrase tags share a same function.
Wecompare different k for TG-RNN.
The accuracy isshown in Figure 5.
Our model obtains the best per-formance when k is 6, which is accordant with thestatistics in Table 3.0 2 4 6 8 10 12k0.420.430.440.450.460.470.48accuracyTG-RNNAdaMC-RNNRNNFigure 5: The accuracy for TG-RNN with differ-ent k.For TE-RNN/RNTN, the key parameter to tuneis the dimension of tag vectors.
In the corpus, wehave 70 types of tags for leaf nodes (words) and in-terior nodes (phrases).
Infrequent tags whose fre-quency is less than 1,000 are ignored.
There are30 tags left and we learn an embedding for eachof these frequent tags.
We varies the dimension ofthe embedding defrom 0 to 30.Figure 6 shows the accuracy for TE-RNN andTE-RNTN with different dimensions of de.
Ourmodel obtains the best performance when deis8 for TE-RNN and 6 for TE-RNTN.
The re-sults show that too small dimensions may not besufficient to encode the syntactic information oftags and too large dimensions damage the perfor-mance.0 5 10 15 20 25 30de0.420.430.440.450.460.470.480.49accuracyTE-RNTNTE-RNNAdaMC-RNTNAdaMC-RNNRNTNRNNFigure 6: The accuracy for TE-RNN and TE-RNTN with different dimensions of de.5.5 Tag Vectors AnalysisIn order to prove tag vectors obtained from tagembedded models are meaningful, we inspect thesimilarity between vectors of tags.
For each tagvector, we find the nearest neighbors based on Eu-clidean distance, summarized in Table 4.Tag Most Similar TagsJJ (Adjective)ADJP(Adjective Phrase)VP (Verb Phrase)VBD (past tense)VBN (past participle).
(Dot) : (Colon)Table 4: Top 1 or 2 nearest neighboring tags withdefinition in brackets.Adjectives and verbs are of significant impor-tance in sentiment analysis.
Although ?JJ?
and?ADJP?
are word and phrase tag respectively, theyhave similar tag vectors, because of playing thesame role of Adjective in sentences.
?VP?, ?VBD?and ?VBN?
with similar representations all repre-sent verbs.
What is more interesting is that thenearest neighbor of dot is colon, probably becauseboth of them are punctuation marks.
Note that tagclassification is none of our training objectives andsurprisingly the vectors of similiar tags are clus-tered together, which can provides additional in-formation during sentence composition.6 ConclusionIn this paper, we present two ways to leverage syn-tactic knowledge in Recursive Neural Networks.1372The first way is to use different composition func-tions for phrases with different tags so that thecomposition processing is guided by phrase types(TG-RNN).
The second way is to learn tag em-beddings and combine tag and word embeddingsduring composition (TE-RNN/RNTN).
The pro-posed models are not only effective (w.r.t com-peting performance) but also efficient (w.r.t well-controlled parameter scale).
Experiment resultsshow that our models are among the top perform-ing approaches up to date, but with much less pa-rameters and complexity.AcknowledgmentsThis work was partly supported by the Na-tional Basic Research Program (973 Program) un-der grant No.2012CB316301/2013CB329403, theNational Science Foundation of China under grantNo.61272227/61332007, and the Beijing HigherEducation Young Elite Teacher Project.
The workwas also supported by Tsinghua University Bei-jing Samsung Telecom R&D Center Joint Labora-tory for Intelligent Media Computing.ReferencesFr?ed?eric Bastien, Pascal Lamblin, Razvan Pascanu,James Bergstra, Ian J. Goodfellow, Arnaud Berg-eron, Nicolas Bouchard, and Yoshua Bengio.
2012.Theano: new features and speed improvements.Deep Learning and Unsupervised Feature LearningNIPS 2012 Workshop.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Janvin.
2003.
A neural probabilistic lan-guage model.
The Journal of Machine Learning Re-search, 3:1137?1155.Li Dong, Furu Wei, Ming Zhou, and Ke Xu.
2014.Adaptive multi-compositionality for recursive neu-ral models with applications to sentiment analysis.In AAAI.
AAAI.Karl Moritz Hermann and Phil Blunsom.
2013.
Therole of syntax in vector space models of composi-tional semantics.
In ACL, pages 894?904.
Associa-tion for Computer Linguistics.Ozan Irsoy and Claire Cardie.
2014.
Deep recursiveneural networks for compositionality in language.In NIPS, pages 2096?2104.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network formodelling sentences.
In ACL, pages 655?665.
As-sociation for Computer Linguistics.Yoon Kim.
2014.
Convolutional neural networks forsentence classification.
In EMNLP, pages 1746?1751.
Association for Computational Linguistics.Thomas K Landauer and Susan T Dumais.
1997.
Asolution to plato?s problem: The latent semanticanalysis theory of acquisition, induction, and rep-resentation of knowledge.
Psychological Review,104(2):211.Quoc V Le and TomasMikolov.
2014.
Distributed rep-resentations of sentences and documents.
In ICML,volume 32, pages 1188?1196.Peng Li, Yang Liu, and Maosong Sun.
2013.
Re-cursive autoencoders for ITG-based translation.
InEMNLP, pages 567?577.
Association for ComputerLinguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
CoRR.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013b.
Distributed representa-tions of words and phrases and their compositional-ity.
In NIPS, pages 3111?3119.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In ACL, pages236?244.Sebastian Pad?o and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.Bo Pang and Lillian Lee.
2008.
Opinion mining andsentiment analysis.
Foundations and Trends in In-formation Retrieval, 2(1-2):1?135.Sebastian Rudolph and Eugenie Giesbrecht.
2010.Compositional matrix-space models of language.In ACL, pages 907?916.
Association for ComputerLinguistics.David E Rumelhart, Geoffrey E Hinton, and Ronald JWilliams.
1986.
Learning representations by back-propagating errors.
Nature, 323:533?536.Paul Smolensky.
1990.
Tensor product variable bind-ing and the representation of symbolic structuresin connectionist systems.
Artificial intelligence,46(1):159?216.Richard Socher, Jeffrey Pennington, Eric H Huang,Andrew Y Ng, and Christopher D Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In EMNLP, pages 151?161.
Association for Computational Linguistics.Richard Socher, Brody Huval, Christopher D Man-ning, and Andrew Y Ng.
2012.
Semantic composi-tionality through recursive matrix-vector spaces.
InEMNLP, pages 1201?1211.
Association for Compu-tational Linguistics.1373Richard Socher, John Bauer, Christopher D Manning,and Andrew Y Ng.
2013a.
Parsing with compo-sitional vector grammars.
In ACL, pages 455?465.Association for Computer Linguistics.Richard Socher, Alex Perelygin, Jean Y Wu, JasonChuang, Christopher D Manning, Andrew Y Ng,and Christopher Potts.
2013b.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In EMNLP, pages 1631?1642.
Associa-tion for Computational Linguistics.Peter D Turney, Patrick Pantel, et al 2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37(1):141?188.Sida I Wang and Christopher D Manning.
2012.
Base-lines and bigrams: Simple, good sentiment and topicclassification.
In ACL, pages 90?94.
Association forComputational Linguistics.Ainur Yessenalina and Claire Cardie.
2011.
Compo-sitional matrix-space models for sentiment analysis.In EMNLP, pages 172?182.
Association for Com-puter Linguistics.1374
