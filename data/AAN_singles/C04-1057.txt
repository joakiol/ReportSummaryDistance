A Formal Model for Information Selection in Multi-Sentence TextExtractionElena FilatovaDepartment of Computer ScienceColumbia UniversityNew York, NY 10027, USAfilatova@cs.columbia.eduVasileios HatzivassiloglouCenter for Computational Learning SystemsColumbia UniversityNew York, NY 10027, USAvh@cs.columbia.eduAbstractSelecting important information while account-ing for repetitions is a hard task for both sum-marization and question answering.
We pro-pose a formal model that represents a collec-tion of documents in a two-dimensional spaceof textual and conceptual units with an asso-ciated mapping between these two dimensions.This representation is then used to describe thetask of selecting textual units for a summary oranswer as a formal optimization task.
We pro-vide approximation algorithms and empiricallyvalidate the performance of the proposed modelwhen used with two very different sets of fea-tures, words and atomic events.1 IntroductionMany natural language processing tasks involve thecollection and assembling of pieces of informa-tion from multiple sources, such as different doc-uments or different parts of a document.
Text sum-marization clearly entails selecting the most salientinformation (whether generically or for a specifictask) and putting it together in a coherent sum-mary.
Question answering research has recentlystarted examining the production of multi-sentenceanswers, where multiple pieces of information areincluded in the final output.When the answer or summary consists of mul-tiple separately extracted (or constructed) phrases,sentences, or paragraphs, additional factors influ-ence the selection process.
Obviously, each of theselected text snippets should individually be impor-tant.
However, when many of the competing pas-sages are included in the final output, the issue ofinformation overlap between the parts of the outputcomes up, and a mechanism for addressing redun-dancy is needed.
Current approaches in both sum-marization and long answer generation are primar-ily oriented towards making good decisions for eachpotential part of the output, rather than examiningwhether these parts overlap.
Most current methodsadopt a statistical framework, without full semanticanalysis of the selected content passages; this makesthe comparison of content across multiple selectedtext passages hard, and necessarily approximated bythe textual similarity of those passages.Thus, most current summarization or long-answer question-answering systems employ twolevels of analysis: a content level, where every tex-tual unit is scored according to the concepts or fea-tures it covers, and a textual level, when, beforebeing added to the final output, the textual unitsdeemed to be important are compared to each otherand only those that are not too similar to other can-didates are included in the final answer or summary.This comparison can be performed purely on the ba-sis of text similarity, or on the basis of shared fea-tures that may be the same as the features used toselect the candidate text units in the first place.In this paper, we propose a formal model for in-tegrating these two tasks, simultaneously perform-ing the selection of important text passages and theminimization of information overlap between them.We formalize the problem by positing a textual unitspace, from which all potential parts of the summaryor answer are drawn, a conceptual unit space, whichrepresents the distinct conceptual pieces of informa-tion that should be maximally included in the finaloutput, and a mapping between conceptual and tex-tual units.
All three components of the model areapplication- and task-dependent, allowing for dif-ferent applications to operate on text pieces of dif-ferent granularity and aim to cover different concep-tual features, as appropriate for the task at hand.
Wecast the problem of selecting the best textual unitsas an optimization problem over a general scoringfunction that measures the total coverage of concep-tual units by any given set of textual units, and pro-vide general algorithms for obtaining a solution.By integrating redundancy checking into the se-lection of the textual units we provide a unifiedframework for addressing content overlap that doesnot require external measures of similarity betweentextual units.
We also account for the partial overlapof information between textual units (e.g., a singleshared clause), a situation which is common in nat-ural language but not handled by current methodsfor reducing redundancy.2 Formal Model for Information Selectionand PackingOur model for selecting and packing informationacross multiple text units relies on three compo-nents that are specified by each application.
First,we assume that there is a finite set T of textual unitst1, t2, .
.
.
, tn, a subset of which will form the an-swer or summary.
For most approaches to sum-marization and question answering, which followthe extraction paradigm, the textual units ti willbe obtained by segmenting the input text(s) at anapplication-specified granularity level, so each tiwould typically be a sentence or paragraph.Second, we posit the existence of a finite set Cof conceptual units c1, c2, .
.
.
, cm.
The conceptualunits encode the information that should be presentin the output, and they can be defined in differentways according to the task at hand and the prior-ities of each system.
Obviously, defining the ap-propriate conceptual units is a core problem, akinto feature selection in machine learning: There isno exact definition of what an important concept isthat would apply to all tasks.
Current summariza-tion systems often represent concepts indirectly viatextual features that give high scores to the textualunits that contain important information and shouldbe used in the summary and low scores to those tex-tual units which are not likely to contain informa-tion worth to be included in the final output.
Thus,many summarization approaches use as conceptualunits lexical features like tf*idf weighing of wordsin the input text(s), words used in the titles and sec-tion headings of the source documents (Luhn, 1959;H.P.Edmundson, 1968), or certain cue phrases likesignificant, important and in conclusion (Kupiec etal., 1995; Teufel and Moens, 1997).
Conceptualunits can also be defined out of more basic concep-tual units, based on the co-occurrence of importantconcepts (Barzilay and Elhadad, 1997) or syntac-tic constraints between representations of concepts(Hatzivassiloglou et al, 2001).
Conceptual units donot have to be directly observable as text snippets;they can represent abstract properties that particulartext units may or may not satisfy, for example, statusas a first sentence in a paragraph or generally posi-tion in the source text (Lin and Hovy, 1997).
Somesummarization systems assume that the importanceof a sentence is derivable from a rhetorical repre-sentation of the source text (Marcu, 1997), whileothers leverage information from multiple texts tore-score the importance of conceptual units acrossall the sources (Hatzivassiloglou et al, 2001).No matter how these important concepts are de-fined, different systems use text-observable featuresthat either correspond to the concepts of interest(e.g., words and their frequencies) or point out thosetext units that potentially contain important con-cepts (e.g., position or discourse properties of thetext unit in the source document).
The former classof features can be directly converted to concep-tual units in our representation, while the latter canbe accounted for by postulating abstract conceptualunits associated with a particular status (e.g., firstsentence) for a particular textual unit.
We assumethat each conceptual unit has an associated impor-tance weight wi that indicates how important unit ciis to the overall summary or answer.2.1 A first model: Full correspondenceHaving formally defined the sets T and C of tex-tual and conceptual units, the part that remains inorder to have the complete picture of the constraintsgiven by the data and summarization approach is themapping between textual units and conceptual units.This mapping, a function f : T?C ?
[0, 1], tells ushow well each conceptual unit is covered by a giventextual unit.
Presumably, different approaches willassign different coverage scores for even the samesentences and conceptual units, and the consistencyand quality of these scores would be one way to de-termine the success of each competing approach.We first examine the case where the function f islimited to zero or one values, i.e., each textual uniteither contains/matches a given conceptual featureor not.
This is the case with many simple features,such as words and sentence position.
Then, we de-fine the total information covered by any given sub-set S of T (a proposed summary or answer) asI(S) =?i=1,...,mwi ?
?i (1)where wi is the weight of the concept ci and?i ={ 1, if ?j ?
{1, .
.
.
,m} such that f(tj , ci) = 10, otherwiseIn other words, the information contained in asummary is the sum of the weights of the concep-tual units covered by at least one of the textual unitsincluded in the summary.2.2 Partial correspondence between textualand conceptual unitsDepending on the nature of the conceptual units, theassumption of a 0-1 mapping between textual andconceptual units may or may not be practical or evenfeasible.
For many relatively simple representationsof concepts, this restriction poses no difficulties: theconcept is uniquely identified and can be recognizedas present or absent in a text passage.
However, it ispossible that the concepts have some structure andcan be decomposed to more elementary conceptualunits, or that partial matches between concepts andtext are natural.
For example, if the conceptual unitsrepresent named entities (a common occurrence inlist-type long answers), a partial match between aname found in a text and another name is possi-ble; handling these two names as distinct conceptswould be inaccurate.
Similarly, an event can be rep-resented as a concept with components correspond-ing to participants, time, location, and action, withonly some of these components found in a particularpiece of text.Partial matches between textual and conceptualunits introduce a new problem, however: if two tex-tual units partially cover the same concept, it isnot apparent to what extent the coverage overlaps.Thus, there are multiple ways to revise equation (1)in order to account for partial matches, dependingon how conservative we are on the expected over-lap.
One such way is to assume minimum overlap(the most conservative assumption) and define thetotal information in the summary asI(S) =?i=1,...,mwi ?maxj f(tj , ci) (2)An alternative is to consider that f(tj , ci) repre-sents the extent of the [0, 1] interval correspondingto concept ci that tj covers, and assume that thecoverage is spread over that interval uniformly andindependently across textual units.
Then the com-bined coverage of two textual units tj and tk isf(tj , ci) + f(tk, ci)?
f(tj , ci) ?
f(tk, ci)This operator can be naturally extended to morethan two textual units and plugged into equation (2)in the place of the max operator, resulting into anequation we will refer to as equation (3).
Note thatboth of these equations reduce to our original for-mula for information content (equation (1)) if themapping function f only produces 0 and 1 values.2.3 Length and textual constraintsWe have provided formulae that measure the infor-mation covered by a collection of textual units un-der different mapping constraints.
Obviously, wewant to maximize this information content.
How-ever, this can only sensibly happen when additionalconstraints on the number or length of the selectedtextual units are introduced; otherwise, the full setof available textual units would be a solution thatproffers a maximal value for equations (1)?
(3), i.e.,?S ?
T, I(S) ?
I(T ).
We achieve this by assign-ing a cost pi to each textual unit ti, i = 1, .
.
.
, n,and defining a function P over a set of textual unitsthat provides the total penalty associated with se-lecting those textual units as the output.
In our ab-straction, replacing a textual unit with one or moretextual units that provide the same content shouldonly affect the penalty, and it makes sense to assignthe same cost to a long sentence as to two sentencesproduced by splitting the original sentence.
Also,a shorter sentence should be preferable to a longersentence with the same information content.
Hence,our operational definitions for pi and P arepi = length(ti), P (S) =?ti?Spii.e., the total penalty is equal to the total length ofthe answer in some basic unit (e.g., words).Note however, than in the general case the pi?sneed not depend solely on the length, and the to-tal penalty does not need to be a linear combina-tion of them.
The cost function can depend onfeatures other then length, for example, number ofpronouns?the more pronouns used in a textual unit,the higher the risk of dangling references and thehigher the price should be.
Finding the best costfunction is an interesting research problem by itself.With the introduction of the cost function P (S)our model has two generally competing compo-nents.
One approach is to set a limit on P (S) andoptimize I(S) while keeping P (S) under that limit.This approach is similar to that taken in evaluationsthat keep the length of the output summary withincertain bounds, such as the recent major summa-rization evaluations in the Document Understand-ing Conferences from 2001 to the present (Harmanand Voorhees, 2001).
Another approach would beto combine the two components and assign a com-posite score to each summary, essentially mandat-ing a specific tradeoff between recall and precision;for example, the total score can be defined as a lin-ear combination of I(S) and P (S), in which casethe weights specify the relative importance of cov-erage and precision/brevity, as well as accountingfor scale differences between the two metrics.
Thisapproach is similar to the calculation of recall, pre-cision, and F-measure adopted in the recent NISTevaluation of long answers for definitional questions(Voorhees, 2003).
In this paper, we will follow thefirst tactic of maximizing I(S) with a limit on P (S)rather than attempting to solve the thorny issues ofweighing the two components appropriately.3 Handling Redundancy inSummarizationRedundancy of information has been found usefulin determining what text pieces should be includedduring summarization, on the basis that informationthat is repeated is likely to be central to the topic orevent being discussed.
Earlier work has also recog-nized that, while it is a good idea to select amongthe passages repeating information, it is also impor-tant to avoid repetition of the same information inthe final output.Two main approaches have been proposed foravoiding redundancy in the output.
One approachrelies on grouping together potential output textunits on the basis of their similarity, and outputtingonly a representative from each group (Hatzivas-siloglou et al, 2001).
Sentences can be clusteredin this manner according to word overlap, or by us-ing additional content similarity features.
This ap-proach has been recently applied to the constructionof paragraph-long answers (e.g., (Blair-Goldensohnet al, 2003; Yu and Hatzivassiloglou, 2003)).An alternative approach, proposed for the synthe-sis of information during query-based passage re-trieval is the maximum marginal relevance (MMR)method (Goldstein et al, 2000).
This approach as-signs to each potential new sentence in the output asimilarity score with the sentences already includedin the summary.
Only those sentences that contain asubstantial amount of new information can get intothe summary.
MMR bases this similarity score onword overlap and additional information about thetime when each document was released, and thuscan fail to identify repeated information when para-phrasing is used to convey the same meaning.In contrast to these approaches, our model han-dles redundancy in the output at the same time itselects the output sentences.
It is clear from equa-tions (1)?
(3) that each conceptual unit is countedonly once whether it appears in one or multiple tex-tual units.
Thus, when we find the subset of textualunits that maximizes overall information coveragewith a constraint on the total number or length oftextual units, the model will prefer the collection oftextual units that have minimal overlap of coveredconceptual units.
Our approach offers three advan-tages versus both clustering and MMR: First, it in-tegrates redundancy elimination into the selectionprocess, requiring no additional features for defin-ing a text-level similarity between selected textualunits.
Second, decisions are based on the same fea-tures that drive the summarization itself, not on ad-ditional surface properties of similarity.
Finally, be-cause all decisions are informed by the overlap ofconceptual units, our approach accounts for partialoverlap of information across textual units.
To illus-trate this last point, consider a case where three fea-tures A, B, and C should be covered in the output,and where three textual units are available, cover-ing A and B, A and C, and B and C, respectively.Then our model will determine that selecting anytwo of the textual units is fully sufficient, while thismay not be apparent on the basis of text similaritybetween the three text units; a clustering algorithmmay form three singleton clusters, and MMR maydetermine that each textual unit is sufficiently dif-ferent from each other, especially if A, B, and Care realized with nearly the same number of words.4 Applying the ModelHaving presented a formal metric for the informa-tion content (and optionally the cost) of any poten-tial summary or answer, the task that remains is tooptimize this metric and select the correspondingset of textual units for the final output.
As statedin Section 2.3, one possible way to do this is to fo-cus on the information content metric and introducean additional constraint, limiting the total cost to aconstant.
An alternative is to optimize directly thecomposite function that combines cost and informa-tion content into a single number.We examine the case of zero-one mappings be-tween textual and conceptual units, where the to-tal information content is specified by equation (1).The complexity of the problem depends on thecost function, and whether we optimize I(S) whilekeeping P (S) fixed or whether we optimize a com-bined function of both of those quantities.
We willonly consider the former case in the present paper.We start by examining an artificially simple case,where the cost assigned to each textual unit is 1, andthe function P for combining costs is their sum.
Inthis case, the total cost is equal to the number oftextual units used in a summary.This problem, as we have formalized it above,is identical to the Maximum Set Coverage problemstudied in theoretical computer science: given C, afinite set of weighted elements, a collection T ofsubsets of C, and an integer k, find those k sets thatmaximize the total number of elements in the unionof T ?s members (Hochbaum, 1997).
In our case,the zero-one mapping allows us to view each textualunit as a subset of the conceptual units space, con-taining those conceptual units covered by the tex-tual unit, and k is the total target cost.
Unfortu-nately, maximum set coverage is NP-hard, as it isreducible to the classic set cover problem (given afinite set and a collection of subsets of that set, findthe smallest subset of that collection whose mem-bers?
union is equal to the original set) (Hochbaum,1997).
It follows that more general formulations ofthe cost function that actually are more realistic forour problem (such as defining the total cost as thesum of the lengths of the selected textual units andallowing the textual units to have different lengths)will also result in an NP-hard problem, as we can re-duce these versions to the special case of maximumset coverage.Nevertheless, the correspondence with maximumset coverage provides a silver lining.
Since theproblem is known to be NP-hard, properties ofsimple greedy algorithms have been explored, anda straightforward local maximization method hasbeen proved to give solutions within a known boundof the optimal solution.
The greedy algorithm formaximum set coverage has as follows: Start with anempty solution S, and iteratively add to the S theset Ti that maximizes I(S ?
Ti).
It is provable thatthis algorithm is the best polynomial approximationalgorithm for the problem (Hochbaum, 1997), andthat it achieves a solution bounded as followsI(OPT) ?
I(GREEDY) ?[1?(1?
1k)k]I(OPT)>(1?
1e)I(OPT) ?
0.6321?
I(OPT)where I(OPT) is the information content of the op-timal summary and I(GREEDY) is the informationcontent of the summary produced by this greedy al-gorithm.For the more realistic case where cost is speci-fied as the total length of the summary, and wherewe try to optimize I(S) with a limit on P (S) (seeSection 2.3), we propose two greedy algorithms in-spired by the algorithm above.
Both our algorithmsoperate by first calculating a ranking of the textualunits in decreasing order.
This ranking is for thefirst algorithm, which we call adaptive greedy algo-rithm, identical to the ranking provided by the ba-sic greedy algorithm, i.e., each textual unit receivesas score the increase in I(S) that it generates whenadded to the output, in the order specified by the ba-sic greedy algorithm.
Our second greedy algorithm(dubbed modified greedy algorithm below) modifiesthis ranking by prioritizing the conceptual units withhighest individual weight wi; it ranks first the tex-tual unit that has the highest contribution to I(S)while covering this conceptual unit with the high-est individual weight, and then iteratively proceedswith the textual unit that has the highest contribu-tion to I(S) while covering the next most importantunaccounted for conceptual unit.Given the rankings of textual units, we can thenproduce an output of a given length by adopting ap-propriate stopping criteria for when to stop addingtextual units (in order according to their ranking)to the output.
There is no clear rule for conform-ing to a specific length (for example, DUC 2001 al-lowed submitted summaries to go over ?a reason-able percentage?
of the target length, while DUC2004 cuts summaries mid-sentence at exactly thetarget length).
As the summary length in DUC ismeasured in words, in our experiments we extractedthe specified number of words out of the top sen-tences (truncating the last sentence if necessary).5 ExperimentsTo empirically establish the effectiveness of the pre-sented model we ran experiments comparing evalu-ation scores on summaries obtained with a baselinealgorithm that does not account for redundancy ofinformation and with the two variants of greedy al-gorithms described in Section 4.
We chose summa-rization as the evaluation task because ?ideal?
out-put (prepared by humans) and methods for scoringarbitrary system output were available for this task,but not for evaluating long answers to questions.Data We chose as our input data the documentsets used in the evaluation of multidocument sum-marization during the Document UnderstandingConference (DUC), organized by NIST in 2001(Harman and Voorhees, 2001).
This collection con-tains 30 test document sets, each containing approx-imately 10 news stories on different events; docu-ment sets vary significantly in their internal cohere-ness.
For each document set 12 human-constructedsummaries are provided, 3 for each of the targetlengths of 50, 100, 200, and 400 words.
We se-lected DUC 2001 because unlike later DUCs, idealsummaries are available for multiple lengths.
Weconsider sentences as our textual units.Features In our experiments we used two sets offeatures (i.e., conceptual units).
First, we chosea fairly basic and widely used set of lexical fea-tures, namely the list of words present in each inputtext.
We set the weight of each feature to its tf*idfvalue, taking idf values from http://elib.cs.berkeley.edu/docfreq/.Our alternative set of conceptual units was the listof weighted atomic events extracted from the inputtexts.
An atomic event is a triplet consisting of twonamed entities extracted from a sentence and a con-nector expressed by a verb or an event-related nounthat appears in-between these two named entities.The score of the atomic event depends on the fre-quency of the named entities pair for the input textand the frequency of the connector for that namedentities pair.
Filatova and Hatzivassiloglou (2003)define the procedure for extracting atomic events indetail, and show that these triplets capture the mostimportant relations connecting the major constituentparts of events, such as location, dates and partici-pants.
Our hypothesis is that using these events asconceptual units would provide a reasonable basisfor summarizing texts that are supposed to describeone or more events.Evaluation Metric Given the difficulties in com-ing up with a universally accepted evaluation mea-sure for summarization, and the fact that judgmentsby humans are time-consuming and labor-intensive,we adopted an automated process for comparingsystem-produced summaries to the ideal summarieswritten by humans.
The ROUGE method (Lin andHovy, 2003) is based on n-gram overlap betweenthe system-produced and ideal summaries.
As such,it is a recall-based measure, and it requires thatthe length of the summaries be controlled in or-der to allow for meaningful comparisons.
AlthoughROUGE is only a proxy measure of summary qual-ity, it offers the advantage that it can be readily ap-plied to compare the performance of different sys-tems on the same set of documents, assuming thatideal summaries are available for those documents.Baseline Our baseline method does not considerthe overlap in information content between selectedtextual units.
Instead, we fix the score of each sen-tence as the sum of tf*idf values or atomic eventscores.
At every step we choose the remaining sen-tence with the largest score, until the stopping crite-rion for summary length is satisfied.Results For every version of our baseline andapproximation algorithms, and separately for thetf*idf -weighted words and event features, we get asorted list of sentences extracted according to a par-ticular algorithm.
Then, for each DUC document setwe create four summaries of each suggested length(50, 100, 200, and 400 words) by extracting accord-ingly the first 50, 100, 200, and 400 words from thetop sentences.To evaluate the performance of our summarizerswe compare their outputs against the human modelsof the corresponding length provided by DUC, us-ing the ROUGE-created scores for unigrams.
Sincescores are not comparable across different docu-ment sets, instead of average scores we report thenumber of document sets for which one algorithmoutperforms another.
We compare each of ourLength Events tf*idf50 +3 0100 +4 ?4200 +2 ?4400 +5 0Table 1: Adaptive greedy algorithm versus baseline.Length Events tf*idf50 0 + 7100 +4 + 4200 +8 + 6400 +2 +14Table 2: Modified greedy algorithm versus baseline.approximation algorithms (adaptive and modifiedgreedy) to the baseline.Table 1 shows the number of data sets forwhich the adaptive greedy algorithm outperformsour baseline.
This implementation of our informa-tion packing model improves the ROUGE scores inmost cases when events are used as features, whilethe opposite is true when tf*idf provides the con-ceptual units.
This may be partly explained becauseof the nature of the tf*idf -weighted word features:it is possible that important words cannot be con-sidered independently, and that the repetition of im-portant words in later sentence does not necessarilymean that the sentence offers no new information.Thus words may not provide independent enoughfeatures for our approach to work.Table 2 compares our modified greedy algorithmto the baseline.
In that case, the model offers gainsin performance when both events and words areused as features, and in fact the gains are most pro-nounced with the word features.
For both algo-rithms, the gains are generally minimal for 50 wordsummaries and most pronounced for the longest,400 word summaries.
This validates our approach,as the information packing model has a limited op-portunity to alter the set of selected sentences whenthose sentences are very few (often one or two forthe shortest summaries).It is worth noting that in direct comparisons be-tween the adaptive and modified greedy algorithmwe found the latter to outperform the former.
Wefound also events to lead to better performance thantf*idf -weighted words with statistically significantdifferences.
Events tend to be a particularly goodrepresentation for document sets with well-definedconstituent parts (such as specific participants) thatcluster around a narrow event.
Events not only giveus a higher absolute performance when comparedto just words but also lead to more pronounced im-provement when our model is employed.
A moredetailed analysis of the above experiments togetherwith the discussion of advantages and disadvantagesof our evaluation schema can be found in (Filatovaand Hatzivassiloglou, 2004).6 ConclusionIn this paper we proposed a formal model for in-formation selection and redundancy avoidance insummarization and question-answering.
Withinthis two-dimensional model, summarization andquestion-answering entail mapping textual unitsonto conceptual units, and optimizing the selectionof a subset of textual units that maximizes the in-formation content of the covered conceptual units.The formalization of the process allows us to benefitfrom theoretical results, including suitable approx-imation algorithms.
Experiments using DUC datashowed that this approach does indeed lead to im-provements due to better information packing overa straightforward content selection method.7 AcknowledgementsWe wish to thank Rocco Servedio and MihalisYannakakis for valuable discussions of theoreti-cal foundations of the set cover problem.
Thiswork was supported by ARDA under AdvancedQuestion Answering for Intelligence (AQUAINT)project MDA908-02-C-0008.ReferencesRegina Barzilay and Michael Elhadad.
1997.
Us-ing lexical chains for text summarization.
In Pro-ceedings of the ACL/EACL 1997 Workshop on In-telligent Scalable Text Summarization, Spain.Sasha Blair-Goldensohn, Kathleen R. McKeown,and Andrew Hazen Schlaikjer.
2003.
Defscriber:A hybrid system for definitional qa.
In Proceed-ings of 26th Annual International ACM SIGIRConference, Toronoto, Canada, July.Elena Filatova and Vasileios Hatzivassiloglou.2003.
Domain-independent detection, extraction,and labeling of atomic events.
In Proceedings ofRecent Advances in Natural Language Process-ing Conference, RANLP, Bulgaria.Elena Filatova and Vasileios Hatzivassiloglou.2004.
Event-based extractive summarization.
InProceedings of ACL Workshop on Summariza-tion, Barcelona, Spain, July.Jade Goldstein, Vibhu Mittal, Jaime Carbonell,and Jamie Callan.
2000.
Creating and evaluat-ing multi-document sentence extract summaries.In Proceedings of the ninth international con-ference on Information and knowledge manage-ment, pages 165?172.Donna Harman and Ellen Voorhees, editors.
2001.Proceedings of the Document UnderstandingConference (DUC).
NIST, New Orleans, USA.Vasileios Hatzivassiloglou, Judith L. Klavans,Melissa L. Holcombe, Regina Barzilay, Min-Yen Kan, and Kathleen R. McKeown.
2001.Simfinder: A flexible clustering tool for summa-rization.
In Proceedings of workshop on Auto-matic Summarization, NAACL, Pittsburg, USA.Dorit S. Hochbaum.
1997.
Approximating cov-ering and packing problems: Set cover, vertexcover, independent set, and related problems.
InDorit S. Hochbaum, editor, Approximation Al-gorithms for NP-hard Problems, pages 94?143.PWS Publishing Company, Boston, MA.H.P.Edmundson.
1968.
New methods in automaticextracting.
Journal of the Association for Com-puting Machinery, 23(1):264?285, April.Julian Kupiec, Jan Pedersen, and Francine Chen.1995.
A trainable document summarizer.
In Pro-ceedings of 18th Annual International ACM SI-GIR Conference, pages 68?73, Seattle, USA.Chin-Yew Lin and Eduard Hovy.
1997.
Identify-ing topic by position.
In Proceedings of the 5thConference on Applied Natural Language Pro-cessing, ANLP, Washington, DC.Chin-Yew Lin and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using n-gram co-occurrence statistics.
In Proceedings of 2003Language Technology Conference (HLT-NAACL2003), Edmonton, Canada, May.H.P.
Luhn.
1959.
The automatic creation of litera-ture abstracts.
IBM Journal of Research and De-velopment, 2(2):159?165, April.Daniel Marcu.
1997.
From discourse struc-tures to text summaries.
In Proceedings of theACL/EACL 1997 Workshop on Intelligent Scal-able Text Summarization, pages 82?88, Spain.Simone Teufel and Marc Moens.
1997.
Sentenceextraction as a classification task.
In Proceedingsof the ACL/EACL 1997 Workshop on IntelligentScalable Text Summarizaion, Spain.Ellen M. Voorhees.
2003.
Evaluating answers todefinition questions.
In Proceedings of HLT-NAACL, Edmonton, Canada, May.Hong Yu and Vasileios Hatzivassiloglou.
2003.
To-wards answering opinion questions: Separatingfacts from opinions and identifying the polarity ofopinion sentences.
In Proceedings of the Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP), Sapporo, Japan, July.
