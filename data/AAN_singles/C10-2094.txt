Coling 2010: Poster Volume, pages 819?827,Beijing, August 2010Instance Sense Induction from Attribute SetsRicardo Martin-BruallaGoogle Incrmbrualla@gmail.comEnrique AlfonsecaGoogle Incealfonseca@google.comMarius PascaGoogle Incmars@google.comKeith HallGoogle Inckbhall@google.comEnrique Robledo-ArnuncioGoogle Incera@google.comMassimiliano CiaramitaGoogle Incmassi@google.comAbstractThis paper investigates the new problemof automatic sense induction for instancenames using automatically extracted at-tribute sets.
Several clustering strategiesand data sources are described and eval-uated.
We also discuss the drawbacks ofthe evaluation metrics commonly used insimilar clustering tasks.
The results showimprovements in most metrics with re-spect to the baselines, especially for pol-ysemous instances.1 IntroductionRecent work on information extraction increas-ingly turns its attention to the automatic acqui-sition of open-domain information from largetext collections (Etzioni et al, 2008).
The ac-quired information typically includes instances(e.g.
barack obama or hillary clinton), class la-bels (e.g.
politician or presidential candidate)and relations and attributes of the instances (e.g.president-country or date-of-birth) (Sekine, 2006;Banko et al, 2007).Within the larger area of relation extraction,the acquisition of instance attributes (e.g.
pres-ident for instances of countries, or side effectsfor instances of drugs) plays an important role,since attributes may serve as building blocks inany knowledge base constructed around open-domain classes of instances.
Thus, a varietyof attribute extraction methods mine textual datasources ranging from unstructured (Tokunaga etal., 2005) or structured (Cafarella et al, 2008) textwithin Web documents, to human-compiled ency-clopedia (Wu et al, 2008; Cui et al, 2009) andWeb search query logs (Pas?ca and Van Durme,2007), attempting to extract, for a given class, aranked list of attributes that is as comprehensiveand accurate as possible.Previous work on attribute extraction, however,does not capture or address attributes of polyse-mous instances.
An instance may have differ-ent meanings, and the extracted attributes maynot apply to all of them.
For example, themost salient meanings of darwin are the scientistCharles Darwin, an Australian city, and an op-erating system, plus many less-known meanings.For these ambiguous instances, it is common forthe existing procedures to extract mixed lists ofattributes that belong to incompatible meanings,e.g.
{biography, population, hotels, books}.This paper explores the problem of automati-cally inducing instance senses from the learnedattribute lists, and describes several clustering so-lutions based on a variety of data sources.
Forthat, it brings together research on attribute acqui-sition and on word sense induction.
Results showthat we can generate meaninful groupings of at-tributes for polysemous instance names, while notharming much the monosemous instance namesby generating unwanted clusters for them.
Theresults are much better than for a random base-line, and are superior to the one-in-all and the all-singleton baselines.2 Previous WorkPrevious work on attribute extraction uses a va-riety of types of textual data as sources for miningattributes.
Some methods take advantage of struc-tured and semi-structured text available withinWeb documents.
Examples of this are the use ofmarkup information in HTML documents to ex-819tract patterns and clues around attributes (Yoshi-naga and Torisawa, 2007; Wong and Lam, 2009;Ravi and Pas?ca, 2008), or the use of articleswithin online encyclopedia as sources of struc-tured text for attribute extraction (Suchanek et al,2007; Nastase and Strube, 2008; Wu and Weld,2008).
Regarding unstructured text in Web docu-ments, the method described in (Tokunaga et al,2005) takes various class labels as input, and ap-plies manually-created lexico-syntactic patterns todocument sentences to extract candidate attributesranked using several frequency statistics.
In (Bel-lare et al, 2007), the extraction is guided by a setof seed instances and attributes rather than hand-crafted patterns, with the purpose of generatingtraining data and extract new instance-attributepairs from text.Web search queries have also been used as adata source for attribute extraction, using lexico-syntactic patterns (Pas?ca and Van Durme, 2007) orseed attributes (Pas?ca, 2007) to guide the extrac-tion, and leading to attributes of higher accuracythan those extracted with equivalent techniquesfrom Web documents (Pas?ca et al, 2007).Another related area to this work is the field ofword sense induction: the task of identifying thepossible senses of a word in a corpus using unsu-pervised methods (Yarowsky, 1995), as opposedto traditional disambiguation methods which relyon the availability of a finite and static list of pos-sible meanings.
In (Agirre and Soroa, 2007) aframework is proposed for evaluating such sys-tems.
Word sense induction can be naturally for-mulated as a clustering task.
This introducesthe complication of choosing the right numberof possible senses, hence a Bayesian approach toWSI was proposed which deals with this problemwithin a principled generative framework (Brodyand Lapata, 2009).
Another related line of workTurkey Attributes Darwin Attributesmaps1 capital1 maps1 definition1,3recipes2 culture1 awards2 jobs1pictures1,2 history1 shoes1 tourism1calories2 tourism1 evolution3 biography3facts1,2 nutrition facts2 theory3 attractions1nutrition2 beaches1 weather1 hotels1cooking time2 brands2 pictures1,3 ports4religion1 language1 quotes3 population1Table 1: Attributes extracted for the instancesTurkey and Darwin.is the disambiguation of people names (Mann andYarowsky, 2003).
In SEMEVAL-1, a shared taskwas introduced dedicated to this problem, the WebPeople Search task (Artiles et al, 2007; Artiles etal., 2009).
Disambiguating names is also often ap-proached as a clustering problem.
One challengeshared by word sense induction and name disam-biguation (and most unsupervised settings), is theevaluation.
In both tasks, simple baselines such aspredicting one single cluster tend to outperformmore sophisticated approaches (Agirre and Soroa,2007; Artiles et al, 2007).3 Instance Sense Induction3.1 Problem descriptionThis paper assumes the existence of an attributeextraction procedure.
Using those attributes, ouraim is to identify the coarse-grained meaningswith which each attribute is associated.
As anexample, Table 1 shows the top 16 attributes ex-tracted using the procedure described in (Pas?caand Van Durme, 2007).
Salient meanings forturkey are the country name (labeled as 1 in thetable), and the bird name (labeled as 2).
Some at-tributes are applicable to both meanings (picturesand facts).
The second example, darwin, can re-fer to a city (sense 1), the Darwin Awards (sense2), the person (sense 3), and an operating system(sense 4).Examples of applications that need to dis-criminate between the several meanings of in-stances are user-facing applications requiring theattributes to be organized logically and informa-tion extraction pipelines that depend on the ex-tracted attributes to find values in documents.The problem we are addressing is the automaticinduction of instance senses from the attributesets, by grouping together the attributes that canbe applied to a particular sense.
As in related workon sense induction (Agirre and Soroa, 2007; Ar-tiles et al, 2007), we approach this as a clusteringproblem: finding the right similarity metrics andclustering procedures to identify sets of related at-tributes in an instance.
We propose a clusteringbased on the Expectation-Maximization (EM) al-gorithm (Dempster et al, 1977), exploring differ-ent parameters, similarity sources, and prior dis-tributions.8203.2 Instance and attributes input dataThe input data of instances and attributes has beenobtained, in a fully automated way, followingthe method described in (Pas?ca and Van Durme,2007).
The input dataset is a set of fully anony-mized set of English queries submitted to a popu-lar (anonymized) search engine.
The set containsmillions of unique isolated, individual queries thatare independent from one another.
Each queryis accompanied by its frequency of occurrencein the query logs.
The sum of frequencies ofall queries in the dataset is hundreds of millions.Other sources of similar data are available pub-licly for research purposes (Gao et al, 2007).
Thisextraction method applies a few patterns (e.g., theA of I, or I?s A, or A of I) to queries withinquery logs, where an instance I is one of the mostfrequent 5 million queries from the repository ofisolated queries, and A is a candidate attribute.For each instance, the method extracts ranked listscontaining zero, one or more attributes, along withfrequency-based scores.
For this work, only thetop 32 attributes of each instance were used, in or-der to have an input set for the clustering with areasonable size, but to keep precision at high lev-els.3.3 Per-attribute clustering informationFor each (instance, attribute) pair, the followinginformation is collected:Search results: The top 20 search results (in-cluding titles and snippets) returned by a popularsearch engine for a query created by concatenat-ing the instance and the attribute.
The motivationfor this data source is that the attributes that re-fer to the same meaning of the instance shouldhelp the search engine in selecting web pages thatrefer to that meaning.
The titles and snippets ofthese search results are expected to contain otherterms related to that meaning.
For example, forthe queries [turkey maps] and [turkey culture] thesearch results will contain information related tothe country, whereas [turkey recipes] and [turkeynutritional value] should share many terms aboutthe poultry.Query sessions: A query session is a series ofqueries submitted by a single user within a smallrange of time (Silverstein et al, 1999).
Informa-tion stored in the session logs may include the textFor each (instance, attribute) pair:?
Retrieve all the sessions that contained the query [in-stance attribute].?
Collect the set of all the queries that appeared in thesame session and which are a superstring of instance.?
Remove instance from each of those queries, and out-put the resulting set of query words.Figure 1: Algorithm to collect session phrases as-sociated to attributes.of the queries and metadata, such as the time, thetype of query (e.g., using the normal or the ad-vance form), and user settings such as the Webbrowser used (Silverstein et al, 1999).Users often search for related queries withina session: queries on the culture of the coun-try Turkey will tend to be surrounded by queriesabout topics related to the country; similarly,queries about turkey recipes will tend to be sur-rounded by other queries on recipes.
Therefore,if two attributes refer to the same meaning of theinstance, the distributions of terms that co-occurwith them in the same search sessions is expectedto be similar.
To ensure that the user did notchange intent during the session, we also requirethe queries from which we extract phrases to con-tain the instance of interest.
The pseudocode ofthe procedure is shown in Figure 1.Class labels: As described in (Pas?ca and VanDurme, 2008), we collect for each instance (e.g.,turkey), a ranked list of class labels (e.g., country,location, poultry, food).
The procedure uses a col-lection of Web documents and applies some IsAextraction patterns selected from (Hearst, 1992).Using the (instance, ranked-attributes) and the (in-stance, ranked-class labels) lists, it is possible toaggregate the two datasets to obtain, for each at-tribute, the class labels that are most strongly as-sociated to it (Figure 2).3.4 EM clusteringWe run a set of EM clusterings separately for theattributes of each instance.
The model imple-mented is the following: given an instance, letA = {a1, a2, ..., an} be the set of attributes as-sociated with that instance.
Let T be the vocabu-lary for the terms found in the search results, S thevocabulary of session log terms co-occurring with821For each attribute:?
Collect all the instances that contain that attribute.?
For each class label, average its ranks for those in-stances.
If an instance does not contain a particularclass label, use as rank the size of the longest list ofclass labels plus one.?
Rank the class labels from smaller to larger averagerank.Figure 2: Algorithm to collect class labels associ-ated to attributes.the attribute, and C be the set of all the possibleclass labels.
Let K be the cluster function whichassigns cluster indexes to the attributes.We assume that the distributions for snippetterms, session terms and class labels are condi-tionally independent given the clustering.
Further-more, we assume that the distribution of terms forqueries in a cluster are also conditionally indepen-dent given the cluster assignments:p?
(T |K,A) ?Yjp?
(tj |K,A)p?
(S|K,A) ?Ykp?(sk|K,A)p?
(C|K,A) ?Ylp?
(cl|K,A)The clustering model for each instance (the ex-pectation step) is, therefore:p?
(KT SC|A,?)
=N?ip?(K|A)p?
(T |K,A)p?(S|K,A)p?
(C|K,A)To estimate the parameters of the model, we mustbe able to estimate the following distributions dur-ing the maximization step:?
p?
(tj |K,A) = E?
(tj ,K|A)E?(K|A)?
p?
(sk|K,A) = E?(sk,K|A)E?(K|A)?
p?
(cl|K,A) = E?(cl,K|A)E?
(K|A)One advantage of this approach is that it allowsusing a subset of the available data sources to eval-uate their relative influence on the clustering qual-ity.
In the experiments we have tried all possiblecombinations of the three data sources to find thesettings that give the best results.3.5 Initialization strategiesThe initial assignment of attributes to clusters isimportant, since a bad seed clustering can leadEM to local optima.
We have tried the followingtwo strategies:Random assignment: the attributes are assignedto clusters randomly.
To make the results repeat-able, for each instance we use the instance nameas the seed for the random number generator.K-means: the initial assignments of attributesto clusters is performed using K-means.
In thismodel, we use a simple vector-space-model in thefollowing way:1.
Each attribute is represented with a bag-of-words of the snippets of the search results fora concatenation of the instance name and theattribute.
This is the same data already col-lected for EM.2.
Each of the snippet terms in these bag-of-words is weighted using the tf ?
idf score,with inverse document frequencies estimatedfrom an English web corpus with hundredsof millions of documents.3.
The cosine of the angle of the vectors is usedas the similarity metric between each pair ofattributes.Several values of K have been tried in our exper-iments, as mentioned in Section 4.3.6 Post-processingEM works with a fixed set of clusters.
In orderto decide which is the optimal number of clusters,we have run all the experiments with a number ofclusters K that is large enough to accommodatemost of the queries in our dataset, and we run apost-processing step that merges clusters for in-stances that have less than K meanings.Since we have, for each attribute, a distributionof the most likely class labels (Section 3.3), thepost-processing performs as follows:1.
Generate a list of class labels per cluster, bycombining the ranked lists of per-attributeclass labels as was done in Section 3.3.2.
Merge together all the clusters such that theirsets of top k class labels are the same.The values ofK and k are chosen by doing severalruns with different values on the development set,as described in Section 4.8224 Evaluation and Results4.1 Evaluation metricsThere does not exist a fully agreed evaluationmetric for clustering tasks in NLP (Geiss, 2009;Amigo?
et al, 2009).
Each metric has its ownidiosyncrasies, so we have chosen to computesix different evaluation metrics as described in(Amigo?
et al, 2009).
Empirical results show theyare highly correlated, i.e., tuning a parameter byhill-climbing on F-score typically also improvesthe B3 F-score.Purity (Zhao and Karypis, 2002): Let C bethe clusters to evaluate, L the set of cate-gories (the clusters in the gold-standard), andN the number of clustered items.
Purity isthe average of the precision values: Purity =?i|Ci|N maxj Prec(Ci, Lj), where the precisionfor cluster Ci with respect to category Lj isPrec(Ci, Lj) = |Ci?Lj ||Ci| .
Purity is a precision met-ric.
Inverting the roles of the categories L and theclusters C gives a recall metric, inverse purity,which rewards grouping items together.
The twometrics can be combined in an F-score.B3 Precision (Bagga and Baldwin, 1998): LetL(e) and C(e) denote the gold-standard-categoryand the cluster of an item e. The correctness of therelation between e and other element e?
is definedasCorrectness(e, e?)
=?1 iffL(e) = L(e?)?
C(e) = C(e?
)0 otherwiseThe B3 Precision of an item is the proportionof items in its cluster which belong to its cat-egory, including itself.
The total precision isthe average of the item precisions: B3 Prec =avge[avge?:C(e)=C(e?
)Correctness(e, e?
)]B3 Recall: is calculated in a similar way, invertingthe roles of clusters and categories.
The B3 F-score is obtained by combining B3 precision andB3 recall.4.2 Gold standardsWe have built two annotated sets, one to be usedas a development set for adjusting the parame-ters, and a second one as a test set.
The evalu-ation settings were chosen without knowledge ofPurity Inv.
F-score B3 B3 B3Purity Precision Recall F-score0.94 0.95 0.92 0.90 0.92 0.91Table 2: Inter-judge agreement scores.Polysemous Main meaningsairplane machine, movieapple fruit, companyarmstrong unit, company, personchain reaction company, film, band, chemistrychf airport, currency, heart attackdarwin person, citydavid copperfield book, performer, moviedelta letter, airwaysTable 3: Examples of polysemous instances.the test set.
Each of the two sets contains 75 in-stances chosen randomly from the complete set ofinstances with ranked attributes (Section 3.2 de-scribed the input data).
For the random sampling,the instances were weighted with their frequencyin the query logs as full queries, so that morefrequent instances have higher chance to be cho-sen.
This ensures that uncommon instances arenot overrepresented in the gold-standard.The annotators contributed 50 additional in-stances (25 for development and 25 for testing)that they considered interesting to study, e.g., be-cause of having several salient meanings.Five human annotators were shown the top 32attributes for each instance, and they were askedto cluster them.
We decided to start with a sim-plified version of the problem by considering it ahard clustering task.Table 2 shows that the average agreementscores between judge pairs, measured with thesame evaluation metrics used for the system out-put, are quite high.
In the first three metrics, theF-score is not an average of precision and recall,but a weighted average calculated separately foreach cluster, so it may have a value that is not be-tween the values of precision and recall.The annotated instances were classified asmonosemous/polysemous, depending on wetheror not they had more than one cluster with enough(five) attributes.
This classification allows to re-port separate results for the whole set (where in-stances with just one major sense dominate) andfor the subset of polysemous instances.
Table 3shows examples of polysemous instances.
Exam-823All instances polysemous instancesWeights Purity Inv.
F B3 B3 B3 F Purity Inv.
F B3 B3 B3 FPurity score Prec.
Recall score Purity score Prec.
Recall scoreAll-in-one 0.797 1.000 0.766 0.700 1.000 0.797 0.558 1.000 0.540 0.410 1.000 0.573All-singletons 1.000 0.145 0.187 1.000 0.145 0.242 1.000 0.205 0.266 1.000 0.205 0.333Random 0.888 0.322 0.451 0.851 0.246 0.373 0.685 0.362 0.447 0.595 0.276 0.373Random Only snippets 0.809 0.374 0.417 0.737 0.311 0.410 0.596 0.430 0.401 0.483 0.361 0.399Init.
Only sessions 0.797 0.948 0.728 0.700 0.944 0.753 0.558 1.000 0.540 0.410 1.000 0.573Only class labels 0.798 0.983 0.760 0.701 0.969 0.785 0.561 0.990 0.541 0.415 0.981 0.574No snippets 0.798 0.934 0.723 0.702 0.918 0.744 0.561 0.990 0.541 0.415 0.981 0.574No sessions 0.809 0.374 0.417 0.737 0.311 0.410 0.596 0.430 0.401 0.483 0.361 0.399No class labels 0.809 0.374 0.417 0.737 0.311 0.410 0.596 0.430 0.401 0.483 0.361 0.399All 0.809 0.380 0.420 0.736 0.316 0.414 0.596 0.430 0.400 0.483 0.361 0.399K-Means Only snippets 0.844 0.765 0.700 0.771 0.654 0.675 0.671 0.806 0.587 0.556 0.719 0.611Init.
Only sessions 0.798 0.957 0.736 0.702 0.949 0.759 0.558 1.000 0.540 0.410 1.000 0.573Only class labels 0.824 0.656 0.622 0.747 0.568 0.604 0.641 0.768 0.565 0.519 0.699 0.575No snippets 0.824 0.655 0.622 0.748 0.562 0.598 0.640 0.768 0.565 0.518 0.698 0.574No sessions 0.843 0.770 0.701 0.769 0.661 0.677 0.671 0.806 0.587 0.556 0.719 0.611No class labels 0.844 0.762 0.698 0.771 0.651 0.673 0.671 0.806 0.587 0.556 0.719 0.611All 0.843 0.767 0.699 0.770 0.657 0.675 0.671 0.806 0.587 0.556 0.719 0.611Table 4: Scores over all instances and over polysemous instances.ples of monosemous instances are activision, am-ctheaters, american airlines, ask.com, bebo, dis-ney or einstein.
22% of the instances in the devel-opment set and 13% of the instances in the test setare polysemous.4.3 Parameter tuningWe tuned the different parameters of the algorithmusing the development set.
We performed severalEM runs including all three data sources, modi-fying the following parameters: the smoothing added to the cluster soft-assignment in the Maxi-mization step (Manning et al, 2008), the numberK of clusters for K-Means and EM, and the num-ber k of top ranked class labels that two clustersneed to have in common in order to be mergedat the post-processing step.
The best results wereobtained with  = 0.4, K = 5 and k = 1.
Theseare the values used in the experiments mentionedfrom now on.4.4 EM initialization and data sourcesTable 4 shows the results after running EM overthe development set, using every possible combi-nation of data sources, and the two initializationstrategies (random and K-Means).
Several obser-vations can be drawn from this table:First, as mentioned in Section 2, the evalua-tion metrics are biased towards the all-in-one solu-tion.
This is worsened by the fact that the majorityof the instances in our dataset are monosemous.Therefore, the highest F-scores and B3 F-scoresare obtained by the all-in-one baseline, althoughit is not the most useful clustering.When using only class labels, EM tends to pro-duce results similar to the all-in-one baseline Thiscan be explained by the limited class vocabularywhich makes most of the attributes share class la-bels.
The bad results when using only sessions arecaused by the presence of attributes with no ses-sion terms, due to insufficient data.The random clustering baseline (third line inTable 4) tends to give smaller clusters than EM,because it distributes instances uniformly acrossthe clusters.
This leads to better precision scores,and much worse recall and F-score metrics.From these results, we conclude that snippetterms are the most useful resource for clustering.The other data sources do not provide a signifi-cant improvement over it.
The best results overallfor the polysemous instances, and the highest re-sults for the whole dataset (excluding the outliersthat are too similar to the all-in-one baseline) areobtained using snippet terms.
For these configura-tions, as we expected, the K-Means initializationdoes a better job in avoiding local optima duringEM than the random one.4.5 Post-processingTable 5 includes the results on the developmentset after post-processing, using the best configu-ration for EM (K-Means initialization and snippetterms for EM).
Post-processing slightly hurts theB3 F-score for polysemous terms, but it improvesresults for the whole dataset, as it merges manyclusters for the monosemous instances.824Data Method Purity Inv.
Purity F-score B3 Prec.
B3 Recall B3 F-scoreAll instances All-in-one 0.797 1.000 0.766 0.700 1.000 0.797All-singletons 1.000 0.145 0.187 1.000 0.145 0.242K-Means + EM (snippets) 0.844 0.765 0.700 0.771 0.654 0.675K-Means + EM (snippets) + postprocessing 0.825 0.837 0.728 0.743 0.761 0.722Polysemous All-in-one 0.558 1.000 0.540 0.410 1.000 0.573All-singletons 1.000 0.205 0.266 1.000 0.205 0.333K-Means + EM (snippets) 0.671 0.806 0.587 0.556 0.719 0.611K-Means + EM (snippets) + postprocessing 0.644 0.846 0.592 0.518 0.777 0.607Table 5: Scores only over all and polysemous instances, without and with postprocessing.K-Means output EM output Post-processingpictures, family, logo, biography pictures, biography, inauguration pictures, biography, inaugurationinauguration, song, lyrics, foods, song, lyrics, foods, timeline, song, lyrics, goods, timeline,quotes, timeline, shoes, health care camping, shoes, maps, art, history, camping, shoes, maps, art, historymaps, art, kids, history, speeches official website, facts, speeches official website, facts, speechesofficial website, facts, scandal scandal, blog, music scandal, blog, music, family, kidseconomy, blog, music, flag, camping approval rating, health care, daughtersapproval rating economy approval rating, health care,daughters family, kids, daughters economysymbol logo, quotes, symbol, flag logo, quotes, symbol, definitiondefinition, religion, definition, religion, slogan, books religion, slogan, books, flagslogan, booksTable 6: Attributes extracted for the monosemous instance obama, using snippet terms for EM.4.6 Clustering examplesTables 6 and 7 show examples of clustering resultsfor three instances chosen as representatives of themonosemous and the polysemous subsets.
Theseshow that the output of the K-Means initializationcan uncover some meaningful clusters, but tendsto generate a dominant cluster and a few small orsingleton clusters.
EM distributes the attributesmore evenly across clusters, combining attributesthat are closely related.For monosemous instances like obama, EMgenerates small clusters of highly related at-tributes (e.g, family, kids and daughters).
Post-processing merges some of the clusters together,but it fails to merge all into a single cluster.For darwin, two of the small clusters given byK-Means are actually good, as ports is the only at-tribute of the operating system, and lyrics is one ofthe two attributes referring to a song titled Darwin.EM again redistributes the attributes, creating twolarge and mostly correct clusters.For david copperfield, EM creates two clustersfor the performer, one for the book, one for themovie, and one for tattoo (off-topic for this in-stance).
The two clusters referring to the per-former are merged in the post-processing, withsome errors remaining, e.g, trailer and secondwife are in the wrong cluster.4.7 Results on the test setTable 8 show the results of the EM clustering andthe postprocessing step when executed on the testset.
The settings are those that produced the bestresults on the development set: using EM initial-ized with K-Means, and using only snippet termsfor the generative model.As mentioned above, the test set has a higherproportion of monosemous queries than the de-velopment set, so the all-in-one baseline pro-duces better results than before.
Still, we can seethe same trend happening: for the whole datasetthe F-score metrics are somewhat worse than thebest baseline, given that the evaluation metrics allovervalue the all-in-one baseline, but this can beconsidered an artifact of the metrics.
As with thedevelopment set, using EM produces the best pre-cision scores (except for the all-singletons base-line), and the postprocessing improves precisionand F-score over the all-in-one baseline.
Thewhole system improves considerably the F-scorefor the polysemous terms.5 ConclusionsThis paper investigates the new task of inducinginstance senses using ranked lists of attributes asinput.
It describes a clustering procedure basedon the EM model, capable of integrating differ-825Instance K-Means output EM output Post-processingDarwin maps, shoes, logo, awards, maps, shoes, logo, maps, shoes, logo,weather pictures, quotes, weather jobs, tourism weather jobs, tourismdefinition, jobs, tourism, hotels, attractions, hotels, attractions,biography, hotels, beaches, accommodation, beaches, accommodation,attractions, beaches, tv show, clothing, tv show, clothing,accommodation, tv show, postcode, music, review postcode, music, reviewclothing, postcode, music side effects, airlines, side effects, airlines,facts, review, history prices, lighting prices, lightingside effects, airlines, awards, ports definition, populationprices, lighting evolution, theory, quotes awards, portsports pictures, biography, evolution, theory, quotesevolution, theory, books facts, history, books pictures, biography,lyrics lyrics facts, history, bookspopulation definition, population lyricsDavid Copperfield summary, biography, pictures, biography, pictures, quotes, biography, pictures, girlfriendquotes, strokes, book review, strokes, tricks, tour dates, quotes, strokes, tricks, tattootricks, tour dates, characters, lyrics, dating, logo, tour dates, secrets, lyrics,lyrics, plot, synopsis, dating, filmography, cast members, wives, music, dating, logo,logo, themes, author, official website, trailer, filmography, blog, cast members,filmography, cast members, setting, religion official website, trailer,official website, trailer, book review, review, house, setting, religionsetting, religion reviews book review, review, house,house, reviews tattoo reviewstattoo summary, second wife, summary, second wife,second wife characters, plot, synopsis, characters, plot, synopsis,girlfriend, secrets, wives, themes, author themes, authorreview, music, blog girlfriend, secrets, wives,music, blogTable 7: Attributes extracted for three polysemous instances, using snippet terms for EM.Set Solution Purity Inverse Purity F-score B3 Precision B3 Recall B3 F-scoreAll All-in-one 0.907 1.000 0.892 0.858 1.000 0.908All-singletons 1.000 0.076 0.114 1.000 0.076 0.136Random 0.936 0.325 0.463 0.914 0.243 0.377EM 0.927 0.577 0.664 0.896 0.426 0.561EM+postprocessing 0.919 0.806 0.804 0.878 0.717 0.764Polysemous All-in-one 0.588 1.000 0.586 0.457 1.000 0.613All-singletons 1.000 0.141 0.210 1.000 0.141 0.239Random 0.643 0.382 0.441 0.549 0.288 0.369EM 0.706 0.631 0.556 0.626 0.515 0.547EM+postprocessing 0.675 0.894 0.650 0.564 0.842 0.661Table 8: Scores in the test set.ent data sources, and explores cluster initializa-tion and post-processing strategies.
The evalu-ation shows that the most important of the con-sidered data sources is the snippet terms obtainedfrom search engine results to queries made byconcatenating the instance and the attribute.
Asimple post-processing that merges attribute clus-ters that have common class labels can improverecall for monosemous queries.
The results showimprovements across most metrics with respect toa random baseline, and F-score improvements forpolysemous instances.Future work includes extending the generativemodel to be applied across the board, linking theclustering models of different instances with eachother.
We also intend to explore applications ofthe clustered attributes in order to perform extrin-sic evaluations on these data.ReferencesAgirre, Eneko and Aitor Soroa.
2007.
Semeval-2007 task02: Evaluating word sense induction and discriminationsystems.
In Proceedings of SemEval-2007, pages 7?12.Association for Computational Linguistics.Amigo?, E., J. Gonzalo, J. Artiles, and F. Verdejo.
2009.A comparison of extrinsic clustering evaluation met-rics based on formal constraints.
Information Retrieval,12(4):461?486.Artiles, Javier, Julio Gonzalo, and Satoshi Sekine.
2007.The semeval-2007 WePS evaluation: Establishing abenchmark for the web people search task.
In Proceed-ings of SemEval-2007, pages 64?69.Artiles, J., J. Gonzalo, and S. Sekine.
2009.
Weps 2 evalua-tion campaign: overview of the web people search cluster-ing task.
In 2nd Web People Search Evaluation Workshop(WePS 2009), 18th WWW Conference.Bagga, A. and B. Baldwin.
1998.
Entity-based cross-document co-referencing using the vector space model,Proceedings of the 17th international conference on Com-putational linguistics.
In Proceedings of ACL-98.826Banko, M., Michael J Cafarella, S. Soderland, M. Broad-head, and O. Etzioni.
2007.
Open information extractionfrom the Web.
In Proceedings of IJCAI-07, pages 2670?2676, Hyderabad, India.Bellare, K., P.P.
Talukdar, G. Kumaran, F. Pereira, M. Liber-man, A. McCallum, and M. Dredze.
2007.
Lightly-Supervised Attribute Extraction.
In NIPS 2007 Workshopon Machine Learning for Web Search.Brody, Samuel and Mirella Lapata.
2009.
Bayesian wordsense induction.
In Proceedings of EACL ?09, pages 103?111.Cafarella, M.J., A. Halevy, D.Z.
Wang, and Y. Zhang.
2008.Webtables: Exploring the Power of Tables on the Eeb.Proceedings of the VLDB Endowment archive, 1(1):538?549.Cui, G., Q. Lu, W. Li, and Y. Chen.
2009.
Automatic Acqui-sition of Attributes for Ontology Construction.
In Pro-ceedings of the 22nd International Conference on Com-puter Processing of Oriental Languages, pages 248?259.Springer.Dempster, A.P., N.M. Laird, D.B.
Rubin, et al 1977.
Max-imum likelihood from incomplete data via the EM algo-rithm.
Journal of the Royal Statistical Society.
Series B(Methodological), 39(1):1?38.Etzioni, O., M. Banko, S. Soderland, and S. Weld.
2008.Open Information Extraction from the Web.
Communica-tions of the ACM, 51(12), December.Gao, W., C. Niu, J. Nie, M. Zhou, J. Hu, K. Wong, andH.
Hon.
2007.
Cross-lingual query suggestion usingquery logs of different languages.
In Proceedings ofSIGIR-07, pages 463?470, Amsterdam, The Netherlands.Geiss, J.
2009.
Creating a Gold Standard for Sentence Clus-tering in Multi-Document Summarization.
ACL-IJCNLP2009.Hearst, M. 1992.
Automatic acquisition of hyponyms fromlarge text corpora.
In Proceedings of COLING-92, pages539?545, Nantes, France.Mann, Gideon S. and David Yarowsky.
2003.
Unsuper-vised personal name disambiguation.
In Proceedings ofHLT-NAACL 2003, pages 33?40.
Association for Compu-tational Linguistics.Manning, C.D., P. Raghavan, and H. Schtze.
2008.
Intro-duction to Information Retrieval.
Cambridge UniversityPress New York, NY, USA.Nastase, V. and M. Strube.
2008.
Decoding wikipediacategories for knowledge acquisition.
In Proceedings ofAAAI-08, pages 1219?1224, Chicago, Illinois.Pas?ca, M. and B.
Van Durme.
2007.
What you seek is whatyou get: Extraction of class attributes from query logs.
InProceedings of IJCAI-07, pages 2832?2837, Hyderabad,India.Pas?ca, M. and B.
Van Durme.
2008.
Weakly-supervised ac-quisition of open-domain classes and class attributes fromweb documents and query logs.
In Proceedings of ACL-08, pages 19?27, Columbus, Ohio.Pas?ca, M., B.
Van Durme, and N. Garera.
2007.
The role ofdocuments vs. queries in extracting class attributes fromtext.
In Proceedings of CIKM-07, pages 485?494, Lis-bon, Portugal.Pas?ca, M. 2007.
Organizing and searching the World WideWeb of facts - step two: Harnessing the wisdom of thecrowds.
In Proceedings of WWW-07, pages 101?110,Banff, Canada.Ravi, S. and M. Pas?ca.
2008.
Using Structured Text forLarge-Scale Attribute Extraction.
In CIKM.
ACM NewYork, NY, USA.Sekine, S. 2006.
On-Demand Information Extraction.
InProceedings of the COLING/ACL on Main conferenceposter sessions, pages 731?738.
Association for Compu-tational Linguistics Morristown, NJ, USA.Silverstein, C., H. Marais, M. Henzinger, and M. Moricz.1999.
Analysis of a very large web search engine querylog.
In ACM SIGIR Forum, pages 6?12.
ACM New York,NY, USA.Suchanek, F., G. Kasneci, and G. Weikum.
2007.
Yago:a core of semantic knowledge unifying WordNet andWikipedia.
In Proceedings of WWW-07, pages 697?706,Banff, Canada.Tokunaga, K., J. Kazama, and K. Torisawa.
2005.
Au-tomatic discovery of attribute words from Web docu-ments.
In Proceedings of the 2nd International Joint Con-ference on Natural Language Processing (IJCNLP-05),pages 106?118, Jeju Island, Korea.Wong, T.L.
and W. Lam.
2009.
An Unsupervised Methodfor Joint Information Extraction and Feature MiningAcross Different Web Sites.
Data & Knowledge Engi-neering, 68(1):107?125.Wu, F. and D. Weld.
2008.
Automatically refining theWikipedia infobox ontology.
In Proceedings of WWW-08, pages 635?644, Beijing, China.Wu, F., R. Hoffmann, and D. Weld.
2008.
Informationextraction from Wikipedia: Moving down the long tail.In Proceedings of KDD-08, pages 731?739, Las Vegas,Nevada.Yarowsky, David.
1995.
Unsupervised word sense disam-biguation rivaling supervised methods.
In Proceedings ofACL-95, pages 189?196.
Association for ComputationalLinguistics.Yoshinaga, N. and K. Torisawa.
2007.
Open-DomainAttribute-Value Acquisition from Semi-Structured Texts.In Proceedings of the Workshop on Ontolex, pages 55?66.Zhao, Y. and G. Karypis.
2002.
Criterion functions for docu-ment clustering.
Technical report, Experiments and Anal-ysis University of Minnesota, Department of ComputerScience/Army HPC Research Center.827
