Language and Task Independent Text Categorizationwith Simple Language ModelsFuchun Peng Dale Schuurmans Shaojun WangSchool of Computer Science, University of Waterloo200 University Avenue West, Waterloo, Ontario, Canada, N2L 3G1{f3peng, dale, sjwang}@cs.uwaterloo.caAbstractWe present a simple method for language inde-pendent and task independent text categoriza-tion learning, based on character-level n-gramlanguage models.
Our approach uses simpleinformation theoretic principles and achieveseffective performance across a variety of lan-guages and tasks without requiring feature se-lection or extensive pre-processing.
To demon-strate the language and task independence ofthe proposed technique, we present experimen-tal results on several languages?Greek, En-glish, Chinese and Japanese?in several textcategorization problems?language identifica-tion, authorship attribution, text genre classifi-cation, and topic detection.
Our experimentalresults show that the simple approach achievesstate of the art performance in each case.1 IntroductionText categorization concerns the problem of automati-cally assigning given text passages (paragraphs or doc-uments) into predefined categories.
Due to the rapid ex-plosion of texts in digital form, text categorization hasbecome an important area of research owing to the needto automatically organize and index large text collectionsin various ways.
Such techniques are currently being ap-plied in many areas, including language identification,authorship attribution (Stamatatos et al, 2000), text genreclassification (Kesseler et al, 1997; Stamatatos et al,2000), topic identification (Dumais et al, 1998; Lewis,1992; McCallum, 1998; Yang, 1999), and subjective sen-timent classification (Turney, 2002).Many standard machine learning techniques have beenapplied to automated text categorization problems, suchas naive-Bayes classifiers, support vector machines, lin-ear least squares models, neural networks, and K-nearestneighbor classifiers (Yang, 1999; Sebastiani, 2002).
Acommon aspect of these approaches is that they treat textcategorization as a standard classification problem, andthereby reduce the learning process to two simple steps:feature engineering, and classification learning over thefeature space.
Of these two steps, feature engineering iscritical to achieving good performance in text categoriza-tion problems.
Once good features are identified, almostany reasonable technique for learning a classifier seemsto perform well (Scott, 1999).Unfortunately, the standard classification learningmethodology has several drawbacks for text categoriza-tion.
First, feature construction is usually language de-pendent.
Various techniques such as stop-word removalor stemming require language specific knowledge to de-sign adequately.
Moreover, whether one can use a purelyword-level approach is itself a language dependent issue.In many Asian languages such as Chinese or Japanese,identifying words from character sequences is hard, andany word-based approach must suffer added complexityin coping with segmentation errors.
Second, feature se-lection is task dependent.
For example, tasks like au-thorship attribution or genre classification require atten-tion to linguistic style markers (Stamatatos et al, 2000),whereas topic detection systems rely more heavily on bagof words features.
Third, there are an enormous num-ber of possible features to consider in text categorizationproblems, and standard feature selection approaches donot always cope well in such circumstances.
For exam-ple, given an enormous number of features, the cumu-lative effect of uncommon features can still have an im-portant effect on classification accuracy, even though in-frequent features contribute less information than com-mon features individually.
Consequently, throwing awayuncommon features is usually not an appropriate strat-egy in this domain (Aizawa, 2001).
Another problem isthat feature selection normally uses indirect tests, suchas ?2 or mutual information, which involve setting arbi-Edmonton, May-June 2003Main Papers , pp.
110-117Proceedings of HLT-NAACL 2003trary thresholds and conducting a heuristic greedy searchto find good feature sets.
Finally, by treating text cate-gorization as a classical classification problem, standardapproaches can ignore the fact that texts are written innatural language, meaning that they have many implicitregularities that can be well modeled with specific toolsfrom natural language processing.In this paper, we propose a straightforward text cate-gorization learning method based on learning category-specific, character-level, n-gram language models.
Al-though this is a very simple approach, it has not yet beensystematically investigated in the literature.
We find that,surprisingly, we obtain competitive (and often superior)results to more sophisticated learning and feature con-struction techniques, while requiring almost no featureengineering or pre-processing.
In fact, the overall ap-proach requires almost no language specific or task spe-cific pre-processing to achieve effective performance.The success of this simple method, we think, is due tothe effectiveness of well known statistical language mod-eling techniques, which surprisingly have had little sig-nificant impact on the learning algorithms normally ap-plied to text categorization.
Nevertheless, statistical lan-guage modeling is also concerned with modeling the se-mantic, syntactic, lexicographical and phonological regu-larities of natural language?and would seem to provide anatural foundation for text categorization problems.
Oneinteresting difference, however, is that instead of explic-itly pre-computing features and selecting a subset basedon arbitrary decisions, the language modeling approachsimply considers all character (or word) subsequencesoccurring in the text as candidate features, and implic-itly considers the contribution of every feature in the fi-nal model.
Thus, the language modeling approach com-pletely avoids a potentially error-prone feature selectionprocess.
Also, by applying character-level language mod-els, one also avoids the word segmentation problems thatarise in many Asian languages, and thereby achieves alanguage independent method for constructing accuratetext categorizers.2 n-Gram Language ModelingThe dominant motivation for language modeling has tra-ditionally come from speech recognition, but languagemodels have recently become widely used in many otherapplication areas.The goal of language modeling is to predict the prob-ability of naturally occurring word sequences, s =w1w2...wN ; or more simply, to put high probability onword sequences that actually occur (and low probabilityon word sequences that never occur).
Given a word se-quence w1w2...wN to be used as a test corpus, the qualityof a language model can be measured by the empiricalperplexity and entropy scores on this corpusPerplexity = N???
?N?i=11Pr(wi|w1...wi?1) (1)Entropy = log2 Perplexity (2)where the goal is to minimize these measures.The simplest and most successful approach to lan-guage modeling is still based on the n-gram model.
Bythe chain rule of probability one can write the probabilityof any word sequence asPr(w1w2...wN ) =N?i=1Pr(wi|w1...wi?1) (3)An n-gram model approximates this probability byassuming that the only words relevant to predictingPr(wi|w1...wi?1) are the previous n?
1 words; i.e.Pr(wi|w1...wi?1) = Pr(wi|wi?n+1...wi?1)A straightforward maximum likelihood estimate of n-gram probabilities from a corpus is given by the observedfrequency of each of the patternsPr(wi|wi?n+1...wi?1) = #(wi?n+1...wi)#(wi?n+1...wi?1) (4)where #(.)
denotes the number of occurrences of a spec-ified gram in the training corpus.
Although one could at-tempt to use simple n-gram models to capture long rangedependencies in language, attempting to do so directlyimmediately creates sparse data problems: Using gramsof length up to n entails estimating the probability of Wnevents, where W is the size of the word vocabulary.
Thisquickly overwhelms modern computational and data re-sources for even modest choices of n (beyond 3 to 6).Also, because of the heavy tailed nature of language (i.e.Zipf?s law) one is likely to encounter novel n-grams thatwere never witnessed during training in any test corpus,and therefore some mechanism for assigning non-zeroprobability to novel n-grams is a central and unavoidableissue in statistical language modeling.
One standard ap-proach to smoothing probability estimates to cope withsparse data problems (and to cope with potentially miss-ing n-grams) is to use some sort of back-off estimator.Pr(wi|wi?n+1...wi?1)=??????
?P?r(wi|wi?n+1...wi?1),if #(wi?n+1...wi) > 0?(wi?n+1...wi?1)?
Pr(wi|wi?n+2...wi?1),otherwise(5)whereP?r(wi|wi?n+1...wi?1) = discount#(wi?n+1...wi)#(wi?n+1...wi?1)(6)is the discounted probability and ?
(wi?n+1...wi?1) is anormalization constant?
(wi?n+1...wi?1) =1??x?(wi?n+1...wi?1x)P?r(x|wi?n+1...wi?1)1??x?
(wi?n+1...wi?1x)P?r(x|wi?n+2...wi?1)(7)The discounted probability (6) can be computedwith different smoothing techniques, including absolutesmoothing, Good-Turing smoothing, linear smoothing,and Witten-Bell smoothing (Chen and Goodman, 1998).The details of the smoothing techniques are omitted herefor simplicity.The language models described above use individualwords as the basic unit, although one could instead con-sider models that use individual characters as the ba-sic unit.
The remaining details remain the same in thiscase.
The only difference is that the character vocabu-lary is always much smaller than the word vocabulary,which means that one can normally use a much higherorder, n, in a character-level n-gram model (although thetext spanned by a character model is still usually lessthan that spanned by a word model).
The benefits of thecharacter-level model in the context of text classificationare several-fold: it avoids the need for explicit word seg-mentation in the case of Asian languages, it captures im-portant morphological properties of an author?s writing,it models the typos and misspellings that are common ininformal texts, it can still discover useful inter-word andinter-phrase features, and it greatly reduces the sparsedata problems associated with large vocabulary models.In this paper, we experiment with character-level modelsto achieve flexibility and language independence.3 Language Models as Text ClassifiersOur approach to applying language models to text cat-egorization is to use Bayesian decision theory.
Assumewe wish to classify a text D into a category c ?
C ={c1, ..., c|C|}.
A natural choice is to pick the category cthat has the largest posterior probability given the text.That is,c?
= argmaxc?C{Pr(c|D)} (8)Using Bayes rule, this can be rewritten asc?
= argmaxc?C{Pr(D|c) Pr(c)} (9)= argmaxc?C{Pr(D|c)} (10)= argmaxc?C{ N?i=1Prc(wi|wi?n+1...wi?1)}(11)where deducing Eq.
(10) from Eq.
(9) assumes uniformlyweighted categories (since we have no other prior knowl-edge).
Here, Pr(D|c) is the likelihood of D under cate-gory c, which can be computed by Eq.
(11).
Likelihood isrelated to perplexity and entropy by Eq.
(1) and Eq.
(2).Therefore, our approach is to learn a separate languagemodel for each category, by training on a data set fromthat category.
Then, to categorize a new text D, we sup-ply D to each language model, evaluate the likelihood(or entropy) of D under the model, and pick the winningcategory according to Eq.
(10).The inference of an n-gram based text classifier isvery similar to a naive-Bayes classifier.
In fact, n-gramclassifiers are a straightforward generalization of naive-Bayes: A uni-gram classifier with Laplace smoothingcorresponds exactly to the traditional naive-Bayes clas-sifier.
However, n-gram language models, for larger n,possess many advantages over naive-Bayes classifiers, in-cluding modeling longer context and applying superiorsmoothing techniques in the presence of sparse data.4 Experimental ComparisonWe now proceed to present our results on several textcategorization problems on different languages.
Specif-ically, we consider language identification, Greek author-ship attribution, Greek genre classification, English topicdetection, Chinese topic detection and Japanese topic de-tection.For the sake of consistency with previous re-search (Aizawa, 2001; He et al, 2000; Stamatatos etal., 2000), we measure categorization performance by theoverall accuracy, which is the number of correctly iden-tified texts divided by the total number of texts consid-ered.
We also measure the performance with Macro F-measure, which is the average of the F-measures acrossall categories.
F-measure is a combination of precisionand recall (Yang, 1999).4.1 Language IdentificationThe first text categorization problem we examined waslanguage identification?a useful pre-processing step ininformation retrieval.
Language identification is proba-bly the easiest text classification problem because of thesignificant morphological differences between languages,n Absolute Good-Turing Linear Witten-BellAcc.
F-Mac Acc.
F-Mac Acc.
F-Mac Acc.
F-Mac1 0.57 0.53 0.55 0.49 0.55 0.49 0.55 0.492 0.85 0.84 0.80 0.75 0.84 0.83 0.84 0.823 0.90 0.89 0.79 0.72 0.89 0.88 0.89 0.874 0.87 0.85 0.79 0.72 0.85 0.82 0.88 0.865 0.86 0.85 0.79 0.72 0.87 0.85 0.86 0.836 0.86 0.83 0.79 0.73 0.87 0.85 0.86 0.83Table 1: Results on Greek authorship attributioneven when they are based on the same character set.1 Inour experiments, we considered one chapter of Bible thathad been translated into 6 different languages: English,French, German, Italian, Latin and Spanish.
In eachcase, we reserved twenty sentences from each languagefor testing and used the remainder for training.
For thistask, with only bi-gram character-level models and anysmoothing technique, we achieved 100% accuracy.4.2 Authorship AttributionThe second text categorization problem we examined wasauthor attribution.
A famous example is the case of theFederalist Papers, of which twelve instances are claimedto have been written both by Alexander Hamilton andJames Madison (Holmes and Forsyth, 1995).
Authorshipattribution is more challenging than language identifica-tion because the difference among the authors is muchmore subtle than that among different languages.
We con-sidered a data set used by (Stamatatos et al, 2000) con-sisting of 20 texts written by 10 different modern Greekauthors (totaling 200 documents).
In each case, 10 textsfrom each author were used for training and the remain-ing 10 for testing.The results using different orders of n-gram modelsand different smoothing techniques are shown in Table 1.With 3-grams and absolute smoothing, we observe 90%accuracy.
This result compares favorably to the 72%accuracy reported in (Stamatatos et al, 2000) which isbased on linear least square fit (LLSF).4.3 Text Genre ClassificationThe third problem we examined was text genre classifi-cation, which is an important application in informationretrieval (Kesseler et al, 1997; Lee et al, 2002).
We con-sidered a Greek data set used by (Stamatatos et al, 2000)consisting of 20 texts of 10 different styles extracted fromvarious sources (200 documents total).
For each style, weused 10 texts as training data and the remaining 10 as test-ing.1Language identification from speech is much harder.n Absolute Good-Turing Linear Witten-BellAcc.
F-Mac Acc.
F-Mac Acc.
F-Mac Acc.
F-Mac1 0.31 0.55 0.30 0.54 0.30 0.54 0.30 0.542 0.86 0.86 0.60 0.52 0.82 0.81 0.86 0.863 0.77 0.75 0.65 0.59 0.79 0.77 0.85 0.854 0.69 0.65 0.58 0.50 0.74 0.69 0.76 0.745 0.66 0.61 0.56 0.49 0.69 0.66 0.73 0.706 0.62 0.57 0.49 0.53 0.67 0.63 0.71 0.687 0.63 0.58 0.49 0.53 0.66 0.62 0.70 0.68Table 2: Results on Greek text genre classificationThe results of learning an n-gram based text classifierare shown in Table 2.
The 86% accuracy obtained withbi-gram models compares favorably to the 82% reportedin (Stamatatos et al, 2000), which again is based on amuch deeper NLP analysis.4.4 Topic DetectionThe fourth problem we examined was topic detection intext, which is a heavily researched text categorizationproblem (Dumais et al, 1998; Lewis, 1992; McCallum,1998; Yang, 1999; Sebastiani, 2002).
Here we demon-strate the language independence of the language mod-eling approach by considering experiments on English,Chinese and Japanese data sets.4.4.1 English DataThe English 20 Newsgroup data has been widely usedin topic detection research (McCallum, 1998; Rennie,2001).2 This collection consists of 19,974 non-emptydocuments distributed evenly across 20 newsgroups.
Weuse the newsgroups to form our categories, and randomlyselect 80% of the documents to be used for training andset aside the remaining 20% for testing.In this case, as before, we merely considered text to bea sequence of characters, and learned character-level n-gram models.
The resulting classification accuracies arereported in in Table 3.
With 3-gram (or higher order)models, we consistently obtain accurate performance,peaking at 89% accuracy in the case of 6-gram modelswith Witten-Bell smoothing.
(We note that word-levelmodels were able to achieve 88% accuracy in this case.
)These results compare favorably to the state of the art re-sult of 87.5% accuracy reported in (Rennie, 2001), whichwas based on a combination of an SVM with error correctoutput coding (ECOC).4.4.2 Chinese DataChinese topic detection is often thought to be morechallenging than English, because words are not white-space delimited in Chinese text.
This fact seems to2http://www.ai.mit.edu/?
jrennie/20Newsgroups/n Absolute Good-Turing Linear Witten-BellAcc.
F-Mac Acc.
F-Mac Acc.
F-Mac Acc.
F-Mac1 0.22 0.21 0.22 0.21 0.22 0.21 0.22 0.212 0.68 0.66 0.69 0.67 0.68 0.67 0.67 0.653 0.86 0.86 0.86 0.86 0.86 0.85 0.86 0.864 0.88 0.88 0.88 0.87 0.87 0.87 0.89 0.885 0.89 0.88 0.87 0.87 0.88 0.88 0.89 0.896 0.89 0.88 0.88 0.88 0.88 0.88 0.89 0.897 0.89 0.88 0.88 0.87 0.88 0.88 0.89 0.898 0.88 0.88 0.87 0.87 0.88 0.88 0.89 0.899 0.88 0.88 0.87 0.87 0.88 0.88 0.89 0.89Table 3: Topic detection results on English 20 Newsgroupdatan Absolute Good-Turing Linear Witten-BellAcc.
F-Mac Acc.
F-Mac Acc.
F-Mac Acc.
F-Mac1 0.77 0.77 0.76 0.77 0.76 0.76 0.77 0.772 0.80 0.80 0.80 0.80 0.79 0.79 0.80 0.803 0.80 0.80 0.81 0.81 0.80 0.80 0.80 0.804 0.80 0.80 0.81 0.81 0.81 0.80 0.80 0.80Table 4: Chinese topic detection resultsrequire word segmentation to be performed as a pre-processing step before further classification (He et al,2000).
However, we avoid the need for explicit segmen-tation by simply using a character level n-gram classifier.For Chinese topic detection we considered a data setinvestigated in (He et al, 2000).
The corpus in this caseis a subset of the TREC-5 data set created for research onChinese text retrieval.
To make the data set suitable fortext categorization, documents were first clustered into101 groups that shared the same headline (as indicatedby an SGML tag) and the six most frequent groups wereselected to make a Chinese text categorization data set.In each group, 500 documents were randomly selectedfor training and 100 documents were reserved for testing.We observe over 80% accuracy for this task, using bi-gram (2 Chinese characters) or higher order models.
Thisis the same level of performance reported in (He et al,2000) for an SVM approach using word segmentationand feature selection.4.4.3 Japanese DataJapanese poses the same word segmentation issues asChinese.
Word segmentation is also thought to be neces-sary for Japanese text categorization (Aizawa, 2001), butwe avoid the need again by considering character levellanguage models.We consider the Japanese topic detection data inves-tigated by (Aizawa, 2001).
This data set was con-n Absolute Good-Turing Linear Witten-BellAcc.
F-Mac Acc.
F-Mac Acc.
F-Mac Acc.
F-Mac1 0.33 0.29 0.34 0.29 0.34 0.29 0.34 0.292 0.66 0.62 0.66 0.61 0.66 0.63 0.66 0.623 0.75 0.72 0.75 0.72 0.76 0.73 0.75 0.724 0.81 0.77 0.81 0.76 0.82 0.76 0.81 0.775 0.83 0.77 0.83 0.76 0.83 0.76 0.83 0.776 0.84 0.76 0.83 0.75 0.83 0.75 0.84 0.777 0.84 0.75 0.83 0.74 0.83 0.74 0.84 0.768 0.83 0.74 0.83 0.73 0.83 0.73 0.84 0.76Table 5: Japanese topic detection resultsverted from the NTCIR-J1 data set originally created forJapanese text retrieval research.
The data has 24 cate-gories.
The testing set contains 10,000 documents dis-tributed unevenly between categories (with a minimum of56 and maximum of 2696 documents per category).
Thisimbalanced distribution causes some difficulty since weassumed a uniform prior over categories.
Although this iseasily remedied, we did not fix the problem here.
Never-theless, we obtain experimental results in Table 5 that stillshow an 84% accuracy rate on this problem (for 6-gramor higher order models).
This is the same level of per-formance as that reported in (Aizawa, 2001), which usesan SVM approach with word segmentation, morphologyanalysis and feature selection.5 AnalysisThe perplexity of a test document under a language modeldepends on several factors.
The two most influential fac-tors are the order, n, of the n-gram model and the smooth-ing technique used.
Different choices will result in differ-ent perplexities, which could influence the final decisionin using Eq.
(10).
We now experimentally assess the in-fluence of each of these factors below.5.1 Effects of n-Gram OrderThe order n is a key factor in n-gram language models.If n is too small then the model will not capture enoughcontext.
However, if n is too large then this will createsevere sparse data problems.
Both extremes result in alarger perplexity than the optimal context length.
Figures1 and 2 illustrate the influence of order n on classifica-tion performance and on language model quality in theprevious five experiments (all using absolute smoothing).Note that in this case the entropy (bits per character) isthe average entropy across all testing documents.
Fromthe curves, one can see that as the order increases, classi-fication accuracy increases and testing entropy decreases,presumably because the longer context better captures theregularities of the text.
However, at some point accu-1 2 3 4 5 6 7 80.20.30.40.50.60.70.80.9order n or n?gram modelOverall accuracyGreek authorship attributionGreek genre classificationEnglish Topic DetectionChinese Topic DetectionJapanese Topic DetectionFigure 1: Influence of the order n on the classificationperformance1 2 3 4 5 6 7 8 934567891011order n or n?gram modelEntropyGreek authorship attributionGreek genre classification  English Topic Detection     Chinese Topic Detection     Japanese Topic DetectionFigure 2: The entropy of different n-gram modelsracy begins to decrease and entropy begins to increaseas the sparse data problems begin to set in.
Interest-ingly, the effect is more pronounced in some experiments(Greek genre classification) but less so in other experi-ments (topic detection under any language).
The sensi-tivity in the Greek genre case could still be attributed tothe sparse data problem (the over-fitting problem in genreclassification could be more serious than the other prob-lems, as seen from the entropy curves).5.2 Effects of Smoothing TechniqueAnother key factor affecting the performance of a lan-guage model is the smoothing technique used.
Figures 3and 4 show the effects of smoothing techniques on clas-sification accuracy and testing entropy (Chinese topic de-tection and Japanese topic detection are not shown in thefigure to save space).Here we find that, in most cases, the smoothing tech-nique does not have a significant effect on text catego-rization accuracy, because of the small vocabulary size of1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 60.40.60.81 Greek authorship attributionOverall Accuracy1 2 3 4 5 6 70.20.40.60.81 Greek Genre ClasificationOverall Accuracy1 2 3 4 5 6 7 8 90.20.40.60.81 English Topic DetectionOverall Accuracyorder n of n?gram modelsAbsoluteGood?TuringLinearWitten?BellFigure 3: Influence of smoothing on accuracy1 1.5 2 2.5 3 3.5 4 4.5 5 5.5 633.544.55 Greek authorship attributionEntropy1 2 3 4 5 6 73.544.555.5 Greek Genre ClasificationEntropy1 2 3 4 5 6 7 8 933.544.55 English Topic DetectionEntropyorder n of n?gram modelsAbsolute   Good?TuringLinear     Witten?BellFigure 4: The entropy of different smoothingcharacter level n-gram models.
However, there are twoexceptions?Greek authorship attribution and Greek textgenre classification?where Good-Turing smoothing isnot as effective as other techniques, even though it givesbetter test entropy than some others.
Since our goal is tomake a final decision based on the ranking of perplexi-ties, not just their absolute values, a superior smoothingmethod in the sense of perplexity reduction (i.e.
fromthe perspective of classical language modeling) does notnecessarily lead to a better decision from the perspec-tive of categorization accuracy.
In fact, in all our exper-iments we have found that it is Witten-Bell smoothing,not Good-Turing smoothing, that gives the best results interms of classification accuracy.
Our observation is con-sistent with previous research which reports that Witten-Bell smoothing achieves benchmark performance in char-acter level text compression (Bell et al, 1990).
For themost part, however, one can use any standard smooth-ing technique in these problems and obtain comparableperformance, since the rankings they produce are almostalways the same.5.3 Relation to Previous ResearchIn principle, any language model can be used to performtext categorization based on Eq.
(10).
However, n-grammodels are extremely simple and have been found to beeffective in many applications.
For example, characterlevel n-gram language models can be easily applied toany language, and even non-language sequences such asDNA and music.
Character level n-gram models arewidely used in text compression?e.g., the PPM model(Bell et al, 1990)?and have recently been found to beeffective in text classification problems as well (Teahanand Harper, 2001).
The PPM model is a weighted lin-ear interpolation n-gram models and has been set as abenchmark in text compression for decades.
Building anadaptive PPM model is expensive however (Bell et al,1990), and our back-off models are relatively much sim-pler.
Using compression techniques for text categoriza-tion has also been investigated in (Benedetto et al, 2002),where the authors seek a model that yields the minimumcompression rate increase when a new test document isintroduced.
However, this method is found not to be gen-erally effective nor efficient (Goodman, 2002).
In our ap-proach, we evaluate the perplexity (or entropy) directlyon test documents, and find the outcome to be both effec-tive and efficient.Many previous researchers have realized the impor-tance of n-gram models in designing language indepen-dent text categorization systems (Cavnar and Trenkle,1994; Damashek, 1995).
However, they have used n-grams as features for a traditional feature selection pro-cess, and then deployed classifiers based on calculatingfeature-vector similarities.
Feature selection in such aclassical approach is critical, and many required proce-dures, such as stop word removal, are actually languagedependent.
In our approach, all n-grams are consideredas features and their importance is implicitly weighted bytheir contribution to perplexity.
Thus we avoid an errorprone preliminary feature selection step.6 ConclusionWe have presented an extremely simple approach for lan-guage and task independent text categorization based oncharacter level n-gram language modeling.
The approachis evaluated on four different languages and four differ-ent text categorization problems.
Surprisingly, we ob-serve state of the art or better performance in each case.We have also experimentally analyzed the influence oftwo factors that can affect the accuracy of this approach,and found that for the most part the results are robustto perturbations of the basic method.
The wide appli-cability and simplicity of this approach makes it imme-diately applicable to any sequential data (such as natu-ral language, music, DNA) and yields effective baselineperformance.
We are currently investigating more chal-lenging problems like multiple category classification us-ing the Reuters-21578 data set (Lewis, 1992) and subjec-tive sentiment classification (Turney, 2002).
To us, theseresults suggest that basic statistical language modelingideas might be more relevant to other areas of natural lan-guage processing than commonly perceived.7 AcknowledgmentsResearch supported by Bell University Labs and MI-TACS.ReferencesA.
Aizawa.
2001.
Linguistic Techniques to Improvethe Performance of Automatic Text Categorization.
InProceedings of the Sixth Natural Language ProcessingPacific Rim Symposium (NLPRS2001).T.
Bell, J. Cleary, and I. Witten.
1990.
Text Compression.Prentice Hall.D.
Benedetto, E. Caglioti, and V. Loreto.
2002.
Lan-guage Trees and Zipping.
Physical Review Letters, 88.W.
Cavnar, J. Trenkle.
1994.
N-Gram-Based TextCategorization.
Proceedings of 3rd Annual Sympo-sium on Document Analysis and Information Retrieval(SDAIR-94).S.
Chen and J. Goodman.
1998.
An Empirical Study ofSmoothing Techniques for Language Modeling.
Tech-nical report, TR-10-98, Harvard University.M.
Damashek.
1995.
Gauging Similarity with N-Grams:Language-Independent Categorization of Text?.
Sci-ence, Vol.
267, 10 February, 843 - 848S.
Dumais, J. Platt, D. Heckerman and M. Sahami.
1998.Inductive Learning Algorithms And RepresentationsFor Text Categorization.
In Proceedings of ACM Con-ference on Information and Knowledge Management(CIKM98), Nov. 1998, pp.
148-155.J.
Goodman.
2002.
Comment on Language Trees andZipping.
Unpublished Manuscript.J.
He, A. Tan, and C. Tan.
2000.
A Comparative Study onChinese Text Categorization Methods.
In Proceedingsof PRICAI?2000 International Workshop on Text andWeb Mining, p24-35.D.
Holmes, and R. Forsyth.
1995.
The Federalist Revis-ited: New Directions in Authorship Attribution.
Liter-ary and Linguistic Computing, 10, 111-127.B.
Kessler, G. Nunberg and H. Schu?ze.
1997.
AutomaticDetection of Text Genre.
Proceedings of the Thirty-Fifth Annual Meeting of the Association for Computa-tional Linguistics (ACL1997).Y.
Lee and S. Myaeng.
2002.
Text Genre Classifica-tion with Genre-Revealing and Subject-Revealing Fea-tures.
Proceedings of ACM SIGIR Conference on Re-search and Development in Information Retrieval (SI-GIR2002).D.
Lewis.
1992.
Representation and Learning in Infor-mation Retrieval Phd thesis, Computer Science Dept-ment, Univ.
of Massachusetts.A.
McCallum and K. Nigam.
1998.
A Comparisonof Event Models for Naive Bayes Text Classification.Proceedings of AAAI-98 Workshop on ?Learning forText Categorization?, AAAI Presss.J.
Rennie.
2001.
Improving Multi-class Text Classifi-cation with Naive Bayes.
Master?s Thesis.
M.I.T.
AITechnical Report AITR-2001-004.
2001.S.
Scott and S. Matwin.
1999.
Feature Engineering forText Classification.
In Proceedings of the Sixteenth In-ternational Conference on Machine Learning (ICML?99), pp.
379-388.F.
Sebastiani.
2002.
Machine learning in automated textcategorization.
ACM Computing Surveys, 34(1):1-47.E.
Stamatatos, N. Fakotakis and G. Kokkinakis.
2000.Automatic Text Categorization in Terms of Genre andAuthor.
Computational Linguistics, 26 (4), 471-495.W.
Teahan and D. Harper.
2001.
Using Compression-Based Language Models for Text Categorization.
Pro-ceedings of 2001 Workshop on Language Modelingand Information Retrieval.P.
Turney.
2002.
Thumbs Up or Thumbs Down?
Seman-tic Oritentation Applied to Unsupervised Classificationof Reviews.
Proceedings of 40th Annual Conference ofAssociation for Computational Linguistics (ACL 2002)Y. Yang.
1999.
An Evaluation of Statistical Approachesto Text Categorization.
Information Retrieval, 1(1/2),pp.
67?88.
