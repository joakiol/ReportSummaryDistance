Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 746?755,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsIs Your Anchor Going Up or Down?Fast and Accurate Supervised Topic ModelsThang NguyeniSchool and UMIACSUniversity of Marylandand National Library of Medicine,National Institutes of Healthdaithang@umiacs.umd.eduJordan Boyd-GraberComputer ScienceUniversity of Colorado BoulderJordan.Boyd.Graber@colorado.eduJeff Lund,Kevin Seppi, Eric RinggerComputer ScienceBrigham Young University{jefflund,kseppi}@byu.eduringger@cs.byu.eduAbstractTopic models provide insights into documentcollections, and their supervised extensionsalso capture associated document-level meta-data such as sentiment.
However, inferringsuch models from data is often slow and cannotscale to big data.
We build upon the ?anchor?method for learning topic models to capture therelationship between metadata and latent top-ics by extending the vector-space representa-tion of word-cooccurrence to include metadata-specific dimensions.
These additional dimen-sions reveal new anchor words that reflect spe-cific combinations of metadata and topic.
Weshow that these new latent representations pre-dict sentiment as accurately as supervised topicmodels, and we find these representations morequickly without sacrificing interpretability.Topic models were introduced in an unsupervisedsetting (Blei et al, 2003), aiding in the discovery oftopical structure in text: large corpora can be dis-tilled into human-interpretable themes that facilitatequick understanding.
In addition to illuminating doc-ument collections for humans, topic models haveincreasingly been used for automatic downstreamapplications such as sentiment analysis (Titov andMcDonald, 2008; Paul and Girju, 2010; Nguyen etal., 2013).Unfortunately, the structure discovered by unsuper-vised topic models does not necessarily constitute thebest set of features for tasks such as sentiment analy-sis.
Consider a topic model trained on Amazon prod-uct reviews.
A topic model might discover a topicabout vampire romance.
However, we often want togo deeper, discovering facets of a topic that reflecttopic-specific sentiment, e.g., ?buffy?
and ?spike?
forpositive sentiment vs. ?twilight?
and ?cullen?
fornegative sentiment.
Techniques for discovering suchassociations, called supervised topic models (Sec-tion 2), both produce interpretable topics and predictmetadata values.
While unsupervised topic modelsnow have scalable inference strategies (Hoffman etal., 2013; Zhai et al, 2012), supervised topic modelinference has not received as much attention and of-ten scales poorly.The anchor algorithm is a fast, scalable unsuper-vised approach for finding ?anchor words?
?precisewords with unique co-occurrence patterns that candefine the topics of a collection of documents.
Weaugment the anchor algorithm to find supervisedsentiment-specific anchor words (Section 3).
Ouralgorithm is faster and just as effective as traditionalschemes for supervised topic modeling (Section 4).1 Anchors: Speedy Unsupervised ModelsThe anchor algorithm (Arora et al, 2013) begins witha V ?
V matrix?Q of word co-occurrences, where Vis the size of the vocabulary.
Each word type definesa vector?Qi,?of length V so that?Qi,jencodes the con-ditional probability of seeing word j given that wordi has already been seen.
Spectral methods (Anand-kumar et al, 2012) and the anchor algorithm arefast alternatives to traditional topic model inferenceschemes because they can discover topics via thesesummary statistics (quadratic in the number of types)rather than examining the whole dataset (proportionalto the much larger number of tokens).The anchor algorithm takes its name from the idea746of anchor words?words which unambiguously iden-tify a particular topic.
For instance, ?wicket?
mightbe an anchor word for the cricket topic.
Thus, for anyanchor word a,?Qa,?will look like a topic distribu-tion.
?Qwicket,?will have high probability for ?bowl?,?century?, ?pitch?, and ?bat?
; these words are relatedto cricket, but they cannot be anchor words becausethey are also related to other topics.Because these other non-anchor words could betopically ambiguous, their co-occurrence must be ex-plained through some combination of anchor words;thus for non-anchor word i,?Qi,?=?gk?GCi,k?Qgk,?, (1)where G = {g1, g2, .
.
.
, gK} is the set of K anchorwords.
The coefficients Ci,kof this linear combina-tion correspond to the probability of seeing a topicgiven a word, from which we can recover the proba-bility of a word given a topic (represented in a matrixA) using Bayes?
rule.
In our experiments, we followArora et al (2013) to first estimate?Q based on thetraining data and then recover the C matrixC?i,?= argminCi,?DKL(?Qi,?||?gk?GCi,k?Qgk,?
),where DKL(x, y) denotes the Kullback-Leibler di-vergence between x and y.In addition to discovering topics from a given set ofanchor words as described above, Arora et al (2013)also provide a geometric interpretation of a processfor finding the needed anchor words.
If we viewthe rows of?Q as points in a high-dimensional space,the convex hull of those points provides the anchorwords.1Equation 1 linearly combines anchor words?
co-occurrence vectors?Qgk,?to create the representationof other words.
The convex hull corresponds to theperimeter of the space of all possible co-occurrencevectors that can be formed from the set of basis an-chor vectors.
However, the convex hull only encodes1As discussed by Arora et al (2013), this is a slight simpli-fication, since the most extreme points will be words that onlyappear infrequently.
Thus, there is some nuance to choosingthe anchor words.
For instance, a key step for effective topicmodeling is choosing a minimum number of documents a wordmust appear in before it can be considered an anchor word.
(c.f.Figure 3).lemonToyotawonderfuliPadfleece authorheelauthorantilockcozyfleeceheelawfulToyotaiPadFigure 1: Graphical intuition behind supervised anchorwords.
Anchor words (in gold) form the convex hull ofword co-occurrence probabilities in unsupervised topicmodeling (top).
Adding an additional dimension to cap-ture metadata, such as sentiment, changes the convex hull:positive words appear above the original 2D plane (under-lined) and negative words appear below (in outline).an unsupervised view of the data.
To capture topicsinformed by metadata such as sentiment, we needto explicitly represent the combination of words andmetadata.One problem inherited by the anchor method fromparametric topic models is the determination of thenumber of anchor words (and thus topics) to use.Because word co-occurrence statistics live in an ex-tremely high-dimensional space, the number of an-chor words needed to cover all of the data will bequite high.
Thus, Arora et al (2013) require a userto specify the number of anchor words a priori (justas for parametric topic models).
They use a form ofthe Gram-Schmidt process to find the best words thatenclose the maximum volume of points.747?Q ?New column(s) encodingword-sentiment relationshipp(w1|w1) .
.
....p(wj|wi)p(w1|w1) .
.
....p(wj|wi)S ?p(y(l)|w1)...p(y(l)|wi)Figure 2: We form a new column to capture the relation-ship between words and each sentiment level: per entry isthe conditional probability of observing a sentiment levely(l)given an observation of the word wi.
Adding all ofthese columns to?Q to form an augmented matrix S.2 Supervised Topics: Effective but SlowTopic models discover a set of topics A.
Each topicis a distribution over the V word types in the cor-pus.
Ai,tis the probability of seeing word i in topict.
Supervised topic models relate those topics withpredictions of document metadata such as sentimentby discovering a vector of regression parameters ~?that connects topics to per-document observationsyd(Blei and McAuliffe, 2007).
Blei and McAuliffe(2007) treat this as a regression: seeing one wordwith topic k in document d means that prediction ofydshould be adjusted by ?k.
Given a document?s dis-tribution over topics z?d, the response ydis normallydistributed with mean ~?>z?d.2Typically, the topics are discovered through aprocess of probabilistic inference, either variationalEM (Wang et al, 2009) or Gibbs sampling (Boyd-Graber and Resnik, 2010).
However, these meth-ods scale poorly to large datasets.
Variational infer-ence requires dozens of expensive passes over theentire dataset, and Gibbs sampling requires multipleMarkov chains (Nguyen et al, 2014b).2We are eliding some details in the interest of a more compactpresentation.
The topics used by a document, z?d, are basedon per-token inference of topic assignments; this detail is notrelevant to our contribution, and in Section 4.2 we use existingtechniques to discover documents?
topics.3 Supervised Anchor WordsBecause the anchor algorithm scales so well com-pared to traditional probabilistic inference, we nowunify the supervised topic models of Section 2 withthe anchor algorithm discussed in Section 1.
We doso by augmenting the matrix?Q with an additionaldimension for each metadata attribute, such as senti-ment.
We provide the geometric intuition in Figure 1.Picture the anchor words projected down to twodimensions (Lee and Mimno, 2014): each word is apoint, and the anchor words are the vertices of a poly-gon encompassing every point.
Every non-anchorword can be approximated by a convex combinationof the anchor words (Figure 1, top).Now add an additional dimension as a column to?Q (Figure 2).
This column encodes the metadataspecific to a word.
For example, we have encodedsentiment metadata in a new dimension (Figure 1,bottom).
Neutral sentiment words will stay in theplane inhabited by the other words, positive senti-ment words will move up, and negative sentimentwords will move down.
For simplicity, we only showa single additional dimension, but in general we canadd as many dimensions as needed to encode themetadata.In this new space some of the original anchorwords may still be anchor words (?author?).
Otherwords that were near the convex hull boundary inthe unaugmented representation may become an-chor words in the augmented representation becausethey capture both topic and sentiment (?anti-lock?
vs.?lemon?).
Finally, extreme sentiment words mightbecome anchor words in the new higher-dimensionalspace because they are so important for explainingextreme sentiment values (?wonderful?
vs.
?awful?
).3.1 Words to SentimentHaving explained how a word is connected to senti-ment, we now elaborates on how to model that con-nection using the conditional probability of senti-ment given a particular word.
Assume that sentimentis discretized into a finite set of L sentiment levels{y(1), y(2), .
.
.
, y(L)} and that each document is as-signed to one of these levels.
We define a matrixS of size V ?
(V + L).
The first V columns arethe same as?Q and the L additional columns capturethe relationship of a word to each discrete sentiment748level.For each additional column l, Si,(V +l)?
p(y =y(l)|w = i) is the conditional probability of observ-ing a sentiment level y(l)given an observation ofword i.
We compute the conditional probability of asentiment level y(l)given word iSi,(V +l)?
?d(1 [i ?
d] ?
1[yd= y(l)])?d1 [i ?
d], (2)where the numerator is the number of documentsthat contain word type i and have sentiment levely(l)and the denominator is the number of documentscontaining word i.Given this augmented matrix, we again want tofind the set of anchor words G and coefficients Ci,kthat best capture the relationship between words andsentiment (c.f.
Equation 1)Si,?=?gk?GCi,kSgk,?.
(3)Because we retain the property that non-anchorwords are explained through a linear combinationof the anchor words, our method retains the sametheoretical guarantees of sampling complexity androbustness as the original anchor algorithm.To facilitate direct comparisons, we keep the num-ber of anchor words fixed in our experiments.
Evenso, the introduction of metadata forces the anchormethod to select the words that best capture thismetadata-augmented view of the data.
Consequently,some of the original anchor words will remain, andsome will be replaced by sentiment-specific anchorwords.4 Quantitative Comparison of SupervisedTopic ModelsIn this section, we evaluate the effectiveness of ournew method on a binary sentiment classificationproblem.
Because the supervised anchor algorithm(SUP ANCHOR) finds anchor words (and thus differ-ent topics) which capture the sentiment metadata, weevaluate the degree to which its latent representationimproves upon the original unsupervised anchor algo-rithm (Arora et al, 2013, ANCHOR) for classificationin terms of both accuracy and speed.4.1 Sentiment DatasetsWe use three common sentiment datasets for eval-uation: AMAZON product reviews (Jindal and Liu,2008), YELP restaurant reviews (Jo and Oh, 2011),and TRIPADVISOR hotel reviews (Wang et al, 2010).For each dataset, we preprocess by tokenizing andremoving all non-alphanumeric words and stopwords.As very short reviews are often inscrutable and lackcues to connect to the sentiment, we only considerdocuments with at least thirty words.
We also re-duce the vocabulary size by keeping only words thatappear in a sufficient number of documents: 50 forAMAZON and YELP datasets, and 150 for TRIPADVI-SOR (Table 1).4.2 Documents to LabelsOur goal is to perform binary classification of sen-timent.
Due to a positive skew of the datasets, themedian for all datasets is four out of five.
All 5-starreviews are assigned to y+and the rest of the reviewsare assigned to y?.
Table 1 summarizes the composi-tion of each dataset and the percentage of documentswith high positive sentiment.3We compare the effectiveness of different repre-sentations in predicting high-sentiment documents:unsupervised topic models (LDA), traditional su-pervised topic models (SLDA), the unmodified an-chor algorithm (ANCHOR), our supervised anchoralgorithm (SUP ANCHOR), and a traditional TF-IDF (Salton, 1968, TF-IDF) representation of thewords.The anchor algorithm only provides the topic dis-tribution over words; it does not provide the per-document assignment of topics needed to representthe document in a low-dimensional space necessaryfor producing a prediction yd.
Fortunately, this re-quires a very quick?because the topics are fixed?pass over the documents using a traditional topicmodel inference algorithm.
We use the variational in-ference implementation for LDA of Blei et al (2003)4to obtain z?d, the topic distribution for document d.53Multiclass labeling for each sentiment label also works well,but binary classification simplifies the analysis and presentation.4http://www.cs.princeton.edu/?blei/lda-c/5For other inference schemes, we use native inference to ap-ply pre-trained topics to extract DEV and TEST topic proportions.749Corpus Train Documents Test Documents Tokens Types Percentage with Positive SentimentAMAZON 13,300 3,314 1,031,659 2,662 52.2%TRIPADVISOR 115,384 28,828 12,752,444 4,867 41.5%YELP 13,955 3,482 1,142,555 2,585 27.7%Table 1: Statistics for the datasets employed in the experiments.Classifiers Given a low-dimensional representa-tion of a test document, we predict the document?ssentiment yd.
We have already inferred the topic dis-tribution z?dfor each document, and we use log(z?d)as the features for a classifier.
Feature vectors fromtraining data are used to train the classifiers, and fea-ture vectors from the development or test set are usedto evaluate the classifiers.We run three standard machine learning classi-fiers: decision trees (Quinlan, 1986), logistic regres-sion (Friedman et al, 1998), and a discriminativeclassifier.
For decision trees (hence TREE) and logis-tic regression (hence LOGISTIC), we use SKLEARN.6For the discriminative classifier, we use a linear clas-sifier with hinge loss (hence HINGE) in Vowpal Wab-bit.7Because HINGE outputs a regression value in[0, 1], we use a threshold 0.5 to make predictions.Parameter Tuning Parameter tuning is importantin topic models, so we cross-validate: each sentimentdataset is split randomly into five folds.
We usedfour folds to form the TRAIN set and reserved thelast fold for the TEST set.
All cross-validation resultsare averaged over the four held out DEV sets; thebest cross-validation result provides the parametersettings we use on the TEST set.For ANCHOR and SUP ANCHOR, the parame-ter for the document-level Dirichlet prior ?
is re-quired for inferring document-topic distributionsgiven learned topics.
Despite selecting this parameterusing grid search, ?
does not affect our final results.The same is also true for SLDA: its predictive perfor-mance does not significantly vary as ?
varies, givena fixed number of topics K.8Anchor algorithms are sensitive to the value of an-chor thresholdM (the minimum document frequencyfor a word to be considered an anchor word).
For6http://scikit-learn.org/stable/7http://hunch.net/?vw/8We use the SLDA implementation by Chong Wang: http://www.cs.cmu.edu/?chongw/slda/ to estimate ?.AMAZON TRIPADVISOR YELPl ll l l ll l l l l ll l l l l ll l l l l ll l llll ll ll ll l llll llll ll l llll ll ll ll l llll l ll ll l l l l ll l l l l ll l l l l ll l l l l l0.650.700.750.700.720.740.760.780.720.740.760.780.710.730.750.7720406080100 400 100 400 2400 100 400MAccuracyFigure 3: Grid search for selecting the word-documentthreshold M for SUP ANCHOR based on development setaccuracy.each number of topics K, we perform a grid searchto find the best value of M .
Figure 3 shows theperformance trends.For LDA, we use the Gibbs sampling implemen-tation in Mallet.9For training the model, we runLDA with 5,000 iterations; and for inference (on DEVand TEST) of document topic distribution we iterate100 times, with lag 5 and 50 burn-in iterations.
AsMallet accepts?
?ias a parameter, we always ini-tialize?
?i= 1 and only perform a grid search overdifferent values of ?, the hyper-parameter for Dirich-let prior over the per-topic topic-word distribution,starting from 0.01 and doubling until reaching 0.5.4.3 SUP ANCHOR Outperforms ANCHORLearning topics that jointly reflect words and meta-data improves subsequent prediction.
The results forboth SUP ANCHOR and ANCHOR on the TEST setare shown in Figure 4.
SUP ANCHOR outperformsANCHOR on all datasets.
This trend holds consis-tently for LOGISTIC, TREE, and HINGE methods forsentiment prediction.
For example, with twenty top-ics on the AMAZON dataset, SUP ANCHOR gives an9http://mallet.cs.umass.edu/topics.php750l ll lll l lll l l0.650.700.740.750.760.770.780.790.740.750.760.770.78AMAZONTRIPADVISORYELP20 40 60 80Number Of TopicsAccuracymethod l ANCHOR LDA SLDA SUP.
ANCHORFigure 4: Results on TEST fold, SUP ANCHOR outper-forms ANCHOR, LDA, and SLDA on all three datasets.
Wereport the results based on LOGISTIC as it produces thebest accuracy consistently for ANCHOR, SUP ANCHOR,and LDA.accuracy of 0.71 in comparison to only 0.62 fromANCHOR.
Similarly, with twenty topics on the YELPdataset, SUP ANCHOR has 0.77 accuracy while AN-CHOR has 0.74.
Our SUP ANCHOR model is able toincorporate metadata to learn better representationsfor predicting sentiment.
Moreover, in Section 5 weshow that SUP ANCHOR does not need to sacrificetopic quality to gain predictive power.4.4 SUP ANCHOR Outperforms SLDAMore surprising is that SUP ANCHOR also outper-forms SLDA.
Like SUP ANCHOR, SLDA jointlylearns topics and their relation to metadata such assentiment.
Figure 4 shows that this trend is consistenton all sentiment datasets.
On average, SUP ANCHORis 2.2 percent better than SLDA on AMAZON, and 2.0percent better on both YELP and TRIPADVISOR.
Fur-thermore, SUP ANCHOR is much faster than SLDA.SLDA performs worse than SUP ANCHOR in partbecause SUP ANCHOR is able to jointly find specificlexical terms that improve prediction.
Nguyen et al(2013) show that this improves supervised topic mod-els; forming anchor words around the same stronglexical cues could discover better topics.
In con-trast, SLDA must discover the relationship throughl l l l l l l l l ll0.600.650.700.75 AMAZON0 25 50 75 100Interpolation PercentageAccuracyclassifier l HINGE LOGISTIC TREEFigure 5: Accuracy on AMAZON with twenty topics.SUP ANCHOR produces good representations for senti-ment classification that can be improved by interpolatingwith lexical TF-IDF features.
The interpolation (x-axis)ranges from zero (all TF-IDF features) to one hundred (allSUP ANCHOR topic features).the proxy of topics.4.5 Lexical FeaturesRamage et al (2010) show that interpolating topicand lexical features often provides better classifica-tion than either alone.
Here, we take the same ap-proach and show how different interpolations of topicand lexical features create better classifiers.
We firstselect an interpolation value ?
in {0, 0.1, 0.2, .
.
.
, 1},and we then form a new feature vector by concatenat-ing ?-weighted topic features with (1?
?
)-weightedlexical features.
Figure 5 shows the interplay be-tween topic features and TF-IDF features10as theweight of topic features increases from zero (all TF-IDF) to one hundred (all SUP ANCHOR topic fea-tures) percent on the AMAZON dataset (other datasetsare similar).
Combining both feature sets is betterthan either alone, although the interpolation dependson the classifier.4.6 Runtime AnalysisHaving shown that SUP ANCHOR outperforms bothANCHOR and SLDA, in this section we show thatSUP ANCHOR also inherits the runtime efficiencyfrom ANCHOR.
Table 2 summarizes the runtimeson both AMAZON and TRIPADVISOR; these resultswere obtained using a six-core 2.8GHz Intel XeonX5660.
On the small dataset AMAZON, SUP AN-CHOR finishes the training within one minute, andfor the larger TRIPADVISOR dataset it completes the10As before, we do parameter selection on DEV data and reportfinal TEST results.751Dataset Measure SUP ANCHOR LDA SLDAAMAZONPreprocessing 32 32 32Generating?Q/S 29Training 33 886 4,762LDAC inference 38 (train), 13 (dev/test)Classification <5 <5TRIPADVISORPreprocessing 305 305 305Generating?Q/S 262Training 181 8,158 71,967LDAC inference 830 (train), 280 (dev/test)Classification <5 <5Table 2: Runtime statistics (in seconds) for the AMAZON and TRIPADVISOR datasets.
Blank cells indicate a timingwhich does not apply to a particular model.
SUP ANCHOR is significantly faster than conventional methods.l l l ll l l ll l l l0.0450.0500.0550.050.060.090.110.13AMAZONTRIPADVISORYELP20 40 60 80Number Of TopicsTopic Interpretabilitymethod l ANCHOR LDA SLDA SUP.
ANCHORFigure 6: SUP ANCHOR and ANCHOR produce the sametopic quality.
LDA outperforms all other models and pro-duces the best topics.
Performance of SLDA degradessignificantly as the number of topic increases.learning in around three minutes.
The main bottle-neck for SUP ANCHOR is learning the document dis-tributions over topics, although even this stage is fastfor known topic distributions.
This result is far betterthan the twenty hours required by SLDA to train onTRIPADVISOR.5 Inspecting Anchors and their TopicsOne important evaluation for topic models is howeasy it is for a human reader to understand the top-ics.
In this section, we evaluate topics produced byeach model using topic interpretability (Chang et al,2009).
Topic interpretability measures how humanusers understand topics presented by a topic model-ing algorithm.
We use an automated approximationof interpretability that uses a reference corpus as aproxy for which words belong together (Newmanet al, 2010).
Using half a million documents fromWikipedia, we compute the induced normalized pair-wise mutual information (Lau et al, 2014, NPMI)on the top ten words in topics as a proxy for inter-pretability.Figure 6 shows the NPMI scores for each model.Unsurprisingly, unsupervised models (LDA) producethe best topic quality.
In contrast, supervised mod-els must balance metadata (i.e., response variable)prediction against capturing word meaning.
Conse-quently, SLDA does slightly worse with respect totopic interpretability.SUP ANCHOR and ANCHOR produce the sametopic quality consistently on all datasets.
SinceSUP ANCHOR and ANCHOR have nearly identicalruntime, SUP ANCHOR is better suited for supervisedtasks because it improves classification without sac-rificing interpretability.
It is possible that regulariza-tion would improve the interpretability of these top-ics; Nguyen et al (2014a) show that adding regular-ization removes overly frequent words from anchor-discovered topics.The topics produced by the ANCHOR and SUP AN-CHOR algorithms have many similarities.
In Table 3,nearly all of the anchor words discovered by AN-CHOR are also used by SUP ANCHOR.
These anchorwords tend to describe general food types, such as752Model Anchor Words and Top Words in TopicsANCHOR and SUP ANCHOR pizza burger sushi ice garlic hot amp chicken pork french sand-wich coffee cake steak beer fishwine wine restaurant dinner menu nice night bar table meal experienceANCHOR hour wait hour people minutes line long table waiting worth orderlate night late ive people pretty love youre friends restaurant openfavorite love favorite ive amazing delicious restaurant eat menu fresh awesomeSUP ANCHOR decent pretty didnt restaurant ordered decent wasnt nice night bad starsline line wait people long tacos worth order waiting minutes tacoTable 3: Comparing topics generated for the YELP dataset: anchor words shared by both ANCHOR and SUP ANCHORare listed.
Unique anchor words for each algorithm are listed along with the top ten words for that topic.
For clarity,we pruned words which appear in more than 3000 documents as these words appear in every topic.
The distinctanchor words reflect positive (?favorite?)
and negative (?line?)
sentiment rather than less sentiment-specific qualities ofrestaurants (e.g., restaurants open ?late?).?pizza?
or ?burger?, and characterize the YELP datasetwell.
The similarity of these shared topics explainswhy both ANCHOR and SUP ANCHOR achieve simi-lar topic interpretability scores.To explain the predictive power of SUP ANCHORwe must examine the anchor words and topics uniqueto both algorithms.
The anchor words which areunique to ANCHOR include a general topic aboutwine, and two somewhat coherent topics related totime.
By adding supervision to the model we getthree new anchor words which identify sentimentranging from extremely positive reviews mentioninga favorite restaurant to extremely negative reviewscomplaining about long waits.This general trend is seen across each of thedatasets.
For example, ANCHOR and SUP ANCHORboth discover shared topics describing consumergoods, but SUP ANCHOR replaces two topics dis-cussing headphones with topics describing ?frustrat-ing?
products and ?great?
products.
Similarly, inthe TRIPADVISOR data, both ANCHOR and SUP AN-CHOR share topics about specific destinations, butonly SUP ANCHOR discovers a topic describing ?dis-gusting?
hotel rooms.6 Related WorkImproving the scalability of statistical learning hastaken many forms: creating online approximationsof large batch algorithms (Hoffman et al, 2013; Zhaiet al, 2014) or improving the efficiency of sam-pling (Yao et al, 2009; Hu and Boyd-Graber, 2012;Li et al, 2014).These insights have also improved supervisedtopic models.
For example, Zhu et al (2013) train themax-margin supervised topic models MEDLDA (Zhuet al, 2009) by reformulating the model such that thehinge loss is included inside a collapsed Gibbs sam-pler, rather than being applied externally on the sam-pler using costly SVMs.
Using insights from Smolaand Narayanamurthy (2010), the samplers run in par-allel to train the model.
While these advancementsimprove the scalability of max-margin supervisedtopic models, the improvement is limited by the factthat the sampling algorithm grows with the numberof tokens.In contrast, this paper explores a different veinof research that focuses on using efficient represen-tations of summary statistics to estimate statisticalmodels.
While this has seen great success in unsu-pervised models (Cohen and Collins, 2014), it hasincreasingly also been applied to supervised mod-els.
Wang and Zhu (2014) show how to use tensordecomposition to estimate the parameters of SLDAinstead of sampling to find maximum likelihood es-timates.
In contrast, anchor-based methods rely onnon-negative matrix factorization.We found that a discriminative classifier did notalways perform best on the downstream classificationtask.
Zhu et al (2009) make a comprehensive com-parison between MEDLDA, SLDA, and SVM+LDA,and they show that SVM+LDA performs worse thanMEDLDA and SLDA on binary classification.
It couldbe that better feature preprocessing could improveour performance.753Bag-of-words representations are not ideal for sen-timent tasks.
Rubin et al (2012) introduce Depen-dency LDA which associates individual word tokenswith different labels; their model also outperformslinear SVMs on a very large multi-labeled corpus.Latent variable models that consider grammaticalstructure (Sayeed et al, 2012; Socher et al, 2011;Iyyer et al, 2014) could also be improved throughefficient inference (Cohen and Collins, 2014).7 DiscussionSupervised anchor word topic modeling providesa general framework for learning better topic rep-resentations by taking advantage of both word-cooccurrence and metadata.
Our straightforward ex-tension (Equation 2) places each word in a vectorspace that not only captures co-occurrence with otherterms but also the interaction of the word and its sen-timent, in contrast to algorithms that only considerraw words.While our experiments focus on binary classifica-tion, the same extension is also applicable to multi-class classification.Moreover, supervised anchor word topic model-ing is fast: it inherits the polynomial-time efficiencyfrom the original unsupervised anchor word algo-rithm.
It is also effective: it is better at providingfeatures for classification than unsupervised topicmodels and also better than supervised topic modelswith conventional inference.Our supervised anchor word algorithm offers theability to quickly analyze datasets without the over-head of Gibbs sampling or variational inference, al-lowing users to more quickly understand big dataand to make decisions.
Combining bag-of-wordsanalysis with metadata through efficient, low-latencytopic analysis allows users to have deep insights morequickly.Acknowledgments We thank Daniel Petersen, JimMartin, and the anonymous reviewers for their in-sightful comments.
This work was supported by thecollaborative NSF Grant IIS-1409287 (UMD) and IIS-1409739 (BYU).
Boyd-Graber is also supported byNSF grants IIS-1320538 and NCSE-1422492.ReferencesAnima Anandkumar, Dean P. Foster, Daniel Hsu, ShamKakade, and Yi-Kai Liu.
2012.
A spectral algorithmfor latent Dirichlet alocation.
In Proceedings of Ad-vances in Neural Information Processing Systems.Sanjeev Arora, Rong Ge, Yoni Halpern, David M. Mimno,Ankur Moitra, David Sontag, Yichen Wu, and MichaelZhu.
2013.
A practical algorithm for topic model-ing with provable guarantees.
In Proceedings of theInternational Conference of Machine Learning.David M. Blei and Jon D. McAuliffe.
2007.
Supervisedtopic models.
In Proceedings of Advances in NeuralInformation Processing Systems.David M. Blei, Andrew Ng, and Michael Jordan.
2003.Latent Dirichlet alocation.
Journal of Machine Learn-ing Research, 3.Jordan Boyd-Graber and Philip Resnik.
2010.
Holisticsentiment analysis across languages: Multilingual su-pervised latent Dirichlet alocation.
In Proceedings ofEmpirical Methods in Natural Language Processing.Jonathan Chang, Jordan Boyd-Graber, Chong Wang, SeanGerrish, and David M. Blei.
2009.
Reading tea leaves:How humans interpret topic models.
In Proceedings ofAdvances in Neural Information Processing Systems.Shay B. Cohen and Michael Collins.
2014.
A provablycorrect learning algorithm for latent-variable PCFGs.In Proceedings of the Association for ComputationalLinguistics.Jerome Friedman, Trevor Hastie, and Robert Tibshirani.1998.
Additive logistic regression: a statistical view ofboosting.
Annals of Statistics, 28:2000.Matthew Hoffman, David M. Blei, Chong Wang, and JohnPaisley.
2013.
Stochastic variational inference.
InJournal of Machine Learning Research.Yuening Hu and Jordan Boyd-Graber.
2012.
Efficienttree-based topic modeling.
In Proceedings of the Asso-ciation for Computational Linguistics.Mohit Iyyer, Peter Enns, Jordan Boyd-Graber, and PhilipResnik.
2014.
Political ideology detection using recur-sive neural networks.
In Proceedings of the Associationfor Computational Linguistics.Nitin Jindal and Bing Liu.
2008.
Opinion spam andanalysis.
In Proceedings of First ACM InternationalConference on Web Search and Data Mining.Yohan Jo and Alice H. Oh.
2011.
Aspect and senti-ment unification model for online review analysis.
InProceedings of ACM International Conference on WebSearch and Data Mining.Jey Han Lau, David Newman, and Timothy Baldwin.2014.
Machine reading tea leaves: Automatically eval-uating topic coherence and topic model quality.
InProceedings of the European Chapter of the Associa-tion for Computational Linguistics.754Moontae Lee and David Mimno.
2014.
Low-dimensionalembeddings for interpretable anchor-based topic infer-ence.
In Proceedings of Empirical Methods in NaturalLanguage Processing.Aaron Q Li, Amr Ahmed, Sujith Ravi, and Alexander JSmola.
2014.
Reducing the sampling complexityof topic models.
In Proceedings of the 20th ACMSIGKDD international conference on Knowledge dis-covery and data mining, pages 891?900.
ACM.David Newman, Jey Han Lau, Karl Grieser, and TimothyBaldwin.
2010.
Automatic evaluation of topic coher-ence.
In Conference of the North American Chapter ofthe Association for Computational Linguistics.Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.2013.
Lexical and hierarchical topic regression.
In Pro-ceedings of Advances in Neural Information ProcessingSystems.Thang Nguyen, Yuening Hu, and Jordan Boyd-Graber.2014a.
Anchors regularized: Adding robustness andextensibility to scalable topic-modeling algorithms.
InProceedings of the Association for Computational Lin-guistics.Viet-An Nguyen, Jordan Boyd-Graber, and Philip Resnik.2014b.
Sometimes average is best: The importanceof averaging for prediction using MCMC inference intopic modeling.
In Proceedings of Empirical Methodsin Natural Language Processing.Michael Paul and Roxana Girju.
2010.
A two-dimensional topic-aspect model for discovering multi-faceted topics.
In Association for the Advancement ofArtificial Intelligence.J.
R. Quinlan.
1986.
Induction of decision trees.
1(1):81?106, March.Daniel Ramage, Susan T. Dumais, and Daniel J. Liebling.2010.
Characterizing microblogs with topic models.In International Conference on Weblogs and SocialMedia.Timothy N. Rubin, America Chambers, Padhraic Smyth,and Mark Steyvers.
2012.
Statistical topic models formulti-label document classification.
Journal of Ma-chine Learning Research, 88(1-2):157?208, July.Gerard Salton.
1968.
Automatic Information Organiza-tion and Retrieval.
McGraw Hill Text.Asad B. Sayeed, Jordan Boyd-Graber, Bryan Rusk, andAmy Weinberg.
2012.
Grammatical structures forword-level sentiment detection.
In North AmericanAssociation of Computational Linguistics.Alexander Smola and Shravan Narayanamurthy.
2010.An architecture for parallel topic models.
InternationalConference on Very Large Databases, 3(1-2):703?710.Richard Socher, Jeffrey Pennington, Eric H. Huang, An-drew Y. Ng, and Christopher D. Manning.
2011.
Semi-supervised recursive autoencoders for predicting senti-ment distributions.
In Proceedings of Empirical Meth-ods in Natural Language Processing.Ivan Titov and Ryan McDonald.
2008.
A joint modelof text and aspect ratings for sentiment summarization.In Proceedings of the Association for ComputationalLinguistics.Yining Wang and Jun Zhu.
2014.
Spectral methods forsupervised topic models.
In Proceedings of Advancesin Neural Information Processing Systems.Chong Wang, David Blei, and Li Fei-Fei.
2009.
Simulta-neous image classification and annotation.
In ComputerVision and Pattern Recognition.Hongning Wang, Yue Lu, and Chengxiang Zhai.
2010.Latent aspect rating analysis on review text data: arating regression approach.
In Knowledge Discoveryand Data Mining.Limin Yao, David Mimno, and Andrew McCallum.
2009.Efficient methods for topic model inference on stream-ing document collections.
In Knowledge Discovery andData Mining.Ke Zhai, Jordan Boyd-Graber, Nima Asadi, and MohamadAlkhouja.
2012.
Mr. LDA: A flexible large scale topicmodeling package using variational inference in mapre-duce.
In Proceedings of World Wide Web Conference.Ke Zhai, Jordan Boyd-Graber, and Shay B. Cohen.
2014.Online adaptor grammars with hybrid inference.Jun Zhu, Amr Ahmed, and Eric P. Xing.
2009.
MedLDA:maximum margin supervised topic models for regres-sion and classification.
In Proceedings of the Interna-tional Conference of Machine Learning.Jun Zhu, Xun Zheng, Li Zhou, and Bo Zhang.
2013.Scalable inference in max-margin topic models.
InKnowledge Discovery and Data Mining.755
