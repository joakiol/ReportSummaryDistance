Dependency-Based Construction of SemanticSpace ModelsSebastian Pad?
?Saarland UniversityMirella Lapata?
?University of EdinburghTraditionally, vector-based semantic space models use word co-occurrence counts from largecorpora to represent lexical meaning.
In this article we present a novel framework for construct-ing semantic spaces that takes syntactic relations into account.
We introduce a formalization forthis class of models, which allows linguistic knowledge to guide the construction process.
Weevaluate our framework on a range of tasks relevant for cognitive science and natural languageprocessing: semantic priming, synonymy detection, and word sense disambiguation.
In all cases,our framework obtains results that are comparable or superior to the state of the art.1.
IntroductionVector space models of word co-occurrence have proved a useful framework for repre-senting lexical meaning in a variety of natural language processing (NLP) tasks, suchas word sense discrimination (Sch?tze 1998) and ranking (McCarthy et al 2004), textsegmentation (Choi, Wiemer-Hastings, and Moore 2001), contextual spelling correc-tion (Jones and Martin 1997), automatic thesaurus extraction (Grefenstette 1994; Lin1998a), and notably, information retrieval (Salton, Wang, and Yang 1975).
These modelshave also been popular in cognitive science and figure prominently in several studiessimulating human behavior.
Examples include similarity judgments (McDonald 2000),semantic priming (Lund and Burgess 1996; Landauer and Dumais 1997; Lowe andMcDonald 2000; McDonald and Brew 2004), and text comprehension (Landauer andDumais 1997; Foltz, Kintsch, and Landauer 1998).The popularity of vector-based models in both fields lies in their ability to repre-sent word meaning simply by using distributional statistics.
The central assumptionhere is that the context surrounding a given word provides important informationabout its meaning (Harris 1968).
The semantic properties of words are captured in amulti-dimensional space by vectors that are constructed from large bodies of text byobserving the distributional patterns of co-occurrence with their neighboring words.Co-occurrence information is typically collected in a frequency matrix, where each rowcorresponds to a unique word, commonly referred to as ?target word,?
and each columnrepresents a given linguistic context.
The semantic similarity between any two words?
Computational Linguistics, P.O.
Box 15 11 50, 66041 Saarbr?cken, Germany.
E-mail: pado@coli.uni-sb.de.??
School of Informatics, University of Edinburgh, EH8 9LW, Edinburgh, UK.
E-mail: mlap@inf.ed.ac.uk.Submission received: 20 December 2004; revised submission received: 26 September 2006;accepted for publication: 23 November 2006.?
2007 Association for Computational LinguisticsComputational Linguistics Volume 33, Number 2can then be quantified directly using a distance measure such as cosine or Euclideandistance.Contexts are defined as a small number of words surrounding the target word(Lund and Burgess 1996; Lowe and McDonald 2000) or as entire paragraphs?evendocuments (Salton, Wang, and Yang 1975; Landauer and Dumais 1997).
Latent SemanticAnalysis (LSA; Landauer and Dumais 1997) is an example of a document-based vectorspace model that is commonly used in information retrieval and cognitive science.
Eachtarget word t is represented by a k element vector of paragraphs p1...k and the valueof each vector element is a function of the number of times t occurs in pi.
In contrast,the Hyperspace Analogue to Language model (HAL; Lund and Burgess 1996) createsa word-based semantic space: each target word t is represented by a k element vector,whose dimensions correspond to context words c1...k. The value of each vector elementis a function of the number of times each ci occurs within a window of size n before orafter t in a large corpus.In their simplest incarnation, semantic space models treat context as a set of un-ordered words, without even taking parts of speech into account (e.g., to drink anda drink are represented by a single vector).
In fact, with the exception of function words(e.g., the, down), which are often removed, it is often assumed that all context wordswithin a certain distance from the target word are semantically relevant.
Because nolinguistic knowledge is taken into account, the construction of semantic space models isstraightforward and language-independent?all that is needed is a segmented corpusof written or spoken text.However, the assumption that contextual information contributes indiscriminatelyto a word?s meaning is clearly a simplification.
There is ample evidence demonstratingthat syntactic relations across and within sentences are crucial for sentence and dis-course processing (West and Stanovich 1986; Neville et al 1991; Fodor 1995; Miltsakaki2003) and modulate cognitive behavior in sentence priming tasks (Morris 1994).
Fur-thermore, much research in lexical semantics hypothesizes that the behavior of words,particularly with respect to the expression and interpretation of their arguments, is to alarge extent determined by their meaning (Fillmore 1965; Green 1974; Jackendoff 1983;Talmy 1985; Gropen et al 1989; Pinker 1989; Levin 1993; Goldberg 1995).It is therefore not surprising that there have been efforts to enrich vector-basedmodels with morpho-syntactic information.
Extensions range from part of speech tag-ging (Kanejiya, Kumar, and Prasad 2003; Widdows 2003) to shallow syntactic analy-sis (Grefenstette 1994; Lee 1999; Curran and Moens 2002) and full-blown parsing (Lin1998a).
In these semantic space models, contexts are defined over words bearing asyntactic relationship to the target words of interest.
This makes semantic spaces moreflexible; different types of contexts can be selected; words do not have to co-occur withina small, fixed word window; and word order or argument structure differences can benaturally mirrored in the semantic space.This article proposes a general framework for semantic space models which concep-tualizes context in terms of syntactic relations.
We introduce an algorithm for construct-ing semantic space models from texts annotated with syntactic information (specifi-cally dependency relations) and illustrate how different model classes can be derivedfrom this linguistically rich representation.
Our guiding hypothesis is that syntacticstructure in general and argument structure in particular is a close reflection of lexicalmeaning (Levin 1993).
We thus model meaning by quantifying the degree to whichwords are attested in similar syntactic environments.
The expressive power of ourframework stems from three novel parameters which guide model construction.
Thefirst parameter determines which types of syntactic structures contribute towards the162Pad?
and Lapata Dependency-Based Semantic Spacesrepresentation of lexical meaning.
The second parameter allows us to weigh the relativeimportance of different syntactic relations.
Finally, the third parameter determines howthe semantic space is actually represented, for instance as co-occurrences of words withother words, words with parts of speech, or words with argument relations (e.g., subject,object).We evaluate our framework on tasks relevant for cognitive science and NLP.
Westart by simulating semantic priming, a phenomenon that has received much atten-tion in computational psycholinguistics and is typically modeled using word-basedsemantic spaces (Landauer and Dumais 1997; McDonald and Brew 2004).
We nextconsider the problem of recognizing synonyms by selecting an appropriate synonymfor a target word from a set of semantically related candidate words.
Specifically, weevaluate the performance of our model on synonym questions from the Test of Englishas a Foreign Language (TOEFL).
These are routinely used as a testbed for assessinghow well vector-based models capture lexical knowledge (Landauer and Dumais 1997;Turney 2001; Sahlgren 2006).
Our final experiment concentrates on unsupervised wordsense disambiguation (WSD), thereby exploring the potential of the proposed frame-work for NLP applications requiring large scale semantic processing.
We automaticallyinfer predominant senses in untagged text by incorporating our syntax-based semanticspaces into the modeling paradigm proposed by McCarthy et al (2004).
In all cases, weshow that our framework consistently outperforms word-based models yielding resultsthat are comparable or superior to state of the art.Our contributions are threefold: a novel framework for semantic spaces that in-corporates syntactic information in the form of dependency relations and generalizesprevious syntax-based vector-based models; an application of this framework to a widerange of tasks relevant to cognitive modeling and NLP; and an empirical comparison ofour dependency-based models against state-of-the-art word-based models.In Section 2, we give a brief overview of existing word-based and syntax-basedmodels.
In Section 3, we present our modeling framework and relate it to previous work.Section 4 discusses the parameter settings for our experiments.
Section 5 details ourpriming experiment, Section 6 presents our study on the TOEFL synonymy task, andSection 7 describes our sense-ranking experiment.
Discussion of our results and futurework concludes the article (Section 8).2.
Overview of Semantic Space Models2.1 Word-Based and Syntax-Based ModelsTo facilitate comparisons with our framework, we begin with a brief overview of exist-ing semantic space models.
We describe traditional word-based co-occurrence modelsas exemplified in Lowe (2001), Lowe and McDonald (2000), McDonald (2000), and Levyand Bullinaria (2001), as well as syntax-based models as presented in Grefenstette (1994)and Lin (1998a).Lowe (2001) defines a semantic space model as a quadruple ?B, A, S, V?.
B is the setb1...D of basis elements, the dimensions of the space.
B can be a set of words (Lund andBurgess 1996) or lemmas (McDonald 2000), words with their parts of speech (Widdows2003) or words with a syntactic relation such as subject or object (Lin 1998a).
Usually,the dimensionality of the matrix is restricted to a relatively small number.
A popularchoice is the k most frequent words (minus the stop words) in a corpus, typically 100?2,000 (McDonald 2000; Levy and Bullinaria 2001).
A is a lexical association function163Computational Linguistics Volume 33, Number 2applied to the co-occurrence frequency of target word t with basis element b so that eachword is represented by a vector v = ?A( f (t, b1)), A( f (t, b2)), .
.
.
, A( f (t, bn))?.
If A is theidentity function, the raw frequencies are used.
Functions such as mutual informationor the log-likelihood ratio are often applied to factor out co-occurrences due to chance.S is a similarity measure that maps pairs of vectors onto a continuous-valued scale ofcontextual similarity.
V is an optional transformation that reduces the dimensionalityof the semantic space.
Singular value decomposition (SVD; Berry, Dumais, and O?Brien1994; Golub and Loan 1989) is commonly used for this purpose.
SVD can be thoughtof as a means of inferring latent structure in distributional data, while making sparsematrices more informative.
For the rest of this article, we will ignore V and otherstatistical transformations and concentrate primarily on ways of inducing structurefrom grammatical and syntactic information.To illustrate this definition, we construct a word-based semantic space for the targetwords T = {lorry, carry, sweet, fruit}, using as our corpus the following sentence: A lorrymight carry sweet apples.
For a word-based space, we might use the basis elementsB = {lorry, might, carry, sweet, apples}, a symmetric window of size 2, and identity asthe association function A.
Each target word ti ?
T will then be represented by a five-dimensional row vector, and the value of each vector element will record the numberof times each basis element bi ?
B occurs within a window of two words to the left andtwo words to the right of the target word ti.
The co-occurrence matrix that we obtainaccording to these specifications is shown in Figure 1.
A variety of distance measurescan be used to compute the similarity S between two target words (see Lee [1999] for anoverview), the cosine being the most popular:simcos(x,y ) =n?i=1xiyi?n?i=1x2i?n?i=1y2i(1)Syntax-based semantic space models (Grefenstette 1994; Lin 1998a) go beyond mereco-occurrence by capturing syntactic relationships between words such as subject?verbor modifier?noun, irrespectively of whether they are physically adjacent or not.
Thebasis elements are generally assumed to be tuples (r, w) where w is a word occurring inrelation type r with a target word t. The relations typically reflect argument structure(e.g., subject, object, indirect object) or modification (e.g., adjective?noun, noun?noun)and can be obtained via shallow syntactic processing (Grefenstette 1994; Lee 1999;Curran and Moens 2002) or full parsing (Lin 1998a; Curran and Moens 2002; Curran2004).
The basis elements (r, w) are treated as a single unit and are often called attributes(Grefenstette 1994; Curran and Moens 2002) or features (Lin 1998a).Figure 1Word-based semantic space (symmetric window size 2).164Pad?
and Lapata Dependency-Based Semantic SpacesFigure 2 shows a syntax-based semantic space in the manner of Grefenstette (1994),using the basis elements (subj,lorry), (aux,might), (mod,sweet), and (obj,apples).
Thebinary association function A records whether the target word possesses the feature(denoted by x in Figure 2) or not.
Because the cells of the matrix do not containnumerical values, a similarity measure that is appropriate for categorical values must bechosen.
Grefenstette (1994) uses a weighted version of Jaccard?s coefficient, a measureof association commonly employed in information retrieval (Salton and McGill 1983).Assuming Attr(t) is the set of basis elements co-occurring with t, Jaccard?s coefficient isdefined as:simJacc(t1, t2) =Attr(t1) ?
Attr(t2)Attr(t1) ?
Attr(t2)(2)Lin (1998a) constructs a semantic space similar to Grefenstette (1994) except that thematrix cells represent the number of times a target word t co-occurs with basis element(r, w), as shown in Figure 3.
He proposes an information theoretic similarity measurebased on the distribution of target words and basis elements:simlin(t1, t2) =?
(r,w)?T(t1 )?T(t2)I(t1, r, w) + I(t2, r, w)?
(r,w)?T(t1 )I(t1, r, w) +?
(r,w)?T(t2 )I(t2, r, w)(3)where I(t, r, w) is the mutual information between t and r, w and T(t) is the set of basiselements (r, w) such that I(t, r, w) is positive andI(t, r, w) = logP(t, r, w)P(r)P(w, r)P(t, r)= logP(w|r, t)P(w|r) (4)Figure 2Grefenstette?s (1994) semantic space.Figure 3Lin?s (1988a) semantic space.165Computational Linguistics Volume 33, Number 22.2 DiscussionBecause syntax-based models capture more linguistic structure than word-based mod-els, they should at least in theory provide more informative representations of wordmeaning.
Unfortunately, comparisons between the two types of models have been fewand far between in the literature.
Furthermore, the potential of syntax-based modelshas not been fully realized since most previous approaches limit themselves to a specificmodel class (Grefenstette 1994; Lin 1998a; Lin and Pantel 2001; Curran and Moens 2002).This section discusses these issues in more detail and sketches how we plan to addressthem.Modeling of syntactic context.
All existing syntax-based semantic space models we areaware of incorporate syntactic information in a rather limited fashion.
For example,the construction of the space is either based on all relations (Grefenstette 1994; Lin1998a) or a fixed subset (Lee 1999), but there is no qualitative distinction betweendifferent relations.
Even in cases where many relations are used (Lin 1998a; Lin andPantel 2001), only direct relations are taken into account, ignoring potentially importantco-occurrence patterns between, for instance, the subject and the object of a verb, orbetween a verb and its non-local argument (e.g., in control structures).Comparison between model classes.
Syntax-based vector space models have been used inNLP for a variety of lexicon acquisition tasks ranging from thesaurus extraction (Grefen-stette 1994; Lin 1998a) to paraphrase identification (Lin and Pantel 2001) and collocationdiscovery (Lin 1999; Bannard, Baldwin, and Lascarides 2003; McCarthy, Keller, andCarroll 2003).
Comparisons between word-based and syntax-based models on the sametask are rare, and the effect of syntactic knowledge has not been rigorously investigatedor quantified.
The few studies on this topic reveal an inconclusive picture.
On the onehand, Grefenstette compared the performance of the two classes of models on the taskof automatic thesaurus extraction and found that a syntactically enhanced model gavesignificantly better results over a simple word co-occurrence model.
A replication ofGrefenstette?s study with a more sophisticated parser (Curran and Moens 2002) re-vealed that additional syntactic information yields further improvements.
On the otherhand, attempts to generate more meaningful indexing terms for information retrieval(IR) using syntactic analysis (Salton and Smith 1989; Strzalkowski 1999; Henderson etal.
2002) have been largely unsuccessful.
Experimental results show minimal differencesin retrieval effectiveness at a substantially greater processing cost (see Voorhees [1999]for details).Impact on cognitive modeling.
Despite their widespread use in NLP, syntax-based seman-tic spaces have attracted little attention in cognitive science and computational psy-cholinguistics.
Wiemer-Hastings and Zipitria (2001) construct a semantic space similarto LSA, but enhanced with part-of-speech tags with the aim of modeling human ratersin an intelligent tutoring context.
Their results, however, show that the tagged LSA spaceyields worse performance than a word-based model.
Kanejiya, Kumar, and Prasad(2003) attempt to capture syntactic context in a shallow manner by enhancing targetwords with the parts-of-speech of their immediately preceding words.
They argue thatthis representation can provide useful information for the upcoming target words, as isoften the case in language modeling and left-to-right parsing.
They employ a document-based semantic space, which they submit to SVD and subsequently compare against anLSA model that contains no syntactic information, again in the context of an intelligent166Pad?
and Lapata Dependency-Based Semantic Spacestutoring system.
Their results indicate that the syntactically enhanced model has bettercoverage than the LSA model (i.e., it is able to evaluate more student answers), althoughit displays a lower correlation with human raters than raw LSA.In this article, we argue the case for investigating dependency-based semantic spacemodels in more depth.
We provide a general definition for these models, which incor-porates a wider range of syntactic relations than previously considered and subsumesexisting syntax-based and word-based models.
In order to demonstrate the scope ofour framework, we evaluate our models on tasks popular in both cognitive science andNLP.
Furthermore, in all cases we report comparisons against state of the art word-based models and show that the additional processing cost incurred by syntax-basedmodels is worthwhile.3.
A General Framework for Semantic Space ModelsOnce we move away from words as the basic context unit, the issue of representa-tion of syntactic information becomes pertinent.
An ideal syntactic formalism shouldabstract over surface word order, mirror semantic relationships as closely as possible,and incorporate word-based information in addition to syntactic analysis.
It should bealso applicable to different languages.
These requirements point towards dependencygrammar, which can be considered as an intermediate layer between surface syntax andsemantics.
More formally, dependency relations are asymmetric binary relationshipsbetween a head and a modifier (Tesni?re 1959).
The structure of a sentence is analyzedas a directed graph whose nodes correspond to words.
The graph?s edges correspondto dependency relationships and each edge is labeled with a specific relationship type(e.g., subject, object).The dependency analysis for the sentence A lorry might carry sweet apples is givenin Figure 4.
On the left, the sentence is represented as a graph.
The sentence head is themain verb carry which is modified by its subject lorry, its object apples and the auxiliarymight.
The subject and object are modified respectively by a determiner (a) and an ad-jective (sweet).
On the right side of Figure 4, an adjacency matrix notation is used.
Edgesin the graph are represented as triples of a dependent word (e.g., lorry), a dependencylabel (e.g., [N, subj, V]), and a head word (e.g., carry).
The dependency label consists ofthe part of speech of the modifier (capitalized, e.g., N) , the dependency relation itself(in lower case, e.g., subj), and the part of speech of the head (also capitalized, e.g., V).It is combinations of dependencies like the ones in Figure 4 that will form thecontext over which the semantic space will be constructed.
We base our discussionFigure 4A dependency analysis of the sentence A lorry might carry sweet apples as parse tree (left) and setof head-relation-modifier triples (right).167Computational Linguistics Volume 33, Number 2and experiments on the broad-coverage dependency parser MINIPAR, version 0.5 (Lin1998a, 2001).
However, there is nothing inherent in our formalization that restricts us tothis particular parser.
Any other parser with broadly similar dependency output (e.g.,Briscoe and Carroll 2002) could serve our purposes.In the remainder of this section, we first give a non-technical description of ouralgorithm for the construction of semantic spaces.
Then, we proceed to discuss each con-struction step (context selection, basis mapping, and quantification of co-occurrences)in more detail.
Finally, we show how our framework subsumes existing models.
Table 1lists the notation we use in the rest of the article.3.1 The Construction AlgorithmOur algorithm for creating semantic space models is summarized in Figure 5.
Centralto the construction process is the notion of paths, namely sequences of dependencyedges extracted from the dependency parse of a sentence (we define paths formally inSection 3.2).
Consider again the graph in Figure 4.
Besides individual edges (i.e., pathsof length 1), it contains several longer paths, such as the path between lorry andsweet (?lorry, carry, apples, sweet?
), the path between a and carry (?a, lorry, carry?
), the pathbetween lorry and carry (?lorry, carry?
), and so forth.
The usage of paths allows us torepresent direct and indirect relationships between words and gives rise to three novelparameters:1.
The context selection function cont(t) determines which paths in thegraph contribute towards the representation of target word t. For example,we may choose to consider only paths of length 1, or paths withlength ?
3.
The function is effectively a syntax-based generalization of thetraditional ?window size?
parameter.2.
The path value function v assigns weights to paths, thus allowinglinguistic knowledge to influence the construction of the space.
ForTable 1Summary of notation.b ?
B Basis elementt ?
T Target word typeW(t) Set of tokens of target type tM[t][b] ?
R Cell of semantic space matrix for target word t and basis element b?
Dependency path (in a given dependency tree)?
Set of all undirected paths?s Set of all undirected paths in sentence s?t Set of all undirected paths in a sentence anchored at word tstart(?
), end(?)
First and last node of an undirected pathCat Set of POS categories (for given parser)R Set of dependency relations (for given parser)l : ??
(Cat ?
R ?
Cat)?
Edge (sequence) labeling functioncont : T ?
2?s Local context selection function (subset of paths in sentence s)?
: ??
B Basis element mapping functionv : ??
R Path value functionA : R4 ?
R Lexical association function168Pad?
and Lapata Dependency-Based Semantic SpacesFigure 5Algorithm for construction of semantic space.instance, it can be used to discount longer paths, or give more weight topaths containing subjects and objects as opposed to determiners ormodifiers.3.
The basis mapping function ?
creates the dimensions of the semanticspace.
Although paths themselves could serve as dimensions, theresulting co-occurrence matrix would be overly sparse (this is especiallytrue for lexicalized paths whose number can become unwieldy whenparsing a large corpus).
For this reason, the basis elements formingthe dimensions of the space are defined independently from the pathconstruction.
The basis mapping function maps paths onto basis elementsby collapsing paths deemed functionally equivalent.
For instance, wemay consider paths carrying the same dependency relations, or pathsending in the same word, as equivalent.
We thus disassociate thedefinition of context entities (paths) from the dimensions of the finalspace (basis elements).As discussed in Section 2, the main difference among variants of semantic space modelslies in the specification of basis elements B.
By treating the dependency paths as distinctfrom the basis elements, we obtain a general framework for vector-based models thatcan be parametrized for different tasks and allows for the construction of spaces withbasis elements consisting of words, syntactic entities, or combinations of both.
Thisflexibility, in conjunction with the context selection and path value functions, allowsour model to subsume both traditional word-based and syntax-based models (seeSection 3.6 for more details).3.2 Step 1: Building the ContextThe first step in constructing a semantic space from a large collection of dependencyrelations is to define an appropriate syntactic context for the target words of interest.We define contexts as anchored paths, that is, paths in a dependency graph that startat a particular target word t. Our assumption is that the set of paths anchored att is a superset of the paths that can contribute relevant distributional informationabout t.169Computational Linguistics Volume 33, Number 2Definition 1.
The dependency parse p of a sentence s is a directed graph ps = (Vs, Es),where Es ?
Vs ?
Vs.
The nodes v ?
Vs are labeled with individual words wi.
Forsimplicity, we use nodes and their labels interchangeably, and the set of nodescorresponds to the words of the sentence: Vs = {w1, .
.
.
, wn}.
Each edge e ?
Es bearsa label l : Es ?
Cat ?
R ?
Cat where Cat belongs to a set of POS tags and R to a set ofdependency relations.
We assume that this set is finite and parser-specific.1 We writeedge labels in square brackets.
[Det,det,N] and [N,subj,V] are examples for labelsprovided by MINIPAR (see Figure 4, right-hand side).We are now ready to define paths in our dependency graph, save one importantissue: Should we confine ourselves to directed paths or perhaps disregard the directionof the edges?
In a dependency graph, directed paths can only capture the relationshipbetween a head and its (potentially transitive) dependents (e.g., carry and sweet inFigure 4).
This excludes informative contexts representing, for instance, the relationshipbetween the subject and the object of a predicate (e.g., lorry and apples in Figure 4).
Ourintuition is therefore that directed paths would limit the context too severely.
In thefollowing, we assume undirected paths.Definition 2.
An (undirected) path ?
is an ordered tuple of nodes ?v0, .
.
.
, vn?
?
V?s forsome sentence s that meets the following two constraints:?
i : (vi?1, vi) ?
Es ?
(vi, vi?1) ?
Es (connectedness)?
i?
j : i = j ?
vi = vj (cycle-freeness)In the rest of the article, we use the term path as a shorthand for undirected path.Definition 3.
A path ?
is anchored at a word t iff start(?)
= t. We write ?t ?
?s for theset of all paths anchored at t in sentence s.As an example, the set of paths anchored at lorry in Figure 4 is:{?lorry, carry?, ?lorry, a?, (two paths of length 1)?lorry, carry, apples?, ?lorry, carry, might?, (two paths of length 2)?lorry, carry, apples, sweet?}
(one path of length 3)Definition 4.
The context selection function cont : W ?
2?t assigns to a word t a subsetof the paths anchored at t. We call this subset the syntactic context of t.The context selection function allows direct control over the type of linguistic informa-tion represented in the semantic space.
In traditional vector-based models, the context1 For the sake of simplicity, we use R without a subscript to denote the set of dependency relationsprovided by MINIPAR.
We utilize subscripts to distinguish between general sets (e.g., E for the set of allconceivable edges) and sentence-specific sets (e.g., Es for the set of edges in the parse tree of sentence s).170Pad?
and Lapata Dependency-Based Semantic Spacesselection function does not take any syntactic information into account: All paths ?
areselected for which the absolute difference (abs) between the positions (pos) of the anchorstart(?)
and the end word end(?)
does not exceed the window size k:cont(t) = {?
?
?t | abs(pos(start(?))
?
pos(end(?)))
?
k} (5)The dependency-based models proposed by Grefenstette (1994) and Lin (1998a) con-sider minimal syntactic contexts in the form of individual dependency relations,namely, dependency paths of length 1:cont(t) = {?
?
?t | ||?|| = 1} (6)The context selection function as defined herein permits the elimination of paths fromthe semantic space on the basis of linguistic or other information.
For example, it can beargued that subjects and objects convey more semantic information than determinersor auxiliaries.
We can thus limit our context to the set of all anchored paths consistingexclusively of subject or object dependencies:cont(t) = {?
?
?t | l(?)
?
{[V, subj, N], [V, obj, N]}?}
(7)When this context specification function is applied to the dependency graph in Figure 4,only the edges shown in boxes are retained.
The context of lorry is thus reduced totwo paths: ?lorry, carry?
(length 1) and ?lorry, carry, apples?
(length 2).
The paths ?lorry, a?,?lorry, carry, might?, and ?lorry, carry, apples, sweet?
are omitted because their label se-quences (such as [N,det,Det] for ?lorry, a?)
are disallowed by (7).3.3 Step 2: Basis MappingThe second step in the construction of our semantic space model is to specify itsdimensions, the basis elements, following Lowe?s (2001) terminology.Definition 5.
The basis mapping function ?
: ??
B maps paths onto basis elements.By dissociating dependency paths and basis elements in this way, we decouple theobserved syntactic context from its representation in the final semantic space.
Thebasis mapping allows us to exploit underlying relationships among different paths:Two paths which are (in some sense) equivalent can be mapped onto the same basiselement.
The function effectively introduces a partitioning of paths into equivalenceclasses ?labeled?
by basis elements, thus offering more flexibility in defining the basiselements themselves.Traditional co-occurrence models use a word-based basis mapping.
This means that allpaths ending at word w are mapped onto the basis element w, resulting in a semanticspace with context words as basis elements (recall that all paths in the local context startat the target word):?(?)
= end(?)
(8)171Computational Linguistics Volume 33, Number 2A word-based mapping is also possible when paths are defined over dependencygraphs.
As an example consider the paths anchored at lorry in Figure 4.
Using (8), thesepaths are mapped to the following basis elements:?lorry, carry?
carry?lorry, a?
a?lorry, carry, apples?
apples?lorry, carry, might?
might?lorry, carry, apples, sweet?
sweetA different mapping is used in Grefenstette (1994) and Lin (1998a), who consider onlypaths of length 1.
In their case, paths are mapped onto pairs representing a dependencyrelation r and the end word w (see the discussion in Section 2):?(?)
= (r, end(?))
where ||?|| = 1 ?
?r?
= l(?)
(9)Any plausible and computationally feasible function can be used as basis mapping.However, in this article we restrict ourselves to models which use a word-based ba-sis mapping.
The resulting spaces are similar to traditional word-based spaces?bothuse sets of context words?which allows for direct comparisons between our modelsand word-based alternatives.
Crucially, our models differ from traditional models inthe more general treatment of (syntactic) context: Only paths in the syntactic context,and not surface co-occurrences, contribute towards counts in the matrix.
The contextselection function supports inference over classes of basis elements (which in previousmodels would have been considered distinct) as well as fine-grained control over thetypes of relationships that enter into the space construction.3.4 Step 3: Quantifying Syntactic Co-occurrenceThe last step in the construction of the dependency-based semantic models is to specifythe relative importance (i.e., value) of different paths:Definition 6.
The path value function v assigns a real number to a path: v : ??
R.Traditional models do not exploit this possibility, thus giving equal weight to all paths:vplain(?)
= 1 (10)The path value function provides additional flexibility for incorporating linguistic in-formation into our framework.
Even if two paths are mapped onto the same basiselement (by the basis mapping), the path value function can weigh their respectivecontributions differently.
For instance, it could discount longer paths that expressindirect relationships between words.
An example of such a length-based path value172Pad?
and Lapata Dependency-Based Semantic Spacesfunction is given in Equation (11).
It assigns a value of 1 to paths of length 1 and fractionsto longer paths:vlength(?)
= 1||?|| (11)A more linguistically informed path value function can be defined by taking intoaccount the obliqueness hierarchy of grammatical relations (Keenan and Comrie 1977).According to this hierarchy subjects are more salient than objects, which in turn aremore salient than obliques (e.g., prepositional phrases).
And obliques are more salientthan genitives.
We thus define a linear relation-based weighting scheme that ranks pathsaccording to their most salient grammatical function, without considering their length:vgram?rel(?)
=????????????
?5 if subj ?
l(?
)4 if obj ?
l(?
)3 if obl ?
l(?
)2 if gen ?
l(?
)1 else(12)The path value function assigns a numerical value to each path forming the syntac-tic context of a token t. We can next define the local co-occurrence frequency between tand a basis element b as the sum of the path values v(?)
for all paths ?
?
cont(t) that aremapped onto b.
Because our semantic space construction algorithm operates over wordtypes, we sum the local co-occurrence frequencies for all instances of a target word type t(written as W(t)) to obtain its global co-occurrence frequency.
The latter is a measureof the co-occurrence of t and b over the entire corpus.Definition 7.
The global co-occurrence frequency of a basis element b and a target t iscomputed by the function f : B ?
T ?
R defined byf (b, t) =?w?W(t)???cont(w)??(?)=bv(?
)The global co-occurrence frequency f (b, t) could be used directly as the matrix valueM[b][t].
However, as Lowe (2001) notes, raw counts are likely to give misleading results.This is due to the non-uniform distribution of words in corpora which will introduce afrequency bias so that words with similar frequency will be judged more similar thanthey actually are.
It is therefore advisable to use a lexical association function A to factorout chance co-occurrences explicitly.Our definition allows an arbitrary choice of lexical association function (seeManning and Sch?tze [1999] for an overview).
In our experiments, we follow Loweand McDonald (2000) in using the well-known log-likelihood ratio G2 (Dunning 1993).We can visualize the computation using a two-by-two contingency table whose fourcells correspond to four events (Kilgarriff 2001):t ?
tb k l?
b m n173Computational Linguistics Volume 33, Number 2The top left cell records the frequency k with which t and b co-occur (i.e., k correspondsto raw frequency counts).
The top right cell l records how many times b is attested withany word other than t, the bottom left cell m represents the frequency of any word otherthan b with t, and the bottom right cell n records the frequency of pairs involving neitherb nor t. The function G2 : R4 ?
R is defined asG2(k, l, m, n) = 2(k log k + l log l + m log m + n log n?
(k + l) log(k + l) ?
(k + m) log(k + m)?
(l + n) log(l + n) ?
(m + n) log(m + n)+ (k + l + m + n) log(k + l + m + n))(13)A naive implementation of the log-likelihood ratio would keep track of all four eventsfor each pair (t, b); this strategy would require updating the entire matrix for eachpath and would render the construction of the space prohibitively expensive.
Thiscan be avoided by computing only k = f (t, b), the global co-occurrence frequency,and using the marginal frequencies of paths and targets to estimate l, m and n asfollows:l =?tf (t, b) ?
k m =?bf (t, b) ?
k n =?b?tf (t, b) ?
(k + l + m) (14)For example, l can be computed as the total value of all paths in the corpus that aremapped onto b minus the value of those paths that are anchored at t.3.5 Definition of Semantic SpaceOur framework of semantic space models can now be formally specified by extendingLowe?s (2001) definition from Section 2:Definition 8.
A semantic space is a tuple ?B, T, M, S, A, cont,?, v?.
B is the set ofbasis elements, T the set of target words, and M is the matrix M = B ?
T. We writeM[tj][bi] ?
R for the matrix cell (i, j).
S : T ?
T ?
R is the similarity measure, andA : R4 ?
R is the lexical association function.
Our additional parameters are thecontent selection function cont : T ?
2?s, the basis mapping function ?
: ??
B, andthe path value function v : ??
R.Note that the set of target words T can contain either word types or word tokens.
Inthe preceding definitions, we have assumed that co-occurrence counts are constructedover word types, however the framework can be also used to represent word tokens.
Inthis case, each set of target tokens contains exactly one word (W(t) = {t}), and the outersummation step in Definition 7 trivially does not apply.
We work with type-based spacesin the rest of this article.
The use of tokens may be appropriate for other applicationssuch as word sense discrimination (Sch?tze 1998).We can now construct a semantic space that illustrates our framework.
Consideragain the sentence A lorry might carry sweet apples.
According to Definition 8, inorder to construct vectors for the target words T = {lorry, might, carry, sweet, fruit}, we174Pad?
and Lapata Dependency-Based Semantic Spacesmust provide a context selection function, a basis mapping function, and a path valuefunction.
The space resulting from a context selection function which considers exclu-sively subject and object dependencies (see Equation (7)), a word-based basis mappingfunction (see Equation (8)), and a length-based path value function (see Equation (11)),is shown in Figure 6.3.6 DiscussionWe have proposed a general framework for semantic space models which operateson dependency relations and allows linguistic knowledge to inform the constructionof the semantic space.
The framework is highly flexible: Depending on the contextselection and basis mapping functions, semantic spaces can be constructed over words,words and parts of speech, syntactic relations, or combinations of words and syntacticrelations.
This flexibility unavoidably increases the parameter space of our models,because there is a potentially large number of context selection or path value functionsfor which semantic spaces can be constructed.At the same time, this allows us to subsume existing semantic space models inour framework, and facilitates comparisons across different kinds of spaces (compareFigures 1, 3, and 6).
Our space is sparser than the word-based space in Figure 1,due to the choice of a more selective context specification function (see Equations (5)and (7)).
However, this is expected because our main motivation is to distinguishbetween informative and uninformative syntactico-semantic relations.
Using a minimalcontext selection function results in a space that contains indisputably valid semanticrelations, excluding potentially noisy relations like the one between might and sweet.
Byadding richer linguistic information to the context selection function, the space can beexpanded in a principled manner.
In comparison with previous syntax-based models,which only use direct dependency relations (see Equation (6)), our dependency-basedspace additionally represents indirect semantic relations (e.g., between lorry and apples).A smaller parameter space could have resulted from collapsing the context selectionand path value functions into one parameter, for example, by defining context selectiondirectly as a function from (anchored) paths to their path values, and thus assigning avalue of zero to all paths ?
?
cont(t).
However, we refrained from doing this for tworeasons, a methodological one and a technical one.
On the methodological side, webelieve that it makes sense to keep the two concepts of context selection and contextweighting distinct.
The separation allows us to experiment with different path valuefunctions while keeping the set of paths resulting from context selection constant.On the technical side, the two functions are easier to specify declaratively when keptseparately.
Also, a separate context selection function can be used to efficiently isolaterelevant context paths without having to compute the values for all anchored paths.Figure 6A dependency-based semantic space using context selection function (7), basis mappingfunction (8), and path value function (11).175Computational Linguistics Volume 33, Number 2The context selection function operates over a subset of dependency paths that areanchored, cycle-free, and connected.
These three preconditions on paths are meant toreflect linguistic properties of reasonable syntactic contexts while at the same time theyguarantee the efficient construction of the semantic space.
Anchoredness ensures thatall paths are semantically connected to the target; this also means that the search spacecan be limited to paths starting at the target word.
Cycle-freeness and connectednessexclude linguistically meaningless paths such as paths of infinite length (cycles) or pathsconsisting of several unconnected fragments.
These properties guarantee that contextpaths can be created incrementally, and that construction terminates.3.7 Runtime and ImplementationOur implementation uses path templates to encode the context selection function (seeAppendix A for more details).
The runtime of the semantic space construction algorithmpresented in Section 3 is O(maxg ?
|cont| ?
t), where maxg is the maximal degree of a nodein the grammar, |cont| the number of path templates used for context selection, and t thenumber of target tokens in the corpus.
This assumes that ?(?)
and v(?)
can be computedin constant time, which is warranted in practice because most linguistically interestingpaths will be of limited length (in our study, all paths have a length of at most 4).
Thelinear runtime in the size of the corpus provides a theoretical guarantee that the methodis applicable to large corpora such as the British National Corpus (BNC).A Java implementation of the framework presented in this article is available un-der the GNU General Public License from http://www.coli.uni-saarland.de/?pado/dv/dv.html.
The system can create dependency spaces from the output of MINIPAR (Lin1998b, 2001).
We also provide an interface for integrating other parsers.
The distributionincludes a set of prespecified parameter settings, namely the word-based basis mappingfunction, and the path value and context selection functions used in our experiments.4.
Experimental SetupIn this section, we describe the corpus and parser chosen for our experiments.
We alsodiscuss our parameter and model choice procedure, and introduce the baseline word-based model which we use for comparison with our approach.
Our experiments arethen presented in Sections 5?7.4.1 Corpus and ParserAll our experiments were conducted on the British National Corpus (BNC), a 100-million word collection of samples of written and spoken English (Burnard 1995).
Thecorpus represents a wide range of British English, including samples from newspapers,magazines, books (both academic and fiction), letters, essays, as well as spontaneousconversations, business or government meetings, radio shows, and phone-ins.
The BNChas been used extensively in building vector space models for many tasks relevantfor cognitive science (Patel, Bullinaria, and Levy 1998; McDonald 2000; McDonald andBrew 2004) and NLP (McCarthy et al 2004; Weeds 2003; Widdows 2003).In order to construct dependency spaces, the BNC was parsed with MINIPAR,version 0.5 (Lin 1998b, 2001), a wide-coverage dependency parser.
MINIPAR employs amanually constructed grammar and a lexicon derived from WordNet with the additionof proper names (130,000 entries in total).
Lexicon entries contain part-of-speech and176Pad?
and Lapata Dependency-Based Semantic Spacessubcategorization information.
The grammar is represented as a network of 35 nodes(i.e., grammatical categories) and 59 edges (i.e., types of dependency relationships).MINIPAR uses a distributed chart parsing algorithm.
Grammar rules are implementedas constraints associated with the nodes and edges.
When evaluated on the SUSANNEcorpus (Sampson 1995), the parser achieved a precision of 89% and a recall of 79% inidentifying labeled dependencies (Lin 1998b).4.2 Model SelectionThe construction of semantic space models involves a large number of parameters: thedimensions of the space, the size and type of the employed context, and the choice ofsimilarity function.
A number of studies (Patel, Bullinaria, and Levy 1998; Levy andBullinaria 2001; McDonald 2000) have explored the parameter space for word-basedmodels in detail, using evaluation benchmarks such as human similarity judgments orsynonymy choice tests.
The motivation behind such studies is to identify parametersor parameter classes that yield consistently good performance across tasks.
To avoidoverfitting, exploration of the parameter space is typically performed on a developmentdata set different from the test data (McDonald 2000).The benchmark data set collected by Rubenstein and Goodenough (1965) is rou-tinely used in NLP and cognitive science for development purposes?for example, forevaluating automatic measures of semantic similarity (Resnik 1995; Budanitsky andHirst 2001; Banerjee and Pedersen 2003) or for exploring the parameter space of vec-tor space models (McDonald 2000).
It consists of 65 noun-pairs ranging from highlysynonymous (gem-jewel ) to semantically unrelated (noon-string).
For each pair, a sim-ilarity judgment (on a scale of 0 to 4) was elicited from human subjects.
The averagerating for each pair represents an estimate of the perceived similarity of the two words.Correlation analysis is often used to examine the degree of linear relationship betweenthe human ratings and the corresponding automatically derived similarity values.Following previous work, we explored the parameter space of our dependencymodels on the Rubenstein and Goodenough (1965) data set.
The best performing modelwas then used in all our subsequent experiments.
We expect a dependency modeloptimized on the semantic similarity task to perform well across other related lexicaltasks, which incorporate semantic similarity either directly or indirectly.
This is truefor all tasks reported in this article, namely priming (Experiment 1), inferring whethertwo words are synonyms (Experiment 2), and acquiring predominant word senses(Experiment 3).
Some performance gains could be expected, if parameter optimizationtook place separately for each task.
However, such a strategy would unavoidably leadto overfitting, especially because our data sets are generally small (see Experiments 1and 2).We next detail how parameters were instantiated in our dependency models withan emphasis on the influence of the context selection and path value functions.Parameters.
Dependency contexts were defined over a set of 14 dependency relations,each of which occurred more than 500,000 times in the BNC and which in total ac-counted for about 76 million of the 88 million dependency relations found in the corpus.These relations are: amod (adjective modifier), comp1 (first complement), conj (coordina-tion), fc (finite complement), gen (genitive noun modifier), i (the relationship betweena main clause and a complement clause), lex-mod (lexical modifier), mod (modifier), nn(noun-noun modifier), obj (object of a verb), pcomp-n (nominal complement of preposi-tions), rel (relative clause), s (surface subject), and subj (subject of a verb).
From these,177Computational Linguistics Volume 33, Number 2we constructed three context selection functions (fully described in Appendix A), whichwe implemented as parser-specific templates (one template per non-lexical dependencypath): minimum contexts contain paths of length 1 (27 templates; in Figure 4 sweetand carry are the minimum context for apples).
This definition of syntacticcontext considers only direct relations and corresponds to local verbalpredicate-argument structure. medium contexts add to minimum contexts dependency paths which modelthe internal structure of noun phrases (length ?
3; 59 templates).
Inparticular, the medium context covers phenomena such as coordination,genitive constructions, noun compounds, and different kinds ofmodification. maximum contexts combine all templates defined over the 14 dependencyrelations described above into a rich context representation (length ?
4;123 templates).The context specification functions were combined with the three path value functionsintroduced in Section 3: plain (vplain, see Equation (10)) assigns the same value (namely 1) to everypath.
It is the simplest path value function and assumes that all paths areequally important. length (vlength, see Equation (11)) implements a length-based weightingscheme: It assigns each path a value inversely proportional to its length,thus giving more weight to shorter paths corresponding to more directrelationships. gram-rel (vgram-rel, see Equation (12)) uses the obliqueness hierarchy(Keenan and Comrie 1977) to rank paths according to the salience of theirgrammatical relations.
Specifically, each path is assigned the value of itsmost salient grammatical relation (subjects are more salient than objects,which are more salient than other noun phrases).The combination of the three context selection and three path value functions yieldsnine model instantiations.2 To facilitate comparisons with traditional semantic spacemodels, we used a word-based basis mapping function (see Equation (8)) and the log-likelihood score (see Equation (13)) as our lexical association function.
We also createdsemantic spaces with different dimensions, using the 500, 1,000, and 2,000 most frequentbasis elements obtained from the BNC.
Finally, we experimented with a variety ofsimilarity measures: cosine, Euclidean distance, L1 norm, Jaccard?s coefficient, Kullback-Leibler divergence, skew divergence, and Lin?s (1998a) measure.32 Because the minimum context selection only considers paths of length 1, the combinations minimum-plainand minimum-length are identical.3 The original specification of Lin?s distance measure (Equation (3)) assumes relation?word pairs as basiselements.
Because we work with a word-based basis mapping, we use a simplified version, whereI(t, r, w) reduces to I(t, w) = log P(t,w)P(t)P(w) .178Pad?
and Lapata Dependency-Based Semantic SpacesResults.
The effects of different parameters on modeling semantic similarity (usingRubenstein and Goodenough?s [1965] data set) are illustrated in Tables 2 and 3.
Wereport the Pearson product moment correlation (?Pearson?s r?)
between human ratingsof similarity and vector-based similarity.
Rubenstein and Goodenough report an inter-subject correlation of r = 0.85 on the rating task.
The latter can be considered an upperbound for what can be expected from computational models.
For the sake of brevity, weonly report results with 2,000 basis elements, because we found that models with fewerdimensions (e.g., 500 and 1,000) generally obtained worse performance.
Lin?s (1998a)similarity measure uniformly outperformed all other measures by a large margin.
Forcomparison, we also give the results we obtained with the cosine similarity measure(see Table 2).As can be seen, the gram-rel path value function performs generally worse thanlength or plain.
We suspect that this function is, at least in its present form, too selective,giving a low weight to a large number of possibly informative paths without subjectsor objects.
A similar result is reported in Henderson et al (2002), who find that usingthe obliqueness hierarchy to isolate important index terms in an information retrievaltask degrades performance.
The use of the less fine-grained length path value functiondelivers better results for the medium and maximum context configurations (see Table 3).Finally, we observe that the medium context yields the best overall performance.
Withinthe currently explored parameter space, medium appears to strike the best balance: Itincludes some dependency paths beyond length one (corresponding to informativeindirect relations), but also avoids very long and infrequent contexts which couldpotentially lead to overly sparse representations.
In sum, the best dependency-basedmodel uses the medium content selection and length path value functions, 2,000 basiselements, and Lin?s (1998a) similarity measure.
This model will be used for our subse-quent experiments without additional parameter tuning.
We will refer to this model asthe optimal dependency-based model.Table 2Correlations (Pearson?s r) between elicited similarity and dependency models using the cosinedistance, 2,000 basis elements, and the log-likelihood association function.????????
?ContextPath plain length gram-relminimum 0.45 0.45 0.43medium 0.45 0.45 0.44maximum 0.47 0.46 0.45Table 3Correlations (Pearson?s r) between elicited similarity and dependency models using Lin?s(1998a) similarity measure, 2,000 basis elements and the log-likelihood association function.????????
?ContextPath plain length gram-relminimum 0.58 0.58 0.58medium 0.60 0.62 0.59maximum 0.56 0.59 0.55179Computational Linguistics Volume 33, Number 24.3 Baseline ModelOur experiments will compare the optimal dependency model just described against astate-of-the art word-based vector space model commonly used in the literature.
Thelatter employs a ?bag of words?
definition of context (see Equation (5)), uses wordsas basis elements, and assumes that all words are given equal weight.
In order toallow a fair comparison, we trained the word-based model on the same corpus asthe dependency-based model (the complete BNC) and selected parameters that havebeen considered ?optimal?
in the literature (Patel, Bullinaria, and Levy 1998; Lowe andMcDonald 2000; McDonald 2000).
Specifically, we built a word-based model with asymmetric 10 word window as context and the most frequent 500 content words fromthe BNC as dimensions.4 We used log-likelihood as our lexical association function andthe cosine similarity measure5 as our distance measure.5.
Experiment 1: Single-Word PrimingA large number of modeling studies in psycholinguistics have focused on simulatingsemantic priming phenomena (Lund and Burgess 1996; Lowe and McDonald 2000;McDonald 2000; McDonald and Brew 2004).
The semantic priming paradigm provides anatural test bed for semantic space models, as it concentrates on the semantic similarityor dissimilarity between words, and it is precisely this type of lexical relation thatvector-based models should capture.
If dependency-based models indeed representmore linguistic knowledge, they should be able to model semantic priming better thantraditional word-based models.In this experiment, we focus on Hodgson?s (1991) single-word lexical priming study.In single-word semantic priming, the transient presentation of a prime word like tigerdirectly facilitates pronunciation or lexical decision on a target word like lion: responsesare usually faster and more accurate when the prime is semantically related to thetarget than when it is unrelated.
Hodgson set out to investigate which types of lexicalrelations induce priming.
He collected a set of 144 word pairs exemplifying six differentlexical relations: (a) synonymy (words with the same meaning, e.g., value and worth);(b) superordination and subordination (one word is an instance of the kind expressed bythe other word, e.g., pain and sensation); (c) category coordination (words which expresstwo instances of a common superordinate concept, e.g., truck and train); (d) antonymy(words with opposite meaning, e.g., friend and enemy); (e) conceptual association (thefirst word subjects produce in free association given the other word, e.g., leash and dog);and (f) phrasal association (words which co-occur in phrases, e.g., private and property).The pairs covered the most prevalent parts of speech (adjectives, verbs, and nouns); theywere selected to be unambiguous examples of the relation type they instantiate andwere matched for frequency.
Hogdson found equivalent priming effects (i.e., reducedreading times) for all six types of lexical relation, indicating that priming was notrestricted to particular types of prime?target relation.The priming effects reported in Hodgson (1991) have recently been modeled byMcDonald and Brew (2004), using an incremental vector-based model of contextual4 Increasing the dimensions of the space to 1,000 and 2,000 degraded performance.
Smaller contextwindows did not yield performance gains either.5 We repeated all experiments for the word-based model with Lin?s (1998a) distance measure, obtainingconsistently worse results.180Pad?
and Lapata Dependency-Based Semantic Spacesfacilitation.
Their ICE model (short for Incremental Construction of Semantic Expec-tations) simulates the difference in effort between processing a target word precededby a related prime and processing the same target preceded by an unrelated prime.This is achieved by quantifying the ability of the distributional characteristics of theprime word to predict the distributional properties of the target.
The prime word isrepresented by a vector of probabilities which reflects the likely location in semanticspace of the upcoming word.
When the target word is observed, the representationis updated using a Bayesian inference mechanism to reflect the newly arrived infor-mation.
McDonald and Brew use a traditional semantic space that takes only wordco-occurrences into account and is defined over the 500 most frequent words of thespoken portion of the BNC.
They measure distance in semantic space using relativeentropy (also known as Kullback?Leibler divergence) and successfully model the databy predicting that the distance should be lower for related prime-target pairs than forunrelated prime?target pairs.5.1 MethodIn this experiment we follow McDonald and Brew?s (2004) methodology in simulatingsemantic priming.
However, because our primary focus is on the representation of thesemantic space, we do not adopt their incremental model of semantic processing.
Wesimply model reading time for prime?target pairs by distance in the semantic space,without making explicit predictions about upcoming words.From the 143 prime?target pairs listed in Hodgson (1991) (one synonymy pair ismissing in the original data set), seven pairs containing at least one low-frequency word(less than 100 occurrences in the BNC) were removed to avoid creating vectors withunreliable counts.6 We constructed a dependency-based model with the parameters thatyielded best performance on our development set (see Section 4.2) and a baseline word-based model (see Section 4.3).
Each prime?target pair was represented by two vectors(one corresponding to the prime and one corresponding to the target).These prime?target pairs form the items in this experiment.
The independent vari-ables (i.e., the variables directly manipulated by Hodgson [1991] in his original experi-ment) are (1) the type of Lexical Relation (antonyms, synonyms, conceptual associates,phrasal associates, category coordinates, superordinate-subordinates), and (2) the Prime(related, unrelated).
The dependent variable (i.e., the quantity being measured) is thedistance between the vector space representations of the prime and the target.
Thepriming effect is simulated by comparing the distances between Related and Unrelatedprime?target pairs.
Because the original materials do not provide Unrelated primes, weemulated the unrelated pairs as described in McDonald and Brew (2004), by using theaverage distance of a target to all other primes of the same relation.We test two hypotheses: first, that our dependency-based model can simulate se-mantic priming.
Failure to do so would indicate that our model is deficient because itcannot capture basic semantic relatedness, a notion underlying many tasks in cognitivescience and NLP.
Second, we predict that the dependency-based model will be better atsimulating priming than a traditional word-based one.6 Low-frequency words are deemed to produce high variance vectors because the co-occurrence countsneeded to determine M[t][b] will be unreliable (see McDonald [2000] for further evidence).
Variance canbe decreased by providing more data or by smoothing; however, we leave this to future work.181Computational Linguistics Volume 33, Number 25.2 ResultsWe carried out a two-way analysis of variance (ANOVA) on the simulated primingdata generated by the optimal dependency-based and the baseline word-based model.The factors were the two independent variables introduced herein, namely LexicalRelation (six levels) and Prime (two levels).
A reliable Prime effect was observed forthe dependency-based model (F(1, 129) = 182.46, MSE = 0.93, p < 0.01): the distancebetween a target and its Related prime was significantly smaller than between a tar-get and an Unrelated prime.
We also observed a reliable Prime effect for the tradi-tional word-based model that did not use any syntactic information (F(1, 129) = 106.69,MSE = 2.92, p < 0.01).
There was no main effect of Lexical Relation for either model(F(5, 129) < 1).The fact that the analysis of variance has produced a significant F for the two modelsonly indicates that there are differences between the Related and Unrelated prime-targetmeans that cannot be attributed to error.
Ideally, we would like to compare the twomodels, for example, by quantifying the magnitude of the Prime effect.
Eta-squared(?2) is a statistic7 often used to measure the strength of an experimental effect (Howell2002).
It is analogous to r2 in correlation analysis and represents how much of theoverall variability in the dependent variable (in our case distance in semantic space)can be explained or accounted for by the independent variable (i.e., Prime).
The use of?2 allowed us to perform comparisons between models (the higher the ?2, the better themodel).
The Prime effect size was greater for the dependency model, which obtained an?2 of 0.332 compared to the word-based model whose ?2 was 0.284.
In other words, thedependency model accounted for 33.2% of the variance, whereas the word-based modelaccounted for 28.4%.To establish whether the priming effect observed by the dependency model holdsacross all relations, we next conducted separate ANOVAS for each type of Lexical Rela-tion.
The ANOVAS revealed reliable priming effects for all six relations.
Table 4 shows themean distance values for each relation in the Related and Unrelated condition and thePrime Effect size for the dependency model.
The latter was estimated as the differencein distance values between related and unrelated prime-target pairs (asterisks indicatewhether the difference is statistically significant, according to a two-tailed paired t-test).For comparison, we also report the Prime Effect size that McDonald and Brew (2004)obtained in their simulation.To summarize, our results indicate that a semantic space model defined over depen-dency relations simulates direct priming across a wide range of lexical relations.
Fur-thermore, our model obtained a priming effect that is not only reliable but also greaterin magnitude than the one obtained by a traditional word-based model.
Although weused a less sophisticated model than McDonald and Brew (2004), without an updateprocedure and an explicit computation of expectations, we obtained priming effectsacross all relations.
In fact, we consider the two models complementary.
McDonald andBrew?s model could straightforwardly incorporate syntax-based semantic spaces likethe ones defined in this article.We next examine synonymy, a single lexical relation, in more detail and assesswhether the proposed dependency model can reliably distinguish synonyms from non-synonyms.
This capability may be exploited to automatically generate corpus-based7 Eta-squared is defined as ?2 =SSeffectSStotalwhere SSeffect is the variance (sum of squares) created by oneparticular effect (Prime in our case) and SStotal is the variance of all observations together.182Pad?
and Lapata Dependency-Based Semantic SpacesTable 4Mean distance values for Related and Unrelated prime?target pairs; Prime Effect size(= Related ?
Unrelated) for the dependency model and ICE.Lexical Relation N Related Unrelated Effect Effect(dependency) (ICE)Synonymy 23 0.267 0.102 0.165** 0.063Superordination 21 0.227 0.121 0.106** 0.067Category coordination 23 0.256 0.119 0.137** 0.074Antonymy 24 0.292 0.127 0.165** 0.097Conceptual association 23 0.204 0.121 0.083** 0.086Phrasal association 22 0.146 0.103 0.043** 0.058**p < 0.01 (2-tailed)thesauri (Grefenstette 1994; Lin 1998a; Curran and Moens 2002) or used in applicationsthat utilize semantic similarity.
Examples include contextual spelling correction (Jonesand Martin 1997), summarization (Barzilay 2003; Erkan and Radev 2004) and questionanswering (Lin and Pantel 2001).6.
Experiment 2: Detecting SynonymyThe Test of English as a Foreign Language (TOEFL) is commonly used as a benchmarkfor comparing the merits of different similarity models.
The test is designed to assessnon-native speakers?
knowledge of English.
It consists of multiple-choice questions,each involving a target word embedded in a sentence and four potential synonyms.The task is to identify the real synonym.
An example is shown below where crossroadsis the real synonym for intersection.You will find the office at the main intersection.
(a) place (b) crossroads (c) roundabout (d) buildingLandauer and Dumais (1997) were the first to propose the TOEFL items as a test forlexical semantic similarity.
Their LSA model achieved an accuracy of 64.4% on 80 items,a performance comparable to the average score attained by non-native speakers takingthe test.
Sahlgren (2006) uses Random Indexing, a method comparable to LSA, torepresent the meaning of words and reports a 75.0% accuracy on the same TOEFL items.It should be noted that both Landauer and Dumais and Sahlgren report results on seendata, that is, parameters are optimized on the entire data set until performance haspeaked.Rather than assuming that similar words tend to occur in similar contexts, Turney(2001) and Higgins (2004) propose models that capitalize on the collocational nature ofsemantically related words.
Two words are considered similar if they tend to occur neareach other.
Turney uses pointwise mutual information (PMI) to measure the similaritybetween a target word and each of its candidate synonyms.
Co-occurrence frequenciesare retrieved from the Web using an information retrieval (IR) engine:SimilarityPMI?IR(w1, w2) =P(w1, w2)P(w1)P(w2)?
hits(w1 NEAR w2)hits(w1)hits(w2)(15)183Computational Linguistics Volume 33, Number 2where P(w1, w2) is estimated by the number of hits (i.e., number of documents) returnedby the IR engine (Turney used AltaVista) when submitting a query with the NEARoperator.8 The PMI-IR model obtained an accuracy of 72.5% on the TOEFL data set.Higgins (2004) proposes a modification to Equation (15): He dispenses with theNEAR operator by concentrating on word pairs that are strictly adjacent:SimilarityLC?IR(w1, w2) =min(hits(w1, w2), hits(w2, w1))hits(w1)hits(w2)(16)Note that Equation (16) takes the minimum number of hits for the two possible ordersw1, w2 and w2, w1 in an attempt to rule out the effects of collocations and part-of-speechambiguities.
The LC-IR (local-context information retrieval) model outperformed PMI-IR, achieving an accuracy of 81.3% on the TOEFL items.6.1 MethodFor this experiment, we used the TOEFL benchmark data set9 (80 items).
We com-pared our optimal dependency-based model against the baseline word-based model.We would also like to compare the vector-based models against Turney?s (2001) andHiggins?
(2004) collocational models.
Ideally, such a comparison should take place onthe same corpus.
Unfortunately, downloading and parsing a snapshot of the whole Webis outside the scope of the present article.
Instead, we assessed the performance of thesemodels on the BNC, using a search engine which simulated AltaVista.
Specifically, weindexed the BNC using Glimpse (Manber and Wu 1994), a fast and flexible indexing andquery system.10 Glimpse supports approximate and exact matching, Boolean queries,wild cards, regular expressions, and many other options.For the PMI-IR model, we estimated hits(w1 NEAR w2) by retrieving and countingthe number of documents containing w1 and w2 or w2 and w1 in the same sentence.The target w1 and its candidate synonym w2 did not have to be adjacent, but thenumber of the intervening words was bounded by the length of the sentence.
Thefrequencies hits(w1) and hits(w2) were estimated similarly by counting the number ofdocuments in which w1 and w2 occurred.
Ties were resolved by randomly selectingone of the candidate synonyms.
The BNC proved too small a corpus for the LC-IR model, which relies on w1 and w2 occurring in directly adjacent positions.
Thisis not a problem when frequencies are obtained from Web-scale corpora, but in ourcase most queries retrieved no documents at all (96.6% of hits(w1, w2) and 95% ofhits(w2, w1) were zero).
We thus report only the performance of the PMI-IR model onthe BNC.The models performed a decision task similar to TOEFL test takers: They had todecide which one of the four alternatives was synonymous with the target word.
Forthe vector-based models, we computed the distance between the vector representingthe candidate word and each of the candidate synonyms, and selected the candidatewith the smallest distance.
Analogously, the candidate with the largest PMI-IR valuewas chosen for Turney?s (2001) model.
Accuracy was measured as the percentage of8 The NEAR operator constrains the search to documents that contain w1 and w2 within ten words of oneanother, in either order.9 The items were kindly provided to us by Thomas Landauer.10 The software can be downloaded from http://webglimpse.net/download.php.184Pad?
and Lapata Dependency-Based Semantic SpacesTable 5Comparison of different models on the TOEFL synonymy task.Model Corpus Accuracy (%)Random baseline ?
25.0Word-based space BNC 61.3?Dependency space BNC 73.0?
*PMI-IR BNC 61.3?PMI-IR Web 72.5?
*LC-IR Web 81.3?
*?Significantly better than random guessing.
*Significantly better than word-based vector model.right decisions the model made.
We also report the accuracy of a naive baseline modelwhich guesses synonyms at random.In this experiment, we aim to show that the superior performance of the depen-dency model carries over to a different task and data set.
We are further interested to seewhether linguistic information (represented in our case by dependency paths) makesup for the vast amounts of data required by the collocational models.
We thereforecompare directly previously proposed Web-based similarity models with BNC-basedvector space models.6.2 ResultsOur results11 are summarized in Table 5.
We used a ?2 test to determine whether thedifferences in accuracy are statistically significant.
Not surprisingly, all models are sig-nificantly better than random guessing (p < 0.01).
The dependency model significantlyoutperforms the word-based model and PMI-IR when the latter uses BNC frequencies(p < 0.05).
PMI-IR performs comparably to our model when using Web frequencies.The Web-based LC-IR numerically outperforms the dependency model, however thedifference is not statistically significant on the TOEFL data set (p < 1).
Expectedly, Web-based PMI-IR and LC-IR are significantly better than the word-based vector model andthe BNC-based PMI-IR (p < 0.05).Our results show that the dependency-based model retains its advantage over theword-based model on the synonymy detection task.
On the BNC, it also outperformsthe collocation-based PMI-IR.
Our interpretation is that the conceptually simpler collo-cation models suffer from data sparseness, whereas the dependency model can profitfrom the additional distributional information it incorporates.
It is a matter of futurework to examine whether dependency models can carry over their advantage to largercorpora.Our following experiment applies the dependency space introduced in this articleto word sense disambiguation (WSD), a task which has received much attention in NLPand is ultimately important for document understanding.11 We omit LSA (Landauer and Dumais 1997) and Random indexing (Sahlgren 2006) from our comparison,because these models were not evaluated on unseen data.185Computational Linguistics Volume 33, Number 27.
Experiment 3: Sense RankingThe ability to identify the intended reading of a polysemous word (the word sense)in context is crucial for accomplishing many NLP tasks.
Examples include lexiconacquisition, discourse parsing, or metonymy resolution.
Applications such as questionanswering or machine translation could also benefit from large scale word sense disam-biguation (WSD).Given the importance of WSD for basic NLP tasks and multilingual applications,a variety of approaches have been proposed for disambiguating word senses.
To date,most accurate WSD systems are supervised and rely on the availability of training data(see Yarowsky and Florian [2002], Mihalcea and Edmonds [2004], and the referencestherein).
Although supervised methods typically achieve better performance than theirunsupervised alternatives, their applicability is limited to those words for which sense-labeled data exists, and their accuracy is strongly correlated with the amount of labeleddata available.
Furthermore, if the distribution of senses is skewed, as is often the case,the simple heuristic of choosing the most common or predominant sense in the trainingdata (henceforth ?the first sense heuristic?)
delivers results competitive with supervisedapproaches based on local context (Hoste et al 2002).Obtaining the first sense heuristic via annotation is obviously costly and timeconsuming.
More importantly, one would expect that a word?s first sense varies acrossdomains and text genres (the word court in legal documents will most likely mean?tribunal?
rather than ?yard?).
Therefore, manual annotation must be redone for mostnew languages, domains, and sense inventories.
McCarthy et al (2004) show that theannotation bottleneck can be avoided by inferring the first sense heuristic automaticallyfrom raw text.
They argue that, even though the first sense heuristic is not a WSDmethod in itself, it can be usefully combined with context-based disambiguation meth-ods in order to alleviate the data requirements for WSD.
Their method builds on theobservation that a word?s distributionally similar neighbors often provide cues about itssenses.
In their model, sense ranking is equivalent to quantifying the degree of similaritybetween each neighbor and each sense description of a polysemous word.
The sensemost similar to the neighbors is the first sense.McCarthy et al?s (2004) approach crucially relies on the quality of the set of neigh-bors to acquire more or less accurate first senses.
In this experiment, we examinewhether the dependency-based models discussed in this article can be used for the senseranking task, thereby assessing their potential for practical NLP tasks.
The aims of ourexperiment are twofold: (1) to investigate whether our dependency-based frameworkcan be used to acquire distributionally similar words that differ in quality from thoseobtained with word-based models and (2) to observe their impact on WSD.
We first de-scribe McCarthy et al?s sense-ranking model, which forms the basis of our experiments,and then detail our methodology and results.7.1 The Sense-Ranking ModelLet w be a word, N(w) = {n1, n2, .
.
.
, nk} the set of the k most similar words to w, andS(w) = {ws1, ws2, .
.
.wsn} the set of senses for w. McCarthy et al?s (2004) model assignseach sense wsi a ?predominant sense score?
PS(wsi) as follows:PS(wsi) =?nj?N(w)simdistr(w, nj) ?simsem(wsi, nj)?wsi??S(w)simsem(wsi?
, nj)(17)186Pad?
and Lapata Dependency-Based Semantic Spaceswheresimsem(wsi, nj) = maxwsx?S(nj )simWN(wsi, wsx) (18)The predominant sense of w is simply the one with the largest PS(wsi), that is, the sensethat is maximally similar to its neighbors nj ?
N(w) according to Equations (17) and (18).This sense ranking model has four free parameters: (1) the semantic space overwhich distributionally similar words are acquired, (2) the measure of distributionalsimilarity (simdistr), (3) the number of neighbors taken into account (k), and (4) themeasure of sense similarity (simWN).
The PS score combines distributional similarityand sense similarity, taking into account both lexical knowledge gathered from corporaand the organization and structure of the lexical resource that provides the sense inven-tory.
A large number of sense similarity measures have been developed for WordNetand WordNet-like taxonomies.
These vary from simple edge-counting (Rada, Mili, andBicknell 1989) to attempts to factor in peculiarities of the network structure by consid-ering link direction (Hirst and St-Onge 1998), relative depth (Leacock and Chodorow1998), and density (Agirre and Rigau 1996).
A number of hybrid approaches have alsobeen proposed that combine WordNet with corpus statistics (Resnik 1995; Jiang andConrath 1997).McCarthy et al (2004) use their ranking model to automatically infer the first sensesof all nouns attested in SemCor, a subset of the Brown corpus containing 23,346 lemmasannotated with senses according to WordNet 1.6.
They acquire distributionally similarwords from a large collection of dependency relations obtained from the written partof the BNC (90 million words) using Briscoe and Carroll?s (2002) parser.
Their modelconsiders solely dependency paths of length one (see context selection function (5)),and is restricted to a small set of dependency relations (verb?subject, verb?object,noun?noun, and adjective?noun).
They employ a basis mapping function that mapspaths to (r, w) tuples (see Equation (9)) and Lin?s information-theoretic similaritymeasure (see Equation (3)).
They obtained a type-level accuracy of 54% (a randombaseline achieved 32%) at recovering the most prevalent sense (using 50 neighborsand either Lesk?s [1986] or Jiang and Conrath?s [1997] measures).
They also used atoken disambiguator that always defaults to the automatically acquired first sense andobtained a token-level disambiguation accuracy of 48% for Lesk (50 neighbors) and46% for Jiang and Conrath (50 neighbors).
Their baseline for this task was 24%.7.2 MethodWe replicated McCarthy et al?s (2004) study using our optimal dependency-basedmodel (medium context selection, length path value functions, 2,000 basis elements, Lin?s[1998a] similarity measure, and the log-likelihood association function) and the baselineword-based model.
We used Equation (17) to find the first sense for all polysemousnouns in SemCor (according to WordNet 1.6).
Following McCarthy et al, we onlyconsidered polysemous nouns attested in SemCor with a frequency > 2, and in ourparsed version of the BNC with a frequency ?
10.
The total number of nouns afterapplying the frequency cutoffs was 2,75012 and the average sense ambiguity was 4.5512 McCarthy et al (2004) use 2,595 nouns.
The slight variation is due to the different parsers employed inthe two studies.
Recall that we obtain dependency relations using MINIPAR (Lin 1998b), whereasMcCarthy et al employ Briscoe and Carroll?s (2002) parser.187Computational Linguistics Volume 33, Number 2(the most ambiguous word had 30 senses, and least ambiguous 2).
For each one of the2,750 nouns, we generated the set of its distributionally similar neighbors from the setof the nouns in the intersection between the BNC and WordNet (15,656 in total).We did not experiment in detail with WordNet-based similarity measures or withthe number of distributionally similar neighbors required for the computation of theprevalence score.
McCarthy et al (2004) undertook a thorough comparison and ob-tained best results with 50 neighbors using Lesk?s (1986) and Jiang and Conrath?s (1997)measures.
They argue that the latter measure is more efficient for large scale WSD anduse it exclusively in all subsequent work (McCarthy et al 2004; Koeling, McCarthy, andCarroll 2005).
We thus adopted the parameters that McCarthy et al found to be optimal,namely 50 neighbors and Jiang and Conrath?s similarity measure, which we will brieflydescribe.Jiang and Conrath?s (1997) measure estimates the similarity between two wordsenses by combining taxonomic information with corpus data.
It is based on the notionof information content (IC) of a WordNet synset s. IC is defined as the negative log-likelihood of s:IC(s) = ?
log p(s) (19)Jiang and Conrath define a distance measure that combines IC with edge countingby taking into account local density, node depth, and link type.
They introduce twoparameters, ?
and ?, that control the influence of node depth and density, respectively.Setting ?
to zero and ?
to one, their measure simplifies to:Djcn(s1, s2) = log p(s1) + log p(s2) ?
2 ?
log p(lso(s1, s2)) (20)where lso(s1, s2) is the lowest super-ordinate (most specific common subsumer) ofsynsets (that is, senses) s1 and s2.
We used the WordNet Similarity Package (Pedersen,Patwardhan, and Michelizzi 2004), which provides an implementation of Jiang andConrath?s (1997) measure (version 0.06).13 We re-estimated the IC counts from the BNCbecause those provided with the package are derived from the manually annotatedSemCor and would positively bias our results.We replicated McCarthy et al?s (2004) procedure for evaluating the acquired pre-dominant sense against the manually annotated SemCor.
We use the following notationto describe our evaluation measures: W is the set of all word types (|W| = 2, 570) andWps is the set of word types with a predominant sense, that is, with a sense that is morefrequent than the second sense in SemCor (|Wps| = 2, 338).
S(w) is the set of WordNetsenses for word type w, and T(w) the set of all tokens of w. Finally, we use pssc(w) andpsr(w) to refer to the predominant sense of word w according to SemCor and the senseranking model, respectively, and sensesc(t) to denote the sense annotated in SemCor fora particular token t.We first evaluate our models performance on the sense ranking task (Accsr),namely, on identifying the predominant sense for a word type, if one exists:Accsr =|{w ?
Wps | pssc(w) = psr(w)}||Wps|(21)13 The package is publicly available from http://www.d.umn.edu/?tpederse/similarity.html.188Pad?
and Lapata Dependency-Based Semantic SpacesA baseline for the sense ranking task can be easily defined by selecting a sense at ran-dom for each word type from its sense inventory and assuming that this is the first sense:Randomsr = 1|Wps|?w ?Wps1|S(w)| (22)Like McCarthy et al (2004), we also assessed the word sense disambiguation potential(Accwsd) of the automatically acquired first senses for each word token.
We assignedthe predominant sense (according to the ranking model) to every noun token, withouttaking its context into account, and measured the ratio of tokens for which the firstsense given by the ranking model is identical to the SemCor gold standard sense:Accwsd =?w?W|{t ?
T(w) | psr(w) = sensesc(t)}|?w?W|T(w)| (23)A baseline disambiguator can be defined by assigning a random sense to each token:Randomwsd = 1?w?W|T(w)|?w?W|T(w)| 1|S(w)| (24)7.3 ResultsTable 6 shows the results for the optimal dependency-based model, the random base-line, the baseline word-based model, and McCarthy et al?s (2004) state of the art model.As an upper bound, we report WSD accuracy when defaulting to the first (i.e., mostfrequent) sense provided by SemCor.
All models use 50 nearest neighbors and Jiang andConrath?s (1997) WordNet-based semantic similarity measure.
As far as distributionalsimilarity is concerned, our dependency model employs Lin?s (1998a) measure andso do McCarthy et al, whereas the traditional word co-occurrence model uses cosine.Our model differs from McCarthy et al in the context selection, path value, and basismapping functions (see the subsequent discussion).
We used a ?2 test to determine ifthe differences in performance are statistically significant.
Note that we have a slightlydifferent set of nouns from McCarthy et al (2004); this is due to the use of a differentTable 6Results on sense ranking and WSD tasks, using 50 neighbors and the Jiang and Conrath (1995)distance measure.Models Accsr AccwsdRandom baseline 31.0 25.4Word-based space 49.3?
49.9?$Dependency space 54.3?
* 54.3?
* $McCarthy et al 54.0?
* 46.0?Upper bound ?
67.0?Significantly better than random baseline.
*Significantly better than word-based model.$Significantly better than McCarthy et al189Computational Linguistics Volume 33, Number 2parser and a larger corpus.
We work on the assumption that this difference is negligible.We use a set of diacritics to denote statistical significance, explanations for which areprovided in Table 6.We first consider the predominant sense acquisition task (Accsr).
Table 6 showsthat all models significantly outperform the random baseline (p < 0.01).
Furthermore,both the dependency-based model and McCarthy et al (2004) significantly outperformthe word-based model.
The two dependency models yield comparable performances(p < 1).
For the WSD task, we also observe that all models significantly outperformthe random baseline (p < 0.01).
Our dependency model significantly outperforms theword-based model and McCarthy et al (p < 0.01).
The word-based model performssignificantly better than McCarthy et al (p < 0.01).
All models expectedly performworse than the upper bound (p < 0.01).An interesting observation is that our dependency model outperforms McCarthyet al (2004) by a large margin (8.3%) on the WSD task, whereas the two models yieldcomparable performances on sense ranking.
Also, the word-based model performssignificantly better than McCarthy et al on WSD, while it is significantly worse thanMcCarthy et al in sense ranking.
This indicates that the words for which each modeldelivers the first sense correctly are different.
Indeed, inspection of the first senseassignments reveals that McCarthy et al and our dependency model have only 35.7%nouns in common for which they predict the first sense correctly.
McCarthy et alhas 34.8% nouns in common with the word-based model, which in turn has 40.3%nouns in common with our dependency model.To follow up on this observation, we investigated how ambiguity and word fre-quency influence the performance of our ranking model.
In theory, an automaticallyacquired sense ranker should have a good accuracy on all ambiguous words in orderto do well on WSD.
However, in practice the sense ranker?s performance dependscrucially on its ability to correctly predict the first sense for highly frequent and highlyambiguous words.
An additional complicating factor is the sense distribution of thewords in question.
For words whose sense distributions are not particularly skewed,getting the first sense wrong will not be entirely detrimental as long as the WSD methodmisclassifies relatively frequent senses as predominant.Take, for example, the word corner, which is attested 61 times in Semcor and has11 senses according to WordNet 1.6.
Among these, sense 1 is found seventeen times,sense 2 fifteen, sense 3 ten, and sense 4 nine (all other senses have considerably smallerfrequencies).
Now suppose that the sense ranking method wrongly identifies sense 2as the predominant sense for corner.
Using this sense, our WSD system will correctlydisambiguate 24.6% of the instances of corner in Semcor, despite the fact that it will notreceive any credit for identifying the first sense.
Note that the right first sense wouldyield only slightly better accuracy (i.e., 27.4%).We grouped all ambiguous noun tokens in SemCor into five frequency bands (fre-quencies were estimated from the BNC as it constitutes a larger sample of English thanSemcor).
Table 7 illustrates our models?
sense ranking and WSD accuracy according tothese bands; we also list the average sense ambiguity and number of word types for eachband.
As can be seen, our dependency model obtains consistently good performance onboth tasks, even in the high ambiguity bands (Bands 1,000?5,000 and 5,000+, highlightedin Table 7).
The obtained accuracies are well above the baseline of choosing a sense atrandom (for example, an average ambiguity of 8.3 in the 5000+ band corresponds toa random baseline of 12% in the sense ranking task).
This is not entirely surprising;frequent words are represented by more reliable vectors.
As a result, the acquiredneighbors are of higher quality, which counteracts the increased ambiguity.190Pad?
and Lapata Dependency-Based Semantic SpacesThe results in Table 7 furthermore reveal that WSD performance exceeds senseranking accuracy in high-frequency bands (most notably in Band 1,000?5,000), whichseems counterintuitive.
This effect can be explained by taking into account the observedsense frequencies and the types of errors introduced by our model in these bands.
Thedistribution of senses in the high-frequency bands tends to be less skewed, at least ac-cording to Semcor (82% of nouns in Band 1,000?5,000 and 65% in Band 5,000+ have afirst sense with frequency <50).
Our model?s mistakes are often ?near misses?, that is,the first and second sense ranks are flipped.
Specifically, near misses are observed for25% of the noun types in Band 1,000?5,000, and 15% in Band 5,000+.
Now, for nounswith non-skewed sense distributions, disambiguating with the second sense will boostWSD accuracy even though this is not the case for sense ranking (see the previousdiscussion).Our results show that semantic space models defined according to the frameworkpresented in this article can be successfully used for the automatic acquisition of firstsenses from raw text.
We obtained results similar to McCarthy et al (2004) on the senseranking task and demonstrated that our model performs significantly better on WSD.Furthermore, it outperformed a word-based semantic space on both tasks.
Our modeldiffers from McCarthy et al in three important ways: (a) following our terminology,they use a semantic space with the minimum context selection (paths of length one)and plain path value (no path weighting) functions, whereas our model employs themedium content selection and length path value functions; (b) their space is constructedover a limited set of dependency paths, namely subject, object, and adjective-nounmodification relations, whereas our model uses a wider range of relations includinginformation about tense (for example, whether a complement is finite or not), relativi-sation, etc.
(see Section 4.2 for details); and (c) their basis mapping function maps pathsto tuples whereas we employ a word-based function and restrict the dimensions of thespace to the 2,000 most frequent elements (McCarthy et al do not employ any cutoffs).Furthermore, they used a slightly smaller corpus (only the written part of the BNC,amounting to 90% of the total corpus) and a different parser (Briscoe and Carroll 2002).Although replicating our study with Briscoe and Carroll?s parser (2002) is outside ofthe scope of this article, we should note that the two parsers yield comparable perform-ances and employ a similar inventory of dependency relations (see Curran (2004) formore discussion).
We thus suspect that differences in WSD accuracy cannot be uniquelyattributed to parser performance.
We can, however, assess whether the difference is dueto corpus size by examining its effect on the performance of our model.
If it is indeedsensitive to corpus size, we would expect a relatively large drop in performance whenTable 7Sense ranking and WSD accuracy for the dependency-based model as word frequency andaverage sense ambiguity are varied.FBand AvgAmbig Types accsr accwsd<50 3.29 174 0.53 0.4650?200 3.60 489 0.54 0.49200?1,000 4.29 1,014 0.57 0.541,000?5,000 5.65 583 0.51 0.575,000+ 8.32 78 0.50 0.51FBand = frequency band; AvgAmbig = average WordNet sense ambiguity within frequencyband; Types = number of noun types within frequency band.191Computational Linguistics Volume 33, Number 2our semantic space is built on smaller corpora.
We randomized the order of sentencesin the BNC and constructed semantic spaces on data sets progressively increasing insize: The first space was constructed from 5% of the BNC, the next from 10%, and soon.
We tested each model on the SemCor data (see Section 7.2).
Figure 7 shows theresulting learning curves.
When the dependency model is constructed on 5% of theBNC, it delivers a WSD accuracy of 51%, which eventually increases to 54.3% whenthe entire corpus is used.
This result indicates that the model performs well whentrained on a small corpus and that its good performance cannot be attributed solelyto corpus size.
However, it also suggests that a large increase in corpus size is necessaryto obtain substantial improvements with the present sense ranking strategy, whichuses distributional similarity as a corrective for taxonomy-based similarity: Accuracyincreases by approximately 4% when our corpus size increases by a factor of 20.We believe that the differences in performance between the two models are largelydue to differences in the basis mapping function.
Because McCarthy et al (2004) use allavailable basis elements, their semantic space grows linearly with vocabulary (i.e., cor-pus) size.
Each target word is represented by a set of ?features?
?relation?word pairswith a non-zero occurrence frequency?which may vary widely between target words.In contrast, our model defines a modest number of basis elements (2,000) which areshared between all target words.
The resulting representation is a vector space which isless sparse and the resulting neighbors capture more succinctly the semantic propertiesof words.
Additional evidence comes from the performance of the word-based model,which also uses a word basis mapping function and a fixed number of dimensions (500words).
Although this model does not incorporate syntactic information in any way,it manages to outperform McCarthy et al on the WSD task.
In sum, we attribute thesuperior performance of the vector-based model to two key factors: low dimensionalityFigure 7Learning curve for the dependency-based model on a randomized version for the BNC: accuracyof predominant sense acquisition (solid) and WSD (dashed) with varying corpus size.192Pad?
and Lapata Dependency-Based Semantic Spaces(as seen by the comparison to McCarthy et al) and the incorporation of linguisticknowledge (as seen by the comparison to the word-based model).8.
General DiscussionIn this article, we presented a general framework for the construction of semantic spacemodels.
The framework operates on paths of dependency relations, allowing linguisticknowledge to guide the construction of semantic spaces.
It extends previous workon traditional word-based semantic space models as well as syntax-based models byproviding a principled way for defining the context and the dimensions of the semanticspace.
More specifically, we isolated three important parameters of space construction:the context selection function, the basis mapping function, and the path value function.In combination, these three functions determine which paths (e.g., local or distant),dimensions (e.g., words, parts of speech or word?relation tuples), and dependencyrelations (e.g., subjects, objects) contribute towards the construction of a semantic space.We evaluated our framework on tasks relevant for NLP and cognitive science andcompared it against state-of-the-art models.
Experiment 1 revealed that semantic spacemodels defined over dependency relations adequately simulate semantic priming.
Ex-periments 2 and 3 examined the usefulness of our framework for NLP: we used ourmodel to detect synonymy relations and to automatically acquire prevalent senses forpolysemous words.
In all cases, syntactically enriched models outperformed traditionalword-based models that did not take account of syntax.Our strategy in the present study was to define a small number of generic parame-terizations, evaluate the resulting models on a development set, and select a broadlyoptimal model for testing on unseen data.
Therefore, our models were not specificallytuned for the tasks at hand and we have only explored a relatively small subset of theparameter space.
Our examination of different parameter combinations in Section 4.2revealed that medium syntactic contents yield consistently better performance whencombined with a path value function that penalizes longer paths (length).
An importantavenue for future work concerns defining more fine-grained path value functions.
Ourresults show that a path value function inspired by the obliqueness hierarchy deliversworse results than the linguistically naive length function.
Alternatively, we could definea function that combines gram-rel with length, or more generally learn a weightingscheme for paths by optimizing some objective function.Our experiments concentrated on spaces that used solely a basis mapping functionthat maps dependency paths to words.
It should also be interesting to experiment withdifferent types of basis mapping functions.
For example, we could experiment withmore coarse-grained functions based on parts-of-speech or more fine-grained ones suchas the relation?word pairs used by McCarthy et al (2004).
We would also like to observethe impact of singular value decomposition (SVD) on our semantic spaces along thelines of Kanejiya, Kumar, and Prasad?s (2003) cognitive modeling work.
They use SVDto reduce the dimensionality of a semantic space that uses (word, part-of-speech) pairsas basis elements, obtaining better coverage compared with an LSA space constructedover word co-occurrences.
Further studies must examine the effect of parser quality onthe obtained co-occurrences, and the influence of the chosen similarity measure.We have just scratched the surface of the possibilities for the framework discussedin this article.
The potential applications are many and varied both for cognitive scienceand NLP.
Our syntactically enriched models retain the simplicity of word co-occurrencemodels while allowing for the role of syntactic structure to influence the representa-tion of the semantic space.
The resulting vectors have a higher degree of linguistic193Computational Linguistics Volume 33, Number 2plausibility?it is not mere lexical association that accounts for the meaning of wordsbut rather their lexical and syntactic dependencies.
Arguably, this property holds greatpromise for languages less configurational than English.
A prediction that we intend totest in the future is that syntax-based semantic space models should be able to representmeaning more adequately than traditional word-based models for languages that allowconstituent scrambling (e.g., German) or have free word order (e.g., Czech).It remains to be seen whether our models can capture the wide range of data thattraditional and LSA-based models have accounted for.
Possible future experiments in-clude mediated priming (Lowe and McDonald 2000) and multiple priming (McDonaldand Brew 2004), intelligent tutoring (Kanejiya, Kumar, and Prasad 2003), and coherencerating (Foltz, Kintsch, and Landauer 1998).
A number of NLP tasks could also benefitfrom the framework presented in this article.
Examples include word sense discrimi-nation (Lin 1998a; Sch?tze 1998), automatic thesaurus construction (Grefenstette 1994;Curran and Moens 2002), automatic clustering, lexicon acquisition, and in generalsimilarity-based approaches to NLP.Appendix A.
Context Selection FunctionsIn what follows we present the context selection functions we used in our experiments.These are encoded as non-lexicalized path templates and are distributed as part of thesoftware package that implements our dependency-based semantic space framework(see Section 3.7 for details).
Each context selection function cont is represented by aset of path templates, Temp(cont).
Each path template directly corresponds to a pathlabel sequence.
Path templates are denoted by a comma-separated sequence of one ormore edge labels; each edge label is a colon-separated triple POS1:relation:POS2 (seeDefinition 1).
The semantics of a set of path templates Temp(c) is as follows: For a targetword t and a context selection function c, the context of t consists of all paths ?t (i.e., allpaths anchored at t) so that there is a path template temp ?
Temp(c) that matches thelabel sequence l(?t).Minimum:A:amod:VA:mod:AA:mod:AA:mod:NA:mod:PrepA:mod:VA:subj:NN:conj:NN:gen:NN:mod:AN:mod:PrepN:nn:NN:obj:VN:pcomp-n:PrepN:subj:AN:subj:NN:subj:V(null):lex-mod:VPrep:mod:APrep:mod:NPrep:mod:VPrep:pcomp-n:NV:amod:AV:lex-mod:(null)V:mod:AV:mod:PrepV:obj:NV:subj:NMedium contains all minimumtemplates and:A:mod:N,N:lex-mod:(null)A:mod:N,N:nn:NA:subj:N,N:lex-mod:(null)A:subj:N,N:nn:NN:conj:N,N:lex-mod:(null)N:conj:N,N:nn:NN:gen:N,N:lex-mod:(null)N:gen:N,N:nn:NN:nn:N,N:conj:NN:nn:N,N:conj:N,N:nn:NN:nn:N,N:gen:N194Pad?
and Lapata Dependency-Based Semantic SpacesN:nn:N,N:gen:N,N:nn:NN:nn:N,N:mod:AN:nn:N,N:mod:PredN:nn:N,N:obj:VN:nn:N,N:subj:AN:nn:N,N:subj:V(null):lex-mod:N,N:conj:N(null):lex-mod:N,N:conj:N,N:lex-mod:(null)(null):lex-mod:N,N:gen:N(null):lex-mod:N,N:gen:N,N:lex-mod:(null)(null):lex-mod:N,N:mod:A(null):lex-mod:N,N:mod:Pred(null):lex-mod:N,N:obj:V(null):lex-mod:N,N:subj:A(null):lex-mod:N,N:subj:VPrep:mod:N,N:lex-mod:(null)Prep:mod:N,N:nn:NV:obj:N,N:lex-mod:(null)V:obj:N,N:nn:NV:subj:N,N:lex-mod:(null)V:subj:N,N:nn:NMaximum contains all mediumtemplates and:A:mod:A,A:mod:N,N:lex-mod:(null)A:mod:A,A:mod:N,N:nn:NA:mod:Prep,Prep:pcomp-n:N,N:lex-mod:(null)N:mod:Prep,Prep:pcomp-n:N,N:lex-mod:(null)N:mod:Prep,Prep:pcomp-n:N,N:nn:NN:nn:N,N:mod:A,A:mod:AN:nn:N,N:mod:Prep,Prep:pcomp-n:NN:nn:N,N:mod:Prep,Prep:pcomp-n:N,N:nn:NN:nn:N,N:obj:V,V:subj:NN:nn:N,N:obj:V,V:subj:N,N:nn:NN:nn:N,N:pcomp-n:PrepN:nn:N,N:pcomp-n:Prep,Prep:mod:NN:nn:N,N:pcomp-n:Prep,Prep:mod:N,N:nn:NN:nn:N,N:subj:V,V:obj:NN:nn:N,N:subj:V,V:obj:N,N:nn:NN:nn:N,V:s:C,C:fc:VN:obj:V,V:subj:N,N:lex-mod:(null)N:obj:V,V:subj:N,N:nn:NN:pcomp-n:Prep,Prep:mod:N,N:lex-mod:(null)N:pcomp-n:Prep,Prep:mod:N,N:nn:NN:subj:V,V:obj:N,N:lex-mod:(null)N:subj:V,V:obj:N,N:nn:N(null):lex-mod:N,N:mod:A,A:mod:A(null):lex-mod:N,N:mod:Prep,Prep:pcomp-n:N(null):lex-mod:N,N:mod:Prep,Prep:pcomp-n:N,N:lex-mod:(null)(null):lex-mod:N,N:obj:V,V:subj:N(null):lex-mod:N,N:obj:V,V:subj:N,N:lex-mod:(null)(null):lex-mod:N,N:pcomp-n:Pred,Prep:mod:A(null):lex-mod:N,N:pcomp-n:Prep(null):lex-mod:N,N:pcomp-n:Prep,Prep:mod:N(null):lex-mod:N,N:pcomp-n:Prep,Prep:mod:N,N:lex-mod:(null)(null):lex-mod:N,N:pcomp-n:Prep,Prep:mod:V(null):lex-mod:N,N:rel:C,C:i:V(null):lex-mod:N,N:subj:V,V:obj:N(null):lex-mod:N,N:subj:V,V:obj:N,N:lex-mod:(null)(null):lex-mod:N,V:s:C,C:fc:VPrep:pcomp-n:N,N:lex-mod:(null)Prep:pcomp-n:N,N:nn:NV:fc:C,C:s:N,N:lex-mod:(null)V:fc:C,C:s:N,N:nn:NV:i:C,C:rel:N,N:lex-mod:(null)V:mod:Prep,Prep:pcomp-n:N,N:lex-mod:(null)AcknowledgmentsWe are grateful to Diana McCarthy forproviding us with the results of her systemon our data.
We are also grateful to fouranonymous reviewers for ComputationalLinguistics whose feedback helped tosubstantially improve the present article.
Wealso thank Colin Bannard, Gemma Boleda,Amit Dubey, Katrin Erk, Frank Keller, UlrikePad?, and Caroline Sporleder for usefulcomments and suggestions.
A preliminaryversion of this work was published in theproceedings of ACL 2003; we thank theanonymous reviewers of that paper for theircomments.ReferencesAgirre, Eneko and German Rigau.
1996.Word sense disambiguation usingconceptual density.
In Proceedingsof the 16th International Conference onComputational Linguistics, pages 16?22,Copenhagen, Denmark.Banerjee, Satanjeev and Ted Pedersen.
2003.Extended gloss overlaps as a measureof semantic relatedness.
In Proceedingsof the 18th International Joint Conferenceon Artificial Intelligence, pages 805?810,Acapulco, Mexico.Bannard, Colin, Timothy Baldwin, andAlex Lascarides.
2003.
A statistical195Computational Linguistics Volume 33, Number 2approach to the semantics ofverb-particles.
In Proceedings of the ACLWorkshop on Multiword Expressions:Analysis, Acquisition and Treatment,pages 65?72, Sapporo, Japan.Barzilay, Regina.
2003.
Information Fusion forMulti-Document Summarization:Paraphrasing and Generation.Ph.D.
thesis,Columbia University, New York.Berry, Michael W., Susan T. Dumais, andGavin W. O?Brien.
1994.
Using linearalgebra for intelligent informationretrieval.
SIAM Review, 37(4):573?595.Briscoe, Ted and John Carroll.
2002.
Robustaccurate statistical annotation of generaltext.
In Proceedings of the 3rd InternationalConference on Language Resources andEvaluation, pages 1499?1504, Las Palmas,Canary Islands.Budanitsky, Alexander and Graeme Hirst.2001.
Semantic distance in WordNet:An experimental, application-orientedevaluation of five measures.
In Proceedingsof ACL Workshop on WordNet andOther Lexical Resources, pages 29?34,Pittsburgh, PA.Burnard, Lou, 1995.
Users Guide for the BritishNational Corpus.
British National CorpusConsortium, Oxford UniversityComputing Service, Oxford, UK.Choi, Freddy, Peter Wiemer-Hastings,and Johanna Moore.
2001.
LatentSemantic Analysis for text segmentation.In Proceedings of the 6th Conference onEmpirical Methods in Natural LanguageProcessing, pages 109?117, Seattle, WA.Curran, James R. 2004.
From Distributionalto Semantic Similarity.
Ph.D. thesis,University of Edinburgh.Curran, James R. and Marc Moens.
2002.Scaling context space.
In Proceedings of the40th Annual Meeting of the Association forComputational Linguistics, pages 231?238,Philadelphia, PA.Dunning, Ted.
1993.
Accurate methods forthe statistics of surprise and coincidence.Computational Linguistics, 19(1):61?74.Erkan, G?nes and Dragomir R. Radev.2004.
Lexrank: Graph-based centrality assalience in text summarization.
Journal ofArtificial Intelligence Research, 22:457?479.Fillmore, Charles.
1965.
Indirect ObjectConstructions and the Ordering ofTransformations.
Mouton, The Hague.Fodor, Janet Dean.
1995.
Comprehendingsentence structure.
In Lila R. Gleitman andMark Liberman, editors, Invitation toCognitive Science, volume 1.
MIT Press,Cambridge, MA, pages 209?246.Foltz, Peter W., Walter Kintsch, andThomas K. Landauer.
1998.
Themeasurement of textual coherencewith latent semantic analysis.
DiscourseProcess, 15:285?307.Goldberg, Adele.
1995.
Constructions.Chicago University Press, Chicago.Golub, Gene H. and Charles F. Van Loan.1989.
Matrix Computations.
Johns HopkinsSeries in the Mathematical Sciences.
JohnsHopkins University Press, Baltimore,3rd edition.Green, Georgia.
1974.
Semantics and SyntacticRegularity.
Indiana University Press,Bloomington.Grefenstette, Gregory.
1994.
Explorations inAutomatic Thesaurus Discovery.
KluwerAcademic Publishers, Dordrecht.Gropen, Jess, Steven Pinker, MichelleHollander, Richard Goldberg, andRonald Wilson.
1989.
The learnabilityand acquisition of the dative alternation.Language, 65(2):203?257.Harris, Zellig.
1968.
Mathematical Structuresof Language.
Wiley, New York.Henderson, James, Paola Merlo, Ivan Petroff,and Gerold Schneider.
2002.
Usingsyntactic analysis to increase efficiency invisualizing text collections.
In Proceedingsof the 19th International Conference onComputational Linguistics, pages 335?341,Taipei, Taiwan.Higgins, Derrick.
2004.
Which statisticsreflect semantics?
Rethinking synonymyand word similarity.
In Proceedings of theInternational Conference on LinguisticEvidence, pages 265?284, T?bingen,Germany.Hirst, Graeme and David St-Onge.
1998.Lexical chains as representations ofcontext for the detection and correction ofmalapropisms.
In Christiane Fellbaum,editor, WordNet: An Electronic LexicalDatabase.
MIT Press, Cambridge, MA,pages 305?332.Hodgson, James M. 1991.
Informationalconstraints on pre-lexical priming.Language and Cognitive Processes, 6:169?205.Hoste, V?ronique, Iris Hendrickx, WalterDaelemans, and Antal van den Bosch.2002.
Parameter optimization formachine-learning of word sensedisambiguation.
Language Engineering,8(4):311?325.Howell, David C. 2002.
Statistical Methodsfor Psychology.
Duxbury, Pacific Grove,CA, 5th edition.Jackendoff, Ray.
1983.
Semantic and Cognition.The MIT Press, Cambridge, MA.196Pad?
and Lapata Dependency-Based Semantic SpacesJiang, Jay J. and David W. Conrath.
1997.Semantic similarity based on corpusstatistics and lexical taxonomy.
InProceedings of the 10th InternationalConference on Research in ComputationalLinguistics, pages 19?33, Taipei, Taiwan.Jones, Michael P. and James H. Martin.1997.
Contextual spelling correction usingLatent Semantic Analysis.
In Proceedingsof the 5th Conference on Applied NaturalLanguage Processing, pages 166?173,Washington, DC.Kanejiya, Dharmendra, Arun Kumar,and Surendra Prasad.
2003.
Automaticevaluation of students?
answers usingsyntactically enhanced LSA.
In Proceedingsof the HLT-NAACL Workshop on BuildingEducational Applications Using NaturalLanguage Processing, pages 53?60,Edmonton, Canada.Keenan, Edward and Bernard Comrie.
1977.Noun phrase accessibility and universalgrammar.
Linguistic Inquiry, 8:62?100.Kilgarriff, Adam.
2001.
Comparing corpora.International Journal of Corpus Linguistics,6(1):97?133.Koeling, Rob, Diana McCarthy, and JohnCarroll.
2005.
Domain-specific sensedistributions and predominant senseacquisition.
In Proceedings of the JointHuman Language Technology Conference andConference on Empirical Methods in NaturalLanguage Processing, pages 419?426,Vancouver, Canada.Landauer, Thomas and Susan T. Dumais.1997.
A solution to Plato?s problem:The latent semantic analysis theory ofacquisition, induction, and representationof knowledge.
Psychological Review,104(2):211?240.Leacock, Claudia and Martin Chodorow.1998.
Combining local context andWordNet similarity for word senseidentification.
In Christiane Fellbaum,editor, WordNet: An Electronic LexicalDatabase.
MIT Press, Cambridge, MA,pages 265?283.Lee, Lillian.
1999.
Measures of distributionalsimilarity.
In Proceedings of the 37th AnnualMeeting of the Association for ComputationalLinguistics, pages 25?32, College Park, MA.Lesk, Michael.
1986.
Automatic sensedisambiguation: How to tell a pine conefrom an ice cream cone.
In Proceedings ofthe 1986 Special Interest Group inDocumentation, pages 24?26, New York.Levin, Beth.
1993.
English Verb Classes andAlternations: A Preliminary Investigation.University of Chicago Press, Chicago.Levy, Joseph P. and John A. Bullinaria.
2001.Learning lexical properties from wordusage patterns.
In Robert M. French andJacques P.
Sougn?, editors, ConnectionistModels of Learning, Development, andEvolution.
Springer, pages 273?282.Lin, Dekang.
1998a.
Automatic retrievaland clustering of similar words.
InProceedings of the Joint Annual Meetingof the Association for ComputationalLinguistics and International Conference onComputational Linguistics, pages 768?774,Montr?al, Canada.Lin, Dekang.
1998b.
Dependency-basedevaluation of MINIPAR.
In Proceedingsof the LREC Workshop on the Evaluationof Parsing Systems, pages 234?241,Granada, Spain.Lin, Dekang.
1999.
Automatic identificationof non-compositional phrases.
InProceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics,pages 317?324, College Park, MA.Lin, Dekang.
2001.
LaTaT: Language and textanalysis tools.
In Proceedings of the 1stHuman Language Technology Conference,pages 222?227, San Francisco, CA.Lin, Dekang and Patrick Pantel.
2001.Discovery of inference rules for questionanswering.
Natural Language Engineering,7(4):342?360.Lowe, Will.
2001.
Towards a theory ofsemantic space.
In Proceedings ofthe 23rd Annual Conference of theCognitive Science Society, pages 576?581,Edinburgh, UK.Lowe, Will and Scott McDonald.
2000.
Thedirect route: Mediated priming in semanticspace.
In Proceedings of the 22nd AnnualConference of the Cognitive Science Society,pages 675?680, Philadelphia, PA.Lund, Kevin and Curt Burgess.
1996.Producing high-dimensional semanticspaces from lexical co-occurrence.Behavior Research Methods, Instruments,and Computers, 28:203?208.Manber, Udi and Sun Wu.
1994.
GLIMPSE: atool to search through entire file systems.In Proceedings of the USENIX Winter 1994Technical Conference, pages 23?32, SanFrancisco, CA.Manning, Chris and Hinrich Sch?tze.
1999.Foundations of Statistical Natural LanguageProcessing.
MIT Press, Cambridge, MA.McCarthy, Diana, Bill Keller, and JohnCarroll.
2003.
Detecting a continuumof compositionality in phrasal verbs.In Proceedings of the ACL Workshopon Multiword Expressions: Analysis,197Computational Linguistics Volume 33, Number 2Acquisition and Treatment, pages 73?80,Sapporo, Japan.McCarthy, Diana, Rob Koeling, JulieWeeds, and John Carroll.
2004.
Findingpredominant senses in untagged text.
InProceedings of the 42th Annual Meeting of theAssociation for Computational Linguistics,pages 280?287, Barcelona, Spain.McDonald, Scott.
2000.
EnvironmentalDeterminants of Lexical Processing Effort.Ph.D.
thesis, University of Edinburgh.McDonald, Scott and Chris Brew.
2004.
Adistributional model of semantic contexteffects in lexical processing.
In Proceedingsof the 42th Annual Meeting of the Associationfor Computational Linguistics, pages 17?24,Barcelona, Spain.Mihalcea, Rada and Phil Edmonds, editors.2004.
Proceedings of Senseval-3: The ThirdInternational Workshop on the Evaluation ofSystems for the Semantic Analysis of Text,Barcelona, Spain.Miltsakaki, Eleni.
2003.
The Syntax-DiscourseInterface: Effects of the Main-SubordinateDistinction on Attention Structure.
Ph.D.thesis, University of Pennsylvania.Morris, Robin K. 1994.
Lexical andmessage-level sentence context effectson fixation times in reading.
Journal ofExperimental Psychology: Learning,Memory, and Cognition, (20):92?103.Neville, Helen, Janet L. Nichol, AndrewBarss, Kenneth I. Forster, and Merrill F.Garrett.
1991.
Syntactically based sentenceprosessing classes: Evidence formevent-related brain potentials.
Journal ofCognitive Neuroscience, 3:151?165.Patel, Malti, John A. Bullinaria, andJoseph P. Levy.
1998.
Extractingsemantic representations from largetext corpora.
In Proceedings of the4th Neural Computation and PsychologyWorkshop: Connectionist Representations,pages 199?212, London.Pedersen, Ted, Siddharth Patwardhan,and Jason Michelizzi.
2004.WordNet::Similarity?measuring therelatedness of concepts.
In Proceedings ofthe Joint Human Language TechnologyConference and Annual Meeting of the NorthAmerican Chapter of the Association forComputational Linguistics, pages 38?41,Boston, MA.Pinker, Steven.
1989.
Learnability andCognition: The Acquisition of ArgumentStructure.
The MIT Press, Cambridge, MA.Rada, Roy, Hafedh Mili, and Ellen Bicknell.1989.
Development and application of ametric on semantic nets.
IEEE Transactionson Systems, Man, and Cybernetics,19(1):17?30.Resnik, Philip.
1995.
Using informationcontent to evaluate semantic similarity.In Proceedings of 14th International JointConference on Artificial Intelligence,pages 448?453, Montr?al, Canada.Rubenstein, Herbert and John B.Goodenough.
1965.
Contextual correlatesof synonymy.
Communications of the ACM,8(10):627?633.Sahlgren, Magnus.
2006.
The Word-SpaceModel: Using Distributional Analysis toRepresent Syntagmatic and ParadigmaticRelations Between Words inHigh-Dimensional Vector Spaces.
Ph.D.thesis, Stockholm University.Salton, Gerard and Michael J. McGill.
1983.Introduction to Modern Information Retrieval.McGraw-Hill, New York.Salton, Gerard and Maria Smith.
1989.
Onthe application of syntactic methodologiesin automatic text indexing.
In Proceedingsof the 12th ACM SIGIR Conference,pages 137?150, Cambridge, MA.Salton, G., A. Wang, and C. Yang.1975.
A vector-space model forinformation retrieval.
Journal of theAmerican Society for Information Science,18:613?620.Sampson, Geoffrey R. 1995.
English for theComputer.
Oxford University Press,Oxford.Sch?tze, Hinrich.
1998.
Automatic wordsense discrimination.
ComputationalLinguistics, 24(1):97?124.Strzalkowski, Tomek, editor.
1999.
NaturalLanguage Information Retrieval.
KluwerAcademic Publishers, Dordrecht.Talmy, L. 1985.
Lexicalisation patterns:Semantic structure in lexical forms.In T. Shopen, editor, LanguageTypology and Syntactic Description III:Grammatical Categories and the Lexicon.Cambrige University Press, Cambridge,pages 57?149.Tesni?re, Lucien.
1959.
Elements de syntaxestructurale.
Klincksieck, Paris.Turney, Peter D. 2001.
Mining the Webfor synonyms: PMI-IR versus LSAon TOEFL.
In Proceedings of the 12thEuropean Conference on MachineLearning, pages 491?502, Freiburg,Germany.Voorhees, Ellen M. 1999.
Natural languageprocessing and information retrieval.In 2nd School on Information Extraction(SCIE99), pages 32?48, Frascati (Rome),Italy.198Pad?
and Lapata Dependency-Based Semantic SpacesWeeds, Julie.
2003.
Measures and Applicationsof Lexical Distributional Similarity.
Ph.D.thesis, University of Sussex, UK.West, R. F. and K. E. Stanovich.
1986.
Robusteffects of syntactic structure on visualword processing.
Journal of Memory andCognition, 14:104?112.Widdows, Dominic.
2003.
Unsupervisedmethods for developing taxonomies bycombining syntactic and statisticalinformation.
In Proceedings of the JointHuman Language Technology Conferenceand Annual Meeting of the North AmericanChapter of the Association for ComputationalLinguistics, pages 197?204, Edmonton,Canada.Wiemer-Hastings, Peter and IraideZipitria.
2001.
Rules for syntax, vectorsfor semantics.
In Proceedings of the23rd Annual Conference of the CognitiveScience Society, pages 1140?1145,Edinburgh, UK.Yarowsky, David and Radu Florian.
2002.Evaluating sense disambiguation acrossdiverse parameter spaces.
NaturalLanguage Engineering, 9(4):293?310.199
