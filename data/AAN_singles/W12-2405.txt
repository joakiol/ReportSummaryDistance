Proceedings of the 2012 Workshop on Biomedical Natural Language Processing (BioNLP 2012), pages 38?46,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsAnalyzing Patient Records to Establish If and When a Patient Suffered froma Medical ConditionJames Cogley, Nicola Stokes, Joe Carthy and John DunnionSchool of Computer Science and InformaticsUniversity College DublinDublin, IrelandJames.Cogley@ucdconnect.ie {Nicola.Stokes, Joe.Carthy, John.Dunnion}@ucd.ieAbstractThe growth of digital clinical data has raisedquestions as to how best to leverage this datato aid the world of healthcare.
Promising ap-plication areas include Information Retrievaland Question-Answering systems.
Such sys-tems require an in-depth understanding of thetexts that are processed.
One aspect of thisunderstanding is knowing if a medical con-dition outlined in a patient record is recent,or if it occurred in the past.
As well as this,patient records often discuss other individu-als such as family members.
This presentsa second problem - determining if a medi-cal condition is experienced by the patient de-scribed in the report or some other individ-ual.
In this paper, we investigate the suitabil-ity of a machine learning (ML) based systemfor resolving these tasks on a previously unex-plored collection of Patient History and Phys-ical Examination reports.
Our results showthat our novel Score-based feature approachoutperforms the standard Linguistic and Con-textual features described in the related litera-ture.
Specifically, near-perfect performance isachieved in resolving if a patient experienceda condition.
While for the task of establish-ing when a patient experienced a condition,our ML system significantly outperforms theConText system (87% versus 69% f-score, re-spectively).1 IntroductionThe growth of the digitization of clinical docu-ments has fostered interest in how to best lever-age this data in providing assistance in the worldof healthcare, including novel information re-trieval (Voorhees and Tong, 2010), question an-swering (Demner-Fushman and Lin, 2007; Patrickand Li, 2011) and clinical summarization sys-tems (Feblowitz et al, 2011).Given the richness of the language found in clin-ical reports, novel systems require a deeper under-standing of this textual data.
One aspect of this un-derstanding is the assertion status of medical condi-tions (Demner-Fushman et al, 2011).
The assertionstatus of a medical condition may refer to NegationResolution, Temporal Grounding (deciding if a con-dition is currently or historical, and Condition Attri-bution (deciding if a condition is experienced by thepatient described in the report or some other individ-ual).
The focus of this paper rests on the latter twotasks of Temporal Grounding and Condition Attribu-tion as Negation has been satisfactorily addressed inChapman et al (2007).Several approaches, ranging in complexity, havebeen proposed for resolving temporal information.Hripcsak et al (2005) modeled the task as a con-straint satisfaction problem.
Another rule-based ap-proach that achieved moderate results uses regularexpressions matching occurrences of trigger terms(Chapman et al 2007).
A trigger term in this contextrefers to a term or phrase that provides strong evi-dence supporting the attribution (e.g., patient, fam-ily member) or temporality (e.g., current, past) ofa condition.
Given the limitations of solely us-ing pre-composed trigger term lists, recent focushas been placed on the use of rule-based learningsystems with different feature sets (Mowery et al,2009).
Section headers, tense and aspect are investi-gated as features, with promising results for the tem-porality task achieving an accuracy score of 89%.However, the authors?
acknowledge that conclusionsdrawn must be tentative as a majority class classifierachieved an accuracy of 86.9% (only 13% of condi-tions in the dataset are historical).38This paper extends current work in the domain inthe following ways.
The dataset used in these exper-iments is generated from a collection of previouslyunannotated History & Physical (H&P) Examina-tion reports.
Prior work has focused on other reporttypes such as discharge summaries and emergencydepartment reports.
In these cases the distributionof historical and recent conditions is often heavilyskewed in favour of descriptions of recent conditionsexperienced by the patient.
As H&P reports aim toprovide a comprehensive picture of a patient?s pastand present state, a more uniform distribution of his-torical and recent conditions is present in this reporttype.
This work extends previous work by exploringthe use of machine learning (ML) as an alternative tohand-crafted rule based systems and rule-based MLapproaches to resolving these tasks.In this work, a comparative analysis of severalML algorithms from different paradigms are eval-uated, in order to determine the best approach forour tasks.
Building on this, the performance of fourautomatically extracted feature sets are evaluated,identifying their contributions and also their interac-tions.
This work also extends existing work by au-tomatically extracting features that were previouslyextracted manually as well as the proposal of a setof novel score-based features.
The performance ofthe ML algorithms are compared to the rule-basedsystem - ConText.
Our results show that the MLapproaches significantly outperform this rule-basedsystem on the Temporal Grounding task.2 Related WorkNatural Language Processing techniques havebeen shown to have many different uses in ClinicalText Analysis, such as in the representation (Sageret al, 1994) and understanding (Christensen, 2002)of clinical narratives, and frequently now in the con-text of more elaborate large-scale systems, such as aclinical decision support system (Demner-Fushmanet al, 2009).Given the sensitive nature of clinical narrativesand the difficulty in obtaining data collections forexperimental purposes, evaluation and comparisonof NLP systems in this domain is difficult.
However,recently anonymised data provided by the Biomedi-cal Language Understanding (BLU) Lab at the Uni-versity of Pittsburgh as well as datasets providedas part of the i2b2/VA 2010 challenge (Uzuner etal., 2011), has greatly aided NLP research into theprocessing of clinical narratives.
The dataset pro-vided by BLU Lab and used in this work con-sists of 101,711 reports of several different reporttypes ranging from discharge summaries to surgicalpathology reports.
The report types differ in con-tent, technical language and structure.
For example,surgical pathology reports are very technical and ex-plicit in the information that they convey, e.g.
resultsof a biopsy, blood cell counts etc.
In contrast, dis-charge summaries and consultation reports are moreexpressive, and aim to create a more complete pa-tient profile, e.g.
including work and personal cir-cumstances.
The BLU Lab have published a num-ber of papers on the topic of resolving the assertionstatus of medical conditions (Chapman et al, 2007;Harkema et al, 2009; Mowery et al, 2009).
TheirConText algorithm (Chapman et al, 2007) useshandcrafted regular expressions, along with triggerterms and termination terms to determine character-istics of a condition mention in a text.
The conditioncharacteristics investigated included negation, tem-porality (recent, historical, hypothetical) and experi-encer (patient, other).
Their approach worked verywell on the negation and hypothetical temporality,achieving an f-score of 97% in determining nega-tion and an f-score of 88% in resolving hypotheticalconditions.
However, the approach was less success-ful when determining historical conditions and theirexperiencer, with f-scores of 71% and 67%, respec-tively.
These results were generated on emergencyroom reports only.In more recent work, their algorithm was ap-plied to 5 other types of clinical document, namely:surgical pathology, operative procedure, radiol-ogy, echocardiogram and discharge summaries(Harkema et al, 2009).
Results achieved on thesenew datasets were largely the same, with f-scoresfor negation ranging between 75% and 95%, and forhypothetical conditions ranging between 76% and96%.
Again, a marked drop-off was seen for histor-ical conditions, with few occurrences of conditionsfor other experiencers annotated in the datasets (i.e.relatives) making it difficult to draw definitive con-clusions from this work.Although this manual rule based approach hasperformed well and is advocated due to its ease ofimplementation (Meystre et al, 2008), Harkema etal.
(2009) note its limitations in resolving historicalconditions, and encourage the possibility of statisti-cal classifiers in which information other than lexi-cal items, are considered as features.
Further workinvestigating the use of Machine Learning (Uzuneret al, 2009; Mowery et al, 2009) has seen posi-39tive breakthroughs in resolving the assertion statusof medical conditions.The 2010 i2b2 challenge (Uzuner et al, 2011)provided a rigid and standardized platform for eval-uating systems in resolving the assertion status ofmedical conditions found in Discharge Summaries.The challenge consisted of three subtasks:ConceptExtraction, Assertion and Relation Identification.The second subtask of Assertion involved the devel-opment of systems that resolved the assertion sta-tus of medical conditions.
As part of the asser-tion task there were six possible assertion statuses:present, absent, uncertain, conditional, or not associ-ated with the patient.
Systems submitted to this chal-lenge ranged from more simplistic pattern matchingtechniques to more complex supervised and semi-supervised approaches (de Bruijn et al, 2011; Clarket al, 2011).
The datasets used in the 2010 i2b2challenge were not available to non-participants atthe time the experiments presented in this work wereconducted.
We plan to explore these datasets infuture work.
This research investigates patient vs.non-patient conditions as well as past vs. presentconditions in order to fine tune feature-sets that maybe generalized to further assertion statuses.In the context of this paper, while the BLU Labclinical report collection is available, the medicalcondition annotations are not.
As already stated, ourintention is to explore a machine learning approachto these tasks.
For this purpose we annotated a por-tion of the consultation report section of the collec-tion.
There were two reasons for this - firstly, theBLU Lab have not reported results on this reporttype so there is no duplication of annotation effortand secondly, it turns out that the consultation re-ports are a much richer source of historical condi-tions and condition attribution than any of the reporttypes annotated previously.3 Method3.1 CorpusFor this task, 120 H&P reports were randomlyextracted from the BluLab?s NLP repository.
Asalready stated, this report type?s fuller descriptionsmake it richer than previous datasets in instancesof condition attribution and temporal grounding.
Abreakdown in the distributions of these annotationscan be seen in Tables 1 and 2.H&P reports may vary in the organization of con-tent, but the content is mostly uniform, expressingthe same information about patients (Sager et al,1987).
As well as this, many reports feature head-ings for different sections of the report (past medicalhistory, impression), information which can be usedas features in a classification task.
Before annotat-ing conditions found in the text, preprocessing wasrequired in order to retain such information.Table 1: Annotated Condition Attribution OccurrencesClass CountPatient 872Other 93Total 965Table 2: Annotated Temporal Grounding OccurrencesClass CountHistorical 448Recent 424Total 8723.1.1 PreprocessingPreprocessing of the data consisted of a simpleJava program that extended Lingpipe1 tools in or-der to correctly split sentences on this dataset, andextract the heading for the section in which the sen-tence is contained.The preprocessing outputs the sentence number,followed by a separator, the sentence?s contents andthe heading under which the sentence features.
Sen-tences were split for ease of annotation and alsoto allow parsing and part-of-speech tagging by theC&C2 parsing tools.
C&C was chosen for its scala-bilty, speed and the accuracy of its biomedical lan-guage models.
A cursory analysis of its output in-dicates that its performance is acceptable.
As C&Cdoes not provide a sentence splitter, Lingpipe?s split-ter was availed of for this task.3.1.2 AnnotationAnnotation of the dataset was performed by twoannotators over a 60 hour period.
The inter-annotator agreement was measured using the kappastatistic (Carletta, 1996).
A kappa statistic of 0.78was achieved.
The annotators were presented withthe collection, to annotate with an XML like tag?CONDITION?.
This tag must have two attributes,?EXP?
representing condition attribution and ?HIST?1http://alias-i.com/lingpipe/2http://svn.ask.it.usyd.edu.au/trac/candc40representing the temporal grounding of the condi-tion.?
HIST: A value of 1 indicates the occurrence of ahistorical condition, where 0 describes a currentor recent condition.
e.g.
?The patient presentedwith <CONDITION NUM=?1?
HIST=?0?> re-nal failure </CONDITION>?
would indicate thecondition ?renal failure?
is current.?
EXP: A value of 1 implies the expe-riencer is the patient with 0 signifying?other?.
e.g.
?The patient has a fam-ily history of <CONDITION NUM=?1?EXP=?0?>hypertension </CONDITION>?signifies the condition ?hypertension?
is notexperienced by the patient.3.2 Machine Learning AlgorithmsEarly work in the resolution of assertion statusprimarily focused on the use of manually createdrule-based systems, with more recent work focusingon statistical and ML methods.
However, the do-main of ML contains many sub-paradigms and vary-ing approaches to classification.
In this paper, threeML methods that have not been previously appliedto this task are explored.
These three classifiers,namely Naive Bayes, k-Nearest Neighbour and Ran-dom Forest represent the paradigms of probabilistic,lazy and ensemble learning, respectively.Naive Bayes is a probabilistic classifier imple-menting Bayes Theorem.
As a result, features im-plemented using this classifier are deemed to be in-dependent.
Despite this strong assumption it hasbeen shown to be more than successful in text classi-fication tasks such as spam filtering (Provost, 1999).k-Nearest Neighbour (kNN) (Cover and Hart,1967) is a simple pattern recognition algorithm thatclassifies an instance according to its distance to thek closest training instances.
This algorithm has beenchosen to represent the paradigm of lazy learning,i.e.
there is no training phase as all computationis performed at the classification stage.
Despite itssimplicity, k-NN has often produce high accuracyresults in comparison to other approaches (Caruana,2006).The final classifier chosen for this task representsthe state-of-the-art in machine learning, namely theRandom Forest algorithm (Breiman, 2001).
A Ran-dom Forest consists of many different decision trees,combining bagging (Breiman, 1996), and randomfeature selection.3.3 FeaturesIn this section, a list of features contributing tothis task are presented.
All features are automati-cally extracted using a set of tools developed by theauthors.
Section 3.3.1 presents score-based featuresthat are unique to this work.
Section 3.3.2 describesthe implementation of features outlined in Chapmanet al(2007).
Section 3.3.3 and Section 3.3.4 presentfeatures similar to those investigated in Mowery etal.
(2009).3.3.1 Score based featuresScored based features used in this system extendand reinforce Trigger List features by computing anormalized score for the number of occurrences ofTrigger List terms3.
This feature aims to add fur-ther granularity to the decision making of the ML al-gorithms, presenting a floating point number ratherthan a binary one.The equation for computing these scores is de-fined as follows.s =Nt(Nw ?
Sw)(1)Nt represents the number of trigger terms found inthe sentence that contains the condition, Nw is thetotal number of words in the sentence, with Sw beingthe number of stopwords4.
These scores are calcu-lated for each type of trigger term.
For example, fortrigger type relative mention, a score is calculatedusing mentions of relatives in the sentence.3.3.2 Trigger List Features?
precededByHistTerm: This feature performsa look-up for trigger terms from the historicalword list, checking if it directly precedes thecondition.
An example historical trigger termwould be ?history of?
as in ?a history of dia-betes?.
If a condition, such as diabetes, is mod-ified by a historical trigger term, it will return 1,otherwise 0.?
containsHistMention: This is a weakerform of precededByHistTerm, checking sim-ply if a trigger term from the historical list oc-curs in the same sentence as the condition.
Ifone does, it will return 1 otherwise 0.?
hasRelativeMention: If the sentence whichcontains the condition also contains a trigger3These trigger lists may be downloaded at http://csserver.ucd.ie/?jcogley/downloads/wordlists.tar.gz4The list of stopwords may be downloaded athttp://csserver.ucd.ie/?jcogley/downloads/stopwords.txt41term from the experiencer list such as ?mother?,?father?
or ?uncle?
it will return 1, otherwise 0.?
hasPatientMention: 1 if the sentence men-tions the patient, otherwise 0.?
containsDeath: 1 if it contains the terms ?de-ceased?, ?died?
from the death trigger terms listotherwise 0.
A sentence describing death is morelikely to refer to a relative, rather than the pa-tient.?
mentionsCommunity: 1 if one of ?area?,?community?
from the geographical trigger listis mentioned, otherwise 0.
If a sentence de-scribes a community, as in ?there has been a re-cent outbreak of flu in the area?, it is not refer-ring to the patient, therefore the condition shouldnot be attributed to the patient.?
precededByWith: 1 if the condition is directlypreceded by ?with?, otherwise 0.
?with?
wasfound to have higher frequency when describ-ing patients rather than individuals other than thepatient.
e.g.
?Patient presented with high bloodpressure and fever.??
containsPseudoTerms: Pseudo-historicalterms or phrases may mention a term that isfound in the Historical list, but do not indicatethat a condition mention in the same sentence isbeing used in a historical context.
For example,?poor history?
is a pseudo-historical triggerterm.
It uses a historical trigger term (?history?
);however ?poor history?
refers to the incompletenature of the patient?s medical history, not thehistorical nature of their condition.
This featurereturns 1 on the occurrence of a pseudo triggerterm, otherwise 0.3.3.3 Contextual featuresIn resolving the textual context of conditions, itis important to look at what surrounds the conditionbeyond the lexical items.
With these contextual fea-tures, we can capture that section in which a sen-tence occurs, and how many conditions occur in thesentence.?
isInFamHist: The importance of header infor-mation is motivated by the assumption that con-ditions that fall under explicit headings, are morethan likely to have a greater affinity to the head-ing.
This feature returns 1 if it is under FamilyHistory, 0 otherwise.?
isInList: A binary feature denoting whethera condition occurs as part of a list of conditions,with one condition per line.
Returns 1 if it is amember of such a list, otherwise 0.?
numOfConditions: This feature represents thenumber of conditions present in a given sen-tence.
A higher number of conditions indicatesthat the condition may be part of a list.
Sentencesthat contain a list of conditions tend to list pastconditions rather than recently suffered illnesses.3.3.4 Linguistically motivated featuresThree features were designed to monitor the ef-fect of the verb tense on a condition.
This featurehas already been shown to aid the classification pro-cess (Mowery et al, 2009).
For this task, linguisticfeatures were extracted from the output of the C&Cparsing tool, using both part-of-speech tags alongwith dependency information.?
hasPastTense: A binary feature with 1 indi-cating the sentence contains a past tense verb, 0otherwise.
e.g.
?The patient previously sufferedrenal failure?
would return 1.
If a condition ismodified by a past tense verb, it has occurred inthe past.?
hasPresentTense: A binary feature with 1indicating the sentence contains a present tenseverb, 0 otherwise.
If a condition is modified by apresent tense verb, the condition is current.
e.g.
?the patient presents coughing?.?
containsModalVerb: A binary feature with 1indicating the sentence contains a modal verb,0 otherwise.
e.g.
?palpitations may have beencaused by anxiety?.
The presence of the modal?may?
following the condition indicates the con-dition is currently being examined and is there-fore recent.?
tenseInClause: Analyzes the tense found inthe same syntactic clause as the condition beingexamined.
For example, in ?abdominal pain hasceased, but patient now complains of lower ex-tremity pain?, ?abdominal pain?
has a past tensewithin its clausal boundary, where the clausewhich contains ?lower extremity pain?
has apresent tense verb.?
tenseChange: Determines whether the verbtense used in the clause that contains the con-dition differs with the verb in another clause inthe sentence.
e.g.
?Migraines persist yet palpi-tations resolved?.
This feature allows finer gran-ularity in resolving the tense surrounding condi-tions, such as the description of current condi-tions in the context of the patient?s history.424 Experiment Setup & EvaluationThere are two aims of the experiments reportedin this section: firstly, to evaluate the performanceof ML algorithms in resolving the assertion status ofmedical conditions.
Secondly, to assess the perfor-mance of individual feature sets in order to discoverthe most contributory and informatory features orsets of features.
To evaluate the latter, classificationsusing all possible combinations of feature sets (listedin Table 3) were performed.Table 3: Feature-set CombinationsID Feature-SetsTrigLingConScore trigger, linguistic, score-based, contextualTrigLingScore trigger, linguistic, score-basedTrigLingCon trigger, linguistic, contextualTrigConScore trigger, score-based, contextualLingConScore linguistic, score-based, contextualTrigLing trigger, linguisticTrigScore trigger, score-basedTrigCon trigger, contextualLingScore linguistic, score-basedLingCon linguistic, contextualConScore score-based, contextualTrigger triggerLing linguisticScore score-basedCon contextual4.1 EvaluationThe systems are evaluated by the metrics preci-sion, recall and f-score:precision =TPTP + FPrecall =TPTP + FNf =2?
Precision?RecallPrecision + Recallwhere TP is the number of true positives, FP is the num-ber of false positives, FN is the number of false negatives.Systems are evaluated using both cross-validationand hold-out methods.
In the hold-out method thereare two data sets, one used for training the classifierand a second for testing it on a blind sub-set of testmaterial.
10-fold cross-validation is performed onthe training sets and final results are reported in thispaper on the held-out blind test set.
Three hold-outclassification splits were experimented with (i.e.,train/test splits: 30%/70%; 50%/50%; 70%/30%).We found that results for each of the data splits andcross-validation experiments were largely uniform.To avoid repetition of results, Section 5 focuses onexperiments using a held-out method training/testsplit of 70%/30%.
All hold-out experiments wereimplemented using Weka?s (Hall et al, 2009) Ex-perimenter interface.
Cross-Validation experimentswere performed using a script developed by the au-thors in conjunction with Weka?s API.
This allowedthe ML approaches and the ConText algorithm to beevaluated against the same test-folds.4.1.1 Comparison with a rule-based systemConText (Chapman et al, 2007) is a simple yeteffective rule-based system designed to resolve theassertion status of medical conditions.
Comparativeanalysis is performed between an implementation ofConText5 and the ML approaches described in 3.2.The ML systems were trained on 70% of the dataset(610 conditions).
The remaining 30% (262 condi-tions) was used as a test set for both ConText andthe Machine Learning systems.
For cross-validationexperiments, ConText was evaluated against eachtest set fold.
For the Condition Attribution exper-iments training was performed on 675 conditionswith testing performed on 290 conditions.
In eval-uating Temporal Grounding the training set com-prised of 610 conditions with the test-set containing262 conditions.5 Experimental ResultsThis section reports results of the experimentsoutlined in Section 4.5.1 Condition AttributionIn a system that resolves the assertion status ofmedical conditions, it is of benefit to know who isexperiencing the medical condition before resolvingmore complex information such as temporality.
Inthis section, preliminary results on Condition Attri-bution are presented.
The dataset used in evaluat-ing the effectiveness of Condition Attribution washighly skewed, as shown in Table 1.
This is a naturalskew caused simply by the fact that reports discussthe patient more than other individuals (e.g., bloodrelatives).
As a result the baseline score using a Ma-jority Class classifier achieved an f-score of 95%(Table 4).
Given these results, the contextual fea-ture set contributes most, as shown by the removalof the contextual feature set in TrigLingScore coin-ciding with a drop in performance.
However, theskewed dataset resulted in no statistical significance5http://code.google.com/p/negex/downloads/detail?name=GeneralConText.Java.v.1.0_10272010.zip43between classifiers and feature-sets.Table 4: Selected feature-sets (f-score) using Cross-Validation for the Condition Attribution taskID RFor kNN NB Maj.TrigLingConScore 100% 100% 100% 95%TrigLingScore 96% 96% 96% 95%TrigConScore 100% 100% 100% 95%Con 100% 100% 100% 95%In this task, ConText achieved an f-score of 99%.As there is little difference in scores achieved be-tween ConText and the approaches in Table 4 - amanual rule-based system can be considered ade-quate for this task.Taking a closer look at the performance of the fea-ture sets, we see that the contextual feature set con-tributes most to the task with the removal of contex-tual features coinciding with a drop in performancee.g., TrigLingScore in Table 4.
The strength of thisfeature set lies with the feature isInFamHist.
Thisfeature simply checks if the condition occurs underthe heading ?Family History?.
Its highly influen-tial performance would indicate that its quite rarefor the mention of another individual anywhere elsein a clinical report.
The Con run, which is solelycomposed of contextual features achieves near per-fect performance, an indication that the contributionof other features to the task of Condition Attribu-tion is minimal.
Although this work used only H&Preports, this finding may be generalized to other re-port types such as Discharge Summaries which alsoexplicitly mark sections pertaining to other individ-uals.5.2 Temporal GroundingThe distribution of past and recent medical con-ditions is not skewed (as in the Condition Attribu-tion task), and hence it presents a more challeng-ing classification task.
Despite the varying per-formance of individual classifiers and feature setsthe results obtained are again largely similar acrosscross-validation and hold-out methods, with the per-formance of each training set fitting the distribu-tion in Figure 1.
Initial experiments investigated theuse of another state-of-the-art classifier, the SupportVector Machine using a polykernel, however due toits relatively poor performance (70% f-score, withits precision soundly beaten by other approaches) itwill not be discussed in further detail.Random Forest proved to be the most effectiveclassifier across almost all feature sets.
However,kNN was a very near second place - Random Forestonly significantly6 outperformed kNN on two occa-sions (TrigLingConScore, LingConScore).
In con-strast, Naive Bayes performed poorly - despite hav-ing outperformed all other systems on the precisionmetric, it failed to outperform the baseline majorityclassifier on the recall.Although the precision of ConText matches thatof the Random Forest and kNN (Table 5), it is alsolet down by its recall performance.
As a result, thereis a statistical significant difference between its f-score and that of the Random Forest and kNN.Table 5: Temporal Grounding ConText ComparisonSystem Precision Recall F-scorekNN 80% 80% 80%RandomForest 82% 84% 83%NaiveBayes 91% 61% 72%ConText 80% 61% 69%Majority 55% 100% 71%6 DiscussionThe distribution of recent and historical condi-tions for the task of Temporal Grounding is moreevenly distributed than that used in Condition Attri-bution, allowing for a more interesting comparisonof the approaches and features employed.Figure 1 shows the performance of each ML foreach feature-set combination.
Random Forest wasexpectedly the best performing algorithm.
However,more surprising was the comparative performanceof the often overlooked kNN algorithm.
Both ap-proaches statistically significantly outperformed therule-based system ConText.
Though the rule basedsystem matched the high performing ML systems interms of precision, it was significantly outperformedwith respect to recall.The most contributory feature set in the ML runswas the novel score-based feature set.
This featurecreates a normalized score for the occurrence of trig-ger terms in the same sentence as the medical con-dition in question.
It was designed to reinforce theimportance of trigger terms, by providing a numericscore to support the binary Trigger List feature.
Theaddition of score-based features to any of the fea-ture combinations coincided with a statistical signif-icant boost in performance, with Score (composedsolely of score-based features) outperforming half ofall other feature combinations as seen in Figure 1,.On the contrary, the effect of contextual featureson the performance of the algorithms for Temporal6Significance calculated by Paired T-Test with 95% confi-dence.4430%?40%?50%?60%?70%?80%?90%?100%?TrigLingConScoreTrigLingScoreTrigLingConTrigConScoreLingConScoreTrigLingTrigScore TrigConLingScore LingConConScore Trig Ling Score ConRan.?Forest?
kNN?
Naive?Bayes?
Maj.?Class?Figure 1: Temporal Grounding f-score performance with 70% Training DataGrounding is minimal, or even detrimental to thetask.
For example, in Figure 1, the performanceof the kNN algorithm increases from TrigLingCon-Score to TrigLingScore with the removal of contex-tual features.
The performance of the Random For-est is unaffected by such detrimental features as itperforms its own feature selection prior to classifi-cation.
Though there are several feature set com-binations reaching a high level of performance, themost effective approach combines trigger list terms,linguistic and score based features with the RandomForest algorithm.These experiments extend previous work by pro-viding a systematic, automated approach to featureextraction for the purpose of ML approaches to Tem-poral Grounding.
They also indicate the high per-formance and contribution of our novel score-basedfeatures.
These features are not designed to solelyclassify instances found in H&P reports and canbe applied to other clinical reports such as Dis-charge Summaries and Emergency Department re-ports.
Previous work has involved the use of thelatter mentioned report types.
Unfortunately, giventhe terms-of-use of these datasets they could not bemade available to authors to facilitate comparativeexperiments.7 ConclusionIn this paper, we proposed the use of machinelearning (ML) in resolving if and when a patientexperienced a medical condition.
The implementedML algorithms made use of features comprising oftrigger terms, linguistic and contextual information,and novel score-based features.
In an evaluation ofthese feature sets it was found that score-based fea-tures contributed to the task of resolving when a pa-tient experienced a medical condition.The ML approaches were also evaluated againstthe rule-based system ConText on a new annotateddataset of History & Physical (H&P) ExaminationReports.
In this evaluation it was discovered that thetask of resolving if a condition was experienced bythe patient was adequately solved by the ConTextsystem, achieving an f-score of 99%.
Although, theML approaches proposed achieved perfect perfor-mance, there is no statistical significance betweenthe result sets.
However, the more challenging taskof deciding when a patient experienced a medicalcondition is deemed to be best suited to a ML ap-proach, with the top performing classifier RandomForest achieving an f-score of 87%, significantlyoutperforming ConText which achieved 69% on thesame dataset .The results achieved in these tasks have paved theway for several avenues of future work.
We be-lieve that the performance of these tasks is now suffi-ciently accurate to justify their inclusion in an Infor-mation Retrieval (IR) application.
It is our intentionto use our medical condition analysis techniques toannotate clinical documents and build an advancedIR system capable of taking advantage of this markup in the context of the TREC Medical RecordsTrack 20127.
With the availability of datasets suchas that of the i2b2 Shared Task 2010 data, furtherwork will include experimentation on these datasetsas well as an investigation into further assertion sta-tuses.8 AcknowledgmentsWe are grateful to Dr Martina Naughton for heradvice on many aspects of this paper.
We also wishto acknowledge the support of Science FoundationIreland, who fund this research under grant number10/RFP/CMS2836.7http://groups.google.com/group/trec-med45ReferencesL.
Breiman.
1996.
Bagging predictors.
Machine Learn-ing, 24:123?140.L.
Breiman.
2001.
Random forests.
Machine Learning,45:5?32.J.
Carletta.
1996.
Assessing agreement on classificationtasks: the kappa statistic.
Computational Linguistics,22(2):249 ?
254.R.
Caruana.
2006.
An empirical comparison of super-vised learning algorithms.
In Proceedings of 23rd In-ternational Conference on Machine Learning.W.
W. Chapman, D. Chu, and J. N. Dowling.
2007.
Con-text: An algorithm for identifying contextual featuresfrom clinical text.
In BioNLP 2007: Biological, trans-lational, and clinical language processing, pages 81?88, June.L.
M. Christensen.
2002.
Mplus: A probabilistic med-ical language understanding system.
In Proceedingsof Workshop on Natural Language Processing in theBiomedical Domain, pages 29?36.C.
Clark, J. Aberdeen, M. Coarr, D. Tresner-Kirsch,B.
Wellner, A. Yeh, and L. Hirschman.
2011.
Mitresystem for clinical assertion status classification.
Jour-nal of the American Medical Informatics Association.T.
Cover and P. Hart.
1967.
Nearest neighbor patternclassification.
Transactions on Information Theory.B.
de Bruijn, C. Cherry, S. Kiritchenko, J. Martin, andX.
Zhu.
2011.
Machine-learned solutions for threestages of clinical information extraction: the state ofthe art at i2b2 2010.
Journal of the American MedicalInformatics Association.D.
Demner-Fushman and J. Lin.
2007.
Answeringclinical questions with knowledge-based and statisti-cal techniques.
In Computational Linguistics, pages63?103.D Demner-Fushman, W W. Chapman, and C J. McDon-ald.
2009.
What can natural language processing dofor clinical decision support?
Journal of BiomedicalInformatics, 42:760?772.D.
Demner-Fushman, S. Abhyankar, A. Jimeno-Yepes,R.
Loane, B. Rance, F. Lang, N. Ide, E. Apostolova,and A. R. Aronson.
2011.
A knowledge-based ap-proach to medical records retrieval.
In TREC 2011Working Notes.J.
Feblowitz, A. Wright, H. Singh, L. Samal, and D. Sit-tig.
2011.
Summarization of clinical information: Aconceptual model.
Biomedical Informatics.M.
Hall, F. Eibe, G. Holmes, B. Pfahringer, P. Reute-mann, and I. H. Witten.
2009.
The weka data miningsoftware: An update.
SIGKDD Explorations.H.
Harkema, J. N. Dowling, T. Thornblade, and W. W.Chapman.
2009.
Context: An algorithm for identify-ing contextual features from clinical text.
Journal ofBiomedical Informatics, 42(5):839?851.G.
Hripcsak, L. Zhou, S. Parsons, A. K. Das, and S. B.Johnson.
2005.
Modeling electronic discharge sum-maries as a simple temporal constraint satisfactionproblem.
Journal of the American Medical Informat-ics Association, 12(1):55?63, January.S.
M. Meystre, G. K. Savova, K. C. Kipper-Schuler, andJ.
F. Hurdle.
2008.
Extracting information from tex-tual documents in the electronic health record: a re-view of recent research.
IMIA Yearbook of MedicalInformatics, pages 128?144.D.
L. Mowery, H Harkema, J. N. Dowling, J. L. Lust-garten, and W. W. Chapman.
2009.
Distinguishinghistorical from current problems in clinical reports-which textual features help?
In Proceedings of theWorkshop on Current Trends in Biomedical NaturalLanguage Processin, pages 10?18.
Association forComputational Linguistics.J.
Patrick and M. Li.
2011.
An ontolotgy for clinicalquestions about the contents of patients notes.
Journalof Biomedical Informatics.J.
Provost.
1999.
Naive-bayes vs. rule-learning in classi-fication of email.
Technical report, The University ofTexas at Austin.N.
Sager, C. Friedman, and M.S.
Lyman.
1987.
Medi-cal Language Processing: Computer Management ofNarrative Data.
Addison-Wesley.N.
Sager, M. Lyman, C. Bucknall, N. Nhan, and L. J.Tick.
1994.
Natural language processing and the rep-resentation of clinical data.
Journal of the AmericanMedical Informatics Association, 1:142?160.O.
Uzuner, X. Zhang, and T. Sibanda.
2009.
Machinelearning and rule-based approaches to assertion classi-fication.
Journal of the American Medical InformaticsAssociation, 16(1):109?115.O?.
Uzuner, BR.
South, S. Shen, and SL.
DuVall.
2011.2010 i2b2/va challenge on concepts, assertions, and re-lations in clinical text.
Journal of the American Medi-cal Informatics Association.E.
Voorhees and R. Tong.
2010.
Overview of the trec2011 medical records track.
preprint.46
