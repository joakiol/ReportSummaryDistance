Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 526?533,Sydney, July 2006. c?2006 Association for Computational LinguisticsSentence Ordering with Manifold-based Classification inMulti-Document SummarizationPaul D JiCentre for Linguistics and PhilologyUniversity of Oxfordpaul_dji@yahoo.co.ukStephen PulmanCentre for Linguistics and PhilologyUniversity of Oxfordsgp@clg.ox.ac.ukAbstractIn this paper, we propose a sentenceordering algorithm using a semi-supervisedsentence classification and historicalordering strategy.
The classification is basedon the manifold structure underlyingsentences, addressing the problem of limitedlabeled data.
The historical ordering helps toensure topic continuity and avoid topic bias.Experiments demonstrate that the method iseffective.1.
IntroductionSentence ordering has been a concern in textplanning and concept-to-text generation (Reiter etal., 2000).
Recently, it has also drawn attention inmulti-document summarization (Barzilay et al,2002; Lapata, 2003; Bollegala et al, 2005).
Sincesummary sentences generally come fromdifferent sources in multi-documentsummarization, an optimal ordering is crucial tomake summaries coherent and readable.In general, the strategies for sentence orderingin multi-document summarization fall in twocategories.
One is chronological ordering(Barzilay et al, 2002; Bollegala et al, 2005),which is based on time-related features of thedocuments.
However, such temporal features maybe not available in all cases.
Furthermore,temporal inference in texts is still a problem, inspite of some progress in automaticdisambiguation of temporal information (Filatovaet al, 2001).Another strategy is majority ordering (MO)(McKeown et al, 2001; Barzilay et al, 2002), inwhich each summary sentence is mapped to atheme, i.e., a set of similar sentences in thedocuments, and the order of these sentencesdetermines that for summary sentences.
To dothat, a directed theme graph is built, in which if atheme A occurs behind another theme B in adocument, B is linked to A no matter how faraway they are located.
However, this may lead towrong theme correlations, since B?s occurrencemay rely on a third theme C and have nothing todo with A.
In addition, when outputting themeorders, MO uses a kind of heuristic that chooses atheme based on its in-out edge difference in thedirected theme graph.
This may cause topicdisruption, since the next choice may have nolink with previous choices.Lapata (2003) proposed a probabilisticordering (PO) method for text structuring, whichcan be adapted to majority ordering if the trainingtexts are those documents to be summarized.
Theprimary evidence for the ordering are informativefeatures of sentences, including words and theirgrammatical dependence relations, which needsreliable parsing of the text.
Unlike in MO,selection of the next sentence here is based on themost recent one.
However, this may lead to topicbias: i.e.
too many sentences on the same topic.In this paper, we propose a historical ordering(HO) strategy, in which the selection of the nextsentence is based on the whole history ofselection, not just the most recent choice.
This526strategy helps to ensure continuity of topics but toavoid topic bias at the same time.To do that, we need to map summary sentencesto those in documents.
We formalize this as akind of classification problem, with summarysentences as class labels.
Since there are very few(only one) labeled examples for each class, weadopt a kind of semi-supervised classificationmethod, which makes use of the manifoldstructure underlying the sentences to do theclassification.
A common assumption behind thislearning paradigm is that the manifold structureamong the data, revealed by higher density,provides a global comparison between data points(Szummer et al, 2001; Zhu et al, 2003; Zhou etal., 2003).
Under such an assumption, even onelabeled example is enough for classification, ifonly the structure is determined.The remainder of the paper is organized asfollows.
In section 2, we give an overview of theproposed method.
In section 3~5, we talk aboutthe method including sentence networks,classification and ordering.
In section 6, wepresent experiments and evaluations.
Finally insection 7, we give some conclusions and futurework.2.
OverviewFig.
1 gives the overall structure of the proposedmethod, which includes three modules:construction of sentence networks, sentenceclassification and sentence ordering.Fig.
1.
Algorithm OverviewThe first step is to build a sentence neighborhoodnetwork with weights on edges, which can serveas the basis for a Markov random walk (Tishby etal., 2000).
The neighborhood is based onsimilarity between sentences, and weights onedges can be seen as transition probabilities forthe random walk.
From this network, we canderive new representations for sentences.The second step is to make a classification ofsentences, with each summary sentence as a classlabel.
Since only one labeled example exists foreach class, we use a semi-supervised methodbased on a Markov random walk to reveal themanifold structure for the classification.The third step is to order summary sentencesaccording to the original positions of theirpartners in the same class.
During this process,the next selection of a sentence is based on thewhole history of selection, i.e., the association ofthe sentence with all those already selected.3.
Sentence Network ConstructionSuppose S is the set of all sentences in thedocuments and a summary (a summary sentencemay be not a document sentence), let S={s1,s2, ?, sN} with a distance metric d(si,sj), thedistance between two sentences si and sj, which isbased on the Jensen-Shannon divergence (Lin,1991).
We construct a graph with sentences aspoints by sorting the distances among the pointsin an ascending order and repeatedly connectingtwo points according to the order until aconnected graph is obtained.
Then, we assign aweight wi,j, as in (1), to each edge based on thedistance.
)/),(exp()1,?jiji ssdw ?=The weights are symmetric, wi,i=1 and wi,j=0 forall non-neighbors (?
is set as 0.6 in this work).
2)is the one-step transition probability p(si, sj) fromsi to sj based on weights of neighbors.
?=kkijijiwwssp,,),()2Sentence network constructionSentence classificationSummary sentence ordering527Let M be the N?N matrix and Mi,j= p(si, sj), thenMt is the tth Markov random walk matrix, whose i,j-th entry is the probability pt(si, sj) of thetransition from si to sj after t steps.
In this way,each sentence sj is associated with a vector ofconditional probabilities pt(si, sj), i=1, ?, N,which form a new manifold-based representationfor sj.
With such representations, sentences areclose whenever they have a similar distributionover the starting points.
Notice that therepresentations depend on the step parameter t(Tishby et al, 2000).
With smaller values of t,unlabeled points may be not connected withlabeled ones; with bigger values of t, the pointsmay be indistinguishable.
So, an appropriate tshould be estimated.4.
Sentence ClassificationSuppose s1, s2, ?, sL are summary sentences andtheir labels are c1, c2, ?, cL respectively.
In ourcase, each summary sentence is assigned with aunique class label ci, 1?i?L.
This also means thatfor each class ci, there is only one labeledexample, i.e., the summary sentence, si.Let S={(s1, c1), (s2, c2), ?, (sL, cL), sL+1,?, sN},then the task of sentence classification is to inferthe labels for unlabeled sentences, sL+1,?, sN.Through the classification, we can get similarsentences for each summary sentence.
To do that,we assume that each sentence has a distributionp(ck|si), 1?k?L, 1?i?N, and these probabilities areto be estimated from the data.Seeing a sentence as a sample from the t stepMarkov random walk in the sentence graph, wehave the following interpretation of p(ck|si).
?=jtjkik ijpscpscp ),()|()|()3This means that the probability of si belongingto ck is dependent on the probabilities of thosesentences belonging to ck which will transit to siafter t steps and their transition probabilities.With the conditional log-likelihood of labeledsentences 4) as the estimation criterion, we canuse the EM algorithm to estimate p(ck|si), inwhich the E-step and M-step are 5) and 6)respectively.?
?
?= = ==LkLktNjjkkk kjpscpscp1 1 1),()|(log)|(log)4?==Lktiktikkki kipscpkipscpcssp1),()|(/),()|(),|()5??
?=Lkkkikkiik csspcsspscp1),|(/),|()|()6The final class ci for si is given in 7).
)|(maxarg)7 ikci scpc k=p(ci|si) is called the membership probability of si.After classification, each sentence is assigned alabel according to 7).One key problem in this setting is to estimatethe parameter t. A possible strategy for that is bycross validation, but it needs a large amount oflabeled data.
Here, following Szummer et al,2001, we use marginal difference of probabilitiesof sentences falling in different classes as theestimation criterion, which is given in 8).?
??
????
?=Ss LkkkLkscpscpLSm11))|()|(max()()8To maximize 8), we can get an appropriate valuefor the parameter t, which means that a better tshould make sentences belong to some classesmore prominently.
Notice that the classesrepresented by summary sentences may beincomplete for all the sentences occurring in thedocuments, so some sentences will belong to theclasses without obviously different probabilities.To avoid such sentences in the estimation of t, weonly choose the top (40%) sentences in a classbased on their membership probabilities.5.
Sentence OrderingAfter sentence classification, we get a class ofsimilar sentences for each summary sentence,which is also a member of the class.
With thesesentence classes, we create a directed class graphbased on the order of their member sentences indocuments.
In the graph, each sentence class is a528node, and there exists a directed edge ei,j fromone node ci to another cj if and only if there is siin ci immediately appearing before sj in cj in thedocuments (the sentences not in classes areneglected).
The weight of ei,j, Fi,j, captures thefrequency of such occurrence.
We add oneadditional node denoting an initial class c0, and itlinks to each class with a directed edge e0,j, theweight F0,j of which is the frequency of themember sentences of the class appearing at thebeginning of the documents.Suppose the input is the class graph G=<C, E>,where C = {c1, c2, ?, cL} is the set of the classes,E={ei,j|1?i, j?L} is the set of the directed edges,and o is the ordering of the classes.
Fig.
2 givesthe ordering algorithm.--------------------------------------------------i) iCck Fc i ,0max?
?ii) o?
o ckiii) For all ci in C, ikii FFF ,,0,0 +?iv) Remove ck from C and ek,j and ei,k from E;v) Repeat i)-iv) while C?
{ c0}vi) Return the order o.--------------------------------------------------------Fig.
2 Ordering algorithmIn the algorithm, there are two main steps.
Step i)selects the class whose member sentences occurmost frequently immediately after those in c0.Step iii) updates the weights of the edges e0,i.
Infact, it can be seen as merge of the original c0 andck, and in this sense the updated c0 represents thehistory of selections.In contrast to the MO algorithm, the orderingalgorithm here (HO) uses immediate back-frontco-occurrence, while the MO algorithm usesrelative back-front locations.
On the other hand,the selection of a class is dependent on previousselections in HO, while in MO, the selection of aclass is mainly dependent on its in-out edgedifference.In contrast to the PO algorithm, the selection ofa class in HO is dependent on all previousselections, while in PO, the selection is onlyrelated to the most recent one.As an example, Fig.
3 gives an initial classgraph.
The output orderings by PO and HO are[c1, c3, c4, c2] and [c1, c3, c2, c4] respectively.
Thedifference lies in whether to select c4 or c2 afterselection of c3.
PO selects c4 since it onlyconsiders the most recent selection, while HOselects c2 because it considers all previousselections including c1.c09c15        6c2                  c31       21           c4Fig.
3 Initial graph for PO and HOAs another example, Fig.
4 gives the order of theclasses in individual documents.1) c2   c3   c12) c2   c3   c13) c3   c2   c14) c3   c2   c15) c3   c26) c2   c37) c1    c2   c3  c2   c3   c2   c3   c2Fig.
4.
Class orders in documentsFrom 1)-6), we can see some regularity amongthe order of the classes: c2 and c3 areinterchangeable, while c1 always appears behindc2 or c3.
From 7), we can see that c2 and c3 stillco-occur, while c1 happens to occur at thebeginning of the document.
Thus, the appropriateordering should be [c2, c3, c1] or [c3, c2, c1].
Fig.
5is the graph built by MO.c24     6  6     2c33  2c1Fig.
5 Graph by MO529According to MO, the first node to be selectedwill be c1, since the difference of its in-out edges(+3) is bigger than that (-2, -1) of other two nodes.Then the in-out edge differences for c2 or c3 areboth 0 after removing edges associated with c1,and either c2 or c3 will be selected.
Thus, theoutput ordering should be [c1, c2, c3] or [c1, c3, c2].c21      6   6     2c3           30  2    3c1      1      c0Fig.
6 Graph by HOFig.
6 is the class graph built by HO.
Accordingto HO, the first node to be selected will be c2 or c3,since e0,1=e0,2=3>e0,1=1.
Suppose c2 is firstlyselected, then e0,3 ?
e0,3+e2,3=3+6=9, while e0,1 ?e0,1+e2,1=1+2=3, so c3 will be selected then.Finally the output ordering will be [c2, c3, c1].Similarly, if c3 is firstly selected, the outputordering will be [c3, c2, c1].6 Experiments and Evaluation6.1 DataWe used the DUC04 document dataset.
Thedataset contains 50 document clusters and eachcluster includes 20 content-related documents.For each cluster, 4 manual summaries areprovided.6.2 Evaluation MeasureThe proposed method in this paper consists oftwo main steps: sentence classification andsentence ordering.
For classification, we usedpointwise entropy (Dash et al, 2000) to measurethe quality of the classification result due to lackof enough labeled data.
For a n?m matrix M,whose row vectors are normalized as 1, itspointwise entropy is defined in 9).
))1log()1(log()()9,,1 1,, jijini mjjiji MMMMME ?
?+?= ????
?
?Intuitively, if Mi,j is close to 0 or 1, E(M) tendstowards 0, which corresponds to clearerdistinctions between classes; otherwise E(M)tends towards 1, which means there are no clearboundaries between classes.
For comparisonbetween different matrices, E(M) needs to beaveraged over n?m.For sentence ordering, we used Kendall?s ?coefficient (Lapata, 2003), as defined in 10),2/)1()(21)10?
?=NNN I?where, NI is number of inversions of consecutivesentences needed to transform output of thealgorithm to manual summaries.
The measureranges from -1 for inverse ranks to +1 foridentical ranks, and can also be seen as a kind ofedit similarity between two ranks: smaller valuesfor lower similarity, and bigger values for highersimilarity.6.3 Evaluation of ClassificationFor sentence classification, we need to estimatethe parameter t. We randomly chose 5 documentclusters and one manual summary from the four.Fig.
7 shows the change of the average marginover all the top 40% sentences in a cluster with tvarying from 3 to 25.00.20.40.60.83 5 10 15 20 25t valuesaveragemargincluster 1cluster 2cluster 3cluster 4cluster 5Fig.
7.
Average margin and tFig.
7 indicates that the average margin changeswith t for each cluster and the values of tmaximizing the margin are different for differentclusters.
For the 5 clusters, the estimated t is 16, 8,14, 12 and 21 respectively.
So we need to530estimate the best t for each cluster.After estimation of t, EM was used to estimatethe membership probabilities.
Table 1 gives theaverage pointwise entropy for top 10% to top100% sentences in each cluster, where sentenceswere ordered by their membership probabilities.The values were averaged over 20 runs, and foreach run, 10 document clusters and one summarywere randomly selected, and the entropy wasaveraged over the summaries.Sentences E_Semi E_SVM Significance10% 0.23 0.22 ~20% 0.26 0.27 ~30% 0.32 0.43 *40% 0.35 0.49 **50% 0.42 0.51 *60% 0.46 0.55 *70% 0.48 0.57 *80% 0.59 0.62 ~90% 0.65 0.69 ~100% 0.70 0.73 ~Table 1.
Entropy of classification resultIn Table 1, the column E_Semi shows entropiesof the semi-supervised classification.
It indicatesthat the entropy increases as more sentences areconsidered.
This is not surprising since thesentences are ordered by their membershipprobabilities in a cluster, which can be seen as akind of measure for closeness between sentencesand cluster centroids, and the boundaries betweenclusters become dim with more sentencesconsidered.To compare the performance between thissemi-supervised classification and a standardsupervised method like Support Vector Machines(SVM), Table 1 also lists the average entropy of aSVM (E_SVM) over the runs.
Similarly, wefound that the entropy also increases as sentencesincrease.
Table 2 also gives the significance signover the runs, where *, ** and ~ representp-values <=0.01, (0.01, 0.05] and >0.05, andindicate that the entropy of the semi-supervisedclassification is lower, significantly lower, oralmost the same as that of SVM respectively.Table 1 demonstrates that when the top 10% or20% sentences are considered, the performancebetween the two algorithms shows no difference.The reason may be that these top sentences arecloser to cluster centroids in both cases, and thecluster boundaries in both algorithms are clear interms of these sentences.For the top 30% sentences, the entropy forsemi-supervised classification is lower than thatfor a SVM, and for the top 40%, the differencebecomes significantly lower.
The reason may goto the substantial assumptions behind the twoalgorithms.
SVM, based on local comparison, issuccessful only when more labeled data isavailable.
With only one sentence labeled as inour case, the semi-supervised method, based onglobal distribution, makes use of a large amountof unlabeled data to reveal the underlyingmanifold structure.
Thus, the performance ismuch better than that of a SVM when moresentences are considered.For the top 50% to 70% sentences, E_Semi isstill lower, but not by much.
The reason may bethat some noisy documents are starting to beincluded.
For the top 80% to 100% sentences, theperformance shows no difference again.
Thereason may be that the lower ranking sentencesmay belong to other classes than thoserepresented by summary sentences, and withthese sentences included, the cluster boundariesbecome unclear in both cases.6.4 Evaluation of OrderingWe used the same classification results to test theperformance of our ordering algorithm HO aswell as MO and PO.
Table 2 lists the Kendall?s ?coefficient values for the three algorithms (?_1).The value was averaged over 20 runs, and foreach run, 10 summaries were randomly selectedand the ?
score was averaged over summaries.Since a summary sentence tends to generalize531some sentences in the documents, we also tried tocombine two or three consecutive sentences intoone, and tested their ordering performance (?_2and ?_3) respectively.?
HO MO PO?_1  0.42 0.31 0.33?_2 0.33 0.26 0.29?_3 0.27 0.21 0.25Table 2. ?
scores for HO, MO and POTable 2 indicates that the combination ofsentences harms the performance.
To see why, wechecked the classification results, and found thatthe pointwise entropies for two and threesentence combinations (for the top 40% sentencein each cluster) increase 12.4% and 18.2%respectively.
This means that the cluster structurebecomes less clear with two or three sentencecombinations, which would lead to less similarsentences being clustered with summarysentences.
This result also suggests that if thesummary sentence subsumes multiple sentencesin the documents, they tend to be not consecutive.Fig.
8 shows change of ?
scores with differentnumber of sentences used for ordering, where xaxis denotes top (1-x)*100% sentences in eachcluster.
The score was averaged over 20 runs, andfor each run, 10 summaries were randomlyselected and evaluated.00.10.20.30.40.50.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1least probability?Fig.
8. ?
scores and number of sentencesFig.
8 indicates that with fewer sentences (x>=0.7) used for ordering, the performancedecreases.
The reason may be that with fewer andfewer sentences used, the result is deficienttraining data for the ordering.
On the other hand,with more sentences used (x <0.6), theperformance also decreases.
The reason may bethat as more sentences are used, the noisysentences could dominate the ordering.
That?swhy we considered only the top 40% sentences ineach cluster as training data for sentencereordering here.As an example, the following is a summary fora cluster of documents about Central Americanstorms, in which the ordering is given manually.1) A category 5 storm, Hurricane Mitch roared across the northwestCaribbean with 180 mph winds across a 350-mile front thatdevastated the mainland and islands of Central America.2) Although the force of the storm diminished, at least 8,000 peopledied from wind, waves and flood damage.3) The greatest losses were in Honduras where some 6,076 peopleperished.4) Around 2,000 people were killed in Nicaragua, 239 in El Salvador,194 in Guatemala, seven in Costa Rica and six in Mexico.5) At least 569,000 people were homeless across Central America.6) Aid was sent from many sources (European Union, the UN, USand Mexico).7) Relief efforts are hampered by extensive damage.Compared with the manual ordering, ouralgorithm HO outputs the ordering [1, 3, 4, 2, 5, 6,7].
In contrast, PO and MO created the orderings[1, 3, 4, 5, 6, 7, 2] and [1, 3, 2, 6, 4, 5, 7]respectively.
In HO?s output, sentence 2 was putin the wrong position.
To check why this was so,we found that sentences in cluster 2 and cluster 3(clusters containing sentence 2 or sentence 3)were very similar, and the size of cluster 3 wasbigger than that of cluster 2.
Also we found thatsentences in cluster 4 mostly followed those incluster 3.
This may explain why the ordering [1, 3,4] occurred.
Due to the link between cluster 2 andcluster 1 or 3, sentence 2 followed sentence 4 inthe ordering.
In PO, sentence 2 was put at the endof the ordering, since it only considered the mostrecent selection when determining next, so cluster1 would not be considered when determining the5324th position.
This suggests that consideration ofselection history does in fact help to group thoserelated sentences more closely, although sentence2 was ranked lower than expected in the example.In MO, we found sentence 2 was putimmediately behind sentence 3.
The reason wasthat, after sentence 1 and 3 were selected, thein-edges of the node representing cluster 2became 0 in the cluster directed graph, and itsin-out edge difference became the biggest amongall nodes in the graph, so it was chosen.
Forsimilar reasons, sentence 6 was put behindsentence 2.
This suggests that it may be difficultto consider the selection history in MO, since itsselection is mainly based on the current status ofclusters.6.
Conclusion and Future WorkIn this paper, we propose a sentence orderingmethod for multi-document summarization basedon semi-supervised classification and historicalordering.
For sentence classification, thesemi-supervised classification groups sentencesbased on their global distribution, rather than onlocal comparisons.
Thus, even with a smallamount of labeled data (just 1 labeled example inour case) we nevertheless ensure goodperformance for sentence classification.For sentence ordering, we propose a kind ofhistory-based ordering strategy, which determinesthe next selection based on the whole selectionhistory, rather than the most recent singleselection in probabilistic ordering, which couldresult in topic bias, or in-out difference in MO,which could result in topic disruption.In this work, we mainly use sentence-levelinformation, including sentence similarity andsentence order, etc.
In future, we may explore therole of term-level or word-level features, e.g.,proper nouns, in the ordering of summarysentences.
To make summaries more coherent andreadable, we may also need to discover how todetect and control topic movement automaticsummaries.
One specific task is how to generateco-reference among sentences in summaries.
Inaddition, we will also try other semi-supervisedclassification methods, and other evaluationmetrics, etc.ReferenceBarzilay, R N. Elhadad, and K. McKeown.
2002.Inferring strategies for sentence ordering inmultidocument news summarization.
Journal ofArtificial Intelligence Research, 17:35?55.Bollegala D. Okazaki, N. Ishizuka, M. 2005.
AMachine Learning Approach to Sentence Orderingfor Multidocument Summarization, in Proceedingsof IJCNLP.Dash M. and H. Liu, (2000) Unsupervised featureselection, proceedings of PAKDD.Filatova, E. & Hovy, E. (2001) Assigning time-stampsto event-clauses.
In Proceedings of AACL/EACLworkshop on Temporal and Spatial InformationProcessing.Lapata, M. 2003.
Probabilistic text structuring:Experiments with sentence ordering.
In Proceedingsof the annual meeting of ACL 545?552.Lin, J.
1991.
Divergence Measures Based on theShannon Entropy.
IEEE Transactions onInformation Theory, 37:1, 145?150.McKeown K., Barzilay R. Evans D., HatzivassiloglouV., Kan M., Schiffman B., &Teufel, S. (2001).Columbia multi-document summarization:Approach and evaluation.
In Proceedings of DUC.Reiter, Ehud and Robert Dale.
2000.
Building NaturalLanguage Generation Systems.
CambridgeUniversity Press, Cambridge.Szummer M. and T. Jaakkola.
(2001) Partially labeledclassification with markov random walks.
NIPS14.Tishby, N, Slonim, N. (2000) Data clustering byMarkovian relaxation and the InformationBottleneck Method.
NIPS 13.Zhu, X., Ghahramani, Z., & Lafferty, J.
(2003)Semi-Supervised Learning Using Gaussian Fieldsand Harmonic Functions.
ICML-2003.Zhou D., Bousquet, O., Lal, T.N., Weston J.
&Schokopf B.
(2003).
Learning with local and GlobalConsistency.
NIPS 16. pp: 321-328.533
