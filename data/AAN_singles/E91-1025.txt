Parsing without lexicon: the MorP systemAbstractMorP is a system for automatic wordclass assignment on the basis of surfacefeatures.
It has a very small lexicon ofform words (%o entries), and for the restworks entirely on morphological andconfigurational patterns.
This makes itrobust and fast, and in spite of the(deliberate) restrictedness of the system,its performance reaches an average ac-curacy level above 91% when run on un-restricted Swedish text.Keywords :  parsing, morphology.The development of the parser to bepresented has been supported by theSwedish Research Council for theHumanities.
The parser is called MorP,for morphology based parser, and thehypotheses behind it can be formulatedthus:a) It is to a large extent possible todecide the word class of words inrunning text from pure surface criteria,such as the morphology of the wordstogether with the configurations thatthey appear in.b) These surface criteria can be de-scribed so dearly that an automaticidentification of word class will bepossible.c) Surface criteria give signals thatwill suffice to give a word class identi-fication with a level of around or aboveGunnel K~llgrenUniversity of StockholmDepartment ofComputational LinguisticsS-106 91 StockholmSwedengunne/@com.qz.segunnel@/ing.su.se90% correctness, atleast for a languagewith asmuch inflectionalSwedish.morphology asA parser was constructed along theselines, which are first presented inBrodda(1982), and the predictions of the hy-potheses were found to hold fairly well.The project is reported in publications inSwedish (K/illgren 1984a) and English(K/illgren 1984b, 1985, 1991a) and theparser has been tested in a practical ap-plication in connection with informationretrieval (K/illgren 1984c, 1991a).
We alsoplan to use the parser in a project aimedat building a large tagged corpus ofSwedish (the SUC corpus, K/illgren 1990,1991b).
The MorP parser is implementedin a high-level string manipulating lan-guage developed at Stockholm Univer-sity by Benny Brodda.
The language iscalled Beta and fuller descriptions of itcan be found in Brodda (1990).
The ver-sion of Beta that is used here is a PC/DOSimplementation written in Pascal?
(Malkior-Carlvik 1990), but Macintoshand DEC versions also exist.The rules of the parser are partitionedbetween different subprograms that per-form recognition of different surface pat-terns of written language.
The first pro-grams work on single words andsegments of words and add their analy-- 143-sis directly into the string.
Later pro-grams look at the markings in the stringand their configurations.
The programscan add markings on previously un-marked words, but can also changemarkings inserted by earlier programs.The units identified by the programs areword classes and two kinds of largerconstituents: noun phrases and preposi-tional phrases.
The latter constituents areestablished mainly as a step in theprocess of identifying word class fromcontextual criteria.
After the processing,the original string is restored and thefinal result of the analysis is given in theform of tags, either after or below thewords or constituents.An interesting feature of the MorPparser is its way of handling non-deter-ministic situations by simply postponingthe decision until enough information isavailable.
The postponing of decisions ispartly done with the use of ambiguousword class markers that are insertedwherever the morphological informa-tion signals two possible word classes.Hereby, all other word classes are ex-cluded, which reduces the number ofpossible choices considerably, and laterprograms can use the information in theambiguous markers both to performanalysis that does no t require full disam-biguation and to ultimately resolve theambiguity.AN EVALUATION OF THE PARSERIn an evaluation of the MorP parser,two texts of which there exists a manualtagging were chosen and cut at the firstsentence boundary after 1,000 words.The texts were run through the MorPparser and the output was compared tothe manual tagging of the texts.MorP was run by a batch file that callsthe programs sequentially and builds upa series of intermediate outputs fromeach program.
Neither the programsthemselves nor this mode of runningthem has in any way been optimized fortime, e.g., unproportionally much time isspent on opening and dosing both rulefiles and text files.
To run a full parse onan AT/386 took 1 minute 5 seconds forone text (1,006 words), giving an averageof 0.065 sec/word, and for the other text(1,004 words) it took I minute I second,average 0.061 sec/word.
With 10,000words, the average is 0.055 sec/word.The larger amounts of text hat can be runin batch, the shorter the relative pro-cessing time will be, and if file handlingwere carried out differently, time woulddecrease considerably.
The figures forruntime could thus be much improved inseveral ways in applications wherespeed was a desirable factor.In evaluating the accuracy of the out-put, single tagged words have beendirectly compared to the correspondingwords in the manually tagged texts.When complex phrases are built up, theirinternal analysis is successively removedwhen it has played its role and is of nomore use in the process.
The tags ofwords in phrases are thus evaluated inthe following way: If a word has had anunambiguous tag at an earlier stage ofthe process that has been removed whenbuilding up the phrase, that tag iscounted.
(Earlier tags can be seen in theintermediate outputs.)
If a word has hadno tag at all or an ambiguous one andthen been incorporated into a phrase, itis regarded as having the word class thatthe incorporation presupposes it to have.That tag is then compared to that of themanually tagged text.- 144  -The errors can be of three kinds: er-roneous word class assignment,unsolved ambiguity, and no assign-ment at all, which is rather a special caseof unsolved ambiguity, cf.
below.
Thefigures for the three kinds are givenbelow.Table of results of word class assignmentNumber Correctof words word classN %Text 303 1,006 920 91.5Text 402 1,004 917 91.3Total 2,010 1,837 91.4possible.
Rather than trimming theparser by increasing the lexicon, itshould first be evaluated as it is, and inaccordance with its basic principles,before any amendments are added to it.It should also be noted that MorP hasbeen tested and evaluated on texts thatare quite different from those on whichit was first developed.Number of errorsWrong Zero Ambig.54 29 343 40 497 69 7These results are remarkably good, inspite of the fact that many other systemsare reported to reach an accuracy of 96-97%.
(Garside 1987, Marshall 1987,DeRose 1988, Church 1988, Ejerhed 1987,O'Shaughnessy 1989.)
Those systems,however, all use "heavier artillery" thanMorP, that has been deliberately re-stricted in accordance with the hypothe-ses presented above.
This restrictivenessconcerns both the size of the lexicon andthe ways of carrying out disambiguation.It is always difficult o define criteria forthe correctness of parses, and the MorPparser must be judged in relation to therestrictions and the limited claims et upfor it.All, or most, errors can of cause beavoided if all disturbing words are put ina lexicon, but now the trick was to get asfar as possible with as little lexicon asIf we look at the roles that differentparts of the MorP parser play in theanalysis, we see that the lexical rules(which are only 435 in number) cover54% of the 2,010 running words of thetexts.
The two texts differ somewhat onthis point.
One of them (text 402) con-tains very many quantifiers which arefound in the lexicon, and that text has58% of its running words covered.
Text303 has 50% coverage after the lexicalrules, a figure that is more "normal" incomparison with my earlier experienceswith the parser.
As can be seen from thetable, the higher proportion of wordscovered by lexicon in text 402 does nothave an overall positive ffect on the finalresult.
The fact that a word is covered bythe lexical rules is by no means aguarantee that it is correctly identified, asthe lexicon only assigns the most prob-able word class.145 -The first three subprograms of MorPwork entirely on the level of singlewords.
After they have been run, disam-biguation proper starts.
The MorP out-put in this intermediate situation is that75% of the running words are marked asbeing unambiguous (though some ofthem later have their tags changed), 11%are marked as two-ways ambiguous, and14% are unmarked.
In practice, thismeans that the latter are four-ways am-biguous, as they can finally come out asnouns, verbs, or adjectives, or remainuntagged.The syntactic part of MorP, coveredby four subprograms, performs both dis-ambiguation and identification of pre-viously unmarked words, which, asstated above, can be seen as a generaliza-tion of the disambiguation process.
Thispart is entirely based on linguistic pat-terns rather than statistical ones.
Ofcourse, there is "statistics" in the disam-biguation rules as well as in the lexicalassignment of tags, in the sense that theentire system is an implementation f myown intuitions as a native speaker ofSwedish, and such intuitions certainlycomprise a feeling for what is more orless common in a language.
Still, MorPwould certainly gain a lot if it were basedon actual statistics on, e.g., the structureof noun phrases or the placement ofadverbials.
The errors arising from theapplication of syntactic patterns in theparsing of the two texts however arelyseem to be due to occurrence of in-frequent patterns, but more to erroneousdisambiguation of the words that arefitted into the patterns.Next, I will give a few examples fromthe texts of the kind of errors that willtypically occur with a simplified Systemlike MorP.
Errors can arise from the lex-icon, from the morphological analysis,from the syntactic disambiguation, andfrom combinations of these.
In text 402,there is also a misspelling, the non- ex-istent form ut ters t  for the adverb yt ters t'ultimately'.
This is correctly treated as aregularly formed adverb, which showssome of the robustness of MorP.We have only a few instances in thesetexts where a word has been erroneouslymarked by the lexicon.
Most notorious isthe case with the word om that can eitherbe a preposition, 'about', or a conjunc-tion, 'if'.
It is marked as a preposition inthe lexicon and a later rule retags it as aconjunction if it has not been amalga-mated with a following noun phrase toform a prepositional phrase by the end ofthe processing.
Mostly, however, it is im-possible to decide the interpretation ofthe word om from its close context, asif-clauses almost always tart with a sub-ject noun phrase.
In the two texts, omoccurs 17 times, 9 times as a prepositionand 8 times as a conjunction.
One of theconjunctions i correctly retagged by thejust mentioned rule, while the others re-main uncorrected.
Regrettably, one ofthe prepositions has also been retaggedas a conjunction, as it is followed by athat-clause and not by a noun phrase.
Ofthe 7 erroneously marked conjunctions,3 are sentence-initial, while no occur-rence of the word as a preposition issentence-initial.
A possible heuristicwould then be to have a retagging rulefor this position before the rules thatbuild prepositional phrases apply.
A re-markable fact is that none of the conjunc-tions om is followed by a later s/I 'then'.A long- range context check looking for' i f -  then' expressions would thus addnothing to the results here.The case with om is a good and typi-cal example of a situation where more- 146  -statistics would be of great advantage inimproving and refining the rules, butwhere there will always be a rest class ofinsoluble cases and cases which are con-trary to the rules.Still, there are not many words: in thesample texts where the tagging done bylexicon is wrong.
This is remarkable, asthe lexicon always assigns exactly onetag, not a set of tags, even if a word isambiguous.The morphological analysis carriesout a very substantial task and, con-sequently, isa large source of errors.
Oneexample is the noun bevis 'proof', whichoccurs several times in one of the texts.
Ithas a very prototypical verbal ook, withthe prefix be-, a monosyllabic stem seem-ingly ending in a vowel and followed bya passive -s, exactly like the verbs beses,beg/ts, betros, bebos, etc.
It is justa coin-cidence that the verb is bevisa, not bevi,and the noun is formed by a rare deletionrather that by adding a derivationalending.
A similar error is when the nounresultat 'result' is treated as a supineverb, as -at is a very common, very pro-ductive supine ending.Disambiguation of course also addsmany errors, as the patterns for thoserules are less clear than the patterns forword structure, and as all errors, am-biguities and doubtful cases from earlierprograms accumulate as the processingproceeds.
Often it is the ambiguous-marked words that are disambiguatedwrongly or not at all.
In one of the textsthere is for instance the alleged finiteverb djungler 'jungles'.
A foregoingadverb has caused the ambiguousending -er to be classified as signallingpresent tense verb rather than pluralnoun.
The remaining ambiguities alsooften belong to this class of words, but onthe whole, it is surprising how few of theambiguous-marked words that remainin the output.The set of words that are still un-marked by the end of the process is com-paratively large.
A possible heuristicmight be to make them all nouns, as thatis the largest open word class, and asmost singular and many plural indefinitenouns have no clear morphological char-acteristics in Swedish.
A closer look atthe unmarked words reveals that this isnot such a good idea: of 69 unmarkedwords, 25 are nouns, 18 adjectives, and18 verbs.
One is a numeral, one is a veryrare preposition that is a homograph of aslightly more common noun, 2 areadverbs with homographs inother wordclasses, and 2 are the first part of con-joined compounds, comparable to ex-pressions like 'pre- or postprocessing'.The hyphenated first part gets no markin these cases.
They could be done awaywith by manual preprocessing, as alsothe not infrequent cases occurring inheadlines, where syntactic structure isoften too reduced to be of any help.
Forthe rest, a careful examination of theirword structure and context seems pro-mising, but more data is needed.By this, I hope to have shown thatparsing without lexicon is both possibleand interesting, and can give insightsabout he structure of natural languagesthat can be of use also in less restrictedsystems.REFERENCESBrodda, B.
1982.
An Experiment inHeur-istic Parsing, in Papers from the 7th Scandi-navian Conference of Linguistics, Dec.1982.
Department of General Linguistics,Publication o.
10, Helsinki 1983.- 147 -Brodda, B.
1990.
Do Corpus Work withPC Beta, (and) be your own ComputationalLinguist o appear in Johansson, S. & Sten-strOm, A.- B.
(eds): English Computer Cor-pora, Mouton-de Gruyter, Berlin 1990/91(under publication).Church, K.W.
1988.
A Stochastic PartsProgram and Noun Phrase Parser for Unre-stricted Text, in Proceedings of the SecondConference on Applied Natural LanguageProcessing, Austin, Texas.DeRose, S.J.
1988.
Grammatical cate-gory disambiguation by statistical optimiza-tion.
Computational Linguistics Vol.
14:1.Ejerhed, E. 1987.
Finding Noun Phrasesand Clauses in Unrestricted Text: On the Useof Stochastic and Finitary Methods in TextAnalysis.
MS, AT&T Bell Labs.Garside, R. 1987.
The CLAWS word-tag-ging system, in Garside, R., G. Leech & G.Sampson (eds.
), 1987.Garside, R., G. Leech & G.
Sampson(eds.).
1987.
The Computational Analysisof English.
Longman.K~illgren, G. 1984a.
HP-systemet somgenv/ig vid syntaktisk markning av texter, inSvenskans beskrivning 14, p. 39-45.
Uni-versity of Lund.Kifllgren, G. 1984b.
HP - A HeuristicFinite State Parser Based on Morpholo .g~, inSAgvall-Hein, Anna :(ed.)
De nordiskadatalingvistikdagarna 1983, p. 155-162.University of Uppsala.
:K~illgren, G. 1984c.
Automatisk ex-cerpering av substantiv ur 1Opande t xt.
Ettm6jligt hjiflpmedel vid automatisk indexer-ing?
IRl-rapport 1984:1.
The Swedish Lawand Informatics Research Institute, Stock-holm University.K~llgren, G. 1985.
A Pattern MatchingParser, in Togeby, Ole (ed.)
Papers from theEighth Scandinavian Conference of Lin-guistics.
Copenhagen University.Kallgren, G. 1990.
"The first million ishardest o get": Building a Large TaggedCorpus as Automatically as Possible.
Pro,.ceedings from Coling '90.
Helsinki.Kitllgren, G. 1991a.
Making Maximal useof Surface Criteria in Large Scale Parsing: theMorp Parser, Papers from the Institute ofLinguistics, University of Stockholm(PILUS).K/lllgren, G. 1991b.
Storskaligt korpusar-bete ph dator.
En presentation av SUC-kor-pusen.
Svenskans beskrivning 1990.
Uni-versity of Uppsala.Malkior, S. & Carlvik, M. 1990.
PC BetaReference.
Institute of Linguistics, Stock-holm University.Marshall, I.
1987.
Tag selection usingprobabilistic methods, in Garside, R., G.Leech & G. Sampson (eds.
), 1987.O'Shaughnessy, D. 1989.
Parsing with aSmall Dictionary for Applications uch asText to Speech.
Computational LinguisticsVol.
15:2.- 148 -
