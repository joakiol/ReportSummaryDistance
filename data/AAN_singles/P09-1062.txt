Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 549?557,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPSummarizing multiple spoken documents: finding evidence fromuntranscribed audioXiaodan Zhu, Gerald Penn and Frank RudziczUniversity of Toronto10 King?s College Rd.,Toronto, M5S 3G4, ON, Canada{xzhu,gpenn,frank}@cs.toronto.eduAbstractThis paper presents a model for summa-rizing multiple untranscribed spoken doc-uments.
Without assuming the availabil-ity of transcripts, the model modifies arecently proposed unsupervised algorithmto detect re-occurring acoustic patterns inspeech and uses them to estimate similari-ties between utterances, which are in turnused to identify salient utterances and re-move redundancies.
This model is of in-terest due to its independence from spo-ken language transcription, an error-proneand resource-intensive process, its abil-ity to integrate multiple sources of infor-mation on the same topic, and its noveluse of acoustic patterns that extends pre-vious work on low-level prosodic featuredetection.
We compare the performance ofthis model with that achieved using man-ual and automatic transcripts, and find thatthis new approach is roughly equivalentto having access to ASR transcripts withword error rates in the 33?37% range with-out actually having to do the ASR, plusit better handles utterances with out-of-vocabulary words.1 IntroductionSummarizing spoken documents has been exten-sively studied over the past several years (Pennand Zhu, 2008; Maskey and Hirschberg, 2005;Murray et al, 2005; Christensen et al, 2004;Zechner, 2001).
Conventionally called speechsummarization, although speech connotes morethan spoken documents themselves, it is motivatedby the demand for better ways to navigate spokencontent and the natural difficulty in doing so ?speech is inherently more linear or sequential thantext in its traditional delivery.Previous research on speech summarization hasaddressed several important problems in this field(see Section 2.1).
All of this work, however,has focused on single-document summarizationand the integration of fairly simplistic acousticfeatures, inspired by work in descriptive linguis-tics.
The issues of navigating speech content aremagnified when dealing with larger collections ?multiple spoken documents on the same topic.
Forexample, when one is browsing news broadcastscovering the same events or call-centre record-ings related to the same type of customer ques-tions, content redundancy is a prominent issue.Multi-document summarization on written docu-ments has been studied for more than a decade(see Section 2.2).
Unfortunately, no such efforthas been made on audio documents yet.An obvious way to summarize multiple spo-ken documents is to adopt the transcribe-and-summarize approach, in which automatic speechrecognition (ASR) is first employed to acquirewritten transcripts.
Speech summarization is ac-cordingly reduced to a text summarization taskconducted on error-prone transcripts.Such an approach, however, encounters severalproblems.
First, assuming the availability of ASRis not always valid for many languages other thanEnglish that one may want to summarize.
Evenwhen it is, transcription quality is often an issue?training ASR models requires collecting and an-notating corpora on specific languages, dialects,or even different domains.
Although recognitionerrors do not significantly impair extractive sum-marizers (Christensen et al, 2004; Zhu and Penn,2006), error-laden transcripts are not necessarilybrowseable if recognition errors are higher thancertain thresholds (Munteanu et al, 2006).
Insuch situations, audio summaries are an alterna-tive when salient content can be identified directlyfrom untranscribed audio.
Third, the underlyingparadigm of most ASR models aims to solve a549classification problem, in which speech is seg-mented and classified into pre-existing categories(words).
Words not in the predefined dictionaryare certain to be misrecognized without excep-tion.
This out-of-vocabulary (OOV) problem isunavoidable in the regular ASR framework, al-though it is more likely to happen on salient wordssuch as named entities or domain-specific terms.Our approach uses acoustic evidence from theuntranscribed audio stream.
Consider text sum-marization first: many well-known models suchas MMR (Carbonell and Goldstein, 1998) andMEAD (Radev et al, 2004) rely on the reoccur-rence statistics of words.
That is, if we switchany word w1 with another word w2 across anentire corpus, the ranking of extracts (often sen-tences) will be unaffected, because no word-specific knowledge is involved.
These mod-els have achieved state-of-the-art performance intranscript-based speech summarization (Zechner,2001; Penn and Zhu, 2008).
For spoken docu-ments, such reoccurrence statistics are availabledirectly from the speech signal.
In recent years, avariant of dynamic time warping (DTW) has beenproposed to find reoccurring patterns in the speechsignal (Park and Glass, 2008).
This method hasbeen successfully applied to tasks such as worddetection (Park and Glass, 2006) and topic bound-ary detection (Malioutov et al, 2007).Motivated by the work above, this paper ex-plores the approach to summarizing multiple spo-ken documents directly over an untranscribed au-dio stream.
Such a model is of interest because ofits independence from ASR.
It is directly applica-ble to audio recordings in languages or domainswhen ASR is not possible or transcription qualityis low.
In principle, this approach is free from theOOV problem inherent to ASR.
The premise ofthis approach, however, is to reliably find reoccur-ing acoustic patterns in audio, which is challeng-ing because of noise and pronunciation varianceexisting in the speech signal, as well as the dif-ficulty of finding alignments with proper lengthscorresponding to words well.
Therefore, our pri-mary goal in this paper is to empirically determinethe extent to which acoustic information alone caneffectively replace conventional speech recogni-tion with or without simple prosodic feature de-tection within the multi-document speech summa-rization task.
As shown below, a modification ofthe Park-Glass approach amounts to the efficacyof a 33-37% WER ASR engine in the domainof multiple spoken document summarization, andalso has better treatment of OOV items.
Park-Glass similarity scores by themselves can attributea high score to distorted paths that, in our context,ultimately leads to too many false-alarm align-ments, even after applying the distortion thresh-old.
We introduce additional distortion penaltyand subpath length constraints on their scoring todiscourage this possibility.2 Related work2.1 Speech summarizationAlthough abstractive summarization is more de-sirable, the state-of-the-art research on speechsummarization has been less ambitious, focus-ing primarily on extractive summarization, whichpresents the most important N% of words,phrases, utterances, or speaker turns of a spo-ken document.
The presentation can be in tran-scripts (Zechner, 2001), edited speech data (Fu-rui et al, 2003), or a combination of these (Heet al, 2000).
Audio data amenable to summa-rization include meeting recordings (Murray et al,2005), telephone conversations (Zhu and Penn,2006; Zechner, 2001), news broadcasts (Maskeyand Hirschberg, 2005; Christensen et al, 2004),presentations (He et al, 2000; Zhang et al, 2007;Penn and Zhu, 2008), etc.Although extractive summarization is not asideal as abstractive summarization, it outperformsseveral comparable alternatives.
Tucker and Whit-taker (2008) have shown that extractive summa-rization is generally preferable to time compres-sion, which speeds up the playback of audio doc-uments with either fixed or variable rates.
He etal.
(2000) have shown that either playing back im-portant audio-video segments or just highlightingthe corresponding transcripts is significantly bet-ter than providing users with full transcripts, elec-tronic slides, or both for browsing presentationrecordings.Given the limitations associated with ASR, it isno surprise that previous work (He et al, 1999;Maskey and Hirschberg, 2005; Murray et al,2005; Zhu and Penn, 2006) has studied featuresavailable in audio.
The focus, however, is pri-marily limited to prosody.
The assumption is thatprosodic effects such as stress can indicate salientinformation.
Since a direct modeling of compli-cated compound prosodic effects like stress is dif-550ficult, they have used basic features of prosody in-stead, such as pitch, energy, duration, and pauses.The usefulness of prosody was found to be verylimited by itself, if the effect of utterance length isnot considered (Penn and Zhu, 2008).
In multiple-spoken-document summarization, it is unlikelythat prosody will be more useful in predicatingsalience than in single document summarization.Furthermore, prosody is also unlikely to be appli-cable to detecting or handling redundancy, whichis prominent in the multiple-document setting.All of the work above has been conducted onsingle-document summarization.
In this paperwe are interested in summarizing multiple spo-ken documents by using reoccurrence statistics ofacoustic patterns.2.2 Multiple-document summarizationMulti-document summarization on written texthas been studied for over a decade.
Comparedwith the single-document task, it needs to removemore content, cope with prominent redundancy,and organize content from different sources prop-erly.
This field has been pioneered by early worksuch as the SUMMONS architecture (Mckeownand Radev, 1995; Radev and McKeown, 1998).Several well-known models have been proposed,i.e., MMR (Carbonell and Goldstein, 1998), multi-Gen (Barzilay et al, 1999), and MEAD (Radevet al, 2004).
Multi-document summarization hasreceived intensive study at DUC.
1 Unfortunately,no such efforts have been extended to summarizemultiple spoken documents yet.Abstractive approaches have been studied sincethe beginning.
A famous effort in this directionis the information fusion approach proposed inBarzilay et al (1999).
However, for error-pronetranscripts of spoken documents, an abstractivemethod still seems to be too ambitious for the timebeing.
As in single-spoken-document summariza-tion, this paper focuses on the extractive approach.Among the extractive models, MMR (Carbonelland Goldstein, 1998) and MEAD (Radev et al,2004), are possibly the most widely known.
Bothof them are linear models that balance salience andredundancy.
Although in principle, these mod-els allow for any estimates of salience and re-dundancy, they themselves calculate these scoreswith word reoccurrence statistics, e.g., tf.idf,and yield state-of-the-art performance.
MMR it-1http://duc.nist.gov/eratively selects sentences that are similar to theentire documents, but dissimilar to the previouslyselected sentences to avoid redundancy.
Its de-tails will be revisited below.
MEAD uses a redun-dancy removal mechanism similar to MMR, butto decide the salience of a sentence to the wholetopic, MEAD uses not only its similarity scorebut also sentence position, e.g., the first sentenceof each new story is considered important.
Ourwork adopts the general framework of MMR andMEAD to study the effectiveness of the acousticpattern evidence found in untranscribed audio.3 An acoustics-based approachThe acoustics-based summarization techniqueproposed in this paper consists of three consecu-tive components.
First, we detect acoustic patternsthat recur between pairs of utterances in a set ofdocuments that discuss a common topic.
The as-sumption here is that lemmata, words, or phrasesthat are shared between utterances are more likelyto be acoustically similar.
The next step is to com-pute a relatedness score between each pair of ut-terances, given the matching patterns found in thefirst step.
This yields a symmetric relatedness ma-trix for the entire document set.
Finally, the relat-edness matrix is incorporated into a general sum-marization model, where it is used for utteranceselection.3.1 Finding common acoustic patternsOur goal is to identify subsequences within acous-tic sequences that appear highly similar to regionswithin other sequences, where each sequence con-sists of a progression of overlapping 20ms vec-tors (frames).
In order to find those shared pat-terns, we apply a modification of the segmen-tal dynamic time warping (SDTW) algorithm topairs of audio sequences.
This method is similarto standard DTW, except that it computes multi-ple constrained alignments, each within predeter-mined bands of the similarity matrix (Park andGlass, 2008).2 SDTW has been successfully ap-plied to problems such as topic boundary detec-tion (Malioutov et al, 2007) and word detection(Park and Glass, 2006).
An example applicationof SDTW is shown in Figure 1, which shows theresults of two utterances from the TDT-4 Englishdataset:2Park and Glass (2008) used Euclidean distance.
We usedcosine distance instead, which was found to be better on ourheld-out dataset.551I: the explosion in aden harbor killed seven-teen u.s. sailors and injured other thirtynine last month.II: seventeen sailors were killed.These two utterances share three words: killed,seventeen, and sailors, though in different orders.The upper panel of Figure 1 shows a matrix offrame-level similarity scores between these twoutterances where lighter grey represents highersimilarity.
The lower panel shows the four mostsimilar shared subpaths, three of which corre-spond to the common words, as determined by theapproach detailed below.Figure 1: Using segmental dynamic time warpingto find matching acoustic patterns between two ut-terances.Calculating MFCCThe first step of SDTW is to represent each utter-ance as sequences of Mel-frequency cepstral coef-ficient (MFCC) vectors, a commonly used repre-sentation of the spectral characteristics of speechacoustics.
First, conventional short-time Fouriertransforms are applied to overlapping 20ms Ham-ming windows of the speech amplitude signal.The resulting spectral energy is then weightedby filters on the Mel-scale and converted to 39-dimensional feature vectors, each consisting of 12MFCCs, one normalized log-energy term, as wellas the first and second derivatives of these 13 com-ponents over time.
The MFCC features used inthe acoustics-based approach are the same as thoseused below in the ASR systems.As in (Park and Glass, 2008), an additionalwhitening step is taken to normalize the varianceson each of these 39 dimensions.
The similaritiesbetween frames are then estimated using cosinedistance.
All similarity scores are then normalizedto the range of [0, 1], which yields similarity ma-trices exemplified in the upper panel of Figure 1.Finding optimal pathsFor each similarity matrix obtained above, localalignments of matching patterns need to be found,as shown in the lower panel of Figure 1.
A sin-gle global DTW alignment is not adequate, sincewords or phrases held in common between utter-ances may occur in any order.
For example, in Fig-ure 1 killed occurs before all other shared words inone document and after all of these in the other, soa single alignment path that monotonically seeksthe lower right-hand corner of the similarity ma-trix could not possibly match all common words.Instead, multiple DTWs are applied, each startingfrom different points on the left or top edges of thesimilarity matrix, and ending at different points onthe bottom or right edges, respectively.
The widthof this diagonal band is proportional to the esti-mated number of words per sequence.Given an M -by-N matrix of frame-level simi-larity scores, the top-left corner is considered theorigin, and the bottom-right corner represents analignment of the last frames in each sequence.
Foreach of the multiple starting points p0 = (x0, y0)where either x0 = 0 or y0 = 0, but not neces-sarily both, we apply DTW to find paths P =p0, p1, ..., pK that maximize?0?
i?
K sim(pi),where sim(pi) is the cosine similarity score ofpoint pi = (xi, yi) in the matrix.
Each point on thepath, pi, is subject to the constraint |xi ?
yi| < T ,where T limits the distortion of the path, as wedetermine experimentally.
The ending points arepK = (xK , yK) with either xK = N or yK =M .
For considerations of efficiency, the multi-ple DTW processes do not start from every pointon the left or top edges.
Instead, they skip everyT such starting points, which still guarantees thatthere will be no blind-spot in the matrices that areinaccessible to all DTW search paths.Finding optimal subpathsAfter the multiple DTW paths are calculated, theoptimal subpath on each is then detected in or-der to find the local alignments where the simi-larity is maximal, which is where we expect ac-tual matched phrases to occur.
For a given pathP = p0, p2, ..., pK , the optimal subpath is definedto be a continuous subpath, P ?
= pm, pm+1..., pn552that maximizes?m?i?n sim(pi)n?m+1 , 0 ?
n ?
m ?
k,and m ?
n + 1 ?
L. That is, the subpath is atleast as long as L and has the maximal averagesimilarity.
L is used to avoid short alignments thatcorrespond to subword segments or short functionwords.
The value of L is determined on a devel-opment set.The version of SDTW employed by (Malioutovet al, 2007) and Park and Glass (2008) employedan algorithm of complexity O(Klog(L)) from(Lin et al, 2002) to find subpaths.
Lin et al (2002)have also proven that the length of the optimal sub-path is between L and 2L?
1, inclusively.
There-fore, our version uses a very simple algorithm?just search and find the maximum of average simi-larities among all possible subpaths with lengthsbetween L and 2L ?
1.
Although the theoreti-cal upper bound for this algorithm is O(KL), inpractice we have found no significant increase incomputation time compared with the O(Klog(L))algorithm?L is actually a constant for both Parkand Glass (2008) and us, it is much smaller thanK, and the O(Klog(L)) algorithm has (constant)overhead of calculating right-skew partitions.In our implementation, since most of the time isspent on calculating the average similarity scoreson candidate subpaths, all average scores aretherefore pre-calculated incrementally and saved.We have also parallelized the computation of sim-ilarities by topics over several computer clusters.A detailed comparison of different parallelizationtechniques has been conducted by Gajjar et al(2008).
In addition, comparing time efficiencybetween the acoustics-based approach and ASR-based summarizers is interesting but not straight-forward since a great deal of comparable program-ming optimization needs to be additionally consid-ered in the present approach.3.2 Estimating utterance-level similarityIn the previous stage, we calculated frame-levelsimilarities between utterance pairs and used theseto find potential matching patterns between theutterances.
With this information, we estimateutterance-level similarities by estimating the num-bers of true subpath alignments between two utter-ances, which are in turn determined by combiningthe following features associated with subpaths:Similarity of subpathWe compute similarity features on each subpath.We have obtained the average similarity score ofeach subpath as discussed in Section 3.1.
Basedon this, we calculate relative similarity scores,which are computed by dividing the original sim-ilarity of a given subpath by the average similar-ity of its surrounding background.
The motivationfor capturing the relative similarity is to punishsubpaths that cannot distinguish themselves fromtheir background, e.g., those found in a block ofhigh-similarity regions caused by certain acousticnoise.Distortion scoreWarped subpaths are less likely to correspond tovalid matching patterns than straighter ones.
Inaddition to removing very distorted subpaths byapplying a distortion threshold as in (Park andGlass, 2008), we also quantitatively measured theremaining ones.
We fit each of them with least-square linear regression and estimate the residuescores.
As discussed above, each point on a sub-path satisfies |xi ?
yi| < T , so the residue cannotbe bigger than T .
We used this to normalize thedistortion scores to the range of [0,1].Subpath lengthGiven two subpaths with nearly identical averagesimilarity scores, we suggest that the longer of thetwo is more likely to refer to content of interestthat is shared between two speech utterances, e.g.,named entities.
Longer subpaths may in this sensetherefore be more useful in identifying similaritiesand redundancies within a speech summarizationsystem.
As discussed above, since the length of asubpath len(P ?)
has been proven to fall betweenL and 2L ?
1, i.e., L ?
len(P ?)
?
2L ?
1,given a parameter L, we normalize the path lengthto (len(P ?)
?
L)/L, corresponding to the range[0,1).The similarity scores of subpaths can vary widelyover different spoken documents.
We do not usethe raw similarity score of a subpath, but ratherits rank.
For example, given an utterance pair, thetop-1 subpath is more likely to be a true alignmentthan the rest, even if its distortion score may behigher.
The similarity ranks are combined withdistortion scores and subpath lengths simply asfollows.
We divide subpaths into the top 1, 3, 5,and 10 by their raw similarity scores.
For sub-paths in each group, we check whether their dis-tortion scores are below and lengths are above553some thresholds.
If they are, in any group, thenthe corresponding subpaths are selected as ?true?alignments for the purposes of building utterance-level similarity matrix.
The numbers of true align-ments are used to measure the similarity betweentwo utterances.
We therefore have 8 threshold pa-rameters to estimate, and subpaths with similarityscores outside the top 10 are ignored.
The rankgroups are checked one after another in a decisionlist.
Powell?s algorithm (Press et al, 2007) is usedto find the optimal parameters that directly mini-mize summarization errors made by the acoustics-based model relative to utterances selected frommanual transcripts.3.3 Extractive summarizationOnce the similarity matrix between sentences in atopic is acquired, we can conduct extractive sum-marization by using the matrix to estimate bothsimilarity and redundancy.
As discussed above,we take the general framework of MMR andMEAD, i.e., a linear model combining salienceand redundancy.
In practice, we used MMR in ourexperiments, since the original MEAD considersalso sentence positions 3 , which can always beenadded later as in (Penn and Zhu, 2008).To facilitate our discussion below, we briefly re-visit MMR here.
MMR (Carbonell and Goldstein,1998) iteratively augments the summary with ut-terances that are most similar to the documentset under consideration, but most dissimilar to thepreviously selected utterances in that summary, asshown in the equation below.
Here, the sim1 termrepresents the similarity between a sentence andthe document set it belongs to.
The assumption isthat a sentence having a higher sim1 would betterrepresent the content of the documents.
The sim2term represents the similarity between a candidatesentence and sentences already in the summary.
Itis used to control redundancy.
For the transcript-based systems, the sim1 and sim2 scores in thispaper are measured by the number of words sharedbetween a sentence and a sentence/document setmentioned above, weighted by the idf scores ofthese words, which is similar to the calculation ofsentence centroid values by Radev et al (2004).3The usefulness of position varies significantly in differ-ent genres (Penn and Zhu, 2008).
Even in the news domain,the style of broadcast news differs from written news, forexample, the first sentence often serves to attract audiences(Christensen et al, 2004) and is hence less important as inwritten news.
Without consideration of position, MEAD ismore similar to MMR.Note that the acoustics-based approach estimatesthis by using the method discussed above in Sec-tion 3.2.Nextsent = argmaxtnr,j(?
sim1(doc, tnr,j)?
(1 ?
?
)maxtr,ksim2(tnr,j, tr,k))4 Experimental setupWe use the TDT-4 dataset for our evaluation,which consists of annotated news broadcastsgrouped into common topics.
Since our aim in thispaper is to study the achievable performance of theaudio-based model, we grouped together news sto-ries by their news anchors for each topic.
Then weselected the largest 20 groups for our experiments.Each of these contained between 5 and 20 articles.We compare our acoustics-only approachagainst transcripts produced automatically fromtwo ASR systems.
The first set of transcriptswas obtained directly from the TDT-4 database.These transcripts contain a word error rate of12.6%, which is comparable to the best accura-cies obtained in the literature on this data set.We also run a custom ASR system designed toproduce transcripts at various degrees of accu-racy in order to simulate the type of performanceone might expect given languages with sparsertraining corpora.
These custom acoustic mod-els consist of context-dependent tri-phone unitstrained on HUB-4 broadcast news data by se-quential Viterbi forced alignment.
During eachround of forced alignment, the maximum likeli-hood linear regression (MLLR) transform is usedon gender-dependent models to improve the align-ment quality.
Language models are also trained onHUB-4 data.Our aim in this paper is to study the achievableperformance of the audio-based model.
Insteadof evaluating the result against human generatedsummaries, we directly compare the performanceagainst the summaries obtained by using manualtranscripts, which we take as an upper bound tothe audio-based system?s performance.
This ob-viously does not preclude using the audio-basedsystem together with other features such as utter-ance position, length, speaker?s roles, and mostothers used in the literature (Penn and Zhu, 2008).Here, we do not want our results to be affected bythem with the hope of observing the difference ac-curately.
As such, we quantify success based onROUGE (Lin, 2004) scores.
Our goal is to evalu-554ate whether the relatedness of spoken documentscan reasonably be gleaned solely from the surfaceacoustic information.5 Experimental resultsWe aim to empirically determine the extent towhich acoustic information alone can effectivelyreplace conventional speech recognition within themulti-document speech summarization task.
SinceASR performance can vary greatly as we dis-cussed above, we compare our system againstautomatic transcripts having word error rates of12.6%, 20.9%, 29.2%, and 35.5% on the samespeech source.
We changed our language mod-els by restricting the training data so as to obtainthe worst WER and then interpolated the corre-sponding transcripts with the TDT-4 original au-tomatic transcripts to obtain the rest.
Figure 2shows ROUGE scores for our acoustics-only sys-tem, as depicted by horizontal lines, as well asthose for the extractive summaries given automatictranscripts having different WERs, as depictedby points.
Dotted lines represent the 95% con-fidence intervals of the transcript-based models.Figure 2 reveals that, typically, as the WERs of au-tomatic transcripts increase to around 33%-37%,the difference between the transcript-based and theacoustics-based models is no longer significant.These observations are consistent across sum-maries with different fixed lengths, namely 10%,20%, and 30% of the lengths of the source docu-ments for the top, middle, and bottom rows of Fig-ure 2, respectively.
The consistency of this trend isshown across both ROUGE-2 and ROUGE-SU4,which are the official measures used in the DUCevaluation.
We also varied the MMR parameter ?within a typical range of 0.4?1, which yielded thesame observation.Since the acoustics-based approach can be ap-plied to any data domain and to any languagein principle, this would be of special interestwhen those situations yield relatively high WERwith conventional ASR.
Figure 2 also shows theROUGE scores achievable by selecting utterancesuniformly at random for extractive summarization,which are significantly lower than all other pre-sented methods and corroborate the usefulness ofacoustic information.Although our acoustics-based method performssimilarly to automatic transcripts with 33-37%WER, the errors observed are not the same, which0 0.1 0.2 0.3 0.4 0.50.70.750.80.850.90.951Len=10% Rand=0.197ROUGE?SU4Word error rate0 0.1 0.2 0.3 0.4 0.50.70.750.80.850.90.951Len=20%, Rand=0.340ROUGE?SU4Word error rate0 0.1 0.2 0.3 0.4 0.50.70.750.80.850.90.951Len=30%, Rand=0.402ROUGE?SU4Word error rate0 0.1 0.2 0.3 0.4 0.50.70.750.80.850.90.951Len=10%, Rand=0.176ROUGE?2Word error rate0 0.1 0.2 0.3 0.4 0.50.70.750.80.850.90.951Len=20%, Rand=0.324ROUGE?2Word error rate0 0.1 0.2 0.3 0.4 0.50.70.750.80.850.90.951Len=30%, Rand=0.389ROUGE?2Word error rateFigure 2: ROUGE scores and 95% confidence in-tervals for the MMR-based extractive summariesproduced from our acoustics-only approach (hori-zontal lines), and from ASR-generated transcriptshaving varying WER (points).
The top, middle,and bottom rows of subfigures correspond to sum-maries whose lengths are fixed at 10%, 20%, and30% the sizes of the source text, respectively.
?
inMMR takes 1, 0.7, and 0.4 in these rows, respec-tively.we attribute to fundamental differences betweenthese two methods.
Table 1 presents the numberof different utterances correctly selected by theacoustics-based and ASR-based methods acrossthree categories, namely those sentences that arecorrectly selected by both methods, those ap-pearing only in the acoustics-based summaries,and those appearing only in the ASR-based sum-maries.
These are shown for summaries havingdifferent proportional lengths relative to the sourcedocuments and at different WERs.
Again, correct-ness here means that the utterance is also selectedwhen using a manual transcript, since that is ourdefined topline.A manual analysis of the corpus shows thatutterances correctly included in summaries by555Summ.
Both ASR Aco.-length only onlyWER=12.6%10% 85 37 820% 185 62 1230% 297 87 20WER=20.9%10% 83 36 1020% 178 65 1930% 293 79 24WER=29.2%10% 77 34 1620% 172 58 2530% 286 64 31WER=35.5%10% 75 33 1820% 164 54 3330% 272 67 45Table 1: Utterances correctly selected by boththe ASR-based models and acoustics-based ap-proach, or by either of them, under differentWERs (12.6%, 20.9%, 29.2%, and 35.5%) andsummary lengths (10%, 20%, and 30% utterancesof the original documents)the acoustics-based method often contain out-of-vocabulary errors in the corresponding ASR tran-scripts.
For example, given the news topic of thebombing of the U.S. destroyer ship Cole in Yemen,the ASR-based method always mistook the wordCole, which was not in the vocabulary, for cold,khol, and called.
Although named entities anddomain-specific terms are often highly relevantto the documents in which they are referenced,these types of words are often not included inASR vocabularies, due to their relative global rar-ity.
Importantly, an unsupervised acoustics-basedapproach such as ours does not suffer from thisfundamental discord.
At the very least, these find-ings suggest that ASR-based summarization sys-tems augmented with our type of approach mightbe more robust against out-of-vocabulary errors.It is, however, very encouraging that an acoustics-based approach can perform to within a typicalWER range within non-broadcast-news domains,although those domains can likewise be morechallenging for the acoustics-based approach.
Fur-ther experimentation is necessary.
It is also of sci-entific interest to be able to quantify this WER asan acoustics-only baseline for further research onASR-based spoken document summarizers.6 Conclusions and future workIn text summarization, statistics based on wordcounts have traditionally served as the foundationof state-of-the-art models.
In this paper, the simi-larity of utterances is estimated directly from re-curring acoustic patterns in untranscribed audiosequences.
These relatedness scores are then in-tegrated into a maximum marginal relevance lin-ear model to estimate the salience and redundancyof those utterance for extractive summarization.Our empirical results show that the summarizationperformance given acoustic information alone isstatistically indistinguishable from that of modernASR on broadcast news in cases where the WERof the latter approaches 33%-37%.
This is an en-couraging result in cases where summarization isrequired, but ASR is not available or speech recog-nition performance is degraded.
Additional anal-ysis suggests that the acoustics-based approachis useful in overcoming situations where out-of-vocabulary error may be more prevalent, and wesuggest that a hybrid approach of traditional ASRwith acoustics-based pattern matching may be themost desirable future direction of research.One limitation of the current analysis is thatsummaries are extracted only for collections ofspoken documents from among similar speakers.Namely, none of the topics under analysis consistsof a mix of male and female speakers.
We are cur-rently investigating supervised methods to learnjoint probabilistic models relating the acoustics ofgroups of speakers in order to normalize acousticsimilarity matrices (Toda et al, 2001).
We sug-gest that if a stochastic transfer function betweenmale and female voices can be estimated, then thesomewhat disparate acoustics of these groups ofspeakers may be more easily compared.ReferencesR.
Barzilay, K. McKeown, and M. Elhadad.
1999.
In-formation fusion in the context of multi-documentsummarization.
In Proc.
of the 37th Association forComputational Linguistics, pages 550?557.J.
G. Carbonell and J. Goldstein.
1998.
The use ofmmr, diversity-based reranking for reordering doc-uments and producing summaries.
In Proceedingsof the 21st annual international ACM SIGIR con-ference on research and development in informationretrieval, pages 335?336.H.
Christensen, B. Kolluru, Y. Gotoh, and S. Renals.2004.
From text summarisation to style-specific556summarisation for broadcast news.
In Proceedingsof the 26th European Conference on Information Re-trieval (ECIR-2004), pages 223?237.S.
Furui, T. Kikuichi, Y. Shinnaka, and C. Hori.
2003.Speech-to-speech and speech to text summarization.In First International workshop on Language Un-derstanding and Agents for Real World Interaction.M.
Gajjar, R. Govindarajan, and T. V. Sreenivas.
2008.Online unsupervised pattern discovery in speech us-ing parallelization.
In Proc.
Interspeech, pages2458?2461.L.
He, E. Sanocki, A. Gupta, and J. Grudin.
1999.Auto-summarization of audio-video presentations.In Proceedings of the seventh ACM internationalconference on Multimedia, pages 489?498.L.
He, E. Sanocki, A. Gupta, and J. Grudin.
2000.Comparing presentation summaries: Slides vs. read-ing vs. listening.
In Proceedings of ACM CHI, pages177?184.Y.
Lin, T. Jiang, and Chao.
K. 2002.
Efficient al-gorithms for locating the length-constrained heavi-est segments with applications to biomolecular se-quence analysis.
J.
Computer and System Science,63(3):570?586.C.
Lin.
2004.
Rouge: a package for automaticevaluation of summaries.
In Proceedings of the42st Annual Meeting of the Association for Com-putational Linguistics (ACL), Text SummarizationBranches Out Workshop, pages 74?81.I Malioutov, A.
Park, B. Barzilay, and J.
Glass.
2007.Making sense of sound: Unsupervised topic seg-mentation over acoustic input.
In Proc.
ACL, pages504?511.S.
Maskey and J. Hirschberg.
2005.
Comparing lexial,acoustic/prosodic, discourse and structural featuresfor speech summarization.
In Proceedings of the9th European Conference on Speech Communica-tion and Technology (Eurospeech), pages 621?624.K.
Mckeown and D.R.
Radev.
1995.
Generating sum-maries of multiple news articles.
In Proc.
of SIGIR,pages 72?82.C.
Munteanu, R. Baecker, G Penn, E. Toms, andE.
James.
2006.
Effect of speech recognition ac-curacy rates on the usefulness and usability of we-bcast archives.
In Proceedings of SIGCHI, pages493?502.G.
Murray, S. Renals, and J. Carletta.
2005.Extractive summarization of meeting recordings.In Proceedings of the 9th European Conferenceon Speech Communication and Technology (Eu-rospeech), pages 593?596.A.
Park and J.
Glass.
2006.
Unsupervised word ac-quisition from speech using pattern discovery.
Proc.ICASSP, pages 409?412.A.
Park and J.
Glass.
2008.
Unsupervised pattern dis-covery in speech.
IEEE Trans.
ASLP, 16(1):186?197.G.
Penn and X. Zhu.
2008.
A critical reassessment ofevaluation baselines for speech summarization.
InProc.
of the 46th Association for Computational Lin-guistics, pages 407?478.W.H.
Press, S.A. Teukolsky, W.T.
Vetterling, and B.P.Flannery.
2007.
Numerical recipes: The art of sci-ence computing.D.
Radev and K. McKeown.
1998.
Generating naturallanguage summaries from multiple on-line sources.In Computational Linguistics, pages 469?500.D.
Radev, H. Jing, M. Stys, and D. Tam.
2004.Centroid-based summarization of multiple docu-ments.
Information Processing and Management,40:919?938.T.
Toda, H. Saruwatari, and K. Shikano.
2001.
Voiceconversion algorithm based on gaussian mixturemodel with dynamic frequency warping of straightspectrum.
In Proc.
ICASPP, pages 841?844.S.
Tucker and S. Whittaker.
2008.
Temporal compres-sion of speech: an evaluation.
IEEE Transactionson Audio, Speech and Language Processing, pages790?796.K.
Zechner.
2001.
Automatic Summarization of Spo-ken Dialogues in Unrestricted Domains.
Ph.D. the-sis, Carnegie Mellon University.J.
Zhang, H. Chan, P. Fung, and L Cao.
2007.
Compar-ative study on speech summarization of broadcastnews and lecture speech.
In Proc.
of Interspeech,pages 2781?2784.X.
Zhu and G. Penn.
2006.
Summarization of spon-taneous conversations.
In Proceedings of the 9thInternational Conference on Spoken Language Pro-cessing, pages 1531?1534.557
