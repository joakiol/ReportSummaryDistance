Developing an Evaluation Methodologyfor Spoken Language SystemsMadeleine Bates, Sean Boisen and John MakhoulBBN Systems and Technologies Corporation10 Moulton StreetCambridge, MA 02138AbstractThere has been a long-standing methodology forevaluating work in speech recognition (SR), but untilrecently no community-wide methodology existed for eithernatural language (NL) researchers or speech understanding(SU) researchers for evaluating the systems they developed.Recently considerable progress has been made by anumber of groups involved in the DARPA SpokenLanguage Systems (SLS) program to agree on amethodology for comparative evaluation of SLS systems,and that methodology is being used in practice for the firsttime.This paper gives an overview of the process that wasfollowed in creating a meaningful evaluation mechanism,describes the current mechanism, and presents omedirections for future development.1.
A Brief HistoryThe work reported in this paper began in 1988, whenpeople working on the DARPA Spoken Language Systemsprogram and others working on other aspects of naturallanguage processing met at a workshop on NL evaluation\[1\].
At that meeting, considerable time was devoted toclarifying the issues of black-box evaluation and glass-boxevaluation.Since that meeting, the SLS community formed acommittee on evaluation, chaired by Dave Pallett of NIST.The charge of this committee was to develop amethodologyfor data collection, training data dissemination, and testingfor SLS systems under development (see \[2\] and \[3\]).
Theemphasis of the committee's work has been on automaticevaluation of quefies to an air travel information system.The first community-wide evaluation using the firstversion of methodology developed by this committee tookplace in June, 1990.
It is reported in \[4\].2.
The IssuesWhy are systems for NL understanding, or speechunderstanding, more difficult o evaluate than SR systems?The key difference is that that the output of speechrecognition is easy to specify -- it is a character stringcontaining the words that were spoken as input to thesystem - and it is trivially easy to determine the "fight"answer and to compare it to the output of a particular SRsystem.
Each of these steps,1.
specifying the form that output should take,2.
determining the right output for particular input, and3.
comparing the fight answer to the output of a particularsystem,is very problematic for NL and SU systems.~mU~ Ii iRecognition Ur dingMeaning ReI :~seat~i~I Application \[CodeGeneratorSQL Commands~to User ,.4-I ResponseCGener~t~ApplimtlonSysw.m ~DBMS)Answer 1Figure 1: A Typical Speech Understanding System3.
The GoalThe goal of the work was to produce a well-defined,meaningful evaluation methodology (implemented using anautomatic evaluation system) which will both permitmeaningful comparisons between different systems and alsoallow us to track the improvement i  a single NL systemover time.
The systems are assumed to be front ends to aninteractive application (database inquiry) in a particulardomain (ATIS - an air travel information system).The intent is to evaluate specifically NL understandingcapabilities, not other aspects of a system, such as the userinterface, or the utility (or speed) of performing a particulartask with a system that includes aNL component.1024.
The Evaluat ion F rameworkThe methodology that was developed is very similar instyle to that which has been used for speech recognitionsystems for several years.
It is:1.
Collect a set of data as large as feasible, underconditions as realistic as possible.2.
Reserve some of that data as a test set, and distributethe rest as a training set.3.
Develop answers for the items in the test set, and anautomatic omparison program to compare those "right"answers with the answers produced by various ystems.4.
Send the test set to the sites, where they will beprocessed unseen and without modifications to the system.The answers are then returned and run through the evaluationproc~ure, and the results reported.Figure 2 illustrates the relationship between an SLSsystem and the evaluation system.Evaluating body Developer provides providesI~ SLSkernelII I II II IComparator 1-=1*CAS = CommonAnswer SpecificationFigure 2: The Evaluation Framework4.1 Collecting DataA method of data collection called "Wizard scenarios" wasused to collect raw data (speech and transcribed text).
Thissystem is described in \[5\].
It resulted in the collection of anumber of human-machine dialogues.
This data exhibitssome interesting characteristics.Because much of the data in the database is represented inencoded form (e.g., "L" for "Lunch", and "F" for "FirstClass"), and because the names of the database fields (whichshow up as column headers in the answers shown to theusers) are often abbreviated or cryptic, many of the users'questions centered on finding out the meaning of some codewhich was printed as part of the answer to a previousquestion.
1 This is a characteristic of this particular database,and is not necessarily shared by other databases ordomains.The amount of data shown to the user influences therange of subsequent queries.
For example, when the userasks for a list of flights and the answer includes not just theflight numbers but also the departarture and arrival times andlocations, the meal service, and so on, there is never anyneed for a follow-up question like "When does flight AA123get in?"
or "Is lunch served on that flight?
".The language obtained in Wizard scenarios is verystrongly influenced by the particular task, the domain anddatabase being used, and the amount and form of datareturned to the user.
Therefore, restricting the training andtest sets to data collected in Wizard scenarios in the AIrSdomain means that the language does not exhibit very manyexamples of some phenomena (such as quantification,negation, and complex conjunction) which are known toappear frequently in database interfaces to other domains.4.2 Classifying DataOne of the first things to become clear was that not all ofthe collected ata was suitable as test data, and thus it wasdesirable that the training data be marked to indicate whichqueries one might reasonably expect o find in the test set.The notion emerged of having a number of classes ofdata, each more inclusive than the last, so that we couldbegin with a core (Class A) which was clearly definable andpossible to evaluate automatically, and, as we came tounderstand the evaluation process better, which could beextended to other types of queries (Classes B, C, etc.
).Several possible classification systems were presented anddiscussed in great detail.
The one which has been agreed onat this time, Class A, is defined in Appendix A; Classes B,C, etc.
have not yet been defined.4.3 Agreeing on MeaningHow do you know whether a system has given the fightanswer to a question like "List the mid-day flights fromBoston to Dallas", "Which non-stop flights from Boston toDallas serve meals?
", or "What's the fare on AA 167?
"It is necessary, for cross-site comparisons, toagree on themeaning of terms uch as "mid-day", "meals" (e.g., does that1For example, 23 of the 90 queries in the June 1990 testset were requests for the meaning of an abbreviation orcode.103mean any food at all, or does it include full meals butexclude snacks), and "the fare of a flight" (because in thisdatabase, flights don't have simple fares, but a flighttogether with a class determine the fare).The process of defining precisely what is meant by manywords and phrases is still not complete, but Appendix Blists the current points of agreement.
Without thisagreement, many systems would produce very differentanswers for the same questions, all of them equally fightaccording to the systems' own definitions of the terms, butnot amenable to automatic evaluation.4.4 Developing Canonical AnswersIt is not enough to agree on meaning, it is also necessaryto have a common understanding of what is to be producedas the answer, or part of the answer, to a question.For example, if a user asks "What is the departure time ofthe earliest flight from San Francisco to Atlanta?
", onesystem might reply with a single time and another mightreply with that time plus additional columns containing thecarrier and flight number, a third system might also includethe arrival time and the origin and destination airports.None of these answers could be said to be wrong, althoughone might argue about he advantages and disadvantages ofterseness and verbosity.It was agreed that, for the sake of automatic evaluation, acanonical answer (the minimum "fight" answer) should bedeveloped for each Class A query in the training set, and thatthe canonical answer should be that answer etrieved by acanonical SQL expression.
That is, the fight answer wasdefined by the expression which produces the answer fromthe database, as well as the answer etrieved.
This ensuresA) that it is possible to retrieve the canonical answer viaSQL, B) that even if the answer is empty or otherwiselimited in content, it is possible for system developers tounderstand what was expected by looking at the SQL, and C)the canonical answer contains the least amount ofinformation eeded to determine that the system produced thefight answer.Thus it was agreed that for identifying a "flight" theunique flight id would be used, not the carrier and flightnumber (since there may be several different flights calledAA 123 connecting different cities with different departuretimes andother characteristics).What should be produced for an answer is determined bothby domain-independent li guistic principles \[2\] and domain-specific stipulation (Appendix B).
The language used toexpress the answers is defined in Appendix C.4.5 Developing ComparatorsA final necessary component is, of course, a program tocompare the canonical answers to those produced by varioussystems.
Two programs were constructed to do this, onewritten in Common Lisp by BBN and one written in C byNIST.
Their functionality is substantially similar; anyoneinterested in obtaining the code for these comparators shouldcontact Bill Fisher at NIST or Sean Boisen at BBN.The task of answer comparison is complicatedsubstantially by the fact that the canonical answer isintentionally minimal, but the answer supplied by a systemmay contain extra information.
Some intelligence is neededto determine when two answer match (i.e.
simple identitytests won't work).4.6 Choosing a Test SetFor the first evaluation, a test set of 90 Class A querieswas chosen from dialogues collected by 4 subjects who werenot represented in the training set.
We believe that makesthe test set harder than necessary, since it is clear that thereis not enough training data to illustrate many of thelinguistic forms used in this domain, and there is also astrong indication that new users tend to stick with aparticular way of asking questions.4.7 Presenting ResultsExpressing results can be almost as complicated asobtaining them.
Originally it was thought hat a simple "Xpercent correct" measure would be sufficient, however itbecame clear that not all systems could answer all questions,and that there was a significant difference between giving awrong answer and giving no answer at all, so the resultswere presented as: Number fight, Number wrong, Numbernot answered.
How harshly systems hould be judged forgiving wrong answers was not determined.5.
Strengths of this MethodologyIt forces advance agreement on the meaning of criticalterms and on at least minimal information to be included inthe answer.It is objective, to the extent hat a method for selectingtestable queries can be defined, and to the extent hat theagreements mentioned above can be reached.It requires less human effort (primarily in the creating ofcanonical examples and answers) than non-automatic, moresubjective valuation..
It is thus better suited to large testsets.Flexible comparison means system developers need notdevelop a completely separate system for evaluationpurposes.
With only minor formatting changes,substantially the same system can be used for otherpurposes.It can be easily extended, as discussed in section 7 below.1046.
Weaknesses of this MethodologyIt does not distinguish between merely acceptable answersand very good answers (although the comparators could bemade to take this into account if multiple canonical answerswith associated acceptability levels could be provided).It does not distinguish between some cases, and may thusgive undue credit to a system that "over answers".
Forexample, if the system prints out the carrier, flight number,arrival and departure times and locations, and meal serviceevery time it is asked about a flight, then the answers to"What is the arrival time of AA 123", "What is thedestination of AA 123", "What meal is served on AA 123"and "List flight AA 123" could all produce xactly the sameanswer and be scored correct on all of them, since thecanonical answers would be a subset of the informationprinted (note that it still must correctly distinguish flightAA 123 from other flights).It cannot ell if a system gets the right answer for thewrong reason.
This is an" 11 unavoidable problem with"black-box" evaluation, but it can be mitigated by use oflarger test sets.It does not adequately measure the handling of somephenomena, such as extended dialogues.7.
Suggestions for The FutureOur experience thus far has shown that the methodologyof developing well-defined test set criteria, combined withautomatic evaluation of canonical answers, is a useful one.7.1 Challenge Training SetOne of the strongest needs of the SLS community at thistime is more training data.
Instead of a few hundred trainingqueries, several thousand are needed.
One way of obtainingmore data is simply to continue to run Wizard scenarios tocollect them, but this process is rather slow, and tends toyield numerous examples of very similar queries.We suggest hat a separate training set, called theChallenge Set be created.
To form this set, each of the fivesystem-developing sites would create, using any means theywish, 500 Class A queries, together with canonical SQL andanswers for them.
Each site would be encouraged toincludequeries that show the scope of their system and that create achallenge for other sites to match.
Every site would then berequired to report on the results of running their system onthe challenge set.
This use of "crucial examples" is similarto more traditional linguistic methods.7.2 Beyond C lass  A7.2.1ContextThe existing Class A definition excludes all sentenceswhose interpretation requires context outside the sentenceitself, i.e.
"Which of those flights are non-stop?".
The onlyobstacle to including such sentences i agreement on whatdiscourse phenomenon to allow, what the meanings incontext should be, and when the context should be reset(because sentences have been excluded for other reasons).The existing evaluation framework naturally extends to suchcases, assuming the system can produce answers withoutuser interaction.A proposal has been made \[6\] to standardize outputdisplays in an attempt to reset context for the evaluation ofdiscourse.
This would make it possible to evaluate queriescontaining references that are display-specific, but not manyqueries in the training data are of this type, and we believethat there are simpler ways of evaluating common discoursephenomena.7.2.2AmbiguityA simple extension to the language for expressinganswers would allow more than one answer to be returnedfor a query.
At a minimum, this could be used to giveseveral alternatives: an answer matching any alternativewould then be scored as correct.
For example, the answer tothe query "What is the distance from the San Franciscoairport to downtown" could be either the distance to SanFrancisco or the distances to San Francisco and Oakland(since both of those cities are served by the San Franciscoairport).
A more sophisticated approach would be to assigndifferent weights to these alternatives, so systems obtainingthe preferred reading would score the highest.References1.
Palmer, M., T. Finin and S. Walter, Report on theWorkshop on the Evaluation of Natural LanguageProcessing Systems, unpublished report of a RADCsponsored workshop, 1988.2.
Boisen, Scan, Lance A. Ramshaw, Damaris Ayuso, andMadeleine Bates, A Proposal for SLS Evaluation, inProceedings of the DARPA Speech and Natural LanguageWorkshop, October 1989.3.
Ramshaw, Lance A and Sean Boisen, An SLS AnswerComparator.
SLS Note No.
7, BBN Systems andTechnologies Corporation, Cambridge, MA, May 25, 1990.4.
Pallett, David S., et al DARPA ATIS Test Results June1990, this volume5.
Hemphill, Charles, TI Implementation of CorpusCollection, this volume.6.
Hirschman, Lynette, et al Beyond Class A: A Proposalfor Automatic Evaluation of Discourse, this volume.7.
Pallet, David S., William M. Fisher, Jonathan G. Fiscus,Tools for the Analysis of Benchmark Speech RecognitionTests, Proceedings of ICASSP 1990, p. 97.105Appendix A: The Current:Definit ion of "Class A"Class A queries will be identified by exception.
Class Aqueries will be those that are none of the following:1. context dependent2.
vague, ambiguous, disambiguated only by context, orotherwise failing to yield a single canonical db answer3.
grossly ill-formed4.
other unanswerable queries5.
queries from a noncooperative subject.These exclusionary categories are described below.A.1.
Context dependentThere seem to be two broad subeategories here:a. queries containing explicit reference to apreceding answer or question, such as "What classes ofservice are available on those flights?"b.
queries whose scope is implicitly assumed to belimited by a preceding answer or question, such as "Whichflights go to Dallas?"
in a context hat limits attention tosome particular set of flights to Washington DC.It is noted that some queries in the second subcategorycould, is isolation, also receive a reasonable context-independent interpretation.
For example, in context, "Pleaselist an interpretation of the classes," is likely to mean theclasses displayed in the preceding answer, and thus is contextdependent.
It also could have a reasonable use referring toall classes.
Such queries will be specially marked in theprocess of selecting class A queries.A.2.
Vague, ambiguous, disambiguatedonly by context, or otherwise failing toyield a single canonical database answer.Some of the particular cases noted so far include:a. attachment ambiguities.
These will be excludedONLY if it is not possible for an ordinary person to pick thepreferred reading (without resort o context).b.
"What does X mean?"
These are out, unless Xis an abbreviation code that has a table that expands the codeinto a descriptive word, phrase, or set of attributes.
Thequery is not acceptable, however, if X is a code that hasmore than one possible meaning according to what field itappears in, unless there is disambiguating context WITHINthe query, such as "What does fare code X mean?
"2 As distributed by Robert Moore of SRI on May 10,1990.c.
"Give me information about X."
These queriescould be allowed, if someone will produce a table ofallowable instances of X together with what informationshould be provided.
Pending that happening, these queriesare out.A .3 .
Gross ly  ill-formed queriesAs long as the query is interpretable, only utterances thatappear not to be attempts to speak normal conversationalEnglish will be excluded.
For example, we should excludeattempts to speak some imagined form of "computerese"rather than normal English: "Origin Dallas, destinationBoston, list flights."A.4.
Other unanswerable queriesa.
queries not given a database answer by thewizard.
This may include some queries that pass all ourtests, but if the wizard did not generate a DB query, then wedon't have anything to evaluate on.b.
utterances that cannot be interpreted as queries,or that are incoherent.c.
queries that request information not in thedatabase.d.
queries that refer to the way that information ispresented.e.
"meta queries" about system capabilities orstructure or limits of the database.A.5.
Queries from a noncooperativesubjectUtterances that are clearly designed to try to break thesystem should be excluded: "Given that city A is Oaklandand city B is Fort Worth show me all flights from A to B."A.6.
Additional CommentsMinor syntactic or semantic ill-formedness -- if the queryis interpretable, it will be accepted, unless it is so ill-formedthat it is clear that it is not intended to be normalconversational English.Presupposition failures -- all presuppositions about thenumber of answers (either existence or uniqueness) will beignored.
These are the only types of presupposition failuresnoted to date.
Any other types of presupposition failure thatmake the query truly unanswerable will presumably result inthe wizard being unable to generate a database query, andwill be ruled out on those grounds.Multi-sentence utterances -- These will not automaticallybe ruled out.
The examples cited so far are clearlyinterpretable asexpressing multiple constraints hat "can becombined into a single query.106Appendix B: Current Definitions ofKey Concepts in the ATIS DomainB.1 BasicsA large class of tables in the database have entries thatcan be taken as defining things that can be asked for in aquery.
In the answer, each of these things will be identifiedby giving a value of the primary key of its table.
Thesetables are:Table Name English Term(s) Primary Keyaircraft aircraft, equipment aircraftcodeairline airline airline_codeairport airport airport_codecity city citycodecompound_class service classes fareclassday names of the days day_codefare fare far,codeflight flight flight_codefoocl_service meals meal_codeground_service ground transportation city_code,airportcode,transportcodemonth months monthnumberrestriction restrictions restrict_codestate names of states stat~codetime_zone time zones time_zone_codetransport transport code transport_codeB.2 Special meaningsIn this arena, certain English expressions have specialmeanings, particularly in terms of the database distributed byTI in the spring of 1990.
Here are the ones we have agreedon: (In the following, "A.B" refers to field B of table A.)B.2.1.
Flights.A flight "between X and Y" means "from X to Y".In an expression of the form "flight number N", where Nis a number, N will always be interpreted as referring to theflight number (flight.flight_number).
"Flight code N" willunambiguously refer to flight.flight_code.
"Flight N" willrefer to flight.flight_number if N is in the range 0 <= N <=9999 but to flight.flight_code if N >= 100000.A "one-way" flight is a flight with a fare whose one-waycost is non-empty.Principle: if an attribute "X" of a fare, such as "one-way"or "coach", is used as a modifier of a flight, it will beinterpreted as "a flight with an X fare".B.2.2.
Fare (classes).A "one-way" fare is one with a non-empty one-way cost.In determining what is the "cheapest fare", one-way fareswill be included.A "coach" fare is one whose compound_class.class_type ="COACH".
Similarly, the fare modifiers "first class","business class", and "thrift class" refer to values of thecompound_class.class_type field.A reference to ranking of fares, e.g.
"fares that are Y classor better", will be interpreted as a reference to the rank of theassociated base fare (class_of_service.rank).A "d i scounted  fare"  iscompound_class.discounted = "YES".one whoseAn "excursion fare" is one with a restriction code(fare.restrict_code) that contains the string "AP", "EX", orI t  ~ jT 'L~I I  eA "family fare" is the same thing as an "excursion fare".A "special fare" is one with a non-null restriction code(fare.restrict_code).B.2.3.
Time.The normal answer to otherwise unmodified "when"queries will be a time of day, not a date or a duration.The answer to queries like "On what days does flight Xfly" will be a list of day.day_code fields, not a flight_daysstring.B.2.4.
Units.All units will be the same as those implicit in thedatabase (e.g.
feet for aircraft.wing_span, but miles foraircraft.range_miles, durations in minutes).B.2.5.
Meals.For purposes of determining flights "with meals/mealservice", snacks will count as a meal.
"List the types of meal" should produce one tuple permeal, not a single meal_code string.B.2.6.
"With"-modification clauses.
"Show me all the flights from X to Y with their fares"will require the identification of both flights and their fares(so if there are 2 flights, each with three fares, the answerwill have 6 tuples, each with at least the flight_code andfare_code).
In general, queries asking for information fromtwo or more separate tables in the database will require thelogical union of fields that would identify each table entryseparately.B.2.7.
Itinerary.The "itinerary" of a flight refers to the set of all non-stoplegs of that flight.
When an "itinerary" is asked for, eachleg of the flight will be identified by the origin anddestination cities for that leg, e.g.
(CBOS" "ATL") CATL""DFW")).107B.2.8.
"What kind of".
"What kind of X is Y" queries, where Y is clearly a kindof X, will be interpreted as equivalent to "what does Ymean?
", where Y is a primary key value for the table referredto by X (see 10 below).B.2.9.
Classes of ServiceReferences to classes of service will be taken as referringto the contents of the compound_class table (not theclass of service table).Queries about (unmodified) "class X", e.g.
"What is classX?
", will be interpreted as referring to the set ofcompound_class.fare_class entries for which "X" is thefare_class, not the base_class, e.g.
'(CX"))', not'(("XA")("XB"))'.B.2.10.
Requests for meaning.Requests for the "meaning" of something will only beinterpretable if that thing is a code with a canned efinitionin the database.
Here are the things so defined, with thefields containing their decoding:Table Key Field Decoding Fieldaircraft aircrafucode aircraft_typeairline airline_code airline_nameairport airporucode airporUnamecity city_code city_namecede_description code descriptioncolumn_table heading column_descriptionday daycode dayjaamefoodservice meal_code meal descriptioninterval period begin_time,end_timemonth month_number month_namestate stat~code state_nametime_zone time_zone_code time_zone_nametransport lransporUcode transporUdescfipfionB.2.11.
Stops.A request for a flight's stops will be interpreted as askingfor the final stop in addition to intermediate stops.B.2.12.
Yes/no Questions.Literal yes-or-no questions, that is, queries that in normaldiscourse would be best answered with "yes" or "no" if takenliterally, may be answered by either a boolean value("YES/TRUE/NO/FALSE") or a relation, expressed as atable.
Any non-null relation will be considered equivalent to"YES" or "TRUE" and the null relation will be consideredequivalent to "NO" or "FALSE".B.2.13.
Near.A city and an airport will be considered "near" (or"nearby") each other iff the city is served by the airport, andtwo cities will be considered "near" (or "nearby") each otheriff there is an airport hat serves them both.B.2.14.
American.When it is clear that an airline is being referred to, theterm "American" by itself will be taken as unambiguouslyreferring to American Airlines.B.2.15.
Vague Queries.Vague queries of the form "Give me information aboutX" or "Describe X" or "What is X", where X refers to atable, will be interpreted as equivalent to "Show me X".When X is an attribute or attribute value, such queries willbe interpreted as "What does X mean?
".Appendix C: Current Definition ofCAS AnswersThe following BNF describes the syntax of the CommonAnswer Specification (CAS) for the ATIS domain:answer--> scalar-value I relation I NO ANSWERboolean-value --> yes I true I no I falsenumber-value --> integer I real-numberrelation --> ( tuple* )scalar-value --> boolean-value I number-value I stringtuple --> ( value + )value --> scalar-value INILWe assume as primitives the values integer, real-number,and string.
Integers and reals are not distinguished, and onlynon-exponential real numbers are allowed.
Strings mustalways be enclosed in double quotes (e.e., "DFW"), are case-sensitive, and should be upper-case (since strings in theATIS database are).
The special tokens yes, no, true,false, no_answer, and nil are not case-sensitive.Answer relations must be derived from the existingrelations in the database, either by substituting andcombining relations or by operations like averaging,summation, etc.
NIL as the representation f missing datais allowed as a special case for any value, so a legal answerindicating the costs of ground transportation i Bostonwould be:( ( "L" 5.00 ) CR" nil ) CA" nil ) CR" nil ) )Empty tuples are not allowed (but empty relations are).All the tuples in a relation must have the same number ofvalues, those values must be of the same respective types(boolean, string, or number), and the types in the answermust be the same as the types in the database (i.e., databasevalues like "1335" cannot be converted from strings tonumbers in answer expressions).AcknowledgementThis work was supported by the Defense Advancedprojects Agency and monitored by the Office of NavalResearch under Contract No.
N00014-89-C-0008.108
