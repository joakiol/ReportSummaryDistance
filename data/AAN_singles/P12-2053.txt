Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 270?274,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsInformation-theoretic Multi-view Domain AdaptationPei Yang1,3, Wei Gao2, Qi Tan1, Kam-Fai Wong31South China University of Technology, Guangzhou, China{yangpei,tanqi}@scut.edu.cn2Qatar Computing Research Institute, Qatar Foundation, Doha, Qatarwgao@qf.org.qa3The Chinese University of Hong Kong, Shatin, N.T., Hong Kongkfwong@se.cuhk.edu.hkAbstractWe use multiple views for cross-domain doc-ument classification.
The main idea is tostrengthen the views?
consistency for targetdata with source training data by identify-ing the correlations of domain-specific fea-tures from different domains.
We presentan Information-theoretic Multi-view Adapta-tion Model (IMAM) based on a multi-wayclustering scheme, where word and link clus-ters can draw together seemingly unrelateddomain-specific features from both sides anditeratively boost the consistency between doc-ument clusterings based on word and linkviews.
Experiments show that IMAM signifi-cantly outperforms state-of-the-art baselines.1 IntroductionDomain adaptation has been shown useful to manynatural language processing applications includingdocument classification (Sarinnapakorn and Kubat,2007), sentiment classification (Blitzer et al, 2007),part-of-speech tagging (Jiang and Zhai, 2007) andentity mention detection (Daume?
III and Marcu,2006).Documents can be represented by multiple inde-pendent sets of features such as words and link struc-tures of the documents.
Multi-view learning aimsto improve classifiers by leveraging the redundancyand consistency among these multiple views (Blumand Mitchell, 1998; Ru?ping and Scheffer, 2005; Ab-ney, 2002).
Existing methods were designed fordata from single domain, assuming that either viewalone is sufficient to predict the target class accu-rately.
However, this view-consistency assumptionis largely violated in the setting of domain adapta-tion where training and test data are drawn from dif-ferent distributions.Little research was done for multi-view domainadaptation.
In this work, we present an Information-theoretical Multi-view Adaptation Model (IMAM)based on co-clustering framework (Dhillon et al,2003) that combines the two learning paradigms totransfer class information across domains in multi-ple transformed feature spaces.
IMAM exploits amulti-way-clustering-based classification scheme tosimultaneously cluster documents, words and linksinto their respective clusters.
In particular, the wordand link clusterings can automatically associate thecorrelated features from different domains.
Suchcorrelations bridge the domain gap and enhance theconsistency of views for clustering (i.e., classifying)the target data.
Results show that IMAM signifi-cantly outperforms the state-of-the-art baselines.2 Related WorkThe work closely related to ours was done by Daiet al (2007), where they proposed co-clustering-based classification (CoCC) for adaptation learning.CoCC was extended from information-theoretic co-clustering (Dhillon et al, 2003), where in-domainconstraints were added to word clusters to providethe class structure and partial categorization knowl-edge.
However, CoCC is a single-view algorithm.Although multi-view learning (Blum andMitchell, 1998; Dasgupta et al, 2001; Abney,2002; Sridharan and Kakade, 2008) is commonwithin a single domain, it is not well studied undercross-domain settings.
Chen et al (2011) proposed270CODA for adaptation based on co-training (Blumand Mitchell, 1998), which is however a pseudomulti-view algorithm where original data has onlyone view.
Therefore, it is not suitable for thetrue multi-view case as ours.
Zhang et al (2011)proposed an instance-level multi-view transferalgorithm that integrates classification loss and viewconsistency terms based on large margin framework.However, instance-based approach is generally poorsince new target features lack support from sourcedata (Blitzer et al, 2011).
We focus on feature-levelmulti-view adaptation.3 Our ModelIntuitively, source-specific and target-specific fea-tures can be drawn together by mining theirco-occurrence with domain-independent (common)features, which helps bridge the distribution gap.Meanwhile, the view consistency on target data canbe strengthened if target-specific features are appro-priately bundled with source-specific features.
Ourmodel leverages the complementary cooperation be-tween different views to yield better adaptation per-formance.3.1 RepresentationLet DS be the source training documents and DTbe the unlabeled target documents.
Let C be the setof class labels.
Each source document ds ?
DS islabeled with a unique class label c ?
C. Our goalis to assign each target document dt ?
DT to anappropriate class as accurately as possible.Let W be the vocabulary of the entire documentcollectionD = DS?DT .
LetL be the set of all links(hyperlinks or citations) among documents.
Eachd ?
D can be represented by two views, i.e., a bag-of-words set {w} and a bag-of-links set {l}.Our model explores multi-way clustering that si-multaneously clusters documents, words and links.Let D?, W?
and L?
be the respective clustering of doc-uments, words and links.
The clustering functionsare defined as CD(d) = d?
for document, CW (w) =w?
for word and CL(l) = l?
for link, where d?, w?
and l?represent the corresponding clusters.3.2 ObjectivesWe extend the information-theoretic co-clusteringframework (Dhillon et al, 2003) to incorporate theloss frommultiple views.
Let I(X,Y ) be mutual in-formation (MI) of variables X and Y , our objectiveis to minimize the MI loss of two different views:?
= ?
?
?W + (1?
?)
?
?L (1)where?W = I(DT ,W )?
I(D?T , W? )
+ ?
?
[I(C,W )?
I(C, W?
)]?L = I(DT , L)?
I(D?T , L?)
+ ?
?[I(C,L)?
I(C, L?
)]?W and ?L are the loss terms based on word viewand link view, respectively, traded off by ?.
?
bal-ances the effect of word or link clusters from co-clustering.
When ?
= 1, the function relies on textonly that reduces to CoCC (Dai et al, 2007).For any x ?
x?, we define conditional distributionq(x|y?)
= p(x|x?)p(x?|y?)
under co-clustering (X?, Y?
)based on Dhillon et al (2003).
Therefore, for anyw ?
w?, l ?
l?, d ?
d?
and c ?
C, we can calculatea set of conditional distributions: q(w|d?
), q(d|w?),q(l|d?
), q(d|l?
), q(c|w?
), q(c|l?).Eq.
1 is hard to optimize due to its combinatorialnature.
We transform it to the equivalent form basedon Kullback-Leibler (KL) divergence between twoconditional distributions p(x|y) and q(x|y?
), whereD(p(x|y)||q(x|y?))
=?x p(x|y)logp(x|y)q(x|y?)
.Lemma 1 (Objective functions) Equation 1 canbe turned into the form of alternate minimization:(i) For document clustering, we minimize?
=?dp(d)?D(d, d?)
+ ?C(W?
, L?
),where ?C(W?
, L?)
is a constant1 and?D(d, d?)
=?
?
D(p(w|d)||q(w|d?
))+ (1?
?)
?
D(p(l|d)||q(l|d?)).
(ii) For word and link clustering, we minimize?
= ?
?wp(w)?W (w, w?)+(1??
)?lp(l)?L(l, l?
),where for any feature v (e.g., w or l) in feature setV (e.g., W or L), we have?V (v, v?)
=D(p(d|v)||q(d|v?
))+ ?
?
D(p(c|v)||q(c|v?
)).1We can obtain that ?C(W?
, L?)
=?[?
(I(C,W )?
I(C, W? ))
+ (1?
?)(I(C,L)?
I(C, L?
))],which is constant since word/link clusters keep fixed during thedocument clustering step.271Lemma 12 allows us to alternately reorder eitherdocuments or both words and links by fixing theother in such a way that the MI loss in Eq.
1 de-creases monotonically.4 Consistency of Multiple ViewsIn this section, we present how the consistency ofdocument clustering on target data could be en-hanced among multiple views, which is the key issueof our multi-view adaptation method.According to Lemma 1, minimizing ?D(d, d?)
foreach d can reduce the objective function value itera-tively (t denotes round id):C(t+1)D (d) = argmind?[?
?
D(p(w|d)||q(t)(w|d?))+(1?
?)
?
D(p(l|d)||q(t)(l|d?
))](2)In each iteration, the optimal document cluster-ing function C(t+1)D is to minimize the weighted sumof KL-divergences used in word-view and link-viewdocument clustering functions as shown above.
Theoptimal word-view and link-view clustering func-tions can be denoted as follows:C(t+1)DW (d) = argmind?D(p(w|d)||q(t)(w|d?))
(3)C(t+1)DL (d) = argmind?D(p(l|d)||q(t)(l|d?))
(4)Our central idea is that the document clusteringsC(t+1)DW and C(t+1)DL based on the two views are drawncloser in each iteration due to the word and linkclusterings that bring together seemingly unrelatedsource-specific and target-specific features.
Mean-while, C(t+1)D combines the two views and reallo-cates the documents so that it remains consistentwith the view-based clusterings as much as possi-ble.
The more consistent the views, the better thedocument clustering, and then the better the wordand link clustering, which creates a positive cycle.4.1 Disagreement Rate of ViewsFor any document, a consistency indicator functionwith respect to the two view-based clusterings canbe defined as follows (t is omitted for simplicity):2Due to space limit, the proof of all lemmas will be given ina long version of the paper.Definition 1 (Indicator function) For any d ?
D,?CDW ,CDL (d) ={1, if CDW (d) = CDL(d);0, otherwiseThen we define the disagreement rate between twoview-based clustering functions:Definition 2 (Disagreement rate)?
(CDW , CDL) = 1?
?d?D ?CDW ,CDL (d)|D| (5)Abney (2002) suggests that the disagreement rateof two independent hypotheses upper-bounds the er-ror rate of either hypothesis.
By minimizing the dis-agreement rate on unlabeled data, the error rate ofeach view can be minimized (so does the overall er-ror).
However, Eq.
5 is not continuous nor convex,which is difficult to optimize directly.
By using theoptimization based on Lemma 1, we can show em-pirically that disagreement rate is monotonically de-creased (see Section 5).4.2 View CombinationIn practice, view-based document clusterings inEq.
3 and 4 are not computed explicitly.
Instead,Eq.
2 directly optimizes view combination and pro-duces the document clustering.
Therefore, it is nec-essary to disclose how consistent it could be with theview-based clusterings.Suppose ?
= {FD|FD(d) = d?, d?
?
D?}
isthe set of all document clustering functions.
Forany FD ?
?, we obtain the disagreement rate?
(FD, CDW ?
CDL), where CDW ?
CDL denotes theclustering resulting from the overlap of the view-based clusterings.Lemma 2 CD always minimizes the disagreementrate for any FD ?
?
such that?
(CD, CDW ?
CDL) = minFD???
(FD, CDW ?
CDL)Meanwhile, ?
(CD, CDW ?
CDL) = ?
(CDW , CDL).Lemma 2 suggests that IMAM always finds thedocument clustering with the minimal disagreementrate to the overlap of view-based clusterings, and theminimal value of disagreement rate equals to the dis-agreement rate of the view-based clusterings.272Table 1: View disagreement rate ?
and error rate ?
thatdecrease with iterations and their Pearson?s correlation ?.Round 1 2 3 4 5 ?DA-EC ?
0.194 0.153 0.149 0.144 0.144 0.998?
0.340 0.132 0.111 0.101 0.095DA-NT ?
0.147 0.083 0.071 0.065 0.064 0.996?
0.295 0.100 0.076 0.069 0.064DA-OS ?
0.129 0.064 0.052 0.047 0.041 0.998?
0.252 0.092 0.068 0.060 0.052DA-ML ?
0.166 0.102 0.071 0.065 0.064 0.984?
0.306 0.107 0.076 0.062 0.054EC-NT ?
0.311 0.250 0.228 0.219 0.217 0.988?
0.321 0.137 0.112 0.096 0.0895 Experiments and ResultsData and SetupCora (McCallum et al, 2000) is an online archiveof computer science articles.
The documents in thearchive are categorized into a hierarchical structure.We selected a subset of Cora, which contains 5 topcategories and 10 sub-categories.
We used a similarway as Dai et al (2007) to construct our training andtest sets.
For each set, we chose two top categories,one as positive class and the other as the negative.Different sub-categories were deemed as differentdomains.
The task is defined as top category classifi-cation.
For example, the dataset denoted as DA-ECconsists of source domain: DA 1(+), EC 1(-); andtarget domain: DA 2(+), EC 2(-).The classification error rate ?
is measured as theproportion of misclassified target documents.
In or-der to avoid the infinity values, we applied Laplaciansmoothing when computing the KL-divergence.
Wetuned ?, ?
and the number of word/link clusters bycross-validation on the training data.Results and DiscussionsTable 1 shows the monotonic decrease of view dis-agreement rate ?
and error rate ?
with the iterationsand their Pearson?s correlation ?
is nearly perfectlypositive.
This indicates that IMAM gradually im-proves adaptation by strengthening the view consis-tency.
This is achieved by the reinforcement of wordand link clusterings that draw together target- andsource-specific features that are originally unrelatedbut co-occur with the common features.We compared IMAM with (1) Transductive SVM(TSVM) (Joachims, 1999) using both words andlinks features; (2) Co-Training (Blum and Mitchell,Table 2: Comparison of error rate with baselines.Data TSVM Co-Train CoCC MVTL-LM IMAMDA-EC 0.214 0.230 0.149 0.192 0.138DA-NT 0.114 0.163 0.106 0.108 0.069DA-OS 0.262 0.175 0.075 0.068 0.039DA-ML 0.107 0.171 0.109 0.183 0.047EC-NT 0.177 0.296 0.225 0.261 0.192EC-OS 0.245 0.175 0.137 0.176 0.074EC-ML 0.168 0.206 0.203 0.264 0.173NT-OS 0.396 0.220 0.107 0.288 0.070NT-ML 0.101 0.132 0.054 0.071 0.032OS-ML 0.179 0.128 0.051 0.126 0.021Average 0.196 0.190 0.122 0.174 0.0851998); (3) CoCC (Dai et al, 2007): Co-clustering-based single-view transfer learner (with text viewonly); and (4) MVTL-LM (Zhang et al, 2011):Large-margin-based multi-view transfer learner.Table 2 shows the results.
Co-Training performeda little better than TSVM by boosting the confidenceof classifiers built on the distinct views in a comple-mentary way.
But since Co-Training doesn?t con-sider the distribution gap, it performed clearly worsethan CoCC even though CoCC has only one view.IMAM significantly outperformed CoCC on allthe datasets.
In average, the error rate of IMAMis 30.3% lower than that of CoCC.
This is becauseIMAM effectively leverages distinct and comple-mentary views.
Compared to CoCC, using sourcetraining data to improve the view consistency on tar-get data is the key competency of IMAM.MVTL-LM performed worse than CoCC.
It sug-gests that instance-based approach is not effectivewhen the data of different domains are drawn fromdifferent feature spaces.
Although MVTL-LM regu-lates view consistency, it cannot identify the associ-ations between target- and source-specific featuresthat is the key to the success of adaptation espe-cially when domain gap is large and less common-ality could be found.
In contrast, CoCC and IMAMuses multi-way clustering to find such correlations.6 ConclusionWe presented a novel feature-level multi-view do-main adaptation approach.
The thrust is to incor-porate distinct views of document features into theinformation-theoretic co-clustering framework andstrengthen the consistency of views on clustering(i.e., classifying) target documents.
The improve-ments over the state-of-the-arts are significant.273ReferencesSteven Abney.
2002.
Bootstrapping.
In Proceedings ofthe 40th Annual Meeting on Association for Computa-tional Linguistics, pages 360-367.John Blitzer, Mark Dredze and Fernado Pereira.
2007.Biographies, Bollywood, Boom-boxes and Blenders:Domain Adaptation for Sentiment Classification.
InProceedings of the 45th Annual Meeting of the Associ-ation of Computational Linguistics, pages 440-447.John Blitzer, Sham Kakade and Dean P. Foster.
2011.Domain Adaptation with Coupled Subspaces.
In Pro-ceedings of the 14th International Conference on Arti-ficial Intelligence and Statistics (AISTATS), pages 173-181.Avrim Blum and Tom Mitchell.
1998.
Combining La-beled and Unlabeled Data with Co-Training.
In Pro-ceedings of the 11th Annual Conference on Computa-tional Learning Theory, pages 92-100.Minmin Chen, Killian Q. Weinberger and John Blitzer.2011.
Co-Training for Domain Adaptation.
In Pro-ceedings of NIPS, pages 1-9.Wenyuan Dai, Gui-Rong Xue, Qiang Yang and YongYu.
2007.
Co-clustering Based Classification for Out-of-domain Documents.
In Proceedings of the 13thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 210-219.Sanjoy Dasgupta, Michael L. Littman and DavidMcAllester.
2001.
PAC Generalization Bounds forCo-Training.
In Proceeding of NIPS, pages 375-382.Hal Daume?
III and Daniel Marcu.
2006.
Domain Adap-tation for Statistical Classifiers.
Journal of ArtificialIntelligence Research, 26(2006):101-126.Inderjit S. Dhillon, Subramanyam Mallela and Dharmen-dra S. Modha.
2003.
Information-Theoretic Co-clustering.
In Proceedings of the ninth ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining, pages 210-219.Thorsten Joachims.
1999.
Transductive Inference forText Classification using Support Vector Machines.
InProceedings of Sixteenth International Conference onMachine Learning, pages 200-209.Jing Jiang and Chengxiang Zhai.
2007.
Instance Weight-ing for Domain Adaptation in NLP.
In Proceedings ofthe 45th Annual Meeting of the Association of Compu-tational Linguistics, pages 264-271.Andrew K. McCallum, Kamal Nigam, Jason Rennie andKristie Seymore.
2000.
Automating the Constructionof Internet Portals with Machine Learning.
Informa-tion Retrieval, 3(2):127-163.Stephan Ru?ping and Tobias Scheffer.
2005.
Learningwith Multiple Views.
In Proceedings of ICML Work-shop on Learning with Multiple Views.Kanoksri Sarinnapakorn and Miroslav Kubat.
2007.Combining Sub-classifiers in Text Categorization: ADST-Based Solution and a Case Study.
IEEE Transac-tions Knowledge and Data Engineering, 19(12):1638-1651.Karthik Sridharan and Sham M. Kakade.
2008.
An In-formation Theoretic Framework for Multi-view Learn-ing.
In Proceedings of the 21st Annual Conference onComputational Learning Theory, pages 403-414.Dan Zhang, Jingrui He, Yan Liu, Luo Si and Richard D.Lawrence.
2011.
Multi-view Transfer Learning witha Large Margin Approach.
In Proceedings of the 17thACM SIGKDD International Conference on Knowl-edge Discovery and Data Mining, pages 1208-1216.274
