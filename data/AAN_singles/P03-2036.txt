Comparison between CFG filtering techniques for LTAG and HPSGNaoki Yoshinaga??
University of Tokyo7-3-1 Hongo, Bunkyo-ku,Tokyo, 113-0033, Japanyoshinag@is.s.u-tokyo.ac.jpKentaro Torisawa??
Japan Advanced Instituteof Science and Technology1-1 Asahidai, Tatsunokuchi,Ishikawa, 923-1292, Japantorisawa@jaist.ac.jpJun?ichi Tsujii???
CREST, JST (Japan Scienceand Technology Corporation)Hon-cho 4-1-8, Kawaguchi-shi,Saitama, 332-0012, Japantsujii@is.s.u-tokyo.ac.jpAbstractAn empirical comparison of CFG filteringtechniques for LTAG and HPSG is pre-sented.
We demonstrate that an approx-imation of HPSG produces a more effec-tive CFG filter than that of LTAG.
We alsoinvestigate the reason for that difference.1 IntroductionVarious parsing techniques have been developedfor lexicalized grammars such as LexicalizedTree Adjoining Grammar (LTAG) (Schabes et al,1988), and Head-Driven Phrase Structure Gram-mar (HPSG) (Pollard and Sag, 1994).
Along withthe independent development of parsing techniquesfor individual grammar formalisms, some of themhave been adapted to other formalisms (Schabes etal., 1988; van Noord, 1994; Yoshida et al, 1999;Torisawa et al, 2000).
However, these realiza-tions sometimes exhibit quite different performancein each grammar formalism (Yoshida et al, 1999;Yoshinaga et al, 2001).
If we could identify an al-gorithmic difference that causes performance differ-ence, it would reveal advantages and disadvantagesof the different realizations.
This should also allowus to integrate the advantages of the realizations intoone generic parsing technique, which yields the fur-ther advancement of the whole parsing community.In this paper, we compare CFG filtering tech-niques for LTAG (Harbusch, 1990; Poller andBecker, 1998) and HPSG (Torisawa et al, 2000;Kiefer and Krieger, 2000), following an approach toparsing comparison among different grammar for-malisms (Yoshinaga et al, 2001).
The key ideaof the approach is to use strongly equivalent gram-mars, which generate equivalent parse results for thesame input, obtained by a grammar conversion asdemonstrated by Yoshinaga and Miyao (2001).
Theparsers with CFG filtering predict possible parsetrees by a CFG approximated from a given grammar.Comparison of those parsers are interesting becauseeffective CFG filters allow us to bring the empiricaltime complexity of the parsers close to that of CFGparsing.
Investigating the difference between theways of context-free (CF) approximation of LTAGand HPSG will thereby enlighten a way of furtheroptimization for both techniques.We performed a comparison between the exist-ing CFG filtering techniques for LTAG (Poller andBecker, 1998) and HPSG (Torisawa et al, 2000),using strongly equivalent grammars obtained byconverting LTAGs extracted from the Penn Tree-bank (Marcus et al, 1993) into HPSG-style.
Wecompared the parsers with respect to the size of theapproximated CFG and its effectiveness as a filter.2 BackgroundIn this section, we introduce a grammar conver-sion (Yoshinaga and Miyao, 2001) and CFG filter-ing (Harbusch, 1990; Poller and Becker, 1998; Tori-sawa et al, 2000; Kiefer and Krieger, 2000).2.1 Grammar conversionThe grammar conversion consists of a conversionof LTAG elementary trees to HPSG lexical entriesand an emulation of substitution and adjunction bySNP VPV NPSNP VPV S5.15.
?5.25.2.1 5.2.29.19.?9.29.2.1 9.2.2Tree 5: Tree 9: SCFG rulesNP VPVP V NPVP V S5.?
5.1 5.29.?
9.1 9.25.2 5.2.1 5.2.29.2 9.2.1 9.2.2Figure 1: Extraction of CFG from LTAGpre-determined grammar rules.
An LTAG elemen-tary tree is first converted into canonical elementarytrees which have only one anchor and whose sub-trees of depth n (?1) contain at least one anchor.
Acanonical elementary tree is then converted into anHPSG lexical entry by regarding the leaf nodes asarguments and by storing them in a stack.We can perform a comparison between LTAG andHPSG parsers using strongly equivalent grammarsobtained by the above conversion.
This is becausestrongly equivalent grammars can be a substitute forthe same grammar in different grammar formalisms.2.2 CFG filtering techniquesAn initial offline step of CFG filtering is performedto approximate a given grammar with a CFG.
Theobtained CFG is used as an efficient device to com-pute the necessary conditions for parse trees.The CFG filtering generally consists of two steps.In phase 1, the parser first predicts possible parsetrees using the approximated CFG, and then filtersout irrelevant edges by a top-down traversal startingfrom roots of successful context-free derivations.
Inphase 2, it then eliminates invalid parse trees by us-ing constraints in the given grammar.
We call theremaining edges that are used for the phase 2 pars-ing essential edges.The parsers with CFG filtering used in our ex-periments follow the above parsing strategy, but aredifferent in the way the CF approximation and theelimination of impossible parse trees in phase 2 areperformed.
In the following sections, we briefly de-scribe the CF approximation and the elimination ofimpossible parse trees in each realization.2.2.1 CF approximation of LTAGIn CFG filtering techniques for LTAG (Harbusch,1990; Poller and Becker, 1998), every branching ofelementary trees in a given grammar is extracted asa CFG rule as shown in Figure 1.Grammar rulelexicalSYNSEM  ?signSYNSEM  ?signSYNSEM  ?phrasalSYNSEM  ?Grammar rulephrasalSYNSEM  ?signSYNSEM  ?signSYNSEM  ?phrasalSYNSEM  ?phrasalSYNSEM  ?ABCXYB X AC Y BsignSYNSEM  ?signSYNSEM  ?CFG rulesFigure 2: Extraction of CFG from HPSGBecause the obtained CFG can reflect only localconstraints given in each local structure of the el-ementary trees, it generates invalid parse trees thatconnect local trees in different elementary trees.
Inorder to eliminate such parse trees, a link betweenbranchings is preserved as a node number whichrecords a unique node address (a subscript attachedto each node in Figure 1).
We can eliminate theseparse trees by traversing essential edges in a bottom-up manner and recursively propagating ok-flag froma node number x to a node number y when a connec-tion between x and y is allowed in the LTAG gram-mar.
We call this propagation ok-prop.2.2.2 CF approximation of HPSGIn CFG filtering techniques for HPSG (Torisawaet al, 2000; Kiefer and Krieger, 2000), the extrac-tion process of a CFG from a given HPSG gram-mar starts by recursively instantiating daughters of agrammar rule with lexical entries and generated fea-ture structures until new feature structures are notgenerated as shown in Figure 2.
We must imposerestrictions on values of some features (i.e., ignor-ing them) and/or the number of rule applications inorder to guarantee the termination of the rule appli-cation.
A CFG is obtained by regarding each initialand generated feature structures as nonterminals andtransition relations between them as CFG rules.Although the obtained CFG can reflect local andglobal constraints given in the whole structure oflexical entries, it generates invalid parse trees be-cause they do not reflect upon constraints given bythe values of features that are ignored in phase 1.These parse trees are eliminated in phase 2 by apply-ing a grammar rule that corresponds to the appliedCFG rule.
We call this rule application rule-app.Table 1: The size of extracted LTAGs (tree tem-plates) and approximated CFGs (above: the numberof nonterminals; below: the number of rules)Grammar G2 G2-4 G2-6 G2-8 G2-10 G2-21LTAG 1,488 2,412 3,139 3,536 3,999 6,085CFGPB 65 66 66 66 67 67716 954 1,090 1,158 1,229 1,552CFGTNT 1,989 3,118 4,009 4,468 5,034 7,45418,323 35,541 50,115 58,356 68,239 118,464Table 2: Parsing performance (sec.)
with thestrongly equivalent grammars for Section 2 of WSJParser G2 G2-4 G2-6 G2-8 G2-10 G2-21PB 1.4 9.1 17.4 24.0 34.2 124.3TNT 0.044 0.097 0.144 0.182 0.224 0.5423 Comparison with CFG filteringIn this section, we compare a pair of CFG filter-ing techniques for LTAG (Poller and Becker, 1998)and HPSG (Torisawa et al, 2000) described in Sec-tion 2.2.1 and 2.2.2.
We hereafter refer to PB andTNT for the C++ implementations of the former anda valiant1 of the latter, respectively.2We first acquired LTAGs by a method pro-posed in Miyao et al (2003) from Sections 2-21 ofthe Wall Street Journal (WSJ) in the Penn Tree-bank (Marcus et al, 1993) and its subsets.3 We thenconverted them into strongly equivalent HPSG-stylegrammars using the grammar conversion describedin Section 2.1.
Table 1 shows the size of CFG ap-proximated from the strongly equivalent grammars.Gx, CFGPB, and CFGTNT henceforth refer to theLTAG extracted from Section x of WSJ and CFGsapproximated from Gx by PB and TNT, respectively.The size of CFGTNT is much larger than that ofCFGPB.
By investigating parsing performance usingthese CFGs, we show that the larger size of CFGTNTresulted in better parsing performance.Table 2 shows the parse time with 254 sentencesof length n (?10) from Section 2 of WSJ (the av-erage length is 6.72 words).4 This result shows notonly that TNT achieved a drastic speed-up against1All daughters of rules are instantiated in the approximation.2In phase 1, PB performs Earley (Earley, 1970) parsingwhile TNT performs CKY (Younger, 1967) parsing.3The elementary trees in the LTAGs are binarized.4We used a subset of the training corpus to avoid the com-plication of using default lexical entries for unknown words.Table 3: The numbers of essential edges with thestrongly equivalent grammars for Section 02 of WSJParser G2 G2-4 G2-6 G2-8 G2-10 G2-21PB 791 1,435 1,924 2,192 2,566 3,976TNT 63 121 174 218 265 536Table 4: The success rate (%) of phase 2 operationsOperations G2 G2-4 G2-6 G2-8 G2-10 G2-21ok-prop (PB) 38.5 34.3 33.1 32.3 31.7 31.0rule-app (TNT) 100 100 100 100 100 100PB, but also that performance difference betweenthem increases with the larger size of the grammars.In order to estimate the degree of CF approxima-tion, we measured the number of essential (inactive)edges of phase 1.
Table 3 shows the number of theessential edges.
The number of essential edges pro-duced by PB is much larger than that produced byTNT .
We then investigated the effect on phase 2as caused by the different number of the essentialedges.
Table 4 shows the success rate of ok-propand rule-app.
The success rate of rule-app is 100%,5whereas that of ok-prop is quite low.6 These resultsindicate that CFGTNT is superior to CFGPB with re-spect to the degree of the CF approximation.We can explain the reason for this difference byinvestigating how TNT approximates HPSG-stylegrammars converted from LTAGs.
As describedin Section 2.1, the grammar conversion preservesthe whole structure of each elementary tree (pre-cisely, a canonical elementary tree) in a stack, andgrammar rules manipulate a head element of thestack.
A generated feature structure in the approxi-mation process thus corresponds to the whole unpro-cessed portion of a canonical elementary tree.
Thisimplies that successful context-free derivations ob-tained by CFGTNT basically involve elementary treesin which all substitution and adjunction have suc-ceeded.
However, CFGPB (also a CFG producedby the other work (Harbusch, 1990)) cannot avoidgenerating invalid parse trees that connect two lo-5This means that the extracted LTAGs should be compatiblewith CFG and were completely converted to CFGs by TNT .6Similar results were obtained in preliminary experimentsusing the XTAG English grammar (The XTAG Research Group,2001) without features (parse time (sec.
)/success rate (%) forPB and TNT were 15.3/30.6 and 0.606/71.2 with the same sen-tences), though space limitations preclude complete results.cal structures where adjunction takes place betweenthem.
We measured with G2-21 the proportion of thenumber of ok-prop between two node numbers ofnodes that take adjunction and its success rate.
Itoccupied 87% of the total number of ok-prop andits success rate was only 22%.
These results sug-gest that the global contexts in a given grammar isessential to obtain an effective CFG filter.It should be noted that the above investigation alsotells us another way of CF approximation of LTAG.We first define a unique way of tree traversal such ashead-corner traversal (van Noord, 1994) on whichwe can perform a sequential application of substitu-tion and adjunction.
We then recursively apply sub-stitution and adjunction on that traversal to an ele-mentary tree and a generated tree structure.
Becausethe processed portions of generated tree structuresare no longer used later, we regard the unprocessedportions of the tree structures as nonterminals ofCFG.
We can thereby construct another CFG filter-ing for LTAG by combining this CFG filter with anexisting LTAG parsing algorithm (van Noord, 1994).4 Conclusion and future directionWe presented an empirical comparison of LTAG andHPSG parsers with CFG filtering.
We compared theparsers with strongly equivalent grammars obtainedby converting LTAGs extracted from the Penn Tree-bank into HPSG-style.
Experimental results showedthat the existing CF approximation of HPSG (Tori-sawa et al, 2000) produced a more effective filterthan that of LTAG (Poller and Becker, 1998).
By in-vestigating the different ways of CF approximation,we concluded that the global constraints in a givengrammar is essential to obtain an effective filter.We are going to integrate the advantage of the CFapproximation of HPSG into that of LTAG in orderto establish another CFG filtering for LTAG.
We willalso conduct experiments on trade-offs between thedegree of CF approximation and the size of approx-imated CFGs as in Maxwell III and Kaplan (1993).AcknowledgmentWe thank Yousuke Sakao for his help in profilingTNT parser and anonymous reviewers for their help-ful comments.
This work was partially supported byJSPS Research Fellowships for Young Scientists.ReferencesJ.
Earley.
1970.
An efficient context-free parsing algo-rithm.
Communications of the ACM, 6(8):451?455.K.
Harbusch.
1990.
An efficient parsing algorithm forTree Adjoining Grammars.
In Proc.
of ACL, pages284?291.B.
Kiefer and H.-U.
Krieger.
2000.
A Context-Free ap-proximation of Head-Driven Phrase Structure Gram-mar.
In Proc.
of IWPT, pages 135?146.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a large annotated corpus of En-glish: the Penn Treebank.
Computational Linguistics,19(2):313?330.J.
T. Maxwell III and R. M. Kaplan.
1993.
The interfacebetween phrasal and functional constraints.
Computa-tional Linguistics, 19(4):571?590.Y.
Miyao, T. Ninomiya, and J. Tsujii.
2003.
Lexicalizedgrammar acquisition.
In Proc.
of EACL companionvolume, pages 127?130.C.
Pollard and I.
A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press.P.
Poller and T. Becker.
1998.
Two-step TAG parsingrevisited.
In Proc.
of TAG+4, pages 143?146.Y.
Schabes, A.
Abeille?, and A. K. Joshi.
1988.
Pars-ing strategies with ?lexicalized?
grammars: Applica-tion to Tree Adjoining Grammars.
In Proc.
of COL-ING, pages 578?583.The XTAG Research Group.
2001.
A Lexicalized TreeAdjoining Grammar for English.
Technical ReportIRCS-01-03, IRCS, University of Pennsylvania.K.
Torisawa, K. Nishida, Y. Miyao, and J. Tsujii.
2000.An HPSG parser with CFG filtering.
Natural Lan-guage Engineering, 6(1):63?80.G.
van Noord.
1994.
Head corner parsing for TAG.Computational Intelligence, 10(4):525?534.M.
Yoshida, T. Ninomiya, K. Torisawa, T. Makino, andJ.
Tsujii.
1999.
Efficient FB-LTAG parser and its par-allelization.
In Proc.
of PACLING, pages 90?103.N.
Yoshinaga and Y. Miyao.
2001.
Grammar conver-sion from LTAG to HPSG.
In Proc.
of ESSLLI StudentSession, pages 309?324.N.
Yoshinaga, Y. Miyao, K. Torisawa, and J. Tsujii.2001.
Efficient LTAG parsing using HPSG parsers.
InProc.
of PACLING, pages 342?351.D.
H. Younger.
1967.
Recognition and parsing ofcontext-free languages in time n3.
Information andControl, 2(10):189?208, February.
