Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 636?644,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsVerbose, Laconic or Just Right: A Simple Computational Model ofContent Appropriateness under Length ConstraintsAnnie Louis?School of InfomaticsUniversity of EdinburghEdinburgh EH8 9ABalouis@inf.ed.ac.ukAni NenkovaComputer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19103nenkova@seas.upenn.eduAbstractLength constraints impose implicit re-quirements on the type of content thatcan be included in a text.
Here we pro-pose the first model to computationally as-sess if a text deviates from these require-ments.
Specifically, our model predictsthe appropriate length for texts based oncontent types present in a snippet of con-stant length.
We consider a range of fea-tures to approximate content type, includ-ing syntactic phrasing, constituent com-pression probability, presence of namedentities, sentence specificity and inter-sentence continuity.
Weights for these fea-tures are learned using a corpus of sum-maries written by experts and on highquality journalistic writing.
During testtime, the difference between actual andpredicted length allows us to quantify textverbosity.
We use data from manual eval-uation of summarization systems to as-sess the verbosity scores produced by ourmodel.
We show that the automatic ver-bosity scores are significantly negativelycorrelated with manual content qualityscores given to the summaries.1 IntroductionIn dialog, the appropriate length of a speaker turnand the amount of detail in it are hugely influ-enced by the pragmatic context.
For example whatconstitutes an appropriate answer to the question?How was your vacation??
would be very differentwhen the question is asked as two acquaintancespass each other in the corridor or right after twofriends have ordered dinner at a restaurant.
Simi-larly in writing, content is tailored to explicitly de-fined or implicitly inferred constraints on the ap-?Work done while at University of Pennsylvania.50 word summary:The De Beers cartel has kept the diamond market stableby matching supply to demand.
African nations haverecently demanded better terms from the cartel.
Afterthe Soviet breakup, De Beers contracted for diamondswith the Yukutian Republic.
The US remains the largestdiamond market, followed by Japan.100 word summary:The De Beers cartel, controlled by the Oppenheimerfamily controls 80% of the uncut diamond marketthrough its Central Selling Organization.
The cartelhas kept the diamond market stable by maintaining abuffer pool of diamonds for matching supply to demand.De Beers opened a new mine in 1992 and extended thelife of two others through underground mining.Innovations have included automated processing andbussing workers in daily from their homes.
Africannations have recently demanded better terms.
Afterthe Soviet breakup, De Beers contracted for diamondswith the Yukutian Republic.
The US remains the largestdiamond market, followed by Japan.Table 1: 50 and 100 word summaries written bythe same person for the same set of documentspropriate length of text.
Many academics have ex-perienced the frustration of needing to adjust theirwriting when they need to write a short abstract oftwo hundred words or an answer to reviewer in nomore than five hundred words.For a specific application-related example con-sider the texts in Table 1.
These are summaries ofa set of news articles discussing the De Beers di-amond cartel, written by the same person.1Thefirst text is written with the instruction to producea summary of about 50 words while the latter isin response to a request for a 100 word summary.Obviously the longer summary contains more de-tails.
It doesn?t however simply extend the shortersummary with more sentences; additional details1These summaries come from the Document Understand-ing Conference dataset (year 2001).636are interspersed with the original shorter summary.The performance of a range of human-machineapplications can be enhanced if they had the abil-ity to predict the appropriate length of a systemcontribution and the type of content appropriatefor that length.
Such applications include docu-ment generation (O?Donnell, 1997), soccer com-mentator (Chen and Mooney, 2008) and questionanswering with different compression rates for dif-ferent types of questions (Kaisser et al., 2008).Predicting the type of content appropriate for thegiven length alone would be highly desirable, forexample in automatic essay grading, summariza-tion and even in information retrieval, in whichverbose writing is particularly undesirable.
In thisrespect, our work supplements recent computa-tional methods to predict varied aspects of writingquality, such as popular writing style and phras-ing in novels (Ganjigunte Ashok et al., 2013), sci-ence journalism (Louis and Nenkova, 2013), andsocial media content (Danescu-Niculescu-Mizil etal., 2012; Lakkaraju et al., 2013).Our work is the first to explore text verbosity.We introduce a simple application-oriented defi-nition of verbosity and a model to automaticallypredict verbosity scores.
We start with a briefoverview of our approach in the next section.2 Text length and contentappropriatenessIn this first model of verbosity, we do not carryout an elaborate annotation experiment to createlabels for verbosity.
There are two main reasonsfor this choice: a) People find it hard to distinguishbetween individual aspects of quality and oftenthe ratings for different aspects are highly corre-lated (Conroy and Dang, 2008; Pitler et al., 2010)b) Moreover, for verbosity in particular, the mostappropriate data for annotation would be conciseand verbose versions of the same text (possibly ofsimilar lengths).
It is more likely that people candistinguish between verbosity of these controlledpairs compared to ratings on an individual arti-cle.
Such writing samples are not easily available.So we have avoided the uncertainties in annotationin this first work by adopting a simpler approachbased on three key ideas.
(i) We define a concise article of length l as ?anarticle that has the appropriate types of content ex-pected in an article of length l?.
Note that lengthis not equal to verbosity in our model.
Our defi-nition allows for articles of different lengths to beconsidered concise.
Verbosity depends on the ap-propriateness of content for the article length.
(ii) We model this appropriateness of contentfor the given length restriction via a set of easilycomputable features that serve as proxies for (a)type of content and level of detail (syntactic fea-tures and sentence specificity) (b) sentence com-plexity (simple readability-related features), (c)secondary details (syntactic structures with highcompression probability) and (d) structure (dis-course relations and inter-sentence continuity).
(iii) Forgoing any explicit annotation, we sim-ply train the model on professionally written textin which we assume content is appropriately tai-lored to the length requirements.
We train a re-gression model on the well-written texts to predictthe length of an article based on a single snippet offixed (short) length from the article.
For a new testarticle, we can obtain a predicted length from thismodel (length supposing the article is written con-cisely) based on a short snippet.
We use the mis-match between the predicted and actual text lengthof the article to determine if it is verbose.We believe that this definition of verbosity hasnatural uses in applications such as summariza-tion.
For example, current systems do not distin-guish the task of summary creation for differenttarget lengths.
They simply try to maximize esti-mated sentence importance and to minimize repet-itive information.
They pay no attention to the factthat the same type of sentences are unlikely to bean optimal selection for both a 50 word and a 400word summary.We now briefly present the formal definition ofthe problem of content appropriateness for a spec-ified text length.
Let T = (t1, t2, ...tn) be a collec-tion of concisely-written texts and let l(ti) denotethe length of text ti.
The learning task is to obtaina function based on the content type properties oftiwhich helps to predict l(ti).
More specifically,we are given a snippet from ti, called sti, of a con-stant length k where k is a parameter of our modeland k < mintjl(tj).
The mapping f is learnedbased on the constant length snippet only and theaim is to predict the original text length.f(sti)?
?l(ti)In our work we choose to work with topical seg-ments from documents rather than the completedocuments themselves.637Once the model is trained, we identify the ver-bosity for a test article as follows: Let us considera new topic segment txduring test time.
Let thelength of the segment be l. We obtain a snippetstxof size k from tx.
Now assume that our modelpredicts f(stx) =?l.Case 1:?l ' l, the content type in txmatchesthe content types generally present in articles oflength l. We consider such articles as concise.Case 2:?l  l, the type of content included intxis really suitable for longer and detailed topicsegments.
Thus txis likely conveying too muchdetail given its length i.e.
it is verbose.Case 3:?l  l, the content in txis of thetype that a skillful writer would include in a muchshorter and less detail-oriented text.
Thus txislikely lacking appropriate details (laconic).We compute the following scores to quantifyverbosity:Predicted length.
is the model prediction?l.Verbosity degree.
This score is the differencebetween the predicted length and the actual lengthof the text,?l ?
l. Positive values of the score indi-cate the degree of verbosity, negative values indi-cate that the text is laconic.Deviation score.
Since both being verbose andbeing laconic is potentially problematic for text,we define a score which does not differentiate thetype of mismatch.
This score is given by the abso-lute magnitude |?l ?
l|.The next section describes the features used forindicating the content type of a snippet.
In Section4, we test the features on a four-way classificationtask to predict the length of a human-written sum-mary based on a snippet of the summary.
In Sec-tion 5, we extend our model to a regression set-ting by learning feature weights on news articles ofvaried lengths from the New York Times (NYT),which we consider to be a sample in which contentis chosen appropriately for each article length.
Fi-nally in Section 6 we evaluate the model trainedon NYT articles on machine-produced summariesand confirm that summaries scored with higherverbosity by our model also receive poor contentquality scores during manual evaluation.3 Features mapping content type toappropriate lengthWe propose a diverse set of 87 features for charac-terizing content type.
These features are computedover the constant length snippet sampled from anarticle.
All the syntax based features are com-puted from the constituency trees produced fromthe Stanford Parser (Klein and Manning, 2003).Length of units (10 features).This set of features captures basic word andsentence length, and redundancy properties of thesnippet.
It includes number of sentences, averagesentence length in words, average word length incharacters, and type to token ratio.
We also in-clude the counts of noun phrases, verb phrasesand prepositional phrases and the average lengthin words of these three phrase types.Syntactic realization (30 features).We compute the grammatical productions in aset of around 47,000 sentences taken from theAQUAINT corpus (Graff, 2002) We select themost frequent 15 productions in this set that in-volve a description of entities, i.e the LHS (left-hand side) of the production is a noun phrase.
Thecount of each of these productions is added as afeature allowing us to track what type of informa-tion about the entities is conveyed in the snippet.We also add features for the most frequent 15 pro-ductions whose LHS is not a noun phrase.Discourse relations (5 features).These features are based on the hypothesis thatdifferent discourse relations would vary in theirappropriateness for articles of different lengths.For example causal information may be includedonly in more detailed texts.We use a tool (Pitler and Nenkova, 2009) toidentify all explicit discourse connectives in oursnippets, along with the general semantic classof the connective (temporal, comparison, contin-gency and expansion).
We use the number of dis-course connectives of each of the four types as fea-tures, as well as the total number of connectives.Continuity (6 features).These features capture the degree to which ad-jacent sentences in the snippet are related and con-tinue the topic.
The amount of continuity forsubtopics is likely to vary for long and short texts.We add the number of pronouns and determin-ers as two features.
Another feature is the averageword overlap value between adjacent sentences.For computing the overlap measure, we representevery sentence as a vector where each dimensionrepresents a word.
The number of times the wordappears in the sentence is the value for that di-mension.
Cosine similarity is computed between638the vectors of adjacent sentences and the averagevalue of the similarity across all pairs of adjacentsentences is the feature value.We also run the Stanford Coreference tool(Raghunathan et al., 2010) to identity pronoun andentity coreference links within the snippet.
Thenumber of total coreference links, and the numberof intra- and inter-sentence links are added as threeseparate features.Amount of detail (7 features).To indicate descriptive words, we compute thenumber of adjectives and adverbs (two features).We also include the total number of named enti-ties (NEs), average length of NEs in words andthe number of sentences that do not have any NEs.The named entities were identified using the Stan-ford NER recognition tool (Finkel et al., 2005).We also use the predictions of a classifiertrained to identify general versus specific sen-tences.
We use a data set of general and spe-cific sentences and features described in Louis andNenkova (2011) to implement a sentence speci-ficity model.
The classifier produces a binary pre-diction and also a graded score for specificity.
Weadd two features?the percentage of specific sen-tences and the average specificity score of words.Compression likelihood (29 features).These features use an external source of infor-mation about content importance.
Specifically, weuse data commonly employed to develop statisti-cal models for sentence compression (Knight andMarcu, 2002; McDonald, 2006; Galley and McK-eown, 2007).
It consists of pairs of sentencesin an original text and a professional summaryof that text.
In every pair, one of the sentences(source) appeared in the original text and the otheris a shorter version with the superfluous detailsdeleted.
Both sentences were produced by people.We use the dataset created by Galley and McKe-own (2007).
The sentences are taken from the ZiffDavis Corpus which contains articles about tech-nology products.
This data also contains align-ment between the constituency parse nodes of thesource and summary sentence pair.
Through thealignment it is possible to track nodes that wherepreserved during compression.On this data, we identify for every productionin the source sentence whether it undergoes dele-tion in the compressed sentence.
A production(LHS ?
RHS) is said to undergo deletion wheneither the LHS node or any of the nodes in theRHS do not appear in the compressed sentence.Only productions which involve non-terminals inthe RHS are used for this analysis as lexical itemscould be rather corpus-specific.
The proportionof times a production undergoes deletion is calledthe deletion probability.
We also incorporate fre-quency of the production with the deletion proba-bility to obtain a representative set of 25 produc-tions which are frequently deleted and also occurcommonly.
This deletion score is computed as:deletion probability * log(frequency of productionin source sentences)Parentheticals appear in the list as would beexpected and also productions involving con-junctions, prepositional phrases and subordinateclauses.
We expect that such productions will in-dicate the presence of details that are only appro-priate for longer texts.To compute the compression-related featuresfor a snippet, we first obtain the set of all pro-ductions in the sentences from the snippet.
Weadd features that indicate the number of times eachof the top 25 ?most deleted?
productions was usedin the snippet.
We also use the sum, average andproduct of deletion probabilities for set of snippetproductions as features.
The product feature givesthe likelihood of the text being deleted.
We alsoadd the perplexity value based on this likelihood,P?1/nwhere P is the likelihood and n is the num-ber of productions from the snippet for which wehave deletion information in our data.2For training a model, we need texts which wecan assume are written in a concise manner.
Weuse two sources of data?summaries written bypeople and high quality news articles.4 A classification model on expertsummariesHere we use a collection of news summaries writ-ten by expert analysts for four different lengthsand build a classification model to predict givena snippet what is the length of the summary fromwhich the snippet was taken.
This task only differ-entiates four lengths but is a useful first approachfor testing our assumptions and features.4.1 DataWe use human written summaries from the Doc-ument Understanding Conference (DUC3) evalua-2Some productions may not have appeared in the ZiffDavis Corpus.3http://duc.nist.gov639tion workshops conducted in 2001 and 2002.
Aninput given for summarization contains 10 to 15documents on a topic.
The person had to create50, 100, 200 and 400 word summaries for each ofthe inputs.
These summary writers are retired in-formation analysts and we can assume that theirsummaries are of high quality and concise nature.Further, the four different length summaries for aninput are produced by the same person.4There-fore differences in length are not confounded bydifferences in writing style of different people.The 2001 dataset has 90 summaries for each ofthe four lengths.
In 2002, there are 116 summariesfor each length.
All of the summaries are abstracts,i.e.
people wrote the summary in their own words,with the exception of one set.
In 2002, abstractswere only created for 50, 100 and 200 lengths.However, extracts created by people are availablefor 400 words.
In extracts, the summary writeris only allowed to choose complete sentences (noedits can be done), however, the sentences can beordered in the summary and people tend to createcoherent extractive summaries as well.
Since it isdesirable to have data for another length, we alsoinclude the 400-word extracts from the 2002 data.4.2 Snippet selectionWe choose 50 words as the snippet length forour experiment since the length of the shortestsummaries is 50.
We experiment with multipleways to select a snippet: the first 50 words of thesummary (START), the last 50 words (END) and50 words starting at a randomly chosen sentence(RANDOM).
However, we do not truncate any sen-tence in the middle to meet the constraint for 50words.
We allow a leeway of 20 words so thatsnippets can range from 30 to 70 words.
When asnippet could not be created within this word limit(eg.
the summary has one sentence which is longerthan 70 words), we ignore the example.4.3 Classification resultsThe task is to predict the length of the summaryfrom which the fixed length snippet was taken, i.e.4-way classification?50, 100, 200 or a 400 wordsummary.
We trained an SVM classifier with a ra-dial basis kernel on the 2001 data.
The regulariza-tion and kernel parameters were tuned using 10-fold cross validation on the training set.
The accu-racies of classification on the 2002 data are shown4Different inputs however may be summarized by differ-ent assessors.snippet position accuracySTART 38.4RANDOM 34.4END 39.3Table 2: Length prediction results on DUC sum-mariesin Table 2.
Since there are four equal classes, therandom baseline performance is 25%.The START and END position snippets gave thebest accuracies, 38% and 39% which are 13-14%absolute improvement above the baseline.
At thesame time, there is much scope for improvement.The confusion matrices showed that 50 and 400word lengths, the extreme ones in this dataset,were the easiest to predict.
Most of the confusionsoccur with the 100 and 200 word summaries.The overall accuracy is slightly better whensnippets from the END of the summary are cho-sen compared to those from the START.
However,with START snippets, better prediction of differentlength summaries was obtained, whereas the ac-curacy in the END case comes mainly from correctprediction of 50 and 400 word summaries.
So weuse the START selection for further experiments.5 A regression approach based on NewYork Times editorialsWe next build a model where we predict a widerrange of lengths compared to just the four classeswe had before.
Here our training set comprisesnews articles from the New York Times (NYT)based on the assumption that edited news from agood source would be of high quality overall.5.1 DataWe obtain the text of the articles from the NYTAnnotated Corpus (Sandhaus, 2008).
We choosethe articles from the opinion section of the news-paper since they are likely to have good topic con-tinuity and related content compared to generalnews which often contain lists of facts.
We fur-ther use only the editorial articles to ensure thatthe articles are of high quality.We collect 10,724 opinion articles from years2000 to 2007 of the NYT.
We divide each articleinto topic segments using the unsupervised topicsegmentation method developed by Eisenstein andBarzilay (2008).
We use the following heuristic todecide on the number of topic segments for eacharticle.
If the article has fewer than 50 sentences,we create segments such that the expected length640of a segment is 10 sentences, i.e, we assign thenumber of segments as number of sentences di-vided by 10.
When the article is longer, we create5 segments.
This step gives us 18,167 topic seg-ments, ranging in length from 14 to 773 words.We use a stratified sampling method to selecttraining and test examples.
Starting from 90 wordsand upto a maximum length of 500 words, we di-vide the range into bins in increments of 30 words.From each bin we select 100 texts for training andaround 35 for testing.
There are 2,100 topic seg-ments in the training set and 681 for testing.5.2 Training approachWe use 100 word snippets for our experiments.We learn a linear regression model on the train-ing data using lm function in R (R DevelopmentCore Team, 2011).
The features which turned outsignificant in the model are shown in Table 3.
Thesignificance value shown is associated with a t-testto determine if the feature can be ignored from themodel.
We report the coefficients for the signifi-cant features under column ?Beta?.
The R-squaredvalue of the model is 0.219.Many of the most significant features are relatedto entities.
Longer texts are associated with largernumber of noun phrases but they tend not to beproper names.
Average word and sentence lengthalso increase with article length, at the same time,longer articles have shorter verb phrases.
Specificsentences and determiners are also positively re-lated to article length.
At the discourse level, com-parison relations increase with length.5.3 Accuracy of predictionsOn the test data, the lengths predicted by themodel have a Pearson correlation of 0.44 with thetrue length of the topic segment.
The correlation ishighly significant (p-value < 2.2e-16).
The Spear-man correlation value is 0.43 and the Kendall Tauis 0.29, both also highly significant.
These resultsshow that our model can distinguish content typesfor a range of article lengths.6 Text quality assessment for automaticsummariesIn the models above, we learned weights which re-late the features to the length of concisely writtenhuman summaries and NYT articles.
Now we usethe model to compute verbosity scores and assessFeature Beta p-valuePositive coefficientstotal noun phrases 6.052e+00 ***avg.
word length 3.201e+01 ***avg.
sent.
length 3.430e+00 **avg.
NP length 6.557e+00 *no.
of adverbs 4.244e+00 **% specific sentences 4.773e+01 **comparison relations 9.296e+00 .determiners 2.955e+00 *NP?
NP PP 4.305e+00 *NP?
NP NP 1.174e+01 *PP?
IN S 7.268e+00 .WHNP?WDT 1.196e+01 **Negative coefficientsNP?
NNP -8.630e+00 ***no.
of sentences -2.498e+01 **no.
of relations -1.128e+01 **avg.
VP length -2.982e+00 **type token ratio -1.784e+02 *NP?
NP , SBAR -1.567e+01 *NP?
NP , NP -9.582e+00 *NP?
DT NN -3.423e+00 .VP?
VBD -1.189e+01 .S?
S : S .
-1.951e+01 .ADVP?
RB -4.198e+00 .Table 3: Significant regression coefficients in thelength prediction model on NYT editorials.
?
***?indicates p-value < 0.001, ?**?
is p-value < 0.01,?*?
is < 0.05 and ?.?
is < 0.1how well they correlate with text quality scores as-signed by people.We perform this evaluation for the system sum-maries produced during the 2006 DUC evalua-tion workshop.
There are 22 automatic systems inthat evaluation.5Each system produced 250 wordsummaries for each of 20 multidocument inputs.Each summary was evaluated by DUC assessorsfor multiple dimensions of quality.
We examinehow the verbosity predictions from our model arerelated to these summary scores.
In this experi-ment, we use automatic summaries only.6.1 Gold-standard summary scoresTwo kinds of manual scores?content and linguis-tic quality?are available for each summary fromthe DUC dataset.
One type of content score,the ?pyramid score?
(Nenkova et al., 2007) com-putes the overlap of semantic units of the systemsummary with that present in human-written sum-maries for the same input.
For the other contentscore, called ?content responsiveness?, assessorsdirectly provide a rating to summaries on a scalefrom 1 (very poor) to 5 (very good) without usingany reference human summaries.5We use only the set of systems for which pyramid scoresare also available.641Verbosity scores Corr.
with actual lengthpredicted length -0.01verbosity degree -0.29deviation score -0.27Table 4: Relationship between verbosity scoresand summary lengthLinguistic quality is evaluated separately fromcontent for different aspects.
Manually assignedscores are available for non-redundancy (absenceof repetitive information), focus (well-establishedtopic), and coherence (good flow from sentence tosentence).
For each aspect, the summary is ratedon a scale from 1 (very poor) to 5 (very good).This dataset is less ideal for our task in someways as system summaries often lack coherentarrangement of sentences.
Some of our fea-tures which rely on coreference and adjacent sen-tence overlaps when computed on these sum-maries could be misleading.
However, this datacontains large scale quality ratings for differentquality aspects which allow us to examine our ver-bosity predictions across multiple dimensions.6.2 Verbosity scores and summary qualityWe choose the first 100 words of each summaryas the snippet.
No topic segmentation was doneon the summary data.
We use the NYT regres-sion model to predict the expected lengths of thesesummaries and compute its verbosity and devia-tion scores as defined in Section 2.We also compute two other measures for com-parison.Actual length.
To understand how the ver-bosity scores are related to the length of the sum-mary, we also keep track of the actual number ofwords present in the summary.Redundancy score: We also add a simple scoreto our analysis to indicate redundancy between ad-jacent sentences in the summary.
It is simple mea-sure of verbosity since repetitive information leadsto lower informativeness overall.
The score is thecosine similarity based sentence overlap measuredescribed in Section 3.For each of the 22 automatic systems, the scoresof its 20 summaries (one for each input) are av-eraged.
(We ignore empty summaries and thosewhich are much smaller than the 100 word snip-pet that we require).
We find the average val-ues for both our verbosity based scores aboveand the gold-standard scores (pyramid, content re-sponsiveness, focus, non-redundancy and coher-Content qualityscores Pyramid Resp.actual length 0.64* 0.43*predicted length -0.29 -0.11verbosity degree -0.47* -0.23deviation score -0.44* -0.29redundancy score -0.01 -0.06Linguistic qualityscores Non-red Focus Coher.actual length -0.32 -0.25 -0.32predicted length 0.48* 0.39.0.38.verbosity degree 0.55* 0.44* 0.46*deviation score 0.53* 0.40.0.42.redundancy score 0.06 0.32 0.23Table 5: Pearson correlations between verbosityscores and gold standard summary quality scoresence).
We also compute the average value of thesummary lengths for each system.First we examine the relationship between ver-bosity scores and the actual summary lengths.
ThePearson correlations between the three verbositymeasures and true length of the summaries are re-ported in Table 4.
The verbosity scores are not sig-nificantly related to summary length.
They seemto have an inverse relationship but the correlationsare not significant even at 90% confidence level.This result supports our hypothesis that verbosityscores based on expected length are different fromthe actual summary length.Next Table 5 presents the Pearson correlationsof the verbosity measures with gold standard sum-mary quality scores.
Since the number of points(systems) is only 22, we indicate whether the cor-relations are significant at two levels, 0.05 (markedby a ?*?
superscript) and 0.1 (a ?.?
superscript).The first line of the table indicates that longersummaries are associated with higher contentscores both according to pyramid and content re-sponsiveness evaluations.
This result also supportsour hypothesis that length alone does not indicateverbosity.
Longer summaries on average have bet-ter content quality.
The length is not significantlyrelated to linguistic quality scores but there is anegative relationship in general.On the other hand, all the three verbosity scoreshave a negative correlation with content scores.The verbosity degree score is the strongest in-dicator of summary quality with -0.47 (signifi-cant) correlation with pyramid score.
At the sametime however, verbosity is preferred for linguis-tic quality.
This effect could arise due to the factthese summaries are bags of unordered sentences.Therefore verbose style could be perceived as hav-642System 23?s summary: Actual length = 253 words, Predicted length = 343 words, Verbosity degree = 90A senior Scotland Yard police officer apologized to the parents of a black teenager slain five years ago in a race killingthat has become the focus of debate over relations between police and ethnic minorities.
Black teenager Stephen Lawrencewas stabbed to death at a bus-stop in Eltham, south London by five white youngsters six years ago.
The parents of themurdered black teenager Stephen Lawrence began legal action against the men suspected of his killing.
Two suspects in theStephen Lawrence murder case and one other man were arrested on suspicion of theft by Kent Police.
The five mensuspected of killing Stephen Lawrence were thumped and pelted with bottles by an enraged crowd Tuesday after a day ofevasive and implausible evidence that made a mockery of their appearance before the public inquiry.
The dawn raids cameas police questioned three men in connection with the country?s most notorious racist crime: the unsolved 1993 murder ofblack teenager Stephen Lawrence.
A public inquiry after the Lawrence case found London police institutionally racist,prompting a government pledge to take a more active role in combating racial intolerance.
The report, commissioned afterpolice botched the investigation into the 1993 racially motivated murder of a black teenager, Stephen Lawrence has putpressure on Sir Paul Condon, the Metropolitan Police chief, to resign.
British authorities and police have learned fromthe 1993 murder of black teen-ager Stephen Lawrence by a gang of white youths and the failure of the police toSystem 18?s summary: Actual length = 244 words, Predicted length = 597 words, Verbosity degree = 353The government, which has received praise from backers of the Lawrence family for its pursuit of the case, came in forcriticism on Monday for actions it took this weekend to prevent publication of a leaked version of the report, which isdue to be made public on Wednesday.
Sir William Macpherson, a retired High Court justice who was the author of thereport and chairman of the eight-month government inquiry, defined institutional racism as ?the collective failure of anorganization to provide an appropriate professional service to people because of their color, culture or ethnic origin?reflected, he said, in ?processes, attitudes and behavior which amounts to discrimination through unwitting prejudiceignorance, thoughtlessness and racist stereotyping.?
Richard Norton-Taylor, whose play about Lawrence?s killing, ?TheColor of Justice,?
has been playing to rave reviews in London, said that the attention paid to the Lawrence case andothers was a sign that British attitudes toward the overarching authority of the police and other institutions werefinally being called into question.
She said British authorities and police have learned from the 1993 murder of blackteenager Stephen Lawrence by a gang of white youths and the failure of the police to investigate his death adequatelyA senior Scotland Yard police officer Wednesday apologized to the parents of a black teenager slain five years ago in arace killing that has become the focus of debate over relations between police and ethnic minorities.Table 6: Summaries produced by two systems for input D0624 (DUC 2006) shown with the verbosityscores from our modeling greater coherence compared to short and suc-cinct sentences which are jumbled such that it ishard to decipher the full story.The simple redundancy score (last row of thetable) does not have any significant relationshipto quality scores.
One reason could be that mostsummarization systems make an effort to reduceredundant information (Carbonell and Goldstein,1998) and therefore a simple measure of wordoverlap is not helpful for distinguishing quality.As examples of the predictions from our model,Table 6 shows two summaries produced for thesame input by two different systems.
They bothhave almost the same actual length but the first re-ceived a prediction close to its actual length whilethe other is predicted with a much higher verbositydegree score.
Intuitively, the second example ismore verbose compared to the first one.
Accordingto the manual evaluations as well, the first sum-mary receives a higher score of 0.4062 (pyramid)compared to 0.2969 for the second summary.7 ConclusionsThere are several ways in which our approach canbe improved.
In this first work, we have avoidedthe complexities of manual annotation.
In fu-ture, we will explore the feasibility of human an-notations of verbosity on a suitable corpus, suchas news articles on the same topic from differentsources.
In addition, our current approach onlyconsiders a snippet of the text or topic segmentduring prediction but ignores the writing in the re-maining text.
In future work, we plan to use a slid-ing window to obtain and aggregate length predic-tions while considering the full text.AcknowledgementsThis work was partially supported by a NSF CA-REER 0953445 award.
We also thank the anony-mous reviewers for their comments.643ReferencesJ.
Carbonell and J. Goldstein.
1998.
The use of mmr,diversity-based reranking for reordering documentsand producing summaries.
In Proceedings of SIGIR,pages 335?336.D.
L. Chen and R. J. Mooney.
2008.
Learning tosportscast: a test of grounded language acquisition.In Proceedings of ICML, pages 128?135.J.
M. Conroy and H. T. Dang.
2008.
Mind the gap:Dangers of divorcing evaluations of summary con-tent from linguistic quality.
In Proceedings of COL-ING, pages 145?152.C.
Danescu-Niculescu-Mizil, J. Cheng, J. Kleinberg,and L. Lee.
2012.
You had me at hello: How phras-ing affects memorability.
In Proceedings of ACL,pages 892?901.J.
Eisenstein and R. Barzilay.
2008.
Bayesian un-supervised topic segmentation.
In Proceedings ofEMNLP, pages 334?343.J.
R. Finkel, T. Grenager, and C. Manning.
2005.
In-corporating non-local information into informationextraction systems by gibbs sampling.
In Proceed-ings of ACL, pages 363?370.M.
Galley and K. McKeown.
2007.
Lexicalizedmarkov grammars for sentence compression.
InProceedings of HLT-NAACL.V.
Ganjigunte Ashok, S. Feng, and Y. Choi.
2013.
Suc-cess with style: Using writing style to predict thesuccess of novels.
In Proceedings of EMNLP, pages1753?1764.D.
Graff.
2002.
The AQUAINT Corpus of EnglishNews Text.
Corpus number LDC2002T31, Linguis-tic Data Consortium, Philadelphia.M.
Kaisser, M. A. Hearst, and J.
B. Lowe.
2008.
Im-proving search results quality by customizing sum-mary lengths.
In Proceedings of ACL-HLT, pages701?709.D.
Klein and C.D.
Manning.
2003.
Accurate unlexi-calized parsing.
In Proceedings of ACL, pages 423?430.K.
Knight and D. Marcu.
2002.
Summarization be-yond sentence extraction: A probabilistic approachto sentence compression.
Artificial Intelligence,139(1).H.
Lakkaraju, J. J. McAuley, and J. Leskovec.
2013.What?s in a name?
understanding the interplay be-tween titles, content, and communities in social me-dia.
In ICWSM.A.
Louis and A. Nenkova.
2011.
Automatic identifica-tion of general and specific sentences by leveragingdiscourse annotations.
In Proceedings of IJCNLP,pages 605?613.A.
Louis and A. Nenkova.
2013.
What makes writinggreat?
first experiments on article quality predictionin the science journalism domain.
TACL, 1:341?352.R.
McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In Proceed-ings of EACL.A.
Nenkova, R. Passonneau, and K. McKeown.
2007.The pyramid method: Incorporating human con-tent selection variation in summarization evaluation.ACM Trans.
Speech Lang.
Process., 4(2):4.M.
O?Donnell.
1997.
Variable-length on-line docu-ment generation.
In Proceedings of the 6th Euro-pean Workshop on Natural Language Generation.E.
Pitler and A. Nenkova.
2009.
Using syntax to dis-ambiguate explicit discourse connectives in text.
InProceedings of ACL-IJCNLP, pages 13?16.E.
Pitler, A. Louis, and A. Nenkova.
2010.
Automaticevaluation of linguistic quality in multi-documentsummarization.
In Proceedings of ACL.R Development Core Team, 2011.
R: A Language andEnvironment for Statistical Computing.
R Founda-tion for Statistical Computing.K.
Raghunathan, H. Lee, S. Rangarajan, N. Chambers,M.
Surdeanu, D. Jurafsky, and C. Manning.
2010.
Amulti-pass sieve for coreference resolution.
In Pro-ceedings of EMNLP, pages 492?501.E.
Sandhaus.
2008.
The New York Times AnnotatedCorpus.
Corpus number LDC2008T19, LinguisticData Consortium, Philadelphia.644
