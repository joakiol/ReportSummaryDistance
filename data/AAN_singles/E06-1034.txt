From detecting errors to automatically correcting themMarkus DickinsonDepartment of LinguisticsGeorgetown Universitymad87@georgetown.eduAbstractFaced with the problem of annotation er-rors in part-of-speech (POS) annotatedcorpora, we develop a method for auto-matically correcting such errors.
Build-ing on top of a successful error detectionmethod, we first try correcting a corpus us-ing two off-the-shelf POS taggers, basedon the idea that they enforce consistency;with this, we find some improvement.
Af-ter some discussion of the tagging process,we alter the tagging model to better ac-count for problematic tagging distinctions.This modification results in significantlyimproved performance, reducing the errorrate of the corpus.1 IntroductionAnnotated corpora serve as training material andas ?gold standard?
testing material for the devel-opment of tools in computational linguistics, andas a source of data for theoretical linguists search-ing for relevant language patterns.
However, theycontain annotation errors, and such errors provideunreliable training and evaluation data, as has beenpreviously shown (see ch.
1 of Dickinson (2005)and references therein).
Improving the quality oflinguistic annotation where possible is thus a keyissue for the use of annotated corpora in computa-tional and theoretical linguistics.Research has gone into automatically detect-ing annotation errors for part-of-speech annota-tion (van Halteren, 2000; Kve?to?n and Oliva, 2002;Dickinson and Meurers, 2003), yet there hasbeen virtually no work on automatically or semi-automatically correcting such annotation errors.11Oliva (2001) specifies hand-written rules to detect andAutomatic correction can speed up corpus im-provement efforts and provide new data for NLPtechnology training on the corpus.
Additionally,an investigation into automatic correction forcesus to re-evaluate the technology using the corpus,providing new insights into such technology.We propose in this paper to automatically cor-rect part-of-speech (POS) annotation errors in cor-pora, by adapting existing technology for POS dis-ambiguation.
We build the correction work ontop of a POS error detection phase, described insection 2.
In section 3 we discuss how to eval-uate corpus correction work, given that we haveno benchmark corpus to compare with.
We turnto the actual work of correction in section 4, us-ing two different POS taggers as automatic cor-rectors and using the Wall Street Journal (WSJ)corpus as our data.
After more thoroughly investi-gating how problematic tagging distinctions affectthe POS disambiguation task, in section 5 wemod-ify the tagging model in order to better accountfor these distinctions, and we show this to signifi-cantly reduce the error rate of a corpus.It might be objected that automatic correctionof annotation errors will cause information to belost or will make the corpus worse than it was,but the construction of a large corpus generallyrequires semi-automated methods of annotation,and automatic tools must be used sensibly at everystage in the corpus building process.
Automatedannotation methods are not perfect, but humansalso add errors, from biases and inconsistent judg-ments.
Thus, automatic corpus correction methodscan be used semi-automatically, just as the originalcorpus creation methods were used.then correct errors, but there is no general correction scheme.2652 Detecting POS Annotation ErrorsTo correct part-of-speech (POS) annotation errors,one has to first detect such errors.
Although thereare POS error detection approaches, using, e.g.,anomaly detection (Eskin, 2000), our approachbuilds on the variation n-gram algorithm intro-duced in Dickinson and Meurers (2003) and Dick-inson (2005).
As we will show in section 5, sucha method is useful for correction because it high-lights recurring problematic tag distinctions in thecorpus.The idea behind the variation n-gram approachis that a string occurring more than once can oc-cur with different labels in a corpus, which is re-ferred to as variation.
Variation is caused by oneof two reasons: i) ambiguity: there is a type ofstring with multiple possible labels and differentcorpus occurrences of that string realize the differ-ent options, or ii) error: the tagging of a string isinconsistent across comparable occurrences.The more similar the context of a variation, themore likely the variation is an error.
In Dickin-son and Meurers (2003), contexts are composedof words, and identity of the context is required.The term variation n-gram refers to an n-gram (ofwords) in a corpus that contains a string annotateddifferently in another occurrence of the same n-gram in the corpus.
The string exhibiting the vari-ation is referred to as the variation nucleus.For example, in the WSJ corpus, part of thePenn Treebank 3 release (Marcus et al, 1993), thestring in (1) is a variation 12-gram since off is avariation nucleus that in one corpus occurrence istagged as a preposition (IN), while in another it istagged as a particle (RP).
(1) to ward off a hostile takeover attempt bytwo European shipping concernsOnce the variation n-grams for a corpus havebeen computed, heuristics are employed to clas-sify the variations into errors and ambiguities.
Themost effective heuristic takes into account the factthat natural languages favor the use of local de-pendencies over non-local ones: nuclei found atthe fringe of an n-gram are more likely to be gen-uine ambiguities than those occurring with at leastone word of surrounding context.Running the variation n-gram error detectionmethod on the WSJ turns up 7141 distinct2 non-2Being distinct means each corpus position is only takeninto account for the longest variation n-gram it occurs in.fringe nuclei, of which an estimated 92.8%, or6627, are erroneous.3 Since a variation nucleusrefers to multiple corpus positions, this precisionis a precision on types; we, however, are correct-ing tokens.
Still, this precision is high enough toexperiment with error correction.3 MethodologySince we intend to correct a corpus with POS an-notation errors, we have no true benchmark bywhich to gauge the accuracy of the corrected cor-pus, and we thus created a hand-checked sub-corpus.
Using the variation n-gram output, weflagged every non-fringe variation nucleus (token)as a potential error, giving us 21,575 flagged po-sitions in the WSJ.
From this set, we sampled 300positions, removed the tag for each position, andhand-marked what the correct tag should be, basedsolely on the tagset definitions given in the WSJtagging manual (Santorini, 1990), i.e., blind to theoriginal data.
Because some of the tagset distinc-tions were not defined clearly enough in the guide-lines, in 20 cases we could not decide what the ex-act tag should be.
For the purposes of comparison,we score a match with either tag as correct since ahuman could not disambiguate such cases.For the benchmark, we find that 201 positionsin our sample set of 300 are correct, giving us aprecision of 67%.
A correction method must thensurpass this precision figure in order to be useful.4 Approach to correctionSince our error detection phase relies on variationin annotation, i.e., the inconsistent application ofPOS labels across the corpus, we propose to cor-rect such errors by enforcing consistency in thetext.
As van Halteren (2000) points out, POS tag-gers can be used to enforce consistency, and so weemploy off-the-shelf supervised POS taggers forerror correction.
The procedure is as follows:1.
Train the tagger on the entire corpus.2.
Run the trained tagger over the same corpus.3.
For the positions the variation n-gram detec-tion method flags as potentially erroneous,choose the label obtained in step 2.We do not split training data from testing data be-cause we want to apply the patterns found in the3The recall cannot easily be estimated, but this is still asignificant number of errors.266whole corpus to the corpus we want to correct,which happens to be the same corpus.4 If the tag-ger has learned the consistent patterns in the cor-pus, it will then generalize these patterns to theproblematic parts of the corpus.This approach hinges on high-quality error de-tection since in general we cannot assume that dis-crepancies between a POS tagger and the bench-mark are errors in the benchmark.
Van Hal-teren (2000), for example, found that his taggerwas correct in only 20% of disagreements with thebenchmark.
By focusing only on the variation-flagged positions, we expect the tagger decisionsto be more often correct than incorrect.We use two off-the-shelf taggers for correc-tion, the Markov model tagger TnT (Brants, 2000)and the Decision Tree Tagger (Schmid, 1997),which we will abbreviate as DTT.
Both taggersuse probabilistic contextual and lexical informa-tion to disambiguate a tag at a particular cor-pus position.
The difference is that TnT obtainscontextual probabilities from maximum likelihoodcounts, whereas DTT constructs binary-branchingdecision trees to obtain contextual probabilities.In both cases, instead of looking at n-grams ofwords, the taggers use n-grams of tags.
This gen-eralization is desirable, as the variation n-grammethod shows that the corpus has conflicting la-bels for the exact same sequence of n words.Results For the TnT tagger, we obtain an overallprecision of 71.67% (215/300) on the 300 hand-annotated samples.
For the DTT tagger, we get ahigher precision, that of 76.33% (229/300).
TheDTT results are a significant improvement overthe original corpus precision of 67% (p = .0045),while the TnT results are not.As mentioned, tagger-benchmark disagree-ments are more commonly tagger errors, but wefind the opposite for variation-flagged positions.Narrowing in on the positions which the taggerchanged, we find a precision of 58.56% (65/111)for TnT and 65.59% (69/107) for DTT.
As the goalof correction is to change tags with 100% accu-racy, we place a priority in improving these fig-ures.One likely reason that DTT outperforms TnT is4Note, then, that some typical tagging issues, such asdealing with unknown words, are not an issue for us.5All p-values in this paper are from McNemar?s Test (Mc-Nemar, 1947) for analyzing matched dichotomous data (i.e.,a correct or incorrect score for each corpus position from bothmodels).its more flexible context.
For instance, in example(2)?which DTT correctly changes and TnT doesnot?
to know that such should be changed fromadjective (JJ) to pre-determiner (PDT), one onlyneed look at the following determiner (DT) an,and that provides enough context to disambiguate.TnT uses a fixed context of trigrams, and so canbe swayed by irrelevant tags?here, the previoustags?which DTT can in principle ignore.6(2) Mr. Bush was n?t interested in such/JJ an in-formal get-together .5 Modifying the tagging modelThe errors detected by the variation n-grammethod arise from variation in the corpus, of-ten reflecting decisions difficult for annotators tomaintain over the entire corpus, for example, thedistinction between preposition (IN) and particle(RP) (as in (1)).
Although these distinctions arelisted in the tagging guidelines (Santorini, 1990),nowhere are they encoded in the tags themselves;thus, a tagger has no direct way of knowing that INand RP are easily confusable but IN and NN (com-mon noun) are not.
In order to improve automaticcorrection, we can add information about these re-curring distinctions to the tagging model, makingthe tagger aware of the difficult distinctions.
Buthow do we make a tagger ?aware?
of a relevantproblematic distinction?Consider the domain of POS tagging.
Everyword patterns uniquely, yet there are generaliza-tions about words which we capture by group-ing them into POS classes.
By grouping wordsinto the same class, there is often a claim thatthese words share distributional properties.
Buthow true this is depends on one?s tagset (see, e.g.,De?jean (2000)).
If we can alter the tagset to bet-ter match the distributional facts, we can improvecorrection.To see how problematic distinctions can assistin altering the tagset, consider the words away andaboard, both of which can be adverbs (RB) in thePenn Treebank, as shown in (3a) and (4a).
In ex-ample (3b), we find that away can also be a par-ticle (RP), thus making it a part of the ambigu-ity class RB/RP.
On the other hand, as shown in(4b), aboard can be a preposition (IN), but not aparticle, putting it in the ambiguity class IN/RB.Crucially, not only do away and aboard belong6As DTT does not provide a way of viewing output trees,we cannot confirm that this is the reason for improvement.267to different ambiguity classes, but their adverbialuses are also distinguished.
The adverbial awayis followed by from, a construction forbidden foraboard.
When we examine the RB/RP words, wefind that they form a natural class: apart, aside,and away, all of which can be followed by from.
(3) a. the Cray-3 machine is at least anotheryear away/RB from a ... prototypeb.
A lot of people think 0 I will giveaway/RP the store(4) a. Saturday ?s crash ... that *T* killed 132of the 146 people aboard/RBb.
These are used * aboard/IN military heli-coptersAlthough not every ambiguity class is socleanly delineated, this example demonstrates thatsuch classes can be used to redefine a taggingmodel with more unified groupings.5.1 Using complex ambiguity tagsWe thus propose splitting a class such as RB intosubclasses, using these ambiguity classes?JJ/RB,NN/RB, IN/RB, etc.
?akin to previous work onsplitting labels in order to obtain better statistics(e.g., Brants (1996); Ule (2003)) for situationswith ?the same label but different usage?
(Ule,2003, p. 181).
By taking this approach, we arenarrowing in on what annotators were instructedto focus on, namely ?difficult tagging decisions,?
(Santorini, 1990, p. 7).We implement this idea by assigning words anew, complex tag composed of its ambiguity classand the benchmark tag for that position.
For ex-ample, ago has the ambiguity class IN/RB, and inexample (5a), it resolves to RB.
Thus, followingthe notation in Pla and Molina (2004), we assignago the complex ambiguity tag <IN/RB,RB> inthe training data, as shown in (5b).
(5) a. ago/RBb.
ago/<IN/RB,RB>Complex ambiguity tags can provide better dis-tinctions than the unaltered tags.
For example,words which vary between IN and RB and taggedas IN (e.g., ago, tagged <IN/RB,IN>) can ignorethe contextual information that words varying be-tween DT (determiner) and IN (e.g., that, tagged<DT/IN,IN>) provide.
This proposal is in thespirit of a tagger like that described in Marquez etal (2000), which breaks the POS tagging probleminto one problem for each ambiguity class, but be-cause we alter the tagset here, different underlyingtagging algorithms can be used.To take an example, consider the 5-gram rev-enue of about $ 370 as it is tagged by TnT.
The5-gram (at position 1344) in the WSJ is annotatedas in (6).
The tag for about is incorrect since?about when used to mean ?approximately?
shouldbe tagged as an adverb (RB), rather than a prepo-sition (IN)?
(Santorini, 1990, p.
22).
(6) revenue/NN of/IN about/IN $/$ 370/CDBetween of and $, the word about varies be-tween preposition (IN) and adverb (RB): it is IN67 times and RB 65 times.
After training TnT onthe original corpus, we find that RB is a slightlybetter predictor of the following $ tag, as shown in(7), but, due to the surrounding probabilities, IN isthe tag TnT assigns.
(7) a. p($|IN,RB) = .0859b.
p($|IN,IN) = .0635The difference between probabilities is morepronounced in the model with complex ambigu-ity tags.
The word about generally varies betweenthree tags: IN, RB, and RP (particle), receiving theambiguity class IN/RB/RP (as of also does).
ForIN/RB/RP words, RB is significantly more proba-ble in this context than IN, as shown in (8).
(8) a. p($|<IN/RB/RP,IN>,<IN/RB/RP,RB>)= .6016b.
p($|<IN/RB/RP,IN>,<IN/RB/RP,IN>)= .1256Comparing (7) and (8), we see that RB for theambiguity class of IN/RB/RP behaves differentlythan the general class of RB words.We have just shown that the contextual proba-bilities of an n-gram tagger are affected when us-ing complex ambiguity tags; lexical probabilitiesare also dramatically changed.
The relevant prob-abilities were originally as in (9), but for the mod-ified corpus, we have the probabilities in (10).
(9) a. p(about|IN) = 2074/134926 = .0154b.
p(about|RB) = 785/42207 = .0186(10) a. p(about|<IN/RB/RP,IN>)= 2074/64046 = .0324b.
p(about|<IN/RB/RP,RB>)= 785/2045 = .3839268These altered probabilities provide informationsimilar to that found in a lexicalized tagger?i.e., about behaves differently than the rest of itsclass?but the altered contextual probabilities, un-like a lexicalized tagger, bring general IN/RB/RPclass information to bear on this tagging situation.Combining the two, we get the correct tag RB atthis position.Since variation errors are errors for words withprominent ambiguity classes, zeroing in on theseambiguity classes should provide more accurateprobabilities.
For this to work, however, we haveto ensure that we have the most effective ambigu-ity class for every word.5.2 Assigning complex ambiguity tagsIn the tagging literature (e.g., Cutting et al(1992))an ambiguity class is often composed of the set ofevery possible tag for a word.
For correction, us-ing every possible tag for an ambiguity class willresult in too many classes, for two reasons: 1)there are erroneous tags which should not be partof the ambiguity class, and 2) some classes are ir-relevant for disambiguating variation positions.Guided by these considerations, we use the pro-cedure below to assign complex ambiguity tags toall words in the corpus, based on whether a word isa non-fringe variation nucleus and thus flagged asa potential error by the variation n-gram method(choice 1), or is not a nucleus (choice 2).1.
Every word which is a variation word (nu-cleus of a non-fringe variation) or type-identical to a variation word is assigned:(a) a complex tag reflecting the ambiguityclass of all relevant ambiguities in thenon-fringe variation nuclei; or(b) a simple tag reflecting no ambiguity, ifthe tag is irrelevant.2.
Based on their relevant unigram tags, non-variation words are assigned:(a) a complex tag, if the word?s ambiguitytag also appears as a variation ambigu-ity; or(b) a simple tag, otherwise.Variation words (choice 1) We start with vari-ation nuclei because these are the potential errorswe wish to correct.
An example of choice 1a isago, which varies between IN and RB as a nu-cleus, and so receives the tag <IN/RB,IN> whenit resolves to IN and <IN/RB,RB> when it re-solves to RB.The choices are based on relevance, though; in-stead of simply assigning all tags occurring in anambiguity to an ambiguity class, we filter out am-biguities which we deem irrelevant.
Similar toBrill and Pop (1999) and Schmid (1997), we dothis by examining the variation unigrams and re-moving tags which occur less than 0.01 of thetime for a word and less than 10 times overall.This eliminates variations like ,/DT where DT ap-pears 4210 times for an, but the comma tag ap-pears only once.
Doing this means that an cannow be grouped with other unambiguous deter-miners (DT).
In addition to removing some erro-neous classes, we gain generality and avoid datasparseness by using fewer ambiguity classes.This pruning also means that some variationwords will receive tags which are not part of avariation, which is when choice 1b is selected.
Forinstance, if the class is IN/RB and the current tagis JJ, it gets JJ instead of <IN/RB,JJ> because aword varying between IN and RB should not re-solve to JJ.
This situation also arises because weare deriving the ambiguity tags only from the non-fringe nuclei but are additionally assigning themto type-identical words in the corpus.
Words in-volved in a variation may elsewhere have tagsnever involved in a variation.
For example, Ad-vertisers occurs as a non-fringe nucleus varyingbetween NNP (proper noun) and NNPS (pluralproper noun).
In non-variation positions, it ap-pears as a plural common noun (NNS), which wetag as NNS because NNS is not relevant to thevariation (NNP/NNPS) we wish to distinguish.Onemore note is needed to explain how we han-dled the vertical slashes used in the Penn Tree-bank annotation.
Vertical slashes represent uncer-tainty between two tags?e.g., JJ|VBN means theannotator could not decide between JJ and VBN(past participle).
Variation between JJ, VBN, andJJ|VBN is simply variation between JJ and VBN,and we represent it by the class JJ/VBN, therebyensuring that JJ/VBN has more data.In short, we assign complex ambiguity tags tovariation words whenever possible (choice 1a), butbecause of pruning and because of non-variationtags for a word, we have to assign simple tags tosome corpus positions (choice 1b).Non-variation words (choice 2) In order tohave more data for a tag, non-variation words also269take complex ambiguity tags.
For words whichare not a part of a variation nucleus, we simi-larly determine relevance and then assign a com-plex ambiguity tag if the ambiguity is elsewhereinvolved in a non-fringe nucleus (choice 2a).
Forinstance, even though join is never a non-fringevariation nucleus, it gets the tag <VB/VBP,VB>in the first sentence of the treebank because its am-biguity class VB/VBP is represented in the non-fringe nuclei.On the other hand, we ignore ambiguity classeswhich have no bearing on correction (choice 2b).For example, ours varies between JJ and PRP (per-sonal pronoun), but no non-fringe variation nucleihave this same ambiguity class, so no complexambiguity tag is assigned.
Our treatment of non-variation words increases the amount of relevantdata (choice 2a) and still puts all non-varying datatogether (choice 2b).Uniform assignment of tags Why do we allowonly one ambiguity class per word over the wholecorpus?
Consider the variation nucleus traded:in publicly traded investments, traded varies be-tween JJ and VBN, but in contracts traded on, itvaries between VBN and VBD (past tense verb).
Itseems like it would be useful to keep the JJ/VBNcases separate from the VBD/VBN ones, so that atagger can learn one set of patterns for JJ/VBN anda different set for VBD/VBN.
While that mighthave its benefits, there are several reasons why re-stricting words to a single ambiguity class is de-sirable, i.e., why we assign traded the ambiguityclass JJ/VBD/VBN in this case.First, we want to group as many of the word oc-currences as possible together into a single class.Using JJ/VBN and VBD/VBN as two separate am-biguity classes would mean that traded as VBNlacks a pattern of its own.Secondly, multiple ambiguity classes for aword can increase the number of possible tagsfor a word.
For example, instead of havingonly the tag <JJ/VBD/VBN,VBN> for traded asVBN, we would have both <JJ/VBN,VBN> and<VBD/VBN,VBN>.
With such an increase in thenumber of tags, data sparseness becomes a prob-lem.Finally, although we know what the exact ambi-guity in question is for a non-fringe nucleus, it istoo difficult to go through position by position toguess the correct ambiguity for every other spot.
Ifwe encounter a JJ/VBD/VBN word like followedtagged as VBN, for example, we cannot know forsure whether this is an instance where JJ/VBNwasthe decision which had to be made or if VBD/VBNwas the difficult choice; keeping only one ambigu-ity class per word allows us to avoid guessing.5.3 Results with complex ambiguity tagsUsing complex ambiguity tags increases the sizeof the tagset from 80 tags in the original corpus 7to 418 tags in the altered tagset, 53 of which aresimple (e.g.
IN) and 365 of which are complex(e.g.
<IN/RB,IN>).TnT Examining the 300 samples of variationpositions from the WSJ corpus for the TnT tag-ger with complex ambiguity tags, we find that234 spots are correctly tagged, for a precision of78.00%.
Additionally, we find 73.86% (65/88)precision for tags which have been changed fromthe original corpus.
The 78% precision is a signif-icant improvement both over the original TnT pre-cision of 71.67% (p = .008) and the benchmark of67% (p = .001).
Perhaps more revealing is the im-provement in the precision of the changed tokens,from 58.56% to 73.86%.
With 73.86% precisionfor changed positions, this means that we expectapproximately 3968 of the 5373 changes that thetagger makes, out of 21,575 flagged positions, tobe correct changes.
Thus, the error rate of the cor-pus will be reduced.Decision Tree Tagger (DTT) Using complexambiguity tags with DTT results in an overall pre-cision of 78.33% (235/300) and a precision of73.56% (64/87) for the changed positions.
We im-prove the overall error correction precision, from76.33% to 78.33%, and the tagging of changed po-sitions, going from 65.59% to 73.56%.The results for all four models, plus the base-line, are summarized in figure 1.
From these fig-ures, it seems that the solution for error correctionlies less in what tagging method is used and morein the information we give each method.The improvement in changed positions for bothTnT and DTT is partly attributable to the fact thatboth tagging models are making fewer changes.Indeed, training TnT on the original corpus andthen testing on the same corpus results in a 97.37%similarity, but a TnT model trained on complexambiguity tags results in 98.49% similarity with7The number of tags here counts tags with vertical slashesseparately.270Total ChangedBaseline 67.00% N/ATnT 71.67% 58.56% (65/111)C.A.
TnT 78.00% 73.86% (65/88)DTT 76.33% 65.59% (69/107)C.A.
DTT 78.33% 73.56% (64/87)Figure 1: Summary of resultsthe original.
DTT sees a parallel overall improve-ment, from 97.47% to 98.33%.
Clearly, then, eachcomplex ambiguity model is a closer fit to the orig-inal corpus.
Whether this means it is an overallbetter POS tagging model is an open question.Remaining issues We have shown that we canimprove the annotation of a corpus by using tag-ging models with complex ambiguity tags, but canwe improve even further?
To do so, there are sev-eral obstacles to overcome.First, some distinctions cannot be handled by anautomated system without semantic or non-localinformation.
As Marquez and Padro (1997) pointout, distinctions such as that between JJ and VBNare essentially semantic distinctions without anystructural basis.
For example, in the phrase pro-posed offering, the reason that proposed should beVBN is that it indicates a specific event.
Since ourmethod uses no external semantic information, wehave no way to know how to correct this.8Other distinctions, such as the one betweenVBD and VBN, require some form of non-localknowledge in order to disambiguate because it de-pends on the presence or absence of an auxiliaryverb, which can be arbitrarily far away.Secondly, sometimes the corpus was more of-ten wrong than right for a particular pattern.
Thiscan be illustrated by looking at the word later inexample (11), from the WSJ corpus.
In the tag-ging manual (Santorini, 1990, p. 25), we find thedescription of later as in (12).
(11) Now , 13 years later , Mr. Lane has revivedhis Artist ...(12) later should be tagged as a simpleadverb (RB) rather than as a com-parative adverb (RBR), unless itsmeaning is clearly comparative.
A8Note that it could be argued that this lack of a structuraldistinction contributed to the inconsistency among annotatorsin the first place and thus made error detection successful.useful diagnostic is that the com-parative later can be preceded byeven or still.In example (11), along with the fact that thisis 13 years later as compared to now (i.e., com-parative), one can say Now, (even) 13 years later,Mr.
Lane has revived his Artist ..., favoring RBRas a tag.
But the trigram years later , occurs 16times, 12 as RB and 4 as RBR.
Assuming RBR iscorrect, we clearly have a lot of wrong annotationin the corpus, even though here the corpus is cor-rectly annotated as RBR.
As seen in (13), in thecontext of following CD and NNS, RBR is muchless likely for TnT than either RB or JJ.
(13) a. p(JJ|CD,NNS) = .0366b.
p(RB|CD,NNS) = .0531c.
p(RBR|CD,NNS) = .0044As shown in (14), even when we use complexambiguity tags, we still find this favoritism for RBbecause of the overwhelmingly wrong data in thecorpus.
However, we note that although RB is fa-vored, its next closest competitor is now RBR?not JJ?and RB is no longer favored by as muchas it was over RBR.
We have more appropriatelynarrowed down the list of proper tags for this posi-tion by using complex ambiguity tags, but becauseof too much incorrect annotation, we still generatethe wrong tag.
(14) a. p(<JJ/RB/RBR,JJ>|CD,NNS) = .0002b.
p(<JJ/RB/RBR,RB>|CD,NNS)= .0054c.
p(<JJ/RB/RBR,RBR>|CD,NNS)=.0017These issues show that automatic correctionmust be used with care, but they also highlight par-ticular aspects of this tagset that any POS taggingmethod will have difficulty overcoming, and theeffect of wrong data again serves to illustrate theproblem of annotation errors in training data.6 Summary and OutlookWe have demonstrated the effectiveness of usingPOS tagging technology to correct a corpus, oncean error detection method has identified poten-tially erroneous corpus positions.
We first showedthat using a tagger as is provides moderate re-sults, but adapting a tagger to account for problem-atic tag distinctions in the data?i.e., using com-plex ambiguity tags?performs much better and271reduces the true error rate of a corpus.
The distinc-tions in the tagging model have more of an impacton the precision of correction than the underlyingtagging algorithm.Despite the gain in accuracy, we pointed outthat there are still several residual problems whichare difficult for any tagging system.
Future workwill go into automatically sorting the tags so thatthe difficult disambiguation decisions can be dealtwith differently from the easily disambiguatedcorpus positions.
Additionally, we will want totest the method on a variety of corpora and tag-ging schemes and gauge the impact of correc-tion on POS tagger training and evaluation.
Wehypothesize that this method will work for anytagset with potentially confusing distinctions be-tween tags, but this is yet to be tested.The method of adapting a tagging model by us-ing complex ambiguity tags originated from anunderstanding that the POS tagging process iscrucially dependent upon the tagset distinctions.Based on this, the correction work described inthis paper can be extended to the general task ofPOS tagging, as a tagger using complex ambiguityclasses is attempting to tackle the difficult distinc-tions in a corpus.
To pursue this line of research,work has to go into defining ambiguity classes forall words in the corpus, instead of focusing onwords involved in variations.Acknowledgments I would like to thank Det-mar Meurers for helpful discussion, StephanieDickinson for her statistical assistance, and thethree anonymous reviewers for their comments.ReferencesThorsten Brants.
1996.
Estimating Markov modelstructures.
In Proceedings of ICSLP-96, pages 893?896, Philadelphia, PA.Thorsten Brants.
2000.
TnT ?
a statistical part-of-speech tagger.
In Proceedings of ANLP-2000, pages224?231, Seattle, WA.Eric Brill and Mihai Pop.
1999.
Unsupervised learn-ing of disambiguation rules for part of speech tag-ging.
In Kenneth W. Church, editor, Natural Lan-guage Processing Using Very Large Corpora, pages27?42.
Kluwer Academic Press, Dordrecht.Doug Cutting, Julian Kupiec, Jan Pedersen, and Pene-lope Sibun.
1992.
A practical part-of-speech tagger.In Proceedings of ANLP-92, pages 133?140, Trento,Italy.Herve?
De?jean.
2000.
How to evaluate and comparetagsets?
a proposal.
In Proceedings of LREC-00,Athens.Markus Dickinson and W. Detmar Meurers.
2003.Detecting errors in part-of-speech annotation.
InProceedings of EACL-03, pages 107?114, Budapest,Hungary.Markus Dickinson.
2005.
Error detection and correc-tion in annotated corpora.
Ph.D. thesis, The OhioState University.Eleazar Eskin.
2000.
Automatic corpus correctionwith anomaly detection.
In Proceedings of NAACL-00, pages 148?153, Seattle, Washington.Pavel Kve?to?n and Karel Oliva.
2002.
Achieving analmost correct PoS-tagged corpus.
In Text, Speechand Dialogue (TSD 2002), pages 19?26, Heidelberg.Springer.Mitchell Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Lluis Marquez and Lluis Padro.
1997.
A flexiblePOS tagger using an automatically acquired lan-guage model.
In Proceedings of ACL-97, pages238?245, Madrid, Spain.Lluis Marquez, Lluis Padro, and Horacio Rodriguez.2000.
A machine learning approach to POS tagging.Machine Learning, 39(1):59?91.Quinn McNemar.
1947.
Note on the sampling errorof the difference between correlated proportions orpercentages.
Psychometrika, 12:153?157.Karel Oliva.
2001.
The possibilities of automatic de-tection/correction of errors in tagged corpora: a pilotstudy on a German corpus.
In Text, Speech and Di-alogue (TSD 2001), pages 39?46.
Springer.Ferran Pla and AntonioMolina.
2004.
Improving part-of-speech tagging using lexicalized HMMs.
NaturalLanguage Engineering, 10(2):167?189.Beatrice Santorini.
1990.
Part-of-speech taggingguidelines for the Penn Treebank project (3rd revi-sion, 2nd printing).
Technical Report MS-CIS-90-47, The University of Pennsylvania, Philadelphia,PA, June.Helmut Schmid.
1997.
Probabilistic part-of-speechtagging using decision trees.
In D.H. Jones and H.L.Somers, editors, New Methods in Language Process-ing, pages 154?164.
UCL Press, London.Tylman Ule.
2003.
Directed treebank refinement forPCFG parsing.
In Proceedings of TLT 2003, pages177?188, Va?xjo?, Sweden.Hans van Halteren.
2000.
The detection of incon-sistency in manually tagged text.
In Anne Abeille?,Thosten Brants, and Hans Uszkoreit, editors, Pro-ceedings of LINC-00, Luxembourg.272
