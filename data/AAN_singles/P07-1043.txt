Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 336?343,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsSentence generation as a planning problemAlexander KollerCenter for Computational Learning SystemsColumbia Universitykoller@cs.columbia.eduMatthew StoneComputer ScienceRutgers UniversityMatthew.Stone@rutgers.eduAbstractWe translate sentence generation from TAGgrammars with semantic and pragmatic in-formation into a planning problem by encod-ing the contribution of each word declara-tively and explicitly.
This allows us to ex-ploit the performance of off-the-shelf plan-ners.
It also opens up new perspectives onreferring expression generation and the rela-tionship between language and action.1 IntroductionSystems that produce natural language must synthe-size the primitives of linguistic structure into well-formed utterances that make desired contributions todiscourse.
This is fundamentally a planning prob-lem: Each linguistic primitive makes certain con-tributions while potentially introducing new goals.In this paper, we make this perspective explicit bytranslating the sentence generation problem of TAGgrammars with semantic and pragmatic informationinto a planning problem stated in the widely usedPlanning Domain Definition Language (PDDL, Mc-Dermott (2000)).
The encoding provides a cleanseparation between computation and linguistic mod-elling and is open to future extensions.
It also allowsus to benefit from the past and ongoing advances inthe performance of off-the-shelf planners (Blum andFurst, 1997; Kautz and Selman, 1998; Hoffmannand Nebel, 2001).While there have been previous systems that en-code generation as planning (Cohen and Perrault,1979; Appelt, 1985; Heeman and Hirst, 1995), ourapproach is distinguished from these systems by itsfocus on the grammatically specified contributionsof each individual word (and the TAG tree it an-chors) to syntax, semantics, and local pragmatics(Hobbs et al, 1993).
For example, words directlyachieve content goals by adding a corresponding se-mantic primitive to the conversational record.
Wedeliberately avoid reasoning about utterances as co-ordinated rational behavior, as earlier systems did;this allows us to get by with a much simpler logic.The problem we solve encompasses the genera-tion of referring expressions (REs) as a special case.Unlike some approaches (Dale and Reiter, 1995;Heeman and Hirst, 1995), we do not have to dis-tinguish between generating NPs and expressions ofother syntactic categories.
We develop a new per-spective on the lifecycle of a distractor, which allowsus to generate more succinct REs by taking the restof the utterance into account.
More generally, we donot split the process of sentence generation into twoseparate steps of sentence planning and realization,as most other systems do, but solve the joint prob-lem in a single integrated step.
This can potentiallyallow us to generate higher-quality sentences.
Weshare these advantages with systems such as SPUD(Stone et al, 2003).Crucially, however, our approach describes thedynamics of interpretation explicitly and declara-tively.
We do not need to assume extra machin-ery beyond the encoding of words as PDDL plan-ning operators; for example, our planning opera-tors give a self-contained description of how eachindividual word contributes to resolving references.This makes our encoding more direct and transpar-ent than those in work like Thomason and Hobbs(1997) and Stone et al (2003).We present our encoding in a sequence of steps,each of which adds more linguistic information to336the planning operators.
After a brief review of LTAGand PDDL, we first focus on syntax alone and showhow to cast the problem of generating grammaticallywell-formed LTAG trees as a planning problem inSection 2.
In Section 3, we add semantics to the ele-mentary trees and add goals to communicate specificcontent (this corresponds to surface realization).
Wecomplete the account by modeling referring expres-sions and go through an example.
Finally, we assessthe practical efficiency of our approach and discussfuture work in Section 4.2 Grammaticality as planningWe start by reviewing the LTAG grammar formal-ism and giving an intuition of how LTAG gen-eration is planning.
We then add semantic rolesto the LTAG elementary trees in order to distin-guish different substitution nodes.
Finally, we re-view the PDDL planning specification language andshow how LTAG grammaticality can be encoded asa PDDL problem and how we can reconstruct anLTAG derivation from the plan.2.1 Tree-adjoining grammarsThe grammar formalism we use here is that of lex-icalized tree-adjoining grammars (LTAG; Joshi andSchabes (1997)).
An LTAG grammar consists of afinite set of lexicalized elementary trees as shown inFig.
1a.
Each elementary tree contains exactly oneanchor node, which is labelled by a word.
Elemen-tary trees can contain substitution nodes, which aremarked by down arrows (?).
Those elementary treesthat are auxiliary trees also contain exactly one footnode, which is marked with an asterisk (?).
Treesthat are not auxiliary trees are called initial trees.Elementary trees can be combined by substitutionand adjunction to form larger trees.
Substitutionis the operation of replacing a substitution node ofsome tree by another initial tree with the same rootlabel.
Adjunction is the operation of splicing an aux-iliary tree into some node v of a tree, in such a waythat the root of the auxiliary tree becomes the childof v?s parent, and the foot node becomes the parentof v?s children.
If a node carries a null adjunctionconstraint (indicated by no-adjoin), no adjunction isallowed at this node; if it carries an obligatory ad-junction constraint (indicated by adjoin!
), an auxil-SNP ?VPVlikesNPtheNP *PNMaryNwhite N *Marylikesrabbitwhite(a)(b)(c)NP ?NrabbitNPadjoin!NPSNP VPVlikesNPPNMaryNPtheNwhite Nrabbittheno-adjoinFigure 1: Building a derived (b) and a derivation tree(c) by combining elementary trees (a).iary tree must be adjoined there.In Fig.
1a, we have combined some ele-mentary trees by substitution (indicated by thedashed/magenta arrows) and adjunction (dotted/bluearrows).
The result of these operations is the derivedtree in Fig.
1b.
The derivation tree in Fig.
1c rep-resents the tree combination operations we used byhaving one node per elementary tree and drawing asolid edge if we combined the two trees by substitu-tion, and a dashed edge for adjunctions.2.2 The basic ideaConsider the process of constructing a derivationtree top-down.
To build the tree in Fig.
1c, say, westart with the empty derivation tree and an obligationto generate an expression of category S. We satisfythis obligation by adding the tree for ?likes?
as theroot of the derivation; but in doing so, we have in-troduced new unfilled substitution nodes of categoryNP, i.e.
the derivation tree is not complete.
We usethe NP tree for ?Mary?
to fill one substitution nodeand the NP tree for ?rabbit?
to fill the other.
Thisfills both substitution nodes, but the ?rabbit?
tree in-troduces an obligatory adjunction constraint, whichwe must satisfy by adjoining the auxiliary tree for?the?.
We now have a grammatical derivation tree,but we are free to continue by adding more auxiliarytrees, such as the one for ?white?.As we have just presented it, the generation ofderivation trees is essentially a planning problem.A planning problem involves states and actions thatcan move from one state to another.
The task is tofind a sequence of actions that moves us from the337initial state to a state that satisfies all the goals.
Inour case, the states are defined by the unfilled sub-stitution nodes, the unsatisfied obligatory adjunctionconstraints, and the nodes that are available for ad-junction in some (possibly incomplete) derivationtree.
Each action adds a single elementary tree to thederivation, removing some of these ?open nodes?while introducing new ones.
The initial state is asso-ciated with the empty derivation tree and a require-ment to generate an expression for the given root cat-egory.
The goal is for the current derivation tree tobe grammatically complete.2.3 Semantic rolesFormalizing this intuition requires unique names foreach node in the derived tree.
Such names are nec-essary to distinguish the different open substitutionnodes that still need to be filled, or the differentavailable adjunction sites; in the example, the plan-ner needed to be aware that ?likes?
introduces twoseparate NP substitution nodes to fill.There are many ways to assign these names.
Onethat works particularly well in the context of PDDL(as we will see below) is to assume that each nodein an elementary tree, except for ones with null ad-junction constraints, is marked with a semantic role,and that all substitution nodes are marked with dif-ferent roles.
Nothing hinges on the particular role in-ventory; here we assume an inventory including theroles ag for ?agent?
and pat for ?patient?.
We alsoassume one special role self, which must be used forthe root of each elementary tree and must never beused for substitution nodes.We can now assign a unique name to every sub-stitution node in a derived tree by assigning arbitrarybut distinct indices to each use of an elementary tree,and giving the substitution node with role r in the el-ementary tree with index i the identity i.r.
In the ex-ample, let?s say the ?likes?
tree has index 1 and thesemantic roles for the substitution nodes were ag andpat, respectively.
The planner action that adds thistree would then require substitution of one NP withidentity 1.ag and another NP with identity 1.pat; the?Mary?
tree would satisfy the first requirement andthe ?rabbit?
tree the second.
If we assume that noelementary tree contains two internal nodes with thesame category and role, we can refer to adjunctionopportunities in a similar way.Action S-likes-1(u).
Precond: subst(S,u),step(1)Effect: ?subst(S,u),subst(NP,1.ag),subst(NP,1.pat),?step(1),step(2)Action NP-Mary-2(u).
Precond: subst(NP,u),step(2)Effect: ?subst(NP,u),?step(2),step(3)Action NP-rabbit-3(u).
Precond: subst(NP,u),step(3)Effect: ?subst(NP,u),canadjoin(NP,u),mustadjoin(NP,u),?step(3),step(4)Action NP-the-4(u).
Precond: canadjoin(NP,u),step(4)Effect: ?mustadjoin(NP,u),?step(4),step(5)Figure 2: Some actions for the grammar in Fig.
1.2.4 Encoding in PDDLNow we are ready to encode the problem of generat-ing grammatical LTAG derivation trees into PDDL.PDDL (McDermott, 2000) is the standard input lan-guage for modern planning systems.
It is based onthe well-known STRIPS language (Fikes and Nils-son, 1971).
In this paradigm, a planning state isdefined as a finite set of ground atoms of predicatelogic that are true in this state; all other atoms are as-sumed to be false.
Actions have a number of param-eters, as well as a precondition and effect, both ofwhich are logical formulas.
When a planner tries toapply an action, it will first create an action instanceby binding all parameters to constants from the do-main.
It must then verify that the precondition of theaction instance is satisfied in the current state.
If so,the action can be applied, in which case the effect isprocessed in order to change the state.
In STRIPS,the precondition and effect both had to be conjunc-tions of atoms or negated atoms; positive effects areinterpreted as making the atom true in the new state,and negative ones as making it false.
PDDL per-mits numerous extensions to the formulas that canbe used as preconditions and effects.Each action in our planning problem encodes theeffect of adding some elementary tree to the deriva-tion tree.
An initial tree with root category A trans-lates to an action with a parameter u for the iden-tity of the node that the current tree is substitutedinto.
The action carries the precondition subst(A,u),and so can only be applied if u is an open substi-tution node in the current derivation with the cor-rect category A. Auxiliary trees are analogous, butcarry the precondition canadjoin(A,u).
The effectof an initial tree is to remove the subst conditionfrom the planning state (to record that the substitu-338S-likes-1(1.self)subst(S,1.self)subst(NP,1.ag)NP-Mary-2(1.ag)subst(NP,1.pat)NP-rabbit-3(1.pat)mustadjoin(NP,1.pat)NP-the-4(1.pat)canadjoin(NP,1.pat)subst(NP,1.pat)canadjoin(NP,1.pat)step(1)step(2)step(3)step(4)step(5)Figure 3: A plan for the actions in Fig.
2.tion node u is now filled); an auxiliary tree has aneffect ?mustadjoin(A,u) to indicate that any oblig-atory adjunction constraint is satisfied but leaves thecanadjoin condition in place to allow multiple ad-junctions into the same node.
In both cases, effectsadd subst, canadjoin and mustadjoin atoms repre-senting the substitution nodes and adjunction sitesthat are introduced by the new elementary tree.One remaining complication is that an action mustassign new identities to the nodes it introduces; thusit must have access to a tree index that was not usedin the derivation tree so far.
We use the number ofthe current plan step as the index.
We add an atomstep(1) to the initial state of the planning problem,and we introduce k different copies of the actions foreach elementary tree, where k is some upper limiton the plan size.
These actions are identical, exceptthat the i-th copy has an extra precondition step(i)and effects ?step(i) and step(i+1).
It is no restric-tion to assume an upper limit on the plan size, asmost modern planners search for plans smaller thana given maximum length anyway.Fig.
2 shows some of the actions into which thegrammar in Fig.
1 translates.
We display only onecopy of each action and have left out most of thecanadjoin effects.
In addition, we use an initial statecontaining the atoms subst(S,1.self) and step(1)and a final state consisting of the following goal:?A,u.?subst(A,u)??A,u.
?mustadjoin(A,u).We can then send the actions and the initial stateand goal specifications to any off-the-shelf plannerand obtain the plan in Fig.
3.
The straight arrows inthe picture link the actions to their preconditions and(positive) effects; the curved arrows indicate atomsthat carry over from one state to the next withoutbeing changed by the action.
Atoms are printed inboldface iff they contradict the goal.This plan can be read as a derivation tree that hasone node for each action instance in the plan, and anedge from node u to node v if u establishes a substor canadjoin fact that is a precondition of v. Thesecausal links are drawn as bold edges in Fig.
3.
Themapping is unique for substitution edges becausesubst atoms are removed by every action that hasthem as their precondition.
There may be multipleaction instances in the plan that introduce the sameatom canadjoin(A,u).
In this case, we can freelychoose one of these instances as the parent.3 Sentence generation as planningNow we extend this encoding to deal with semanticsand referring expressions.3.1 Communicative goalsIn order to use the planner as a surface realiza-tion algorithm for TAG along the lines of Kollerand Striegnitz (2002), we attach semantic content toeach elementary tree and require that the sentenceachieves a certain communicative goal.
We also usea knowledge base that specifies the speaker?s knowl-edge, and require that we can only use trees that ex-press information in this knowledge base.We follow Stone et al (2003) in formalizing thesemantic content of a lexicalized elementary tree t asa finite set of atoms; but unlike in earlier approaches,we use the semantic roles in t as the arguments ofthese atoms.
For instance, the semantic content ofthe ?likes?
tree in Fig.
1 is {like(self,ag,pat)} (seealso the semcon entries in Fig.
4).
The knowledgebase is some finite set of ground atoms; in the exam-ple, it could contain such entries as like(e,m,r) andrabbit(r).
Finally, the communicative goal is somesubset of the knowledge base, such as {like(e,m,r)}.We implement unsatisfied communicative goalsas flaws that the plan must remedy.
To this end,we add an atom cg(P,a1, .
.
.
,an) for each elementP(a1, .
.
.
,an) of the communicative goal to the ini-tial state, and we add a corresponding conjunct?P,x1, .
.
.
,xn.
?cg(P,x1, .
.
.
,xn) to the goal.
In ad-dition, we add an atom skb(P,a1, .
.
.
,an) to theinitial state for each element P(a1, .
.
.
,an) of the(speaker?s) knowledge base.339We then add parameters x1, .
.
.
,xn to each actionwith n semantic roles (including self).
These newparameters are intended to be bound to individualconstants in the knowledge base by the planner.
Foreach elementary tree t and possible step index i, weestablish the relationship between these parametersand the roles in two steps.
First we fix a function idthat maps the semantic roles of t to node identities.It maps self to u and each other role r to i.r.
Second,we fix a function ref that maps the outputs of id bi-jectively to the parameters x1, .
.
.
,xn, in such a waythat ref(u) = x1.We can then capture the contribution of the i-thaction for t to the communicative goal by giving itan effect ?cg(P, ref(id(r1)), .
.
.
, ref(id(rn))) for eachelement P(r1, .
.
.
,rn) of the elementary tree?s seman-tic content.
We restrict ourselves to only expressingtrue statements by giving the action a preconditionskb(P, ref(id(r1)), .
.
.
, ref(id(rn))) for each elementof the semantic content.In order to keep track of the connection betweennode identities and individuals for future reference,each action gets an effect referent(id(r), ref(id(r)))for each semantic role r except self.
We enforce theconnection between u and x1 by adding a precondi-tion referent(u,x1).In the example, the most interesting action in thisrespect is the one for the elementary tree for ?likes?.This action looks as follows:Action S-likes-1(u,x1,x2,x3).Precond: subst(S,u),step(1), referent(u,x1),skb(like,x1,x2,x3)Effect: ?subst(S,u),subst(NP,1.ag),subst(NP,1.pat),?step(1),step(2),referent(1.ag,x2), referent(1.pat,x3),?cg(like,x1,x2,x3)We can run a planner and interpret the plan asabove; the main difference is that complete plans notonly correspond to grammatical derivation trees, butalso express all communicative goals.
Notice thatthis encoding models some aspects of lexical choice:The semantic content sets of the elementary treesneed not be singletons, and so there may be multipleways of partitioning the communicative goal into thecontent sets of various elementary trees.3.2 Referring expressionsFinally, we extend the system to deal with the gen-eration of referring expressions.
While this prob-lem is typically taken to require the generation of anoun phrase that refers uniquely to some individual,we don?t need to make any assumptions about thesyntactic category here.
Moreover, we consider theproblem in the wider context of generating referringexpressions within a sentence, which can allow us togenerate more succinct expressions.Because a referring expression must allow thehearer to identify the intended referent uniquely,we keep track of the hearer?s knowledge base sep-arately.
We use atoms hkb(P,a1, .
.
.
,an), as withskb above.
In addition, we assume pragmaticinformation of the form pkb(P,a1, .
.
.
,an).
Thethree pragmatic predicates that we will use here arehearer-new, indicating that the hearer does not knowabout the existence of an individual and can?t infer it(Stone et al, 2003), hearer-old for the opposite, andcontextset.
The context set of an intended referent isthe set of all individuals that the hearer might possi-bly confuse it with (DeVault et al, 2004).
It is emptyfor hearer-new individuals.
To say that b is in a?scontext set, we put the atom pkb(contextset,a,b)into the initial state.In addition to the semantic content, we equip ev-ery elementary tree in the grammar with a seman-tic requirement and a pragmatic condition (Stoneet al, 2003).
The semantic requirement is a set ofatoms spelling out presuppositions of an elementarytree that can help the hearer identify what its argu-ments refer to.
For instance, ?likes?
has the selec-tional restriction that its agent must be animate; thusthe hearer will not consider inanimate individuals asdistractors for the referring expression in agent posi-tion.
The pragmatic condition is a set of atoms overthe predicates in the pragmatic knowledge base.In our setting, every substitution node that is in-troduced during the derivation introduces a new re-ferring expression.
This means that we can dis-tinguish the referring expressions by the identityof the substitution node that introduced them.
Foreach referring expression u (where u is a node iden-tity), we keep track of the distractors in atomsof the form distractor(u,x).
The presence of anatom distractor(u,a) in some planning state repre-sents the fact that the current derivation tree is notyet informative enough to allow the hearer to iden-tify the intended referent for u uniquely; a is an-other individual that is not the intended referent,340but consistent with the partial referring expressionwe have constructed so far.
We enforce uniquenessof all referring expressions by adding the conjunct?u,x?distractor(u,x) to the planning goal.Now whenever an action introduces a new substi-tution node u, it will also introduce some distractoratoms to record the initial distractors for the refer-ring expression at u.
An individual a is in the initialdistractor set for the substitution node with role rif (a) it is not the intended referent, (b) it is in thecontext set of the intended referent, and (c) thereis a choice of individuals for the other parametersof the action that satisfies the semantic requirementtogether with a.
This is expressed by adding thefollowing effect for each substitution node; the con-junction is over the elements P(r1, .
.
.
,rn) of the se-mantic requirement, and there is one universal quan-tifier for y and for each parameter x j of the actionexcept for ref(id(r)).
?y,x1, .
.
.
,xn(y 6= ref(id(r))?pkb(contextset, ref(id(r)),y)?Vhkb(P, ref(id(r1)), .
.
.
, ref(id(rn)))[y/ref(id(r))])?
distractor(id(r),y)On the other hand, a distractor a for a referring ex-pression introduced at u is removed when we substi-tute or adjoin an elementary tree into u which rulesa out.
For instance, the elementary tree for ?rabbit?will remove all non-rabbits from the distractor set ofthe substitution node into which it is substituted.
Weachieve this by adding the following effect to eachaction; here the conjunction is over all elements ofthe semantic content.?y.
(?Vhkb(P, ref(id(r1)), .
.
.
, ref(id(rn))))[y/x1]?
?distractor(u,y),Finally, each action gets its pragmatic conditionas a precondition.3.3 The exampleBy way of example, Fig.
5 shows the full versionsof the actions from Fig.
2, for the extended gram-mar in Fig.
4.
Let?s say that the hearer knowsabout two rabbits r (which is white) and r?
(whichis not), about a person m with the name Mary, andabout an event e, and that the context set of r is{r,r?,m,e}.
Let?s also say that our communicativegoal is {like(e,m,r)}.
In this case, the first actioninstance in Fig.
3, S-likes-1(1.self,e,m,r), intro-duces a substitution node with identity 1.pat.
TheS:selfNP:ag ?VP:selfV:selflikesNP:selfthe NP:self *NP:selfa NP:self *NP:selfPN:selfMaryN:selfrabbitN:selfwhite N:self *semcon: {like(self,ag,pat)}semreq: {animate(ag)}semcon: { }semreq: { }pragcon: {hearer-old(self)}semcon: { }semreq: { }pragcon: {hearer-new(self)}semcon: {white(self)}semcon: {name(self, mary)}semcon: {rabbit(self)}NP:pat ?adjoin!NP:selfFigure 4: The extended example grammar.initial distractor set of this node is {r?,m} ?
the setof all individuals in r?s context set except for inan-imate objects (which violate the semantic require-ment) and r itself.
The NP-rabbit-3 action removesm from the distractor set, but at the end of the plan inFig.
3, r?
is still a distractor, i.e.
we have not reacheda goal state.
We can complete the plan by perform-ing a final action NP-white-5(1.pat,r), which willremove this distractor and achieve the planning goal.We can still reconstruct a derivation tree from thecomplete plan literally as described in Section 2.Now let?s say that the hearer did not know aboutthe existence of the individual r before the utterancewe are generating.
We model this by marking r ashearer-new in the pragmatic knowledge base and as-signing it an empty context set.
In this case, the re-ferring expression 1.pat would be initialized with anempty distractor set.
This entitles us to use the actionNP-a-4 and generate the four-step plan correspond-ing to the sentence ?Mary likes a rabbit.
?4 Discussion and future workIn conclusion, let?s look in more detail at computa-tional issues and the role of mutually constrainingreferring expressions.341Action S-likes-1(u,x1,x2,x3).Precond: referent(u,x1),skb(like,x1,x2,x3),subst(S,u),step(1)Effect: ?cg(like,x1,x2,x3),?subst(S,u),?step(1),step(2),subst(NP,1.ag),subst(NP,1.pat),?y.
?hkb(like,y,x2,x3) ?
?distractor(u,y),?y,x1,x3.x2 6= y?pkb(contextset,x2,y)?animate(y) ?
distractor(1.ag,y),?y,x1,x2.x3 6= y?pkb(contextset,x3,y) ?
distractor(1.pat,y)Action NP-Mary-2(u,x1).Precond: referent(u,x1),skb(name,x1,mary),subst(NP,u),step(2)Effect: ?cg(name,x1,mary),?subst(NP,u),?step(2),step(3),?y.
?hkb(name,y,mary) ?
?distractor(u,y)Action NP-rabbit-3(u,x1).Precond: referent(u,x1),skb(rabbit,x1),subst(N,u),step(3)Effect: ?cg(rabbit,x1),?subst(N,u),?step(3),step(4),canadjoin(NP,u),mustadjoin(NP,u),?y.
?hkb(rabbit,y) ?
?distractor(u,y)Action NP-the-4(u,x1).Precond: referent(u,x1),canadjoin(NP,u),step(4),pkb(hearer-old,x1)Effect: ?mustadjoin(NP,u),?step(4),step(5)Action NP-a-4(u,x1).Precond: referent(u,x1),canadjoin(NP,u),step(4),pkb(hearer-new,x1)Effect: ?mustadjoin(NP,u),?step(4),step(5)Action NP-white-5(u,x1).Precond: referent(u,x1),skb(white,x1),canadjoin(NP,u),step(5)Effect: ?cg(white,x1),?mustadjoin(NP,u),?step(5),step(6),?y.
?hkb(white,y) ?
?distractor(u,y)Figure 5: Some of the actions corresponding to the grammar in Fig.
4.4.1 Computational issuesWe lack the space to present the formal definitionof the sentence generation problem we encode intoPDDL.
However, this problem is NP-complete, byreduction of Hamiltonian Cycle ?
unsurprisingly,given that it encompasses realization, and the verysimilar realization problem in Koller and Striegnitz(2002) is NP-hard.
So any algorithm for our prob-lem must be prepared for exponential runtimes.We have implemented the translation described inthis paper and experimented with a number of differ-ent grammars, knowledge bases, and planners.
TheFF planner (Hoffmann and Nebel, 2001) can com-pute the plans in Section 3.3 in under 100 ms us-ing the grammar in Fig.
4.
If we add 10 more lex-icon entries to the grammar, the runtime grows to190 ms; and for 20 more entries, to 360 ms. Theruntime also grows with the plan length: It takes410 ms to generate a sentence ?Mary likes the Adj.
.
.
Adj rabbit?
with four adjectives and 890 ms forsix adjectives, corresponding to a plan length of 10.We compared these results against a planning-basedreimplementation of SPUD?s greedy search heuris-tic (Stone et al, 2003).
This system is faster than FFfor small inputs (360 ms for four adjectives), but be-comes slower as inputs grow larger (1000 ms for sixadjectives); but notice that while FF is also a heuris-tic planner, it is guaranteed to find a solution if oneexists, unlike SPUD.Planners have made tremendous progress in effi-ciency in the past decade, and by encoding sentencegeneration as a planning problem, we are set to profitfrom any future improvements; it is an advantageof the planning approach that we can compare verydifferent search strategies like FF?s and SPUD?s inthe same framework.
However, our PDDL problemsare challenging for modern planners because mostplanners start by computing all instances of atomsand actions.
In our experiments, FF generally spentonly about 10% of the runtime on search and therest on computing the instances; that is, there is a lotof room for optimization.
For larger grammars andknowledge bases, the number of instances can easilygrow into the billions.
In future work, we will there-fore collaborate with experts on planning systems tocompute action instances only by need.4.2 Referring expressionsIn our analysis of referring expressions, the tree tthat introduces the new substitution nodes typicallyinitializes the distractor sets with proper subsets ofthe entire domain.
This allows us to generate suc-cinct descriptions by encoding t?s presuppositionsas semantic requirements, and localizes the inter-actions between the referring expressions generatedfor different substitution nodes within t?s action.342However, an important detail in the encoding ofreferring expressions above is that an individual acounts as a distractor for the role r if there is anytuple of values that satisfies the semantic require-ment and has a in the r-component.
This is correct,but can sometimes lead to overly complicated refer-ring expressions.
An example is the construction ?Xtakes Y from Z?, which presupposes that Y is in Z.In a scenario that involves multiple rabbits, multiplehats, and multiple individuals that are inside otherindividuals, but only one pair of a rabbit r inside ahat h, the expression ?X takes the rabbit from thehat?
is sufficient to refer uniquely to r and h (Stoneand Webber, 1998).
Our system would try to gen-erate an expression for Y that suffices by itself todistinguish r from all distractors, and similarly forZ.
We will explore this issue further in future work.5 ConclusionIn this paper, we have shown how sentence gener-ation with TAG grammars and semantic and prag-matic information can be encoded into PDDL.
Ourencoding is declarative in that it can be used withany correct planning algorithm, and explicit in thatthe actions capture the complete effect of a word onthe syntactic, semantic, and local pragmatic goals.In terms of expressive power, it captures the core ofSPUD, except for its inference capabilities.This work is practically relevant because it opensup the possibility of using efficient planners to makegenerators faster and more flexible.
Conversely, ourPDDL problems are a challenge for current plan-ners and open up NLG as an application domain thatplanning research itself can target.Theoretically, our encoding provides a newframework for understanding and exploring the gen-eral relationships between language and action.
Itsuggests new ways of going beyond SPUD?s expres-sive power, to formulate utterances that describe anddisambiguate concurrent real-world actions or ex-ploit the dynamics of linguistic context within andacross sentences.Acknowledgments.
This work was funded by a DFG re-search fellowship and the NSF grants HLC 0308121, IGERT0549115, and HSD 0624191.
We are indebted to Henry Kautzfor his advice on planning systems, and to Owen Rambow, Bon-nie Webber, and the anonymous reviewers for feedback.ReferencesD.
Appelt.
1985.
Planning English Sentences.
Cam-bridge University Press, Cambridge England.A.
Blum and M. Furst.
1997.
Fast planning throughgraph analysis.
Artificial Intelligence, 90:281?300.P.
R. Cohen and C. R. Perrault.
1979.
Elements of aplan-based theory of speech acts.
Cognitive Science,3(3):177?212.R.
Dale and E. Reiter.
1995.
Computational interpreta-tions of the Gricean maxims in the generation of refer-ring expressions.
Cognitive Science, 19.D.
DeVault, C. Rich, and C. Sidner.
2004.
Natural lan-guage generation and discourse context: Computingdistractor sets from the focus stack.
In Proc.
FLAIRS.R.
Fikes and N. Nilsson.
1971.
STRIPS: A new approachin the application of theorem proving to problem solv-ing.
Artificial Intelligence, 2:189?208.P.
Heeman and G. Hirst.
1995.
Collaborating onreferring expressions.
Computational Linguistics,21(3):351?382.J.
Hobbs, M. Stickel, D. Appelt, and P. Martin.
1993.Interpretation as abduction.
Artificial Intelligence,63:69?142.J.
Hoffmann and B. Nebel.
2001.
The FF planningsystem: Fast plan generation through heuristic search.Journal of Artificial Intelligence Research, 14.A.
Joshi and Y. Schabes.
1997.
Tree-Adjoining Gram-mars.
In G. Rozenberg and A. Salomaa, editors,Handbook of Formal Languages, chapter 2, pages 69?123.
Springer-Verlag, Berlin.H.
Kautz and B. Selman.
1998.
Blackbox: A new ap-proach to the application of theorem proving to prob-lem solving.
In Workshop Planning as CombinatorialSearch, AIPS-98.A.
Koller and K. Striegnitz.
2002.
Generation as depen-dency parsing.
In Proc.
40th ACL, Philadelphia.D.
V. McDermott.
2000.
The 1998 AI Planning SystemsCompetition.
AI Magazine, 21(2):35?55.M.
Stone and B. Webber.
1998.
Textual economythrough close coupling of syntax and semantics.
InProc.
INLG.M.
Stone, C. Doran, B.Webber, T. Bleam, andM.
Palmer.2003.
Microplanning with communicative inten-tions: The SPUD system.
Computational Intelligence,19(4):311?381.R.
Thomason and J. Hobbs.
1997.
Interrelating interpre-tation and generation in an abductive framework.
InAAAI Fall Symposium on Communicative Action.343
