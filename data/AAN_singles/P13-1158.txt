Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1608?1618,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsParaphrase-Driven Learning for Open Question AnsweringAnthony Fader Luke Zettlemoyer Oren EtzioniComputer Science & EngineeringUniversity of WashingtonSeattle, WA 98195{afader, lsz, etzioni}@cs.washington.eduAbstractWe study question answering as a ma-chine learning problem, and induce a func-tion that maps open-domain questions toqueries over a database of web extrac-tions.
Given a large, community-authored,question-paraphrase corpus, we demon-strate that it is possible to learn a se-mantic lexicon and linear ranking func-tion without manually annotating ques-tions.
Our approach automatically gener-alizes a seed lexicon and includes a scal-able, parallelized perceptron parameter es-timation scheme.
Experiments show thatour approach more than quadruples the re-call of the seed lexicon, with only an 8%loss in precision.1 IntroductionOpen-domain question answering (QA) is a long-standing, unsolved problem.
The central challengeis to automate every step of QA system construc-tion, including gathering large databases and an-swering questions against these databases.
Whilethere has been significant work on large-scale in-formation extraction (IE) from unstructured text(Banko et al, 2007; Hoffmann et al, 2010; Riedelet al, 2010), the problem of answering questionswith the noisy knowledge bases that IE systemsproduce has received less attention.
In this paper,we present an approach for learning to map ques-tions to formal queries over a large, open-domaindatabase of extracted facts (Fader et al, 2011).Our system learns from a large, noisy, question-paraphrase corpus, where question clusters havea common but unknown query, and can spana diverse set of topics.
Table 1 shows exam-ple paraphrase clusters for a set of factual ques-tions.
Such data provides strong signal for learn-ing about lexical variation, but there are a numberWho wrote the Winnie the Pooh books?Who is the author of winnie the pooh?What was the name of the authur of winnie the pooh?Who wrote the series of books for Winnie the poo?Who wrote the children?s storybook ?Winnie the Pooh?
?Who is poohs creator?What relieves a hangover?What is the best cure for a hangover?The best way to recover from a hangover?Best remedy for a hangover?What takes away a hangover?How do you lose a hangover?What helps hangover symptoms?What are social networking sites used for?Why do people use social networking sites worldwide?Advantages of using social network sites?Why do people use social networks a lot?Why do people communicate on social networking sites?What are the pros and cons of social networking sites?How do you say Santa Claus in Sweden?Say santa clause in sweden?How do you say santa clause in swedish?How do they say santa in Sweden?In Sweden what is santa called?Who is sweden santa?Table 1: Examples of paraphrase clusters from theWikiAnswers corpus.
Within each cluster, there isa wide range of syntactic and lexical variations.of challenges.
Given that the data is community-authored, it will inevitably be incomplete, containincorrectly tagged paraphrases, non-factual ques-tions, and other sources of noise.Our core contribution is a new learning ap-proach that scalably sifts through this para-phrase noise, learning to answer a broad classof factual questions.
We focus on answer-ing open-domain questions that can be answeredwith single-relation queries, e.g.
all of the para-phrases of ?Who wrote Winnie the Pooh??
and?What cures a hangover??
in Table 1.
Thealgorithm answers such questions by mappingthem to executable queries over a tuple storecontaining relations such as authored(milne,winnie-the-pooh) and treat(bloody-mary,hangover-symptoms).1608The approach automatically induces lexicalstructures, which are combined to build queries forunseen questions.
It learns lexical equivalences forrelations (e.g., wrote, authored, and creator), en-tities (e.g., Winnie the Pooh or Pooh Bear), andquestion templates (e.g., Who r the e books?
andWho is the r of e?).
Crucially, the approachdoes not require any explicit labeling of the ques-tions in our paraphrase corpus.
Instead, we use16 seed question templates and string-matching tofind high-quality queries for a small subset of thequestions.
The algorithm uses learned word align-ments to aggressively generalize the seeds, pro-ducing a large set of possible lexical equivalences.We then learn a linear ranking model to filter thelearned lexical equivalences, keeping only thosethat are likely to answer questions well in practice.Experimental results on 18 million paraphrasepairs gathered from WikiAnswers1 demonstratethe effectiveness of the overall approach.
Weperformed an end-to-end evaluation against adatabase of 15 million facts automatically ex-tracted from general web text (Fader et al, 2011).On known-answerable questions, the approachachieved 42% recall, with 77% precision, morethan quadrupling the recall over a baseline system.In sum, we make the following contributions:?
We introduce PARALEX, an end-to-end open-domain question answering system.?
We describe scalable learning algorithms thatinduce general question templates and lexicalvariants of entities and relations.
These algo-rithms require no manual annotation and canbe applied to large, noisy databases of rela-tional triples.?
We evaluate PARALEX on the end-task of an-swering questions from WikiAnswers using adatabase of web extractions, and show that itoutperforms baseline systems.?
We release our learned lexicon andquestion-paraphrase dataset to theresearch community, available athttp://openie.cs.washington.edu.2 Related WorkOur work builds upon two major threads of re-search in natural language processing: informa-tion extraction (IE), and natural language inter-faces to databases (NLIDB).1http://wiki.answers.com/Research in IE has been moving towards thegoal of extracting facts from large text corpora,across many domains, with minimal supervision(Mintz et al, 2009; Hoffmann et al, 2010; Riedelet al, 2010; Hoffmann et al, 2011; Banko et al,2007; Yao et al, 2012).
While much progresshas been made in converting text into structuredknowledge, there has been little work on an-swering natural language questions over thesedatabases.
There has been some work on QA overweb text (Kwok et al, 2001; Brill et al, 2002), butthese systems do not operate over extracted rela-tional data.The NLIDB problem has been studied fordecades (Grosz et al, 1987; Katz, 1997).
Morerecently, researchers have created systems thatuse machine learning techniques to automaticallyconstruct question answering systems from data(Zelle and Mooney, 1996; Popescu et al, 2004;Zettlemoyer and Collins, 2005; Clarke et al, 2010;Liang et al, 2011).
These systems have the abil-ity to handle questions with complex semanticson small domain-specific databases like GeoQuery(Tang and Mooney, 2001) or subsets of Freebase(Cai and Yates, 2013), but have yet to scale to thetask of general, open-domain question answering.In contrast, our system answers questions withmore limited semantics, but does so at a very largescale in an open-domain manner.
Some work hasbeen made towards more general databases likeDBpedia (Yahya et al, 2012; Unger et al, 2012),but these systems rely on hand-written templatesfor question interpretation.The learning algorithms presented in this pa-per are similar to algorithms used for paraphraseextraction from sentence-aligned corpora (Barzi-lay and McKeown, 2001; Barzilay and Lee, 2003;Quirk et al, 2004; Bannard and Callison-Burch,2005; Callison-Burch, 2008; Marton et al, 2009).However, we use a paraphrase corpus for extract-ing lexical items relating natural language patternsto database concepts, as opposed to relationshipsbetween pairs of natural language utterances.3 Overview of the ApproachIn this section, we give a high-level overview ofthe rest of the paper.Problem Our goal is to learn a function that willmap a natural language question x to a query zover a database D. The database D is a collectionof assertions in the form r(e1, e2) where r is a bi-1609nary relation from a vocabulary R, and e1 and e2are entities from a vocabulary E. We assume thatthe elements of R and E are human-interpretablestrings like population or new-york.
In ourexperiments, R and E contain millions of en-tries representing ambiguous and overlapping con-cepts.
The database is equipped with a simple in-terface that accepts queries in the form r(?, e2) orr(e1, ?).
When executed, these queries return allentities e that satisfy the given relationship.
Thus,our task is to find the query z that best captures thesemantics of the question x.Model The question answering model includes alexicon and a linear ranking function.
The lexiconL associates natural language patterns to databaseconcepts, thereby defining the space of queriesthat can be derived from the input question (seeTable 2).
Lexical entries can pair strings withdatabase entities (nyc and new-york), strings withdatabase relations (big and population), or ques-tion patterns with templated database queries (howr is e?
and r(?,e)).
We describe this model inmore detail in Section 4.Learning The learning algorithm induces a lex-icon L and estimates the parameters ?
of thelinear ranking function.
We learn L by boot-strapping from an initial seed lexicon L0 over acorpus of question paraphrases C = {(x, x?)
:x?
is a paraphrase of x}, like the examples in Ta-ble 1.
We estimate ?
by using the initial lexicon toautomatically label queries in the paraphrase cor-pus, as described in Section 5.2.
The final resultis a scalable learning algorithm that requires nomanual annotation of questions.Evaluation In Section 8, we evaluate our systemagainst various baselines on the end-task of ques-tion answering against a large database of factsextracted from the web.
We use held-out known-answerable questions from WikiAnswers as a testset.4 Question Answering ModelTo answer questions, we must find the best queryfor a given natural language question.4.1 Lexicon and DerivationsTo define the space of possible queries, PARALEXuses a lexicon L that encodes mappings from nat-ural language to database concepts (entities, rela-tions, and queries).
Each entry in L is a pair (p, d)Entry Type NL Pattern DB ConceptEntity nyc new-yorkRelation big populationQuestion (1-Arg.)
how big is e population(?, e)Question (2-Arg.)
how r is e r(?, e)Table 2: Example lexical entries.where p is a pattern and d is an associated databaseconcept.
Table 2 gives examples of the entry typesin L: entity, relation, and question patterns.Entity patterns match a contiguous string ofwords and are associated with some database en-tity e ?
E.Relation patterns match a contiguous string ofwords and are associated with a relation r ?
R andan argument ordering (e.g.
the string child couldbe modeled as either parent-of or child-of withopposite argument ordering).Question patterns match an entire questionstring, with gaps that recursively match an en-tity or relation patterns.
Question patterns are as-sociated with a templated database query, wherethe values of the variables are determined by thematched entity and relation patterns.
A questionpattern may be 1-Argument, with a variable foran entity pattern, or 2-Argument, with variablesfor an entity pattern and a relation pattern.
A 2-argument question pattern may also invert the ar-gument order of the matched relation pattern, e.g.who r e?
may have the opposite argument orderof who did e r?The lexicon is used to generate a derivation yfrom an input question x to a database query z.For example, the entries in Table 2 can be usedto make the following derivation from the ques-tion How big is nyc?
to the query population(?,new-york):This derivation proceeds in two steps: first match-ing a question form like How r is e?
and thenmapping big to population and nyc to new-york.Factoring the derivation this way allows the lexi-cal entries for big and nyc to be reused in semanti-1610cally equivalent variants like nyc how big is it?
orapproximately how big is nyc?
This factorizationhelps the system generalize to novel questions thatdo not appear in the training set.We model a derivation as a set of (pi, di) pairs,where each pi matches a substring of x, the sub-strings cover all words in x, and the database con-cepts di compose to form z. Derivations are rootedat either a 1-argument or 2-argument question en-try and have entity or relation entries as leaves.4.2 Linear Ranking FunctionIn general, multiple queries may be derived from asingle input question x using a lexicon L. Many ofthese derivations may be incorrect due to noise inL.
Given a question x, we consider all derivationsy and score them with ?
??
(x, y), where ?
(x, y) isa n-dimensional feature representation and ?
is an-dimensional parameter vector.
Let GEN(x;L)be the set of all derivations y that can be generatedfrom x using L. The best derivation y?
(x) accord-ing to the model (?, L) is given by:y?
(x) = argmaxy?GEN(x;L)?
?
?
(x, y)The best query z?
(x) can be computed directlyfrom the derivation y?
(x).Computing the set GEN(x;L) involves findingall 1-Argument and 2-Argument question patternsthat match x, and then enumerating all possibledatabase concepts that match entity and relationstrings.
When the database and lexicon are large,this becomes intractable.
We prune GEN(x;L)using the model parameters ?
by only consideringthe N -best question patterns that match x, beforeadditionally enumerating any relations or entities.For the end-to-end QA task, we return a rankedlist of answers from the k highest scoring queries.We score an answer a with the highest score of allderivations that generate a query with answer a.5 LearningPARALEX uses a two-part learning algorithm; itfirst induces an overly general lexicon (Section5.1) and then learns to score derivations to increaseaccuracy (Section 5.2).
Both algorithms rely on aninitial seed lexicon, which we describe in Section7.4.5.1 Lexical LearningThe lexical learning algorithm constructs a lexi-con L from a corpus of question paraphrases C ={(x, x?)
: x?
is a paraphrase of x}, where we as-sume that all paraphrased questions (x, x?)
can beanswered with a single, initially unknown, query(Table 1 shows example paraphrases).
This as-sumption allows the algorithm to generalize fromthe initial seed lexicon L0, greatly increasing thelexical coverage.As an example, consider the paraphrase pair x= What is the population of New York?
and x?
=How big is NYC?
Suppose x can be mapped to aquery under L0 using the following derivation y:what is the r of e = r(?, e)population = populationnew york = new-yorkWe can induce new lexical items by aligning thepatterns used in y to substrings in x?.
For example,suppose we know that the words in (x, x?)
align inthe following way:Using this information, we can hypothesize thathow r is e, big, and nyc should have the same in-terpretations as what is the r of e, population, andnew york, respectively, and create the new entries:how r is e = r(?, e)big = populationnyc = new-yorkWe call this procedure InduceLex(x, x?, y, A),which takes a paraphrase pair (x, x?
), a derivationy of x, and a word alignment A, and returns a newset of lexical entries.
Before formally describingInduceLex we need to introduce some definitions.Let n and n?
be the number of words in x andx?.
Let [k] denote the set of integers {1, .
.
.
, k}.A word alignment A between x and x?
is a subsetof [n] ?
[n?].
A phrase alignment is a pair of in-dex sets (I, I ?)
where I ?
[n] and I ?
?
[n?].
Aphrase alignment (I, I ?)
is consistent with a wordalignment A if for all (i, i?)
?
A, i ?
I if and onlyif i?
?
I ?.
In other words, a phrase alignment isconsistent with a word alignment if the words inthe phrases are aligned only with each other, andnot with any outside words.We will now define InduceLex(x, x?, y, A) forthe case where the derivation y consists of a 2-argument question entry (pq, dq), a relation entry1611function LEARNLEXICONInputs:- A corpus C of paraphrases (x, x?).
(Table 1)- An initial lexicon L0 of (pattern, concept) pairs.- A word alignment function WordAlign(x, x?).
(Section 6)- Initial parameters ?0.- A function GEN(x;L) that derives queries froma question x using lexicon L. (Section 4)- A function InduceLex(x, x?, y, A) that inducesnew lexical items from the paraphrases (x, x?)
us-ing their word alignment A and a derivation y ofx.
(Section 5.1)Output: A learned lexicon L.L = {}for all x, x?
?
C doif GEN(x;L0) is not empty thenA?WordAlign(x, x?)y?
?
argmaxy?GEN(x;L0) ?0 ?
?
(x, y)L?
L ?
InduceLex(x, x?, y?, A)return LFigure 1: Our lexicon learning algorithm.
(pr, dr), and an entity entry (pe, de), as shown inthe example above.2 InduceLex returns the set ofall triples (p?q, dq), (p?r, dr), (p?e, de) such that forall p?q, p?r, p?e such that1.
p?q, p?r, p?e are a partition of the words in x?.2.
The phrase pairs (pq, p?q), (pr, p?r), (pe, p?e)are consistent with the word alignment A.3.
The p?r and p?e are contiguous spans of wordsin x?.Figure 1 shows the complete lexical learning al-gorithm.
In practice, for a given paraphrase pair(x, x?)
and alignment A, InduceLex will gener-ate multiple sets of new lexical entries, resultingin a lexicon with millions of entries.
We use anexisting statistical word alignment algorithm forWordAlign (see Section 6).
In the next section,we will introduce a scalable approach for learningto score derivations to filter out lexical items thatgeneralize poorly.5.2 Parameter LearningParameter learning is necessary for filtering outderivations that use incorrect lexical entries likenew mexico = mexico, which arise from noise inthe paraphrases and noise in the word alignment.2InduceLex has similar behavior for the other type ofderivation, which consists of a 1-argument question entry(pq, dq) and an entity (pe, de).We use the hidden variable structured perceptronalgorithm to learn ?
from a list of (question x,query z) training examples.
We adopt the itera-tive parameter mixing variation of the perceptron(McDonald et al, 2010) to scale to a large numberof training examples.Figure 2 shows the parameter learning algo-rithm.
The parameter learning algorithm operatesin two stages.
First, we use the initial lexiconL0 to automatically generate (question x, query z)training examples from the paraphrase corpus C.Then we feed the training examples into the learn-ing algorithm, which estimates parameters for thelearned lexicon L.Because the number of training examples islarge, we adopt a parallel perceptron approach.We first randomly partition the training data Tinto K equally-sized subsets T1, .
.
.
, TK .
We thenperform perceptron learning on each partition inparallel.
Finally, the learned weights from eachparallel run are aggregated by taking a uniformlyweighted average of each partition?s parametervector.
This procedure is repeated for T iterations.The training data consists of (question x, queryz) pairs, but our scoring model is over (questionx, derivation y) pairs, which are unobserved inthe training data.
We use a hidden variable ver-sion of the perceptron algorithm (Collins, 2002),where the model parameters are updated using thehighest scoring derivation y?
that will generate thecorrect query z using the learned lexicon L.6 DataFor our database D, we use the publicly avail-able set of 15 million REVERB extractions (Faderet al, 2011).3 The database consists of a setof triples r(e1, e2) over a vocabulary of ap-proximately 600K relations and 2M entities, ex-tracted from the ClueWeb09 corpus.4 The RE-VERB database contains a large cross-section ofgeneral world-knowledge, and thus is a goodtestbed for developing an open-domain QA sys-tem.
However, the extractions are noisy, unnor-malized (e.g., the strings obama, barack-obama,and president-obama all appear as distinct en-tities), and ambiguous (e.g., the relation born-incontains facts about both dates and locations).3We used version 1.1, downloaded from http://reverb.cs.washington.edu/.4The full set of REVERB extractions from ClueWeb09contains over six billion triples.
We used the smaller subsetof triples to simplify our experiments.1612function LEARNPARAMETERSInputs:- A corpus C of paraphrases (x, x?).
(Table 1)- An initial lexicon L0 of (pattern, db concept)pairs.- A learned lexiconL of (pattern, db concept) pairs.- Initial parameters ?0.- Number of perceptron epochs T .- Number of training-data shards K.- A function GEN(x;L) that derives queries froma question x using lexicon L. (Section 4)- A function PerceptronEpoch(T , ?, L) that runsa single epoch of the hidden-variable structuredperceptron algorithm on training set T with initialparameters ?, returning a new parameter vector??.
(Section 5.2)Output: A learned parameter vector ?.// Step 1: Generate Training Examples TT = {}for all x, x?
?
C doif GEN(x;L0) is not empty theny?
?
argmaxy?GEN(x;L0) ?0 ?
?
(x, y)z?
?
query of y?Add (x?, z?)
to T// Step 2: Learn Parameters from TRandomly partition T into shards T1, .
.
.
, TKfor t = 1 .
.
.
T do// Executed on k processors?k,t = PerceptronEpoch(Tk, ?t?1, L)// Average the weights?t = 1K?k ?k,treturn ?TFigure 2: Our parameter learning algorithm.Our paraphrase corpus C was constructed fromthe collaboratively edited QA site WikiAnswers.WikiAnswers users can tag pairs of questions asalternate wordings of each other.
We harvesteda set of 18M of these question-paraphrase pairs,with 2.4M distinct questions in the corpus.To estimate the precision of the paraphrase cor-pus, we randomly sampled a set of 100 pairs andmanually tagged them as ?paraphrase?
or ?not-paraphrase.?
We found that 55% of the sampledpairs are valid paraphrased.
Most of the incorrectparaphrases were questions that were related, butnot paraphrased e.g.
How big is the biggest mall?and Most expensive mall in the world?We word-aligned each paraphrase pair usingthe MGIZA++ implementation of IBM Model 4(Och and Ney, 2000; Gao and Vogel, 2008).
Theword-alignment algorithm was run in each direc-tion (x, x?)
and (x?, x) and then combined usingthe grow-diag-final-and heuristic (Koehn et al,2003).7 Experimental SetupWe compare the following systems:?
PARALEX: the full system, using the lexicallearning and parameter learning algorithmsfrom Section 5.?
NoParam: PARALEX without the learnedparameters.?
InitOnly: PARALEX using only the initialseed lexicon.We evaluate the systems?
performance on the end-task of QA on WikiAnswers questions.7.1 Test SetA major challenge for evaluation is that the RE-VERB database is incomplete.
A system may cor-rectly map a test question to a valid query, onlyto return 0 results when executed against the in-complete database.
We factor out this source oferror by semi-automatically constructing a sampleof questions that are known to be answerable us-ing the REVERB database, and thus allows for ameaningful comparison on the task of question un-derstanding.To create the evaluation set, we identified ques-tions x in a held out portion of the WikiAnswerscorpus such that (1) x can be mapped to somequery z using an initial lexicon (described in Sec-tion 7.4), and (2) when z is executed against thedatabase, it returns at least one answer.
We thenadd x and all of its paraphrases as our evaluationset.
For example, the question What is the lan-guage of Hong-Kong satisfies these requirements,so we added these questions to the evaluation set:What is the language of Hong-Kong?What language do people in hong kong use?How many languages are spoken in hong kong?How many languages hong kong people use?In Hong Kong what language is spoken?Language of Hong-kong?This methodology allows us to evaluate the sys-tems?
ability to handle syntactic and lexical varia-tions of questions that should have the same an-swers.
We created 37 question clusters, result-ing in a total of 698 questions.
We removed allof these questions and their paraphrases from thetraining set.
We also manually filtered out any in-correct paraphrases that appeared in the test clus-ters.We then created a gold-standard set of (x, a, l)triples, where x is a question, a is an answer, and l1613Question Pattern Database Querywho r e r(?, e)what r e r(?, e)who does e r r(e, ?
)what does e r r(e, ?
)what is the r of e r(?, e)who is the r of e r(?, e)what is r by e r(e, ?
)who is e?s r r(?, e)what is e?s r r(?, e)who is r by e r(e, ?
)when did e r r-in(e, ?
)when did e r r-on(e, ?
)when was e r r-in(e, ?
)when was e r r-on(e, ?
)where was e r r-in(e, ?
)where did e r r-in(e, ?
)Table 3: The question patterns used in the initiallexicon L0.is a label (correct or incorrect).
To create the gold-standard, we first ran each system on the evalua-tion questions to generate (x, a) pairs.
Then wemanually tagged each pair with a label l. Thisresulted in a set of approximately 2, 000 humanjudgments.
If (x, a) was tagged with label l and x?is a paraphrase of x, we automatically added thelabeling (x?, a, l), since questions in the same clus-ter should have the same answer sets.
This processresulted in a gold standard set of approximately48, 000 (x, a, l) triples.7.2 MetricsWe use two types of metrics to score the systems.The first metric measures the precision and recallof each system?s highest ranked answer.
Precisionis the fraction of predicted answers that are cor-rect and recall is the fraction of questions where acorrect answer was predicted.
The second metricmeasures the accuracy of the entire ranked answerset returned for a question.
We compute the meanaverage precision (MAP) of each systems?
output,which measures the average precision over all lev-els of recall.7.3 Features and SettingsThe feature representation ?
(x, y) consists of in-dicator functions for each lexical entry (p, d) ?
Lused in the derivation y.
For parameter learning,we use an initial weight vector ?0 = 0, use T = 20F1 Precision Recall MAPPARALEX 0.54 0.77 0.42 0.22NoParam 0.30 0.53 0.20 0.08InitOnly 0.18 0.84 0.10 0.04Table 4: Performance on WikiAnswers questionsknown to be answerable using REVERB.F1 Precision Recall MAPPARALEX 0.54 0.77 0.42 0.22No 2-Arg.
0.40 0.86 0.26 0.12No 1-Arg 0.35 0.81 0.22 0.11No Relations 0.18 0.84 0.10 0.03No Entity 0.36 0.55 0.27 0.15Table 5: Ablation of the learned lexical items.0.0 0.1 0.2 0.3 0.4 0.5Recall0.50.60.70.80.91.0PrecisionPARALEXNo 2-Arg.Initial LexiconFigure 3: Precision-recall curves for PARALEXwith and without 2-argument question patterns.iterations and shard the training data into K = 10pieces.
We limit each system to return the top 100database queries for each test sentence.
All inputwords are lowercased and lemmatized.7.4 Initial LexiconBoth the lexical learning and parameter learningalgorithms rely on an initial seed lexicon L0.
Theinitial lexicon allows the learning algorithms tobootstrap from the paraphrase corpus.We construct L0 from a set of 16 hand-written2-argument question patterns and the output of theidentity transformation on the entity and relationstrings in the database.
Table 3 shows the questionpatterns that were used in L0.8 ResultsTable 4 shows the performance of PARALEX onthe test questions.
PARALEX outperforms thebaseline systems in terms of both F1 and MAP.The lexicon-learning algorithm boosts the recallby a factor of 4 over the initial lexicon, show-ing the utility of the InduceLex algorithm.
The1614String Learned Database Relations for Stringget rid of treatment-for, cause, get-rid-of, cure-for, easiest-way-to-get-rid-ofword word-for, slang-term-for, definition-of, meaning-of, synonym-ofspeak speak-language-in, language-speak-in, principal-language-of, dialect-ofuseful main-use-of, purpose-of, importance-of, property-of, usefulness-ofString Learned Database Entities for Stringsmoking smoking, tobacco-smoking, cigarette, smoking-cigar, smoke, quit-smokingradiation radiation, electromagnetic-radiation, nuclear-radiationvancouver vancouver, vancouver-city, vancouver-island, vancouver-british-columbiaprotein protein, protein-synthesis, plasma-protein, monomer, dnaTable 6: Examples of relation and entity synonyms learned from the WikiAnswers paraphrase corpus.parameter-learning algorithm also results in alarge gain in both precision and recall: InduceLexgenerates a noisy set of patterns, so selecting thebest query for a question is more challenging.Table 5 shows an ablation of the different typesof lexical items learned by PARALEX.
For eachrow, we removed the learned lexical items fromeach of the types described in Section 4, keepingonly the initial seed lexical items.
The learned 2-argument question templates significantly increasethe recall of the system.
This increased recallcame at a cost, lowering precision from 0.86 to0.77.
Thresholding the query score allows us totrade precision for recall, as shown in Figure 3.Table 6 shows some examples of the learned en-tity and relation synonyms.The 2-argument question templates help PAR-ALEX generalize over different variations of thesame question, like the test questions shown inTable 7.
For each question, PARALEX combinesa 2-argument question template (shown below thequestions) with the rules celebrate = holiday-ofand christians = christians to derive a fullquery.
Factoring the problem this way allowsPARALEX to reuse the same rules in differentsyntactic configurations.
Note that the imperfecttraining data can lead to overly-specific templateslike what are the religious r of e, which can loweraccuracy.9 Error AnalysisTo understand how close we are to the goal ofopen-domain QA, we ran PARALEX on an unre-stricted sample of questions from WikiAnswers.We used the same methodology as described in theprevious section, where PARALEX returns the topanswer for each question using REVERB.We found that PARALEX performs significantlyworse on this dataset, with recall maxing out at ap-Celebrations for Christians?r for e?Celebrations of Christians?r of e?What are some celebrations for Christians?what are some r for e?What are some celebrations of the Christians?what are some r of e?What are some of Christians celebrations?what are some of e r?What celebrations do Christians do?what r do e do?What did Christians celebrate?what did e r?What are the religious celebrations of Christians?what are the religious r of e?What celebration do Christians celebrate?what r do e celebrate?Table 7: Questions from the test set with 2-argument question patterns that PARALEX used toderive a correct query.proximately 6% of the questions answered at pre-cision 0.4.
This is not surprising, since the testquestions are not restricted to topics covered bythe REVERB database, and may be too complex tobe answered by any database of relational triples.We performed an error analysis on a sampleof 100 questions that were either incorrectly an-swered or unanswered.
We examined the can-didate queries that PARALEX generated for eachquestion and tagged each query as correct (wouldreturn a valid answer given a correct and com-plete database) or incorrect.
Because the inputquestions are unrestricted, we also judged whetherthe questions could be faithfully represented as ar(?, e) or r(e, ?)
query over the database vocabu-lary.
Table 8 shows the distribution of errors.The largest source of error (36%) were on com-1615plex questions that could not be represented as aquery for various reasons.
We categorized thesequestions into groups.
The largest group (14%)were questions that need n-ary or higher-orderdatabase relations, for example How long doesit take to drive from Sacramento to Cancun?
orWhat do cats and dogs have in common?
Approx-imately 13% of the questions were how-to ques-tions like How do you make axes in minecraft?whose answers are a sequence of steps, insteadof a database entity.
Lastly, 9% of the questionsrequire database operators like joins, for exampleWhen were Bobby Orr?s children born?The second largest source of error (32%) werequestions that could be represented as a query, butwhere PARALEX was unable to derive any cor-rect queries.
For example, the question Thingsgrown on Nigerian farms?
was not mapped toany queries, even though the REVERB databasecontains the relation grown-in and the entitynigeria.
We found that 13% of the incorrectquestions were cases where the entity was not rec-ognized, 12% were cases where the relation wasnot recognized, and 6% were cases where both theentity and relation were not recognized.We found that 28% of the errors were caseswhere PARALEX derived a query that we judged tobe correct, but returned no answers when executedagainst the database.
For example, given the ques-tion How much can a dietician earn?
PARALEXderived the query salary-of(?, dietician) butthis returned no answers in the REVERB database.Finally, approximately 4% of the questions in-cluded typos or were judged to be inscrutable, forexample Barovier hiriacy of evidence based forpressure sore?Discussion Our experiments show that the learn-ing algorithms described in Section 5 allow PAR-ALEX to generalize beyond an initial lexicon andanswer questions with significantly higher accu-racy.
Our error analysis on an unrestricted set ofWikiAnswers questions shows that PARALEX isstill far from the goal of truly high-recall, open-domain QA.
We found that many questions askedon WikiAnswers are either too complex to bemapped to a simple relational query, or are notcovered by the REVERB database.
Further, ap-proximately one third of the missing recall is dueto entity and relation recognition errors.Incorrectly Answered/Unanswered Questions36% Complex QuestionsNeed n-ary or higher-order relations (14%)Answer is a set of instructions (13%)Need database operators e.g.
joins (9%)32% Entity or Relation Recognition ErrorsEntity recognition errors (13%)Relation recognition errors (12%)Entity & relation recognition errors (7%)28% Incomplete DatabaseDerived a correct query, but no answers4% Typos/Inscrutable QuestionsTable 8: Error distribution of PARALEX on an un-restricted sample of questions from the WikiAn-swers dataset.10 ConclusionWe introduced a new learning approach that in-duces a complete question-answering system froma large corpus of noisy question-paraphrases.
Us-ing only a seed lexicon, the approach automat-ically learns a lexicon and linear ranking func-tion that demonstrated high accuracy on a held-outevaluation set.A number of open challenges remain.
First,precision could likely be improved by addingnew features to the ranking function.
Second,we would like to generalize the question under-standing framework to produce more complexqueries, constructed within a compositional se-mantic framework, but without sacrificing scala-bility.
Third, we would also like to extend thesystem with other large databases like Freebase orDBpedia.
Lastly, we believe that it would be pos-sible to leverage the user-provided answers fromWikiAnswers as a source of supervision.AcknowledgmentsThis research was supported in part by ONR grantN00014-11-1-0294, DARPA contract FA8750-09-C-0179, a gift from Google, a gift from VulcanInc., and carried out at the University of Washing-ton?s Turing Center.
We would like to thank YoavArtzi, Tom Kwiatkowski, Yuval Marton, Mausam,Dan Weld, and the anonymous reviewers for theirhelpful comments.1616ReferencesMichele Banko, Michael J. Cafarella, Stephen Soder-land, Matthew Broadhead, and Oren Etzioni.
2007.Open Information Extraction from the Web.
In Pro-ceedings of the 20th international joint conferenceon Artifical intelligence.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics.Regina Barzilay and Lillian Lee.
2003.
Learningto Paraphrase: An Unsupervised Approach UsingMultiple-Sequence Alignment.
In Proceedings ofthe 2003 Conference of the North American Chapterof the Association for Computational Linguistics.Regina Barzilay and Kathleen R. McKeown.
2001.Extracting Paraphrases from a Parallel Corpus.
InProceedings of the 39th Annual Meeting on Associ-ation for Computational Linguistics.Eric Brill, Susan Dumais, and Michele Banko.
2002.An Analysis of the AskMSR Question-AnsweringSystem.
In Proceedings of Empirical Methods inNatural Language Processing.Qingqing Cai and Alexander Yates.
2013.
Large-scaleSemantic Parsing via Schema Matching and LexiconExtension.
In Proceedings of the Annual Meeting ofthe Association for Computational Linguistics.Chris Callison-Burch.
2008.
Syntactic Constraintson Paraphrases Extracted from Parallel Corpora.
InProceedings of the 2008 Conference on EmpiricalMethods in Natural Language Processing.James Clarke, Dan Goldwasser, Ming-Wei Chang, andDan Roth.
2010.
Driving Semantic Parsing fromthe World?s Response.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning.Michael Collins.
2002.
Discriminative Training Meth-ods for Hidden Markov Models: Theory and Exper-iments with Perceptron Algorithms.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying Relations for Open InformationExtraction.
In Proceedings of the 2011 Conferenceon Empirical Methods in Natural Language Pro-cessing.Qin Gao and Stephan Vogel.
2008.
Parallel Imple-mentations of Word Alignment Tool.
In Proc.
of theACL 2008 Software Engineering, Testing, and Qual-ity Assurance Workshop.Barbara J. Grosz, Douglas E. Appelt, Paul A. Mar-tin, and Fernando C. N. Pereira.
1987.
TEAM:An Experiment in the Design of TransportableNatural-Language Interfaces.
Artificial Intelligence,32(2):173?243.Raphael Hoffmann, Congle Zhang, and Daniel S.Weld.
2010.
Learning 5000 relational extractors.In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics.Raphael Hoffmann, Congle Zhang, Xiao Ling,Luke Zettlemoyer, and Daniel S. Weld.
2011.Knowledge-Based Weak Supervision for Informa-tion Extraction of Overlapping Relations.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics.Boris Katz.
1997.
Annotating the World Wide Webusing Natural Language.
In RIAO, pages 136?159.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
In Pro-ceedings of the 2003 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics.Cody Kwok, Oren Etzioni, and Daniel S. Weld.
2001.Scaling Question Answering to the Web.
ACMTrans.
Inf.
Syst., 19(3):242?262.Percy Liang, Michael Jordan, and Dan Klein.
2011.Learning Dependency-Based Compositional Se-mantics.
In Proceedings of the 49th Annual Meetingof the Association for Computational Linguistics.Yuval Marton, Chris Callison-Burch, and PhilipResnik.
2009.
Improved Statistical Machine Trans-lation Using Monolingually-Derived Paraphrases.In Proceedings of the 2009 Conference on Empiri-cal Methods in Natural Language Processing.Ryan McDonald, Keith Hall, and Gideon Mann.
2010.Distributed training strategies for the structured per-ceptron.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics.Mike Mintz, Steven Bills, Rion Snow, and Daniel Ju-rafsky.
2009.
Distant Supervision for Relation Ex-traction Without Labeled Data.
In Proceedings ofthe Joint Conference of the 47th Annual Meeting ofthe ACL.Franz Josef Och and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
In Proceedings of the38th Annual Meeting of the Association for Compu-tational Linguistics.Ana-Maria Popescu, Alex Armanasu, Oren Etzioni,David Ko, and Alexander Yates.
2004.
ModernNatural Language Interfaces to Databases: Compos-ing Statistical Parsing with Semantic Tractability.
InProceedings of the Twentieth International Confer-ence on Computational Linguistics.Chris Quirk, Chris Brockett, and William Dolan.
2004.Monolingual Machine Translation for ParaphraseGeneration.
In Proceedings of the 2004 Conferenceon Empirical Methods in Natural Language Pro-cessing.1617Sebastian Riedel, Limin Yao, and Andrew McCallum.2010.
Modeling Relations and Their Mentions with-out Labeled Text.
In Proceedings of the 2010 Euro-pean conference on Machine learning and Knowl-edge Discovery in Databases.Lappoon R. Tang and Raymond J. Mooney.
2001.
Us-ing Multiple Clause Constructors in Inductive LogicProgramming for Semantic Parsing.Christina Unger, Lorenz Bu?hmann, Jens Lehmann,Axel-Cyrille Ngonga Ngomo, Daniel Gerber, andPhilipp Cimiano.
2012.
Template-Based QuestionAnswering over RDF Data.
In Proceedings of the21st World Wide Web Conference 2012.Mohamed Yahya, Klaus Berberich, Shady Elbas-suoni, Maya Ramanath, Volker Tresp, and GerhardWeikum.
2012.
Natural Language Questions forthe Web of Data.
In Proceedings of the 2012 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning.Limin Yao, Sebastian Riedel, and Andrew McCallum.2012.
Unsupervised Relation Discovery with SenseDisambiguation.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Lin-guistics.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to Parse Database Queries Using Inductive LogicProgramming.
In Proceedings of the Thirteenth Na-tional Conference on Artificial Intelligence.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to Map Sentences to Logical Form: Struc-tured Classification with Probabilistic CategorialGrammars.
In Proceedings of the 21st Conferencein Uncertainty in Artificial Intelligence.1618
