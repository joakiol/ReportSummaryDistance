Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 408?418,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsAligning Opinions: Cross-Lingual Opinion Mining with DependenciesMariana S. C.
Almeida?
?Cl?audia Pinto?Helena Figueira?Pedro Mendes?Andr?e F. T.
Martins??
?Priberam Labs, Alameda D. Afonso Henriques, 41, 2o, 1000-123 Lisboa, Portugal?Instituto de Telecomunicac?
?oes, Instituto Superior T?ecnico, 1049-001 Lisboa, Portugal{mla,atm}@priberam.ptAbstractWe propose a cross-lingual framework forfine-grained opinion mining using bitextprojection.
The only requirements are arunning system in a source language andword-aligned parallel data.
Our methodprojects opinion frames from the source tothe target language, and then trains a sys-tem on the target language using the auto-matic annotations.
Key to our approach isa novel dependency-based model for opin-ion mining, which we show, as a byprod-uct, to be on par with the current stateof the art for English, while avoiding theneed for integer programming or rerank-ing.
In cross-lingual mode (English to Por-tuguese), our approach compares favor-ably to a supervised system (with scarcelabeled data), and to a delexicalized modeltrained using universal tags and bilingualword embeddings.1 IntroductionThe goal of opinion mining is to extract opinionsand sentiments from text (Pang and Lee, 2008;Wilson, 2008; Liu, 2012).
With the advent of so-cial media and the increasing amount of data avail-able on the Web, this has become a very activearea of research, with applications in summariza-tion of customer reviews (Hu and Liu, 2004; Wu etal., 2011), tracking of newswire and blogs (Ku etal., 2006), question answering (Yu and Hatzivas-siloglou, 2003), and text-to-speech synthesis (Almet al, 2005).While early work has focused on determiningsentiment at document and sentence level (Panget al, 2002; Turney, 2002; Balog et al, 2006),research has gradually progressed towards fine-grained opinion mining, where rather than deter-mining global sentiment, the goal is to parse textinto opinion frames, identifying opinion expres-sions, agents, targets, and polarities (Ding et al,2008), or addressing compositionality (Socher etal., 2013b).
Since the release of the MPQA cor-pus1(Wiebe et al, 2005; Wilson, 2008), a stan-dard corpus for fine-grained opinion mining ofnews documents, a long string of work has beenproduced (reviewed in ?2).
Despite the large vol-ume of prior work, opinion mining has by andlarge been limited to monolingual approaches inEnglish.2This is explained by the heavy effortof annotation necessary for current learning-basedapproaches to succeed, which delays the deploy-ment of opinion miners for new languages.We bridge the existing gap by proposing across-lingual approach to fine-grained opinionmining via bitext projection.
This technique hasbeen quite effective in several NLP tasks, suchas part-of-speech (POS) tagging (T?ackstr?om etal., 2013), named entity recognition (Wang andManning, 2014), syntactic parsing (Yarowsky andNgai, 2001; Hwa et al, 2005), semantic role label-ing (Pad?o and Lapata, 2009), and coreference res-olution (Martins, 2015).
Given a corpus of parallelsentences (bitext), the idea is to run a pre-trainedsystem on the source side and then to use wordalignments to transfer the produced annotations tothe target side, creating an automatic training cor-pus for the impoverished language.To alleviate the complexity of the task, westart by introducing a lightweight representation?called dependency-based opinion mining?andconvert the MPQA corpus to this formalism (?3).We propose a simple arc-factored model that per-mits easy decoding (?4) and we show that, despite1http://mpqa.cs.pitt.edu/corpora/mpqa_corpus.2Besides English, monolingual systems have also beendeveloped for Chinese and Japanese (Seki et al, 2007), Ger-man (Clematide et al, 2012) and Bengali (Das and Bandy-opadhyay, 2010).408its simplicity, this model is on par with state-of-the-art opinion mining systems for English (?5).Then, through bitext projection, we transfer thesedependency-based opinion frames to Portuguese(our target language), and train a system on theresulting corpus (?6).As part of this work, a validation corpus in Por-tuguese with subjectivity annotations was created,along with a translation of the MPQA Subjectiv-ity lexicon of Wilson et al (2005).3Experimentalevaluation (?7) shows that our cross-lingual ap-proach surpasses a supervised system trained ona small corpus in the target language, as well as adelexicalized baseline trained using universal POStags, bilingual word embeddings and a projectedlexicon.2 Related WorkA considerable amount of work on fine-grainedopinion mining is based on the MPQA corpus.Kim and Hovy (2006) proposed a method for find-ing opinion holders and topics, with the aid of a se-mantic role labeler.
Choi et al (2005) and Breck etal.
(2007) used CRFs for finding opinion holdersand recognizing opinion expressions, respectively.The two things are predicted jointly by Choi et al(2006), with integer programming, and Johanssonand Moschitti (2010), via reranking.
The samemethod was applied later for joint prediction ofopinion expressions and their polarities (Johans-son and Moschitti, 2011).
The advantage of ajoint model was also shown by Choi and Cardie(2010) and Yang and Cardie (2014).
Yang andCardie (2012) classified expressions with a semi-Markov decoder, outperforming a B-I-O tagger; inlater work, the same authors proposed an ILP de-coder to jointly retrieve opinion expressions, hold-ers, and targets (Yang and Cardie, 2013).
A morerecent work (?Irsoy and Cardie, 2014) proposes arecurrent neural network to identify opinion spans.All the approaches above rely on a span-basedrepresentation of the opinion elements.
Thismakes joint decoding procedures more compli-cated, since they must forbid overlap of opinionelements or add further constraints, leading to in-teger programming or reranking strategies.
Be-sides, there is little consensus about what shouldbe the correct span boundaries, the inter-annotatoragreement being quite low (Wiebe et al, 2005).
In3The Portuguese corpus and the lexicon are available athttp://labs.priberam.com/Resources.constrast, we use dependencies to model opinionelements and relations, leading to a compact repre-sentation that does not depend on spans and whichis tractable to decode.
A dependency scheme wasalso used by Wu et al (2011) for fine-grainedopinion mining.
Our work differs in which wemine opinions in news articles instead of productreviews, a considerably different task.
In addition,the approach of Wu et al (2011) relies on ?spannodes?
(instead of head words), requiring solvingan ILP followed by an approximate heuristic.Query-based multilingual opinion mining wasaddressed in several NTCIR shared tasks (Sekiet al, 2007; Seki et al, 2010).4However, toour best knowledge, a cross-lingual approach hasnever been attempted.
Some steps were taken byMihalcea et al (2007) and Banea et al (2008),who translated an English lexicon and the MPQAcorpus to Romanian and Spanish, but for the muchsimpler task of sentence-level subjectivity anal-ysis.
Cross-lingual sentiment classification wasaddressed by Wan (2009), Prettenhofer and Stein(2010) and Wei and Pal (2010) at document level,and by Lu et al (2011) at sentence level.
Recently,Gui et al (2013) applied projection learning foropinion mining in Chinese.
However, this workonly addresses agent detection and requires trans-lating the MPQA corpus.
While all these worksare relevant, none addresses fine-grained opinionmining in its full generality, where the goal is topredict full opinion frames.3 Dependency-Based Opinion MiningThis work addresses various elements of subjec-tivity annotated in the MPQA corpus, namely:?
direct-subjective expressions (henceforth, opin-ions) that are direct mentions of a private state,e.g.
opinions, beliefs, emotions, sentiments,speculations, goals, etc.;?
the opinion agent, i.e., the holder of the opinion;?
the opinion target, i.e., what is being arguedabout;?
the opinion polarity, i.e., the sentiment (posi-tive, negative or neutral) towards the target.As an example, consider the sentence in Fig-ure 1, which has two opinions, expressed by the4NTCIR-8 had a cross-lingual track but in a very differ-ent sense: there, queries and documents are in different lan-guages; in contrast, we transfer a model accross languages.409spans ?is believed?
(O1) and ?are against?
(O2).The first opinion has an implicit agent and a neu-tral polarity toward the target ?the rich elites?(T1).
This target is also the agent (A2) of the sec-ond opinion, which has a negative polarity toward?Hugo Ch?avez?
(T2).3.1 MotivationAs noted in prior work (Choi et al, 2005; Kim andHovy, 2006; Johansson and Moschitti, 2010), onesource of difficulty when learning opinion min-ers on MPQA is with the boundaries of the en-tity spans.
The fact that no criterion for choosingthese boundaries is explicitly defined in the anno-tation guidelines (Wiebe et al, 2005) leads to alow inter-annotator agreement.
To circumvent thisproblem and make the learning task easier, we de-part from the classical span-based approaches to-ward dependency-based opinion mining.
Thisdecision is inspired by the success of dependencymodels for syntax and semantics (Buchholz andMarsi, 2006; Surdeanu et al, 2008).
These depen-dency relations can be further converted to opinionspans (as described in ?3.3), or directly used asfeatures in downstream applications.
As we willsee, a compact representation based on dependen-cies can achieve state-of-the-art results and has theadvantage of being easily transferred to other lan-guages through a parallel corpus.3.2 Dependency GraphFigure 1 depicts a sentence-level dependency rep-resentation for fine-grained opinion mining.
Theoverall structure is a graph whose nodes are headwords (plus two special nodes, root and null),connected by labeled arcs, as outlined below.Determining head nodes.
The three opinion el-ements that we want to detect (opinions, agentsand targets) are each represented by a head node,which corresponds to a single word (underlined inFigure 1).
When converting the MPQA corpusto dependencies, we determine this ?representa-tive?
word automatically, by using the followingsimple heuristic: we first parse the sentence us-ing the Stanford dependency parser (Socher et al,2013a); then, we pick the last word in the spanwhose syntactic parent is outside the span (if thespan is a syntactic phrase, there is only one wordwhose parent is outside the span, which is the lex-ical head).
The same heuristic has been used foridentifying the heads of mention spans in corefer-ence resolution (Durrett and Klein, 2013).Defining labeled arcs.
The opinion relations arerepresented as labeled arcs that link these headnodes.
Two artificial nodes are added: a rootnode, which links to all nodes that represent opin-ion words, with the label OPINION; and a nullnode, which is used for representing implicit re-lations.
To represent opinion-agent relations, wedraw an arc labeled AGENT toward the agent word.For opinion-target relations, the arc is toward thetarget word and has one of the labels TARGET:0,TARGET:+, or TARGET:-; this encodes the polarityin addition to the type of relation.
We also includeimplicit arcs for opinion elements whose agent ortarget is not mentioned inside the sentence?theseare modeled as arcs pointing to the null node.Dependency opinion graph.
We have the fol-lowing requirements for a well-formed depen-dency opinion graph:1.
No self-arcs or arcs linking root to null.2.
An arc is labeled as OPINION if and only if itcomes from the root node.3.
Arcs labeled as AGENT or TARGET must comefrom an opinion node (i.e., a node with an in-coming OPINION arc).4.
Every opinion node has exactly one AGENT andone TARGET outgoing arcs (possibly implicit).5Similarly to prior work (Choi and Cardie, 2010;Johansson and Moschitti, 2011; Johansson andMoschitti, 2013), we map the MPQA?s polarity-into three levels: positive, negative and neutral,where the latter includes spans without polarityannotation or annotated as ?both?.
As in Johans-son and Moschitti (2013), we also ignore the ?un-certain?
aspect of the annotated polarities.3.3 Dependency-to-Span ConversionTo evaluate the opinion miner against manual an-notations and compare with other systems, weneed a procedure to convert back from predicteddependencies to spans.
In this work, we useda very simple procedure that we next describe,5Even though this assumption is not always met in prac-tice, it is typical in MPQA (only 10% of the opinions havemultiple agents, typically coreferent; and only 13% havemultiple targets).
When multiple agents or targets exist, wekeep the ones that are closest to the opinion expression.410Figure 1: Example of an opinion mining graph in our dependency formalism.
Heads are underlined.which assumes the sentence was previously parsedusing a syntactic dependency parser.To generate agent and target spans, we computethe largest span, containing the head word, whosewords are all descendants in the dependency parsetree and that are, simultaneously, not punctuations.To generate opinion spans, we start with the headword and expand the span by adding all neigh-bouring verbal words.
In the case of English, wealso allow adverbs, adjectives, modal verbs andthe word to, when expanding to the left.The application of this simple approach to thegold dependency graphs in the training partitionof the MPQA leads to oracle F1scores of 86.0%,95.8% and 93.0% in the reconstruction of opinion,agent and target spans, respectively, according tothe proportional scores described in ?5.2.4 Arc-Factored ModelOne of the advantages of the dependency represen-tation is that we can easily decode opinion-agent-target relations without the need of complicatedconstrained sequence models or integer program-ming, as done in prior work (Choi et al, 2006;Yang and Cardie, 2012; Yang and Cardie, 2013).4.1 DecodingWe model dependency-based opinion mining as astructured classification problem.
Let x be a sen-tence and y ?
Y(x) a set of well-formed depen-dency graphs, according to the constraints statedin ?3.
We define a score function that decomposesas a sum of labeled arc scores,f(x, y) =?a?yfa(x, ya) (1)where yais a labeled arc and the sum is over thearcs of the graph y.
We use a linear model withweight vector w and local features ?a(x, ya):fa(x, ya) = w ?
?a(x, ya).
(2)For making predictions, we need to computey?
= arg maxy?Y(x)f(x, y).
(3)Under the assumptions stated in ?3, this problemdecouples into independent maximization prob-lems (one for each possible opinion word in thesentence).
The detailed procedure is as follows,where arcs a can take the form o ?
h (opinion toagent) and o ?
t (opinion to target).
For everycandidate opinion word o:1.
Obtain the most compatible agent word,?h :=argmaxhfo?h(x, AGENT);2.
Obtain the best target word and its polarity,(?t, p?)
:= argmaxt,pfo?t(x, TARGET:p);3.
Compute the total score of this candidateopinion as so:= froot?o(x, OPINION) +fo?h?
(x, AGENT) + fo?t?
(x, TARGET:p?).
Then,if so?
0, add the arcs root ?
o, o ?
?h, ando ?
?t to the dependency graph, respectivelywith labels OPINION, AGENT, and TARGET:p?.For a sentence with L words, this decoding pro-cedure takes O(L2) time.
In practice, we speedup this process by pruning from the candidate listarcs whose connected POS were not observed inthe training set and whose length were larger thanthe ones observed in the training set.4.2 FeaturesWe now describe our features ?a, which arecomputed after processing the sentence to predictPOS tags, syntactic dependency trees, lemmas andvoice (active or passive) information.
For English,we used the Stanford dependency parser (Socheret al, 2013a) for the syntactic annotations, thePorter stemmer to compute word stems, and a setof rules for computing the voice of each word.
OurPortuguese corpus include all these preprocessingelements (?6.3), with the exception of the voice in-formation (features depending on voice were onlyused for English).We also used the Subjectivity Lexicon6of Wil-son et al (2005) that we translated to Portuguese6http://mpqa.cs.pitt.edu/lexicons/subj_lexicon/411(?6.3), and a set of negation words (e.g.
not, never,nor) and quantity words (e.g.
very, much, less)collected for both languages.Our arc-factored features are described below;they are inspired by prior work on dependencyparsing (Martins et al, 2013) and fine-grainedopinion mining (Breck et al, 2007; Johansson andMoschitti, 2013).Opinion features.
We define a set of featuresthat only look at the opinion word; special sym-bols are used if the opinion is connected to a rootor null node.
The features below are also con-joined with the arc label.?
OPINION WORD.
The word itself, the lemma,the POS, and the voice.
Conjunction of the wordwith the POS, and of the lemma with the POS.?
BIGRAMS.
Bigrams of words and POS corre-sponding to the opinion word conjoined with itsprevious (and next) word.?
LEXICON (BASIC).
Conjunction of the strengthand polarity of the opinion word in the Subjec-tivity Lexicon6(e.g., ?weaksubj+neg?).?
LEXICON (COUNT).
Number of subjectivewords (total, positive and negative) in a sen-tence, with and without being conjoined withthe polarity of the opinion word in the lexicon.?
LEXICON (CONTEXT).
For each word that is inthe lexicon and within the 4-word context of theopinion, the form and the polarity of that wordin the lexicon, with and without being conjoinedwith the form and the polarity in the lexicon ofthe opinion word.
Besides the 4-word context,we also used the next/previous word in the sen-tence which is in the lexicon.?
NEGATION AND QUANTITY WORDS.
Withinthe 4-word context, features indicating if a wordis a negation or quantity word, conjoined withthe word itself and the opinion word.?
SYNTACTIC PATH.
The number of words up tothe top of the syntactic dependency tree, and thesequence of POS tags in that path.Opinion-Argument features.
In case of arcsthat neither connect to null nor root, the fea-tures above are also conjoined with the binned dis-tance between the two words.For these arcs, wedid not use the LEXICON (COUNT)/(CONTEXT)features, but we added features regarding the pairof opinion-argument words (below).?
OPINION-ARGUMENT WORD PAIR.
Severalconjunctions of word form, POS, voice and syn-tactic dependency relations corresponding to thepair opinion-argument.?
OPINION-ARGUMENT SYNTACTIC PATH.
Thesyntactic path from the opinion word to the ar-gument, conjoined with the POS and the de-pendency relations in the path (in Figure 1, forthe agent ?elites?
headed by ?are?
with relationnsuj, we have: ?VBP?NNS?
and ?nsuj??
).For arcs that neither connect to null or root,we conjoin voice features with the label, distance,and the direction of the arc.
For these arcs, wealso include back-off features where the polarityinformation is removed from the (target) labels.5 English Monolingual ExperimentsIn a first set of experiments, we evaluated theperformance of our dependency-based model foropinion mining (?3) in the MPQA English corpus.5.1 LearningWe trained arc-factored models by running 25epochs of max-loss MIRA (Crammer et al, 2006).Our cost function takes into account mismatchesbetween predicted and gold dependencies, witha cost CPon labeled arcs incorrectly predicted(false positives) and a cost CR= 1 ?
CPonmissed gold labeled arcs (false negatives).
Thecost CP, the regularization constant, and the num-ber of epochs were tuned in the development set.5.2 Evaluation MetricsOpinion spans (Op.)
are evaluated with F1scores,according to two matching criteria commonlyused in the literature: overlap matching (OM),where a predicted span is counted as correct ifit overlaps a gold one, and proportional match-ing (PM), proposed by Johansson and Moschitti(2010).
For the latter, we use the following for-mula for the recall, where we consider the sets ofgold (G) and predicted (P) opinion spans:7R(G,P) =?p?Pmaxg?G|g?p|/|p||P|; (4)7This metric is slightly different from the PM metric of Jo-hansson and Moschitti (2010), in which recall was computedas R(G,P) =?p?P?g?G|g?p|/|p||P|.
The reason why wereplace the ?sum?
by a ?max?
is that each predicted span p in(4) could contribute to the recall with a value greater than 1.Since most of the predicted spans only overlap a single goldspan, this fix has a very small effect in the final scores.412the precision is P (G,P) = R(P,G).
We also re-port metrics based on a head matching (HM) cri-terion, where a predicted span is considered cor-rect if its syntactic head matches the head of thegold span.
We consider that a pair opinion-agent(Op-Ag.)
or opinion-target (Op-Tg.)
is correctlyextracted according to the OM or the HM criteria,if both the elements satisfy these criteria and therelation holds in the gold data.
We also computethe metric described in Johansson and Moschitti(2010) which measures how well agents of opin-ions are predicted based on a proportional match-ing (PM) criterion.
This metric is applied to eval-uate the extraction of both agents and targets.
Fi-nally, to evaluate the opinions?
polarities (Op-Pol.metric) we consider as correct opinions where thespan and polarity both match the gold ones.5.3 Results: Dependency-Based ModelWe assess the quality of our monolingualdependency-based model by comparing it to therecent state-of-the-art approach of Johansson andMoschitti (2013), whose code is available online.8That paper reports the performance of a basicspan-based pipeline system (which extracts opin-ions with a CRF, followed by two separate classi-fiers to detect polarities and agents), and of a moresophisticated system that applies a reranking pro-cedure to account for more complex features thatconsider interactions accross opinion elements.We ran experiments using the same data andMPQA partitions as Johansson and Moschitti(2013).
However, since our system is designed forpredicting opinion, agents and targets together, weremoved the documents that were not annotatedwith targets.
The final train/development/test setshave a total of 6,774/1,404/2,559 sentences and3,834/881/1,426 opinions, respectively.Table 1 reports the results; since the systemsof Johansson and Moschitti (2013) do not pre-dict targets, Table 1 omits target scores.9We ob-serve that our dependency-based system achievesresults competitive with the best results of Johans-son and Moschitti (2013) and clearly above theones reached by their basic system that does notuse re-ranking features.
Though the two systemsare not fully comparable,10the results in Table 18http://demo.spraakdata.gu.se/richard/unitn_opinion/details.html9We will report target scores later in ?7.10Our system makes use of target annotations to predictthe opinion frames, while Johansson and Moschitti (2013)show that our dependency-based approach (?3.2)followed by a simple dependency-to-span conver-sion (?3.3) is, despite its simplicity, on par witha top-performing opinion mining system.
We con-jecture that this is due to the ability to extract opin-ions, agents, and targets jointly using exact decod-ing.
Note that our proposed dependency schemewould also be able to include additional global fea-tures relating pairs of opinions (by adding scoresto pairs of opinion arcs) or two opinions havingthe same agent (by adding scores to pairs of agentarcs sharing its argument), similar to the rerankingfeatures used by Johansson and Moschitti (2013).Similar second-order scores have been used insyntactic and semantic dependency parsing (Mar-tins et al, 2013; Martins and Almeida, 2014), butwith an increase in the complexity of the modeland of the decoder.6 Cross-Lingual Opinion MiningWe now turn to the problem of learning a opin-ion mining system for a resource-poor language(Portuguese), in a cross-lingual manner.
We usea bitext projection approach (?6.1), whose onlyrequirements are a model for a resource-rich lan-guage (English) and parallel data (?6.2).6.1 Bitext ProjectionOur methodology is outlined as Algorithm 1.
Forsimplicity, we call the source and target languagesEnglish (e) and ?foreign?
(f ), respectively.
Theprocedure is inspired by the idea of bitext pro-jection (Yarowsky and Ngai, 2001).
We start bytraining an English system on the labeled data Le(line 1), which in our case is the MPQA v.2.0 cor-pus.
This system is then used to label the Englishside of the parallel data, automatically identifyingopinion frames (line 2).
The next step is to runa word aligner on the parallel data (line 3).
Theautomatic alignments are then used to project theopinion frames to the target language (along withsome filtering), yielding an automatic corpus?D(f)(line 4), which finally serves to train a system forthe target language (line 5).6.2 Parallel DataWe use an English-Portuguese parallel corpusbased on the scientific news Brazilian magazineRevista Pesquisa FAPESP, collected by Aziz andhas access not only to direct subjective spans but also to sub-jective expressions annotations with their agents and polarityinformation.413JM13, BASIC JM13, RERANKING OUR SYSTEMHM PM OM HM PM OM HM PM OMOp.
56.3 56.2 60.6 58.6 59.2 63.7 61.6* 59.8 65.1Op-Ag.
40.3 47.1 44.9 42.4 51.4 48.1 45.7* 51.4 50.3*Op-Tg.
- - - - - - 31.3* 48.3* 48.3*Op-Pol.
46.1 45.9 49.3 48.5 48.9* 52.5 47.9 47.0 50.7Table 1: Method comparison: F1scores obtained in the MPQA corpus, for our dependency based methodand the approaches in Johansson and Moschitti (2013), with and without reranking.
The symbol *indicates that the best system beats the other systems with statistical significance, with p < 0.05 andaccording to a bootstrap resampling test (Koehn, 2004).Figure 2: Excerpt of a bitext document from FAPESP, with automatic opinion dependencies.
The anno-tations are directly projected to Portuguese via automatic word alignments.Algorithm 1 Cross-Lingual Opinion MiningInput: Labeled data Le, parallel data Deand Df.Output: Target opinion mining system Sf.1: Se?
LEARNOPINIONMINER(Le)2:?De?
RUNOPINIONMINER(Se,De)3: De?f?
RUNWORDALIGNER(De,Df)4:?Df?
PROJECTANDFILTER(De?f,?De)5: Sf?
LEARNOPINIONMINER(?Df)Specia (2011).
Though this corpus is in Brazil-ian Portuguese (while our validation corpus is inEuropean Portuguese), we preferred FAPESP overother commonly used parallel corpora (such as theEuroparl and UN datasets), since it is closer toour newswire target domain, with a smaller promi-nence of direct speech.
We computed word align-ments using the Berkeley aligner (Liang et al,2006), intersected them and filtered out all thealignments whose confidence is below 0.95.After annotating the English side of FAPESPwith the pre-trained system (?Dein Algorithm 1,with a total of 166,719 sentences and 81,492 opin-ions), the high confidence alignments (De?f) areused to project the annotations to the Portugueseside of the corpus.
The automatic annotations pro-duced by our dependency-based system are easilytransferred at a word level (for words with highconfidence alignments), as illustrated in Figure 2.To improve the quality of the resulting corpus,we excluded sentences whose alignments coverless than 70% of the words in the target side ofthe corpus, or sentences whose opinion elementswere not fully projected through high confidencealignments.
At this point, we obtain an automat-ically annotated corpus in Portuguese (?Df), with106,064 sentences and 32,817 opinions.6.3 Portuguese Opinion Mining CorpusFor validation purposes, we also created a Por-tuguese corpus with manually annotated fine-grained opinions.
The corpus consists of a sub-set of the documents of the Priberam CompressiveSummarization Corpus11(Almeida et al, 2014),which contains 80 news topics with 10 documentseach, collected from several Portuguese newspa-pers, TV and radio websites in the biennia 2010?2011 and 2012?2013.
In the scope of the currentwork, we selected and annotated one document ofeach of the 80 topics.
The first biennium was se-lected as the test set and the second biennium wassplit into development and training sets (see Ta-11http://labs.priberam.com/Resources/PCSC414ble 2 for statistics).#doc.
#sent.
#opin.Train 20 441 240Dev 20 225 197Test 40 560 391Table 2: Number of documents, sentences andopinions in the Portuguese Corpus.HM PM OMOp.
77.0 76.7 79.2Op-Ag.
69.1 72.3 73.5Op-Tg.
61.9 65.4 71.4Op-Pol.
49.4 49.1 50.7Table 3: Inter-annotator agreement in the test par-tition (shown are F1scores).The corpus was annotated in a similar vein asthe MPQA (Wiebe et al, 2005), with the additionof the head node for each element of the opin-ion frame.
It includes spans for direct-subjectiveexpressions with intensity and polarity informa-tion; agent spans; and target spans.
The annotationwas carried out by three linguists, after reading theMPQA annotation guidelines (Wiebe et al, 2005;Wilson, 2008) and having a small practice periodusing the provided examples and some MPQA an-notated sentences.
Each document was annotatedby two of the three linguists and then revised bythe third linguist, who (in case of any doubts) dis-cussed with the initial annotators to reach for thefinal consensus.
Scores for inter-annotator agree-ment are shown in Table 3.The corpus was annotated with automaticPOS tags and dependency parse trees usingTurboParser (Martins et al, 2013).12We usedan in-house lemmatizer to obtain lemmas foreach inflected word in the corpus.
A Por-tuguese lexicon of subjectivity was created bytranslating the words in the Subjectivity Lex-icon of Wilson et al (2005).
The annotatedcorpus and the translated subjectivity lexiconare available at http://labs.priberam.com/Resources/Fine-Grained-Opinion-Corpus,and http://labs.priberam.com/Resources/Subjectivity-Lexicon-PT, respectively.12http://www.ark.cs.cmu.edu/TurboParserOUR SYSTEM DELEXICALIZEDHM PM OM HM PM OMOp.
65.7 63.5 69.8 50.1 45.8 52.7Op-Ag.
47.6 48.8 51.1 33.8 34.8 35.7Op-Tg.
34.9 44.8 50.3 19.9 28.0 32.1Op-Pol.
51.5 50.2 54.4 36.7 34.7 38.8Table 4: F1scores obtained in English (MPQA),for our full system and the DELEXICALIZED one.7 Cross-Lingual ExperimentsIn a final set of experiments, we compare threesystems of fine-grained opinion mining for Por-tuguese.
All were trained as described in ?5.1.7.1 System DescriptionBaseline #1: Supervised System.
A SUPER-VISED system was trained on the small Portuguesetraining set described in ?6.3.
Though being asmall training corpus, this is, to the best of ourknowledge, the only existing corpus with fine-grained opinions in Portuguese.
We used the samearc-factored model and features described in ?4.Baseline #2: Delexicalized System with Bilin-gual Embeddings.
This baseline consists of adirect model transfer: a DELEXICALIZED systemis trained in the source language, without lan-guage specific features, so that it can be directlyapplied to the target language.
Despite its simplic-ity, this strategy managed to provide a fairly strongbaseline in several NLP tasks (Zeman and Resnik,2008; McDonald et al, 2011; S?gaard, 2011).To achieve a unified feature representation, wemapped all language-specific POS tags to univer-sal tags (Petrov et al, 2012), and removed allfeatures depending on the dependency relations,but maintained those depending on the syntacticpath (but not on the dependency relations them-selves).
In addition, we replaced the lexical fea-tures by 128-dimensional cross-lingual word em-beddings.13To obtain these bilingual neural em-beddings, we ran the method of Hermann andBlunsom (2014) on the parallel data (?6.1).
Wescaled the embeddings by a factor of 2.0 (selectedon the dev-set), following the procedure describedin Turian et al (2010).We trained the English delexicalized system onthe MPQA corpus, using the same test documents13A delexicalized system trained without the word embed-dings had a worse performance.415BASELINE #1 (SUP.)
BASELINE #2 (DELEX.)
BITEXT PROJECTIONHM PM OM HM PM OM HM PM OMOp.
49.4 48.7 50.8 33.1 32.1 34.3 58.0* 55.7* 58.0*Op-Ag.
23.5 27.2 31.5 14.3 18.8 20.0 30.8* 31.2* 36.2*Op-Tg.
23.0 24.9 30.6 11.0 15.7 19.0 29.4* 29.4* 35.6*Op-Pol.
24.1 23.8 24.7 16.6 16.4 17.6 35.7* 34.1* 35.7*Table 5: Comparison of cross-lingual approaches.
F1scores obtained in our Portuguese validation corpususing: a SUPERVISED system trained on the small available data, a DELEXICALIZED system trained withuniversal POS tags and multilingual embeddings and our BITEXT PROJECTION OF DEPENDENCIES.The symbol * indicates that the best system beats the other systems with statistical significance, withp < 0.05 and according to a bootstrap resampling test (Koehn, 2004).as Riloff and Wiebe (2003) and whose list is avail-able with the corpus, but selecting only documentsannotated with targets.
We randomly split theremaining documents into train and developmentsets, respectively with a total of 6,471 and 782 sen-tences.14Table 4 shows the performance of thedelexicalized baseline in English, compared witha lexicalized system.
We will see how this modelbehaves in a cross-lingual setting in ?7.2.Our System: Bitext Projection of Opinion De-pendencies.
Finally, we implemented our cross-lingual BITEXT approach (?6).
We trained the(lexicalized) English model on the MPQA corpus(the performance of this model is shown in Ta-ble 4).
Then, we ran this model on the Englishside of the parallel corpus, generating automaticannotations, and projected these annotations to thePortuguese side, as described in ?6.2.
Finally, aPortuguese model was trained on these projectedannotations using the arc-factored model and fea-tures described in ?4.7.2 ComparisonTable 5 shows the F1scores obtained by the threesystems on the Portuguese test partition.
We ob-serve that the BITEXT approach outperformed theSUPERVISED and the DELEXICALIZED ones in allmetrics with a considerable margin, which showsthe effectiveness of our proposed method.
TheSUPERVISED system suffers from the fact that thetraining set is too small to allow good general-ization; the bitext projection method, in contrast,can create arbitrarily large training corpora with-out any annotation effort.
The performance of14Note that this split is different from the one we used in?5.
There we used the same split as Johansson and Moschitti(2013), for a fair comparison with their system; here, we fol-low the standard MPQA test partition.the DELEXICALIZED system is rather disappoint-ing.
This result is justified by a decrease of per-formance in English due to the delexicalization(cf.
Table 4), followed by an extra loss of qualitydue to language differences.Though our BITEXT approach scores the best,the scores are behind the range of values ob-tained for English (Table 4), and far from the inter-annotator agreement numbers (Table 3), suggest-ing room for improvement.
The polarity scores inTable 5 appear to be relatively low.
This fact isprobably be justified with the annotator agreementscores (Table 3) which are considerably lower forthese metrics.8 ConclusionsWe presented a cross-lingual framework for fine-grained opinion mining.
We used a bitext pro-jection technique to transfer dependency-basedopinion frames from English to Portuguese.
Ex-perimentally, our dependency model achievedstate-of-the-art results for English, and the Por-tuguese system trained with bitext projection out-performed two baselines: a supervised systemtrained on a small dataset, and a delexicalizedmodel with bilingual word embeddings.9 AcknowledgementsWe would like to thank the anonymous review-ers for their insightful comments, and RichardJohansson for sharing his code and for answer-ing several questions.
This work was par-tially supported by the EU/FEDER programme,QREN/POR Lisboa (Portugal), under the Intel-ligo project (contract 2012/24803) and by a FCTgrants UID/EEA/50008/2013 and PTDC/EEI-SII/2312/2012.416ReferencesCecilia Ovesdotter Alm, Dan Roth, and Richard Sproat.2005.
Emotions from text: machine learning for text-based emotion prediction.
In EMNLP.Miguel B. Almeida, Mariana S. C. Almeida, Andr?e F. T. Mar-tins, Helena Figueira, Pedro Mendes, and Cl?audia Pinto.2014.
Priberam compressive summarization corpus: Anew multi-document summarization corpus for europeanportuguese.
In LREC.Wilker Aziz and Lucia Specia.
2011.
Fully automatic com-pilation of a Portuguese-English parallel corpus for statis-tical machine translation.
In STIL.Krisztian Balog, Gilad Mishne, and Maarten de Rijke.
2006.Why are they excited?
: Identifying and explaining spikesin blog mood levels.
In EACL.Carmen Banea, Rada Mihalcea, Janyce Wiebe, and SamerHassan.
2008.
Multilingual subjectivity analysis usingmachine translation.
In EMNLP.Eric Breck, Yejin Choi, and Claire Cardie.
2007.
Identifyingexpressions of opinion in context.
In IJCAI.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-X sharedtask on multilingual dependency parsing.
In CoNLL.Yejin Choi and Claire Cardie.
2010.
Hierarchical sequentiallearning for extracting opinions and their attributes.
InACL.Yejin Choi, Claire Cardie, Ellen Riloff, and Siddharth Pat-wardhan.
2005.
Identifying sources of opinions with con-ditional random fields and extraction patterns.
In EMNLP.Yejin Choi, Eric Breck, and Claire Cardie.
2006.
Joint ex-traction of entities and relations for opinion recognition.In EMNLP.Simon Clematide, Stefan Gindl, Manfred Klenner, Ste-fanos Petrakis, Robert Remus, Josef Ruppenhofer, UlliWaltinger, and Michael Wiegand.
2012.
MLSA A Multi-layered Reference Corpus for German Sentiment Analy-sis.
In LREC.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online Passive-Aggressive Algorithms.
Journal of Machine Learning Re-search.Dipankar Das and Sivaji Bandyopadhyay.
2010.
Labelingemotion in bengali blog corpus a fine grained tagging atsentence level.
In (ALR8), COLING.Xiaowen Ding, Bing Liu, and Philip S Yu.
2008.
A holisticlexicon-based approach to opinion mining.
In WSDM.Greg Durrett and Dan Klein.
2013.
Easy victories and uphillbattles in coreference resolution.
In EMNLP.Lin Gui, Ruifeng Xu, Jun Xu, and Chenxiang Liu.
2013.A cross-lingual approach for opinion holder extraction.Journal of Computational Information Systems, 9(6).Karl Moritz Hermann and Phil Blunsom.
2014.
MultilingualModels for Compositional Distributional Semantics.
InACL.Minqing Hu and Bing Liu.
2004.
Mining opinion features incustomer reviews.
In AAAI.Rebecca Hwa, Philip Resnik, Amy Weinberg, Clara Cabezas,and Okan Kolak.
2005.
Bootstrapping parsers via syn-tactic projection across parallel texts.
Natural LanguageEngineering, 11(3).Ozan?Irsoy and Claire Cardie.
2014.
Opinion mining withdeep recurrent neural networks.
In EMNLP.Richard Johansson and Alessandro Moschitti.
2010.
Rerank-ing models in fine-grained opinion analysis.
In COLING.Richard Johansson and Alessandro Moschitti.
2011.
Extract-ing opinion expressions and their polarities: exploration ofpipelines and joint models.
In ACL.Richard Johansson and Alessandro Moschitti.
2013.
Rela-tional features in fine-grained opinion analysis.
Computa-tional Linguistics, 39(3).Soo-Min Kim and Eduard Hovy.
2006.
Extracting opinions,opinion holders, and topics expressed in online news me-dia text.
In SST.P.
Koehn.
2004.
Statistical signicance tests for machinetranslation evaluation.
In ACL.Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.
2006.Opinion extraction, summarization and tracking in newsand blog corpora.
In AAAI.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Alignmentby agreement.
In NAACL.Bing Liu.
2012.
Sentiment analysis and opinion min-ing.
Synthesis Lectures on Human Language Technolo-gies, 5(1).Bin Lu, Chenhao Tan, Claire Cardie, and Benjamin K. Tsou.2011.
Joint bilingual sentiment classification with unla-beled parallel corpora.
In ACL.Andr?e F. T. Martins and M. S. C. Almeida.
2014.
Priberam:A turbo semantic parser with second order features.
InSemEval.Andr?e F. T. Martins, Miguel B. Almeida, and Noah A.Smith.
2013.
Turning on the turbo: Fast third-order non-projective turbo parsers.
In ACL.Andr?e F. T. Martins.
2015.
Transferring coreference re-solvers with posterior regularization.
In ACL.Ryan McDonald, Slav Petrov, and Keith Hall.
2011.
Multi-source transfer of delexicalized dependency parsers.
InEMNLP.Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007.Learning multilingual subjective language via cross-lingual projections.
In ACL.Sebastian Pad?o and Mirella Lapata.
2009.
Cross-lingual an-notation projection for semantic roles.
Journal of Artifi-cial Intelligence Research, 36(1).Bo Pang and Lillian Lee.
2008.
Opinion mining and sen-timent analysis.
Foundations and Trends in InformationRetrieval, 2(1-2).Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002.Thumbs up?
: Sentiment classification using machinelearning techniques.
In EMNLP.417Slav Petrov, Dipanjan Das, and Ryan McDonald.
2012.
Auniversal part-of-speech tagset.
In LREC.Peter Prettenhofer and Benno Stein.
2010.
Cross-languagetext classification using structural correspondence learn-ing.
In ACL.Ellen Riloff and Janyce Wiebe.
2003.
Learning extractionpatterns for subjective expressions.
In EMNLP.Yohei Seki, David Kirk Evans, Lun-Wei Ku, Hsin-Hsi Chen,Noriko Kando, and Chin-Yew Lin.
2007.
Overview ofopinion analysis pilot task at NTCIR-6.
In NTCIR-6.Yohei Seki, Lun-Wei Ku, Le Sun, Hsin-Hsi Chen, and NorikoKando.
2010.
Overview of opinion analysis pilot task atNTCIR-8: A Step Toward Cross Lingual Opinion Analy-sis.
In NTCIR-8.Richard Socher, John Bauer, Christopher D. Manning, andAndrew Y. Ng.
2013a.
Parsing with compositional vectorgrammars.
In ACL.Richard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang,Christopher D Manning, Andrew Y Ng, and ChristopherPotts.
2013b.
Recursive deep models for semantic com-positionality over a sentiment treebank.
In EMNLP.Anders S?gaard.
2011.
Data point selection for cross-language adaptation of dependency parsers.
In ACL.Mihai Surdeanu, Richard Johansson, Adam Meyers, Lu?
?sM`arquez, and Joakim Nivre.
2008.
The CoNLL-2008Shared Task on Joint Parsing of Syntactic and SemanticDependencies.
In CoNLL.Oscar T?ackstr?om, Dipanjan Das, Slav Petrov, Ryan McDon-ald, and Joakim Nivre.
2013.
Token and type constraintsfor cross-lingual part-of-speech tagging.
Trans.
of the As-sociation for Computational Linguistics.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.
Wordrepresentations: a simple and general method for semi-supervised learning.
In ACL.Peter D. Turney.
2002.
Thumbs up or thumbs down?
: Se-mantic orientation applied to unsupervised classificationof reviews.
In ACL.Xiaojun Wan.
2009.
Co-training for cross-lingual sentimentclassification.
In ACL.Mengqiu Wang and Chris Manning.
2014.
Cross-lingualprojected expectation regularization for weakly super-vised learning.
Trans.
of the Association for Computa-tional Linguistics, 2.Bin Wei and Christopher Pal.
2010.
Cross lingual adaptation:an experiment on sentiment classifications.
In ACL.Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.Annotating expressions of opinions and emotions in lan-guage.
Language Resources and Evaluation, 39(2-3).Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005.Recognizing contextual polarity in phrase-level sentimentanalysis.
In EMNLP.Theresa Wilson.
2008.
Fine-Grained Subjectivity Analysis.Ph.D.
thesis, University of Pittsburgh.Yuanbin Wu, Qi Zhang, Xuanjing Huang, and Lide Wu.2011.
Structural opinion mining for graph-based senti-ment representation.
In EMNLP.Bishan Yang and Claire Cardie.
2012.
Extracting opinionexpressions with semi-markov conditional random fields.In EMNLP.Bishan Yang and Claire Cardie.
2013.
Joint inference forfine-grained opinion extraction.
In ACL.Bishan Yang and Claire Cardie.
2014.
Joint modeling ofopinion expression extraction and attribute classification.Trans.
of the Association for Computational Linguistics.David Yarowsky and Grace Ngai.
2001.
Inducing multilin-gual pos taggers and np bracketers via robust projectionacross aligned corpora.
In NAACL.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: Separating facts from opin-ions and identifying the polarity of opinion sentences.
InEMNLP.Daniel Zeman and Philip Resnik.
2008.
Cross-languageparser adaptation between related languages.
In IJCNLP.418
