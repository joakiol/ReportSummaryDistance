Augmenting the kappa statistic to determine interannotator reliabilityfor multiply labeled data pointsAndrew RosenbergDepartment of Computer ScienceColumbia Universityamaxwell@cs.columbia.eduEd BinkowskiDepartment of Mathematics & StatisticsHunter Collegeebinkowski@juno.comAbstractThis paper describes a method for evaluatinginterannotator reliability in an email corpusannotated for type (e.g., question, answer, so-cial chat) when annotators are allowed to as-sign multiple labels to a message.
Anaugmentation is proposed to Cohen?s kappastatistic which permits all data to be includedin the reliability measure and which furtherpermits the identification of more or less re-liably annotated data points.1 IntroductionReliable annotated data are necessary for a wide varietyof natural language processing tasks.
Machine learningalgorithms commonly employed to tackle languageproblems from syntactic parsing to prosodic analysisand information retrieval all require annotated data fortraining and testing.
The reliability of these computa-tional solutions is intricately tied to the accuracy of theannotated data used in their development.
Human errorand subjectivity make deciding the accuracy of annota-tions an intractable problem.
While the objective cor-rectness of human annotations cannot be determinedalgorithmically, the degree to which the annotatorsagree in their labeling of a corpus can be quickly andsimply statistically determined using Cohen?s (1960)kappa measure.
Because human artifacts are less likelyto co-occur simultaneously in two annotators, the kappastatistic is used to measure interannotator reliability.This paper will describe an email classification andsummarization project which presented a problem forinterlabeler reliability computation since annotatorswere allowed to label data with one or two labels(Rambow, et al, 2004).
The existing kappa statisticcomputation does not obviously extend to accommodatethe presence of a secondary label.
The augmentation tothe algorithm presented in this paper allows for both amore accurate assessment of interannotator reliabilityand a unique insight into the data and how the annota-tors have employed the optional second label.
Section 2will describe the categorization project.
Section 3 willpresent a description of the annotated corpus.
Section 4will describe why the kappa statistic for determininginterannotator agreement in its basic form cannot effec-tively be applied to this corpus.
Section 5 will present away to augment the algorithm computing kappa statisticto provide greater insight into user annotations.
Section6 will analyze the results of applying this new algorithmto the annotated corpus.2 Project DescriptionThis inquiry into interannotator reliability measure-ments was spawned by problems encountered during aproject classifying and summarizing email messages.
Inthis project email messages are classified into one of tenclasses.
This classification facilitates email thread re-construction as well as summarization.
Distinct emailcategories have distinct structural and linguistic ele-ments and thus ought to be summarized differently.
Forthe casual email user, the luxuries of summarization andautomated classification for the dozen or so daily mes-sages may be rather superfluous, but for those with hun-dreds of important emails per day, automaticsummarization and categorization can provide an effi-cient and convenient way to both scan new messages(e.g., if the sender responds to a question, the categorywill be ?answer?, while the summary will contain theresponse) and retrieve old ones (e.g., ?Display allscheduling emails received last week?).
While the pro-ject intends to apply machine learning techniques toboth facets, this paper will be focusing on thecategorization component.3 Corpus DescriptionThe corpus used is a collection of 380 email messagesmarked by two annotators with either one or two of thefollowing labels: question, answer, broadcast, at-tachment transmission, planning-meeting schedul-ing, planning scheduling, planning, action item,technical discussion, and social chat.
If two labels areused, one is designated primary and the other secondary.These ten categories were selected in order to direct theautomatic summarization of email messages.This corpus is a subset of a larger corpus of ap-proximately 1000 messages exchanged between mem-bers of the Columbia University chapter of theAssociation for Computing Machinery (ACM) in 2001.The annotation of the rest of corpus is in progress.4 Standard Kappa ShortcomingsCommonly, the kappa statistic is used to measure inter-annotator agreement.
It determines how strongly twoannotators agree by comparing the probability of thetwo agreeing by chance with the observed agreement.
Ifthe observed agreement is significantly greater than thatexpected by chance, then it is safe to say that the twoannotators agree in their judgments.
Mathematically,)(1)()(EpEpApK--= where K is the kappa value, p(A) isthe probability of the actual outcome and p(E) is  theprobability of the expected outcome as predicted bychance.When each data point in a corpus is assigned a singlelabel, calculating p(A) is straightforward: simply countup the number of times the two annotators agree anddivide by the total number of annotations.
However, inlabeling this email corpus, labelers were allowed to se-lect either a single label or two labels designating one asprimary and one as secondary.The option of a secondary label increases the possi-ble labeling combinations between two annotators five-fold.
In the format ?
{<A?s labels>, <B?s labels>}?
thepossibilities are as follows: {a,a}, {a,b},  {ab,a},{ab,b},  {ab,c}, {ab,ab}, {ab,ba}, {ab,ac}, {ab,bc},{ab,cd}.
The algorithm initially used to calculate thekappa statistic simply discarded the optional secondarylabel.
This solution is unacceptable for two reasons.
1)It makes the reliability metric inconsistent with the an-notation instructions.
Why offer the option of a secon-dary label, if it is to be categorically ignored?
2) Itdiscards useful information regarding partial agreementby treating situations corresponding to {ab,ba},{ab,bc} and {ab, b} as simple disagreements.Despite this complication, the objective in comput-ing p(A) remains the same, count the agreements anddivide by the number of annotations.
But how shouldthe partial agreement cases ({ab, a}, {ab, b}, {ab,ba},{ab,ac}, and {ab,bc}) be counted?
For example, whenconsidering a message that clearly contained both aquestion and an answer, one annotator had labeled themessage as primarily question and secondarily answer,with another primarily answer and secondarily ques-tion.
Should such an annotation be considered anagreement, as the two concur on the content of the mes-sage?
Or disagreement, as they differ in their employ ofprimary and secondary?
To what degree do two annota-tors agree if one labels a message primarily a andsecondarily b and the other labels it simply a or simplyb?
What if there is agreement on the primary label anddiscrepancy on the secondary?
Or vice versa?
In thetraditional Boolean assignment, each combinationwould have to be counted as either agreement or dis-agreement.
Instead, in order to compute a useful valueof p(A),  we propose to assign a degree of agreement toeach.
This is similar in concept to Krippendorff?s(1980) alpha measure for multiple observers.5 Kappa Algorithm AugmentationTo augment the computation of the kappa statistic, weconsider annotations marked with primary and secon-dary labels not as two distinct selections, but as onedivided selection.1  When an annotator selects a singlelabel for a message, that label-message pair is assigneda score of 1.0.
When an annotator selects a primary andsecondary label, a weight p is assigned to the primarylabel and (1-p) to the secondary label for the corre-sponding label-message pair.
Before computing thekappa score for the corpus, a single value p where 0.5 ?p ?
1.0 must be selected.
If p = 1.0 the secondary labelsare completely ignored, while if p = 0.5, secondary andprimary labels are given equal weight.
By examiningthe resulting kappa score at different values of p, insightinto how the annotators are employing the optional sec-ondary label can be gained.
Moreover, single messagescan be trivially isolated in order to reveal how each datapoint has been annotated with respect to primary andsecondary labels.
Landis and Koch (1977) present amethod for calculating a weighted kappa measure.
Thismethod is useful for single annotations where the cate-gories have an obvious relationship to each other, butdoes not extend to multiply labeled data points whererelationships between categories are unknown.1 Before settling on this approach, we considered count-ing each annotation equivalently whether primary or secon-dary.
This made computation of p(A) and p(E) morecomplex, and by ignoring the primary/secondary distinctionoffered less insight into the use of the labels.5.1 Compute p(A)To compute p(A), the observed probability, two annota-tion matrices are created, one for each annotator.
Theseannotation matrices, Mannotator, have N rows and M col-umns, where n is the number of messages and m is thenumber of labels.
These annotation matrices are propa-gated as follows.1],[ =yxM A , if A marked only label y for mes-sage x.pyxM A =],[ , if A marked label y as the primarylabel for message x.pyxM A -= 1],[ , if A marked label y as the sec-ondary label for message x.0],[ =yxM A , otherwise.Table 1 shows a sample set of annotations on 5 mes-sages by annotator A.
Table 2 shows the resulting MAbased on the annotation data in Table 1 where p=0.6.Msg1 Msg2 Msg3 Msg4 Msg5a,b b,a b c c,bTable 1.
Sample annotation data from labeler Aa  b  c  dMsg1 0.6 0.4 0 0Msg2 0.4 0.6 0 0Msg3 0 1 0 0Msg4 0 0 1 0Msg5 0 0.4 0.6 0Total 1 2.4 1.6 0 5Table 2.
MA based on Table 1 data (p=0.6;N=5).With the two annotation matrices, MA and MB, anagreement matrix, Ag, is constructed where],[*],[],[ yxMyxMyxAg BA= .
A total, ?, is setto the sum of all cells of Ag.
Finally,NApa=)( .5.2 Compute p(E)Instead of assuming an even distribution of labels, wecompute p(E), the expected probability, using the rela-tive frequencies of each annotator?s labeling preference.Using the above annotation matrices, relative frequencyvectors, Freqannotator, are generated.
Table 3 showsFreqA based on MA from Table 2.?==NxAA NyxMyFreq1],[][a b c d0.2 0.48 0.32 0Table 3.
FreqA from MA in Table 2 (p=0.6;N=5).Using these two frequency vectors,?==MyBA yFreqyFreqEp1][*][)( .5.3 Calculate ?
?The equation for the augmented kappa statistic remainsthe same in the presence of this augmentation.
)(1)()('EpEpApK--=6 ResultsThis technique is not meant to inflate the kappa scores,but rather to provide further insight into how the annota-tors are using the two labels.
Execution of this aug-mented kappa algorithm on this corpus suggests that theannotation guidelines need revision before the supersetcorpus is completely annotated.
(Only 150 of 380 mes-sages present a label for use in a machine learning ex-periment with ??>0.6.)
The exact nature of theadjustments is yet undetermined.
However, both a strictspecification of when the secondary label ought to beused, and reconsideration of the ten available labelswould likely improve the annotation effort.When we examine our labeled data, we find theaverage kappa statistic across the three annotators didnot increase through examination of the secondary la-bels.
If we ignore the secondary labels (p=1.0), the av-erage ??=0.299.
When primary and secondary labelsare given equal weight (p=0.5), the average ?
?=0.281.By examining the average kappa statistic for eachmessage individually at different p values, messages canbe quickly categorized into four classes: those that dem-onstrate greatest agreement at p = 1.0; those with great-est agreement at p = 0.5; those that yield a nearlyconstant low kappa value and those that yield a nearlyconstant high kappa value.
These classes suggest certaincharacteristics about the component messages, and canbe employed to improve the ongoing annotation proc-ess.
Class 1) Those messages that show a constant, highkappa score are those that are consistently categorizedwith a single label.
(92/380 messages.)
Class 2) Thosemessages with a constant, low kappa are those messagesthat are least consistently annotated regardless ofwhether a secondary label is used or not.
(183/380 mes-sages.)
Class 3) Messages that show greater agreementat p = 1.0 than at p = 0.5 demonstrate greater inconsis-tency when the annotators opt to use the secondary la-bels but are in (greater) agreement regarding theprimary label.
Whether the primary label is more gen-eral or more specific depends on, hopefully, annotationstandards, but in the absence of rigorous instructions,individual annotator preference.
(58/380 messages.
)Class 4) Messages that show greater agreement at p =0.5 than at p = 1.0 are those messages where the pri-mary and secondary labels are switched by some anno-tators, the above {ab,ba} case.
From inspection, thismost often occurs when the two features are not in ageneral/specific relationship (e.g., planning and ques-tion being selected for a message that contains a ques-tion about planning), but are rather concurrent features(e.g., question and answer being labeled on a messagethat obviously includes both a question and an answer).
(47/380 messages.)
Each of the four categories of mes-sages can be utilized to a distinct end towards improve-ment of annotation instructions and/or annotationstandards.
Class 1 messages are clear examples of thelabels.
Class 2 messages are problematic.
These mes-sages can be used to redirect the annotators, revise theannotation manual or reconsider the annotation stan-dards.
Class 3 messages are those in which annotatorsuse the optional secondary label, but not consistently.These messages can be employed to reinstruct the anno-tators as to the expected use of the secondary label.Class 4 messages pose a real dilemma.
When thesemessages in fact do contain two concurrent features,they are not going to be good examples for machinelearning experiments.
While representative of bothcategories, they will (most likely) at feature analysis(the critical component of machine learning algorithms)be poor exemplars of each.
While the fate of Class 4messages is uncertain2, identification of these awkwardexamples is an important first step in handling theirautomatic classification.7 ConclusionCalculating a useful metric for interannotator reliabilitywhen each data point is marked with optionally one ortwo labels proved to be a complicated task.
Multiplelabels raise the possibility of partial agreement betweentwo annotators.
In order to compute the observed prob-ability (p(A)) component of the kappa statistic a con-stant weight, p, between 0.5 and 1.0 is selected.
Eachsingleton annotation is then assigned a weight of 1,while the primary label of a doubleton annotation isassigned a weight of p, the secondary 1-p.  Theseweights are then used to determine the partial agreementin the calculation of p(A).
This augmentation to thealgorithm for computing kappa is not meant to inflatethe reliability metric, but rather to allow for a morethorough view of annotated data.
By examining how2 One potential solution would be to create a new anno-tation category for each commonly occurring pair.
Whileeach Class 4 message would remain a poor exemplar of eachcomponent category, it would be a good exemplar of thisnew ?mixed?
type.the annotated components of a corpus demonstrateagreement at varying levels of p, insight is gained intohow the annotators are viewing these data and how theyemploy the optional secondary label.8 Future WorkThe problem that spawned this study has led to furtherdiscussions about how to get the most information outof apparently unreliably labeled data.
The above proc-ess shows how it is possible to classify messages into afew categories by their reliability at different levels of p.However, even when interlabeler reliability is relativelylow, annotated data can be leveraged to improve theconfidence in assigning labels to messages.
Annotatorscan be ranked by ?how well they agree with the group?using kappa.
Messages (or other labeled data) can beranked by ?how well the group agrees on its label?
us-ing variance or ?p*ln(p).
Annotator rankings can beused to weight ?better?
annotators greater than ?worse?annotators.
Similarly, message rankings can be used toweight ?better?
messages greater than ?worse?
mes-sages.
The weighted annotator data can be used to re-compute the message weights.
These new messageweights can then be used to recompute annotatorweights.
Repeating this alternation until the weightsshow minimal change will minimize the contributionsof unreliable annotators and poorly annotated messagesto the assignment of labels to messages, thereby increas-ing confidence in the results.
An implementation of this?sharpening?
algorithm is currently under development.AcknowledgmentsThanks to Becky Passonneau for her insightful com-ments on an intermediate draft.
This work would nothave been possible without the support and advice ofJulia Hirschberg, Owen Rambow and Lokesh Shrestha.This research was supported by a grant from NSF/KDD#IIS-98-17434.ReferencesJ.
A. Cohen.
1960.
Educational and PsychologicalMeasurement, 20(1):37-46.Klaus Krippendorff.
1980.
Content Analysis, an Intro-duction to Its Methodology.
Thousand Oaks, CA.J.
R. Landis and G.G.
Koch.
1977.
Biometrics,33(1):159-174Owen Rambow and Lokesh Shrestha and John Chenand Charles Lewis.
2004.
Summarizing EmailThreads.
Under submission.
