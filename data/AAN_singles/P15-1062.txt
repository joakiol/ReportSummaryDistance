Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 635?644,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsSemantic Representations for Domain Adaptation:A Case Study on the Tree Kernel-based Method for Relation ExtractionThien Huu Nguyen?, Barbara Plank?and Ralph Grishman?
?Computer Science Department, New York University, New York, NY 10003, USA?Center for Language Technology, University of Copenhagen, Denmarkthien@cs.nyu.edu,bplank@cst.dk,grishman@cs.nyu.eduAbstractWe study the application of word embed-dings to generate semantic representationsfor the domain adaptation problem of re-lation extraction (RE) in the tree kernel-based method.
We systematically evaluatevarious techniques to generate the seman-tic representations and demonstrate thatthey are effective to improve the general-ization performance of a tree kernel-basedrelation extractor across domains (up to7% relative improvement).
In addition,we compare the tree kernel-based and thefeature-based method for RE in a compat-ible way, on the same resources and set-tings, to gain insights into which kind ofsystem is more robust to domain changes.Our results and error analysis shows thatthe tree kernel-based method outperformsthe feature-based approach.1 IntroductionRelation Extraction (RE) is an important aspect ofinformation extraction that aims to discover thesemantic relationships between two entity men-tions appearing in the same sentence.
Previousresearch on RE has followed either the kernel-based approach (Zelenko et al, 2003; Bunescuand Mooney, 2005; Zhao and Grishman, 2005;Zhang et al, 2006; Bunescu, 2007; Qian et al,2008; Nguyen et al, 2009) or the feature-based ap-proach (Kambhatla, 2004; Grishman et al, 2005;Zhou et al, 2005; Jiang and Zhai, 2007a; Chanand Roth, 2010; Sun et al, 2011).
Usually, insuch supervised machine learning systems, it is as-sumed that the training data and the data to whichthe RE system is applied to are sampled inde-pendently and identically from the same distribu-tion.
This assumption is often violated in realityand exemplified in the fact that the performanceof the traditional RE techniques degrades signif-icantly in such a domain mismatch case (Plankand Moschitti, 2013).
To alleviate this perfor-mance loss, we need to resort to domain adaptation(DA) techniques to adapt a system trained on somesource domain to perform well on new target do-mains.
We here focus on the unsupervised domainadaptation (i.e., no labeled target data) and single-system DA (Petrov and McDonald, 2012; Plankand Moschitti, 2013), i.e., building a single sys-tem that is able to cope with different, yet relatedtarget domains.While DA has been investigated extensively inthe last decade for various natural language pro-cessing (NLP) tasks, the examination of DA forRE is only very recent.
To the best of our knowl-edge, there have been only three studies on DAfor RE (Plank and Moschitti, 2013; Nguyen andGrishman, 2014; Nguyen et al, 2014).
Of these,Nguyen et al (2014) follow the supervised DAparadigm and assume some labeled data in thetarget domains.
In contrast, Plank and Moschitti(2013) and Nguyen and Grishman (2014) workon the unsupervised DA.
In our view, unsuper-vised DA is more challenging, but more realisticand practical for RE as we usually do not knowwhich target domains we need to work on in ad-vance, thus cannot expect to possess labeled dataof the target domains.
Our current work thereforefocuses on the single-system unsupervised DA.Besides, note that this setting tries to construct asingle system that can work robustly with differ-ent but related domains (multiple target domains),thus being different from most previous studies onDA (Blitzer et al, 2006; Blitzer et al, 2007) whichhave attempted to design a specialized system forevery specific target domain.Plank and Moschitti (2013) propose to embedword clusters and latent semantic analysis (LSA)of words into tree kernels for DA of RE, whileNguyen and Grishman (2014) studies the appli-635cation of word clusters and word embeddings forDA of RE on the feature-based method.
Althoughword clusters (Brown et al, 1992) have been em-ployed by both studies to improve the performanceof relation extractors across domains, the appli-cation of word embeddings (Bengio et al, 2003;Mnih and Hinton, 2008; Turian et al, 2010) forDA of RE is only examined in the feature-basedmethod and never explored in the tree kernel-based method so far, giving rise to the first ques-tion we want to address in this paper:(i) Can word embeddings help the tree kernel-based methods on DA for RE and more impor-tantly, in which way can we do it effectively?This question is important as word embeddingsare real valued vectors, while the tree kernel-basedmethods rely on the symbolic matches or mis-matches of concrete labels in the parse trees tocompute the kernels.
It is unclear at the first glancehow to encode word embeddings into the tree ker-nels effectively so that word embeddings couldhelp to improve the generalization performance ofRE.
One way is to use word embeddings to com-pute similarities between words and embed thesesimilarity scores into the kernel functions, e.g.,by resembling the method of Plank and Moschitti(2013) that exploited LSA (in the semantic syntac-tic tree kernel (SSTK), cf.
?2.1).
We explore vari-ous methods to apply word embeddings to gener-ate the semantic representations for DA of RE anddemonstrate that semantic representations are veryeffective to significantly improve the portability ofthe relation extractors based on the tree kernels,bringing us to the second question:(ii) Between the feature-based method inNguyen and Grishman (2014) and the SSTKmethod in Plank and Moschitti (2013), whichmethod is better for DA of RE, given the recentdiscovery of word embeddings for both methods?It is worth noting that besides the approach dif-ference, these two works employ rather differentresources and settings in their evaluation, mak-ing it impossible to directly compare their perfor-mance.
In particular, while Plank and Moschitti(2013) only use the path-enclosed trees inducedfrom the constituent parse trees as the represen-tation for relation mentions, Nguyen and Grish-man (2014) include a rich set of features extractedfrom multiple resources such as constituent trees,dependency trees, gazetteers, semantic resourcesin the representation.
Besides, Plank and Mos-chitti (2013) consider the direction of relations intheir evaluation (i.e, distinguishing between rela-tion classes and their inverses) but Nguyen andGrishman (2014) disregard this relation direction.Finally, we note that although both studies evalu-ate their systems on the ACE 2005 dataset, theyactually have different dataset partitions.
In orderto overcome this limitation, we conduct an eval-uation in which the two methods are directed touse the same resources and settings, and are thuscompared in a compatible manner to achieve an in-sight on their effectiveness for DA of RE.
In fact,the problem of incompatible comparison is unfor-tunately very common in the RE literature (Wang,2008; Plank and Moschitti, 2013) and we believethere is a need to tackle this increasing confusionin this line of research.
Therefore, this is actu-ally the first attempt to compare the two methods(tree kernel-based and feature-based) on the samesettings.
To ease the comparison for future workand circumvent the Zigglebottom pitfall (Pedersen,2008), the entire setup and package is available.12 Relation Extraction ApproachesIn the following, we introduce the two relation ex-traction systems further examined in this study.2.1 Tree kernel-based MethodIn the tree kernel-based method (Moschitti, 2006;Moschitti, 2008; Plank and Moschitti, 2013), arelation mention (the two entity mentions andthe sentence containing them) is representedby the path-enclosed tree (PET), the smallestconstituency-based subtree including the two tar-get entity mentions (Zhang et al, 2006).
The syn-tactic tree kernel (STK) is then defined to computethe similarity between two PET trees (where tar-get entities are marked) by counting the commonsub-trees, without enumerating the whole frag-ment space (Moschitti, 2006; Moschitti, 2008).STK is then applied in the support vector ma-chines (SVMs) for RE.
The major limitation ofSTK is its inability to match two trees that sharethe same substructure, but involve different thoughsemantically related terminal nodes (words).
Thisis caused by the hard matches between words,and consequently between sequences containingthem.
For instance, in the following example takenfrom Plank and Moschitti (2013), the two frag-ments ?governor from Texas?
and ?head of Mary-1https://bitbucket.org/nycphre/limo-re636land?
would not match in STK although they havevery similar syntactic structures and basically con-vey the same relationship.Plank and Moschitti (2013) propose to resolvethis issue for STK using the semantic syntac-tic tree kernel (SSTK) (Bloehdorn and Moschitti,2007) and apply it to the domain adaptation prob-lem of RE.
The two following techniques are uti-lized to activate the SSTK: (i) replace the part-of-speech nodes in the PET trees by the new oneslabeled by the word clusters of the correspondingterminals (words); (ii) replace the binary similar-ity scores between words (i.e, either 1 or 0) bythe similarities induced from the latent semanticanalysis (LSA) of large corpus.
The former gener-alizes the part-of-speech similarity to the seman-tic similarity on word clusters; the latter, on theother hand, allows soft matches between wordsthat have the same latent semantic but differ insymbolic representation.
Both techniques empha-size the invariants of word semantics in differentdomains, thus being helpful to alleviate the vocab-ulary difference across domains.2.2 Feature-based MethodIn the feature-based method (Zhou et al, 2005;Sun et al, 2011; Nguyen and Grishman, 2014), re-lation mentions are first transformed into rich fea-ture vectors that capture various characteristics ofthe relation mentions (i.e, lexicon, syntax, seman-tics etc).
The resulting vectors are then fed into thestatistical classifiers such as Maximum Entropy(MaxEnt) to perform classification for RE.The main reason for the performance loss ofthe feature-based systems on new domains is thebehavioral changes of the features when domainsshift.
Some features might be very informative inthe source domain but become less relevant in thetarget domains.
For instance, some words, thatare very indicative in the source domain mightnot appear in the target domains (lexical sparsity).Consequently, the models putting high weights onsuch words (features) in the source domain willfail to perform well on the target domains.
Nguyenand Grishman (2014) address this problem for thefeature-based method in DA of RE by introduc-ing word embeddings as additional features.
Therationale is based on the fact that word embed-dings are low dimensional and real valued vec-tors, capturing latent syntactic and semantic prop-erties of words (Bengio et al, 2003; Mnih andHinton, 2008; Turian et al, 2010).
The embed-dings of symbolically different words are oftenclose to each other if they have similar semanticand syntactic functions.
This again helps to mit-igate the lexical sparsity or the vocabulary differ-ence between the domains and has proven helpfulfor, amongst others, the feature-based method inDA of RE.2.3 Tree Kernel-based vs Feature-basedThe feature-based method explicitly encapsulatesthe linguistic intuition and domain expertise forRE into the features, while the tree kernel-basedmethod avoids the complicated feature engineer-ing and implicitly encode the features into thecomputation of the tree kernels.
Which methodis better for DA of RE?In order to ensure the two methods (Plank andMoschitti, 2013; Nguyen and Grishman, 2014) arecompared compatibly on the same resources, wemake sure the two systems have access to the sameamount of information.
Thus, we follow Plankand Moschitti (2013) and use the PET trees (be-side word clusters and word embeddings) as theonly resource the two methods can exploit.For the feature-based method, we utilize allthe features extractable from the PET trees thatare standardly used in the state-of-the-art feature-based systems for DA of RE (Nguyen and Gr-ishman, 2014).
Specifically, the feature set em-ployed in this paper (denoted by FET) includes:the lexical features, i.e., the context words, thehead words, the bigrams, the number of words,the lexical path, the order of mention (Zhou et al,2005; Sun et al, 2011); and the syntactic features,i.e., the path connecting the two mentions in PETand the unigrams, bigrams, trigrams along thispath (Zhou et al, 2005; Jiang and Zhai, 2007a).Hypothesis: Assuming identical settings andresources, we hypothesize that the tree kernel-based method is better than the feature-basedmethod for DA of RE.
This is motivated becauseof at least two reasons: (i) the tree kernel-basedmethod implicitly encodes a more comprehen-sive feature set (involving all the sub-trees in thePETs), thus potentially captures more domain-independent features to be useful for DA of RE;(ii) the tree kernel-based method avoids the in-clusion of fine-tuned and domain-specific featuresoriginated from the excessive feature engineer-ing (i.e., hand-designing feature sets based on the637linguistic intuition for specific domains) of thefeature-based method.3 Word Embeddings & Tree KernelsIn this section, we first give the intuition thatguides us in designing the proposed methods.
Inparticular, one limitation of the syntactic seman-tic tree kernel presented in Plank and Moschitti(2013) (?2.1) is that semantics is highly tied tosyntax (the PET trees) in the kernel computation,limiting the generalization capacity of semanticsto the extent of syntactic matches.
If two rela-tion mentions have different syntactic structures,the two relation mentions will not match, althoughthey share the same semantic representation andexpress the same relation class.
For instance, thetwo fragments ?Tom is the CEO of the company?and ?the company, headed by Tom?
express thesame relationship between ?Tom?
and ?company?based on the semantics of their context words,but cannot be matched in SSTK as their syntac-tic structures are different.
In such a case, it isdesirable to have a representation of relation men-tions that is grounded on the semantics of the con-text words and reflects the latent semantics of thewhole relation mentions.
This representation isexpected to be general enough to be effective ondifferent domains.
Once the semantic representa-tion of relation mentions is established, we can useit in conjunction with the traditional tree kernelsto extend their coverage.
The benefit is mutual asboth semantics and syntax help to generalize rela-tion mentions to improve the recall, but also con-strain each other to support precision.
This is thebasic idea of our approach, which we compare tothe previous methods.3.1 MethodsWe propose to utilize word embeddings of the con-text words as the principal components to obtainsemantic representations for relation mentions inthe tree kernel-based methods.
Besides more tra-ditional approaches to exploit word embeddings,we investigate representations that go beyond theword level and use compositionality embeddingsfor domain adaptation for the first time.In general, suppose we are able to acquire anadditional real-valued vector Vifrom word embed-dings to semantically represent a relation mentionRi(along with the PET tree Ti), leading to the newrepresentation of Ri= (Ti, Vi).
The new kernelfunction in this case is then defined by:Knew(Ri, Rj) = (1?
?
)SSTK(Ti, Tj) + ?Kvec(Vi, Vj)where Kvec(Vi, Vj) is some standard vector ker-nel like the polynomial kernels.
?
is a trade-offparameter and indicates whether the system at-tributes more weight to the traditional SSTK or thenew semantic kernel Kvec.In this work, we consider the following meth-ods to obtain the semantic representation Vifromthe word embeddings of the context words of Ri(assuming d is the dimensionality of the word em-beddings):HEAD: Vi= the concatenation of the word em-beddings of the two entity mention heads of Ri.This representation is inherited from Nguyen andGrishman (2014) that only examine embeddingsat the word level separately for the feature-basedmethod without considering the compositionalityembeddings of relation mentions.
The dimension-ality of HEAD is 2d.According to the principle of compositional-ity (Werning et al, 2006; Baroni and Zamparelli,2010; Paperno et al, 2014), the meaning of a com-plex expression is determined by the meanings ofits components and the rules to combine them.
Westudy the following two compositionality embed-dings for relation mentions that can be generatedfrom the embeddings of the context words:PHRASE: Vi= the mean of the embeddingsof the words contained in the PET tree TiofRi.
Although this composition is simple, it is infact competitive to the more complicated methodsbased on recursive neural networks (Socher et al,2012b; Blacoe and Lapata, 2012; Sterckx et al,2014) on representing phrase semantics.TREE: This is motivated by the training of re-cursive neural networks (Socher et al, 2012a) forsemantic compositionality and attempts to aggre-gate the context words embeddings syntactically.In particular, we compute an embedding for everynode in the PET tree in a bottom-up manner.
Theembeddings of the leaves are the embeddings ofthe words associated with them while the embed-dings of the internal nodes are the means of theembeddings of their children nodes.
We use theembeddings of the root of the PET tree to representthe relation mention in this case.
Both PHRASEand TREE have d dimensions.It is also interesting to examine combinations ofthese three representations (cf., Table 1).638SIM: Finally, for completeness, we experi-ment with a more obvious way to introduceword embeddings into tree kernels, resemblingmore closely the approach of Plank and Moschitti(2013).
In particularly, the SIM method simplyreplaces the similarity scores between word pairsobtained from LSA by the cosine similarities be-tween the word embeddings to be used in theSSTK kernel.4 Experiments4.1 Dataset, Resources and ParametersWe use the word clusters trained by Plank andMoschitti (2013) on the ukWaC corpus (Baroniet al, 2009) with 2 billion words, and the C&Wword embeddings from Turian el al.
(2010)2with50 dimensions following Nguyen and Grishman(2014).
In order to make the comparisons com-patible, we introduce word embeddings into thetree kernel by extending the package provided byPlank and Moschitti (2013), which uses the Char-niak parser to obtain the constituent trees, theSVM-light-TK for the SSTK kernel in SVM, thedirectional relation classes, etc.
We utilize the de-fault vector kernel in the SVM-light-TK package(d=3).
For the feature-based method, we apply theMaxEnt classifier in the MALLET3package withthe L2 regularizer on the hierarchical architecturefor relation extraction as in Nguyen and Grishman(2014).Following prior work, we evaluate the sys-tems on the ACE 2005 dataset which involves 6domains: broadcast news (bn), newswire (nw),broadcast conversation (bc), telephone conversa-tion (cts), weblogs (wl) and usenet (un).
The unionof bn and nw (news) is used as the source domainwhile bc, cts and wl play the role of the target do-mains.
We take half of bc as the only target de-velopment set, and use the remaining data and do-mains for testing.
The dataset partition is exactlythe same as in Plank and Moschitti (2013).
Asdescribed in their paper, the target domains quitediffer from the source domain in the relation dis-tributions and vocabulary.4.2 Word Embeddings for Tree KernelWe investigate the effectiveness of different se-mantic representations (?3.1) in tree kernels by2http://metaoptimize.com/projects/wordreprs/3http://mallet.cs.umass.edu/0 0.1 0.3 0.5 0.7 0.9 146485052?F-measureFigure 1: ?
vs F-measure on PET+HEAD+PHRASEtaking the PET tree as the baseline4, and evaluatethe performance of the representations when com-bined with the baseline on the bc development set.Method P R F1PET (Plank and Moschitti, 2013) 52.2 41.7 46.4PET+SIM 39.4 37.2 38.3PET+HEAD 60.4 44.9 51.5PET+PHRASE 58.4 40.7 48.0PET+TREE 59.8 42.2 49.5PET+HEAD+PHRASE 63.2 46.2 53.4PET+HEAD+TREE 61.0 45.7 52.3PET+PHRASE+TREE 59.2 42.4 49.4PET+HEAD+PHRASE+TREE 60.8 45.2 51.9Table 1: Performance on the bc dev set for PET.
Best com-bination (HEAD+PHRASE) is denoted WED in Table 2Table 1 shows the results.
The main conclusionsinclude:(i) The substitution of LSA similarity scoreswith the word embedding cosine similarities(SIM) does not help to improve the performanceof the tree kernel method.
(ii) When employed independently, both theword level embeddings (HEAD) and the compo-sitionality embeddings (PHRASE, TREE) are ef-fective for the tree kernel-based method on DA forRE, showing a slight advantage for HEAD.
(iii) Thus, the compositionality embeddingsPHRASE and TREE seem to capture differentinformation with respect to the word level em-beddings HEAD.
We expect the combination ofHEAD with either PHRASE or TREE to furtherimprove performance.
This is the case whenadding one of them at a time.
PHRASE and TREEseem to capture similar information, combining all(last row in Table 1) is not the overall best sys-tem.
The best performance is achieved when theHEAD and PHRASE embeddings are utilized at4By using their system we obtained the same results.639nw+bn (in-dom.)
bc cts wl# System: P: R: F1: P: R: F1: P: R: F1: P: R: F1:1 PET (Plank and Moschitti, 2013) 50.6 42.1 46.0 51.2 40.6 45.3 51.0 37.8 43.4 35.4 32.8 34.02 PET+WED 55.8 48.7 52.0 57.3 45.7 50.8 54.0 38.1 44.7 40.1 36.5 38.23 PET WC 55.4 44.6 49.4 54.3 41.4 47.0 55.9 37.1 44.6 40.0 32.7 36.04 PET WC+WED 56.3 48.2 51.9 57.0 44.3 49.8 56.1 38.1 45.4 40.7 36.1 38.25 PET LSA 52.3 44.1 47.9 51.4 41.7 46.0 49.7 36.5 42.1 38.1 36.5 37.36 PET LSA+WED 55.2 48.5 51.6 58.8 45.8 51.5 54.1 38.1 44.7 40.9 38.5 39.67 PET+PET WC 55.0 46.5 50.4 54.4 43.4 48.3 54.1 38.1 44.7 38.4 34.5 36.38 PET+PET WC+WED 56.3 50.3 53.1 57.5 46.6 51.5 55.6 39.8 46.4 41.5 37.9 39.69 PET+PET LSA 52.7 46.6 49.5 53.9 45.2 49.2 49.9 37.6 42.9 37.9 38.3 38.110 PET+PET LSA+WED 55.5 49.9 52.6 56.8 45.8 50.8 52.5 38.6 44.5 41.6 39.3 40.511 PET+PET WC+PET LSA 55.1 45.9 50.1 55.3 43.1 48.5 53.1 37.0 43.6 39.9 35.8 37.812 PET+PET WC+PET LSA+WED 55.0 48.8 51.7 58.5 47.3 52.3 52.6 38.8 44.7 42.3 38.9 40.5Table 2: In-domain (first column) and out-of-domain performance (columns two to four) on ACE 2005.
Systems of the rowsnot in gray come from Plank and Moschitti (2013) (the baselines).
WED means HEAD+PHRASE.the same time, reaching an F1 of 53.4% (comparedto 46.4% of the baseline) on the development set.The results in Table 1 are obtained using thetrade-off parameter ?
= 0.7.
Figure 1 addi-tionally shows the variation of the performancewith changing ?
(for the best system on dev, i.e.,for the representation PET+HEAD+PHRASE).As we can see, the performance for ?
> 0.5 isin general better, suggesting a preference for thesemantic representation over the syntactic repre-sentation in DA for RE.
The performance reachesits peak when the suitable amounts of semanticsand syntax are combined (i.e, ?
= 0.7).In the following experiments, we use theembedding combination (HEAD+PHRASE) with?
= 0.7 for the tree kernels, denoted WED.4.3 Domain Adaptation ExperimentsIn this section, we examine the semantic rep-resentation for DA of RE in the tree kernel-based method.
In particular, we take the sys-tems using the PET trees, word clusters and LSAin Plank and Moschitti (2013) as the baselinesand augment them with the embeddings WED =HEAD+PHRASE.
We report the performance ofthese augmented systems in Table 2 for the twoscenarios: (i) in-domain: both training and test-ing are performed on the source domain via 5-foldcross validation and (ii) out-of-domain: modelsare trained on the source domain but evaluated onthe three target domains.
To summarize, we find:First, word embeddings seem to subsume wordclusters in the tree kernel-based method (compar-ing rows 2 and 4, and except domain cts) whileword embeddings and LSA actually encode dif-ferent information (comparing rows 2 and 6 forthe out-of-domain experiments) and their combi-nation would be helpful for DA of RE.Second, regarding composite kernels, givenword embeddings, the addition of the baseline ker-nel (PET) is in general useful for the augmentedkernels PET WC and PET LSA (comparing rows4 and 8, rows 6 and 10) although it is less pro-nounced for PET LSA.Third and most importantly, for all the systemsin Plank and Moschitti (2013) (the baselines) andfor all the target domains, whether word clustersand LSA are utilized or not, we consistently wit-ness the performance improvement of the base-lines when combined with word embedding (com-paring systems X and X+WED where X is somebaseline system).
The best out-of-domain perfor-mance is achieved when word embeddings are em-ployed in conjunction with the composite kernels(PET+PET WC+PET LSA for the target domainsbc and wl, and PET+PET WC for the target do-main cts).
To be more concrete, the best systemwith word embeddings (row 12 in Table 2) signif-icantly outperforms the best system in Plank andMoschitti (2013) with p < 0.05, an improvementof 3.7%, 1.1% and 2.7% on the target domains bc,cts and wl respectively, demonstrating the bene-fit of word embeddings for DA of RE in the treekernel-based method.4.4 Tree Kernel-based vs Feature-based DAof REThis section aims to compare the tree kernel-basedmethod in Plank and Moschitti (2013) and thefeature-based method in Nguyen and Grishman(2014) for DA of RE on the same settings (i.e,same dataset partition, the same pre-processing640nw+bn (in-dom.)
bc cts wlSystem: P: R: F1: P: R: F1: P: R: F1: P: R: F1:Tree kernel-based:PET+PET WC+HEAD+PHRASE 56.3 50.3 53.1 57.5 46.6 51.5 55.6 39.8 46.4 41.5 37.9 39.6Feature-based:FET+WC+HEAD 44.5 51.0 47.5 46.5 49.3 47.8 44.5 40.0 42.1 35.4 39.5 37.3FET+WC+TREE 44.4 50.2 47.1 46.4 48.7 47.6 43.7 40.3 41.9 32.7 36.7 34.6FET+WC+HEAD+PHRASE 44.9 51.6 48.0 46.0 49.1 47.5 45.2 41.5 43.3 34.7 39.2 36.8FET+WC+HEAD+TREE 45.1 51.0 47.8 46.9 48.4 47.6 43.8 39.5 41.5 34.7 38.8 36.6Table 3: Tree kernel-based in Plank and Moschitti (2013) vs feature-based in Nguyen and Grishman (2014).
All the compar-isons between the tree kernel-based method and the feature-based method in this table are significant with p < 0.05.procedure, the same model of directional relationclasses, the same PET trees for tree kernels andfeature extraction, the same word clusters and thesame word embeddings).
We first evaluate thefeature-based system with different combinationsof embeddings (i.e, HEAD, PHRASE and TREE)on the bc development set.
Based on the evalua-tion results, we then discuss the effect of the se-mantic representations on the feature-based sys-tem and the tree kernel-based system, and thencompare the performance of the two methodswhen they are augmented with their best corre-sponding embedding combinations.System P R F1B 51.2 49.4 50.3B+HEAD 55.8 52.4 54.0B+PHRASE 50.7 46.2 48.4B+TREE 53.6 51.1 52.3B+HEAD+PHRASE 53.2 50.1 51.6B+HEAD+TREE 54.9 51.4 53.1B+PHRASE+TREE 50.7 48.4 49.5B+HEAD+PHRASE+TREE 52.7 49.4 51.0Table 4: Performance of the feature-based method (dev).Table 4 presents the evaluation results on the bcdevelopment for the feature-based system whereB is the baseline feature set consisting of FETand word clusters (WC) (Nguyen and Grishman,2014).The Role of Semantic Representations Con-sidering Table 4 for the feature-based method andTable 1 for the tree kernel-based method, we seethat when combined with the HEAD embeddings,the compositionality embedding TREE is more ef-fective for the feature-based method, in contrast tothe tree kernel-based method, where the PHRASEembeddings are better.
This can be partly ex-plained by the fact that the tree kernel-basedmethod emphasizes the syntactic structure of therelation mentions, while the feature-based methodexploits the sequential structure more.
Conse-quently, the syntactic semantics of TREE are morehelpful for the feature-based method, whereas thesequential semantics of PHRASE are more usefulfor the tree kernel-based method.Performance Comparison The three best em-bedding combinations for the feature-based sys-tem in Table 4 are (listed by performance order):(HEAD), (HEAD+TREE) and (TREE), where(HEAD) is also the best word level method em-ployed in Nguyen and Grishman (2014).
Inorder to enable a fairer and clearer evaluation,when doing comparison, we use both the threebest embedding combinations in the feature-based method and the best embedding combina-tion (HEAD+PHRASE) in the tree kernel-basedmethod.
In the tree kernel-based method, we donot employ the LSA information as it comes in theform of similarity scores between pairs of words,and it is not clear how to encode this informationinto the feature-based method effectively.
Finally,we utilize the composite kernel for its demon-strated effectiveness in Section 4.3.The most important observation from the ex-perimental results (shown in Table 3) is that overall the target domains, the tree kernel-based sys-tem is significantly better than the feature-basedsystems with p < 0.05 (assuming the same re-sources and settings mentioned above).
In fact,there are large margins between the tree kernel-based and the feature-based methods in this case(i.e, about 3.7% for bc, 3.1% for cts and 2.3% forwl), clearly confirming the hypothesis about theadvantage of the tree kernel-based method overthe feature-based method on DA for RE in Section2.3.5 AnalysisThis section analyzes the output of the systems togain more insights into their operation.641Word Embeddings for the Tree-kernel basedMethod We focus on the comparison of the bestmodel in Plank and Moschitti (2013) (row 11in Table 2) (called P) with the same model butaugmented with the embedding WED (row 12 inTabel 2) (called P+WED).
One of the most inter-esting insights is that the embedding WED helpsto semantically generalize the phrases connectingthe two target entity mentions beyond the syntacticconstraints.
For instance, model P fails to discoverthe relation between ?Chuck Hagel?
and ?Viet-nam?
in the sentence (of the target domain bc):?Sergeant Chuck Hagel was seriously woundedtwice in Vietnam.?
(i.e, it returns the NONE re-lation as the prediction) as the substructure asso-ciated with ?seriously wounded twice?
does notappear with any relation in the source domain.Model P+WED, on the other hand, correctly pre-dicts the PHYS (Located) relation between thetwo entities as the PHRASE embedding of ?ChuckHagel was seriously wounded twice in Vietnam.?
(phrase X1) is very close to the embedding of thesource domain phrase: ?Stewart faces up to 30years in prison?
(phrase X2) (annotated with thePHYS relation between ?Stewart?
and ?prison?
).In fact, X2 is only the 9th closest phrase inthe source domain of X1.
The closest phrase ofX1 in the source domain is X3: the phrase be-tween ?Iraqi soldiers?
and ?herself?
in the sen-tence ?The Washington Post is reporting she shotseveral Iraqi soldiers before she was capturedand she was shot herself , too.?.
However, as thesyntactical structure of X1 is more similar to X2?s,and is remarkably different from X3 as well as theother closest phrases (ranked from 2nd to 8th), thenew kernel function Knewwould still prefer X2due to its trade-off between syntax and semantics.Tree Kernel-based vs Feature-based From theanalysis of the systems in Table 3, we find that,among others, the tree kernel-based method im-proves the precision significantly via the seman-tic and syntactic refinement it maintains.
Let usconsider the following phrase of the target domainbc: ?troops have dislodged stubborn Iraqi sol-diers?
(called Y1).
The feature-based systems inTable 3 incorrectly predict the ORG-AFF relation(Employment or Membership) between ?Iraqi sol-diers?
and ?troops?.
This is mainly due to the highweights of the features linking the words ?troop?and ?soldiers?
with the relation type ORG-AFF inthe feature-based models, which is, in turn, orig-inated from the high correlation of these wordsand the relation type in the training data of thesource domain (domain bias).
The tree kernel-based model in Table 3 successfully recognizes theNONE relation in this case.
A closer examinationshows that the phrase with the closest embeddingto Y1 in the source domain is Y2: ?Iraqi soldiersabandoned their posts?,5which is annotated withthe NONE relation between ?Iraqi soldiers?
and?their posts?.
As the syntactic structure of Y2 isalso very similar to Y1, it is not surprising that Y1is closest to Y2 in the new kernel function, conse-quently helping the tree kernel-based method workcorrectly in this case.6 Related workWord embeddings are only applied to RE recently.Socher et al (2012b) use word embeddings as in-put for matrix-vector recursive neural networks inrelation classification while Zeng et al (2014),and Nguyen and Grishman (2015) employ wordembeddings in the framework of convolutionalneural networks for relation classification and ex-traction, respectively.
Sterckx et al (2014) uti-lize word embeddings to reduce noise of trainingdata in distant supervision.
Kuksa et al (2010)present a string kernel for bio-relation extractionwith word embeddings, and Yu et al (2014; 2015)study the factor-based compositional embeddingmodels.
However, none of this work examinesword embeddings for tree kernels as well as do-main adaptation as we do.Regarding DA, in the unsupervised DA setting,Huang and Yates (2010) attempt to learn multi-dimensional feature representations while Blitzeret al (2006) introduce structural correspondencelearning.
Daum?e (2007) proposes an easy adapta-tion framework (EA) while Xiao and Guo (2013)present a log-bilinear language adaptation tech-nique in the supervised DA setting.
Unfortunately,all of this work assumes some prior (in the form ofeither labeled or unlabeled data) on the target do-mains for the sequential labeling tasks, in contrastto our single-system unsupervised DA setting forrelation extraction.
An alternative method that isalso popular to DA is instance weighting (Jiangand Zhai, 2007b).
However, as shown by Plankand Moschitti (2013), instance weighting is not5The full sentence is: ?After today?s air strikes, Iraqi sol-diers abandoned their posts and surrendered to Kurdish fight-ers.
?.642very useful for DA of RE.7 ConclusionIn order to improve the generalization of rela-tion extractors, we propose to augment the seman-tic syntactic tree kernels with the semantic rep-resentation of relation mentions, generated fromthe word embeddings of the context words.
Themethod demonstrates strong promise for the DAof RE, i.e, it significantly improves the best sys-tem of Plank and Moschitti (2013) (up to 7% rela-tive improvement).
Moreover, we perform a com-patible comparison between the tree kernel-basedmethod and the feature-based method on the samesettings and resources, which suggests that the treekernel-based method (Plank and Moschitti, 2013)is better than the feature-based method (Nguyenand Grishman, 2014) for DA of RE.
An error anal-ysis is conducted to get a deeper comprehension ofthe systems.
Our future plan is to investigate othersyntactic and semantic structures (such as depen-dency trees, abstract meaning representation etc)for DA of RE, as well as continue the comparisonbetween the kernel-based method and the feature-based method when they are allowed to exploitmore resources.ReferencesMarco Baroni and Roberto Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InEMNLP.Marco Baroni, Silvia Bernardini, Adriano Ferraresi,and Eros Zanchetta.
2009.
The WaCky wide web:a collection of very large linguistically processedweb-crawled corpora.
In Language Resources andEvaluation, pages 209?226.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
In Journal of Machine Learning Re-search 3, pages 1137?1155.William Blacoe and Mirella Lapata.
2012.
A com-parison of vector-based representations for semanticcomposition.
In EMNLP.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP.John Blitzer, Mark Dredze, and Fernando Pereira.2007.
Biographies, bollywood, boom-boxes andblenders: Domain adaptation for sentiment classi-fication.
In ACL.Stephan Bloehdorn and Alessandro Moschitti.
2007.Exploiting Structure and Semantics for ExpressiveText Kernels.
In CIKM.Peter F. Brown, Peter V. deSouza, Robert L. Mercer,Vincent J. Della Pietra, and Jenifer C. Lai.
1992.Class-based n-gram models of natural language.
InComputational Linguistics, pages 467?479.Razvan C. Bunescu and Raymond J. Mooney.
2005.
Ashortest path dependency kernel for relation extrac-tion.
In EMNLP.Razvan C. Bunescu.
2007.
Learning to extract rela-tions from the web using minimal supervision.
InACL.Yee Seng Chan and Dan Roth.
2010.
Exploiting back-ground knowledge for relation extraction.
In COL-ING.Hal Daume.
2007.
Frustratingly easy domain adapta-tion.
In ACL.Ralph Grishman, David Westbrook, and Adam Meyers.2005.
Nyu?s english ace 2005 system description.In The ACE 2005 Evaluation Workshop.Fei Huang and Alexander Yates.
2010.
Exploringrepresentation-learning approaches to domain adap-tation.
In The ACL Workshop on Domain Adaptationfor Natural Language Processing (DANLP).Jing Jiang and ChengXiang Zhai.
2007a.
A systematicexploration of the feature space for relation extrac-tion.
In NAACL-HLT.Jing Jiang and ChengXiang Zhai.
2007b.
Instanceweighting for domain adaptation in nlp.
In ACL.Nanda Kambhatla.
2004.
Combining lexical, syntac-tic, and semantic features with maximum entropymodels for information extraction.
In ACL.Pavel Kuksa, Yanjun Qi, Bing Bai, Ronan Col-lobert, Jason Weston, Vladimir Pavlovic, andXia Ning.
2010.
Semi-supervised abstraction-augmented string kernel for multi-level bio-relationextraction.
In ECML PKDD.Andriy Mnih and Geoffrey Hinton.
2008.
A scalablehierarchical distributed language model.
In NIPS.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In ECML.Alessandro Moschitti.
2008.
Kernel methods, syntaxand semantics for relational text categorization.
InCIKM.Thien Huu Nguyen and Ralph Grishman.
2014.
Em-ploying word representations and regularization fordomain adaptation of relation extraction.
In ACL.643Thien Huu Nguyen and Ralph Grishman.
2015.
Rela-tion extraction: Perspective from convolutional neu-ral networks.
In The NAACL Workshop on VectorSpace Modeling for NLP (VSM).T.
Truc-Vien Nguyen, Alessandro Moschitti, andGiuseppe Riccardi.
2009.
Convolution kernels onconstituent, dependency and sequential structuresfor relation extraction.
In EMNLP.Luan Minh Nguyen, W. Ivor Tsang, A. Kian MingChai, and Leong Hai Chieu.
2014.
Robust domainadaptation for relation extraction via clustering con-sistency.
In ACL.Denis Paperno, The Nghia Pham, and Marco Baroni.2014.
A practical and linguistically-motivated ap-proach to compositional distributional semantics.
InACL.Ted Pedersen.
2008.
Empiricism is not a matter offaith.
In Computational Linguistics 3, pages 465?470.Slav Petrov and Ryan McDonald.
2012.
Overview ofthe 2012 shared task on parsing the web.
In The FirstWorkshop on Syntactic Analysis of Non-CanonicalLanguage (SANCL).Barbara Plank and Alessandro Moschitti.
2013.
Em-bedding semantic similarity in tree kernels for do-main adaptation of relation extraction.
In ACL.Longhua Qian, Guodong Zhou, Fang Kong, QiaomingZhu, and Peide Qian.
2008.
Exploiting constituentdependencies for tree kernel-based semantic relationextraction.
In COLING.Richard Socher, Alex Perelygin, Jean Y. Wu, JasonChuang, Christopher D. Manning, Andrew Y. Ng,and Christopher Potts.
2012a.
Recursive deep mod-els for semantic compositionality over a sentimenttreebank.
In EMNLP-CoNLL.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012b.
Semantic com-positionality through recursive matrix-vector spaces.In EMNLP.Lucas Sterckx, Thomas Demeester, Johannes Deleu,and Chris Develder.
2014.
Using active learningand semantic clustering for noise reduction in dis-tant supervision.
In AKBC.Ang Sun, Ralph Grishman, and Satoshi Sekine.
2011.Semi-supervised relation extraction with large-scaleword clustering.
In ACL.Joseph Turian, Lev-Arie Ratinov, and Yoshua Bengio.2010.
Word representations: A simple and generalmethod for semi-supervised learning.
In ACL.Mengqiu Wang.
2008.
A re-examination of depen-dency path kernels for relation extraction.
In IJC-NLP.Markus Werning, Edouard Machery, and GerhardSchurz.
2006.
Compositionality of meaning andcontent: Foundational issues (linguistics & philoso-phy).
In Linguistics & philosophy.Min Xiao and Yuhong Guo.
2013.
Domain adapta-tion for sequence labeling tasks with a probabilisticlanguage adaptation model.
In ICML.Mo Yu, Matthew Gormley, and Mark Dredze.
2014.Factor-based compositional embedding models.
InThe NIPS workshop on Learning Semantics.Mo Yu, Matthew Gormley, and Mark Dredze.
2015.Combining word embeddings and feature embed-dings for fine-grained relation extraction.
InNAACL.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relation ex-traction.
In Journal of Machine Learning Research3, pages 1083?1106.Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,and Jun Zhao.
2014.
Relation classification via con-volutional deep neural network.
In COLING.Min Zhang, Jie Zhang, Jian Su, and Guodong Zhou.2006.
A composite kernel to extract relations be-tween entities with both flat and structured features.In COLING-ACL.Shubin Zhao and Ralph Grishman.
2005.
Extractingrelations with integrated information using kernelmethods.
In ACL.GuoDong Zhou, Jian Su, Jie Zhang, and Min Zhang.2005.
Exploring various knowledge in relation ex-traction.
In ACL.644
