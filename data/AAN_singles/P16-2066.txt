Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 406?411,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsPhrase Table Pruning via Submodular Function MaximizationMasaaki Nishino and Jun Suzuki and Masaaki NagataNTT Communication Science Laboratories, NTT Corporation2-4 Hikaridai, Seika-cho, Soraku-gun, Kyoto, 619-0237, Japan{nishino.masaaki,suzuki.jun,nagata.masaaki}@lab.ntt.co.jpAbstractPhrase table pruning is the act of re-moving phrase pairs from a phrase tableto make it smaller, ideally removing theleast useful phrases first.
We propose aphrase table pruning method that formu-lates the task as a submodular functionmaximization problem, and solves it byusing a greedy heuristic algorithm.
Theproposed method can scale with input sizeand long phrases, and experiments showthat it achieves higher BLEU scores thanstate-of-the-art pruning methods.1 IntroductionA phrase table, a key component of phrase-basedstatistical machine translation (PBMT) systems,consists of a set of phrase pairs.
A phrase pair is apair of source and target language phrases, and isused as the atomic translation unit.
Today?s PBMTsystems have to store and process large phrase ta-bles that contain more than 100M phrase pairs,and their sheer size prevents PBMT systems forrunning in resource-limited environments such asmobile phones.
Even if a computer has enoughresources, the large phrase tables increase turn-around time and prevent the rapid development ofMT systems.Phrase table pruning is the technique of remov-ing ineffective phrase pairs from a phrase tableto make it smaller while minimizing the perfor-mance degradation.
Existing phrase table pruningmethods use different metrics to rank the phrasepairs contained in the table, and then remove low-ranked pairs.
Metrics used in previous work arefrequency, conditional probability, and Fisher?sexact test score (Johnson et al, 2007).
Zens etal.
(2012) evaluated many phrase table pruningmethods, and concluded that entropy-based prun-ing method (Ling et al, 2012; Zens et al, 2012)offers the best performance.
The entropy-basedpruning method uses entropy to measure the re-dundancy of a phrase pair, where we say a phrasepair is redundant if it can be replaced by otherphrase pairs.
The entropy-based pruning methodruns in time linear to the number of phrase-pairs.Unfortunately, its running time is also exponentialto the length of phrases contained in the phrasepairs, since it contains the problem of finding anoptimal phrase alignment, which is known to beNP-hard (DeNero and Klein, 2008).
Therefore,the method can be impractical if the phrase pairsconsist of longer phrases.In this paper, we introduce a novel phrase ta-ble pruning method that formulates and solvesthe phrase table pruning problem as a submodu-lar function maximization problem.
A submodularfunction is a kind of set function that satisfies thesubmodularity property.
Generally, the submod-ular function maximization problem is NP-hard,however, it is known that (1 ?
1/e) optimal solu-tions can be obtained by using a simple greedy al-gorithm (Nemhauser et al, 1978).
Since a greedyalgorithm scales with large inputs, our method canbe applicable to large phrase tables.One key factor of the proposed method is itscarefully designed objective function that evalu-ates the quality of a given phrase table.
In this pa-per, we use a simple monotone submodular func-tion that evaluates the quality of a given phrasetable by its coverage of a training corpus.
Ourmethod is simple, parameter free, and does notcause exponential explosion of the computationtime with longer phrases.
We conduct experimentswith two different language pairs, and show thatthe proposed method shows higher BLEU scoresthan state-of-the-art pruning methods.4062 Submodular Function MaximizationLet ?
be a base set consisting of M elements, andg : 2?7?
R be a set function that upon the input ofX ?
?
returns a real value.
If g is a submodularfunction, then it satisfies the conditiong(X ?
{x})?
g(X) ?
g(Y ?
{x})?
g(Y ) ,where X,Y ?
2?, X ?
Y , and x ?
?
\ Y .
Thiscondition represents the diminishing return prop-erty of a submodular function, i.e., the increase inthe value of the function due to the addition ofitem x to Y is always smaller than that obtainedby adding x to any subset X ?
Y .
We say a sub-modular function is monotone if g(Y ) ?
g(X)for any X,Y ?
2?satisfying X ?
Y .
Since asubmodular function has many useful properties,it appears in a wide range of applications (Kempeet al, 2003; Lin and Bilmes, 2010; Kirchhoff andBilmes, 2014).The maximization problem of a monotone sub-modular function under cardinality constraints isformulated asMaximize g(X)Subject to X ?
2?and |X| ?
K ,where g(X) is a monotone submodular functionand K is the parameter that defines maximum car-dinality.
This problem is known to be NP-hard, buta greedy algorithm can find an approximate solu-tion whose score is certified to be (1 ?
1/e) opti-mal (Nemhauser et al, 1978).
Algorithm 1 showsa greedy approximation method the can solve thesubmodular function maximization problem undercardinality constraints.
This algorithm first setsX ?
?, and adds item x??
?
\ X that maxi-mizes g(X ?
{x?})?
g(X) to X until |X| = K.Assuming that the evaluation of g(X) can beperformed in constant time, the running time ofthe greedy algorithm is O(MK) because we needO(M) evaluations of g(X) for selecting x?thatmaximizes g(X ?
{x?})
?
g(X), and these eval-uations are repeated K times.
If we naively applythe algorithm to situations where M is very large,then the algorithm may not work in reasonablerunning time.
However, an accelerated greedyalgorithm can work with large inputs (Minoux,1978; Leskovec et al, 2007), since it can dras-tically reduce the number of function evaluationsfrom MK.
We applied the accelerated greedy al-gorithm in the following experiments, and found itAlgorithm 1 Greedy algorithm for maximizing asubmodular functionInput: Base set ?, cardinality KOutput: X ?
2?satisfying |X| = K.1: X ?
?2: while |X| < K do3: x??
arg maxx?
?\Xg(X ?
{x})?
g(X)4: X ?
X ?
{x?
}5: output Xcould solve the problems in 24 hours.
Moreover,further enhancement can be achieved by apply-ing distributed algorithms (Mirzasoleiman et al,2013) and stochastic greedy algorithms (Mirza-soleiman et al, 2015).3 Phrase Table PruningWe first define some notations.
Let ?
={x1, .
.
.
, xM} be a phrase table that has M phrasepairs.
Each phrase pair, xi, consists of a sourcelanguage phrase, pi, and a target language phrase,qi, and is written as xi= ?pi, qi?.
Phrases piandqiare sequences of words pi= (pi1, .
.
.
, pi|pi|)and qi= (qi1, .
.
.
, qi|qi|), where pijrepresents thej-th word of piand qijrepresents the j-th wordof qi.
Let tibe the i-th translation pair containedin the training corpus, namely ti= ?fi, ei?, wherefiand eiare source and target sentences, respec-tively.
Let N be the number of translation pairscontained in the corpus.
fiand eiare representedas sequences of words fi= (fi1, .
.
.
, fi|fi|) andei= (ei1, .
.
.
, ei|ei|), where fijis the j-th word ofsentence fiand eijis the j-th word of sentence ei.Definition 1.
Let xj= ?pj, qj?
be a phrase pairand ti= ?fi, ei?
be a translation pair.
We say xjappears in tiif pjis contained in fias a subse-quence and qjis contained in eias a subsequence.We say phrase pair xjcovers word fikif xjap-pears in ?fi, ei?
and fikis contained in the subse-quence that equals pj.
Similarly, we say xjcoverseikif xjappears in ?fi, ei?
and eikis contained inthe subsequence that equals qj.Using the above definitions, we describe hereour phrase-table pruning algorithm; it formulatesthe task as a combinatorial optimization problem.Since phrase table pruning is the problem of find-ing a subset of ?, we formulate the problem as asubmodular function maximization problem undercardinality constraints, i.e., the problem is finding407X ?
?
that maximizes objective function g(X)while satisfying the condition |X| = K, whereK is the size of pruned phrase table.
If g(X) isa monotone submodular function, we can applyAlgorithm 1 to obtain an (1 ?
1/e) approximatesolution.
We use the following objective function.g(X) =N?i=1|fi|?k=1log [c(X, fik) + 1]+N?i=1|ei|?k=1log [c(X, eik) + 1] ,where c(X, fik) is the number of phrase pairs con-tained in X that cover fik, the k-th word of the i-th source sentence fi.
Similarly, c(X, eik) is thenumber of phrase pairs that cover eik.Example 1.
Consider phrase table X holdingphrase pairs x1= ?
(das Haus), (the house)?,x2= ?
(Haus), (house)?, and x3=?
(das Haus), (the building)?.
If a corpusconsists of a pair of sentences f1= ?das Haus istklein?
and e1= ?this house is small?, then x1andx2appear in ?f1, e1?
and word f12= ?Haus?
iscovered by x1and x2.
Hence c(X, f12) = 2.This objective function basically gives highscores to X if it contains many words of the train-ing corpus.
However, since we take the logarithmof cover counts c(X, fik) and c(X, eik), g(X) be-comes high when X covers many different words.This objective function prefers to select phrasepairs that frequently appear in the training corpusbut with low redundantly.
This objective functionprefers pruned phrase tableX that contains phrasepairs that frequently appear in the training corpus,with no redundant phrase pairs.
We prove the sub-modularity of the objective function below.Proposition 1. g(X) is a monotone submodularfunction.Proof.
Apparently, every c(X, fik) and c(X, eik)is a monotone function ofX , and it satisfies the di-minishing return property since c(X ?
{x}, fik)?c(X, fik) = c(Y ?
{x}, fik) ?
c(Y, fik) for anyX ?
Y and x 6?
Y .
If function h(X) is mono-tone and submodular, then ?
(h(X)) is also mono-tone and submodular for any concave function?
: R 7?
R. Since log(X) is concave, everylog[c(X, fik)+1] and log[c(X, eik)+1] is a mono-tone submodular function.
Finally, if h1, .
.
.
, hnare monotone and submodular, then?ihiis alsomonotone and submodular.
Thus g(X) is mono-tone and submodular.Computation costs If we know all countsc(X, fik) and c(X, eik) for all fik, eik, then g(X?
{x}) can be evaluated in time linear with the num-ber of words contained in the training corpus1.Thus our algorithm does not cause exponentialexplosion of the computation time with longerphrases.4 Evaluation4.1 SettingsWe conducted experiments on the Chinese-English and Arabic-English datasets used in NISTOpenMT 2012.
In each experiment, English wasset as the target language.
We used Moses (Koehnet al, 2007) as the phrase-based machine transla-tion system.
We used the 5-gram Kneser-Ney lan-guage model trained separately using the EnglishGigaWord V5 corpus (LDC2011T07), a monolin-gual corpus distributed at WMT 2012, and GoogleWeb 1T 5-gram data (LDC2006T13).
Wordalignments are obtained by running giza++ (Ochand Ney, 2003) included in the Moses sys-tem.
As the test data, we used 1378 segmentsfor the Arabic-English dataset and 2190 seg-ments for the Chinese-English dataset, where alltest segments have 4 references (LDC2013T07,LDC2013T03).
The tuning set consists of about5000 segments gathered from MT02 to MT06evaluation sets (LDC2010T10, LDC2010T11,LDC2010T12, LDC2010T14, LDC2010T17).
Weset the maximum length of extracted phrases to 7.Table 1 shows the sizes of phrase tables.
Follow-ing the settings used in (Zens et al, 2012), wereduce the effects of other components by usingthe same feature weights obtained by running theMERT training algorithm (Och, 2003) on full sizephrase tables and tuning data to all pruned tables.We run MERT for 10 times to obtain 10 differ-ent feature weights.
The BLEU scores reportedin the following experiments are the averages ofthe results obtained by using these different fea-ture weights.We adopt the entropy-based pruning methodused in (Ling et al, 2012; Zens et al, 2012) asthe baseline method, since it shows best BLEU1Running time can be further reduced if we compute theset of words covered by each phrase pair xibefore executingthe greedy algorithm.408Language Pair Number of phrase pairsArabic-English 234MChinese-English 169MTable 1: Phrase table sizes.scores as per (Zens et al, 2012).
We used the pa-rameter value of the entropy-based method sug-gested in (Zens et al, 2012).
We also comparedwith the significance-based method (Johnson etal., 2007), which uses Fisher?s exact test to calcu-late significance scores of phrase pairs and prunesless-significant phrase pairs.4.2 ResultsFigure 1 and Figure 2 show the BLEU scores ofpruned tables.
The horizontal axis is the number ofphrase pairs contained in a table, and the verticalaxis is the BLEU score.
The values in the figureare difference of BLEU scores between the pro-posed method and the baseline method that showshigher score.
In the experiment with the Arabic-English dataset, both methods can remove 80% ofphrase pairs without losing 1 BLEU point, and theproposed method shows better performance thanthe baseline methods for all table sizes.
The differ-ence in BLEU scores becomes larger when tablesizes are small.
In the experiment on the Chinese-English dataset, both methods can remove 80% ofphrase pairs without losing 1 BLEU point, and theproposed method also shows comparable or betterperformance.
The difference in BLEU scores alsobecomes larger when table sizes are small.Figure 3 shows phrase table sizes in the bina-rized and compressed phrase table format used inMoses (Junczys-Dowmunt, 2012).
The horizon-tal axis is the number of phrase pairs contained inthe table, and the vertical axis is phrase table size.We can see that there is a linear relationship be-tween phrase table sizes and the number of phrasepairs.
The original phrase table requires 2.8GBmemory.
In contrast, the 90% pruned table onlyrequires 350MB of memory.
This result shows theeffectiveness of phrase table pruning on reducingresource requirements in practical situations.5 Related WorkPrevious phrase table pruning methods fall intotwo groups.
Self-contained methods only useresources already used in the MT system, e.g.,training corpus and phrase tables.
Entropy-based100 101 102 103Phrase pairs [M]3234363840BLEU[%]0.870.870.49 0.190.02 0.12 0.050.00ProposedEntropyFisherFigure 1: BLEU score as a function of the numberof phrase pairs (Arabic-English).100 101 102 103Phrase pairs [M]182022242628BLEU[%]0.400.390.32 0.090.10 -0.14 -0.03 0.00ProposedEntropyFisherFigure 2: BLEU score as a function of the numberof phrase pairs (Chinese-English).methods (Ling et al, 2012; Zens et al, 2012), asignificance-based method (Johnson et al, 2007),and our method are self-contained methods.
Nonself-contained methods exploit usage statistics forphrase pairs (Eck et al, 2007) and additional bilin-gual corpora (Chen et al, 2009).
Since self con-tained methods require additional resources, it iseasy to apply to existing MT systems.Effectiveness of the submodular functions max-imization formulation is confirmed in various NLPapplications including text summarization (Linand Bilmes, 2010; Lin and Bilmes, 2011)and training data selection for machine transla-tion (Kirchhoff and Bilmes, 2014).
These methodsare used for selecting a subset that contains impor-tant items but not redundant items.
This paper canbe seen as applying the subset selection formula-tion to the phrase table pruning problem.6 ConclusionWe have introduced a method that solves thephrase table pruning problem as a submodularfunction maximization problem under cardinal-409100 101 102 103Phrase pairs [M]101102103104Tablesize[MB]ProposedEntropyFisherFigure 3: Moses compact phrase table size as afunction of the number of phrase pairs (Arabic-English).ity constraints.
Finding an optimal solution ofthe problem is NP-hard, so we apply a scalablegreedy heuristic to find (1 ?
1/e) optimal solu-tions.
Experiments showed that our greedy al-gorithm, which uses a relatively simple objec-tive function, can achieve better performance thanstate-of-the-art pruning methods.Our proposed method can be easily extended byusing other types of submodular functions.
Theobjective function used in this paper is a simpleone, but it is easily enhanced by the addition ofmetrics used in existing phrase table pruning tech-niques, such as Fisher?s exact test scores and en-tropy scores.
Testing such kinds of objective func-tion enhancements is an important future task.ReferencesYu Chen, Martin Kay, and Andreas Eisele.
2009.
In-tersecting multilingual data for faster and better sta-tistical translations.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics (NAACL-HLT), pages128?136.John DeNero and Dan Klein.
2008.
The complex-ity of phrase alignment problems.
In Proceedingsof the 46th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies (ACL-HLT), pages 25?28.Matthias Eck, Stephan Vogel, and Alex Waibel.
2007.Translation model pruning via usage statistics forstatistical machine translation.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (NAACL-HLT), pages 21?24.Howard Johnson, Joel Martin, George Foster, andRoland Kuhn.
2007.
Improving translation qual-ity by discarding most of the phrasetable.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 967?975.Marcin Junczys-Dowmunt.
2012.
Phrasal rank-encoding: Exploiting phrase redundancy and trans-lational relations for phrase table compression.
ThePrague Bulletin of Mathematical Linguistics, 98:63?74.David Kempe, Jon Kleinberg, and?Eva Tardos.
2003.Maximizing the spread of influence through a socialnetwork.
In Proceedings of the 9th ACM SIGKDDinternational conference on Knowledge discoveryand data mining (KDD), pages 137?146.Katrin Kirchhoff and Jeff Bilmes.
2014.
Submod-ularity for data selection in machine translation.In Proceedings of the 2014 Conference on Em-pirical Methods in Natural Language Processing(EMNLP), pages 131?141.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, Alexan-dra Constantin, and Evan Herbst.
2007.
Moses:Open source toolkit for statistical machine transla-tion.
In Proceedings of the 45th Annual Meeting ofthe Association for Computational Linguistics Com-panion Volume Proceedings of the Demo and PosterSessions, pages 177?180.Jure Leskovec, Andreas Krause, Carlos Guestrin,Christos Faloutsos, Jeanne VanBriesen, and NatalieGlance.
2007.
Cost-effective outbreak detection innetworks.
In Proceedings of the 13th ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining (KDD), pages 420?429.Hui Lin and Jeff Bilmes.
2010.
Multi-document sum-marization via budgeted maximization of submod-ular functions.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics (NAACL-HLT), pages 912?920.Hui Lin and Jeff Bilmes.
2011.
A class of submodu-lar functions for document summarization.
In Pro-ceedings of the 49th Annual Meeting of the Associ-ation for Computational Linguistics: Human Lan-guage Technologies (ACL-HLT), pages 510?520.Wang Ling, Jo?ao Grac?a, Isabel Trancoso, and AlanBlack.
2012.
Entropy-based pruning for phrase-based machine translation.
In Proceedings ofthe 2012 Joint Conference on Empirical Meth-ods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 962?971.Michel Minoux.
1978.
Accelerated greedy algorithmsfor maximizing submodular set functions.
In Pro-ceedings of the 8th IFIP Conference on OptimizationTechniques, pages 234?243.410Baharan Mirzasoleiman, Amin Karbasi, Rik Sarkar,and Andreas Krause.
2013.
Distributed submodularmaximization: Identifying representative elementsin massive data.
In Advances in Neural InformationProcessing Systems (NIPS), pages 2049?2057.Baharan Mirzasoleiman, Ashwinkumar Badanidiyuru,Amin Karbasi, Jan Vondr?ak, and Andreas Krause.2015.
Lazier than lazy greedy.
In Proceedings ofthe 29th AAAI Conference on Artificial Intelligence(AAAI), pages 1812?1818.George L Nemhauser, Laurence A Wolsey, and Mar-shall L Fisher.
1978.
An analysis of approximationsfor maximizing submodular set functionsi.
Mathe-matical Programming, 14(1):265?294.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting of the Association for Com-putational Linguistics (ACL), pages 160?167.Richard Zens, Daisy Stanton, and Peng Xu.
2012.
Asystematic comparison of phrase table pruning tech-niques.
In Proceedings of the 2012 Joint Confer-ence on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 972?983.411
