SE(IMENTING A SENTENf,I?
INTO MOItl)IIEM1,;SUSING STNI' ISTIC INFOI{MATION BI,TFWEEN WORI )SShiho NobesawaJunya Tsutsumi, Tomoaki Nitta, Kotaro One, Sun Da Jiang, M~Lsakazu NakanishiN;tkanishi L;d)oratoryFaculty of Science and Technology, Keio UniversityABSTRACTThis paper is on dividing non-separated language sen-tences (whose words are not separated from each otherwith a space or other separaters) into morphemesusing statistical information, not grammatical infor-mation which is often used in NLP.
In this paperwe describe our method and experimental result onJapanese and Chinese se~,tences.
As will be seen inthe body of this paper, the result shows that this sys-tent is etlicient for most of tile sentences.1 INTRODUCTION AND MOTIVAT IONAn English sentence has several words and those wordsare separated with a space, it is e~usy to divide anEnglish sentence into words.
I\[owever a a apalmse sen-tence needs parsing if you want to pick up the words inthe sentence.
This paper is on dividing non-separatedlanguage sentences into words(morphemes) withoutusing any grammatical information.
Instead, this sys-tem uses the statistic information between morphenwsto select best ways of segmenting sentences in non-separated languages.Thinldng about segmenting a sentence into pieces,it is not very hard to divide a sentence using a cer-tain dictionary for that.
The problem is how to de-cide which 'segmentation' the t)est answer is.
For ex-aml)le , there must be several ways of segmenting aJapanese sentence written in lliragana(Jal)a,lese al-phabet).
Maybe a lot more than 'several'.
So, tomake the segmenting system useful, we have to cot>sider how to pick up the right segmented sentencesfrom all the possible seems-like-scgrne, ted sentences,This system is to use statistical inforn,ation be-tween morphemes to see how 'sentence-like'(how 'likely'to happen a.s a sentence) the se.gmented string is.
Toget the statistical association between words, mutualinformation(MI) comes to be one of the most inter-esting method.
In this paper MI is used to calculatethe relationship betwee.n words found ill the given sen-tence.
A corpus of sentences i used to gain the MI.
'Fo implement this method, we iml)lemented a sys-tem MSS(Morphological Segmentation using Statisti-cal information).
What  MSS does is to find the bestway of segmenting a non-separated language, sentenceinto morphemes without depending on granamatiealinformation.
We can apply this system to many lan-guages.~2 ) / \ [ORPHOLOGICAL  ANALYS IS2.1  What ;  a Morpho log ica l  Ana lys i s  I sA morpheme is the smallest refit of a string of char-acters which has a certain linguistic l/leaning itself.
Itincludes both content words and flmction words, inthis l)aper the definition of a morl)heme is a string ofcharacters which is looked u I) in tile dictionary.Morphoh)gical analysis is to:l) recognize the smallest units making up tile givensentellceif the sentence is of a l|on-separated hmguage,divide the sentence into morphenms (auto-matic segmentation), and2) check the morlflmmes whether they are the rightunits to make up the sentence.2 .2  Segment ing  MethodsWe have some ways to segment a non-separated sen-tence into meaningflll morphemes.
These three meth-ods exl)lained below are the most popular ones to seg-ment ,I apanese sentences.?
The  longest -sc 'gment  method:l~,ead the given sentence fi'om left to right andcut it with longest l)ossible segment.
For exam-pie, if we get 'isheohl' first we look for segmentswilich uses the/ i rs t  few lette,'s in it,'i' and 'is'.it is ol)vious that 'i';' is loIlger thall 'i', SO tilesystem takes 'is' as the segment.
Then it triesthe s;tllle method to find the segnlents in 'heold'and tinds 'he' and 'old'.The, least-bunsetsu egment ing  m(',thod:Get al the possible segmentations of the inputsentence and choose the segmentation(s) whichhas least buusetsu in it.. 'l'his method is to seg:-ment Japanese sentence.s, which have contentwords anti function words together in one bun-setsu most of the time.
This method helps not tocut a se, ntenee into too small meaningless pieces.Lettm'-tyl)e, segment ing  method:In Japanese language we have three kinds of let-ters called Iliragana, Katakana and Kanji.
This227method divides a Japanese sentence into mean-ingful segments checking the type of letters.2.3 The  Necess i ty  o f  Morpho log ica lAnalys isWhen we translate an English sentence into anotherlanguage, the easiest way is to change the words inthe sentence into the corresponded words in the tar-get language.
It is not a very hard job.
All we haveto do is to look up the words in the dictionary, flow-ever when it comes to a non-separated language, it isnot as simple.
An non-separated language does notshow the segments included in a sentence.
For ex-ample, a Japanese sentence does not have any spacebetween words.
A Japanese-speaking person can di-vide a Japanese sentence into words very easily, how-ever, without arty knowledge in Japanese it is im-possible.
When we want a machine to translate annon-separated language into another language, firstwe need to segment he given sentence into words.Japanese is not the only language which needs themorphological segmentation.
For example, Chineseand Korean are non-separated too.
We can apply thisMSS system to those languages too, with very simplepreparation.
We do not have to change the system,just prepare the corpus for the purpose.2.4 P rob lems o f  Morpho log ica lAnalys isThe biggest problems through the segmentation of annon-separated language sentence are the ambiguityand unknown words.For example,niwanihaniwatorigairu.~: ?-?2 N ~: w6niwa niha niwatori ga iruA cock is in the yard./E I,c t.~ -<NI ?
'0 ~: v, 6 oniwa niha niwa tori ga iruTwo birds are in the yard.1~ tc *gN ~ ~ .a: N 7oniwa ni haniwa tori ga iruA clay-figure robber is in the yard.Those sentences are all made of same strings but theincluded morphemes are different.
With dill>rent seg-ments a sentence can have several meanings.
Japaneseh~ three types of letters: I\[iragana, Katakana andKanji.
l I i ragana and Katakana are both phoneticsymbols, and each Kanji letters has its own mean-ings.
We can put several Kanji letters to one l l i raganaword.
This makes morphological analysis of Japanesesentence very difficult.
A Japanese sentence can havemore than one morphological segmentation and it isnot easy to figure out which one makes sense.
Eventwo  or  n lo re  seg lnentat ion  can  be  ' cor rec t '  lbr  one  sen-tence.To get the right segmentation of a sentence onemay need not only morphological nalysis but also se-mantic analysis or grammatical parsing.
In this paperno grammatical information is used arid MI betweenmorphemes becomes the key to solve this problem.rio deal with unknown words is a big problem innatural language processing(NLP) too.
To recognizeunknown segments in tim sentences, we have to dis-cuss the likelihood of tim unknown segment being alinguistic word.
In this pal)er unknown words are notacceptable as a 'morpheme'.
We define that 'mor-pheme' is a string of characters which is registered inthe dictionary.3 CALCULAT ING T I lE  SCORES OFSENTENCES3 .1  Scores  o f  SentencesWhen the system searches the ways to divide a sen-tence into morphemes, more than one segmentationcome out most of the time.
What we want is one(o r  more)  'correct' segmeutation and we do not needany other possibilities.
If there arc many ways of seg-,nenting, we need to select the best one of them.
Forthat purpose the system introduced the 'scores of sen-tences'.3.2 Mutua l  In fo rmat ionA mutual information(MI)\[1\]\[2\]\[3\] is tile informationof the ~ussociation f several things.
When it comes toNLI', M I is used I.o see the relationship between two(or more) certain words.The expression below shows the definition of theMI for NI, P:l'(wl, w2)Ml(wt ;w2) = 1o9 l'(Wl )P(w2) (t)lo i : a wordP(wi)  : the probabi l i ty wl appears  in a corpusP(wl ,w,2) : the probabi l i ty w~ and 'w2 comes outtogether  in a corpusTiffs expression means that when wl and w.2 hasa strong association between them, P(wt)P(w~) <<P(wt,w2) i.e.
MI(wl,w2) >> 0.
When wl and w~do not have any special association, P(w,)P(w.a)P(wl,w2) i.e.
Ml(wl,'w2) ~ O.
And wl,en wx andw2 come out together very rarely, P(wl)P(w2) >>,'(~,,, ,,,~) i.e.
M X(w,,,~,~) << 0.2283.3 Ca lcu la t ing  the  Score  o f  a SentenceUsing the words in the given dictionary, it is easy tomake up a 'sentence'.
llowever, it is hard to con-sider whether the 'sentence' is a correct one or not.The meaning of 'correct sentence' is a sentence whichmakes sense.
For example, 'I am Tom.'
can makesense, however, 'Green the adzabak arc the a ranfour.'
is hardly took ms a meaningful sentence.
'Fhescore is to show how 'sentence-like' the given string ofmorphemes i .
Segmenting ~t non-sel)arated languagesentence, we often get a lot of meaningless strings ofmorphemes.
To pick up secms-likc-mea,fingfid stringsfrom the segmentations, we use MI.Actually what we use in tim calculation is not l, hereal MI described in section 3.2.
The MI expressionin section 3.2 introduced the bigrams.
A bigram is apossibility of having two certain words together in acorpus, as you see in the expression(l).
Instead of thebigram we use a new method named d-bigram here inthis paper\[3\].3.3.1 D-b igramThe idea of bigrams and trigraiT~s are often used inthe studies on NLP.
A bigram is the information ofthe association between two certain words and a tri-gram is the information among three.
We use a newidea named d-bigram in this paper\[3\].
A d-bigram isthe possibility that two words wt and w2 come outtogether at a distance of d words in a corpus.
Forexample, if we get 'he is Tom' as input sentence, wehave three d-bigram data:('he' 'is' 1)(' is' 'Tom' 1)('he' 'Tom' 2)('he' 'is' 1) means the information of the associationof the two words 'tie' and 'is' appear at the distanceof 1 word in the corpus.3.4 Calcu la t ionThe expression to calculate the scores between twowords is\[3\]:t'(wl, w~, d) Mid(w1, w,2, d) = 1o9~~ (2)lu i : ;t wordd : d i s tance  of  the  two  words  Wl and  w2P(wi )  : the  poss ib i l i ty  the  wm'd wl  appearsin the  coq)usP (w l ,w2,d)  : the  poss ib i l i ty  w l  and  w2 eoll'le outd words  away f l 'om each  o therin the  corpusAs the value of Mid gets bigger, the more thosewords have the ,association.
And the score of a sen-tence is calculated with these Mid data(expression(2)).The definition of the sentence score is\[l\]:ia(W)= 9 9 Mia(wi,w'+ d,d)d-' (a)i :0  d : ld : d is tance  of  the two  wordsm : d is tance  l im i t?1.
: the  llUllti|lel" Of Wol'ds il l t i le Selttel lCeI~ll : it se l t tencewi : The  i - th  morpheme in the  sentence  I~VThis expression(3) calculates the scores with thealgoritlmt below:1) Calculate Mld of every pair of words included inthe given sentence.2) Give a certain weight accordiug to the distance, dto all those Mid.3) Sum up those 3~7~.
The sum is the score of thesentence.Church and l lanks said in their pN)er\[1\] that theinformation between l.wo remote wo,'ds h~s less mean-ing in a sentence when it comes to the semantic analy-sis.
According to the idea we l)ut d 2 in the expressionso that nearer pair can be more effective in calculatingthe score of the sentence.4 Tns  SYSTSM MSS4.1 Overv iewM,qS takes a l l iragana sentence as its input.
First,M,qS picks Ul) the morphemes found ill the giwm sen-tence with checking the dictionary.
The system readsthe sentence from left to rigltt, cutting out every pos-sibility.
Each segment of the sentence is looked up inthe dictionary and if it is found in the dictionary thesystem recognize the segnlent as a morpheme.
Thosemorphemes are replaced by its corresponded Kanji(orll iragana, Katakana or mixed) morpheme(s).
As itis tohl in section 2.4, a l l iragana morpheme can haveseveral corresponded l(anji (or other lettered) mor-phemes.
In that case all the segments correspondedto the found l | i ragana morpheme, are memorized asmorl)hemes found in the sentence,.
All the found mor-phemes are nunfl)ered by its position in the sentence.After picking Illl all the n,orphenu.
's in I.he sentencethe system tries to put them together mtd brings themup back to sentence(tat)h~ I).\[nl)ut a l l iragana sentence.Cut out t, he morphemes.lIMake up sentences with the morphemes.tICalculate the score of sentencesusing the mutual information.gCompare.
the scores of all the.
made-up sentencesand get the best-marked oneas the most 'sentence-like' sentence.Then the system compares those sentences madeup with found morl)he.mes and sees which one is the229Table 1: MSS example0 1 2 3 44 5 6 7 8IT.
~ "98 9 10 11 12(('~t/~" 03) ( '~"  12) ('ff~" 23) (" L'23)( '~"  ad) ('1:" 4 s ) ( '~"  , l s ) ( '~ '67){' ' )"  78)('|77-" 89) (' l , '" 910)("R"a" 911) ( '~'~" 911)( '~" 1112))( '~"  0a)1( '~"  a4)1( '~"  4s )lfai led1( '$~"  , e)1 (-~" sg)!
('~,~" 9 1o)1failed1( 'm~" o tl)ll~?ceptedIt('P."
l l  l~}taccepted( ( '~ / , ' '~"  " ,~"  " t : "  "N--a" "~ ' )most 'sentence-like'.
For that purpose this system cal-culate the score of likelihood of each sentences(section3.4).4.2 The  CorpusA corpus is a set of sentences, These sentences areof target language.
For example, when we apply thissystem to Japanese morphological nalysis we need acorpus of Japanese sentences which are already seg-mented.The corpus prepared for the paper is the trans-lation of English textbooks for Japanese junior highschool students.
The reason why we selected juniorhigh school textbooks is that the sentences in the text-books are simple and do not include too many words.This is a good environment for evaluating this system.4.3 The  Dic t ionaryThe dictionary for MSS is made of two part.
One isthe heading words and the other is the morphemescorresponded to the headings.
There may be morethan one morphemes attached to one heading word.The second part which has morphemes i of type list,so that it can have several morphemes.Japanese : ( "  I , ,~"  (" ~: ~ .... ~:\[ o "))heading word morphemesChinese : ( " t iny"  (" ~,, .
.
.
.
~t" ) )heading word morpherne~5 RESULTSImplement MSS to all input sentences and get thescore of each segmentation.
After getting the listof segmentations, look for the 'correct' segmented-sentence and see where in the list tile right one is.The data shows the scores the 'correct' segmentationsgot(table 2).Table 2: Experiment in Japanesecorpusdictionaryi nputnumber  ofinput sentencedistance limitabout 630 J~tp,'tnese ntences(with three kinds of letters mixed)about 1500 heading words(includes morphemesnot in tile corpus)lion-segmented Ja.p;~nese lltencesusing lllragana onlyabout 100 e~tch5~ -V~scorea 99%loo%7 100%95%E 80%2nd best T ~ 3rd best100% 100%100 % 100 %100% :100%98 % 98 %90 % 95 %the very sentences in tile corpusreplaced one rnorllheme in the sentence(the buried morpheme is in the corpus)replaced one morpheme in the sentence(tile buried morpbeme is not in the corpus)sentences not in the corpus(the morphemes are all in tim corpus)sentences not in the corpus(include morphemes not; in the corpus)5.1 Ext )e r iment  in  JapaneseAccording to the experimental results(table 2), it isobvious that MSS is w.'ry useful.
The table 2 showsthat most of the sentences, no matter whether thesentences are in the.
corpus or not, are segmented cor-rectly.
We find the right segmentation getting thebest score in the list of possible segmentations, c~is tile data when the input sentences are in corpus.That is, all the 'correct' morphemes have associationbetween each other.
That have a strong effect in cal-culating the sco,'es of sentences.
The condition is al-most same for fl and 7.
Though the sentence has oneword replaced, all other words in the sentence haverelationship between them.
Tim sentences in 7 in-elude one word which is not in the corpus, but stilltile 'correct' sentence can get the best score amongthe possibilities.
We can say that the data c~, fl and7 are very successfld.230llowever, we shouhl remember that not all the sen-tences in the given corpus wouht get the best scorethrough the list.
MSS does trot cheek the corpus itselfwhen it calculate the score, it just use the Mid, theessential information of the corpus.
That is, whetherthe input sentence is written in the corpus or notdoes not make any effect in calculating scores directly.Ilowever, since MSS uses Mid to calculate the.
scores,the fact that every two morphemes in the sentencehave connection between them raises the score higher.When it comes to the sentences which are not incorpus themselves, the ratio that the 'correct' sen-tence get the best score gets down (see table 2, data~, e).The sentences of 6 and g are not found in the cor-pus.
Even some sentences which are of spoken lan-guage and not grammatically correct are included inthe input sentences.
It can be said that those ~ ande sentences arc nearer to the real worhl of Japaneselanguage.
For ti sentences we used only morphemeswhich are in the corpus.
That means that all tim mor-phenres used in the 5 sentences have their own MI,I.And e sentences have both morphemes it( the corpusand the ones not in the corpus.
The morphemes whicharc not in the corpus do not have any Ml(l. Table 2shows that MSS gets quite good result eve(, thoughthe input sentences arc not in the corpus.
MSS do nottake the necessary information directly from the co>pus and it uses the MIa instead.
This method makesthe information generalize.d and this is the reason why5 and e can get good results too.
Mid comes to }>ethe key to use the effect of the MI between morphemesindirectly so that wc can put the information of themssoeiation between morphemes to practical use.
Thisis what we expected and MSS works successfldly atthis point.5.2 The CorpusIn this paper we used the translation of English text:books for Japanese junior high school students.
Pri-mary textbooks are kiud of a closed worhl which havelimited words in it an<l the included sentences aremostly in some lixed styles, in good graummr.
Thecorpus we used in this pal)er has about 630 sentenceswhich have three types of Japanese letters all mixed.This corpus is too small to take ms a model of the ,'ealworld, however, for this pal>e( it is big enough.
Actu-ally, the results of this paper shows that this systemworks efficiently even though the corpus is small.The dictionary an<l the statistical information aregot from the given corpus.
So, the experimental re=suit totally depends on the corpus.
That is, selectingwhich corpus to take to implement, we can use I.hissystem ill many purposes(section 5.5).5.3 Comparison with the OtherMethodsIt is not easy to compare this system with other seg-,nenting methods.
We coral)are with tile least-bunsetsumethod here ill this paper.The least-bunselsv method segment the given sen-tences into morphemes and fin(l the segmentationswith least bunselsu.
This method makes all the seg-mentation first an(l selects the seems-like-best seg-mentations.
This is the same way MSS does.
Thedifference is that the least-bdnsetsv method checkesthe nmnber of tile bumselsu instead of calculating thescores of sen(el ites.Let us think about implementing a sentence themorl)hcmes are l,ot in the dictionary.
That meansthat the morphemes do not have any statistical in-formations between them.
In this situation MSS cannot use statistical informations to get the scores.
Ofcourse MSS caliculate the scores of sentences accord:ing to tile statistical informations between given mor-phemes, llowe.ver, all the Ml,l say that they have noassociation I)etween t\]le (~lorpherlles.
When there is nopossibility that the two morl>hemes appears togetherill the corpus, we give a minus score ~s tit('.
Ml,t wdue,so, as the result, with more morphemes the score ofthe+ sentence gets lower.
That is, tire segmentationwhich has less segments ill it gets better scores.
Nowcompare it with the least-bunsetsu method.
With us-ing MSS the h.'ast-morpheme segme.ntations are se-lected as the goo(I answer, q'hat is tile same waythe least-bunsetsu method selects the best one.
'\['hismeans that MSS and the least-bttnscts.le m thod havethe same efficiency when it comes to the sentenceswhich morl(hemes are not in the corpus.
It is obviousthat when the sentence has morphemes in the corpusthe ellicie.ncy of this systern gets umch higher(table2).Now it is proved that MSS is, at least, as etli:cicnt as the least-b'unsets'~ nmthod, no matter whatsentence it takes.
We show a data which describesI.his(tabh~ 3).
"Fable 3 is a good exanq)le of the c;use whelL the.input sentence has few morphemes which are in thecorl)uS.
This dal.a shows that in I.his situal.ion I.here isan outstanding relation between the number of mor-l)hemes and the scores of the segmented se.ntenees.This example(table 3) has an ambiguity how to seg-ment the sentence using the registere(l morphemes,and all the morphemes which causes the alnbiguityare not in the given (:orpus.
Those umrl)hemes notin the corpus do not have any statistical informationbetweel, them and we have no way to select which isbett<.'r.
So, the scores of sentences are Ul) to the lengthof the s<~gmented sentence, that is, the number howmany morl)hemes the sentence has.
'\['he segmentedsentence which has least segments gets the best score,since MSS gives a minus score for unknown mssocia-tion between morphemes.
That means that with moresegments in the sentence the score gets lower.
This sit-ZT/Table 3: MSS and The least-bvnselsu methodinput : a non-segmentedJapanese tliragana sentencenot in the corpusall unknown morphemes in the sentenceare registered in the (lictionary(some morphemes in the corpusare included)" sumomo mo n lon lo  h ie  memo no  ilCh\]the  number  o fthe  morphemes  6 7 8 9 10the  scores  ofthe  sentences  -65 ,0  -79 .6  -9,1.3 -108 .9  -123.5the  number  o ft i le  segmented  5 20  21 8 1sentencest i le  tcor rec t lsegmentat ion  ~k"MSS Ot i le  leas t -bunsetsu 0methodmorphemes included : " ?
.
.
.
.
~2 "in the corpus : " no  .
.
.
.
I l l( l l lO "morphemes not included : " IAI .
.
.
.
~4!.
~ "in the corpus : " uchi .
.
.
.
sunm "" sumomo " *' h ie  j~" ~t"'P n lOUlO ~puation is resemble to the way how the least-bunseisumethod selects the answer.5.4 Experiment in ChineseThe theme of tiffs paper is to segment non-separaLe(\]language sentences into morphemes.
In this paper wedescribed on segmentation f Japanese non-segmentedsentences only but we are working on Chinese sen-tences too.
This MSS is not for Japanese only.
It canbe used for other non-separated languages too.
"lbimplement for other languages, we just need to pre-pare the corpus for that  and make up the dictionaryfrom it.l lere is the example of implementing MSS for Chi-nese language(table 4).
The input is a string of char-acters which shows the pronounciations of a Chinesesentence.
MSS changes it into Chinese character sen-teces, segmenting the given string.5.5 Changing the CorpusTo implement iffs MSS system, we only need a ee lpus.
The dictionary is made from the corpus.
ThisTal)le 4: Experiment in Chineseinput : nashiyizhangditu.correct answer output sentences scores-~ )J\[~ ~: - - ,~  ; t~.
15.04735)Jl~ ~!
- -~  .
t t~\ ] .
-14.80836)JI~ {0~ --'\]~ ~1~.
-14.80836gives MSS system a lot of usages and posibilities.Most of the NLP systems need grammatical i,ffofmalleus, and it is very hard to make up a certaingrammatical rule to use in a NLP.
The corpus MSSneeds to implement is very easy to get.
As it is de-scribed in the previous ection, a corpus is a set of realsentence.s.
We can use IVISS in other languages or inother purposes just getting a certain corpus for thatand making up a dictionary from the corpus.
That  is,MSS is available in many lmrposes with very simple,easy preparation.6 CONCLUSIONThis paper shows that this automatic segmenting sys-tem MSS is quite efficient for segmentation of non-separated language sentences.
MSS do not use anygrammatical information to divide input sentences.Instead, MSS uses MI l)etween morphenres included inthe input sentence to select the best segmentation(s)frorn all the possibilities.
According to the resultsof the experiments, MSS can segment ahnost all thesentences 'correctly'.
This is such a remarkable result.When it comes to the sentences which are not in thecorpus the ratio of selecting the right segmentationas the best answer get a little bit lower, however, theresult is considerably good enough.The result shows that using Mid between mor-phemes is a very effective method of selecting 'correct'sentences, aml this means a lot in NLP.REFERENCES\[1\] Kenneth Church, William Gale, Patrick lhmks,and Donald llindle.
Parsing, Word Associationsand Typical Predlcate-Argument t{,elations.
In-ternational Parsing Workshop, 1989.\[2\] Frank Smadja.
Itow to compile a hilingual collo-cational lexicon automatically.
Statislically-basedNatural Language Programming Techniques, pages57--63, 1992.\[3\] dunya Tsutsurni, Tomoaki Nitta, Kotaro One,and Shlho Nobesawa.
A Multi-Lingual Transla-tion System Based on A Statistical Model(writtenin Jal)anese).
JSAI Technical report, SIG-PPAI-9302-2, pages 7-12, 1993.232\[4\] David M.Magerman and Mitchell P.Marcus.
Pars-ing a Natural Language Using Mutual InformationStatistics.
AAAI, 1990.\[5\] It.Brown, J.Cocke, S.Della Pietra, V.Della Pietra,F.Jelinek, R.Mercer, and P.Roossin.
A Statist, i-eal Approach to Language Translation.
l'roc, ofCOLING-88, pages 71-76, 1989.233
