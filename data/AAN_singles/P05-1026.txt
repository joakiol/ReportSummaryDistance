Proceedings of the 43rd Annual Meeting of the ACL, pages 205?214,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsExperiments with Interactive Question-AnsweringSanda Harabagiu, Andrew Hickl, John Lehmann, and Dan MoldovanLanguage Computer CorporationRichardson, Texas USAsanda@languagecomputer.comAbstractThis paper describes a novel frameworkfor interactive question-answering (Q/A)based on predictive questioning.
Gen-erated off-line from topic representationsof complex scenarios, predictive ques-tions represent requests for informationthat capture the most salient (and diverse)aspects of a topic.
We present experimen-tal results from large user studies (featur-ing a fully-implemented interactive Q/Asystem named FERRET) that demonstratesthat surprising performance is achieved byintegrating predictive questions into thecontext of a Q/A dialogue.1 IntroductionIn this paper, we propose a new architecture forinteractive question-answering based on predictivequestioning.
We present experimental results froma currently-implemented interactive Q/A system,named FERRET, that demonstrates that surprisingperformance is achieved by integrating sources oftopic information into the context of a Q/A dialogue.In interactive Q/A, professional users engage inextended dialogues with automatic Q/A systems inorder to obtain information relevant to a complexscenario.
Unlike Q/A in isolation, where the per-formance of a system is evaluated in terms of howwell answers returned by a system meet the specificinformation requirements of a single question, theperformance of interactive Q/A systems have tradi-tionally been evaluated by analyzing aspects of thedialogue as a whole.
Q/A dialogues have been evalu-ated in terms of (1) efficiency, defined as the numberof questions that the user must pose to find particu-lar information, (2) effectiveness, defined by the rel-evance of the answers returned, (3) user satisfaction.In order to maximize performance in these threeareas, interactive Q/A systems need a predictive di-alogue architecture that enables them to propose re-lated questions about the relevant information thatcould be returned to a user, given a domain of inter-est.
We argue that interactive Q/A systems dependon three factors: (1) the effective representation ofthe topic of a dialogue, (2) the dynamic recognitionof the structure of the dialogue, and (3) the ability toreturn relevant answers to a particular question.In this paper, we describe results from experi-ments we conducted with our own interactive Q/Asystem, FERRET, under the auspices of the ARDAAQUAINT1 program, involving 8 different dialoguescenarios and more than 30 users.
The results pre-sented here illustrate the role of predictive question-ing in enhancing the performance of Q/A interac-tions.In the remainder of this paper, we describe a newarchitecture for interactive Q/A.
Section 2 presentsthe functionality of several of FERRET?s modulesand describes the NLP techniques it relies upon.
InSection 3, we present one of the dialogue scenar-ios and the topic representations we have employed.Section 4 highlights the management of the inter-action between the user and FERRET, while Sec-tion 5 presents the results of evaluating our proposed1AQUAINT is an acronym for Advanced QUestion Answer-ing for INTelligence.205DialogueManagementCollectionDocumentQuestionSimilarityAnswerFusion(PDN)NetworkDialoguePredictiveAnswerFusionContextManagementDialogue ShellOnline Question AnsweringTopicPredictive DialogueQuestionAnswerDecompositionQuestionInformationExtractionRepresentationOff?line Question AnsweringDatabase (QUAB)Question?AnswerFigure 1: FERRET - A Predictive Interactive Question-Answering Architecture.model, and Section 6 summarizes the conclusions.2 Interactive Question-AnsweringWe have found that the quality of interactions pro-duced by an interactive Q/A system can be greatlyenhanced by predicting the range of questions thata user might ask in the context of a given topic.If a large database of topic-relevant questions wereavailable for a wide variety of topics, the accuracyof a state-of-the-art Q/A system such as (Harabagiuet al, 2003) could be enhanced.In FERRET, our interactive Q/A system, we storesuch ?predicted?
pairs of questions and answers in adatabase known as the Question Answer Database(or QUAB).
FERRET uses this large set of topic-relevant question-and-answer pairs to improve theinteraction with the user by suggesting new ques-tions.
For example, when a user asks a questionlike (Q1) (as illustrated in Table 1), FERRET returnsan answer to the question (A1) and proposes (Q2),(Q3), and (Q4) as suggestions of possible continua-tions of the dialogue.
Users then choose how to con-tinue the interaction by either (1) ignoring the sug-gestions made by the system and proposing a differ-ent question, or by (2) selecting one of the proposedquestions and examining its answer.Figure 1 illustrates the architecture of FERRET.The interactions are managed by a dialogue shell,which processes questions by transforming theminto their corresponding predicate-argument struc-tures2.The data collection used in our experiments was2We have employed the same representation of predicate-argument structures as those encoded in PropBank.
We use asemantic parser (described in (Surdeanu et al, 2003)) that rec-ognizes predicate-argument structures.
(Q1) What weapons are included in Egypt?s stockpiles?
(A1) The Israelis point to comments made by former President Anwar Sadat,who in 1970 stated that Egypt has biological weapons stored inrefrigerators ready to use against Israel if need be.
The program mightinclude ?plague, botulism toxin, encephalitis virus, anthrax,Rift Valley fever and mycotoxicosis.?
(Q2) Where did Egypt inherit its first stockpiles of chemical weapons?
(Q3) Is there evidence that Egypt has dismantled its stockpiles of weapons?
(Q4) Where are Egypt?s weapons stockpiles located?
(Q5) Who oversees Egypt?s weapons stockpiles?Table 1: User question and proposed questions from QUABsmade available by the Center for Non-ProliferationStudies (CNS)3.Modules from the FERRET?s dialogue shell inter-act with modules from the predictive dialogue block.Central to the predictive dialogue is the topic repre-sentation for each scenario, which enables the pop-ulation of a Predictive Dialogue Network (PDN).The PDN consists of a large set of questions thatwere asked or predicted for each topic.
It is a net-work because questions are related by ?similarity?links, which are computed by the Question Simi-larity module.
The topic representation enables anInformation Extraction module based on (Surdeanuand Harabagiu, 2002) to find topic-relevant infor-mation in the document collection and to use it asanswers for the QUABs.
The questions associatedwith each predicted answer are generated from pat-terns that are related to the extraction patterns usedfor identifying topic relevant information.
The qual-ity of the dialog between the user and FERRET de-pends on the quality of the topic representations andthe coverage of the QUABs.3The Center for Non-Proliferation Studies at the MonterreyInstitute of International Studies distributes collections of printand online documents on weapons of mass destruction.
Moreinformation at: http://cns.miis.edu.206GENERAL BACKGROUND1) Country Profile3) Military Operations: Army, Navy, Air Force, Leaders, Capabilities, Intentions4) Allies/Partners: Coalition Forces5) Weapons: Chemical, Biological, Materials, Stockpiles, Facilities, Access, Research Efforts, Scientists6) Citizens: Population, Growth Rate, Education8) Economics: Growth Domestic Product, Growth Rate, Imports9) Threat Perception: Border and Surrounding States, International, Terrorist Groups10) Behaviour: Threats, Invasions, Sponsorship and Harboring of Bad Actors13) Leadership:7) Industrial: Major Industrires, Exports, Power Sources14) Behaviour: Threats to use WMDs, Actual Usage, Sophistication of Attack, Anectodal or SimultaneousServing as a background to the scenarios, the following list contains subject areas that may be relevantto the scenarios under examination, and it is provided to assist the analyst in generating questions.2) Government: Type of, Leadership, RelationsSCENARIO: Assessment of Egypt?s Biological WeaponsAs terrorist Activity in Egypt increases, the Commanderof the United States Army believes a better understandingof Egypt?s Military capabilities is needed.
Egypt?sbiological weapons database needs to be updated tocorrespond with the Commander?s request.
Focus yourinvestigation on Egypt?s access to old technology,assistance received from the Soviet Union for developmentof their pharmaceutical infrastructure, production oftoxins and BW agents, stockpiles, exportation of thesematerials and development technology to Middle Easterncountries, and the effect that this information will have onthe United States and Coalition Forces in the Middle East.Please incorporate any other related information toyour report.11) Transportation Infrastructure: Kilometers of Road, Rail, Air Runways, Harbors and Ports, Rivers12) Beliefs: Ideology, Goals, Intentions15) Weapons: Chemical, Bilogical, Materials, Stockpiles, Facilities, AccessFigure 2: Example of a Dialogue Scenario.3 Modeling the Dialogue TopicOur experiments in interactive Q/A were based onseveral scenarios that were presented to us as partof the ARDA Metrics Challenge Dialogue Work-shop.
Figure 2 illustrates one of these scenarios.
Itis to be noted that the general background consistsof a list of subject areas, whereas the scenario is anarration in which several sub-topics are identified(e.g.
production of toxins or exportation of materi-als).
The creation of scenarios for interactive Q/Arequires several different types of domain-specificknowledge and a level of operational expertise notavailable to most system developers.
In addition toidentifying a particular domain of interest, scenar-ios must specify the set of relevant actors, outcomes,and related topics that are expected to operate withinthe domain of interest, the salient associations thatmay exist between entities and events in the sce-nario, and the specific timeframe and location thatbound the scenario in space and time.
In addition,real-world scenarios also need to identify certain op-erational parameters as well, such as the identity ofthe scenario?s sponsor (i.e.
the organization spon-soring the research) and audience (i.e.
the organiza-tion receiving the information), as well as a series ofevidence conditions which specify how much verifi-cation information must be subject to before it canbe accepted as fact.
We assume the set of sub-topicsmentioned in the general background and the sce-nario can be used together to define a topic structurethat will govern future interactions with the Q/A sys-tem.
In order to model this structure, the topic rep-resentation that we create considers separate topicsignatures for each sub-topic.The notion of topic signatures was first introducedin (Lin and Hovy, 2000).
For each subtopic in a sce-nario, given (a) documents relevant to the sub-topicand (b) documents not relevant to the subtopic, a sta-tistical method based on the likelihood ratio is usedto discover a weighted list of the most topic-specificconcepts, known as the topic signature.
Later workby (Harabagiu, 2004) demonstrated that topic sig-natures can be further enhanced by discovering themost relevant relations that exist between pairs ofconcepts.
However, both of these types of topic rep-resentations are limited by the fact that they requirethe identification of topic-relevant documents priorto the discovery of the topic signatures.
In our ex-periments, we were only presented with a set of doc-uments relevant to a particular scenario; no furtherrelevance information was provided for individualsubject areas or sub-topics.In order to solve the problem of finding relevantdocuments for each subtopic, we considered fourdifferent approaches:  Approach 1: All documents in the CNS col-lection were initially clustered using K-NearestNeighbor (KNN) clustering (Dudani, 1976).Each cluster that contained at least one key-word that described the sub-topic was deemedrelevant to the topic.  Approach 2: Since individual documents maycontain discourse segments pertaining to differ-ent sub-topics, we first used TextTiling (Hearst,1994) to automatically segment all of the doc-uments in the CNS collection into individualtext tiles.
These individual discourse segments207then served as input to the KNN clustering al-gorithm described in Approach 1.  Approach 3: In this approach, relevant docu-ments were discovered simultaneously with thediscovery of topic signatures.
First, we asso-ciated a binary seed relation   for each eachsub-topic .
(Seed relations were created bothby hand and using the method presented in(Harabagiu, 2004).)
Since seed relations are bydefinition relevant to a particular subtopic, theycan be used to determine a binary partition ofthe document collection  into (1) a relevantset of documents  (that is, the documents rel-evant to relation    ) and (2) a set of non-relevantdocuments  -  .
Inspired by the method pre-sented in (Yangarber et al, 2000), a topic sig-nature (as calculated by (Harabagiu, 2004)) isthen produced for the set of documents in  .For each subtopic defined as part of the di-alogue scenario, documents relevant to a cor-responding seed relation    are added to  iffthe relation   meets the density criterion (asdefined in (Yangarber et al, 2000)).
If 	 rep-resents the set of documents where   is recog-nized, then the density criterion can be definedas:.
Once 	 is added to  , thena new topic signature is calculated for  .
Rela-tions extracted from the new topic signature canthen be used to determine a new document par-tition by re-iterating the discovery of the topicsignature and of the documents relevant to eachsubtopic.  Approach 4: Approach 4 implements the tech-nique described in Approach 3, but operatesat the level of discourse segments (or texttiles)rather than at the level of full documents.
Aswith Approach 2, segments were produced us-ing the TextTiling algorithm.In modeling the dialogue scenarios, we consid-ered three types of topic-relevant relations: (1)structural relations, which represent hypernymyor meronymy relations between topic-relevant con-cepts, (2) definition relations, which uncover thecharacteristic properties of a concept, and (3) ex-traction relations, which model the most relevantevents or states associated with a sub-topic.
Al-though structural relations and definition relationsare discovered reliably using patterns available fromour Q/A system (Harabagiu et al, 2003), we foundonly extraction relations to be useful in determiningthe set of documents relevant to a subtopic.
Struc-tural relations were available from concept ontolo-gies implemented in the Q/A system.
The definitionrelations were identified by patterns used for pro-cessing definition questions.Extraction relations are discovered by processingdocuments in order to identify three types of rela-tions, including: (1) syntactic attachment relations(including subject-verb, object-verb, and verb-PPrelations), (2) predicate-argument relations, and (3)salience-based relations that can be used to encodelong-distance dependencies between topic-relevantconcepts.
(Salience-based relations are discoveredusing a technique first reported in (Harabagiu, 2004)which approximates a Centering Theory-style ap-proach (Kameyama, 1997) to the resolution ofcoreference.
)Subtopic: Egypt?s production of toxins and BW agentsTopic Signature:produce ?
phosphorous trichloride (TOXIN)house ?
ORGANIZATIONcultivate ?
non?pathogenic Bacilus Subtilis (TOXIN)produce ?
mycotoxins (TOXIN)acquire ?
FACILITYSubtopic: Egypt?s allies and partnersTopic Signature:provide ?
COUNTRYcultivate ?
COUNTRYsupply ?
precursorscooperate ?
COUNTRYtrain ?
PERSONsupply ?
know?howFigure 3: Example of two topic signatures acquiredfor the scenario illustrated in Figure 2.We made the extraction relations associated witheach topic signature more general (a) by replacingwords with their (morphological) root form (e.g.wounded with wound, weapons with weapon), (b)by replacing lexemes with their subsuming categoryfrom an ontology of 100,000 words (e.g.
truck is re-placed by VEHICLE, ARTIFACT, or OBJECT), and (c)by replacing each name with its name class (Egyptwith COUNTRY).
Figure 3 illustrates the topic sig-natures resulting for the scenario illustrated in Fig-ure 2.Once extraction relations were obtained for a par-ticular set of documents, the resulting set of re-lations were ranked according to a method pro-posed in (Yangarber, 2003).
Under this approach,208the score associated with each relation is given by:   	fifffl	ffi !  , where " 	#" rep-resents the cardinality of the documents where therelation is identified, and 	! 	 represents sup-port associated with the relation   . 	! 	 is de-fined as the sum of the relevance of each documentin 	 : 	! 	$%'&)(*,+.- .
The relevanceof a document that contains a topic-significant re-lation can be defined as: */+.-0214365(,71438  	9 , where :represents the topic signatureof the subtopic4 .
The accuracy of the relation, then,is given by: 8    	;<%'&)(*/+>=@?.-A3%CBEDF*/+>=HG.-9 .
Here, *,+?.- measures the rel-evance of a subtopic to a particular document-,while */+G.- measures the relevance of-to an-other subtopic,B .We use a different learner for each subtopic in or-der to train simultaneously on each iteration.
(Thecalculation of topic signatures continues to iterateuntil there are no more relations that can be addedto the overall topic signature.)
When the precisionof a relation to a subtopic is computed, it takesinto account the negative evidence of its relevanceto any other subtopicJIB .
If8  JKML ,the relation is not included in the topic signature,where relations are ranked by the score)  	N8  	+O 	! 	9 .Representing topics in terms of relevant conceptsand relations is important for the processing of ques-tions asked within the context of a given topic.
Forinteractive Q/A, however, the ideal topic-structuredrepresentation would be in the form of question-answer pairs (QUABs) that model the individualsegments of the scenario.
We have currently cre-ated two sets of QUABs: a handcrafted set andan automatically-generated set.
For the manually-created set of QUABs, 4 linguists manually gener-ated 3210 question-answer pairs for each of the 8dialogue scenarios considered in our experiments.In a separate effort, we devised a process for au-tomatically populating the QUAB for each scenario.In order to generate question-answer pairs for eachsubtopic, we first identified relevant text passages inthe document collection to serve as ?answers?
andthen generated individual questions that could be an-4Initially, P Q contains only the seed relation.
Additionalrelations can be added with each iteration.swered by each answer passage.R Answer Identification: We defined an an-swer passage as a contiguous sequence of sentenceswith a positive answer rank and a passage priceof K 4.
To select answer passages for each sub-topic , we calculate an answer rank,  SUTWVSXY%?   Z , that sums across the scores of eachrelation from the topic signature that is identified inthe same text window.
Initially, the text windowis set to one sentence.
(If the sentence is part of aquote, however, the text window is immediately ex-panded to encompass the entire sentence that con-tains the quote.)
Each passage with  SUTWV  SX\[]L isthen considered to be a candidate answer passage.The text window of each candidate answer passageis then expanded to include the following sentence.If the answer rank does not increase with the addi-tion of the succeeding sentence, then the price (! )
ofthe candidate answer passage is incremented by 1,otherwise it is decremented by 1.
The text windowof each candidate answer passage continues to ex-pand until!_^ .
Before the ranked list of candidateanswers can be considered by the Question Genera-tion module, answer passages with a positive price!are stripped of the last!sentences.ANSWERIn the early 1970s, Egyptian President Anwar Sadatvalidates that Egypt has a BW stockpile.Predicate?Argument StructuresP1: validatearguments: A0 = E2: Answer Type: DefinitionA1 = P2: havearguments: A0 = E3A1 = E4ArgM?TMP: E1: Answer Type: TimeP3: admitReference 4 (relational)Egyptian President XE5: BW programReference 2 (metonymic)Reference 3 (part?whole)QUESTIONSDefinition Pattern: Who is X?Q1: Who is Anwar Sadat?Pattern: When did E3 P1 to P2 E4?Q2: When did Egypt validate to having BW stockpiles?Pattern: When did E3 P3 to P2 E4?Q3: When did Egypt admit to having BW stockpiles?Pattern: When did E3 P3 to P2 E5?Q4: When did Egypt admint to having a BW program?E1: "in the early 1970s"; Category: TIMEE2: "Egyptian President Anwar Sadat"; Category: PERSONE3: "Egypt"; Category: COUNTRYE4: "BW stockpile"; Category: UNKNOWN4 entities2 predicates: P1="validate"; P2="has"PROCESSINGReference 1 (definitional)Figure 4: Associating Questions with Answers.R Question Generation: In order to automati-cally generate questions from answer passages, weconsidered the following two problems:  Problem 1: Every word in an answer passagecan refer to an entity, a relation, or an event.
Inorder for question generation be successful, wemust determine whether a particular reference209is ?interesting?
enough to the scenario such thatit deserves to be mentioned in a topic-relevantquestion.
For example, Figure 4 illustrates ananswer that includes two predicates and fourentities.
In this case, four types of reference areused to associate these linguistic objects withother related objects: (a) definitional reference,used to link entity (E1) ?Anwar Sadat?
to a cor-responding attribute ?Egyptian President?, (b)metonymic reference, since (E1) can be coercedinto (E2), (c) part-whole reference, since ?BWstockpiles?
(E4) necessarily imply the existenceof a ?BW program?
(E5), and (d) relational ref-erence, since validating is subsumed as partof the meaning of declaring (as determined byWordNet glosses), while admitting can be de-fined in terms of declaring, as in declaring [tobe true].ANSWEREgyptian Deputy Minister Mahmud Salim states that Egypt?sEgyptians have "adequate means of retaliating without delay".enemies would never use BW because they are aware that thePredicates: P?1=state; P?2 = never use; P3 = be aware;Causality:P?2(BW) = NON?NEGATIVE RESULT(P5); P?5 = "obstacle"Reference: P?1          P?6 = viewQUESTIONSDoes Egypt view the possesion of BW as an obstacle?Does Egypt view the possesion of BW as a deterrent?P?4 = have         P"4 = "the possesion"P"4 = "the possesion" = nominalization(P?4) = EFFECT(P?2(BW))PROCESSINGspecializationPattern: Does Egypt P?6 P"4(BW) as a P?5?Figure 5: Questions for Implied Causal Relations.  Problem 2: We have found that the identifica-tion of the association between a candidate an-swer and a question depends on (a) the recogni-tion of predicates and entities based on both theoutput of a named entity recognizer and a se-mantic parser (Surdeanu et al, 2003) and theirstructuring into predicate-argument frames, (b)the resolution of reference (addressed in Prob-lem 1), (c) the recognition of implicit rela-tions between predications stated in the answer.Some of these implicit relations are referential,as is the relation between predicates 8<and 8 illustrated in Figure 4.
A special case of im-plicit relations are the causal relations.
Fig-ure 5 illustrates an answer where a causal re-lation exists and is marked by the cue phrasebecause.
Predicates ?
like those in Figure 5 ?can be phrasal (like 8  ) or negative (like 8ffi ).Causality is established between predicates 8 ffiand 8 ?
as they are the ones that ultimately de-termine the selection of the answer.
The predi-cate!can be substituted by its nominalizationsince O<of8ffi is BW, the same argument istransferred to 8  .
The causality implied by theanswer from Figure 5 has two components: (1)the effect (i.e.
the predicate 8   ) and (2) the re-sult, which eliminates the semantic effect of thenegative polarity item never by implying thepredicate!
, obstacle.
The questions that aregenerated are based on question patterns asso-ciated with causal relations and therefore allowdifferent degrees for the specificity of the resul-tative, i.e obstacle or deterrent.We generated several questions for each answerpassage.
Questions were generated based on pat-terns that were acquired to model interrogationsusing relations between predicates and their argu-ments.
Such interrogations are based on (1) as-sociations between the answer type (e.g.
DATE)and the question stem (e.g.
?when?
and (2) therelation between predicates, question stem and thewords that determine the answer type (Narayananand Harabagiu, 2004).
In order to obtain thesepredicate-argument patterns, we used 30% (approxi-mately 1500 questions) of the handcrafted question-answer pairs, selected at random from each of the 8dialogue scenarios.
As Figures 4 and 5 illustrate, weused patterns based on (a) embedded predicates and(b) causal or counterfactual predicates.4 Managing Interactive Q/A DialoguesAs illustrated in Figure 1, the main idea of man-aging dialogues in which interactions with the Q/Asystem occur is based on the notion of predictions,i.e.
by proposing to the user a small set of questionsthat tackle the same subject as her question (as illus-trated in Table 1).
The advantage is that the user canfollow-up with one of the pre-processed questions,that has a correct answer and resides in one of theQUABs.
This enhances the effectiveness of the dia-logue.
It also may impact on the efficiency, i.e.
thenumber of questions being asked if the QUABs havegood coverage of the subject areas of the scenario.Moreover, complex questions, that generally are notprocessed with high accuracy by current state-of-the-art Q/A systems, are associated with predictivequestions that represent decompositions based on210similarities between predicates and arguments of theoriginal question and the predicted questions.The selection of the questions from the QUABsthat are proposed for each user question is based ona similarity-metric that ranks the QUAB questions.To compute the similarity metric, we have experi-mented with seven different metrics.
The first fourmetrics were introduced in (Lytinen and Tomuro,2002).  Similarity Metric 1 is based on two process-ing steps:(a) the content words of the questions areweighted using the  - measure used in In-formation Retrieval     1fifffl Z9&?, where  is the number ofquestions in the QUAB, -   is the num-ber of questions containing    and   isthe number of times    appears in the ques-tion.
This allows the user question and anyQUAB question to be transformed into twovectors, ffffffflfiffiand flffffff "!ffi;(b) the term vector similarity is used to computethe similarity between the user question andany question from the QUAB: # ff%$   J%??&9%ffi?('%ffi?  Similarity Metric 2 is based on the percent ofuser question terms that appear in the QUABquestion.
It is obtained by finding the intersec-tion of the terms in the term vectors of the twoquestions.  Similarity Metric 3 is based on semantic in-formation available from WordNet.
It involves:(a) finding the minimum path between Word-Net concepts.
Given two terms  <and   ffi ,each with T and ) WordNet senses<* <ffffff ,+.- andffi* <ffffff fl/-.
The se-mantic distance between the terms 0 < ffi isdefined by the minimum of all the possible pair-wise semantic distances between<andffi :0 < ffi13254= ?
(%76 G(  B , where  B is the path length between    and   B .
(b) the semantic similarity between the userquestion :8<ffiffffff+ffiand the QUABquestion :9;:<:ffiffffff:</ffito be definedas   ,):=:>/?7@67ACB?7A67@7 @B7 A, whereD:E:>FN%E(,7HG <<BIKJLNM<OQPMRE6F  Similarity Metric 4 is based on the questiontype similarity.
Instead of using the questionclass, determined by its stem, whenever wecould recognize the answer type expected bythe question, we used it for matching.
As back-off only, we used a question type similaritybased on a matrix akin to the one reported in(Lytinen and Tomuro, 2002)  Similarity Metric 5 is based on question con-cepts rather than question terms.
In order totranslate question terms into concepts, we re-placed (a) question stems (i.e.
a WH-word +NP construction) with expected answer types(taken from the answer type hierarchy em-ployed by FERRET?s Q/A system) and (b)named entities with corresponding their corre-sponding classes.
Remaining nouns and verbswere also replaced with their WordNet seman-tic classes, as well.
Each concept was then as-sociated with a weight: concepts derived fromnamed entities classes were weighted heavierthan concepts from answer types, which werein turn weighted heavier than concepts takenfrom WordNet clases.
Similarity was then com-puted across ?matching?
concepts.
5 The resul-tant similarity score was based on three vari-ables:S= sum of the weights of all concepts matchedbetween a user query ( T) and a QUAB query( TVU );W= sum of the weights of all unmatched con-cepts in T;X= sum of the weights of all unmatched con-cepts in TVU ;The similarity between Tand TYU was calcu-lated asS3!
'W 3!U'X , where!and!U were used as coefficients to penalize the con-tribution of unmatched concepts in Tand TVUrespectively.
6  Similarity Metric 6 is based on the fact that the5In the case of ambiguous nouns and verbs associated withmultiple WordNet classes, all possible classes for a term wereconsidered in matching.6We set Z @ = 0.4 and Z[ = 0.1 in our experiments.211Q1: Does Iran have an indigenous CW program?
(1b) Has the plant at Qazvin been linked to CW production?
(1c) What CW does Iran produce?
(1a) How did Iran start its CW program?Q2: Where are Iran?s CW facilities located?
(2a) What factories in Iran could produce CW?
(2b) Where are Iran?s stockpiles of CW?
(2c) Where has Iran bought equipment to produce CW?Q3: What is Iran?s goal for its CW program?
(3a) What motivated Iran to expand its chemical weapons program?
(3b) How do CW figure into Iran?s long?term strategic plan?
(3c) What are Iran?s future CW plans?QUABs:QUABs:QUABs:Answer(A3):Answer(A2):Answer (A1):Although Iran is making a concerted effort to attain an independent production capability for all aspects of chemicalweapons program, it remains dependent on foreign sources for chemical warfare?related technologies.According to several sources, Iran?s primary suspected chemical weapons production facility is located in the city of Damghan.In their pursuit of regional hegemony, Iran and Iraq probably regard CW weapons and missiles as necessary to support theirpolitical and military objectives.
Possession of chemical weapons would likely lead to increased intimidation of their Gulf,neighbors, as well as increased willingness to confront the United States.Figure 6: A sample interactive Q/A dialogue.QUAB questions are clustered based on theirmapping to a vector of important concepts inthe QUAB.The clustering was done using theK-Nearest Neighbor (KNN) method (Dudani,1976).
Instead of measuring the similarity be-tween the user question and each question inthe QUAB, similarities are computed only be-tween the user question and the centroid ofeach cluster.  Similarity Metric 7 was derived from the re-sults of Similarity Metrics 5 and 6 above.
Inthis case, if the QUAB question ( T U ) that wasdeemed to be most similar to a user question( T) under Similarity Metric 5 is containedin the cluster of QUAB questions deemed tobe most similar to Tunder Similarity Metric6, then TVU receives a cluster adjustment scorein order to boost its ranking within its QUABcluster.
We calculate the cluster adjustmentscore as  )    & BTYU  )1 3 99  ) , where   represents the differencein rank between the centroid of the cluster andthe previous rank of the QUAB question T U .In the currently-implemented version of FERRET,we used Similarity Metric 5 to automatically iden-tify the set of 10 QUAB questions that were mostsimilar to a user?s question.
These question-and-answer pairs were then returned to the user ?
alongwith answers from FERRET?s automatic Q/A system?
as potential continuations of the Q/A dialogue.
Weused the remaining 6 similarity metrics described inthis section to manually assess the impact of simi-larity on a Q/A dialogue.5 Experiments with Interactive Q/ADialoguesTo date, we have used FERRET to produce over 90Q/A dialogues with human users.
Figure 6 illustratesthree turns from a real dialogue from a human userinvestigating Iran?s chemical weapons prorgram.
Asit can be seen coherence can be established betweenthe user?s questions and the system?s answers (e.g.Q3 is related to both A1 and A3) as well as betweenthe QUABs and the user?s follow-up questions (e.g.QUAB (1b) is more related to Q2 than either Q1 orA1).
Coherence alone is not sufficient to analyze thequality of interactions, however.In order to better understand interactive Q/A dia-logues, we have conducted three sets of experimentswith human users of FERRET.
In these experiments,users were allotted two hours to interact with Ferretto gather information requested by a dialogue sce-nario similar to the one presented in Figure 2.
InExperiment 1 (E1), 8 U.S. Navy Reserve (USNR)intelligence analysts used FERRET to research 8 dif-ferent scenarios related to chemical and biologicalweapons.
Experiment 2 and Experiment 3 consid-ered several of the same scenarios addressed in E1:E2 included 24 mixed teams of analysts and noviceusers working with 2 scenarios, while E3 featured 4USNR analysts working with 6 of the original 8 sce-narios.
(Details for each experiment are provided inTable 2.)
Users were also given a task to focus their212research; in E1 and E3, users prepared a short reportdetailing their findings; in E2, users were given a listof ?challenge?
questions to answer.Exp Users QUABs?
Scenarios TopicsE1 8 Yes 8 Egypt BW, Russia CW, SouthAfrica CW, India CW, NorthKorea CBW, Pakistan CW,Libya CW, Iran CWE2 24 Yes 2 Egypt BW, Russia CWE3 4 No 6 Egypt BW, Russia CW, NorthKorea CBW, Pakistan CWIndia CW, Libya CW, Iran CWTable 2: Experiment detailsIn E1 and E2, users had access to a total of 3210QUAB questions that had been hand-created by de-velopers for each the 8 dialogue scenarios.
(Table 3provides totals for each scenario.)
In E3, users per-formed research with a version of FERRET that in-cluded no QUABs at all.Scenario Handcrafted QUABsINDIA 460LIBYA 414IRAN 522NORTH KOREA 316PAKISTAN 322SOUTH AFRICA 454RUSSIA 366EGYPT 356Testing Total 3210Table 3: QUAB distribution over scenariosWe have evaluated FERRET by measuring effi-ciency, effectiveness, and user satisfaction:Efficiency FERRET?s QUAB collection enabledusers in our experiments to find more relevant infor-mation by asking fewer questions.
When manually-created QUABs were available (E1 and E2), userssubmitted an average of 12.25 questions each ses-sion.
When no QUABs were available (E3), usersentered a total of 44.5 questions per session.
Table 4lists the number of QUAB question-answer pairs se-lected by users and the number of user questions en-tered by users during the 8 scenarios considered inE1.
In E2, freed from the task of writing a researchreport, users asked significantly (p   0.05) fewerquestions and selected fewer QUABs than they didin E1.
(See Table 5).Effectiveness QUAB question-answer pairs alsoimproved the overall accuracy of the answers re-turned by FERRET.
To measure the effectiveness ofa Q/A dialogue, human annotators were used to per-form a post-hoc analysis of how relevant the QUABpairs returned by FERRET were to each questionCountry n QUAB User Q Total(avg.)
(avg.)
(avg.
)India 2 21.5 13.0 34.5Libya 2 12.0 9.0 21.0Iran 2 18.5 11.0 29.5N.Korea 2 16.5 7.5 34.0Pakistan 2 29.5 15.5 45.0S.Africa 2 14.5 6.0 20.5Russia 2 13.5 15.5 29.0Egypt 2 15.0 20.5 35.5TOTAL(E1) 16 17.63 12.25 29.88Table 4: Efficiency of Dialogues in Experiment 1Country n QUAB User Q Total(avg.)
(avg.)
(avg.
)Russia 24 8.2 5.5 13.7Egypt 24 10.8 7.6 18.4TOTAL(E2) 48 9.50 6.55 16.05Table 5: Efficiency of Dialogues in Experiment 2entered by a user: each QUAB pair returned wasgraded as ?relevant?
or ?irrelevant?
to a user ques-tion in a forced-choice task.
Aggregate relevancescores were used to calculate (1) the percentage ofrelevant QUAB pairs returned and (2) the mean re-ciprocal rank (MRR) for each user question.
MRR isdefined as <+ %F<<?, whree   is the lowest rank ofany relevant answer for the user query7.
Table 6describes the performance of FERRET when each ofthe 7 similarity measures presented in Section 4 areused to return QUAB pairs in response to a query.When only answers from FERRET?s automatic Q/Asystem were available to users, only 15.7% of sys-tem responses were deemed to be relevant to a user?squery.
In contrast, when manually-generated QUABpairs were introduced, as high as 84% of the sys-tem?s responses were deemed to be relevant.
Theresults listed in Table 6 show that the best metric isSimilarity Metric 5.
Thse results suggest that theselection of relevant questions depends on sophis-ticated similarity measures that rely on conceptualhierarchies and semantic recognizers.We evaluated the quality of each of the foursets of automatically-generated QUABs in a sim-ilar fashion.
For each question submitted by auser in E1, E2, and E3, we collected the top 5QUAB question-answer pairs (as determined bySimilarity Metric 5) that FERRET returned.
As withthe manually-generated QUABs, the automatically-7We chose MRR as our scoring metric because it reflects thefact that a user is most likely to examine the first few answersfrom any system, but that all correct answers returned by thesystem have some value because users will sometimes examinea very large list of query results.213% of Top 5 Responses % of Top 1 Responses MRRRelevant to User Q Relevant to User QWithout QUAB 15.73% 26.85% 0.325Similarity 1 82.61% 60.63% 0.703Similarity 2 79.95% 58.45% 0.681Similarity 3 79.47% 56.04% 0.664Similarity 4 78.26% 46.14% 0.592Similarity 5 84.06% 68.36% 0.753Similarity 6 81.64% 56.04% 0.671Similarity 7 84.54% 64.01% 0.730Table 6: Effectiveness of dialogsgenerated pairs were submitted to human assessorswho annotated each as ?relevant?
or irrelevant to theuser?s query.
Aggregate scores are presented in Ta-ble 7.Egypt RussiaApproach % of Top 5 % of Top 5Responses Rel.
MRR Responses Rel.
MRRto User Q to User QApproach 1 40.01% 0.295 60.25% 0.310Approach 2 36.00% 0.243 72.00% 0.475Approach 3 44.62% 0.271 60.00% 0.297Approach 4 68.05% 0.510 68.00% 0.406Table 7: Quality of QUABs acquired automaticallyUser Satisfaction Users were consistently satis-fied with their interactions with FERRET.
In all threeexperiments, respondents claimed that they foundthat FERRET (1) gave meaningful answers, (2) pro-vided useful suggestions, (3) helped answer spe-cific questions, and (4) promoted their general un-derstanding of the issues considered in the scenario.Complete results of this study are presented in Ta-ble 88.Factor E1 E2 E3Promoted understanding 3.40 3.20 3.75Helped with specific questions 3.70 3.60 3.25Make good use of questions 3.40 3.55 3.0Gave new scenario insights 3.00 3.10 2.2Gave good collection coverage 3.75 3.70 3.75Stimulated user thinking 3.50 3.20 2.75Easy to use 3.50 3.55 4.10Expanded understanding 3.40 3.20 3.00Gave meaningful answers 4.10 3.60 2.75Was helpful 4.00 3.75 3.25Helped with new search methods 2.75 3.05 2.25Provided novel suggestions 3.25 3.40 2.65Is ready for work environment 2.85 2.80 3.25Would speed up work 3.25 3.25 3.00Overall like of system 3.75 3.60 3.75Table 8: User Satisfaction Survey Results6 ConclusionsWe believe that the quality of Q/A interactions de-pends on the modeling of scenario topics.
An idealmodel is provided by question-answer databases(QUABs) that are created off-line and then used to8Evaluation scale: 1-does not describe the system, 5-completely describes the systemmake suggestions to a user of potential relevant con-tinuations of a discourse.
In this paper, we havepresented FERRET, an interactive Q/A system whichmakes use of a novel Q/A architecture that integratesQUAB question-answer pairs into the processing ofquestions.
Experiments with FERRET have shownthat, in addition to being rapidly adopted by users asvalid suggestions, the incorporation of QUABs intoQ/A can greatly improve the overall accuracy of aninteractive Q/A dialogue.ReferencesS.
Dudani.
1976.
The distance-weighted k-nearest-neighbourrule.
IEEE Transactions on Systems, Man, and Cybernetics,SMC-6(4):325?327.S.
Harabagiu, D. Moldovan, C. Clark, M. Bowden, J. Williams,and J. Bensley.
2003.
Answer Mining by Combining Ex-traction Techniques with Abductive Reasoning.
In Proceed-ings of the Twelfth Text Retrieval Conference (TREC 2003).Sanda Harabagiu.
2004.
Incremental Topic Representations.In Proceedings of the 20th COLING Conference, Geneva,Switzerland.Marti Hearst.
1994.
Multi-Paragraph Segmentation of Exposi-tory Text.
In Proceedings of the 32nd Meeting of the Associ-ation for Computational Linguistics, pages 9?16.Megumi Kameyama.
1997.
Recognizing Referential Links: AnInformation Extraction Perspective.
In Workshop of Opera-tional Factors in Practical, Robust Anaphora Resolution forUnrestricted Texts, (ACL-97/EACL-97), pages 46?53.Chin-Yew Lin and Eduard Hovy.
2000.
The Automated Acqui-sition of Topic Signatures for Text Summarization.
In Pro-ceedings of the 18th COLING Conference, pages 495?501.S.
Lytinen and N. Tomuro.
2002.
The Use of Question Typesto Match Questions in FAQFinder.
In Papers from the 2002AAAI Spring Symposium on Mining Answers from Texts andKnowledge Bases, pages 46?53.Srini Narayanan and Sanda Harabagiu.
2004.
Question An-swering Based on Semantic Structures.
In Proceedings ofthe 20th COLING Conference, Geneva, Switzerland.Mihai Surdeanu and Sanda M. Harabagiu.
2002.
Infratructurefor open-domanin information extraction.
In Conference forHuman Language Technology (HLT-2002).Mihai Surdeanu, Sanda M. Harabagiu, John Williams, and PaulAarseth.
2003.
Using predicate-argument structures for in-formation extraction.
In ACL, pages 8?15.Roman Yangarber, Ralph Grishman, Pasi Tapanainen, and SiljaHuttunen.
2000.
Automatic Acquisition of Domain Knowl-edge for Information Extraction.
In Proceedings of the 18thCOLING Conference, pages 940?946.Roman Yangarber.
2003.
Counter-Training in Discovery ofSemantic Patterns.
In Proceedings of the 41th Meeting of theAssociation for Computational Linguistics, pages 343?350.214
