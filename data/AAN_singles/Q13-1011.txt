Transactions of the Association for Computational Linguistics, 1 (2013) 125?138.
Action Editor: Sharon Goldwater.Submitted 10/2012; Revised 3/2013; Published 5/2013.
c?2013 Association for Computational Linguistics.Modeling Child Divergences from Adult GrammarSam SahakianUniversity of Wisconsin-Madisonsahakian@cs.wisc.eduBenjamin SnyderUniversity of Wisconsin-Madisonbsnyder@cs.wisc.eduAbstractDuring the course of first language acquisi-tion, children produce linguistic forms that donot conform to adult grammar.
In this paper,we introduce a data set and approach for sys-tematically modeling this child-adult grammardivergence.
Our corpus consists of child sen-tences with corrected adult forms.
We bridgethe gap between these forms with a discrim-inatively reranked noisy channel model thattranslates child sentences into equivalent adultutterances.
Our method outperforms MT andESL baselines, reducing child error by 20%.Our model allows us to chart specific aspectsof grammar development in longitudinal stud-ies of children, and investigate the hypothesisthat children share a common developmentalpath in language acquisition.1 IntroductionSince the publication of the Brown Study (1973),the existence of standard stages of development hasbeen an underlying assumption in the study of firstlanguage learning.
As a child moves towards lan-guage mastery, their language use grows predictablyto include more complex syntactic structures, even-tually converging to full adult usage.
In the course ofthis process, children may produce linguistic formsthat do not conform to the grammatical standard.From the adult point of view these are language er-rors, a label which implies a faulty production.
Con-sidering the work-in-progress nature of a child lan-guage learner, these divergences could also be de-scribed as expressions of the structural differencesbetween child and adult grammar.
The predictabilityof these divergences has been observed by psychol-ogists, linguists and parents (Owens, 2008).1Our work leverages the differences between childand adult language to make two contributions to-wards the study of language acquisition.
First, weprovide a corpus of errorful child sentences anno-tated with adult-like rephrasings.
This data will al-low researchers to test hypotheses and build modelsrelating the development of child language to adultforms.
Our second contribution is a probabilisticmodel trained on our corpus that predicts a gram-matical rephrasing given an errorful child sentence.The generative assumption of our model is thatsentences begin in underlying adult forms, and arethen stochastically transformed into observed childutterances.
Given an observed child utterance s, wecalculate the probability of the corrected adult trans-lation t asP (t|s) ?
P (s|t)P (t),where P (t) is an adult language model and P (s|t)is a noise model crafted to capture child grammarerrors like omission of certain function words andcorruptions of tense or declension.
The parame-ters of this noise model are estimated using our cor-pus of child and adult-form utterances, using EM tohandle unobserved word alignments.
We use thisgenerative model to produce n-best lists of candi-date corrections which are then reranked using longrange sentence features in a discriminative frame-work (Collins and Roark, 2004).1For the remainder of this paper we use ?error?
and ?diver-gence?
interchangeably.125One could argue that our noisy channel modelmirrors the cognitive process of child language pro-duction by appealing to the hypothesis that childrenrapidly learn adult-like grammar but produce errorsdue to performance factors (Bloom, 1990; Ham-burger and Crain, 1984).
That being said, our pri-mary goal in this paper is not cognitive plausibility,but rather the creation of a practical tool to aid inthe empirical study of language acquisition.
By au-tomatically inferring adult-like forms of child sen-tences, our model can highlight and compare devel-opmental trends of children over time using largequantities of data, while minimizing the need for hu-man annotation.Besides this, our model?s predictive success it-self has theoretical implications.
By aggregatingtraining and testing data across children, our modelinstantiates the Brown hypothesis of a shared de-velopmental path.
Even when adequate per-childtraining data exists, using data only from other chil-dren leads to no degradation in performance, sug-gesting that the learned parameters capture generalchild language phenomena and not just individualhabits.
Besides aggregating across children, ourmodel coarsely lumps together all stages of devel-opment, providing a frozen snapshot of child gram-mar.
This establishes a baseline for more cognitivelyplausible and temporally dynamic models.We compare our correction system against twobaselines, a phrase-based Machine Translation (MT)system, and a model designed for English SecondLanguage (ESL) error correction.
Relative to thebest performing baseline, our approach achieves a30% decrease in word error-rate and a four pointincrease in BLEU score.
We analyze the perfor-mance of our system on various child error cate-gories, highlighting our model?s strengths (correct-ing be drops and morphological overgeneralizations)as well as its weaknesses (correcting pronoun andauxiliary drops).
We also assess the learning rateof our model, showing that very little annotationis needed to achieve high performance.
Finally, toshowcase a potential application, we use our modelto chart one aspect of four children?s grammar ac-quisition over time.
While generally vindicating theBrown thesis of a common developmental path, theresults point to subtleties in variation across individ-uals that merit further investigation.2 Background and Related WorkWhile child error correction is a novel task, com-putational methods are frequently used to study firstlanguage acquisition.
The computational study ofspeech is facilitated by TalkBank (MacWhinney,2007), a large database of transcribed dialogues in-cluding CHILDES (MacWhinney, 2000), a subsec-tion composed entirely of child conversation data.Computational tools have been developed specif-ically for the large-scale analysis of CHILDES.These tools enable further computational study suchas the automatic calculation of the language devel-opment metrics IPSYN (Sagae et al 2005) andD-Level (Lu, 2009), or the automatic formula-tion of novel language development metrics them-selves (Sahakian and Snyder, 2012).The availability of child language is also key tothe design of computational models of languagelearning (Alishahi, 2010), which can support theplausibility of proposed human strategies for taskslike semantic role labeling (Connor et al 2008) orword learning (Regier, 2005).
To our knowledgethis paper is the first work on error correction inthe first language learning domain.
Previous workhas employed a classifier-based approach to iden-tify speech errors indicative of language disordersin children (Morley and Prud?hommeaux, 2012).Automatic correction of second language (L2)writing is a common objective in computer assistedlanguage learning (CALL).
These tasks generallytarget high-frequency error categories including ar-ticle, word-form, and preposition choice.
Previouswork in CALL error correction includes identify-ing word choice errors in TOEFL essays based oncontext (Chodorow and Leacock, 2000), correctingerrors with a generative lattice and PCFG rerank-ing (Lee and Seneff, 2006), and identifying a broadrange of errors in ESL essays by examining linguis-tic features of words in sequence (Gamon, 2011).
Ina 2011 shared ESL correction task (Dale and Kilgar-riff, 2011), the best performing system (Rozovskayaet al 2011) corrected preposition, article, punctu-ation and spelling errors by building classifiers foreach category.
This line of work is grounded in thepractical application of automatic error correction asa learning tool for ESL students.Statistical Machine Translation (SMT) has been126applied in diverse contexts including grammar cor-rection as well as paraphrasing (Quirk et al 2004),question answering (Echihabi and Marcu, 2003) andprediction of twitter responses (Ritter et al 2011).In the realm of error correction, SMT has been ap-plied to identify and correct spelling errors in inter-net search queries (Sun et al 2010).
Within CALL,Park and Levy (2011) took an unsupervised SMTapproach to ESL error correction using Weighted Fi-nite State Transducers (FSTs).
The work describedin this paper is inspired by that of Park and Levy,and in Section 6 we detail differences between ourapproaches.
We also include their model as a base-line.3 DataTo train and evaluate our translation system, we firstcollected a corpus of 1,000 errorful child-languageutterances from the American English portion of theCHILDES database.
To encourage diversity in thegrammatical divergences captured by our corpus,our data is drawn from a large pool of studies (seebibliography for the full list of citations).In the annotation process, candidate child sen-tences were randomly selected from the pool andclassified by hand as either grammatically correct,divergent or unclassifiable (when it was not possi-ble to tell what a child is trying to say).
We con-tinued this process until 1,000 divergent sentenceswere found.
Along the way we also encountered5,197 grammatically correct utterances and 909 thatwere unclassifiable.2 Because CHILDES includesspeech samples from children of diverse age, back-ground and language ability, our corpus does notcapture any specific stage of language development.Instead, the corpus represents a general snapshot ofa learner who has not yet mastered English as theirfirst language.To provide the grammatically correct counterpartto child data, our errorful sentences were correctedby workers on Amazon?s Mechanical Turk web ser-vice.
Given a child utterance and its surroundingconversational context, annotators were instructedto translate the child utterance into adult-like En-glish.
We limited eligible workers to native English2These hand-classified sentences are available online alongwith our set of errorful sentences.Error Type Child UtteranceInsertion I did locked it.Inflection More cookie?Deletion That not how.Lemma Choice I got grain.Overgeneralization I drawed it.Table 1: Examples of error types captured by our model.speakers residing in the US.
We also required anno-tators to follow a brief tutorial in which they prac-tice correcting sample utterances according to ourguidelines.
These guidelines instructed workers tominimally alter sentences to be grammatically con-sistent with a conversation or written letter, withoutaltering underlying meaning.
Annotators were eval-uated on a worker-by-worker basis and rejected inthe rare case that they ignored our guidelines.
Ac-cepted workers were paid 7 cents for correcting eachset of 5 sentences.
To achieve a consistent judgment,we posted each set of sentences for correction by 7different annotators.Once multiple reference translations were ob-tained we selected a single best correction by plu-rality, arbitrating ties as necessary.
There were sev-eral cases in which corrections obtained by pluralitydecision did not perfectly follow instructions.
Thesewere manually corrected.
Both the raw translationsprovided by individual annotators as well as the cu-rated final adult forms are provided online as part ofour data set.3 Resulting pairs of errorful child sen-tences and their adult-like corrections were split into73% training, 7% development and 20% test data,which we use to build, tune and evaluate our gram-mar correction system.
In the final test phase, devel-opment data is included in the training set.4 ModelAccording to our generative model, adult-like utter-ances are formed and then transformed by a noisychannel to become child sentences.
The structureof our noise model is tailored to match our observa-tions of common child errors.
These include: func-tion word insertions, function word deletions, swapsof function words and, inflectional changes to con-tent words.
Examples of each error type are given3Data is available athttp://pages.cs.wisc.edu/~bsnyder127in Table 1.
Our model does not allow reorderings,and can thus be described in terms of word-by-wordstochastic transformations to the adult sentence.We use 10 word classes to parameterize ourmodel: pronouns, negators, wh-words, conjunc-tions, prepositions, determiners, modal verbs, ?be?verbs, other auxiliary verbs, and lexical contentwords.
The list of words in each class is providedas part of our data set.
For each input adult word w,the model generates output word w?
as a hierarchi-cal series of draws from multinomial distributions,conditioned on the original word w and its class c.All distributions receive an asymmetric Dirichletprior which favors retention of the adult word.
Withthe sole exception of word insertions, the distribu-tions are parameterized and learned during train-ing.
Our model consists of 217 multinomial distri-butions, with 6,718 free parameters.The precise form and parameterization of ourmodel were handcrafted for performance on the de-velopment data, using trial and error.
We also con-sidered more fine-grained model forms (i.e.
oneparameter for each non-lexical input-output wordpair), as well as coarser parameterizations (i.e.a single shared parameter denoting any inflectionchange).
The model we describe here seemed toachieve the best balance of specificity and general-ization.We now present pseudocode describing the noisemodel?s operation upon processing each word,along with a brief description of each step.Action selection (lines 3-7): On reading an inputword, an action category a is selected from a prob-ability distribution conditioned on the input word?sclass.
Our model allows up to two function wordinsertions or deletions in a row before a swap is re-quired.
Lexical content words may not be deleted orinserted, only swapped.Insert and Delete (lines 8-15): The deletioncase requires no decision after action selection.
Inthe insertion case, the class of the inserted word,c?, is selected conditioned on cPREV, the class ofthe previous adult word.
The precise identity ofthe inserted word is then drawn from a uniformdistribution over words in class c?.
It is importantto note that in the insertion case, the input word ata given iteration will be re-processed at the nextiteration (lines 33-35).insdel?
0for word w with class c, inflection f , lemma `do3: if insdel = 2 thena?
swapelse6: a ?
{insert, delete, swap} | cend ifif a = delete then9: insdel++c?
?
w?
?
12: else if a = insert theninsdel++c?
?
classes | cPREV, insert15: w?
?
words in c?
| insertelseinsdel?
018: c?
?
cif c ?
uninflected-classes thenw?
?
words in c | w, swap21: else if c = aux then`?
?
aux-lemmas | `, swapf ?
?
inflections | f, swap24: w?
?
COMBINE(`?, f ?
)elsef ?
?
inflections | f, swap27: w?
?
COMBINE(`, f ?
)end ifend if30: if w?
?
irregular thenw?
?
OVERGEN(w?)
?
{w?
}end if33: if a = insert thengoto line 3end if36: end forSwap (lines 16 - 29): In the swap case, a wordof given class is substituted for another word in thesame class.
Depending on the source word?s class,swaps are handled in slightly different ways.
If theword is a modal, conjunction, determiner, preposi-tion, ?wh-?
word or negative, it is considered ?unin-128flected.?
In these cases, a new word w?
is selectedfrom all words in class c, conditioned on the sourceword w.If w is an auxiliary verb, the swap procedure con-sists of two parallel steps.
A lemma is selectedfrom possible auxiliary lemmas, conditioned on thelemma of the source word.4 In the second step, anoutput inflection type is selected from a distributionconditioned on the source word?s inflection.
Theprecise output word is fully specified by the choiceof lemma and conjugation.If w is not in either of the above two categories, itis a lexical word, and our model only allows changesin conjugation or declension.
If the source word isa noun it may swap to singular or plural form con-ditioned on the source form.
If the word is a verb,it may swap to any conjugated or non-finite form,again conditioned on the source form.
Lexical wordsthat are not marked by CELEX (Baayen et al 1996)as nouns or verbs may only swap to the exact sameword.Overgeneralization (lines 30-32): Finally, thenoisy channel considers the possibility of produc-ing overgeneralized word forms (like ?maked?
and?childs?)
in place of their correct irregular forms.The OVERGEN function produces the incorrect over-generalized form.
We draw from a distributionwhich chooses between this form and the correctoriginal word.
Our model maintains separate dis-tributions for nouns (overgeneralized plurals) andverbs (overgeneralized past tense).5 ImplementationIn this section, we describe steps necessary tobuild, train and test our error correction model.Weighted Finite State Transducers (FSTs) used inour model are constructed with OpenFst (Allauzenet al 2007).5.1 Sentence FSTsThese FSTs provide the basis for our translation pro-cess.
We represent sentences by building a simplelinear chain FST, progressing from node to nodewith each arc accepting and yielding one word inthe sentence.
All arcs are weighted with probabilityone.4Auxiliary lemmas include have, do, go, will, and get.5.2 Noise FSTThe noise model provides a conditional probabilityover child sentences given an adult sentence.
We en-code this model as a FST with several states, allow-ing us to track the number of consecutive insertionsor deletions.
We allow only two of these operationsin a row, thereby constraining the length of the out-put sentence.
This constraint results in three states(insdel = 0, insdel = 1, insdel = 2), along withan end state.
In our training data, only 2 sentencepairs cannot be described by the noise model due tothis constraint.Each arc in the FST has an  or adult-languageword as input symbol, and a possibly errorful child-language word or  as output symbol.
Each arcweight is the probability of transducing the inputword to the output word, determined according tothe parameterized distributions described in Sec-tion 4.
Arcs corresponding to insertions or dele-tions lead to a new state (insdel++) and are not al-lowed from state insdel = 2.
Substitution arcs alllead back to state insdel = 0.
Word class infor-mation is given by a set of word lists for each non-lexical class.5 Inflectional information is derivedfrom CELEX.5.3 Language Model FSTThe language model provides a prior distributionover adult form sentences.
We build a a trigramlanguage model FST with Kneser-Ney smoothingusing OpenGRM (Roark et al 2012).
The lan-guage model is trained on all parent speech in theCHILDES studies from which our errorful sentencesare drawn.In the language model FST, the input and outputwords of each arc are identical.
Arcs are weightedwith the probability of the n-gram beginning withsome prefix associated with the source node, andending with the arc?s input/output word.
In thissetup, the probability of a string is the total weightof the path accepting and emitting that string.5.4 TrainingAs detailed in Section 4, our noise model consists ofa series of multinomial distributions which govern5Word lists are included for reference with our dataset.1290 1that:that2is:<eps>3him:himhis:himhim:himhis:him4hat:hathats:hat5.
:.Figure 1: A simplified decoding FST for the child sentence ?That him hat.?
In an actual decoding FST many moretransduction arcs exist, including those translating ?that?
and ?him?
to any determiner and pronoun, respectively,and affording opportunities for many more deletions and insertions.
Input and output strings given by FST pathscorrespond to possible adult-to-child translations.the transformation from adult word to child word, al-lowing limited insertions and deletions.
We estimateparameters ?
for these distributions that maximizetheir posterior probability given the observed train-ing sentences {(s, t)}.
Since our language modelP (t) does not depend on on the noise model param-eters, this objective is equivalent to jointly maximiz-ing the prior and the conditional likelihoods of childsentences given adult sentences:argmax?P (?
)?P (s|t, ?
)To represent all possible derivations of each childsentence s from its adult translation t, we composethe sentence FSTs with the noise model, obtaining:FSTtrain = FSTt ?
FSTnoise ?
FSTsEach path through FSTtrain corresponds to a sin-gle derivation d, with path weight P (s, d|t, ?).
Bysumming all path weights, we obtain P (s|t, ?).
Weuse a MAP-EM algorithm to maximize our objectivewhile summing over all possible derivations.Our training scheme relies on FSTs weighted inthe V-expectation semiring (Eisner, 2001), imple-mented using code from fstrain (Dreyer et al 2008).Besides carrying probabilities, arc weights are sup-plemented with a vector to indicate parameter countsinvolved in the arc traversal.
The V-expectationsemiring is designed so that the total arc weight ofall paths through the FST yields both the probabil-ity P (s|t, ?
), along with expected parameter counts.Our EM algorithm proceeds as follows: We startby initializing all parameters to uniform distribu-tions with random noise.
We then weight the arcsin FSTnoise accordingly.
For each sentence pair(s, t), we build FSTtrain by composition with ournoise model, as described in the previous paragraph.We then compute the total arc weight of all pathsthrough FSTtrain by relabeling all input and outputsymbols to  and then reducing FSTtrain to a singlestate using epsilon removal (Mohri, 2008).
The stop-ping weight of this single state is the sum of all pathsthrough the original FST, yielding the probabilityP (s|t, ?
), along with expected parameter counts ac-cording to our current distributions.
We then reesti-mate ?
using the expected counts plus pseudo-countsgiven by priors, and repeat this process until conver-gence.Besides smoothing our estimated distributions,the pseudo-counts given by our asymmetric Dirich-let priors favor multinomials that retain the adultword form (swaps, identical lemmas, and identicalinflections).
Concretely, we use pseudo-counts of.5 for these favored outcomes, and pseudo-counts of.01 for all others.6In practice, 109 of the child sentences in our dataset cannot be translated into a corresponding adultversion using our model.
This is due to a range ofrare phenomena like rephrasing, lexical word swapsand word-order errors.
In these cases, the composedFST has no valid paths from start to finish and thesentence is removed from training.
We run EM for100 iterations, at which time the log likelihood of allsentences generally converges to within .01.5.5 DecodingAfter training our noise model, we apply the sys-tem to translate divergent child language to adult-like speech.
As in training, the noise FST is com-posed with the FST for each child sentence s. In6corresponding to Dirichlet hyperparameters of 1.5 and 1.01respectively.130place of the adult sentence, the language model FSTis used, yielding:FSTdecode = FSTlm ?
FSTnoise ?
FSTsEach path through FSTdecode corresponds to anadult translation and derivation (t, d), with pathweight P (s, d|t, ?
)P (t).
Thus, the highest-weightpath corresponds to the most likely translation andderivation pair:argmaxt,dP (t, d|s, ?
)We use a dynamic program to find the n highest-weight paths with distinct adult sentences t. This canbe viewed as finding the n most likely adult trans-lations, using a Viterbi approximation P (t|s, ?)
=argmaxd P (t, d|s, ?).
In our experiments we setn = 50.
A simplified FSTdecode example is shownin Figure 1.5.6 Discriminative RerankingTo more flexibly capture long range syntactic fea-tures, we embed our noisy channel model in a dis-criminative reranking procedure.
For each child sen-tence s, we take the n-best candidate translationst1, .
.
.
, tn from the underlying generative model, asdescribed in the previous section.
We then map eachcandidate translation ti to a d-dimensional featurevector f(s, ti).
The reranking model then uses a d-dimensional weight vector ?
to predict the candidatetranslation with highest linear score:t?
= argmaxti?
?
f(s, ti)To simulate test conditions, we train the weight vec-tor on n-best lists from 8-fold cross-validation overtraining data, using the averaged perceptron rerank-ing algorithm (Collins and Roark, 2004).
Since then-best list might not include the exact gold-standardcorrection, a target correction which maximizes ourevaluation metric is chosen from the list.
The n-bestlist is non-linearly separable, so perceptron trainingiterates for 1000 rounds, when it is terminated with-out converging.Our feature function f(s, ti) yields nine booleanand real-valued features derived from (i) the FSTthat generates child sentence s from candidate adult-form ti, and (ii) the POS sequence and dependencyparse of candidate ti obtained with the StanfordParser (de Marneffe et al 2006).
Features were se-lected based on their performance in reranking held-out development data from the training set.
Rerank-ing features are given below:GenerativeModel Probabilities: We first includethe joint probability of the child sentence s and can-didate translation ti, given by the generative model:Plm(ti)Pnoise(s|ti).
We also isolate the candidatetranslation?s language model and noise model prob-abilities as features.
Since both of these proba-bilities naturally favor shorter sentences, we scalethem to sentence length, yielding n?Plm(ti) andn?Pnoise(s|ti) respectively.
By not scaling the jointprobability, we allow the reranker to learn its ownbias towards longer or shorter corrected sentences.Contains Noun Subject, Accusative Noun Sub-ject: The first boolean feature indicates whetherthe dependency parse of candidate translation ti con-tains a ?nsubj?
relation.
The second indicates if a?nsubj?
relation exists where the dependent is an ac-cusative pronoun (e.g.
?Him ate the cookie?).
Thesefeatures and the one following have previously beenused in classifier based error detection (Morley andPrud?hommeaux, 2012).Contains Finite Verb: This boolean feature istrue if the POS tags of ti include a finite verb.
Thisfeature differentiates structures like ?I am going?from ?I going.
?Question Template Features: We define tem-plates for wh- and yes-no questions.
A sentencefits the wh- question template if it begins with a wh-word, followed by an auxiliary or copula verb (e.g.
?Who did...?).
A sentence fits the yes-no templatewhen it begins with an auxiliary or copula verb, thena noun subject followed by a verb or adjective (e.g.
?Are you going...?).
We include one boolean featurefor each of these templates indicating when a tem-plate match is inappropriate, when the original childutterance terminates in a period instead of a questionmark.
In addition to the two features for inappropri-ate template matches, we have a single feature thatsignals appropriate matches of either question tem-plate ?
when the original child utterance terminatesin a question mark.131Child Utterance Human Correction Machine CorrectionI am not put in my mouth.
I am not putting it in my mouth.
I am not going to put it in my mouth.This one have water?
Does this one have water?
This one has water?Want to read the book.
I want to read the book.
You want to read the book.Why you going to get two?
Why are you going to get two?
Why are you going to have two?You very sticky.
You are very sticky.
You are very sticky.He no like.
He does not like it.
He does not like that.Yeah it looks a lady.
Yeah it looks like a lady Yeah it looks like a lady.Eleanor come too.
Eleanor came too.
Eleanor come too.Desk in here.
The desk is in here Desk is in here.Why he?s doc?
Why is he called doc?
He?s up doc?Table 2: Randomly selected test output generated by our complete error correction model, along with correspondingchild utterances and human corrections.6 Experiments and AnalysisBaselines We compare our system?s performancewith two pre-existing baselines.
The first is a stan-dard phrase-based machine translation system usingMOSES (Koehn et al 2007) with GIZA++ (Ochand Ney, 2003) word alignments.
We hold out 9%of the training data for tuning using the MERT algo-rithm with BLEU objective (Och, 2003).The second baseline is our implementation of theESL error correction system described by Park andLevy (2011).
Like our system, this baseline trainsFST noise models using EM in the V-expectationsemiring.
Our noise model is crafted specificallyfor the child language domain, and so differs fromPark and Levy?s in several ways: First, we capture awider range of word-swaps, with richer parameteri-zation allowing many more translation options.
As aresult, our model has 6,718 parameters, many morethan the ESL model?s 187.
These parameters corre-spond to learned probability distributions, whereasin the ESL model many of the distributions are fixedas uniform.
We also capture a larger class of errors,including deletions, change of auxiliary lemma, andinflectional overgeneralizations.
Finally, we use adiscriminative reranking step to model long-rangesyntactic dependencies.
Although the ESL model isoriginally geared towards fully unsupervised train-ing, we train this baseline in the same supervisedframework as our model.Evaluation and Performance We train all modelson 80% of our child-adult sentence pairs and test onthe remaining 20%.
For illustration, selected outputfrom our model is shown in Table 2.Predictions are evaluated with BLEU score (Pap-ineni et al 2002) and Word Error Rate (WER), de-fined as the minimum string edit distance (in words)between reference and predicted translations, di-vided by length of the reference.
As a control,we compare all results against scores for the uncor-rected child sentences themselves.
As reported inTable 3, our model achieves the best scores for bothmetrics.
BLEU score increases from 50 for childsentences to 62, while WER is reduced from .271 to.224.
Interestingly, MOSES achieves a BLEU scoreof 58 ?
still four points below our model ?
but ac-tually increases WER to .449.
For both metrics, theESL system increases error.
This is not surprisinggiven that its intended application is in an entirelydifferent domain.Error Analysis We measured the performance ofour model over the six most common categoriesof child divergence, including deletions of variousfunction words and overgeneralizations of past tenseforms (e.g.
?maked?
for ?made?).
We first iden-tified model parameters associated with each cate-gory, and then counted the number of correct and in-correct parameter firings on the test sentences.
AsTable 4 indicates, our model performs reasonablywell on ?be?
verb deletions, preposition deletions,and overgeneralizations, but has difficulty correctingpronoun and auxiliary deletions.In general, hypothesizing dropped words burdensthe noise model by adding additional draws frommultinomial distributions to the derivation.
To pre-132BLEU WERWER reranking 62.12 .224BLEU reranking 60.86 .231No reranking 60.37 .233Moses 58.29 .449ESL 40.76 .318Child Sentences 49.55 .271Table 3: WER and BLEU scores.
Our system?s perfor-mance using various reranking schemes (BLEU objec-tive, WER objective and none) is contrasted with MosesMT and ESL error correction baselines, as well as un-corrected test sentences.
Best performance under eachmetric is shown in bold.dict a deletion, either the language model or thereranker must strongly prefer including the omit-ted word.
A syntax-based noise model may achievebetter performance in detecting and correcting childword drops.While our model parameterization and perfor-mance rely on the largely constrained nature ofchild language errors, we observe some instances inwhich it is overly restrictive.
For 10% of utterancesin our corpus, it is impossible to recover the exactgold-standard adult sentence.
These sentences fea-ture errors like reordering or lexical lemma swaps ?for example ?I talk Mexican?
for ?I speak Spanish.
?While our model may correct other errors in thesesentences, a perfect correction is unattainable.Sometimes, our model produces appropriateforms which by happenstance do not conform to theannotators?
decision.
For example, in the secondrow of Table 2, the model corrects ?This one havewater??
to ?This one has water?
?, instead of themore verbose correction chosen by the annotators(?Does this one have water??).
Similarly, our modelsometimes produces corrections which seem appro-priate in isolation, but do not preserve the meaningimplied by the larger conversational context.
For ex-ample, in row three of Table 2, the sentence ?Wantto read the book.?
is recognized both by our hu-man annotators and the system as requiring a pro-noun subject.
Unlike the annotators, however, themodel has no knowledge of conversational context,so it chooses the highest probability pronoun ?
inthis case ?you?
?
instead of the contextually correct?I.
?Error Type Count F1 P RBe Deletions 63 .84 .84 .84Pronoun Deletions 30 .15 .38 .1Aux.
Deletions 30 .21 .44 .13Prep.
Deletions 26 .65 .82 .54Det.
Deletions 22 .48 .73 .36Overgen.
Past 7 .92 1.0 .86Table 4: Frequency of the six most common error typesin test data, along with our model?s corresponding F-measure, precision and recall.
All counts are ?.12 atp = .05 under a binomial normal approximation inter-val.0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100%404550556065.20.22.24.26.28.30.32% Train DataBLEU WERFigure 2: Performance with limited training data.
WERis drawn as the dashed line, and BLEU as the solid line.Learning Curves In Figure 2, we see that thelearning curves for our model initially rise sharply,then remain relatively flat.
Using only 10% ofour training data (80 sentences), we increase BLEUfrom 44 (using just the language model) to almost61.
We only reach our reported BLEU score of 62when adding the final 20% of training data.
Thisresult emphasizes the specificity of our parameteri-zation.
Because our model is so tailored to the child-language scenario, only a few examples of each er-ror type are needed to find good parameter values.We suspect that more annotated data would lead to acontinued but slow increase in performance.Training and Testing across Children We useour system to investigate the hypothesis that lan-guage acquisition follows a similar path across chil-dren (Brown, 1973).
To test this hypothesis, we trainour model on all children excluding Adam, whoalone is responsible for 21% of our sentences.
Wethen test the learned model on the separated Adam133Trained on: BLEU WERAdam 72.58 .226All Others 69.83 .186Uncorrected 45.54 .278Table 5: Performance on Adam?s sentences training onother children, versus training on himself.
Best perfor-mance under each metric is shown in bold.data.
These results are contrasted with performanceof 8-fold cross validation training and testing solelyon Adam?s utterances.
Performance statistics aregiven in Table 5.We first note that models trained in both scenar-ios lead to large error reductions over the child sen-tences.
This provides evidence that our model cap-tures general, and not child-specific, error patterns.Although training exclusively on Adam does leadto increased BLEU score (72.58 vs 69.83), WER isminimized when using the larger volume of train-ing data from other children (.186 vs .226).
Takenas a whole, these results suggest that training andtesting on separate children does not degrade perfor-mance.
This finding supports the general hypothesisof shared developmental paths.Plotting Child Language Errors over Time Af-ter training on annotated data, we predict diver-gences in all available data from the children inRoger Brown?s 1973 study ?
Adam, Eve and Sarah?
as well as Abe (Kuczaj, 1977), a child from a sep-arate study over a similar age-range.
We plot eachchild?s per-utterance frequency of preposition omis-sions in Figure 3.
Since we evaluate over 65,000utterances and reranking has no impact on preposi-tion drop prediction, we skip the reranking step tosave computation.In Figure 3, we see that Adam and Sarah?s prepo-sition drops spike early, and then gradually decreasein frequency as their preposition use moves towardsthat of an adult.
Although Eve?s data covers an ear-lier time period, we see that her pattern of prepo-sition drops shows a similar spike and gradual de-crease.
This is consistent with Eve?s general lan-guage precocity.
Brown?s conclusion ?
that the lan-guage development of these three children advancedin similar stages at different times ?
is consistentwith our predictions.
However, when we examine0% 12 1% 22 2% 32 3% 42 4%5565156535657565%56056015603 89.
Trai.n.D8ta8BaLEUWR?D???an????an.R?aL?na??aR?
?Figure 3: Automatically detected preposition omissionsin un-annotated utterances from four children over time.Assuming perfect model predictions, frequencies are?.002 at p = .05 under a binomial normal approxima-tion interval.
Prediction error is given in Table 4.Abe we do not observe the same pattern.7 Thispoints to a degree of variance across children, andsuggests the use of our model as a tool for furtherempirical refinement of language development hy-potheses.Discussion Our error correction system is de-signed to be more constrained than a full-scale MTsystem, focusing parameter learning on errors thatare known to be common to child language learn-ers.
Reorderings are prohibited, lexical word swapsare limited to inflectional changes, and deletions arerestricted to function word categories.
By highly re-stricting our hypothesis space, we provide an induc-tive bias for our model that matches the child lan-guage domain.
This is particularly important sincethe size of our training set is much smaller than thatusually used in MT.
Indeed, as Figure 2 shows, verylittle data is needed to achieve good performance.In contrast, the ESL baseline suffers because itsgenerative model is too restricted for the domainof transcribed child language.
As shown above inTable 4, child deletions of function words are themost frequent error types in our data.
Since the ESLmodel does not capture word deletions, and has amore restricted notion of word swaps, 88% of childsentences in our training corpus cannot be translatedto their reference adult versions.
The result is thatthe ESL model tends to rely too heavily on the lan-guage model.
For example, on the sentence ?I com-7Though it is of course possible that a similar spike anddrop-off occurred earlier in Abe?s development.134ing to you,?
the ESL model improves n-gram prob-ability by producing ?I came to you?
instead of thecorrect ?I am coming to you?.
This increases errorover the child sentence itself.In addition to the domain-specific generativemodel, our approach has the advantage of long-range syntactic information encoded by rerankingfeatures.
Although the perceptron algorithm placeshigh weight on the generative model probability, italters the predictions in 17 out of 201 test sentences,in all cases an improvement.
Three of these rerank-ing changes add a noun subject, five enforce ques-tion structure, and nine add a main verb.7 Conclusion and Future WorkIn this paper we introduce a corpus of divergentchild sentences with corresponding adult forms, en-abling the systematic computational modeling ofchild language by relating it to adult grammar.
Wepropose a child-to-adult translation task as a meansto investigate child language development, and pro-vide an initial model for this task.Our model is based on a noisy-channel assump-tion, allowing for the deletion and corruption of in-dividual words, and is trained using FST techniques.Despite the debatable cognitive plausibility of oursetup, our results demonstrate that our model cap-tures many standard divergences and reduces theaverage error of child sentences by approximately20%, with high performance on specific frequentlyoccurring error types.The model allows us to chart aspects of languagedevelopment over time, without the need for addi-tional human annotation.
Our experiments show thatchildren share common developmental stages in lan-guage learning, while pointing to child-specific sub-tleties in preposition use.In future work, we intend to dynamically modelchild language ability as it grows and shifts in re-sponse to internal processes and external stimuli.We also plan to develop and train models specializ-ing in the detection of specific error categories.
Byexplicitly shifting our model?s objective from child-adult translation to the detection of some particularerror, we hope to improve our analysis of child di-vergences over time.AcknowledgmentsThe authors thank the reviewers and acknowledgesupport by the NSF (grant IIS-1116676) and a re-search gift from Google.
Any opinions, findings, orconclusions are those of the authors, and do not nec-essarily reflect the views of the NSF.ReferencesA.
Alishahi.
2010.
Computational modeling of humanlanguage acquisition.
Synthesis Lectures on HumanLanguage Technologies, 3(1):1?107.C.
Allauzen, M. Riley, J. Schalkwyk, W. Skut, andM.
Mohri.
2007.
OpenFst: A general and efficientweighted finite-state transducer library.
Implementa-tion and Application of Automata, pages 11?23.R.H.
Baayen, R. Piepenbrock, and L. Gulikers.
1996.CELEX2 (CD-ROM).
Linguistic Data Consortium.E.
Bates, I. Bretherton, and L. Snyder.
1988.
From firstwords to grammar: Individual differences and disso-ciable mechanisms.
Cambridge University Press.D.C.
Bellinger and J.B. Gleason.
1982.
Sex differencesin parental directives to young children.
Sex Roles,8(11):1123?1139.L.
Bliss.
1988.
The development of modals.
Journal ofApplied Developmental Psychology, 9:253?261.L.
Bloom, L. Hood, and P. Lightbown.
1974.
Imitation inlanguage development: If, when, and why.
CognitivePsychology, 6(3):380?420.L.
Bloom, P. Lightbown, L. Hood, M. Bowerman,M.
Maratsos, and M.P.
Maratsos.
1975.
Structure andvariation in child language.
Monographs of the Soci-ety for Research in Child Development, pages 1?97.L.
Bloom.
1973.
One word at a time: The use of singleword utterances before syntax.
Mouton.P.
Bloom.
1990.
Subjectless sentences in child language.Linguistic Inquiry, 21(4):491?504.J.N.
Bohannon III and A.L.
Marquis.
1977.
Chil-dren?s control of adult speech.
Child Development,48(3):1002?1008.R.
Brown.
1973.
A first language: The early stages.Harvard University Press.V.
Carlson-Luden.
1979.
Causal understanding in the10-month-old.
Ph.D. thesis, University of Colorado atBoulder.E.C.
Carterette and M.H.
Jones.
1974.
Informal speech:Alphabetic & phonemic texts with statistical analysesand tables.
University of California Press.M.
Chodorow and C. Leacock.
2000.
An unsupervisedmethod for detecting grammatical errors.
In Proceed-ings of the North American Chapter of the Associationfor Computational Linguistics, pages 140?147.135M.
Collins and B. Roark.
2004.
Incremental parsing withthe perceptron algorithm.
In Proceedings of the Asso-ciation for Computational Linguistics, pages 111?118,Barcelona, Spain, July.M.
Connor, Y. Gertner, C. Fisher, and D. Roth.
2008.Baby SRL: Modeling early language acquisition.
InProceedings of the Conference on Computational Nat-ural Language Learning, pages 81?88.R.
Dale and A. Kilgarriff.
2011.
Helping our own: TheHOO 2011 pilot shared task.
In Proceedings of the Eu-ropean Workshop on Natural Language Generation,pages 242?249.M.C.
de Marneffe, B. MacCartney, and C.D.
Manning.2006.
Generating typed dependency parses fromphrase structure parses.
In Proceedings of The In-ternational Conference on Language Resources andEvaluation, volume 6, pages 449?454.M.J.
Demetras, K.N.
Post, and C.E.
Snow.
1986.
Feed-back to first language learners: The role of repetitionsand clarification questions.
Journal of Child Lan-guage, 13(2):275?292.M.J.
Demetras.
1989.
Working parents?
conversationalresponses to their two-year-old sons.M.
Dreyer, J.R. Smith, and J. Eisner.
2008.
Latent-variable modeling of string transductions with finite-state methods.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing,pages 1080?1089.A.
Echihabi and D. Marcu.
2003.
A noisy-channel ap-proach to question answering.
In Proceedings of theAssociation for Computational Linguistics, pages 16?23.J.
Eisner.
2001.
Expectation semirings: Flexible EM forlearning finite-state transducers.
In Proceedings of theESSLLI workshop on finite-state methods in NLP.M.
Gamon.
2011.
High-order sequence modeling forlanguage learner error detection.
In Proceedings of theWorkshop on Innovative Use of NLP for Building Ed-ucational Applications, pages 180?189.L.C.G.
Haggerty.
1930.
What a two-and-one-half-year-old child said in one day.
The Pedagogical Seminaryand Journal of Genetic Psychology, 37(1):75?101.W.S.
Hall, W.C. Tirre, A.L.
Brown, J.C. Campoine, P.F.Nardulli, HO Abdulrahman, MA Sozen, W.C. Schno-brich, H. Cecen, J.G.
Barnitz, et al1979.
Thecommunicative environment of young children: Socialclass, ethnic, and situational differences.
Bulletin ofthe Center for Children?s Books, 32:08.W.S.
Hall, W.E.
Nagy, and R.L.
Linn.
1980.
Spokenwords: Effects of situation and social group on oralword usage and frequency.
University of Illinois atUrbana-Champaign, Center for the Study of Reading.W.S.
Hall, W.E.
Nagy, and G. Nottenburg.
1981.
Sit-uational variation in the use of internal state words.Technical report, University of Illinois at Urbana-Champaign, Center for the Study of Reading.H.
Hamburger and S. Crain.
1984.
Acquisition of cogni-tive compiling.
Cognition, 17(2):85?136.R.P.
Higginson.
1987.
Fixing: Assimilation in languageacquisition.
University Microfilms International.M.H.
Jones and E.C.
Carterette.
1963.
Redundancyin children?s free-reading choices.
Journal of VerbalLearning and Verbal Behavior, 2(5-6):489?493.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, et al2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the Association for Computational Linguis-tics (Interactive Poster and Demonstration Sessions),pages 177?180.S.
A. Kuczaj.
1977.
The acquisition of regular and irreg-ular past tense forms.
Journal of Verbal Learning andVerbal Behavior, 16(5):589?600.J.
Lee and S. Seneff.
2006.
Automatic grammar cor-rection for second-language learners.
In Proceedingsof the International Conference on Spoken LanguageProcessing.X.
Lu.
2009.
Automatic measurement of syntactic com-plexity in child language acquisition.
InternationalJournal of Corpus Linguistics, 14(1):3?28.B.
MacWhinney.
2000.
The CHILDES project: Tools foranalyzing talk, volume 2.
Psychology Press.B.
MacWhinney.
2007.
The TalkBank project.
Cre-ating and digitizing language corpora: SynchronicDatabases, 1:163?180.M.
Mohri.
2008.
System and method of epsilon removalof weighted automata and transducers, June 3.
USPatent 7,383,185.E.
Morley and E. Prud?hommeaux.
2012.
Using con-stituency and dependency parse features to identify er-rorful words in disordered language.
In Proceedingsof the Workshop on Child, Computer and Interaction.A.
Ninio, C.E.
Snow, B.A.
Pan, and P.R.
Rollins.1994.
Classifying communicative acts in children?sinteractions.
Journal of Communication Disorders,27(2):157?187.F.J.
Och and H. Ney.
2003.
A systematic comparison ofvarious statistical alignment models.
ComputationalLinguistics, 29(1):19?51.F.J.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proceedings of the Associationfor Computational Linguistics, pages 160?167.R.E.
Owens.
2008.
Language development: An intro-duction.
Pearson Education, Inc.136K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu.
2002.BLEU: a method for automatic evaluation of machinetranslation.
In Proceedings of the Association forComputational Linguistics, pages 311?318.Y.A.
Park and R. Levy.
2011.
Automated whole sentencegrammar correction using a noisy channel model.
Pro-ceedings of the Association for Computational Lin-guistics, pages 934?944.A.M.
Peters.
1987.
The role of imitation in the devel-oping syntax of a blind child in perspectives on repeti-tion.
Text, 7(3):289?311.K.
Post.
1992.
The language learning environment oflaterborns in a rural Florida community.
Ph.D. thesis,Harvard University.C.
Quirk, C. Brockett, and W. Dolan.
2004.
Monolin-gual machine translation for paraphrase generation.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing, pages 142?149.T.
Regier.
2005.
The emergence of words: Attentionallearning in form and meaning.
Cognitive Science,29(6):819?865.A.
Ritter, C. Cherry, and W.B.
Dolan.
2011.
Data-drivenresponse generation in social media.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 583?593.B.
Roark, R. Sproat, C. Allauzen, M. Riley, J. Sorensen,and T. Tai.
2012.
The OpenGrm open-source finite-state grammar software libraries.
In Proceedings ofthe Association for Computational Linguistics (SystemDemonstrations), pages 61?66.A.
Rozovskaya, M. Sammons, J. Gioja, and D. Roth.2011.
University of Illinois system in HOO text cor-rection shared task.
In Proceedings of the EuropeanWorkshop on Natural Language Generation, pages263?266.J.
Sachs.
1983.
Talking about the there and then: Theemergence of displaced reference in parent-child dis-course.
Children?s Language, 4.K.
Sagae, A. Lavie, and B. MacWhinney.
2005.
Auto-matic measurement of syntactic development in childlanguage.
In Proceedings of the Association for Com-putational Linguistics, pages 197?204.S.
Sahakian and B. Snyder.
2012.
Automatically learn-ing measures of child language development.
Pro-ceedings of the Association for Computational Lin-guistics (Volume 2: Short Papers), pages 95?99.C.E.
Snow, F. Shonkoff, K. Lee, and H. Levin.
1986.Learning to play doctor: Effects of sex, age, and ex-perience in hospital.
Discourse Processes, 9(4):461?473.E.L.
Stine and J.N.
Bohannon.
1983.
Imitations, inter-actions, and language acquisition.
Journal of ChildLanguage, 10(03):589?603.X.
Sun, J. Gao, D. Micol, and C. Quirk.
2010.
Learningphrase-based spelling error models from clickthroughdata.
In Proceedings of the Association for Computa-tional Linguistics, pages 266?274.P.
Suppes.
1974.
The semantics of children?s language.American Psychologist, 29(2):103.T.Z.
Tardif.
1994.
Adult-to-child speech and languageacquisition in Mandarin Chinese.
Ph.D. thesis, YaleUniversity.V.
Valian.
1991.
Syntactic subjects in the early speech ofAmerican and Italian children.
Cognition, 40(1-2):21?81.L.
Van Houten.
1986.
Role of maternal input inthe acquisition process: The communicative strategiesof adolescent and older mothers with their languagelearning children.
In Boston University Conference onLanguage Development.A.
Warren-Leubecker and J.N.
Bohannon III.
1984.
Into-nation patterns in child-directed speech: Mother-fatherdifferences.
Child Development, 55(4):1379?1385.A.
Warren.
1982.
Sex differences in speech to children.Ph.D.
thesis, Georgia Institute of Technology.B.
Wilson and A.M. Peters.
1988.
What are you cookin?on a hot?
: A three-year-old blind child?s ?violation?
ofuniversal constraints on constituent movement.
Lan-guage, 64:249?273.137138
