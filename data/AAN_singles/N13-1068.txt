Proceedings of NAACL-HLT 2013, pages 607?611,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsEstimating effect size across datasetsAnders S?gaardCenter for Language TechnologyUniversity of Copenhagensoegaard@hum.ku.dkAbstractMost NLP tools are applied to text that is dif-ferent from the kind of text they were eval-uated on.
Common evaluation practice pre-scribes significance testing across data pointsin available test data, but typically we onlyhave a single test sample.
This short paperargues that in order to assess the robustnessof NLP tools we need to evaluate them ondiverse samples, and we consider the prob-lem of finding the most appropriate way to es-timate the true effect size across datasets ofour systems over their baselines.
We applymeta-analysis and show experimentally ?
bycomparing estimated error reduction over ob-served error reduction on held-out datasets ?that this method is significantly more predic-tive of success than the usual practice of usingmacro- or micro-averages.
Finally, we presenta new parametric meta-analysis based on non-standard assumptions that seems superior tostandard parametric meta-analysis.1 IntroductionNLP tools and online services such as the StanfordParser or Google Translate are used for a wide va-riety of purposes and therefore also on very differ-ent kinds of data.
Some use the Stanford Parserto parse literature (van Cranenburgh, 2012), whileothers use it for processing social media content(Brown, 2011).
The parser, however, was not neces-sarily evaluated on literature or social media contentduring development.
Still, users typically expectreasonable performance on any natural language in-put.
This paper asks what we as developers can doto estimate the effect of a change to our system ?
noton the labeled test data that happens to be availableto us, but on future, still unseen datasets provided byour end users.The usual practice in NLP is to evaluate a sys-tem on a small sample of held-out labeled data.The observed effect size on this sample is then val-idated by significance testing across data points,testing whether the observed difference in perfor-mance means is likely to be due to mere chance.The preferred significance test is probably the non-parametric paired bootstrap (Efron and Tibshirani,1993; Berg-Kirkpatrick et al 2012), but many re-searchers also resort to Student?s t-test for depen-dent means relying on the assumption that their met-ric scores are normally distributed.Such significance tests tell us nothing about howlikely our change to our system is to lead to improve-ments on new datasets.
The significance tests all relyon the assumption that our datapoints are sampledi.i.d.
at random.
The significance tests only tell ushow likely it is that the observed difference in per-formance means would change if we sampled a big-ger test sample the same way we sampled the onewe have available to us right now.In standard machine learning papers a similar sit-uation arises.
If we are developing a new percep-tron learning algorithm, for example, we are inter-ested in how likely the new learning algorithm is toperform better than other perceptron learning algo-rithms across datasets, and we may for that reasonevaluate it on a large set of repository datasets.Demsar (2006) presents motivation for using non-parametric methods such as the Wilcoxon signed607rank test to estimate significance across datasets.The t-test is based on means, and typically resultsacross datasets are not commensurable.
The t-test is also extremely sentitive to outliers.
Noticealso that typically we do not have enough datasetsto do paired bootstrapping (van den Noortgate andOnghena, 2005).In this paper we will assume that the Wilcoxonsigned rank test provides a reasonable estimate ofthe significance of an observed difference in perfor-mance means across datasets, or of the significanceof observed error reductions, but note that this stilldepends on the assumption that datasets are sampledi.i.d.
at random.
More importantly, a non-parametrictest across data sets does not provide an actual esti-mate of the effect size.
Estimating effect size is im-portant, e.g.
when there is a trade-off between per-formance gains and computational efficiency.In evaluations across datasets in NLP we typicallyuse the macro-average as an estimate of effect size,but in other fields such as psychology or medicine itis more common to use a weighted mean obtainedusing what is known as the fixed effects model orthe random effects model for meta-analysis.The experiments reported on in this paper fo-cus on estimating error reduction and show thatmeta-analysis is generally superior to macro- andmicro-average in terms of predicting future error re-ductions.
Parametric meta-analysis, however, over-parameterizes the distribution of error reductions,leading to some instability.
While meta-analysis isgenerally superior to macro-average, it is sometimesoff by a large margin.
We therefore introduce a newparametric meta-analysis that seems better suited topredicting error reductions.
In our experiments testset sizes are balanced, so micro-averages will benear-identical to macro-averages.2 Meta-analysisMeta-analysis is the statistical analysis of the ef-fect sizes of several studies and is very popularin fields such as psychology or medicine.
Meta-analysis has not been applied very often to NLP.In NLP most people work on applying new meth-ods to old datasets, and meta-analysis is designedto analyze series of studies applying old methods tonew datasets, e.g.
running the same experiments onnew subjects.
However, meta-analysis is applicableto experiments with multiple datasets.In psychology or medicine you often see stud-ies running similar experiments on different sam-ples with very different results.
Meta-analysis stemsfrom the observation that if we want to estimate aneffect from a large set of studies, the average ef-fect across all the studies will put too much weighton results obtained on small datasets in which youtypically see more variance.
The most popular ap-proaches to meta-analysis are the fixed effects andthe random effects model.
The fixed effects model isapplicable when you assume a true effect size (esti-mated by the individual studies).
If you cannot makethat assumption because the studies may differ invarious aspects, leading the within-study estimatesto be estimates of slightly different effect sizes, youneed to use the random effects model.
Both ap-proaches to meta-analysis are parametric and rely onthe effect sizes to be normally distributed.2.1 Fixed effects modelIn the fixed effects model we weight the effect sizesT1, .
.
.
, TM ?
or error reductions, in our case ?
bythe inverse of the variance vi in the study, i.e.
wi =1vi .
The combined effect size T is then:T?
=?Mi?1wiTi?Mi?1 wiThe variance of the combined effect is now:v = 1?Mi?1 wiand the 95% confidence interval is then T?
?1.96?v.2.2 Random effects modelIn the random effects model we replace the variancevi with the variance plus between-studies variance?2:?2 =?ki?1 wiT 2i ?
(?ki?1 wiTi)2?ki?1 wi?
df?ki?1 wi ?
?ki?1 w2i?ki?1 wi(1)with df = N ?
1, except all negative values arereplaced by 0.608?comp rec sci talk othersys others sport vehicles politics religion(a) (b) (c) (d) (e) (f) (g) (h)Figure 1: Hierarchical structure of 20 Newsgroups.
(a) IBM, MAC, (b) GRAPHICS, MS-WINDOWS, X-WINDOWS,(c) BASEBALL, HOCKEY, (d) AUTOS, MOTORCYCLES, (e) CRYPTOGRAPHY, ELECTRONICS, MEDICINE, SPACE,(f) GUNS, MIDEAST, MISCELLANEOUS, (g) ATHEISM, CHRISTIANITY, MISCELLANEOUS, (h) FORSALEmacro-av fixed random gumbelk = 5err.
-0.1656 -0.0350 -0.0428 -0.0400p-value - < 0.001 < 0.001 < 0.001k = 10err.
-0.1402 -0.0329 -0.0413 -0.0359p-value - < 0.001 < 0.001 < 0.001k = 15err.
-0.0809 -0.0799 -0.0804 -0.0704p-value - < 0.001 < 0.001 < 0.001Figure 2: Using macro-average and meta-analysis to pre-dict error reductions on document classification datasetsbased on k observations.
The scores are averages across20 experiments.
The p-values were computed usingWilcoxon signed rank tests.The random effects model is obviously more con-servative in its confidence intervals, and often wewill not be able to obtain significance across datasetsusing a random effects model.
If, for example, weapply a fixed effects model to test whether Bernoullinaive Bayes (NB) fairs better than a perceptron (P)model on 25 randomly extracted cross-domain doc-ument classification problem instances from the 20Newsgroups dataset (see Sect.
4), the 95% confi-dence interval is [3.9%, 5.2%].
The weighted meanis 4.6% (macro-average 3.9%).
Using a randomeffects model on the same 25 datasets, the 95%confidence interval becomes [?6.5%, 6.6%].
Theweighted mean estimate is also slighly differentfrom that of a fixed effects model.
The first questionwe ask is which of these models provides the best es-timate of effect size as observed on future datasets?2.3 The error reductions distributionBoth the fixed effects and the random effects modelassume that effect sizes are normally distributed.
Wecan apply Darling-Anderson tests to test whether er-ror reductions in 20 Newsgroups are in fact normallydistributed.
Even a small sample of ten 20 News-groups datasets provides enough evidence to rejectthe hypothesis that error reductions (of NB overP) are normally distributed.
The Darling-Andersontests consistently tell us that the chance that our sam-ple distribtutions of error reductions are normallydistributed is below 1%.
The over-paramaterizationmeans that the estimates we get are unstable.
Whileboth models are superior to macro-average esti-mates, they may provide ?far-off?
estimates.Using Darling-Anderson tests we could also re-ject the hypothesis that error reductions were lo-gistically distributed, but we did not find evidencefor rejecting the hypothesis that error reductions areGumbel-distributed.1 Gumbel distributions are usedto model error distributions in the latent variable for-mulation of multinomial logit regression.
A para-metric meta-analysis model based on the assumptionthat error reductions are Gumbel distributed is an in-teresting alternative to non-parametric meta-analysis(Hedges and Olkin, 1984; van den Noortgate andOnghena, 2005), since there seems to be little con-sensus in the literature about the best way to ap-proach non-parametric meta-analysis.Gumbel distributions take the following form:1?
ez?e?zwhere z = x???
with ?
the location, and ?
thescale.
We fit a Gumbel distribution to our weightederror reductions (wiTi) and compute the combined1Abidin et al(2012) has shown that Darling-Anderson issuperior to other goodness-of-fit tests for Gumbel distributions.609macro-av fixed random gumbelk = 5err.
0.0531 0.0525 0.0526 0.0489p-value - ?
0.98 ?
0.98 ?
0.79k = 7err.
0.0928 0.0852 0.0852 0.0858p-value - < 0.001 < 0.001 < 0.001k = 9err.
0.0587 0.05743 0.05743 0.0532p-value - ?
0.68 ?
0.68 ?
0.13Figure 3: Using macro-average and meta-analysis to pre-dict error reductions in cross-lingual dependency parsing.See text for details.effectT?
= ?
+ 0.57721?1M?Mi?1 wiwhere 0.57721 is the Euler-Mascheroni constant,and the variance of the combined effect v = pi26 ?2.3 Experiments in document classificationand dependency parsingOur first experiment makes use of the 20 News-groups document classification dataset.2 The top-ics in 20 Newsgroups are hierarchically structured,which enables us to extract a large set of binaryclassification problems with considerable bias be-tween source and target data (Chen et al 2009;Sun et al 2011).
See the hierarchy in Figure 1.We extract 20 high-level binary classification prob-lems by considering all pairs of top-level cate-gories, e.g.
COMPUTERS-RECREATIVE (comp-rec).For each of these 20 problems, we have differ-ent possible datasets, e.g.
IBM-BASEBALL, MAC-MOTORCYCLES, etc.
A problem instance takestraining and test data from two different datasets be-long to the same high-level problem.
For exam-ple a problem instance could be learning to dis-tinguish articles about Macintosh and motorcyclesMAC-MOTORCYCLES (evaluated on the 20 News-groups test section) using labeled data from IBM-BASEBALL (the training section).
In total we have288 available problem instances in the 20 News-groups dataset.In our first experiment we are interested in pre-dicting the error reductions of a naive Bayes learner2http://people.csail.mit.edu/jrennie/20Newsgroups/over a perceptron model.
We use publicly availableimplementations with default parameters.3 In eachexperiment we randomly select k datasets and es-timate the true effect size using macro-average, afixed effects model, a random effects model, anda corrected random effects model.
In order to es-timate the within-study variance we take 50 pairedbootstrap samples of the system outputs.
We evalu-ate our estimates against the observed average effectacross 5 new randomly extracted datasets.
For eachk we repeat the experiment 20 times and report aver-age error.
We vary k to see how many observationsare needed for our estimates to be reliable.The results are presented in Figure 2.
We note thatmeta-analysis provides much better estimates thanmacro-averages across the board.
Our parametricmeta-analysis based on the assumption that error re-ductions are Gumbel distributed performs best withmore observations.Our second experiment repeats the same proce-dure using available data from cross-lingual depen-dency parsing.
We use the submitted results by par-ticipants in the CoNLL-X shared task (Buchholz andMarsi, 2006) and try to predict the error reduction ofone system over another given k many observations.Given that we only have 12 submissions per systemwe use k ?
{5, 7, 9} randomly extracted datasetsfor observations and test on another five randomlyextracted datasets.
While results (Figure 3) are onlystatistically significant with k = 7, we see that meta-analysis estimates effect size across data sets betterthan macro-average in all cases.4 ConclusionsWe have argued that evaluation across datasets isimportant for developing robust NLP tools, andthat meta-analysis can provide better estimatesof effect size across datasets than macro-average.We also noted that parametric meta-analysis over-parameterizes error reduction distributions and sug-gested a new parametric method for estimating ef-fect size across datasets.AcknowledgementsAnders S?gaard is funded by the ERC Starting GrantLOWLANDS No.
313695.3http://scikit-learn.org/stable/610ReferencesNahdiya Abidin, Mohd Adam, and Habshah Midi.
2012.The goodness-of-fit test for Gumbel distribution: acomparative study.
MATEMATIKA, 28(1):35?48.Taylor Berg-Kirkpatrick, David Burkett, and Dan Klein.2012.
An empirical investigation of statistical signifi-cance in nlp.
In EMNLP.Gregory Brown.
2011.
An error analysis of relation ex-traction in social media documents.
In ACL.Sabine Buchholz and Erwin Marsi.
2006.
CoNLL-XShared Task on Multilingual Dependency Parsing.
InCoNLL.Bo Chen, Wai Lam, Ivor Tsang, and Tak-Lam Wong.2009.
Extracting discriminative concepts for domainadaptation in text mining.
In KDD.Janez Demsar.
2006.
Statistical comparisons of clas-sifiers over multiple data sets.
Journal of MachineLearning Research, 7:1?30.Bradley Efron and Robert Tibshirani.
1993.
An introduc-tion to the bootstrap.
Chapman & Hall, Boca Raton,FL.Larry Hedges and Ingram Olkin.
1984.
Nonparametricestimators of effect size in meta-analysis.
Psychologi-cal Bulletin, 96:573?580.Qian Sun, Rita Chattopadhyay, Sethuraman Pan-chanathan, and Jieping Ye.
2011.
Two-stage weight-ing framework for multi-source domain adaptation.
InNIPS.Andreas van Cranenburgh.
2012.
Literary author-ship attribution with phrase-structure fragments.
InWorkshop on Computational Linguistics for Litera-ture, NAACL.Wim van den Noortgate and Patrick Onghena.
2005.Parametric and nonparametric bootstrap methods formeta-analysis.
Behavior Research Methods, 37:11?22.611
