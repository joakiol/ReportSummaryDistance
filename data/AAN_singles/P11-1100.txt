Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 997?1006,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsAutomatically Evaluating Text Coherence Using Discourse RelationsZiheng Lin, Hwee Tou Ng and Min-Yen KanDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417{linzihen,nght,kanmy}@comp.nus.edu.sgAbstractWe present a novel model to represent andassess the discourse coherence of text.
Ourmodel assumes that coherent text implicitlyfavors certain types of discourse relation tran-sitions.
We implement this model and apply ittowards the text ordering ranking task, whichaims to discern an original text from a per-muted ordering of its sentences.
The experi-mental results demonstrate that our model isable to significantly outperform the state-of-the-art coherence model by Barzilay and Lap-ata (2005), reducing the error rate of the previ-ous approach by an average of 29% over threedata sets against human upper bounds.
We fur-ther show that our model is synergistic withthe previous approach, demonstrating an errorreduction of 73% when the features from bothmodels are combined for the task.1 IntroductionThe coherence of a text is usually reflected by its dis-course structure and relations.
In Rhetorical Struc-ture Theory (RST), Mann and Thompson (1988) ob-served that certain RST relations tend to favor oneof two possible canonical orderings.
Some rela-tions (e.g., Concessive and Conditional) favor ar-ranging their satellite span before the nucleus span.In contrast, other relations (e.g., Elaboration and Ev-idence) usually order their nucleus before the satel-lite.
If a text that uses non-canonical relation order-ings is rewritten to use canonical orderings, it oftenimproves text quality and coherence.This notion of preferential ordering of discourserelations is observed in natural language in general,and generalizes to other discourse frameworks asidefrom RST.
The following example shows a Contrastrelation between the two sentences.
(1) [ Everyone agrees that most of the nation?s oldbridges need to be repaired or replaced.
]S1 [ Butthere?s disagreement over how to do it.
]S2Here the second sentence provides contrasting infor-mation to the first.
If this order is violated withoutrewording (i.e., if the two sentences are swapped), itproduces an incoherent text (Marcu, 1996).In addition to the intra-relation ordering, suchpreferences also extend to inter-relation ordering:(2) [ The Constitution does not expressly give thepresident such power.
]S1 [ However, the presidentdoes have a duty not to violate the Constitution.
]S2[ The question is whether his only means ofdefense is the veto.
]S3The second sentence above provides a contrast to theprevious sentence and an explanation for the nextone.
This pattern of Contrast-followed-by-Cause israther common in text (Pitler et al, 2008).
Orderingthe three sentences differently results in incoherent,cryptic text.Thus coherent text exhibits measurable prefer-ences for specific intra- and inter-discourse relationordering.
Our key idea is to use the converse of thisphenomenon to assess the coherence of a text.
Inthis paper, we detail our model to capture the coher-ence of a text based on the statistical distribution ofthe discourse structure and relations.
Our methodspecifically focuses on the discourse relation transi-tions between adjacent sentences, modeling them ina discourse role matrix.997Our study makes additional contributions.
We im-plement and validate our model on three data sets,which show robust improvements over the currentstate-of-the-art for coherence assessment.
We alsoprovide the first assessment of the upper-bound ofhuman performance on the standard task of distin-guishing coherent from incoherent orderings.
To thebest our knowledge, this is also the first study inwhich we show output from an automatic discourseparser helps in coherence modeling.2 Related WorkThe study of coherence in discourse has led to manylinguistic theories, of which we only discuss algo-rithms that have been reduced to practice.Barzilay and Lapata (2005; 2008) proposed anentity-based model to represent and assess local tex-tual coherence.
The model is motivated by Center-ing Theory (Grosz et al, 1995), which states thatsubsequent sentences in a locally coherent text arelikely to continue to focus on the same entities asin previous sentences.
Barzilay and Lapata op-erationalized Centering Theory by creating an en-tity grid model to capture discourse entity transi-tions at the sentence-to-sentence level, and demon-strated their model?s ability to discern coherent textsfrom incoherent ones.
Barzilay and Lee (2004) pro-posed a domain-dependent HMM model to capturetopic shift in a text, where topics are represented byhidden states and sentences are observations.
Theglobal coherence of a text can then be summarizedby the overall probability of topic shift from the firstsentence to the last.
Following these two directions,Soricut and Marcu (2006) and Elsner et al (2007)combined the entity-based and HMM-based modelsand demonstrated that these two models are comple-mentary to each other in coherence assessment.Our approach differs from these models in thatit introduces and operationalizes another indicatorof discourse coherence, by modeling a text?s dis-course relation transitions.
Karamanis (2007) hastried to integrate local discourse relations into theCentering-based coherence metrics for the task ofinformation ordering, but was not able to obtain im-provement over the baseline method, which is partlydue to the much smaller data set and the way thediscourse relation information is utilized in heuristicconstraints and rules.To implement our proposal, we need to identifythe text?s discourse relations.
This task, discourseparsing, has been a recent focus of study in the nat-ural language processing (NLP) community, largelyenabled by the availability of large-scale discourseannotated corpora (Wellner and Pustejovsky, 2007;Elwell and Baldridge, 2008; Lin et al, 2009; Pitleret al, 2009; Pitler and Nenkova, 2009; Lin et al,2010; Wang et al, 2010).
The Penn Discourse Tree-bank (PDTB) (Prasad et al, 2008) is such a cor-pus which provides a discourse-level annotation ontop of the Penn Treebank, following a predicate-argument approach (Webber, 2004).
Crucially, thePDTB provides annotations not only on explicit (i.e.,signaled by discourse connectives such as because)discourse relations, but also implicit (i.e., inferredby readers) ones.3 Using Discourse RelationsTo utilize discourse relations of a text, we first ap-ply automatic discourse parsing on the input text.While any discourse framework, such as the Rhetor-ical Structure Theory (RST), could be applied in ourwork to encode discourse information, we have cho-sen to work with the Discourse Lexicalized Tree Ad-joining Grammar (D-LTAG) by Webber (2004) asembodied in the PDTB, as a PDTB-styled discourseparser1 developed by Lin et al (2010) has recentlybecome freely available.This parser tags each explicit/implicit relationwith two levels of relation types.
In this work,we utilize the four PDTB Level-1 types: Temporal(Temp), Contingency (Cont), Comparison (Comp),and Expansion (Exp).
This parser automaticallyidentifies the discourse relations, labels the argu-ment spans, and classifies the relation types, includ-ing identifying common entity and no relation (En-tRel and NoRel) as types.A simple approach to directly model the connec-tions among discourse relations is to use the se-quence of discourse relation transitions.
Text (2) inSection 1 can be represented by S1Comp??
S2 Cont?
?S3, for instance, when we use Level-1 types.
Insuch a basic approach, we can compile a distribu-1http://wing.comp.nus.edu.sg/?linzihen/parser/998tion of the n-gram discourse relation transition se-quences in gold standard coherent text, and a similarone for incoherent text.
For example, the above textwould generate the transition bigram Comp?Cont.We can build a classifier to distinguish one from theother through learned examples or using a suitabledistribution distance measure (e.g., KL Divergence).In our pilot work where we implemented such abasic model with n-gram features for relation tran-sitions, the performance was very poor.
Our analy-sis revealed a serious shortcoming: as the discourserelation transitions in short texts are few in num-ber, we have very little data to base the coherencejudgment on.
However, when faced with even shorttext excerpts, humans can distinguish coherent textsfrom incoherent ones, as exemplified in our exam-ple texts.
The basic approach also does not modelthe intra-relation preference.
In Text (1), a Com-parison (Comp) relation would be recorded betweenthe two sentences, irregardless of whether S1 or S2comes first.
However, it is clear that the ordering of(S1 ?
S2) is more coherent.4 A Refined ApproachThe central problem with the basic approach is in itssparse modeling of discourse relations.
In develop-ing an improved model, we need to better exploit thediscourse parser?s output to provide more circum-stantial evidence to support the system?s coherencedecision.In this section, we introduce the concept of a dis-course role matrix which aims to capture an ex-panded set of discourse relation transition patterns.We describe how to represent the coherence of a textwith its discourse relations and how to transformsuch information into a matrix representation.
Wethen illustrate how we use the matrix to formulate apreference ranking problem.4.1 Discourse Role MatrixFigure 1 shows a text and its gold standard PDTBdiscourse relations.
When a term appears in a dis-course relation, the discourse role of this term isdefined as the discourse relation type plus the argu-ment span in which the term is located (i.e., the argu-ment tag).
For instance, consider the term ?cananea?in the first relation.
Since the relation type is a[ Japan normally depends heavily on the HighlandValley and Cananea mines as well as the Bougainvillemine in Papua New Guinea.
]S1 [ Recently, Japanhas been buying copper elsewhere.
]S2 [ [ But asHighland Valley and Cananea begin operating, ]C3.1[ they are expected to resume their roles as Japan?ssuppliers.
]C3.2 ]S3 [ [ According to Fred Demler,metals economist for Drexel Burnham Lambert, NewYork, ]C4.1 [ ?Highland Valley has already startedoperating ]C4.2 [ and Cananea is expected to do sosoon.?
]C4.3 ]S45 discourse relations are present in the above text:1.
Implicit Comparison between S1 as Arg1, and S2as Arg22.
Explicit Comparison using ?but?
between S2 asArg1, and S3 as Arg23.
Explicit Temporal using ?as?
within S3 (ClauseC3.1 as Arg1, and C3.2 as Arg2)4.
Implicit Expansion between S3 as Arg1, and S4as Arg25.
Explicit Expansion using ?and?
within S4(Clause C4.2 as Arg1, and C4.3 as Arg2)Figure 1: An excerpt with four contiguous sentences fromwsj 0437, showing five gold standard discourse relations.?Cananea?
is highlighted for illustration.S# Termscopper cananea operat depend .
.
.S1 nil Comp.Arg1 nil Comp.Arg1S2Comp.Arg2nil nil nilComp.Arg1S3 nilComp.Arg2 Comp.Arg2nilTemp.Arg1 Temp.Arg1Exp.Arg1 Exp.Arg1S4 nil Exp.Arg2Exp.Arg1nilExp.Arg2Table 1: Discourse role matrix fragment for Figure 1.Rows correspond to sentences, columns to stemmedterms, and cells contain extracted discourse roles.Comparison and ?cananea?
is found in the Arg1span, the discourse role of ?cananea?
is defined asComp.Arg1.
When terms appear in different rela-tions and/or argument spans, they obtain differentdiscourse roles in the text.
For instance, ?cananea?plays a different discourse role of Temp.Arg1 in thethird relation in Figure 1.
In the fourth relation,since ?cananea?
appears in both argument spans, ithas two additional discourse roles, Exp.Arg1 and999Exp.Arg2.
The discourse role matrix thus representsthe different discourse roles of the terms across thecontinuous text units.
We use sentences as the textunits, and define terms to be the stemmed formsof the open class words: nouns, verbs, adjectives,and adverbs.
We formulate the discourse role matrixsuch that it encodes the discourse roles of the termsacross adjacent sentences.Table 1 shows a fragment of the matrix represen-tation of the text in Figure 1.
Columns correspond tothe extracted terms; rows, the contiguous sentences.A cell CTi,Sj then contains the set of the discourseroles of the term Ti that appears in sentence Sj .
Forexample, the term ?cananea?
from S1 takes part inthe first relation, so the cell Ccananea,S1 contains therole Comp.Arg1.
A cell may be empty (nil, as inCcananea,S2) or contain multiple discourse roles (asin Ccananea,S3 , as ?cananea?
in S3 participates inthe second, third, and fourth relations).
Given thesediscourse relations, building the matrix is straight-forward: we note down the relations that a term Tifrom a sentence Sj participates in, and record its dis-course roles in the respective cell.We hypothesize that the sequence of discourserole transitions in a coherent text provides clues thatdistinguish it from an incoherent text.
The discourserole matrix thus provides the foundation for com-puting such role transitions, on a per term basis.
Infact, each column of the matrix corresponds to alexical chain (Morris and Hirst, 1991) for a partic-ular term across the whole text.
The key differencesfrom the traditional lexical chains are that our chainnodes?
entities are simplified (they share the samestemmed form, instead being connected by WordNetrelations), but are further enriched by being typedwith discourse relations.We compile the set of sub-sequences of discourserole transitions for every term in the matrix.
Thesetransitions tell us how the discourse role of a termvaries through the progression of the text.
For in-stance, ?cananea?
functions as Comp.Arg1 in S1 andComp.Arg2 in S3, and plays the role of Exp.Arg1and Exp.Arg2 in S3 and S4, respectively.
As wehave six relation types (Temp(oral), Cont(ingency),Comp(arison), Exp(ansion), EntRel and NoRel) andtwo argument tags (Arg1 and Arg2) for each type,we have a total of 6 ?
2 = 12 possible dis-course roles, plus a nil value.
We define a dis-course role transition as the sub-sequence of dis-course roles for a term in multiple consecutive sen-tences.
For example, the discourse role transition of?cananea?
from S1 to S2 is Comp.Arg1?nil.
As acell may contain multiple discourse roles, a transi-tion may produce multiple sub-sequences.
For ex-ample, the length 2 sub-sequences for ?cananea?from S3 to S4, are Comp.Arg2?Exp.Arg2,Temp.Arg1?Exp.Arg2, and Exp.Arg1?Exp.Arg2.Each sub-sequence has a probability that can becomputed from the matrix.
To illustrate the calcu-lation, suppose the matrix fragment in Table 1 isthe entire discourse role matrix.
Then since thereare in total 25 length 2 sub-sequences and the sub-sequence Comp.Arg2?Exp.Arg2 has a count oftwo, its probability is 2/25 = 0.08.
A key prop-erty of our approach is that, while discourse tran-sitions are captured locally on a per-term basis, theprobabilities of the discourse transitions are aggre-gated globally, across all terms.
We believe that theoverall distribution of discourse role transitions fora coherent text is distinguishable from that for an in-coherent text.
Our model captures the distributionaldifferences of such sub-sequences in coherent andincoherent text in training to determine an unseentext?s coherence.
To evaluate the coherence of a text,we extract sub-sequences with various lengths fromthe discourse role matrix as features2 and computethe sub-sequence probabilities as the feature values.To further refine the computation of the sub-sequence distribution, we follow (Barzilay and La-pata, 2005) and divide the matrix into a salient ma-trix and a non-salient matrix.
Terms (columns) witha frequency greater than a threshold form the salientmatrix, while the rest form the non-salient matrix.The sub-sequence distributions are then calculatedseparately for these two matrices.4.2 Preference RankingWhile some texts can be said to be simply coherentor incoherent, often it is a matter of degree.
A textcan be less coherent when compared to one text, butmore coherent when compared to another.
As such,since the notion of coherence is relative, we feelthat coherence assessment is better represented as2Sub-sequences consisting of only nil values are not used asfeatures.1000a ranking problem rather than a classification prob-lem.
Given a pair of texts, the system ranks thembased on how coherent they are.
Applications ofsuch a system include differentiating a text from itspermutation (i.e., the sentence ordering of the textis shuffled) and identifying a more well-written es-say from a pair.
Such a system can easily generalizefrom pairwise ranking into listwise, suitable for theordinal ranking of a set of texts.
Coherence scoringequations can also be deduced (Lapata and Barzilay,2005) from such a model, yielding coherence scores.To induce a model for preference ranking, we usethe SVMlight package3 by (Joachims, 1999) withthe preference ranking configuration for training andtesting.
All parameters are set to their default values.5 ExperimentsWe evaluate our coherence model on the task of textordering ranking, a standard coherence evaluationtask used in both (Barzilay and Lapata, 2005) and(Elsner et al, 2007).
In this task, the system isasked to decide which of two texts is more coherent.The pair of texts consists of a source text and oneof its permutations (i.e., the text?s sentence order israndomized).
Assuming that the original text is al-ways more discourse-coherent than its permutation,an ideal system will prefer the original to the per-muted text.
A system?s accuracy is thus the numberof times the system correctly chooses the originaldivided by the total number of test pairs.In order to acquire a large data set for training andtesting, we follow the approach in (Barzilay and La-pata, 2005) to create a collection of synthetic datafrom Wall Street Journal (WSJ) articles in the PennTreebank.
All of the WSJ articles are randomly splitinto a training and a testing set; 40 articles are heldout from the training set for development.
For eacharticle, its sentences are permuted up to 20 times tocreate a set of permutations4 .
Each permutation ispaired with its source text to form a pair.We also evaluate on two other data collections(cf.
Table 2), provided by (Barzilay and Lapata,2005), for a direct comparison with their entity-based model.
These two data sets consist of Associ-ated Press articles about earthquakes from the North3http://svmlight.joachims.org/4Short articles may produce less than 20 permutations.WSJ Earthquakes AccidentsTrain # Articles 1040 97 100# Pairs 19120 1862 1996Avg.
# Sents 22.0 10.4 11.5Test # Articles 1079 99 100# Pairs 19896 1956 1986Table 2: Details of the WSJ, Earthquakes, and Accidentsdata sets, showing the number of training/testing articles,number of pairs of articles, and average length of an arti-cle (in sentences).American News Corpus, and narratives from the Na-tional Transportation Safety Board.
These collec-tions are much smaller than the WSJ data, as eachtraining/testing set contains only up to 100 sourcearticles.
Similar to the WSJ data, we construct pairsby permuting each source article up to 20 times.Our model has two parameters: (1) the term fre-quency (TF) that is used as a threshold to iden-tify salient terms, and (2) the lengths of the sub-sequences that are extracted as features.
These pa-rameters are tuned on the development set, and thebest ones that produce the optimal accuracy areTF >= 2 and lengths of the sub-sequences <= 3.We must also be careful in using the automaticdiscourse parser.
We note that the discourse parserof Lin et al (2010) comes trained on the PDTB,which provides annotations on top of the whole WSJdata.
As we also use the WSJ data for evaluation,we must avoid parsing an article that has alreadybeen used in training the parser to prevent trainingon the test data.
We re-train the parser with 24 WSJsections and use the trained parser to parse the sen-tences in our WSJ collection from the remainingsection.
We repeat this re-training/parsing processfor all 25 sections.
Because the Earthquakes andAccidents data do not overlap with the WSJ trainingdata, we use the parser as distributed to parse thesetwo data sets.
Since the discourse parser utilizesparagraph boundaries but a permuted text does nothave such boundaries, we ignore paragraph bound-aries and treat the source text as if it has only oneparagraph.
This is to make sure that we do not givethe system extra information because of this differ-ence between the source and permuted text.10015.1 Human EvaluationWhile the text ordering ranking task has been usedin previous studies, two key questions about this taskhave remained unaddressed in the previous work:(1) to what extent is the assumption that the sourcetext is more coherent than its permutation correct?and (2) how well do humans perform on this task?The answer to the first is needed to validate the cor-rectness of this synthetic task, while the second aimsto obtain the upper bound for evaluation.
We con-duct a human evaluation to answer these questions.We randomly select 50 source text/permutationpairs from each of the WSJ, Earthquakes, and Ac-cidents training sets.
We observe that some of thesource texts have formulaic structures in their ini-tial sentences that give away the correct ordering.Sources from the Earthquakes data always beginwith a headline sentence and a location-newswiresentence, and many sources from the Accidents datastart with two sentences of ?This is preliminary.
.
.
errors.
Any errors .
.
.
completed.?
We removethese sentences from the source and permuted texts,to avoid the subjects judging based on these clues in-stead of textual coherence.
For each set of 50 pairs,we assigned two human subjects (who are not au-thors of this paper) to perform the ranking.
The sub-jects are told to identify the source text from the pair.When both subjects rank a source text higher than itspermutation, we interpret it as the subjects agreeingthat the source text is more coherent than the permu-tation.
Table 3 shows the inter-subject agreements.WSJ Earthquakes Accidents Overall90.0 90.0 94.0 91.3Table 3: Inter-subject agreements on the three data sets.While our study is limited and only indicative, weconclude from these results that the task is tractable.Also, since our subjects?
judgments correlate highlywith the gold standard, the assumption that the orig-inal text is always more coherent than the permutedtext is supported.
Importantly though, human per-formance is not perfect, suggesting fair upper boundlimits on system performance.
We note that the Ac-cidents data set is relatively easier to rank, as it hasa higher upper bound than the other two.5.2 BaselineBarzilay and Lapata (2005) showed that their entity-based model is able to distinguish a source text fromits permutation accurately.
Thus, it can serve as agood comparison point for our discourse relation-based model.
We compare against their Syn-tax+Salience setting.
Since they did not automat-ically determine the coreferential information of apermuted text but obtained that from its correspond-ing source text, we do not perform automatic coref-erence resolution in our reimplementation of theirsystem.
For fair comparison, we follow their experi-ment settings as closely as possible.
We re-use theirEarthquakes and Accidents dataset as is, using theirexact permutations and pre-processing.
For the WSJdata, we need to perform our own pre-processing,thus we employed the Stanford parser5 to performsentence segmentation and constituent parsing, fol-lowed by entity extraction.5.3 ResultsWe perform a series of experiments to answer thefollowing four questions:1.
Does our model outperform the baseline?2.
How do the different features derived from us-ing relation types, argument tags, and salienceinformation affect performance?3.
Can the combination of the baseline and ourmodel outperform the single models?4.
How does system performance of these modelscompare with human performance on the task?Baseline results are shown in the first row of Ta-ble 4.
The results on the Earthquakes and Accidentsdata are quite similar to those published in (Barzilayand Lapata, 2005) (they reported 83.4% on Earth-quakes and 89.7% on Accidents), validating the cor-rectness of our reimplementation of their method.Row 2 in Table 4 shows the overall performanceof the proposed refined model, answering Question1.
The model setting of Type+Arg+Sal means thatthe model makes use of the discourse roles consist-ing of 1) relation types and 2) argument tags (e.g.,5http://nlp.stanford.edu/software/lex-parser.shtml1002WSJ Earthquakes AccidentsBaseline 85.71 83.59 89.93Type+Arg+Sal 88.06** 86.50** 89.38Arg+Sal 88.28** 85.89* 87.06Type+Sal 87.06** 82.98 86.05Type+Arg 85.98 82.67 87.87Baseline & 89.25** 89.72** 91.64**Type+Arg+SalTable 4: Test set ranking accuracy.
The first row showsthe baseline performance, the next four show our modelwith different settings, and the last row is a combinedmodel.
Double (**) and single (*) asterisks indicate thatthe respective model significantly outperforms the base-line at p < 0.01 and p < 0.05, respectively.
We followBarzilay and Lapata (2008) and use the Fisher Sign test.the discourse role Comp.Arg2 consists of the typeComp(arison) and the tag Arg2), and 3) two dis-tinct feature sets from salient and non-salient terms.Comparing these accuracies to the baseline, ourmodel significantly outperforms the baseline withp < 0.01 in the WSJ and Earthquakes data setswith accuracy increments of 2.35% and 2.91%, re-spectively.
In Accidents, our model?s performanceis slightly lower than the baseline, but the differenceis not statistically significant.To answer Question 2, we perform feature abla-tion testing.
We eliminate each of the informationsources from the full model.
In Row 3, we firstdelete relation types from the discourse roles, whichcauses discourse roles to only contain the argumenttags.
A discourse role such as Comp.Arg2 becomesArg2 after deleting the relation type.
ComparingRow 3 to Row 2, we see performance reductions onthe Earthquakes and Accidents data after eliminat-ing type information.
Row 4 measures the effect ofomitting argument tags (Type+Sal).
In this setting,the discourse role Comp.Arg2 reduces to Comp.
Wesee a large reduction in performance across all threedata sets.
This model is also most similar to the ba-sic na?
?ve model in Section 3.
These results suggestthat the argument tag information plays an impor-tant role in our discourse role transition model.
Row5 omits the salience information (Type+Arg), whichalso markedly reduces performance.
This result sup-ports the use of salience, in line with the conclusiondrawn in (Barzilay and Lapata, 2005).To answer Question 3, we train and test a com-bined model using features from both the baselineand our model (shown as Row 6 in Table 4).
Theentity-based model of Barzilay and Lapata (2005)connects the local entity transition with textual co-herence, while our model looks at the patterns ofdiscourse relation transitions.
As these two modelsfocus on different aspects of coherence, we expectthat they are complementary to each other.
The com-bined model in all three data sets gives the highestperformance in comparison to all single models, andit significantly outperforms the baseline model withp < 0.01.
This confirms that the combined model islinguistically richer than the single models as it inte-grates different information together, and the entity-based model and our model are synergistic.To answer Question 4, when compared to the hu-man upper bound (Table 3), the performance gapsfor the baseline model are relatively large, whilethose for our full model are more acceptable inthe WSJ and Earthquakes data.
For the combinedmodel, the error rates are significantly reduced inall three data sets.
The average error rate reduc-tions against 100% are 9.57% for the full model and26.37% for the combined model.
If we compute theaverage error rate reductions against the human up-per bounds (rather than an oracular 100%), the aver-age error rate reduction for the full model is 29% andthat for the combined model is 73%.
While these areonly indicative results, they do highlight the signifi-cant gains that our model is making towards reach-ing human performance levels.We further note that some of the permuted textsmay read as coherently as the original text.
This phe-nomenon has been observed in several natural lan-guage synthesis tasks such as generation and sum-marization, in which a single gold standard is inade-quate to fully assess performance.
As such, both au-tomated systems and humans may actually performbetter than our performance measures indicate.
Weleave it to future work to measure the impact of thisphenomenon.6 Analysis and DiscussionWhen we compare the accuracies of the full modelin the three data sets (Row 2), the accuracy in theAccidents data is the highest (89.38%), followed by1003that in the WSJ (88.06%), with Earthquakes at thelowest (86.50%).
To explain the variation, we exam-ine the ratio between the number of the relations inthe article and the article length (i.e., number of sen-tences).
This ratio is 1.22 for the Accidents sourcearticles, 1.2 for the WSJ, and 1.08 for Earthquakes.The relation/length ratio gives us an idea of how of-ten a sentence participates in discourse relations.
Ahigh ratio means that the article is densely intercon-nected by discourse relations, and may make dis-tinguishing this article from its permutation easiercompared to that for a loosely connected article.We expect that when a text contains more dis-course relation types (i.e., Temporal, Contingency,Comparison, Expansion) and less EntRel and NoReltypes, it is easier to compute how coherent this textis.
This is because compared to EntRel and NoRel,these four discourse relations can combine to pro-duce meaningful transitions, such as the exampleText (2).
To examine how this affects performance,we calculate the average ratio between the numberof the four discourse relations in the permuted textand the length for the permuted text.
The ratio is0.58 for those that are correctly ranked by our sys-tem, and 0.48 for those that are incorrectly ranked,which supports our hypothesis.We also examined the learning curves for ourType+Arg+Sal model, the baseline model, and thecombined model on the data sets, as shown in Fig-ure 2(a)?2(c).
In the WSJ data, the accuracies forall three models increase rapidly as more pairs areadded to the training set.
After 2,000 pairs, the in-crease slows until 8,000 pairs, after which the curveis nearly flat.
From the curves, our model consis-tently performs better than the baseline with a signif-icant gap, and the combined model also consistentlyand significantly outperforms the other two.
Onlyabout half of the total training data is needed to reachoptimal performance for all three models.
The learn-ing curves in the Earthquakes data show that the per-formance for all models is always increasing as moretraining pairs are utilized.
The Type+Arg+Sal andcombined models start with lower accuracies thanthe baseline, but catch up with it at 1,000 and 400pairs, respectively, and consistently outperform thebaseline beyond this point.
On the other hand, thelearning curves for the Type+Arg+Sal and baselinemodels in Accidents do not show any one curve con-55606570758085900  4000  8000  12000  16000  20000Accuracy(%)Number of pairs in training dataCombinedType+Arg+SalBaseline(a) WSJ55606570758085900  400  800  1200  1600  2000Accuracy(%)Number of pairs in training dataCombinedType+Arg+SalBaseline(b) Earthquakes55606570758085900  400  800  1200  1600  2000Accuracy(%)Number of pairs in training dataCombinedType+Arg+SalBaseline(c) AccidentsFigure 2: Learning curves for the Type+Arg+Sal, thebaseline, and the combined models on the three data sets.sistently better than the other: our model outper-forms in the middle segment but underperforms inthe first and last segments.
The curve for the com-bined model shows a consistently significant gap be-tween it and the other two curves after the point at400 pairs.With the performance of the model as it is, howcan future work improve upon it?
We point out oneweakness that we plan to explore.
We use the fullType+Arg+Sal model trained on the WSJ training1004data to test Text (2) from the introduction.
As (2)has 3 sentences, permuting it gives rise to 5 permu-tations.
The model is able to correctly rank fourof these 5 pairs.
The only permutation it fails onis (S3 ?
S1 ?
S2), when the last sentence ismoved to the beginning.
A very good clue of co-herence in Text (2) is the explicit Comp relationbetween S1 and S2.
Since this clue is retained in(S3 ?
S1 ?
S2), it is difficult for the system to dis-tinguish this ordering from the source.
In contrast,as this clue is not present in the other four permuta-tions, it is easier to distinguish them as incoherent.By modeling longer range discourse relation transi-tions, we may be able to discern these two cases.While performance on identifying explicit dis-course relations in the PDTB is as high as93% (Pitler et al, 2008), identifying implicit oneshas been shown to be a difficult task with accuracyof 40% at Level-2 types (Lin et al, 2009).
As theoverall performance of the PDTB parser is still lessaccurate than we hope it to be, we expect that ourproposed model will give better performance thanit does now, when the current PDTB parser perfor-mance is improved.7 ConclusionWe have proposed a new model for discourse co-herence that leverages the observation that coherenttexts preferentially follow certain discourse struc-tures.
We posit that these structures can be cap-tured in and represented by the patterns of discourserelation transitions.
We first demonstrate that sim-ply using the sequence of discourse relation tran-sition leads to sparse features and is insufficient todistinguish coherent from incoherent text.
To ad-dress this, our method transforms the discourse re-lation transitions into a discourse role matrix.
Thematrix schematically represents term occurrences intext units and associates each occurrence with itsdiscourse roles in the text units.
In our approach,n-gram sub-sequences of transitions per term in thediscourse role matrix then constitute the more fine-grained evidence used in our model to distinguishcoherence from incoherence.When applied to distinguish a source text froma sentence-reordered permutation, our model sig-nificantly outperforms the previous state-of-the-art,the entity-based local coherence model.
While theentity-based model captures repetitive mentions ofentities, our discourse relation-based model gleansits evidence from the argumentative and discoursestructure of the text.
Our model is complementary tothe entity-based model, as it tackles the same prob-lem from a different perspective.
Experiments vali-date our claim, with a combined model outperform-ing both single models.The idea of modeling coherence with discourserelations and formulating it in a discourse role ma-trix can also be applied to other NLP tasks.
Weplan to apply our methodology to other tasks, suchas summarization, text generation and essay scoring,which also need to produce and assess discourse co-herence.ReferencesRegina Barzilay and Mirella Lapata.
2005.
Modelinglocal coherence: an entity-based approach.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL 2005), pages141?148, Morristown, NJ, USA.
Association for Com-putational Linguistics.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Computa-tional Linguistics, 34:1?34, March.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In Proceedings ofthe Human Language Technology Conference / NorthAmerican Chapter of the Association for Computa-tional Linguistics Annual Meeting 2004.Micha Elsner, Joseph Austerweil, and Eugene Charniak.2007.
A unified local and global model for dis-course coherence.
In Proceedings of the Conferenceon Human Language Technology and North AmericanChapter of the Association for Computational Linguis-tics (HLT-NAACL 2007), Rochester, New York, USA,April.Robert Elwell and Jason Baldridge.
2008.
Discourseconnective argument identification with connectivespecific rankers.
In Proceedings of the IEEE Inter-national Conference on Semantic Computing (ICSC2010), Washington, DC, USA.Barbara J. Grosz, Scott Weinstein, and Aravind K. Joshi.1995.
Centering: a framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225, June.Thorsten Joachims.
1999.
Making large-scale sup-port vector machine learning practical.
In Bernhard1005Schlkopf, Christopher J. C. Burges, and Alexander J.Smola, editors, Advances in Kernel Methods ?
SupportVector Learning, pages 169?184.
MIT Press, Cam-bridge, MA, USA.Nikiforos Karamanis.
2007.
Supplementing entity co-herence with local rhetorical relations for informationordering.
Journal of Logic, Language and Informa-tion, 16:445?464, October.Mirella Lapata and Regina Barzilay.
2005.
Automaticevaluation of text coherence: Models and representa-tions.
In Leslie Pack Kaelbling and Alessandro Saf-fiotti, editors, Proceedings of the Nineteenth Interna-tional Joint Conference on Artificial Intelligence, Ed-inburgh, Scotland, UK.Ziheng Lin, Min-Yen Kan, and Hwee Tou Ng.
2009.Recognizing implicit discourse relations in the PennDiscourse Treebank.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP 2009), Singapore.Ziheng Lin, Hwee Tou Ng, and Min-Yen Kan. 2010.
APDTB-styled end-to-end discourse parser.
TechnicalReport TRB8/10, School of Computing, National Uni-versity of Singapore, August.William C. Mann and Sandra A. Thompson.
1988.Rhetorical Structure Theory: Toward a functional the-ory of text organization.
Text, 8(3):243?281.Daniel Marcu.
1996.
Distinguishing between coher-ent and incoherent texts.
In The Proceedings of theStudent Conference on Computational Linguistics inMontreal, pages 136?143.Jane Morris and Graeme Hirst.
1991.
Lexical cohesioncomputed by thesaural relations as an indicator of thestructure of text.
Computational Linguistics, 17:21?48, March.Emily Pitler and Ani Nenkova.
2009.
Using syntaxto disambiguate explicit discourse connectives in text.In Proceedings of the ACL-IJCNLP 2009 ConferenceShort Papers, Singapore.Emily Pitler, Mridhula Raghupathy, Hena Mehta, AniNenkova, Alan Lee, and Aravind Joshi.
2008.
Easilyidentifiable discourse relations.
In Proceedings of the22nd International Conference on Computational Lin-guistics (COLING 2008) Short Papers, Manchester,UK.Emily Pitler, Annie Louis, and Ani Nenkova.
2009.
Au-tomatic sense prediction for implicit discourse rela-tions in text.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP (ACL-IJCNLP 2009), Sin-gapore.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Milt-sakaki, Livio Robaldo, Aravind Joshi, and BonnieWebber.
2008.
The Penn Discourse Treebank 2.0.In Proceedings of the 6th International Conference onLanguage Resources and Evaluation (LREC 2008).Radu Soricut and Daniel Marcu.
2006.
Discourse gener-ation using utility-trained coherence models.
In Pro-ceedings of the COLING/ACL Main Conference PosterSessions, pages 803?810, Morristown, NJ, USA.
As-sociation for Computational Linguistics.WenTing Wang, Jian Su, and Chew Lim Tan.
2010.
Ker-nel based discourse relation recognition with tempo-ral ordering information.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics (ACL 2010), Uppsala, Sweden, July.Bonnie Webber.
2004.
D-LTAG: Extending lexicalizedTAG to discourse.
Cognitive Science, 28(5):751?779.Ben Wellner and James Pustejovsky.
2007.
Automati-cally identifying the arguments of discourse connec-tives.
In Proceedings of the 2007 Joint Conferenceon Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL 2007), Prague, Czech Republic.1006
