Sequent ia l  Mode l  Se lect ion for Word Sense D isambiguat ion  *Ted Pedersen  t and  Rebecca  Brucet  and  Janyce  Wiebe:~tDepar tment  of Computer  Science and  Eng ineer ingSouthern  Method is t  Univers i ty ,  Dal las,  TX  75275: IDepar tment  of Computer  ScienceNew Mexico State  Univers i ty ,  Las Cruces,  NM 88003pedersen@seas, smu.
edu, rbruce@seas, smu.
edu, wiebe~cs, nmsu.
eduAbst rac tStatistical models of word-sense disam-biguation are often based on a small num-ber of contextual features or on a modelthat is assumed to characterize the inter-actions among a set of features.
Modelselection is presented as an alternative tothese approaches, where a sequential searchof possible models is conducted in order tofind the model that best characterizes theinteractions among features.
This paperexpands existing model selection method-ology and presents the first comparativestudy of model selection search strategiesand evaluation criteria when applied to theproblem of building probabilistic lassifiersfor word-sense disambiguation.1 I n t roduct ionIn this paper word-sense disambiguation is cast asa problem in supervised learning, where a classifieris induced from a corpus of sense-tagged text.
Sup-pose there is a training sample where each sense-tagged sentence is represented by the feature vari-ables (F1, .
.
.
,  Fn-1, S).
Selected contextual proper-ties of the sentence are represented by (F1, ?
?., Fn-1)and the sense of the ambiguous word is representedby S. Our task is to induce a classifier that willpredict the value of S given an untagged sentencerepresented by the contextual feature variables.We adopt a statistical approach whereby a prob-abilistic model is selected that describes the inter-actions among the feature variables.
Such a modelcan form the basis of a probabilistic lassifier sinceit specifies the probability of observing any and allcombinations of the values of the feature variables.Suppose our training sample has N sense-taggedsentences.
There are q possible combinations of val-ues for the n feature variables, where each such com-bination is represented by a feature vector.
Let?
This research was supported by the Office of NavalResearch under grant number N00014-95-1-0776.fi and Oi be the frequency and probability of ob-serving the i th feature vector, respectively.
Then( f l , .
.
.
,  fq) has a multinomial distribution with pa-rameters (N, 81,. .
.
,  8q).
The 0 parameters, i.e., thejoint parameters, define the joint probability distri-bution of the feature variables.
These are the pa-rameters of the fully saturated model, the model inwhich the value of each variable directly affects thevalues of all the other variables.
These parameterscan be estimated as maximum likelihood estimates(MLEs), such that the estimate of 8i, ~/, is ~.For these estimates to be reliable, each of the qpossible combinations of feature values must occurin the training sample.
This is unlikely for NLP datasamples, which are often sparse and highly skewed(c.f., e.g.
(Pedersen et al, 1996) and (Zipf, 1935)).However, if the data sample can be adequatelycharacterized by a less complex model, i.e., a modelin which there are fewer interactions between vari-ables, then more reliable parameter estimates can beobtained: In the case of decomposable models (Dar-roch et al, 1980; see below), the parameters ofa lesscomplex model are parameters of marginal distribu-tions, so the MLEs involve frequencies of combina-tions of values of only subsets of the variables in themodel.
How well a model characterizes the train-ing sample is determined by measuring the fit of themodel to the sample, i.e., how well the distributiondefined by the model matches the distribution ob-served in the training sample.A good strategy for developing probabilistic las-sifters is to perform an explicit model search to se-lect the model to use in classification.
This pa-per presents the results of a comparative study ofsearch strategies and evaluation criteria for measur-ing model fit.
We restrict he selection process to theclass of decomposable models (Darroch et al, 1980),since restricting model search to this class has manycomputational dvantages.We begin with a short description of decompos-able models (in section 2).
Search strategies (in sec-tion 3) and model evaluation (in section 4) are de-scribed next, followed by the results of an extensivedisambiguation experiment involving 12 ambiguous388words (in sections 5 and 6).
We discuss related work(in section 7) and close with recommendations forsearch strategy and evaluation criterion when select-ing models for word-sense disambiguation.2 Decomposable ModelsDecomposable models are a subset of the classof graphical models (Whittaker, 1990) which arein turn a subset of the class of log-linear models(Bishop et al, 1975).
Familiar examples of decom-posable models are Naive Bayes and n-gram models.They are characterized by the following properties(Bruce and Wiebe, 1994b):1.
In a graphical model, variables are either inter-dependent or conditionally independent of oneanother.
1 All graphical models have a graphi-cal representation such that each variable in themodel is mapped to a node in the graph, andthere is an undirected edge between each pairof nodes corresponding to interdependent vari-ables.
The sets of completely connected nodes(i.e., cliques) correspond to sets of interdepen-dent variables.
Any two nodes that are not di-rectly connected by an edge are conditionallyindependent given the values of the nodes onthe path that connects them.2.
Decomposable models are those graphical mod-els that express the joint distribution as theproduct of the marginal distributions of thevariables in the maximal cliques of the graphicalrepresentation, scaled by the marginal distribu-tions of variables common to two or more ofthese maximal sets.
Because their joint distri-butions have such closed-form expressions, theparameters can be estimated irectly from thetraining data without the need for an iterativefitting procedure (as is required, for example, toestimate the parameters of maximum entropymodels; (Berger et al, 1996)).3.
Although there are far fewer decomposablemodels than log-linear models for a given set offeature variables, it has been shown that theyhave substantially the same expressive power(Whittaker, 1990).The joint parameter estimate "d Fl'F2'F3's \]~,\]~..f3,~, is theprobability that the feature vector ( f l ,  f~., .1:3, si) willbe observed in a training sample where each ob-servation is represented by the feature variables(F1, F~, F3, S).
Suppose that the graphical represen-tation of a decomposable model is defined by the twocliques (i.e., marginals) (F1, S) and (F2, F3, S).
Thefrequencies of these marginals, f (F1 = f l ,  S = si)and f (F2 = f2,F3 = f3 ,S  = si), are sufficientstatistics in that they provide enough information1F2 and Fs are conditionally independent given S ifp( F2lFs, S) = p( F21S).to calculate maximum likelihood estimates of themodel parameters.
MLEs of the model parametersare simply the marginal frequencies normalized bythe sample size N. The joint parameter estimate isformulated as a normalized product:~,,F=,F,,S /(F,=/,,S=,,) I(F~=/=,F,=I,,S=,,)x , f  =,f 3,s,  "= N X Nl(S=,,)N(1)Rather than having to observe the complete fea-ture vector (ft, f.~, f3, si) in the training sample toestimate the joint parameter, it is only necessary toobserve the marginals (ft, si) and (f2, f3, si).3 Model Search StrategiesThe search strategies presented in this paper arebackward sequential search (BSS) and forward se-quential search (FSS).
Sequential searches evaluatemodels of increasing (FSS) or decreasing (BSS) lev-els of complexity, where complexity is defined by thenumber of interactions among the feature variables(i.e., the number of edges in the graphical represen-tation of the model).A backward sequential search (BSS) begins bydesignating the saturated model as the currentmodel.
A saturated model has complexity leveli = n(n-t) where n is the number of feature vari- 2ables.
At each stage in BSS we generate the set ofdecomposable models of complexity level i - 1 thatcan be created by removing an edge from the cur-rent model of complexity level i.
Each member ofthis set is a hypothesized model and is judged bythe evaluation criterion to  determine which modelresults in the least degradation in fit from the cur-rent model--that model becomes the current modeland the search continues.
The search stops when ei-ther (1) every hypothesized model results in an un-acceptably high degradation i fit or (2) the currentmodel has a complexity level of zero.A forward sequential search (FSS) begins by des-ignating the model of independence as the currentmodel.
The model of independence has complexitylevel i = 0 since there are no interactions among thefeature variables.
At each stage in FSS we generatethe set of decomposable models of complexity leveli + 1 that can be created by adding an edge to thecurrent model of complexity level i.
Each member ofthis set is a hypothesized model and is judged by theevaluation criterion to determine which model re-sults in the greatest improvement in fit from the cur-rent model--that model becomes the current modeland the search continues.
The search stops wheneither (1) every hypothesized model results in anunacceptably small increase in fit or (2) the currentmodel is saturated.For sparse samples FSS is a natural choice sinceearly in the search the models are of low complexity.389The number of model parameters i small and theyhave more reliable estimated values.
On the otherhand, BSS begins with a saturated model whose pa-rameter estimates are known to be unreliable.During both BSS and FSS, model selection alsoperforms feature selection.
If a model is selectedwhere there is no edge connecting a feature variableto the classification variable then that feature is notrelevant o the classification being performed.4 Mode l  Eva luat ion  Cr i te r iaEvaluation criteria fall into two broad classes, signifi-cance tests and information criteria.
This paper con-siders two significance tests, the exact conditionaltest (Kreiner, 1987) and the Log-likelihood ratiostatistic G 2 (Bishop et al, 1975), and two informa-tion criteria, Akaike's Information Criterion (AIC)(Akaike, 1974) and the Bayesian Information Crite-rion (BIC) (Schwarz, 1978).4.1 S ign i f i cance  tes tsThe Log-likelihood ratio statistic G 2 is defined as:qei = F_, .
f ,  ?
log  (2)where fi and ei are the observed and expected countsof the i th feature vector, respectively.
The observedcount f i  is simply the frequency in the training sam-ple.
The expected count ei is calculated from thefrequencies in the training data assuming that thehypothesized model, i.e., the model generated in thesearch, adequately fits the sample.
The smaller thevalue of G 2 the better the fit of the hypothesizedmodel.The distribution of G 2 is asymptotically approx-imated by the X 2 distribution (G 2 ,,~ X 2) with ad-justed degrees of freedom (dof) equal to the numberof model parameters that have non-zero estimatesgiven the training sample.
The significance of amodel is equal to the probability of observing itsreference G ~ in the X 2 distribution with appropriatedof.
A hypothesized model is accepted if the signif-icance (i.e., probability) of its reference G ~ value isgreater than, in the case of FSS, or less than, in thecase of BSS, some pre-determined cutoff, a.An alternative to using a X 2 approximation is todefine the exact conditional distribution of G 2.
Theexact conditional distribution of G 2 is the distribu:tion of G ~ values that would be observed for com-parable data samples randomly generated from themodel being tested.
The significance of G 2 based onthe exact conditional distribution does not rely on anasymptotic approximation and is accurate for sparseand skewed data samples (Pedersen et al, 1996)4.2 In fo rmat ion  cr i ter iaThe family of model evaluation criteria known asinformation criteria have the following expression:IC,~ = G 2 - ~ x do f  (3)where G ~ and dof  are defined above.
Members ofthis family are distinguished by their different valuesof ~.
AIC corresponds to g = 2.
BIC correspondsto ~ = log(N),  where N is the sample size.The various information criteria are an alterna-tive to using a pre-defined significance level (a) tojudge the acceptability of a model.
AIC and BIC re-ward good model fit and penalize models with largenumbers of parameters.
The parameter penalty isexpressed as ~ x do f ,  where the size of the penaltyis the adjusted degrees of freedom, and the weightof the penalty is controlled by x.During BSS the hypothesized model with thelargest negative IC,~ value is selected as the cur-rent model of complexity level i - 1, while duringFSS the hypothesized model with the largest pos-itive IC,~ value is selected as the current model ofcomplexity level i + 1.
The search stops when theIC,~ values for all hypothesized models are greaterthan zero in the case of BSS, or less than zero in thecase of FSS.5 Exper imenta l  DataThe sense-tagged text and feature set used inthese experiments are the same as in (Bruce et al,1996).
The text consists of every sentence from theACL/DCI Wall Street Journal corpus that containsany of the nouns interest, bill, concern, and drug,any of the verbs close, help, agree, and include, orany of the adjectives chief, public, last, and common.The extracted sentences have been hand-taggedwith senses defined in the Longman Dictionary ofContemporary English (LDOCE).
There are be-tween 800 and 3,000 sense-tagged sentences for eachof the 12 words.
This data was randomly dividedinto training and test samples at a 10:1 ratio.A sentence with an ambiguous word is representedby a feature set with three types of contextual fea-ture variables: 2 (1) The morphological feature (E)indicates if an ambiguous noun is plural or not.
Forverbs it indicates the tense of the verb.
This featureis not used for adjectives.
(2) The POS featureshave one of 25 possible POS tags, derived from thefirst letter of the tags in the ACL/DCI WSJ cor-pus.
There are four POS feature variables repre-senting the POS of the two words immediately pre-ceding (L1, L2) and following (R1, R2) the ambigu-ous word.
(3) The three binary collocation-specificfeatures (C1, C2, Ca) indicate i fa particular word oc-curs in a sentence with an ambiguous word.2An alternative feature set for this data is utilizedwith an exemplar-based learning algorithm in (Ng andLee, 1996).390The sparse nature of our data can be illustratedby interest.
There are 6 possible values for the sensevariable.
Combined with the other feature variablesthis results in 37,500,000 possible feature vectors (orjoint parameters).
However, we have a training sam-ple of only 2,100 instances.6 Exper imenta l  Resu l tsIn total, eight different decomposable models wereselected via a model search for each of the 12 words.Each of the eight models is due to a different com-bination of search strategy and evaluation criterion.Two additional classifiers were evaluated to serve asbenchmarks.
The default classifier assigns every in-stance of an ambiguous word with its most frequentsense in the training sample.
The Naive Bayes clas-sifier uses a model that assumes that each contex-tual feature variable is conditionally independent ofall other contextual variables given the value of thesense variable.6.1 Accuracy  compar i sonThe accuracy 3 of each of these classifiers for eachof the 12 words is shown in Figure 1.
The highestaccuracy for each word is in bold type while any ac-curacies less than the default classifier are italicized.The complexity of the model selected is shown inparenthesis.
For convenience, we refer to model se-lection using, for example, a search strategy of FSSand the evaluation criterion AIC as FSS AIC.Overall AIC selects the most accurate models dur-ing both BSS and FSS.
BSS AIC finds the most ac-curate model for 6 of 12 words while FSS AIC findsthe most accurate for 4 of 12 words.
BSS BIC andthe Naive Bayes find the most accurate model for 3of 12 words.
Each of the other combinations findsthe most most accurate model for 2 of 12 words ex-cept for FSS exact conditional which never finds themost accurate model.Neither AIC nor BIC ever selects a model thatresults in accuracy less than the default classifier.However, FSS exact conditional has accuracy lessthan the default for 6 of 12 words and BSS exactconditional has accuracy less than the default for 3of 12 words.
BSS G 2 -~X 2 and FSS G 2,,~ X 2 haveless than default accuracy for 2 of 12 and 1 of 12words, respectively.The accuracy of the significance tests vary greatlydepending on the choice of c~.
Of the various (~ valuesthat were tested, .01, .05, .001, and .0001, the valueof .0001 was found to produce the most accuratemodels.
Other values of c~ will certainly led to otherresults.
The information criteria do not require thesetting of any such cut-off values.A low complexity model that results in high accu-racy disambiguation is the ultimate goal.
Figure 13The percentage of ambiguous words in a held outtest sample that are disambiguated correctly.shows that BIC and G 2 ,-~ X 2 select lower complexitymodels than either AIC or the exact conditional test.However, both appear to sacrifice accuracy whencompared to AIC.
BIC assesses a greater parame-ter penalty (~ = log(N)) than does AIC (~ = 2),causing BSS BIC to remove more interactions thanBSS AIC.
Likewise, FSS BIC adds fewer interactionsthan FSS AIC.
In both cases BIC selects modelswhose complexity is too low and adversely affectsaccuracy when compared to AIC.The Naive Bayes classifier achieves a high levelof accuracy using a model of low complexity.
Infact, while the Naive Bayes classifier is most accu-rate for only 3 of the 12 words, the average accu-racy of the Naive Bayes classifiers for all 12 wordsis higher than the average classification accuracy re-sulting from any combination of the search strategiesand evaluation criteria.
The average complexity ofthe Naive Bayes models is also lower than the av-erage complexity of the models resulting from anycombination of the search strategies and evaluationcriteria except BSS BIC and FSS BIC.6.2 Search  s t ra tegy  and  accuracyAn evaluation criterion that finds models of simi-lar accuracy using either BSS or FSS is to be pre-ferred over one that does not.
Overall the infor-mation criteria are not greatly affected by a changein the search strategy, as illustrated in Figure 3.Each point on this plot represents the accuracy ofthe models selected for a word by the same evalua-tion criterion using BSS and FSS.
If this point fallsclose to the line BSS = FSS  then there is littleor no difference between the accuracy of the modelsselected uring FSS and BSS.AIC exhibits only minor deviation from BSS =FSS.
This is also illustrated by the fact that theaverage accuracy between BSS AIC and FSS AIConly differs by .0013.
The significance tests, espe-cially the exact conditional, are more affected bythe search strategy.
It is clear that BSS exact condi-tional is much more accurate than FSS exact condi-tional.
FSS G 2 -~ X 2 is slightly more accurate thanBSS G 2 ,-, X ~.6.3 Feature  select ion:  in teres tFigure 2 shows the models selected by the variouscombinations of search strategy and evaluation cri-terion for interest.During BSS, AIC removed feature L2 from themodel, BIC removed L1,L2, R1 and R2, G 2 "-, X 2removed no features, and the exact conditional testremoved C2.
During FSS, AIC never added R2, BICnever added C1, C3, L1, L~ and R~, and G ~ ~, X 2 andthe exact conditional test added all the features.G 2 ~ X 2 is the most consistent of the evaluationcriteria in feature selection.
During both BSS andFSS it found that all the features were relevant oclassification.391Defaultinterestlastpublicagree .7660bill .7090chief .8750close .6815common .8696concern .6510drug .6721help .7266include .9325.5205.9387average.5056Naive Search G 2 ,,~ X 2 exact AICBayes ~ = .0001 a = .0001.9362 (8) BSS .8936 (8) .9149 (10) .9220 (15)FSS .9291 (12) .9007 (15) .9362 (13).8657 (8) BSS .6567 (22) .6194 (25) .8507 (26)FSS .7985 (20) .6855 (28) .8582 (20).9643 (7) BSS .9464 (6) .9196 (17) .9643 (14)FSS .9464 (6) .9196 (18) .9643 (14).8344 (8) BSS .7580 (12) .7516 (13) .8408 (13)FSS .7898 (13) .7006 (19) .8408 (10).9130 (7) BSS .9217 (4) .8696 (10) .8957 (7)FSS .9217 (4) .7391 (16) .8957 (7).8725 (8) BSS .8255 (5) .7651 (15) .8389 (16)FSS .8255 (17) .7047 (24) .8255 (13).8279 (8) BSS .8115 (10) .8443 (7) .8443 (14)FSS .8115 (10) .5164 (19) .8115 (12).7698 (8) BSS .7410 (7) .7698 (6) .7914 (6)FSS .7554 (3) .7770 (9) .7914 (4).9448 (8) BSS .9571 (6) .9571 (3) .9387 (16)FSS .9571 (6) .7423 (2g) .9448 (9).7336 (8) BSS .6885 (24) .4959 (24) .7418 (21)FSS .7172 (22) ,4590 (3g) .7336 (15).9264 (7) BSS .9080 (8) .8865 (9) .9417 (14)FSS .8804 (15) .8466 (18) .9417 (14).5843 (7) BSS .5393 (7) .5393 (9) .5169 (8)FSS .5281 (6) .5506 (11) .5281 (6).8477 (8) I BSS I .8039(10) .7778 (12) .8406 (14)I FSS I .8217 (11) .7119 (19) .8393 (11).7373Figure h Accuracy comparisonBIC.9433 (9).9433 (7).88o6 (7).8433 (11).9554 (6).9643 (7)758o (3).7580 (3).8783 (2).8783 (2).7181 (6).8389 (9).7787 (9).7787 (9).7554 (4).7554 (4).9387 (8).9325 (9).6311 (6).6926 (4).9417 (9).9387 (2).5506 (3).55o6 (3).8108 (6).8229 (6)Criterion SearchG 2 ,~ X ~ BSSFSSExact BSSFSSAIC BSSFSSBIC BSSFSSNaive Bayes noneModel( C1E L1L2S)( C1C2C3L1L2S)( C1C2C3R1S)I C2 E L1L2S)( C1 R1R2S)(C2C3L1L2S)(C3R1R2S)C1 ELI L2S)( C1 L~ L2R1R2S)( C3L1L2R~ R2S)I CI E L1L2 R1R2S)( C3L1L2 R1R2S)( C2 ELI L2 R1R2S)C1C2C3E L1S)( C1C3RI S)( CI C3R2S)I E L, L2S)( C2 E L2S)( C1 R1S)( C3L, S)( C3R1S)C2ES)(C~C3S)I C ES)(R S) C1S)( C~S)( C3S)( ES)( L1 S)( L2S)( RI S)( R2S)Figure 2: Models selected: interest392FSS0.90.80.70.60.5,exact I ~ i i ' ~-t"1G 2 ~ X 2 +MooBIC x ~?
lBSS=FSS .
.
.
.
x _~ ~ |+ -$~eo"I I0.5 0.60.4 ' ' '0.4 0.7 0.8 0.9 1BSSFigure 3: Effect of Search StrategyAIC found seven features to be relevant in bothBSS and FSS.
When using AIC, the only differencein the feature set selected during FSS as comparedto that selected during BSS is the part of speechfeature that is found to be irrelevant: during BSS L2is removed and during FSS R2 is never added.
Allother criteria exhibit more variation between FSSand BSS in feature set selection.6.4 Mode l  se lect ion:  in teres tHere we consider the results of each stage of thesequential model selection for interest.
Figures 4through 7 show the accuracy and recall 4 for thebest fitting model at each level of complexity in thesearch.
The rightmost point on each plot for eachevaluation criterion is the measure associated withthe model ultimately selected.These plots illustrate that BSS BIC selects mod-els of too low complexity.
In Figure 4 BSS BIC has"gone past" much more accurate models than theone it selected.
We observe the related problem forFSS BIC.
In Figure 6 FSS BIC adds too few in-teractions and does not select as accurate a modelas FSS AIC.
The exact conditional test suffers fromthe reverse problem of BIC.
BSS exact conditionalremoves only a few interactions while FSS exact con-ditional adds many interactions, and in both casesthe resulting models have poor accuracy.The difference between BSS and FSS is clearly il-4The percentage ofambiguous words in a held out testsample that are disambiguated, correctly or not.
A wordis not disambiguated if the model parameters needed toassign a sense tag cannot be estimated from the trainingsample.lustrated by these plots.
AIC and BIC eliminate in-teractions that have high dof's (and thus have largenumbers of parameters) much earlier in BSS thanthe significance tests.
This rapid reduction in thenumber of parameters results in a rapid increasesin accuracy (Figure 4) and recall for AIC and BIC(Figure 5) relative to the significance tests as theyproduce models with smaller numbers of parametersthat can be estimated more reliably.However, during the early stages of FSS the num-ber of parameters in the models is very small and thedifferences between the information criteria and thesignificance tests are minimized.
The major differ-ence among the criteria in Figures 6 and 7 is that theexact conditional test adds many more interactions.7 Re la ted  WorkStatistical analysis of NLP data has often been lim-ited to the application of standard models, suchas n-gram (Markov chain) models and the NaiveBayes model.
While n-grams perform well in par t -of-speech tagging and speech processing, they re-quire a fixed interdependency structure that is inap-propriate for the broad class of contextual featuresused in word-sense disambiguation.
However, theNaive Bayes classifier has been found to performwell for word-sense disambiguation both here andin a variety of other works (e.g., (Bruce and Wiebe,1994a), (Gale et al, 1992), (Leacock et al, 1993),and (Mooney, 1996)).In order to utilize models with more complicatedinteractions among feature variables, (Bruce andWiebe, 1994b) introduce the use of sequential modelselection and decomposable models for word-sensedisambiguation.
~Alternative probabilistic approaches have involvedusing a single contextual feature to perform disam-biguation (e.g., (Brown et al, 1991), (Dagan et al,1991), and (Yarowsky, 1993) present echniques foridentifying the optimal feature to use in disambigua-tion).
Maximum Entropy models have been used toexpress the interactions among multiple feature vari-ables (e.g., (Berger et al, 1996)), but within thisframework no systematic study of interactions hasbeen proposed.
Decision tree induction has beenapplied to word-sense disambiguation (e.g.
(Black,1988) and (Mooney, 1996)) but, while it is a type ofmodel selection, the models are not parametric.SThey recommended a model selection procedure us-ing BSS and the exact conditional test in combinationwith a test for model predictive power.
In their proce-dure, the exact conditional test was used to guide thegeneration of new models and the test of model predic-tive power was used to select he final model from amongthose generated uring the search.393%I I I I0.9 -0 .8 -0.70.60.5 .
A AIC ~ " BIC 00.44 Exact ~ = .0001 .A.
?G ~ ~,, X 2 a = .0001 "~'"0.3 , , i i i i35 30 25 20 15 10 5# of interactions in modelFigure 4: BSS accuracy: interest%t I I 1 I I I1 -0.90.80 .7  ~ ~ .
Z ~  ?%*"A.%0.5"~ AIC x ~.0 4 \[- Exact  a = .0001 .A.
?0 5 10 15 20 25 30# of interactions in modelFigure 6: FSS accuracy: interestI35%10.90.80.70.60.5 j0.40.3f I .olo I I I It I35 30 25 20 15 10 5of interactions in modelFigure 5: BSS recall: interest%0.90.80.70.60.50.40.3I I I IAI IA%-AICBIC <>Exact a -- .0001 .A.
?G 2 ,,~ X 2 a = .0001 .e .
-I I I i I i0 5 10 15 20 25 30# of interactions in modelFigure 7: FSS recall: interest353948 Conc lus ionSequential model selection is a viable means ofchoosing a probabilistic model to perform word-sense disambiguation.
We recommend AIC as theevaluation criterion during model selection due tothe following:1.
It is difficult to set an appropriate cutoff value(a) for a significance test.2.
The information criteria AIC and BIC are morerobust to changes in search strategy.3.
BIC removes too many interactions and resultsin models of too low complexity.The choice of search strategy when using AIC isless critical than when using significance tests.
How-ever, we recommend FSS for sparse data (NLP datais typically sparse) since it reduces the impact of veryhigh degrees of freedom and the resultant unreliableparameter estimates on model selection.The Naive Bayes classifier is based on a low com-plexity model that is shown to lead to high accuracy.If feature selection is not in doubt (i.e., it is fairlycertain that all of the features are somehow relevantto classification) then this is a reasonable approach.However, if some features are of questionable valuethe Naive Bayes model will continue to utilize themwhile sequential model selection will disregard them.All of the search strategies and evaluation crite-ria discussed are implemented in the public domainprogram CoCo (Badsberg, 1995).Re ferencesH.
Akaike.
1974.
A new look at the statistical modelidentification.
IEEE Transactions on AutomaticControl, AC-19(6):716-723.J.
Badsberg.
1995.
An Environment for GraphicalModels.
Ph.D. thesis, Aalborg University.A.
Berger, S. Della Pietra, and V. Della Pietra.1996.
A maximum entropy approach to naturallanguage processing.
Computational Linguistics,22(1):39-71.Y.
Bishop, S. Fienberg, and P. Holland.
1975.Discrete Multivariate Analysis.
The MIT Press,Cambridge, MA.E.
Black.
1988.
An experiment in computationaldiscrimination of English word senses.
IBM Jour-nal of Research and Development, 32(2):185-194.P.
Brown, S. Della Pietra, and R. Mercer.
1991.Word sense disambiguation using statistical meth-ods.
In Proceedings of the 29th Annual Meetingof the Association for Computational Linguistics,pages 264-304.R.
Bruce and J. Wiebe.
1994a.
A new approachto word sense disambiguation.
In Proceedings ofthe ARPA Workshop on Human Language Tech-nology, pages 244-249.R.
Bruce and J. Wiebe.
1994b.
Word-sense dis-ambiguation using decomposable models.
In Pro-ceedings of the 32nd Annual Meeting of the Asso-ciation for Computational Linguistics, pages 139-146.R.
Bruce, J. Wiebe, and T. Pedersen.
1996.
Themeasure of a model.
In Proceedings of the Confer-ence on Empirical Methods in Natural LanguageProcessing, pages 101-112.I.
Dagan, A. Itai, and U. Schwall.
1991.
Two lan-guages are more informative than one.
In Proceed-ings of the 29th Annual Meeting of the Associationfor Computational Linguistics, pages 130-137.J.
Darroch, S. Lauritzen, and T. Speed.
1980.Markov fields and log-linear interaction modelsfor contingency tables.
The Annals of Statistics,8(3):522-539.W.
Gale, K. Church, and D. Yarowsky.
1992.
Amethod for disambiguating word senses in a largecorpus.
Computers and the Humanities, 26:415-439.S.
Kreiner.
1987.
Analysis of multidimensional con-tingency tables by exact conditional tests: Tech-niques and strategies.
Scandinavian Journal ofStatistics, 14:97-112.C.
Leacock, G. Towell, and E. Voorhees.
1993.Corpus-based statistical sense resolution.
In Pro-ceedings of the ARPA Workshop on Human Lan-guage Technology.R.
Mooney.
1996.
Comparative experiments on dis-ambiguating word senses: An illustration of therole of bias in machine learning.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing.H.T.
Ng and H.B.
Lee.
1996.
Integrating multi-ple knowledge sources to disambiguate word sense:An exemplar-based approach.
In Proceedings ofthe 34th Annual Meeting of the Society for Com-putational Linguistics, pages 40-47.T.
Pedersen, M. Kayaalp, and R. Bruce.
1996.
Sig-nificant lexical relationships.
In Proceedings ofthe 13th National Conference on Artificial Intelli-gence, pages 455-460.G.
Schwarz.
1978.
Estimating the dimension of amodel.
The Annals of Statistics, 6(2):461-464.J.
Whittaker.
1990.
Graphical Models in AppliedMultivariate Statistics.
John Wiley, New York.D.
Yarowsky.
1993.
One sense per collocation.
InProceedings of the ARPA Workshop on HumanLanguage Technology, pages 266-271.G.
Zipf.
1935.
The Psycho-Biology of Language.Houghton Mifflin, Boston, MA.395
