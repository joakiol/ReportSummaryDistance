Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1392?1402,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsCoSimRank: A Flexible & Efficient Graph-Theoretic Similarity MeasureSascha Rothe and Hinrich Sch?utzeCenter for Information & Language ProcessingUniversity of Munichsascha@cis.lmu.deAbstractWe present CoSimRank, a graph-theoreticsimilarity measure that is efficient becauseit can compute a single node similaritywithout having to compute the similaritiesof the entire graph.
We present equivalentformalizations that show CoSimRank?sclose relationship to Personalized Page-Rank and SimRank and also show howwe can take advantage of fast matrix mul-tiplication algorithms to compute CoSim-Rank.
Another advantage of CoSimRankis that it can be flexibly extended from ba-sic node-node similarity to several othergraph-theoretic similarity measures.
In anexperimental evaluation on the tasks ofsynonym extraction and bilingual lexiconextraction, CoSimRank is faster or moreaccurate than previous approaches.1 IntroductionGraph-theoretic algorithms have been successfullyapplied to many problems in NLP (Mihalcea andRadev, 2011).
These algorithms are often based onPageRank (Brin and Page, 1998) and other central-ity measures (e.g., (Erkan and Radev, 2004)).
Analternative for tasks involving similarity is Sim-Rank (Jeh and Widom, 2002).
SimRank is basedon the simple intuition that nodes in a graph shouldbe considered as similar to the extent that theirneighbors are similar.
Unfortunately, SimRankhas time complexity O(n3) (where n is the num-ber of nodes in the graph) and therefore does notscale to the large graphs that are typical of NLP.This paper introduces CoSimRank,1a newgraph-theoretic algorithm for computing nodesimilarity that combines features of SimRank andPageRank.
Our key observation is that to computethe similarity of two nodes, we need not consider1Code available at code.google.com/p/cisternall other nodes in the graph as SimRank does; in-stead, CoSimRank starts random walks from thetwo nodes and computes their similarity at eachtime step.
This offers large savings in computa-tion time if we only need the similarities of a smallsubset of all n2node similarities.These two cases ?
computing a few similari-ties and computing many similarities ?
correspondto two different representations we can computeCoSimRank on: a vector representation, which isfast for only a few similarities, and a matrix repre-sentation, which can take advantage of fast matrixmultiplication algorithms.CoSimRank can be used to compute many vari-ations of basic node similarity ?
including similar-ity for graphs with weighted and typed edges andsimilarity for sets of nodes.
Thus, CoSimRank hasthe added advantage of being a flexible tool for dif-ferent types of applications.The extension of CoSimRank to similarityacross graphs is important for the application ofbilingual lexicon extraction: given a set of corre-spondences between nodes in two graphsA andB(corresponding to two different languages), a pairof nodes (a ?
A, b ?
B) is a good candidate for atranslation pair if their node similarity is high.
Inan experimental evaluation, we show that CoSim-Rank is more efficient and more accurate than bothSimRank and PageRank-based algorithms.This paper is structured as follows.
Section 2discusses related work.
Section 3 introducesCoSimRank.
In Section 4, we compare CoSim-Rank and SimRank.
By providing some usefulextensions, we demonstrate the great flexibility ofCoSimRank (Section 5).
We perform an exper-imental evaluation of CoSimRank in Section 6.Section 7 summarizes the paper.2 Related WorkOur work is unsupervised.
We therefore do notreview graph-based methods that make extensive1392use of supervised learning (e.g., de Melo andWeikum (2012)).Since the original version of SimRank (Jeh andWidom, 2002) has complexity O(n4), many ex-tensions have been proposed to speed up its calcu-lation.
A Monte Carlo algorithm, which is scalableto the whole web, was suggested by Fogaras andR?acz (2005).
However, in an evaluation of this al-gorithm we found that it does not give competitiveresults (see Section 6).
A matrix representation ofSimRank called SimFusion (Xi et al, 2005) im-proves the computational complexity from O(n4)to O(n3).
Lizorkin et al (2010) also reduce com-plexity to O(n3) by selecting essential node pairsand using partial sums.
They also give a usefuloverview of SimRank, SimFusion and the MonteCarlo methods of Fogaras and R?acz (2005).
Anon-iterative computation for SimRank was intro-duced by Li et al (2010).
This is especially usefulfor dynamic graphs.
However, all of these meth-ods have to run SimRank on the entire graph andare not efficient enough for very large graphs.
Weare interested in applications that only need a frac-tion of all O(n2) pairwise similarities.
The algo-rithm we propose below is an order of magnitudefaster in such applications because it is based on alocal formulation of the similarity measure.2Apart from SimRank, many other similaritymeasures have been proposed.
Leicht et al (2006)introduce a similarity measure that is also based onthe idea that nodes are similar when their neigh-bors are, but that is designed for bipartite graphs.However, most graphs in NLP are not bipartite andJeh and Widom (2002) also proposed a SimRankvariant for bipartite graphs.Another important similarity measure is cosinesimilarity of Personalized PageRank (PPR) vec-tors.
We will refer to this measure as PPR+cos.Hughes and Ramage (2007) find that PPR+coshas high correlation with human similarity judg-ments on WordNet-based graphs.
Agirre et al(2009) use PPR+cos for WordNet and for cross-lingual studies.
Like CoSimRank, PPR+cos isefficient when computing single node pair simi-larities; we therefore use it as one of our base-lines below.
This method is also used by Changet al (2013) for semantic relatedness.
They alsoexperimented with Euclidean distance and KL-2A reviewer suggests that CoSimRank is an efficient ver-sion of SimRank in a way analogous to SALSA?s (Lempeland Moran, 2000) relationship to HITS (Kleinberg, 1999) inthat different aspects of similarity are decoupled.divergence.
Interestingly, a simpler method per-formed best when comparing with human simi-larity judgments.
In this method only the entriescorresponding to the compared nodes are used fora similarity score.
Rao et al (2008) comparedPPR+cos to other graph based similarity mea-sures like shortest-path and bounded-length ran-dom walks.
PPR+cos performed best except fora new similarity measure based on commute time.We do not compare against this new measure as ituses the graph Laplacian and so cannot be com-puted for a single node pair.One reason CoSimRank is efficient is that weneed only compute a few iterations of the randomwalk.
This is often true of this type of algorithm;cf.
(Sch?utze and Walsh, 2008).LexRank (Erkan and Radev, 2004) is similar toPPR+cos in that it combines PageRank and cosine;it initializes the sentence similarity matrix of adocument using cosine and then applies PageRankto compute lexical centrality.
Despite this superfi-cial relatedness, applications like lexicon extrac-tion that look for similar entities and applicationsthat look for central entities are quite different.In addition to faster versions of SimRank, therehas also been work on extensions of SimRank.Dorow et al (2009) and Laws et al (2010) ex-tend SimRank to edge weights, edge labels andmultiple graphs.
We use their Multi-Edge Extrac-tion (MEE) algorithm as one of our baselines be-low.
A similar graph of dependency structures wasbuilt by Minkov and Cohen (2008).
They applieddifferent similarity measures, e.g., cosine of de-pendency vectors or a new algorithm called path-constrained graph walk, on synonym extraction(Minkov and Cohen, 2012).
We compare CoSim-Rank with their results in our experiments (seeSection 6).Some other applications of SimRank or othergraph based similarity measures in NLP includework on document similarity (Li et al, 2009),the transfer of sentiment information between lan-guages (Scheible et al, 2010) and named entitydisambiguation (Han and Zhao, 2010).
Hoang andKan (2010) use SimRank for related work sum-marization.
Muthukrishnan et al (2010) combinelink based similarity and content based similarityfor document clustering and classification.These approaches use at least one of cosine sim-ilarity, PageRank and SimRank.
CoSimRank caneither be interpreted as an efficient version of Sim-1393Rank or as a version of Personalized PageRankfor similarity measurement.
The novelty is thatwe compute similarity for vectors that are inducedusing a new algorithm, so that the similarity mea-surement is much more efficient when an applica-tion only needs a fraction of all O(n2) pairwisesimilarities.3 CoSimRankWe first first give an intuitive introduction ofCoSimRank as a Personalized PageRank (PPR)derivative.
Later on, we will give a matrix formu-lation to compare CoSimRank with SimRank.3.1 Personalized PageRankHaveliwala (2002) introduced Personalized Page-Rank ?
or topic-sensitive PageRank ?
based on theidea that the uniform damping vector p(0)can bereplaced by a personalized vector, which dependson node i.
We usually set p(0)(i) = ei, with eibe-ing a vector of the standard basis, i.e., the ithentryis 1 and all other entries are 0.
The PPR vector ofnode i is given by:p(k)(i) = dAp(k?1)(i) + (1?
d)p(0)(i) (1)where A is the stochastic matrix of the Markovchain, i.e., the row normalized adjacency matrix.The damping factor d ?
(0, 1) ensures that thecomputation converges.
The PPR vector after kiterations is given by p(k).To visualize this formula, one can imagine arandom surfer starting at node i and following oneof the links with probability d or jumping back tothe starting node i with probability (1?
d).
Entryi of the converged PPR vector represents the prob-ability that the random surfer is on node i after anunlimited number of steps.To simulate the behavior of SimRank we willsimplify this equation and set the damping factord = 1.
We will re-add a damping factor later inthe calculation.p(k)= Ap(k?1)(2)Note that the personalization vector p(0)was elim-inated, but is still present as the starting vector ofthe iteration.3.2 Similarity of vectorsLet p(i) be the PPR vector of node i.
The cosineof two vectors u and v is computed by dividingFigure 1: Graph motivating CoSimRank algo-rithm.
Whereas PPR gives relatively high similar-ity to the pair (law,suit), CoSimRank assigns thepair similarity 0.the inner product ?u, v?
by the lengths of the vec-tors.
The cosine of two PPR vectors can be used asa similarity measure for the corresponding nodes(Hughes and Ramage, 2007; Agirre et al, 2009):s(i, j) =?p(i), p(j)?|p(i)||p(j)|(3)This measure s(i, j) looks at the probability thata random walker is on a certain edge after an un-limited number of steps.
This is potentially prob-lematic as the example in Figure 1 shows.
ThePPR vectors of suit and dress will have someweight on tailor, which is good.
However, thePPR vector of law will also have a non-zero weightfor tailor.
So law and dress are similar because ofthe node tailor.
This is undesirable.We can prevent this type of spurious similarityby taking into account the path the random surfertook to get to a particular node.
We formalize thisby defining CoSimRank s(i, j) as follows:s(i, j) =?
?k=0ck?p(k)(i), p(k)(j)?
(4)where p(k)(i) is the PPR vector of node i fromEq.
2 after k iterations.
We compare the PPR vec-tors at each time step k. The sum of all similaritiesis the value of CoSimRank, i.e., the final similar-ity.
We add a damping factor c, so that early meet-ings are more valuable than later meetings.To compute the similarity of two vectors u andv we use the inner product ?
?, ??
in Eq.
4 for tworeasons:1.
This is similar to cosine similarity except thatthe 1-norm is used instead of the 2-norm.Since our vectors are probability vectors, wehave?p(i), p(j)?|p(i)||p(j)|= ?p(i), p(j)?1394for the 1-norm.32.
Without expensive normalization, we cangive a simple matrix formalization of CoSim-Rank and compute it efficiently using fastmatrix multiplication algorithms.Later on, the following iterative computation ofCoSimRank will prove useful:s(k)(i, j) = ck?p(k)(i), p(k)(j)?+ s(k?1)(i, j)(5)3.3 Matrix formulationThe matrix formulation of CoSimRank is:S(0)= ES(1)= cAAT+ S(0)S(2)= c2A2(AT)2+ S(1).
.
.S(k)= ckAk(AT)k+ S(k?1)(6)We will see in Section 5 that this formulation is thebasis for a very efficient version of CoSimRank.3.4 Convergence propertiesAs the PPR vectors have only positive values, wecan easily see in Eq.
4 that the CoSimRank ofone node pair is monotonically non-decreasing.For the dot product of two vectors, the Cauchy-Schwarz inequality gives the upper bound:?u, v?
?
?u?
?v?where ?x?
is the norm of x.
From Eq.
2 we get??p(k)?
?1= 1, where ??
?1is the 1-norm.
We alsoknow from elementary functional analysis that the1-norm is the biggest of all p-norms and so onehas??p(k)???
1.
It follows that CoSimRank growsmore slowly than a geometric series and convergesif |c| < 1:s(i, j) ???k=0ck=11?
cIf an upper bound of 1 is desired for s(i, j) (in-stead of 1/(1?
c)), then we can use s?:s?
(i, j) = (1?
c)s(i, j)3This type of similarity measure has also been used andinvestigated by?O S?eaghdha and Copestake (2008), Cha(2007), Jebara et al (2004) (probability product kernel) and(Jaakkola et al, 1999) (Fisher kernel) among others.4 Comparison to SimRankThe original SimRank equation can be written asfollows (Jeh and Widom, 2002):r(i, j) =????
?1, if i = jc|N(i)||N(j)|?k?N(i)l?N(j)r(k, l), elsewhere N(i) denotes the nodes connected to i.SimRank is computed iteratively.
With A be-ing the normalized adjacency matrix we can writeSimRank in matrix formulation:R(0)= ER(k)= max{cAR(k?1)AT, R(0)} (7)where the maximum of two matrices refers to theelement-wise maximum.
We will now prove by in-duction that the matrix formulation of CoSimRank(Eq.
6) is equivalent to:S?
(k)= cAS?
(k?1)AT+ S(0)(8)and thus very similar to SimRank (Eq.
7).The base case S(1)= S?
(1)is trivial.
Inductivestep:S?
(k)(8)= cAS?
(k?1)AT+ S(0)= cA(ck?1Ak?1(AT)k?1+ S(k?2))AT+ S(0)= ckAk(AT)k+ cAS(k?2)AT+ S(0)= ckAk(AT)k+ S(k?1)(6)= S(k)Comparing Eqs.
7 and 8, we see that SimRankand CoSimRank are very similar except that theyinitialize the similarities on the diagonal differ-ently.
Whereas SimRank sets each of these en-tries back to one at each iteration, CoSimRankadds one.
Thus, when computing the two similar-ity measures iteratively, the diagonal element (i, i)will be set to 1 by both methods for those initial it-erations for which this entry is 0 for cAS(k?1)AT(i.e., before applying either max or add).
Themethods diverge when the entry is 6= 0 for the firsttime.Complexity of computing all n2similarities.The matrix formulas of both SimRank (Eq.
7)and CoSimRank (Eq.
8) have time complexityO(n3) or ?
if we want to take the higher efficiencyof computation for sparse graphs into account ?O(dn2) where n is the number of nodes and d the1395average degree.
Space complexity is O(n2) forboth algorithms.Complexity of computing k2n2similar-ities.
In most cases, we only want to computek2similarities for k nodes.
For CoSimRank, wecompute the k PPR vectors inO(kdn) (Eq.
2) andcompute the k2similarities in O(k2n) (Eq.
5).
Ifd < k, then the time complexity of CoSimRankis O(k2n).
If we only compute a single similar-ity, then the complexity is O(dn).
In contrast, thecomplexity of SimRank is the same as in the all-similarities case: O(dn2).
It is not obvious how todesign a lower-complexity version of SimRank forthis case.
Thus, we have reduced SimRank?s cu-bic time complexity to a quadratic time complex-ity for CoSimRank or ?
assuming that the aver-age degree d does not depend on n ?
SimRank?squadratic time complexity to linear time complex-ity for the case of computing few similarities.Space complexity for computing k2similaritiesis O(kn) since we need only store k vectors, notthe complete similarity matrix.
This complexitycan be exploited even for the all similarities appli-cation: If the matrix formulation cannot be usedbecause the O(n2) similarity matrix is too big foravailable memory, then we can compute all sim-ilarities in batches ?
and if desired in parallel ?whose size is chosen such that the vectors of eachbatch still fit in memory.In summary, CoSimRank and SimRank havesimilar space and time complexities for comput-ing all n2similarities.
For the more typical casethat we only want to compute a fraction of all sim-ilarities, we have recast the global SimRank for-mulation as a local CoSimRank formulation.
As aresult, time and space complexities are much im-proved.
In Section 6, we will show that this is alsotrue in practice.5 ExtensionsWe will show now that the basic CoSimRank algo-rithm can be extended in a number of ways and isthus a flexible tool for different NLP applications.5.1 Weighted edgesThe use of weighted edges was first proposed inthe PageRank patent.
It is straightforward andeasy to implement by replacing the row normal-ized adjacency matrixA with an arbitrary stochas-tic matrix P .
We can use this edge weighted Page-Rank for CoSimRank.5.2 CoSimRank across graphsWe often want to compute the similarity of nodesin two different graphs with a known node-nodecorrespondence; this is the scenario we are facedwith in the lexicon extraction task (see Section 6).A variant of SimRank for this task was presentedby Dorow et al (2009).
We will now present anequivalent method for CoSimRank.
We denote thenumber of nodes in the two graphs U and V by|U | and |V |, respectively.
We compute PPR vec-tors p ?
R|U |and q ?
R|V |for each graph.
LetS(0)?
R|U |?|V |be the known node-node corre-spondences.
The analog of CoSimRank (Eq.
4)for two graphs is then:s(i, j) =??k=0ck?
(u,v)?S(0)p(k)u(i)q(k)v(j) (9)The matrix formulation (cf.
Eq.
6) is:S(k)= ckAkS(0)(BT)k+ S(k?1)(10)whereA andB are row-normalized adjacency ma-trices.
We can interpret S(0)as a change of basis.A similar approach for word embeddings was pub-lished by Mikolov et al (2013).
They call S(0)thetranslation matrix.5.3 Typed edgesTo be able to directly compare to prior work in ourexperiments, we also present a method to integratea set of typed edges T in the CoSimRank calcula-tion.
For this we will compute a similarity matrixfor each edge type ?
and merge them into one ma-trix for the next iteration:S(k)=(c|T |???TA?S(k?1)BT?
)+ S(0)(11)This formula is identical to the random surfermodel where two surfers only meet iff they areon the same node and used the same edge type toget there.
A more strict claim would be to use thesame edge type at any time of their journey:S(k)=ck|T |k??
?Tk(k?i=1A?i)S(0)(k?1?i=0BT?k?i)+ S(k?1)(12)We will not use Eq.
12 due to its space complexity.13965.4 Similarity of sets of nodesCoSimRank can also be used to compute the sim-ilarity s(V,W ) of two sets V and W of nodes,e.g., short text snippets.
We are not including thismethod in our experiments, but we will give theequation here, as traditional document similaritymeasures (e.g., cosine similarity) perform poorlyon this task although there also are known alter-natives with good results (Sahami and Heilman,2006).
For a set V , the initial PPR vector is givenby:p(0)i(V ) ={1|V |, if i ?
V0, elseWe then reuse Eq.
4 to compute s(V,W ):s(V,W ) =?
?k=0ck?p(k)(V ), p(k)(W )?In summary, modifications proposed for Sim-Rank (weighted and typed edges, similarity acrossgraphs) as well as modifications proposed forPageRank (sets of nodes) can also be applied toCoSimRank.
This makes CoSimRank a very flex-ible similarity measure.We will test the first three extensions experi-mentally in the next section and leave similarityof node sets for future work.6 ExperimentsWe evaluate CoSimRank for the tasks of syn-onym extraction and bilingual lexicon extraction.We use the basic version of CoSimRank (Eq.
4)for synonym extraction and the two-graph version(Eq.
9) for lexicon extraction, both with weightededges.
Our motivation for this application is thattwo words that are synonyms of each other shouldhave similar lexical neighbors and that two wordsthat are translations of each other should haveneighbors that correspond to each other; thus, ineach case the nodes should be similar in the graph-theoretic sense and CoSimRank should be able toidentify this similarity.We use the English and German graphs pub-lished by Laws et al (2010), including edgeweighting and normalization.
Nodes are nouns,adjectives and verbs occurring in Wikipedia.There are three types of edges, corresponding tothree types of syntactic configurations extractedfrom the parsed Wikipedias: adjective-noun, verb-object and noun-noun coordination.
Table 1 givesexamples and number of nodes and edges.Edge typesrelation entities description exampleamod a, v adjective-noun a fast cardobj v, n verb-object drive a carncrd n, n noun-noun cars and bussesGraph statisticsnodes nouns adjectives verbsde 34,544 10,067 2,828en 22,258 12,878 4,866edges ncrd amod dobjde 65,299 417,151 143,905en 288,878 686,069 510,351Table 1: Edge types (above) and number of nodesand edges (below)6.1 BaselinesWe propose CoSimRank as an efficient algorithmfor computing the similarity of nodes in a graph.Consequently, we compare against the two mainmethods for this task in NLP: SimRank and exten-sions of PageRank.We also compare against the MEE (Multi-EdgeExtraction) variant of SimRank (Dorow et al,2009), which handles labeled edges more effi-ciently than SimRank:S?
(k)=c|T |??
?TA?S(k?1)BT?S(k)= max{S?
(k), S(0)}where A?is the row-normalized adjacency matrixfor edge type ?
(see edge types in Table 1).Apart from SimRank, extensions of PageRankare the main methods for computing the similar-ity of nodes in graphs in NLP (e.g., Hughes andRamage (2007), Agirre et al (2009) and other pa-pers discussed in related work).
Generally, thesemethods compute the Personalized PageRank foreach node (see Eq.
1).
When the computation hasconverged, the similarity of two nodes is given bythe cosine similarity of the Personalized PageRankvectors.
We implemented this method for our ex-periments and call it PPR+cos.6.2 Synonym ExtractionWe use TS68, a test set of 68 synonym pairs pub-lished by Minkov and Cohen (2012) for evalua-tion.
This gold standard lists a single word as the1397P@1 P@10 MRRone-synonymPPR+cos 20.6% 52.9% 0.32SimRank 25.0% 61.8% 0.37CoSimRank 25.0% 61.8% 0.37Typed CoSimRank 23.5% 63.2% 0.37extendedPPR+cos 32.6% 73.5% 0.48SimRank 45.6% 83.8% 0.59CoSimRank 45.6% 83.8% 0.59Typed CoSimRank 44.1% 83.8% 0.59Table 2: Results for synonym extraction on TS68.Best result in each column in bold.correct synonym even if there are several equallyacceptable near-synonyms (see Table 3 for exam-ples).
We call this the one-synonym evaluation.Three native English speakers were asked to marksynonyms, that were proposed by a baseline or byCoSimRank, i.e.
ranked in the top 10.
If all threeof them agreed on one word as being a synonymin at least one meaning, we added this as a correctanswer to the test set.
We call this the ?extended?evaluation (see Table 2).Synonym extraction is run on the English graph.To calculate PPR+cos, we computed 20 iterationswith a decay factor of 0.8 and used the cosine sim-ilarity with the 2-norm in the denominator to com-pare two vectors.
For the other three methods, wealso used a decay factor of 0.8 and computed 5 it-erations.
Recall that CoSimRank uses the simpleinner product ?
?, ??
to compare vectors.Our evaluation measures are proportion ofwords correctly translated by word in the topposition (P@1), proportion of words correctlytranslated by a word in one of the top 10 posi-tions (P@10) and Mean Reciprocal Rank (MRR).CoSimRank?s MRR scores of 0.37 (one-synonym)and 0.59 (extended) are the same or better than allbaselines (see Table 2).
CoSimRank and SimRankhave the same P@1 and P@10 accuracy (althoughthey differed on some decisions).
CoSimRank isbetter than PPR+cos on both evaluations, but asthis test set is very small, the results are not signif-icant.
Table 3 shows a sample of synonyms pro-posed by CoSimRank.Minkov and Cohen (2012) tested cosine andrandom-walk measures on grammatical relation-keyword expected extractedmovie film filmmodern contemporary contemporarydemonstrate protest showattractive appealing beautifuleconomic profitable financialclose shut openTable 3: Examples for extracted synonyms.
Cor-rect synonyms according to extended evaluation inbold.ships (similar to our setup) as well as on cooccur-rence statistics.
The MRR scores for these meth-ods range from 0.29 to 0.59.
(MRR is equivalentto MAP as reported by Minkov and Cohen (2012)when there is only one correct answer.)
Theirbest number (0.59) is better than our one-synonymresult; however, they performed manual postpro-cessing of results ?
e.g., discarding words that aremorphologically or semantically related to otherwords in the list ?
so our fully automatic resultscannot be directly compared.6.3 Lexicon ExtractionWe evaluate lexicon extraction on TS1000, a testset of 1000 items, (Laws et al, 2010) each con-sisting of an English word and its German transla-tions.
For lexicon extraction, we use the same pa-rameters as in the synonym extraction task for allfour similarity measures.
We use a seed dictionaryof 12,630 word pairs to establish node-node corre-spondences between the two graphs.
We removea search keyword from the seed dictionary beforecalculating similarities for it, something that thearchitecture of CoSimRank makes easy becausewe can use a different seed dictionary S(0)for ev-ery keyword.Both CoSimRank methods outperform Sim-Rank significantly (see Table 4).
The differ-ence between CoSimRank with and without typededges is not significant.
(This observation was alsomade for SimRank on a smaller graph and test set(Laws et al, 2010).
)PPR+cos?s performance at 14.8% correct trans-lations is much lower than SimRank and CoSim-Rank.
The disadvantage of this similarity mea-sure is significant and even more visible on bilin-gual lexicon extraction than on synonym extrac-tion (see Table 2).
The reason might be that weare not comparing the whole PPR vector anymore,1398P@1 P@10PPR+cos 14.8%?45.7%?SimRank MEE 48.0%?76.0%?CoSimRank 61.1% 84.0%Typed CoSimRank 61.4% 83.9%Table 4: Results for bilingual lexicon extraction(TS1000 EN ?
DE).
Best result in each columnin bold.but only entries which occur in the seed dictionary(see Eq.
9).
As the seed dictionary contains 12,630word pairs, this means that only every fourth entryof the PPR vector (the German graph has 47,439nodes) is used for similarity calculation.
This isalso true for CoSimRank, but it seems that CoSim-Rank is more stable because we compare morethan one vector.
?We also experimented with the method of Fog-aras and R?acz (2005).
We tried a number of differ-ent ways of modifying it for weighted graphs: (i)running the random walks with the weighted ad-jacency matrix as Markov matrix, (ii) storing theweight (product of each edge weight) of a randomwalk and using it as a factor if two walks meetand (iii) a combination of both.
We needed about10,000 random walks in all three conditions.
As aresult, the computational time was approximately30 minutes per test word, so this method is evenslower than SimRank for our application.
The ac-curacies P@1 and P@10 were worse in all experi-ments than those of CoSimRank.6.4 Run time performanceTable 5 compares the run time performance ofCoSimRank with the baselines.
We ran all exper-iments on a 64-bit Linux machine with 64 IntelXenon X7560 2.27Ghz CPUs and 1TB RAM.
Thecalculated time is the sum of the time spent in usermode and the time spent in kernel mode.
The ac-tual wall clock time was significantly lower as weused up to 64 CPUs.Compared to SimRank, CoSimRank is morethan 40 times faster on synonym extraction and sixtimes faster on lexicon extraction.
SimRank is ata disadvantage because it computes all similaritiesin the graph regardless of the size of the test set;it is particularly inefficient on synonym extractionbecause the English graph contains a large number?significantly worse than CoSimRank (?
= 0.05, one-tailed Z-Test)synonym extraction lexicon extraction(68 word pairs) (1000 word pairs)PPR+cos 2,228 2,195SimRank 23,423 14,418CoSimRank 524 2,342Typed CoSimRank 615 6,108Table 5: Execution times in minutes for CoSim-Rank and the baselines.
Best result in each columnin bold.of edges (see Table 1).Compared to PPR+cos, CoSimRank is roughlyfour times faster on synonym extraction and hascomparable performance on lexicon extraction.We compute 20 iterations of PPR+cos to reachconvergence and then calculate a single cosinesimilarity.
For CoSimRank, we need only com-pute five iterations to reach convergence, but wehave to compute a vector similarity in each itera-tion.
The counteracting effects of fewer iterationsand more vector similarity computations can giveeither CoSimRank or PPR+cos an advantage, asis the case for synonym extraction and lexicon ex-traction, respectively.CoSimRank should generally be three timesfaster than typed CoSimRank since the typed ver-sion has to repeat the computation for each ofthe three types.
This effect is only visible on thelarger test set (lexicon extraction) because the gen-eral computation overhead is about the same on asmaller test set.6.5 Comparison with WINTIANHere we address inducing a bilingual lexicon froma seed set based on grammatical relations foundby a parser.
An alternative approach is to in-duce a bilingual lexicon from Wikipedia?s inter-wiki links (Rapp et al, 2012).
These two ap-proaches have different strengths and weaknesses;e.g., the interwiki-link-based approach does notrequire a seed set, but it can only be applied tocomparable corpora that consist of corresponding?
although not necessarily ?parallel?
?
documents.Despite these differences it is still interesting tocompare the two algorithms.
Rapp et al (2012)kindly provided their test set to us.
It contains1000 English words and a single correct Germantranslation for each.
We evaluate on a subset wecall TS774 that consists of the 774 test word pairsthat are in the intersection of words covered by the1399P@1 P@10Wintian 43.8% 55.4%?CoSimRank 43.0% 73.6%Table 6: Results for bilingual lexicon extraction(TS774 DE?
EN).
Best result in each column inbold.WINTIAN Wikipedia data (Rapp et al, 2012) andwords covered by our data.
Most of the 226 miss-ing word pairs are adverbs, prepositions and pluralforms that are not covered by our graphs due to theconstruction algorithm we use: lemmatization, re-striction to adjectives, nouns and verbs etc.Table 6 shows that CoSimRank is slightly, butnot significantly worse than WINTIAN on P@1(43.0 vs 43.8), but significantly better on P@10(73.6 vs 55.4).4The reason could be that CoSim-Rank is a more effective algorithm than WIN-TIAN; but the different initializations (seed set vsinterwiki links) or the different linguistic represen-tations (grammatical relations vs bag-of-words)could also be responsible.6.6 Error AnalysisThe results on TS774 can be considered conserva-tive since only one translation is accepted as beingcorrect.
In reality other translations might also beacceptable (e.g., both street and road for Stra?e).In contrast, TS1000 accepts more than one cor-rect translation.
Additionally, TS774 was createdby translating English words into German (usingGoogle translate).
We are now testing the reversedirection.
So we are doomed to fail if the originalEnglish word is a less common translation of anambiguous German word.
For example, the En-glish word gulf was translated by Google to Golf,but the most common sense of Golf is the sport.Hence our algorithm will incorrectly translate itback to golf.As we can see in Table 7, we also face the prob-lems discussed by Laws et al (2010): the algo-rithm sometimes picks cohyponyms (which canstill be seen as reasonable) and antonyms (whichare clear errors).Contrary to our intuition, the edge-typed vari-ant of CoSimRank did not perform significantlybetter than the non-edge-typed version.
Looking4We achieved better results for CoSimRank by optimizingthe damping factor, but in this paper, we only present resultsfor a fixed damping factor of 0.8.keyword gold standard CoSimRankarm poor impoverishederreichen reach achievegehen go walkdirekt directly directweit far furtherbreit wide narrowreduzieren reduce increaseStunde hour secondWesten west southwestJunge boy childTable 7: Examples for CoSimRank translation er-rors on TS774.
We counted translations as incor-rect if they were not listed in the gold standardeven if they were correct translations according towww.dict.cc (in bold).at Table 1, we see that there is only one edge typeconnecting adjectives.
The same is true for verbs.The random surfer only has a real choice betweendifferent edge types when she is on a noun node.Combined with the fact that only the last edge typeis important this has absolutely no effect for a ran-dom surfer meeting at adjectives or verbs.Two possible solutions would be (i) to use morefine-grained edge types, (ii) to apply Eq.
12, inwhich the edge type of each step is important.However, this will increase the memory needed forcalculation.7 SummaryWe have presented CoSimRank, a new similar-ity measure that can be computed for a singlenode pair without relying on the similarities in thewhole graph.
We gave two different formaliza-tions of CoSimRank: (i) a derivation from Person-alized PageRank and (ii) a matrix representationthat can take advantage of fast matrix multipli-cation algorithms.
We also presented extensionsof CoSimRank for a number of applications, thusdemonstrating the flexibility of CoSimRank as asimilarity measure.We showed that CoSimRank is superior toSimRank in time and space complexity; andwe demonstrated that CoSimRank performs bet-ter than PPR+cos on two similarity computationtasks.Acknowledgments.
This work was supportedby DFG (SCHU 2246/2-2).1400ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pas?ca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and wordnet-based approaches.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,NAACL ?09, pages 19?27.Sergey Brin and Lawrence Page.
1998.
The anatomyof a large-scale hypertextual web search engine.
InWWW, pages 107?117.Sung-Hyuk Cha.
2007.
Comprehensive survey on dis-tance/similarity measures between probability den-sity functions.
Mathematical Models and Methodsin Applied Sciences, 1(4):300?307.Ching-Yun Chang, Stephen Clark, and Brian Harring-ton.
2013.
Getting creative with semantic similarity.In Semantic Computing (ICSC), 2013 IEEE SeventhInternational Conference on, pages 330?333.Gerard de Melo and Gerhard Weikum.
2012.
Uwn: Alarge multilingual lexical knowledge base.
In ACL(System Demonstrations), pages 151?156.Beate Dorow, Florian Laws, Lukas Michelbacher,Christian Scheible, and Jason Utt.
2009.
A graph-theoretic algorithm for automatic extension of trans-lation lexicons.
In Proceedings of the Workshop onGeometrical Models of Natural Language Seman-tics, GEMS ?09, pages 91?95.G?unes Erkan and Dragomir R. Radev.
2004.
Lexrank:Graph-based lexical centrality as salience in textsummarization.
J. Artif.
Intell.
Res.
(JAIR), 22:457?479.D?aniel Fogaras and Bal?azs R?acz.
2005.
Scalinglink-based similarity search.
In Proceedings of the14th international conference on World Wide Web,WWW ?05, pages 641?650.Xianpei Han and Jun Zhao.
2010.
Structural semanticrelatedness: a knowledge-based method to namedentity disambiguation.
In Proceedings of the 48thAnnual Meeting of the Association for Computa-tional Linguistics, ACL ?10, pages 50?59.Taher H. Haveliwala.
2002.
Topic-sensitive pagerank.In Proceedings of the 11th international conferenceon World Wide Web, WWW ?02, pages 517?526.Cong Duy Vu Hoang and Min-Yen Kan. 2010.
To-wards automated related work summarization.
InProceedings of the 23rd International Conference onComputational Linguistics: Posters, COLING ?10,pages 427?435.Thad Hughes and Daniel Ramage.
2007.
Lexical se-mantic relatedness with random graph walks.
InEMNLP-CoNLL, pages 581?589.Tommi Jaakkola, David Haussler, et al 1999.
Exploit-ing generative models in discriminative classifiers.Advances in neural information processing systems,pages 487?493.Tony Jebara, Risi Kondor, and Andrew Howard.
2004.Probability product kernels.
The Journal of MachineLearning Research, 5:819?844.Glen Jeh and Jennifer Widom.
2002.
Simrank: ameasure of structural-context similarity.
In Proceed-ings of the eighth ACM SIGKDD international con-ference on Knowledge discovery and data mining,KDD ?02, pages 538?543.Jon M. Kleinberg.
1999.
Authoritative sources ina hyperlinked environment.
Journal of the ACM,46(5):604?632.Florian Laws, Lukas ichelbacher, Beate Dorow, Chris-tian Scheible, Ulrich Heid, and Hinrich Sch?utze.2010.
A linguistically grounded graph modelfor bilingual lexicon extraction.
In Coling 2010:Posters, pages 614?622.Elizabeth Leicht, Petter Holme, and Mark Newman.2006.
Vertex similarity in networks.
Physical Re-view E, 73(2):026120.Ronny Lempel and Shlomo Moran.
2000.
Thestochastic approach for link-structure analysis(salsa) and the tkc effect.
Computer Networks,33(1):387?401.Pei Li, Zhixu Li, Hongyan Liu, Jun He, and Xiaoy-ong Du.
2009.
Using link-based content analy-sis to measure document similarity effectively.
InProceedings of the Joint International Conferenceson Advances in Data and Web Management, AP-Web/WAIM ?09, pages 455?467.Cuiping Li, Jiawei Han, Guoming He, Xin Jin, YizhouSun, Yintao Yu, and Tianyi Wu.
2010.
Fast com-putation of simrank for static and dynamic informa-tion networks.
In Proceedings of the 13th Interna-tional Conference on Extending Database Technol-ogy, EDBT ?10, pages 465?476.Dmitry Lizorkin, Pavel Velikhov, Maxim Grinev, andDenis Turdakov.
2010.
Accuracy estimate and op-timization techniques for simrank computation.
TheVLDB Journal?The International Journal on VeryLarge Data Bases, 19(1):45?66.Rada Mihalcea and Dragomir Radev.
2011.
Graph-based natural language processing and informationretrieval.
Cambridge University Press.Tomas Mikolov, Quoc V Le, and Ilya Sutskever.
2013.Exploiting similarities among languages for ma-chine translation.
arXiv preprint arXiv:1309.4168.Einat Minkov and William W. Cohen.
2008.
Learn-ing graph walk based similarity measures for parsedtext.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP?08, pages 907?916.1401Einat Minkov and William W. Cohen.
2012.
Graphbased similarity measures for synonym extractionfrom parsed text.
In Workshop Proceedings ofTextGraphs-7 on Graph-based Methods for NaturalLanguage Processing, TextGraphs-7 ?12, pages 20?24.Pradeep Muthukrishnan, Dragomir Radev, andQiaozhu Mei.
2010.
Edge weight regularizationover multiple graphs for similarity learning.
In DataMining (ICDM), 2010 IEEE 10th InternationalConference on, pages 374?383.Diarmuid?O S?eaghdha and Ann Copestake.
2008.
Se-mantic classification with distributional kernels.
InProceedings of the 22nd International Conferenceon Computational Linguistics-Volume 1, pages 649?656.Delip Rao, David Yarowsky, and Chris Callison-Burch.2008.
Affinity measures based on the graph Lapla-cian.
In Proceedings of the 3rd Textgraphs Work-shop on Graph-Based Algorithms for Natural Lan-guage Processing, TextGraphs-3, pages 41?48.Reinhard Rapp, Serge Sharoff, and Bogdan Babych.2012.
Identifying word translations from compa-rable documents without a seed lexicon.
In LREC,pages 460?466.Mehran Sahami and Timothy D. Heilman.
2006.
Aweb-based kernel function for measuring the simi-larity of short text snippets.
In Proceedings of the15th international conference on World Wide Web,WWW ?06, pages 377?386.Christian Scheible, Florian Laws, Lukas Michelbacher,and Hinrich Sch?utze.
2010.
Sentiment transla-tion through multi-edge graphs.
In Proceedingsof the 23rd International Conference on Compu-tational Linguistics: Posters, COLING ?10, pages1104?1112.Hinrich Sch?utze and Michael Walsh.
2008.
A graph-theoretic model of lexical syntactic acquisition.
InEMNLP, pages 917?926.Wensi Xi, Edward A.
Fox, Weiguo Fan, Benyu Zhang,Zheng Chen, Jun Yan, and Dong Zhuang.
2005.Simfusion: measuring similarity using unified re-lationship matrix.
In Proceedings of the 28th an-nual international ACM SIGIR conference on Re-search and development in information retrieval, SI-GIR ?05, pages 130?137.1402
