Proceedings of the SIGDIAL 2013 Conference, pages 145?147,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsAIDA: Artificial Intelligent Dialogue AgentRafael E. Banchs, Ridong Jiang, Seokhwan Kim, Arthur Niswar, Kheng Hui YeoNatural Language Understanding Lab, Human Language Technology DepartmentInstitute for Infocomm Research, Singapore 138632{rembanchs,rjiang,kims,aniswar,yeokh}@i2r.a-star.edu.sgAbstractThis demo paper describes our Artificial Intel-ligent Dialogue Agent (AIDA), a dialoguemanagement and orchestration platform underdevelopment at the Institute for Infocomm Re-search.
Among other features, it integrates dif-ferent human-computer interaction enginesacross multiple domains and communicationstyles such as command, question answering,task-oriented dialogue and chat-oriented dia-logue.
The platform accepts both speech andtext as input modalities by either direct micro-phone/keyboard connections or by means ofmobile device wireless connection.
The outputinterface, which is supported by a talking ava-tar, integrates speech and text along with othervisual aids.1 IntroductionSome recent efforts towards the development ofa more comprehensive framework for dialoguesupported applications include research on multi-domain or multi-task dialogue agents (Komataniet.
al 2006, Lee et.
al 2009, Nakano et.
al 2011,Lee et.
al 2012).
With this direction in mind, ourArtificial Intelligent Dialogue Agent (AIDA) hasbeen created aiming the following two objec-tives: (1) serving as a demonstrator platform forshowcasing different dialogue systems and relat-ed technologies, and (2) providing an experi-mental framework for conducting research in thearea of dialogue management and orchestration.The main objective of this paper is to presentand describe the main characteristics of AIDA.The rest of the paper is structured as follows.First, in section 2, a description of APOLLO, thesoftware integration platform supporting AIDAis presented.
Then, in section 3, the main featuresof AIDA as a dialogue management and orches-tration platform are described, and a real exam-ple of human interaction with AIDA is reported.Finally, in section 4, our conclusions and futurework plans are presented.2 The APOLLO Integration PlatformAPOLLO (Jiang et al2012) is a componentpluggable dialogue framework, which allows forthe interconnection and control of the differentcomponents required for the implementation ofdialogue systems.
This framework allows for theinteroperability of four different classes of com-ponents: dialogue (ASR, NLU, NLG, TTS, etc.
),managers (vertical domain-dependent task man-agers), input/output (speech, text, image and vid-eo devices), and backend (databases, web crawl-ers and indexes, rules and inference engines).The different components can be connected toAPOLLO either by means of specifically createdplug-ins or by using TCP-IP based socket com-munications.
All component interactions are con-trolled by using XML scripts.
Figure 1 presents ageneral overview of the APOLLO framework.Figure 1: The APOLLO framework1453 Main Features of AIDAAIDA (Artificial Intelligent Dialogue Agent) is adialogue management and orchestration plat-form, which is implemented over the APOLLOframework.
In AIDA, different communicationtask styles (command, question answering, task-oriented dialogue and chatting) are hierarchicallyorganized according to their atomicity; i.e.
moreatomic (less interruptible) tasks are given prefer-ence over less atomic (more interruptible) tasks.In the case of the chatting engine, as it is theleast atomic task of all, it is located in the bottomof the hierarchy.
This engine also behaves as aback-off system, which is responsible for takingcare of all the user interactions that other enginesfail to resolve properly.In AIDA, a dialogue orchestration mechanismis used to simultaneously address the problemsof domain switching and task selection.
One ofthe main components of this mechanism is theuser intention inference module, which makesinformed decisions for selecting and assigningturns across the different individual engines inthe platform.Domain and task selection decisions are madebased on three different sources of information:the current user utterance, which includes stand-ard semantic and pragmatic features extractedfrom the user utterance; engine informationstates, which takes into account individual in-formation states from all active engines in theplatform; and system expectations, which is con-structed based on the most recent history of user-system interactions, the task hierarchy previouslydescribed and the archived profile of the currentuser interacting with the system.Our current implementation of AIDA inte-grates six different dialogue engines: (BC) a basiccommand application, which is responsible forserving basic requests such as accessing calendarand clock applications, interfacing with searchengines, displaying maps, etc.
; (RA) a reception-ist application, which consists of a question an-swering system for providing information aboutthe Fusionopolis Complex; (IR) I2R informationsystem, which implements as question answeringsystem about our institute; (FR) a flight reserva-tion system, which consists of a frame-based dia-logue engine that uses statistical natural languageunderstanding; (RR) a restaurant recommenda-tion system, which implements a three-stageframe-based dialogue system that uses rule-basenatural language understanding, and (CH) ourIRIS chatting agent (Banchs and Li, 2012).Regarding input/output modalities, speech andtext can be used as input channels for user utter-ances.
Direct connections via microphone andkeyboard are supported, as well as remote con-nections via mobile devices.Additionally, audio and video inputs are usedto provide AIDA with user identification andtracking capabilities.
In the first case, speakeridentification techniques are used to compare thevoice profile of the current speaker with a set ofusers already known by the system.
In the secondcase, face detection and tracking are used incombination with sound localization to deter-mine what the current speaker?s location is whendealing with multi-party dialogue scenarios.The main output of AIDA is composed of abrowser interface in which several frames, in-cluding different visual elements, are presentedalong with a talking avatar.
The different visualelements include a banner, a text display and ageneral purpose frame for displaying differenthtml files, images or URL?s as required.For avatar and text-to-speech, AIDA relies onthe SitePal API (SitePal 2013).
This API allowsfor controlling both the avatar speech and gaze.Speech functionalities are used for generating aspoken form of the system?s output (which isalso displayed as text in one of the frames of theinterface) and gaze functionalities are used incombination with face tracking to direct the ava-tar attention to the user who is speaking.Figure 2 illustrates the main output interfaceof AIDA.Figure 2: AIDA?s output interface.146Finally, an actual example of a user interactionwith the AIDA platform is presented in Table 1.In the table, the following information is provid-ed for each of the turns composing the dialogue:the user utterance, the corresponding system re-sponse, the engine that was responsible for gen-erating the response, and the action taken by thesystem (if any action was taken).Turn Speaker Utterance/Action1USER Good morning, how are you?AIDA-CH How can I help you?2USER Can you help me to book a flight?AIDA-FR Flight reservation, how can I help you?
(action) Displays flight reservation main screen3USER I want to fly from Singapore to TokyoAIDA-FR What day do you want to leave?4USER Next Monday in the eveningAIDA-FR What day do you want to return?5USER The second of April in the morningAIDA-FR Here is your itinerary(action) Displays flight schedule on the screen6USER Is there any coffee shop nearby?AIDA-RAThere are three coffee shops in this buil-ding: Jass Caf?, Starbucks and Ya Kun(action) Shows logos of retrieved coffee shops7USER Do you drink coffee?AIDA-CH Coffee?8USER Yes, coffeeAIDA-CH No9USER Why not?AIDA-CH Well, for one thing you can?t spell.10USER Please suggest a good Italian restaurantAIDA-RRWhat area of the city do you want me tosearch for?
(action) Displays restaurant system main screen11USER Near SentosaAIDA-RROk, I have found one restaurant for you:Alkaff Mansion Ristorante.
Do youwant more information or do you wantto make a reservation?
(action) Displays http://www.alkaff.com.sg/12USER Yes pleaseAIDA-RROk, I will do a reservation for you!
And,how many guests is this reservation for?13USER Only two personsAIDA-RRAnd, will you be interested in dinner orlunch?14USER Please make it for tonightAIDA-RR Your reservation has been made.
(action) Displays reservation details15USER Great, thank youAIDA-CH You are welcomeTable 2: Example of a user interaction with AIDA4 Conclusions and Future WorkIn this demo paper we have presented AIDA (Ar-tificial Intelligent Dialogue Agent), the dialoguemanagement and orchestration platform currentlyunder development at the Institute for InfocommResearch in Singapore.AIDA integrates different communication taskstyles (command, question answering, dialogueand chat) into a single conversational agent.
Italso combines speech recognition and synthesis,image processing and avatar technologies to im-prove the user-system interaction experience.Our future plans include the incorporation ofmore vertical applications into the platform, aswell as the integration of machine translationengines to develop multilingual capabilities.Demonstration PlanDuring the SIGDIAL demo presentation, the fol-lowing functionalities will be demonstrated: textand speech input; dialogue orchestration amongreceptionist, flight reservation, I2R informationsystem, restaurant booking and chatting agent;and avatar-supported speech and visual outputinterface.
For the case of speech input and ava-tar-supported output, the use of these technolo-gies is subject to the availability of internet con-nection at the location of the demo.ReferencesR.
E. Banchs and H. Li.
2012.
IRIS: a chat-orienteddialogue system based on the vector space model,in Demo Session of Association of ComputationalLinguistics, pp.
37?42.R.
Jiang, Y. K. Tan, D. K. Limbu and H. Li.
2012.Component pluggable dialogue framework and itsapplication to social robots.
In Proc.
Int?l Work-shop on Spoken Language Dialog Systems.K.
Komatani, N. Kanda, M. Nakano, K. Nakadai, H.Tsujino, T. Ogata and H. G. Okuno.
2006.
Multi-domain spoken dialogue system with extensibilityand robustness against speech recognition errors.
InProc.
SIGdial Workshop on Discourse and Dia-logue, pp.
9?17.C.
Lee, S. Jung, S. Kim and G. G. Lee.
2009.
Exam-ple-based dialog modeling for practical multi-domain dialog system.
Speech Communication, 51,pp.
466?484.I.
Lee, S. Kim, K. Kim, D. Lee, J. Choi, S. Ryu andG.
G. Lee.
2012.
A two step approach for efficientdomain selection in multi-domain dialog systems.In Proc.
Int?l Workshop on Spoken Dialogue Sys-tems.M.
Nakano, S. Sato, K. Komatani, K. Matsutama, K.Funakoshi and H. G. Okuno.
2011.
A two stagedomain selection framework for extensible multi-domain spoken dialogue systems.
In Proc.
SIGdialWorkshop on Discourse and Dialogue.SitePal API & Programmer Information, accessed onJune 27th, 2013 http://www.sitepal.com/support/147
