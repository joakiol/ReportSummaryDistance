Zock/Rapp/Huang (eds.
): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 50?59,Dublin, Ireland, August 23, 2014.NaDiR: Naive Distributional Response GenerationInstitut f?urMaschinelle SprachverarbeitungUniversit?at StuttgartGabriella LapesaInstitut f?urKognitionswissenschaftUniversit?at Osnabr?uckglapesa@uos.deStefan EvertProfessur f?urKorpuslinguistikFAU Erlangen-N?urnbergstefan.evert@fau.deAbstractThis paper describes NaDiR (Naive DIstributional Response generation), a corpus-based systemthat, from a set of word stimuli as an input, generates a response word relying on associationstrength and distributional similarity.
NaDiR participated in the CogALex 2014 shared task onmultiword associations (restricted systems track), operationalizing the task as a ranking problem:candidate words from a large vocabulary are ranked by their average association or similarity toa given set of stimuli.
We also report on a number of experiments conducted on the sharedtask data, comparing first-order models (based on co-occurrence and statistical association) tosecond-order models (based on distributional similarity).1 IntroductionThis paper describes NaDiR, a corpus-based system designed for the reverse association task.
NaDiRis an acronym for Naive Distributional Response generation.
NaDiR is naive because it is based on avery simple algorithm that operationalizes the multiword association task as a ranking problem: candi-date words from a large vocabulary are ranked by their average statistical association or distributionalsimilarity to a given set of stimuli, then the highest-ranked candidate is selected as NaDiR?s response.We compare models based on collocations (first-order models, see Evert (2008) for an overview) tomodels based on distributional similarity (second-order models; see Sahlgren (2006), Turney and Pan-tel (2010), and reference therein for a review).
Previous work on this task showed that co-occurrencemodels outperform distributional semantic models (henceforth, DSMs), and that using rank measuresimproves performance because it accounts for directionality of the association/similarity (e.g., the asso-ciation from stimulus to response may be larger than the association from response to stimulus).
Ourresults corroborate both claims.The paper is structured as follows: section 2 provides an overview of the task and of the problemswe encountered in its implementation; section 3 summarizes related work; section 4 describes NaDiR indetail; section 5 reports the results of our experiments on the shared task training and test data; section 6describes ongoing and future work on NaDiR.2 The Task and its ProblemsThe shared task datasets are derived from the Edinburgh Associative Thesaurus (Kiss et al., 1973)1.
TheEdinburgh Associative Thesaurus (henceforth, EAT) contains free associations to approximately 8000English cue words.
For each cue (e.g., visual) EAT lists all associations collected in the survey (e.g., aid,eyes, aids, see, eye, seen, sight, etc.)
sorted according to the number of subjects who responded with therespective word.
The CogALex shared task on multiword association is based on the EAT dataset, andis in fact a reverse association task (Rapp, 2014).
The top five responses for a target word are providedas stimuli (e.g., aid, eyes, aids, see, eye), and the participating systems are required to generate theoriginal cue as a response (e.g., visual).
The training and the test sets are random extracts of 2000 EATThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1http://www.eat.rl.ac.uk/50items each, with minimal pre-processing (only items containing multiword units and non-alphabeticalcharacters were discarded).A key problem we had to tackle while developing our system was the unrestricted set of possible re-sponses in combination with a discrete association task, which requires the algorithm to pick exactly theright answer out of tens of thousands of possible responses.
This feature makes this task much more dif-ficult than the multiple-choice tasks often used to evaluate distributional semantic models.
The problemis further complicated by the fact that the response may be an inflected form and only a prediction of theexact form was accepted as a correct answer.
The need for a solution to these issues motivates variousaspects of the NaDiR algorithm, described in section 4.3 Related WorkPrevious studies based on free association norms differ considerably in terms of the type of task (regularfree association task ?
one stimulus, one response vs. multiword association task ?
many stimuli, oneresponse), gold standards, and key features of the evaluated models (e.g., source corpora used and choiceof a candidate vocabulary from which responses are selected).In regular free association tasks (one stimulus, one response), responses are known to contain bothparadigmatically and syntagmatically related words.
Rapp (2002) proposes to integrate first-order (co-occurrence lists) and second-order (bag-of-words DSMs) information to distinguish syntagmatic fromparadigmatic relations by exploiting the comparison of most salient collocates and nearest neighbors.A task derived from the EAT norms was used in the ESSLLI 2008 shared task2.
Results from first-order co-occurrence data turned out to be much better than those from second-order DSMs, in line withthe findings made by Rapp (2002) and Wettler et al.
(2005).A similar picture emerges from studies on the multiword association task.
Models based on first-orderco-occurrence (collocations) outperform models based on vector similarity.
This superiority, however, isnot validated via a direct comparison: results were obtained by studies with different features and goals(see Rapp (2014) for a review; see Griffiths et al.
(2007) and Smith et al.
(2013) for evaluations ofmodels based on Latent Semantic Analysis).
A specific feature of successful studies on the multiwordassociation task is that they introduce an element of directionality (Rapp, 2013; Rapp, 2014), whichallows a correct implementation of the directionality of the modeled effects (from stimulus to response).Our survey of related studies motivated the choice to base NaDiR on first-order or second-order co-occurrence statistics, and to use collocate or neighbor rank to account for directionality.
Our main contri-bution to research on the reverse association task is a systematic experimental comparison of first-orderand second-order models (using the same gold standard, same source corpus, and same candidate vocab-ulary), which enables us to give a sound answer to the question whether first-order models are indeedsuperior for multiword association tasks.4 NaDiRNaDiR operationalizes the multiword association task as a ranking problem.
For each set of stimuli,the possible response words (?candidates?)
are ranked according to their average association strength ordistributional similarity to the stimulus words.
The top-ranked candidate is selected as NaDiR?s response.One advantage of the ranking approach is that it provides additional insights into the experimental results:if the model prediction is not correct, the rank of the correct answer can be used as a measure how ?close?the model came to the human associations.Since neither a fixed set of response candidates nor an indication of the source of the training andtest data were available (and we did not google for the training sets), we compiled a large vocabulary ofpossible responses.
We believe that restricting the vocabulary to the 8,033 cue words in the EAT wouldhave improved our results considerably.
More details concerning the choice of the candidate vocabularyare reported in section 4.1.2http://wordspace.collocations.de/doku.php/data:esslli2008:correlation with freeassociation norms51NaDiR uses either first-order or second-order co-occurrence statistics to predict the associationstrength between stimuli and responses.
In the first case (?collocations?
), we apply one of several stan-dard statistical association measures to co-occurrence counts obtained from a large corpus.
In the secondcase, association is quantified by cosine similarity in a distributional semantic model built from the samecorpus.
Both first-order and second-order statistics were collected from UKWaC in order to compete inthe constrained track of the shared task.Recent experiments (Hare et al., 2009; Lapesa and Evert, 2013; Lapesa et al., to appear) suggestthat semantic relations are often better captured by neighbour ranks rather than direct use of statisticalassociation measures or cosine similarity values.
Therefore, NaDiR can alternatively quantify associationstrength by collocate rank and similarity by neighbour rank.
In our experiments (section 5), we comparethe different approaches.NaDiR is designed for the multiword association task, and it contains additional features related to theparticular design of the CogALex shared task:?
We reduce the number of candidates by selecting the most likely response POS with a machine-learning algorithm (section 4.1);?
NaDiR operates on lemmatized data in order to reduce sparseness.
We lemmatize stimuli using aheuristic method (section 4.1), predict a response lemma, and then use machine-learning techniquesto generate a plausible word form (section 4.3).4.1 Pre-processing and VocabularyOur experiments were conducted on the UKWaC3corpus.
UKWaC contains 2 billion words, web-crawled from the .uk domain between 2005 and 2007.
The release of UKWaC also contains linguisticannotation (pos-tagging and lemmatization) performed with Tree Tagger4.To assign a part-of-speech tag and a lemma to every word in the dataset without relying on externaltools, we adopted the following mapping strategy based on the linguistic annotation already available inUKWaC:1.
We extracted all attested wordform/part of speech/lemma combinations from UKWaC, togetherwith their frequency;2.
Every word form in the training set was assigned to the most frequent part of speech/lemma combi-nation attested in UKWaC.We believe that the advantages of constructing distributional models based on lemmatized words over-come the drawbacks of this type of out-of-context lemmatization and part-of-speech assignment.The part-of-speech information added to every word in the dataset by the mapping procedure wasused to train a classifier that, given the parts of speech of the stimuli, predicts the part of speech of theresponse.
We trained a support-vector machine, using the svm function from the R package e10715,with standard settings.The part-of-speech classifier is based on a coarse part-of-speech tagset with only five tags: N (noun),J (adjective), V (verb), R (adverb), other (closed-class words).
We considered each row of the datasetas an observation, with the part of speech of the response as predicted value, and the part of speech ofthe stimulus words as predictors.
Every observation is represented as a bag of tags, i.e., a vector listingfor each of the five tags how often it occurs among the stimuli.
For example, if a set of stimuli contains3 nouns, one verb and one adjective, the corresponding bag-of-tags vector looks as follows: {N = 3; V =1; J = 1; R = 0; other = 0}.
On the training set, the part-of-speech classifier achieves an accuracy of72%.The vocabulary of our models only contains lemmatized open-class words (this information is avail-able in the annotation of the corpus).
By inspecting the frequencies of stimuli and response words in thetraining dataset, we established a reasonable minimum frequency threshold for candidate words of 100occurrences in UKWaC.
With this threshold, only 10 response words and 16 stimulus words from the3wacky.sslmit.unibo.it/doku.php?id=corpora4http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/5http://cran.r-project.org/web/packages/e1071/index.html52training dataset are excluded from the vocabulary.
Given the large size of the dataset, we decided thata minimal loss in coverage would be justified by the reduced computational complexity.
The resultingcandidate vocabulary contains 155,811 words.4.2 First- and Second-order StatisticsThe aim of this section is to describe the parameters involved in the collection of first-order and second-order statistics from UKWaC.
All models have been built and evaluated using the UCS toolkit6and thewordspace package for R (Evert, to appear)7.First-order ModelsCollocation data are compiled from UKWaC based on the vocabulary described in section 4.1.
Bothnodes (rows of the co-occurrence matrix) and collocates (columns of the co-occurrence matrix) are cho-sen from this vocabulary.
Collection of first-order models involved the manipulation of a number ofparameters, briefly summarized below.We adopted three different window sizes:?
symmetric window, 2 words to the left and to the right of the node;?
asymmetric window, 3 words to the left of the node;?
asymmetric window, 3 words to the right of the node.We tested the following association scores (Evert, 2008):?
co-occurrence frequency;?
simple log-likelihood (similar to local MI used by Baroni and Lenci (2010));?
conditional probability.Our experiments involved a third parameter, the index of association strength, which determines al-ternative ways of quantifying the degree of association between targets and contexts in the first-ordermodel.
Given two words a and b represented in a first-order model, we propose two alternative ways ofquantifying the degree of association between a and b.
The first option (and standard in corpus-basedmodeling) is to compute the association score between a and b.
The alternative choice is based on rankamong collocates.
Given two words a and b, in our task stimulus and potential response, we consider:?
forward rank: the rank of the potential response among the collocates of the stimulus;?
backward rank: the rank of the stimulus among the collocates of the potential response;?
average rank: the average of forward and backward rank.Second-order ModelsBased on the results of a large-scale evaluation of DSM parameters (Lapesa and Evert, under review)and the modeling of semantic priming effects (Lapesa and Evert, 2013; Lapesa et al., to appear), weidentified a robust configuration of parameters for second-order models that we decided to adopt in thisstudy.
Second-order models involved in our experiments share the following parameter settings:?
The target words (rows) are defined by the vocabulary described in section 4.1.?
The context words (columns) are the 50,000 most frequent context words in the respective co-occurrence matrices.
The 50 most frequent words in UKWaC are discarded.?
Co-occurrence vectors are scored with a sparse version of simple-log likelihood, in which negativevalues clamped to zero in order to preserve the sparseness of the co-occurrence matrix.
Scoredvectors are rescaled by applying a logarithmic transformation.?
We reduce the scored co-occurrence matrix to 1000 latent dimensions using randomized SVD(Halko et al., 2009).?
We adopt cosine distance (i.e.
the angle between vectors) as a distance metric for the computationof vector similarity.6http://www.collocations.de/software.html7http://r-forge.r-project.org/projects/wordspace/53Our experiments on second-order models involved the manipulation of two parameters: window sizeand index of association strength.The size of the context window quantifies the amount of shared context involved in the computation ofsimilarity.
We expect the manipulation of window size to be crucial in determining model performance,as different context windows will enable the model to capture different types of relations between re-sponse and stimulus words (Sahlgren, 2006; Lapesa et al., to appear).
In our experiments with NaDiR,we adopted three different window sizes:?
symmetric window, 2 words to the left and to the right of the target;?
symmetric window, 4 words to the left and to the right of the target;?
symmetric window, 16 words to the left and to the right of the target.The values for index of association strength are the same as for the first-order models, computing ranksamong the nearest neighbors of the stimulus or response word.
The use of rank-based measures is ofparticular interest, because: (i) it allows us to model directionality (while, for example, cosine distance issymmetric); (ii) it already proved successful in modeling behavioral data (Hare et al., 2009; Lapesa andEvert, 2013); (iii) since the vocabulary of first-order and second-order models are identical, rank-basedmeasures allow a direct comparison between the two classes of models, as well as experiments based ontheir combination.4.3 Response GenerationTo generate a response for a set of stimuli in the training/test dataset, we apply the following procedure:1.
For each set of stimuli, we compute association strengths or similarities between each stimulus andeach response candidate, adopting one of the measures described in section 4.2.2.
From the set of potential responses, we select the words whose POS agrees with the predictions ofthe classifier described in section 4.1.
Stimulus words are discarded from the potential answers.3.
We compute the average association strength or similarity across all five stimuli; if a stimulus doesnot appear in the model, it is simply omitted from the average.4.
The top-ranked candidate is the POS-disambiguated lemma suggested as a response by NaDiR.5.
We generate a suitable word form by inverting the heuristic lemmatization; if the full Penn tag (e.g.,NNS: noun, common, plural; NN: noun, common, singular or mass, etc.)
of the response is known,this step can be implemented as a deterministic lookup (since a word form is usually determineduniquely by lemma and Penn tag).
We therefore trained a second SVM classifier that predicts thefull Penn tag of the response based on the full tags of the stimuli.
On the training set, this part-of-speech classifier reaches an accuracy of 68%.5 ExperimentsIn our experiments, we compared first-order (collocations) and second-order (DSM) models; for eachclass of models, we evaluated the different parameter values described in section 4.2.
Table 1 summarizesthe evaluated parameters for first-order and second-order models.Model Window Score Relatedness Indexfirst-order symmetric, 2 frequency association scoreleft 3, right 0 simple log-likelihood forward rankleft 0, right 3 conditional probability backward rankaverage ranksecond-order symmetric, 2 simple log-likelihood distancesymmetric, 4 forward ranksymmetric, 16 backward rankaverage rankTable 1: Evaluated Parameters for First- and Second-order Models54Tables 2 to 5 display the results of our experiments on the training data, separately for first-order (tables2-4) and second-order models (table 5).
Parameter configurations are reported in the Parameter column8.The number of correct responses in the lemmatized version is reported in the column Lemma (showinghow often our system predicted the correct lemma).
The column Wordform reports the number of correctresponses for which, before inverting the lemmatization, the inflected form was already identical to thelemma.
As the task of predicting exactly one word is particularly difficult, we further characterize theperformance of our evaluated models by reporting the number of cases in which the correct answer fromthe training set was among the first 10 (< 10), 50 (< 50), or 100 (< 100) ranked candidates.
In the lastcolumn, we report the average rank of the correct responses (Avg correct).The results reported in tables 2 to 5 allowed us to identify best parameter configurations for the first-order (symmetric 2 words window, frequency, backward rank) and second-order models (2 words win-dow, distance).
We evaluated these configurations on the test data (table 6).
Table 7 compares theperformance of the best first-order and the best second-order model on the training and test datasets,both for lemmatized response (Training-Lemma, Test-Lemma) and generation of the correct word form(Training-Inflected, Test-Inflected).A considerable portion of the experiments reported in this paper were conducted after the submissiondeadline of the CogALex shared task.
As a consequence, our submitted results do not correspond to thebest overall configuration found in the evaluation study.
The submission was based on a second ordermodel, a 4-word window, and cosine distance as index of distributional similarity.
In this configuration,NaDiR generated 262 correct responses, corresponding to an accuracy of 13%.Parameters Lemma Wordform < 10 < 50 < 100 Avg correctFreqass2 2 85 372 561 1400Freqfwd0 0 77 359 550 6258Freqbwd555 464 973 1269 1369 1546Freqavg424 322 677 848 934 5969Simple-llass33 28 237 721 985 933Simple-llfwd405 319 760 916 947 12031Simple-llbwd531 444 914 1141 1253 1971Simple-llavg490 388 785 918 950 11645Cond.probass18 16 329 746 970 978Cond.probfwd0 0 77 359 550 6258Cond.probbwd422 359 856 1129 1255 1719Cond.probavg343 256 611 860 971 5948Table 2: First Order Models - Symmetric Window: 2 words to the left/right of the node - Training Data5.1 DiscussionThe results of our experiments are in line with the tendencies identified in the literature (see section3).
First-order models based on direct co-occurrence (high scores are assigned to words that co-occur),outperform second-order models based on distributional similarity (smaller distances between words thatoccur in similar contexts).For the first-order models, the best index of association strength is backward rank (the rank of thestimulus among the collocates of the potential response), fully congruent with the experimental setting(in the EAT norm, subjects produced the stimuli as free associations of the expected response).
Surpris-ingly, frequency outperforms simple-log likelihood (which is usually considered to be among the bestassociation measures for the identification of collocations).
In line with the results achieved by Rapp(2014), a symmetric window of 2 words to the left and to the right of the target achieves best results.For the second-order models, the smallest context window (2 words) achieves the best performance.8Abbreviations used in the tables: ass = association score; dist = distance; fwd = forward rank; bwd = backward rank; avg= average rank.55Parameters Lemma Wordform < 10 < 50 < 100 Avg correctFreqass1 1 63 279 450 1733Freqfwd0 0 32 219 395 7575Freqbwd358 292 789 1124 1247 1974Freqavg277 191 515 690 793 7251Simple-llass23 18 196 618 878 1259Simple-llfwd271 196 605 789 842 14177Simple-llbwd369 296 737 1002 1135 2848Simple-llavg346 251 636 798 845 13760Cond.probass7 6 209 588 806 1234Cond.probfwd0 0 32 219 395 7575Cond.probbwd284 230 659 974 1109 2318Cond.probavg201 137 462 711 851 7230Table 3: First Order Models ?
Asymmetric Window: 3 words to the left of the node ?
Training DataParameters Lemma Wordform < 10 < 50 < 100 Avg correctFreqass1 1 63 279 450 1733Freqfwd0 0 32 219 395 7575Freqbwd358 292 789 1124 1247 1974Freqavg277 191 515 690 793 7251Simple-llass25 22 220 643 891 1168Simple-llfwd321 250 708 895 936 12244Simple-llbwd507 424 884 1142 1246 2223Simple-llavg402 314 740 901 939 11868Cond.probass26 20 279 665 864 1282Cond.probfwd0 0 59 298 498 7543Cond.probbwd381 319 791 1094 1201 1981Cond.probavg278 209 535 800 922 7214Table 4: First Order Models ?
Asymmetric Window: 3 words to the right of the node ?
Training DataParameters Lemma Wordform < 10 < 50 < 100 Avg correct2dist264 208 686 1077 1224 9362fwd127 83 380 703 849 15602bwd73 56 275 584 720 35242avg157 106 436 750 911 15074dist255 200 665 1037 1195 9974fwd108 73 338 651 824 17504bwd77 57 254 545 694 38434avg129 87 397 710 862 169416dist206 158 546 910 1062 143316fwd63 40 252 512 667 248116bwd49 37 188 449 581 494916avg79 56 282 560 713 2416Table 5: Second order models ?
Training dataConsidering the good results from collocation-based models, we would have expected a better perfor-mance from larger windows, traditionally considered to be more sensitive to syntagmatic relations.
Asignificant difference between first-order and second-order models is the fact that neighbor rank worksless well than the distance between vectors, while collocate rank outperformed the association scores.56Model Lemma Wordform < 10 < 50 < 100 Avg correctfirst-order 572 490 1010 1303 1408 1366second-order 304 246 734 1119 1256 569Table 6: Best models (first order and second order) ?
Performance on test dataModel Training-Lemma Training-Inflected Test-Lemma Test-Inflectedfirst-order 27.7% (555) 26.9% (538) 28.6% (572) 27.7% (554)second-order 13.2% (264) 12.0% (241) 15.0% (304) 14.0% (279)Table 7: Performance (% accuracy and number of correct responses) of the best first-order and second-order model on training vs. test dataset (lemmatized response vs. response with restored inflection)The observation for second-order models contrasts with previous work showing that rank consistentlyoutperforms distance in modeling priming effects (Lapesa and Evert, 2013; Lapesa et al., to appear) andalso in standard tasks such as prediction of similarity ratings and noun clustering (Lapesa and Evert, un-der review).
Among the standard tasks, the only case in which the use of neighbor rank did not producesignificant improvements with respect to vector distance was the TOEFL multiple-choice synonymy task.Despite clear differences, the TOEFL task and the reverse association task share the property that theyinvolve multiple stimuli.
The results presented in this paper, together with those achieved on the TOEFLtask, seem to suggest that a better strategy for the use of neighbor rank needs to be developed whenmultiple stimuli are involved.6 Conclusions and Future WorkThe results of the evaluation reported in this paper confirmed the tendencies identified in previous studies:first-order models, based on direct co-occurrence, outperform second-order models, based on distribu-tional similarity.
We consider the experimental results described in this paper as a first exploration intothe dynamics of the reverse association task, and we believe that our systematic evaluation of first- andsecond-order models represents a good starting point for future work, which targets improvements ofNaDiR at many levels.The first point of improvement concerns the size of the vocabulary.
We aim at finding a more op-timal cutoff on the training data, for example by implementing a frequency bias similar to Wettler etal.
(2005).
We are confident that NaDiR will significantly benefit from a smaller range of potentialresponses (compared to the 155,811 lemmatized candidate words in the current version).We are also conducting experiments using log ranks instead of plain ranks: since we compute an arith-metic mean of the rank values, a single very high rank (from a poorly matched stimulus) will dominatethe average.
We therefore assume that log ranks will improve results and make NaDiR?s responses morerobust.An interesting research direction targets the integration of first- and second-order statistics in the pro-cess of response generation.
The evaluation results reported in this paper revealed that a very smallcontext window achieves the best performance for second-order models: as widely acknowledged in theliterature (Sahlgren, 2006; Lapesa et al., to appear), smaller context windows highlight paradigmaticrelations.
First-order models, on the other hand, highlight syntagmatic relations (Rapp, 2002).
The bestsecond-order and first-order models from the evaluation reported in this paper are likely to focus on dif-ferent types of relations between response and stimulus words: this leads us to believe that an integrationof the two sources may produce improvements in NaDiR?s performance.At a general level, we plan to make more elaborate use of the training data.
In the experimentspresented in this paper, training data were used to set a frequency threshold for potential responses, trainthe part-of-speech classifiers, and find the best configuration for first- and second-order models.A possible new application of NaDiR is the modeling of datasets containing semantic norms or conceptproperties, such as the McRae norms (McRae et al., 2005) or BLESS (Baroni and Lenci, 2011).
Thosedatasets are standard in DSM evaluation, and their modeling can be implemented in terms of a reverse57association task, with the additional advantage that the relations between concepts and properties in thosedatasets are labelled with property types for the McRae norms (e.g., encyclopedic, taxonomic, situated)or semantic relations (e.g., hypernymy, meronymy, event-related) for BLESS.
This allows a specificevaluation for each property type or semantic relation, which will in turn give new insights into thesemantic knowledge encoded in the different corpus-based representations (first order vs. second ordervs.
hybrid) and how model parameters affect these representations (e.g., window size in the comparisonof syntagmatic vs. paradigmatic relations).AcknowledgmentsGabriella Lapesa?s research is funded by the DFG Collaborative Research Centre SFB 732 (Universityof Stuttgart).ReferencesMarco Baroni and Alessandro Lenci.
2010.
Distributional memory: A general framework for corpus-basedsemantics.
Computational Linguistics, 36(4):1?49.Marco Baroni and Alessandro Lenci.
2011.
How we BLESSed distributional semantic evaluation.
In Proceedingsof the GEMS 2011 Workshop on GEometrical Models of Natural Language Semantics, GEMS ?11, pages 1?10.Association for Computational Linguistics.Stefan Evert.
2008.
Corpora and collocations.
In Anke L?udeling and Merja Kyt?o, editors, Corpus Linguistics.
AnInternational Handbook, chapter 58.
Mouton de Gruyter, Berlin, New York.Stefan Evert.
to appear.
Distributional semantics in R with the wordspace package.
In Proceedings of COLING2014: System Demonstrations.Thomas L. Griffiths, Mark Steyvers, and Joshua B. Tenenbaum.
2007.
Topics in semantic representation.
Psycho-logical Review, 114:211?244.Nathan Halko, Per-Gunnar Martinsson, and Joel A. Tropp.
2009.
Finding structure with randomness: Stochasticalgorithms for constructing approximate matrix decompositions.
Technical Report 2009-05, ACM, CaliforniaInstitute of Technology.Mary Hare, Michael Jones, Caroline Thomson, Sarah Kelly, and Ken McRae.
2009.
Activating event knowledge.Cognition, 111(2):151?167.G.
R. Kiss, C. Armstrong, R. Milroy, and J. Piper.
1973.
An associative thesaurus of English and its computeranalysis.
In The Computer and Literary Studies.
Edinburgh University Press.Gabriella Lapesa and Stefan Evert.
2013.
Evaluating neighbor rank and distance measures as predictors of se-mantic priming.
In Proceedings of the ACL Workshop on Cognitive Modeling and Computational Linguistics(CMCL 2013), pages 66?74.Gabriella Lapesa, Stefan Evert, and Sabine Schulte im Walde.
to appear.
Contrasting syntagmatic and paradig-matic relations: Insights from distributional semantic models.
In Proceedings of the 3rd Joint Conference onLexical and Computational Semantics (*SEM).
Dublin, Ireland, August 2014.Ken McRae, George Cree, Mark Seidenberg, and Chris McNorgan.
2005.
Semantic feature production norms fora large set of living and nonliving things.
Behavior Research Methods, 4(37):547?559.Reinhard Rapp.
2002.
The computation of word associations: Comparing syntagmatic and paradigmatic ap-proaches.
In Proceedings of the 19th International Conference on Computational Linguistics - Volume 1, pages1?7.Reinhard Rapp.
2013.
From stimulus to associations and back.
In Proceedings of the 10th Workshop on NaturalLanguage Processing and Cognitive Science.Reinhard Rapp.
2014.
Corpus-based computation of reverse associations.
In Proceedings of the Ninth Interna-tional Conference on Language Resources and Evaluation (LREC?14).Magnus Sahlgren.
2006.
The Word-Space Model: Using distributional analysis to represent syntagmatic andparadigmatic relations between words in high-dimensional vector spaces.
Ph.D. thesis, University of Stockolm.58Kevin A. Smith, David E. Huber, and Edward Vul.
2013.
Multiply-constrained semantic search in the remoteassociates test.
Cognition, 128(1):64?75.Peter D. Turney and Patrick Pantel.
2010.
From frequency to meaning: Vector space models of semantics.
Journalof Artificial Intelligence Research, 37:141?188.Manfred Wettler, Reinhard Rapp, and Peter Sedlmeier.
2005.
Free word associations correspond to contiguitiesbetween words in texts.
Journal of Quantitative Linguistics, 1(12):111?122.59
