Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 869?879,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsStatistical Machine Translation with Local Language ModelsChristof MonzInformatics Institute, University of AmsterdamP.O.
Box 94323, 1090 GH Amsterdam, The Netherlandsc.monz@uva.nlAbstractPart-of-speech language modeling is com-monly used as a component in statistical ma-chine translation systems, but there is mixedevidence that its usage leads to significant im-provements.
We argue that its limited effec-tiveness is due to the lack of lexicalization.We introduce a new approach that builds aseparate local language model for each wordand part-of-speech pair.
The resulting mod-els lead to more context-sensitive probabil-ity distributions and we also exploit the factthat different local models are used to esti-mate the language model probability of eachword during decoding.
Our approach is evalu-ated for Arabic- and Chinese-to-English trans-lation.
We show that it leads to statisticallysignificant improvements for multiple test setsand also across different genres, when com-pared against a competitive baseline and a sys-tem using a part-of-speech model.1 IntroductionLanguage models are an important component ofcurrent statistical machine translation systems.
Theyaffect the selection of phrase translation candidatesand reordering choices by estimating the probabilitythat an application of a phrase translation is a flu-ent continuation of the current translation hypoth-esis.
The size and domain of the language modelcan have a significant impact on translation quality.Brants et al (2007) have shown that each doublingof the training data from the news domain (used tobuild the language model), leads to improvements ofapproximately 0.5 BLEU points.
On the other hand,each doubling using general web data leads to im-provements of approximately 0.15 BLEU points.While large n-gram language models do leadto improved translation quality, they still lack anygeneralization beyond the surface forms (Schwenk,2007).
Consider example (1), which is a short sen-tence fragment from the MT09 Arabic-English testset, with the corresponding machine translation out-put (1.b), from a phrase-based statistical machinetranslation system, and reference translation (1.c).
(1) a.
?Yj.
??
?QJ?
?J?Am?
HAm'Q?
? ?J??
g ......
??
???A?E@?b.
... the background of press statements ofcontroversial and accused him ...c. ... the background of controversial pressstatements and accused him ...Clearly, the adjective ?controversial?
should pre-cede the nouns ?press statement?, but since the AFPand Xinhua portions of the Gigaword corpus, usedto build the language model for the translation sys-tem, do not contain this surface n-gram, translationswith obviously ungrammatical constructions such as(1.b) can result.
For unseen n-grams, one would liketo model adjectives as being likely to precede nounsin English, for example.A straightforward approach to address this is toexploit the part-of-speech (POS) tags of the tar-get words during translation (Kirchhoff and Yang,2005).
Though models exploiting POS informationare not expressive enough to model long-distancedependencies, they can account for locally ungram-matical constructions such as (1.b).
Several attemptshave been made to interpolate POS language models869with surface models.
Under constrained data condi-tions, this can lead to improvements.
But once largeramounts of training data are used, the gains obtainedfrom adding POS language models decline substan-tially.
This raises the question of why POS languagemodels are not more effective.
We argue that one ofthe short-comings of previous approaches to usingPOS language models is that these models are es-timated globally, not lexically anchored, and hencerather context insensitive.In this paper, we introduce a novel approach thatbuilds and uses individual, local POS language mod-els for each word in the vocabulary.
Our experimentsshow that it leads to statistically significant improve-ments over a competitive baseline, using lexical-ized reordering and a sizable 5-gram word languagemodel, as well as a standard 7-gram POS languagemodel approach.2 Part-of-Speech Language Models2.1 BackgroundTypically, POS language models are used like word-based language models.
N-grams are extracted froma POS-tagged corpus and an n-gram language modelis built from that.
While word-based models esti-mate the probability of a string ofmwords by Equa-tion 2, POS-based models estimate the probability ofstring of m POS tags by Equation 3.p(wm1 ) ?m?i=1p(wi|wi?1i?n+1) (2)p(tm1 ) ?m?i=1p(ti|ti?1i?n+1) (3)where, n is the order of the language model, and wjirefers to the sub-sequence of words (or tags) frompositions i to j.Word language models can be built directly fromlarge text corpora, such as LDC?s Gigaword corpus,but POS models require texts that are annotated withPOS tags.
Ideally, one would use manually anno-tated corpora such as the Penn Treebank (Marcus etal., 1993), but since those tend to be small, most ap-proaches rely on larger corpora which have been au-tomatically annotated by a POS tagger or a parser(Koehn et al, 2008).
Though automated annotationinevitably contains errors, it is assumed that this isameliorated by the increased size of annotated data.The event space of a language models is of size|V |n, where V is the vocabulary, and n is the orderof the language model.
The vocabulary of POS mod-els, (typically ranging between 40 and 100 tags), ismuch smaller than the vocabulary of a word model,which can easily approach a million words.
Nev-ertheless, most POS language modeling approachesapply some form of smoothing to account for unseenevents (Bonneau-Maynard et al, 2007).To deploy POS language models in machinetranslation, translation candidates need to be anno-tated with POS tags.
Each target phrase e?
in a phrasepair (f?
, e?)
can be associated with a number of POStag sequences t?e?.
Heeman (1998) shows that usingthe joint probability leads to improved perplexity forPOS models.
For machine translation one can sumover all possible tag sequences, as in Equation 4.p(e|f) = arg maxe?tp(e, t|f) (4)Summing over all possible tag sequences has the dis-advantage that it requires one to keep this informa-tion during decoding.
Below, we opt for an approxi-mate solution, where each target phrase is annotatedwith the most likely POS tag sequence given thesource and target phrase: t?e?
= arg maxt?
p(t?|e?, f?
).2.2 Effectiveness of POS Language ModelsReported results on the effectiveness of POS lan-guage models for machine translation are mixed, inparticular when translating into languages that arenot morphologically rich, such as English.
Whilethey rarely seem to hurt translation quality, theredoes not seem to be a clear consensus that they sig-nificantly improve quality either.Koehn and Hoang (2007) have reported an in-crease of 0.86 BLEU points for German-to-Englishtranslation for small training data.
After relaxingphrase-matching to include lemma and morpholog-ical information on the source side, POS languagemodels lead to a decrease of -0.42 BLEU points.
Su-pertagging encapsulates more contextual informa-tion than POS tags and Birch et al (2007) reportimprovements when comparing a supertag languagemodel to a baseline using a word language model870only.
Once the baseline incorporates lexicalized dis-tortion (Tillmann, 2004; Koehn et al, 2005), theseimprovements disappear.
Factored language mod-els have not resulted in significant improvements ei-ther.
Kirchhoff and Yang (2005) report slight im-provements when re-ranking the n-best lists of theirdecoder, which word tri-grams.
But these improve-ments are less than those gained by re-ranking then-best lists with a 4-gram word language model.The impact of POS language models dependsamong other things on the size of the parallel cor-pus, the size and order of the word language model,and whether lexicalized distortion models are used.To gauge the potential effectiveness of POS lan-guage models without taking into consideration allthese factors, we isolate the contribution of the lan-guage model by simulating machine translation out-put using English data only (Al-Onaizan and Pap-ineni, 2006; Post and Gildea, 2008).
Taking a setof POS-tagged reference translations of the MT04Arabic-to-English test set, each English sentence israndomly chunked into n-grams of average lengththree.
The chunks of each sentence, with their cor-responding POS tags, are randomly reordered.
Thisis repeated 500 times for each sentence in the testset.
The smoothed sentence BLEU score (ignor-ing brevity penalty) is computed for each reorderedsentence with respect to all reference translations.The higher the BLEU score, the more well-formedthe reordering is.
As each reordered sentence onlycontains words from at least one of the referencetranslations, the uni-gram precision is always 1.0.The language model probability is then computedfor each reordering.
Table 1 shows the average cor-relations between language model probabilities andBLEU scores.We can see that the surface language model corre-lates moderately well with BLEU, explaining about49% (r2 = 0.49) of the variation, whereas the POSlanguage model does not correlate with BLEU atall.1 On the other hand, local language models alone(as introduced in Section 3) correlate with BLEUonly slightly worse than surface models.
The high-est correlation is seen when they are interpolatedwith word models.
The BLEU scores in Table 11Interpolating both models does not lead to further correla-tion improvements.LM Kendall?s ?
Pearson r BLEU[%]wordLM 0.53 0.71 80.20POS 7gLM 0.01 0.01 48.44locLM 0.45 0.62 76.03?wordLM+(1??
)locLM 0.54 0.73 80.98(?
= 0.92)Table 1: Correlation between randomly permuted Englishreference translations and BLEU.are computed using the 1-best sentences after re-ranking.
These system-agnostic correlation resultslook promising for our local models and the end-to-end translation results in Section 5 confirm theseinitial findings.3 Local Language ModelsIn this section, we introduce a novel approach to lan-guage modeling that is more context-sensitive thanstandard POS language models.
Instead of using oneglobal POS language model that is built by using allof a mono-lingual corpus in the target language, webuild individual models, or local models, for eachword-POS pair using the POS tags surrounding eachoccurrence of that pair.
This adds an aspect of lex-icalization that is entirely absent in previous POSlanguage models.
The effect is that the resulting n-gram probability distributions of each local modelare more biased towards the contextual constraintsof each individual word-POS pair.
This is similar tothe idea of cached language models (Kuhn, 1988),but more fine-grained and with a tighter integrationof POS and lexical information.3.1 Definition of Local Language ModelsEach conditional probability of order n in a localmodel for the word-POS pair w : t is of the form:pw:t(tn, pn|t1 :p1, .
.
.
, tn?1 :pn?1)where ti refers to POS tags and pi to positions rel-ative to an occurrence of the pair (w : t).
For ex-ample, consider the sentence fragment in Figure 1.The conditional local n-gram probabilities (a?d) aregenerated from the occurrence of the word told withPOS tag VBD.
Probability (c) in Figure 1 estimatesthat a word with POS tag NN occurs two positionsto the right of told, given the n-gram history that anoun occurs to its left and a determiner to its right.871position .
.
.
11 12 13 14 15 16 17 .
.
.relativeposition .
.
.
-3 -2 -1 0 +1 +2 +3 .
.
.word .
.
.
the new mayor told the reporter to .
.
.POS .
.
.
DT JJ NN VBD DT NN TO .
.
.
(a) ptold:VBD(NN:-1|DT:-3 JJ:-2) (c) ptold:VBD(NN:+2|NN:-1 DT:+1)(b) ptold:VBD(DT:+1|JJ:-2 NN:-1) (d) ptold:VBD(TO:+3|DT:+1 NN:+2)Figure 1: Sentence fragment with the tri-gram probabilities (a?d) linked to told.For each local model we use a sliding window con-sidering all n-grams of length n starting n words tothe left and ending n words to the right of an occur-rence of the word-POS pair of the model at hand.All local model probabilities are smoothed us-ing Witten-Bell smoothing and interpolation.2 POStags are annotated with positional information todistinguish between lower-order estimates such asptold :VBD(NN+2) and ptold :VBD(NN+3) both ofwhich can arise when backing off during smooth-ing.
Without positional information, ptold :VBD(NN)only estimates the probability of the tag NN occur-ring within the proximity of told.3A local model of order n contains the conditionalprobabilities for words occurring at relative posi-tions -1, +1, .
.
.
+n.
Therefore the probability ofa word occurrence is estimated by all local mod-els covering this word?s position.
Figure 2 showsschematically how overlapping n-gram probabilitiesinteract.
E.g., the probability of word wi+2 is basedon the probability of the local model for wi+1, wi,wi?1, and wi?2 (the last two are not shown in Fig-ure 2 for space reasons).
Formally, the conditionalprobability of a word-POS pair, given its word andPOS tag history is defined in Equation 5.p(wi, ti |wi?1i?n+1, ti?1i?n+1) =pwi:ti(ti?1 : -1 | ?ti?n : -n, .
.
.
, ti?2 : -2?
)?n?1?j=0pwi?n+j :ti?n+j (ti :n?j |Hi,n[j, ?])
(5)2The smaller event space of local models often leads to in-complete counts-of-counts, preventing the use of Kneser-Neysmoothing (Chen and Goodman, 1999).3Despite the notational similarities, our approach should notbe confused with projected POS models, which use source sidePOS tags to model reordering (Och et al, 2004)..........w1wi-3wi-2wi-1wiwi+1wi+2wi+3...wmn-gram historypredicted wordposition of currentlocal modelFigure 2: Schema of overlapping local language modelapplications.where Hi,n is an n?n matrix specifying the historyof the word at position i.
Each row j of Hi,n rep-resents the history of the conditional probability be-longing to the local model associated with positioni?n+j.
Each entry Hi,n[j, k] is defined as follows:Hi,n[j, k] ={ti?n+k :k ?
j if j 6= k otherwisewhere ti?n+k is the POS tag at position i ?
n + kand k ?
j is the relative position with respect to thediagonal of Hi,n, i.e., the position of the local lan-guage model corresponding to row j. Hi,n[j, ?]
is thejth row vector from which the jth entry (the emptyelement) has been removed.
For instance, given theexample in Figure 1, H14,3 isH14,3 =?
? JJ:+1 NN:+2DT:-1  NN:+1DT:-2 JJ:-1 ?
?For convenience we assume that the row and col-umn indices are 0-based, i.e., the upper-left entry ofa matrix is referred to by Hi,n[0, 0].
In this example,H14,3[1, ?]
= ?DT:-1, NN:+1?.872position 0 1 2 3 4 5 6token <s> cuba frees more dissidents .
</s>POS tag <s> NNP VBZ JJR NNS .
</s>p(cuba, NNP|w00, t00) = pcuba:NNP(<s>:-1) ?
p<s>:<s>(NNP:+1)p(frees, VBZ|w10, t10) = pfrees:VBZ(NNP:-1|<s>:-2) ?
p<s>:<s>(VBZ:+2|NNP:+1)?pcuba:NNP(VBZ:+1|<s>:-1)p(more, JJR|w20, t20) = pmore:JJR(VBZ:-1|NNP:-3 VBZ:-2) ?
p<s>:<s>(JJR:+3|NNP:+1 VBZ:+2)?pcuba:NNP(JJR:+2|<s>:-1 VBZ:+1) ?
pfrees:VBZ(JJR:+1|<s>:-2 NNP:-1)p(dissidents, NNS|w31, t31) = pdissidents:NNS(JJR:-1|NNP:-3 VBZ:-2) ?
pcuba:NNP(NNS:+3|VBZ:+1 JJR:+2)?pfrees:VBZ(NNS:+2|NNP:-1 JJR:+1) ?
pmore:JJR(NNS:+1|NNP:-2 VBZ:-1)p(.
, .|w42, t42) = p.:.
(NNS:-1|VBZ:-3 JJR:-2) ?
pfrees:VBZ(.
:+3|JJR:+1 NNS:+2)?pmore:JJR(.
:+2|VBZ:-1 NNS:+1) ?
pdissidents:NNS(.
:+1|VBZ:-2 JJR:-1)p(</s>, </s>|w53, t53) = p</s>:</s>(.
:-1|JJR:-3 NNS:-2) ?
pmore:JJR(</s>:+3|NNS:+1 .
:+2)?pdissidents:NNS(</s>:+2|JJR:-1 .
:+1) ?
p.:.
(</s>:+1|JJR:-2 NNS:-1)Figure 3: Language model probability computation for the sentence ?Cuba frees more dissidents.?
using our locallanguage modeling approach.The example in Figure 3 shows word-by-wordhow tri-gram local language models are used tocompute the probability of a whole sentence.Our local language model approach also bearssome resemblance to statistical approaches to mod-eling subcategorization frames (Manning, 1993).While our approach is more general by consideringall words and not just focusing on verbal subcatego-rization frames, it is also more shallow in the sensethat only part-of-speech categories are consideredwhich does not model any contextual relationshipson the phrase level.3.2 Building Local Language ModelsTo build the local language models, we use theSRILM toolkit (Stolcke, 2002), which is commonlyapplied in speech recognition and statistical machinetranslation.
While SRILM collects n-gram statisticsfrom all n-grams occurring in a corpus to build asingle global language model, we build a languagemodel for each word-POS pair only using the n-grams within the proximity of occurrences for thatword-POS pair in a POS-tagged corpus.
This resultsin separate n-gram count files, which are then pro-cessed by SRILM to build the individual languagemodels.4 Charniak?s parser (Charniak, 2000) is usedto POS tag the corpus.4The pre-processing scripts are available at http://www.science.uva.nl/?christof/locLM/.3.3 Decoder IntegrationSeveral approaches that integrate POS languagemodels have focused on n-best list re-ranking only(Hasan et al, 2006; Wang et al, 2007).
Often thisis due to the computational (and implementational)complexities of integrating more complex languagemodels with the decoder, although it is expected thata tighter integration with the decoder itself leads tobetter improvements than n-best list re-ranking.Integrating our local language modeling approachwith a decoder is straightforward.
Our baselinedecoder already uses SRILM?s API for computingword language model probabilities.
Since SRILMsupports arbitrarily many language models, locallanguage models can be added using the same func-tionalities of SRILM?s API.
For the experiments dis-cussed in Section 4, we add about 150,000 locallanguage models to the word model.
All local lan-guage model probabilities are coupled with the samefeature weight.
Potentially, improvements could begained from using separate weights for individuallocal models, but this would require an optimiza-tion procedure such as MIRA (Chiang et al, 2009),which can handle a larger number of features.During decoding no POS tagging ambiguities areresolved.
Each target phrase is associated with itsmost likely POS tag sequence, given the source andtarget side of the phrase pair; see Section 2.1.8734 Experimental SetupThree approaches are compared in our experiments:the baseline system is a phrase-based statistical ma-chine translation system (Koehn et al, 2003), verysimilar to Moses (Koehn et al, 2007), using a word-based 5-gram language model.
The second approachextends the baseline by including a 7-gram POS-based language model.
The third approach repre-sents the work described in this paper, extending thebaseline by including 4-gram local language models.Translation quality is evaluated for two languagepairs: Arabic-to-English and Chinese-to-English.NIST?s MT-Eval test sets are used for both pairs.Only resources allowed under NIST?s constraineddata conditions are used to train the language, trans-lation, and lexicalized distortion models.To see whether our local language models resultin improvements over a competitive baseline, wedesigned the baseline to use a large 5-gram wordlanguage model and lexicalized distortion model-ing, both of which are known to cancel-out improve-ments gained from POS language models (Birch etal., 2007; Kirchhoff and Yang, 2005).
The 5-gramword language model is trained on the Xinhua andAFP sections of the Gigaword corpus (3rd edition,LDC2007T40) and the target side of the bitext.
Weremoved from the training data all documents re-leased during the periods that overlap with the pub-lication dates of the documents included in our de-velopment or test data sets.
In total, 630 million to-kens were used to build the word language model.The language model was trained using SRILM withmodified Kneser-Ney smoothing and interpolation(Chen and Goodman, 1999).
It is common practicenot to include higher-order n-grams that occur fewerthan a predefined number of times.
Here, we appliedrather conservative cut-offs, by ignoring 3-, 4-, and5-grams that occurred only once.
The 7-gram POSand 4-gram local language models were both trainedon the POS tagged English side of the bitext and10M sentences from Gigaword?s Xinhua and AFPsections.The data for building the translation modelswere primarily drawn from the parallel news re-sources distributed by the Linguistic Data Consor-tium (LDC).5 The Arabic-English bitext consists5LDC catalog numbers for Arabic-English: LDC2004E72,of 11.4M source and 12.6M target tokens, and theChinese-English bitext of 10.6M source and 12.3Mtarget tokens.
Word alignment was performed run-ning GIZA++ in both directions and generating thesymmetric alignments using the ?grow-diag-final-and?
heuristics.All three approaches, including the baseline, uselexicalized distortion, distinguishing between mono-tone, swap, and discontinuous reordering, all withrespect to the previous and next phrase (Koehn etal., 2005).
The distortion limit is set to 5 for Arabic-to-English, and 6 for Chinese-to-English.
For eachsource phrase the top 30 translations are considered.For tuning and testing we use NIST?s official MT-Eval test sets.
MT04 was used as the developmentset for both language pairs.
Testing was carried outon MT05 to MT09 for Arabic-English and MT05to MT08 for Chinese-English.
NIST did not re-lease a new Chinese-English test set for MT-Eval2009.
Parameter tuning of the decoder was donewith minimum error rate training (MERT) (Och,2003), adapted to BLEU maximization.As evaluation metrics we used NIST?s adapta-tion of BLEU-4 (Papineni et al, 2001), version 13a,where the brevity penalty is based on the referencetranslation with the closest length, and translationerror rate (TER) version 0.7.25 (Snover et al, 2006).All results reported here are case-insensitive.
TERscores are shown as 1-TER.To see whether the differences between the ap-proaches we compared in our experiments are sta-tistically significant, we apply approximate random-ization (Noreen, 1989); Riezler and Maxwell (2005)have shown that approximate randomization is lesssensitive to Type-I errors, i.e., less likely to falselyreject the null hypothesis, than bootstrap resampling(Koehn, 2004) in the context of machine translation.5 Results and AnalysisThe Arabic-to-English results are shown in Ta-ble 2, and the Chinese-to-English results in Ta-ble 3.
All results are subdivided by genre followingNIST?s genre classification.
Note that MT06 con-LDC2004T17, LDC2004T18, LDC2005E46, LDC2005E83,LDC2006E25, LDC2006E34, LDC2006E85, LDC2006E92,and LDC2007T08.
For Chinese-English: LDC2002E18,LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06,LDC2006E34, LDC2006E85, and LDC2006E92.874systems and MT04 MT05 MT06 MT08 MT09 MT05?09improvements tune NW NW WB ALL NW WB ALL NW WB ALL NW WB ALLBLEU[%]1a wordLM 51.90 53.83 46.76 34.69 43.41 48.77 33.26 42.37 52.97 34.25 44.34 50.51 34.00 45.632a +posLM 51.92 54.29 47.02 34.44 43.51 48.81 33.30 42.31 53.52 34.04 44.36 50.89 33.87 45.703a > wordLM +0.02 +0.46N +0.26 ?0.25 +0.10 +0.04 +0.04 ?0.06 +0.55N ?0.21 +0.02 +0.38N ?0.13 +0.074a +locLM 52.65 55.08 47.24 35.17 43.88 49.61 33.67 42.92 54.39 34.40 44.82 51.57 34.33 46.225a > wordLM +0.75N +1.25N +0.48N +0.48M +0.47N +0.84N +0.41 +0.55N +1.42N +0.15 +0.48N +1.06N +0.33M +0.59N6a > +posLM +0.73N +0.79N +0.22 +0.73N +0.37M +0.80N +0.37 +0.61N +0.87N +0.36 +0.46N +0.68N +0.46N +0.52N1-TER[%]1b wordLM 58.32 59.04 54.27 45.62 51.68 55.59 44.41 50.69 59.90 46.43 53.03 56.94 45.49 53.132b +posLM 58.54 59.72 54.90 45.67 52.14 55.75 44.64 50.89 60.49 46.72 53.47 57.46 45.70 53.553b > wordLM +0.22M +0.68N +0.63N +0.05 +0.46N +0.16 +0.23 +0.20M +0.59N +0.29M +0.44N +0.52N +0.21N +0.42N4b +locLM 58.95 60.06 54.88 45.62 52.11 56.42 44.91 51.38 60.91 46.84 53.74 57.79 45.83 53.815b > wordLM +0.63N +1.02N +0.61N +0.00 +0.43N +0.83N +0.50N +0.69N +1.01N +0.41M +0.71N +0.85N +0.34N +0.68N6b > +posLM +0.41N +0.34M ?0.02 ?0.05 ?0.03 +0.67N +0.27 +0.49N +0.42M +0.12 +0.27M +0.33N +0.13 +0.26N# segments 1,353 1,056 1,033 764 1,797 813 547 1,360 586 727 1,313 3,488 2,038 5,526Table 2: Results for Arabic-to-English translation.
Comparison of our approach (+locLM, rows 4a/b) to the baselineusing a word language model (wordLM, rows 1a/b) and a competing approach using a POS-based language model(+posLM, rows 2a/b).
Results are presented using BLEU[%] (rows 1a?6a) and 1-TER[%] (rows 1b?6b) and brokendown by genre: NW=newswire, WB=web, and ALL=NW?WB.
Rows 3a/b, 5a/b, and 6a/b show the relative improve-ments over the system mentioned to the right of the > sign.
Statistically significant improvements/declines (usingapproximate randomization) at the p < .01 level are marked N/ H and M/ O at the p < .05 level.tains the genres ?broadcast news?
and ?newsgroup?.In both tables, the former has been classified under?newswire?
and the latter under ?web?.The first approach is the baseline system?wordLM?
(rows 1a/b in Tables 2 and 3), which usesa 5-gram word-based language model.
The next ap-proach ?+posLM?
extends the baseline by adding a7-gram POS language model (rows 2a/b in both ta-bles).
Rows 3a/b show the relative improvementsover the baseline.
The third approach ?+locLM?
(rows 4a/b) uses local language models in additionto the baseline?s word-based model.
Note that +lo-cLM does not use the 7-gram POS language modelas well.
Rows 5a/b show the relative improvementsof the local modeling approach over the baseline androws 6a/b the improvements over the approach usinga POS language model.Let us first take a closer look at the Arabic-to-English results in Table 2.
The approach using aPOS language model results in statistically signifi-cant improvements for only one test set (MT05) andthe newswire documents of MT09.
The average im-provements across all sets and genres are negligible(+0.07 BLEU).
Our local language modeling ap-proach achieves the highest BLEU scores for all testsets and across all genres.
In particular, the improve-ments of +1.06 BLEU for newswire documents aresubstantial.
With the exception of MT08-WB andMT09-WB all BLEU improvements over the base-line are statistically significant.When evaluating with 1-TER, local languagemodeling also achieves the best results, with the ex-ception of MT06, where the POS language modelapproach performs slightly better.Turning to the Chinese-English results in Table 3,we see similar improvements in BLEU.
The im-provements of using a POS language model are neg-ligible (+0.04 BLEU).
Here as well, local languagemodeling leads to the best results, with substantialimprovements of +0.88 BLEU for web documents.The major difference between Arabic-English andChinese-English is the discrepancy between BLEUscore improvements and decreases in 1-TER.
Whilewe cannot explain this discrepancy, it is worth not-ing that similar discrepancies between BLEU andTER and Arabic-to-English and Chinese-to-Englishtranslation can be found in the literature.
The resultsdescribed in Shen et al (2009) show a strong cor-relation between BLEU and 1-TER improvements66Shen et al (2009) report TER rather than 1-TER scores.875systems and MT04 MT05 MT06 MT08 MT05?08improvements tune NW NW WB ALL NW WB ALL NW WB ALLBLEU[%]1a wordLM 37.32 32.55 33.33 23.40 31.16 28.67 17.57 24.03 31.93 19.82 29.302a +posLM 37.32 32.47 33.13 23.67 31.06 28.63 18.46 24.35 31.82 20.46 29.343a > wordLM +0.00 ?0.08 ?0.20 +0.27 ?0.10 ?0.04 +0.89N +0.32 ?0.11 +0.64N +0.044a +locLM 38.15 33.05 33.33 24.62 31.42 29.52 18.24 24.79 32.36 20.70 29.825a > wordLM +0.83N +0.50M +0.00 +1.22N +0.26 +0.85N +0.67M +0.76N +0.43N +0.88N +0.52N6a > +posLM +0.83N +0.58N +0.20 +0.95N +0.36M +0.89N ?0.22 +0.44M +0.54N +0.24 +0.48N1-TER[%]1b wordLM 42.81 40.73 42.99 39.42 42.15 40.42 36.77 38.78 41.53 37.77 40.632b +posLM 42.50 40.60 42.75 38.87 41.84 39.76 36.75 38.41 41.23 37.55 40.343b > wordLM ?0.31O ?0.13 ?0.24 ?0.55 ?0.31O ?0.66H ?0.02 ?0.37O ?0.30H ?0.22 ?0.29H4b +locLM 42.77 40.49 42.62 39.40 41.86 40.00 36.11 38.26 41.20 37.35 40.275b > wordLM ?0.04 ?0.24 ?0.37 ?0.02 ?0.29 ?0.42 ?0.66H ?0.52H ?0.33O ?0.42O ?0.36H6b > posLM +0.27 ?0.11 ?0.13 +0.53 +0.02 +0.24 ?0.64H ?0.15 ?0.03 ?0.20 ?0.07# segments 1,788 1,082 1,181 483 1,664 691 666 1,357 2,954 1,149 4,103Table 3: Comparison of our system for Chinese-to-English translation.
See Table 2 for details on notation.for Arabic-to-English on the MT06 and MT08 sets,but for Chinese-to-English the correlation seems tobe much weaker and BLEU improvements of +0.75can correspond to decreases of up to -0.80 in 1-TER.One of the motivations of using POS languagemodels in general, and local language models in ourcase, is to improve the fluency of translations, whichshould be reflected in increased precision for higher-order n-grams.
Table 4 shows that this is the casewhen comparing local modeling to both word andPOS language models for Arabic-to-English trans-lation.
The same trend, but to a somewhat weakerdegree can be observed for Chinese-to-English.Prec-1 Prec-2 Prec-3 Prec-4 BPArabic-English (MT05?09)wordLM 81.38 54.51 38.10 26.99 0.987+posLM 81.81 54.82 38.34 27.17 0.983+locLM 81.90 55.35 39.01 27.86 0.981Chinese-English (MT05?08)wordLM 75.03 40.56 22.55 12.93 0.955+posLM 74.81 40.30 22.41 12.83 0.962+locLM 74.24 40.70 22.83 13.19 0.966Table 4: BLEU n-gram precision (1?n?4) and BrevityPenalty (BP) scores over all test sets.The effectiveness of a POS language model of-ten diminishes with improved translation quality ofthe base system to which it is added.
Naturally,we are interested in the extent that this diminish-ing effect also holds for our local language mod-els.
A full experimental setup, varying all relevantfactors, such as language, translation, and distor-tion model size, and the various meta-parameters,is beyond the scope of this paper.
Nevertheless,we can gauge this by taking a closer look at thedistribution of improvements within our experi-ments.
Figure 4 shows performance improvementsin document-level BLEU for both language pairs.The document-level BLEU score for the baselinesystem is plotted on the x-axis and improvements areplotted on the y-axis.
The dotted line is the linearfit (using least square regression).
If the effective-ness of either added model (POS or local) dimin-ishes with increasing translation quality, we wouldexpect a declining regression line.
This is not thecase for Arabic-to-English translation.
Relative im-provements for both added models increase as thetranslation quality of the baseline increases.
Theslope of both regression fits is almost identical, butthe y-intercept is larger for our local modeling ap-proach.
Note that the small slope is also due to dif-ference in scale between full BLEU scores and rel-ative improvements.
We can observe the oppositefor Chinese-to-English translation, where the slopeis negative.
Both models seem to help more fordocuments with lower baseline translation quality.For the POS model, the regression line intersectswith the neutral line (?0 improvement) at around31 BLEU, which is close to the average BLEU scoreand in line with its negligible improvements (see Ta-876-8-6-4-2024681010  20  30  40  50  60  70document BLEUimprovementsbaseline document BLEU scoredocumentleast square fitneutral-8-6-4-2024681010  20  30  40  50  60  70document BLEUimprovementsbaseline document BLEU scoredocumentleast square fitneutralArabic-English: +posLM > wordLM Arabic-English: +locLM > wordLM-8-6-4-2024681010  20  30  40  50document BLEUimprovementsbaseline document BLEU scoredocumentleast square fitneutral-8-6-4-2024681010  20  30  40  50document BLEUimprovementsbaseline document BLEU scoredocumentleast square fitneutralChinese-English: +posLM > wordLM Chinese-English: +locLM > wordLMFigure 4: Correlation between baseline BLEU scores for individual documents and the relative, absolute improvementsachieved by +posLM (left) and +locLM (right).
BLEU scores (and improvements) are computed at the document level.ble 3).
For the local language model, the regres-sion line intersects with the neutral line at about40 BLEU, suggesting that until translation qualityimproves substantially, local language models couldstill have a positive impact.6 Related WorkThe main goal of this paper is to show that by tyingPOS language models to lexical items, we get moreaccurate distributions for specific words.
The workon factored language models (Bilmes and Kirchhoff,2003) is related to our work to the extent that it alsomixes POS tags with lexical information, albeit ina very different manner.
Factored language modelsuse more general representations, such as POS tagsor stems, only during back-off.
Kirchhoff and Yang(2005) applied factored language models to machinetranslation but the improvements were negligible.Collins et al (2005) proposed a discriminativelanguage modeling approach that uses mixtures ofPOS and surface information and showed that itleads to a reduction in speech recognition word er-ror rates.
On the other hand, their approach seemsmore suited for n-best list re-ranking and it is notclear whether those improvements carry over to ma-chine translation.
Li and Khudanpur (2008) adaptedthis discriminative approach to machine translationre-ranking but used surface forms only.Wang et al (2007) and Zheng et al (2008)use elaborately enriched representations, called su-per abstract role values (Wang and Harper, 2002),which capture contextual dependencies using lexi-cal categories, role labels, and dependency grammarstructures.
So far their approach has been limited tore-ranking n-best lists only.7 ConclusionThough POS language models do not lead to signif-icant improvements over a competitive baseline, wehave shown that a competitive phrase-based baselinesystem can benefit from using POS information bybuilding lexically anchored local models.
Our localmodel approach does not only lead to more context-specific probability distributions, but also takes ad-877vantage of the language model probability of eachword being based on all surrounding local models.The evaluations for Arabic- and Chinese-to-Englishshow that local models lead to statistically signifi-cant improvements across different test sets and gen-res.
Correlating the translation quality of the base-line with the improvements that result from addinglocal models, further suggests that these improve-ments are sustainable and should carry over to im-proved baseline systems.AcknowledgmentsThis research was funded in part by the EuropeanCommission through the CoSyne project FP7-ICT-4-248531, the European Commission?s ICT Pol-icy Support Program as part of the Competitive-ness and Innovation Framework Program, CIP ICT-PSP under grant agreement nr.
250430, and thePROMISE Network of Excellence co-funded by the7th Framework Programme of the European Com-mission, grant agreement no.
258191.ReferencesYaser Al-Onaizan and Kishore Papineni.
2006.
Distor-tion models for statistical machine translation.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and the 44th annual meeting ofthe Association for Computational Linguistics, pages529?536.Jeff A. Bilmes and Katrin Kirchhoff.
2003.
Factoredlanguage models and generalized parallel backoff.
InProceedings of the the North American Chapter of theAssociation for Computational Linguistics on HumanLanguage Technology, pages 4?6.Alexandra Birch, Miles Osborne, and Philipp Koehn.2007.
CCG supertags in factored statistical machinetranslation.
In Proceedings of the Second Workshopon Statistical Machine Translation, pages 9?16.He?le`ne Bonneau-Maynard, Alexandre Allauzen, DanielDe?chelotte, and Holger Schwenk.
2007.
Combiningmorphosyntactic enriched representation with n-bestreranking in statistical translation.
In Proceedings ofthe NAACL-HLT Workshop on Syntax and Structure inStatistical Translation, pages 65?71.Thorsten Brants, Ashok C. Popat, Peng Xu, Franz J. Och,and Jeffrey Dean.
2007.
Large language models inmachine translation.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 858?867.Eugene Charniak.
2000.
A maximum-entropy-inspiredparser.
In Proceedings of the 1st North Americanchapter of the Association for Computational Linguis-tics conference, pages 132?139.Stanley F. Chen and Joshua Goodman.
1999.
An empiri-cal study of smoothing techniques for language model-ing.
Computer Speech and Language, 13(4):359?393.David Chiang, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine transla-tion.
In Proceedings of the North American Chapter ofthe Association for Computational Linguistics, pages218?226.Michael Collins, Brian Roark, and Murat Saraclar.2005.
Discriminative syntactic language modeling forspeech recognition.
In Proceedings of the 43rd An-nual Meeting on Association for Computational Lin-guistics, pages 507?514.Sas?a Hasan, Oliver Bender, and Hermann Ney.
2006.Reranking translation hypotheses using structuralproperties.
In Proceedings of the EACL Workshop onLearning Structured Information in Natural LanguageApplications, pages 41?48.Peter Heeman.
1998.
POS tagging versus classes in lan-guage modeling.
In Proceedings of the Sixth Work-shop on Very Large Corpora, pages 179?187.Katrin Kirchhoff and Mei Yang.
2005.
Improved lan-guage modeling for statistical machine translation.
InProceedings of the ACLWorkshop on Building and Us-ing Parallel Texts, pages 125?128.Philipp Koehn and Hieu Hoang.
2007.
Factored transla-tion models.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 868?876.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Pro-ceedings of the 2003 Conference of the North Ameri-can Chapter of the Association for Computational Lin-guistics on Human Language Technology, pages 48?54.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system descriptionfor the 2005 IWSLT speech translation evaluation.
InProceedings of the International Workshop on SpokenLanguage Translation.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-878active Poster and Demonstration Sessions, pages 177?180.Philipp Koehn, Abhishek Arun, and Hieu Hoang.
2008.Towards better machine translation quality for thegerman?english language pairs.
In Proceedings of theThird Workshop on Statistical Machine Translation,pages 139?142.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing, pages 388?395.Roland Kuhn.
1988.
Speech recognition and the fre-quency of recently used words: a modified Markovmodel for natural language.
In Proceedings of the 12thconference on Computational Linguistics, pages 348?350.Zhifei Li and Sanjeev Khudanpur.
2008.
Large-scalediscriminative n-gram language models for statisti- calmachine translation.
In Proceedings of AMTA, pages133?142.Christopher D. Manning.
1993.
Automatic acquisition ofa large subcategorization dictionary from corpora.
InProceedings of the 31st Annual Meeting of the Associ-ation for Computational Linguistics, pages 235?242.Mitchell P. Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of english: the penn treebank.
Computational Lin-guistics, 19:313?330.Eric W. Noreen.
1989.
Computer Intensive Meth-ods for Testing Hypotheses.
An Introduction.
Wiley-Interscience.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, ALex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2004.
Asmorgasbord of features for statistical machine trans-lation.
In Proceedings of the 2004 Meeting of theNorth American chapter of the Association for Com-putational Linguistics, pages 161?168.Franz-Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of the41st Annual Meeting of the Association for Computa-tional Linguistics (ACL), pages 160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: A method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Computa-tional Linguistics (ACL 2001), pages 311?318.Matt Post and Daniel Gildea.
2008.
Parsers as languagemodels for statistical machine translation.
In Proceed-ings of the Eighth Conference of the Association forMachine Translation in the Americas, pages 172?181.Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significance test-ing for MT.
In Proceedings of the ACL Workshop onIntrinsic and Extrinsic Evaluation Measures for Ma-chine Translation and/or Summarization, pages 57?64.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech and Language, 21:492?518.Libin Shen, Jinxi Xu, Bing Zhang, Spyros Matsoukas,and Ralph Weischedel.
2009.
Effective use of linguis-tic and contextual information for statistical machinetranslation.
In Proceedings of Empirical Methods inNatural Language Processing, pages 72?80.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of Association for Machine Translationin the Americas, pages 223?231.Andreas Stolcke.
2002.
SRILM?an extensible lan-guage modeling toolkit.
In Proceedings of the Inter-national Conference on Spoken Language Processing,pages 901?904.Christoph Tillmann.
2004.
A unigram orientation modelfor statistical machine translation.
In Proceedings ofthe Human Language Technology and North AmericanAssociation for Computational Linguistics Conference(HLT/NAACL-04), pages 101?104.Wen Wang and Mary P. Harper.
2002.
The Super-ARV language model: investigating the effectivenessof tightly integrating multiple knowledge sources.
InProceedings of Empirical Methods in Natural Lan-guage Processing, pages 238?247.Wen Wang, Andreas Stolcke, and Jing Zheng.
2007.Reranking machine translation hypotheses with struc-tured and web-based language models.
In IEEE Work-shop on Automatic Speech Recognition & Understand-ing, pages 159?164.Jing Zheng, Necip Fazil Ayan, Wen Wang, Dimitra Ver-gyri, Nicolas Scheffer, and Andreas Stolcke.
2008.SRI systems in the NIST MT08 Evaluation.
In Pro-ceedings of the NIST 2008 Open MT Evaluation Work-shop.879
