The Acquisition and Use ofContext-Dependent Grammars for EnglishRobert E Simmons*University of TexasYeong-Ho Yu tUniversity of TexasThis paper introduces a paradigm of context-dependent grammar (CDG) and an acquisitionsystem that, through interactive teaching sessions, accumulates the CDG rules.
The resultingcontext-sensitive rules are used by a stack-based, shift~reduce parser to compute unambiguoussyntactic structures of sentences.
The acquisition system and parser have been applied to the phrasestructure and case analyses of 345 sentences, mainly from newswire stories, with 99% accuracy.Extrapolation from our current grammar predicts that about 25 thousand CDG rule exampleswill be sufficient o train the system in phrase structure analysis of most news stories.
Overall,this research concludes that CDG is a computationally and conceptually tractable approach forthe construction of sentence grammar for large subsets of natural anguage text.1.
IntroductionAn enduring goal for natural anguage processing (NLP) researchers has been to con-struct computer programs that can read narrative, descriptive texts such as newspaperstories and translate them into knowledge structures that can answer questions, clas-sify the content, and provide summaries or other useful abstractions of the text.
Anessential aspect of any such NLP system is parsing--to translate the indefinitely long,recursively embedded strings of words into definite ordered structures of constituentelements.
Despite decades of research, parsing remains a difficult computation thatoften results in incomplete, ambiguous tructures; and computational grammars fornatural anguages remain notably incomplete.
In this paper we suggest that a solutionto these problems may be found in the use of context-sensitive rules applied by adeterministic shift/reduce parser.A system is described for rapid acquisition of a context-sensitive grammar basedon ordinary news text.
The resulting grammar is accessed by deterministic, bottom-up parsers to compute phrase structure or case analyses of texts that the grammarscoven The acquisition system allows a linguist to teach a CDG grammar by showingexamples of parsing successive constituents of sentences.
At this writing, 16,275 ex-ample constituents have been shown to the system and used to parse 345 sentencesranging from 10 to 60 words in length achieving 99% accuracy.
These examples com-press to a grammar of 3,843 rules that are equally effective in parsing.
Extrapolationfrom our data suggests that acquiring an almost complete phrase structure grammarfor AP Wire text will require about 25,000 example rules.
The procedure is furtherdemonstrated to apply directly to computing superficial case analyses from Englishsentences.?
Department of Computer Sciences, AI Lab, University of Texas, Austin TX 78712.
E-mail @cs.texas.edut Boeing Helicopter Computer Svces, Philadelphia, PA(~) 1992 Association for Computational LinguisticsComputational Linguistics Volume 18, Number 4One of the first lessons in natural or formal language analysis is the Chomsky(1957) hierarchy of formal grammars, which classifies grammar forms from unre-stricted rewrite rules, through context-sensitive, context-free, and the most restricted,regular grammars.
It is usually conceded that pure, context-free grammars are notpowerful enough to account for the syntactic analysis of natural anguages (NL) suchas English, Japanese, or Dutch, and most NL research in computational linguistics hasused either augmented context-flee or ad hoc grammars.
The conventional wisdomis that context-sensitive grammars probably would be too large and conceptually andcomputationally untractable.
There is also an unspoken supposition that the use ofa context-sensitive grammar implies using the kind of complex parser required forparsing a fully context~sensitive language.However, NL research based on simulated neural networks took a context-basedapproach.
One of the first hints came from the striking finding from Sejnowski andRosenberg's NETtalk (1988), that seven-character contexts were largely sufficient omap each character of a printed word into its corresponding phoneme---where eachcharacter actually maps in various contexts into several different phonemes.
For ac-complishing linguistic case analyses McClelland and Kawamoto (1986) and Miikku-lainen and Dyer (1989) used the entire context of phrases and sentences to map stringcontexts into case structures.
Robert Allen (1987) mapped nine-word sentences of En-glish into Spanish translations, and Yu and Simmons (1990) accomplished comparablecontext-sensitive translations between English and German simple sentences.
It wasapparent that the contexts in which a word occurred provided information to a neuralnetwork that was sufficient o select correct word sense and syntactic structure forotherwise ambiguous usages of language.In order to solve a problem of accepting indefinitely long, complex sentences ina fixed-size neural network, Simmons and Yu (1990) showed a method for traininga network to act as a context-sensitive grammar.
A sequential program accessed thatgrammar with a deterministic, single-path parser and accurately parsed descriptivetexts.
Continuing that research, 2,000 rules were accumulated and a network wastrained using a back-propagation method.
The training of this network required tendays of continuous computation on a Symbolics Lisp Machine.
We observed that thetraining cost increased by more than the square of the number of training examples andcalculated that 10,000-20,000 rules might well tax a supercomputer.
Sowe decided thatstoring the grammar in a hash table would form a far less expensive option, providedwe could define a selection algorithm comparable to that provided by the trainedneural network.In this paper we describe such a selection formula to select rules for context-sensitive parsing, a system for acquiring context-sensitive rules, and experiments inanalysis and application of the grammar to ordinary newspaper text.
We show thatthe application of context-sensitive rules by a deterministic shift/reduce parser is aconceptually and computationally tractable approach to NLP that may allow us toaccumulate practical grammars for large subsets of English texts.2.
Context-Dependent ParsingIn NL research most interest has centered on context-free grammars (CFG), augmentedwith feature tests and transformations, used to describe the phrase structure of sen-tences.
There is a broad literature on Generalized Phrase Structure Grammar (Gazdaret al 1985), Unification Grammars of various types (Shieber 1986), and Augmented392Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for EnglishTransition Networks (J. Allen 1987).
Gazdar (1988) calls attention to a subcategory ofcontext-sensitive grammars called indexed languages and illustrates ome applicabilityto natural anguages, and Joshi illustrates an application of "mild context-sensitivity"(Joshi 1987), but in general, NL computation with context-sensitive grammars is alargely unexplored area.While a few advanced NLP laboratories have developed grammars and parsingcapabilities for significantly large subsets of natural anguage, 1 it cannot be denied thatmassive ffort was required and that the results are plagued by ambiguous interpreta-tions.
These grammars are typically a context-free form, augmented by complex featuretests, transformations, and occasionally, arbitrary programs.
The combination of evenan efficient parser with such intricate grammars may greatly increase computationalcomplexity of the parsing system (Tomita 1985).
It is extremely difficult to write andmaintain such grammars, and they must frequently be revised and retested to ensureinternal consistency as new rules are added.
We argue here that an acquisition sys-tem for accumulating context-sensitive rules and their application by a deterministicshift/reduce parser will greatly simplify the process of constructing and maintainingnatural anguage parsing systems.Although we use context-sensitive rules of the formuXv ~ uYvthey are interpreted by a shift/reduce parser with the result that they can be appliedsuccessfully to the LR(k) subset of context-free languages.
Unless the parser is aug-mented to include shifts in both directions, the system cannot parse context-sensitivelanguages.
It is an open question as to whether English is or is not context-sensitive,but it definitely includes discontinuous constituents hat may be separated by indefi-nitely many symbols.
For this reason, future developments of the system may requireoperations beyond shift and reduce in the parser.
To avoid the easy misinterpretationthat our present system applies to context-sensitive languages, we call it Context-Dependent Grammar (CDG).We begin with the simple notion of a shift/reduce parser.
Given a stack and aninput string of symbols, the shift/reduce parser may only shift a symbol to the stack(Figure la) or reduce n symbols on the stack by rewriting them as a single symbol(Figure lb).
We further constrain the parser to reduce no more than two symbols onthe stack to a single symbol.
The parsing terminates when the stack contains only asingle root element and the input string is empty.
Usually this class of parser appliesa CFG to a sentence, but it is equally applicable to CDG.2.1 CDG Rule FormsThe theoretical viewpoint is that the parse of a sentence is a sequence of states, eachcomposed of a condition of the stack and the input string.
The sequence nds success-fully when the stack contains only the root element (e.g.
SNT), and the input string is1 Notable examples include the large augmented CFGs at IBM Yorktown Hts, the Univ.
of Pennsylvania,and the Linguistic Research Ctr.
at the Univ.
of Texas.393Computational Linguistics Volume 18, Number 4INPUT SENTENCEt._i Ci+l t__i+2 .
.
.
.t.
tnNrkSTACKbottomINPUT SENTENCEt.i+l t 1+2 f_i+$ .
.
.
.t_lnNT._~,t lSTACKbottomt i,t m,.~,t I are terminals.N'T_'~ is a ~on-terminal_(a) Shift OperationINPUT SENTENCE INPUT SENTENCEt i t i+1 t_i+2 .
.
.
.
t_i t_i+l t__i+2 .
.
.
.t .mA~'_tSTACKbottoma~rdUSTACKbottomt i,t m,...,t I are terminals.N'T_~, NT~ are non-terminals.
(b) Reduce OperationFigure 1Shift/reduce parser.empty.
Each state can be seen as the left half of a context-sensitive rule whose righthalf is the succeeding state.stacksinputs ~ stacks+ l inputs+ lHowever, sentences may be of any length and are often more than forty words,so the resulting strings and stacks would form very cumbersome rules of variablelengths.
To avoid this difficulty, the stack and input parts of a rule are limited to fivesymbols each.
In the following example the stack and input parts are separated bythe symbol "*/' as the idea is applied to the sentence "The old man from Spain atefish."
The symbol _ stands for blank, art for article, adj for adjective, p for preposition,n for noun, and v for verb.
The syntactic classes are assigned by dictionary lookup ina context-sensitive dictionary.
2394Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for EnglishThe o ld  man f rom Spa in  a te  f i sha r t  ad j  n p n v n* a r t  ad j  n p na r t  * ad j  n p n v_ _ _ a r t  ad j  * n p n v n_ _ a r t  ad j  n * p n v n __ _ _ a r t  np  * p n v n _. .
.
.
n p * p n v n __ _ _np  p*  n vn_  __ _np  p n*  v n_  _ __ _ _ np pp  * v n _ _ _. .
.
.
n p * v n _ _ __ _ _ n p v * n_ _ n p v n *_ _ _ np vp  *snt  *The analysis terminates with an empty input string and the single symbol "snt"on the stack, successfully completing the parse.
Note that the first four operations canbe described as shifts followed by the two reductions, adj n ~ np, and art np ~ np.Subsequently the p and n were shifted onto the stack and then reduced to a pp; thenthe np and pp on the stack were reduced to an np, followed by the shifting of v andn, their reduction to vp, and a final reduction of np vp ---* snt.
Illustrations imilar tothis are often used to introduce the concept of parsing in AI texts on natural anguage(e.g.J.
Allen 1987).We could perfectly well record the grammar  in pairs of successive states as follows:___ np p*  n v n __--* __ np p n*  v n _____ np p n*  v n ___7  ___ np pp*  v n ___but some economy can be achieved by recording the operation and possible label asthe right half of a rule.
So for the example immediately above, we record:_ _ _ np  p * n v n _ _ - -+ (S )_ _ n p p n * v n _ _ _ - - * ( R p p )where S shifts and (R pp) replaces the top two elements of the stack with pp to formthe next state of the parse.Thus a windowed context of ten symbols is created as the left half of a rule and anoperation as the right half.
Note that if the stack were limited to the top two elements,and the input to a single element, the rule system would reduce to a binary rule CFG.The example in Figure 2 shows how a sentence "Treatment is a complete rest anda special diet" is parsed by a context sensitive shift /reduce parser.
Terminal symbolsare lowercase, while nonterminals are uppercase.
The shaded areas represent the parts2 Described in Section 7.3.395Computational Linguistics Volume 18, Number 4Treatment  is a complete  rest  and  a spec ia l  diet.
( n v det  adj n cn jdet  adj n)I Input bottom-~------~ ~ top ne~ ~ lastN!i.. .
.
.
.
.
.
............-.-...........-...-.iiiiiiiiiii!iliiii iii~ii::i::i::i::~iiiiiiiiiiiiiii !ii oi !{{{ii{iiiiiiiiiN{{!;!
{,nnn vn v detn v det adjv det adj nn v det NPn v NPn v NP cnjv NP cnj detNP cnj det adjcnj det adj nNP cnj det NPv NP cnj NPn v NP CNPn v NPn VPSn v det adj nv det adj n cnjdet adj n cnj detadj n cnj det adjn cnj det adj ncnj det adj ncnj det adj ncnj det adj ndet adj nadJ nnWindowed ContextOperationshiftshiftshiftshiftshiftreduce to NPreduce to NPshiftshiftshiftshiftreduce to NPreduce to NPreduce to CNPreduce to NPreduce to VPreduce to SdoneF igure  2An example of windowed context.of the context invisible to the system.
The next operation is solely decided by thewindowed context.
It can be observed that the last state in the analysis is the singlesymbol SNT--the designated root symbol, on the stack along with an empty inputstring, successfully completing the parse.And this is the CDG form of rule used in the phrase structure analysis.2.2  A lgor i thm for  the  Sh i f t /Reduce  ParserThe parser accepts a string of syntactic word classes as its input and forms a ten-symbol vector, five symbols each from the stack and the input string.
It looks up thisvector as the left half of a production in the grammar and interprets the right half ofthe production as an instruction to modify the stack and input sequences to constructthe next state of the parse.
To accomplish these tasks, it maintains two stacks, one forthe input string and one for the syntactic onstituents.
These stacks may be arbitrarilylarge.An algorithm for the parser is described in Figure 3.
The most important partof this algorithm is to find an applicable CDG rule from the grammar.
Finding sucha rule is based on the current windowed context.
If there is a rule whose left sideexactly matches the current windowed context, that rule will be applied.
However,realistically, it is often the case that there is no exact match with any rule.
Therefore,it is necessary to find a rule that best matches the current context.396Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for EnglishCD-SR-Parser(Input,Cdg)Input is a string of syntactic lasses for the given sentence.Cdg is the given CDG grammar ules.Stack := emptydo until(Input = empty and Stack = (SNT))Windowed-context := Append(Top-five(stack),First-ilve(input))Operation := Consult_CDG(Window-context,Cdg)if First(Operation) = SHIFTthen Stack := Push(First(Input),Stack)Input := Rest(Input)else Stack := Push(Second(Operation),Pop(Pop(Sta~k)))end doThe functions, Top_five and First-five, return the lists of top (or first) five elements of theStack and the Input respectively.
If  there are not enough elements, these procedures padwith blanks.
The function Append concatenates two lists into one.
Consult_CDG consultsthe given CDG rules to find the next operation to take.
The details of this function are thesubject of the next section.
Push and Pop add or delete one element o/from a stack whileFirst and Second return the first or second elements of a list, respectively.
Rest returns thegiven list minus the first element.Figure 3Context-sensitive shift reduce parser.2.3 Consulting the CDG RulesThere are two related issues in consulting the CDG rules.
One is the computationalrepresentation f CDG rules, and the other is the method for selecting an applicablerule.In the traditional CFG paradigms, a CFG rule is applicable if the left-hand side ofthe rule exactly matches the top elements of the stack.
However, in our CDG paradigm,a perfect match between the left side of a CDG rule and the current state cannot beassured, and in most cases, apartial match must suffice for the rule to be applied.
Sincemany rules may partially match the current context, the best matching rule should beselected.One way to do this is to use a neural network.
Through the back-propagationalgorithm (Rumelhart, Hinton, and Williams 1986), a feed-forward network can betrained to memorize the CDG rules.
After successful training, the network can be usedto retrieve the best matching rule.
However, this approach based on ~ neural networkusually takes considerable training time.
For instance, in our previous experiment(Simmons and Yu 1990), training a network for about 2,000 CDG rules took severaldays of computation.
Therefore, this approach as an intrinsic problem for scaling up,at least on the present generation of neural net simulation software.Another method is based on a hash table in which every CDG rule is storedaccording to its top two elements of the stack--the fourth and fifth elements of theleft half of the rule.
Given the current windowed context, the top two elements of thestack are used to retrieve all the relevant rules from the hash table.397Computational Linguistics Volume 18, Number 4We use no more than 64 word and phrase class symbols, so there can be no morethan 4,096 possible pairs.
The effect is to divide the large number of rules into nomore than 4,096 subgroups, each of which will have a manageable subset.
In fact,with 16,275 rules we discovered that we have only 823 pairs and the average numberof rules per subgroup is 19.8; however, for frequently occurring pairs the number ofrules in the subgroups can be much larger.
The problem is to determine what scoringformula should be used to find the rule that best matches a parsing context.Sejnowski and Rosenberg (1988) analyzed the weight matrix that resulted fromtraining NETtalk and discovered a triangular function with the apex centered at thecharacter in the window and the weights falling off in proportion to distance fromthat character.
We decided that the best matching rule in our system would followa similar pattern with maximum weights for the top two elements on the stack withweights decreasing in both directions with distance from those positions.
The scoringfunction we use is developed as follows:Let T4 be the set of vectors {RI~R2,... ,Rn}where Ri is the vector \[rl, r2,..., rl0\]Let C be the vector \[Cl, Ca,..., c10\]Let #(ci, ri) be a matching function whose value is 1 if ci = ri, and 0 otherwise.TZ is the entire set of rules, Ri is (the left half of) a particular ule, and C is theparse context.Then/-4' is the subset of T4 where if Ri E T~ I then #(ri4,c4) ?
#(ris~cs) = 1.Access of the hash table with the top two elements of the stack, c4, c5 producesthe set T4'.We can now define the scoring function for each Ri C T~ I.3 i0Score = ~_, t~(ci, ri) .
i+  ~_, #(ci, ri)(11 - i )i=1 i=6The first summation scores the matches between the stack elements of the ruleand the current context, and the second summation scores the matches between theelements in the input string.
If two items of the rule and context match, the total scoreis increased by the weight assigned to that position.
The maximum score for a perfectmatch is 21 according to the above formula.From several experiments, varying the length of vector and the weights, particu-larly those assigned to blanks, it has been determined that this formula gave the bestperformance among those tested.
More importantly, it has worked well in the currentphrase structure and case analysis experiments.It was an unexpected surprise to us 3 that using context-sensitive productions, anelementary, deterministic, parsing algorithm proved adequate to provide 99% correct,unambiguous anAalyses for the entire text studied.3.
Grammar Acquisition for CDGConstructing an augmented phrase structure grammar of whatever type unification,GPSG, or ATN--is a painful process usually involving a well-trained linguistic teamof several people.
These types of grammar equire that a CFG recognition rule such3 But perhaps not to Marcus (1980) and Berwick (1985), who promote the study of deterministic parsing.398Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for Englishas np vp ~ snt be supported by such additional information as the fact that the npand vp agree in number, that the np is characterized by particular features uch ascount, animate, etc., and that the vp can or cannot accept certain types of complements.The additional features make the rules exceedingly complex and difficult to prepareand debug.
College students can be taught easily to make a phrase structure tree torepresent a sentence, but it requires considerable linguistic training to deal successfullywith a feature grammar.We have seen in the preceding section that a CFG is derived from recording thesuccessive states of the parses of sentences.
Thus it was natural for us to develop aninteractive acquisition system that would assist a linguist (or a student) in constructingsuch parses to produce easily large sets of example CFG rules.
4 The system continuedto evolve as a consequence of our use until we had included capabilities to:?
read in text and data files?
compile dictionary and grammar  tables from completed text files?
select a sentence to continue processing or revise?
look up words in a dictionary to suggest he syntactic lass for the wordin context when assigning syntactic lasses to the words in a sentence?
compare ach state of the parse with rules in the current grammar  topredict the shift /reduce operation.
A carriage return signals that the useraccepts the prompt,  or the typing in of the desired operation overrides it.?
compute and display the parse tree from the local grammar  aftercompletion of each sentence, or from the global total g rammar  at anytime?
provide backing up and editing capability to correct errors?
print help messages and guide the user?
compile dictionary and grammar  entries at the completion of eachsentence, insuring no duplicate entries?
save completed or partially completed grammar  files.The resulting tool, GRAMAQ, enables a linguist to construct a context-sensitivegrammar  for a text corpus at the rate of several sentences per hour.
Thousands ofrules are accumulated with only weeks of effort in contrast o the years required for acomparable system of augmented CFG rules.
About ten weeks of effort were requiredto produce the 16,275 rules on which this study is based.
Since GRAMAQ's promptsbecome more accurate as the dictionary and grammar  grow in size, there is a positiveacceleration in the speed of grammar  accumulation and the linguist's task graduallyconverges to one of alert supervision of the system's prompts.A slightly different version of GRAMAQ is Caseaq, which uses operations thatcreate case constituents to accumulate a context-sensitive grammar  that transforms4 Starting with an Emacs editor, it was fairly easy to read in a file of sentences and to assign each wordits syntactic lass according to its context.
Then the asterisk was inserted at the beginning of thesyntactic string, the string was copied to the next line, the asterisk moved if a shift operation wasindicated, or the top two symbols on the stack were rewritten if a reduce was required--just as weconstructed the example in the preceding section.
Naturally enough, we soon made Emacs macros tohelp us, and then escalated to a Lisp program that would print the stack-*-string and interpret ourshift/reduce commands to produce a new state of the parse.399Computational Linguistics Volume 18, Number 4Text States Sentences Wds/Snt Mn-Wds/SntHepatitis 236 12 4-19 10.3Measles 316 10 4-25 16.3News Story 470 10 9-51 23.5APWire-Robots 1005 21 11-53 26.0APWire-Rocket 1437 25 8-47 29.2APWire-Shuttle 598 14 12-32 21.9Total 4062 92 4-53 22.8Table 1Characteristics of a sample of the text corpus.sentences directly to case structures with no intermediate stage of phrase structuretrees.
It has the same functionality as GRAMAQ but allows the linguist user to specifya case argument and value as the transformation of syntactic elements on the stack,and to rename the head of such a constituent by a syntactic label.
Figure 9 in Section 7.3illustrates the acquisition of case grammar.4.
Experiments with CDGThere are a number of critical questions that need be answered if the claim that CDGgrammars are useful is to be supported.?
Can they be used to obtain accurate parses for real texts??
Do they reduce ambiguity in the parsing process??
How well do the rules generalize to new texts??
How large must a CFG be to encompass the syntactic structures for mostnewspaper text?4.1 Parsing and Ambiguity with CDGOver the course of this study we accumulated 345 sentences mainly from newswiretexts.
The first two articles were brief disease descriptions from a youth encyclopedia;the remaining fifteen were newspaper articles from February 1989 using the terms "starwars," "SDI," or "Strategic Defense Initiative."
Table 1 characterizes typical articles bythe number of CDG rules or states, number of sentences, the range of sentence lengths,and the average number of words per sentence.We developed our approach to acquiring and parsing context-sensitive grammarson the first two simple texts, and then used GRAMAQ to redo those texts and toconstruct productions for the news stories.
The total text numbered 345 sentences,which accumulated 16,275 context-sensitive rules--an average of 47 per sentence.The parser embodying the algorithm illustrated earlier in Figure I was augmentedto compare the constituents it constructed with those prescribed uring grammar ac-quisition by the linguist.
In parsing the 345 sentences, 335 parses exactly matched thelinguist's original judgement.
In nine cases in which differences occurred, the parseswere judged correct, but slightly different sequences of parse states occurred.
Thetenth case clearly made an attachment error--of  an introductory adverbial phrase inthe sentence "Hours later, Baghdad announced .
.
.
.  "
This was mistakenly attached to"Baghdad."
This evaluation shows that the grammar was in precise agreement with400Robert F. Simmons and Yeong-Ho Yu Context-Dependent Grammars for EnglishAnother mission soon scheduled that also would have priority over the shuttle is the firstfiring of a trident two intercontinental r nge missile from a submerged submarine.~ N P  ~ art eazotb~n mis~on ~ NP adv soon  V P ~  palm scheduled NP  .
~ -  xeapron ~ thatv ~ haveSNT VP n ~ l~iorityp ~ overart then ~ shuttle- -  vbeVP ~ ~ _ _ ~ - ~ ~ ~  theprprt firingI ' ?'
o_PP~-% a r t ~  aNP NP n tridentn twox / adj intercontinental<n rangeNP missilePP ~ p -  froma~ - "  NP N-~ ~paprt submergedr~t" Nn  submarineFigure 4Sentence parse.the linguist 97% of the time and completed correct parses in 99.7% of the 345 sentencesfrom which it was derived.
Since our primary interest was in evaluating the effective-ness of the CDG, all these evaluations were based on using correct syntactic lassesfor the words in the sentences.
The context-sensitive dictionary lookup procedure de-scribed in Section 7.3 is 99.5% accurate, but it assigns 40 word classes incorrectly.
As aconsequence, using this procedure would result in a reduction of about 10% accuracyin parsing.An output of a sentence from the parser is displayed as a tree in Figure 4.
Sincethe whole mechanism is coded in Lisp, the actual output of the system is a nested listthat is then printed as a tree.Notice in this figure that the PP at the bottom modifies the NP composed of "thefirst firing of a trident two intercontinental range missile" not just the word "firing.
"Since the parsing is bottom-up, left-to-right, he constituents are formed in the naturalorder of words encountered in the sentence and the terminals of the tree can be readtop-to-bottom to give their ordering in the sentence.Although 345 sentences totaling 8594 words is a small selection from the infiniteset of possible English sentences, it is large enough to assure us that the CDG isa reasonable form of grammar.
Since the deterministic parsing algorithm selects asingle interpretation, which we have seen almost perfectly agrees with the linguist'sparsings, it is apparent hat, at least for this size text sample, there is little difficultywith ambiguous interpretations.401Computational Linguistics Volume 18, Number 45.
Generalization of CDGThe purpose of accumulating sample rules from texts is to achieve a grammar gen-eral enough to analyze new texts it has never seen.
To be useful, the grammar mustgeneralize.
There are at least three aspects of generalization to be considered.How well does the grammar generalize at the sentence l vel?
That is,how well does the grammar parse new sentences that it has notpreviously experienced?How well does the grammar generalize at the operation level?
That is,how well does the grammar predict he correct Shift/Reduce operationduring acquisition of new sentences?How much does the rule retention strategy affect generalization?
Forinstance, when the grammar predicts the same output as a new ruledoes, and the new rule is not saved, how well does the resultinggrammar parse?5.1 Generalization at the Sentence LevelThe complete parse of a sentence is a sequence of states recognized by the grammar(whether it be CDG or any other).
If all the constituents of the new sentence can berecognized, the new sentence can be parsed correctly.
It will be seen in a later para-graph that with 16,275 rules, the grammar predicts the output of new rules correctlyabout 85% of the time.
For the average sentence with 47 states, only 85% or about 40states can be expected to be predicted correctly; consequently the deterministic parsewill frequently fail.
In fact, 5 of 14 new sentences parsed correctly in a brief experimentthat used a grammar based on 320 sentences to attempt to parse the new, 20-sentencetext.
Considering that only a single path was followed by the deterministic parser, wepredicted that a multiple-path parser would perform somewhat better for this aspectof generalization.
I  fact, our initial experiments with a beam search parser esultedin successful parses of 15 of the 20 new sentences using the same grammar based onthe 320 sentences.5.2 Generalization at the Operation LevelThis level of generalization is of central significance to the grammar acquisition system.When GRAMAQ looks up a state in the grammar it finds the best matching state withthe same top two elements on the stack, and offers the right half of this rule as itssuggestion to the linguist.
How often is this prediction correct?To answer this question we compiled the grammar of 16,275 rules in cumulativeincrements of1,017 rules using a procedure, union-grammar, that would only add a ruleto the grammar if the grammar did not already predict its operation.
We call the resulta "minimal-grammar," and it contains 3,843 rules.
The black line of Figure 5 showsthat with the first 1,000 rules 40% were new; with an accumulation of5,000, 18% werenew rules.
By the time 16,000 rules have been accumulated, the curve has flattened toan average of 16% new rules added.
This means that the acquisition system will makecorrect prompts about 84% of the time and the linguist will only need to correct hesystem's suggestions about 3 or 4 times in 20 context presentations.402Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English5040r~-\] 302010. .
.
.
.
.
~"  .
.
.
.
.
.
t .
.
.
.
.
.
.
t .
.
.
.
.
.
.
t .
.
.
.
.
.
.
t .
.
.
.
.
.
.
t .
.
.
.
.
.
.
.
.
.
.
.
.
.
!
.
.
.
.
.
.
.
!
.
.
.
.
.
.
.
+ . '
" ' " t  .
.
.
.
.
.
t .
.
.
.
.
.
.
!
.
.
.
.
.
.
.
t .
.
.
.
.
.
.
!
.
.
.
.
.
.
.
!
.
.
.
.
.
.
.
!i .
.~ .
I I I I I I l I I l : I -' I ;I , | I I I I : I I l I , I l; I ; ; ! '
I I " | ; ", ' I I II i I I I I I , l , , , I ' ' '.
.
.
.
.
.
.
.
i ....... ; ....... ~ ....... i ....... i ....... ~ ....... 1 ....... i ....... 4 ...... 4 ...... ~- ....... i ....... ; ....... i ....... i ....... i.... .... i .... i .... i .... J ... L ... i ... i ... i ... J ... ... i....... * ...... G.,,--.~ ....... i ....... J ....... !
....... !
....... i ....... i ....... 4 ...... 4 ...... 4 ....... !
....... !
....... !
....... i ....... iii i i I i I -:1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17Accumulated Rules by ThousandsFigure 5Generalization of CDG rules.5.3 Rule Retention and General izat ionIf two parsing grammars account equally well for the same sentences, the one withfewer rules is less redundant, more abstract, and the one to be preferred.
We used theunion-grammar procedure to produce and study the minimal grammar for the 16,275rules (rule-examples) derived from the sample text.
Union-grammar records a newrule for a rule-example: s1.
if best matching rule has an operation that doesn't match2.
if best matching rule ties with another ule whose operation does notmatch3.
if 2 is true, and score = 21 we have a full contradiction and list the ruleas an error.Six contradictions occurred in the grammar; five were inconsistent treatments of "SNT"followed by one or more punctuation marks, while the sixth offered both a shift and a"pp" for a preposition-noun followed by a preposition.
The latter case is an attachmentambiguity not resolvable by syntax.In the first pass as shown in Table 2, the text resulted in 3,194 rules compared with16,275 possible rules.
That is, 13,081 possible CDG rules were not retained becausealready existing rules would match and predict the operation.
However, using thoserules to parse the same text gave very poor results: zero correct parses at the sentencelevel.
Therefore, the process of compiling a minimal grammar was repeated startingwith those 3,194 rules.
This time only 619 new rules were added.
The purpose of this5 These definite conditions are due to an analysis by Mark Ring.403Computational Linguistics Volume 18, Number 4Pass UntWined Retained Total Rules1 13081 3194 162752 15656 619 162753 16245 18 162754 16275 0 16275Table 2Four passes with minimal grammar.repetition is to get rid of the effect that the rules added later change the predictionsmade earlier.
Finally, in a fourth repetition of the process no rules were new.The resulting grammar of 3,843 rules succeeds in parsing the text with only occa-sional minor errors in attaching constituents.
It is to be emphasized that the unretainedrules are similar but not identical to those in the minimal grammar.We can observe that this technique of minimal retention by "unioning" new rulesto the grammar esults in a compression of the order 16,275/3,843 or 4.2 to 1, withoutincrease in error.
If this ratio holds for larger grammars, then if the linguist accu-mulates 40,000 training-example rules to account for the syntax of a given subset oflanguage, that grammar can be compressed automatically to about 10,000 rules thatwill accomplish the same task.6.
Predicting the Size of CDGsWhen any kind of acquisition system is used to accumulate knowledge, one veryinteresting question is, when will the knowledge be complete nough for the intendedapplication?
In our case, how many CDG rules will be sufficient o cover almost allnewswire stories?
To answer this question, an extrapolation can be used to find a pointwhen the solid line of Figure 5 intersects with the y-axis.
However, the CDG curve isdescending too slowly to make a reliable extrapolation.Therefore, another question was investigated instead: when will the CDG rulesinclude a complete set of CFG rules?
Note that a CDG rule is equivalent to a CFG ruleif the context is limited to the top two elements of the stack.
What the other elementsin the context accomplish is to make one rule preferable to another that has the sametop two elements of the stack, but a different context.We allow 64 symbols in our phrase structure analysis.
That means, there are 642possible combinations for the top two elements of the stack.
For each combination,there are 65 possible operations: 6 a shift or a reduction to another symbol.
Among16,275 CDG rules, we studied how many different CFG rules can be derived by elim-inating the context.
We found 844 different CFG rules that used 600 different left-sidepairs of symbols.
This shows that a given context free pair of symbols averages 1.4different operations.
7Then, as we did with CDG rules, we measured how many new CFG rules wereadded in an accumulative fashion.
The shaded line of Figure 5 shows the result.6 Actually, there are fewer than 65 possible operations since the stack elements can be reduced only tononterminal symbols.7 We actually use only 48 different symbols, o only 482 or 2,304 combinations could have occurred.
Thefraction 600/2,304 yields .26, the proportion of the combinatoric space that is actually used, so far.404Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English100 .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
~.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
:!
!!
!I_m?
?
.
.
.
.
.
.
.or ,Z 10  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
l1100 1 ,000  10 ,000  25,000 100,000Nbr  of Accumulated  RulesExtrapolation thegray line, predicts that 99% of the context free pairs will be achieved with theaccumulation of 25,000 context sensitive rues.Figure 6Log-log plot of new CFG rules.Notice that the line has descended to about 1.5% errors at 16,000 rules.
To make anextrapolation easier, a log-log graph shows the same data in Figure 6.
From this graph,it can be predicted that, after about 25,000 CDG rules are accumulated, the grammarwill encompass a CFG component that is 99% complete.
Beyond this point, additionalCDG rules will add almost no new CFG rules, but only fine-tune the grammar so thatit can resolve ambiguities more effectively.Also, it is our belief that, after the CDG reaches that point, a multi-path, beam-search parser will be able to parse most newswire stories very reliably.
This belief isbased on our initial experiment that used a beam search parser to test generalizationof the grammar to find parses for fifteen out of twenty new sentences.7.
Acquiring Case GrammarExplicating the phrase structure constituents of sentences i an essential aspect incomputer recognition of meaning.
Case analysis organizes the constituents into a hi-erarchical structure of labeled propositions.
The propositions can be used directly toanswer questions and are the basis of schemas, scripts, and frames that are used toadd meaning to otherwise inexplicit exts.
As a result of the experiments with acquir-ing CDG and exploring its properties for parsing phrase structures, we became fairlyconfident that we could generalize the system to acquisition and parsing based on agrammar that would compute syntactic ase structures directly from syntactic strings.Direct translation from string to structure is supported by neural network experimentssuch as those by McClelland and Kawamoto (1986), Miikkulainen and Dyer (1989), Yuand Simmons (1990), and Leow and Simmons (1990).
We reasoned that if we couldacquire case grammar with something approaching the simplicity of acquiring phrasestructure rules, the result could be of great value for NL applications.405Computational Linguistics Volume 18, Number 47.1 Case StructureCook (1989) reviewed twenty years of linguistic research on case analysis of naturallanguage sentences.
He synthesized the various theories into a system that dependson the subclassification f verbs into twelve categories, and it is apparent from hisreview that with a fine subcategorization of verbs and nominals, case analysis can beaccomplished asa purely syntactic operation--subject to he limitations of attachmentambiguities that are not resolvable by syntax.
This conclusion is somewhat at variancewith those AI approaches that require a syntactic analysis to be followed by a semanticoperation that filters and transforms yntactic onstituents o compute case-labeledpropositions (e.g.
Rim 1990), but it is consistent with the neural network experienceof directly mapping from sentence to case structure, and with the AI research thatseeks to integrate syntactic and semantic processing while translating sentences topropositional structures.Linguistic theories of case structure have been concerned only with single propo-sitions headed by verb predications; they have been largely silent with regard to thestructure of noun phrases and the relations among embedded and sequential proposi-tions.
Additional conventions for managing these complications have been developedin Simmons (1984) and Alterman (1985) and are used here.The central notion of a case analysis is to translate sentence strings into a nestedstructure of case relations (or predicates) where each relation has a head term and anindefinite number of labeled arguments.
An argument may itself be a case relation.Thus a sentence, as in the examples below, forms a tree of case relations.The old man from Spain ate fish.
(eat Agt (man Mod old From spain) Obj fish)(is ObjlObj2Another mission scheduledsoonisthefirstfiringofatrident missilefrom a submerged submarine.
(mission Mod another Obj* (scheduled Vmod soon))(firing Mod first Det the Of (missile Nmod trident Det a)From (submarine Mod submerged Det a)))Note that mission is in Obj* relation to scheduled.
This means the object of scheduledis mission, and the expression can be read as "another mission such that mission isscheduled soon."
An asterisk as a suffix to a label always signals the reverse directionfor the label.There is a small set of case relations for verb arguments, uch as verbmodifier,agent, object, beneficiary, experiencer, location, state, time, direction, etc.
For nouns thereare determiner, modifier, quantifier, amount, nounmodifier, preposition, and reverse verbrelations, agt*, obj*, ben*, etc.
Prepositions and conjunctions are usually used directlyas argument labels while sentence conjunctions such as because, while, before, after, etc.are represented as heads of propositions that relate two other propositions with thelabels preceding, post, antecedent, and consequent.
For example, "Because she ate fish andchips earlier, Mary was not hungry.
"(because Ante (ate Agt she Obj (fish And chips) Vmod earlier)Conse (was Vmod not Objl mary State hungry))Verbs are subcategorized as vao, vabo, vo, va, vhav, vbe where a is agent, o is object,b is beneficiary and vhav is a form of have and vbe a form of be.
So far, only the406Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for Englishsubcategory of time has been necessary in subcategorizing nouns to accomplish thisform of case analysis, but in general, a lexical semantics i required to resolve syntacticattachment ambiguities.
The complete set of case relations is presumed to be small,but no one has yet claimed a complete numeration of them.Other case systems uch as those taught by Schank (1980) and Jackendoff (1983)classify predicate names into such primitives as Do, Event, Thing, Mtrans, Ptrans, Go,Action, etc., to approximate some form of "language of thought" but the present ap-proach is less ambitious, proposing merely to represent in a fairly formal fashion theorganization of the words in a sentence.
Subsequent operations on this admittedlysuperficial class of case structures, when augmented with a system of shallow lexi-cal semantics, have been shown to accomplish question answering, focus tracking oftopics throughout a text, automatic outlining, and summarization of texts (Seo 1990;Rim 1990).
One strong constraint on this type of analysis is that the resulting casestructure must maintain all information present in the text so that the text may beexactly reconstituted from the analysis.7.2 Syntactic Analysis of Case StructureWe've seen earlier that a shift/reduce-rename operation is sufficient o parse mostsentences into phrase structures.
Case structure, however, requires transformationsin addition to these operations.
To form a case structure it is frequently necessaryto change the order of constituents and to insert case labels.
Following Jackendoff'sprinciple of grammatical constraint, which argues essentially that semantic interpretationis frequently reflected in the syntactic form, case transformations are accomplished aseach syntactic onstituent is discovered.
Thus when a verb, say throw and an NP, saycoconuts are on top of the stack, one must not only create a VP, but also decide thecase, Obj, and form the constituent, (throw Obj coconuts).
This can be accomplished incustomary approaches to parsing by using augmented context free recognition rulesof the form:VP~VPNP/  lob j2where the numbers following the slash refer to the text dominated by the syntacticclass in the referenced position, (ordered left-to-right) in the right half of the rule.The resulting constituents can be accumulated to form the case analysis of a sentence(Simmons 1984).We develop augmented context-sensitive rules following the same principle.
Letus look again at the example "The old man from Spain ate fish," this time to developcase relations.
* art  adj n f rom n vao n ; sh i f tart  * adj n f rom n vao n ; sh i f tart  adj * n f rom n vao n ; sh i f tart  adj n * f rom n vao n ; i modart  n * f rom n vao n ; 1 detn * f rom n vao n ; sh i f tn f rom * n vao n ; sh i f tn f rom n * vao n ; 3 2 1n * vao n ; sh i f tn vao * n ; 2 agtvao * n ; sh i f tvao n * ; 1 obj2 (man Mod old)2 (man Mod o ld  Det  the)(man Mod o ld  Det  the  F rom spain)1 (ate Agt  (man Mod o ld  ... )2 (ate Agt  (man ...) Obj f ish)407Computational Linguistics Volume 18, Number 4Stack Case-TransformV..nvpasvadj nnl n2n vaon vovbe vvabo nn vpasvprep nprep nby nsnt becausebecause sntand nsnt afterafter sntn mod adjn2 nmod nl1 agt 21 obj 21 vbe 2 vpasv2 ben 1 vao1 obj 23 2 13 2 11 prep 21 conse 22 ante 11 2 31 pre 22 post 1Table 3Some typical case transformations for syntactic onstituentsIn this example the case transformation immediately follows the semicolon, and theresult of the transformation is shown in parentheses further to the right.
The result inthe final constituent is:(ate Agt (man Mod old Det the From spain) Obj fish).Note that we did not rename the syntactic onstituents as NP or VP in this example,because we were not interested in showing the phrase structure tree.
Renaming in caseanalysis need only be done when it is necessary to pass on information accumulatedfrom an earlier constituent.For example, in "fish were eaten by birds," the CS parse is as follows:* n vbe ppart by n ; shiftn * vbe ppart by n ; shiftn vbe * ppart by n ; shiftn vbe ppart * by n ; I vbe 2, vpasv (eaten Vbe were)n vpasv * by n ; I obj 2 (eaten Vbe were Obj fish)vpasv * by n ; shiftvpasv by * n ; shiftvpasv by n * ; i prep 2 (birds Prep by)vpasv n * ; 2 agt 1 (eaten Vbe were Obj f ish Agt (birds Prep by))Here, it was necessary to rename the combination of a past participle and its auxiliaryas a passive verb, vpasv, so that the syntactic subject and object could be recognizedas Obj and Agent, respectively.
We also chose to use the argument name Prep to form(birds Prep by) so that we could then call that constituent Agent.We can see that the reduce operation has become a reduce-transform-rename opera-tion where numbers refer to elements of the stack, the second term provides a caseargument label, the ordering provides a transformation, and an optional fourth ele-ment may rename the constituent.
A sample of typical case transformations is shownassociated with the top elements of the stack in Table 3.
In this table, the first elementof the stack is in the third position in the left side of the table, and the number  I refersto that position, 2 to the second, and 3 to the first.
As an aid to the reader the first two408Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for EnglishCS-CASE-Parser( input,cdg)Input is a string of syntactic lasses for the given sentence.Cdg is the given CDG grammar ules.stack := emptyoutputstack := emptydo until(input = empty and 2nd(stack) = blank)window-context := append(top-.five(stack),first_five(input))operation := consult_CDG(window-context,cdg)if first(operation) = SHIFTthen stack := push(first(input),stack)input := rest(input)else stack := push(select(operation),pop(pop(stack)))outputstack := make_constituent(operation,outputstack)end doFigure 7Algorithm for case parse.entries in the table refer literally by symbol rather than by reference to the stack.
Thesymbols vao and vabo are subclasses of verbs that take, respectively, agent and object;and agent, beneficiary, and object.
The symbol v.. refers to any verb.
Forms of the verbbe are referred to as vbe, and passivization is marked by relabeling a verb by addingthe suffix -pasv.Parsing case structuresFrom the discussion above we may observe that the flow of control in accomplishinga case parse is identical to that of a phrase structure parse.
The difference lies in thefact that when a constituent is recognized (see Figure 7):in phrase structure, a new name is substituted for its stack elements, anda constituent is formed by listing the name and its elementsin case analysis, a case transformation is applied to designated elementson the stack to construct a constituent, and the head (i.e.
the first elementof the transformation) is substituted for its elements--unless a new nameis provided for that substitution.Consequently the algorithm used in phrase structure analysis is easily adapted to caseanalysis.
The difference lies in interpreting and applying the operation to make a newconstituent and a new stack.In the algorithm shown above, we revise the stack by attaching either the headof the new constituent, or its new name, to the stack resulting from the removal ofall elements in the new constituent.
The function select chooses either a new nameif present, or the first element, the head of the operation.
Makeconstituent applies thetransformation rule to form a new constituent from the output stack and pushes theconstituent onto the output stack, which is first reduced by removing the elementsused in the constituent.
Again, the algorithm is a deterministic, first (best) path parser409Computational Linguistics Volume 18, Number 4with behavior essentially the same as the phrase structure parser.
But this versionaccomplishes transformations to construct a case structure analysis.7.3 Acquisition System for Case GrammarThe acquisition system, like the parser, required only minor revisions to accept casegrammar.
It must apply a shift or any transformation toconstruct the new stack-stringfor the linguist user, and it must record the shift or transformation as the right halfof a context-sensitive rule--still composed of a ten-symbol left half and an operationas the right half.
Consequently, the system will be illustrated in Figure 9 rather thandescribed in detail.Earlier we mentioned the context-sensitive dictionary.
This is compiled by associ-ating with each word the linguist's in-context assignments of each syntactic word classin which it is experienced.
When the dictionary is built, the occurrence frequencies ofeach word class are accumulated for each word.
A primitive grammar of four-tuplesterminating with each word class is also formed and hashed in a table of syntacticpaths.
The procedure to determine a word class in context,,, first obtains the candidates from the dictionary.,, For each candidate wc, it forms a four-tuple, vec, by adding it to the cdrof each immediately preceding vec, stored in IPC.?
Each such vec is tested against he table of syntactic paths;if it has been seen previously, it is added to the list of IPCs,otherwise it is eliminated.If the union of first elements of the IPC list is a single word class, that isthe choice.
If not, the word's most frequent word class among the unionof surviving classes for the word is chosen.The effect of this procedure is to examine a context of plus and minus three wordsto determine the word class in question.
Although a larger context based on five-tuple paths is slightly more effective, there is a tradeoff between accuracy and storagerequirements.The word class selection procedure was tested on the 8,310 words of the 345-sentence sample of text.
A score of 99.52% correct was achieved, with 8,270 wordscorrectly assigned.
As a comparison, the most frequent category for a word resultedin 8,137 correct assignments for a score of 97.52%.
Although there are only 3,298word types with an average of 3.7 tokens per type, the occurrence of single word classusages for words in this sample is very high, thus accounting for the effectiveness of thesimpler heuristic of assignment of the most frequent category.
However, since the effectof misassignment of word class can often ruin the parse, the use of the more complexprocedure isamply justified.
Analysis of the 40 errors in word class assignment showed7 confusions of nouns and verbs that will certainly cause errors in parsing; otherconfusions of adjective/noun, and adverb/preposition are less devastating, but stillserious enough to require further improvements in the procedure.The word class selection procedure is adequate to form the prompts in the lexicalacquisition phase, but the statistics on parsing effectiveness given earlier depend onperfect word class assignments.Shown in Figure 8 is the system's presentation of a sentence and its requests foreach word's syntactic lass.
The protocol in Figure 9 shows the acquisition of shift410Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for EnglishLex ica l  Acqu is i t ion :  The  sys tem prompts  for  syntact i c  classes are in cap i ta ls .
The  user  accepts  thesystem's prompt  w i th  a car r iage  re turn ,  cr  or types  in a syntact i c  c lass in lower case.
We show user 'sresponses in bold- face,  us ing c r  for car r iage  re turn .
Other  abbrev ia t ions  are wc for word  class, y or n foryes or no, and  b for backup.
(THE LAUNCH OF D ISCOVERY AND ITS  F IVE  ASTRONAUTS HAS BEEN DELAYED AT-LEASTTWO DAYS UNTIL  MARCH ELEVENTH BECAUSE-OF  A CRUSHED ELECTRICAL  PART ON AMAIN ENGINE COMMA OFF IC IALS  SA ID)process  th is  one?
- y or n yTHE cr for de fau l t  else we or b defau l t  is: ART  c rLAUNCH cr for de fau l t  else wc or b c r  ;user made an  error  s ince there  was  no defau l tLAUNCH cr  for defau l t  else wc or b n ;system repeated  the  questionOF cr for de fau l t  else wc or b defau l t  is: OF  c rD ISCOVERY cr for de fau l t  else we or b nAND cr for de fau l t  else wc or b defau l t  is: CONJ  e rITS cr for de fau l t  else wc or b b ;user dec ided to redo "and"AND cr for defau l t  else wc or b defau l t  is: CONJ  andITS  cr  for de fau l t  else wc or b pproni !
sk ipp ing  most  o f  the  sentence .
.
.A cr for de fau l t  else wc or b defau l t  is: ART  crMAIN cr for defau l t  else wc or b nENGINE cr for defau l t  else wc or b nCOMMA cr for defau l t  else wc or b defau l t  is: COMMA crOFF IC IALS  cr  for de fau l t  else wc or b nSA ID cr for defau l t  else wc or b vaoFigure 8Illustration of dictionary acquisition.and transformation rules for the sentence.
What we notice in this second protocol isthat the stack shows syntactic labels but the input string presented to the linguistis in English.
As the system constructs a CS rule, however, the vector containing fiveelements of stack and five of input string is composed entirely of syntactic lasses.
TheEnglish input string better enables the linguist o maintain the meaningful context heor she uses to analyze the sentence.
About five to ten minutes were required to makethe judgments for this sentence.
Appendix A shows the rules acquired in the session.When rules for the sentence were completed, the system added the new syntacticclasses and rules to the grammar, then offered to parse the sentence.
The resultingparse is shown in Figure 10.The case acquisition system was used on the texts described earlier in Table 1 toaccumulate 3,700 example CDG case rules.
Because the case transformations refer tothree stack elements and the number of case labels is large, we expected and found thata much larger sample of text would be required to obtain the levels of generalizationseen in the phrase structure xperiments.Accumulated in increments of400 rules, the case curve flattens at about 2,400 ruleswith an average of 33% error in prediction compared to the 20% found in analysisof the same number of phrase structure rules.
The compressed or minimal grammarfor this set of case rules reduces the 3,700 rules to 1,633, a compression ratio in thiscase of 2.3 examples accounted for by each rule.
The resulting compressed grammarparses the texts with 99% accuracy.
These statistics are from our initial study of a casegrammar, and they should be taken only as preliminary estimates of what a morethorough study may show.411Computat iona l  L inguist ics Vo lume 18, Number  4Case-Grammar  Acquis i t ion:  The options are h for a help message, b for backup one state, s for shift,case-trans for a case transformation, and cr for carriage return to accept a system prompt.
System promptsaxe capitalized in parentheses, user responses are in lower case.
Where no appaxent response is shown, theuser did a carriage return to accept the prompt.
The first line shows the syntactic lasses for the words inthe sentence.
(ART N OF N AND PPRON ADJ N VHAV VBE VAO AT-LEAST ADJ N UNTIL N N BECAUSE-OFART PPART AD3 N ON ART N N COMMA N VAO)(* THE LAUNCH OF DISCOVERY AND ITS FIVE ASTRONAUTS HAS BEEN DELAYED AT-LEASTTWO DAYS UNTIL MARCH ELEVENTH BECAUSE-OF A CRUSHED ELECTRICAL PART ON AMAIN ENGINE COMMA OFFICIALS SAID)options-axe h b s case-trans or cr for default: (S)(ART * LAUNCH OF DISCOVERY AND ITS ... SAID)options-axe h b s case-trans or cr for default: (S)(ART N * OF DISCOVERY AND ITS FIVE ... SAID)options-axe h b s case-trans or cr for default: (S) 1 det 2(N * OF DISCOVERY AND ITS FIVE ... SAID)options-axe h b s case-trans or cr for default: (S)i sk ipp ing  severa l  shi f ts(N OF N AND PPRON AD3 N * HAS BEEN DELAYED AT-LEAST ... SAID)options-axe h b s case-trans or cr for default: (S) 1 rood 2(N OF N AND PPRON N * HAS BEEN DELAYED AT-LEAST ... SAID)options-axe h b s case-trans or cr for default: NIL 1 possby 2(N OF N AND N * HAS BEEN DELAYED AT-LEAST ... SAID)options-axe h b s case-trans or cr for default: NIL 3 2 1(N OF N * HAS BEEN DELAYED AT-LEAST TWO ... SAID)options-axe h b s case-trans or cr for default: (3 2 1)(N * HAS BEEN DELAYED AT-LEAST TWO ... SAID)options-axe h b s case-trans or cr for default: (S)(N VHAV * BEEN DELAYED AT-LEAST TWO DAYS ... SAID)options-axe h b s case-trans or cr for default: (10B J  2) s(N VHAV VBE * DELAYED AT-LEAST TWO DAYS ... SAID)options-axe h b s case-trans or cr for default: (S) 1 anx 2(N VBE * DELAYED AT-LEAST TWO DAYS UNTIL ... SAID)options-axe h b s case-trans or cr for default: (S)(N VBE VAO * AT-LEAST TWO DAYS UNTIL MARCH ... SAID)options-axe h b s case-trans or cr for default: (1 VBE 2 VAOPASV)(N VAOPASV * AT-LEAST TWO DAYS UNTIL MARCH ... SAID)options-axe h b s case-trans or cr for default: (10B J  2 /!
sk ipp ing  now to BECAUSE(VAOPASV UNTIL N * BECAUSE-OF A CRUSHED ELECTRICAL ... SAID)options-axe h b s case-trans or cr for default: (S) 3 2 1(VAOPASV * BECAUSE-OF A CRUSHED ELECTRICAL PART ... SAID)options-axe h b s case-trans or cr for default: (S)(VAOPASV BECAUSE-OF * A CRUSHED ELECTRICAL PART ON .. SAID)options-are h b s case-trans or cr for default: NIL 1 conse 2(BECAUSE-OF * A CRUSHED ELECTRICAL PART ON ... SAID)options-axe h b s case-trans or cr for default: NIL s: sk ipp ing  now to the  end(BECAUSE-OF COMMA N VAO *)options-are h b s case-trans or cr for default: (1 OB3 2) 1 agt 2(BECAUSE-OF COMMA VAO *)options-axe h b s case-trans or cr for default: NIL 1 obj 3Figure 9I l lustrat ion of case grammar  acquis it ion.8.
Discussion and ConclusionsI t  seems remarkab le  that  a l though the  theory  o f  context -sens i t ive  grammars  appearedin  Chomsky  (1957), fo rmal  context -sens i t ive  ru les  seem not  to  have  been used  pre -412Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for Englishsa id  ~ AGT - '~  OfficialsIOBJ ~ because-ofm CONSE ~ de layed ~ VBE ''?~ been~ AUX' -~ hasOBJ - '~  l aunch- -  DET- ' -~  theIOF"-~ discovery  ~ AND ~ ~.
ron~oJ ts~ MOD "~ fivePOSSBY "~ itsAT-LEAST ~ days -  MOD ~ twoUNTIL ~ eleventh ~ NMOD - -~ marchANTE ~ part ~ MOD ~ electrical~ MOD ~ crushedDET ''4t" aON " -~ engineL NMOD -~mainDET - '~  aThe launch of discovery and its five astronauts has been delayed at-least wo days untilmarch eleventh because-of a crushed electrical part on a main engine comma officials said.Figure 10Case analysis of a sentence.viously in computational parsing.
As researchers we seem simply to have assumed,without experimentation, that context-sensitive grammars would be too large andcumbersome tobe a practical approach to automatic parsing.
In fact, context-sensitive,binary phrase structure rules with a context composed of the preceding three stacksymbols and the next five input symbols,stack1_3 binary-rule input1_5 ---* operationprovide several encouraging properties.The linguist uses the full context of the sentence to make a simpledecision: either shift a new element onto the stack or combine the toptwo elements into a phrase category.The system compiles a CS rule composed of ten symbols, the top fiveelements of the stack and the next five elements of the input string.
Thecontext of the embedded binary rule specializes that rule for use insimilar environments, thus providing selection criteria to the parser forthe choice of shift or reduce, and for assigning the phrase name that hasmost frequently been used in similar environments.
The context providesa simple but powerful approach to preference parsing.413Computational Linguistics Volume 18, Number 4?
As a result, a deterministic bottom-up arser is notably successful infinding precisely the parse tree that the linguist who constructed theanalysis of a sentence had in mind--and this is true whether thegrammar is stored as a trained neural network or in the form ofhash-table ntries.?
Despite the large combinatoric space for selecting 1 of 64 symbols ineach of 10 slots in the rules--641?
possible patterns experiments inaccumulating phrase structure grammar suggest hat a fairly completegrammar will require only about 25,000 CS rules.?
It is also the case that when redundant rules are removed the CSgrammar is reduced by a factor of four and still maintains its accuracy inparsing.?
Because of the simplicity and regular form of the rule structure, it hasproved possible to construct an acquisition system that greatly facilitatesthe accumulation of grammar.
The acquisition system presents contextsand suggests operations that have previously been used with similarcontexts; thus it helps the linguist to maintain consistency of judgments.?
Parsing with context-sensitive rules generalizes from phrase structurerewriting rules to the transformational rules required by case analysis.Since the case analysis rules retain a regular, simple form, the acquisitionsystem also generalizes to case grammar.Despite such advantageous properties, a few cautions hould be noted.
First, thedeterministic parsing algorithm is sufficient o apply the CDG to the sentences fromwhich the grammar was derived, but to accomplish effective generalization to newsentences, a bandwidth parsing algorithm that follows multiple parsing paths is supe-rior.
Second, the 99% accuracy of the parsing will deteriorate markedly if the dictionarylookup makes errors in word assignment.
Thirdly, the shift/reduce parsing is unableto give correct analyses for such embedded iscontinuous constituents as "I saw theman yesterday who ... .  "
Finally, the actual parsing structures that we have presentedhere are skeletal.
We did not mark mood, aspect or tense of verbs, number for nouns,or deal with long distance dependencies.
We do not resolve pronoun references; andwe do not complete llipses in conjunctive and other constructions.Each of these shortcomings i the subject of continuing research.
For the present,the output of the case parser provides the nested, labeled, propositional structureswhich, supported by a semantic knowledge base, we have customarily used to ac-complish focus-tracking of topics through a continuous text to compute labeled out-lines and other forms of discourse structure (Seo 1990; Rim 1990; Alterman 1985).During this process of discourse analysis, some degapping, completion of ellipsis, andpronoun resolution is accomplished.8.1 ConclusionsFrom the studies presented in this paper we conclude:.
Context-Dependent Grammars (CDGs) are computationally andconceptually tractable formalisms that can be composed easily by alinguist and effectively used by a deterministic parser to compute phrasestructures and case analyses for subsets of newspaper English.414Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for English2.
The contextual portions of the CDG rules and the scoring formula thatselects the rule that best matches the parsing context allow adeterministic parser to provide preferred parses, reflecting the linguist'smeaning-based judgments.3.
The CDG acquisition system described earlier simplifies linguisticjudgments and greatly improves a linguist's ability to construct relativelylarge grammars rapidly.4.
Although a deterministic, bottom-up arser has been sufficient toprovide highly accurate parses for the 345-sentence sample of news textstudied here, we believe that a multi-path parser proves superior in itsability to analyze sentences beyond the sample on which the grammarwas developed.5.
With 3,843 compressed CDG rules, the acquisition system is about 85%accurate in suggesting the correct parsing for constituents from texts ithas not experienced.6.
For phrase structure analysis, the context-free core of the CS rules will be99% complete when we have accumulated about 25,000 CS rules.
At thatpoint it should be possible for a multi-path parser to find a satisfactoryanalysis for almost all news story sentences.We have shown that the acquisition and parsing techniques apply also to CDGgrammars for computing structures of case propositions to represent sentences.
Inthis application, however, much more research is needed to better define linguisticsystems for case analysis, and for their application to higher levels of natural languageunderstanding.AcknowledgmentsThis work was partially supported by theArmy Research Office under contractDAAG29-84-K-0060.ReferencesAlterman, Richard (1985).
"A dictionarybased on concept coherence," ArtificialIntelligence, 25, 153-186.Allen, James (1987).
Natural LanguageUnderstanding.
Benjamin Cummings.Allen, Robert (1987).
"Several studies onnatural language and back propagation.
"In Proceedings, International Conference onNeural Networks.
San Diego.Berwick, Robert C. (1985).
The Acquisition ofSyntactic Knowledge, Vol.
2, 335-341.
MITPress.Chomsky, Noam (1957).
Syntactic Structures.Mouton.Cook, Walter (1989).
Case Grammar Theory.Georgetown University Press.Gazdar, Gerald (1988).
"Applicability ofindexed grammars to natural languages.
"In Linguistic Theory and ComputerApplications, edited by P. Whitelock, et al,Academic Press, 37--67.Gazdar, Gerald, and Mellish, Chris (1989).Natural Language Processing inLISP.Addison-Wesley.Gazdar, Gerald; Klein, E.; and Pullum, G.;and Sag, I.
(1985).
Generalized PhraseStructure Grammar.
Harvard UniversityPress.Jackendoff, Ray (1983).
Semantics andCognition, MIT Press, Cambridge, Mass.,1983.Joshi, Aravind (1987).
"An introduction totree-adjoining grammars" InMathematicsof Language, dited by A. Manaster-Ramer,87-114.
John Benjamins.Leow, Wee-Keng, and Simmons, R. E (1990).
"A constraint satisfaction network forcase analysis," AI Technical ReportAI90-129, Department ofComputerScience, University of Texas, Austin.Marcus, M. P. (1980).
A Theory of SyntacticRecognition for Natural Language.
MITPress.McClelland, J. L., and Kawamoto, A. H.(1986).
"Mechanisms ofsentenceprocessing: Assigning roles toconstituents."
In Parallel Distributed415Computational Linguistics Volume 18, Number 4Processing, Vol.
2, edited byJ.
L. McClelland and D. E. Rumelhart,MIT Press, 272-326.Miikkulainen, Risto, and Dyer, M. (1989).
"Amodular neural network architecture forsequential paraphrasing of script-basedstories," Artificial Intelligence Lab.,Department of Computer Science, UCLA.Rim, Hae-Chang (1990).
Computing outlinesfrom descriptive t xts.
Doctoral dissertation,University of Texas, Austin.Rumelhart, David E.; Hinton, G. E.; andWilliams, R. J.
(1986).
"Learning internalrepresentations by error propagation."
InParallel Distributed Processing, edited byD.
E. Rumelhart and J. L. McClelland,MIT Press, 318-362.Schank, Roger C. (1980).
"Language andmemory," Cognitive Science, 4(3).Seo, Jungyun (1990).
Text driven constructionof discourse structures for understandingdescriptive t xts.
Doctoral dissertation,University of Texas, Austin.Sejnowski, Xerrence J., and Rosenberg, C.(1988).
"NETtalk: A parallel network thatlearns to read aloud."
In Neurocomputing,edited by Anderson and Rosenfeld, MITPress.Shieber, Stuart M. (1986).
An Introduction toUnification Based Approaches toGrammar.University of Chicago Press.Simmons, Robert F. (1984).
Computationsfrom the English.
Prentice Hall.Simmons, Robert F., and Yu, Yeong-Ho(1990).
"Training a neural network to be acontext sensitive grammar."
InProceedings, 5th Rocky Mountain AIConference.
Las Cruces, NM.Tomita, M. (1985).
Efficient Parsing for NaturalLanguage.
Kluwer Academic Publishers.Yu, Yeong-Ho, and Simmons, R. E (1990).
"Descending epsilon in back-propagation:A technique for better generalization," inpress, Proc.
Int.
Jt.
Conf.
Neural Networks,San Diego.416Robert E Simmons and Yeong-Ho Yu Context-Dependent Grammars for EnglishAppendix A.
Rules from the Case Acquisition SessionBlanks in the 10-symbol vectors are signified by the letter B.
((THE LAUNCH OF DISCOVERY AND ITS FIVE ASTRONAUTS HAS BEENDELAYED AT-LEAST TWO DAYS UNTIL MARCH ELEVENTH BECAUSE-OF ACRUSHED ELECTRICAL PART ON A MAIN ENGINE COMMA OFFICIALS SAID)(ART N OF N AND PPRON ADJ N VHAV VBE VAO AT-LEAST ADJ N UNTILN N BECAUSE-OF ART PPART ADJ N ON ART N N COMMA N VAO)(((B B B B B ART N OF N AND) (S))((B B B B ART N OF N AND PPRON) (S))((B B B ART N OF N AND PPRON ADJ) (I DET 2))((B B B B N OF N AND PPRON ADJ) (S))((B B B N OF N AND PPRON ADJ N) (S))((B B N OF N AND PPRON ADJ N VHAV) (S))((B N OF N AND PPRON ADJ N VHAV VBE) (S))((N OF N AND PPRON ADJ N VHAV VBE VAO) (S))((OF N AND PPRON ADJ N VHAV VBE VAO AT-LEAST) (S))((N AND PPRON ADJ N VHAV VBE VAO AT-LEAST ADJ) (i MOD 2))((OF N AND PPRON N VHAV VBE VAO AT-LEAST ADJ) (I POSSBY 2))((N OF N AND N VHAV VBE VAO AT-LEAST ADJ) (3 2 i))((B B N OF N VHAV VBE VAO AT-LEAST ADJ) (3 2 I))((B B B B N VHAV VBE VAO AT-LEAST ADJ) (S))((B B B N VHAV VBE VAO AT-LEAST ADJ N) (S))((B B N VHAV VBE VAO AT-LEAST ADJ N UNTIL) (i AUX 2))((B B B N VBE VAO AT-LEAST ADJ N UNTIL) (S))((B B N VBE VAO AT-LEAST ADJ N UNTIL N) (I VBE 2 VAOPASV))((B B B N VAOPASV AT-LEAST ADJ N UNTIL N) (i OBJ 2))((B B B B VAOPASV AT-LEAST ADJ N UNTIL N) (S))((B B B VAOPASV AT-LEAST ADJ N UNTIL N N) (S))((B B VAOPASV AT-LEAST ADJ N UNTIL N N BECAUSE-OF) (S))((B VAOPASV AT-LEAST ADJ N UNTIL N N BECAUSE-OF ART) (i MOD 2))((B B VAOPASV AT-LEAST N UNTIL N N BECAUSE-OF ART) (3 2 i))((B B B B VAOPASV UNTIL N N BECAUSE-OF ART) (S))((B B B VAOPASV UNTIL N N BECAUSE-OF ART PPART) (S))((B B VAOPASV UNTIL N N BECAUSE-OF ART PPART ADJ) (S))((B VAOPASV UNTIL N N BECAUSE-OF ART PPART ADJ N) (i NMOD 2))((B B VAOPASV UNTIL N BECAUSE-OF ART PPART ADJ N) (3 2 i))((B B B B VAOPASV BECAUSE-OF ART PPART ADJ N) (S))((B B B VAOPASV BECAUSE-OF ART PPART ADJ N ON)(i CONSE 2))((B B B B BECAUSE-OF ART PPART ADJ N ON) (S))((B B B BECAUSE-OF ART PPART ADJ N ON ART) (S))((B B BECAUSE-OF ART PPART ADJ N ON ART N) (S))((B BECAUSE-OF ART PPART ADJ N ON ART N N) (S))((BECAUSE-OF ART PPART ADJ N ON ART N N COMMA) (i MOD 2))((B BECAUSE-OF ART PPART N ON ART N N COMMA) (i MOD 2))417Computational Linguistics Volume 18, Number 4((B B BECAUSE-OF ART N ON ART N N COMMA) (i DET 2))((B B B BECAUSE-OF N ON ART N N COMMA) (S))((B B BECAUSE-OF N ON ART N N COMMA N) (S))((B BECAUSE-OF N ON ART N N COMMA N VAO) (S))((BECAUSE-OF N ON ART N N COMMA N VAO B) (S))((N ON ART N N COMMA N VAO B B) (i NMOD 2))((BECAUSE-OF N ON ART N COMMA N VAO B B) (i DET 2))((B BECAUSE-OF N ON N COMMA N VAO B B) (3 2 I))((B B B BECAUSE-OF N COMMA N VAO B B) (2 ANTE i))((B B B B BECAUSE-OF COMMA N VAO B B) (S))((B B B BECAUSE-OF COMMA N VAO B B B) (S))((B B BECAUSE-OF COMMA N VAO B B B B) (S))((B BECAUSE-OF COMMA N VAO B B B B B) (i AGT 2))((B B BECAUSE-OF COMMA VAO B B B B B) (I OBJ 3))))418
