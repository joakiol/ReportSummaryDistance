Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 513?520,Sydney, July 2006. c?2006 Association for Computational LinguisticsImproved Discriminative Bilingual Word AlignmentRobert C. Moore Wen-tau Yih Andreas BodeMicrosoft ResearchRedmond, WA 98052, USA{bobmoore,scottyhi,abode}@microsoft.comAbstractFor many years, statistical machine trans-lation relied on generative models to pro-vide bilingual word alignments.
In 2005,several independent efforts showed thatdiscriminative models could be used toenhance or replace the standard genera-tive approach.
Building on this work,we demonstrate substantial improvementin word-alignment accuracy, partly thoughimproved training methods, but predomi-nantly through selection of more and bet-ter features.
Our best model produces thelowest alignment error rate yet reported onCanadian Hansards bilingual data.1 IntroductionUntil recently, almost all work in statistical ma-chine translation was based on word alignmentsobtained from combinations of generative prob-abalistic models developed at IBM by Brown etal.
(1993), sometimes augmented by an HMM-based model or Och and Ney?s ?Model 6?
(Ochand Ney, 2003).
In 2005, however, several in-dependent efforts (Liu et al, 2005; Fraser andMarcu, 2005; Ayan et al, 2005; Taskar et al,2005; Moore, 2005; Ittycheriah and Roukos,2005) demonstrated that discriminatively trainedmodels can equal or surpass the alignment accu-racy of the standard models, if the usual unla-beled bilingual training corpus is supplementedwith human-annotated word alignments for onlya small subset of the training data.The work cited above makes use of varioustraining procedures and a wide variety of features.Indeed, whereas it can be difficult to design a fac-torization of a generative model that incorporatesall the desired information, it is relatively easy toadd arbitrary features to a discriminative model.We take advantage of this, building on our ex-isting framework (Moore, 2005), to substantiallyreduce the alignment error rate (AER) we previ-ously reported, given the same training and testdata.
Through a careful choice of features, andmodest improvements in training procedures, weobtain the lowest error rate yet reported for wordalignment of Canadian Hansards data.2 Overall ApproachAs in our previous work (Moore, 2005), we traintwo models we call stage 1 and stage 2, both inthe form of a weighted linear combination of fea-ture values extracted from a pair of sentences anda proposed word alignment of them.
The possiblealignment having the highest overall score is se-lected for each sentence pair.
Thus, for a sentencepair (e, f) we seek the alignment a?
such thata?
= argmaxan?i=1?ifi(a, e, f)where the fi are features and the ?i are weights.The models are trained on a large number of bilin-gual sentence pairs, a small number of whichhave hand-created word alignments provided tothe training procedure.
A set of hand alignmentsof a different subset of the overall training corpusis used to evaluate the models.In the stage 1 model, all the features are basedon surface statistics of the training data, plus thehypothesized alignment.
The entire training cor-pus is then automatically aligned using this model.The stage 2 model uses features based not onlyon the parallel sentences themselves but also onstatistics of the alignments produced by the stage5131 model.
The stage 1 model is discussed in Sec-tion 3, and the stage 2 model, in Section 4.
Afterexperimenting with many features and combina-tions of features, we made the final selection basedon minimizing training set AER.For alignment search, we use a method nearlyidentical to our previous beam search procedure,which we do not discuss in detail.
We made twominor modifications to handle the possiblity thatmore than one alignment may have the same score,which we previously did not take into account.First, we modified the beam search so that thebeam size dynamically expands if needed to ac-comodate all the possible alignments that have thesame score.
Second we implemented a structuraltie breaker, so that the same alignment will alwaysbe chosen as the one-best from a set of alignmentshaving the same score.
Neither of these changessignificantly affected the alignment results.The principal training method is an adaptationof averaged perceptron learning as described byCollins (2002).
The differences between our cur-rent and earlier training methods mainly addressthe observation that perceptron training is verysensitive to the order in which data is presented tothe learner.
We also investigated the large-margintraining technique described by Tsochantaridis etal.
(2004).
The training procedures are describedin Sections 5 and 6.3 Stage 1 ModelIn our previous stage 1 model, we used five fea-tures.
The most informative feature was the sumof bilingual word-association scores for all linkedword pairs, computed as a log likelihood ratio.
Weused two features to measure the degree of non-monotonicity of alignments, based on traversingthe alignment in the order of the source sentencetokens, and noting the instances where the corre-sponding target sentence tokens were not in left-to-right order.
One feature counted the number oftimes there was a backwards jump in the order ofthe target sentence tokens, and the other summedthe magnitudes of these jumps.
In order to modelthe trade-off between one-to-one and many-to-onealignments, we included a feature that counted thenumber of alignment links such that one of thelinked words participated in another link.
Our fifthfeature was the count of the number of words inthe sentence pair left unaligned.In addition to these five features, we employedtwo hard constraints.
One constraint was that theonly alignment patterns allowed were 1?1, 1?2, 1?3, 2?1, and 3?1.
Thus, many-to-many link pat-terns were disallowed, and a single word could belinked to at most three other words.
The secondconstraint was that a possible link was consideredonly if it involved the strongest degree of associ-ation within the sentence pair for at least one ofthe words to be linked.
If both words had strongerassociations with other words in the sentence pair,then the link was disallowed.Our new stage 1 model includes all the featureswe used previously, plus the constraint on align-ment patterns.
The constraint involving strongestassociation is not used.
In addition, our new stage1 model employs the following features:association score rank features We define therank of an association with respect to a word in asentence pair to be the number of association types(word-type to word-type) for that word that havehigher association scores, such that words of bothtypes occur in the sentence pair.
The contraint onstrength of association we previously used can bestated as a requirement that no link be consideredunless the corresponding association is of rank 0for at least one of the words.
We replace this hardconstraint with two features based on associationrank.
One feature totals the sum of the associa-tion ranks with respect to both words involved ineach link.
The second feature sums the minimumof association ranks with respect to both words in-volved in each link.
For alignments that obey theprevious hard constraint, the value of this secondfeature would always be 0.jump distance difference feature In our origi-nal models, the only features relating to word or-der were those measuring nonmonotonicity.
Thelikelihoods of various forward jump distanceswere not modeled.
If alignments are denseenough, measuring nonmonotonicity gets at thisindirectly; if every word is aligned, it is impossibleto have large forward jumps without correspond-ingly large backwards jumps, because somethinghas to link to the words that are jumped over.
Ifword alignments are sparse, however, due to freetranslation, it is possible to have alignments withvery different forward jumps, but the same back-wards jumps.
To differentiate such alignments,we introduce a feature that sums the differencesbetween the distance between consecutive aligned514source words and the distance between the closesttarget words they are aligned to.many-to-one jump distance features It seemsintuitive that the likelihood of a large forwardjump on either the source or target side of an align-ment is much less if the jump is between wordsthat are both linked to the same word of the otherlanguage.
This motivates the distinction betweenthe d1 and d>1 parameters in IBM Models 4 and 5.We model this by including two features.
One fea-ture sums, for each word w, the number of wordsnot linked to w that fall between the first and lastwords linked to w. The other features counts onlysuch words that are linked to some word other thanw.
The intuition here is that it is not so bad to havea function word not linked to anything, betweentwo words linked to the same word.exact match feature We have a feature thatsums the number of words linked to identicalwords.
This is motivated by the fact that propernames or specialized terms are often the same inboth languages, and we want to take advantage ofthis to link such words even when they are too rareto have a high association score.lexical features Taskar et al (2005) gain con-siderable benefit by including features countingthe links between particular high frequency words.They use 25 such features, covering all pairs ofthe five most frequent non-punctuation words ineach language.
We adopt this type of feature butdo so more agressively.
We include features forall bilingual word pairs that have at least two co-occurrences in the labeled training data.
In addi-tion, we include features counting the number ofunlinked occurrences of each word having at leasttwo occurrences in the labeled training data.In training our new stage 1 model, we were con-cerned that using so many lexical features mightresult in overfitting to the training data.
To try toprevent this, we train the stage 1 model by first op-timizing the weights for all other features, then op-timizing the weights for the lexical features, withthe other weights held fixed to their optimium val-ues without lexical features.4 Stage 2 ModelIn our original stage 2 model, we replaced the log-likelihood-based word association statistic withthe logarithm of the estimated conditional prob-ability of a cluster of words being linked by thestage 1 model, given that they co-occur in apair of aligned sentences, computed over the full(500,000 sentence pairs) training data.
We esti-mated these probabilities using a discounted max-imum likelihood estimate, in which a small fixedamount was subtracted from each link count:LPd(w1, .
.
.
, wk) =links1(w1, .
.
.
, wk)?
dcooc(w1, .
.
.
, wk)LPd(w1, .
.
.
, wk) represents the estimated condi-tional link probability for the cluster of wordsw1, .
.
.
, wk; links1(w1, .
.
.
, wk) is the number oftimes they are linked by the stage 1 model, d isthe discount; and cooc(w1, .
.
.
, wk) is the numberof times they co-occur.
We found that d = 0.4seemed to minimize training set AER.An important difference between our stage 1and stage 2 models is that the stage 1 model con-siders each word-to-word link separately, but al-lows multiple links per word, as long as they leadto an alignment consisting only of one-to-one andone-to-many links (in either direction).
The stage2 model, however, uses conditional probabilitiesfor both one-to-one and one-to-many clusters, butrequires all clusters to be disjoint.
Our originalstage 2 model incorporated the same addtional fea-tures as our original stage 1 model, except that thefeature that counts the number of links involved innon-one-to-one link clusters was omitted.Our new stage 2 model differs in a number ofways from the original version.
First we replacethe estimated conditional probability of a clusterof words being linked with the estimated condi-tional odds of a cluster of words being linked:LO(w1, .
.
.
, wk) =links1(w1, .
.
.
, wk) + 1(cooc(w1, .
.
.
, wk)?
links1(w1, .
.
.
, wk)) + 1LO(w1, .
.
.
, wk) represents the estimated con-ditional link odds for the cluster of wordsw1, .
.
.
, wk.
Note that we use ?add-one?
smooth-ing in place of a discount.Additional features in our new stage 2 model in-clude the unaligned word feature used previously,plus the following features:symmetrized nonmonotonicity feature Wesymmetrize the previous nonmonontonicity fea-ture that sums the magnitude of backwards jumps,by averaging the sum of of backwards jumps inthe target sentence order relative to the source515sentence order, with the sum of the backwardsjumps in the source sentence order relative to thetarget sentence order.
We omit the feature thatcounts the number of backwards jumps.multi-link feature This feature counts the num-ber of link clusters that are not one-to-one.
Thisenables us to model whether the link scores forthese clusters are more or less reliable than the linkscores for one-to-one clusters.empirically parameterized jump distance fea-ture We take advantage of the stage 1 alignmentto incorporate a feature measuring the jump dis-tances between alignment links that are more so-phisticated than simply measuring the differencein source and target distances, as in our stage 1model.
We measure the (signed) source and targetdistances between all pairs of links in the stage 1alignment of the full training data.
From this, weestimate the odds of each possible target distancegiven the corresponding source distance:JO(dt|ds) =C(target dist = dt ?
source dist = ds) + 1C(target dist 6= dt ?
source dist = ds) + 1We similarly estimate the odds of each possi-ble source distance given the corresponding targetdistance.
The feature values consist of the sumof the scaled log odds of the jumps between con-secutive links in a hypothesized alignment, com-puted in both source sentence and target sentenceorder.
This feature is applied only when both thesource and target jump distances are non-zero, sothat it applies only to jumps between clusters, notto jumps on the ?many?
side of a many-to-onecluster.
We found it necessary to linearly scalethese feature values in order to get good results (interms of training set AER) when using perceptrontraining.1 We found empirically that we could getgood results in terms of training set AER by divid-ing each log odds estimate by the largest absolutevalue of any such estimate computed.5 Perceptron TrainingWe optimize feature weights using a modificationof averaged perceptron learning as described byCollins (2002).
Given an initial set of featureweight values, the algorithm iterates through the1Note that this is purely for effective training, since aftertraining, one could adjust the feature weights according to thescale factor, and use the original feature values.labeled training data multiple times, comparing,for each sentence pair, the best alignment ahyp ac-cording to the current model with the referencealignment aref .
At each sentence pair, the weightfor each feature is is incremented by a multiple ofthe difference between the value of the feature forthe best alignment according to the model and thevalue of the feature for the reference alignment:?i ?
?i + ?
(fi(aref , e, f)?
fi(ahyp, e, f))The updated feature weights are used to computeahyp for the next sentence pair.
The multiplier ?is called the learning rate.
In the averaged percep-tron, the feature weights for the final model arethe average of the weight values over all the datarather than simply the values after the final sen-tence pair of the final iteration.Differences between our approach and Collins?sinclude averaging feature weights over each passthrough the data, rather than over all passes; ran-domizing the order of the data for each learn-ing pass; and performing an evaluation pass af-ter each learning pass, with feature weights fixedto their average values for the preceding learningpass, during which training set AER is measured.This procedure is iterated until a local minimumon training set AER is found.We initialize the weight of the anticipated most-informative feature (word-association scores instage 1; conditional link probabilities or odds instage 2) to 1.0, with other feature weights intial-ized to 0.
The weight for the most informative fea-ture is not updated.
Allowing all weights to varyallows many equivalent sets of weights that differonly by a constant scale factor.
Fixing one weighteliminates a spurious apparent degree of freedom.Previously, we set the learning rate ?
differentlyin training his stage 1 and stage 2 models.
For thestage 2 model, we used a single learning rate of0.01.
For the stage 1 model, we used a sequenceof learning rates: 1000, 100, 10, and 1.0.
At eachtransition between learning rates, we re-initializedthe feature weights to the optimum values foundwith the previous learning rate.In our current work, we make a number of mod-ifications to this procedure.
We reset the featureweights to the best averaged values we have yetseen at the begining of each learning pass throughthe data.
Anecdotally, this seems to result in fasterconvergence to a local AER minimum.
We alsouse multiple learning rates for both the stage 1 and516stage 2 models, setting the learning rates automat-ically.
The initial learning rate is the maximum ab-solute value (for one word pair/cluster) of the wordassociation, link probability, or link odds feature,divided by the number of labeled training sentencepairs.
Since many of the feature values are simplecounts, this allows a minimal difference of 1 inthe feature value, if repeated in every training ex-ample, to permit a count feature to have as largea weighted value as the most informative feature,after a single pass through the data.After the learning search terminates for a givenlearning rate, we reduce the learning rate by a fac-tor of 10, and iterate until we judge that we are ata local minimum for this learning rate.
We con-tinue with progressively smaller learning rates un-til an entire pass through the data produces fea-ture weights that differ so little from their valuesat the beginning of the pass that the training setAER does not change.Two final modifications are inspired by the real-ization that the results of perceptron training arevery sensitive to the order in which the data ispresented.
Since we randomize the order of thedata on every pass, if we make a pass through thetraining data, and the training set AER increases, itmay be that we simply encountered an unfortunateordering of the data.
Therefore, when training setAER increases, we retry two additional times withthe same initial weights, but different random or-derings of the data, before giving up and trying asmaller learning rate.
Finally, we repeat the entiretraining process multiple times, and average thefeature weights resulting from each of these runs.We currently use 10 runs of each model.
This finalaveraging is inspired by the idea of ?Bayes-pointmachines?
(Herbrich and Graepel, 2001).6 SVM TrainingAfter extensive experiments with perceptron train-ing, we wanted to see if we could improve the re-sults obtained with our best stage 2 model by usinga more sophisticated training method.
Perceptrontraining has been shown to obtain good results forsome problems, but occasionally very poor resultsare reported, notably by Taskar et al (2005) for theword-alignment problem.
We adopted the supportvector machine (SVM) method for structured out-put spaces of Tsochantaridis et al (2005), usingJoachims?
SV M struct package.Like standard SVM learning, this method triesto find the hyperplane that separates the trainingexamples with the largest margin.
Despite a verylarge number of possible output labels (e.g., allpossible alignments of a given pair of sentences),the optimal hyperplane can be efficiently approx-imated given the desired error rate, using a cut-ting plane algorithm.
In each iteration of the al-gorithm, it adds the ?best?
incorrect predictionsgiven the current model as constraints, and opti-mizes the weight vector subject only to them.The main advantage of this algorithm is thatit does not pose special restrictions on the out-put structure, as long as ?decoding?
can be doneefficiently.
This is crucial to us because sev-eral features we found very effective in this taskare difficult to incorporate into structured learningmethods that require decomposable features.
Thismethod also allows a variety of loss functions, butwe use only simple 0-1 loss, which in our casemeans whether or not the alignment of a sentencepair is completely correct, since this worked aswell as anything else we tried.Our SVM method has a number of free param-eters, which we tried tuning in two different ways.One way is minimizing training set AER, whichis how we chose the stopping points in perceptrontraining.
The other is five-fold cross validation.In this method, we train five times on 80% of thetraining data and test on the other 20%, with fivedisjoint subsets used for testing.
The parametervalues yielding the best averaged AER on the fivetest subsets of the training set are used to train thefinal model on the entire training set.7 EvaluationWe used the same training and test data as in ourprevious work, a subset of the Canadian Hansardsbilingual corpus supplied for the bilingual wordalignment workshop held at HLT-NAACL 2003(Mihalcea and Pedersen, 2003).
This subset com-prised 500,000 English-French sentences pairs, in-cluding 224 manually word-aligned sentence pairsfor labeled training data, and 223 labeled sen-tences pairs as test data.
Automatic sentencealignment of the training data was provided by Ul-rich Germann, and the hand alignments of the la-beled data were created by Franz Och and Her-mann Ney (Och and Ney, 2003).For baselines, Table 1 shows the test set re-sults we previously reported, along with results forIBM Model 4, trained with Och?s Giza++ software517Alignment Recall Precision AERPrev LLR 0.829 0.848 0.160CLP1 0.889 0.934 0.086CLP2 0.898 0.947 0.075Giza E?
F 0.870 0.890 0.118Giza F?
E 0.876 0.907 0.106Giza union 0.929 0.845 0.124Giza intersection 0.817 0.981 0.097Giza refined 0.908 0.929 0.079Table 1: Baseline Results.package, using the default configuration file (Ochand Ney, 2003).2 ?Prev LLR?
is our earlier stage1 model, and CLP1 and CLP2 are two versionsof our earlier stage 2 model.
For CLP1, condi-tional link probabilities were estimated from thealignments produced by our ?Prev LLR?
model,and for CLP2, they were obtained from a yetearlier, heuristic alignment model.
Results forIBM Model 4 are reported for models trained inboth directions, English-to-French and French-to-English, and for the union, intersection, and whatOch and Ney (2003) call the ?refined?
combina-tion of the those two alignments.Results for our new stage 1 model are presentedin Table 2.
The first line is for the model describedin Section 3, optimizing non-lexical features be-fore lexical features.
The second line gives resultsfor optimizing all features simultaneously.
Thenext line omits lexical features entirely.
The lastline is for our original stage 1 model, but trainedusing our improved perceptron training method.As we can see, our best stage 1 model reducesthe error rate of previous stage 1 model by almosthalf.
Comparing the first two lines shows that two-phase training of non-lexical and lexical featuresproduces a 0.7% reduction in test set error.
Al-though the purpose of the two-phase training wasto mitigate overfitting to the training data, we alsofound training set AER was reduced (7.3% vs.8.8%).
Taken all together, the results show a 7.9%total reduction in error rate: 4.0% from new non-lexical features, 3.3% from lexical features withtwo-phase training, and 0.6% from other improve-ments in perceptron training.Table 3 presents results for perceptron trainingof our new stage 2 model.
The first line is for themodel as described in Section 4.
Since the use oflog odds is somewhat unusual, in the second line2Thanks to Chris Quirk for providing Giza++ alignments.Alignment Recall Precision AERTwo-phase train 0.907 0.928 0.081One-phase train 0.911 0.912 0.088No lex feats 0.889 0.885 0.114Prev LLR (new train) 0.834 0.855 0.154Table 2: Stage 1 Model Results.Alignment Recall Precision AERLog odds 0.935 0.964 0.049Log probs 0.934 0.962 0.051CLP1 (new A & T) 0.925 0.952 0.060CLP1 (new A) 0.917 0.955 0.063Table 3: Stage 2 Model Results.we show results for a similiar model, but using logprobabilities instead of log odds for both the linkmodel and the jump model.
This result is 0.2%worse than the log-odds-based model, but the dif-ference is small enough to warrant testing its sig-nificance.
Comparing the errors on each test sen-tence pair with a 2-tailed paired t test, the resultswere suggestive, but not significant (p = 0.28)The third line of Table 3 shows results for ourearlier CLP1 model with probabilities estimatedfrom our new stage 1 model alignments (?newA?
), using our recent modifications to perceptrontraining (?new T?).
These results are significantlyworse than either of the two preceding models(p < 0.0008).
The fourth line is for the samemodel and stage 1 alignments, but with our earlierperceptron training method.
While the results are0.3% worse than with our new training method,the difference is not significant (p = 0.62).Table 4 shows the results of SVM training ofthe model that was best under perceptron training,tuning free parameters either by minimizing erroron the entire training set or by 5-fold cross val-idation on the training set.
The cross-validationmethod produced slightly lower test-set AER, butboth results rounded to 4.7%.
While these resultsare somewhat better than with perceptron training,the differences are not significant (p ?
0.47).8 Comparisons to Other WorkAt the time we carried out the experiments de-scribed above, our sub-5% AER results were thebest we were aware of for word alignment ofCanadian Hansards bilingual data, although directcomparisons are problematic due to differences in518Alignment Recall Precision AERMin train err 0.941 0.962 0.0475 ?
CV 0.942 0.962 0.047Table 4: SVM Training Results.total training data, labeled training data, and testdata.
The best previously reported result was byOch and Ney (2003), who obtained 5.2% AERfor a combination including all the IBM mod-els except Model 2, plus the HMM model andtheir Model 6, together with a bilingual dictionary,for the refined alignment combination, trained onthree times as much data as we used.Cherry and Lin?s (2003) method obtained anAER of 5.7% as reported by Mihalcea and Peder-sen (2003), the previous lowest reported error ratefor a method that makes no use of the IBM mod-els.
Cherry and Lin?s method is similar to oursin using explicit estimates of the probability of alink given the co-occurence of the linked words;but it is generative rather than discriminative, it re-quires a parser for the English side of the corpus,and it does not model many-to-one links.
Taskaret al (2005) reported 5.4% AER for a discrimina-tive model that includes predictions from the inter-section of IBM Model 4 alignments as a feature.Their best result without using information fromthe IBM models was 10.7% AER.After completing the experiments described inSection 7, we became aware further developmentsin the line of research reported by Taskar et al(Lacoste-Julien et al, 2006).
By modifying theirprevious approach to allow many-to-one align-ments and first-order interactions between align-ments, Lacoste-Julien et al have improved theirbest AER without using information from themore complex IBM models to 6.2%.
Their bestresult, however, is obtained from a model that in-cludes both a feature recording intersected IBMModel 4 predictions, plus a feature whose val-ues are the alignment probabilities obtained from apair of HMM alignment models trained in both di-rections in such a way that they agree on the align-ment probabilities (Liang et al, 2006).
With thismodel, they obtained a much lower 3.8% AER.Lacoste-Julien very graciously provided boththe IBM Model 4 predictions and the probabili-ties estimated by the bidirectional HMM modelsthat they had used to compute these additional fea-ture values.
We then added features based on thisinformation to see how much we could improveour best model.
We also eliminated one other dif-ference between our results and those of Lacoste-Julien et al, by training on all 1.1 million English-French sentence pairs from the 2003 word align-ment workshop, rather than the 500,000 sentencepairs we had been using.Since all our other feature values derived fromprobabilities are expressed as log odds, we alsoconverted the HMM probabilities estimated byLiang et al to log odds.
To make this well de-fined in all cases, we thresholded high probabili-ties (including 1.0) at 0.999999, and low probabil-ities (including 0.0) at 0.1 (which we found pro-duced lower training set error than using a verysmall non-zero probability, although we have notsearched systematically for the optimal value).In our latest experiments, we first establishedthat simply increasing the unlabled training datato 1.1 million sentence pairs made very little dif-ference, reducing the test-set AER of our stage 2model under perceptron training only from 4.9%to 4.8%.
Combining our stage 2 model featureswith the HMM log odds feature using SVM train-ing with 5-fold cross validation yielded a substan-tial reduction in test-set AER to 3.9% (96.9% pre-cision, 95.1% recall).
We found it somewhat dif-ficult to improve these results further by includingIBM Model 4 intersection feature.
We finally ob-tained our best results, however, for both training-set and test-set AER, by holding the stage 2 modelfeature weights at the values obtained by SVMtraining with the HMM log odds feature, and op-timizing the HMM log odds feature weight andIBM Model 4 intersection feature weight with per-ceptron training.3 This produced a test-set AER of3.7% (96.9% precision, 95.5% recall).9 ConclusionsFor Canadian Hansards data, the test-set AER of4.7% for our stage 2 model is one of the lowestyet reported for an aligner that makes no use ofthe expensive IBM models, and our test-set AERof 3.7% for the stage 2 model in combination withthe HMM log odds and Model 4 intersection fea-tures is the lowest yet reported for any aligner.4Perhaps if any general conclusion is to be drawnfrom our results, it is that in creating a discrim-3At this writing we have not yet had time to try this withSVM training.4However, the difference between our result and the 3.8%of Lacoste-Julien et al is almost certainly not significant.519inative word alignment model, the model struc-ture and features matter the most, with the dis-criminative training method of secondary impor-tance.
While we obtained a small improvementsby varying the training method, few of the differ-ences were statistically significant.
Having betterfeatures was much more important.ReferencesNecip Fazil Ayan, Bonnie J. Dorr, andChristof Monz.
2005.
NeurAlign: CombiningWord Alignments Using Neural Networks.
InProceedings of the Human Language Technol-ogy Conference and Conference on EmpiricalMethods in Natural Language Processing,pp.
65?72, Vancouver, British Columbia.Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra, and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation:Parameter Estimation.
Computational Linguis-tics, 19(2):263?311.Colin Cherry and Dekang Lin.
2003.
A Proba-bility Model to Improve Word Alignment.
InProceedings of the 41st Annual Meeting of theACL, pp.
88?95, Sapporo, Japan.Michael Collins.
2002.
Discriminative TrainingMethods for Hidden Markov Models: Theoryand Experiments with Perceptron Algorithms.In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing,pp.
1?8, Philadelphia, Pennsylvania.Alexander Fraser and Daniel Marcu.
2005.
ISI?sParticipation in the Romanian-English Align-ment Task.
In Proceedings of the ACL Work-shop on Building and Using Parallel Texts,pp.
91?94, Ann Arbor, Michigan.Ralf Herbrich and Thore Graepel.
2001.
LargeScale Bayes Point Machines Advances.
InNeural Information Processing Systems 13,pp.
528?534.Abraham Ittycheriah and Salim Roukos.
2005.
AMaximum Entropy Word Aligner for Arabic-English Machine Translation.
In Proceedingsof the Human Language Technology Conferenceand Conference on Empirical Methods in Nat-ural Language Processing, pp.
89?96, Vancou-ver, British Columbia.Simon Lacoste-Julien, Ben Taskar, Dan Klein, andMichael Jordan.
2006.
Word Alignment viaQuadratic Assignment.
In Proceedings of theHuman Language Technology Conference of theNorth American Chapter of the Association forComputational Linguistics, pp.
112?119, NewYork City.Percy Liang, Ben Taskar, and Dan Klein.
2006.Alignment by Agreement.
In Proceedings of theHuman Language Technology Conference of theNorth American Chapter of the Association forComputational Linguistics, pp.
104?111, NewYork City.Yang Liu, Qun Liu, and Shouxun Lin.
2005.
Log-linear Models for Word Alignment.
In Proceed-ings of the 43rd Annual Meeting of the ACL,pp.
459?466, Ann Arbor, Michigan.Rada Mihalcea and Ted Pedersen.
2003.
An Eval-uation Exercise for Word Alignment.
In Pro-ceedings of the HLT-NAACL 2003 Workshop,Building and Using Parallel Texts: Data DrivenMachine Translation and Beyond, pp.
1?6, Ed-monton, Alberta.Robert C. Moore.
2005.
A Discriminative Frame-work for Bilingual Word Alignment.
In Pro-ceedings of the Human Language TechnologyConference and Conference on Empirical Meth-ods in Natural Language Processing, pp.
81?88, Vancouver, British Columbia.Franz Joseph Och and Hermann Ney.
2003.
ASystematic Comparison of Various StatisticalAlignment Models.
Computational Linguistics,29(1):19?51.Ben Taskar, Simon Lacoste-Julien, and Dan Klein.2005.
A Discriminative Matching Approachto Word Alignment.
In Proceedings of theHuman Language Technology Conference andConference on Empirical Methods in NaturalLanguage Processing, pp.
73?80, Vancouver,British Columbia.Ioannis Tsochantaridis, Thomas Hofmann,Thorsten Joachims, and Yasemin Altun.
2005.Large Margin Methods for Structured andInterdependent Output Variables.
Journalof Machine Learning Research (JMLR),pp.
1453?1484.520
