Proceedings of NAACL-HLT 2013, pages 661?667,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsSupersense Tagging for Arabic: the MT-in-the-Middle AttackNathan Schneider?
Behrang Mohit?
Chris Dyer?
Kemal Oflazer?
Noah A. Smith?School of Computer ScienceCarnegie Mellon University?Pittsburgh, PA 15213, USA?Doha, Qatar{nschneid@cs.,behrang@,cdyer@cs.,ko@cs.,nasmith@cs.
}cmu.eduAbstractWe consider the task of tagging Arabic nounswith WordNet supersenses.
Three approachesare evaluated.
The first uses an expert-crafted but limited-coverage lexicon, ArabicWordNet, and heuristics.
The second uses un-supervised sequence modeling.
The third andmost successful approach uses machine trans-lation to translate the Arabic into English,which is automatically tagged with Englishsupersenses, the results of which are then pro-jected back into Arabic.
Analysis shows gainsand remaining obstacles in four Wikipediatopical domains.1 IntroductionA taxonomic view of lexical semantics groups wordsenses/usages into categories of varying granulari-ties.
WordNet supersense tags denote coarse seman-tic classes, including person and artifact (for nouns)and motion and weather (for verbs); these categoriescan be taken as the top level of a taxonomy.
Nominalsupersense tagging (Ciaramita and Johnson, 2003)is the task of identifying lexical chunks in the sen-tence for common as well as proper nouns, and la-beling each with one of the 25 nominal supersensecategories.
Figure 1 illustrates two such labelings ofan Arabic sentence.
Like the narrower problem ofnamed entity recognition, supersense tagging of textholds attraction as a way of inferring representationsthat move toward language independence.
Here weconsider the problem of nominal supersense taggingfor Arabic, a language with ca.
300 million speak-ers and moderate linguistic resources, including aWordNet (Elkateb et al 2006), annotated datasets(Maamouri et al 2004; Hovy et al 2006), monolin-gual corpora, and large amounts of Arabic-Englishparallel data.The supervised learning approach that is usedin state-of-the-art English supersense taggers (Cia-.
HA?JJ.?J?
@Y?
@?K ??
??
???
??Y?
@?J?
@ QKY?
?
?jJKAnn-A Gloss Ann-Bcontrolscommunicationmanagercommunicationthe-windowsinattribute configuration relationshape and-layout shapecommunicationwindowscommunicationthe-applications?The window manager controls the configuration andlayout of application windows.
?Figure 1: A sentence from the ?X Window System?
ar-ticle with supersense taggings from two annotators andpost hoc English glosses and translation.ramita and Altun, 2006) is problematic for Ara-bic, since there are supersense annotations for onlya small amount of Arabic text (65,000 words bySchneider et al 2012, versus the 360,000 words thatare annotated for English).
Here, we reserve thatcorpus for development and evaluation, not training.We explore several approaches in this paper, themost effective of which is to (1) translate the Arabicsentence into English, returning the alignment struc-ture between the source and target, (2) apply En-glish supersense tagging to the target sentence, and(3) heuristically project the tags back to the Arabicsentence across these alignments.
This ?MT-in-the-middle?
approach has also been successfully usedfor mention detection (Zitouni and Florian, 2008)and coreference resolution (Rahman and Ng, 2012).We first discuss the task and relevant resources(?2), then the approaches we explored (?3), and fi-nally present experimental results and analysis in ?4.2 Task and ResourcesA gold standard corpus of sentences annotatedwith nominal supersenses (as in figure 1) fa-cilitates automatic evaluation of supersense tag-gers.
For development and evaluation we use661the AQMAR Arabic Wikipedia Supersense Corpus1(Schneider et al 2012), which augmented a namedentity corpus (Mohit et al 2012) with nominalsupersense tags.
The corpus consists of 28 ar-ticles selected from four topical areas: history(e.g., ?Islamic Golden Age?
), science (?Atom?
),sports (?Real Madrid?
), and technology (?Linux?
).Schneider et al(2012) found the distributions ofsupersense categories in these four topical domainsto be markedly different; e.g., most instances ofcommunication (which includes kinds of software)occurred in the technology domain, whereas mostsubstances were found in the science domain.The 18 test articles have 1,393 sentences (39,916tokens) annotated at least once.2 As the corpuswas released with two annotators?
(partially overlap-ping) taggings, rather than a single gold standard,we treat the output of each annotator as a separatetest set.
Both annotated some of every article; thefirst (Ann-A) annotated 759 sentences, the second(Ann-B) 811 sentences.Lexicon.
What became known as ?supersensetags?
arose from a high-level partitioning of synsetsin the original English WordNet (Fellbaum, 1998)into lexicographer files.
Arabic WordNet (AWN)(Elkateb et al 2006) allows us to recover super-sense categories for some 10,500 Arabic nominaltypes, since many of the synsets in AWN are cross-referenced to English WordNet, and can thereforebe associated with supersense categories.
Further,OntoNotes contains named entity annotations forArabic (Hovy et al 2006).From these, we construct an Arabic supersenselexicon, mapping Arabic noun lemmas to supersensetags.
This lexicon contains 23,000 types, of which11,000 are multiword units.
Token coverage of thetest set is 18% (see table 1).
Lexical units encoun-tered in the test data were up to 9-ways supersense-ambiguous; the average ambiguity of in-vocabularytokens was 2.0 supersenses.Unlabeled Arabic text.
For unsupervised learn-ing we collected 100,000 words of Arabic Wikipediatext, not constrained by topic.
The articles in thissample were subject to a minimum length threshold1http://www.ark.cs.cmu.edu/ArabicSST2Our development/test split of the data follows Mohit et al(2012), but we exclude two test set documents??Light?
and?Ibn Tolun Mosque?
?due to preprocessing issues.and are all cross-linked to corresponding articles inEnglish, Chinese, and German.Arabic?English machine translation.
We usedtwo independently developed Arabic-English MTsystems.
One (QCRI) is a phrase-based system(Koehn et al 2003), similar to Moses (Koehn etal., 2007); the other (cdec) is a hierarchical phrase-based system (Chiang, 2007), as implemented incdec (Dyer et al 2010).
Both were trained onabout 370M tokens of parallel data provided by theLDC (by volume, mostly newswire and UN data).Each system includes preprocessing for Arabic mor-phological segmentation and orthographic normal-ization.3 The QCRI system used a 5-gram modi-fied Kneser-Ney language model that generated full-cased forms (Chen and Goodman, 1999).
cdecused a 4-gram KN language model over lowercaseforms and was recased in a post-processing step.Both language models were trained using the Giga-word v. 4 corpus.
Both systems were tuned to opti-mize BLEU on a held-out development set (Papineniet al 2002).English supersense tagger.
For English super-sense tagging, an open-source reimplementation ofthe approach of Ciaramita and Altun (2006) wasreleased by Michael Heilman.4 This tagger wastrained on the SemCor corpus (Miller et al 1993)and achieves 77% F1 in-domain.3 MethodsWe explored 3 approaches to the supersense taggingof Arabic: heuristic tagging with a lexicon, unsuper-vised sequence tagging, and MT-in-the-middle.3.1 Heuristic Tagging with a LexiconUsing the lexicon built from AWN and OntoNotes(see ?2), our heuristic approach works as follows:1.
Stem and vocalize; we used MADA (Habashand Rambow, 2005; Roth et al 2008).2.
Greedily detect word sequences matching lexi-con entries from left to right.3.
If a lexicon entry has more than one associatedsupersense, Arabic WordNet synsets are3QCRI accomplishes this using MADA (Habash and Ram-bow, 2005; Roth et al 2008).
cdec includes a custom CRF-based segmenter and standard normalization rules.4http://www.ark.cs.cmu.edu/mheilman/questions662E?
person location artifact substance Automatic English supersense tagginge?
1 2 3 4 5 6 7 8 9 English sentencea 1 2 3 4 5 6 Arabic sentence (e.g., token 6 aligns to English tokens 7?9)N P N A N N Arabic POS taggingA?
person location artifact Projected supersense taggingFigure 2: A hypothetical aligned sentence pair of 9 English words (with their supersense tags) and 6 Ara-bic words (with their POS tags).
Step 4 of the projection procedure constructs the Arabic-to-English mapping{1?person11, 4?location43, {5, 6}?artifact76}, resulting in the tagging shown in the bottom row.weighted to favor earlier senses (presumedby lexicographers to be more frequent) andthen the supersense with the greatest aggregateweight is selected.
Formally: Let senses(w) bethe ordered list of AWN senses of lemma w.Let senses(w, s) ?
senses(w) be those sensesthat map to a given supersense s. We choosearg maxs(|senses(w, s)|/ mini:senses(w)i?senses(w,s) i).3.2 Unsupervised Sequence ModelsUnsupervised sequence labeling is our second ap-proach (Merialdo, 1994).
Although it was largelydeveloped for part-of-speech tagging, the hope isto use in-domain Arabic data (the unannotatedWikipedia corpus discussed in ?2) to infer clus-ters that correlate well with supersense groupings.We applied the generative, feature-based model ofBerg-Kirkpatrick et al(2010), replicating a feature-set used previously for NER (Mohit et al 2012)?including context tokens, character n-grams, andPOS?and adding the vocalized stem and severalstem shape features: 1) ContainsDigit?
; 2) dig-its replaced by #; 3) digit sequences replaced by# (for stems mixing digits with other characters);4) YearLike?
?true for 4-digit numerals starting with19 or 20; 5) LatinWord?, per the morphological an-alysis; 6) the shape feature of Ciaramita and Al-tun (2006) (Latin words only).
We used 50 itera-tions of learning (tuned on dev data).
For evaluation,a many-to-one mapping from unsupervised clustersto supersense tags is greedily induced to maximizetheir correspondence on evaluation data.3.3 MT-in-the-MiddleA standard approach to using supervised linguisticresources in a second language is cross-lingual pro-jection (Yarowsky and Ngai, 2001; Yarowsky et al2001; Smith and Smith, 2004; Hwa et al 2005; Mi-halcea et al 2007; Burkett and Klein, 2008; Burkettet al 2010; Das and Petrov, 2011; Kim et al 2012,who use parallel sentences extracted from Wikipediafor NER).
The simplest such approach starts with analigned parallel corpus, applies supersense taggingto the English side, and projects the labels throughthe word alignments.
A supervised monolingual tag-ger is then trained on the projected labels.
Prelimi-nary experiments, however, showed that this under-performed even the simple heuristic baseline above(likely due to domain mismatch), so it was aban-doned in favor of a technique that we call MT-in-the-middle projection.This approach does not depend on having par-allel data in the training domain, but rather on anArabic?English machine translation system thatcan be applied to the sentences we wish to tag.
Theapproach is inspired by token-level pseudo-paralleldata methods of previous work (Zitouni and Flo-rian, 2008; Rahman and Ng, 2012).
MT output forthis language pair is far from perfect?especially forWikipedia text, which is distant from the domainof the translation system?s training data?but, in thespirit of Church and Hovy (1993), we conjecture thatit may still be useful.
The method is as follows:1.
Preprocess the input Arabic sentence a tomatch the decoder?s model of Arabic.2.
Translate the sentence, recovering not justthe English output e?
but also the deriva-tion/alignment structure z relating words and/orphrases of the English output to words and/orphrases of the Arabic input.3.
Apply the English supersense tagger to the En-glish translation, discarding any verbal super-sense tags.
Call the tagger output E?.4.
Project the supersense tags back to the Ara-bic sentence, yielding A?
: Each Arabic tokena ?
a that is (a) a noun, or (b) an adjec-tive following 0 or more adjectives following anoun is mapped to the first English supersensemention in E?
containing some word alignedto a.
Then, for each English supersense men-663Coverage Ann-A Ann-BNouns All Tokens Mentions P R F1 P R F1Lexicon heuristics (?3.1) 8,058 33% 8,465 18% 8,407 32 55 16 29 21.6 37.9 29 53 15 27 19.4 35.6Unsupervised (?3.2) 20 59 16 48 17.5 52.6 14 56 10 39 11.6 45.9MT-in-the-middle(?3.3)QCRI 14,401 59% 16,461 35% 12,861 34 65 27 50 29.9 56.4 36 64 28 51 31.6 56.6cdec 14,270 58% 15,542 33% 13,704 37 69 31 57 33.8 62.4 38 67 32 56 34.6 61.0MTitM + Lex.
cdec 16,798 68% 18,461 40% 16,623 35 64 36 65 35.5 64.6 36 63 36 63 36.0 63.2Table 1: Supersense tagging results on the test set: coverage measures5 and gold-standard evaluation?exact la-beled/unlabeled6 mention precision, recall, and F-score against each annotator.
The last row is a hybrid: MT-in-the-middle followed by lexicon heuristics to improve recall.
Best single-technique and best hybrid results are bolded.tion, all its mapped Arabic words are groupedinto a single mention and the supersense cat-egory for that mention is projected.
Figure 2illustrates this procedure.
The cdec systemprovides word alignments for its translationsderived from the training data; whereas QCRIonly produces phrase-level alignments, so forevery aligned phrase pair ?a?, e??
?
z, we con-sider every word in a?
as aligned to every wordin e?
(introducing noise when English super-sense mention boundaries do not line up withphrase boundaries).4 Experiments and AnalysisTable 1 compares the techniques (?3) for full Arabicsupersense tagging.7 The number of nouns, tokens,and mentions covered by the automatic tagging isreported, as is the mention-level evaluation againsthuman annotations.
The evaluation is reported sep-arately for the two annotators in the dataset.With heuristic lexicon lookup, 18% of the tokensare marked as part of a nominal supersense mention.Both labeled and unlabeled mention recall with thismethod are below 30%; labeled precision is about30%, and unlabeled mention precision is above50%.
From this we conclude that the biggest prob-lems are (a) out-of-vocabulary items and (b) poorsemantic disambiguation of in-vocabulary items.The unsupervised sequence tagger does evenworse on the labeled evaluation.
It has some successat detecting supersense mentions?unlabeled recallis substantially improved, and unlabeled precision is5The unsupervised evaluation greedily maps clusters to tags,separately for each version of the test set; coverage numbersthus differ and are not shown here.6Unlabeled tagging refers to noun chunk detection only.7It was produced in part using the chunkeval.py script: seehttps://github.com/nschneid/pyutilslightly improved.
But it seems to be much worseat assigning semantic categories; the number of la-beled true positive mentions is actually lower thanwith the lexicon-based approach.MT-in-the-middle is by far the most success-ful single approach: both systems outperform thelexicon-only baseline by about 10 F1 points, de-spite many errors in the automatic translation, En-glish tagging, and projection, as well as underlyinglinguistic differences between English and Arabic.The baseline?s unlabeled recall is doubled, indicat-ing substantially more nominal expressions are de-tected, in addition to the improved labeled scores.We further tested simple hybrids combining thelexicon-based and MT-based approaches.
ApplyingMT-in-the-middle first, then expanding token cover-age with the lexicon improves recall at a small costto precision (table 1, last row).
Combining the tech-niques in the reverse order is slightly worse than MT-based projection without consulting the lexicon.MT-in-the middle improves upon the lexicon-onlybaseline, yet performance is still dwarfed by the su-pervised English tagger (at least in the SemCor eval-uation; see ?2), and also well below the 70% inter-annotator F1 reported by Schneider et al(2012).
Wetherefore examine the weaknesses of our approachfor Arabic.4.1 MT for ProjectionIn analyzing our projection framework, we per-formed a small-scale MT evaluation with theWikipedia data.
Reference English translations for140 Arabic Wikipedia sentences?5 per article inthe corpus?were elicited from a bilingual linguist.Table 2 compares the two systems under three stan-dard metrics of overall sentence translation quality.88BLEU (Papineni et al 2002); METEOR (Banerjee andLavie, 2005; Lavie and Denkowski, 2009), with default options;664.
????
@ ??
@Yg.?Q?
??Jj ??@?J.k.
???
@?K ?
?k ?
?m' ( HAK?Q?
?B@ )?J.
?
A??
@HAJj ??
@ 	???K.
Am?
???PY?@??
?JKQCRI: .......corn consists of a negative ................shipments ( electron hovering around the nucleus of the ...............shipment ) are very small in the center .
(3/6)cdec: The .......corn is composed of negative ................shipments ( .................electronics ) cloud hovering over the nucleus of a very small positive ...............shipment in thecenter .
(2/6).
HAJ??J?
@ P@?
??
?@??
?
Q?E ??
?A 	?KQ.
?
A?
,????D??
@ ???DJ??.
HAJKA?D??
?A 	?KQ.
?
@I?
?AKQCRI: Portugal qualified for the finals very easily , Portugal defeated throughout the ............mission ..................liquidations .
(3/5)cdec: Portugal qualified easily for the finals , Portugal unbeaten throughout the ...........journey .
(3/4)Figure 3: Example Arabic inputs and the outputs of the two MT systems, with lexical projection precision ratios.While the resulting number of sentences with refer-ences is far from ideal and there is only one refer-ence translation for each, all three measures favorQCRI.For a targeted measure of lexical translation qual-ity of noun expressions, we elicited acceptabilityjudgments from a bilingual annotator for translatedand supersense-projected phrases.
Given each MTsystem output (for the same 140 sentences) in whichmentions predicted by the English supervised taggerwere highlighted, along with the Arabic source sen-tence, the judge was asked for each English mentionwhether it was a valid translation.9 We call this lexi-cal projection precision.
Figure 3 shows examples,and the last column of table 2 gives overall statistics.Upwards of 90% of the lexical translations were ac-cepted; as with the automatic MT measures, QCRIslightly outperforms cdec (especially in the technol-ogy and sports domains10).
Of the problematic lex-ical translations, some are almost certainly domaineffects: e.g., corn or maize instead of atom.
Othersare more nuanced, e.g., shipments for charges andelectronics for electrons.
Transliteration errors in-cluded IMAX in place of EMACS and genoa lynx forGNU Linux.
However, lexical projection precisionseems to be a relatively small part of the problem,especially considering that not all translation errorslead to supersense tagging errors.Lexical projection recall was not measured, butnoun token coverage (see table 1) is instructive.Most noun tokens ought to be tagged; 77% is thenoun coverage rate in the gold standard.
In table 1,and translation edit rate (TER) (Snover et al 2006)9The judge did not see alignments or supersense categories.10For technology articles, the differences in F1 scores be-tween the two systems were 6.1 and 4.2 for Ann-A and Ann-B,respectively.
For sports the respective differences were 4.3 and4.4.
In the other domains the differences never exceeded 3.3.
In-terestingly, this is the only generalization about topical domainperformance we were able to find that holds across annotatorsand systems, in contrast with the stark topical effects observedby Mohit et al(2012) for NER.BLEU METEOR TER Lex.
Prec.QCRI 32.86 32.10 0.46 91.9%cdec 28.84 31.38 0.49 90.0%Table 2: MT quality measures comparing the two sys-tems over 140 sentences.
The first three are automaticmeasures with 1 reference translation.
For the fourth, abilingual judged the translation acceptability of phrasesthat were identified as supersense mentions by the En-glish tagger (lexical projection precision).noun coverage gains track overall improvements.If QCRI produces better translations, why is cdecmore useful for supersense tagging?
As noted in?3.3, cdec gives word-level alignments (even whenthe decoder uses large phrasal units for translation),allowing for more precise projections.11 We suspectthis is especially important in light of findings thatlarger phrase sizes are indicative of better transla-tions (Gamon et al 2005), so these are exactly thecases where we expect the translations to be valu-able.5 ConclusionTo our knowledge, this is the first study of automaticArabic supersense tagging.
We have shown empiri-cally that an MT-in-the-middle technique is most ef-fective of several approaches that do not require la-beled training data.
Analysis sheds light on severalchallenges that would need to be overcome for betterArabic lexical semantic tagging.AcknowledgmentsWe thank Wajdi Zaghouani for the translation of the Ara-bic Wikipedia MT set, Francisco Guzman and PreslavNakov for the output of QCRI?s MT system, and WaleedAmmar and anonymous reviewers for their comments.This publication was made possible by grant NPRP-08-485-1-083 from the Qatar National Research Fund (amember of the Qatar Foundation).
The statements madeherein are solely the responsibility of the authors.11Our experiments use QCRI as an off-the-shelf system.
Asa reviewer notes, it could be retrained to produce word-levelalignments, which would likely improve the accuracy of super-sense tag projection.665ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:an automatic metric for MT evaluation with improvedcorrelation with human judgments.
In Proceedings ofthe ACL Workshop on Intrinsic and Extrinsic Evalu-ation Measures for Machine Translation and/or Sum-marization, pages 65?72, Ann Arbor, Michigan, June.Association for Computational Linguistics.Taylor Berg-Kirkpatrick, Alexandre Bouchard-C?t?,John DeNero, and Dan Klein.
2010.
Painless un-supervised learning with features.
In Human Lan-guage Technologies: The 2010 Annual Conference ofthe North American Chapter of the Association forComputational Linguistics (NAACL-HLT 2010), pages582?590, Los Angeles, California, June.
Associationfor Computational Linguistics.David Burkett and Dan Klein.
2008.
Two languages arebetter than one (for syntactic parsing).
In Proceed-ings of the 2008 Conference on Empirical Methods inNatural Language Processing (EMNLP 2008), pages877?886, Honolulu, Hawaii, October.
Association forComputational Linguistics.David Burkett, Slav Petrov, John Blitzer, and Dan Klein.2010.
Learning better monolingual models with unan-notated bilingual text.
In Proceedings of the Four-teenth Conference on Computational Natural Lan-guage Learning (CoNLL 2010), pages 46?54, Up-psala, Sweden, July.
Association for ComputationalLinguistics.Stanley F. Chen and Joshua Goodman.
1999.
An empir-ical study of smoothing techniques for language mod-eling.
Computer Speech & Language, 13(4):359?393,October.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228,June.Kenneth W. Church and Eduard H. Hovy.
1993.
Goodapplications for crummy machine translation.
Ma-chine Translation, 8(4):239?258, December.Massimiliano Ciaramita and Yasemin Altun.
2006.Broad-coverage sense disambiguation and informationextraction with a supersense sequence tagger.
In Pro-ceedings of the 2006 Conference on Empirical Meth-ods in Natural Language Processing, pages 594?602,Sydney, Australia, July.
Association for Computa-tional Linguistics.Massimiliano Ciaramita and Mark Johnson.
2003.
Su-persense tagging of unknown nouns in WordNet.
InProceedings of the 2003 Conference on EmpiricalMethods in Natural Language Processing, pages 168?175, Sapporo, Japan, July.Dipanjan Das and Slav Petrov.
2011.
Unsupervised part-of-speech tagging with bilingual graph-based projec-tions.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies (ACL-HLT 2011), pages600?609, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JonathanWeese, Ferhan Ture, Phil Blunsom, Hendra Setiawan,Vladimir Eidelman, and Philip Resnik.
2010. cdec: Adecoder, alignment, and learning framework for finite-state and context-free translation models.
In Proceed-ings of the ACL 2010 System Demonstrations, pages7?12, Uppsala, Sweden, July.
Association for Compu-tational Linguistics.Sabri Elkateb, William Black, Horacio Rodr?guez, MusaAlkhalifa, Piek Vossen, Adam Pease, and ChristianeFellbaum.
2006.
Building a WordNet for Arabic.In Proceedings of the Fifth International Conferenceon Language Resources and Evaluation (LREC 2006),pages 29?34, Genoa, Italy, May.Christiane Fellbaum, editor.
1998.
WordNet: an elec-tronic lexical database.
MIT Press, Cambridge, MA.Michael Gamon, Anthony Aue, and Martine Smets.2005.
Sentence-level MT evaluation without refer-ence translations: Beyond language modeling.
In Pro-ceedings of the 10th European Association for Ma-chine Translation Conference (EAMT 2005), pages103?111, Budapest, May.Nizar Habash and Owen Rambow.
2005.
Arabic tok-enization, part-of-speech tagging and morphologicaldisambiguation in one fell swoop.
In Proceedings ofthe 43rd Annual Meeting of the Association for Com-putational Linguistics (ACL?05), pages 573?580, AnnArbor, Michigan, June.
Association for ComputationalLinguistics.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
OntoNotes:the 90% solution.
In Proceedings of the Human Lan-guage Technology Conference of the NAACL (HLT-NAACL 2006), pages 57?60, New York City, USA,June.
Association for Computational Linguistics.Rebecca Hwa, Philip Resnik, Amy Weinberg, ClaraCabezas, and Okan Kolak.
2005.
Bootstrap-ping parsers via syntactic projection across paralleltexts.
Natural Language Engineering, 11(3):311?325,September.Sungchul Kim, Kristina Toutanova, and Hwanjo Yu.2012.
Multilingual named entity recognition usingparallel data and metadata from Wikipedia.
In Pro-ceedings of the 50th Annual Meeting of the Associa-tion for Computational Linguistics (ACL 2012), pages694?702, Jeju Island, Korea, July.
Association forComputational Linguistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-666ings of the 2003 Conference of the North AmericanChapter of the Association for Computational Linguis-tics on Human Language Technology (HLT-NAACL2003), pages 48?54, Edmonton, Canada.
Associationfor Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondr?ej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics Demo and Poster Sessions,pages 177?180, Prague, Czech Republic, June.
Asso-ciation for Computational Linguistics.Alon Lavie and Michael J. Denkowski.
2009.
TheMETEOR metric for automatic evaluation of machinetranslation.
Machine Translation, 23(2?3):105?115,September.Mohamed Maamouri, Ann Bies, Tim Buckwalter, andWigdan Mekki.
2004.
The Penn Arabic Treebank:building a large-scale annotated Arabic corpus.
InNEMLAR Conference on Arabic Language Resourcesand Tools, pages 102?109.Bernard Merialdo.
1994.
Tagging English text witha probabilistic model.
Computational Linguistics,20(2):155?171.Rada Mihalcea, Carmen Banea, and Janyce Wiebe.
2007.Learning multilingual subjective language via cross-lingual projections.
In Proceedings of the 45th AnnualMeeting of the Association of Computational Linguis-tics (ACL 2007), pages 976?983, Prague, Czech Re-public, June.
Association for Computational Linguis-tics.George A. Miller, Claudia Leacock, Randee Tengi, andRoss T. Bunker.
1993.
A semantic concordance.
InProceedings of the Workshop on Human LanguageTechnology (HLT ?93), pages 303?308, Plainsboro,NJ, USA, March.
Association for Computational Lin-guistics.Behrang Mohit, Nathan Schneider, Rishav Bhowmick,Kemal Oflazer, and Noah A. Smith.
2012.Recall-oriented learning of named entities in ArabicWikipedia.
In Proceedings of the 13th Conference ofthe European Chapter of the Association for Computa-tional Linguistics (EACL 2012), pages 162?173, Avi-gnon, France, April.
Association for ComputationalLinguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL 2002), pages 311?318, Philadelphia,Pennsylvania, USA, July.
Association for Computa-tional Linguistics.Altaf Rahman and Vincent Ng.
2012.
Translation-based projection for multilingual coreference resolu-tion.
In Proceedings of the 2012 Conference of theNorth American Chapter of the Association for Com-putational Linguistics: Human Language Technolo-gies (NAACL-HLT 2012), pages 720?730, Montr?al,Canada, June.
Association for Computational Linguis-tics.Ryan Roth, Owen Rambow, Nizar Habash, Mona Diab,and Cynthia Rudin.
2008.
Arabic morphological tag-ging, diacritization, and lemmatization using lexememodels and feature ranking.
In Proceedings of ACL-08: HLT, pages 117?120, Columbus, Ohio, June.
As-sociation for Computational Linguistics.Nathan Schneider, Behrang Mohit, Kemal Oflazer, andNoah A. Smith.
2012.
Coarse lexical semantic an-notation with supersenses: an Arabic case study.
InProceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics (ACL 2012),pages 253?258, Jeju Island, Korea, July.
Associationfor Computational Linguistics.David A. Smith and Noah A. Smith.
2004.
Bilingualparsing with factored estimation: using English toparse Korean.
In Proceedings of the 2004 Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP?04), pages 49?56, Barcelona, Spain.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A studyof translation edit rate with targeted human annota-tion.
In Proceedings of the 7th Conference of theAssociation for Machine Translation in the Ameri-cas (AMTA 2006), pages 223?231, Cambridge, Mas-sachusetts, USA, August.David Yarowsky and Grace Ngai.
2001.
Inducing mul-tilingual POS taggers and NP bracketers via robustprojection across aligned corpora.
In Proceedingsof the Second Meeting of the North American Chap-ter of the Association for Computational Linguistics(NAACL?01), Pittsburgh, Pennsylvania, USA, June.David Yarowsky, Grace Ngai, and Richard Wicentowski.2001.
Inducing multilingual text analysis tools via ro-bust projection across aligned corpora.
In Proceed-ings of the First International Conference on HumanLanguage Technology Research (HLT?01), San Diego,California, USA, March.Imed Zitouni and Radu Florian.
2008.
Mention detec-tion crossing the language barrier.
In Proceedings ofthe 2008 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP 2008), pages 600?609,Honolulu, Hawaii, October.
Association for Computa-tional Linguistics.667
