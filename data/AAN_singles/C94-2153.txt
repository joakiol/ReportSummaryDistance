AN EFFICIENT SYNTACTIC TAGGING TOOL FOR CORPORA @Ming Zhou Changning HuangDept.
of Computer Science, "l~inghua University,Beijing, 100084, China.A BSTRA CTThe tree bank is an important resources tbrMT and linguistics researches, but it requires thatlarge number of sentences be annotated withsyntactic information.
It is time consuming andtroublesome, and dil'ficult to keep consistency, if'annotation is done manually.
In this paper, wcpresented a new technique for the semi-automatictagging of Chinese tcxt.
The system takes as inputChinese text, and outputs the syntactically taggedsentence(dependency tree).
We use dependencygrammar and employ a stack based shift /reducecontext-dependent parser as the tagging mecha-nism.
The system works in human-machinecooperative way, in which the machine can acquiretagging rules from human intervention.
The auto-mation level can be improved step by step by ac-cumulating rules during annotation.
In addition,good consistency of tagging is guaranteed.KEYWORDS: syntactic tagging, tree bank1.
INTRODUCTIONIn recent years, the corpora, eithermonolingual or bilingual,plays an important role inMT and linguistics rcscarches(Komatsu, jin &Yasuhara, 1993; Sato, 1993; \[sabcllc &Dymetman,t993).
This is because the corpora withlarge amount of running text is considered as anideal resources of linguistic knowledge.
However,to acquire knowledge \['rom the corpora(Watenabc,1993; Mitamura, Nyberg, Carboncll, 1993), oreffectively use the scntcnces as examples, as in ex-ample based approach(Nagao, 1984, O. Furusc &H.Iida, 1992), the corpora has to be annotated withcertain inlbrmation which may be ofmorphological information, syntactic inl'ormationand semantic information.Take Chinese monolingual corpora, For in-stance, the raw corpora, i.c.
the text which has notbccn scgmcntcd into word strings, can only be uscdtbr statistics of Chinesc character, howevcr, if youwant to work out the frequency of words, thecorpora has to bc segmcntcd into word strings, i.c.,it has to be annotated with word boundary infor-mation.
Further morc, if you want to obtain theco-occurrence frcqucncy of each two adjacent partof speeches, which is helpful to the study of part ofspeech (POS) tagging, you must annotate thecorpora with POS inIbrmation.
And if" you want toextract the syntactic knowledge from corpus, thecorpus must be attached with syntactic informationsuch as dependency relation and phrase structureetc., and such a corpora is called tree bank which isused as the rcsources for knowledge acquisition andcxamplcs in EBMT research.There are usually five levels of annotation tbra corpora, which includes word boundary tagging,POS tagging, sense tagging, syntactic relation tag-ging and semantic relation tagging, with the depthof tagging increases.
To improve the tagging auto-marion and keep good consistency, a mechanism isrcquircd at each level of tagging to acquire know-ledge fiom hunaan intervention and the annotatedcorpus.
The knowledge acquired should be fedback to the tagging model to improve the taggingautomation and correctness.Our group has bcen doing the research onChincse corpus for many years, and has done suc-cessful experiments on word boundary tagging,POS tagging(Bai & Xia, 1992), sense tagging(Tong,Huang & Guo, 1993).
The syntactic relation tag-ging, however, has not been resolved well becauseof some reasons.
First, there is no clear answerabout which grammar lbrmalism, such as phrasestructure granamar, or dependency grammar or anyothcr grammar is suitable for large scale runningtext syntactic tagging?
Second, how to savehumanZs labor from tagging, and keep good(i) supported by National Foundation of Natural Science of China.949consistency?For the first question, some rescarchers adoptphrase structure grammar (PSG) as thc taggingformalisms(Lecch & Garside 1991), and someadopt dependency grammar(DG) 1993, Komatsu,Jin, & Yasuhara, 1993).
In comparison with PSG,the authors think, DG has some advantages.
First,it is economical and convcnient to use DG for thcsyntactic relation tagging of corpus because there isno non-terminal node in the parse tree o fDG;  Scc-nd, DG stresses relations among individual words,the acquisition of collocation knowledge andsyntactic relation among words is straight; Third,there is relatively straight map bctween dependencytree and case reprcsentation.Based on the above discussion, the authorschosen dependency grammar as the syntacticformalism for corpora, and defined 44 kinds of dc-pendency relation tbr Chinese(Zhou & Huang1993).For the second question, we must develop anefficicnt tagging tool, fbr which wc nccd takc ac-count of two factors: (1) the power of acquiring tag-ging knowledge from the human intervention, in or-der to improve the automation level; (2) the abilityot" keeping ood consistency.Simmons & Yu (1992) introduced thecontext-dependent grammar for English parsing,in which the context-dependent rules can be ac-quired through an interactive mechanism, thephrase structure analysis and case analysis were con-ducted through a stack based shi ft /shi f t  parser,with success ratio reached as high as 99%.
Inspircdby their work, we designed a dependency relationtagging tool \['or Chinese corpus, called CSTT.CSTT takes the context-dependent grammar aswell.
It can learn the humants knowledge oftagging.
In the initial stage, the tagging is mainlydone by human, the system records the operationof human and forms tagging rules, when the rulesare accumulated to some number, the system canhelp human to tag, such as provides human with an-notation operations which human did belbre in thesame context, or even do some annotation itself insome cases.
The annotation automation gets higherand higher and good consistency is thusguaranteed.
It should be mentioned that since PSGnon-terminal symbols are used in shift / reduce tag-ging process, CSTT can produce syntactically tagged sentences of PSG version as well.
In addition,both versions of tree can be mapped into each otherby providing with a set of transfcr ules.A small corpora of 1300 sentences of daily lifeis used for experiment, with the average length of20 Chinese characters per sentence,For the first 300sentences, 1455 rules were obtained, and for thewhole corpora,totally 6521 rules was obtained.
Thetagging automation was improved continually withthe rules increased, and the automatic tagging ratiois above 50% after 1200 sentences were tagged.2 DESIGN OF CSTT2.1 The context-dependent shift/reduce taggingm ech a nislnThe proccss of context-dependent tagging isthat when a sentence is input(the input string is thesequence of part of speech), we look up the rulebase with the top two elements of the stack to seewhether there exist rules coinciding with the currentcontext.
If not, human operation is required to de-termine whether educe or shift.
If reduce, then fur-ther decides what phrase structure will be con-structed, and what dependency relation will be con-structed bctwecn these top two elements.
The sys-tem records the current context and the operationsto tbrms a ncw rule, and put it into rule base.Formally, context dependent rule is represented as:c~xyfl~ s (Shif't)c~xy\[l~(z,y,h) (ReduceWhere x, y are the top two elements in thestack, and cqfl are the context on the left hand o fxand the context on the right hand of yrespectively.The context is represented as a se-quence of part o1" speeches.
There are two actionson the right hand of a rule, shift action denoted ass, and reduce action denoted as(z,?,h).For reduceaction, z denotes the phrase structure after reduc-tion, and ?
denotes the dependency relation be-tween x and y, h denotes which clement is the headof the phrase structure and dependency relation.
Bytt='A'means the top clement is the head, h='B 'means that the second top clement of the stack isthe head.
Now let/s sce the tagging process for asimple sentence:950R VY R USDE A NG(where, R: pronoun,  VY: verb ,~.u,  USDE:ufl~ju, A: adj., NG:  general noun.
)Table 1< Stack > ~t k< Input string >The contcxt -dcpcndent  sh i f t / reduce  tagging processAction Phrasestructure.
.
.
.
.
:/c4:<R> <VY> < R> <USDE> <A > <NG > < + > shift.
.
.
.
< R>: ;~<VY> <R> <USDE> <A > <NG > <+ > shift- - -<R> <VY>~:< R> <USDE> <A > <NG > <.
> reduce- - - -<SV>:~<R><US1)E><A><NG><.
> shift- - -<SV><R>: i~<USDE><A> <NG> <.
> shift- -<SV><R> <USDE>:~-<A><NG> <.
> reduce- - -<SV><I)E>:~<A> <NG><?
> shift- -<SV><DE><A>r\ ]$<NG><?
> shift-<SV><I)E> <A> <NG>-t~<?+'+' > reduce- -<SV><DE><NP>~<+ > reduce- - -<SV> <NP>:/:#-<?
> reduce.
.
.
.
.
<SS > #:/k < o > shift- - -<88><o >:~ reduce.
.
.
.
< SP > :/=/: popDependencyrelationSV SUBI)E DEPNP ATTANP ATTASS OBJSP MARKGOV(where, SV: sub ject -verb phrase, DE: ~/II<J ~'structure, NP: noun phrase, SS: sub-scntcnce,  SP:sentence.
SUB: subject, DEP: u((,j ,, structure,ATTA:  modifier, OBJ: object, MARK:  punctua-tion mark,  GOV: the predicate of sentence.
)l )epcndency relation is represented as a tripleof the form <modi f icr ,  head,the dcpendcncy rela-t ion > .The tagging result is represented as a sct el"triples: { < 4.~, ,~ ,SUB >,  < ~:.
,Ni l ,GOV >,  < 4tf ~, ,ft<O,DEP>,  < ~I*,J,}\])lJ.~,ATTA >,  <)f,)I\]IJS.,A'FTA >,<Jl/l ~ ,~h~ ,OBJ > }.At each stcp, we can obtain arule by recording the content of stack and input str-ing, and the operat ion(shift  or reduce) given by us-er.
II' the operat ion is a reduction, the phrase struc-ture and dependency relation arc to be decided byuser.
I lere are two rulcs obtained:- - -<  R> <VY>-~<R> < USDE> <A> <NG><+ >-~(SV,SUB,A)- -<SV><R> <USDE>~4z<A><NG> <o >-~sAfter the reduction, the phrasc structurcformed rcplaces the top two elements in the stack.And the head will reprcscnt his phrase in later pro+ccss.
Since scntcnecs varies with its length, we usetbrcc elements on thc lcl't side of the top two cle-ments in the stack and the top I'ivc clemcnts in thcinput string as the context.The input is a scqucnce ot+ the part of speech ofa sentence, and the output  is the depcndency treedcnotcd as a set of triple oF the form (modifier,hcad, the dependency relation), and as a by -prod-uct, context -dependent  rules are acquired.
It is ob-viously that we can work out the phrase structuretrcc as well by modifying the algor i thm (not de-tailcd in this papcr).l,ct CDG be the context -dcpendent  rule basewhich were acquired bctbre ,CDG is empty if" thesystem is just put into use.
NUMBER-OF-AC-T ION records the number  of total actions(eithershift or reduce) during tagging,NUMBER-OF-AUTOMATION is the numberof actions(given by the system itselt) which are con-l irmed to bc right by human.
The automat ic  tag-ging ratio is therefore sct as NUMBER-OF-A I ) -TOMAT1ON / NUMBER-OF-ACT IONS.At present, the system is under supervision,human intervention is applied at each step either toconfirm the actions given by the system or to ap-pend new actions.
Idcally, the tagging processshould be nearly full automatic  with min imum hu-man intervention.
But it is a long term process.
Webelieved that with the size of corpora tagged in-creases, the automat ic  tagging ratio will be im-proved, and whcrt it reaches to a degree of high2.2 The tagging algorithmenough, human intervention may be removed, or itmay only be needed in the case that no rule ismatched.Table 2 The supervised tagging algorithmBEGINSTACK = EMPTYNUMBER-OF-AUTOMATION = 0NUM BER-OF-ACTION = 0DO UNTIL (INPUT = EMPTY AND STACK = EMPTY))CONTEXT = APPEND(TOP-FIVE(STACK),FI RST-FIVE(INPUT)) / * get the context * /RULE-LIST = CONSULT-TO-CDG(CONTEXT) / * match with CDG * /RULE =CONSULT-TO-HUMAN(RULE-L IST) /  * human intervention * /IF(RULE= FIRST(RULE-LIST)) / * the default operation is right * /NUM BER-OF-AUTOMATION++NUMBER-OF-ACTION++IF RHS(RULE) =/S 'STACK = PUSH(FIRST(INPUT),STAC K)ELSE{LET (Z,y, h)BE RIIS OF THE RULELETX= FIRST(STACK) Y= SECOND(STACK)BUILD A PHRASE STRUCTURE Z VROM XAND YSTACK = PUSH(Z,POP(POP(STACK)))/ * the phrase structure rcplace the top two clements of the stack * /IF h = 'A'BUILD-DEPENDENCY-RELATION(HEAD(Y),HEAD(X),y)/ * build the dependency triple * /ELSEIF h = 'B'BUILD-DEPENDENCY-R ELATION(H EAD(X),I IEAD(Y),7)/ * build the dependency triple * /}IF(INPUT= EMPTY ANt) NUMBER(STACK)=I) STACK=POP(STACK)ENDDOENDFunction TOP-FIVE, FIRST-FIVE return thefirst five elements of the stack and input stringrespectively.
If there are less than five elements in thestack or in the input string, then fills with blanks.
AP-PEND merges two lists to obtain the current context.CONSULT-TO-CDG looks up the rule base and re-turns a list of rules matching with the current context.The list is empty when no rule is matched.
If the list isnot empty, rules are sorted in descending order of theirusage frequency.
If human/s intervention is dcfault(thismay be available when the automatic tagging ratioreaches to some high degree), the system will take a ac-tion according to the rule of the highest frequency.CONSULT-TO-HUMAN returns only one rule byhmnan's inspection.
In this interactive process, human isasked to dctermine what action should be taken, he firstinspect the rule-list to see if there is already a rulecorrectly confirming with current context, if not, heshould tell the system whether "shift" or '/reduce", if "re-duce", he is requested to tell the system what phrasestructure and what dependency relation is to be built,and which element, the top element of the stack, or thesecond is the head.
A new rule will be acquired whenhuman makes a different operation from existing roles,by recording the current context and the operation.NUMBER-OF-AUTOMATION records the timesthat the rule with the highest frequency coincides withhuman's decision, which means that if the system worksin automatic way, the rule with the highest frequency isright.
NUMBER-OF-ACTIONS records the totaltimes of operation(shift or reduce) during tagging.
The952HEAD returns the head word of a phrase.
The functionPUSIt  means push an element into stack, and POP popstop element out of  stack, F IRST and SECOND returntbe first clement and second element of a list respectively.In matching process, weighted matching approach(Simmons & Yu, 1992) is used.
Assmnc the set of CDGrules is R= { RI, R2, .., Rm} , where the left hand ofeach rule is Ri= {rid ri2.. , ril0} , assume the context ofthe top two elements of the stack is C TM {% c a, .., cs0} ,where c 4 and c s arc the top two elements in the stack,we set up a match function:lt(Ci, rii) = 1, if e i = rii ,.u(ci, rii) = 0, if cjI = ripThe score function isL i0SCORE= it(cl,r,),i+ ~it(c,,r,)(ll--i)l= i  ~-6some cases.
CDG base is controlled dynamical ly so thatto keep high efficiency of matching.
A rule will be re-moved from the CDG base if it is seldom used.3 EXPERIMFNT AND ANALYSIS3.1 The experimentA small corpora of  1300 sentences of  daily life isprepared for experiment, with the average length as 20Chinese characters per sentence, the corpora covers mainclasses of  Chinese simple declarative sentences.The ex-periments is conducted in the following steps:(1) input a sentence;(2) word segmentation;(3) part of  speech tagging.The tagging model is a b i -gram modcl(Bai & Xia,1991), and the correct ratio is about 94% , so human con-firmation is needed.
(4) tagging the dependency relation by CSTT.A rule is preferred if and only if SCORE is greaterthan a threshold { set in advance.
{=2 l  means fullmatching.
In the beginning of the system, the full match-ing is recommended in order to deduce the conflict.
Andafter certain period of  tagging, we may set the thresholdsmaller than 21 to overcome the shortage of  rules inAs shown in Table 3, 1455 rules was obtained fromthe first 300 sentences.
In the whole experiment, totally6521 rules was obtained.
The more sentences tagged, thehigher automatic tagging ratio may be.
After 1200 sen-tenccs have been tagged, the ratio of  automatic opera-tion is above 50%.Table 3 The experiment resultSentence 1 -300  400 500 600No.
of1455 447 384 455rules accq uircdNo .
o f2072 768 776 792operat ionNo .
o f  auto487 291 336 281operat ionautomat ic23.5 37.8 43.3 35.rat io700 80?486 628851 834317 12190056584623730.01000 1100 1200 1300572 564 483 492837 1153 1164 1111210 572 641 58025.1 49.6 55.1 52.29533.2 Discussion(1) The rule conflictAlthough this system has some power fordisambiguation due to the context-dependentrules, it is difticult to resolve someambiguities.Therelbre it is easy to understand thata eonllict will occur if some ambiguity is encoun-tered.
For example, the sequence ofVG A NG maybe {(A, VG, COMPLEMENT),(NG, VG, OBJ)}or {(A, NG, ATTA), (NG, VG, OBJ)}, and the se-quence NGI  NG2 may be {(NG2, NG1,COORDINATE)} or {(NGI, NG2, ATTA)} as thefollowing two pairs of sentence demonstrate:(i)(ii)VG Atreat well~J~form goodNG NGplane gunwood tableNGrelation(A, VG, Complement)5<l '\[i~ (A, NG, ATTA)habit(NG, NG COORDINATE)(NG, NG, ATTA)Thcre arc two kinds of ambiguities, one is con-textual depcndcnt ambiguity, another is contextualindependent ambiguity.
For the former, CSTT canresole some of them.
For example, ~(VG)~L,(NG1)I'/,J (USDE)~'~ (NG2)is an ambiguousphrasc(which may be {(VG, nil, GOV), (NG1,USDE, DEP), (USDE, NG2, ATTA), (NG2, VG,OBJ)} which means "killcd the hunter's dog',or{(VG, USDE, DEP), (NG1, VG, OBJ), (USDE,NG2, ATTA), (NG2, nil, GOV)} which means thedog which killed the hunter.
However, if the con-text is considered, the ambiguity may be resolved:VG NG USDE NG VG YM Q VG NG USDE NGUn\[brtunately, CSTT canq resolve the ambi-guity of the later, human-intervcntionis ecessary.
(2) The convergence of the CDG ruleAccording to the analysis of (Simmons & Yu1992), 25,000 CDG rules will be sufficient o coverthe 99% phenomenon of English common sen-tences.
In this sense, the CDG rule is convergent.
Ifwe are only for syntactic tagging, the convergenceissues can be avoided temporally, if the automaticratio reaches above 80%, we can stop acquisition,at this time the tagging can already provide lotshelp to the users.
Of course, if we make some effec-tive attempts to CSTT, it may be developed into anel'licicnt dependency parser as well.4.
CONCLUDING REMARKIn this paper, we presented that dependencygrammar is a suitable formalism for syntactic tag-ging and presented a new technique for developinga syntactic tagging tool lbr large corpora, in whicha simple shi f t / reduce mechanism was employedand context dependent rules were accumulated dur-ing tagging.
The supervised tagging algorithm isdescribed.
The experiment shows that automatictagging ratio rises up continually with the numberof sentence increases, and good consistency is kept.This idea may be helpful for POS tagging and casetagging of corpora as well.We hope the automatic tagging ratio will raiseabove 80% in the future by enlarging the size ofrule base, so that it can be practically used lbrsyntactic tagging oF running text.REFERENCESBai, Shuan-hu, Yin Xia(1992).
A Scheme ForTagging Chinese Running Text.
Prec.
of NLPRS,p25-26, 1991, Singapore.Furuse O, II.
Iida(1992).
An example-basedmcthod For transl'cr-drivcn machine translation.Prec.
4th TMI-92.
Montreal, 1992.Isabclle, Pierre, Marc Dymetman et a1.
(1993).Translation Analysis and translation automation.Prec.
of TMI-93, p201-217.Komatsu, Eiji, Cui Jin, and HiroshiYasuhara(1983).
A mono-l ingual corpus-basedmachine translation of the inter lingua method.Prec.
of TM\[-93, p24-46.Leech, Geolt'erey and Roger Garside(1991).Running a grammar factory, the production ofsyntactically analyzed corpora or "tree banks".
In:English Computer Corpora, p15-32, Mouton deGruyter, 1991.Mitamura, Tcrko, Eric h. Nyberg, 3rd and954Jaime G. Carbonell(1993).
Automated corpus ana-lysis and the acquisition of large, multi-lingualknowledge bases lbr MT.
Proc.
of TMI -93, p292-301, Kyoto, Japan, July 1993.Nagao, M.(1984).
A framework of a mechani-cal translation between Japanese and English byanalogy example, In: A. Elithorn, R. Benerji, (Ed.
),Artificial and Human Intelligence,Elsevier:Amsterdam.Sato, Satoshi(1993).
Example-based transla-tion of technical terms.
Proc.
of TMI-93, p58-68.Simmons, F. Robert, Yeong-Ho Yu(1992).The Acquisition and Use of Context- DependentGrammars for English.
Computational Lin-guisties, Vol.
18, No.4, 1992.Tong, Xiang, Changning Huang, andChcngming Guo(1993).
Example-Based SenseTagging of Running Chinese Text.
Proc.
of theworkshop on very large corpus, Academic and In-dustrial Perspectives, p102-112, Columbus, Ohio,USA,June 22, 1993.Watanabe, Hideo(1993).
A method for ex-tracting translation patterns from translation ex-amples.
Proe.
of TMI-93, p292-301, Kyoto,Japan, July 1993.Zhou, Ming, and Changning Huang(1993).Viewing the Dependency parsing as a statisticallybased tagging process.
Proe.
NLPRS'93, Japan,Dec.
6-7, 1993.955
