Proceedings of NAACL HLT 2007, Companion Volume, pages 37?40,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsSituated Models of Meaning for Sports Video RetrievalMichael FleischmanMIT Media Labmbf@mit.eduDeb RoyMIT Media Labdkroy@media.mit.eduAbstractSituated models of meaning ground words in thenon-linguistic context, or situation, to which theyrefer.
Applying such models to sports video re-trieval requires learning appropriate representa-tions for complex events.
We propose a methodthat uses data mining to discover temporal pat-terns in video, and pair these patterns with associ-ated closed captioning text.
This paired corpus isused to train a situated model of meaning that sig-nificantly improves video retrieval performance.1 IntroductionRecent advances in digital broadcasting and re-cording allow fans access to an unprecedentedamount of sports video.
The growing need tomanage and search large video collections presentsa challenge to traditional information retrieval (IR)technologies.
Such methods cannot be directlyapplied to video data, even when closed captiontranscripts are available; for, unlike text docu-ments, the occurrence of a query term in a video isoften not enough to assume the video?s relevanceto that query.
For example, when searchingthrough video of baseball games, returning all clipsin which the phrase ?home run?
occurs, resultsprimarily in video of events where a home rundoes not actually occur.
This follows from the factthat in sports, as in life, people often talk not aboutwhat is currently happening, but rather, they talkabout what did, might, or will happen in the future.Traditional IR techniques cannot address suchproblems because they model the meaning of aquery term strictly by that term?s relationship toother terms.
To build systems that successfullysearch video, IR techniques should be extended toexploit not just linguistic information but also ele-ments of the non-linguistic context, or situation,that surrounds language use.
This paper presents amethod for video event retrieval from broadcastsports that achieves this by learning a situatedmodel of meaning from an unlabeled video corpus.The framework for the current model is derivedfrom previous work on computational models ofverb learning (Fleischman & Roy, 2005).
In thisearlier work, meaning is defined by a probabilisticmapping between words and representations of thenon-linguistic events to which those words refer.In applying this framework to events in video, wefollow recent work on video surveillance in whichcomplex events are represented as temporal rela-tions between lower level sub-events (Hongen etal., 2004).
While in the surveillance domain, handcrafted event representations have been used suc-cessfully, the greater variability of content inbroadcast sports demands an automatic method fordesigning event representations.The primary focus of this paper is to present amethod for mining such representations from largevideo corpora, and to describe how these represen-tations can be mapped to natural language.
Wefocus on a pilot dataset of broadcast baseballgames.
Pilot video retrieval tests show that using asituated model significantly improves perform-ances over traditional language modeling methods.2 Situated Models of MeaningBuilding situated models of meaning operates inthree phases (see Figure 1): first, raw video data isabstracted into multiple streams of discrete fea-tures.
Temporal data mining techniques are thenapplied to these feature streams to discover hierar-chical temporal patterns.
These temporal patternsform the event representations that are thenmapped to words from the closed caption stream.2.1 Feature ExtractionThe first step in representing events in video is toabstract the very high dimensional raw video datainto more semantically meaningful streams of in-formation.
Ideally, these streams would corre-spond to basic events that occur in sports video(e.g., hitting, throwing, catching, kicking, etc.
).Due to the limitations of computer vision tech-niques, extracting such ideal features is often in-feasible.
However, by exploiting the ?language of37Figure 1.
Video processing pipeline for learning situated models of meaning.film?
that is used to produce sports video, informa-tive features can be extracted that are also easy tocompute.
Thus, although we cannot easily identifya player hitting the ball, we can easily detect fea-tures that correlate with hitting: e.g., when a scenefocusing on the pitching mound immediatelyjumps to one zooming in on the field (Figure 1).While such correlations are not perfect, pilot testsshow that baseball events can be classified usingsuch features (Fleischman et.
al., in prep).Importantly, this is the only phase of our frame-work that is domain specific; i.e., it is the only as-pect of the framework designed specifically for usewith baseball data.
Although many feature typescan be extracted, we focus on only two featuretypes: visual context, and camera motion.Visual ContextVisual context features encode general propertiesof the visual scene in a video segment.
The firststep in extracting such features is to split the rawvideo into ?shots?
based on changes in the visualscene due to editing (e.g., jumping from a close upof the pitcher to a wide angle of the field).
Shotdetection is a well studied problem in multimediaresearch; in this work, we use the method ofTardini et al (2005) because of its speed andproven performance on sports video.After a game is segmented into shots, each shotis categorized into one of three categories: pitch-ing-scene, field-scene, or other.
Categorization isbased on image features (e.g., color histograms,edge detection, motion analysis) extracted from anindividual key frame chosen from that shot.
A de-cision tree is trained (with bagging and boosting)using the WEKA machine learning toolkit thatachieves over 97% accuracy on a held out dataset.Camera MotionWhereas visual context features provide informa-tion about the global situation that is being ob-served, camera motion features afford more preciseinformation about the actions occurring in thevideo.
The intuition here is that the camera is astand in for a viewer?s focus of attention.
As ac-tion in the video takes place, the camera moves tofollow it, mirroring the action itself, and providingan informative feature for event representation.Detecting camera motion (i.e., pan/tilt/zoom) is awell-studied problem in video analysis.
We usethe system of (Bouthemy et al, 1999) which com-putes the pan, tilt, and zoom motions using the pa-rameters of a two-dimensional affine model fit toevery pair of sequential frames in a video segment.The output of this system is then clustered intocharacteristic camera motions (e.g.
zooming in fastwhile panning slightly left) using a 1st order Hid-den Markov Model  with 15 states, implementedusing the Graphical Modeling Toolkit (GMTK).2.2 Temporal Pattern MiningIn this step, temporal patterns are mined from thefeatures abstracted from the raw video data.
Asdescribed above, ideal semantic features (such ashitting and catching) cannot be extracted easilyfrom video.
We hypothesize that finding temporalpatterns between scene and camera motion featurescan produce representations that are highly corre-lated with sports events.
Importantly, such tempo-ral patterns are not strictly sequential, but rather,are composed of features that can occur in complexand varied temporal relations to each other.
Forexample, Figure 1 shows the representation for afly ball event that is composed of: a camera pan-38ning up followed by a camera pan down, occurringduring a field scene, and before a pitching scene.Following previous work in video content classi-fication (Fleischman et al, 2006), we use tech-niques from temporal data mining to discoverevent patterns from feature streams.
The algorithmwe use is fully unsupervised.
It processes featurestreams by examining the relations that occur be-tween individual features within a moving timewindow.
Following Allen (1984), any two featuresthat occur within this window must be in one ofseven temporal relations with each other (e.g.
be-fore, during, etc.).
The algorithm keeps track ofhow often each of these relations is observed, andafter the entire video corpus is analyzed, uses chi-square analyses to determine which relations aresignificant.
The algorithm iterates through thedata, and relations between individual features thatare found significant in one iteration (e.g.
[BEFORE, camera panning up, camera panningdown]), are themselves treated as individual fea-tures in the next.
This allows the system to buildup higher-order nested relations in each iteration(e.g.
[DURING, [BEFORE, camera panning up,camera panning down], field scene]]).
The tempo-ral patterns found significant in this way are thenused as the event representations that are thenmapped to words.2.3 Linguistic MappingThe last step in building a situated model of mean-ing is to map words onto the representations ofevents mined from the raw video.
We equate thelearning of this mapping to the problem of estimat-ing the conditional probability distribution of aword given a video event representation.
Similarto work in image retrieval (Barnard et al, 2003),we cast the problem in terms of Machine Transla-tion: given a paired corpus of words and a set ofvideo event representations to which they refer, wemake the IBM Model 1 assumption and use theexpectation-maximization method to estimate theparameters (Brown et al, 1993):?=+=mjajm jvideowordplCvideowordp1)|()1()|((1)This paired corpus is created from a corpus ofraw video by first abstracting each video into thefeature streams described above.
For every shotclassified as a pitching scene, a new instance iscreated in the paired corpus corresponding to anevent that starts at the beginning of that shot andends exactly four shots after.
This definition of anevent follows from the fact that most events inbaseball must start with a pitch and usually do notlast longer than four shots (Gong et al, 2004).For each of these events in the paired corpus, arepresentation of the video is generated by match-ing all patterns (and the nested sub-patterns) foundfrom temporal mining to the feature streams of theevent.
These video representations are then pairedwith all the words from the closed captioning thatoccur during that event (plus/minus 10 seconds).3 ExperimentsWork on video IR in the news domain often fo-cuses on indexing video data using a set of imageclassifiers that categorize shots into pre-determinedconcepts (e.g.
flag, outdoors, George Bush, etc.
).Text queries must then be translated (sometimesmanually) in terms of these concepts (Worring &Snoek, 2006).
Our work focuses on a more auto-mated approach that is closer to traditional IR tech-niques.
Our framework extends the languagemodeling approach of Ponte and Croft (1998) byincorporating a situated model of meaning.In Ponte and Croft (1998), documents relevant toa query are ranked based on the probability thateach document generated each query term.
Wefollow this approach for video events, making theassumption that the relevance of an event to aquery depends both on the words associated withthe event (i.e.
what was said while the event oc-curred), as well as the situational context modeledby the video event representations:?
?
?=querywordvideowordpcaptionwordpeventqueryp )1()|()|()|( ??
(2)The p(word|caption) is estimated using the lan-guage modeling technique described in Ponte andCroft (1998).
The p(word|video) is estimated as inequation 1 above.
?
is used to weight the models.DataThe system has been evaluated on a pilot set of 6broadcast baseball games totaling about 15 hoursand 1200 distinct events.
The data representsvideo of 9 different teams, at 4 different stadiums,broadcast on 4 different stations.
Highlights (i.e.,events which terminate with the player either outor safe) were hand annotated, and categorized ac-cording to the type of the event (e.g., strikeout vs.homerun), the location of the event (e.g., right fieldvs.
infield), and the nature of the event (e.g., flyball vs. line drive).
Each of these categories was39used to automatically select query terms to be usedin testing.
Similar to Berger & Lafferty (1999), theprobability distribution of terms given a category isestimated using a normalized log-likelihood ratio(Moore, 2004), and query terms are sampled ran-domly from this distribution.
This gives us a set ofqueries for each annotated category (e.g., strikeout:?miss, chasing?
; flyball: ?fly, streak?).
Althoughmuch noisier than human produced queries, thisprocedure generates a large amount of test queriesfor which relevant results can easily be determined(e.g., if a returned event for the query ?fly, streak?is of the flyball category, it is marked relevant).Experiments are reported using 6-fold crossvalidation during which five games are used totrain the situated model while the sixth is held outfor testing.
Because data is sparse, the situationmodel is trained only on the hand annotated high-light events.
However, retrieval is always testedusing both highlight and non-highlight events.Figure 2.
Effect of situated model on video IR.ResultsFigure 2 shows results for 520 automatically gen-erated queries of one to four words in length.Mean average precision (MAP), a common metricthat combines elements of precision, recall, andranking, is used to measure the relevance of the topfive results returned for each query.
We show re-sults for the system using only linguistic informa-tion (i.e.
?=1), only non-linguistic information (i.e.
?=0), and both information together (i.e.
?=0.5).The poor performance of the system using onlynon-linguistic information is expected given thelimited training data and the simple features usedto represent events.
Interestingly, using only lin-guistic information produces similarly poor per-formance.
This is a direct result of announcers?tendency to discuss topics not currently occurringin the video.
By combining text and video analy-ses, though, the system performs significantly bet-ter (p<0.01) by determining when the observedlanguage actually refers to the situation at hand.4 ConclusionWe have presented a framework for video retrievalthat significantly out-performs traditional IR meth-ods applied to closed caption text.
Our new ap-proach incorporates the visual content of baseballvideo using automatically learned event represen-tations to model the situated meaning of words.Results indicate that integration of situational con-text dramatically improves performance over tradi-tional methods alone.
In future work we willexamine the effects of applying situated models ofmeaning to other tasks (e.g., machine translation).ReferencesAllen, J.F.
(1984).
A General Model of Action and Time.
Arti-ficial Intelligence.
23(2).Barnard, K, Duygulu, P, de Freitas, N, Forsyth, D, Blei, D,and Jordan, M. (2003), "Matching Words and Pictures,"Journal of Machine Learning Research, Vol 3.Berger, A.  and Lafferty, J.
(1999).
Information Retrieval asStatistical Translation.
In Proceedings of SIGIR-99.Bouthemy, P., Gelgon, M., Ganansia, F. (1999).
A unifiedapproach to shot change detection and camera motion char-acterization.
IEEE Trans.
on Circuits and Systems for VideoTechnology, 9(7):1030-1044.Brown, P., Della Pietra, S., Della Pietra, V. Mercer, R. (1993).The mathematics of machine translation: Parameter estima-tion.
Computational Linguistics, 19(10).Fleischman, M. and Roy, D. (2005).
Intentional Context inSituated Language Learning.
In Proc.
of 9th Conference onComp.
Natural Language Learning.Fleischman, M., DeCamp, P. Roy, D.  (2006).
Mining Tempo-ral Patterns of Movement for Video Content Classification.The 8th ACM SIGMM International Workshop on Multi-media Information Retrieval.Fleischman, M., Roy, B., Roy, D. (in prep.).
Automated Fea-ture Engineering inBaseball Highlight Classification.Gong, Y., Han, M., Hua, W., Xu, W.  (2004).
Maximum en-tropy model-based baseball highlight detection and classifi-cation.
Computer Vision and Image Understanding.
96(2).Hongen, S., Nevatia, R. Bremond, F. (2004).
Video-basedevent recognition: activity representation and probabilisticrecognition methods.
Computer Vision and Image Under-standing.
96(2).
pp: 129 - 162Moore, Robert C. (2004).
Improving IBM Word AlignmentModel 1. in Proc.
of 42nd ACL.Ponte, J.M., and Croft, W.B.
(1998).
A Language ModelingApproach to Information Retrieval.
In Proc.
of SIGIR?98.Tardini, G. Grana C., Marchi, R., Cucchiara, R., (2005).
ShotDetection and Motion Analysis for Automatic MPEG-7Annotation of Sports Videos.
In 13th International Confer-ence on Image Analysis and Processing.Worring, M., Snoek, C.. (2006).
Semantic Indexing and Re-trieval of Video.
Tutorial at ACM Multimedia40
