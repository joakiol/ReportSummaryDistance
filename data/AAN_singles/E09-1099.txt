Proceedings of the 12th Conference of the European Chapter of the ACL, pages 870?878,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsLanguage ID in the Context of Harvesting Language Data off the WebFei XiaUniversity of WashingtonSeattle, WA 98195, USAfxia@u.washington.eduWilliam D. LewisMicrosoft ResearchRedmond, WA 98052, USAwilewis@microsoft.comHoifung PoonUniversity of WashingtonSeattle, WA 98195, USAhoifung@cs.washington.eduAbstractAs the arm of NLP technologies extendsbeyond a small core of languages, tech-niques for working with instances of lan-guage data across hundreds to thousandsof languages may require revisiting and re-calibrating the tried and true methods thatare used.
Of the NLP techniques that hasbeen treated as ?solved?
is language iden-tification (language ID) of written text.However, we argue that language ID isfar from solved when one considers in-put spanning not dozens of languages, butrather hundreds to thousands, a numberthat one approaches when harvesting lan-guage data found on the Web.
We formu-late language ID as a coreference resolu-tion problem and apply it to a Web harvest-ing task for a specific linguistic data typeand achieve a much higher accuracy thanlong accepted language ID approaches.1 IntroductionA large number of the world?s languages havebeen documented by linguists; it is now increas-ingly common to post current research and datato the Web, often in the form of language snip-pets embedded in scholarly papers.
A particu-larly common format for linguistic data posted tothe Web is ?interlinearized text?, a format usedto present language data and analysis relevant toa particular argument or investigation.
Since in-terlinear examples consist of orthographically orphonetically encoded language data aligned withan English translation, the ?corpus?
of interlinearexamples found on the Web, when taken together,constitute a significant multilingual, parallel cor-pus covering hundreds to thousands of the world?slanguages.
Previous work has discussed methodsfor harvesting interlinear text off the Web (Lewis,2006), enriching it via structural projections (Xiaand Lewis, 2007), and even making it available totypological analyses (Lewis and Xia, 2008) andsearch (Xia and Lewis, 2008).One challenge with harvesting interlinear dataoff the Web is language identification of the har-vested data.
There have been extensive studieson language identification (language ID) of writ-ten text, and a review of previous research on thistopic can be found in (Hughes et al, 2006).
In gen-eral, a language ID method requires a collectionof text for training, something on the order of athousand or more characters.
These methods workwell for languages with rich language resources;for instance, Cavnar and Trenkle?s N-gram-basedalgorithm achieved an accuracy as high as 99.8%when tested on newsgroup articles across eightlanguages (Cavnar and Trenkle, 1994).
However,the performance is much worse (with accuracydropping to as low as 1.66%) if there is very lit-tle language data for training and the number oflanguages being evaluated reaches a few hundred.In this paper, we treat the language ID of har-vested linguistic data as a coreference resolutionproblem.
Our method, although narrowly focusedon this very specific data type, makes it possible tocollect small snippets of language data across hun-dreds of languages and use the data for linguisticsearch and bootstrapping NLP tools.2 Background2.1 Interlinear glossed text (IGT)In linguistics, the practice of presenting languagedata in interlinear form has a long history, go-ing back at least to the time of the structural-ists.
Interlinear Glossed Text, or IGT, is oftenused to present data and analysis on a languagethat the reader may not know much about, andis frequently included in scholarly linguistic doc-uments.
The canonical form of an IGT consists870of three lines: a line for the language in question(i.e., the language line), an English gloss line, andan English translation.
Table 1 shows the begin-ning of a linguistic document (Baker and Stewart,1996) which contains two IGTs: one in lines 30-32, and the other in lines 34-36.
The line numbersare added for the sake of convenience.1: THE ADJ/VERB DISTINCTION: EDO EVIDENCE2:3: Mark C. Baker and Osamuyimen Thompson Stewart4: McGill University....27: The following shows a similar minimal pair from Edo,28: a Kwa language spoken in Nigeria (Agheyisi 1990).29:30: (2) a. E`me`ri?
mo`se?.31: Mary be.beautiful(V)32: ?Mary is beautiful.
?33:34: b. E`me`ri?
*(ye?)
mo`se?.35: Mary be.beautiful(A)36: ?Mary is beautiful (A).
?...Table 1: A linguistic document that contains IGT:words in boldface are potential language names2.2 The Online Database of Interlinear text(ODIN)ODIN, the Online Database of INterlinear text, isa resource built from data harvested from schol-arly documents (Lewis, 2006).
It was built inthree steps: (1) crawling the Web to retrieve doc-uments that may contain IGT, (2) extracting IGTfrom the retrieved documents, and (3) identifyingthe language codes of the extracted IGTs.
Theidentified IGTs are then extracted and stored in adatabase (the ODIN database), which can be easilysearched with a GUI interface.1ODIN currently consists about 189,000 IGT in-stances extracted from three thousand documents,with close to a thousand languages represented.In addition, there are another 130,000 additionalIGT-bearing documents that have been crawledand are waiting for further process.
Once theseadditional documents are processed, the databaseis expected to expand significantly.ODIN is a valuable resource for linguists, as itcan be searched for IGTs that belong to a partic-ular language or a language family, or those thatcontain a particular linguistic construction (e.g.,passive, wh-movement).
In addition, there have1http://odin.linguistlist.orgbeen some preliminary studies that show the bene-fits of using the resource for NLP.
For instance, ourprevious work shows that automatically enrichedIGT data can be used to answer typological ques-tions (e.g., the canonical word order of a language)with a high accuracy (Lewis and Xia, 2008), andthe information could serve as prototypes for pro-totype learning (Haghighi and Klein, 2006).3 The language ID task for ODINAs the size of ODIN increases dramatically, it iscrucial to have a reliable module that automati-cally identifies the correct language code for eachnew extracted IGT to be added to ODIN.
The cur-rent ODIN system uses two language identifiers:one is based on simple heuristics, and the otheron Cavnar and Trenkle?s algorithm (1994).
How-ever, because the task here is very different froma typical language ID task (see below), both algo-rithms work poorly, with accuracy falling below55%.
The focus of this paper is on building newlanguage identifiers with a much higher accuracy.3.1 The data setA small portion of the IGTs in ODIN havebeen assigned the correct language code semi-automatically.
Table 2 shows the size of the dataset.
We use it for training and testing, and all re-sults reported in the paper are the average of run-ning 10-fold cross validation on the data set unlessspecified otherwise.Table 2: The data set for the language ID task# of IGT-bearing documents 1160# of IGT instances 15,239# of words on the language lines 77,063# of languages 6383.2 The special properties of the taskThe task in hand is very different from a typicallanguage ID task in several respects:?
Large number of languages: The number oflanguages in our data set is 638 and that of thecurrent ODIN database is close to a thousand.As more data is added to ODIN, the numberof languages may reach several thousand asnewly added linguistic documents could referto any of approximately eight thousand livingor dead languages.871?
The use of language code: When dealingwith only a few dozen languages, languagenames might be sufficient to identify lan-guages.
This is not true when dealing witha large number of languages, because somelanguages have multiple names, and somelanguage names refer to multiple languages(see Section 4.2).
To address this problem,we use language codes, since we can (mostly)ensure that each language code maps to ex-actly one language, and each language mapsto exactly one code.?
Unseen languages: In this data set, about10% of IGT instances in the test data belongto some languages that have never appearedin the training data.
We call it the unseenlanguage problem.
This problem turns out tobe the major obstacle to existing language IDmethods.?
Extremely limited amount of training dataper language: On average, each language inthe training data has only 23 IGTs (116 wordtokens in the language lines) available, and45.3% of the languages have no more than10 word tokens in the training data.?
The length of test instances: The languagelines in IGT are often very short.
The aver-age length in this data set is 5.1 words.
About0.26% of the language lines in the data set aretotally empty due to the errors introduced inthe crawling or IGT extraction steps.?
Encoding issues: For languages that do notuse Roman scripts in their writing system,the authors of documents often choose to useRomanized scripts (e.g., pinyin for Chinese),making the encoding less informative.?
Multilingual documents: About 40% of doc-uments in the data set contain IGTs frommultiple languages.
Therefore, the languageID prediction should be made for each indi-vidual IGT, not for the whole document.?
Context information: In this task, IGTs arepart of a document and there are often variouscues in the document (e.g., language names)that could help predict the language ID ofspecific IGT instances.Hughes and his colleagues (2006) identifiedeleven open questions in the domain of languageID that they believed were not adequately ad-dressed in published research to date.
Interest-ingly, our task encounters eight out of the elevenopen questions.
Because of these properties, ex-isting language ID algorithms do not perform wellwhen applied to the task (see Section 6).4 Using context informationVarious cues in the document can help predict thelanguage ID of IGTs, and they are represented asfeatures in our systems.4.1 Feature templatesThe following feature templates are used in our ex-periments.
(F1): The nearest language that precedes the cur-rent IGT.
(F2): The languages that appear in the neighbor-hood of the IGT or at the beginning or theend of a document.2 Another feature checksthe most frequent language occurring in thedocument.
(F3): For each language in the training data, webuild three token lists: one for word uni-grams, one for morph unigrams and the thirdfor character ngrams (n ?
4).
These wordlists are compared with the token lists builtfrom the language line of the current IGT.
(F4): Similar to (F3), but the comparison is be-tween the token lists built from the currentIGT with the ones built from other IGTs inthe same document.
If some IGTs in thesame document share the same tokens, theyare likely to belong to the same language.Here, all the features are binary: for features inF3 and F4, we use thresholds to turn real-valuedfeatures into binary ones.
F1-F3 features canbe calculated by looking at the documents only,whereas F4 features require knowing the languagecodes of other IGTs in the same document.4.2 Language tableTo identify language names in a document andmap language names to language codes, we needa language table that lists all the (language code,2For the experiments reported here, we use any line within50 lines of the IGT or the first 50 or the last 50 lines of thedocument.872language name) pairs.
There are three existing lan-guage tables: (1) ISO 639-3 maintained by SILInternational,3 (2) the 15th edition of the Ethno-logue,4 and (3) the list of ancient and dead lan-guages maintained by LinguistList.5 6 We mergedthe three tables, as shown in Table 3.Table 3: Various language name tablesLanguage table # of lang # of langcodes (code, name) pairs(1) ISO 639-3 7702 9312(2) Ethnologue v15 7299 42789(3) LinguistList table 231 232Merged table 7816 47728The mapping between language names and lan-guage codes is many-to-many.
A language codeoften has several alternate names in addition to theprimary name.
For instance, the language codeaaa maps to names such as Alumu, Tesu, Arum,Alumu-Tesu, Alumu, Arum-Cesu, Arum-Chessu,and Arum-Tesu.
While most language names mapto only one language code, there are exceptions.For instance, the name Edo can map to either binor lew.
Out of 44,071 unique language names inthe merged language table, 2625 of them (5.95%)are ambiguous.7To identify language names in a document, weimplemented a simple language name detector thatscans the document from left to right and finds thelongest string that is a language name accordingto the language table.
The language name is thenmapped to language codes.
If a language name isambiguous, all the corresponding language codesare considered by later stages.
In Table 1, thelanguage names identified by the detector are inboldface.
The detector can produce false positive(e.g., Thompson) because a language name canhave other meanings.
Also, the language table isby no means complete and the detector is not ableto recognize any language names that are missingfrom the table.3http://www.sil.org/iso639-3/download.asp4http://www.ethnologue.com/codes/default.asp#using5http://linguistlist.org/forms/langs/GetListOfAncientLgs.html6While ISO 639-3 is supposed to include all the languagecodes appearing in the other two lists, there is a lag in theadoption of new codes, which means the ISO 639-3 list con-tinues to be somewhat out-of-date with the lists from whichit is compiled since these other lists change periodically.7Among the ambiguous names, 1996 names each map totwo language codes, 407 map to three codes, 130 map to fourcodes, and so on.
The most ambiguous name is Miao, whichmaps to fourteen language codes.5 Formulating the language ID taskThe language ID task here can be treated as twodifferent learning problems.5.1 As a classification problemThe language ID task can be treated as a classifica-tion problem.
A classifier is a function that mapsa training/test instance x to a class label y, and yis a member of a pre-defined label set C .
For lan-guage ID, the training/test instance corresponds toa document (or an IGT in our case), and C is theset of language codes.
We call this approach theclassification (CL) approach.Most, if not all, of previous language ID meth-ods, fall into this category.
They differ with re-spect to the underlying learning algorithms and thechoice of features or similarity functions.
Whenapplying a feature-based algorithm (e.g., Maxi-mum entropy) and using the features in Section4.1, the feature vectors for the two IGTs in Ta-ble 1 are shown in Table 4.
Each line has the for-mat ?instance name true lang code feat name1feat name2 ...?, where feat names are the namesof features that are present in the instance.
Takethe first IGT as an example, its true language codeis bin; the nearest language name (nearLC) is Edowhose language code is bin or lew; the languagesthat appear before the IGT includes Edo (bin orlew), Thompson (thp), and so on.
The presence ofLMw1 bin and LMm1 bin means that the overlapbetween the word/morph lists for bin and the onesbuilt from the current IGT is higher than somethreshold.
The feature vector for the second IGTlooks similar, except that it includes a F4 featureIIw1 bin, which says that the overlap between theword list built from the other IGTs in the samedocument with language code bin and the wordlist built from the current IGT is above a thresh-old.
Note that language codes are part of featurenames; therefore, a simple feature template suchas nearest language (nearLC) corresponds to hun-dreds or even thousands of features (nearLC xxx).The CL approach has several major limitations.First, it cannot handle the unseen language prob-lem: if an IGT in the test data belongs to a lan-guage that does not appear in the training data, thisapproach cannot classify it correctly.
Second, thelack of parameter tying in this approach makes itunable to generalize between different languages.For instance, if the word German appears right be-fore an IGT, the IGT is likely to be German.
The873igt1 bin nearLC bin nearLC lew prev50 bin prev50 lew prev50 thp ... LMw1 bin LMm1 bin ...igt2 bin nearLC bin nearLC lew prev50 bin prev50 lew prev50 thp ... LMw1 bin LMm1 bin ... IIw1 bin ...Table 4: Feature vectors for the IGTs in Table 1 when using the CL approach (Edo: bin/lew, Thompson:thp, Kwa: etu/fip/kwb)same is true if the word German is replaced by an-other language name.
But this property cannot beleveraged easily by the CL approach without mod-ifying the learning algorithm.
This results in a pro-liferation of parameters, making learning harderand more prone to overfitting.5.2 As a coreference resolution problemA different way of handling the language ID taskis to treat it as a coreference resolution problem: amention is an IGT or a language name appearingin a document, an entity is a language code, andfinding the language code for an IGT is the same aslinking a mention (i.e., an IGT) to an entity (i.e., alanguage code).8 We call this approach the CoRefapproach.
The major difference between the CLapproach and the CoRef approach is the role oflanguage code: in the former, language code is aclass label to be used to tag an IGT; and in the lat-ter, language code is an entity which an IGT canbe linked to.The language ID task shares many similaritieswith a typical coreference resolution task.
Forinstance, language names are similar to propernouns in that they are often unambiguous.
IGTinstances are like pronouns in that they often referto language names appearing in the neighborhood.Once the language ID task is framed as a CoRefproblem, all the existing algorithms on CoRef canbe applied to the task, as discussed below.5.2.1 Sequence labeling using traditionalclassifiersOne common approach to the CoRef problem pro-cesses the mentions sequentially and determine foreach mention whether it should start a new entityor be linked to an existing mention (e.g., (Soonet al, 2001; Ng and Cardie, 2002; Luo, 2007));that is, the approach makes a series of decisions,8There are minor differences between the language ID andcoreference resolution tasks.
For instance, each entity in thelanguage ID task must be assigned a language code.
Thismeans that ambiguous language names will evoke multipleentities, each with a different language code.
These differ-ences are reflected in our algorithms.one decision per (mention, entity) pair.
Apply-ing this to the language ID task, the (mention, en-tity) pair would correspond to an (IGT, lang code)pair, and each decision would have two possibili-ties: Same when the IGT belongs to the languageor Diff when the IGT does not.
Once the decisionsare made for all the pairs, a post-processing proce-dure would check all the pairs for an IGT and linkthe IGT to the language code with which the pairhas the highest confidence score.Using the same kinds of features in Section 4.1,the feature vectors for the two IGTs in Table 1 areshown in Table 5.
Comparing Table 4 and 5 re-veals the differences between the CL approach andthe CoRef approach: the CoRef approach has onlytwo class labels (Same and Diff) where the CL ap-proach has hundreds of labels (one for each lan-guage code); the CoRef approach has much fewernumber of features because language code is notpart of feature names; the CoRef approach hasmore training instances as each training instancecorresponds to an (IGT, lang code) pair.igt1-bin same nearLC prev50 LMw1 LMm1 ...igt1-lew diff nearLC prev50 ...igt1-thp diff prev50 ......igt2-bin same nearLC prev50 LMw1 LMm1 IIw1 ...igt2-lew diff nearLC prev50 ...igt2-thp diff prev50 ......Table 5: Feature vectors for the IGTs in Table 1when using the CoRef approach with sequence la-beling methods5.2.2 Joint Inference Using Markov LogicRecently, joint inference has become a topic ofkeen interests in both the machine learning andNLP communities (e.g., (Bakir et al, 2007; Sut-ton et al, 2006; Poon and Domingos, 2007)).There have been increasing interests in formulat-ing coreference resolution in a joint model andconducting joint inference to leverage dependen-874cies among the mentions and entities (e.g., (Well-ner et al, 2004; Denis and Baldridge, 2007; Poonand Domingos, 2008)).
We have built a jointmodel for language ID in Markov logic (Richard-son and Domingos, 2006).Markov logic is a probabilistic extension offirst-order logic that makes it possible to com-pactly specify probability distributions over com-plex relational domains.
A Markov logic net-work (MLN) is a set of weighted first-orderclauses.
Together with a set of constants, it de-fines a Markov network with one node per groundatom and one feature per ground clause.
Theweight of a feature is the weight of the first-orderclause that originated it.
The probability of astate x in such a network is given by P (x) =(1/Z) exp (?i wifi(x)), where Z is a normaliza-tion constant, wi is the weight of the ith clause,fi = 1 if the ith clause is true, and fi = 0otherwise.
Conditional probabilities can be com-puted using Markov chain Monte Carlo (e.g., MC-SAT (Poon and Domingos, 2006)).
The weightscan be learned using pseudo-likelihood trainingwith L-BFGS (Richardson and Domingos, 2006).Markov logic is one of the most powerful rep-resentations for joint inference with uncertainty,and an implementation of its existing learning andinference algorithms is publicly available in theAlchemy package (Kok et al, 2007).To use the features defined in Section 4.1, ourMLN includes two evidence predicates: the firstone is HasFeature(i, l, f) where f is a feature inF1-F3.
The predicate is true iff the IGT-languagepair (i, l) has feature f .
The second predicate isHasRelation(i1, i2, r) where r is a relation thatcorresponds to a feature in F4; this predicate istrue iff relation r holds between two IGTs i1, i2.The query predicate is IsSame(i, l), which is trueiff IGT i is in language l. Table 6 shows the pred-icates instantiated from the two IGTs in Table 1.The language ID task can be captured in ourMLN with just three formulas:IsSame(i, l)HasFeature(i, l,+f) ?
IsSame(i, l)HasRelation(i1, i2,+r)?
IsSame(i1, l)?
IsSame(i2, l)The first formula captures the default probabil-ity that an IGT belongs to a particular language.IsSame(igt1, bin)HasFeature(igt1, bin, nearLC)HasFeature(igt1, bin, prev50)HasFeature(igt1, bin, LMw1)...HasFeature(igt1, lew, nearLC)HasFeature(igt1, lew, prev50)...IsSame(igt2, bin)HasFeature(igt2, bin, nearLC)HasFeature(igt2, bin, prev50)HasFeature(igt2, bin, LMw1)...HasRelation(igt1, igt2, IIw1)...Table 6: The predicates instantiated from the IGTsin Table 1The second one captures the conditional likeli-hoods of an IGT being in a language given the fea-tures.
The third formula says that two IGTs prob-ably belong to the same language if they have acertain relation r.The plus sign before f and r in the formulassignifies that the MLN will learn a separate weightfor each individual feature f and relation r. Notethat there is no plus sign before i and l, allowingthe MLN to achieve parameter tying by sharing thesame weights for different instances or languages.5.2.3 The advantage of the Coref approachBoth methods of the CoRef approach address thelimitations of the CL approach: both can handlethe unseen language problem, and both do param-eter tying in a natural way.
Not only does parame-ter tying reduce the number of parameters, it alsomakes it possible to accumulate evidence amongdifferent languages and different IGTs.6 ExperimentsIn this section, we compare the two approachesto the language ID task: the CL approach and theCoRef approach.
In our experiments, we run 10-fold cross validation (90% for training and 10%for testing) on the data set in Table 2 and reportthe average of language ID accuracy.The two approaches have different upperbounds.
The upper bound of the CL approach isthe percentage of IGTs in the test data that be-long to a seen language.
The upper bound of theCoRef approach is the percentage of IGTs in thetest data that belong to a language whose languagename appears in the same document.
For the dataset in Table 2, the upper bounds are 90.33% and875Table 7: The performance of the CL approach (# of classes: about 600, # of training instances=13,723)Upper bound of TextCat MaxEnt classifier using context informationCL approach F1 F1-F2 F1-F3 F1-F4 (cheating)# of features N/A N/A 769 5492 8226 8793w/o the language filter 90.33 51.38 49.74 61.55 64.19 66.47w/ the language filter 88.95 60.72 56.69 64.95 67.03 69.2097.31% respectively.
When the training data ismuch smaller, the upper bound of the CL approachwould decrease tremendously, whereas the upperbound of the CoRef approach remains the same.6.1 The CL approachAs mentioned before, most existing language IDalgorithm falls into this category.
We choseTextCat,9 an implementation of Cavnar-Trenkle?salgorithm (1994), as an example of these algo-rithms.
In order to take advantage of the con-text information, we trained several classifiers(e.g., decision tree, Naive Bayes, and maximumentropy) using the Mallet package (McCallum,2002) and a SVM classifier using the libSVMpackage (Chang and Lin, 2001).The result is in Table 7.
The first column showsthe upper bound of the CL approach; the secondcolumn is the result of running TextCat;10 the restof the table lists the result of running a MaxEntclassifier with different feature sets.11 F4 featuresrequire knowing the language code of other IGTsin the document.
In the F1-F4 cheating exper-iments, the language codes of other IGTs comefrom the gold standard.
We did not implementbeam search for this because the difference be-tween the cheating results and the results withoutF4 features is relatively small and both are muchworse than the results in the CoRef approach.In Table 7, the first row shows the number offeatures; the second row shows the accuracy of thetwo classifiers; the last row is the accuracy whena post-processing filter is added: the filter takesthe ranked language list produced by a classifier,throws away all the languages in the list that donot appear in the document, and then outputs thehighest ranked language in the remaining list.There are several observations.
First, applyingthe post-processing filter improves performance,9http://odur.let.rug.nl/ vannoord/TextCat/10We varied the lexicon size (m) ?
an important tuned pa-rameter for the algorithm ?
from 100 and 800 and observeda minor change to accuracy.
The numbers reported here arewith lexicon size set to 800.11The MaxEnt classifier slightly outperforms other classi-fiers with the same feature set.albeit it also lowers the upper bound of algorithmsas the correct language names might not appearin the document.
Second, the MaxEnt classifierhas hundreds of classes, thousands of features, andmillions of model parameters.
This will cause se-vere sparse data and overfitting problems.6.2 The CoRef approachFor the CoRef approach, we built two systems asdescribed in Section 5: the first system is a Max-Ent classifier with beam search, and the secondone is a MLN for joint inference.12 The resultsare in Table 8.13In the first system, the values of F4 featuresfor the test data come from the gold standardin the F1-F4 cheating experiments, and comefrom beam search in the non-cheating experi-ments.14 In the second system, the predicateHasRelation(i1, i2, r) instantiated from the testdata is treated as evidence in the F1-F4 cheat-ing experiments, and as query in the F1-F4 non-cheating experiments.The results for the two systems are very similarsince they use same kinds of features.
However,with Markov logic, it is easy to add predicates andformulas to allow joint inference.
Therefore, webelieve that Markov logic offers more potential toincorporate arbitrary prior knowledge and lever-age further opportunities in joint inference.Tables 7-8 show that, with the same kind of fea-tures and the same amount of training data, theCoRef approach has higher upper bound, fewermodel parameters, more training instances, andmuch higher accuracy than the CL approach.
Thisstudy shows that properly formulating a task intoa learning problem is very important.12For learning and inference, we used the existing im-plementations of pseudo-likelihood training and MC-SAT inAlchemy with default parameters.13No language filter is needed since the approach links anIGT to only the language names appearing in the document.14It turns out that for this task the size of beam does notmatter much and simply using the top choice by the Max-Ent classifier for each IGT almost always produces the bestresults, so that is the setting used for this table and Table 9.876Table 8: The performance of the CoRef approach (# of classes=2, # of training instances=511,039)Upper bound of F1 F1-F2 F1-F3 F1-F4 F1-F4CoRef approach (cheating) (Non-cheating)# of features N/A 2 12 17 22 22Sequence labeling 97.31 54.37 66.32 83.49 90.26 85.10Markov logic model 97.31 54.98 65.94 83.44 90.37 84.70Table 9: The performance of the CoRef approach with less training data (the upper bound of the Corefapproach remains 97.31%)% of training F1 F1-F2 F1-F3 F1-F4 F1-F4 Upper bound ofdata used (cheating) (non-cheating) the CL approach0.1% 54.37 54.84 65.28 81.21 70.15 1.660.5% 54.37 62.78 76.74 87.17 80.24 21.151.0% 54.37 60.58 76.09 87.24 81.20 28.9210% 54.37 62.13 77.07 87.20 83.08 54.456.3 Experiments with much less dataTable 8 shows that the CoRef approach has veryfew features and a much larger number of traininginstances; therefore, it is likely that the approachwould work well even with much less trainingdata.
To test the idea, we trained the model withonly a small fraction of the original training dataand tested on the same test data.
The results withthe first system are in Table 9.
Notice that the up-per bound of the CoRef approach remains the sameas before.
In contrast, the upper bound for the CLmodel is much lower, as shown in the last columnof the table.
The table shows when there is verylittle training data, the CoRef approach still per-forms decently, whereas the CL approach wouldtotally fail due to the extremely low upper bounds.6.4 Error analysisSeveral factors contribute to the gap between thebest CoRef system and its upper bound.
First,when several language names appear in closerange, the surface positions of the language namesare often insufficient to determine the prominenceof the languages.
For instance, in pattern ?Similarto L1, L2 ...?, L2 is the more prominent than L1;whereas in pattern ?L1, a L2 language, ...?, L1 is.The system sometimes chooses a wrong languagein this case.Second, the language name detector describedin Section 4.2 produces many false negative (dueto the incompleteness of the language table) andfalse positive (due to the fact that language namesoften have other meanings).Third, when a language name is ambiguous,choosing the correct language code often requiresknowledge that might not even be present in thedocument.
For instance, a language name couldrefer to a list of related languages spoken in thesame region, and assigning a correct languagecode would require knowledge about the subtledifferences among those languages.7 Conclusion and future workIn this paper we describe a language identificationmethodology that achieves high accuracy with avery small amount of training data for hundredsof languages, significantly outperforming existinglanguage ID algorithms applied to the task.
Thegain comes from two sources: by taking advan-tage of context information in the document, andby formulating the task as a coreference resolutionproblem.Our method can be adapted to harvest otherkinds of linguistic data from the Web (e.g., lexiconentries, word lists, transcriptions, etc.)
and buildother ODIN-like resources.
Providing a means forrapidly increasing the amount of data in ODIN,while at the same time automatically increasingthe number of languages, can have a significantpositive impact on the linguistic community, acommunity that already benefits from the existingsearch facility in ODIN.
Likewise, the increasedsize of the resulting ODIN database could pro-vide sufficient data to bootstrap NLP tools (e.g.,POS taggers and parsers) for a large number oflow-density languages, greatly benefitting both thefields of linguistics and NLP.Acknowledgements This work has been sup-ported, in part, by the NSF grants BCS-0748919and BCS-0720670 and ONR grant N00014-08-1-0670.
We would also like to thank three anony-mous reviewers for their valuable comments.877ReferencesMark C. Baker and Osamuyimen Thompson Stewart.1996.
Unaccusativity and the adjective/verb distinc-tion: Edo evidence.
In Proceedings of the Fifth An-nual Conference on Document Analysis and Infor-mation Retrieval (SDAIR), Amherst, Mass.G.
Bakir, T. Hofmann, B. Scholkopf, A. Smola,B.
Taskar, and S. Vishwanathan (eds).
2007.
Pre-dicting Structured Data.
MIT Press.William B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In Proceedings ofSDAIR-94, 3rd Annual Symposium on DocumentAnalysis and Information Retrieval, pages 161?175,Las Vegas, US.Chih-Chung Chang and Chih-Jen Lin, 2001.
LIBSVM:a library for support vector machines.
Available athttp://www.csie.ntu.edu.tw/ cjlin/libsvm.Pascal Denis and Jason Baldridge.
2007.
Joint de-termination of anaphoricity and coreference reso-lution using integer programming.
In Proc.
ofthe Conference on Human Language Technologies(HLT/NAACL 2007), pages 236?243, Rochester,New York, April.Aria Haghighi and Dan Klein.
2006.
Prototype-driven grammar induction.
In Proceedings of the21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associ-ation for Computational Linguistics (COLING/ACL2006), pages 881?888, Sydney, Australia, July.
As-sociation for Computational Linguistics.Baden Hughes, Timothy Baldwin, Steven Bird, JeremyNicholson, and Andrew MacKinlay.
2006.
Recon-sidering language identification for written languageresources.
In Proceedings of the 5th InternationalConference on Language Resources and Evaluation(LREC2006), pages 485?488, Genoa, Italy.S.
Kok, P. Singla, M. Richardson, P. Domingos,M.
Sumner, H Poon, and D. Lowd.
2007.
TheAlchemy system for statistical relational AI.
Tech-nical report, Dept.
of CSE, Univ.
of Washington.William Lewis and Fei Xia.
2008.
Automatically Iden-tifying Computationally Relevant Typological Fea-tures.
In Proc.
of the Third International Joint Con-ference on Natural Language Processing (IJCNLP-2008), Hyderabad, India.William Lewis.
2006.
ODIN: A Model for Adaptingand Enriching Legacy Infrastructure.
In Proc.
of thee-Humanities Workshop, held in cooperation with e-Science 2006: 2nd IEEE International Conferenceon e-Science and Grid Computing, Amsterdam.Xiaoqiang Luo.
2007.
Coreference or not: Atwin model for coreference resolution.
In Proc.
ofthe Conference on Human Language Technologies(HLT/NAACL 2007), pages 73?80, Rochester, NewYork.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Vincent Ng and Claire Cardie.
2002.
Improving Ma-chine Learning Approaches to Coreference Reso-lution.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics(ACL-2002), pages 104?111, Philadelphia.H.
Poon and P. Domingos.
2006.
Sound and effi-cient inference with probabilistic and deterministicdependencies.
In Proc.
of AAAI-06.Hoifung Poon and Pedro Domingos.
2007.
Joint in-ference in information extraction.
In Proceedingsof the Twenty-Second National Conference on Artifi-cial Intelligence (AAAI), pages 913?918, Vancouver,Canada.
AAAI Press.H.
Poon and P. Domingos.
2008.
Joint unsupervisedcoreference resolution with markov logic.
In Proc.of the 13th Conf.
on Empirical Methods in NaturalLanguage Processing (EMNLP-2008).M.
Richardson and P. Domingos.
2006.
Markov logicnetworks.
Machine Learning, pages 107?136.Wee Meng Soon, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A machine learning ap-proach to coreference resolution of noun phrases.Computational Linguistics, 27(4).Charles Sutton, Andrew McCallum, and Jeff Bilmes(eds.).
2006.
Proc.
of the HLT/NAACL-06 Work-shop on Joint Inference for Natural Language Pro-cessing.B.
Wellner, A. McCallum, F. Peng, and M. Hay.
2004.An integrated, conditional model of information ex-traction and coreference with application to citationmatching.
In Proc.
of the 20th Conference on Un-certainty in AI (UAI 2004).Fei Xia and William Lewis.
2007.
Multilingual struc-tural projection across interlinear text.
In Proc.
ofthe Conference on Human Language Technologies(HLT/NAACL 2007), pages 452?459, Rochester,New York.Fei Xia and William Lewis.
2008.
RepurposingTheoretical Linguistic Data for Tool Developmentand Search.
In Proc.
of the Third InternationalJoint Conference on Natural Language Processing(IJCNLP-2008), Hyderabad, India.878
