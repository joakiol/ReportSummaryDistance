First Joint Conference on Lexical and Computational Semantics (*SEM), pages 635?642,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUNT: A Supervised Synergistic Approachto Semantic Text SimilarityCarmen Banea, Samer Hassan, Michael Mohler, Rada MihalceaUniversity of North TexasDenton, TX, USA{CarmenBanea,SamerHassan,MichaelMohler}@my.unt.edu, rada@cs.unt.eduAbstractThis paper presents the systems that we par-ticipated with in the Semantic Text Similar-ity task at SEMEVAL 2012.
Based on priorresearch in semantic similarity and related-ness, we combine various methods in a ma-chine learning framework.
The three varia-tions submitted during the task evaluation pe-riod ranked number 5, 9 and 14 among the 89participating systems.
Our evaluations showthat corpus-based methods display a more ro-bust behavior on the training data, yet com-bining a variety of methods allows a learningalgorithm to achieve a superior decision thanthat achievable by any of the individual parts.1 IntroductionMeasures of text similarity have been used for along time in applications in natural language pro-cessing and related areas.
One of the earliest ap-plications of text similarity is perhaps the vector-space model used in information retrieval, where thedocument most relevant to an input query is deter-mined by ranking documents in a collection in re-versed order of their similarity to the given query(Salton and Lesk, 1971).
Text similarity has alsobeen used for relevance feedback and text classifi-cation (Rocchio, 1971), word sense disambiguation(Lesk, 1986; Schutze, 1998), and more recently forextractive summarization (Salton et al, 1997), andmethods for automatic evaluation of machine trans-lation (Papineni et al, 2002) or text summarization(Lin and Hovy, 2003).
Measures of text similaritywere also found useful for the evaluation of text co-herence (Lapata and Barzilay, 2005).Earlier work on this task has primarily focused onsimple lexical matching methods, which produce asimilarity score based on the number of lexical unitsthat occur in both input segments.
Improvementsto this simple method have considered stemming,stop-word removal, part-of-speech tagging, longestsubsequence matching, as well as various weight-ing and normalization factors (Salton and Buckley,1997).
While successful to a certain degree, theselexical similarity methods cannot always identify thesemantic similarity of texts.
For instance, there isan obvious similarity between the text segments Iown a dog and I have an animal, but most of thecurrent text similarity metrics will fail in identifyingany kind of connection between these texts.More recently, researchers have started to con-sider the possibility of combining the large numberof word-to-word semantic similarity measures (e.g.,(Jiang and Conrath, 1997; Leacock and Chodorow,1998; Lin, 1998; Resnik, 1995)) within a semanticsimilarity method that works for entire texts.
Themethods proposed to date in this direction mainlyconsist of either bipartite-graph matching strate-gies that aggregate word-to-word similarity into atext similarity score (Mihalcea et al, 2006; Islamand Inkpen, 2009; Hassan and Mihalcea, 2011;Mohler et al, 2011), or data-driven methods thatperform component-wise additions of semantic vec-tor representations as obtained with corpus measuressuch as Latent Semantic Analysis (Landauer et al,1997), Explicit Semantic Analysis (Gabrilovich andMarkovitch, 2007), or Salient Semantic Analysis(Hassan and Mihalcea, 2011).In this paper, we describe the system with which635we participated in the SEMEVAL 2012 task on se-mantic text similarity (Agirre et al, 2012).
The sys-tem builds upon our earlier work on corpus-basedand knowledge-based methods of text semantic sim-ilarity (Mihalcea et al, 2006; Hassan and Mihal-cea, 2011; Mohler et al, 2011), and combines allthese previous methods into a meta-system by us-ing machine learning.
The framework provided bythe task organizers also enabled us to perform an in-depth analysis of the various components used in oursystem, and draw conclusions concerning the roleplayed by the different resources, features, and al-gorithms in building a state-of-the-art semantic textsimilarity system.2 Related WorkOver the past years, the research community hasfocused on computing semantic relatedness usingmethods that are either knowledge-based or corpus-based.
Knowledge-based methods derive a measureof relatedness by utilizing lexical resources and on-tologies such as WordNet (Miller, 1995) to measuredefinitional overlap, term distance within a graph-ical taxonomy, or term depth in the taxonomy asa measure of specificity.
We explore several ofthese measures in depth in Section 3.3.1.
On theother side, corpus-based measures such as LatentSemantic Analysis (LSA) (Landauer et al, 1997),Explicit Semantic Analysis (ESA) (Gabrilovichand Markovitch, 2007), Salient Semantic Analysis(SSA) (Hassan and Mihalcea, 2011), Pointwise Mu-tual Information (PMI) (Church and Hanks, 1990),PMI-IR (Turney, 2001), Second Order PMI (Islamand Inkpen, 2006), Hyperspace Analogues to Lan-guage (Burgess et al, 1998) and distributional simi-larity (Lin, 1998) employ probabilistic approachesto decode the semantics of words.
They consistof unsupervised methods that utilize the contextualinformation and patterns observed in raw text tobuild semantic profiles of words.
Unlike knowledge-based methods, which suffer from limited coverage,corpus-based measures are able to induce a similar-ity between any given two words, as long as theyappear in the very large corpus used as training.3 Semantic Textual Similarity SystemThe system we proposed for the SEMEVAL 2012Semantic Textual Similarity task builds upon bothknowledge- and corpus-based methods previouslydescribed in (Mihalcea et al, 2006; Hassan and Mi-halcea, 2011; Mohler et al, 2011).
The predictionsof these independent systems, paired with additionalsalient features, are leveraged by a meta-system thatemploys machine learning.
In this section, we willelaborate further on the resources we use, our fea-tures, and the components of our machine learningsystem.
We will start by describing the task setup.3.1 Task SetupThe training data released by the task organiz-ers consists of three datasets showcasing two sen-tences per line and a manually assigned similarityscore ranging from 0 (no relation) to 5 (semanti-cally equivalent).
The datasets1 provided are takenfrom the Microsoft Research Paraphrase Corpus(MSRpar), the Microsoft Research Video Descrip-tion Corpus (MSRvid), and the WMT2008 devel-opment dataset (Europarl section)(SMTeuroparl);they each consist of about 750 sentence pairs withthe class distribution varying with each dataset.
Thetesting data contains additional sentences from thesame collections as the training data as well asfrom two additional unknown sets (OnWN andSMTnews); they range from 399 to 750 sentencepairs.
The reader may refer to (Agirre et al, 2012)for additional information regarding this task.3.2 ResourcesWikipedia2 is a free on-line encyclopedia, represent-ing the outcome of a continuous collaborative effortof a large number of volunteer contributors.
Virtu-ally any Internet user can create or edit a Wikipediaweb page, and this ?freedom of contribution?
has apositive impact on both the quantity (fast-growingnumber of articles) and the quality (potential mis-takes are quickly corrected within the collaborativeenvironment) of this on-line resource.
The basic en-try in Wikipedia is an article which describes an en-tity or an event, and which, in addition to untagged1http://www.cs.york.ac.uk/semeval-2012/task6/data/uploads/datasets/train-readme.txt2www.wikipedia.org636content, also consists of hyperlinked text to otherpages within or outside of Wikipedia.
These hyper-links are meant to guide the reader to pages that pro-vide additional information / clarifications, so thata better understanding of the primary concept canbe achieved.
The structure of Wikipedia in terms ofpages and hyperlinks is exploited directly by seman-tic similarity methods such as ESA (Gabrilovich andMarkovitch, 2007), or SSA (Hassan and Mihalcea,2011).WordNet (Miller, 1995) is a manually crafted lex-ical resource that maintains semantic relationshipsbetween basic units of meaning, or synsets.
A synsetgroups together senses of different words that sharea very similar meaning, which act in a particu-lar context as synonyms.
Each synset is accompa-nied by a gloss or definition, and one or two ex-amples illustrating usage in the given context.
Un-like a traditional thesaurus, the structure of Word-Net is able to encode additional relationships be-side synonymy, such as antonymy, hypernymy, hy-ponymy, meronymy, entailment, etc., which vari-ous knowledge-based methods use to derive seman-tic similarity.3.3 FeaturesOur meta-system uses several features, which canbe grouped into knowledge-based, corpus-based,and bipartite graph matching, as described below.The abbreviations appearing between parenthesesby each method allow for easy cross-referencingwith the evaluations provided in Table 1.3.3.1 Knowledge-based Semantic SimilarityFeaturesFollowing prior work from our group (Mihalceaet al, 2006; Mohler and Mihalcea, 2009), we em-ploy several WordNet-based similarity metrics forthe task of sentence-level similarity.
Briefly, foreach open-class word in one of the input texts, wecompute the maximum semantic similarity (usingthe WordNet::Similarity package (Pedersen et al,2004)) that can be obtained by pairing it with anyopen-class word in the other input text.
All theword-to-word similarity scores obtained in this wayare summed and normalized to the length of the twoinput texts.
We provide below a short descriptionfor each of the similarity metrics employed by thissystem3.The shortest path (Path) similarity is determinedas:Simpath =1length(1)where length is the length of the shortest path be-tween two concepts using node-counting (includingthe end nodes).The Leacock & Chodorow (Leacock andChodorow, 1998) (LCH) similarity is determinedas:Simlch = ?
loglength2 ?D(2)where length is the length of the shortest path be-tween two concepts using node-counting, and D isthe maximum depth of the taxonomy.The Lesk (Lesk) similarity of two concepts is de-fined as a function of the overlap between the cor-responding definitions, as provided by a dictionary.It is based on an algorithm proposed by Lesk (1986)as a solution for word sense disambiguation.The Wu & Palmer (Wu and Palmer, 1994) (WUP )similarity metric measures the depth of two givenconcepts in the WordNet taxonomy, and the depthof the least common subsumer (LCS), and combinesthese figures into a similarity score:Simwup =2 ?
depth(LCS)depth(concept1) + depth(concept2)(3)The measure introduced by Resnik (Resnik, 1995)(RES) returns the information content (IC) of theLCS of two concepts:Simres = IC(LCS) (4)where IC is defined as:IC(c) = ?
logP (c) (5)and P (c) is the probability of encountering an in-stance of concept c in a large corpus.The measure introduced by Lin (Lin, 1998) (Lin)builds on Resnik?s measure of similarity, and addsa normalization factor consisting of the informationcontent of the two input concepts:Simlin =2 ?
IC(LCS)IC(concept1) + IC(concept2)(6)3We point out that the similarity metric proposed by Hirst &St. Onge was not considered due to the time constraints associ-ated with the STS task.637We also consider the Jiang & Conrath (Jiang andConrath, 1997) (JCN ) measure of similarity:Simjnc =1IC(concept1) + IC(concept2)?
2 ?
IC(LCS)(7)Each of the measures listed above is used as a fea-ture by our meta-system.3.3.2 Corpus-based Semantic SimilarityFeaturesWhile most of the corpus-based methods inducesemantic profiles in a word-space, where the seman-tic profile of a word is expressed in terms of its co-occurrence with other words, LSA, ESA and SSAstand out as different, since they rely on a concept-space representation.
In these methods, the semanticprofile of a word is expressed in terms of the im-plicit (LSA), explicit (ESA), or salient (SSA) con-cepts.
This departure from the sparse word-space toa denser, richer, and unambiguous concept-space re-solves one of the fundamental problems in semanticrelatedness, namely the vocabulary mismatch.
In theexperiments reported in this paper, all the corpus-based methods are trained on the English Wikipediadownload from October 2008, with approximately6 million articles, and more than 9.5 million hyper-links.Latent Semantic Analysis (LSA) (Landauer et al,1997).
In LSA, term-context associations are cap-tured by means of a dimensionality reduction op-erated by a singular value decomposition (SVD)on the term-by-context matrix T, where the ma-trix is induced from a large corpus.
This reduc-tion entails the abstraction of meaning by collaps-ing similar contexts and discounting noisy and ir-relevant ones, hence transforming the real worldterm-context space into a word-latent-concept spacewhich achieves a much deeper and concrete seman-tic representation of words.Explicit Semantic Analysis (ESA) (Gabrilovichand Markovitch, 2007).
ESA uses encyclopedicknowledge in an information retrieval framework togenerate a semantic interpretation of words.
Sinceencyclopedic knowledge is typically organized intoconcepts (or topics), each concept is further de-scribed using definitions and examples.
ESA relieson the distribution of words inside the encyclopedicdescriptions.
It builds semantic representations fora given word using a word-document association,where the document represents a Wikipedia article(concept).
ESA is in effect a Vector Space Model(VSM) built using Wikipedia corpus, where vectorsrepresents word-articles association.Salient Semantic Analysis (SSA) (Hassan and Mi-halcea, 2011).
SSA incorporates a similar seman-tic abstraction and interpretation of words as ESA,yet it uses salient concepts gathered from encyclo-pedic knowledge, where a ?concept?
represents anunambiguous word or phrase with a concrete mean-ing, and which affords an encyclopedic definition.Saliency in this case is determined based on theword being hyperlinked (either trough manual or au-tomatic annotations) in context, implying that theyare highly relevant to the given text.
SSA is an ex-ample of Generalized Vector Space Model (GVSM),where vectors represent word-concepts associations.In order to determine the similarity of two textfragments , we employ two variations: the typicalcosine similarity (cos) and a best alignment strat-egy (align), which we explain in more detail below.Both variations were paired with the LSA, ESA,and SSA systems resulting in six similarity scoresthat were used as features by our meta-system,namely LSAcos, LSAalign, ESAcos, ESAalign,SSAcos, and SSAalign.Best Alignment Strategy (align).
Let Ta and Tb betwo text fragments of size a and b respectively.
Afterremoving all stopwords, we first determine the num-ber of shared terms (?)
between Ta and Tb.
Second,we calculate the semantic relatedness of all possiblepairings between non-shared terms in Ta and Tb.
Wefurther filter these possible combinations by creatinga list ?
which holds the strongest semantic pairingsbetween the fragments?
terms, such that each termcan only belong to one and only one pair.Sim(Ta, Tb) =(?
+?|?|i=1 ?i)?
(2ab)a + b(8)where ?
is the number of shared terms between thetext fragments and ?i is the similarity score for theith pairing.3.3.3 Bipartite Graph MatchingIn an attempt to move beyond the bag-of-wordsparadigm described thus far, we attempt to compute638a set of dependency graph alignment scores basedon previous work in automatic short-answer grading(Mohler et al, 2011).
This score, computed in twostages, is used as a feature by our meta-system.In the first stage, the system is provided with thedependency graphs for each pair of sentences4.
Foreach node in one dependency graph, we compute asimilarity score for each node in the other depen-dency graph based upon a set of lexical, semantic,and syntactic features applied to both the pair ofnodes and their corresponding subgraphs (i.e.
the setof nodes reachable from a given node by followingdirectional governor-to-dependant links).
The scor-ing function is trained on a small set of manuallyaligned graphs using the averaged perceptron algo-rithm.We define a total of 64 features5 to be used to traina machine learning system to compute subgraph-subgraph similarity.
Of these, 32 are based upon thebag-of-words semantic similarity of the subgraphsusing the metrics described in Section 3.3.1 as wellas a Wikipedia-trained LSA model.
The remaining32 features are lexico-syntactic features associatedwith the parent nodes of the subgraphs and are de-scribed in more detail in our earlier paper.We then calculate weights associated with thesefeatures using an averaged version of the percep-tron algorithm (Freund and Schapire, 1999; Collins,2002) trained on a set of 32 manually annotatedinstructor/student answer pairs selected from theshort-answer grading corpus (MM2011).
Thesepairs contain 7303 node pairs (656 matches, 6647non-matches).
Once the weights are calculated, asimilarity score for each pair of nodes can be com-puted by taking the dot product of the feature vectorwith the weights.In the second stage, the node similarity scores cal-culated in the previous step are used to find an op-timal alignment for the pair of dependency graphs.We begin with a bipartite graph where each nodein one graph is represented by a node on the leftside of the bipartite graph and each node in the other4We here use the output of the Stanford Dependency Parserin collapse/propagate mode with some modifications as de-scribed in our earlier work.5With the exception of the four features based upon the Hirst& St.Onge similarity metric, these are equivalent to the featuresused in previous work.graph is represented by a node on the right side.
Theweight associated with each edge is the score com-puted for each node-node pair in the previous stage.The bipartite graph is then augmented by addingdummy nodes to both sides which are allowed tomatch any node with a score of zero.
An optimalalignment between the two graphs is then computedefficiently using the Hungarian algorithm.
Note thatthis results in an optimal matching, not a mapping,so that an individual node is associated with at mostone node in the other answer.
After finding the opti-mal match, we produce four alignment-based scoresby optionally normalizing by the number of nodesand/or weighting the node-alignments according tothe idf scores of the words.6 This results in fouralignment scores listed as graphnone, graphnorm,graphidf , graphidfnorm.3.3.4 BaselinesAs a baseline, we also utilize several lexical bag-of-words approaches where each sentence is repre-sented by a vector of tokens and the similarity of thetwo sentences can be computed by finding the co-sine of the angle between their representative vectorsusing term frequency (tf ) or term frequency mul-tiplied by inverse document frequency (tf.idf )6, orby using simple overlap between the vectors?
dimen-sions (overlap).3.4 Machine Learning3.4.1 AlgorithmsAll the systems described above are used to gen-erate a score for each training and test sample (seeSection 3.1).
These scores are then aggregated persample, and used in a supervised learning frame-work.
We decided to use a regression model, insteadof classification, since the requirements for the taskspecify that we should provide a score in the range of0 to 5.
We could have used classification paired withbucketed ranges, yet classification does not take intoconsideration the underlying ordinality of the scores(i.e.
a score of 4.5 is closer to either 4 or 5, butfarther away from 0), which is a noticeable handi-cap in this scenario.
We tried both linear and sup-6The document frequency scores were taken from the BritishNational Corpus (BNC).639port vector regression7 by performing 10 fold cross-validation on the train data, yet the latter algorithmconsistently performs better, no matter what kernelwas chosen.
Thus we decided to use support vec-tor regression (Smola and Schoelkopf, 1998) with aPearson VII function-based kernel.Due to its different learning methodology, andsince it is suited for predicting continuous classes,our second system uses the M5P decision tree al-gorithm (Quinlan, 1992; Wang and Witten, 1997),which outperforms support vector regression on the10 fold cross-validation performed on the SMTeu-roparl train set, while providing competitive resultson the other train sets (within .01 Pearson correla-tion).3.4.2 SetupWe submitted three system variations, namelyIndividualRegression, IndividualDecTree,and CombinedRegression.
The first word de-scribes the training data; for individual, for theknown test sets we trained on the correspondingtrain sets, while for the unknown test sets we trainedon all the train sets combined; for combined,for each test set we trained on all the train setscombined.
The second word refers to the learningmethodology, where Regression stands for supportvector regression, and DecTree stands for M5Pdecision tree.4 Results and DiscussionWe include in Table 1 the Pearson correlations ob-tained by comparing the predictions of each fea-ture to the gold standard for the three train datasets.We notice that the corpus based metrics display aconsistent performance across the three train sets,when compared to the other methods, includingknowledge-based.
Furthermore, the best alignmentstrategy (align) for corpus based models outper-forms similarity scores based on traditional cosinesimilarity.
It is interesting to note that simple base-lines such as tf , tf.idf and overlap offer signifi-cant correlations with all the train sets without ac-cess to additional knowledge inferred by knowledgeor corpus-based methods.
In the case of the bipar-7Implementations provided through the Weka framework(Hall et al, 2009).System MSRpar MSRvid SMTeuroparlPath 0.49 0.62 0.50LCH 0.48 0.49 0.45Lesk 0.48 0.59 0.50WUP 0.46 0.38 0.42RES 0.47 0.55 0.48Lin 0.49 0.54 0.48JCN 0.49 0.63 0.51LSAalign 0.44 0.57 0.61LSAcos 0.37 0.74 0.56ESAalign 0.52 0.70 0.62ESAcos 0.30 0.71 0.53SSAalign 0.46 0.61 0.65SSAcos 0.22 0.63 0.39graphnone 0.42 0.50 0.21graphnorm 0.48 0.43 0.59graphidf 0.16 0.67 0.16graphidfnorm 0.08 0.60 0.19tf.idf 0.45 0.63 0.41tf 0.45 0.69 0.51overlap 0.44 0.69 0.27Table 1: Correlation of individual features for the trainingsets with the gold standardstite graph matching, the graphnorm variation pro-vides the strongest correlation results across all thedatasets.We include the evaluation results provided by thetask organizers in Table 2.
They indicate that our in-tuition in using a support vector regression strategywas correct.
While the IndividualRegression wasour strongest system on the training data, the sameranking applies to the test data (including the addi-tional two surprise datasets) as well, earning it thefifth place among the 89 participating systems, witha Pearson correlation of 0.7846.Regarding the decision tree based learning(IndividualDecTree), despite its more robust be-havior on the train sets, it achieved slightly loweroutcome on the test data, at 0.7677 correlation.
Webelieve this happened because decision trees have atendency to overfit training data, as they generate arigid structure which is unforgiving to minor devia-tions in the test data.
Nonetheless, this second vari-ation still ranks in the top 10% of the submitted sys-tems.As an alternative approach to handle unknown testdata (e.g.
different distributions, genres), we opted640Run ALL Rank Mean RankMean MSRpar MSRvid SMTeuroparl OnWN SMTnewsIndividualRegression 0.7846 5 0.6162 13 0.5353 0.8750 0.4203 0.6715 0.4033IndividualDecTree 0.7677 9 0.5947 25 0.5693 0.8688 0.4203 0.6491 0.2256CombinedRegression 0.7418 14 0.6159 14 0.5032 0.8695 0.4797 0.6715 0.4033Table 2: Evaluation results and ranking published by the task organizersto also include the CombinedRegression strategyas our third variation.
This seems to have been fruit-ful for MSRvid, SMTeuroparl, and the two sur-prise datasets (ONWn and SMTnews).
In thecase of SMTeuroparl, this expanded training setachieves a better performance than learning fromthe corresponding training set alne, gaining an im-provement of 0.0776 correlation points.
Unfortu-nately, the variation has some losses, particularly forthe MSRpar dataset (0.0321), yet it is able to con-sistently model and handle a wider variety of texttypes.5 ConclusionThis paper describes the three system variations ourteam participated with in the Semantic Text Similar-ity task in SEMEVAL 2012.
Our focus has been toproduce a synergistic approach, striving to achieve asuperior result than attainable by each system indi-vidually.
We have considered a variety of methodsfor inferring semantic similarity, including knowl-edge and corpus-based methods.
These were lever-aged in a machine-learning framework, where ourpreferred learning algorithm is support vector re-gression, due to its ability to deal with continuousclasses and to dampen the effect of noisy features,while augmenting more robust ones.
While it is al-ways preferable to use similar test and train sets,when information regarding the test dataset is un-available, we show that a robust performance canbe achieved by combining all train data from dif-ferent sources into a single set and allowing a ma-chine learner to make predictions.
Overall, it wasinteresting to note that corpus-based methods main-tain strong results on all train datasets in compari-son to knowledge-based methods.
Our three systemsranked number 5, 9 and 14 among the 89 systemsparticipating in the task.AcknowledgmentsThis material is based in part upon work sup-ported by the National Science Foundation CA-REER award #0747340 and IIS award #1018613.Any opinions, findings, and conclusions or recom-mendations expressed in this material are those ofthe authors and do not necessarily reflect the viewsof the National Science Foundation.ReferencesE.
Agirre, D. Cer, M. Diab, and A. Gonzalez.
2012.Semeval-2012 task 6: A pilot on semantic textual sim-ilarity.
In Proceedings of the 6th International Work-shop on Semantic Evaluation (SemEval 2012), in con-junction with the First Joint Conference on Lexical andComputational Semantics (*SEM 2012).C.
Burgess, K. Livesay, and K. Lund.
1998.
Explorationsin context space: words, sentences, discourse.
Dis-course Processes, 25(2):211?257.K.
Church and P. Hanks.
1990.
Word association norms,mutual information, and lexicography.
ComputationalLinguistics, 16(1):22?29.M.
Collins.
2002.
Discriminative training methodsfor hidden Markov models: Theory and experimentswith perceptron algorithms.
In Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP-02), Philadelphia, PA,July.Y.
Freund and R. Schapire.
1999.
Large margin clas-sification using the perceptron algorithm.
MachineLearning, 37:277?296.E.
Gabrilovich and S. Markovitch.
2007.
Computingsemantic relatedness using Wikipedia-based explicitsemantic analysis.
In Proceedings of the 20th Inter-national Joint Conference on Artificial Intelligence,pages 1606?1611, Hyderabad, India.M.
Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reute-mann, and I. H. Witten.
2009.
The WEKA DataMining Software: An Update.
SIGKDD Explorations,11(1).S.
Hassan and R. Mihalcea.
2011.
Measuring semanticrelatedness using salient encyclopedic concepts.
Arti-ficial Intelligence, Special Issue, xx(xx).641A.
Islam and D. Inkpen.
2006.
Second order co-occurrence PMI for determining the semantic similar-ity of words.
In Proceedings of the Fifth Conferenceon Language Resources and Evaluation, volume 2,Genoa, Italy, July.A.
Islam and D. Inkpen.
2009.
Semantic Similarity ofShort Texts.
In Nicolas Nicolov, Galia Angelova, andRuslan Mitkov, editors, Recent Advances in NaturalLanguage Processing V, volume 309 of Current Issuesin Linguistic Theory, pages 227?236.
John Benjamins,Amsterdam & Philadelphia.J.
J. Jiang and D. W. Conrath.
1997.
Semantic similaritybased on corpus statistics and lexical taxonomy.
InInternational Conference Research on ComputationalLinguistics (ROCLING X), pages 9008+, September.T.
K. Landauer, D. Laham, B. Rehder, and M. E.Schreiner.
1997.
How well can passage meaning bederived without using word order?
a comparison oflatent semantic analysis and humans.M.
Lapata and R. Barzilay.
2005.
Automatic evaluationof text coherence: Models and representations.
In Pro-ceedings of the 19th International Joint Conference onArtificial Intelligence, Edinburgh.C.
Leacock and M. Chodorow, 1998.
Combining localcontext and WordNet similarity for word sense identi-fication, pages 305?332.M.
Lesk.
1986.
Automatic sense disambiguation us-ing machine readable dictionaries: how to tell a pinecone from an ice cream cone.
In SIGDOC ?86: Pro-ceedings of the 5th annual international conference onSystems documentation, pages 24?26, New York, NY,USA.
ACM.C.
Y. Lin and E. H. Hovy.
2003.
Automatic evaluation ofsummaries using n-gram co-occurrence statistics.
InProceedings of Human Language Technology Confer-ence (HLT-NAACL 2003), Edmonton, Canada, May.D.
Lin.
1998.
An information-theoretic definition ofsimilarity.
In Proceedings of the Fifteenth Interna-tional Conference on Machine Learning, pages 296?304, Madison, Wisconsin.R.
Mihalcea, C. Corley, and C. Strapparava.
2006.Corpus-based and knowledge-based measures of textsemantic similarity.
In Proceedings of the AmericanAssociation for Artificial Intelligence (AAAI 2006),pages 775?780, Boston, MA, US.G.
A. Miller.
1995.
WordNet: a Lexical database forenglish.
Communications of the Association for Com-puting Machinery, 38(11):39?41.M.
Mohler and R. Mihalcea.
2009.
Text-to-text seman-tic similarity for automatic short answer grading.
InProceedings of the European Association for Compu-tational Linguistics (EACL 2009), Athens, Greece.M.
Mohler, R. Bunescu, and R. Mihalcea.
2011.
Learn-ing to grade short answer questions using semanticsimilarity measures and dependency graph alignments.In Proceedings of the Association for ComputationalLinguistics ?
Human Language Technologies (ACL-HLT 2011), Portland, Oregon, USA.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.Bleu: a method for automatic evaluation of machinetranslation.
In Proceedings of the 40th Annual Meet-ing of the Association for Computational Linguistics,pages 311?318, Philadelphia, PA.T.
Pedersen, S. Patwardhan, and J. Michelizzi.
2004.WordNet:: Similarity-Measuring the Relatedness ofConcepts.
Proceedings of the National Conference onArtificial Intelligence, pages 1024?1025.R.
J. Quinlan.
1992.
Learning with continuous classes.In 5th Australian Joint Conference on Artificial Intel-ligence, pages 343?348, Singapore.
World Scientific.P.
Resnik.
1995.
Using information content to evaluatesemantic similarity in a taxonomy.
In In Proceedingsof the 14th International Joint Conference on ArtificialIntelligence, pages 448?453.J.
Rocchio, 1971.
Relevance feedback in information re-trieval.
Prentice Hall, Ing.
Englewood Cliffs, New Jer-sey.G.
Salton and C. Buckley.
1997.
Term weighting ap-proaches in automatic text retrieval.
In Readings inInformation Retrieval.
Morgan Kaufmann Publishers,San Francisco, CA.G.
Salton and M.E.
Lesk, 1971.
The SMART RetrievalSystem: Experiments in Automatic Document Process-ing, chapter Computer evaluation of indexing and textprocessing.
Prentice Hall, Ing.
Englewood Cliffs, NewJersey.G.
Salton, A. Singhal, M. Mitra, and C. Buckley.
1997.Automatic text structuring and summarization.
Infor-mation Processing and Management, 2(32).H.
Schutze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?124.A.
J. Smola and B. Schoelkopf.
1998.
A tutorial on sup-port vector regression.
NeuroCOLT2 Technical Re-port NC2-TR-1998-030.P.
D. Turney.
2001.
Mining the Web for Synonyms:PMI-IR versus LSA on TOEFL.
In Proceedings ofthe 12th European Conference on Machine Learning,pages 491?502, Freiburg, Germany.Y.
Wang and I. H. Witten.
1997.
Induction of model treesfor predicting continuous classes.
In Poster papers ofthe 9th European Conference on Machine Learning.Springer.Z.
Wu and M. Palmer.
1994.
Verbs semantics and lexicalselection.
In Proceedings of the 32nd annual meetingon Association for Computational Linguistics, pages133?-138, Las Cruces, New Mexico.642
