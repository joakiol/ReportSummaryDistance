Coling 2010: Poster Volume, pages 955?962,Beijing, August 2010The Role of Queries in Ranking Labeled Instances Extracted from TextMarius Pas?caGoogle Inc.mars@google.comAbstractA weakly supervised method usesanonymized search queries to induce aranking among class labels extracted fromunstructured text for various instances.The accuracy of the extracted class labelsexceeds that of previous methods, overevaluation sets of instances associatedwith Web search queries.1 IntroductionClasses pertaining to unrestricted domains (e.g.,west african countries, science fiction films, slrcameras) and their instances (cape verde, avatar,canon eos 7d) play a disproportionately importantrole in Web search.
They occur prominently inWeb documents and among search queries sub-mitted most frequently by Web users (Jansen etal., 2000).
They also serve as building blocks informal representation of human knowledge, andare useful in a variety of text processing tasks.Recent work on offline acquisition of fine-grained, labeled classes of instances appliesmanually-created (Banko et al, 2007; Talukdar etal., 2008) or automatically-learned (Snow et al,2006) extraction patterns to large document col-lections.
Although various methods exploit addi-tional textual resources to increase accuracy (VanDurme and Pas?ca, 2008) and coverage (Talukdaret al, 2008), some of the extracted class labelsare inevitably less useful (works) or spurious (carmakers) for an associated instance (avatar).
InWeb search, the relative ranking of documents re-turned for a query directly affects the outcome ofthe search.
Similarly, the relative ranking amongclass labels extracted for a given instance influ-ences any applications using the labels.Our paper proposes the use of features otherthan those computed over the underlying doc-ument collection, such as the frequency of co-occurrence or diversity of extraction patterns pro-ducing a given pair (Etzioni et al, 2005), to deter-mine the relative ranking of various class labels,given a class instance.
Concretely, the methodtakes advantage of the co-occurrence of a classlabel and an instance within search queries fromanonymized query logs.
It re-ranks lists of classlabels produced for an instance by standard ex-traction patterns, to promote class labels that co-occur with the instance.
This corresponds to a softranking approach, focusing on the ranking of can-didate extractions such as the less relevant onesare ranked lower, as opposed to removed whendeemed unreliable based on various clues.By using queries in ranking, the ranked listsof class labels available for various instances areinstrumental in determining the classes to whichgiven sets of instances belong.
The accuracy ofthe class labels exceeds that of previous work,over evaluation sets of instances associated withWeb search queries.
The results confirm the use-fulness of the extracted IsA repository, which re-mains general-purpose and is not tailored to anyparticular task.2 Instance Class Ranking2.1 Extraction of Instances and ClassesThe initial extraction of labeled instances relieson hand-written patterns from (Hearst, 1992),widely used in work on extracting hierarchiesfrom text (Snow et al, 2006; Ponzetto and Strube,9552007):?[..]
C [such as|including] I [and|,|.
]?,where I is a potential instance (e.g., diderot) andC is a potential class label (e.g., writers).Following (Van Durme and Pas?ca, 2008), theboundaries of potential class labels C are approx-imated from the part-of-speech tags of the sen-tence words, whereas the boundaries of instancesI are identified by checking that I occurs as anentire query in query logs.
Since users type manyqueries in lower case, the collected data is con-verted to lower case.When applied to inherently-noisy Web docu-ments, the extraction patterns may produce irrele-vant extractions (Kozareva et al, 2008).
Causes oferrors include incorrect detection of possible enu-merations, as in companies such as Procter andGamble (Downey et al, 2007); incorrect estima-tion of the boundaries of class labels, due to in-correct attachment as in years from on a limitednumber of vehicles over the past few years, includ-ing the Chevrolet Corvette; subjective (famous ac-tors) (Hovy et al, 2009), relational (competitors,nearby landmarks) and otherwise less useful (oth-ers, topics) class labels; or questionable sourcesentences, as in Large mammals such as deer andwild turkeys can be [..] (Van Durme and Pas?ca,2008).As a solution, recent work uses additional evi-dence, as a means to filter the pairs extracted bypatterns, thus trading off coverage for higher pre-cision.
The repository extracted from a similarly-sized Web document collection using the sameinitial extraction patterns as here, after a weightedintersection of pairs extracted with patterns andclusters of distributionally similar phrases, con-tains a total of 9,080 class labels associated with263,000 instances in (Van Durme and Pas?ca,2008).
Subsequent extensions of the repository,using data derived from tables within Web doc-uments, increase instance coverage and induce aranking among class labels of each instance, butdo not increase the number of class labels (Taluk-dar et al, 2008).
Due to aggressive filtering, theresulting number of class labels is higher than theoften-small sets of entity types studied previously,but may still be insufficient given the diversity ofWeb search queries.2.2 Ranking of Classes per InstanceAs an alternative, the soft ranking approach pro-posed here attempts to rank better class labelshigher, without necessarily removing class labelsdeemed incorrect according to various criteria.For each instance I , the associated class labels areranked in the following stages:1) Apply the scoring formula below, resultingin a ranked list of class labels L1(I):Score(I, C) = Size({Pattern(I,C)})2 ?
Freq(I, C)Thus, a class label C is deemed more relevantfor an instance I if C is extracted by multiple ex-traction patterns and its original frequency-basedscore is higher.2) For each term within any class label fromL1(I), compute a score equal to the frequencysum of the term within anonymized queries con-taining the instance I as a prefix, and the termanywhere else in the queries.
Each class label isassigned the geometric mean of the scores of itsterms, after ignoring stop words.
The class labelsare ranked according to the means, resulting in aranked list L2(I).
In case of ties, L2(I) preservesthe relative ranking from L1(I).
Thus, a class la-bel is deemed more relevant if its individual termsoccur in popular queries containing the instance.3) Compute a merged ranked list of class labelsout of the ranked lists L1(I) and L2(I), by sortingthe class labels in decreasing order of the inverseof the average rank, computed with the followingformula:MergedScore(C) = 2Rank(C, L1) + Rank(C, L2)where 2 is the number of input lists of class la-bels, and Rank(C, Li) is the rank of C in the listLi of class labels computed for the correspond-ing input instance.
The rank is set to 1000, if Cis not present in the list Li.
By using only therelative ranks of the class labels within the inputlists, and not on their scores, the outcome of themerging is less sensitive to how class labels of agiven instance are scored within the IsA reposi-tory.
In case of ties, the scores of the class labelsfrom L1(I) serve as a secondary ranking criterion.Note that the third stage is introduced becauserelying on query logs to estimate the relevance of956class labels exposes the ranking method to signifi-cant noise.
On one hand, arguably useful class la-bels (e.g., authors) may not occur in queries alongwith the respective instances (diderot).
On theother hand, for each query containing an instanceand (part of) useful class labels, there are manyother queries containing, e.g., attributes (diderotbiography or diderot beliefs) or the name of abook in the query diderot the nun.
Therefore, theranked lists L2(I) may be too noisy to be used di-rectly as rankings of the class labels for I .3 Experimental Setting3.1 Textual Data SourcesThe acquisition of the IsA repository relies on un-structured text available within Web documentsand search queries.
The collection of queries isa sample of 50 million unique, fully-anonymizedqueries in English submitted by Web users in2009.
Each query is accompanied by its frequencyof occurrence in the logs.
The document col-lection consists of a sample of 100 million doc-uments in English.
The textual portion of thedocuments is cleaned of HTML, tokenized, splitinto sentences and part-of-speech tagged using theTnT tagger (Brants, 2000).3.2 Experimental RunsThe experimental runs correspond to differentmethods for extracting and ranking pairs of an in-stance and a class:?
as available in the repository from (Talukdaret al, 2008), which is collected from a docu-ment collection similar in size to the one usedhere plus a collection of Web tables, in a rundenoted Rg;?
from the repository extracted here, with classlabels of an instance ranked based on the fre-quency and the number of extraction patterns(see Score(I, C) in Section 2), in run Rs;?
from the repository extracted here, with classlabels of an instance ranked based on theMergedScore from Section 2, in run Ru.3.3 Evaluation ProcedureThe manual evaluation of open-domain informa-tion extraction output is time consuming (Bankoet al, 2007).
Fortunately, it is possible to im-plement an automatic evaluation procedure forranked lists of class labels, based on existing re-sources and systems.
Assume that a gold stan-dard is available, containing gold class labels thatare each associated with a gold set of their in-stances.
The creation of such gold standards isdiscussed later.
Based on the gold standard, theranked lists of class labels available within an IsArepository can be automatically evaluated as fol-lows.
First, for each gold label, the ranked listsof class labels of individual gold instances are re-trieved from the IsA repository.
Second, the in-dividual retrieved lists are merged into a rankedlist of class labels, associated with the gold label.The merged list is computed using an extensionof the MergedScore formula described earlierin Section 2.
Third, the merged list is comparedagainst the gold label, to estimate the accuracy ofthe merged list.
Intuitively, a ranked list of classlabels is a better approximation of a gold label, ifclass labels situated at better ranks in the list arecloser in meaning to the gold label.3.4 Evaluation MetricGiven a gold label and a list of class labels, if any,derived from the IsA repository, the rank of thehighest class label that matches the gold label de-termines the score assigned to the gold label, inthe form of the reciprocal rank, max(1/rankmatch).Thus, if the gold label matches a class label at rank1, 2, 3, 4 or 5 in the computed list, the gold labelreceives a score of 1, 0.5, 0.33, 0.25 or 0.2 respec-tively.
The score is 0 if the gold label does notmatch any of the top 20 class labels.
The overallscore over the entire set of gold labels is the meanreciprocal rank (MRR) score over all gold labelsfrom the set.
Two types of MRR scores are auto-matically computed:?
MRRf considers a gold label and a class la-bel to match if they are identical;?
MRRp considers a gold label and a class la-bel to match if one or more of their tokensthat are not stop words are identical.957During matching, all string comparisons arecase-insensitive, and all tokens are first convertedto their singular form (e.g., european countriesto european country) when available, by usingWordNet?s morphological routines.
Thus, insur-ance carriers and insurance companies are con-sidered to not match in MRRf scores, but matchin MRRp scores, whereas insurance companiesand insurance company match in both MRRf andMRRp scores.
Note that both MRRf and MRRpscores fail to give any credit to arguably validand useful class labels, such as insurers for thegold label insurance carriers, or asian nationsfor the gold label asia countries.
On the otherhand, MRRp scores may give credit to less rele-vant class labels, such as insurance policies for thegold label insurance carriers.
Therefore, MRRpis an approximate, and MRRf is a conservative,lower-bound estimate of the actual usefulness ofthe computed ranked lists of class labels as ap-proximations of the semantics of the gold labels.4 Evaluation Results4.1 Evaluation Sets of QueriesA random sample of anonymized, class-seekingqueries (e.g., video game characters or smart-phone) submitted by Web users to GoogleSquared 1 over a 30-day interval is filtered, to re-move queries for which Google Squared returnsfewer than 10 instances at the time of the evalua-tion.
The resulting evaluation set of queries, de-noted Qe, contains 807 queries, each associatedwith a ranked list of between 10 and 100 instancesautomatically extracted by Google Squared.Since the instances available as input for eachquery as part of Qe are automatically extracted,they may (e.g., acorn a7000) or may not (e.g.,konrad zuse) be true instances of the respectivequeries (e.g., computers).
A second evaluationset Qm is assembled as a subset of 40 queriesfrom Qe, such that the instances available for eachquery in Qm are correct.
For this purpose, eachinstance returned by Google Squared for the 401Google Squared (http://www.google.com/squared) is aWeb search tool taking as input class-seeking queries (e.g.,insurance companies) and returning lists of instances (e.g.,allstate, state farm insurance), along with attributes (e.g., in-dustry, headquarters) and values for each instance.Query Set: Sample of QueriesQe (807 queries): 2009 movies, amino acids,asian countries, bank, board games, buildings,capitals, chemical functional groups, clothes,computer language, dairy farms near modestoca, disease, egyptian pharaohs, eu countries,french presidents, german islands, hawaiian is-lands, illegal drugs, irc clients, lakes, mac-intosh models, mobile operator india, nbaplayers, nobel prize winners, orchids, photoeditors, programming languages, renaissanceartists, roller costers, science fiction tv series,slr cameras, soul singers, states of india, tal-iban members, thomas edison inventions, u.s.presidents, us president, water slidesQm (40 queries): actors, airlines, birds, cars,celebrities, computer languages, digital cam-era, dog breeds, drugs, endangered animals,european countries, fruits, greek gods, hor-ror movies, ipods, names, netbooks, operat-ing systems, park slope restaurants, presidents,ps3 games, religions, renaissance artists, rockbands, universities, university, vitaminsTable 1: Size and composition of evaluation setsof queries associated with non-filtered (Qe) ormanually-filtered (Qm) instancesqueries from Qm is reviewed by at least three hu-man annotators.
Instances deemed highly rele-vant (out of 5 possible grades) with high inter-annotator agreement are retained.
As a result, the40 queries from Qm are associated with between8 and 33 human-validated instances.Table 1 shows a sample of the queries from Qeand queries from Qm.
A small number of queriesare slight lexical variations of one another, such asu.s.
presidents and us presidents in Qe, or univer-sities and university in Qm.
In general, however,the sets cover a wide range of domains of inter-est, including entertainment for 2009 movies androck bands; biology for endangered animals andamino acids; geography for asian countries andhawaiian islands; food for fruits; history for egyp-tian pharaohs and greek gods; health for drugsand vitamins; and technology for photo editorsand ipods.
Some of the queries from Table 1are specific enough that computing them exactly,958AccuracyIQ 3 5 10 15CI 5 10 20 5 10 20 5 10 20 5 10 20MRRf computed over Qe:Rg 0.106 0.112 0.112 0.121 0.122 0.123 0.131 0.135 0.127 0.134 0.132 0.127Rs 0.186 0.195 0.198 0.198 0.207 0.210 0.204 0.214 0.218 0.206 0.216 0.221Ru 0.202 0.211 0.216 0.232 0.238 0.244 0.245 0.255 0.257 0.245 0.252 0.254MRRp computed over Qe:Rg 0.390 0.399 0.394 0.420 0.420 0.413 0.443 0.443 0.435 0.439 0.431 0.425Rs 0.489 0.495 0.495 0.517 0.528 0.529 0.541 0.553 0.557 0.551 0.557 0.557Ru 0.520 0.531 0.533 0.564 0.573 0.578 0.590 0.601 0.602 0.598 0.603 0.601MRRf computed over Qm:Rg 0.284 0.289 0.295 0.305 0.327 0.322 0.320 0.335 0.335 0.334 0.328 0.337Rs 0.406 0.436 0.442 0.431 0.447 0.466 0.467 0.470 0.501 0.484 0.501 0.554Ru 0.423 0.426 0.429 0.436 0.483 0.508 0.500 0.526 0.530 0.520 0.540 0.524MRRp computed over Qm:Rg 0.507 0.517 0.531 0.495 0.509 0.518 0.555 0.553 0.550 0.563 0.561 0.572Rs 0.667 0.662 0.660 0.675 0.677 0.699 0.702 0.695 0.716 0.756 0.765 0.787Ru 0.711 0.703 0.680 0.734 0.731 0.748 0.733 0.797 0.782 0.799 0.834 0.819Table 2: Accuracy of instance set labeling, as full-match (MRRf ) or partial-match (MRRp) scores overthe evaluation sets of queries associated with non-filtered instances (Qe) or manually-filtered instances(Qm), for various experimental runs (IQ=number of instances available in the input evaluation sets thatare used for retrieving class labels; CI=number of class labels retrieved from IsA repository per inputinstance)even from a comprehensive, perfect list of ex-tracted instance, would be very difficult whetherdone automatically or manually.
Examples ofsuch queries are dairy farms near modesto ca andscience fiction tv series, but also mobile opera-tor india (phrase expressed as keywords) in Qe, orpark slope restaurants (specific location) in Qm.Access to a system such as Google Squared isuseful, but not necessary to conduct the evalua-tion.
Given other sets of queries, it is straightfor-ward, albeit time consuming, to create evaluationsets similar to Qm, by manually compiling correctinstances, for each selected query or concept.Following the general evaluation procedure,each query from the sets Qe and Qm acts as a goldclass label associated with its set of instances.Given a query and its instances I from the evalu-ation sets Qe or Qm, we compute merged, rankedlists of class labels, by merging the ranked lists ofclass labels available in the underlying IsA reposi-tory for each instance I .
The evaluation comparesthe merged lists of class labels, on one hand, andthe corresponding queries from Qe or Qm, on theother hand.4.2 Accuracy of Class LabelsTable 2 summarizes results from comparative ex-periments, quantifying a) horizontally, the impactof alternative parameter settings on the computedlists of class labels; and b) vertically, the compar-ative accuracy of the experimental runs over thequery sets.
The experimental parameters are thenumber of input instances from the evaluation setsthat are used for retrieving class labels, IQ, set to3, 5, 10 and 15; and the number of class labelsretrieved per input instance, CI , set to 5, 10 and20.The scores over Qm are higher than thoseover Qe, confirming the intuition that the higher-quality input set of instances available in Qm rel-ative to Qe should lead to higher-quality class la-bels for the corresponding queries.
When IQ isfixed, increasing CI leads to small, if any, scoreimprovements.
Conversely, when CI is fixed,959even small values of IQ, such as 3 or 5 (that is,very small sets of instances provided as input) pro-duce scores that are competitive with those ob-tained with a higher value like.
This suggests thatuseful class labels can be generated even in ex-treme scenarios, where the number of instancesavailable as input is as small as 3 or 5.For most combinations of parameter settingsand on both query sets, run Ru produces the high-est scores.
In particular, when IQ is set to 10 andCI to 20, run Ru identifies the original query asan exact match among the top four class labelsreturned; and as a partial match among the toptwo class labels returned, as an average over theQe set.
In this case, the original query is iden-tified at ranks 1, 2, 3, 4 and 5 for 16.8%, 8.7%,6.1%, 3.7% and 1.7% of the queries, as an ex-act match; and for 48.8%, 14.2%, 6.1%, 3.6% and1.9% respectively, as a partial match.
The corre-sponding MRRf score of 0.257 over the Qe setobtained with run Ru is higher than with run Rs,and much higher than with run Rg.
In all experi-ments, the higher scores of Ru can be attributed tohigher coverage of class labels, relative to Rg; andhigher-quality lists of class labels, relative to Rsbut also to Rg, despite the fact that Rg combineshigh-precision seed data with using both unstruc-tured and structured text as sources of class labels(cf.
(Talukdar et al, 2008)).
Among combinationsof parameter settings described in Table 2, valuesaround 15 for IQ and 20 for CI give the highestscores over both Qe and Qm.5 Related Work5.1 Extraction of IsA RepositoriesKnowledge including instances and classes can bemanually compiled by experts (Fellbaum, 1998)or collaboratively by non-experts (Singh et al,2002).
Alternatively, classes of instances acquiredautomatically from text are potentially less ex-pensive to acquire, maintain and grow, and theircoverage and scope are theoretically bound onlyby the size of the underlying data source.
Ex-isting methods for extracting classes of instancesacquire sets of instances that are each either un-labeled (Wang and Cohen, 2008; Pennacchiottiand Pantel, 2009; Lin and Wu, 2009), or as-sociated with a class label (Pantel and Pennac-chiotti, 2006; Banko et al, 2007; Wang and Co-hen, 2009).
When associated with a class la-bel, the sets of instances may be organized asflat sets or hierarchically, relative to existing hi-erarchies such as WordNet (Snow et al, 2006) orthe category network within Wikipedia (Wu andWeld, 2008; Ponzetto and Navigli, 2009).
Semi-structured text was shown to be a complemen-tary resource to unstructured text, for the purposeof extracting relations from Web documents (Ca-farella et al, 2008).The role of anonymized query logs in Web-based information extraction has been exploredin the tasks of class attribute extraction (Pas?caand Van Durme, 2007) and instance set ex-pansion (Pennacchiotti and Pantel, 2009).
Ourmethod illustrates the usefulness of queries con-sidered in isolation from one another, in rankingclass labels in extracted IsA repositories.5.2 Labeling of Instance SetsPrevious work on generating relevant labels, givensets or clusters of items, focuses on scenarioswhere the items within the clusters are descrip-tions of, or full-length documents within docu-ment collections.
The documents are available asa flat set (Cutting et al, 1993; Carmel et al, 2009)or are hierarchically organized (Treeratpituk andCallan, 2006).
Relying on semi-structured con-tent assembled manually as part of the struc-ture of Wikipedia articles, such as article titlesor categories, the method introduced in (Carmelet al, 2009) derives labels for clusters contain-ing 100 full-length documents each.
In contrast,our method relies on IsA relations automaticallyextracted from unstructured text within arbitraryWeb documents, and computes labels given tex-tual input that is orders of magnitude smaller, i.e.,around 10 phrases (instances).
The experimentsdescribed in (Carmel et al, 2009) assign labels toone of 20 sets of newsgroup documents from astandard benchmark.
Each set of documents is as-sociated with a higher-level, coarse-grained labelused as a gold label against which the generatedlabels are compared.
In comparison, our experi-ments compute text-derived class labels for finer-grained, often highly-specific gold labels.960Reducing the granularity of the items to be la-beled from full documents to condensed docu-ment descriptions, (Geraci et al, 2006) submitsarbitrary search queries to external Web search en-gines.
It organizes the top 200 returned Web doc-uments into clusters, by analyzing the text snip-pets associated with each document in the outputfrom the search engines.
Any words and phrasesfrom the snippets may be selected as labels for theclusters, which in general leads to labels that arenot intended to capture any classes that may be as-sociated to the query.
For example, labels of clus-ters generated in (Geraci et al, 2006) include arm-strong ceilings, italia, armstrong sul sito and louisjazz for the query armstrong; and madonnaweb,music, madonna online and madonna itself for thequery madonna.
The amount of text available asinput for the purpose of labeling is at least two or-ders of magnitude larger than in our method, andthe task of selecting any phrases as labels, as op-posed to selecting only labels that correspond toclasses, is more relaxed and likely easier.Another approach specifically addresses theproblem of generating labels for sets of instances,where the labels are extracted from unstructuredtext.
In (Pantel and Ravichandran, 2004), given acollection of news articles that is both cleaner andsmaller than Web document collections, a syn-tactic parser is applied to document sentences inorder to identify and exploit syntactic dependen-cies for the purpose of selecting candidate classlabels.
Such methods are comparatively less ap-plicable to Web document collections, due to scal-ability issues associated with parsing a large setof Web documents of variable quality.
Moreover,the class labels generated in (Pantel and Ravichan-dran, 2004) tend to be rather coarse-grained.
Forexample, the top labels generated for a set of Chi-nese universities (qinghua university, fudan uni-versity, beijing university) are university, institu-tion, stock-holder, college and school.6 ConclusionThe method presented in this paper produces anIsA repository whose class labels have highercoverage and accuracy than with recent meth-ods operating on document collections.
This isdone by injecting useful ranking signals frominherently-noisy queries, rather than making bi-nary, coverage-reducing quality decisions on theextracted data.
Current work investigates the use-fulness of the extracted class labels in the gener-ation of flat or hierarchical query refinements forclass-seeking queries.AcknowledgmentsThe author thanks Randolph Brown for assistancein assembling the evaluation sets of class-seekingqueries.ReferencesBanko, M., Michael J Cafarella, S. Soderland,M.
Broadhead, and O. Etzioni.
2007.
Open infor-mation extraction from the Web.
In Proceedings ofthe 20th International Joint Conference on ArtificialIntelligence (IJCAI-07), pages 2670?2676, Hyder-abad, India.Brants, T. 2000.
TnT - a statistical part of speechtagger.
In Proceedings of the 6th Conference onApplied Natural Language Processing (ANLP-00),pages 224?231, Seattle, Washington.Cafarella, M., A. Halevy, D. Wang, E. Wu, andY.
Zhang.
2008.
WebTables: Exploring the powerof tables on the Web.
In Proceedings of the 34thConference on Very Large Data Bases (VLDB-08),pages 538?549, Auckland, New Zealand.Carmel, D., H. Roitman, and N. Zwerding.
2009.
En-hancing cluster labeling using Wikipedia.
In Pro-ceedings of the 32nd ACM Conference on Researchand Development in Information Retrieval (SIGIR-09), pages 139?146, Boston, Massachusetts.Cutting, D., D. Karger, and J. Pedersen.
1993.Constant interaction-time scatter/gather browsing ofvery large document collections.
In Proceedings ofthe 16th ACM Conference on Research and Devel-opment in Information Retrieval (SIGIR-93), pages126?134, Pittsburgh, Pennsylvania.Downey, D., M. Broadhead, and O. Etzioni.
2007.
Lo-cating complex named entities in Web text.
In Pro-ceedings of the 20th International Joint Conferenceon Artificial Intelligence (IJCAI-07), pages 2733?2739, Hyderabad, India.Etzioni, O., M. Cafarella, D. Downey, A. Popescu,T.
Shaked, S. Soderland, D. Weld, and A. Yates.2005.
Unsupervised named-entity extraction fromthe Web: an experimental study.
Artificial Intelli-gence, 165(1):91?134.Fellbaum, C., editor.
1998.
WordNet: An Elec-tronic Lexical Database and Some of its Applica-tions.
MIT Press.961Geraci, F., M. Pellegrini, M. Maggini, and F. Sebas-tiani.
2006.
Cluster generation and cluster la-belling for Web snippets: A fast and accurate hi-erarchical solution.
In Proceedings of the 13th Con-ference on String Processing and Information Re-trieval (SPIRE-06), pages 25?36, Glasgow, Scot-land.Hearst, M. 1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the14th International Conference on ComputationalLinguistics (COLING-92), pages 539?545, Nantes,France.Hovy, E., Z. Kozareva, and E. Riloff.
2009.
Towardcompleteness in concept extraction and classifica-tion.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Process-ing (EMNLP-09), pages 948?957, Singapore.Jansen, B., A. Spink, and T. Saracevic.
2000.
Reallife, real users, and real needs: a study and analysisof user queries on the Web.
Information Processingand Management, 36(2):207?227.Kozareva, Z., E. Riloff, and E. Hovy.
2008.
Seman-tic class learning from the web with hyponym pat-tern linkage graphs.
In Proceedings of the 46th An-nual Meeting of the Association for ComputationalLinguistics (ACL-08), pages 1048?1056, Columbus,Ohio.Lin, D. and X. Wu.
2009.
Phrase clustering for dis-criminative learning.
In Proceedings of the 47thAnnual Meeting of the Association for Computa-tional Linguistics (ACL-IJCNLP-09), pages 1030?1038, Singapore.Pas?ca, M. and B.
Van Durme.
2007.
What you seekis what you get: Extraction of class attributes fromquery logs.
In Proceedings of the 20th InternationalJoint Conference on Artificial Intelligence (IJCAI-07), pages 2832?2837, Hyderabad, India.Pantel, P. and M. Pennacchiotti.
2006.
Espresso:Leveraging generic patterns for automatically har-vesting semantic relations.
In Proceedings of the21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics (COLING-ACL-06), pages 113?120, Sydney, Australia.Pantel, P. and D. Ravichandran.
2004.
Automati-cally labeling semantic classes.
In Proceedings ofthe 2004 Human Language Technology Conference(HLT-NAACL-04), pages 321?328, Boston, Mas-sachusetts.Pennacchiotti, M. and P. Pantel.
2009.
Entity extrac-tion via ensemble semantics.
In Proceedings of the2009 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP-09), pages 238?247, Singapore.Ponzetto, S. and R. Navigli.
2009.
Large-scale tax-onomy mapping for restructuring and integratingWikipedia.
In Proceedings of the 21st InternationalJoint Conference on Artificial Intelligence (IJCAI-09), pages 2083?2088, Pasadena, California.Ponzetto, S. and M. Strube.
2007.
Deriving a largescale taxonomy from Wikipedia.
In Proceedingsof the 22nd National Conference on Artificial In-telligence (AAAI-07), pages 1440?1447, Vancouver,British Columbia.Singh, P., T. Lin, E. Mueller, G. Lim, T. Perkins,and W. Zhu.
2002.
Open Mind Common Sense:Knowledge acquisition from the general public.
InProceedings of the ODBASE Conference (ODBASE-02), pages 1223?1237.Snow, R., D. Jurafsky, and A. Ng.
2006.
Semantic tax-onomy induction from heterogenous evidence.
InProceedings of the 21st International Conference onComputational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics(COLING-ACL-06), pages 801?808, Sydney, Aus-tralia.Talukdar, P., J. Reisinger, M. Pas?ca, D. Ravichan-dran, R. Bhagat, and F. Pereira.
2008.
Weakly-supervised acquisition of labeled class instances us-ing graph random walks.
In Proceedings of the2008 Conference on Empirical Methods in Natu-ral Language Processing (EMNLP-08), pages 582?590, Honolulu, Hawaii.Treeratpituk, P. and J. Callan.
2006.
Automatically la-beling hierarchical clusters.
In Proceedings of the7th Annual Conference on Digital Government Re-search (DGO-06), pages 167?176, San Diego, Cali-fornia.Van Durme, B. and M. Pas?ca.
2008.
Finding cars, god-desses and enzymes: Parametrizable acquisition oflabeled instances for open-domain information ex-traction.
In Proceedings of the 23rd National Con-ference on Artificial Intelligence (AAAI-08), pages1243?1248, Chicago, Illinois.Wang, R. and W. Cohen.
2008.
Iterative set expan-sion of named entities using the web.
In Proceed-ings of the International Conference on Data Min-ing (ICDM-08), pages 1091?1096, Pisa, Italy.Wang, R. and W. Cohen.
2009.
Automatic set instanceextraction using the Web.
In Proceedings of the47th Annual Meeting of the Association for Compu-tational Linguistics (ACL-IJCNLP-09), pages 441?449, Singapore.Wu, F. and D. Weld.
2008.
Automatically refining theWikipedia infobox ontology.
In Proceedings of the17th World Wide Web Conference (WWW-08), pages635?644, Beijing, China.962
