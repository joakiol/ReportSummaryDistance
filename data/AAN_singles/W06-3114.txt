Proceedings of the Workshop on Statistical Machine Translation, pages 102?121,New York City, June 2006. c?2006 Association for Computational LinguisticsManual and Automatic Evaluation of Machine Translationbetween European LanguagesPhilipp KoehnSchool of InformaticsUniversity of Edinburghpkoehn@inf.ed.ac.ukChristof MonzDepartment of Computer ScienceQueen Mary, University of Londonchristof@dcs.qmul.ac.ukAbstractWe evaluated machine translation perfor-mance for six European language pairsthat participated in a shared task: translat-ing French, German, Spanish texts to En-glish and back.
Evaluation was done auto-matically using the BLEU score and man-ually on fluency and adequacy.For the 2006 NAACL/HLT Workshop on Ma-chine Translation, we organized a shared task toevaluate machine translation performance.
14 teamsfrom 11 institutions participated, ranging from com-mercial companies, industrial research labs to indi-vidual graduate students.The motivation for such a competition is to estab-lish baseline performance numbers for defined train-ing scenarios and test sets.
We assembled variousforms of data and resources: a baseline MT system,language models, prepared training and test sets,resulting in actual machine translation output fromseveral state-of-the-art systems and manual evalua-tions.
All this is available at the workshop website1.The shared task is a follow-up to the one we orga-nized in the previous year, at a similar venue (Koehnand Monz, 2005).
As then, we concentrated on thetranslation of European languages and the use of theEuroparl corpus for training.
Again, most systemsthat participated could be categorized as statisticalphrase-based systems.
While there is now a num-ber of competitions ?
DARPA/NIST (Li, 2005),IWSLT (Eck and Hori, 2005), TC-Star ?
this onefocuses on text translation between various Euro-pean languages.This year?s shared task changed in some aspectsfrom last year?s:?
We carried out a manual evaluation in additionto the automatic scoring.
Manual evaluation1http://www.statmt.org/wmt06/was done by the participants.
This revealedinteresting clues about the properties of auto-matic and manual scoring.?
We evaluated translation from English, in ad-dition to into English.
English was againpaired with German, French, and Spanish.We dropped, however, one of the languages,Finnish, partly to keep the number of tracksmanageable, partly because we assumed that itwould be hard to find enough Finnish speakersfor the manual evaluation.?
We included an out-of-domain test set.
This al-lows us to compare machine translation perfor-mance in-domain and out-of-domain.1 Evaluation FrameworkThe evaluation framework for the shared task is sim-ilar to the one used in last year?s shared task.
Train-ing and testing is based on the Europarl corpus.
Fig-ure 1 provides some statistics about this corpus.1.1 Baseline systemTo lower the barrier of entrance to the competition,we provided a complete baseline MT system, alongwith data resources.
To summarize, we provided:?
sentence-aligned, tokenized training corpus?
a development and development test set?
trained language models for each language?
the phrase-based MT decoder Pharaoh?
a training script to build models for PharaohThe performance of the baseline system is simi-lar to the best submissions in last year?s shared task.We are currently working on a complete open sourceimplementation of a training and decoding system,which should become available over the summer.102Training corpusSpanish ?
English French ?
English German ?
EnglishSentences 730,740 688,031 751,088Foreign words 15,676,710 15,323,737 15,256,793English words 15,222,105 13,808,104 16,052,269Distinct foreign words 102,886 80,349 195,291Distinct English words 64,123 61,627 65,889Language model dataEnglish Spanish French GermanSentence 1,003,349 1,070,305 1,066,974 1,078,141Words 27,493,499 29,129,720 31,604,879 26,562,167In-domain test setEnglish Spanish French GermanSentences 2,000Words 59,307 61,824 66,783 55,533Unseen words 141 206 164 387Ratio of unseen words 0.23% 0.40% 0.24% 0.70%Distinct words 6,031 7,719 7,230 8,812Distinct unseen words 139 203 163 385Out-of-domain test setEnglish Spanish French GermanSentences 1,064Words 25,919 29,826 31,937 26,818Unseen words 464 368 839 913Ratio of unseen words 1.79% 1.23% 2.62% 3.40%Distinct words 5,166 5,689 5,728 6,594Distinct unseen words 340 267 375 637Figure 1: Properties of the training and test sets used in the shared task.
The training data is the Europarl cor-pus, from which also the in-domain test set is taken.
There is twice as much language modelling data, sincetraining data for the machine translation system is filtered against sentences of length larger than 40 words.Out-of-domain test data is from the Project Syndicate web site, a compendium of political commentary.103ID Participantcmu Carnegie Mellon University, USA (Zollmann and Venugopal, 2006)lcc Language Computer Corporation, USA (Olteanu et al, 2006b)ms Microsoft, USA (Menezes et al, 2006)nrc National Research Council, Canada (Johnson et al, 2006)ntt Nippon Telegraph and Telephone, Japan (Watanabe et al, 2006)rali RALI, University of Montreal, Canada (Patry et al, 2006)systran Systran, Franceuedin-birch University of Edinburgh, UK ?
Alexandra Birch (Birch et al, 2006)uedin-phi University of Edinburgh, UK ?
Philipp Koehn (Birch et al, 2006)upc-jg University of Catalonia, Spain ?
Jesu?s Gime?nez (Gime?nez and Ma`rquez, 2006)upc-jmc University of Catalonia, Spain ?
Josep Maria Crego (Crego et al, 2006)upc-mr University of Catalonia, Spain ?
Marta Ruiz Costa-jussa` (Costa-jussa` et al, 2006)upv University of Valencia, Spain (Sa?nchez and Bened?
?, 2006)utd University of Texas at Dallas, USA (Olteanu et al, 2006a)Figure 2: Participants in the shared task.
Not all groups participated in all translation directions.1.2 Test DataThe test data was again drawn from a segment ofthe Europarl corpus from the fourth quarter of 2000,which is excluded from the training data.
Partici-pants were also provided with two sets of 2,000 sen-tences of parallel text to be used for system develop-ment and tuning.In addition to the Europarl test set, we also col-lected 29 editorials from the Project Syndicate web-site2, which are published in all the four languagesof the shared task.
We aligned the texts at a sen-tence level across all four languages, resulting in1064 sentence per language.
For statistics on thistest set, refer to Figure 1.The out-of-domain test set differs from the Eu-roparl data in various ways.
The text type are edi-torials instead of speech transcripts.
The domain isgeneral politics, economics and science.
However, itis also mostly political content (even if not focusedon the internal workings of the European Union) andopinion.1.3 ParticipantsWe received submissions from 14 groups from 11institutions, as listed in Figure 2.
Most of thesegroups follow a phrase-based statistical approach tomachine translation.
Microsoft?s approach uses de-2http://www.project-syndicate.com/pendency trees, others use hierarchical phrase mod-els.
Systran submitted their commercial rule-basedsystem that was not tuned to the Europarl corpus.About half of the participants of last year?s sharedtask participated again.
The other half was replacedby other participants, so we ended up with roughlythe same number.
Compared to last year?s sharedtask, the participants represent more long-term re-search efforts.
This may be the sign of a maturingresearch environment.While building a machine translation system isa serious undertaking, in future we hope to attractmore newcomers to the field by keeping the barrierof entry as low as possible.For more on the participating systems, please re-fer to the respective system description in the pro-ceedings of the workshop.2 Automatic EvaluationFor the automatic evaluation, we used BLEU, since itis the most established metric in the field.
The BLEUmetric, as all currently proposed automatic metrics,is occasionally suspected to be biased towards sta-tistical systems, especially the phrase-based systemscurrently in use.
It rewards matches of n-gram se-quences, but measures only at most indirectly over-all grammatical coherence.The BLEU score has been shown to correlatewell with human judgement, when statistical ma-104chine translation systems are compared (Dodding-ton, 2002; Przybocki, 2004; Li, 2005).
However, arecent study (Callison-Burch et al, 2006), pointedout that this correlation may not always be strong.They demonstrated this with the comparison of sta-tistical systems against (a) manually post-edited MToutput, and (b) a rule-based commercial system.The development of automatic scoring methods isan open field of research.
It was our hope that thiscompetition, which included the manual and auto-matic evaluation of statistical systems and one rule-based commercial system, will give further insightinto the relation between automatic and manual eval-uation.
At the very least, we are creating a data re-source (the manual annotations) that may the basisof future research in evaluation metrics.2.1 Computing BLEU ScoresWe computed BLEU scores for each submission witha single reference translation.
For each sentence,we counted how many n-grams in the system outputalso occurred in the reference translation.
By takingthe ratio of matching n-grams to the total number ofn-grams in the system output, we obtain the preci-sion pn for each n-gram order n. These values forn-gram precision are combined into a BLEU score:BLEU = BP ?
exp(4?n=1log pn) (1)BP = min(1, e1?r/c) (2)The formula for the BLEU metric also includes abrevity penalty for too short output, which is basedon the total number of words in the system output cand in the reference r.BLEU is sensitive to tokenization.
Because ofthis, we retokenized and lowercased submitted out-put with our own tokenizer, which was also used toprepare the training and test data.2.2 Statistical SignificanceConfidence Interval: Since BLEU scores are notcomputed on the sentence level, traditional methodsto compute statistical significance and confidenceintervals do not apply.
Hence, we use the bootstrapresampling method described by Koehn (2004).Following this method, we repeatedly ?
say,1000 times ?
sample sets of sentences from the out-put of each system, measure their BLEU score, anduse these 1000 BLEU scores as basis for estimatinga confidence interval.
When dropping the top andbottom 2.5% the remaining BLEU scores define therange of the confidence interval.Pairwise comparison: We can use the same methodto assess the statistical significance of one systemoutperforming another.
If two systems?
scores areclose, this may simply be a random effect in the testdata.
To check for this, we do pairwise bootstrap re-sampling: Again, we repeatedly sample sets of sen-tences, this time from both systems, and comparetheir BLEU scores on these sets.
If one system is bet-ter in 95% of the sample sets, we conclude that itshigher BLEU score is statistically significantly bet-ter.The bootstrap method has been critized by Riezlerand Maxwell (2005) and Collins et al (2005), as be-ing too optimistic in deciding for statistical signifi-cant difference between systems.
We are thereforeapplying a different method, which has been used atthe 2005 DARPA/NIST evaluation.We divide up each test set into blocks of 20 sen-tences (100 blocks for the in-domain test set, 53blocks for the out-of-domain test set), check for eachblock, if one system has a higher BLEU score thanthe other, and then use the sign test.The sign test checks, how likely a sample of betterand worse BLEU scores would have been generatedby two systems of equal performance.Let say, if we find one system doing better on 20of the blocks, and worse on 80 of the blocks, is itsignificantly worse?
We check, how likely only upto k = 20 better scores out of n = 100 would havebeen generated by two equal systems, using the bi-nomial distribution:p(0..k;n, p) =k?i=0(in)pipn?i= 0.5nk?i=0(in) (3)If p(0..k;n, p) < 0.05, or p(0..k;n, p) > 0.95then we have a statistically significant difference be-tween the systems.105Figure 3: Annotation tool for manual judgement of adequacy and fluency of the system output.
Translationsfrom 5 randomly selected systems for a randomly selected sentence is presented.
No additional informationbeyond the instructions on this page are given to the judges.
The tool tracks and reports annotation speed.3 Manual EvaluationWhile automatic measures are an invaluable toolfor the day-to-day development of machine trans-lation systems, they are only a imperfect substitutefor human assessment of translation quality, or asthe acronym BLEU puts it, a bilingual evaluationunderstudy.Many human evaluation metrics have been pro-posed.
Also, the argument has been made that ma-chine translation performance should be evaluatedvia task-based evaluation metrics, i.e.
how much itassists performing a useful task, such as supportinghuman translators or aiding the analysis of texts.The main disadvantage of manual evaluation isthat it is time-consuming and thus too expensive todo frequently.
In this shared task, we were also con-fronted with this problem, and since we had no fund-ing for paying human judgements, we asked partic-ipants in the evaluation to share the burden.
Par-ticipants and other volunteers contributed about 180hours of labor in the manual evaluation.3.1 Collecting Human JudgementsWe asked participants to each judge 200?300 sen-tences in terms of fluency and adequacy, the mostcommonly used manual evaluation metrics.
We set-tled on contrastive evaluations of 5 system outputsfor a single test sentence.
See Figure 3 for a screen-shot of the evaluation tool.Presenting the output of several system allowsthe human judge to make more informed judge-ments, contrasting the quality of the different sys-tems.
The judgements tend to be done more in formof a ranking of the different systems.
We assumedthat such a contrastive assessment would be benefi-cial for an evaluation that essentially pits differentsystems against each other.While we had up to 11 submissions for a trans-lation direction, we did decide against presentingall 11 system outputs to the human judge.
Our ini-tial experimentation with the evaluation tool showedthat this is often too overwhelming.Making the ten judgements (2 types for 5 sys-tems) takes on average 2 minutes.
Typically, judges106initially spent about 3 minutes per sentence, but thenaccelerate with experience.
Judges where excludedfrom assessing the quality of MT systems that weresubmitted by their institution.
Sentences and sys-tems were randomly selected and randomly shuffledfor presentation.We collected around 300?400 judgements perjudgement type (adequacy or fluency), per system,per language pair.
This is less than the 694 judge-ments 2004 DARPA/NIST evaluation, or the 532judgements in the 2005 DARPA/NIST evaluation.This decreases the statistical significance of our re-sults compared to those studies.
The number ofjudgements is additionally fragmented by our break-up of sentences into in-domain and out-of-domain.3.2 Normalizing the judgementsThe human judges were presented with the follow-ing definition of adequacy and fluency, but no addi-tional instructions:Adequacy Fluency5 All Meaning Flawless English4 Most Meaning Good English3 Much Meaning Non-native English2 Little Meaning Disfluent English1 None IncomprehensibleJudges varied in the average score they handedout.
The average fluency judgement per judgeranged from 2.33 to 3.67, the average adequacyjudgement ranged from 2.56 to 4.13.
Since differentjudges judged different systems (recall that judgeswere excluded to judge system output from theirown institution), we normalized the scores.The normalized judgement per judge is the rawjudgement plus (3 minus average raw judgement forthis judge).
In words, the judgements are normal-ized, so that the average normalized judgement perjudge is 3.Another way to view the judgements is that theyare less quality judgements of machine translationsystems per se, but rankings of machine translationsystems.
In fact, it is very difficult to maintain con-sistent standards, on what (say) an adequacy judge-ment of 3 means even for a specific language pair.The way judgements are collected, human judgestend to use the scores to rank systems against eachother.
If one system is perfect, another has slightflaws and the third more flaws, a judge is inclinedto hand out judgements of 5, 4, and 3.
On the otherhand, when all systems produce muddled output, butone is better, and one is worse, but not completelywrong, a judge is inclined to hand out judgements of4, 3, and 2.
The judgement of 4 in the first case willgo to a vastly better system output than in the secondcase.We therefore also normalized judgements on aper-sentence basis.
The normalized judgement persentence is the raw judgement plus (0 minus averageraw judgement for this judge on this sentence).Systems that generally do better than others willreceive a positive average normalized judgement persentence.
Systems that generally do worse than oth-ers will receive a negative one.One may argue with these efforts on normaliza-tion, and ultimately their value should be assessedby assessing their impact on inter-annotator agree-ment.
Given the limited number of judgements wereceived, we did not try to evaluate this.3.3 Statistical SignificanceConfidence Interval: To estimate confidence inter-vals for the average mean scores for the systems, weuse standard significance testing.Given a set of n sentences, we can compute thesample mean x?
and sample variance s2 of the indi-vidual sentence judgements xi:x?
=1nn?i=1xi (4)s2 =1n?
1n?i=1(xi ?
x?
)2 (5)The extend of the confidence interval [x?
?d, x?+d]can be computed byd = 1.96 ?s?n(6)Pairwise Comparison: As for the automatic evalu-ation metric, we want to be able to rank different sys-tems against each other, for which we need assess-ments of statistical significance on the differencesbetween a pair of systems.Unfortunately, we have much less data to workwith than with the automatic scores.
The way we107Basis Diff.
RatioSign test on BLEU 331 75%Bootstrap on BLEU 348 78%Sign test on Fluency 224 50%Sign test on Adequacy 225 51%Figure 4: Number and ratio of statistically signifi-cant distinction between system performance.
Au-tomatic scores are computed on a larger tested thanmanual scores (3064 sentences vs. 300?400 sen-tences).collected manual judgements, we do not necessar-ily have the same sentence judged for both systems(judges evaluate 5 systems out of the 8?10 partici-pating systems).Still, for about good number of sentences, we dohave this direct comparison, which allows us to ap-ply the sign test, as described in Section 2.2.4 Results and AnalysisThe results of the manual and automatic evaluationof the participating system translations is detailed inthe figures at the end of this paper.
The scores andconfidence intervals are detailed first in the Figures7?10 in table form (including ranks), and then ingraphical form in Figures 11?16.
In the graphs, sys-tem scores are indicated by a point, the confidenceintervals by shaded areas around the point.In all figures, we present the per-sentence normal-ized judgements.
The normalization on a per-judgebasis gave very similar ranking, only slightly lessconsistent with the ranking from the pairwise com-parisons.The confidence intervals are computed by boot-strap resampling for BLEU, and by standard signif-icance testing for the manual scores, as describedearlier in the paper.Pairwise comparison is done using the sign test.Often, two systems can not be distinguished witha confidence of over 95%, so there are ranked thesame.
This actually happens quite frequently (morebelow), so that the rankings are broad estimates.
Forinstance: if 10 systems participate, and one systemdoes better than 3 others, worse then 2, and is notsignificant different from the remaining 4, its rank isin the interval 3?7.Domain BLEU Fluency Adequacyin-domain 26.63 3.17 3.58out-of-domain 20.37 2.74 3.08Figure 5: Evaluation scores for in-domain and out-of-domain test sets, averaged over all systems4.1 Close resultsAt first glance, we quickly recognize that many sys-tems are scored very similar, both in terms of man-ual judgement and BLEU.
There may be occasion-ally a system clearly at the top or at the bottom, butmost systems are so close that it is hard to distin-guish them.In Figure 4, we displayed the number of systemcomparisons, for which we concluded statistical sig-nificance.
For the automatic scoring method BLEU,we can distinguish three quarters of the systems.While the Bootstrap method is slightly more sensi-tive, it is very much in line with the sign test on textblocks.For the manual scoring, we can distinguish onlyhalf of the systems, both in terms of fluency and ad-equacy.
More judgements would have enabled usto make better distinctions, but it is not clear whatthe upper limit is.
We can check, what the conse-quences of less manual annotation of results wouldhave been: With half the number of manual judge-ments, we can distinguish about 40% of the systems,10% less.4.2 In-domain vs. out-of-domainThe test set included 2000 sentences from theEuroparl corpus, but also 1064 sentences out-of-domain test data.
Since the inclusion of out-of-domain test data was a very late decision, the par-ticipants were not informed of this.
So, this was asurprise element due to practical reasons, not mal-ice.All systems (except for Systran, which was nottuned to Europarl) did considerably worse on out-of-domain training data.
This is demonstrated byaverage scores over all systems, in terms of BLEU,fluency and adequacy, as displayed in Figure 5.The manual scores are averages over the raw un-normalized scores.108Language Pair BLEU Fluency AdequacyFrench-English 26.09 3.25 3.61Spanish-English 28.18 3.19 3.71German-English 21.17 2.87 3.10English-French 28.33 2.86 3.16English-Spanish 27.49 2.86 3.34English-German 14.01 3.15 3.65Figure 6: Average scores for different languagepairs.
Manual scoring is done by different judges,resulting in a not very meaningful comparison.4.3 Language pairsIt is well know that language pairs such as English-German pose more challenges to machine transla-tion systems than language pairs such as French-English.
Different sentence structure and rich targetlanguage morphology are two reasons for this.Again, we can compute average scores for all sys-tems for the different language pairs (Figure 6).
Thedifferences in difficulty are better reflected in theBLEU scores than in the raw un-normalized man-ual judgements.
The easiest language pair accordingto BLEU (English-French: 28.33) received worsemanual scores than the hardest (English-German:14.01).
This is because different judges focused ondifferent language pairs.
Hence, the different av-erages of manual scores for the different languagepairs reflect the behaviour of the judges, not thequality of the systems on different language pairs.4.4 Manual judgement vs. BLEUGiven the closeness of most systems and the wideover-lapping confidence intervals it is hard to makestrong statements about the correlation between hu-man judgements and automatic scoring methodssuch as BLEU.We confirm the finding by Callison-Burch et al(2006) that the rule-based system of Systran is notadequately appreciated by BLEU.
In-domain Sys-tran scores on this metric are lower than all statisticalsystems, even the ones that have much worse humanscores.
Surprisingly, this effect is much less obviousfor out-of-domain test data.
For instance, for out-of-domain English-French, Systran has the best BLEUand manual scores.Our suspicion is that BLEU is very sensitive tojargon, to selecting exactly the right words, andnot synonyms that human judges may appreciateas equally good.
This is can not be the only ex-planation, since the discrepancy still holds, for in-stance, for out-of-domain French-English, whereSystran receives among the best adequacy and flu-ency scores, but a worse BLEU score than all butone statistical system.This data set of manual judgements should pro-vide a fruitful resource for research on better auto-matic scoring methods.4.5 Best systemsSo, who won the competition?
The best answerto this is: many research labs have very competi-tive systems whose performance is hard to tell apart.This is not completely surprising, since all systemsuse very similar technology.For some language pairs (such as German-English) system performance is more divergent thanfor others (such as English-French), at least as mea-sured by BLEU.The statistical systems seem to still lag be-hind the commercial rule-based competition whentranslating into morphological rich languages, asdemonstrated by the results for English-German andEnglish-French.The predominate focus of building systems thattranslate into English has ignored so far the difficultissues of generating rich morphology which may notbe determined solely by local context.4.6 Comments on Manual EvaluationThis is the first time that we organized a large-scalemanual evaluation.
While we used the standard met-rics of the community, the we way presented trans-lations and prompted for assessment differed fromother evaluation campaigns.
For instance, in therecent IWSLT evaluation, first fluency annotationswere solicited (while withholding the source sen-tence), and then adequacy annotations.Almost all annotators reported difficulties inmaintaining a consistent standard for fluency and ad-equacy judgements, but nevertheless most did notexplicitly move towards a ranking-based evaluation.Almost all annotators expressed their preference tomove to a ranking-based evaluation in the future.
Afew pointed out that adequacy should be broken up109into two criteria: (a) are all source words covered?
(b) does the translation have the same meaning, in-cluding connotations?Annotators suggested that long sentences are al-most impossible to judge.
Since all long sen-tence translation are somewhat muddled, even a con-trastive evaluation between systems was difficult.
Afew annotators suggested to break up long sentencesinto clauses and evaluate these separately.Not every annotator was fluent in both the sourceand the target language.
While it is essential to befluent in the target language, it is not strictly nec-essary to know the source language, if a referencetranslation was given.
However, ince we extractedthe test corpus automatically from web sources, thereference translation was not always accurate ?
dueto sentence alignment errors, or because translatorsdid not adhere to a strict sentence-by-sentence trans-lation (say, using pronouns when referring to enti-ties mentioned in the previous sentence).
Lack ofcorrect reference translations was pointed out as ashort-coming of our evaluation.
One annotator sug-gested that this was the case for as much as 10% ofour test sentences.
Annotators argued for the impor-tance of having correct and even multiple references.It was also proposed to allow annotators to skipsentences that they are unable to judge.5 ConclusionsWe carried out an extensive manual and automaticevaluation of machine translation performance onEuropean language pairs.
While many systems hadsimilar performance, the results offer interesting in-sights, especially about the relative performance ofstatistical and rule-based systems.Due to many similarly performing systems, weare not able to draw strong conclusions on the ques-tion of correlation of manual and automatic evalua-tion metrics.
The bias of automatic methods in favorof statistical systems seems to be less pronounced onout-of-domain test data.The manual evaluation of scoring translation ona graded scale from 1?5 seems to be very hard toperform.
Replacing this with an ranked evalua-tion seems to be more suitable.
Human judges alsopointed out difficulties with the evaluation of longsentences.AcknowledgementsThe manual evaluation would not have been possiblewithout the contributions of the manual annotators:Jesus Andres Ferrer, Abhishek Arun, Amittai Axel-rod, Alexandra Birch, Chris Callison-Burch, JorgeCivera, Marta Ruiz Costa-jussa`, Josep Maria Crego,Elsa Cubel, Chris Irwin Davis, Loic Dugast, ChrisDyer, Andreas Eisele, Cameron Fordyce, Jesu?sGime?nez, Fabrizio Gotti, Hieu Hoang, Eric JoanisHoward Johnson, Philipp Koehn, Beata Kouchnir,Roland Kuhn, Elliott Macklovitch, Arul Menezes,Marian Olteanu, Chris Quirk, Reinhard Rapp, FatihaSadat, Joan Andreu Sa`nchez, Germa?n Sanchis,Michel Simard, Ashish Venugopal, and Taro Watan-abe.This work was supported in part under the GALEprogram of the Defense Advanced Research ProjectsAgency, Contract No.
HR0011-06-C-0022.ReferencesBirch, A., Callison-Burch, C., Osborne, M., andKoehn, P. (2006).
Constraining the phrase-based,joint probability statistical translation model.
InProceedings on the Workshop on Statistical Ma-chine Translation, pages 154?157, New YorkCity.
Association for Computational Linguistics.Callison-Burch, C., Osborne, M., and Koehn, P.(2006).
Re-evaluating the role of BLEU in ma-chine translation research.
In Proceedings ofEACL.Collins, M., Koehn, P., and Kucerova, I.
(2005).Clause restructuring for statistical machine trans-lation.
In Proceedings of ACL.Costa-jussa`, M. R., Crego, J. M., de Gispert, A.,Lambert, P., Khalilov, M., Marin?o, J.
B., Fonol-losa, J.
A. R., and Banchs, R. (2006).
Talp phrase-based statistical translation system for europeanlanguage pairs.
In Proceedings on the Workshopon Statistical Machine Translation, pages 142?145, New York City.
Association for Computa-tional Linguistics.Crego, J. M., de Gispert, A., Lambert, P., Costa-jussa`, M. R., Khalilov, M., Banchs, R., Marin?o,J.
B., and Fonollosa, J.
A. R. (2006).
N-gram-based smt system enhanced with reordering pat-terns.
In Proceedings on the Workshop on Statis-110tical Machine Translation, pages 162?165, NewYork City.
Association for Computational Lin-guistics.Doddington, G. (2002).
The NIST automated mea-sure and its relation to IBM?s BLEU.
In Proceed-ings of LREC-2002 Workshopon Machine Trans-lation Evaluation: Human Evaluators Meet Auto-mated Metrics, Gran Canaria, Spain.Eck, M. and Hori, C. (2005).
Overview of the iwslt2005 evaluation campaign.
In Proc.
of the Inter-national Workshop on Spoken Language Transla-tion.Gime?nez, J. and Ma`rquez, L. (2006).
The ldv-combo system for smt.
In Proceedings onthe Workshop on Statistical Machine Translation,pages 166?169, New York City.
Association forComputational Linguistics.Johnson, H., Sadat, F., Foster, G., Kuhn, R., Simard,M., Joanis, E., and Larkin, S. (2006).
Portage:with smoothed phrase tables and segment choicemodels.
In Proceedings on the Workshop onStatistical Machine Translation, pages 134?137,New York City.
Association for ComputationalLinguistics.Koehn, P. (2004).
Statistical significance tests formachine translation evaluation.
In Lin, D. andWu, D., editors, Proceedings of EMNLP 2004,pages 388?395, Barcelona, Spain.
Association forComputational Linguistics.Koehn, P. and Monz, C. (2005).
Shared task: Statis-tical machine translation between European lan-guages.
In Proceedings of the ACL Workshopon Building and Using Parallel Texts, pages 119?124, Ann Arbor, Michigan.
Association for Com-putational Linguistics.Li, A.
(2005).
Results of the 2005 NIST machinetranslation evaluation.
In Machine TranslationWorkshop.Menezes, A., Toutanova, K., and Quirk, C. (2006).Microsoft research treelet translation system:Naacl 2006 europarl evaluation.
In Proceedingson the Workshop on Statistical Machine Transla-tion, pages 158?161, New York City.
Associationfor Computational Linguistics.Olteanu, M., Davis, C., Volosen, I., and Moldovan,D.
(2006a).
Phramer - an open source statisti-cal phrase-based translator.
In Proceedings onthe Workshop on Statistical Machine Translation,pages 146?149, New York City.
Association forComputational Linguistics.Olteanu, M., Suriyentrakorn, P., and Moldovan, D.(2006b).
Language models and reranking for ma-chine translation.
In Proceedings on the Workshopon Statistical Machine Translation, pages 150?153, New York City.
Association for Computa-tional Linguistics.Patry, A., Gotti, F., and Langlais, P. (2006).
Mood atwork: Ramses versus pharaoh.
In Proceedings onthe Workshop on Statistical Machine Translation,pages 126?129, New York City.
Association forComputational Linguistics.Przybocki, M. (2004).
NIST machine translation2004 evaluation ?
summary of results.
In MachineTranslation Evaluation Workshop.Riezler, S. and Maxwell, J. T. (2005).
On some pit-falls in automatic evaluation and significance test-ing for MT.
In Proceedings of the ACL Workshopon Intrinsic and Extrinsic Evaluation Measuresfor Machine Translation and/or Summarization,pages 57?64, Ann Arbor, Michigan.
Associationfor Computational Linguistics.Sa?nchez, J.
A. and Bened?
?, J. M. (2006).
Stochas-tic inversion transduction grammars for obtainingword phrases for phrase-based statistical machinetranslation.
In Proceedings on the Workshop onStatistical Machine Translation, pages 130?133,New York City.
Association for ComputationalLinguistics.Watanabe, T., Tsukada, H., and Isozaki, H. (2006).Ntt system description for the wmt2006 sharedtask.
In Proceedings on the Workshop on Statis-tical Machine Translation, pages 122?125, NewYork City.
Association for Computational Lin-guistics.Zollmann, A. and Venugopal, A.
(2006).
Syntaxaugmented machine translation via chart parsing.In Proceedings on the Workshop on StatisticalMachine Translation, pages 138?141, New YorkCity.
Association for Computational Linguistics.111French-English (In Domain)Adequacy (rank) Fluency (rank) BLEU (rank)upc-jmc +0.19?0.08 (1-7) +0.09?0.08 (1-8) 30.42?0.86 (1-6)lcc +0.14?0.07 (1-6) +0.13?0.06 (1-7) 30.81?0.85 (1-4)utd +0.13?0.08 (1-7) +0.14?0.07 (1-6) 30.53?0.87 (2-7)upc-mr +0.13?0.08 (1-8) +0.13?0.07 (1-6) 30.33?0.88 (1-7)nrc +0.12?0.10 (1-7) +0.06?0.11 (2-6) 29.62?0.84 (8)ntt +0.11?0.08 (1-8) +0.14?0.08 (2-8) 30.72?0.87 (1-7)cmu +0.10?0.08 (3-7) +0.05?0.07 (4-8) 30.18?0.80 (2-7)rali -0.02?0.08 (5-8) +0.00?0.08 (3-9) 30.39?0.91 (3-7)systran -0.08?0.09 (9) -0.17?0.09 (8-9) 21.44?0.65 (10)upv -0.76?0.09 (10) -0.52?0.09 (10) 24.10?0.89 (9)Spanish-English (In Domain)Adequacy (rank) Fluency (rank) BLEU (rank)upc-jmc +0.15?0.08 (1-7) +0.18?0.08 (1-6) 31.01?0.97 (1-5)ntt +0.10?0.08 (1-7) +0.10?0.08 (1-8) 31.29?0.88 (1-5)lcc +0.08?0.07 (1-8) +0.04?0.06 (2-8) 31.46?0.87 (1-4)utd +0.08?0.06 (1-8) +0.08?0.07 (2-7) 31.10?0.89 (1-5)nrc +0.06?0.10 (2-8) +0.08?0.07 (1-9) 30.04?0.79 (6)upc-mr +0.06?0.07 (1-8) +0.08?0.07 (1-6) 29.43?0.83 (7)uedin-birch +0.03?0.11 (1-8) -0.07?0.15 (2-10) 29.01?0.81 (8)rali +0.00?0.07 (3-9) -0.02?0.07 (3-9) 30.80?0.87 (2-5)upc-jg -0.10?0.07 (7-9) -0.11?0.07 (6-9) 28.03?0.83 (9)upv -0.45?0.10 (10) -0.41?0.10 (9-10) 23.91?0.83 (10)German-English (In Domain)Adequacy (rank) Fluency (rank) BLEU (rank)uedin-phi +0.30?0.09 (1-2) +0.33?0.08 (1) 27.30?0.86 (1)lcc +0.15?0.07 (2-7) +0.12?0.07 (2-7) 25.97?0.81 (2)nrc +0.12?0.07 (2-7) +0.14?0.07 (2-6) 24.54?0.80 (5-7)utd +0.08?0.07 (3-7) +0.01?0.08 (2-8) 25.44?0.85 (3-4)ntt +0.07?0.08 (2-9) +0.06?0.09 (2-8) 25.64?0.83 (3-4)upc-mr +0.00?0.09 (3-9) -0.21?0.09 (6-9) 23.68?0.79 (8)rali -0.01?0.06 (4-9) +0.00?0.07 (3-9) 24.60?0.80 (5-7)upc-jmc -0.02?0.09 (2-9) -0.04?0.09 (3-9) 24.43?0.86 (5-7)systran -0.05?0.10 (3-9) -0.05?0.09 (3-9) 15.86?0.59 (10)upv -0.55?0.09 (10) -0.38?0.08 (10) 18.08?0.77 (9)Figure 7: Evaluation of translation to English on in-domain test data112English-French (In Domain)Adequacy (rank) Fluency (rank) BLEU (rank)nrc +0.08?0.09 (1-5) +0.09?0.09 (1-5) 31.75?0.83 (1-6)upc-mr +0.08?0.08 (1-4) +0.04?0.07 (1-5) 31.50?0.76 (1-6)upc-jmc +0.03?0.09 (1-6) +0.02?0.08 (1-6) 31.75?0.78 (1-5)systran -0.01?0.12 (2-7) +0.06?0.12 (1-6) 25.07?0.71 (7)utd -0.03?0.07 (3-7) -0.05?0.07 (3-7) 31.42?0.85 (3-6)rali -0.08?0.09 (1-7) -0.09?0.09 (2-7) 31.79?0.85 (1-6)ntt -0.09?0.09 (4-7) -0.06?0.08 (4-7) 31.92?0.84 (1-5)English-Spanish (In Domain)Adequacy (rank) Fluency (rank) BLEU (rank)ms +0.23?0.09 (1-5) +0.13?0.09 (1-7) 29.76?0.82 (7-8)upc-mr +0.20?0.09 (1-4) +0.17?0.09 (1-5) 31.06?0.86 (1-4)utd +0.18?0.08 (1-5) +0.15?0.08 (1-6) 30.73?0.90 (1-4)nrc +0.12?0.09 (2-7) +0.17?0.08 (1-6) 29.97?0.86 (5-6)ntt +0.10?0.09 (3-7) +0.14?0.08 (1-6) 30.93?0.85 (1-4)upc-jmc +0.04?0.10 (2-7) +0.01?0.08 (2-7) 30.44?0.86 (1-4)rali -0.05?0.08 (5-8) -0.03?0.08 (6-8) 29.38?0.85 (5-6)uedin-birch -0.18?0.14 (6-9) -0.17?0.13 (6-10) 28.49?0.87 (7-8)upc-jg -0.32?0.11 (9) -0.37?0.09 (8-10) 27.46?0.78 (9)upv -0.83?0.15 (9-10) -0.59?0.15 (8-10) 23.17?0.73 (10)English-German (In Domain)Adequacy (rank) Fluency (rank) BLEU (rank)upc-mr +0.28?0.08 (1-3) +0.14?0.08 (1-5) 17.24?0.81 (3-5)ntt +0.19?0.08 (1-5) +0.09?0.06 (2-6) 18.15?0.89 (1-3)upc-jmc +0.17?0.08 (1-5) +0.13?0.08 (1-4) 17.73?0.81 (1-3)nrc +0.17?0.08 (2-4) +0.11?0.08 (1-5) 17.52?0.78 (4-5)rali +0.08?0.10 (3-6) +0.03?0.09 (2-6) 17.93?0.85 (1-4)systran -0.08?0.11 (5-6) +0.00?0.10 (3-6) 9.84?0.52 (7)upv -0.84?0.12 (7) -0.51?0.10 (7) 13.37?0.78 (6)Figure 8: Evaluation of translation from English on in-domain test data113French-English (Out of Domain)Adequacy (rank) Fluency (rank) BLEU (rank)upc-jmc +0.23?0.09 (1-5) +0.13?0.11 (1-8) 21.79?0.92 (1-4)cmu +0.22?0.11 (1-8) +0.13?0.09 (1-9) 21.15?0.86 (4-7)systran +0.19?0.15 (1-8) +0.15?0.14 (1-7) 19.42?0.82 (9)lcc +0.13?0.12 (1-9) +0.11?0.11 (1-9) 21.77?0.88 (1-5)upc-mr +0.12?0.12 (2-8) +0.11?0.10 (1-7) 21.95?0.94 (1-3)utd +0.04?0.10 (1-9) +0.01?0.10 (1-8) 21.39?0.94 (3-7)ntt -0.02?0.12 (3-9) +0.08?0.11 (1-9) 21.34?0.85 (3-7)nrc -0.03?0.14 (3-8) +0.00?0.11 (3-9) 21.15?0.86 (3-7)rali -0.09?0.12 (4-9) -0.10?0.11 (5-9) 20.17?0.85 (8)upv -0.76?0.16 (10) -0.58?0.14 (10) 15.55?0.79 (10)Spanish-English (Out of Domain)Adequacy (rank) Fluency (rank) BLEU (rank)upc-jmc +0.28?0.10 (1-2) +0.17?0.10 (1-6) 27.92?0.94 (1-3)uedin-birch +0.25?0.16 (1-7) +0.18?0.19 (1-6) 25.20?0.91 (5-8)nrc +0.18?0.16 (2-8) +0.09?0.09 (1-8) 25.40?0.94 (5-7)ntt +0.11?0.10 (2-7) +0.17?0.10 (2-6) 26.85?0.89 (3-4)upc-mr +0.08?0.11 (2-8) +0.10?0.10 (1-7) 25.62?0.87 (5-8)lcc +0.04?0.10 (4-9) +0.07?0.11 (3-7) 27.18?0.92 (1-4)utd +0.03?0.11 (2-9) +0.03?0.10 (2-8) 27.41?0.96 (1-3)upc-jg -0.09?0.11 (4-9) -0.09?0.09 (7-9) 23.42?0.87 (9)rali -0.09?0.11 (4-9) -0.15?0.11 (6-9) 25.03?0.91 (6-8)upv -0.63?0.14 (10) -0.47?0.11 (10) 19.17?0.78 (10)German-English (Out of Domain)Adequacy (rank) Fluency (rank) BLEU (rank)systran +0.30?0.12 (1-4) +0.21?0.12 (1-4) 15.56?0.71 (7-9)uedin-phi +0.22?0.09 (1-6) +0.21?0.10 (1-7) 18.87?0.84 (1)lcc +0.18?0.10 (1-6) +0.20?0.10 (1-7) 17.96?0.79 (2-3)utd +0.08?0.09 (2-7) +0.07?0.08 (2-6) 16.97?0.76 (4-6)ntt +0.07?0.12 (1-9) +0.21?0.13 (1-7) 17.37?0.76 (3-5)nrc +0.04?0.10 (3-8) +0.04?0.09 (2-8) 15.93?0.76 (7-8)upc-mr +0.02?0.10 (4-8) -0.11?0.09 (6-8) 16.89?0.79 (4-6)upc-jmc -0.01?0.10 (4-8) -0.04?0.11 (3-9) 17.57?0.80 (2-5)rali -0.14?0.08 (8-9) -0.14?0.08 (8-9) 15.22?0.69 (8-9)upv -0.64?0.11 (10) -0.54?0.09 (10) 11.78?0.71 (10)Figure 9: Evaluation of translation to English on out-of-domain test data114English-French (Out of Domain)Adequacy (rank) Fluency (rank) BLEU (rank)systran +0.50?0.20 (1) +0.41?0.18 (1) 25.31?0.88 (1)upc-jmc +0.09?0.11 (2-5) +0.09?0.11 (2-4) 23.30?0.75 (2-6)upc-mr +0.09?0.11 (2-4) +0.04?0.09 (2-4) 23.21?0.75 (2-6)utd -0.02?0.11 (2-6) -0.05?0.09 (2-6) 22.79?0.86 (7)rali -0.12?0.12 (4-7) -0.17?0.12 (5-7) 23.34?0.89 (2-6)nrc -0.13?0.13 (4-7) -0.16?0.10 (4-7) 23.66?0.91 (2-5)ntt -0.23?0.12 (4-7) -0.06?0.10 (4-7) 22.99?0.96 (3-6)English-Spanish (Out of Domain)Adequacy (rank) Fluency (rank) BLEU (rank)upc-mr +0.35?0.11 (1-3) +0.19?0.10 (1-6) 26.62?0.92 (1-2)ms +0.33?0.16 (1-7) +0.15?0.13 (1-8) 26.15?0.88 (6-7)utd +0.21?0.13 (2-6) +0.13?0.11 (1-7) 25.26?0.78 (3-5)nrc +0.18?0.12 (1-6) +0.07?0.11 (2-7) 25.58?0.85 (3-5)upc-jmc +0.17?0.15 (2-7) +0.24?0.12 (1-6) 25.59?0.95 (3-5)ntt +0.12?0.13 (2-7) +0.12?0.13 (1-7) 26.52?0.90 (1-2)rali -0.17?0.16 (6-8) -0.05?0.13 (4-8) 24.03?0.83 (6-8)uedin-birch -0.36?0.24 (6-10) -0.16?0.16 (5-9) 23.18?0.88 (7-8)upc-jg -0.45?0.13 (8-9) -0.42?0.10 (9-10) 22.04?0.84 (9)upv -1.09?0.21 (9) -0.64?0.19 (8-9) 16.83?0.72 (10)English-German (Out of Domain)Adequacy (rank) Fluency (rank) BLEU (rank)systran +0.47?0.15 (1) +0.39?0.15 (1-2) 10.78?0.69 (1-6)upc-mr +0.31?0.13 (2-3) +0.21?0.11 (1-3) 10.96?0.70 (1-5)upc-jmc +0.22?0.14 (2-3) +0.01?0.10 (3-6) 10.64?0.66 (1-6)rali +0.13?0.12 (4-6) -0.06?0.10 (4-6) 10.57?0.65 (1-6)nrc +0.00?0.11 (4-6) +0.05?0.09 (2-6) 10.64?0.65 (2-6)ntt -0.03?0.12 (4-6) +0.08?0.11 (3-5) 10.51?0.64 (1-6)upv -0.94?0.13 (7) -0.57?0.10 (7) 6.55?0.53 (7)Figure 10: Evaluation of translation from English on out-of-domain test data115French-EnglishIn domain Out of Domain21 22 23 24 25 26 27 28 29 30 31BLEU-0.8-0.7-0.6-0.5-0.4-0.3-0.2-0.1-0.00.10.20.3Adequacy?rali?utd??systran?lccnrc??upv??ntt?cmu?upc-jmc?upc-mr?
?15 16 17 18 19 20 21 22BLEU-0.8-0.7-0.6-0.5-0.4-0.3-0.2-0.1-0.00.10.20.3Adequacyrali?utd?systran?lcc??nrc??upv?nttcmu?
?upc-jmc?upc-mr21 22 23 24 25 26 27 28 29 30 31BLEU-0.5-0.4-0.3-0.2-0.1-0.00.10.2Fluency?rali?utd??systran??lcc?nrc??upvntt?cmuupc-jmc?upc-mr?
?15 16 17 18 19 20 21 22BLEU-0.6-0.5-0.4-0.3-0.2-0.1-0.00.10.2Fluencyrali??utdsystran???lccnrc??upvntt?cmu?
?upc-jmc?
?upc-mrFigure 11: Correlation between manual and automatic scores for French-English116Spanish-EnglishIn Domain Out of Domain23 24 25 26 27 28 29 30 31 32BLEU-0.5-0.4-0.3-0.2-0.1-0.00.10.2Adequacy?rali?upc-jg?
?utd?lcc?upv?nrcntt?uedin-birch?upc-jmc?upc-mr?19 20 21 22 23 24 25 26 27 28BLEU-0.7-0.6-0.5-0.4-0.3-0.2-0.1-0.00.10.20.3Adequacy?raliupc-jg??utdlcc??upv?nrc?nttuedin-birch?
?upc-jmcupc-mr?23 24 25 26 27 28 29 30 31 32BLEU-0.5-0.4-0.3-0.2-0.1-0.00.10.2Fluency?raliupc-jg??utd?
?lcc?upvnrc?
?ntt?uedin-birch?upc-jmcupc-mr?19 20 21 22 23 24 25 26 27 28BLEU-0.5-0.4-0.3-0.2-0.1-0.00.10.2Fluency?rali?upc-jg?utd?lcc?upvnrc?ntt?uedin-birch?
?upc-jmc?upc-mrFigure 12: Correlation between manual and automatic scores for Spanish-English117German-EnglishIn Domain Out of Domain15 16 17 18 19 20 21 22 23 24 25 26 27BLEU-0.6-0.5-0.4-0.3-0.2-0.1-0.00.10.20.30.4Adequacy?raliutd??systranlcc?nrc??upv?nttupc-mr?
?upc-jmcuedin-phi?12 13 14 15 16 17 18 19 20BLEU-0.6-0.5-0.4-0.3-0.2-0.1-0.00.10.20.30.4Adequacy?raliutd???systranlcc?nrc??upv?ntt?upc-mr?
?upc-jmc?uedin-phi15 16 17 18 19 20 21 22 23 24 25 26 27BLEU-0.4-0.3-0.2-0.1-0.00.10.20.30.4Fluencyrali??
?utd?systran?lccnrc?
?upv?ntt?upc-mrupc-jmc?uedin-phi?12 13 14 15 16 17 18 19 20BLEU-0.5-0.4-0.3-0.2-0.1-0.00.10.20.30.4Fluency?rali?utdsystran?
??lccnrc??upvntt??upc-mr?upc-jmcuedin-phi?
?Figure 13: Correlation between manual and automatic scores for German-English118English-FrenchIn Domain Out of Domain25 26 27 28 29 30 31 32BLEU-0.3-0.2-0.10.00.10.2Adequacy?nrc?nttrali??upc-jmcupc-mr?utd?
?systran20 21 22 23 24 25 26BLEU-0.3-0.2-0.10.00.10.20.30.40.5Adequacy?nrc?nttrali?
?upc-jmcupc-mrutd?systran?25 26 27 28 29 30 31 32BLEU-0.3-0.2-0.10.00.10.2Fluency?nrc?ntt?rali?upc-jmcupc-mr?utd?
?systran20 21 22 23 24 25 26BLEU-0.3-0.2-0.10.00.10.20.30.40.5Fluency?nrc?nttrali?
?upc-jmcupc-mr?utd?systran?Figure 14: Correlation between manual and automatic scores for English-French119English-SpanishIn Domain Out of Domain23 24 25 26 27 28 29 30 31 32BLEU-0.9-0.8-0.7-0.6-0.5-0.4-0.3-0.2-0.1-0.00.10.20.3Adequacyrali?upc-jg??utdnrc??upv?nttms?
?uedin-birch?upc-jmc?upc-mr16 17 18 19 20 21 22 23 24 25 26 27BLEU-1.1-1.0-0.9-0.8-0.7-0.6-0.5-0.4-0.3-0.2-0.1-0.00.10.20.30.4Adequacy?raliupc-jg?utd???nrc??upv?nttms?
?uedin-birchupc-jmc?upc-mr23 24 25 26 27 28 29 30 31 32BLEU-0.6-0.5-0.4-0.3-0.2-0.1-0.00.10.2Fluency?raliupc-jg???utdnrc??upvntt??ms?uedin-birch?
?upc-jmc?upc-mr16 17 18 19 20 21 22 23 24 25 26 27BLEU-0.7-0.6-0.5-0.4-0.3-0.2-0.1-0.00.10.20.3Fluency?raliupc-jg?utd??nrc?upv?nttms??uedin-birchupc-jmc?
?upc-mrFigure 15: Correlation between manual and automatic scores for English-Spanish120English-GermanIn Domain Out of Domain9 10 11 12 13 14 15 16 17 18 19BLEU-0.9-0.8-0.7-0.6-0.5-0.4-0.3-0.2-0.1-0.00.10.20.3Adequacynrc??upv?rali?ntt?upc-mrupc-jmc??
?systran6 7 8 9 10 11BLEU-1.0-0.9-0.8-0.7-0.6-0.5-0.4-0.3-0.2-0.1-0.00.10.20.30.40.5Adequacy?nrc?upvrali?ntt??upc-mrupc-jmc?
?systran9 10 11 12 13 14 15 16 17 18 19BLEU-0.5-0.4-0.3-0.2-0.1-0.00.10.2Fluencynrc??upvrali??nttupc-mr?
?upc-jmc?systran6 7 8 9 10 11BLEU-0.6-0.5-0.4-0.3-0.2-0.1-0.00.10.20.30.4Fluency?nrc?upvrali?ntt?upc-mr?
?upc-jmc?systranFigure 16: Correlation between manual and automatic scores for English-German121
