PLASER: Pronunciation Learning via Automatic Speech RecognitionBrian Mak, Manhung Siu, Mimi Ng, Yik-Cheung Tam?, Yu-Chung Chan?Kin-Wah Chan, Ka-Yee Leung, Simon Ho, Fong-Ho Chong, Jimmy Wong, and Jacqueline LoHong Kong University of Science and TechnologyDepartment of Computer Science, andDepartment of Electronic and Electrical Engineering{bmak, eemsiu, ssmimi}@ust.hkAbstractPLASER is a multimedia tool with instantfeedback designed to teach English pronunci-ation for high-school students of Hong Kongwhose mother tongue is Cantonese Chinese.The objective is to teach correct pronunciationand not to assess a student?s overall pronuncia-tion quality.
Major challenges related to speechrecognition technology include: allowance fornon-native accent, reliable and corrective feed-backs, and visualization of errors.PLASER employs hidden Markov modelsto represent position-dependent Englishphonemes.
They are discriminatively trainedusing the standard American English TIMITcorpus together with a set of TIMIT utterancescollected from ?good?
local English speakers.There are two kinds of speaking exercises:minimal-pair exercises and word exercises.In the word exercises, PLASER computesa confidence-based score for each phonemeof the given word, and paints each vowel orconsonant segment in the word using a novel3-color scheme to indicate their pronunciationaccuracy.
PLASER was used by 900 studentsof grade 7 and 8 over a period of 2?3 months.About 80% of the students said that they pre-ferred using PLASER over traditional Englishclasses to learn pronunciation.
A pronunciationtest was also conducted before and after theyused PLASER.
The result from 210 studentsshows that the students?
pronunciation skillwas improved.
(The statistics is significant atthe 99% confidence level.)?Mr.
Tam is now a graduate student at the Department ofComputer Science at Carnegie Mellon University.?Mr.
Chan is now working at SpeechWorks Inc.1 IntroductionThe phenomenal advances in automatic speech recogni-tion (ASR) technologies in the last decade led to the re-cent employment of the technologies in computer-aidedlanguage learning (CALL) 1.
One example is the LIS-TEN project (Mostow et al, 1994).
However, one hasto bear in mind that the goal of ASR in most other com-mon classification applications (such as automated callcenters, dictation, etc.)
is orthogonal to that in CALL:while the former requires ASR in general to be forgiv-ing to allophonic variations due to speaker idiosyncrasiesor accent, pronunciation learning demands strict distinc-tion among different sounds though the extent of strict-ness could be very subjective with a human teacher.
Asa result, technologies developed for mainstream ASR ap-plications may not work satisfactorily for pronunciationlearning.In the area of pronunciation learning, ASR has beenused in CALL for two different purposes: teachingcorrect pronunciation of a foreign language to stu-dents (Kawai and Hirose, 2000), and assessing the pro-nunciation quality of a speaker speaking a foreign lan-guage (Witt and Young, 2000; Neumeyer et al, 2000;Franco et al, 2000).
The former asks for accurate andprecise phoneme recognition while the latter may toler-ate more recognition noises.
The judgment for the for-mer task is comparatively more objective than that forthe latter which, on the other hand, is usually required tocorrelate well with human judges.
In this paper, we de-scribe a multimedia tool we built for high-school studentsin Hong Kong to self-learn American English pronuncia-tion.
Their mother tongue is Cantonese Chinese.
The ob-jective is to teach correct pronunciation of basic Englishphonemes (possibly with local accent), and not to assessa student?s overall pronunciation quality.
Although there1CALL applies many different technologies to help lan-guage learning, but this paper concerns only the one area ofpronunciation learning in CALL.exist commercial products for the purpose, they have twomajor problems: First, they are not built for Cantonese-speaking Chinese; and, second, the feedback from theseproducts does not pinpoint precisely which phonemes arepoorly pronounced and which phonemes are well pro-nounced.
As a matter of fact, most of these systems onlyprovide an overall score for a word or utterance.
As thefeedback is not indicative, students would not know howto improve or correct their mistakes.
One reason is therelatively poor performance of phoneme recognition ?the best phoneme recognition accuracy is about 75% forthe TIMIT corpus.We took a pragmatic view and designed a multimedialearning tool called PLASER ?
Pronunciation Learningvia Automatic SpEech Recognition ?
according to ourfollowing beliefs and guidelines:1.
It is an illusive goal for average students to learnto speak a second language without local accent.Therefore, PLASER should be tolerant to mi-nor Cantonese accents, lest the students becometoo frustrated from continually getting low scores.For example, there is no ?r?
sound in Cantoneseand consequently Cantonese usually speaks the ?r?phoneme with weak retroflexion.2.
Performance of phoneme recognition over a longcontinuous utterance is still far from being satisfac-tory for pedagogical purpose.3.
PLASER?s performance must be reliable even at theexpense of lower accuracy.4.
To be useful for correcting mistakes, PLASER mustprovide meaningful and indicative feedbacks to pin-point which parts of an utterance are wrongly pro-nounced and to what extent.5.
The knowledge of IPA symbols is not a pre-requisiteto learning pronunciation.This paper is organized as follows: in the next Section,we first present the overall system design of PLASER.This is followed by a discussion of our acoustic modelsin Section 3.
Section 4 gives a detailed description ofour confidence-based approach in pronunciation scoring,and the related feedback visualization is given in Section5.
Both quantitative and qualitative evaluation results aregiven in Section 6.
Finally, we summarize the lessons welearned in building PLASER and point out some futureworks in Section 7.Table 1: Phonemes that are taught in PLASER (writtenin TIMIT-bet)Lesson# Phoneme Pair Lesson# Phoneme Pair1 iy ih 11 k g2 eh ey 12 s z3 ae ah 13 sh zh4 aa ao 14 ch jh5 ax er 15 f v6 ow uh 16 th dh7 uw ay 17 m n8 oy aw 18 ng h9 p b 19 l r10 t d 20 w y2 PLASER: System DesignPLASER runs under Microsoft Windows (98, NT, 2000)with an easy-to-use web-like interface requiring onlystandard utilities such as the Internet Explorer and MediaPlayer.
PLASER consists of 20 lessons, and each lessonteaches two American English phonemes as shown in Ta-ble 1.
The two phonemes in a lesson are usually the mostconfusable pair among the 40 phonemes.
PLASER con-tains a lot of word examples and for each word there areits English spelling, its Chinese translation, a picture, anda pronunciation video-clip (PVC) which a native Amer-ican English speaker helped record.
A user may readand listen to the materials of each word as many timesas he likes at his own pace.
Besides descriptive materials,PLASER uses four types of exercises to teach pronunci-ation:Read-Along Exercise: Basic pronunciation drills withno assessment.Minimal-Pair Listening Exercise: This is used to trainusers?
ear.
Words from one minimal pairs are ran-domly embedded in a sentence that makes perfectsense with either word in the pair.
A user listens torecordings of such sentences and chooses betweenthe two words.Minimal-Pair Speaking Exercise: Similar to theMinimal-Pair Listening Exercise except that nowonly minimal pairs are given and a user is asked tosay them.
A student may pick any one of the twowords to say but not to mix up with its counterpartin the pair.
It is a two-class classification problem.Word-List Speaking Exercise: A student may pick anyword from a list to say, and PLASER has to decidehow well each phoneme in the word is pronounced.Figure 1: A snapshot of PLASER running its word exer-ciseFig.
1 shows a snapshot of PLASER running the Word-List Speaking Exercise in the lesson teaching the twophonemes: ?ih?
and ?iy?.
The user has selected the word?cheese?
to practise.
The top left panel tells how to pro-duce the phoneme ?iy?
with the help of an animated GIFthat shows a cross-sectional view of the vocal tract dur-ing the phoneme?s production.
At the bottom right panelare the word?s spelling, its Chinese translation, its pic-ture, plus a recording button and a playback button.
Theword?s PVC is shown at the top right panel.
The mid-dle panel in the screen is reserved for feedbacks.
Thefeedback for Word-List Speaking Exercise consists of anoverall score for the practising word (?cheese?
here) aswell as a confidence score for each individual phonemein the word using a novel 3-color scheme.
Confidencescores are derived from a log-likelihood ratio between thedesired target and some reference.
Garbage rejection isalso implemented in a similar manner.
Refer Section 4and 5 for more details.As a self-learning as well as a teaching aid, the lengthof each lesson is designed to take about 25?30 minutesto complete.
Students?
performance is recorded for laterreviews by students themselves if PLASER is used asa learning tool, or by teachers if PLASER is used as ateaching aid.3 Acoustic ModellingFor the development of PLASER?s acoustic models, addi-tional speech data were collected from local high-schoolstudents:HKTIMIT: A set of TIMIT utterances collected froma group of 61 local (Cantonese) high-school stu-dents who spoke ?good?
English to the local stan-dard.
There are 29 females and 32 males, and eachrecorded 250 TIMIT sentences.
The data were di-vided into a training set of 9,163 utterances from 17females and 20 males, and a test set of 6,015 utter-ances from 12 females and 13 males.MP-DATA: A superset of words used in PLASER?sminimal-pair exercises recorded by eight high-school students, 4 males and 4 females, each speak-ing ?300 words for a total of 2,431 words.WL-DATA: A superset of words used in PLASER?sword exercises by the same eight students whorecorded the MP-DATA for a total of 2,265 words.All data were recorded with the same conditions as thoseof TIMIT.
In addition, all utterances of MP-DATA andWL-DATA were phonetically transcribed.The standard American English TIMIT corpus to-gether with the HKTIMIT corpus were used to developCantonese-accented English phoneme HMMs.
The com-mon 13 mel-frequency cepstral coefficients and their firstand second order derivatives were used for acoustic repre-sentation.
All phoneme HMMs have three real states, andthere are an additional 3-state silence model and a 1-stateshort-pause HMM.
Three kinds of modelling techniqueswere investigated:Context-Independent Modelling: Context-independent HMMs (CIHMM) were trainedfor the 40 phonemes taught in PLASER.
Includingthe silence and short-pause models, there are totally42 HMMs.Position-Dependent HMM (PDHMM): Due to con-cerns of limited computing resources in local pub-lic schools, a restricted form of context-dependentmodelling was chosen.
Since PLASER will onlyperform phoneme recognition on isolated words, wepostulate that it may be important to capture theword-boundary effect of a phoneme.
Thus, threevariants of each phoneme are modelled dependingon whether it appears at the beginning, in the mid-dle, or at the end of a word.
(MCE) Discriminative Training: With the goal of min-imizing classification errors in a development data-set which is WL-DATA in our case, word-basedMCE/GPD algorithm (Juang and Katagiri, 1992;Chou, 2000) was applied to improve the EM-trainedacoustic models.We started with a baseline system using 40 mono-phones with 24 mixtures per state.
It gives a phonemerecognition accuracy of 39.9% on the HKTIMIT test set.The low accuracy perhaps indicates an unexpected lowerModelling Technique ClassificationAcc.
of MP-DATACIHMM, 24 mixtures 81.50CIHMM, 24 mixtures + MCE 84.48PDHMM, 20 mixtures 82.83PDHMM, 20 mixtures + MCE 85.29Table 2: Investigation of various modelling techniques onminimal-pair classificationEnglish proficiency of local students as well as a largedeviation of local English from native American English.We then investigated PDHMM and MCE training, andgauged our progress by the classification accuracy ofminimal pairs in the MP-DATA set.
The results are tabu-lated in Table 2.By using PDHMMs, the inventory of models is onlyincreased by three times, requiring little additional com-putational resources.
Yet they result in a relative errorreduction of 7.2%.
MCE discriminative training gives anadditional relative improvement of about 14?16%.4 Confidence-based Phoneme AssessmentThe assessment of pronunciation accuracy is cast as aphoneme verification problem.
The posterior probabil-ity of a phoneme is used as the Goodness of Pronunci-ation measure (GOP), which has been shown in manyworks (Witt and Young, 2000; Franco et al, 2000) that itis a good measure.
PLASER computes both a GOP scoreand a normalized GOP score for two types of feedback aswill be discussed in Section 5.When a student runs a PLASER word exercise, s/hewill randomly pick a word from a list and watches itspronunciation video-clip (PVC).
When s/he feels com-fortable to try s/he records her/his voice speaking theword.
PLASER then computes a confidence-based GOPfor each phoneme in the word as follows.STEP 1: PLASER consults its dictionary for the stan-dard phonemic transcription of the word whichshould be the same as that of its PVC.STEP 2: Based on the transcription, forced alignment isperformed on the student?s speech.STEP 3: For each acoustic segment Xu of phoneme yu(where u denotes the phoneme index), PLASERcomputes its GOP(yu), su, as its posterior probabil-ity by the following log-likelihood ratio normalizedby its duration Tu:su = logProb(yu|Xu)?
1Tu ?
log[p(Xu|yu)p(yu)?Nk=1 p(Xu|yk)p(yk)](1)?
1Tu ?
log[ p(Xu|yu)p(Xu|yjmax)](2)where N is the number of phonemes, and jmax isthe phoneme model that gives the highest likelihoodof the given segment.
This GOP is used with somethresholds to decide if the phoneme is pronouncedcorrectly.In practice, the denominator in Equation 2 is re-placed by the Viterbi likelihood of the segment givenby a phone loop.
Notice that the Viterbi path ofa segment may contain more than one phonememodel.STEP 4: Besides the raw GOP score, GOP(yu) = sucomputed in STEP 3, a normalized GOP score isalso computed by normalizing the GOP score to therange [0.0 .. 1.0] using a sigmoid function.
That is,the normalized GOP for the phoneme yu is given bysigmoid(su) = 11 + exp (?
?su + ?)
(3)where the parameters ?
and ?
are empirically found.The current PLASER implementation has some mod-ifications due to practical reasons: The phone loop forcomputing the denominator of Equation 2 uses only themiddle-position PDHMM of each phoneme plus the si-lence and short pause models for faster computation.
Forgreater computation savings, the phone loop may also bereplaced by a single Gaussian Mixture Model (GMM)trained by all phoneme segments in the training data.
Inour experience, a GMM with 32 mixtures suffices with aslight degradation in performance.5 Visualization of Recognition ResultsTwo kinds of feedback of different resolutions are givenfor the word exercise:?
an overall phoneme score of the whole word; and,?
a phoneme-by-phoneme assessment by a 3-colorscheme.5.1 Overall Phoneme Score of a WordThe use of posterior probability as the GOP score for as-sessing the accuracy of a phoneme segment allows us toreadily define an overall phoneme score (PS) for a wordas a weighted sum of the normalized GOPs of its com-posing phonemes:PS(word) =N?k=1wk ?
normalized-GOP(phonemek) (4)where wk is the weighting of the k-th phoneme amongthe N phonemes composing the word.
In the currentPLASER, all phonemes in a word are equally weighted.5.2 A 3-Color Feedback Scheme for PhonemeConfidenceThe usefulness of an overall confidence for a word maybe limited as it does not pinpoint the pronunciation accu-racy of each phoneme in the word, and thus, the user stilldoes not know how to correct his mistakes when the scoreis not good.
Any attempt to report phoneme confidencescore has to face the following two problems:?
unless users can read phonemic transcriptions, itis not clear how to report the confidence scores atphoneme level; and,?
unless the phoneme confidence scores are highly re-liable, reporting its precise value may be too risky.Our solution is a visual feedback that gives a color tothe letters in the word spelling to indicate the pronuncia-tion accuracy of their associated phonemes.
To do that,STEP 1: We first designed a rule-based algorithm tomap each phoneme in the transcription of aword to its spelling letters.
For example, forthe word ?beat?
with the phonemic transcrip-tion ?/b/ /iy/ /t/?, the three phonemes aremapped to the letters ?b?, ?ea?
and ?t?
respec-tively.
On the other hand, for the word ?eve?
withthe phonemic transcription ?/iy/ /v/?, the twophonemes are mapped to the letters ?e?
and ?v?
re-spectively while the last letter ?e?
is not mapped toany phoneme.STEP 2: A novel 3-color scheme was devised to reducethe preciseness of phoneme confidence scores.
Twothresholds were found for each phoneme to label itsconfidence as good, fair, or bad.
If the confidencescore of a phoneme is good/fair/bad, its correspond-ing spelling letter(s) is/are painted in blue/green/redrespectively.
Two examples are shown in Fig.
2.
Theuse of colors is also more appealing to users.To find the two thresholds in the 3-color scheme, wetreated the problem as a bi-threshold verification prob-lem.
The detailed algorithm is beyond the scope of thispaper and will only be briefly described here.
For details,please refer to (Ho and Mak, 2003).Firstly, one has to decide how forgiving one wants tobe and specifies the following two figures:?
the false acceptance rate (FA) for an incorrectly pro-nounced phoneme; and,?
the false rejection rate (FR) for a correctly pro-nounced phoneme.If one sets FA very low, it will be hard to get ?blue?scores; on the other hand, if one sets FR very low, it maybe too forgiving and ?red?
scores will rarely show up.Due to its bi-threshold nature, it turns out that in such cir-cumstances, simple method to determine the two thresh-olds will results in dominating ?green?
scores with little?blue?
or ?red?
scores.
The more complicated algorithmin (Ho and Mak, 2003) tries to avoid that.Furthermore, due to scarcity of training data in thedevelopment data set, the phonemes were grouped into9 phoneme classes in PLASER, and class-dependentthresholds were determined from the development dataset.
The 9 phoneme classes are: affricates, diphthongs,fricatives, nasals, semi-vowels, stops, back vowels, midvowels, and front vowels.b e a tbadgoodfair(a) beateunusedgoodfairv e(b) eveFigure 2: A three-color scheme for showing phonemeconfidence (The figure has to be read with color print-outs, or electronically on a color display.
The lettersmarked with ?bad?, ?fair?, ?good?, and ?unused?
arepainted in red, green, blue, and gray respectively.
)6 EvaluationA beta version of PLASER was tested by 900 students ofGrade Seven and Eight over a period of about 3 monthsin twelve high schools.
Both quantitative and qualitativeevaluations were conducted to gauge the effectiveness ofusing PLASER to learn English pronunciation.6.1 Quantitative EvaluationA pronunciation test consisting of speaking 60 wordswas conducted once before a student even started touse PLASER and once after they finished the 3-monthPLASER trial.
The recordings from 210 students weresuccessfully collected.
Recordings were not obtainedfrom the rest of students for various reasons:?
Some schools did not have time to do the evaluationrecordings due to schedule problems.?
Some recordings were poorly administered; e.g.parts of utterances were missing in the files.?
Some schools accidently erased or lost the recordedspeech files in their computers.At the end, recordings from 210 students were found tobe good enough for evaluation.
Their recordings weretranscribed and compared with the correct transcriptionsto find their pronunciation accuracies.
The two his-tograms in Fig.
3 summarize their pronunciation accura-cies of the 60 words before and after they practiced withPLASER.
Here are some detailed statistics:?
73% of the students had their pronunciation accu-racy improved by an average of (absolute) 4.53%.?
The remaining 27% of the students got worse forunknown reasons by an average of 2.68%.?
Collectively we observe an obvious improvement:the mean accuracy after the use of PLASER isgreater than that before using PLASER, and themean difference is statistically significant at the 99%confidence level.6.2 Qualitative EvaluationIn addition, a questionnaire survey was conducted to getcomments and suggestions from teachers and students af-ter they finished the study.
Some figures are worth men-tioning:?
77% of the students believed that their pronuncia-tion skill was improved after using PLASER while91% of school teachers believed their students?
pro-nunciation had improved.?
77% of the students like to use PLASER to learnEnglish pronunciation.?
53% of the students preferred using PLASER to thetraditional classroom teaching method to learn En-glish pronunciation while 73% of the teachers wouldprefer their students using PLASER to self-learnpronunciation.01020304050607080900-0.499 0.5-0.549 0.55-0.599 0.6-0.649 0.65-0.699 0.7-0.749 0.75-0.799 0.8-0.849 0.85-0.99Pronunciation Correctness (min.
= 0.0; max = 1.0)NumberofStudentsbefore using PLASERafter using PLASERFigure 3: Results of the pronunciation evaluation test?
All teachers would recommend their students to usePLASER to learn English pronunciation.7 Discussion & Future WorksMore work is being planned to further improvePLASER?s performance.
Robustness is the key problem.In the school environment, one simply cannot expect thestudents to use learning tools quietly.
In addition, aboutforty students use a language laboratory at the same time.Since the headset microphones available in all schools arenot uni-directional, recordings from neighboring studentsare picked up on top of the user?s.
This kind of ?bab-ble noise?
hurts PLASER?s performance to a great ex-tent: not only does it affect the accuracy of our phonemerecognizer, various thresholds used in our confidence-based scoring and noise rejection are affected too.
Var-ious well-known robust techniques such as spectral sub-traction (Boll, 1979), MLLR adaptation (Leggetter andWoodland, 1995), parallel model combination (Gales andYoung, 1996), and stochastic matching (Sankar and Lee,1996), etc.
are being investigated.To further improve phoneme discrimination, we aretrying to build statistical models to test the presence ofarticulatory features in each phoneme (Leung and Siu,2003).
The outcome of the test will be a posterior prob-ability of an articulatory feature which will then be com-bined with the score from the acoustic models with thehope to give even better accuracy.Finally, the recognizer has to be optimized for theslower machines used in many local schools.8 AcknowledgementsThis work is supported by the Hong Kong Quality Edu-cation Fund under the grant number QEF99/00.EG01.ReferencesS.F.
Boll.
1979.
Suppression of Acoustic Noise inSpeech Using Spectral Subtraction.
IEEE Transac-tions on Acoustics, Speech and Signal Processing,24:113?120.W.
Chou.
2000.
Discriminant-Function-Based Mini-mum Recognition Error Rate Pattern-Recognition Ap-proach to Speech Recognition.
Proceedings of theIEEE, 88(8):1201?1223, August.H.
Franco, L. Neumeyer, V. Digalakis, and O. Ronen.2000.
Combination of Machine Scores for AutomaticGrading of Pronunciation Quality.
Speech Communi-cations, 30(2?3):121?130, Feb.M.J.F.
Gales and S.J.
Young.
1996.
Robust ContinuousSpeech Recognition Using Parallel Model Combina-tion.
IEEE Transactions on Speech and Audio Pro-cessing, 4(5):352?359, September.Simon Ho and Brian Mak.
2003.
English Pronuncia-tion Evaluation as a Bi-threshold Phoneme VerificationProblem.
In Proceedings of the European Conferenceon Speech Communication and Technology, (submit-ted).B.H.
Juang and S. Katagiri.
1992.
Discriminative Train-ing for Minimum Error Classification.
IEEE Transac-tion on Signal Processing, 40(12):3043?3054, Dec.G.
Kawai and K. Hirose.
2000.
Teaching the Pro-nunciation of Japanese Double-mora Phonemes usingSpeech Recognition Technology.
Speech Communica-tions, 30(2?3):83?93, Feb.C.J.
Leggetter and P.C.
Woodland.
1995.
MaximumLikelihood Linear Regression for Speaker Adaptationof Continuous Density Hidden Markov Models.
Jour-nal of Computer Speech and Language, 9(2):171?185,April.K.
Y. Leung and M. H. Siu.
2003.
Phone LevelConfidence Measure Using Articulatory Features.
InProceedings of the IEEE International Conference onAcoustics, Speech, and Signal Processing.J.
Mostow, S. Roth, A. G. Hauptmann, and M. Kane.1994.
A Prototype Reading Coach that Listens.
InProceedings of the Twelfth National Conference on Ar-tificial Intelligence (AAAI-94), American Associationfor Artificial Intelligence, pages 785?792.L.
Neumeyer, H. Franco, V. Digalakis, and M. Weintraub.2000.
Automatic Scoring of Pronunciation Quality.Speech Communications, 30(2?3):83?93, Feb.A.
Sankar and C.H.
Lee.
1996.
A Maximum-LikelihoodApproach to Stochastic Matching for Robust SpeechRecognition.
IEEE Transactions on Speech and AudioProcessing, 4(3):190?202.S.M.
Witt and S.J.
Young.
2000.
Phone-level Pronuncia-tion Scoring and Assessment for Interactive LanguageLearning.
Speech Communications, 30(2?3):95?108,Feb.
