Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 207?214,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsEdinburgh?s Syntax-Based Systems at WMT 2014Philip Williams1, Rico Sennrich1, Maria Nadejde1,Matthias Huck1, Eva Hasler1, Philipp Koehn1,21School of Informatics, University of Edinburgh2Center for Speech and Language Processing, The Johns Hopkins UniversityAbstractThis paper describes the string-to-tree sys-tems built at the University of Edin-burgh for the WMT 2014 shared trans-lation task.
We developed systems forEnglish-German, Czech-English, French-English, German-English, Hindi-English,and Russian-English.
This year weimproved our English-German systemthrough target-side compound splitting,morphosyntactic constraints, and refine-ments to parse tree annotation; we ad-dressed the out-of-vocabulary problem us-ing transliteration for Hindi and Rus-sian and using morphological reductionfor Russian; we improved our German-English system through tree binarization;and we reduced system development timeby filtering the tuning sets.1 IntroductionFor this year?s WMT shared translation task webuilt syntax-based systems for six language pairs:?
English-German ?
German-English?
Czech-English ?
Hindi-English?
French-English ?
Russian-EnglishAs last year (Nadejde et al., 2013), our systems arebased on the string-to-tree pipeline implementedin the Moses toolkit (Koehn et al., 2007).We paid particular attention to the production ofgrammatical German, trying various parsers andincorporating target-side compound splitting andmorphosyntactic constraints; for Hindi and Rus-sian, we employed the new Moses transliterationmodel to handle out-of-vocabulary words; and forGerman to English, we experimented with tree bi-narization, obtaining good results from right bina-rization.We also present our first syntax-based resultsfor French-English, the scale of which defeated uslast year.
This year we were able to train a sys-tem using all available training data, a task thatwas made considerably easier through principledfiltering of the tuning set.
Although our systemwas not ready in time for human evaluation, wepresent BLEU scores in this paper.In addition to the five single-system submis-sions described here, we also contributed ourEnglish-German and German-English systems foruse in the collaborative EU-BRIDGE system com-bination effort (Freitag et al., 2014).This paper is organised as follows.
In Sec-tion 2 we describe the core setup that is com-mon to all systems.
In subsequent sections we de-scribe language-pair specific variations and exten-sions.
For each language pair, we present resultsfor both the development test set (newstest2013in most cases) and for the filtered test set (new-stest2014) that was provided after the system sub-mission deadline.
We refer to these as ?devtest?and ?test?, respectively.2 System Overview2.1 Pre-processingThe training data was normalized using the WMTnormalize-punctuation.perl script thentokenized and truecased.
Where the target lan-guage was English, we used the Moses tokenizer?s-penn option, which uses a tokenization schemethat more closely matches that of the parser.
Forthe English-German system we used the defaultMoses tokenization scheme, which is similar tothat of the German parsers.For the systems that translate into English, weused the Berkeley parser (Petrov et al., 2006;Petrov and Klein, 2007) to parse the target-side ofthe training corpus.
As we will describe in Sec-tion 3, we tried a variety of parsers for German.We did not perform any corpus filtering otherthan the standard Moses method, which removes207sentence pairs with dubious length ratios and sen-tence pairs where parsing fails for the target-sidesentence.2.2 Translation ModelOur translation grammar is a synchronous context-free grammar (SCFG) with phrase-structure labelson the target side and the generic non-terminal la-bel X on the source side.The grammar was extracted from the word-aligned parallel data using the Moses implemen-tation (Williams and Koehn, 2012) of the GHKMalgorithm (Galley et al., 2004; Galley et al., 2006).For word alignment we used MGIZA++ (Gao andVogel, 2008), a multi-threaded implementation ofGIZA++ (Och and Ney, 2003).Minimal GHKM rules were composed intolarger rules subject to parameterized restrictionson size defined in terms of the resulting target treefragment.
A good choice of parameter settingsdepends on the annotation style of the target-sideparse trees.
We used the settings shown in Table 1,which were chosen empirically during the devel-opment of last years?
systems:Parameter ValueRule depth 5Node count 20Rule size 5Table 1: Parameter settings for rule composition.Further to the restrictions on rule composition,fully non-lexical unary rules were eliminated us-ing the method described in Chung et al.
(2011)and rules with scope greater than 3 (Hopkins andLangmead, 2010) were pruned from the trans-lation grammar.
Scope pruning makes parsingtractable without the need for grammar binariza-tion.2.3 Language ModelWe used all available monolingual data to train5-gram language models.
Language modelsfor each monolingual corpus were trained usingthe SRILM toolkit (Stolcke, 2002) with modi-fied Kneser-Ney smoothing (Chen and Goodman,1998) and then interpolated using weights tuned tominimize perplexity on the development set.2.4 Feature FunctionsOur feature functions are unchanged from the pre-vious two years.
They include the n-gram lan-guage model probability of the derivation?s targetyield, its word count, and various scores for thesynchronous derivation.Each grammar rule has a number of pre-computed scores.
For a grammar rule r of the formC ?
?
?, ?,?
?where C is a target-side non-terminal label, ?
is astring of source terminals and non-terminals, ?
isa string of target terminals and non-terminals, and?
is a one-to-one correspondence between sourceand target non-terminals, we score the rule accord-ing to the following functions:?
p (C, ?
| ?,?)
and p (?
| C, ?,?
), the directand indirect translation probabilities.?
plex(?
| ?)
and plex(?
| ?
), the direct andindirect lexical weights (Koehn et al., 2003).?
ppcfg(pi), the monolingual PCFG probabilityof the tree fragment pi from which the rulewas extracted.?
exp(?1/count(r)), a rule rareness penalty.?
exp(1), a rule penalty.
The main grammarand glue grammars have distinct penalty fea-tures.2.5 TuningThe feature weights were tuned using the Mosesimplementation of MERT (Och, 2003) for all sys-tems except English-to-German, for which weused k-best MIRA (Cherry and Foster, 2012) dueto the larger number of features.We used tuning sentences drawn from all ofthe previous years?
test sets (except newstest2013,which was used as the development test set).
Inorder to speed up the tuning process, we used sub-sets of the full tuning sets with sentence pairs upto length 30 (Max-30) and further applied a fil-tering technique to reduce the tuning set size to2,000 sentence pairs for the language pairs involv-ing German, French and Czech1.
We also experi-mented with random subsets of size 2,000.For the filtering technique, we make the as-sumption that finding suitable weights for all thefeature functions requires the optimizer to see arange of feature values and to see hypotheses thatcan partially match the reference translations inorder to rank the hypotheses.
For example, if a1For Russian and Hindi, the development sets are smallerand no filtering was applied.208tuning example contains many out-of-vocabularywords or is difficult to translate for other reasons,this will result in low quality translation hypothe-ses and provide the system with little evidence forwhich features are useful to produce good transla-tions.
Therefore, we select high quality examplesusing a smooth version of sentence-BLEU com-puted on the 1-best output of a single decoder runon the development set.
Standard sentence-BLEUtends to select short examples because they aremore likely to have perfect n-gram matches withthe reference translation.
Very short sentence pairsare less informative for tuning but also tend to havemore extreme source-target length ratios whichcan affect the weight of the word penalty.
Thus,we penalize short examples by padding the de-coder output with a fixed number of non-matchingtokens2to the left and right before computingsentence-BLEU.
This has the effect of reducingthe precision of short sentences against the refer-ence translation while affecting longer sentencesproportionally less.
Experiments on phrase-basedsystems have shown that the resulting tuning setsare of comparable diversity as randomly selectedsets in terms of their feature vectors and maintainBLEU scores in comparison with tuning on the en-tire development set.Table 2 shows the size of the full tuning setsand the size of the subsets with up to length 30,Table 3 shows the results of tuning with differentsets.
Reducing the tuning sets to Max-30 resultsin a speed-up in tuning time but affects the per-formance on some of the devtest/test sets (mostlyfor Czech-English).
However, tuning on the fullset took more than 18 days using 12 cores forGerman-English which is not feasible when try-ing out several model variations.
Further filter-ing these subsets to a size of 2,000 sentence pairsas described above maintains the BLEU scores inmost cases and even improves the scores in somecases.
This indicates that the quality of the se-lected examples is more important than the totalnumber of tuning examples.
However, the exper-iments with random subsets from Max-30 showthat random selection also yields results which im-prove over the results with Max-30 in most cases,though are not always as good as with the filteredsets.3The filtered tuning sets yield reasonable per-2These can be arbitrary tokens that do not match any ref-erence token.3For random subsets from the full tuning set the perfor-mance was similar but resulted in standard deviations of upformance compared to the full tuning sets exceptfor the German-English devtest set where perfor-mance drops by 0.5 BLEU4.Tuning set Cs-En En-De De-EnFull 13,055 13,071 13,071Max-30 10,392 9,151 10,610Table 2: Size of full tuning sets and with sentencelength up to 30.devtestTuning set Cs-En En-De De-EnFull 25.1 19.9 26.7Max-30 24.7 19.8 26.2Filtered 24.9 19.8 26.2Random 24.8 19.7 26.4testTuning set Cs-En En-De De-EnFull 27.5 19.2 26.9Max-30 27.2 19.2 27.0Filtered 27.5 19.1 27.2Random 27.3 19.4 27.0Table 3: BLEU results on devtest and test sets withdifferent tuning sets: Full, Max-30, filtered subsetsof Max-30 and average of three random subsets ofMax-30 (size of filtered/random subsets: 2,000).3 English to GermanWe use the projective output of the dependencyparser ParZu (Sennrich et al., 2013) for the syn-tactic annotation of our primary submission.
Con-trastive systems were built with other parsers: Bit-Par (Schmid, 2004), the German Stanford Parser(Rafferty and Manning, 2008), and the GermanBerkeley Parser (Petrov and Klein, 2007; Petrovand Klein, 2008).The set of syntactic labels provided by ParZuhas been refined to reduce overgeneralization phe-nomena.
Specifically, we disambiguate the labelsROOT (used for the root of a sentence, but alsocommas, punctuation marks, and sentence frag-ments), KON and CJ (coordinations of differentconstituents), and GMOD (pre- or postmodifyinggenitive modifier).to 0.36 across three random sets.4Note however that due to the long tuning times, we arereporting single tuning runs.209NNSEGMENTgerichtCOMPJUNC@s@SEGMENTberufungCOMPJUNC@es@SEGMENTBundFigure 1: Syntactic representation of split com-pound Bundesberufungsgericht (Engl: federal ap-peals court).We discriminatively learn non-terminal labelsfor unknown words using sparse features, ratherthan estimating a probability distribution of non-terminal labels from singleton statistics in thetraining corpus.We perform target-side compound splitting, us-ing a hybrid method described by Fritzinger andFraser (2010) that combines a finite-state mor-phology and corpus statistics.
As finite-state mor-phology analyzer, we use Zmorge (Sennrich andKunz, 2014).
An original contribution of ourexperiments is a syntactic representation of splitcompounds which eliminates typical problemswith target-side compound splitting, namely er-roneous reorderings and compound merging.
Werepresent split compounds as a syntactic tree withthe last segment as head, preceded by a modifier.A modifier consists of an optional modifier, a seg-ment and a (possibly empty) joining element.
Anexample is shown in Figure 1.
This hierarchicalrepresentation ensures that compounds can be eas-ily merged in post-processing (by removing thespaces and special characters around joining ele-ments), and that no segments are placed outside ofa compound in the translation.We use unification-based constraints to modelmorphological agreement within German nounphrases, and between subjects and verbs (Williamsand Koehn, 2011).
Additionally, we add con-straints that operate on the internal tree structure ofthe translation hypotheses, to enforce several syn-tactic constraints that were frequently violated inthe baseline system:?
correct subcategorization of auxiliary/modalverbs in regards to the inflection of the fullverb.?
passive clauses are not allowed to have ac-cusative objects.systemBLEUdevtest testStanford Parser 19.0 18.3Berkeley Parser 19.3 18.6BitPar 19.5 18.6ParZu 19.6 19.1+ modified label set 19.8 19.1+ discriminative UNK weights 19.9 19.2+ German compound splitting 20.0 19.8+ grammatical constraints 20.2 20.1Table 4: English to German translation resultson devtest (newstest2013) and test (newstest2014)sets.?
relative clauses must contain a relative (or in-terrogative) pronoun in their first constituent.Table 4 shows BLEU scores with systemstrained with different parsers, and for our exten-sions of the baseline system.4 Czech to EnglishFor Czech to English we used the core setup de-scribed in Section 2 without modification.
Table 5shows the BLEU scores.BLEUsystem devtest testbaseline 24.8 27.0Table 5: Czech to English results on the devtest(newstest2013) and test (newstest2014) sets.5 French to EnglishFor French to English, alignment of the parallelcorpus was performed using fast_align (Dyer etal., 2013) instead of MGIZA++ due to the largevolume of parallel data.Table 6 shows BLEU scores for the system andTable 7 shows the resulting grammar sizes afterfiltering for the evaluation sets.BLEUsystem devtest testbaseline 29.4 32.3Table 6: French to English results on the devtest(newsdev2013) and test (newstest2014) sets.210system devtest testbaseline 86,341,766 88,657,327Table 7: Grammar sizes of the French to En-glish system after filtering for the devtest (new-stest2013) and test (newstest2014) sets.6 German to EnglishGerman compounds were split using the scriptprovided with Moses.For training the primary system, the target parsetrees were restructured before rule extraction byright binarization.
Since binarization strategiesincrease the tree depth and number of nodes byadding virtual non-terminals, we increased the ex-traction parameters to: Rule Depth = 7, NodeCount = 100, Rule Size = 7.
A thorough in-vestigation of binarization methods for restructur-ing Penn Treebank style trees was carried out byWang et al.
(2007).Table 8 shows BLEU scores for the baselinesystem and two systems employing different bi-narization strategies.
Table 9 shows the result-ing grammar sizes after filtering for the evaluationsets.
Results on the development set showed noimprovement when left binarization was used forrestructuring the trees, although the grammar sizeincreased significantly.BLEUsystem devtest testbaseline 26.2 27.2+ right binarization (primary) 26.8 28.2+ left binarization 26.3 -Table 8: German to English results on the devtest(newsdev2013) and test (newstest2014) sets.system devtest testbaseline 11,462,976 13,811,304+ right binarization 24,851,982 29,133,910+ left binarization 21,387,976 -Table 9: Grammar sizes of the German to En-glish systems after filtering for the devtest (new-stest2013) and test (newstest2014) sets.7 Hindi to EnglishEnglish-Hindi has the least parallel training dataof this year?s language pairs.
Out-of-vocabulary(OOV) input words are therefore a comparativelylarge source of translation error: in the devtest set(newsdev2014) and filtered test set (newstest2014)the average OOV rates are 1.08 and 1.16 unknownwords per sentence, respectively.Assuming a significant fraction of OOV wordsto be named entities and thus amenable to translit-eration, we applied the post-processing translitera-tion method described in Durrani et al.
(2014) andimplemented in Moses.
In brief, this is an unsuper-vised method that i) uses EM to induce a corpus oftransliteration examples from the parallel trainingdata; ii) learns a monotone character-level phrase-based SMT model from the transliteration corpus;and iii) substitutes transliterations for OOVs in thesystem output by using the monolingual languagemodel and other features to select between translit-eration candidates.5Table 10 shows BLEU scores with and withouttransliteration on the devtest and filtered test sets.Due to a bug in the submitted system, the languagemodel trained on the HindEnCorp corpus was usedfor transliteration candidate selection rather thanthe full interpolated language model.
This wasfixed subsequent to submission.BLEUsystem devtest testbaseline 12.9 14.7+ transliteration (submission) 13.3 15.1+ transliteration (fixed) 13.6 15.5Table 10: Hindi to English results with and with-out transliteration on the devtest (newsdev2014)and test (newstest2014) sets.Transliteration increased 1-gram precision from48.1% to 49.4% for devtest and from 49.1% to50.6% for test.
Of the 2,913 OOV words in test,938 (32.2%) of transliterations exactly match thereference.
Manual inspection reveals that there arealso many near matches.
For instance, translitera-tion produces Bernat Jackie where the reference isJacqui Barnat.8 Russian to EnglishCompared to Hindi-English, the Russian-Englishlanguage pair has over six times as much paralleldata.
Nonetheless, OOVs remain a problem: theaverage OOV rates are approximately half those5This is the variant referred to as Method 2 in Dur-rani et al.
(2014).211of Hindi-English, at 0.47 and 0.51 unknown wordsper sentence for the devtest (newstest2013) and fil-tered test (newstest2014) sets, respectively.
Weaddress this in part using the same transliterationmethod as for Hindi-English.Data sparsity issues for this language pair areexacerbated by the rich inflectional morphology ofRussian.
Many Russian word forms express gram-matical distinctions that are either absent from En-glish translations (like grammatical gender) or areexpressed by different means (like grammaticalfunction being expressed through syntactic config-uration rather than case).
We adopt the widely-used approach of simplifying morphologically-complex source forms to remove distinctions thatwe believe to be redundant.
Our method is simi-lar to that of Weller et al.
(2013) except that oursis much more conservative (in their experiments,Weller et al.
(2013) found morphological reduc-tion to harm translation indicating that useful in-formation was likely to have been discarded).We used TreeTagger (Schmid, 1994) to obtaina lemma-tag pair for each Russian word.
The tagspecifies the word class and various morphosyn-tactic feature values.
For example, the adjective???????????????
(?republican?)
gets the lemma-tag pair ???????????????
+ Afpfsnf, wherethe code A indicates the word class and the re-maining codes indicate values for the type, degree,gender, number, case, and definiteness features.Like Weller et al.
(2013), we selectively re-placed surface forms with their lemmas and re-duced tags, reducing tags through feature dele-tion.
We restricted morphological reduction to ad-jectives and verbs, leaving all other word formsunchanged.
Table 11 shows the features thatwere deleted.
We focused on contextual inflec-tion, making the assumption that inflectional dis-tinctions required by agreement alone were theleast likely to be useful for translation (since thesame information was marked elsewhere in thesentence) and also the most likely to be the sourceof ?spurious?
variation.Table 12 shows the BLEU scores for Russian-English with transliteration and morphological re-duction.
The effect of transliteration was smallerthan for Hindi-English, as might be expected fromthe lower baseline OOV rate.
1-gram precision in-creased from 57.1% to 57.6% for devtest and from62.9% to 63.6% for test.
Morphological reductiondecreased the initial OOV rates by 3.5% and 4.1%Adjective VerbType 7 Type 7Degree 3 VForm 3Gender 7 Tense 3Number 7 Person 3Case 7 Number 3Definiteness 7 Gender 7Voice 3Definiteness 7Aspect 3Case 3Table 11: Feature values that are retained (3)or deleted (7) during morphological reduction ofRussian.BLEUsystem devtest testbaseline 23.3 29.7+ transliteration 23.7 30.3+ morphological reduction 23.8 30.3Table 12: Russian to English results on the devtest(newstest2013) and test (newstest2014) sets.on the devtest and filtered test sets.
After bothmorphological and transliteration the 1-gram pre-cisions for devtest and test were 57.7% and 63.8%.9 ConclusionWe have described Edinburgh?s syntax-based sys-tems in the WMT 2014 shared translation task.Building upon the already-strong string-to-treesystems developed for previous years?
sharedtranslation tasks, we have achieved substantial im-provements over our baseline setup: we improvedtranslation into German through target-side com-pound splitting, morphosyntactic constraints, andrefinements to parse tree annotation; we have ad-dressed unknown words using transliteration (forHindi and Russian) and morphological reduction(for Russian); and we have improved our German-English system through tree binarization.AcknowledgementsThe research leading to these results has receivedfunding from the European Union Seventh Frame-work Programme (FP7/2007-2013) under grantagreement no287658 (EU-BRIDGE).Rico Sennrich has received funding from theSwiss National Science Foundation under grantP2ZHP1_148717.212ReferencesStanley F. Chen and Joshua Goodman.
1998.
An em-pirical study of smoothing techniques for languagemodeling.
Technical report, Harvard University.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, pages427?436, Montr?al, Canada, June.
Association forComputational Linguistics.Tagyoung Chung, Licheng Fang, and Daniel Gildea.2011.
Issues concerning decoding with synchronouscontext-free grammar.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies,pages 413?417, Portland, Oregon, USA, June.Nadir Durrani, Hassan Sajjad, Hieu Hoang, and PhilippKoehn.
2014.
Integrating an Unsupervised Translit-eration Model into Statistical Machine Translation.In Proceedings of the 15th Conference of the Euro-pean Chapter of the ACL (EACL 2014), Gothenburg,Sweden, April.
To appear.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A simple, fast, and effective reparameteriza-tion of ibm model 2.
In In Proc.
NAACL/HLT 2013,pages 644?648.Markus Freitag, Stephan Peitz, Joern Wuebker, Her-mann Ney, Matthias Huck, Rico Sennrich, NadirDurrani, Maria Nadejde, Philip Williams, PhilippKoehn, Teresa Herrmann, Eunah Cho, and AlexWaibel.
2014.
EU-BRIDGE MT: Combined Ma-chine Translation.
In Proceedings of the ACL 2014Ninth Workshop on Statistical Machine Translation,Baltimore, MD, USA, June.Fabienne Fritzinger and Alexander Fraser.
2010.
Howto Avoid Burning Ducks: Combining LinguisticAnalysis and Corpus Statistics for German Com-pound Processing.
In Proceedings of the Joint FifthWorkshop on Statistical Machine Translation andMetricsMATR, WMT ?10, pages 224?234, Uppsala,Sweden.Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a Translation Rule?In HLT-NAACL ?04.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In ACL-44: Proceedings of the 21st International Confer-ence on Computational Linguistics and the 44th an-nual meeting of the Association for ComputationalLinguistics, pages 961?968, Morristown, NJ, USA.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, SETQA-NLP ?08, pages 49?57, Stroudsburg, PA, USA.Mark Hopkins and Greg Langmead.
2010.
SCFG de-coding without binarization.
In Proceedings of the2010 Conference on Empirical Methods in NaturalLanguage Processing, pages 646?655, Cambridge,MA, October.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InNAACL ?03: Proceedings of the 2003 Conferenceof the North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology, pages 48?54, Morristown, NJ, USA.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Morristown, NJ, USA.Association for Computational Linguistics.Maria Nadejde, Philip Williams, and Philipp Koehn.2013.
Edinburgh?s Syntax-Based Machine Transla-tion Systems.
In Proceedings of the Eighth Work-shop on Statistical Machine Translation, pages 170?176, Sofia, Bulgaria, August.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Comput.
Linguist., 29(1):19?51, March.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Com-putational Linguistics - Volume 1, ACL ?03, pages160?167, Morristown, NJ, USA.Slav Petrov and Dan Klein.
2007.
Improved Inferencefor Unlexicalized Parsing.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics; Proceedings of the Main Confer-ence, pages 404?411, Rochester, New York, April.Slav Petrov and Dan Klein.
2008.
Parsing Germanwith Latent Variable Grammars.
In Proceedings ofthe Workshop on Parsing German at ACL ?08, pages33?39, Columbus, OH, USA, June.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the As-sociation for Computational Linguistics, ACL-44,pages 433?440.Anna N. Rafferty and Christopher D. Manning.
2008.Parsing Three German Treebanks: Lexicalized andUnlexicalized Baselines.
In Proceedings of the213Workshop on Parsing German at ACL ?08, pages 40?46, Columbus, OH, USA, June.Helmut Schmid.
1994.
Probabilistic part-of-speechtagging using decision trees.
In International Con-ference on New Methods in Language Processing,pages 44?49, Manchester, UK.Helmut Schmid.
2004.
Efficient Parsing of HighlyAmbiguous Context-Free Grammars with Bit Vec-tors.
In Proc.
of the Int.
Conf.
on ComputationalLinguistics (COLING), Geneva, Switzerland, Au-gust.Rico Sennrich and Beat Kunz.
2014.
Zmorge: A Ger-man Morphological Lexicon Extracted from Wik-tionary.
In Proceedings of the 9th InternationalConference on Language Resources and Evaluation(LREC 2014), Reykjavik, Iceland, May.Rico Sennrich, Martin Volk, and Gerold Schneider.2013.
Exploiting Synergies Between Open Re-sources for German Dependency Parsing, POS-tagging, and Morphological Analysis.
In Proceed-ings of the International Conference Recent Ad-vances in Natural Language Processing 2013, pages601?609, Hissar, Bulgaria.Andreas Stolcke.
2002.
SRILM - an extensiblelanguage modeling toolkit.
In Intl.
Conf.
SpokenLanguage Processing, Denver, Colorado, September2002.Wei Wang, Kevin Knight, Daniel Marcu, and MarinaRey.
2007.
Binarizing Syntax Trees to ImproveSyntax-Based Machine Translation Accuracy.
InJoint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning, pages 746?754.Marion Weller, Max Kisselew, Svetlana Smekalova,Alexander Fraser, Helmut Schmid, Nadir Durrani,Hassan Sajjad, and Rich?rd Farkas.
2013.
Munich-Edinburgh-Stuttgart submissions at WMT13: Mor-phological and syntactic processing for SMT.
InProceedings of the Eighth Workshop on StatisticalMachine Translation, pages 232?239, Sofia, Bul-garia, August.Philip Williams and Philipp Koehn.
2011.
AgreementConstraints for Statistical Machine Translation intoGerman.
In Proceedings of the Sixth Workshop onStatistical Machine Translation, pages 217?226, Ed-inburgh, Scotland, July.Philip Williams and Philipp Koehn.
2012.
GHKMRule Extraction and Scope-3 Parsing in Moses.
InProceedings of the Seventh Workshop on Statisti-cal Machine Translation, pages 388?394, Montr?al,Canada, June.214
