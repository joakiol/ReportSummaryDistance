Coling 2010: Poster Volume, pages 241?249,Beijing, August 2010Enhanced Sentiment Learning Using Twitter Hashtags and SmileysDmitry Davidov?
1 Oren Tsur?
21ICNC / 2Institute of Computer ScienceThe Hebrew University{oren,arir}@cs.huji.ac.ilAri Rappoport 2AbstractAutomated identification of diverse sen-timent types can be beneficial for manyNLP systems such as review summariza-tion and public media analysis.
In some ofthese systems there is an option of assign-ing a sentiment value to a single sentenceor a very short text.In this paper we propose a supervisedsentiment classification framework whichis based on data from Twitter, a popu-lar microblogging service.
By utilizing50 Twitter tags and 15 smileys as sen-timent labels, this framework avoids theneed for labor intensive manual annota-tion, allowing identification and classifi-cation of diverse sentiment types of shorttexts.
We evaluate the contribution of dif-ferent feature types for sentiment classifi-cation and show that our framework suc-cessfully identifies sentiment types of un-tagged sentences.
The quality of the senti-ment identification was also confirmed byhuman judges.
We also explore dependen-cies and overlap between different sen-timent types represented by smileys andTwitter hashtags.1 IntroductionA huge amount of social media including news,forums, product reviews and blogs contain nu-merous sentiment-based sentences.
Sentiment isdefined as ?a personal belief or judgment that?
* Both authors equally contributed to this paper.is not founded on proof or certainty?1.
Senti-ment expressions may describe the mood of thewriter (happy/sad/bored/grateful/...) or the opin-ion of the writer towards some specific entity (Xis great/I hate X, etc.
).Automated identification of diverse sentimenttypes can be beneficial for many NLP sys-tems such as review summarization systems, dia-logue systems and public media analysis systems.Sometimes it is directly requested by the user toobtain articles or sentences with a certain senti-ment value (e.g Give me all positive reviews ofproduct X/ Show me articles which explain whymovie X is boring).
In some other cases obtainingsentiment value can greatly enhance informationextraction tasks like review summarization.
Whilethe majority of existing sentiment extraction sys-tems focus on polarity identification (e.g., positivevs.
negative reviews) or extraction of a handful ofpre-specified mood labels, there are many usefuland relatively unexplored sentiment types.Sentiment extraction systems usually requirean extensive set of manually supplied sentimentwords or a handcrafted sentiment-specific dataset.With the recent popularity of article tagging, somesocial media types like blogs allow users to addsentiment tags to articles.
This allows to use blogsas a large user-labeled dataset for sentiment learn-ing and identification.
However, the set of senti-ment tags in most blog platforms is somewhat re-stricted.
Moreover, the assigned tag applies to thewhole blog post while a finer grained sentimentextraction is needed (McDonald et al, 2007).With the recent popularity of the Twitter micro-blogging service, a huge amount of frequently1WordNet 2.1 definitions.241self-standing short textual sentences (tweets) be-came openly available for the research commu-nity.
Many of these tweets contain a wide vari-ety of user-defined hashtags.
Some of these tagsare sentiment tags which assign one or more senti-ment values to a tweet.
In this paper we propose away to utilize such tagged Twitter data for classi-fication of a wide variety of sentiment types fromtext.We utilize 50 Twitter tags and 15 smileys assentiment labels which allow us to build a clas-sifier for dozens of sentiment types for short tex-tual sentences.
In our study we use four differentfeature types (punctuation, words, n-grams andpatterns) for sentiment classification and evaluatethe contribution of each feature type for this task.We show that our framework successfully identi-fies sentiment types of the untagged tweets.
Weconfirm the quality of our algorithm using humanjudges.We also explore the dependencies and overlapbetween different sentiment types represented bysmileys and Twitter tags.Section 2 describes related work.
Section 3details classification features and the algorithm,while Section 4 describes the dataset and labels.Automated and manual evaluation protocols andresults are presented in Section 5, followed by ashort discussion.2 Related workSentiment analysis tasks typically combine twodifferent tasks: (1) Identifying sentiment expres-sions, and (2) determining the polarity (sometimescalled valence) of the expressed sentiment.
Thesetasks are closely related as the purpose of mostworks is to determine whether a sentence bears apositive or a negative (implicit or explicit) opinionabout the target of the sentiment.Several works (Wiebe, 2000; Turney, 2002;Riloff, 2003; Whitelaw et al, 2005) use lexical re-sources and decide whether a sentence expressesa sentiment by the presence of lexical items (sen-timent words).
Others combine additional featuretypes for this decision (Yu and Hatzivassiloglou,2003; Kim and Hovy, 2004; Wilson et al, 2005;Bloom et al, 2007; McDonald et al, 2007; Titovand McDonald, 2008a; Melville et al, 2009).It was suggested that sentiment words may havedifferent senses (Esuli and Sebastiani, 2006; An-dreevskaia and Bergler, 2006; Wiebe and Mihal-cea, 2006), thus word sense disambiguation canimprove sentiment analysis systems (Akkaya etal., 2009).
All works mentioned above identifyevaluative sentiment expressions and their polar-ity.Another line of works aims at identifying abroader range of sentiment classes expressing var-ious emotions such as happiness, sadness, bore-dom, fear, and gratitude, regardless (or in addi-tion to) positive or negative evaluations.
Mihalceaand Liu (2006) derive lists of words and phraseswith happiness factor from a corpus of blog posts,where each post is annotated by the blogger witha mood label.
Balog et al (2006) use the moodannotation of blog posts coupled with news datain order to discover the events that drive the dom-inant moods expressed in blogs.
Mishne (2005)used an ontology of over 100 moods assignedto blog posts to classify blog texts according tomoods.
While (Mishne, 2005) classifies a blog en-try (post), (Mihalcea and Liu, 2006) assign a hap-piness factor to specific words and expressions.Mishne used a much broader range of moods.Strapparava and Mihalcea (2008) classify blogposts and news headlines to six sentiment cate-gories.While most of the works on sentiment analy-sis focus on full text, some works address senti-ment analysis in the phrasal and sentence level,see (Yu and Hatzivassiloglou, 2003; Wilson et al,2005; McDonald et al, 2007; Titov and McDon-ald, 2008a; Titov and McDonald, 2008b; Wilsonet al, 2009; Tsur et al, 2010) among others.Only a few studies analyze the sentiment andpolarity of tweets targeted at major brands.
Jansenet al (2009) used a commercial sentiment ana-lyzer as well as a manually labeled corpus.
Davi-dov et al (2010) analyze the use of the #sarcasmhashtag and its contribution to automatic recogni-tion of sarcastic tweets.
To the best of our knowl-edge, there are no works employing Twitter hash-tags to learn a wide range of emotions and the re-lations between the different emotions.2423 Sentiment classification frameworkBelow we propose a set of classification featuresand present the algorithm for sentiment classifica-tion.3.1 Classification featuresWe utilize four basic feature types for sentimentclassification: single word features, n-gram fea-tures, pattern features and punctuation features.For the classification, all feature types are com-bined into a single feature vector.3.1.1 Word-based and n-gram-based featuresEach word appearing in a sentence serves as abinary feature with weight equal to the invertedcount of this word in the Twitter corpus.
We alsotook each consecutive word sequence containing2?5 words as a binary n-gram feature using a sim-ilar weighting strategy.
Thus n-gram features al-ways have a higher weight than features of theircomponent words, and rare words have a higherweight than common words.
Words or n-gramsappearing in less than 0.5% of the training set sen-tences do not constitute a feature.
ASCII smileysand other punctuation sequences containing twoor more consecutive punctuation symbols wereused as single-word features.
Word features alsoinclude the substituted meta-words for URLs, ref-erences and hashtags (see Subsection 4.1).3.1.2 Pattern-based featuresOur main feature type is based on surface pat-terns.
For automated extraction of patterns, wefollowed the pattern definitions given in (Davidovand Rappoport, 2006).
We classified words intohigh-frequency words (HFWs) and content words(CWs).
A word whose corpus frequency is more(less) than FH (FC) is considered to be a HFW(CW).We estimate word frequency from the train-ing set rather than from an external corpus.
Unlike(Davidov and Rappoport, 2006), we consider allsingle punctuation characters or consecutive se-quences of punctuation characters as HFWs.
Wealso consider URL, REF, and HASHTAG tags asHFWs for pattern extraction.
We define a patternas an ordered sequence of high frequency wordsand slots for content words.
Following (Davidovand Rappoport, 2008), the FH and FC thresholdswere set to 1000 words per million (upper boundfor FC) and 100 words per million (lower boundfor FH )2.The patterns allow 2?6 HFWs and 1?5 slots forCWs.
To avoid collection of patterns which cap-ture only a part of a meaningful multiword ex-pression, we require patterns to start and to endwith a HFW.
Thus a minimal pattern is of theform [HFW] [CW slot] [HFW].
For each sentenceit is possible to generate dozens of different pat-terns that may overlap.
As with words and n-gramfeatures, we do not treat as features any patternswhich appear in less than 0.5% of the training setsentences.Since each feature vector is based on a singlesentence (tweet), we would like to allow approx-imate pattern matching for enhancement of learn-ing flexibility.
The value of a pattern feature isestimated according the one of the following fourscenarios3:????????????????????????????????????????????????
?1count(p) : Exact match ?
all the pattern componentsappear in the sentence in correctorder without any additional words.
?count(p) : Sparse match ?
same as exact matchbut additional non-matching words canbe inserted between pattern components.?
?nN?count(p) : Incomplete match ?
only n > 1 of Npattern components appear inthe sentence, while some non-matchingwords can be inserted in-between.At least one of the appearing componentsshould be a HFW.0 : No match ?
nothing or only a singlepattern component appears in the sentence.0 ?
?
?
1 and 0 ?
?
?
1 are parameters we useto assign reduced scores for imperfect matches.Since the patterns we use are relatively long, ex-act matches are uncommon, and taking advantageof partial matches allows us to significantly re-duce the sparsity of the feature vectors.
We used?
= ?
= 0.1 in all experiments.This pattern based framework was proven effi-cient for sarcasm detection in (Tsur et al, 2010;2Note that the FH and FC bounds allow overlap betweensome HFWs and CWs.
See (Davidov and Rappoport, 2008)for a short discussion.3As with word and n-gram features, the maximal featureweight of a pattern p is defined as the inverse count of a pat-tern in the complete Twitter corpus.243Davidov et al, 2010).3.1.3 Efficiency of feature selectionSince we avoid selection of textual featureswhich have a training set frequency below 0.5%,we perform feature selection incrementally, oneach stage using the frequencies of the featuresobtained during the previous stages.
Thus firstwe estimate the frequencies of single words inthe training set, then we only consider creationof n-grams from single words with sufficient fre-quency, finally we only consider patterns com-posed from sufficiently frequent words and n-grams.3.1.4 Punctuation-based featuresIn addition to pattern-based features we usedthe following generic features: (1) Sentencelength in words, (2) Number of ?!?
characters inthe sentence, (3) Number of ???
characters in thesentence, (4) Number of quotes in the sentence,and (5) Number of capitalized/all capitals wordsin the sentence.
All these features were normal-ized by dividing them by the (maximal observedvalue times averaged maximal value of the otherfeature groups), thus the maximal weight of eachof these features is equal to the averaged weightof a single pattern/word/n-gram feature.3.2 Classification algorithmIn order to assign a sentiment label to new exam-ples in the test set we use a k-nearest neighbors(kNN)-like strategy.
We construct a feature vec-tor for each example in the training and the testset.
We would like to assign a sentiment class toeach example in the test set.
For each feature vec-tor V in the test set, we compute the Euclideandistance to each of the matching vectors in thetraining set, where matching vectors are defined asones which share at least one pattern/n-gram/wordfeature with v.Let ti, i = 1 .
.
.
k be the k vectors with low-est Euclidean distance to v4 with assigned labelsLi, i = 1 .
.
.
k. We calculate the mean distanced(ti, v) for this set of vectors and drop from the setup to five outliers for which the distance was morethen twice the mean distance.
The label assigned4We used k = 10 for all experiments.to v is the label of the majority of the remainingvectors.If a similar number of remaining vectors havedifferent labels, we assigned to the test vector themost frequent of these labels according to theirfrequency in the dataset.
If there are no matchingvectors found for v, we assigned the default ?nosentiment?
label since there is significantly morenon-sentiment sentences than sentiment sentencesin Twitter.4 Twitter dataset and sentiment tagsIn our experiments we used an extensive Twit-ter data collection as training and testing sets.
Inour training sets we utilize sentiment hashtags andsmileys as classification labels.
Below we de-scribe this dataset in detail.4.1 Twitter datasetWe have used a Twitter dataset generously pro-vided to us by Brendan O?Connor.
This datasetincludes over 475 million tweets comprisingroughly 15% of all public, non-?low quality?tweets created from May 2009 to Jan 2010.Tweets are short sentences limited to 140 UTF-8 characters.
All non-English tweets and tweetswhich contain less than 5 proper English words5were removed from the dataset.Apart of simple text, tweets may contain URLaddresses, references to other Twitter users (ap-pear as @<user>) or a content tags (also calledhashtags) assigned by the tweeter (#<tag>)which we use as labels for our supervised clas-sification framework.Two examples of typical tweets are: ?#ipad#sucks and 6,510 people agree.
See more on Ipadsucks page: http://j.mp/4OiYyg?
?, and ?Pay nomind to those who talk behind ur back, it sim-ply means that u?re 2 steps ahead.
#ihatequotes?.Note that in the first example the hashtaggedwords are a grammatical part of the sentence (itbecomes meaningless without them) while #ihate-qoutes of the second example is a mere sentimentlabel and not part of the sentence.
Also note thathashtags can be composed of multiple words (withno spaces).5Identification of proper English words was based on anavailable WN-based English dictionary244Category # of tags % agreementStrong sentiment 52 87Likely sentiment 70 66Context-dependent 110 61Focused 45 75No sentiment 3564 99Table 1: Annotation results (2 judges) for the 3852 mostfrequent tweeter tags.
The second column displays the av-erage number of tags, and the last column shows % of tagsannotated similarly by two judges.During preprocessing, we have replaced URLlinks, hashtags and references by URL/REF/TAGmeta-words.
This substitution obviously hadsome effect on the pattern recognition phase (seeSection 3.1.2), however, our algorithm is robustenough to overcome this distortion.4.2 Hashtag-based sentiment labelsThe Twitter dataset contains above 2.5 million dif-ferent user-defined hashtags.
Many tweets includemore than a single tag and 3852 ?frequent?
tagsappear in more than 1000 different tweets.
Twohuman judges manually annotated these frequenttags into five different categories: 1 ?
strong sen-timent (e.g #sucks in the example above), 2 ?most likely sentiment (e.g., #notcute), 3 ?
context-dependent sentiment (e.g., #shoutsout), 4 ?
fo-cused sentiment (e.g., #tmobilesucks where thetarget of the sentiment is part of the hashtag), and5 ?
no sentiment (e.g.
#obama).
Table 1 showsannotation results and the percentage of similarlyassigned values for each category.We selected 50 hashtags annotated ?1?
or ?2?by both judges.
For each of these tags we automat-ically sampled 1000 tweets resulting in 50000 la-beled tweets.
We avoided sampling tweets whichinclude more than one of the sampled hashtags.As a no-sentiment dataset we randomly sampled10000 tweets with no hashtags/smileys from thewhole dataset assuming that such a random sam-ple is unlikely to contain a significant amount ofsentiment sentences.4.3 Smiley-based sentiment labelsWhile there exist many ?official?
lists of possibleASCII smileys, most of these smileys are infre-quent or not commonly accepted and used as sen-timent indicators by online communities.
We usedthe Amazon Mechanical Turk (AMT) service inorder to obtain a list of the most commonly usedand unambiguous ASCII smileys.
We asked eachof ten AMT human subjects to provide at least 6commonly used ASCII mood-indicating smileystogether with one or more single-word descrip-tions of the smiley-related mood state.
From theobtained list of smileys we selected a subset of 15smileys which were (1) provided by at least threehuman subjects, (2) described by at least two hu-man subject using the same single-word descrip-tion, and (3) appear at least 1000 times in ourTwitter dataset.
We then sampled 1000 tweets foreach of these smileys, using these smileys as sen-timent tags in the sentiment classification frame-work described in the previous section.5 Evaluation and ResultsThe purpose of our evaluation was to learn howwell our framework can identify and distinguishbetween sentiment types defined by tags or smi-leys and to test if our framework can be success-fully used to identify sentiment types in new un-tagged sentences.5.1 Evaluation using cross-validationIn the first experiment we evaluated the consis-tency and quality of sentiment classification us-ing cross-validation over the training set.
Fullyautomated evaluation allowed us to test the per-formance of our algorithm under several dif-ferent feature settings: Pn+W-M-Pt-, Pn+W+M-Pt-,Pn+W+M+Pt-, Pn-W-M-Pt+ and FULL, where +/?stands for utilization/omission of the followingfeature types: Pn:punctuation, W:Word, M:n-grams (M stands for ?multi?
), Pt:patterns.
FULLstands for utilization of all feature types.In this experimental setting, the training set wasdivided to 10 parts and a 10-fold cross validationtest is executed.
Each time, we use 9 parts as thelabeled training data for feature selection and con-struction of labeled vectors and the remaining partis used as a test set.
The process was repeated tentimes.
To avoid utilization of labels as strong fea-tures in the test set, we removed all instances ofinvolved label hashtags/smileys from the tweetsused as the test set.245Setup Smileys Hashtagsrandom 0.06 0.02Pn+W-M-Pt- 0.16 0.06Pn+W+M-Pt- 0.25 0.15Pn+W+M+Pt- 0.29 0.18Pn-W-M-Pt+ 0.5 0.26FULL 0.64 0.31Table 2: Multi-class classification results for smileys andhashtags.
The table shows averaged harmonic f-score for 10-fold cross validation.
51 (16) sentiment classes were used forhashtags (smileys).Multi-class classification.
Under multi-classclassification we attempt to assign a single label(51 labels in case of hashtags and 16 labels in caseof smileys) to each of vectors in the test set.
Notethat the random baseline for this task is 0.02 (0.06)for hashtags (smileys).
Table 2 shows the perfor-mance of our framework for these tasks.Results are significantly above the randombaseline and definitely nontrivial considering theequal class sizes in the test set.
While still rel-atively low (0.31 for hashtags and 0.64 for smi-leys), we observe much better performance forsmileys which is expected due to the lower num-ber of sentiment types.The relatively low performance of hashtags canbe explained by ambiguity of the hashtags andsome overlap of sentiments.
Examination of clas-sified sentences reveals that many of them canbe reasonably assigned to more than one of theavailable hashtags or smileys.
Thus a tweet ?I?mreading stuff that I DON?T understand again!
ha-haha...wth am I doing?
may reasonably matchtags #sarcasm, #damn, #haha, #lol, #humor, #an-gry etc.
Close examination of the incorrectlyclassified examples also reveals that substantialamount of tweets utilize hashtags to explicitly in-dicate the specific hashtagged sentiment, in thesecases that no sentiment value could be perceivedby readers unless indicated explicitly, e.g.
?DeBlob game review posted on our blog.
#fun?.Obviously, our framework fails to process suchcases and captures noise since no sentiment datais present in the processed text labeled with a spe-cific sentiment label.Binary classification.
In the binary classifica-tion experiments, we classified a sentence as ei-ther appropriate for a particular tag or as not bear-Hashtags Avg #hate #jealous #cute #outrageousPn+W-M-Pt- 0.57 0.6 0.55 0.63 0.53Pn+W+M-Pt- 0.64 0.64 0.67 0.66 0.6Pn+W+M+Pt- 0.69 0.66 0.67 0.69 0.64Pn-W-M-Pt+ 0.73 0.75 0.7 0.69 0.69FULL 0.8 0.83 0.76 0.71 0.78Smileys Avg :) ; ) X( : dPn+W-M-Pt- 0.64 0.66 0.67 0.56 0.65Pn+W+M-Pt- 0.7 0.73 0.72 0.64 0.69Pn+W+M+Pt- 0.7 0.74 0.75 0.66 0.69Pn-W-M-Pt+ 0.75 0.78 0.75 0.68 0.72FULL 0.86 0.87 0.9 0.74 0.81Table 3: Binary classification results for smileys and hash-tags.
Avg column shows averaged harmonic f-score for 10-fold cross validation over all 50(15) sentiment hashtags (smi-leys).ing any sentiment6.
For each of the 50 (15) labelsfor hashtags (smileys) we have performed a bi-nary classification when providing as training/testsets only positive examples of the specific senti-ment label together with non-sentiment examples.Table 3 shows averaged results for this case andspecific results for selected tags.
We can see thatour framework successfully identifies diverse sen-timent types.
Obviously the results are much bet-ter than those of multi-class classification, and theobserved > 0.8 precision confirms the usefulnessof the proposed framework for sentiment classifi-cation of a variety of different sentiment types.We can see that even for binary classificationsettings, classification of smiley-labeled sentencesis a substantially easier task compared to classifi-cation of hashtag-labeled tweets.
Comparing thecontributed performance of different feature typeswe can see that punctuation, word and pattern fea-tures, each provide a substantial boost for classi-fication quality while we observe only a marginalboost when adding n-grams as classification fea-tures.
We can also see that pattern features con-tribute the performance more than all other fea-tures together.5.2 Evaluation with human judgesIn the second set of experiments we evaluated ourframework on a test set of unseen and untaggedtweets (thus tweets that were not part of the train-6Note that this is a useful application in itself, as a filterthat extracts sentiment sentences from a corpus for furtherfocused study/processing.246ing data), comparing its output to tags assigned byhuman judges.
We applied our framework withits FULL setting, learning the sentiment tags fromthe training set for hashtags and smileys (sepa-rately) and executed the framework on the reducedTweeter dataset (without untagged data) allowingit to identify at least five sentences for each senti-ment class.In order to make the evaluation harsher, we re-moved all tweets containing at least one of therelevant classification hashtags (or smileys).
Foreach of the resulting 250 sentences for hashtags,and 75 sentences for smileys we generated an ?as-signment task?.
Each task presents a human judgewith a sentence and a list of ten possible hash-tags.
One tag from this list was provided by ouralgorithm, 8 other tags were sampled from the re-maining 49 (14) available sentiment tags, and thetenth tag is from the list of frequent non-sentimenttags (e.g.
travel or obama).
The human judge wasrequested to select the 0-2 most appropriate tagsfrom the list.
Allowing assignment of multipletags conforms to the observation that even shortsentences may express several different sentimenttypes and to the observation that some of the se-lected sentiment tags might express similar senti-ment types.We used the Amazon Mechanical Turk serviceto present the tasks to English-speaking subjects.Each subject was given 50 tasks for Twitter hash-tags or 25 questions for smileys.
To ensure thequality of assignments, we added to each test fivemanually selected, clearly sentiment bearing, as-signment tasks from the tagged Twitter sentencesused in the training set.
Each set was presented tofour subjects.
If a human subject failed to providethe intended ?correct?
answer to at least two ofthe control set questions we reject him/her fromthe calculation.
In our evaluation the algorithmis considered to be correct if one of the tags se-lected by a human judge was also selected by thealgorithm.
Table 4 shows results for human judge-ment classification.
The agreement score for thistask was ?
= 0.41 (we consider agreement whenat least one of two selected items are shared).Table 4 shows that the majority of tags selectedby humans matched those selected by the algo-rithm.
Precision of smiley tags is substantiallySetup % Correct % No sentiment ControlSmileys 84% 6% 92%Hashtags 77% 10% 90%Table 4: Results of human evaluation.
The second col-umn indicates percentage of sentences where judges find noappropriate tags from the list.
The third column shows per-formance on the control set.Hashtags #happy #sad #crazy # bored#sad 0.67 - - -#crazy 0.67 0.25 - -#bored 0.05 0.42 0.35 -#fun 1.21 0.06 1.17 0.43Smileys :) ; ) : ( X(; ) 3.35 - - -: ( 3.12 0.53 - -X( 1.74 0.47 2.18 -: S 1.74 0.42 1.4 0.15Table 5: Percentage of co-appearance of tags in tweetercorpus.higher than of hashtag labels, due to the lessernumber of possible smileys and the lesser ambi-guity of smileys in comparison to hashtags.5.3 Exploration of feature dependenciesOur algorithm assigns a single sentiment typefor each tweet.
However, as discussed above,some sentiment types overlap (e.g., #awesome and#amazing).
Many sentences may express severaltypes of sentiment (e.g., #fun and #scary in ?OhMy God http://goo.gl/fb/K2N5z #entertainment#fun #pictures #photography #scary #teaparty?
).We would like to estimate such inter-sentimentdependencies and overlap automatically from thelabeled data.
We use two different methods foroverlap estimation: tag co-occurrence and featureoverlap.5.3.1 Tag co-occurrenceMany tweets contain more than a single hash-tag or a single smiley type.
As mentioned, we ex-clude such tweets from the training set to reduceambiguity.
However such tag co-appearances canbe used for sentiment overlap estimation.
We cal-culated the relative co-occurrence frequencies ofsome hashtags and smileys.
Table 5 shows someof the observed co-appearance ratios.
As expectedsome of the observed tags frequently co-appearwith other similar tags.247Hashtags #happy #sad #crazy # bored#sad 12.8 - - -#crazy 14.2 3.5 - -#bored 2.4 11.1 2.1 -#fun 19.6 2.1 15 4.4Smileys :) ; ) : ( X(; ) 35.9 - - -: ( 31.9 10.5 - -X( 8.1 10.2 36 -: S 10.5 12.6 21.6 6.1Table 6: Percentage of shared features in feature vectorsfor different tags.Interestingly, it appears that a relatively highratio of co-appearance of tags is with oppositemeanings (e.g., ?#ilove eating but #ihate feelingfat lol?
or ?happy days of training going to endin a few days #sad #happy?).
This is possibly dueto frequently expressed contrast sentiment typesin the same sentence ?
a fascinating phenomenareflecting the great complexity of the human emo-tional state (and expression).5.3.2 Feature overlapIn our framework we have created a set of fea-ture vectors for each of the Twitter sentiment tags.Comparison of shared features in feature vectorsets allows us to estimate dependencies betweendifferent sentiment types even when direct tag co-occurrence data is very sparse.
A feature is con-sidered to be shared between two different senti-ment labels if for both sentiment labels there isat least a single example in the training set whichhas a positive value of this feature.
In order to au-tomatically analyze such dependencies we calcu-late the percentage of sharedWord/n-gram/Patternfeatures between different sentiment labels.
Table6 shows the observed feature overlap values forselected sentiment tags.We observe the trend of results obtained bycomparison of shared feature vectors is similar tothose obtained by means of label co-occurrence,although the numbers of the shared features arehigher.
These results, demonstrating the pattern-based similarity of conflicting, sometimes contra-dicting, emotions are interesting from a psycho-logical and cognitive perspective.6 ConclusionWe presented a framework which allows an au-tomatic identification and classification of varioussentiment types in short text fragments which isbased on Twitter data.
Our framework is a su-pervised classification one which utilizes Twitterhashtags and smileys as training labels.
The sub-stantial coverage and size of the processed Twit-ter data allowed us to identify dozens of senti-ment types without any labor-intensive manuallylabeled training sets or pre-provided sentiment-specific features or sentiment words.We evaluated diverse feature types for senti-ment extraction including punctuation, patterns,words and n-grams, confirming that each fea-ture type contributes to the sentiment classifica-tion framework.
We also proposed two differentmethods which allow an automatic identificationof sentiment type overlap and inter-dependencies.In the future these methods can be used for au-tomated clustering of sentiment types and senti-ment dependency rules.
While hashtag labels arespecific to Twitter data, the obtained feature vec-tors are not heavily Twitter-specific and in the fu-ture we would like to explore the applicability ofTwitter data for sentiment multi-class identifica-tion and classification in other domains.ReferencesAkkaya, Cem, Janyce Wiebe, and Rada Mihalcea.2009.
Subjectivity word sense disambiguation.
InEMNLP.Andreevskaia, A. and S. Bergler.
2006.
Mining word-net for fuzzy sentiment: Sentiment tag extractionfrom wordnet glosses.
In EACL.Balog, Krisztian, Gilad Mishne, and Maarten de Ri-jke.
2006.
Why are they excited?
identifying andexplaining spikes in blog mood levels.
In EACL.Bloom, Kenneth, Navendu Garg, and Shlomo Arga-mon.
2007.
Extracting appraisal expressions.
InHLT/NAACL.Davidov, D. and A. Rappoport.
2006.
Efficientunsupervised discovery of word categories usingsymmetric patterns and high frequency words.
InCOLING-ACL.248Davidov, D. and A. Rappoport.
2008.
Unsuper-vised discovery of generic relationships using pat-tern clusters and its evaluation by automatically gen-erated sat analogy questions.
In ACL.Davidov, D., O. Tsur, and A. Rappoport.
2010.Semi-supervised recognition of sarcastic sentencesin twitter and amazon.
In CoNLL.Esuli, Andrea and Fabrizio Sebastiani.
2006.
Senti-wordnet: A publicly available lexical resource foropinion mining.
In LREC.Jansen, B.J., M. Zhang, K. Sobel, and A. Chowdury.2009.
Twitter power: Tweets as electronic word ofmouth.
Journal of the American Society for Infor-mation Science and Technology.Kim, S.M.
and E. Hovy.
2004.
Determining the senti-ment of opinions.
In COLING.McDonald, Ryan, Kerry Hannan, Tyler Neylon, MikeWells, and Jeff Reynar.
2007.
Structured modelsfor fine-to-coarse sentiment analysis.
In ACL.Melville, Prem, Wojciech Gryc, and Richard D.Lawrence.
2009.
Sentiment analysis of blogs bycombining lexical knowledge with text classifica-tion.
In KDD.
ACM.Mihalcea, Rada and Hugo Liu.
2006.
A corpus-based approach to finding happiness.
In In AAAI2006 Symposium on Computational Approaches toAnalysing Weblogs.
AAAI Press.Mishne, Gilad.
2005.
Experiments with mood clas-sification in blog posts.
In Proceedings of the 1stWorkshop on Stylistic Analysis Of Text.Riloff, Ellen.
2003.
Learning extraction patterns forsubjective expressions.
In EMNLP.Strapparava, Carlo and Rada Mihalcea.
2008.
Learn-ing to identify emotions in text.
In SAC.Titov, Ivan and Ryan McDonald.
2008a.
A jointmodel of text and aspect ratings for sentiment sum-marization.
In ACL/HLT, June.Titov, Ivan and Ryan McDonald.
2008b.
Modelingonline reviews with multi-grain topic models.
InWWW, pages 111?120, New York, NY, USA.
ACM.Tsur, Oren, Dmitry Davidov, and Ari Rappoport.2010.
Icwsm ?
a great catchy name: Semi-supervised recognition of sarcastic sentences inproduct reviews.
In AAAI-ICWSM.Turney, Peter D. 2002.
Thumbs up or thumbs down?semantic orientation applied to unsupervised classi-fication of reviews.
In ACL ?02, volume 40.Whitelaw, Casey, Navendu Garg, and Shlomo Arga-mon.
2005.
Using appraisal groups for sentimentanalysis.
In CIKM.Wiebe, Janyce and Rada Mihalcea.
2006.
Word senseand subjectivity.
In COLING/ACL, Sydney, AUS.Wiebe, Janyce M. 2000.
Learning subjective adjec-tives from corpora.
In AAAI.Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In HLT/EMNLP.Wilson, Theresa, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analy-sis.
Computational Linguistics, 35(3):399?433.Yu, Hong and Vasileios Hatzivassiloglou.
2003.
To-wards answering opinion questions: Separatingfacts from opinions and identifying the polarity ofopinion sentences.
In EMNLP.249
