Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 344?354,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLeveraging Linguistic Structure For Open Domain InformationExtractionGabor Angeli Melvin Johnson PremkumarDepartment of Computer ScienceStanford University{angeli, melvinj, manning}@cs.stanford.eduChristopher D. ManningAbstractRelation triples produced by open domaininformation extraction (open IE) systemsare useful for question answering, infer-ence, and other IE tasks.
Traditionallythese are extracted using a large set of pat-terns; however, this approach is brittle onout-of-domain text and long-range depen-dencies, and gives no insight into the sub-structure of the arguments.
We replace thislarge pattern set with a few patterns forcanonically structured sentences, and shiftthe focus to a classifier which learns toextract self-contained clauses from longersentences.
We then run natural logic infer-ence over these short clauses to determinethe maximally specific arguments for eachcandidate triple.
We show that our ap-proach outperforms a state-of-the-art openIE system on the end-to-end TAC-KBP2013 Slot Filling task.1 IntroductionOpen information extraction (open IE) has beenshown to be useful in a number of NLP tasks, suchas question answering (Fader et al, 2014), rela-tion extraction (Soderland et al, 2010), and infor-mation retrieval (Etzioni, 2011).
Conventionally,open IE systems search a collection of patternsover either the surface form or dependency treeof a sentence.
Although a small set of patternscovers most simple sentences (e.g., subject verbobject constructions), relevant relations are oftenspread across clauses (see Figure 1) or presentedin a non-canonical form.Systems like Ollie (Mausam et al, 2012) ap-proach this problem by using a bootstrappingmethod to create a large corpus of broad-coveragepartially lexicalized patterns.
Although this iseffective at capturing many of these patterns, itBorn in Honolulu, Hawaii, Obama is a US Citizen.Our System Ollie(Obama; is; US citizen) (Obama; is; a US citizen)(Obama; born in; (Obama; be born in; Honolulu)Honolulu, Hawaii) (Honolulu; be born in; Hawaii)(Obama; is citizen of; US)Friends give true praise.Enemies give fake praise.Our System Ollie(friends; give; true praise) (friends; give; true praise)(friends; give; praise)(enemies; give; fake praise) (enemies; give; fake praise)Heinz Fischer of Austria visits the USOur System Ollie(Heinz Fischer; visits; US) (Heinz Fischer of Austria;visits; the US)Figure 1: Open IE extractions produced bythe system, alongside extractions from the state-of-the-art Ollie system.
Generating coherentclauses before applying patterns helps reduce falsematches such as (Honolulu; be born in; Hawaii).Inference over the sub-structure of arguments, inturn, allows us to drop unnecessary information(e.g., of Austria), but only when it is warranted(e.g., keep fake in fake praise).can lead to unintuitive behavior on out-of-domaintext.
For instance, while Obama is president isextracted correctly by Ollie as (Obama; is; pres-ident), replacing is with are in cats are felinesproduces no extractions.
Furthermore, existingsystems struggle at producing canonical argumentforms ?
for example, in Figure 1 the argumentHeinz Fischer of Austria is likely less useful fordownstream applications than Heinz Fischer.In this paper, we shift the burden of extractinginformative and broad coverage triples away fromthis large pattern set.
Rather, we first pre-processthe sentence in linguistically motivated ways toproduce coherent clauses which are (1) logically344entailed by the original sentence, and (2) easy tosegment into open IE triples.
Our approach con-sists of two stages: we first learn a classifier forsplitting a sentence into shorter utterances (Sec-tion 3), and then appeal to natural logic (S?anchezValencia, 1991) to maximally shorten these utter-ances while maintaining necessary context (Sec-tion 4.1).
A small set of 14 hand-crafted patternscan then be used to segment an utterance into anopen IE triple.We treat the first stage as a greedy search prob-lem: we traverse a dependency parse tree recur-sively, at each step predicting whether an edgeshould yield an independent clause.
Importantly,in many cases na?
?vely yielding a clause on a de-pendency edge produces an incomplete utterance(e.g., Born in Honolulu, Hawaii, from Figure 1).These are often attributable to control relation-ships, where either the subject or object of thegoverning clause controls the subject of the sub-ordinate clause.
We therefore allow the producedclause to sometimes inherit the subject or objectof its governor.
This allows us to capture a largevariety of long range dependencies with a conciseclassifier.From these independent clauses, we then extractshorter sentences, which will produce shorter ar-guments more likely to be useful for downstreamapplications.
A natural framework for solving thisproblem is natural logic ?
a proof system built onthe syntax of human language (see Section 4.1).We can then observe that Heinz Fischer of Aus-tria visits China entails that Heinz Fischer visitsChina.
On the other hand, we respect situationswhere it is incorrect to shorten an argument.
Forexample, No house cats have rabies should not en-tail that cats have rabies, or even that house catshave rabies.When careful attention to logical validity is nec-essary ?
such as textual entailment ?
this approachcaptures even more subtle phenomena.
For exam-ple, whereas all rabbits eat fresh vegetables yields(rabbits; eat; vegetables), the apparently similarsentence all young rabbits drink milk does notyield (rabbits; drink; milk).We show that our new system performs well ona real world evaluation ?
the TAC KBP Slot Fillingchallenge (Surdeanu, 2013).
We outperform bothan official submission on open IE, and a baselineof replacing our extractor with Ollie, a state-of-the-art open IE systems.2 Related WorkThere is a large body of work on open informationextraction.
One line of work begins with Text-Runner (Yates et al, 2007) and ReVerb (Faderet al, 2011), which make use of computation-ally efficient surface patterns over tokens.
Withthe introduction of fast dependency parsers, Ol-lie (Mausam et al, 2012) continues in the samespirit but with learned dependency patterns, im-proving on the earlier WOE system (Wu and Weld,2010).
The Never Ending Language Learningproject (Carlson et al, 2010) has a similar aim,iteratively learning more facts from the internetfrom a seed set of examples.
Exemplar (Mesquitaet al, 2013) adapts the open IE framework to n-ary relationships similar to semantic role labeling,but without the expensive machinery.Open IE triples have been used in a numberof applications ?
for example, learning entail-ment graphs for new triples (Berant et al, 2011),and matrix factorization for unifying open IE andstructured relations (Yao et al, 2012; Riedel etal., 2013).
In each of these cases, the concise ex-tractions provided by open IE allow for efficientsymbolic methods for entailment, such as Markovlogic networks or matrix factorization.Prior work on the KBP challenge can be cate-gorized into a number of approaches.
The mostcommon of these are distantly supervised relationextractors (Craven and Kumlien, 1999; Wu andWeld, 2007; Mintz et al, 2009; Sun et al, 2011),and rule based systems (Soderland, 1997; Grish-man and Min, 2010; Chen et al, 2010).
However,both of these approaches require careful tuning tothe task, and need to be trained explicitly on theKBP relation schema.
Soderland et al (2013) sub-mitted a system to KBP making use of open IE re-lations and an easily constructed mapping to KBPrelations; we use this as a baseline for our empiri-cal evaluation.Prior work has used natural logic for RTE-styletextual entailment, as a formalism well-suited forformal semantics in neural networks, and as aframework for common-sense reasoning (Mac-Cartney and Manning, 2009; Watanabe et al,2012; Bowman et al, 2014; Angeli and Manning,2013).
We adopt the precise semantics of Icardand Moss (2014).
Our approach of finding shortentailments from a longer utterance is similar inspirit to work on textual entailment for informa-tion extraction (Romano et al, 2006).345Born in a small town, she took the midnight train going anywhere.prep inamoddetvmodnsubjdobjnndetvmod dobjshe Born in a small townprep inamoddetnsubj(input) (extracted clause)?
?she took the midnight train going anywhere she took the midnight trainBorn in a small town, she took the midnight train she took midnight trainBorn in a town, she took the midnight train .
.
.she Born in small townshe Born in a townshe Born in town?
?
(she; took; midnight train)(she; born in; small town)(she; born in; town)Figure 2: An illustration of our approach.
From left to right, a sentence yields a number of independentclauses (e.g., she Born in a small town ?
see Section 3).
From top to bottom, each clause produces a setof entailed shorter utterances, and segments the ones which match an atomic pattern into a relation triple(see Section 4.1).3 Inter-Clause Open IEIn the first stage of our method, we produce a setof self-contained clauses from a longer utterance.Our objective is to produce a set of clauses whichcan stand on their own syntactically and seman-tically, and are entailed by the original sentence(see Figure 2).
Note that this task is not specific toextracting open IE triples.
Conventional relationextractors, entailment systems, and other NLP ap-plications may also benefit from such a system.We frame this task as a search problem.
At agiven node in the parse tree, we classify each out-going arc e = pl??
c, from the governor p to a de-pendent c with [collapsed] Stanford Dependencylabel l, into an action to perform on that arc.
Oncewe have chosen an action to take on that arc, wecan recurse on the dependent node.
We decom-pose the action into two parts: (1) the action totake on the outgoing edge e, and (2) the actionto take on the governor p. For example, in ourmotivating example, we are considering the arc:e = tookvmod????
born.
In this case, the correctaction is to (1) yield a new clause rooted at born,and (2) interpret the subject of born as the subjectof took.We proceed to describe this action space inmore detail, followed by an explanation of ourtraining data, and finally our classifier.3.1 Action SpaceThe three actions we can perform on a dependencyedge are:Yield Yields a new clause on this depen-dency arc.
A canonical case of this action isthe arc suggestccomp?????
brush in Dentists suggestthat you should brush your teeth, yielding youshould brush your teeth.Recurse Recurse on this dependency arc, butdo not yield it as a new clause.
For example,in the sentence faeries are dancing in the fieldwhere I lost my bike, we must recurse throughthe intermediate constituent the field where I lostmy bike ?
which itself is not relevant ?
to get tothe clause of interest: I lost my bike.Stop Do not recurse on this arc, as the subtreeunder this arc is not entailed by the parent sen-tence.
This is the case, for example, for mostleaf nodes (furry cats are cute should not entailthe clause furry), and is an important action forthe efficiency of the algorithm.With these three actions, a search path throughthe tree becomes a sequence of Recurse andYield actions, terminated by a Stop action (or leafnode).
For example, a search sequence ARecurse?????
?BY ield????
CStop????
D would yield a clause rootedat C. A sequence AY ield????
BY ield????
CStop????
Dwould yield clauses rooted at both B and C. Find-ing all such sequences is in general exponential inthe size of the tree.
In practice, during training werun breadth first search to collect the first 10 000sequences.
During inference we run uniform costsearch until our classifier predictions fall below a346given threshold.For the Stop action, we do not need to furtherspecify an action to take on the parent node.
How-ever, for both of the other actions, it is often thecase that we would like to capture a controller inthe higher clause.
We define three such commonactions:Subject Controller If the arc we are consider-ing is not already a subject arc, we can copy thesubject of the parent node and attach it as a sub-ject of the child node.
This is the action taken inthe example Born in a small town, she took themidnight train.Object Controller Analogous to the subjectcontroller action above, but taking the object in-stead.
This is the intended action for exampleslike I persuaded Fred to leave the room.1Parent Subject If the arc we are taking is theonly outgoing arc from a node, we take the par-ent node as the (passive) subject of the child.This is the action taken in the example Obama,our 44thpresident to yield a clause with the se-mantics of Obama [is] our 44thpresident.Although additional actions are easy to imagine,we found empirically that these cover a wide rangeof applicable cases.
We turn our attention to thetraining data for learning these actions.3.2 TrainingWe collect a noisy dataset to train our clause gen-eration model.
We leverage the distant supervisionassumption for relation extraction, which creates anoisy corpus of sentences annotated with relationmentions (subject and object spans in the sentencewith a known relation).
Then, we take this anno-tation as itself distant supervision for a correct se-quence of actions to take: any sequence which re-covers the known relation is correct.We use a small subset of the KBP source doc-uments for 2010 (Ji et al, 2010) and 2013 (Sur-deanu, 2013) as our distantly supervised corpus.To try to maximize the density of known relationsin the training sentences, we take all sentenceswhich have at least one known relation for ev-ery 10 tokens in the sentence, resulting in 43 155sentences.
In addition, we incorporate the 23 725manually annotated examples from Angeli et al(2014).1The system currently misses most most such cases dueto insufficient support in the training data.Once we are given a collection of labeled sen-tences, we assume that a sequence of actionswhich leads to a correct extraction of a knownrelation is a positive sequence.
A correct ex-traction is any extraction we produce from ourmodel (see Section 4) which has the same argu-ments as the known relation.
For instance, if weknow that Obama was born in Hawaii from thesentence Born in Hawaii, Obama .
.
.
, and an ac-tion sequence produces the triple (Obama, born in,Hawaii), then we take that action sequence as apositive sequence.Any sequence of actions which results in aclause which produces no relations is in turn con-sidered a negative sequence.
The third case to con-sider is a sequence of actions which produces arelation, but it is not one of the annotated rela-tions.
This arises from the incomplete negativesproblem in distantly supervised relation extraction(Min et al, 2013): since our knowledge base isnot exhaustive, we cannot be sure if an extractedrelation is incorrect or correct but previously un-known.
Although many of these unmatched re-lations are indeed incorrect, the dataset is suffi-ciently biased towards the STOP action that theoccasional false negative hurts end-to-end perfor-mance.
Therefore, we simply discard such se-quences.Given a set of noisy positive and negative se-quences, we construct training data for our actionclassifier.
All but the last action in a positive se-quence are added to the training set with the labelRecurse; the last action is added with the labelSplit.
Only the last action in a negative sequenceis added with the label Stop.
We partition the fea-ture space of our dataset according to the actionapplied to the parent node.3.3 InferenceWe train a multinomial logistic regression classi-fier on our noisy training data, using the featuresin Table 1.
The most salient features are the labelof the edge being taken, the incoming edge to theparent of the edge being taken, neighboring edgesfor both the parent and child of the edge, and thepart of speech tag of the endpoints of the edge.The dataset is weighted to give 3?
weight to ex-amples in the Recurse class, as precision errorsin this class are relatively harmless for accuracy,while recall errors are directly harmful to recall.Inference now reduces to a search problem.
Be-347Feature Class Feature TemplatesEdge taken {l, short name(l)}Last edge taken {incoming edge(p)}Neighbors of parent {nbr(p), (p, nbr(p))}Grandchild edges {out edge(c),(e, out edge(c))}Grandchild count {count (nbr(echild))(e, count (nbr(echild)))}Has subject/object ?e?{e,echild}?l?
{subj,obj}1(l ?
nbr(e))POS tag signature {pos(p), pos(c),(pos(p), pos(c))}Features at root {1(p = root),POS(p)}Table 1: Features for the clause splitter model, de-ciding to split on the arc e = pl??
c. The fea-ture class is a high level description of features;the feature templates are the particular templatesused.
For instance, the POS signature contains thetag of the parent, the tag of the child, and both tagsjoined in a single feature.
Note that all features arejoined with the action to be taken on the parent.ginning at the root of the tree, we consider everyoutgoing edge.
For every possible action to beperformed on the parent (i.e., clone subject, cloneroot, no action), we apply our trained classifier todetermine whether we (1) split the edge off as aclause, and recurse; (2) do not split the edge, andrecurse; or (3) do not recurse.
In the first twocases, we recurse on the child of the arc, and con-tinue until either all arcs have been exhausted, orall remaining candidate arcs have been marked asnot recursable.We will use the scores from this classifier toinform the score assigned to our generated openIE extractions (Section 4).
The score of a clauseis the product of the scores of actions taken toreach the clause.
The score of an extraction willbe this score multiplied by the score of the extrac-tion given the clause.4 Intra-Clause Open IEWe now turn to the task of generating a maximallycompact sentence which retains the core seman-tics of the original utterance, and parsing the sen-tence into a conventional open IE subject verb ob-ject triple.
This is often a key component in down-stream applications, where extractions need to benot only correct, but also informative.
Whereasan argument like Heinz Fischer of Austria is oftencorrect, a downstream application must apply fur-ther processing to recover information about eitherHeinz Fischer, or Austria.
Moreover, it must do sowithout the ability to appeal to the larger contextof the sentence.4.1 Validating Deletions with Natural LogicWe adopt a subset of natural logic semantics dic-tating contexts in which lexical items can be re-moved.
Natural logic as a formalism capturescommon logical inferences appealing directly tothe form of language, rather than parsing to a spe-cialized logical syntax.
It provides a proof theoryfor lexical mutations to a sentence which eitherpreserve or negate the truth of the premise.For instance, if all rabbits eat vegetables thenall cute rabbits eat vegetables, since we are al-lowed to mutate the lexical item rabbit to cuterabbit.
This is done by observing that rabbit isin scope of the first argument to the operator all.Since all induces a downward polarity environ-ment for its first argument, we are allowed to re-place rabbit with an item which is more specific ?in this case cute rabbit.
To contrast, the operatorsome induces an upward polarity environment forits first argument, and therefore we may derive theinference from cute rabbit to rabbit in: some cuterabbits are small therefore some rabbits are small.For a more comprehensive introduction to naturallogic, see van Benthem (2008).We mark the scopes of all operators (all, no,many, etc.)
in a sentence, and from this deter-mine whether every lexical item can be replacedby something more general (has upward polarity),more specific (downward polarity), or neither.
Inthe absence of operators, all items have upwardspolarity.Each dependency arc is then classified intowhether deleting the dependent of that arc makesthe governing constituent at that node moregeneral, more specific (a rare case), or nei-ther.2For example, removing the amod edge incuteamod????
rabbit yields the more general lexicalitem rabbit.
However, removing the nsubj edge inFidonsubj????
runs would yield the unentailed (andnonsensical) phrase runs.
The last, rare, case isan edge that causes the resulting item to be morespecific ?
e.g., quantmod: aboutquantmod???????
200 ismore general than 200.2We use the Stanford Dependencies representation (deMarneffe and Manning, 2008).348For most dependencies, this semantics can behard-coded with high accuracy.
However, thereare at least two cases where more attention is war-ranted.
The first of these concerns non-subsectiveadjectives: for example a fake gun is not a gun.
Forthis case, we make use of the list of non-subsectiveadjectives collected in Nayak et al (2014), andprohibit their deletion as a hard constraint.The second concern is with prepositional at-tachment, and direct object edges.
For example,whereas Alice went to the playgroundprep with??????
?Bob entails that Alice went to the playground, itis not meaningful to infer that Alice is friendsprep with???????
Bob entails Alice is friends.
Analo-gously, Alice playeddobj???
baseball on Sunday en-tails that Alice played on Sunday; but, Obamasigneddobj???
the bill on Sunday should not entailthe awkward phrase *Obama signed on Sunday.We learn these attachment affinities empiricallyfrom the syntactic n-grams corpus of Goldbergand Orwant (2013).
This gives us counts for howoften object and preposition edges occur in thecontext of the governing verb and relevant neigh-boring edges.
We hypothesize that edges whichare frequently seen to co-occur are likely to beessential to the meaning of the sentence.
To thisend, we compute the probability of seeing an arcof a given type, conditioned on the most specificcontext we have statistics for.
These contexts, andthe order we back off to more general contexts, isgiven in Figure 3.To compute a score s of deleting the edge fromthe affinity probability p collected from the syn-tactic n-grams, we simply cap the affinity and sub-tract it from 1:s = 1?min(1,pK)where K is a hyperparameter denoting the mini-mum fraction of the time an edge should occur ina context to be considered entirely unremovable.In our experiments, we set K =13.The score of an extraction, then, is the productof the scores of each deletion multiplied by thescore from the clause splitting step in Section 3.4.2 Atomic PatternsOnce a set of short entailed sentences is produced,it becomes straightforward to segment them intoconventional open IE triples.
We employ 6 sim-ple dependency patterns, given in Table 2, whichObama signed the bill into law on Fridaynsubjdobjdetprep intoprep onprepbackoff????????????
?p(prep on |Obama signed billnsubjdobj)p(prep on |Obama signed lawnsubjprep into)p(prep on |Obama signednsubj)p(prep on | signed)dobjbackoff{p(dobj |Obama signed billnsubjdobj)p(dobj | signed)Figure 3: The ordered list of backoff probabilitieswhen deciding to drop a prepositional phrase or di-rect object.
The most specific context is chosen forwhich an empirical probability exists; if no con-text is found then we allow dropping prepositionalphrases and disallow dropping direct objects.
Notethat this backoff arbitrarily orders contexts of thesame size.Input Extractioncats play with yarn (cats; play with; yarn)fish like to swim (fish; like to; swim)cats have tails (cats; have; tails)cats are cute (cats; are; cute)Tom and Jerry are fighting (Tom; fighting; Jerry)There are cats with tails (cats; have; tails)Table 2: The six dependency patterns used to seg-ment an atomic sentence into an open IE triple.cover the majority of atomic relations we are in-terested in.When information is available to disambiguatethe substructure of compound nouns (e.g., namedentity segmentation), we extract additional re-lations with 5 dependency and 3 TokensRegex(Chang and Manning, 2014) surface form patterns.These are given in Table 3; we refer to theseas nominal relations.
Note that the constraint ofnamed entity information is by no means requiredfor the system.
In other applications ?
for exam-ple, applications in vision ?
the otherwise trivialnominal relations could be quite useful.349KBP Relation Open IE Relation PMI2KBP Relation Open IE Relation PMI2Org:Founded found in 1.17 Per:Date Of Birth be bear on 1.83be found in 1.15 bear on 1.28Org:Dissolved *buy Chrysler in 0.95 Per:Date Of Death die on 0.70*membership in 0.60 be assassinate on 0.65Org:LOC Of HQ in 2.12 Per:LOC Of Birth be bear in 1.21base in 1.82 Per:LOC Of Death *elect president of 2.89Org:Member Of *tough away game in 1.80 Per:Religion speak about 0.67*away game in 1.80 popular for 0.60Org:Parents ?s bank 1.65 Per:Parents daughter of 0.54*also add to 1.52 son of 1.52Org:Founded By invest fund of 1.48 Per:LOC Residence of 1.48own stake besides 1.18 *independent from 1.18Table 4: A selection of the mapping from KBP to lemmatized open IE relations, conditioned on the typesof the arguments being correct.
The top one or two relations are shown for 7 person and 6 organizationrelations.
Incorrect or dubious mappings are marked with an asterisk.Input ExtractionDurin, son of Thorin (Durin; is son of; Thorin)Thorin?s son, Durin (Thorin; ?s son; Durin)IBM CEO Rometty (Rometty; is CEO of; IBM)President Obama (Obama; is; President)Fischer of Austria (Fischer; is of; Austria)IBM?s research group (IBM; ?s; research group)US president Obama (Obama; president of; US)Our president, Obama, (Our president; be; Obama)Table 3: The eight patterns used to segment a nounphrase into an open IE triple.
The first five are de-pendency patterns; the last three are surface pat-terns.5 Mapping OpenIE to a Known RelationSchemaA common use case for open IE systems is to mapthem to a known relation schema.
This can eitherbe done manually with minimal annotation effort,or automatically from available training data.
Weuse both methods in our TAC-KBP evaluation.
Acollection of relation mappings was constructedby a single annotator in approximately a day,3anda relation mapping was learned using the proce-dure described in this section.We map open IE relations to the KBP schemaby searching for co-occurring relations in a largedistantly-labeled corpus, and marking open IE and3The official submission we compare against claimed twoweeks for constructing their manual mapping, although a ver-sion of their system constructed in only 3 hours performsnearly as well.KBP relation pairs which have a high PMI2value(B?eatrice, 1994; Evert, 2005) conditioned on theirtype signatures matching.
To compute PMI2, wecollect probabilities for the open IE and KBP re-lation co-occurring, the probability of the open IErelation occurring, and the probability of the KBPrelation occurring.
Each of these probabilities isconditioned on the type signature of the relation.For example, the joint probability of KBP relationrkand open IE relation ro, given a type signatureof t1, t2, would bep(rk, ro| t1, t2) =count(rk, ro, t1, t2)?r?k,r?ocount(r?k, r?o, t1, t2).Omitting the conditioning on the type signaturefor notational convenience, and defining p(rk) andp(ro) analogously, we can then compute The PMI2value between the two relations:PMI2(rk, ro) = log(p(rk, ro)2p(rk) ?
p(ro))Note that in addition to being a measurerelated to PMI, this captures a notion simi-lar to alignment by agreement (Liang et al,2006); the formula can be equivalently writtenas log [p(rk| ro)p(ro| rk)].
It is also function-ally the same as the JC WordNet distance measure(Jiang and Conrath, 1997).Some sample type checked relation mappingsare given in Table 4.
In addition to intuitive map-pings (e.g., found in?Org:Founded), we can notesome rare, but high precision pairs (e.g., investfund of ?
Org:Founded By).
We can also see350the noise in distant supervision occasionally per-meate the mapping, e.g., with elect president of ?Per:LOC Of Death ?
a president is likely to die inhis own country.6 EvaluationWe evaluate our approach in the context of a real-world end-to-end relation extraction task ?
theTAC KBP Slot Filling challenge.
In Slot Filling,we are given a large unlabeled corpus of text, afixed schema of relations (see Section 5), and aset of query entities.
The task is to find all rela-tion triples in the corpus that have as a subject thequery entity, and as a relation one of the definedrelations.
This can be viewed intuitively as popu-lating Wikipedia Infoboxes from a large unstruc-tured corpus of text.We compare our approach to the Universityof Washington submission to TAC-KBP 2013(Soderland et al, 2013).
Their system usedOpenIE v4.0 (a successor to Ollie) run over theKBP corpus and then they generated a mappingfrom the extracted relations to the fixed schema.Unlike our system, Open IE v4.0 employs a se-mantic role component extracting structured SRLframes, alongside a conventional open IE system.Furthermore, the UW submission allows for ex-tracting relations and entities from substrings ofan open IE triple argument.
For example, fromthe triple (Smith; was appointed; acting director ofAcme Corporation), they extract that Smith is em-ployed by Acme Corporation.
We disallow suchextractions, passing the burden of finding correctprecise extractions to the open IE system itself (seeSection 4).For entity linking, the UW submission uses TomLin?s entity linker (Lin et al, 2012); our sub-mission uses the Illinois Wikifier (Ratinov et al,2011) without the relational inference component,for efficiency.
For coreference, UW uses the Stan-ford coreference system (Lee et al, 2011); we em-ploy a variant of the simple coref system describedin (Pink et al, 2014).We report our results in Table 5.4UW Offi-cial refers to the official submission in the 2013challenge; we show a 3.1 F1improvement (to 22.74All results are reported with the anydoc flag set to truein the evaluation script, meaning that only the truth of theextracted knowledge base entry and not the associated prove-nance is scored.
In absence of human evaluators, this is inorder to not penalize our system unfairly for extracting a newcorrect provenance.System P R F1UW Official?69.8 11.4 19.6Ollie?57.4 4.8 8.9+ Nominal Rels?57.7 11.8 19.6Our System- Nominal Rels?64.3 8.6 15.2+ Nominal Rels?61.9 13.9 22.7+ Alt.
Name 57.8 17.8 27.1+ Alt.
Name + Website 58.6 18.6 28.3Table 5: A summary of our results on the end-to-end KBP Slot Filling task.
UW official is thesubmission made to the 2013 challenge.
The sec-ond row is the accuracy of Ollie embedded inour framework, and of Ollie evaluated with nom-inal relations from our system.
Lastly, we reportour system, our system with nominal relations re-moved, and our system combined with an alternatenames detector and rule-based website detector.Comparable systems are marked with a dagger?orasterisk?.F1) over this submission, evaluated using a com-parable approach.
A common technique in KBPsystems but not employed by the official UW sub-mission in 2013 is to add alternate names basedon entity linking and coreference.
Additionally,websites are often extracted using heuristic name-matching as they are hard to capture with tradi-tional relation extraction techniques.
If we makeuse of both of these, our end-to-end accuracy be-comes 28.2 F1.We attempt to remove the variance in scoresfrom the influence of other components in an end-to-end KBP system.
We ran the Ollie open IE sys-tem (Mausam et al, 2012) in an identical frame-work to ours, and report accuracy in Table 5.
Notethat when an argument to an Ollie extraction con-tains a named entity, we take the argument to bethat named entity.
The low performance of thissystem can be partially attributed to its inability toextract nominal relations.
To normalize for this,we report results when the Ollie extractions aresupplemented with the nominal relations producedby our system (Ollie + Nominal Rels in Table 5).Conversely, we can remove the nominal relationextractions from our system; in both cases we out-perform Ollie on the task.3510.00 0.02 0.04 0.06 0.08 0.10 0.12 0.14Recall0.00.20.40.60.81.0PrecisionOllieOur System (without nominals)Figure 4: A precision/recall curve for Ollie andour system (without nominals).
For clarity, recallis plotted on a range from 0 to 0.15.6.1 DiscussionWe plot a precision/recall curve of our extractionsin Figure 4 in order to get an informal sense ofthe calibration of our confidence estimates.
Sinceconfidences only apply to standard extractions, weplot the curves without including any of the nom-inal relations.
The confidence of a KBP extrac-tion in our system is calculated as the sum of theconfidences of the open IE extractions that supportit.
So, for instance, if we find (Obama; be bearin; Hawaii) n times with confidences c1.
.
.
cn,the confidence of the KBP extraction would be?ni=0ci.
It is therefore important to note thatthe curve in Figure 4 necessarily conflates theconfidences of individual extractions, and the fre-quency of an extraction.With this in mind, the curves lend some inter-esting insights.
Although our system is very highprecision on the most confident extractions, it hasa large dip in precision early in the curve.
Thissuggests that the model is extracting multiple in-stances of a bad relation.
Systematic errors inthe clause splitter are the likely cause of these er-rors.
While the approach of splitting sentencesinto clauses generalizes better to out-of-domaintext, it is reasonable that the errors made in theclause splitter manifest across a range of sentencesmore often than the fine-grained patterns of Olliewould.On the right half of the PR curve, however, oursystem achieves both higher precision and extendsto a higher recall than Ollie.
Furthermore, thecurve is relatively smooth near the tail, suggestingthat indeed we are learning a reasonable estimateof confidence for extractions that have only onesupporting instance in the text ?
empirically, 46%of our extractions.In total, we extract 42 662 862 open IE tripleswhich link to a pair of entities in the corpus(i.e., are candidate KBP extractions), covering1 180 770 relation types.
202 797 of these rela-tion types appear in more than 10 extraction in-stances; 28 782 in more than 100 instances, and4079 in more than 1000 instances.
308 293 rela-tion types appear only once.
Note that our systemover-produces extractions when both a general andspecific extraction are warranted; therefore thesenumbers are an overestimate of the number of se-mantically meaningful facts.For comparison, Ollie extracted 12 274 319triples, covering 2 873 239 relation types.1 983 300 of these appeared only once; 69 010appeared in more than 10 instances, 7951 in morethan 100 instances, and 870 in more than 1000instances.7 ConclusionWe have presented a system for extracting opendomain relation triples by breaking a long sen-tence into short, coherent clauses, and then find-ing the maximally simple relation triples which arewarranted given each of these clauses.
This allowsthe system to have a greater awareness of the con-text of each extraction, and to provide informativetriples to downstream applications.
We show thatour approach performs well on one such down-stream application: the KBP Slot Filling task.AcknowledgmentsWe thank the anonymous reviewers for theirthoughtful feedback.
Stanford University grate-fully acknowledges the support of a Natural Lan-guage Understanding-focused gift from GoogleInc.
and the Defense Advanced Research ProjectsAgency (DARPA) Deep Exploration and Filter-ing of Text (DEFT) Program under Air Force Re-search Laboratory (AFRL) contract no.
FA8750-13-2-0040.
Any opinions, findings, and conclu-sion or recommendations expressed in this mate-rial are those of the authors and do not necessarilyreflect the view of the DARPA, AFRL, or the USgovernment.352ReferencesGabor Angeli and Christopher D. Manning.
2013.Philosophers are mortal: Inferring the truth of un-seen facts.
In CoNLL.Gabor Angeli, Julie Tibshirani, Jean Y. Wu, andChristopher D. Manning.
2014.
Combining dis-tant and partial supervision for relation extraction.In EMNLP.DAILLE B?eatrice.
1994.
Approche mixte pourl?extraction automatique de terminologie: statis-tique lexicale et filtres linguistiques.
Ph.D. thesis,Th`ese de Doctorat.
Universit?e de Paris VII.Jonathan Berant, Ido Dagan, and Jacob Goldberger.2011.
Global learning of typed entailment rules.
InProceedings of ACL, Portland, OR.Samuel R. Bowman, Christopher Potts, and Christo-pher D. Manning.
2014.
Recursive neuralnetworks can learn logical semantics.
CoRR,(arXiv:1406.1827).Andrew Carlson, Justin Betteridge, Bryan Kisiel,Burr Settles, Estevam R Hruschka Jr, and Tom MMitchell.
2010.
Toward an architecture for never-ending language learning.
In AAAI.Angel X. Chang and Christopher D. Manning.
2014.TokensRegex: Defining cascaded regular expres-sions over tokens.
Technical Report CSTR 2014-02,Department of Computer Science, Stanford Univer-sity.Zheng Chen, Suzanne Tamang, Adam Lee, XiangLi, Wen-Pin Lin, Matthew Snover, Javier Artiles,Marissa Passantino, and Heng Ji.
2010.
CUNY-BLENDER.
In TAC-KBP.Mark Craven and Johan Kumlien.
1999.
Constructingbiological knowledge bases by extracting informa-tion from text sources.
In AAAI.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependencies rep-resentation.
In Coling 2008: Proceedings of theworkshop on Cross-Framework and Cross-DomainParser Evaluation.Oren Etzioni.
2011.
Search needs a shake-up.
Nature,476(7358):25?26.Stefan Evert.
2005.
The statistics of word cooccur-rences: word pairs and collocations.
Ph.D. thesis,Universit at Stuttgart.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In EMNLP.Anthony Fader, Luke Zettlemoyer, and Oren Etzioni.2014.
Open question answering over curated andextracted knowledge bases.
In KDD.Yoav Goldberg and Jon Orwant.
2013.
A dataset ofsyntactic-ngrams over time from a very large corpusof english books.
In *SEM.Ralph Grishman and Bonan Min.
2010.
New YorkUniversity KBP 2010 slot-filling system.
In Proc.TAC 2010 Workshop.Thomas Icard, III and Lawrence Moss.
2014.
Recentprogress on monotonicity.
Linguistic Issues in Lan-guage Technology.Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-fitt, and Joe Ellis.
2010.
Overview of the tac 2010knowledge base population track.
In Third TextAnalysis Conference.Jay J Jiang and David W Conrath.
1997.
Semanticsimilarity based on corpus statistics and lexical tax-onomy.
Proceedings of the 10th International Con-ference on Research on Computational Linguistics.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s multi-pass sieve corefer-ence resolution system at the conll-2011 shared task.In CoNLL Shared Task.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In NAACL-HLT.Thomas Lin, Mausam, and Oren Etzioni.
2012.
Nonoun phrase left behind: detecting and typing un-linkable entities.
In EMNLP-CoNLL.Bill MacCartney and Christopher D Manning.
2009.An extended model of natural logic.
In Proceedingsof the eighth international conference on computa-tional semantics.Mausam, Michael Schmitz, Robert Bart, StephenSoderland, and Oren Etzioni.
2012.
Open languagelearning for information extraction.
In EMNLP.Filipe Mesquita, Jordan Schmidek, and Denilson Bar-bosa.
2013.
Effectiveness and efficiency of openrelation extraction.
In EMNLP.Bonan Min, Ralph Grishman, Li Wan, Chang Wang,and David Gondek.
2013.
Distant supervision forrelation extraction with an incomplete knowledgebase.
In NAACL-HLT.Mike Mintz, Steven Bills, Rion Snow, and Dan Juraf-sky.
2009.
Distant supervision for relation extrac-tion without labeled data.
In ACL.Neha Nayak, Mark Kowarsky, Gabor Angeli, andChristopher D. Manning.
2014.
A dictionary ofnonsubsective adjectives.
Technical Report CSTR2014-04, Department of Computer Science, Stan-ford University, October.Glen Pink, Joel Nothman, and R. James Curran.
2014.Analysing recall loss in named entity slot filling.
InEMNLP.353Lev Ratinov, Dan Roth, Doug Downey, and Mike An-derson.
2011.
Local and global algorithms for dis-ambiguation to wikipedia.
In ACL.Sebastian Riedel, Limin Yao, Andrew McCallum, andBenjamin M Marlin.
2013.
Relation extractionwith matrix factorization and universal schemas.
InNAACL-HLT.Lorenza Romano, Milen Kouylekov, Idan Szpektor,Ido Dagan, and Alberto Lavelli.
2006.
Investigat-ing a generic paraphrase-based approach for relationextraction.
EACL.V?
?ctor Manuel S?anchez S?anchez Valencia.
1991.
Stud-ies on natural logic and categorial grammar.
Ph.D.thesis, University of Amsterdam.Stephen Soderland, Brendan Roof, Bo Qin, Shi Xu,Mausam, and Oren Etzioni.
2010.
Adapting openinformation extraction to domain-specific relations.AI Magazine, 31(3):93?102.Stephen Soderland, John Gilmer, Robert Bart, Oren Et-zioni, and Daniel S. Weld.
2013.
Open informationextraction to KBP relations in 3 hours.
In Text Anal-ysis Conference.Stephen G Soderland.
1997.
Learning text analysisrules for domain-specific natural language process-ing.
Ph.D. thesis, University of Massachusetts.Ang Sun, Ralph Grishman, Wei Xu, and Bonan Min.2011.
New York University 2011 system for KBPslot filling.
In Proceedings of the Text AnalyticsConference.Mihai Surdeanu.
2013.
Overview of the tac2013knowledge base population evaluation: English slotfilling and temporal slot filling.
In Sixth Text Analy-sis Conference.Johan van Benthem.
2008.
A brief history of naturallogic.
Technical Report PP-2008-05, University ofAmsterdam.Yotaro Watanabe, Junta Mizuno, Eric Nichols, NaoakiOkazaki, and Kentaro Inui.
2012.
A latent discrim-inative model for compositional entailment relationrecognition using natural logic.
In COLING.Fei Wu and Daniel S Weld.
2007.
Autonomously se-mantifying wikipedia.
In Proceedings of the six-teenth ACM conference on information and knowl-edge management.
ACM.Fei Wu and Daniel S Weld.
2010.
Open informationextraction using wikipedia.
In ACL.
Association forComputational Linguistics.Limin Yao, Sebastian Riedel, and Andrew McCal-lum.
2012.
Probabilistic databases of universalschema.
In Proceedings of the Joint Workshop onAutomatic Knowledge Base Construction and Web-scale Knowledge Extraction.Alexander Yates, Michael Cafarella, Michele Banko,Oren Etzioni, Matthew Broadhead, and StephenSoderland.
2007.
TextRunner: Open informationextraction on the web.
In ACL-HLT.354
