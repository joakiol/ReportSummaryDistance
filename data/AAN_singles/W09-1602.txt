Proceedings of CLIAWS3, Third International Cross Lingual Information Access Workshop, pages 3?11,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsSpeech Retrieval in Unknown Languages: a Pilot Study?Xiaodan Zhuang# Jui Ting Huang# Mark Hasegawa-JohnsonBeckman Institute, Department of Electrical and Computer EngineeringUniversity of Illinois at Urbana-Champaign, U.S.A.{xzhuang2,jhuang29,jhasegaw}@uiuc.eduAbstractMost cross-lingual speech retrieval assumesintensive knowledge about all involved lan-guages.
However, such resource may not ex-ist for some less popular languages.
Someapplications call for speech retrieval in un-known languages.
In this work, we lever-age on a quasi-language-independent subwordrecognizer trained on multiple languages, toobtain an abstracted representation of speechdata in an unknown language.
Language-independent query expansion is achieved ei-ther by allowing a wide lattice output for anaudio query, or by taking advantage of dis-tinctive features in speech articulation to pro-pose subwords most similar to the given sub-words in a query.
We propose using a re-trieval model based on finite state machinesfor fuzzy matching of speech sound patterns,and further for speech retrieval.
A pilot studyof speech retrieval in unknown languages ispresented, using English, Spanish and Russianas training languages, and Croatian as the un-known target language.1 IntroductionDramatic increase in recorded speech media calls forefficient retrieval of audio files.
Accessing speechmedia of a foreign language is a particularly impor-tant and challenging task, often referred to as cross-lingual speech retrieval or cross-lingual spoken doc-ument retrieval.
?This research is funded by NSF grants 0534106 and0703624.
The authors would like to thank Su-Youn Yoon forinspiring discussion.
#The student authors contribute equally.Previous work on cross-lingual speech retrievalmostly leverages on intensive knowledge about allthe languages involved.
Most reported work inves-tigates retrieval in a target language, in response toaudio or text queries given in a different source lan-guage (Meng et al, 2000; Virga and Khudanpur,2003).
Usually, the speech media in the target lan-guage, and the audio queries in the source language,are converted to speech recognition transcripts us-ing large-vocabulary automatic speech recognizers(LVASR) trained for the target language and thesource language respectively.
The text queries, ortranscribed audio queries, are translated to the tar-get language.
Text retrieval techniques are appliedto retrieve speech, by retrieving the correspond-ing LVASR transcription in the target language.
Insuch systems, a large-vocabulary speech recognizertrained on the target language is essential, whichrequires the existence of a dictionary and labeledacoustic training data in that language.LVASR currently do not exist for most of the 6000languages on Earth.
In some situations, knowledgeabout the target language is limited, and definitelynot sufficient to enable training LVASR.
Imaginean audio database in a target language unknown toa user, who needs to retrieve spoken content rel-evant to some audible query in this unknown lan-guage.
For example, the user knows how the name?Obama?
is pronounced in the target language, andwants to retrieve all spoken documents that containthe query word, from a database in this unknownlanguage.
A linguist might find himself/herself inthis scenario when he or she tries to collect a largenumber of utterances containing some particular3phrases in an unknown language.
Similarly, an in-formation analyst might wish to leverage on speechretrieval in unknown languages to organize criticalinformation before engaging linguistic experts forfiner analysis.
We refer to such retrieval tasks asspeech retrieval in unknown languages, in which lit-tle knowledge about the target language is assumed.A human linguist attempting to manually per-form speech retrieval in an unknown languagewould necessarily map the perceived speech (bothdatabase and query) into some cognitive abstractionor schema, representing, perhaps, the phonetic dis-tinctions that he or she has been trained to hear.Matching and retrieval of speech would then be per-formed based on such an abstraction.
Two cog-nitive processes, assimilation and accommodation,take place when human brains are to process newinformation (Bernstein et al, 2007), such as speechin an unknown language.
In accommodation, the in-ternal stored knowledge adapts to new informationwith which it is confronted.
In assimilation, the newinformation, e.g., speech in an unknown language, ismapped to previously stored information, e.g., sub-words (phones) as defined by knowledge about thelanguages known to the listener.This paper models speech retrieval in unknownlanguages using a machine learning model of pho-netic assimilation.
A quasi-language-independentsubword recognizer is trained to capture salient sub-words and their acoustic distribution in multiplelanguages.
This recognizer is applied on an un-known language, therefore mapping segments of theunknown speech to subwords in the known lan-guages.
Through this machine cognitive process,the database and queries in the unknown languageare represented as sequences of quasi-language-independent subwords.
Speech retrieval is per-formed based on such representation.
Figure 1 illus-trates that speech retrieval in an unknown languagecan be modeled as a special case of assimilation.This task differs from the more widely studiedknown-language speech retrieval task, in that no lin-guistic knowledge of the target language is assumed.We can only leverage on knowledge that can beapplied by assimilation to the multiple known lan-guages.
Therefore, this task is more like a cross-lingual sound pattern retrieval task, leveraged onquasi-language-independent subwords, rather thanFigure 1: Automatic speech retrieval in an unknown lan-guage (below) is modeled as a special case of the cogni-tive process called assimilation (above).a translated spoken word/phrase retrieval task us-ing target language LVASR transcripts, as in mostcross-lingual speech retrieval systems.
The quasi-language-independent subword recognizer is trainedon speech data other than the target language, andtherefore generates much noisier recognition results,owing to potential mismatch between acoustic distri-butions, lack of dictionary and lack of a word-levellanguage model.To manage the extra difficulty, we adopt a sub-word lattice representation to encode a wide hypoth-esis space of recognized speech in the target lan-guage.
Language-independent query expansion isachieved either by allowing a wide lattice outputfor an audio query, or by taking advantage of dis-tinctive features in speech articulation to proposequasi-language-independent subwords most similarto the given subwords in a query.
Finite state ma-chines (FSM) constructed from the speech latticesare used to allow for fuzzy matching of speechsound patterns, and further for retrieval in unknownlanguages.We carry out a pilot study of speech retrievalin unknown languages, using English, Spanish andRussian as training languages, and Croatian as theunknown target language.
To explain the effect ofadditional knowledge about the target language, wedemonstrate the improvements in retrieval perfor-mance that result by incrementally making availablesubword sequence models and acoustic models forthe target language.2 Quasi-Language-Independent subwordModels2.1 Deriving a subword setBased on the assumption that an audible phrase in anunknown language can be represented as a sequence4of subwords, the question is to find an appropriateset of subword symbols.
Schultz and Waibel (2001)reported that a global unit set for the sourcelanguages based on International Phonetic Alpha-bet (IPA) symbols outperforms language-dependentphonetic units in cross-lingual word recognitiontasks, whereas language-dependent phonetic unitsare better models for multilingual word recognition(in which the target language is also one of thesource languages).
A multilingual task might ben-efit from partitioning the feature space according tolanguage identity, i.e., to have different subsets ofmodels aiming at different languages.
By contrast,a cross-lingual task calls for one consistent set ofmodels with language-independent properties in or-der to maximize portability into the new language.To capture the necessary distinctions between dif-ferent phones across languages, we first pool to-gether individual phone inventories for source lan-guages, each of which has its phones tagged witha language identity, and then performed bottom-upclustering on the phone pool based on pairwise sim-ilarity between their acoustic models.
Each clusterrepresents one distinct language-independent sub-word symbol.
Since this set is still derived frommultiple languages, we refer to these subword unitsas quasi-language-independent subwords.
A quasi-language-independent subword set is derived by thefollowing steps:First, we encode all speech in the known lan-guages using a language-dependent phone set.
Eachsymbol in this set is defined by the phone iden-tity and the language identity.
One single-Gaussianthree-state left-to-right HMM is trained for each ofthese subword units.Second, similarity between the language-dependent phones is estimated by the approximatedKL divergence between corresponding acousticmodels.
As shown in (Vihola et al, 2002), KLdivergence between single-Gaussian left-to-rightHMMs can be approximated in closed form byEquation 1,KLD(U, V ) =S?i=1riS?j=1aUij log(aUij/aVij)(1)+S?i=1riI(bUi : bVi), (2)where aij is the transition probability to hidden statej, and bi and ri are the observation distributionand steady-state probability for hidden state i. Forsingle-Gaussian distribution, I(bUi : bVi)can be ap-proximated by,I(bUi : bVi ) = 12[log???Vi?????Ui?
?+ tr(?Ui((?Vi)?1 ?
(?Ui)?1))+ tr((?Vi)?1 (?Ui ?
?Vi) (?Ui ?
?Vi)T) ].Third, we use the Affinity Propagation algorithm(Frey and Dueck, 2007) to conduct pairwise cluster-ing of phones based on the approximated KL diver-gence between acoustic models.
The tendency for adata point (a phone) to be an exemplar of a clusteris controlled by the preference value assigned to thatphone.
The preference of a phone i is set as followsto favor frequent phones to be cluster centers:p(i) = k log(Ci), (3)where Ci is the count of the phone i, and k is a nor-malization term to control the total number of clus-ters.
To discourage subwords from the same lan-guage to join a same cluster, pairwise distance be-tween them are offset by an additional amount, com-parable to the maximum pairwise distance betweenthe models.The resultant subword set is supposed to cap-ture quasi-language-independent phonetic informa-tion, and each subword unit has relatively distinctiveacoustic distribution.
These subwords are encodedusing the corresponding cluster exemplars as surro-gates.2.2 Recognizing subwordsAn automatic speech recognition (ASR) system(Jelinek, 1998) serves to recognize both queriesand speech database, with acoustic models for thelanguage-independent subwords derived from theknown languages as described in section 2.1.
Thefront-end features extracted from the speech dataare 39-dimensional features including 12 PerceptualLinear Prediction (PLP) coefficients and their en-ergy, as well as the first-order and second order re-gression coefficients.5We create context-dependent models for eachsubword, using the same strategy for build-ing context-dependent triphone models in LVASR(Woodland et al, 1994).
A ?triphone?
is a subwordwith its context defined as its immediate precedingand following subwords.
Each triphone is repre-sented by a continuous three-state left-to-right Hid-den Markov Model (HMM).
Additionally, there is aone-state HMM for silence, two three-state HMMsfor noise and unknown sound respectively.
Thenumber of Gaussian mixtures (9 to 21 Gaussians) isoptimized according to a development set consistingof speech in the known languages.
A standard tree-based state tying technique is adopted for parametersharing between subwords with similar contexts.The ?language model?
(LM), or more preciselysubword sequence model, should generalize fromthe known languages to the unknown language.
Ourtrial experiments showed that unigram statistics ofsubwords and their triphones is more transferableacross languages than N-gram statistics.
We also as-sume that infrequent triphones are less likely to besalient units that would carry the properties of theunknown language.
Thus, we select the top frequenttriphones and map the rest of the triphones to theircenter phones, forming a mixed vocabulary of fre-quent triphones and context-independent subwords.The frequencies of these vocabulary entries are usedto estimate an unigram LM in the ASR system.
Tri-phones in the ASR output are mapped back to itscenter subwords before the retrieval stage.3 Speech Retrieval through SubwordIndexingIn many cross-lingual speech retrieval systems, thespeech media are processed by a large-vocabularyautomatic speech recognizer (LVASR), which hasaccess to vocabulary, dictionary, word languagemodel and acoustic models for the target lan-guage.
With all these resources, state-of-the-artspeech recognition could give reasonable hypoth-esized word transcript, enabling direct applicationof text retrieval techniques.
However, this is notthe case in speech retrieval in unknown languages.Moreover, without the higher level linguistic knowl-edge, such as a word dictionary, this task aims tofind speech patterns that sound similar, as approxi-mated by sequences of quasi-language-independentsubwords.
Therefore, the sequential information inthe hypothesized subwords is critical.To deal with the significant noise in the subwordrecognition output, and to emphasize the sequentialinformation, we use the recognizer to obtain sub-word lattices instead of one-best hypotheses.
Theselattices can be represented as weighted automata,which are compact representations of a large num-ber of alternative subword sequences, each asso-ciated with a weight indicating the uncertainty ofthe data.
Therefore, indexing speech in unknownlanguage can be achieved by indexing the corre-sponding weighted automata with quasi-language-independent subwords associated with the state tran-sitions.We adopt the weighted automata indexation algo-rithm reported in (Allauzen et al, 2004), which isoptimal for searching subword sequences, as it takestime linear in the sum of the query size and the num-ber of speech media entries where it appears.
Theautomata indexation algorithm also preserves the se-quential information, which is crutial for this task.We leverage on two kinds of knowledge for queryexpansion, namely empirical phone confusion andknowledge-based phone confusion.
An illustrationof our speech retrieval system is presented in Fig-ure 2.
We detail the indexing approaching as well asquery expansion and retrieval in this section.Figure 2: Framework of speech retrieval through subwordindexing63.1 Subword Finite State Machines as SpeechIndicesWe construct a full index that can be used to searchfor a query within all the speech utterances ui, i ?1, ..., n. In particular, this is achieved by construct-ing a weighted finite-state transducer T , mappingeach query x to the set of speech utterances whereit appears.
Each returned speech utterance u is as-signed a score, which is the negative log of the ex-pected count of the query x in utterance u.The subword lattice for speech utterance ui can berepresented as a weighted finite state automata Ai,whose path weights correspond to the joint proba-bility of the observed speech and the hypothesizedsubword sequence.
To get an automata whose pathweights correspond to desired negative log of poste-rior probabilities, we simply need to apply a generalweight-pushing algorithm to Ai in the log semiring,resulting in an automata Bi.
In this automata Bi,the probability of a given string x is the sum of theprobability of all paths that contains x.The key point of constructing the index transducerTi for uttereance ui is to introduce new paths thatenable matching between a query and any portionsof the original paths, while properly normalizing thepath weights.
This is achieved by factor selection in(Allauzen et al, 2004).
First, null output is intro-duced to each transition in the automata, convertingthe automata into a transducer.
Second, a new tran-sition is introduced from a new unique initial state toeach existing state, with null input and output.
Theweight associated with this transition is the negativelog of the forward probability.
Similarly, a new tran-sition is created from each state to a new unique finalstate, with null input and output as the label i of thecurrent utterance ui.
The assicated weight is the neg-ative log of the backward probability.
General finitestate machine optimization operations (Allauzen etal., 2007) of weighted ?-removal, determinizationand minimization over the log semiring can be ap-plied to the resulting transducer.
As shown in (Al-lauzen et al, 2004), the path with input of string xand output of label i has a weight corresponding tothe negative log of the expected count of x in utter-ance ui.To optimize the retrieval time, we divide all ut-terances into a few groups.
Within each group, theutterance index transducers are unioned and deter-minized to get one single index transducer for thegroup.
It is then feasible to expedite retrieval byprocessing each group index transducer in a paral-lel fashion.3.2 Query ExpansionWhile sequential information is important, ex-act string match is very unplausible in this chal-lenging task, even when subword lattices encodemany alternative recognition hypotheses.
Language-independent query expansion is therefore critical forsuccess in retrieval.
We carry out query expansioneither by allowing a wide lattice output for an audioquery, or by taking advantage of distinctive featuresin speech articulation to propose quasi-language-independent subwords most similar to the given sub-words in a query.In particular, for a spoken query, ASR will gen-erate a subword lattice instead of a one-best sub-word sequence hypothesis.
With the lattice, the au-dio query is encoded by the best hypothesis fromASR and its empirical phone confusion.
The latticecan then be represented as a finite-state automata.However, when the query is given as a targetlanguage subword sequence, we can no longer usethe recognizer to obtain an expanded query.
Fur-thermore, some target language subwords may noteven exist in the quasi-language-independent sub-word set in the recognizer.
In this case, knowledge-based phone confusion is engaged via the use of aset of distinctive features Fj , j ?
1, ...,M for hu-man speech (Chomsky and Halle, 1968), includinglabial, alveolar, post-alveolar, retroflex, voiced, as-pirated, front, back, etc.We estimate similarity from phone a to phone b,or more precisely, substitution tendency as in Equa-tion 4,DFsim(a, b) = log NabNa (4)whereNab =M?j=1(F aj ?
F bj = 1),Na =M?j=1(F aj 6= 0).7The target subword sequence is first mapped tothe derived subword set, by locating the identicalor nearest member phone in the clustering and thenadopting the surrogate for that cluster.
This con-verted sequence of derived subwords is further ex-panded by adding the most likely alternative quasi-language-independent subwords, parallel to eachoriginal subword.
Transitions to these alternativesubwords are associated with the corresponding sub-stitution tendency based on distinctive features.3.3 SearchAn expanded query, either obtained from an audioquery or a subword sequence query, is representedas a weighted finite state automata.
Searching thisquery in the utterances is achieved by composing thequery automata with the index transducer.
This re-sults in another finite state transducer, which is fur-ther processed by projection on output, removal of?
arcs and determinization.
The output is a list ofretrieved speech utterances, each with the expectedcount of the query.Apparently, the precision and recall of the re-trieval results vary with the width of the subwordlattices used for indexing as well as how much thequery is expanded.
We control the width of the sub-word lattices via the number of tokens and the max-imum probability decrease allowed for each step inthe Viterbi decoding.
The extend to which a sub-word sequence query is expanded is determined bythe lowest allowed similarity between the originalphone and an alternative phone.
These parametersare set empirically.4 Experiments4.1 DatasetThe known language pool should cover as many lan-guage families as possible so that the derived sub-words could better approximate language indepen-dence.
However, as a pilot study, this paper reportsexperiments using only languages within the Indo-European family.
Table 1 summarizes the size ofspeech data from each language.
Croatian is usedas the unknown target language, and the other threelanguages are the known languages used for de-riving and training the quasi-language-independentsubword models.
We extracted 80% of all speakersper language for training, and 10% as a developmentset.Language ID Hours Spks StyleCroatian hrv 21.3 201 Read+answersEnglish hub 13.6 406 BroadcastSpanish spa 14.6 120 Read+answersRussian rus 2.5 63 Read+answersTable 1: Summary for data: language ID, total length,number of speakers and speaking style for each language.4.2 SettingsThe speech retrieval task aims to find speech utter-ances that contain a particular query.
We use twokinds of queries: 1) subword sequence queries, tran-scribed as a sequence of phonetic symbols in the tar-get language; 2) audio queries, each being an audiosegment of the speech query in the target language.Since we aim to match speech patterns that soundlike each other, the queries used in this experimentare relatively short, about 3 to 5 syllables.
This addsto the challenge in that very limited redundant in-formation is available for query-utterance matching.There are totally 40 subword sequences and 40 audioqueries, each occurs in between 18 and 38 utterancesout of a set of 576 utterances.In addition to a cross-lingual retrieval system builtusing only the known languages, we incrementallyaugment resource on the target language to buildmore knowledgeable systems.AM0LM0: Both the acoustic model (AM)and the language model (LM) are quasi-language-independent, trained using data in multiple knownlanguages.
This happens when no transcribedspeech data or a defined phone set exist for the tar-get language.
Essentially the system has no directknowledge about the target language.AM0LMt: This setting examines the perfor-mance gap due to the acoustic model mismatchby using a quasi-language-independent AM, but atarget language LM.
Suppose that a word dictio-nary with phonetic transcription and possibly sometext data from the target language are available,for training a target language subword LM.
To findthe mapping between target triphones and language-independent source AMs, linguistic knowledge andphonetic symbol notation are the only information8we can use.
First, we map each of target mono-phones to source phone symbols: Any source clusterthat contains a phonetic symbol with the same nota-tion as the target phonetic symbol becomes a surro-gate symbol for that target phone.
If a target phoneis unseen to the known languages, the most similarphone will be chosen first.
The similarity is based onthe distinctive features, as discussed in Section 3.2.Second, the target triphones are converted to possi-ble source triphones for which acoustic models ex-ist.
Each target triphone not modeled in the sourcelanguage AM is replaced with the corresponding di-phone (subword pair) if it exists, otherwise the cen-ter phone.AMtLM0: This setting examines the perfor-mance gap due to the language model mismatch byusing a quasi-language-independent source LM, buta target language AM.
For the source triphones andmonophones that do not exist in the target AM, theyare mapped to target AMs in a way similar as de-scribed above.AMtLMt: Both AM and LM are trained for thetarget language.
This setting provides an upperbound of the performance for different settings.4.3 MetricsWe evaluate the performance for both subwordrecognition and speech retrieval, measured as fol-lows.Recognition Accuracy: The ground truth is en-coded using subwords in the target language whilethe recognition output is encoded using quasi-language-independent subwords in Section 2.
Tomeasure the recognition accuracy, we label eachquasi-language-independent subword cluster usingthe most frequent target language subword that ap-pears in that cluster.
The hypothesis subword se-quence is then compared against the groundtruth us-ing a dynamic-programming-based string alignmentprocedure.
The recognition accuracy is defined asREC ?
ACC = H?IN ?
100%, where H , I , andN are the numbers of correct labels, insertion errorsand groundtruth labels respectively.Retrieval Precision: The retrieval performanceis measured using Mean Average Precision (IR ?MAP ), defined as the mean of the Average Preci-sion (AP ) for a set of different queries x. MeanAverage Precision (IR ?
MAP ) can be defined inEquation 5. n is the number of ordered retrievedutterances and R is the total number of relevant ut-terances.
fi is an indicator function whether the ithretrieved utterance does contain the query.
Precisionpm for top m retrieved utterances can be calculatedas pm = 1m?mk=1 f(k).IR?MAP = 1QQ?x=1AP (x),AP (x) = 1R(x)n(x)?i=1fi(x)pi(x).
(5)We use IR ?
MAPA and IR ?
MAPS to denotethe retrieval MAP for audio queries and subword se-quence queries respectively.4.4 ResultsTable 2 presents a few examples of the derivedquasi-language-independent subwords.
As dis-cussed in Section 2, these subwords are obtained bybottom-up clustering of all the language-dependentIPA phones in the multiple known languages.
Thesame IPA symbol across languages may lie in thesame cluster, e.g., /z/ in Cluster 1, or different clus-ters, e.g., /j/ in Cluster 3 and 4.
Although symbolswithin the same language are discouraged to be inone cluster, it still desirably happens for highly sim-ilar pairs, e.g., /1/rus and /j/rus in Cluster 4.Cluster ID Surrogate Other phone members1 /z/hub /z/spa, /z/rus, /zj/rus2 /tSj/rus /tS/hub, /tS/spa3 /j/hub /j/spa4 /i:/hub /1/rus, /j/rusTable 2: Examples of quasi-language-independent sub-words, as clusters of source language IPAs.Table 3 compares the subword recognitionand retrieval performance for the quasi-language-independent subwords and IPA phones.
We canSetting REC ?
ACC IR?MAPA IR?MAPSIPA 37.18% 17.90% 31.40%AM0LM0 42.52% 23.24% 32.62%Table 3: Performance of quasi-languange-independentsubword and IPA.9Setting AMtLMt AMtLM0 AM0LMt AM0LM0REC ?
ACC 73.45% 67.29% 49.88% 42.52%IR?MAPA 58.82% 52.38% 28.32% 23.24%IR?MAPS 76.96% 51.86% 34.95% 32.62%Table 4: Performance of subword recognition and speechretrieval.see that on the unknown language Croatian, the de-rived quasi-language-independent subwords outper-form the IPA symbol set in both phone recognitionand retrieval using two kinds of queries.narrow wide1020304050607080Query ExpansionIR?MAP(%)AMtLMtAMtLM0AM0LMtAM0LM0Figure 3: Speech retrieval performance for subword se-quence queriesnarrow wide15202530354045505560Query ExpansionIR?MAP(%)AMtLMtAMtLM0AM0LMtAM0LM0Figure 4: Speech retrieval performance for audio queriesTable 4 presents the subword recognition accu-racy and retrieval performance with optimal querywidth.
Figure 3 and Figure 4 presents speechretrieval performance at varying query widths forsubword sequence queries and audio queries re-spectively.
It is shown that speech retrieval incompletely unknown language achieves MAP of23.24% and 32.62% while the system trained usingthe most available knowledge about the target lan-guage reaches MAP of 58.82% and 76.96%, for au-dio queries and subword sequence queries respec-tively.
We also demonstrate access to phone fre-quency (AM0LMt) and acoustic data (AMtLM0)both boosts retrieval performance, and the effect isroughly additive (AMtLMt).5 Conclusion and DiscussionIn this work, we present a speech retrieval approachin unknown languages.
This approach leverageson speech recognition based on quasi-language-independent subword models derived from multi-ple known languages, and finite state machine basedfuzzy speech pattern matching and retrieval.
Ourexperiments use Croatian as the unknown languageand English, Russian and Spanish as the known lan-guages.
Results show that the derived subwords out-perform the IPA symbols, and access to the subwordlanguage model and acoustic models in the unknownlanguage explains the gap between this challengingtask and retrieval with knowledge about the targetlanguage.The proposed retrieval approach on unknown lan-guages can be viewed as a machine learning modelof phonetic assimiliation, in which the segmentsin an unknown language are mapped to language-independent subwords learned from the multipleknown languages.
However, another important cog-nitive process, i.e., accomodation, is not yet mod-eled.
We believe the capability to create new sub-words unseen in the known languages would leadto improved performance.
In particular, speech seg-ments that are hypothesized by the quasi-language-independent subword recognizer with very low con-fidence scores can be clustered to form these newsubwords, accomodating to the unknown language.The approach in this work can be readily scaledup to much larger speech corpora.
In particular,larger corpora would make it more practical to im-plement the accomodation process discussed above.Besides, that would also enable online adaptationof the model parameters of the quasi-language-independent subword recognizer.
Both are believedto promise reduced gap between retrieval perfor-mance in a known language and an unknown lan-guage, and are potential future work beyond this pa-per.10ReferencesC.
Allauzen, M. Mohri, and M. Saraclar.
2004.
Gen-eral indexation of weighted automata ?
application tospoken utterance retrieval.
In Proc.
HLT-NAACL.C.
Allauzen, M. Riley, J. Schalkwyk, W. Skut, andM.Mohri.
2007.
Openfst: A general and effi-cient weighted finite-state transducer library.
In Proc.CIAA.Bernstein, Penner, Clarke-Stewart, and Roy.
2007.
Psy-chology.
Houghton Mifflin Company.Noam Chomsky and Morris Halle.
1968.
The SoundPattern of English.
New York: Harper and Row.Brendan J. Frey and Delbert Dueck.
2007.
Clusteringby passing messages between data points.
Science,315:972?976.Frederick Jelinek.
1998.
Statistical Methods for SpeechRecognition.
The MIT Press.Helen Meng, Berlin Chen, Erika Grams, Sanjeev Khu-danpur, Wai-Kit Lo, Gina-Anne Levow, Douglas Oard,Patrick Schone, Karen Tang, Hsin-Min Wang, andJian Qiang Wang.
2000.
Mandarin-english informa-tion (MEI): Investigating translingual speech retrieval.http://www.clsp.jhu.edu/ws2000/final reports/mei/ws00mei.pdf.Tanja Schultz and Alex Waibel.
2001.
Language inde-pendent and language adaptive acoustic modeling forspeech recognition.
Speech Communication, 35:31?51.M.
Vihola, M. Harju, P. Salmela, J. Suontausta, andJ.
Savela.
2002.
Two dissimilarity measures for hmmsand their application in phoneme model clustering.
InProc.
ICASSP, volume 1, pages I?933 ?
I?936.Paola Virga and Sanjeev Khudanpur.
2003.
Transliter-ation of proper names in crosslingual information re-trieval.
In Proc.
ACL 2003 workshop MLNER.P.C.
Woodland, J.J. Odell, V. Valtchev, and S.J.
Young.1994.
Large vocabulary continuous speech recogni-tion using HTK.
In Proc.
ICASSP, volume 2, pagesII/125?II/128.11
