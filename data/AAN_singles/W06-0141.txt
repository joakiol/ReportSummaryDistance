Proceedings of the Fifth SIGHAN Workshop on Chinese Language Processing, pages 217?220,Sydney, July 2006. c?2006 Association for Computational LinguisticsDesigning Special Post-processing Rules for SVM-basedChinese Word SegmentationMuhua Zhu, Yilin Wang, Zhenxing Wang, Huizhen Wang, Jingbo ZhuNatural Language Processing LabNortheastern UniversityNo.3-11, Wenhua Road, Shenyang, Liaoning, China, 110004{zhumh, wangyl, wangzx, wanghz}@ics.neu.edu.cnzhujingbo@mail.neu.edu.cnAbstractWe participated in the Third Interna-tional Chinese Word Segmentation Bake-off.
Specifically, we evaluated our Chi-nese word segmenter NEUCipSeg inthe close track, on all four corpora,namely Academis Sinica (AS), City Uni-versity of Hong Kong (CITYU), Mi-crosoft Research (MSRA), and Univer-sity of Pennsylvania/University of Col-orado (UPENN).
Based on Support Vec-tor Machines (SVMs), a basic segmenteris designed regarding Chinese word seg-mentation as a problem of character-basedtagging.
Moreover, we proposed post-processing rules specially taking into ac-count the properties of results broughtout by the basic segmenter.
Our systemachieved good ranks in all four corpora.1 SVM-based Chinese Word SegmenterWe built out segmentation system following (Xueand Shen, 2003), regarding Chinese word segmen-tation as a problem of character-based tagging.Instead of Maximum Entropy, we utilized Sup-port Vector Machines as an alternate.
SVMs area state-of-the-art learning algorithm, owing theirsuccess mainly to the ability in control of general-ization error upper-bound, and the smooth integra-tion with kernel methods.
See details in (Vapnik,1995).
We adopted svm-light1 as the specificimplementation of the model.1.1 Problem FormalizationBy formalizing Chinese word segmentation intothe problem of character-based tagging, we as-1http://svmlight.joachims.org/signed each character to one and only one of thefour classes: word-prefix, word-suffix,word-stem and single-character.
Forexample, given a two-word sequence?????
?, the Chinese words for ?Southeast Asia(???)
people(?)
?, the character ??
?is as-signed to the category word-prefix, indicatingthe beginning of a word;??
?is assigned to thecategory word-stem, indicating the middle po-sition of a word; ??
?belongs to the categoryword-suffix, meaning the ending of a Chineseword; and last,??
?is assigned to the categorysingle-character, indicating that the singlecharacter itself is a word.1.2 Feature TemplatesWe utilized four of the five basic feature templatessuggested in (Low et al , 2005), described asfollows:?
Cn(n = ?2,?1, 0, 1, 2)?
CnCn+ 1(n = ?2,?1, 0, 1)?
Pu(C0)?
T (C?2)T (C?1)T (C0)T (C1)T (C2)where C refers to a Chinese character.
The firsttwo templates specify a context window with thesize of five characters, where C0 stands for thecurrent character: the former describes individualcharacters and the latter presents bigrams withinthe context window.
The third template checksif current character is a punctuation or not, andthe last one encodes characters?
type, includingfour types: numbers, dates, English letters andthe type representing other characters.
See de-tail description and the example in (Low et al, 2005).
We dropped template C?1C1, since,217in experiments, it seemed not to perform wellwhen incorporated by SVMs.
Slightly differentfrom (Low et al , 2005), character set repre-senting dates are expanded to include ??????????????????????
?,the Chinese characters for ?day?, ?month?, ?year?,?hour?,?minute?,?second?, respectively.2 Post-processing RulesSegmentation results of SVM-based segmenterhave their particular properties.
In respect to theproperties of segmentation results produced by theSVM-based segmenter, we extracted solely fromtraining data comprehensive and effective post-processing rules, which are grouped into two cate-gories: The rules, termed IV rules, make ef-forts to fix segmentation errors of character se-quences, which appear both in training and test-ing data; Rules seek to recall some OOV(OutOf Vocabulary) words, termed OOV rules.
Inpractice, we sampled out a subset from train-ing dataset as a development set for the analysisof segmentation results produced by SVM-basedsegmenter.
Note that, in the following, we definedVocabulary to be the collection of words ap-pearing in training dataset and SegmentationUnit to be any isolated character sequence as-sumed to be a valid word by a segmenter.
Asegmentation unit can be a correctly seg-mented word or an incorrectly segmented charac-ter sequence.2.1 IV RulesThe following rules are named IV rules, pur-suing the consistence between segmentation re-sults and training data.
The intuition underlyingthe rules is that since training data give somewhatspecific descriptions for most of the words in it, acharacter sequence in testing data should be seg-mented in accordance with training data as muchas possible.Ahead of post-processing, all words in thetraining data are grouped into two distinct sets:the uniquity set, which consists of wordswith unique segmentation in training data and theambiguity set, which includes words havingmore than one distinct segmentations in trainingdata.
For example, the character sequence????
?has two kinds of segmentations, as??
???
(new century) and?????
(as a compo-nent of some Named-Entity, such as the name of arestaurant).?
For each word in the uniquity set, checkwhether it is wrongly segmented into morethan one segmentation units by the SVM-based segmenter.
If true, the continuous seg-mentation units corresponding to the wordare grouped into the united one.
The in-tuition underlying this post-processing ruleis that SVM-based segmenter prefers two-character words or single-character wordswhen confronting the case that the segmenterhas low self-confidence in some character-sequence segmentation.
For example, ?????
(duplicate) was segmented as ????
?and ????
(unify) was splitinto ??
??.
This phenomenon iscaused by the imbalanced data distribution.Specifically, characters belonging to categoryword-stem are much less than other threecategories.?
For each segmentation unit in the resultproduced by SVM-based segmenter, checkwhether the unit can be segmented into morethan one IV words and, meanwhile, the wordsexist in a successive form for at least once intraining data .
If true, replace the segmen-tation unit with corresponding continuouslyexisting words.
The intuition underlying thisrule is that SVM-based segmenter tends tocombine a word with some suffix, such as??????
?, two Chinese charactersrepresenting ?person?.
For example, ???
??
(Person in registration) tends to begrouped as a single unit.?
For any sequence in the ambiguity set, suchas ????
?, check if the correct seg-mentation can be determined by the con-text surrounding the sequence.
Without los-ing the generality, in the following explana-tion, we assume each sequence in the am-biguity set has two distinct segmentations.we collected from training data the wordpreceding a sequence where each existenceof the sequence has one of its segmenta-tions, into a collection, named precedingword set, and, correspondingly, the fol-lowing word into another set, which istermed following word set.
Analog-ically, we can produce preceding word218set and following word set for an-other case of segmentation.
When an am-biguous sequence appears in testing data, thesurrounding context (in fact, just one preced-ing word and a following word) is extracted.If the context has overlapping with either ofthe pre-extracted contexts of the same se-quence which are from training data, the seg-mentation corresponding to one of the con-texts is retained.?
More over, we took a look into the annotationerrors existing in training data.
We assumethere unavoidably exist some annotation mis-takes.
For example, in UPENN, the sequence????
(abbreviation for China and Amer-ica) exists, for eighty-seven times, as a wholeword and only one time, exists as??
?
?.We regarded the segmentation??
?
?asan annotation error.
Generally, when the ra-tio of two kinds of segmentations is greaterthan a pre-determined threshold (the value isset seven in our system), the sequence is re-moved from the ambiguity set and added asa word of unique segmentation into the uniq-uity set.2.2 OOV RulesThe following rules are termed OOV rules,since they are utilized to recall some of thewrongly segmented OOV words.
A OOV wordis frequently segmented into two continuous OOVsegmentation units.
For example, the OOVword?????
(Vatican) was frequently seg-mented as ???
?
?, where both ???
?and ??
?are OOV character sequences.Continuous OOVs present a strong clue of po-tential segmentation errors.
A rule is designedto merge some of continuous OOVs into a cor-rect segmentation unit.
The designed rule is ap-plicable to all four corpora.
Moreover, since dis-tinction between different segmentation standardsfrequently leads to very different segmentation ofa same OOV words in different corpora, we de-signed rules particularly for MSRA and UPENNrespectively, to recall more OOVs.?
For two continuous OOVs, check whetherat least one of them is a single-characterword.
If true, group the continuous OOVsinto a segmentation unit.
The reason forthe constraint of at least one of continuousOOVs being single-character word is that notall continuous OOVs should be combined,for example, ???
??
?, both ????
(Germany merchant) and????
(thecompany name) are OOVs, but this sequenceis a valid segmentation unit.
On the otherhand, we assume appropriately that most ofthe cases for character being single-characterword have been covered by training data.That is, once a single character is a OOV seg-mentation unit, there exists a segmentationerror with high possibility.?
MSRA has very different segmentation stan-dard from other three corpora, mainly be-cause it requires to group several continuouswords together into a Name Entity.
For ex-ample, the word???????
(the Min-istry of Foreign Affairs of China) appear-ing in MSRA is generally annotated into twowords in other corpora, as????(China)and?????
(the Ministry of Foreign Af-fairs).
In our system, we first gathered allthe words from the training data whose lengthare greater than six Chinese characters, filter-ing out dates and numbers, which was cov-ered by Finite State Automation asa pre-processing stage.
For each words col-lected, regard the first two and three charac-ters as NE prefix, which indicates the be-ginning of a Name Entity.
The collection ofprefixes is termed Sp(refix).
Analogously, thecollection Ss(uffix) of suffixes is brought upin the same way.
Obviously not all the pre-fixes (suffixes) are good indicators for NameEntities.
Partly inheriting from (Brill, 1995),we applied error-driven learning to filter pre-fixes in Sp and suffixes in Ss.
Specifically,if a prefix and a suffix are both matched ina sequence, all the characters between them,together with the prefix and the suffix, aremerged into a single segmentation unit.
Theresulted unit is compared with correspondingsequence in training data.
If they were not ex-actly matched, the prefix and suffix were re-moved from collections respectively.
Finallyresulted Sp and Ss are utilized to recognizeName Entities in the initial segmentation re-sults.?
UPENN has different segmentation standardfrom other three corpora in that, for some219Corpus R P F ROOV RIVAS 0.949 0.940 0.944 0.694 0.960MSRA 0.955 0.956 0.956 0.650 0.966UPENN 0.940 0.914 0.927 0.634 0.969CITYU 0.965 0.971 0.968 0.719 0.981Table 1: Our official SIGHAN bakeoff resultsLocations, such as ?????
(Beijing) and Organizations, such as ?????
(the Ministry of Foreign Affairs), thelast Chinese character presents a clue thatthe character with high possibility is a suf-fix of some words.
In fact, SVM-based seg-menter sometimes mistakenly split an OOVword into a segmentation unit followed by asuffix.
Thus, when some suffixes exist as asingle-character segmentation unit, it shouldbe grouped with the preceding segmentationunit.
Undoubtedly not all suffixes are appro-priate to this rule.
To gather a clean collec-tion of suffixes, we first clustered together thewords with the same suffix, filtering accord-ing to the number of instances in each clus-ter.
Second, the same as above, error-drivenmethod is utilized to retain effective suffixes.3 Evaluation ResultsWe evaluated the Chinese word segmentationsystem in the close track, on all four cor-pora, namely Academis Sinica (AS), City Uni-versity of Hong Kong (CITYU), Microsoft Re-search (MSRA), and University of Pennsylva-nia/University of Colorado (UPENN).
The resultsare depicted in Table 1, where columns R,P andF refer to Recall, Precision, F measurerespectively, and ROOV , RIV for the recall of out-of-vocabulary words and in-vocabulary words.In addition to final results reported in Bake-off, we also conducted a series of experiments toevaluate the contributions of IV rules and OOVrules.
The experimental results are showed inTable 2, where V1, V2, V3 represent versionsof our segmenters, which compose differently ofcomponents.
In detail, V1 represents the basicSVM-based segmenter; V2 represents the seg-menter which applied IV rules following SVM-based segmentation; V3 represents the segmentercomposing of all the components, that is, includ-ing SVM-based segmenter, IV rules and OOVrules.
Since the OOV ratio is much lower than IVcorrespondence, the improvement made by OOVrules is not so dramatic as IV rules.Corpus V1 v2 v3AS 0.932 0.94 0.944MSRA 0.939 0.954 0.956UPENN 0.914 0.923 0.927CITYU 0.955 0.966 0.968Table 2: Word segmentation accuracy(F Measure)resulted from post-processing rules4 Conclusions and future workWe added post-processing rules to SVM-basedsegmenter.
By doing so, we our segmentation sys-tem achieved comparable results in the close track,on all four corpora.
But on the other hand, post-processing rules have the problems of confliction,which limits the number of rules.
We expect totransform rules into features of SVM-based seg-menter, thus incorporating information carried byrules in a more elaborate manner.AcknowledgementsThis research was supported in part by the Na-tional Natural Science Foundation of China(No.60473140) and by Program for New Century Ex-cellent Talents in University(No.
NCET-05-0287).ReferencesNianwen Xue and Libin Shen.
2003.
Chinese Wordsegmentation as LMR tagging.
In Proceedings ofthe Second SIGHAN Workshop on Chinese Lan-guage Processing,pages 176-179.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Berlin: Springer-Verlag.Jin Kiat Low, Hwee Tou Ng and Wenyuan Guo.
2005.A Maximum Entropy Approach to Chinese WordSegmentation.
In Proceeding of the Fifth SIGHANWorkshop on Chinese Language Processing, pages161-164.Eric.Brill.
1995.
Transformation-based error-drivenlearning and natural language processing:A casestudy in part-of-speech tagging.
Computational Lin-guistics, 21(4):543-565.220
