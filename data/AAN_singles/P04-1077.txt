Automatic Evaluation of Machine Translation Quality Using Longest Com-mon Subsequence and Skip-Bigram StatisticsChin-Yew Lin and Franz Josef OchInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292, USA{cyl,och}@isi.eduAbstractIn this paper we describe two new objectiveautomatic evaluation methods for machinetranslation.
The first method is based on long-est common subsequence between a candidatetranslation and a set of reference translations.Longest common subsequence takes into ac-count sentence level structure similarity natu-rally and identifies longest co-occurring in-sequence n-grams automatically.
The secondmethod relaxes strict n-gram matching to skip-bigram matching.
Skip-bigram is any pair ofwords in their sentence order.
Skip-bigram co-occurrence statistics measure the overlap ofskip-bigrams between a candidate translationand a set of reference translations.
The empiri-cal results show that both methods correlatewith human judgments very well in both ade-quacy and fluency.1 IntroductionUsing objective functions to automatically evalu-ate machine translation quality is not new.
Su et al(1992) proposed a method based on measuring editdistance (Levenshtein 1966) between candidateand reference translations.
Akiba et al (2001) ex-tended the idea to accommodate multiple refer-ences.
Nie?en et al (2000) calculated the length-normalized edit distance, called word error rate(WER), between a candidate and multiple refer-ence translations.
Leusch et al (2003) proposed arelated measure called position-independent worderror rate (PER) that did not consider word posi-tion, i.e.
using bag-of-words instead.
Instead oferror measures, we can also use accuracy measuresthat compute similarity between candidate and ref-erence translations in proportion to the number ofcommon words between them as suggested byMelamed (1995).
An n-gram co-occurrence meas-ure, BLEU, proposed by Papineni et al (2001) thatcalculates co-occurrence statistics based on n-gramoverlaps have shown great potential.
A variant ofBLEU developed by NIST (2002) has been used intwo recent large-scale machine translation evalua-tions.Recently, Turian et al (2003) indicated thatstandard accuracy measures such as recall, preci-sion, and the F-measure can also be used in evalua-tion of machine translation.
However, results basedon their method, General Text Matcher (GTM),showed that unigram F-measure correlated bestwith human judgments while assigning moreweight to higher n-gram (n > 1) matches achievedsimilar performance as Bleu.
Since unigrammatches do not distinguish words in consecutivepositions from words in the wrong order, measuresbased on position-independent unigram matchesare not sensitive to word order and sentence levelstructure.
Therefore, systems optimized for theseunigram-based measures might generate adequatebut not fluent target language.Since BLEU has been used to report the perform-ance of many machine translation systems and ithas been shown to correlate well with humanjudgments, we will explain BLEU in more detailand point out its limitations in the next section.
Wethen introduce a new evaluation method calledROUGE-L that measures sentence-to-sentencesimilarity based on the longest common subse-quence statistics between a candidate translationand a set of reference translations in Section 3.Section 4 describes another automatic evaluationmethod called ROUGE-S that computes skip-bigram co-occurrence statistics.
Section 5 presentsthe evaluation results of ROUGE-L, and ROUGE-S and compare them with BLEU, GTM, NIST,PER, and WER in correlation with human judg-ments in terms of adequacy and fluency.
We con-clude this paper and discuss extensions of thecurrent work in Section 6.2 BLEU and N-gram Co-OccurrenceTo automatically evaluate machine translationsthe machine translation community recentlyadopted an n-gram co-occurrence scoring proce-dure BLEU (Papineni et al 2001).
In two recentlarge-scale machine translation evaluations spon-sored by NIST, a closely related automatic evalua-tion method, simply called NIST score, was used.The NIST (NIST 2002) scoring method is based onBLEU.The main idea of BLEU is to measure the simi-larity between a candidate translation and a set ofreference translations with a numerical metric.They used a weighted average of variable length n-gram matches between system translations and aset of human reference translations and showedthat the weighted average metric correlating highlywith human assessments.BLEU measures how well a machine translationoverlaps with multiple human translations using n-gram co-occurrence statistics.
N-gram precision inBLEU is computed as follows:?
??
??
???
???
?=}{}{)()(CandidatesC CgramnCandidatesC Cgramnclipn gramnCountgramnCountp  (1)Where Countclip(n-gram) is the maximum num-ber of n-grams co-occurring in a candidate transla-tion and a reference translation, and Count(n-gram) is the number of n-grams in the candidatetranslation.
To prevent very short translations thattry to maximize their precision scores, BLEU adds abrevity penalty, BP, to the formula:)2(1|)|/||1( ???????>=?
rcifercifBP crWhere |c| is the length of the candidate transla-tion and |r| is the length of the reference transla-tion.
The BLEU formula is then written as follows:)3(logexp1??????
?= ?=Nnnn pwBPBLEUThe weighting factor, wn, is set at 1/N.Although BLEU has been shown to correlate wellwith human assessments, it has a few things thatcan be improved.
First the subjective application ofthe brevity penalty can be replaced with a recallrelated parameter that is sensitive to referencelength.
Although brevity penalty will penalize can-didate translations with low recall by a factor of e(1-|r|/|c|), it would be nice if we can use the traditionalrecall measure that has been a well known measurein NLP as suggested by Melamed (2003).
Ofcourse we have to make sure the resulting compos-ite function of precision and recall is still correlateshighly with human judgments.Second, although BLEU uses high order n-gram(n>1) matches to favor candidate sentences withconsecutive word matches and to estimate theirfluency, it does not consider sentence level struc-ture.
For example, given the following sentences:S1.
police killed the gunmanS2.
police kill the gunman1S3.
the gunman kill policeWe only consider BLEU with unigram and bi-gram, i.e.
N=2, for the purpose of explanation andcall this BLEU-2.
Using S1 as the reference and S2and S3 as the candidate translations, S2 and S3would have the same BLEU-2 score, since theyboth have one bigram and three unigram matches2.However, S2 and S3 have very different meanings.Third, BLEU is a geometric mean of unigram toN-gram precisions.
Any candidate translationwithout a N-gram match has a per-sentence BLEUscore of zero.
Although BLEU is usually calculatedover the whole test corpus, it is still desirable tohave a measure that works reliably at sentencelevel for diagnostic and introspection purpose.To address these issues, we propose three newautomatic evaluation measures based on longestcommon subsequence statistics and skip bigramco-occurrence statistics in the following sections.3 Longest Common Subsequence3.1 ROUGE-LA sequence Z = [z1, z2, ..., zn] is a subsequence ofanother sequence X = [x1, x2, ..., xm], if there existsa strict increasing sequence [i1, i2, ..., ik] of indicesof X such that for all j = 1, 2, ..., k, we have xij = zj(Cormen et al 1989).
Given two sequences X andY, the longest common subsequence (LCS) of Xand Y is a common subsequence with maximumlength.
We can find the LCS of two sequences oflength m and n using standard dynamic program-ming technique in O(mn) time.LCS has been used to identify cognate candi-dates during construction of N-best translationlexicons from parallel text.
Melamed (1995) usedthe ratio (LCSR) between the length of the LCS oftwo words and the length of the longer word of thetwo words to measure the cognateness betweenthem.
He used as an approximate string matchingalgorithm.
Saggion et al (2002) used normalizedpairwise LCS (NP-LCS) to compare similarity be-tween two texts in automatic summarizationevaluation.
NP-LCS can be shown as a special caseof Equation (6) with ?
= 1.
However, they did notprovide the correlation analysis of NP-LCS with1 This is a real machine translation output.2 The ?kill?
in S2 or S3 does not match with ?killed?
inS1 in strict word-to-word comparison.human judgments and its effectiveness as an auto-matic evaluation measure.To apply LCS in machine translation evaluation,we view a translation as a sequence of words.
Theintuition is that the longer the LCS of two transla-tions is, the more similar the two translations are.We propose using LCS-based F-measure to esti-mate the similarity between two translations X oflength m and Y of length n, assuming X is a refer-ence translation and Y is a candidate translation, asfollows:RlcsmYXLCS ),(=       (4)PlcsnYXLCS ),(=       (5)FlcslcslcslcslcsPRPR22 )1(?
?++=   (6)Where LCS(X,Y) is the length of a longest commonsubsequence of X and Y, and ?
= Plcs/Rlcs when?Flcs/?Rlcs_=_?Flcs/?Plcs.
We call the LCS-based F-measure, i.e.
Equation 6, ROUGE-L. Notice thatROUGE-L is 1 when X = Y since LCS(X,Y) = m orn; while ROUGE-L is zero when LCS(X,Y) = 0, i.e.there is nothing in common between X and Y. F-measure or its equivalents has been shown to havemet several theoretical criteria in measuring accu-racy involving more than one factor (Van Rijsber-gen 1979).
The composite factors are LCS-basedrecall and precision in this case.
Melamed et al(2003) used unigram F-measure to estimate ma-chine translation quality and showed that unigramF-measure was as good as BLEU.One advantage of using LCS is that it does notrequire consecutive matches but in-sequencematches that reflect sentence level word order as n-grams.
The other advantage is that it automaticallyincludes longest in-sequence common n-grams,therefore no predefined n-gram length is necessary.ROUGE-L as defined in Equation 6 has the prop-erty that its value is less than or equal to the mini-mum of unigram F-measure of X and Y. Unigramrecall reflects the proportion of words in X (refer-ence translation) that are also present in Y (candi-date translation); while unigram precision is theproportion of words in Y that are also in X. Uni-gram recall and precision count all co-occurringwords regardless their orders; while ROUGE-Lcounts only in-sequence co-occurrences.By only awarding credit to in-sequence unigrammatches, ROUGE-L also captures sentence levelstructure in a natural way.
Consider again the ex-ample given in Section 2 that is copied here forconvenience:S1.
police killed the gunmanS2.
police kill the gunmanS3.
the gunman kill policeAs we have shown earlier, BLEU-2 cannot differ-entiate S2 from S3.
However, S2 has a ROUGE-Lscore of 3/4 = 0.75 and S3 has a ROUGE-L scoreof 2/4 = 0.5, with ?
= 1.
Therefore S2 is better thanS3 according to ROUGE-L.
This example also il-lustrated that ROUGE-L can work reliably at sen-tence level.However, LCS only counts the main in-sequencewords; therefore, other longest common subse-quences and shorter sequences are not reflected inthe final score.
For example, consider the follow-ing candidate sentence:S4.
the gunman police killedUsing S1 as its reference, LCS counts either ?thegunman?
or ?police killed?, but not both; therefore,S4 has the same ROUGE-L score as S3.
BLEU-2would prefer S4 over S3.
In Section 4, we will in-troduce skip-bigram co-occurrence statistics thatdo not have this problem while still keeping theadvantage of in-sequence (not necessary consecu-tive) matching that reflects sentence level wordorder.3.2 Multiple ReferencesSo far, we only demonstrated how to computeROUGE-L using a single reference.
When multiplereferences are used, we take the maximum LCSmatches between a candidate translation, c, of nwords and a set of u reference translations of  mjwords.
The LCS-based F-measure can becomputed as follows:Rlcs-multi ???????
?==jjuj mcrLCS ),(max 1       (7)Plcs-multi ???????
?== ncrLCS juj),(max 1       (8)Flcs-multimultilcsmultilcsmultilcsmultilcsPRPR???
?++= 22 )1(??
(9)where ?
= Plcs-multi/Rlcs-multi when ?Flcs-multi/?Rlcs-multi_=_?Flcs-multi/?Plcs-multi.This procedure is also applied to computation ofROUGE-S when multiple references are used.
Inthe next section, we introduce the skip-bigram co-occurrence statistics.
In the next section, we de-scribe how to extend ROUGE-L to assign morecredits to longest common subsequences with con-secutive words.3.3 ROUGE-W: Weighted Longest CommonSubsequenceLCS has many nice properties as we have de-scribed in the previous sections.
Unfortunately, thebasic LCS also has a problem that it does not dif-ferentiate LCSes of different spatial relationswithin their embedding sequences.
For example,given a reference sequence X and two candidatesequences Y1 and Y2 as follows:X:  [A B C D E F G]Y1: [A B C D H I K]Y2:  [A H B K C I D]Y1 and Y2 have the same ROUGE-L score.
How-ever, in this case, Y1 should be the better choicethan Y2 because Y1 has consecutive matches.
Toimprove the basic LCS method, we can simply re-member the length of consecutive matches encoun-tered so far to a regular two dimensional dynamicprogram table computing LCS.
We call thisweighted LCS (WLCS) and use k to indicate thelength of the current consecutive matches ending atwords xi and yj.
Given two sentences X and Y, theWLCS score of X and Y can be computed using thefollowing dynamic programming procedure:(1) For (i = 0; i <=m; i++)c(i,j) = 0  // initialize c-tablew(i,j) = 0 // initialize w-table(2) For (i = 1; i <= m; i++)For (j = 1; j <= n; j++)If xi = yj Then// the length of consecutive matches at// position i-1 and j-1k = w(i-1,j-1)c(i,j) = c(i-1,j-1) + f(k+1) ?
f(k)// remember the length of consecutive// matches at position i, jw(i,j) = k+1OtherwiseIf c(i-1,j) > c(i,j-1) Thenc(i,j) = c(i-1,j)w(i,j) = 0           // no match at i, jElse c(i,j) = c(i,j-1)w(i,j) = 0           // no match at i, j(3) WLCS(X,Y) = c(m,n)Where c is the dynamic programming table, c(i,j)stores the WLCS score ending at word xi of X andyj of Y, w is the table storing the length of consecu-tive matches ended at c table position i and j, and fis a function of consecutive matches at the tableposition, c(i,j).
Notice that by providing differentweighting function f, we can parameterize theWLCS algorithm to assign different credit to con-secutive in-sequence matches.The weighting function f must have the propertythat f(x+y) > f(x) + f(y) for any positive integers xand y.
In other words, consecutive matches areawarded more scores than non-consecutivematches.
For example, f(k)-=-?k ?
?
when k >= 0,and ?, ?
> 0.
This function charges a gap penaltyof ??
for each non-consecutive n-gram sequences.Another possible function family is the polynomialfamily of the form k?
where -?
> 1.
However, inorder to normalize the final ROUGE-W score, wealso prefer to have a function that has a close forminverse function.
For example, f(k)-=-k2 has a closeform inverse function f -1(k)-=-k1/2.
F-measurebased on WLCS can be computed as follows,given two sequences X of length m and Y of lengthn:Rwlcs ????????=?
)(),(1mfYXWLCSf       (10)Pwlcs ????????=?
)(),(1nfYXWLCSf       (11)FwlcswlcswlcswlcswlcsPRPR22 )1(?
?++=           (12)Where f -1 is the inverse function of f. We call theWLCS-based F-measure, i.e.
Equation 12,ROUGE-W.
Using Equation 12 and f(k)-=-k2 as theweighting function, the ROUGE-W scores for se-quences Y1 and Y2 are 0.571 and 0.286 respec-tively.
Therefore, Y1 would be ranked higher thanY2 using WLCS.
We use the polynomial functionof the form k?
in the ROUGE evaluation package.
Inthe next section, we introduce the skip-bigram co-occurrence statistics.4 ROUGE-S: Skip-Bigram Co-OccurrenceStatisticsSkip-bigram is any pair of words in their sen-tence order, allowing for arbitrary gaps.
Skip-bigram co-occurrence statistics measure the over-lap of skip-bigrams between a candidate translationand a set of reference translations.
Using the ex-ample given in Section 3.1:S1.
police killed the gunmanS2.
police kill the gunmanS3.
the gunman kill policeS4.
the gunman police killedEach sentence has C(4,2)3 = 6 skip-bigrams.
Forexample, S1 has the following skip-bigrams:3 Combination: C(4,2) = 4!/(2!*2!)
= 6.
(?police killed?, ?police the?, ?police gunman?,?killed the?, ?killed gunman?, ?the gunman?
)S2 has three skip-bigram matches with S1 (?po-lice the?, ?police gunman?, ?the gunman?
), S3 hasone skip-bigram match with S1 (?the gunman?
),and S4 has two skip-bigram matches with S1 (?po-lice killed?, ?the gunman?).
Given translations Xof length m and Y of length n, assuming X is a ref-erence translation and Y is a candidate translation,we compute skip-bigram-based F-measure as fol-lows:Rskip2 )2,(),(2mCYXSKIP=           (13)Pskip2 )2,(),(2nCYXSKIP=           (14)Fskip2222222 )1(skipskipskipskipPRPR?
?++=   (15)Where SKIP2(X,Y) is the number of skip-bigrammatches between X and Y, ?
= Pskip2/Rskip2 when?Fskip2/?Rskip2_=_?Fskip2/?Pskip2, and  C is the combi-nation function.
We call the skip-bigram-based F-measure, i.e.
Equation 15, ROUGE-S.Using Equation 15 with ?
= 1 and S1 as the ref-erence, S2?s ROUGE-S score is 0.5, S3 is 0.167,and S4 is 0.333.
Therefore, S2 is better than S3 andS4, and S4 is better than S3.
This result is moreintuitive than using BLEU-2 and ROUGE-L. Oneadvantage of skip-bigram vs. BLEU is that it doesnot require consecutive matches but is still sensi-tive to word order.
Comparing skip-bigram withLCS, skip-bigram counts all in-order matchingword pairs while LCS only counts one longestcommon subsequence.We can limit the maximum skip distance, dskip,between two in-order words that is allowed to forma skip-bigram.
Applying such constraint, we limitskip-bigram formation to a fix window size.
There-fore, computation time can be reduced and hope-fully performance can be as good as the versionwithout such constraint.
For example, if we set dskipto 0 then ROUGE-S is equivalent to bigram over-lap.
If we set dskip to 4 then only word pairs of atmost 4 words apart can form skip-bigrams.Adjusting Equations 13, 14, and 15 to use maxi-mum skip distance limit is straightforward: weonly count the skip-bigram matches, SKIP2(X,Y),within the maximum skip distance and replace de-nominators of Equations 13, C(m,2), and 14,C(n,2), with the actual numbers of within distanceskip-bigrams from the reference and the candidaterespectively.In the next section, we present the evaluations ofROUGE-L, ROUGE-S, and compare their per-formance with other automatic evaluation meas-ures.5 EvaluationsOne of the goals of developing automatic evalua-tion measures is to replace labor-intensive humanevaluations.
Therefore the first criterion to assessthe usefulness of an automatic evaluation measureis to show that it correlates highly with humanjudgments in different evaluation settings.
How-ever, high quality large-scale human judgments arehard to come by.
Fortunately, we have access toeight MT systems?
outputs, their human assess-ment data, and the reference translations from 2003NIST Chinese MT evaluation (NIST 2002a).
Therewere 919 sentence segments in the corpus.
We firstcomputed averages of the adequacy and fluencyscores of each system assigned by human evalua-tors.
For the input of automatic evaluation meth-ods, we created three evaluation sets from the MToutputs:1.
Case set: The original system outputs withcase information.2.
NoCase set: All words were convertedinto lower case, i.e.
no case informationwas used.
This set was used to examinewhether human assessments were affectedby case information since not all MT sys-tems generate properly cased output.3.
Stem set: All words were converted intolower case and stemmed using the Porterstemmer (Porter 1980).
Since ROUGEcomputed similarity on surface wordlevel, stemmed version allowed ROUGEto perform more lenient matches.To accommodate multiple references, we use aJackknifing procedure.
Given N references, wecompute the best score over N sets of N-1 refer-ences.
The final score is the average of the N bestscores using N different sets of N-1 references.The Jackknifing procedure is adopted since weoften need to compare system and human perform-ance and the reference translations are usually theonly human translations available.
Using this pro-cedure, we are able to estimate average human per-formance by averaging N best scores of onereference vs. the rest N-1 references.We then computed average BLEU1-12 4 , GTMwith exponents of 1.0, 2.0, and 3.0, NIST, WER,and PER scores over these three sets.
Finally weapplied ROUGE-L, ROUGE-W with weightingfunction k1.2, and ROUGE-S without skip distance4 BLEUN computes BLEU over n-grams up to length N.Only BLEU1, BLEU4, and BLEU12 are shown in Table 1.limit and with skip distant limits of 0, 4, and 9.Correlation analysis based on two different correla-tion statistics, Pearson?s ?
and Spearman?s ?, withrespect to adequacy and fluency are shown in Ta-ble 1.The Pearson?s correlation coefficient5 measures thestrength and direction of a linear relationship be-tween any two variables, i.e.
automatic metricscore and human assigned mean coverage score inour case.
It ranges from +1 to -1.
A correlation of 1means that there is a perfect positive linear rela-tionship between the two variables, a correlation of-1 means that there is a perfect negative linear rela-tionship between them, and  a correlation of 0means that there is no linear relationship betweenthem.
Since we would like to use automaticevaluation metric not only in comparing systems5 For a quick overview of the Pearson?s coefficient, see:http://davidmlane.com/hyperstat/A34739.html.but also in in-house system development, a goodlinear correlation with human judgment would en-able us to use automatic scores to predict corre-sponding human judgment scores.
Therefore,Pearson?s correlation coefficient is a good measureto look at.Spearman?s correlation coefficient 6  is also ameasure of correlation between two variables.
It isa non-parametric measure and is a special case ofthe Pearson?s correlation coefficient when the val-ues of data are converted into ranks before comput-ing the coefficient.
Spearman?s correlationcoefficient does not assume the correlation be-tween the variables is linear.
Therefore it is a use-ful correlation indicator even when good linearcorrelation, for example, according to Pearson?scorrelation coefficient between two variables could6 For a quick overview of the Spearman?s coefficient, see:http://davidmlane.com/hyperstat/A62436.html.AdequacyMethod P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%UBLEU1 0.86 0.83 0.89 0.80 0.71 0.90 0.87 0.84 0.90 0.76 0.67 0.89 0.91 0.89 0.93 0.85 0.76 0.95BLEU4 0.77 0.72 0.81 0.77 0.71 0.89 0.79 0.75 0.82 0.67 0.55 0.83 0.82 0.78 0.85 0.76 0.67 0.89BLEU12 0.66 0.60 0.72 0.53 0.44 0.65 0.72 0.57 0.81 0.65 0.25 0.88 0.72 0.58 0.81 0.66 0.28 0.88NIST 0.89 0.86 0.92 0.78 0.71 0.89 0.87 0.85 0.90 0.80 0.74 0.92 0.90 0.88 0.93 0.88 0.83 0.97WER 0.47 0.41 0.53 0.56 0.45 0.74 0.43 0.37 0.49 0.66 0.60 0.82 0.48 0.42 0.54 0.66 0.60 0.81PER 0.67 0.62 0.72 0.56 0.48 0.75 0.63 0.58 0.68 0.67 0.60 0.83 0.72 0.68 0.76 0.69 0.62 0.86ROUGE-L 0.87 0.84 0.90 0.84 0.79 0.93 0.89 0.86 0.92 0.84 0.71 0.94 0.92 0.90 0.94 0.87 0.76 0.95ROUGE-W 0.84 0.81 0.87 0.83 0.74 0.90 0.85 0.82 0.88 0.77 0.67 0.90 0.89 0.86 0.91 0.86 0.76 0.95ROUGE-S* 0.85 0.81 0.88 0.83 0.76 0.90 0.90 0.88 0.93 0.82 0.70 0.92 0.95 0.93 0.97 0.85 0.76 0.94ROUGE-S0 0.82 0.78 0.85 0.82 0.71 0.90 0.84 0.81 0.87 0.76 0.67 0.90 0.87 0.84 0.90 0.82 0.68 0.90ROUGE-S4 0.82 0.78 0.85 0.84 0.79 0.93 0.87 0.85 0.90 0.83 0.71 0.90 0.92 0.90 0.94 0.84 0.74 0.93ROUGE-S9 0.84 0.80 0.87 0.84 0.79 0.92 0.89 0.86 0.92 0.84 0.76 0.93 0.94 0.92 0.96 0.84 0.76 0.94GTM10 0.82 0.79 0.85 0.79 0.74 0.83 0.91 0.89 0.94 0.84 0.79 0.93 0.94 0.92 0.96 0.84 0.79 0.92GTM20 0.77 0.73 0.81 0.76 0.69 0.88 0.79 0.76 0.83 0.70 0.55 0.83 0.83 0.79 0.86 0.80 0.67 0.90GTM30 0.74 0.70 0.78 0.73 0.60 0.86 0.74 0.70 0.78 0.63 0.52 0.79 0.77 0.73 0.81 0.64 0.52 0.80FluencyMethod P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%U P 95%L 95%U S 95%L 95%UBLEU1 0.81 0.75 0.86 0.76 0.62 0.90 0.73 0.67 0.79 0.70 0.62 0.81 0.70 0.63 0.77 0.79 0.67 0.90BLEU4 0.86 0.81 0.90 0.74 0.62 0.86 0.83 0.78 0.88 0.68 0.60 0.81 0.83 0.78 0.88 0.70 0.62 0.81BLEU12 0.87 0.76 0.93 0.66 0.33 0.79 0.93 0.81 0.97 0.78 0.44 0.94 0.93 0.84 0.97 0.80 0.49 0.94NIST 0.81 0.75 0.87 0.74 0.62 0.86 0.70 0.64 0.77 0.68 0.60 0.79 0.68 0.61 0.75 0.77 0.67 0.88WER 0.69 0.62 0.75 0.68 0.57 0.85 0.59 0.51 0.66 0.70 0.57 0.82 0.60 0.52 0.68 0.69 0.57 0.81PER 0.79 0.74 0.85 0.67 0.57 0.82 0.68 0.60 0.73 0.69 0.60 0.81 0.70 0.63 0.76 0.65 0.57 0.79ROUGE-L 0.83 0.77 0.88 0.80 0.67 0.90 0.76 0.69 0.82 0.79 0.64 0.90 0.73 0.66 0.80 0.78 0.67 0.90ROUGE-W 0.85 0.80 0.90 0.79 0.63 0.90 0.78 0.73 0.84 0.72 0.62 0.83 0.77 0.71 0.83 0.78 0.67 0.90ROUGE-S* 0.84 0.78 0.89 0.79 0.62 0.90 0.80 0.74 0.86 0.77 0.64 0.90 0.78 0.71 0.84 0.79 0.69 0.90ROUGE-S0 0.87 0.81 0.91 0.78 0.62 0.90 0.83 0.78 0.88 0.71 0.62 0.82 0.82 0.77 0.88 0.76 0.62 0.90ROUGE-S4 0.84 0.79 0.89 0.80 0.67 0.90 0.82 0.77 0.87 0.78 0.64 0.90 0.81 0.75 0.86 0.79 0.67 0.90ROUGE-S9 0.84 0.79 0.89 0.80 0.67 0.90 0.81 0.76 0.87 0.79 0.69 0.90 0.79 0.73 0.85 0.79 0.69 0.90GTM10 0.73 0.66 0.79 0.76 0.60 0.87 0.71 0.64 0.78 0.80 0.67 0.90 0.66 0.58 0.74 0.80 0.64 0.90GTM20 0.86 0.81 0.90 0.80 0.67 0.90 0.83 0.77 0.88 0.69 0.62 0.81 0.83 0.77 0.87 0.74 0.62 0.89GTM30 0.87 0.81 0.91 0.79 0.67 0.90 0.83 0.77 0.87 0.73 0.62 0.83 0.83 0.77 0.88 0.71 0.60 0.83With Case Information (Case) Lower Case (NoCase) Lower Case & Stemmed (Stem)With Case Information (Case) Lower Case (NoCase) Lower Case & Stemmed (Stem)Table 1.
Pearson?s ?
and Spearman?s ?
correlations of automatic evaluation measures vs. adequacyand fluency: BLEU1, 4, and 12 are BLEU with maximum of 1, 4, and 12 grams, NIST is the NISTscore, ROUGE-L is LCS-based F-measure (?
= 1), ROUGE-W is weighted LCS-based  F-measure (?= 1).
ROUGE-S* is skip-bigram-based co-occurrence statistics with any skip distance limit, ROUGE-SN is skip-bigram-based F-measure (?
= 1) with maximum skip distance of N, PER is position inde-pendent word error rate, and WER is word error rate.
GTM 10, 20, and 30 are general text matcherwith exponents of 1.0, 2.0, and 3.0.
(Note, only BLEU1, 4, and 12 are shown here to preserve space.
)not be found.
It also suits the NIST MT evaluationscenario where multiple systems are ranked ac-cording to some performance metrics.To estimate the significance of these correlationstatistics, we applied bootstrap resampling, gener-ating random samples of the 919 different sentencesegments.
The lower and upper values of 95% con-fidence interval are also shown in the table.
Dark(green) cells are the best correlation numbers intheir categories and light gray cells are statisticallyequivalent to the best numbers in their categories.Analyzing all runs according to the adequacy andfluency table, we make the following observations:Applying the stemmer achieves higher correla-tion with adequacy but keeping case informationachieves higher correlation with fluency except forBLEU7-12 (only BLEU12 is shown).
For example,the Pearson?s ?
(P) correlation of ROUGE-S* withadequacy increases from 0.85 (Case) to 0.95(Stem) while its Pearson?s ?
correlation with flu-ency drops from 0.84 (Case) to 0.78 (Stem).
Wewill focus our discussions on the Stem set in ade-quacy and Case set in fluency.The Pearson's ?
correlation values in the Stemset of the Adequacy Table, indicates that ROUGE-L and ROUGE-S with a skip distance longer than 0correlate highly and linearly with adequacy andoutperform BLEU and NIST.
ROUGE-S* achievesthat best correlation with a Pearson?s ?
of 0.95.Measures favoring consecutive matches, i.e.BLEU4 and 12, ROUGE-W, GTM20 and 30,ROUGE-S0 (bigram), and WER have lower Pear-son?s ?.
Among them WER (0.48) that tends topenalize small word movement is the worst per-former.
One interesting observation is that longerBLEU has lower correlation with adequacy.Spearman?s ?
values generally agree with Pear-son's ?
but have more equivalents.The Pearson's ?
correlation values in the Stemset of the Fluency Table, indicates that BLEU12 hasthe highest correlation (0.93) with fluency.
How-ever, it is statistically indistinguishable with 95%confidence from all other metrics shown in theCase set of the Fluency Table except for WER andGTM10.GTM10 has good correlation with human judg-ments in adequacy but not fluency; while GTM20and GTM30, i.e.
GTM with exponent larger than1.0, has good correlation with human judgment influency but not adequacy.ROUGE-L and ROUGE-S*, 4, and 9 are goodautomatic evaluation metric candidates since theyperform as well as BLEU in fluency correlationanalysis and outperform BLEU4 and 12 signifi-cantly in adequacy.
Among them, ROUGE-L is thebest metric in both adequacy and fluency correla-tion with human judgment according to Spear-man?s correlation coefficient and is statisticallyindistinguishable from the best metrics in bothadequacy and fluency correlation with humanjudgment according to Pearson?s correlation coef-ficient.6 ConclusionIn this paper we presented two new objectiveautomatic evaluation methods for machine transla-tion, ROUGE-L based on longest common subse-quence (LCS) statistics between a candidatetranslation and a set of reference translations.Longest common subsequence takes into accountsentence level structure similarity naturally andidentifies longest co-occurring in-sequence n-grams automatically while this is a free parameterin BLEU.To give proper credit to shorter common se-quences that are ignored by LCS but still retain theflexibility of non-consecutive matches, we pro-posed counting skip bigram co-occurrence.
Theskip-bigram-based ROUGE-S* (without skip dis-tance restriction) had the best Pearson's ?
correla-tion of 0.95 in adequacy when all words werelower case and stemmed.
ROUGE-L, ROUGE-W,ROUGE-S*, ROUGE-S4, and ROUGE-S9 wereequal performers to BLEU in measuring fluency.However, they have the advantage that we can ap-ply them on sentence level while longer BLEU suchas BLEU12 would not differentiate any sentenceswith length shorter than 12 words (i.e.
no 12-grammatches).
We plan to explore their correlation withhuman judgments on sentence-level in the future.We also confirmed empirically that adequacy andfluency focused on different aspects of machinetranslations.
Adequacy placed more emphasis onterms co-occurred in candidate and reference trans-lations as shown in the higher correlations in Stemset than Case set in Table 1; while the reverse wastrue in the terms of fluency.The evaluation results of ROUGE-L, ROUGE-W, and ROUGE-S in machine translation evalua-tion are very encouraging.
However, these meas-ures in their current forms are still only applyingstring-to-string matching.
We have shown that bet-ter correlation with adequacy can be reached byapplying stemmer.
In the next step, we plan to ex-tend them to accommodate synonyms and para-phrases.
For example, we can use an existingthesaurus such as WordNet (Miller 1990) or creat-ing a customized one by applying automated syno-nym set discovery methods (Pantel and Lin 2002)to identify potential synonyms.
Paraphrases canalso be automatically acquired using statisticalmethods as shown by Barzilay and Lee (2003).Once we have acquired synonym and paraphrasedata, we then need to design a soft matching func-tion that assigns partial credits to these approxi-mate matches.
In this scenario, statisticallygenerated data has the advantage of being able toprovide scores reflecting the strength of similaritybetween synonyms and paraphrased.ROUGE-L, ROUGE-W, and ROUGE-S havealso been applied in automatic evaluation of sum-marization and achieved very promising results(Lin 2004).
In Lin and Och (2004), we proposed aframework that automatically evaluated automaticMT evaluation metrics using only manual transla-tions without further human involvement.
Accord-ing to the results reported in that paper, ROUGE-L,ROUGE-W, and ROUGE-S also outperformedBLEU and NIST.ReferencesAkiba, Y., K. Imamura, and E. Sumita.
2001.
Us-ing Multiple Edit Distances to AutomaticallyRank Machine Translation Output.
In Proceed-ings of the MT Summit VIII, Santiago de Com-postela, Spain.Barzilay, R. and L. Lee.
2003.
Learning to Para-phrase: An Unsupervised Approach Using Mul-tiple-Sequence Alignmen.
In Proceeding ofNAACL-HLT 2003, Edmonton, Canada.Leusch, G., N. Ueffing, and H. Ney.
2003.
ANovel String-to-String Distance Measure withApplications to Machine Translation Evaluation.In Proceedings of MT Summit IX, New Orleans,U.S.A.Levenshtein, V. I.
1966.
Binary codes capable ofcorrecting deletions, insertions and reversals.Soviet Physics Doklady.Lin, C.Y.
2004.
ROUGE: A Package for AutomaticEvaluation of Summaries.
In Proceedings of theWorkshop on Text Summarization BranchesOut, post-conference workshop of ACL 2004,Barcelona, Spain.Lin, C.-Y.
and F. J. Och.
2004.
ORANGE: a Methodfor Evaluating Automatic Evaluation Metrics forMachine Translation.
In Proceedings of 20th In-ternational Conference on Computational Lin-guistic (COLING 2004), Geneva, Switzerland.Miller, G. 1990.
WordNet: An Online Lexical Da-tabase.
International Journal of Lexicography,3(4).Melamed, I.D.
1995.
Automatic Evaluation andUniform Filter Cascades for Inducing N-bestTranslation Lexicons.
In Proceedings of the 3rdWorkshop on Very Large Corpora (WVLC3).Boston, U.S.A.Melamed, I.D., R. Green and J. P. Turian.
2003.Precision and Recall of Machine Translation.
InProceedings of NAACL/HLT 2003, Edmonton,Canada.Nie?en S., F.J. Och, G, Leusch, H. Ney.
2000.
AnEvaluation Tool for Machine Translation: FastEvaluation for MT Research.
In Proceedings ofthe 2nd International Conference on LanguageResources and Evaluation, Athens, Greece.NIST.
2002.
Automatic Evaluation of MachineTranslation Quality using N-gram Co-Occurrence Statistics.
AAAAAAAAAAAhttp://www.nist.gov/speech/tests/mt/doc/ngram-study.pdfPantel, P. and Lin, D. 2002.
Discovering WordSenses from Text.
In Proceedings of SIGKDD-02.
Edmonton, Canada.Papineni, K., S. Roukos, T. Ward, and W.-J.
Zhu.2001.
BLEU: a Method for Automatic Evaluationof Machine Translation.
IBM Research ReportRC22176 (W0109-022).Porter, M.F.
1980.
An Algorithm for Suffix Strip-ping.
Program, 14, pp.
130-137.Saggion H., D. Radev, S. Teufel, and W. Lam.2002.
Meta-Evaluation of Summaries in aCross-Lingual Environment Using Content-Based Metrics.
In Proceedings of COLING-2002, Taipei, Taiwan.Su, K.-Y., M.-W. Wu, and J.-S. Chang.
1992.
ANew Quantitative Quality Measure for MachineTranslation System.
In Proceedings ofCOLING-92, Nantes, France.Thompson, H. S. 1991.
Automatic Evaluation ofTranslation Quality: Outline of Methodologyand Report on Pilot Experiment.
In Proceedingsof the Evaluator?s Forum, ISSCO, Geneva,Switzerland.Turian, J. P., L. Shen, and I. D. Melamed.
2003.Evaluation of Machine Translation and itsEvaluation.
In Proceedings of MT Summit IX,New Orleans, U.S.A.Van Rijsbergen, C.J.
1979.
Information Retrieval.Butterworths.
London.
