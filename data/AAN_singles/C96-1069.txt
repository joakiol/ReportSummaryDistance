An Automat ic  C lus ter ing  of  Ar t i c les  Us ing  D ic t ionaryDef in i t ionsFumiyo FUKUMOTO Yoshimi SUZUKI~Dept.
of Electrical Engineering a,nd Computer Science, Yalnanashi University4-3-11 T~keda, Kofu 4(10 ,b~pan{fukumotoOskye, ysuzuki~suwaj }.
es i .
yamanashi, ac .
jpAbstractIn this paper, we propose a statisticalapproach for clustering of artMes us-ing on-line dictionary definitions.
Oneof the characteristics of our approach isthat every sense of word in artMes is au-tomatically disambiguated using dictio-nary definitions.
The other is that in or-der to cope with the problem of a phrasallexicon, linking which links words withtheir semantically similar words in arti-cles is introduced in our method.
Theresults of experiments demonstrate theeffectiveness of the proposed method.1 IntroductionThere has been quite a lot of research con-cerned with automatic clustering of articles orautonmtic identification of selnantically similararticles(Walker, 1986), (Guthrie, 1994), (Yuasa,1995).
Most of these works deal with entirely di fferent articles.In general, the 1)rot)l(m: that the same word ca:,be used differently in different sul)jeet domains isless problematic in entirely ditferent artMes, suchas 'weather forecasts', 'medical rel)orts' , and 'com-puter manuals'.
Because these articles are charac-terised by a larger number of different words thanthat of the same words.
However, in texts froma restricted omain such as financial artMes, e.gWall Street Journal (WS,I in short) (Libernmn,1990), one encounters quite a large number of popysemous words.
Therefore, polyseinous words of_ten hamper the I)recise cla~ssification of artMes,each of which belongs to the restricted subject do-nlaiII.In this paper, we report an experimental studyfor clustering of articles by using on-line dic-tionary definitions attd show how dictionary-definition can use effectively to classify articles,each of which belongs to the restricted subject do-main.
We first describe a method for disambiguat-ing word-senses in articles based on dictionary def-initions.
Then, we present a method for classifyingarticles and finally, we rel)ort some ext)eriments inorder to show the effect of the method.2 Related WorkOne of major approaches in automatic lusteringof articles is based on statistical information ofwords in ,~rticles.
Every article is characterisedby a vector, each dimension of which is associatedwith a specific word in articles, and every coordi-nate of the artMe is represented by tern: weight-ing.
Tern1 weighting methods have been widelystudied in iufornmtion retrieval research (Salton,1983), (Jones, 1972) and some of then: are usedin an automatic clustering of articles.
Guthrieand Yuasa used word frequencies for weighting(Guthrie, 1994), (Yuasa, 1995), and Tokunagaused weighted inverse document frequency whichis a word frequency within the document dividedby its fl'equency throughout he entire documentcollection (Tokunaga, 1994).
The results of thesemethods when al)plied to articles' cbussificationtask, seem to show its etfectiveness.
However,these works do not seriously deal with the 1)roblemof polysemy.The alternative al)l)roach is based on dictio-nary's infl)rlnation as a thesaurus.
One of majorproblems using thesaurus ('ategories a.s sense rep-rese::tation is a statistical sparseness for thesauruswords, since they are nmstly rather uncommonwords (Niwa, 1995).
Yuasa reported the exper-imental results when using word frequencies forweighting within large documents were better re-suits in clustering (lo('unmnts as those when EDRelectronic dictionary as a thesaurus (Yuasa, 1995).The technique developed by Walker also used(lietionary's infornmtion and seems to cope withthe discrimination of polysemy (Walker, 1986).He used the semantic odes of the Longmau Dic-tionary of Contemporary English in order to de-termine the subject donmin for a set of texts.
Fora given text, each word is checked against he dic-tionary to determine the semantic codes associ-ate(l with it.
By accumulating the frequencies forthese senses and then ordering the list, of cate-gories in terms of frequency, the subject matter of406the text can be identified.
However, ~us he admits,a phrasal lexicon, such as Atlantic Seaboard, NewEngland gives a negative influence for clustering,since it can not be regarded ~us units, i.e.
eachword which is the element of a 1)hrasal exicon isassigned to each semantic ode.The approach proposed in this paper focuseson these l)roblems, i.e.
1)olysemy and a phrasallexicon.
Like Guthrie and Yuasa's methods, ourapproach adopts a vector representation, i.e.
ev-ery article is characterised by a vector.
However~while their ~pproaehes assign each (:oor(linate ofa vector to each word in artMes, we use a word(noun) of wtfich sense is disambiguated.
Our dis-ambiguation method of word-senses is based onNiwa's method whMt use(l the similarit;y 1)etweentwo sentences, i.e.
a sentevee which contains apolysenmus noun and a sevtenee of dictionary-definition.
In order to cope with Walker's l ) rob -l em,  for the results of disand)iguation technique,semantic relativeness of words are cMeulated, andsemantically related words are grout)ed together.We used WSJ corpus as test artich,s in the ex-periments in order to see how our metho(l caneffectively classify artMes, eacl, <)f whi<:h beh)ngste the restricted subject domain, i.e.
WS.I.3 Framework3.1  Word-Sense  D isambiguat ionEvery sense of words in artMes which shouldbe (:lustered is automatical ly disambiguated inadvance.
Word-sense dismnl)iguation (WSD inshort) is a serious problem for NLP, and a wlri('tyof al)l)roaches have been 1)roposed for solving it(Ih'own, 1991), (Yarowsky, 1992).Our disalnbiguation method is based on Niwa'smethod which used the similarity 1)etween a sen-tenee containing a t)olysemous noun and a sen=tence of dictionary-definition.
Let x be a t)olyse-mous noun and a sentence X beX "  ?
?
?
~ 3 : -n~ ?
?
?
~ a ' - i  ~ ~1:~ ~1:1 , ? "
" ~ ilYn~ " " 'The vector representation of X isV(X) = ~ V(xi)where V(xi) isV(xi) = (Mu(xi,o~),...,Mu(xi,om))Here, Mu(x, y) is the v',due of mutual informationproposed by (Church, 1991).
oj,...,om (We callthem basic words) are selected the 1000th mostfrequent words in the reference Collins EnglishDictionary (Lil)erman, 1990) .Let word x have senses sl,s2,...,sp and thedictionary-definition of si beYsi: " " ,Y -n , ' " ,Y - I ,Y ,  Yt," " ' ,Yn,"  ""The similarity of X and }~i is measured t)y theimter l)roduct of their normalised vectors and isdetined as follows:v(x )  ?
vo; )= Iv (x ) I I  vo%dl  (1)We infer that the sense of word x in X is si ifHi're(X, };i) is maximnm alnong t'~ ,...,}~p.Giw:n ml article, the procedure for WSD is ap-plied to each word (noun) in an article, i.e.
thesense of each noun is estimated using formula (1)and the word is rel)laced 1)y its sense.
Tat)le 1shows samI)le of the results of our disambiguationnn'thod.Tabh~ 1: The results of the WSD lnethodInput A munber of major aMines adoptedcontinental aMines' .-.Output A number5 of major airlinesladopted  cont inental2 airlines2 .
.
.
_In Tal)le I, underline signifies polysenmus nolln.'()utlmt.'
shows that ea('h noun is rel)laced l)y asyml)ol word which corresl)onds to each sense ofa word.
We call ' In lmt'  and ~()utput' in Table 1,mt (rriginal artMe and a new artMe, respectively.Tabh, 2: The definition of 'nnntber'~W\] .
:mmdwr2:Iltllllber3;nltHl|)er4:llllllt\])erS:Every mmd)er occupies a uniqueposition iv a sequence.He was lieu (HIe of Ollr nllllll)er.A telel)hOnC numl)er.~h(" was nu Inber  seVelt ill tit(, ra(,c.A large nmnber of people.Table 2 shows the definition of hmml)er' in theCollins English, Dictionary.
'numl)erl '  ~ 'nunt-l)er5' are symbol words and show different sensesOf ' l l un lber ' .3.2  L ink ing  Nouns  w i th  the i rSemant ica l ly  S imi la r  NounsOur method for classification of articles uses theresults of dismnbiguation method.
The problemshere are:1.
The frequency of ewwy disambiguated nounin new articles is lower than that of every pol-ysemous noun in oriqinal articles.
For exaln-ple, the frequency of 'nulnber5' in Table 1 islower than that of 'number 't.
Furthermore,some nouns in articles may be semanticallysimilar with each other.
For example, 'num-ber5' in Table 2 and 'sum4' in Table 3 arcahnost the saine sense.2.
A phr~sal exicon which Walker suggested inhis method gives a negatiw~ influence for clas-sification.1If all 'mlmber' are used ~s ~nunJ)er5' sense, theflequency of 'number' is the same as 'numl)erS'.407Table 3: The definition of 'sum' in the dict ionarys tun1:S l l l l l2 :S l l l l i 3 :sunl4:sunlS:The result of the addition of num-~?
l'S,l ie  o r  inore  eohnt lns  or  rows  o fnumbers to be added.The limit of the first n terms of aconverging infinite series as ,~ tendsto infinity.He borrows ellorlnoltS sluns.The essence or gist of a matter.Table 4: Pairs of nouns with Dis(vl ,v2) wfluesBBK0.125 share1 company10.140 giorgio di0.215 shares2 share20.262 share2 corot)any10.345 new3 yorklIn order to cope with these prol)lems, we linkednouns in new articles with their semantical ly  sim-i lar nouns.
The procedm'es for l inking are the fol-lowing five stages.Stage One: Calculating MuThe first stage for l inking nouns with their se-manticMly sinfilar nouns is to calculate Mu be-tween noun pMr x and y in new articles.
In orderto get a reliable stat ist ical  data,  we merged everynew article into one and used it to calculate Mu.The results are used in the following stages.Stage Two: Representing every noun as a vectorThe goal of this stage is to rel)resent every nounin a new article as a vector.
Using ~t term weight-ing method,  nouns iI| a new article would be rep-resented by vector of the formv = (2)where wl is the element of a new arti(:le and cor-responds to the weight of the noun wl.
In ourmethod,  the weight of wi is the wdue of Mu be-tween v and wi which is calculated in Stage One.Stage Three: Measuring similarity betweenvectorsGiven a vector representation of nouns in n e warticles ~s ill forlnula (2), a dissimi lar i ty betweentwo words (noun) vl,  v2 in an article would beobtMned by using formula (3).
A dissimilar itymeasure is the degree of deviat ion of the grout)in an n-dimensionM Eucl idean space, where 'n isthe number of nouns which co-occur with t~ 1 and'U 2 .Dis(v l ,v2)  = E~=,  Ej'~=,(vlj - ffj)'2 (a)~0 = ( fh , " ' , .q ; , )  is the centre of gravity and I .q Iis the length of it.
A group with a smMler valueof (3) is considered semantically less deviant.Stage Four: Clustering methodFor a set of nouns Wl~ 'W2~ " ' ' ,  w,~ of a newarticle, we calculate the semantic  devi~ttion valueof all possible pairs of nouns.Table 4 shows sample of the results of nounswith their semantic deviat ion values.Iu Table 4, 'BBK '  shows the topic of the arti-cle which is tagging in the WSJ, i.e.
'Buybacks ' .The value of Table 4 shows the semantic  deviat ionvMue of two nol lnS 2.The.
clustering algor i thm is appl ied to the setsshown in Table 4 and produced a set of semanticclusters, which are.
ordered in the as('ending orderof their semantic deviat ion wdues.
We adoptednon-overlal)ping , group average method in ourclustering technique ( Jardine, 1991).
The sampleresults of clustering is shown in Table 5.Table 5: Chtstering results of 'BBK '0.125 \[share1 company1\]0.140 \[giorgio di\]0.215 \[shares2 shm'e2\]0.251 \[sharel comp~myl stmre2 shares2\]The wdue of TaMe 5 shows the selnantie deviat ionvalue of the cluster.Stage Five: Linking nouns with their semanti-cally similar nounsWe selected ifferent 49 ar tMes  from 1988, 1989WSJ, and appl ied to Stage One ~ Four.
Fromthese results, we manuMly selected (:lusters whichare judged to be semantical ly similar.
For the se-lected chtsters, if there is a noun which belongsto several clusters, these clusters are grouped to-gether.
As a result, each cluster is added to asequential number.
The sample of the results areshown in Tal)le 6.Table 6: The results of Stage FiveSe(l .
n l l l i lword l  :"lUol'd2 :Iuol~d3 :word4  :word5  :SemanticMly similar nounsbank3, banks3emiada3, emmda4Amerieanl, expresslco., corp., eompanyl ?
?
?August, June, July, Sept. Oct. -.
.new2 york2eIn Table 4, there m'c some nouns which are notadded to the number, '1' ~,, '5', e.g.
'giorgio', 'di'.This shows that for these words, there is only onemeaning in the dictionary.408'Seq.
hum' in Table 6 shows a sequential numt)er,'wordl', ...,'word,,' whi(:h are added to the grou 1)of semantically similar nouns 3.
Tal)le 6 shows,for examl)le , 'new2' and 'york2' are xemanti('allysimilar and form a phn~sal lexicon.3.3  Clus ter ing  of  Ar t i c lesAccording to Table 6, freqllen('y of every word innew artMes ix counted, i.e.
if a word in a ne, warticle t)ehmgs to the gron l) shown ill Tal)h' 6,the word is rel)laced t)y its rel)resentative mmfl)('r'wordi' and th(' fre(luency of 'word/ '  is count('d.For (,xalnlJe, 'l)ank3' and 'banks3' in a new ~Lrti-cle are rel)laced by 'wordi', aud the frequen('y of'wordi' equals to the total nulnl)er of fr('quency of'bank3'  and q)anks3'.Using a term weighting method, articles wouhlbe represented 1)y vectors of the formA = (wt ,w. , ' " ,w . )
(4)where Wi (:orresl)on(ls to the weight of the nouni.
The weight is used to the fr('(lu(mcy of noun.Given the vector rel)resentations of articles as informula (4), a similarity between Ai and Aj arecaJculated using formula (1).
The greater thewtlue of Sim(Ai, Aj) is, the ntore xinfilar thesetwo articles are.
The ('lustering Mgorithm whh:his described in Stage Four is appticd to each 1)ah ' ofarticles, and t)roduces a set of ('lusters whh'h areordered in the des(:ending order of ~heir semanticsimilarity wdues.4 Exper imentsWe have conducted flmr ('xl)eriments, i.e.
q!
'req','Dis', 'Link', and 'Method'  in order to exanlinehow WSD me, thod and linking words with their se-mantically similar words(linking method in short)atfect the clustering results.
'Fl'eq' is fl'equency-t)a~sed exlmriment, i.e.
we use word frequency forweighting and do not use WSD and linking meth-ods.
'Dis' is con(:erned with disambiguationd)asedexperim(mt, i.e.
the (:lustering algorithm is ap-plied to new artMes.
'lAnk' ix con/:erned withlinking-l)~used experiment, i.e.
we applied linkingmethod to original artMes.
'Method'  shows ourproposed method.4.1 DataThe training tort)us we have used ix the 1988,1!
)89 WSJ ill ACL /DCI  CD-I{OM whi(.h ('onsistsof al)out 280,000 1)art-of-spee('h tagged sentences(Brill, 1992).
From this eorlmx, we seh,cted atrandom 49 (lifferent articles for test data, eachof which (-onsixts of 3,500 sentences and has dif-ferent tel)it ilallle wlfich is tagging in the WS,I.We classified 49 artMes into eight categories, e,g.Sin our experiments, m equals to 238.
'market news', 'food.
restaurant' ,  etc.
The di('tio-nary we have used is Collins English Dictionaryin ACL /DCI  CD-ROM.in WSD nwthod, the (:o-occurrence of x and yf'or cah:ulating Mu is that the two words (x,y) al)-pear in the training (:orl)uS in this order in a win-dow of 100 words, i.e.
a: is folh)wed by y withina 100-word distance.
This is because, the largerwin(h)w sizes might be ('onsidered to be useful forextra('ting s(unanti(' relationshil)s between ltOltllS.Basic words are sele('te(l the lO00th most fre(luentwords in the reference Collins English, Dictionary.
'\['he length of a selltetl(:(, ~" which contains a 1)ol -yxemons n(mn and the h'ngth of a sentence ofdi('tionary-defilfition are maximuln 20 words.
Forea('h t)olysemous nmm, we selected the tirst top 5definitions in the (lictionary.In linking m(,thod, a window size of the c(>o('('urren('e of .c and y for ('ah'ulating Mu is thesame as that in WSD method, i.e.
~L window of 100words.
W(' xeh,cted 969 ~ 9128 different (noun,nomt) pairs for each article, 377 ~ 1259 tilt%r-ell| llOllllS Oil condition that frequ(,ncies and Mu~,,,..or 1,,w (f(.,, :j) _> 5, M, , ( .
,  v) _> a) t,, per-mit a relial)le statistical analysis 4.
As a result ofStage Four, we nlanually selected (:lusters wlfichare judged to 1)e semanti(:ally similar.
As a result,w(' sele('te(l clusters on (:ondition that the thresh-old value for similarity wax 0.475.
For the seh'cted('lusters, if there ix a noun which belongs to xev-('ral ehtsters, thex(, chlsters are grouped together.As a r(,sult, we obtahwd 238 clusters in all.4.2  Resn l ts  o f  the  exper imentsThe results are showit in TM)le 7.
'\[h\])ie 7: The results of the experimentsAr|;i('h, Num Freq Link Dis Method5 10 4 4 5 8\[0 10 4 6 6 915 10 7 7 7 820 l.O 6 6 6 6Total 40 21 23 24 31(%) (-) (52.5) (57.5) (60.0) (77.5)In Tal)le 7, 'Article' means the munber of articleswhich are sele(:ted from test data.
~Nltnl' iiIPallsthe, nunlber for each 'Article', i.e.
we selected 1(Isets for each 'Article'.
'Freq', 'Link', 'Dis', and<Method' show the nulnlmr of sets which are clus-tered (:orrectly in ea(:h experiment.The samph' results of 'Article = 20' fl)r each(,xperiment is shown in Figure 1, 2, 3, and 4.In Figure 1, 2, 3, mid 4, the X-axis is the sim-ilarity wdue.
A1)l)reviation words in each Figureand categories are shown in Talile 8.4 Her(', f(x, y) is the munl)er of total co-occurrencesof words :e and y in this order in st window size of 100words.4090.9 0.7 0 5 X:slmllarity valueI I 0.638 I *X\['-- BBK ~_~420 |TNMmarket~ STK 2'~~1news L ~  26016REC 0 .206metal -~CSretailing- RET ~\[ood r- RFD O .141restauranU- FODAROe em ca t_MT CFigure 1: Tile results of 'Freq' experimentnews t._ DIV ~ \[ 0.890RETCMD .
-~981~843 a762ARC) _:=:_==_.aTNM 0.956~C S ~382BVG _____1 ~_1~871CEO PRO ~ _  ~.841 0,651FOD ~ ~'814 ~ 2 0 4ENV ' I \[-'--HEA I II MTCSTKBONFigure 2: The results of 'Link' experiment0.7 0.5 0.3 I I - - I  *,X .
.
.
.
0,679 V ~~.53omarketlT~N'~l~---I ~413news / DI~_____ ~ ~_ 0.263metal- PCS ~ IHI 0 55 -- 0 263 \[restaurant\] R E C ~  ~ ~\ ]I FOP  - -  , \[ \[ It.. RFD ' I IBON- 0,1'~ Ienvironment- ENV 0~7N kscience- ARO ~ \[_J \[farm~CMD 0~35 ~172 \[chemical\[- HEA- ,~,oo -,--.~j t-MTC- J 0.073Figure 3: The results of 'Dis' experiment5 Discussion1.
WSD methodAccording to Table 7, there, are 24 sets whichcould be (:lustered correctly in 'Dis', while 21 setsin 'Freq'.
Examining the results shown in Fig-ure 3, 'BVG'  and 'HRD'  are correctly classifiedinto 'food ?
restaurant' and 'market news', respec-tively.
However, the results of 'Freq' (Figure 1)shows that they are classified incorre<'tly.
Table0.9 0T7r~l~h~JL'9~4969 015 ?
X BB 1- \[ DIV J ~.9231 STK -~ ~L922market I TN M 0~72~L~913news I ~ \ [  L863I ~ ~  tAs2st -HRD~ \] 10.819sc ience-  ARO - ~metal-- PCS 0.893' L, \[- B VG ~58~-756I FOD ~ I tlfood I PRO ~\] I I .
- ' "  restaurant\[ hE~-  ~7~J~ "a"t._ RF~I) retailing-- RRF!l ),' ~ ~0:845l I 0"5~73,environment-- ENVehemical\[-MT C 0farm-~M~-Figure 4: The results of 'Method' experimentTable 8: Topic: and category nameCategorymarketnewssciencemetalfoodrestaurantTopicBBK: BuybacksBON: Bond Market NewsCEO: Dow Jones interviewDIV: dividendsERN: em'ningsHI/D: Hem'd on the streetSTK: stock mm'ketTNM: tender offersARO: aerospacePCS: precious metals, stones, goldBVG: beveragesFOD: food productsPRO: corporate profileREC: recreation, entertainmentRFD: restaurant, supermarketretailing RET: retailingenvironmentchemicalfarmENV: environmentHEA: health care providers, medicineMTC: medicM and biotechnologyCMD: commodity news, farm products9 shows different senses of word ill 'BVG',  and'HRD'  which could be discriminated in 'Dis'.In Table 9, for example, 'security' is high freqtlen-ties and used ill 'being secure' sense ill 'BVG'  ar-tMe, while 'security' is 'certificate of creditorshiI)'sense in 'HRD'.
One possible cause that the re-sults of 'Freq' is worse than 'Dis' is that thesepolyselnous words which are high-frequencies arenot recognised polysemy in 'Freq'.2.
L ink ing methodAs shown in Table 7, there are 23 sets whichcould be clustered correctly in 'Link', while 21 setsill 'Freq'.
For example, 'ERN'  and 'HRD' are bothconcerned with 'market news'.
In Figure 2, theyare clustered with high similarity wflue(0.943),while in Figure 1, they are not(0.260).Exalnilfing the results, there are 811 nounsin 'ERN'  article, and 714 nouns in 'HRD',  and410Table 9: Different word-senses in BVG and HRDsecurityratesalestockBVGthe state ofbeing securea quantity in relationthe exchange of goodstotal goodsHRDcertificate ofcreditorshipl)rice of chargethe alllOllltt of soldstock marketof these, 'shares', 'stock', and 'share' which aresemantically similar ~re included.
In linkingmethod, there are 251 nmmn in 'ERN'  and 492nouns in 'HRD'  whi('h ~tre repl~tccd for represen-tative words.
However, in 'Freq', each noun cor-responds different coordinate, and regards to dif-ferent meaning.
As a result, these tol)ics are clus-tered with low similarity wdue.3.
Our  methodTit('.
results of 'Method'  show tha,t 31 out of 40sets are cbLssified correctly, att(I the per('entage at-tained was 77.5%, while 'Freq', 'Link', and 'Din'ext)eriment att,~tined 52.5%, 57.5%, 6().0%, renl)e(:-tively.
This shows the effe(-tivelmss of our method.In Figure 4, the ~u'ticles ,tre judged to ('l,tssifyinto eight categories.
Examining 'ERN' ,  'CEO'and 'CMD'  in Figure 1, 'CE() '  and 'CM1)' aregrouped together, while they have (lifferent c~,t-egories with each other.
On the other hand,in Figure 3, 'ERN'  and 'CE() '  ar(, groul)ed to-gether corre('tly.
Examining the nouns which arc1)elonging to 'ERN'  mid 'CE()' ,  'p lant ' ( factoryand food senses), 'oi l '(petrohmnl and food), 'or-der'(colmn~nd ;md dema.nd), and 'interent'(del)tand curiosity) whi(:h are high frequencies ~re cor-rectly dismnbiguated.
Furthermore, in Figure 4,'ERN'  mM 'CEO'  are classified into 'market news',and 'CMD'  are cb~ssilied into 'fm:m', correctly.
Forexample, 'plant'  which is used in ' factory' sense inlinked with semanti('~lly silnib~r words, 'ntanuf;w-turing', ' factory',  'production' ,  or ' job' et('..
In asimibtr way, ' i)bmt' which in uned in flood' senseis linked with 'environmeltt', 'forest'.
As a result,the articles are classified correctly.As shown in Table 7, there arc 9 nets whichcould not 1)e clustered correctly in our method.
Apossible improwmmnt is that we use all definitionsof words in the dictionary.
We s(qeeted the firsttop 5 definitions in the dictionary for each nounand used theln in the cxperilnent.
However, thereare some words of which the memfings are not in-cluded these selected definitions.
Thin (:~mses thefact theft it is hard to get a higher percentage ofcorrect clustering.
Another interesting t)ossibil -ity in to use ml altermttive weighting policy, sucha,s the widf ( weigh.te, d invcr.sc docwmcnt fre, qucncy)(Tokunaga, 1994).
The widf is reported to have amarked ~ulwmtage over the idf ( invers~ .documentfrequency) for the text categoris~Ltion tank.6 Conc lus ion\Ve have rei)orted an exl)erimentad study for clus-tering of ~rticles by using on-line (lictiom~ry deft-nitions mid showed how dictionary-definitiolt camuse effectively to classify articles, ea('h of which be-longs to the restricted sul)ject domain.
In orderto Col)e with the relnainiug i)rol)lems inentionedin section 5 and apply thin work to practical use,we will conduct further e?perilnents.ReferencesP.
F. Brown et al, 1991.
V~ror(1-Sense DisambiguationUsing Statisti('al Methods.
In Proc.
of the 291h An-w~tal Meeting of the A CL, pl ).
264-270.E.
Brill, 1992.
A siml)le rule-I)~Lsed 1)art of speech tag-ger.
In Proc.
of th, c 3nd conference on applied natu-ral language procc,ssiug, ACL, pp.
152-155.
Trcnto,Italy, 1992.K.
W. Church (,t al., 1991.
Using Statistics in Lexi-eal Analysis, Lexical acquisition: Exploiting on-linere.qource~ to build a lea:icon.
(Zernik Uri (ed.
)), pp.115-164, London, Lawrence Erlbaum Associates.L.
Guthric and E. Walker, "DOCUMENT CLASSI-FICATION BY MACHINE: Theory and Practice",h, Proc.
of the 15th, International Confercncc onComputational Linguistics, Kyot% Japan, 1994, l)P.1059-1063N..lardine and 11.
Sibson, 1968.
The constructionof hierarchic and non-hierarchic classifications.
InComputer ,lourrtal, i)p. 177-1.84.K.
S. Jones, 1973.
A statistical interl)retation of termsp(~cificity and its apl)lieation in retrieval.
Journalof Documentation, 28 (1973) 1, pp.
11-21.M.
IAberman, editor.
1991.
CD-ROM I, Associationfor Comlmtational Linguistics Data Collection Ini-tiative, University of Pennsylvania.Y.
Niwa and Y. Nitta, 1995.
Statistical Word SenseDisalnbiguation Using Dictionary Definitions InProc.
of the Natural Language Processing PacificRim Sympoaium '95, Seoul, Korea, pp.
665-670.G.
Salton and M. a. M('Gill, 1983.
Introduction toModern hfformation Retrieval.
McGraw-Hill, 1983.T.
Tokunaga nd M. Iwayalna, 1994.
Text Categori-s~fl;ioll based on Weighted Inverse Doenment FI'e-quency IPS.\] SIG l~.cl)orts, 94-NL-1f)0, 1994.I).
Yarowsky, "Word sense (lismnbiguation using sta-tistical models of I/oget's categories trained on largecorl)ora" , In Proc.
of the 14th International Confc>e'ncc on Computational Linguistics, Nantes, France,1992, l)P. 454-46{)N. Yuasa ('t al., 1995.
Cb~ssifying ArtMes Using Lex-ical Co-occurrence in Large Document DatabmsesIn TTu'na.
of Infl~rmation Processing Society Japan,pp.
1819-1827, 36 (1995) 8.1), Walker and I/.
Amsler, 1986.
The Use of Machine-l-leadable Dietionm:ies in Sublanguage m,Mysis, An-alyzing Language in Restricted omains, (Grishmanaim Kittredge (cd.
)), pp.
69-84, Lawrence Erlbaum,ltillsdale, NJ.
11987) 2.411
