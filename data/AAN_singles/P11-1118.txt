Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1179?1189,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsDisentangling Chat with Local Coherence ModelsMicha ElsnerSchool of InformaticsUniversity of Edinburghmelsner0@gmail.comEugene CharniakDepartment of Computer ScienceBrown University, Providence, RI 02912ec@cs.brown.eduAbstractWe evaluate several popular models of localdiscourse coherence for domain and task gen-erality by applying them to chat disentangle-ment.
Using experiments on synthetic multi-party conversations, we show that most mod-els transfer well from text to dialogue.
Co-herence models improve results overall whengood parses and topic models are available,and on a constrained task for real chat data.1 IntroductionOne property of a well-written document is coher-ence, the way each sentence ts into its context?
sen-tences should be interpretable in light of what hascome before, and in turn make it possible to inter-pret what comes after.
Models of coherence haveprimarily been used for text-based generation tasks:ordering units of text for multidocument summariza-tion or inserting new text into an existing article.In general, the corpora used consist of informativewriting, and the tasks used for evaluation considerdifferent ways of reordering the same set of textualunits.
But the theoretical concept of coherence goesbeyond both this domain and this task setting?
andso should coherence models.This paper evaluates a variety of local coher-ence models on the task of chat disentanglement or?threading?
: separating a transcript of a multipartyinteraction into independent conversations1.
Suchsimultaneous conversations occur in internet chat1A public implementation is available via https://bitbucket.org/melsner/browncoherence.rooms, and on shared voice channels such as push-to-talk radio.
In these situations, a single, correctlydisentangled, conversational thread will be coherent,since the speakers involved understand the normalrules of discourse, but the transcript as a whole willnot be.
Thus, a good model of coherence should beable to disentangle sentences as well as order them.There are several differences between disentan-glement and the newswire sentence-ordering taskstypically used to evaluate coherence models.
Inter-net chat comes from a different domain, one wheretopics vary widely and no reliable syntactic annota-tions are available.
The disentanglement task mea-sures different capabilities of a model, since it com-pares documents that are not permuted versions ofone another.
Finally, full disentanglement requiresa large-scale search, which is computationally dif-cult.
We move toward disentanglement in stages,carrying out a series of experiments to measure thecontribution of each of these factors.As an intermediary between newswire and inter-net chat, we adopt the SWITCHBOARD (SWBD) cor-pus.
SWBD contains recorded telephone conversa-tions with known topics and hand-annotated parsetrees; this allows us to control for the performanceof our parser and other informational resources.
Tocompare the two algorithmic settings, we use SWBDfor ordering experiments, and also articially entan-gle pairs of telephone dialogues to create synthetictranscripts which we can disentangle.
Finally, wepresent results on actual internet chat corpora.On synthetic SWBD transcripts, local coherencemodels improve performance considerably over ourbaseline model, Elsner and Charniak (2008b).
On1179internet chat, we continue to do better on a con-strained disentanglement task, though so far, we areunable to apply these improvements to the full task.We suspect that, with better low-level annotationtools for the chat domain and a good way of integrat-ing prior information, our improvements on SWBDcould transfer fully to IRC chat.2 Related workThere is extensive previous work on coherence mod-els for text ordering; we describe several specicmodels below, in section 2.
This study focuses onmodels of local coherence, which relate text to itsimmediate context.
There has also been work onglobal coherence, the structure of a document as awhole (Chen et al, 2009; Eisenstein and Barzilay,2008; Barzilay and Lee, 2004), typically modeledin terms of sequential topics.
We avoid using themhere, because we do not believe topic sequences arepredictable in conversation and because such modelstend to be algorithmically cumbersome.In addition to text ordering, local coherence mod-els have also been used to score the uency of textswritten by humans or produced by machine (Pitlerand Nenkova, 2008; Lapata, 2006; Miltsakaki andKukich, 2004).
Like disentanglement, these tasksprovide an algorithmic setting that differs from or-dering, and so can demonstrate previously unknownweaknesses in models.
However, the target genre isstill informative writing, so they reveal little aboutcross-domain exibility.The task of disentanglement or ?threading?
forinternet chat was introduced by Shen et al (2006).Elsner and Charniak (2008b) created the publiclyavailable #LINUX corpus; the best published re-sults on this corpus are those of Wang and Oard(2009).
These two studies use overlapping unigramsto measure similarity between two sentences; Wangand Oard (2009) use a message expansion tech-nique to incorporate context beyond a single sen-tence.
Unigram overlaps are used to model coher-ence, but more sophisticated methods using syntax(Lapata and Barzilay, 2005) or lexical features (La-pata, 2003) often outperform them on ordering tasks.This study compares several of these methods withElsner and Charniak (2008b), which we use as abaseline because there is a publicly available imple-mentation2.Adams (2008) also created and released a disen-tanglement corpus.
They use LDA (Blei et al, 2001)to discover latent topics in their corpus, then measur-ing similarity by looking for shared topics.
Thesefeatures fail to improve their performance, which ispuzzling in light of the success of topic modeling forother coherence and segmentation problems (Eisen-stein and Barzilay, 2008; Foltz et al, 1998).
Theresults of this study suggest that topic models canhelp with disentanglement, but that it is difcult tond useful topics for IRC chat.A few studies have attempted to disentangle con-versational speech (Aoki et al, 2003; Aoki et al,2006), mostly using temporal features.
For the mostpart, however, this research has focused on auditoryprocessing in the context of the cocktail party prob-lem, the task of attending to a specic speaker ina noisy room (Haykin and Chen, 2005).
Utterancecontent has some inuence on what the listener per-ceives, but only for extremely salient cues such asthe listener's name (Moray, 1959), so cocktail partyresearch does not typically use lexical models.3 ModelsIn this section, we briey describe the models we in-tend to evaluate.
Most of them are drawn from pre-vious work; one, the topical entity grid, is a novelextension of the entity grid.
For the experiments be-low, we train the models on SWBD, sometimes aug-mented with a larger set of automatically parsed con-versations from the FISHER corpus.
Since the twocorpora are quite similar, FISHER is a useful sourcefor extra data; McClosky et al (2010) uses it forthis purpose in parsing experiments.
(We continueto use SWBD/FISHER even for experiments on IRC,because we do not have enough disentangled train-ing data to learn lexical relationships.
)3.1 Entity gridThe entity grid (Lapata and Barzilay, 2005; Barzilayand Lapata, 2005) is an attempt to model some prin-ciples of Centering Theory (Grosz et al, 1995) in astatistical manner.
It represents a document in termsof entities and their syntactic roles: subject (S), ob-ject (O), other (X) and not present (-).
In each new2cs.brown.edu/?melsner1180utterance, the grid predicts the role in which eachentity will appear, given its history of roles in theprevious sentences, plus a salience feature countingthe total number of times the entity occurs.
For in-stance, for an entity which is the subject of sentence1, the object of sentence 2, and occurs four times intotal, the grid predicts its role in sentence 3 accord-ing to the conditional P (jS;O; sal = 4).As in previous work, we treat each noun in a doc-ument as denoting a single entity, rather than usinga coreference technique to attempt to resolve them.In our development experiments, we noticed thatcoreferent nouns often occur farther apart in conver-sation than in newswire, since they are frequentlyreferred to by pronouns and deictics in the interim.Therefore, we extend the history to six previous ut-terances.
For robustness with this long history, wemodel the conditional probabilities using multilabellogistic regression rather than maximum likelihood.This requires the assumption of a linear model, butmakes the estimator less vulnerable to overttingdue to sparsity, increasing performance by about 2%in development experiments.3.2 Topical entity gridThis model is a variant of the generative entitygrid, intended to take into account topical informa-tion.
To create the topical entity grid, we learn aset of topic-to-word distributions for our corpus us-ing LDA (Blei et al, 2001)3with 200 latent top-ics.
This model embeds our vocabulary in a low-dimensional space: we represent each word w asthe vector of topic probabilities p(tijw).
We ex-perimented with several ways to measure relation-ships between words in this space, starting with thestandard cosine.
However, the cosine can depend onsmall variations in probability (for instance, if w hasmost of its mass in dimension 1, then it is sensitiveto the exact weight of v for topic 1, even if this es-sentially never happens).To control for this tendency, we instead use themagnitude of the dimension of greatest similarity:sim(w; v) = maximin(wi; vi)Tomodel coherence, we generalize the binary his-3www.cs.princeton.edu/?blei/topicmodeling.htmltory features of the standard entity grid, which de-tect, for example, whether entity e is the subject ofthe previous sentence.
In the topical entity grid, weinstead compute a real-valued feature which sumsup the similarity between entity e and the subject(s)of the previous sentence.These features can detect a transition like: ?TheHouse voted yesterday.
The Senate will consider thebill today.?.
If ?House?
and ?Senate?
have a highsimilarity, then the feature will have a high value,predicting that ?Senate?
is a good subject for the cur-rent sentence.
As in the previous section, we learnthe conditional probabilities with logistic regression;we train in parallel by splitting the data and averag-ing (Mann et al, 2009).
The topics are trained onFISHER, and on NANC for news.3.3 IBM-1The IBM translation model was rst considered forcoherence by Soricut and Marcu (2006), although aless probabilistically elegant version was proposedearlier (Lapata, 2003).
This model attempts to gen-erate the content words of the next sentence by trans-lating them from the words of the previous sentence,plus a null word; thus, it will learn alignments be-tween pairs of words that tend to occur in adjacentsentences.
We learn parameters on the FISHER cor-pus, and on NANC for news.3.4 PronounsThe use of a generative pronoun resolver for co-herence modeling originates in Elsner and Char-niak (2008a).
That paper used a supervised model(Ge et al, 1998), but we adapt a newer, unsuper-vised model which they also make publicly available(Charniak and Elsner, 2009)4.
They model each pro-noun as generated by an antecedent somewhere inthe previous two sentences.
If a good antecedent isfound, the probability of the pronoun's occurrencewill be high; otherwise, the probability is low, sig-naling that the text is less coherent because the pro-noun is hard to interpret correctly.We use the model as distributed for news text.
Forconversation, we adapt it by running a few iterationsof their EM training algorithm on the FISHER data.4bllip.cs.brown.edu/resources.shtml\#software11813.5 Discourse-newnessBuilding on work from summarization (Nenkovaand McKeown, 2003) and coreference resolution(Poesio et al, 2005), Elsner and Charniak (2008a)use a model which recognizes discourse-new versusold NPs as a coherence model.
For instance, themodel can learn that ?President Barack Obama?
isa more likely rst reference than ?Obama?.
Follow-ing their work, we score discourse-newness with amaximum-entropy classier using syntactic featurescounting different types of NP modiers, and we useNP head identity as a proxy for coreference.3.6 Chat-specic featuresMost disentanglement models use non-linguistic in-formation alongside lexical features; in fact, times-tamps and speaker identities are usually better cuesthan words are.
We capture three essential non-linguistic features using simple generative models.The rst feature is the time gap between one utter-ance and the next within the same thread.
Consistentshort gaps are a sign of normal turn-taking behavior;long pauses do occur, but much more rarely (Aoki etal., 2003).
We round all time gaps to the nearest sec-ond and model the distribution of time gaps using ahistogram, choosing bucket sizes adaptively so thateach bucket contains at least four datapoints.The second feature is speaker identity; conver-sations usually involve a small subset of the to-tal number of speakers, and a few core speakersmake most of the utterances.
We model the distri-bution of speakers in each conversation using a Chi-nese Restaurant Process (CRP) (Aldous, 1985) (tun-ing the dispersion  to maximize development pe-formance).
The CRP's ?rich-get-richer?
dynamicscapture our intuitions, favoring conversations domi-nated by a few vociferous speakers.Finally, we model name mentioning.
Speakersin IRC chat often use their addressee's names to co-ordinate the chat (O'Neill and Martin, 2003), andthis is a powerful source of information (Elsner andCharniak, 2008b).
Our model classies each utter-ance into either the start or continuation of a conver-sational turn, by checking if the previous utterancehad the same speaker.
Given this status, it computesprobabilities for three outcomes: no name mention,a mention of someone who has previously spokenin the conversation, or a mention of someone else.
(The third option is extremely rare; this accountsfor most of the model's predictive power).
We learnthese probabilities from IRC training data.3.7 Model combinationTo combine these different models, we adopt thelog-linear framework of Soricut and Marcu (2006).Here, each model Piis assigned a weight i, and thecombined score P (d) is proportional to:Xiilog(Pi(d))The weights  can be learned discriminatively,maximizing the probability of d relative to a task-specic contrast set.
For ordering experiments, thecontrast set is a single random permutation of d; weexplain the training regime for disentanglement be-low, in subsection 4.1.4 Comparing orderings of SWBDTo measure the differences in performance causedby moving from news to a conversational domain,we rst compare our models on an ordering task,discrimination (Barzilay and Lapata, 2005; Karama-nis et al, 2004).
In this task, we take an originaldocument and randomly permute its sentences, cre-ating an articial incoherent document.
We then testto see if our model prefers the coherent original.For SWBD, rather than compare permutationsof the individual utterances, we permute conversa-tional turns (sets of consecutive utterances by eachspeaker), since turns are natural discourse units inconversation.
We take documents numbered 2000?3999 as training/development and the remainder astest, yielding 505 training and 153 test documents;we evaluate 20 permutations per document.
As acomparison, we also show results for the same mod-els on WSJ, using the train-test split from Elsner andCharniak (2008a); the test set is sections 14-24, to-talling 1004 documents.Purandare and Litman (2008) carry out similar ex-periments on distinguishing permuted SWBD doc-uments, using lexical and WordNet features in amodel similar to Lapata (2003).
Their accuracy forthis task (which they call ?switch-hard?)
is roughly68%.1182WSJ SWBDEGrid 76.4z 86.0Topical EGrid 71.8z 70.9zIBM-1 77.2z 84.9yPronouns 69.6z 71.7zDisc-new 72.3z 55.0zCombined 81.9 88.4-EGrid 81.0 87.5-Topical EGrid 82.2 90.5-IBM-1 79.0z 88.9-Pronouns 81.3 88.5-Disc-new 82.2 88.4Table 1: Discrimination F scores on news and dialogue.z indicates a signicant difference from the combinedmodel at p=.01 and y at p=.05.In Table 1, we show the results for individualmodels, for the combined model, and ablation re-sults for mixtures without each component.
WSJ ismore difcult than SWBD overall because, on av-erage, news articles are shorter than SWBD con-versations.
Short documents are harder, becausepermuting disrupts them less.
The best SWBD re-sult is 91%; the best WSJ result is 82% (both formixtures without the topical entity grid).
The WSJresult is state-of-the-art for the dataset, improvingslightly on Elsner and Charniak (2008a) at 81%.
Wetest results for signicance using the non-parametricMann-Whitney U test.Controlling for the fact that discrimination is eas-ier on SWBD, most of the individual models performsimilarly in both corpora.
The strongest models inboth cases are the entity grid and IBM-1 (at about77% for news, 85% for dialogue).
Pronouns and thetopical entity grid are weaker.
The major outlier isthe discourse-new model, whose performance dropsfrom 72% for news to only 55%, just above chance,for conversation.The model combination results show that all themodels are quite closely correlated, since leavingout any single model does not degrade the combi-nation very much (only one of the ablations is sig-nicantly worse than the combination).
The mostcritical in news is IBM-1 (decreasing performanceby 3% when removed); in conversation, it is theentity grid (decreasing by about 1%).
The topicalentity grid actually has a (nonsignicant) negativeimpact on combined performance, implying that itspredictive power in this setting comes mainly frominformation that other models also capture, but thatit is noisier and less reliable.
In each domain, thecombined models outperform the best single model,showing the information provided by the weakermodels is not completely redundant.Overall, these results suggest that most previ-ously proposed local coherence models are domain-general; they work on conversation as well asnews.
The exception is the discourse-newnessmodel, which benets most from the specic con-ventions of a written style.
Full names with titles(like ?President Barack Obama?)
are more commonin news, while conversation tends to involve fewercompletely unfamiliar entities and more cases ofbridging reference, in which grounding informationis given implicitly (Nissim, 2006).
Due to its poorperformance, we omit the discourse-newness modelin our remaining experiments.5 Disentangling SWBDWe now turn to the task of disentanglement, test-ing whether models that are good at ordering alsodo well in this new setting.
We would like to holdthe domain constant, but we do not have any disen-tanglement data recorded from naturally occurringspeech, so we create synthetic instances by mergingpairs of SWBD dialogues.
Doing so creates an arti-cial transcript in which two pairs of people appearto be talking simultaneously over a shared channel.The situation is somewhat contrived in that eachpair of speakers converses only with each other,never breaking into the other pair's dialogue andrarely using devices like name mentioning to makeit clear who they are addressing.
Since this makesspeaker identity a perfect cue for disentanglement,we do not use it in this section.
The only chat-specic model we use is time.Because we are not using speaker information, weremove all utterances which do not contain a nounbefore constructing synthetic transcripts?
these aremostly backchannels like ?Yeah?.
Such utterancescannot be correctly assigned by our coherence mod-els, which deal with content; we suspect most ofthem could be dealt with by associating them withthe nearest utterance from the same speaker.1183Once the backchannels are stripped, we can cre-ate a synthetic transcript.
For each dialogue, we rstsimulate timestamps by sampling the number of sec-onds between each utterance and the next from a dis-cretized Gaussian: bN(0; 2:5)c. The interleaving ofthe conversations is dictated by the timestamps.
Wetruncate the longer conversation at the length of theshorter; this ensures a baseline score of 50% for thedegenerate model that assigns all utterances to thesame conversation.We create synthetic instances of two types?
thosewhere the two entangled conversations had differ-ent topical prompts and those where they were thesame.
(Each dialogue in SWBD focuses on a prese-lected topic, such as shing or movies.)
We entangledialogues from our ordering development set to usefor mixture training and validation; for testing, weuse 100 instances of each type, constructed from di-alogues in our test set.When disentangling, we treat each thread as inde-pendent of the others.
In other words, the probabilityof the entire transcript is the product of the probabil-ities of the component threads.
Our objective is tond the set of threads maximizing this.
As a com-parison, we use the model of Elsner and Charniak(2008b) as a baseline.
To make their implementa-tion comparable to ours, in this section we constrainit to nd only two threads.5.1 Disentangling a single utteranceOur rst disentanglement task is to correctly assigna single utterance, given the true structure of the restof the transcript.
For each utterance, we comparetwo versions of the transcript, the original, and aversion where it is swapped into the other thread.Our accuracy measures how often our models preferthe original.
Unlike full-scale disentanglement, thistask does not require a computationally demandingsearch, so it is possible to run experiments quickly.We also use it to train our mixture models for disen-tanglement, by construct a training example for eachutterance i in our training transcripts.
Since the El-sner and Charniak (2008b) model maximizes a cor-relation clustering objective which sums up indepen-dent edge weights, we can also use it to disentanglea single sentence efciently.Our results are shown in Table 2.
Again, re-sults for individual models are above the line, thenDifferent Same Avg.EGrid 80.2 72.9 76.6Topical EGrid 81.7 73.3 77.5IBM-1 70.4 66.7 68.5Pronouns 53.1 50.1 51.6Time 58.5 57.4 57.9Combined 86.8 79.6 83.2-EGrid 86.0 79.1 82.6-Topical EGrid 85.2 78.7 81.9-IBM-1 86.2 78.7 82.4-Pronouns 86.8 79.4 83.1-Time 84.5 76.7 80.6E+C `08 78.2 73.5 75.8Table 2: Average accuracy for disentanglement of a sin-gle utterance on 200 synthetic multiparty conversationsfrom SWBD test.our combined model, and nally ablation results formixtures omitting a single model.
The results showthat, for a pair of dialogues that differ in topic, ourbest model can assign a single sentence with 87%accuracy.
For the same topic, the accuracy is 80%.In each case, these results improve on (Elsner andCharniak, 2008b), which scores 78% and 74%.Changing to this new task has a substantial im-pact on performance.
The topical model, which per-formed poorly for ordering, is actually stronger thanthe entity grid in this setting.
IBM-1 underperformseither grid model (69% to 77%); on ordering, it wasnearly as good (85% to 86%).Despite their ordering performance of 72%, pro-nouns are essentially useless for this task, at 52%.This decline is due partly to domain, and partlyto task setting.
Although SWBD contains morepronominals than WSJ, many of them are rstand second-person pronouns or deictics, which ourmodel does not attempt to resolve.
Since the disen-tanglement task involves moving only a single sen-tence, if moving this sentence does not sever a re-solvable pronoun from its antecedent, the model willbe unable to make a good decision.As before, the ablation results show that all themodels are quite correlated, since removing any sin-gle model from the mixture causes only a small de-crease in performance.
The largest drop (83% to81%) is caused by removing time; though time isa weak model on its own, it is completely orthogo-1184nal to the other models, since unlike them, it doesnot depend on the words in the sentences.Comparing results between ?different topic?
and?same topic?
instances shows that ?same topic?
isharder?
by about 7% for the combined model.
TheIBM model has a relatively small gap of 3.7%, andin the ablation results, removing it causes a largerdrop in performance for ?same?
than ?different?
;this suggests it is somewhat more robust to similar-ity in topic than entity grids.Disentanglement accuracy is hard to predict givenordering performance; the two tasks plainly makedifferent demands on models.
One difference is thatthe models which use longer histories (the two entitygrids) remain strong, while the models consideringonly one or two previous sentences (IBM and pro-nouns) do not do as well.
Since the changes beingconsidered here affect only a single sentence, whilepermutation affects the entire transcript, more his-tory may help by making the model more sensitiveto small changes.5.2 Disentangling an entire transcriptWe now turn to the task of disentangling an entiretranscript at once.
This is a practical task, motivatedby applications such as search and information re-trieval.
However, it is more difcult than assign-ing only a single utterance, because decisions areinterrelated?
an error on one utterance may causea cascade of poor decisions further down.
It is alsocomputationally harder.We use tabu search (Glover and Laguna, 1997) tond a good solution.
The search repeatedly nds andmoves the utterance which would most improve themodel score if swapped from one thread to the other.Unlike greedy search, tabu search is constrained notto repeat a solution that it has recently visited; thisforces it to keep exploring when it reaches a localmaximum.
We run 500 iterations of tabu search(usually nding the rst local maximum after about100) and return the best solution found.We measure performance with one-to-one over-lap, which maps the two clusters to the two golddialogues, then measures percent correct5.
Our re-sults (Table 3) show that, for transcripts with dif-ferent topics, our disentanglement has 68% over-5The other popular metrics, F and loc3, are correlated.Different Same Avg.EGrid 60.3 57.1 58.7Topical EGrid 62.3 56.8 59.6IBM-1 56.5 55.2 55.9Pronouns 54.5 54.4 54.4Time 55.4 53.8 54.6Combined 67.9 59.8 63.9E+C `08 59.1 57.4 58.3Table 3: One-to-one overlap between disentanglement re-sults and truth on 200 synthetic multiparty conversationsfrom SWBD test.lap with truth, extracting about two thirds of thestructure correctly; this is substantially better thanElsner and Charniak (2008b), which scores 59%.Where the entangled conversations have the sametopic, performance is lower, about 60%, but still bet-ter than the comparison model with 57%.
Since cor-relations with the previous section are fairly reliable,and the disentanglement procedure is computation-ally intensive, we omit ablation experiments.As we expect, full disentanglement is more dif-cult than single-sentence disentanglement (com-bined scores drop by about 20%), but the single-sentence task is a good predictor of relative perfor-mance.
Entity grid models do best, the IBM modelremains useful, but less so than for discrimination,and pronouns are very weak.
The IBM model per-forms similarly under both metrics (56% and 57%),while other models perform worse on loc3.
Thissupports our suggestion that IBM's decline in per-formance from ordering is indeed due to its using asingle sentence history; it is still capable of gettinglocal structures right, but misses global ones.6 IRC dataIn this section, we move from synthetic data toreal multiparty discourse recorded from internet chatrooms.
We use two datasets: the #LINUX corpus(Elsner and Charniak, 2008b), and three larger cor-pora, #IPHONE, #PHYSICS and #PYTHON (Adams,2008).
We use the 1000-line ?development?
sec-tion of #LINUX for tuning our mixture models andthe 800-line ?test?
section for development experi-ments.
We reserve the Adams (2008) corpora fortesting; together, they consist of 19581 lines of chat,with each section containing 500 to 1000 lines.1185Chat-specic 74.0+EGrid 79.3+Topical EGrid 76.8+IBM-1 76.3+Pronouns 73.9+EGrid/Topic/IBM-1 78.3E+C `08b 76.4Table 4: Accuracy for single utterance disentanglement,averaged over annotations of 800 lines of #LINUX data.In order to use syntactic models like the entitygrid, we parse the transcripts using (McClosky etal., 2006).
Performance is bad, although the parserdoes identify most of the NPs; poor results are typi-cal for a standard parser on chat (Foster, 2010).
Wepostprocess the parse trees to retag ?lol?, ?haha?
and?yes?
as UH (rather than NN, NNP and JJ).In this section, we use all three of our chat-specic models (sec.
2.0.6; time, speaker andmen-tion) as a baseline.
This baseline is relatively strong,so we evaluate our other models in combination withit.6.1 Disentangling a single sentenceAs before, we show results on correctly disentan-gling a single sentence, given the correct structureof the rest of the transcript.
We average perfor-mance on each transcript over the different annota-tions, then average the transcripts, weighing them bylength to give each utterance equal weight.Table 4 gives results on our development corpus,#LINUX.
Our best result, for the chat-specic fea-tures plus entity grid, is 79%, improving on the com-parison model, Elsner and Charniak (2008b), whichgets 76%.
(Although the table only presents an av-erage over all annotations of the dataset, this modelis also more accurate for each individual annota-tor than the comparison model.)
We then ran thesame model, chat-specic features plus entity grid,on the test corpora from Adams (2008).
These re-sults (Table 5) are also better than Elsner and Char-niak (2008b), at an average of 93% over 89%.As pointed out in Elsner and Charniak (2008b),the chat-specic features are quite powerful in thisdomain, and it is hard to improve over them.
Elsnerand Charniak (2008b), which has simple lexical fea-tures, mostly based on unigram overlap, increases#IPHONE #PHYSICS #PYTHON+EGrid 92.3 96.6 91.1E+C `08b 89.0 90.2 88.4Table 5: Average accuracy for disentanglement of a sin-gle utterance for 19581 total lines from Adams (2008).performance over baseline by 2%.
Both IBM andthe topical entity grid achieve similar gains.
The en-tity grid does better, increasing performance to 79%.Pronouns, as before for SWBD, are useless.We believe that the entity grid's good perfor-mance here is due mostly to two factors: its use ofa long history, and its lack of lexicalization.
Thegrid looks at the previous six sentences, which dif-ferentiates it from the IBM model and from Elsnerand Charniak (2008b), which treats each pair of sen-tences independently.
Using this long history helpsto distinguish important nouns from unimportantones better than frequency alone.
We suspect thatour lexicalized models, IBM and the topical entitygrid, are hampered by poor parameter settings, sincetheir parameters were learned on FISHER rather thanIRC chat.
In particular, we believe this explains whythe topical entity grid, which slightly outperformedthe entity grid on SWBD, is much worse here.6.2 Full disentanglementRunning our tabu search algorithm on the full disen-tanglement task yields disappointing results.
Accu-racies on the #LINUX dataset are not only worse thanprevious work, but also worse than simple baselineslike creating one thread for each speaker.
The modelnds far too many threads?
it detects over 300, whenthe true number is about 81 (averaging over annota-tions).
This appears to be related to biases in ourchat-specic models as well as in the entity grid;the time model (which generates gaps between adja-cent sentences) and the speaker model (which usesa CRP) both assign probability 1 to single-utteranceconversations.
The entity grid also has a bias towardshort conversations, because unseen entities are em-pirically more likely to occur toward the beginningof a conversation than in the middle.A major weakness in our model is that we aimonly to maximize coherence of the individual con-versations, with no prior on the likely length or num-ber of conversations that will appear in the tran-1186script.
This allows the model to create far too manyconversations.
Integrating a prior into our frame-work is not straightforward because we currentlytrain our mixture to maximize single-utterance dis-entanglement performance, and the prior is not use-ful for this task.We experimented with xing parts of the tran-script to the solution obtained by Elsner and Char-niak (2008b), then using tabu search to ll in thegaps.
This constrains the number of conversationsand their approximate positions.
With this structurein place, we were able to obtain scores comparableto Elsner and Charniak (2008b), but not improve-ments.
It appears that our performance increase onsingle-sentence disentanglement does not transfer tothis task because of cascading errors and the neces-sity of using external constraints.7 ConclusionsWe demonstrate that several popular models of lo-cal coherence transfer well to the conversational do-main, suggesting that they do indeed capture coher-ence in general rather than specic conventions ofnewswire text.
However, their performance acrosstasks is not as stable; in particular, models whichuse less history information are worse for disentan-glement.Our results study suggest that while sophisticatedcoherence models can potentially contribute to dis-entanglement, they would benet greatly from im-proved low-level resources for internet chat.
Bet-ter parsing, or at least NP chunking, would help formodels like the entity grid which rely on syntacticrole information.
Larger training sets, or some kindof transfer learning, could improve the learning oftopics and other lexical parameters.
In particular,our results on SWBD data conrm the conjecture of(Adams, 2008) that LDA topic modeling is in prin-ciple a useful tool for disentanglement?
we believe atopic-based model could also work on IRC chat, butwould require a better set of extracted topics.
Withbetter parameters for these models and the integra-tion of a prior, we believe that our good performanceon SWBD and single-utterance disentanglement forIRC can be extended to full-scale disentanglementof IRC.AcknowledgementsWe are extremely grateful to Regina Barzilay, MarkJohnson, Rebecca Mason, Ben Swanson and NealFox for their comments, to Craig Martell for theNPS chat datasets and to three anonymous review-ers.
This work was funded by a Google Fellowshipfor Natural Language Processing.ReferencesPaige H. Adams.
2008.
Conversation Thread Extractionand Topic Detection in Text-based Chat.
Ph.D. thesis,Naval Postgraduate School.David Aldous.
1985.
Exchangeability and related top-ics.
In Ecole d'Ete de Probabilities de Saint-FlourXIII 1983, pages 1?198.
Springer.Paul M. Aoki, Matthew Romaine, Margaret H. Szyman-ski, James D. Thornton, Daniel Wilson, and AllisonWoodruff.
2003.
The mad hatter's cocktail party: asocial mobile audio space supporting multiple simul-taneous conversations.
In CHI '03: Proceedings of theSIGCHI conference on Human factors in computingsystems, pages 425?432, New York, NY, USA.
ACMPress.Paul M. Aoki, Margaret H. Szymanski, Luke D.Plurkowski, James D. Thornton, Allison Woodruff,and Weilie Yi.
2006.
Where's the ?party?
in ?multi-party??
: analyzing the structure of small-group socia-ble talk.
In CSCW '06: Proceedings of the 2006 20thanniversary conference on Computer supported coop-erative work, pages 393?402, New York, NY, USA.ACM Press.Regina Barzilay and Mirella Lapata.
2005.
Modeling lo-cal coherence: an entity-based approach.
In Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics (ACL'05).Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applicationsto generation and summarization.
In HLT-NAACL2004: Proceedings of the Main Conference, pages113?120.David Blei, Andrew Y. Ng, and Michael I. Jordan.2001.
Latent Dirichlet alocation.
Journal of MachineLearning Research, 3:2003.Eugene Charniak and Micha Elsner.
2009.
EM worksfor pronoun anaphora resolution.
In Proceedings ofEACL, Athens, Greece.Harr Chen, S.R.K.
Branavan, Regina Barzilay, andDavid R. Karger.
2009.
Global models of documentstructure using latent permutations.
In Proceedingsof Human Language Technologies: The 2009 Annual1187Conference of the North American Chapter of the As-sociation for Computational Linguistics, pages 371?379, Boulder, Colorado, June.
Association for Com-putational Linguistics.Jacob Eisenstein and Regina Barzilay.
2008.
Bayesianunsupervised topic segmentation.
In EMNLP, pages334?343.Micha Elsner and Eugene Charniak.
2008a.Coreference-inspired coherence modeling.
InProceedings of ACL-08: HLT, Short Papers, pages41?44, Columbus, Ohio, June.
Association forComputational Linguistics.Micha Elsner and Eugene Charniak.
2008b.
You talk-ing to me?
a corpus and algorithm for conversationdisentanglement.
In Proceedings of ACL-08: HLT,pages 834?842, Columbus, Ohio, June.
Associationfor Computational Linguistics.Peter Foltz, Walter Kintsch, and Thomas Landauer.1998.
The measurement of textual coherence withlatent semantic analysis.
Discourse Processes,25(2&3):285?307.Jennifer Foster.
2010.
?cba to check the spelling?
: In-vestigating parser performance on discussion forumposts.
In Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages381?384, Los Angeles, California, June.
Associationfor Computational Linguistics.Niyu Ge, John Hale, and Eugene Charniak.
1998.
A sta-tistical approach to anaphora resolution.
In Proceed-ings of the Sixth Workshop on Very Large Corpora,pages 161?171, Orlando, Florida.
Harcourt Brace.Fred Glover and Manuel Laguna.
1997.
Tabu Search.University of Colorado at Boulder.Barbara J. Grosz, Aravind K. Joshi, and Scott Weinstein.1995.
Centering: A framework for modeling the lo-cal coherence of discourse.
Computational Linguis-tics, 21(2):203?225.Simon Haykin and Zhe Chen.
2005.
The Cocktail PartyProblem.
Neural Computation, 17(9):1875?1902.Nikiforos Karamanis, Massimo Poesio, Chris Mellish,and Jon Oberlander.
2004.
Evaluating centering-based metrics of coherence.
In ACL, pages 391?398.Mirella Lapata and Regina Barzilay.
2005.
Automaticevaluation of text coherence: Models and representa-tions.
In IJCAI, pages 1085?1090.Mirella Lapata.
2003.
Probabilistic text structuring: Ex-periments with sentence ordering.
In Proceedings ofthe annual meeting of ACL, 2003.Mirella Lapata.
2006.
Automatic evaluation of informa-tion ordering: Kendall's tau.
Computational Linguis-tics, 32(4):1?14.Gideon Mann, Ryan McDonald, Mehryar Mohri, NathanSilberman, and Dan Walker.
2009.
Efcient large-scale distributed training of conditional maximum en-tropy models.
In Y. Bengio, D. Schuurmans, J. Laf-ferty, C. K. I. Williams, and A. Culotta, editors, Ad-vances in Neural Information Processing Systems 22,pages 1231?1239.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceed-ings of the Human Language Technology Conferenceof the NAACL, Main Conference, pages 152?159.David McClosky, Eugene Charniak, and Mark Johnson.2010.
Automatic domain adaptation for parsing.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, pages 28?36,Los Angeles, California, June.
Association for Com-putational Linguistics.Eleni Miltsakaki and K. Kukich.
2004.
Evaluation of textcoherence for electronic essay scoring systems.
Nat.Lang.
Eng., 10(1):25?55.Neville Moray.
1959.
Attention in dichotic listening: Af-fective cues and the inuence of instructions.
Quar-terly Journal of Experimental Psychology, 11(1):56?60.Ani Nenkova and Kathleen McKeown.
2003.
Refer-ences to named entities: a corpus study.
In NAACL'03, pages 70?72.Malvina Nissim.
2006.
Learning information status ofdiscourse entities.
In Proceedings of EMNLP, pages94?102, Morristown, NJ, USA.
Association for Com-putational Linguistics.Jacki O'Neill and David Martin.
2003.
Text chat in ac-tion.
In GROUP '03: Proceedings of the 2003 inter-national ACM SIGGROUP conference on Supportinggroup work, pages 40?49, New York, NY, USA.
ACMPress.Emily Pitler and Ani Nenkova.
2008.
Revisiting read-ability: A unied framework for predicting text qual-ity.
In Proceedings of the 2008 Conference on Empir-ical Methods in Natural Language Processing, pages186?195, Honolulu, Hawaii, October.
Association forComputational Linguistics.Massimo Poesio, Mijail Alexandrov-Kabadjov, RenataVieira, Rodrigo Goulart, and Olga Uryupina.
2005.Does discourse-new detection help denite descriptionresolution?
In Proceedings of the Sixth InternationalWorkshop on Computational Semantics, Tillburg.Amruta Purandare and Diane J. Litman.
2008.
Analyz-ing dialog coherence using transition patterns in lexi-cal and semantic features.
In FLAIRS Conference'08,pages 195?200.Dou Shen, Qiang Yang, Jian-Tao Sun, and Zheng Chen.2006.
Thread detection in dynamic text message1188streams.
In SIGIR '06: Proceedings of the 29th annualinternational ACM SIGIR conference on Research anddevelopment in information retrieval, pages 35?42,New York, NY, USA.
ACM.Radu Soricut and Daniel Marcu.
2006.
Discourse gener-ation using utility-trained coherence models.
In Pro-ceedings of the Association for Computational Lin-guistics Conference (ACL-2006).Lidan Wang and Douglas W. Oard.
2009.
Context-basedmessage expansion for disentanglement of interleavedtext conversations.
In Proceedings of NAACL-09.1189
