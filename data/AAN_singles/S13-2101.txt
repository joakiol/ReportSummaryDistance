Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 603?607, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsCU : Computational Assessment of Short Free Text Answers - A Tool forEvaluating Students?
UnderstandingIfeyinwa OkoyeInstitute of Cognitive ScienceDept.
of Computer ScienceUniversity of ColoradoBoulder, CO 80309, USAokoye@colorado.eduSteven BethardInstitute of Cognitive ScienceDept.
of Computer ScienceUniversity of ColoradoBoulder, CO 80309, USAbethard@colorado.eduTamara SumnerInstitute of Cognitive ScienceDept.
of Computer ScienceUniversity of ColoradoBoulder, CO 80309, USAsumner@colorado.eduAbstractAssessing student understanding by evaluat-ing their free text answers to posed questionsis a very important task.
However, manually,it is time-consuming and computationally, it isdifficult.
This paper details our shallow NLPapproach to computationally assessing studentfree text answers when a reference answer isprovided.
For four out of the five test sets, oursystem achieved an overall accuracy above themedian and mean.1 IntroductionAssessing student understanding is one of the holygrails of education (Redecker et al 2012).
If we(teachers, tutors, intelligent tutors, potential employ-ers, parents and school administrators) know whatand how much a student knows, then we know whatthe student still needs to learn.
And then, can ef-ficiently and effectively educate the student.
How-ever, the task of assessing what exactly a student un-derstands about a particular topic can be expensive,difficult and subjective.Using multiple choice questionnaires is one ofthe most prevalent forms of assessing student under-standing because it is easy and fast, both manuallyand computationally.
However there has been a lotof pushback from educators about the validity of re-sults gotten from multiple choice questionnaires.Assessing student understanding by evaluatingstudent free text answers either written or spoken isone of the preferred alternatives to multiple choicequestionnaires.
As an assessment tool, free text an-swers can illuminate what and how much a studentknows since the student is forced to recall terms andmake connections between those terms rather thanjust picking one out of several options.
However,assessing free text answers manually is tedious, ex-pensive and time-consuming, hence the search for acomputational option.There are three main issues that can limit the com-putational approach and corresponding performancewhen assessing free text answers: (1) the unit ofassessment, (2) the reference and (3) the level ofassessment.
The unit of assessment can be words,facets, phrases, sentences, short answers or essays.The reference is the correct answer and what isbeing compared to the student answer.
Most re-searchers generate the reference manually (Noorbe-hbahani and Kardan, 2011; Graesser et al 2004) butsome have focused on automatically generating thereference (Ahmad, 2009).
The level of assessmentcan be coarse with 2 categories such as correct andincorrect or more finer-grained with up to 19 cate-gories as in (Ahmad, 2009).
In general, the finer-grained assessments are more difficult to assess.2 The Student Response Analysis TaskThe student response analysis task was posed as fol-lows: Given a question, a known correct/referenceanswer and a 1 or 2 sentence student answer, classifythe student answer into two, three or five categories.The two categories were correct and incorrect; thethree categories were correct, contradictory and in-correct; while the five categories were correct, par-tially correct but incomplete, contradictory, irrele-vant and not in the domain (Dzikovska et al 2013).We chose to work on the 2-way response task only603because for our application, we need to simply knowif a student answer is correct or incorrect.
Our ap-plication is an interactive essay-based personalizedlearning environment (Bethard et al 2012).The overarching goal of our application is to cre-ate a scalable online service that recommends re-sources to users based on the their conceptual under-standing expressed in an essay or short answer form.Our application automatically constructs a domainknowledge base from digital library resources andidentifies the core concepts in the domain knowl-edge base.It detects flaws and gaps in users?
sci-ence knowledge and recommends digital library re-sources to address users?
misconceptions and knowl-edge gaps.
The gaps are detected by identifying thecore concepts which the user has not discussed.
Theflaws (incorrect understanding/misconceptions) arecurrently being identified by a process of (1) seg-menting a student essay into sentences, (2) align-ing the student sentence to a sentence in the domainknowledge base and (3) using the system we devel-oped for the student response analysis task to deter-mine if the student sentence is correct or incorrect.The development of our misconception detectionalgorithm has been limited by the alignment task.However, with the data set from the student responseanalysis task containing correct alignments, we hopeto be able to use it to make improvements to ourmisconception detection algorithm.
We discuss ourcurrent misconception detection system below.3 System DescriptionOur system mainly exploits shallow NLP tech-niques, in particular text overlap, to see how muchwe can gain from using a simple system and howmuch more some more semantic features could addto the simple system.
Although we have access tothe question which a 1-2 sentence student answercorresponds to, we chose not to use that in our sys-tem because in our application we do not have ac-cess to that information.
We were trying to build asystem that would work in our current essay-basedapplication.Some of the student answers in the dataset have aparticular reference answer which they match.
How-ever, we do not make use of this information in oursystem either.
We assume that for a particular ques-tion, all the corresponding reference answers can beused to determine the correctness of any of the stu-dent answers.3.1 FeaturesThe features we use are:1.
CosineSimilarity : This is the average cosinesimilarity (Jurafsky and James, 2000) betweena student answer vector and all the correspond-ing reference answer vectors.
The vectors arebased on word counts.
The words were low-ercased and included stopwords and punctua-tions.2.
CosineSimilarityNormalized : This is the av-erage cosine similarity between a student an-swer vector and all the corresponding referenceanswer vectors, with the word counts withinthe vectors divided by the word counts in Gi-gaword, a background corpus.
We dividedthe raw counts by the counts in Gigaword toensure that punctuations, stopwords and othernon-discriminatory words do not artificially in-crease the cosine similarity.3.
UnigramRefStudent : This is the average un-igram coverage of the reference answers by astudent answer.
To calculate this, the studentanswer and all the corresponding reference an-swers are tokenized into unigrams.
Next, foreach reference answer, we count the number ofunigrams in the reference answer that are con-tained in the student answer and divide it by thenumber of unigrams in the reference answer.The value we get for this feature, is the aver-age over all the reference answers.4.
UnigramStudentRef : This is the average uni-gram coverage of the student answer by the ref-erence answers.
To calculate this, the studentanswer and all the corresponding reference an-swers are tokenized into unigrams.
Next, foreach reference answer, we count the numberof unigrams in the student answer that are con-tained in the reference answer and divide it bythe number of unigrams in the student answer.The value we get for this feature, is the averageover all the reference answers.6045.
BigramRefStudent : This is similar to the Un-igramRefStudent feature, but using bigrams.6.
BigramStudentRef : This is similar to the Un-igramStudentRef feature, but using bigrams.7.
LemmaRefStudent : This is similar to the Un-igramRefStudent feature, but in this case, thelemmas are used in place of words.8.
LemmaStudentRef : This is similar to the Un-igramStudentRef feature, but in this case, thelemmas are used in place of words.9.
UnigramPosRefStudent : This is similar tothe UnigramRefStudent feature, but we usepart-of-speech unigrams for this feature inplace of word unigrams.10.
UnigramPosStudentRef : This is similar tothe UnigramStudentRef feature, but we usepart-of-speech unigrams for this feature inplace of word unigrams.11.
BigramPosRefStudent : This is similar to theBigramRefStudent feature, but we use part-of-speech bigrams for this feature in place of wordunigrams.12.
BigramPosStudentRef : This is similar to theBigramStudentRef feature, but we use part-of-speech bigrams for this feature in place of wordunigrams.3.2 ImplementationWe used the ClearTK (Ogren et al 2008) toolkitwithin Eclipse to extract features from the studentand reference sentences.
We trained a LibSVM(Chang and Lin, 2011) binary classifier to classify afeature vector into two classes, correct or incorrect.We used the default parameters for LibSVM exceptfor the cost parameter, for which we tried differentvalues.
However, the default value of 1 gave us thebest result on the training set.
Our two runs/systemsare essentially the same system but with a cost pa-rameter of 1 and 10.4 ResultsThe Student Response Analysis Task overall re-sult can be found in the Task description paper(Dzikovska et al 2013).
The CU system achieveda ranking of above the mean and median for fourof the five different test sets.
We perfomed belowthe mean and median on the sciEntsBank unseen an-swers.
The accuracy result for the test data is shownin Table 4.
The results on our training data anda breakdown of the contribution of each feature isshown in Table 5.
In Table 5 ALL refers to all thefeatures while ALL-CosineSimilarity is all the fea-tures excluding the CosineSimilarity feature.Systembeetleun-seenan-swersbeetleun-seenques-tionssciEntsBankun-seenan-swerssciEntsBankun-seenques-tionssciEntsBankun-seendo-mainsCUrun10.786 0.718 0.656 0.674 0.693CUrun20.784 0.717 0.654 0.671 0.691Table 1: Overall Accuracy results for CU system on thetest Data5 DiscussionAs can be seen from Table 4 and further elaboratedon in (Dzikovska et al 2013), there were two maindatasets, Beetle and SciEntsBank.
The Beetle dataset has multiple reference answer per question whilethe SciEntsBank has one reference answer per ques-tion.
Our system did better on the beetle data setthan the SciEntsBank data set, both during devel-opment and on the final test sets.
This leads us tobelieve that our system will do well when there aremultiple reference answers rather than just one.We analyzed the training data to understandwhere our system was failing and what we could doto make it better.
We tried removing stopwords be-fore constructing the feature vectors but that madethe results worse.
Here are two examples where re-moving the stopwords will make it impossible to as-certain the validity of the student answer:?
It was connected.
becomes connected605?
It will work because that is closing the switch.becomes work closing switchBecause the student answers are free text and usepronouns in place of the nouns that were in the ques-tion, the stop words are important to provide context.Feature Type Beetle& sci-EntsBank1 ALL 0.7032 ALL - CosineSimilarity 0.7023 ALL - CosineSimilari-tyNormalized0.7004 ALL - UnigramRefStudent 0.7025 ALL - UnigramStudentRef 0.7016 ALL - BigramRefStudent 0.7027 ALL - BigramStudentRef 0.6998 ALL - LemmaRefStudent 0.7019 ALL - LemmaStudentRef 0.70010 ALL - UnigramPosRefStu-dent0.70311 ALL - UnigramPosStuden-tRef0.70312 ALL - BigramPosRefStu-dent0.70213 ALL - BigramPosStuden-tRef0.702Table 2: Accuracy results for 5X cross validation on thetraining dataCurrently, we are working on extracting andadding several features that we did not use for thetask due to time constraints, to see if they improveour result.
Some of the things we are working onare:1.
Resolving CoreferenceWe will use the current state-of-art coreferencesystem and assume that the question precedesthe student answer in a paragraph when resolv-ing coreference.2.
Compare main predicatesThe question is how to assign a value to the se-mantic similarity between the main predicates.If the predicates are separate and connect, thenthere should be a way to indicate that the men-tion of one of them in the reference, precludesthe validity of the student answer being correctif it mentions the other.
However, we also haveto take negation into account here.
not sepa-rated and connected should be marked as verysimilar if not equal.
We plan to include the al-gorithm from the best system in the semanticsimilarity task to our current system.3.
Compare main subject and object from asyntactic parse or the numbered argumentsin semantic role label argumentsWe have to resolve coreference for this to workwell.
And again, we run into the problem ofhow to assign a semantic similarity value to twowords that might not share the same synset inontologies such as Wordnet.4.
Optimize parameters and explore other clas-sifiers Throughout developing and testing oursystem, we used only the LibSVM classifierand only optimized the cost parameter.
How-ever, there might be a different classifier orset of options that can model the data better.We hope to run through most of the classifiersavailable and see if using a different one, withdifferent options improves our accuracy.6 ConclusionWe have shown that there is value in using shallowNLP features to judge the validity of free answer textwhen the reference answers are given.
However,looking at the sentences that our system labeled ascorrect and the gold standard incorrect or vice versa,it is clear that we have to delve into more seman-tic features if we want our system to be more accu-rate.
We hope to keep working on this task in sub-sequent years to ensure continuous improvements insystems that can assess student knowledge by eval-uating free answer texts.
Such systems will be ableto give students the formative feedback they needto help them learn better.
In addition, such systemswill provide teachers, intelligent tutors and adminis-trators with feedback about student knowledge, so asto help them adapt their curriculum, teaching and tu-toring methods to better serve students?
knowledgeneeds.606ReferencesFaisal Ahmad.
2009.
Generating conceptually personal-ized interactions for educational digital libraries usingconcept maps.
Ph.D. thesis, University of Colorado atBoulder.Steven Bethard, Haojie Hang, Ifeyinwa Okoye, James HMartin, Md Arafat Sultan, and Tamara Sumner.
2012.Identifying science concepts and student misconcep-tions in an interactive essay writing tutor.
In Proceed-ings of the Seventh Workshop on Building EducationalApplications Using NLP, pages 12?21.
Association forComputational Linguistics.Chih-Chung Chang and Chih-Jen Lin.
2011.
Libsvm:a library for support vector machines.
ACM Trans-actions on Intelligent Systems and Technology (TIST),2(3):27.Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,Claudia Leacock, Danilo Giampiccolo, Luisa Ben-tivogli, Peter Clark, Ido Dagan, and Hoa Trang Dang.2013.
Semeval-2013 task 7: The joint student re-sponse analysis and 8th recognizing textual entailmentchallenge.
In *SEM 2013: The First Joint Conferenceon Lexical and Computational Semantics, Atlanta,Georgia, USA, 13-14 June.
Association for Compu-tational Linguistics.Arthur Graesser, Shulan Lu, George Jackson, HeatherMitchell, Mathew Ventura, Andrew Olney, and MaxLouwerse.
2004.
AutoTutor: A tutor with dialoguein natural language.
Behavior Research Methods,36:180?192.Daniel Jurafsky and H James.
2000.
Speech and lan-guage processing an introduction to natural languageprocessing, computational linguistics, and speech.F Noorbehbahani and AA Kardan.
2011.
The automaticassessment of free text answers using a modified bleualgorithm.
Computers & Education, 56(2):337?345.Philip V Ogren, Philipp G Wetzler, and Steven J Bethard.2008.
Cleartk: A uima toolkit for statistical natu-ral language processing.
Towards Enhanced Inter-operability for Large HLT Systems: UIMA for NLP,page 32.Christine Redecker, Yves Punie, and Anusca Ferrari.2012.
eassessment for 21st century learning and skills.In 21st Century Learning for 21st Century Skills,pages 292?305.
Springer.607
