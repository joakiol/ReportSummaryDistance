Proceedings of the Fourteenth Conference on Computational Natural Language Learning: Shared Task, pages 40?47,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsMemory-Based Resolution of In-Sentence Scopes of Hedge CuesRoser Morante, Vincent Van Asch, Walter DaelemansCLiPS - University of AntwerpPrinsstraat 13B-2000 Antwerpen, Belgium{Roser.Morante,Walter.Daelemans,Vincent.VanAsch}@ua.ac.beAbstractIn this paper we describe the machinelearning systems that we submitted to theCoNLL-2010 Shared Task on Learning toDetect Hedges and Their Scope in Nat-ural Language Text.
Task 1 on detect-ing uncertain information was performedby an SVM-based system to process theWikipedia data and by a memory-basedsystem to process the biological data.Task 2 on resolving in-sentence scopes ofhedge cues, was performed by a memory-based system that relies on informationfrom syntactic dependencies.
This systemscored the highest F1 (57.32) of Task 2.1 IntroductionIn this paper we describe the machine learningsystems that CLiPS1 submitted to the closed trackof the CoNLL-2010 Shared Task on Learning toDetect Hedges and Their Scope in Natural Lan-guage Text (Farkas et al, 2010).2 The task con-sists of two subtasks: detecting whether a sentencecontains uncertain information (Task 1), and re-solving in-sentence scopes of hedge cues (Task 2).To solve Task 1, systems are required to classifysentences into two classes, ?Certain?
or ?Uncer-tain?, depending on whether the sentence containsfactual or uncertain information.
Three annotatedtraining sets are provided: Wikipedia paragraphs(WIKI), biological abstracts (BIO-ABS) and bio-logical full articles (BIO-ART).
The two test setsconsist of WIKI and BIO-ART data.Task 2 requires identifying hedge cues and find-ing their scope in biomedical texts.
Finding thescope of a hedge cue means determining at sen-tence level which words in the sentence are af-fected by the hedge cue.
For a sentence like the1Web page: http://www.clips.ua.ac.be2Web page: http://www.inf.u-szeged.hu/rgai/conll2010stone in (1) extracted from the BIO-ART trainingcorpus, systems have to identify likely and sug-gested as hedge cues, and they have to find thatlikely scopes over the full sentence, and that sug-gested scopes over by the role of murine MIB inTNF?
signaling.
A scope will be correctly re-solved only if both the cue and the scope are cor-rectly identified.
(1) <xcope id=2> The conservation from Drosophila tomammals of these two structurally distinct butfunctionally similar E3 ubiquitin ligases is <cueref=2>likely</cue> to reflect a combination ofevolutionary advantages associated with: (i)specialized expression pattern, as evidenced by thecell-specific expression of the neur gene in sensoryorgan precursor cells [52]; (ii) specialized function, as<xcope id=1> <cue ref=1>suggested</cue> by therole of murine MIB in TNF?
signaling</xcope> [32];(iii) regulation of protein stability, localization, and/oractivity</xcope>.Systems are to be trained on BIO-ABS andBIO-ART and tested on BIO-ART.
Example (1)shows that sentences in the BIO-ART dataset canbe quite complex because of their length, becauseof their structure - very often they contain enu-merations, and because they contain bibliographicreferences and references to tables and figures.Handling these phenomena is necessary to detectscopes correctly in the setting of this task.
Notethat the scope of suggested above does not includethe bibliographic reference [32], whereas the scopeof likely includes all the bibliographic references,and that the scope of likely does not include thefinal punctuation mark.In the case of the BIO data, we approach Task1 as a prerequisite for Task 2.
Therefore we treatthem as two consecutive classification tasks: a firstone that consists of classifying the tokens of a sen-tence as being at the beginning of a hedge sig-nal, inside or outside.
This allows the system tofind multiword hedge cues.
We tag a sentence asuncertain if at least a hedge cue is found in thesentence.
The second classification task consists40of classifying the tokens of a sentence as beingthe first element of the scope, the last, or nei-ther.
This happens as many times as there arehedge cues in the sentence.
The two classificationtasks are implemented using memory-based learn-ers.
Memory-based language processing (Daele-mans and van den Bosch, 2005) is based on theidea that NLP problems can be solved by reuse ofsolved examples of the problem stored in memory.Given a new problem, the most similar examplesare retrieved, and a solution is extrapolated fromthem.Section 2 is devoted to related work.
In Sec-tion 3 we describe how the data have been prepro-cessed.
In Section 4 and Section 5 we present thesystems that perform Task 1 and Task 2.
Finally,Section 6 puts forward some conclusions.2 Related workHedging has been broadly treated from a theoret-ical perspective.
The term hedging is originallydue to Lakoff (1972).
Palmer (1986) defines aterm related to hedging, epistemic modality, whichexpresses the speaker?s degree of commitment tothe truth of a proposition.
Hyland (1998) focusesspecifically on scientific texts.
He proposes a prag-matic classification of hedge expressions based onan exhaustive analysis of a corpus.
The catalogueof hedging cues includes modal auxiliaries, epis-temic lexical verbs, epistemic adjectives, adverbs,nouns, and a variety of non?lexical cues.
Lightet al (2004) analyse the use of speculative lan-guage in MEDLINE abstracts.
Some NLP appli-cations incorporate modality information (Fried-man et al, 1994; Di Marco and Mercer, 2005).As for annotated corpora, Thompson et al (2008)report on a list of words and phrases that expressmodality in biomedical texts and put forward a cat-egorisation scheme.
Additionally, the BioScopecorpus (Vincze et al, 2008) consists of a collec-tion of clinical free-texts, biological full papers,and biological abstracts annotated with negationand speculation cues and their scope.Although only a few pieces of research have fo-cused on processing negation, the two tasks of theCoNLL-2010 Shared Task have been addressedpreviously.
As for Task 1, Medlock and Briscoe(2007) provide a definition of what they considerto be hedge instances and define hedge classifi-cation as a weakly supervised machine learningtask.
The method they use to derive a learningmodel from a seed corpus is based on iterativelypredicting labels for unlabeled training samples.They report experiments with SVMs on a datasetthat they make publicly available3.
The experi-ments achieve a recall/precision break even point(BEP) of 0.76.
They apply a bag-of-words ap-proach to sample representation.
Medlock (2008)presents an extension of this work by experiment-ing with more features (part-of-speech, lemmas,and bigrams).
With a lemma representation thesystem achieves a peak performance of 0.80 BEP,and with bigrams of 0.82 BEP.
Szarvas (2008) fol-lows Medlock and Briscoe (2007) in classifyingsentences as being speculative or non-speculative.Szarvas develops a MaxEnt system that incor-porates bigrams and trigrams in the feature rep-resentation and performs a complex feature se-lection procedure in order to reduce the numberof keyword candidates.
It achieves up to 0.85BEP and 85.08 F1 by using an external dictio-nary.
Kilicoglu and Bergler (2008) apply a lin-guistically motivated approach to the same clas-sification task by using knowledge from existinglexical resources and incorporating syntactic pat-terns.
Additionally, hedge cues are weighted byautomatically assigning an information gain mea-sure and by assigning weights semi?automaticallydepending on their types and centrality to hedging.The system achieves results of 0.85 BEP.As for Task 2, previous work (Morante andDaelemans, 2009; O?zgu?r and Radev, 2009) hasfocused on finding the scope of hedge cues inthe BioScope corpus (Vincze et al, 2008).
Bothsystems approach the task in two steps, identify-ing the hedge cues and finding their scope.
Themain difference between the two systems is thatMorante and Daelemans (2009) perform the sec-ond phase with a machine learner, whereas O?zgurand Radev (2009) perform the second phase witha rule-based system that exploits syntactic infor-mation.The approach to resolving the scopes of hedgecues that we present in this paper is similar tothe approach followed in Morante and Daelemans(2009) in that the task is modelled in the sameway.
A difference between the two systems is thatthis system uses only one classifier to solve Task2, whereas the system described in Morante andDaelemans (2009) used three classifiers and a met-3Available athttp://www.benmedlock.co.uk/hedgeclassif.html.41alearner.
Another difference is that the system inMorante and Daelemans (2009) used shallow syn-tactic features, whereas this system uses featuresfrom both shallow and dependency syntax.
A thirddifference is that that system did not use a lexiconof cues, whereas this system uses a lexicon gener-ated from the training data.3 PreprocessingAs a first step, we preprocess the data in orderto extract features for the machine learners.
Weconvert the xml files into a token-per-token rep-resentation, following the standard CoNLL for-mat (Buchholz and Marsi, 2006), where sentencesare separated by a blank line and fields are sepa-rated by a single tab character.
A sentence consistsof a sequence of tokens, each one starting on a newline.The WIKI data are processed with the MemoryBased Shallow Parser (MBSP) (Daelemans andvan den Bosch, 2005) in order to obtain lemmas,part-of-speech (PoS) tags, and syntactic chunks,and with the MaltParser (Nivre, 2006) in order toobtain dependency trees.
The BIO data are pro-cessed with the GDep parser (Sagae and Tsujii,2007) in order to get the same information.# WORD LEMMA PoS CHUNK NE D LABEL C S1 The The DT B-NP O 3 NMOD O O O2 structural structural JJ I-NP O 3 NMOD O O O3 evidence evidence NN I-NP O 4 SUB O O O4 lends lend VBZ B-VP O 0 ROOT B F O5 strong strong JJ B-NP O 6 NMOD I O O6 support support NN I-NP O 4 OBJ I O O7 to to TO B-PP O 6 NMOD O O O8 the the DT B-NP O 11 NMOD O O O9 inferred inferred JJ I-NP O 11 NMOD B O F10 domain domain NN I-NP O 11 NMOD O O O11 pair pair NN I-NP O 7 PMOD O L L12 , , , O O 4 P O O O13 resulting result VBG B-VP O 4 VMOD O O O14 in in IN B-PP O 13 VMOD O O O15 a a DT B-NP O 18 NMOD O O O16 high high JJ I-NP O 18 NMOD O O O17 confidence confidence NN I-NP O 18 NMOD O O O18 set set NN I-NP O 14 PMOD O O O19 of of IN B-PP O 18 NMOD O O O20 domain domain NN B-NP O 21 NMOD O O O21 pairs pair NNS I-NP O 19 PMOD O O O22 .
.
.
O O 4 P O O OTable 1: Preprocessed sentence.Table 1 shows a preprocessed sentence with thefollowing information per token: the token num-ber in the sentence, word, lemma, PoS tag, chunktag, named entity tag, head of token in the depen-dency tree, dependency label, cue tag, and scopetags separated by a space, for as many cues asthere are in the sentence.In order to check whether the conversion fromthe xml format to the CoNLL format is a sourceof error propagation, we convert the gold CoNLLfiles into xml format and we run the scorer pro-vided by the task organisers.
The results obtainedare listed in Table 2.Task 1 Task 2WIKI BIO-ART BIO-ABS BIO-ART BIO-ABSF1 100.00 100.00 100.00 99.10 99.66Table 2: Evaluation of the conversion from xml toCoNLL format.4 Task 1: Detecting uncertaininformationIn Task 1 sentences have to be classified as con-taining uncertain or unreliable information or not.The task is performed differently for theWIKI andfor the BIO data, since we are interested in findingthe hedge cues in the BIO data, as a first step to-wards Task 2.4.1 Wikipedia system (WIKI)In the WIKI data a sentence is marked as uncertainif it contains at least one weasel, or cue for uncer-tainty.
The list of weasels is quite extensive andcontains a high number of unique occurrences.
Forexample, the training data contain 3133 weaselsand 1984 weasel types, of which 63% are unique.This means that a machine learner will have diffi-culties in performing the classification task.
Evenso, some generic structures can be discoveredin the list of weasels.
For example, the differ-ent weasels A few people and A few sprawlinggrounds follow a pattern.
We manually select the42 most frequent informative tokens4 from the listof weasels in the training partition.
In the remain-der of this section we will refer to these tokens asweasel cues.Because of the wide range of weasels, we optfor predicting the (un)certainty of a sentence, in-stead of identifying the weasels.
The sentenceclassification is done in three steps: instance cre-ation, SVM classification and sentence labeling.4Weasel cues: few, number, variety, bit, great, majority,range, variety, all, almost, arguably, certain, commonly, gen-erally, largely, little, many, may, most, much, numerous, of-ten, one, other, others, perhaps, plenty of, popular, possibly,probably, quite, relatively, reportedly, several, some, suggest,there be, the well-known, various, very, wide, widely.424.1.1 Instance creationAlthough we only want to predict the (un)certaintyof a sentence as a whole, we classify every tokenin the sentence separately.
After parsing the datawe create one instance per token, with the excep-tion of tokens that have a part-of-speech from thelist: #, $, :, LS, RP, UH, WP$, or WRB.
The ex-clusion of these tokens is meant to simplify theclassification task.The features used by the system during classifi-cation are the following:?
About the token: word, lemma, PoS tag, chunk tag,dependency head, and dependency label.?
About the token context: lemma, PoS tag, chunk tagand dependency label of the two tokens to the left andright of the token in focus in the string of words of thesentence.?
About the weasel cues: a binary marker that indicateswhether the token in focus is a weasel cue or not, and anumber defining the number of weasel cues that thereare in the entire sentence.These instances with 24 non-binary featurescarry the positive class label if the sentence is un-certain.
We use a binarization script that rewritesthe instance to a format that can be used with asupport vector machine and during this process,feature values that occur less than 2 times areomitted.4.1.2 SVM classificationTo label the instances of the unseen data we useSVMlight (Joachims, 2002).
We performed someexperiments with different settings and decidedto only change the type of kernel from the de-fault linear kernel to a polynomial kernel.
Forthe Wikipedia training data, the training of the246,876 instances with 68417 features took ap-proximately 22.5 hours on a 32 bit, 2.2GHz, 2GBRAM Mac OS X machine.4.1.3 Sentence labelingIn this last step, we collect all instances from thesame sentence and inspect the predicted labels forevery token.
If more than 5% of the instances aremarked as uncertain, the whole sentence is markedas uncertain.
The idea behind the setup is thatmany tokens are very ambiguous in respect to un-certainty because they do not carry any informa-tion.
Fewer tokens are still ambiguous, but containsome information, and a small set of tokens are al-most unambiguous.
This small set of informativetokens does not have to coincide with weasels norweasels cues.
The result is that we cannot predictthe actual weasels in a sentence, but we get an in-dication of the presence of tokens that are commonin uncertain sentences.4.2 Biological system (BIO)The system that processes the BIO data is differentfrom the system that processes theWIKI data.
TheBIO system uses a classifier that predicts whethera token is at the beginning of a hedge signal, insideor outside.
So, instances represent tokens.
The in-stance features encode the following information:?
About the token: word, lemma, PoS tag, chunk tag, anddependency label.?
About the context to the left and right in the string ofwords of the sentence: word of the two previous andthree next tokens, lemma and dependency label of pre-vious and next tokens, deplabel, and chunk tag and PoSof next token.
A binary feature indicating whether thenext token has an SBAR chunk tag.?
About the context in the syntactic dependency tree:chain of PoS tags, chunk tags and dependency labelof children of token; word, lemma, PoS tag, chunk tag,and dependency label of father; combined tag with thelemma of the token and the lemma of its father; chainof dependency labels from token to ROOT.
Lemma ofnext token, if next token is syntactic child of token.
Iftoken is a verb, lemma of the head of the token that isits subject.?
Dictionary features.
We extract a list of hedge cuesfrom the training corpus.
Based on this list, two binaryfeatures indicate whether token and next token are po-tential cues.?
Lemmas of the first noun, first verb and first adjectivein the sentence.The classifier is the decision tree IGTree as im-plemented in TiMBL (version 6.2) 5(Daelemanset al, 2009), a fast heuristic approximation of k-nn, that makes a heuristic approximation of near-est neighbor search by a top down traversal of thetree.
It was parameterised by using overlap as thesimilarity metric and information gain for featureweighting.
Running the system on the test datatakes 10.44 seconds in a 64 bit 2.8GHz 8GB RAMIntel Xeon machine with 4 cores.4.3 ResultsAll the results published in the paper are calcu-lated with the official scorer provided by the taskorganisers.
We provide precision (P), recall (R)and F1.
The official results of Task 1 are pre-sented in Table 3.
We produce in-domain and5TiMBL: http://ilk.uvt.nl/timbl43cross-domain results.
The BIO in-domain re-sults have been produced with the BIO system,by training on the training data BIO-ABS+BIO-ART, and testing on the test data BIO-ART.
TheWIKI in-domain results have been produced bythe WIKI system by training on WIKI and test-ing on WIKI.
The BIO cross-domain results havebeen produced with the BIO system, by train-ing on BIO-ABS+BIO-ART+WIKI and testing onBIO-ART.
The WIKI cross-domain results havebeen produced with the WIKI system by train-ing on BIO-ABS+BIO-ART+WIKI and testing onWIKI.
Training the SVM with BIO-ABS+BIO-ART+WIKI augmented the training time exponen-tially and the system did not finish on time for sub-mission.
We report post-evaluation results.In-domain Cross-domainP R F1 P R F1WIKI 80.55 44.49 57.32 80.64* 44.94* 57.71*BIO 81.15 82.28 81.71 80.54 83.29 81.89Table 3: Uncertainty detection results (Task 1 -closed track).
Post-evaluation results are markedwith *.In-domain results confirm that uncertain sen-tences inWikipedia text are more difficult to detectthan uncertain sentences in biological text.
Thisis caused by a loss in recall of the WIKI system.Compared to results obtained by other systemsparticipating in the CoNLL-2010 Shared Task, theBIO system performs 4.47 F1 lower than the bestsystem, and the WIKI system performs 2.85 F1lower.
This indicates that there is room for im-provement.
As for cross-domain results, we can-not conclude that the cross-domain data harm theperformance of the system, but we cannot stateeither that the cross-domain data improve the re-sults.
Since we performed Task 1 as a step towardsTask 2, it is interesting to know what is the per-formance of the system in identifying hedge cues.Results are shown in Table 4.
One of the mainsources of errors in detecting the cues are due tothe cue or.
Of the 52 occurrences in the test corpusBIO-ART, the system produces 3 true positives, 8false positives and 49 false negatives.In-domain Cross-domainP R F1 P R F1Bio 78.75 74.69 76.67 78.14 75.45 76.77Table 4: Cue matching results (Task 1 - closedtrack).5 Task 2: Resolution of in-sentencescopes of hedge cuesTask 2 consists of resolving in-sentence scopes ofhedge cues in biological texts.
The system per-forms this task in two steps, classification andpostprocessing, taking as input the output of thesystem that finds cues.5.1 ClassificationIn the classification step a memory-based classi-fier classifies tokens as being the first token in thescope sequence, the last, or neither, for as manycues as there are in the sentence.
An instance rep-resents a pair of a predicted hedge cue and a token.All tokens in a sentence are paired with all hedgecues that occur in the sentence.The classifier used is an IB1 memory?based al-gorithm as implemented in TiMBL (version 6.2)6(Daelemans et al, 2009), a memory-based classi-fier based on the k-nearest neighbor rule (Coverand Hart, 1967).
The IB1 algorithm is parame-terised by using overlap as the similarity metric,gain ratio for feature weighting, using 7 k-nearestneighbors, and weighting the class vote of neigh-bors as a function of their inverse linear distance.Running the system on the test data takes 53 min-utes in a 64 bit 2.8GHz 8GB RAM Intel Xeon ma-chine with 4 cores.The features extracted to perform the classifi-cation task are listed below.
Because, as notedby O?zgu?r and Radev (2009) and stated in the an-notation guidelines of the BioScope corpus7, thescope of a cue can be determined from its lemma,PoS tag, and from the syntactic construction of theclause (passive voice vs. active, coordination, sub-ordination), we use, among others, features thatencode information from the dependency tree.?
About the cue: chain of words, PoS label, dependencylabel, chunk label, chunk type; word, PoS tag, chunktag, and chunk type of the three previous and next to-kens in the string of words in the sentence; first andlast word, chain of PoS tags, and chain of words of thechunk where cue is embedded, and the same featuresfor the two previous and two next chunks; binary fea-ture indicating whether cue is the first, last or other to-ken in sentence; binary feature indicating whether cueis in a clause with a copulative construction; PoS tagand dependency label of the head of cue in the depen-dency tree; binary feature indicating whether cue is lo-cated before or after its syntactic head in the string of6TiMBL: http://ilk.uvt.nl/timbl.7Available at: http://www.inf.u-szeged.hu/rgai/project/nlp/bioscope/Annotation%20guidelines2.1.pdf.44words of the sentence; feature indicating whether cueis followed by an S-BAR or a coordinate construction.?
About the token: word, PoS tag, dependency label,chunk tag, chunk type; word, PoS tag, chunk tag, andchunk type of the three previous and three next tokensin the string of words of the sentence; chain of PoStag and lemmas of two and three tokens to the right oftoken in the string of words of the sentence; first andlast word, chain of PoS tags, and chain of words of thechunk where token is embedded, and the same featuresfor the two previous and two next chunks; PoS tag anddeplabel of head of token in the dependency tree; bi-nary feature indicating whether token is part of a cue.?
About the token in relation to cue: binary features indi-cating whether token is located before or after cue andbefore or after the syntactic head of cue in the stringof words of the sentence; chain of PoS tags betweencue and token in the string of words of the sentence;normalised distance between cue and token (number oftokens in between divided by total number of tokens);chain of chunks between cue and token; feature indi-cating whether token is located before cue, after cue orwihtin cue.?
About the dependency tree: feature indicating who isancestor (cue, token, other); chain of dependency la-bels and chain of PoS tags from cue to common an-cestor, and from token to common ancestor, if there isa common ancestor; chain of dependency labels andchain of PoS from token to cue, if cue is ancestor of to-ken; chain of dependency labels and chain of PoS fromcue to token, if token is ancestor of cue; chain of de-pendency labels and PoS from cue to ROOT and fromtoken to ROOT.Features indicating whether token is a candidate to bethe first token of scope (FEAT-FIRST), and whethertoken is a candidate to be the last token of the scope(FEAT-LAST).
These features are calculated by aheuristics that takes into account detailed informationof the dependency tree.
The value of FEAT-FIRST de-pends on whether the clause is in active or in passivevoice, on the PoS of the cue, and on the lemma in somecases (for example, verbs appear, seem).
The value ofFEAT-LAST depends on the PoS of the cue.5.2 PostprocessingIn the corpora provided for this task, scopes areannotated as continuous sequences of tokens thatinclude the cue.
However, the classifiers only pre-dict the first and last element of the scope.
In or-der to guarantee that all scopes are continuous se-quences of tokens we apply a first postprocessingstep (P-SCOPE) that builds the sequence of scopebased on the following rules:1.
If one token has been predicted as FIRST and one asLAST, the sequence is formed by the tokens betweenFIRST and LAST.2.
If one token has been predicted as FIRST and none hasbeen predicted as LAST, the sequence is formed by thetokens between FIRST and the first token that has value1 for FEAT-LAST.3.
If one token has been predicted as FIRST and morethan one as LAST, the sequence is formed by the tokensbetween FIRST and the first token predicted as LASTthat is located after cue.4.
If one token has been predicted as LAST and none asFIRST, the sequence will start at the hedge cue and itwill finish at the token predicted as LAST.5.
If no token has been predicted as FIRST and more thanone as LAST, the sequence will start at the hedge cueand will end at the first token predicted as LAST afterthe hedge signal.6.
If one token has been predicted as LAST and more thanone as FIRST, the sequence will start at the cue.7.
If no tokens have been predicted as FIRST and no to-kens have been predicted as LAST, the sequence willstart at the hedge cue and will end at the first token thathas value 1 for FEAT-LAST.The system predicts 987 scopes in total.
Ofthese, 1 FIRST and 1 LAST are predicted in 762cases; a different number of predictions is madefor FIRST and for LAST in 217 cases; no FIRSTand no LAST are predicted in 5 cases, and 2FIRST and 2 LAST are predicted in 3 cases.
In 52cases no FIRST is predicted, in 93 cases no LASTis predicted.Additionally, as exemplified in Example 1 inSection 1, bibliographic references and referencesto tables and figures do not always fall under thescope of cues, when the references appear at theend of the scope sequence.
If references that ap-pear at the end of the sentence have been predictedby the classifier within the scope of the cue, thesereferences are set out of the scope in a second post-processing step (P-REF).5.3 ResultsThe official results of Task 2 are presented in Ta-ble 5.
The system scores 57.32 F1, which is thehighest score of the systems that participated inthis task.In-domainP R F1BIO 59.62 55.18 57.32Table 5: Scope resolution official results (Task 2 -closed track).In order to know what is the effect of the post-processing steps, we evaluate the output of thesystem before performing step P-REF and beforeperforming step P-SCOPE.
Table 6 shows the re-sults of the evaluation.
Without P-REF, the perfor-mance decreases in 7.30 F1.
This is caused by the45fact that a considerable proportion of scopes endin a reference to bibliography, tables, or figures.Without P-SCOPE it decreases 4.50 F1 more.
Thisis caused, mostly, by the cases in which the classi-fier does not predict the LAST class.In-domainP R F1BIO before P-REF 51.98 48.20 50.02BIO before P-SCOPE 48.82 44.43 46.52Table 6: Scope resolution results before postpro-cessing steps.It is not really possible to compare the scoresobtained in this task to existing research previousto the CoNLL-2010 Shared Task, namely the re-sults obtained by O?zgu?r and Radev (2009) on theBioScope corpus with a rule-based system and byMorante and Daelemans (2009) on the same cor-pus with a combination of classifiers.
O?zgu?r andRadev (2009) report accuracy scores (61.13 on fulltext), but no F measures are reported.
Morante andDaelemans (2009) report percentage of correctscopes for the full text data set (42.37), obtainedby training on the abstracts data set, whereas theresults presented in Table 5 are reported in F mea-sures and obtained in by training and testing onother corpora.
Additionally, the system has beentrained on a corpus that contains abstracts and fulltext articles, instead of only abstracts.
However,it is possible to confirm that, even with informa-tion on dependency syntax, resolving the scopes ofhedge cues in biological texts is not a trivial task.The scores obtained in this task are much lowerthan the scores obtained in other tasks that involvesemantic processing, like semantic role labeling.The errors of the system in Task 2 are causedby different factors.
First, there is error propaga-tion from the system that finds cues.
Second, thesystem heavily relies on information from the syn-tactic dependency tree.
The parser used to prepro-cess the data (GDep) has been trained on abstracts,instead of full articles, which means that the per-formance on full articles will be lower, since sen-tence are longer and more complex.
Third, en-coding the information of the dependency treein features for the learner is not a straightfor-ward process.
In particular, some errors in resolv-ing the scope are caused by keeping subordinateclauses within the scope, as in sentence (2), where,apart from not identifying speculated as a cue, thesystem wrongly includes resulting in fewer high-confidence sequence assignments within the scopeof may.
This error is caused in the instance con-struction phase, because token assignments getsvalue 1 for feature FEAT-LAST and token algo-rithm gets value 0, whereas it should have beenotherwise.
(2) We speculated that the presence of multiple isotopepeaks per fragment ion in the high resolution OrbitrapMS/MS scans <xcope id=1><cue ref=1>may</cue> degrade the sensitivity of the searchalgorithm, resulting in fewer high-confidencesequence assignments</xcope>.Additionally, the test corpus contains an articleabout the annotation of a corpus of hedge cues,thus, an article that contains metalanguage.
Oursystem can not deal with sentences like the one in(3), in which all cues with their scopes are falsepositives.
(3) For example, the word <xcope id=1><cue ref=1>may</cue> in sentence 1</xcope>) <xcope id=2><cue ref=2>indicates that</cue> there is someuncertainty about the truth of the event, whilst thephrase Our results show that in 2) <xcope id=3><cue ref=3>indicates that</cue> there isexperimental evidence to back up the event describedby encodes</xcope></xcope>.6 Conclusions and future researchIn this paper we presented the machine learningsystems that we submitted to the CoNLL-2010Shared Task on Learning to Detect Hedges andTheir Scope in Natural Language Text.
The BIOdata were processed by memory-based systems inTask 1 and Task 2.
The system that performs Task2 relies on information from syntactic dependen-cies.
This system scored the highest F1 (57.32) ofTask 2.As for Task 1, in-domain results confirm thatuncertain sentences inWikipedia text are more dif-ficult to detect than uncertain sentences in biolog-ical text.
One of the reasons is that the number ofweasels is much higher and diverse than the num-ber of hedge cues.
BIO cross-domain results showthat adding WIKI data to the training set causes aslight decrease in precision and a slight increasein recall.
The errors of the BIO system show thatsome cues, like or are difficult to identify, becausethey are ambiguous.
As for Task 2, results indi-cate that resolving the scopes of hedge cues in bi-ological texts is not a trivial task.
The scores ob-tained in this task are much lower than the scoresobtained in other tasks that involve semantic pro-cessing, like semantic role labeling.
The results46are influenced by propagation of errors from iden-tifying cues, errors in the dependency tree, the ex-traction process of syntactic information from thedependency tree to encode it in the features, andthe presence of metalanguage on hedge cues in thetest corpus.
Future research will focus on improv-ing the identification of hedge cues and on usingdifferent machine learning techniques to resolvethe scope of cues.AcknowledgementsThe research reported in this paper was made pos-sible through financial support from the Universityof Antwerp (GOA project BIOGRAPH).ReferencesSabine Buchholz and Erwin Marsi.
2006.
CoNLL-X shared task on multilingual dependency parsing.In Proceedings of the CoNLL-X Shared Task, NewYork.
SIGNLL.Thomas M. Cover and Peter E. Hart.
1967.
Nearestneighbor pattern classification.
Institute of Electri-cal and Electronics Engineers Transactions on In-formation Theory, 13:21?27.Walter Daelemans and Antal van den Bosch.
2005.Memory-based language processing.
CambridgeUniversity Press, Cambridge, UK.Walter Daelemans, Jakub Zavrel, Ko Van der Sloot, andAntal Van den Bosch.
2009.
TiMBL: Tilburg Mem-ory Based Learner, version 6.2, Reference Guide.Number 09-01 in Technical Report Series.
Tilburg,The Netherlands.Chrysanne Di Marco and Robert E. Mercer, 2005.Computing attitude and affect in text: Theory andapplications, chapter Hedging in scientific articlesas a means of classifying citations.
Springer-Verlag,Dordrecht.Richa?rd Farkas, Veronika Vincze, Gyo?rgy Mo?ra, Ja?nosCsirik, and Gyo?rgy Szarvas.
2010.
The CoNLL-2010 Shared Task: Learning to Detect Hedges andtheir Scope in Natural Language Text.
In Proceed-ings of the Fourteenth Conference on ComputationalNatural Language Learning (CoNLL-2010): SharedTask, pages 1?12, Uppsala, Sweden, July.
Associa-tion for Computational Linguistics.Carol Friedman, Philip Alderson, John Austin, James J.Cimino, and Stephen B. Johnson.
1994.
A generalnatural?language text processor for clinical radiol-ogy.
Journal of the American Medical InformaticsAssociation, 1(2):161?174.Ken Hyland.
1998.
Hedging in scientific research ar-ticles.
John Benjamins B.V, Amsterdam.Thorsten Joachims.
2002.
Learning to Classify TextUsing Support Vector Machines, volume 668 of TheSpringer International Series in Engineering andComputer Science.
Springer.Halil Kilicoglu and Sabine Bergler.
2008.
Recogniz-ing speculative language in biomedical research ar-ticles: a linguistically motivated perspective.
BMCBioinformatics, 9(Suppl 11):S10.George Lakoff.
1972.
Hedges: a study in meaningcriteria and the logic of fuzzy concepts.
ChicagoLinguistics Society Papers, 8:183?228.Marc Light, Xin Y.Qiu, and Padmini Srinivasan.
2004.The language of bioscience: facts, speculations, andstatements in between.
In Proceedings of the Bi-oLINK 2004, pages 17?24.Ben Medlock and Ted Briscoe.
2007.
Weakly super-vised learning for hedge classification in scientificliterature.
In Proceedings of ACL 2007, pages 992?999.Ben Medlock.
2008.
Exploring hedge identification inbiomedical literature.
Journal of Biomedical Infor-matics, 41:636?654.Roser Morante and Walter Daelemans.
2009.
Learn-ing the scope of hedge cues in biomedical texts.
InProceedings of BioNLP 2009, pages 28?36, Boul-der, Colorado.Joakim Nivre.
2006.
Inductive Dependency Parsing,volume 34 of Text, Speech and Language Technol-ogy.
Springer.Arzucan O?zgu?r and Dragomir R. Radev.
2009.
Detect-ing speculations and their scopes in scientific text.In Proceedings of EMNLP 2009, pages 1398?1407,Singapore.Frank R. Palmer.
1986.
Mood and modality.
CUP,Cambridge, UK.Kenji Sagae and Jun?ichi Tsujii.
2007.
Dependencyparsing and domain adaptation with LR models andparser ensembles.
In Proceedings of CoNLL 2007:Shared Task, pages 82?94, Prague, Czech Republic.Gyo?rgy Szarvas.
2008.
Hedge classification inbiomedical texts with a weakly supervised selectionof keywords.
In Proceedings of ACL 2008, pages281?289, Columbus, Ohio, USA.
ACL.Paul Thompson, Giulia Venturi, John McNaught,Simonetta Montemagni, and Sophia Ananiadou.2008.
Categorising modality in biomedical texts.
InProceedings of the LREC 2008 Workshop on Build-ing and Evaluating Resources for Biomedical TextMining 2008, pages 27?34, Marrakech.
LREC.Veronika Vincze, Gyo?rgy Szarvas, Richa?rd Farkas,Gyo?rgy Mo?ra, and Ja?nos Csirik.
2008.
The Bio-Scope corpus: biomedical texts annotated for uncer-tainty, negation and their scopes.
BMC Bioinformat-ics, 9(Suppl 11):S9.47
