Proceedings of the EACL 2009 Workshop on Language Technologies for African Languages ?
AfLaT 2009, pages 1?8,Athens, Greece, 31 March 2009. c?2009 Association for Computational LinguisticsCollecting and evaluating speech recognition corporafor nine Southern Bantu languagesJaco Badenhorst, Charl van Heerden, Marelie Davel and Etienne BarnardHLT Research Group, Meraka Institute, CSIR, South Africajbadenhorst@csir.co.za, mdavel@csir.co.zacvheerden@csir.co.za, ebarnard@csir.co.zaAbstractWe describe the Lwazi corpus for auto-matic speech recognition (ASR), a newtelephone speech corpus which includesdata from nine Southern Bantu lan-guages.
Because of practical constraints,the amount of speech per language isrelatively small compared to major cor-pora in world languages, and we reporton our investigation of the stability ofthe ASR models derived from the corpus.We also report on phoneme distance mea-sures across languages, and describe initialphone recognisers that were developed us-ing this data.1 IntroductionThere is a widespread belief that spoken dialogsystems (SDSs) will have a significant impact inthe developing countries of Africa (Tucker andShalonova, 2004), where the availability of alter-native information sources is often low.
Tradi-tional computer infrastructure is scarce in Africa,but telephone networks (especially cellular net-works) are spreading rapidly.
In addition, speech-based access to information may empower illiter-ate or semi-literate people, 98% of whom live inthe developing world.Spoken dialog systems can play a useful rolein a wide range of applications.
Of particular im-portance in Africa are applications such as ed-ucation, using speech-enabled learning softwareor kiosks and information dissemination throughmedia such as telephone-based information sys-tems.
Significant benefits can be envisioned ifinformation is provided in domains such as agri-culture (Nasfors, 2007), health care (Sherwani etal., ; Sharma et al, 2009) and government ser-vices (Barnard et al, 2003).
In order to makeSDSs a reality in Africa, technology componentssuch as text-to-speech (TTS) systems and auto-matic speech recognition (ASR) systems are re-quired.
The latter category of technologies is thefocus of the current contribution.Speech recognition systems exist for only ahandful of African languages (Roux et al, ; Seidand Gambck, 2005; Abdillahi et al, 2006), andto our knowledge no service available to the gen-eral public currently uses ASR in an indigenousAfrican language.
A significant reason for thisstate of affairs is the lack of sufficient linguisticresources in the African languages.
Most impor-tantly, modern speech recognition systems use sta-tistical models which are trained on corpora ofrelevant speech (i.e.
appropriate for the recogni-tion task in terms of the language used, the pro-file of the speakers, speaking style, etc.)
Thisspeech generally needs to be curated and tran-scribed prior to the development of ASR systems,and for most applications speech from a largenumber of speakers is required in order to achieveacceptable system performance.
On the Africancontinent, where infrastructure such as computernetworks is less developed than in countries suchas America, Japan and the European countries, thedevelopment of such speech corpora is a signifi-cant hurdle to the development of ASR systems.The complexity of speech corpus developmentis strongly correlated with the amount of data thatis required, since the number of speakers that needto be canvassed and the amount of speech thatmust be curated and transcribed are major fac-tors in determining the feasibility of such devel-opment.
In order to minimise this complexity, itis important to have tools and guidelines that canbe used to assist in designing the smallest cor-pora that will be sufficient for typical applicationsof ASR systems.
As minimal corpora can be ex-tended by sharing data across languages, tools arealso required to indicate when data sharing will bebeneficial and when detrimental.1In this paper we describe and evaluate a newspeech corpus of South African languages cur-rently under development (the Lwazi corpus) andevaluate the extent in which computational anal-ysis tools can provide further guidelines for ASRcorpus design in resource-scarce languages.2 Project LwaziThe goal of Project Lwazi is to provide SouthAfrican citizens with information and informa-tion services in their home language, over thetelephone, in an efficient and affordable manner.Commissioned by the South African Departmentof Arts and Culture, the activities of this three yearproject (2006-2009) include the development ofcore language technology resources and compo-nents for all the official languages of South Africa,where, for the majority of these, no prior languagetechnology components were available.The core linguistic resources being developedinclude phoneme sets, electronic pronunciationdictionaries and the speech and text corpora re-quired to develop automated speech recognition(ASR) and text-to-speech (TTS) systems for alleleven official languages of South Africa.
Theusability of these resources will be demonstratedduring a national pilot planned for the third quar-ter of 2009.
All outputs from the project are be-ing released as open source software and opencontent(Meraka-Institute, 2009).Resources are being developed for all nineSouthern Bantu languages that are recognised asofficial languages in South Africa (SA).
These lan-guages are: (1) isiZulu (zul1) and isiXhosa (xho),the two Nguni languages most widely spoken inSA.
Together these form the home language of41% of the SA population.
(2) The three Sotholanguages: Sepedi (nso), Setswana (tsn), Sesotho(sot), together the home language of 26% of theSA population.
(3) The two Nguni languages lesswidely spoken in SA: siSwati (ssw) and isiNde-bele (nbl), together the home language of 4% ofthe SA population.
(4) Xitsonga (tso) and Tshiv-enda (ven), the home languages of 4% and 2% ofthe SA population, respectively (Lehohla, 2003).
(The other two official languages of South Africaare Germanic languages, namely English (eng)and Afrikaans (afr).
)For all these languages, new pronunciation dic-1After each language name, the ISO 639-3:2007 languagecode is provided in brackets.tionaries, text and speech corpora are being de-veloped.
ASR speech corpora consist of ap-proximately 200 speakers per language, produc-ing read and elicited speech, recorded over a tele-phone channel.
Each speaker produced approxi-mately 30 utterances, 16 of these were randomlyselected from a phonetically balanced corpus andthe remainder consist of short words and phrases:answers to open questions, answers to yes/noquestions, spelt words, dates and numbers.
Thespeaker population was selected to provide a bal-anced profile with regard to age, gender and typeof telephone (cellphone or landline).3 Related workBelow, we review earlier work relevant to the de-velopment of speech recognisers for languageswith limited resources.
This includes both ASRsystem design (Sec.
3.1) and ASR corpus design(Sec.
3.2).
In Sec.
3.3, we also review the ana-lytical tools that we utilise in order to investigatecorpus design systematically.3.1 ASR for resource-scarce languagesThe main linguistic resources required when de-veloping ASR systems for telephone based sys-tems are electronic pronunciation dictionaries, an-notated audio corpora (used to construct acous-tic models) and recognition grammars.
An ASRaudio corpus consists of recordings from multi-ple speakers, with each utterance carefully tran-scribed orthographically and markers used to indi-cate non-speech and other events important froman ASR perspective.
Both the collection of ap-propriate speech from multiple speakers and theaccurate annotation of this speech are resource-intensive processes, and therefore corpora forresource-scarce languages tend to be very small(1 to 10 hours of audio) when compared to thespeech corpora used to build commercial systemsfor world languages (hundreds to thousands ofhours per language).Different approaches have been used to bestutilise limited audio resources when developingASR systems.
Bootstrapping has been shown tobe a very efficient technique for the rapid devel-opment of pronunciation dictionaries, even whenutilising linguistic assistants with limited phonetictraining (Davel and Barnard, 2004).Small audio corpora can be used efficiently byutilising techniques that share data across lan-2guages, either by developing multilingual ASRsystems (a single system that simultaneouslyrecognises different languages), or by using addi-tional source data to supplement the training datathat exists in the target language.
Various datasharing techniques for language-dependant acous-tic modelling have been studied, including cross-language transfer, data pooling, language adap-tation and bootstrapping (Wheatley et al, 1994;Schultz and Waibel, 2001; Byrne et al, 2000).Both (Wheatley et al, 1994) and (Schultz andWaibel, 2001) found that useful gains could beobtained by sharing data across languages withthe size of the benefit dependent on the similar-ity of the sound systems of the languages com-bined.
In the only cross-lingual adaptation studyusing African languages (Niesler, 2007), similargains have not yet been observed.3.2 ASR corpus designCorpus design techniques for ASR are generallyaimed at specifying or selecting the most appro-priate subset of data from a larger domain in orderto optimise recognition accuracy, often while ex-plicitly minimising the size of the selected corpus.This is achieved through various techniques thataim to include as much variability in the data aspossible, while simultaneously ensuring that thecorpus matches the intended operating environ-ment as accurately as possible.Three directions are primarily employed: (1)explicit specification of phonotactic, speaker andchannel variability during corpus development, (2)automated selection of informative subsets of datafrom larger corpora, with the smaller subset yield-ing comparable results, and (3) the use of activelearning to optimise existing speech recognitionsystems.
All three techniques provide a perspec-tive on the sources of variation inherent in a speechcorpus, and the effect of this variation on speechrecognition accuracy.In (Nagroski et al, 2003), Principle ComponentAnalysis (PCA) is used to cluster data acousti-cally.
These clusters then serve as a starting pointfor selecting the optimal utterances from a train-ing database.
As a consequence of the clusteringtechnique, it is possible to characterise some of theacoustic properties of the data being analysed, andto obtain an understanding of the major sources ofvariation, such as different speakers and genders(Riccardi and Hakkani-Tur, 2003).Active and unsupervised learning methods canbe combined to circumvent the need for tran-scribing massive amounts of data (Riccardi andHakkani-Tur, 2003).
The most informative untran-scribed data is selected for a human to label, basedon acoustic evidence of a partially and iterativelytrained ASR system.
From such work, it soon be-comes evident that the optimisation of the amountof variation inherent to training data is needed,since randomly selected additional data does notnecessarily improve recognition accuracy.
By fo-cusing on the selection (based on existing tran-scriptions) of a uniform distribution across differ-ent speech units such as words and phonemes, im-provements are obtained (Wu et al, 2007).In our focus on resource-scarce languages, themain aim is to understand the amount of data thatneeds to be collected in order to achieve accept-able accuracy.
This is achieved through the useof analytic measures of data variability, which wedescribe next.3.3 Evaluating phoneme stabilityIn (Badenhorst and Davel, 2008) a technique isdeveloped that estimates how stable a specificphoneme is, given a specific set of training data.This statistical measure provides an indication ofthe effect that additional training data will have onrecognition accuracy: the higher the stability, theless the benefit of additional speech data.The model stability measure utilises the Bhat-tacharyya bound (Fukunaga, 1990), a widely-usedupper bound of the Bayes error.
If Pi and pi(X)denote the prior probability and class-conditionaldensity function for class i, respectively, the Bhat-tacharyya bound ?
is calculated as:?
=?P1P2?
?p1(X)p2(X)dX (1)When both density functions are Gaussian withmean ?i and covariance matrix ?i, integration of?
leads to a closed-form expression for ?:?
=?P1P2e??
(1/2) (2)where?
(1/2) = 18(?2 ?
?1)T[?1 +?22]?1(?2 ?
?1)+ 12 ln|?1+?22 |?|?1||?2|(3)is referred to as the Bhattacharyya distance.3In order to estimate the stability of an acous-tic model, the training data for that model is sep-arated into a number of disjoint subsets.
All sub-sets are selected to be mutually exclusive with re-spect to the speakers they contain.
For each sub-set, a separate acoustic model is trained, and theBhattacharyya bound between each pair of mod-els calculated.
By calculating both the mean ofthis bound and the standard deviation of this mea-sure across the various model pairs, a statisticallysound measure of model estimation stability is ob-tained.4 Computational analysis of the LwazicorpusWe now report on our analysis of the Lwazispeech corpus, using the stability measure de-scribed above.
Here, we focus on four languages(isiNdebele, siSwati, isiZulu and Tshivenda) forreasons of space; later, we shall see that the otherlanguages behave quite similarly.4.1 Experimental designFor each phoneme in each of our target lan-guages, we extract all the phoneme occurrencesfrom the 150 speakers with the most utterances perphoneme.
We utilise the technique described inSec.
3.3 to estimate the Bhattacharyya bound bothwhen evaluating phoneme variability and modeldistance.
In both cases we separate the data foreach phoneme into 5 disjoint subsets.
We calcu-late the mean of the 10 distances obtained betweenthe various intra-phoneme model pairs when mea-suring phoneme stability, and the mean of the25 distances obtained between the various inter-phoneme model pairs when measuring phonemedistance.In order to be able to control the number ofphoneme observations used to train our acousticmodels, we first train a speech recognition systemand then use forced alignment to label all of theutterances using the systems described in Sec.
5.Mel-frequency cepstral coefficients (MFCCs) withcepstral mean and variance normalisation are usedas features, as described in Sec.
5.4.2 Analysis of phoneme variabilityIn an earlier analysis of phoneme variability ofan English corpus (Badenhorst and Davel, 2008),it was observed that similar trends are observedwhen utilising different numbers of mixtures ina Gaussian mixture model.
For both context de-pendent and context independent models similartrends are also observed.
(Asymptotes occur later,but trends remain similar.)
Because of the limitedsize of the Lwazi corpus, we therefore only reporton single-mixture context-independent models inthe current section.As we also observe similar trends for phonemeswithin the same broad categories, we report onone or two examples from several broad categorieswhich occur in most of our target languages.
Us-ing SAMPA notation, the following phonemes areselected: /a/ (vowels), /m/ (nasals), /b/ and /g/(voiced plosives) and /s/ (unvoiced fricatives), af-ter verifying that these phonemes are indeed rep-resentative of the larger groups.Figures 1 and 2 demonstrate the effects of vari-able numbers of phonemes and speakers, respec-tively, on the value of the mean Bhattacharyyabound.
This value should approach 0.5 for a modelfully trained on a sufficiently representative set ofdata.
In Fig.
1 we see that the various broad cate-gories of sounds approach the asymptotic bound indifferent ways.
The vowels and nasals require thelargest number of phoneme occurrences to reacha given level, whereas the fricatives and plosivesconverge quite rapidly (With 10 observations perspeaker, both the fricatives and plosives achievevalues of 0.48 or better for all languages, in con-trast to the vowels and nasals which require 30 ob-servations to reach similar stability).
Note that weemployed 30 speakers per phoneme group, sincethat is the largest number achievable with our pro-tocol.For the results in Fig.
2, we keep the numberof phoneme occurrences per speaker fixed at 20(this ensures that we have sufficient data for allphonemes, and corresponds with reasonable con-vergence in Fig.
1).
It is clear that additionalspeakers would still improve the modelling ac-curacy for especially the vowels and nasals.
Weobserve that the voiced plosives and fricativesquickly achieve high values for the bound (closeto the ideal 0.5).Figures 1 and 2 ?
as well as similar figures forthe other phoneme classes and languages we havestudied ?
suggest that all phoneme categories re-quire at least 20 training speakers to achieve rea-sonable levels of convergence (bound levels of0.48 or better).
The number of phoneme observa-tions required per speaker is more variable, rang-4Figure 1: Effect of number of phoneme utterances per speaker on mean of Bhattacharyya bound fordifferent phoneme groups using data from 30 speakersing from less than 10 for the voiceless fricativesto 30 or more for vowels, liquids and nasals.
Wereturn to these observations below.4.3 Distances between languagesIn Sec.
3.1 it was pointed out that the simi-larities between the same phonemes in differentlanguages are important predictors of the bene-fit achievable from pooling the data from thoselanguages.
Armed with the knowledge that sta-ble models can be estimated with 30 speakers perphoneme and between 10 and 30 phonemes oc-currences per speaker, we now turn to the task ofmeasuring distances between phonemes in variouslanguages.We again use the mean Bhattacharyya boundto compare phonemes, and obtain values betweenall possible combinations of phonemes.
Resultsare shown for the isiNdebele phonemes /n/ and /a/in Fig.
3.
As expected, similar phonemes fromthe different languages are closer to one anotherthan different phonemes of the same language.However, the details of the distances are quite re-vealing: for /a/, siSwati is closest to the isiN-debele model, as would be expected given theirclose linguistic relationship, but for /n/, the Tshiv-enda model is found to be closer than either ofthe other Nguni languages.
For comparative pur-poses, we have included one non-Bantu language(Afrikaans), and we see that its models are indeedsignificantly more dissimilar from the isiNdebelemodel than any of the Bantu languages.
In fact,the Afrikaans /n/ is about as distant from isiNde-bele /n/ as isiNdebele and isiZulu /l/ are!5 Initial ASR resultsIn order to verify the usability of the Lwazi cor-pus for speech recognition, we develop initialASR systems for all 11 official South Africanlanguages.
A summary of the data statistics forthe Bantu languages investigated is shown in Tab.1, and recognition accuracies achieved are sum-marised in Tab.
2.
For these tests, data from 30speakers per language were used as test data, withthe remaining data being used for training.Although the Southern Bantu languages aretone languages, our systems do not encode tonal5Figure 2: Effect of number of speakers on mean of Bhattacharyya bound for different phoneme groupsusing 20 utterances per speakerLanguage total # # speech # distinctminutes minutes phonemesisiNdebele 564 465 46isiXhosa 470 370 52isiZulu 525 407 46Tshivenda 354 286 38Sepedi 394 301 45Sesotho 387 313 44Setswana 379 295 34siSwati 603 479 39Xitsonga 378 316 54N-TIMIT 315 - 39Table 1: A summary of the Lwazi ASR corpus:Bantu languages.information, since tone is unlikely to be impor-tant for small-to-medium vocabulary applications(Zerbian and Barnard, 2008).As the initial pronunciation dictionaries weredeveloped to provide good coverage of the lan-guage in general, these dictionaries did not coverthe entire ASR corpus.
Grapheme-to-phonemerules are therefore extracted from the generaldictionaries using the Default&Refine algorithm(Davel and Barnard, 2008) and used to generatemissing pronunciations.We use HTK 3.4 to build a context-dependentcross-word HMM-based phoneme recogniser withtriphone models.
Each model had 3 emittingstates with 7 mixtures per state.
39 features areused: 13 MFCCs together with their first and sec-ond order derivatives.
Cepstral Mean Normali-sation (CMN) as well as Cepstral Variance Nor-malisation (CMV) are used to perform speaker-independent normalisation.
A diagonal covariancematrix is used; to partially compensate for this in-correct assumption of feature independence semi-tied transforms are applied.
A flat phone-basedlanguage model is employed throughout.As a rough benchmark of acceptable phoneme-recognition accuracy, recently reported results ob-tained by (Morales et al, 2008) on a similar-sizedtelephone corpus in American English (N-TIMIT)are also shown in Tab.
2.
We see that the Lwaziresults compare very well with this benchmark.An important issue in ASR corpus design is6Figure 3: Effective distances in terms of the mean of the Bhattacharyya bound between a single phoneme(/n/-nbl top and /a/-nbl bottom) and each of its closest matches within the set of phonemes investigated.Language % corr % acc avg # total #phons speakersisiNdebele 74.21 65.41 28.66 200isiXhosa 69.25 57.24 17.79 210isiZulu 71.18 60.95 23.42 201Tshivenda 76.37 66.78 19.53 201Sepedi 66.44 55.19 16.45 199Sesotho 68.17 54.79 18.57 200Setswana 69.00 56.19 20.85 207siSwati 74.19 64.46 30.66 208Xitsonga 70.32 59.41 14.35 199N-TIMIT 64.07 55.73 - -Table 2: Initial results for South African ASR sys-tems.
The column labelled ?avg # phonemes?
liststhe average number of phoneme occurrences foreach phoneme for each speaker.the trade-off between the number of speakers andthe amount of data per speaker (Wheatley et al,1994).
The figures in Sec.
4.2 are not conclusiveon this trade-off, so we have also investigated theeffect of reducing either the number of speakersor the amount of data per speaker when trainingthe isiZulu and Tshivenda recognisers.
As shownin Fig.
4, the impact of both forms of reductionis comparable across languages and different de-grees of reduction, in agreement with the resultsof Sec.
4.2.These results indicate that we now have a firmFigure 4: The influence of a reduction in trainingcorpus size on phone recognition accuracy.baseline to investigate data-efficient training meth-ods such as those described in Sec.
3.1.6 ConclusionIn this paper we have introduced a new tele-phone speech corpus which contains data fromnine Southern Bantu languages.
Our stability anal-ysis shows that the speaker variety as well asthe amount of speech per speaker is sufficient toachieve acceptable model stability, and this con-clusion is confirmed by the successful training ofphone recognisers in all the languages.
We con-firm the observation in (Badenhorst and Davel,2008) that different phone classes have different7data requirements, but even for the more demand-ing classes (vowels, nasals, liquids) our amount ofdata seems sufficient.
Our results suggest that sim-ilar accuracies may be achievable by using morespeech from fewer speakers ?
a finding that maybe useful for the further development of speechcorpora in resource-scarce languages.Based on the proven stability of our models, wehave performed some preliminary measurementsof the distances between the phones in the dif-ferent languages; such distance measurements arelikely to be important for the sharing of data acrosslanguages in order to further improve ASR accu-racy.
The development of real-world applicationsusing this data is currently an active topic of re-search; for that purpose, we are continuing to in-vestigate additional methods to improve recogni-tion accuracy with such relatively small corpora,including cross-language data sharing and effi-cient adaptation methods.ReferencesNimaan Abdillahi, Pascal Nocera, and Jean-FranoisBonastre.
2006.
Automatic transcription of Somalilanguage.
In Interspeech, pages 289?292, Pitts-burgh, PA.J.A.C.
Badenhorst and M.H.
Davel.
2008.
Data re-quirements for speaker independent acoustic mod-els.
In PRASA, pages 147?152.E.
Barnard, L. Cloete, and H. Patel.
2003.
Languageand technology literacy barriers to accessing govern-ment services.
Lecture Notes in Computer Science,2739:37?42.W.
Byrne, P. Beyerlein, J. M. Huerta, S. Khudanpur,B.
Marthi, J. Morgan, N. Peterek, J. Picone, D. Ver-gyri1, and W. Wang.
2000.
Towards language inde-pendent acoustic modeling.
In ICASSP, volume 2,pages 1029?1032, Istanbul, Turkey.M.
Davel and E. Barnard.
2004.
The efficient cre-ation of pronunication dictionaries: human factorsin bootstrapping.
In Interspeech, pages 2797?2800,Jeju, Korea, Oct.M.
Davel and E. Barnard.
2008.
Pronunciation predi-cation with Default&Refine.
Computer Speech andLanguage, 22:374?393, Oct.K.
Fukunaga.
1990.
Introduction to Statistical PatternRecognition.
Academic Press, Inc., 2nd edition.Pali Lehohla.
2003.
Census 2001: Census in brief.Statistics South Africa.Meraka-Institute.
2009.
Lwazi ASR corpus.
Online:http://www.meraka.org.za/lwazi.N.
Morales, J. Tejedor, J. Garrido, J. Colas, and D.T.Toledano.
2008.
STC-TIMIT: Generation of asingle-channel telephone corpus.
In LREC, pages391?395, Marrakech, Morocco.A.
Nagroski, L. Boves, and H. Steeneken.
2003.
Insearch of optimal data selection for training of auto-matic speech recognition systems.
ASRU workshop,pages 67?72, Nov.P.
Nasfors.
2007.
Efficient voice information servicesfor developing countries.
Master?s thesis, Depart-ment of Information Technology, Uppsala Univer-sity.T.
Niesler.
2007.
Language-dependent state clusteringfor multilingual acoustic modeling.
Speech Commu-nication, 49:453?463.G.
Riccardi and D. Hakkani-Tur.
2003.
Active andunsupervised learning for automatic speech recog-nition.
In Eurospeech, pages 1825?1828, Geneva,Switzerland.J.C.
Roux, E.C.
Botha, and J.A.
du Preez.
Develop-ing a multilingual telephone based information sys-tem in african languages.
In LREC, pages 975?980,Athens, Greece.T.
Schultz and A. Waibel.
2001.
Language-independent and language-adaptive acoustic model-ing for speech recognition.
Speech Communication,35:31?51, Aug.Hussien Seid and Bjrn Gambck.
2005.
A speaker inde-pendent continuous speech recognizer for Amharic.In Interspeech, pages 3349?3352, Lisboa, Portugal,Oct.A.
Sharma, M. Plauche, C. Kuun, and E. Barnard.2009.
HIV health information access using spokendialogue systems: Touchtone vs. speech.
Acceptedat IEEE Int.
Conf.
on ICTD.J.
Sherwani, N. Ali, S. Mirza, A. Fatma, Y. Memon,M.
Karim, R. Tongia, and R. Rosenfeld.
Healthline:Speech-based access to health information by low-literate users.
In IEEE Int.
Conf.
on ICTD, pages131?139.R.
Tucker and K. Shalonova.
2004.
The Local Lan-guage Speech Technology Initiative.
In SCALLAConf., Nepal.B.
Wheatley, K. Kondo, W. Anderson, andY.
Muthusumy.
1994.
An evaluation of cross-language adaptation for rapid HMM developmentin a new language.
In ICASSP, pages 237?240,Adelaide.Y.
Wu, R. Zhang, and A. Rudnicky.
2007.
Data selec-tion for speech recognition.
ASRU workshop, pages562?565, Dec.S.
Zerbian and E. Barnard.
2008.
Phonetics of into-nation in South African Bantu languages.
SouthernAfrican Linguistics and Applied Language Studies,26(2):235?254.8
