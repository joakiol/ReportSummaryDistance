Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 138?142, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsKnCe2013-CORE:Semantic Text Similarity by use of Knowledge BasesHermann ZiakKnow-Center GmbHGraz University of TechnologyInffeldgasse 13/ 6.
Stock8010 Graz, Austriahziak@know-center.atRoman KernKnow-Center GmbHGraz University of TechnologyInffeldgasse 13/ 6.
Stock8010 Graz, Austriarkern@know-center.atAbstractIn this paper we describe KnCe2013-CORE,a system to compute the semantic similarityof two short text snippets.
The system com-putes a number of features which are gath-ered from different knowledge bases, namelyWordNet, Wikipedia and Wiktionary.
Thesimilarity scores derived from these featuresare then fed into several multilayer perceptronneuronal networks.
Depending on the sizeof the text snippets different parameters forthe neural networks are used.
The final out-put of the neural networks is compared to hu-man judged data.
In the evaluation our systemperformed sufficiently well for text snippetsof equal length, but the performance droppedconsiderably once the pairs of text snippetsdiffer in size.1 IntroductionThe task of the semantic sentence similarity is to as-sign a score to a given pair of sentences.
This scoreshould reflect the degree by which the two sentencesrepresent the same meaning.
The semantic similar-ity of two sentences could then be used in a num-ber of different application scenarios, for example itcould help to improve the performance of informa-tion retrieval systems.In the past, systems based on regression mod-els in combination with well chosen features havedemonstrated good performance on this topic[4] [6].Therefore we took this approach as a starting pointto develop our semantic similarity system; addition-ally, we integrated a number of existing knowledgebases into our system.
With it, trained with the datadiscussed in the task specification of last year[1], weparticipated in the shared task of SEM 2013.Additionally, to the similarity based on the fea-tures derived from the external knowledge bases, weemploy a neural network to compute the final simi-larity score.
The motivation to use a supervised ma-chine learning algorithm has been the observationthat the semantic similarity is heavily influenced bythe context of the human evaluator.
A financial ex-pert for example would judge sentences with finan-cial topics different to non financial experts, if oc-curring numbers differ from each other.The remainder of the paper is organised as fol-lows: In Section 2 we described our system, themain features and the neuronal network to combinedifferent feature sets.
In Section 3 the calculationmethod of our feature values is discribed.
In Sec-tion 4 we report the results of our system based onour experiments and the submitted results of the testdata.
In Section 5 and 6 we discuss the results andthe outcome of our work.2 System Overview2.1 ProcessingInitially the system puts the sentence pairs of thewhole training set through our annotation pipeline.After this process the sentence pairs are comparedto each other by our different feature scoring algo-rithms.
The result is a list of scores for each of thesepairs where every score represents a feature or partof a feature.
The processed sentences are now sep-arated by their length and used to train the neuronal138network models for each length group.
The testingdata is also grouped based on the sentence lengthand the score for each pair is determined by a rele-vant model.2.2 Token FeaturesThe first set of features are simply the tokens fromthe two respective sentences.
This feature set shouldperform well, if exactly the same words are usedwithin the pair of sentences to be compared.
Butas soon as words are replaced by their synonyms orother semantically related words, this feature set willnot be able to capture the true similarity.
Used with-out other features it could even lead to false posi-tive matches, for example given sentences with sim-ilar content but containing antonyms.
The tokenizerused by our system was based on the OpenNLPmaximum entropy tokenizer, which detects tokenboundaries based on probability model.2.3 Wiktionary FeaturesWhile the collaboratively created encyclopediaWikipedia receives a lot of attention from the gen-eral public, as well as the research community, thefree dictionary Wiktionary1 is far lesser known.
TheWiktionary dictionary stores the information in asemi-structured way using Wikimedia syntax, wherea single page represents a single word or phrase.Therefore we developed a parser to extract relevantinformation.
In our case we were especially inter-ested in semantically related terms, where the se-mantic relationship is:Representations: Set of word forms for a spe-cific term.
These terms are expected to indicate thehighest semantic similarity.
This includes all flex-ions, for example the ?s?
suffix for plural forms.Synonyms: List of synonyms for the term.Hyponyms: List of more specific terms.Hypernym: Terms which represent more generalterms.Antonym: List of terms, which represent an op-posing sense.Related Terms: Terms, with a semantic relation-ship, which does not fall in the aforementioned cat-egories.
For example related terms for ?bank?
are1http://en.wiktionary.org?bankrupt?.
Related terms represent only a weak se-mantic similarity.Derived Terms: Terms, with overlapping wordforms, such as ?bank holiday?, ?bankroll?
and ?data-bank?
for the term ?bank?.
From all the semanticrelationship types, derived terms are the weakest in-dicator for their similarities.2.4 WordNet FeaturesThe WordNet[5][2] features were generated identi-cally to the Wiktionary features.
We used the Word-Net off line database and the provided library to geta broader knowledge base.
Therefore we extract thesemantically related terms of each token and savedeach class of relation.
Where each dependency classproduced an one value in the final feature score listof the sentence pairs.2.5 Wikification FeatureWe applied a Named Entity Recognition component,which has been trained using Wikipedia categoriesas input.
Given a sentence it will annotate all foundconcepts that match a Wikipedia article, togetherwith a confidence score.
So for every found entryby the annotator there is a list of possible associ-ated topics.
The confidence score can then be usedto score the topic information, in the final step theevaluation values where calculated as follows:scorewiki(s1, s2) =|T1 ?
T2|norm(T1, T2)where T1 and T2 are the set of topics of the twosentences and norm is the mean of the confidencescores of the topics.2.6 Other FeaturesAlthough we mainly focused our approach on thethree core features above, others seemed to be usefulto improve the performance of the system of whichsome are described below.Numbers and Financial Expression Feature:Some sentence pairs showed particular variationsbetween the main features and their actual score.Many of these sentence pairs where quite similarin their semantic topic but contained financial ex-pressions or numbers that differed.
Therefore theseexpressions where extracted and compared againsteach other with a descending score.139NGrams Feature: The ngram overlapping fea-ture is based on a noun-phrase detection which re-turns the noun-phrases in different ngrams.
Thisnoun-phrase detection is a pos tagger pattern whichmatches multiple nouns preceding adjectives and de-terminers.
In both sentences the ngrams where ex-tracted and compared to each other returning onlythe biggest overlapping.
In the end, to produce theevaluation values, the word-count of the overlappingngrams were taken.3 Distance calculationFor the calculation of the distance of the differentfeatures we chose a slightly modified version of theJacquard similarity coefficient.Jsc(w, l) =wlWhere in this case w stands for the intersection ofthe selected feature, and l forla+lb2 where la and lbare the length of the sentences with or without stop-words depending on the selected feature.
The as-sumption was that for some features the gap betweensentences where one has many stop-words and sen-tences with none would have a crucial impact but forothers it would be detrimental.
In regard to this weused, depending on the feature, the words or wordsexcluding stop-words.3.1 ScoringOne of the main issues at the beginning of our re-search was how to signal the absence of features tothe neuronal network.
As our feature scores dependon the length of the sentence, the absence of a partic-ular feature (e.g.
financial values) and detected fea-tures without intersections (e.g.
none of the foundfinancial values in the sentences are intersecting) inthe sentence pairs would lead to the same result.Therefore we applied two different similarityscores based on the feature set.
They differ in theresult they give, if there is no overlap between thetwo feature sets.For a simple term similarity we defined our simi-larity score asscore(w, s, l) ={?1 : s = 0 or w = 0Jsc(w, l) : w > 0where w stands for the intersections and S for theword-count of the sentences.
The system returns thesimilarity of -1 for no overlap, which signals no sim-ilarity at all.
For fully overlapping feature sets, thescore is 1.For other features, where we did not expect themto occur in every sentence, for example numbers orfinancial terms, the similarity score was defined asfollows:score(w, s, l) ={1 : s = 0 or w = 0Jsc(w, l) : w > 0In this case the score would yield 1 decreasing fornon overlapping feature sets and will drop to -1 themore features differentiated.
This redefines the nor-mal state as equivalent to a total similarity of allfound features and only if features differ this valuedrops.3.2 Sentence Length GroupingFrom tests with the training data we found that oursystem performed very diversly with both long andshort sentences although our features where normal-ized to the sentence length.
To cover this problemwe separated the whole collection of training datainto different groups based on their length, each ofthe groups were later used to train their own model.Finally the testing data were also divided into thisgroups and were applied on the group model.3.3 Neural NetworkWe applyied multilayer perceptron neuronal net-works on the individual sentence length groups.
Sofor each group of sentence length we computed sep-arately the weights of the neural network.
To modelthe neural networks we used the open-source libraryNeuroph.2.
This network was defined with a 48-input layer, which represented the extracted featurescores, 4 hidden layers, and a 1-output layer whichrepresents the similarity score of the sentences.
Forthe runs referenced by table 1 and 2 we used 400000iterations, which gave us the best results in our tests,with a maximum error of 0.001 and a learning rateof 0.0012http://neuroph.sourceforge.net1404 Evaluation and ResultsThe following results of our system where producedby our test-run after the challenge deadline.
Forthe first run we split each training set in halfe, self-evident without the use of the datasets published af-ter the challenge, and used the other half to validateour system.
See table 1 for result, which contain oursystem.MSRvid MSRpar SMTeuroparlGrouping 0.69 0.55 0.50Without Grouping 0.66 0.52 0.62Table 1: Run with and without sentence length groupingon the training setFor the validation the whole 2013 test set wasused as it wasnot used for training.
In table 2 theresults of our system on the test-set are listed.
Whenusing the sentence length grouping and without sen-tence length grouping just using a single neural net-work for all sentence similarities.FNWN headlines OnWN SMTGrouping 0.08 0.66 0.62 0.21Without Grouping 0.38 0.62 0.39 0.25Table 2: Results of our system with and without sentencelength grouping on the test setFinally, we report the results from the originalevaluation of the STS-SharedTask in table 3.FNWN headlines OnWN SMTKnCe2013-all 0.11 0.35 0.35 0.16KnCe2013-diff 0.13 0.40 0.35 0.18KnCe2013-set 0.04 0.05 -0.15 -0.06Table 3: The submission to the challenge5 DiscussionBased on the results we can summarize that oursubmitted system, worked well for data with veryshort and simple sentences, such as the MSRvid;however for the longer the sentences the perfor-mance declined.
The grouping based on the inputlength worked well for sentences of similar lengthwhen compared, as we used the average length ofboth sentences to group them, but it seamed to failfor sentences with very diverse lengths like in theFNWN data set as shown in table 2.
Comparing theresults of the official submission to the test runs ofour system it underperformed in all datasets.
We as-sume that the poor results in the submission run werecaused by badly chosen training settings.6 ConclusionIn our system for semantic sentence similarity wetried to integrate a number of external knowledgebases to improve its performance.
(Viz.
WordNet,Wikipedia, Wiktionary) Furthermore, we integrateda neural network component to replicate the similar-ity score assigned by human judges.
We used dif-ferent sets of neural networks, depending on the sizeof the sentences.
In the evaluation we found thatour system worked well for the most datasets.
Butas soon as the pairs of sentences differed too muchin size, or the sentences were very long, the perfor-mance decreased.
In future work we will considerto tackle this problem with partial matching[3] andto introduces features to extract core statements ofshort texts.AcknowledgementsThe Know-Center is funded within the AustrianCOMET Program - Competence Centers for Excel-lent Technologies - under the auspices of the Aus-trian Federal Ministry of Transport, Innovation andTechnology, the Austrian Federal Ministry of Econ-omy, Family and Youth and by the State of Styria.COMET is managed by the Austrian Research Pro-motion Agency FFG.References[1] Eneko Agirre, Daniel Cer, Mona Diab, andAitor Gonza?lez.
Semeval-2012 task 6: A pi-lot on semantic textual similarity.
In SEM2012: The First Joint Conference on Lexicaland Computational Semantics (SemEval 2012),Montreal, Canada, 2012.
[2] Christiane Fellbaum, editor.
WordNet: An Elec-tronic Lexical Database.
MIT Press, Cam-bridge, MA, 1998.141[3] Prodromos Malakasiotis and Ion Androutsopou-los.
Learning textual entailment using svms andstring similarity measures.
[4] Nikos Malandrakis, Elias Iosif, and Alexan-dros Potamianos.
Deeppurple: estimating sen-tence semantic similarity using n-gram regres-sion models and web snippets.
In Proceed-ings of the First Joint Conference on Lexicaland Computational Semantics - Volume 1: Pro-ceedings of the main conference and the sharedtask, and Volume 2: Proceedings of the Sixth In-ternational Workshop on Semantic Evaluation,SemEval ?12, pages 565?570, Stroudsburg, PA,USA, 2012.
Association for Computational Lin-guistics.
[5] George A. Miller.
Wordnet: a lexical databasefor english.
Commun.
ACM, 38(11):39?41,November 1995.
[6] Frane S?aric?, Goran Glavas?, Mladen Karan, JanS?najder, and Bojana Dalbelo Bas?ic?.
Takelab:Systems for measuring semantic text similarity.In Proceedings of the Sixth International Work-shop on Semantic Evaluation (SemEval 2012),pages 441?448, Montre?al, Canada, 7-8 June2012.
Association for Computational Linguis-tics.142
