Proceedings of the 11th Meeting of the ACL-SIGMORPHON, ACL 2010, pages 28?37,Uppsala, Sweden, 15 July 2010. c?2010 Association for Computational LinguisticsMaximum Likelihood Estimation of Feature-based DistributionsJeffrey Heinz and Cesar KoiralaUniversity of DelawareNewark, Delaware, USA{heinz,koirala}@udel.eduAbstractMotivated by recent work in phonotac-tic learning (Hayes and Wilson 2008, Al-bright 2009), this paper shows how to de-fine feature-based probability distributionswhose parameters can be provably effi-ciently estimated.
The main idea is thatthese distributions are defined as a prod-uct of simpler distributions (cf.
Ghahra-mani and Jordan 1997).
One advantageof this framework is it draws attention towhat is minimally necessary to describeand learn phonological feature interactionsin phonotactic patterns.
The ?bottom-up?approach adopted here is contrasted withthe ?top-down?
approach in Hayes andWilson (2008), and it is argued that thebottom-up approach is more analyticallytransparent.1 IntroductionThe hypothesis that the atomic units of phonologyare phonological features, and not segments, is oneof the tenets of modern phonology (Jakobson etal., 1952; Chomsky and Halle, 1968).
Accord-ing to this hypothesis, segments are essentiallyepiphenomenal and exist only by virtue of beinga shorthand description of a collection of moreprimitive units?the features.
Incorporating thishypothesis into phonological learning models hasbeen the focus of much influential work (Gildeaand Jurafsky, 1996; Wilson, 2006; Hayes and Wil-son, 2008; Moreton, 2008; Albright, 2009).This paper makes three contributions.
The firstcontribution is a framework within which:1. researchers can choose which statistical in-dependence assumptions to make regardingphonological features;2. feature systems can be fully integrated intostrictly local (McNaughton and Papert, 1971)(i.e.
n-gram models (Jurafsky and Martin,2008)) and strictly piecewise models (Rogerset al, 2009; Heinz and Rogers, 2010) inorder to define families of provably well-formed, feature-based probability distribu-tions that are provably efficiently estimable.The main idea is to define a family of distribu-tions as the normalized product of simpler distri-butions.
Each simpler distribution can be repre-sented by a Probabilistic Deterministic Finite Ac-ceptor (PDFA), and the product of these PDFAsdefines the actual distribution.
When a family ofdistributions F is defined in this way, F may havemany fewer parameters than if F is defined overthe product PDFA directly.
This is because the pa-rameters of the distributions are defined in termsof the factors which combine in predictable waysvia the product.
Fewer parameters means accurateestimation occurs with less data and, relatedly, thefamily contains fewer distributions.This idea is not new.
It is explicit in Facto-rial Hidden Markov Models (FHMMs) (Ghahra-mani and Jordan, 1997; Saul and Jordan, 1999),and more recently underlies approaches to de-scribing and inferring regular string transductions(Dreyer et al, 2008; Dreyer and Eisner, 2009).Although HMMs and probabilistic finite-state au-tomata describe the same class of distributions(Vidal et al, 2005a; Vidal et al, 2005b), this paperpresents these ideas in formal language-theoreticand automata-theoretic terms because (1) there areno hidden states and is thus simpler than FHMMs,(2) determinstic automata have several desirableproperties crucially used here, and (3) PDFAsadd probabilities to structure whereas HMMs addstructure to probabilities and the authors are morecomfortable with the former perspective (for fur-ther discussion, see Vidal et al (2005a,b)).The second contribution illustrates the mainidea with a feature-based bigram model with a28strong statistical independence assumption: notwo features interact.
This is shown to capture ex-actly the intuition that sounds with like featureshave like distributions.
Also, the assumption ofnon-interacting features is shown to be too strongbecause like sounds do not have like distributionsin actual phonotactic patterns.
Four kinds of fea-tural interactions are identified and possible solu-tions are discussed.Finally, we compare this proposal with Hayesand Wilson (2008).
Essentially, the model hererepresents a ?bottom-up?
approach whereas theirsis ?top-down.?
?Top-down?
models, which con-sider every set of features as potentially interact-ing in every allowable context, face the difficultproblem of searching a vast space and often re-sort to heuristic-based methods, which are diffi-cult to analyze.
To illustrate, we suggest that therole played by phonological features in the phono-tactic learner in Hayes and Wilson (2008) is notwell-understood.
We demonstrate that classes ofall segments but one (i.e.
the complement classesof single segments) play a significant role, whichdiminishes the contribution provided by naturalclasses themselves (i.e.
ones made by phonologi-cal features).
In contrast, the proposed model hereis analytically transparent.This paper is organized as follows.
?2 reviewssome background.
?3 discusses bigram modelsand ?4 defines feature systems and feature-baseddistributions.
?5 develops a model with a strongindependence assumption and ?6 discusses feat-ural interaction.
?7 dicusses Hayes and Wilson(2008) and ?8 concludes.2 PreliminariesWe start with mostly standard notation.
P(A) isthe powerset of A. ?
denotes a finite set of sym-bols and a string over ?
is a finite sequence ofthese symbols.
?+ and ??
denote all strings overthis alphabet of nonzero but finite length, and ofany finite length, respectively.
A function f withdomain A and codomain B is written f : A ?
B.When discussing partial functions, the notation ?and ?
indicate for particular arguments whetherthe function is undefined and defined, respectively.A language L is a subset of ??.
A stochasticlanguage D is a probability distribution over ?
?.The probability p of word w with respect to D iswritten PrD(w) = p. Recall that all distributionsD must satisfy?w??
?PrD(w) = 1.
If L is lan-guage then PrD(L) =?w?LPrD(w).
Since alldistributions in this paper are stochastic languages,we use the two terms interchangeably.A Probabilistic Deterministic Finite-state Automaton (PDFA) is a tupleM = ?Q,?, q0, ?, F, T ?
where Q is the stateset, ?
is the alphabet, q0is the start state, ?
isa deterministic transition function, F and T arethe final-state and transition probabilities.
Inparticular, T : Q ?
?
?
R+ and F : Q ?
R+such thatfor all q ?
Q, F (q) +???
?T (q, ?)
= 1.
(1)PDFAs are typically represented as labeled di-rected graphs (e.g.
M?
in Figure 1).A PDFA M generates a stochastic languageDM.
If it exists, the (unique) path for a word w =a0.
.
.
akbelonging to ??
through a PDFA is asequence ?
(q0, a0), (q1, a1), .
.
.
, (qk, ak)?, whereqi+1= ?
(qi, ai).
The probability a PDFA assignstow is obtained by multiplying the transition prob-abilities with the final probability along w?s path ifit exists, and zero otherwise.PrDM(w) =(k?i=0T (qi, ai))?F (qk+1) (2)if ?d(q0, w)?
and 0 otherwiseA stochastic language is regular deterministic iffthere is a PDFA which generates it.The structural components of a PDFAM is thedeterministic finite-state automata (DFA) given bythe states Q, alphabet ?, transitions ?, and initialstate q0of M. By the structure of a PDFA, wemean its structural components.1 Each PDFA Mdefines a family of distributions given by the pos-sible instantiations of T and F satisfying Equa-tion 1.
These distributions have at most |Q|?
(|?|+1) parameters (since for each state there are |?|possible transitions plus the possibility of finality.
)These are, for all q ?
Q and ?
?
?, the proba-bilities T (q, ?)
and F (q).
To make the connectionto probability theory, we sometimes write these asPr(?
| q) and Pr(# | q), respectively.We define the product of PDFAs in terms ofco-emission probabilities (Vidal et al, 2005a).Let M1= ?Q1,?1, q01, ?1, F1, T1?
and M2=1This is up to the renaming of states so PDFA with iso-morphic structural components are said to have the samestructure.29?Q2,?2, q02, ?2, F2, T2?
be PDFAs.
The proba-bility that ?1is emitted from q1?
Q1at thesame moment ?2is emitted from q2?
Q2isCT (?1, ?2, q1, q2) = T1(q1, ?1)?T2(q2, ?2).
Sim-ilarly, the probability that a word simultaneouslyends at q1?
Q1and at q2?
Q2is CF (q1, q2) =F1(q1)?F2(q2).Definition 1 The normalized co-emission productof PDFAs M1and M2is M = M1?
M2=?Q,?, q0, ?, F, T ?
where1.
Q, q0, and F are defined in terms of thestandard DFA product over the state spaceQ1?Q2(Hopcroft et al, 2001).2. ?
= ?1?
?23.
For all ?q1, q2?
?
Q and ?
?1, ?2?
?
?, ?
(?q1, q2?, ?
?1, ?2?)
= ?q?1, q?2?
iff?1(q1, ?1) = q?1and ?2(q2, ?2) = q?2.24.
For all ?q1, q2?
?
Q,(a) let Z(?q1, q2?)
= CF (?q1, q2?)
+???1,?2??
?CT (?1, ?2, q1, q2) be thenormalization term; and(b) F (?q1, q2?)
=CF (q1,q2)Z; and(c) for all ?
?1, ?2?
?
?,T (?q1, q2?, ?
?1, ?2?)
=CT (??1,?2,q1,q2?
)ZIn other words, the numerators of T and F aredefined to be the co-emission probabilities, anddivision by Z ensures that M defines a well-formed probability distribution.3 The normalizedco-emission product effectively adopts a statisti-cal independence assumption between the statesof M1and M2.
If S is a list of PDFAs, we write?S for their product (note order of product is ir-relevant up to renaming of the states).The maximum likelihood (ML) estimation ofregular deterministic distributions is a solvedproblem when the structure of the PDFA is known(Vidal et al, 2005a; Vidal et al, 2005b; de laHiguera, 2010).
Let S be a finite sample of wordsdrawn from a regular deterministic distribution D.The problem is to estimate parameters T and F of2Note that restricting ?
to cases when ?1= ?2obtainsthe standard definition of ?
= ?1?
?2(Hopcroft et al, 2001).The reason we maintain two alphabets becomes clear in ?4.3Z(?q1, q2?)
is less than one whenever either F1(q1) orF2(q2) are neither zero nor one.M so that DMapproaches D using the widely-adopted ML criterion (Equation 3).
(?T ,?F ) = argmaxT,F(?w?SPrM(w))(3)It is well-known that if D is generated by somePDFAM?
with the same structural components asM, then the ML estimate of S with respect to Mguarantees that DMapproaches D as the size ofS goes to infinity (Vidal et al, 2005a; Vidal et al,2005b; de la Higuera, 2010).Finding the ML estimate of a finite sample Swith respect to M is simple provided M is de-terministic with known structural components.
In-formally, the corpus is passed through the PDFA,and the paths of each word through the corpus aretracked to obtain counts, which are then normal-ized by state.
Let M = ?Q,?, ?, q0, F, T ?
be thePDFA whose parameters F and T are to be esti-mated.
For all states q ?
Q and symbols ?
?
?,The ML estimation of the probability of T (q, ?
)is obtained by dividing the number of times thistransition is used in parsing the sample S by thenumber of times state q is encountered in the pars-ing of S. Similarly, the ML estimation of F (q) isobtained by calculating the relative frequency ofstate q being final with state q being encounteredin the parsing of S. For both cases, the division isnormalizing; i.e.
it guarantees that there is a well-formed probability distribution at each state.
Fig-ure 1 illustrates the counts obtained for a machineM with sample S = {abca}.4 Figure 1 showsa DFA with counts and the PDFA obtained afternormalizing these counts.3 Strictly local distributionsIn formal language theory, strictly k-local lan-guages occupy the bottom rung of a subregularhierarchy which makes distinctions on the basisof contiguous subsequences (McNaughton and Pa-pert, 1971; Rogers and Pullum, to appear; Rogerset al, 2009).
They are also the categorical coun-terpart to stochastic languages describable with n-gram models (where n = k) (Garcia et al, 1990;Jurafsky and Martin, 2008).
Since stochastic lan-guages are distributions, we refer to strictly k-local stochastic languages as strictly k-local distri-4Technically,M is neither a simple DFA or PDFA; rather,it has been called a Frequency DFA.
We do not formally de-fine them here, see de la Higuera (2010).30A:1a :2b:1c:1A:1/5a:2/5b:1/5c:1/5M M?Figure 1: M shows the counts obtained by parsingit with sample S = {abca}.
M?
shows the proba-bilities obtained after normalizing those counts.butions (SLDk).
We illustrate with SLD2(bigrammodels) for ease of exposition.For an alphabet ?, SL2distributions have(|?| + 1)2 parameters.
These are, for all ?, ?
??
?
{#}, the probabilities Pr(?
| ?).
The proba-bility of w = ?1.
.
.
?nis given in Equation 4.Pr(w)def= Pr(?1| #)?
Pr(?2| ?1)?
.
.
.?
Pr(# | ?n)(4)PDFA representations of SL2distributions havethe following structure: Q = ?
?
{#}, q0= #,and for all q ?
Q and ?
?
?, it is the case that?
(q, ?)
= ?.As an example, the DFA in Figure 2 providesthe structure of PDFAs which recognize SL2dis-tributions with ?
= {a, b, c}.
Plainly, the param-eters of the model are given by assigning proba-bilities to each transition and to the ending at eachstate.
In fact, for all ?
?
?
and ?
?
?
?
{#},Pr(?
| ?)
is T (?, ?)
and Pr(# | ?)
is F (?
).It follows that the probability of a particular paththrough the model corresponds to Equation 4.
Thestructure of a SL2distribution for alphabet ?
isgiven byMSL2(?
).Additionally, given a finite sample S ?
?
?, theML estimate of S with respect to the family ofdistributions describable with MSL2(?)
is givenby counting the parse of S through MSL2(?)
andthen normalizing as described in ?2.
This is equiv-alent to the procedure described in Jurafsky andMartin (2008, chap.
4).4 Feature-based distributionsThis section first introduces feature systems.
Thenit defines feature-based SL2distributions whichmake the strong independence assumption that notwo features interact.
It explains how to findbacbacbacbac#abcFigure 2: MSL2({a, b, c}) represents the structureof SL2distributions when ?
= {a, b, c}.F Ga + -b + +c - +Table 1: An example of a feature system with ?
={a, b, c} and two features F and G.the ML estimate of samples with respect to suchdistributions.
This section closes by identifyingkinds of featural interactions in phonotactic pat-terns, and discusses how such interactions can beaddressed within this framework.4.1 Feature systemsAssume the elements of the alphabet share prop-erties, called features.
For concreteness, let eachfeature be a total function F : ?
?
VF, wherethe codomain VFis a finite set of values.
A fi-nite vector of features F = ?F1, .
.
.
, Fn?
is calleda feature system.
Table 1 provides an exampleof a feature system with F = ?F,G?
and valuesVF= VG= {+,?
}.We extend the domain of all features F ?
Fto ?+, so that F (?1.
.
.
?n) = F (?1) .
.
.
F (?n).For example, using the feature system in Table 1,F (abc) = + + ?
and G(abc) = ?
+ +.
Wealso extend the domain of F to all languages:F (L) = ?w?Lf(w).
We also extend the notationso that F(?)
= ?F1(?
), .
.
.
, Fn(?)?.
For example,F(c) = ??F,+G?
(feature indices are includedfor readability).For feature F : ?
?
VF, let F?1 be the inversefunction with domain VFand codomain P(?
).For example in Table 1, G?1(+) = {b, c}.
F?1is similarly defined, i.e.
F?1(??F,+G?)
= {c}.31If, for all arguments ~v, F?1(~v) is nonempty thenthe feature system is exhaustive.
If, for all argu-ments ~v such that F?1(~v) is nonempty, it is thecase that |F?1(~v)| = 1 then the feature system isdistinctive.
E.g.
the feature system in Table 1 innot exhaustive since F?1(??F,?G?)
= ?, but it isdistinctive since where F?1 is nonempty, it picksout exactly one element of the alphabet.Generally, phonological feature systems for aparticular language are distinctive but not exhaus-tive.
Any feature system F can be made exhaustiveby adding finitely many symbols to the alphabet(since F is finite).
Let ??
denote an alphabet ob-tained by adding to ?
the fewest symbols whichmake F exhaustive.Each feature system also defines a set of indi-cator functions VF =?f?F(Vf?
{f}) with do-main ?
such that ?v, f?(?)
= 1 iff f(?)
= v and0 otherwise.
In the example in Table 1, VF ={+F,?F,+G,?G} (omitting angle braces forreadability).
For all f ?
F, the set VFfis theVF restricted to f .
So continuing our example,VFF= {+F,?F}.4.2 Feature-based distributionsWe now define feature-based SL2distributions un-der the strong independence assumption that notwo features interact.
For feature system F =?F1.
.
.
Fn?, there are n PDFAs, one for each fea-ture.
The normalized co-emission product of thesePDFAs essentially defines the distribution.
Foreach Fi, the structure of its PDFA is given byMSL2(VFi).
For example, MF= MSL2(VF )andMG= MSL2(VG) in figures 3 and 4 illustratethe finite-state representation of feature-based SL2distributions given the feature system in Table 1.5The states of each machine make distinctions ac-cording to features F and G, respectively.
The pa-rameters of these distributions are given by assign-ing probabilities to each transition and to the end-ing at each state (except for Pr(# | #)).6Thus there are 2|VF| +?F?F|VFF|2+ 1 pa-rameters for feature-based SL2distributions.
Forexample, the feature system in Table 1 defines adistribution with 2?
4 + 22 + 22 + 1 = 17 param-5For readability, featural information in the states andtransitions is included in these figures.
By definition, thestates and transitions are only labeled with elements of VFand VG, respectively.
In this case, that makes the structuresof the two machines identical.6It is possible to replace Pr(# | #) with two parameters,Pr(# | #F) Pr(# | #G), but for ease of exposition we donot pursue this further.-F-F+F+F-F+F-F+F#Figure 3: MFrepresents a SL2distribution withrespect to feature F.-G-G+G+G-G+G-G+G#Figure 4: MGrepresents a SL2distribution withrespect to feature G.eters, which include Pr(# | +F ), Pr(+F | #),Pr(+F | +F ), Pr(+F | ?F ), .
.
.
, the G equiva-lents, and Pr(# | #).
Let SLD2Fbe the family ofdistributions given by all possible parameter set-tings (i.e.
all possible probability assignments foreachMSL2(VFi) in accordance with Equation 1.
)The normalized co-emission product defines thefeature-based distribution.
For example, the struc-ture of the product of MFand MGis shown inFigure 5.As defined, the normalized co-emission productcan result in states and transitions that cannot beinterpreted by non-exhaustive feature systems.
Anexample of this is in Figure 5 since ??F,?G?
isnot interpretable by the feature system in Table 1.We make the system exhaustive by letting ??
=?
?
{d} and setting F(d) = ?
?F,?G?.What is the probability of a given b in thefeature-based model?
According to the normal-ized co-emission product (Defintion 1), it isPr(a | b) = Pr(?+F,?G?
| ?+F,+G?)
=Pr(+F | +F )?Pr(?G | +G)Zwhere Z = Z(?+F,+G?)
equals????
?Pr(F (?)
| +F )?Pr(G(?)
| +G)+ (Pr(# | +F )?Pr(# | +G)Generally, for an exhuastive distinctive featuresystem F = ?F1, .
.
.
, Fn?, and for all ?, ?
?
?,32#+F,-G+F,-G+F ,+G+F ,+G-F,+G-F,+G-F,-G-F,-G+F,-G+F ,+G-F,+G-F,-G+F,-G+F ,+G-F,+G-F,-G+F,-G+F ,+G-F,+G-F,-G+F,-G+F ,+G-F,+G-F,-GFigure 5: The structure of the product ofMFandMG.the Pr(?
| ?)
is given by Equation 5.
First, thenormalization term is provided.
LetZ(?)
=???????1?i?nPr(Fi(?)
| Fi(?))?
?+?1?i?nPr(# | Fi(?))ThenPr(?
| ?)
=?1?i?nPr(Fi(?)
| Fi(?))Z(?
)(5)The probabilities Pr(?
| #) and Pr(# | ?
)are similarly decomposed into featural parameters.Finally, like SL2distributions, the probability of aword w ?
??
is given by Equation 4.
We havethus proved the following.Theorem 1 The parameters of a feature-basedSL2distribution define a well-formed probabilitydistribution over ?
?.Proof It is sufficient to show for all ?
?
?
?
{#}that?????{#}Pr(?
| ?)
= 1 since in thiscase, Equation 4 yields a well-formed probabilitydistribution over ??.
This follows directly fromthe definition of the normalized co-emissionproduct (Definition 1).
The normalized co-emission product adopts astatistical independence assumption, which here isbetween features since each machine represents asingle feature.
For example, consider Pr(a | b) =Pr(??F,+G?
| ?+F,+G?).
The probabilityPr(??F,+G?
| ?+F,+G?)
cannot be arbitrar-ily different from the probabilities Pr(?F | +F )and Pr(+G | +G); it is not an independent pa-rameter.
In fact, because Pr(a | b) is computeddirectly as the normalized product of parametersPr(?F | +F ) and Pr(+G | +G), the assump-tion is that the features F and G do not interact.
Inother words, this model describes exactly the stateof affairs one expects if there is no statistical in-teraction between phonological features.
In termsof inference, this means if one sound is observedto occur in some context (at least contexts dis-tinguishable by SL2models), then similar sounds(i.e.
those that share many of its featural values)are expected to occur in this context as well.4.3 ML estimationThe ML estimate of feature-based SL2distribu-tions is obtained by counting the parse of a samplethrough each feature machine, and normalizing theresults.
This is because the parameters of the dis-tribution are the probabilities on the feature ma-chines, whose product determines the actual dis-tribution.
The following theorem follows imme-diately from the PDFA representation of feature-based SL2distributions.Theorem 2 Let F = ?F1, .
.
.
Fn?
and let D be de-scribed by M =?1?i?nMSL2(VF i).
Considera finite sample S drawn from D. Then the ML es-timate of S with respect to SLD2Fis obtained byfinding, for each Fi?
F, the ML estimate of Fi(S)with respect toMSL2(VF i).Proof The ML estimate of S with respect toSLD2Freturns the parameter values that maxi-mize the likelihood of S within the family SLD2F.The parameters of D ?SLD2Fare found on the33states of each MSL2(VFi).
By definition, eachMSL2(VFi) describes a probability distributionover Fi(??
), as well as a family of distributions.Therefore finding the MLE of S with respect toSLD2Fmeans finding the MLE estimate of Fi(S)with respect to eachMSL2(VFi).Optimizing the ML estimate of Fi(S) foreach Mi= MSL2(VFi) means that as |Fi(S)|increases, the estimates ?TMiand ?FMiapproachthe true values TMiand FMi.
It follows thatas |S| increases, ?TMand ?FMapproach the truevalues of TMand FMand consequently DMapproaches D. 4.4 DiscussionFeature-based models can have significantly fewerparameters than segment-based models.
Con-sider binary feature systems, where |VF| = 2|F|.An exhaustive feature system with 10 binary fea-tures describes an alphabet with 1024 symbols.Segment-based bigram models have (1024+1)2 =1, 050, 625 parameters, but the feature-based oneonly has 40 + 40 + 1 = 81 parameters!
Con-sequently, much less training data is required toaccurately estimate the parameters of the model.Another way of describing this is in terms of ex-pressivity.
For given feature system, feature-basedSL2distributions are a proper subset of SL2dis-tributions since, as the the PDFA representationsmake clear, every feature-based distribution can bedescribed by a segmental bigram model, but notvice versa.
The fact that feature-based distribu-tions have potentially far fewer parameters is a re-flection of the restrictive nature of the model.
Thestatistical independence assumption constrains thesystem in predictable ways.
The next sectionshows exactly what feature-based generalizationlooks like under these assumptions.5 ExamplesThis section demonstrates feature-based gener-alization by comparing it with segment-basedgeneralization, using a small corpus S ={aaab, caca, acab, cbb} and the feature systemin Table 1.
Tables 2 and 3 show the results ofML estimation of S with respect to segment-basedSL2distributions (unsmoothed bigram model)and feature-based SL2distributions, respectively.Each table shows the Pr(?
| ?)
for all ?, ?
?
{a, b, c, d,#} (where F(d) = ??F,?G?
), for?P(?
| ?
)a b c d #a 0.29 0.29 0.29 0.
0.14b 0.
0.25 0.
0.
0.75?
c 0.75 0.25 0.
0.
0.d 0.
0.
0.
0.
0.# 0.5 0.
0.5 0.
0.Table 2: ML estimates of parameters of segment-based SL2distributions.?P(?
| ?
)a b c d #a 0.22 0.43 0.17 0.09 0.09b 0.32 0.21 0.09 0.13 0.26?
c 0.60 0.40 0.
0 0.d 0.33 0.67 0 0 0# 0.25 0.25 0.25 0.25 0.Table 3: ML estimates of parameters of feature-based SL2distributions.ease of comparison.Observe the sharp divergence between the twomodels in certain cells.
For example, no words be-gin with b in the sample.
Hence the segment-basedML estimates of Pr(b | #) is zero.
Conversely,the feature-based ML estimate is nonzero becauseb, like a, is +F, and b, like c, is +G, and both aand c begin words.
Also, notice nonzero probabil-ities are assigned to d occuring after a and b. Thisis because F(d) = ??F,?G?
and the followingsequences all occur in the corpus: [+F][-F] (ac),[+G][-G] (ca), and [-G][-G] (aa).
On the otherhand, zero probabilities are assigned to d ocurringafter c and d because there are no cc sequences inthe corpus and hence the probability of [-F] occur-ing after [-F] is zero.This simple example demonstrates exactly howthe model works.
Generalizations are made on thebasis of individual features, not individual sym-bols.
In fact, segments are truly epiphenomenal inthis model, as demonstrated by the nonzero prob-abilties assigned to segments outside the originalfeature system (here, this is d).
To sum up, thismodel captures exactly the idea that the distribu-tion of segments is conditioned on the distribu-tions of its features.346 Featural interactionIn many empirical cases of interest, features dointeract, which suggests the strong independenceassumption is incorrect for modeling phonotacticlearning.There are at least four kinds of featural inter-action.
First, different features may be prohib-ited from occuring simultaneously in certain con-texts.
As an example of the first type considerthe fact that both velars and nasal sounds occurword-initially in English, but the velar nasal maynot.
Second, specific languages may prohibit dif-ferent features from simultaneously occuring in allcontexts.
In English, for example, there are syl-labic sounds and obstruents but no syllabic obstru-ents.
Third, different features may be universallyincompatible: e.g.
no vowels are both [+high] and[+low].
The last type of interaction is that differentfeatures may be prohibited from occuring syntag-matically.
For example, some languages prohibitvoiceless sounds from occuring after nasals.Although the independence assumption is toostrong, it is still useful.
First, it allows researchersto quantify the extent to which data can be ex-plained without invoking featural interaction.
Forexample, following Hayes and Wilson (2008), wemay be interested in how well human acceptabil-ity judgements collected by Scholes (1966) can beexplained if different features do not interact.
Af-ter training the feature-based SL2model on a cor-pus of word initial onsets adapted from the CMUpronouncing dictionary (Hayes and Wilson, 2008,395-396) and using a standard phonological fea-ture system (Hayes, 2009, chap.
4), it achievesa correlation (Spearman?s r) of 0.751.7 In otherwords, roughly three quarters of the acceptabilityjudgements are explained without relying on feat-ural interaction (or segments).Secondly, the incorrect predictions of the modelare in principle detectable.
For example, recallthat English has word-inital velars and nasals, butno word-inital velar nasals.
A one-cell chi-squaredtest can determine whether the observed numberof [#N] is significantly below the expected numberaccording to the feature-based distribution, whichcould lead to a new parameter being adopted todescribe the interaction of the [dorsal] and [nasal]7We use the feature chart in Hayes (2009) because it con-tains over 150 IPA symbols (and not just English phonemes).Featural combinations not in the chart were assumed to beimpossible (e.g.
[+high,+low]) and were zeroed out.features word-initially.
The details of these proce-dures are left for future research and are likely todraw from the rich literature on Bayesian networks(Pearl, 1989; Ghahramani, 1998).More important, however, is this framework al-lows researchers to construct the independence as-sumptions they want into the model in at least twoways.
First, universally incompatible features canbe excluded.
For example, suppose [-F] and [-G]in the feature system in Table 1 are anatomicallyincompatible like [+low] and [+high].
If desired,they can be excluded from the model essentiallyby zeroing out any probability mass assigned tosuch combinations and re-normalizing.Second, models can be defined where multiplefeatures are permitted to interact.
For example,suppose features F and G from Table 1 are em-bedded in a larger feature system.
The machinein Figure 5 can be defined to be a factor of themodel, and now interactions between F and G willbe learned, including syntagmatic ones.
The flex-ibility of the framework and the generality of thenormalized co-emission product allow researchersto consider feature-based distributions which al-low any two features to interact but which pro-hibit three-feature interactions, or which allow anythree features to interact but which prohibit four-feature interactions, or models where only certainfeatures are permitted to interact but not others(perhaps because they belong to the same node in afeature geometry (Clements, 1985; Clements andHume, 1995).87 Hayes and Wilson (2008)This section introduces the Hayes and Wilson(2008) (henceforth HW) phonotactic learner andshows that the contribution features play in gener-alization is not as clear as previously thought.HW propose an inductive model which ac-quires a maxent grammar defined by weightedconstraints.
Each constraint is described as a se-quence of natural classes using phonological fea-tures.
The constraint format also allows referenceto word boundaries and at most one complementclass.
(The complement class of S ?
?
is ?/S.
)For example, the constraint*#[?
-voice,+anterior,+strident][-approximant]means that in word-initial C1C2clusters, if C2is anasal or obstruent, then C1must be [s].8Note if all features are permitted to interact, this yieldsthe segmental bigram model.35Hayes and Wilson maxent models rfeatures & complement classes 0.946no features & complement classes 0.937features & no complement classes 0.914no features & no complement classes 0.885Table 4: Correlations of different settings versionsof HW maxent model with Scholes data.HW report that the model obtains a correlation(Spearman?s r) of 0.946 with blick test data fromScholes (1966).
HW and Albright (2009) attributethis high correlation to the model?s use of naturalclasses and phonological features.
HW also reportthat when the model is run without features, thegrammar obtained scores an r value of only 0.885,implying that the gain in correlation is due specif-ically to the use of phonological features.However, there are two relevant issues.
The firstis the use of complement classes.
If features arenot used but complement classes are (in effect onlyallowing the model to refer to single segments andthe complements of single segments, e.g.
[t] and[?t]) then in fact the grammar obtained scores anr value of 0.936, a result comparable to the onereported.9 Table 4 shows the r values obtained bythe HW learner under different conditions.
Notewe replicate the main result of r = 0.946 whenusing both features and complement classes.10This exercise reveals that phonological featuresplay a smaller role in the HW phonotactic learnerthan previously thought.
Features are helpful, butnot as much as complement classes of single seg-ments (though features with complement classesyields the best result by this measure).The second issue relates to the first: the questionof whether additional parameters are worth thegain in empirical coverage.
Wilson and Obdeyn(2009) provide an excellent discussion of themodel comparison literature and provide a rigor-ous comparative analysis of computational mod-eleling of OCP restrictions.
Here we only raise thequestions and leave the answers to future research.Compare the HW learners in the first two rowsin Table 4.
Is the ?
0.01 gain in r score worththe additional parameters which refer to phono-9Examination of the output grammar reveals heavy re-liance on the complement class [?s], which is not surprisinggiven the discussion of [sC] clusters in HW.10This software is available on Bruce Hayes?
webpage:http://www.linguistics.ucla.edu/people/hayes/Phonotactics/index.htm.logically natural classes?
Also, the feature-basedSL2model in ?4 only receives an r score of 0.751,much lower than the results in Table 4.
Yet thismodel has far fewer parameters not only becausethe maxent models in Table 4 keep track of tri-grams, but also because of its strong independenceassumption.
As mentioned, this result is infor-mative because it reveals how much can be ex-plained without featural interaction.
In the con-text of model comparison, this particular modelprovides an inductive baseline against which theutility of additional parameters invoking featuralinteraction ought to be measured.8 ConclusionThe current proposal explicitly embeds the Jakob-sonian hypothesis that the primitive unit ofphonology is the phonological feature into aphonotactic learning model.
While this paperspecifically shows how to integrate features inton-gram models to describe feature-based strictlyn-local distributions, these techniques can be ap-plied to other regular deterministic distributions,such as strictly k-piecewise models, which de-scribe long-distance dependencies, like the onesfound in consonant and vowel harmony (Heinz, toappear; Heinz and Rogers, 2010).In contrast to models which assume that allfeatures potentially interact, a baseline modelwas specifically introduced under the assumptionthat no two features interact.
In this way, the?bottom-up?
approach to feature-based general-ization shifts the focus of inquiry to the featuralinteractions necessary (and ultimately sufficient)to describe and learn phonotactic patterns.
Theframework introduced here shows how researcherscan study feature interaction in phonotactic mod-els in a systematic, transparent way.AcknowledgmentsWe thank Bill Idsardi, Tim O?Neill, Jim Rogers,Robert Wilder, Colin Wilson and the U. ofDelaware?s phonology/phonetics group for valu-able discussion.
Special thanks to Mark Ellisonfor helpful comments, to Adam Albright for illu-minating remarks on the types of featural interac-tion in phonotactic patterns, and to Jason Eisnerfor bringing to our attention FHMMs and other re-lated work.36ReferencesAdam Albright.
2009.
Feature-based generalisationas a source of gradient acceptability.
Phonology,26(1):9?41.Noam Chomsky and Morris Halle.
1968.
The SoundPattern of English.
Harper & Row, New York.G.N.
Clements and Elizabeth V. Hume.
1995.
Theinternal organization of speech sounds.
In John A.Goldsmith, editor, The handbook of phonologicaltheory, chapter 7.
Blackwell, Cambridge, MA.George N. Clements.
1985.
The geometry of phono-logical features.
Phonology Yearbook, 2:225?252.Colin de la Higuera.
2010.
Grammatical Inference:Learning Automata and Grammars.
CambridgeUniversity Press.Markus Dreyer and Jason Eisner.
2009.
Graphicalmodels over multiple strings.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 101?110, Singa-pore, August.Markus Dreyer, Jason R. Smith, and Jason Eisner.2008.
Latent-variable modeling of string transduc-tions with finite-state methods.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 1080?1089,Honolulu, October.Pedro Garcia, Enrique Vidal, and Jose?
Oncina.
1990.Learning locally testable languages in the strictsense.
In Proceedings of the Workshop on Algorith-mic Learning Theory, pages 325?338.Zoubin Ghahramani and Michael I. Jordan.
1997.
Fac-torial hidden markov models.
Machine Learning,29(2):245?273.Zoubin Ghahramani.
1998.
Learning dynamicbayesian networks.
In Adaptive Processing ofSequences and Data Structures, pages 168?197.Springer-Verlag.Daniel Gildea and Daniel Jurafsky.
1996.
Learn-ing bias and phonological-rule induction.
Compu-tational Linguistics, 24(4).Bruce Hayes and ColinWilson.
2008.
Amaximum en-tropy model of phonotactics and phonotactic learn-ing.
Linguistic Inquiry, 39:379?440.Bruce Hayes.
2009.
Introductory Phonology.
Wiley-Blackwell.Jeffrey Heinz and James Rogers.
2010.
Estimatingstrictly piecewise distributions.
In Proceedings ofthe 48th AnnualMeeting of the Association for Com-putational Linguistics, Uppsala, Sweden.Jeffrey Heinz.
to appear.
Learning long-distancephonotactics.
Linguistic Inquiry, 41(4).John Hopcroft, Rajeev Motwani, and Jeffrey Ullman.2001.
Introduction to Automata Theory, Languages,and Computation.
Boston, MA: Addison-Wesley.Roman Jakobson, C. Gunnar, M. Fant, and MorrisHalle.
1952.
Preliminaries to Speech Analysis.MIT Press.Daniel Jurafsky and James Martin.
2008.
Speechand Language Processing: An Introduction to Nat-ural Language Processing, Speech Recognition, andComputational Linguistics.
Prentice-Hall, UpperSaddle River, NJ, 2nd edition.Robert McNaughton and Seymour Papert.
1971.Counter-Free Automata.
MIT Press.Elliot Moreton.
2008.
Analytic bias and phonologicaltypology.
Phonology, 25(1):83?127.Judea Pearl.
1989.
Probabilistic Reasoning in In-telligent Systems: Networks of Plausible Inference.Morgan Kauffman.James Rogers and Geoffrey Pullum.
to appear.
Auralpattern recognition experiments and the subregularhierarchy.
Journal of Logic, Language and Infor-mation.James Rogers, Jeffrey Heinz, Gil Bailey, Matt Edlef-sen, Molly Visscher, David Wellcome, and SeanWibel.
2009.
On languages piecewise testable inthe strict sense.
In Proceedings of the 11th Meetingof the Assocation for Mathematics of Language.Lawrence K. Saul and Michael I. Jordan.
1999.
Mixedmemory markov models: Decomposing complexstochastic processes as mixtures of simpler ones.Machine Learning, 37(1):75?87.Robert J. Scholes.
1966.
Phonotactic grammaticality.Mouton, The Hague.Enrique Vidal, Franck Thollard, Colin de la Higuera,Francisco Casacuberta, and Rafael C. Carrasco.2005a.
Probabilistic finite-state machines-part I.IEEE Transactions on Pattern Analysis andMachineIntelligence, 27(7):1013?1025.Enrique Vidal, Frank Thollard, Colin de la Higuera,Francisco Casacuberta, and Rafael C. Carrasco.2005b.
Probabilistic finite-state machines-part II.IEEE Transactions on Pattern Analysis andMachineIntelligence, 27(7):1026?1039.Colin Wilson and Marieke Obdeyn.
2009.
Simplifyingsubsidiary theory: statistical evidence from arabic,muna, shona, and wargamay.
Johns Hopkins Uni-versity.Colin Wilson.
2006.
Learning phonology with sub-stantive bias: An experimental and computationalstudy of velar palatalization.
Cognitive Science,30(5):945?982.37
