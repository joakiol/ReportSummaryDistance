Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 556?560,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsEvolutionary Hierarchical Dirichlet Process for Timeline SummarizationJiwei LiSchool of Computer ScienceCornell UniversityIthaca, NY, 14853jl3226@cornell.eduSujian LiLaboratory of Computational LinguisticsPeking UniversityBejing, P.R.China, 150001lisujian@pku.edu.cnAbstractTimeline summarization aims at generat-ing concise summaries and giving read-ers a faster and better access to under-stand the evolution of news.
It is a newchallenge which combines salience rank-ing problem with novelty detection.
Pre-vious researches in this field seldom ex-plore the evolutionary pattern of topicssuch as birth, splitting, merging, develop-ing and death.
In this paper, we developa novel model called Evolutionary Hier-archical Dirichlet Process(EHDP) to cap-ture the topic evolution pattern in time-line summarization.
In EHDP, time vary-ing information is formulated as a seriesof HDPs by considering time-dependentinformation.
Experiments on 6 differentdatasets which contain 3156 documentsdemonstrates the good performance of oursystem with regard to ROUGE scores.1 IntroductionFaced with thousands of news articles, people usu-ally try to ask the general aspects such as thebeginning, the evolutionary pattern and the end.General search engines simply return the top rank-ing articles according to query relevance and failto trace how a specific event goes.
Timeline sum-marization, which aims at generating a series ofconcise summaries for news collection publishedat different epochs can give readers a faster andbetter access to understand the evolution of news.The key of timeline summarization is how toselect sentences which can tell readers the evolu-tionary pattern of topics in the event.
It is verycommon that the themes of a corpus evolve overtime, and topics of adjacent epochs usually exhibitstrong correlations.
Thus, it is important to modeltopics across different documents and over differ-ent time periods to detect how the events evolve.The task of timelime summarization is firstlyproposed by Allan et al(2001) by extracting clus-ters of noun phases and name entities.
Chieu etal.
(2004) built a similar system in unit of sentenceswith interest and burstiness.
However, these meth-ods seldom explored the evolutionary character-istics of news.
Recently, Yan et al(2011) ex-tended the graph based sentence ranking algorithmused in traditional multi-document summarization(MDS) to timeline generation by projecting sen-tences from different time into one plane.
Theyfurther explored the timeline task from the opti-mization of a function considering the combina-tion of different respects such as relevance, cover-age, coherence and diversity (Yan et al, 2011b).However, their approaches just treat timeline gen-eration as a sentence ranking or optimization prob-lem and seldom explore the topic information liedin the corpus.Recently, topic models have been widely usedfor capturing the dynamics of topics via time.Many dynamic approaches based on LDA model(Blei et al, 2003) or Hierarchical Dirichelt Pro-cesses(HDP) (Teh et al, 2006) have been pro-posed to discover the evolving patterns in the cor-pus as well as the snapshot clusters at each timeepoch (Blei and Lafferty, 2006; Chakrabarti et al,2006; Wang and McCallum, 2007; Caron et al,2007; Ren et al, 2008; Ahmed and Xing, 2008;Zhang et al, 2010).In this paper, we propose EHDP: a evolution-ary hierarchical Dirichlet process (HDP) modelfor timeline summarization.
In EHDP, each HDPis built for multiple corpora at each time epoch,and the time dependencies are incorporated intoepochs under the Markovian assumptions.
Topicpopularity and topic-word distribution can be in-ferred from a Chinese Restaurant Process (CRP).Sentences are selected into timelines by consider-ing different aspects such as topic relevance, cov-erage and coherence.
We built the evaluation sys-556tems which contain 6 real datasets and perfor-mance of different models is evaluated accord-ing to the ROUGE metrics.
Experimental resultsdemonstrate the effectiveness of our model .2 EHDP for Timeline Summarization2.1 Problem FormulationGiven a general query Q = {wqi}i=Qni=1 , we firstlyobtain a set of query related documents.
We no-tate different corpus as C = {Ct}t=Tt=1 accordingto their published time where Ct = {Dti}i=Nti=1 de-notes the document collection published at epocht.
Document Dti is formulated as a collection ofsentences {stij}j=Ntij=1 .
Each sentence is presentedwith a series of words stij = {wtijl}l=Ntijl=1 and as-sociated with a topic ?tij .
V denotes the vocabu-lary size.
The output of the algorithm is a seriesof timelines summarization I = {It}t=Tt=1 whereIt ?
Ct2.2 EHDPOur EHDP model is illustrated in Figure 2.
Specif-ically, each corpus Ct is modeled as a HDP.
TheseHDP shares an identical base measure G0, whichserves as an overall bookkeeping of overall mea-sures.
We use Gt0 to denote the base measure ateach epoch and draw the local measureGti for eachdocument at time t from Gt0.
In EHDP, each sen-tence is assigned to an aspect ?tij with the consid-eration of words within current sentence.To consider time dependency information inEHDP, we link all time specific base measures Gt0with a temporal Dirichlet mixture model as fol-lows:Gt0 ?
DP (?t,1KG0+1K??
?=0F (v, ?)?Gt?
?0 ) (1)where F (v, ?)
= exp(?
?/v) denotes the expo-nential kernel function that controls the influenceof neighboring corpus.
K denotes the normaliza-tion factor where K = 1 + ??
?=0 F (v, ?).
?
isthe time width and ?
is the decay factor.
In Chi-nese Restaurant Process (CRP), each document isreferred to a restaurant and sentences are com-pared to customers.
Customers in the restaurantsit around different tables and each table btin is as-sociated with a dish (topic) ?tin according to thedish menu.
Let mtk denote the number of ta-bles enjoying dish k in all restaurants at epoch t,mtk =?Nti=1?Ntibn=1 1(?tin = k).
We redefinefor each epoch t ?
[1, T ]1. draw global measureGt0 ?
DP (?, 1KG0 + 1K??
?=0 F (v, ?)Gt?
?0 )2. for each document Dti at epoch t,2.1 draw local measure Gti ?
DP (?,Gt0)2.2 for each sentence stij in Dtidraw aspect ?tij ?
Gtifor w ?
stij draw w ?
f(w)|?tijFigure 1: Generation process for EHDPanother parameter Mtk to incorporate time depen-dency into EHDP.Mtk =??
?=0F (v, ?)
?mt?
?,k (2)Let ntib denote the number of sentences sittingaround table b, in document i at epoch t. In CRPfor EHDP, when a new customer stij comes in,he can sit on the existing table with probabilityntib/(nti?1+?
), sharing the dish (topic) ?tib servedat that table or picking a new table with probabil-ity ?/(nti ?
1 + ?).
The customer has to selecta dish from the global dish menu if he chooses anew table.
A dish that has already been shared inthe global menu would be chosen with probabilityM tk/(?kM tk+?)
and a new dish with probability?/(?kM tk + ?).
?tij |?ti1, ..., ?tij?1, ?
??
?tb=?ijntibnti ?
1 + ??
?jb +?nti ?
1 + ??
?newjb?newti |?, ?
?
?kMtk?iMti + ??
?k +?
?iMti + ?G0 (3)We can see that EHDP degenerates into a series ofindependent HDPs when ?
= 0 and one globalHDP when ?
= T and v = ?, as discussed inAmred and Xings work (2008).2.3 Sentence Selection StrategyThe task of timeline summarization aims to pro-duce a summary for each time and the generatedsummary should meet criteria such as relevance ,coverage and coherence (Li et al, 2009).
To carefor these three criteria, we propose a topic scoringalgorithm based on Kullback-Leibler(KL) diver-gence.
We introduce the decreasing logistic func-tion ?
(x) = 1/(1 + ex) to map the distance intointerval (0,1).557Figure 2: Graphical model of EHDP.Relevance: the summary should be related withthe proposed query Q.FR(It) = ?
(KL(It||Q))Coverage: the summary should highly generalizeimportant topics mentioned in document collec-tion at epoch t.FCv(It) = ?
(KL(It||Ct))Coherence: News evolves over time and a goodcomponent summary is coherent with neighboringcorpus so that a timeline tracks the gradual evolu-tion trajectory for multiple correlative news.FCh(It) =??=?/2?=?
?/2 F (v, ?)
?
?(KL(It||Ct??))??=?/2?=?
?/2 F (v, ?
)Let Score(It) denote the score of the summaryand it is calculated in Equ.
(4).Score(It) = ?1FR(It)+?2FCv(It)+?3FCh(It)(4)?i ?i = 1.
Sentences with higher score are se-lected into timeline.
To avoid aspect redundancy,MMR strategy (Goldstein et al, 1999) is adoptedin the process of sentence selection.3 Experiments3.1 Experiments set-upWe downloaded 3156 news articles from selectedsources such as BBC, New York Times and CNNwith various time spans and built the evaluationsystems which contains 6 real datasets.
The newsbelongs to different categories of Rule of Interpre-tation (ROI) (Kumaran and Allan, 2004).
Detailedstatistics are shown in Table 1.
Dataset 2(Deep-water Horizon oil spill), 3(Haiti Earthquake) and5(Hurricane Sandy) are used as training data andNew Source Nation News Source NationBBC UK New York Times USGuardian UK Washington Post USCNN US Fox News USABC US MSNBC USTable 1: New sources of datasetsNews Subjects (Query) #docs #epoch1.Michael Jackson Death 744 1622.Deepwater Horizon oil spill 642 1273.Haiti Earthquake 247 834.American Presidential Election 1246 2865.Hurricane Sandy 317 586.Jerry Sandusky Sexual Abuse 320 74Table 2: Detailed information for datasetsthe rest are used as test data.
Summary at eachepoch is truncated to the same length of 50 words.Summaries produced by baseline systems andours are automatically evaluated through ROUGEevaluation metrics (Lin and Hovy, 2003).
Forthe space limit, we only report three ROUGEROUGE-2-F and ROUGE-W-F score.
Referencetimeline in ROUGE evaluation is manually gener-ated by using Amazon Mechanical Turk1.
Work-ers were asked to generate reference timeline fornews at each epoch in less than 50 words and wecollect 790 timelines in total.3.2 Parameter TuningTo tune the parameters ?
(i = 1, 2, 3) and v in oursystem, we adopt a gradient search strategy.
Wefirstly fix ?i to 1/3.
Then we perform experimentson with setting different values of v/#epoch inthe range from 0.02 to 0.2 at the interval of 0.02.We find that the Rouge score reaches its peak atround 0.1 and drops afterwards in the experiments.Next, we set the value of v is set to 0.1 ?
#epochand gradually change the value of ?1 from 0 to 1with interval of 0.05, with simultaneously fixing?2 and ?3 to the same value of (1 ?
?1)/2.
Theperformance gets better as ?1 increases from 0 to0.25 and then declines.
Then we set the value of?1 to 0.25 and change the value of ?2 from 0 to0.75 with interval of 0.05.
And the value of ?2 isset to 0.4, and ?3 is set to 0.35 correspondingly.3.3 Comparison with other topic modelsIn this subsection, we compare our model with4 topic model baselines on the test data.
Stand-HDP(1): A topic approach that models differenttime epochs as a series of independent HDPs with-out considering time dependency.
Stand-HDP(2):1http://mturk.com558M.J.
Death US Election S. Sexual AbuseSystem R2 RW R2 RW R2 RWEHDP 0.089 0.130 0.081 0.154 0.086 0.152Stand-HDP(1) 0.080 0.127 0.075 0.134 0.072 0.138Stand-HDP(2) 0.077 0.124 0.072 0.127 0.071 0.131Dyn-LDA 0.080 0.129 0.073 0.130 0.077 0.134Stan-LDA 0.072 0.117 0.065 0.122 0.071 0.121Table 3: Comparison with topic modelsM.J.
Death US Election S. Sexual AbuseSystem R2 RW R2 RW R2 RWEHDP 0.089 0.130 0.081 0.154 0.086 0.152Centroid 0.057 0.101 0.054 0.098 0.060 0.132Manifold 0.053 0.108 0.060 0.111 0.069 0.128ETS 0.078 0.120 0.073 0.130 0.075 0.135Chieu 0.064 0.107 0.064 0.122 0.071 0.131Table 4: Comparison with other baselinesA global HDP which models the whole time spanas a restaurant.
The third baseline, Dynamic-LDA is based on Blei and Laffery(2007)?s workand Stan-LDA is based on standard LDA model.In LDA based models, aspect number is prede-fined as 80 2.
Experimental results of differentmodels are shown in Table 2.
As we can see,EHDP achieves better results than the two stan-dard HDP baselines where time information is notadequately considered.
We also find an interestingresult that Stan-HDP performs better than Stan-LDA.
This is partly because new aspects can beautomatically detected in HDP.
As we know, howto determine topic number in the LDA-based mod-els is still an open problem.3.4 Comparison with other baselinesWe implement several baselines used in tradi-tional summarization or timeline summarizationfor comparison.
(1) Centroid applies the MEADalgorithm (Radev et al, 2004) according to thefeatures including centroid value, position andfirst-sentence overlap.
(2) Manifold is a graphbased unsupervised method for summarization,and the score of each sentence is got from thepropagation through the graph (Wan et al, 2007).
(3) ETS is the timeline summarization approachdeveloped by Yan et al, (2011a), which is a graphbased approach with optimized global and localbiased summarization.
(4) Chieu is the time-line system provided by (Chieu and Lee, 2004)utilizing interest and bursty ranking but neglect-ing trans-temporal news evolution.
As we cansee from Table 3, Centroid and Manifold getthe worst results.
This is probably because meth-ods in multi-document summarization only care2In our experiments, the aspect number is set as 50, 80,100 and 120 respectively and we select the best performedresult with the aspect number as 80about sentence selection and neglect the noveltydetection task.
We can also see that EHDP underour proposed framework outputs existing timelinesummarization approaches ETS and chieu.
Ourapproach outputs Yan et al,(2011a)s model by6.9% and 9.3% respectively with regard to the av-erage score of ROUGE-2-F and ROUGE-W-F.4 ConclusionIn this paper we present an evolutionary HDPmodel for timeline summarization.
Our EHDP ex-tends original HDP by incorporating time depen-dencies and background information.
We also de-velop an effective sentence selection strategy forcandidate in the summaries.
Experimental resultson real multi-time news demonstrate the effective-ness of our topic model.Oct.
3, 2012S1: The first debate between President Obama and Mitt Rom-ney, so long anticipated, quickly sunk into an unenlighteningrecitation of tired talking points and mendacity.
S2.
Mr. Rom-ney wants to restore the Bush-era tax cut that expires at the endof this year and largely benefits the wealthyOct.
11, 2012S1: The vice presidential debate took place on Thursday, Oc-tober 11 at Kentucky?sCentre College, and was moderated byMartha Raddatz.
S2: The first and only debate between VicePresident Joe Biden and Congressman Paul Ryan focused ondomestic and foreign policy.
The domestic policy segments in-cluded questions on health care, abortionOct.
16, 2012S1.
President Obama fights back in his second debate with MittRomney, banishing some of the doubts he raised in their firstshowdown.
S2: The second debate dealt primarily with domes-tic affairs and include some segues into foreign policy.
includ-ing taxes, unemployment, job creation, the national debt, energyand women?s rights, both legal andTable 5: Selected timeline summarization gener-ated by EHDP for American Presidential Election5 AcknowledgementThis research has been supported by NSFC grants(No.61273278), National Key Technology RDProgram (No:2011BAH1B0403), National 863Program (No.2012AA011101) and National So-cial Science Foundation (No.12ZD227).ReferencesAmr Ahmed and Eric Xing.
Dynamic non-parametricmixture models and the recurrent chinese restaurantprocess.
2008.
In SDM.559James Allan, Rahul Gupta and Vikas Khandelwal.Temporal summaries of new topics.
2001.
In Pro-ceedings of the 24th annual international ACM SI-GIR conference on Research and development in in-formation retrievalDavid Blei, Andrew Ng and Micheal Jordan.
2003.Latent dirichlet alocation.
In Journal of MachineLearning Research.David Blei and John Lafferty.
Dynamic topic models.2006.
In Proceedings of the 23rd international con-ference on Machine learning.Francois Carol, Manuel Davy and Arnaud Doucet.Generalized poly urn for time-varying dirichlet pro-cess mixtures.
2007.
In Proceedings of the Interna-tional Conference on Uncertainty in Artificial Intel-ligence.Deepayan Chakrabarti, Ravi Kumar and AndrewTomkins.
Evolutionary Clustering.
InProceedings ofthe 12th ACM SIGKDD international conferenceKnowledge discoveryand data mining.Hai-Leong Chieu and Yoong-Keok Lee.
Query basedevent extraction along a timeline.
In Proceedings ofthe 27th annual international ACM SIGIR confer-ence on Research and development in informationretrievalGiridhar Kumaran and James Allan.
2004.
Text classifi-cation and named entities for new event detection.
InProceedings of the 27th annual international ACMSIGIR04.Liangda Li, Ke Zhou, Gui-Rong Xue, Hongyuan Zhaand Yong Yu.
Enhancing diversity, coverage and bal-ance for summarization through structure learning.In Proceedings of the 18th international conferenceon World wide web.Chin-Yew Lin and Eduard Hovy.
Automatic evaluationof summaries using n-gram co-occurrence statistics.In Proceedings of the Human Language TechnologyConference of the NAACL.
2003.Dragomar Radev, Hongyan.
Jing, and Malgorzata Stys.2004.
Centroid-based summarization of multipledocuments.
In Information Processing and Manage-ment.Lu Ren, David Dunson and Lawrence Carin.
The dy-namic hierarchical Dirichlet process.
2008.
In Pro-ceedings of the 25th international conference onMachine Learning.Xiaojun Wan, Jianwu Yang and Jianguo Xiao.2007.
Manifold-ranking based topic-focused multi-document summarization.
In Proceedings of Inter-national Joint Conference on Artificial Intelligence.Xuerui Wang and Andrew MaCallum.
Topics overtime: a non-Markov continuous-time model of topi-cal trends.
In Proceedings of the 12th ACM SIGKDDinternational conference on Knowledge discoveryand data mining.Yee Whye Teh, Michael Jordan, Matthew Beal andDavid Blei.
Hierarchical Dirichlet Processes.
InAmerican Statistical Association.Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,Xiaoming Li and Yan Zhang.
2011a.
EvolutionaryTimeline Summarization: a Balanced OptimizationFramework via Iterative Substitution.
In Proceed-ings of the 34th international ACM SIGIR confer-ence on Research and development in InformationRetrieval.Rui Yan, Liang Kong, Congrui Huang, Xiaojun Wan,Jahna Otterbacher, Xiaoming Li and Yan Zhang.Timeline Generation Evolutionary Trans-TemporalSummarization.
2011b.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing.Jianwen Zhang, Yangqiu Song, Changshui Zhang andShixia Liu.
2010.
Evolutionary Hierarchical Dirich-let Processes for Multiple Correlated Time-varyingCorpora.
In Proceedings of the 16th ACM SIGKDDinternational conference on Knowledge discoveryand data mining.560
