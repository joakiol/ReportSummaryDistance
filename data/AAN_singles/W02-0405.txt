Using Summaries in Document RetrievalMark WassonLexisNexis, a Division of Reed Elsevier, plc9443 Springboro PikeMiamisburg, OH, USA 45342mark.wasson@lexisnexis.comAbstractThis paper examines the role that summariescan play in document retrieval.
Thirtysearches are applied to full-text andsummaries only in large documentcollections, and the results are evaluatedusing two different evaluation scopes.
Theresults support the view that those customersegments who want smaller answer setsfocused on highly relevant documentsbenefit from limiting their searches tosummaries.
On the other hand, thosecustomer segments who wish to retrieve allreferences to some topic should continue tosearch full-text.1 IntroductionThe goal that motivated the creation ofSearchable LEAD in news documents in theLexisNexis collection was to provide somecustomer segments with a tool that helps themfocus their retrieval results on a limited numberof highly relevant documents, where a highlyrelevant document with respect to some query isa document that is substantially about the querytopic.A good, general purpose document summaryshould capture the major topics presented in adocument.
Presumably if we can capture majortopics in summaries, then a search that isrestricted to summaries should do a better job oflimiting retrieval results to highly relevantdocuments about those topics.To support this, we created SearchableLEAD, a process that identifies and labels theleading sentences or paragraphs of newsdocuments as a separate searchable LEAD field.A customer?s query, e.g., BUSH AND GORE,could easily be limited to the LEAD, e.g.,LEAD(BUSH AND GORE), or HEADLINEand LEAD combination, e.g., HLEAD(BUSHAND GORE).
Through three separateexperiments, the value of leading text as ageneral purpose summary for news documentshas been verified.
This paper describes a fourthexperiment that investigates whether and howsearches limited to this type of summary benefitthe targeted customersIn many information retrieval experiments, asingle user perspective, i.e., a single answer key,is used to evaluate the results.
If thatperspective matches that of the targetedcustomer set, the evaluation is meaningful.However, different customer segments performinformation seeking tasks with different goalsand perspectives in mind, even when they areinterested in the same topic.
Just as potentialsearch tool enhancements are not one-size-fits-all, a one-size-fits-all answer key should not beused to determine the value of a search aid fortwo sets of customers with fundamentallydifferent goals.
In this experiment, the results ofeach query were evaluated using two differentuser perspectives, highly relevant referencesonly and all references.
Through this approachwe were able to determine whether SearchableLEAD satisfied the goal that motivated itscreation.2 Defining a Summary for NewsArticlesFor this investigation, the leading text of newsdocuments is used as a basis for creatingdocument summaries ?
specifically thedefinition of Searchable LEAD found in Wasson(1998).Brandow et al (1995) compared summariesthey created using tf-idf-based sentenceextraction to fixed amounts of leading text ?Philadelphia, July 2002, pp.
37-44.
Association for Computational Linguistics.Proceedings of the Workshop on Automatic Summarization (including DUC 2002),approximately 60, 150 and 250 words long, inthree separate trials ?
generated using a slightlymodified version of our production SearchableLEAD text processing software.
In that effort,Searchable LEAD-based extracts were judged tobe acceptable as summaries for general newsarticles 92% of the time.
This comparedfavorably to the 74% reported for thosesummaries created through sentence extraction.However, that test was limited to only 250 newsarticles.Wasson (1998) reported on a larger scaleversion of this evaluation, although in that workSearchable LEAD was used as-is.
SearchableLEAD-based extracts resulted in an averagecompression ratio of 13% in that test.Compression ratios generally ranged betweenabout 5-20% for most documents, depending ondocument length; with Searchable LEAD, thenumber of leading sentences and paragraphsincluded in the leading text field was linked todocument length.
For a shorter document, theSearchable LEAD might consist of only a singlesentence.
For long documents, SearchableLEAD might consist of the first three paragraphsor more of the document.The Searchable LEAD-based extracts wereevaluated on their acceptability as summaries inmore than 2,727 documents.
For the 1,951general news articles in that test corpus,Searchable LEADs were judged to be acceptableas summaries 94.1% of the time, a result that isnot appreciably different from that reported byBrandow et al (1995), especially when sevennewsbrief type documents are excluded fromtheir results.
For the other types of documentsin the corpus, including lists, newsbriefs andtranscripts, acceptability rates were somewhat tosubstantially lower, as Table 1 shows.DocumentTypeNumber ofDocumentsAcceptabilityRateGeneral News 1,951 94.1%Lists 86 12.8%Newsbriefs 191 24.6%Transcripts 499 70.3%Table 1.
LEAD as Summary AcceptabilityRates for Document TypesZhou (1999) reported the results of anexperiment where Searchable LEADs werecompared to summaries created by two internalprototype and three commercially availablesentence extraction summary generators in adocument relevance judgment task, whereevaluators used each summary to determine thecorresponding document?s relevance to a topic.The result of that evaluation showed that the topfive systems, including Searchable LEAD,statistically tied in this task.3 Related WorkIn addition to evaluating the value of bothSearchable LEAD-based and sentenceextraction-based extracts for their value asgeneral summaries, Brandow et al (1995) alsoreported on the results of a limited experimentexamining the differences between summary-only versus full-text searching.
In testsinvolving twelve Boolean queries applied to acorpus of about 20,000 documents extractedfrom the LexisNexis NEWS library, they foundthat average precision increased from 37% forsearches applied to full-text to 45% for searchesapplied to sentence extraction-based extracts and47% for searches applied to leading text-basedextracts.
This was more than offset by largedrops in relative recall, 100% for full-textcompared to 56% for sentence extraction-basedextracts and 58% for leading text-based extracts.
(Relative recall assumes that the full-text queriesachieved 100% recall; due to limited resourceson the project, there was no attempt to determineactual recall rates.
)In addition to its limited scale, there weretwo key problems with this evaluation.
First,although Brandow et al (1995) correctlyreported that Searchable LEAD was introducedto enhance search precision, Searchable LEADalso targeted only a subset of our customersegments, specifically those customers whowanted to retrieve only highly relevantdocuments (in LexisNexis-internal jargon, werefer to these as on-point or major referencedocuments).
This point was not mentioned inBrandow et al (1995), nor was it reflected intheir search evaluation.
Second, theconvenience of using relative recallnotwithstanding, this approach to measuringrecall will generally magnify the difference inrecall that should be expected when comparingfull-text and summary-only search results.Sumita & Iida (1997) tested both leading text-based extracts and tf-idf-based sentence extractsof up to three sentences in an experimentinvolving 10 queries and 600 Japanese languagenews articles.
They reported that limitingsearches to such summaries both improved theeffectiveness for retrieving highly relevantdocuments, but also helped exclude otherrelevant documents with lower levels ofrelevance.Sakai & Sparck Jones (2001) examined thevalue of summaries for general informationretrieval and a pseudo-relevance feedbackmodel, in their case using 30 queries applied to anearly-39,000 document corpus derived from theTREC collection.
The Okapi Basic SearchSystem was used.
Precision evaluation focusedon both the top 1000 and top 10 relevanceranked documents retrieved.
The authorsconcluded that a summary-only search may beas effective as full-text for precision-orientedsearching of highly relevant documents.Incorporating both summaries and full-textdocuments into their pseudo-relevance feedbackmodel was significantly more effective thanusing summaries only.4 User Evaluation ScopesMost information retrieval experiments calculaterecall, precision and the corresponding f-measure from a single evaluation perspective orevaluation scope.
All documents are judged tobe relevant or irrelevant with respect to that onescope.
However, commercial informationservices now report that they handle millions ofsearches a day for their customers.
It is notreasonable to assume that all of the people usingthose services have the same perspective onrelevance, and yet that is often how we evaluatenew search aids and features.Our customers employ a variety of searchstrategies, depending on their topics, informationinterests, and the point they are at in theirinformation seeking task.
At one end, we seesome customers just starting out on aninformation seeking task, where they typicallyare looking for a few highly relevant documentsto help introduce themselves to the topic.Basically they are trying to provide themselveswith a good starting point.
At the other extreme,we see customers in public relations,competitive intelligence or in the due diligencephase of their information seeking task.
Thesecustomers often want to retrieve all references tothe topic, even those documents that provideeven the most limited or mundane information.Although some may see this simply as thecustomary recall-precision trade-off, that is notthe case.
A document that contains a passingreference to some topic is relevant to those withthe all reference evaluation scope (retrieval ofthat document is considered successful recall),but it is irrelevant to those with a highly relevantreference evaluation scope (retrieval of thatdocument is considered a precision error).
Adocument?s relevance with respect to somecustomer?s evaluation scope is what drivescustomer perceptions of the resulting recall andprecision.
Instead of a recall-precision trade-off,we have multiple evaluation scopes for whichrecall and precision are determined.We recognize the differences in evaluationscopes in a single user over time whenproposing learning systems and personalizationtools that adapt retrieval or routing results to auser?s changing interests (e.g., Lam et al, 1996),but we do not recognize these differences whenwe use single answer key evaluations.
As aresult, over the years, we have seen a number ofpotentially useful search enhancementsdismissed not because they failed to showimprovement for any targeted subset ofcustomers, but rather because they failed toshow improvement when using a single generalevaluation standard (Harmon, 1991; Voorhees,1994; Sparck Jones, 1999).
Query expansionfunctionality such as some types ofmorphological or synonym expansion, forexample, may produce a drop in precision thatoffsets any improvements to recall, but we havefound that customer segments who requireretrieving all references to their topic are willingto put up with a lot of irrelevant information tomake sure that they see everything.
Of course,those customers would still like to have betterprecision, but they require better recall.This was also a problem with the limitedretrieval experiment reported in Brandow et al(1995).
Although Searchable LEAD wasintroduced specifically to support the subset ofusers seeking only highly relevant documents,Brandow et al (1995) did not make thisdistinction when evaluating their test of twelveBoolean queries.For each query evaluated in the experimentreported here, two user evaluation scopes werecreated.
One represented Searchable LEAD?stargeted customer segment and its desire toretrieve only highly relevant documents; theother represented the due diligence customersegment, which prefers to retrieve all documentsthat contain information about the topicregardless of how little or how much.5 The Experiment5.1 Test CorpusSearchable LEAD was tested in the LexisNexisNEWS library, a commercial collection of full-text news documents from thousands of sources,including newspapers, magazines, wire services,abstract services, trade journals, transcriptservices and other sources.
The document typesin this document collection reflected thesesources.
Date-bounded subsets of this collectionwere used, with date ranges varying in lengthfrom one day (typically more than 45,000documents searched) to two years (typicallymore than 32 million documents searched).5.2 Search Topics and Topic ScopeFor this investigation thirty topics were selectedand defined.
The following are a few of thetopics included in the set of topics:?
General information about Exxon Corp.?
Biographical information about BillMcCartney, founder of Promise Keepers?
Office Depot revenue and earningsinformation?
A specific Dallas Cowboys-CincinnatiBengals football game?
Expensive outhouses in national parksFor each of the thirty topics, two scopestatements were created, where a scopestatement is a description of what is considered arelevant document with respect to the topic.One scope statement, the highly relevantreference evaluation scope, defined what wouldconstitute a highly relevant document.
Thesescope statements typically combinedquantitative measures with a number of specificpieces of information that must be present in aretrieved document for it to be considered highlyrelevant.
Requiring some specific pieces ofinformation to be present added objectivity tothe evaluation process.The second scope statement, the all referenceevaluation scope, defined the minimuminformation about the topic that must be presentin order to consider the document relevant fromthat perspective.
For a named entity topic, adocument relevant to the all reference scopemight include as little as a single occurrence ofthe entity?s name.The highly relevant reference evaluationscope for the Office Depot query required amongother things revenues, earnings (loss)information, and related per-share information.The all reference evaluation scope required atleast one of the financial performance measures,with revenue typically being the one found inretrieved documents.The highly relevant reference evaluationscope for the Dallas Cowboys-CincinnatiBengals football game query required somespecific game statistics, none of which wererequired for the all reference evaluation scope.Thus, a pre-game story concerning whether aplayer might play was relevant to the allreference evaluation scope but it was irrelevantto the highly relevant reference evaluationscope.
After all, articles written before the gametook place obviously could not include gamestatistics.More than half the topics focused on namedentities.
This is consistent with our observationsof customer search topics applied to news data,and this user behavior has also been reportedelsewhere (e.g., Thompson & Dozier, 1997).One effect of this was that the recall andprecision rates we would observe in thisexperiment were higher than what is commonlyreported for Boolean search results.
Becausemany proper names are relatively unambiguous,and because articles about some named entityalmost always mention the name, some of thequeries had much higher accuracy rates thanmight otherwise be expected, and that pulledoverall average accuracy rates up somewhat.The Boolean search EXXON, for example,virtually assures us of 100% recall regardless ofwhich evaluation scope is used.
Althoughindividual Exxon service stations are mentionedperiodically in the news, most news articles thatmention Exxon are in fact about the major oilcompany, ensuring fairly high precision for theall references evaluation scope.5.3 QueriesSearchable LEAD was created to be used with aBoolean search engine.
With 20% of newsdocuments in our archives containing fewer than100 words, a sizeable number of documentshave one-sentence LEADs, which would be oflittle value to search engines that rely on termfrequency.Through our own experience and routineobservations of World Wide Web searchers,most customer queries are quite short, typicallyone or two words or phrases, perhaps connectedby one Boolean operator.
Similarly shortqueries were created for use in this evaluation,such as the following:?
EXXON?
BILL MCCARTNEY?
OFFICE DEPOT AND EARNINGS?
BENGALS AND COWBOYS?
NATIONAL PARK AND OUTHOUSEIn some cases, a date restriction was explicitlyadded to the query.
In all other cases, a mostrecent two-year period default date restrictionwas used.There was no attempt to maximize theaccuracy of the queries tested.
Rather, the goalwas to use queries that mimic typical userbehavior in order to see how Searchable LEADimpacts typical users.5.4 TestingEach query was applied and correspondingretrieval results evaluated in four ways, once foreach evaluation scope-text scope combination:?
All reference, full-text?
All reference, LEAD only?
Highly relevant reference, full-text?
Highly relevant reference, LEAD onlyThe all reference/full-text combination wasevaluated first.
Because this combinationretrieves at least all the documents retrieved byany of the other  search-evaluation scopecombinations, it was possible to use the resultsof this evaluation to create an answer key thatcould also be used by the other evaluations inorder to ensure consistency of documentrelevance judgments with respect to evaluationscope for all the combinations.Each test query was applied to all of thedocuments in date-restricted subsets of the AllNews (ALLNWS) file in the LexisNexis NEWSlibrary.
A date restriction was used to limit thenumber of documents to be examined whenverifying the results.
In addition to applying andevaluating the query created for a given topic,additional queries were used in order to findpotential recall errors, that is, relevantdocuments with respect to the evaluation scopeof the topic that were missed by the originalquery.
For the Dallas Cowboys-CincinnatiBengals football game topic, for example, inaddition to the test query BENGALS ANDCOWBOYS, other queries used to search thedate range of documents in order to identifypotential recall errors included the following:?
CINCINNATI AND (COWBOYS ORFOOTBALL) AND NOT(BENGALSAND COWBOYS)?
DALLAS AND (BENGALS ORFOOTBALL OR OHIO) ANDNOT(BENGALS AND COWBOYS)?
CINERGY AND NOT(BENGALS ANDCOWBOYS) (Cinergy Field is the name ofthe football field where the game wasplayed)All documents retrieved by such queries wereexamined for their degree of relevance in orderto produce more accurate recall results in thistest.There was no particular attempt to match thedate range exactly to a specific event, acharacteristic of this test (and typical userbehavior) that often contributed to the number ofprecision errors.
For example, the DallasCowboys-Cincinnati Bengals football gameoccurred in the previous week, specifically threedays earlier, but documents retrieved from theentire week were examined.
Criteria for ahighly relevant reference to this game includedcertain game statistics.
Stories written beforethe game could not possibly include suchinformation, so they were counted as precisionerrors for the highly relevant referenceevaluation scope.
From a customer?sperspective, our routine reverse chronologicalpresentation of retrieved documents would haveeffectively hidden such errors from customersuntil after the desired information was obtained.For evaluation purposes, however, the entiredate range was evaluated.Full-text queries were limited to HEADLINEand BODY fields of documents.
LEAD onlyqueries were limited to the LEAD sub-field ofthe BODY field.
Most news documents in theLexisNexis service also have one or more meta-data fields that may include named entity and/ortopic-indicating controlled vocabulary terms, inaddition to other information.
Limiting queriesto the HEADLINE, BODY and LEAD fieldsfocused the evaluation on the impact of usingsummaries as opposed to that of using otherpossible editorial enhancements of the data.5.5 EvaluationThe purpose of Searchable LEAD as a retrievalaid is to help some customer segments retrieve ahighly relevant documents about some topic, andto minimize the number of irrelevant documentsand documents that only contain passingreferences to the topic in the answer set.
IfSearchable LEAD works, one would expect thatqueries restricted to the LEAD field would resultin higher precision than queries applied to thefull-text would.For the all reference evaluation scope, onewould expect recall to fall when shifting fromfull-text to LEAD.
After all, a general summarylike LEAD typically only includes informationon major points in the document.The impact on recall for the highly relevantreference evaluation scope is less certain.Because the Searchable LEAD represents anacceptable summary in only 94% of generalnews articles, and a lower figure in other typesof documents found in the LexisNexis NEWSlibrary, it is also reasonable to assume that somedecline in recall would also occur with thisevaluation scope.
Given that relevantdocuments with this evaluation scope mustinclude all the targeted information, recall errorsas defined by this scope may actually eliminateinformation redundancy, and thus are notnecessarily critical to the customer.
However,the way in which basic pieces of information arepresented can also be revealing, so suchredundant documents may still be useful.Calculating recall in these cases thus is stillworthwhile.Recall and precision rates were calculated foreach query for each evaluation scope-text scopecombination.
For each full-text/LEAD pair,recall and precision rates were compared to seehow consistent increases and decreases werewith respect to expectations.6 ResultsThirty queries were applied first to full-text andthen limited to LEAD only.
The results of eachapproach were evaluated twice, once from theall reference evaluation scope and once from thehighly relevant reference evaluation scope.For the customer perspective that SearchableLEAD targets ?
the highly relevant referenceevaluation scope ?
there was a sizeableimprovement in answer set precision, whichincreased an average of .286, from an average of.230 to an average of .516 when the query waslimited to the LEAD.
As Table 2 also shows,recall decreased an average of .192 across thethirty queries.
The average standard f-measureacross the thirty queries increased .150, from.300 to .450.Full-text LEADAvg.
Recall .785 .593Avg.
Precision .230 .516Avg.
f-measure .300 .450Table 2.
Averages for thirty queries using thehighly relevant reference evaluationscope.
(NOTE:  The f-measure listed under LEAD islower than both the corresponding recall andprecision.
Keep in mind that the figures aboverepresent averages for thirty queries.
The f-measure .450 thus is not based on a recall rate of.593 and a precision rate of .516 but rather it isthe average of thirty individual f-measures.
Thisalso explains f-measures provided in Tables 3and 4.
)When evaluated from the perspective ofcustomers who want to retrieve all references toa topic, restricting the query to the LEAD onaverage resulted in a substantial drop in recall,from an average of .704 to an average of .232, adrop of .472 on average.
The small .082increase in average precision rates barelyprovides any offsetting benefits, as Table 3shows.
Average f-measures across the thirtyqueries dropped .324.
Customers who want toretrieve all references not surprisingly do notbenefit at all from using Searchable LEAD.Full-text LEADRecall .704 .232Precision .777 .859f-measure .657 .333Table 3.
Averages for thirty queries using theall reference evaluation scope.The trends represented by the results in thesetables were generally consistent with the resultsfor individual queries.
When using the highlyrelevant reference evaluation scope, precisionrates and f-measures increased for 26 of thethirty queries when shifting from full-text toLEAD.
When using the all reference evaluationscope, recall rates decreased or stayed steadyand f-measures decreased for all thirty querieswhen shifting from full-text to LEAD.For all queries tested, the number ofdocuments retrieved when using SearchableLEAD not surprisingly was lower than whenusing full-text.
Searchable LEAD-based answersets on average were one fourth the size of full-text-based answer sets, 50.6 documents vs.198.7 documents, respectively.7 DiscussionAs Table 2 and answer set size statistics show,targeted customers benefit when limiting theirqueries to the Searchable LEAD.
As Table 3suggests, non-targeted customers such as thosewho want documents with any references to thetopics clearly should not use Searchable LEAD.Most information retrieval system evaluationsdo not take differing customer perspectives intoaccount.
If we were to combine the results ofour all reference and highly relevant referenceevaluation scopes into one general evaluationpool as is done in Table 4, we would still note asignificant improvement in precision.
However,based on the falling f-measure, some mightconclude that summaries are simply anotherfailed attempt to improve information retrievalwith the help of natural language processing.For the average customer overall, that isprobably a fair conclusion.
However, for thecustomer segments that prefer to retrieve a fewgood highly relevant documents as they start aninformation seeking task, Searchable LEADhelped to produce smaller answer sets that weremore focused on the highly relevant documentsthat those customers target.Full-text LEADAvg.
Recall .745 .413Avg.
Precision .504 .688Avg.
f-measure .479 .392Table 4.
Averages for thirty queries combiningboth the highly relevant reference andall reference evaluation scopes.As for other retrieval tasks, when creating adocument categorization system, we did gainsome benefits when weighting terms found inheadlines and leading text in news documents abit higher (Wasson, 2000), but that effect islimited to news data.
A colleague investigatingan internal tf-idf-based search engine found nobenefits to putting extra emphasis on termsfound in the first paragraph of news articles, butthat was a rather limited test.
Neither of thesewere evaluated from multiple user perspectives,although in the case of Wasson (2000) theoriginal project goal was to identify andcategorize only highly relevant documents8 ConclusionCustomers of online services approach theirinformation seeking tasks from manyperspectives, and yet most IR evaluations areconducted from a single user perspective.
Usinga single user perspective is easier, but it riskshiding the potential benefits of some new featurefrom key customer segments who might value it.News document summaries such as those in theform of Searchable LEAD provide a way to helpsome customer segments retrieve smaller answersets that are focused on highly relevantdocuments.
But the benefits of this are onlyapparent when the results are evaluated fromthat perspective.It is not simply a trade-off between recall andprecision, but one between recall and precisionwith respect to the definition of relevance thatthat different customer segments have.
Ananswer set of ten documents that mention but donot comment on some topic may result in 100%precision for the all reference evaluation scopebut 0% precision for the highly relevantreference evaluation scope.
Customer segmentscan and do have such widely divergent views ofrelevance.This evaluation showed that a customerseeking to retrieve a few highly relevantdocuments about some topic would benefit fromusing Searchable LEAD, retrieving smalleranswer sets and a higher proportion of highlyrelevant documents in that answer set.
Acustomer wanting to retrieve all documents thatrefer to the topic should avoid Searchable LEADand instead continue to use full-text search.ReferencesBrandow, R., Mitze, K. and Rau, L. (1995)Automatic Condensation of ElectronicPublications by Sentence Selection.Information Processing & Management, 31/5.Harmon, D. (1991) How Effective is Suffixing?Journal of the American Society forInformation Science, 42/1.Lam, W., Mukhopadhyay, S., Mostafa, J., andPalakal, M. (1996)  Detection of Shifts inUser Interests for Personalized InformationFiltering.
Proceedings of the 19th AnnualInternational ACM SIGIR Conference onResearch and Development in InformationRetrieval.Sakai, T., and Spark Jones, K. (2001).
GenericSummaries for Indexing in InformationRetrieval.
Proceedings of the 24th AnnualInternational ACM SIGIR Conference onResearch and Development in InformationRetrieval.Sparck Jones, K. (1999)  What is the Role ofNLP in Text Retrieval.
In ?Natural LanguageInformation Retrieval?, T. Strzalkowski, ed.,Kluwer Academic Publishers, Dordrecht, TheNetherlands.Sumita, E., & Iida, H. (1997).
InformationRetrieval Using a Statistical AbstractionModel.
Proceedings of the 3rd AnnualMeeting of the Association for NaturalLanguage Processing.Thompson, P. and Dozier, C. (1997)  NameSearching and Information Retrieval.Proceedings of the 2nd Conference onEmpirical Methods in Natural LanguageProcessing.Voorhees, E. (1994) Query Expansion UsingLexical Semantic Relations.
Proceedings ofthe 17th International ACM-SIGIRConference on Research and Development inInformation Retrieval.Wasson, M. (1998) Using Leading Text forNews Summaries:  Evaluation Results andImplications for Commercial SummarizationApplications.
COLING-ACL ?98 ConferenceProceedings.Wasson, M. (2000)  Large-scale ControlledVocabulary Indexing for Named Entities.Proceedings of the 6th Applied NaturalLanguage Processing Conference.Zhou, J.
(1999)  Phrasal Terms in Real-WorldIR Applications.
In ?Natural LanguageInformation Retrieval?, T. Strzalkowski, ed.,Kluwer Academic Publishers, Dordrecht, TheNetherlands.
