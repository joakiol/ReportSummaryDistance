Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 325?331,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsPARADISE-style Evaluation of a Human-Human Library CorpusRebecca J. PassonneauCenter for ComputationalLearning SystemsColumbia Universitybecky@cs.columbia.eduIrene AlvaradoSchool of Engineering andApplied ScienceColumbia Universityia2178@columbia.eduPhil CroneSimon JeromeColumbia CollegeColumbia Universityptc2107@columbia.edusgj2111@columbia.eduAbstractWe apply a PARADISE-style evaluation to ahuman-human dialogue corpus that was  col-lected to support the design of a spoken dialo-gue system for library transactions.
The bookrequest dialogue task we investigate is infor-mational in nature: a book request is consi-dered successful if the librarian is able toidentify a specific book for the patron.PARADISE assumes that user satisfaction canbe modeled as a regression over task successand dialogue costs.
The PARADISE modelwe derive includes features that characterizetwo types of qualitative features.
The first hasto do with the specificity of the communica-tive goals, given a request for an item.
Thesecond has to do with the number and locationof overlapping turns, which can sometimessignal rapport between the speakers.1 IntroductionThe PARADISE method for evaluating task-basedspoken dialogue systems (SDSs) assumes that usersatisfaction can be modeled as a multivariate linearregression on measures of task success and dialo-gue costs (Walker, et al 1998).
Dialogue costsaddress efficiency, such as length of time on task,and effort, such as number of times the SDS failsto understand an utterance and re-prompts the user.It has been used to compare subjects performingthe same or similar tasks across distinct SDSs(Sanders, et al 2002).
To our knowledge, it has notbeen applied to human-human dialogue.For human-human task-based dialogues, wehypothesized that user satisfaction would not bepredicted well by measures of success and dialo-gue costs alone.
We expected that qualitative cha-racteristics of human-human dialogue, such as themanner in which a dialogue goal is pursued, couldcounterbalance high dialogue costs.
To test thishypothesis, we performed a PARADISE-like eval-uation of a corpus of human-human library trans-action dialogues that was originally collected tosupport the design of our SDS (Passonneau, et al2010).
The communicative task we examine is toidentify a specific set of books of interest from thelibrary?s holdings.
This can be straightforward ifthe patron requests a book by catalogue number.
Itcan be complex if the patron does not have com-plete bibliographic information, or if the request isnon-specific.
A book request is successful whenthe librarian identifies a specific book that ad-dresses the patron?s request.Task success was predictive on a training set,but not on a held-out test set.
Dialogue costs wereless reliably predictive.
Two additional factors wefound to be moderate predictors pertained to thenumber of book requests that were non-specific innature, and the amount and location of overlappingturns.
We refer to these as qualitative features.
Anon-specific book request can lead to a collabora-tive identification of a specific book, and the costsincurred can be worth the effort.
We speculate thatoverlapping turns during non-task-oriented subdia-logue reflects positive rapport between the speak-ers, while the role of overlapping turns during task-oriented subdialogue is contingent on other charac-teristics of the task, such as whether the goal isspecific or non-specific.The three following sections discuss relatedwork, our corpus, and our annotation proceduresand reliability.
We then present how we measure325user satisfaction, informational task success onbook requests, and various dialogue costs.
This isfollowed by results of the application ofPARADISE to the human-human corpus.2 Related WorkIt is commonly assumed that human-computer in-teraction should closely resemble human-humaninteraction.
For example, the originators of socialpresence theory  propose that media that moreclosely resemble face-to-face communication pro-vide a higher degree of social presence, or aware-ness of the communicative partner (Short, et al1976), which in turn leads  to communicative suc-cess.
A similar idea is seen in the origins of mediarichness theory (Daft and Lengel 1984), which de-fines media with more ?richness?
as having morecommunication cues, and thus enhancing task suc-cess.
A key component of this assumption is that,if computers are created with human-like qualitiesthen people will view computers similarly to hu-mans.
We hypothesize that human-machine dialo-gue need not resemble human-human dialogue inall respects, thus we earlier proposed a method toinvestigate human-machine dialogue despite thelarge disparity in the spoken language processingabilities of humans versus machines (Levin andPassonneau 2006), and applied it work described inthis proceedings (Gordon, et al 2011).
Here, weapply PARADISE to human-human dialogue tofacilitate comparison.Turn-taking in conversation has received asignificant amount of attention.
Early work ex-amined the types of turn-taking attempts and thereasons why such attempts either succeed or fail(Beattie 1982).
Recent research has focused on theacoustic, lexical, and discourse-relevant cues thatindicate a transition between speakers (Be?u?2009, Gravano and Hirschberg 2009).
More recent-ly, turn-taking has been examined in the context ofmulti-tasking dialogues (Yang, et al 2011).
TheLoqui human-human dialogues often involve mul-tiple tasks.
We do not annotate who has the floor,but we do transcribe overlapping speech, wherethere may be competition for the turn  (see below).3 Loqui Human-Human CorpusOur baseline SDS, CheckItOut, is modeled onlibrary transactions for the Andrew Heiskell Brailleand Talking Book Library of New York City, andis part of the Library of Congress.
Patrons requestbooks from librarians by telephone, and receivebook orders (primarily in recorded format) by mail.Early in the project, we recorded 175 patron-librarian calls at the Heiskell Library, 82 of whichwe identified to be primarily about book informa-tion and book orders.
These were transcribed withan XML transcription tool, and utterances werealigned with the speech signal.
The total number ofwords is approximately 24,670, or about 300words per dialogue.
Our transcription conventionsare documented on our website.1To facilitate analysis of the interactive structureof many types of interaction, such as spontaneousspoken dialogue, email, and task-oriented dialogue,we previously developed Dialogue Function Unit(DFU) annotation (Hu, et al 2009).
The primarymotivation was to capture information about adja-cency pairs, sequences of communicative acts inwhich an initial utterance calls forth a respondingone (Sacks, et al 1974).
DFUs encode links be-tween the elements of an adjacency pair, and a re-stricted set of dialogue acts designed to generalizeacross genres of interaction.
Trained annotatorsapplied DFU annotations to all 82 dialogues.To measure task success and dialogue costs, wedeveloped an additional annotation process thatbuilds on DFU annotation, as described next.4 TSC AnnotationIn our human-human corpus, each patron has adifferent set of goals.
For most of the dialogues, atleast some of the patron?s goals are to requestbooks from the librarian.
Other goals include re-questing an update to the patron?s profile informa-tion, requesting new equipment for listening torecorded books, and so on.
The three-step methoddeveloped for annotating task success, dialoguecosts and qualitative features (TSC Annotation)consists of an annotation step to determine whattasks are being executed, and two tabulation steps.The 82 dialogues that had already been annotatedfor DFUs were then annotated for task success anddialogue costs.2 Three annotators were trained inthe annotation over the course of several one-hoursessions, each of which was devoted to a different1See resources link at http://www1.ccls.columbia.edu/~Loqui/.2 The guidelines are at http://www1.ccls.columbia.edu/~Loqui/resources.html.326sample dialogue.
Pairs of annotators worked oneach dialogue, with one annotator reviewing theother?s work.
Disagreements were adjudicated, andinterannotator agreement was measured on threedialogues.4.1 AnnotationThe annotation procedure starts by dividing a tran-scription of a dialogue into a covering sequence ofcommunicative tasks (Dialogue Task Units, orDTUs).
Each DTU encompasses a complete ideawith a single goal.
It ends when both speakers havecollaboratively closed the topic, per the notion ofcollaborative contributions to discourse found in(Clark and Schaefer 1989).
Each DTU is labeledwith its type.
The two types of DTUs of most re-levance here are book requests (BRs; where a pa-tron requests a book), and librarian proposals (LPs;where the librarian proposes a book for the patron).Each BR or LP is numbered.
Other DTU typesinclude Inform (e.g., patron requests the librarianto provide a synopsis of a book), and Request-Action (e.g., patron requests the librarian updatethe patron?s profile).
After the DTUs have beenannotated, success and task measures are tabulatedfor the book requests (BR and LP): the start andend lines, the specificity of the request (a requestfor any book by a given author is non-specific),and whether the task was successful.Figure 1 shows part of a book request DTU.The DTU in Figure 1 is unsuccessful; the librarianis unable to identify the book the patron seeks.Several DTUs might pertain to the same goal, pur-sued in different ways.
For example, the DTU il-lustrated here is the second of three in which thepatron tries to request a book called The DogWhisperer.
The dialogue contains 7 DTUs devotedto this request, which is ultimately successful.16.1.0?
L?
wh??wha??do?you?have?the?author????
?
[Request?Info:?author?of?book]?17.1.0?
P?
Cesar?Millan???
?
[Inform:?author?is?Cesar?Millan]?18.1.0?
L?
M?I?L?A?N???
?
[Request?Info:?is?librarian's?spelling?correct]?19.1.0?
P?
yes?20.1.0?
L?
<non?speaking?librarian?activity>?21.1.1?
P?
can?you?hold?on?just?{one?second}??
?
[Request?Action:?can?librarian?hold]?21.1.2?
L?
{sure?sure}??
?
[Confirm]?22.1.0?
P?
I?m?back?23.1.1?
L?
I?m?sorry?I?m?not?seeing?anything?{by?him}??
?
[Inform:?Nothing?by?this?author]?23.1.2?
P?
{really}??
?
[Request?Info:?yes/no]??24.1.0?
L?
no??
?
[Disconfirm]??
?
BOOK?REQUEST?1.1?Figure 1.
Book request DTUFigure 1 also illustrates how we transcribeoverlapping utterances.
Each line in Figure 1 cor-responds to an utterance, or in the case of overlap-ping speech, to a time segment consisting of anutterance with some overlap.
Patron utterance21.1.1 is transcribed as ending with overlappingspeech (in curly braces) where the librarian is alsospeaking within the same time segment (21.1.2).This is followed by the patron?s utterance 22.1.0.The next time segment (23) also has an overlap,followed by the librarian?s turn 24.1.0.
As a result,we can investigate the proportion of utterances in adialogue or subdialogue with overlapping speech,and the types of segments where overlaps occur.4.3 Interannotator AgreementTo assess interannotator agreement among thethree annotators, we randomly selected dialoguesfrom a set that had already been annotated until weidentified three that had been annotated by distinctpairs of annotators.
Each was then annotated by adifferent third annotator who had not been a mem-ber of the original pair.
Interannotator agreementon DTU boundaries and labels was measured usingKrippendorff?s alpha (Krippendorff 1980).
Alpharanges from 0 for no agreement above chance pre-diction, given the rate at which each annotationvalue is used, to 1 or -1, for perfect agreement ordisagreement.The three dialogues had alpha values of 0.87,0.77 and 0.66, thus all well above agreement thatcould have resulted from chance.
The dialoguewith the highest agreement had 1 book requestconsisting of 2 DTUs.
The first DTU had a non-specific request for two books by a given author,that was later reformulated in the second DTU as aspecific request--by author and titles--for the twobooks.
The dialogue with the next highest agree-ment had 12 specific book requests by cataloguenumber, and one DTU per book request.
The di-alogue with the lowest agreement had 5 book re-quests, with one DTU per book request.
Two wereby catalogue number, one was by author, and onewas by author and title.3275.
Perceived User SatisfactionAn indirect measure of User Satisfaction for eachdialogue was provided by two annotators who lis-tened to the audio while reviewing the transcripts.The annotators completed a user satisfaction sur-vey that was nearly identical to one used in anevaluation of CheckItOut, the SDS modeled on thelibrary transactions; references to the system werereplaced with the librarian.
It contained ten ques-tions covering the librarian?s clarity, friendliness,helpfulness, and ability to communicate.
The anno-tators rated the perceived response of the callerwith regard to the survey questions.
On a 1 to 5scale where 5 was the greatest satisfaction, therange was [3.8, 4.7], thus overall, patrons wereperceived to be quite satisfied.6.
Task SuccessThe dialogue task investigated here is information-al in nature, rather than a borrowing task.
That is, abook request is considered successful if the libra-rian is able to identify the specific book the calleris requesting, or if the librarian and patron are ableto specify a book in the library?s holdings that thecaller wants to borrow.
The actual availability ofthe book is not relevant.
Some patrons request aspecific book, and provide alternative means toidentify the book, such as catalogue number versustitle.
Some seek unspecified books by a particularauthor, or books in a given genre.We calculate task success as the ratio of suc-cessfully identified books to requested books.
Thetotal number of books requested ranged from 1 to24.
Patron-initiated book requests as well as libra-rian-initiated proposals are included in the tabula-tion.
In addition, we tabulate the number ofspecific book requests that change in the type ofinformation provided (RC, title, author, genre, etc.
)as well as the number of book requests that changein their specificity (non-specific to specific).
Final-ly, we tabulate how many of these changes lead tosuccessful identifications of books.In general, task success was extremely high.More than 90% of book requests were successful;for 78% of the dialogues, all book requests weresuccessful.
This high success rate is to be expected,given that most callers are requesting specificcbooks they learn about from a library newsletter, ormaking non-specific requests that the librarian cansatisfy.7.
Dialogue Costs and Qualitative FeaturesAlong with two measures of task success (numberof successfully identified books: Successful.ID;percent of requested books that are successfullyidentified: Percent.Successful), we have 48 meas-ures of dialogue costs and qualitative features.
Thefull list appears in column 1 of the table in Appen-dix A.
Dialogue costs consist of measures such asthe total number of turns, the total number of turnsin book requests, the total number of utterances,counts of interruptions and misunderstandings byeither party, and so on.
Qualitative features includeextensive clarifications, the types of book request,and overlapping utterances.An extensive clarification serves to clarifysome misunderstanding by the caller, and generallythese segments take at least ten turns.We classify each book request into one of sev-en types.
These are non-specific by author, non-specific by genre, specific author, specific title,specific author and title, specific set, and specificcatalogue number.
As shown in the Appendix, wealso tabulate the total number of specific book re-quests per dialogue (S.Total) and the total numberof non-specific requests (NS.Total).We tabulate overlapping utterances in a varie-ty of ways.
The average number of overlappingutterances per dialogue is 13.9.
A breakdown ofoverlapping utterances into those that occur inbook requests versus other types of DTU gives amean of 4.36 for book requests compared with8.74 otherwise.
We speculate that the differenceresults from the potential for overlapping utter-ances to impede understanding when the utterancegoals are to request and share information aboutbooks.
In these contexts, overlap may reflect com-petition for the floor.
In contrast, overlapping ut-terances at points in the dialogue that pertain to thesocial dimension may be more indicative of rap-port between the patron and the librarian, as a ref-lection of sharing the floor.
We do not attempt todistinguish overlaps with positive versus negativeeffects.
We do, however, tabulate overlappingspeech in different types of DTUs, such as bookrequest DTUs versus other DTUs.To illustrate the role of the qualitative fea-tures, we discus one of the dialogues in our corpusthat exemplifies a property of these human-humandialogues that we believe could inform SDS de-sign: high user satisfaction can occur despite low328success rate on the communicative tasks.
Dialo-gue 4 had the lowest task success of all dialogues(62.5%), yet perceived user satisfaction was quitehigh (4.7).
This dialogue had a large number ofbook requests and librarian proposals, with a mixof requests for specific books by catalogue num-ber, title, or author and title, along with non-specific requests for works by given authors.
Italso had a fairly high proportion of overlappingspeech.
As we discuss next, both dimensions arerepresented in the quantitative PARADISE modelsfor predicting user satisfaction.8.
PARADISE ResultsPARADISE predicts user satisfaction as a linearcombination of task success and cost variables.Here we apply PARADISE to the Loqui librarycorpus, and add qualitative features to task successand dialogue costs.
Six of the dialogues had nobook requests, thus did not exemplify the task,namely to identify books for the patron in the li-brary?s holdings.
These six were eliminated.We split the data into independent training andtest sets.
From the 76 dialogues with book re-quests, we randomly selected 50 for deriving a re-gression model.
These dialogues had a total of 211book requests (mean=4.22).
We reserved 26 dialo-gues for an independent test of how well the fea-tures from the user satisfaction model on thetraining set predicted user satisfaction on the testset.
The test set had 73 book requests (mean=2.81).To explore the data, we first did Analysis ofVariance (ANOVA) tests on the 50 individual fea-tures as predictors of perceived user satisfaction onthe training set.
Certain features that are typicallypredictive for SDSs were also predictive here.Those that were most predictive on their own in-cluded the proportion of book requests successfullyidentified (Pct.Successful), and several cost meas-ures such as total length in utterances, and the totalnumber of interruptions and misunderstandings.However, other features that were predictive herethat are not typical of human-machine dialoguewere the number of utterances with overlappingspeech (Simultaneous.Utterances), and the numberof book requests that evolved from non-specific  tospecific (Change.NS.to.S).Given the relatively small size of our corpus,and the large number of variables, we pruned the30 features from the trained model before usingthem to build a regression on the test set.
All ana-lyses were done in the R Statistical Package(http://www.r-project.org/).
We used the R func-tion step to apply the Akaike Information Crite-rion to guide the search through the model space.The resulting model relies on 30 of the 50 va-riables, and has a multiple R-squared of 0.9063 (p=0.0001342).
Appendix A indicates the 30 featuresselected, and their p-values.
For the pruned model,we selected half of the 30 features that contributedmost to the best model found through the stepfunction on the training set.
The pruned model hada multiple R-squared of 0.5334 (p=0.0075).
Whenwe used the same features on the test set, the R-squared was 0.7866  (p=0.0416).
However, thesignificance of individual features differed in train-ing versus test.
Appendix A lists the 15 featuresand their p-values on the training and test sets.On the training data, the most significant fea-tures were Pct.Successful, the total number of di-alogue segments pertaining to book requests(including librarian proposals; BR.request.segs),and the total number of book requests (Total.BR).The number of non specific book requests thatevolved into specific requests (Change.NS.to.S)and the number of utterances per turn (Utter-ances.Turns) were marginally significant.On the test data, the most significant variableswere the ratio of overlapping utterances in seg-ments that were not about book requests to bookrequest segments (noBRLP.Overlap.per.TotalRe-questSegments), the total number of non-specificbook requests (NS.Total), and the number of over-lapping utterances (Overlap.Utterances).9.
ConclusionThe human-human corpus examined here is an ap-propriate corpus to compare with human-machinedialogue, in that our SDS was modeled on the bookrequests in the human-human corpus.
The R2 val-ues indicate that the regression models based onthe 15 features fit the data well, yet the coefficientsand probabilities are very different.
In part, this isdue to the large number of variables we investi-gated, relative to the small size of the corpus.Nevertheless, the results presented here point to anumber of dimensions of human-human dialoguethat contribute to user satisfaction beyond thosethat are typically considered when evaluating hu-man-machine dialogue.329ReferencesBeattie, G. W. 1982.
Turn-taking and interruption inpolitical interviews: Margaret Thatcher and JimCallaghan compared and contrasted.
Semiotica, 39 (1-2): 93-114.Be?u?, ?.
2009.
Are we 'in sync': Turn-taking incollaborative dialogues.
In 10th Interspeech, pp.
2167-2170.Clark, H. H. and E. F. Schaefer 1989.
Contributing todiscourse.
Cognitive Science, 13 259-294.Daft, R. L. and R. H. Lengel 1984.
Informationrichness: A new approach to manager behavior andorganization design.
Research in OrganizationalBehavior, 6 191-233.Gordon, J., et al 2011.
Learning to balance groundingrationales for dialogue systems.
In 12th Annual SIGdialMeeting on Discourse and Dialogue (SIGdial 12).Gravano, A. and J. Hirschberg.
2009.
Turn-yieldingcues in task-oriented dialogue.
In 10th Annual Meetingof SIGDIAL, pp.
253-261.Hu, J., et al 2009.
Contrasting the interaction structureof an email and a telephone corpus: A machine learningapproach to annotation of dialogue function units.
In10th SIGDIAL on Dialogue and Discourse, pp.
357-366.Krippendorff, K. 1980.
Content Analysis: AnIntroduction to Its Methodology.
Beverly Hills, CA:Sage Publications.Levin, E. and R. J. Passonneau.
2006.
A WOz Variantwith Contrastive Conditions.
In Interspeech SateliteWorkshop, Dialogue on Dialogues: MultidisciplinaryEvaluation of Speech-based Interactive Systems.Passonneau, R. J., et al 2010.
Learning About VoiceSearch for Spoken Dialogue Systems.
In 11th AnnualConference of the North American Chapter of theAssociation for Computational Linguistics (NAACLHLT 2010), pp.
840-848.Sacks, H., et al 1974.
A simplest systematics for theorganization of turn-taking for conversation.
Language,50 (4): 696-735.Sanders, G. A., et al 2002.
Effects of word error rate inthe DARPA Communicator data during 2000 and 2001.International Journal of speech Technology, 7 293-309.Short, J., et al 1976.
The social psychology oftelecommunications.
Chichester: John Wiley.Walker, M. A., et al 1998.
Evaluating Spoken DialogueAgents with PARADISE: Two Case Studies.
ComputerSpeech and Language, 12 317-348.Yang, F., et al 2011.
An Investigation of interruptionsand resumptions in multi-tasking dialogues.Computational Linguistics, 37 (1): 75-104.330Appendix A: FeaturesVariable TrainingCoeff.Trainingp-valuePrunedCoeffPrunedp-valueTestCoeff.Testp-value1 Successful.ID2 Pct.Successful 0.504001 0.005118 0.356516 0.01219 -0.04154 0.867443 Change.NS.to.S 1.440471 0.023525 0.287376 0.05761 0.10284 0.228764 Successful.NS.to.S -1.450301 0.0486565 Change.S.to.S6 Successful.S.to.S7 BR.request.segs -0.201228 0.119857 -0.147057 0.00837 0.02566 0.792778 LP.request.segs 0.146464 0.0731389 Total.Request.Segments10 Total.BR 0.448858 0.001813 0.147945 0.01220 -0.09960 0.3579611 Segments.per.BR 0.296577 0.047333 0.123411 0.17907 -0.08707 0.5990312 NS.Author -0.216559 0.09083013 NS.Genre -0.138867 0.24933914 S.Title15 S.AuthorTitle16 S.Set -0.953284 6.61e-0517 S.RC -0.158897 0.10475218 S.Author19 S.Total20 NS.Total   0.013265 0.75986 -0.27280 0.0071621 Turns.in.BR22 Utterances    -0.005613 0.01396723 Interruptions 0.187876 0.002704 -0.050500 0.29683 -0.29078 0.0537824 Misunderstandings25 Simultaneous.Utterances -0.151491 0.001967 -0.008705 0.21024 0.02329 0.0417926 Extensive.Clarifications -0.181057 1.76e-05 -0.022723 0.25767 -0.08685 0.1160827 S.U.Conventional 0.142152 0.00616828 S.U.Inform 0.141891 0.00161929 S.U.Sidebar  0.107238 0.04730330 S.U.BR.RC 0.142538 0.00646731 S.U.BR.Title 0.245880 0.00041532 S.U.BR.Title.and.Author 0.136412 0.00258133 S.U.BR.Genre34 S.U.LP 0.176515 0.01559835 S.U.R.A.
0.171413 0.00145936 S.U.IR.IRA 0.166315 0.00199437 Utterances.Turns -0.392267 0.020190 -0.256307 0.08077 0.01731 0.9567438 Total.Turns.BR39 Turns.in.BR.BR -0.015623 0.09357340 BR.Utterances -8.875951 0.000603 -1.104338 0.55174 2.59438 0.3343941 NS.Total.per.BR 0.183761 0.177739 -0.102524 0.33547 0.31111 0.1000442 S.U.BRLP43 S.U.BRLP.per.BR44 S.U.BRLP.per.TotalRequestSegs45 S.U.nonBRLP46 S.U.nonBRLP.per.BR47 S.U.nonBRLP.per.TotalRequestSegs 0.024492 0.117363 0.007839 0.33727 -0.06000 0.0084848 S.nonRC49 S.nonRC.per.BR   -0.370227 0.064299 -0.062149 0.46085 -0.08072 0.4770450 S.nonRC.per.TotalRequestSegs331
