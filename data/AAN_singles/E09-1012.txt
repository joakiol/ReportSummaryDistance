Proceedings of the 12th Conference of the European Chapter of the ACL, pages 94?102,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsIncremental Parsing Models for Dialog Task StructureSrinivas Bangalore and Amanda J. StentAT&T Labs ?
Research, Inc., 180 Park Avenue,Florham Park, NJ 07932, USA{srini,stent}@research.att.comAbstractIn this paper, we present an integratedmodel of the two central tasks of dialogmanagement: interpreting user actions andgenerating system actions.
We model theinterpretation task as a classi?cation prob-lem and the generation task as a predic-tion problem.
These two tasks are inter-leaved in an incremental parsing-based di-alog model.
We compare three alterna-tive parsing methods for this dialog modelusing a corpus of human-human spokendialog from a catalog ordering domainthat has been annotated for dialog actsand task/subtask information.
We contrastthe amount of context provided by eachmethod and its impact on performance.1 IntroductionCorpora of spoken dialog are now widely avail-able, and frequently come with annotations fortasks/games, dialog acts, named entities and ele-ments of syntactic structure.
These types of infor-mation provide rich clues for building dialog mod-els (Grosz and Sidner, 1986).
Dialog models canbe built of?ine (for dialog mining and summariza-tion), or online (for dialog management).A dialog manager is the component of a dia-log system that is responsible for interpreting useractions in the dialog context, and for generatingsystem actions.
Needless to say, a dialog manageroperates incrementally as the dialog progresses.
Intypical commercial dialog systems, the interpre-tation and generation processes operate indepen-dently of each other, with only a small amount ofshared context.
By contrast, in this paper we de-scribe a dialog model that (1) tightly integrates in-terpretation and generation, (2) makes explicit thetype and amount of shared context, (3) includesthe task structure of the dialog in the context, (4)can be trained from dialog data, and (5) runs in-crementally, parsing the dialog as it occurs and in-terleaving generation and interpretation.At the core of our model is a parser that in-crementally builds the dialog task structure as thedialog progresses.
In this paper, we experimentwith three different incremental tree-based parsingmethods.
We compare these methods using a cor-pus of human-human spoken dialogs in a catalogordering domain that has been annotated for dialogacts and task/subtask information.
We show thatall these methods outperform a baseline methodfor recovering the dialog structure.The rest of this paper is structured as follows:In Section 2, we review related work.
In Sec-tion 3, we present our view of the structure of task-oriented human-human dialogs.
In Section 4, wepresent the parsing approaches included in our ex-periments.
In Section 5, we describe our data andexperiments.
Finally, in Section 6, we present con-clusions and describe our current and future work.2 Related WorkThere are two threads of research that are relevantto our work: work on parsing (written and spoken)discourse, and work on plan-based dialog models.Discourse Parsing Discourse parsing is the pro-cess of building a hierarchical model of a dis-course from its basic elements (sentences orclauses), as one would build a parse of a sen-tence from its words.
There has now been con-siderable work on discourse parsing using statisti-cal bottom-up parsing (Soricut and Marcu, 2003),hierarchical agglomerative clustering (Sporlederand Lascarides, 2004), parsing from lexicalizedtree-adjoining grammars (Cristea, 2000), and rule-based approaches that use rhetorical relations anddiscourse cues (Forbes et al, 2003; Polanyi et al,2004; LeThanh et al, 2004).
With the exception ofCristea (2000), most of this research has been lim-ited to non-incremental parsing of textual mono-logues where, in contrast to incremental dialogparsing, predicting a system action is not relevant.The work on discourse parsing that is mostsimilar to ours is that of Baldridge and Las-carides (2005).
They used a probabilistic head-driven parsing method (described in (Collins,2003)) to construct rhetorical structure trees for aspoken dialog corpus.
However, their parser was94DialogTaskTopic/SubtaskTopic/SubtaskTask TaskClauseUtteranceUtteranceUtteranceTopic/SubtaskDialogAct,Pred?Args DialogAct,Pred?Args DialogAct,Pred?ArgsFigure 1: A schema of a shared plan tree for adialog.not incremental; it used global features such as thenumber of turn changes.
Also, it focused strictlyin interpretation of input utterances; it could notpredict actions by either dialog partner.In contrast to other work on discourse parsing,we wish to use the parsing process directly for di-alog management (rather than for information ex-traction or summarization).
This in?uences ourapproach to dialog modeling in two ways.
First,the subtask tree we build represents the functionaltask structure of the dialog (rather than the rhetor-ical structure of the dialog).
Second, our dialogparser must be entirely incremental.Plan-Based Dialog Models Plan-based ap-proaches to dialog modeling, like ours, operate di-rectly on the dialog?s task structure.
The processof task-oriented dialog is treated as a special caseof AI-style plan recognition (Sidner, 1985; Litmanand Allen, 1987; Rich and Sidner, 1997; Carberry,2001; Bohus and Rudnicky, 2003; Lochbaum,1998).
Plan-based dialog models are used for bothinterpretation of user utterances and prediction ofagent actions.
In addition to the hand-crafted mod-els listed above, researchers have built stochasticplan recognition models for interaction, includ-ing ones based on Hidden Markov Models (Bui,2003; Blaylock and Allen, 2006) and on proba-bilistic context-free grammars (Alexandersson andReithinger, 1997; Pynadath and Wellman, 2000).In this area, the work most closely related toours is that of Barrett and Weld (Barrett and Weld,1994), who build an incremental bottom-up parserOpeningOrder PlacementContact InfoDelivery InfoShipping InfoClosingSummaryPayment InfoOrder ItemFigure 2: Sample output (subtask tree) from aparse-based model for the catalog ordering do-main.to parse plans.
Their parser, however, was notprobabilistic or targeted at dialog processing.3 Dialog StructureWe consider a task-oriented dialog to be the re-sult of incremental creation of a shared plan bythe participants (Lochbaum, 1998).
The sharedplan is represented as a single tree T that incorpo-rates the task/subtask structure, dialog acts, syn-tactic structure and lexical content of the dialog,as shown in Figure 1.
A task is a sequence of sub-tasks ST ?
S. A subtask is a sequence of dialogacts DA ?
D. Each dialog act corresponds to oneclause spoken by one speaker, customer (cu) oragent (ca) (for which we may have acoustic, lexi-cal, syntactic and semantic representations).Figure 2 shows the subtask tree for a sample di-alog in our domain (catalog ordering).
An orderplacement task is typically composed of the se-quence of subtasks opening, contact-information,order-item, related-offers, summary.
Subtasks canbe nested; the nesting can be as deep as ?ve lev-els in our data.
Most often the nesting is at theleftmost or rightmost frontier of the subtask tree.As the dialog proceeds, an utterance from a par-ticipant is accommodated into the subtask tree inan incremental manner, much like an incremen-tal syntactic parser accommodates the next wordinto a partial parse tree (Alexandersson and Rei-thinger, 1997).
An illustration of the incrementalevolution of dialog structure is shown in Figure 4.However, while a syntactic parser processes in-put from a single source, our dialog parser parsesuser-system exchanges: user utterances are inter-preted, while system utterances are generated.
Sothe steps taken by our dialog parser to incorpo-rate an utterance into the subtask tree depend onwhether the utterance was produced by the agentor the user (as shown in Figure 3).User utterances Each user turn is split intoclauses (utterances).
Each clause is supertagged95Interpretation of a user?s utterance:DAC : daui = argmaxdu?DP (du|cui , STi?1i?k , DAi?1i?k, ci?1i?k)(1)STC : stui = argmaxsu?SP (su|daui , cui , STi?1i?k , DAi?1i?k, ci?1i?k)(2)Generation of an agent?s utterance:STP : stai = argmaxsa?SP (sa|ST i?1i?k , DAi?1i?k, ci?1i?k)(3)DAP : daai = argmaxda?DP (da|stai , STi?1i?k , DAi?1i?k, ci?1i?k)(4)Table 1: Equations used for modeling dialog act and sub-task labeling of agent and user utterances.
cui /cai = thewords, syntactic information and named entities associatedwith the ith utterance of the dialog, spoken by user/agentu/a.
daui /daai = the dialog act of the ith utterance, spokenby user/agent u/a.
stui /stai = the subtask label of the ith ut-terance, spoken by user/agent u/a.
DAi?1i?k represents thedialog act tags for utterances i?
1 to i?
k.and labeled with named entities1.
Interpretation ofthe clause (cui ) involves assigning a dialog act la-bel (daui ) and a subtask label (stui ).
We use STi?1i?k ,DAi?1i?k, and ci?1i?k to represent the sequence of pre-ceeding k subtask labels, dialog act labels andclauses respectively.
The dialog act label daui isdetermined from information about the clause and(a kth order approximation of) the subtask tree sofar (Ti?1 = (ST i?1i?k , DAi?1i?k, ci?1i?k)), as shown inEquation 1 (Table 1).
The subtask label stui is de-termined from information about the clause, its di-alog act and the subtask tree so far, as shown inEquation 2.
Then, the clause is incorporated intothe subtask tree.Agent utterances In contrast, a dialog sys-tem starts planning an agent utterance by iden-tifying the subtask to contribute to next, stai ,based on the subtask tree so far (Ti?1 =(ST i?1i?k , DAi?1i?k, ci?1i?k)), as shown in Equation 3(Table 1) .
Then, it chooses the dialog act of theutterance, daai , based on the subtask tree so far andthe chosen subtask for the utterance, as shown inEquation 4.
Finally, it generates an utterance, cai ,to realize its communicative intent (representedas a subtask and dialog act pair, with associatednamed entities)2.Note that the current clause cui is used in the1This results in a syntactic parse of the clause and couldbe done incrementally as well.2We do not address utterance realization in this paper.Figure 3: Dialog management processconditioning context of the interpretation model(for user utterances), but the corresponding clausefor the agent utterance cai is to be predicted andhence is not part of conditioning context in thegeneration model.4 Dialog ParsingA dialog parser can produce a ?shallow?
or ?deep?tree structure.
A shallow parse is one in whichutterances are grouped together into subtasks, butthe dominance relations among subtasks are nottracked.
We call this model a chunk-based dia-log model (Bangalore et al, 2006).
The chunk-based model has limitations.
For example, dom-inance relations among subtasks are importantfor dialog processes such as anaphora resolu-tion (Grosz and Sidner, 1986).
Also, the chunk-based model is representationally inadequate forcenter-embedded nestings of subtasks, which dooccur in our domain, although less frequently thanthe more prevalent ?tail-recursive?
structures.We use the term parse-based dialog model torefer to deep parsing models for dialog whichnot only segment the dialog into chunks but alsopredict dominance relations among chunks.
Forthis paper, we experimented with three alternativemethods for building parse-based models: shift-reduce, start-complete and connection path.Each of these operates on the subtask tree forthe dialog incrementally, from left-to-right, withaccess only to the preceding dialog context, asshown in Figure 4.
They differ in the parsing ac-tions and the data structures used by the parser;this has implications for robustness to errors.
Theinstructions to reconstruct the parse are either en-tirely encoded in the stack (in the shift-reducemethod), or entirely in the parsing actions (in thestart-complete and connection path methods).
Foreach of the four types of parsing action requiredto build the parse tree (see Table 1), we construct96......Order Item TaskOpeningHello Request(MakeOrder) Acknumber with area codesecondpleaseAckContact?Infocan i have yourhome telephone thank youpleaseAckContact?Infothankyouto place an order secondfor callingXYZ catalogthis is maryhow may Ihelp youyes one yes one thank youfor callingXYZ catalogthis is maryhow may Ihelp youAckOrder Item TaskOpeningHello Request(MakeOrder)yes i would likeOpeningHello Request(MakeOrder) Ackthank youfor callingOrder Item Taskyes pleaseShipping?Addresscan i have yourhome telephonenumber with area code......XYZ catalogContact?Infoto place an orderyes i would likeyouthank.........Ackthis is maryhow may Ihelp youyes onesecondplease ......Request(MakeOrder) Ackthank youfor callingXYZ catalogthis is maryHelloyouthankto place an orderyes i would likeOrder Item TaskOpeninghow may IyouthankClosing.........may we deliver thisorder to your homeyes i would likehelp youyes onesecondpleaseAckto place an orderyes i would likeyouthankto place an orderhelp youyes onesecondpleaseAckContact?Infomay we deliver thisorder to your home......yes pleasehow may IShipping?AddressRequest(MakeOrder) Ackthank youfor callingXYZ catalogthis is maryHellocan i have yourhome telephonenumber with area code......Order Item TaskOpeningShipping?AddressRequest(MakeOrder) Ackthank youfor callingXYZ catalogthis is maryHellocan i have yourhome telephonenumber with area code......Order Item TaskOpeninghow may Iyouthankyes i would likehelp youyes onesecondpleaseAckContact?Infoto place an orderFigure 4: An illustration of incremental evolution of dialog structurea feature vector containing contextual informationfor the parsing action (see Section 5.1).
These fea-ture vectors and the associated parser actions areused to train maximum entropy models (Berger etal., 1996).
These models are then used to incre-mentally incorporate the utterances for a new di-alog into that dialog?s subtask tree as the dialogprogresses, as shown in Figure 3.4.1 Shift-Reduce MethodIn this method, the subtask tree is recoveredthrough a right-branching shift-reduce parsingprocess (Hall et al, 2006; Sagae and Lavie, 2006).The parser shifts each utterance on to the stack.
Itthen inspects the stack and decides whether to doone or more reduce actions that result in the cre-ation of subtrees in the subtask tree.
The parsermaintains two data structures ?
a stack and a tree.The actions of the parser change the contents ofthe stack and create nodes in the dialog tree struc-ture.
The actions for the parser include unary-reduce-X, binary-reduce-X and shift, where X iseach of the non-terminals (subtask labels) in thetree.
Shift pushes a token representing the utter-ance onto the stack; binary-reduce-X pops two to-kens off the stack and pushes the non-terminal X;and unary-reduce-X pops one token off the stackand pushes the non-terminal X.
Each type of re-duce action creates a constituent X in the dialogtree and the tree(s) associated with the reduced el-ements as subtree(s) of X.
At the end of the dialog,the output is a binary branching subtask tree.Consider the example subdialog A: would youlike a free magazine?
U: no.
The process-ing of this dialog using our shift-reduce dialogparser would proceed as follows: the STP modelpredicts shift for sta; the DAP model predictsYNP(Promotions) for daa; the generator outputswould you like a free magazine?
; and the parsershifts a token representing this utterance onto thestack.
Then, the customer says no.
The DACmodel classi?es dau as No; the STC model clas-si?es stu as shift and binary-reduce-special-offer;and the parser shifts a token representing the ut-terance onto the stack, before popping the top twoelements off the stack and adding the subtree forspecial-order into the dialog?s subtask tree.4.2 Start-Complete MethodIn the shift-reduce method, the dialog tree is con-structed as a side effect of the actions performedon the stack: each reduce action on the stack in-troduces a non-terminal in the tree.
By contrast,in the start-complete method the instructions tobuild the tree are directly encoded in the parser ac-tions.
A stack is used to maintain the global parsestate.
The actions the parser can take are similarto those described in (Ratnaparkhi, 1997).
Theparser must decide whether to join each new termi-nal onto the existing left-hand edge of the tree, orstart a new subtree.
The actions for the parser in-clude start-X, n-start-X, complete-X, u-complete-X and b-complete-X, where X is each of the non-terminals (subtask labels) in the tree.
Start-Xpushes a token representing the current utteranceonto the stack; n-start-X pushes non-terminal Xonto the stack; complete-X pushes a token repre-senting the current utterance onto the stack, then97pops the top two tokens off the stack and pushesthe non-terminal X; u-complete-X pops the top to-ken off the stack and pushes the non-terminal X;and b-complete-X pops the top two tokens off thestack and pushes the non-terminal X.
This methodproduces a dialog subtask tree directly, rather thanproducing an equivalent binary-branching tree.Consider the same subdialog as before, A:would you like a free magazine?
U: no.
Theprocessing of this dialog using our start-completedialog parser would proceed as follows: the STPmodel predicts start-special-offer for sta; the DAPmodel predicts YNP(Promotions) for daa; the gen-erator outputs would you like a free magazine?
;and the parser shifts a token representing this ut-terance onto the stack.
Then, the customer saysno.
The DAC model classi?es dau as No; the STCmodel classi?es stu as complete-special-offer; andthe parser shifts a token representing the utter-ance onto the stack, before popping the top twoelements off the stack and adding the subtree forspecial-order into the dialog?s subtask tree.4.3 Connection Path MethodIn contrast to the shift-reduce and the start-complete methods described above, the connec-tion path method does not use a stack to track theglobal state of the parse.
Instead, the parser di-rectly predicts the connection path (path from theroot to the terminal) for each utterance.
The col-lection of connection paths for all the utterances ina dialog de?nes the parse tree.
This encoding waspreviously used for incremental sentence parsingby (Costa et al, 2001).
With this method, thereare many more choices of decision for the parser(195 decisions for our data) compared to the shift-reduce (32) and start-complete (82) methods.Consider the same subdialog as before, A:would you like a free magazine?
U: no.
The pro-cessing of this dialog using our connection pathdialog parser would proceed as follows.
First, theSTP model predicts S-special-offer for sta; theDAP model predicts YNP(Promotions) for daa;the generator outputs would you like a free mag-azine?
; and the parser adds a subtree rooted atspecial-offer, with one terminal for the current ut-terance, into the top of the subtask tree.
Then,the customer says no.
The DAC model classi-?es dau as No and the STC model classi?es stuas S-special-offer.
Since the right frontier of thesubtask tree has a subtree matching this path, theType Task/subtask labelsCall-level call-forward, closing, misc-other, open-ing, out-of-domain, sub-callTask-level check-availability, contact-info,delivery-info, discount, order-change,order-item, order-problem, payment-info, related-offer, shipping-address,special-offer, summaryTable 2: Task/subtask labels in CHILDType SubtypeAsk InfoExplain Catalog, CC Related, Discount, Order InfoOrder Problem, Payment Rel, Product InfoPromotions, Related Offer, ShippingConvers- Ack, Goodbye, Hello, Help, Hold,-ational YoureWelcome, Thanks, Yes, No, Ack,Repeat, Not(Information)Request Code, Order Problem, Address, Catalog,CC Related, Change Order, Conf, Credit,Customer Info, Info, Make Order, Name,Order Info, Order Status, Payment Rel,Phone Number, Product Info, Promotions,Shipping, Store InfoYNQ Address, Email, Info, Order Info,Order Status,Promotions, Related OfferTable 3: Dialog act labels in CHILDparser simply incorporates the current utterance asa terminal of the special-offer subtree.5 Data and ExperimentsTo evaluate our parse-based dialog model, we used817 two-party dialogs from the CHILD corpus oftelephone-based dialogs in a catalog-purchasingdomain.
Each dialog was transcribed by hand;all numbers (telephone, credit card, etc.)
wereremoved for privacy reasons.
The average di-alog in this data set had 60 turns.
The di-alogs were automatically segmented into utter-ances and automatically annotated with part-of-speech tag and supertag information and namedentities.
They were annotated by hand for dia-log acts and tasks/subtasks.
The dialog act andtask/subtask labels are given in Tables 2 and 3.5.1 FeaturesIn our experiments we used the following featuresfor each utterance: (a) the speaker ID; (b) uni-grams, bigrams and trigrams of the words; (c) un-igrams, bigrams and trigrams of the part of speechtags; (d) unigrams, bigrams and trigrams of the su-pertags; (e) binary features indicating the presenceor absence of particular types of named entity; (f)the dialog act (determined by the parser); (g) thetask/subtask label (determined by the parser); and(h) the parser stack at the current utterance (deter-98mined by the parser).
Each input feature vector foragent subtask prediction has these features for upto three utterances of left-hand context (see Equa-tion 3).
Each input feature vector for dialog actprediction has the same features as for agent sub-task prediction, plus the actual or predicted sub-task label (see Equation 4).
Each input featurevector for dialog act interpretation has features a-h for up to three utterances of left-hand context,plus the current utterance (see Equation 1).
Eachinput feature vector for user subtask classi?cationhas the same features as for user dialog act inter-pretation, plus the actual or classi?ed dialog act(see Equation 2).The label for each input feature vector is theparsing action (for subtask classi?cation and pre-diction) or the dialog act label (for dialog act clas-si?cation and prediction).
If more than one pars-ing action takes place on a particular utterance(e.g.
a shift and then a reduce), the feature vec-tor is repeated twice with different stack contents.5.2 Training MethodWe randomly selected roughly 90% of the dialogsfor training, and used the remainder for testing.We separately trained models for: user dia-log act classi?cation (DAC, Equation 1); usertask/subtask classi?cation (STC, Equation 2);agent task/subtask prediction (STP, Equation 3);and agent dialog act prediction (DAP, Equation 4).In order to estimate the conditional distributionsshown in Table 1, we use the general technique ofchoosing the MaxEnt distribution that properly es-timates the average of each feature over the train-ing data (Berger et al, 1996).
We use the machinelearning toolkit LLAMA (Haffner, 2006), whichencodes multiclass classi?cation problems usingbinary MaxEnt classi?ers to increase the speed oftraining and to scale the method to large data sets.5.3 Decoding MethodThe decoding process for the three parsing meth-ods is illustrated in Figure 3 and has four stages:STP, DAP, DAC, and STC.
As already explained,each of these steps in the decoding process is mod-eled as either a prediction task or a classi?ca-tion task.
The decoder constructs an input featurevector depending on the amount of context beingused.
This feature vector is used to query the ap-propriate classi?er model to obtain a vector of la-bels with weights.
The parser action labels (STPand STC) are used to extend the subtask tree.
Forexample, in the shift-reduce method, shift resultsin a push action on the stack, while reduce-X re-sults in popping the top two elements off the stackand pushing X on to the stack.
The dialog act la-bels (DAP and DAC) are used to label the leavesof the subtask tree (the utterances).The decoder can use n-best results from theclassi?er to enlarge the search space.
In orderto manage the search space effectively, the de-coder uses a beam pruning strategy.
The decod-ing process proceeds until the end of the dialog isreached.
In this paper, we assume that the end ofthe dialog is given to the decoder3.Given that the classi?ers are error-prone in theirassignment of labels, the parsing step of the de-coder needs to be robust to these errors.
We ex-ploit the state of the stack in the different meth-ods to rule out incompatible parser actions (e.g.
areduce-X action when the stack has one element,a shift action on an already shifted utterance).
Wealso use n-best results to alleviate the impact ofclassi?cation errors.
Finally, at the end of the di-alog, if there are unattached constituents on thestack, the decoder attaches them as sibling con-stituents to produce a rooted tree structure.
Theseconstraints contribute to robustness, but cannot beused with the connection path method, since anyconnection path (parsing action) suggested by theclassi?er can be incorporated into the incrementalparse tree.
Consequently, in the connection pathmethod there are fewer opportunities to correct theerrors made by the classi?ers.5.4 Evaluation MetricsWe evaluate dialog act classi?cation and predic-tion by comparing the automatically assigned di-alog act tags to the reference dialog act tags.For these tasks we report accuracy.
We evaluatesubtask classi?cation and prediction by compar-ing the subtask trees output by the different pars-ing methods to the reference subtask tree.
Weuse the labeled crossing bracket metric (typicallyused in the syntactic parsing literature (Harrison etal., 1991)), which computes recall, precision andcrossing brackets for the constituents (subtrees) ina hypothesized parse tree given the reference parsetree.
We report F-measure, which is a combinationof recall and precision.For each task, performance is reported for 1, 3,3This is an unrealistic assumption if the decoder is toserve as a dialog model.
We expect to address this limitationin future work.995, and 10-best dynamic decoding as well as oracle(Or) and for 0, 1 and 3 utterances of context.5.5 Results0 113 0 133 0 153 0 1103 0 1Or3020406080100Number utterances historyNbestFstart?completeconnection?pathsshift?reduceFigure 5: Performance of parse-based methods forsubtask tree buildingFigure 5 shows the performance of the differentmethods for determining the subtask tree of the di-alog.
Wider beam widths do not lead to improvedperformance for any method.
One utterance ofcontext is best for shift-reduce and start-join; threeis best for the connection path method.
The shift-reduce method performs the best.
With 1 utter-ance of context, its 1-best f-score is 47.86, as com-pared with 34.91 for start-complete, 25.13 for theconnection path method, and 21.32 for the chunk-based baseline.
These performance differences arestatistically signi?cant at p < .001.
However, thebest performance for the shift-reduce method isstill signi?cantly worse than oracle.All of the methods are subject to some ?stick-iness?, a certain preference to stay within thecurrent subtask rather than starting a new one.Also, all of the methods tended to perform poorlyon parsing subtasks that occur rarely (e.g.
call-forward, order-change) or that occur at many dif-ferent locations in the dialog (e.g.
out-of-domain,order-problem, check-availability).
For example,the shift-reduce method did not make many shifterrors but did frequently b-reduce on an incor-rect non-terminal (indicating trouble identifyingsubtask boundaries).
Some non-terminals mostlikely to be labeled incorrectly by this method(for both agent and user) are: call-forward, order-change, summary, order-problem, opening andout-of-domain.Similarly, the start-complete method frequentlymislabeled a non-terminal in a complete action,e.g.
misc-other, check-availability, summary orcontact-info.
It also quite frequently mislabelednonterminals in n-start actions, e.g.
order-item,contact-info or summary.
Both of these errors in-dicate trouble identifying subtask boundaries.It is harder to analyze the output from the con-nection path method.
This method is more likelyto mislabel tree-internal nodes than those imme-diately above the leaves.
However, the samenon-terminals show up as error-prone for thismethod as for the others: out-of-domain, check-availability, order-problem and summary.0 113 0 133 0 153 0 1103 0 1Or30.00.20.40.60.81.0Number utterances historyNbestAccuracystart?completeconnection?pathsshift?reduceFigure 6: Performance of dialog act assignment touser?s utterances.Figure 6 shows accuracy for classi?cation ofuser dialog acts.
Wider beam widths do notlead to sign?cantly improved performance for anymethod.
Zero utterances of context gives the high-est accuracy for all methods.
All methods per-form fairly well, but no method signi?cantly out-performs any other: with 0 utterances of context,1-best accuracy is .681 for the connection pathmethod, .698 for the start-complete method and.698 for the shift-reduce method.
We note thatthese results are competitive with those reportedin the literature (e.g.
(Poesio and Mikheev, 1998;Sera?n and Eugenio, 2004)), although the dialogcorpus and the label sets are different.The most common errors in dialog act classi?-cation occur with dialog acts that occur 40 timesor fewer in the testing data (out of 3610 testingutterances), and with Not(Information).Figure 7 shows accuracy for prediction of agentdialog acts.
Performance for this task is lower than100Speaker Utterance Shift-Reduce Start-Complete Connection PathA This is Sally shift, Hello start-opening, Hello opening S, HelloA How may I help you shift, binary-reduce-out-of-domain, Hellocomplete-opening,Helloopening S, HelloB Yes Not(Information), shift,binary-reduce-out-of-domainNot(Information),complete-openingNot(Information), open-ing SB Um I would like to placean order pleaseRquest(Make-Order), shift,binary-reduce-openingRquest(Make-Order),complete-opening,n-start-SRquest(Make-Order),opening SA May I have your tele-phone number with thearea codeshift, Acknowledge start-contact-info, Ac-knowledgecontact-info S,Request(Phone-Number)B Uh the phone number is[number]Explain(Phone-Number),shift, binary-reduce-contact-infoExplain(Phone-Number), complete-contact-infoExplain(Phone-Number),contact-info STable 4: Dialog extract with subtask tree building actions for three parsing methods0 113 0 133 0 153 0 1103 0 1Or30.00.20.40.60.81.0Number utterances historyNbestAccuracystart?completeconnection?pathsshift?reduceFigure 7: Performance of dialog act predictionused to generate agent utterances.that for dialog act classi?cation because this is aprediction task.
Wider beam widths do not gener-ally lead to improved performance for any method.Three utterances of context generally gives thebest performance.
The shift-reduce method per-forms signi?cantly better than the connection pathmethod with a beam width of 1 (p < .01), but notat larger beam widths; there are no other signi?-cant performance differences between methods at3 utterances of context.
With 3 utterances of con-text, 1-best accuracies are .286 for the connectionpath method, .329 for the start-complete methodand .356 for the shift-reduce method.The most common errors in dialog act predic-tion occur with rare dialog acts, Not(Information),and the prediction of Acknowledge at the start of aturn (we did not remove grounding acts from thedata).
With the shift-reduce method, some YNQacts are commonly mislabeled.
With all methods,dialog acts pertaining to Order-Info and Product-Info acts are commonly mislabeled, which couldpotentially indicate that these labels require a sub-tle distinction between information pertaining toan order and information pertaining to a product.Table 4 shows the parsing actions performed byeach of our methods on the dialog snippet pre-sented in Figure 4.
For this example, the connec-tion path method?s output is correct in all cases.6 Conclusions and Future WorkIn this paper, we present a parsing-based modelof task-oriented dialog that tightly integrates in-terpretation and generation using a subtask treerepresentation, can be trained from data, and runsincrementally for use in dialog management.
Atthe core of this model is a parser that incremen-tally builds the dialog task structure as it interpretsuser actions and generates system actions.
We ex-periment with three different incremental parsingmethods for our dialog model.
Our proposed shift-reduce method is the best-performing so far, andperformance of this method for dialog act classi?-cation and task/subtask modeling is good enoughto be usable.
However, performance of all themethods for dialog act prediction is too low to beuseful at the moment.
In future work, we will ex-plore improved models for this task that make useof global information about the task (e.g.
whethereach possible subtask has yet been completed;whether required and optional task-related con-cepts such as shipping address have been ?lled).We will also separate grounding and task-relatedbehaviors in our model.101ReferencesJ.
Alexandersson and N. Reithinger.
1997.
Learningdialogue structures from a corpus.
In Proceedingsof Eurospeech.J.
Baldridge and A. Lascarides.
2005.
Probabilistichead-driven parsing for discourse.
In Proceedingsof CoNLL.S.
Bangalore, G. Di Fabbrizio, and A. Stent.
2006.Learning the structure of task-driven human-humandialogs.
In Proceedings of COLING/ACL.A.
Barrett and D. Weld.
1994.
Task-decomposition viaplan parsing.
In Proceedings of AAAI.A.
Berger, S.D.
Pietra, and V.D.
Pietra.
1996.
A Max-imum Entropy Approach to Natural Language Pro-cessing.
Computational Linguistics, 22(1):39?71.N.
Blaylock and J. F. Allen.
2006.
Hierarchical instan-tiated goal recognition.
In Proceedings of the AAAIWorkshop on Modeling Others from Observations.D.
Bohus and A. Rudnicky.
2003.
RavenClaw: Dialogmanagement using hierarchical task decompositionand an expectation agenda.
In Proceedings of Eu-rospeech.H.H.
Bui.
2003.
A general model for online probabal-istic plan recognition.
In Proceedings of IJCAI.S.
Carberry.
2001.
Techniques for plan recogni-tion.
User Modeling and User-Adapted Interaction,11(1?2):31?48.M.
Collins.
2003.
Head-driven statistical models fornatural language parsing.
Computational Linguis-tics, 29(4):589?638.F.
Costa, V. Lombardo, P. Frasconi, and G. Soda.
2001.Wide coverage incremental parsing by learning at-tachment preferences.
In Proceedings of the Con-ference of the Italian Association for Artificial Intel-ligence (AIIA).D.
Cristea.
2000.
An incremental discourse parser ar-chitecture.
In Proceedings of the 2nd InternationalConference on Natural Language Processing.K.
Forbes, E. Miltsakaki, R. Prasad, A. Sarkar,A.
Joshi, and B. Webber.
2003.
D-LTAG system:Discourse parsing with a lexicalized tree-adjoininggrammar.
Journal of Logic, Language and Informa-tion, 12(3):261?279.B.J.
Grosz and C.L.
Sidner.
1986.
Attention, inten-tions and the structure of discourse.
ComputationalLinguistics, 12(3):175?204.P.
Haffner.
2006.
Scaling large margin classi?ers forspoken language understanding.
Speech Communi-cation, 48(3?4):239?261.J.
Hall, J. Nivre, and J. Nilsson.
2006.
Discriminativeclassi?ers for deterministic dependency parsing.
InProceedings of COLING/ACL.P.
Harrison, S. Abney, D. Fleckenger, C. Gdaniec,R.
Grishman, D. Hindle, B. Ingria, M. Marcus,B.
Santorini, and T. Strzalkowski.
1991.
Evaluatingsyntax performance of parser/grammars of English.In Proceedings of the Workshop on Evaluating Nat-ural Language Processing Systems, ACL.H.
LeThanh, G. Abeysinghe, and C. Huyck.
2004.Generating discourse structures for written texts.
InProceedings of COLING.D.
Litman and J. Allen.
1987.
A plan recognitionmodel for subdialogs in conversations.
CognitiveScience, 11(2):163?200.K.
Lochbaum.
1998.
A collaborative planning modelof intentional structure.
Computational Linguistics,24(4):525?572.M.
Poesio and A. Mikheev.
1998.
The predictivepower of game structure in dialogue act recognition:experimental results using maximum entropy esti-mation.
In Proceedings of ICSLP.L.
Polanyi, C. Culy, M. van den Berg, G. L. Thione, andD.
Ahn.
2004.
A rule based approach to discourseparsing.
In Proceedings of SIGdial.D.V.
Pynadath and M.P.
Wellman.
2000.
Probabilisticstate-dependent grammars for plan recognition.
InProceedings of UAI.A.
Ratnaparkhi.
1997.
A linear observed time statis-tical parser based on maximum entropy models.
InProceedings of EMNLP.C.
Rich and C.L.
Sidner.
1997.
COLLAGEN: Whenagents collaborate with people.
In Proceedings ofthe First International Conference on AutonomousAgents.K.
Sagae and A. Lavie.
2006.
A best-?rst proba-bilistic shift-reduce parser.
In Proceedings of COL-ING/ACL.R.
Sera?n and B.
Di Eugenio.
2004.
FLSA: Extendinglatent semantic analysis with features for dialogueact classi?cation.
In Proceedings of ACL.C.L.
Sidner.
1985.
Plan parsing for intended re-sponse recognition in discourse.
Computational In-telligence, 1(1):1?10.R.
Soricut and D. Marcu.
2003.
Sentence level dis-course parsing using syntactic and lexical informa-tion.
In Proceedings of NAACL/HLT.C.
Sporleder and A. Lascarides.
2004.
Combining hi-erarchical clustering and machine learning to pre-dict high-level discourse structure.
In Proceedingsof COLING.102
