Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 242?249,Sydney, July 2006. c?2006 Association for Computational LinguisticsModeling Impression in Probabilistic Transliteration into ChineseLiLi Xu?
Atsushi FujiiGraduate School of Library,Information and Media StudiesUniversity of Tsukuba1-2 Kasuga, Tsukuba, 305-8550, Japanfujii@slis.tsukuba.ac.jpTetsuya IshikawaThe Historiographical InstituteThe University of Tokyo3-1 Hongo 7-chome, Bunkyo-kuTokyo, 133-0033, Japanishikawa@hi.u-tokyo.ac.jpAbstractFor transliterating foreign words into Chi-nese, the pronunciation of a source wordis spelled out with Kanji characters.
Be-cause Kanji comprises ideograms, an indi-vidual pronunciation may be representedby more than one character.
However,because different Kanji characters conveydifferent meanings and impressions, char-acters must be selected carefully.
In thispaper, we propose a transliteration methodthat models both pronunciation and im-pression, whereas existing methods do notmodel impression.
Given a source wordand impression keywords related to thesource word, our method derives possibletransliteration candidates and sorts themaccording to their probability.
We evalu-ate our method experimentally.1 IntroductionReflecting the rapid growth of science, technology,and economies, new technical terms and productnames have progressively been created.
Thesenew words have also been imported into differentlanguages.
There are three fundamental methodsfor importing foreign words into a language.In the first method?translation?the meaningof the source word in question is represented byan existing or new word in the target language.In the second method?transliteration?thepronunciation of the source word is represented byusing the phonetic alphabet of the target language,such as Katakana in Japanese and Hangul in Ko-rean.?
This work was done when the first author was a grad-uate student at University of Tsukuba, who currently worksfor Hitachi Construction Machinery Co., Ltd.In the third method, the source word is spelledout as it is.
However, the misuse of this methoddecreases the understandability and readability ofthe target language.While translation is time-consuming, requiringselection of an existing word or generation of anew word that correctly represents the meaning ofthe source word, transliteration can be performedrapidly.
However, the situation is complicated forChinese, where a phonetic alphabet is not used andKanji is used to spell out both conventional Chi-nese words and foreign words.Because Kanji comprises ideograms, an in-dividual pronunciation can potentially be repre-sented by more than one character.
However, ifseveral Kanji strings are related to the same pro-nunciation of the source word, their meanings willbe different and will therefore convey different im-pressions.For example, ?Coca-Cola?
can be representedby different Kanji strings in Chinese.
The offi-cial transliteration is ?????
?, which comprises???
(tasty)?
and ???
(pleasant)?, and is there-fore associated with a positive connotation.However, there are a number of Kanji stringsthat represent similar pronunciations to that of?Coca-Cola?, but which are associated with in-appropriate impressions for a beverage, such as??????.
This word includes ???
?, which isassociated with choking.Therefore, Kanji characters must be selectedcarefully during transliteration into Chinese.
Thisis especially important when foreign companiesintend to introduce their names and products intoChina.In this paper, we propose a method that modelsboth impression and pronunciation for translitera-tion into Chinese.242Section 2 surveys previous research into auto-matic transliteration, in order to clarify the mean-ing and contribution of our research.
Section 3elaborates on our transliteration method.
Section 4evaluates the effectiveness of our method.2 Related WorkIn a broad sense, the term ?transliteration?
hasbeen used to refer to two tasks.The first task is transliteration in the strictsense, which creates new words in a target lan-guage (Haizhou et al, 2004; Wan and Verspoor,1998).The second task is back-transliteration (Fujiiand Ishikawa, 2001; Jeong et al, 1999; Knightand Graehl, 1998; Qu et al, 2003), which iden-tifies the source word corresponding to an exist-ing transliterated word.
Back-transliteration is in-tended mainly for cross-lingual information re-trieval and machine translation.Both transliteration tasks require methods thatmodel pronunciation in the source and target lan-guages.However, by definition, in back-transliteration,the word in question has already been transliter-ated and the meaning or impression of the sourceword does not have to be considered.
Thus, back-transliteration is outside the scope of this paper.In the following, we use the term ?translitera-tion?
to refer to transliteration in the strict sense.Existing transliteration methods for Chi-nese (Haizhou et al, 2004; Wan and Verspoor,1998) aim to spell out foreign names of peopleand places, and do not model impression.However, as exemplified by ?Coca-Cola?
inSection 1, the impression of words needs to bemodeled in the transliteration of proper names,such as companies and products.
The contribu-tion of our research is to incorporate a model ofimpression into automatic transliteration.3 Methodology3.1 OverviewFigure 1 shows our transliteration method, whichmodels both pronunciation and impression whentransliterating foreign words into Chinese.
Wewill explain the entire process of our translitera-tion method in terms of Figure 1.The input for our method is twofold.
First, asource word to be transliterated into Chinese is re-quested.
Second, one or more words that describesource word Impression keyword(s)pronunciation model impression modelranked list of transliteration candidatesranking candidates??
(safeguard)??
(another person)??
(live)??
(nutrition)????
(bitamin)?, ?, ?, ?, ?, ???????????
?Transliteration candidates Kanji charactersFigure 1: Overview of our transliteration methodfor Chinese.the impression of the source word, which we call?impression keywords?, are requested.
Currently,impression keywords must be provided manuallyin Chinese.
The output of our method is one ormore Kanji strings.In an example scenario using our method, a userhas a good command of Chinese and intends tointroduce something (e.g., a company or product)into China.
It is reasonable to assume that this usercan provide one or more Chinese impression key-words to associate with the target object.Using the pronunciation model, the source wordis converted into a set of Kanji strings whose pro-nunciation is similar to that of the source word.Each of these Kanji strings is a transliteration can-didate.Currently, we use Japanese Katakana words assource words, because Katakana words can easilybe converted into pronunciations using the Latinalphabet.
However, in principle, any language thatuses phonetic script can be a source language forour method.
In Figure 1, the Katakana word ?bita-min (vitamin)?
is used as an example source word.Using the impression model, impression key-words are converted into a set of Kanji characters.A simple implementation is to segment each im-pression keyword into characters.However, because it is difficult for a user to pro-vide an exhaustive list of appropriate keywordsand characters, our impression model derives char-acters that are not included in the impression key-words.Because of the potentially large number of se-lected candidates, we need to rank the candidates.We model both pronunciation and impression in243a probabilistic framework, so that transliterationcandidates are sorted according to their probabil-ity score.Transliteration candidates that include manycharacters derived from the impression model arepreferred.
In other words, the Kanji charactersderived via the impression model are used to re-rank the candidates derived via the pronunciationmodel.We elaborate on our probabilistic transliterationmodel in Section 3.2.
We then discuss the pronun-ciation and impression models in Sections 3.3 and3.4, respectively.3.2 Probabilistic Transliteration ModelGiven a romanized Japanese Katakana word Rand a set of impression keywords W , our pur-pose is to select the Kanji string K that maxi-mizes P (K|R,W ), which is evaluated as shownin Equation (1), using Bayes?
theorem.P (K|R,W ) = P (R,W |K) ?
P (K)P (R,W )?
P (R|K) ?
P (W |K) ?
P (K)P (R,W )?
P (R|K) ?
P (W |K) ?
P (K)(1)In the second line of Equation (1), we assume theconditional independence of R and W given K.In the third line, we omit P (R,W ), which is in-dependent of K. This does not affect the rela-tive rank of Kanji strings, when ranked in termsof P (K|R,W ).In Figure 1, R and W are ?bitamin?
and????????????
?, respectively, and a Kcandidate is ????
?.If a user intends to select more than one Kanjistring, those Ks associated with higher probabili-ties should be selected.As shown in Equation (1), P (K|R,W ) canbe approximated by the product of P (R|K),P (W |K), and P (K).
We call these three factorsthe pronunciation, impression, and language mod-els, respectively.The language model, P (K), models the proba-bility of K irrespective of R and W .
In probabilis-tic natural language processing, P (K) is usuallyrealized by a word or character N-gram model, andtherefore a K that appears frequently in a corpusis assigned a high probability.However, because our purpose is to generatenew words, the use of statistics obtained from ex-isting corpora is not effective.
Therefore, we con-sider P (K) to be constant for every K.In summary, P (K|R,W ) is approximated by aproduct of P (R|K) and P (W |K).
The quality ofour transliteration method will depend on the im-plementation of the pronunciation and impressionmodels.3.3 Pronunciation ModelThe pronunciation model, P (R|K), models theprobability that a roman representation R is se-lected, given a Kanji string K.In Japanese, the Hepburn and Kunrei systemsare commonly used for romanization purposes.We use the Hepburn system.
We use Pinyin asa representation for Kanji characters.
We decom-pose K into Kanji characters and associate K withR on a character-by-character basis.
We calculateP (R|K) as shown in Equation (2).P (R|K) ?
P (R|Y ) ?
P (Y |K)?N?i=1P (ri|yi) ?N?j=1P (yj |kj)(2)Y denotes the Pinyin strings representing the pro-nunciation of K. ki denotes a single Kanji char-acter.
ri and yi denote substrings of R and Y ,respectively.
R, Y , and K are decomposed intothe same number of elements, namely N .
We cal-culate P (ri|yi) and P (yi|ki) as shown in Equa-tion (3).P (ri|yi) = F (ri, yi)?rF (r, yi)P (yi|ki) = F (yi, ki)?yF (y, ki)(3)F (x, y) denotes the co-occurrence frequency of xand y.
We need the co-occurrence frequencies ofri and yi and the co-occurrence frequencies of yiand ki in order to calculate P (R|K).We used a bilingual dictionary comprising 1 140Katakana words, most of which are technicalterms and proper nouns, and their transliterationsinto Chinese, which are annotated with Pinyin.
Wemanually corresponded 151 pairs of Katakana androman characters on a mora-by-mora basis, andromanized Katakana characters in the dictionaryautomatically.We obtained 1 140 tuples, of the form< R, Y,K >.
Because the number of tuples was244manageable, we obtained the element-by-elementR, Y , and K correspondences manually.
Finally,we calculated F (ri, yi) and F (yi, ki).If there are many tuples, and the process of man-ual correspondence is expensive, we can automatethe process as performed in existing transliterationmethods, such as the EM algorithm (Knight andGraehl, 1998) or DP matching (Fujii and Ishikawa,2001).The above calculations are performed off-line.In the online process, we consider all possible seg-mentations of a single Katakana word.
For exam-ple, the romanized Katakana word ?bitamin (vi-tamin)?
corresponds to two Pinyin strings and issegmented differently, as follows:?
bi-ta-min: wei-ta-ming,?
bi-ta-mi-n: wei-ta-mi-an.3.4 Impression ModelThe impression model, P (W |K), models theprobability that W is selected as a set of impres-sion keywords, given Kanji string K. As in thecalculation of P (R|K) in Equation (2), we de-compose W and K into elements, in calculatingP (W |K).W is decomposed into a set of words, wi, andK is decomposed into a set of Kanji characters, kj .We calculate P (W |K) as a product of P (wi|kj),which is the probability that wi is selected as animpression keyword given kj .However, unlike Equation (2), the numbers ofwi and kj derived from W and K are not alwaysthe same, because users are allowed to provide anarbitrary number of impression keywords.
There-fore, for each kj we select the wi that maximizesP (wi|kj) and approximate P (W |K) as shown inEquation (4).P (W |K) ?
?jmaxwi P (wi|kj) (4)Figure 2 shows an example in which the four Chi-nese words in the ?wi?
column are also used inFigure 1.We calculate P (wi|kj) by Equation (5).P (wi|kj) = F (wi, kj)?wF (w, kj)(5)As in Equation (3), F (x, y) denotes the co-occurrence frequency of x and y.0.6??????
????0.1????0.40.3??
????0.5????
?iw jk3??
??
??
??_???
3??_?
h3??_?
h3??_?
hhFigure 2: Example calculation of P (W |K).In summary, we need co-occurrences of eachword and character in Chinese.These co-occurrences can potentially be col-lected from existing language resources, such ascorpora in Chinese.However, it is desirable to collect an associationbetween a word and a character, not simply theirco-occurrence in corpora.
Therefore, we useda dictionary of Kanji in Chinese, in which eachKanji character entry is explained via sentences,and often exemplified by one or more words thatinclude that character.We selected 599 entry characters that are oftenused to spell out foreign words.
Then we collectedthe frequencies with which each word is used toexplain each entry character.Because Chinese sentences lack lexical seg-mentation, we used SuperMorpho1 to perform amorphological analysis of explanation sentencesand example words.
As a result, 16 943 word typeswere extracted.
We used all of these words to cal-culate the co-occurrence frequencies, irrespectiveof the parts of speech.Table 1 shows examples of Kanji characters,Chinese words, and their co-occurrence frequen-cies in the dictionary.However, P (wi|kj) cannot be calculated for theKanji characters not modeled in our method (i.e.,the Kanji characters not included in the 599 entrycharacters).
Thus, for smoothing purposes, we ex-perimentally set P (wi|kj) at 0.001 for those kj notmodeled.4 Experiments4.1 MethodWe evaluated our transliteration method experi-mentally.
Because the contribution of our researchis the incorporation of the impression model in atransliteration method, we used a method that usesonly the pronunciation model as a control.1http://www.omronsoft.com/245Table 1: Example of characters, words, and theirco-occurrence frequencies.jk  iw  ),( ji kwF  jk  iw  ),( ji kwF  jk  iw  ),( ji kwF? ? 39 ? ? 3 ? ?
? 2? ?
? 8 ? ?
? 2 ? ?
? 1? ? 4 ? ? 43 ? ?
? 5? ? 4 ? ?
? 2 ? ?
? 2? ?
? 2 ? ?
? 2 ? ? 51? ? 1 ? ?
? 2 ? ? 5? ? 2 ? ?
? 2 ? ? 3? ?
? 2 ? ?
? 4 ? ?
? 11? ?
? 2 ? ?
? 2 ? ?
? 2? ?
? 3 ? ?
? 1 ? ?
? 7? ?
? 1 ? ?
? 2 ? ?
? 5From a Japanese?Chinese dictionary, we se-lected 210 Katakana words that had been translit-erated into Chinese, and used these Katakanawords as test words.
Each test word can be clas-sified into one of the following five categories:products, companies, places, persons, or generalwords.
Details of the categories of test inputs areshown in Table 2.Three Chinese graduate students who had agood command of Japanese served as assessorsand produced reference data.
None of the asses-sors was an author of this paper.
The assessorsperformed the same task for the same test wordsindependently, in order to enhance the objectivityof the results.We produced the reference data via the follow-ing procedure.First, for each test word, each assessor pro-vided one or more impression keywords in Chi-nese.
We did not restrict the number of impressionkeywords per test word, which was determined byeach assessor.If an assessor provided more than one impres-sion keyword for a single test word, he/she wasrequested to sort them in order of preference, sothat we could investigate the effect of the numberof impression keywords on the evaluation results,by changing the number of top keywords used fortransliteration purposes.We provided the assessors with the descriptionsfor the test words from the source dictionary, sothat the assessors could understand the meaningof each test word.Second, for each test word, we applied the con-trol method and our method independently, whichproduced two lists of ranked transliteration candi-dates.
Because the impression keywords providedby the assessors were used only in our method, theTable 2: Categories of test words.Example wordCategory # WordsJapanese Chinese EnglishProduct 63 ????
??
AudiCompany 49 ????
???
EpsonPlace 36 ????
???
OhioPerson 21 ????
??
ChopinGeneral 41 ?????
???
angelranked list produced by the control was the samefor all assessors.Third, for each test word, each assessor identi-fied one or more correct transliterations, accordingto their impression of the test word.
It was impor-tant not to reveal to the assessors which methodproduced which candidates.By these means, we selected the top 100transliteration candidates from the two ranked listsfor the control and our method.
We merged thesecandidates, removed duplications, and sorted theremaining candidates by the character code.As a result, the assessors judged the correctnessof up to 200 candidates for each test word.
How-ever, for some test words, assessors were not ableto find correct transliterations in the candidate list.The resultant reference data was used to eval-uate the accuracy of a test method in rankingtransliteration candidates.
We used the averagerank of correct answers in the list as the evalua-tion measure.
If more than one correct answer wasfound for a single test word, we first averaged theranks of these answers and then averaged the ranksover the test words.Although we used the top 100 candidates forjudgment purposes, the entire ranked list was usedto evaluate each method.
Therefore, the averagerank of correct answers can potentially be over100.
The average number of candidates per testword was 31 779.Because our method uses the impression modelto re-rank the candidates produced by the pronun-ciation model, the lists for the control and ourmethod comprise the same candidates.
Therefore,it is fair to compare these two methods by the av-erage rank of the correct answers.For each test word, there is more than one typeof ?correct answer?, as follows:(a) transliteration candidates judged as correctby the assessors independently (translitera-246tion candidates judged as correct by at leastone assessor);(b) transliteration candidates judged as correctby all assessors;(c) transliterations defined in the source dictio-nary.In (a), the coverage of correct answers is thelargest, whereas the objectivity of the judgment isthe lowest.In (c), the objectivity of the judgment is thelargest, whereas the coverage of correct answersis the lowest.
Although for each Katakana wordthe source dictionary gives only one transliterationthat is commonly used, there are a number of ap-propriate out-of-dictionary transliterations.In (b), where the assessors did not disagreeabout the correctness, the coverage of correctnessand the objectivity are both middle ranked.Because none of the above answer types is per-fect, we used all three types independently.4.2 Results and AnalysesTables 3?5 show the results of comparative exper-iments using the answer types (a)?
(c) above, re-spectively.In Tables 3?5, the column ?# of test words?
de-notes the number of test words for which at leastone correct answer exists.
While the values in thesecond column of Table 3 are different dependingon the assessor, in Tables 4 and 5 the values of thesecond column are the same for all assessors.The columns ?Avg.
# of KW?
and ?Avg.
# ofanswers?
denote the number of impression key-words and the number of correct answers per testword, respectively.
While the values in the fourthcolumn of Table 3 are different depending on theassessor, in Tables 4 and 5 the values of the fourthcolumn are the same for all assessors.In Tables 4 and 5, the average rank of correct an-swers for the control is the same for all assessors.However, the average rank of correct answers forour method is different depending on the assessor,because the impression keywords used dependedon the assessor.The two columns in ?Avg.
rank?
denote the av-erage ranks of correct answers for the control andfor our method, respectively.
Looking at Tables 3?5, it can be seen that our method outperformed thecontrol in ranking transliteration candidates, irre-spective of the assessor and the answer type.The average rank of correct answers for ourmethod in Table 5 was lower than those in Tables 3and 4.
One reason is that the correct answers in thesource dictionary are not always related to the im-pression keywords provided by the assessors.Table 6 presents the results in Table 3 on acategory-by-category basis.
Because the resultswere similar for answer types (b) and (c), we showonly the answer type (a) results, for the sake ofconciseness.
Looking at Table 6, it can be seenthat our method outperformed the control in rank-ing transliteration candidates, irrespective of thecategory of test words.Our method was effective for transliteratingnames of places and people, although these typesof words are usually transliterated independentlyof their impressions, compared with the names ofproducts and companies.One reason is that, in the dictionary of Kanjiused to produce the impression model, the expla-nation of an entry sometimes includes a phrase,such as ?this character is often used for a person?sname?.
Assessors provided the word ?person?
inChinese as an impression keyword for a numberof person names.
As a result, transliteration can-didates that included characters typically used fora person?s name were highly ranked.It may be argued that, because the impressionmodel was produced using Kanji characters thatare often used for transliteration purposes, the im-pression model could possibly rank correct an-swers better than the pronunciation model.
How-ever, the pronunciation model was also producedfrom Kanji characters used for transliteration pur-poses.Figure 3 shows the distribution of correct an-swers for different ranges of ranks, using answertype (a).
The number of correct answers in the top10 for our method is approximately twice that ofthe control.
In addition, by our method, most ofthe correct answers can be found in the top 100candidates.
Because the results were similar foranswer types (b) and (c), we show only the answertype (a) results, for the sake of conciseness.As explained in Section 4.1, for each test word,the assessors were requested to sort the impressionkeywords in order of preference.
We analyzed therelation between the number of impression key-words used for the transliteration and the averagerank of correct answers, by varying the thresholdfor the number of top impression keywords used.247Table 3: Results obtained with answer type (a).Avg.
rankAssessor # of test words Avg.
# of KW Avg.
# of answers Control Our methodA 205 5.1 3.8 706 82B 204 5.8 3.8 728 44C 199 3.5 2.6 1 130 28Avg.
203 4.8 3.4 855 51Table 4: Results obtained with answer type (b).Avg.
rankAssessor # of test words Avg.
# of KW Avg.
# of answers Control Our methodA 108 5.1 1.1 297 22B 108 5.8 1.1 297 23C 108 3.5 1.1 297 18Avg.
108 4.8 1.1 297 21Table 5: Results obtained with answer type (c).Avg.
rankAssessor # of test words Avg.
# of KW Avg.
# of answers Control Our methodA 210 5.1 1 1 738 260B 210 5.8 1 1 738 249C 210 3.5 1 1 738 103Avg.
210 4.8 1 1 738 204Table 6: Results obtained with answer type (a) on a category-by-category basis.Avg.
rankCategory # of test words Avg.
# of KW Avg.
# of answers Control Our methodProduct 144 4.8 3.5 1 527 64Company 186 4.7 3.6 742 54Place 102 4.8 3.7 777 46Person 61 5.0 3.4 766 51General 115 4.7 2.6 280 38Avg.
122 4.8 3.4 818 51????????????????????????????????
???????????????????????????????????????????????????????????????????????????????????????????
?Figure 3: Distribution of average rank for correct answers.248Table 7 shows the average rank of correct an-swers for different numbers of impression key-words, on an assessor-by-assessor basis.
By com-paring Tables 3 and 7, we see that even if a sin-gle impression keyword was provided, the averagerank of correct answers was higher than that forthe control.
In addition, the average rank of correctanswers was generally improved by increasing thenumber of impression keywords.Finally, we investigated changes in the rank ofcorrect answers caused by our method.
Table 8shows the results, in which ?Higher?
and ?Lower?denote the number of correct answers whose ranksdetermined by our method were higher or lower,respectively, than those determined by the control.For approximately 30% of the correct answers,our method decreased the control?s rank.
Errorswere mainly caused by correct answers containingKanji characters that were not modeled in the im-pression model.
Although we used a smoothingtechnique for characters not in the model, the re-sult was not satisfactory.
To resolve this problem,the number of characters in the impression modelshould be increased.In summary, our method, which uses both theimpression and pronunciation models, ranked cor-rect transliterations more highly than a methodthat used only the pronunciation model.
We con-clude that the impression model is effective fortransliterating foreign words into Chinese.
At thesame time, we concede that there is room for im-provement in the impression model.5 ConclusionFor transliterating foreign words into Chinese, thepronunciation of a source word is spelled out withKanji characters.
Because Kanji characters areideograms, a single pronunciation can be repre-sented by more than one character.
However, be-cause different Kanji characters convey differentmeanings and impressions, characters must be se-lected carefully.In this paper, we proposed a transliterationmethod that models both pronunciation and im-pression, compared to existing methods that donot model impression.
Given a source word andimpression keywords related to the source word,our method derives possible transliteration candi-dates, and sorts them according to their probabil-ity.
We showed the effectiveness of our methodexperimentally.Table 7: Relation between the number of impres-sion keywords and average rank of correct answerswith answer type (a).# of KWAssessor 1 2 3A 103 94 92B 64 60 52C 113 73 34Table 8: Changes in ranks of correct answerscaused by our method.Avg.
rankAnswer type # of answers Higher Lower(a) 2 070 1 431 639(b) 360 250 110(c) 630 422 208Future work will include collecting impressionkeywords automatically, and adapting the lan-guage model to the category of source words.ReferencesAtsushi Fujii and Tetsuya Ishikawa.
2001.Japanese/English cross-language informationretrieval: Exploration of query translation andtransliteration.
Computers and the Humanities,35(4):389?420.Li Haizhou, Zhang Min, and Su Jian.
2004.
A jointsource-channel model for machine transliteration.In Proceedings of the 42nd Annual Meeting of theAssociation for Computational Linguistics, pages160?167.Kil Soon Jeong, Sung Hyon Myaeng, Jae Sung Lee,and Key-Sun Choi.
1999.
Automatic identificationand back-transliteration of foreign words for infor-mation retrieval.
Information Processing & Man-agement, 35:523?540.Kevin Knight and Jonathan Graehl.
1998.
Ma-chine transliteration.
Computational Linguistics,24(4):599?612.Yan Qu, Gregory Grefenstette, and David A. Evans.2003.
Automatic transliteration for Japanese-to-English text retrieval.
In Proceedings of the 26th An-nual International ACM SIGIR Conference on Re-search and Development in Information Retrieval,pages 353?360.Stephen Wan and Cornelia Maria Verspoor.
1998.
Au-tomatic English-Chinese name transliteration for de-velopment of multilingual resources.
In Proceed-ings of the 36th Annual Meeting of the Associationfor Computational Linguistics and the 17th Inter-national Conference on Computational Linguistics,pages 1352?1356.249
