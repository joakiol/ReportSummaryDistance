Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 177?184,Sydney, July 2006. c?2006 Association for Computational LinguisticsTrace Prediction and RecoveryWith Unlexicalized PCFGs and Slash FeaturesHelmut SchmidIMS, University of Stuttgartschmid@ims.uni-stuttgart.deAbstractThis paper describes a parser which gen-erates parse trees with empty elements inwhich traces and fillers are co-indexed.The parser is an unlexicalized PCFGparser which is guaranteed to return themost probable parse.
The grammar isextracted from a version of the PENNtreebank which was automatically anno-tated with features in the style of Kleinand Manning (2003).
The annotation in-cludes GPSG-style slash features whichlink traces and fillers, and other featureswhich improve the general parsing accu-racy.
In an evaluation on the PENN tree-bank (Marcus et al, 1993), the parseroutperformed other unlexicalized PCFGparsers in terms of labeled bracketing f-score.
Its results for the empty cate-gory prediction task and the trace-filler co-indexation task exceed all previously re-ported results with 84.1% and 77.4% f-score, respectively.1 IntroductionEmpty categories (also called null elements) areused in the annotation of the PENN treebank (Mar-cus et al, 1993) in order to represent syntacticphenomena like constituent movement (e.g.
wh-extraction), discontinuous constituents, and miss-ing elements (PRO elements, empty complemen-tizers and relative pronouns).
Moved constituentsare co-indexed with a trace which is located atthe position where the moved constituent is to beinterpreted.
Figure 1 shows an example of con-stituent movement in a relative clause.Empty categories provide important informa-tion for the semantic interpretation, in particularNPNPNNSthingsSBARWHPP-1INofWHNPWDTwhichSNP-SBJPRPtheyVPVBPareADJP-PRDJJunawarePP-NONE-*T*-1Figure 1: Co-indexation of traces and fillersfor determining the predicate-argument structureof a sentence.
However, most broad-coverage sta-tistical parsers (Collins, 1997; Charniak, 2000,and others) which are trained on the PENN tree-bank generate parse trees without empty cate-gories.
In order to augment such parsers withempty category prediction, three rather differentstrategies have been proposed: (i) pre-processingof the input sentence with a tagger which insertsempty categories into the input string of the parser(Dienes and Dubey, 2003b; Dienes and Dubey,2003a).
The parser treats the empty elementslike normal input tokens.
(ii) post-processingof the parse trees with a pattern matcher whichadds empty categories after parsing (Johnson,2001; Campbell, 2004; Levy and Manning, 2004)(iii) in-processing of the empty categories with aslash percolation mechanism (Dienes and Dubey,2003b; Dienes and Dubey, 2003a).
The empty el-ements are here generated by the grammar.Good results have been obtained with all threeapproaches, but (Dienes and Dubey, 2003b) re-ported that in their experiments, the in-processingof the empty categories only worked with lexi-calized parsing.
They explain that their unlex-177icalized PCFG parser produced poor results be-cause the beam search strategy applied there elim-inated many correct constituents with empty ele-ments.
The scores of these constituents were toolow compared with the scores of constituents with-out empty elements.
They speculated that ?doingan exhaustive search might help?
here.In this paper, we confirm this hypothesis andshow that it is possible to accurately predict emptycategories with unlexicalized PCFG parsing andslash features if the true Viterbi parse is com-puted.
In our experiments, we used the BitParparser (Schmid, 2004) and a PCFG which was ex-tracted from a version of the PENN treebank thatwas automatically annotated with features in thestyle of (Klein and Manning, 2003).2 Feature AnnotationA context-free grammar which generates emptycategories has to make sure that a filler exists foreach trace and vice versa.
A well-known tech-nique which enforces this constraint is the GPSG-style percolation of a slash feature: All con-stituents on the direct path from the trace to thefiller are annotated with a special feature whichrepresents the category of the filler as shown in fig-ure 2.
In order to restore the original treebank an-NPNPNNSthingsSBARWHPP/WHPPINofWHNPWDTwhichS/WHPPNP-SBJPRPtheyVP/WHPPVBPareADJP-PRD/WHPPJJunawarePP/WHPP-NONE-/WHPP*T*/WHPPFigure 2: Slash features: The filler node of cate-gory WHNP is linked to the trace node via perco-lation of a slash feature.
The trace node is labeledwith *T*.notation with co-reference indices from the repre-sentation with slash features, the parse tree has tobe traversed starting at a trace node and followingthe nodes annotated with the respective filler cate-gory until the filler node is encountered.
Normally,the filler node is a sister node of an ancestor nodeof the trace, i.e.
the filler c-commands the tracenode, but in case of clausal fillers it is also possi-ble that the filler dominates the trace.
An exampleis the sentence ?S-1 She had ?
he informed her *-1 ?
kidney trouble?
whose parse tree is shown infigure 3.Besides the slash features, we used other fea-tures in order to improve the parsing accuracy ofthe PCFG, inspired by the work of Klein and Man-ning (2003).
The most important ones of thesefeatures1 will now be described in detail.
Sec-tion 4.3 shows the impact of these features onlabeled bracketing accuracy and empty categoryprediction.VP feature VPs were annotated with a featurethat distinguishes between finite, infinitive, to-infinitive, gerund, past participle, and passive VPs.S feature The S node feature distinguishes be-tween imperatives, finite clauses, and several typesof small clauses.Parent features Modifier categories like SBAR,PP, ADVP, RB and NP-ADV were annotated witha parent feature (cf.
Johnson (1998)).
Theparent features distinguish between verbal (VP),adjectival (ADJP, WHADJP), adverbial (ADVP,WHADVP), nominal (NP, WHNP, QP), preposi-tional (PP) and other parents.PENN tags The PENN treebank annotation usessemantic tags to refine syntactic categories.
Mostparsers ignore this information.
We preservedthe tags ADV, CLR, DIR, EXT, IMP, LGS, LOC,MNR, NOM, PRD, PRP, SBJ and TMP in combi-nation with selected categories.Auxiliary feature We added a feature to thepart-of-speech tags of verbs in order to distinguishbetween be, do, have, and full verbs.Agreement feature Finite VPs are marked with3s (n3s) if they are headed by a verb with part-of-speech VBZ (VBP).Genitive feature NP nodes which dominate anode of the category POS (possessive marker) aremarked with a genitive flag.Base NPs NPs dominating a node of categoryNN, NNS, NNP, NNPS, DT, CD, JJ, JJR, JJS, PRP,RB, or EX are marked as base NPs.1The complete annotation program is availablefrom the author?s home page at http://www.ims.uni-stuttgart.de/ schmid178S-1NP-SBJPRPSheVPVBDhadPRN:?SNP-SBJPRPheVPVBDinformedNPPRPherSBAR-NONE-0S-NONE-*T*-1:?NPNNkidneyNNtrouble..Figure 3: Example of a filler which dominates its traceIN feature The part-of-speech tags of the 45most frequent prepositions were lexicalized byadding the preposition as a feature.
The new part-of-speech tag of the preposition ?by?
is ?IN/by?.Irregular adverbs The part-of-speech tags ofthe adverbs ?as?, ?so?, ?about?, and ?not?
werealso lexicalized.Currency feature NP and QP nodes are markedwith a currency flag if they dominate a node ofcategory $, #, or SYM.Percent feature Nodes of the category NP orQP are marked with a percent flag if they dominatethe subtree (NN %).
Any node which immediatelydominates the token %, is marked, as well.Punctuation feature Nodes which dominatesentential punctuation (.?!)
are marked.DT feature Nodes of category DT are split intoindefinite articles (a, an), definite articles (the),and demonstratives (this, that, those, these).WH feature The wh-tags (WDT, WP, WRB,WDT) of the words which, what, who, how, andthat are also lexicalized.Colon feature The part-of-speech tag ?:?
was re-placed with ?
;?, ???
or ?...?
if it dominated a cor-responding token.DomV feature Nodes of a non-verbal syntacticcategory are marked with a feature if they domi-nate a node of category VP, SINV, S, SQ, SBAR,or SBARQ.Gap feature S nodes dominating an empty NPare marked with the feature gap.Subcategorization feature The part-of-speechtags of verbs are annotated with a feature whichencodes the sequence of arguments.
The encod-ing maps reflexive NPs to r, NP/NP-PRD/SBAR-NOM to n, ADJP-PRD to j, ADVP-PRD to a,PRT to t, PP/PP-DIR to p, SBAR/SBAR-CLR tob, S/fin to sf, S/ppres/gap to sg, S/to/gap to st,other S nodes to so, VP/ppres to vg, VP/ppast tovn, VP/pas to vp, VP/inf to vi, and other VPs tovo.
A verb with an NP and a PP argument, forinstance, is annotated with the feature np.Adjectives, adverbs, and nouns may also get asubcat feature which encodes a single argumentusing a less fine-grained encoding which maps PPto p, NP to n, S to s, and SBAR to b.
A node ofcategory NN or NNS e.g.
is marked with a subcatfeature if it is followed by an argument categoryunless the argument is a PP which is headed bythe preposition of.RC feature In relative clauses with an emptyrelative pronoun of category WHADVP, we markthe SBAR node of the relative clause, the NP nodeto which it is attached, and its head child of cate-gory NN or NNS, if the head word is either way,ways, reason, reasons, day, days, time, moment,place, or position.
This feature helps the parserto correctly insert WHADVP rather than WHNP.Figure 4 shows a sample tree.TMP features Each node on the path betweenan NP-TMP or PP-TMP node and its nominal headis labeled with the feature tmp.
This feature helpsthe parser to identify temporal NPs and PPs.MNR and EXT features Similarly, each nodeon the path between an NP-EXT, NP-MNR orADVP-TMP node and its head is labeled with the179NPNP/xNN/xtimeSBAR/xWHADVP-1-NONE-0SNP-SBJ-NONE-*VPTOtoVPVBrelaxADVP-TMP-NONE-*T*-1Figure 4: Annotation of relative clauses withempty relative pronoun of category WHADVPfeature ext or mnr.ADJP features Nodes of category ADJP whichare dominated by an NP node are labeled with thefeature ?post?
if they are in final position and thefeature ?attr?
otherwise.JJ feature Nodes of category JJ which are dom-inated by an ADJP-PRD node are labeled with thefeature ?prd?.JJ-tmp feature JJ nodes which are dominatedby an NP-TMP node and which themselves dom-inate one of the words ?last?, ?next?, ?late?, ?pre-vious?, ?early?, or ?past?
are labeled with tmp.QP feature If some node dominates an NP nodefollowed by an NP-ADV node as in (NP (NP onedollar) (NP-ADV a day)), the first child NP nodeis labeled with the feature ?qp?.
If the parent is anNP node, it is also labeled with ?qp?.NP-pp feature NP nodes which dominate a PPnode are labeled with the feature pp.
If this PPitself is headed by the preposition of, then it is an-notated with the feature of.MWL feature In adverbial phrases which nei-ther dominate an adverb nor another adverbialphrase, we lexicalize the part-of-speech tags of asmall set of words like ?least?
(at least), ?kind?, or?sort?
which appear frequently in such adverbialphrases.Case feature Pronouns like he or him , but notambiguous pronouns like it are marked with nomor acc, respectively.Expletives If a subject NP dominates an NPwhich consists of the pronoun it, and an S-trace insentences like It is important to..., the dominatedNP is marked with the feature expl.LST feature The parent nodes of LST nodes2are marked with the feature lst.Complex conjunctions In SBAR constituentsstarting with an IN and an NN child node (usu-ally indicating one of the two complex conjunc-tions ?in order to?
or ?in case of?
), we mark theNN child with the feature sbar.LGS feature The PENN treebank marks thelogical subject of passive clauses which are real-ized by a by-PP with the semantic tag LGS.
Wemove this tag to the dominating PP.OC feature Verbs are marked with an objectcontrol feature if they have an NP argument whichdominates an NP filler and an S argument whichdominates an NP trace.
An example is the sen-tence She asked him to come.Corrections The part-of-speech tags of thePENN treebank are not always correct.
Some ofthe errors (like the tag NNS in VP-initial position)can be identified and corrected automatically inthe training data.
Correcting tags did not alwaysimprove parsing accuracy, so it was done selec-tively.The gap and domV features described abovewere also used by Klein and Manning (2003).All features were automatically added to thePENN treebank by means of an annotation pro-gram.
Figure 5 shows an example of an annotatedparse tree.3 Parameter SmoothingWe extracted the grammar from sections 2?21 ofthe annotated version of the PENN treebank.
Inorder to increase the coverage of the grammar,we selectively applied markovization to the gram-mar (cf.
Klein and Manning (2003)) by replacinglong infrequent rules with a set of binary rules.Markovization was only applied if none of thenon-terminals on the right hand side of the rulehad a slash feature in order to avoid negative ef-fects on the slash feature percolation mechanism.The probabilities of the grammar rules weredirectly estimated with relative frequencies.
Nosmoothing was applied, here.
The lexical prob-abilities, on the other hand, were smoothed with2LST annotates the list symbol in enumerations.180S/fin/.NP-SBJ/3s/domV_<S>NP/base/3s/explPRP/explItS_<S>-NONE-_<S>*EXP*_#<S>VP/3s+<S>VBZ/pst?sPP/VIN/upupPP/PPTOtoNP/basePRPyouS/to/gap+#<S>NP-SBJ-NONE-*VP/toTOtoVP/infVV/rprotectNP/refl/basePRP/reflyourselfFigure 5: An Annotated Parse Treethe following technique which was adopted fromKlein and Manning (2003).
Each word is assignedto one of 216 word classes.
The word classesare defined with regular expressions.
Examplesare the class [A-Za-z0-9-]+-oldwhich con-tains the word 20-year-old, the class [a-z][a-z]+ifies which contains clarifies, and a classwhich contains a list of capitalized adjectives likeAdvanced.
The word classes are ordered.
If astring is matched by the regular expressions ofmore than one word class, then it is assigned to thefirst of these word classes.
For each word class,we compute part-of-speech probabilities with rel-ative frequencies.
The part-of-speech frequen-cies of a wordare smoothed by addingthe part-of-speech probabilityof the wordclassaccording to equation 1 in order to ob-tain the smoothed frequency  .
The part-of-speech probability of the word class is weightedby a parameter  whose value was set to 4 aftertesting on held-out data.
The lexical probabilitiesare finally estimated from the smoothed frequen-cies according to equation 2. 	 	fffifl	 (1)ffi 	  !#"%$  fl&'(2)4 EvaluationIn our experiments, we used the usual splitting ofthe PENN treebank into training data (sections 2?21), held-out data (section 22), and test data (sec-tion 23).The grammar extracted from the automaticallyannotated version of the training corpus contained52,297 rules with 3,453 different non-terminals.Subtrees which dominated only empty categorieswere collapsed into a single empty element sym-bol.
The parser skips over these symbols duringparsing, but adds them to the output parse.
Over-all, there were 308 different empty element sym-bols in the grammar.Parsing section 23 took 169 minutes on a Dual-Opteron system with 2.2 GHz CPUs, which isabout 4.2 seconds per sentence.precision recall f-scorethis paper 86.9 86.3 86.6Klein/Manning 86.3 85.1 85.7Table 1: Labeled bracketing accuracy on sec-tion 23Table 1 shows the labeled bracketing accuracyof the parser on the whole section 23 and com-pares it to the results reported in Klein and Man-ning (2003) for sentences with up to 100 words.4.1 Empty Category PredictionTable 2 reports the accuracy of the parser in theempty category (EC) prediction task for ECs oc-curring more than 6 times.
Following Johnson(2001), an empty category was considered cor-rect if the treebank parse contained an empty nodeof the same category at the same string position.Empty SBAR nodes which dominate an empty Snode are treated as a single empty element andlisted as SBAR-S in table 2.Frequent types of empty elements are recog-nized quite reliably.
Exceptions are the tracesof adverbial and prepositional phrases where therecall was only 65% and 48%, respectively, andempty relative pronouns of type WHNP andWHADVP with f-scores around 60%.
A couple ofempty relative pronouns of type WHADVP weremis-analyzed as WHNP which explains why theprecision is higher than the recall for WHADVP,but vice versa for WHNP.181prec.
recall f-sc.
freq.NP * 87.0 85.9 86.5 1607NP *T* 84.9 87.6 86.2 5080 95.2 89.7 92.3 416*U* 95.3 93.8 94.5 388ADVP *T* 80.3 64.7 71.7 170S *T* 86.7 93.8 90.1 160SBAR-S *T* 88.5 76.7 82.1 120WHNP 0 57.6 63.6 60.4 107WHADVP 0 75.0 50.0 60.0 36PP *ICH* 11.1 3.4 5.3 29PP *T* 73.7 48.3 58.3 29SBAR *EXP* 28.6 12.5 17.4 16VP *?
* 33.3 40.0 36.4 15S *ICH* 61.5 57.1 59.3 14S *EXP* 66.7 71.4 69.0 14SBAR *ICH* 60.0 25.0 35.3 12NP *?
* 50.0 9.1 15.4 11ADJP *T* 100.0 77.8 87.5 9SBAR-S *?
* 66.7 25.0 36.4 8VP *T* 100.0 37.5 54.5 8overall 86.0 82.3 84.1 3716Table 2: Accuracy of empty category predictionon section 23.
The first column shows the type ofthe empty element and ?
except for empty comple-mentizers and empty units ?
also the category.
Thelast column shows the frequency in the test data.The accuracy of the pseudo attachment labels*RNR*, *ICH*, *EXP*, and *PPA* was gener-ally low with a precision of 41%, recall of 21%,and f-score of 28%.
Empty elements with a testcorpus frequency below 8 were almost never gen-erated by the parser.4.2 Co-IndexationTable 3 shows the accuracy of the parser on theco-indexation task.
A co-indexation of a trace anda filler is represented by a 5-tuple consisting ofthe category and the string position of the trace,as well as the category, start and end position ofthe filler.
A co-indexation is judged correct if thetreebank parse contains the same 5-tuple.For NP3 and S4 traces of type ?
*T*?, the co-indexation results are quite good with 85% and92% f-score, respectively.
For ?
*T*?-traces of3NP traces of type *T* result from wh-extraction in ques-tions and relative clauses and from fronting.4S traces of type *T* occur in sentences with quotedspeech like the sentence ?That?s true!
?, he said *T*.other categories and for NP traces of type ?
*?,5 theparser shows high precision, but moderate recall.The recall of infrequent types of empty elementsis again low, as in the recognition task.prec.
rec.
f-sc.
freq.NP * 81.1 72.1 76.4 1140WH NP *T* 83.7 86.8 85.2 507S *T* 92.0 91.0 91.5 277WH ADVP *T* 78.6 63.2 70.1 163PP *ICH* 14.3 3.4 5.6 29WH PP *T* 68.8 50.0 57.9 22SBAR *EXP* 25.0 12.5 16.7 16S *ICH* 57.1 53.3 55.2 15S *EXP* 66.7 71.4 69.0 14SBAR *ICH* 60.0 25.0 35.3 12VP *T* 33.3 12.5 18.2 8ADVP *T* 60.0 42.9 50.0 7PP *T* 100.0 28.6 44.4 7overall 81.7 73.5 77.4 2264Table 3: Co-indexation accuracy on section 23.The first column shows the category and type ofthe trace.
If the filler category of the filler is dif-ferent from the category of the trace, it is added infront.
The filler category is abbreviated to ?WH?if the rest is identical to the trace category.
Thelast column shows the frequency in the test data.In order to get an impression how often EC pre-diction errors resulted from misplacement ratherthan omission, we computed EC prediction accu-racies without comparing the EC positions.
Weobserved the largest f-score increase for ADVP*T* and PP *T*, where attachment ambiguitiesare likely, and for VP *?
* which is infrequent.4.3 Feature EvaluationWe ran a series of evaluations on held-out data inorder to determine the impact of the different fea-tures which we described in section 2 on the pars-ing accuracy.
In each run, we deleted one of thefeatures and measured how the accuracy changedcompared to the baseline system with all features.The results are shown in table 4.5The trace type ?*?
combines two types of traces withdifferent linguistic properties, namely empty objects of pas-sive constructions which are co-indexed with the subject, andempty subjects of participial and infinitive clauses which areco-indexed with an NP of the matrix clause.182Feature LB EC CIslash feature 0.43 ?
?VP features 2.93 6.38 5.46PENN tags 2.34 4.54 6.75IN feature 2.02 2.57 5.63S features 0.49 3.08 4.13V subcat feature 0.68 3.17 2.94punctuation feat.
0.82 1.11 1.86all PENN tags 0.84 0.69 2.03domV feature 1.76 0.15 0.00gap feature 0.04 1.20 1.32DT feature 0.57 0.44 0.99RC feature 0.00 1.11 1.10colon feature 0.41 0.84 0.44ADV parent 0.50 0.04 0.93auxiliary feat.
0.40 0.29 0.77SBAR parent 0.45 0.24 0.71agreement feat.
0.05 0.52 1.15ADVP subcat feat.
0.33 0.32 0.55genitive feat.
0.39 0.29 0.44NP subcat feat.
0.33 0.08 0.76no-tmp 0.14 0.90 0.16base NP feat.
0.47 -0.24 0.55tag correction 0.13 0.37 0.44irr.
adverb feat.
0.04 0.56 0.39PP parent 0.08 0.04 0.82ADJP features 0.14 0.41 0.33currency feat.
0.06 0.82 0.00qp feature 0.13 0.14 0.50PP tmp feature -0.24 0.65 0.60WH feature 0.11 0.25 0.27percent feat.
0.34 -0.10 0.10NP-ADV parent f. 0.07 0.14 0.39MNR feature 0.08 0.35 0.11JJ feature 0.08 0.18 0.27case feature 0.05 0.14 0.27Expletive feat.
-0.01 0.16 0.27LGS feature 0.17 0.07 0.00ADJ subcat 0.00 0.00 0.33OC feature 0.00 0.00 0.22JJ-tmp feat.
0.09 0.00 0.00refl.
pronoun 0.02 -0.03 0.16EXT feature -0.04 0.09 0.16MWL feature 0.05 0.00 0.00complex conj.
f. 0.07 -0.07 0.00LST feature 0.12 -0.12 -0.11NP-pp feature 0.13 -0.57 -0.39Table 4: Differences between the baseline f-scoresfor labeled bracketing, EC prediction, and co-indexation (CI) and the f-scores without the spec-ified feature.5 ComparisonTable 7 compares the empty category predictionresults of our parser with those reported in John-son (2001), Dienes and Dubey (2003b) and Camp-bell (2004).
In terms of recall and f-score, ourparser outperforms the other parsers.
In terms ofprecision, the tagger of Dienes and Dubey is thebest, but its recall is the lowest of all systems.prec.
recall f-scorethis paper 86.0 82.3 84.1Campbell 85.2 81.7 83.4Dienes & Dubey 86.5 72.9 79.1Johnson 85 74 79Table 5: Accuracy of empty category predictionon section 23The good performance of our parser on theempty element recognition task is remarkable con-sidering the fact that its performance on the la-beled bracketing task is 3% lower than that of theCharniak (2000) parser used by Campbell (2004).prec.
recall f-scorethis paper 81.7 73.5 77.4Campbell 78.3 75.1 76.7Dienes & Dubey (b) 81.5 68.7 74.6Dienes & Dubey (a) 80.5 66.0 72.6Johnson 73 63 68Table 6: Co-indexation accuracy on section 23Table 6 compares our co-indexation results withthose reported in Johnson (2001), Dienes andDubey (2003b), Dienes and Dubey (2003a), andCampbell (2004).
Our parser achieves the highestprecision and f-score.
Campbell (2004) reports ahigher recall, but lower precision.Table 7 shows the trace prediction accuraciesof our parser, Johnson?s (2001) parser with parserinput and perfect input, and Campbell?s (2004)parser with perfect input.
The accuracy of John-son?s parser is consistently lower than that ofthe other parsers and it has particular difficultieswith ADVP traces, SBAR traces, and empty rela-tive pronouns (WHNP 0).
Campbell?s parser andour parser cannot be directly compared, but whenwe take the respective performance difference toJohnson?s parser as evidence, we might concludethat Campbell?s parser works particularly well onNP *, *U*, and WHNP 0, whereas our system183paper J1 J2 CNP * 83.2 82 91 97.5NP *T* 86.2 81 91 96.20 92.3 88 96 98.5*U* 94.5 92 95 98.6ADVP *T* 71.7 56 66 79.9S *T* 90.1 88 90 92.7SBAR-S *T* 82.1 70 74 84.4WHNP 0 60.4 47 77 92.4WHADVP 0 60.0 ?
?
73.3Table 7: Comparison of the empty category pre-diction accuracies for different categories in thispaper (paper), in (Johnson, 2001) with parser input(J1), in (Johnson, 2001) with perfect input (J2),and in (Campbell, 2004) with perfect input.is slightly better on empty complementizers (0),ADVP traces, and SBAR traces.6 SummaryWe presented an unlexicalized PCFG parser whichapplies a slash feature percolation mechanism togenerate parse trees with empty elements and co-indexation of traces and fillers.
The grammarwas extracted from a version of the PENN tree-bank which was annotated with slash features anda set of other features that were added in orderto improve the general parsing accuracy.
Theparser computes true Viterbi parses unlike mostother parsers for treebank grammars which are notguaranteed to produce the most likely parse treebecause they apply pruning strategies like beamsearch.We evaluated the parser using the standardPENN treebank training and test data.
The labeledbracketing f-score of 86.6% is ?
to our knowl-edge ?
the best f-score reported for unlexical-ized PCFGs, exceeding that of Klein and Man-ning (2003) by almost 1%.
On the empty cate-gory prediction task, our parser outperforms thebest previously reported system (Campbell, 2004)by 0.7% reaching an f-score of 84.1%, althoughthe general parsing accuracy of our unlexicalizedparser is 3% lower than that of the parser used byCampbell (2004).
Our parser also ranks highestin terms of the co-indexation accuracy with 77.4%f-score, again outperforming the system of Camp-bell (2004) by 0.7%.ReferencesRichard Campbell.
2004.
Using linguistic principlesto recover empty categories.
In Proceedings of the42nd Annual Meeting of the ACL, pages 645?652,Barcelona, Spain.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the 1st Meet-ing of the North American Chapter of the Associ-ation for Computational Linguistics (ANLP-NAACL2000), pages 132?139, Seattle, Washington.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proceedings of the35th Annual Meeting of the ACL, Madrid, Spain.P?ter Dienes and Amit Dubey.
2003a.
Antecedentrecovery: Experiments with a trace tagger.
InProceedings of the 2003 Conference on EmpiricalMethods in Natural Language Processing, Sapporo,Japan.P?ter Dienes and Amit Dubey.
2003b.
Deep syntac-tic processing by combining shallow methods.
InProceedings of the 41st Annual Meeting of the ACL,pages 431?438, Sapporo, Japan.Mark Johnson.
1998.
PCFG models of linguis-tic tree representations.
Computational Linguistics,24(4):613?632.Mark Johnson.
2001.
A simple pattern-matching al-gorithm for recovering empty nodes and their an-tecedents.
In Proceedings of the 39th Annual Meet-ing of the ACL, pages 136?143, Toulouse, France.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the ACL, pages 423?430,Sapporo, Japan.Roger Levy and Christopher D. Manning.
2004.
Deepdependencies from context-free statistical parsers:Correcting the surface dependency approximation.In Proceedings of the 42nd Annual Meeting of theACL, pages 327?334, Barcelona, Spain.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19(2):313?330, June.Helmut Schmid.
2004.
Efficient parsing of highlyambiguous context-free grammars with bit vectors.In Proceedings of the 20th International Conferenceon Computational Linguistics (COLING 2004), vol-ume 1, pages 162?168, Geneva, Switzerland.184
