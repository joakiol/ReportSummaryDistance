Proceedings of the Second Workshop on Statistical Machine Translation, pages 197?202,Prague, June 2007. c?2007 Association for Computational LinguisticsThe ISL Phrase-Based MT System for the 2007 ACL Workshop onStatistical Machine TranslationM.
Paulik1,2, K. Rottmann2, J. Niehues2, S. Hildebrand 1,2 and S. Vogel11Interactive Systems Laboratories, Carnegie Mellon University, Pittsburgh, PA, USA2Institut fu?r Theoretische Informatik, Universita?t Karlsruhe (TH), Karlsruhe, Germany{paulik|silja|vogel}@cs.cmu.edu ; {jniehues|rottmann}@ira.uka.deAbstractIn this paper we describe the Interactive Sys-tems Laboratories (ISL) phrase-based ma-chine translation system used in the sharedtask ?Machine Translation for EuropeanLanguages?
of the ACL 2007 Workshop onStatistical Machine Translation.
We presentresults for a system combination of theISL syntax-augmented MT system and theISL phrase-based system by combining andrescoring the n-best lists of the two systems.We also investigate the combination of twoof our phrase-based systems translating fromdifferent source languages, namely Spanishand German, into their common target lan-guage, English.1 IntroductionThe shared task of the ACL 2007 Workshop on Sta-tistical Machine Translation focuses on the auto-matic translation of European language pairs.
Theworkshop provides common training sets for trans-lation model training and language model trainingto allow for easy comparison of results between theparticipants.Interactive Systems Laboratories participated in theEnglish ?
Spanish Europarl and News Commen-tary task as well as in the English ?
German Eu-roparl task.
This paper describes the phrase-basedmachine translation (MT) system that was appliedto these tasks.
We also investigate the feasibilityof combining the ISL syntax-augmented MT system(Zollmann et al, 2007) with our phrase-based sys-tem by combining and rescoring the n-best lists pro-duced by both systems for the Spanish ?
EnglishEuroparl task.
Furthermore, we apply the same com-bination technique to combine two of our phrase-based systems that operate on different source lan-guages (Spanish and German), but share the sametarget language (English).The paper is organized as follows.
In section 2 wegive a general description of our phrase-based sta-tistical machine translation system.
Section 3 givesan overview of the data and of the final systemsused for the English ?
Spanish Europarl and NewsCommentary tasks, along with corresponding per-formance numbers.
Section 4 shows the data, finalsystems and results for the English ?
German Eu-roparl task.
In Section 5, we present our experimentsinvolving a combination of the syntax-augmentedMT system with the phrase-based MT system and acombination of the Spanish ?
English and German?
English phrase-based systems.2 The ISL Phrase-Based MT System2.1 Word and Phrase AlignmentPhrase-to-phrase translation pairs are extracted bytraining IBM Model-4 word alignments in both di-rections, using the GIZA++ toolkit (Och and Ney,2000), and then extracting phrase pair candidateswhich are consistent with these alignments, start-ing from the intersection of both alignments.
Thisis done with the help of phrase model trainingcode provided by University of Edinburgh duringthe NAACL 2006 Workshop on Statistical MachineTranslation (Koehn and Monz, 2006).
The raw rel-197ative frequency estimates found in the phrase trans-lation tables are then smoothed by applying modi-fied Kneser-Ney discounting as explained in (Fosteret al, 2006).
The resulting phrase translation tablesare pruned by using the combined translation modelscore as determined by Minimum Error Rate (MER)optimization on the development set.2.2 Word ReorderingWe apply a part-of-speech (POS) based reorderingscheme (J. M. Crego et al, 2006) to the POS-taggedsource sentences before decoding.
For this, we usethe GIZA++ alignments and the POS-tagged sourceside of the training corpus to learn reordering rulesthat achieve a (locally) monotone alignment.
Fig-ure 1 shows an example in which three reorderingrules are extracted from the POS tags of an En-glish source sentence and its corresponding Span-ish GIZA++ alignment.
Before translation, we con-struct lattices for every source sentence.
The latticesinclude the original source sentence along with allthe reorderings that are consistent with the learnedrules.
All incoming edges of the lattice are anno-tated with distortion model scores.
Figure 2 gives anexample of such a lattice.
In the subsequent latticedecoding step, we apply either monotone decodingor decoding with a reduced local reordering window,typically of size 2.2.3 Decoder and MER TrainingThe ISL beam search decoder (Vogel, 2003) com-bines all the different model scores to find the besttranslation.
Here, the following models were used:?
The translation model, i.e.
the phrase-to-phrase translations extracted from the bilingualcorpus, annoted with four translation modelscores.
These four scores are the smoothed for-ward and backward phrase translation proba-bilities and the forward and backward lexicalweights.?
A 4-gram language model.
The SRI languagemodel toolkit was used to train the languagemodel and we applied modified Kneser-Neysmoothing.?
An internal word reordering model in additionto the already described POS-based reordering.We all agree on thatPRP DT VB IN DTEn {4} esto {5} estamos {1} todos {2} de {} acuerdo {3}?
PRP DT VB IN DT :   4 ?
5 ?
1 ?
2 ?
3?
PRP DT VB:   2 ?
3 ?
1 ?
PRP DT VB IN:   3 ?
4 ?
1 ?
2Figure 1: Rule extraction for the POS-based reorder-ing scheme.This internal reordering model assigns highercosts to longer distance reordering.?
Simple word and phrase count models.
Theformer is essentially used to compensate forthe tendency of the language model to prefershorter translations, while the latter can givepreference to longer phrases, potentially im-proving fluency.The ISL SMT decoder is capable of loadingseveral language models (LMs) at the same time,namely n-gram SRI language models with n up to4 and suffix array language models (Zhang and Vo-gel, 2006) of arbitrary length.
While we typicallysee gains in performance for using suffix array LMswith longer histories, we restricted ourselves here toone 4-gram SRI LM only, due to a limited amountof available LM training data.
The decoding processitself is organized in two stages.
First, all availableword and phrase translations are found and insertedinto a so-called translation lattice.
Then the bestcombination of these partial translations is foundby doing a best path search through the translationlattice, where we also allow for word reorderingswithin a predefined local reordering window.To optimize the system towards a maximal BLEUor NIST score, we use Minimum Error Rate (MER)Training as described in (Och, 2003).
For eachmodel weight, MER applies a multi-linear searchon the development set n-best list produced by thesystem.
Due to the limited numbers of translationsin the n-best list, these new model weights are sub-optimal.
To compensate for this, a new full trans-lation is done.
The resulting new n-best list is thenmerged with the old n-best list and the optimizationprocess is repeated.
Typically, the translation qualityconverges after three iterations.198120 3honourable1.0000Members1.0000 honourable0.3299Members0.6701 675 8we1.0000have1.0000have0.9175a0.08254,1.0000 a1.0000?
?Honourable Members, we have a challenging agenda?Figure 2: Example for a source sentence lattice fromthe POS-based reordering scheme.English Spanishsentence pairs 1259914unique sent.
pairs 1240151sentence length 25.3 26.3words 31.84 M 33.16 Mvocabulary 266.9 K 346.3 KTable 1: Corpus statistics for the English/SpanishEuroparl corpus.3 Spanish?
English Europarl and NewsCommentary Task3.1 Data and Translation TasksThe systems for the English ?
Spanish translationtasks were trained on the sentence-aligned Europarlcorpus (Koehn, 2005).
Detailed corpus statistics canbe found in Table 1.
The available parallel NewsCommentary training data of approximately 1 mil-lion running words for both languages was onlyused as additional language model training data, toadapt our in-domain (Europarl) system to the out-of-domain (News Commentary) task.The development sets consist of 2000 Europarlsentences (dev-EU) and 1057 News Commentarysentences (dev-NC).
The available development-test data consists of 2 x 2000 Europarl sentences(devtest-EU and test06-EU) and 1064 News Com-mentary sentences (test06-NC).
All developmentand development-test sets have only one referencetranslation per sentence.3.2 Data NormalizationThe ACL shared task is very close in form and con-tent to the Final Text Editions (FTE) task of the TC-STAR (TC-STAR, 2004) evaluation.
For this rea-son, we decided to apply a similar normalizationscheme to the training data as was applied in our TC-STAR verbatim SMT system.
Although trained on?verbatimized?
data that did not contain any num-bers, but rather had all numbers and dates spelledout, it yielded consistently better results than ourTC-STAR FTE SMT system.
When translating FTEcontent, the verbatim system treated all numbers asunknown words, i.e.
they were left unchanged dur-ing translation.
To compensate for this, we appliedextended postprocessing to the translations that con-ducts the necessary conversions between Spanishand English numbers, e.g.
the conversion of deci-mal comma in Spanish to decimal point in English.Other key points which we adopted from this nor-malization scheme were the tokenization of punc-tuation marks, the true-casing of the first word ofeach sentence, as well as extended cleaning of thetraining data.
The latter mainly consisted of the re-moval of sections with a highly unbalanced sourceto target words ratio and the removal of unusualstring combinations and document references, likefor example ?B5-0918/2000?, ?
(COM(2000) 335 -C5-0386/2000 - 2000/0143(CNS))?, etc.Based on this normalization scheme, we trained andoptimized a baseline in-domain system on accord-ingly normalized source and reference sentences.For optimization, we combined the available de-velopment sets for the Europarl task and the NewsCommentary task.
In order to further improvethe applied normalization scheme, we experimentedwith replacing all numbers with the string ?NMBR?,rather than spelling them out and by replacing alldocument identifiers with the string ?DCMNT?,rather than deleting them.
This was first done forthe language model training data only, and then forall data, i.e.
for the bilingual training data and forthe development set source and reference sentences.In the latter case, the respective tags were again re-placed by the correct numbers and document identi-fiers during postprocessing.
Table 2 shows the casesensitive BLEU scores for the three normalizationapproaches on the English ?
Spanish Europarl andNews Commentary development sets.
These scoreswere computed with the official NIST scoring scriptagainst the original (not normalized) references.3.3 In-domain SystemAs mentioned above, we combined the Europarl andNews Commentary development sets when optimiz-ing the in-domain system.
This resulted in only one199Task baseline LM only all dataEuroparl 30.94 31.20 31.26News Com.
31.28 31.39 31.73Table 2: Case sensitive BLEU scores on the in-domain and out-of-domain development sets for thethree different normalization schemes.Task Eng ?
Spa Spa ?
Engdev-EU 31.29 31.77dev-NC 31.81 31.12devtest-EU 31.01 31.40test06-EU 31.87 31.76test06-NC 30.23 29.22Table 3: Case sensitive BLEU scores for the finalEnglish ?
Spanish in-domain systems.set of scaling factors, i.e.
the in-domain systemapplies the same scaling factors for translating in-domain data as for translating out-of-domain data.Our baseline system applied only monotone latticedecoding.
For our final in-domain system, we used alocal reordering window of length 2, which accountsfor the slightly higher scores when compared to thebaseline system.
The BLEU scores for both trans-lation directions on the different development anddevelopment-test sets can be found in Table 3.3.4 Out-of-domain SystemIn order to adapt our in-domain system towards theout-of-domain News Commentary task, we consid-ered two approaches based on language model adap-tation.
First, we interpolated the in-domain LMwith an out-of-domain LM computed on the avail-able News Commentary training data.
The inter-polation weights were chosen such as to achieve aminimal LM perplexity on the out-of-domain de-velopment set.
For both languages, the interpo-lation weights were approximately 0.5.
Our sec-ond approach was to simply load the out-of-domainLM as an additional LM into our decoder.
In bothcases, we optimized the translation system on theout-of-domain development data only.
For the sec-ond approach, MER optimization assigned three tofour times higher scaling factors to the consider-ably smaller out-domain LM than to the original in-domain LM.
Table 4 shows the results in BLEU onthe out-of-domain development and development-test sets for both translation directions.
While load-Eng ?
Spa Spa ?
EngTask interp 2 LMs interp 2 LMsdev-NC 33.31 33.28 32.61 32.70test06-NC 32.55 32.15 30.73 30.55Table 4: Case sensitive BLEU scores for the finalEnglish ?
Spanish out-of-domain systems.ing a second LM gives similar or slightly better re-sults on the development set during MER optimiza-tion, we see consistently worse results on the unseendevelopment-test set.
This, in the context of the rela-tively small amount of development data, can be ex-plained by stronger overfitting during optimization.4 English?
German Europarl TaskThe systems for the English ?
German translationtasks were trained on the sentence-aligned Europarlcorpus only.
The complete corpus consists of ap-proximately 32 million English and 30 million Ger-man words.We applied a similar normalization scheme to thetraining data as for the English ?
Spanish system.The main difference was that we did not replacenumbers and that we removed all document refer-ences.
In the translation process, the document ref-erences were treated as unknown words and there-fore left unchanged.
As above, we trained and op-timized a first baseline system on the normalizedsource and reference sentences.
However, we usedonly the Europarl task development set during opti-mization.
To achieve further improvements on theGerman ?
English task, we applied a compoundsplitting technique.
The compound splitting wasbased on (Koehn and Knight, 2003) and was appliedon the lowercased source sentences.
The words gen-erated by the compound splitting were afterwardstrue-cased.
Instead of replacing a compound byits separate parts, we added a parallel path into thesource sentence lattices used for translation.
Thesource sentence lattices were augmented with scoreson their edges indicating whether each edge repre-sents a word of the original text or if it was gener-ated during compound splitting.Table 5 shows the case-sensitive BLEU scores forthe final German ?
English systems.
In contrastto the English ?
Spanish systems, we used onlymonotonous decoding on the lattices containing the200task Eng ?
Ger Ger ?
Engdev-EU 18.58 23.85devtest-EU 18.50 23.87test06-EU 18.39 23.88Table 5: Case sensitive BLEU scores for the finalEnglish ?
German in-domain systems.syntactical reorderings.5 System Combination via n-best ListCombination and Rescoring5.1 N-best List RescoringFor n-best list rescoring we used unique 500-bestlists, which may have less than 500 entries forsome sentences.
In this evaluation, we used sev-eral features computed from different informationsources such as features from the translation sys-tem, additional language models, IBM-1 word lex-ica and the n-best list itself.
We calculated 4 fea-tures from the IBM-1 word lexica: the word proba-bility sum as well as the maximum word probabil-ity in both language directions.
From the n-best listitself, we calculated three different sets of scores.A position-dependent word agreement score as de-scribed in (Ueffing and Ney, 2005) with a positionwindow instead of the Levenshtein alignment, then-best list n-gram probability as described in (Zensand Ney, 2006) and a position-independent n-gramagreement, which is a variation on the first two.
Totune the feature combination weights, we used MERoptimization.Rescoring the n-best lists from our individual sys-tems did not give significant improvements on theavailable unseen development-test data.
For this rea-son, we did not apply n-best list rescoring to the indi-vidual systems.
However, we investigated the feasi-bility of combining two different systems by rescor-ing the joint n-best lists of both systems.
The corre-sponding results are described in the following sec-tions.5.2 Combining Syntax-Augmented MT andPhrase-Based MTOn the Spanish ?
English in-domain task, we par-ticipated not only with the ISL phrase-based SMTsystem as described in this paper, but also withthe ISL syntax-augmented system.
The syntax-task PHRA SYNT COMBdev-EU 31.77 32.48 32.77test06-EU 31.76 32.15 32.27Table 6: Results for combining the syntax-augmented system (SYNT) with the phrase-basedsystem (PHRA).augmented system was trained on the same normal-ized data as the phrase-based system.
However, itwas optimized on the in-domain development setonly.
More details on the syntax-augmented systemcan be found in (Zollmann et al, 2007).
Table 6lists the respective BLEU scores of both systems aswell as the BLEU score achieved by combining andrescoring the individual 500-best lists.5.3 Combining MT Systems with DifferentSource Languages(Och and Ney, 2001) describes methods for trans-lating text given in multiple source languages into asingle target language.
The ultimate goal is to im-prove the translation quality when translating fromone source language, for example English into mul-tiple target languages, such as Spanish and German.This can be done by first translating the English doc-ument into German and then using the translation asan additional source, when translating to Spanish.Another scenario where a multi-source translationbecomes desirable was described in (Paulik et al,2005).
The goal was to improve the quality of au-tomatic speech recognition (ASR) systems by em-ploying human-provided simultaneous translations.By using automatic speech translation systems totranslate the speech of the human interpreters backinto the source language, it is possible to bias thesource language ASR system with the additionalknowledge.
Having these two frameworks in mind,we investigated the possibility of combining our in-domain German ?
English and Spanish ?
Englishtranslation systems using n-best list rescoring.
Ta-ble 7 shows the corresponding results.
Even thoughthe German ?
English translation performance wasapproximately 8 BLEU below the translation perfor-mance of the Spanish ?
English system, we wereable to improve the final translation performance byup to 1 BLEU.201task Spa ?
Eng Ger ?
Eng Comb.dev-EU 31.77 23.85 32.76devtest-EU 31.40 23.87 32.41test06-EU 31.76 23.88 32.51Table 7: Results for combining the Spanish ?
En-glish and German ?
English phrase-based systemson the in-domain tasks.6 ConclusionWe described the ISL phrase-based statistical ma-chine translation systems that were used for the 2007ACL Workshop on Statistical Machine Translation.Using the available out-of-domain News Commen-tary task training data for language model adapta-tion, we were able to significantly increase the per-formance on the out-of-domain task by 2.3 BLEUfor English ?
Spanish and by 1.3 BLEU for Span-ish ?
English.
We also showed the feasibility ofcombining different MT systems by combining andrescoring their resprective n-best lists.
In particular,we focused on the combination of our phrase-basedand syntax-augmented systems and the combinationof two phrase-based systems operating on differentsource languages.
While we saw only a minimal im-provement of 0.1 BLEU for the phrase-based andsyntax-augmented combination, we gained up to 1BLEU, in case of the multi-source translation.ReferencesG.
Foster, R. Kuhn, and H. Johnson.
2006.
PhrasetableSmoothing for Statistical Machine Translation.
InProc.
of Empirical Methods in Natural Language Pro-cessing, Sydney, Australia.J.
M. Crego et al 2006.
N-gram-based SMT SystemEnhanced with Reordering Patterns.
In Proc.
of theWorkshop on Statistical Machine Translation, pages162?165, New York, USA.P.
Koehn and K. Knight.
2003.
Empirical Methods forCompound Splitting.
In Proc.
of the tenth conferenceon European chapter of the Association for Computa-tional Linguistics, pages 187?193, Budapest, Hungary.P.
Koehn and C. Monz.
2006.
Manual and AutomaticEvaluation of Machine Translation between EuropeanLangauges.
In Proc.
of the Workshop on Statisti-cal Machine Translation, pages 102?121, New York,USA.P.
Koehn.
2005.
Europarl: A Parallel Corpus for Statis-tical Machine Translation.
In Proc.
of Machine Trans-lation Summit.F.J.
Och and H. Ney.
2000.
Improved Statistical Align-ment Models.
In Proc.
of the 38th Annual Meet-ing of the Association for Computational Linguistics,Hongkong, China.F.
J. Och and H. Ney.
2001.
Statistical Multi-SourceTranslation.
In Proc.
of Machine Translation Summit,pages 253?258, Santiago de Compostela, Spain.F.
J. Och.
2003.
Minimum Error Rate Training in Statis-tical Machine Translation.
In Proc.
of the 41st AnnualMeeting of the Association for Computational Linguis-tics, pages 160 ?
167, Sapporo, Japan.M.
Paulik, S. Stueker, C. Fuegen, T. Schultz, T. Schaaf,and A. Waibel.
2005.
Speech Translation EnhancedAutomatic Speech Recognition.
In Proc.
of the Work-shop on Automatic Speech Recognition and Under-standing, San Juan, Puerto Rico.TC-STAR.
2004.
Technology and Corpora for Speech toSpeech Translation.
http://www.tc-star.org.N.
Ueffing and H. Ney.
2005.
Word-Level Con-fidence Estimation for Machine Translation usingPhrase-Based Translation Models.
In Proc.
of HLTand EMNLP, pages 763?770, Vancouver, BritishColumbia, Canada.S.
Vogel.
2003.
SMT Decoder Dissected: Word Re-ordering.
In Proc.
of Int.
Conf.
on Natural Lan-guage Processing and Knowledge Engineering, Bei-jing, China.R.
Zens and H. Ney.
2006.
N-gram Posterior Proba-bilities for Statistical Machine Translation.
In Proc.of the Workshop on Statistical Machine Translation,pages 72?77, New York, USA.Y.
Zhang and S. Vogel.
2006.
Suffix Array and its Ap-plications in Empirical Natural Language Processing.In the Technical Report CMU-LTI-06-010, Pittsburgh,USA.A.
Zollmann, A. Venugopal, M. Paulik, and S. Vogel.2007.
The Syntax Augmented MT (SAMT) systemat the Shared Task for the 2007 ACL Workshop onStatistical Machine Translation.
In Proc.
of ACL 2007Workshop on Statistical MachineTranslation, Prague,Czech Republic.202
