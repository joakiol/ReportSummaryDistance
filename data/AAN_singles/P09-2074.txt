Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 293?296,Suntec, Singapore, 4 August 2009.c?2009 ACL and AFNLPMarkov Random Topic FieldsHal Daum?e IIISchool of ComputingUniversity of UtahSalt Lake City, UT 84112me@hal3.nameAbstractMost approaches to topic modeling as-sume an independence between docu-ments that is frequently violated.
Wepresent an topic model that makes useof one or more user-specified graphs de-scribing relationships between documents.These graph are encoded in the form of aMarkov random field over topics and serveto encourage related documents to havesimilar topic structures.
Experiments onshow upwards of a 10% improvement inmodeling performance.1 IntroductionOne often wishes to apply topic models to largedocument collections.
In these large collections,we usually have meta-information about how onedocument relates to another.
Perhaps two docu-ments share an author; perhaps one document citesanother; perhaps two documents are published inthe same journal or conference.
We often believethat documents related in such a way should havesimilar topical structures.
We encode this in aprobabilistic fashion by imposing an (undirected)Markov random field (MRF) on top of a standardtopic model (see Section 3).
The edge potentialsin the MRF encode the fact that ?connected?
doc-uments should share similar topic structures, mea-sured by some parameterized distance function.Inference in the resulting model is complicatedby the addition of edge potentials in the MRF.We demonstrate that a hybrid Gibbs/Metropolis-Hastings sampler is able to efficiently explore theposterior distribution (see Section 4).In experiments (Section 5), we explore severalvariations on our basic model.
The first is to ex-plore the importance of being able to tune thestrength of the potentials in the MRF as part of theinference procedure.
This turns out to be of utmostimportance.
The second is to study the importanceof the form of the distance metric used to specifythe edge potentials.
Again, this has a significantimpact on performance.
Finally, we consider theuse of multiple graphs for a single model and findthat the power of combined graphs also leads tosignificantly better models.2 BackgroundProbabilistic topic models propose that text canbe considered as a mixture of words drawn fromone or more ?topics?
(Deerwester et al, 1990;Blei et al, 2003).
The model we build on is la-tent Dirichlet alocation (Blei et al, 2003) (hence-forth, LDA).
LDA stipulates the following gener-ative model for a document collection:1.
For each document d = 1 .
.
.
D:(a) Choose a topic mixture ?d?
Dir(?
)(b) For each word in d, n = 1 .
.
.
Nd:i.
Choose a topic zdn?Mult(?d)ii.
Choose a word wdn?Mult(?zdn)Here, ?
is a hyperparameter vector of length K,where K is the desired number of topics.
Eachdocument has a topic distribution ?dover theseK topics and each word is associated with pre-cisely one topic (indicated by zdn).
Each topick = 1 .
.
.K is a unigram distribution over words(aka, a multinomial) parameterized by a vector?k.
The associated graphical model for LDA isshown in Figure 1.
Here, we have added a fewadditional hyperparameters: we place a Gam(a, b)prior independently on each component of ?
anda Dir(?, .
.
.
, ?)
prior on each of the ?s.The joint distribution over all random variablesspecified by LDA is:p(?,?,z,?,w) =YkGam(?k| a, b)Dir(?k| ?)
(1)YdDir(?d| ?
)YnMult(zdn| ?d)Mult(wdn| ?zdn)Many inference methods have been developedfor this model; the approach upon which we293wz?
?ND?Kab?Figure 1: Graphical model for LDA.build is the collapsed Gibbs sampler (Griffiths andSteyvers, 2006).
Here, the random variables ?
and?
are analytically integrated out.
The main sam-pling variables are the zdnindicators (as well asthe hyperparameters: ?
and a, b).
The conditionaldistribution for zdnconditioned on all other vari-ables in the model gives the following Gibbs sam-pling distribution p(zdn= k):#?dnz=k+ ?kPk?
(#?dnz=k?+ ?k?
)#?dnz=k,w=wdn+ ?Pk?
(#?dnz=k?,w=wdn+ ?
)(2)Here, #?dn?denotes the number of times event?
occurs in the entire corpus, excluding word nin document d. Intuitively, the first term is a(smoothed) relative frequency of topic k occur-ring; the second term is a (smoothed) relative fre-quency of topic k giving rise to word wdn.A Markov random field specifies a joint dis-tribution over a collection of random variablesx1, .
.
.
, xN.
An undirected graph structure stip-ulates how the joint distribution factorizes overthese variables.
Given a graph G = (V,E), whereV = {x1, .
.
.
, xN}, let C denote a subset of allthe cliques of G. Then, the MRF specifies the jointdistribution as: p(x) =1Z?c?C?c(xc).
Here,Z =?x?c?C?c(xc) is the partition function,xcis the subset of x contained in clique c and ?cis any non-negative function that measures how?good?
a particular configuration of variables xcis.
The ?s are called potential functions.3 Markov Random Topic FieldsSuppose that we have access to a collection ofdocuments, but do not believe that these docu-ments are all independent.
In this case, the gener-ative story of LDA no longer makes sense: relateddocuments are more likely to have ?similar?
topicstructures.
For instance, in the scientific commu-nity, if paper A cites paper B, we would (a priori)expect the topic distributions for papers A and Bto be related.
Similarly, if two papers share an au-thor, we might expect them to be topically related.Doc 1 Doc 2Doc 3Doc 4Doc 5Doc 6wz?Nwz?Nwz?Nwz?Nwz?Nwz?NFigure 2: Example Markov Random Topic Field (variables?
and ?
are excluded for clarify).Of if they are both published at EMNLP.
Or if theyare published in the same year, or come out of thesame institution, or many other possibilities.Regardless of the source of this notion of simi-larity, we suppose that we can represent the rela-tionship between documents in the form of a graphG = (V,E).
The vertices in this graph are the doc-uments and the edges indicate relatedness.
Notethat the resulting model will not be fully genera-tive, but is still probabilistically well defined.3.1 Single GraphThere are multiple possibilities for augmentingLDA with such graph structure.
We could ?link?the topic distributions ?
over related documents;we could ?like?
the topic indicators z over relateddocuments.
We consider the former because itleads to a more natural model.
The idea is to ?un-roll?
the D-plate in the graphical model for LDA(Figure 1) and connect (via undirected links) the?
variables associated with connected documents.Figure 2 shows an example MRTF over six docu-ments, with thick edges connecting the ?
variablesof ?related?
documents.
Note that each ?
still has?
as a parent and each w has ?
as a parent: theseare left off for figure clarity.The model is a straightforward ?integration?
ofLDA and an MRF specified by the document re-lationships G. We begin with the joint distributionspecified by LDA (see Eq (1)) and add in edge po-tentials for each edge in the document graph G that?encourage?
the topic distributions of neighboringdocuments to be similar.
The potentials all havethe form:?d,d?(?d,?d?)
= exp[?`d,d??(?d,?d?
)](3)Here, `d,d?is a ?measure of strength?
of the im-portance of the connection between d and d?
(andwill be inferred as part of the model).
?
is a dis-tance metric measuring the dissimilarity between?dand ?d?.
For now, this is Euclidean distance294(i.e., ?(?d,?d?)
= ||?d?
?d?||); later, we showthat alternative distance metrics are preferable.Adding the graph structure necessitates the ad-dition of hyperparameters `efor every edge e ?
E.We place an exponential prior on each 1/`ewithparameter ?
: p(`e| ?)
= ?
exp(??/`e).
Finally,we place a vague Gam(?a, ?b) prior on ?.3.2 Multiple GraphsIn many applications, there may be multiplegraphs that apply to the same data set, G1, .
.
.
,GJ.In this case, we construct a single MRF based onthe union of these graph structures.
Each edge nowhas L-many parameters (one for each graph j) `je.Each graph also has its own exponential prior pa-rameter ?j.
Together, this yields:?d,d?(?d,?d?)
= exp[??j`jd,d??(?d,?d?
)](4)Here, the sum ranges only over those graphsthat have (d, d?)
in their edge set.4 InferenceInference in MRTFs is somewhat complicatedfrom inference in LDA, due to the introductionof the additional potential functions.
In partic-ular, while it is possible to analytically integrateout ?
in LDA (due to multinomial/Dirichlet con-jugacy), this is no longer possible in MRTFs.
Thismeans that we must explicitly represent (and sam-ple over) the topic distributions ?
in the MRTF.This means that we must sample over the fol-lowing set of variables: ?, ?, z, ` and ?.
Sam-pling for ?
remains unchanged from the LDAcase.
Sampling for variables except ?
is easy:zdn= k : ?dk#?dnz=k,w=wdn+ ??k?
(#?dnz=k?,w=wdn+ ?)(5)1/`d,d??
Exp(?+ ?(?d,?d?))(6)?
?
Gam(?a+ |E| , ?b+?e`e)(7)The latter two follow from simple conjugacy.When we use multiple graphs, we assign a sepa-rate ?
for each graph.For sampling ?, we resort to a Metropolis-Hastings step.
Our proposal distribution is theDirichlet posterior over ?, given all the current as-signments.
The acceptance probability then justdepends on the graph distances.
In particular,once ?dis drawn from the posterior Dirichlet, theacceptance probability becomes?d?
?N (d)?d,d?,where N (d) denotes the neighbors of d. For each0 200 400 600 8008090100110120130140# of iterationsperplexityauthbookcitehttptime*none*yearFigure 3: Held-out perplexity for different graphs.document, we run 10 Metropolis steps; the accep-tance rates are roughly 25%.5 ExperimentsOur experiments are on a collection for 7441 doc-ument abstracts crawled from CiteSeer.
The crawlwas seeded with a collection of ten documentsfrom each of: ACL, EMNLP, SIGIR, ICML,NIPS, UAI.
This yields 650 thousand words of textafter remove stop words.
We use the followinggraphs (number in parens is the number of edges):auth: shared author (47k)book: shared booktitle/journal (227k)cite: one cites the other (18k)http: source file from same domain (147k)time: published within one year (4122k)year: published in the same year (2101k)Other graph structures are of course possible, butthese were the most straightforward to cull.The first thing we look at is convergence ofthe samplers for the different graphs.
See Fig-ure 3.
Here, we can see that the author graph andthe citation graph provide improved perplexity tothe straightforward LDA model (called ?*none*?
),and that convergence occurs in a few hundred iter-ations.
Due to their size, the final two graphs ledto significantly slower inference than the first four,so results with those graphs are incomplete.Tuning Graph Parameters.
The next item weinvestigate is whether it is important to tune thegraph connectivity weights (the ` and ?
variables).It turns out this is incredibly important; see Fig-ure 4.
This is the same set of results as Figure 3,but without ` and ?
tuning.
We see that the graph-based methods do not improve over the baseline.2950 200 400 600 8008090100110120130140# of iterationsperplexityauthbookcitehttp*none*timeyearFigure 4: Held-out perplexity for difference graph struc-tures without graph parameter tuning.0 200 400 600 8008090100110120130140# of iterationsperplexityBhattacharyyaHellingerEuclideanLogitFigure 5: Held-out perplexity for different distance metrics.DistanceMetric.
Next, we investigate the use ofdifferent distance metrics.
We experiments withBhattacharyya, Hellinger, Euclidean and logistic-Euclidean.
See Figure 5 (this is just for the authgraph).
Here, we see that Bhattacharyya andHellinger (well motivated distances for probabilitydistributions) outperform the Euclidean metrics.Using Multiple Graphs Finally, we compareresults using combinations of graphs.
Here, werun every sampler for 500 iterations and computestandard deviations based on ten runs (year andtime are excluded).
The results are in Table 1.Here, we can see that adding graphs (almost) al-ways helps and never hurts.
By adding all thegraphs together, we are able to achieve an abso-lute reduction in perplexity of 9 points (roughly10%).
As discussed, this hinges on the tuning ofthe graph parameters to allow different graphs tohave different amounts of influence.6 DiscussionWe have presented a graph-augmented model fortopic models and shown that a simple combinedGibbs/MH sampler is efficient in these models.
*none*92.1http92.2book90.2cite88.4auth87.9book+http89.9cite+http88.6auth+http88.0book+cite86.9auth+book85.1auth+cite84.3book+cite+http87.9auth+cite+http85.5auth+book+http85.3auth+book+cite83.7all83.1Table 1: Comparison of held-out perplexities for vary-ing graph structures with two standard deviation error bars;grouped by number of graphs.
Grey bars are indistinguish-able from best model in previous group; blue bars are at leasttwo stddevs better; red bars are at least four stddevs better.Using data from the scientific domain, we haveshown that we can achieve significant reductionsin perplexity on held-out data using these mod-els.
Our model resembles recent work on hyper-text topic models (Gruber et al, 2008; Sun et al,2008) and blog influence (Nallapati and Cohen,2008), but is specifically tailored toward undi-rected models.
Ours is an alternative to the re-cently proposed Markov Topic Models approach(Wang et al, 2009).
While the goal of these twomodels is similar, the approaches differ fairly dra-matically: we use the graph structure to informthe per-document topic distributions; they use thegraph structure to inform the unigram models as-sociated with each topic.
It would be worthwhileto directly compare these two approaches.ReferencesDavid Blei, Andrew Ng, and Michael Jordan.
2003.
LatentDirichlet alocation.
JMLR, 3.Scott C. Deerwester, Susan T. Dumais, Thomas K. Landauer,George W. Furnas, and Richard A. Harshman.
1990.
In-dexing by latent semantic analysis.
JASIS, 41(6).Tom Griffiths and Mark Steyvers.
2006.
Probabilistic topicmodels.
In Latent Semantic Analysis: A Road to Meaning.Amit Gruber, Michal Rosen-Zvi, , and Yair Weiss.
2008.Latent topic models for hypertext.
In UAI.Ramesh Nallapati and William Cohen.
2008.
Link-PLSA-LDA: A new unsupervised model for topics and influenceof blogs.
In Conference for Webblogs and Social Media.Congkai Sun, Bin Gao, Zhenfu Cao, and Hang Li.
2008.HTM: A topic model for hypertexts.
In EMNLP.Chong Wang, Bo Thiesson, Christopher Meek, and DavidBlei.
2009.
Markov topic models.
In AI-Stats.296
