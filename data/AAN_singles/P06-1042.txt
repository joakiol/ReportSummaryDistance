Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 329?336,Sydney, July 2006. c?2006 Association for Computational LinguisticsError mining in parsing resultsBeno?t Sagot and ?ric de la ClergerieProjet ATOLL - INRIADomaine de Voluceau, B.P.
10578153 Le Chesnay Cedex, France{benoit.sagot,eric.de_la_clergerie}@inria.frAbstractWe introduce an error mining techniquefor automatically detecting errors in re-sources that are used in parsing systems.We applied this technique on parsing re-sults produced on several million words bytwo distinct parsing systems, which sharethe syntactic lexicon and the pre-parsingprocessing chain.
We were thus able toidentify missing and erroneous informa-tion in these resources.1 IntroductionNatural language parsing is a hard task, partly be-cause of the complexity and the volume of infor-mation that have to be taken into account aboutwords and syntactic constructions.
However, itis necessary to have access to such information,stored in resources such as lexica and grammars,and to try and minimize the amount of missingand erroneous information in these resources.
Toachieve this, the use of these resources at a large-scale in parsers is a very promising approach (vanNoord, 2004), and in particular the analysis of sit-uations that lead to a parsing failure: one can learnfrom one?s own mistakes.We introduce a probabilistic model that allowsto identify forms and form bigrams that may bethe source of errors, thanks to a corpus of parsedsentences.
In order to facilitate the exploitation offorms and form bigrams detected by the model,and in particular to identify causes of errors, wehave developed a visualization environment.
Thewhole system has been tested on parsing resultsproduced for several multi-million-word corporaand with two different parsers for French, namelySXLFG and FRMG.However, the error mining technique whichis the topic of this paper is fully system- andlanguage-independent.
It could be applied with-out any change on parsing results produced by anysystem working on any language.
The only infor-mation that is needed is a boolean value for eachsentence which indicates if it has been success-fully parsed or not.2 Principles2.1 General ideaThe idea we implemented is inspired from (vanNoord, 2004).
In order to identify missing and er-roneous information in a parsing system, one cananalyze a large corpus and study with statisticaltools what differentiates sentences for which pars-ing succeeded from sentences for which it failed.The simplest application of this idea is to lookfor forms, called suspicious forms, that are foundmore frequently in sentences that could not beparsed.
This is what van Noord (2004) does, with-out trying to identify a suspicious form in any sen-tence whose parsing failed, and thus without tak-ing into account the fact that there is (at least)one cause of error in each unparsable sentence.1On the contrary, we will look, in each sentenceon which parsing failed, for the form that hasthe highest probability of being the cause of thisfailure: it is the main suspect of the sentence.This form may be incorrectly or only partially de-scribed in the lexicon, it may take part in construc-tions that are not described in the grammar, or itmay exemplify imperfections of the pre-syntacticprocessing chain.
This idea can be easily extendedto sequences of forms, which is what we do by tak-1Indeed, he defines the suspicion rate of a form f as therate of unparsable sentences among sentences that contain f .329ing form bigrams into account, but also to lemmas(or sequences of lemmas).2.2 Form-level probabilistic modelWe suppose that the corpus is split in sentences,sentences being segmented in forms.
We denoteby si the i-th sentence.
We denote by oi,j, (1 ?j ?
|si|) the occurrences of forms that constitutesi, and by F (oi,j) the corresponding forms.
Fi-nally, we call error the function that associates toeach sentence si either 1, if si?s parsing failed, and0 if it succeeded.Let Of be the set of the occurrences of a formf in the corpus: Of = {oi,j|F (oi,j) = f}.
Thenumber of occurrences of f in the corpus is there-fore |Of |.Let us define at first the mean global suspicionrate S, that is the mean probability that a given oc-currence of a form be the cause of a parsing fail-ure.
We make the assumption that the failure ofthe parsing of a sentence has a unique cause (here,a unique form.
.
.
).
This assumption, which is notnecessarily exactly verified, simplifies the modeland leads to good results.
If we call occtotal thetotal amount of forms in the corpus, we have then:S = ?ierror(si)occtotalLet f be a form, that occurs as the j-th form ofsentence si, which means that F (oi,j) = f .
Let usassume that si?s parsing failed: error(si) = 1.
Wecall suspicion rate of the j-th form oi,j of sentencesi the probability, denoted by Si,j , that the occur-rence oi,j of form form f be the cause of the si?sparsing failure.
If, on the contrary, si?s parsingsucceeded, its occurrences have a suspicion ratethat is equal to zero.We then define the mean suspicion rate Sf ofa form f as the mean of all suspicion rates of itsoccurrences:Sf =1|Of |?
?oi,j?OfSi,jTo compute these rates, we use a fix-point al-gorithm by iterating a certain amount of times thefollowing computations.
Let us assume that wejust completed the n-th iteration: we know, foreach sentence si, and for each occurrence oi,j ofthis sentence, the estimation of its suspicion rateSi,j as computed by the n-th iteration, estimationthat is denoted by S(n)i,j .
From this estimation, wecompute the n + 1-th estimation of the mean sus-picion rate of each form f , denoted by S(n+1)f :S(n+1)f =1|Of |?
?oi,j?OfS(n)i,jThis rate2 allows us to compute a new estima-tion of the suspicion rate of all occurrences, bygiving to each occurrence if a sentence si a sus-picion rate S(n+1)i,j that is exactly the estimationS(n+1)f of the mean suspicion rate of Sf of the cor-responding form, and then to perform a sentence-level normalization.
Thus:S(n+1)i,j = error(si) ?S(n+1)F (oi,j)?1?j?|si| S(n+1)F (oi,j)At this point, the n+1-th iteration is completed,and we can resume again these computations, un-til convergence on a fix-point.
To begin the wholeprocess, we just say, for an occurrence oi,j of sen-tence si, that S(0)i,j = error(si)/|si|.
This meansthat for a non-parsable sentence, we start from abaseline where all of its occurrences have an equalprobability of being the cause of the failure.After a few dozens of iterations, we get stabi-lized estimations of the mean suspicion rate eachform, which allows:?
to identify the forms that most probably causeerrors,?
for each form f , to identify non-parsable sen-tences si where an occurrence oi,j ?
Of of fis a main suspect and where oi,j has a very2We also performed experiment in which Sf was esti-mated by an other estimator, namely the smoothed mean sus-picion rate, denoted by S?
(n)f , that takes into account the num-ber of occurrences of f .
Indeed, the confidence we can havein the estimation S(n)f is lower if the number of occurrencesof f is lower.
Hence the idea to smooth S(n)f by replacing itwith a weighted mean S?
(n)f between S(n)f and S, where theweights ?
and 1 ?
?
depend on |Of |: if |Of | is high, S?
(n)fwill be close from S(n)f ; if it is low, it will be closer from S:S?
(n)f = ?
(|Of |) ?
S(n)f + (1 ?
?
(|Of |)) ?
S.In these experiments, we used the smoothing function?
(|Of |) = 1 ?
e?
?|Of | with ?
= 0.1.
But this model,used with the ranking according to Mf = Sf ?
ln |Of | (seebelow), leads results that are very similar to those obtainedwithout smoothing.
Therefore, we describe the smoothing-less model, which has the advantage not to use an empiricallychosen smoothing function.330high suspicion rate among all occurrences ofform f .We implemented this algorithm as a perl script,with strong optimizations of data structures so asto reduce memory and time usage.
In particu-lar, form-level structures are shared between sen-tences.2.3 Extensions of the modelThis model gives already very good results, as weshall see in section 4.
However, it can be extendedin different ways, some of which we already im-plemented.First of all, it is possible not to stick to forms.Indeed, we do not only work on forms, but on cou-ples made out of a form (a lexical entry) and oneor several token(s) that correspond to this form inthe raw text (a token is a portion of text delimitedby spaces or punctuation tokens).Moreover, one can look for the cause of the fail-ure of the parsing of a sentence not only in thepresence of a form in this sentence, but also in thepresence of a bigram3 of forms.
To perform this,one just needs to extend the notions of form andoccurrence, by saying that a (generalized) form isa unigram or a bigram of forms, and that a (gen-eralized) occurrence is an occurrence of a gener-alized form, i.e., an occurrence of a unigram or abigram of forms.
The results we present in sec-tion 4 includes this extension, as well as the previ-ous one.Another possible generalization would be totake into account facts about the sentence that arenot simultaneous (such as form unigrams and formbigrams) but mutually exclusive, and that musttherefore be probabilized as well.
We have not yetimplemented such a mechanism, but it would bevery interesting, because it would allow to go be-yond forms or n-grams of forms, and to manipu-late also lemmas (since a given form has usuallyseveral possible lemmas).3 ExperimentsIn order to validate our approach, we appliedthese principles to look for error causes in pars-ing results given by two deep parsing systems forFrench, FRMG and SXLFG, on large corpora.3One could generalize this to n-grams, but as n getshigher the number of occurrences of n-grams gets lower,hence leading to non-significant statistics.3.1 ParsersBoth parsing systems we used are based on deepnon-probabilistic parsers.
They share:?
the Lefff 2 syntactic lexicon for French(Sagot et al, 2005), that contains 500,000 en-tries (representing 400,000 different forms) ;each lexical entry contains morphological in-formation, sub-categorization frames (whenrelevant), and complementary syntactic infor-mation, in particular for verbal forms (con-trols, attributives, impersonals,.
.
.
),?
the SXPipe pre-syntactic processing chain(Sagot and Boullier, 2005), that converts araw text in a sequence of DAGs of forms thatare present in the Lefff ; SXPipe contains,among other modules, a sentence-level seg-menter, a tokenization and spelling-error cor-rection module, named-entities recognizers,and a non-deterministic multi-word identifier.But FRMG and SXLFG use completely differentparsers, that rely on different formalisms, on dif-ferent grammars and on different parser builder.Therefore, the comparison of error mining resultson the output of these two systems makes it possi-ble to distinguish errors coming from the Lefff orfrom SXPipe from those coming to one grammaror the other.
Let us describe in more details thecharacteristics of these two parsers.The FRMG parser (Thomasset and Villemontede la Clergerie, 2005) is based on a compact TAGfor French that is automatically generated froma meta-grammar.
The compilation and executionof the parser is performed in the framework ofthe DYALOG system (Villemonte de la Clergerie,2005).The SXLFG parser (Boullier and Sagot, 2005b;Boullier and Sagot, 2005a) is an efficient and ro-bust LFG parser.
Parsing is performed in twosteps.
First, an Earley-like parser builds a sharedforest that represents all constituent structures thatsatisfy the context-free skeleton of the grammar.Then functional structures are built, in one or morebottom-up passes.
Parsing efficiency is achievedthanks to several techniques such as compact datarepresentation, systematic use of structure andcomputation sharing, lazy evaluation and heuristicand almost non-destructive pruning during pars-ing.Both parsers implement also advanced error re-covery and tolerance techniques, but they were331corpus #sentences #success (%) #forms #occ S (%) DateMD/FRMG 330,938 136,885 (41.30%) 255,616 10,422,926 1.86% Jul.
05MD/SXLFG 567,039 343,988 (60.66%) 327,785 14,482,059 1.54% Mar.
05EASy/FRMG 39,872 16,477 (41.32%) 61,135 878,156 2.66% Dec. 05EASy/SXLFG 39,872 21,067 (52.84%) 61,135 878,156 2.15% Dec. 05Table 1: General information on corpora and parsing resultsuseless for the experiments described here, sincewe want only to distinguish sentences that receivea full parse (without any recovery technique) fromthose that do not.3.2 CorporaWe parsed with these two systems the followingcorpora:MD corpus : This corpus is made out of 14.5million words (570,000 sentences) of generaljournalistic corpus that are articles from theMonde diplomatique.EASy corpus : This is the 40,000-sentence cor-pus that has been built for the EASy parsingevaluation campaign for French (Paroubek etal., 2005).
We only used the raw corpus(without taking into account the fact that amanual parse is available for 10% of all sen-tences).
The EASy corpus contains severalsub-corpora of varied style: journalistic, lit-eracy, legal, medical, transcription of oral, e-mail, questions, etc.Both corpora are raw in the sense that no clean-ing whatsoever has been performed so as to elimi-nate some sequences of characters that can not re-ally be considered as sentences.Table 1 gives some general information on thesecorpora as well as the results we got with bothparsing systems.
It shall be noticed that bothparsers did not parse exactly the same set and thesame number of sentences for the MD corpus, andthat they do not define in the exactly same way thenotion of sentence.3.3 Results visualization environmentWe developed a visualization tool for the results ofthe error mining, that allows to examine and an-notate them.
It has the form of an HTML pagethat uses dynamic generation methods, in particu-lar javascript.
An example is shown on Figure 1.To achieve this, suspicious forms are ranked ac-cording to a measure Mf that models, for a givenform f , the benefit there is to try and correct the(potential) corresponding error in the resources.
Auser who wants to concentrate on almost certainerrors rather than on most frequent ones can visu-alize suspicious forms ranked according to Mf =Sf .
On the contrary, a user who wants to concen-trate on most frequent potential errors, rather thanon the confidence that the algorithm has given toerrors, can visualize suspicious forms ranked ac-cording to4 Mf = Sf |Of |.
The default choice,which is adopted to produce all tables shown inthis paper, is a balance between these two possi-bilities, and ranks suspicious forms according toMf = Sf ?
ln |Of |.The visualization environment allows to browsethrough (ranked) suspicious forms in a scrollinglist on the left part of the page (A).
When the suspi-cious form is associated to a token that is the sameas the form, only the form is shown.
Otherwise,the token is separated from the form by the sym-bol ?
/ ?.
The right part of the page shows variouspieces of information about the currently selectedform.
After having given its rank according to theranking measure Mf that has been chosen (B), afield is available to add or edit an annotation as-sociated with the suspicious form (D).
These an-notations, aimed to ease the analysis of the errormining results by linguists and by the developersof parsers and resources (lexica, grammars), aresaved in a database (SQLITE).
Statistical informa-tion is also given about f (E), including its numberof occurrences occf , the number of occurrences off in non-parsable sentences, the final estimationof its mean suspicion rate Sf and the rate err(f)of non-parsable sentences among those where fappears.
This indications are complemented by abrief summary of the iterative process that showsthe convergence of the successive estimations ofSf .
The lower part of the page gives a mean toidentify the cause of f -related errors by showing4Let f be a form.
The suspicion rate Sf can be consideredas the probability for a particular occurrence of f to causea parsing error.
Therefore, Sf |Of | models the number ofoccurrences of f that do cause a parsing error.332ABCDEFGHFigure 1: Error mining results visualization environment (results are shown for MD/FRMG).f ?s entries in the Lefff lexicon (G) as well as non-parsable sentences where f is the main suspectand where one of its occurrences has a particularlyhigh suspicion rate5 (H).The whole page (with annotations) can be sentby e-mail, for example to the developer of the lex-icon or to the developer of one parser or the other(C).4 ResultsIn this section, we mostly focus on the results ofour error mining algorithm on the parsing resultsprovided by SXLFG on the MD corpus.
We firstpresent results when only forms are taken into ac-count, and then give an insight on results whenboth forms and form bigrams are considered.5Such an information, which is extremely valuable for thedevelopers of the resources, can not be obtained by global(form-level and not occurrence-level) approaches such as theerr(f)-based approach of (van Noord, 2004).
Indeed, enu-merating all sentences which include a given form f , andwhich did not receive a full parse, is not precise enough:it would show at the same time sentences wich fail be-cause of f (e.g., because its lexical entry lacks a given sub-categorization frame) and sentences which fail for an otherindependent reason.4.1 Finding suspicious formsThe execution of our error mining script onMD/SXLFG, with imax = 50 iterations and whenonly (isolated) forms are taken into account, takesless than one hour on a 3.2 GHz PC runningLinux with a 1.5 Go RAM.
It outputs 18,334 rele-vant suspicious forms (out of the 327,785 possibleones), where a relevant suspicious form is definedas a form f that satisfies the following arbitraryconstraints:6 S(imax)f > 1, 5 ?
S and |Of | > 5.We still can not prove theoretically the conver-gence of the algorithm.7 But among the 1000 best-ranked forms, the last iteration induces a meanvariation of the suspicion rate that is less than0.01%.On a smaller corpus like the EASy corpus, 200iterations take 260s.
The algorithm outputs lessthan 3,000 relevant suspicious forms (out of the61,125 possible ones).
Convergence information6These constraints filter results, but all forms are takeninto account during all iterations of the algorithm.7However, the algorithms shares many common pointswith iterative algorithm that are known to converge and thathave been proposed to find maximum entropy probability dis-tributions under a set of constraints (Berger et al, 1996).Such an algorithm is compared to ours later on in this paper.333is the same as what has been said above for theMD corpus.Table 2 gives an idea of the repartition of sus-picious forms w.r.t.
their frequency (for FRMG onMD), showing that rare forms have a greater prob-ability to be suspicious.
The most frequent suspi-cious form is the double-quote, with (only) Sf =9%, partly because of segmentation problems.4.2 Analyzing resultsTable 3 gives an insight on the output of our algo-rithm on parsing results obtained by SXLFG on theMD corpus.
For each form f (in fact, for each cou-ple of the form (token,form)), this table displays itssuspicion rate and its number of occurrences, aswell as the rate err(f) of non-parsable sentencesamong those where f appears and a short manualanalysis of the underlying error.In fact, a more in-depth manual analysis of theresults shows that they are very good: errors arecorrectly identified, that can be associated withfour error sources: (1) the Lefff lexicon, (2) theSXPipe pre-syntactic processing chain, (3) imper-fections of the grammar, but also (4) problems re-lated to the corpus itself (and to the fact that itis a raw corpus, with meta-data and typographicnoise).On the EASy corpus, results are also relevant,but sometimes more difficult to interpret, becauseof the relative small size of the corpus and becauseof its heterogeneity.
In particular, it contains e-mail and oral transcriptions sub-corpora that in-troduce a lot of noise.
Segmentation problems(caused both by SXPipe and by the corpus itself,which is already segmented) play an especiallyimportant role.4.3 Comparing results with results of otheralgorithmsIn order to validate our approach, we comparedour results with results given by two other relevantalgorithms:?
van Noord?s (van Noord, 2004) (form-leveland non-iterative) evaluation of err(f) (therate of non-parsable sentences among sen-tences containing the form f ),?
a standard (occurrence-level and iterative)maximum entropy evaluation of each form?scontribution to the success or the failure ofa sentence (we used the MEGAM package(Daum?
III, 2004)).As done for our algorithm, we do not rank formsdirectly according to the suspicion rate Sf com-puted by these algorithms.
Instead, we use the Mfmeasure presented above (Mf = Sf ?ln |Of |).
Us-ing directly van Noord?s measure selects as mostsuspicious words very rare words, which showsthe importance of a good balance between suspi-cion rate and frequency (as noted by (van Noord,2004) in the discussion of his results).
This remarkapplies to the maximum entropy measure as well.Table 4 shows for all algorithms the 10 best-ranked suspicious forms, complemented by a man-ual evaluation of their relevance.
One clearly seesthat our approach leads to the best results.
VanNoord?s technique has been initially designed tofind errors in resources that already ensured a veryhigh coverage.
On our systems, whose develop-ment is less advanced, this technique ranks as mostsuspicious forms those which are simply the mostfrequent ones.
It seems to be the case for the stan-dard maximum entropy algorithm, thus showingthe importance to take into account the fact thatthere is at least one cause of error in any sentencewhose parsing failed, not only to identify a mainsuspicious form in each sentence, but also to getrelevant global results.4.4 Comparing results for both parsersWe complemented the separated study of errormining results on the output of both parsers byan analysis of merged results.
We computed foreach form the harmonic mean of both measuresMf = Sf ?
ln |Of | obtained for each parsing sys-tem.
Results (not shown here) are very interest-ing, because they identify errors that come mostlyfrom resources that are shared by both systems(the Lefff lexicon and the pre-syntactic processingchain SXPipe).
Although some errors come fromcommon lacks of coverage in both grammars, itis nevertheless a very efficient mean to get a firstrepartition between error sources.4.5 Introducing form bigramsAs said before, we also performed experimentswhere not only forms but also form bigrams aretreated as potential causes of errors.
This approachallows to identify situations where a form is not initself a relevant cause of error, but leads often toa parse failure when immediately followed or pre-ceded by an other form.Table 5 shows best-ranked form bigrams (formsthat are ranked in-between are not shown, to em-334#occ > 100 000 > 10 000 > 1000 > 100 > 10#forms 13 84 947 8345 40 393#suspicious forms (%) 1 (7.6%) 13 (15.5%) 177 (18.7%) 1919 (23%) 12 022 (29.8%)Table 2: Suspicious forms repartition for MD/FRMGRank Token(s)/form S(50)f |Of | err(f) Mf Error cause1 _____/_UNDERSCORE 100% 6399 100% 8.76 corpus: typographic noise2 (...) 46% 2168 67% 2.82 SXPipe: should be treated as skippable words3 2_]/_NUMBER 76% 30 93% 2.58 SXPipe: bad treatment of list constructs4 priv?es 39% 589 87% 2.53 Lefff : misses as an adjective5 Haaretz/_Uw 51% 149 70% 2.53 SXPipe: needs local grammars for references6 contest?
52% 122 90% 2.52 Lefff : misses as an adjective7 occup?s 38% 601 86% 2.42 Lefff : misses as an adjective8 priv?e 35% 834 82% 2.38 Lefff : misses as an adjective9 [...] 44% 193 71% 2.33 SXPipe: should be treated as skippable words10 faudrait 36% 603 85% 2.32 Lefff : can have a nominal objectTable 3: Analysis of the 10 best-ranked forms (ranked according to Mf = Sf ?
ln |Of |)this paper global maxentRank Token(s)/form Eval Token(s)/form Eval Token(s)/form Eval1 _____/_UNDERSCORE ++ * + pour -2 (...) ++ , - ) -3 2_]/_NUMBER ++ livre - ?
-4 priv?es ++ .
- qu?il/qu?
-5 Haaretz/_Uw ++ de - sont -6 contest?
++ ; - le -7 occup?s ++ : - qu?un/qu?
+8 priv?e ++ la - qu?un/un +9 [...] ++ ?
?trang?res - que -10 faudrait ++ lecteurs - pourrait -Table 4: The 10 best-ranked suspicious forms, according the the Mf measure, as computed by differentalgorithms: ours (this paper), a standard maximum entropy algorithm (maxent) and van Noord?s rateerr(f) (global).Rank Tokens and forms Mf Error cause4 Toutes/toutes les 2.73 grammar: badly treated pre-determiner adjective6 y en 2,34 grammar: problem with the construction il y en a. .
.7 in ?
1.81 Lefff : in misses as a preposition, which happends before book titles (hence the ?
)10 donne ?
1.44 Lefff : donner should sub-categorize ?-vcomps (donner ?
voir.
.
.
)11 de demain 1.19 Lefff : demain misses as common noun (standard adv are not preceded by prep)16 ( 22/_NUMBER 0.86 grammar: footnote references not treated16 22/_NUMBER ) 0.86 as aboveTable 5: Best ranked form bigrams (forms ranked inbetween are not shown; ranked according to Mf =Sf ?
ln |Of |).
These results have been computed on a subset of the MD corpus (60,000 sentences).335phasize bigram results), with the same data as intable 3.5 Conclusions and perspectivesAs we have shown, parsing large corpora allowsto set up error mining techniques, so as to identifymissing and erroneous information in the differ-ent resources that are used by full-featured pars-ing systems.
The technique described in this pa-per and its implementation on forms and form bi-grams has already allowed us to detect many errorsand omissions in the Lefff lexicon, to point out in-appropriate behaviors of the SXPipe pre-syntacticprocessing chain, and to reveal the lack of cover-age of the grammars for certain phenomena.We intend to carry on and extend this work.First of all, the visualization environment can beenhanced, as is the case for the implementation ofthe algorithm itself.We would also like to integrate to the modelthe possibility that facts taken into account (to-day, forms and form bigrams) are not necessar-ily certain, because some of them could be theconsequence of an ambiguity.
For example, fora given form, several lemmas are often possible.The probabilization of these lemmas would thusallow to look for most suspicious lemmas.We are already working on a module that willallow not only to detect errors, for example inthe lexicon, but also to propose a correction.
Toachieve this, we want to parse anew all non-parsable sentences, after having replaced theirmain suspects by a special form that receivesunder-specified lexical information.
These infor-mation can be either very general, or can be com-puted by appropriate generalization patterns ap-plied on the information associated by the lexiconwith the original form.
A statistical study of thenew parsing results will make it possible to pro-pose corrections concerning the involved forms.ReferencesA.
Berger, S. Della Pietra, and V. Della Pietra.
1996.
Amaximun entropy approach to natural language pro-cessing.
Computational Linguistics, 22(1):pp.
39?71.Pierre Boullier and Beno?t Sagot.
2005a.
Analyse syn-taxique profonde ?
grande ?chelle: SXLFG.
Traite-ment Automatique des Langues (T.A.L.
), 46(2).Pierre Boullier and Beno?t Sagot.
2005b.
Efficientand robust LFG parsing: SxLfg.
In Proceedings ofIWPT?05, Vancouver, Canada, October.Hal Daum?
III.
2004.
Notes on CG and LM-BFGSoptimization of logistic regression.
Paper availableat http://www.isi.edu/~hdaume/docs/daume04cg-bfgs.ps, implementation avail-able at http://www.isi.edu/~hdaume/megam/.Patrick Paroubek, Louis-Gabriel Pouillot, IsabelleRobba, and Anne Vilnat.
2005.
EASy : cam-pagne d?
?valuation des analyseurs syntaxiques.
InProceedings of the EASy workshop of TALN 2005,Dourdan, France.Beno?t Sagot and Pierre Boullier.
2005.
From raw cor-pus to word lattices: robust pre-parsing processing.In Proceedings of L&TC 2005, Poznan?, Pologne.Beno?t Sagot, Lionel Cl?ment, ?ric Villemonte de laClergerie, and Pierre Boullier.
2005.
Vers unm?ta-lexique pour le fran?ais : architecture, acqui-sition, utilisation.
Journ?e d?
?tude de l?ATALA surl?interface lexique-grammaire et les lexiques syntax-iques et s?mantiques, March.Fran?ois Thomasset and ?ric Villemonte de la Clerg-erie.
2005.
Comment obtenir plus des m?ta-grammaires.
In Proceedings of TALN?05, Dourdan,France, June.
ATALA.Gertjan van Noord.
2004.
Error mining for wide-coverage grammar engineering.
In Proc.
of ACL2004, Barcelona, Spain.
?ric Villemonte de la Clergerie.
2005.
DyALog: atabular logic programming based environment forNLP.
In Proceedings of 2nd International Work-shop on Constraint Solving and Language Process-ing (CSLP?05), Barcelona, Spain, October.336
