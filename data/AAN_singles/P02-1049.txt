What?s the Trouble: Automatically Identifying Problematic Dialogues inDARPA Communicator Dialogue SystemsHelen Wright Hastie, Rashmi Prasad, Marilyn WalkerAT& T Labs - Research180 Park Ave, Florham Park, N.J. 07932, U.S.A.hhastie,rjprasad,walker@research.att.comAbstractSpoken dialogue systems promise effi-cient and natural access to informationservices from any phone.
Recently, spo-ken dialogue systems for widely used ap-plications such as email, travel informa-tion, and customer care have moved fromresearch labs into commercial use.
Theseapplications can receive millions of callsa month.
This huge amount of spokendialogue data has led to a need for fullyautomatic methods for selecting a subsetof caller dialogues that are most likelyto be useful for further system improve-ment, to be stored, transcribed and furtheranalyzed.
This paper reports results onautomatically training a Problematic Di-alogue Identifier to classify problematichuman-computer dialogues using a corpusof 1242 DARPA Communicator dialoguesin the travel planning domain.
We showthat using fully automatic features we canidentify classes of problematic dialogueswith accuracies from 67% to 89%.1 IntroductionSpoken dialogue systems promise efficient and nat-ural access to a large variety of information servicesfrom any phone.
Deployed systems and researchprototypes exist for applications such as personalemail and calendars, travel and restaurant informa-tion, personal banking, and customer care.
Withinthe last few years, several spoken dialogue systemsfor widely used applications have moved from re-search labs into commercial use (Baggia et al, 1998;Gorin et al, 1997).
These applications can receivemillions of calls a month.
There is a strong require-ment for automatic methods to identify and extractdialogues that provide training data for further sys-tem development.As a spoken dialogue system is developed, it isfirst tested as a prototype, then fielded in a limitedsetting, possibly running with human supervision(Gorin et al, 1997), and finally deployed.
At eachstage from research prototype to deployed commer-cial application, the system is constantly undergoingfurther development.
When a system is prototypedin house or first tested in the field, human subjectsare often paid to use the system and give detailedfeedback on task completion and user satisfaction(Baggia et al, 1998; Walker et al, 2001).
Evenwhen a system is deployed, it often keeps evolving,either because customers want to do different thingswith it, or because new tasks arise out of develop-ments in the underlying application.
However, realcustomers of a deployed system may not be willingto give detailed feedback.Thus, the widespread use of these systems hascreated a data management and analysis problem.System designers need to constantly track systemperformance, identify problems, and fix them.
Sys-tem modules such as automatic speech recognition(ASR), natural language understanding (NLU) anddialogue management may rely on training data col-lected at each phase.
ASR performance assessmentrelies on full transcription of the utterances.
Dia-logue manager assessment relies on a human inter-face expert reading a full transcription of the dia-logue or listening to a recording of it, possibly whileexamining the logfiles to understand the interactionbetween all the components.
However, because ofthe high volume of calls, spoken dialogue serviceproviders typically can only afford to store, tran-scribe, and analyze a small fraction of the dialogues.Computational Linguistics (ACL), Philadelphia, July 2002, pp.
384-391.Proceedings of the 40th Annual Meeting of the Association forTherefore, there is a great need for methods forboth automatically evaluating system performance,and for extracting subsets of dialogues that providegood training data for system improvement.
This isa difficult problem because by the time a system isdeployed, typically over 90% of the dialogue inter-actions result in completed tasks and satisfied users.Dialogues such as these do not provide very use-ful training data for further system development be-cause there is little to be learned when the dialoguegoes well.Previous research on spoken dialogue evaluationproposed the application of automatic classifiers foridentifying and predicting of problematic dialogues(Litman et al, 1999; Walker et al, 2002) for thepurpose of automatically adapting the dialogue man-ager.
Here we apply similar methods to the dialoguecorpus data-mining problem described above.
Wereport results on automatically training a Problem-atic Dialogue Identifier (PDI) to classify problem-atic human-computer dialogues using the October-2001 DARPA Communicator corpus.Section 2 describes our approach and the dialoguecorpus.
Section 3 describes how we use the DATEdialogue act tagging scheme to define input featuresfor the PDI.
Section 4 presents a method and resultsfor automatically predicting task completion.
Sec-tion 5 presents results for predicting problematic di-alogues based on the user?s satisfaction.
We showthat we identify task failure dialogues with 85% ac-curacy (baseline 59%) and dialogues with low usersatisfaction with up to 89% accuracy.
We discuss theapplication of the PDI to data mining in Section 6.Finally, we summarize the paper and discuss futurework.2 Corpus, Methods and DataOur experiments apply CLASSIFICATION and RE-GRESSION trees (CART) (Brieman et al, 1984) totrain a Problematic Dialogue Identifier (PDI) froma corpus of human-computer dialogues.
CLASSI-FICATION trees are used for categorical responsevariables and REGRESSION trees are used for con-tinuous response variables.
CART trees are binarydecision trees.
A CLASSIFICATION tree specifieswhat queries to perform on the features to maximizeCLASSIFICATION ACCURACY, while REGRESSIONtrees derive a set of queries to maximize the COR-RELATION of the predicted value and the originalvalue.
Like other machine learners, CART takes asinput the allowed values for the response variables;the names and ranges of values of a fixed set of inputfeatures; and training data specifying the responsevariable value and the input feature values for eachexample in a training set.
Below, we specify howthe PDI was trained, first describing the corpus, thenthe response variables, and finally the input featuresderived from the corpus.Corpus: We train and test the PDI on the DARPACommunicator October-2001 corpus of 1242 dia-logues.
This corpus represents interactions withreal users, with eight different Communicator travelplanning systems, over a period of six months fromApril to October of 2001.
The dialogue tasks rangefrom simple domestic round trips to multileg inter-national trips requiring both car and hotel arrange-ments.
The corpus includes logfiles with loggedevents for each system and user turn; hand transcrip-tions and automatic speech recognizer (ASR) tran-scription for each user utterance; information de-rived from a user profile such as user dialect region;and a User Satisfaction survey and hand-labelledTask Completion metric for each dialogue.
We ran-domly divide the corpus into 80% training (894 dia-logues) and 20% testing (248 dialogues).Defining the Response Variables: In principle,either low User Satisfaction or failure to completethe task could be used to define problematic dia-logues.
Therefore, both of these are candidate re-sponse variables to be examined.
The User Satisfac-tion measure derived from the user survey ranges be-tween 5 and 25.
Task Completion is a ternary mea-sure where no Task Completion is indicated by 0,completion of only the airline itinerary is indicatedby 1, and completion of both the airline itinerary andground arrangements, such as car and hotel book-ings, is indicated by 2.
We also defined a binary ver-sion of Task Completion, where Binary Task Com-pletion=0 when no task or subtask was complete(equivalent to Task Completion=0), and Binary TaskCompletion=1 where all or some of the task wascomplete (equivalent to Task Completion=1 or TaskCompletion=2).Figure 1 shows the frequency of dialogues forvarying User Satisfaction for cases where TaskCompletion is 0 (solid line) and Task Completionis greater than 0 (dotted lines).
Note that Task Com-pletion is 1 or 2 for a number of dialogues for whichUser Satisfaction is low.
Figure 2 illustrates such adialogue (system turns are labelled S, user turns asU, and ASR hypotheses as REC).
Here, low UserSatisfaction may be due to the fact that the user hadto repeat herself many times before the system un-derstood the departure city.
An automatic surrogatefor ASR accuracy (such as ASR confidence) wouldFigure 1: User Satisfaction for dialogues where TaskCompletion is 0, 1 or 2not be adequate for identifying this problematic di-alogue, because here either the dialogue manager orthe SLU component is at fault.
Another dialoguesubset of interest in Figure 1 is one for which TaskCompletion is 0 but User Satisfaction is high.
Acommon cause for non-Task Completion in these di-alogues is database access problems.
The fact thatthe interaction went well until this point is not cap-tured by the Task Completion metric.
Thus we de-cided to use both User Satisfaction and Task Com-pletion as response variables in separate experimentsin order to allow the PDI to capture the relationshipbetween these two variables.Input Features: Both User Satisfaction and TaskCompletion predictors are trained using two types ofautomatically extractable features: (1) logfile fea-tures; and (2) system dialogue act types.
Thelogfile features include efficiency metrics such asthe number of system and user turns spent on thetask; the total time of the dialogue in milliseconds;and situational metrics, such as phone type1 whichaffects ASR.
The system dialogue act types areextracted from the logfiles using the DATE (Dia-logue Act Tagging for Evaluation) automatic tag-1We assume this is automatically derivable by automaticnumber identification (ANI).S13: Sorry, I must have misunderstood.
I just heard yousay that the departure city was Minto, Arkansas.Was the departure city Minneapolis-St-Paul, Minto,Arkansas or Milan, Italy?U14: THE DEPARTURE CITY IS MINNEAPOLISSAINT PAULREC: the departure city in minneapolis saint paulS15: Here?s a summary of the second flight segment:I?ve got you leaving from Minto, Arkansas on Fri-day, October twelfth in the morning and arriving innewark flying on continental.
Is this correct?U16: NO I WANT TO LEAVE FROM MINNEAPOLISREC: no i want to leave from minneapolisRepeat dialogue segment 3 timesS25: Sorry, I must have misunderstood.
I just heard yousay that the departure city was Minto, Arkansas.
Wasthe departure city Minto, Arkansas or Minneapolis-St-Paul ?U26: THE DEPARTURE CITY IS MINNEAPOLISSAINT PAULREC: the departure city minneapolis by allS27: Recapping the second flight segment: I have youleaving out of minneapolis-st-paul on Friday, Octo-ber twelfth in the morning and arriving in newarkflying on continental.
Is this correct?U28: YESREC: yesFigure 2: Sample low user satisfaction dialogueging scheme (Walker et al, 2001).
The purposeof these features is to extract numerical correlatesof system dialogue behaviors.
This dialogue act la-belling procedure is detailed in Section 3.Figure 3 summarizes the types of features usedto train the User Satisfaction predictor.
In additionto the efficiency metrics and the DATE labels, TaskSuccess can itself be used as a predictor.
This caneither be the hand-labelled feature or an approxima-tion as predicted by the Task Completion Predictor,described in Section 4.
Figure 4 shows the systemdesign for automatically predicting User Satisfac-  Efficiency Measures?
Hand-labelled: WERR, SERR?
Automatic: TimeOnTask, TurnsOnTask, Nu-mOverlaps, MeanUsrTurnDur, MeanWrdsPerUs-rTurn, MeanSysTurnDur, MeanWrdsPerSysTurn,DeadAlive, Phone-type, SessionNumber  Qualitative Measures?
Automatic: DATE Unigrams, e.g.
present-info:flight, acknowledgement:flight booking etc.?
Automatic: DATE Bigrams, e.g.
present-info:flight+acknowledgement:flight booking etc.  Task Success Features?
Hand-labelled: HL Task Completion?
Automatic: Auto Task CompletionFigure 3: Features used to train the User SatisfactionPrediction treetion with the three types of input features.DATEOutputofSLSCompletionAuto Task CompletionCARTPredictorUserSatisfactionTaskPredictorTAGGERAutomaticLogfileFeaturesDATERulesFigure 4: Schema for User Satisfaction prediction3 Extracting DATE FeaturesThe dialogue act labelling of the corpus followsthe DATE tagging scheme (Walker et al, 2001).In DATE, utterance classification is done alongthree cross-cutting orthogonal dimensions.
TheCONVERSATIONAL-DOMAIN dimension specifiesthe domain of discourse that an utterance is about.The SPEECH ACT dimension captures distinctionsbetween communicative goals such as requestinginformation (REQUEST-INFO) or presenting infor-mation (PRESENT-INFO).
The TASK-SUBTASK di-mension specifies which travel reservation subtaskthe utterance contributes to.
The SPEECH ACT andCONVERSATIONAL-DOMAIN dimensions are gen-eral across domains, while the TASK-SUBTASK di-mension is domain- and sometimes system-specific.Within the conversational domain dimension,DATE distinguishes three domains (see Figure 5).The ABOUT-TASK domain is necessary for evaluat-ing a dialogue system?s ability to collaborate witha speaker on achieving the task goal.
The ABOUT-COMMUNICATION domain reflects the system goalof managing the verbal channel of communicationand providing evidence of what has been under-stood.
All implicit and explicit confirmations areabout communication.
The ABOUT-SITUATION-FRAME domain pertains to the goal of managing theuser?s expectations about how to interact with thesystem.DATE distinguishes 11 speech acts.
Examples ofeach speech act are shown in Figure 6.The TASK-SUBTASK dimension distinguishesamong 28 subtasks, some of which can also begrouped at a level below the top level task.
TheTOP-LEVEL-TRIP task describes the task which con-tains as its subtasks the ORIGIN, DESTINATION,Conversational Domain ExampleABOUT-TASK And what time didja wannaleave?ABOUT-COMMUNICATIONLeaving from Miami.ABOUT-SITUATION-FRAMEYou may say repeat, help meout, start over, or, that?s wrongFigure 5: Example utterances distinguished withinthe Conversational Domain DimensionSpeech-Act ExampleREQUEST-INFO And, what city are you flying to?PRESENT-INFO The airfare for this trip is 390 dol-lars.OFFER Would you like me to hold this op-tion?ACKNOWLEDGMENT I will book this leg.BACKCHANNEL Okay.STATUS-REPORT Accessing the database; thismight take a few seconds.EXPLICIT-CONFIRMYou will depart on September 1st.Is that correct?IMPLICIT-CONFIRMLeaving from Dallas.INSTRUCTION Try saying a short sentence.APOLOGY Sorry, I didn?t understand that.OPENING-CLOSINGHello.
Welcome to the C M UCommunicator.Figure 6: Example speech act utterancesDATE, TIME, AIRLINE, TRIP-TYPE, RETRIEVALand ITINERARY tasks.
The GROUND task includesboth the HOTEL and CAR-RENTAL subtasks.
TheHOTEL task includes both the HOTEL-NAME andHOTEL-LOCATION subtasks.2For the DATE labelling of the corpus, we imple-mented an extended version of the pattern matcherthat was used for tagging the Communicator June2000 corpus (Walker et al, 2001).
This methodidentified and labelled an utterance or utterance se-quence automatically by reference to a database ofutterance patterns that were hand-labelled with theDATE tags.
Before applying the pattern matcher,a named-entity labeler was applied to the systemutterances, matching named-entities relevant in thetravel domain, such as city, airport, car, hotel, airlinenames etc..
The named-entity labeler was also ap-plied to the utterance patterns in the pattern databaseto allow for generality in the expression of com-municative goals specified within DATE.
For thisnamed-entity labelling task, we collected vocabularylists from the sites, which maintained such lists for2ABOUT-SITUATION-FRAME utterances are not specific toany particular task and can be used for any subtask, for example,system statements that it misunderstood.
Such utterances aregiven a ?meta?
dialogue act status in the task dimension.developing their system.3 The extension of the pat-tern matcher for the 2001 corpus labelling was donebecause we found that systems had augmented theirinventory of named entities and utterance patternsfrom 2000 to 2001, and these were not accountedfor by the 2000 tagger database.
For the extension,we collected a fresh set of vocabulary lists from thesites and augmented the pattern database with ad-ditional 800 labelled utterance patterns.
We alsoimplemented a contextual rule-based postprocessorthat takes any remaining unlabelled utterances andattempts to label them by looking at their surround-ing DATE labels.
More details about the extendedtagger can be found in (Prasad and Walker, 2002).On the 2001 corpus, we were able to label 98.4 of the data.
A hand evaluation of 10 randomly se-lected dialogues from each system shows that weachieved a classification accuracy of 96  at the ut-terance level.For User Satisfaction Prediction, we found thatthe distribution of DATE acts were better capturedby using the frequency normalized over the totalnumber of dialogue acts.
In addition to these un-igram proportions, the bigram frequencies of theDATE dialogue acts were also calculated.
In the fol-lowing two sections, we discuss which DATE labelsare discriminatory for predicting Task Completionand User Satisfaction.4 The Task Completion PredictorIn order to automatically predict Task Comple-tion, we train a CLASSIFICATION tree to catego-rize dialogues into Task Completion=0, Task Com-pletion=1 or Task Completion=2.
Recall that aCLASSIFICATION tree attempts to maximize CLAS-SIFICATION ACCURACY, results for Task Comple-tion are thus given in terms of percentage of dia-logues correctly classified.
The majority class base-line is 59.3% (dialogues where Task Completion=1).The tree was trained on a number of different in-put features.
The most discriminatory ones, how-ever, were derived from the DATE tagger.
Weuse the primitive DATE tags in conjunction with afeature called GroundCheck (GC), a boolean fea-ture indicating the existence of DATE tags relatedto making ground arrangements, specifically re-quest info:hotel name, request info:hotel location,offer:hotel and offer:rental.Table 1 gives the results for Task Completion pre-diction accuracy using the various types of features.3The named entities were preclassified into their respectivesemantic classes by the sites.Baseline Auto ALF + ALF +Logfile GC GC+ DATETC 59% 59% 79% 85%BTC 86% 86% 86% 92%Table 1: Task Completion (TC) and Binary TaskCompletion (BTC) prediction results, using auto-matic logfile features (ALF), GroundCheck (GC)and DATE unigram frequenciesThe first row is for predicting ternary Task Comple-tion, and the second for predicting binary Task Com-pletion.
Using automatic logfile features (ALF) isnot effective for the prediction of either types of TaskCompletion.
However, the use of GroundCheck re-sults in an accuracy of 79% for the ternary TaskCompletion which is significantly above the base-line (df = 247, t = -6.264, p  .0001).
Adding in theother DATE features yields an accuracy of 85%.
ForBinary Task Completion it is only the use of all theDATE features that yields an improvement over thebaseline of 92%, which is significant (df = 247, t =5.83, p  .0001).A diagram of the trained decision tree for ternaryTask Completion is given in Figure 7.
At any junc-tion in the tree, if the query is true then one takesthe path down the right-hand side of the tree, oth-erwise one takes the left-hand side.
The leaf nodescontain the predicted value.
The GroundCheck fea-ture is at the top of the tree and divides the datainto Task Completion  2 and Task Completion  2.If GroundCheck  1, then the tree estimates thatTask Completion is 2, which is the best fit for thedata given the input features.
If GroundCheck  0and there is an acknowledgment of a booking, thenprobably a flight has been booked, therefore, TaskCompletion is predicted to be 1.
Interestingly, ifthere is no acknowledgment of a booking then TaskCompletion  0, unless the system got to the stage ofasking the user for an airline preference and if re-quest info:top level trip  2.
More than one of theseDATE types indicates that there was a problem in thedialogue and that the information gathering phasestarted over from the beginning.The binary Task Completion decision tree simplychecks if an acknowledgement:flight bookinghas occurred.
If it has, then Binary Task Com-pletion=1, otherwise it looks for the DATE actabout situation frame:instruction:meta situation info,which captures the fact that the system has toldthe user what the system can and cannot do, orhas informed the user about the current state of thetask.
This must help with Task Completion, as thetree tells us that if one or more of these acts areobserved then Task Completion=1, otherwise TaskCompletion=0.TC=1GroundCheck =0TC=2request_info:airline <1request_info:top_level_trip < 2acknow.
: flight_booking< 1TC=0TC=1TC=0 TC=1Figure 7: Classification Tree for predicting TaskCompletion (TC)5 The User Satisfaction PredictorFeature Log LF + LF +used features unigram bigramHL TC 0.587 0.584 0.592Auto TC 0.438 0.434 0.472HL BTC 0.608 0.607 0.614Auto BTC 0.477 0.47 0.484Table 2: Correlation results using logfile fea-tures (LF), adding unigram proportions and bigramcounts, for trees tested on either hand-labelled (HL)or automatically derived Task Completion (TC) andBinary Task Completion (BTC)Quantitative Results: Recall that REGRESSIONtrees attempt to maximize the CORRELATION of thepredicted value and the original value.
Thus, the re-sults of the User Satisfaction predictor are given interms of the correlation between the predicted UserSatisfaction and actual User Satisfaction as calcu-lated from the user survey.
Here, we also provide R for comparison with previous studies.
Table 2 givesthe correlation results for User Satisfaction for dif-ferent feature sets.
The User Satisfaction predictoris trained using the hand-labelled Task Completionfeature for a topline result and using the automati-cally obtained Task Completion (Auto TC) for thefully automatic results.
We also give results usingBinary Task Completion (BTC) as a substitute forTask Completion.
The first column gives results us-ing features extracted from the logfile; the secondcolumn indicates results using the DATE unigramproportions and the third column indicates resultswhen both the DATE unigram and bigram featuresare available.The first row of Table 2 indicates that perfor-mance across the three feature sets is indistinguish-able when hand-labelled Task Completion (HL TC)is used as the Task Completion input feature.
Acomparison of Row 1 and Row 2 shows that thePDI performs significantly worse using only auto-matic features (z = 3.18).
Row 2 also indicates thatthe DATE bigrams help performance, although thedifference between R = .438 and R = .472 is notsignificant.
The third and fourth rows of Table 1indicate that for predicting User Satisfaction, Bi-nary Task Completion is as good as or better thanTernary Task Completion.
The highest correlation of0.614 (   	) uses hand-labelled Binary TaskCompletion and the logfile features and DATE uni-gram proportions and bigram counts.
Again, we seethat the Automatic Binary Task Completion (AutoBTC) performs significantly worse than the hand-labelled version (z = -3.18).
Row 4 includes the besttotally automatic system: using Automatic BinaryTask Completion and DATE unigrams and bigramsyields a correlation of 0.484 ( 	 ).Regression Tree Interpretation: It is interest-ing to examine the trees to see which features areused for predicting User Satisfaction.
A metriccalled Feature Usage Frequency indicates which fea-tures are the most discriminatory in the CART tree.Specifically, Feature Usage Frequency counts howoften a feature is queried for each data point, nor-malized so that the sum of Feature Usage Frequencyvalues for all the features sums to one.
The higher afeature is in the tree, the more times it is queried.
Tocalculate the Feature Usage Frequency, we groupedthe features into three types: Task Completion, Log-file features and DATE frequencies.
Feature Us-age Frequency for the logfile features is 37%.
TaskCompletion occurs only twice in the tree, however,it makes up 31because it occurs at the top of thetree.
The Feature Usage Frequency for DATE cat-egory frequency is 32%.
We will discuss each ofthese three groups of features in turn.The most used logfile feature is TurnsOnTaskwhich is the number of turns which are task-oriented, for example, initial instructions on howto use the system are not taken as a TurnOnTask.Shorter dialogues tend to have a higher User Sat-isfaction.
This is reflected in the User Satisfactionscores in the tree.
However, dialogues which arelong (TurnsOnTask  79 ) can be satisfactory (UserSatisfaction = 15.2) as long as the task that is com-pleted is long, i.e., if ground arrangements are madein that dialogue (Task Completion=2).
If ground ar-rangements are not made, the User Satisfaction islower (11.6).
Phone type is another important fea-ture queried in the tree, so that dialogues conductedover corded phones have higher satisfaction.
Thisis likely to be due to better recognition performancefrom corded phones.As mentioned previously, Task Completion is atthe top of the tree and is therefore the most queriedfeature.
This captures the relationship between TaskCompletion and User Satisfaction as illustrated inFigure 1.Finally, it is interesting to examine which DATEtags the tree uses.
If there have been more thanthree acknowledgments of bookings, then severallegs of a journey have been successfully booked,therefore User Satisfaction is high.
In particular,User Satisfaction is high if the system has askedif the user would like a price for their itinerarywhich is one of the final dialogue acts a systemdoes before the task is completed.
The DATE actabout comm:apology:meta slu reject is a measureof the system?s level of misunderstanding.
There-fore, the more of these dialogue act types the lowerUser Satisfaction.
This part of the tree uses lengthin a similar way described earlier, whereby long di-alogues are only allocated lower User Satisfactionif they do not involve ground arrangements.
Usersdo not seem to mind longer dialogues as long asthe system gives a number of implicit confirma-tions.
The dialogue act request info:top level tripusually occurs at the start of the dialogue and re-quests the initial travel plan.
If there are more thanone of this dialogue act, it indicates that a START-OVER occurred due to system failure, and this leadsto lower User Satisfaction.
A rule containing thebigram request info:depart day month date+USERstates that if there is more than one occurrence of thisrequest then User Satisfaction will be lower.
USERis the single category used for user-turns.
No auto-matic method of predicting user speech act is avail-able yet for this data.
A repetition of this DATEbigram indicates that a misunderstanding occurredthe first time it was requested, or that the task ismulti-leg in which case User Satisfaction is gener-ally lower.The tree that uses Binary Task Completion isidentical to the tree described above, apart fromone binary decision which differentiates dialogueswhere Task Completion=1 and Task Completion=2.Instead of making this distinction, it just uses dia-logue length to indicate the complexity of the task.In the original tree, long dialogues are not penalizedif they have achieved a complex task (i.e.
if TaskCompletion=2).
The Binary Task Completion treehas no way of making this distinction and thereforejust penalizes very long dialogues (where TurnsOn-Task  110).
The Feature Usage Frequency for theTask Completion features is reduced from 31% to21%, and the Feature Usage Frequency for the log-file features increases to 47%.
We have shown thatthis more general tree produces slightly better re-sults.6 Results for Identifying ProblematicDialogues for Data MiningSo far, we have described a PDI that predicts UserSatisfaction as a continuous variable.
For data min-ing, system developers will want to extract dialogueswith predicted User Satisfaction below a particularthreshold.
This threshhold could vary during dif-ferent stages of system development.
As the sys-tem is fine tuned there will be fewer and fewer dia-logues with low User Satisfaction, therefore in orderto find the interesting dialogues for system develop-ment one would have to raise the User Satisfactionthreshold.
In order to illustrate the potential valueof our PDI, consider an example threshhold of 12which divides the data into 73.4% good dialogueswhere User Satisfaction  12 which is our baselineresult.Table 3 gives the recall and precision for the PDIsdescribed above which use hand-labelled Task Com-pletion and Auto Task Completion.
In the data,26.6% of the dialogues are problematic (User Sat-isfaction is under 12), whereas the PDI using hand-labelled Task Completion predicts that 21.8% areproblematic.
Of the problematic dialogues, 54.5%are classified correctly (Recall).
Of the dialoguesthat it classes as problematic 66.7% are problematic(Precision).
The results for the automatic systemshow an improvement in Recall: it identifies moreproblematic dialogues correctly (66.7%) but the pre-cision is lower.What do these numbers mean in terms of our orig-inal goal of reducing the number of dialogues thatneed to be transcribed to find good cases to useTask Completion Dialogue Recall Prec.Hand-labelled Good 90% 84.5%Hand-labelled Problematic 54.5% 66.7%Automatic Good 88.5% 81.3%Automatic Problematic 66.7% 58.0%Table 3: Precision and Recall for good and prob-lematic dialogues (where a good dialogue has UserSatisfaction  12) for the PDI using hand-labelledTask Completion and Auto Task Completionfor system improvement?
If one had a budget totranscribe 20% of the dataset containing 100 dia-logues, then by randomly extracting 20 dialogues,one would transcribe 5 problematic dialogues and 15good dialogues.
Using the fully automatic PDI, onewould obtain 12 problematic dialogues and 8 gooddialogues.
To look at it another way, to extract 15problematic dialogues out of 100, 55% of the datawould need transcribing.
To obtain 15 problem-atic dialogues using the fully automatic PDI, only26% of the data would need transcribing.
This is amassive improvement over randomly choosing dia-logues.7 Discussion and Future DevelopmentsThis paper presented a Problematic Dialogue Identi-fier which system developers can use for evaluationand to extract problematic dialogues from a largedataset for system development.
We describe PDIsfor predicting both Task Completion and User Satis-faction in the DARPA Communicator October 2001corpus.There has been little previous work on recogniz-ing problematic dialogues.
However, a number ofstudies have been done on predicting specific errorsin a dialogue, using a variety of automatic and hand-labelled features, such as ASR confidence and se-mantic labels (Aberdeen et al, 2001; Hirschberg etal., 2000; Levow, 1998; Litman et al, 1999).
Pre-vious work on predicting problematic dialogues be-fore the end of the dialogue (Walker et al, 2002)achieved accuracies of 87% using hand-labelled fea-tures (baseline 67%).
Our automatic Task Comple-tion PDI achieves an accuracy of 85%.Previous work also predicted User Satisfactionby applying multi-variate linear regression featureswith and without DATE features and showed thatDATE improved the model fit from   	to (Walker et al, 2001).
Our best modelhas an 	.
One potential explanation for thisdifference is that the DATE features are most usefulin combination with non-automatic features such asWord Accuracy which the previous study used.
TheUser Satisfaction PDI using fully automatic featuresachieves a correlation of 0.484.In future work, we hope to improve our results bytrying different machine learning methods; includ-ing the user?s dialogue act types as input features;and testing these methods in new domains.8 AcknowledgmentsThe work reported in this paper was partially fundedby DARPA contract MDA972-99-3-0003.ReferencesJ.
Aberdeen, C. Doran, and L. Damianos.
2001.
Finding errorsautomatically in semantically tagged dialogues.
In HumanLanguage Technology Conference.P.
Baggia, G. Castagneri, and M. Danieli.
1998.
Field Trialsof the Italian ARISE Train Timetable System.
In Interac-tive Voice Technology for Telecommunications Applications,IVTTA, pages 97?102.L.
Brieman, J. H. Friedman, R. A. Olshen, and C. J. Stone.1984.
Classification and Regression Trees.
Wadsworth andBrooks, Monterey California.A.L.
Gorin, G. Riccardi, and J.H.
Wright.
1997.
How may ihelp you?
Speech Communication, 23:113?127.J.
B. Hirschberg, D. J. Litman, and M. Swerts.
2000.
Gener-alizing prosodic prediction of speech recognition errors.
InProceedings of the 6th International Conference of SpokenLanguage Processing (ICSLP-2000).G.
Levow.
1998.
Characterizing and recognizing spoken cor-rections in human-computer dialogue.
In Proceedings of the36th Annual Meeting of the Association of ComputationalLinguistics, pages 736?742.D.
J. Litman, M. A. Walker, and M. J. Kearns.
1999.
Automaticdetection of poor speech recognition at the dialogue level.In Proceedings of the Thirty Seventh Annual Meeting of theAssociation of Computational Linguistics, pages 309?316.R.
Prasad and M. Walker.
2002.
Training a dialogue act taggerfor human-human and human-computer travel dialogues.
InProceedings of the 3rd SIGdial Workshop on Discourse andDialogue, Philadelphia PA.M.
Walker, R. Passonneau, and J. Boland.
2001.
Quantita-tive and qualitative evaluation of darpa communicator spo-ken dialogue systems.
In Proceedings of the 39rd AnnualMeeting of the Association for Computational Linguistics(ACL/EACL-2001).M.
Walker, I. Langkilde-Geary, H. Wright Hastie, J. Wright,and A. Gorin.
2002.
Automatically training a problematicdialogue predictor for a spoken dialogue system.
JAIR.
