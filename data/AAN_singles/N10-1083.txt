Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 582?590,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsPainless Unsupervised Learning with FeaturesTaylor Berg-Kirkpatrick Alexandre Bouchard-Co?te?
John DeNero Dan KleinComputer Science DivisionUniversity of California at Berkeley{tberg, bouchard, denero, klein}@cs.berkeley.eduAbstractWe show how features can easily be addedto standard generative models for unsuper-vised learning, without requiring complexnew training methods.
In particular, eachcomponent multinomial of a generative modelcan be turned into a miniature logistic regres-sion model if feature locality permits.
The in-tuitive EM algorithm still applies, but with agradient-based M-step familiar from discrim-inative training of logistic regression mod-els.
We apply this technique to part-of-speechinduction, grammar induction, word align-ment, and word segmentation, incorporatinga few linguistically-motivated features intothe standard generative model for each task.These feature-enhanced models each outper-form their basic counterparts by a substantialmargin, and even compete with and surpassmore complex state-of-the-art models.1 IntroductionUnsupervised learning methods have been increas-ingly successful in recent NLP research.
The rea-sons are varied: increased supplies of unlabeleddata, improved understanding of modeling methods,additional choices of optimization algorithms, and,perhaps most importantly for the present work, in-corporation of richer domain knowledge into struc-tured models.
Unfortunately, that knowledge hasgenerally been encoded in the form of conditionalindependence structure, which means that injectingit is both tricky (because the connection betweenindependence and knowledge is subtle) and time-consuming (because new structure often necessitatesnew inference algorithms).In this paper, we present a range of experimentswherein we improve existing unsupervised modelsby declaratively adding richer features.
In particu-lar, we parameterize the local multinomials of exist-ing generative models using features, in a way whichdoes not require complex new machinery but whichstill provides substantial flexibility.
In the feature-engineering paradigm, one can worry less about thebackbone structure and instead use hand-designedfeatures to declaratively inject domain knowledgeinto a model.
While feature engineering has his-torically been associated with discriminative, super-vised learning settings, we argue that it can andshould be applied more broadly to the unsupervisedsetting.The idea of using features in unsupervised learn-ing is neither new nor even controversial.
Manytop unsupervised results use feature-based mod-els (Smith and Eisner, 2005; Haghighi and Klein,2006).
However, such approaches have presentedtheir own barriers, from challenging normalizationproblems, to neighborhood design, to the need forcomplex optimization procedures.
As a result, mostwork still focuses on the stable and intuitive ap-proach of using the EM algorithm to optimize datalikelihood in locally normalized, generative models.The primary contribution of this paper is todemonstrate the clear empirical success of a sim-ple and accessible approach to unsupervised learn-ing with features, which can be optimized by us-ing standard NLP building blocks.
We considerthe same generative, locally-normalized models thatdominate past work on a range of tasks.
However,we follow Chen (2003), Bisani and Ney (2008), andBouchard-Co?te?
et al (2008), and allow each com-ponent multinomial of the model to be a miniaturemulti-class logistic regression model.
In this case,the EM algorithm still applies with the E-step un-changed.
The M-step involves gradient-based train-ing familiar from standard supervised logistic re-gression (i.e., maximum entropy models).
By inte-grating these two familiar learning techniques, weadd features to unsupervised models without any582specialized learning or inference.A second contribution of this work is to show thatfurther gains can be achieved by directly optimiz-ing data likelihood with LBFGS (Liu et al, 1989).This alternative optimization procedure requires noadditional machinery beyond what EM uses.
Thisapproach is still very simple to implement, and wefound that it empirically outperforms EM.This paper is largely empirical; the underlying op-timization techniques are known, even if the overallapproach will be novel to many readers.
As an em-pirical demonstration, our results span an array ofunsupervised learning tasks: part-of-speech induc-tion, grammar induction, word alignment, and wordsegmentation.
In each task, we show that declaring afew linguistically motivated feature templates yieldsstate-of-the-art results.2 ModelsWe start by explaining our feature-enhanced modelfor part-of-speech (POS) induction.
This particularexample illustrates our approach to adding featuresto unsupervised models in a well-known NLP task.We then explain how the technique applies moregenerally.2.1 Example: Part-of-Speech InductionPOS induction consists of labeling words in textwith POS tags.
A hidden Markov model (HMM) is astandard model for this task, used in both a frequen-tist setting (Merialdo, 1994; Elworthy, 1994) and ina Bayesian setting (Goldwater and Griffiths, 2007;Johnson, 2007).A POS HMM generates a sequence of words inorder.
In each generation step, an observed wordemission yi and a hidden successor POS tag zi+1 aregenerated independently, conditioned on the currentPOS tag zi .
This process continues until an absorb-ing stop state is generated by the transition model.There are two types of conditional distributions inthe model?emission and transition probabilities?that are both multinomial probability distributions.The joint likelihood factors into these distributions:P?
(Y = y,Z = z) = P?
(Z1 = z1) ?|z|?i=1P?
(Yi = yi|Zi = zi) ?
P?
(Zi+1 = zi+1|Zi = zi)The emission distribution P?
(Yi = yi|Zi = zi) isparameterized by conditional probabilities ?y,z,EMITfor each word y given tag z. Alternatively, we canexpress this emission distribution as the output of alogistic regression model, replacing the explicit con-ditional probability table by a logistic function pa-rameterized by weights and features:?y,z,EMIT(w) =exp ?w, f(y, z, EMIT)??y?
exp ?w, f(y?, z, EMIT)?This feature-based logistic expression is equivalentto the flat multinomial in the case that the featurefunction f(y, z, EMIT) consists of all indicator fea-tures on tuples (y, z, EMIT), which we call BASICfeatures.
The equivalence follows by setting weightwy,z,EMIT = log(?y,z,EMIT).1 This formulation isknown as the natural parameterization of the multi-nomial distribution.In order to enhance this emission distribution, weinclude coarse features in f(y, z, EMIT), in addi-tion to the BASIC features.
Crucially, these featurescan be active across multiple (y, z) values.
In thisway, the model can abstract general patterns, suchas a POS tag co-occurring with an inflectional mor-pheme.
We discuss specific POS features in Sec-tion 4.2.2 General Directed ModelsLike the HMM, all of the models we propose arebased on locally normalized generative decisionsthat condition on some context.
In general, let X =(Z,Y) denote the sequence of generation steps (ran-dom variables) where Z contains all hidden randomvariables and Y contains all observed random vari-ables.
The joint probability of this directed modelfactors as:Pw(X = x) =?i?IPw(Xi = xi?
?Xpi(i) = xpi(i)),where Xpi(i) denotes the parents of Xi and I is theindex set of the variables in X.In the models that we use, each factor in the aboveexpression is the output of a local logistic regression1As long as no transition or emission probabilities are equalto zero.
When zeros are present, for instance to model that anabsorbing stop state can only transition to itself, it is often possi-ble to absorb these zeros into a base measure.
All the argumentsin this paper carry with a structured base measure; we drop it forsimplicity.583model parameterized by w:Pw`Xi = d?
?Xpi(i) = c?=exp?w, f(d, c, t)?Pd?
exp?w, f(d?, c, t)?Above, d is the generative decision value for Xipicked by the model, c is the conditioning contexttuple of values for the parents of Xi, and t is thetype of decision being made.
For instance, the POSHMM has two types of decisions: transitions andemissions.
In the emission model, the type t is EMIT,the decision d is a word and the context c is a tag.The denominator normalizes the factor to be a prob-ability distribution over decisions.The objective function we derive from this modelis the marginal likelihood of the observations y,along with a regularization term:L(w) = log Pw(Y = y)?
?||w||22 (1)This model has two advantages over the more preva-lent form of a feature-rich unsupervised model, theglobally normalized Markov random field.2 First,as we explain in Section 3, optimizing our objec-tive does not require computing expectations overthe joint distribution.
In the case of the POS HMM,for example, we do not need to enumerate an in-finite sum of products of potentials when optimiz-ing, in contrast to Haghighi and Klein (2006).
Sec-ond, we found that locally normalized models em-pirically outperform their globally normalized coun-terparts, despite their efficiency and simplicity.3 Optimization3.1 Optimizing with Expectation MaximizationIn this section, we describe the EM algorithm ap-plied to our feature-rich, locally normalized models.For models parameterized by standard multinomi-als, EM optimizes L(?)
= log P?
(Y = y) (Demp-ster et al, 1977).
The E-step computes expectedcounts for each tuple of decision d, context c, andmultinomial type t:ed,c,t?E?
[?i?I(Xi =d, Xpi(i) =c, t)???
?Y = y](2)2The locally normalized model class is actually equivalentto its globally normalized counterpart when the former meetsthe following three conditions: (1) The graphical model is adirected tree.
(2) The BASIC features are included in f .
(3) Wedo not include regularization in the model (?
= 0).
This followsfrom Smith and Johnson (2007).These expected counts are then normalized in theM-step to re-estimate ?
:?d,c,t ?ed,c,t?d?
ed?,c,tNormalizing expected counts in this way maximizesthe expected complete log likelihood with respect tothe current model parameters.EM can likewise optimize L(w) for our locallynormalized models with logistic parameterizations.The E-step first precomputes multinomial parame-ters from w for each decision, context, and type:?d,c,t(w)?exp?w, f(d, c, t)??d?
exp?w, f(d?, c, t)?Then, expected counts e are computed accord-ing to Equation 2.
In the case of POS induction,expected counts are computed with the forward-backward algorithm in both the standard and logisticparameterizations.
The only change is that the con-ditional probabilities ?
are now functions of w.The M-step changes more substantially, but stillrelies on canonical NLP learning methods.
We wishto choose w to optimize the regularized expectedcomplete log likelihood:?
(w, e) =?d,c,ted,c,t log ?d,c,t(w)?
?||w||22 (3)We optimize this objective via a gradient-basedsearch algorithm like LBFGS.
The gradient with re-spect to w takes the form??
(w, e) =?d,c,ted,c,t ??d,c,t(w)?
2?
?w (4)?d,c,t(w) = f(d, c, t)??d?
?d?,c,t(w)f(d?, c, t)This gradient matches that of regularized logis-tic regression in a supervised model: the differ-ence ?
between the observed and expected features,summed over every decision and context.
In the su-pervised case, we would observe the count of occur-rences of (d, c, t), but in the unsupervised M-step,we instead substitute expected counts ed,c,t.This gradient-based M-step is an iterative proce-dure.
For each different value of w considered dur-ing the search, we must recompute ?
(w), which re-quires computation in proportion to the size of the584parameter space.
However, e stays fixed throughoutthe M-step.
Algorithm 1 outlines EM in its entirety.The subroutine climb(?, ?, ?)
represents a generic op-timization step such as an LBFGS iteration.Algorithm 1 Feature-enhanced EMrepeatCompute expected counts e  Eq.
2repeatCompute ?
(w, e)  Eq.
3Compute??
(w, e)  Eq.
4w ?
climb(w, ?
(w, e),??
(w, e))until convergenceuntil convergence3.2 Direct Marginal Likelihood OptimizationAnother approach to optimizing Equation 1 is tocompute the gradient of the log marginal likelihooddirectly (Salakhutdinov et al, 2003).
The gradientturns out to have the same form as Equation 4, withthe key difference that ed,c,t is recomputed for everydifferent value of w. Algorithm 2 outlines the proce-dure.
Justification for this algorithm appears in theAppendix.Algorithm 2 Feature-enhanced direct gradientrepeatCompute expected counts e  Eq.
2Compute L(w)  Eq.
1Compute ??
(w, e)  Eq.
4w ?
climb(w, L(w),??
(w, e))until convergenceIn practice, we find that this optimization ap-proach leads to higher task accuracy for severalmodels.
However, in cases where computing ed,c,tis expensive, EM can be a more efficient alternative.4 Part-of-Speech InductionWe now describe experiments that demonstrate theeffectiveness of locally normalized logistic models.We first use the bigram HMM described in Sec-tion 2.1 for POS induction, which has two types ofmultinomials.
For type EMIT, the decisions d arewords and contexts c are tags.
For type TRANS, thedecisions and contexts are both tags.4.1 POS Induction FeaturesWe use the same set of features used by Haghighiand Klein (2006) in their baseline globally normal-ized Markov random field (MRF) model.
These areall coarse features on emission contexts that activatefor words with certain orthographic properties.
Weuse only the BASIC features for transitions.
Foran emission with word y and tag z, we use thefollowing feature templates:BASIC:  (y = ?, z = ?
)CONTAINS-DIGIT: Check if y contains digit and conjoinwith z:(containsDigit(y) = ?, z = ?
)CONTAINS-HYPHEN:  (containsHyphen(x) = ?, z = ?
)INITIAL-CAP: Check if the first letter of y iscapitalized:  (isCap(y) = ?, z = ?
)N-GRAM: Indicator functions for character n-grams of up to length 3 present in y.4.2 POS Induction Data and EvaluationWe train and test on the entire WSJ tag corpus (Mar-cus et al, 1993).
We attempt the most difficult ver-sion of this task where the only information our sys-tem can make use of is the unlabeled text itself.
Inparticular, we do not make use of a tagging dictio-nary.
We use 45 tag clusters, the number of POS tagsthat appear in the WSJ corpus.
There is an identifi-ability issue when evaluating inferred tags.
In or-der to measure accuracy on the hand-labeled corpus,we map each cluster to the tag that gives the highestaccuracy, the many-1 evaluation approach (Johnson,2007).
We run all POS induction models for 1000iterations, with 10 random initializations.
The meanand standard deviation of many-1 accuracy appearsin Table 1.4.3 POS Induction ResultsWe compare our model to the basic HMM and a bi-gram version of the feature-enhanced MRF model ofHaghighi and Klein (2006).
Using EM, we achievea many-1 accuracy of 68.1.
This outperforms thebasic HMM baseline by a 5.0 margin.
The samemodel, trained using the direct gradient approach,achieves a many-1 accuracy of 75.5, outperformingthe basic HMM baseline by a margin of 12.4.
Theseresults show that the direct gradient approach can of-fer additional boosts in performance when used witha feature-enhanced model.
We also outperform the585globally normalized MRF, which uses the same setof features and which we train using a direct gradi-ent approach.To the best of our knowledge, our system achievesthe best performance to date on the WSJ corpus fortotally unsupervised POS tagging.35 Grammar InductionWe next apply our technique to a grammar inductiontask: the unsupervised learning of dependency parsetrees via the dependency model with valence (DMV)(Klein and Manning, 2004).
A dependency parse isa directed tree over tokens in a sentence.
Each edgeof the tree specifies a directed dependency from ahead token to a dependent, or argument token.
Thus,the number of dependencies in a parse is exactly thenumber of tokens in the sentence, not counting theartificial root token.5.1 Dependency Model with ValenceThe DMV defines a probability distribution over de-pendency parse trees.
In this head-outward attach-ment model, a parse and the word tokens are derivedtogether through a recursive generative process.
Foreach token generated so far, starting with the root, aset of left dependents is generated, followed by a setof right dependents.There are two types of multinomial distributionsin this model.
The Bernoulli STOP probabilities?d,c,STOP capture the valence of a particular head.
Forthis type, the decision d is whether or not to stopgenerating arguments, and the context c contains thecurrent head h, direction ?
and adjacency adj.
Ifa head?s stop probability is high, it will be encour-aged to accept few arguments.
The ATTACH multi-nomial probability distributions ?d,c,ATTACH captureattachment preferences of heads.
For this type, a de-cision d is an argument token a, and the context cconsists of a head h and a direction ?.We take the same approach as previous work(Klein and Manning, 2004; Cohen and Smith, 2009)and use gold POS tags in place of words.3Haghighi and Klein (2006) achieve higher accuracies bymaking use of labeled prototypes.
We do not use any externalinformation.5.2 Grammar Induction FeaturesOne way to inject knowledge into a dependencymodel is to encode the similarity between the vari-ous morphological variants of nouns and verbs.
Weencode this similarity by incorporating features intoboth the STOP and the ATTACH probabilities.
Theattachment features appear below; the stop featuretemplates are similar and are therefore omitted.BASIC:  (a = ?, h = ?, ?
= ?
)NOUN: Generalize the morphological variants ofnouns by using isNoun(?
):(a = ?, isNoun(h) = ?, ?
= ?
)(isNoun(a) = ?, h = ?, ?
= ?
)(isNoun(a) = ?, isNoun(h) = ?, ?
= ?
)VERB: Same as above, generalizing verbs insteadof nouns by using isVerb(?
)NOUN-VERB: Same as above, generalizing withisVerbOrNoun(?)
= isVerb(?)?
isNoun(?
)BACK-OFF: We add versions of all other features thatignore direction or adjacency.While the model has the expressive power to al-low specific morphological variants to have theirown behaviors, the existence of coarse features en-courages uniform analyses, which in turn gives bet-ter accuracies.Cohen and Smith?s (2009) method has similarcharacteristics.
They add a shared logistic-normalprior (SLN) to the DMV in order to tie multinomialparameters across related derivation events.
Theyachieve their best results by only tying parame-ters between different multinomials when the cor-responding contexts are headed by nouns and verbs.This observation motivates the features we choose toincorporate into the DMV.5.3 Grammar Induction Data and EvaluationFor our English experiments we train and report di-rected attachment accuracy on portions of the WSJcorpus.
We work with a standard, reduced version ofWSJ, WSJ10, that contains only sentences of length10 or less after punctuation has been removed.
Wetrain on sections 2-21, and use section 22 as a de-velopment set.
We report accuracy on section 23.These are the same training, development, and testsets used by Cohen and Smith (2009).
The regular-ization parameter (?)
is tuned on the developmentset to maximize accuracy.For our Chinese experiments, we use the samecorpus and training/test split as Cohen and Smith586(2009).
We train on sections 1-270 of the Penn Chi-nese Treebank (Xue et al, 2002), similarly reduced(CTB10).
We test on sections 271-300 of CTB10,and use sections 400-454 as a development set.The DMV is known to be sensitive to initializa-tion.
We use the deterministic harmonic initializerfrom Klein and Manning (2004).
We ran each op-timization procedure for 100 iterations.
The resultsare reported in Table 1.5.4 Grammar Induction ResultsWe are able to outperform Cohen and Smith?s (2009)best system, which requires a more complicatedvariational inference method, on both English andChinese data sets.
Their system achieves an accu-racy of 61.3 for English and an accuracy of 51.9 forChinese.4 Our feature-enhanced model, trained us-ing the direct gradient approach, achieves an accu-racy of 63.0 for English, and an accuracy of 53.6 forChinese.
To our knowledge, our method for feature-based dependency parse induction outperforms allexisting methods that make the same set of condi-tional independence assumptions as the DMV.6 Word AlignmentWord alignment is a core machine learning com-ponent of statistical machine translation systems,and one of the few NLP tasks that is dominantlysolved using unsupervised techniques.
The pur-pose of word alignment models is to induce a cor-respondence between the words of a sentence andthe words of its translation.6.1 Word Alignment ModelsWe consider two classic generative alignment mod-els that are both used heavily today, IBM Model 1(Brown et al, 1994) and the HMM alignment model(Ney and Vogel, 1996).
These models generate ahidden alignment vector z and an observed foreignsentence y, all conditioned on an observed Englishsentence e. The likelihood of both models takes theform:P (y, z|e) =?jp(zj = i|zj?1) ?
?yj ,ei,ALIGN4Using additional bilingual data, Cohen and Smith (2009)achieve an accuracy of 62.0 for English, and an accuracy of52.0 for Chinese, still below our results.Model Inference Reg EvalPOS Induction ?
Many-1WSJBasic-HMM EM ?
63.1 (1.3)Feature-MRF LBFGS 0.1 59.6 (6.9)Feature-HMM EM 1.0 68.1 (1.7)LBFGS 1.0 75.5 (1.1)Grammar Induction ?
DirWSJ10Basic-DMV EM ?
47.8Feature-DMV EM 0.05 48.3LBFGS 10.0 63.0(Cohen and Smith, 2009) 61.3CTB10Basic-DMV EM ?
42.5Feature-DMV EM 1.0 49.9LBFGS 5.0 53.6(Cohen and Smith, 2009) 51.9Word Alignment ?
AERNISTChEn Basic-Model 1 EM ?
38.0Feature-Model 1 EM ?
35.6Basic-HMM EM ?
33.8Feature-HMM EM ?
30.0Word Segmentation ?
F1BRBasic-Unigram EM ?
76.9 (0.1)Feature-Unigram EM 0.2 84.5 (0.5)LBFGS 0.2 88.0 (0.1)(Johnson and Goldwater, 2009) 87Table 1: Locally normalized feature-based models outperformall proposed baselines for all four tasks.
LBFGS outperformedEM in all cases where the algorithm was sufficiently fast to run.Details of each experiment appear in the main text.The distortion term p(zj = i|zj?1) is uniform inModel 1, and Markovian in the HMM.
See Liang etal.
(2006) for details on the specific variant of thedistortion model of the HMM that we used.
We usethese standard distortion models in both the baselineand feature-enhanced word alignment systems.The bilexical emission model ?y,e,ALIGN differen-tiates our feature-enhanced system from the base-line system.
In the former, the emission model is astandard conditional multinomial that represents theprobability that decision word y is generated fromcontext word e, while in our system, the emissionmodel is re-parameterized as a logistic regressionmodel and feature-enhanced.Many supervised feature-based alignment modelshave been developed.
In fact, this logistic parame-terization of the HMM has been proposed before andyielded alignment improvements, but was trainedusing supervised estimation techniques (Varea et al,2002).5 However, most full translation systems to-5Varea et al (2002) describes unsupervised EM optimiza-tion with logistic regression models at a high level?their dy-namic training approach?but provides no experiments.587day rely on unsupervised learning so that the modelsmay be applied easily to many language pairs.
Ourapproach provides efficient and consistent unsuper-vised estimation for feature-rich alignment models.6.2 Word Alignment FeaturesThe BASIC features on pairs of lexical itemsprovide strong baseline performance.
We addcoarse features to the model in order to injectprior knowledge and tie together lexical items withsimilar characteristics.BASIC:  (e = ?, y = ?
)EDIT-DISTANCE:  (dist(y, e) = ?
)DICTIONARY:  ((y, e) ?
D) for dictionary D.STEM:  (stem(e) = ?, y = ?)
for Porter stemmer.PREFIX:  (prefix(e) = ?, y = ?)
for prefixes oflength 4.CHARACTER:  (e = ?, charAt(y, i) = ?)
for index i inthe Chinese word.These features correspond to several commonaugmentations of word alignment models, such asadding dictionary priors and truncating long words,but here we integrate them all coherently into a sin-gle model.6.3 Word Alignment Data and EvaluationWe evaluate on the standard hand-aligned portionof the NIST 2002 Chinese-English development set(Ayan et al, 2005).
The set is annotated with sure Sand possible P alignments.
We measure alignmentquality using alignment error rate (AER) (Och andNey, 2000).We train the models on 10,000 sentences of FBISChinese-English newswire.
This is not a large-scaleexperiment, but large enough to be relevant for low-resource languages.
LBFGS experiments are notprovided because computing expectations in thesemodels is too computationally intensive to run formany iterations.
Hence, EM training is a more ap-propriate optimization approach: computing the M-step gradient requires only summing over word typepairs, while the marginal likelihood gradient neededfor LBFGS requires summing over training sentencealignments.
The final alignments, in both the base-line and the feature-enhanced models, are computedby training the generative models in both directions,combining the result with hard union competitivethresholding (DeNero and Klein, 2007), and us-ing agreement training for the HMM (Liang et al,2006).
The combination of these techniques yieldsa state-of-the-art unsupervised baseline for Chinese-English.6.4 Word Alignment ResultsFor both IBM Model 1 and the HMM alignmentmodel, EM training with feature-enhanced modelsoutperforms the standard multinomial models, by2.4 and 3.8 AER respectively.6 As expected, largepositive weights are assigned to both the dictionaryand edit distance features.
Stem and character fea-tures also contribute to the performance gain.7 Word SegmentationFinally, we show that it is possible to improve uponthe simple and effective word segmentation modelpresented in Liang and Klein (2009) by addingphonological features.
Unsupervised word segmen-tation is the task of identifying word boundaries insentences where spaces have been removed.
For asequence of characters y = (y1, ..., yn), a segmen-tation is a sequence of segments z = (z1, ..., z|z|)such that z is a partition of y and each zi is a con-tiguous subsequence of y. Unsupervised models forthis task infer word boundaries from corpora of sen-tences of characters without ever seeing examples ofwell-formed words.7.1 Unigram Double-Exponential ModelLiang and Klein?s (2009) unigram double-exponential model corresponds to a simplederivational process where sentences of charactersx are generated a word at a time, drawn from amultinomial over all possible strings ?z,SEGMENT.For this type, there is no context and the decision isthe particular string generated.
In order to avoid thedegenerate MLE that assigns mass only to singlesegment sentences it is helpful to independentlygenerate a length for each segment from a fixeddistribution.
Liang and Klein (2009) constrain in-dividual segments to have maximum length 10 andgenerate lengths from the following distribution:?l,LENGTH = exp(?l1.6) when 1 ?
l ?
10.
Theirmodel is deficient since it is possible to generate6The best published results for this dataset are supervised,and trained on 17 times more data (Haghighi et al, 2009).588lengths that are inconsistent with the actual lengthsof the generated segments.
The likelihood equationis given by:P (Y = y,Z = z) =?STOP|z|?i=1[(1?
?STOP) ?zi,SEGMENT exp(?|zi|1.6)]7.2 Segmentation Data and EvaluationWe train and test on the phonetic version of theBernstein-Ratner corpus (1987).
This is the sameset-up used by Liang and Klein (2009), Goldwateret al (2006), and Johnson and Goldwater (2009).This corpus consists of 9790 child-directed utter-ances transcribed using a phonetic representation.We measure segment F1 score on the entire corpus.We run all word segmentation models for 300 iter-ations with 10 random initializations and report themean and standard deviation of F1 in Table 1.7.3 Segmentation FeaturesThe SEGMENT multinomial is the important distri-bution in this model.
We use the following features:BASIC:  (z = ?
)LENGTH:  (length(z) = ?
)NUMBER-VOWELS:  (numVowels(z) = ?
)PHONO-CLASS-PREF:  (prefix(coarsePhonemes(z)) = ?
)PHONO-CLASS-PREF:  (suffix(coarsePhonemes(z)) = ?
)The phonological class prefix and suffix featuresproject each phoneme of a string to a coarser classand then take prefix and suffix indicators on thestring of projected characters.
We include two ver-sions of these features that use projections with dif-ferent levels of coarseness.
The goal of these fea-tures is to help the model learn general phoneticshapes that correspond to well-formed word bound-aries.As is the case in general for our method, thefeature-enhanced unigram model still respects theconditional independence assumptions that the stan-dard unigram model makes, and inference is stillperformed using a simple dynamic program to com-pute expected sufficient statistics, which are just seg-ment counts.7.4 Segmentation ResultsTo our knowledge our system achieves the best per-formance to date on the Bernstein-Ratner corpus,with an F1 of 88.0.
It is substantially simpler thanthe non-parametric Bayesian models proposed byJohnson et al (2007), which require sampling pro-cedures to perform inference and achieve an F1 of87 (Johnson and Goldwater, 2009).
Similar to ourother results, the direct gradient approach outper-forms EM for feature-enhanced models, and bothapproaches outperform the baseline, which achievesan F1 of 76.9.8 ConclusionWe have shown that simple, locally normalizedmodels can effectively incorporate features into un-supervised models.
These enriched models canbe easily optimized using standard NLP build-ing blocks.
Beyond the four tasks explored inthis paper?POS tagging, DMV grammar induc-tion, word alignment, and word segmentation?themethod can be applied to many other tasks, for ex-ample grounded semantics, unsupervised PCFG in-duction, document clustering, and anaphora resolu-tion.AcknowledgementsWe thank Percy Liang for making his word segmen-tation code available to us, and the anonymous re-viewers for their comments.Appendix: OptimizationIn this section, we derive the gradient of the log marginal likeli-hood needed for the direct gradient approach.
Let w0 be the cur-rent weights in Algorithm 2 and e = e(w0) be the expectationsunder these weights as computed in Equation 2.
In order to jus-tify Algorithm 2, we need to prove that ?L(w0) = ??
(w0, e).We use the following simple lemma: if ?, ?
are real-valuedfunctions such that: (1) ?
(w0) = ?
(w0) for some w0; (2)?
(w) ?
?
(w) on an open set containing w0; and (3), ?
and ?are differentiable at w0; then ??
(w0) = ??
(w0).We set ?
(w) = L(w) and ?
(w) = ?
(w, e)?PzPw0(Z =z|Y = y) log Pw0(Z = z|Y = y).
If we can show that ?, ?satisfy the conditions of the lemma we are done since the secondterm of ?
depends on w0, but not on w.Property (3) can be easily checked, and property (2) followsfrom Jensen?s inequality.
Finally, property (1) follows fromLemma 2 of Neal and Hinton (1998).589ReferencesN.
F. Ayan, B. Dorr, and C. Monz.
2005.
Combiningword alignments using neural networks.
In EmpiricalMethods in Natural Language Processing.N.
Bernstein-Ratner.
1987.
The phonology of parent-child speech.
K. Nelson and A. van Kleeck.M.
Bisani and H. Ney.
2008.
Joint-sequence models forgrapheme-to-phoneme conversion.A.
Bouchard-Co?te?, P. Liang, D. Klein, and T. L. Griffiths.2008.
A probabilistic approach to language change.In Neural Information Processing Systems.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1994.
The mathematics of statisticalmachine translation: Parameter estimation.
Computa-tional Linguistics.S.
F. Chen.
2003.
Conditional and joint models forgrapheme-to-phoneme conversion.
In Eurospeech.S.
B. Cohen and N. A. Smith.
2009.
Shared logistic nor-mal distributions for soft parameter tying in unsuper-vised grammar induction.
In North American Chapterof the Association for Computational Linguistics.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.Maximum likelihood from incomplete data via the EMalgorithm.
Journal of the Royal Statistical Society.
Se-ries B (Methodological).J.
DeNero and D. Klein.
2007.
Tailoring word align-ments to syntactic machine translation.
In Associationfor Computational Linguistics.D.
Elworthy.
1994.
Does Baum-Welch re-estimationhelp taggers?
In Association for Computational Lin-guistics.S.
Goldwater and T. L. Griffiths.
2007.
A fully Bayesianapproach to unsupervised part-of-speech tagging.
InAssociation for Computational Linguistics.S.
Goldwater, T. L. Griffiths, and M. Johnson.
2006.Contextual dependencies in unsupervised word seg-mentation.
In International Conference on Computa-tional Linguistics/Association for Computational Lin-guistics.A.
Haghighi and D. Klein.
2006.
Prototype-driven learn-ing for sequence models.
In Association for Computa-tional Linguistics.A.
Haghighi, J. Blitzer, J. DeNero, and D. Klein.
2009.Better word alignments with supervised ITG models.In Association for Computational Linguistics.M.
Johnson and S. Goldwater.
2009.
Improving non-parametric Bayesian inference: Experiments on unsu-pervised word segmentation with adaptor grammars.In North American Chapter of the Association forComputational Linguistics.M.
Johnson, T. L. Griffiths, and S. Goldwater.
2007.Adaptor grammars: a framework for specifying com-positional nonparametric Bayesian models.
In NeuralInformation Processing Systems.M.
Johnson.
2007.
Why doesnt EM find good HMMPOS-taggers?
In Empirical Methods in Natural Lan-guage Processing/Computational Natural LanguageLearning.D.
Klein and C. D. Manning.
2004.
Corpus-based in-duction of syntactic structure: Models of dependencyand constituency.
In Association for ComputationalLinguistics.P.
Liang and D. Klein.
2009.
Online EM for unsuper-vised models.
In North American Chapter of the As-sociation for Computational Linguistics.P.
Liang, B. Taskar, and D. Klein.
2006.
Alignment byagreement.
In North American Chapter of the Associ-ation for Computational Linguistics.D.
C. Liu, J. Nocedal, and C. Dong.
1989.
On the limitedmemory BFGS method for large scale optimization.Mathematical Programming.M.
P. Marcus, M. A. Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of English:the penn treebank.
Computational Linguistics.B.
Merialdo.
1994.
Tagging English text with a proba-bilistic model.
Computational Linguistics.R.
Neal and G. E. Hinton.
1998.
A view of the EMalgorithm that justifies incremental, sparse, and othervariants.
In Learning in Graphical Models.
KluwerAcademic Publishers.H.
Ney and S. Vogel.
1996.
HMM-based word alignmentin statistical translation.
In International Conferenceon Computational Linguistics.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
In Association for Computational Lin-guistics.R.
Salakhutdinov, S. Roweis, and Z. Ghahramani.
2003.Optimization with EM and expectation-conjugate-gradient.
In International Conference on MachineLearning.N.
A. Smith and J. Eisner.
2005.
Contrastive estimation:Training log-linear models on unlabeled data.
In As-sociation for Computational Linguistics.N.
A. Smith and M. Johnson.
2007.
Weighted and prob-abilistic context-free grammars are equally expressive.Computational Linguistics.I.
G. Varea, F. J. Och, H. Ney, and F. Casacuberta.
2002.Refined lexicon models for statistical machine transla-tion using a maximum entropy approach.
In Associa-tion for Computational Linguistics.N.
Xue, F-D Chiou, and M. Palmer.
2002.
Building alarge-scale annotated Chinese corpus.
In InternationalConference on Computational Linguistics.590
