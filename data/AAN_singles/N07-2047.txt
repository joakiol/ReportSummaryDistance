Proceedings of NAACL HLT 2007, Companion Volume, pages 185?188,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsKernel Regression Based Machine TranslationZhuoran Wang and John Shawe-TaylorDepartment of Computer ScienceUniversity College LondonLondon, WC1E 6BTUnited Kingdom{z.wang, jst}@cs.ucl.ac.ukSandor SzedmakSchool of Electronics and Computer ScienceUniversity of SouthamptonSouthampton, SO17 1BJUnited Kingdomss03v@ecs.soton.ac.ukAbstractWe present a novel machine translationframework based on kernel regressiontechniques.
In our model, the translationtask is viewed as a string-to-string map-ping, for which a regression type learningis employed with both the source and thetarget sentences embedded into their ker-nel induced feature spaces.
We report theexperiments on a French-English transla-tion task showing encouraging results.1 IntroductionFig.
1 illustrates an example of phrase alignmentfor statistical machine translation (SMT).
A roughlinear relation is shown by the co-occurences ofphrases in bilingual sentence pairs, which motivatesus to introduce a novel study on the SMT task:If we define the feature space Hx of our sourcelanguage X as all its possible phrases (i.e.
informa-tive blended word n-grams), and define the mapping?x : X ?
Hx, then a sentence x ?
X can be ex-pressed by its feature vector ?x(x) ?
Hx.
Eachcomponent of ?x(x) is indexed by a phrase with thevalue being the frequency of it in x.
The definitionof the feature space Hy of our target language Y canbe made in a similar way, with corresponding map-ping ?y : Y ?
Hy.
Now in the machine translationtask, given S = {(xi, yi) : xi ?
X , yi ?
Y, i =1, .
.
.
,m}, a set of sample sentence pairs where yiis the translation of xi, we are trying to learn W amatrix represented linear operator, such that:?y(y) = f(x) = W?x(x) (1)we returnto markedquestionsmarqu?esnousrevenousauxquestionsFigure 1: Phrase alignment in SMTto predict the translation y for a new sentence x.Comparing with traditional methods, this modelgives us a theoretical framework to capture higher-dimensional dependencies within the sentences.
Tosolve the multi-output regression problem, we inves-tigate two models, least squares regression (LSR)similar to the technique presented in (Cortes et al,2005), and maximum margin regression (MMR) in-troduced in (Szedmak et al, 2006).The rest of the paper is organized as follows.
Sec-tion 2 gives a brief review of the regression models.Section 3 details the solution to the pre-image prob-lem.
We report the experimental results in Section4, with discussions in Section 5.2 Kernel Regression with Vector Outputs2.1 Kernel Induced Feature SpaceIn the practical learning process, only the inner prod-ucts of the feature vectors are needed (see Section2.2, 2.3 and 3), so we can perform the so-calledkernel trick to avoid dealing with the very high-dimensional feature vectors explicitly.
That is, forx, z ?
X , a kernel function is defined as:?x(x, z) = ??x(x),?x(z)?
= ?x(x)?
?x(z) (2)185Similarly, a kernel function ?y(?, ?)
is defined in Hy.In our case, the blended n-spectrum string ker-nel (Lodhi et al, 2002) that compares two stringsby counting how many (contiguous) substrings oflength from 1 up to n they have in common, is a goodchoice for the kernel function to induce our featurespaces Hx and Hy implicitly, even though it bringsin some uninformative features (word n-grams) aswell, when compared to our original definition.2.2 Least Squares RegressionA basic method to solve the problem in Eq.
1 is leastsquares regression that seeks the matrix W mini-mizing the squared loss in Hy on the training set S:min ?WMx ?My?2F (3)where Mx = [?x(x1), ...,?x(xm)], My =[?y(y1), ...,?y(ym)], and ?
?
?F denotes the Frobe-nius norm.Differentiating the expression and setting it tozero gives:2WMxM?x ?
2MyM?x = 0?
W = MyK?1x M?x (4)where Kx = M?x Mx = (?x(xi, xj)1?i,j?m) is theGram matrix.2.3 Maximum Margin RegressionAn alternative solution to our regression learn-ing problem is proposed in (Szedmak et al,2006), called maximum margin regression.
If L2-normalized feature vectors are used in Eq.
1, de-noted by ??x(?)
and ??y(?
), MMR solves the follow-ing optimization:min 12?W?2F + Cm?i=1?i (5)s.t.
???y(yi),W?
?x(xi)?Hy ?
1?
?i,?i > 0, i = 1, .
.
.
,m.where C > 0 is the regularization coefficient, and?i are the slack variables.
The Lagrange dual formwith dual variables ?i gives:minm?i,j=1?i?j ?
?x(xi, xj)?
?y(yi, yj)?m?i=1?is.t.
0 ?
?i ?
C, i = 1, .
.
.
,m. (6)where ?
?x(?, ?)
and ?
?y(?, ?)
denote the kernel func-tions associated to the respective normalized featurevectors.This dual problem can be solved efficiently witha perceptron algorithm based on an incrementalsubgradient method, of which the bounds on thecomplexity and achievable margin can be found in(Szedmak et al, 2006).Then according to Karush-Kuhn-Tucker theory,W is expressed as:W =m?i=1?i??y(yi)??x(xi)?
(7)In practice, MMR works better when the distribu-tion of the training points are symmetrical.
So wecenter the data before normalizing them.
If ?Sx =1m?mi=1 ?x(xi) is the centre of mass of the sourcesentence sample set {xi} in the feature space, thenew feature map is given by ??x(?)
= ?x(?)
?
?Sx .The similar operation is performed on ?y(?)
to ob-tain ??y(?).
Then the L2-normalizations of ??x(?)
and??y(?)
yield our final feature vectors ??x(?)
and ??y(?
).3 Pre-image SolutionTo find the pre-image sentence y = f?1(x) can beachieved by seeking yt that has the minimum lossbetween its feature vector ?y(yt) and our predictionf(x).
That is (Eq.
8: LSR, Eq.
9: MMR):yt = arg miny?Y(x)?W?x(x)?
?y(y)?2= arg miny?Y(x)?y(y, y)?
2ky(y)K?1x kx(x) (8)yt = arg miny?Y(x)1?
???y(y),W?
?x(x)?Hy= arg maxy?Y(x)m?i=1?i?
?y(yi, y)?
?x(xi, x) (9)where Y(x) ?
Y is a finite set covering all po-tential translations for the given source sentencex, and kx(?)
= (?x(?, xi)1?i?m) and ky(?)
=(?y(?, yi)1?i?m) are m?
1 column matrices.A proper Y(x) can be generated according to alexicon that contains possible translations for everycomponent (word or phrase) in x.
But the size of itwill grow exponentially with the length of x, whichposes implementation problem for a decoding algo-rithm.186In earlier systems, several heuristic search meth-ods were developed, of which a typical exampleis Koehn (2004)?s beam search decoder for phrase-based models.
However, in our case, because of the?y(y, y) item in Eq.
8 and the normalization opera-tion in MMR, neither the expression in Eq.
8 northe one in Eq.
9 can be decomposed into a sumof subfunctions each involving feature componentsin a local area only.
It means we cannot estimateexactly how well a part of the source sentence istranslated, until we obtain a translation for the entiresentence, which prevents us doing a straightforwardbeam search similar to (Koehn, 2004).To simplify the situation, we restrict the reorder-ing (distortion) of phrases that yield the output sen-tences by only allowing adjacent phrases to ex-change their positions.
(The discussion of this strat-egy can be found in (Tillmann, 2004).)
We use x[i:j]and y[i:j] to denote the substrings of x and y that be-gin with the ith word and end with the jth.
Now, ifwe go back to the implementation of a beam search,the current distortion restriction guarantees that ineach expansion of the search states (hypotheses) wehave x[1:lx] translated to a y[1:ly], either like state (a)or like state (b) in Fig.
2, where lx is the number ofwords translated in the source sentence, and ly is thenumber of words obtained in the translation.We assume that if y is a good translation of x,then y[1:ly] is a good translation of x[1:lx] as well.
Sowe can expect that the squared loss ?W?x(x[1:lx])?
?y(y[1:ly])?2 in the LSR is small, or the inner prod-uct ???y(y[1:ly]),W?
?x(x[1:lx])?Hy in the MMR islarge, for the hypothesis yielding a good translation.According to Eq.
8 and Eq.
9, the hypotheses in thesearch stacks can thus be reranked with the follow-ing score functions (Eq.
10: LSR, Eq.
11: MMR):Score(x[1:lx], y[1:ly]) = (10)?y(y[1:ly], y[1:ly])?
2ky(y[1:ly])K?1x kx(x[1:lx])Score(x[1:lx], y[1:ly]) =m?i=1?i?
?y(yi, y[1:ly])?
?x(xi, x[1:lx]) (11)Therefore, to solve the pre-image problem, wejust employ the same beam search algorithm as(Koehn, 2004), except we limit the derivation of newhypotheses with the distortion restriction mentionednous revenous aux questionswe return to questionsmarqu?es ?
(a)(b)marked?nous revenous aux questionswe return to questionsmarqu?esFigure 2: Search states with the limited distortion.above.
However, our score functions will bringmore runtime complexities when compared with tra-ditional probabilistic methods.
The time complexityof a naive implementation of the blended n-spectrumstring kernel between two sentences si and sj isO(n|si||sj|), where |?| denotes the length of the sen-tence.
So the score function in Eq.
11 results in anaverage runtime complexity of O(mnlyl), where l isthe average length of the sentences yi in the trainingset.
Note here ?
?x(x[1:lx], xi) can be pre-computedfor lx from 1 to |x| before the beam search, whichcalls for O(m|x|) space.
The average runtime com-plexity of the score function in Eq.
10 will be thesame if we pre-compute K?1x kx(x[1:lx]).4 Experimental Results4.1 Resource DescriptionBaseline System To compare with previous work,we take Pharaoh (Koehn, 2004) as a baseline system,with its default settings (translation table size 10,beam size 100).
We train a trigram language modelwith the SRILM toolkit (Stocke, 2002).
Whilst, theparameters for the maximum entropy model are de-veloped based on the minimum error rate trainingmethod (Och, 2003).In the following experiments, to facilitate com-parison, each time we train our regression modelsand the language model and translation model forPharaoh on a common corpus, and use the samephrase translation table as Pharaoh?s to decode oursystems.
According to our preliminary experiments,with the beam size of 100, the search errors of oursystems can be limited within 1.5%.Corpora To evaluate our models, we randomlytake 12,000 sentences from the French-English por-tion of the 1996?2003 Europarl corpus (Koehn,2005) for scaling-up training, 300 for test (Test), and300 for the development of Pharaoh (Dev).
Some187Vocabulary Words PerplexityFr En Fr En Dev Test4k 5084 4039 43k 39k 32.25 31.926k 6426 5058 64k 59k 30.81 29.038k 7377 5716 85k 79k 29.91 28.9410k 8252 6339 106k 98k 27.55 27.0912k 9006 6861 127k 118k 27.19 26.41Table 1: Statistics of the corpora.characteristics of the corpora are summarized in Ta-ble 1.4.2 ResultsBased on the 4k training corpus, we test the per-formance of the blended n-spectrum string kernel inLSR and MMR using BLEU score, with n increas-ing from 2 to 7.
Fig.
3 shows the results.
It can befound that the performance becomes stable when nreaches a certain value.
Finally, we choose the 3-spectrum for LSR, and the 5-spectrum for MMR.Then we scale up the training set, and compare theperformance of our models with Pharaoh in Fig.
4.We can see that the LSR model performs almost aswell as Pharaoh, whose differences of BLEU scoreare within 0.5% when the training set is larger than6k.
But MMR model performs worse than the base-line.
With the training set of 12k, it is outperformedby Pharaoh by 3.5%.5 DiscussionsAlthough at this stage the main contribution isstill conceptual, the capability of our approach tobe applied to machine translation is still demon-strated.
Comparable performance to previous workis achieved by the LSR model.But a main problem we face is to scale-up thetraining set, as in practice the training set for SMTwill be much larger than several thousand sentences.A method to speed up the training is proposed in(Cortes et al, 2005).
By approximating the Grammatrix with a n ?
m (n ?
m) low-rank matrix,the time complexity of the matrix inversion opera-tion can be reduced from O(m3) to O(n2m).
Butthe space complexity of O(nm) in their algorithm isstill too expensive for SMT tasks.
Subset selectiontechniques could give a solution to this problem, of2 3 4 5 6 7262830323436384042MMRLSRFigure 3: BLEU(%) versus n-spectrum4000 6000 8000 10000 120003032343638404244PharaohLSRMMRFigure 4: BLEU(%) versus training set sizewhich we will leave the further exploration to futurework.AcknowledgementsThe authors acknowledge the support of the EU un-der the IST project No.
FP6-033917.ReferencesC.
Cortes, M. Mohri, and J. Weston.
2005.
A general re-gression technique for learning transductions.
In Proc.of ICML?05.P.
Koehn.
2004.
Pharaoh: A beam search decoderfor phrase-based statistical machine translation mod-els.
In Proc.
of AMTA 2004.P.
Koehn.
2005.
Europarl: A parallel corpus for statisti-cal machine translation.
In MT Summit X.H.
Lodhi, C. Saunders, J. Shawe-Taylor, N. Cristianini,and C. Watkins.
2002.
Text classification using stringkernels.
J. Mach.
Learn.
Res., 2:419?444.F.
J. Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proc.
of ACL?03.A.
Stocke.
2002.
SRILM ?
an extensible language mod-eling toolkit.
In Proc.
of ICSLP?02.S.
Szedmak, J. Shawe-Taylor, and E. Parado-Hernandez.2006.
Learning via linear operators: Maximum mar-gin regression; multiclass and multiview learning atone-class complexity.
Technical report, PASCAL,Southampton, UK.C.
Tillmann.
2004.
A unigram orientation modelfor statistical machine translation.
In Proc.
of HLT-NAACL?04.188
