Proceedings of NAACL HLT 2007, Companion Volume, pages 65?68,Rochester, NY, April 2007. c?2007 Association for Computational Linguistics?
?ROVER: Improving System Combination with ClassificationD.
Hillard?, B.
Hoffmeister?, M.
Ostendorf?, R.
Schlu?ter?, H.
Ney?
?SSLI, Electrical Engineering Dept., University of Washington, Seattle, WA{hillard,mo}@ee.washington.edu?Informatik 6, Computer Science Dept., RWTH Aachen University, Aachen, Germany{hoffmeister,schlueter,ney}@cs.rwth-aachen.deAbstractWe present an improved system combinationtechnique, ??ROVER.
Our approach obtains sig-nificant improvements over ROVER, and isconsistently better across varying numbers ofcomponent systems.
A classifier is trained onfeatures from the system lattices, and selectsthe final word hypothesis by learning cues tochoose the system that is most likely to becorrect at each word location.
This approachachieves the best result published to date onthe TC-STAR 2006 English speech recognitionevaluation set.1 IntroductionState-of-the-art automatic speech recognition (ASR) sys-tems today usually include multiple contrasting systems,which are ultimately combined to produce the final hy-pothesis.
There is consensus that improvements fromcombination are usually best when systems are suffi-ciently different, but there is uncertainty about which sys-tem combination method performs the best.
In addition,the success of commonly used combination techniquesvaries depending on the number of systems that are com-bined (Hoffmeister et al, 2007).
In this work, we developa system combination method that outperforms all previ-ously known techniques and is also robust to the numberof component systems.
The relative improvements overROVER are particularly large for combination when onlyusing two systems.The aim of system combination for ASR is to mini-mize the expected word error rate (WER) given multiplesystem outputs, which are ideally annotated with wordconfidence information.
The most widely used systemcombination approach to date is ROVER (Fiscus, 1997).It is a simple voting mechanism over just the top hy-pothesis from each component system.
Two alternativesthat incorporate information about multiple hypothesesand leverage word posterior probabilities are confusionnetwork (CN) combination (Mangu et al, 2000; Ever-mann and Woodland, 2000) and minimum Time FrameWord Error (min-fWER) decoding (Hoffmeister et al,2006), discussed further in the next section.
Previouswork found that among ROVER, CN combination, andmin-fWER combination, no one method was consistentlysuperior across varying numbers and types of systems(Hoffmeister et al, 2007).The main contribution of this work is to develop anapproach that always outperforms other possible systemcombination methods.
We train a classifier to learn whichsystem should be selected for each output word, usingfeatures that describe the characteristics of the compo-nent systems.
ROVER alignments on the 1-best hypothe-ses are used for decoding, but many of the features arederived from the system lattices.
The classifier learns aselection strategy (i.e.
a decision function) from a devel-opment set and then is able to make better selections onthe evaluation data then the current 1-best or lattice-basedsystem combination approaches.Next, Section 2 describes previous work in systemcombination techniques.
Section 3 describes our ap-proach, and Section 4 provides experiments and results.Finally, we summarize the approach and findings in Sec-tion 5.2 Previous WorkPrevious work in speech recognition system combinationhas produced significant improvements over the resultspossible with just a single system.
The most popular, andoften best performing method is ROVER (Fiscus, 1997),which selects the word that the most systems agree onat a particular location (majority voting).
An extendedversion of ROVER also weights system votes by the wordconfidence produced by the system (confidence voting).Further improvements have been achieved by includ-ing multiple system alternatives, with methods such asConfusion Network Combination (CNC) (Evermann andWoodland, 2000), or N-Best ROVER (Stolcke et al,2000), which is a special case of CNC.
Alternatively, thecombination can be performed at the frame level (min-fWER) (Hoffmeister et al, 2006).
Recent work foundthat the best system combination method depended on thenumber of systems being combined (Hoffmeister et al,2007).
When only two systems are available, approachesconsidering multiple alternatives per system were bet-65ter, but as the number of systems increased the standardROVER with confidence scores was more robust andsometimes even better than CNC or min-fWER combi-nation.Another approach (Zhang and Rudnicky, 2006) usedtwo stages of neural networks to select a system at eachword, with features that capture word frequency, posteri-ors at the frame, word, and utterance level, LM back-offmode, and system accuracy.
They obtained consistent butsmall improvements over ROVER: between 0.7 and 1.7%relative gains for systems with about 30% WER.3 ApproachWe develop a system that uses the ROVER alignment butlearns to consistently make better decisions than thoseof standard ROVER.
We call the new system ?
?ROVER,where the ??
stands for improved results, and/or intelligentdecisions.
The following sections discuss the compo-nents of our approach.
First, we emulate the approachof ROVER in our lattice preprocessing and system align-ment.
We then introduce new methods to extract hypoth-esis features and train a classifier that selects the bestsystem at each slot in the alignment.3.1 Lattice PreparationOur experiments use lattice sets from four different sites.Naturally, these lattice sets differ in their vocabulary,segmentation, and density.
A compatible vocabulary isessential for good combination performance.
The mainproblems are related to contractions, e.g.
?you?ve?
and?you have?, and the alternatives in writing foreign names,e.g.
?Schro?der?
and ?Schroder?.
In ASR this problem iswell-known and is addressed in scoring by using map-pings that allow alternative forms of the same word.Such a mapping is provided within the TC-STAR Eval-uation Campaign and we used it to normalize the lat-tices.
In case of multiple alternative forms we used onlythe most frequent one.
Allowing multiple parallel alter-natives would have distorted the posterior probabilitiesderived from the lattice.
Furthermore, we allowed onlyone-to-one or one-to-many mappings.
In the latter casewe distributed the time of the lattice arc according to thecharacter lengths of the target words.In order to create comparable posterior probabilitiesover the lattice sets we pruned them to equal averagedensity.
The least dense lattice set defined the targetdensity: around 25 for the development and around 30for the evaluation set.Finally, we unified the segmentation by concatenat-ing the lattices recording-wise.
The concatenation wascomplicated by segmentations with overlapping regions,but our final concatenated lattices scored equally to theoriginal lattice sets.
The unified segmentation is neededfor lattice-based system combination methods like frame-based combination.3.2 System AlignmentsIn this work we decided to use the ROVER alignment asthe basis for our system combination approach.
At firstglance the search space used by ROVER is very limitedbecause only the first-best hypothesis from each compo-nent system is used.
But the oracle error rate is often verylow, normally less than half of the best system?s error rate.The ROVER alignment can be interpreted as a confu-sion network with an equal number of arcs in each slot.The number of arcs per slot equals the number of compo-nent systems and thus makes the training and applicationof a classifier straightforward.For the production of the alignments we use a stan-dard, dynamic programming-based matching algorithmthat minimizes the global cost between two hypothesis.The local cost function is based on the time overlap oftwo words and is identical to the one used by the ROVERtool.
We also did experiments with alternative local costfunctions based on word equalities, but could not outper-form the simple, time overlap-based distance function.3.3 Hypothesis FeaturesWe generate a cohort of features for each slot in thealignment, which is then used as input to train the classi-fier.
The features incorporate knowledge about the scoresfrom the original systems, as well as comparisons amongeach of the systems.
The following paragraphs enumeratethe six classes of feature types used in our experiments(with their names rendered in italics).The primary, and most important feature class coversthe basic set of features which indicate string matchesamong the top hypotheses from each system.
In addition,we include the systems?
frame-based word confidence.These features are all the information available to thestandard ROVER with confidences voting.An additional class of features provides extended con-fidence information about each system?s hypothesis.
Thisfeature class includes the confusion network (CN) wordconfidence, CN slot entropy, and the number of alter-natives in the CN slot.
The raw language model andacoustic scores are also available.
In addition, it in-cludes a frame-based confidence that is computed fromonly the acoustic model, and a frame-based confidencethat is computed from only the language model score.Frame-based confidences are calculated from the latticesaccording to (Wessel et al, 1998); the CN-algorithm isan extension of (Xue and Zhao, 2005).The next class of features describes durational aspectsof the top hypothesis for each system, including: charac-ter length, frame duration, frames per character, and if theword is the empty or null word.
A feature that normalizesthe frames per character by the average over a windowof ten words is also generated.
Here we use charactersas a proxy for phones, because phone information is notavailable from all component systems.We also identify the system dependent top error wordsfor the development set, as well as the words that occurto the left and right of the system errors.
We encode thisinformation by indicating if a system word is on the listof top ten errors or the top one hundred list, and likewiseif the left or right system context word is found in theircorresponding lists.In order to provide comparisons across systems, wecompute the character distance (the cost of aligning thewords at the character level) between the system words66and provide that as a feature.
In addition, we include theconfidence of a system word as computed by the frame-wise posteriors of each of the other systems.
This allowseach of the other systems to ?score?
the hypothesis ofa system in question.
These cross-system confidencescould also act as an indicator for when one system?s hy-pothesis is an OOV-word for another system.
We alsocompute the standard, confidence-based ROVER hypoth-esis at each slot, and indicate whether or not a systemagrees with ROVER?s decision.The last set of features is computed relative to thecombined min-fWER decoding.
A confidence for eachsystem word is calculated from the combined frame-wiseposteriors of all component systems.
The final featureindicates whether each system word agrees with the com-bined systems?
min-fWER hypothesis.3.4 ClassifierAfter producing a set of features to characterize the sys-tems, we train a classifier with these features that willdecide which system will propose the final hypothesis ateach slot in the multiple alignment.
The target classesinclude one for each system and a null class (which isselected when none of the system outputs are chosen, i.e.a system insertion).The training data begins with the multiple alignmentof the hypothesis systems, which is then aligned to thereference words.
The learning target for each slot is theset of systems which match the reference word, or thenull class if no systems match the reference word.
Onlyslots where there is disagreement between the systems?1-best hypotheses are included in training and testing.The classifier for our work is Boostexter (Schapire andSinger, 2000) using real Adaboost.MH with logistic loss(which outperformed exponential loss in preliminary ex-periments).
Boostexter trains a series of weak classifiers(tree stumps), while also updating the weights of eachtraining sample such that examples that are harder toclassify receive more weight.
The weak classifiers arethen combined with the weights learned in training topredict the most likely class in testing.
The main dimen-sions for model tuning are feature selection and numberof iterations, which are selected on the development setas described in the next section.4 ExperimentsWe first perform experiments using cross-validation onthe development set to determine the impact of differentfeature classes, and to select the optimal number of iter-ations for Boostexter training.
We then apply the modelsto the evaluation set.4.1 Experimental setupIn our experiments we combine lattice sets for the Englishtask of the TC-STAR 2006 Evaluation Campaign fromfour sites.
The TC-STAR project partners kindly pro-vided RWTH their development and evaluation lattices.Systems and lattice sets are described in (Hoffmeister etal., 2007).Table 1 summarizes the best results achieved on thesingle lattice sets.
The latter columns show the results ofViterbi min-fWER CNdev eval dev eval dev eval1 10.5 9.0 10.3 8.6 10.4 8.62 11.4 9.0 11.4 9.5 11.6 9.13 12.8 10.4 12.5 10.4 12.6 10.24 13.9 11.9 13.9 11.8 13.9 11.8Table 1: WER[%] results for single systems.CN and min-fWER based posterior decoding (Mangu etal., 2000; Wessel et al, 2001).4.2 Feature analysis on development dataWe evaluate the various feature classes from Section 3.3on the development set with a cross validation testingstrategy.
The results in Tables 2 and 3 are generatedwith ten-fold cross validation, which maintains a cleanseparation of training and testing data.
The total numberof training samples (alignment slots where there is systemdisagreement) is about 3,700 for the 2 system case, 5,500for the 3 system case, and 6,800 for the 4 system case.The WER results for different feature conditions on thedevelopment set are presented in Table 2.
The typicalROVER with word confidences is provided in the firstrow for comparison, and the remainder of the rows con-tain the results for various configurations of features thatare made available to the classifier.The basic features are just those that encode the sameinformation as ROVER, but the classifier is still able tolearn better decisions than ROVER with only these fea-tures.
Each of the following rows provides the results foradding a single feature class to the basic features, so thatthe impact of each type can be evaluated.The last two rows contain combinations of featureclasses.
First, the best three classes are added, and thenall features.
Using just the best three classes achievesalmost the best results, but a small improvement is gainedwhen all features are added.
The number of iterations intraining is also optimized on the development set by se-lecting the number with the lowest average classificationerror across the ten splits of the training data.Features 2 System 3 System 4 SystemROVER 10.2% 8.8% 9.0%basic 9.4% 8.6% 8.5%+confidences 9.3% 8.7% 8.4%+durational 9.2% 8.6% 8.4%+top error 9.0% 8.5% 8.4%+comparisons 8.9% 8.6% 8.4%+min-fWER 8.5% 8.5% 8.4%+top+cmp+fWER 8.3% 8.3% 8.2%all features 8.3% 8.2% 8.2%Table 2: WER results for development data with differentfeature classes.6788.599.51010.5114321[%] WERROVER(maj.)ROVER(conf.
)min-fWERiROVER2 System 3 System 4 SystemROVER (maj.) 10.8% 9.1% 9.1%ROVER (conf.)
10.1% 8.8% 9.0%min-fWER 9.6% 9.2 % 8.9 %?
?ROVER 8.3% 8.2% 8.2%oracle 6.5% 5.4% 4.7%Table 3: WER[%] results for development data withmanual segmentation, and using cross-validation for?
?ROVER.4.3 Results on evaluation dataAfter analyzing the features and selecting the optimalnumber of training iterations on the development data,we train a final model on the full development set andthen apply it to the evaluation set.
In all cases our clas-sifier achieves a lower WER than ROVER (statisticallysignificant by NIST matched pairs test).
Table 3 and Ta-ble 4 present a comparison of the ROVER with majorityvoting, confidence voting, frame-based combination, andour improved ROVER (?
?ROVER).5 ConclusionsIn summary, we develop ?
?ROVER, a method for sys-tem combination that outperforms ROVER consistentlyacross varying numbers of component systems.
The rela-tive improvement compared to ROVER is especially largefor the case of combining two systems (14.5% on theevaluation set).
The relative improvements are larger thanany we know of to date, and the four system case achievesthe best published result on the TC-STAR English evalu-ation set.
The classifier requires relatively little trainingdata and utilizes features easily available from systemlattices.Future work will investigate additional classifiers, clas-sifier combination, and expanded training data.
We arealso interested in applying a language model to decodean alignment network that has been scored with our clas-sifier.ReferencesG.
Evermann and P. Woodland.
2000.
Posterior probabilitydecoding, confidence estimation and system combination.
InNIST Speech Transcription Workshop.6.577.588.599.54321[%] WERROVER(maj.)ROVER(conf.
)min-fWERiROVER2 System 3 System 4 SystemROVER(maj.) 9.0% 7.2% 7.3%ROVER(conf.)
8.2% 7.1% 7.0%min-fWER 7.6 % 7.4 % 7.2 %?
?ROVER 7.1% 6.9% 6.7%oracle 5.2% 4.1% 3.6%Table 4: WER[%] results for evaluation data.J.G.
Fiscus.
1997.
A post-processing system to yield reducedword error rates: Recognizer Output Voting Error Reduction(ROVER).
In Proc.
ASRU.B.
Hoffmeister, T. Klein, R. Schlu?ter, and H. Ney.
2006.
Framebased system combination and a comparison with weightedROVER and CNC.
In Proc.
ICSLP.B.
Hoffmeister, D. Hillard, S. Hahn, R. Schu?lter, M. Ostendorf,and H. Ney.
2007.
Cross-site and intra-site ASR systemcombination: Comparisons on lattice and 1-best methods.
InProc.
ICASSP.L.
Mangu, E. Brill, and A. Stolcke.
2000.
Finding consensusin speech recognition: word error minimization and otherapplications of confusion networks.
Computer Speech andLanguage, 14:373?400.R.
E. Schapire and Y.
Singer.
2000.
Boostexter: A boosting-based system for text categorization.
Machine Learning,39(2/3):135?168.A.
Stolcke, H. Bratt, J. Butzberger, H. Franco, V. Gadde,M.
Plauche, C. Richey, E. Shriberg, K. Sonmez, J. Zheng,and F. Weng.
2000.
The SRI March 2000 Hub-5 conver-sational speech transcription system.
In NIST Speech Tran-scription Workshop.F.
Wessel, K. Macherey, and R. Schlu?ter.
1998.
Using wordprobabilities as confidence measures.
In Proc.
ICASSP.F.
Wessel, R. Schlu?ter, and H. Ney.
2001.
Explicit word errorminimization using word hypothesis posterior probabilities.In Proc.
ICASSP, volume 1.Jian Xue and Yunxin Zhao.
2005.
Improved confusion networkalgorithm and shortest path search from word lattice.
InProc.
ICASSP.R.
Zhang and A. Rudnicky.
2006.
Investigations of issuesfor using multiple acoustic models to improve continuousspeech recognition.
In Proc.
ICSLP.68
