Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 100?103,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsCorry: A System for Coreference ResolutionOlga UryupinaCiMeC, University of Trentouryupina@gmail.comAbstractCorry is a system for coreference resolutionin English.
It supports both local (Soon etal.
(2001)-style) and global (Integer LinearProgramming, Denis and Baldridge (2007)-style) models of coreference.
Corry relies on arich linguistically motivated feature set, whichhas, however, been manually reduced to 64features for efficiency reasons.
Three runshave been submitted for the SemEval task 1on Coreference Resolution (Recasens et al,2010), optimizing Corry?s performance forBLANC (Recasens and Hovy, in prep), MUC(Vilain et al, 1995) and CEAF (Luo, 2005).Corry runs have shown the best performancelevel among all the systems in their track forthe corresponding metric.1 IntroductionCorry is a system for coreference resolution in En-glish.
It supports both local (Soon et al (2001)-style)and global (ILP, Denis and Baldridge (2007)-style)models of coreference.
The backbone of the systemis a family of SVM classifiers for pairs of mentions:each mention type receives its own classifier.
A sep-arate anaphoricity classifier is learned for the ILPsetting.
Corry relies on a rich linguistically moti-vated feature set, which has, however, been manu-ally reduced to 64 features for efficiency reasons.Corry has only participated in the ?open?
setting,as it has already a number of preprocessing mod-ules integrated into the system: the Stanford NLPtoolkit for parsing (Klein and Manning, 2003) andNE-tagging (Finkel et al, 2005), Wordnet for se-mantic classes and the U.S. census data for assigninggender values to person names.Three runs have been submitted for the Se-mEval task 1 on Coreference Resolution, optimizingCorry?s performance for BLANC, MUC and CEAF.The runs differ with respect to the model (local forBLANC, global for MUC and CEAF) and the defi-nition of mention types.2 Preprocessing and Mention ExtractionIn our previous study (Uryupina, 2008) we haveshown that up to 35% recall and 20% precision er-rors in coreference resolution for MUC corpora aredue to inaccurate mention detection.
We have there-fore invested substantial efforts into our mention de-tection module.Most state-of-the-art coreference resolution sys-tems operate either on gold markables or on theoutput of an ACE-style mention detection module.We are not aware of extensive studies on mentionextraction algorithms for such datasets as SemEval(OntoNotes) where mentions are complex NPs notconstrained with respect to their semantic types.We rely on the Stanford NLP toolkit for extract-ing named entities (Finkel et al, 2005) and parsetrees for each sentence (Klein and Manning, 2003).We then merge the output of the NE-tagger and theparser to create a list of mentions in the followingway:1.
Named entities are considered mentions ifthey correspond to a sequence of parsing con-straints.2.
Pronouns are considered mentions if they arenot a part of an NE-mention.3.
NPs are considered ?candidate mentions?
ifthey are not a part of an NE-mention.
The set of100candidate mentions is then filtered to eliminatepairs of NPs with the same head noun (coor-dinate NPs receive unique artificial heads).
Forpossessive NPs we adjust the boundaries andthe head to exclude the ??s?
token.
The remain-ing candidates are aligned with NE-mentions ?if an NE and an NP have the same last word,they are considered the same mention of a spe-cial type.
Finally, the list of candidates is op-tionally filtered using a small stop-list (for ex-ample, all the ?there?
NPs in ?There is ..?
arediscarded).We rely on the Stanford NLP toolkit, WordNetand the U.S. census data to assign numerous proper-ties to our mentions: semantic type, number, genderand others.3 FeaturesCorry relies on two SVM1 classifiers for coreferenceand anaphoricity.
The former determines whethertwo given mentions Miand Mjare coreferent ornot.
The latter determines whether a given mentionMiis anaphoric or discourse new.
In Section 4 weshow how these classifiers help us build coreferencechains.
We use the SVM-Light package (Joachims,1999) for learning our classifiers.The strength of our system lies in its rich fea-ture set for the coreference classifier.
In our previousstudies (Uryupina, 2006; 2007) we have tested up to351 nominal/continuous (1096 boolean/continuous)features showing significant improvements over ba-sic feature sets advocated in the literature.
For theSemEval task 1, we have reduced our rich feature setto 64 nominal/continuous features for efficiency rea-sons: on the one hand, our new set is large enough tocover complex linguistic patterns of coreference, onthe other hand, it allows us to test different settingsand investigate possibilities for global modeling.Our anaphoricity classifier is used by the ILPmodel.
It relies on 26 boolean/continuous features.More details on the classifier itself can be found in(Uryupina, 2003).1Corry supports a number of machine learning algorithms:C4.5, TiMBL, Ripper, MaxEnt and SVM.
See Uryupina (2006)for a comparison of Corry?s performance with different learners.4 ModelingCorry supports both global and local views of coref-erence.
Our evaluation experiments (cf.
Section 5)show that the choice of a particular model should bemotivated by the desired scoring metric.Our local model of coreference is a reimplementa-tion of the algorithm, proposed by Soon et al (2001)with an extended feature set.
The core of Soon etal.
?s (2001) approach is a link-based classifier: itdetermines whether a given pair of markables arecoreferent or not.
During testing, a greedy cluster-ing algorithm (link-first) is next used to build coref-erence chains on the output of the classifier.We have slightly extended this model to allowseparate classifiers for different mention types: eachcandidate anaphor receives a type (e.g.
?pronoun?
)and is processed with a corresponding classifier.
We,thus, rely on a family of classifiers, with the samefeature set and the same machine learner.
The ex-act definition of mention types is a parameter to bedetermined empirically on the development set.Our global model is largely motivated by Denisand Baldridge (2007; 2008) and Finkel and Manning(2008).
Following these studies, we use Integer Lin-ear Programming to find the most globally optimalsolution, given the decisions made by our corefer-ence and anaphoricity classifiers.In general, an ILP problem is determined by anobjective function to be maximized (or minimized)and a set of task-specific constraints.
The functionis defined by costs link<i,j>, and dnewjreflectingpotential gains and losses for committing to specificvariable assignments.
We assume that costs can bepositive (for pairs of markables that are likely to becoreferent) or negative (for pairs of markables thatare unlikely to be coreferent).
The costs are com-puted by an external module (such as a family of lo-cal classifiers described above).
The objective func-tion then takes the form:max(?<i,j>link<i,j>?
L<i,j>?
?jdnewj?Dj)(1)Binary variables L<i,j>indicate that two mark-ables Miand Mjare coreferent in the output assign-ment.
Binary variables Djindicate that the mark-able Mjis considered anaphoric in the output as-signment.
The ILP solver thus assigns values to101L<i,j>,?i, j : i < j and Dj,?j whilst maximizingthe objective in (1).
We take the transitive closure ofall the proposed L<i,j>to build the output partition.Note that the objective in (1) is not constrainedin any way and will thus allow illegal variable as-signments.
For example it does not constrain theassignment of L and D variables to be consistentwith one another and does not enforce transitivity.The following constraints suggested in the literature(Denis and Baldridge, 2007; Denis and Baldridge,2008; Finkel and Manning, 2008) ensure that theseand other coreference properties are respected:1.
Best-link constraintB :?iL<i,j>?
1,?j (2)2.
Transitivity constraints?i, j, k : i < j < kT : L<i,j>+ L<j,k>?
1 ?
L<i,k>(3)L : L<j,k>+ L<i,k>?
1 ?
L<i,j>(4)R : L<i,j>+ L<i,k>?
1 ?
L<j,k>(5)3.
Anaphoricity constraintsA :?iL<i,j>>= Dj?j (6)D : L<i,j>?
Dj?i, j (7)We refer the reader to the above-mentioned pa-pers for detailed discussions of these constraints andtheir impact on coreference resolution.
As we showin Section 5 below, the usability of a particular con-straint should be determined experimentally basedon the desired system behaviour.5 Evaluation5.1 DevelopmentCorry has participated in the gold and regular opensettings for English.
We have collected a number ofruns on the development data to optimize the per-formance level for a particular score: BLANC (Re-casens and Hovy, in prep), MUC (Vilain et al, 1995)or CEAF (Luo, 2005).
The runs differ with respect tothe model (local vs. global with varying sets of con-straints) and the definition of mention types.
We de-liberately left the B-CUBE score (Bagga and Bald-win, 1998) completely out of our preliminary ex-periments.
The official SemEval scorer was used forthese experiments.Our experiments on the development set showthat no configuration is able to produce equally re-liable scores according to all the metrics (note, forexample, that on the test set the BLANC differencebetween Corry-M and Corry-B in the gold settingis almost 10%).
We believe that it is a challengingpoint for future research.We have selected the best configurations for eachscore and submitted them as separate runs.
TheCorry-C system, optimized for CEAF-?4, is a globalmodel with the L, D and A constraints.
For the goldsetting, mention types are defined as pronouns andnon-pronouns.
For the regular setting, the systemdistinguishes between ?speech?
pronouns, 3rd per-son pronouns, names and nominals.Corry-M, optimized for MUC, is a global modelwith the D constraint and separate classifiers forpronouns, names and nominals.
Note that, comparedto Corry-C, this setting allows for more coreferencelinks ?
it is well known from the literature (cf., forexample, Bagga and Baldwin (1998)) that the MUCmetric is biased towards recall.Finally, Corry-B, optimized for BLANC, is alocal model that distinguishes between pronouns,nominals and names.
The fact that such a simplemodel is able to outperform much more complexversions of Corry strengthens the importance of fea-ture engineering.5.2 TestingTable 1 shows the SemEval task 1 scores for thegold/regular open setting.
Corry has shown reliableperformance for both mention detection and coref-erence resolution.
For mention detection, Corry?s F-score is 4% higher than the one of the competing ap-proach.
For coreference, all the Corry runs yieldedthe best performance level for a score under opti-mization.Finally, for the B-CUBE metric that had not beenoptimized at all, Corry lost only marginally to theRelaxCor system in the gold setting and came firstin the regular setting.6 ConclusionWe have presented Corry ?
a system for coreferenceresolution in English.
Our plans include extending itto cover multiple languages.
However, as the mainstrength of Corry lies in its rich linguistically moti-vated feature set, this remains an issue.102Mention detection CEAF MUC B3 BLANCR P F1 R P F1 R P F1 R P F1 R P F1Language: en, Information: open, Annotation: goldCorry-B 100 100 100 77.5 77.5 77.5 56.1 57.5 56.8 82.6 85.7 84.1 69.3 75.3 71.8Corry-C 100 100 100 77.7 77.7 77.7 57.4 58.3 57.9 83.1 84.7 83.9 71.3 71.6 71.5Corry-M 100 100 100 73.8 73.8 73.8 62.5 56.2 59.2 85.5 78.6 81.9 76.2 58.8 62.7RelaxCor 100 100 100 75.8 75.8 75.8 22.6 70.5 34.2 75.2 96.7 84.6 58.0 83.8 62.7Language: en, Information: open, Annotation: regularBART 76.1 69.8 72.8 70.1 64.3 67.1 62.8 52.4 57.1 74.9 67.7 71.1 55.3 73.2 57.7Corry-B 79.8 76.4 78.1 70.4 67.4 68.9 55.0 54.2 54.6 73.7 74.1 73.9 57.1 75.7 60.6Corry-C 79.8 76.4 78.1 70.9 67.9 69.4 54.7 55.5 55.1 73.8 73.1 73.5 57.4 63.8 59.4Corry-M 79.8 76.4 78.1 66.3 63.5 64.8 61.5 53.4 57.2 76.8 66.5 71.3 58.5 56.2 57.1Table 1: System scores for the gold/regular open setting.
The best F-score for each metric shown in bold.An important advantage of Corry is its flexibility:the system allows for a number of modeling solu-tions that can be tested on the development set tooptimize the performance level for a particular ob-jective.
Our SemEval task 1 results confirm that asystem might benefit a lot from a direct optimizationfor a given performance metric.ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithmsfor scoring coreference chains.
In Proceedings of theLinguistic Coreference Workshop at the InternationalConference on Language Resources and Evaluation(LREC-1998), pages 563?566.Pascal Denis and Jason Baldridge.
2007.
Joint determi-nation of anaphoricity and coreference resolution us-ing integer programming.
In Proceedings of the An-nual Meeting of the North American Chapter of the As-sociation for Computational Linguistics - Human Lan-guage Technology Conference (NAACL/HLT-2007).Pascal Denis and Jason Baldridge.
2008.
Corefer-ence with named entity classification and transitiv-ity constraints and evaluation with MUC, B-CUBED,and CEAF.
In Proceedings of Corpus-Based Ap-proaches to Coreference Resolution in Romance Lan-guages (CBA 2008).Jenny Rose Finkel and Christopher D. Manning.
2008.Enforcing transitivity in coreference resolution.
InProceedings of the 46th Annual Meeting of the Associ-ation for Computational Linguistics (ACL 2008), ShortPapers, pages 45?48.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by Gibbs sam-pling.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics, pages363?370.Thorsten Joachims.
1999.
Making large-scale SVMlearning practical.
In B. Scho?lkopf, C. Burges, andA.
Smola, editors, Advances in Kernel Methods - Sup-port Vector Learning.
MIT-Press.Dan Klein and Christopher Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of the 41st An-nual Meeting of the Association for ComputationalLinguistics, pages 423?430.Xiaoqiang Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of the Annual Meetingof the North American Chapter of the Association forComputational Linguistics - Human Language Tech-nology Conference (NAACL/HLT-2005), pages 25?32.Marta Recasens and Eduard Hovy.
in prep.
BLANC: Im-plementing the rand index for coreference evaluation.Marta Recasens, Llu?
?s Ma`rquez, Emili Sapena,M.Anto`nia Mart?
?, Mariona Taule?, Ve?ronique Hoste,Massimo Poesio, and Yannick Versley.
2010.SemEval-2010 Task 1: Coreference resolution inmultiple languages.
In Proceedings of the 5thInternational Workshop on Semantic Evaluations(SemEval-2010), Uppsala, Sweden.Wee Meng Soon, Hwee Tou Ng, and Daniel Chung YongLim.
2001.
A machine learning approach to corefer-ence resolution of noun phrases.
Computational Lin-guistics (Special Issue on Computational AnaphoraResolution), 27(4):521?544.Olga Uryupina.
2003.
High-precision identification ofdiscourse-new and unique noun phrases.
In Proceed-ings of the ACL?03 Student Workshop, pages 80?86.Olga Uryupina.
2006.
Coreference resolution with andwithout linguistic knowledge.
In Proceedings of theLanguage Resources and Evaluation Conference.Olga Uryupina.
2007.
Knowledge Acquisition for Coref-erence Resolution.
Ph.D. thesis, Saarland University.Olga Uryupina.
2008.
Error analysis for learning-basedcoreference resolution.
In Proceedings of the Lan-guage Resources and Evaluation Conference.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A model-theoretic coreference scoring scheme.
In Proceedingsof the 6th Message Understanding Conference, pages45?52.103
