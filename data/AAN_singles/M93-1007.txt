MUC-5 EVALUATION METRIC SNancy Chinchor, Ph .D.Science Applications International Corporatio n10260 Campus Point Drive, MIS A2-FSan Diego, CA 9212 1chinchor@ gso .saic .com(619) 458-2614Beth SundheimNaval Command, Control, and Ocean Surveillance Cente rRDT&E Division (NRaD)Information Access Technology Project Team, Code 44208San Diego, CA 92152-742 0sundheim @nosc.milINTRODUCTIONThe metrics used for the Fifth Message Understanding Conference (MUC-5) evaluation are a major updateto those used for MUC-4 in 1992.
The official MUC-5 metrics express error rates while the official MUC-4 metric sexpress performance in terms of recall and precision (used for MUC-5 only as "unofficial" metrics) .
This paperdiscusses the current metrics and the reasons for their adoption .SCORE REPORTSThe MUC-5 Scoring System is evaluation software that aligns and scores the templates produced by th einformation extraction systems under evaluation in comparison to an "answer key" created by humans .
The ScoringSystem produces comprehensive summary reports showing the overall scores for the templates in the test set ; thesemay be supplemented by detailed score reports showing scores for each template individually.
Figure 1 shows asample summary score report in the joint ventures task domain for the error metrics ; Figure 2 shows a correspondingsummary score report for the recall-precision metrics .Scoring Categorie sThe basic scoring categories are found in the score report under the column headings COR, PAR, INC ,XCR, XPA, XIC, MIS, SPU, and NON.
These categories have not fundamentally changed since the MUC-4evaluation.
The rows in the body of the score report are for the various slots and objects in the template ; various totalsappear at the bottom .For the MUC-5 evaluation, alignment of system responses (i .e ., templates, objects, and slot-fillers generatedby the system under evaluation) with the answer key was done fully automatically, and scoring was don einteractively.
In interactive scoring mode, the evaluator is queried for a scoring decision only under certaincircumstances; under most circumstances, the scoring decisions are made automatically .
The meaning of each of th escoring categories is described below and summarized in Table 1 .?
If the response and the key are deemed to be equivalent, the category is correct (COR); if interactivelyassigned, a tally appears in both the COR and XCR (interactive correct) columns .?
If the response and the key are judged to be a near match, the category is partial (PAR) ; if interactivelyassigned, a tally appears in both the PAR and XPA (interactive partial) columns .69SLOT POS ACT COR PAR INC XCR XPA XIC SPU MIS NON ERR UND OVG SUS +<template>conten tsubtotals<tie-up-relatistatusentityjoint-ventureownershi pactivitysubtotals<entity>namealiaseslocationnationalit ytype28228234838934838 934838934838979183418021 21031223873671809 2024976105787293735938932233826521 297610572822892892892315351016 4214114574955423214010 0716000000000004461 90006666494234442296 111552 31 1940000000000040000000000000003 853000000000000081319009494949430 5885410 965 024722414 615610124705 3535353162563512943516615911614015 416601 11 11 100961801 028602739944350900353535485 1625 9575339475369734101 51 51515203 13433241 71 8324358170242424243342443032232438464 8230222221 51 86171781 93181 01 2ALL OBJECTSMATCHED ONLY121251391 39140 9729679367931491491562115966858512412454051624362 11039399639966 136301139171 91 5Richness-Normalized ErrorWrongReq-fills All-fills Min-errMax-err10662.511813 12138 0.8784 0.9026Error Rate Per WordWrongWord-count Error-rate10662.592862 0.1148Figure 1 : Sample Error Score Report .?
If the key and response do not match, the category is incorrect (INC) ; if interactively assigned, a tall yappears in both the INC and XIC (interactive incorrect) columns .?
If the key has a fill and the response has no corresponding fill, the category is missing (MIS) .?
If the response has a fill which has no corresponding fill in the key, the category is spurious (SPU) .
?If the key and response are both left blank, then the category is noncommittal (NON) .The columns in Figures 1 and 2 labelled possible (POS) and actual (ACT) contain the tallies of the numbe rof slot fillers that should be generated and the number of fillers that the system under evaluation actually generated ,respectively.
Possible is the sum of the correct, partial, incorrect, and missing .
Actual is the sum of the correct, partial ,incorrect, and spurious .
These tallies are used in the computation of some of the evaluation metrics .
The total possibl eis system-dependent and is therefore computed by summing the tallies assigned to the system responses rather tha nby simply summing the slot fillers to be found in the key template .
In contrast, a system-independent metric will beexplained in a later section .70contentstatusentityactivit ysubtotals<entity>namealiaseslocationtype<template>subtotals<tie-up-relatijoint-ventureownershipnationalitySLOT PO S282348348348348791180103871809976872359k.32226597ACT2823893893893899342121223672024105793738933821 21057CO R2822892828923 153510 1642141145749554232140100716PAR0000000Q:0004461 900INC066664423444229611155231 194XCR0000000000040000XPA00000000000385300XIC00000000000813190SPU0949494943058854109650247224146156101247MIS053535353162563512943516615911 6140154166NO N01 11100096180102860273994435090RE C100838383666856625563776665463873PRE100747474595748525856716 160444768UN D01 51 51 51 52031343324171832435817OVG0242424243342443032232438464823ALL OBJECT SMATCHED ONLYTEXT FILTERING12125 1391 39140 97292512626793 149 15626793 149 1159242**685124685124***5405 3621 399 61624 1039 399 620911574930397570111 7969248F-MEASURESP&R 2P&RP&2 R52.75 50.6655 .02Figure 2: Sample Recall-Precision Score Report.Summary RowsThe two summary rows in the score report labelled "ALL OBJECTS" and "MATCHED ONLY" show th eaccumulated tallies obtained by scoring spurious and missing objects in different manners .
Templates may containTable 1 : Scoring Categories .q Correctq Partialq Incorrectq SpuriousD Missingq Noncommittalresponse = keyresponse a keyresponse keykey is blank and response is no tresponse is blank and key is no tkey and response are both blank7 1more than one instance of a kind of object, e .g ., more than one <entity> object .
The keys and responses may not agreein the number of objects generated .
These cases lead to spurious and/or missing objects .
Opinions as to how muc hsystems should be penalized for spurious or missing objects differ depending upon the requirements of th eapplication in mind .
These differing views have lead us to provide the two ways of scoring spurious and missin ginformation as outlined in Table 2 .The MATCHED ONLY manner of scoring penalizes the least for missing and spurious objects by scorin gthem only in the object ID slot.
This object ID score does not impact the overall score because the object ID slot is no tincluded in the summary tallies ; the tallies include only the individual slots .
ALL OBJECTS is a stricter manner o fscoring because it penalizes for both the slot fills missing in the missing objects and the slots filled in the spuriou sobject .
The metrics calculated based on the scores in the ALL OBJECTS row of the error score report are the officia lMUC-5 scores .qMatched OnlyMissing and spurious objects scored in object slot onlyqAll ObjectsMissing object slots scored as missingSpurious object slots scored as spuriousTable 2: Manners of Scoring .Evaluation Metric sThe rightmost four columns in both the error score report and the recall-precision score report contain th escores for the evaluation metrics.
These are computed for each object and slot in the template, and overall scores ar eshown at the bottom .The primary evaluation metrics for MUC-5 have been changed from those used in previous MU Cevaluations .
The reasoning behind this change will be described in a later section .
First, the formulas used to calculat ethe evaluation metrics on the score reports will be given .Error MetricsThe error per response fill (ERR) is the official measure of MUC-5 system performance .
This measure i scalculated as the number wrong divided by the total (possible plus spurious) as shown in Table 3 .
It is dependent onthe system because tallies change according to the amount of spurious data generated and according to how th esystem tilled slots that have optional or alternate fills in the key.
(See the discussion below on richness-normalize derror metric .
)Table 3 also shows the computation of three secondary metrics -- undergeneration, overgeneration, an dsubstitution -- which isolate the three elements constituting overall error .
Undergeneration and overgeneration were i nuse for MUC-4 as well, and this is why they appear in both the error score report and the recall-precision score report .Those metrics are computed the same way for both reports .
The substitution metric is new for MUC-5 and is foun donly in the error score report .
The metric is not isolated in the recall-precision view on information extraction ; this i sbecause it is a (negative) factor in both recall and precision ; in the error-based view, on the other hand, it is isolated a sa distinct type of error.
The reader should note that the denominator in each of the secondary metrics is differen tbecause each metric offers a distinct perspective on the errors that a system can make .72Primary Metric Error per response fill = wrong =INC + PAR/2 + MIS + SP UtotalCOR+PAR+INC+MIS+SP USecondary Metrics Undergeneration =Overgeneration =MIS _POSMI SCOR + PAR + INC + MI SSPUSPU_ACTCOR + PAR + INC + SP USubstitution =INC + PAR/ 2COR + PAR + INCTable 3: System-dependent Error Metrics .The error per response fill has been chosen as the primary measure reported for a system for this evaluatio nbecause developers now need to focus on the sources of errors, explain them, and remedy them to push the state o fthe art.
For example, if System A has the raw scores shown in Figure 3, its error per response fill is calculated a sfollows :wrong=INC+PAR/2+MIS+SPU=25+5+0+ 10=4 0total=COR+PAR+INC+MIS+SPU=10+10+25+0+10=5 5wrong/total = 40/55 = 73%While the error per response fill metric and the undergeneration, overgeneration, and substitution metrics ar edesigned to suit the system developers' need for performance diagnostics, a different measure that is as independen tof the system and the text sample as possible may be more useful in some other circumstances .
The richness -normalized error measure is designed to measure errors relative to the amount of information to be extracted from th etexts.
This metric is shown in one of the summary rows at the bottom of the error score report .COR PAR INC SPU MIS NON ER RSYSTEM AI 1010251003573Figure 3 : System A .Richness-normalized error is calculated by dividing the number of errors per word by the number of key fill sper word .
This calculation reduces to the number of errors divided by the fill-count .
If a program manager i sconsidering use of a system on a distinct class of documents from the ones the system was tested on, this measure wil lpredict the number of errors the system will make, given the richness of the new set of documents .Due to the optional and alternate fills in the key, there will be a range of fill-counts from the minimu mnumber of fills required to the maximum number of fills allowed .
The difference between the two numbers represen t"discretionary" fills, i .e ., ones that represent the ambiguity inherent in the text) The formaulas for calculating theminimum and maximum richness-normalized error appear in Table 4 .1 .
For further information on the variability inherent in the key templates, please refer to the published ver-sion of the proceedings, which will contain a paper about the text and template corpora.73Richness-Minimum Error =wrongINC + PAR/2 + MIS + SPU_All - fillsRequired + Optional + MaximumAlternat eNormalizedError Maximum Error =wrongINC + PAR/2 + MIS + SPU?Req - fillsRequired + MinimumAlternateTable 4 : Richness-normalized error.For example, if system B has the raw scores in Figure 4 and if the key is filled as in Figure 5, the fill-coun twill range from the minimum required fills, which is a sum of Required Fills + Minimum Alternate Discretionar yFills (20+ 10), to the maximum allowed fills, which is the sum of Required Fills + Optional Discretionary Fills +Maximum Alternate Discretionary Fills (20 + 10 + 30) .
For this system, the richness-normalized error will rangefrom 40/60 to 40/30 or 0.67 to 1 .33 .Note that the maximum richness-normalized error can be greater than 1 .00 because the fill-count in the keycan he less than the number wrong for a system that overgenerates .
Note also that the minimum richness-normalizederror can he less than the error per response fill because the (system-independent) fill-count in the key can be greate rthan the (system-dependent) total used in the denominator in error per response fill .The error score report also contains a row called "Error Rate per Word," but it should be noted that thi smetric is not comparable between the Japanese and the English and is not highly accurate for Japanese .POS ACT COR PAR INC XCR XPA XIC SPU MIS NON ERR UNDOVG SU BSYSTEM B10 10 520 10 35WrongReq-fillsAll-fillsMin-errMax-err4030600 .671 .33Figure 4 : System B .REQUIRE DFILLSOptionalDISCRETIONARY FILLSAlternateBLANKSMinimum Maximu m20 10 10 30 35Figure 5 : Key Fills for System B.Recall precision MetricsWe have designated the recall, precision, and F-measure metrics that were used for MUC-4 as unofficia lsecondary metrics for MUC-5 in order to maintain continuity with previous MUCs .
They can be used to explaincurrent performance in comparison to past performance.
Further analysis is still necessary to determine thei rcontribution to the evaluation of data extraction systems as compared to the error-based metrics .Richness-Normalized Error74The recall-precision evaluation metrics were adapted from the field of Information Retrieval (IR) an dextended for the MUC evaluations .
They measure four different aspects of performance and an overall, combine dview of performance .
The four evaluation metrics of recall, precision, undergeneration, and overgeneration ar ccalculated for the slots and in the summary score rows (see Table 5) .
The fifth metric, the F-measure, is a combinedscore for the entire system and is listed at the bottom of the report .Recall (REC) is the percentage of possible answers which were correct .
Precision (PRE) is the percentage o factual answers given which were correct .
A system has a high recall score if it does well relative to the number of slo tfills in the key.
A system has a high precision score if it does well relative to the number of slot fills it attempted :In IR, a common way of representing the characteristic performance of systems is in a precision-recal lgraph.
Normally, as recall goes up, precision tends to go down and vice versa [I ] .
To directly measureunderpopulation or overpopulation of the template database by the information extraction systems, we introduced th emeasures of undergeneration and overgeneration .recall=correct+(nartial x 0.5 )possibleprecision=correct+(partial x 0.5) ,actualundergenerationmissingpossibleovergeneration=spuriousactualTable 5: Recall- Precision Evaluation Metrics .Methods have been developed for combining the measures of recall and precision to get a single measure .
I nMUC-4, we used van Rijsbergen's F-measure [1, 2] for this purpose .
The F-measure provides a way of combinin grecall and precision to get a single measure which falls between recall and precision .
Recall and precision can hav erelative weights in the calculation of the F-measure, giving it the flexibility to be useful in the context of differen tapplication requirements .
The formula for calculating the F-measure is :(132 +1 .0)xPx R(02 xP)+ Rwhere P is precision, R is recall, and is the relative importance given to recall over precision .
If recall and precisionare of equal weight, Q = 1 .0.
This value is shown in the score report under the heading "P&R."
The heading "2P&R "is for recall half as important as precision (R = 0.5) .
The heading "P&2R" is for recall twice as important as precisio n(f3 = 2 .0) .
The F-measure is calculated from the recall and precision values in the ALL OBJECTS row .Note that the F-measure is higher if the values of recall and precision are more towards the center of th eprecision-recall graph than at the extremes and their sums are the same .
So, for R = 1 .0, a system which has recall o f50% and precision of 50% has a higher F-measure than a system which has recall of 20% and precision of 80%.
Thi sbehavior is what we wanted from this single measure, which we expected would encourage developers to pus hoverall performance and, at the same time, to minimize the trade-off between the competing requirements fo rminimal missing, spurious, and substitution types of error .F=75An example showing the new metrics and the old (along with the pertinent scoring categories) for thre etheoretical systems is given in Figures 6 and 7 .
In this example, the error per response fill is the same for each of th ethree systems even though the F-measures are different.
However, the secondary metrics of undergeneration ,overgeneration, and substitution serve to distinguish the three systems .
This hypothetical example points out th eimportant role that the secondary metrics could play in system analysis as well as the analysis of the quality of th eextracted information .POS ACT COR PAR INC SPU MIS NON ERR UND OVG SU BSYSTEM A 45 55 10 10 25 10 0 35 73 0 18 67SYSTEM B 45 35 10 10 5 10 20 35 73 44 29 40SYSTEM C 55 35 10 10 15 0 20 35 73 36 0 57Figure 6 : Three Systems with Equal Error per Response Fill .POS ACT COR PAR INC SPU MIS NON FP&R REC PR ESYSTEM A 45 55 10 10 25 10 0 35 29.70 33 27SYSTEM B 45 35 10 10 5 10 20 35 37.34 33 43SYSTEM C 55 35 10 10 15 0 20 35 33.17 27 43Figure 7: Unofficial Metrics for Three Systems with Equal Error per Response Fill .Also appearing in the recall-precision score report is a row called "Text Filtering ."
The purpose of this row i sto report how well systems distinguish relevant articles from irrelevant articles .
The scoring program keeps track ofhow many times each of the situations in the contingency table arises for a system (see Table 6) .
It then uses thosevalues to calculate the entries in the Text Filtering row .
The evaluation metrics are calculated for the row as indicate dby the formulas at the bottom of Table 6 .The Role of the Noncommittal Scoring Categor yThe reader will have noticed that the category of "noncommittal" responses has been omitted from themetrics .
Although this may not seem reasonable from an applications perspective, from a research perspective w ebelieve that the exclusion of noncommittal responses results in a much less distorted cross-system view o fperformance .
The question comes down to whether systems normally leave a slot blank out of knowledge or whethe rthey do so out of a lack of knowledge .
Highly immature systems tend either to overgenerate to an extreme, leavin gfew blanks, or to undergenerate to an extreme, leaving many blanks .
The latter type of immature system is morecommon and may benefit unfairly from a metric that considers a noncommittal response to be a correct response,especially if there are relatively many blanks in the key templates .If, for example, noncommittals were considered correct responses and included in the denominator of theerror per response fill measure, the rankings of all 17 MUC-4 systems on TST3 (the name of one of the two test set sused in the evaluation) would change.
The most radical changes would be for immature systems whose number ofnoncommittals greatly outweighs all other categories of response .
Since there are a lot of immature systems evaluate dfor MUC-5 (as there were for MUC-4) and since the average number of fills in the answer-key templates for MUC- 5is only about half of what it was for MUC-4, the distortions of the results for MUC-5 have the potential to be eve ngreater than they were for MUC-4 .
However, the potential effect on the MUC-5 evaluation is damped somewhat b ythe fact that the MUC-5 template consists of objects that are aligned separately ; response objects that contain aninsufficient amount of slot-fillers to warrant an alignment with a key object are not scored against a key object at th eslot level .
Nonetheless, we believe that omitting noncommittals from the metrics provides a better basis fo rcomparison across the full range of MUC-5 (and MUC-4) systems and provides a more accurate assessment of the -state of the art .7 6Decides RelevantRelevant I sCorrectaIrrelevant I sCorrectb a+bDecides Irrelevant c d c+da+c b+d a+b+c+d = nPOS ACT COR PAR INC ICR IPA Spu MIS NONRecall = a/(a+c)Undergeneration = c/(a+c)Precision = a/(a+b)Overgeneration = b/(a+b)TextFiltering a+c a+ba c dbTable 6 : Text Filtering.CHANGES TO THE METRICS FROM PREVIOUS EVALUATION SThe changes to the evaluation metrics are expected to enable three different types of evaluation "users "(NLP researchers, program managers, and potential customers) to assess and compare system performance in ameaningful way.
It is also hoped that the changes will correct deficiencies in the evaluation that may unwittingl yencourage conservative development strategies on the part of the researchers and that may also limit the evaluation' smeaningfulness to other evaluation users.Although the terms recall and precision were borrowed from IR, the metrics themselves represent asignificant departure from the contingency table model, which underlies the IR version of the metrics .
The task o fextraction is a complex one that includes elements of information detection and classification, plus open-ende dgeneration of strings and object pointers .
The focus on recall and precision as primary metrics for the last few year shas had some advantages, among them the following :?
they bring out the fundamental tension between spurious and missing data ;?
they require that evaluation users view system performance along more than one dimension ;?they present a positive view of system performance, which may have helped to make the NL Presearchers more comfortable with the idea of submitting their systems to evaluation .However, recall and precision have the disadvantage of making a two-way distinction between error type s(spurious and missing) when in fact there are three types of error.
The third kind of error is captured by thesubstitution metric ; it is accounted for by the categories of incorrect and ( .5 times) partial .
Substitution errors arctaken into account in the recall-precision metrics to the extent that they contribute to the denominator of both recal land precision ; however, this type of error is not isolated, and its inclusion in the denominator of recall and precisionprevents those metrics from revealing to what extent a system's shortfalls are due to substitution rather than t omissing (in the case of recall) or spurious (in the case of precision) .77In a way, the recall-precision metrics view substitution as a blend of missing and spurious ; a system did no tsimply produce the wrong fill, but rather produced a spurious fill on the one hand and missed a fill on the other hand .This is a reasonable model of system behavior in many cases, but not in others, especially when a response is scoredpartially correct .
These deficiencies of the recall and precision metrics make the use of the error per response fil lreasonable, as long as it is accompanied by the secondary metrics of overgeneration (spurious), undergeneratio n(missing), and substitution (incorrect, including half of the partial) .The F-measure, which was introduced for MUC-4 in response to needs of researchers and programmanagers for a ranking metric, has come to be used more generally than just for cross-system comparisons .
Bybecoming the one metric of focus, it has been competing with recall and precision for the role of primary metric ,thereby weakening two of the major advantages that recall and precision originally had .
Furthermore, now thatperformance of some systems is in or approaching the 50% range, recall and precision are at a disadvantage fo rmotivating researchers to push performance of the top systems through the more difficult stages ahead because the yfocus on the positive aspects of performance .
These factors make the adoption of error per response fill as the primarymetric a reasonable next step in determining the best way to measure performance .The statistical significance results from MUC-5 give us feedback on how well the error metric and the F -measure distinguish systems .
The results show that there are no differences between the rankings determined by erro rper response fi11 2 and the rankings determined by F-measure.
The error per response fill distinguishes systems slightlybetter ; four more system pairs were significantly different in their error per response fill than were significantl ydifferent in their F-measure .
The error per response fill also shows a tendency towards clustering systems in slightl yclearer groups than the F-measure for EJV due to its ability to distinguish systems slightly better .The richness-normalized error represents another change from previous evaluations and was motivated b ythe desire for a system-independent metric .
The nature of this metric requires that spurious behavior be ignored .
Thesearch for such a metric led us to innovate one in which two values, a minimum and maximum, were calculated sinc elanguage understanding necessarily involves variability in interpretation .
It remains to be seen whether ignoringovergeneration interferes with the predictive quality of the richness-normalized error metric .REFERENCES[1] Frakes, W.B.
and Baeza-Yates, R .
(eds.)
(1992) Information Retrieval: Data Structures & Algorithms .Englewood Cliffs : Prentice Hall .
[2] Van Rijsbergen, C .J .
(1979) Information Retrieval.
London : Butterworths .
[3] Nierstrasz, O.
(1989) "A Survey of Object-Oriented Concepts" in W. Kim and F. H. Lochovsky (Eds .)
Object-Oriented Concepts, Databases, and Applications .
New York : Addison-Wesley.2 .
Although rounded numbers appear in the score report, floating point values of error per response till wer eused for statistical analyses .78
