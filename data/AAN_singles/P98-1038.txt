PAT-Trees with the Deletion Function as the Learning Devicefor Linguistic PatternsKeh-Jiann Chen, Wen Tsuei, and Lee-Feng ChienCKIP, Institute of Information Science,Academia Sinica, Nankang, Taipei 1 15, TaiwanAbstractIn this study, a learning device based on the PAT-tree data structures was developed.
The originalPAT-trees were enhanced with the deletionfunction to emulate human learning competence.The learning process worked as follows.
Thelinguistic patterns from the text corpus areinserted into the PAT-tree one by one.
Since thememory was limited, hopefully, the important andnew patterns would be retained in the PAT-treeand the old and unimportant patterns would bereleased from the tree automatically.
Theproposed PAT-trees with the deletion functionhave the following advantages.
1) They are easyto construct and maintain.
2) Any prefix sub-string and its frequency count through PAT-treecan be searched very quickly.
3) The spacerequirement for a PAT-tree is linear with respectto the size of the input text.
4) The insertion of anew element can be carried out at any timewithout being blocked by the memory constraintsbecause the free space is released through thedeletion of unimportant elements.Experiments on learning high frequency bi-grams were carried out under different memorysize constraints.
High recall rates were achieved.The results show that the proposed PAT-trees canbe used as on-line learning devices.1.
IntroductionHuman beings remember useful and importantinformation and gradually forget old andunimportant information in order to accommodatenew information.
Under the constraint of memorycapacity, it is important to have a learningmechanism that utilizes memory to store and toretrieve information efficiently and flexiblywithout loss of important information.
We don'tknow how human memory functions exactly, butthe issue of creating computers with similarcompetence is one of the most important problemsbeing studied.
We are especially interested incomputer learning of linguistic patterns withoutthe problem of running out of memory.To implement such a learning device, a datastructure, equipped with the following functions,is needed: a) accept and store the on-line input ofcharacter/word patterns, b) efficiently access andretrieve stored patterns, c) accept unlimitedamounts of data and at the same time retain themost important as well as the most recent inputpatterns.
To meet the above needs, the PAT-treedata structure was originally considered a possiblecandidate to start with.
The original design of thePAT-tree can be traced back to 1968.
Morrison\[Morrison, 68\] proposed a data structure called the"Practical Algorithm to Retrieve InformationCoded in Alphanumeric"(PATRICIA).
It is avariation of the binary search tree with binaryrepresentation f keys.
In 1987, Gonnet \[Gonnet,87\] introduced semi-infinite strings and storedthem into PATRICIA trees.
A PATRICIA treeconstructed over all the possible semi-infinitestrings of a text is then called a PAT-tree.
Manykinds of searching functions can be easilyperformed on a PAT-tree, such as prefix searching,range searching, longest repetition searching andso on.
A modification of the PAT-tree was done tofit the needs of Chinese processing in 1996 byHung \[Hung, 96\], in which the finite strings wereused instead of semi-infinite strings.
Since finite244strings are not unique in a text as semi-infinitestrings are, frequency counts are stored in treenodes.
In addition to its searching functions, thefrequencies of any prefix sub-strings can beaccessed very easily in the modified PAT-tree.Hence, statistical evaluations between sub-strings,such as probabilities, conditional probabilities,and mutual information, can be computed.It is easy to insert new elements into PAT-trees, but memory constrains have made themunable to accept unlimited amounts of information,hence limiting their potential use as learningdevices.
In reality, only important orrepresentative data should be retained.
Old andunimportant data can be replaced by new data.Thus, aside from the original PAT-tree, thedeletion mechanism was implemented, whichallowed memory to be released for the purpose ofstoring the most recent inputs when the originalmemory was exhausted.
With this mechanism, thePAT-tree is now enhanced and has the ability toaccept unlimited amounts of information.
Onceevaluation functions for data importance areobtained, the PAT-tree will have the potential tobe an on-line learning device.
We review theoriginal PAT-tree and its properties in section 2.In section 3,we describe the PAT-tree withdeletion in detail.
In section 4, we give the resultsobtained after different deletion criteria weretested to see how it performed on learning wordbi-gram collocations under different sizes ofmemory.
Some other possible applications and asimple conclusion are given in the last section.2.
The Original PAT-treeIn this section, we review the original version ofthe PAT-tree and provide enough backgroundinformation for the following discussion.2.1 Definit ion of Pat-tree2.1.1 PATRICIABefore defining the PAT-tree, we first show howPATRICIA works.PATRICIA is a special kind of trie\[Fredkin60\].
In a trie, there are two different kinds ofnodesqbranch decision nodes and element nodes.Branch decision nodes are the search decision-makers, and the element nodes contain real data.To process strings, if branch decisions are madeon each bit, a complete binary tree is formedwhere the depth is equal to the number of bits ofthe longest strings.
For example, suppose there are6 strings in the data set, and that each is 4 bitslong.
Then, the complete binary search tree is thatshown in Fig.
2.1.IO011Fig 2.1 The complete binar,' tree of the 6 dataApparently, it is very wasteful.
Manyelement nodes and branch nodes are null.
If thosenodes are removed, then a tree called a"compressed digital search trie" \[Flajolet 86\], asshown in Fig.
2.2, is formed.
It is more efficient,but an additional field to denote the comparing bitfor branching decision should be included in eachdecision node.
In addition, the searched resultsmay not exactly match the input keys, since onlysome of the bits are compared uring the searchprocess.
Therefore, a matching between thesearched results and their search keys is required.Morrison \[Morrison, 68\] improved the triestructure further.
Instead of classifying nodes intobranch nodes and element nodes, Morrisoncombined the above two kinds of nodes into auniform representation, called an augmentedbranch node.
The structure of an augmentedbranch node is the same as that of a decision nodeof the trie except that an additional field forstoring elements is included.
Whenever anelement should be inserted, it is inserted "up" to abranch node instead of creating a new elementnode as a leaf node.
For example, the compresseddigital search trie shown in Fig 2.2 has theequivalent PATRICIA like Fig 2.3.
It is noticedthat each element is stored in an upper node or initself.
How the data elements are inserted will bediscussed in the next section.
Another differencehere is the additional root node.
This is because ina binary tree, the number of leaf nodes is alwaysgreater than that of internal nodes by one.Whether a leaf node is reached is determined bythe upward links.2450010 0011 1000 1011Fig.
2.2 Compressed igital search trie.OtHO IIMNIFig 2.3 PATRIC IA2.1.2 PAT-treeGonnet \[Gonnet, 87\] extended PATRICIA tohandle semi-infinite strings.
The data structure iscalled a PAT-tree.
It is exactly like PATRICIAexcept that storage for the finite strings is replacedby the starting position of the semi-infinite stringsin the text.Suppose there is a text T with n basic units.T = U l t l2 .
.
.
t t , , .
Consider the prefix sub-strings ofT's which start from certain positions and go on asnecessary to the right, such as u,u2.
.
.u, ,  .
.
.
.
.U2U 3...tt,, .
.
.
.
.
U3U4.. .U n .... and so  on .
Since eachof these strings has got an end to the left but noneto the right, they are so-called semi-infinite strings.Note here that whenever a semi-infinite stringextends beyond the end of the text, null charactersare appended.
These null characters are differentfrom any basic units in the text.
Then, all thesemi-infinite strings starting from differentpositions are different.
Owing to the additionalfield for comparing bits in each decision node ofPATRICIA, PATRICIA can handle branchdecisions for the semi-infinite strings (since afterall, there is only a finite number of sensibledecisions to separate all the elements of semi-infinite strings in each input set).
A PAT-tree isconstructed by storing all the starting positions ofsemi-infinite strings in a text using PATRICIA.There are many useful functions which caneasily be implemented on PAT-trees, such asprefix searching, range searching, longestrepetition searching and so on.Insert(to-end substring Sub, PAT tree rooted at R){/ /  Search Sub in the PAT tree@4--~,n <-- Left(p);while ( CompareBit ( n ) > CompareBit ( p ) ) {p <---n;if the same bit as CompareBit ( p ) at Sub is 0n <--- Le f t (p ) ;elsen <---- Right ( p );}if ( Data (n )  = Sub) {/ /  Sub is a l ready in the PAT tree, just// increase the count.
No need to insert.Occurrence n ) (----- Occurrence ( n ) + 1 ;return;}// F ind the appropr iate  pos i t ion  to insert SUb //into the PAT tree (SUb wi l l  be inserted// between p and n)b <--- the first bit where Data ( n ) and Sub differ;p <---R;n (--- Le f t (p ) ;while ( (CompareBit ( n ) > CompareBit ( p ) ) and(CompareBit ( p ) < b) ) {p <---n;if the same bit as CompareBit ( p ) at Sub is 0n <--- Le f t (p ) ;elsen 4-- Right (p ) ;}/ / Insert SUb into the PAT tree, between p and n// In it iate a new nodeNN ~--" new node;CompareBit ( NN ) <--- b;Data (NN)  ~'-  Sub;Occurrence ( NN ) <--- 1 ;/ /  Insert the new nodeIf the bth bit of Sub is 0 {}else {}if n iselseLeft ( NN ) 4-- NN;Right ( NN ) ~--- n;Left (NN)  ~.- n;Right (NN)  ~ NN;the Left of pLe f t (p )  ~-- NN;Right ( p ) 4-- NN;Algorithm 2.1 PAT tree InsertionHung \[Hung, 96\] took advantage of prefixsearching in Chinese processing and revised thePAT-tree.
All the different basic unit positionswere exhaustively visited as in a PAT-tree, but thestrings did not go right to the end of the text.
Theyonly stopped at the ends of the sentences.
We callthese finite strings "to-end sub-strings".
In this246way, the saved strings will not necessarily beunique.
Thus, the frequency counts of the stringsmust be added.
A field denoting the frequency ofa prefix was also added to the tree node.
Withthese changes, the PAT-tree is more than a toolfor searching prefixes; it also provides theirfrequencies.The data structure of a complete node of aPAT-tree is as follows.Node: a record ofDecision hit: an integer to denote the decision bit.Frequency: the frequency count of the prefix sub-string.
.Data element: a data string or a pointer of a semi-infinite string.Data count: the frequency count of the data string.Left: the left pointer points downward to the leftsub-tree or points upward to a data node.Right: the right pointer points downward to theright sub-tree or points upward to a data node.End of the record.The construction process for a PAT-tree is nothingmore than a consecutive insertion process forinput strings.
The detailed insertion procedure isgiven in Algorithm 2.1 and the searchingprocedure in Algorithm 2.2.SearchforFrequencyof ( Pattern )(p ~ R/*the root of PAT-tree*/;n (--- Le f t (p ) ;while ( ( CompareBit ( n ) > CompareBit ( p ) ) and( CompareBit ( n ) _< total bits of Pattern ) ){p (--- n;if the "CompareBit ( p )"th bit of Pattern is 0n (--- Le f t (p ) ;elsen (--- Right ( p );lif ( Data ( n ) :~ Pattern )return O;if ( CompareBit ( n ) > total bits of Pattern )return TerminalCounts ( n );elsereturn Occurrence ( n );Algorithm 2.2 Search for frequency of a pattern in PAT-treeThe advantages of PAT-trees are as follows:(1) They are easy to construct and maintain.
(2)Any prefix sub-string and its frequency count canbe found very quickly using a PAT-tree.
(3) Thespace requirement for a PAT-tree is linear to thesize of the input text.3.
Pat-tree with the deletion funct ionThe block diagram of the PAT-tree with thedeletion function is shown in figure 3.1.Pat treeconstruction orextentionDeletionThe main partI EvaluationFig.
3.1 The Block Diagram of PAT-tree Construction.Implementing the deletion function requires twofunctions.
One is the evaluation function thatevaluates the data elements to find the leastimportant element.
The second function is releaseof the least important element from the PAT-treeand return of the freed node.3.1 The Evaluation functionDue to the limited memory capacity of a PAT-tree,old and unimportant elements have to beidentified and then deleted from the tree in orderto accommodate new elements.
Evaluation isbased on the following two criteria: a) the oldnessof the elements, and b) the importance of theelements.
Evaluation of an element has to bebalanced between these criteria.
The oldness of anelement is judged by how long the element hasresided in the PAT-tree.
It seems that a new fieldin each node of a PAT-tree is needed to store thetime when the element was inserted.
When the n-th element was inserted, the time was n. Theresident element will become old when newelements are gradually inserted into the tree.However, old elements might become more andmore important if they reoccur in the input text.The frequency count of an element is a simplecriterion for measuring the importance of an247element.
Of course, different importance measurescan be employed, such as mutual information orconditional probability between a prefix andsuffix.
Nonetheless, the frequency count is a verysimple and useful measurement.To simplify the matter, a unified criterion isadopted.
Under this criterion no additional storageis needed to register time.
A time lapse will bedelayed in order to revisit and evaluate a node,and hopefully, the frequency counts of importantelements will be increased uring the time lapse.It is implemented by way of a circular-like arrayof tree nodes.
A PAT-tree will be constructed byinserting new elements.
The insertion processtakes a free node for each element from the arrayin the increasing order of their indexes until thearray is exhausted.
The deletion process will thenbe triggered.
The evaluation process will scan theelements according to the array index sequence,which is different from the tree order, to find theleast important element in the first k elements todelete.
The freed node will be used to store thenewly arriving element.
The next position of thecurrent deleted node will be the starting index ofthe next k nodes for evaluation.
In this way, it isguaranteed that the minimal time lapse to visit thesame node will be at least the size of the PAT-treedivided by k.In section 4, we describe xperiments carriedout on the learning of high frequency word bi-grams.
The above mentioned time lapse and thefrequency measurement for importance were usedas the evaluation criteria to determine the learningperformance under different memory constraints.3.2 The Deletion functionDeleting a node from a PAT-tree is a bitcomplicated since the proper structure of the PAT-tree has to be maintained after the deletion process.The pointers and the last decision node have to bemodified.
The deletion procedure is illustratedstep by step by the example in Fig.
3.2.
Supposethat the element in the node x has to be deleted, i.e.the node x has to be returned free.
Hence, the lastdecision node y is no longer necessary since it isthe last decision bit which makes the branchdecision between DATA(x) and the strings in theleft subtree of y.
Therefore, DATA(x) andDECISION(y) can be removed, and the pointershave to be reset properly.
In step 1, a) DATA(x) isreplaced by DATA(y), b) the backward pointer inz pointing to y is replaced by x, and c) the pointerof the parent node of y which points to y isreplaced by the left pointer of y.
After step 1, thePAT-tree structure is properly reset.
However thenode y is deleted instead of x.
This will not affectthe searching of the PAT-tree, but it will damagethe algorithm of the evaluation function to keepthe time lapse properly.
Therefore, the wholerecord of the data in x is copied to y, and is resetto the left pointer of the parent node of x be y inthe step 2.
Of course, it is not necessary to dividethe deletion process into the above two steps.
Thisis just for the sake of clear illustration.
In theactual implementation, management of thosepointers has to be handled carefully.
Since there isno backward pointer which points to a parentdecision node, the relevant nodes and theirancestor relations have to be accessed and retainedafter searching DATA(x) and DATA(y)...?"
.
?o"Dclctc thisTermC~py the data .ol.o o,"* ,?o?,"xx, , .~FrEFig.
3.2 The deletion process4.
Learning word collocations byPat-treesThe following simple experiments were carriedout in order to determine the learning performanceof the PAT-tree under different memoryconstraints.
We wanted to find out how the highfrequency word bi-grams were retained when thetotal number of different word bi-grams muchgreater than the size of the PAT-tree.2484.1 The testing environmentWe used the Sinica corpus as our testing data.
TheSinica corpus is a 3,500,000-word Chinese corpusin which the words are delimited by blanks andtagged with their part-of-speeches\[Chen 96\].
Tosimplify the experimental process, the word lengthwas limited to 4 characters.
Those words that hadmore than four characters were truncated.
Apreprocessor, called reader, read the word bi-grams from the corpus sequentially and did thetruncation.
Then the reader fed the bi-grams to theconstruction process for the Pat-tree.
There were2172634 bigrams and 1180399 different bi-grams.Since the number of nodes in the PAT-trees wasmuch less than the number of input bi-grams, thedeletion process was carried out and some bi-grams was removed from the PAT-tree.
The recallrates of each different frequency bi-grams underthe different memory constraints were examinedto determine how the PAT-tree performed withlearning important information.4.2 Experimental ResultsTable 4.1 Finding the minimum of the next 200 nodes.
"~--?, /~1 e~l :w41 4/641 .~1 6/641 7/641 ~/6?>256 15 I(X 100 10C II/ 1(1 10(} ICE?
16~ 15 10( I(D IOC 1~ 1(}{ 100 I(E?
75 lff~ lOC 100 10~ 1(~ 10( 100 I(E>66 99.9?, lff lff.3 IOC I(I I(D !
(I\] 1(?>56 99.T~ 10( lff3 I(E lOC 1(I) I03 IOC>46 98..: 99,oA ICO IOC l(I 1()0 1(I\] I(IC'>3J 96., 99.8~ 1(13 1~ 10~ I(I) l(I\] I(?>36 94.6J 99.6\] 1(30 10{ 113( 1(I) 10(3 1(1(:>2.5 91.6~ 98.
'A 99.93 I0C 1(}( I(D 1(I\] 1(1{:>2~ 85.4~ 97.
(12 99.63 99.~ 100 l(ll l(I; I(/?
/3  76.1!
92.87 98.37 99.61 99.89 t~).94 99.9~ l(I?
It; 62.3.'
83.2 93.19 %.95, 98.5 t ~.3 99.~ 99.~>J 39.4~ 60.95 74.~ 83.1~ 88.55 91.86 94.18 %.31>2 23.52 43.56 57.01 66.4z 73.~ 78.78 83.
(~ 86.65>,~ 14,8: 29.34 43.55 52.22 59.45 65.37 70.55 74.81?
i 6.51 12.97 19.44 25.6\[ 31.85 38.04 44.62 48.7~Different ime lapses and PAT-tree sizes weretested to see how they performed bycomparing the results with the ideal cases.The ideal cases were obtained using aprocedure in which the input bi-grams werepre-sorted according to their frequency counts.The bi-grams were inserted in descendingorder of their frequencies.
Each bi-gram wasinserted n times, where n was its frequency.According to the deletion criterion, undersuch an ideal case, the PAT-tree will retain asmany high frequency bi-grams as it can.Table 4.2 Input bi-grams in descending order of theiraencies.1/64 "2/64 .t,/64 4/64 5/64 6/64 7/64 8/64>250 I~  ICE ICE ICE ICE ICE l~ ICE?
100 ICE ICE ICE 1~ ICE, IOC IOC IOC?
75 I(\]0 lifo lif0 1(30 ICE 10C 10G lffd>60 ICE ICE ICE ICE 10C 1~ 10C I0C>.Y0 ICE ICE ICE ICE ICE IOC I(E 1~>40 IO(J ICE ICE lO0 ICE ICE 1~ ICE>35 ICE lO(l ICE ICE lie lOC IOC lOC>30 ICE ICE ICE ICE IOC IOC IOC ICE>,~ 10C 1013 ICE ICE I(l: 15 10C ICE>20 10C ICE ICE ICE 10L ICE 10C lCE?
15 ICE ICE ICE 100 1~; lCE IOC ICE?
16 lOf IOC 10: 1('?
10?
I0(.
1(.
( IOC>5 46.12 92.2~ lff~ ICE 10( lO( IOC IOC>3 24 48 72 ~ I0(.
10( lff~ IOC>2 15 3(\] 45 6C 7f 9C IOC IOC>1 6.55 13.1 19.65 26.N 32.74 39.2~ 46.2~ 52.3t~The deletion process worked as follows.
Afixed number of nodes were checked starting fromthe last modified node, and the one with theminimal frequency was chosen for deletion.
Sincethe pointer was moving forward along the indexof the array, a time lapse was guaranteed to revisita node.
Hopefully the high frequency bi-gramswould reoccur during the time lapse.
Differentforward steps, such as 100, 150, 200, 250, and 300,were tested, and the results show that deletion ofthe least important elements within 200 nodes ledto the best result.
However the performanceresults of different steps were not very different.Table 4.1 shows the testing results of step size 200with different PAT-tree sizes.
Table 4.2 shows theresults under the ideal cases.
Comparing theresults between Table 4.1 and Table 4.2, it is seenthat the recall rates of the important bi-gramsunder the normal learning process weresatisfactory.
Each row denotes the recall rates of abi-gram greater than the frequency under differentsizes of PAT-tree.
For instance, the row 10 inTable 4.1 shows that the bi-grams which had thefrequency greater than 20, were retained asfollows: 85.46%, 97.02%, 99.63%, 99.95%, 100%,100%, 100%, and 100%, when the size of thePAT-tree was 1/64, 2/64 ..... 8/64 of the totalnumber of the different bi-grams, respectively.5.
ConclusionThe most appealing features of the PAT-tree with249deletion are the efficient searching for patternsand its on-line learning property.
It has thepotential to be a good on-line training tool.
Due tothe fast growing WWW, the supply of electronictexts is almost unlimited and provides on-linetraining data for natural language processing.Following are a few possible applications of PAT-trees with deletion.a) Learning of high frequency patterns byinputting unlimited amounts of patterns.
Thepatterns might be character/word n-grams orcollocations.
Thus, new words can be extracted.The language model of variable length n-gramscan be trained.b) The most recently inserted patterns will beretained in the PAT-tree for a while as if it hasa short term memory.
Therefore, it can on-lineadjust the language model to adapt to thecurrent input text.c) Multiple PAT-trees can be applied to learn thecharacteristic patterns of different domains ordifferent style texts.
They can be utilized assignatures for auto-classification f texts.With the deletion mechanism, the memorylimitation is reduced to some extent.
Theperformance of the learning process also relies onthe good evaluation criteria.
Different applicationsrequire different evaluation criteria.
Therefore,under the current PAT-tree system, the evaluationfunction is left open for user design.Suffix search can be done throughconstruction of a PAT-tree containing reverse text.Wiidcard search can be done by traversing sub-trees.
When a wiidcard is encountered, anindefinite number of decision bits should beskipped.To cope with the memory limitation on thecore memory, secondary memory might berequired.
In order to speed up memory accessing,a PAT-tree can be split into a PAT-forest.
Eachtime, only the top-level sub-tree and a demandedlower level PAT-tree will resided in the corememory.
The lower level PAT-tree will beswapped according to demand.Referencesde la Briandais, R. 1959.
File searching usingvariable length keys.
AFIPS western JCC, pp.295-98, San Francisco.
Calif,Chen, Keh-Jiann, Chu-Ren Huang, Li-Ping Changand Hui-Li Hsu.
1996.
Sinica Corpus: DesignMeghodology for Balanced Copra.
11" PacificAsia Conference on Language, Information,and Computation (PA CLIC I1).
pp.
167-176.Flajolet, P. and R. Sedgewick.
1986.
Digitalsearch trees revisited.
SlAM J Computing,15;748-67.Frakes, William B. and Ricardo Baeza-Yates.1992.
Information Retrieval, Data Structuresand Algorithms.
Prentice-Hall.Fredkin, E. 1960.
Trie memory.
CACM.
3, 490-99.Gonnet, G. 1987.
PAT 3.1: An Efficient TextSearching System, User's Mannual.
UW Centrefor the New OED, University of Waterloo.Hung, J. C. 1996.
Dynamic Language Modelingfor Mandarin Speech Retrieval for Home PageInformation.
Master thesis, National TaiwanUniversity.Morrison, D. 1968.
PATRICIA-PractialAlgorithm to Retrieve Information Coded inAlphanumeric.
JA CM, 15 ;514-34250
