Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 558?566,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPImproving Tree-to-Tree Translation with Packed ForestsYang Liu and Yajuan Lu?
and Qun LiuKey Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesP.O.
Box 2704, Beijing 100190, China{yliu,lvyajuan,liuqun}@ict.ac.cnAbstractCurrent tree-to-tree models suffer fromparsing errors as they usually use only 1-best parses for rule extraction and decod-ing.
We instead propose a forest-basedtree-to-tree model that uses packed forests.The model is based on a probabilis-tic synchronous tree substitution gram-mar (STSG), which can be learned fromaligned forest pairs automatically.
The de-coder finds ways of decomposing trees inthe source forest into elementary trees us-ing the source projection of STSG whilebuilding target forest in parallel.
Compa-rable to the state-of-the-art phrase-basedsystem Moses, using packed forests intree-to-tree translation results in a signif-icant absolute improvement of 3.6 BLEUpoints over using 1-best trees.1 IntroductionApproaches to syntax-based statistical machinetranslation make use of parallel data with syntacticannotations, either in the form of phrase structuretrees or dependency trees.
They can be roughlydivided into three categories: string-to-tree mod-els (e.g., (Galley et al, 2006; Marcu et al, 2006;Shen et al, 2008)), tree-to-string models (e.g.,(Liu et al, 2006; Huang et al, 2006)), and tree-to-tree models (e.g., (Eisner, 2003; Ding and Palmer,2005; Cowan et al, 2006; Zhang et al, 2008)).By modeling the syntax of both source and tar-get languages, tree-to-tree approaches have the po-tential benefit of providing rules linguistically bet-ter motivated.
However, while string-to-tree andtree-to-string models demonstrate promising re-sults in empirical evaluations, tree-to-tree modelshave still been underachieving.We believe that tree-to-tree models face twomajor challenges.
First, tree-to-tree models aremore vulnerable to parsing errors.
Obtainingsyntactic annotations in quantity usually entailsrunning automatic parsers on a parallel corpus.As the amount and domain of the data used totrain parsers are relatively limited, parsers willinevitably output ill-formed trees when handlingreal-world text.
Guided by such noisy syntactic in-formation, syntax-based models that rely on 1-bestparses are prone to learn noisy translation rulesin training phase and produce degenerate trans-lations in decoding phase (Quirk and Corston-Oliver, 2006).
This situation aggravates for tree-to-tree models that use syntax on both sides.Second, tree-to-tree rules provide poorer rulecoverage.
As a tree-to-tree rule requires that theremust be trees on both sides, tree-to-tree mod-els lose a larger amount of linguistically unmoti-vated mappings.
Studies reveal that the absence ofsuch non-syntactic mappings will impair transla-tion quality dramatically (Marcu et al, 2006; Liuet al, 2007; DeNeefe et al, 2007; Zhang et al,2008).Compactly encoding exponentially manyparses, packed forests prove to be an excellentfit for alleviating the above two problems (Mi etal., 2008; Mi and Huang, 2008).
In this paper,we propose a forest-based tree-to-tree model.
Tolearn STSG rules from aligned forest pairs, we in-troduce a series of notions for identifying minimaltree-to-tree rules.
Our decoder first converts thesource forest to a translation forest and then findsthe best derivation that has the source yield of onesource tree in the forest.
Comparable to Moses,our forest-based tree-to-tree model achieves anabsolute improvement of 3.6 BLEU points overconventional tree-based model.558IP1NP2 VP3PP4 VP-B5NP-B6 NP-B7 NP-B8NR9 CC10P11 NR12 VV13 AS14 NN15bushi yu shalong juxing le huitanBush held a talk with SharonNNP16 VBD17 DT18 NN19 IN20 NNP21NP22 NP23 NP24NP25 PP26NP27VP28S 29Figure 1: An aligned packed forest pair.
Eachnode is assigned a unique identity for reference.The solid lines denote hyperedges and the dashedlines denote word alignments.
Shaded nodes arefrontier nodes.2 ModelFigure 1 shows an aligned forest pair for a Chinesesentence and an English sentence.
The solid linesdenote hyperedges and the dashed lines denoteword alignments between the two forests.
Eachnode is assigned a unique identity for reference.Each hyperedge is associated with a probability,which we omit in Figure 1 for clarity.
In a forest,a node usually has multiple incoming hyperedges.We use IN(v) to denote the set of incoming hy-peredges of node v. For example, the source node?IP1?
has following two incoming hyperedges: 1e1 = ?
(NP-B6,VP3), IP1?e2 = ?
(NP2,VP-B5), IP1?1As there are both source and target forests, it might beconfusing by just using a span to refer to a node.
In addition,some nodes will often have the same labels and spans.
There-fore, it is more convenient to use an identity for referring to anode.
The notation ?IP1?
denotes the node that has a label of?IP?
and has an identity of ?1?.Formally, a packed parse forest is a compactrepresentation of all the derivations (i.e., parsetrees) for a given sentence under a context-freegrammar.
Huang and Chiang (2005) define a for-est as a tuple ?V,E, v?,R?, where V is a finite setof nodes, E is a finite set of hyperedges, v?
?
V isa distinguished node that denotes the goal item inparsing, and R is the set of weights.
For a givensentence w1:l = w1 .
.
.
wl, each node v ?
V is inthe form of Xi,j , which denotes the recognition ofnon-terminal X spanning the substring from posi-tions i through j (that is, wi+1 .
.
.
wj).
Each hy-peredge e ?
E is a triple e = ?T (e), h(e), f(e)?,where h(e) ?
V is its head, T (e) ?
V ?
is a vectorof tail nodes, and f(e) is a weight function fromR|T (e)| to R.Our forest-based tree-to-tree model is based ona probabilistic STSG (Eisner, 2003).
Formally,an STSG can be defined as a quintuple G =?Fs,Ft,Ss,St, P ?, where?
Fs andFt are the source and target alhabets,respectively,?
Ss and St are the source and target start sym-bols, and?
P is a set of production rules.
A rule r is atriple ?ts, tt,??
that describes the correspon-dence ?
between a source tree ts and a targettree tt.To integrate packed forests into tree-to-treetranslation, we model the process of synchronousgeneration of a source forest Fs and a target forestFt using a probabilistic STSG grammar:Pr(Fs, Ft) =?Ts?Fs?Tt?FtPr(Ts, Tt)=?Ts?Fs?Tt?Ft?d?DPr(d)=?Ts?Fs?Tt?Ft?d?D?r?dp(r) (1)where Ts is a source tree, Tt is a target tree, D isthe set of all possible derivations that transform Tsinto Tt, d is one such derivation, and r is a tree-to-tree rule.Table 1 shows a derivation of the forest pair inFigure 1.
A derivation is a sequence of tree-to-treerules.
Note that we use x to represent a nontermi-nal.559(1) IP(x1:NP-B, x2:VP)?
S(x1:NP, x2:VP)(2) NP-B(x1:NR)?
NP(x1:NNP)(3) NR(bushi)?
NNP(Bush)(4) VP(x1:PP, VP-B(x2:VV, AS(le), x3:NP-B))?
VP(x2:VBD, NP(DT(a), x3:NP), x1:PP)(5) PP(x1:P, x2:NP-B)?
PP(x1:IN, x2:NP)(6) P(yu)?
IN(with)(7) NP-B(x1:NR)?
NP(x1:NP)(8) NR(shalong) ?
NNP(Sharon)(9) VV(juxing) ?
VBD(held)(10) NP-B(x1:NN)?
NP(x1:NN)(11) NN(huitan) ?
NN(talk)Table 1: A minimal derivation of the forest pair in Figure 1.id span cspan complement consistent frontier counterparts1 1-6 1-2, 4-6 1 1 292 1-3 1, 5-6 2, 4 0 03 2-6 2, 4-6 1 1 1 284 2-3 5-6 1-2, 4 1 1 25, 265 4-6 2, 4 1, 5-6 1 06 1-1 1 2, 4-6 1 1 16, 227 3-3 6 1-2, 4-5 1 1 21, 248 6-6 4 1-2, 5-6 1 1 19, 239 1-1 1 2, 4-6 1 1 16, 2210 2-2 5 1-2, 4, 6 1 1 2011 2-2 5 1-2, 4, 6 1 1 2012 3-3 6 1-2, 4-5 1 1 21, 2413 4-4 2 1, 4-6 1 1 1714 5-5 1-2, 4-6 1 015 6-6 4 1-2, 5-6 1 1 19, 2316 1-1 1 2-4, 6 1 1 6, 917 2-2 4 1-3, 6 1 1 1318 3-3 1-4, 6 1 019 4-4 6 1-4 1 1 8, 1520 5-5 2 1, 3-4, 6 1 1 10, 1121 6-6 3 1-2, 4, 6 1 1 7, 1222 1-1 1 2-4, 6 1 1 6, 923 3-4 6 1-4 1 1 8, 1524 6-6 3 1-2, 4, 6 1 1 7, 1225 5-6 2-3 1, 4, 6 1 1 426 5-6 2-3 1, 4, 6 1 1 427 3-6 2-3, 6 1, 4 0 028 2-6 2-4, 6 1 1 1 329 1-6 1-4, 6 1 1 1Table 2: Node attributes of the example forest pair.3 Rule ExtractionGiven an aligned forest pair as shown in Figure1, how to extract all valid tree-to-tree rules thatexplain its synchronous generation process?
Byconstructing a theory that gives formal seman-tics to word alignments, Galley et al (2004)give principled answers to these questions for ex-tracting tree-to-string rules.
Their GHKM proce-dure draws connections among word alignments,derivations, and rules.
They first identify thetree nodes that subsume tree-string pairs consis-tent with word alignments and then extract rulesfrom these nodes.
By this means, GHKM provesto be able to extract all valid tree-to-string rulesfrom training instances.
Although originally de-veloped for the tree-to-string case, it is possible toextend GHKM to extract all valid tree-to-tree rulesfrom aligned packed forests.In this section, we introduce our tree-to-tree ruleextraction method adapted from GHKM, whichinvolves four steps: (1) identifying the correspon-dence between the nodes in forest pairs, (2) iden-tifying minimum rules, (3) inferring composedrules, and (4) estimating rule probabilities.3.1 Identifying Correspondence BetweenNodesTo learn tree-to-tree rules, we need to find alignedtree pairs in the forest pairs.
To do this, the start-ing point is to identify the correspondence be-tween nodes.
We propose a number of attributesfor nodes, most of which derive from GHKM, tofacilitate the identification.Definition 1 Given a node v, its span ?
(v) is anindex set of the words it covers.For example, the span of the source node?VP-B5?
is {4, 5, 6} as it covers three sourcewords: ?juxing?, ?le?, and ?huitan?.
For conve-nience, we use {4-6} to denotes a contiguous span{4, 5, 6}.Definition 2 Given a node v, its correspondingspan ?
(v) is the index set of aligned words on an-other side.For example, the corresponding span of thesource node ?VP-B5?
is {2, 4}, corresponding tothe target words ?held?
and ?talk?.Definition 3 Given a node v, its complement span?
(v) is the union of corresponding spans of nodesthat are neither antecedents nor descendants of v.For example, the complement span of the sourcenode ?VP-B5?
is {1, 5-6}, corresponding to targetwords ?Bush?, ?with?, and ?Sharon?.Definition 4 A node v is said to be consistent withalignment if and only if closure(?(v))??
(v) = ?.For example, the closure of the correspondingspan of the source node ?VP-B5?
is {2-4} andits complement span is {1, 5-6}.
As the intersec-tion of the closure and the complement span is anempty set, the source node ?VP-B5?
is consistentwith the alignment.560PP4NP-B7P11 NR12PP4P11 NP-B7PP4NP-B7P11 NR12PP26IN20NP24NNP21PP4P11 NP-B7PP26IN 20 NP24(a) (b) (c) (d)Figure 2: (a) A frontier tree; (b) a minimal frontier tree; (c) a frontier tree pair; (d) a minimal frontiertree pair.
All trees are taken from the example forest pair in Figure 1.
Shaded nodes are frontier nodes.Each node is assigned an identity for reference.Definition 5 A node v is said to be a frontier nodeif and only if:1. v is consistent;2.
There exists at least one consistent node v?
onanother side satisfying:?
closure(?(v?))
?
?(v);?
closure(?
(v)) ?
?(v?).v?
is said to be a counterpart of v. We use ?
(v) todenote the set of counterparts of v.A frontier node often has multiple counter-parts on another side due to the usage of unaryrules in parsers.
For example, the source node?NP-B6?
has two counterparts on the target side:?NNP16?
and ?NP22?.
Conversely, the target node?NNP16?
also has two counterparts counterpartson the source side: ?NR9?
and ?NP-B6?.The node attributes of the example forest pairare listed in Table 2.
We use identities to refer tonodes.
?cspan?
denotes corresponding span and?complement?
denotes complement span.
In Fig-ure 1, there are 12 frontier nodes (highlighted byshading) on the source side and 12 frontier nodeson the target side.
Note that while a consistentnode is equal to a frontier node in GHKM, this isnot the case in our method because we have a treeon the target side.
Frontier nodes play a criticalrole in forest-based rule extraction because theyindicate where to cut the forest pairs to obtain tree-to-tree rules.3.2 Identifying Minimum RulesGiven the frontier nodes, the next step is to iden-tify aligned tree pairs, from which tree-to-treerules derive.
Following Galley et al (2006), wedistinguish between minimal and composed rules.As a composed rule can be decomposed as a se-quence of minimal rules, we are particularly inter-ested in how to extract minimal rules.
Also, we in-troduce a number of notions to help identify mini-mal rules.Definition 6 A frontier tree is a subtree in a forestsatisfying:1.
Its root is a frontier node;2.
If the tree contains only one node, it must bea lexicalized frontier node;3.
If the tree contains more than one nodes,its leaves are either non-lexicalized frontiernodes or lexicalized non-frontier nodes.For example, Figure 2(a) shows a frontier treein which all nodes are frontier nodes.Definition 7 A minimal frontier tree is a frontiertree such that all nodes other than the root andleaves are non-frontier nodes.For example, Figure 2(b) shows a minimal fron-tier tree.Definition 8 A frontier tree pair is a triple?ts, tt,??
satisfying:1. ts is a source frontier tree;5612. tt is a target frontier tree;3.
The root of ts is a counterpart of that of tt;4.
There is a one-to-one correspondence ?
be-tween the frontier leaves of ts and tt.For example, Figure 2(c) shows a frontier treepair.Definition 9 A frontier tree pair ?ts, tt,??
is saidto be a subgraph of another frontier tree pair?ts?, tt?,???
if and only if:1. root(ts) = root(ts?);2.
root(tt) = root(tt?);3.
ts is a subgraph of ts?;4.
tt is a subgraph of tt?.For example, the frontier tree pair shown in Fig-ure 2(d) is a subgraph of that in Figure 2(c).Definition 10 A frontier tree pair is said to beminimal if and only if it is not a subgraph of anyother frontier tree pair that shares with the sameroot.For example, Figure 2(d) shows a minimal fron-tier tree pair.Our goal is to find the minimal frontier treepairs, which correspond to minimal tree-to-treerules.
For example, the tree pair shown in Figure2(d) denotes a minimal rule as follows:PP(x1:P,x2:NP-B)?
PP(x1:IN, x2:NP)Figure 3 shows the algorithm for identifyingminimal frontier tree pairs.
The input is a sourceforest Fs, a target forest Ft, and a source frontiernode v (line 1).
We use a set P to store collectedminimal frontier tree pairs (line 2).
We first callthe procedure FINDTREES(Fs , v) to identify a setof frontier trees rooted at v in Fs (line 3).
For ex-ample, for the source frontier node ?PP4?
in Figure1, we obtain two frontier trees:(PP4(P11)(NP-B7))(PP4(P11)(NP-B7(NR12)))Then, we try to find the set of correspondingtarget frontier trees (i.e., Tt).
For each counter-part v?
of v (line 5), we call the procedure FIND-TREES(Ft, v?)
to identify a set of frontier treesrooted at v?
in Ft (line 6).
For example, the source1: procedure FINDTREEPAIRS(Fs , Ft, v)2: P = ?3: Ts ?
FINDTREES(Fs , v)4: Tt ?
?5: for v?
?
?
(v) do6: Tt ?
Tt?
FINDTREES(Ft , v?
)7: end for8: for ?ts, tt?
?
Ts ?
Tt do9: if ts ?
tt then10: P ?
P ?
{?ts, tt,??
}11: end if12: end for13: for ?ts, tt,??
?
P do14: if ?
?ts?, tt?,???
?
P : ?ts?, tt?,???
?
?ts, tt,??
then15: P ?
P ?
{?ts, tt,??
}16: end if17: end for18: end procedureFigure 3: Algorithm for identifying minimal fron-tier tree pairs.frontier node ?PP4?
has two counterparts on thetarget side: ?NP25?
and ?PP26?.
There are fourtarget frontier trees rooted at the two nodes:(NP25(IN20)(NP24))(NP25(IN20)(NP24(NNP21)))(PP26(IN20)(NP24))(PP26(IN20)(NP24(NNP21)))Therefore, there are 2 ?
4 = 8 pairs of trees.We examine each tree pair ?ts, tt?
(line 8) to seewhether it is a frontier tree pair (line 9) and thenupdate P (line 10).
In the above example, all theeight tree pairs are frontier tree pairs.Finally, we keep only minimal frontier tree pairsin P (lines 13-15).
As a result, we obtain thefollowing two minimal frontier tree pairs for thesource frontier node ?PP4?:(PP4(P11)(NP-B7))?
(NP25(IN20)(NP24))(PP4(P11)(NP-B7))?
(PP26(IN20)(NP24))To maintain a reasonable rule table size, we re-strict that the number of nodes in a tree of an STSGrule is no greater than n, which we refer to as max-imal node count.It seems more efficient to let the procedureFINDTREES(F, v) to search for minimal frontier562trees rather than frontier trees.
However, a min-imal frontier tree pair is not necessarily a pair ofminimal frontier trees.
On our Chinese-Englishcorpus, we find that 38% of minimal frontier treepairs are not pairs of minimal frontier trees.
As aresult, we have to first collect all frontier tree pairsand then decide on the minimal ones.Table 1 shows some minimal rules extractedfrom the forest pair shown in Figure 1.3.3 Inferring Composed RulesAfter minimal rules are learned, composed rulescan be obtained by composing two or more min-imal rules.
For example, the composition of thesecond rule and the third rule in Table 1 producesa new rule:NP-B(NR(shalong))?
NP(NNP(Sharon))While minimal rules derive from minimal fron-tier tree pairs, composed rules correspond to non-minimal frontier tree pairs.3.4 Estimating Rule ProbabilitiesWe follow Mi and Huang (2008) to estimate thefractional count of a rule extracted from an alignedforest pair.
Intuitively, the relative frequency of asubtree that occurs in a forest is the sum of all thetrees that traverse the subtree divided by the sumof all trees in the forest.
Instead of enumeratingall trees explicitly and computing the sum of treeprobabilities, we resort to inside and outside prob-abilities for efficient calculation:c(r) =p(ts)?
?(root(ts))?
?v?leaves(ts) ?(v)?(v?s)?p(tt)?
?(root(tt))?
?v?leaves(tt) ?(v)?
(v?t)where c(r) is the fractional count of a rule, ts is thesource tree in r, tt is the target tree in r, root(?)
afunction that gets tree root, leaves(?)
is a functionthat gets tree leaves, and ?
(v) and ?
(v) are outsideand inside probabilities, respectively.4 DecodingGiven a source packed forest Fs, our decoder findsthe target yield of the single best derivation d thathas source yield of Ts(d) ?
Fs:e?
= e(argmaxd s.t.
Ts(d)?Fsp(d))(2)We extend the model in Eq.
1 to a log-linearmodel (Och and Ney, 2002) that uses the follow-ing eight features: relative frequencies in two di-rections, lexical weights in two directions, num-ber of rules used, language model score, numberof target words produced, and the probability ofmatched source tree (Mi et al, 2008).Given a source parse forest and an STSG gram-mar G, we first apply the conversion algorithmproposed by Mi et al (2008) to produce a trans-lation forest.
The translation forest has a simi-lar hypergraph structure.
While the nodes are thesame as those of the parse forest, each hyperedgeis associated with an STSG rule.
Then, the de-coder runs on the translation forest.
We use thecube pruning method (Chiang, 2007) to approxi-mately intersect the translation forest with the lan-guage model.
Traversing the translation forest ina bottom-up order, the decoder tries to build tar-get parses at each node.
After the first pass, weuse lazy Algorithm 3 (Huang and Chiang, 2005)to generate k-best translations for minimum errorrate training.5 Experiments5.1 Data PreparationWe evaluated our model on Chinese-to-Englishtranslation.
The training corpus contains 840KChinese words and 950K English words.
A tri-gram language model was trained on the Englishsentences of the training corpus.
We used the 2002NIST MT Evaluation test set as our developmentset, and used the 2005 NIST MT Evaluation testset as our test set.
We evaluated the translationquality using the BLEU metric, as calculated bymteval-v11b.pl with its default setting except thatwe used case-insensitive matching of n-grams.To obtain packed forests, we used the Chineseparser (Xiong et al, 2005) modified by HaitaoMi and the English parser (Charniak and Johnson,2005) modified by Liang Huang to produce en-tire parse forests.
Then, we ran the Python scripts(Huang, 2008) provided by Liang Huang to out-put packed forests.
To prune the packed forests,Huang (2008) uses inside and outside probabili-ties to compute the distance of the best derivationthat traverses a hyperedge away from the glob-ally best derivation.
A hyperedge will be prunedaway if the difference is greater than a thresholdp.
Nodes with all incoming hyperedges prunedare also pruned.
The greater the threshold p is,563p avg trees # of rules BLEU0 1 73, 614 0.2021 ?
0.00892 238.94 105, 214 0.2165 ?
0.00815 5.78 ?
106 347, 526 0.2336 ?
0.00788 6.59 ?
107 573, 738 0.2373 ?
0.008210 1.05 ?
108 743, 211 0.2385 ?
0.0084Table 3: Comparison of BLEU scores for tree-based and forest-based tree-to-tree models.0.040.050.060.070.080.090.100  1  2  3  4  5  6  7  8  9  10  11coveragemaximal node countp=0p=2p=5p=8p=10Figure 4: Coverage of lexicalized STSG rules onbilingual phrases.the more parses are encoded in a packed forest.We obtained word alignments of the trainingdata by first running GIZA++ (Och and Ney, 2003)and then applying the refinement rule ?grow-diag-final-and?
(Koehn et al, 2003).5.2 Forests Vs. 1-best TreesTable 3 shows the BLEU scores of tree-based andforest-based tree-to-tree models achieved on thetest set over different pruning thresholds.
p is thethreshold for pruning packed forests, ?avg trees?is the average number of trees encoded in one for-est on the test set, and ?# of rules?
is the numberof STSG rules used on the test set.
We restrict thatboth source and target trees in a tree-to-tree rulecan contain at most 10 nodes (i.e., the maximalnode count n = 10).
The 95% confidence inter-vals were computed using Zhang ?s significancetester (Zhang et al, 2004).We chose five different pruning thresholds inour experiments: p = 0, 2, 5, 8, 10.
The forestspruned by p = 0 contained only 1-best tree persentence.
With the increase of p, the average num-ber of trees encoded in one forest rose dramati-cally.
When p was set to 10, there were over 100Mparses encoded in one forest on average.p extraction decoding0 1.26 6.762 2.35 8.525 6.34 14.878 8.51 19.7810 10.21 25.81Table 4: Comparison of rule extraction time (sec-onds/1000 sentence pairs) and decoding time (sec-ond/sentence)Moreover, the more trees are encoded in packedforests, the more rules are made available toforest-based models.
The number of rules whenp = 10 was almost 10 times of p = 0.
With theincrease of the number of rules used, the BLEUscore increased accordingly.
This suggests thatpacked forests enable tree-to-tree model to learnmore useful rules on the training data.
However,when a pack forest encodes over 1M parses persentence, the improvements are less significant,which echoes the results in (Mi et al, 2008).The forest-based tree-to-tree model outper-forms the original model that uses 1-best treesdramatically.
The absolute improvement of 3.6BLEU points (from 0.2021 to 0.2385) is statis-tically significant at p < 0.01 using the sign-test as described by Collins et al (2005), with700(+1), 360(-1), and 15(0).
We also ran Moses(Koehn et al, 2007) with its default setting us-ing the same data and obtained a BLEU score of0.2366, slightly lower than our best result (i.e.,0.2385).
But this difference is not statistically sig-nificant.5.3 Effect on Rule CoverageFigure 4 demonstrates the effect of pruning thresh-old and maximal node count on rule coverage.We extracted phrase pairs from the training datato investigate how many phrase pairs can be cap-tured by lexicalized tree-to-tree rules that con-tain only terminals.
We set the maximal lengthof phrase pairs to 10.
For tree-based tree-to-treemodel, the coverage was below 8% even the max-imal node count was set to 10.
This suggests thatconventional tree-to-tree models lose over 92%linguistically unmotivated mappings due to hardsyntactic constraints.
The absence of such non-syntactic mappings prevents tree-based tree-to-tree models from achieving comparable results tophrase-based models.
With more parses included5640.090.100.110.120.130.140.150.160.170.180.190.200  1  2  3  4  5  6  7  8  9  10  11BLEUmaximal node countFigure 5: Effect of maximal node count on BLEUscores.in packed forests, the rule coverage increased ac-cordingly.
When p = 10 and n = 10, the cov-erage was 9.7%, higher than that of p = 0.
Asa result, packed forests enable tree-to-tree modelsto capture more useful source-target mappings andtherefore improve translation quality.
25.4 Training and Decoding TimeTable 4 gives the rule extraction time (sec-onds/1000 sentence pairs) and decoding time (sec-ond/sentence) with varying pruning thresholds.We found that the extraction time grew faster thandecoding time with the increase of p. One possi-ble reason is that the number of frontier tree pairs(see Figure 3) rose dramatically when more parseswere included in packed forests.5.5 Effect of Maximal Node CountFigure 5 shows the effect of maximal node counton BLEU scores.
With the increase of maximalnode count, the BLEU score increased dramati-cally.
This implies that allowing tree-to-tree rulesto capture larger contexts will strengthen the ex-pressive power of tree-to-tree model.5.6 Results on Larger DataWe also conducted an experiment on larger datato further examine the effectiveness of our ap-proach.
We concatenated the small corpus weused above and the FBIS corpus.
After remov-ing the sentences that we failed to obtain forests,2Note that even we used packed forests, the rule coveragewas still very low.
One reason is that we set the maximalphrase length to 10 words, while an STSG rule with 10 nodesin each tree usually cannot subsume 10 words.the new training corpus contained about 260K sen-tence pairs with 7.39M Chinese words and 9.41MEnglish words.
We set the forest pruning thresholdp = 5.
Moses obtained a BLEU score of 0.3043and our forest-based tree-to-tree system achieveda BLEU score of 0.3059.
The difference is still notsignificant statistically.6 Related WorkIn machine translation, the concept of packed for-est is first used by Huang and Chiang (2007) tocharacterize the search space of decoding with lan-guage models.
The first direct use of packed for-est is proposed by Mi et al (2008).
They replace1-best trees with packed forests both in trainingand decoding and show superior translation qual-ity over the state-of-the-art hierarchical phrase-based system.
We follow the same direction andapply packed forests to tree-to-tree translation.Zhang et al (2008) present a tree-to-tree modelthat uses STSG.
To capture non-syntactic phrases,they apply tree-sequence rules (Liu et al, 2007)to tree-to-tree models.
Their extraction algorithmfirst identifies initial rules and then obtains abstractrules.
While this method works for 1-best treepairs, it cannot be applied to packed forest pairsbecause it is impractical to enumerate all tree pairsover a phrase pair.While Galley (2004) describes extracting tree-to-string rules from 1-best trees, Mi and Huang etal.
(2008) go further by proposing a method forextracting tree-to-string rules from aligned forest-string pairs.
We follow their work and focus onidentifying tree-tree pairs in a forest pair, which ismore difficult than the tree-to-string case.7 ConclusionWe have shown how to improve tree-to-tree trans-lation with packed forests, which compactly en-code exponentially many parses.
To learn STSGrules from aligned forest pairs, we first identifyminimal rules and then get composed rules.
Thedecoder finds the best derivation that have thesource yield of one source tree in the forest.
Ex-periments show that using packed forests in tree-to-tree translation results in dramatic improve-ments over using 1-best trees.
Our system alsoachieves comparable performance with the state-of-the-art phrase-based system Moses.565AcknowledgementThe authors were supported by National NaturalScience Foundation of China, Contracts 60603095and 60736014, and 863 State Key Project No.2006AA010108.
Part of this work was donewhile Yang Liu was visiting the SMT group ledby Stephan Vogel at CMU.
We thank the anony-mous reviewers for their insightful comments.Many thanks go to Liang Huang, Haitao Mi, andHao Xiong for their invaluable help in producingpacked forests.
We are also grateful to AndreasZollmann, Vamshi Ambati, and Kevin Gimpel fortheir helpful feedback.ReferencesEugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proc.
of ACL 2005.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2).Brooke Cowan, Ivona Kuc?erova?, and Michael Collins.2006.
A discriminative model for tree-to-tree trans-lation.
In Proc.
of EMNLP 2006.Steve DeNeefe, Kevin Knight, Wei Wang, and DanielMarcu.
2007.
What can syntax-based MT learnfrom phrase-based MT?
In Proc.
of EMNLP 2007.Yuan Ding and Martha Palmer.
2005.
Machine trans-lation using probabilistic synchronous dependencyinsertion grammars.
In Proc.
of ACL 2005.Jason Eisner.
2003.
Learning non-isomorphic treemappings for machine translation.
In Proc.
of ACL2003 (Companion Volume).Michel Galley, Mark Hopkins, Kevin Knight, andDaniel Marcu.
2004.
What?s in a translation rule?In Proc.
of NAACL/HLT 2004.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proc.of COLING/ACL 2006.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proc.
of IWPT 2005.Liang Huang and David Chiang.
2007.
Forest rescor-ing: Faster decoding with integrated language mod-els.
In Proc.
of ACL 2007.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proc.
of AMTA 2006.Liang Huang.
2008.
Forest reranking: Discrimina-tive parsing with non-local features.
In Proc.
ofACL/HLT 2008.Phillip Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proc.
ofNAACL 2003.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProc.
of ACL 2007 (demonstration session).Yang Liu, Qun Liu, and Shouxun Lin.
2006.
Tree-to-string alignment template for statistical machinetranslation.
In Proc.
of COLING/ACL 2006.Yang Liu, Yun Huang, Qun Liu, and Shouxun Lin.2007.
Forest-to-string statistical translation rules.
InProc.
of ACL 2007.Daniel Marcu, Wei Wang, Abdessamad Echihabi, andKevin Knight.
2006.
Spmt: Statistical machinetranslation with syntactified target language phrases.In Proc.
of EMNLP 2006.Haitao Mi and Liang Huang.
2008.
Forest-based trans-lation rule extraction.
In Proc.
of EMNLP 2008.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proc.
of ACL/HLT 2008.Franz J. Och and Hermann Ney.
2002.
Discriminativetraining and maximum entropy models for statisticalmachine translation.
In Proc.
of ACL 2002.Franz J. Och and Hermann Ney.
2003.
A systematiccomparison of various statistical alignment models.Computational Linguistics, 29(1).Chris Quirk and Simon Corston-Oliver.
2006.
Theimpact of parsing quality on syntactically-informedstatistical machine translation.
In Proc.
of EMNLP2006.Libin Shen, Jinxi Xu, and Ralph Weischedel.
2008.
Anew string-to-dependency machine translation algo-rithm with a target dependency language model.
InProc.
of ACL/HLT 2008.Deyi Xiong, Shuanglong Li, Qun Liu, and ShouxunLin.
2005.
Parsing the penn chinese treebank withsemantic knowledge.
In Proc.
of IJCNLP 2005.Ying Zhang, Stephan Vogel, and Alex Waibel.
2004.Interpreting bleu/nist scores how much improve-ment do we need to have a better system?
In Proc.of LREC 2004.Min Zhang, Hongfei Jiang, Aiti Aw, Haizhou Li,Chew Lim Tan, and Sheng Li.
2008.
A treesequence alignment-based tree-to-tree translationmodel.
In Proc.
of ACL/HLT 2008.566
