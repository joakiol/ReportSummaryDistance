Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 181?189,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPActive Learning for Multilingual Statistical Machine Translation?Gholamreza Haffari and Anoop SarkarSchool of Computing Science, Simon Fraser UniversityBritish Columbia, Canada{ghaffar1,anoop}@cs.sfu.caAbstractStatistical machine translation (SMT)models require bilingual corpora for train-ing, and these corpora are often multi-lingual with parallel text in multiple lan-guages simultaneously.
We introduce anactive learning task of adding a new lan-guage to an existing multilingual set ofparallel text and constructing high qualityMT systems, from each language in thecollection into this new target language.We show that adding a new language usingactive learning to the EuroParl corpus pro-vides a significant improvement comparedto a random sentence selection baseline.We also provide new highly effective sen-tence selection methods that improve ALfor phrase-based SMT in the multilingualand single language pair setting.1 IntroductionThe main source of training data for statisticalmachine translation (SMT) models is a parallelcorpus.
In many cases, the same information isavailable in multiple languages simultaneously asa multilingual parallel corpus, e.g., European Par-liament (EuroParl) and U.N. proceedings.
In thispaper, we consider how to use active learning (AL)in order to add a new language to such a multilin-gual parallel corpus and at the same time we con-struct an MT system from each language in theoriginal corpus into this new target language.
Weintroduce a novel combined measure of translationquality for multiple target language outputs (thesame content from multiple source languages).The multilingual setting provides new opportu-nities for AL over and above a single languagepair.
This setting is similar to the multi-task ALscenario (Reichart et al, 2008).
In our case, themultiple tasks are individual machine translationtasks for several language pairs.
The nature of thetranslation processes vary from any of the source?Thanks to James Peltier for systems support for our ex-periments.
This research was partially supported by NSERC,Canada (RGPIN: 264905) and an IBM Faculty Award.languages to the new language depending on thecharacteristics of each source-target language pair,hence these tasks are competing for annotating thesame resource.
However it may be that in a singlelanguage pair, AL would pick a particular sentencefor annotation, but in a multilingual setting, a dif-ferent source language might be able to provide agood translation, thus saving annotation effort.
Inthis paper, we explore how multiple MT systemscan be used to effectively pick instances that aremore likely to improve training quality.Active learning is framed as an iterative learn-ing process.
In each iteration new human labeledinstances (manual translations) are added to thetraining data based on their expected training qual-ity.
However, if we start with only a small amountof initial parallel data for the new target language,then translation quality is very poor and requiresa very large injection of human labeled data tobe effective.
To deal with this, we use a novelframework for active learning: we assume we aregiven a small amount of parallel text and a largeamount of monolingual source language text; us-ing these resources, we create a large noisy par-allel text which we then iteratively improve usingsmall injections of human translations.
When webuild multiple MT systems from multiple sourcelanguages to the new target language, each MTsystem can be seen as a different ?view?
on the de-sired output translation.
Thus, we can train ourmultiple MT systems using either self-training orco-training (Blum and Mitchell, 1998).
In self-training each MT system is re-trained using humanlabeled data plus its own noisy translation outputon the unlabeled data.
In co-training each MT sys-tem is re-trained using human labeled data plusnoisy translation output from the other MT sys-tems in the ensemble.
We use consensus transla-tions (He et al, 2008; Rosti et al, 2007; Matusovet al, 2006) as an effective method for co-trainingbetween multiple MT systems.This paper makes the following contributions:?
We provide a new framework for multilingualMT, in which we build multiple MT systemsand add a new language to an existing multi-lingual parallel corpus.
The multilingual set-181ting allows new features for active learningwhich we exploit to improve translation qual-ity while reducing annotation effort.?
We introduce new highly effective sentenceselection methods that improve phrase-basedSMT in the multilingual and single languagepair setting.?
We describe a novel co-training based activelearning framework that exploits consensustranslations to effectively select only thosesentences that are difficult to translate for allMT systems, thus sharing annotation cost.?
We show that using active learning to adda new language to the EuroParl corpus pro-vides a significant improvement compared tothe strong random sentence selection base-line.2 AL-SMT: Multilingual SettingConsider a multilingual parallel corpus, such asEuroParl, which contains parallel sentences forseveral languages.
Our goal is to add a new lan-guage to this corpus, and at the same time to con-struct high quality MT systems from the existinglanguages (in the multilingual corpus) to the newlanguage.
This goal is formalized by the followingobjective function:O =D?d=1?d ?
TQ(MF d?E) (1)where F d?s are the source languages in the mul-tilingual corpus (D is the total number of lan-guages), and E is the new language.
The transla-tion quality is measured by TQ for individual sys-temsMF d?E ; it can be BLEU score or WER/PER(Word error rate and position independent WER)which induces a maximization or minimizationproblem, respectively.
The non-negative weights?d reflect the importance of the different transla-tion tasks and?d ?d = 1.
AL-SMT formulationfor single language pair is a special case of thisformulation where only one of the ?d?s in the ob-jective function (1) is one and the rest are zero.Moreover the algorithmic framework that we in-troduce in Sec.
2.1 for AL in the multilingual set-ting includes the single language pair setting as aspecial case (Haffari et al, 2009).We denote the large unlabeled multilingual cor-pus by U := {(f1j , .., fDj )}, and the small labeledmultilingual corpus by L := {(f1i , .., fDi , ei)}.
Weoverload the term entry to denote a tuple in L orin U (it should be clear from the context).
For asingle language pair we use U and L.2.1 The Algorithmic FrameworkAlgorithm 1 represents our AL approach for themultilingual setting.
We train our initial MT sys-tems {MF d?E}Dd=1 on the multilingual corpus L,and use them to translate all monolingual sen-tences in U.
We denote sentences in U togetherwith their multiple translations by U+ (line 4 ofAlgorithm 1).
Then we retrain the SMT sys-tems on L ?
U+ and use the resulting model todecode the test set.
Afterwards, we select andremove a subset of highly informative sentencesfrom U, and add those sentences together withtheir human-provided translations to L. This pro-cess is continued iteratively until a certain level oftranslation quality is met (we use the BLEU score,WER and PER) (Papineni et al, 2002).
In thebaseline, against which we compare our sentenceselection methods, the sentences are chosen ran-domly.When (re-)training the models, two phrase ta-bles are learned for each SMT model: one fromthe labeled data L and the other one from pseudo-labeled data U+ (which we call the main and aux-iliary phrase tables respectively).
(Ueffing et al,2007; Haffari et al, 2009) show that treating U+as a source for a new feature function in a log-linear model for SMT (Och and Ney, 2004) allowsus to maximally take advantage of unlabeled databy finding a weight for this feature using minimumerror-rate training (MERT) (Och, 2003).Since each entry in U+ has multiple transla-tions, there are two options when building the aux-iliary table for a particular language pair (F d, E):(i) to use the corresponding translation ed of thesource language in a self-training setting, or (ii) touse the consensus translation among all the trans-lation candidates (e1, .., eD) in a co-training set-ting (sharing information between multiple SMTmodels).A whole range of methods exist in the literaturefor combining the output translations of multipleMT systems for a single language pair, operatingeither at the sentence, phrase, or word level (He etal., 2008; Rosti et al, 2007; Matusov et al, 2006).The method that we use in this work operates atthe sentence level, and picks a single high qual-ity translation from the union of the n-best listsgenerated by multiple SMT models.
Sec.
5 gives182Algorithm 1 AL-SMT-Multiple1: Given multilingual corpora L and U2: {MF d?E}Dd=1 = multrain(L, ?
)3: for t = 1, 2, ... do4: U+ = multranslate(U, {MF d?E}Dd=1)5: Select k sentences from U+, and ask a hu-man for their true translations.6: Remove the k sentences from U, and addthe k sentence pairs (translated by human)to L7: {MF d?E}Dd=1 = multrain(L,U+)8: Monitor the performance on the test set9: end formore details about features which are used in ourconsensus finding method, and how it is trained.Now let us address the important question of se-lecting highly informative sentences (step 5 in theAlgorithm 1) in the following section.3 Sentence Selection: Multiple LanguagePairsThe goal is to optimize the objective function(1) with minimum human effort in providing thetranslations.
This motivates selecting sentenceswhich are maximally beneficial for all the MT sys-tems.
In this section, we present several protocolsfor sentence selection based on the combined in-formation from multiple language pairs.3.1 Alternating SelectionThe simplest selection protocol is to choose k sen-tences (entries) in the first iteration of AL whichimprove maximally the first modelMF 1?E , whileignoring other models.
In the second iteration, thesentences are selected with respect to the secondmodel, and so on (Reichart et al, 2008).3.2 Combined RankingPick any AL-SMT scoring method for a single lan-guage pair (see Sec.
4).
Using this method, werank the entries in unlabeled data U for each trans-lation task defined by language pair (F d, E).
Thisresults in several ranking lists, each of which rep-resents the importance of entries with respect toa particular translation task.
We combine theserankings using a combined score:Score((f1, .., fD))=D?d=1?dRankd(fd)Rankd(.)
is the ranking of a sentence in the list forthe dth translation task (Reichart et al, 2008).3.3 Disagreement Among the TranslationsDisagreement among the candidate translations ofa particular entry is evidence for the difficulty ofthat entry for different translation models.
Thereason is that disagreement increases the possibil-ity that most of the translations are not correct.Therefore it would be beneficial to ask human forthe translation of these hard entries.Now the question is how to quantify the no-tion of disagreement among the candidate trans-lations (e1, .., eD).
We propose two measures ofdisagreement which are related to the portion ofshared n-grams (n ?
4) among the translations:?
Let ec be the consensus among all the can-didate translations, then define the disagree-ment as?d ?d(1?
BLEU(ec, ed)).?
Based on the disagreement of every pairof candidate translations:?d ?d?d?
(1 ?BLEU(ed?, ed)).For the single language pair setting, (Haffari etal., 2009) presents and compares several sentenceselection methods for statistical phrase-based ma-chine translation.
We introduce novel techniqueswhich outperform those methods in the next sec-tion.4 Sentence Selection: Single LanguagePairPhrases are basic units of translation in phrase-based SMT models.
The phrases which may po-tentially be extracted from a sentence indicate itsinformativeness.
The more new phrases a sen-tence can offer, the more informative it is; since itboosts the generalization of the model.
Addition-ally phrase translation probabilities need to be es-timated accurately, which means sentences that of-fer phrases whose occurrences in the corpus wererare are informative.
When selecting new sen-tences for human translation, we need to pay atten-tion to this tradeoff between exploration and ex-ploitation, i.e.
selecting sentences to discover newphrases v.s.
estimating accurately the phrase trans-lation probabilities.
Smoothing techniques partlyhandle accurate estimation of translation probabil-ities when the events occur rarely (indeed it is themain reason for smoothing).
So we mainly focuson how to expand effectively the lexicon or set ofphrases of the model.The more frequent a phrase (not a phrase pair)is in the unlabeled data, the more important it is to183know its translation; since it is more likely to seeit in test data (specially when the test data is in-domain with respect to unlabeled data).
The morefrequent a phrase is in the labeled data, the moreunimportant it is; since probably we have observedmost of its translations.In the labeled dataL, phrases are the ones whichare extracted by the SMT models; but what arethe candidate phrases in the unlabeled data U?We use the currently trained SMT models to an-swer this question.
Each translation in the n-bestlist of translations (generated by the SMT mod-els) corresponds to a particular segmentation ofa sentence, which breaks that sentence into sev-eral fragments (see Fig.
1).
Some of these frag-ments are the source language part of a phrase pairavailable in the phrase table, which we call regularphrases and denote their set byXregs for a sentences.
However, there are some fragments in the sen-tence which are not covered by the phrase table ?possibly because of the OOVs (out-of-vocabularywords) or the constraints imposed by the phraseextraction algorithm ?
called Xoovs for a sentences.
Each member of Xoovs offers a set of potentialphrases (also referred to as OOV phrases) whichare not observed due to the latent segmentation ofthis fragment.
We present two generative modelsfor the phrases and show how to estimate and usethem for sentence selection.4.1 Model 1In the first model, the generative story is to gen-erate phrases for each sentence based on indepen-dent draws from a multinomial.
The sample spaceof the multinomial consists of both regular andOOV phrases.We build two models, i.e.
two multinomials,one for labeled data and the other one for unla-beled data.
Each model is trained by maximizingthe log-likelihood of its corresponding data:LD :=?s?D?P (s)?x?XslogP (x|?D) (2)where D is either L or U , ?P (s) is the empiri-cal distribution of the sentences1, and ?D is theparameter vector of the corresponding probability1P?
(s) is the number of times that the sentence s is seenin D divided by the number of all sentences in D.distribution.
When x ?
Xoovs , we will haveP (x|?U ) =?h?HxP (x, h|?U )=?h?HxP (h)P (x|h,?U )=1|Hx|?h?Hx?y?Y hx?U (y) (3)where Hx is the space of all possible segmenta-tions for the OOV fragment x, Y hx is the result-ing phrases from x based on the segmentation h,and ?U (y) is the probability of the OOV phrasey in the multinomial associated with U .
We letHx to be all possible segmentations of the frag-ment x for which the resulting phrase lengths arenot greater than the maximum length constraint forphrase extraction in the underlying SMT model.Since we do not know anything about the segmen-tations a priori, we have put a uniform distributionover such segmentations.Maximizing (2) to find the maximum likelihoodparameters for this model is an extremely diffi-cult problem2.
Therefore, we maximize the fol-lowing lower-bound on the log-likelihood whichis derived using Jensen?s inequality:LD ?
?s?D?P (s)[ ?x?Xregslog ?D(x)+?x?Xoovs?h?Hx1|Hx|?y?Y hxlog ?D(y)](4)Maximizing (4) amounts to set the probability ofeach regular / potential phrase proportional to itscount / expected count in the data D.Let ?k(xi:j) be the number of possible segmen-tations from position i to position j of an OOVfragment x, and k is the maximum phrase length;?k(x1:|x|) =????
?0, if |x| = 01, if |x| = 1?ki=1 ?k(xi+1:|x|), otherwisewhich gives us a dynamic programming algorithmto compute the number of segmentation |Hx| =?k(x1:|x|) of the OOV fragment x.
The expectedcount of a potential phrase y based on an OOVsegment x is (see Fig.
1.c):E[y|x] =?i?j ?
[y=xi:j ]?k(x1:i?1)?k(xj+1:|x|)?k(x)2Setting partial derivatives of the Lagrangian to zeroamounts to finding the roots of a system of multivariate poly-nomials (a major topic in Algebraic Geometry).184i will go to school on fridayRegular PhrasesOOV segmentgotoschoolgo toto school2/32/31/31/31/3i willin fridayXXXXXX.01.004.........(a)potential phr.sourcetarget probcount(b)(c)Figure 1: The given sentence in (b) is segmented, based on the source side phrases extracted from the phrase table in (a), toyield regular phrases and OOV segment.
The table in (c) shows the potential phrases extracted from the OOV segment ?go toschool?
and their expected counts (denoted by count) where the maximum length for the potential phrases is set to 2.
In theexample, ?go to school?
has 3 segmentations with maximum phrase length 2: (go)(to school), (go to)(school), (go)(to)(school).where ?
[C] is 1 if the condition C is true, and zerootherwise.
We have used the fact that the num-ber of occurrences of a phrase spanning the indices[i, j] is the product of the number of segmentationsof the left and the right sub-fragments, which are?k(x1:i?1) and ?k(xj+1:|x|) respectively.4.2 Model 2In the second model, we consider a mixture modelof two multinomials responsible for generatingphrases in each of the labeled and unlabeled datasets.
To generate a phrase, we first toss a coin anddepending on the outcome we either generate thephrase from the multinomial associated with regu-lar phrases ?regU or potential phrases ?oovU :P (x|?U ) := ?U?regU (x) + (1?
?U )?oovU (x)where ?U includes the mixing weight ?
and theparameter vectors of the two multinomials.
Themixture model associated with L is written simi-larly.
The parameter estimation is based on maxi-mizing a lower-bound on the log-likelihood whichis similar to what was done for the Model 1.4.3 Sentence ScoringThe sentence score is a linear combination of twoterms: one coming from regular phrases and theother from OOV phrases:?1(s) :=?|Xregs |?x?XregslogP (x|?U )P (x|?L)+1?
?|Xoovs |?x?Xoovs?h?Hx1|Hx|log?y?Y hxP (y|?U )P (y|?L)where we use either Model 1 or Model 2 forP (.|?D).
The first term is the log probability ra-tio of regular phrases under phrase models corre-sponding to unlabeled and labeled data, and thesecond term is the expected log probability ratio(ELPR) under the two models.
Another option forthe contribution of OOV phrases is to take log ofexpected probability ratio (LEPR):?2(s) :=?|Xregs |?x?XregslogP (x|?U )P (x|?L)+1?
?|Xoovs |?x?Xoovslog?h?Hx1|Hx|?y?Y hxP (y|?U )P (y|?L)It is not difficult to prove that there is no differencebetween Model 1 and Model 2 when ELPR scor-ing is used for sentence selection.
However, thesituation is different for LEPR scoring: the twomodels produce different sentence rankings in thiscase.5 ExperimentsCorpora.
We pre-processed the EuroParl corpus(http://www.statmt.org/europarl) (Koehn, 2005)and built a multilingual parallel corpus with653,513 sentences, excluding the Q4/2000 por-tion of the data (2000-10 to 2000-12) which isreserved as the test set.
We subsampled 5,000sentences as the labeled data L and 20,000 sen-tences as U for the pool of untranslated sentences(while hiding the English part).
The test set con-sists of 2,000 multi-language sentences and comesfrom the multilingual parallel corpus built fromQ4/2000 portion of the data.Consensus Finding.
Let T be the union of the n-best lists of translations for a particular sentence.The consensus translation tc isargmaxt?Tw1LM(t)|t|+w2Qd(t)|t|+w3Rd(t)+w4,dwhere LM(t) is the score from a 3-gram languagemodel, Qd(t) is the translation score generated bythe decoder for MF d?E if t is produced by thedth SMT model, Rd(t) is the rank of the transla-tion in the n-best list produced by the dth model,w4,d is a bias term for each translation model tomake their scores comparable, and |t| is the length1851000 2000 3000 4000 500022.622.722.822.92323.123.223.323.423.523.6Added SentencesBLEU ScoreFrench to EnglishModel 2 ?
LEPRModel 1 ?
ELPRGeom PhraseRandom1000 2000 3000 4000 500023.223.423.623.82424.224.424.624.825Added SentencesBLEU ScoreSpanish to EnglishModel 2 ?
LEPRModel 1 ?
ELPRGeom PhraseRandom1000 2000 3000 4000 500016.216.416.616.81717.217.417.617.8Added SentencesBLEU ScoreGerman to EnglishModel 2 ?
LEPRModel 1 ?
ELPRGeom PhraseRandomFigure 2: The performance of different sentence selection strategies as the iteration of AL loop goes on for three translationtasks.
Plots show the performance of sentence selection methods for single language pair in Sec.
4 compared to the GeomPhrase(Haffari et al, 2009) and random sentence selection baseline.of the translation sentence.
The number of weightswi is 3 plus the number of source languages, andthey are trained using minimum error-rate training(MERT) to maximize the BLEU score (Och, 2003)on a development set.Parameters.
We use add- smoothing where  =.5 to smooth the probabilities in Sec.
4; moreover?
= .4 for ELPR and LEPR sentence scoring andmaximum phrase length k is set to 4.
For the mul-tilingual experiments (which involve four sourcelanguages) we set ?d = .25 to make the impor-tance of individual translation tasks equal.0 1000 2000 3000 4000 50001818.51919.52020.5Added SentencesAvgBLEUScoreMulilingual da?de?nl?sv to enSelf?TrainingCo?TrainingFigure 3: Random sentence selection baseline using self-training and co-training (Germanic languages to English).5.1 ResultsFirst we evaluate the proposed sentence selectionmethods in Sec.
4 for the single language pair.Then the best method from the single languagepair setting is used to evaluate sentence selectionmethods for AL in multilingual setting.
Afterbuilding the initial MT system for each experi-ment, we select and remove 500 sentences fromU and add them together with translations to L for10 total iterations.
The random sentence selectionbaselines are averaged over 3 independent runs.mode self-train co-trainMethod wer per wer perCombined Rank 40.2 30.0 40.0 29.6Alternate 41.0 30.2 40.1 30.1Disagree-Pairwise 41.9 32.0 40.5 30.9Disagree-Center 41.8 31.8 40.6 30.7Random Baseline 41.6 31.0 40.5 30.7Germanic languages to Englishmode self-train co-trainMethod wer per wer perCombined Rank 37.7 27.3 37.3 27.0Alternate 37.7 27.3 37.3 27.0Random Baseline 38.6 28.1 38.1 27.6Romance languages to EnglishTable 1: Comparison of multilingual selection methods withWER (word error rate), PER (position independent WER).95% confidence interval for WER numbers is 0.7 and for PERnumbers is 0.5.
Bold: best result, italic: significantly better.We use three language pairs in our single lan-guage pair experiments: French-English, German-English, and Spanish- English.
In addition to ran-dom sentence selection baseline, we also comparethe methods proposed in this paper to the bestmethod reported in (Haffari et al, 2009) denotedby GeomPhrase, which differs from our modelssince it considers each individual OOV segment asa single OOV phrase and does not consider subse-quences.
The results are presented in Fig.
2.
Se-lecting sentences based on our proposed methodsoutperform the random sentence selection baselineand GeomPhrase.
We suspect for the situationswhere L is out-of-domain and the average phraselength is relatively small, our method will outper-form GeomPhrase even more.For the multilingual experiments, we use Ger-manic (German, Dutch, Danish, Swedish) and Ro-mance (French, Spanish, Italian, Portuguese3) lan-3A reviewer pointed out that EuroParl English-Portuguese1860 1000 2000 3000 4000 500018.218.418.618.81919.219.419.619.820Added SentencesAvgBLEU ScoreSelf?Train Mulilingual da?de?nl?sv to enAlternateCombineRankDisagree?PairwiseDisagree?CenterRandom1000 1500 2000 2500 3000 3500 4000 4500 500019.319.419.519.619.719.819.92020.120.220.3Added SentencesAvgBLEU ScoreCo?Train Mulilingual da?de?nl?sv to enAlternateCombineRankDisagree?PairwiseDisagree?CenterRandom0 1000 2000 3000 4000 500021.621.82222.222.422.622.82323.223.423.6Added SentencesAvgBLEU ScoreSelf?Train Mulilingual fr?es?it?pt to enAlternateCombineRankRandom1000 1500 2000 2500 3000 3500 4000 4500 500022.622.82323.223.423.623.8Added SentencesAvgBLEU ScoreCo?Train Mulilingual fr?es?it?pt to enAlternateCombineRankRandomFigure 4: The left/right plot show the performance of our AL methods for multilingual setting combined with self-training/co-training.
The sentence selection methods from Sec.
3 are compared with random sentence selection baseline.
The top plots cor-respond to Danish-German-Dutch-Swedish to English, and the bottom plots correspond to French-Spanish-Italian-Portugueseto English.guages as the source and English as the target lan-guage as two sets of experiments.4 Fig.
3 showsthe performance of random sentence selection forAL combined with self-training/co-training for themulti-source translation from the four Germaniclanguages to English.
It shows that the co-trainingmode outperforms the self-training mode by al-most 1 BLEU point.
The results of selectionstrategies in the multilingual setting are presentedin Fig.
4 and Tbl.
1.
Having noticed that Model1 with ELPR performs well in the single languagepair setting, we use it to rank entries for individualtranslation tasks.
Then these rankings are used by?Alternate?
and ?Combined Rank?
selection strate-gies in the multilingual case.
The ?CombinedRank?
method outperforms all the other methodsincluding the strong random selection baseline inboth self-training and co-training modes.
Thedisagreement-based selection methods underper-form the baseline for translation of Germanic lan-guages to English, so we omitted them for the Ro-mance language experiments.5.2 AnalysisThe basis for our proposed methods has been thepopularity of regular/OOV phrases in U and theirdata is very noisy and future work should omit this pair.4Choice of Germanic and Romance for our experimentalsetting is inspired by results in (Cohn and Lapata, 2007)unpopularity in L, which is measured by P (x|?U )P (x|?L) .We need P (x|?U ), the estimated distribution ofphrases in U , to be as similar as possible to P ?
(x),the true distribution of phrases in U .
We investi-gate this issue for regular/OOV phrases as follows:?
Using the output of the initially trained MT sys-tem on L, we extract the regular/OOV phrases asdescribed in ?4.
The smoothed relative frequen-cies give us the regular/OOV phrasal distributions.?
Using the true English translation of the sen-tences in U , we extract the true phrases.
Separat-ing the phrases into two sets of regular and OOVphrases defined by the previous step, we use thesmoothed relative frequencies and form the trueOOV/regular phrasal distributions.We use the KL-divergence to see how dissim-ilar are a pair of given probability distributions.As Tbl.
2 shows, the KL-divergence between thetrue and estimated distributions are less than thatDe2En Fr2En Es2EnKL(P ?reg ?
Preg) 4.37 4.17 4.38KL(P ?reg ?
unif ) 5.37 5.21 5.80KL(P ?oov ?
Poov) 3.04 4.58 4.73KL(P ?oov ?
unif ) 3.41 4.75 4.99Table 2: For regular/OOV phrases, the KL-divergence be-tween the true distribution (P ?)
and the estimated (P ) or uni-form (unif ) distributions are shown, where:KL(P ?
?
P ) :=Px P?
(x) log P?
(x)P (x) .187100 101 102 103 104 10510?610?510?410?310?210?1100RankProbabilityRegular Phrases in UEstimated DistributionTrue Distribution100 101 102 103 104 10510?610?510?410?310?210?1100RankProbabilityOOV Phrases in UEstimated DistributionTrue DistributionFigure 5: The log-log Zipf plots representing the true andestimated probabilities of a (source) phrase vs the rank ofthat phrase in the German to English translation task.
Theplots for the Spanish to English and French to English tasksare also similar to the above plots, and confirm a power lawbehavior in the true phrasal distributions.between the true and uniform distributions, in allthree language pairs.
Since uniform distributionconveys no information, this is evidence that thereis some information encoded in the estimated dis-tribution about the true distribution.
Howeverwe noticed that the true distributions of regu-lar/OOV phrases exhibit Zipfian (power law) be-havior5 which is not well captured by the esti-mated distributions (see Fig.
5).
Enhancing the es-timated distributions to capture this power law be-havior would improve the quality of the proposedsentence selection methods.6 Related Work(Haffari et al, 2009) provides results for activelearning for MT using a single language pair.
Ourwork generalizes to the use of multilingual corporausing new methods that are not possible with a sin-gle language pair.
In this paper, we also introducenew selection methods that outperform the meth-ods in (Haffari et al, 2009) even for MT with asingle language pair.
In addition in this paper byconsidering multilingual parallel corpora we wereable to introduce co-training for AL, while (Haf-fari et al, 2009) only use self-training since theyare using a single language pair.5This observation is at the phrase level and not at the word(Zipf, 1932) or even n-gram level (Ha et al, 2002).
(Reichart et al, 2008) introduces multi-task ac-tive learning where unlabeled data require annota-tions for multiple tasks, e.g.
they consider named-entities and parse trees, and showed that multi-ple tasks helps selection compared to individualtasks.
Our setting is different in that the target lan-guage is the same across multiple MT tasks, whichwe exploit to use consensus translations and co-training to improve active learning performance.
(Callison-Burch and Osborne, 2003b; Callison-Burch and Osborne, 2003a) provide a co-trainingapproach to MT, where one language pair createsdata for another language pair.
In contrast, ourco-training approach uses consensus translationsand our setting for active learning is very differ-ent from their semi-supervised setting.
A Ph.D.proposal by Chris Callison-Burch (Callison-burch,2003) lays out the promise of AL for SMT andproposes some algorithms.
However, the lack ofexperimental results means that performance andfeasibility of those methods cannot be comparedto ours.While we use consensus translations (He et al,2008; Rosti et al, 2007; Matusov et al, 2006)as an effective method for co-training in this pa-per, unlike consensus for system combination, thesource languages for each of our MT systems aredifferent, which rules out a set of popular methodsfor obtaining consensus translations which assumetranslation for a single language pair.
Finally, webriefly note that triangulation (see (Cohn and Lap-ata, 2007)) is orthogonal to the use of co-trainingin our work, since it only enhances each MT sys-tem in our ensemble by exploiting the multilingualdata.
In future work, we plan to incorporate trian-gulation into our active learning approach.7 ConclusionThis paper introduced the novel active learningtask of adding a new language to an existing multi-lingual set of parallel text.
We construct SMT sys-tems from each language in the collection into thenew target language.
We show that we can take ad-vantage of multilingual corpora to decrease anno-tation effort thanks to the highly effective sentenceselection methods we devised for active learningin the single language-pair setting which we thenapplied to the multilingual sentence selection pro-tocols.
In the multilingual setting, a novel co-training method for active learning in SMT is pro-posed using consensus translations which outper-forms AL-SMT with self-training.188ReferencesAvrim Blum and Tom Mitchell.
1998.
Combin-ing Labeled and Unlabeled Data with Co-Training.In Proceedings of the Eleventh Annual Conferenceon Computational Learning Theory (COLT 1998),Madison, Wisconsin, USA, July 24-26.
ACM.Chris Callison-Burch and Miles Osborne.
2003a.Bootstrapping parallel corpora.
In NAACL work-shop: Building and Using Parallel Texts: DataDriven Machine Translation and Beyond.Chris Callison-Burch and Miles Osborne.
2003b.
Co-training for statistical machine translation.
In Pro-ceedings of the 6th Annual CLUK Research Collo-quium.Chris Callison-burch.
2003.
Active learning for statis-tical machine translation.
In PhD Proposal, Edin-burgh University.Trevor Cohn and Mirella Lapata.
2007.
Machinetranslation by triangulation: Making effective use ofmulti-parallel corpora.
In ACL.Le Quan Ha, E. I. Sicilia-Garcia, Ji Ming, and F.J.Smith.
2002.
Extension of zipf?s law to words andphrases.
In Proceedings of the 19th internationalconference on Computational linguistics.Gholamreza Haffari, Maxim Roy, and Anoop Sarkar.2009.
Active learning for statistical phrase-basedmachine translation.
In NAACL.Xiaodong He, Mei Yang, Jianfeng Gao, PatrickNguyen, and Robert Moore.
2008.
Indirect-hmm-based hypothesis alignment for combining outputsfrom machine translation systems.
In EMNLP.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing consensus translation from multi-ple machine translation systems using enhanced hy-potheses alignment.
In EACL.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30(4):417?449.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In ACL ?03: Pro-ceedings of the 41st Annual Meeting on Associationfor Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Weijing Zhu.
2002.
Bleu: A method for automatic eval-uation of machine translation.
In ACL ?02: Proceed-ings of the 41st Annual Meeting on Association forComputational Linguistics.Roi Reichart, Katrin Tomanek, Udo Hahn, and AriRappoport.
2008.
Multi-task active learning for lin-guistic annotations.
In ACL.Antti-Veikko Rosti, Necip Fazil Ayan, Bing Xiang,Spyros Matsoukas, Richard M. Schwartz, and Bon-nie Jean Dorr.
2007.
Combining outputs from mul-tiple machine translation systems.
In NAACL.Nicola Ueffing, Gholamreza Haffari, and AnoopSarkar.
2007.
Transductive learning for statisticalmachine translation.
In ACL.George Zipf.
1932.
Selective Studies and the Principleof Relative Frequency in Language.
Harvard Uni-versity Press.189
