Applying Spectral Clustering for Chinese Word Sense InductionZhengyan He, Yang Song, Houfeng WangKey Laboratory of Computational Linguistics (Peking University)Ministry of Education,China{hezhengyan, ysong, wanghf}@pku.edu.cnAbstractSense Induction is the process of identify-ing the word sense given its context, oftentreated as a clustering task.
This paper ex-plores the use of spectral cluster methodwhich incorporates word features and n-gram features to determine which clusterthe word belongs to, each cluster repre-sents one sense in the given document set.1 IntroductionWord Sense Induction(WSI) is defined as theprocess of identifying different senses of a tar-get word in a given context in an unsupervisedmethod.
It?s different from word sense disam-biguation(WSD) in that senses in WSD are as-sumed to be known.
The disadvantage of WSDis that it derives the senses of word from existingdictionaries or other corpus and the senses cannotbe extended to other domains.
WSI can overcomethis problem as it can automatically derive wordsenses from the given document set, or a specificdomain.Many different approaches based on co-occurence have been proposed so far.
Bordag(2006) proposes an approach that uses tripletsof co-occurences.
The most significant co-occurences of target word are used to build tripletsthat consist of the target word and its two co-occurences.
Then intersection built from the co-occurence list of each word in the triplet is usedas feature vector.
After merging similar tripletsthat have more than 80% overlapping words, clus-tering is performed on the triplets.
Triplets withfewer than 4 intersection words are removed in or-der to reduce noise.LDA model has also been applied to WSI(Brody and Lapata, 2009).
Brody proposes amethod that treats document and topics in LDAas word context and senses respectively.
The pro-cess of generating the context words is as follows:first generate sense from a multinomial distribu-tion given context, then generate context wordsgiven sense.
They also derive a layered modelto incorporate different kind of features and useGibbs sampling method to solve the problem.Graph-based methods become popular recently.These methods use the co-occurence graph ofcontext words to obtain sense clusters based onsub-graph density.
Markov clustering(MCL) hasbeen used to identify dense regions of graph(Agirre and Soroa, 2007).Spectral clustering performs well on problemsin which points cluster based on shape.
Themethod is that first compute the Laplace matrixof the affinity matrix, then reform the data pointsby stacking the largest eigenvectors of the Laplacematrix in columns, finally cluster the new datapoints using a more simple clustering method likek-means (Ng et al, 2001).2 MethodologyOur approach follows a common cluster modelthat represents the given context as a word vec-tor and later uses a spectral clustering method togroup each instance in its own cluster.Different types of polysemy may arise and themost significant distinction may be the syntacticclasses of the word and the conceptually differ-ent senses (Bordag, 2006).
Thus we must extractthe features able to distinguish these differences.They are:Local tokens: the word occuring in the window-3 ?
+3;Local bigram feature: bigram within -5 ?
+5Chinese character range;The above two features model the syntactic us-age of a specific sense of a Chinese word.Topical or conceptual feature: the contentwords (pos-tagged as noun, verb, adjective) withinthe given sentence.
As the sentence in the trainingset seems generally short, a short window may notcontains enough infomation.We represent the words in a 0-1 vector accord-ing to their existence in a given sentence.
Then thesimilarity measure between two given sentences isderived from their cosine similarity.
We find that itis difficult to define the relative importance of dif-ferent types of features in order to combine themin one vector space, and find that ignoring weightachieve better result.
Brody (2009) achieves thisin LDA model through a layered model with dif-ferent probability of feature given sense.Later we use a spectral clustering method fromR kernlab package (Karatzoglou et al, 2004)which implements the algorithm described in (Nget al, 2001).
Instead of using the Gaussian kernelmatrix as the similarity matrix we use the cosinesimilarity derived above.One observation is that instances with the sametarget word sense often appear in the same con-text.
However, for some verb in Chinese, it is of-ten the case that one sense relates to a concreteobject while the other relates to a more broad andabstract concept and the context varies consider-ably.
Simple word co-occurence cannot define agood similarity measure to group these cases intoone cluster.
We must consider semantic related-ness measures between contexts.3 PerformanceOur system performs well on the training set.
Twomethods are used to evaluate the performance un-der different features.method precision recall F-scorePurity-based 81.11 83.19 81.99B-cubed 74.41 76.51 75.33Table 1: The performance of training setOur system finally gets a F-score of 0.7598 onthe test set.4 ConclusionOur experiment in the Chinese word sense induc-tion task performs good with respect to the relativesmall corpus(only the training set).
But only con-sidering token co-occurence cannot achieve betterresult.
Moreover, it is difficult to define a simi-larity measure solely based on lexicon infomationwith no regard to semantic relatedness.
Finally,combining different types of features seems to beanother challenge in our model.5 AcknowledgmentsThis research is supported by National NaturalScience Foundation of Chinese (No.9092001).ReferencesAgirre, Eneko and Aitor Soroa.
2007.
Ubc-as: a graphbased unsupervised system for induction and classi-fication.
In SemEval ?07: Proceedings of the 4thInternational Workshop on Semantic Evaluations,pages 346?349, Morristown, NJ, USA.
Associationfor Computational Linguistics.Bordag, Stefan.
2006.
Word sense induction: Triplet-based clustering and automatic evaluation.
InEACL.
The Association for Computer Linguistics.Brody, Samuel and Mirella Lapata.
2009.
Bayesianword sense induction.
In EACL, pages 103?111.The Association for Computer Linguistics.Karatzoglou, Alexandros, Alex Smola, Kurt Hornik,and Achim Zeileis.
2004. kernlab ?
an S4 pack-age for kernel methods in R. Journal of StatisticalSoftware, 11(9):1?20.Ng, Andrew Y., Michael I. Jordan, and Yair Weiss.2001.
On spectral clustering: Analysis and an al-gorithm.
In Advances in Neural Information Pro-cessing Systems 14, pages 849?856.
MIT Press.
