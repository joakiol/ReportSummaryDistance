Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 952?959,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsPipeline IterationKristy Hollingshead and Brian RoarkCenter for Spoken Language Understanding, OGI School of Science & EngineeringOregon Health & Science University, Beaverton, Oregon, 97006 USA{hollingk,roark}@cslu.ogi.eduAbstractThis paper presents pipeline iteration, an ap-proach that uses output from later stagesof a pipeline to constrain earlier stages ofthe same pipeline.
We demonstrate sig-nificant improvements in a state-of-the-artPCFG parsing pipeline using base-phraseconstraints, derived either from later stagesof the parsing pipeline or from a finite-state shallow parser.
The best performanceis achieved by reranking the union of un-constrained parses and relatively heavily-constrained parses.1 IntroductionA ?pipeline?
system consists of a sequence of pro-cessing stages such that the output from one stageprovides the input to the next.
Each stage in such apipeline identifies a subset of the possible solutions,and later stages are constrained to find solutionswithin that subset.
For example, a part-of-speechtagger could constrain a ?base phrase?
chunker (Rat-naparkhi, 1999), or the n-best output of a parsercould constrain a reranker (Charniak and Johnson,2005).
A pipeline is typically used to reduce searchcomplexity for rich models used in later stages, usu-ally at the risk that the best solutions may be prunedin early stages.Pipeline systems are ubiquitous in natural lan-guage processing, used not only in parsing (Rat-naparkhi, 1999; Charniak, 2000), but also machinetranslation (Och and Ney, 2003) and speech recogni-tion (Fiscus, 1997; Goel et al, 2000), among others.Despite the widespread use of pipelines, they havebeen understudied, with very little work on gen-eral techniques for designing and improving pipelinesystems (although cf.
Finkel et al (2006)).
This pa-per presents one such general technique, here ap-plied to stochastic parsing, whereby output fromlater stages of a pipeline is used to constrain earlierstages of the same pipeline.
To our knowledge, thisis the first time such a pipeline architecture has beenproposed.It may seem surprising that later stages of apipeline, already constrained to be consistent withthe output of earlier stages, can profitably informthe earlier stages in a second pass.
However, thericher models used in later stages of a pipeline pro-vide a better distribution over the subset of possiblesolutions produced by the early stages, effectivelyresolving some of the ambiguities that account formuch of the original variation.
If an earlier stage isthen constrained in a second pass not to vary with re-spect to these resolved ambiguities, it will be forcedto find other variations, which may include better so-lutions than were originally provided.To give a rough illustration, consider the Venn di-agram in Fig.
1(i).
Set A represents the original sub-set of possible solutions passed along by the earlierstage, and the dark shaded region represents high-probability solutions according to later stages.
Ifsome constraints are then extracted from these high-probability solutions, defining a subset of solutions(S) that rule out some of A, the early stage will beforced to produce a different set (B).
Constraintsderived from later stages of the pipeline focus thesearch in an area believed to contain high-qualitycandidates.Another scenario is to use a different model al-together to constrain the pipeline.
In this scenario,(i) (ii)ABSABSFigure 1: Two Venn diagrams, representing (i) constraintsderived from later stages of an iterated pipelined system; and(ii) constraints derived from a different model.952represented in Fig.
1(ii), the other model constrainsthe early stage to be consistent with some subset ofsolutions (S), which may be largely or completelydisjoint from the original set A.
Again, a different set(B) results, which may include better results than A.Whereas when iterating we are guaranteed that thenew subset S will overlap at least partially with theoriginal subset A, that is not the case when makinguse of constraints from a separately trained model.In this paper, we investigate pipeline iterationwithin the context of the Charniak and Johnson(2005) parsing pipeline, by constraining parses tobe consistent with a base-phrase tree.
We derivethese base-phrase constraints from three sources: thereranking stage of the parsing pipeline; a finite-stateshallow parser (Hollingshead et al, 2005); and acombination of the output from these two sources.We compare the relative performance of these threesources and find the best performance improvementsusing constraints derived from a weighted combina-tion of shallow parser output and reranker output.The Charniak parsing pipeline has been exten-sively studied over the past decade, with a num-ber of papers focused on improving early stages ofthe pipeline (Charniak et al, 1998; Caraballo andCharniak, 1998; Blaheta and Charniak, 1999; Halland Johnson, 2004; Charniak et al, 2006) as wellas many focused on optimizing final parse accuracy(Charniak, 2000; Charniak and Johnson, 2005; Mc-Closky et al, 2006).
This focus on optimization hasmade system improvements very difficult to achieve;yet our relatively simple architecture yields statisti-cally significant improvements, making pipeline it-eration a promising approach for other tasks.2 ApproachOur approach uses the Charniak state-of-the-artparsing pipeline.
The well-known Charniak (2000)coarse-to-fine parser is a two-stage parsing pipeline,in which the first stage uses a vanilla PCFG to pop-ulate a chart of parse constituents.
The secondstage, constrained to only those items in the first-stage chart, uses a refined grammar to generate ann-best list of parse candidates.
Charniak and John-son (2005) extended this pipeline with a discrimina-tive maximum entropy model to rerank the n-bestparse candidates, deriving a significant benefit fromthe richer model employed by the reranker.For our experiments, we modified the parser1 to1ftp://ftp.cs.brown.edu/pub/nlparser/Base ShallowParser Phrases PhrasesCharniak parser-best 91.9 94.4reranker-best 92.8 94.8Finite-state shallow parser 91.7 94.3Table 1: F-scores on WSJ section 24 of output from twoparsers on the similar tasks of base-phrase parsing and shallow-phrase parsing.
For evaluation, base and shallow phrases areextracted from the Charniak/Johnson full-parse output.allow us to optionally provide base-phrase trees toconstrain the first stage of parsing.2.1 Base PhrasesFollowing Ratnaparkhi (1999), we define a basephrase as any parse node with only preterminal chil-dren.
Unlike the shallow phrases defined for theCoNLL-2000 Shared Task (Tjong Kim Sang andBuchholz, 2000), base phrases correspond directlyto constituents that appear in full parses, and hencecan provide a straightforward constraint on edgeswithin a chart parser.
In contrast, shallow phrasescollapse certain non-constituents?such as auxiliarychains?into a single phrase, and hence are not di-rectly applicable as constraints on a chart parser.We have two methods for deriving base-phraseannotations for a string.
First, we trained a finite-state shallow parser on base phrases extracted fromthe Penn Wall St. Journal (WSJ) Treebank (Marcuset al, 1993).
The treebank trees are pre-processedidentically to the procedure for training the Charniakparser, e.g., empty nodes and function tags are re-moved.
The shallow parser is trained using the per-ceptron algorithm, with a feature set nearly identicalto that from Sha and Pereira (2003), and achievescomparable performance to that paper.
See Holling-shead et al (2005) for more details.
Second, basephrases can be extracted from the full-parse outputof the Charniak and Johnson (2005) reranker, via asimple script to extract nodes with only preterminalchildren.Table 1 shows these systems?
bracketing accu-racy on both the base-phrase and shallow parsingtasks for WSJ section 24; each system was trainedon WSJ sections 02-21.
From this table we cansee that base phrases are substantially more difficultthan shallow phrases to annotate.
Output from thefinite-state shallow parser is roughly as accurate asoutput extracted from the Charniak parser-best trees,though a fair amount below output extracted fromthe reranker-best trees.In addition to using base phrase constraints fromthese two sources independently, we also looked at953combining the predictions of both to obtain more re-liable constraints.
We next present a method of com-bining output from multiple parsers based on com-bined precision and recall optimization.2.2 Combining Parser n-best ListsIn order to select high-likelihood constraints for thepipeline, we may want to extract annotations withhigh levels of agreement (?consensus hypotheses?
)between candidates.
In addition, we may want tofavor precision over recall to avoid erroneous con-straints within the pipeline as much as possible.Here we discuss how a technique presented in Good-man?s thesis (1998) can be applied to do this.We will first present this within a general chartparsing approach, then move to how we use it for n-best lists.
Let T be the set of trees for a particularinput, and let a parse T ?
T be considered as a setof labeled spans.
Then, for all labeled spans X ?
T ,we can calculate the posterior probability ?
(X) asfollows:?
(X) =?T?TP(T )JX ?
T K?T ?
?T P(T ?
)(1)where JX ?
T K ={1 if X ?
T0 otherwise.Goodman (1996; 1998) presents a method for us-ing the posterior probability of constituents to maxi-mize the expected labeled recall of binary branchingtrees, as follows:T?
= argmaxT?T?X?T?
(X) (2)Essentially, find the tree with the maximum sum ofthe posterior probabilities of its constituents.
Thisis done by computing the posterior probabilitiesof constituents in a chart, typically via the Inside-Outside algorithm (Baker, 1979; Lari and Young,1990), followed by a final CYK-like pass to find thetree maximizing the sum.For non-binary branching trees, where precisionand recall may differ, Goodman (1998, Ch.3) pro-poses the following combined metric for balancingprecision and recall:T?
= argmaxT?T?X?T(?(X)?
?)
(3)where ?
ranges from 0 to 1.
Setting ?=0 is equiv-alent to Eq.
2 and thus optimizes recall, and setting?=1 optimizes precision; Appendix 5 at the end ofthis paper presents brief derivations of these met-rics.2 Thus, ?
functions as a mixing factor to balancerecall and precision.This approach also gives us a straightforward wayto combine n-best outputs of multiple systems.
Todo this, we construct a chart of the constituents in thetrees from the n-best lists, and allow any combina-tion of constituents that results in a tree ?
even onewith no internal structure.
In such a way, we canproduce trees that only include a small number ofhigh-certainty constituents, and leave the remainderof the string unconstrained, even if such trees werenot candidates in the original n-best lists.For simplicity, we will here discuss the combina-tion of two n-best lists, though it generalizes in theobvious way to an arbitrary number of lists.
Let Tbe the union of the two n-best lists.
For all treesT ?
T , let P1(T ) be the probability of T in the firstn-best list, andP2(T ) the probability of T in the sec-ond n-best list.
Then, we define P(T ) as follows:P(T ) = ?P1(T )?T ?
?T P1(T ?
)+P2(T )?T ?
?T P2(T ?
)(4)where the parameter ?
dictates the relative weight ofP1 versus P2 in the combination.3For this paper, we combined two n-best lists ofbase-phrase trees.
Although there is no hierarchi-cal structure in base-phrase annotations, they can berepresented as flat trees, as shown in Fig.
2(a).
Weconstructed a chart from the two lists being com-bined, using Eq.
4 to define P(T ) in Eq.
1.
We wishto consider every possible combination of the basephrases, so for the final CYK-like pass to find theargmax tree, we included rules for attaching eachpreterminal directly to the root of the tree, in addi-tion to rules permitting any combination of hypoth-esized base phrases.Consider the trees in Fig.
2.
Figure 2(a) is ashallow parse with three NP base phrases; Figure2(b) is the same parse where the ROOT produc-tion has been binarized for the final CYK-like pass,which requires binary productions.
If we includeproductions of the form ?ROOT ?
X ROOT?
and?ROOT ?
X Y?
for all non-terminals X and Y (in-cluding POS tags), then any tree-structured com-bination of base phrases hypothesized in either n-2Our notation differs slightly from that in Goodman (1998),though the approaches are formally equivalent.3Note that P1 and P2 are normalized in eq.
4, and thus arenot required to be true probabilities.
In turn, P is normalizedwhen used in eq.
1, such that the posterior probability ?
is atrue probability.
Hence P need not be normalized in eq.
4.954(a)ROOT  @@PPPPPPPNP HHDTtheNNbrokerVBDsoldNP HHDTtheNNSstocksNPNNyesterday(b)ROOTHHHNP HHDTtheNNbrokerROOT HHVBDsoldROOT HHNP HHDTtheNNSstocksNPNNyesterday(c)SHHHHNP HHDTtheNNbrokerVPHHHHVBDsoldNP HHDTtheNNSstocksNPNNyesterdayFigure 2: Base-phrase trees (a) as produced for an n-best list and (b) after root-binarization for n-best list combination.
Full-parsetree (c) consistent with constraining base-phrase tree (a).87 88 89 90 91 92 93 94 95 96 978687888990919293949596precisionrecallCharniak ?
reranked (solid viterbi)Finite?state shallow parser (solid viterbi)Charniak reranked + Finite?stateFigure 3: The tradeoff between recall and precision using arange of ?
values (Eq.
3) to select high-probability annotationsfrom an n-best list.
Results are shown on 50-best lists of base-phrase parses from two parsers, and on the combination of thetwo lists.best list is allowed, including the one with no basephrases at all.
Note that, for the purpose of findingthe argmax tree in Eq.
3, we only sum the posteriorprobabilities of base-phrase constituents, and not theROOT symbol or POS tags.Figure 3 shows the results of performing this com-bined precision/recall optimization on three separaten-best lists: the 50-best list of base-phrase trees ex-tracted from the full-parse output of the Charniakand Johnson (2005) reranker; the 50-best list outputby the Hollingshead et al (2005) finite-state shallowparser; and the weighted combination of the two listsat various values of ?
in Eq.
3.
For the combination,we set ?=2 in Eq.
4, with the Charniak and Johnson(2005) reranker providing P1, effectively giving thereranker twice the weight of the shallow parser indetermining the posteriors.
The shallow parser hasperceptron scores as weights, and the distribution ofthese scores after a softmax normalization was toopeaked to be of utility, so we used the normalizedreciprocal rank of each candidate as P2 in Eq.
4.We point out several details in these results.First, using this method does not result in an F-measure improvement over the Viterbi-best base-phrase parses (shown as solid symbols in the graph)for either the reranker or shallow parser.
Also, us-ing this model effects a greater improvement in pre-cision than in recall, which is unsurprising withthese non-hierarchical annotations; unlike full pars-ing (where long sequences of unary productions canimprove recall arbitrarily), in base-phrase parsing,any given span can have only one non-terminal.
Fi-nally, we see that the combination of the two n-bestlists improves over either list in isolation.3 Experimental SetupFor our experiments we constructed a simple parsingpipeline, shown in Fig.
4.
At the core of the pipelineis the Charniak and Johnson (2005) coarse-to-fineparser and MaxEnt reranker, described in Sec.
2.The parser constitutes the first and second stages ofour pipeline, and the reranker the final stage.
Fol-lowing Charniak and Johnson (2005), we set theparser to output 50-best parses for all experimentsdescribed here.
We constrain only the first stage ofthe parser: during chart construction, we disallowany constituents that conflict with the constraints, asdescribed in detail in the next section.3.1 Parser ConstraintsWe use base phrases, as defined in Sec.
2.1, to con-strain the first stage of our parsing pipeline.
Underthese constraints, full parses must be consistent withthe base-phrase tree provided as input to the parser,i.e., any valid parse must contain all of the base-phrase constituents in the constraining tree.
Thefull-parse tree in Fig.
2(c), for example, is consis-tent with the base-phrase tree in Fig.
2(a).Implementing these constraints in a parser isstraightforward, one of the advantages of using basephrases as constraints.
Since the internal structureof base phrases is, by definition, limited to preter-minal children, we can constrain the entire parse byconstraining the parents of the appropriate pretermi-nal nodes.
For any preterminal that occurs withinthe span of a constraining base phrase, the onlyvalid parent is a node matching both the span (startand end points) and the label of the provided base955A3ShallowParserCoarseParserFineParserRerankerDCBextractedbase phrasesA1A2+Figure 4: The iterated parsing pipeline.
In the first iteration,the coarse parser may be either unconstrained, or constrainedby base phrases from the shallow parser (A1).
In the seconditeration, base phrase constraints may be extracted either fromreranker output (A2) or from a weighted combination of shal-low parser output and reranker output (A3).
Multiple sets ofn-best parses, as output by the coarse-to-fine parser under dif-ferent constraint conditions, may be joined in a set union (C).phrase.
All other proposed parent-nodes are re-jected.
In such a way, for any parse to cover theentire string, it would have to be consistent with theconstraining base-phrase tree.Words that fall outside of any base-phrase con-straint are unconstrained in how they attach withinthe parse; hence, a base-phrase tree with few wordscovered by base-phrase constraints will result in alarger search space than one with many words cov-ered by base phrases.
We also put no restrictions onthe preterminal labels, even within the base phrases.We normalized for punctuation.
If the parser fails tofind a valid parse with the constraints, then we liftthe constraints and allow any parse constituent orig-inally proposed by the first-stage parser.3.2 Experimental ConditionsOur experiments will demonstrate the effects of con-straining the Charniak parser under several differ-ent conditions.
The baseline system places no con-straints on the parser.
The remaining experimen-tal conditions each consider one of three possiblesources of the base phrase constraints: (1) the basephrases output by the finite-state shallow parser;(2) the base phrases extracted from output of thereranker; and (3) a combination of the output fromthe shallow parser and the reranker, which is pro-duced using the techniques outlined in Sec.
2.2.Constraints are enforced as described in Sec.
3.1.Unconstrained For our baseline system, werun the Charniak and Johnson (2005) parser andreranker with default parameters.
The parser is pro-vided with treebank-tokenized text and, as men-tioned previously, outputs 50-best parse candidatesto the reranker.FS-constrained The FS-constrained conditionprovides a comparison point of non-iterated con-straints.
Under this condition, the one-best base-System LR LP FFinite-state shallow parser 91.3 92.0 91.7Charniak reranker-best 92.2 93.3 92.8Combination (?=0.5) 92.2 94.1 93.2Combination (?=0.9) 81.0 97.4 88.4Table 2: Labeled recall (LR), precision (LP), and F-scoreson WSJ section 24 of base-phrase trees produced by the threepossible sources of constraints.phrase tree output by the finite-state shallow parseris input as a constraint to the Charniak parser.
Werun the parser and reranker as before, under con-straints from the shallow parser.
The accuracy ofthe constraints used under this condition is shown inthe first row of Table 2.
Note that this condition isnot an instance of pipeline iteration, but is includedto show the performance levels that can be achievedwithout iteration.Reranker-constrained We will use thereranker-constrained condition to examine the ef-fects of pipeline iteration, with no input from othermodels outside the pipeline.
We take the reranker-best full-parse output under the condition of uncon-strained search, and extract the corresponding base-phrase tree.
We run the parser and reranker as be-fore, now with constraints from the reranker.
Theaccuracy of the constraints used under this conditionis shown in the second row of Table 2.Combo-constrained The combo-constrainedconditions are designed to compare the effects ofgenerating constraints with different combinationparameterizations, i.e., different ?
parameters in Eq.3.
For this experimental condition, we extract base-phrase trees from the n-best full-parse trees outputby the reranker.
We combine this list with the n-bestlist output by the finite-state shallow parser, exactlyas described in Sec.
2.2, again with the reranker pro-viding P1 and ?=2 in Eq.
4.
We examined a rangeof operating points from ?=0.4 to ?=0.9, and re-port two points here (?=0.5 and ?=0.9), which rep-resent the highest overall accuracy and the highestprecision, respectively, as shown in Table 2.Constrained and Unconstrained Union Wheniterating this pipeline, the original n-best list of fullparses output from the unconstrained parser is avail-able at no additional cost, and our final set of ex-perimental conditions investigate taking the unionof constrained and unconstrained n-best lists.
Theimposed constraints can result in candidate sets thatare largely (or completely) disjoint from the uncon-strained sets, and it may be that the unconstrainedset is in many cases superior to the constrained set.956Constraints Parser-best Reranker-best Oracle-best # CandidatesBaseline (Unconstrained, 50-best) 88.92 90.24 95.95 47.9FS-constrained 88.44 89.50 94.10 46.2Reranker-constrained 89.60 90.46 95.07 46.9Combo-constrained (?=0.5) 89.81 90.74 95.41 46.3Combo-constrained (?=0.9) 89.34 90.43 95.91 47.5Table 3: Full-parse F-scores on WSJ section 24.
The unconstrained search (first row) provides a baseline comparison for theeffects of constraining the search space.
The last four rows demonstrate the effect of various constraint conditions.Even our high-precision constraints did not reach100% precision, attesting to the fact that there wassome error in all constrained conditions.
By con-structing the union of the two n-best lists, we cantake advantage of the new constrained candidate setwithout running the risk that the constraints have re-sulted in a worse n-best list.
Note that the parserprobabilities are produced from the same model inboth passes, and are hence directly comparable.The output of the second pass of the pipelinecould be used to constrain a third pass, for multiplepipeline iterations.
However, we found that furtheriterations provided no additional improvements.3.3 DataUnless stated otherwise, all reported results will beF-scores on WSJ section 24 of the Penn WSJ Tree-bank, which was our development set.
Training datawas WSJ sections 02-21, with section 00 as held-out data.
Crossfold validation (20-fold with 2,000sentences per fold) was used to train the rerankerfor every condition.
Evaluation was performed us-ing evalb under standard parameterizations.
WSJsection 23 was used only for final testing.4 Results & DiscussionWe evaluate the one-best parse candidates beforeand after reranking (parser-best and reranker-best,respectively).
We additionally provide the best-possible F-score in the n-best list (oracle-best) andthe number of unique candidates in the list.Table 3 presents trials showing the effect of con-straining the parser under various conditions.
Con-straining the parser to the base phrases producedby the finite-state shallow parser (FS-constrained)hurts performance by half a point.
Constraining theparser to the base phrases produced by the reranker,however, provides a 0.7 percent improvement in theparser-best accuracy, and a 0.2 percent improvementafter reranking.
Combining the two base-phrase n-best lists to derive the constraints provides furtherimprovements when ?=0.5, to a total improvementof 0.9 and 0.5 percent over parser-best and reranker-best accuracy, respectively.
Performance degradesat ?=0.9 relative to ?=0.5, indicating that, even ata lower precision, more constraints are beneficial.The oracle rate decreases under all of the con-strained conditions as compared to the baseline,demonstrating that the parser was prevented fromfinding some of the best solutions that were orig-inally found.
However, the improvement in F-score shows that the constraints assisted the parserin achieving high-quality solutions despite this de-graded oracle accuracy of the lists.Table 4 shows the results when taking the unionof the constrained and unconstrained lists prior toreranking.
Several interesting points can be notedin this table.
First, despite the fact that the FS-constrained condition hurts performance in Table3, the union provides a 0.5 percent improvementover the baseline in the parser-best performance.This indicates that, in some cases, the Charniakparser is scoring parses in the constrained set higherthan in the unconstrained set, which is evidence ofsearch errors in the unconstrained condition.
Onecan see from the number of candidates that the FS-constrained condition provides the set of candidatesmost disjoint from the original unconstrained parser,leading to the largest number of candidates in theunion.
Surprisingly, even though this set providedthe highest parser-best F-score of all of the unionsets, it did not lead to significant overall improve-ments after reranking.In all other conditions, taking the union de-creases the parser-best accuracy when compared tothe corresponding constrained output, but improvesthe reranker-best accuracy in all but the combo-constrained ?=0.9 condition.
One explanation forthe lower performance at ?=0.9 versus ?=0.5 isseen in the number of candidates, about 7.5 fewerthan in the ?=0.5 condition.
There are fewer con-straints in the high-precision condition, so the re-sulting n-best lists do not diverge as much from theoriginal lists, leading to less diversity in their union.The gains in performance should not be attributedto increasing the number of candidates nor to allow-957Constraints Parser-best Reranker-best Oracle-best # CandidatesBaseline (Unconstrained, 50-best) 88.92 90.24 95.95 47.9Unconstrained ?
FS-constrained 89.39 90.27 96.61 74.9Unconstrained ?
Reranker-constrained 89.23 90.59 96.48 70.3Unconstrained ?
Combo (?=0.5) 89.28 90.78 96.53 69.7Unconstrained ?
Combo (?=0.9) 89.03 90.44 96.40 62.1Unconstrained (100-best) 88.82 90.13 96.38 95.2Unconstrained (50-best, beam?2) 89.01 90.45 96.13 48.1Table 4: Full-parse F-scores on WSJ section 24 after taking the set union of unconstrained and constrained parser output underthe 4 different constraint conditions.
Also, F-score for 100-best parses, and 50-best parses with an increased beam threshold, outputby the Charniak parser under the unconstrained condition.Constraints F-scoreBaseline (Unconstrained, 50-best) 91.06Unconstrained ?
Combo (?=0.5) 91.48Table 5: Full-parse F-scores on WSJ section 23 for our best-performing system on WSJ section 24.
The 0.4 percent F-scoreimprovement is significant at p < 0.001.ing the parser more time to generate the parses.
Thepenultimate row in Table 4 shows the results with100-best lists output in the unconstrained condition,which does not improve upon the 50-best perfor-mance, despite an improved oracle F-score.
Sincethe second iteration through the parsing pipelineclearly increases the overall processing time by afactor of two, we also compare against output ob-tained by doubling the coarse-parser?s beam thresh-old.
The last row in Table 4 shows that the increasedthreshold yields an insignificant improvement overthe baseline, despite a very large processing burden.We applied our best-performing model (Uncon-strained ?
Combo, ?=0.5) to the test set, WSJ sec-tion 23, for comparison against the baseline system.Table 5 shows a 0.4 percent F-score improvementover the baseline for that section, which is statisti-cally significant at p < 0.001, using the stratifiedshuffling test (Yeh, 2000).5 Conclusion & Future WorkIn summary, we have demonstrated that pipeline it-eration can be useful in improving system perfor-mance, by constraining early stages of the pipelinewith output derived from later stages.
While thecurrent work made use of a particular kind ofconstraint?base phrases?many others could be ex-tracted as well.
Preliminary results extending thework presented in this paper show parser accuracyimprovements from pipeline iteration when usingconstraints based on an unlabeled partial bracketingof the string.
Higher-level phrase segmentations orfully specified trees over portions of the string mightalso prove to be effective constraints.
The tech-niques shown here are by no means limited to pars-ing pipelines, and could easily be applied to othertasks making use of pipeline architectures.AcknowledgmentsThanks to Martin Jansche for useful discussions ontopics related to this paper.
The first author of thispaper was supported under an NSF Graduate Re-search Fellowship.
In addition, this research wassupported in part by NSF Grant #IIS-0447214.
Anyopinions, findings, conclusions or recommendationsexpressed in this publication are those of the authorsand do not necessarily reflect the views of the NSF.ReferencesJ.K.
Baker.
1979.
Trainable grammars for speech recognition.In Speech Communication papers for the 97th Meeting of theAcoustical Society of America.D.
Blaheta and E. Charniak.
1999.
Automatic compensationfor parser figure-of-merit flaws.
In Proceedings of the 37thAnnual Meeting of ACL, pages 513?518.S.
Caraballo and E. Charniak.
1998.
New figures of meritfor best-first probabilistic chart parsing.
Computational Lin-guistics, 24(2):275?298.E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best pars-ing and MaxEnt discriminative reranking.
In Proceedings ofthe 43rd Annual Meeting of ACL, pages 173?180.E.
Charniak, S. Goldwater, and M. Johnson.
1998.
Edge-basedbest-first chart parsing.
In Proceedings of the 6th Workshopfor Very Large Corpora, pages 127?133.E.
Charniak, M. Johnson, M. Elsner, J.L.
Austerweil, D. Ellis,S.R.
Iyangar, J. Moore, M.T.
Pozar, C. Hill, T.Q.
Vu, andI.
Haxton.
2006.
Multi-level course-to-fine PCFG parsing.In Proceedings of the HLT-NAACL Annual Meeting, pages168?175.E.
Charniak.
2000.
A Maximum-Entropy-inspired parser.
InProceedings of the 1st Annual Meeting of NAACL and 6thConference on ANLP, pages 132?139.J.R.
Finkel, C.D.
Manning, and A.Y.
Ng.
2006.
Solving theproblem of cascading errors: Approximate Bayesian infer-ence for linguistic annotation pipelines.
In Proceedings ofEMNLP, pages 618?626.J.
Fiscus.
1997.
A post-processing system to yield reducedword error rates: Recognizer output voting error reduction(ROVER).
In Proceedings of the IEEE Workshop on Auto-matic Speech Recognition and Understanding.V.
Goel, S. Kumar, and W. Byrne.
2000.
Segmental minimumBayes-risk ASR voting strategies.
In Proceedings of ICSLP,pages 139?142.958J.
Goodman.
1996.
Parsing algorithms and metrics.
In Pro-ceedings of the 34th Annual Meeting of ACL, pages 177?183.J.
Goodman.
1998.
Parsing inside-out.
Ph.D. thesis, HarvardUniversity.K.
Hall and M. Johnson.
2004.
Attention shifting for parsingspeech.
In Proceedings of the 42nd Annual Meeting of ACL,pages 40?46.K.
Hollingshead, S. Fisher, and B. Roark.
2005.
Comparingand combining finite-state and context-free parsers.
In Pro-ceedings of HLT-EMNLP, pages 787?794.K.
Lari and S.J.
Young.
1990.
The estimation of stochasticcontext-free grammars using the inside-outside algorithm.Computer Speech and Language, 4(1):35?56.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.
1993.Building a large annotated corpus of English: The Penn tree-bank.
Computational Linguistics, 19:314?330.D.
McClosky, E. Charniak, and M. Johnson.
2006.
Rerankingand self-training for parser adaptation.
In Proceedings ofCOLING-ACL, pages 337?344.F.J.
Och and H. Ney.
2003.
A systematic comparison of variousstatistical alignment models.
Computational Linguistics, 29.A.
Ratnaparkhi.
1999.
Learning to parse natural language withmaximum entropy models.
Machine Learning, 34(1-3):151?175.F.
Sha and F. Pereira.
2003.
Shallow parsing with conditionalrandom fields.
In Proceedings of the HLT-NAACL AnnualMeeting, pages 134?141.E.F.
Tjong Kim Sang and S. Buchholz.
2000.
Introduction tothe CoNLL-2000 shared task: Chunking.
In Proceedings ofCoNLL, pages 127?132.A.
Yeh.
2000.
More accurate tests for the statistical signifi-cance of result differences.
In Proceedings of the 18th Inter-national COLING, pages 947?953.Appendix A Combined Precision/RecallDecodingRecall that T is the set of trees for a particular input,and each T ?
T is considered as a set of labeledspans.
For all labeled spans X ?
T , we can calcu-late the posterior probability ?
(X) as follows:?
(X) =?T?TP(T )JX ?
T K?T ?
?T P(T ?
)where JX ?
T K ={1 if X ?
T0 otherwise.If ?
is the reference tree, the labeled precision(LP) and labeled recall (LR) of a T relative to ?
aredefined asLP =|T ?
?
||T |LR =|T ?
?
||?
|where |T | denotes the size of the set T .A metric very close to LR is |T ?
?
|, the numberof nodes in common between the tree and the ref-erence tree.
To maximize the expected value (E) ofthis metric, we want to find the tree T?
as follows:T?
= argmaxT?TE[|T??
|]= argmaxT?T?T ?
?TP(T ?
)[|T?T ?|]?T ??
?T P(T??
)= argmaxT?T?T ?
?TP(T ?
)?X?T JX ?
T?K?T ??
?T P(T??
)= argmaxT?T?X?T?T ?
?TP(T ?
)JX ?
T ?K?T ??
?T P(T??
)= argmaxT?T?X?T?
(X) (5)This exactly maximizes the expected LR in thecase of binary branching trees, and is closely re-lated to LR for non-binary branching trees.
Simi-lar to maximizing the expected number of match-ing nodes, we can minimize the expected number ofnon-matching nodes, for a metric related to LP:T?
= argminT?TE[|T | ?
|T??
|]= argmaxT?TE[|T??
| ?
|T |]= argmaxT?T?T ?
?TP(T ?
)[|T?T ?| ?
|T |]?T ??
?T P(T??
)= argmaxT?T?T ?
?TP(T ?
)?X?T (JX ?
T?K ?
1)?T ??
?T P(T??
)= argmaxT?T?X?T?T ?
?TP(T ?
)(JX ?
T ?K ?
1)?T ??
?T P(T??
)= argmaxT?T?X?T(?(X)?
1) (6)Finally, we can combine these two metrics in alinear combination.
Let ?
be a mixing factor from 0to 1.
Then we can optimize the weighted sum:T?
= argmaxT?TE[(1?
?)|T??
|+ ?(|T??
| ?
|T |)]= argmaxT?T(1?
?)E[|T??
|]+ ?E[|T??
| ?
|T |]= argmaxT?T[(1?
?)?X?T?(X)]+[??X?T(?(X)?
1)]= argmaxT?T?X?T(?(X)?
?)
(7)The result is a combined metric for balancing preci-sion and recall.
Note that, if ?=0, Eq.
7 is the sameas Eq.
5 and thus maximizes the LR metric; and if?=1, Eq.
7 is the same as Eq.
6 and thus maximizesthe LP metric.959
