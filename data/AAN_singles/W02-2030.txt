Feature Selection for a Rich HPSG Grammar Using Decision TreesKristina Toutanova and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305-9040, USAAbstractThis paper examines feature selection for log linearmodels over rich constraint-based grammar (HPSG)representations by building decision trees over fea-tures in corresponding probabilistic context freegrammars (PCFGs).
We show that single decisiontrees do not make optimal use of the available in-formation; constructed ensembles of decision treesbased on different feature subspaces show signifi-cant performance gains (14% parse selection errorreduction).
We compare the performance of thelearned PCFG grammars and log linear models overthe same features.1 IntroductionHand-built NLP grammars frequently have a depthof linguistic representation and constraints notpresent in current treebanks, giving them poten-tial importance for tasks requiring deeper process-ing.
On the other hand, these manually built gram-mars need to solve the disambiguation problem tobe practically usable.This paper presents work on the problem of prob-abilistic parse selection from among a set of al-ternatives licensed by a hand-built grammar in thecontext of the newly developed Redwoods HPSGtreebank (Oepen et al, 2002).
HPSG (Head-drivenPhrase Structure Grammar) is a modern constraint-based lexicalist (unification) grammar, described inPollard and Sag (1994).The Redwoods treebank makes available syntac-tic and semantic analyses of much greater depththan, for example, the Penn Treebank.
Thereforethere are a large number of features available thatcould be used by stochastic models for disambigua-tion.
Other researchers have worked on extractingfeatures useful for disambiguation from unificationgrammar analyses and have built log linear mod-els a.k.a.
Stochastic Unification Based Grammars(Johnson et al, 1999; Riezler et al, 2000).
Herewe also use log linear models to estimate condi-tional probabilities of sentence analyses.
Since fea-ture selection is almost prohibitive for these mod-els, because of high computational costs, we usePCFG models to select features for log linear mod-els.
Even though this method may be expected to besuboptimal, it proves to be useful.
We select fea-tures for PCFGs using decision trees and use thesame features in a conditional log linear model.
Wecompare the performance of the two models usingequivalent features.Our PCFG models are comparable to branchingprocess models for parsing the Penn Treebank, inwhich the next state of the model depends on a his-tory of features.
In most recent parsing work the his-tory consists of a small number of manually selectedfeatures (Charniak, 1997; Collins, 1997).
Otherresearchers have proposed automatically selectingthe conditioning information for various states ofthe model, thus potentially increasing greatly thespace of possible features and selectively choosingthe best predictors for each situation.
Decision treeshave been applied for feature selection for statisticalparsing models by Magerman (1995) and Haruno etal.
(1998).
Another example of automatic featureselection for parsing is in the context of a determin-istic parsing model that chooses parse actions basedon automatically induced decision structures overa very rich feature set (Hermjakob and Mooney,1997).Our experiments in feature selection using deci-sion trees suggest that single decision trees may notbe able to make optimal use of a large number of rel-evant features.
This may be due to the greedy searchprocedures or to the fact that trees combine informa-tion from different features only through partition-ing of the space.
For example they have difficulty inweighing evidence from different features withoutfully partitioning the space.A common approach to overcoming some of theproblems with decision trees ?
such as reducingtheir variance or increasing their representationalpower ?
has been building ensembles of decisiontrees by, for example, bagging (Breiman, 1996) orboosting (Freund and Schapire, 1996).
Haruno etal.
(1998) have experimented with boosting deci-sion trees, reporting significant gains.
Our approachis to build separate decision trees using different (al-though not disjoint) subsets of the feature space andthen to combine their estimates by using the aver-age of their predictions.
A similar method basedon random feature subspaces has been proposed byHo (1998), who found that the random feature sub-space method outperformed bagging and boostingfor datasets with a large number of relevant featureswhere there is redundancy in the features.
Other ex-amples of ensemble combination based on differentfeature subspaces include Zheng (1998) who learnscombinations of Naive Bayes classifiers and Zenobiand Cunningham (2001) who create ensembles ofkNN classifiers.We begin by describing the information our HPSGcorpus makes available and the subset we have at-tempted to use in our models.
Next we describe ourensembles of decision trees for learning parameter-izations of branching process models.
Finally, wereport parse disambiguation results for these modelsand corresponding conditional log linear models.2 Characteristics of the Treebank andFeatures UsedThe Redwoods treebank (Oepen et al, 2002) isan under-construction treebank of sentences corre-sponding to a particular HPSG grammar, the LinGOERG (Flickinger, 2000).
The current preliminaryversion contains 10,000 sentences of spoken di-alog material drawn from the Verbmobil project.The Redwoods treebank makes available the en-tire HPSG signs for sentence analyses, but we haveused in our experiments only small subsets of thisrepresentation.
These are (i) derivation trees com-posed of identifiers of lexical items and construc-tions used to build the analysis, and (ii) semanticdependency trees which encode semantic head-to-head relations.
The Redwoods treebank providesdeeper semantics expressed in the Minimum Recur-sion Semantics formalism (Copestake et al, 2001),but in the present experiments we have not exploredthis fully.The nodes in the derivation trees represent com-bining rule schemas of the HPSG grammar, and notIMPERHCOMPHCOMPBSE_VERB_INFLLET_V1letUSusBSE_VERB_INFLSEE_V3seeFigure 1: Derivation tree for the sentence Let us seephrasal categories of the standard sort.
The wholeHPSG analyses can be recreated from the deriva-tion trees, using the grammar.
The preterminals ofthe derivation trees are lexical labels.
These aremuch finer grained than Penn Treebank pretermi-nals tags, and more akin to those used in Tree-Adjoining Grammar models (Bangalore and Joshi,1999).
There are a total of about 8, 000 lexical la-bels occurring in the treebank.
One might conjec-ture that a supertagging approach could go a longway toward parse disambiguation.
However, an up-per bound for such an approach for our corpus isbelow 55 percent parse selection accuracy, whichis the accuracy of an oracle tagger that chooses atrandom among the parses having the correct tag se-quence (Oepen et al, 2002).The semantic dependency trees are labelled withrelations most of which correspond to words in thesentence.
These labels provide some abstraction be-cause some classes of words have the same semanticlabel ?
for example all days of week are groupedin one class, as are all numbers.As an example the derivation tree for one analysisof the short sentence Let us see is shown in figure 1.The semantic dependency tree for the same sentenceis:let_relpron_rel see_understand_relIn addition to this information we have used themain part of speech information of the lexical headto annotate nodes in the derivation trees with labelslike verb, noun, preposition, etc.Other information that we have not explored in-cludes subcategorization information, lexical types(these are a rich set of about 500 syntactic types), in-dividual features such as tense, aspect, gender, etc.Another resource is the type hierarchy which can beexplored to form equivalence classes on which tobase statistical estimation.3 Models3.1 Generative ModelsWe learn generative models that assign probabilitiesto the derivation trees and the dependency trees.
Wetrain these models separately and in the final stagewe combine them to yield a probability or score foran entire sentence analysis.
We rank the possibleanalyses produced by the HPSG grammar in accor-dance with the estimated scores.We first describe learning such a generativemodel for derivation trees using a single decisiontree and a set of available features.
We will callthe set of available features {f1, .
.
.
, fm} a history.We estimate the probability of a derivation tree asP (t) = ?n?t P (expansion(n)|history(n)).
Inother words, the probability of the derivation treeis the product of the probabilities of the expansionof each node given its history of available features.Given a training corpus of derivation trees cor-responding to preferred analyses of sentences welearn the distribution P (expansion|history) us-ing decision trees.
We used a standard decisiontree learning algorithm where splits are determinedbased on gain ratio (Quinlan, 1993).
We grew thetrees fully and we calculated final expansion prob-abilities at the leaves by linear interpolation withestimates one level above.
This is a similar, butmore limited, strategy to the one used by Magerman(1995).The features over derivation trees which we madeavailable to the learner are shown in Table 1.
Thenode direction features indicate whether a node is aleft child, a right child, or a single child.
A num-ber of ancestor features were added to the history.The grammar used, the LinGO ERG has rules whichare maximally binary, and the complements and ad-juncts of a head are collected through multiple rules.Moreover, it makes extensive use of unary rules forvarious kinds of ?type changing?
operations.
A sim-ple PCFG is reasonably effective to the extent thatimportant dependencies are jointly expressed in alocal tree, as is mostly the case for the much flatterrepresentations used in the Penn Treebank.
Here,this is not the case, and the inclusion of ancestornodes in the history makes necessary informationmore often local in our models.
Grandparent anno-tation was used previously by Charniak and Carroll(1994) and Johnson (1998).No.
Name Example0 Node Label HCOMP1 Parent Node Label HCOMP2 Node Direction left3 Parent Node Direction none4 Grandparent Node Label IMPER5 Great Grandparent Top ?
yes6 Left Sister Node Label HCOMP7 Left Preterminal US8 Preterminal to the Left of 7 LET_V19 Category of Node verbTable 1: Features over derivation treesNo Name Example0 Node Label let_rel1 Direction of Dependent left2 Number of Intervening Dependents 13 Parent Node Label top4 Label to the Left or Right pron_relTable 2: Features over semantic dependency treesSimilarly we learn generative models over se-mantic dependency trees.
For these trees the ex-pansion of a node is viewed as consisting of sepa-rate trials for each dependent.
Any conditional de-pendencies among children of a node can be cap-tured by expanding the history.
The features usedfor the semantic dependency trees are shown in Ta-ble 2.
This set of only 5 features for semantic treesmakes the feature subset selection method less ap-plicable since there is no obvious redundancy in theset.
However the method still outperforms a singledecision tree.
The model for generation of semanticdependents to the left and right is as follows: Firstthe left dependents are generated from right to leftgiven the head, its parent, right sister, and the num-ber of dependents to the left that have already beengenerated.
After that, the right dependents are gen-erated from left to right, given the head, its parent,left sister and number of dependents to the right thathave already been generated.
We also add stop sym-bols at the ends to the left and right.
This modelis very similar to the markovized rule models inCollins (1997).
For example, the joint probabilityof the dependents of let_rel in the above examplewould be:P (stop|let_rel,left,0,top,none)?P (pron_rel|let_rel,right,0,top,stop)?P (see_understand_rel|let_rel,right,1,top,pron_rel)?P (stop|let_rel,right,2,top,see_understand_rel)3.2 Conditional Log Linear ModelsA conditional log linear model for estimating theprobability of an HPSG analysis given a sentence hasa set of features {f1, .
.
.
, fm} defined over analysesand a set of corresponding weights {?1, .
.
.
, ?m}for them.
In this work we have defined features overderivation trees and syntactic trees as described forthe branching process models.For a sentence s with possible analyses t1, .
.
.
, tk,the conditional probability for analysis ti is givenby:P (ti|s) =exp(?mj=1 fj(ti)?j)?ki?=1 exp(?mj=1 fj(ti?
)?j) (1)As in Johnson et al (1999) we trained the modelby maximizing the conditional likelihood of thepreferred analyses and using a Gaussian prior forsmoothing (Chen and Rosenfeld, 1999).
We usedconjugate gradient for optimization.Given an ensemble of decision trees estimatingprobabilities P (expansion|history) we define fea-tures for a corresponding log linear model as fol-lows: For each path from the root to a leaf in any ofthe decision trees, and for each possible expansionfor that path that was seen in the training set, weadd a feature feh(t).
For a tree t, this feature has asvalue the number of time the expansion e occurredin t with the history h.4 ExperimentsWe present experimental results comparing theparse ranking performance of different models.
Theaccuracy results are averaged over a ten-fold cross-validation on the data set summarized in Table 3.The sentences in this data set have exactly one pre-ferred parse selected by a human annotator.
At thisearly stage, the treebank is expected to be noisy be-cause all annotation was done by a single annotator.Accuracy results denote the percentage of test sen-tences for which the highest ranked analysis was thecorrect one.
This measure scores whole sentence ac-curacy and is therefore more strict than the labelledprecision/recall measures and more appropriate forthe task of parse ranking.
When a model ranks aset of m parses highest with equal scores and one ofthose parses is the preferred parse in the treebank,we compute the accuracy on this sentence as 1/m.To give an idea about the difficulty of the task onthe corpus we have used, we also show a baselinewhich is the expected accuracy of choosing a parsesentences length lex ambig struct ambig5277 7.0 4.1 7.3Table 3: Annotated corpus used in experiments: Thecolumns are, from left to right, the total number of sen-tences, average length, and average lexical and structuralambiguityModel Generative Log LinearTest Train Test TrainRandom 26.00 26.00 26.00 26.00PCFG-S 67.27 72.23 79.34 85.31PCFG-GP 72.39 83.89 81.52 91.56PCFG-DTAll 75.57 96.51 81.82 97.61Table 4: Parse ranking accuracy of syntactic mod-els: single decision tree compared to simpler mod-elsat random and accuracy results from simpler modelsthat have been used broadly in NLP.
PCFG-S is asimple PCFG model where we only have the nodelabel (feature 0) in the history, and PCFG-GP hasonly the node and its parent?s labels (features 0 and1) as in PCFG grammars with grandparent annota-tion.Table 4 shows the accuracy of parse selection ofthe three simple models mentioned above definedover derivation trees and the accuracy achieved bya single decision tree (PCFG-DTAll) using all fea-tures in Table 1.
The third column contains accuracyresults for log linear models using the same features.We can note from Table 4 that the genera-tive models greatly benefit from the addition ofmore conditioning information, while the log lin-ear model performs very well even with only simplerule features, and its accuracy does not increase sosharply with the addition of more complex features.The error reduction from PCFG-S to PCFG-DTAllis 25.36%, while the corresponding error reductionfor the log linear model is 12%.
The error reductionfor the log linear model from PCFG-GP to PCFG-DTAll is very small which suggests an overfitting ef-fect.
PCFG-S is doing much worse than the log lin-ear model with the same features, and this is true forthe training data as well as for the test data.
A partialexplanation for this is the fact that PCFG-S tries tomaximize the likelihood of the correct parses understrong independence assumptions, whereas the loglinear model need only worry about making the cor-rect parses more probable than the incorrect ones.Next we show results comparing the single deci-Type of Model All Features Feature SubspacesPCFG Log linear PCFG Log linearDerivation Trees 75.57 81.82 78.97 82.24Dependency Trees 67.38 69.91 68.88 73.50Combined Feature Subspaces Accuracy 80.10 83.32Table 5: Parse ranking accuracy: single decision trees and ensemblession tree model (PCFG-DTAll) to an ensemble of 11decision trees based on different feature subspaces.The decision trees in the ensemble are used to rankthe possible parses of a sentence individually andthen their votes are combined using a simple ma-jority vote.
The sets of features in each decisiontree are obtained by removing two features from thewhole space.
The left preterminal features (featureswith numbers 7 and 8) participate in only one de-cision tree.
Also, features 2, 3, and 5 participatein all decision trees since they have very few pos-sible values and should not partition the space tooquickly.
The feature space of each of the 10 de-cision trees not containing the left preterminal fea-tures was formed by removing two of the featuresfrom among those with numbers {0, 1, 4, 6, and 9}from the initial feature space (minus features 7 and8).
This method for constructing feature subspacesis heuristic, but is based on the intuition of removingthe features that have the largest numbers of possi-ble values.1Table 5 shows the accuracy results for mod-els based on derivation trees, semantic dependencytrees, and a combined model.
The first row showsparse ranking accuracy using derivation trees ofgenerative and log linear models over the same fea-tures.
Results are shown for features selected by a asingle decision tree, and an ensemble of 11 decisiontree models based on different feature subspaces asdescribed above.
The relative improvement in accu-racy of the log linear model from single to multipledecision trees is fairly small.The second row shows corresponding models forthe semantic dependency trees.
Since there are asmall number of features used for this task, theperformance gain from using feature subspaces is1We also preformed an experiment where we removed ev-ery combination of two features from the whole space of fea-tures 0?8 to obtain subspaces.
This results in a large numberof feature subspaces (36).
The performance of this method wasslightly worse than the result reported in Table 5 (78.52%).
Wepreferred to work with an ensemble of 11 decision trees forcomputational reasons.not so large.
It should be noted that there is a90.9% upper bound on parse ranking accuracy us-ing semantic trees only.
This is because for manysentences there are several analyses with the samesemantic dependency structure.
Interestingly, forsemantic trees the difference between the log lin-ear and generative models is not so large.
Finally,the last row shows the combination of models overderivation trees and semantic trees.
The feature sub-space ensemble of 11 decision tree models for thederivation trees is combined with the ensemble of5 feature subspace models over semantic dependen-cies to yield a larger ensemble that ranks possiblesentence analyses based on weighted majority vote(with smaller weights for the semantic models).
Theimprovement for PCFG models from combining thesyntactic and semantic models is about 5.4% errorreduction from the error rate of the better (syntac-tic) models.
The corresponding log linear modelcontains all features from the syntactic and semanticdecision trees in the ensemble.
The error reductiondue to the addition of semantics is 6.1% for the loglinear model.
Overall the gains from using semanticinformation are not as good as we expected.
Furtherresearch remains to be done in this area.The results show that decision trees and ensem-bles of decision trees can be used to greatly improvethe performance of generative models over deriva-tion trees and dependency trees.
The performanceof generative models using a lot of conditioning in-formation approaches the performance of log linearmodels although the latter remain clearly superior.The corresponding improvement in log linear mod-els when adding more complex features is not aslarge as the improvement in generative models.
Onthe other hand, there might be better ways to incor-porate the information from additional history in loglinear models.5 Error AnalysisIt is interesting to see what the hard disambiguationdecisions are, that the combined syntactic-semanticmodels can not at present get right.We analyzed some of the errors made by the bestlog linear model defined over derivation trees andsemantic dependency trees.
We selected for analysissentences that the model got wrong on one of thetraining - test splits in the 10 fold cross-validationon the whole corpus.
The error analysis suggeststhe following breakdown:?
About 40% of errors are due to inconsistencyor errors in annotation?
About 15% of the errors are due to grammarlimitations?
About 45% of the errors are real errors and wecould hope to get them rightThe inconsistency in annotation hurts the perfor-mance of the model both when in the training datasome sentences were annotated incorrectly and themodel tried to fit its parameters to explain them, andwhen in the test data the model chose the correctanalysis but it was scored as incorrect because ofincorrect annotation.
It is not straightforward to de-tect inconsistencies in the training data by inspect-ing test data errors.
Therefore the percentages wehave reported are not exact.The log linear model seems to be more suscepti-ble to errors in the training set annotation than thePCFG models, because it can easily adjust its pa-rameters to fit the noise (causing overfitting), espe-cially when given a large number of features.
Thismight partly explain why the log linear model doesnot profit greatly over this data set from the additionof a large number of features.A significant portion of the real errors made bythe model are PP attachment errors.
Another classof errors come from parallel structures and long dis-tance dependencies.
For example, the model didnot disambiguate correctly the sentence Is anywherefrom two thirty to five on Thursday fine?, preferringthe interpretation from [two thirty] to [five on Thurs-day] rather than what would be the more commonmeaning [from [two thirty] to [five]] [on Thurs-day].
This disambiguation decision seems to re-quire common world knowledge or it might be ad-dressable with addition of knowledge about paral-lel structures.
( (Johnson et al, 1999) add featuresmeasuring parallelism).We also compared the errors made by the best loglinear model using only derivation tree features tothe ones made by the combined model.
The largemajority of the errors made by the combined modelwere also made by the syntactic model.
Examplesof errors corrected with the help of semantic infor-mation include:The sentence How about on the twenty fourthMonday?
(punctuation is not present in the corpus)was analyzed by the model based on derivation treesto refer to the Monday after twenty three Mondaysfrom now, whereas the more common interpretationwould be that the day being referred to is the twentyfourth day of the month, and it is also a Monday.There were several errors of this sort corrected bythe dependency trees model.Another interesting error corrected by the seman-tic model was for the sentence: We will get a caband go.
The syntactic model chose the interpreta-tion of that sentence in the sense: We will becomea cab and go, which was overruled by the semanticmodel.6 Conclusions and Future WorkWe have presented work on building probabilisticmodels for HPSG parse disambiguation.
As thenumber of available features increases it becomesmore important to select relevant features automati-cally.
We have shown that decision trees using dif-ferent feature subspaces can be combined in ensem-bles that choose the correct parse with higher accu-racy than individual models.Our plans for future work include exploring moreinformation from the HPSG signs and defining fea-tures that capture long distance dependencies.
An-other line of future research is defining models overthe deeper MRS semantic representations, possiblyin conjunction with clustering of semantic types.7 AcknowledgementsWe would particularly like to thank Stephan Oepen,for directing the Redwoods treebanking project andgetting us set up with the HPSG development envi-ronment, and Dan Flickinger, for explanations ofthe LinGO ERG and help with the error analysis.Thanks also to Stuart Shieber and other participantsin the Redwoods project meetings for many discus-sions.
And our thanks to Dan Klein for letting ususe his implementation of conjugate gradient, andto the anonymous CoNLL-2002 reviewers for help-ful comments.
This paper is based upon work sup-ported in part by the National Science Foundationunder Grant No.
0085896.ReferencesSrinivas Bangalore and Aravind K. Joshi.
1999.Supertagging: An approach to almost parsing.Computational Linguistics, 25(2).Leo Breiman.
1996.
Bagging predictors.
MachineLearning, 24(2):123?140.Eugene Charniak and G. Carroll.
1994.
Context-sensitive statistics for improved grammatical lan-guage models.
In Proceedings of the TwelthNational Conference on Artificial Intelligence,pages 742 ?
747, Seattle, WA.Eugene Charniak.
1997.
Statistical parsing with acontext-free grammar and word statistics.
In Pro-ceedings of the Fourteenth National Conferenceon Artificial Intelligence, pages 598 ?
603, Provi-dence, RI.S.
Chen and R. Rosenfeld.
1999.
A gaussian priorfor smoothing maximum entropy models.
InTechnical Report CMUCS -99-108.Michael John Collins.
1997.
Three generative, lexi-calised models for statistical parsing.
In Proceed-ings of the 35th Meeting of the Association forComputational Linguistics and the 7th Confer-ence of the European Chapter of the ACL, pages16 ?
23, Madrid, Spain.Ann Copestake, Alex Lascarides, and Dan Flickin-ger.
2001.
An algebra for semantic constructionin constraint-based grammars.
In Proceedings ofthe 39th Meeting of the Association for Compu-tational Linguistics, Toulouse, France.Dan Flickinger.
2000.
On building a more effi-cient grammar by exploiting types.
Natural Lan-guage Engineering, 6 (1) (Special Issue on Effi-cient Processing with HPSG):15 ?
28.Yoav Freund and Robert E. Schapire.
1996.
Exper-iments with a new boosting algorithm.
In Inter-national Conference on Machine Learning, pages148?156.Masahiko Haruno, Satoshi Shirai, and YoshifumiOoyama.
1998.
Using decision trees to con-struct a practical parser.
In Proceedings of the36th Meeting of the Association for Computa-tional Linguistics, pages 505 ?
511.Ulf Hermjakob and Reymond J. Mooney.
1997.Learning parse and translation decisions from ex-amples with rich context.
In Proceedings of the35th Meeting of the Association for Computa-tional Linguistics and the 7th Conference of theEuropean Chapter of the ACL, pages 482 ?
489.Tin Kam Ho.
1998.
The random subspace methodfor constructing decision forests.
IEEE Transac-tions on Pattern Analysis and Machine Intelli-gence, 20(8):832?844.Mark Johnson, Stuart Geman, Stephen Canon,Zhiyi Chi, and Stefan Riezler.
1999.
Estimatorsfor stochastic ?unification-based?
grammars.
InProceedings of the 37th Meeting of the Associ-ation for Computational Linguistics, pages 535 ?541, College Park, MD.Mark Johnson.
1998.
PCFG models of linguistictree representations.
Computational Linguistics,24:613?632.D.
M. Magerman.
1995.
Statistical decision-treemodels for parsing.
In Proceedings of the 33rdMeeting of the Association for ComputationalLinguistics.Stephan Oepen, Kristina Toutanova, Stuart Shieber,Christopher Manning, Dan Flickinger, and Thor-sten Brants.
2002.
The LinGo Redwoods tree-bank: Motivation and preliminary applications.In COLING 19.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven Phrase Structure Grammar.
University ofChicago Press.J.
R. Quinlan.
1993.
C4.5 Programs for MachineLearning.
Morgan Kaufmann.Stefan Riezler, Detlef Prescher, Jonas Kuhn, andMark Johnson.
2000.
Lexicalized stochasticmodeling of constraint-based grammars usinglog-linear measures and EM training.
In Pro-ceedings of the 38th Meeting of the Associationfor Computational Linguistics, Hong Kong.Gabriele Zenobi and Padraig Cunningham.
2001.Using diversity in preparing ensembles of classi-fiers based on different feature subsets to mini-mize generalization error.
In ECML, pages 576?587.Z.
Zheng.
1998.
Naive Bayesian classifier commit-tees.
In ECML, pages 196?207.
