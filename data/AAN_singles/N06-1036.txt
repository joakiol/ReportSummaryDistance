Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 280?287,New York, June 2006. c?2006 Association for Computational LinguisticsBackoff Model Training using Partially Observed Data:Application to Dialog Act TaggingGang Ji and Jeff BilmesDepartment of Electrical EngineeringUniversity of WashingtonSeattle, WA 98105-2500{gang,bilmes}@ee.washington.eduAbstractDialog act (DA) tags are useful for manyapplications in natural language process-ing and automatic speech recognition.
Inthis work, we introduce hidden backoffmodels (HBMs) where a large generalizedbackoff model is trained, using an embed-ded expectation-maximization (EM) pro-cedure, on data that is partially observed.We use HBMs as word models condi-tioned on both DAs and (hidden) DA-segments.
Experimental results on theICSI meeting recorder dialog act corpusshow that our procedure can strictly in-crease likelihood on training data and caneffectively reduce errors on test data.
Inthe best case, test error can be reduced by6.1% relative to our baseline, an improve-ment on previously reported models thatalso use prosody.
We also compare withour own prosody-based model, and showthat our HBM is competitive even withoutthe use of prosody.
We have not yet suc-ceeded, however, in combining the bene-fits of both prosody and the HBM.1 IntroductionDiscourse patterns in natural conversations andmeetings are well known to provide interesting anduseful information about human conversational be-havior.
They thus attract research from many differ-ent and beneficial perspectives.
Dialog acts (DAs)(Searle, 1969), which reflect the functions that ut-terances serve in a discourse, are one type of suchpatterns.
Detecting and understanding dialog actpatterns can provide benefit to systems such as au-tomatic speech recognition (ASR) (Stolcke et al,1998), machine dialog translation (Lee et al, 1998),and general natural language processing (NLP) (Ju-rafsky et al, 1997b; He and Young, 2003).
DA pat-tern recognition is an instance of ?tagging.?
Manydifferent techniques have been quite successful inthis endeavor, including hidden Markov models (Ju-rafsky et al, 1997a; Stolcke et al, 1998), seman-tic classification trees and polygrams (Mast et al,1996), maximum entropy models (Ang et al, 2005),and other language models (Reithinger et al, 1996;Reithinger and Klesen, 1997).
Like other taggingtasks, DA recognition can also be achieved usingconditional random fields (Lafferty et al, 2001; Sut-ton et al, 2004) and general discriminative model-ing on structured outputs (Bartlett et al, 2004).
Inmany sequential data analysis tasks (speech, lan-guage, or DNA sequence analysis), standard dy-namic Bayesian networks (DBNs) (Murphy, 2002)have shown great flexibility and are widely used.
In(Ji and Bilmes, 2005), for example, an analysis ofDA tagging using DBNs is performed, where themodels avoid label bias by structural changes andavoid data sparseness by using a generalized back-off procedures (Bilmes and Kirchhoff, 2003).Most DA classification procedures assume thatwithin a sentence of a particular fixed DA type,there is a fixed word distribution over the entire sen-tence.
Similar to (Ma et al, 2000) (and see cita-tions therein), we have found, however, that intra-280sentence discourse patterns are inherently dynamic.Moreover, the patterns are specific to each type ofDA, meaning a sentence will go through a DA-specific sequence of sub-DA phases or ?states.?
Agenerative description of this phenomena is that aDA is first chosen, and then words are generatedaccording to both the DA and to the relative posi-tion of the word in that sentence.
For example, a?statement?
(one type of DA) can consist of a sub-ject (noun phrase), verb phrase, and object (nounphrase).
This particular sequence might be differentfor a different DA (e.g., a ?back-channel?).
Our be-lief is that explicitly modeling these internal statescan help a DA-classification system in conversa-tional meetings or dialogs.In this work, we describe an approach that ismotivated by several aspects of the typical DA-classification procedure.
First, it is rare to have sub-DAs labeled in training data, and indeed this is trueof the corpus (Shriberg et al, 2004) that we use.Therefore, some form of unsupervised clustering orpre-shallow-parsing of sub-DAs must be performed.In such a model, these sub-DAs are essentially un-known hidden variables that ideally could be trainedwith an expectation-maximization (EM) procedure.Second, when training models of language, it is nec-essary to employ some form of smoothing method-ology since otherwise data-sparseness would renderstandard maximum-likelihood trained models use-less.
Third, discrete conditional probability distri-butions formed using backoff models that have beensmoothed (particularly using modified Kneser-Ney(Chen and Goodman, 1998)) have been extremelysuccessful in many language modeling tasks.
Train-ing backoff models, however, requires that all datais observed so that data counts can be formed.
In-deed, our DA-specific word models (implementedvia backoff) will also need to condition on the cur-rent sub-DA, which at training time is unknown.We therefore have developed a procedure that al-lows us to train generalized backoff models (Bilmesand Kirchhoff, 2003), even when some or all of thevariables involved in the model are hidden.
We thuscall our models hidden backoff models (HBMs).
Ourmethod is indeed a form of embedded EM training(Morgan and Bourlard, 1990), and more generallyis a specific form of EM (Neal and Hinton, 1998).Our approach is similar to (Ma et al, 2000), exceptour underlying language models are backoff-basedand thus retain the benefits of advanced smoothingmethods, and we utilize both a normal and a backoffEM step as will be seen.
We moreover wrap up theabove ideas in the framework of dynamic Bayesiannetworks, which are used to represent and train allof our models.We evaluate our methods on the ICSI meetingrecorder dialog act (MRDA) (Shriberg et al, 2004)corpus, and find that our novel hidden backoff modelcan significantly improve dialog tagging accuracy.With a different number of hidden states for eachDA, a relative reduction in tagging error rate asmuch as 6.1% can be achieved.
Our best HBM resultshows an accuracy that improves on the best known(to our knowledge) result on this corpora which isone that uses acoustic prosody as a feature.
We havemoreover developed our own prosody model andwhile we have not been able to usefully employ bothprosody and the HBM technique together, our HBMis competitive in this case as well.
Furthermore, ourresults show the effectiveness of our embedded EMprocedure, as we demonstrate that it increases train-ing log likelihoods, while simultaneously reducingerror rate.Section 2 briefly summarizes our baseline DBN-based models for DA tagging tasks.
In Section 3,we introduce our HBMs.
Section 4 contains experi-mental evaluations on the MRDA corpus and finallySection 5 concludes.2 DBN-based Models for TaggingDynamic Bayesian networks (DBNs) (Murphy,2002) are widely used in sequential data analysissuch as automatic speech recognition (ASR) andDNA sequencing analysis (Durbin et al, 1999).
Ahidden Markov model (HMM) for DA tagging as in(Stolcke et al, 1998) is one such instance.Figure 1 shows a generative DBN model that willbe taken as our baseline.
This DBN shows a pro-logue (the first time slice of the model), an epilogue(the last slice), and a chunk that is repeated suffi-ciently to fit the entire data stream.
In this case,the data stream consists of the words of a meet-ing conversation, where individuals within the meet-ing (hopefully) take turns speaking.
In our model,the entire meeting conversation, and all turns of all281sentence changeDA <s>dialog actword <s>wordprologue chunk epilogueFigure 1: Baseline generative DBN for DA tagging.speakers, are strung together into a single streamrather than treating each turn in the meeting indi-vidually.
This approach has the benefit that we areable to integrate a temporal DA-to-DA model (suchas a DA bigram).In all our models, to simplify we assume that thesentence change information is known (as is com-mon with this corpus (Shriberg et al, 2004)).
Wenext describe Figure 1 in detail.
Normally, the sen-tence change variable is not set, so that we are withina sentence (or a particular DA).
When a sentencechange does not occur, the DA stays the same fromslice to slice.
During this time, we use a DA-specificlanguage model (implemented via a backoff strat-egy) to score the words within the current DA.When a sentence change event does occur, a newDA is predicted based on the DA from the previoussentence (using a DA bigram).
At the beginning ofa sentence, rather than conditioning on the last wordof the previous sentence, we condition on the specialstart of sentence <s> token, as shown in the figureby having a special parent that is used only whensentence change is true.
Lastly, at the very beginningof a meeting, a special start of DA token is used.The joint probability under this baseline model iswritten as follows:P (W, D) =?kP (dk|dk?1)?
?iP (wk,i|wk,i?1, dk),(1)where W = {wk,i} is the word sequence, D = {dk}is the DA sequence, dk is the DA of the k-th sen-tence, and wk,i is the i-th word of the k-th sentencein the meeting.Because all variables are observed when trainingour baseline, we use the SRILM toolkit (Stolcke,2002), modified Kneser-Ney smoothing (Chen andGoodman, 1998), and factored extensions (Bilmesand Kirchhoff, 2003).
In evaluations, the Viterbi al-gorithm (Viterbi, 1967) can be used to find the bestDA sequence path from the words of the meetingaccording to the joint distribution in Equation (1).3 Hidden Backoff ModelsWhen analyzing discourse patterns, it can be seenthat sentences with different DAs usually have dif-ferent internal structures.
Accordingly, in this workwe do not assume sentences for each dialog act havethe same hidden state patterns.
For instance (and asmentioned above), a statement can consist of a nounfollowed by a verb phase.A problem, however, is that sub-DAs are not an-notated in our training corpus.
While clustering andannotation of these phrases is already a widely de-veloped research topic (Pieraccini and Levin, 1991;Lee et al, 1997; Gildea and Jurafsky, 2002), in ourapproach we use an EM algorithm to learn these hid-den sub-DAs in a data-driven fashion.
Pictorially,we add a layer of hidden states to our baseline DBNas illustrated in Figure 2.sentence changeDA <s>dialog actword <s>wordprologue chunk epiloguehidden stateFigure 2: Hidden backoff model for DA tagging.Under this model, the joint probability is:P (W, S, D) =?kP (dk|dk?1)?
?i[P (sk,i|sk,i?1, dk)?
P (wk,i|wk,i?1, sk,i, dk)] ,(2)282where S = {sk,i} is the hidden state sequence, sk,iis the hidden state at the i-th position of the k-th sen-tence, and other variables are the same as before.Similar to our baseline model, the DA bigramP (dk|dk?1) can be modeled using a backoff bi-gram.
Moreover, if the states {sk,i} are knownduring training, the word prediction probabilityP (wk,i|wk,i?1, sk,i, dk) can also use backoff and betrained accordingly.
The hidden state sequence isunknown, however, and thus cannot be used to pro-duce a standard backoff model.
What we desire isan ability to utilize a backoff model (to mitigate datasparseness effects) while simultaneously retainingthe state as a hidden (rather than an observed) vari-able, and also have a procedure that trains the entiremodel to improve overall model likelihood.Expectation-maximization (EM) algorithms arewell-known to be able to train models with hiddenstates.
Furthermore, standard advanced smoothingmethods such as modified Kneser-Ney smoothing(Chen and Goodman, 1998) utilize integer counts(rather than fractional ones), and they moreoverneed ?meta?
counts (or counts of counts).
There-fore, in order to train this model, we propose anembedded training algorithm that cycles between astandard EM training procedure (to train the hiddenstate distribution), and a stage where the most likelyhidden states (and their counts and meta counts) areused externally to train a backoff model.
This pro-cedure can be described in detail as follows:Input : W ?
meeting word sequenceInput : D ?
DA sequenceOutput : P (sk,i|sk,i?1) - state transition CPTOutput : P (wk,i|wk,i?1, sk,i, dk) - word modelrandomly generate a sequence S;1backoff train P (wk,i|wk,i?1, sk,i, dk);2while not ?converged?
do3EM train P (sk,i|sk,i?1);4calculate best S?
sequence by Viterbi;5backoff train P (wk,i|wk,i?1, s?k,i, dk);6end7Algorithm 1: Embedded training for HBMsIn the algorithm, the input contains words anda DA for each sentence in the meeting.
The out-put is the corresponding conditional probability ta-ble (CPT) for hidden state transitions, and a back-off model for word prediction.
Because we train thebackoff model when some of the variables are hid-den, we call the result a hidden backoff model.
Whilewe have seen embedded Viterbi training used in thepast for simultaneously training heterogeneous mod-els (e.g., Markov chains and Neural Networks (Mor-gan and Bourlard, 1990)), this is the first instanceof training backoff-models that involve hidden vari-ables that we are aware of.While embedded Viterbi estimation is not guar-anteed to have the same convergence (or fixed-pointunder convergence) as normal EM (Lember andKoloydenko, 2004), we find empirically this to bethe case (see examples below).
Moreover, our algo-rithm can easily be modified so that instead of tak-ing a Viterbi alignment in step 5, we instead use aset of random samples generated under the currentmodel.
In this case, it can be shown using a law-of-large numbers argument that having sufficient sam-ples guarantees the algorithm will converge (we willinvestigate this modification in future work).Of course, when decoding with such a model, aconventional Viterbi algorithm can still be used tocalculate the best DA sequence.4 Experimental ResultsWe evaluated our hidden backoff model on theICSI meeting recorder dialog act (MRDA) corpus(Shriberg et al, 2004).
MRDA is a rich data set thatcontains 75 natural meetings on different topics witheach meeting involving about 6 participants.
DA an-notations from ICSI were based on a previous ap-proach in (Jurafsky et al, 1997b) with some adapta-tion for meetings in a number of ways described in(Bhagat et al, 2003).
Each DA contains a main tag,several optional special tags and an optional ?disrup-tion?
form.
The total number of distinct DAs in thecorpus is as large as 1260.
In order to make the prob-lem comparable to other work (Ang et al, 2005), aDA tag sub-set is used in our experiments that con-tains back channels (b), place holders (h), questions(q), statements (s), and disruptions (x).
In our eval-uations, among the entire 75 conversations, 51 areused as the training set, 11 are used as the develop-ment set, 11 are used as test set, and the remaining3 are not used.
For each experiment, we used a ge-netic algorithm to search for the best factored lan-guage model structure on the development set and283we report the best results.Our baseline system is the generative modelshown in Figure 1 and uses a backoff implementa-tion of the word model, and is optimized on the de-velopment set.
We use the SRILM toolkit with ex-tensions (Bilmes and Kirchhoff, 2003) to train, anduse GMTK (Bilmes and Zweig, 2002) for decoding.Our baseline system has an error rate of 19.7% onthe test set, which is comparable to other approacheson the same task (Ang et al, 2005).4.1 Same number of states for all DAsTo compare against our baseline, we use HBMs inthe model shown in Figure 2.
To train, we followedAlgorithm 1 as described before and as is here de-tailed in Figure 3.Initialization:- randomize states- train word FLMConvergence:- llh change < 0.2%- or 10 iterations- find best state path- train word FLMText Text3-epochEM trainingNo- find best state path- train word FLMText Text5-epochEM trainingYesFigure 3: Embedded training: llh = log likelihoodIn this implementation, an upper triangular ma-trix (with self-transitions along the diagonal) is usedfor the hidden state transition probability table sothat sub-DA states only propagate in one direction.When initializing the hidden state sequence of a DA,we expanded the states uniformly along the sen-tence.
This initial alignment is then used for HBMtraining.
In the word models used in our experi-ments, the backoff path first drops previous words,then does a parallel backoff to hidden state and DAusing a mean combination strategy.The HBM thus obtained was then fed into themain loop of our embedded EM algorithm.
Thetraining was considered to have ?converged?
if ei-ther it exceeded 10 iterations (which never hap-pened) or the relative log likelihood change was lessthan 0.2%.
Within each embedded iteration, threeEM epochs were used.
After each EM iteration,a Viterbi alignment was performed thus obtainingwhat we expect to be a better hidden state alignment.This updated alignment, was then used to train anew HBM.
The newly generated model was then fedback into the embedded training loop until it con-verged.
After the procedure met our convergencecriteria, an additional five EM epochs were carriedout in order to provide a good hidden state transi-tion probability table.
Finally, after Viterbi align-ment and text generation was performed, the wordHBM was trained from the best state sequence.To evaluate our hidden backoff model, the Viterbialgorithm was used to find the best DA sequence ac-cording to test data, and the tagging error rates werecalculated.
In our first experiment, an equal num-ber of hidden states for all DAs were used in eachmodel.
The effect of this number on the accuracy ofDA tagging is shown in Table 1.Table 1: HBMs, different numbers of hidden states.# states error improvementbaseline 19.7% ?2-state 18.7% 5.1%3-state 19.5% 1.0%For the baseline system, the backoff path firstdrops dialog act, and for the HBMs, all backoffpaths drop hidden state first and drop DA sec-ond.
From Table 1 we see that with two hidden statesfor every DA the system can reduce the tagging errorrate by more than 5% relative.
As a comparison, in(Ang et al, 2005), where conditional maximum en-tropy models (which are conditionally trained) areused, the error rate is 18.8% when using both wordand acoustic prosody features, and and 20.5% with-out prosody.
When the number of hidden states in-creases to 3, the improvement decreases even thoughit is still (very slightly) better than the baseline.
Webelieve the reasons are as follows: First, assumingdifferent DAs have the same number of hidden statesmay not be appropriate.
For example, back chan-nels usually have shorter sentences and are constantin discourse pattern over a DA.
On the other hand,284questions and statements typically have longer, andmore complex, discourse structures.
Second, evenunder the same DA, the structure and inherent lengthof sentence can vary.
For example, ?yes?
can also bea statement even though it has only one word.
There-fore, one-word statements need completely differ-ent hidden state patterns than those in subject-verb-object like statements ?
having one monolithic 3-state model for statements might be inappropriate.This issue is discussed further in Section 4.4.4.2 Different states for different DAsIn order to mitigate the first problem describedabove, we allow different numbers of hidden statesfor each DA.
This, however, leads to a combinato-rial explosion of possibilities if done in a na?
?ve fash-ion.
Therefore, we attempted only a small numberof combinations based on the statistics of numbersof words in each DA given in Table 2.Table 2: Length statistics of different DAs.DA mean median std p(b) 1.0423 1 0.2361 0.4954(h) 1.3145 1 0.7759 0.4660(q) 6.5032 5 6.3323 0.3377(s) 8.6011 7 7.8380 0.3013(x) 1.7201 1 1.1308 0.4257Table 2 shows the mean and median number ofwords per sentence for each DA as well as the stan-dard deviation.
Also, the last column provides the pvalue according to fitting the length histogram to ageometric distribution (1 ?
p)np.
As we expected,back channels (b) and place holders (h) tend to haveshorter sentences while questions (q) and statements(s) have longer ones.
From this analysis, we usefewer states for (b) and (h) and more states for (q)and (s).
For disruptions (x), the standard deviation ofnumber of words histogram is relatively high com-pared with (b) and (h), so we also used more hiddenstates in this case.
In our experimental results below,we used one state for (b) and (h), and various num-bers of hidden states for other DAs.
Tagging errorrates are shown in Table 3.From Table 3, we see that using different num-bers of hidden states for different DAs can producebetter models.
Among all the experiments we per-Table 3: Number of hidden states for different DAs.b h q s x error improvement1 1 4 4 1 18.9% 4.1%1 1 3 3 2 18.9% 4.1%1 1 2 2 2 18.7% 5.1%1 1 3 2 2 18.6% 5.6%1 1 3 2 2 18.5% 6.1%formed, the best case is given by three states for (q),two states for (s) and (x), and one state for (b) and(h).
This combination gives 6.1% relative reductionof error rate from the baseline.4.3 Effect of embedded EM trainingIncorporating backoff smoothing procedures intoBayesian networks (and hidden variable training inparticular) can show benefits for any data domainwhere smoothing is necessary.
To understand theproperties of our algorithm a bit better, after eachtraining iteration using a partially trained model, wecalculated both the log likelihood of the training setand the tagging error rate of the test data.
Figure 4shows these results using the best configuration fromthe previous section (three states for (q), two for(s)/(x) and one for (b)/(h)).
This example is typicalof the convergence we see of Algorithm 1, whichempirically suggests that our procedure may be sim-ilar to a generalized EM (Neal and Hinton, 1998).1 2 3 4 5 6 718192021222324iterationserror rate(%)?1.2?1.15?1.1?1.05x 106loglikelihoodllhbaselineerror rateFigure 4: Embedded EM training performance.We find that the log likelihood after each EMtraining is strictly increasing, suggesting that ourembedded EM algorithm for hidden backoff models285is improving the overall joint likelihood of the train-ing data according to the model.
This strict increaseof likelihood combined with the fact that Viterbitraining does not have the same theoretical conver-gence guarantees as does normal EM indicates thatmore detailed theoretical analysis of this algorithmused with these particular models is desirable.From the figure we also see that both the loglikelihood and tagging error rate ?converge?
af-ter around four iterations of embedded training.This quick convergence indicates that our embeddedtraining procedure is effective.
The leveling of theerror rates after several iterations shows that modelover-fitting appears not to be an issue presumablydue to the smoothed embedded backoff models.4.4 Discussion and Error AnalysisA large portion of our tagging errors are due to con-fusing the DA of short sentences such as ?yeah?, and?right?.
The sentence, ?yeah?
can either be a backchannel or an affirmative statement.
There are alsocases where ?yeah??
is a question.
These types ofconfusions are difficult to remove in the prosody-less framework but there are several possibilities.First, we can allow the use of a ?fork and join?
tran-sition matrix, where we fork to each DA-specificcondition (e.g., short or long) and join thereafter.Alternatively, hidden Markov chain structuring al-gorithms or context (i.e., conditioning the numberof sub-DAs on the previous DA) might be helpful.Finding a proper number of hidden states for eachDA is also challenging.
In our preliminary work, wesimply explored different combinations using sim-ple statistics of the data.
A systematic procedurewould be more beneficial.
In this work, we alsodid not perform any hidden state tying within dif-ferent DAs.
In practice, some states in statementsshould be able to be beneficially tied with otherstates within questions.
Our results show that havingthree states for all DAs is not as good as two statesfor all.
But with tying, more states might be moresuccessfully used.4.5 Influence of Prosody CuesIt has been shown that prosody cues provide use-ful information in DA tagging tasks (Shriberg etal., 1998; Ang et al, 2005).
We also incorporatedprosody features in our models.
We used ESPSget f0 based on RAPT algorithm (Talkin, 1995) toget F0 values.
For each speaker, mean and variancenormalization is performed.
For each word, a linearregression is carried on the normalized F0 values.We quantize the slope values into 20 bins and treatthose as prosody features associated with each word.After adding the prosody features, the simple gener-ative model as shown in Figure 5 gives 18.4% errorrate, which is 6.6% improvement over our baseline.There is no statistical difference between the bestperformance of this prosody model and the earlierbest HBM.
This implies that the HBM can obtainas good performance as a prosody-based model butwithout using prosody.sentence changeDA <s>dialog actword <s>wordprologue chunk epilogueprosodyFigure 5: Generative prosody model for DA tagging.The next obvious step is to combine an HBM withthe prosody information.
Strangely, even after ex-perimenting with many different models (includingones where prosody depends on DA; prosody de-pends on DA and the hidden state; prosody dependson DA, hidden state, and word; and many varia-tions thereof), we were unsuccessful in obtaininga complementary benefit when using both prosodyand an HBM.
One hypothesis is that our prosodyfeatures are at the word-level (rather than at the DAlevel).
Another problem might be the small size ofthe MRDA corpus relative to the model complexity.Yet a third hypothesis is that the errors corrected byboth methods are the same ?
indeed, we have ver-ified that the corrected errors overlap by more than50%.
We plan further investigations in future work.2865 ConclusionsIn this work, we introduced a training method forhidden backoff models (HBMs) to solve a problemin DA tagging where smoothed backoff models in-volving training-time hidden variables are useful.We tested this procedure in the context of dynamicBayesian networks.
Different hidden states wereused to model different positions in a DA.
Accordingto empirical evaluations, our embedded EM algo-rithm effectively increases log likelihood on trainingdata and reduces DA tagging error rate on test data.If different numbers of hidden states are used for dif-ferent DAs, we find that our prosody-independentHBM reduces the tagging error rate by 6.1% rela-tive to the baseline, a result that improves upon pre-viously reported work that uses prosody, and that iscomparable to our own new result that also incorpo-rates prosody.
We have not yet been able to combinethe benefits of both an HBM and prosody informa-tion.
This material is based upon work supportedby the National Science Foundation under Grant No.IIS-0121396.ReferencesJ.
Ang et al 2005.
Automatic dialog act segmentation andclassification in multiparty meetings.
In ICASSP.P.
Bartlett et al 2004.
Exponentiated gradient algorithms forlarge-margin structured classification.
In NIPS.S.
Bhagat et al 2003.
Labeling guide for dialog act tags in themeeting recordering meetings.
Technical Report 2, Interna-tional Computer Science Insititute.J.
Bilmes and K. Kirchhoff.
2003.
Factored language mod-els and generalized parallel backoff.
In Human Lang.
Tech.,North American Chapter of Asssoc.
Comp.
Ling., Edmonton,Alberta, May/June.J.
Bilmes and G. Zweig.
2002.
The Graphical Models Toolkit:An open source software system for speech and time-seriesprocessing.
Proc.
IEEE Intl.
Conf.
on Acoustics, Speech, andSignal Processing.S.
Chen and J. Goodman.
1998.
An empirical study of smooth-ing techniques for language modeling.
Technical report,Computer Science Group, Harvard University.R.
Durbin et al 1999.
Biological Sequence Analysis: Prob-abilistic Models of Proteins and Nucleic Acids.
CambridgeUniversity Press.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling of seman-tic roles.
Computational Linguistics, 28(3):245?288.Y.
He and S. Young.
2003.
A data-driven spoken language un-derstanding system.
In Proc.
IEEE Workshop on AutomaticSpeech Recognition and Understanding, pages 583?588.G.
Ji and J. Bilmes.
2005.
Dialog act tagging using graphicalmodels.
In Proc.
IEEE Intl.
Conf.
on Acoustics, Speech, andSignal Processing, Philadelphia, PA, March.D.
Jurafsky et al 1997a.
Automatic detection of discoursestructure for speech recognition and understanding.
In Proc.IEEE Workshop on Speech Recognition and Understanding.D.
Jurafsky et al 1997b.
Switchboard SWBD-DAMSL shallow-discourse-function annotation coders man-ual.
Technical Report 97-02, Institute of Cognitive Science,University of Colorado.J.
Lafferty et al 2001.
Conditional random fields: Probabilisticmodels for segmenting and labeling sequence data.
In ICML.K.
Lee et al 1997.
Restricted representation of phrase struc-ture grammar for building a tree annotated corpus of Korean.Natural Language Engineering, 3(2-3):215?230.H.
Lee et al 1998.
Speech act analysis model of Korean utter-ances for automatic dialog translation.
J.
KISS(B) (Softwareand Applications), 25(10):1443?1452.J.
Lember and A. Koloydenko.
2004.
Adjusted viterbi training.a proof of concept.
In Submission.K.
Ma et al 2000.
Bi-modal sentence structure for languagemodeling.
Speech Communication, 31(1):51?67.M.
Mast et al 1996.
Automatic classification of dialog actswith semantic classification trees and polygrams.
Connec-tionist, Statistical and Symbolic Approaches to Learning forNatural Language Processing, pages 217?229.N.
Morgan and H. Bourlard.
1990.
Continuous speech recogni-tion using multilayer perceptrons with hidden Markov mod-els.
In ICASSP, pages 413?416.K.
Murphy.
2002.
Dynamic Bayesian Networks, Represen-tation, Inference, and Learning.
Ph.D. thesis, MIT, Dept.Computer Science.R.
Neal and G. Hinton.
1998.
A view of the EM algorithm thatjustifies incremental, sparse, and other variants.
In Learningin Graphical Models, pages 355?368.
Dordrecht: KluwerAcademic Publishers.R.
Pieraccini and E. Levin.
1991.
Stochastic representation ofsemantic structure for speech understanding.
In Eurospeech,volume 2, pages 383?386.N.
Reithinger and M. Klesen.
1997.
Dialogue act classificationusing language models.
In Eurospeech.N.
Reithinger et al 1996.
Predicting dialogue acts for a speech-to-speech translation system.
In ICLSP, pages 654?657.J.
Searle.
1969.
Speech Acts: An Essay in the Philosophy ofLanguage.
Cambridge University Press.E.
Shriberg et al 1998.
Can prosody aid the automatic classi-fication of dialog acts in conversational speech?
Languageand Speech, 41(3?4):439?487.E.
Shriberg et al 2004.
The ICSI meeting recorder dialog act(MRDA) corpus.
In Proc.
of the 5th SIGdial Workshop onDiscourse and Dialogue, pages 97?100.A.
Stolcke et al 1998.
Dialog act modeling for conversa-tional speech.
In Proc.
AAAI Spring Symp.
on Appl.
MachineLearning to Discourse Processing, pages 98?105.A.
Stolcke.
2002.
SRILM ?
an extensible language modelingtoolkit.
In ICLSP, volume 2, pages 901?904.C.
Sutton et al 2004.
Dynamic conditional random fields: fac-torized probabilistic models for labeling and segmenting se-quence data.
In ICML.D.
Talkin.
1995.
A robust algorithm for pitch tracking (rapt).In W. B. Kleijn and K.K.
Paliwal, editors, Speech Codingand Synthesis, pages 495?518.
Elsevier Science.A.
Viterbi.
1967.
Error bounds for convolutional codes and anasymptotically optimum decoding algorithm.
IEEE Trans.on Information Theory, 13(2):260?269.287
