tRuEcasIngLucian Vlad Lita ?Carnegie Mellonllita@cs.cmu.eduAbe IttycheriahIBM T.J. Watsonabei@us.ibm.comSalim RoukosIBM T.J. Watsonroukos@us.ibm.comNanda KambhatlaIBM T.J. Watsonnanda@us.ibm.comAbstractTruecasing is the process of restoringcase information to badly-cased or non-cased text.
This paper explores truecas-ing issues and proposes a statistical, lan-guage modeling based truecaser whichachieves an accuracy of ?98% on newsarticles.
Task based evaluation shows a26% F-measure improvement in namedentity recognition when using truecasing.In the context of automatic content ex-traction, mention detection on automaticspeech recognition text is also improvedby a factor of 8.
Truecasing also en-hances machine translation output legibil-ity and yields a BLEU score improvementof 80.2%.
This paper argues for the use oftruecasing as a valuable component in textprocessing applications.1 IntroductionWhile it is true that large, high quality text corporaare becoming a reality, it is also true that the digitalworld is flooded with enormous collections of lowquality natural language text.
Transcripts from var-ious audio sources, automatic speech recognition,optical character recognition, online messaging andgaming, email, and the web are just a few exam-ples of raw text sources with content often producedin a hurry, containing misspellings, insertions, dele-tions, grammatical errors, neologisms, jargon terms?
Work done at IBM TJ Watson Research Centeretc.
We want to enhance the quality of such sourcesin order to produce better rule-based systems andsharper statistical models.This paper focuses on truecasing, which is theprocess of restoring case information to raw text.Besides text rEaDaBILiTY, truecasing enhances thequality of case-carrying data, brings into the pic-ture new corpora originally considered too noisy forvarious NLP tasks, and performs case normalizationacross styles, sources, and genres.Consider the following mildly ambiguous sen-tence ?us rep. james pond showed up riding an itand going to a now meeting?.
The case-carrying al-ternative ?US Rep. James Pond showed up riding anIT and going to a NOW meeting?
is arguably betterfit to be subjected to further processing.Broadcast news transcripts contain casing errorswhich reduce the performance of tasks such asnamed entity tagging.
Automatic speech recognitionproduces non-cased text.
Headlines, teasers, sectionheaders - which carry high information content - arenot properly cased for tasks such as question answer-ing.
Truecasing is an essential step in transformingthese types of data into cleaner sources to be used byNLP applications.
?the president?
and ?the President?
are two viablesurface forms that correctly convey the same infor-mation in the same context.
Such discrepancies areusually due to differences in news source, authors,and stylistic choices.
Truecasing can be used as anormalization tool across corpora in order to pro-duce consistent, context sensitive, case information;it consistently reduces expressions to their statisticalcanonical form.In this paper, we attempt to show the benefits oftruecasing in general as a valuable building blockfor NLP applications rather than promoting a spe-cific implementation.
We explore several truecasingissues and propose a statistical, language modelingbased truecaser, showing its performance on newsarticles.
Then, we present a straight forward appli-cation of truecasing on machine translation output.Finally, we demonstrate the considerable benefits oftruecasing through task based evaluations on namedentity tagging and automatic content extraction.1.1 Related WorkTruecasing can be viewed in a lexical ambiguity res-olution framework (Yarowsky, 1994) as discriminat-ing among several versions of a word, which hap-pen to have different surface forms (casings).
Word-sense disambiguation is a broad scope problem thathas been tackled with fairly good results generallydue to the fact that context is a very good pre-dictor when choosing the sense of a word.
(Galeet al, 1994) mention good results on limited caserestoration experiments on toy problems with 100words.
They also observe that real world problemsgenerally exhibit around 90% case restoration accu-racy.
(Mikheev, 1999) also approaches casing dis-ambiguation but models only instances when capi-talization is expected: first word in a sentence, aftera period, and after quotes.
(Chieu and Ng, 2002)attempted to extract named entities from non-casedtext by using a weaker classifier but without focus-ing on regular text or case restoration.Accents can be viewed as additional surface formsor alternate word casings.
From this perspective, ei-ther accent identification can be extended to truecas-ing or truecasing can be extended to incorporate ac-cent restoration.
(Yarowsky, 1994) reports good re-sults with statistical methods for Spanish and Frenchaccent restoration.Truecasing is also a specialized method forspelling correction by relaxing the notion of casingto spelling variations.
There is a vast literature onspelling correction (Jones and Martin, 1997; Gold-ing and Roth, 1996) using both linguistic and statis-tical approaches.
Also, (Brill and Moore, 2000) ap-ply a noisy channel model, based on generic stringto string edits, to spelling correction.2 ApproachIn this paper we take a statistical approach to true-casing.
First we present the baseline: a simple,straight forward unigram model which performs rea-sonably well in most cases.
Then, we propose a bet-ter, more flexible statistical truecaser based on lan-guage modeling.From a truecasing perspective we observe fourgeneral classes of words: all lowercase (LC), firstletter uppercase (UC), all letters uppercase (CA), andmixed case word MC).
The MC class could be fur-ther refined into meaningful subclasses but for thepurpose of this paper it is sufficient to correctly iden-tify specific true MC forms for each MC instance.We are interested in correctly assigning case la-bels to words (tokens) in natural language text.
Thisrepresents the ability to discriminate between classlabels for the same lexical item, taking into accountthe surrounding words.
We are interested in casingword combinations observed during training as wellas new phrases.
The model requires the ability togeneralize in order to recognize that even though thepossibly misspelled token ?lenon?
has never beenseen before, words in the same context usually takethe UC form.2.1 Baseline: The Unigram ModelThe goal of this paper is to show the benefits of true-casing in general.
The unigram baseline (presentedbelow) is introduced in order to put task based eval-uations in perspective and not to be used as a straw-man baseline.The vast majority of vocabulary items have onlyone surface form.
Hence, it is only natural to adoptthe unigram model as a baseline for truecasing.
Inmost situations, the unigram model is a simple andefficient model for surface form restoration.
Thismethod associates with each surface form a scorebased on the frequency of occurrence.
The decodingis very simple: the true case of a token is predictedby the most likely case of that token.The unigram model?s upper bound on truecasingperformance is given by the percentage of tokensthat occur during decoding under their most frequentcase.
Approximately 12% of the vocabulary itemshave been observed under more than one surfaceform.
Hence it is inevitable for the unigram modelto fail on tokens such as ?new?.
Due to the over-whelming frequency of its LC form, ?new?
will takethis particular form regardless of what token followsit.
For both ?information?
and ?york?
as subsequentwords, ?new?
will be labeled as LC.
For the lattercase, ?new?
occurs under one of its less frequent sur-face forms.2.2 TruecaserThe truecasing strategy that we are proposing seeksto capture local context and bootstrap it across asentence.
The case of a token will depend on themost likely meaning of the sentence - where localmeaning is approximated by n-grams observed dur-ing training.
However, the local context of a fewwords alone is not enough for case disambiguation.Our proposed method employs sentence level con-text as well.We capture local context through a trigram lan-guage model, but the case label is decided at a sen-tence level.
A reasonable improvement over the un-igram model would have been to decide the wordcasing given the previous two lexical items and theircorresponding case content.
However, this greedyapproach still disregards global cues.
Our goal isto maximize the probability of a larger text segment(i.e.
a sentence) occurring under a certain surfaceform.
Towards this goal, we first build a languagemodel that can provide local context statistics.2.2.1 Building a Language ModelLanguage modeling provides features for a label-ing scheme.
These features are based on the prob-ability of a lexical item and a case content condi-tioned on the history of previous two lexical itemsand their corresponding case content:Pmodel(w3|w2, w1) = ?trigramP (w3|w2, w1)+ ?bigramP (w3|w2)+ ?unigramP (w3)+ ?uniformP0 (1)where trigram, bigram, unigram, and uniform prob-abilities are scaled by individual ?is which arelearned by observing training examples.
wi repre-sents a word with a case tag treated as a unit forprobability estimation.2.2.2 Sentence Level DecodingUsing the language model probabilities we de-code the case information at a sentence level.
Weconstruct a trellis (figure 1) which incorporates allthe sentence surface forms as well as the featurescomputed during training.
A node in this trellis con-sists of a lexical item, a position in the sentence, apossible casing, as well as a history of the previoustwo lexical items and their corresponding case con-tent.
Hence, for each token, all surface forms willappear as nodes carrying additional context infor-mation.
In the trellis, thicker arrows indicate highertransition probabilities.Figure 1: Given individual histories, the decodingsdelay and DeLay, are most probable - perhaps in thecontext of ?time delay?
and respectively ?SenatorTom DeLay?The trellis can be viewed as a Hidden MarkovModel (HMM) computing the state sequencewhich best explains the observations.
The states(q1, q2, ?
?
?
, qn) of the HMM are combinations ofcase and context information, the transition proba-bilities are the language model (?)
based features,and the observations (O1O2 ?
?
?Ot) are lexical items.During decoding, the Viterbi algorithm (Rabiner,1989) is used to compute the highest probabilitystate sequence (q??
at sentence level) that yields thedesired case information:q??
= argmaxqi1qi2??
?qitP (qi1qi2 ?
?
?
qit|O1O2 ?
?
?Ot, ?
)(2)where P (qi1qi2 ?
?
?
qit|O1O2 ?
?
?Ot, ?)
is the proba-bility of a given sequence conditioned on the obser-vation sequence and the model parameters.
A moresophisticated approach could be envisioned, whereeither the observations or the states are more expres-sive.
These alternate design choices are not exploredin this paper.Testing speed depends on the width and length ofthe trellis and the overall decoding complexity is:Cdecoding = O(SMH+1) where S is the sentencesize, M is the number of surface forms we are will-ing to consider for each word, and H is the historysize (H = 3 in the trigram case).2.3 Unknown WordsIn order for truecasing to be generalizable it mustdeal with unknown words ?
words not seen duringtraining.
For large training sets, an extreme assump-tion is that most words and corresponding casingspossible in a language have been observed duringtraining.
Hence, most new tokens seen during de-coding are going to be either proper nouns or mis-spellings.
The simplest strategy is to consider allunknown words as being of the UC form (i.e.
peo-ple?s names, places, organizations).Another approach is to replace the less frequentvocabulary items with case-carrying special tokens.During training, the word mispeling is replaced withby UNKNOWN LC and the word Lenon with UN-KNOWN UC.
This transformation is based on theobservation that similar types of infrequent wordswill occur during decoding.
This transformation cre-ates the precedent of unknown words of a particularformat being observed in a certain context.
When atruly unknown word will be seen in the same con-text, the most appropriate casing will be applied.This was the method used in our experiments.
Asimilar method is to apply the case-carrying specialtoken transformation only to a small random sam-ple of all tokens, thus capturing context regardlessof frequency of occurrence.2.4 Mixed CasingA reasonable truecasing strategy is to focus on to-ken classification into three categories: LC, UC, andCA.
In most text corpora mixed case tokens such asMcCartney, CoOl, and TheBeatles occur with mod-erate frequency.
Some NLP tasks might prefer map-ping MC tokens starting with an uppercase letter intothe UC surface form.
This technique will reduce thefeature space and allow for sharper models.
How-ever, the decoding process can be generalized to in-clude mixed cases in order to find a closer fit to thetrue sentence.
In a clean version of the AQUAINT(ARDA) news stories corpus, ?
90% of the tokensoccurred under the most frequent surface form (fig-ure 2).Figure 2: News domain casing distributionThe expensive brute force approach will considerall possible casings of a word.
Even with the fullcasing space covered, some mixed cases will not beseen during training and the language model prob-abilities for n-grams containing certain words willback off to an unknown word strategy.
A more fea-sible method is to account only for the mixed caseitems observed during training, relying on a largeenough training corpus.
A variable beam decod-ing will assign non-zero probabilities to all knowncasings of each word.
An n-best approximation issomewhat faster and easier to implement and is theapproach employed in our experiments.
During thesentence-level decoding only the n-most-frequentmixed casings seen during training are considered.If the true capitalization is not among these n-bestversions, the decoding is not correct.
Additional lex-ical and morphological features might be needed ifidentifying MC instances is critical.2.5 First Word in the SentenceThe first word in a sentence is generally under theUC form.
This sentence-begin indicator is some-times ambiguous even when paired with sentence-end indicators such as the period.
While sentencesplitting is not within the scope of this paper, wewant to emphasize the fact that many NLP taskswould benefit from knowing the true case of the firstword in the sentence, thus avoiding having to learnthe fact that beginning of sentences are artificiallyimportant.
Since it is uneventful to convert the firstletter of a sentence to uppercase, a more interest-ing problem from a truecasing perspective is to learnhow to predict the correct case of the first word in asentence (i.e.
not always UC).If the language model is built on clean sentencesaccounting for sentence boundaries, the decodingwill most likely uppercase the first letter of any sen-tence.
On the other hand, if the language modelis trained on clean sentences disregarding sentenceboundaries, the model will be less accurate since dif-ferent casings will be presented for the same contextand artificial n-grams will be seen when transition-ing between sentences.
One way to obtain the de-sired effect is to discard the first n tokens in the train-ing sentences in order to escape the sentence-begineffect.
The language model is then built on smoothercontext.
A similar effect can be obtained by initial-izing the decoding with n-gram state probabilities sothat the boundary information is masked.3 EvaluationBoth the unigram model and the language modelbased truecaser were trained on the AQUAINT(ARDA) and TREC (NIST) corpora, each consist-ing of 500M token news stories from various newsagencies.
The truecaser was built using IBM?sViaVoiceTMlanguage modeling tools.
These toolsimplement trigram language models using deletedinterpolation for backing off if the trigram is notfound in the training data.
The resulting model?sperplexity is 108.Since there is no absolute truth when truecasing asentence, the experiments need to be built with somereference in mind.
Our assumption is that profes-sionally written news articles are very close to anintangible absolute truth in terms of casing.
Fur-thermore, we ignore the impact of diverging stylisticforms, assuming the differences are minor.Based on the above assumptions we judge thetruecasing methods on four different test sets.
Thefirst test set (APR) consists of the August 25,2002 ?
top 20 news stories from Associated Pressand Reuters excluding titles, headlines, and sec-tion headers which together form the second test set(APR+).
The third test set (ACE) consists of ear-?Randomly chosen test dateFigure 3: LM truecaser vs. unigram baseline.lier news stories from AP and New York Times be-longing to the ACE dataset.
The last test set (MT)includes a set of machine translation references (i.e.human translations) of news articles from the Xin-hua agency.
The sizes of the data sets are as follows:APR - 12k tokens, ACE - 90k tokens, and MT - 63ktokens.
For both truecasing methods, we computedthe agreement with the original news story consid-ered to be the ground truth.3.1 ResultsThe language model based truecaser consistentlydisplayed a significant error reduction in caserestoration over the unigram model (figure 3).
Oncurrent news stories, the truecaser agreement withthe original articles is ?
98%.Titles and headlines usually have a higher con-centration of named entities than normal text.
Thisalso means that they need a more complex model toassign case information more accurately.
The LMbased truecaser performs better in this environmentwhile the unigram model misses named entity com-ponents which happen to have a less frequent surfaceform.3.2 Qualitative AnalysisThe original reference articles are assumed to havethe absolute true form.
However, differences fromthese original articles and the truecased articles arenot always casing errors.
The truecaser tends tomodify the first word in a quotation if it is notproper name: ?There has been?
becomes ?there hasbeen?.
It also makes changes which could be con-sidered a correction of the original article: ?XinhuaBLEU BreakdownSystem BLEU 1gr Precision 2gr Precision 3gr Precision 4gr Precisionall lowercase 0.1306 0.6016 0.2294 0.1040 0.0528rule based 0.1466 0.6176 0.2479 0.1169 0.06271gr truecasing 0.2206 0.6948 0.3328 0.1722 0.09881gr truecasing+ 0.2261 0.6963 0.3372 0.1734 0.0997lm truecasing 0.2596 0.7102 0.3635 0.2066 0.1303lm truecasing+ 0.2642 0.7107 0.3667 0.2066 0.1302Table 1: BLEU score for several truecasing strategies.
(truecasing+ methods additionally employ the ?firstsentence letter uppercased?
rule adjustment).Baseline With TruecasingClass Recall Precision F Recall Precision FENAMEX 48.46 36.04 41.34 59.02 52.65 55.66 (+34.64%)NUMEX 64.61 72.02 68.11 70.37 79.51 74.66 (+9.62%)TIMEX 47.68 52.26 49.87 61.98 75.99 68.27 (+36.90%)Overall 52.50 44.84 48.37 62.01 60.42 61.20 (+26.52%)Table 2: Named Entity Recognition performance with truecasing and without (baseline).news agency?
becomes ?Xinhua News Agency?
and?northern alliance?
is truecased as ?Northern Al-liance?.
In more ambiguous cases both the originalversion and the truecased fragment represent differ-ent stylistic forms: ?prime minister Hekmatyar?
be-comes ?Prime Minister Hekmatyar?.There are also cases where the truecaser describedin this paper makes errors.
New movie names aresometimes miss-cased: ?my big fat greek wedding?or ?signs?.
In conducive contexts, person namesare correctly cased: ?DeLay said in?.
However, inambiguous, adverse contexts they are considered tobe common nouns: ?pond?
or ?to delay that?.
Un-seen organization names which make perfectly nor-mal phrases are erroneously cased as well: ?interna-tional security assistance force?.3.3 Application: Machine TranslationPost-ProcessingWe have applied truecasing as a post-processing stepto a state of the art machine translation system in or-der to improve readability.
For translation betweenChinese and English, or Japanese and English, thereis no transfer of case information.
In these situationsthe translation output has no case information and itis beneficial to apply truecasing as a post-processingstep.
This makes the output more legible and thesystem performance increases if case information isrequired.We have applied truecasing to Chinese-to-Englishtranslation output.
The data source consists of newsstories (2500 sentences) from the Xinhua NewsAgency.
The news stories are first translated, thensubjected to truecasing.
The translation output isevaluated with BLEU (Papineni et al, 2001), whichis a robust, language independent automatic ma-chine translation evaluation method.
BLEU scoresare highly correlated to human judges scores, pro-viding a way to perform frequent and accurate au-tomated evaluations.
BLEU uses a modified n-gramprecision metric and a weighting scheme that placesmore emphasis on longer n-grams.In table 1, both truecasing methods are applied tomachine translation output with and without upper-casing the first letter in each sentence.
The truecas-ing methods are compared against the all letters low-ercased version of the articles as well as against anexisting rule-based system which is aware of a lim-ited number of entity casings such as dates, cities,and countries.
The LM based truecaser is very ef-fective in increasing the readability of articles andcaptures an important aspect that the BLEU score issensitive to.
Truecasig the translation output yieldsBaseline With TruecasingSource Recall Precision F Recall Precision FBNEWS ASR 23 3 5 56 39 46 (+820.00%)BNEWS HUMAN 77 66 71 77 68 72 (+1.41%)XINHUA 76 71 73 79 72 75 (+2.74%)Table 3: Results of ACE mention detection with and without truecasing.an improvement ?
of 80.2% in BLEU score over theexisting rule base system.3.4 Task Based EvaluationCase restoration and normalization can be employedfor more complex tasks.
We have successfully lever-aged truecasing in improving named entity recogni-tion and automatic content extraction.3.4.1 Named Entity TaggingIn order to evaluate the effect of truecasing on ex-tracting named entity labels, we tested an existingnamed entity system on a test set that has signif-icant case mismatch to the training of the system.The base system is an HMM based tagger, similarto (Bikel et al, 1997).
The system has 31 semanticcategories which are extensions on the MUC cate-gories.
The tagger creates a lattice of decisions cor-responding to tokenized words in the input stream.When tagging a word wi in a sentence of wordsw0...wN , two possibilities.
If a tag begins:p(tN1 |wN1 )i = p(ti|ti?1, wi?1)p?
(wi|ti, wi?1)If a tag continues:p(tN1 |wN1 )i = p(wi|ti, wi?1)The ?
indicates that the distribution is formed fromwords that are the first words of entities.
The p?
dis-tribution predicts the probability of seeing that wordgiven the tag and the previous word instead of thetag and previous tag.
Each word has a set of fea-tures, some of which indicate the casing and embed-ded punctuation.
These models have several levelsof back-off when the exact trigram has not been seenin training.
A trellis spanning the 31 futures is builtfor each word in a sentence and the best path is de-rived using the Viterbi algorithm.
?Truecasing improves legibility, not the translation itselfThe performance of the system shown in table 2indicate an overall 26.52% F-measure improvementwhen using truecasing.
The alternative to truecas-ing text is to destroy case information in the train-ing material 	 SNORIFY procedure in (Bikel et al,1997).
Case is an important feature in detectingmost named entities but particularly so for the titleof a work, an organization, or an ambiguous wordwith two frequent cases.
Truecasing the sentence isessential in detecting that ?To Kill a Mockingbird?
isthe name of a book, especially if the quotation marksare left off.3.4.2 Automatic Content ExtractionAutomatic Content Extraction (ACE) is task fo-cusing on the extraction of mentions of entities andrelations between them from textual data.
The tex-tual documents are from newswire, broadcast newswith text derived from automatic speech recognition(ASR), and newspaper with text derived from opticalcharacter recognition (OCR) sources.
The mentiondetection task (ace, 2001) comprises the extractionof named (e.g.
?Mr.
Isaac Asimov?
), nominal (e.g.
?the complete author?
), and pronominal (e.g.
?him?
)mentions of Persons, Organizations, Locations, Fa-cilities, and Geo-Political Entities.The automatically transcribed (using ASR) broad-cast news documents and the translated XinhuaNews Agency (XINHUA) documents in the ACEcorpus do not contain any case information, whilehuman transcribed broadcast news documents con-tain casing errors (e.g.
?George bush?).
This prob-lem occurs especially when the data source is noisyor the articles are poorly written.For all documents from broadcast news (humantranscribed and automatically transcribed) and XIN-HUA sources, we extracted mentions before and af-ter applying truecasing.
The ASR transcribed broad-cast news data comprised 86 documents containinga total of 15,535 words, the human transcribed ver-sion contained 15,131 words.
There were only twoXINHUA documents in the ACE test set containinga total of 601 words.
None of this data or any ACEdata was used for training the truecasing models.Table 3 shows the result of running our ACE par-ticipating maximum entropy mention detection sys-tem on the raw text, as well as on truecased text.
ForASR transcribed documents, we obtained an eightfold improvement in mention detection from 5% F-measure to 46% F-measure.
The low baseline scoreis mostly due to the fact that our system has beentrained on newswire stories available from previousACE evaluations, while the latest test data includedASR output.
It is very likely that the improvementdue to truecasing will be more modest for the nextACE evaluation when our system will be trained onASR output as well.4 Possible Improvements & Future WorkAlthough the statistical model we have consideredperforms very well, further improvements must gobeyond language modeling, enhancing how expres-sive the model is.
Additional features are neededduring decoding to capture context outside of thecurrent lexical item, medium range context, as wellas discontinuous context.
Another potentially help-ful feature to consider would provide a distribu-tion over similar lexical items, perhaps using anedit/phonetic distance.Truecasing can be extended to cover a more gen-eral notion surface form to include accents.
De-pending on the context, words might take differentsurface forms.
Since punctuation is a notion exten-sion to surface form, shallow punctuation restora-tion (e.g.
word followed by comma) can also be ad-dressed through truecasing.5 ConclusionsWe have discussed truecasing, the process of restor-ing case information to badly-cased or non-casedtext, and we have proposed a statistical, languagemodeling based truecaser which has an agreementof ?98% with professionally written news articles.Although its most direct impact is improving legibil-ity, truecasing is useful in case normalization acrossstyles, genres, and sources.
Truecasing is a valu-able component in further natural language process-ing.
Task based evaluation shows a 26% F-measureimprovement in named entity recognition when us-ing truecasing.
In the context of automatic contentextraction, mention detection on automatic speechrecognition text is improved by a factor of 8.
True-casing also enhances machine translation output leg-ibility and yields a BLEU score improvement of80.2% over the original system.References2001.
Entity detection and tracking.
ACE Pilot StudyTask Definition.D.
Bikel, S. Miller, R. Schwartz, and R. Weischedel.1997.
Nymble: A high-performance learning namefinder.
pages 194?201.E.
Brill and R. C. Moore.
2000.
An improved errormodel for noisy channel spelling correction.
ACL.H.L.
Chieu and H.T.
Ng.
2002.
Teaching a weaker clas-sifier: Named entity recognition on upper case text.William A. Gale, Kenneth W. Church, and DavidYarowsky.
1994.
Discrimination decisions for100,000-dimensional spaces.
Current Issues in Com-putational Linguistics, pages 429?450.Andrew R. Golding and Dan Roth.
1996.
Applying win-now to context-sensitive spelling correction.
ICML.M.
P. Jones and J. H. Martin.
1997.
Contextual spellingcorrection using latent semantic analysis.
ANLP.A.
Mikheev.
1999.
A knowledge-free method for capi-talized word disambiguation.Kishore Papineni, Salim Roukos, Todd Ward, andWei Jing Zhu.
2001.
Bleu: a method for automaticevaluation of machine translation.
IBM Research Re-port.L.
R. Rabiner.
1989.
A tutorial on hidden markov modelsand selected applications in speech recognition.
Read-ings in Speech Recognition, pages 267?295.David Yarowsky.
1994.
Decision lists for ambiguity res-olution: Application to accent restoration in spanishand french.
ACL, pages 88?95.
