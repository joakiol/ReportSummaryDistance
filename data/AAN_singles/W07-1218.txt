Proceedings of the 5th Workshop on Important Unresolved Matters, pages 136?143,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsValidation and Regression Testing for a Cross-linguisic Grammar ResourceEmily M. Bender, Laurie Poulson, Scott Drellishak, Chris EvansUniversity of WashingtonDepartment of LinguisticsSeattle WA 98195-4340 USA{ebender,lpoulson,sfd,chrisev@u.washington.edu}AbstractWe present a validation methodology fora cross-linguistic grammar resource whichproduces output in the form of small gram-mars based on elicited typological descrip-tions.
Evaluating the resource entails sam-pling from a very large space of languagetypes, the type and range of which precludethe use of standard test suites developmenttechniques.
We produce a database fromwhich gold standard test suites for thesegrammars can be generated on demand, in-cluding well-formed strings paired with allof their valid semantic representations aswell as a sample of ill-formed strings.
Thesestring-semantics pairs are selected from aset of candidates by a system of regular-expression based filters.
The filters amountto an alternative grammar building system,whose generative capacity is limited com-pared to the actual grammars.
We performerror analysis of the discrepancies betweenthe test suites and grammars for a range oflanguage types, and update both systems ap-propriately.
The resulting resource serves asa point of comparison for regression testingin future development.1 IntroductionThe development and maintenance of test suites isintegral to the process of writing deep linguisticgrammars (Oepen and Flickinger, 1998; Butt andKing, 2003).
Such test suites typically contain hand-constructed examples illustrating the grammaticalphenomena treated by the grammar as well as rep-resentative examples taken from texts from the tar-get domain.
In combination with test suite manage-ment software such as [incr tsdb()] (Oepen, 2002),they are used for validation and regression testing ofprecision (deep linguistic) grammars as well as theexploration of potential changes to the grammar.In this paper, we consider what happens when theprecision grammar resource being developed isn?t agrammar of a particular language, but rather a cross-linguistic grammar resource.
In particular, we con-sider the LinGO Grammar Matrix (Bender et al,2002; Bender and Flickinger, 2005).
There are sev-eral (related) obstacles to making effective use oftest suites in this scenario: (1) The Matrix coregrammar isn?t itself a grammar, and therefore can?tparse any strings.
(2) There is no single languagemodeled by the cross-linguistic resource from whichto draw test strings.
(3) The space of possible gram-mars (alternatively, language types) modeled by theresource is enormous, well beyond the scope of whatcan be thoroughly explored.We present a methodology for the validation andregression testing of the Grammar Matrix that ad-dresses these obstacles, developing the ideas origi-nally proposed in (Poulson, 2006).
In its broad out-lines, our methodology looks like this:?
Define an abstract vocabulary to be used for testsuite purposes.?
Define an initial small set of string-semanticspairs.?
Construct a large set of variations on the string-semantics pairs.136?
Define a set of filters that can delineate the le-gitimate string-semantics pairs for a particularlanguage typeThe filters in effect constitute a parallel grammardefinition system, albeit one that creates ?grammars?of very limited generative capacity.
As such, the out-put of the filters cannot be taken as ground truth.Rather, it serves as a point of comparison that al-lows us to find discrepancies between the filters andthe Grammar Matrix which in turn can lead us toerrors in the Grammar Matrix.2 BackgroundThe Grammar Matrix is an open-source starter kitdesigned to jump-start the development of broad-coverage precision grammars, capable of both pars-ing and generation and suitable for use in a vari-ety of NLP applications.
The Grammar Matrix iswritten within the HPSG framework (Pollard andSag, 1994), using Minimal Recursion Semantics(Copestake et al, 2005) for the semantic represen-tations.
The particular formalism we use is TDL(type description language) as interpreted by theLKB (Copestake, 2002) grammar development en-vironment.
Initial work on the Matrix (Bender etal., 2002; Flickinger and Bender, 2003) focused onthe development of a cross-linguistic core grammar.The core grammar provides a solid foundation forsustained development of linguistically-motivatedyet computationally tractable grammars (e.g., (Hel-lan and Haugereid, 2003; Kordoni and Neu, 2005)).However, the core grammar alone cannot parseand generate sentences: it needs to be specializedwith language-specific information such as the or-der of daughters in its rules (e.g., head-subject orsubject-head), and it needs a lexicon.
Althoughword order and many other phenomena vary acrosslanguages, there are still recurring patterns.
To al-low reuse of grammar code across languages and toincrease the size of the jump-start provided by theMatrix, in more recent work (Bender and Flickinger,2005; Drellishak and Bender, 2005), we have beendeveloping ?libraries?
implementing realizations ofvarious linguistic phenomena.
Through a web in-terface, grammar developers can configure an initialstarter grammar by filling out a typological question-naire about their language, which in turn calls a CGIscript to ?compile?
a grammar (including language-specific rule types, lexical entry types, rule entries,and lexical entries) by making appropriate selectionsfrom the libraries.
These little grammars describevery small fragments of the languages they model,but they are not toys.
Their purpose is to be goodstarting points for further development.The initial set of libraries includes: basic word or-der of major constituents in matrix clauses (SOV etal), optionality/obligatoriness of determiners, noun-determiner order, NP v. PP arguments of intransitiveand transitive verbs, strategies for expressing senten-tial negation and yes-no questions, and strategies forconstituent coordination.
Even with this small set ofphenomena covered (and limiting ourselves for test-ing purposes to maximally two coordination strate-gies per language), we have already defined a spaceof hundreds of thousands of possible grammars.13 The Non-modularity of LinguisticPhenomenaIn this section we discuss our findings so far aboutthe non-modularity of linguistic phenomena, and ar-gue that this makes the testing of a broad sample ofgrammars even more pressing.The Grammar Matrix customization system readsin the user?s language specification and then outputslanguage-specific definitions of types (rule types,lexical entry types and ancillary structures) that in-herit from types defined in the crosslinguistic coreof the Matrix but add constraints appropriate for thelanguage at hand.
Usability considerations put twoimportant constraints on this system: (1) The ques-tions must be ones that are sensible to linguists, whotend to consider phenomena one at a time.
(2) Theoutput grammar code must be both readable andmaintainable.
To achieve readable grammar codein the output TDL, among other things, we followthe guideline that any given constraint is stated onlyonce.
If multiple types require the same constraint,they should all inherit from some supertype bearingthat constraint.
In addition, all constraints pertainingto a particular type are stated in one place.In light of the these usability considerations, we1If all of the choices in the customization system were in-dependent, we would have more than 2 x 1027 grammars.
Inactuality, constraints on possible combinations of choices limitthis space considerably.137comp-head-phrase := basic-head-1st-comp-phrase & head-final.subj-head-phrase := basic-head-subj-phrase & head-final &[ HEAD-DTR.SYNSEM.LOCAL.CAT.VAL.COMPS < > ].Figure 1: Specialized phrase structure rule types for SOV languagehave found that it is not possible to treat the li-braries as black-box modules with respect to eachother.
The libraries are interdependent, and the por-tions of the script that interpret one part of the inputquestionnaire frequently need to make reference toinformation elicited by other parts of the question-naire.
For example, the customization system imple-ments major constituent word order by specializingthe head-complement and head-subject rule typesprovided in the core grammar.
In an SOV language,these would both be cross-classified with the typehead-final, and the head-subject rule would furtherbe constrained to take only complement-saturatedphrases as its head daughter.
The TDL encoding ofthese constraints is shown in Figure 1.Following standard practice in HPSG, we use thehead-complement phrase not only for ordinary VPs,but also for PPs, CPs, and auxiliary-headed VPs,etc.
Consider Polish, a free word order language thatnonetheless has prepositions.
To allow complementson either side of the head, we instantiate both head-comp and comp-head rules, inheriting from head-initial and head-final respectively.
Yet the preposi-tions must be barred from the head-final version lestthe grammar license postpositional phrases by mis-take.
We do this by constraining the HEAD value ofthe comp-head phrase.
Similarly, question particles(such as est-ce que in French or ma in Mandarin)are treated as complementizers: heads that select foran S complement.
Since these, too, may differ intheir word order properties from verbs (and preposi-tions), we need information about the question par-ticles (elicited with the rest of the information aboutyes-no questions) before we have complete informa-tion about the head-complement rule.
Furthermore,it is not simply a question of adding constraints toexisting types.
Consider the case of an SOV lan-guage with prepositions and sentence-initial ques-tion particles.
This language would need a head-initial head-comp rule that can take only preposi-tions and complementizers as its head.
To expressthe disjunction, we must use the supertype to prepand comp.
This, in turn, means that we can?t decidewhat constraint to put on the head value of the head-comp rule until we?ve considered questions as wellas the basic word order facts.We expect to study the issue of (non-)modularityas we add additional libraries to the resource and toinvestigate whether the grammar code can be refac-tored in such a way as to make the libraries into truemodules.
We suspect it might be possible to reducethe degree of interdependence, but not to achievecompletely independent libraries, because syntacticphenomena are inherently interdependent.
Agree-ment in NP coordination provides an example.
InEnglish and many other languages, coordinated NPsare always plural and the person of the coordinatedNP is the minimal person value of the coordinands.
(1) a.
A cat and a dog are/*is chasing a mouse.b.
Kim and I should handle this ourselves.c.
You and Kim should handle this yourselves.Gender systems often display a similar hierarchy ofvalues, as with French coordinated NPs, where thewhole NP is feminine iff all coordinands are femi-nine and masculine otherwise.
Thus it appears thatit is not possible to define all of the necessary con-straints on the coordination rules without having ac-cess to information about the agreement system.Even if we were able to make our analyses ofdifferent linguistic phenomena completely modular,however, we would still need to test their interactionin the analysis of particular sentences.
Any sentencethat illustrates sentential negation, a matrix yes-noquestion, or coordination also necessarily illustratesat least some aspects of word order, the presencev.
absence of determiners and case-marking adpo-sitions, and the subcategorization of the verb thatheads the sentence.
Furthermore, broad-coveragegrammars need to allow negation, questions, coor-dination etc.
all to appear in the same sentence.Given this non-modularity, we would ideally liketo be able to validate (and do regression testing on)the full set of grammars generable by the customiza-138Form Description Optionsdet determinern1, n2 nouns det is optional, obligatory, impossibleiv, tv intransitive, transitive verb subj, obj are NP or PPp-nom, p-acc case-marking adpositions preposition or postpositionneg negative element adverb, prefix, suffixco1, co2 coordination marks word, prefix, suffixqpart question particleTable 1: Standardized lexicontion system.
To approximate such thoroughness, weinstead sample from the grammar space.4 MethodologyThis section describes in some detail our methodol-ogy for creating test suites on the basis of language-type descriptions.
A language type is a collectionof feature-value pairs representing a possible setof answers to the Matrix customization question-naire.
We refer to these as language types ratherthan languages, because the grammars produced bythe customization system are underspecified with re-spect to actual languages, i.e., one and the samestarter grammar might be extended into multiplemodels corresponding to multiple actual human lan-guages.
Accordingly, when we talk about the pre-dicted (well)formedness, or (un)grammaticality, of acandidate string, we are referring to its predicted sta-tus with respect to a language type definition, not itsgrammaticality in any particular (human) language.4.1 Implementation: Python and MySQLThe test suite generation system includes a MySQLdatabase, a collection of Python scripts that interactwith the database, and some stored SQL queries.
Asthe set of items we are manipulating is quite large(and will grow as new items are added to test ad-ditional libraries), using a database is essential forrapid retrieval of relevant items.
Furthermore, thedatabase facilitates the separation of procedural anddeclarative knowledge in the definition of the filters.4.2 Abstract vocabulary for abstract stringsA grammar needs not just syntactic constructionsand lexical types, but also an actual lexicon.
Sincewe are working at the level of language types, weare free to define this lexicon in whatever way ismost convenient.
Much of the idiosyncrasy in lan-guage resides in the lexicon, both in the form of mor-phemes and in the particular grammatical and collo-cational constraints associated with them.
Of thesethree, only the grammatical constraints are manip-ulated in any interesting way within the GrammarMatrix customization system.
Therefore, for the testsuite, we define all of the language types to draw theforms of their lexical items from a shared, standard-ized vocabulary.
Table 1 illustrates the vocabularyalong with the options that are currently availablefor varying the grammatical constraints on the lex-ical entries.
Using the same word forms for eachgrammar contributes substantially to building a sin-gle resource that can be adapted for the testing ofeach language type.4.3 Constructing master item setWe use string to refer to a sequence of words tobe input to a grammar and result as the (expected)semantic representation.
An item is a particularpair of string and result.
Among strings, we haveseed strings provided by the Matrix developers toseed the test suite, and constructed strings derivedfrom those seed strings.
The constructor functionis the algorithm for deriving new strings from theseed strings.
Seed strings are arranged into seman-tic equivalence classes, from which one representa-tive is designated the harvester string.
We parse theharvester string with some appropriate grammar (de-rived from the Matrix customization system) to ex-tract the semantic representation (result) to be pairedwith each member of the equivalence class.The seed strings, when looked at as bags of words,should cover all possible realizations of the phe-nomenon treated by the library.
For example, thenegation library allows both inflectional and adver-bial negation, as well as negation expressed throughboth inflection and an adverb together.
To illustrate139negation of transitive sentences (allowing for lan-guages with and without determiners2), we requirethe seed strings in (2):(2) Semtag: neg1 Semtag: neg2n1 n2 neg tv det n1 det n2 neg tvn1 n2 neg-tv det n1 det n2 neg-tvn1 n2 tv-neg det n1 det n2 tv-negn1 n2 neg neg-tv det n1 det n2 neg neg-tvn1 n2 neg tv-neg det n1 det n2 neg tv-negSentential negation has the same semantic reflexacross all of its realizations, but the presence v. ab-sence of overt determiners does have a semantic ef-fect.
Accordingly, the seed strings shown in (2) canbe grouped into two semantic equivalence classes,shown as the first and second columns in the table,and associated with the semantic tags ?neg1?
and?neg2?, respectively.
The two strings in the first rowcould be designated as the harvester strings, associ-ated with a grammar for an SOV language with op-tional determiners preceding the noun and sententialnegation expressed as a pre-head modifier of V.We use the LKB in conjunction with [incr tsdb()]to parse the harvester strings from all of the equiva-lence classes with the appropriate grammars.
Thenthe seed strings and the parsing results from the har-vester strings, as well as their semantic tags, arestored and linked in our relational database.
We usethe constructor function to create new strings fromthese seed strings.
This produces the master item setthat provides the basis for the test suites.Currently, we have only one constructor function(?permute?)
which takes in a seed string and returnsall unique permutations of the morphemes in thatseed string.3 This constructor function is effectivein producing test items that cover the range of wordorder variations currently permitted by the GrammarMatrix customization system.
Currently, most of theother kinds of variation countenanced (e.g., adver-bial v. inflectional negation or presence v. absenceof determiners) is handled through the initial seedstring construction.
As the range of phenomena han-dled by the customization system expands, we willdevelop more sophisticated constructor functions to2We require additional seed strings to account for languageswith and without case-marking adpositions3?permute?
strips off any affixes, permutes the stems, andthen attaches the affixes to the stems in all possible ways.handle, for example, the addition of all possible casesuffixes to each noun in the sentence.4.4 FiltersThe master item set provides us with an inventoryfrom which we can find positive (grammatical) ex-amples for any language type generated by the sys-tem as well as interesting negative examples for anylanguage type.
To do so, we filter the master itemset, in two steps.4.4.1 Universal FiltersThe first step is the application of ?universal?
fil-ters, which mark any item known to be ungrammat-ical across all language types currently produced bythe system.
For example, the word order library doesnot currently provide an analysis of radically non-configurational languages with discontinuous NPs(e.g., Warlpiri (Hale, 1981)).
Accordingly, (3) willbe ungrammatical across all language types:(3) det det n1 n2 tvThe universal filter definitions (provided by thedevelopers) each comprise one or more regular ex-pressions, a filter type that specifies how the regularexpressions are to be applied, and a list of seman-tic tags specifying which equivalence classes theyapply to.
For example, the filter that would catchexample (3) above is defined as in (4):(4) Filter Type: reject-unless-matchRegexp: (det (n1|n2).
*det (n1|n2))|(det (n1|n2).
*(n1|n2) det)|((n1|n2) det.
*det (n1|n2))|((n1|n2) det.
*(n1|n2) det)Sem-class: [semantic classes for all transitivesentences with two determiners.
]We apply each filter to every item in the database.For each filter whose semantic-class value includesthe semantic class of the item at hand, we store theresult (pass or fail) of the filter on that item.
We canthen query the database to produce a list of all of thepotentially well-formed items.4.4.2 Specific FiltersThe next step is to run the filters that find thegrammatical examples for a particular languagetype.
In order to facilitate sampling of the entirelanguage space, we define these filters to be sensi-tive not to complete language type definitions, but140rather to particular features (or small sets of fea-tures) of a language type.
Thus in addition to thefilter type, regular expression, and semantic classfields, the language-specific filters also encode par-tial descriptions of the language types to which theyapply, in the form of feature-value declarations.
Forexample, the filter in (5) plays a role in selectingthe correct form of negated sentences for languagetypes with both inflectional and adverbial negationin complementary distribution (like English n?t andsentential not).
The first regular expression checksfor neg surrounded by white space (i.e., the negativeadverb) and the second for the negative affixes.
(5) Filter Type: reject-if-both-matchRegexp1: (\s|?
)neg(\s|$)Regexp2: -neg|neg-Sem-class: [sem.
classes for all neg.
sent.
]Lg-feat: and(infl neg:on,adv neg:on,multineg:comp)This filter uses a conjunctive language feature spec-ification (three feature-value pairs that must apply),but disjunctions are also possible.
These specifica-tions are converted to disjunctive normal form be-fore further processing.As with the universal filters, the results of the spe-cific filters are stored in the database.
We processeach item that passed all of the universal filters witheach specific filter.
Whenever a filter?s semantic-class value matches the semantic-class of the itemat hand, we store the value assigned by the filter(pass or fail).
We also store the feature-value pairsrequired by each filter, so that we can look up therelevant filters for a language-type definition.4.4.3 Recursive Linguistic PhenomenaMaking the filters relative to particular semanticclasses allows us to use information about the lexi-cal items in the sentences in the definition of the fil-ters.
This makes it easier to write regular-expressionbased filters that can work across many differentcomplete language types.
Complications arise, how-ever, in examples illustrating recursive phenomenaTo handle such phenomena with our finite-state sys-tem, we do multiple passes with the filters.
All itemswith coordination are first processed with the co-ordination filters, and then rewritten to replace anywell-formed coordinations with single constituents.These rewritten strings are then processed with therest of the filters, and we store the results as the re-sults for those filters on the original strings.4.5 Language typesThe final kind of information we store in thedatabase is definitions of language types.
Eventhough our system allows us to create test suites fornew language types on demand, we still store thelanguage-type definitions of language types we havetested, for future regression testing purposes.
Whena language type is read in, the list of feature-valuepairs defining it is compared to the list of feature-groups declared by the filters.
For each group offeature-value pairs present in the language-type def-inition, we find all of the filters that use that group.We then query the database for all items that passthe filters relevant to the language type.
This listof items represents all those in the master item setpredicted to be well-formed for this language type.From the complement of this set, we also take a ran-dom selection of items to test for overgeneration.4.6 Validation of grammarsOnce we have created the test suite for a partic-ular language type, the developer runs the Matrixcustomization system to get a starter grammar forthe same language type.
The test suite is loadedinto [incr tsdb()] and processed with the grammar.
[incr tsdb()] allows the developer to compare thegrammar?s output with the test suite at varying lev-els of detail: Do all and only the items predicted tobe well-formed parse?
Do they get the same numberof readings as predicted?
Do they get the semanticrepresentations predicted?
A discrepancy at any ofthese levels points to an error in either the GrammarMatrix or the test suite generation system.
The de-veloper can query the database to find which filterspassed or failed a particular example as well as todiscover the provenance of the example and whichphenomena it is meant to test.This methodology provides the ability to gener-ate test suites for any arbitrary language type on de-mand.
Although this appears to eliminate the need tostore the test suites we do, in fact, store informationabout previous test suites.
This allows us to track theevolution of the Grammar Matrix in relation to thoseparticular language types over time.1414.7 Investment and ReturnThe input required from the developer in order to testany new library is as follows: (1) Seed strings illus-trating the range of expressions handled by the newlibrary, organized into equivalence classes.
(2) Des-ignated harvester strings for each equivalence classand a grammar or grammars that can parse them toget the target semantic representation.
(3) Universalfilters specific to the phenomenon and seed strings.
(4) Specific filters picking out the right items foreach language type.
(5) Analysis of discrepanciesbetween the test suite and the generated grammars.This is a substantial investment on the part of the de-veloper but we believe the investment is worth it forthe return of being able to validate a library additionand test for any loss of coverage going forward.Arnold et al (1994) note that writing grammarsto generate test suites is impractical if the test suitegenerating grammars aren?t substantially simpler towrite than the ?actual?
grammars being tested.
Eventhough this system requires some effort to maintain,we believe the methodology remains practical fortwo reasons.
First, the input required from the de-veloper enumerated above is closely related to theknowledge discovered in the course of building thelibraries in the first place.
Second, the fact that thefilters are sensitive to only particular features of lan-guage types means that a relatively small number offilters can create test suites for a very large numberof language types.5 Related WorkKinyon and Rambow (2003) present an approach togenerating test suites on the basis of descriptionsof languages.
The language descriptions are Meta-Grammar (MG) hierarchies.
Their approach appearsto be more flexible than the one presented here insome ways, and more constrained in others.
It doesnot need any input strings, but rather produces testitems from the language description.
In addition,it annotates the output in multiple ways, includingphrase structure, dependency structure, and LFG F-structure.
On the other hand, there is no apparentprovision for creating negative (ungrammatical) testdata and it is does not appear possible to composenew MG descriptions on the fly.
Furthermore, thefocus of the MG test suite work appears to be thegeneration of test suites for other grammar develop-ment projects, but the MGs themselves are crosslin-guistic resources in need of validation and testing.An interesting area for future work would be thecomparison between the test suites generated by thesystem described here and the MG test suites.The key to the test-suite development process pro-posed here is to leverage the work already beingdone by the Matrix developers into a largely auto-mated process for creating test-suite items.
The in-formation required from the developers is essentiallya structured and systematic version of the knowledgethat is required for the creation of libraries in the firstplace.
This basic approach, is also the basis for theapproach taken in (Bro?ker, 2000); the specific formsof knowledge leveraged, and the test-suite develop-ment strategies used, however, are quite different.6 Future WorkThe addition of the next library to the Grammar Ma-trix will provide us with an opportunity to try toquantify the effect of this methodology.
With theGrammar Matrix and the filters stabilized, the vali-dation of a new library can be carefully tracked.
Wecan try to quantify the number of errors obtained andthe source of the errors, e.g., library or filters.In addition to this kind of quantification and erroranalysis as a means of validating this methodology,we also intend to undertake a comparison of the testsuites created from our database to hand built cre-ated for Matrix-derived grammars by students in themultilingual grammar engineering course at the Uni-versity of Washington.4 Students in this class eachdevelop grammars for a different language, and cre-ate test suites of positive and negative examples aspart of their development process.
We plan to usethe lexical types in the grammars to define a map-ping from the surface lexical items used in the testsuites to our abstract vocabulary.
We can then com-pare the hand built and autogenerated test suites inorder to gauge the thoroughness of the system pre-sented here.7 ConclusionThe methodology outlined in this paper addressesthe three obstacles noted in the introduction: Al-4http://courses.washington.edu/ling567142though the Grammar Matrix core itself isn?t a gram-mar (1), we test it by deriving grammars from it.Since we are testing the derived grammars, we aresimultaneously testing both the Matrix core gram-mar, the libraries, and the customization script.
Al-though there is no single language being modeledfrom which to draw strings (2), we can nonethe-less find a relevant set of strings and associatethese strings with annotations of expected well-formedness.
The lexical formatives of the stringsare drawn from a standardized set of abstract forms.The well-formedness predictions are made on thebasis of the system of filters.
The system of filtersdoesn?t represent ground truth, but rather a secondpathway to the judgments in addition to the directuse of the Matrix-derived starter grammars.
Thesepathways are independent enough that the one canserve as an error check on the other.
The space ofpossible language types remains too large for thor-ough testing (3).
However, since our system allowsfor the efficient derivation of a test suite for any arbi-trary language type, it is inexpensive to sample thatlanguage-type space in many different ways.AcknowledgmentsThis work has been supported by NSF grant BCS-0644097.ReferencesDoug Arnold, Martin Rondell, and Frederik Fouvry.1994.
Design and implementation of test suite tools.Technical Report LRE 62-089 D-WP5, University ofEssex, UK.Emily M. Bender and Dan Flickinger.
2005.
Rapid pro-totyping of scalable grammars: Towards modularity inextensions to a language-independent core.
In Proc.IJCNLP-05 (Posters/Demos).Emily M. Bender, Dan Flickinger, and Stephan Oepen.2002.
The grammar matrix: An open-source starter-kit for the rapid development of cross-linguisticallyconsistent broad-coverage precision grammars.
InProc.
the Workshop on Grammar Engineering andEvaluation COLING 2002, pages 8?14.Norbert Bro?ker.
2000.
The use of instrumentation ingrammar engineering.
In Proc.
COLING 2000, pages118?124.Miriam Butt and Tracy Holloway King.
2003.
Gram-mar writing, testing, and evaluation.
In Handbook forLanguage Engineers, pages 129?179.
CSLI.Ann Copestake, Dan Flickinger, Carl Pollard, and Ivan A.Sag.
2005.
Minimal recursion semantics: An intro-duction.
Research on Language & Computation, 3(2?3):281?332.Ann Copestake.
2002.
Implementing Typed FeatureStructure Grammars.
CSLI.Scott Drellishak and Emily M. Bender.
2005.
A coordi-nation module for a crosslinguistic grammar resource.In Stefan Mu?ller, editor, The Proc.
HPSG 2005, pages108?128.
CSLI.Dan Flickinger and Emily M. Bender.
2003.
Compo-sitional semantics in a multilingual grammar resource.In Proc.
the Workshop on Ideas and Strategies for Mul-tilingual Grammar Development, ESSLLI 2003, pages33?42.Kenneth Hale.
1981.
On the position of Warlpiri in thetypology of the base.
Distributed by Indiana Univer-sity Linguistics Club, Bloomington.Lars Hellan and Petter Haugereid.
2003.
NorSource: Anexercise in Matrix grammar-building design.
In Proc.the Workshop on Ideas and Strategies for MultilingualGrammar Development, ESSLLI 2003, pages 41?48.Alexandra Kinyon and Owen Rambow.
2003.
The meta-grammar: A cross-framework and cross-language test-suite generation tool.
In Proc.
4th International Work-shop on Linguistically Interpreted Corpora.Valia Kordoni and Julia Neu.
2005.
Deep analysisof Modern Greek.
In Keh-Yih Su, Jun?ichi Tsujii,and Jong-Hyeok Lee, editors, Lecture Notes in Com-puter Science, volume 3248, pages 674?683.
Springer-Verlag.Stephan Oepen and Daniel P. Flickinger.
1998.
Towardssystematic grammar profiling.
Test suite technologyten years after.
Journal of Computer Speech and Lan-guage, 12 (4) (Special Issue on Evaluation):411 ?
436.Stephan Oepen.
2002.
Competence and PerformanceProfiling for Constraint-based Grammars: A NewMethodology, Toolkit, and Applications.
Ph.D. thesis,Universita?t des Saarlandes.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
The University of Chicago Press.Laurie Poulson.
2006.
Evaluating a cross-linguisticgrammar model: Methodology and gold-standard re-source development.
Master?s thesis, University ofWashington.143
