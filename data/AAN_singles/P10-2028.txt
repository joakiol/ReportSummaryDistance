Proceedings of the ACL 2010 Conference Short Papers, pages 151?155,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsUnsupervised Discourse Segmentationof Documents with Inherently Parallel StructureMinwoo Jeong and Ivan TitovSaarland UniversitySaarbru?cken, Germany{m.jeong|titov}@mmci.uni-saarland.deAbstractDocuments often have inherently parallelstructure: they may consist of a text andcommentaries, or an abstract and a body,or parts presenting alternative views onthe same problem.
Revealing relations be-tween the parts by jointly segmenting andpredicting links between the segments,would help to visualize such documentsand construct friendlier user interfaces.
Toaddress this problem, we propose an un-supervised Bayesian model for joint dis-course segmentation and alignment.
Weapply our method to the ?English as a sec-ond language?
podcast dataset where eachepisode is composed of two parallel parts:a story and an explanatory lecture.
Thepredicted topical links uncover hidden re-lations between the stories and the lec-tures.
In this domain, our method achievescompetitive results, rivaling those of a pre-viously proposed supervised technique.1 IntroductionMany documents consist of parts exhibiting a highdegree of parallelism: e.g., abstract and body ofacademic publications, summaries and detailednews stories, etc.
This is especially common withthe emergence of the Web 2.0 technologies: manytexts on the web are now accompanied with com-ments and discussions.
Segmentation of these par-allel parts into coherent fragments and discoveryof hidden relations between them would facilitatethe development of better user interfaces and im-prove the performance of summarization and in-formation retrieval systems.Discourse segmentation of the documents com-posed of parallel parts is a novel and challeng-ing problem, as previous research has mostly fo-cused on the linear segmentation of isolated texts(e.g., (Hearst, 1994)).
The most straightforwardapproach would be to use a pipeline strategy,where an existing segmentation algorithm findsdiscourse boundaries of each part independently,and then the segments are aligned.
Or, conversely,a sentence-alignment stage can be followed by asegmentation stage.
However, as we will see in ourexperiments, these strategies may result in poorsegmentation and alignment quality.To address this problem, we construct a non-parametric Bayesian model for joint segmenta-tion and alignment of parallel parts.
In com-parison with the discussed pipeline approaches,our method has two important advantages: (1) itleverages the lexical cohesion phenomenon (Hal-liday and Hasan, 1976) in modeling the paral-lel parts of documents, and (2) ensures that theeffective number of segments can grow adap-tively.
Lexical cohesion is an idea that topically-coherent segments display compact lexical distri-butions (Hearst, 1994; Utiyama and Isahara, 2001;Eisenstein and Barzilay, 2008).
We hypothesizethat not only isolated fragments but also eachgroup of linked fragments displays a compact andconsistent lexical distribution, and our generativemodel leverages this inter-part cohesion assump-tion.In this paper, we consider the dataset of ?En-glish as a second language?
(ESL) podcast1, whereeach episode consists of two parallel parts: a story(an example monologue or dialogue) and an ex-planatory lecture discussing the meaning and us-age of English expressions appearing in the story.Fig.
1 presents an example episode, consisting oftwo parallel parts, and their hidden topical rela-tions.2 From the figure we may conclude that thereis a tendency of word repetition between each pairof aligned segments, illustrating our hypothesis ofcompactness of their joint distribution.
Our goal is1http://www.eslpod.com/2Episode no.
232 post on Jan. 08, 2007.151I have a day job, but I recently started a small business on the side.I didn't know anything about accounting and my friend, Roland, said that he would give me some advice.Roland: So, the reason that you need to do your bookkeeping is so you can manage your cash flow.This podcast is all about business vocabulary related to accounting.
The title of the podcast is Business Bookkeeping.
...
The story begins by Magdalena saying that she has a day job.
A day job is your regular job that you work at from nine in the morning 'til five in the afternoon, for        example.
She also has a small business on the side.
... Magdalena continues by saying that she didn't know anything about accounting and her friend,      Roland, said he would give her some advice.
Accounting is the job of keeping correct records of the money you spend; it's very similar to      bookkeeping.
... Roland begins by saying that the reason that you need to do your bookkeeping is so you can       manage your cash flow.
Cash flow, flow, means having enough money to run your business - to pay your bills.
... ...Story Lecture transcript...Figure 1: An example episode of ESL podcast.
Co-occurred words are represented in italic and underline.to divide the lecture transcript into discourse unitsand to align each unit to the related segment of thestory.
Predicting these structures for the ESL pod-cast could be the first step in development of ane-learning system and a podcast search engine forESL learners.2 Related WorkDiscourse segmentation has been an active areaof research (Hearst, 1994; Utiyama and Isahara,2001; Galley et al, 2003; Malioutov and Barzilay,2006).
Our work extends the Bayesian segmenta-tion model (Eisenstein and Barzilay, 2008) for iso-lated texts, to the problem of segmenting parallelparts of documents.The task of aligning each sentence of an abstractto one or more sentences of the body has beenstudied in the context of summarization (Marcu,1999; Jing, 2002; Daume?
and Marcu, 2004).
Ourwork is different in that we do not try to extractthe most relevant sentence but rather aim to findcoherent fragments with maximally overlappinglexical distributions.
Similarly, the query-focusedsummarization (e.g., (Daume?
and Marcu, 2006))is also related but it focuses on sentence extractionrather than on joint segmentation.We are aware of only one previous work on jointsegmentation and alignment of multiple texts (Sunet al, 2007) but their approach is based on similar-ity functions rather than on modeling lexical cohe-sion in the generative framework.
Our application,the analysis of the ESL podcast, was previouslystudied in (Noh et al, 2010).
They proposed a su-pervised method which is driven by pairwise clas-sification decisions.
The main drawback of theirapproach is that it neglects the discourse structureand the lexical cohesion phenomenon.3 ModelIn this section we describe our model for discoursesegmentation of documents with inherently paral-lel structure.
We start by clarifying our assump-tions about their structure.We assume that a document x consists of Kparallel parts, that is, x = {x(k)}k=1:K , andeach part of the document consists of segments,x(k) = {s(k)i }i=1:I .
Note that the effective num-ber of fragments I is unknown.
Each segment caneither be specific to this part (drawn from a part-specific language model ?
(k)i ) or correspond tothe entire document (drawn from a document-levellanguage model ?
(doc)i ).
For example, the firstand the second sentences of the lecture transcriptin Fig.
1 are part-specific, whereas other linkedsentences belong to the document-level segments.The document-level language models define top-ical links between segments in different parts ofthe document, whereas the part-specific languagemodels define the linear segmentation of the re-maining unaligned text.Each document-level language model corre-sponds to the set of aligned segments, at most onesegment per part.
Similarly, each part-specific lan-guage model corresponds to a single segment ofthe single corresponding part.
Note that all thedocuments are modeled independently, as we aimnot to discover collection-level topics (as e.g.
in(Blei et al, 2003)), but to perform joint discoursesegmentation and alignment.Unlike (Eisenstein and Barzilay, 2008), we can-not make an assumption that the number of seg-ments is known a-priori, as the effective number ofpart-specific segments can vary significantly fromdocument to document, depending on their sizeand structure.
To tackle this problem, we useDirichlet processes (DP) (Ferguson, 1973) to de-152fine priors on the number of segments.
We incor-porate them in our model in a similar way as itis done for the Latent Dirichlet Allocation (LDA)by Yu et al (2005).
Unlike the standard LDA, thetopic proportions are chosen not from a Dirichletprior but from the marginal distribution GEM(?
)defined by the stick breaking construction (Sethu-raman, 1994), where ?
is the concentration param-eter of the underlying DP distribution.
GEM(?
)defines a distribution of partitions of the unit inter-val into a countable number of parts.The formal definition of our model is as follows:?
Draw the document-level topic proportions ?
(doc) ?GEM(?(doc)).?
Choose the document-level language model ?
(doc)i ?Dir(?
(doc)) for i ?
{1, 2, .
.
.}.?
Draw the part-specific topic proportions ?
(k) ?GEM(?
(k)) for k ?
{1, .
.
.
,K}.?
Choose the part-specific language models ?
(k)i ?Dir(?
(k)) for k ?
{1, .
.
.
,K} and i ?
{1, 2, .
.
.}.?
For each part k and each sentence n:?
Draw type t(k)n ?
Unif(Doc, Part).?
If (t(k)n = Doc); draw topic z(k)n ?
?
(doc); gen-erate words x(k)n ?Mult(?(doc)z(k)n)?
Otherwise; draw topic z(k)n ?
?
(k); generatewords x(k)n ?Mult(?
(k)z(k)n).The priors ?
(doc), ?
(k), ?
(doc) and ?
(k) can beestimated at learning time using non-informativehyperpriors (as we do in our experiments), or setmanually to indicate preferences of segmentationgranularity.At inference time, we enforce each latent topicz(k)n to be assigned to a contiguous span of text,assuming that coherent topics are not recurringacross the document (Halliday and Hasan, 1976).It also reduces the search space and, consequently,speeds up our sampling-based inference by reduc-ing the time needed for Monte Carlo chains tomix.
In fact, this constraint can be integrated in themodel definition but it would significantly compli-cate the model description.4 InferenceAs exact inference is intractable, we follow Eisen-stein and Barzilay (2008) and instead use aMetropolis-Hastings (MH) algorithm.
At eachiteration of the MH algorithm, a new potentialalignment-segmentation pair (z?, t?)
is drawn froma proposal distribution Q(z?, t?|z, t), where (z, t)(a) (b) (c)Figure 2: Three types of moves: (a) shift, (b) splitand (c) merge.is the current segmentation and its type.
The newpair (z?, t?)
is accepted with the probabilitymin(1,P (z?, t?,x)Q(z?, t?|z, t)P (z, t,x)Q(z, t|z?, t?
)).In order to implement the MH algorithm for ourmodel, we need to define the set of potential moves(i.e.
admissible changes from (z, t) to (z?, t?
)),and the proposal distribution Q over these moves.If the actual number of segments is known andonly a linear discourse structure is acceptable, thena single move, shift of the segment border (Fig.2(a)), is sufficient (Eisenstein and Barzilay, 2008).In our case, however, a more complex set of movesis required.We make two assumptions which are moti-vated by the problem considered in Section 5:we assume that (1) we are given the number ofdocument-level segments and also that (2) thealigned segments appear in the same order in eachpart of the document.
With these assumptions inmind, we introduce two additional moves (Fig.2(b) and (c)):?
Split move: select a segment, and split it atone of the spanned sentences; if the segmentwas a document-level segment then one ofthe fragments becomes the same document-level segment.?
Merge move: select a pair of adjacent seg-ments where at least one of the segments ispart-specific, and merge them; if one of themwas a document-level segment then the newsegment has the same document-level topic.All the moves are selected with the uniform prob-ability, and the distance c for the shift move isdrawn from the proposal distribution proportionalto c?1/cmax .
The moves are selected indepen-dently for each part.Although the above two assumptions are notcrucial as a simple modification to the set of moveswould support both introduction and deletion ofdocument-level fragments, this modification wasnot necessary for our experiments.1535 Experiment5.1 Dataset and setupDataset We apply our model to the ESL podcastdataset (Noh et al, 2010) of 200 episodes, withan average of 17 sentences per story and 80 sen-tences per lecture transcript.
The gold standardalignments assign each fragment of the story to asegment of the lecture transcript.
We can inducesegmentations at different levels of granularity onboth the story and the lecture side.
However, giventhat the segmentation of the story was obtained byan automatic sentence splitter, there is no reasonto attempt to reproduce this segmentation.
There-fore, for quantitative evaluation purposes we fol-low Noh et al (2010) and restrict our model toalignment structures which agree with the givensegmentation of the story.
For all evaluations, weapply standard stemming algorithm and removecommon stop words.Evaluationmetrics To measure the quality of seg-mentation of the lecture transcript, we use twostandard metrics, Pk (Beeferman et al, 1999) andWindowDiff (WD) (Pevzner and Hearst, 2002),but both metrics disregard the alignment links (i.e.the topic labels).
Consequently, we also use themacro-averaged F1 score on pairs of aligned span,which measures both the segmentation and align-ment quality.Baseline Since there has been little previous re-search on this problem, we compare our resultsagainst two straightforward unsupervised base-lines.
For the first baseline, we consider thepairwise sentence alignment (SentAlign) basedon the unigram and bigram overlap.
The sec-ond baseline is a pipeline approach (Pipeline),where we first segment the lecture transcript withBayesSeg (Eisenstein and Barzilay, 2008) andthen use the pairwise alignment to find their bestalignment to the segments of the story.Our model We evaluate our joint model of seg-mentation and alignment both with and withoutthe split/merge moves.
For the model withoutthese moves, we set the desired number of seg-ments in the lecture to be equal to the actual num-ber of segments in the story I .
In this setting,the moves can only adjust positions of the seg-ment borders.
For the model with the split/mergemoves, we start with the same number of segmentsI but it can be increased or decreased during in-ference.
For evaluation of our model, we run ourinference algorithm from five random states, andMethod Pk WD 1?
F1Uniform 0.453 0.458 0.682SentAlign 0.446 0.547 0.313Pipeline (I) 0.250 0.249 0.443Pipeline (2I+1) 0.268 0.289 0.318Our model (I) 0.193 0.204 0.254+split/merge 0.181 0.193 0.239Table 1: Results on the ESL podcast dataset.
Forall metrics, lower values are better.take the 100,000th iteration of each chain as a sam-ple.
Results are the average over these five runs.Also we perform L-BFGS optimization to auto-matically adjust the non-informative hyperpriorsafter each 1,000 iterations of sampling.5.2 ResultTable 1 summarizes the obtained results.
?Uni-form?
denotes the minimal baseline which uni-formly draws a random set of I spans for each lec-ture, and then aligns them to the segments of thestory preserving the linear order.
Also, we con-sider two variants of the pipeline approach: seg-menting the lecture on I and 2I + 1 segments, re-spectively.3 Our joint model substantially outper-forms the baselines.
The difference is statisticallysignificant with the level p < .01 measured withthe paired t-test.
The significant improvement overthe pipeline results demonstrates benefits of jointmodeling for the considered problem.
Moreover,additional benefits are obtained by using the DPpriors and the split/merge moves (the last line inTable 1).
Finally, our model significantly outper-forms the previously proposed supervised model(Noh et al, 2010): they report micro-averaged F1score 0.698 while our best model achieves 0.778with the same metric.
This observation confirmsthat lexical cohesion modeling is crucial for suc-cessful discourse analysis.6 ConclusionsWe studied the problem of joint discourse segmen-tation and alignment of documents with inherentlyparallel structure and achieved favorable results onthe ESL podcast dataset outperforming the cas-caded baselines.
Accurate prediction of these hid-den relations would open interesting possibilities3The use of the DP priors and the split/merge moves onthe first stage of the pipeline did not result in any improve-ment in accuracy.154for construction of friendlier user interfaces.
Oneexample being an application which, given a user-selected fragment of the abstract, produces a sum-mary from the aligned segment of the documentbody.AcknowledgmentThe authors acknowledge the support of theExcellence Cluster on Multimodal Computingand Interaction (MMCI), and also thank MikhailKozhevnikov and the anonymous reviewers fortheir valuable comments, and Hyungjong Noh forproviding their data.ReferencesDoug Beeferman, Adam Berger, and John Lafferty.1999.
Statistical models for text segmentation.Computational Linguistics, 34(1?3):177?210.David M. Blei, Andrew Ng, and Michael I. Jordan.2003.
Latent dirichlet alocation.
JMLR, 3:993?1022.Hal Daume?
and Daniel Marcu.
2004.
A phrase-basedhmm approach to document/abstract alignment.
InProceedings of EMNLP, pages 137?144.Hal Daume?
and Daniel Marcu.
2006.
Bayesian query-focused summarization.
In Proceedings of ACL,pages 305?312.Jacob Eisenstein and Regina Barzilay.
2008.
Bayesianunsupervised topic segmentation.
In Proceedings ofEMNLP, pages 334?343.Thomas S. Ferguson.
1973.
A Bayesian analysis ofsome non-parametric problems.
Annals of Statistics,1:209?230.Michel Galley, Kathleen R. McKeown, Eric Fosler-Lussier, and Hongyan Jing.
2003.
Discourse seg-mentation of multi-party conversation.
In Proceed-ings of ACL, pages 562?569.M.
A. K. Halliday and Ruqaiya Hasan.
1976.
Cohe-sion in English.
Longman.Marti Hearst.
1994.
Multi-paragraph segmentation ofexpository text.
In Proceedings of ACL, pages 9?16.Hongyan Jing.
2002.
Using hidden Markov modelingto decompose human-written summaries.
Computa-tional Linguistics, 28(4):527?543.Igor Malioutov and Regina Barzilay.
2006.
Minimumcut model for spoken lecture segmentation.
In Pro-ceedings of ACL, pages 25?32.Daniel Marcu.
1999.
The automatic construction oflarge-scale corpora for summarization research.
InProceedings of ACM SIGIR, pages 137?144.Hyungjong Noh, Minwoo Jeong, Sungjin Lee,Jonghoon Lee, and Gary Geunbae Lee.
2010.Script-description pair extraction from text docu-ments of English as second language podcast.
InProceedings of the 2nd International Conference onComputer Supported Education.Lev Pevzner and Marti Hearst.
2002.
A critique andimprovement of an evaluation metric for text seg-mentation.
Computational Linguistics, 28(1):19?36.Jayaram Sethuraman.
1994.
A constructive definitionof Dirichlet priors.
Statistica Sinica, 4:639?650.Bingjun Sun, Prasenjit Mitra, C. Lee Giles, John Yen,and Hongyuan Zha.
2007.
Topic segmentationwith shared topic detection and alignment of mul-tiple documents.
In Proceedings of ACM SIGIR,pages 199?206.Masao Utiyama and Hitoshi Isahara.
2001.
A statis-tical model for domain-independent text segmenta-tion.
In Proceedings of ACL, pages 491?498.Kai Yu, Shipeng Yu, and Vokler Tresp.
2005.
Dirichletenhanced latent semantic analysis.
In Proceedingsof AISTATS.155
