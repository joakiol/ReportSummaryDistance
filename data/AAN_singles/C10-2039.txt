Coling 2010: Poster Volume, pages 338?346,Beijing, August 2010Comparing the performance of two TAG-based surface realisers usingcontrolled grammar traversalClaire GardentCNRS/LORIAclaire.gardent@loria.frBenjamin Gottesmanacrolinx GmbHben.gottesman@acrolinx.comLaura Perez-BeltrachiniUniversite?
Henri Poincare?/LORIAlaura.perez@loria.frAbstractWe present GENSEM, a tool for generat-ing input semantic representations for twosentence generators based on the same re-versible Tree Adjoining Grammar.
Wethen show how GENSEM can be usedto produced large and controlled bench-marks and test the relative performance ofthese generators.1 IntroductionAlthough computational grammars are mostlyused for parsing, they can also be used to gener-ate sentences.
This has been done, for instance,to detect overgeneration by the grammar (Gardentand Kow, 2007).
Sentences that are generated butare ungrammatical indicate flaws in the grammar.This has also been done to test a parser (Neder-hof, 1996; Purdom, 1972).
Using the sentencesgenerated from the grammar ensures that the sen-tences given to the parser are in the language it de-fines.
Hence a parse failure necessarily indicatesa flaw in the parser?s design as opposed to a lackof coverage by the grammar.Here we investigate a third option, namely, thefocused benchmarking of sentence realisers basedon reversible grammars, i.e.
on grammars that canbe used both to produce sentences from a semanticrepresentation and semantic representations froma sentence.More specifically, we present a linguistically-controlled grammar traversal algorithm for TreeAdjoining Grammar (TAG) which, when appliedto a reversible TAG, permits producing arbitrarilymany of the semantic representations associatedby this TAG with the sentences it generates.
Wethen show that the semantic representations thusproduced can be used to compare the relative per-formance of two sentence generators based on thisgrammar.Although the present paper concentrates onTree Adjoining Grammar realisers, it is worthpointing out that the semantic representations pro-duced could potentially be used to evaluate anysurface realiser whose input is a flat semantic for-mula.Section 2 discusses related work and motivatesthe approach.
Section 3 presents GENSEM, theDCG-based grammar traversal algorithm we de-veloped.
We show, in particular, that the use ofa DCG permits controlling grammar traversal insuch a way as to systematically generate sets of se-mantic representations covering certain computa-tionally or linguistically interesting cases.
Finally,Section 4 reports on the benchmarking of two sur-face realisers with respect to a GENSEM-producedbenchmark.2 MotivationsPrevious work on benchmark construction fortesting the performance of surface realisers fallsinto two camps depending on whether or not therealiser uses a reversible grammar, that is, a gram-mar that can be used for both parsing and genera-tion.To test a surface realiser based on a largereversible Head-Driven Phrase Structure Gram-mar (HPSG), Carroll et al (1999) use a smalltest set of two hand-constructed and 40 parsing-derived cases to test the impact of intersectivemodifiers1 on generation performance.
More re-cently, Carroll and Oepen (2005) present a perfor-1As first noted by Brew (1992) and Kay (1996), given aset of n modifiers all modifying the same structure, all pos-sible intermediate structures will be constructed, i.e., 2n+1.338mance evaluation which uses as a benchmark theset of semantic representations produced by pars-ing 130 sentences from the Penn Treebank andmanually selecting the correct semantic represen-tations.
Finally, White (2004) profiles a CCG2-based sentence realiser using two domain-focusedreversible CCGs to produce two test suites of 549and 276 ?
semantic formula, target sentence ?pairs, respectively.For realisers that are not based on a reversiblegrammar, there are approaches which derive largesets of realiser input from the Penn Treebank(PTB).
For example, Langkilde-Geary (2002)proposes to translate the PTB annotations into aformat accepted by her sentence generator Halo-gen.
The output of this generator can then be au-tomatically compared with the PTB sentence fromwhich the corresponding input was derived.
Simi-larly, Callaway (2003) builds an evaluation bench-mark by transforming PTB trees into a formatsuitable for the KPML realiser he uses.In all of the above cases, the data is derivedfrom real world sentences, thereby exemplifying?real world complexity?.
If the corpus is largeenough (as in the case of the PTB), the data canfurthermore be expected to cover a broad range ofsyntactic phenomena.
Moreover, the data, beingderived from real world sentences, is not biasedtowards system-specific capabilities.
Nonethe-less, there are also limits to these approaches.First, they fail to support graduated perfor-mance testing on constructs such as intersectivemodifiers or lexical ambiguity, which are knownto be problematic for surface realisation.Second, the construction of the benchmark is inboth cases time consuming.
In the reversible ap-proach, for each input sentence, the correct inter-pretation must be manually selected from amongthe semantic formulae produced by the parser.
Asa side effect, the constructed benchmarks remainrelatively small (825 in the case of White (2004);130 in Carroll and Oepen (2005)).
In the caseof a benchmark derived by transformation froma syntactically annotated corpus, the implemen-tation of the converter is both time-intensive andcorpus-bound.
For instance, Callaway (2003) re-2Combinatory Categorial Grammarports that the implementation of such a proces-sor for the SURGE realiser was the most time-consuming part of the evaluation with the result-ing component containing 4000 lines of code and900 rules.As we shall show in the following sections,the GENSEM approach to benchmark constructionaims to address both of these shortcomings.
Byusing a DCG to implement grammar traversal, itpermits both a full automation of the benchmarkcreation and some control over the type and thedistribution of the benchmark items.3 GenSemAs mentioned above, GENSEM is a grammartraversal algorithm for TAG.
We first present thespecific TAG used for traversal, namely SEMX-TAG (Alahverdzhieva, 2008) (section 3.1).
Wethen show how to automatically derive a DCGthat describes the derivation trees of this gram-mar (section 3.2).
Finally, we show how this DCGencoding permits generating formulae while en-abling control over the set of semantic representa-tions to be produced (section 3.3).3.1 SemXTAGThe SEMXTAG grammar used by GENSEM andby the two surface realisers is a Feature-BasedLexicalised Tree Adjoining Grammar augmentedwith a unification-based semantics as described byGardent and Kallmeyer (2003).
We briefly intro-duce each of these components and describe thegrammar coverage.FTAG.
A Feature-based TAG (Vijay-Shankerand Joshi, 1988) consists of a set of (auxil-iary or initial) elementary trees and of two tree-composition operations: substitution and adjunc-tion.
Initial trees are trees whose leaves are la-belled with substitution nodes (marked with adownarrow) or terminal categories.
Auxiliarytrees are distinguished by a foot node (markedwith a star) whose category must be the same asthat of the root node.
Substitution inserts a treeonto a substitution node of some other tree whileadjunction inserts an auxiliary tree into a tree.
Inan FTAG, the tree nodes are furthermore deco-rated with two feature structures (called top and339bottom) which are unified during derivation asfollows.
On substitution, the top of the substitu-tion node is unified with the top of the root nodeof the tree being substituted in.
On adjunction,the top of the root of the auxiliary tree is unifiedwith the top of the node where adjunction takesplace; and the bottom features of the foot node areunified with the bottom features of this node.
Atthe end of a derivation, the top and bottom of allnodes in the derived tree are unified.
Finally, eachsentence derivation in an FTAG is associated withboth a derived tree representing the phrase struc-ture of the sentence and a derivation tree record-ing how the corresponding elementary trees werecombined to form the derived tree.FTAG with semantics.
To associate seman-tic representations with natural language expres-sions, the FTAG is modified as proposed by Gar-dent and Kallmeyer (2003).NPjJohnname(j,john)SbNP?c VPbaVarunsrun(a,c)VPxoften VP*xoften(x)?
name(j,john), run(a,j), often(a)Figure 1: Flat semantics for ?John often runs?Each elementary tree is associated with a flatsemantic representation.
For instance, in Fig-ure 1,3 the trees for John, runs, and often are asso-ciated with the semantics name(j,john), run(a,c),and often(x), respectively.
Importantly, the argu-ments of a semantic functor are represented byunification variables which occur both in the se-mantic representation of this functor and on somenodes of the associated syntactic tree.
For in-stance in Figure 1, the semantic index c occur-ring in the semantic representation of runs alsooccurs on the subject substitution node of the as-sociated elementary tree.
The value of semanticarguments is determined by the unifications re-sulting from adjunction and substitution.
For in-stance, the semantic index c in the tree for runs is3Cx/Cx abbreviate a node with category C and atop/bottom feature structure including the feature-value pair{ index : x}.unified during substitution with the semantic in-dex labelling the root node of the tree for John.As a result, the semantics of John often runs is{name(j,john),run(a,j),often(a)}.SemXTAG.
SEMXTAG is an FTAG for En-glish augmented with a unification-based compo-sitional semantics of the type described above.Its syntactic coverage approaches that of XTAG,the FTAG developed for English by the XTAGgroup (The XTAG Research Group, 2001).
Likethis grammar, it contains around 1300 elementarytrees and covers auxiliaries, copula, raising andsmall clause constructions, topicalization, relativeclauses, infinitives, gerunds, passives, adjuncts,ditransitives and datives, ergatives, it-clefts, wh-clefts, PRO constructions, noun-noun modifica-tion, extraposition, sentential adjuncts, impera-tives and resultatives.3.2 Converting SemXTAG to a DCGWe would like to be able to traverse SEMXTAG inorder to generate semantic representations that arelicensed by it.
In the DCG formalism, a grammaris represented as a set of Prolog definite clauses,and Prolog?s query mechanism provides built-ingrammar traversal.
We take advantage of this byderiving a DCG from SEMXTAG and then usingProlog queries to generate semantic representa-tions that are associated with sentences in the lan-guage described by it.Another advantage of the DCG formalism isthat arbitrary Prolog goals can be inserted into arule, to constrain when the rule applies or to bindvariables occurring in it.
We use this to groundderivations with lexical items, which are repre-sented using Prolog assertions.
We also use it tocontrol Prolog?s grammar traversal in such a wayas to generate sets of semantic formulae coveringcertain computationally interesting cases (see sec-tion 3.3).Our algorithm for converting SEMXTAG to aDCG is inspired by Schmitz and Le Roux (2008),who derive from an FTAG a feature-based reg-ular tree grammar (RTG) whose language is thederivation trees of the FTAG.
Indeed, in our im-plementation, we derive a DCG from such anRTG, thereby taking advantage of a SEMXTAG-340to-RTG converter previously implemented by Syl-vain Schmitz.TAG to RTG.
In the conversion to RTG4, eachelementary tree in SEMXTAG is converted to arule that models the contribution of the tree toa TAG derivation.
A TAG derivation involvesthe selection of an initial tree, which has somenodes requiring substitution and some permittingadjunction.
Let us think of the potential adjunc-tion sites as requiring, rather than permitting, ad-junction, but such that the requirement can be sat-isfied by ?null?
adjunction.
Inserting another treeinto this initial tree satisfies one of the substitutionor adjunction requirements, but introduces somenew requirements into the resulting tree, in theform of its own substitution nodes and adjunctionsites.Thus, intuitively, the RTG representation of aSEMXTAG elementary tree is a rule that rewritesthe satisfied requirement as a local tree whose rootis a unique identifier of the tree and whose leavesare the introduced requirements.
A requirementof a substitution or adjunction of a tree of rootcategory X is written as XS or XA, respectively.Here, for example, is the translation to RTG of theTAG tree (minus semantics) for runs in Figure 1,using the word anchoring the tree as its identifier(the superscripts abbreviate feature structures: b/trefers to the bottom/top feature structure and theupper case letters to the semantic index value, so[idx : X] is abbreviated to X):S[t:T ]S ?
runs(S[t:T,b:B]A NP[t:C]S V P[t:B,b:A]A V[t:A]A )The semantics of the SEMXTAG tree are carriedover as-is to the corresponding RTG rule.
Fur-ther, the feature structures labelling the nodes ofSEMXTAG trees are carried over to the RTG rulesso as to correctly interact with substitution andadjunction (see Schmitz and Le Roux (2008) formore details on this part of the conversion pro-cess).To account for the optionality of adjunction,there are additional rules allowing any adjunction4For a more precise description of the FTAG to RTG con-version see Schmitz and Le Roux (2008).requirement to be rewritten as the symbol ?, a ter-minal symbol of the RTG.The terminal symbols of the RTG are thus thetree identifiers and the symbol ?, and its non-terminals are XS and XA for each terminal ornon-terminal X of SEMXTAG.RTG to DCG.
Since the right-hand side of eachRTG rule is a local tree ?
that is, a tree of depth nomore than one ?
we can flatten each of them intoa list consisting of the root node followed by theleaves without losing any structural information.This is the insight underlying the RTG-to-DCGconversion step.
Each RTG rule is converted toa DCG rule that is essentially identical except forthis flattening of the right-hand side.
Here is thetranslation to DCG of the RTG rule above5:rule(s,init,Top,Bot,Sem;S;N;VP;V)--> [runs],{lexicon(runs,n0V,[run])},rule(s,aux,Top,[B],S),rule(np,init,[C],_,N),rule(vp,aux,[B],[A],VP),rule(v,aux,[A],_,V),{Sem =.. [run,A,C]}.We represent non-terminals of the DCG us-ing the rule predicate, whose five (non-hidden)6arguments, in order, are the category, the sub-script (init for subscript S, aux for subscriptA), the top and bottom feature values, and the se-mantics.
Feature structures are represented us-ing Prolog lists with a fixed argument positionfor each attribute in the grammar (in this ex-ample, only the index attribute).
The semanticsassociated with the left-hand-side symbol (here,Sem;S;N;VP;V, with the semicolon represent-ing semantic conjunction) are composed of the se-mantics associated with this rule and those associ-ated with each of the right-hand-side symbols.The language of the resulting DCG is neitherthe language of the RTG nor the language ofSEMXTAG, and indeed the language of the DCGdoes not interest us but rather its derivation trees.5In practice, the lexicon is factored out, so there is no rulespecifically for runs, but one for intransitive verbs (n0V) ingeneral.
Each rule hooks into the lexicon, so that a giveninvocation of a rule is grounded by a particular lexical item.6The ??
> notation is syntactic sugar for the usual Pro-log : ?
definite clause notation with two hidden argumentson each predicate.
The hidden arguments jointly representthe list of terminals dominated by the symbol.341These are correlated one-to-one with the trees inthe language described by the RTG, i.e.
with thederivation trees of SEMXTAG, and the latter canbe trivially reconstructed from the DCG deriva-tions.
From a SEMXTAG derivation tree, one cancompose the semantic representation of the asso-ciated sentence, and in fact this semantic compo-sition occurs as a side effect of a Prolog queryagainst the DCG, allowing semantic representa-tions licensed by SEMXTAG to be returned asquery results.We define a Prolog predicate for queryingagainst the DCG, as follows.
Its one input argu-ment, Cat, is the label of the root node of thederivation tree (typically s), and its one output ar-gument, Sem, is the semantic representation asso-ciated with that tree7.genSem(Cat,Sem) :-rule(Cat,init,_,_,Sem,_,[]).3.3 Control parametersIn order to give the users some control over thesorts of semantic representations that they getback from a query against the DCG, we augmentthe DCG in such a way as to allow control overthe TAG family8 of the root tree in the derivationtree, over the number and type of adjunctions inthe derivation, and over the depth of substitutions.To implement control over the family is quite sim-ple: we need merely to index the DCG rules byfamily and modify the GENSEM call accordingly.For instance, the above DCG rule becomes :rule(s,init,Top,Bot,n0V,Sem;S;NP;VP;V)--> [runs],{lexicon(runs,n0V,[run])},...We implement restrictions on adjunctions byadding an additional argument to the grammarsymbols, namely a vector of non-negative inte-gers representing the number of non-null adjunc-tions of each type that are in the derivation sub-tree dominated by the symbol.
By ?type?
of ad-junction, we mean the category of the adjunc-7The 6th and 7th arguments of the rule call are the hiddenarguments needed by the DCG.8TAG families group together trees which belong to-gether, in particular, the trees associated with various real-isation of a specific subcategorisation type.
Thus, here thenotion of TAG family is equivalent to that of subcategorisa-tion type.tion site.
In DCG terms, a non-null adjunctionof a category X is represented as the expansion ofan x/aux symbol other than as ?.
So, for ex-ample, a DCG symbol associated with the vec-tor [1,0,0,0,0], where the five dimensions ofthe vector correspond to the n, np, v, vp, and scategories, respectively, dominates a subtree con-taining exactly one n/aux symbol expanded bya non-epsilon rule, and no other aux symbol ex-panded by a non-epsilon rule.
We link the vectorassociated with the root of the derivation to thequery predicate.We define a special predicate to handle thedivvying up of a mother node?s vector among thedaughters, taking advantage of the fact that theDCG formalism permits the insertion of arbitraryProlog goals into a rule.Finally, we add an additional argument to theDCG rule and to the GENSEM?s call to controlthe traversal depth with respect to the number ofsubstitutions applied.
The overall depth of eachderivation is therefore constrained both by theuser defined adjunctions and substitution depthconstraints.Our query predicate now has four input argu-ments and one output argument:genSem(Cat,Fam,[N,NP,V,VP,S],Dth,Sem):-rule(Cat,init,_,_,Fam,[N,NP,V,VP,S],Dth,Sem,_,[]).4 Using GENSEM for benchmarkingWe now show how GENSEM can be put to workfor comparing two TAG-based surface realisers,namely GENI (Gardent and Kow, 2007) and RT-GEN (Perez-Beltrachini, 2009).
These two realis-ers follow globally similar algorithms but differ inseveral respects.
We show how GENSEM can beused to produce benchmarks that are tailored totest hypotheses about how these differences mightimpact performance.
We then use this GENSEM-generated benchmark to compare the performanceof the two realisers.4.1 GenI and RTGenBoth GENI and RTGEN use the SEMXTAG gram-mar described in section 3.1.
Moreover, both re-alisers follow an algorithm pipelining three mainphases.
First, lexical selection selects from the342grammar those elementary trees whose semanticssubsumes part of the input semantics.
Second,the tree combining phase systematically tries tocombine trees using substitution and adjunction.Third, the retrieval phase extracts the yields ofthe complete derived trees, thereby producing thegenerated sentence(s).There are also differences however.
We nowspell these out and indicate how they might im-pact the relative performance of the two surfacerealisers.Derived vs. derivation trees.
While GENI con-structs derived trees, RTGEN uses the RTG en-coding of SEMXTAG sketched in the previoussection to construct derivation trees.
These arethen unraveled into derived trees at the final re-trieval stage.
As noted by Koller and Striegnitz(2002), these trees are simpler than TAG elemen-tary trees, which can favourably impact perfor-mance.Interleaving of feature constraint solving andsyntactic analysis.
GENI integrates in the treecombining phase a filtering step in which the ini-tial search space is pruned by eliminating fromit all combinations of TAG elementary trees thatcover the input semantics but cannot possibly leadto a valid derived tree.
This filtering eliminatesall combinations of trees such that either the cat-egory of a substitution node cannot be cancelledout by that of the root node of a different tree, ora root node fails to have a matching substitutionsite.
Importantly, filtering ignores feature infor-mation and tree combining takes place after filter-ing.
RTGEN, on the other hand, directly combinesderivation trees decorated with full feature struc-ture information.Handling of intersective modifiers.
GENI andRTGEN differ in their strategies for handlingmodification.Adapting Carroll and Oepen?s (2005) proposalto TAG, GENI adopts a two-step tree-combiningprocess such that in the first step, only substitu-tion applies, while in the second, only adjunc-tion is used.
Although the number of intermediatestructures generated is still 2n for n modifiers, thisstrategy has the effect of blocking these 2n struc-tures from multiplying out with other structures inthe chart.RTGEN, on the other hand, uses a standard Ear-ley algorithm that includes sharing and packing.Sharing allows intermediate structures commonto several derivations to be represented once onlywhile packing groups together partial derivationtrees with identical semantic coverage and similarcombinatorics (same number and type of substitu-tion and adjunction requirements), keeping onlyone representative of such groups in the chart.In this way, intermediate structures covering thesame set of intersective modifiers in a differentorder are only represented once and the negativeimpact of intersective modifiers is lessened.4.2 Two GENSEM benchmarksWe use GENSEM to construct two benchmarks de-signed to test the impact of the differences be-tween the two realisers and, more specifically, tocompare the relative performance of both realisers(i) on cases involving intersective modifiers and(ii) on cases of varying overall complexity.The MODIFIERS benchmark focuses onintersective modifiers and contains semanticformulae corresponding to sentences in-volving an increasing number of modifiers.Recall that GENSEM calls are of the formgensem(Cat,Family,[N,NP,V,VP,S],Dth,Sem)where N,NP,V,VP,S indicates the number ofrequired adjunctions in N, NP, V, VP and S,respectively, while Family constrains the subcate-gorisation type of the root tree in the derivationsproduced by GENSEM.
To produce formulaeinvolving the lexical selection of intersectivemodifiers, we set the following constraints.
Cat isset to s and Family is set to either n0V (intransitiveverbs) or n0Vn1 (transitive verbs).
Furthermore,N and V P vary from 0 to 4 thereby requiring theadjunction of 0 to 4 N and/or VP modifiers.
Allother adjunction counters are set to null.
To avoidproducing formulae with identical derivation treesbut distinct lemmas, we use a restricted lexiconcontaining one lemma of each syntactic type,e.g.
one transitive verb, one intransitive verb, etc.Given these settings, GENSEM produces 1 789formulae whose adjunction requirements varyfrom 1 to 6.
For instance, the semantic formula343{sleep(b,c),man(c),a(c),blue(c),sleep(i,c),carefully(b)} (Asleeping blue man sleeps carefully) extractedfrom the MODIFIERS benchmark contains twoNP adjunctions and one VP adjunction.The MODIFIERS benchmark is tailored to fo-cus on cases involving a varying number of in-tersective modifiers.
To support a comparison ofthe realisers on this dimension, it displays little orno variation w.r.t.
other dimensions, such as verbtype and non-modifying adjunctions.To measure the performance of the two realiserson cases of varying overall complexity, we con-struct a second benchmark (COMPLEXITY) dis-playing such variety.
The GENSEM parametersfor the construction of this suite are the follow-ing.
The verb type (Family) is one of 28 possibleverb types9.
The number and type of required ad-junctions vary from 0 to 4 for N adjunctions, 0 to1 for NP , 0 to 4 for V P and 0 to 1 for S. The re-sulting benchmark contains 890 semantic formu-lae covering an extensive set of verb types and ofadjunction requirements.4.3 ResultsUsing the two GENSEM-generated benchmarks,we now compare GENI and RTGEN.
We plot theaverage number of chart items against both thenumber of intersective modifiers present in the in-put (Figure 3) and the size of the Initial SearchSpace (ISS), i.e., the number of combinations ofelementary TAG trees covering the input seman-tics to be explored after the lexical selection step(Figure 2).
In our case, the ISS gives a moremeaningful idea about the complexity than con-sidering only the number of input literals.
In anFTAG, the number of elementary trees selected9The 28 verb types areEn1V,n0BEn1,n0lVN1Pn2,n0V,n0Va1,n0VAN1,n0VAN1Pn2,n0VDAN1,n0VDAN1Pn2,n0VDN1,n0VDN1Pn2,n0Vn1,n0VN1,n0Vn1Pn2,n0VN1Pn2,n0Vn2n1,n0Vpl,n0Vpln1,n0Vpn1,n0VPn1,n0Vs1,REn1VA2,REn1VPn2,Rn0Vn1A2,Rn0Vn1Pn2,s0V,s0Vn1,s0Vton1.
The notational conventionfor verb types is from XTAG and reads as follows.
Sub-scripts indicate the thematic role of the verb argument.
nindicates a nominal, Pn a PP and s a sentential argument.
plis a verbal particle.
Upper case letters describe the syntacticfunctor type: V is a verb, E an ergative, R a resultative andBE the copula.
Sequences of upper case letters such asVAN in n0VAN1 indicate a multiword functor with syntacticcategories V, A, and N. For instance, n0Vn1 indicates a verbtaking two nominal arguments (e.g., like) and n0VAN1 averb locution such as to cry bloody murder.0-100100-10001000-50005000-1000010000-100000100000-500000500000-1000000morethan1000000102103104105106pp pp pp p pInitial Search Space (ISS) sizeunpackedchartsizeRTGEN-allRTGEN-level0p RTGEN-selectiveGENIFigure 2: Performance of realisation approacheson the COMPLEXITY benchmark, average un-packed chart size as a function of the ISS com-plexity.by a given literal may vary considerably depend-ing on the number and the size of the tree familiesselected by this literal.
For instance, a literal se-lecting the n0Vn2n1 class will select many moretrees than a literal selecting the n0V family be-cause there are many more ways of realising thethree arguments of a ditransitive verb than the sin-gle subject argument of an intransitive one.
Chartitems include all elementary trees selected by thelexical selection step as well as the intermediateand final structures produced by the tree combin-ing phase.
In RTGEN, we distinguish betweenthe number of structures built before unpacking(packed chart) and the number of structures ob-tained after unpacking (unpacked chart).Both realisers are implemetned in different pro-gramming languages, GENI is implemented inHaskell whereas RTGEN in Prolog.
As for thetime results comparison, preliminary experimentsshow that GENI is faster is simple input cases.
Onthe other hand, in the case of more complex cases,the point of producing much less intermediate re-sults pays off compared to the overhead of thechart/agenda operations.344Overall efficiency.
The plot in Figure 2 showsthe results obtained by running both realisers onthe COMPLEXITY benchmark.
Recall (cf.
sec-tion 4.2) that the COMPLEXITY benchmark con-tains input with varying verb arity and a varyingnumber of required adjunctions.
Hence it providescases of increasing complexity in terms of ISS tobe explored.
Furthermore, test cases in the bench-mark trigger sentence realisation involving certainTAG families, which have a certain number oftrees.
Those trees within a family often have iden-tical combinatorics but different features.
Conse-quently, the COMPLEXITY benchmark also pro-vides an appropriate testbed for testing the im-pact of feature structure information on the twoapproaches to tree combination.The graphs show that as complexity increases,the performance delta between GENI and RT-GEN increases.
We conjecture that as complex-ity grows, the filtering used by GENI does notsuffice to reduce the search space to a manage-able size.
Conversely, the overhead introduced byRTGEN?s all-in-one, tree-combining Earley withpacking strategy seems compensated throughoutby the construction of a derivation rather than aderived tree and pays off increasingly as complex-ity increases.Modifiers.
Figure 3 plots the results obtained byrunning the realisers on the MODIFIERS bench-mark.
Here again, RTGEN outperforms GENI andthe delta between the two realisers grows with thenumber of intersective modifiers to be handled.
Acloser look at the data shows that the global con-straints set by GENSEM on the number of requiredadjunctions covers an important range of varia-tion in the data complexity.
For instance, thereare cases where 4 modifiers modify the same NP(or VP) and cases where the modifiers are dis-tributed over two NPs.
Similarly, literals intro-duced into the formula by a GENSEM adjunctionrequirement vary in terms of the number of auxil-iary trees whose selection they trigger.
The steepcurve in GENI?s plot suggests that although thedelayed adjunction mechanism helps in avoidingthe proliferation of intermediate incomplete mod-ifiers?
structures, the lexical ambiguity of modi-fiers still poses a problem.
In contrast, RTGEN?s0 1 2 3 4 5 6 7103104ppppppnumber of modifiersunpackedchartsizeRTGEN-allRTGEN-level0p RTGEN-selectiveGENIFigure 3: Performance of realisation approacheson the MODIFIERS benchmark, average unpackedchart size as a function of the number of modifiers.packing uniformly applies to word order varia-tions and to the cases of lexical ambiguity raisedby intersective modifiers because the items havethe same combinatoric potential and the same se-mantics.5 ConclusionSurface realisers are complex systems that need tohandle diverse input and require complex compu-tation.
Testing raises among other things the issueof coverage ?
how can the potential input spacebe covered?
?
and of test data creation ?
shouldthis data be hand tailored, created randomly, orderived from real world text?In this paper, we presented an approach whichpermits automating the creation of test input forsurface realisers whose input is a flat semantic for-mula.
The approach differs from other existingevaluation schemes in two ways.
First, it permitsproducing arbitrarily many inputs.
Second, it sup-ports the construction of grammar-controlled, lin-guistically focused benchmarks.We are currently working on further extendingGENSEM with more powerful (recursive) controlrestrictions on the grammar traversal; on com-bining GENSEM with tools for detecting grammarovergeneration; and on producing a benchmarkthat could be made available to the community fortesting surface realisers whose input is either a de-pendency tree or a flat semantic formula.345ReferencesAlahverdzhieva, K. 2008.
XTAG using XMG.
Mas-ter?s thesis, U. Nancy 2.
Erasmus Mundus Master?Language and Communication Technology?.Brew, C. 1992.
Letting the cat out of the bag: Gen-eration for shake-and-bake MT.
In Proceedings ofCOLING ?92, Nantes, France.Callaway, Charles B.
2003.
Evaluating coverage forlarge symbolic NLG grammars.
In 18th IJCAI,pages 811?817, Aug.Carroll, John and Stephan Oepen.
2005.
High effi-ciency realization for a wide-coverage unificationgrammar.
2nd IJCNLP.Carroll, John, A. Copestake, D. Flickinger, andV.
Paznan?ski.
1999.
An efficient chart generatorfor (semi-)lexicalist grammars.
In Proceedings ofEWNLG ?99.Gardent, Claire and Laura Kallmeyer.
2003.
Seman-tic construction in FTAG.
In 10th EACL, Budapest,Hungary.Gardent, Claire and Eric Kow.
2007.
Spotting over-generation suspects.
In 11th European Workshopon Natural Language Generation (ENLG).Kay, Martin.
1996.
Chart generation.
In Proceedingsof the 34th annual meeting on Association for Com-putational Linguistics, pages 200?204, Morristown,NJ, USA.
Association for Computational Linguis-tics.Koller, Alexander and Kristina Striegnitz.
2002.
Gen-eration as dependency parsing.
In Proceedings ofthe 40th ACL, Philadelphia.Langkilde-Geary, Irene.
2002.
An empirical verifi-cation of coverage and correctness for a general-purpose sentence generator.
In Proceedings of theINLG.Nederhof, M.-J.
1996.
Efficient generation of randomsentences.
Natural Language Engineering, 2(1):1?13.Perez-Beltrachini, Laura.
2009.
Using regulartree grammars to reduce the search space in sur-face realisation.
Master?s thesis, Erasmus MundusMaster Language and Communication Technology,Nancy/Bolzano.Purdom, Paul.
1972.
A sentence generator for testingparsers.
BIT, 12(3):366?375.Schmitz, S. and J.
Le Roux.
2008.
Feature uni-fication in TAG derivation trees.
In Gardent, C.and A. Sarkar, editors, Proceedings of the 9th In-ternational Workshop on Tree Adjoining Grammarsand Related Formalisms (TAG+?08), pages 141?148, Tu?bingen, Germany.The XTAG Research Group.
2001.
A lexicalised treeadjoining grammar for english.
Technical report,Institute for Research in Cognitive Science, Univer-sity of Pennsylvannia.Vijay-Shanker, K. and AK Joshi.
1988.
Feature Struc-tures Based Tree Adjoining Grammars.
Proceed-ings of the 12th conference on Computational lin-guistics, 55:v2.White, Michael.
2004.
Reining in CCG chart realiza-tion.
In INLG, pages 182?191.346
