Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 600?608, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsAssessment of ESL Learners?
Syntactic Competence Based on SimilarityMeasuresSu-Youn YoonEducational Testing ServicePrinceton, NJ 08541syoon@ets.orgSuma BhatBeckman Institute,Urbana, IL 61801spbhat2@illinois.eduAbstractThis study presents a novel method thatmeasures English language learners?
syntac-tic competence towards improving automatedspeech scoring systems.
In contrast to mostprevious studies which focus on the length ofproduction units such as the mean length ofclauses, we focused on capturing the differ-ences in the distribution of morpho-syntacticfeatures or grammatical expressions acrossproficiency.
We estimated the syntactic com-petence through the use of corpus-based NLPtechniques.
Assuming that the range and so-phistication of grammatical expressions canbe captured by the distribution of Part-of-Speech (POS) tags, vector space models ofPOS tags were constructed.
We use a largecorpus of English learners?
responses that areclassified into four proficiency levels by hu-man raters.
Our proposed feature measuresthe similarity of a given response with themost proficient group and is then estimates thelearner?s syntactic competence level.Widely outperforming the state-of-the-artmeasures of syntactic complexity, our methodattained a significant correlation with human-rated scores.
The correlation between human-rated scores and features based on manualtranscription was 0.43 and the same based onASR-hypothesis was slightly lower, 0.42.
Animportant advantage of our method is its ro-bustness against speech recognition errors notto mention the simplicity of feature genera-tion that captures a reasonable set of learner-specific syntactic errors.1 IntroductionThis study provides a novel method that measuresESL (English as a second language) learners?
com-petence in grammar usage (syntactic competence).Being interdisciplinary in nature, it shows how tocombine the core findings in the ESL literature withvarious empirical NLP techniques for the purpose ofautomated scoring.Grammar usage is one of the dimensions of lan-guage ability that is assessed during non-native pro-ficiency level testing in a foreign language.
Overallproficiency in the target language can be assessedby testing the abilities in various areas including flu-ency, pronunciation, and intonation; grammar andvocabulary; and discourse structure.
Testing rubricsfor human raters contain descriptors used for thesubjective assessment of several of these features.With the recent move towards the objective assess-ment of language ability (spoken and written), it isimperative that we develop methods for quantifyingthese abilities and measuring them automatically.Ortega (2003) indicated that ?the range of formsthat surface in language production and the degreeof sophistication of such forms?
were two impor-tant areas in grammar usage and called the combina-tion of these two areas ?syntactic complexity.?
Fea-tures that measure syntactic complexity have beenfrequently studied in ESL literature and have beenfound to be highly correlated with students?
profi-ciency levels in writing.Studies in automated speech scoring have focusedon fluency (Cucchiarini et al2000; Cucchiarini etal., 2002), pronunciation (Witt and Young, 1997;600Witt, 1999; Franco et al1997; Neumeyer et al2000), and intonation (Zechner et al2009), and rel-atively fewer studies have been conducted on gram-mar usage.
More recently, Lu (2010), Chen andYoon (2011) and Chen and Zechner (2011) havemeasured syntactic competence in speech scoring.Chen and Yoon (2011) estimated the complexity ofsentences based on the average length of the clausesor sentences.
In addition to these length measures,Lu (2010) and Chen and Zechner (2011) measuredthe parse-tree based features such as the mean depthof parsing tree levels.
However, these studies foundthat these measures did not show satisfactory empir-ical performance in automatic speech scoring (Chenand Yoon, 2011; Chen and Zechner, 2011) when thefeatures were calculated from the output of a speechrecognition engine.This study considers new features that measuresyntactic complexity and is novel in two importantways.
First, in contrast to most features that in-fer syntactic complexity based upon the length ofthe unit, we directly measure students?
sophistica-tion and range in grammar usage.
Second, insteadof rating a student?s response using a scale based onnative speech production, our experiments compareit with a similar body of learners?
speech.
Elicit-ing native speakers?
data and rating it for grammarusage (supervised approach) can be arbitrary, sincethere can be a very wide range of possible grammat-ical structures that native speakers utilize.
Instead,we proceed in a semi-supervised fashion.
A largeamount of learners?
spoken responses were collectedand classified into four groups according to theirproficiency level.
We then sought to find how dis-tinct the proficiency classes were based on the distri-bution of POS tags.
Given a student?s response, wecalculated the similarity with a sample of responsesfor each score level based on the proportion and dis-tribution of Part-of-Speech using NLP techniques.POS tag distribution has been used in varioustasks such as text genre classification (Feldman etal., 2009); in a language testing context, it has beenused in grammatical error detection (Chodorow andLeacock, 2000; Tetreault and Chodorow, 2008) andessay scoring.
Recently, Roark et al2011) ex-plored POS tag distribution to capture the differ-ences in syntactic complexity between healthy sub-jects and subjects with mild cognitive impairment,but no other research has used POS tag distributionin measuring syntactic complexity, to the best of au-thors?
knowledge.An assessment of ESL learners?
syntactic compe-tence should consider the structure of sentences as awhole - a task which may not be captured by the sim-plistic POS tag distribution.
However, studies of Lu(2010) and Chen and Zechner (2011) showed thatmore complex syntactic features are unreliable inASR-based scoring system.
Furthermore, we showthat POS unigrams or bigrams indeed capture a rea-sonable portion of learners?
range and sophisticationof grammar usage in our discussion in Section 7.This paper will proceed as follows: we will re-view related work in Section 2 and present themethod to calculate syntactic complexity in Section3.
Data and experiment setup will be explained inSection 4 and Section 5.
The results will be pre-sented in Section 6.
Finally, in Section 7, we discussthe levels of syntactic competence that are capturedusing our proposed measure.2 Related WorkSecond Language Acquisition (SLA) researchershave developed many quantitative measures to es-timate the level of acquisition of syntactic compe-tence.
Bardovi-Harlig and Bofman (1989) classi-fied these measures into two groups.
The first groupis related to the acquisition of specific morphosyn-tactic features or grammatical expressions.
Tests ofnegations or relative clauses - whether these expres-sions occurred in the test responses without errors -fell into this group (hereafter, the expression-basedgroup).
The second group is related to the length ofthe clause or the relationship between clauses andhence not tied to particular structures (hereafter, thelength-based group).
Examples of the second groupmeasures include the average length of clause unitand dependent clauses per sentence unit.These syntactic measures have been extensivelystudied in ESL writing.
Ortega (2003) synthesized25 research studies which employed syntactic mea-sures on ESL writing and reported a significant re-lationship between the proposed features and writ-ing proficiency.
He reported that a subset of featuressuch as the mean length of the clause unit increasedwith students?
proficiency.
More recently, Lu (2010)601has conducted a more systematic study using an au-tomated system.
He applied 14 syntactic measuresto a large database of Chinese learners?
writing sam-ples and found that syntactic measures were strongpredictors of students?
writing proficiency.Studies in the area of automated speech scor-ing have only recently begun to actively investi-gate the usefulness of syntactic measures for scoringspontaneous speech (Chen et al2010; Bernsteinet al2010).
These have identified clause bound-aries (identified from manual annotations and au-tomatically) and obtained length-based features.
Inaddition to these conventional syntactic complexityfeatures, Lu (2009) implemented an automated sys-tem that calculates the revised Developmental Level(D-Level) Scale (Covington et al2006) using nat-ural language processing (NLP) techniques.
Theoriginal D-Level Scale was proposed by Rosenbergand Abbeduto (1987) based primarily on observa-tions of child language acquisition.
They classifiedchildren?s grammatical acquisition into 7 differentgroups according to the presence of certain types ofcomplex sentences.
The revised D-Level Scale clas-sified sentences into the eight levels according to thepresence of particular grammatical expressions.
Forinstance, level 0 is comprised of simple sentences,while level 5 is comprised of sentences joined bysubordinating conjunction or nonfinite clauses in anadjunct position.
The D-Level Scale has been lessstudied in the speech scoring.
To our knowledge,Chen and Zechner (2011) is the only study that ap-plied the D-Level analyzer to ESL learners?
spokenresponses.In contrast to ESL writing, applying syntacticcomplexity features, both conventional length-basedfeatures and D-Level features, presents serious ob-stacles for speaking.
First, the length of the spo-ken responses are typically shorter than written re-sponses.
Most measures are based on sentence orsentence-like units, and in speaking tests that elicitonly a few sentences the measures are less reli-able.
Chen and Yoon (2011) observed a markeddecrease in correlation between syntactic measuresand proficiency as response length decreased.
Inaddition, speech recognition errors only worsen thesituation.
Chen and Zechner (2011) showed thatthe significant correlation between syntactic mea-sures and speech proficiency (correlation coefficient= 0.49) became insignificant when they were appliedto the speech recognition word hypotheses.
Errorsin speech recognition seriously influenced the mea-sures and decreased the performance.
Due to theseproblems, the existing syntactic measures do notseem reliable enough for being used in automatedspeech proficiency scoring.In this study, we propose novel syntactic measureswhich are relatively robust against speech recogni-tion errors and are reliable in short responses.
Incontrast to recent studies focusing on length-basedfeatures, we focus on capturing differences in thedistribution of morphosyntactic features or gram-matical expressions across proficiency levels.
We in-vestigate the distribution of a broader class of gram-matical forms through the use of corpus-based NLPtechniques.3 MethodMany previous studies, that assess syntactic com-plexity based on the distribution of morpho-syntactic features and grammatical expressions, lim-ited their experiments to a few grammatical expres-sions.
Covington et al2006) and Lu (2009) cov-ered all sentence types, but their approaches werebased on expert observation (supervised rubrics),and descriptions of each level were brief and ab-stract.
It is important to develop a more detailed andrefined scale, but developing scales in a supervisedway is difficult due to the subjectivity and the com-plexity of structures involved.In order to overcome this problem, we employedNLP technology and a corpus-based approach.
Wehypothesize that the level of acquired grammaticalforms is signaled by the distribution of the POS tags,and the differences in grammatical proficiency re-sult in differences in POS tag distribution.
Based onthis assumption, we collected large amount of ESLlearners?
spoken responses and classified them intofour groups according to their proficiency levels.The syntactic competence was estimated based onthe similarity between the test responses and learn-ers?
corpus.A POS-based vector space model (VSM), inwhich the response belonging to separate profi-ciency levels were converted to vectors and the sim-ilarity between vectors were calculated using cosine602similarity measure and tf-idf weighting, was em-ployed.
Such a score-category-based VSM has beenused in automated essay scoring.
Attali and Burstein(2006) to assess the lexical content of an essay bycomparing the words in the test essay with the wordsin a sample essays from each score category.
Weextend this to assessment of grammar usage usingvectors of POS tags.Proficient speakers use complicated grammati-cal expressions, while beginners use simple expres-sions and sentences with frequent grammatical er-rors.
POS tags (or sequences) capturing these ex-pressions may be seen in corresponding proportionsin each score group.
These distributional differencesare captured by inverse-document frequency.In addition, we identify frequent POS tag se-quences as those having high mutual informationand include them in our experiments.
Temple (2000)pointed out that the proficient learners are charac-terized by increased automaticity in speech produc-tion.
These speakers tend to memorize frequentlyused multi-word sequences as a chunk and retrievethe whole chunk as a single unit.
The degree of auto-maticity can be captured by the frequent occurrenceof POS sequences with high mutual information.We quantify the usefulness of the generated fea-tures for the purpose of automatic scoring by firstconsidering its correlation with the human scores.We then compare the performance of our featureswith those in Lu (2011), where the features are acollection of measures of syntactic complexity thathave shown promising directions in previous stud-ies.4 DataTwo different sets of data were used in this study:the AEST 48K dataset and AEST balanced dataset.Both were collections of responses from the AEST,a high-stakes test of English proficiency and hadno overlaps.
The AEST assessment consists of 6items in which speakers are prompted to provide re-sponses lasting between 45 and 60 seconds per item.In summary, approximately 3 minutes of speech iscollected per speaker.Among the 6 items, two items are tasks that askexaminees to provide information or opinions on fa-miliar topics based on their personal experience orbackground knowledge.
The four remaining itemsare integrated tasks that include other language skillssuch as listening and reading.
All items extractspontaneous, unconstrained natural speech.
Thesize, purpose, and speakers?
native language infor-mation for each dataset is summarized in Table 1.Each response was rated by trained human ratersusing a 4-point scoring scale, where 1 indicatesa low speaking proficiency and 4 indicates a highspeaking proficiency.
In order to evaluate the relia-bility of the human ratings, the data should be scoredby two raters.
Since none of the AEST balanceddata was double scored the inter-rater agreement ra-tio was estimated using a large (41K) double-scoreddataset using the same scoring guidelines and scor-ing process.
The Pearson correlation coefficient was0.63 suggesting a reasonable inter-rater agreement.The distribution of the scores for this data can befound in Table 2.We used the AEST 48K dataset as the trainingdata and the AEST balanced dataset as the evalua-tion data.5 Experiments5.1 OverviewOur experimental procedure is as follows.
All tran-scriptions were tagged using the POS tagger de-scribed in Section 5.3 and POS tag sequences wereextracted.
Next, the POS-based VSMs (one foreach score class) were created using the AEST 48Kdataset.
Finally, for a given test response in theAEST balanced dataset, similarity features weregenerated.A score-class-specific POS-based VSM was cre-ated using POS tags generated from the manual tran-scriptions.
For evaluation, two different types oftranscriptions (manual transcription and word hy-potheses from the speech recognizer described inSection 5.2) were used in order to investigate the in-fluence of speech recognition errors in the featureperformance.5.2 Speech recognitionAn HMM recognizer was trained on AEST 48Kdataset - approximately 733 hours of non-nativespeech collected from 7872 speakers.
A gender in-dependent triphone acoustic model and combination603Corpus name Purpose Number ofspeakersNumber ofresponsesNative languages Size(Hrs)AEST 48KdataASR training andPOS model train-ing7872 47227 China (20%), Korea (19%),Japanese (7%), India (7%), oth-ers (46%)733AEST bal-anced dataFeature develop-ment and evalua-tion480 2880 Korean (15%), Chinese (14%),Japanese (7%), Spanish (9%),Others (55%)44Table 1: Data size and speakers native languagesCorpus name Size Score1 Score2 Score3 Score4AEST 48K data Number of files 1953 16834 23106 5334(%) 4 36 49 11AEST balanced data Number of files 141 1133 1266 340(%) 5 40 45 12Table 2: Proficiency scores and data sizesof bigram, trigram, and four-gram language modelswere used.
The word error rate (WER) on the held-out test dataset was 27%.5.3 POS taggerPOS tags were generated using the POS tagger im-plemented in the OpenNLP toolkit.
It was trainedon the Switchboard (SWBD) corpus.
This POS tag-ger was trained on about 528K word/tag pairs andachieved a tagging accuracy of 96.3% on a test setof 379K words.
The Penn POS tag set was used inthe tagger.5.4 Unit generation using mutual informationPOS bigrams with high mutual information were se-lected and used as a single unit.
First, all POS bi-grams which occurred less than 50 times were fil-tered out.
Next, the remaining POS tag bigramswere sorted by their mutual information scores, andtwo different sets (top50 and top110) were selected.The selected POS pairs were transformed into com-pound tags.
As a result, we generated three setsof POS units by this process: the original POS setwithout the compound unit (Base), the original setand an additional 50 compound units (Base+mi50),and the original set and an additional 110 units(Base+mi110).Finally, unigram, bigram and trigram were gener-ated for each set separately.
The size of total termsin each condition was presented in table 3.Base Base+mi50 Base+mi110Unigram 42 93 151Bigram 1366 4284 9691Trigram 21918 54856 135430Table 3: Number of terms used in VSMs5.5 Building VSMsFor each ngram, three sets of VSMs were built us-ing three sets of tags as terms, yielding a total ofnine VSMs.
The results were based on the individ-ual model and we did not combine any models.5.6 Cosine similarity-based featuresThe cosine similarity has been frequently used inthe information retrieval field to identify the relevantdocuments for the given query.
This measures thesimilarity between a given query and a document bymeasuring the cosine of the angle between vectors ina high-dimensional space, whereby each term in thequery and documents corresponding to a unique di-mension.
If a document is relevant to the query, thenit shares many terms resulting in a small angle.
Inthis study, the term was a single or compound POStag (unigram,bigram or trigram) weighted by its tf-idf, and the document was the response.First, the inverse document frequency was calcu-lated from the training data, and each response wastreated as a document.
Next, responses in the same604Unigram Bigram TrigramBase Base+mi50Base+mi110Base Base+mi50Base+mi110Base Base+mi50Base+mi110Trans-cription0.301** 0.297** 0.329** 0.427** 0.361** 0.366** 0.402** 0.322** 0.295**ASR 0.246** 0.272** 0.304** 0.415** 0.348** 0.347** 0.373** 0.311** 0.282**Table 4: Pearson correlation coefficients between ngram-based features and expert proficiency scores** Correlation is significant at the 0.01 levelscore group were concatenated, and a single vectorwas generated for each score group.
A total of 4vectors were generated using training data.
For eachtest response, a similarity score was calculated asfollows:cos(~q, ~dj) =nPi=1qidjinPi=1qi2nPi=1di2qi ?
tf(ti, ~q)?
log(Ndf(ti))dji ?
tf(ti, ~dj)?
log(Ndf(ti))where ~q is a vector of the test response,~dj is a vector of the scoreGroupj ,n is the total number of POS tags,tf(ti, ~q) is the term frequency of POS tag ti in thetest response,tf(ti, ~dj) is the term frequency of POS tag ti in thescoreGroupj ,N is the total number of training responses,df(ti) is the document frequency of POS tag ti inthe total training responsesFinally, a total of 4 cos scores (one per scoregroup) were generated.
Among these four values,the cos4, the similarity score to the responses in thescore group 4, was selected as a feature with the fol-lowing intuition.
cos4 measures the similarity of agiven test response to the representative vector ofscore class 4; the larger the value, the closer it wouldbe to score class 4.6 Results6.1 CorrelationTable 4 shows correlations between cosine similarityfeatures and proficiency scores rated by experts.The bigram-based features outperformed bothunigram-based and trigram-based features.
In par-ticular, the similarities using the base tag set withbigrams achieved the best performance.
By addingthe mutual information-based compound units to theoriginal POS tag sets, the performance of featuresimproved in the unigram models.
However, therewas no performance gain in either bigram or tri-gram models; on the contrary, there was a largedrop in performance.
Unigrams have good coveragebut limited power in distinguishing different scorelevels.
On the other hand, trigrams have oppositecharacteristics.
Bigrams seem to strike a balancein both coverage and complexity (from among thethree considered here) and may thus have resulted inthe best performace.The performance of ASR-based features werecomparable to that of transcription-based features.The best performing feature among ASR-based-features were from the bigram and base set, withcorrelations nearly the same as the best performingone among the transcription-based-features.
See-ing how close the correlations were in the case oftranscription-based and ASR-hypothesis based fea-ture extraction, we conclude that the proposed mea-sure is robust to ASR errors.6.2 Comparison with other Measures ofSyntactic ComplexityWe compared the performance of our features withthe features of syntactic complexity proposed in (Lu,2011).
Towards this, the clause boundaries of theASR hypotheses, were automatically detected usingthe automated clause boundary detection method1.1The automated clause boundary detection method in thisstudy was a Maximum Entropy Model based on word bigrams,POS tag bigrams, and pause features.
The method achieved an605The utterances were then parsed using the StanfordParser, and a total of 22 features including bothlength-related features and parse-tree based featureswere generated using (Lu, 2011).
Finally, we calcu-lated Pearson correlation coefficients between thesefeatures and human proficiency scores.Study Feature CorrelationCurrent study bigram based cos4 0.41**(Lu, 2011) DCC 0.14**Table 5: Comparison between (Lu, 2011) and this study** Correlation is significant at the 0.01 levelAs indicated in Table 5, the best performing fea-ture was mean number of dependent clauses perclause (DCC) and the correlation r was 0.14.
Nofeatures other than DCC achieved statistically sig-nificant correlation.
Our best performing feature (bi-gram based cos4) widely outperformed the best ofLu (2011)?s features (correlations approximately 0.3apart).A logical explanation for the poor performance ofLu (2011)?s features is that the features are gener-ated using multi-stage automated process, and theerrors in each process contributes the low featureperformance.
For instance, the errors in the auto-mated clause boundary detection may result in a se-rious drop in the performance.
With the spoken re-sponses being particularly short (a typical responsein the data set had 10 clauses on average), even oneerror in clause boundary detection can seriously af-fect the reliability of features.7 DiscussionWhile the measure of syntactic competence that westudy here is an abstraction of the overall syntacticcompetence, without consideration of specific con-structions, we analyzed the results further with theintention of casting light on the level of details ofsyntactic competence that can be explained usingour measure.
Furthermore, this section will showthat bigram POS sequences can yield significant in-formation on the range and sophistication of gram-mar usage in the specific assessment context (spon-F-score of 0.60 on the non-native speakers?
ASR hypotheses.A detailed description of the method is presented in (Chen andZechner, 2011)taneous speech comprised of only declarative sen-tences).ESL speakers with high proficiency scores are ex-pected to use more complicated grammatical expres-sions that result in a high proportion of POS tagsrelated to these expressions in that score group.
Thedistribution of POS tags was analyzed in detail in or-der to investigate whether there were systematic dis-tributional changes according to proficiency levels.Owing to space constraints, we restrict our discus-sion to the analysis using unigrams (base and com-pund).
For each score group, the POS tags weresorted based on the frequencies in training data, andthe rank orders were calculated.
The more frequentthe POS tag, the higher its rank.A total of 150 POS tags, including the originalPOS tag set and top 110 compound tags, were clas-sified into 5 classes:?
Absence-of-low-proficiency (ABS): Group ofPOS tags that appear in all score groups exceptthe lowest proficiency group;?
Increase (INC): Group of POS tags whoseranks increase consistently as proficiency in-creases;?
Decrease (DEC): Group of POS tags whoseranks decrease consistently as proficiency in-creases;?
Constant (CON): Group of POS tags whoseranks remain same despite change in profi-ciency;?
Mix: Group of POS tags of with no consistentpattern in the ranks.Table 6 presents the number of POS tags in eachclass.ABS INC DEC CON Mix14 37 33 18 48Table 6: Tag distribution and proficiency scoresThe ?ABS?
class mostly consists of ?WP?
and?WDT?
; more than 50% of tags in this class are re-lated to these two tags.
?WP?
is a Wh-pronoun while?WDT?
is a Wh-determiner.
Since most sentences in606our data are declarative sentences, ?Wh?
phrase sig-nals the use of relative clause.
Therefore, the lackof these tags strongly support the hypothesis that thespeakers in score group 1 showed incompetence inthe use of relative clauses or their use in limited sit-uations.The ?INC?
class can be sub-classified into threegroups: verb, comparative, and relative clause.
Verbgroup is includes the infinitive (TO VB), passive(VB VBN, VBD VBN, VBN, VBN IN, VBN RP),and gerund forms (VBG, VBG RP, VBG TO).
Next,the comparative group encompasses comparativeconstructions.
Finally, the relative clause group sig-nals the presence of relative clauses.
The increasedproportion of these tags reflects the use of morecomplicated tense forms and modal forms as wellas more frequent use of relative clauses.
It supportsthe hypothesis that speakers with higher proficiencyscores tend to use more complicated grammaticalexpressions.The ?DEC?
class can be sub-classified into fivegroups: noun, simple tense verb, GW and UH,non-compound, and comparative.
The noun groupis comprised of many noun or proper noun-relatedexpressions, and their high proportions are consis-tent with the tendency that less proficient speakersuse nouns more frequently.
Secondly, the simpletense verb group is comprised of the base form (VB)and simple present and past forms(PRP VBD, VB,VBD TO, VBP TO, VBZ).
The expressions in thesegroups are simpler than those in ?Increase?
group.The ?UH?
tag is for interjection and filler wordssuch as ?uh?
and ?um?, while the ?GW?
tag is forword-fragments.
These two spontaneous speechphenomena are strongly related to fluency, and itsignals problems in speech production.
Frequentoccurrences of these two tags are evidence of fre-quent planning problems and their inclusion in the?DEC?
class suggests that instances of speech plan-ning problems decrease with increased proficiency.Tags in the non-compound group, such as ?DT?,?MD?, ?RBS?, and ?TO?, have related compoundtags.
The non-compound tags are associated withthe expressions that do not co-occur with stronglyrelated words, and they tend to be related to errors.For instance, the non-compound ?MD?
tag signalsthat there is an expression that a modal verb is notfollowed by ?VB?
(base form) and as seen in the ex-amples, ?the project may can change?
and ?the othersmust can not be good?, they are related to grammat-ical errors.Finally, the comparative group includes?RBR JJR?.
The decrease of ?RBR JJR?
is re-lated to the correct acquisition of the comparativeform.
?RBR?
is for comparative adverbs and ?JJR?
isfor comparative adjectives, and the combination oftwo tags is strongly related to double-marked errorssuch as ?more easier?.
In the intermediate stage inthe acquisition of comparative form, learners tendto use the double-marked form.
The compound tagscorrectly capture this erroneous stage.The ?Decrease?
class also includes three Wh-related tags (WDT NN, WDT VBP, WRB), but theproportion is much smaller than the ?Increase?
class.The above analysis shows that the combination oforiginal and compound POS tags correctly capturesystematic changes in the grammatical expressionsaccording to changes in proficiency levels.The robust performance of our proposed mea-sure to speech recognition errors may be better ap-preciated in the context of similar studies.
Com-pared with the state-of-the art measures of syntac-tic complexity proposed in Lu (2011) our featuresachieve significantly better performance especiallywhen generated from ASR hypotheses.
It is tobe noted that the performance drop between thetranscription-based feature and the ASR hypothesis-based feature was marginal.8 ConclusionsIn this paper, we presented features that measuresyntactic competence for the automated speech scor-ing.
The features measured the range and sophisti-cation of grammatical expressions based on POS tagdistributions.
A corpus with a large number of learn-ers?
responses was collected and classified into fourgroups according to proficiency levels.
The syntac-tic competence of the test response was estimated byidentifying the most similar group from the learners?corpus.
Furthermore, speech recognition errors onlyresulted in a minor performance drop.
The robust-ness against speech recognition errors is an impor-tant advantage of our method.607AcknowledgmentsThe authors would like to thank Shasha Xie, KlausZechner, and Keelan Evanini for their valuable com-ments, help with data preparation and experiments.ReferencesYigal Attali and Jill Burstein.
2006.
Automated essayscoring with e?rater R v.2.
The Journal of Technology,Learning, and Assessment, 4(3).Kathleen Bardovi-Harlig and Theodora Bofman.
1989.Attainment of syntactic and morphological accuracyby advanced language learners.
Studies in SecondLanguage Acquisition, 11:17?34.Jared Bernstein, Jian Cheng, and Masanori Suzuki.
2010.Fluency and structural complexity as predictors of L2oral proficiency.
In Proceedings of InterSpeech 2010,Tokyo, Japan, September.Lei Chen and Su-Youn Yoon.
2011.
Detecting structuralevents for assessing non-native speech.
In Proceed-ings of the 6th Workshop on Innovative Use of NLP forBuilding Educational Applications, pages 38?45.Miao Chen and Klaus Zechner.
2011.
Computing andevaluating syntactic complexity features for automatedscoring of spontaneous non-native speech.
In Pro-ceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics 2011, pages 722?731.Lei Chen, Joel Tetreault, and Xiaoming Xi.
2010.Towards using structural events to assess non-nativespeech.
In Proceedings of the NAACL HLT 2010 FifthWorkshop on Innovative Use of NLP for Building Ed-ucational Applications, pages 74?79.Martin Chodorow and Claudia Leacock.
2000.
An unsu-pervised method for detecting grammatical errors.
InIn Proceedings of NAACL00, pages 140?147.Michael A. Covington, Congzhou He, Cati Brown, Lo-rina Naci, and John Brown.
2006.
How complexis that sentence?
A proposed revision of the Rosen-berg and Abbeduto D-Level Scale.
Technical report,CASPR Research Report 2006-01, Athens, GA: TheUniversity of Georgia, Artificial Intelligence Center.Catia Cucchiarini, Helmer Strik, and Lou Boves.
2000.Quantitative assessment of second language learners?fluency: Comparisons between read and spontaneousspeech.
The Journal of the Acoustical Society of Amer-ica, 107(2):989?999.Catia Cucchiarini, Helmer Strik, and Lou Boves.
2002.Quantitative assessment of second language learners?fluency: Comparisons between read and spontaneousspeech.
The Journal of the Acoustical Society of Amer-ica, 111(6):2862?2873.Sergey Feldman, M.A.
Marin, Maria Ostendorf, andMaya R. Gupta.
2009.
Part-of-speech histograms forgenre classification of text.
In Acoustics, Speech andSignal Processing, 2009.
ICASSP 2009.
IEEE Interna-tional Conference on, pages 4781 ?4784, april.Horacio Franco, Leonardo Neumeyer, Yoon Kim, andOrith Ronen.
1997.
Automatic pronunciation scoringfor language instruction.
In Proceedings of ICASSP97, pages 1471?1474.Xiaofei Lu.
2009.
Automatic measurement of syntac-tic complexity in child language acquisition.
Interna-tional Journal of Corpus Linguistics, 14(1):3?28.Xiaofei Lu.
2010.
Automatic analysis of syntacticcomplexity in second language writing.
InternationalJournal of Corpus Linguistics, 15(4):474?496.Xiaofei Lu.
2011.
L2 syntactic complex-ity analyze.
Retrieved March 17, 2012 fromhttp://www.personal.psu.edu/xxl13/downloads/l2sca.html/.Leonardo Neumeyer, Horacio Franco, Vassilios Di-galakis, and Mitchel Weintraub.
2000.
Automaticscoring of pronunciation quality.
Speech Communi-cation, pages 88?93.Lourdes Ortega.
2003.
Syntactic complexity measuresand their relationship to L2 proficiency: A researchsynthesis of college?level L2 writing.
Applied Lin-guistics, 24(4):492?518.Brian.
Roark, Margaret Mitchell, John-Paul.
Hosom,Kristy Hollingshead, and Jeffrey Kaye.
2011.
Spo-ken language derived measures for detecting mild cog-nitive impairment.
Audio, Speech, and LanguageProcessing, IEEE Transactions on, 19(7):2081 ?2090,sept.Sheldon Rosenberg and Leonard Abbeduto.
1987.
Indi-cators of linguistic competence in the peer group con-versational behavior of mildly retarded adults.
AppliedPsycholinguistics, 8:19?32.Liz Temple.
2000.
Second language learner speech pro-duction.
Studia Linguistica, pages 288?297.Joel R. Tetreault and Martin Chodorow.
2008.
The upsand downs of preposition error detection in esl writing.In In Proceedings of COLING.Silke Witt and Steve Young.
1997.
Performance mea-sures for phone-level pronunciation teaching in CALL.In Proceedings of the Workshop on Speech Technologyin Language Learning, pages 99?102.Silke Witt.
1999.
Use of the speech recognition incomputer-assisted language learning.
Unpublisheddissertation, Cambridge University Engineering de-partment, Cambridge, U.K.Klaus Zechner, Derrick Higgins, Xiaoming Xi, andDavid M. Williamson.
2009.
Automatic scoring ofnon-native spontaneous speech in tests of spoken en-glish.
Speech Communication, 51:883?895, October.608
