Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1200?1209,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsFine-Grained Class Label Markup of Search QueriesJoseph Reisinger?Department of Computer SciencesThe University of Texas at AustinAustin, Texas 78712joeraii@cs.utexas.eduMarius Pas?caGoogle Inc.1600 Amphitheatre ParkwayMountain View, California 94043mars@google.comAbstractWe develop a novel approach to the seman-tic analysis of short text segments and demon-strate its utility on a large corpus of Websearch queries.
Extracting meaning from shorttext segments is difficult as there is littlesemantic redundancy between terms; hencemethods based on shallow semantic analy-sis may fail to accurately estimate meaning.Furthermore search queries lack explicit syn-tax often used to determine intent in ques-tion answering.
In this paper we propose ahybrid model of semantic analysis combin-ing explicit class-label extraction with a la-tent class PCFG.
This class-label correlation(CLC) model admits a robust parallel approxi-mation, allowing it to scale to large amounts ofquery data.
We demonstrate its performancein terms of (1) its predicted label accuracy onpolysemous queries and (2) its ability to accu-rately chunk queries into base constituents.1 IntroductionSearch queries are generally short and rarely containmuch explicit syntax, making query understanding apurely semantic endeavor.
Furthermore, as in noun-phrase understanding, shallow lexical semantics isoften irrelevant or misleading; e.g., the query [trop-ical breeze cleaners] has little to do with island va-cations, nor are desert birds relevant to [1970 roadrunner], which refers to a car model.This paper introduces class-label correlation(CLC), a novel unsupervised approach to extract-?Contributions made during an internship at Google.ing shallow semantic content that combines class-based semantic markup (e.g., road runner is a carmodel) with a latent variable model for capturingweakly compositional interactions between queryconstituents.
Constituents are tagged with IsA classlabels from a large, automatically extracted lexicon,using a probabilistic context free grammar (PCFG).Correlations between the resulting label?term dis-tributions are captured using a set of latent produc-tion rules specified by a hierarchical Dirichlet Pro-cess (Teh et al, 2006) with latent data groupings.Concretely, the IsA tags capture the inventoryof potential meanings (e.g., jaguar can be labeledas european car or large cat) and relevant con-stituent spans, while the latent variable model per-forms sense and theme disambiguation (e.g., [jaguarhabitat] would lend evidence for the large cat la-bel).
In addition to broad sense disambiguation, CLCcan distinguish closely related usages, e.g., the useof dell in [dell motherboard replacement] and [dellstock price].1 Furthermore, by employing IsA classlabeling as a preliminary step, CLC can account forcommon non-compositional phrases, such as big ap-ple unlike systems relying purely on lexical seman-tics.
Additional examples can be found later, in Fig-ure 5.In addition to improving query understanding, po-tential applications of CLC include: (1) relation ex-traction (Baeza-Yates and Tiberi, 2007), (2) querysubstitutions or broad matching (Jones et al, 2006),and (3) classifying other short textual fragmentssuch as SMS messages or tweets.We implement a parallel inference procedure for1Dell the computer system vs. Dell the technology company.1200CLC and evaluate it on a sample of 500M searchqueries along two dimensions: (1) query constituentchunking precision (i.e., how accurate are the in-ferred spans breaks; cf., Bergsma and Wang (2007);Tan and Peng (2008)), and (2) class label assign-ment precision (i.e., given the query intent, how rel-evant are the inferred class labels), paying particu-lar attention to cases where queries contain ambigu-ous constituents.
CLC compares favorably to sev-eral simpler submodels, with gains in performancestemming from coarse-graining related class labelsand increasing the number of clusters used to cap-ture between-label correlations.
(Paper organization): Section 2 discusses relevantbackground, Section 3 introduces the CLC model,Section 4 describes the experimental setup em-ployed, Section 5 details results, Section 6 intro-duces areas for future work and Section 7 concludes.2 BackgroundQuery understanding has been studied extensivelyin previous literature.
Li (2010) defines the se-mantic structure of noun-phrase queries as intentheads (attributes) coupled with some number of in-tent modifiers (attribute values), e.g., the query [al-ice in wonderland 2010 cast] is comprised of an in-tent head cast and two intent modifiers alice in won-derland and 2010.
In this work we focus on seman-tic class markup of query constituents, but our ap-proach could be easily extended to account for querystructure as well.Popescu et al (2010) describe a similar class-label-based approach for query interpretation, ex-plicitly modeling the importance of each label fora given entity.
However, details of their implemen-tation were not publicly available, as of publicationof this paper.For simplicity, we extract class labels using theseed-based approach proposed by Van Durme andPas?ca (2008) (in particular Pas?ca (2010)) which gen-eralizes Hearst (1992).
Talukdar and Pereira (2010)use graph-based semi-supervised learning to acquireclass-instance labels; Wang et al (2009) introduce asimilar CRF-based approach but only apply it to asmall number of verticals (i.e., Computing and Elec-tronics or Clothing and Shoes).
Snow et al (2006)describe a learning approach for automatically ac-quiring patterns indicative of hypernym (IsA) rela-tions.
Semantic class label lexicons derived fromany of these approaches can be used as input to CLC.Several authors have studied query clustering inthe context of information retrieval (e.g., Beefermanand Berger, 2000).
Our approach is novel in thisregard, as we cluster queries in order to capture cor-relations between span labels, rather than explicitlyfor query understanding.Tratz and Hovy (2010) propose a taxonomy forclassifying and interpreting noun-compounds, fo-cusing specifically on the relationships holding be-tween constituents.
Our approach yields similar top-ical decompositions of noun-phrases in queries andis completely unsupervised.Jones et al (2006) propose an automatic methodfor query substitution, i.e., replacing a given querywith another query with the similar meaning, over-coming issues with poor paraphrase coverage in tailqueries.
Correlations mined by our approach arereadily useful for downstream query substitution.Bergsma and Wang (2007) develop a super-vised approach to query chunking using 500 hand-segmented queries from the AOL corpus.
Tan andPeng (2008) develop a generative model of querysegmentation that makes use of a language modeland concepts derived from Wikipedia article titles.CLC differs fundamentally in that it learns con-cept label markup in addition to segmentation anduses in-domain concepts derived from queries them-selves.
This work also differs from both of thesestudies significantly in scope, training on 500Mqueries instead of just 500.At the level of class-label markup, our model isrelated to Bayesian PCFGs (Liang et al, 2007; John-son et al, 2007b), and is a particular realization of anAdaptor Grammar (Johnson et al, 2007a; Johnson,2010).Szpektor et al (2008) introduce a model of con-textual preferences, generalizing the notion of selec-tional preference (cf.
Ritter et al, 2010) to arbitraryterms, allowing for context-sensitive inference.
Ourapproach differs in its use of class-instance labels forgeneralizing terms, a necessary step for dealing withthe lack of syntactic information in queries.1201?C?L?Lvinyl windowsbrightonseaside towns building materialsquery clusterslabel clusterslabel pcfgquery constituentsFigure 1: Overview of CLC markup generation forthe query [brighton vinyl windows].
Arrows denotemultinomial distributions.3 Latent Class-Label CorrelationInput to CLC consists of raw search queries and apartial grammar mapping class labels to query spans(e.g., building materials?vinyl windows).
CLC in-fers two additional latent productions types on topof these class labels: (1) a potentially infinite set oflabel clusters ?Llk coarse-graining the raw input labelproductions V , and (2) a finite set of query clusters?Cci specifying distributions over label clusters; seeFigure 1 for an overview.Operationally, CLC is implemented as a Hierar-chical Dirichlet Process (HDP; Teh et al, 2006) withlatent groups coupled with a Probabilistic ContextFree Grammar (PCFG) likelihood function (Figure2).
We motivate our use of an HDP latent classmodel instead of a full PCFG with binary produc-tions by the fact that the space of possible binaryrule combinations is prohibitively large (561K baselabels; 314B binary rules).
The next sections discussthe three main components of CLC: ?3.1 the raw IsAclass labels, ?3.2 the PCFG likelihood, and ?3.3 theHDP with latent groupings.3.1 IsA Label ExtractionIsA class labels (hypernyms) V are extracted froma large corpus of raw Web text using the methodproposed by Van Durme and Pas?ca (2008) and ex-tended by Pas?ca (2010).
Manually specified patternsare used to extract a seed set of class labels and theresulting label lists are reranked using cluster puritymeasures.
561K labels for base noun phrases arecollected.
Table 1 shows an example set of classlabels extracted for several common noun phrases.Similar repositories of IsA labels, extracted usingother methods, are available for experimental pur-class label?query spanrecreational facilities?jacuzzirural areas?walesdestinations?walesseaside towns?brightonbuilding materials?vinyl windowsconsumer goods?european clothingTable 1: Example production rules collected usingthe semi-supervised approach of Van Durme andPas?ca (2008).poses (Talukdar and Pereira, 2010).
In addition toextracted rules, the CLC grammar is augmented witha set of null rules, one per unigram, ensuring thatevery query has a valid parse.3.2 Class-Label PCFGIn addition to the observed class-label productionrules, CLC incorporates two sets of latent produc-tion rules coupled via an HDP (Figure 1).
Classlabel?query span productions extracted from rawtext are clustered into a set of latent label produc-tion clusters L = {l1, .
.
.
, l?}.
Each label pro-duction cluster lk defines a multinomial distributionover class labels V parametrized by ?Llk .
Conceptu-ally, ?Llk captures a set of class labels with similarproductions that are found in similar queries, for ex-ample the class labels states, northeast states, u.s.states, state areas, eastern states, and certain statesmight be included in the same coarse-grained clusterdue to similarities in their productions.Each query q ?
Q is assigned to a latent querycluster cq ?
C{c1, .
.
.
, c?
}, which defines a dis-tribution over label production clusters L, denoted?Ccq .
Query clusters capture broad correlations be-tween label production clusters and are necessary forperforming sense disambiguation and capturing se-lectional preference.
Query clusters and label pro-duction clusters are linked using a single HDP, al-lowing the number of label clusters to vary over thecourse of Gibbs sampling, based on the variance ofthe underlying data (Section 3.3).
Viewed as a gram-mar, CLC only contains unary rules mapping labelsto query spans; production correlations are captureddirectly by the query cluster, unlike in HDP-PCFG(Liang et al, 2007), as branching parses over the en-1202Indices CardinalityHDP base measure ?
?
GEM(?)
- |L| ?
?Query cluster ?Ci ?
DP(?C ,?)
i ?
|C| |L| ?
?Label cluster ?Lk ?
Dirichlet(?L) k ?
|L| |V |Query cluster indpiq ?
Dirichlet(?)
q ?
|Q| |C|cq ?
piq q ?
|Q| 1Label cluster ind zq,t ?
?Ccq t ?
q, q ?
|Q| 1Label ind lq,t ?
?Lzq,t t ?
q, q ?
|Q| 1cz?qtl!L???
?label clusters!C|C|?0query clusters?Figure 2: Generative process and graphical model for CLC.
The top section of the model is the standardHDP prior; the middle section is the additional machinery necessary for modeling latent groupings and thebottom section contains the indicators for the latent class model.
PCFG likelihood is not shown.tire label sparse are intractably large.Given a query q, a query cluster assignment cq anda set of label production clustersL, we define a parseof q to be a sequence of productions tq forming aparse tree consuming all the tokens in q.
As withBayesian PCFGs (Johnson, 2010), the probability ofa tree tq is the product of the probabilities of theproduction rules used to construct itP (tq|?L,?C , cq) =?r?RqP (r|?Llr)P (lr|?Ccq)where Rq is the set of production rules used to de-rive tq, P (r|?Llr) is the probability of r given its labelcluster assignment lr, and P (lr|?Ccq) is the probabil-ity of label cluster lr in query cluster c.The probability of a query q is the sum of theprobabilities of the parse trees that can generate it,P (q|?L,?C , cq) =?
{t|y(t)=q}P (t|?L,?C , cq)where {t|y(t) = q} is the set of trees with q as theiryield (i.e., generate the string of tokens in q).3.3 Hierarchical Dirichlet Process with LatentGroupsWe complete the Bayesian generative specificationof CLC with an HDP prior linking ?C and ?L.
TheHDP is a Bayesian generative model of shared struc-ture for grouped data (Teh et al, 2006).
A set ofbase clusters ?
?
GEM(?)
is drawn from a Dirich-let Process with base measure ?
using the stick-breaking construction, and clusters for each group k,?
?
HDP-LG base-measure smoother; higher val-ues lead to more uniform mass over labelclusters.
?C ?
Query cluster smoothing; higher values leadto more uniform mass over label clusters.
?L ?
Label cluster smoothing; higher values leadto more label diversity within clusters.?
?
Query cluster assignment smoothing; highervalues lead to more uniform assignment.Table 2: CLC-HDP-LG hyperparameters.
?Ck ?
DP(?
), are drawn from a separate DirichletProcess with base measure ?, defined over the spaceof label clusters.
Data in each group k are condi-tionally independent given ?.
Intuitively, ?
definesa common ?menu?
of label clusters, and each querycluster ?Ck defines a separate distribution over thelabel clusters.In order to account for variable query-cluster as-signment, we extend the HDP model with latentgroupings piq ?
Dir(?)
for each query.
The re-sulting Hierarchical Dirichlet Process with LatentGroups (HDP-LG) can be used to define a set ofquery clusters over a set of (potentially infinite) baselabel clusters (Figure 2).
Each query cluster ?C (la-tent group) assigns weight to different subsets of theavailable label clusters ?L, capturing correlationsbetween them at the query level.
Each query q main-tains a distribution over query clusters piq, capturingits affinity for each latent group.
The full generativespecification of CLC is shown in Figure 2; hyperpa-rameters are shown in Table 2.In addition to the full joint CLC model, we evalu-1203ate several simpler models:1.
CLC-BASE ?
no query clusters, one label perlabel cluster.2.
CLC-DPMM ?
no query clusters, DPMM(?C)distribution over labels.3.
CLC-HDP-LG ?
full HDP-LG model with |C|query clusters over a potentially infinite num-ber of query clusters.as well as various hyperparameter settings.3.4 Parallel Approximate Gibbs SamplerWe perform inference in CLC via Gibbs sampling,leveraging Multinomial-Dirichlet conjugacy to inte-grate out pi, ?C and ?L (Teh et al, 2006; Johnsonet al, 2007b).
The remaining indicator variables c, zand l are sampled iteratively, conditional on all othervariable assignments.
Although there are an expo-nential number of parse trees for a given query, thisspace can be sampled efficiently using dynamic pro-gramming (Finkel et al, 2006; Johnson et al, 2007b)In order to apply CLC to Web-scale data, weimplement an efficient parallel approximate Gibbssampler in the MapReduce framework Dean andGhemawat (2004).
Each Gibbs iteration consistsof a single MapReduce step for sampling, followedby an additional MapReduce step for computingmarginal counts.
2 Relevant assignments c, z andl are stored locally with each query and are dis-tributed across compute nodes.
Each node is respon-sible only for resampling assignments for its localset of queries.
Marginals are fetched opportunisti-cally from a separate distributed hash server as theyare needed by the sampler.
Each Map step computesa single Gibbs step for 10% of the available data, us-ing the marginals computed at the previous step.
Byresampling only 10% of the available data each it-eration, we minimize the potentially negative effectsof using the previous step?s marginal distribution.4 Experimental Setup4.1 Query CorpusOur dataset consists of a sample of 450M En-glish queries submitted by anonymous Web users to2This approximation and architecture is similar to Smolaand Narayanamurthy (2010).Query lengthdensity0.10.20.30.42 4 6 8 10 12Figure 3: Distribution in the query corpus, bro-ken down by query length (red/solid=all queries;blue/dashed=queries with ambiguous spans); mostqueries contain between 2-6 tokens.Google.
The queries have an average of 3.81 tokensper query (1.7B tokens).
Single token queries are re-moved as the model is incapable of using context todisambiguate their meaning.
Figure 3 shows the dis-tribution of remaining queries.
During training, weinclude 10 copies of each query (4.5B queries total),allowing an estimate of the Bayes average posteriorfrom a single Gibbs sample.4.2 EvaluationsQuery markup is evaluated for phrase-chunking pre-cision (Section 5.1) and label precision (Section 5.2)by human raters across two different samples: (1)an unbiased sample from the original corpus, and(2) a biased sample of queries containing ambigu-ous spans.Two raters scored a total of 10K labels from 800spans across 300 queries.
Span labels were markedas incorrect (0.0), badspan (0.0), ambiguous (0.5),or correct (1.0), with numeric scores for label pre-cision as indicated.
Chunking precision is measuredas the percentage of labels not marked as badspan.We report two sets of precision scores depend-ing on how null labels are handled: Strict evaluationtreats null-labeled spans as incorrect, while Normalevaluation removes null-labeled spans from the pre-cision calculation.
Normal evaluation was includedsince the simpler models (e.g., CLC-BASE) tend toproduce a significantly higher number of null assign-ments.Model evaluations were broken down into max-imum a posteriori (MAP) and Bayes average esti-mates.
MAP estimates are calculated as the singlemost likely label/cluster assignment across all querycopies; all assignments in the sample are averaged1204%cluster moves0.00.20.40.60.850 100 150 200 250%labelmoves0.250.300.350.400.450.5050 100 150 200 250Gibbs iterations%null rules0.0400.0450.0500.0550.0600.0650.07050 100 150 200 250Figure 4: Convergence rates of CLC-BASE (red/solid), CLC-HDP-LG 100C,40L(green/dashed), CLC-HDP-LG 1000C,40L(blue/dotted) in terms of % of query cluster swaps,label cluster swaps and null rule assignments.to obtain the Bayes average precision estimate.35 ResultsA total of five variants of CLC were evaluated withdifferent combinations of |C| and HDP prior con-centration ?C (controlling the effective number oflabel clusters).
Referring to models in terms of theirparametrizations is potentially confusing.
There-fore, we will make use of the fact that models with?C = 1 yielded roughly 40 label clusters on aver-age, and models with ?C = 0.1 yielded roughly 200label clusters, naming model variants simply by thenumber of query and label clusters: (1) CLC-BASE,(2) CLC-DPMM 1C-40L, (3) CLC-HDP-LG 100C-40L, (4) CLC-HDP-LG 1000C-40L, and (5) CLC-HDP-LG 1000C-200L.
Figure 4 shows the modelconvergence for CLC-BASE, CLC-HDP-LG 100C-40L, and CLC-HDP-LG 1000C-40L.3We calculate the Bayes average precision estimates atthe top 10 (Bayes@10) and top 20 (Bayes@20) parse trees,weighted by probability.5.1 Chunking PrecisionChunking precision scores for each model areshown in Table 3 (average % of labels not markedbadspan).
CLC-HDP-LG 1000C-40L has the high-est precision across both MAP and Bayes esti-mates (?93% accuracy), followed by CLC-HDP-LG1000C-200L (?90% accuracy) and CLC-DPMM 1C-40L (?85%).
CLC-BASE performed the worst bya significant margin (?78%), indicating that labelcoarse-graining is more important than query clus-tering for chunking accuracy.
No significant dif-ferences in label chunking accuracy were found be-tween Bayes and MAP inference.5.2 Predicting Span LabelsThe full CLC-HDP-LG model variants obtain higherlabel precision than the simpler models, with CLC-HDP-LG 1000C-40L achieving the highest precisionof the three (?63% accuracy).
Increasing the num-ber of label clusters too high, however, significantlyreduces precision: CLC-HDP-LG 1000C-200L ob-tains only ?51% accuracy.
However, comparingto CLC-DPMM 1C-40L and CLC-BASE demonstratesthat the addition of label clusters and query clustersboth lead to gains in label precision.
These relativerankings are robust across strict and normal evalua-tion regimes.The breakdown over MAP and Bayes posteriorestimation is less clear when considering label pre-cision: the simpler models CLC-BASE and CLC-DPMM 1C-40L perform significantly worse thanBayes when using MAP estimation, while in CLC-HDP-LG the reverse holds.There is little evidence for correlation betweenprecision and query length (weak, not statisticallysignificant negative correlation using Spearman?s ?
).This result is interesting as the relative prevalenceof natural language queries increases with querylength, potentially degrading performance.
How-ever, we did find a strong positive correlation be-tween precision and the number of labels produc-tions applicable to a query, i.e., production rule fer-tility is a potential indicator of semantic quality.Finally, the histogram column in Table 3 showsthe distribution of rater responses for each model.In general, the more precise models tend to havea significantly lower proportion of missing spans1205Model Chunking Label Precision Ambiguous Label Precision Spearman?s ?Precision normal strict hist normal strict q. len # labelsClass-Label Correlation BaseBayes@10 78.7?1.1 37.7?1.2 35.8?1.2 35.4?2.0 33.2?1.9 -0.13 0.51?Bayes@20 78.7?1.1 37.7?1.2 35.8?1.2 35.4?2.0 33.2?1.9 -0.13 0.51?MAP 76.3?2.2 33.3?2.2 31.8?2.2 36.2?4.0 33.2?3.8 -0.13 0.52?Class-Label Correlation DPMM 1C 40LBayes@10 84.9?0.4 46.6?0.6 44.3?0.5 36.0?1.1 33.7?1.0 -0.05 0.25Bayes@20 84.8?0.4 47.4?0.5 45.2?0.5 37.8?1.0 35.5?1.0 -0.02 0.23MAP 84.1?0.8 42.6?1.0 40.5?0.9 11.2?1.3 10.6?1.3 -0.03 0.12Class-Label Correlation HDP-LG 100C 40LBayes@10 83.8?0.4 55.6?0.5 51.0?0.5 55.6?1.0 47.7?1.0 0.03 0.44?Bayes@20 83.6?0.4 56.9?0.5 52.3?0.5 57.4?1.0 49.8?0.9 0.04 0.41?MAP 82.7?0.5 58.5?0.5 53.6?0.5 60.4?1.1 51.5?1.0 0.02 0.41?Class-Label Correlation HDP-LG 1000C 40LBayes@10 93.1?0.2 61.1?0.3 60.0?0.3 43.2?0.9 40.2?0.9 -0.06 0.26?Bayes@20 92.8?0.2 62.6?0.3 61.7?0.3 44.9?0.8 42.2?0.8 -0.10 0.27?MAP 92.7?0.2 63.7?0.3 62.7?0.3 44.1?0.9 41.1?0.9 -0.12 0.28?Class-Label Correlation HDP-LG 1000C 200LBayes@10 90.3?0.5 50.9?0.8 48.6?0.7 45.8?1.5 42.5?1.3 -0.10 0.13Bayes@20 89.9?0.5 50.2?0.7 48.0?0.7 44.4?1.4 41.3?1.3 -0.08 0.11MAP 90.0?0.6 51.0?0.8 48.9?0.8 49.2?1.5 46.0?1.4 -0.07 0.04Table 3: Chunking and label precision across five models.
Confidence intervals are standard error; sparklinesshow distribution of precision scores (left is zero, right is one).
Hist shows the distribution of human ratingresponse (log y scale): green/first is correct, blue/second is ambiguous, cyan/third is missing and red/fourthis incorrect.
Spearman?s ?
columns give label precision correlations with query length (weak negative corre-lation) and the number of applicable labels (weak to strong positive correlation); dots indicate significance.
(blue/second bar; due to null rule assignment) in ad-ditional to more correct (green/first) and fewer in-correct (red/fourth) spans.5.3 High Polysemy SubsetWe repeat the analysis of label precision on a subsetof queries containing one of the manually-selectedpolysemous spans shown in Table 4.
The CLC-HDP-LG -based models still significantly outper-form the simpler models, but unlike in the broadersetting, CLC-HDP-LG 100C-40L significantly out-performs CLC-HDP-LG 1000C-40L, indicating thatlower query cluster granularity helps address poly-semy (Table 3).5.4 Error AnalysisFigure 5 gives examples of both high-precision andlow-precision queries markups inferred by CLC-HDP-LG.
In general, CLC performs well on querieswith clear intent head / intent modifier structure (Li,acapella, alamo, apple, atlas, bad, bank, batman,beloved, black forest, bravo, bush, canton, casino,champion, club, comet, concord, dallas, diamond,driver, english, ford, gamma, ion, lemon, man-hattan, navy, pa, palm, port, put, resident evil,ronaldo, sacred heart, saturn, seven, solution, so-pranos, sparta, supra, texas, village, wolf, youngTable 4: Samples from a list of 90 manually se-lected ambiguous spans used to evaluate model per-formance under polysemy.2010).
More complex queries, such as [never knowuntil you try quotes] or [how old do you have to bea bartender in new york] do not fit this model; how-ever, expanding the set of extracted labels to alsocover instances such as never know until you trywould mitigate this problem, motivating the use ofn-gram language models with semantic markup.A large number of mistakes made by CLC are1206Top10%Bottom20%Middle20%Figure 5: Examples of high- and low-precision query markups inferred by CLC-HDP-LG.
Black text is theoriginal query; lines indicate potential spans; small text shows potential labels colored and numbered bylabel cluster; small bar shows percentage of assignments to that label cluster.due to named-entity categories with weak seman-tics such as rock bands or businesses (e.g., [tropi-cal breeze cleaners], [cosmic railroad band] or [so-pranos cigars]).
When the named entity is commonenough, it is detected by the rule set, but for the longtail of named entities this is not the case.
One poten-tial solution is to use a stronger notion of selectionalpreference and slot-filling, rather than just relying oncorrelation between labels.Other examples of common errors include inter-preting weymouth in [weymouth train time table] asa town in Massachusetts instead of a town in the UK(lack of domain knowledge), and using lower qual-ity semantic labels (e.g., neighboring countries forfrance, or great retailers for target).6 Discussion and Future WorkAdding both latent label clusters (DPMM) and la-tent query clusters (extending to HDP-LG) improvechunking and label precision over the baseline CLC-BASE system.
The label clusters are important be-cause they capture intra-group correlations betweenclass labels, while the query clusters are importantfor capturing inter-group correlations.
However, thealgorithm is sensitive to the relative number of clus-ters in each case: Too many labels/label clusters rel-1207ative to the number of query clusters make it difficultto learn correlations (O(n2) query clusters are re-quired to capture pairwise interactions).
Too manyquery clusters, on the other hand, make the modelintractable computationally.
The HDP automates se-lecting the number of clusters, but still requires man-ual hyperparameter setting.
(Future Work) Many query slots have weak se-mantics and hence are misleading for CLC.
Forexample [pacific breeze cleaners] or [dale hartleysubaru] should be parsed such that the type of theleading slot is determined not by its direct content,but by its context; seeing subaru or cleaners aftera noun-phrase slot is a strong indicator of its type(dealership or shop name).
The current CLC modelonly couples these slots through their correlations inquery clusters, not directly through relative positionor context.
Binary productions in the PCFG or a dis-criminative learning model would help address this.Finally, we did not measure label coverage withrespect to a human evaluation set; coverage is use-ful as it indicates whether our inferred semantics arebiased with respect to human norms.7 ConclusionsWe introduced CLC, a set of latent variable PCFGmodels for semantic analysis of short textual seg-ments.
CLC captures semantic information in theform of interactions between clusters of automati-cally extracted class-labels, e.g., finding that place-names commonly co-occur with business-names.We applied CLC to a corpus containing 500M searchqueries, demonstrating its scalability and straight-forward parallel implementation using frameworkslike MapReduce or Hadoop.
CLC was able to chunkqueries into spans more accurately and infer moreprecise labels than several sub-models even across ahighly ambiguous query subset.
The key to obtain-ing these results was coarse-graining the input class-label set and using a latent variable model to captureinteractions between coarse-grained labels.ReferencesR.
Baeza-Yates and A. Tiberi.
2007.
Extracting semanticrelations from query logs.
In Proceedings of the 13thACM Conference on Knowledge Discovery and DataMining (KDD-07), pages 76?85.
San Jose, California.D.
Beeferman and A. Berger.
2000.
Agglomerative clus-tering of a search engine query log.
In Proceedings ofthe 6th ACM SIGKDD Conference on Knowledge Dis-covery and Data Mining (KDD-00), pages 407?416.S.
Bergsma and Q. Wang.
2007.
Learning noun phrasequery segmentation.
In Proceedings of the 2007 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP-07), pages 819?826.
Prague,Czech Republic.J.
Dean and S. Ghemawat.
2004.
MapReduce: Simpli-fied data processing on large clusters.
In Proceedingsof the 6th Symposium on Operating Systems Designand Implementation (OSDI-04), pages 137?150.
SanFrancisco, California.J.
Finkel, C. Manning, and A. Ng.
2006.
Solving theproblem of cascading errors: Approximate Bayesianinference for linguistic annotation pipelines.
In Pro-ceedings of the 2006 Conference on Empirical Meth-ods in Natural Language Processing (EMNLP-06),pages 618?626.
Sydney, Australia.M.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the 14th In-ternational Conference on Computational Linguistics(COLING-92), pages 539?545.
Nantes, France.M.
Johnson.
2010.
PCFGs, topic models, adaptorgrammars and learning topical collocations and thestructure of proper names.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics (ACL-10), pages 1148?1157.
Up-psala, Sweden.M.
Johnson, T. Griffiths, and S. Goldwater.
2007a.
Adap-tor grammars: a framework for specifying composi-tional nonparametric bayesian models.
In Advancesin Neural Information Processing Systems 19, pages641?648.
Vancouver, Canada.M.
Johnson, T. Griffiths, and S. Goldwater.
2007b.Bayesian inference for PCFGs via Markov ChainMonte Carlo.
In Proceedings of the 2007 Confer-ence of the North American Association for Computa-tional Linguistics (NAACL-HLT-07), pages 139?146.Rochester, New York.R.
Jones, B. Rey, O. Madani, and W. Greiner.
2006.
Gen-erating query substitutions.
In Proceedings of the 15hWorld Wide Web Conference (WWW-06), pages 387?396.
Edinburgh, Scotland.X.
Li.
2010.
Understanding the semantic structure ofnoun phrase queries.
In Proceedings of the 48thAnnual Meeting of the Association for Computa-tional Linguistics (ACL-10), pages 1337?1345.
Upp-sala, Sweden.1208P.
Liang, S. Petrov, M. Jordan, and D. Klein.
2007.
Theinfinite PCFG using hierarchical Dirichlet processes.In Proceedings of the 2007 Conference on EmpiricalMethods in Natural Language Processing (EMNLP-07), pages 688?697.
Prague, Czech Republic.M.
Pas?ca.
2010.
The role of queries in ranking labeled in-stances extracted from text.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics (COLING-10), pages 955?962.
Beijing, China.A.
Popescu, P. Pantel, and G. Mishne.
2010.
Seman-tic lexicon adaptation for use in query interpretation.In Proceedings of the 19th World Wide Web Confer-ence (WWW-10), pages 1167?1168.
Raleigh, NorthCarolina.A.
Ritter, Mausam, and O. Etzioni.
2010.
A latent Dirich-let alocation method for selectional preferences.
InProceedings of the 48th Annual Meeting of the Associ-ation for Computational Linguistics (ACL-10), pages424?434.
Uppsala, Sweden.A.
Smola and S. Narayanamurthy.
2010.
An architec-ture for parallel topic models.
In Proceedings of the36th Conference on Very Large Data Bases (VLDB-10), pages 703?710.
singapore.R.
Snow, D. Jurafsky, and A. Ng.
2006.
Semantic tax-onomy induction from heterogenous evidence.
In Pro-ceedings of the 21st International Conference on Com-putational Linguistics and 44th Annual Meeting of theAssociation for Computational Linguistics (COLING-ACL-06), pages 801?808.
Sydney, Australia.I.
Szpektor, I. Dagan, R. Bar-Haim, and J. Goldberger.2008.
Contextual preferences.
In Proceedings of the46th Annual Meeting of the Association for Computa-tional Linguistics (ACL-08), pages 683?691.
Colum-bus, Ohio.P.
Talukdar and F. Pereira.
2010.
Experiments in graph-based semi-supervised learning methods for class-instance acquisition.
In Proceedings of the 48thAnnual Meeting of the Association for Computa-tional Linguistics (ACL-10), pages 1473?1481.
Upp-sala, Sweden.B.
Tan and F. Peng.
2008.
Unsupervised query segmenta-tion using generative language models and Wikipedia.In Proceedings of the 17th World Wide Web Confer-ence (WWW-08), pages 347?356.
Beijing, China.Y.
Teh, M. Jordan, M. Beal, and D. Blei.
2006.
Hier-archical Dirichlet processes.
Journal of the AmericanStatistical Association, 101(476):1566?1581.S.
Tratz and E. Hovy.
2010.
A taxonomy, dataset, andclassifier for automatic noun compound interpretation.In Proceedings of the 48th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL-10), pages678?687.
Uppsala, Sweden.B.
Van Durme and M. Pas?ca.
2008.
Finding cars, god-desses and enzymes: Parametrizable acquisition of la-beled instances for open-domain information extrac-tion.
In Proceedings of the 23rd National Confer-ence on Artificial Intelligence (AAAI-08), pages 1243?1248.
Chicago, Illinois.T.
Wang, R. Hoffmann, X. Li, and J. Szymanski.2009.
Semi-supervised learning of semantic classesfor query understanding: from the Web and for theWeb.
In Proceedings of the 18th International Con-ference on Information and Knowledge Management(CIKM-09), pages 37?46.
Hong Kong, China.1209
