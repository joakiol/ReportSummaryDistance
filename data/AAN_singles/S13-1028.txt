Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 194?201, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsSOFTCARDINALITY-CORE: Improving Text Overlap withDistributional Measures for Semantic Textual SimilaritySergio Jimenez, Claudia BecerraUniversidad Nacional de ColombiaCiudad Universitaria,edificio 453, oficina 114Bogot?, Colombiasgjimenezv@unal.edu.cocjbecerrac@unal.edu.coAlexander GelbukhCIC-IPNAv.
Juan Dios B?tiz, esq.
Av.
Mendiz?bal,Col.
Nueva Industrial Vallejo,CP 07738, DF, M?xicowww.gelbukh.comAbstractSoft cardinality has been shown to be a verystrong text-overlapping baseline for the task ofmeasuring semantic textual similarity (STS),obtaining 3rd place in SemEval-2012.
At*SEM-2013 shared task, beside the plain text-overlapping approach, we tested within softcardinality two distributional word-similarityfunctions derived from the ukWack corpus.Unfortunately, we combined these measureswith other features using regression, obtain-ing positions 18th, 22nd and 23rd among the90 participants systems in the official rank-ing.
Already after the release of the gold stan-dard annotations of the test data, we observedthat using only the similarity measures with-out combining them with other features wouldhave obtained positions 6th, 7th and 8th; more-over, an arithmetic average of these similaritymeasures would have been 4th(mean=0.5747).This paper describes both the 3 systems asthey were submitted and the similarity mea-sures that would obtained those better results.1 IntroductionThe task of textual semantic similarity (STS) con-sists in providing a similarity function on pairs oftexts that correlates with human judgments.
Sucha function has many practical applications in NLPtasks (e.g.
summarization, question answering, tex-tual entailment, paraphrasing, machine translationevaluation, among others), which makes this taskparticularly important.
Numerous efforts have beendevoted to this task (Lee et al 2005; Mihalcea et al2006) and major evaluation campaigns have beenheld at SemEval-2012 (Agirre et al 2012) and in*SEM-2013 (Agirre et al 2013).The experimental setup of STS in 2012 consistedof three data sets, roughly divided in 50% for train-ing and for testing, which contained text pairs manu-ally annotated as a gold standard.
Furthermore, twodata sets were provided for surprise testing.
Themeasure of performance was the average of the cor-relations per data set weighted by the number ofpairs in each data set (mean).
The best performingsystems were UKP (B?r et al 2012) mean=0.6773,TakeLab (?aric et al 2012) mean=0.6753 and softcardinality (Jimenez et al 2012) mean=0.6708.UKP and TakeLab systems used a large number ofresources (see (Agirre et al 2012)) such as dictio-naries, a distributional thesaurus, monolingual cor-pora, Wikipedia, WordNet, distributional similar-ity measures, KB similarity, POS tagger, machinelearning and others.
Unlike those systems, the softcardinality approach used mainly text overlappingand conventional text preprocessing such as remov-ing of stop words, stemming and idf term weighting.This shows that the additional gain in performancefrom using external resources is small and that thesoft cardinality approach is a very challenging base-line for the STS task.
Soft cardinality has beenpreviously shown (Jimenez and Gelbukh, 2012) tobe also a good baseline for other applications suchas information retrieval, entity matching, paraphrasedetection and recognizing textual entailment.Soft cardinality approach to constructing similar-ity functions (Jimenez et al 2010) consists in usingany cardinality-based resemblance coefficient (suchas Jaccard or Dice) but substituting the classical set194cardinality with a softened counting function calledsoft cardinality.
For example, the soft cardinality ofa set containing three very similar elements is closeto (though larger than) 1, while for three very dif-ferent elements it is close to (though less than) 3.To use the soft cardinality with texts, they are repre-sented as sets of words, and a word-similarity func-tion is used for the soft counting of the words.
Forthe sake of completeness, we give a brief overviewof the soft-cardinality method in Section 3.The resemblance coefficient used in our participa-tion is a modified version of Tversky?s ratio model(Tversky, 1977).
Apart from the two parameters ofthis coefficient, a new parameter was included andfunctions max and min were used to make it sym-metrical.
The rationale for this new coefficient isgiven in Section 2.Three word similarity features used in our sys-tems are described in Section 4.
The one is a mea-sure of character q-gram overlapping, which reusesthe coefficient proposed in Section 2; this measure isdescribed in subsection 4.1.
The other two ones aredistributional measures obtained from the ukWackcorpus (Baroni et al 2009), which is a collection ofweb-crawled documents containing about 1.9 billionwords in English.
The second measure is, again, areuse of the coefficient specified in Section 2, but us-ing instead sets of occurrences (and co-occurrences)of words in sentences in the ukWack corpus; thismeasure is described in subsection 4.2.
Finally, thethird one, which is a normalized version of point-wise mutual information (PMI), is described in sub-section 4.3.The parameters of the three text-similarity func-tions derived from the combination of the proposedcoefficient of resemblance (Section 2), the soft car-dinality (Section 3) and the three word-similaritymeasures (Section 4) were adjusted to maximize thecorrelation with the 2012 STS gold standard data.At this point, these soft-cardinality similarity func-tions can provide predictions for the test data.
How-ever, we decided to test the approach of learning aresemblance function from the training data insteadof using a preset resemblance coefficient.
Basically,most resemblance coefficients are ternary functionsF (x, y, z) where x = |A|, y = |B| and z = |A?B|:e.g.
Dice coefficient is F (x, y, z) = 2z/x+y and Jac-card is F (x, y, z) = z/x+y?z.
Thus, this functioncan be learned using a regression model, providingcardinalities x, y and z as features and the gold stan-dard value as the target function.
The results ob-tained for the text-similarity functions and the re-gression approach are presented in Section 7.Unfortunately, when using a regressor trainedwith 2012 STS data and tested with 2013 surprisedata we observed that the results worsened ratherthan improved.
A short explanation of this is over-fitting.
A more detailed discussion of this, togetherwith an assessment of the performance gain obtainedby the use of distributional measures is provided inSection 8.Finally, in Section 9 the conclusions of our partic-ipation in this evaluation campaign are presented.2 Symmetrical Tversky?s Ratio ModelIn the field of mathematical psychology Tverskyproposed the ratio model (TRM) (Tversky, 1977)motivated by the imbalance that humans have onthe selection of the referent to compare things.
Thismodel is a parameterized resemblance coefficient tocompare two sets A and B given by the followingexpression:trm(A,B) =|A ?B|?|A \B|+ ?|B \A|+ |A ?B|,Having ?, ?
?
0.
The numerator represents thecommonality between A and B, and the denomina-tor represents the referent for comparison.
Parame-ters ?
and ?
represent the preference in the selectionof A or B as referent.
Tversky associated the setcardinality, to the stimuli of the objects being com-pared.
Let us consider a Tversky?s example of the70s: A is North Corea, B is red China and stimuliis the prominence of the country.
When subjects as-sessed the similarity between A and B, they tendedto select the country with less prominence as ref-erent.
Tversky observed that ?
was larger than ?when subjects compared countries, symbols, textsand sounds.
Our motivation is to use this model byadjusting the parameters ?
and ?
for better modelinghuman similarity judgments for short texts.However, this is not a symmetric model and theparameters ?
and ?, have the dual interpretation ofmodeling the asymmetry in the referent selection,while controlling the balance between |A ?
B| and195|A?B|+ |B ?A| as well.
The following reformu-lation, called symmetric TRM (strm), is intended toaddress these issues:strm(A,B) =c?
(?a+ (1?
?)
b) + c, (1)a = min(|A ?
B|, |B ?
A|), b = max(|A ?B|, |B ?
A|) and c = |A ?
B| + bias.
In strm, ?models only the balance between the differences inthe cardinalities of A and B, and ?
models the bal-ance between |A?B| and |A?B|+|B?A|.
Further-more, the use of functions min and max makes themeasure to be symmetric.
Although the motivationfor the bias parameter is empirical, we believe thatthis reduces the effect of the common features thatare frequent and therefore less informative, e.g.
stopwords.
Note that for ?
= 0.5,?
= 1 and bias = 0,strm is equivalent to Dice?s coefficient.
Similarity,for ?
= 0.5,?
= 2 and bias = 0, strm is equivalentto the Jaccard?s coefficient.3 Soft CardinalityThe cardinality of a set is its number of elements.
Bydefinition, the sets do not allow repeated elements,so if a collection of elements contains repetitions itscardinality is the number of different elements.
Theclassical set cardinality does not take into accountsimilar elements, i.e.
only the identical elementsin a collection counted once.
The soft cardinality(Jimenez et al 2010) considers not only identicalelements but also similar using an auxiliary similar-ity function sim, which compares pairs of elements.This cardinality can be calculated for a collection ofelements A with the following expression:|A|?
=n?i=1wi?
?n?j=1sim(ai, aj)p??
?1(2)A ={a1, a2, .
.
.
, an}; wi ?
0; p ?
0; 1 >sim(x, y) ?
0, x 6= y; and sim(x, x) = 1.
Theparameter p controls the degree of "softness" ofthe cardinality.
This formulation has the propertyof reproducing classical cardinality when p is largeand/or when sim is a rigid function that returns 1only for identical elements and 0 otherwise.
The co-efficients wi are the weights associated with each el-ement.
In text applications elements ai are wordsand weights wi represent the importance or infor-mative character of each word (e.g.
idf weights).The apostrophe is used to differentiate soft cardinal-ity from the classic set cardinality.4 Word SimilarityAnalogous to the STS, the word similarity is the taskof measuring the relationship of a couple of wordsin a way correlated with human judgments.
Sincewhen Rubenstein and Goodenough (1965) providedthe first data set, this task has been addressed pri-marily through semantic networks (Resnik, 1999;Pedersen et al 2004) and distributional measures(Agirre et al 2009).
However, other simpler ap-proaches such as edit-distance (Levenshtein, 1966)and stemming (Porter, 1980) can also be used.
Forinstance, the former identifies the similarity between"song" and "sing", and later that between "sing" and"singing".
This section presents three approachesfor word similarity that can be plugged into the softcardinality expression in eq.
2.4.1 Q-grams similarityQ-grams are the collection of consecutive-overlapped sub-strings of length q obtainedfrom the character string in a word.
For instance,the 2-grams (bi-grams) and 3-grams (trigrams) rep-resentation of the word ?sing?
are {?#s?, ?si?, ?in?,?ng?, ?g#?}
and {?#si?, ?sin?, ?ing?, ?ng#?}
respec-tively.
The character ?#?
is a padding character thatdistinguishes q-grams at the beginning and endingof a word.
If the number of characters in a word isgreater or equal than q its representation in q-gramsis the word itself (e.g.
the 6-grams in ?sing?
are{?sing?}).
Moreover, the 1-grams (unigrams) and0-grams representations of ?sing?
are {?s?, ?i?, ?n?,?g?}
and {?sing?}.
A word can also be representedby combining multiple representations of q-grams.For instance, the combined representation of ?sing?using 0-grams, unigrams, and bi-grams is {?sing?,?s?, ?i?, ?n?, ?g?, ?#s?, ?si?, ?in?, ?ng?, ?g#?
}, denotedby [0:2]-grams.
In practice a range [q1 : q2] ofq-grams can be used having 0 ?
q1 < q2.The proposed word-similarity function (namedqgrams) first represents a pair of words using[q1 : q2]-grams and then compares them reusingthe strm coefficient (eq.1).
The parameters of the196qgrams function are q1, q2, ?qgrams, ?qgrams, andbiasqgrams.
These parameters are sub-scripted todistinguish them from their counterparts at the text-similarity functions.4.2 Context-Set Distributional SimilarityThe hypothesis of this measure is that the co-occurrence of two words in a sentence is a hint ofthe possible relationship between them.
Let us de-fine sf(t) as the sentence frequency of a word t ina corpus.
The sentence frequency is equivalent tothe well known document frequency but uses sen-tences instead of documents.
Similarly sf(tA ?
tB)is the number of sentences where words tA and tBco-occur.
The idea is to compute a similarity func-tion between tA and tB representing them as A andB, which are sets of the sentences where tA and tBoccur.
Similarly, A?B is the set of sentences whereboth words co-occur.
The required cardinalities canbe obtained from the sentence frequencies by: |A| =sf(tA); |B| = sf(tB) and |A ?
B| = sf(tA ?
tB).These cardinalities are combined reusing again thestrm coefficient (eq.
1) to obtain a word-similarityfunction.
The parameters of this function, which werefer to it as csds, are ?csds, ?csds and biascsds.4.3 Normalized Point-wise Mutual InformationThe pointwise mutual information (PMI) is a mea-sure of relationship between two random variables.PMI is calculated by the following expression:pmi(tA, tB) = log2(P (tA ?
tB)P (tA) ?
P (tB))PMI has been used to measure the relatedness ofpairs of words using the number of the hits returnedby a search engine (Turney, 2001; Bollegala et al2007).
However, PMI cannot be used directly assim function in eq.2.
The alternative is to normal-ize it dividing it by log2(P (tA ?
tB)) obtaining avalue in the [1,?1] interval.
This measure returns1 for complete co-occurrence, 0 for independenceand -1 for ?never?
co-occurring.
Given that the re-sults in the interval (0,-1] are not relevant, the finalnormalized-trimmed expression is:npmi(tA, tB) = max[pmi(tA, tB)log2(P (tA ?
tB)), 0](3)The probabilities required by PMI can be obtainedby MLE using sentence frequencies in a large cor-pus: P (tA) ?sf(tA)S , P (tB) ?sf(tB)S ,and P (tA ?tB) ?sf(tA?tB)S .
Where S is the total number ofsentences in the corpus.5 Text-similarity FunctionsThe ?building blocks?
proposed in sections 2,3 and 4, are assembled to build three text-similarity functions, namely STSqgrams, STScsdsand STSnpmi.
The first component is the strm re-semblance coefficient (eq.
1), which takes as argu-ments a pair of texts represented as bags of wordswith importance weights associated with each word.In the following subsection 5.1 a detailed descrip-tion of the procedure for obtaining such weightedbag-of-words is provided.The strm coefficient is enhanced by replac-ing the classical cardinality by the soft cardinality,which exploits two resources: importance weightsassociated with each word (weights wi) and pair-wise comparisons among words (sim).
UnlikeSTSqgrams measure, STScsds and STSnpmi mea-sures require statistics from a large corpus.
A briefdescription of the used corpus and the method forobtaining such statistics is described in subsection5.2.
Finally, the three proposed text-similarity func-tions contain free parameters that need to be ad-justed.
The method used to get those parameters isdescribed in subsection 5.3.5.1 Preprocessing and Term WeightingAll training and test texts were preprocessed withthe following sequence of actions: i) text stringswere tokenized, ii) uppercase characters are con-verted into lower-cased equivalents, iii) stop-wordswere removed, iv) punctuation marks were removed,and v) words were stemmed using Porter?s algorithm(1980).
Then each stemmed word was weightedwith idf (Jones, 2004) calculated using the entirecollection of texts.5.2 Sentence Frequencies from CorpusThe sentence frequencies sf(t) and sf(tA ?
tB) re-quired by csds and npmi word-similarity func-tions were obtained from the ukWack corpus (Ba-roni et al 2009).
This corpus has roughly 1.9 bil-197lion words, 87.8 millions of sentences and 2.7 mil-lions of documents.
The corpus was iterated sen-tence by sentence with the same preprocessing thatwas described in the previous section, looking forall occurrences of words and word pairs from thefull training and test texts.
The target words werestored in a trie, making the entire corpus iterationtook about 90 minutes in a laptop with 4GB and a1.3Ghz processor.5.3 Parameter optimizationThe three proposed text-similarity functions haveseveral parameters: p exponent in the soft car-dinality; ?, ?, and bias in strm coefficient;their sub-scripted versions in qgrams and csdsword-similarity functions; and finally q1and q2 forqgrams function.
Parameter sets for each of thethree text-similarity functions were optimized us-ing the full STS-SemEval-2012 data.
The functionto maximize was the correlation between similar-ity scores against the gold standard in the trainingdata.
The set of parameters for each similarity func-tion were optimized using a greedy hill-climbing ap-proach by using steps of 0.01 for all parameters ex-cept q1 and q2 that used 1 as step.
The initial valueswere p = 1, ?
= 0.5, ?
= 1, bias = 0, q1 = 2 andq2 = 3.
All parameters were optimized until im-provement in the function to maximize was below0.0001.
The obtained values are :STSqgrams p = 1.32,?
= 0.52, ?
= 0.64, bias =?0.45, q1 = 0, q2 = 2, ?qgrams = 0.95,?qgrams = 1.44, biasqgrams = ?0.44.STScsds p = 0.5, ?
= 0.63, ?
= 0.69, bias =?2.05, ?csds = 1.34, ?csds = 2.57, biascsds =?1.22 .STSnpmi p = 6.17,?
= 0.83, ?
= 0.64, bias =?2.11.6 Regression for STSThe use of regression is motivated by the follow-ing experiment.
First, a synthetic data set with1,000 instances was generated with the followingthree features: |A| = RandomBetween(1, 100),|B| = RandomBetween(1, 100) and |A ?
B| =RandomBetween(0,min[|A|, |B|]).
Secondly, a#1 STSsim #11 |A?B|?/|A|?#2 |A|?
#12 |A?B|?/|B|?#3 |B|?
#13 |A|?
?
|B|?#4 |A ?B|?
#14 |A?B|?/|A?B|?#5 |A ?B|?
#15 2?|A?B|?/|A|?+|B|?#6 |A \B|?
#16 |A?B|/min[|A|,|B|]#7 |B \A|?
#17 |A?B|?/max[|A|?,|B|?
]#8 |A ?B ?A ?B|?
#18 |A?B|?/?|A|?
?|B|?#9 |A?B|?/|A|?
#19 |A?B|?+|A|?+|B|?2?|A|?
?|B|?#10 |B?A|?/|B|?
#20 gold standardTable 1: Feature set for regressionlinear regressor was trained using the Dice?s coef-ficient (i.e.
2|A ?
B|/|A| + |B|) as target function.The Pearson correlation obtained using 4-fold cross-validation as method of evaluation was r = 0.93.Besides, a Reduced Error Pruning (REP) tree (Wit-ten and Frank, 2005) boosted with 30 iterations ofBagging (Breiman, 1996) was used instead of thelinear regressor obtaining r = 0.99.
We concludedthat a particular resemblance coefficient can be ac-curately approximated using a nonlinear regressionalgorithm and training data.This approach can be used for replacing the strmcoefficient by a similarity function learned from STStraining data.
The three features used in the previ-ous experiment were extended to a total of 19 (seetable 1) plus the gold standard as target.
The feature#1 is the score of the corresponding text-similarityfunction described in the previous section.
Threesets of features were constructed, each with 19 fea-tures using the soft cardinality in combination withthe word-similarity functions qgrams, csds andnpmi.
Let us name these feature sets as fs:qgrams,fs:csds and fs:npmi.
The submission labeled run1was obtained using the feature set fs:qgrams (19 fea-tures).
The submission labeled run2 was obtainedusing the aggregation of fs:qgrams and fs:csds (19?2 = 38 features).
Finally, run3 was the aggregationof fs:grams, fs:csds and fs:npmi (19 ?
3 = 57 fea-tures).7 Results in *SEM 2013 Shared TaskIn this section three groups of systems are describedby using the functions and models proposed in theprevious sections.
The first group (and simplest)198Data set STSqgrams STScsds STSnpmi averageheadlines 0.7625 0.7243 0.7379 0.7562OnWN 0.7022 0.7050 0.6832 0.7063FNWM 0.2704 0.3713 0.4215 0.3940SMT 0.3151 0.3325 0.3408 0.3402mean 0.5570 0.5592 0.5653 0.5747rank 8 7 6 4Table 2: Unofficial results using text-similarity functionsData set run1 run2 run3headlines 0.7591 0.7632 0.7640OnWN 0.7159 0.7239 0.7485FNWM 0.2806 0.3679 0.3487SMT 0.2820 0.2786 0.2952mean 0.5491 0.5586 0.5690rank 14 8 4Table 3: Unofficial results using linear regressionof systems consist in using the scores of the threetext-similarity functions STSqgrams, STScsds andSTSnpmi.
Table 2 shows the unofficial results ofthese three systems.
The bottom row shows the posi-tions that these systems would have obtained if theyhad been submitted to the *SEM shared task 2013.The last column shows the results of a system thatcombines the scores of three measures on a singlescore calculating the arithmetic mean.
This is thebest performing system obtained with the methodsdescribed in this paper.Tables 3 and 4 show unofficial and official re-sults of the method described in section 6 usinglinear regression and Bagging (30 iterations)+REPtree respectively.
These results were obtained usingWEKA (Hall et al 2009).8 DiscussionContrary to the observation we made in trainingdata, the methods that used regression to predict thegold standard performed poorly compared with thetext similarity functions proposed in Section 5.
Thatis, the results in Table 2 overcome those in Tables 3and 4.
Also in training data, Bagging+REP tree sur-passed linear regression, but, as can be seen in tables3 and 4 the opposite happened in test data.
This isa clear symptom of overfitting.
However, the OnWNData set run1 run2 run3headlines 0.6410 0.6713 0.6603OnWN 0.7360 0.7412 0.7401FNWM 0.3442 0.3838 0.3347SMT 0.3035 0.2981 0.2900mean 0.5273 0.5402 0.5294rank 23 18 22Table 4: Official results of the submitted runs to STS*SEM 2013 shared task using Bagging + REP tree forregressiondata set was an exception, which obtained the bestresults using linear regression.
OnWN was the onlyone among the 2013 data sets that was not a sur-prise data set.
Probably the 5.97% relative improve-ment obtained in run3 by the linear regression versusthe best result in Table 2 may be justified owing tosome patterns discovered by the linear regressor inthe OnWN?2012 training data which are projectedon the OnWN?2013 test data.It is worth noting that in all three sets of results,the lowest mean was consistently obtained by thetext-overlapping methods, namely STSqgrams andrun1.
The relative improvement in mean due tothe use of distributional measures against the text-overlapping methods was 3.18%, 3.62% and 2.45%in each set of results (see Tables 2, 3 and 4).
InFNWM data set, the biggest improvements achieved55.88%, 31.11% and 11.50% respectively in thethree groups of results, followed by SMT data set.Both in FNWN data set as in SMT, the texts are sys-tematically longer than those found in OnWN andheadlines.
This result suggests that the improvementdue to distributional measures is more significant inlonger texts than in the shorter ones.Lastly, it is also important to notice thatthe STSqgrams text-similarity function obtainedmean = 0.5570, which proved again to be a verystrong text-overlapping baseline for the STS task.9 ConclusionsWe participated in the CORE-STS shared task in*SEM 2013 with satisfactory results obtaining po-sitions 18th, 22nd, and 23rd in the official ranking.Our systems were based on a new parameterizedresemblance coefficient derived from the Tversky?s199ratio model in combination with the soft cardinal-ity.
The three proposed text-similarity functionsused q-grams overlapping and distributional mea-sures obtained from the ukWack corpus.
These text-similarity functions would have been attained posi-tions 6th, 7th and 8th in the official ranking, besidesa simple average of them would have reached the4thplace.
Another important conclusion was that theplain text-overlapping method was consistently im-proved by the incremental use of the proposed distri-butional measures.
This result was most noticeablein long texts.In conclusion, the proposed text-similarity func-tions proved to be competitive despite their simplic-ity and the few resources used.AcknowledgmentsThis research was funded in part by the Systemsand Industrial Engineering Department, the Officeof Student Welfare of the National University ofColombia, Bogot?, and through a grant from theColombian Department for Science, Technologyand Innovation, Colciencias, proj.
1101-521-28465with funding from ?El Patrimonio Aut?nomo FondoNacional de Financiamiento para la Ciencia, la Tec-nolog?a y la Innovaci?n, Francisco Jos?
de Caldas.
?The Section 2 was proposed during the first author?sinternship at Microsoft Research in 2012.
The thirdauthor recognizes the support from Mexican Gov-ernment (SNI, COFAA-IPN, SIP 20131702, CONA-CYT 50206-H) and CONACYT?DST India (proj.122030 ?Answer Validation through Textual Entail-ment?).
Entailment?
).ReferencesEneko Agirre, Enrique Alfonseca, Keith Hall, JanaKravalova, Marius Pasca, and Aitor Soroa.
2009.A study on similarity and relatedness using distribu-tional and WordNet-based approaches.
In Proceedingsof Human Language Technologies: The 2009 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics, NAACL ?09,pages 19?27, Stroudsburg, PA, USA.
Association forComputational Linguistics.Eneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-Agirre Aitor.
2012.
SemEval-2012 task 6: A pilot onsemantic textual similarity.
In Proceedings of the 6thInternational Workshop on Semantic Evaluation (Se-mEval@*SEM 2012), Montreal,Canada.
Associationfor Computational Linguistics.Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*SEM 2013 sharedtask: Semantic textual similarity, including a pilot ontyped-similarity.
Atlanta, Georgia, USA.
Associationfor Computational Linguistics.Daniel B?r, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
UKP: computing semantic textualsimilarity by combining multiple content similaritymeasures.
In Proceedings of the 6th InternationalWorkshop on Semantic Evaluation (SemEval *SEM2012), Montreal, Canada.
Association for Computa-tional Linguistics.Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The WaCky wide web: acollection of very large linguistically processed web-crawled corpora.
Language resources and evaluation,43(3):209?226.Danushka Bollegala, Yutaka Matsuto, and MitsuruIshizuka.
2007.
Measuring semantic similarity be-tween words using web search engines.
In Proceed-ings of the 16th international conference on WorldWide Web, WWW ?07, pages 757?766, New York,NY, USA.
ACM.Leo Breiman.
1996.
Bagging predictors.
MachineLearning, 24(2):123?140.Mark Hall, Frank Eibe, Geoffrey Holmes, and BernhardPfahringer.
2009.
The WEKA data mining software:An update.
SIGKDD Explorations, 11(1):10?18.Sergio Jimenez and Alexander Gelbukh.
2012.
Baselinesfor natural language processing tasks.
Appl.
Comput.Math., 11(2):180?199.Sergio Jimenez, Fabio Gonzalez, and Alexander Gel-bukh.
2010.
Text comparison using soft cardinality.In Edgar Chavez and Stefano Lonardi, editors, StringProcessing and Information Retrieval, volume 6393 ofLNCS, pages 297?302.
Springer, Berlin, Heidelberg.Sergio Jimenez, Claudia Becerra, and Alexander Gel-bukh.
2012.
Soft cardinality: A parameterized sim-ilarity function for text comparison.
In Proceedings ofthe 6th International Workshop on Semantic Evalua-tion (SemEval *SEM 2012), Montreal, Canada.Karen Sp?rck Jones.
2004.
A statistical interpretation ofterm specificity and its application in retrieval.
Jour-nal of Documentation, 60(5):493?502, October.Michael D Lee, B.M.
Pincombe, and Matthew Welsh.2005.
An empirical evaluation of models of text docu-ment similarity.
IN COGSCI2005, pages 1254?1259.Vladimir I. Levenshtein.
1966.
Binary codes capable ofcorrecting deletions, insertions, and reversals.
SovietPhysics Doklady, 10(8):707?710.200Rada Mihalcea, Courtney Corley, and Carlo Strapparava.2006.
Corpus-based and knowledge-based measuresof text semantic similarity.
In In AAAI?06, pages 775?780.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
WordNet::Similarity: measuring the re-latedness of concepts.
In Proceedings HLT-NAACL?Demonstration Papers, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.Martin Porter.
1980.
An algorithm for suffix stripping.Program, 3(14):130?137, October.Phillip Resnik.
1999.
Semantic similarity in a taxonomy:An information-based measure and its application toproblems of ambiguity in natural language.
Journal ofArtificial Intelligence Research, 11:95?130.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Commun.
ACM,8(10):627?633, October.Frane ?aric, Goran Glava?, Mladen Karan, Jan ?najder,and Bojana Dalbelo Ba?ic.
2012.
TakeLab: systemsfor measuring semantic text similarity.
In Proceedingsof the 6th International Workshop on Semantic Eval-uation (SemEval *SEM 2012), Montreal, Canada.
As-sociation for Computational Linguistics.Peter D. Turney.
2001.
Mining the web for synonyms:PMI-IR versus LSA on TOEFL.
In Luc De Raedt andPeter Flach, editors, Machine Learning: ECML 2001,number 2167 in Lecture Notes in Computer Science,pages 491?502.
Springer Berlin Heidelberg, January.Amos Tversky.
1977.
Features of similarity.
Psycholog-ical Review, 84(4):327?352, July.I.H.
Witten and E. Frank.
2005.
Data Mining: Prac-tical Machine Learning Tools and Techniques.
Mor-gan Kaufmann Publishers Inc., San Francisto, CA, 2ndedition.201
