Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 429?433,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsIs Machine Translation Ripe for Cross-lingual Sentiment Classification?Kevin Duh and Akinori Fujino and Masaaki NagataNTT Communication Science Laboratories2-4 Hikari-dai, Seika-cho, Kyoto 619-0237, JAPAN{kevin.duh,fujino.akinori,nagata.masaaki}@lab.ntt.co.jpAbstractRecent advances in Machine Translation (MT)have brought forth a new paradigm for build-ing NLP applications in low-resource scenar-ios.
To build a sentiment classifier for alanguage with no labeled resources, one cantranslate labeled data from another language,then train a classifier on the translated text.This can be viewed as a domain adaptationproblem, where labeled translations and testdata have some mismatch.
Various prior workhave achieved positive results using this ap-proach.In this opinion piece, we take a step back andmake some general statements about cross-lingual adaptation problems.
First, we claimthat domain mismatch is not caused by MTerrors, and accuracy degradation will occureven in the case of perfect MT.
Second, we ar-gue that the cross-lingual adaptation problemis qualitatively different from other (monolin-gual) adaptation problems in NLP; thus newadaptation algorithms ought to be considered.This paper will describe a series of carefully-designed experiments that led us to these con-clusions.1 SummaryQuestion 1: If MT gave perfect translations (seman-tically), do we still have a domain adaptation chal-lenge in cross-lingual sentiment classification?Answer: Yes.
The reason is that while many trans-lations of a word may be valid, the MT system mighthave a systematic bias.
For example, the word ?awe-some?
might be prevalent in English reviews, but intranslated reviews, the word ?excellent?
is generatedinstead.
From the perspective of MT, this translationis correct and preserves sentiment polarity.
But fromthe perspective of a classifier, there is a domain mis-match due to differences in word distributions.Question 2: Can we apply standard adaptation algo-rithms developed for other (monolingual) adaptationproblems to cross-lingual adaptation?Answer: No.
It appears that the interaction betweentarget unlabeled data and source data can be ratherunexpected in the case of cross-lingual adaptation.We do not know the reason, but our experimentsshow that the accuracy of adaptation algorithms incross-lingual scenarios have much higher variancethan monolingual scenarios.The goal of this opinion piece is to argue the needto better understand the characteristics of domainadaptation in cross-lingual problems.
We invite thereader to disagree with our conclusion (that the truebarrier to good performance is not insufficient MTquality, but inappropriate domain adaptation meth-ods).
Here we present a series of experiments thatled us to this conclusion.
First we describe the ex-periment design (?2) and baselines (?3), before an-swering Question 1 (?4) and Question 2 (?5).2 Experiment DesignThe cross-lingual setup is this: we have labeled datafrom source domain S and wish to build a sentimentclassifier for target domain T .
Domain mismatchcan arise from language differences (e.g.
English vs.translated text) or market differences (e.g.
DVD vs.Book reviews).
Our experiments will involve fixing429T to a common testset and varying S. This allows usto experiment with different settings for adaptation.We use the Amazon review dataset of Pretten-hofer (2010)1, due to its wide range of languages(English [EN], Japanese [JP], French [FR], Ger-man [DE]) and markets (music, DVD, books).
Un-like Prettenhofer (2010), we reverse the direction ofcross-lingual adaptation and consider English as tar-get.
English is not a low-resource language, but thissetting allows for more comparisons.
Each sourcedataset has 2000 reviews, equally balanced betweenpositive and negative.
The target has 2000 test sam-ples, large unlabeled data (25k, 30k, 50k samplesrespectively for Music, DVD, and Books), and anadditional 2000 labeled data reserved for oracle ex-periments.
Texts in JP, FR, and DE are translatedword-by-word into English with Google Translate.2We perform three sets of experiments, shown inTable 1.
Table 2 lists all the results; we will interpretthem in the following sections.Target (T ) Source (S)1 Music-EN Music-JP, Music-FR, Music-DE,DVD-EN, Book-EN2 DVD-EN DVD-JP, DVD-FR, DVD-DE,Music-EN, Book-EN3 Book-EN Book-JP, Book-FR, Book-DE,Music-EN, DVD-ENTable 1: Experiment setups: Fix T , vary S.3 How much performance degradationoccurs in cross-lingual adaptation?First, we need to quantify the accuracy degrada-tion under different source data, without consider-ation of domain adaptation methods.
So we traina SVM classifier on labeled source data3, and di-rectly apply it on test data.
The oracle setting, whichhas no domain-mismatch (e.g.
train on Music-EN,test on Music-EN), achieves an average test accu-racy of (81.6 + 80.9 + 80.0)/3 = 80.8%4.
Aver-1http://www.webis.de/research/corpora/webis-cls-102This is done by querying foreign words to build a bilingualdictionary.
The words are converted to tfidf unigram features.3For all methods we try here, 5% of the 2000 labeled sourcesamples are held-out for parameter tuning.4See column EN of Table 2, Supervised SVM results.age cross-lingual accuracies are: 69.4% (JP), 75.6%(FR), 77.0% (DE), so degradations compared to or-acle are: -11% (JP), -5% (FR), -4% (DE).5 Cross-market degradations are around -6%6.Observation 1: Degradations due to market andlanguage mismatch are comparable in several cases(e.g.
MUSIC-DE and DVD-EN perform similarlyfor target MUSIC-EN).
Observation 2: The rankingof source language by decreasing accuracy is DE >FR > JP.
Does this mean JP-EN is a more difficultlanguage pair for MT?
The next section will showthat this is not necessarily the case.
Certainly, thedomain mismatch for JP is larger than DE, but thiscould be due to phenomenon other than MT errors.4 Where exactly is the domain mismatch?4.1 Theory of Domain AdaptationWe analyze domain adaptation by the concepts oflabeling and instance mismatch (Jiang and Zhai,2007).
Let pt(x, y) = pt(y|x)pt(x) be the targetdistribution of samples x (e.g.
unigram feature vec-tor) and labels y (positive / negative).
Let ps(x, y) =ps(y|x)ps(x) be the corresponding source distribu-tion.
We assume that one (or both) of the followingdistributions differ between source and target:?
Instance mismatch: ps(x) 6= pt(x).?
Labeling mismatch: ps(y|x) 6= pt(y|x).Instance mismatch implies that the input featurevectors have different distribution (e.g.
one datasetuses the word ?excellent?
often, while the other usesthe word ?awesome?).
This degrades performancebecause classifiers trained on ?excellent?
might notknow how to classify texts with the word ?awe-some.?
The solution is to tie together these features(Blitzer et al, 2006) or re-weight the input distribu-tion (Sugiyama et al, 2008).
Under some assump-tions (i.e.
covariate shift), oracle accuracy can beachieved theoretically (Shimodaira, 2000).Labeling mismatch implies the same input hasdifferent labels in different domains.
For exam-ple, the JP word meaning ?excellent?
may be mis-translated as ?bad?
in English.
Then, positive JP5See ?Adapt by Language?
columns of Table 2.
NoteJP+FR+DE condition has 6000 labeled samples, so is not di-rectly comparable to other adaptation scenarios (2000 samples).Nevertheless, mixing languages seem to give good results.6See ?Adapt by Market?
columns of Table 2.430Target Classifier Oracle Adapt by Language Adapt by MarketEN JP FR DE JP+FR+DE MUSIC DVD BOOKMUSIC-EN Supervised SVM 81.6 68.5 75.2 76.3 80.3 - 76.8 74.1Adapted TSVM 79.6 73.0 74.6 77.9 78.6 - 78.4 75.6DVD-EN Supervised SVM 80.9 70.1 76.4 77.4 79.7 75.2 - 74.5Adapted TSVM 81.0 71.4 75.5 76.3 78.4 74.8 - 76.7BOOK-EN Supervised SVM 80.0 69.6 75.4 77.4 79.9 73.4 76.2 -Adapted TSVM 81.2 73.8 77.6 76.7 79.5 75.1 77.4 -Table 2: Test accuracies (%) for English Music/DVD/Book reviews.
Each column is an adaptation scenario usingdifferent source data.
The source data may vary by language or by market.
For example, the first row shows that forthe target of Music-EN, the accuracy of a SVM trained on translated JP reviews (in the same market) is 68.5, while theaccuracy of a SVM trained on DVD reviews (in the same language) is 76.8.
?Oracle?
indicates training on the samemarket and same language domain as the target.
?JP+FR+DE?
indicates the concatenation of JP, FR, DE as sourcedata.
Boldface shows the winner of Supervised vs. Adapted.reviews will be associated with the word ?bad?
:ps(y = +1|x = bad) will be high, whereas the trueconditional distribution should have high pt(y =?1|x = bad) instead.
There are several cases forlabeling mismatch, depending on how the polaritychanges (Table 3).
The solution is to filter out thesenoisy samples (Jiang and Zhai, 2007) or optimizeloosely-linked objectives through shared parametersor Bayesian priors (Finkel and Manning, 2009).Which mismatch is responsible for accuracydegradations in cross-lingual adaptation??
Instance mismatch: Systematic MT bias gener-ates word distributions different from naturally-occurring English.
(Translation may be valid.)?
Label mismatch: MT error mis-translates a wordinto something with different polarity.Conclusion from ?4.2 and ?4.3: Instance mis-match occurs often; MT error appears minimal.Mis-translated polarity Effect?
?
0 Loose a discriminativee.g.
(?good?
?
?the?)
feature0 ?
?
Increased overlap ine.g.
(?the?
?
?good?)
positive/negative data+ ?
?
and ?
?
+ Association withe.g.
(?good?
?
?bad?)
opposite labelTable 3: Label mismatch: mis-translating positive (+),negative (?
), or neutral (0) words have different effects.We think the first two cases have graceful degradation,but the third case may be catastrophic.4.2 Analysis of Instance MismatchTo measure instance mismatch, we compute statis-tics between ps(x) and pt(x), or approximationsthereof: First, we calculate a (normalized) averagefeature from all samples of source S, which repre-sents the unigram distribution of MT output.
Simi-larly, the average feature vector for target T approx-imates the unigram distribution of English reviewspt(x).
Then we measure:?
KL Divergence between Avg(S) and Avg(T ),where Avg() is the average vector.?
Set Coverage of Avg(T ) on Avg(S): how manyword (type) in T appears at least once in S.Both measures correlate strongly with final accu-racy, as seen in Figure 1.
The correlation coefficientsare r = ?0.78 for KL Divergence and r = 0.71 forCoverage, both statistically significant (p < 0.05).This implies that instance mismatch is an importantreason for the degradations seen in Section 3.74.3 Analysis of Labeling MismatchWe measure labeling mismatch by looking at dif-ferences in the weight vectors of oracle SVM andadapted SVM.
Intuitively, if a feature has positiveweight in the oracle SVM, but negative weight in theadapted SVM, then it is likely a MT mis-translation7The observant reader may notice that cross-market pointsexhibit higher coverage but equal accuracy (74-78%) to somecross-lingual points.
This suggests that MT output may be moreconstrained in vocabulary than naturally-occurring English.43168 70 72 74 76 78 80 8200.050.10.150.20.250.30.35AccuracyKLDivergence68 70 72 74 76 78 80 820.40.50.60.70.80.9AccuracyTestCoverageFigure 1: KL Divergence and Coverage vs. accuracy.
(o)are cross-lingual and (x) are cross-market data points.is causing the polarity flip.
Algorithm 1 (withK=2000) shows how we compute polarity flip rate.8We found that the polarity flip rate does not cor-relate well with accuracy at all (r = 0.04).
Conclu-sion: Labeling mismatch is not a factor in perfor-mance degradation.
Nevertheless, we note there is asurprising large number of flips (24% on average).
Amanual check of the flipped words in BOOK-JP re-vealed few MT mistakes.
Only 3.7% of 450 randomEN-JP word pairs checked can be judged as blatantlyincorrect (without sentence context).
The majorityof flipped words do not have a clear sentiment ori-entation (e.g.
?amazon?, ?human?, ?moreover?
).5 Are standard adaptation algorithmsapplicable to cross-lingual problems?One of the breakthroughs in cross-lingual text clas-sification is the realization that it can be cast as do-main adaptation.
This makes available a host of pre-existing adaptation algorithms for improving oversupervised results.
However, we argue that it may be8The feature normalization in Step 1 is important to ensurethat the weight magnitudes are comparable.Algorithm 1 Measuring labeling mismatchInput: Weight vectors for source ws and target wtInput: Target data average sample vector avg(T )Output: Polarity flip rate f1: Normalize: ws = avg(T ) * ws; wt = avg(T ) * wt2: Set S+ = { K most positive features in ws}3: Set S?
= { K most negative features in ws}4: Set T+ = { K most positive features in wt}5: Set T?
= { K most negative features in wt}6: for each feature i ?
T+ do7: if i ?
S?
then f = f + 18: end for9: for each feature j ?
T?
do10: if j ?
S+ then f = f + 111: end for12: f = f2Kbetter to ?adapt?
the standard adaptation algorithmto the cross-lingual setting.
We arrived at this con-clusion by trying the adapted counterpart of SVMsoff-the-shelf.
Recently, (Bergamo and Torresani,2010) showed that Transductive SVMs (TSVM),originally developed for semi-supervised learning,are also strong adaptation methods.
The idea is totrain on source data like a SVM, but encourage theclassification boundary to divide through low den-sity regions in the unlabeled target data.Table 2 shows that TSVM outperforms SVM inall but one case for cross-market adaptation, butgives mixed results for cross-lingual adaptation.This is a puzzling result considering that both usethe same unlabeled data.
Why does TSVM exhibitsuch a large variance on cross-lingual problems, butnot on cross-market problems?
Is unlabeled targetdata interacting with source data in some unexpectedway?Certainly there are several successful studies(Wan, 2009; Wei and Pal, 2010; Banea et al, 2008),but we think it is important to consider the possi-bility that cross-lingual adaptation has some fun-damental differences.
We conjecture that adaptingfrom artificially-generated text (e.g.
MT output)is a different story than adapting from naturally-occurring text (e.g.
cross-market).
In short, MT isripe for cross-lingual adaptation; what is not ripe isprobably our understanding of the special character-istics of the adaptation problem.432ReferencesCarmen Banea, Rada Mihalcea, Janyce Wiebe, andSamer Hassan.
2008.
Multilingual subjectivity analy-sis using machine translation.
In Proc.
of Conferenceon Empirical Methods in Natural Language Process-ing (EMNLP).Alessandro Bergamo and Lorenzo Torresani.
2010.
Ex-ploiting weakly-labeled web images to improve ob-ject classification: a domain adaptation approach.
InAdvances in Neural Information Processing Systems(NIPS).John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In Proc.
of Conference on EmpiricalMethods in Natural Language Processing (EMNLP).Jenny Rose Finkel and Chris Manning.
2009.
Hierarchi-cal Bayesian domain adaptation.
In Proc.
of NAACLHuman Language Technologies (HLT).Jing Jiang and ChengXiang Zhai.
2007.
Instance weight-ing for domain adaptation in NLP.
In Proc.
of the As-sociation for Computational Linguistics (ACL).Peter Prettenhofer and Benno Stein.
2010.
Cross-language text classification using structural correspon-dence learning.
In Proc.
of the Association for Com-putational Linguistics (ACL).Hidetoshi Shimodaira.
2000.
Improving predictive in-ference under covariate shift by weighting the log-likelihood function.
Journal of Statistical Planningand Inferenc, 90.Masashi Sugiyama, Taiji Suzuki, Shinichi Nakajima,Hisashi Kashima, Paul von Bu?nau, and MotoakiKawanabe.
2008.
Direct importance estimation forcovariate shift adaptation.
Annals of the Institute ofStatistical Mathematics, 60(4).Xiaojun Wan.
2009.
Co-training for cross-lingual sen-timent classification.
In Proc.
of the Association forComputational Linguistics (ACL).Bin Wei and Chris Pal.
2010.
Cross lingual adaptation:an experiment on sentiment classification.
In Proceed-ings of the ACL 2010 Conference Short Papers.433
