Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 273?277,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsBuilding Japanese Textual Entailment Specialized Data Setsfor Inference of Basic Sentence RelationsKimi Kaneko ?
Yusuke Miyao ?
Daisuke Bekki ??
Ochanomizu University, Tokyo, Japan?
National Institute of Informatics, Tokyo, Japan?
{kaneko.kimi | bekki}@is.ocha.ac.jp?
yusuke@nii.ac.jpAbstractThis paper proposes a methodology forgenerating specialized Japanese data setsfor textual entailment, which consists ofpairs decomposed into basic sentence rela-tions.
We experimented with our method-ology over a number of pairs taken fromthe RITE-2 data set.
We comparedour methodology with existing studiesin terms of agreement, frequencies andtimes, and we evaluated its validity by in-vestigating recognition accuracy.1 IntroductionIn recognizing textual entailment (RTE), auto-mated systems assess whether a human readerwould consider that, given a snippet of text t1 andsome unspecified (but restricted) world knowl-edge, a second snippet of text t2 is true.
An ex-ample is given below.Ex.
1) Example of a sentence pair for RTE?
Label: Y?
t1: Shakespeare wrote Hamlet and Macbeth.?
t2: Shakespeare is the author of Hamlet.?Label?
on line 1 shows whether textual entail-ment (TE) holds between t1 and t2.
The pair islabeled ?Y?
if the pair exhibits TE and ?N?
other-wise.It is difficult for computers to make such as-sessments because pairs have multiple interrelatedbasic sentence relations (BSRs, for detailed in-formation on BSRs, see section 3).
Recognizingeach BSRs in pairs exactly is difficult for com-puters.
Therefore, we should generate special-ized data sets consisting of t1-t2 pairs decomposedinto BSRs and a methodology for generating suchdata sets since such data and methodologies forJapanese are unavailable at present.This paper proposes a methodology for gener-ating specialized Japanese data sets for TE thatconsist of monothematic t1-t2 pairs (i.e., pairs inwhich only one BSR relevant to the entailmentrelation is highlighted and isolated).
In addition,we compare our methodology with existing stud-ies and analyze its validity.2 Existing StudiesSammons et al(2010) point out that it is necessaryto establish a methodology for decomposing pairsinto chains of BSRs, and that establishing suchmethodology will enable understanding of howother existing studies can be combined to solveproblems in natural language processing and iden-tification of currently unsolvable problems.
Sam-mons et al experimented with their methodologyover the RTE-5 data set and showed that the recog-nition accuracy of a system trained with their spe-cialized data set was higher than that of the systemtrained with the original data set.
In addition, Ben-tivogli et al(2010) proposed a methodology forclassifying more details than was possible in thestudy by Sammons et al.However, these studies were based on only En-glish data sets.
In this regard, the word-orderrules and the grammar of many languages (suchas Japanese) are different from those of English.We thus cannot assess the validity of methodolo-gies for any Japanese data set because each lan-guage has different usages.
Therefore, it is neces-sary to assess the validity of such methodologieswith specialized Japanese data sets.Kotani et al (2008) generated specializedJapanese data sets for RTE that were designedsuch that each pair included only one BSR.
How-ever, in that approach the data set is generated ar-tificially, and BSRs between pairs of real worldtexts cannot be analyzed.We develop our methodology by generatingspecialized data sets from a collection of pairsfrom RITE-21 binary class (BC) subtask data setscontaining sentences from Wikipedia.
RITE-2 is273an evaluation-based workshop focusing on RTE.Four subtasks are available in RITE-2, one ofwhich is the BC subtask whereby systems assesswhether there is TE between t1 and t2.
The rea-son why we apply our methodology to part of theRITE-2 BC subtask data set is that we can con-sider the validity of the methodology in view ofthe recognition accuracy by using the data setsgenerated in RITE-2 tasks, and that we can an-alyze BSRs in real texts by using sentence pairsextracted from Wikipedia.3 MethodologyIn this study, we extended and refined the method-ology defined in Bentivogli et al(2010) and devel-oped a methodology for generating Japanese datasets broken down into BSRs and non-BSRs as de-fined below.Basic sentence relations (BSRs):?
Lexical: Synonymy, Hypernymy, Entailment,Meronymy;?
Phrasal: Synonymy, Hypernymy, Entailment,Meronymy, Nominalization, Corference;?
Syntactic: Scrambling, Case alteration, Modi-fier, Transparent head, Clause, List, Apposi-tion, Relative clause;?
Reasoning: Temporal, Spatial, Quantity, Im-plicit relation, Inference;Non-basic sentence relations (non-BSRs)??
Disagreement: Lexical, Phrasal, Modal, Mod-ifier, Temporal, Spatial, Quantity;Mainly, we used relations defined in Bentivogliet al(2010) and divided Synonymy, Hypernymy,Entailment and Meronymy into Lexical andPhrasal.
The differences between our study andBentivogli et al(2010) are as follows.
Demonymyand Statements in Bentivogli et al(2010) werenot considered in our study because they werenot necessary for Japanese data sets.
In addi-tion, Scrambling, Entailment, Disagreement:temporal, Disagreement: spatial and Disagree-ment: quantity were newly added in our study.Scrambling is a rule for changing the order ofphrases and clauses.
Entailment is a rule wherebythe latter sentence is true whenever the former istrue (e.g., ?divorce??
?marry?).
Entailment is arule different from Synonymy, Hypernymy andMeronymy.The rules for decomposition are schematized asfollows:1http://www.cl.ecei.tohoku.ac.jp/rite2/doku.php?
Break down pairs into BSRs in order to bringt1 close to t2 gradually, as the interpretationof the converted sentence becomes wider?
Label each pair of BSRs or non-BSRssuch that each pair is decomposed to ensurethat there are not multiple BSRsAn example is shown below, where the underlinedparts represent the revised points.t1?
????????
?????
?
?????
???
?Shakespearenom Hamlet com Macbethacc writepast?Shakespeare wrote Hamlet and Macbeth.?
[List] ????????
??????
???
?Shakespearenom Hamletacc writepast?Shakespeare wrote Hamlet.?t2?
[Synonymy] ????????
??????
??
????
?phrasal Shakespearenom Hamletgen authorcomp becop?Shakespeare is the author of Hamlet.
?Table 1: Example of a pair with TEAn example of a pair without TE is shown below.t1?
??????
????????
??
?Bulgarianom Eurasia.continentdat becop?Bulgaria is on the Eurasian continent.?
[Entailment] ??????
????
?????
phrasal Bulgarianom continental.statecomp becop?Bulgaria is a continental state.?t2?
[Disagreement] ??????
??
????
?lexical Bulgarianom island.countrycomp becop?Bulgaria is an island country.
?Table 2: Example of a pair without TE (Part 1)To facilitate TE assessments like Table 3, non-BSR labels were used in decomposing pairs.
Inaddition, we allowed labels to be used severaltimes when some BSRs in a pair are related to ?N?assessments.t1?
??????
????????
??
?Bulgarianom Eurasia.continentdat becop?Bulgaria is on the Eurasian continent.?
[Disagreement] ??????
????????
???
?modal Bulgarianom Eurasia.continentdat becop?neg?Bulgaria is not on the Eurasian continent.?t2?
[Synonymy] ??????
??????
?????
?lexical Bulgarianom Europedat belongcop?neg?Bulgaria does not belong to Europe.
?Table 3: Example of a pair without TE (Part 2)As mentioned above, the idea here is to decom-pose pairs in order to bring t1 closer to t2, thelatter of which in principle has a wider semanticscope.
We prohibited the conversion of t2 becauseit was possible to decompose the pairs such thatthey could be true even if there was no TE.
Never-theless, since it is sometimes easier to convert t2,274we allowed the conversion of t2 in only the casethat t1 contradicted t2 and the scope of t2 did notoverlap with that of t1 even if t2 was converted andTE would be unchanged.
An example in case thatwe allowed to convert t2 is shown below.
Bold-faced types in Table 4 shows that it becomes easyto compare t1 with t2 by converting to t2.t1?
???
??????
??????
?Tomnom today breakfastacc eatpast?neg?Tom didn?t eat breakfast today.?
[Scrambling] ???
???
???
??????
?today Tomnom breakfastacc eatpast?neg?Today, Tom didn?t eat breakfast.?t2?
???
???
???
???
?this.morning Tomnom breadacc eatpast?This morning, Tom ate bread and salad.?
[Entailment] ???
???
???
????
?phrasal today Tomnom breakfastacc eatpast?Today, Tom ate breakfast.?
[Disagreement] ?????????????
?modal ?Today, Tom ate breakfast.
?Table 4: Example of conversion of t24 Results4.1 Comparison with Existing StudiesWe applied our methodology to 173 pairs from theRITE-2 BC subtask data set.
The pairs were de-composed by one annotator, and the decomposedpairs were assigned labels by two annotators.
Dur-ing labeling, we used the labels presented in Sec-tion 3 and ?unknown?
in cases where pairs couldnot be labeled.
Our methodology was developedbased on 112 pairs, and by using the other 61 pairs,we evaluated the inter-annotator agreement as wellas the frequencies and times of decomposition.The agreement for 241 monothematic pairs gen-erated from 61 pairs amounted to 0.83 and wascomputed as follows.
The kappa coefficient forthem amounted 0.81.Agreement = ?Agreed??
labels/Total 2Bentivogli et al (2010) reported an agreementrate of 0.78, although they computed the agree-ment by using the Dice coefficient (Dice, 1945),and therefore the results are not directly compara-ble to ours.
Nevertheless, the close values suggest2Because the ?Agreed?
pairs were clear to be classi-fied as ?Agreed?, where ?Total?
is the number of pairs la-beled ?Agreed?
subtracted from the number of labeled pairs.?Agreed?
labels is the number of pairs labeled ?Agreed?
sub-tract from the number of pairs with the same label assignedby the two annotators.that our methodology is comparable to that in Ben-tivogli?s study in terms of agreement.Table 5 shows the distribution of monothematicpairs with respect to original Y/N pairs.Originalpairs Monothematic pairsY N TotalY (32) 116 ?
116N (29) 96 29 125Total (61) 212 29 241Table 5: Distribution of monothematic pairs withrespect to original Y/N pairsWhen the methodology was applied to 61 pairs,a total of 241 and an average of 3.95 monothe-matic pairs were derived.
The average was slightlygreater than the 2.98 reported in (Bentivogli et al,2010).
For pairs originally labeled ?Y?
and ?N?, anaverage of 3.62 and 3.31 monothematic pairs werederived, respectively.
Both average values wereslightly higher than the values of 3.03 and 2.80 re-ported in (Bentivogli et al, 2010).
On the basis ofthe small differences between the average valuesin our study and those in (Bentivogli et al, 2010),we are justified in saying that our methodology isvalid.Table 6 3 shows the distribution of BSRs in t1-t2 pairs in an existing study and the present study.We can see from Table 6 thatCorferencewas seenmore frequently in Bentivogli?s study than in ourstudy, while Entailment and Scrambling wereseen more frequently in our study.
This demon-strates that differences between languages are rele-vant to the distribution and classification of BSRs.An average of 5 and 4 original pairs were de-composed per hour in our study and Bentivogli?sstudy, respectively.
This indicates that the com-plexity of our methodology is not much differentfrom that in Bentivogli et al(2010).4.2 Evaluation of Accuracy in BSRIn the RITE-2 formal run4, 15 teams used our spe-cialized data set for the evaluation of their systems.Table 7 shows the average of F1 scores5 for eachBSR.Scrambling and Modifier yielded high scores(close to 90%).
The score of List was also3Because ?lexical?
and ?phrasal?
are classified togetherin Bentivogli et al(2010), they are not shown separately inTable 6.4In RITE-2, data generated by our methodology were re-leased as ?unit test data?.5The traditional F1 score is the harmonic mean of preci-sion and recall.275BSR Monothematic pairsBentivogli et al Present studyTotal Y N Total Y NSynonymy 25 22 3 45 45 0Hypernymy 5 3 2 5 5 0Entailment - - - 44 44 0Meronymy 7 4 3 1 1 0Nominalization 9 9 0 1 1 0Corference 49 48 1 3 3 0Scrambling - - - 15 15 0Case alteration 7 5 2 7 7 0Modifier 25 15 10 42 42 0Transparent head 6 6 0 1 1 0Clause 5 4 1 14 14 0List 1 1 0 3 3 0Apposition 3 2 1 1 1 0Relative clause 1 1 0 8 8 0Temporal 2 1 1 1 1 0Spatial 1 1 0 1 1 0Quantity 6 0 6 0 0 0Implicit relation 7 7 0 18 18 0Inference 40 26 14 2 2 0Disagreement: lexical/phrasal 3 0 3 27 0 27Disagreement: modal 1 0 1 1 0 1Disagreement: temporal - - - 1 0 1Disagreement: spatial - - - 0 0 0Disagreement: quantity - - - 0 0 0Demonymy 1 1 0 - - -Statements 1 1 0 - - -total 205 157 48 241 212 29Table 6: Distribution of BSRs in t1-t2 pairs in anexisting study and in the present study using ourmethodologyBSR F1(%) Monothematic MissPairsScrambling 89.6 15 4Modifier 88.8 42 0List 88.6 3 0Temporal 85.7 1 1Relative clause 85.4 8 2Clause 85.0 14 2Hypernymy: lexical 85.0 5 1Disagreement: phrasal 80.1 25 0Case alteration 79.9 7 2Synonymy: lexical 79.7 9 6Transparent head 78.6 1 2Implicit relation 75.7 18 2Synonymy: phrasal 73.6 36 9Corference 70.9 3 1Entailment: phrasal 70.2 44 7Disagreement: lexical 69.0 2 0Meronymy: lexical 64.3 1 1Nominalization 64.3 1 0Apposition 50.0 1 1Spatial 50.0 1 1Inference 40.5 2 2Disagreement: modal 35.7 1 0Disagreement: temporal 28.6 1 1Total - 241 41Table 7: Average F1 scores in BSR and frequen-cies of misclassifications by annotatorsnearly 90%, although the data sets included only3 instances.
These scores were high becausepairs with these BSRs are easily recognized interms of syntactic structure.
By contrast, Dis-agreement: temporal, Disagreement: modal,Inference, Spatial and Apposition yielded lowscores (less than 50%).
The scores of Disagree-ment: lexical, Nominalization and Disagree-ment: Meronymy were about 50-70%.
BSRsthat yielded scores of less than 70% occurred lessthan 3 times, and those that yielded scores of notmore than 70% occurred 3 times or more, exceptfor Temporal and Transparent head.
Therefore,the frequencies of BSRs are related to F1 scores,and we should consider how to build systems thatrecognize infrequent BSRs accurately.
In addi-tion, F1 scores in Synonymy: phrasal and En-tailment: phrasal are low, although these are la-beled frequently.
This is one possible direction offuture work.Table 7 also shows the number of pairs in BSRto which the two annotators assigned different la-bels.
For example, one annotator labeled t2 [Ap-position] while the other labeled t2 [Spatial] inthe following pair:Ex.
2) Example of a pair for RTE?
t1: Tokyo, the capital of Japan, is in Asia.?
t2: The capital of Japan is in Asia.We can see from Table 7 that the F1 scores forBSRs, which are often assessed as different by dif-ferent people, are generally low, except for severallabels, such as Synonymy: lexical and Scram-bling.
For this reason, we can conjecture thatcases in which computers experience difficulty de-termining the correct labels are correlated withcases in which humans also experience such dif-ficulty.5 ConclusionsThis paper presented a methodology for generat-ing Japanese data sets broken down into BSRsand Non-BSRs, and we conducted experiments inwhich we applied our methodology to 61 pairsextracted from the RITE-2 BC subtask data set.We compared our method with that of Bentivogliet al(2010) in terms of agreement as well asfrequencies and times of decomposition, and weobtained similar results.
This demonstrated thatour methodology is as feasible as Bentivogli etal.
(2010) and that differences between languagesemerge only as the different sets of labels and thedifferent distributions of BSRs.
In addition, 241monothematic pairs were recognized by comput-ers, and we showed that both the frequencies ofBSRs and the rate of misclassification by humansare relevant to F1 scores.Decomposition patterns were not empiricallycompared in the present study and will be investi-gated in future work.
We will also develop an RTEinference system by using our specialized data set.276ReferencesBentivogli, L., Cabrio, E., Dagan, I, Giampiccolo, D.,Leggio, M. L., Magnini,B.
2010.
Building TextualEntailment Specialized Data Sets: a Methodologyfor Isolating Linguistic Phenomena Relevant to In-ference.
In Proceedings of LREC 2010, Valletta,Malta.Dagan, I, Glickman, O., Magnini, B.
2005.
Recog-nizing Textual Entailment Challenge.
In Proc.
ofthe First PASCAL Challenges Workshop on RTE.Southampton, U.K.Kotani, M., Shibata, T., Nakata, T, Kurohashi, S. 2008.Building Textual Entailment Japanese Data Sets andRecognizing Reasoning Relations Based on Syn-onymy Acquired Automatically.
In Proceedings ofthe 14th Annual Meeting of the Association for Nat-ural Language Processing, Tokyo, Japan.Magnini, B., Cabrio, E. 2009.
Combining Special-izedd Entailment Engines.
In Proceedings of LTC?09.
Poznan, Poland.Dice, L. R. 1945.
Measures of the amount of ecologicassociation between species.
Ecology, 26(3):297-302.Mark Sammons, V.G.Vinod Vydiswaran, Dan Roth.2010.
?Ask not what textual entailment can do foryou...?.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, Uppsala, Sweden, pp.
1199-1208.277
