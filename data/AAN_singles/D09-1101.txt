Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 968?977,Singapore, 6-7 August 2009. c?2009 ACL and AFNLPSupervised Models for Coreference ResolutionAltaf Rahman and Vincent NgHuman Language Technology Research InstituteUniversity of Texas at DallasRichardson, TX 75083-0688{altaf,vince}@hlt.utdallas.eduAbstractTraditional learning-based coreference re-solvers operate by training a mention-pair classifier for determining whether twomentions are coreferent or not.
Two in-dependent lines of recent research haveattempted to improve these mention-pairclassifiers, one by learning a mention-ranking model to rank preceding men-tions for a given anaphor, and the otherby training an entity-mention classifierto determine whether a preceding clus-ter is coreferent with a given mention.We propose a cluster-ranking approach tocoreference resolution that combines thestrengths of mention rankers and entity-mention models.
We additionally showhow our cluster-ranking framework natu-rally allows discourse-new entity detectionto be learned jointly with coreference res-olution.
Experimental results on the ACEdata sets demonstrate its superior perfor-mance to competing approaches.1 IntroductionNoun phrase (NP) coreference resolution is thetask of identifying which NPs (or mentions) re-fer to the same real-world entity or concept.
Tra-ditional learning-based coreference resolvers op-erate by training a model for classifying whethertwo mentions are co-referring or not (e.g., Soonet al (2001), Ng and Cardie (2002b), Kehler et al(2004), Ponzetto and Strube (2006)).
Despite theirinitial successes, these mention-pair models haveat least two major weaknesses.
First, since eachcandidate antecedent for a mention to be resolved(henceforth an active mention) is considered inde-pendently of the others, these models only deter-mine how good a candidate antecedent is relativeto the active mention, but not how good a candi-date antecedent is relative to other candidates.
Inother words, they fail to answer the critical ques-tion of which candidate antecedent is most prob-able.
Second, they have limitations in their ex-pressiveness: the information extracted from thetwo mentions alone may not be sufficient for mak-ing an informed coreference decision, especially ifthe candidate antecedent is a pronoun (which is se-mantically empty) or a mention that lacks descrip-tive information such as gender (e.g., Clinton).To address the first weakness, researchers haveattempted to train a mention-ranking model fordetermining which candidate antecedent is mostprobable given an active mention (e.g., Denis andBaldridge (2008)).
Ranking is arguably a morenatural reformulation of coreference resolutionthan classification, as a ranker allows all candidateantecedents to be considered simultaneously andtherefore directly captures the competition amongthem.
Another desirable consequence is that thereexists a natural resolution strategy for a rankingapproach: a mention is resolved to the candidateantecedent that has the highest rank.
This con-trasts with classification-based approaches, wheremany clustering algorithms have been employedto co-ordinate the pairwise coreference decisions(because it is unclear which one is the best).To address the second weakness, researchershave investigated the acquisition of entity-mentioncoreference models (e.g., Luo et al (2004), Yanget al (2004)).
Unlike mention-pair models, theseentity-mention models are trained to determinewhether an active mention belongs to a preced-ing, possibly partially-formed, coreference cluster.Hence, they can employ cluster-level features (i.e.,features that are defined over any subset of men-tions in a preceding cluster), which makes themmore expressive than mention-pair models.Motivated in part by these recently developedmodels, we propose in this paper a cluster-ranking approach to coreference resolution thatcombines the strengths of mention-ranking mod-968els and entity-mention models.
Specifically, werecast coreference as the problem of determiningwhich of a set of preceding coreference clustersis the best to link to an active mention using alearned cluster ranker.
In addition, we show howdiscourse-new detection (i.e., the task of determin-ing whether a mention introduces a new entity ina discourse) can be learned jointly with corefer-ence resolution in our cluster-ranking framework.It is worth noting that researchers typically adopta pipeline coreference architecture, performingdiscourse-new detection prior to coreference res-olution and using the resulting information to pre-vent a coreference system from resolving men-tions that are determined to be discourse-new (seePoesio et al (2004) for an overview).
As a re-sult, errors in discourse-new detection could bepropagated to the resolver, possibly leading to adeterioration of coreference performance (see Ngand Cardie (2002a)).
Jointly learning discourse-new detection and coreference resolution can po-tentially address this error-propagation problem.In sum, we believe our work makes three maincontributions to coreference resolution:Proposing a simple, yet effective coreferencemodel.
Our work advances the state-of-the-artin coreference resolution by bringing learning-based coreference systems to the next level ofperformance.
When evaluated on the ACE 2005coreference data sets, cluster rankers outperformthree competing models ?
mention-pair, entity-mention, and mention-ranking models ?
by alarge margin.
Also, our joint-learning approachto discourse-new detection and coreference reso-lution consistently yields cluster rankers that out-perform those adopting the pipeline architecture.Equally importantly, cluster rankers are conceptu-ally simple and easy to implement and do not relyon sophisticated training and inference proceduresto make coreference decisions in dependent rela-tion to each other, unlike relational coreferencemodels (see McCallum and Wellner (2004)).Bridging the gap between machine-learningapproaches and linguistically-motivated ap-proaches to coreference resolution.
While ma-chine learning approaches to coreference resolu-tion have received a lot of attention since the mid-90s, popular learning-based coreference frame-works such as the mention-pair model are ar-guably rather unsatisfactory from a linguistic pointof view.
In particular, they have not leveragedadvances in discourse-based anaphora resolutionresearch in the 70s and 80s.
Our work bridgesthis gap by realizing in a new machine learn-ing framework ideas rooted in Lappin and Leass?s(1994) heuristic-based pronoun resolver, which inturn was motivated by classic salience-based ap-proaches to anaphora resolution.Revealing the importance of adopting the rightmodel.
While entity-mention models have pre-viously been shown to be worse or at bestmarginally better than their mention-pair counter-parts (Luo et al, 2004; Yang et al, 2008), ourcluster-ranking models, which are a natural exten-sion of entity-mention models, significantly out-performed all competing approaches.
This sug-gests that the use of an appropriate learning frame-work can bring us a long way towards high-performance coreference resolution.The rest of the paper is structured as follows.Section 2 discusses related work.
Section 3 de-scribes our baseline coreference models: mention-pair, entity-mention, and mention-ranking.
Wediscuss our cluster-ranking approach in Section 4,evaluate it in Section 5, and conclude in Section 6.2 Related WorkHeuristic-based cluster ranking.
As men-tioned previously, the work most related to ours isLappin and Leass (1994), whose goal is to performpronoun resolution by assigning an anaphoric pro-noun to the highest-scored preceding cluster.
Nev-ertheless, Lappin and Leass?s work differs fromours in several respects.
First, they only tacklepronoun resolution rather than the full coreferencetask.
Second, their algorithm is heuristic-based; inparticular, the score assigned to a preceding clus-ter is computed by summing over the weights as-sociated with the factors applicable to the cluster,where the weights are determined heuristically,rather than learned, unlike ours.Like many heuristic-based pronoun resolvers(e.g., Mitkov (1998)), they first apply a set of con-straints to filter grammatically incompatible can-didate antecedents and then rank the remainingones using salience factors.
As a result, theircluster-ranking model employs only factors thatcapture the salience of a cluster, and can thereforebe viewed as a simple model of attentional state(see Grosz and Sidner (1986)) realized by coref-erence clusters.
By contrast, our resolution strat-egy is learned without applying hand-coded con-969straints in a separate filtering step.
In particular,we attempt to determine the compatibility betweena cluster and an active mention, using factors thatdetermine not only salience (e.g., the distance be-tween the cluster and the mention) but also lexicaland grammatical compatibility, for instance.Entity-mention coreference models.
Luo et al(2004) represent one of the earliest attempts toinvestigate learning-based entity-mention models.They use the ANY predicate to generate cluster-level features as follows: given a binary-valuedfeature X defined over a pair of mentions, theyintroduce an ANY-X cluster-level feature, whichhas the value TRUE if X is true between the activemention and any mention in the preceding clus-ter under consideration.
Contrary to common wis-dom, this entity-mention model underperforms itsmention-pair counterpart in spite of the general-ization from mention-pair to cluster-level features.In Yang et al?s (2004) entity-mention model, atraining instance is composed of an active men-tion mk, a preceding cluster C, and a mentionmjin C that is closest in distance to mkin theassociated text.
The feature set used to repre-sent the instance is primarily composed of fea-tures that describe the relationship between mjand mk, as well as a few cluster-level features.In other words, the model still relies heavily onfeatures used in a mention-pair model.
In par-ticular, the inclusion of mjin the feature vectorrepresentation to some extent reflects the authors?lack of confidence that a strong entity-mentionmodel can be trained without mention-pair-basedfeatures.
Our ranking model, on the other hand, istrained without such features.
More recently, Yanget al (2008) have proposed another entity-mentionmodel trained by inductive logic programming.Like their previous work, the scarcity of cluster-level predicates (only two are used) under-exploitsthe expressiveness of entity-mention models.Mention ranking.
The notion of ranking can-didate antecedents can be traced back to center-ing algorithms, many of which use grammaticalroles to rank forward-looking centers (see Groszet al (1995), Walker et al (1998), and Mitkov(2002)).
However, mention ranking has beenemployed in learning-based coreference resolversonly recently.
As mentioned before, Denis andBaldridge (2008) train a mention-ranking model.Their work can be viewed as an extension of Yanget al?s (2003) twin-candidate coreference model,which ranks only two candidate antecedents at atime.
Unlike ours, however, their model ranksmentions rather than clusters, and relies on anindependently-trained discourse-new detector.Discourse-new detection.
Discourse-new de-tection is often tackled independently of coref-erence resolution.
Pleonastic its have been de-tected using heuristics (e.g., Kennedy and Bogu-raev (1996)) and learning-based techniques suchas rule learning (e.g., Mu?ller (2006)), kernels (e.g.,Versley et al (2008)), and distributional methods(e.g., Bergsma et al (2008)).
Non-anaphoric defi-nite descriptions have been detected using heuris-tics (e.g., Vieira and Poesio (2000)) and unsu-pervised methods (e.g., Bean and Riloff (1999)).General discourse-new detectors that are applica-ble to different types of NPs have been built usingheuristics (e.g., Byron and Gegg-Harrison (2004))and modeled generatively (e.g., Elsner and Char-niak (2007)) and discriminatively (e.g., Uryupina(2003)).
There have also been attempts to performjoint inference for discourse-new detection andcoreference resolution using integer linear pro-gramming (ILP), where a discourse-new classifierand a coreference classifier are trained indepen-dently of each other, and then ILP is applied as apost-processing step to jointly infer discourse-newand coreference decisions so that they are consis-tent with each other (e.g., Denis and Baldridge(2007)).
Joint inference is different from our joint-learning approach, which allows the two tasks tobe learned jointly and not independently.3 Baseline Coreference ModelsIn this section, we describe three coreference mod-els that will serve as our baselines: the mention-pair model, the entity-mention model, and themention-ranking model.
For illustrative purposes,we will use the text segment shown in Figure 1.Each mention m in the segment is annotated as[m]cidmid, where mid is the mention id and cid isthe id of the cluster to which m belongs.
As wecan see, the mentions are partitioned into four sets,with Barack Obama, his, and he in one cluster, andeach of the remaining mentions in its own cluster.3.1 Mention-Pair ModelAs noted before, a mention-pair model is a clas-sifier that decides whether or not an active men-tion mkis coreferent with a candidate antecedentmj.
Each instance i(mj,mk) represents mjand970[Barack Obama]11nominated [Hillary Rodham Clinton]22as[[his]13secretary of state]34on [Monday]45.
[He]16...Figure 1: An illustrative examplemkand consists of the 39 features shown in Ta-ble 1.
These features have largely been employedby state-of-the-art learning-based coreference sys-tems (e.g., Soon et al (2001), Ng and Cardie(2002b), Bengtson and Roth (2008)), and are com-puted automatically.
As can be seen, the featuresare divided into four blocks.
The first two blocksconsist of features that describe the properties ofmjand mk, respectively, and the last two blocksof features describe the relationship between mjand mk.
The classification associated with a train-ing instance is either positive or negative, depend-ing on whether mjand mkare coreferent.If one training instance were created from eachpair of mentions, the negative instances wouldsignificantly outnumber the positives, yieldinga skewed class distribution that will typicallyhave an adverse effect on model training.
Asa result, only a subset of mention pairs willbe generated for training.
Following Soon etal.
(2001), we create (1) a positive instance foreach discourse-old mention mkand its closestantecedent mj; and (2) a negative instance formkpaired with each of the intervening mentions,mj+1,mj+2, .
.
.
,mk?1.
In our running exampleshown in Figure 1, three training instances willbe generated for He: i(Monday, He), i(secretaryof state, He), and i(his, He).
The first two ofthese instances will be labeled as negative, andthe last one will be labeled as positive.
To train amention-pair classifier, we use the SVM learningalgorithm from the SVMlight package (Joachims,2002), converting all multi-valued features into anequivalent set of binary-valued features.After training, the resulting SVM classifier isused to identify an antecedent for a mention in atest text.
Specifically, an active mention mkse-lects as its antecedent the closest preceding men-tion that is classified as coreferent with mk.
If mkis not classified as coreferent with any precedingmention, it will be considered discourse-new (i.e.,no antecedent will be selected for mk).3.2 Entity-Mention ModelUnlike a mention-pair model, an entity-mentionmodel is a classifier that decides whether or notan active mention mkis coreferent with a par-tial cluster cjthat precedes mk.
Each traininginstance, i(cj,mk), represents cjand mk.
Thefeatures for an instance can be divided into twotypes: (1) features that describe mk(i.e, thoseshown in the second block of Table 1), and (2)cluster-level features, which describe the relation-ship between cjand mk.
Motivated by previ-ous work (Luo et al, 2004; Culotta et al, 2007;Yang et al, 2008), we create cluster-level fea-tures from mention-pair features using four pred-icates: NONE, MOST-FALSE, MOST-TRUE, andALL.
Specifically, for each feature X shown inthe last two blocks in Table 1, we first convert Xinto an equivalent set of binary-valued features ifit is multi-valued.
Then, for each resulting binary-valued feature Xb, we create four binary-valuedcluster-level features: (1) NONE-Xbis true whenXbis false between mkand each mention in cj; (2)MOST-FALSE-Xbis true when Xbis true betweenmkand less than half (but at least one) of the men-tions in cj; (3) MOST-TRUE-Xbis true when Xbistrue between mkand at least half (but not all) ofthe mentions in cj; and (4) ALL-Xbis true when Xbis true between mkand each mention in cj.
Hence,for each Xb, exactly one of these four cluster-levelfeatures evaluates to true.Following Yang et al (2008), we create (1) apositive instance for each discourse-old mentionmkand the preceding cluster cjto which it be-longs; and (2) a negative instance for mkpairedwith each partial cluster whose last mention ap-pears between mkand its closest antecedent (i.e.,the last mention of cj).
Consider again our run-ning example.
Three training instances will begenerated for He: i({Monday}, He), i({secretaryof state}, He), and i({Barack Obama, his}, He).The first two of these instances will be labeled asnegative, and the last one will be labeled as pos-itive.
As in the mention-pair model, we train anentity-mention classifier using the SVM learner.After training, the resulting classifier is used toidentify a preceding cluster for a mention in a testtext.
Specifically, the mentions are processed ina left-to-right manner.
For each active mentionmk, a test instance is created between mkandeach of the preceding clusters formed so far.
Allthe test instances are then presented to the classi-fier.
Finally, mkwill be linked to the closest pre-ceding cluster that is classified as coreferent withmk.
If mkis not classified as coreferent with any971Features describing mj, a candidate antecedent1 PRONOUN 1 Y if mjis a pronoun; else N2 SUBJECT 1 Y if mjis a subject; else N3 NESTED 1 Y if mjis a nested NP; else NFeatures describing mk, the mention to be resolved4 NUMBER 2 SINGULAR or PLURAL, determined using a lexicon5 GENDER 2 MALE, FEMALE, NEUTER, or UNKNOWN, determined using a list of common first names6 PRONOUN 2 Y if mkis a pronoun; else N7 NESTED 2 Y if mkis a nested NP; else N8 SEMCLASS 2 the semantic class of mk; can be one of PERSON, LOCATION, ORGANIZATION, DATE, TIME,MONEY, PERCENT, OBJECT, OTHERS, determined using WordNet and an NE recognizer9 ANIMACY 2 Y if mkis determined as HUMAN or ANIMAL by WordNet and an NE recognizer; else N10 PRO TYPE 2 the nominative case of mkif it is a pronoun; else NA.
E.g., the feature value for him is HEFeatures describing the relationship between mj, a candidate antecedent and mk, the mention to be resolved11 HEAD MATCH C if the mentions have the same head noun; else I12 STR MATCH C if the mentions are the same string; else I13 SUBSTR MATCH C if one mention is a substring of the other; else I14 PRO STR MATCH C if both mentions are pronominal and are the same string; else I15 PN STR MATCH C if both mentions are proper names and are the same string; else I16 NONPRO STR MATCH C if the two mentions are both non-pronominal and are the same string; else I17 MODIFIER MATCH C if the mentions have the same modifiers; NA if one of both of them don?t have a modifier;else I18 PRO TYPE MATCH C if both mentions are pronominal and are either the same pronoun or different only w.r.t.case; NA if at least one of them is not pronominal; else I19 NUMBER C if the mentions agree in number; I if they disagree; NA if the number for one or bothmentions cannot be determined20 GENDER C if the mentions agree in gender; I if they disagree; NA if the gender for one or both mentionscannot be determined21 AGREEMENT C if the mentions agree in both gender and number; I if they disagree in both number andgender; else NA22 ANIMACY C if the mentions match in animacy; I if they don?t; NA if the animacy for one or both mentionscannot be determined23 BOTH PRONOUNS C if both mentions are pronouns; I if neither are pronouns; else NA24 BOTH PROPER NOUNS C if both mentions are proper nouns; I if neither are proper nouns; else NA25 MAXIMALNP C if the two mentions does not have the same maximial NP projection; else I26 SPAN C if neither mention spans the other; else I27 INDEFINITE C if mkis an indefinite NP and is not in an appositive relationship; else I28 APPOSITIVE C if the mentions are in an appositive relationship; else I29 COPULAR C if the mentions are in a copular construction; else I30 SEMCLASS C if the mentions have the same semantic class; I if they don?t; NA if the semantic classinformation for one or both mentions cannot be determined31 ALIAS C if one mention is an abbreviation or an acronym of the other; else I32 DISTANCE binned values for sentence distance between the mentionsAdditional features describing the relationship between mj, a candidate antecedent and mk, the mention to be resolved33 NUMBER?
the concatenation of the NUMBER 2 feature values of mjand mk.
E.g., if mjis Clinton andmkis they, the feature value is SINGULAR-PLURAL, since mjis singular and mkis plural34 GENDER?
the concatenation of the GENDER 2 feature values of mjand mk35 PRONOUN?
the concatenation of the PRONOUN 2 feature values of mjand mk36 NESTED?
the concatenation of the NESTED 2 feature values of mjand mk37 SEMCLASS?
the concatenation of the SEMCLASS 2 feature values of mjand mk38 ANIMACY?
the concatenation of the ANIMACY 2 feature values of mjand mk39 PRO TYPE?
the concatenation of the PRO TYPE 2 feature values of mjand mkTable 1: The feature set for coreference resolution.
Non-relational features describe a mention and inmost cases take on a value of YES or NO.
Relational features describe the relationship between the twomentions and indicate whether they are COMPATIBLE, INCOMPATIBLE or NOT APPLICABLE.preceding cluster, it will be considered discourse-new.
Note that all partial clusters preceding mkare formed incrementally based on the predictionsof the classifier for the first k ?
1 mentions.3.3 Mention-Ranking ModelAs noted before, a ranking model imposes aranking on all the candidate antecedents of anactive mention mk.
To train a ranker, weuse the SVM ranker-learning algorithm from theSVMlight package.
Like the mention-pair model,each training instance i(mj,mk) represents mkand a preceding mention mj.
In fact, the fea-tures that represent the instance as well as themethod for creating training instances are identi-cal to those employed by the mention-pair model.972The only difference lies in the assignment ofclass values to training instances.
Assuming thatSkis the set of training instances created foranaphoric mention mk, the class value for an in-stance i(mj,mk) in Skis the rank of mjamongcompeting candidate antecedents, which is 2 ifmjis the closest antecedent of mk, and 1 other-wise.1 To exemplify, consider our running exam-ple.
As in the mention-pair model, three traininginstances will be generated for He: i(Monday, He),i(secretary of state, He), i(his, He).
The third in-stance will have a class value of 2, and the remain-ing two will have a class value of 1.After training, the mention-ranking model is ap-plied to rank the candidate antecedents for an ac-tive mention in a test text as follows.
Given an ac-tive mention mk, we follow Denis and Baldridge(2008) and use an independently-trained classifierto determine whether mkis discourse-new.
If so,mkwill not be resolved.
Otherwise, we create testinstances for mkby pairing it with each of its pre-ceding mentions.
The test instances are then pre-sented to the ranker, and the preceding mentionthat is assigned the largest value by the ranker isselected as the antecedent of mk.The discourse-new classifier used in the resolu-tion step is trained with 26 of the 37 features2 de-scribed in Ng and Cardie (2002a) that are deemeduseful for distinguishing between anaphoric andnon-anaphoric mentions.
These features can bebroadly divided into two types: (1) features thatencode the form of the mention (e.g., NP type,number, definiteness), and (2) features that com-pare the mention to one of its preceding mentions.4 Coreference as Cluster RankingIn this section, we describe our cluster-ranking ap-proach to NP coreference.
As noted before, ourapproach aims to combine the strengths of entity-mention models and mention-ranking models.4.1 Training and Applying a Cluster RankerFor ease of exposition, we will describe in thissubsection how to train and apply a cluster rankerwhen it is used in a pipeline architecture, wherediscourse-new detection is performed prior tocoreference resolution.
In the next subsection, wewill show how the two tasks can be learned jointly.1A larger class value implies a better rank in SVMlight.2The 11 features that we did not employ are CONJ,POSSESSIVE, MODIFIER, POSTMODIFIED, SPECIAL NOUNS,POST, SUBCLASS, TITLE, and the positional features.Recall that a cluster ranker ranks a set of pre-ceding clusters for an active mention mk.
Sincea cluster ranker is a hybrid of a mention-rankingmodel and an entity-mention model, the way it istrained and applied is also a hybrid of the two.In particular, the instance representation employedby a cluster ranker is identical to that used byan entity-mention model, where each training in-stance i(cj, mk) represents a preceding cluster cjand a discourse-old mention mkand consists ofcluster-level features formed from predicates.
Un-like in an entity-mention model, however, in acluster ranker, (1) a training instance is created be-tween each discourse-old mention mkand each ofits preceding clusters; and (2) since we are train-ing a model for ranking clusters, the assignment ofclass values to training instances is similar to thatof a mention ranker.
Specifically, the class value ofa training instance i(cj, mk) created for mkis therank of cjamong the competing clusters, which is2 if mkbelongs to cj, and 1 otherwise.Applying the learned cluster ranker to a test textis similar to applying a mention ranker.
Specifi-cally, the mentions are processed in a left-to-rightmanner.
For each active mention mk, we firstapply an independently-trained classifier to deter-mine if mkis discourse-new.
If so, mkwill not beresolved.
Otherwise, we create test instances formkby pairing it with each of its preceding clus-ters.
The test instances are then presented to theranker, and mkis linked to the cluster that is as-signed the highest value by the ranker.
Note thatthese partial clusters preceding mkare formed in-crementally based on the predictions of the rankerfor the first k?1 mentions; no gold-standard coref-erence information is used in their formation.4.2 Joint Discourse-New Detection andCoreference ResolutionThe cluster ranker described above can be usedto determine which preceding cluster a discourse-old mention should be linked to, but it cannot beused to determine whether a mention is discourse-new or not.
The reason is simple: all the traininginstances are generated from discourse-old men-tions.
Hence, to jointly learn discourse-new de-tection and coreference resolution, we must trainthe ranker using instances generated from bothdiscourse-old and discourse-new mentions.Specifically, when training the ranker, we pro-vide each active mention with the option to start973a new cluster by creating an additional instancethat (1) contains features that solely describe theactive mention (i.e., the features shown in the sec-ond block of Table 1), and (2) has the highest rankvalue among competing clusters (i.e., 2) if it isdiscourse-new and the lowest rank value (i.e., 1)otherwise.
The main advantage of jointly learningthe two tasks is that it allows the ranking modelto evaluate all possible options for an active men-tion (i.e., whether to resolve it, and if so, whichpreceding cluster is the best) simultaneously.After training, the resulting cluster ranker pro-cesses the mentions in a test text in a left-to-rightmanner.
For each active mention mk, we createtest instances for it by pairing it with each of itspreceding clusters.
To allow for the possibility thatmkis discourse-new, we create an additional testinstance that contains features that solely describethe active mention (similar to what we did in thetraining step above).
All these test instances arethen presented to the ranker.
If the additional testinstance is assigned the highest rank value by theranker, then mkis classified as discourse-new andwill not be resolved.
Otherwise, mkis linked tothe cluster that has the highest rank.
As before,all partial clusters preceding mkare formed incre-mentally based on the predictions of the ranker forthe first k ?
1 mentions.5 Evaluation5.1 Experimental SetupCorpus.
We use the ACE 2005 coreference cor-pus as released by the LDC, which consists of the599 training documents used in the official ACEevaluation.3 To ensure diversity, the corpus wascreated by selecting documents from six differentsources: Broadcast News (bn), Broadcast Con-versations (bc), Newswire (nw), Webblog (wb),Usenet (un), and conversational telephone speech(cts).
The number of documents belonging to eachsource is shown in Table 2.
For evaluation, we par-tition the 599 documents into a training set and atest set following a 80/20 ratio, ensuring that thetwo sets have the same proportion of documentsfrom the six sources.Mention extractor.
We evaluate each corefer-ence model using both true mentions (i.e., goldstandard mentions4) and system mentions (i.e., au-3Since we did not participate in ACE 2005, we do nothave access to the official test set.4Note that only mention boundaries are used.Dataset bn bc nw wl un cts# of documents 60 226 106 119 49 39Table 2: Statistics for the ACE 2005 corpustomatically identified mentions).
To extract sys-tem mentions from a test text, we trained a men-tion extractor on the training texts.
Following Flo-rian et al (2004), we recast mention extraction asa sequence labeling task, where we assign to eachtoken in a test text a label that indicates whether itbegins a mention, is inside a mention, or is outsidea mention.
Hence, to learn the extractor, we createone training instance for each token in a trainingtext and derive its class value (one of b, i, and o)from the annotated data.
Each instance representswi, the token under consideration, and consists of29 linguistic features, many of which are modeledafter the systems of Bikel et al (1999) and Florianet al (2004), as described below.Lexical (7): Tokens in a window of 7:{wi?3, .
.
.
, wi+3}.Capitalization (4): Determine whether wiIsAllCap, IsInitCap, IsCapPeriod, andIsAllLower (see Bikel et al (1999)).Morphological (8): wi?s prefixes and suffixes oflength one, two, three, and four.Grammatical (1): The part-of-speech (POS)tag of wiobtained using the Stanford log-linearPOS tagger (Toutanova et al, 2003).Semantic (1): The named entity (NE) tag of wiobtained using the Stanford CRF-based NE recog-nizer (Finkel et al, 2005).Gazetteers (8): Eight dictionaries containingpronouns (77 entries), common words and wordsthat are not names (399.6k), person names (83.6k),person titles and honorifics (761), vehicle words(226), location names (1.8k), company names(77.6k), and nouns extracted from WordNet thatare hyponyms of PERSON (6.3k).We employ CRF++5, a C++ implementation ofconditional random fields, for training the mentiondetector, which achieves an F-score of 86.7 (86.1recall, 87.2 precision) on the test set.
These ex-tracted mentions are to be used as system mentionsin our coreference experiments.Scoring programs.
To score the output of acoreference model, we employ three scoring pro-grams: MUC (Vilain et al, 1995), B3 (Bagga andBaldwin, 1998), and ?3-CEAF (Luo, 2005).5Available from http://crfpp.sourceforge.net974There is a complication, however.
When scor-ing a response (i.e., system-generated) partitionagainst a key (i.e., gold-standard) partition, a scor-ing program needs to construct a mapping betweenthe mentions in the response and those in the key.If the response is generated using true mentions,then every mention in the response is mapped tosome mention in the key and vice versa; in otherwords, there are no twinless (i.e., unmapped) men-tions (Stoyanov et al, 2009).
However, this isnot the case when system mentions are used.
Theaforementioned complication does not arise fromthe construction of the mapping, but from the factthat Bagga and Baldwin (1998) and Luo (2005) donot specify how to apply B3 and CEAF to scorepartitions generated from system mentions.We propose a simple solution to this problem:we remove all and only those twinless systemmentions that are singletons before applying B3and CEAF.
The reason is simple: since the coref-erence resolver has successfully identified thesementions as singletons, it should not be penal-ized, and removing them allows us to avoid suchpenalty.
Note that we only remove twinless (as op-posed to all) system mentions that are singletons:this allows us to reward a resolver for success-ful identification of singleton mentions that havetwins, thus overcoming a major weakness of andcommon criticism against the MUC scorer.
Also,we retain twinless system mentions that are non-singletons, as the resolver should be penalized foridentifying spurious coreference relations.
On theother hand, we do not remove twinless mentionsin the key partition, as we want to ensure that theresolver makes the correct (non-)coreference de-cisions for them.
We believe that our proposal ad-dresses Stoyanov et al?s (2009) problem of hav-ing very low precision when applying the CEAFscorer to score partitions of system mentions.5.2 Results and DiscussionsThe mention-pair baseline.
We train our firstbaseline, the mention-pair coreference classifier,using the SVM learning algorithm as implementedin the SVMlight package (Joachims, 2002).6 Re-sults of this baseline using true mentions and sys-tem mentions, shown in row 1 of Tables 3 and 4,are reported in terms of recall (R), precision (P),and F-score (F) provided by the three scoring pro-6For this and subsequent uses of the SVM learner in ourexperiments, we set al parameters to their default values.grams.
As we can see, this baseline achieves F-scores of 54.3?70.0 and 53.4?62.5 for true men-tions and system mentions, respectively.The entity-mention baseline.
Next, we trainour second baseline, the entity-mention corefer-ence classifier, using the SVM learner.
Results ofthis baseline are shown in row 2 of Tables 3 and4.
For true mentions, this baseline achieves an F-score of 54.8?70.7.
In comparison to the mention-pair baseline, F-score rises insignificantly accord-ing to all three scorers.7 Similar trends can be ob-served for system mentions, where the F-scoresbetween the two models are statistically indistin-guishable across the board.
While the insignifi-cant performance difference is somewhat surpris-ing given the improved expressiveness of entity-mention models over mention-pair models, similartrends have been reported by Luo et al (2004).The mention-ranking baseline.
Our third base-line is the mention-ranking coreference model,trained using the ranker-learning algorithm inSVMlight.
To identify discourse-new mentions,we employ two methods.
In the first method, weadopt a pipeline architecture, where we train anSVM classifier for discourse-new detection inde-pendently of the mention ranker on the training setusing the 26 features described in Section 3.3.
Wethen apply the resulting classifier to each test textto filter discourse-new mentions prior to corefer-ence resolution.
Results of the mention ranker areshown in row 3 of Tables 3 and 4.
As we cansee, the ranker achieves F-scores of 57.8?71.2 and54.1?65.4 for true mentions and system mentions,respectively, yielding a significant improvementover the entity-mention baseline in all but one case(MUC/true mentions).In the second method, we perform discourse-new detection jointly with coreference resolutionusing the method described in Section 4.2.
Whilewe discussed this joint learning method in the con-text of cluster ranking, it should be easy to seethat the method is equally applicable to a men-tion ranker.
Results of the mention ranker usingthis joint architecture are shown in row 4 of Ta-bles 3 and 4.
As we can see, the ranker achievesF-scores of 61.6?73.4 and 55.6?67.1 for true men-tions and system mentions, respectively.
For bothtypes of mentions, the improvements over the cor-responding results for the entity-mention baseline7We use Approximate Randomization (Noreen, 1989) fortesting statistical significance, with p set to 0.05.975MUC CEAF B3Coreference Model R P F R P F R P F1 Mention-pair model 71.7 69.2 70.4 54.3 54.3 54.3 53.3 63.6 58.02 Entity-mention model 71.7 69.7 70.7 54.8 54.8 54.8 53.2 65.1 58.53 Mention-ranking model (Pipeline) 68.7 73.9 71.2 57.8 57.8 57.8 55.8 63.9 59.64 Mention-ranking model (Joint) 69.4 77.8 73.4 61.6 61.6 61.6 57.0 70.1 62.95 Cluster-ranking model (Pipeline) 71.7 78.2 74.8 61.8 61.8 61.8 58.2 69.1 63.26 Cluster-ranking model (Joint) 69.9 83.3 76.0 63.3 63.3 63.3 56.0 74.6 64.0Table 3: MUC, CEAF, and B3 coreference results using true mentions.MUC CEAF B3Coreference Model R P F R P F R P F1 Mention-pair model 70.0 56.4 62.5 56.1 51.0 53.4 50.8 57.9 54.12 Entity-mention model 68.5 57.2 62.3 56.3 50.2 53.1 51.2 57.8 54.33 Mention-ranking model (Pipeline) 62.2 68.9 65.4 51.6 56.7 54.1 52.3 61.8 56.64 Mention-ranking model (Joint) 62.1 73.0 67.1 53.0 58.5 55.6 50.4 65.5 56.95 Cluster-ranking model (Pipeline) 65.3 72.3 68.7 54.1 59.3 56.6 55.3 63.7 59.26 Cluster-ranking model (Joint) 64.1 75.4 69.3 56.7 62.6 59.5 54.4 70.5 61.4Table 4: MUC, CEAF, and B3 coreference results using system mentions.are significant, and suggest that mention ranking isa precision-enhancing device.
Moreover, in com-parison to the pipeline architecture in row 3, wesee that F-score rises significantly by 2.2?3.8% fortrue mentions, and improves by a smaller marginof 0.3?1.7% for system mentions.
These resultsdemonstrate the benefits of joint modeling.Our cluster-ranking model.
Finally, we evalu-ate our cluster-ranking model.
As in the mention-ranking baseline, we employ both the pipeline ar-chitecture and the joint architecture for discourse-new detection.
Results are shown in rows 5 and6 of Tables 3 and 4, respectively, for the two ar-chitectures.
When true mentions are used, thepipeline architecture yields an F-score of 61.8?74.8, which represents a significant improvementover the mention ranker adopting the pipeline ar-chitecture.
With the joint architecture, the clus-ter ranker achieves an F-score of 63.3?76.0.
Thisalso represents a significant improvement over themention ranker adopting the joint architecture, thebest of the baselines, and suggests that clusterranking is a better precision-enhancing model thanmention ranking.
Moreover, comparing the re-sults in these two rows reveals the superiority ofthe joint architecture over the pipeline architec-ture, particularly in terms of its ability to enhancesystem precision.
Similar performance trends canbe observed when system mentions are used.6 ConclusionsWe have presented a cluster-ranking approach thatrecasts the mention resolution process as the prob-lem of finding the best preceding cluster to link anactive mention to.
Crucially, our approach com-bines the strengths of entity-mention models andmention-ranking models.
Experimental results onthe ACE 2005 corpus show that (1) jointly learn-ing coreference resolution and discourse-new de-tection allows the cluster ranker to achieve bet-ter performance than adopting a pipeline corefer-ence architecture; and (2) our cluster ranker signif-icantly outperforms the mention ranker, the best ofthe three baseline coreference models, under boththe pipeline architecture and the joint architecture.Overall, we believe that our cluster-ranking ap-proach advances the state-of-the-art in coreferenceresolution both theoretically and empirically.AcknowledgmentsWe thank the three anonymous reviewers for theirinvaluable comments on the paper.
This work wassupported in part by NSF Grant IIS-0812261.ReferencesA.
Bagga and B. Baldwin.
1998.
Entity-based cross-document coreferencing using the vector spacemodel.
In Proc.
of COLING-ACL, pages 79?85.D.
Bean and E. Riloff.
1999.
Corpus-based identifica-tion of non-anaphoric noun phrases.
In Proc.
of theACL, pages 373?380.E.
Bengtson and D. Roth.
2008.
Understanding thevalues of features for coreference resolution.
InProc.
of EMNLP, pages 294?303.S.
Bergsma, D. Lin, and R. Goebel.
2008.
Distribu-tional identification of non-referential pronouns.
InProc.
of ACL-08:HLT, pages 10?18.976D.
Bikel, R. Schwartz, and R. Weischedel.
1999.
Analgorithm that learns what?s in a name.
MachineLearning, 34(1?3):211?231.D.
Byron and W. Gegg-Harrison.
2004.
Eliminatingnon-referring noun phrases from coreference resolu-tion.
In Proc.
of DAARC, pages 21?26.A.
Culotta, M. Wick, and A. McCallum.
2007.
First-order probabilistic models for coreference resolu-tion.
In Proc.
of NAACL-HLT, pages 81?88.P.
Denis and J. Baldridge.
2007.
Global, joint determi-nation of anaphoricity and coreference resolution us-ing integer programming.
In Proc.
of NAACL-HLT,pages 236?243.P.
Denis and J. Baldridge.
2008.
Specialized modelsand ranking for coreference resolution.
In Proc.
ofEMNLP, pages 660?669.M.
Elsner and E. Charniak.
2007.
A generativediscourse-new model for text coherence.
TechnicalReport CS-07-04, Brown University.J.
R. Finkel, T. Grenager, and C. Manning.
2005.
In-corporating non-local information into informationextraction systems by Gibbs sampling.
In Proc.
ofthe ACL, pages 363?370.R.
Florian, H. Hassan, A. Ittycheriah, H. Jing,N.
Kambhatla, X. Luo, N. Nicolov, and I. Zitouni.2004.
A statistical model for multilingual entity de-tection and tracking.
In Proc.
of HLT/NAACL.B.
J. Grosz, A. K. Joshi, and S. Weinstein.
1995.Centering: A framework for modeling the local co-herence of discourse.
Computational Linguistics,21(2):203?226.B.
J. Grosz and C. L. Sidner.
1986.
Attention, inten-tions, and the structure of discourse.
ComputationalLinguistics, 12(3):175?204.T.
Joachims.
2002.
Optimizing search engines usingclickthrough data.
In Proc.
of KDD, pages 133?142.A.
Kehler, D. Appelt, L. Taylor, and A. Simma.
2004.The (non)utility of predicate-argument frequenciesfor pronoun interpretation.
In Proc.
of HLT/NAACL.C.
Kennedy and B. Boguraev.
1996.
Anaphor for ev-eryone: Pronominal anaphora resolution without aparser.
In Proc.
of COLING, pages 113?118.S.
Lappin and H. Leass.
1994.
An algorithm forpronominal anaphora resolution.
ComputationalLinguistics, 20(4):535?562.X.
Luo, A. Ittycheriah, H. Jing, N. Kambhatla, andS.
Roukos.
2004.
A mention-synchronous coref-erence resolution algorithm based on the Bell tree.In Proc.
of the ACL, pages 135?142.X.
Luo.
2005.
On coreference resolution performancemetrics.
In Proc.
of HLT/EMNLP, pages 25?32.A.
McCallum and B. Wellner.
2004.
Conditional mod-els of identity uncertainty with application to nouncoreference.
In Advances in NIPS.R.
Mitkov.
2002.
Anaphora Resolution.
Longman.R.
Mitkov.
1998.
Robust pronoun resolution with lim-ited knowledge.
In Proc.
of COLING/ACL, pages869?875.C.
Mu?ller.
2006.
Automatic detection of nonrefer-ential it in spoken multi-party dialog.
In Proc.
ofEACL, pages 49?56.V.
Ng and C. Cardie.
2002a.
Identifying anaphoric andnon-anaphoric noun phrases to improve coreferenceresolution.
In Proc.
of COLING, pages 730?736.V.
Ng and C. Cardie.
2002b.
Improving machine learn-ing approaches to coreference resolution.
In Proc.
ofthe ACL, pages 104?111.E.
W. Noreen.
1989.
Computer Intensive Methods forTesting Hypothesis: An Introduction.
John Wiley &Sons.M.
Poesio, O. Uryupina, R. Vieira, M. Alexandrov-Kabadjov, and R. Goulart.
2004.
Discourse-newdetectors for definite description resolution: A sur-vey and a preliminary proposal.
In Proc.
of the ACLWorkshop on Reference Resolution.S.
P. Ponzetto and M. Strube.
2006.
Exploiting seman-tic role labeling, WordNet and Wikipedia for coref-erence resolution.
In Proc.
of HLT/NAACL, pages192?199.W.
M. Soon, H. T. Ng, and D. Lim.
2001.
Amachine learning approach to coreference resolu-tion of noun phrases.
Computational Linguistics,27(4):521?544.V.
Stoyanov, N. Gilbert, C. Cardie, and E. Riloff.
2009.Conundrums in noun phrase coreference resolution:Making sense of the state-of-the-art.
In Proc.
of theACL.K.
Toutanova, D. Klein, C. Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging witha cyclic dependency network.
In Proc.
of HLT-NAACL, pages 252?259.O.
Uryupina.
2003.
High-precision identification ofdiscourse new and unique noun phrases.
In Proc.
ofthe ACL Student Research Workshop.Y.
Versley, A. Moschitti, M. Poesio, and X. Yang.2008.
Coreference systems based on kernel meth-ods.
In Proc.
of COLING, pages 961?968.R.
Vieira and M. Poesio.
2000.
Processing definite de-scriptions in corpora.
In Corpus-based and Compu-tational Approaches to Discourse Anaphora, pages189?212.
UCL Press.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic coreferencescoring scheme.
In Proc.
of MUC-6, pages 45?52.M.
Walker, A. Joshi, and E. Prince, editors.
1998.Centering Theory in Discourse.
Oxford UniversityPress.X.
Yang, G. Zhou, J. Su, and C. L. Tan.
2003.Coreference resolution using competitive learningapproach.
In Proc.
of the ACL, pages 176?183.X.
Yang, J. Su, G. Zhou, and C. L. Tan.
2004.
An NP-cluster based approach to coreference resolution.
InProc.
of COLING, pages 226?232.X.
Yang, J. Su, J. Lang, C. L. Tan, and S. Li.
2008.An entity-mention model for coreference resolutionwith inductive logic programming.
In Proc.
of theACL, pages 843?851.977
