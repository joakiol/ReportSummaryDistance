Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 73?76,New York, June 2006. c?2006 Association for Computational LinguisticsEarly Deletion of Fillers In Processing Conversational SpeechMatthew Lease and Mark JohnsonBrown Laboratory for Linguistic Information Processing (BLLIP)Brown UniversityProvidence, RI 02912{mlease,mj}@cs.brown.eduAbstractThis paper evaluates the benefit of deleting fillers(e.g.
you know, like) early in parsing conver-sational speech.
Readability studies have shownthat disfluencies (fillers and speech repairs) maybe deleted from transcripts without compromisingmeaning (Jones et al, 2003), and deleting repairsprior to parsing has been shown to improve itsaccuracy (Charniak and Johnson, 2001).
We ex-plore whether this strategy of early deletion is alsobeneficial with regard to fillers.
Reported exper-iments measure the effect of early deletion underin-domain and out-of-domain parser training con-ditions using a state-of-the-art parser (Charniak,2000).
While early deletion is found to yield onlymodest benefit for in-domain parsing, significantimprovement is achieved for out-of-domain adap-tation.
This suggests a potentially broader role fordisfluency modeling in adapting text-based toolsfor processing conversational speech.1 IntroductionThis paper evaluates the benefit of deleting fillersearly in parsing conversational speech.
We followLDC (2004) conventions in using the term filler toencompass a broad set of vocalized space-fillers thatcan introduce syntactic (and semantic) ambiguity.For example, in the questionsDid you know I do that?Is it like that one?colloquial use of fillers, indicated below through useof commas, can yield alternative readingsDid, you know, I do that?Is it, like, that one?Readings of the first example differ in querying lis-tener knowledge versus speaker action, while read-ings of the second differ in querying similarity ver-sus exact match.
Though an engaged listener rarelyhas difficulty distinguishing between such alterna-tives, studies show that deleting disfluencies fromtranscripts improves readability with no reduction inreading comprehension (Jones et al, 2003).The fact that disfluencies can be completely re-moved without compromising meaning is important.Earlier work had already made this claim regard-ing speech repairs1 and argued that there was con-sequently little value in syntactically analyzing re-pairs or evaluating our ability to do so (Charniakand Johnson, 2001).
Moreover, this work showedthat collateral damage to parse accuracy caused byrepairs could be averted by deleting them prior toparsing, and this finding has been confirmed in sub-sequent studies (Kahn et al, 2005; Harper et al,2005).
But whereas speech repairs have receivedsignificant attention in the parsing literature, fillershave been relatively neglected.
While one studyhas shown that the presence of interjection and par-enthetical constituents in conversational speech re-duces parse accuracy (Engel et al, 2002), these con-stituent types are defined to cover both fluent anddisfluent speech phenomena (Taylor, 1996), leavingthe impact of fillers alone unclear.In our study, disfluency annotations (Taylor,1995) are leveraged to identify fillers precisely, andthese annotations are merged with treebank syn-tax.
Extending the arguments of Charniak and John-son with regard to repairs (2001), we argue thereis little value in recovering the syntactic structure1See (Core and Schubert, 1999) for a prototypical counter-example that rarely occurs in practice.73of fillers, and we relax evaluation metrics accord-ingly (?3.2).
Experiments performed (?3.3) use astate-of-the-art parser (Charniak, 2000) to study theimpact of early filler deletion under in-domain andout-of-domain (i.e.
adaptation) training conditions.In terms of adaptation, there is tremendous poten-tial in applying textual tools and training data toprocessing transcribed speech (e.g.
machine trans-lation, information extraction, etc.
), and bleachingspeech data to more closely resemble text has beenshown to improve accuracy with some text-basedprocessing tasks (Rosenfeld et al, 1995).
For ourstudy, a state-of-the-art filler detector (Johnson et al,2004) is employed to delete fillers prior to parsing.Results show parse accuracy improves significantly,suggesting disfluency filtering may have a broad rolein enabling text-based processing of speech data.2 Disfluency in BriefIn this section we give a brief introduction to disflu-ency, providing an excerpt from Switchboard (Graffand Bird, 2000) that demonstrates typical productionof repairs and fillers in conversational speech.We follow previous work (Shriberg, 1994) in de-scribing a repair in terms of three parts: the reparan-dum (the material repaired), the corrected alteration,and between these an optional interregnum (or edit-ing term) consisting of one or more fillers.
Our no-tion of fillers encompasses filled pauses (e.g.
uh,um, ah) as well as other vocalized space-fillersannotated by LDC (Taylor, 1995), such as youknow, i mean, like, so, well, etc.
An-notations shown here are typeset with the followingconventions: fillers are bold, [reparanda] are square-bracketed, and alterations are underlined.S1: Uh first um i need to know uh howdo you feel [about] uh about sending uhan elderly uh family member to a nursinghomeS2: Well of course [it?s] you know it?s oneof the last few things in the world you?dever want to do you know unless it?s justyou know really you know uh [for their]uh you know for their own goodThough disfluencies rarely complicate understand-ing for an engaged listener, deleting them from tran-scripts improves readability with no reduction inreading comprehension (Jones et al, 2003).
For au-tomated analysis of speech data, this means we mayfreely explore processing alternatives which deletedisfluencies without compromising meaning.3 ExperimentsThis section reports parsing experiments studyingthe effect of early deletion under in-domain and out-of-domain parser training conditions using the Au-gust 2005 release of the Charniak parser (2000).
Wedescribe data and evaluation metrics used, then pro-ceed to describe the experiments.3.1 DataConversational speech data was drawn from theSwitchboard corpus (Graff and Bird, 2000), whichannotates disfluency (Taylor, 1995) as well as syn-tax.
Our division of the corpus follows that usedin (Charniak and Johnson, 2001).
Speech recognizer(ASR) output is approximated by removing punctua-tion, partial words, and capitalization, but we do usereference words, representing an upperbound condi-tion of perfect ASR.
Likewise, annotated sentenceboundaries are taken to represent oracle boundarydetection.
Because fillers are annotated only indisfluency markup, we perform an automatic treetransform to merge these two levels of annotation:each span of contiguous filler words were prunedfrom their corresponding tree and then reinserted atthe same position under a flat FILLER constituent,attached as highly as possible.
Transforms wereachieved using TSurgeon2 and Lingua::Treebank3.For our out-of-domain training condition, theparser was trained on sections 2-21 of the Wall StreetJournal (WSJ) corpus (Marcus et al, 1993).
Punctu-ation and capitalization were removed to bleach ourour textual training data to more closely resemblespeech (Rosenfeld et al, 1995).
We also tried auto-matically changing numbers, symbols, and abbrevi-ations in the training text to match how they wouldbe read (Roark, 2002), but this did not improve ac-curacy and so is not discussed further.3.2 Evaluation MetricsAs discussed earlier (?1), Charniak and John-son (2001) have argued that speech repairs do not2http://nlp.stanford.edu/software/tsurgeon.shtml3http://www.cpan.org74contribute to meaning and so there is little valuein syntactically analyzing repairs or evaluating ourability to do so.
Consequently, they relaxed stan-dard PARSEVAL (Black et al, 1991) to treat EDITEDconstituents like punctuation: adjacent EDITED con-stituents are merged, and the internal structure andattachment of EDITED constituents is not evaluated.We propose generalizing this approach to disfluencyat large, i.e.
fillers as well as repairs.
Note that thedetails of appropriate evaluation metrics for parsedspeech data is orthogonal to the parsing methodsproposed here: however parsing is performed, weshould avoid wasting metric attention evaluatingsyntax of words that do not contribute toward mean-ing and instead evaluate only how well such wordscan be identified.Relaxed metric treatment of disfluency wasachieved via simple parameterization of the SPar-seval tool (Harper et al, 2005).
SParseval alsohas the added benefit of calculating a dependency-based evaluation alongside PARSEVAL?s bracket-based measure.
The dependency metric performssyntactic head-matching for each word using a setof given head percolation rules (derived from Char-niak?s parser (2000)), and its relaxed formulationignores terminals spanned by FILLER and EDITEDconstituents.
We found this metric offered additionalinsights in analyzing some of our results.3.3 ResultsIn the first set of experiments, we train the parser onSwitchboard and contrast early deletion of disfluen-cies (identified by an oracle) versus parsing in themore usual fashion.
Our method for early deletiongeneralizes the approach used with repairs in (Char-niak and Johnson, 2001): contiguous filler and editwords are deleted from the input strings, the stringsare parsed, and the removed words are reinsertedinto the output trees under the appropriate flat con-stituent, FILLER or EDITED.Results in Table 1 give F-scores for PARSEVALand dependency-based parse accuracy (?3.2), as wellas per-word edit and filler detection accuracy (i.e.how well the parser does in identifying which termi-nals should be spanned by EDITED and FILLER con-stituents when early deletion is not performed).
Wesee that the parser correctly identifies filler wordswith 93.1% f-score, and that early deletion of fillersTable 1: F-scores on Switchboard when trained in-domain.
LB and Dep refer to relaxed labelled-bracket and dependency parse metrics (?3.2).
Editand filler word detection f-scores are also shown.Edits Fillers Edit F Filler F LB Deporacle oracle 100.0 100.0 88.9 88.5oracle parser 100.0 93.1 87.8 87.9parser oracle 64.3 100.0 85.0 85.6parser parser 62.4 94.1 83.9 85.0(via oracle knowledge) yields only a modest im-provement in parsing accuracy (87.8% to 88.9%bracket-based, 87.9% to 88.5% dependency-based).We conclude from this that for in-domain training,early deletion of fillers has limited potential to im-prove parsing accuracy relative to what has beenseen with repairs.
It is still worth noting, however,that the parser does perform better when fillers areabsent, consistent with Engel et al?s findings (2002).While fillers have been reported to often occur atmajor clause boundaries (Shriberg, 1994), suggest-ing their presence may benefit parsing, we do notfind this to be the case.
Results shown for repair de-tection accuracy and its impact on parsing are con-sistent with previous work (Charniak and Johnson,2001; Kahn et al, 2005; Harper et al, 2005).Our second set of experiments reports the effectof deleting fillers early when the parser is trained ontext only (WSJ, ?3.1).
Our motivation here is to seeif disfluency modeling, particularly filler detection,can help bleach speech data to more closely resem-ble text, thereby improving our ability to process itusing text-based methods and training data (Rosen-feld et al, 1995).
Again we contrast standardparsing with deleting disfluencies early (via oracleknowledge).
Given our particular interest in fillers,we also report the effect of detecting them via astate-of-the-art system (Johnson et al, 2004).Results appear in Table 2.
It is worth noting thatsince our text-trained parser never produces FILLERor EDITED constituents, the bracket-based metricpenalizes it for each such constituent appearing inthe gold trees.
Similarly, since the dependencymetric ignores terminals occurring under these con-stituents in the gold trees, the metric penalizes theparser for producing dependencies for these termi-75Table 2: F-scores parsing Switchboard when trainedon WSJ.
Edit word detection varies between parserand oracle, and filler word detection varies betweennone, system (Johnson et al, 2004), and oracle.Filler F, LB, and Dep are defined as in Table 1.Edits Fillers Filler F LB Deporacle oracle 100.0 83.6 81.4oracle detect 89.3 81.6 80.5oracle none - 71.8 75.4none oracle 100.0 76.3 76.7none detect 74.6 75.9 91.3none none - 66.8 71.5nals.
Taken together, the two metrics provide a com-plementary perspective in interpreting results.The trend observed across metrics and edit detec-tion conditions shows that early deletion of system-detected fillers improves parsing accuracy 5-10%.As seen with in-domain training, early deletion ofrepairs is again seen to have a significant effect.Given that state-of-the-art edit detection performs atabout 80% f-measure (Johnson and Charniak, 2004),much of the benefit derived here from oracle re-pair detection should be realizable in practice.
Thebroader conclusion we draw from these results isthat disfluency modeling has significant potential toimprove text-based processing of speech data.4 ConclusionWhile early deletion of fillers has limited benefit forin-domain parsing of speech data, it can play an im-portant role in bleaching speech data for more accu-rate text-based processing.
Alternative methods ofintegrating detected filler information, such as parsereranking (Kahn et al, 2005), also merit investiga-tion.
It will also be important to evaluate the inter-action with ASR error and sentence boundary de-tection error.
In terms of bleaching, we saw thateven with oracle detection of disfluency, our text-trained model still significantly under-performed thein-domain model, indicating additional methods forbleaching are still needed.
We also plan to evaluat-ing the benefit of disfluency modeling in bleachingspeech data for text-based machine translation.AcknowledgmentsThis work was supported by NSF grants 0121285, LIS9720368,and IIS0095940, and DARPA GALE contract HR0011-06-2-0001.
We would like to thank Brian Roark, Mary Harper, andthe rest of the JHU PASSED team for its support of this work.ReferencesE.
Charniak and M. Johnson.
2001.
Edit detection and parsingfor transcribed speech.
In Proc.
NAACL, pages 118?126.E.
Charniak.
2000.
A maximum-entropy-inspired parser.
InProc.
NAACL, pages 132?139.M.G.
Core and L.K.
Schubert.
1999.
A syntactic frameworkfor speech repairs and other disruptions.
In Proc.
ACL, pages413?420.E.
Black et al 1991.
Procedure for quantitatively comparingthe syntactic coverage of English grammars.
In Proc.
Work-shop on Speech and Natural Language, pages 306?311.D.
Engel, E. Charniak, and M. Johnson.
2002.
Parsing anddisfluency placement.
In Proc.
EMNLP, pages 49?54.D.
Graff and S. Bird.
2000.
Many uses, many annotations forlarge speech corpora: Switchboard and TDT as case studies.In Proc.
LREC, pages 427?433.M.
Harper et al 2005 Johns Hopkins Summer Workshop FinalReport on Parsing and Spoken Structural Event Detection.J.G.
Kahn et al 2005.
Effective use of prosody in parsingconversational speech.
In Proc.
HLT/EMNLP, 233?240.M.
Johnson and E. Charniak.
2004.
A TAG-based noisy chan-nel model of speech repairs.
In Proc.
ACL, pages 33?39.M.
Johnson, E. Charniak, and M. Lease.
2004.
An improvedmodel for recognizing disfluencies in conversational speech.In Proc.
Rich Text 2004 Fall Workshop (RT-04F).D.
Jones et al 2003.
Measuring the readability of automaticspeech-to-text transcripts.
In Proc.
Eurospeech, 1585?1588.Linguistic Data Consortium (LDC).
2004.
Simple metadataannotation specification version 6.2.M.
Marcus et al 1993.
Building a large annotated corpus ofEnglish: The Penn Treebank.
Computational Linguistics,19(2): 313?330.B.
Roark.
2002.
Markov parsing: Lattice rescoring with a sta-tistical parser.
In Proc.
ACL, pages 287?294.R.
Rosenfeld et al 1995.
Error analysis and disfluency mod-eling in the Swichboard domain: 1995 JHU Summer Work-shop project team report.E.
Shriberg.
1994.
Preliminaries to a Theory of Speech Disflu-encies.
Ph.D. thesis, UC Berkeley.A.
Taylor, 1995.
Revision of Meteer et al?s Dysfluency Annota-tion Stylebook for the Switchboard Corpus.
LDC.A.
Taylor, 1996.
Bracketing Switchboard: An addendum to theTreebank II Bracketing Guidelines.
LDC.76
