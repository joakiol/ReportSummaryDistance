Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 161?164,New York, June 2006. c?2006 Association for Computational LinguisticsUsing the Web to Disambiguate AcronymsEiichiro Sumita1, 21 NiCT2 ATR SLCKyoto 619-0288, JAPANeiichiro.sumita@atr.jpFumiaki Sugaya33 KDDI R&D LabsSaitama 356-8502, JAPANfsugaya@kddilabs.jpAbstractThis paper proposes an automatic methodfor disambiguating an acronym with mul-tiple definitions, considering the contextsurrounding the acronym.
First, themethod obtains the Web pages that in-clude both the acronym and its definitions.Second, the method feeds them to the ma-chine learner.
Cross-validation tests re-sults indicate that the current accuracy ofobtaining the appropriate definition for anacronym is around 92% for two ambigu-ous definitions and around 86% for fiveambiguous definitions.1 IntroductionAcronyms are short forms of multiword expres-sions (we call them definitions) that are very con-venient and commonly used, and are constantlyinvented independently everywhere.
What eachone stands for, however, is often ambiguous.
Forexample, ?ACL?
has many different definitions,including ?Anterior Cruciate Ligament (an in-jury),?
?Access Control List (a concept in com-puter security),?
and ?Association forComputational Linguistics (an academic society).
?People tend to write acronyms without their defini-tion added nearby (Table 1), because acronyms areused to avoid the need to type long expressions.Consequently, there is a strong need to disambigu-ate acronyms in order to correctly analyze or re-trieve text.
It is crucial to recognize the correctacronym definition in information retrieval such asa blog search.
Moreover, we need to know themeaning of an acronym to translate it correctly.
Tothe best of our knowledge, no other studies haveapproached this problem.Figure 1 Acronyms and their definitions co-occur in some pages of the WebOn the other side of the coin, an acronymshould be defined in its neighborhood.
For instance,one may find pages that include a certain acronymand its definition (Figure 1).First, our proposed method obtains Web pagesthat include both an acronym and its definitions.Second, the method feeds them to the machinelearner, and the classification program can deter-mine the correct definition according to the contextinformation around the acronym in question.Definition 1 Anterior Cruciate Ligament http://www.ehealthmd.com/library/acltearsShe ended up with a torn ACL, MCL and did some other damage to her knee.
(http://aphotofreak.blogspot.com/2006/01/ill-give-you-everything-i-have-good.html)Definition 2 Access Control List http://en.wikipedia.org/wikiCalculating a user?s effective permissions requires more than simply looking up that user?s name in the ACL.
(http://www.mcsa-exam.com/2006/02/02/effective-permissions.html)Definition 3 Association for Computational Linguistics http://www.aclweb.org/It will be published in the upcoming leading ACL conference.
(http://pahendra.blogspot.com/2005/06/june-14th.html)Table 1 Acronym ?ACL?
without its definition in three different meanings found in blogs161Here, we assume that the list of possible defi-nitions for an acronym is given from sources ex-ternal to this work.
Listing pairs of acronyms andtheir original definitions, on which many studieshave been done, such as Nadeau and Turney(2005), results in high performance.
Some sitessuch as http://www.acronymsearch.com/ orhttp://www.findacronym.com/ provide us withthis function.This paper is arranged as follows.
Section 2explains our solution to the problem, and Section3 reports experimental results.
In Sections 4 and 5we follow with some discussions and relatedworks, and the paper concludes in Section 6.2 The proposalThe idea behind this proposal is based on the ob-servation that an acronym often co-occurs with itsdefinition within a single Web page (Figure 1).For example, the acronym ACL co-occurs withone of its definitions, ?Association for Computa-tional Linguistics,?
211,000 times according togoogle.com.Our proposal is a kind of word-sense disam-biguation (Pedersen and Mihalcea, 2005).
The hitpages can provide us with training data for disam-biguating the acronym in question, and the snip-pets in the pages are fed into the learner of aclassifier.
Features used in classification will beexplained in the latter half of this subsection.We do not stick to a certain method of machinelearning; any state-of-the-art method will suffice.In this paper we employed the decision-tree learn-ing program provided in the WEKA project.Collecting the training data from the WebOur input is the acronym in question, A, and theset of its definitions, {Dk | k=1~K}.for all k =1~K do1.
Search the Web using query of?A AND Dk.?2.
Obtain the set of snippets, {Sl(A, Dk)| l=1~L}.3.
Separate Dk from Sl and obtainthe set of trainingdata,{(Tl(A), Dk)| l=1~L}.EndIn the experiment, L is set to 1,000.
Thus, wehave for each definition Dk of A, at most 1,000training data.Training the classifierFrom training data Tl(A), we create feature vec-tors, which are fed into the learner of the decisiontree with correct definition Dk for the acronym A.Here, we write Tl(A) as W-m W-(m-1) ... W-2 W-1A W1 W2 ... Wm-1 Wm, where m is from 2 to M,which is called the window size hereafter.We use keywords within the window of thesnippet as features, which are binary, i.e., if thekeyword exists in Tl(A), then it is true.
Otherwise,it is null.Keywords are defined in this experiment as thetop N frequent words 1, but for A in the bag con-sisting of all words in {Tl(A)}.
For example, key-words for ?ACL?
are ?Air, Control, and,Advanced, Agents, MS, Computational, Akumiitti,Cruciate, org, of, CMOS, Language, BOS, Agent,gt, HTML, Meeting, with, html, Linguistics, List,Active, EOS, USA, is, access, Adobe, ACL, ACM,BETA, Manager, list, Proceedings, In, A, League,knee, Anterior, ligament, injuries, reconstruction,injury, on, The, tears, tear, control, as, a, Injury, lt,for, Annual, Association, Access, An, that, this,may, an, you, quot, in, the, one, can, This, by, or,be, to, Logic, 39, are, has, 1, from, middot.
?3 Experiment3.1 Acronym and definition preparationWe downloaded a list of acronyms in capital let-ters only from Wikipedia and filtered them byeliminating acronyms shorter than three letters.Then we obtained definitions for each acronymfrom http://www.acronymsearch.com/ and dis-carded acronyms that have less than five defini-tions.
Finally, we randomly selected 20 acronyms.We now have 20 typical acronyms whose am-biguity is more than or equal to five.
For each ac-ronym A, a list of definitions { Dk  | k=1~KK>=5 }, whose elements are ordered by the countof page including A and Dk, is used for the ex-periment.1 In this paper, N is set to 100.1623.2 Ambiguity and accuracyHere we examine the relationship between thedegree of ambiguity and classification accuracyby using a cross-validation test for the trainingdata.#Class M=2 M=5 M=10 Base2 88.7% 90.1% 92.4% 82.3%Table 2 Ambiguity of two#Class M=2 M=5 M=10 Base5 78.6% 82.6% 86.0% 76.5%Table 3 Ambiguity of fiveAmbiguity of twoThe first experiment was performed with the se-lected twenty acronyms by limiting the top twomost frequent definitions.
Table 2 summarizes theten-fold cross validation.
While the accuracychanges acronym by acronym, the average is highabout 90% of the time.
The M in the table denotesthe window size, and the longer the window, thehigher the accuracy.The ?base?
column displays the average accu-racy of the baseline method that always picks themost frequent definition.
The proposed methodachieves better accuracy than the baseline.Ambiguity of fiveNext, we move on to the ambiguity of five (Table3).
As expected, the performance is poorer thanthe abovementioned case, though it is still high,i.e., the average is about 80%.
Other than this, ourobservations were similar to those for the ambigu-ity of two.20.00%30.00%40.00%50.00%60.00%70.00%80.00%90.00%100.00%CEC POP SALT PAL PCI MIPS INT LSD HID RFC BBC UDP WAP ITU VDT NBA CRT JCB EFT ISPAcronymsClassificationaccuracyProposed (W = 10) BaseFigure 2 Bias in distribution of definitions (ambiguity of 5)4 Discussion on biased data4.1 Problem caused by biased distributionand a countermeasure against itFor some words, the baseline is more accuratethan the proposed method because the baselinemethod reaches all occurrences on the Web thanksto the search engine, whereas our method limitsthe number of training data by L as mentioned inSection 2.
The average quantity of training datawas about 830 due to the limit of L, 1,000.
Thedistribution of these training data is rather flat.This causes our classifier to fail in some cases.For example, for the acronym ?ISP,?
the most fre-quent definition out of five has a share of 99.9%(Table 4) on the Web, whereas the distribution inthe training data is different from the sharp distri-bution.
Thus, our classification accuracy is not asgood as that of the baseline.Considering the acronym ?CEC,?
the most fre-quent out of five definitions has the much smallershare of 26.3% on the Web (Table 5), whereas the163distribution in the training data is similar to theflat distribution of real data.
Furthermore, the de-cision tree learns the classification well, whereasthe baseline method performs terribly.These two extreme cases indicate that for someacronyms, our proposed method is beaten by thebaseline method.
The slanting line in Figure 2shows the baseline performance compared withour proposed method.
In the case where ourmethod is strong, the gain is large, and where ourmethod is weak, the reduction is relatively small.The average performance of our proposed methodis higher than that of the baseline.Definition Page hitsInternet Service Provider 3,590,000International Standardized Profile 776Integrated Support Plan 474Interactive String Processor 287Integrated System Peripheral control 266Table 4 Sharp distribution for ?ISP?Definition Page hitsCalifornia Energy Commission 161,000Council for Exceptional Children 159,000Commission of the European Communities 138,000Commission for Environmental Cooperation 77,400Cation Exchange Capacity 76,400Table 5 Flat distribution for ?CEC?A possible countermeasure to this problemwould be to incorporate prior probability into thelearning process.4.2 Possible dissimilarity of training and realdataThe training data used in the above experimentwere only the type of snippets that contain acro-nyms and their definitions; there is no guaranteefor documents that contain only acronyms aresimilar to the training data.
Therefore, learning isnot necessarily successful for real data.
However,we tested our algorithm for a similar problem in-troduced in Section 5.1, where we conducted anopen test and found a promising result, suggestingthat the above-mentioned fear is groundless.5 Related works5.1 Reading proper namesThe contribution of this paper is to propose amethod to use Web pages for a disambiguationtask.
The method is applicable to different prob-lems such as reading Japanese proper names(Sumita and Sugaya, 2006).
Using a Web pagecontaining a name and its syllabary, it is possibleto learn how to read proper names with multiplereadings in a similar way.
The accuracy in ourexperiment was around 90% for open data.5.2 The Web as a corpusRecently, the Web has been used as a corpus inthe NLP community, where mainly counts of hitpages have been exploited (Kilgarriff and Grefen-stette, 2003).
However, our proposal, Web-BasedLanguage Modeling (Sarikaya, 2005), and Boot-strapping Large Sense-Tagged corpora (Mihalcea,2002) use the content within the hit pages.6 ConclusionThis paper proposed an automatic method of dis-ambiguating an acronym with multiple definitions,considering the context.
First, the method obtainsthe Web pages that include both the acronym andits definitions.
Second, the method feeds them tothe learner for classification.
Cross-validation testresults obtained to date indicate that the accuracyof obtaining the most appropriate definition for anacronym is around 92% for two ambiguous defini-tions and around 86% for five ambiguous defini-tions.ReferencesA.
Kilgarriff and G. Grefenstette.
2003.
?Introductionto the special issue on the Web as a corpus,?
Com-putational Linguistics 29(3): 333-348.Rada.
F. Mihalcea, 2002.
?Bootstrapping Large Sense-Tagged Corpora,?
Proc.
of LREC, pp.
1407-1411.David Nadeau and Peter D. Turney, 2005.
?A super-vised learning approach to acronym identification,?18th Canadian Conference on Artificial Intelligence,LNAI3501.Ted Pedersen and Rada.
F. Mihalcea, ?Advances inWord Sense Disambiguation,?
tutorial at ACL 2005.http://www.d.umn.edu/~tpederse/WSDTutorial.html.Ruhi Sarikaya, Hong-kwang Jeff Kuo, and Yuqing Gao,2005.
Impact of Web-Based Language Modeling onSpeech Understanding, Proc.
of ASRU, pp.
268-271.Eiichiro Sumita and Fumiaki Sugaya,.
2006.
?WordPronunciation Disambiguation using the Web,?
Proc.of HLT-NAACL 2006.164
