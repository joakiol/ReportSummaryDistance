Proceedings of the 2009 Named Entities Workshop, ACL-IJCNLP 2009, pages 52?56,Suntec, Singapore, 7 August 2009. c?2009 ACL and AFNLPAbstractThe system presented in this paper usesphrase-based statistical machine translation(SMT) techniques to directly transliterate be-tween all language pairs in this shared task.The technique makes no language specific as-sumptions, uses no dictionaries or explicitphonetic information.
The translation processtransforms sequences of tokens in the sourcelanguage directly into to sequences of tokensin the target.
All language pairs were transli-terated by applying this technique in a singleunified manner.
The machine translation sys-tem used was a system comprised of twophrase-based SMT decoders.
The first gener-ated from the first token of the target to thelast.
The second system generated the targetfrom last to first.
Our results show that if onlyone of these decoding strategies is to be cho-sen, the optimal choice depends on the lan-guages involved, and that in general a combi-nation of the two approaches is able to outper-form either approach.1 IntroductionIt is possible to couch the task of machine trans-literation as a task of machine translation.
Bothprocesses involve the transformation of se-quences of tokens in one language into se-quences of tokens in another language.
Theprinciple differences between the machine trans-lation and language translation are:?
Transliteration does not normally re-quire the re-ordering of tokens that aregenerated  in the target?
The number of types (the vocabularysize) in both source and target languagesis considerably less for the translitera-tion taskWe take a statistical machine translation pa-radigm (Brown at al., 1991) as the basis for oursystems.
The work in this paper is related to thework of (Finch and Sumita, 2008) who also useSMT directly to transliterate.We view the task of machine transliterationas a process of machine translation at the cha-racter level (Donoual and LePage, 2006).
Weuse state of the art phrase-based statistical ma-chine translation systems (Koehn et al, 2003) toperform the transliteration.
By adopting this ap-proach we were able to build systems for all ofthe language pairs in the shared task using pre-cisely the same procedures.
No modeling of thephonetics of either source or target language(Knight and Graehl, 1997) was necessary, sincethe approach is simply a direct transformation ofsequences of tokens in the source language intosequences of tokens in the target.2 OverviewOur approach differs from the approach of(Finch and Sumita, 2008) in that we decode bi-directional.
In a typical statistical machine trans-lation system the sequence of target tokens isgenerated in a left-to-right manner, by left-to-right here we mean the target sequence is gener-ated from the first token to its last.
During thegeneration process the models (in particular thetarget language model) are able to refer to onlythe target tokens that have already been generat-ed.
In our approach, by using decoders that de-code in both directions we are able to exploitcontext to the left and to the right of target to-kens being generated.
Furthermore, we expectour system to gain because it is a combination oftwo different MT systems that are performingthe same task.3 Experimental ConditionsIn our experiments we used an in-house phrase-based statistical machine translation decodercalled CleopATRa.
This decoder operates onexactly the same principles as the publiclyavailable MOSES decoder (Koehn et al, 2003).Like MOSES we utilize a future cost in our cal-culations.
Our decoder was modified to be ableto run two instances of the decoder at the sameTransliteration by Bidirectional Statistical Machine TranslationAndrew FinchNICT2-2-2 HikaridaiKeihanna Science City619-0288 JAPANandrew.finch@nict.go.jpEiichiro SumitaNICT2-2-2 HikaridaiKeihanna Science City619-0288 JAPANeiichiro.sumita@nict.go.jp52time.
One instance decoding from left-to-rightthe other decoding from right-to-left.
The hypo-theses being combined by linearly interpolatingthe scores from both decoders at the end of thedecoding process.
In addition, the decoders wereconstrained decode in a monotone manner.
Thatis, they were not allowed to re-order the phrasesduring decoding.
The decoders were also confi-gured to produce a list of unique sequences oftokens in their n-best lists.
During SMT decod-ing it is possible to derive the same sequence oftokens in multiple ways.
Multiply occurring se-quences of this form were combined into a sin-gle hypothesis in the n-best list by summingtheir scores.3.1 Pre-processingIn order to reduce data sparseness issues wetook the decision to work with data in only itslowercase form.
The only target language withcase information was Russian.
During the para-meter tuning phase (where output translationsare compared against a set of references) werestored the case for Russian by simply capita-lizing the first character of each word.We chose not to perform any tokenization forany of the language pairs in the shared task.
Wechose this approach for several reasons:?
It allowed us to have a single unifiedapproach for all language pairs?
It was in the spirit of the evaluation, asit did not require specialist knowledgeoutside of the supplied corpora?
It enabled us to handle the Chinesenames that occurred in the JapaneseRomaji-Japanese Kanji taskHowever we believe that a more effectiveapproach for Japanese-Kanji task may have beento re-tokenize the alphabetic characters into ka-na (for example transforming ?k a?
into the kanaconsonant vowel pair ?ka?)
since these are thebasic building blocks of the Japanese language.3.2 TrainingFor the final submission, all systems weretrained on the union of the training data and de-velopment data.
It was felt that the training setwas sufficiently small that the inclusion of thedevelopment data into the training set wouldyield a reasonable boost in performance by in-creasing the coverage of the language model andphrase table.
The language models and transla-tion models were therefore built from all thedata, and the log-linear weights used to combinethe models of the systems were tuned using sys-tems trained only on the training data.
The de-velopment data in this case being held-out.
Itwas assumed that these parameters would per-form well in the systems trained on the com-bined development/training corpora.3.3 Parameter TuningThe SMT systems were tuned using the mini-mum error rate training procedure introduced in(Och, 2003).
For convenience, we used BLEUas a proxy for the various metrics used in theshared task evaluation.
The BLEU score isFigure 1: The decoding process for multi-word sequencesWord 1 Word 2 Word mSegment into individual words and decode each word independentlyDecodeDecodeDecoden-besthypothesis 1hypothesis 2...hypothesis nn-besthypothesis 1hypothesis 2...hypothesis nn-besthypothesis 1hypothesis 2...hypothesis nSearch for the best path53commonly used to evaluate the performance ofmachine translation systems and is a function ofthe geometric mean of n-gram precision.
Table 1shows the effect of tuning for BLEU on theACC (1-best accuracy) scores for several lan-guages.
Improvements in the BLEU score alsogave improvements in ACC.
Tuning to maxim-ize the BLEU score gave improvements for alllanguage pairs and in all of the evaluation me-trics used in this shared task.
Nonetheless, it isreasonable to assume that one would be able toimprove the performance in a particular evalua-tion metric by doing minimum error rate train-ing specifically for that metric.3.3.1 Multi-word sequencesThe data for some languages (for example Hin-di) contained some multi-word sequences.
Theseposed a challenge for our approach, and gave usthe following alternatives:?
Introduce a <space> token into the se-quence, and treat it as one long charac-ter sequence to transliterate; or?
Segment the word sequences into indi-vidual words and transliterate these in-dependently, combining the n-best hy-pothesis lists for all the individual wordsin the sequence into a single output se-quence.We adopted both approaches for the trainingof our systems.
For those multi-word sequenceswhere the number of words in the source andtarget matched, the latter approach was taken.For those where the numbers of source and tar-get words differed, the former approach wastaken.
The decoding process for multi-word se-quences is shown in Figure 1.
This approachwas only used during the parameter tuning onthe development set, and in experiments to eva-luate the system performance on developmentdata since no multi-word sequences occurred inthe test data.During recombination, the score for the targetword sequence was calculated as the product ofthe scores of each hypothesis for each word.Therefore a search over all combinations of hy-potheses was required.
In almost all cases wewere able to perform a full search.
For the rarelong word sequences in the data, a beam searchstrategy was adopted.3.3.2 Bidirectional DecodingIn SMT it is usual to decode generating the tar-get sequence in order from the first token to thelast token (we refer to this as left-to-right decod-ing, as this is the usual term for this, eventhough it may be confusing as some languagesare naturally written from right-to-left).
Sincethe decoding process is symmetrical, it is alsopossible to reverse the decoding process, gene-rating from the end of the target sequence to thestart (we will refer to this as right-to-left decod-ing).
This reverse decoding is counter-intuitivesince language is generated in a left-to-rightmanner by humans (by definition), however, inpilot experiments on language translation, wefound that the best decoding strategy varies de-pending on the languages involved.
The analo-gue of this observation was observed in ourtransliteration results (Table 1).
For some lan-guage pairs, a left-to-right decoding strategyperformed better, and for other language pairsthe right-to-left strategy was preferable.Our pilot experiments also showed that com-bining the hypotheses from both decodingprocesses almost always gave better results thatthe best of either left-to-right or right-to-left de-coding.
We observe a similar effect in the expe-riments presented here, although our results hereare less consistent.
This is possibly due to thedifferences in the size of the data sets used forthe experiments.
The data used in the experi-ments here being an order of magnitude smaller.4 ResultsThe results of our experiments are shown in Ta-ble 1.
These results are from a closed evaluationon development data.
Only the training datawere used to build the system?s models, the de-velopment data being used to tune the log-linearweights for the translation engines?
models andfor evaluation.
We show results for the case ofequal interpolation weights of the left-to-rightand right-to-left decoders.
For the final submis-En-Ch En-Ja En-Ko En-Ru Jn-JkAfter tuning 0.908 0.772 0.622 0.914 0.769Before tuning 0.871 0.635 0.543 0.832 0.737Table 1: The effect on 1-best accuracy by tuning with respect to BLEU score54sion these weights were tuned on the develop-ment data.
The bidirectional performance wasthe best strategy for all but En-Ja and En-Ka interms of ACC.
This varies for other metrics butin general the bidirectional system most oftengave the highest performance.5 ConclusionOur results show the performance of state of theart phrase-based machine translation techniqueson the task of transliteration.
We show that it isreasonable to use the BLEU score to tune thesystem, and that bidirectional decoding can im-prove performance.
In future work we wouldlike to consider more tightly coupling the de-coders, introducing monotonicity into thealignment process, and adding contextual fea-tures into the translation models.AcknowledgementsThe results presented in this paper draw on thefollowing data sets.
For Chinese-English, Li etal., 2004.
For Japanese-English, Korean-English, and Japanese(romaji)-Japanese(kanji),the reader is referred to the CJK website:http://www.cjk.org.
For Hindi-English, Tamil-English, Kannada-English and Russian-Englishthe data sets originated from the work of Kura-man and Kellner, 2007.ReferencesPeter Brown, S. Della Pietra, V. Della Pietra, and R.Mercer (1991).
The mathematics of statistical ma-chine translation: parameter estimation.
Computa-tional Linguistics, 19(2), 263-311.Etienne Denoual and  Yves Lepage.
2006.
The cha-racter as an appropriate unit of processing for non-Language DecodingStrategy ACCMeanF-score MRR MAP_ref MAP_10 MAP_sysEn-Ch?
0.908 0.972 0.908 0.266 0.266 0.908?
0.914 0.974 0.914 0.268 0.268 0.914?
0.915 0.974 0.915 0.268 0.268 0.915En-Hi?
0.788 0.969 0.788 0.231 0.231 0.788?
0.785 0.968 0.785 0.230 0.230 0.785?
0.790 0.970 0.790 0.231 0.231 0.790En-Ja?
0.773 0.950 0.793 0.251 0.251 0.776?
0.767 0.948 0.785 0.249 0.249 0.768?
0.769 0.949 0.789 0.250 0.250 0.771En-Ka?
0.682 0.954 0.684 0.202 0.202 0.683?
0.660 0.953 0.661 0.195 0.195 0.660?
0.674 0.955 0.675 0.199 0.199 0.674En-Ko?
0.622 0.850 0.623 0.183 0.183 0.622?
0.620 0.851 0.621 0.182 0.182 0.619?
0.627 0.853 0.628 0.184 0.184 0.626En-Ru?
0.915 0.982 0.915 0.268 0.268 0.915?
0.921 0.983 0.921 0.270 0.270 0.921?
0.922 0.983 0.922 0.270 0.270 0.922En-Ta?
0.731 0.963 0.732 0.216 0.216 0.731?
0.734 0.962 0.735 0.217 0.217 0.735?
0.748 0.965 0.749 0.221 0.221 0.749Jn-Jk?
0.769 0.869 0.797 0.301 0.301 0.766?
0.766 0.862 0.792 0.299 0.299 0.761?
0.772 0.867 0.799 0.300 0.300 0.767Table 2: Results showing the peformance of three decoding strategies with respect to the evaluationmetrics used for the shared task.
Here ?
?denotes left-to-right decoding, ?
denotes right-to-left de-coding and ?
denotes bidirectional decoding.Key to Language Acronyms: En = English, Ch = Chinese, Hi = Hindi, Ja = Japanese Katakana, Ka =Kannada, Ko = Korean, Ru = Russian, Ta = Tamil, Jn = Japanese Romaji, Jk = Japanese Kanji.55segmenting languages, Proceedings of the 12thAnnual Meeting of The Association of NLP, pp.731-734.Kevin Knight and Jonathan Graehl.
1997.
MachineTransliteration.
Proceedings of the Thirty-FifthAnnual Meeting of the Association for Computa-tional Linguistics and Eighth Conference of theEuropean Chapter of the Association for Compu-tational Linguistics, pp.
128-135, Somerset, NewJersey.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical Phrase-Based Translation.
InProceedings of the Human Language TechnologyConference 2003 (HLT-NAACL 2003), Edmonton,Canada.Franz Josef Och, ?Minimum error rate training forstatistical machine translation,?
Proceedings of theACL, 2003.Kumaran A., Kellner T., "A generic framework formachine transliteration", Proc.
of the 30th SIGIR,2007Haizhou Li, Min Zhang, Jian Su, English-Chinese(EnCh): "A joint source channel model for ma-chine transliteration", Proc.
of the 42nd ACL,2004.56
