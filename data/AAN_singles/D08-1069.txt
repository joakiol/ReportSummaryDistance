Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 660?669,Honolulu, October 2008. c?2008 Association for Computational LinguisticsSpecialized models and ranking for coreference resolutionPascal DenisALPAGE Project TeamINRIA RocquencourtF-78153 Le Chesnay, Francepascal.denis@inria.frJason BaldridgeDepartment of LinguisticsUniversity of Texas at AustinAustin, TX 78712-0198, USAjbaldrid@mail.utexas.eduAbstractThis paper investigates two strategies for im-proving coreference resolution: (1) trainingseparate models that specialize in particu-lar types of mentions (e.g., pronouns versusproper nouns) and (2) using a ranking lossfunction rather than a classification function.In addition to being conceptually simple, thesemodifications of the standard single-model,classification-based approach also deliver sig-nificant performance improvements.
Specifi-cally, we show that on the ACE corpus bothstrategies produce f -score gains of more than3% across the three coreference evaluationmetrics (MUC, B3, and CEAF).1 IntroductionCoreference resolution is the task of partitioning aset of entity mentions in a text, where each par-tition corresponds to some entity in an underlyingdiscourse model.
While early machine learning ap-proaches for the task relied on local, discriminativeclassifiers (Soon et al, 2001; Ng and Cardie, 2002b;Morton, 2000; Kehler et al, 2004), more recent ap-proaches use joint and/or global models (McCallumand Wellner, 2004; Ng, 2004; Daume?
III and Marcu,2005; Denis and Baldridge, 2007a).
This shift im-proves performance, but the systems are consider-ably more complex and often less efficient.
Here,we explore two simple modifications of the first typeof approach that yield performance gains which arecomparable, and sometimes better, to those obtainedwith these more complex systems.
These modifica-tions involve: (i) the use of rankers instead of clas-sifiers, and (ii) the use of linguistically motivated,specialized models for different types of mentions.Ranking models provide a theoretically more ad-equate and empirically better alternative approachto pronoun resolution than standard classification-based approaches (Denis and Baldridge, 2007b).In essence, ranking models directly capture duringtraining the competition among potential antecedentcandidates, instead of considering them indepen-dently.
This gives the ranker additional discrimina-tive power and in turn better antecedent selection ac-curacy.
Here, we show that ranking is also effectivefor the wider task of coreference resolution.Coreference resolution involves several differenttypes of anaphoric expressions: third-person pro-nouns, speech pronouns (i.e., first and second personpronouns), proper names, definite descriptions andother types of nominals (e.g., anaphoric uses of in-definite, quantified, and bare noun phrases).
Differ-ent anaphoric expressions exhibit different patternsof resolution and are sensitive to different factors(Ariel, 1988; van der Sandt, 1992; Gundel et al,1993), yet most machine learning approaches haveignored these differences and handle these differentphenomena with a single, monolithic model.
A fewexceptions are worth noting.
Morton (2000) and Ng(2005b) propose different classifiers models for dif-ferent NPs for coreference resolution and pronounresolution, respectively.
Other partially capture thedifferential preferences between different anaphorsvia different sample selection strategies during train-ing (Ng and Cardie, 2002b; Uryupina, 2004).
Morerecently, Haghighi and Klein (2007) use the distinc-tion between pronouns, nominals and proper nouns660in their unsupervised, generative model for corefer-ence resolution; for their model, this is absolutelycritical for achieving better accuracy.
Here, we showthat using specialized models for different typesof referential expressions improves performance forsupervised models (both classifiers and rankers).Both these strategies lead to improvements forall three standard coreference metrics: MUC (Vilainet al, 1995), B3 (Bagga and Baldwin, 1998), andCEAF (Luo, 2005).
In particular, our specializedranker system provides absolute f -score improve-ments against an otherwise identical standard clas-sifier system by 3.2%, 3.1%, and 3.6% for MUC, B3,and CEAF, respectively.2 RankingNumerous approaches to anaphora and coreferenceresolution reduce these tasks to a binary classifica-tion task, whereby pairs of mentions are classified ascoreferential or not (McCarthy and Lehnert, 1995;Soon et al, 2001; Ng and Cardie, 2002b).
Usuallyused in combination with a greedy right-to-left clus-tering, these approaches make very strong indepen-dence assumptions.
Not only do they model eachcoreference decision separately, they actually modeleach pair of mentions as a separate event.
Recast-ing these tasks as ranking tasks partly addresses thisproblem by directly making the comparison betweendifferent candidate antecedents for an anaphor partof the training criterion.
Each candidate is assigneda conditional probability with respect to the entirecandidate set.
(Re)rankers have been successfullyapplied to numerous NLP tasks, such as parse se-lection (Osborne and Baldridge, 2004; Toutanova etal., 2004), parse reranking (Collins and Duffy, 2002;Charniak and Johnson, 2005), question-answering(Ravichandran et al, 2003).The twin-candidate classification approach pro-posed by (Yang et al, 2003) shares some similaritieswith the ranker in making the comparison betweencandidate antecedents part of training.
An importantdifference however is that under the twin-candidateapproach, candidates are compared in pairwise fash-ion (and the best overall candidate is the one that haswon the most round robin contests), while the rankerconsiders the entire candidate set at once.
Anotheradvantage of the ranking approach is that its com-plexity is only square in the number of mentions,while that of the twin-candidate model is cubic (seeDenis and Baldridge (2007b) for a more detailedcomparison in the context of pronoun resolution).Our ranking models for coreference take the fol-lowing log-linear form:Prk(?i|pi) =expm?j=1wjfj(pi, ?i)?kexpm?j=1wjfj(pi, ?k)(1)where pi stands for the anaphoric expression, ?i foran antecedent candidate, fj the weighted features ofthe model.
The denominator consists of a normal-ization factor over the k candidate mentions.
Modelparameters were estimated with the limited memoryvariable metric algorithm and Gaussian smoothing(?2=1000), using TADM (Malouf, 2002).For the training of the different ranking models,we use the following procedure.
For each model, in-stances are created by pairing each anaphor of theproper type (e.g., definite description) with a set ofcandidates which contains: (i) a true antecedent, and(ii) a set of non-antecedents.
The selection of thetrue antecedent varies depending on the model weare training: for pronominal forms, the antecedentis selected as the closest preceding mention in thechain; for non-pronominal forms, we used the clos-est preceding non-pronominal mention in the chainas the antecedent.
For the creation of the non-antecedent set, we collect all the non-antecedentsthat appear in a window of two sentences around theantecedent.1 At test time, we consider all precedingmentions as potential antecedents.Not all referential expressions in a given docu-ment are anaphors: some expressions introduce adiscourse entity, rather than accessing an existingone.
Thus, coreference resolvers must have a way ofidentifying such ?discourse-new?
expressions.
Thisis easily handled in the standard classification ap-proach: a mention will not be resolved if none of itscandidates is classified positively (i.e., as coreferen-tial).
The problem is more troublesome for rankers,which always pick an antecedent from the candidate1We suspect that different varying windows might be moreappropriate for different types of expressions, but leaves this forfurther investigations.661set.
A natural solution is to use a model that specifi-cally predicts the discourse status (discourse-new vs.discourse-old) of each expression: only expressionsthat are classified as ?discourse-old?
by this modelare considered by rankers.Ng and Cardie (Ng and Cardie, 2002a) introducedthe use of an ?anaphoricity?
classifier to act as a fil-ter for coreference resolution in order to correct er-rors where antecedents are mistakenly identified fornon-anaphoric mentions or antecedents are not de-termined for mentions which are indeed anaphoric.Their approach produced significant improvementsin precision, but with consequent larger losses in re-call.
Ng (2004) improves recall by optimizing theanaphoricity threshold.
By using joint inference foranaphoricity and coreference, Denis and Baldridge(2007a) avoid cascade-induced errors without theneed to separately optimize the threshold.We use a similar discourse status classifier to Ngand Cardie?s as a filter on mentions for our rankers.We rely on three main types of information sources:(i) the form of mention (e.g., type of linguistic ex-pression, number of tokens), (ii) positional featuresin the text, (iii) comparisons of the given mention tothe mentions that precede it in the text.
Evaluated onthe ACE datasets, training the model on the traintexts, and applying the classifier to the devtesttexts, the model achieves an overall accuracy scoreof 80.8%, compared to a baseline of 59.7% whenpredicting the majority class (?discourse-old?
).3 Specialized modelsOur second strategy is to use different, specializedmodels for different referential expressions, simi-larly to Elwell and Baldridge?s (2008) use of connec-tive specific models for identifying the arguments ofdiscourse connectives.
For this, one must determinealong which dimension to split such expressions.For example, Ng (2005b) learns models for each setof anaphors that are lexically identical (e.g., I, he,they, etc.).
This option is possible for closed setslike pronouns, but not for other types of anaphorslike proper names and definite descriptions.
Anotheroption is to rely on the particular linguistic form ofthe different expressions, as signaled by the headword category and the determiner (if any).
Moreconcretely, we use separate models for the follow-ing types: (i) third person pronouns, (ii) speech pro-nouns, (iii) proper names, (iv) definite descriptions,and (v) others (i.e., all expressions that don?t fall intothe previous categories).The correlation between the form of a referen-tial expression and its anaphoric behavior is actuallycentral to various linguistic accounts (Prince, 1981;Ariel, 1988; Gundel et al, 1993).
Basically, the ideais that linguistic form is an indicator of the status ofthe corresponding referent in the discourse model.That is, the use by the speaker of a particular lin-guistic form corresponds to a particular level of acti-vation (or familiarity or salience or accessibility) in(what she thinks is) the addressee?s discourse model.For many authors, the relation takes the form of acontinuum and is often represented in the form of areferential hierarchy, such as:Accessibility Hierarchy (Ariel, 1988)Zero pronouns >> Pronouns >> Demonstra-tive pronouns >> Demonstrative NPs >>Short PNs >> Definite descriptions >> FullPNs >> Full PNs + appositiveThe higher up, the more accessible (or salient) theentity is.
At the extremes are pronouns (these formstypically require a previous mention in the local con-text) and proper names (these forms are often usedwithout previous mentions of the entity).
This typeof hierarchy is validated by corpus studies of thedistribution of different types of expressions.
Forinstance, pronouns find their antecedents very lo-cally (in a window of 1-2 sentences), while propernames predominantly find theirs at longer distances(Ariel, 1988).2 Using discourse structure, Asher etal.
(2006) show that while anaphoric pronouns sys-tematically obey the right-frontier constraint (i.e.,their antecedents have to appear on the right edgeof the discourse graph), this is less so for definites,and even less so for proper names.From a machine learning perspective, these find-ings suggest that features encoding some aspect ofsalience (e.g., distance, syntactic context) are likelyto receive different sets of parameters depending onthe form of the anaphor.
This therefore suggeststhat better parameters are likely to be learned in the2Haghighi and Klein?s (2007) generative coreference modelmirrors this in the posterior distribution which it assigns to men-tion types given their salience (see their Table 1).662Type/Count train test3rd pron.
4, 389 1, 093speech pron.
2, 178 610proper names 7, 868 1, 532def.
NPs 3, 124 796others 1, 763 568Total 19, 322 4, 599Table 1: Distribution of the different anaphors in ACEcontext of different models.3 While the above stud-ies focus primarily on salience, there are of courseother dimensions according to which anaphors differin their resolution preferences.
Thus, the resolutionof lexical expressions like definite descriptions andproper names is likely to benefit from the inclusionof features that compare the strings of the anaphorand the candidate antecedent (e.g., string matching)and features that identify particular syntactic config-urations like appositive structures.
This type of in-formation is however much less likely to help in theresolution of pronominal forms.
The problem is that,within a single model, such features are likely to re-ceive strong parameters (due to the fact that they aregood predictors for lexical anaphors) in a way thatmight eventually hurt pronominal resolutions.Note that our split of referential types onlypartially cover the referential hierarchies of Ariel(1988) or Gundel et al (1993).
Thus, there is no sep-arate model for demonstrative noun phrases and pro-nouns: these are very rare in the corpus we used (i.e.,the ACE corpus).4 These expressions were thereforehandled through the ?others?
model.
There is how-ever a model for first and second person pronouns(i.e., speech pronouns): this is justified by the factthat these pronouns behave differently from theirthird person counterparts.
These forms indeed of-ten behave like deictics (i.e., they refer to discourseparticipants) or they appear within a quote.The total number of anaphors (i.e., of mentionsthat are not chain heads) in the data is 19, 322 and4, 599 for training and testing, respectively.
The dis-tribution of each anaphoric type is presented in Ta-ble 1.
Roughly, third person pronouns account for3Another possible approach would consist in introducingdifferent salience-based features encoding the form of theanaphor.4There are only 114 demonstrative NPs and 12 demonstra-tive pronouns in the entire ACE training.Linguistic Formpn ?
is a proper name {1,0}def np ?
is a definite description {1,0}indef np ?
is an indefinite description {1,0}pro ?
is a pronoun {1,0}Contextleft pos POS of the token preceding ?right pos POS of the token following ?surr pos pair of POS for the tokens surrounding ?Distances dist Binned values for sentence distance between pi and ?np dist Binned values for mention distance between pi and ?Morphosyntactic Agreementgender pairs of attributes {masc, fem, neut, unk} for pi and ?number pairs of attributes {sg, pl} for pi and ?person pairs of attributes {1, 2, 3, 4, 5, 6} for pi and ?Semantic compatibilitywn sense pairs of Wordnet senses for pi and ?String similaritystr match pi and ?
have identical strings {1,0}left substr one mention is a left substring of the other {1,0}right substr one mention is a right substring of the other {1,0}hd match pi and ?
have the same head word {1,0}Appositionapposition pi and ?
are in an appositive structure {1,0}Acronymacronym pi is an acronym of ?
or vice versa {1,0}Table 2: Features used by coreference models.22-24% of all anaphors in the entire corpus, speechpronouns for 11-13%, proper names for 33-40%,and definite descriptions for 16-17%.
The distribu-tion is slightly different from one dataset to another,probably reflecting genre differences.
For instance,BNEWS shows a larger proportion of pronouns ingeneral (pronominal forms account for 40-44% ofall the anaphoric forms).We use five broad types of features for all mentiontypes, plus three others used by specific types, sum-marized in Table 3.
Our feature extraction relies onlimited linguistic processing: we only made use of asentence detector, a tokenizer, a POS tagger (as pro-vided by the OpenNLP Toolkit5) and the WordNet6database.
Since we did not use parser, lexical headsfor the NP mentions were computed using simpleheuristics relying solely on POS sequences.
Table 2describes in detail the entire feature set, and Table 3shows which features were used for which models.Linguistic form: the referential form of the an-tecedent candidate: a proper name, a definite de-5http://opennlp.sf.net.6http://wordnet.princeton.edu/663Features/Types 3P SP PN Def-NP OthLing.
form?
?
?
?
?Context?
?
?
?
?Distance?
?
?
?
?Agreement.?
?
?
?
?Sem.
compat.?
?
?
?
?Str.
sim.?
?
?Apposition?
?Acronym?Table 3: Features for each type of referential expression.scription, an indefinite NP, or a pronoun.Context: the context of the antecedent candidate:these features can be seen as approximations of thegrammatical roles, as indicators of the salience ofthe potential candidate (Grosz et al, 1995).
Forinstance, this includes the part of speech tags sur-rounding the candidate, as well as a feature thatindicates whether the potential antecedent is thefirst mention in a sentence (approximating subject-hood), and a feature indicating whether the candi-date is embedded inside another mention.Distance: the distance between the anaphor andthe candidate, measured by the number of sentencesand mentions between them.Morphosyntactic agreement: indicators of thegender, number, and person of the two mentions.These are determined for non-pronominal NPs withheuristics based on POS tags (e.g., NN vs. NNS fornumber) and actual mention strings (e.g., whetherthe mention contains a male/female first name orhonorific for gender).
These features consist of pairsof attributes, ensuring that not only strict agreement(e.g., singular-singular) but also mere compatibility(e.g., masculine-unknown) is captured.Semantic compatibility: features designed to as-sess whether the two mentions are semanticallycompatible.
For these features, we use the Word-Net database: in particular, we collected the syn-onym set (or synset) as well as the synset of theirdirect hypernyms associated with each mention.
Inthe case of common nouns, we used the synset asso-ciated with the first sense associated with the men-tion?s head word.
In the case of proper names, weused the synset associated with the name if avail-able, and the string itself otherwise.
For pronouns(which are not part of Wordnet), we simply used thepronominal form.All these features were used in all five models.While one may question the use of distance for non-pronominal anaphors,7 their inclusion can be justi-fied in that they might predict some ?obviation?
ef-fects.
Definite descriptions and proper names aresensitive to distance too, although not in the sameway as pronouns are: they show a preference for an-tecedents that appear outside a window of one or twosentences (Ariel, 1988).Several features are used only for particular men-tion types:String similarity: similarity of the anaphor andthe candidate strings.
Examples are perfect stringmatch, substring matches, and head match (i.e., thetwo mentions share the same head word).Appositive: whether the anaphor is an appositiveof the antecedent candidate.
Since we do not haveaccess to syntactic structure, we use heuristics (e.g.,the presence of a comma between the two mentions)to extract this feature.Acronym: whether the anaphor string is anacronym of the candidate string (or vice versa): e.g.,NSF and National Science Foundation.4 Coreference systemsWe evaluate several systems to explore the effect ofranking versus classification and specialized versusmonolithic models.
The different systems follow ageneric architecture.
Let M be the set of mentionspresent in a document.
For all models, each mentionm ?
M is associated at test time with a set of an-tecedent candidates Cm, which includes all the men-tions that linearly precede m. The best candidate isdetermined by the model in use.
The final output ofeach system consists in a list of mention pairs (i.e.,the coreference links) which in turn defines (throughreflexive, transitive closure) a partition over the setM.
Our models are summarized in Table 4.The use of the discourse status filter is straightfor-ward.
For each mention m?M, the discourse status7In fact, Morton (2000) does not use distance in this case.664Model Disc.Model Name Type Specialized?
StatusCLASS class No NoCLASS+DS class No YesCLASS+SP class Yes NoCLASS+DS+SP class Yes YesRANK+DS+SP rank Yes YesTable 4: Model names and their properties.model is first applied to determine whether m intro-duces a new discourse entity (i.e., it is classified as?new?)
or refers back to an existing entity (i.e., itis classified as ?old?).
If m is classified as ?new?,the process terminates and goes to the next mention.If m is classified as ?old?, m along with its set ofantecedent candidates Cm is sent to the model.For classifiers, we replicate the procedures of Ngand Cardie (2002b).
During training, instances areformed by pairing each anaphor with each of its pre-ceding candidates, until the antecedent is reached:the closest preceding antecedent in the case of apronominal anaphor, or the closest non-pronominalantecedent for other anaphor types.
For classifiers,the use of a discourse status filter at test time is op-tional.
When a filter is not used, then a mentionis left unresolved if none of the pairs created for agiven mention is classified positively.
If several pairsfor a given mention are classified positively, then thepair with the highest score is selected (i.e., ?Best-First?
link selection).
If a filter is used, then the can-didate with the highest score is selected, even if theprobability of coreference is less than one-half.8The use of specialized models is simple, for bothclassifiers and rankers.
Specialized models are cre-ated for: (i) third person pronouns, (ii) speech pro-nouns, (iii) proper names, (iv) definite descriptions,(v) other types of phrases.
The mention type is de-8This is very similar to the approach of Ng and Cardie(2002a).
An important difference is that their system does notnecessarily yield an antecedent for each of the anaphors pro-posed by the discourse status model.
In their system, if thecoreference classifier finds that none of the candidates for a?new?
mention are coreferential, it leaves it unresolved.
In thiscase, the coreference model acts as an additional filter.
Not sur-prisingly, these authors report gains in precision but compar-atively larger losses in recall.
Our development experimentsrevealed that forcing a decision on items identified as new pro-vided performed better across all metrics.System Accuracy3rd pron.
82.2speech pron.
66.9proper names 83.5def.
NPs 66.5others 63.6Table 5: Accuracy of the different ranker models.termined and the best candidate is chosen by theappropriate model Following Elwell and Baldridge(2008), these models could be interpolated with amonolithic model, or even word specific models, butwe have not explored that option here.The feature sets for the classifiers in the base-line systems includes all the features that were usedfor the described in Section 3.
For the classi-fiers that do not use specialized models (CLASS andCLASS+DS), we have also added extra features de-scribing the linguistic form of the potential anaphor(whether it is a pronoun, a proper name, and soon).
This is in accordance with standard feature setsin the pairwise approach.
It gives these models achance to learn weights more appropriately for thedifferent types within a single, monolithic model.5 ExperimentsWe use the ACE corpus (Phase 2).
The corpus hasthree parts, each corresponding to a different genre:newspaper texts (NPAPER), newswire texts (NWIRE),and broadcast news (BNEWS).
Each set is split intoa train part and a devtest part.
In our experi-ments, we consider only true ACE mentions.5.1 Antecedent selection resultsWe first evaluate the specialized ranker modelsindividually on the task of anaphora resolution:their ability to select a correct antecedent for eachanaphor.
Following common practice in this task,we report results in terms of accuracy, which is sim-ply the ratio of correctly resolved anaphors.
Thecandidate set during testing was formed by takingall the mentions that appear before the anaphor.Also, we assume that correctly resolving an anaphoramounts to selecting any of the previous mentions inthe entity as the antecedent.
The accuracy scores forthe different models are presented in Table 5.665The best accuracy results on the entire ACE cor-pus are found first for the proper name resolver witha score of 83.5%, then for the third person pronounresolver with 82.2%, then for the definite descrip-tion and speech pronoun resolvers with 66.9% and66.5% respectively.
The worst scores are obtainedfor the ?others?
category.
The high scores for thethird person pronoun and the proper name rankersmost likely follow from the fact that the resolutionof these expressions relies on simple, reliable pre-dictors, such as distance and morphosyntactic agree-ment for pronouns, and string similarity features forproper names.
The resolution of definite descrip-tions and other types of lexical NPs (which are han-dled through the ?others?
model) are much morechallenging: they rely on lexical semantic and worldknowledge, which is only partially encoded via ourWordNet-based features.
Finally, note that the reso-lution of speech pronouns is also much harder thanthat of the other pronominal forms: these expres-sions are much less (if at all) constrained by re-cency and agreement.
Furthermore, these expres-sions show a lot of cataphoric uses, which are notconsidered by our models.
The low scores for the?others?
category is likely due to the fact that it en-compasses very different referential expressions.5.2 Coreference ResultsFor evaluating the coreference performance, we relyon three primary metrics: (i) the link based MUCmetric (Vilain et al, 1995), the mention based B3metric (Bagga and Baldwin, 1998), and the entitybased CEAF metric (Luo, 2005).
Common to thesemetrics is: (i) they operate by comparing the set ofchains S produced by the system against the truechains T , and (ii) they report performance in termsof recall and precision.
There are however impor-tant differences in how each metric computes thesescores, each producing a different bias.MUC scores are based on the number of links(pairs of mentions) common to S and T .
Recallis the number of common links divided by the to-tal number of links in T ; precision is the number ofcommon links divided by the total number of linksin S. This focus gives MUC two main biases.
First,it favors systems that create large chains (and thusfewer entities).
For instance, a system that producesa single chain achieves 100% recall without severedegradation in precision.
Second, it ignores singlemention entities, which are involved in no links.9The B3 metric was designed to address the MUCmetric?s shortcomings.
It is mention-based: it com-putes both recall and precision scores for each men-tion i.
Let S be the system chain containing m, Tbe the true chain containing m. The set of correctelements in S is thus |S ?
T |.
The recall score fora mention i is |S?T ||T | , while the precision score for iis |S?T ||S| .
Overall recall/precision is obtained by av-eraging over the individual mention scores.
The factthat this metric is mention-based by definition solvesthe problem of single mention entities.
Also solvedis the bias favoring larger chains, since this will bepenalized in the precision score of each mention.The Constrained Entity Aligned F-Measure(CEAF) (Luo, 2005).
aligns each system chain Swith at most one true chain T .
It finds the best one-to-one mapping between the set of chains S and T ,which is equivalent to finding the optimal alignmentin a bipartite graph.
The best mapping maximizesthe similarity over pairs of chains (Si, Ti), wherethe similarity between two chains is the number ofcommon mentions to the two chains.
With CEAF,recall is computed as the total similarity divided bythe number of mentions in all the T (i.e., the self-similarity), while precision is the total similarity di-vided by the number of mentions in S.Table 6 gives scores for all three metricsfor the different models on the entire ACEcorpus.
Two main patterns emerge: sig-nificant improvements are obtained by usingspecialized models (CLASS vs CLASS+SP andCLASS+DS vs CLASS+DS+SP) and by using aranker (CLASS+DS+SP vs RANK+DS+SP).
Overall,the RANK+DS+SP system significantly outperformsthe other systems on the three different metrics.10The f -scores for RANK+DS+SP are 71.6% withthe MUC metric, 72.7% with the B3, and 67.0%with the CEAF metric.
These scores place theRANK+DS+SP among the best coreference resolu-tion systems, since most existing systems are typi-cally under the bar of the 70% in f -score with the9It is worth noting that the MUC corpus does not annotatesingle mention entities.10Statistical significance was determined with t-tests for bothrecall and precision scores, with p < 0.05.666System MUC B3 CEAFR P F R P F FCLASS 60.8 72.6 66.2 62.4 77.7 69.2 62.3CLASS+DS 64.9 72.3 68.4 65.6 74.1 69.6 63.4CLASS+SP 64.8 74.5 69.3 65.3 79.1 71.5 65.0CLASS+DS+SP 66.8 74.4 70.4 66.4 77.0 71.3 65.3RANK+DS+SP 67.9 75.7 71.6 66.8 79.8 72.7 67.0Table 6: Recall (R), Precision (P), and f -score (F) results on the entire ACE corpus using the MUC, B3, and CEAFmetrics.
Note that R=P=F for CEAF when using true mentions, as we do here.MUC and B3 metrics (Ng, 2005a).
An interestingpoint of comparison is provided by Ng (2007), whoalso relies on true mentions and reports MUC f -scores only slightly superior to ours (73.8%) whilerelying on perfect semantic class information.
Hisbest results otherwise are 64.6%.
The fact thatour improvements are consistent across the differentevaluation metrics is remarkable, especially giventhat these three metrics are quite different in theway they compute their scores.
The gains in f -score range from 1.2 to 5.4% on the MUC metric(i.e., error reductions of 4 to 15.9%), from 1.4 to3.5% on the B3 metric (i.e., error reductions of 4.8to 11.4%), and from 1.7 to 4.7% on the CEAF met-ric (i.e., error reductions of 6.9 to 17%).
The largerimprovements come from recall, with improvementsranging from 1.9 to 7.1% with MUC, from 2.4 to5.6% with B3.11 This suggests that RANK+DS+SPpredicts many more valid coreference links than theother systems.
Smaller but still significant gains aremade in precision: RANK+DS+SP is also able to re-duce the proportion of invalid links.The overall improvements found withRANK+DS+SP suggest that it is able to capi-talize on the better antecedent selection capabilitiesoffered by the ranking approach.
This is supportedby the error analysis on the development data.Errors made by a coreference system can be con-ceptualized as falling into three main classes: (i)?missed anaphors?
(i.e., an anaphoric mention thatfails to be linked to a previous mention), (ii) ?spuri-ous anaphors?
(i.e., an non-anaphoric mention thatis linked to a previous mention), and (iii) ?invalidresolutions?
(i.e., a true anaphor that is linked to a11Recall that recall and precision scores are identical withCEAF, due to the fact that we are using true mention boundaries.incorrect antecedent).
The two first types of errorpertain to the determination of the discourse statusof the mention, while the third regards the selectionof an antecedent (i.e., anaphora resolution).
Con-sidering the systems?
invalid resolutions, we foundthat the RANK+DS+SP had a much lower error rate:only 17.9% of all true anaphors were incorrectlyresolved by this system, against 23.1% for CLASS,24.9% for CLASS+DS, 20.4% for CLASS+SP, and22.1% for CLASS+DS+SP.Denis (2007) provides multi-metric scores for theJOINT-ILP model of Denis and Baldridge (2007a),which uses integer linear programming for joint in-ference over coreference resolution and discoursestatus: f -scores of 73.3%, 68.0%, and 58.9% forMUC, B3, and CEAF, respectively.
Despite the factthat this MUC score beats RANK+DS+SP?s, it is ac-tually worse than even the basic model CLASS forB3 and CEAF.
This difference fact that MUC givesmore recall credit for large chains without a conse-quent precision reduction, and shows the importanceof using B3 and CEAF scores in addition to MUC.Denis (2007) also extends the JOINT-ILP systemby adding named entity resolution and constraintson transitivity with respect to coreference links.
Thebest model reported there (JOINT-DS-NE-AE-ILP)obtains f -scores of 70.1%, 72.7%, and 66.2% forMUC, B3, and CEAF, respectively.
Interestingly,RANK+DS+SP actually performs better across allmetrics despite being a simpler model with fewersources of information.5.3 Oracle resultsUsing specialized rankers with a discourse statusclassifier yields coreference performance superior tothat given by various classification-based baselinesystems.
Crucially, these improvements have been667System MUC B3 CEAFR P F R P F FRANK+DS+SP 67.9 75.7 71.6 66.8 79.8 72.7 67.0RANK+DS-ORACLE+SP 79.1 79.1 79.1 75.4 76.0 75.7 76.9LINK-ORACLE 78.8 100.0 88.1 74.3 100.0 85.2 79.7Table 7: Recall (R), Precision (P), and f -score (F) results for RANK+DS-ORACLE+SP and LINK-ORACLE on theentire ACE corpus.possible using a discourse status model that has anaccuracy of just 80.8%.
Clearly, the performanceof the discourse status module has a direct impacton the performance of the entire coreference sys-tem.
On the one hand, misclassified anaphors aresimply not resolved by the rankers: this limits therecall of the coreference system.
On the other hand,misclassified non-anaphors are linked to a previousmention: this limits precision.In order to further assess the impact of the er-rors made by the discourse status classifier, we buildtwo different oracle systems.
The first oracle sys-tem, RANK+DS-ORACLE+SP, uses the specializedrankers in combination with a perfect discourse sta-tus classifier.
That is, this system knows for eachmention whether it is anaphoric or not: the only er-rors made by such a system are invalid resolutions.RANK+DS-ORACLE+SP thus provides an upper-bound for the RANK+DS+SP model.
The results forthis oracle are given in Table 7: they show substan-tial improvements over RANK+DS+SP, which sug-gests that the RANK+DS+SP has also the potentialto be further improved if used in combination with amore accurate discourse status classifier.The second oracle system, LINK-ORACLE, usesthe discourse status classifier with a perfect corefer-ence resolver.
That is, this system has perfect knowl-edge regarding the antecedents of anaphors: the er-rors made by such a system are only errors in thediscourse status of mentions.
The results for LINK-ORACLE are also reported in Table 7.
These figuresshow that however accurate our models are at pick-ing a correct antecedent for a true anaphor, the bestthey can achieve in terms of f -scores is 88.1% withMUC, 85.2% with B3, and 79.7% with CEAF.6 ConclusionWe present and evaluate two straight-forward tac-tics for improving coreference resolution: (i) rank-ing models, and (ii) separate, specialized modelsfor different types of referring expressions.
Thespecialized rankers are used in combination witha discourse status classifier which determines thementions that are sent to the rankers.
This simplepipeline architecture produces significant improve-ments over various implementations of the standard,classifier-based coreference system.
In turn, thesestrategies could be integrated with the joint infer-ence models we have explored elsewhere (Denis andBaldridge, 2007a; Denis, 2007) and which have ob-tained performance improvements that are orthogo-nal to those obtained here.This paper?s improvements are consistent acrossthe three main coreference evaluation metrics: MUC,B3, and CEAF.12 We attribute improvements to: (i)the better antecedent selection capabilities offeredby the ranking approach, and (ii) the division of la-bor between specialized models, allowing each oneto better model the corresponding distribution.AcknowledgmentsWe would like to thank Nicholas Asher, AndyKehler, Ray Mooney, and the three anonymous re-viewers for their comments.
This work was sup-ported by NSF grant IIS-0535154.ReferencesM.
Ariel.
1988.
Referring and accessibility.
Journal ofLinguistics, pages 65?87.N.
Asher, P. Denis, and B. Reese.
2006.
Names and popsand discourse structure.
In Workshop on Constraintsin Discourse, Maynooth, Ireland.A.
Bagga and B. Baldwin.
1998.
Algorithms for scor-ing coreference chains.
In Proceedings of LREC 1998,pages 563?566.12We strongly advocate that coreference results should neverbe presented in terms of MUC scores alone.668E.
Charniak and M. Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative reranking.
InProceedings of ACL 2005, Ann Arbor, Michigan.M.
Collins and N. Duffy.
2002.
New ranking algo-rithms for parsing and tagging: Kernels over discretestructures and the voted perceptron.
In Proceedings ofACL 2002, pages 263?270, Philadelphia, PA.H.
Daume?
III and D. Marcu.
2005.
A large-scale ex-ploration of effective global features for a joint entitydetection and tracking model.
In Proceedings of HLT-EMNLP 2005, Vancouver, Canada.P.
Denis and J. Baldridge.
2007a.
Joint determination ofanaphoricity and coreference resolution using integerprogramming.
In Proceedings of HLT-NAACL 2007,Rochester, NY.P.
Denis and J. Baldridge.
2007b.
A ranking approachto pronoun resolution.
In Proceedings of IJCAI 2007,Hyderabad, India.Pascal Denis.
2007.
New Learning Models for RobustReference Resolution.
Ph.D. thesis, The University ofTexas at Austin.R.
Elwell and J. Baldridge.
2008.
Discourse connec-tive argument identification with connective specificrankers.
In Proceedings of the International Confer-ence on Semantic Computing, Santa Clara, CA.B.
Grosz, A. Joshi, and S. Weinstein.
1995.
Centering:A framework for modelling the local coherence of dis-course.
Computational Linguistics, 2(21).J.
K. Gundel, N. Hedberg, and R. Zacharski.
1993.
Cog-nitive status and the form of referring expressions indiscourse.
Language, 69:274?307.A.
Haghighi and D. Klein.
2007.
Unsupervised coref-erence resolution in a nonparametric Bayesian model.In Proceedings ACL 2007, pages 848?855, Prague,Czech Republic.A.
Kehler, D. Appelt, L. Taylor, and A. Simma.
2004.The (non)utility of predicate-argument frequenciesfor pronoun interpretation.
In Proceedings of HLT-NAACL 2004.X.
Luo.
2005.
On coreference resolution performancemetrics.
In Proceedings of HLT-NAACL 2005, pages25?32.R.
Malouf.
2002.
A comparison of algorithms for max-imum entropy parameter estimation.
In Proceedingsof the Sixth Workshop on Natural Language Learning,pages 49?55, Taipei, Taiwan.A.
McCallum and B. Wellner.
2004.
Conditional modelsof identity uncertainty with application to noun coref-erence.
In Proceedings of NIPS 2004.J.
F. McCarthy and W. G. Lehnert.
1995.
Using deci-sion trees for coreference resolution.
In IJCAI, pages1050?1055.T.
Morton.
2000.
Coreference for NLP applications.
InProceedings of ACL 2000, Hong Kong.V.
Ng and C. Cardie.
2002a.
Identifying anaphoric andnon-anaphoric noun phrases to improve coreferenceresolution.
In Proceedings of COLING 2002.V.
Ng and C. Cardie.
2002b.
Improving machine learn-ing approaches to coreference resolution.
In Proceed-ings of ACL 2002, pages 104?111.V.
Ng.
2004.
Learning noun phrase anaphoricity to im-prove coreference resolution: Issues in representationand optimization.
In Proceedings of ACL 2004.V.
Ng.
2005a.
Machine learning for coreference reso-lution: From local classification to global ranking.
InProceedings of ACL 2005, pages 157?164, Ann Arbor,MI.V.
Ng.
2005b.
Supervised ranking for pronoun resolu-tion: Some recent improvements.
In Proceedings ofAAAI 2005.V.
Ng.
2007.
Semantic class induction and coreferenceresolution.
In Proceedings of ACL 2007.M.
Osborne and J. Baldridge.
2004.
Ensemble-basedactive learning for parse selection.
In Proceedings ofHLT-NAACL 2004, pages 89?96, Boston, MA.E.
F. Prince.
1981.
Toward a taxonomy of given-newinformation.
In P. Cole, editor, Radical Pragmatics,pages 223?255.
Academic Press, New York.D.
Ravichandran, E. Hovy, and F. J. Och.
2003.
Sta-tistical QA - classifier vs re-ranker: What?s the differ-ence?
In Proceedings of the ACL Workshop on Mul-tilingual Summarization and Question Answering?Machine Learning and Beyond.W.
M. Soon, H. T. Ng, and D. Lim.
2001.
A machinelearning approach to coreference resolution of nounphrases.
Computational Linguistics, 27(4):521?544.K.
Toutanova, P. Markova, and C. Manning.
2004.
Theleaf projection path view of parse trees: Exploringstring kernels for HPSG parse selection.
In Proceed-ings of EMNLP 2004, pages 166?173, Barcelona.O.
Uryupina.
2004.
Linguistically motivated sample se-lection for coreference resolution.
In Proceedings ofDAARC 2004, Furnas.R.
van der Sandt.
1992.
Presupposition projection asanaphora resolution.
Journal of Semantics, 9:333?377.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A model-theoretic corefer-ence scoring scheme.
In Proceedings fo the 6th Mes-sage Understanding Conference (MUC-6), pages 45?52, San Mateo, CA.
Morgan Kaufmann.X.
Yang, G. Zhou, J. Su, and C.L.
Tan.
2003.
Corefer-ence resolution using competitive learning approach.In Proceedings of ACL 2003, pages 176?183.669
