Spotting the ?Odd-one-out?
: Data-Driven Error Detection and Correctionin Textual DatabasesCaroline Sporleder, Marieke van Erp, Tijn Porcelijn and Antal van den BoschILK / Language and Information ScienceTilburg University, P.O.
Box 90153,5000 LE Tilburg, The Netherlands{C.Sporleder,M.G.J.vanErp,M.Porcelijn,Antal.vdnBosch}@uvt.nlAbstractWe present two methods for semi-automatic detection and correction of er-rors in textual databases.
The first method(horizontal correction) aims at correct-ing inconsistent values within a databaserecord, while the second (vertical correc-tion) focuses on values which were en-tered in the wrong column.
Both methodsare data-driven and language-independent.We utilise supervised machine learning,but the training data is obtained automat-ically from the database; no manual anno-tation is required.
Our experiments showthat a significant proportion of errors canbe detected by the two methods.
Further-more, both methods were found to lead toa precision that is high enough to makesemi-automatic error correction feasible.1 IntroductionOver the last decades, more and more informationhas become available in digital form; a major partof this information is textual.
While some tex-tual information is stored in raw or typeset form(i.e., as more or less flat text), a lot is semi-structured in databases.
A popular example ofa textual database is Amazon?s book database,1which contains fields for ?author?, ?title?, ?pub-lisher?, ?summary?
etc.
Information about collec-tions in the cultural heritage domain is also fre-quently stored in (semi-)textual databases.
Exam-ples of publicly accessible databases of this typeare the University of St. Andrews?s photographic1http://www.amazon.comcollection2 or the Nederlands Soortenregister.3Such databases are an important resource forresearchers in the field, especially if the contentscan be systematically searched and queried.
How-ever, information retrieval from databases can beadversely affected by errors and inconsistencies inthe data.
For example, a zoologist interested infinding out about the different biotopes (i.e., habi-tats) in which a given species was found, mightquery a zoological specimens database for the con-tent of the BIOTOPE column for all specimensof that species.
Whenever information about thebiotope was entered in the wrong column, that par-ticular record will not be retrieved by such a query.Similarly, if an entry erroneously lists the wrongspecies, it will also not be retrieved.Usually it is impossible to avoid errors com-pletely, even in well maintained databases.
Errorscan arise for a variety of reasons, ranging fromtechnical limitations (e.g., copy-and-paste errors)to different interpretations of what type of infor-mation should be entered into different databasefields.
The latter situation is especially preva-lent if the database is maintained by several peo-ple.
Manual identification and correction of er-rors is frequently infeasible due to the size of thedatabase.
A more realistic approach would be touse automatic means to identify potential errors;these could then be flagged and presented to a hu-man expert, and subsequently corrected manuallyor semi-automatically.
Error detection and correc-tion can be performed as a pre-processing step forinformation extraction from databases, or it can beinterleaved with it.In this paper, we explore whether it is possi-2http://special.st-andrews.ac.uk/saspecial/3http://www.nederlandsesoorten.nl40ble to detect and correct potential errors in tex-tual databases by applying data-driven clean-upmethods which are able to work in the absenceof background knowledge (e.g., knowledge aboutthe domain or the structure of the database) andinstead rely on the data itself to discover inconsis-tencies and errors.
Ideally, error detection shouldalso be language independent, i.e., require no orfew language specific tools, such as part-of-speechtaggers or chunkers.
Aiming for language in-dependence is motivated by the observation thatmany databases, especially in the cultural heritagedomain, are multi-lingual and contain strings oftext in various languages.
If textual data-cleaningmethods are to be useful for such databases, theyshould ideally be able to process all text strings,not only those in the majority language.While there has been a significant amount ofprevious research on identifying and correcting er-rors in data sets, most methods are not particularlysuitable for textual databases (see Section 2).
Wepresent two methods which are.
Both methods aredata-driven and knowledge-lean; errors are iden-tified through comparisons with other databasefields.
We utilise supervised machine learning,but the training data is derived directly from thedatabase, i.e., no manual annotation of data is nec-essary.
In the first method, the database fields ofindividual entries are compared, and improbablecombinations are flagged as potential errors.
Be-cause the focus is on individual entries, i.e., rowsin the database, we call this horizontal error cor-rection.
The second method aims at a differenttype of error, namely values which were enteredin the wrong column of the database.
Potentialerrors of this type are determined by comparingthe content of a database cell to (the cells of) alldatabase columns and determining which columnit fits best.
Because the focus is on columns, werefer to this method as vertical error correction.2 Related WorkThere is a considerable body of previous workon the generic issue of data cleaning.
Muchof the research directed specifically at databasesfocuses on identifying identical records whentwo databases are merged (Herna?ndez and Stolfo,1998; Galhardas et al, 1999).
This is a non-trivialproblem as records of the same objects comingfrom different sources typically differ in their pri-mary keys.
There may also be subtle differencesin other database fields.
For example, names maybe entered in different formats (e.g., John Smithvs.
Smith, J.)
or there may be typos which make itdifficult to match fields (e.g., John Smith vs. JonSmith).4In a wider context, a lot of research hasbeen dedicated to the identification of outliers indatasets.
Various strategies have been proposed.The earliest work uses probability distributions tomodel the data; all instances which deviate toomuch from the distributions are flagged as out-liers (Hawkins, 1980).
This approach is calleddistribution-based.
In clustering-based methods,a clustering algorithm is applied to the data andinstances which cannot be grouped under any clus-ter, or clusters which only contain very few in-stances are assumed to be outliers (e.g., Jiang etal.
(2001)).
Depth-based methods (e.g., Ruts andRousseeuw (1996)) use some definition of depthto organise instances in layers in the data space;outliers are assumed to occupy shallow layers.Distance-based methods (Knorr and Ng, 1998)utilise a k-nearest neighbour approach where out-liers are defined, for example, as those instanceswhose distance to their nearest neighbour exceedsa certain threshold.
Finally, Marcus and Maletic(2000) propose a method which learns associationrules for the data; records that do not conform toany rules are then assumed to be potential outliers.In principle, techniques developed to detect out-liers can be applied to databases as well, for in-stance to identify cell values that are exceptional inthe context of other values in a given column, or toidentify database entries that seem unlikely com-pared to other entries.
However, most methodsare not particularly suited for textual databases.Some approaches only work with numeric data(e.g., distribution-based methods), others can dealwith categorical data (e.g., distance-based meth-ods) but treat all database fields as atoms.
Fordatabases with free text fields it can be fruitful tolook at individual tokens within a text string.
Forinstance, units of measurement (m, ft, etc.)
may bevery common in one column (such as ALTITUDE)but may indicate an error when they occur in an-other column (such as COLLECTOR).4The problem of whether two proper noun phrases re-fer to the same entity has also received attention outside thedatabase community (Bagga, 1998).413 DataWe tested our error correction methods on adatabase containing information about animalspecimens collected by researchers at Naturalis,the Dutch Natural History Museum.5 Thedatabase contains 16,870 entries and 35 columns.Each entry provides information about one or sev-eral specimens, for example, who collected it,where and when it was found, its position in thezoological taxonomy, the publication which firstdescribed and classified the specimen, and so on.Some columns contain fairly free text (e.g., SPE-CIAL REMARKS), others contain textual content6of a specific type and in a relatively fixed format,such as proper names (e.g., COLLECTOR or LO-CATION), bibliographical information (PUBLICA-TION), dates (e.g., COLLECTION DATE) or num-bers (e.g., REGISTRATION NUMBER).Some database cells are left unfilled; just un-der 40% of all cells are filled (i.e., 229,430 cells).There is a relatively large variance in the numberof different values in each column, ranging fromthree for CLASS (i.e., Reptilia, Amphibia, and aremark pointing to a taxonomic inconsistency inthe entry) to over 2,000 for SPECIAL REMARKS,which is only filled for a minority of the entries.On the other hand there is also some repetitionof cell contents, even for the free text columns,which often contain formulaic expressions.
Forexample, the strings no further data available or(found) dead on road occur repeatedly in the spe-cial remarks field.
A certain amount of repetitionis characteristic for many textual databases, andwe exploit this in our error correction methods.While most of the entries are in Dutch or En-glish, the database also contains text strings in sev-eral other languages, such as Portuguese or French(and Latin for the taxonomic names).
In principle,there is no limit to which languages can occur inthe database.
For example, the PUBLICATION col-umn often contains text strings (e.g., the title ofthe publication) in languages other than Dutch orEnglish.4 Horizontal Error CorrectionThe different fields in a database are often notstatistically independent; i.e., for a given entry,5http://www.naturalis.nl6We use the term textual content in the widest possiblesense, i.e., comprising all character strings, including datesand numbers.the likelihood of a particular value in one fieldmay be dependent on the values in (some of) theother fields.
In our database, for example, thereis an interdependency between the LOCATION andthe COUNTRY columns: the probability that theCOUNTRY column contains the value South Africaincreases if the LOCATION column contains thestring Tafel Mountain (and vice versa).
Similarinterdependencies hold between other columns,such as LOCATION and ALTITUDE, or COUNTRYand BIOTOPE, or between the columns encodinga specimen?s position in the zoological taxonomy(e.g., SPECIES and FAMILY).
Given enough data,many of these interdependencies can be deter-mined automatically and exploited to identify fieldvalues that are likely to be erroneous.This idea bears some similarity to the approachby Marcus and Maletic (2000) who infer associ-ation rules for a data set and then look for out-liers relative to these rules.
However, we do notexplicitly infer rules.
Instead, we trained TiMBL(Daelemans et al, 2004), a memory-based learner,to predict the value of a field given the values ofother fields for the entry.
If the predicted valuediffers from the original value, it is signalled as apotential error to a human annotator.We applied the method to the taxonomic fields(CLASS, ORDER, FAMILY, GENUS, SPECIES andSUB-SPECIES), because it is possible, albeit some-what time-consuming, for a non-expert to checkthe values of these fields against a published zoo-logical taxonomy.
We split the data into 80% train-ing set, 10% development set and 10% test set.
Asnot all taxonomic fields are filled for all entries,the exact sizes for each data set differ, dependingon which field is to be predicted (see Table 1).We used the development data to set TiMBL?sparameters, such as the number of nearest neigh-bours to be taken into account or the similaritymetric (van den Bosch, 2004).
Ideally, one wouldwant to choose the setting which optimised the er-ror detection accuracy.
However, this would re-quire manual annotation of the errors in the devel-opment set.
As this is fairly time consuming, weabstained from it.
Instead we chose the parametersetting which maximised the value prediction ac-curacy for each taxonomic field, i.e.
the setting forwhich the disagreement between the values pre-dicted by TiMBL and the values in the databasewas smallest.
The motivation for this was that ahigh prediction accuracy will minimise the num-42ber of potential errors that get flagged (i.e., dis-agreements between TiMBL and the database) andthus, hopefully, lead to a higher error detectionprecision, i.e., less work for the human annotatorwho has to check the potential errors.training devel.
testCLASS 7,495 937 937ORDER 7,493 937 937FAMILY 7,425 928 928GENUS 7,891 986 986SPECIES 7,873 984 984SUB-SPECIES 1,949 243 243Table 1: Data set sizes for taxonomic fieldsWe also used the development data to performsome feature selection.
We compared (i) usingthe values of all other fields (for a given entry) asfeatures and (ii) only using the other taxonomicfields plus the author field, which encodes whichtaxonomist first described the species to which agiven specimen belongs.7 The reduced feature setwas found to lead to better or equal performancefor all taxonomic fields and was thus used in theexperiments reported below.For each taxonomic field, we then trainedTiMBL on the training set and applied it to thetest set, using the optimised parameter settings.Table 2 shows the value prediction accuracies foreach taxonomic field and the accuracies achievedby two baseline classifiers: (i) randomly select-ing a value from the values found in the trainingset (random) and (ii) always predicting the (train-ing set) majority value (majority).
The predic-tion accuracies are relatively high, even for thelowest fields in the taxonomy, SPECIES and SUB-SPECIES, which should be the most difficult to pre-dict.
Hence it is in principle possible to predict thevalue of a taxonomic field from the values of otherfields in the database.
To determine whether thetaxonomic fields are exceptional in this respect,we also tested how well non-taxonomic fields canbe predicted.
We found that all fields can be pre-dicted with a relatively high accuracy.
The low-est accuracy (63%) is obtained for the BIOTOPEfield.
For most fields, accuracies of around 70%7The author information provides useful cues for the pre-diction of taxonomic fields because taxonomists often spe-cialise on a particular zoological group.
For example, a tax-onomist who specialises on Ranidae (frogs) is unlikely tohave published a description of a species belonging to Ser-pentes (snakes).are achieved; this applies even to the ?free text?fields like SPECIAL REMARKS.TiMBL random majorityCLASS 99.87% 50.00% 54.98%ORDER 98.29% 1.92% 18.59%FAMILY 98.02% 0.35% 10.13%GENUS 92.57% 10.00% 44.76%SPECIES 89.93% 0.20% 7.67%SUB-SPECIES 95.03% 0.98% 21.35%Table 2: Test set prediction accuracies for taxo-nomic field values (horizontal method)To determine whether this method is suitablefor semi-automatic error correction, we looked atthe cases in which the value predicted by TiMBLdiffered from the original value.
There are threepotential reasons for such a disagreement: (i) thevalue predicted by TiMBL is wrong, (ii) the valuepredicted by TiMBL is correct and the originalvalue in the database is wrong, and (iii) both val-ues are correct and the two terms are (zoological)synonyms.
For the fields CLASS, ORDER, FAM-ILY and GENUS, we checked the values predictedby TiMBL against two published zoological tax-onomies8 and counted how many times the pre-dicted value was the correct value.
We did notcheck the two lowest fields (SUB SPECIES andSPECIES), as the correct values for these fields canonly be determined reliably by looking at the spec-imens themselves, not by looking at the other tax-onomic values for an entry.
For the evaluation, wefocused on error correction rather than error detec-tion, hence cases where both the value predictedby TiMBL and the original value in the databasewere wrong, were counted as TiMBL errors.Table 3 shows the results (the absolute numbersof database errors, synonyms and TiMBL errorsare shown in brackets).
It can be seen that TiMBLdetects several errors in the database and predictsthe correct values for them.
It also finds severalsynonyms.
For GENUS, however, the vast ma-jority of disagreements between TiMBL and thedatabase is due to TiMBL errors.
This can be ex-plained by the fact that GENUS is relatively lowin the taxonomy (directly above SPECIES).
As thevalues of higher fields only provide limited cues8We used the ITIS Catalogue of Life (http://www.species2000.org/2005/search.php)and the EMBL Reptile Database (http://www.embl-heidelberg.de/?uetz/LivingReptiles.html).43disagreements database errors synonyms TiMBL errorsCLASS 2 50.00% (1) 0% (0) 50.00% (1)ORDER 26 38.00% (10) 19.00% (5) 43.00% (11)FAMILY 33 9.09% (3) 36.36% (12) 54.55% (18)GENUS 135 5.93% (8) 4.44% (6) 89.63% (121)Table 3: Error correction precision (horizontal method)for the value of a lower field, the lower a field is inthe taxonomy the more difficult it is to predict itsvalue accurately.So far we have only looked at the precisionof our error detection method (i.e., what propor-tion of flagged errors are real errors).
Error de-tection recall (i.e., the proportion of real errorsthat is flagged) is often difficult to determine pre-cisely because this would involve manually check-ing the dataset (or a significant subset) for errors,which is typically quite time-consuming.
How-ever, if errors are identified and corrected semi-automatically, recall is more important than pre-cision; a low precision means more work for thehuman expert who is checking the potential errors,a low recall, however, means that many errors arenot detected at all, which may severely limit theusefulness of the system.To estimate the recall obtained by the horizontalerror detection method, we introduced errors arti-ficially and determined what percentage of theseartificial errors was detected.
For each taxonomicfield, we changed the value of 10% of the entries,which were randomly selected.
In these entries,the original values were replaced by one of theother attested values for this field.
The new valuewas selected randomly and with uniform probabil-ity for all values.
Of course, this method can onlyprovide an estimate of the true recall, as it is possi-ble that real errors are distributed differently, e.g.,some values may be more easily confused by hu-mans than others.
Table 4 shows the results.
Theestimated recall is fairly high; in all cases above90%.
This suggests that a significant proportionof the errors is detected by our method.5 Vertical Error CorrectionWhile the horizontal method described in the pre-vious section aimed at correcting values whichare inconsistent with the remaining fields of adatabase entry, vertical error correction is aimedat a different type of error, namely, text stringswhich were entered in the wrong column of therecallCLASS 95.56%ORDER 96.82%FAMILY 96.15%GENUS 93.09%SPECIES 96.75%SUB SPECIES 95.38%Table 4: Recall for artificially introduced errors(horizontal method)database.
For example, in our database, informa-tion about the biotope in which a specimen wasfound may have been entered in the SPECIAL RE-MARKS column rather than the BIOTOPE column.Errors of this type are quite frequent.
They canbe accidental, i.e., the person entering the infor-mation inadvertently chose the wrong column, butthey can also be due to misinterpretation, e.g., theperson entering the information may believe that itfits the SPECIAL REMARKS column better than theBIOTOPE column or they may not know that thereis a BIOTOPE column.
Some of these errors mayalso stem from changes in the database structureitself, e.g., maybe the BIOTOPE column was onlyadded after the data was entered.9Identifying this type of error can be recast as atext classification task: given the content of a cell,i.e., a string of text, the aim is to determine whichcolumn the string most likely belongs to.
Textstrings which are classified as belonging to a dif-ferent column than they are currently in, representa potential error.
Recasting error detection as atext classification problem allows the use of super-vised machine learning methods, as training data(i.e., text strings labelled with the column they be-long to) can easily be obtained from the database.We tokenised the text strings in all databasefields10 and labelled them with the column they9Many databases, especially in the cultural heritage do-main, are not designed and maintained by database experts.Over time, such database are likely to evolve and changestructurally.
In our specimens database, for example, severalcolumns were only added at later stages.10We used a rule-based tokeniser for Dutch developed by44occur in.
Each string was represented as a vec-tor of 48 features, encoding the (i) string itself andsome of its typographical properties (13 features),and (ii) its similarity with each of the 35 columns(in terms of weighted token overlap) (35 features).The typographical properties we encoded were:the number of tokens in the string and whether itcontained an initial (i.e., an individual capitalisedletter), a number, a unit of measurement (e.g., km),punctuation, an abbreviation, a word (as opposedto only numbers, punctuation etc.
), a capitalisedword, a non-capitalised word, a short word (< 4characters), a long word, or a complex word (e.g.,containing a hyphen).The similarity between a string, consisting of aset T of tokens t1 .
.
.
tn, and a column colx wasdefined as:sim(T, colx) =?ni=1 ti ?
tfidfti,colx|T |where tfidfticolx is the tfidf weight (term fre-quency - inverse document frequency, cf.
(Sparck-Jones, 1972)) of token ti in column colx.
Thisweight encodes how representative a token is ofa column.
The term frequency, tfti,colx , of a tokenti in column colx is the number of occurrences ofti in colx divided by the number of occurrencesof all tokens in colx.
The term frequency is 0 ifthe token does not occur in the column.
The in-verse document frequency, idfti , of a token ti isthe number of all columns in the database dividedby the number of columns containing ti.
Finally,the tfidf weight for a term ti in column colx is de-fined as:tfidfti,colx = tfti,colx log idftiA high tfidf weight for a given token in a givencolumn means that the token frequently occurs inthat column but rarely in other columns, thus thetoken is a good indicator for that column.
Typ-ically tfidf weights are only calculated for con-tent words, however we calculated them for alltokens, partly because the use of stop word liststo filter out function words would have jeopar-dised the language independence of our methodand partly because function words and even punc-tuation can be very useful for distinguishing dif-ferent columns.
For example, prepositions such asunder often indicate BIOTOPE, as in under a stone.Sabine Buchholz.
The inclusion of multi-lingual abbrevi-ations in the rule set ensures that this tokeniser is robustenough to also cope with text strings in English and otherWestern European languages.To assign a text string to one of the 35 databasecolumns, we trained TiMBL (Daelemans et al,2004) on the feature vectors of all other databasecells labelled with the column they belong to.11Cases where the predicted column differed fromthe current column of the string were recorded aspotential errors.We applied the classifier to all filled databasecells.
For each of the strings identified as potentialerrors, we checked manually (i) whether this wasa real error (i.e., error detection) and (ii) whetherthe column predicted by the classifier was the cor-rect one (i.e., error correction).
While checkingfor this type of error is much faster than checkingfor errors in the taxonomic fields, it is sometimesdifficult to tell whether a flagged error is a real er-ror.
In some cases it is not obvious which col-umn a string belongs to, for example because twocolumns are very similar in content (such as LO-CATION and FINDING PLACE), in other cases thecontent of a database field contains several piecesof information which would best be located in dif-ferent columns.
For instance, the string found withbroken neck near Karlobag arguably could be splitbetween the SPECIAL REMARKS and the LOCA-TION columns.
We were conservative in the firstcase, i.e., we did not count an error as correctlyidentified if the string could belong to the origi-nal column, but we gave the algorithm credit forflagging potential errors where part of the stringshould be in a different column.The results are shown in the second column (un-filtered) in Table 5.
The classifier found 836 poten-tial errors, 148 of these were found to be real er-rors.
For 100 of the correctly identified errors thepredicted column was the correct column.
Someof the corrected errors can be found in Table 6.Note that the system corrected errors in both En-glish and Dutch text strings without requiring lan-guage identification or any language-specific re-sources (apart from tokenisation).We also calculated the precision of error detec-tion (i.e., the number of real errors divided by thenumber of flagged errors) and the error correctionaccuracy (i.e., the number of correctly correctederrors divided by the number correctly identifiederrors).
The error detection precision is relativelylow (17.70%).
In general a low precision meansrelatively more work for the human expert check-11We used the default settings (IB1, Weighted OverlapMetric, Information Gain Ratio weighting) and k=3.45string original column corrected columnop boom ongeveer 2,5 m boven grond SPECIAL REMARKS BIOTOPE(on a tree about 2.5 m above ground)25 km N.N.W Antalya SPECIAL REMARKS LOCATION1700 M BIOTOPE ALTITUDEgestorven in gevangenschap 23 september 1994 LOCATION SPECIAL REMARKS(died in captivity 23 September 1994)roadside bordering secondary forest LOCATION BIOTOPESuriname Exp.
1970 COLLECTION NUMBER COLLECTOR(Surinam Expedition 1970)Table 6: Examples of automatically corrected errors (vertical method)unfiltered filteredflagged errors 836 262real errors 148 67correctly corrected 100 54precision error detection 17.70 % 25.57%accuracy error correction 67.57% 80.60%Table 5: Results automatic error detection and cor-rection for all database fields (vertical method)ing the flagged errors.
However, note that the sys-tem considerably reduces the number of databasefields that have to be checked (i.e., 836 out of229,430 filled fields).
We also found that, for thistype of error, error checking can be done relativelyquickly even by a non-expert; checking the 836 er-rors took less than 30 minutes.
Furthermore, thecorrection accuracy is fairly high (67.57%), i.e.,for most of the correctly identified errors the cor-rect column is suggested.
This means that for mosterrors the user can simply choose the column sug-gested by the classifier.In an attempt to increase the detection preci-sion we applied two filters and only flagged errorswhich passed these filters.
First, we filtered outpotential errors if the original and the predictedcolumn were of a similar type (e.g., if both con-tained person names or dates) as we noticed thatour method was very prone to misclassificationsin these cases.12 For example, if the name M.S.Hoogmoed occurs several times in the COLLEC-TOR column and a few times in the DONATOR col-umn, the latter cases are flagged by the system aspotential errors.
However, it is entirely normal fora person to occur in both the COLLECTOR and theDONATOR column.
What is more, it is impossible12Note, that this filter requires a (very limited) amount ofbackground knowledge, i.e.
knowledge about which columnsare of a similar type.to determine on the basis of the text string M.S.Hoogmoed alone, whether the correct column forthis string in a given entry is DONATOR or COL-LECTOR or both.13 Secondly, we only flagged er-rors where the predicted column was empty for thecurrent database entry.
If the predicted column isalready occupied, the string is unlikely to belongto that column (unless the string in that column isalso an error).
The third column in Table 5 (fil-tered) shows the results.
It can be seen that de-tection precision increases to 25.57% and correc-tion precision to 80.60%, however the system alsofinds noticeably fewer errors (67 vs. 148).Prec.
Rec.BIOTOPE 20.09% 94.00%PUBLICATION 6.90% 100.00%SPECIAL REMARKS 16.11% 24.00%Table 7: Precision and Recall for three free textcolumns (vertical method)Estimating the error detection recall (i.e., thenumber of identified errors divided by the over-all number of errors in the database) would in-volve manually identifying all the errors in thedatabase.
This was not feasible for the databaseas a whole.
Instead we manually checked threeof the free text columns, namely, BIOTOPE, PUB-LICATION and SPECIAL REMARKS, for errors andcalculated the recall and precision for these.
Ta-ble 7 shows the results.
For BIOTOPE and PUB-LICATION the recall is relatively high (94% and100%, respectively), for SPECIAL REMARKS it ismuch lower (24%).
The low recall for SPECIALREMARKS is probably due to the fact that this col-13Note, however, that the horizontal error detection methodproposed in the previous section might detect an erroneousoccurrence of this string (based on the values of other fieldsin the entry).46umn is very heterogeneous, thus it is fairly difficultto find the true errors in it.
While the precision isrelatively low for all three columns, the numberof flagged errors (ranging from 58 for PUBLICA-TION to 298 for SPECIAL REMARKS) is still smallenough for manual checking.6 ConclusionWe have presented two methods for(semi-)automatic error detection and correc-tion in textual databases.
The two methods areaimed at different types of errors: horizontalerror correction attempts to identify and correctinconsistent values within a database record;vertical error correction is aimed at values whichwere accidentally entered in the wrong column.Both methods are data-driven and require littleor no background knowledge.
The methods arealso language-independent and can be applied tomulti-lingual databases.
While we utilise super-vised machine learning, no manual annotationof training data is required, as the training set isobtained directly from the database.We tested the two methods on an animal spec-imens database and found that a significant pro-portion of errors could be detected: up to 97% forhorizontal error detection and up to 100% for ver-tical error detection.
While the error detection pre-cision was fairly low for both methods (up to 55%for the horizontal method and up to 25.57% for thevertical method), the number of potential errorsflagged was still sufficiently small to check manu-ally.
Furthermore, the automatically predicted cor-rection for an error was often the right one.
Hence,it would be feasible to employ the two methods ina semi-automatic error correction set-up where po-tential errors together with a suggested correctionare flagged and presented to a user.As the two error correction methods are to someextent complementary, it would be worthwhile toinvestigate whether they can be combined.
Someerrors flagged by the horizontal method will not bedetected by the vertical method, for instance, val-ues which are valid in a given column, but incon-sistent with the values of other fields.
On the otherhand, values which were entered in the wrong col-umn should, in theory, also be detected by the hor-izontal method.
For example, if the correct FAM-ILY for Rana aurora is Ranidae, it should makeno difference whether the (incorrect) value in theFAMILY field is Bufonidae, which is a valid valuefor FAMILY but the wrong family for Rana aurora,or Amphibia, which is not a valid value for FAM-ILY but the correct CLASS value for Rana aurora;in both cases the error should be detected.
Hence,if both methods predict an error in a given fieldthis should increase the likelihood that there is in-deed an error.
This could be exploited to obtain ahigher precision.
We plan to experiment with thisidea in future research.Acknowledgments The research reported inthis paper was funded by NWO (Netherlands Or-ganisation for Scientific Research) and carried outat the Naturalis Research Labs in Leiden.
Wewould like to thank Pim Arntzen and Erik vanNieukerken from Naturalis for guidance and help-ful discussions.
We are also grateful to two anony-mous reviewers for useful comments.ReferencesA.
Bagga.
1998.
Coreference, Cross-Document Coref-erence, and Information Extraction Methodologies.Ph.D.
thesis, Dept.
of Computer Science, Duke Uni-versity.W.
Daelemans, J. Zavrel, K. van der Sloot, A. van denBosch, 2004.
TiMBL: Tilburg Memory BasedLearner, version 5.1, Reference Guide, 2004.
ILKResearch Group Technical Report Series no.
04-02.H.
Galhardas, D. Florescu, D. Shasha, E. Simon.
1999.An extensible framework for data cleaning.
Tech-nical Report RR-3742, INRIA Technical Report,1999.D.
M. Hawkins.
1980.
Identification of outliers.
Chap-man and Hall, London.M.
A. Herna?ndez, S. J. Stolfo.
1998.
Real-world datais dirty: Data cleansing and the merge/purge prob-lem.
Journal of Data Mining and Knowledge Dis-covery, 2:1?31.M.-F. Jiang, S.-S. Tseng, C.-M. Su.
2001.
Two-phaseclustering process for outliers detection.
PatternRecognition Letters, 22:691?700.E.
M. Knorr, R. T. Ng.
1998.
Algorithms for min-ing distance-based outliers in large datasets.
In Pro-ceedings of the 24th International Conference onVery Large Data Bases (VLDB?98).A.
Marcus, J. I. Maletic.
2000.
Utilizing associationrules for identification of possible errors in data sets.Technical Report TR-CS-00-04, The University ofMemphis, Division of Computer Science, 2000.I.
Ruts, P. J. Rousseeuw.
1996.
Computing depthcontours of bivariate point clouds.
ComputationalStatistics and Data Analysis, 23:153?168.K.
Sparck-Jones.
1972.
A statistical interpretation ofterm specificity and its application in retrieval.
Jour-nal of Documentation, 28:11?21.A.
van den Bosch.
2004.
Wrapped progressive sam-pling search for optimizing learning algorithm pa-rameters.
In Proceedings of the 16th Belgian-DutchConference on Artificial Intelligence, 219?226.47
