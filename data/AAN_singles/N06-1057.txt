Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 447?454,New York, June 2006. c?2006 Association for Computational LinguisticsParaEval: Using Paraphrases to Evaluate Summaries AutomaticallyLiang Zhou, Chin-Yew Lin, Dragos Stefan Munteanu, and Eduard HovyUniversity of Southern CaliforniaInformation Sciences Institute4676 Admiralty WayMarina del Rey, CA 90292-6695{liangz, cyl, dragos, hovy} @isi.eduAbstractParaEval is an automated evaluationmethod for comparing reference and peersummaries.
It facilitates a tiered-comparison strategy where recall-orientedglobal optimal and local greedy searchesfor paraphrase matching are enabled inthe top tiers.
We utilize a domain-independent paraphrase table extractedfrom a large bilingual parallel corpus us-ing methods from Machine Translation(MT).
We show that the quality of ParaE-val?s evaluations, measured by correlatingwith human judgments, closely resemblesthat of ROUGE?s.1 IntroductionContent coverage is commonly measured in sum-mary comparison to assess how much informationfrom the reference summary is included in a peersummary.
Both manual and automatic methodolo-gies have been used.
Naturally, there is a greatamount of confidence in manual evaluation sincehumans can infer, paraphrase, and use worldknowledge to relate text units with similar mean-ings, but which are worded differently.
Humanefforts are preferred if the evaluation task is easilyconducted and managed, and does not need to beperformed repeatedly.
However, when resourcesare limited, automated evaluation methods becomemore desirable.For years, the summarization community hasbeen actively seeking an automatic evaluationmethodology that can be readily applied to varioussummarization tasks.
ROUGE (Lin and Hovy,2003) has gained popularity due to its simplicityand high correlation with human judgments.
Eventhough validated by high correlations with humanjudgments gathered from previous Document Un-derstanding Conference (DUC) experiments, cur-rent automatic procedures (Lin and Hovy, 2003;Hovy et al, 2005) only employ lexical n-grammatching.
The lack of support for word or phrasematching that stretches beyond strict lexicalmatches has limited the expressiveness and utilityof these methods.
We need a mechanism that sup-plements literal matching?i.e.
paraphrase andsynonym?and approximates semantic closeness.In this paper we present ParaEval, an automaticsummarization evaluation method, which facili-tates paraphrase matching in an overall three-levelcomparison strategy.
At the top level, favoringhigher coverage in reference, we perform an opti-mal search via dynamic programming to findmulti-word to multi-word paraphrase matches be-tween phrases in the reference summary (usuallyhuman-written) and those in the peer summary(system-generated).
The non-matching fragmentsfrom the previous level are then searched by agreedy algorithm to find single-word para-phrase/synonym matches.
At the third and the low-est level, we perform literal lexical unigrammatching on the remaining texts.
This tiered designfor summary comparison guarantees at least aROUGE-1 level of summary content matching ifno paraphrases are found.The first two levels employ a paraphrase table.Since manually created multi-word paraphrases-?phrases determined by humans to be paraphrasesof one another?are not available in sufficientquantities, we automatically build a paraphrase447table using methods from the Machine Translation(MT) field.
The assumption made in creating thistable is that if two English phrases are translatedinto the same foreign phrase with high probability(shown in the alignment results from a statisticallytrained alignment algorithm), then the two Englishphrases are paraphrases of each other.This paper is organized in the following way:Section 2 introduces previous work in summariza-tion evaluation; Section 3 describes the motivationbehind this work; paraphrase acquisition is dis-cussed in Section 4; Section 5 explains in detailour summary comparison mechanism; Section 6validates ParaEval with human summary judg-ments; and we conclude and discuss future work inSection 7.2 Previous WorkThere has been considerable work in both manualand automatic summarization evaluations.
Threemost noticeable efforts in manual evaluation areSEE (Lin and Hovy, 2001), Factoid (Van Halterenand Teufel, 2003), and the Pyramid method(Nenkova and Passonneau, 2004).SEE provides a user-friendly environment inwhich human assessors evaluate the quality ofsystem-produced peer summary by comparing it toa reference summary.
Summaries are representedby a list of summary units (sentences, clauses,etc.).
Assessors can assign full or partial contentcoverage score to peer summary units in compari-son to the corresponding reference summary units.Grammaticality can also be graded unit-wise.The goal of the Factoid work is to compare theinformation content of different summaries of thesame text and determine the minimum number ofsummaries, which was shown through experimen-tation to be 20-30, needed to achieve stable con-sensus among 50 human-written summaries.The Pyramid method uses identified consen-sus?a pyramid of phrases created by annota-tors?from multiple reference summaries as thegold-standard reference summary.
Summary com-parisons are performed on Summarization ContentUnits (SCUs) that are approximately of clauselength.To facilitate fast summarization system design-evaluation cycles, ROUGE was created (Lin andHovy, 2003).
It is an automatic evaluation packagethat measures a number of n-gram co-occurrencestatistics between peer and reference summarypairs.
ROUGE was inspired by BLEU (Papineni etal., 2001) which was adopted by the machinetranslation (MT) community for automatic MTevaluation.
A problem with ROUGE is that thesummary units used in automatic comparison areof fixed length.
A more desirable design is to havesummary units of variable size.
This idea was im-plemented in the Basic Elements (BE) framework(Hovy et al, 2005) which has not been completeddue to its lack of support for paraphrase matching.Both ROUGE and BE have been shown to corre-late well with past DUC human summary judg-ments, despite incorporating only lexical matchingon summary units (Lin and Hovy, 2003; Hovy etal., 2005).3 Motivation3.1 Paraphrase MatchingAn important difference that separates currentmanual evaluation methods from their automaticcounterparts is that semantic matching of contentunits is performed by human summary assessors.An essential part of the semantic matching in-volves paraphrase matching?determining whetherphrases worded differently carry the same semanticinformation.
This paraphrase matching process isobserved in the Pyramid annotation procedureshown in (Nenkova and Passonneau, 2004) overthree summary sets (10 summaries each).
In theexample shown in Figure 1 (reproduced fromPyramid results), each of the 10 phrases (numbered1 to 10) extracted from summary sentences carriesthe same semantic content as the overall summarycontent unit labeled SCU1 does.
Each extractedphrase is identified as a summary content unit(SCU).
In our work in building an automaticevaluation procedure that enables paraphraseSCU1: the crime in question was the Lockerbie {Scotland} bombing1 [for the Lockerbie bombing]2 [for blowing up] [over Lockerbie, Scotland]3 [of bombing] [over Lockerbie, Scotland]4 [was blown up over Lockerbie, Scotland, ]5 [the bombing of Pan Am Flight 103]6 [bombing over Lockerbie, Scotland, ]7 [for Lockerbie bombing]8 [bombing of Pan Am flight 103 over Lockerbie.
]9 [linked to the Lockerbie bombing]10 [in the Lockerbie bombing case.
]Figure 1.
Paraphrases created by Pyramid annotation.448matching, we aim to automatically identify these10 phrases as paraphrases of one another.3.2 Synonymy RelationsSynonym matching and paraphrase matching areoften mentioned in the same context in discussionsof extending current automated summarizationevaluation methods to incorporate the matching ofsemantic units.
While evaluating automaticallyextracted paraphrases via WordNet (Miller et al,1990), Barzilay and McKeown (2001) quantita-tively validated that synonymy is not the onlysource of paraphrasing.
We envisage that thisclaim is also valid for summary comparisons.From an in-depth analysis on the manually cre-ated SCUs of the DUC2003 summary set D30042(Nenkova and Passonneau, 2004), we find that54.48% of 1746 cases where a non-stop word fromone SCU did not match with its supposedly hu-man-aligned pairing SCUs are in need of somelevel of paraphrase matching support.
For example,in the first two extracted SCUs (labeled as 1 and 2)in Figure 1?
?for the Lockerbie bombing?
and ?forblowing up ?
over Lockerbie, Scotland?
?nonon-stop word other than the word ?Lockerbie?occurs in both phrases.
But these two phrases werejudged to carry the same semantic meaning be-cause human annotators think the word ?bombing?and the phrase ?blowing up?
refer to the same ac-tion, namely the one associated with ?explosion.
?However, ?bombing?
and ?blowing up?
cannot bematched through synonymy relations by usingWordNet, since one is a noun and the other is averb phrase (if tagged within context).
Even whenthe search is extended to finding synonyms andhypernyms for their categorical variants and/orusing other parts of speech (verb for ?bombing?and noun phrase for ?blowing up?
), a match stillcannot be found.To include paraphrase matching in summaryevaluation, a collection of less-strict paraphrasesmust be created and a matching strategy needs tobe investigated.4 Paraphrase AcquisitionParaphrases are alternative verbalizations for con-veying the same information and are required bymany Natural Language Processing (NLP) appli-cations.
In particular, summary creation andevaluation methods need to recognize paraphrasesand their semantic equivalence.
Unfortunately, wehave yet to incorporate into the evaluation frame-work previous findings in paraphrase identificationand extraction (Barzilay and McKeown, 2001;Pang et al, 2003; Bannard and Callison-Burch,2005).4.1 Related Work on ParaphrasingThree major approaches in paraphrase collectionare manual collection (domain-specific), collectionutilizing existing lexical resources (i.e.
WordNet),and derivation from corpora.
Hermjakob et al(2002) view paraphrase recognition asreformulation by pattern recognition.
Pang et al(2003) use word lattices as paraphrase representa-tions from semantically equivalent translationssets.
Using parallel corpora, Barzilay and McKe-own (2001) identify paraphrases from multipletranslations of classical novels, where as Bannardand Callison-Burch (2005) develop a probabilisticrepresentation for paraphrases extracted from largeMachine Translation (MT) data sets.4.2 Extracting ParaphrasesOur method to automatically construct a large do-main-independent paraphrase collection is basedon the assumption that two different Englishphrases of the same meaning may have the sametranslation in a foreign language.Phrase-based Statistical Machine Translation(SMT) systems analyze large quantities of bilin-gual parallel texts in order to learn translationalalignments between pairs of words and phrases intwo languages (Och and Ney, 2004).
The sentence-based translation model makes word/phrase align-ment decisions probabilistically by computing theoptimal model parameters with application of thestatistical estimation theory.
This alignment proc-ess results in a corpus of word/phrase-aligned par-allel sentences from which we can extract phrasepairs that are translations of each other.
We ran thealignment algorithm from (Och and Ney, 2003) ona Chinese-English parallel corpus of 218 millionEnglish words.
Phrase pairs are extracted by fol-lowing the method described in (Och and Ney,2004) where all contiguous phrase pairs havingconsistent alignments are extraction candidates.The resulting phrase table is of high quality; boththe alignment models and phrase extraction meth-449ods have been shown to produce very good resultsfor SMT.
Using these pairs we build paraphrasesets by joining together all English phrases withthe same Chinese translation.
Figure 2 shows anexample word/phrase alignment for two parallelsentence pairs from our corpus where the phrases?blowing up?
and ?bombing?
have the same Chi-nese translation.
On the right side of the figure weshow the paraphrase set which contains these twophrases, which is typical in our collection of ex-tracted paraphrases.5 Summary Comparison in ParaEvalThis section describes the process of comparing apeer summary against a reference summary and thesummary grading mechanism.5.1 DescriptionWe adopt a three-tier matching strategy for sum-mary comparison.
The score received by a peersummary is the ratio of the number of referencewords matched to the total number of words in thereference summary.
The total number of matchedreference words is the sum of matched words inreference throughout all three tiers.
At the toplevel, favoring high recall coverage, we perform anoptimal search to find multi-word paraphrasematches between phrases in the reference summaryand those in the peer.
Then a greedy search is per-formed to find single-word paraphrase/synonymmatches among the remaining text.
Operationsconducted in these two top levels are marked aslinked rounded rectangles in Figure 3.
At the bot-tom level, we find lexical identity matches, asmarked in rectangles in the example.
If no para-phrases are found, this last level provides a guar-antee of lexical comparison that is equivalent towhat other automated systems give.
In our system,the bottom level currently performs unigrammatching.
Thus, we are ensured with at least aROUGE-1 type of summary comparison.
Alterna-tively, equivalence of other ROUGE configura-tions can replace the ROUGE-1 implementation.There is no theoretical reason why the first twolevels should not merge.
But due to high computa-tional cost in modeling an optimal search, the sepa-ration is needed.
We explain this in detail below.5.2 Multi-Word Paraphrase MatchingIn this section we describe the algorithm that per-forms the multi-word paraphrase matching be-tween phrases from reference and peer summaries.Using the example in Figure 3, this algorithm cre-ates the phrases shown in the rounded rectanglesand establishes the appropriate links indicatingcorresponding paraphrase matches.Problem DescriptionMeasuring content coverage of a peer summaryusing a single reference summary requires com-puting the recall score of how much informationfrom the reference summary is included in thepeer.
A summary unit, either from reference orpeer, cannot be matched for more than once.
ForFigure 2.
An example of paraphrase extraction.Figure 3.
Comparison of summaries.450example, the phrase ?imposed sanctions on Libya?
(r1) in Figure 3?s reference summary was matchedwith the peer summary?s ?voted sanctions againstLibya?
(p1).
If later in the peer summary there isanother phrase p2 that is also a paraphrase of r1, thematch of r1 cannot be counted twice.
Conversely,double counting is not permissible forphrase/words in the peer summary, either.We conceptualize the comparison of peeragainst reference as a task that is to complete overseveral time intervals.
If the reference summarycontains n sentences, there will be n time intervals,where at time ti, phrases from a particular sentencei of the reference summary are being consideredwith all possible phrases from the peer summaryfor paraphrase matches.
A decision needs to bemade at each time interval:?
Do we employ a local greedy match algo-rithm that is recall generous (preferring morematched words from reference) towards only thereference sentence currently being analyzed,?
Or do we need to explore globally, in-specting all reference sentences and find the bestoverall matching combinations?Consider the scenario in Figure 4:1) at t0: L(p1 = r2) > L(p2 = r1) and r2 contains r1.A local search algorithm leads to match(p1, r2).
L() indi-cates the number of words in reference matched by thepeer phrase through paraphrase matching and match()indicates a paraphrase match has occurred (more in thefigure).2) at t1: L(p1 = r3) > L(p1 = r2).
A global algo-rithm reverses the decision match(p1, r2) made at t0 andconcludes match(p1, r3) and match(p2, r1) .
A localsearch algorithm would have returned no match.Clearly, the global search algorithm achieveshigher overall recall (in words).
The matching ofparaphrases between a reference and its peer be-comes a global optimization problem, maximizingthe content coverage of the peer compared in refer-ence.Solution ModelWe use dynamic programming to derive the solu-tion of finding the best paraphrase-matching com-binations.
The optimization problem is as follows:Sentences from a reference summary and a peersummary can be broken into phrases of variouslengths.
A paraphrase lookup table is used to findwhether a reference phrase and a peer phrase areparaphrases of each other.
What is the optimalparaphrase matching combination of phrases fromreference and peer that gives the highest recallscore (in number of matched reference words) forthis given peer?
The solution should be recall ori-ented (favoring a peer phrase that matches morereference words than those match less).Following (Trick, 1997), the solution can becharacterized as:1) This problem can be divided into n stagescorresponding to the n sentences of the referencesummary.
At each stage, a decision is required todetermine the best combination of matched para-phrases between the reference sentence and theentire peer summary that results in no doublecounting of phrases on the peer side.
There is nodouble counting of reference phrases across stagessince we are processing one reference sentence at atime and are finding the best paraphrase matchesusing the entire peer summary.
As long as there isno double counting in peers, we are guaranteed tohave none in reference, either.2) At each stage, we define a number of pos-sible states as follows.
If, out of all possiblephrases of any length extracted from the referencesentence, m phrases were found to have matchingparaphrases in the peer summary, then a state isany subset of the m phrases.3) Since no double counting in matchedphrases/words is allowed in either the referencesummary or the peer summary, the decision ofwhich phrases (leftover text segments in referencePj and ri represent phrases chosen for   paraphrasematching from peer and reference respectively.Pj = ri indicates that the phrase Pj from peer isfound to be a paraphrase to the phrase ri fromreference.L(Pj = ri) indicates the number of words matchedby Pj in ri when they are found to be paraphrases ofeach other.L(Pj = ri) and L(Pj = ri+1) may not be equal if thenumber of words in ri, indicated by L(ri), does notequal to the number of words in ri+1, indicated byL(ri+1).Figure 4.
Local vs. global paraphrase matching.451and in peer) are allowed to match for the next stageis made in the current stage.4) Principle of optimality: at a given state, itis not necessary to know what matches occurred atprevious stages, only on the accumulated recallscore (matched reference words) from previousstages and what text segments (phrases) in peerhave not been taken/matched in previous stages.5) There exists a recursive relationship thatidentifies the optimal decision for stage s (out of ntotal stages), given that stage s+1 has already beensolved.6) The final stage, n (last sentence in refer-ence), is solved by choosing the state that has thehighest accumulated recall score and yet resultedno double counting in any phrase/word in peer thesummary.Figure 5 demonstrates the optimal solution (12reference words matched) for the example shownin Figure 4.
We can express the calculations in thefollowing formulas:where fy(xb) denotes the optimal recall coverage(number of words in the reference summarymatched by the phrases from the peer summary) atstate xb in stage y. r(xb) is the recall coverage givenstate xb.
And c(xb) records the phrases matched inpeer with no double counting, given state xb.5.3 Synonym MatchingAll paraphrases whose pairings do not involvemulti-word to multi-word matching are calledsynonyms in our experiment.
Since these phraseshave either a n-to-1 or 1-to-n matching ratio (suchas the phrases ?blowing up?
and ?bombing?
), agreedy algorithm favoring higher recall coveragereduces the state creation and stage comparisoncosts associated with the optimal procedure(O(m6): O(m3) for state creation, and for 2 stages atany time)).
The paraphrase table described in Sec-tion 4 is used.Synonym matching is performed only on partsof the reference and peer summaries that were notmatched from the multi-word paraphrase-matchingphase.5.4 Lexical MatchingThis matching phase performs straightforwardlexical matching, as exemplified by the text frag-ments marked in rectangles in Figure 3.
Unigramsare used as the units for counting matches in ac-cordance with the previous two matching phases.During all three matching phases, we employeda ROUGE-1 style of counting.
Other alternatives,such as ROUGE-2, ROUGE-SU4, etc., can easilybe adapted to each phase.6  Evaluation of ParaEvalTo evaluate and validate the effectiveness of anautomatic evaluation metric, it is necessary toshow that automatic evaluations correlate withhuman assessments highly, positively, and consis-tently (Lin and Hovy, 2003).
In other words, anautomatic evaluation procedure should be able todistinguish good and bad summarization systemsby assigning scores with close resemblance to hu-mans?
assessments.6.1 Document Understanding ConferenceThe Document Understanding Conference hasprovided large-scale evaluations on both human-created and system-generated summaries annually.Research teams are invited to participate in solvingsummarization problems with their systems.
Sys-tem-generated summaries are then assessed byhumans and/or automatic evaluation procedures.The collection of human judgments on systems andtheir summaries has provided a test-bed for devel-oping and validating automated summary gradingmethods (Lin and Hovy, 2003; Hovy et al, 2005).The correlations reported by ROUGE and BEshow that the evaluation correlations between thesetwo systems and DUC human evaluations aremuch higher on single-document summarizationtasks.
One possible explanation is that when sum-Figure 5.
Solution for the example in Figure 4.452marizing from only one source (text), both human-and system-generated summaries are mostly ex-tractive.
The reason for humans to take phrases (ormaybe even sentences) verbatim is that there is lessmotivation to abstract when the input is not highlyredundant, in contrast to input for multi-documentsummarization tasks, which we speculate allowsmore abstracting.
ROUGE and BE both facilitatelexical n-gram matching, hence, achieving amaz-ing correlations.
Since our baseline matching strat-egy is lexically based when paraphrase matching isnot activated, validation on single-doc summariza-tion results is not repeated in our experiment.6.2 Validation and DiscussionWe use summary judgments from DUC2003?smulti-document summarization (MDS) task toevaluate ParaEval.
During DUC2003, participatingsystems created short summaries (~100 words) for30 document sets.
For each set, one assessor-written summary was used as the reference tocompare peer summaries created by 18 automaticsystems (including baselines) and 3 other human-written summaries.
A system ranking was pro-duced by taking the averaged performance on allsummaries created by systems.
This evaluationprocess is replicated in our validation setup forParaEval.
In all, 630 summary pairs were com-pared.
Pearson?s correlation coefficient is com-puted for the validation tests, using DUC2003assessors?
results as the gold standard.Table 1 illustrates the correlation figures fromthe DUC2003 test set.
ParaEval-para_only showsthe correlation result when using only paraphraseand synonym matching, without the baseline uni-gram matching.
ParaEval-2 uses multi-word para-phrase matching and unigram matching, omittingthe greedy synonym-matching phrase.
ParaEval-3incorporates matching at all three granularity lev-els.We see that the current implementation ofParaEval closely resembles the way ROUGE-1differentiates system-generated summaries.
Webelieve this is due to the identical calculations ofrecall scores.
The score that a peer summary re-ceives from ParaEval depends on the number ofwords matched in the reference summary from itsparaphrase, synonym, and unigram matches.
Thecounting of individual words in reference indicatesa ROUGE-1 design in grading.
However, a de-tailed examination on individual reference-peercomparisons shows that paraphrase and synonymcomparisons and matches, in addition to lexical n-gram matching, do measure a higher level of con-tent coverage.
This is demonstrated in Figure 6aand b.
Strict unigram matching reflects the contentretained by a peer summary mostly in the 0.2-0.4ranges in recall, shown as dark-colored dots in thegraphs.
Allowing paraphrase and synonym match-ing increases the detection of peer coverage to therange of 0.3-0.5, shown as light-colored dots.We conducted a manual evaluation to furtherexamine the paraphrases being matched.
Using 10summaries from the Pyramid data, we asked threehuman subjects to judge the validity of 128 (ran-domly selected) paraphrase pairs extracted andidentified by ParaEval.
Each pair of paraphraseswas coupled with its respective sentences as con-texts.
All paraphrases judged were multi-word.ParaEval received an average precision of 68.0%.The complete agreement between judges is 0.582according to the Kappa coefficient (Cohen, 1960).In Figure 7, we show two examples that the humanjudges consider to be good paraphrases producedand matched by ParaEval.
Judges voiced difficul-DUC-2003 PearsonROUGE-1 0.622ParaEval-para_only 0.41ParaEval-2 0.651ParaEval-3 0.657Table 1.
Correlation with DUC 2003 MDS results.Human Summaries: ParaEval vs. ROUGE-100.10.20.30.40.50.60.7DUC2003 Summary WritersRecall(%wordmatch)ROUGE-1 ScoresParaEval-3 ScoresSystem Summaries: ParaEval vs. ROUGE-100.10.20.30.40.50.60.7DUC2003 SystemsRecall(%wordmatch)ROUGE-1 ScoresParaEval-3 Scoresa).Human-writtensummaries.b).System-generatedsummaries.Figure 6.
A detailed look at the scores assigned bylexical and paraphrase/synonym comparisons.453Figure 7.
Paraphrases matched by ParaEval.ties in determining ?semantic equivalence.?
Therewere cases where paraphrases would be generallyinterchangeable but could not be matched becauseof non-semantic equivalence in their contexts.
Andthere were paraphrases that were determined asmatches, but if taken out of context, would not bedirect replacements of each other.
These two situa-tions are where the judges mostly disagreed.7 Conclusion and Future WorkIn this paper, we have described an automaticsummarization evaluation method, ParaEval, thatfacilitates paraphrase matching using a large do-main-independent paraphrase table extracted froma bilingual parallel corpus.
The three-layer match-ing strategy guarantees a ROUGE-like baselinecomparison if paraphrase matching fails.The paraphrase extraction module from the cur-rent implementation of ParaEval does not dis-criminate among the phrases that are found to beparaphrases of one another.
We wish to incorporatethe probabilistic paraphrase extraction model from(Bannard and Callison-Burch, 2005) to better ap-proximate the relations between paraphrases.
Thisadaptation will also lead to a stochastic model forthe low-level lexical matching and scoring.We chose English-Chinese MT parallel data be-cause they are news-oriented which coincides withthe task genre from DUC.
However, it is unknownhow large a parallel corpus is sufficient in provid-ing a paraphrase collection good enough to helpthe evaluation process.
The quality of the para-phrase table is also affected by changes in the do-main and language pair of the MT parallel data.We plan to use ParaEval to investigate the impactof these changes on paraphrase quality under theassumption that better paraphrase collections leadto better summary evaluation results.The immediate impact and continuation of thedescribed work would be to incorporate paraphrasematching and extraction into the summary creationprocess.
And with ParaEval, it is possible for us toevaluate systems that do incorporate some level ofabstraction, especially paraphrasing.ReferencesBannard, C. and C. Callison-Burch.
2005.
Paraphrasing with bilingualparallel corpora.
Proceedings of ACL-2005.Barzilay, R. and K. McKeown.
2001.
Extracting paraphrases from aparallel corpus.
Proceedings of ACL/EACL-2001.Brown, P. F., S. A. Della Pietra, V. J. Della Pietra, R. L. Mercer.1993.
The mathematics of machine translation: Parameter estima-tion.
Computational Linguistics, 19(2): 263?311, 1993.Cohen, J.
1960.
A coefficient of agreement for nominal scales.
Edu-cation and Psychological Measurement, 43(6):37?46.Diab, M. and P. Resnik.
2002.
An unsupervised method for wordsense tagging using parallel corpora.
Proceedings of ACL-2002.DUC.
2001?2005.
Document Understanding Conferences.Hermjakob, U., A. Echihabi, and D. Marcu.
2002.
Natural languagebased reformulation resource and web exploitation for questionanswering.
Proceedings of TREC-2002.Hovy, E, C.Y.
Lin, and L. Zhou.
2005.
Evaluating DUC 2005 usingbasic elements.
Proceedings of DUC-2005.Hovy, E., C.Y.
Lin, L. Zhou, and J. Fukumoto.
2005a.
Basic Ele-ments.
http://www.isi.edu/~cyl/BE.Lin, C.Y.
2001. http://www.isi.edu/~cyl/SEE.Lin, C.Y.
and E. Hovy.
2003.
Automatic evaluation of summariesusing n-gram co-occurrence statistics.
Proceedings of the HLT-2003.Miller, G.A., R. Beckwith, C. Fellbaum, D. Gross, and K. J. Miller.1990.
Introduction to WordNet: An on-line lexical database.
Inter-national Journal of Lexicography, 3(4): 235?245.Nenkova, A. and R. Passonneau.
2004.
Evaluating content selection insummarization: the pyramid method.
Proceedings of the HLT-NAACL 2004.Och, F. J. and H. Ney.
2003.
A systematic comparison of variousstatistical alignment models.
Computational Linguistics, 29(1): 19-?51, 2003.Och, F. J. and H. Ney.
2004.
The alignment template approach tostatistical machine translation.
Computational Linguistics, 30(4),2004.Pang, B. , K. Knight and D. Marcu.
2003.
Syntax-based alignment ofmultiple translations: extracting paraphrases and generating newsentences.
Proceedings of HLT/NAACL-2003.Papineni, K., S. Roukos, T. Ward, and W. J. Zhu.
IBM research reportBleu: a method for automatic evaluation of machine translationIBM Research Division Technical Report, RC22176, 2001.Trick, M. A.
1997.
A tutorial on dynamic program-ming.http://mat.gsia.cmu.edu/classes/dynamic/dynamic.html.Van Halteren, H. and S. Teufel.
2003.
Examining the consensus be-tween human summaries: initial experiments with factoid analysis.Proceedings of HLT-2003.454
