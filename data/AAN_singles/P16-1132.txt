Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1393?1402,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsA Search-Based Dynamic Reranking Model for Dependency ParsingHao Zhou?
Yue Zhang?
Shujian Huang?
Junsheng Zhou?Xin-Yu Dai?
Jiajun Chen?
?State Key Laboratory for Novel Software Technology, Nanjing University, China?Singapore University of Technology and Design, Singapore?Nanjing Normal University, Chinazhouh@nlp.nju.edu.cn, yue zhang@sutd.edu.sg{huangshujian,daixinyu,chenjj}@nju.edu.cn, zhoujs@njnu.edu.cnAbstractWe propose a novel reranking method toextend a deterministic neural dependencyparser.
Different to conventional k-bestreranking, the proposed model integratessearch and learning by utilizing a dynamicaction revising process, using the rerank-ing model to guide modification for thebase outputs and to rerank the candidates.The dynamic reranking model achievesan absolute 1.78% accuracy improvementover the deterministic baseline parser onPTB, which is the highest improvement byneural rerankers in the literature.1 IntroductionNeural network models have recently been ex-ploited for dependency parsing.
Chen and Man-ning (2014) built a seminal model by replacingthe SVM classifier at the transition-based Malt-Parser (Nivre et al, 2007) with a feed-forwardneural network, achieving significantly higher ac-curacies and faster speed.
As a local and greedyneural baseline, it does not outperform the bestdiscrete-feature parsers, but nevertheless demon-strates strong potentials for neural network modelsin transition-based dependency parsing.Subsequent work aimed to improve the modelof Chen and Manning (2014) in two main direc-tions.
First, global optimization learning and beamsearch inference have been exploited to reduce er-ror propagation (Weiss et al, 2015; Zhou et al,2015).
Second, recurrent neural network modelshave been used to extend the range of neural fea-tures beyond a local window (Dyer et al, 2015;Ballesteros et al, 2015).
These methods give ac-curacies that are competitive to the best results inthe literature.John loves Mary(a) Base TreeJohn loves Mary(b) Tree 1John loves Mary(c) Tree 21.0 S 1.0 S 0.4 S 0.5 L 0.9 L0.3 L 1.0 S 0.5 LRevise 1Tree 1Tree 2Base TreeRevise 21.0 S 1.0 S0.3 L 1.0 S 0.5 R1.0 S 1.0 S(d) 2-step action revising process for sentence ?John lovesMary?.
Numbers before actions are the probabilities forthat action.Figure 1: Example action revising process.
S, L,R stand for the SHIFT, LEFT, RIGHT actions, re-spectively (Section 2).Another direction to extend a baseline parseris reranking (Collins and Koo, 2000; Charniakand Johnson, 2005; Huang, 2008).
Recently,neural network models have been used to con-stituent (Socher et al, 2013; Le et al, 2013)and dependency (Le and Zuidema, 2014; Zhuet al, 2015) parsing reranking.
Compared withrerankers that rely on discrete manual features,neural network rerankers can potentially capturemore global information over whole parse trees.Traditional rerankers are based on chart parsers,which can yield exact k-best lists and forests.For reranking, this is infeasible for the transition-based neural parser and neural reranker, which1393have rather weak feature locality.
In addition, k-best lists from the baseline parser are not necessar-ily the best candidates for a reranker.
Our prelim-inary results show that reranking candidates canbe constructed by modifying unconfident actionsin the baseline parser output, and letting the base-line parser re-decode the sentence from the mod-ified action.
In particular, revising two incorrectactions of the baseline parser yields oracle with97.79% UAS, which increases to 99.74% by revis-ing five actions.
Accordingly, we design a novelsearch-based dynamic reranking algorithm by re-vising baseline parser outputs.For example, the sentence: ?John loves Mary?,the baseline parser generates a base tree (Figure1a) using 5 shift-reduce actions (Figure 1d) of Sec-tion 2.
The gold parse tree can be obtained by a2-step action revising process:Base TreeRevise 1?????
Tree 1Revise 2?????
Tree 2As shown in Figure 1d, we first revise the leastconfident action S of the base tree, running thebaseline parser again from the revised action toobtain tree 1.
This corrects the John x loves de-pendency arc.
Then we obtain the gold parsingtree (tree 2) by further revising the least confidentaction in tree 1 on the second action sequence.Rather than relying on the baseline modelscores alone for deciding the action to re-vise (static search), we build a neural networkmodel to guide which actions to revise, as wellas to rerank the output trees (dynamic search).The resulting model integrates search and learn-ing, yielding the minimum amount of candidatesfor the best accuracies.
Given the extensively fastspeed of the baseline parser, the reranker can beexecuted with high efficiency.Our dynamic search reranker has two main ad-vantages over the static one: the first is train-ing diversity, the dynamic reranker searches overmore different structurally diverse candidate trees,which allows the reranker to distinguish candi-dates more easily; the second is reranking oracle,with the guidance of the reranking model, the dy-namic reranker has a better reranking oracle com-pared to the static reranker.On WSJ, our dynamic reranker achieved94.08% and 93.61% UAS on the development andtest sets, respectively, at a speed of 16.1 sentencesper second.
It yields a 0.44% accuracy improve-ment (+1.78%) from the same number of candi-????
?x h oact olabelW1 W2 W3OutputFigure 2: Hierarchical neural parsing model.dates, compared to a static reranker (+1.34%), ob-taining the largest accuracy improvement amongrelated neural rerankers.2 Baseline Dependency ParserTransition-based dependency parsers scan an in-put sentence from left to right, performing a se-quence of transition actions to predict its parsetree (Nivre, 2008).
We employ the arc-standardsystem (Nivre et al, 2007), which maintainspartially-constructed outputs using a stack, andorders the incoming words in the sentence in aqueue.
Parsing starts with an empty stack and aqueue consisting of the whole input sentence.
Ateach step, a transition action is taken to consumethe input and construct the output.Formally, a parsing state is denoted as ?j, S, L?,where S is a stack of subtrees [.
.
.
s2, s1, s0], j isthe head of the queue (i.e.
[ q0= wj, q1= wj+1?
?
?
]), and L is a set of dependency arcs that hasbeen built.
At each step, the parser chooses one ofthe following actions:?
SHIFT (S): move the front word wjfrom thequeue onto the stacks.?
LEFT-l (L): add an arc with label l betweenthe top two trees on the stack (s1?
s0), andremove s1from the stack.?
RIGHT-l (R): add an arc with label l betweenthe top two trees on the stack (s1?
s0), andremove s0from the stack.Given the sentence ?John loves Mary?, the goldstandard action sequence is S, S, L, S, R.2.1 ModelChen and Manning (2014) proposed a determinis-tic neural dependency parser, which rely on denseembeddings to predict the optimal actions at eachstep.
We propose a variation of Chen and Manning1394(2014), which splits the output layer into two hi-erarchical layers: the action layer and dependencylabel layer.
The hierarchical parser determines aaction in two steps, first deciding the action type,and then the dependency label (Figure 2).At each step of deterministic parsing, the neuralmodel extracts n atomic features from the parsingstate.
We adopt the feature templates of Chen andManning (2014).
Every atomic feature is repre-sented by a feature embedding ei?
Rd, An inputlayer is used to concatenate the n feature embed-dings into a vector x = [e1; e2.
.
.
en], where x ?Rd?n.
Then x is mapped to a dh-dimensional hid-den layer h by a mapping matrix W1?
Rdh?d?nand a cube activation function for feature combi-nation:h = (W1x+ b1)3(1)Our method is different from Chen and Man-ning (2014) in the output layer.
Given the hiddenlayer h, the action type output layer oactand thelabel output layer olabel(ai) of the action type aiare computed asoact= W2h (2)olabel(ai) = Wi3h , (3)Where W2?
Rda?dhis the mapping matrix fromthe hidden layer to the action layer, and dais thenumber of action types.
Wi3?
Rdlabel?dhis themapping matrix from the hidden layer to the cor-responding label layer, dlabelis the number of de-pendency labels.The probability of a labeled action yi,jgiven itshistory Acts and input x is computed as:p(yi,j| x,Acts)= p(ai| x,Acts)?
p(lj| x,Acts, ai) (4)wherep(ai| x,Acts) =eoiact?dak=1eokact(5)p(lj| x,Acts, ai) =eojlabel(ai)?dlabelk=1eoklabel(ai), (6)Here aiis the ithaction in the action layer, and ljis the jthlabel in the label layer for ai.In training, we use the cross-entropy loss tomaximum the probability of training data A:L(?)
= ?
?yi,j?Alog p(yi,j| x,Acts) (7)Experiments show that our hierarchical neuralparser is both faster and slightly accurate than theoriginal neural parser.3 Reranking ScorerWe adopt the recursive convolutional neural net-work (RCNN) of Zhu et al (2015) for scoring fulltrees.
Given a dependency subtree rooted at h,ci(0 < i ?
L) is the ithchild of h. The de-pendency arc (h, ci) is represented by:zi= tanh(W(h,ci)pi) , (8)wherepi= wh?
xci?
d(h,ci)(9)Here pi?
Rnis the concatenation of head wordembedding wh, child phrase representation xciand the distance embeddings d(h,ci).
W(h,ci)?Rm?nis a linear composition matrix, which de-pends on the POS tags of h and ci.
The sub-tree phrase representation xhare computed usinga max-pooling function on rows, over the matrixof arc representations Zh.Zh= [z1, z2, .
.
.
, zL] (10)xhj= maxiZhj,i, 0 < j < m (11)The subtree with the head h is scored by:score(h) =L?i=1vh,cizi(12)Here, vh,ciis the score vector, which is a vector ofparameters that need to be trained.
The score ofthe whole dependency tree y is computed as:st(x, y,?)
=?w?yscore(w), (13)where w is the node in tree y and ?
denotes theset of parameters in the network.4 Search-based Dynamic Reranking forDependency ParsingUsing the hierarchical parser of Section 2 as thebaseline parser, we propose a search-based dy-namic reranking model, which integrates searchand learning by searching the reranking candidatesdynamically, instead of limiting the scope to afixed k-best list.
The efficiency of the rerankingmodel is guaranteed by 3 properties of the base-line parser, namely revising efficiency, probabilitydiversity and search efficiency.1395Revising Depth UAS LAS0 92.28 91.151 95.76 94.422 97.79 96.633 98.77 97.554 99.39 98.155 99.74 98.47Table 1: Oracle of the baseline parser after revis-ing actions.
Revising depth is the maximum num-ber of revised actions for one sentence.Action Type Num Average ProbabilityGoldShift 39194 99.38%Right 19477 98.90%Left 19556 99.61%IncorrectShift 968 84.96%Right 746 85.88%Left 338 85.03%Table 2: Average action probabilities.4.1 Properties of the Baseline ParserTo demonstrate the above three properties, wegive some preliminary results for the baseline.
Toparse the 1,695 sentences in Section 22 of WSJ,our baseline parser needs to perform 78,227 shift-reduce actions.
During the process, if we correctevery encountered incorrectly determined actionand let the baseline parser re-decode the sentencefrom the point, we need to revise 2,052 actions, av-eraging 1.2 actions per sentence.
In other words,the baseline parser can parse the 1,695 sentencescorrectly with 2,052 action being revised.Note that the revise operation is required tochange the action type (i.e.
S, L).
After revisingthe action type, the optimal dependency label willbe chosen for parsing by the hierarchical baselineparser.
We only modify the action type in the re-vising process.
Thus the modified trees are alwaysstructurally different instead of only with differentdependency labels compared to the original one,which guarantees structured diversity.Revising Efficiency It can be seen from Table 1that revising one incorrect action results in 3.5%accuracy improvement.
We obtain a 99.74% UASafter a maximum 5 depth revising.
Although weonly revise the action type, the LAS goes up withthe UAS.
The property of revising efficiency sug-gests that high quality tree candidates can be foundwith a small number of changes.Probability Diversity Actions with lower prob-abilities are more likely to be incorrect.
We com-pute the average probabilities of gold and incor-rect actions in parsing the section 22 of WSJ (Ta-ble 2), finding that most gold actions have veryhigh probabilities.
The average probabilities ofthe gold actions is much higher than that of theincorrectly predicted ones, indicating that revisingactions with lower probabilities can lead to bettertrees.Search Efficiency The fast speed of the baselineparser allows the reranker to search a large num-ber of tree candidates efficiently.
With the graphstack trick (Goldberg et al, 2013), the rerankeronly needs to perform partial parsing to obtain newtrees.
This enables a fast reranker in theory.4.2 Search StrategyGiven an output sequence of actions by the base-line parser, we revise the action with the lowestprobability margin, and start a new branch by tak-ing a new action at this point.
The probability mar-gin of an action a is computed as: p(amax)?p(a),where amaxis the action taken by the baseline,which has the highest model probability.
a is takeninstead of amaxfor this branch, and the baselineparser is executed deterministically until parsingfinishes, thus yielding a new dependency tree.
Werequire that the action type must change in therevision and the most probable dependency labelamong all for the revised action type will be used.Multiple strategies can be used to search forthe revised reranking process.
For example, oneintuitive strategy is best-first, which modifies theaction with the lowest probability margin amongall sequences of actions constructed so far.
Start-ing from the original output of the baseline parser,modifying the action with the lowest probabilitymargin results in a new tree.
According to thebest-first strategy, the action with the lowest prob-ability margin in the two outputs will be revisednext to yield the third output.
The search repeatsuntil k candidates are obtained, which are used ascandidates for reranking.The best-first strategy, however, does not con-sider the quality of the output, which is like agreedy process.
A better candidate ( with higherF1 score) is more likely to take us to the gold tree.With the best-first strategy, we revise one tree ateach time.
If the selected tree is not the optimalone, the revised tree will be less likely the goldone.
Revising a worse output is less likely to gen-erate the gold parse tree compared with revisinga relatively better output.
Our preliminary experi-1396ments confirms this intuition.
As a result, we takea beam search strategy, which uses a beam to holdb outputs to modify.For each tree in beam search, most f actionswith the lowest probability margin are modified,leading to b ?
f new trees.
Here, b is the beamsize, f is the revising factor.
From these trees, theb best are put to the beam for the next step.
Searchstarts with the beam containing only the originalbase parse, and repeats for l steps, where l is calledthe revising depth.
The best tree will be selectedfrom all the trees constructed.
The search processfor example in Figure 1 is illustrated in Figure 3,in which b = 1, f = 3 and l = 2.At each iteration, the b best candidates can bedecided by the baseline parser score alone, whichis the product of the probability of each action.
Wecall this the static search reranking.
As mentionedin the introduction, the baseline model score mightnot be the optimal criteria to select candidates forreranking, since they may not reflect the best or-acle or diversity.
We introduce a dynamic searchstrategy instead, using the reranking model to cal-culate heuristic scores for guiding the search.4.3 Search-Based Dynamic RerankingDoppa et al (2013) propose that structured-prediction by learning guide search should main-tain two different scoring functions, a heuristicfunction for guiding search and a cost functionfor obtaining the best output.
Following Doppaet al (2013), we use the RCNN in Section 3 toyield two different scores, namely a heuristic scorest(x, y,?h) to guide the search of revising, and acost score st(x, y,?c) to select the best tree out-put.Denote b(i) as the beam at i-th step of search,k-best candidates in the beam of i+ 1 step is:b(i+ 1) = argKc?c(i)(st(x, c,?h) + sb(x, c)), (14)where c(i) denotes the set of newly constructedtrees by revising trees in b(i), sb(x, c) is the base-line model score and argK leaves the k best can-didate trees to the next beam.
Finally, the outputtree yiof reranking is selected from all searchedtrees C in the revising processyi= arg maxc?C(st(x, c,?c) + sb(x, c)) (15)Interpolated Reranker In testing, we alsoadopt the popular mixture reranking strat-egy (Hayashi et al, 2013; Le and Mikolov, 2014),Algorithm 1: Training Algorithm for theSearch-Based Dynamic Reranking.Input: Sentence x, Gold Trees yOutput: ?h, ?cfor iter?
1 to N doDh= [];Dk= [];foreach (x, y) ?
(x, y) dobestHScoreT = null;bestCScoreT = null;bestUAST = null;initTree = BASELINEPARSE(x);b1= [initTree];b2= [];for d?
1 to depth doforeach t ?
b1dorevisedActs = SEEK (t);revisedTrees = REVISE (t,revisedActs);bestK = SORT (revisedTrees, ?h)b2.ADD (bestK);bestHScoreT = MAXSCORE(bestHScoreT, revisedTrees, ?h);bestCScoreT = MAXSCORE(bestCScoreT, revisedTrees, ?c);bestUAST = MAXUAS (bestUAST,revisedTrees, y)b1= b2;b2= [];Dh.ADD (x, bestUAST, bestTScoreT);Dc.ADD (x, y, bestCScoreT);UPDATE(Dh, ?h);UPDATE(Dc, ?c);which obtains better reranking performance by alinear combination of the reranking score and thebaseline model score.yi= arg maxy??(xi)(?
(st(xi, y,?c) + st(x, y,?h))+ (1?
?
)sb(xi, y))(16)Here yiis the final output tree for a sentencexi; ?
(xi) returns all the trees candidates of the dy-namic reranking; ?
?
[0, 1] is a hyper-parameter.4.4 TrainingAs k-best neural rerankers (Socher et al, 2013;Zhu et al, 2015), we use the max-margin cri-terion to train our model in a stage-wise man-ner (Doppa et al, 2013).
Given training data Dc= (xi, yi, y?i)Ni=1, where xiis the sentence, y?iis theoutput tree with highest cost score and yiis thecorresponding gold tree, the final training objec-tive is to minimize the loss function J(?c), plus a1397S S S L L S S L S LS S R R SS S S L R ...beam(0) LActionCandidates0.1R 0.1R 1.0R 0.06 .35.24.9S S L S Lbeam(1)S S L S Rbeam(2)Tree CandidatesFigure 3: The beam search revising process of the example in Figure 1 with b = 1, f = 3 and l = 2l2-regularization:J(?c) =1|Dc|?
(xi,yi,y?i)?Dcri(?c) +?2||?c||(17)ri(?c) = max(0, st(xi, y?i,?c)+ ?
(yi, y?i)?
st(xi, yi,?c))(18)Here, ?cis the model, st(xi, yi,?c) is the costreranking score for yi.?
(yi, y?i) =?d?y?i?1{d /?
yi} (19)?
(yi, y?i) is the structured margin loss between yiand y?i, measured by counting the number of incor-rect dependency arcs in the tree (Goodman, 1998;Zhu et al, 2015).Given training data Dh= (xi, y?i, y?
?i)Ni=1for theheuristic score model, the training objective is tominimize the loss between the tree with the bestUAS y?iand the tree with the best heuristic rerank-ing score y?
?i.J(?h) =1|Dh|?(xi,y?i,y?
?i)?Dhri(?h) +?2||?h||(20)ri(?h) = max(0, st(xi, y??i,?h))?
st(xi, y?i,?h)(21)The detailed training algorithm is given by Al-gorithm 1.
AdaGrad (Duchi et al, 2011) updatingwith subgradient (Ratliff et al, 2007) and mini-batch is adopted for optimization.5 Experiments5.1 Set-upOur experiments are performed using the EnglishPenn Treebank (PTB; Marcus et al, (1993)).
Wefollow the standard splits of PTB3, using sections2-21 for training, section 22 for development andsection 23 for final testing.
Following prior workon reranking, we use Penn2Malt1to convert con-stituent trees to dependency trees.
Ten-fold POSjackknifing is used in the training of the baselineparser.
We use the POS-tagger of Collins (2002) toassign POS automatically.
Because our rerankingmodel is a dynamic reranking model, which gen-erates training instances during search, we train 10baseline parsing models on the 10-fold jackknifingdata, and load the baseline parser model dynami-cally for reranking training .We follow Chen and Manning (2014), using theset of pre-trained word embeddings with a dictio-nary size of 13,0002from Collobert et al (2011).The word embeddings were trained on the entireEnglish Wikipedia, which contains about 631 mil-lion words.5.2 Hyper-parametersThere are two different networks in our system,namely a hierarchical feed-forward neural net-work for the baseline parsing and a recursive con-volution network for dynamic reranking.
Thehyper-parameters of the hierarchical parser are setas described by Chen and Manning (2014), withthe embedding size d = 50, the hidden layer sizedh= 300, the regularization parameter ?
= 10?8,the initial learning rate of Adagrad ?
= 0.01 andthe batch size b = 100,000.
We set the hyper-parameters of the RCNN as follows: word embed-ding size dwrnn= 25, distance embedding size ddrnn= 25, initial learning rate of Adagrad ?rnn= 0.1,regularization parameter ?rnn= 10?4, margin lossdiscount ?
= 0.1 and revising factor f = 8.5.3 The Hierarchical Neural ParserShown in Table 3, the proposed hierarchical baseparser is 1.3 times faster, and obtains a slight ac-curacy improvement (Table 3) upon the parser ofChen and Manning (2014).
The reason for the1http://stp.lingfil.uu.se/ nivre/research/Penn2Malt.html2http://ronan.collobert.com/senna/1398Parserdev testSpeedUAS LAS UAS LAShiero 92.28 91.15 91.83 90.76 884.7original 92.00 90.89 91.67 90.62 682.3Table 3: Performance comparison between the hi-erarchical and original neural parsers.
Speed: sen-tences per second.Beam Size 1 2 4 8UAS 93.38 93.45 93.81 93.51Oracle 96.95 97.29 97.80 97.81K 22.57 37.16 65.8 118.7Table 4: Accuracies of the revising reranker withdifferent beam sizes on the development set.speed gain is that smaller output layer leads to lesscomputation of mapping from the hidden layer tothe output layer in neural networks (Morin andBengio, 2005; Mnih and Hinton, 2009).5.4 Development TestsFor the beam search dynamic reranking model, theselection of beam size b and revising depth l affectthe accuracy and efficiency of the reranker.
Wetune the values on the development set.Beam Size A proper beam size balances effi-ciency and accuracy in the search process.
Thereranking accuracies with different beam sizes arelisted in Table 4.
Here, the oracle is the best UASamong searched trees during reranking.
K is thenumber of searched candidate trees in testing.
TheUAS and parsing oracle both go up with increas-ing the beam size.
Reranking with beam size = 4gives the best development performance.
We setthe final beam size as 4 in the next experiments.Revising Depth As shown in Table 5, with re-vising depth increasing from 1 to 3, the rerankerobtains better parsing oracle.
The depth of 3 givesthe best UAS 93.81% on the development set.The parsing oracle stops improving with deeperrevised search.
This may because in the fourthsearch step, the high quality trees begin to fallout the beam, resulting in worse output candi-dates, which make the revising step yield less ora-cle gains.
We set the search depth as 3 in the nextexperiments.Integrating Search and Learning Shown inTable 6, the dynamic and static rerankers bothachieve significant accuracy improvements overthe baseline parser.
The dynamic reranker givesRevising Depth 1 2 3 4UAS 93.22 93.50 93.81 93.53Oracle 96.31 97.57 97.80 97.81K 8.87 38.45 65.8 90.28Table 5: Accuracies of the revised reranker withdifferent revising depths on development set.Search Type UAS +UAS OracleDynamic 93.81 +1.53 97.80Static 93.29 +1.01 97.61Table 6: Comparing dynamic and the static search.much better improvement, although the oracle ofdynamic reranker is only 0.2% higher than thestatic one.
This demostrates the benefit of diver-sity.
The candidates are always the same for staticsearch, but the dynamic reranker searches morediverse tree candidates in different iterations oftraining.To further explore the impact of training diver-sity to dynamic reranking, we also compare thedynamic search reranker of training and testingwith different revising depth.
In Table 7, originis the results by training and testing with the samedepth d. Results of ts is obtained by training withd = 3, and testing with a smaller d. For example,a reranker with training d = 3 and testing d = 2achieves better performance than with training d =2 and testing d = 2.
The testing oracle of the for-mer reranker is lower than the later, yet the formerlearns more from the training instance, obtainingbetter parsing accuracies.
This again indicates thattraining diversity is very important besides the or-acle accuracy.Interpolated Reranker Finally, we mix thebaseline model score and the reranking score byfollowing Hayashi et al (2013) and Zhu et al(2015), and the mixture parameter ?
is optimizedby searching with the step size of 0.005.
With themixture reranking trick, the dynamic reranker ob-tains an accuracy of 94.08% (Table 8), with an im-provement of 0.28% on the development set.5.5 Final ResultsComparison with Dependency Rerankers InTable 9, we compare the search-based dynamicrerankers with a list of dependency rerankers.
Thereranking models of Hayashi et al (2013) andHayashi et al (2011) are forest reranking mod-els.
Le and Zuidema (2014) and Zhu et al (2015)are neural k-best reranking models.
Our dynamic1399Depth 1 2 3ordinaryUAS 93.22 93.50 93.81oracle 96.31 97.57 97.80tsUAS 93.59 93.79 93.81oracle 96.29 93.42 97.80Table 7: Accuracies of the revised reranker withdifferent revising depths on the development set.Type static dynamicw/o mixture 93.29 93.81w/ mixture 93.53 94.08Table 8: Effects of interpolated reranking.reranking model achieves the highest accuracy im-provement over the baseline parser on both the de-velopment and test sets.
We obtain the best perfor-mance on the development set.
Zhu et al (2015)achieved higher accuracy on the test set, but theyadopted a better baseline parser than ours, whichcould not be used in our dynamic reranker becauseit is not fast enough and will make our rerankerslow in practice.Comparing with Neural Dependency ParsersWe also compare parsing accuracies and speedswith a number of neural network dependencyparsers.
Dyer et al (2015) proposed a dependencyparser with stack LSTM; Zhou et al (2015) ap-plied the beam search for structured dependencyparsing.
Both achieved significant accuracy im-provements over the deterministic neural parser ofChen and Manning (2014).
Our dynamic searchreranker obtains a 93.61% UAS on the test set,which is higher than most of the neural parsers ex-cept Weiss et al (2015), who employ a structuredprediction model upon the neural greedy baseline,achieving very high parsing accuracy.5.6 Results on Stanford dependenciesWe also evaluate the proposed static and dynamicrerankers on Staford dependency treebank.
Themain results are consistent with CoNLL depen-dency treebank with the dynamic reranker achiev-ing a 0.41% accuracy improvement upon the staticreranker on test data.
But the parsing accuracyon Stanford dependency is not the state-of-the-art.We speculate that there may be two reasons.
First,the baseline parsing accuracy on Stanford depen-dencies is lower than CoNLL.
Second, all thehyper-parameters are tuned on the CoNLL data.RerankerUASdev testHayashi et al (2011) N/A 92.87 (+0.97)Hayashi et al (2013) N/A 93.12 (+0.62)Le and Zuidema (2014) N/A 93.12 (+1.09)(Zhu et al,2015)baseline 92.45 92.35reranking 93.50 (+1.05) 93.83 (+1.48)This work(CoNLL)baseline 92.28 91.83dynamic 94.08 (+1.80) 93.61 (+1.78)static 93.53 (+1.25) 93.17 (+1.34)Table 9: Comparison of dependency rerankers.6 Related WorkNeural Networks Reranking A line of workhas been proposed to explore reranking using neu-ral networks.
Socher et al (2013) first proposeda neural reranker using a recursive neural net-work for constituent parsing.
Le and Zuidema(2014) extended the neural reranker to dependencyparsing using a inside-outside recursive neuralnetwork (IORNN), which can process trees bothbottom-up and top-down.
Zhu et al (2015) pro-posed a RCNN method, which solved the prob-lem of modeling k-ary parsing tree in dependencyparsing.
The neural rerankers are capable of cap-turing global syntax features across the tree.
Incontrast, the most non-local neural parser withLSTM (Dyer et al, 2015) cannot exploit globalfeatures.
Different to previous neural rerankers,our work in this paper contributes on integrat-ing search and learning for reranking, instead ofproposing a new neural model.Forest Reranking Forest reranking (Huang,2008; Hayashi et al, 2013) offers a different wayto extend the coverage of reranking candidates,with computing the reranking score in the treesforests by decomposing non-local features withcube-pruning (Huang and Chiang, 2005).
In con-trast, the neural reranking score encodes the wholedependency tree, which cannot be decomposed forforest reranking efficiently and accurately.HC-Search Doppa et al (2013) proposed astructured prediction model with HC-Search strat-egy and imitation learning, which is closely re-lated to our work in spirit.
They used the completespace search (Doppa et al, 2012) for sequence la-beling tasks, and the whole search process haltsafter a specific time bound.
Different from them,we propose a dynamic parsing reranking modelbased on the action revising process, which is amulti-step process by revising the least confident1400Type System UAS SpeedNeuralZhou et al (2015) 93.28 14.3Dyer et al (2015)?
93.30 105Weiss et al (2015)?
93.99 N/AWeiss et al (2015) semi ?
94.26 N/APei et al (2015) 93.29 N/AChen et al (2015) 92.60 2.7Chen and Manning (2014) 92.00 1013This work dynamic 93.61 16.1Table 10: Comparison with neural parsers.
Speed:sentences per second.
?
: results are reported onStanford dependencies.
?
: results are run by our-self using their codes.SystemUASdev testbaseline 91.80 91.41dynamic 93.44 (+1.64) 92.95 (+1.57)static 93.09 (+1.29) 92.57 (+1.16)Table 11: Dynamic reranking results on Stanforddependencies.actions from the base output and the search stopsin a given revising depth.
The dynamic rerank-ing model concentrates on extending the train-ing diversity and testing oracle for parsing rerank-ing, which is built on the transition-based parsingframework.7 ConclusionIn this paper, we proposed a search-based dy-namic reranking model using a hierarchical neu-ral base parser and a recursive convolutional neu-ral score model.
The dynamic model is the firstreranker integrating search and learning for de-pendency parsing.
It achieves significant accuracyimprovement (+1.78%) upon the baseline deter-ministic parser.
With the dynamic search process,our reranker obtains a 0.44% accuracy improve-ment upon the static reranker.
The code of this pa-per can be downloaded from http://github.com/zhouh/dynamic-reranker.AcknowledgmentsWe would like to thank the anonymous reviewersfor their insightful comments.
Xin-Yu Dai is thecorresponding author of this paper.
This work wassupported by the Natural Science Foundation ofChina (61472183, 6130158, 61472191), the 863program via 2015AA015406 and Singapore Min-istratry of Education Tier 2 Grant T2MOE201301.ReferencesMiguel Ballesteros, Chris Dyer, and Noah A Smith.2015.
Improved transition-based parsing by model-ing characters instead of words with lstms.
In Pro-ceedings of the 2015 Conference on Empirical Meth-ods in Natural Language Processing.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminativereranking.
In Proceedings of the 43rd Annual Meet-ing on Association for Computational Linguistics,pages 173?180.
Association for Computational Lin-guistics.Danqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Empirical Methods in Natural LanguageProcessing (EMNLP).Xinchi Chen, Yaqian Zhou, Chenxi Zhu, Xipeng Qiu,and Xuanjing Huang.
2015.
Transition-based de-pendency parsing using two heterogeneous gated re-cursive neural networks.
In Proceedings of the 2015Conference on Empirical Methods in Natural Lan-guage Processing.Michael Collins and Terry Koo.
2000.
Dis-criminative reranking for natural language parsing.MACHINE LEARNING-INTERNATIONAL WORK-SHOP THEN CONFERENCE-, pages 175?182.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof the ACL-02 conference on Empirical methods innatural language processing-Volume 10, pages 1?8.Association for Computational Linguistics.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Janardhan Rao Doppa, Alan Fern, and Prasad Tade-palli.
2012.
Output space search for structured pre-diction.
In ICML.Janardhan Rao Doppa, Alan Fern, and Prasad Tade-palli.
2013.
Hc-search: Learning heuristics and costfunctions for structured prediction.
In AAAI, vol-ume 2, page 4.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Chris Dyer, Miguel Ballesteros, Wang Ling, AustinMatthews, and Noah A Smith.
2015.
Transition-based dependency parsing with stack long short-term memory.
In Proceedings of the 53th AnnualMeeting of the Association for Computational Lin-guistics.1401Yoav Goldberg, Kai Zhao, and Liang Huang.
2013.Efficient implementation of beam-search incremen-tal parsers.
In ACL (2), pages 628?633.Joshua Goodman.
1998.
Parsing inside-out.
arXivpreprint cmp-lg/9805007.Katsuhiko Hayashi, Taro Watanabe, Masayuki Asa-hara, and Yuji Matsumoto.
2011.
Third-ordervariational reranking on packed-shared dependencyforests.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 1479?1488.
Association for ComputationalLinguistics.Katsuhiko Hayashi, Shuhei Kondo, and Yuji Mat-sumoto.
2013.
Efficient stacked dependency pars-ing by forest reranking.
Transactions of the Associ-ation for Computational Linguistics, 1:139?150.Liang Huang and David Chiang.
2005.
Better k-bestparsing.
In Proceedings of the Ninth InternationalWorkshop on Parsing Technology, pages 53?64.
As-sociation for Computational Linguistics.Liang Huang.
2008.
Forest reranking: Discriminativeparsing with non-local features.
In ACL, pages 586?594.Quoc V Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
arXivpreprint arXiv:1405.4053.Phong Le and Willem Zuidema.
2014.
The inside-outside recursive neural network model for depen-dency parsing.
In Proceedings of the 2014 Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 729?739.
Associationfor Computational Linguistics.Phong Le, Willem Zuidema, and Remko Scha, 2013.Proceedings of the Workshop on Continuous VectorSpace Models and their Compositionality, chapterLearning from errors: Using vector-based composi-tional semantics for parse reranking, pages 11?19.Association for Computational Linguistics.Mitchell P Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: The penn treebank.
Compu-tational linguistics, 19(2):313?330.Andriy Mnih and Geoffrey E Hinton.
2009.
A scal-able hierarchical distributed language model.
InAdvances in neural information processing systems,pages 1081?1088.Frederic Morin and Yoshua Bengio.
2005.
Hierarchi-cal probabilistic neural network language model.
InProceedings of the international workshop on artifi-cial intelligence and statistics, pages 246?252.
Cite-seer.Joakim Nivre, Johan Hall, Jens Nilsson, AtanasChanev, G?ulsen Eryigit, Sandra K?ubler, SvetoslavMarinov, and Erwin Marsi.
2007.
Maltparser:A language-independent system for data-driven de-pendency parsing.
Natural Language Engineering,13(2):95?135.Joakim Nivre.
2008.
Algorithms for deterministic in-cremental dependency parsing.
Computational Lin-guistics, 34(4):513?553.Wenzhe Pei, Tao Ge, and Baobao Chang.
2015.
Aneffective neural network model for graph-based de-pendency parsing.
In Proc.
of ACL.Nathan D Ratliff, J Andrew Bagnell, and Martin AZinkevich.
2007.
(online) subgradient methods forstructured prediction.Richard Socher, John Bauer, Christopher D Manning,and Andrew Y Ng.
2013.
Parsing with composi-tional vector grammars.
In In Proceedings of theACL conference.
Citeseer.David Weiss, Chris Alberti, Michael Collins, and SlavPetrov.
2015.
Structured training for neural net-work transition-based parsing.
In Proceedings of the53th Annual Meeting of the Association for Compu-tational Linguistics.Hao Zhou, Yue Zhang, Shujian Huang, and JiajunChen.
2015.
A neural probabilistic structured-prediction model for transition-based dependencyparsing.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguisticsand the 7th International Joint Conference on Natu-ral Language Processing (Volume 1: Long Papers),pages 1213?1222, Beijing, China, July.
Associationfor Computational Linguistics.Chenxi Zhu, Xipeng Qiu, Xinchi Chen, and XuanjingHuang.
2015.
A re-ranking model for dependencyparser with recursive convolutional neural network.In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics.
Associ-ation for Computational Linguistics.1402
