Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 693?699,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA Piece of My Mind: A Sentiment Analysis Approachfor Online Dispute DetectionLu WangDepartment of Computer ScienceCornell UniversityIthaca, NY 14853luwang@cs.cornell.eduClaire CardieDepartment of Computer ScienceCornell UniversityIthaca, NY 14853cardie@cs.cornell.eduAbstractWe investigate the novel task of online dis-pute detection and propose a sentiment analy-sis solution to the problem: we aim to identifythe sequence of sentence-level sentiments ex-pressed during a discussion and to use themas features in a classifier that predicts theDISPUTE/NON-DISPUTE label for the dis-cussion as a whole.
We evaluate dispute de-tection approaches on a newly created corpusof Wikipedia Talk page disputes and find thatclassifiers that rely on our sentiment taggingfeatures outperform those that do not.
The bestmodel achieves a very promising F1 score of0.78 and an accuracy of 0.80.1 IntroductionAs the web has grown in popularity and scope, sohas the promise of collaborative information en-vironments for the joint creation and exchange ofknowledge (Jones and Rafaeli, 2000; Sack, 2005).Wikipedia, a wiki-based online encyclopedia, isarguably the best example: its distributed edit-ing environment allows readers to collaborate ascontent editors and has facilitated the productionof over four billion articles1of surprisingly highquality (Giles, 2005) in English alone since its de-but in 2001.Existing studies of collaborative knowledgesystems have shown, however, that the quality ofthe generated content (e.g.
an encyclopedia arti-cle) is highly correlated with the effectiveness ofthe online collaboration (Kittur and Kraut, 2008;Kraut and Resnick, 2012); fruitful collaboration,in turn, inevitably requires dealing with the dis-putes and conflicts that arise (Kittur et al, 2007).Unfortunately, human monitoring of the oftenmassive social media and collaboration sites to de-tect, much less mediate, disputes is not feasible.1http://en.wikipedia.orgIn this work, we investigate the heretofore noveltask of dispute detection in online discussions.Previous work in this general area has analyzeddispute-laden content to discover features corre-lated with conflicts and disputes (Kittur et al,2007).
Research focused primarily on cues de-rived from the edit history of the jointly createdcontent (e.g.
the number of revisions, their tem-poral density (Kittur et al, 2007; Yasseri et al,2012)) and relied on small numbers of manuallyselected discussions known to involve disputes.
Incontrast, we investigate methods for the automaticdetection, i.e.
prediction, of discussions involvingdisputes.
We are also interested in understandingwhether, and which, linguistic features of the dis-cussion are important for dispute detection.Drawing inspiration from studies of human me-diation of online conflicts (e.g.
Billings and Watts(2010), Kittur et al (2007), Kraut and Resnick(2012)), we hypothesize that effective methodsfor dispute detection should take into account thesentiment and opinions expressed by participantsin the collaborative endeavor.
As a result, wepropose a sentiment analysis approach for onlinedispute detection that identifies the sequence ofsentence-level sentiments (i.e.
very negative, neg-ative, neutral, positive, very positive) expressedduring the discussion and uses them as featuresin a classifier that predicts the DISPUTE/NON-DISPUTE label for the discussion as a whole.
Con-sider, for example, the snippet in Figure 1 from theWikipedia Talk page for the article on Philadel-phia; it discusses the choice of a picture for thearticle?s ?infobox?.
The sequence of almost exclu-sively negative statements provides evidence of adispute in this portion of the discussion.Unfortunately, sentence-level sentiment taggingfor this domain is challenging in its own rightdue to the less formal, often ungrammatical, lan-guage and the dynamic nature of online conver-sations.
?Really, grow up?
(segment 3) should6931-Emy111: I think everyone is forgetting that my previous image was thelead image for well over a year!
...> Massimo: I?m sorry to say so, but it is grossly over processed...2-Emy111: i?m glad you paid more money for a camera than I did.
con-grats... i appreciate your constructive criticism.
thank you.> Massimo: I just want to have the best picture as a lead for the article ...3-Emy111: Wow, I am really enjoying this photography debate... [so don?tmake assumptions you know nothing about.
]NN[Really, grow up.
]N[If youall want to complain about Photoshop editing, lets all go buy medium for-mat film cameras, shoot film, and scan it, so no manipulation is possible.
]O[Sound good?
]NN> Massimo: ...
I do feel it is a pity, that you turned out to be a sore loser...Figure 1: From the Wikipedia Talk page for the article?Philadelphia?.
Omitted sentences are indicated by ellipsis.Names of editors are in bold.
The start of each set of relatedturns is numbered; ?>?
is an indicator for the reply structure.presumably be tagged as a negative sentence asshould the sarcastic sentences ?Sounds good??
(inthe same turn) and ?congrats?
and ?thank you?
(in segment 2).
We expect that these, and other,examples will be difficult for the sentence-levelclassifier unless the discourse context of each sen-tence is considered.
Previous research on senti-ment prediction for online discussions, however,focuses on turn-level predictions (Hahn et al,2006; Yin et al, 2012).2As the first work thatpredicts sentence-level sentiment for online dis-cussions, we investigate isotonic Conditional Ran-dom Fields (CRFs) (Mao and Lebanon, 2007) forthe sentiment-tagging task as they preserve the ad-vantages of the popular CRF-based sequential tag-ging models (Lafferty et al, 2001) while provid-ing an efficient mechanism for encoding domainknowledge ?
in our case, a sentiment lexicon ?through isotonic constraints on model parameters.We evaluate our dispute detection approach us-ing a newly created corpus of discussions fromWikipedia Talk pages (3609 disputes, 3609 non-disputes).3We find that classifiers that employ thelearned sentiment features outperform others thatdo not.
The best model achieves a very promis-ing F1 score of 0.78 and an accuracy of 0.80 onthe Wikipedia dispute corpus.
To the best of ourknowledge, this represents the first computationalapproach to automatically identify online disputeson a dataset of scale.Additional Related Work.
Sentiment analysishas been utilized as a key enabling technique ina number of conversation-based applications.
Pre-vious work mainly studies the attitudes in spoken2A notable exception is Hassan et al (2010), which identi-fies sentences containing ?attitudes?
(e.g.
opinions), but doesnot distinguish them w.r.t.
sentiment.
Context information isalso not considered.3The talk page associated with each article records con-versations among editors about the article content and allowseditors to discuss the writing process, e.g.
planning and orga-nizing the content.meetings (Galley et al, 2004; Hahn et al, 2006) orbroadcast conversations (Wang et al, 2011) usingvariants of Conditional Random Fields (Lafferty etal., 2001) and predicts sentiment at the turn-level,while our predictions are made for each sentence.2 Data Construction: A Dispute CorpusWe construct the first dispute detection corpus todate; it consists of dispute and non-dispute discus-sions from Wikipedia Talk pages.Step 1: Get Talk Pages of Disputed Articles.Wikipedia articles are edited by different editors.If an article is observed to have disputes on itstalk page, editors can assign dispute tags to thearticle to flag it for attention.
In this research, weare interested in talk pages whose correspondingarticles are labeled with the following tags:DISPUTED, TOTALLYDISPUTED, DISPUTED-SECTION, TOTALLYDISPUTED-SECTION, POV.The tags indicate that an article is disputed, or theneutrality of the article is disputed (POV).We use the 2013-03-04 Wikipedia data dump,and extract talk pages for articles that are labeledwith dispute tags by checking the revision history.This results in 19,071 talk pages.Step 2: Get Discussions with Disputes.
Dis-pute tags can also be added to talk pages them-selves.
Therefore, in addition to the tags men-tioned above, we also consider the ?Request forComment?
(RFC) tag on talk pages.
According toWikipedia4, RFC is used to request outside opin-ions concerning the disputes.3609 discussions are collected with disputetags found in the revision history.
We furtherclassify dispute discussions into three subcate-gories: CONTROVERSY, REQUEST FOR COM-MENT (RFC), and RESOLVED based on the tagsfound in discussions (see Table 1).
The numbersof discussions for the three types are 42, 3484, and105, respectively.
Note that dispute tags only ap-pear in a small number of articles and talk pages.There may exist other discussions with disputes.Dispute Subcategory Wikipedia Tags on Talk pagesControversy CONTROVERSIAL, TOTALLYDISPUTED,DISPUTED, CALM TALK, POVRequest for Comment RFCResolved Any tag from above + RESOLVEDTable 1: Subcategory for disputes with corresponding tags.Note that each discussion in the RESOLVED class has morethan one tag.Step 3: Get Discussions without Disputes.
Like-wise, we collect non-dispute discussions from4http://en.wikipedia.org/wiki/Wikipedia:Requests_for_comment694pages that are never tagged with disputes.
We con-sider non-dispute discussions with at least 3 dis-tinct speakers and 10 turns.
3609 discussions arerandomly selected with this criterion.
The averageturn numbers for dispute and non-dispute discus-sions are 45.03 and 22.95, respectively.3 Sentence-level Sentiment PredictionThis section describes our sentence-level senti-ment tagger, from which we construct features fordispute detection (Section 4).Consider a discussion comprised of sequentialturns; each turn consists of a sequence of sen-tences.
Our model takes as input the sentencesx = {x1, ?
?
?
, xn} from a single turn, and out-puts the corresponding sequence of sentiment la-bels y = {y1, ?
?
?
, yn}, where yi?
O,O ={NN,N,O,P,PP}.
The labels in O representvery negative (NN), negative (N), neutral (O), pos-itive (P), and very positive (PP), respectively.Given that traditional Conditional RandomFields (CRFs) (Lafferty et al, 2001) ignore the or-dinal relations among sentiment labels, we chooseisotonic CRFs (Mao and Lebanon, 2007) forsentence-level sentiment analysis as they can en-force monotonicity constraints on the parametersconsistent with the ordinal structure and domainknowledge (e.g.
word-level sentiment conveyedvia a lexicon).
Concretely, we take a lexiconM =Mp?Mn, whereMpandMnare two sets of fea-tures (usually words) identified as strongly associ-ated with positive and negative sentiment.
Assume??
?,w?encodes the weight between label ?
andfeature w, for each feature w ?
Mp; then the iso-tonic CRF enforces ?
?
???
???,w??
???
?,w?.For example, when ?totally agree?
is observed intraining, parameter ?
?PP,totally agree?is likely toincrease.
Similar constraints are defined onMn.Our lexicon is built by combining MPQA (Wil-son et al, 2005), General Inquirer (Stone et al,1966), and SentiWordNet (Esuli and Sebastiani,2006) lexicons.
Words with contradictory senti-ments are removed.
We use the features in Table 2for sentiment prediction.Syntactic/Semantic Features.
We have two ver-sions of dependency relation features, the origi-nal form and a form that generalizes a word to itsPOS tag, e.g.
?nsubj(wrong, you)?
is generalizedto ?nsubj(ADJ, you)?
and ?nsubj(wrong, PRP)?.Discourse Features.
We extract the initial uni-gram, bigram, and trigram of each utterance as dis-Lexical Features Syntactic/Semantic Features- unigram/bigram - unigram with POS tag- number of words all uppercased - dependency relation- number of words Conversation FeaturesDiscourse Features - quote overlap with target- initial uni-/bi-/tri-gram - TFIDF similarity with target- repeated punctuations (remove quote first)- hedging phrases collected from Sentiment FeaturesFarkas et al (2010) - connective + sentiment words- number of negators - sentiment dependency relation- sentiment wordsTable 2: Features used in sentence-level sentiment predic-tion.
Numerical features are first normalized by standardiza-tion, then binned into 5 categories.course features (Hirschberg and Litman, 1993).Sentiment Features.
We gather connectives fromthe Penn Discourse TreeBank (Rashmi Prasad andWebber, 2008) and combine them with any senti-ment word that precedes or follows it as new fea-tures.
Sentiment dependency relations are the de-pendency relations that include a sentiment word.We replace those words with their polarity equiv-alents.
For example, relation ?nsubj(wrong, you)?becomes ?nsubj(SentiWordneg, you)?.4 Online Dispute Detection4.1 Training A Sentiment ClassifierDataset.
We train the sentiment classifier usingthe Authority and Alignment in Wikipedia Discus-sions (AAWD) corpus (Bender et al, 2011) on a 5-point scale (i.e.
NN, N, O, P, PP).
AAWD consistsof 221 English Wikipedia discussions with posi-tive and negative alignment annotations.
Annota-tors either label each sentence as positive, negativeor neutral, or label the full turn.
For instances thathave only a turn-level label, we assume all sen-tences have the same label as the turn.
We furthertransform the labels into the five sentiment labels.Sentences annotated as being a positive alignmentby at least two annotators are treated as very posi-tive (PP).
If a sentence is only selected as positiveby one annotator or obtains the label via turn-levelannotation, it is positive (P).
Very negative (NN)and negative (N) are collected in the same way.All others are neutral (O).
Among all 16,501 sen-tences in AAWD, 1,930 and 1,102 are labeled asNN and N. 532 and 99 of them are PP and P. Theother 12,648 are considered neutral.Evaluation.
To evaluate the performance of thesentiment tagger, we compare to two baselines.
(1) Baseline (Polarity): a sentence is predicted aspositive if it has more positive words than nega-tive words, or negative if more negative words areobserved.
Otherwise, it is neutral.
(2) Baseline(Distance) is extended from (Hassan et al, 2010).Each sentiment word is associated with the closest695Pos Neg NeutralBaseline (Polarity) 22.53 38.61 66.45Baseline (Distance) 33.75 55.79 88.97SVM (3-way) 44.62 52.56 80.84CRF (3-way) 56.28 56.37 89.41CRF (5-way) 58.39 56.30 90.10isotonic CRF 68.18 62.53 88.87Table 3: F1 scores for positive and negative alignment onWikipedia Talk pages (AAWD) using 5-fold cross-validation.In each column, bold entries (if any) are statistically signif-icantly higher than all the rest.
We also compare with anSVM and linear CRF trained with three classes (3-way).
Ourmodel based on the isotonic CRF produces significantly bet-ter results than all the other systems.second person pronoun, and a surface distance iscomputed.
An SVM classifier (Joachims, 1999) istrained using features of the sentiment words andminimum/maximum/average of the distances.We also compare with two state-of-the-artmethods that are used in sentiment prediction forconversations: (1) an SVM (RBF kernel) that isemployed for identifying sentiment-bearing sen-tences (Hassan et al, 2010), and (dis)agreementdetection (Yin et al, 2012) in online debates; (2)a Linear CRF for (dis)agreement identification inbroadcast conversations (Wang et al, 2011).We evaluate the systems using standard F1 onclasses of positive, negative, and neutral, wheresamples predicted as PP and P are positive align-ment, and samples tagged as NN and N are neg-ative alignment.
Table 3 describes the main re-sults on the AAWD dataset: our isotonic CRFbased system significantly outperforms the alter-natives for positive and negative alignment detec-tion (paired-t test, p < 0.05).4.2 Dispute DetectionWe model dispute detection as a standard bi-nary classification task, and investigate four majortypes of features as described below.Lexical Features.
We first collect unigram andbigram features for each discussion.Topic Features.
Articles on specific topics, suchas politics or religions, tend to arouse more dis-putes.
We thus extract the category informa-tion of the corresponding article for each talk page.We further utilize unigrams and bigrams ofthe category as topic features.Discussion Features.
This type of feature aimsto capture the structure of the discussion.
Intu-itively, the more turns or the more participantsa discussion has, the more likely there is adispute.
Meanwhile, participants tend to producelonger utterances when they make arguments.We choose number of turns, numberof participants, average number ofwords in each turn as features.
In addi-tion, the frequency of revisions made during thediscussion has been shown to be good indicatorfor controversial articles (Vuong et al, 2008), thatare presumably prone to have disputes.
Therefore,we encode the number of revisions thathappened during the discussion as a feature.Sentiment Features.
This set of features en-code the sentiment distribution and transition inthe discussion.
We train our sentiment taggingmodel on the full AAWD dataset, and run it onthe Wikipedia dispute corpus.Given that consistent negative senti-ment flow usually indicates an ongoingdispute, we first extract features fromsentiment distribution in the formof number/probability of sentimentper type.
We also estimate the sentimenttransition probability P (St?
St+1) fromour predictions, where Stand St+1are sentimentlabels for the current sentence and the next.
Wethen have features as number/portion ofsentiment transitions per type.Features described above mostly depict theglobal sentiment flow in the discussions.
We fur-ther construct a local version of them, since sen-timent distribution may change as discussion pro-ceeds.
For example, less positive sentiment can beobserved as dispute being escalated.
We thus spliteach discussion into three equal length stages, andcreate sentiment distribution and transition fea-tures for each stage.Prec Rec F1 AccBaseline (Random) 50.00 50.00 50.00 50.00Baseline (All dispute) 50.00 100.00 66.67 50.00Logistic Regression 74.76 72.29 73.50 73.94SVMLinear69.81 71.90 70.84 70.41SVMRBF77.38 79.14 78.25 80.00Table 4: Dispute detection results on Wikipedia Talk pages.The numbers are multiplied by 100.
The items in bold are sta-tistically significantly higher than others in the same column(paired-t test, p < 0.05).
SVM with the RBF kernel achievesthe best performance in precision, F1, and accuracy.Results and Error Analysis.
We experiment withlogistic regression, SVM with linear and RBF ker-nels, which are effective methods in multiple textcategorization tasks (Joachims, 1999; Zhang andJ.
Oles, 2001).
We normalize the features by stan-dardization and conduct a 5-fold cross-validation.Two baselines are listed: (1) labels are randomlyassigned; (2) all discussions have disputes.Main results for different classifiers are dis-played in Table 4.
All learning based methods696T1 T2 T3T4T5 T6 T7 T8 T9 T10 T11 T12 T13T14 T15 T16 T17 T1821012SentimentA B C D EFSentiment Flow in Discussion with Unresolved DisputeSample sentences (sentiment in parentheses)A: no, I sincerely plead with you... (N) If not, you are just wasting mytime.
(NN)B: I believe Sweet?s proposal... is quite silly.
(NN)C: Tell you what.
(NN) If you can get two other editors to agree...
I willshut up and sit down.
(NN)D: But some idiot forging your signature claimed that doing so wouldviolate.
(NN)...
Please go have some morning coffee.
(O)E: And I don?t like coffee.
(NN) Good luck to you.
(NN)F: Was that all?
(NN)...
I think that you are in error... (N)T1 T2 T3 T4 T5 T6 T7 T8 T9 T10 T11 T12 T13 T1421012SentimentA BCDE FSentiment Flow in Discussion with Resolved Dispute A: So far so confusing.
(NN)...B: ...
I can not see a rationale for the landrace having its own article...(N) With Turkish Van being a miserable stub, there?s no such rationale forforking off a new article... (NN)...C: I?ve also copied your post immediately above to that article?s talk pagesince it is a great ?nutshell?
summary.
(PP)D: Err.. how can the opposite be true... (N)E: Thanks for this, though I have to say some of the facts floating aroundthis discussion are wrong.
(P)F: Great.
(PP) Let?s make sure the article is clear on this.
(O)Figure 2: Sentiment flow for a discussion with unresolved dispute about the definition of ?white people?
(top) and a dis-cussion with resolved dispute on merging articles about van cat (bottom).
The labels {NN,N,O,P,PP} are mapped to{?2,?1, 0, 1, 2} in sequence.
Sentiment values are convolved by Gaussian smoothing kernel, and cubic-spline interpolation isthen conducted.
Different speakers are represented by curves of different colors.
Dashed vertical lines delimit turns.
Represen-tative sentences are labeled with letters and their sentiment labels are shown on the right.
For unresolved dispute (top), we seethat negative sentiment exists throughout the discussion.
Whereas, for the resolved dispute (bottom), less negative sentiment isobserved at the end of the discussion; participants also show appreciation after the problem is solved (e.g.
E and F in the plot).Prec Rec F1 AccLexical (Lex) 75.86 34.66 47.58 61.82Topic (Top) 68.44 71.46 69.92 69.26Discussion (Dis) 69.73 76.14 72.79 71.54Sentiment (Sentig+l) 72.54 69.52 71.00 71.60Top + Dis 68.49 71.79 70.10 69.38Top + Dis + Sentig77.39 78.36 77.87 77.74Top + Dis + Sentig+l77.38 79.14 78.25 80.00Lex + Top + Dis + Sentig+l78.38 75.12 76.71 77.20Table 5: Dispute detection results with different featuresets by SVM with RBF kernel.
The numbers are multi-plied by 100.
Sentigrepresents global sentiment features, andSentig+lincludes both global and local features.
The numberin bold is statistically significantly higher than other numbersin the same column (paired-t test, p < 0.05), and the italicentry has the highest absolute value.outperform the two baselines, and among them,SVM with the RBF kernel achieves the best F1score and accuracy (0.78 and 0.80).
Experimentalresults with various combinations of features setsare displayed in Table 5.
As it can be seen, senti-ment features obtains the best accuracy among thefour types of features.
A combination of topic, dis-cussion, and sentiment features achieves the bestperformance on recall, F1, and accuracy.
Specif-ically, the accuracy is significantly higher than allthe other systems (paired-t test, p < 0.05).After a closer look at the results, we find twomain reasons for incorrect predictions.
Firstly,sentiment prediction errors get propagated intodispute detection.
Due to the limitation of ex-isting general-purpose lexicons, some opinionateddialog-specific terms are hard to catch.
For exam-ple, ?I told you over and over again...?
stronglysuggests a negative sentiment, but no single wordshows negative connotation.
Constructing a lexi-con tuned for conversational text may improve theperformance.
Secondly, some dispute discussionsare harder to detect than the others due to differ-ent dialog structures.
For instance, the recalls fordispute discussions of ?controversy?, ?RFC?, and?resolved?
are 0.78, 0.79, and 0.86 respectively.We intend to design models that are able to cap-ture dialog structures in the future work.Sentiment Flow Visualization.
We visualize thesentiment flow of two disputed discussions in Fig-ure 2.
The plots reveal persistent negative sen-timent in unresolved disputes (top).
For the re-solved dispute (bottom), participants show grati-tude when the problem is settled.5 ConclusionWe present a sentiment analysis-based approachto online dispute detection.
We create a large-scale dispute corpus from Wikipedia Talk pages tostudy the problem.
A sentiment prediction modelbased on isotonic CRFs is proposed to output sen-timent labels at the sentence-level.
Experimentson our dispute corpus also demonstrate that clas-sifiers trained with sentiment tagging features out-perform others that do not.Acknowledgments We heartily thank the CornellNLP Group, the reviewers, and Yiye Ruan forhelpful comments.
We also thank Emily Ben-der and Mari Ostendorf for providing the AAWDdataset.
This work was supported in part by NSFgrants IIS-0968450 and IIS-1314778, and DARPADEFT Grant FA8750-13-2-0015.
The views andconclusions contained herein are those of the au-thors and should not be interpreted as necessarilyrepresenting the official policies or endorsements,either expressed or implied, of NSF, DARPA orthe U.S. Government.697ReferencesEmily M. Bender, Jonathan T. Morgan, Meghan Ox-ley, Mark Zachry, Brian Hutchinson, Alex Marin,Bin Zhang, and Mari Ostendorf.
2011.
Anno-tating social acts: Authority claims and alignmentmoves in wikipedia talk pages.
In Proceedings ofthe Workshop on Languages in Social Media, LSM?11, pages 48?57, Stroudsburg, PA, USA.
Associa-tion for Computational Linguistics.Matt Billings and Leon Adam Watts.
2010.
Under-standing dispute resolution online: using text to re-flect personal and substantive issues in conflict.
InElizabeth D. Mynatt, Don Schoner, Geraldine Fitz-patrick, Scott E. Hudson, W. Keith Edwards, andTom Rodden, editors, CHI, pages 1447?1456.
ACM.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sen-tiwordnet: A publicly available lexical resourcefor opinion mining.
In In Proceedings of the 5thConference on Language Resources and Evaluation(LREC06), pages 417?422.Rich?ard Farkas, Veronika Vincze, Gy?orgy M?ora, J?anosCsirik, and Gy?orgy Szarvas.
2010.
The conll-2010shared task: Learning to detect hedges and theirscope in natural language text.
In Proceedings ofthe Fourteenth Conference on Computational Natu-ral Language Learning ?
Shared Task, CoNLL ?10:Shared Task, pages 1?12, Stroudsburg, PA, USA.Association for Computational Linguistics.Michel Galley, Kathleen McKeown, Julia Hirschberg,and Elizabeth Shriberg.
2004.
Identifying agree-ment and disagreement in conversational speech:use of Bayesian networks to model pragmatic de-pendencies.
In ACL ?04: Proceedings of the 42ndAnnual Meeting on Association for ComputationalLinguistics, pages 669+, Morristown, NJ, USA.
As-sociation for Computational Linguistics.Jim Giles.
2005.
Internet encyclopaedias go head tohead.
Nature, 438(7070):900?901.Sangyun Hahn, Richard Ladner, and Mari Ostendorf.2006.
Agreement/disagreement classification: Ex-ploiting unlabeled data using contrast classifiers.In Proceedings of the Human Language Technol-ogy Conference of the NAACL, Companion Volume:Short Papers, pages 53?56, New York City, USA,June.
Association for Computational Linguistics.Ahmed Hassan, Vahed Qazvinian, and DragomirRadev.
2010.
What?s with the attitude?
: Identify-ing sentences with attitude in online discussions.
InProceedings of the 2010 Conference on EmpiricalMethods in Natural Language Processing, EMNLP?10, pages 1245?1255, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Julia Hirschberg and Diane Litman.
1993.
Empiricalstudies on the disambiguation of cue phrases.
Com-put.
Linguist., 19(3):501?530, September.Thorsten Joachims.
1999.
Advances in kernel meth-ods.
chapter Making Large-scale Support VectorMachine Learning Practical, pages 169?184.
MITPress, Cambridge, MA, USA.Quentin Jones and Sheizaf Rafaeli.
2000.
Time tosplit, virtually: discourse architecture and commu-nity building create vibrant virtual publics.
Elec-tronic Markets, 10:214?223.Aniket Kittur and Robert E. Kraut.
2008.
Harness-ing the wisdom of crowds in wikipedia: Qualitythrough coordination.
In Proceedings of the 2008ACM Conference on Computer Supported Coopera-tive Work, CSCW ?08, pages 37?46, New York, NY,USA.
ACM.Aniket Kittur, Bongwon Suh, Bryan A. Pendleton, andEd H. Chi.
2007.
He says, she says: Conflict andcoordination in wikipedia.
In Proceedings of theSIGCHI Conference on Human Factors in Comput-ing Systems, CHI ?07, pages 453?462, New York,NY, USA.
ACM.Robert E. Kraut and Paul Resnick.
2012.
Building suc-cessful online communities: Evidence-based socialdesign.
MIT Press, Cambridge, MA.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proceedings of the Eighteenth Inter-national Conference on Machine Learning, ICML?01, pages 282?289, San Francisco, CA, USA.
Mor-gan Kaufmann Publishers Inc.Yi Mao and Guy Lebanon.
2007.
Isotonic conditionalrandom fields and local sentiment flow.
In Advancesin Neural Information Processing Systems.Alan Lee Eleni Miltsakaki Livio Robaldo Ar-avind Joshi Rashmi Prasad, Nikhil Dinesh and Bon-nie Webber.
2008.
The penn discourse tree-bank 2.0.
In Proceedings of the Sixth Interna-tional Conference on Language Resources and Eval-uation (LREC?08), Marrakech, Morocco, may.
Eu-ropean Language Resources Association (ELRA).http://www.lrec-conf.org/proceedings/lrec2008/.Warren Sack.
2005.
Digital formations: It and newarchitectures in the global realm.
chapter Discoursearchitecture and very large-scale conversation, pages242?282.
Princeton University Press, Princeton, NJUSA.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith,and Daniel M. Ogilvie.
1966.
The General In-quirer: A Computer Approach to Content Analysis.MIT Press, Cambridge, MA.Ba-Quy Vuong, Ee-Peng Lim, Aixin Sun, Minh-TamLe, Hady Wirawan Lauw, and Kuiyu Chang.
2008.On ranking controversies in wikipedia: Models andevaluation.
In Proceedings of the 2008 Interna-tional Conference on Web Search and Data Mining,WSDM ?08, pages 171?182, New York, NY, USA.ACM.698Wen Wang, Sibel Yaman, Kristin Precoda, ColleenRichey, and Geoffrey Raymond.
2011.
Detection ofagreement and disagreement in broadcast conversa-tions.
In Proceedings of the 49th Annual Meeting ofthe Association for Computational Linguistics: Hu-man Language Technologies: Short Papers - Volume2, HLT ?11, pages 374?378, Stroudsburg, PA, USA.Association for Computational Linguistics.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2005.
Recognizing contextual polarity in phrase-level sentiment analysis.
In Proceedings of the Con-ference on Human Language Technology and Em-pirical Methods in Natural Language Processing,HLT ?05, pages 347?354, Stroudsburg, PA, USA.Association for Computational Linguistics.Taha Yasseri, R?obert Sumi, Andr?as Rung, Andr?as Kor-nai, and J?anos Kert?esz.
2012.
Dynamics of conflictsin wikipedia.
CoRR, abs/1202.3643.Jie Yin, Paul Thomas, Nalin Narang, and Cecile Paris.2012.
Unifying local and global agreement anddisagreement classification in online debates.
InProceedings of the 3rd Workshop in ComputationalApproaches to Subjectivity and Sentiment Analysis,WASSA ?12, pages 61?69, Stroudsburg, PA, USA.Association for Computational Linguistics.Tong Zhang and Frank J. Oles.
2001.
Text categoriza-tion based on regularized linear classification meth-ods.
Inf.
Retr., 4(1):5?31, April.699
