Word Clustering and Disambiguat ion Based on Co-occurrenceDataHang Li and Naok i  AbeTheory  NEC Laboratory,  Real World Comput ing  Partnershipc /o  C&C Media Research Laboratories.
,  NEC4-1-1 Miyazaki,  Miyamae-ku,  Kawasaki  216-8555, Japan{lihang,abe} @ccm.cl.nec.co.jpAbst rac tWe address the problem of clustering words (or con-structing a thesaurus) based on co-occurrence data,and using the acquired word classes to improve theaccuracy of syntactic disambiguation.
We view thisproblem as that of estimating a joint probability dis-tribution specifying the joint probabilities of wordpairs, such as noun verb pairs.
We propose an effi-cient algorithm based on the Minimum DescriptionLength (MDL) principle for estimating such a prob-ability distribution.
Our method is a natural ex-tension of those proposed in (Brown et al, 1992)and (Li and Abe, 1996), and overcomes their draw-backs while retaining their advantages.
We thencombined this clustering method with the disam-biguation method of (Li and Abe, 1995) to derive adisambiguation method that makes use of both auto-matically constructed thesauruses and a hand-madethesaurus.
The overall disambiguation accuracyachieved by our method is 85.2%, which comparesfavorably against he accuracy (82.4%) obtained bythe state-of-the-art disambiguation method of (Brilland Resnik, 1994).1 In t roduct ionWe address the problem of clustering words, or thatof constructing a thesaurus, based on co-occurrencedata.
We view this problem as that of estimating ajoint probability distribution over word pairs, speci-fying the joint probabilities of word pairs, such asnoun verb pairs.
In this paper, we assume thatthe joint distribution can be expressed in the fol-lowing manner, which is stated for noun verb pairsfor the sake of readability: The joint probability ofa noun and a verb is expressed as the product of thejoint probability of the noun class and the verb classwhich the noun and the verb respectively belong to,and the conditional probabilities of the noun and theverb given their respective classes.As a method for estimating such a probabilitydistribution, we propose an algorithm based on theMinimum Description Length (MDL) principle.
Ourclustering algorithm iteratively merges noun classesand verb classes in turn, in a bottom up fashion.
Foreach merge it performs, it calculates the increasein data description length resulting from mergingany noun (or verb) class pair, and performs themerge having the least increase in data descriptionlength, provided that the increase in data descrip-tion length is less than the reduction in model de-scription length.There have been a number of methods proposed inthe literature to address the word clustering problem(e.g., (Brown et al, 1992; Pereira et al, 1993; Li andAbe, 1996)).
The method proposed in this paper isa natural extension of both Li & Abe's and Brownet als methods, and is an attempt to overcome theirdrawbacks while retaining their advantages.The method of Brown et al which is based on theMaximum Likelihood Estimation (MLE), performsa merge which would result in the least reductionin (average) mutual information.
Our method turnsout to be equivalent to performing the merge withthe least reduction in mutual information, providedthat the reduction isbelow a certain threshold whichdepends on the size of the co-occurrence data andthe number of classes in the current situation.
Thismethod, based on the MDL principle, takes into ac-count both the fit to data and the simplicity of amodel, and thus can help cope with the over-fittingproblem that the MLE-based method of Brown et alfaces.The model employed in (Li and Abe, 1996) isbased on the assumption that the word distributionwithin a class is a uniform distribution, i.e.
everyword in a same class is generated with an equal prob-ability.
Employing such a model has the undesirabletendency of classifying into different classes thosewords that have similar co-occurrence patterns buthave different absolute frequencies.
The proposedmethod, in contrast, employs a model in which dif-ferent words within a same class can have differentconditional generation probabilities, and thus canclassify words in a way that is not affected by words'absolute frequencies and resolve the problem facedby the method of (Li and Abe, 1996).We evaluate our clustering method by using theword classes and the joint probabilities obtained by749it in syntactic disambiguation experiments.
Ourexperimental results indicate that using the wordclasses constructed by our method gives better dis-ambiguation results than when using Li & Abe orBrown et als methods.
By combining thesaurusesautomatically constructed by our method and anexisting hand-made thesaurus (WordNet), we wereable to achieve the overall accuracy of 85.2% for pp-attachment disambiguation, which compares favor-ably against he accuracy (82.4%) obtained using thestate-of-the-art method of (Brill and Resnik, 1994).2 P robab i l i ty  Mode lSuppose available to us are co-occurrence data overtwo sets of words, such as the sample of verbs andthe head words of their direct objects given in Fig.
1.Our goal is to (hierarchically) cluster the two setsof words so that words having similar co-occurrencepatterns are classified in the same class, and outputa thcsaurus for each set of words.winebeerbreadriceeat drink make0 3 10 5 14 0 24 0 0Figure 1: Example co-occurrence dataWe can view this problem as that of estimatingthe best probability model from among a class ofmodels of (probability distributions) which can giverise to the co-occurrence data.In this paper, we consider the following type ofprobability models.
Assume without loss of gener-ality that the two sets of words are a set of nounsA/" and a set of verbs ~;.
A partition T,~ of A/" is aset of noun-classes satisfying UC,,eT,,Cn = A/" andVCi, Cj E Tn, Ci CI Q = 0.
A partition Tv of 1;can be defined analogously.
We then define a proba-bility model of noun-verb co-occurrence by definingthe joint probability of a noun n and a verb v as theproduct of the joint probability of the noun and verbclasses that n and v belong to, and the conditionalprobabilities of n and v given their classes, that is,P(n, v) = P(Cn, Co).
P(nlC,-,) ?
P(vlCo), (1)where Cn and Cv denote the (unique) classes towhich n and v belong.
In this paper, we refer tothis model as the 'hard clustering model,' since it isbased on a type of clustering in which each word canbelong to only one class.
Fig.
2 shows an example ofthe hard clustering model that can give rise to theco-occurrence data in Fig.
1.P(vlOv)r=hP(nlCn).4  w ine0 .4\]0 bread 0.4?
rioemake0.10.1la(Cn,Cv) .
/Figure 2: Example hard clustering model3 Parameter  Est imat ionA particular choice of partitions for a hard clusteringmodel is referred to as a 'discrete' hard-clusteringmodel, with the probability parameters left to beestimated.
The values of these parameters can beestimated based on the co-occurrence data by theMaximum Likelihood Estimation.
For a given set ofco-occurrence data,S = {(nl, Yl), (r~2, V2),.
.
.
,  (r/m, Ore)},the maximum likelihood estimates of the parametersare defined as the values that maximize the followinglikelihood function with respect o the data:m m1"I P(ni, vi) = I I  ( P(nilC,~,).P(vilCo,).P(Cn,, Co,)).i=1 i=1It is easy to see that this is possible by setting theparameters as#(Cn, Co) = f(Cn, C~).,rnw e u v, P( lC ) = f(x)f(C~).Here, m denotes the entire data size, f(Cn, Co) thefrequency of word pairs in class pair (Cn, Co), f (x)the frequency of word x, and f(C~) the frequency ofwords in class C~.4 Mode l  Se lec t ion  Cr i te r ionThe question ow is what criterion should we employto select the best model from among the possiblemodels.
Here we adopt the Minimum DescriptionLength (MDL) principle.
MDL (Rissanen, 1989) isa criterion for data compression and statistical esti-mation proposed in information theory.In applying MDL, we calculate the code length forencoding each model, referred to as the 'model de-scription length' L(M), the code length for encoding750the given data through the model, referred to as the'data description length' L(SIM ) and their sum:L(M, S) = L(M) + L(SIM ).The MDL principle stipulates that, for both datacompression and statistical estimation, the bestprobability model with respect o given data is thatwhich requires the least total description length.The data description length is calculated asL(SIM ) = - ~ log /5(n, v),(n,v)e8where/5 stands for the maximum likelihood estimateof P (as defined in Section 3).We then calculate the model description length ask L(M) = ~ log m,where k denotes the number of free parameters in themodel, and m the entire data size3 In this paper,we ignore the code length for encoding a 'discretemodel,' assuming implicitly that they are equal forall models and consider only the description lengthfor encoding the parameters ofa model as the modeldescription length.If computation time were of no concern, we couldin principle calculate the total description length foreach model and select he optimal model in terms ofMDL.
Since the number of hard clustering modelsis of order O(N g ?
vV), where N and V denote thesize of the noun set and the verb set, respectively, itwould be infeasible to do so.
We therefore need todevise an efficient algorithm that heuristically per-forms this task.5 Clustering AlgorithmThe proposed algorithm, which we call '2D-Clustering,' iteratively selects a suboptimal MDL-model from among those hard clustering modelswhich can be obtained from the current model bymerging a noun (or verb) class pair.
As it turns out,the minimum description length criterion can be re-formalized in terms of (average) mutual information,and a greedy heuristic algorithm can be formulatedto calculate, in each iteration, the reduction of mu-tual information which would result from mergingany noun (or verb) class pair, and perform the merge1 We note that there are alternative ways of calculatingthe parameter description length.
For example, we can sep-arately encode the different ypes of probability parameters;the joint probabilities P(Cn, Cv), and the conditional prob-abilities P(nlCn ) and P(vlCv ).
Since these alternatives areapproximations of one another asymptotically, here we useonly the simplest formulation.
In the full paper, we plan tocompare the empirical behavior of the alternatives.having the least mutual information reduction, pro-vided that the reduction is below a variable threshold.2D-Clustering(S, b , b~)(S is the input co-occurrence data, and bn and byare positive integers.)1.
Initialize the set of noun classes Tn and the setof verb classes Tv as:Tn = {{n}ln E N'},To = {{v}lv E V},where Af and V denote the noun set and theverb set, respectively.2.
Repeat he following three steps:(a) execute Merge(S, Tn, Tv, bn) to update Tn,(b) execute Merge(S, Tv, Tn, b~) to update T,,(c) if T, and T~ are unchanged, go to Step 3.3.
Construct and output a thesaurus for nounsbased on the history of Tn, and one for verbsbased on the history of Tv.Next, we describe the procedure of 'Merge,' as itis being applied to the set of noun classes with theset of verb classes fixed.Merge(S, Tn, Tv, bn)1.
For each class pair in Tn, calculate the reduc-tion of mutual information which would resultfrom merging them.
(The details will follow.
)Discard those class pairs whose mutual informa-tion reduction (2) is not less than the thresholdof(k B -- ka)  ?
l ogm2.mwhere m denotes the total data size, ks thenumber of free parameters in the model beforethe merge, and \]?
A the number of free param-eters in the model after the merge.
Sort theremaining class pairs in ascending order withrespect o mutual information reduction.2.
Merge the first bn class pairs in the sorted list.3.
Output current Tn.We perform (maximum of) bn merges at step 2 forimproving efficiency, which will result in outputtingan at-most bn-ary tree.
Note that, strictly speaking,once we perform one merge, the model will changeand there will no longer be a guarantee that theremaining merges till remain justifiable from theviewpoint of MDL.Next, we explain why the criterion in terms ofdescription length can be reformalized in terms ofmutual information.
We denote the model beforea merge as Ms and the model after the merge as751MA.
According to MDL, MA should have the leastincrease in data description lengthdSndat = L(S\]MA) - L(S\[~IB) > O,and at the same time satisfies(k  B -- k A ) log m6Ldat < 2This is due to the fact that the decrease in modeldescription length equalsL(MB) L(MA) (kB -- kA)logm -- = > 0,2and is identical for each merge.In addition, suppose that )VIA is obtained by merg-ing two noun classes Ci and Cj in MB to a singlenoun class Cq.
We in fact need only calculate thedifference between description lengths with respectto these classes, i.e.,6 Ldat = - EC.,fiT,, EnEco,veC,,  l?g/b(n,v)+ EC~T~ EneC,,veC, log/5(n, v)+ EC~eT~ E,ec j ,oec~ log P(n, v).Now using the identityP(n,v) - P(") P?o) .p(c. ,co)- -  P(C,,) " P(Cv)_ P(C.,C~) P(n) .
P(v) -- p(c,o.p(cv ) ?we can rewrite the above as6Ldat = - ~C~eT~ f(Cij, Co) log P(c'i'co P(co).P(C~)+ Y~C.eT~ f(Ci, Co) log P(C,,C~) P(cd.P(c~)P(C1,Cv) + ~C~eTv f(Cs, Cv)log p(cD.P(c 0 .Thus, the quantity 6Laat is equivalent to the mutualinformation reduction times the data size.
~ We con-elude therefore that in our present context, a cluster-ing with the least data description length increase isequivalent to that with the least mutual informationdecrease.Canceling out P(Cv) and replacing the probabil-ities with their maximum likelihood estimates, weobtain1 _1(  C. "~6Ldat - -  "~ -- ~'~C.eT.
(f( " Co) -4- f (C j ,  Co))1o~/(c"co+l(  c~'cO~' f(cd+l(cD+ ~C~eT. "
f(C/,Cv) log .f(c,)+ EC.eT.
f (Ci ,  Co)log .f (~0) .
(2)2Average  mutua l  in fo rmat ion  between Tn and To is def ineda~/ P(C,~,Cv) \I(Tn'T?
)= E E ~P(Cn'Cv) l?gp(cn) .p(cv)  )"Cn ETn Ct, ETvTherefore, we need calculate only this quantity foreach possible merge at Step 1 of Merge.In our implementation f the algorithm, we firstload the co-occurrence data into a matrix, withnouns corresponding to rows, verbs to columns.When merging a noun class in row i and that inrow j (i < j), for each Co we add f(Ci,Co) andf(Cj,Co) obtaining f (C i j ,  Co), write f(Cij,Co) onrow i, move f(Czast,Co) to row j, and reduce thematrix by one row.By the above implementation, the worst case timecomplexity of the algorithm is O(N 3 ?
V + V 3 ?
N)where N denotes the size of the noun set, V that ofthe verb set.
If we can merge bn and bo classes ateach step, the algorithm will become slightly moreV 3 .
efficient with the time complexity of O( bN--\]-\].
V + ~jg).6 Re la ted  Work6.1 ModelsWe can restrict he hard clustering model (1) by as-suming that words within a same class are generatedwith an equal probability, obtaining1 1P(n,v) = P(C .
,C~) .
lC .
i  ICol'which is equivalent to the model proposed by (Li andAbe, 1996).
Employing this restricted model has theundesirable tendency to classify into different classesthose words that have similar co-occurrence patternsbut have different absolute frequencies.The hard clustering model defined in (1) can alsobe considered to be an extension of the model pro-posed by Brown et al First, dividing (1) by P(v),we obtain(3)P(C~)'P(vIC~) Since hard clustering implies P(o) = 1holds, we haveP(nlO = P(C.IC~).
P(nlC.
).In this way, the hard clustering model turns out to bea class-based bigram model and is similar to Brownet als model.
The difference is that the model of (3)assumes that the clustering for Ca and the clusteringfor C, can be different, while the model of Brown etal assumes that they are the same.A very general model of noun verb joint probabil-ities is a model of the following form:P(n,v)-- E E P(C.,C~).P(n\]C.).P(vlC~).C~EP.
C,, E Pv(4)752Here Fn denotes a set of noun classes satisfyingUc~r.Cn = Af, but not necessarily disjoint.
Sim-ilarly F~ is a set of not necessarily disjoint verbclasses.
We can view the problem of clustering wordsin general as estimation of such a model.
This typeof clustering in which a word can belong to severaldifferent classes is generally referred to as 'soft clus-tering.'
If we assume in the above model that eachverb forms a verb class by itself, then (4) becomesP(n,v) = Z P(C.,v).
P(nlC.
),C~EF~which is equivalent to the model of Pereira et al Onthe other hand, if we restrict he general model of (4)so that both noun classes and verb classes are dis-joint, then we obtain the hard clustering model wepropose here (1).
All of these models, therefore, aresome special cases of (4).
Each specialization comeswith its merit and demerit.
For example, employinga model of soft clustering will make the clusteringprocess more flexible but also make the learning pro-cess more computationally demanding.
Our choiceof hard clustering obviously has the merits and de-merits of the soft clustering model reversed.6.2 Es t imat ion  cr i ter iaOur method is also an extension of that proposedby Brown et alfrom the viewpoint of estimation cri-terion.
Their method merges word classes so thatthe reduction in mutual information, or equivalentlythe increase in data description length, is minimized.Their method has the tendency to overfit the train-ing data, since it is based on MLE.
Employing MDLcan help solve this problem.7 D isambiguat ion  MethodWe apply the acquired word classes, or more specif-ically the probability model of co-occurrence, to theproblem of structural disambiguation.
In particular,we consider the problem of resolving pp-attachmentambiguities in quadruples, like (see, girl, with, tele-scope) and that of resolving ambiguities in com-pound noun triples, like (data, base, system).
Inthe former, we determine to which of 'see' or 'girl'the phrase 'with telescope' should be attached.
Inthe latter, we judge to which of 'base' or 'system'the word 'data' should be attached.We can perform pp-attachment disambiguation bycomparing the probabilities/5~ith (telescopelsee),/Swith (telescop elgirl).
(5)If the former is larger, we attach 'with telescope'to 'see;' if the latter is larger we attach it to 'girl;'otherwise we make no decision.
(Disambiguationcompound noun triples can be performed similarly.
)Since the number of probabilities to be estimatedis extremely large, estimating all of these probabil-ities accurately is generally infeasible (i.e., the datasparseness problem).
Using our clustering model tocalculate these conditional probabilities (by normal-izing the joint probabilities with marginal probabil-ities) can solve this problem.We further enhance our disambiguation methodby the following back-off procedure: We first esti-mate the two probabilities in question using hardclustering models constructed by our method.
Wealso estimate the probabilities using an existing(hand-made) thesaurus with the 'tree cut' estima-tion method of (Li and Abe, 1995), and use theseprobability values when the probabilities estimatedbased on hard clustering models are both zero.
Fi-nally, if both of them are still zero, we make a defaultdecision.8 Exper imenta l  Resu l t s8.1 Qual i tat ive evaluat ionIn this experiment, we used heuristic rules to extractverbs and the head words of their direct objects fromthe lagged texts of the WSJ corpus (ACL/DCI CD-ROM1) consisting of 126,084 sentences.- -  s~are ,  a~et .
data- -  s tock .
~no,  secur~- -  inc .
.
corp .
.co .i bourne ,  home- -  DenK.
group,  f i rmp r ~ e .
tax- -  money,  ca~- -  c~l r .
v l~ l l i c le- -  pro f i t ,  r i sk- -  so .are ,  network- -  p ressure+ powerFigure 3: A part of a constructed thesaurusWe then constructed a number of thesaurusesbased on these data, using our method.
Fig.
3 showsa part of a thesaurus for 100 randomly selectednouns, based on their appearances as direct objectsof 20 randomly selected verbs.
The thesaurus seemsto agree with human intuition to some degree, al-though it is constructed based on a relatively smallamount of co-occurrence data.
For example, 'stock,''security,' and 'bond' are classified together, despitethe fact that their absolute frequencies in the datavary a great deal (272, 59, and 79, respectively.
)The results demonstrate a desirable feature of ourmethod, namely, it classifies words based solely onthe similarities in co-occurrence data, and is not af-fected by the absolute frequencies of the words.8.2 Compound noun d isambiguat ionWe extracted compound noun doubles (e.g., 'database') from the tagged texts of the WSJ corpus andused them as training data, and then conducted753structural disambiguation  compound noun triples(e.g., 'data base system').We first randomly selected 1,000 nouns from thecorpus, and extracted compound noun doubles con-taining those nouns as training data and compoundnoun triples containing those nouns as test data.There were 8,604 training data and 299 test data.We hand-labeled the test data with the correct dis-ambiguation 'answers.
'We performed clustering on the nouns on theleft position and the nouns on the right position inthe training data by using both our method ('2D-Clustering') and Brown et als method ('Brown').We actually implemented an extended version oftheir method, which separately conducts clusteringfor nouns on the left and those on the right (whichshould only improve the performance).0.850.80.75o.70.850.60.55o.~?
Worcl-~ase~ !
ro~vn" "2D.-Clus~enng" .e.-o.~5 ole o.~5 0:7 o.Y5 0'.8 o.~5 o.g CovefarJeFigure 4: Compound noun disambiguation resultsWe next conducted structural disambiguationthe test data, using the probabilities estimated basedon 2D-Clustering and Brown.
We also tested themethod of using the probabilities estimated basedon word co-occurrences, denoted as 'Word-based.'Fig.
4 shows the results in terms of accuracy andcoverage, where coverage refers to the percentageof test data for which the disambiguation methodwas able to make a decision.
Since for Brown thenumber of classes finally created has to be designedin advance, we tried a number of alternatives andobtained results for each of them.
(Note that, for2D-Clustering, the optimal number of classes is au-tomatically selected.
)Table 1: Compound noun disambiguation resultsMethod Acc.
(%)Default 59.2Word-based + Default 73.9Brown + Default 77.32D-Clustering + Default 78.3Tab.
1 shows the final results of all of the abovemethods combined with 'Default,' in which we at-tach the first noun to the neighboring noun whena decision cannot be made by each of the meth-ods.
We see that 2D-Clustering+Default performsthe best.
These results demonstrate a desirable as-pect of 2D-Clustering, namely, its ability of automat-ically selecting the most appropriate l vel of clus-tering, resulting in neither over-generalization norunder-generalization.8.3 PP-at tachment  dlsambiguationWe extracted triples (e.g., 'see, with, telescope')from the bracketed ata of the WSJ corpus (PennTree Bank), and conducted PP-attachment disam-biguation on quadruples.
We randomly generatedten sets of data consisting of different raining andtest data and conducted experiments hrough 'ten-fold cross validation,' i.e., all of the experimentalresults reported below were obtained by taking av-erage over ten trials.Table 2: PP-attachment disambiguation resultsMethod Coy.
(%) Acc.
(%)Default 100 56.2Word-based 32.3 95.6Brown 51.3 98.32D-Clustering 51.3 98.3Li-Abe96 37.3 94.7WordNet 74.3 94.5NounClass-2DC 42.6 97.1We constructed word classes using our method('2D-Clustering') and the method of Brown et al('Brown').
For both methods, following the pro-posal due to (Tokunaga et al, 1995), we separatelyconducted clustering with respect o each of the 10most frequently occurring prepositions (e.g., 'for,''with,' etc).
We did not cluster words for rarelyoccurring prepositions.
We then performed isam-biguation based on 2D-Clustering and Brown.
Wealso tested the method of using the probabilities es-timated based on word co-occurrences, denoted as'Word-based.
'Next, rather than using the conditional probabili-ties estimated by our method, we only used the nounthesauruses constructed byour method, and appliedthe method of (Li and Abe, 1995) to estimate thebest 'tree cut models' within the thesauruses a inorder to estimate the conditional probabilities likethose in (5).
We call the disambiguation methodusing these probability values 'NounClass-2DC.'
Wealso tried the analogous method using thesaurusesconstructed by the method of (Li and Abe, 1996)3The method  of (Li and Abe, 1995) outputs  a 'tree cutmodel '  in a given thesaurus  with condit ional probabil it ies at-tached to all the nodes in the tree cut.
They  use MDL toselect the best tree cut model.754and estimating the best tree cut models (this is ex-actly the disambiguation method proposed in thatpaper).
Finally, we tried using a hand-made the-saurus, WordNet (this is the same as the disam-biguation method used in (Li and Abe, 1995)).
Wedenote these methods as 'Li-Abe96' and 'WordNet,'respectively.Tab.
2 shows the results for all these methods interms of coverage and accuracy.Table 3: PP-attachment disambiguation resultsMethod Acc.
(%)Word-based + DefaultBrown + Default2D-Clustering + DefaultLi-Abe96 + DefaultWordNet + DefaultNounClass-2DC + Default69.576.276.271.082.273.82D-Clustering + WordNet + Default 85.2Brill-Resnik 82.4We then enhanced each of these methods by usinga default rule when a decision cannot be made, whichis indicated as '+Default.'
Tab.
3 shows the resultsof these experiments.We can make a number of observations from theseresults.
(1) 2D-Clustering achieves a broader cover-age than NounClass-2DC.
This is because in orderto estimate the probabilities for disambiguation, theformer exploits more information than the latter.
(2) For Brown, we show here only its best result,which happens to be the same as the result for 2D-Clustering, but in order to obtain this result we hadto take the trouble of conducting a number of tests tofind the best level of clustering.
For 2D-Clustering,this was done once and automatically.
Comparedwith Li-Abe96, 2D-Clustering clearly performs bet-ter.
Therefore we conclude that our method im-proves these previous clustering methods in one wayor another.
(3) 2D-Clustering outperforms WordNetin term of accuracy, but not in terms of coverage.This seems reasonable, since an automatically con-structed thesaurus i more domain dependent andtherefore captures the domain dependent featuresbetter, and thus can help achieve higher accuracy.On the other hand, with the relatively small size oftraining data we had available, its coverage issmallerthan that of a general purpose hand made thesaurus.The result indicates that it makes ense to combineautomatically constructed thesauruses and a hand-made thesaurus, as we have proposed in Section 7.This method of combining both types of the-sauruses '2D-Clustering+WordNet+Default' wasthen tested.
We see that this method performs thebest.
(See Tab.
3.)
Finally, for comparison, wetested the 'transformation-based error-driven learn-ing' proposed in (Brill and Resnik, 1994), which isa state-of-the-art method for pp-attachment disam-biguation.
Tab.
3 shows the result for this methodas 'Brill-Resnik.'
We see that our disambigua-tion method also performs better than Brill-Resnik.
(Note further that for Brill & Resnik's method, weneed to use quadruples as training data, whereasours only requires triples.
)9 ConclusionsWe have proposed anew method of clustering wordsbased on co-occurrence data.
Our method employsa probability model which naturally represents co-occurrence patterns over word pairs, and makes useof an efficient estimation algorithm based on theMDL principle.
Our clustering method improvesupon the previous methods proposed by Brown et aland (Li and Abe, 1996), and furthermore it can beused to derive a disambiguation method with overalldisambiguation accuracy of 85.2%, which improvesthe performance ofa state-of-the-art disambiguationmethod.The proposed algorithm, 2D-Clustering, can beused in practice, as long as the data size is at thelevel of the current Penn Tree Bank.
Yet it is stillrelatively computationally demanding, and thus animportant future task is to further improve on itscomputational efficiency.AcknowledgementWe are grateful to Dr. S. Doi of NEC C&C MediaRes.
Labs.
for his encouragement.
We thank Ms. Y.Yamaguchi of NIS for her programming efforts.ReferencesE.
Brill and P. Resnik.
A rule-based approach toprepositional phrase attachment disambiguation.Proc.
of COLING'9~, pp.
1198-1204.P.
F. Brown, V. J. Della Pietra, P. V. deSouza, J.C. Lai, and R. L. Mercer.
1992.
Class-based n-gram models of natural anguage.
Comp.
Ling.,18(4):283-298.H.
Li and N. Abe.
1995.
Generalizing case framesusing a thesaurus and the MDL principle.
Comp.Ling., (to appear).H.
Li and N. Abe.
1996.
Clustering words with theMDL principle.
Proc.
of COLING'96, pp.
4-9.F.
Pereira, N. Tishby, and L. Lee.
1993.
Distri-butional clustering of English words.
Proc.
ofACL'gg, pp.
183-190.J.
Rissanen.
1989.
Stochastic Complexity in Statisti-cal Inquiry.
World Scientific Publishing Co., Sin-gapore.T.
Tokunaga, M. Iwayama, and H. Tanaka.
Auto-matic thesaurus construction based-on grammat-ical relations.
Proc.
of IJCAI'95, pp.
1308-1313.755
