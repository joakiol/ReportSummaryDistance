INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 22?30,Utica, May 2012. c?2012 Association for Computational LinguisticsTowards a Surface Realization-Oriented Corpus AnnotationLeo WannerICREA andUniversitat Pompeu FabraRoc Boronat 138Barcelona, 08018, Spainleo.wanner@upf.eduSimon MilleUniversitat Pompeu FabraRoc Boronat 138Barcelona, 08018, Spainsimon.mille@upf.eduBernd BohnetUniversita?t StuttgartIMS, Pfaffenwaldring 5bStuttgart, 70569, Germanybohnet@ims.uni-stuttgart.deAbstractUntil recently, deep stochastic surface realiza-tion has been hindered by the lack of seman-tically annotated corpora.
This is about tochange.
Such corpora are increasingly avail-able, e.g., in the context of CoNLL sharedtasks.
However, recent experiments withCoNLL 2009 corpora show that these popu-lar resources, which serve well for other ap-plications, may not do so for generation.
Theattempts to adapt them for generation resultedso far in a better performance of the realizers,but not yet in a genuinely semantic generation-oriented annotation schema.
Our goal is toinitiate a debate on how a generation suit-able annotation schema should be defined.
Wedefine some general principles of a semanticgeneration-oriented annotation and propose anannotation schema that is based on these prin-ciples.
Experiments shows that making thesemantic corpora comply with the suggestedprinciples does not need to have a negative im-pact on the quality of the stochastic generatorstrained on them.1 IntroductionWith the increasing interest in data-driven surfacerealization, the question on the adequate annota-tion of corpora for generation also becomes increas-ingly important.
While in the early days of stochas-tic generation, annotations produced for other ap-plications were used (Knight and Hatzivassiloglou,1995; Langkilde and Knight, 1998; Bangalore andRambow, 2000; Oh and Rudnicky, 2000; Langkilde-Geary, 2002), the poor results obtained, e.g., by(Bohnet et al, 2010) with the original CoNLL 2009corpora, show that annotations that serve well forother applications, may not do so for generation andthus need at least to be adjusted.1 This has alsobeen acknowledged in the run-up to the surface re-alization challenge 2011 (Belz et al, 2011), wherea considerable amount of work has been investedinto the conversion of the annotations of the CoNLL2008 corpora (Surdeanu et al, 2008), i.e., PropBank(Palmer et al, 2005), which served as the referencetreebank, into a more ?generation friendly?
annota-tion.
However, all of the available annotations are toa certain extent still syntactic.
Even PropBank andits generation-oriented variants contain a significantnumber of syntactic features (Bohnet et al, 2011b).Some previous approaches to data-driven genera-tion avoid the problem related to the lack of seman-tic resources in that they use hybrid models that im-ply a symbolic submodule which derives the syntac-tic representation that is then used by the stochas-tic submodule (Knight and Hatzivassiloglou, 1995;Langkilde and Knight, 1998).
(Walker et al, 2002),(Stent et al, 2004), (Wong and Mooney, 2007), and(Mairesse et al, 2010) start from deeper structures:Walker et al and Stent et al from deep-syntacticstructures (Mel?c?uk, 1988), and Wong and Mooneyand Mairesse et al from higher order predicate logicstructures.
However, to the best of our knowledge,1Trained on the original ConLL 2009 corpora, (Bohnet et al,2010)?s SVM-based generator reached a BLEU score of 0.12 forChinese, 0.18 for English, 0.11 for German and 0.14 for Span-ish.
Joining the unconnected parts of the sentence annotations toconnected trees (as required by a stochastic realizer) improvedthe performance to a BLEU score of 0.69 for Chinese, 0.66 forEnglish, 0.61 for German and 0.68 for Spanish.22none of them uses corpora annotated with the struc-tures from which they start.To deep stochastic generation, the use of hybridmodels is not an option and training a realizer onsyntactically-biased annotations is highly problem-atic in the case of data-to-text NLG, which startsfrom numeric time series or conceptual or seman-tic structures: the syntactic features will be simplynot available in the input structures at the momentof application.2.
Therefore, it is crucial to define atheoretically sound semantic annotation that is stillgood in practical terms.Our goal is thus to discuss some general prin-ciples of a semantic generation-oriented annotationschema and offer a first evaluation of its possibleimpact on stochastic generation.
Section2 detailswhat kind of information is available respectivelynot available during data-to-text generation.
Sec-tion 3 states some general principles that constrainan adequate semantic representation, while Section4 formally defines their well-formedness.
Section 5reports then on the experiments made with the pro-posed annotation, and Section6 offers some conclu-sions.2 What can we and what we cannot counton?In data-to-text or ontology-to-text generation, withthe standard content selection?discourse structur-ing?surface generation pipeline in place, and nohard-wired linguistic realization of the individualchunks of the data or ontology structure, the inputto the surface realization module can only be an ab-stract structure that does not contain any syntactic(and even lexical) information.
Conceptual graphsin the sense of Sowa (Sowa, 2000) are structures ofthis kind;3 see Figure 1 for illustration (?Cmpl?
=?Completion?, ?Rcpt?
= ?Recipient?, ?Strt?
= ?Start?,?Attr?
= Attribute, ?Chrc?
= ?Characteristic?, and?Amt?
= ?Amount?).
Content selection accounts forthe determination of the content units that are to becommunicated and Discourse Structuring for the de-limitation of Elementary Discourse Units (EDUs)2Even though in this article we are particularly interested indata-to-text generation, we are convinced that clean semanticand syntactic annotations also facilitate text-to-text generation.3But note that this can be any other content structure.and their organization and for the discursive rela-tions between them (e.g., Bcas (Because) in the Fig-ure).In particular, such a structure cannot contain:?
non-meaningful nodes: governed prepositions(BECAUSE of, CONCENTRATION of), auxil-iaries (passive be), determiners (a, the);?
syntactic connectors (between A and B), rela-tive pronouns, etc.?
syntactic structure information: A modifies B,A is the subject of B, etc.In other words, a deep stochastic generator hasto be able to produce all syntactic phenomena fromgeneric structures that guarantee a certain flexibil-ity when it comes to their surface form (i.e., withoutencoding directly this type of syntactic information).For instance, a concentration of NO2 can be realizedas a NO2 concentration, between 23h00 and 00h00as from 23h00 until 00h00, etc.
This implies thatdeep annotations as, for instance, have been derivedso far from PennTreeBank/PropBank, in which ei-ther all syntactic nodes of the annotation are kept(as in (Bohnet et al, 2010)) or only certain syntac-tic nodes are removed (as THAT complementizersand TO infinitives in the shared task 2011 on sur-face realization (Belz et al, 2011)) still fall shortof a genuine semantic annotation.
Both retain alot of syntactic information which is not accessiblein genuine data-to-text generation: nodes (relativepronouns, governed prepositions and conjunctions,determiners, auxiliaries, etc.)
and edges (relativeclause edges, control edges, modifier vs. argumen-tal edges, etc.
).This lets us raise the question how the annotationpolicies should look like to serve generation welland to what extent existing resources such as Prop-Bank comply with them already.
We believe thatthe answer is critical for the future research agendain generation and will certainly play an outstandingrole in the shared tasks to come.In the next section, we assess the minimal princi-ples which the annotation suitable for (at least) data-to-text generation must follow in order to lead toa core semantic structure.
This core structure stillignores such important information as co-reference,23Figure 1: Sample conceptual structure as could be produced by text planning (Because of a concentration of NO2 of13?g/m3, the NO2 threshold value was exceeded between 23h00 and 00h00)scope, presupposition, etc.
: this information is ob-viously necessary, but it is not absolutely vital fora sufficient restriction of the possible choices facedduring surface generation.
Further efforts will be re-quired to address its annotation in appropriate depth.3 The principles of generation-suitablesemantic annotationBefore talking about generation-suitable annotation,we must make some general assumptions concern-ing NLG as such.
These assumptions are necessary(but might not always be sufficient) to cover deepgeneration in all its subtleties: (i) data-to-text gener-ation starts from an abstract conceptual or semanticrepresentation of the content that is to be renderedinto a well-formed sentence; (ii) data-to-text gener-ation is a series of equivalence mappings from moreabstract to more concrete structures, with a chain ofinflected words as the final structure; (iii) the equiva-lence between the source structure Ss and the targetstructure St is explicit and self-contained, i.e., forthe mapping from Ss to St, only features containedin Ss and St are used.
The first assumption is inthe very nature of the generation task in general; thesecond and the third are owed to requirements of sta-tistical generation (although a number of rule-basedgenerators show these characteristics as well).The three basic assumptions give rise to the fol-lowing four principles.1.
Semanticity: The semantic annotation must cap-ture the meaning and only the meaning of a givensentence.
Functional nodes (auxiliaries, determin-ers, governed conjunctions and prepositions), nodeduplicates and syntactically-motivated arcs shouldnot appear in the semantic structure: they re-flect grammatical and lexical features, and thus al-ready anticipate how the meaning will be worded.For example, meet-AGENT?the (directors), meet-LOCATION?in (Spain), meet-TIME?in (2002)cited in (Buch-Kromann et al, 2011) as semanticannotation of the phrase meeting between the direc-tors in Spain in 2002 in the Copenhagen Depen-dency Treebank does not meet this criterion: the,and both ins are functional nodes.
Node dupli-cates such as the relative pronoun that in the Prop-Bank annotation (But Panama illustrates that theirtheir substitute is a system) that?R-A0-produces(an absurd gridlock) equally reflect syntactic fea-tures, as do syntactically-motivated arc labels of thekind ?R(elative)-A0?.The PropBank annotation of the sentence citedabove also intermingles predicate-argument rela-tions (?Ai?)
with syntactico-functional relations(?AM-MNR?
): gridlock?AM-MNR?absurd.
Thepredicate-argument analysis of modifiers suggestsnamely that they are predicative semantemes thattake as argument the node that governs themin the syntactic structure; in the above struc-ture: absurd?A1?gridlock.
This applies alsoto locatives, temporals and other ?circumstan-tials?, which are most conveniently representedas two-place semantemes: house?A1?location?A2?Barcelona, party?A1?time?A2?yes-terday, and so on.
Although not realized at the sur-face, location, time, etc.
are crucial.242.
Informativity: A propositional semantic annota-tion must be enriched by information structure fea-tures that predetermine the overall syntactic struc-ture (paratactic, hypotactic, parenthetical, .
.
.
), theinternal syntactic structure (subject/object, clefted ornot, any element fronted or not, etc.
), determiner dis-tribution, etc.
in the sentence.
Otherwise, it will bealways underspecified with respect to its syntacticequivalence in that, as a rule, a single semantic struc-ture will correspond to a number of syntactic struc-tures.
This is not to say that with the informationstructure in place we will always achieve a 1:1 cor-respondence between the semantic and syntactic an-notations; further criteria may be needed?includingprosody, style, presupposedness, etc.
However, in-formation structure is crucial.The most relevant information structure featuresare those of Thematicity, Foregroundedness andGivenness.4Thematicity specifies what the utterance states(marked as rheme) and about what it states it(marked as theme).5 Theme/rheme determines, inthe majority of cases, the subject-object structureand the topology of the sentence.
For instance,6[John]theme?A1?
[see?A2?Maria]rheme maybe said to correspond to John?subject?see?dir.obj?Maria and [John?A1?see]rheme?A2?
[Maria]theme to John ?obj?seepass?subject?Maria.
For the generation of relative sentencestructures such as John bought a car which was oldand ugly, we need to accommodate for a recursivedefinition of thematicity: [John]theme?A1?[buy?A2?
[c1 : car]theme?A1?
[old]rheme; c1?A1?
[ugly]rheme]rheme.7 With no recursive (or sec-ondary in Mel?c?uk?s terms) thematicity, we would4We use mainly the terminology and definitions (although insome places significantly simplified) of (Mel?c?uk, 2001), who,to the best of our knowledge, establishes the most detailed cor-relation between information structure and syntactic features.5Similar notions are topic/focus (Sgall et al, 1986) andtopic/comment (Gundel, 1988).6As in PropBank, we use ?Ai?
as argument labels of predica-tive lexemes, but for us, ?A1?
stands for the first argument, ?A2?for the second argument, etc.
That is, in contrast to PropBank,we do not support the use of ?A0?
to refer to a lexeme?s exter-nal argument since the distinction between external and internalarguments is syntactic.7c1 is a ?handle?
in the sense of Minimal Recursion Seman-tics (Copestake et al, 1997).get John bought an old and ugly car.8It is quite easy to find some counter-examplesto the default theme/rheme?syntactic feature cor-relation, in particular in the case of questionsand answers.
For instance, the neutral answerto the question What will John bake tomorrow?,John will bake a cake, would be split as follows:[John?A1?bake]theme ?A2?[cake]rheme.
Inthis case, the main verb at the surface, bake, is in-cluded in the theme and not in the rheme.
Consideralso the sentence In a cross-border transaction, thebuyer is in a different region of the globe from thetarget, where the main theme is in a cross-bordertransaction, i.e., not the subject of the sentence (withthe subject the buyer being the embedded theme ofthe main rheme).
In these cases, the correlation ismore complex, but it undoubtedly exists and needsto be distilled during the training phase.Foregroundedness captures the ?prominence?of the individual elements of the utterance forthe speaker or hearer.
An element is ?fore-grounded?
if it is prominent and ?backgrounded?if it is of lesser prominence; elements that areneither foregrounded nor backgrounded are ?neu-tral?.
A number of correlations can be iden-tified: (i) a ?foregrounded?
A1 argument of averb will trigger a clefting construction; e.g.,[John]foregr;theme?A1?
[see?A2?Maria]rhemewill lead to It was John who saw Maria; similarly,[John?A1?bake]foregr;theme ?A2?
[cake]rhemewill lead to What John will bake is a cake; (ii) a?foregrounded?
A2 argument of a verb will corre-spond to a clefting construction or a dislocation: Itwas Maria, whom John saw; (iii) a ?foregrounded?A1 or A2 argument of a noun will result in an argu-ment promotion, as, e.g., John?s arrival (instead ofarrival of John); (iv) a ?foregrounded?
circumstan-tial will be fronted: Under this tree he used to rest;(v) marking a part of the semantic structure as ?back-grounded?
will lead to a parenthetical construction:John (well known among the students and professorsalike) was invited as guest speaker.
If no elements8We believe that operator scopes (e.g., negations and quan-tifiers) can, to a large extent, be encoded within the thematicstructure; see (Cook and Payne, 2006) for work in the LFG-framework on German, which provides some evidence for this.However, it must be stated that very little work has been doneon the subject until now.25are marked as foregrounded/backgrounded, the de-fault syntactic structure and the default word orderare realized.Givenness captures to what extent an informationelement is present to the hearer.
The elementarygivenness markers ?given?
and ?new?
correlate insyntax with determiner distribution.
Thus, the ?new?marker of an object node will often correspond toan indefinite or zero determiner of the correspond-ing noun: A masked man was seen to enter thebank (man is newly introduced into the discourse).The ?given?
marker will often correlate with a defi-nite determiner: The masked man (whom a passer-by noticed before) was seen to enter the bank.
Todistinguish between demonstratives and definite de-terminers, a gradation of givenness markers as sug-gested by Gundel et al (Gundel et al, 1989) is nec-essary: ?given1/2/3?.As already for Thematicity, numerous examplescan be found where the giveness-syntactic featurecorrelation deviates from the default correlation.
Forinstance, in I have heard a cat, the cat of my neigh-bour, there would be only one single (given) nodecat in the semantic structure, which does not pre-vent the first appearance of cat in the sentence to beindefinite.
In A warrant permits a holder that he ac-quire one share of common stock for $17.50 a share,warrant is given, even if it is marked by an indefinitedeterminer.
Again, this only shows the complexityof the annotation of the information structure, but itdoes not call into question the relevance of the infor-mation structure to NLG.As one of the few treebanks, the Prague Depen-dency Treebank (PDT) (Hajic?
et al, 2006) accountsfor aspects of the information structure in that it an-notates Topic-Focus Articulation in terms of variousdegrees of contextual boundness, which are corre-lated with word order and intonation (Mikulova?
etal., 2006, p.152).3.
Connectivity: The semantic annotation mustensure that the annotation of an utterance formsa connected structure: without a connected struc-ture, generation algorithms that imply a traver-sal of the input structure will fail to generate agrammatical sentence.
For instance, the Prop-Bank annotation of the sentence But Panama il-lustrates that their substitute is a system that pro-duces an absurd gridlock (here shown partially)does not comply with this principle since it con-sists of four unconnected meaning-bearing sub-structures (the single node ?but?
and the subtreesgoverned by ?illustrate?, ?produce?
and ?substi-tute?
): but | Panama?A0?illustrate?A1?that |system?A0?produce?A1?gridlock?AM-MNR?absurd | substitute?A0?their.4 Outline of a Generation-OrientedAnnotationThe definitions below specify the syntactic well-formedness of the semantic annotation.
They do notintend to and cannot substitute a detailed annotationmanual, which is indispensable to achieve a seman-tically accurate annotation.Definition 1: [Semantic Annotation of a sentenceS, SA]SA of S in the text T in language L is a pair?Ssem, Sinf ?, where Ssem is the semantic structureof S (ensuring Semanticity and Connectivity), andSinf is the information structure of S (ensuring In-formativity).Let us define each of the two structures of the se-mantic annotation in turn.Definition 2: [Semantic Structure of a sentence S,Ssem]Ssem of S is a labeled acyclic directed connectedgraph (V,E, ?, ?)
defined over the vertex label al-phabet L := LS ?MC ?MT ?Mt ?Ma (such thatLS ?
(MC ?MT ?Mt ?Ma) = ?)
and the edgelabel alphabet Rsem ?
{A1, A2, A3, A4, A5, A6},with?
V as the set of vertices;?
E as the set of directed edges;?
?
as the function that assigns each v ?
V an ele-ment l ?
L;?
?
as the function that assigns each e ?
E an ele-ment a ?
Rsem;?
LS as the meaning bearing lexical units (LUs) ofS;?
MC ?
{LOC, TMP, EXT, MNR, CAU, DIR,SPEC, ELAB, ADDR} as the ?circumstantial metasemantemes?
(with the labels standing for ?locative?,?temporal?, ?temporal/spatial extension?, ?manner?,?cause?, ?direction?, ?specification?, ?elaboration?,and ?addressee?);?
MT ?
{TIME, TCST} as the ?temporal meta se-mantemes?
(with the labels standing for ?time?
and26?time constituency?);?
Mt ?
{past?, present?, future?}
as the ?timevalue semantemes?;?
Ma ?
{imperfective?, durative?,semelfactive?, iterative?, telic?, atelic?,nil?}
as the ?aspectual value semantemes?9such that the following conditions hold:(a) the edges in Ssem are in accordance with the va-lency structure of the lexical units (LUs) in S: Iflp?Ai?lr ?
Ssem (lp, lr ?
LS , i ?
{1, 2, 3, .
.
.
}),then the semantic valency of lp possesses at least islots and lr fulfils the semantic restrictions of the i-th slot(b) the edges in Ssem are exhaustive: If ?
(nr) =lr ?
L instantiates in S the i-th semantic argumentof ?
(np) = lp, then lp?Ai?lr ?
Ssem(c) Ssem does not contain any duplicated argumentedges: If ?(np)?Ai??
(nr), ?
(np) ?Aj?
?
(nq) ?Ssem (with np, nr, nq ?
N ) then Ai 6= Aj and nr 6=nq(d) circumstantial LUs in S are represented in Ssemby two-place meta-semantemes: If lr ?
Lsem isa locative/temporal/ manner/cause/direction/specifi-cation/elaboration/addressee LU and in the syntac-tic dependency structure of S, lr modifies lp, thenlr?A2-?-A1?lp ?
Ssem (with ?
?
LOC, TMP,MNR, CAU, DIR, SPEC, ELAB, ADDR)(e) verbal tense is captured by the two-place predi-cate TIME: If lp ?
Lsem is a verbal LU then lr?A2-TIME-A1?lp ?
Ssem, with lr ?
Mt(f) verbal aspect is captured by the two-place predi-cate TCST: If lp ?
Lsem is a verbal LU then lr?A2-TCST-A1?lp ?
Ssem, with lr ?
Ma.
(a) implies that no functional node is target of an ar-gument arc: this would contradict the semantic va-lency conditions of any lexeme in S. (b) ensures thatno edge in Ssem is missing: if a given LU is an argu-ment of another LU in the sentence, then there is anedge from the governor LU to the argument LU.
(c)means that no predicate in Ssem possesses in S twodifferent instances of the same argument slot.
Thecircumstantial meta-semantemes in (d) either cap-ture the semantic role of a circumstantial that wouldotherwise get lost or introduce a predicate type for aname.
Most of the circumstantial meta-semantemes9The aspectual feature names are mainly from (Comrie,1976).reflect PropBank?s modifier relations ?AM-X?
(but insemantic, not in syntactico-functional terms), suchthat their names are taken from PropBank or are in-spired by PropBank.
LOC takes as A1 a name of alocation of its A2: Barcelona?A1-LOC-A2?live-A1?John; TMP a temporal expression: yesterday?A1-TMP-A2?arrive-A1?John; MNR a man-ner attribute: player?A1-MNR-A2?solo; CAUthe cause: accept?A1-CAU-A2?reason in Thisis the reason why they accepted it; DIR a spa-tial direction: run around?A2-DIR-A1?circles inI?m running around in circles; SPEC a ?contextspecifier?
: should?A2-SPEC-A1?thought in Youshould leave now, just a thought; ELAB an appos-itive attribute company?A1-ELAB-A2 ?bank inThis company, a bank, closed; and ADDR direct ad-dress: come?A1-ADDR-A2?John in John, comehere!Definition 3: [Information Structure of a sen-tence S, Sinf ]Let Ssem of S be defined as above.
Sinf of S isan undirected labeled hypergraph (V, I) with V asthe set of vertices of S and I the set of hyperedges,with I := {themei (i = 1, 2, .
.
.
), rhemei (i = 1, 2,. .
.
), givenj (j = 1,. .
.
,3), new, foregrounded, back-grounded}.
The following conditions apply:(a) thematicity is recursive, i.e., a thematic hyper-edge contains under specific conditions embeddedtheme/rheme hyperedges: If ?nk ?
themei such that?
(nk) = lp is a verbal lexeme and lp -A1?lr ?Ssem, then ?
themei+1, rhemei+1 ?
themei(b) theme and rheme hyperedges of the same re-cursion level, given and new hyperedges, and fore-grounded and backgrounded hyperedges are dis-joint: themei ?
rhemei = ?
(i = 1, 2, .
.
.
), givenj?
new = ?
(j = 1,. .
.
,3), foregr.
?
backgr.
= ?
(c) any node in Ssem forms part of either theme orrheme: ?np ?
Ssem : np ?
theme1 ?
rheme1.Consider in Figure 2 an example of SA with itstwo structures.10 All syntactic nodes have been re-moved, and all the remaining nodes are connectedin terms of a predicate?argument structure, with nouse of any syntactically motivated edge, so as to en-sure that the structure complies with the Semantic-ity and Connectivity principles.
Figure 2 illustratesthe three main aspects of Informativity: (i) thematic-10The meta-semanteme TCST is not shown in the figure.27ity, with the two theme/rheme oppositions; (ii) fore-groundedness, with the backgrounded part of theprimary rheme; and (iii) givenness, with the attributegivenness and the value 2 on the node program.
Theinformation structure constrains the superficial real-ization of the sentence in that the primary theme willbe the subject of the sentence, and the main node ofthe primary rheme pointing to it will be the mainverb of the same sentence.
The secondary themeand rheme will be realized as an embedded sen-tence in which you will be the subject, that is, forc-ing the realization of a relative clause.
However, itdoes not constrain the appearance of a relative pro-noun.
For instance: we obtained technologies youdo not see anywhere else and we obtained technolo-gies that you do not see anywhere else are possiblerealizations of this structure.
Leaving the relativepronoun in the semantic structure would force onerealization to occur when it does not have to (bothoutputs are equally correct and meaning-equivalentto the other).
Similarly, marking the Soviet spaceprogram as backgrounded leaves some doors openwhen it comes to surface realization: Cosmos, theSoviet space program vs. Cosmos (the Soviet spaceprogram) vs. the Soviet space program Cosmos (ifCosmos is backgrounded too) are possible realiza-tions of this substructure.ELABORATION is an example of a meta-nodeneeded to connect the semantic structure: Cosmosand program have a semantic relation, but neither isactually in the semantic frame of the other?whichis why the introduction of an extra node cannot beavoided.
In this case, we could have a node NAME,but ELABORATION is much more generic and canactually be automatically introduced without any ad-ditional information.5 ExperimentsObviously, the removal of syntactic features froma given standard annotation, with the goal to ob-tain an increasingly more semantic annotation, canonly be accepted if the quality of (deep) stochas-tic generation does not unacceptably decrease.
Toassess this aspect, we converted automatically thePropBank annotation of the WSJ journal as used inthe CoNLL shared task 2009 into an annotation thatcomplies with all of the principles sketched abovefor deep statistical generation and trained (Bohnetet al, 2010)?s generator on this new annotation.11For our experiments, we used the usual training,development and test data split of the WSJ cor-pus (Langkilde-Geary, 2002; Ringger et al, 2004;Bohnet et al, 2010); Table 1 provides an overviewof the used data.set section # sentencestraining 2 - 21 39218development 24 1334test 23 2400Table 1: Data split of the used data in the WSJ CorpusThe resulting BLEU score of our experiment was0.64, which is comparable with the accuracy re-ported in (Bohnet et al, 2010) (namely, 0.659), whoused an annotation that still contained all functionalnodes (such that their generation task was consider-ably more syntactic and thus more straightforward).To assess furthermore whether the automaticallyconverted PropBank already offers some advantagesto other applications than generation, we used it in asemantic role labeling (SRL) experiment with (Bjo?rkelund et al, 2010)?s parser.
The achieved overallaccuracy is 0.818, with all analysis stages (includingthe predicate identification stage) being automatic,which is a rather competitive figure.
In the originalCoNLL SRL setting with Oracle reading, an accu-racy of 0.856 is achieved.Another telling comparison can be made betweenthe outcomes of the First Surface Realization SharedTask (Belz et al, 2011), in which two differentinput representations were given to the competingteams: a shallow representation and a deep repre-sentation.
The shallow structures were unorderedsyntactic dependency trees, with all the tokens ofthe sentence, and the deep structures were predicate-argument graphs with some nodes removed (seeSection 2).
Although the performance of shallowgenerators was higher than the performance of thedeep generators (the StuMaBa shallow generator(Bohnet et al, 2011a) obtained a BLEU score of0.89, as opposed to 0.79 of the StuMaBa deep gen-11Obviously, our conversion can be viewed only preliminary.It does not take into account all the subtleties that need to betaken account?for instance, with respect to the informationstructure; see also Section 6.28Figure 2: Illustration of the semantic annotation of the sentence Through the development of Cosmos, the Soviet spaceprogram, we obtained technologies you do not see anywhere else.erator), the difference is not as striking as one wouldexpect.126 ConclusionsOur experiments and the Surface Realization SharedTask 2011 suggest that making the deep annotationmore semantic does not necessarily imply an unsur-mountable problem for stochastic generation.
Wecan thus conclude that deriving automatically a deepsemantic annotation from PropBank allowed us toobtain very promising results, both for NLG andSRL.
By sticking to universal predicate-argumentstructures, as PropBank does, we maintain the po-tential of the corpus to be mapped to other, more id-iosyncratic, annotations.
Still, automatic conversionwill always remain deficient.
Thus, a flawless iden-tification of semantic predication cannot be guaran-teed.
For instance, when an actancial arc points to apreposition, it is not clear how to deduce whetherthis preposition is semantic or lexical.
Also, thetreatment of phraseological nodes is problematic,as is the annotation of a comprehensive informa-12Note that our results mentioned above cannot be directlycompared with the StuMaBa results during the GenerationChallenges 2011 because the realizers are different.tion structure: the criteria for the automatic deriva-tion of the information structure from the syntacticstructure and the topology of a sentence can onlybe superficial and likely to be even less efficient inlonger and complex sentences.
The annotation ofintersentential coreferences and the identification ofgapped elements are further major hurdles for an au-tomatic derivation of a truly semantic resource.
Asa consequence, we believe that new annotation poli-cies are needed to obtain a high quality semantic re-source.
The best strategy is to start with a conver-sion of an existing semantically annotated treebanksuch as PropBank, revising and extending the resultof this conversion in a manual concerted action?always following truly semantic annotation policies.AcknowledgmentsWe would like to thank the reviewers for their valu-able comments and suggestions and the Penn Tree-bank/PropBank/NomBank team, without whom ourexperiments would not be possible.
Many thanksalso to Mike White for the useful discussions onsome of the topics discussed in the paper.
Althoughwe might still not agree on all of the details, hemade us see the task of generation-oriented annota-29tion from another angle and revise some of our ini-tial assumptions.ReferencesS.
Bangalore and O. Rambow.
2000.
Exploiting a Proba-bilistic Hierarchical Model for Generation.
In Proc.
ofCOLING ?00.A.
Belz, M. White, D. Espinosa, D. Hogan, and A. Stent.2011.
The First Surface Realization Shared Task:Overview and Evaluation Results.
In ENLG11.A.
Bjo?rkelund, B. Bohnet, L. Hafdell, and P. Nugues.2010.
A high-performance syntactic and semantic de-pendency parser.
In Proc.
of COLING ?10: Demon-stration Volume.B.
Bohnet, L. Wanner, S. Mille, and A. Burga.
2010.Broad coverage multilingual deep sentence generationwith a stochastic multi-level realizer.
In Proc.
of COL-ING ?10.B.
Bohnet, S. Mille, B. Favre, and L. Wanner.
2011a.<STUMABA>: From Deep Representation to Sur-face.
In ENLG11.B.
Bohnet, S. Mille, and L. Wanner.
2011b.
Statisti-cal language generation from semantic structures.
InProc.
of International Conference on Dependency Lin-guistics.M.
Buch-Kromann, M. Gylling-J?rgensen, L. Jelbech-Knudsen, I. Korzen, and H. Mu?ller.
2011.
Theinventory of linguistic relations used in the Copen-hagen Dependency Treebanks.
www.cbs.dk/ con-tent/download/149771/1973272/file.B.
Comrie.
1976.
Aspect.
Cambridge University Press,Cambridge.P.
Cook and J. Payne.
2006.
Information Structure andScope in German.
In LFG06.A.
Copestake, D. Flickinger, and I.
Sag.
1997.
Minimalrecursion semantics.
Technical report, CSLI, StanfordUniversity, Stanford.J.
Gundel, N. Hedberg, and R. Zacharski.
1989.
Give-ness, Implicature and Demonstrative Expressions inEnglish Discourse.
In CLS-25, Part II (Parasessionon Language in Context), pages 89?103.
Chicago Lin-guistics Society.J.K.
Gundel.
1988.
?Universals of Topic-CommentStructure?.
In M. Hammond, E. Moravc?ik, andJ.
Wirth, editors, Studies in Syntactic Typology.
JohnBenjamins, Amsterdam & Philadelphia.J.
Hajic?, J.
Panevova?, E.
Hajic?ova?, P. Sgall, P. Pa-jas, J.
S?te?pa?nek, J. Havelka, M.
Mikulova?, andZ.
Z?abokrtsky?.
2006.
Prague Dependency Treebank2.0.K.
Knight and V. Hatzivassiloglou.
1995.
Two-level,many paths generation.
In Proc.
of ACL ?95.I.
Langkilde and K. Knight.
1998.
Generation that ex-ploits corpus-based statistical knowledge.
In Proc.
ofCOLING/ACL ?98.I.
Langkilde-Geary.
2002.
An empirical verification ofcoverage and correctness for a general-purpose sen-tence generator.
In Proc.
of 2nd INLG Conference.F.
Mairesse, M Gas?ic?, F.
Juric??
?c?, S Keizer, B. Thomson,K.
Yu, and S. Young.
2010.
Phrase-based statisticallanguage generation using graphical models and activelearning.
In Proc.
of ACL ?10.I.A.
Mel?c?uk.
1988.
Dependency Syntax: Theory andPractice.
SUNY Press, Albany.I.A.
Mel?c?uk.
2001.
Communicative Organizationin Natural Language (The Semantic-CommunicativeStructure of Sentences).
Benjamins Academic Pub-lishers, Amsterdam.M.
Mikulova?
et al 2006.
Annotation onthe tectogrammatical level in the PragueDependency Treebank: Reference book.www.cbs.dk/content/download/149771/ 1973272/file.A.H.
Oh and A.I.
Rudnicky.
2000.
Stochastic languagegeneration for spoken dialogue systems.
In Proc.
ofANL/NAACL Workshop on Conversational Systems.M.
Palmer, D. Gildea, and P. Kingsbury.
2005.
Theproposition bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?105.E.
Ringger, M. Gamon, R.C.
Moore, D. Rojas, M. Smets,and S. Corston-Oliver.
2004.
Linguistically informedstatistical models of constituent structure for orderingin sentence realization.
In Proceedings of COLING,pages 673?679.P.
Sgall, E.
Hajic?ova?, and J. Panevova?.
1986.
The Mean-ing of the Sentence in its Semantic and Pragmatic As-pects.
Reidel Publishing Company, Dordrecht.J.
F. Sowa.
2000.
Knowledge Representation: Logi-cal, Philosophical, and Computational Foundations.Brooks Cole Publishing Co., Pacific Grove, CA, USA.A.
Stent, R. Prasad, and M. Walker.
2004.
Trainable sen-tence planning for complex information presentationin spoken dialog systems.
In Proc.
of ACL ?04.Mihai Surdeanu, Richard Johansson, Adam Meyers,Llu?
?s Ma`rquez, and Joakim Nivre.
2008.
The CoNLL-2008 shared task on joint parsing of syntactic andsemantic dependencies.
In Proceedings of the 12thCoNLL-2008.M.A.
Walker, O.C.
Rambow, and M. Rogati.
2002.Training a sentence planner for spoken dialogue usingboosting.
Computer Speech and Language, 16:409?433.Y.W.
Wong and R.J. Mooney.
2007.
Generation by in-verting a semantic parser that uses statistical machinetranslation.
In Proc.
of the HLT Conference.30
