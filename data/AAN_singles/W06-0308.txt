Proceedings of the Workshop on Sentiment and Subjectivity in Text, pages 55?62,Sydney, July 2006. c?2006 Association for Computational LinguisticsTowards a validated model for affective classification of textsMichel Ge?ne?reux and Roger EvansNatural Language Technology Group (NLTG)University of Brighton, United Kingdom{M.Genereux,R.P.Evans}@brighton.ac.ukAbstractIn this paper, we present the results ofexperiments aiming to validate a two-dimensional typology of affective states asa suitable basis for affective classificationof texts.
Using a corpus of English weblogposts, annotated for mood by their authors,we trained support vector machine binaryclassifiers to distinguish texts on the ba-sis of their affiliation with one region ofthe space.
We then report on experimentswhich go a step further, using four-classclassifiers based on automated scoring oftexts for each dimension of the typology.Our results indicate that it is possible toextend the standard binary sentiment anal-ysis (positive/negative) approach to a twodimensional model (positive/negative; ac-tive/passive), and provide some evidenceto support a more fine-grained classifica-tion along these two axes.1 IntroductionWe are investigating the subjective use of languagein text and the automatic classification of texts ac-cording to their subjective characteristics, or ?af-fect?.
Our approach is to view affective states(such as ?happy?, ?angry?)
as locations in Osgood?sEvaluation-Activation (EA) space (Osgood et al ,1957), and draws on work in psychology whichhas a long history of work seeking to construct atypology of such affective states (Scherer, 1984).A similar approach has been used more recentlyto describe emotional states that are expressed inspeech (Cowie and Cornelius, 2002; Schro?der andCowie, 2005).
Our overall aim is to determinethe extent to which such a typology can be vali-dated and applied to the task of text classificationusing automatic methods.
In this paper we de-scribe some initial experiments aimed at validatinga basic two dimensional classification of weblogdata, first with Support Vector Machine (SVM)binary classifiers, then with Pointwise Mutual In-formation - Information Retrieval (PMI-IR).
Thedomain of weblog posts is particularly well-suitedfor this task given its highly subjective nature andthe availability of data , including data which hasbeen author-annotated for ?mood?, which is a rea-sonable approximation of ?affect?.Recent attempts to classify weblog posts haveshown modest, but consistent improvements overa 50% baseline, only slightly worse than humanperformance (Mishne, 2005).
One important mile-stone is the elaboration of a typology of affec-tive states.
To devise such a typology, our start-ing point is Figure 1, which is based on a modelof emotion as a multicomponent process (Scherer,1984).
In this model, the distribution of the af-fective states is the result of analysing similar-ity judgments by humans for 235 emotion terms1using cluster-analysis and multidimensional scal-ing techniques to map out the structure as a two-dimensional space.
The positioning of words isnot so much controversial as fuzzy; an affectivestate such as ?angry?
to describe facial expressionin speech may have a slightly different locationthan an ?angry?
weblog post.
In this model, thewell-studied ?sentiment?
classification is simply aspecific case (left vs. right halves of the space).The experiments we describe here seek to go be-yond this basic distinction.
They involve an addi-tional dimension of affect, the activity dimension,allowing textual data to be classified into four cat-egories corresponding to each of the four quad-1Reduced to less than 100 in Figure 1.55Figure 1: Typology of affective states based on (Scherer, 1984)rants in the space.
Ultimately, once scores havebeen ?promoted?
to real measures, classificationcan be more precise; for example, a text is not onlynegative and passive, it is more precisely ?depres-sive?.
With such a more precise classification onemight, for example, be able to detect individualsat risk of suicide.
In Experiment 1, we use bi-nary classifiers to investigate how the four quad-rants defined by the typology hold together, theassumption being that if the typology is correct,the classifiers should perform substantially betterthan a random baseline.
In Experiment 2, we goa step closer towards a more fine-grained classifi-cation by evaluating the performance of an unsu-pervised automated technique for scoring texts onboth axes.
Both these experiments are preliminary?
our long term goal is to be able to validate thewhole typology in terms of computationally effec-tive classification.2 CorpusWe have collected from Livejournal2 a total of346723 weblogs (mood-annotated by authors) in2http://www.livejournal.com.English, from which almost half are annotatedwith a mood belonging to one of the four quad-rants, described as follows:Quadrant1 bellicose, tense, alarmed, envious,hateful, angry, enraged, defiant, annoyed, jealous,indignant, frustrated, distressed, disgusted, sus-picious, discontented, bitter, insulted, distrustful,startled, contemptuous and impatient.Quadrant2 apathetic, disappointed, miserable,dissatisfied, taken aback, worried, languid, feelguilt, ashamed, gloomy, sad, uncomfortable, em-barrassed, melancholic, depress, desperate, hes-itant, bored, wavering, droopy, tired, insecured,anxious, lonely and doubtful.Quadrant3 feel well, impressed, pleased,amourous, astonished, glad, content, hopeful,solemn, attentive, longing, relaxed, serious,serene, content, at ease, friendly, satisfied,calm, contemplative, polite, pensive, peaceful,conscientious, empathic, reverent and sleepy.Quadrant4 happy, ambitious, amused, adven-turous, aroused, astonished, triumphant, excited,56conceited, self confident, courageous, feeling su-perior, enthusiastic, light hearthed, determined,passionate, expectant, interested, joyous and de-lighted.In our experiments, we used 15662 from quad-rant Q1 (see Figure 1), 54940 from Q2, 49779from Q3 and 35634 from Q4.3 Experiment 1: Distinguishing the fourQuadrantsOur hypothesis is that the classification of two dis-joint sets of moods should yield a classification ac-curacy significantly above a baseline of 50%.
Toverify our hypothesis, we conducted a series of ex-periments using machine learning to classify we-blog posts according to their mood, each class cor-responding to one particular quadrant.
We usedSupport Vector Machines (Joachims, 2001) withthree basic classic features (unigrams, POS andstems) to classify the posts as belonging to onequadrant or one of the three others.
For each clas-sification task, we extracted randomly 1000 test-ing examples, and trained separately with 2000,4000, 8000 and 16000 examples.
In each case, ex-amples were divided equally among positive andnegative examples3.
The set of features used var-ied for each of these tasks, they were selected bythresholding each (distinct) training data set, afterremoving words (unigrams) from the categoriespoor in affective content (prepositions, determin-ers, etc.).
To qualify as a feature, each unigram,POS or stem had to occur at least three times inthe training data.
The value of each feature corre-sponds to its number of occurence in the trainingexamples.3.1 ResultsOur hypothesis is that, if the four quadrants de-picted in Figure 1 are a suitable arrangement foraffective states in the EA space, a classifier shouldperform significantly better than chance (50%).Table 1 shows the results for the binary classifi-cation of the quadrants.
In this table, the first col-umn identifies the classification task in the form?P vs N?, where ?P?
stands for positive examplesand ?N?
for negative examples.
The ?Random?
rowshows results for selecting positive and negativeexamples randomly from all four quadrants.
By3For instance, 1000 = 500 positives from one QUAD-RANT + 500 negatives among the other three QUAD-RANTS.micro-averaging accuracy for the classification ofeach quadrant vs all others (rows 10 to 13), weobtain at least 60% accuracy for the four binaryclassifications of the quadrants4.
The first six rowsshow evidence that each quadrant forms a distinc-tive whole, as the classifer can easily decide be-tween any two of them.Testing Size of training set1000 examples 2k 4k 8k 16kQ1 vs Q3 67% 70% 72% 73%Q2 vs Q4 61% 64% 65% 67%Q1 vs Q2 64% 66% 68% 69%Q2 vs Q3 58% 59% 59% 59%Q3 vs Q4 59% 60% 60% 61%Q4 vs Q1 69% 72% 73% 75%Q1+4 vs Q2+3 56% 58% 58% 61%Q3+4 vs Q1+2 62% 65% 67% 66%Random 49% 52% 50% 50%Q1 vs Q2+3+4 67% 72% 72% 73%Q2 vs Q1+3+4 59% 60% 63% 63%Q3 vs Q1+2+4 57% 58% 58% 59%Q4 vs Q1+2+3 60% 63% 65% 65%Micro-accuracy 61% 64% 65% 65%Table 1: Accuracy of binary classification3.2 Analysis of ResultsWe introduce now table 2 that shows two thresh-olds of significance (1% and 5%) for the interpre-tation of current and coming results.
For exam-ple, if we have 1000 trials with each trial having aprobability of success of 0.5, the likelihood of get-ting at least 53.7% of the trials right is only 1%.This gives us a baseline to see how significantlywell above chance a classifier performs.
The SVMalgorithm has linearly separated the data for eachquadrant according to lexical and POS content (thefeatures).
The most sensible explanation is that thefeatures for each class (quadrant) are semanticallyrelated, a piece of information which is relevantfor the model (see section 4).
It is safe to concludethat the results cannot be allocated to chance, thatthere is something else at work that explains the4Micro-averaged accuracy is defined as:?i (tpi + tni)?i (tpi + tni + fpi + fni)where tp stands for ?true positive?, fn for ?false negative?,etc.57Trials Prob(Success) 1% 5%1000 0.50 53.7% 52.6%750 0.50 54.3% 53.1%500 0.50 55.2% 53.6%250 0.50 57.2% 55.2%1000 0.25 28.2% 27.3%750 0.25 28.7% 27.6%500 0.25 29.6% 28.2%250 0.25 31.6% 29.6%Table 2: Statistical Significanceaccuracies consistently well above a baseline, andthis something else is the typology.
These resultsshow that the abstraction offered by the four quad-rants in the model seems correct.
This is also sup-ported by the observation that the classifier showsno improvements over the baseline if trained overa random selection of examples in the entire space.4 Experiment 2: Classification usingSemantic Orientation from AssociationOur next goal is to be able to classify a text accord-ing to more than four classes (positive/negative,active/passive), by undertaking multi-categoryclassification of texts according to particular re-gions of the space, (such as ?angry?, ?sad?, etc.).
Inorder to do that we need a scoring system for eachaxis.
In the following experiments we explore theuse of such scores and give some insights into howto transform these scores of affect as measures ofaffect.Using binary classifiers, we have already estab-lished that if we look at the lexical contents of we-blog posts tagged according to their mood by theirauthor, these mood classes tend to cluster accord-ing to a two-dimensional typology defined by theirsemantic orientation: positive or negative (evalu-ation), active or passive (activity).
Beyond aca-demic importance, the typology really becomes ofpractical interest if we can classify the posts us-ing pre-defined automated scores for both axis.One strategy of scoring is to extract phrases, in-cluding single words, which are good indicatorsof subjectivity in texts, and score them accord-ing to how they relate or ?associate?
to one or theother extremity of each axis.
This strategy, calledSemantic Orientation (SO) from Association (A)has been used successfully (Turney and Littman,2003) to classify texts or adjectives of all sorts ac-cording to their sentiments (in our typology thiscorresponds to the evaluation dimension).
Ac-cording to these scores, a text or adjective can besaid to have, for example, a more or less positiveor negative evaluation.
We will use this strategy togo further in the validation of our model of affec-tive states by scoring also the activity dimension;to our knowledge, this is the first time this strat-egy is employed to get (text) scores for dimen-sions other than evaluation.
In SO-A, we scorethe strength of the association between an indica-tor from the text and a set of positive or negativewords (the paradigms Pwords and Nwords) cap-turing the very positive/active or negative/passivesemantic orientation of the axis poles.
To get theSO-A of a text, we sum over positive scores forindicators positively related to Pwords and nega-tively related to Nwords and negative scores forindicators positively related to Nwords and nega-tively related to Pwords.
In mathematical terms,the SO-A of a text is:Text?ind(Pwords?pA(ind, p) ?Nwords?nA(ind, n))where ind stands for indicator.
Note that the quan-tity of Pwords must be equal to Nwords.To compute A, (Kamps et al , 2004) focuson the use of lexical relations defined in Word-Net5 and define a distance measure between twoterms which amounts to the length of the short-est path that connects the two terms.
This strat-egy is interesting because it constrains all valuesto belong to the [-1,+1] range, but can be appliedonly to a finite set of indicators and has yet tobe tested for the classification of texts.
(Turneyand Littman, 2003) use Pointwise Mutual Infor-mation - Information Retrieval (PMI-IR); PMI-IRoperates on a wider variety of multi-words indi-cators, allowing for contextual information to betaken into account, has been tested extensively ondifferent types of texts, and the scoring system canbe potentially normalized between [-1,+1], as wewill soon see.
PMI (Church and Hanks, 1990) be-tween two phrases is defined as:log2prob(ph1 is near ph2)prob(ph1) ?
prob(ph2)PMI is positive when two phrases tend to co-occurand negative when they tend to be in a comple-mentary distribution.
PMI-IR refers to the fact5http://wordnet.princeton.edu/.58that, as in Informtion Retrieval (IR), multiple oc-currences in the same document count as just oneoccurrence: according to (Turney and Littman,2003), this seems to yield a better measure ofsemantic similarity, providing some resistance tonoise.
Computing probabilities using hit countsfrom IR, this yields to a value for PMI-IR of:lognN ?
(hits(ph1 NEAR ph2) + 1/N)(hits(ph1) + 1) ?
(hits(ph2) + 1)where N is the total number of documents in thecorpus.
We are going to use this method for com-puting A in SO-A, which we call SO-PMI-IR.
Theconfiguration depicted in the remaining of this sec-tion follows mostly (Turney and Littman, 2003).Smoothing values (1/N and 1) are chosen so thatPMI-IR will be zero for words that are not in thecorpus, two phrases are considered NEAR if theyco-occur within a window of 20 words, and log2has been replaced by logn, since the natural log ismore common in the literature for log-odds ratioand this makes no difference for the algorithm.Two crucial aspects of the method are the choiceof indicators to be extracted from the text to beclassified, as well as the sets of positive and neg-ative words to be used as paradigms for the eval-uation and activity dimensions.
The five part-of-speech (POS) patterns from (Turney, 2002) wereused for the extraction of indicators, all involvingat least one adjective or adverb.
POS tags wereacquired with TreeTagger (Schmid, 1994)6.
Ide-ally, words used as paradigms should be contextinsensitive, i.e their semantic orientation is eitheralways positive or negative.
The adjectives good,nice, excellent, positive, fortunate, correct, supe-rior and bad, nasty, poor, negative, unfortunate,wrong, inferior were used as near pure representa-tions of positive and negative evaluation respec-tively, while fast, alive, noisy, young and slow,dead, quiet, old as near pure representations of ac-tive and passive activity (Summers, 1970).Departing from (Turney and Littman, 2003),who uses the Alta Vista advanced search with ap-proximately 350 millions web pages, we used theWaterloo corpus7, with approximately 46 millionspages.
To avoid introducing confusing heuristics,we stick to the configuration described above, but(Turney and Littman, 2003) have experimentedwith different configuation in computing SO-PMI-IR.6(Turney and Littman, 2003) uses (Brill, 1994).7http://canola1.uwaterloo.ca/.4.1 The Typology and SO-PMI-IRWe now use the typology with an automated scor-ing method for semantic orientation.
The resultsare presented in the form of a Confusion Matrix(CM).
In this and the following matrices, the top-left cell indicates the overall accuracy8, the POS-itive (ACTive) and NEGative (PASsive) columnsrepresent the instances in a predicted class, theP/T column (where present) indicates the averagenumber of patterns per text (blog post), E/P indi-cates the average evaluation score per pattern andA/P indicates the average activity score per pat-tern.
Each row represents the instances in an ac-tual class9.First, it is useful to get a clear idea of howthe SO-PMI-IR experimental setup we presentedcompares with (Turney and Littman, 2003) on ahuman-annotated set of words according to theirevaluation dimension: the General Inquirer (GI,(Stone, 1966)) lexicon is made of 3596 words(1614 positives and 1982 negatives)10.
Table 3summarizes the results.
(Turney and Littman,(U) 76.4% POS NEG E/PPOS(1614) 59.3% 40.7% 1.5NEG(1982) 9.6% 90.4% -4.3(T) 82.8% POS NEG E/PPOS(1614) 81.2% 18.8% 3.2NEG(1982) 15.8% 84.2% -3.6Table 3: CM for the GI: (U)Us and (T)(Turney andLittman, 2003)2003) reports an accuracy of 82.8% while clas-sifying those words, while our experiment yieldsan accuracy of 76.4% for the same words.
Theirresults show that their classifier errs very slightlytowards the negative pole (as shown by the accura-cies of both predicted classes) and has a very bal-anced distribution of the word scores (as shownby the almost equal but opposite in signs valuesof E/Ps).
This is some evidence that the paradigmwords are appropriate as near pure representationsof positive and negative evaluation.
By contrast,8Recall that table 2 gives an interpretation of the statisticalsignifiance of accuracy, with trials ?
750 and Prob(success)= 0.5.9For example, in the comparative evaluation shown in ta-ble 3, our classifier classified 59.3% of the 1614 positive in-stances as positive and 40.7% as negative, with an averagescore of 1.5 per pattern.10Note that all moods in the typology present in the GIhave the same polarity for evaluation in both, which is someevidence in favour of the typology.59our classifier appears to be more strongly biasedtowards the negative pole, probably due to the useof different corpora.
This bias11should be kept inmind in the interpretation of the results to come.The second experiment focuses on the wordsfrom the typology.
Table 4 shows the results.
The81.1% POS NEG P/T E/PPOS(43) 60.5% 39.5% 1 0.4NEG(47) 0.0% 100.0% 1 -6.466.7% ACT PAS P/T A/PACT(39) 33.3% 66.7% 1 -0.9PAS(51) 7.8% 92.2% 1 -2.9Table 4: CM for the Typology affective statesvalue of 1 under P/T reflects the fact that the ex-periment amounts, in practical terms, to classify-ing the annotation of the post (a single word).
Forthe evaluation dimension, there is another shift to-wards the negative pole of the axis, which suggeststhat words in the typology are distributed not ex-actly as shown on figure 1, but instead appear tohave a true location shifted towards the negativepole.
The activity dimension also appear to havea negative (i.e passive) bias.
There are two mainpossible reasons for that: words in the typologyshould be shifted towards the passive pole (as inthe evaluation case), or the paradigm words for thepassive pole are not pure representations of the ex-tremity of the pole 12.Having established that our classifier has a neg-ative bias for both axes, we now turn to the classifi-cation of the quadrants per se.
In the next section,we used SO-PMI-IR to classify 1000 randomnlyselected blog posts from our corpus, i.e 250 ineach of the four quadrants.
Some of these postswere found to have no pattern and were thereforenot classified, which means that less than 1000posts were actually classified in each experiment.We also report on the classification of an impor-tant subcategory of these moods called the Big Sixemotions.11Bias can be introduced by the use of a small corpus, inad-equate paradigm words or typology.
In practice, a quick fixfor neutralizing bias would be to normalize the SO-PMI-IRvalues by subtracting the average.
This work aims at tuningthe model to remove bias introduced by unsound paradigmwords or typology.12At the time of experimenting, we were not awareof an equivalent of the GI to independently verify ourparadigm words for activity, but one reviewer pointed outsuch a resource, see http://www.wjh.harvard.edu/?inquirer/spreadsheet_guide.htm.4.2 ResultsOf the 1000 blog posts, there were 938 with atleast one pattern.
Table 5 shows the accuracy forthe classification of these posts.56.8% POS NEG P/T E/PPOS(475) 76.2% 23.8% 10 5.2NEG(463) 63.1% 36.9% 9 3.551.8% ACT PAS P/T A/PACT(461) 20.6% 79.4% 8 -4.3PAS(477) 18.0% 82.0% 11 -4.2Table 5: CM for all MoodsAn important set of emotions found in the liter-ature (Ekman, 1972) has been termed the Big Six.These emotions are fear, anger, happiness, sad-ness, surprise and disgust.
We have used a mini-mally extended set, adding love and desire (Cowieand Cornelius, 2002), to cover all four quadrants(we called this set the Big Eight).
Fear, anger anddisgust belong to quadrant 1, sadness and surprise(we have taken it to be a synonym of ?taken aback?in the typology) belong to quadrant 2, love anddesire (taken to be synonyms of ?amorous?
and?longing?
in the typology) belong to quadrant 3and happy to quadrant 4.
Table 6 shows the resultsfor the classification of the blog posts that weretagged with one of these emotions.
This amountsto classifying the posts containing only the BigEight affective states.59.0% POS NEG P/T E/PPOS(467) 72.4% 27.6% 9 5.1NEG(351) 58.7% 41.3% 6 2.354.9% ACT PAS P/T A/PACT(357) 23.8% 76.2% 8 -4.4PAS(461) 21.0% 79.0% 8 -4.6Table 6: CM for the Big EightIn the remaining two experiments, blog postshave been classifed using a discrete scoring sys-tem.
Disregarding the real value of SO, each pat-tern was scored with a value of +1 for a positivescore and -1 for a negative score.
This amounts tocounting the number of patterns on each side andhas the advantage of providing a normalized valuefor E/T and A/T between -1 and +1.
Normalizedvalues are the first step towards a measure of af-fect, not merely a score, in the sense that it givesan estimate of the strength of affect.
We have not60classified the posts for which the resulting scorewas zero, which means that even fewer posts (741)than the previous experiment were actually evalu-ated.
Table 7 shows the results for all moods andtable 8 for the Big Eight.55.7% POS NEG P/T E/PPOS(374) 53.2% 46.8% 11 0.03NEG(367) 41.7% 58.3% 9 -0.1153.3% ACT PAS P/T A/PACT(357) 21.8% 78.2% 8 -0.3PAS(384) 17.4% 82.6% 12 -0.34Table 7: CM for all Moods: Discrete scoring59.8% POS NEG P/T E/PPOS(373) 52.3% 47.7% 10 0.01NEG(354) 32.2% 67.8% 9 -0.252.8% ACT PAS P/T A/PACT(361) 25.8% 74.2% 10 -0.3PAS(366) 20.5% 79.5% 9 -0.4Table 8: CM for the Big Eight: Discrete scoring4.3 Analysis of ResultsOur concerns about the paradigm words for eval-uating the activity dimension are clearly revealedin the classification results.
The classifier shows aheavy negative (passive) bias in all experiments.The overall accuracy for activity is consistentlybelow that for evaluation: three of them are notstatistically significant at 1% (51.8%, 53.3% and52.8%) and two at even 5% (51.8% and 52.8%).The classifier appears particularly confused in ta-ble 5, averaging a score for active posts (-4.3)smaller than for passive posts (-4.2).
It is notimpossible that the moods present in the typol-ogy may have to be shifted towards the passivedimension, but further research should look firstat finding better paradigm words for activity.
Agood starting point for the calibration of the clas-sifier for activity is the creation of a list of human-annotated words for activity, comparable in size tothe GI list, combined with an experiment similarto the one for which results are reported in table 3.With regards to the evaluation dimension, ta-bles 5 and 6 reveal a positive bias (despite having aclassifier which has a ?built-in?
negative bias, seesection 4.1).
Possible explanations for this phe-nomenon include the use of irony by people innegative posts, blogs which are expressed in morepositive terms than their annotation would suggest,and failure to detect ?negative?
contexts for pat-terns ?
one example of the latter is provided intable 9.
This phenomena appears to be alleviatedMood: bored (evaluation-)Post: gah!!
i need new music, anysuggestions?
by the way,GOOD MUSIC.Patterns: new music [JJ NN] +4.38GOOD MUSIC [JJ NN] +53.40Average SO: +57.78 (evaluation+)Table 9: Missclassified postby the use of discrete scores (see tables 7 and 8).One way of refining the scoring system is to re-duce the effect of scoring antonyms as high as syn-onyms by not counting co-occurences in the cor-pus where the word ?not?
is in the neighbourhood(Turney, 2001).
Also,The long-term goal of this research is to beable to classify texts by locating their normal-ized scores for evaluation and activity between-1 and +1, and we have suggested a simplemethod of achieving that by averaging over dis-crete scores.
However, by combining individualresults for evaluation and activity for each post13,we can already classify text into one of the fourquadrants, and we can expect the average accuracyof this classification to be approximately the prod-uct of the accuracy for each dimension.
Table 10shows the results for the classification directly intoquadrants of the 727 posts already classified intohalves (E?, A?)
in table 8.
The overall accuracyis 31.1% (expected accuracy is 59.8% * 52.8% =31.6%).
There are biases towards Q2 and Q3, butno clear cases of confusion between two or moreclasses.31.1% Q1 Q2 Q3 Q4Q1(180) 21.1% 47.8% 22.2% 8.9%Q2(174) 15.5% 51.1% 25.3% 8.0%Q3(192) 9.9% 42.2% 40.1% 7.8%Q4(181) 9.4% 33.7% 44.8% 12.2%Table 10: CM for Big Eight: Discrete scoringFinally, our experiments show no correlationbetween the length of a post (in number of pat-terns) and the accuracy of the classification.13For example, a post with E- and A+ would be classifiedin Q1.615 Conclusion and Future WorkIn this paper, we have used a machine learning ap-proach to show that there is a relation between thesemantic content of texts and the affective statethey (wish to) convey, so that a typology of affec-tive states based on semantic association is a gooddescription of the distribution of affect in a two-dimensional space.
Using automated methods toscore semantic association, we have demonstrateda method to compute semantic orientation on bothdimensions, giving some insights into how to gobeyond the customary ?sentiment?
analysis.
In theclassification experiments, accuracies were alwaysabove a random baseline, although not always sta-tistically significant.
To improve the typology andthe accuracies of classifiers based on it, a bettercalibration of the activity axis is the most press-ing task.
Our next steps are experiments aimingat refining the translation of scores to normalizedmeasures, so that individual affects can be distin-guished within a single quadrant.
Other interest-ing avenues are studies investigating how well thetypology can be ported to other textual data do-mains, the inclusion of a ?neutral?
tag, and thetreatment of texts with multiple affects.Finally, the domain of weblog posts is attractivebecause of the easy access to annotated data, butwe have found through our experiments that thecontent is very noisy, annotation is not always con-sistent among ?bloggers?, and therefore classifica-tion is difficult.
We should not underestimate thepositive effects that cleaner data, consistent tag-ging and access to bigger corpora would have onthe accuracy of the classifier.AcknowledgementThis work was partially funded by the EuropeanCommission through the Network of ExcellenceEPOCH (?Excellence in Processing Open CulturalHeritage?).
Thanks to Peter Turney for the provi-sion of access to the Waterloo MultiText System.ReferencesEric Brill.
1994.
Some advances in transformation-based part of speech tagging.
Proc.
of 12th NationalConference on AI.
pp.
722-727.
Menlo Park, CA:AAAI Press.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicog-raphy.
Computational Linguistics.
Vol.
16, No 1.pages 22?29, MIT Press, Cambridge, MA, USA.Roddy Cowie and Randolph R. Cornelius.
2002.
De-scribing the emotional states that are expressed inspeech.
Speech Communication 1228.
Elsevier Sci-ence B.V.. 20 June 2002, 28 pages.Paul Ekman.
1972.
Universal and cultural differencesin facial expression of emotion.
J.K. Cole (Eds),Nebraska Symposium on Motivation.
pp 207-282.Lincoln, University of Nebraska Press.Thorsten Joachims.
2001.
Learning to Classify TextUsing Support Vector Machines.
Kluwer AcademicPublishers.Jaap Kamps and Robert J. Mokken and Maarten Marxand Maarten de Rijke.
2004.
Using WordNet tomeasure semantic orientation of adjectives.
Proc.of LREC 2004.
Vol.
IV, pages 1115-1118.Gilad Mishne.
2005.
Experiments with mood classifi-cation in blog posts.
In Style2005 - the 1st Work-shop on Stylistic Analysis Of Text For InformationAccess, at SIGIR 2005.Charles E. Osgood, George J. Suci, and Percy H. Tan-nenbaum.
1957.
The Measurement of Meaning.University of Illinois.Klaus R. Scherer.
1984.
Emotion as a Multicompo-nent Process: A model and some cross-cultural data.In P. Shaver (Ed.)
Review of Personality and SocialPsych.
Vol.
5 (pp.
37-63).
Beverley Hills, CA: Sage.H.
Schmid.
1994.
Probabilistic part-of-speech taggingusing decision trees.
In International Conf.
on NewMethods in Language Processing.
Manchester UK.Marc Schro?der and Roddy Cowie.
2005.
Towardsemotion-sensitive multimodal interfaces.
Invitatedtalk at the Workshop on ?Adapting the interactionstyle to affective factors?
pp.
235-253.
User Mod-elling 2005, July 25, Edinburgh.Philip J.
Stone and Dexter C. Dunphy and Marshall S.Smith and Daniel M. Ogilvie.
1966.
The GeneralInquirer: A Computer Approach to Content Anal-ysis.
MIT Press.
http://www.webuse.umd.edu:9090/.Gene F. Summers.
1970.
Attitude measurement.Chicago: Rand McNally.
pp.
235-253.Peter Turney.
2001.
Mining the Web for Syn-onyms: PMI-IR versus LSA on TOEFL.
Eu-ropean Conference on Machine Learning.pp 491?502.
citeseer.nj.nec.com/turney01mining.html.Peter D. Turney.
2002.
Thumbs Up or ThumbsDown?
Semantic Orientation Applied to Unsuper-vised Classification of Reviews.
Proc.
of the ACL2002.
Philadelphia, USA.
July 8-10, 2002, pp 417-424.Peter D. Turney and Michael L. Littman.
2003.
Mea-suring praise and criticism: Inference of semanticorientation from association.
ACM Trans.
Inf.
Syst.21(4):315346.62
