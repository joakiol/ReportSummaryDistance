Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 726?736,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsImproved Semantic Parsers For If-Then StatementsI.
BeltagyThe University of Texas at Austinbeltagy@cs.utexas.eduChris QuirkMicrosoft Researchchrisq@microsoft.comAbstractDigital personal assistants are becomingboth more common and more useful.
Themajor NLP challenge for personal assis-tants is machine understanding: translat-ing natural language user commands intoan executable representation.
This paperfocuses on understanding rules written asIf-Then statements, though the techniquesshould be portable to other semantic pars-ing tasks.
We view understanding as struc-ture prediction and show improved mod-els using both conventional techniques andneural network models.
We also discussvarious ways to improve generalizationand reduce overfitting: synthetic trainingdata from paraphrase, grammar combina-tions, feature selection and ensembles ofmultiple systems.
An ensemble of thesetechniques achieves a new state of the artresult with 8% accuracy improvement.1 IntroductionThe ability to instruct computers using natural lan-guage clearly allows novice users to better usemodern information technology.
Work in se-mantic parsing has explored mapping natural lan-guage to some formal domain-specific program-ming languages such as database queries (Woods,1977; Zelle and Mooney, 1996; Berant et al,2013; Andreas et al, 2016; Yin et al, 2016),commands to robots (Kate et al, 2005), operat-ing systems (Branavan et al, 2009), and spread-sheets (Gulwani and Marron, 2014).
This pa-per explores the use of neural network models(NN) and conventional models for semantic pars-ing.
Recently approaches using neural networkshave shown great improvements in a number ofareas such as parsing (Vinyals et al, 2015), ma-chine translation (Devlin et al, 2014), and imagecaptioning (Karpathy and Fei-Fei, 2015).
We areamong the first to apply neural network methods tosemantic parsing tasks (Grefenstette et al, 2014;Dong and Lapata, 2016).There are several benchmark datasets for se-mantic parsing, the most well known of which isGeoquery (Zelle and Mooney, 1996).
We targetan If-Then dataset (Quirk et al, 2015) for sev-eral reasons.
First, it is both directly applica-ble to the end-user task of training personal dig-ital assistants.
Second, the training data, drawnfrom the site http://ifttt.com, is com-paratively quite large, containing nearly 100,000recipe-description pairs.
That said, it is several or-ders of magnitude smaller than the data for othertasks where neural networks have been successful.Machine translation datasets, for instance, maycontain billions of tokens.
NN methods appear?data-hungry?.
They require larger datasets to out-perform sparse linear approaches with careful fea-ture engineering, as evidenced in work on syntac-tic parsing (Vinyals et al, 2015).
This makes itinteresting to compare NN models with conven-tional models on this dataset.As in most prior semantic parsing attempts, wemodel natural language understanding as a struc-ture prediction problem.
Each modeling decisionpredicts some small component of the target struc-ture, conditioned on the whole input and all priordecisions.
Because this is a real-world task, thevocabulary is large and varied, with many wordsappearing only rarely.
Overfitting is a clear dan-ger.
We explore several methods to improve gen-eralization.
A classic method is to apply featureselection.
Synthetic data generated by paraphras-ing helps augment the data available.
Adjustingthe conditional structure of our model also makessense, as does creating ensembles of the best per-forming approaches.726An ensemble of the resulting systems achievesa new state-of-the-art result, with an absolute im-provement of 8% in accuracy.
We compare theperformance of a neural network model with lo-gistic regression, and explore in detail the contri-bution of each of them, and why the logistic re-gression is performing better than the neural net-work.2 Related Work2.1 Semantic ParsingSemantic parsing is the task of translating natu-ral language to a meaning representation languagethat the machine can execute.
Various seman-tic parsing tasks have been proposed before, in-cluding querying a database (Zelle and Mooney,1996), following navigation instructions (Chen,2012), translating to Abstract Meaning Represen-tation (AMR) (Artzi et al, 2015), as well as theIf-Then task we explore.
Meaning representa-tion languages vary with the task.
In databasequeries, the meaning representation language is ei-ther the native query language (e.g.
SQL or Pro-log), or some alternative that can be deterministi-cally transformed into the native query language.To follow navigation instructions, the meaningrepresentation language is comprised of sequencesof valid actions: turn left, turn right, move for-ward, etc.
For parsing If-Then rules, the meaningrepresentation is an abstract syntax tree (AST) ina very simple language.
Each root node expandsinto a ?trigger?
and ?action?
pair.
These nodes inturn expand into a set of supported triggers and ac-tions.
We model these trees as an (almost) contextfree grammar1that generates valid If-Then tasks.A number of semantic parsing approaches havebeen proposed, but most fit into the followingbroad divisions.
First, approaches driven byCombinatory Categorical Grammar (CCG) haveproven successful at several semantic parsingtasks.
This approach is attractive in that it simul-taneously provides syntactic and semantic parsesof a natural language utterance.
Syntactic struc-ture helps constrain and guide semantic interpre-tation.
CCG relies heavily on a lexicon that spec-ifies both the syntactic category and formal se-1Information at the leaves of the action may use parame-ters drawn from the trigger.
For instance, consider a rule thatsays ?text me the daily weather report.?
The trigger is a newweather report, and the action is to send an SMS.
The con-tents of that SMS are generated by the trigger, which is nolonger context free.mantics of each lexical item in the language.
Inmany instantiations, the lexicon is learned fromthe training data (Zettlemoyer and Collins, 2005)and grounds directly in the meaning representa-tion.Another approach is to view the semantic pars-ing task as a machine translation task, where thesource language is natural language commandsand the target language is the meaning represen-tation.
Several approaches have applied standardmachine translation techniques to semantic pars-ing (Wong and Mooney, 2006; Andreas et al,2013; Ratnaparkhi, 1999) with successful results.More recently, neural network approaches havebeen developed for semantic parsing, and espe-cially for querying a database.
A neural networkis trained to translate the query and the databaseinto some continuous representation then use it toanswer the query (Andreas et al, 2016; Yin et al,2016).2.2 If-Then datasetWe use a semantic parsing dataset collected fromhttp://ifttt.com, first introduced in Quirket al (2015).
This website publishes a large set ofrecipes in the form of If-Then rules.
Each recipewas authored by a website user to automate sim-ple tasks.
For instance, a recipe could send youa message every time you are tagged on a pic-ture on Facebook.
From a natural language stand-point, the most interesting part of this data is thatalongside each recipe, there is a short natural lan-guage description intended to name or advertisethe task.
This provides a naturalistic albeit oftennoisy source of parallel data for training seman-tic parsing systems.
Some of these descriptionsfaithfully represent the program.
Others are under-specified or suggestive, with many details of therecipe are not uniquely specified or omitted alto-gether.
The task is to predict the correct If-Thencode given a natural language description.As for the code, If-Then statements follow theformatI f T r i g g e r C h a n n e l .T r i g g e r F u n c t i o n ( a r g s )Then Ac t ionChanne l .A c t i o n F u n c t i o n ( a r g s )Every If-Then statement has exactly one triggerand one action.
Each trigger and action consist ofboth a channel and a function.
The channel repre-sents a connection to a service, website, or device727(e.g., Facebook, Android, or ESPN) and providesa set of functions relevant to that channel.
Finally,each of these functions may take a number of argu-ments: to receive a trigger when it becomes sunny,we need to specify the location to watch.
The re-sulting dataset after cleaning and separation con-tains 77,495 training recipes, 5,171 developmentrecipes and 4,294 testing recipes.2.3 Semantic parsing for If-Then rulesBoth CCG and MT-inspired approaches assume afairly strong correspondence between the words inthe natural language request and the concepts inthe meaning representation.
That is, most wordsin the description should correspond to some con-cept in the code, and most concepts in the codeshould correspond to some word in the descrip-tion.
However, prior work on this dataset (Quirket al, 2015) found that this strong correspondenceis often missing.
The descriptions may mentiononly the most crucial or interesting concepts; theremainder of the meaning representation must beinferred from context.
The best performing meth-ods focused primarily on generating well-formedmeaning representations, conditioning their deci-sions on the source language.Quirk et al (2015) proposed two models thatrely on a grammar to generate all valid ASTs.The first model learns a simple classifier for eachproduction in the grammar, treating the sentenceas a bag of features.
No alignment between thelanguage and meaning representation is assumed.The second method attempts to learn a correspon-dence between the language and the code, jointlylearning to select the correct productions in themeaning representation grammar.
Although thelatter approach is more appealing from a modelingstandpoint, empirically it doesn?t perform substan-tially better than the alignment-free model.
Fur-thermore the alignment-free model is much sim-pler to implement and optimize.
Therefore, webuild upon the alignment-free approach.2.4 Neural NetworksNeural network approaches have recently madegreat strides in several natural language process-ing tasks, including machine translation and de-pendency parsing.
Partially these gains are dueto better generalization ability.
Until recently,the NLP community leaned heavily on feature-rich approaches that allow models to learn com-plex relationships from data.
However, impor-IFTRIGGER ACTIONInstagramAnyNewPhotoByYouDropboxAddFileFromURLFigure 1: Derivation tree of If-Then statementof the recipe Autosave your Instagram photos toDropbox.
Arguments of the functions AnyNew-PhotoByYou and AddFileFromURL are ignored.tant features, such as indicator features for wordsand phrases, were often very sparse.
Furthermore,the best systems often relied on manually-inducedfeature combinations (Bohnet, 2010).
Multi-layerneural networks have several advantages.
Words(or, more generally, features) are first embeddedinto a continuous space where similar featuresland in nearby locations; this helps lead to lexicalgeneralization.
The additional hidden layers canmodel feature interactions in complex ways, obvi-ating the need for manual feature template induc-tion.
Feed-forward neural networks with relativelysimple structure have shown great gains in bothdependency parsing (Chen and Manning, 2014)and machine translation (Devlin et al, 2014) with-out the need for complex feature templates andlarge models.
Our NN models here are inspiredby these effective approaches.3 ApproachWe next describe the details of how If-Thenrecipes are constructed given natural language de-scriptions.
As in prior work, we treat semanticparsing as a structure prediction task.
First we de-scribe the structure and features of the model, thenexpand on the details of inference.3.1 GrammarAlong the lines of Quirk et al (2015), we builda context-free grammar baseline.
This grammargenerates only well-formed meaning representa-tions.
In the case of this dataset, meaning repre-sentations always consist of a root production withtwo children: a trigger and an action.
Both triggerand action first generate a channel, then a functionmatching that action.
Optionally we may also gen-erate the arguments of these functions; we do not728evaluate these selections as they are often idiosyn-cratic and specific to the user.
For example, therecipe Autosave your Instagram photos to Drop-box has the following meaning representation:IF I n s t a g r a m .
AnyNewPhotoByYouTHEN Dropbox .
AddFileFromURL (FileURL ={ S o u r c e U r l } ,Fi leName ={C a p t i o n } ,D r opb oxF o ld e rP a th =IFTTT / I n s t a g r a m)If we ignore the function arguments, the resultingmeaning representation is:IF I n s t a g r a m .
AnyNewPhotoByYouTHEN Dropbox .
AddFileFromURLThis examples shows also that most of the functionarguments are not crucial for the representation ofthe If-Then statement.2The grammar we use has productions corre-sponding to every channel and every function.Figure 1 shows an example derivation tree D. Thisgrammar consists of 892 productions: 128 triggerchannels, 487 trigger functions, 99 action channelsand 178 action functions.33.2 ModelOur goal is to learn a model of derivation trees Dgiven a natural sentences S. To predict the deriva-tion for a sentence, we seek the derivation D withmaximum probability given the sentence P (D|S).For the purposes of modeling, we prefer to workwith sequences rather than trees.
Given a deriva-tion tree D, we transform it into a sequence of pro-ductions R(D) = r1, .
.
.
, rnby a top-down, left-to-right tree traversal: r1is the top-most produc-tion, and rnis the bottom right production.
Thesentence S is represented as a set of features f(S).The derivation score P (D|S) is a function ofthe productions of D and those features f(S):P (D|S) =?ri?R(D)P (ri|r1, .
.
.
, ri?1, f(S)) (1)The score of a derivation tree given the sentenceis the product of probabilities of its productions.2Arguments are still important for a few If-Then recipes.For instance, in If there is snow tomorrow send a noti-fication, ?snow?
is an argument to the function Tomor-row?sForecastCallsFor.
We are not handling such cases inthis work.3For this task, it is possible to model the programs as a4-tuple, but using the grammar approach allows us to port thesame technique to other semantic parsing tasks.ri?1ri?2ri?3SriHiddenlayer(s)InputlayerOutputlayerFigure 2: Architecture of the feed-forward neuralnetworks used in this paper.
When predicting ruleri, the prior rules and the whole sentence are usedas input.
Separate parameters are learned for eachposition i.The probability of selecting production rigiventhe sentence S is dependent on the features ofthe sentence as well as the previous productionsr1, .
.
.
, ri?1; namely, all those productions thatare above and to the left of the current produc-tion.
Conditioning on previous productions helpspredicting the next one because it captures theconditional dependencies between the productionsof the derivation tree, an improvement over priorwork (Quirk et al, 2015).
In particular, we canmodel which combinations of triggers and actionsare more compatible, both function and channel.3.3 TrainingTo learn the derivation score P (D|S), weneed to learn probability of productionsP (ri|r1, .
.
.
, ri?1, f(S)).
We learn this prob-ability using a multiclass classifier where theoutput classes are the possible productions in thegrammar.
The classifier is trained to predict thenext production given previous productions andthe sentence features.Each sentence S is represented with a sparsefeature vector f(S).
We used a simple set of fea-tures: word unigrams and bigrams, character tri-grams, and Brown clusters (Liang, 2005).
Eachsentence is represented as a large sparse k-hot vec-tor, where k is the number of features representingS, |f(S)|.
We use a simple one-hot representationof prior rules.For training, we explored two approaches: astandard logistic regression classifier, and a feed729forward neural network classifier.4As for net-work structure, we evaluated models with eitherone or two 200-dimensional hidden layers (withsigmoid activation function) followed by a soft-max output layer to produce a probability for eachproduction.
We tried more than two hidden lay-ers and larger hidden layer size, but the resultswere similar or worse likely because training be-comes more difficult.
Figure 2 shows the archi-tecture of the network we use.
For training, weused a variant of stochastic gradient descent calledRMSprop (Dauphin et al, 2015) that adjusts thelearning rate for each parameter adaptively, alongwith a global learning rate of 1?3.
The mini-batch size was 100, with dropout regularizationfor hidden layers at 0.5 along with an L2 regular-izer with weight 0.005.
Each of these parameterswere tuned on the validation set, though we foundlearning to be robust to minor variations in theseparameters.
All of the neural networks were im-plemented with Theanets (Johnson, 2015).Note that history features r1, .
.
.
, ri?1in clas-sifier training are always correct.
The model isakin to a MEMM, rather than a CRF.
We make thissimplifying assumption for tractability, like manyneural network approaches (Devlin et al, 2014).3.4 InferenceWhen, at test time, we are given a new sentence,we would like to infer its most probable deriva-tion tree D. Classifiers trained as in the priorsection give probability distributions over produc-tions given the sentence and all prior productionsP (ri|r1, .
.
.
, ri?1, f(S)).
Were the distributionto be context free, we could rely on algorithmssimilar to Earley parsing (Earley, 1970) to findthe max derivation.
However, the dependency onprior productions breaks the context free assump-tion.
Therefore, we resort to approximate infer-ence, namely beam search.
Each partial hypothe-sis is grouped into a beam based on the number ofproductions it contains; we use a beam width of 8,and search for the highest scoring hypothesis.4 Improving generalizationThe data set we use for training and testing is pri-marily English but contains a broad vocabulary as4We tried the sequence-to-sequence model withLSTMs (Sutskever et al, 2014) to map word sequenceto the derivation tree productions, but the results were alwayslower than the feed forward network.
This is probablybecause of the lack of enough training data.well as many sentences from other languages suchas Chinese, Arabic, and Russian.
Thus, a seem-ingly large dataset of nearly eighty thousand ex-amples is likely to suffer from overfitting.
In thissection, we discuss a few attempts to improve gen-eralization in the sparse data setting.4.1 Synthetic data using paraphrasesArguably the best, though most expensive, way toreduce overfitting is to collect more training data.In our case, the training data available is limitedand difficult to create.
We propose to augment thetraining data in an automatic though potentiallynoisy way by generating synthetic training pairs.The main idea is that two semantically equiv-alent sentences should have the same meaningrepresentation.
Given an existing training pair,replacing the pair?s linguistic description with aparaphrase leads to a new synthetic training pair.For example, a recipe like Autosave your Insta-gram photos to Dropbox can be paraphrased to Au-tosave your Instagram pictures to Dropbox whileretaining the meaning representation:IF I n s t a g r a m .
AnyNewPhotoByYouTHEN Dropbox .
AddFileFromURL .We first explore paraphrases using WordNetsynonyms.
Every word in the sentence can be re-placed by one of its synonyms that is picked ran-domly (a word is a synonym of itself).
For wordswith multiple senses, we group all synonyms of allsenses, then retain only those synonyms alreadyin the vocabulary of the training data.
This hastwo advantages.
First, we do not increase the vo-cabulary size and therefore avoid overfitting.
Sec-ond, this acts as a simple form of word sense dis-ambiguation.
This adds around 50,000 additionaltraining examples.Next, we consider augmenting the data us-ing the Paraphrase Database (Ganitkevitch et al,2013).
Each original description is converted intoa lattice.
The original word at each position isleft in place with a constant score.
For each wordor phrase in the description found PPDB, we addone arc for each paraphrase, parameterized by thePPDB score of that phrase.
The resulting latticerepresents many possible paraphrases of the input.We select at most 10 diverse paths through thislattice using the method of Gimpel et al (2013).5This adds around 470,000 training examples.5We use a trigram language model, and a weight of 4.730IFTRIGGER ACTIONInstagramAnyNewPhotoByYou DropboxAddFileFromURLFigure 3: Derivation tree of IFTTT statementof the recipe Autosave your Instagram photos toDropbox using the second grammar.4.2 Alternative grammar formulationWe rely on a grammar to generate all valid mean-ing representations and learn models over the pro-ductions of this grammar.
Different factorizationsof the grammar lead to different model distribu-tions.
Our primary grammar is described in Sec-tion 3.1.
A second, alternate grammar formulationhas fewer levels but more productions: it com-bines the channel and function into a single pro-duction, in both the trigger and the action.
Figure 3shows an example derivation tree using this gram-mar.
The size of this grammar is 780 productions(552 triggers + 228 actions).An advantage of this grammar is that it cannotassign probability mass to invalid ASTs, where thefunction is not applicaable to the channel.
On theother hand, this grammar likely does not general-ize as well as the first grammar.
The first grammareffectively has much more data about each chan-nel, which likely improves accuracy.
Functionpredictions can condition on hopefully accuratechannel predictions.
It can also benefit from thefact that some function names are shared amongchannels.
From that perspective, the second gram-mar has fewer training instances for each outcome.4.3 Feature selectionThe training set contains approximately 77K train-ing examples, yet the number of distinct featurestypes (word unigrams and bigrams, character tri-grams, Brown clusters) is approximately 230K.Only 80K features occur in the training set morethan once.
This ratio suggests overfitting may bea major issue.Feature selection likely can improvethese issues.
We used only simple count cutoffs,including only features that occur in the trainingset more than once and more than twice.
Includingfeatures that occur more than once led to improve-ments in practice.4.4 EnsembleFinally, we explore improving generalization bybuilding ensembles of multiple systems.
Evenif systems overfit, they likely overfit in differentways.
When systems agree, they are likely toagree on the correct answer.
Combining their re-sults will suffer less from overfitting.
We use sim-ple majority voting as an ensemble strategy, re-solving ties in an arbitrary but deterministic way.5 EvaluationWe evaluate the performance of the systems byproviding the model with descriptions unseen dur-ing training.
Free parameters of the models weretuned using the development set.
The separationof data into training, development, and test fol-lows Quirk et al (2015).
Two evaluation metricsare used: accuracy on just channel selection andaccuracy of both channel and function.Two major families of approaches are consid-ered: a baseline logistic regression classifier fromscikit-learn (Pedregosa et al, 2011), as well as afeed-forward neural network.
We explore a num-ber of variations, including feature selection andgrammar formulation.5.1 Comparison systemsOur default system was described in section 3, notincluding improvements from section 4 unless oth-erwise noted.
The grammar uses the primary for-mulation from section 3.1.
Neural network mod-els use a single hidden layer by default; we alsoexplore two hidden layers.We evaluate two approaches for generating syn-thetic data.
The first approach, leaning primar-ily on WordNet to generate up to one paraphrasefor each instance, is labeled WN.
The second ap-proach using Paraphrase Database to generate upto ten paraphrases is labeled PPDB.The Alternate grammar line uses the section 4.2grammar, and otherwise default configurations (nosynthetic data, single hidden layer for NN).Feature selection again uses the default config-uration, but uses only those features that occurredmore than once in the training data.Finally we explore ensembles of all approaches.First, we combine all variations within the samemodel family; next, we bring all systems together.To evaluate the impact of individual systems, wealso present results with specific systems removed.731SystemChannel accuracy Full tree accuracyNN LR NN LRQuirk et al (2015) w/o alignment - 46.30 - 33.00Quirk et al (2015) with alignment - 47.40 - 34.50Default configurations 52.93 53.73 39.66 41.87Two hidden layers 46.81 - 32.77 -No hidden layers 50.05 - 38.47 -Synthetic data (WN) 52.45 53.68 38.64 41.55Synthetic data (PPDB) 51.86 52.96 38.86 40.63Alternate grammar 50.09 52.42 39.10 41.15Feature selection 52.91 53.31 39.29 41.34Ensemble of systems above 53.98 53.73 41.06 41.85Ensemble NN + LR 54.31 42.55Ensemble NN + LR (w/o alternate grammar) 54.38 41.90Ensemble NN + LR (w/o synthetic data) 53.98 42.41Table 1: Accuracy of the Neural Network (NN) and Logistic Regression (LR) implementations of oursystem with various configurations.
Channel-only and full tree (channel+function) accuracies are listed.5.2 ResultsTable 1 shows the accuracy of each evaluated sys-tem, and Table 2 explores system performance onimportant subsets of the data.
The first columnspresent accuracy of just the channel, and the lastcolumns present the channel and the function to-gether (the full derivation).
We achieve new state-of-the-art results, showing a 7% absolute improve-ment on the channel-only accuracy and 8% abso-lute improvement on the full derivation tree in themost difficult condition.5.3 DiscussionPartly these improved results are driven by betterfeatures.
Adding more robust representations ofthe input (e.g.
Brown clusters) and conditioningon prior structure of the tree leads to more consis-tent and coherent trees.One key observation is that the logistic regres-sion classifier consistently outperforms the neu-ral network, though by a small margin.
We sus-pect two main causes: optimization difficultiesand training size.
To compare the optimization al-gorithms, Table 1 shows the result of a neural net-work with no hidden layers, which is effectivelyidentical to a logistic regression model.
Stochasticgradient descent used to train the neural networkdid not perform as well as the LIBLINEAR (Fanet al, 2008) solver used to train the logistic re-gression, because the loss function was not opti-mized as well.
Optimization problems are evenmore likely with hidden layers, since the objectiveis no longer convex.Second, the training data is small by neural net-work standards.
Prior attempts to use neural net-works for parsing required larger amounts of train-ing data to exceed the state-of-the-art.
Non-linearmodels are able to capture regularities that linearmodels cannot, but may require more training datato do so.
Table 1 shows that a network with a sin-gle hidden layer outperforms a one with two hid-den layers.
This additional hidden layer seems tomake learning harder (even with layer-wise pre-training).
We also ran an additional experiment,limiting both NN and LR to use word unigramfeatures, and varying the vocabulary size by fre-quency thresholding; the results are in table 3.
LRmodels were more effective when all features werepresent, likely due to their convex objective andsimple regularization.
NN models, on the otherhand, actually outperform LR models when lim-ited to more common vocabulary items.
Givenmore data, NN could likely find representationsthat outperformed manual feature engineering.Although we only considered feed-forwardnerual networks, results on recurrent architecturesDong and Lapata (2016) are in accordance withour findings.
Their LSTM-based approach doesnot achieve great gains on this data set because:?user curated descriptions are often of low quality,and thus align very loosely to their correspond-ing ASTs?.
Even though this training set is largerthan other semantic parsing datasets, the vocabu-lary, sentence structures, and even languages hereare much more diverse, which make it difficult forthe NN to learn useful representations.
Dong andLapata (2016) tried to reduce the impact of thisproblem by evaluating only on the English sub-732Channel Full treeAll: 4,294 recipesposclass 47.4 34.5D&L ?
?NN 52.9 39.7LR 53.7 41.9Ensemble 54.3 42.6oracleturk 48.8 37.8Omit non-English: 3,744 recipesposclass 50.0 36.9D&L 54.3 39.2NN 55.1 41.2LR 56.0 44.3Ensemble 56.8 44.5oracleturk 56.0 43.5Omit non-English, unintelligible: 2,433 recipesposclass 67.2 50.4D&L 68.8 50.5NN 71.3 53.7LR 71.9 56.6Ensemble 72.7 57.1oracleturk 86.2 59.4?3 agree with gold: 760 recipesposclass 81.4 71.0D&L 87.8 75.2NN 88.0 74.3LR 88.8 82.5Ensemble 89.1 82.2oracleturk 100.0 100.0Table 2: System comparisons on various subsetsof the data.
Following Quirk et al (2015), we alsoevaluation on illustrative subsets.
?posclass?
rep-resents the best system from prior work.
D&L isthe best-performing system from Dong and Lapata(2016).
NN and LR are the single best neural net-work, logistic regression models, and Ensemble isthe combination of all systems.
?oracleturk?
rep-resents cases where at least one turker agreed withthe gold standard.set of the data.
Interestingly, our carefully builtfeed-forward networks outperform their approachin almost every subset.Although the neural network with one hiddenlayer does not outperform logistic regression in afeature rich setting, it makes substantially differentpredictions.
An ensemble of their outputs achievesbetter accuracy than either system individually.Our techniques for improving generalization donot improve individual systems.
Yet when all tech-niques are combined in an ensemble, the resultingpredictions are better.
Furthermore, an ensemblewithout the synthetic data or without the alternategrammar has lower accuracy: each technique con-tributes to the final result.SystemFull tree accuracyNN LRAll words 35.79 37.03Count ?
2 37.01 36.91Count ?
3 37.07 36.59Table 3: Accuracy of NN and LR limited to wordunigram features, with three vocabulary sizes: allwords, words occurring at least twice in the train-ing data (13,971 words), and those occurring atleast three times in the training data (8,974 words).5.4 Comparison of logistic regression andneural network approachesWe performed a detailed exploration of the caseswhere either the LR model was correct and the NNmodel was wrong, or vice versa.
Table 4 breaksthese errors into a number of cases:?
Swapped trigger and action.
Here the sys-tem misinterpreted a rule, swapping the trig-ger for the action.
An example NN swap was?Backup Pinboard entries to diigo?
; an exam-ple LR swap was ?Like a photo on tumblr andupload it to your flickr photostream .??
Duplicated.
In this case, the system used thesame channel for both trigger and action, de-spite clear evidence in the language.
For in-stance, the LR model incorrectly used Face-book as both the trigger and channel in thisrecipe: ?New photo on Facebook addec tomy Pryv?.
The NN model correctly identifiedPryv as the target channel, despite the typo inthe recipe.?
Missed word cue.
In many cases there was aclear ?cue word?
in the language that shouldhave forced a correct channel, but the modelpicked the wrong one.
For instance, in ?tweet# stared youtube video?, the trigger should bestarred YouTube videos, but the NN modelincorrectly selected feeds.?
Missed multi-word cue.
Sometimes thecue was a multi-word phrase, such as ?OneDrive?.
The NN model tended to miss thesecues.?
Missed inference.
In certain cases the cuewas more of a loose inference.
Words suchas ?payment?
and ?refund?
should tend to733NN LRError type errors errorsSwapped trigger and action 4 4Duplicated 3 4Missed word cue 8 8Missed multi-word cue 2 0Missed inference 8 0Related channel 5 8Grand Total 30 24Table 4: Count of error cases by type for NN andLR models, in their default configurations.
Thistable only counts those instances in the most cleanset (where three or more turkers agree with thegold program) where exactly one system made anerror.refer to triggers from the Square paymentprovider; the NN seemed to struggle on thesecases.?
Related channel.
Often the true channel isvery difficult to pick: should the system useiOS location or Android location?
NN mod-els seemed to do better on these cases, per-haps picking up on some latent cues in thedata that were not immediately evident to theauthors.In general, a slightly more powerful NN modelwith access to more relevant data might overcomesome of the issues above.We also explored correlations with errors anda number of other criteria, such as text length andfrequency of the channels and functions, but foundno substantial differences.
In general, the remain-ing errors are often plausible given the noisy input.6 Future WorkWe have achieved a new state-of-the-art on thisdataset, though derivation tree accuracy remainslow, around 42%.
While some errors are causedby training data noise and others are due to noisytest instances, there is still room for improvement.We believe synthetic data is a promising direc-tion.
Initial attempts show small improvements;better results may be within reach given more tun-ing.
This may enable gains with recurrent archi-tectures (e.g., LSTMs).The networks here rely primarily on word-basedfeatures.
Character-based models have resulted inimproved syntactic parsing results (Ballesteros etal., 2015).
We believe that noisy data such as theIf-Then corpus would benefit from character mod-elings, since the models could be more robust tospelling errors and variations.Another important future work direction is tomodel the arguments of the If-Then statements.However, that requires segmenting the argumentsinto those that are general across all users, andthose that are specific to the recipe?s author.
Likelythis would require further annotation of the data.7 ConclusionIn this paper, we address a semantic parsing task,namely translating sentences to If-Then state-ments.
We model the task as structure prediction,and show improved models using both neural net-works and logistic regression.
We also discussedvarious ways to improve generalization and re-duce overfitting, including adding synthetic train-ing data by paraphrasing sentences, using multiplegrammars, applying feature selection and ensem-bling multiple systems.
We achieve a new state-of-the-art with 8% absolute accuracy improvement.ReferencesJacob Andreas, Andreas Vlachos, and Stephen Clark.2013.
Semantic parsing as machine translation.
InProceedings of the 51st Annual Meeting of the As-sociation for Computational Linguistics (Volume 2:Short Papers), pages 47?52, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Jacob Andreas, Marcus Rohrbach, Trevor Darrell, andDan Klein.
2016.
Learning to compose neural net-works for question answering.
In NAACL 2016.Yoav Artzi, Kenton Lee, and Luke Zettlemoyer.
2015.Broad-coverage CCG semantic parsing with AMR.In Proceedings of the 2015 Conference on Empiri-cal Methods in Natural Language Processing, pages1699?1710, Lisbon, Portugal, September.
Associa-tion for Computational Linguistics.Miguel Ballesteros, Chris Dyer, and Noah A. Smith.2015.
Improved transition-based parsing by mod-eling characters instead of words with LSTMs.
InProceedings of the 2015 Conference on EmpiricalMethods in Natural Language Processing, pages349?359, Lisbon, Portugal, September.
Associationfor Computational Linguistics.Jonathan Berant, Andrew Chou, Roy Frostig, and PercyLiang.
2013.
Semantic parsing on Freebase fromquestion-answer pairs.
In Proceedings of Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP-13).734Bernd Bohnet.
2010.
Top accuracy and fast depen-dency parsing is not a contradiction.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics (Coling 2010), pages 89?97, Bei-jing, China, August.
Coling 2010 Organizing Com-mittee.S.R.K.
Branavan, Harr Chen, Luke S. Zettlemoyer, andRegina Barzilay.
2009.
Reinforcement learning formapping instructions to actions.
In Joint Conferenceof the 47th Annual Meeting of the Association forComputational Linguistics and the 4th InternationalJoint Conference on Natural Language Processingof the Asian Federation of Natural Language Pro-cessing (ACL-IJCNLP), Singapore.Danqi Chen and Christopher D Manning.
2014.
A fastand accurate dependency parser using neural net-works.
In Proceedings of Conference on EmpiricalMethods in Natural Language Processing (EMNLP-14).David L Chen.
2012.
Fast online lexicon learningfor grounded language acquisition.
In Proceedingsof the 50th Annual Meeting of the Association forComputational Linguistics: Long Papers-Volume 1,pages 430?439.
Association for Computational Lin-guistics.Yann N Dauphin, Harm de Vries, Junyoung Chung,and Yoshua Bengio.
2015.
RMSProp and equili-brated adaptive learning rates for non-convex opti-mization.
arXiv preprint arXiv:1502.04390.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for sta-tistical machine translation.
In Proceedings of the52nd Annual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers), pages1370?1380, Baltimore, Maryland, June.
Associationfor Computational Linguistics.Li Dong and Mirella Lapata.
2016.
Lan-guage to logical form with neural attention.
InarXiv:1601.01280.Jay Earley.
1970.
An efficient context-free parsing al-gorithm.
Commun.
ACM, 13(2):94?102, February.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
LIBLINEAR:A library for large linear classification.
Journal ofMachine Learning Research, 9:1871?1874.Juri Ganitkevitch, Benjamin Van Durme, and ChrisCallison-Burch.
2013.
PPDB: The paraphrasedatabase.
In Proceedings of the 2013 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 758?764, Atlanta, Georgia, June.Association for Computational Linguistics.Kevin Gimpel, Dhruv Batra, Chris Dyer, and GregoryShakhnarovich.
2013.
A systematic exploration ofdiversity in machine translation.
In Proceedings ofthe 2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1100?1111, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.Edward Grefenstette, Phil Blunsom, Nando de Freitas,and Karl Moritz Hermann.
2014.
A deep archi-tecture for semantic parsing.
In Proceedings of theACL 2014 Workshop on Semantic Parsing, pages22?27, Baltimore, MD, June.
Association for Com-putational Linguistics.Sumit Gulwani and Mark Marron.
2014.
Nlyze: Inter-active programming by natural language for spread-sheet data analysis and manipulation.
In SIGMOD.Leif Johnson.
2015.
Theanets.
https://github.com/lmjohns3/theanets.Andrej Karpathy and Li Fei-Fei.
2015.
Deep visual-semantic alignments for generating image descrip-tions.
In The IEEE Conference on Computer Visionand Pattern Recognition (CVPR), June.R.
J. Kate, Y. W. Wong, and R. J. Mooney.
2005.Learning to transform natural to formal languages.In Proceedings of the Twentieth National Confer-ence on Artificial Intelligence (AAAI-05), pages1062?1068, Pittsburgh, PA, July.Percy Liang.
2005.
Semi-supervised learning for nat-ural language.
Ph.D. thesis, Massachusetts Instituteof Technology.F.
Pedregosa, G. Varoquaux, A. Gramfort, V. Michel,B.
Thirion, O. Grisel, M. Blondel, P. Pretten-hofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Pas-sos, D. Cournapeau, M. Brucher, M. Perrot, andE.
Duchesnay.
2011.
Scikit-learn: Machine learn-ing in Python.
Journal of Machine Learning Re-search, 12:2825?2830.Chris Quirk, Raymond Mooney, and Michel Galley.2015.
Language to code: Learning semantic parsersfor if-this-then-that recipes.
In Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics.
Association for ComputationalLinguistics.Adwait Ratnaparkhi.
1999.
Learning to parse naturallanguage with maximum entropy models.
Machinelearning, 34(1-3):151?175.Ilya Sutskever, Oriol Vinyals, and Quoc VV Le.
2014.Sequence to sequence learning with neural net-works.
In Advances in Neural Information Process-ing Systems, pages 3104?3112.Oriol Vinyals, ?ukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2015.
Gram-mar as a foreign language.
In C. Cortes, N.D.Lawrence, D.D.
Lee, M. Sugiyama, R. Garnett, andR.
Garnett, editors, Advances in Neural InformationProcessing Systems 28, pages 2755?2763.
CurranAssociates, Inc.735Yuk Wah Wong and Raymond Mooney.
2006.
Learn-ing for semantic parsing with statistical machinetranslation.
In Proceedings of the Human LanguageTechnology Conference of the NAACL, Main Con-ference, pages 439?446, New York City, USA, June.Association for Computational Linguistics.William A.
Woods.
1977.
Lunar rocks in naturalEnglish: Explorations in natural language questionanswering.
In Antonio Zampoli, editor, LinguisticStructures Processing.
Elsevier North-Holland, NewYork.Pengcheng Yin, Zhengdong Lu, and Ben Kao Hang Li.2016.
Neural enquirer: Learning to query tableswith natural language.
In ICLR 2016.John M. Zelle and Raymond J. Mooney.
1996.
Learn-ing to parse database queries using inductive logicprogramming.
In Proceedings of the Thirteenth Na-tional Conference on Artificial Intelligence (AAAI-96), pages 1050?1055, Portland, OR, August.Luke S. Zettlemoyer and Michael Collins.
2005.Learning to map sentences to logical form: Struc-tured classification with probabilistic categorialgrammars.
In In Proceedings of the 21st Conferenceon Uncertainty in AI, pages 658?666.736
