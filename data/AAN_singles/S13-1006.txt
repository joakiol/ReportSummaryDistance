Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 53?58, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsiKernels-Core: Tree Kernel Learning for Textual SimilarityAliaksei Severyn1 and Massimo Nicosia1 and Alessandro Moschitti1,21University of Trento, DISI, 38123 Povo (TN), Italy{severyn,m.nicosia,moschitti}@disi.unitn.it2Qatar Foundation, QCRI, Doha, Qatar{amoschitti}@qf.org.qaAbstractThis paper describes the participation of iKer-nels system in the Semantic Textual Similar-ity (STS) shared task at *SEM 2013.
Differentfrom the majority of approaches, where a largenumber of pairwise similarity features areused to learn a regression model, our modeldirectly encodes the input texts into syntac-tic/semantic structures.
Our systems rely ontree kernels to automatically extract a rich setof syntactic patterns to learn a similarity scorecorrelated with human judgements.
We ex-periment with different structural representa-tions derived from constituency and depen-dency trees.
While showing large improve-ments over the top results from the previousyear task (STS-2012), our best system ranks21st out of total 88 participated in the STS-2013 task.
Nevertheless, a slight refinement toour model makes it rank 4th.1 IntroductionComparing textual data to establish the degree of se-mantic similarity is of key importance in many Nat-ural Language Processing (NLP) tasks ranging fromdocument categorization to textual entailment andsummarization.
The key aspect of having an accu-rate STS framework is the design of features that canadequately represent various aspects of the similar-ity between texts, e.g.
using lexical, syntactic andsemantic similarity metrics.The majority of approaches to semantic textualsimilarity treat the input text pairs as feature vec-tors where each feature is a score corresponding to acertain type of similarity.
This approach is concep-tually easy to implement and STS-2012 (Agirre etal., 2012) has shown that the best systems were builtfollowing this idea, i.e.
a number of features encod-ing similarity of an input text pair were combined ina single scoring model, such as Linear Regressionor Support Vector Regression (SVR).
One potentiallimitation of using only similarity features to repre-sent a text pair is that of low representation power.The novelty of our approach is that we encode theinput text pairs directly into structural objects, e.g.trees, and rely on the power of kernel learning to ex-tract relevant structures.
This completely differentfrom (Croce et al ), where tree kernels where usedto establish syntactic similarity and then plugged assimilarity features.
To link the documents in a pairwe mark the nodes in the related structures with aspecial relational tag.
In this way effective struc-tural relational patterns are implicitly encoded in thetrees and can be automatically learned by the kernel-based machine learning methods.
We build our sys-tems on top of the features used by two best systemsfrom STS-2012 and combine them with the tree ker-nel models within the Support Vector Regression toderive a single scoring model.
Since the test dataused for evaluation in STS-2013 (Agirre et al 2013)is different from the 2012 data provided for the sys-tem development, domain adaptation represents anadditional challenge.
To address this problem weaugment our feature vector representation with fea-tures extracted from a text pair as a whole to captureindividual properties of each dataset.
Additionally,we experiment with a corpus type classifier and in-clude its prediction score as additional features.
Fi-nally, we use stacking to combine several structuralmodels into the feature vector representation.53In the following sections we describe our ap-proach to combine structural representations withthe pairwise similarity features in a single SVRlearning framework.
We then report results on bothSTS-2012 and 2013 tasks.2 Structural Relational SimilarityIn this section we first describe the kernel frameworkto combine structural and vector models, then weexplain how to construct the tree models and brieflydescribe tree kernels we use to automatically extractthe features.2.1 Structural Kernel LearningIn supervised learning, given the labeled data{(xi, y i)}ni=1, the goal is to estimate a decision func-tion h(x) = y that maps input examples to the tar-get variables.
A conventional approach is to rep-resent a pair of texts as a set of similarity features{fi}, s.t.
the predictions are computed as h(x) =w ?
x =?iwifi, wherew is the model weight vec-tor.
Hence, the learning problem boils down to es-timating the individual weight of each of the sim-ilarity feature fi.
One downside of such approachis that a great deal of similarity information carriedby a given text pair is lost when modeled by singlereal-valued scores.A more versatile approach in terms of the inputrepresentation relies on kernels.
In a typical ker-nel machine, e.g.
SVM, the prediction function fora test input x takes on the following form h(x) =?i ?iyiK(x,xi), where ?i are the model parame-ters estimated from the training data, yi - target vari-ables, xi are support vectors, and K(?, ?)
is a kernelfunction.To encode both structural representation and sim-ilarity feature vectors of input text pairs xi in a sin-gle model, we treat it as the following tuple: xi =?xai ,xbi?
= ?
(tai , vai ), (tbi , vbi)?, where xai xbi are thefirst and the second document of xi, and t and v de-note tree and vector representations respectively.To compute a kernel between two text pairs xiand xj we define the following all-vs-all kernel,where all possible combinations of documents fromeach pair are considered: K(xi,xj) = K(xai ,xaj ) +K(xai ,xbj) + K(xbi ,xaj ) + K(xbi ,xbj).
Each of thekernel computations K between two documents xaand xb can be broken down into the following:K(xa,xb) = KTK(ta, tb) + Kfvec(va, vb), whereKTK computes a tree kernel and Kfvec is a kernelover feature vectors, e.g.
linear, polynomial or RBF,etc.
Further in the text we refer to structural treekernel models as TK and explicit feature vector rep-resentation as fvec.Having defined a way to jointly model text pairsusing structural TK representations along with thesimilarity features fvec, we next briefly review treekernels and our relational structures derived fromconstituency and dependency trees.2.2 Tree KernelsWe use tree structures as our base representationsince they provide sufficient flexibility in represen-tation and allow for easier feature extraction than,for example, graph structures.
We use a Partial TreeKernel (PTK) (Moschitti, 2006) to take care of auto-matic feature extraction and compute KTK(?, ?
).PTK is a tree kernel function that can be ef-fectively applied to both constituency and depen-dency parse trees.
It generalizes a subset tree ker-nel (STK) (Collins and Duffy, 2002) that maps atree into the space of all possible tree fragments con-strained by the rule that the sibling nodes from theirparents cannot be separated.
Different from STKwhere the nodes in the generated tree fragments areconstrained to include none or all of their direct chil-dren, PTK fragments can contain any subset of thefeatures, i.e.
PTK allows for breaking the productionrules.
Consequently, PTK generalizes STK generat-ing an extremely rich feature space, which results inhigher generalization ability.2.3 Relational StructuresThe idea of using relational structures to jointlymodel text pairs was previously proposed in (Sev-eryn and Moschitti, 2012), where shallow syntacticstructures derived from chunks and part-of-speechtags were used to represent question/answer pairs.In this paper, we define novel relational structuresbased on: (i) constituency and (ii) dependency trees.Constituency tree.
Each document in a given textpair is represented by its constituency parse tree.If a document contains multiple sentences they aremerged in a single tree with a common root.
Toencode the structural relationships between docu-54Figure 1: A dependency-based structural representation of a text pair.
REL tag links related fragments.ments in a pair a special REL tag is used to linkthe related structures.
We adopt a simple strategyto establish such links: words from two documentsthat have a common lemma get their parents (POStags) and grandparents, non-terminals, marked witha REL tag.Dependency tree.
We propose to use dependencyrelations between words to derive an alternativestructural representation.
In particular, dependencyrelations are used to link words in a way that wordsare always at the leaf level.
This reordering of thenodes helps to avoid the situation where nodes withwords tend to form long chains.
This is essentialfor PTK to extract meaningful fragments.
We alsoplug part-of-speech tags between the word nodesand nodes carrying their grammatical role.
Againa special REL tag is used to establish relations be-tween tree fragments.
Fig.
1 gives an example ofa dependency-based structure taken from STS-2013headlines dataset.3 Pairwise similarity features.Along with the direct representation of input textpairs as structural objects our framework also en-codes feature vectors (base), which we describebelow.3.1 Baseline featuresWe adopt similarity features from two best perform-ing systems of STS-2012, which were publicly re-leased: namely, the Takelab1 system (S?aric?
et al2012) and the UKP Lab?s system2 (Bar et al 2012).Both systems represent input texts with similar-1http://takelab.fer.hr/sts/2https://code.google.com/p/dkpro-similarity-asl/wiki/SemEval2013ity features which combine multiple text similaritymeasures of varying complexity.UKP provides metrics based on matching of char-acter, word n-grams and common subsequences.
Italso includes features derived from Explicit Seman-tic Analysis vector comparisons and aggregation ofword similarity based on lexical-semantic resources,e.g.
WordNet.
In total it provides 18 features.Takelab includes n-gram matching of varying size,weighted word matching, length difference, Word-Net similarity and vector space similarity wherepairs of input sentences are mapped into Latent Se-mantic Analysis (LSA) space (Turney and Pantel,2010).
The features are computed over several sen-tence representations where stop words are removedand/or lemmas are used in place of raw tokens.The total number of Takelab?s features is 21.
Eventhough some of the UKP and Takelab features over-lap we include all of them in a combined system withthe total of 39 features.3.2 iKernels featuresHere we describe our additional features added tothe fvec representation.
First, we note that wordfrequencies used to compute weighted word match-ings and the word-vector mappings to compute LSAsimilarities required by Takelab features are pro-vided only for the vocabulary extracted from 2012data.
Hence, we use both STS-2012 and 2013 data toobtain the word counts and re-estimate LSA vectorrepresentations.
For the former we extract unigramcounts from Google Books Ngrams3, while for thelatter we use additional corpora as described below.LSA similarity.
To construct LSA word-vectormappings we use the following three sources: (i)3http://storage.googleapis.com/books/ngrams/books/datasetsv2.html55Aquaint4, which consists of more than 1 millionnewswire documents, (ii) ukWaC (Baroni et al2009) - a 2 billion word corpus constructed fromthe Web, and (iii) and a collection of documentsextracted from Wikipedia dump5.
To extract LSAtopics we use GenSim6 software.
We preprocessthe data by lowercasing, removing stopwords andwords with frequency lower than 5.
Finally, we ap-ply tf-idf weighting.
For all representations we fixthe number of dimensions to 250.
For all corporawe use document-level representation, except forWikipedia we also experimented with a sentence-level document representation, which typically pro-vides a more restricted context for estimating word-document distributions.Brown Clusters.
In addition to vector represen-tations derived from LSA, we extract word-vectormappings using Brown word clusters7 (Turian et al2010), where words are organized into a hierarchyand each word is represented as a bit-string.
Weencode each word by a feature vector where eachentry corresponds to a prefix extracted from its bit-string.
We use prefix lengths in the following range:k = {4, 8, 12, 16, 20}.
Finally, the document is rep-resented as a feature vector composed by the indi-vidual word vectors.Term-overlap features.
In addition to the wordoverlap features computed by UKP and Takelabsystems we also compute a cosine similarity overthe following representations: (i) n-grams of part-of-speech tags (up to 4-grams), (ii) SuperSensetags (Ciaramita and Altun, 2006), (iii) named enti-ties, and (iv) dependency triplets.PTK similarity.
We use PTK to provide a syn-tactic similarity score between documents in a pair:PTK(a, b) = PTK(a, b), where as input represen-tations we use dependency and constituency trees.Explicit Semantic Analysis (ESA) similarity.ESA (Gabrilovich and Markovitch, 2007) representsinput documents as vectors of Wikipedia concepts.To compute ESA features we use Lucene8 to in-dex documents extracted from a Wikipedia dump.Given a text pair we retrieve k top documents (i.e.4http://www.ldc.upenn.edu/Catalog/docs/LDC2002T31/5http://dumps.wikimedia.org/6http://radimrehurek.com/gensim/7http://metaoptimize.com/projects/wordreprs/8http://lucene.apache.org/Wikipedia concepts) and compute the metric bylooking at the overlap of the concepts between thedocuments: esak(a, b) =|Wa?Wb|k , where Wa isthe set of concepts retrieved for document a. Wecompute esa features with k ?
{10, 25, 50, 100}.3.3 Corpus type featuresHere we describe two complementary approaches(corpus) in an attempt to alleviate the problem ofdomain adaptation, where the datasets used for train-ing and testing are drawn from different sources.Pair representation.
We treat each pair of texts as awhole and extract the following sets of corpus fea-tures: plain bag-of-words, dependency triplets, pro-duction rules of the syntactic parse tree and a lengthfeature, i.e.
a log-normalized length of the combinedtext.
Each feature set is normalized and added to thefvec model.Corpus classifier.
We use the above set of featuresto train a multi-class classifier to predict for each in-stance its most likely corpus type.
Our categoriescorrespond to five dataset types of STS-2012.
Pre-diction scores for each of the dataset categories arethen plugged as features into the final fvec repre-sentation.
Our multi-class classifier is a one-vs-allbinary SVM trained on the merged data from STS-2012.
We apply 5-fold cross-validation scheme, s.t.for each of the held-out folds we obtain independentpredictions.
The accuracy (averaged over 5-folds)on the STS-2012 data is 92.0%.3.4 StackingTo integrate multiple TK models into a single modelwe apply a classifier stacking approach (Fast andJensen, 2008).
Each of the learned TK models isused to generate predictions which are then pluggedas features into the final fvec representation, s.t.the final model uses only explicit feature vectorrepresentation.
We apply a 5-fold cross-validationscheme to obtain prediction scores in the same man-ner as described above.4 Experimental Evaluation4.1 Experimental setupTo encode TK models along with the similarity fea-ture vectors into a single regression scoring model,56base corpus TKU T I B O M C D ALL Mean MSRp MSRv SMTe OnWN SMTn?
0.7060 0.6087 0.6080 0.8390 0.2540 0.6820 0.4470?
0.7589 0.6863 0.6814 0.8637 0.4950 0.7091 0.5395?
?
0.8079 0.7161 0.7134 0.8837 0.5519 0.7343 0.5607?
?
?
0.8187 0.7137 0.7157 0.8833 0.5131 0.7355 0.5809?
?
?
?
0.8458 0.7047 0.6935 0.8953 0.5080 0.7101 0.5834?
?
?
?
0.8468 0.6954 0.6717 0.8902 0.4652 0.7089 0.6133?
?
?
?
?
0.8539 0.7132 0.6993 0.9005 0.4772 0.7189 0.6481?
?
?
?
?
0.8529 0.7249 0.7080 0.8984 0.5142 0.7263 0.6700Sys1 ?
?
?
?
?
?
0.8546 0.7156 0.6989 0.8979 0.4884 0.7181 0.6609Sys3 ?
?
?
?
?
?
0.8810 0.7416 0.7210 0.8971 0.5912 0.7328 0.6778Sys2 ?
?
?
?
?
?
0.8705 0.7339 0.7039 0.9012 0.5629 0.7376 0.6656UKPbest 0.8239 0.6773 0.6830 0.8739 0.5280 0.6641 0.4937Table 1: System configurations and results on STS-2012.
Column set base lists 3 feature sets : UKP (U), Takelab(T) and iKernels (I); corpus type features (corpus) include plain features (B), corpus classifier (O), and manuallyencoded dataset category (M); TK contains constituency (C) and dependency-based (D) models.
UKPbest is the bestsystem of STS-2012.
First column shows configuration of our three system runs submitted to STS-2013.we use an SVR framework implemented in SVM-Light-TK9.
We use the following parameter settings-t 5 -F 3 -W A -C +, which specifies to usea combination of trees and feature vectors (-C +),PTK over trees (-F 3) computed in all-vs-all mode(-W A) and using polynomial kernel of degree 3 forthe feature vector (active by default).We report the following metrics employed in thefinal evaluation: Pearson correlation for individualtest sets10 and Mean ?
an average score weighted bythe test set size.4.2 STS-2012For STS-2013 task the entire data from STS-2012was provided for the system development.
To com-pare with the best systems of the previous year wefollowed the same setup, where 3 datasets (MSRp,MSRv and SMTe) are used for training and 5 for test-ing (two ?surprise?
datasets were added: OnWN andSMTn).
We use the entire training data to obtain asingle model.Table 1 summarizes the results using structuralmodels (TK), pairwise similarity (base) and corpustype features (corpus).
We first note, that com-bining all three features sets (U, T and I) providesa good match to the best system UKPbest.
Next,adding TK models results in a large improvementbeating the top results in STS-2012.
Furthermore,using corpus features results in even greater im-9http://disi.unitn.it/moschitti/Tree-Kernel.htm10for STS-2012 we also report the results for a concatenationof all five test sets (ALL)provement with the Mean = 0.7416 and PearsonALL = 0.8810.4.3 STS-2013Below we specify the configuration for each of thesubmitted runs (also shown in Table 1) and report theresults on the STS-2013 test sets: headlines (head),OnWN, FNWN, and SMT:Sys1: combines base features (U, T and I), TKmodels (C and D) and plain corpus type features (B).We use STS-2012 data to train a single model.Sys2: different from Sys1 where a single modeltrained on the entire data is used to make predictions,we adopt a different training/test setup to account forthe different nature of the data used for training andtesting.
After performing manual analysis of the testdata we came up with the following strategy to splitthe training data into two sets to learn two differ-ent models: STMe and OnWN (model1) and MSRp,SMTn and STMe (model2); model1 is then used toget predictions for OnWN, FNWN, while model2 isused for SMT and headlines.Sys3: same as Sys1 + a corpus type classifier O asdescribed in Sec.
3.3.Table 2 shows the resulting performance of oursystems and the best UMBC system published in thefinal ranking.
Sys2 appears the most accurate amongour systems, which ranked 21st out of 88.
Compar-ing to the best system across four datasets we ob-serve that it performs reasonably well on the head-lines dataset (it is 5th best), while completely failson the OnWN and FNWN test sets.
After performing57error analysis, we found that TK models underper-form on FNWN and OnWN sets, which appear un-derrepresented in the training data from STS-2012.We build a new system (Sys?2), which is based onSys2, by making two adjustments in the setup: (i)we exclude SMTe from training to obtain predictionson SMT and head and (ii) we remove all TK featuresto train a model for FNWN and OnWN.
This is mo-tivated by the observation that text pairs from STS-2012 yield a paraphrase model, since the texts aresyntactically very similar.
Yet, two datasets fromSTS-2013 FNWN, and OnWN contain text pairswhere documents exhibit completely different struc-tures.
This is misleading for our syntactic similaritymodel learned on the STS-2012.System head OnWN FNWN SMT Mean RankUMBC 0.7642 0.7529 0.5818 0.3804 0.6181 1Sys2 0.7465 0.5572 0.3875 0.3409 0.5339 21Sys1 0.7352 0.5432 0.3842 0.3180 0.5188 28Sys3 0.7395 0.4228 0.3596 0.3294 0.4919 40Sys?2 0.7538 0.6872 0.4478 0.3391 0.5732 4*Table 2: Results on STS-2013.5 Conclusions and Future WorkWe have described our participation in STS-2013task.
Our approach treats text pairs as structuralobjects which provides much richer representationfor the learning algorithm to extract useful patterns.We experiment with structures derived from con-stituency and dependency trees where related frag-ments are linked with a special tag.
Such struc-tures are then used to learn tree kernel models whichcan be efficiently combined with the a feature vectorrepresentation in a single scoring model.
Our ap-proach ranks 1st with a large margin w.r.t.
to thebest systems in STS-2012 task, while it is 21st ac-cording to the final rankings of STS-2013.
Never-theless, a small change in the system setup makesit rank 4th.
Clearly, domain adaptation represents abig challenge in STS-2013 task.
We plan to addressthis issue in our future work.6 AcknowledgementsThis research has been supported by the Euro-pean Community?s Seventh Framework Program(FP7/2007-2013) under the #288024 LIMOSINEproject.ReferencesEneko Agirre, Daniel Cer, Mona Diab, and Gonzalez-Agirre.
2012.
Semeval-2012 task 6: A pilot on se-mantic textual similarity.
In First Joint Conference onLexical and Computational Semantics (*SEM).Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*sem 2013 sharedtask: Semantic textual similarity, including a pilot ontyped-similarity.
In *SEM 2013: The Second JointConference on Lexical and Computational Semantics.Daniel Bar, Chris Biemann, Iryna Gurevych, and TorstenZesch.
2012.
Ukp: Computing semantic textual sim-ilarity by combining multiple content similarity mea-sures.
In Proceedings of the Sixth International Work-shop on Semantic Evaluation (SemEval 2012).Marco Baroni, Silvia Bernardini, Adriano Ferraresi, andEros Zanchetta.
2009.
The wacky wide web: acollection of very large linguistically processed web-crawled corpora.
Language Resources and Evalua-tion, 43(3):209?226.Massimiliano Ciaramita and Yasemin Altun.
2006.Broad-coverage sense disambiguation and informa-tion extraction with a supersense sequence tagger.
InEMNLP.Michael Collins and Nigel Duffy.
2002.
New RankingAlgorithms for Parsing and Tagging: Kernels over Dis-crete Structures, and the Voted Perceptron.
In ACL.Danilo Croce, Paolo Annesi, Valerio Storch, and RobertoBasili.
Unitor: Combining semantic text similarityfunctions through sv regression.
In SemEval 2012.Andrew S. Fast and David Jensen.
2008.
Why stackedmodels perform effective collective classification.
InICDM.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Com-puting semantic relatedness using wikipedia-based ex-plicit semantic analysis.
In IJCAI.A.
Moschitti.
2006.
Efficient convolution kernels fordependency and constituent syntactic trees.
In ECML.Aliaksei Severyn and Alessandro Moschitti.
2012.Structural relationships for large-scale learning of an-swer re-ranking.
In SIGIR.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: a simple and general method forsemi-supervised learning.
In ACL.Peter D. Turney and Patrick Pantel.
2010.
From fre-quency to meaning: vector space models of semantics.J.
Artif.
Int.
Res., 37(1):141?188.Frane S?aric?, Goran Glavas?, Mladen Karan, Jan S?najder,and Bojana Dalbelo Bas?ic?.
2012.
Takelab: Systemsfor measuring semantic text similarity.
In Proceedingsof the Sixth International Workshop on Semantic Eval-uation (SemEval 2012).58
