Fast Computing Grammar-driven Convolution Tree Kernel forSemantic Role LabelingWanxiang Che1?, Min Zhang2, Ai Ti Aw2, Chew Lim Tan3, Ting Liu1, Sheng Li11School of Computer Science and TechnologyHarbin Institute of Technology, China 150001{car,tliu}@ir.hit.edu.cn, lisheng@hit.edu.cn2Institute for Infocomm Research21 Heng Mui Keng Terrace, Singapore 119613{mzhang,aaiti}@i2r.a-star.edu.sg3School of ComputingNational University of Singapore, Singapore 117543tancl@comp.nus.edu.sgAbstractGrammar-driven convolution tree kernel(GTK) has shown promising results for se-mantic role labeling (SRL).
However, thetime complexity of computing the GTK isexponential in theory.
In order to speedup the computing process, we design twofast grammar-driven convolution tree kernel(FGTK) algorithms, which can compute theGTK in polynomial time.
Experimental re-sults on the CoNLL-2005 SRL data showthat our two FGTK algorithms are muchfaster than the GTK.1 IntroductionGiven a sentence, the task of semantic role labeling(SRL) is to analyze the propositions expressed bysome target verbs or nouns and some constituentsof the sentence.
In previous work, data-driven tech-niques, including feature-based and kernel-basedlearning methods, have been extensively studied forSRL (Carreras and Ma`rquez, 2005).Although feature-based methods are regarded asthe state-of-the-art methods and achieve much suc-cess in SRL, kernel-based methods are more effec-tive in capturing structured features than feature-based methods.
In the meanwhile, the syntacticstructure features hidden in a parse tree have beensuggested as an important feature for SRL and needto be further explored in SRL (Gildea and Palmer,2002; Punyakanok et al, 2005).
Moschitti (2004)?The work was mainly done when the author was a visitingstudent at I2Rand Che et al (2006) are two reported work to useconvolution tree kernel (TK) methods (Collins andDuffy, 2001) for SRL and has shown promising re-sults.
However, as a general learning algorithm, theTK only carries out hard matching between two sub-trees without considering any linguistic knowledgein kernel design.
To solve the above issue, Zhanget al (2007) proposed a grammar-driven convolu-tion tree kernel (GTK) for SRL.
The GTK can uti-lize more grammatical structure features via twogrammar-driven approximate matching mechanismsover substructures and nodes.
Experimental resultsshow that the GTK significantly outperforms theTK (Zhang et al, 2007).
Theoretically, the GTKmethod is applicable to any problem that uses syn-tax structure features and can be solved by the TKmethods, such as parsing, relation extraction, and soon.
In this paper, we use SRL as an application totest our proposed algorithms.Although the GTK shows promising results forSRL, one big issue for the kernel is that it needs ex-ponential time to compute the kernel function sinceit need to explicitly list all the possible variationsof two sub-trees in kernel calculation (Zhang et al,2007).
Therefore, this method only works efficientlyon such kinds of datasets where there are not toomany optional nodes in production rule set.
In orderto solve this computation issue, we propose two fastalgorithms to compute the GTK in polynomial time.The remainder of the paper is organized as fol-lows: Section 2 introduces the GTK.
In Section 3,we present our two fast algorithms for computingthe GTK.
The experimental results are shown in Sec-tion 4.
Finally, we conclude our work in Section 5.7812 Grammar-driven Convolution TreeKernelThe GTK features with two grammar-driven ap-proximate matching mechanisms over substructuresand nodes.2.1 Grammar-driven Approximate MatchingGrammar-driven Approximate SubstructureMatching: the TK requires exact matching betweentwo phrase structures.
For example, the two phrasestructures ?NP?DT JJ NN?
(NP?a red car) and?NP?DT NN?
(NP?a car) are not identical, thusthey contribute nothing to the conventional kernelalthough they share core syntactic structure propertyand therefore should play the same semantic rolegiven a predicate.
Zhang et al (2007) introducesthe concept of optional node to capture this phe-nomenon.
For example, in the production rule?NP?DT [JJ] NP?, where [JJ] denotes an optionalnode.
Based on the concept of optional node, thegrammar-driven approximate substructure matchingmechanism is formulated as follows:M(r1, r2) =?i,j(IT (T ir1 , T jr2)?
?ai+bj1 ) (1)where r1 is a production rule, representing a two-layer sub-tree, and likewise for r2.
T ir1 is the ith vari-ation of the sub-tree r1 by removing one ore moreoptional nodes, and likewise for T jr2 .
IT (?, ?)
is a bi-nary function that is 1 iff the two sub-trees are iden-tical and zero otherwise.
?1 (0 ?
?1 ?
1) is a smallpenalty to penalize optional nodes.
ai and bj standfor the numbers of occurrence of removed optionalnodes in subtrees T ir1 and T jr2 , respectively.M(r1, r2) returns the similarity (i.e., the kernelvalue) between the two sub-trees r1 and r2 by sum-ming up the similarities between all possible varia-tions of the sub-trees.Grammar-driven Approximate Node Match-ing: the TK needs an exact matching between twonodes.
But, some similar POSs may represent simi-lar roles, such as NN (dog) and NNS (dogs).
Zhanget al (2007) define some equivalent nodes that canmatch each other with a small penalty ?2 (0 ?
?2 ?1).
This case is called node feature mutation.
Theapproximate node matching can be formulated as:M(f1, f2) =?i,j(If (f i1, f j2 )?
?ai+bj2 ) (2)where f1 is a node feature, f i1 is the ith mutation off1 and ai is 0 iff f i1 and f1 are identical and 1 oth-erwise, and likewise for f2 and bj .
If (?, ?)
is a func-tion that is 1 iff the two features are identical andzero otherwise.
Eq.
(2) sums over all combinationsof feature mutations as the node feature similarity.2.2 The GTKGiven these two approximate matching mecha-nisms, the GTK is defined by beginning with thefeature vector representation of a parse tree T as:??
(T ) = (#subtree1(T ), .
.
.
,#subtreen(T ))where #subtreei(T ) is the occurrence number ofthe ith sub-tree type (subtreei) in T .
Now the GTKis defined as follows:KG(T1, T2) = ???(T1),??
(T2)?=?i #subtreei(T1) ?#subtreei(T2)=?i((?n1?N1 I?subtreei(n1))?
(?n2?N2 I?subtreei(n2)))=?n1?N1?n2?N2 ??
(n1, n2)(3)where N1 and N2 are the sets of nodes in trees T1and T2, respectively.
I ?subtreei(n) is a function thatis ?a1 ?
?b2 iff there is a subtreei rooted at node n andzero otherwise, where a and b are the numbers ofremoved optional nodes and mutated node features,respectively.
??
(n1, n2) is the number of the com-mon subtrees rooted at n1 and n2, i.e.,??
(n1, n2) =?iI ?subtreei(n1) ?
I ?subtreei(n2) (4)??
(n1, n2) can be further computed by the follow-ing recursive rules:R-A: if n1 and n2 are pre-terminals, then:??
(n1, n2) = ?
?M(f1, f2) (5)where f1 and f2 are features of nodes n1 and n2respectively, and M(f1, f2) is defined in Eq.
(2),which can be computed in linear time O(n), wheren is the number of feature mutations.R-B: else if both n1 and n2 are the same non-terminals, then generate all variations of sub-treesof depth one rooted at n1 and n2 (denoted by Tn1782and Tn2 respectively) by removing different optionalnodes, then:??
(n1, n2) = ??
?i,j IT (T in1 , T jn2)?
?ai+bj1?
?nc(n1,i)k=1 (1 + ??
(ch(n1, i, k), ch(n2, j, k)))(6)where T in1 , T jn2 , IT (?, ?
), ai and bj have been ex-plained in Eq.
(1).
nc(n1, i) returns the numberof children of n1 in its ith subtree variation T in1 .ch(n1, i, k) is the kth child of node n1 in its ith vari-ation subtree T in1 , and likewise for ch(n2, j, k).
?
(0 < ?
< 1) is the decay factor.R-C: else ??
(n1, n2) = 03 Fast Computation of the GTKClearly, directly computing Eq.
(6) requires expo-nential time, since it needs to sum up all possiblevariations of the sub-trees with and without optionalnodes.
For example, supposing n1 = ?A?a [b] c[d]?, n2 = ?A?a b c?.
To compute the Eq.
(6), wehave to list all possible variations of n1 and n2?s sub-trees, n1: ?A?a b c d?, ?A?a b c?, ?A?a c d?, ?A?ac?
; n2: ?A?a b c?.
Unfortunately, Zhang et al(2007) did not give any theoretical solution for theissue of exponential computing time.
In this paper,we propose two algorithms to calculate it in polyno-mial time.
Firstly, we recast the issue of computingEq.
(6) as a problem of finding common sub-treeswith and without optional nodes between two sub-trees.
Following this idea, we rewrite Eq.
(6) as:??
(n1, n2) = ??
(1 +lm?p=lx?p(cn1 , cn2)) (7)where cn1 and cn2 are the child node sequences ofn1 and n2, ?p evaluates the number of commonsub-trees with exactly p children (at least includingall non-optional nodes) rooted at n1 and n2, lx =max{np(cn1), np(cn2)} and np(?)
is the number ofnon-optional nodes, lm = min{l(cn1), l(cn2)}andl(?)
returns the number of children.Now let?s study how to calculate ?p(cn1 , cn2) us-ing dynamic programming algorithms.
Here, wepresent two dynamic programming algorithms tocompute it in polynomial time.3.1 Fast Grammar-driven Convolution TreeKernel I (FGTK-I)Our FGTK-I algorithm is motivated by the stringsubsequence kernel (SSK) (Lodhi et al, 2002).Given two child node sequences sx = cn1 andt = cn2 (x is the last child), the SSK uses the fol-lowing recursive formulas to evaluate the ?p:?
?0(s, t) = 1, for all s, t,?
?p(s, t) = 0, ifmin(|s|, |t|) < p, (8)?p(s, t) = 0, ifmin(|s|, |t|) < p, (9)?
?p(sx, t) = ???
?p(sx, t) +?j:tj=x(?
?p?1(s, t[1 : j ?
1]?
?|t|?j+2)),(10)p = 1, .
.
.
, n?
1,?p(sx, t) = ?p(s, t) +?j:tj=x(?
?p?1(s, t[1 : j ?
1]?
?2)).
(11)where ?
?p is an auxiliary function since it is onlythe interior gaps in the subsequences that are penal-ized; ?
is a decay factor only used in the SSK forweighting each extra length unit.
Lodhi et al (2002)explained the correctness of the recursion definedabove.Compared with the SSK kernel, the GTK hasthree different features:f1: In the GTK, only optional nodes can beskipped while the SSK kernel allows any node skip-ping;f2: The GTK penalizes skipped optional nodesonly (including both interior and exterior skippednodes) while the SSK kernel weights the length ofsubsequences (all interior skipped nodes are countedin, but exterior nodes are ignored);f3: The GTK needs to further calculate the num-ber of common sub-trees rooted at each two match-ing node pair x and t[j].To reflect the three considerations, we modify theSSK kernel as follows to calculate the GTK:?0(s, t) = opt(s)?
opt(t)?
?|s|+|t|1 , for all s, t, (12)?p(s, t) = 0, ifmin(|s|, |t|) < p, (13)?p(sx, t) = ?1 ?
?p(sx, t)?
opt(x)+?j:tj=x(?p?1(s, t[1 : j ?
1])?
?|t|?j (14)?opt(t[j + 1 : |t|])???
(x, t[j])).where opt(w) is a binary function, which is 0 ifnon-optional nodes are found in the node sequencew and 1 otherwise (f1); ?1 is the penalty to penalizeskipped optional nodes and the power of ?1 is thenumber of skipped optional nodes (f2); ??
(x, t[j])is defined in Eq.
(7) (f3).
Now let us compare783the FGTK-I and SSK kernel algorithms.
Based onEqs.
(8), (9), (10) and (11), we introduce the opt(?
)function and the penalty ?1 into Eqs.
(12), (13) and(14), respectively.
opt(?)
is to ensure that in theGTK only optional nodes are allowed to be skipped.And only those skipped optional nodes are penal-ized with ?1.
Please note that Eqs.
(10) and (11)are merged into Eq.
(14) because of the differentmeaning of ?
and ?1.
From Eq.
(8), we can seethat the current path in the recursive call will stopand its value becomes zero once non-optional nodeis skipped (when opt(w) = 0).Let us use a sample of n1 = ?A?a [b] c [d]?, n2 =?A?a b c?
to exemplify how the FGTK-I algorithmworks.
In Eq.
(14)?s vocabulary, we have s = ?a [b]c?, t = ?a b c?, x = ?
[d]?, opt(x) = opt([d]) = 1,p = 3.
Then according to Eq (14), ?p(cn1 , cn2) canbe calculated recursively as Eq.
(15) (Please refer tothe next page).Finally, we have ?p(cn1 , cn2) = ?1 ???
(a, a)???
(b, b)???
(c, c)By means of the above algorithm, we can com-pute the ??
(n1, n2) in O(p|cn1 | ?
|cn2 |2) (Lodhi etal., 2002).
This means that the worst case complex-ity of the FGTK-I is O(p?3|N1| ?
|N2|2), where ?
isthe maximum branching factor of the two trees.3.2 Fast Grammar-driven Convolution TreeKernel II (FGTK-II)Our FGTK-II algorithm is motivated by the partialtrees (PTs) kernel (Moschitti, 2006).
The PT kernelalgorithm uses the following recursive formulas toevaluate ?p(cn1 , cn2):?p(cn1 , cn2) =|cn1 |?i=1|cn2 |?j=1?
?p(cn1 [1 : i], cn2 [1 : j]) (16)where cn1 [1 : i] and cn2 [1 : j] are the child sub-sequences of cn1 and cn2 from 1 to i and from 1to j, respectively.
Given two child node sequencess1a = cn1 [1 : i] and s2b = cn2 [1 : j] (a and b arethe last children), the PT kernel computes ?
?p(?, ?)
asfollows:?
?p(s1a, s2b) ={?2??
(a, b)Dp(|s1|, |s2|) if a = b0 else (17)where ??
(a, b) is defined in Eq.
(7) and Dp is recur-sively defined as follows:Dp(k, l) = ?
?p?1(s1[1 : k], s2[1 : l])+?Dp(k, l ?
1) + ?Dp(k ?
1, l) (18)?
?2Dp(k ?
1, l ?
1)D1(k, l) = 1, for all k, l (19)where ?
used in Eqs.
(17) and (18) is a factor topenalize the length of the child sequences.Compared with the PT kernel, the GTK has twodifferent features which are the same as f1 and f2when defining the FGTK-I.To reflect the two considerations, based on the PTkernel algorithm, we define another fast algorithmof computing the GTK as follows:?p(cn1 , cn2 ) =?
|cn1 |i=1?
|cn2 |j=1 ?
?p(cn1 [1 : i], cn2 [1 : j])?opt(cn1 [i+ 1 : |cn1 |])?opt(cn2 [j + 1 : |cn2 |])?
?|cn1 |?i+|cn2 |?j1(20)?
?p(s1a, s2b) ={ ??
(a, b)Dp(|s1|, |s2|) if a = b0 else (21)Dp(k, l) = ?
?p?1(s1[1 : k], s2[1 : l])+?1Dp(k, l ?
1)?
opt(s2[l]) (22)+?1Dp(k ?
1, l)?
opt(s1[k])?
?21Dp(k ?
1, l ?
1)?
opt(s1[k])?
opt(s2[l])D1(k, l) = ?k+l1 ?
opt(s1[1 : k])?
opt(s2[1 : l]), (23)for all k, l?
?p(s1, s2) = 0, if min(|s1|, |s2|) < p (24)where opt(w) and ?1 are the same as them in theFGTK-I.Now let us compare the FGTK-II and the PT al-gorithms.
Based on Eqs.
(16), (18) and (19), we in-troduce the opt(?)
function and the penalty ?1 intoEqs.
(20), (22) and (23), respectively.
This is toensure that in the GTK only optional nodes are al-lowed to be skipped and only those skipped optionalnodes are penalized.
In addition, compared withEq.
(17), the penalty ?2 is removed in Eq.
(21) inview that our kernel only penalizes skipped nodes.Moreover, Eq.
(24) is only for fast computing.
Fi-nally, the same as the FGTK-I, in the FGTK-II thecurrent path in a recursive call will stop and its valuebecomes zero once non-optional node is skipped(when opt(w) = 0).
Here, we still can use an ex-ample to derivate the process of the algorithm stepby step as that for FGTK-I algorithm.
Due to spacelimitation, here, we do not illustrate it in detail.By means of the above algorithms, we can com-pute the ??
(n1, n2) in O(p|cn1 | ?
|cn2 |) (Moschitti,784?p(cn1 , cn2 ) = ?p(?a [b] c [d]?
, ?a b c?
)= ?1 ?
?p(?a [b] c?, ?a b c?)
+ 0 //Since x * t, the second term is 0= ?1 ?
(0 + ?p?1(?a [b]?, ?a b?)?
?3?31 ???
(c, c)) //Since opt(?c?)
= 0, the first term is 0= ?1 ???
(c, c)?
(0 + ?p?2(?a?, ?a b?)?
?2?21 ???
(b, b)) //Since p?
1 > |?a?|,?p?2(?a?, ?a b?)
= 0= ?1 ???
(c, c)?
(0 + ??
(a, a)???
(b, b)) //?p?2(?a?, ?a?)
= ??
(a, a)(15)2006).
This means that the worst complexity of theFGTK-II is O(p?2|N1| ?
|N2|).
It is faster than theFGTK-I?s O(p?3|N1| ?
|N2|2) in theory.
Please notethat the average ?
in natural language parse trees isvery small and the overall complexity of the FGTKscan be further reduced by avoiding the computationof node pairs with different labels (Moschitti, 2006).4 Experiments4.1 Experimental SettingData: We use the CoNLL-2005 SRL shared taskdata (Carreras and Ma`rquez, 2005) as our experi-mental corpus.Classifier: SVM (Vapnik, 1998) is selected as ourclassifier.
In the FGTKs implementation, we mod-ified the binary Tree Kernels in SVM-Light Tool(SVM-Light-TK) (Moschitti, 2006) to a grammar-driven one that encodes the GTK and the two fast dy-namic algorithms inside the well-known SVM-Lighttool (Joachims, 2002).
The parameters are the sameas Zhang et al (2007).Kernel Setup: We use Che et al (2006)?s hybridconvolution tree kernel (the best-reported methodfor kernel-based SRL) as our baseline kernel.
It isdefined as Khybrid = ?Kpath + (1 ?
?
)Kcs (0 ??
?
1)1.
Here, we use the GTK to compute theKpath and the Kcs.In the training data (WSJ sections 02-21), we get4,734 production rules which appear at least 5 times.Finally, we use 1,404 rules with optional nodes forthe approximate structure matching.
For the nodeapproximate matching, we use the same equivalentnode sets as Zhang et al (2007).4.2 Experimental ResultsWe use 30,000 instances (a subset of the entire train-ing set) as our training set to compare the differentkernel computing algorithms 2.
All experiments are1Kpath and Kcs are two TKs to describe predicate-argument link features and argument syntactic structure fea-tures, respectively.
For details, please refer to (Che et al, 2006).2There are about 450,000 identification instances are ex-tracted from training data.conducted on a PC with CPU 2.8GH and memory1G.
Fig.
1 reports the experimental results, wheretraining curves (time vs. # of instances) of fivekernels are illustrated, namely the TK, the FGTK-I, the FGTK-II, the GTK and a polynomial kernel(only for reference).
It clearly demonstrates that ourFGTKs are faster than the GTK algorithm as ex-pected.
However, the improvement seems not sosignificant.
This is not surprising as there are only30.4% rules (1,404 out of 4,734)3 that have optionalnodes and most of them have only one optionalnode4.
Therefore, in this case, it is not time con-suming to list all the possible sub-tree variations andsum them up.
Let us study this issue from computa-tional complexity viewpoint.
Suppose all rules haveexactly one optional node.
This means each rule canonly generate two variations.
Therefore computingEq.
(6) is only 4 times (2*2) slower than the GTKin this case.
In other words, we can say that giventhe constraint that there is only one optional nodein one rule, the time complexity of the GTK is alsoO(|N1| ?
|N2|) 5, where N1 and N2 are the numbersof tree nodes, the same as the TK.120006000800010000Training Time (S) GTKFGTK-I20004000TrainingTime(S)FGTK-IITKPoly0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30Number of Training Instances (103)Figure 1: Training time comparison among differentkernels with rule set having less optional nodes.Moreover, Fig 1 shows that the FGTK-II is fasterthan the FGTK-I.
This is reasonable since as dis-3The percentage is even smaller if we consider all produc-tion (it becomes 14.4% (1,404 out of 9,700)).4There are 1.6 optional nodes in each rule averagely.5Indeed it is O(4 ?
|N1| ?
|N2|).
The parameter 4 is omittedwhen discussing time complexity.785cussed in Subsection 3.2, the FGTK-I?s time com-plexity is O(p?3|N1| ?
|N2|2) while the FGTK-II?s isO(p?2|N1| ?
|N2|).400004500020000250003000035000Training Time (S) GTKFGTK-I050001000015000TrainingTime(S)FGTK-IITKPoly2 4 6 8 10 12 14 16 18 20 22 24 26 28 30Number of Training Instances (103)Figure 2: Training time comparison among differentkernels with rule set having more optional nodes.To further verify the efficiency of our proposedalgorithm, we conduct another experiment.
Here weuse the same setting as that in Fig 1 except that werandomly add more optional nodes in more produc-tion rules.
Table 1 reports the statistics on the tworule set.
Similar to Fig 1, Fig 2 compares the train-ing time of different algorithms.
We can see thatFig 2 convincingly justify that our algorithms aremuch faster than the GTK when the experimentaldata has more optional nodes and rules.Table 1: The rule set comparison between two ex-periments.# rules # rule with atleast optionalnodes# op-tionalnodes# average op-tional nodes perruleExp1 4,734 1,404 2,242 1.6Exp2 4,734 4,520 10,451 2.35 ConclusionThe GTK is a generalization of the TK, which cancapture more linguistic grammar knowledge into thelater and thereby achieve better performance.
How-ever, a biggest issue for the GTK is its comput-ing speed, which needs exponential time in the-ory.
Therefore, in this paper we design two fastgrammar-driven convolution tree kennel (FGTK-Iand II) algorithms which can compute the GTK inpolynomial time.
The experimental results show thatthe FGTKs are much faster than the GTK when dataset has more optional nodes.
We conclude that ourfast algorithms enable the GTK kernel to easily scaleto larger dataset.
Besides the GTK, the idea of ourfast algorithms can be easily used into other similarproblems.To further our study, we will use the FGTK algo-rithms for other natural language processing prob-lems, such as word sense disambiguation, syntacticparsing, and so on.ReferencesXavier Carreras and Llu?
?s Ma`rquez.
2005.
Introductionto the CoNLL-2005 shared task: Semantic role label-ing.
In Proceedings of CoNLL-2005, pages 152?164.Wanxiang Che, Min Zhang, Ting Liu, and Sheng Li.2006.
A hybrid convolution tree kernel for seman-tic role labeling.
In Proceedings of the COLING/ACL2006, Sydney, Australia, July.Michael Collins and Nigel Duffy.
2001.
Convolutionkernels for natural language.
In Proceedings of NIPS-2001.Daniel Gildea and Martha Palmer.
2002.
The necessityof parsing for predicate argument recognition.
In Pro-ceedings of ACL-2002, pages 239?246.Thorsten Joachims.
2002.
Learning to Classify Text Us-ing Support Vector Machines: Methods, Theory andAlgorithms.
Kluwer Academic Publishers.Huma Lodhi, Craig Saunders, John Shawe-Taylor, NelloCristianini, and Chris Watkins.
2002.
Text classifica-tion using string kernels.
Journal of Machine LearningResearch, 2:419?444.Alessandro Moschitti.
2004.
A study on convolution ker-nels for shallow statistic parsing.
In Proceedings ofACL-2004, pages 335?342.Alessandro Moschitti.
2006.
Syntactic kernels for natu-ral language learning: the semantic role labeling case.In Proceedings of the HHLT-NAACL-2006, June.Vasin Punyakanok, Dan Roth, and Wen tau Yih.
2005.The necessity of syntactic parsing for semantic role la-beling.
In Proceedings of IJCAI-2005.Vladimir N. Vapnik.
1998.
Statistical Learning Theory.Wiley.Min Zhang, Wanxiang Che, Aiti Aw, Chew Lim Tan,Guodong Zhou, Ting Liu, and Sheng Li.
2007.
Agrammar-driven convolution tree kernel for semanticrole classification.
In Proceedings of ACL-2007, pages200?207.786
