Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 222?227,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsSemantic Neighborhoods as HypergraphsChris Quirk and Pallavi ChoudhuryMicrosoft ResearchOne Microsoft WayRedmond, WA 98052, USA{chrisq,pallavic}@microsoft.comAbstractAmbiguity preserving representationssuch as lattices are very useful in a num-ber of NLP tasks, including paraphrasegeneration, paraphrase recognition, andmachine translation evaluation.
Latticescompactly represent lexical variation, butword order variation leads to a combina-torial explosion of states.
We advocatehypergraphs as compact representationsfor sets of utterances describing the sameevent or object.
We present a methodto construct hypergraphs from sets ofutterances, and evaluate this method ona simple recognition task.
Given a set ofutterances that describe a single object orevent, we construct such a hypergraph,and demonstrate that it can recognizenovel descriptions of the same event withhigh accuracy.1 IntroductionHumans can construct a broad range of descrip-tions for almost any object or event.
In this paper,we will refer to such objects or events as ground-ings, in the sense of grounded semantics.
Exam-ples of groundings include pictures (Rashtchian etal., 2010), videos (Chen and Dolan, 2011), transla-tions of a sentence from another language (Dreyerand Marcu, 2012), or even paraphrases of the samesentence (Barzilay and Lee, 2003).One crucial problem is recognizing whethernovel utterances are relevant descriptions of thosegroundings.
In the case of machine translation,this is the evaluation problem; for images andvideos, this is recognition and retrieval.
Generat-ing descriptions of events is also often an interest-ing task: we might like to find a novel paraphrasefor a given sentence, or generate a description of agrounding that meets certain criteria (e.g., brevity,use of a restricted vocabulary).Much prior work has used lattices to compactlyrepresent a range of lexical choices (Pang et al,2003).
However, lattices cannot compactly repre-sent alternate word orders, a common occurrencein linguistic descriptions.
Consider the followingexcerpts from a video description corpus (Chenand Dolan, 2011):?
A man is sliding a cat on the floor.?
A boy is cleaning the floor with the cat.?
A cat is being pushed across the floor by aman.Ideally we would like to recognize that the fol-lowing utterance is also a valid description of thatevent: A cat is being pushed across the floor by aboy.
That is difficult with lattice representations.Consider the following context free grammar:S ?
X0 X1| X2 X3X0 ?
a man | a boyX1 ?
is sliding X2 on X4| is cleaning X4 with X2X2 ?
a cat | the catX3 ?
is being pushed across X4 by X0X4 ?
the floorThis grammar compactly captures many lexicaland syntactic variants of the input set.
Note howthe labels act as a kind of multiple-sequence-alignment allowing reordering: spans of tokenscovered by the same label are, in a sense, aligned.This hypergraph or grammar represents a seman-tic neighborhood: a set of utterances that describethe same entity in a semantic space.Semantic neighborhoods are defined in terms ofa grounding.
Two utterances are neighbors withrespect to some grounding (semantic event) if theyare both descriptions of that grounding.
Para-phrases, in contrast, may be defined over all pos-sible groundings.
That is, two words or phrases222are considered paraphrases if there exists somegrounding that they both describe.
The para-phrase relation is more permissive than the seman-tic neighbor relation in that regard.
We believe thatit is much easier to define and evaluate semanticneighbors.
Human annotators may have difficultyseparating paraphrases from unrelated or merelyrelated utterances, and this line may not be con-sistent between judges.
Annotating whether an ut-terance clearly describes a grounding is a mucheasier task.This paper describes a simple method for con-structing hypergraph-shaped Semantic Neighbor-hoods from sets of expressions describing thesame grounding.
The method is evaluated ina paraphrase recognition task, inspired by aCAPTCHA task (Von Ahn et al, 2003).2 Inducing neighborhoodsConstructing a hypergraph to capture a set of utter-ances is a variant of grammar induction.
Given asample of positive examples, we infer a compactand accurate description of the underlying lan-guage.
Conventional grammar induction attemptsto define the set of grammatical sentences in thelanguage.
Here, we search for a grammar over thefluent and adequate descriptions of a particular in-put.
Many of the same techniques still apply.Rather than starting from scratch, we bootstrapfrom an existing English parser.
We begin by pars-ing the set of input utterances.
This parsed set ofutterances acts as a sort of treebank.
Reading off agrammar from this treebank produces a grammarthat can generate not only the seed sentences, butalso a broad range of nearby sentences.
In the caseabove with cat, man, and boy, we would be ableto generate cases legitimate variants where manwas replaced by boy as well as undesired variantswhere man is replaced by cat or floor.
This initialgrammar captures a large neighborhood of nearbyutterances including many such undesirable ones.Therefore, we refine the grammar.Refinements have been in common use in syn-tactic parsing for years now.
Inspired by the re-sult that manual annotations of Treebank cate-gories can substantially increase parser accuracy(Klein and Manning, 2003), several approacheshave been introduced to automatically induce la-tent symbols on existing trees.
We use the split-merge method commonly used in syntactic pars-ing (Petrov et al, 2006).
In its original setting,the refinements captured details beyond that of theoriginal Penn Treebank symbols.
Here, we cap-ture both syntactic and semantic regularities in thedescriptions of a given grounding.As we perform more rounds of refinement, thegrammar becomes tightly constrained to the orig-inal sentences.
Indeed, if we iterated to a fixedpoint, the resulting grammar would parse only theoriginal sentences.
This is a common dilemma inparaphrase learning: the safest meaning preserv-ing rewrite is to change nothing.
We optimize thenumber of split-merge rounds for task-accuracy;two or three rounds works well in practice.
Fig-ure 1 illustrates the process.2.1 Split-merge inductionWe begin with a set of utterances that describea specific grounding.
They are parsed with aconventional Penn Treebank parser (Quirk et al,2012) to produce a type of treebank.
Unlike con-ventional treebanks which are annotated by humanexperts, the trees here are automatically createdand thus are more likely to contain errors.
Thistreebank is the input to the split-merge process.Split: Given an input treebank, we propose re-finements of the symbols in hopes of increasingthe likelihood of the data.
For each original sym-bol in the grammar such as NP, we consider two la-tent refinements: NP0 and NP1.
Each binary rulethen produces 8 possible variants, since the par-ent, left child, and right child now have two possi-ble refinements.
The parameters of this grammarare then optimized using EM.
Although we do notknow the correct set of latent annotations, we cansearch for the parameters that optimize the likeli-hood of the given treebank.
We initialize the pa-rameters of this refined grammar with the countsfrom the original grammar along with a small ran-dom number.
This randomness prevents EM fromstarting on a saddle point by breaking symmetries;Petrov et al describe this in more detail.Merge: After EM has run to completion, wehave a new grammar with twice as many symbolsand eight times as many rules.
Many of these sym-bols may not be necessary, however.
For instance,nouns may require substantial refinement to dis-tinguish a number of different actors and objects,where determiners might not require much refine-ment at all.
Therefore, we discard the splits thatled to the least increase in likelihood, and thenreestimate the grammar once again.223(a) Input:?
the man plays the piano?
the guy plays the keyboard(b) Parses:?
(S (NP (DT the) (NN man))(VP (VBZ plays)(NP (DT the) (NN piano)))?
(S (NP (DT the) (NN guy))(VP (VBZ plays)(NP (DT the) (NN keyboard)))(c) Parses with latent annotations:?
(S (NP0 (DT the) (NN0 man))(VP (VBZ plays)(NP1 (DT the) (NN1 piano)))?
(S (NP0 (DT the) (NN0 guy))(VP (VBZ plays)(NP1 (DT the) (NN1 keyboard)))(d) Refined grammar:S ?
NP0 VPNP0 ?
DT NN0NP1 ?
DT NN1NP ?
VBZ NP1DT ?
theNN0 ?
man | guyNN1 ?
piano | keyboardVBZ ?
playsFigure 1: Example of hypergraph induction.
Firsta conventional Treebank parser converts input ut-terances (a) into parse trees (b).
A grammar couldbe directly read from this small treebank, but itwould conflate all phrases of the same type.
In-stead we induce latent refinements of this smalltreebank (c).
The resulting grammar (d) can matchand generate novel variants of these inputs, suchas the man plays the keyboard and the buy playsthe piano.
While this simplified example sug-gests a single hard assignment of latent annota-tions to symbols, in practice we maintain a dis-tribution over these latent annotations and extracta weighted grammar.Iteration: We run this process in series.
Firstthe original grammar is split, then some of theleast useful splits are discarded.
This refinedgrammar is then split again, with the least usefulsplits discarded once again.
We repeat for a num-ber of iterations based on task accuracy.Final grammar estimation: The EM proce-dure used during split and merge assigns fractionalcounts c(?
?
? )
to each refined symbol Xi and eachproduction Xi ?
Yj Zk.
We estimate the finalgrammar using these fractional counts.P (Xi ?
Yj Zk) =c(Xi, Yj , Zk)c(Xi)In Petrov et al, these latent refinements are laterdiscarded as the goal is to find the best parse withthe original coarse symbols.
Here, we retain thelatent refinements during parsing, since they dis-tinguish semantically related utterances from un-related utterances.
Note in Figure 1 how NN0and NN1 refer to different objects; were we to ig-nore that distinction, the parser would recognizesemantically different utterances such as the pianoplays the piano.2.2 Pruning and smoothingFor both speed and accuracy, we may also prunethe resulting rules.
Pruning low probability rulesincreases the speed of parsing, and tends to in-crease the precision of the matching operation atthe cost of recall.
Here we only use an absolutethreshold; we vary this threshold and inspect theimpact on task accuracy.
Once the fully refinedgrammar has been trained, we only retain thoserules with a probability above some threshold.
Byvarying this threshold t we can adjust precisionand recall: as the low probability rules are re-moved from the grammar, precision tends to in-crease and recall tends to decrease.Another critical issue, especially in these smallgrammars, is smoothing.
When parsing with agrammar obtained from only 20 to 50 sentences,we are very likely to encounter words that havenever been seen before.
We may reasonably re-ject such sentences under the assumption that theyare describing words not present in the trainingcorpus.
However, this may be overly restrictive:we might see additional adjectives, for instance.In this work, we perform a very simple form ofsmoothing.
If the fractional count of a word givena pre-terminal symbol falls below a threshold k,then we consider that instance rare and reserve afraction of its probability mass for unseen words.This accounts for lexical variation of the ground-ing, especially in the least consistently used words.Substantial speedups could be attained by us-ing finite state approximations of this grammar:matching complexity drops to cubic to linear inthe length of the input.
A broad range of approxi-mations are available (Nederhof, 2000).
Since thesmall grammars in our evaluation below seldomexhibit self-embedding (latent state identification224tends to remove recursion), these approximationswould often be tight.3 Experimental evaluationWe explore a task in description recognition.Given a large set of videos and a number of de-scriptions for each video (Chen and Dolan, 2011),we build a system that can recognize fluent andaccurate descriptions of videos.
Such a recognizerhas a number of uses.
One example currently inevaluation is a novel CAPTCHAs: to differentiatea human from a bot, a video is presented, and theresponse must be a reasonably accurate and fluentdescription of this video.We split the above data into training and test.From the training sets, we build a set of recogniz-ers.
Then we present these recognizers with a se-ries of inputs, some of which are from the held outset of correct descriptions of this video, and someof which are from descriptions of other videos.Based on discussions with authors of CAPTCHAsystems, a ratio of actual users to spammers of 2:1seemed reasonable, so we selected one negativeexample for every two positives.
This simulatesthe accuracy of the system when presented with asimple bot that supplies random, well-formed textas CAPTCHA answers.1As a baseline, we compare against a simple tf-idf approach.
In this baseline we first pool allthe training descriptions of the video into a sin-gle virtual document.
We gather term frequen-cies and inverse document frequencies across thewhole corpus.
An incoming utterance to be classi-fied is scored by computing the dot product of itscounted terms with each document; it is assignedto the document with the highest dot product (co-sine similarity).Table 2 demonstrates that a baseline tf-idf ap-proach is a reasonable starting point.
An oracleselection from among the top three is the best per-formance ?
clearly this is a reasonable approach.That said, grammar based approach shows im-provements over the baseline tf-idf, especially inrecall.
Recall is crucial in a CAPTCHA style task:if we fail to recognize utterances provided by hu-mans, we risk frustration or abandonment of theservice protected by the CAPTCHA.
The relativeimportance of false positives versus false negatives1A bot might perform object recognition on the videos andsupply a stream of object names.
We might simulate this byclassifying utterances consisting of appropriate object wordsbut without appropriate syntax or function words.Total videos 2,029Training descriptions 22,198types 5,497tokens 159,963Testing descriptions 15,934types 4,075tokens 114,399Table 1: Characteristics of the evaluation data.The descriptions from the video description cor-pus are randomly partitioned into training and test.
(a)Algorithm S k Prec Rec F-0tf-idf 99.9 46.6 63.6tf-idf (top 3 oracle) 99.9 65.3 79.0grammar 2 1 86.6 51.5 64.62 4 80.2 62.6 70.32 16 74.2 74.2 74.22 32 73.5 76.4 74.93 1 91.1 43.9 59.23 4 83.7 54.4 65.93 16 77.3 65.7 71.13 32 76.4 68.1 72.04 1 94.1 39.7 55.84 4 85.5 51.1 64.04 16 79.1 61.5 69.24 32 78.2 63.9 70.3(b)t S Prec Rec F-0?
4.5?
10?5 2 74.8 73.9 74.4?
4.5?
10?5 3 79.6 60.9 69.0?
4.5?
10?5 4 82.5 53.2 64.7?
3.1?
10?7 2 74.2 75.0 74.6?
3.1?
10?7 3 78.1 64.6 70.7?
3.1?
10?7 4 80.7 58.8 68.1> 0 2 73.4 76.4 74.9> 0 3 76.4 68.1 72.0> 0 4 78.2 63.9 70.3Table 2: Experimental results.
(a) Comparison oftf-idf baseline against grammar based approach,varying several free parameters.
An oracle checksif the correct video is in the top three.
For thegrammar variants, the number of splits S and thesmoothing threshold k are varied.
(b) Variationson the rule pruning threshold t and number ofsplit-merge rounds S. > 0 indicates that all rulesare retained.
Here the smoothing threshold k isfixed at 32.225(a) Input descriptions:?
A cat pops a bunch of little balloons that are on the groung.?
A dog attacks a bunch of balloons.?
A dog is biting balloons and popping them.?
A dog is playing balloons.?
A dog is playing with balloons.?
A dog is playing with balls.?
A dog is popping balloons with its teeth.?
A dog is popping balloons.?
A dog is popping balloons.?
A dog plays with a bunch of balloons.?
A small dog is attacking balloons.?
The dog enjoyed popping balloons.?
The dog popped the balloons.
(b) Top ranked yields from the resulting grammar:+0.085 A dog is popping balloons.+0.062 A dog is playing with balloons.+0.038 A dog is playing balloons.0.038 A dog is attacking balloons.+0.023 A dog plays with a bunch of balloons.+0.023 A dog attacks a bunch of balloons.0.023 A dog pops a bunch of balloons.0.023 A dog popped a bunch of balloons.0.023 A dog enjoyed a bunch of balloons.0.018 The dog is popping balloons.0.015 A dog is biting balloons.0.015 A dog is playing with them.0.015 A dog is playing with its teeth.Figure 2: Example yields from a small grammar.
The descriptions in (a) were parsed as-is (including thetypographical error ?groung?
), and a refined grammar was trained with 4 splits.
The top k yields fromthis grammar along with the probability of that derivation are listed in (b).
A ?+?
symbol indicates thatthe yield was in the training set.
No smoothing or pruning was performed on this grammar.may vary depending on the underlying resource.Adjusting the free parameters of this method al-lows us to achieve different thresholds.
We cansee that rule pruning does not have a large impacton overall results, though it does allow yet anothermeans of tradiing off precision vs. recall.4 ConclusionsWe have presented a method for automaticallyconstructing compact representations of linguis-tic variation.
Although the initial evaluation onlyexplored a simple recognition task, we feel theunderlying approach is relevant to many linguis-tic tasks including machine translation evalua-tion, and natural language command and con-trol systems.
The induction procedure is rathersimple but effective, and addresses some of thereordering limitations associated with prior ap-proaches.
(Barzilay and Lee, 2003) In effect, weare performing a multiple sequence alignment thatallows reordering operations.
The refined symbolsof the grammar act as a correspondence betweenrelated inputs.The quality of the input parser is crucial.
Thismethod only considers one possible parse of theinput.
A straightforward extension would be toconsider an n-best list or packed forest of inputparses, which would allow the method to movepast errors in the first input process.
Perhaps alsothis reliance on symbols from the original Tree-bank is not ideal.
We could merge away some orall of the original distinctions, or explore differentparameterizations of the grammar that allow moreflexibility in parsing.The handling of unseen words is very simple.We are investigating means of including addi-tional paraphrase resources into the training to in-crease the effective lexical knowledge of the sys-tem.
It is inefficient to learn each grammar inde-pendently.
By sharing parameters across differentgroundings, we should be able to identify Seman-tic Neighborhoods with fewer training instances.AcknowledgmentsWe would like to thank William Dolan and theanonymous reviewers for their valuable feedback.ReferencesRegina Barzilay and Lillian Lee.
2003.
Learn-ing to paraphrase: An unsupervised approach us-ing multiple-sequence alignment.
In Proceedings ofNAACL-HLT.David Chen and William Dolan.
2011.
Collectinghighly parallel data for paraphrase evaluation.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, pages 190?200, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Markus Dreyer and Daniel Marcu.
2012.
Hyter:Meaning-equivalent semantics for translation eval-uation.
In Proceedings of the 2012 Conference ofthe North American Chapter of the Association forComputational Linguistics: Human Language Tech-nologies, pages 162?171, Montre?al, Canada, June.Association for Computational Linguistics.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Com-putational Linguistics, pages 423?430, Sapporo,226Japan, July.
Association for Computational Linguis-tics.Mark-Jan Nederhof.
2000.
Practical experiments withregular approximation of context-free languages.Computational Linguistics, 26(1):17?44, March.Bo Pang, Kevin Knight, and Daniel Marcu.
2003.Syntax-based alignment of multiple translations:Extracting paraphrases and generating new sen-tences.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and 44th Annual Meeting of the Associa-tion for Computational Linguistics, pages 433?440,Sydney, Australia, July.
Association for Computa-tional Linguistics.Chris Quirk, Pallavi Choudhury, Jianfeng Gao, HisamiSuzuki, Kristina Toutanova, Michael Gamon, Wen-tau Yih, Colin Cherry, and Lucy Vanderwende.2012.
Msr splat, a language analysis toolkit.
InProceedings of the Demonstration Session at theConference of the North American Chapter of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 21?24, Montre?al,Canada, June.
Association for Computational Lin-guistics.Cyrus Rashtchian, Peter Young, Micah Hodosh, andJulia Hockenmaier.
2010.
Collecting image annota-tions using amazon?s mechanical turk.
In Proceed-ings of the NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s Mechan-ical Turk, pages 139?147, Los Angeles, June.
Asso-ciation for Computational Linguistics.Luis Von Ahn, Manuel Blum, Nicholas J. Hopper, andJohn Langford.
2003.
Captcha: Using hard ai prob-lems for security.
In Eli Biham, editor, Advances inCryptology ?
EUROCRYPT 2003, volume 2656 ofLecture Notes in Computer Science, pages 294?311.Springer Berlin Heidelberg.227
