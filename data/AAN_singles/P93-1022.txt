CONTEXTUAL WORD S IMILARITY  AND EST IMAT IONFROM SPARSE DATAIdo DaganATT Bell Laboratories600 Mountain AvenueMurray Hill, NJ 07974dagan@res earch, art.
tomShau l  MarcusComputer Science DepartmentTechnionHaifa 32000, Israelshaul@cs, t echnion, ac.
il$hau l  Markov i tchComputer Science DepartmentTechnionHaifa 32000, Israelshaulm@cs,  t echnion,  ac.
ilAbst rac tIn recent years there is much interest in wordcooccurrence r lations, such as n-grams, verb-object combinations, or cooccurrence withina limited context.
This paper discusses howto estimate the probability of cooccurrencesthat do not occur in the training data.
Wepresent a method that makes local analogiesbetween each specific unobserved cooccurrenceand other cooccurrences that contain simi-lar words, as determined by an appropriateword similarity metric.
Our evaluation sug-gests that this method performs better thanexisting smoothing methods, and may providean alternative to class based models.1 I n t roduct ionStatistical data on word cooccurrence relationsplay a major role in many corpus based approachesfor natural anguage processing.
Different typesof cooccurrence r lations are in use, such as cooc-currence within a consecutive sequence of words(n-grams), within syntactic relations (verb-object,adjective-noun, etc.)
or the cooccurrence of twowords within a limited distance in the context.
Sta-tistical data about these various cooccurrence r la-tions is employed for a variety of applications, uchas speech recognition (Jelinek, 1990), language gen-eration (Smadja and McKeown, 1990), lexicogra-phy (Church and Hanks, 1990), machine transla-tion (Brown et al, ; Sadler, 1989), informationretrieval (Maarek and Smadja, 1989) and variousdisambiguation tasks (Dagan et al, 1991; Hindleand Rooth, 1991; Grishman et al, 1986; Dagan andItai, 1990).A major problem for the above applications ihow to estimate the probability of cooccurrencesthat were not observed in the training corpus.
Dueto data sparseness in unrestricted language, the ag-gregate probability of such cooccurrences is largeand can easily get to 25% or more, even for a verylarge training corpus (Church and Mercer, 1992).Since applications often have to compare alterna-tive hypothesized cooccurrences, it is importantto distinguish between those unobserved cooccur-rences that are likely to occur in a new piece of textand those that are not.
These distinctions ought obe made using the data that do occur in the cor-pus.
Thus, beyond its own practical importance,the sparse data problem provides an informativetouchstone for theories on generalization a d anal-ogy in linguistic data.The literature suggests two major approachesfor solving the sparse data problem: smoothingand class based methods.
Smoothing methods es-timate the probability of unobserved cooccurrencesusing frequency information (Good, 1953; Katz,1987; Jelinek and Mercer, 1985; Church and Gale,1991).
Church and Gale (Church and Gale, 1991)show, that for unobserved bigrams, the estimates ofseveral smoothing methods closely agree with theprobability that is expected using the frequencies ofthe two words and assuming that their occurrenceis independent ((Church and Gale, 1991), figure 5).Furthermore, using held out data they show thatthis is the probability that should be estimated by asmoothing method that takes into account he fre-quencies of the individual words.
Relying on thisresult, we will use frequency based es~imalion (usingword frequencies) as representative for smoothingestimates of unobserved cooccurrences, for compar-ison purposes.
As will be shown later, the problemwith smoothing estimates i that they ignore theexpected egree of association between the specificwords of the cooccurrence.
For example, we wouldnot like to estimate the same probability for twocooccurrences like 'eat bread' and 'eat cars', de-spite the fact that both 'bread' and 'cars' may havethe same frequency.Class based models (Brown et al, ; Pereiraet al, 1993; Hirschman, 1986; Resnik, 1992) dis-tinguish between unobserved cooccurrences usingclasses of "similar" words.
The probability of a spe-cific cooccurrence is determined using generalizedparameters about the probability of class cooccur-\] 64rence.
This approach, which follows long traditionsin semantic lassification, is very appealing, as itattempts to capture "typical" properties of classesof words.
However, it is not clear at all that un-restricted language is indeed structured the way itis assumed by class based models.
In particular,it is not clear that word cooccurrence patterns canbe structured and generalized to class cooccurrenceparameters without losing too much information.This paper suggests an alternative approachwhich assumes that class based generalizationsshould be avoided, and therefore liminates the in-termediate l vel of word classes.
Like some of theclass based models, we use a similarity metric tomeasure the similarity between cooccurrence pat-terns of words.
But then, rather than using thismetric to construct a set of word classes, we useit to identify the most specific analogies that canhe drawn for each specific estimation.
Thus, toestimate the probability of an unobserved cooccur-fence of words, we use data about other cooccur-fences that were observed in the corpus, and con-tain words that are similar to the given ones.
Forexample, to estimate the probability of the unob-served cooccurrence 'negative results', we use cooc-currences uch as 'positive results' and 'negativenumbers', that do occur in our corpus.The analogies we make are based on the as-sumption that similar word cooccurrences havesimilar values of mutual information.
Accordingly,our similarity metric was developed to capture sim-ilarities between vectors of mutual information val-ues.
In addition, we use an efficient search heuris-tic to identify the most similar words for a givenword, thus making the method computationallyaffordable.
Figure 1 illustrates a portion of thesimilarity network induced by the similarity metric(only some of the edges, with relatively high val-ues, are shown).
This network may be found usefulfor other purposes, independently of the estimationmethod.The estimation method was implemented usingthe relation of cooccurrence of two words withina limited distance in a sentence.
The proposedmethod, however, is general and is applicable foranY type of lexical cooccurrence.
The method wasevaluated in two experiments.
In the first one weachieved a complete scenario f the use of the esti-mation method, by implementing a variant of thed\[Sambiguation method in (Dagan et al, 1991), forsense selection in machine translation.
The esti-mation method was then successfully used to in-crease the coverage of the disambiguation methodby 15%, with an increase of the overall precisioncompared to a naive, frequency based, method.
Inthe second experiment we evaluated the estimationmethod on a data recovery task.
The task sim-ulates a typical scenario in disambiguation, andalso relates to theoretical questions about redun-dancy and idiosyncrasy in cooccurrence data.
Inthis evaluation, which involved 300 examples, theperformance of the estimation method was by 27%better than frequency based estimation.2 Def in i t ionsWe use the term cooccurrence pair, written as(x, y), to denote a cooccurrence of two words in asentence within a distance of no more than d words.When computing the distance d, we ignore functionwords such as prepositions and determiners.
In theexperiments reported here d = 3.A cooccurrence pair can be viewed as a gen-eralization of a bigram, where a bigram is a cooc-currence pair with d = 1 (without ignoring func-tion words).
As with bigrams, a cooccurrence pairis directional, i.e.
(x,y) ?
(y,x).
This capturessome information about the asymmetry in the lin-ear order of linguistic relations, such as the factthat verbs tend to precede their objects and followtheir subjects.The mutual information of a cooccurrence pair,which measures the degree of association betweenthe two words (Church and Hanks, 1990), is definedas (Fano, 1961):P(xly) I(x,y) -- log 2 P(x,y) _ log 2 (1) P(x)P(y) P(x)= log 2 P(y\[x)P(Y)where P(x)  and P(y) are the probabilities of theevents x and y (occurrences of words, in our case)and P(x, y) is the probability of the joint event (acooccurrence pair).We estimate mutual information values usingthe Maximum Likelihood Estimator (MLE):P (x ,y )  _log~.
N f(x,y) \]I(x, y) = log~ P~x)P--(y) ( -d f(x)f(y) "(2)where f denotes the frequency of an eyent andN is the length of the corpus.
While better es-timates for small probabilities are available (Good,1953; Church and Gale, 1991), MLE is the simplestto implement and was adequate for the purpose ofthis study.
Due to the unreliability of measuringnegative mutual information values in corpora thatare not extremely large, we have considered in thiswork any negative value to be 0.
We also set/~(x, y)to 0 if f (x,  y) = 0.
Thus, we assume in both casesthat the association between the two words is asexpected by chance.165paper  art ic les?14I  /\00 1conference .
0.132 .
papers  ~ /~ , ,U. I6 ~ ,  l",, "-,,worksh:p. , , ._  ~0.106 ~ ~ \0 .1260.
4 \?
symposmm ~ jbook  " ' documentat ion0.137Figure 1: A portion of the similarity network.3 Es t imat ion  fo r  an  UnobservedCooccur renceAssume that we have at our disposal a method fordetermining similarity between cooccurrence pat-terns of two words (as described in the next sec-tion).
We say that two cooccurrence pairs, (wl, w2)and (w~, w~), are similar if w~ is similar to wl andw~ is similar to w2.
A special (and stronger) caseof similarity is when the two pairs differ only inone of their words (e.g.
(wl,w~) and (wl,w2)).This special case is less susceptible to noise thanunrestricted similarity, as we replace only one ofthe words in the pair.
In our experiments, whichinvolved rather noisy data, we have used only thisrestricted type of similarity.
The mathematical for-mulations, though, are presented in terms of thegeneral case.The question that arises now is what analo-gies can be drawn between two similar cooccur-rence pairs, (wl,w2) and tw' wt~ Their proba- k 1' 21"bilities cannot be expected to be similar, since theprobabilities of the words in each pair can be dif-ferent.
However, since we assume that wl and w~have similar cooccurrence patterns, and so do w~and w~, it is reasonable to assume that the mutualinformation of the two pairs will be similar (recallthat mutual information measures the degree of as-sociation between the words of the pair).Consider for example the pair (chapter, de-scribes), which does not occur in our corpus 1.
Thispair was found to be similar to the pairs (intro-1 We used a corpus  of about  9 mill ion words of textsin the computer domain, taken from articles posted tothe USENET news system.duction, describes), (book, describes)and (section,describes), that do occur in the corpus.
Sincethese pairs occur in the corpus, we estimate theirmutual information values using equation 2, asshown in Table 1.
We then take the average ofthese mutual information values as the similaritybased estimate for I(chapter, describes), denotedas f(chapter, describes) 2.
This represents the as-sumption that the word 'describes' is associatedwith the word 'chapter' to a similar extent as itis associated with the words 'introduction', 'book'and 'section'.
Table 2 demonstrates how the anal-ogy is carried out also for a pair of unassociatedwords, such as (chapter, knows).In our current implementation, we computei(wl,  w2) using up to 6 most similar words to eachof wl and w~, and averaging the mutual informa-tion values of similar pairs that occur in the corpus(6 is a parameter, tuned for our corpus.
In somecases the similarity method identifies less than 6similar words).Having an estimate for the mutual informationof a pair, we can estimate its expected frequencyin a corpus of the given size using a variation ofequation 2:w2) = d f(wl)f(w2)2I(t?l't?2) (3) /(wl,In our example, f(chapter) = 395, N = 8,871,126and d = 3, getting a similarity based estimate off(chapter, describes)= 3.15.
This value is much2We use I for similarity based estimates, and reservei for the traditional maximum fikefihood estimate.
Thesimilarity based estimate will be used for cooccurrencepairs that do not occur in the corpus.166i(w ,(introduction, describes) 6.85(book, describes) 6.27(section, describes) 6.12f(wl,w2) f(wl) f(w2)5 464 27713 1800 2776 923 277Average:  6.41Table 1: The similarity based estimate as an average on similar pairs: \[(chapter, describes) = 6.41(wl, w2) \[(wl, w=)(introduction, knows) 0(book, knows) 0(section, knows) 0Average:  0f(wl,w2) f (w l )  f(w2)0 464 9280 1800 9280 923 928Table 2: The similarity based estimate for a pair of unassociated words: I(chapter, knows) = 0higher than the frequency based estimate (0.037),reflecting the plausibility of the specific combina-tion of words 3.
On the other hand, the similar-ity based estimate for \](chapter, knows) is 0.124,which is identical to the frequency based estimate,reflecting the fact that there is no expected associ-ation between the two words (notice that the fre-quency based estimate is higher for the second pair,due to the higher frequency of 'knows').4 TheS imi la r i ty  Met r i cAssume that we need to determine the degree ofsimilarity between two words, wl and w2.
Recallthat if we decide that the two words are similar,then we may infer that they have similar mutual in-formation with some other word, w. This inferencewould be reasonable if we find that on average wland w2 indeed have similar mutual information val-ues with other words in the lexicon.
The similaritymetric therefore measures the degree of similaritybetween these mutual information values.We first define the similarity between the mu-tual information values of Wl and w2 relative to asingle other word, w. Since cooccurrence pairs aredirectional, we get two measures, defined by the po-sition of w in the pair.
The left context similarity ofwl and w2 relative to w, termed simL(Wl, w2, w),is defined as the ratio between the two mutual in-formation values, having the larger value in the de-nominator:simL(wl, w2, w) = min(I(w, wl), I(w, w2)) (4)max(I(w, wl), I(w, w2))3The frequency based estimate for the expected fre-quency of a cooccurrence pair, assuming independentoccurrence of the two words and using their individualfrequencies, is -~f(wz)f(w2).
As mentioned earlier, weuse this estimate as representative for smoothing esti-mates of unobserved cooccurrences.This way we get a uniform scale between 0and 1, in which higher values reflect higher similar-ity.
If both mutual information values are 0, thensirnL(wl,w2, w) is defined to be 0.
The right con-text similarity, simn(wl, w2, w), is defined equiva-lently, for I(Wl, w) and I(w2, w) 4.Using definition 4 for each word w in the lex-icon, we get 2 ?
l similarity values for Wl and w2,where I is the size of the lexicon.
The general sim-ilarity between Wl and w2, termed sim(wl, w2), isdefined as a weighted average of these 2 ?
l values.It is necessary to use some weighting mechanism,since small values of mutual information tend to beless significant and more vulnerable to noisy data.We found that the maximal value involved in com-puting the similarity relative to a specific word pro-vides a useful weight for this word in computing theaverage.
Thus, the weight for a specific left contextsimilarity value, WL(Wl, W2, W), is defined as:Wt(wl, w) = max(I(w, wl), :(w, (5)(notice that this is the same as the denominator indefinition 4).
This definition provides intuitivelyappropriate weights, since we would like to givemore weight to context words that have a large mu-tual information value with at least one of Wl andw2.
The mutual information value with the otherword may then be large, providing a strong "vote"for similarity, or may be small, providing a strong"vote" against similarity.
The weight for a spe-cific right context similarity value is defined equiv-alently.
Using these weights, we get the weightedaverage in Figure 2 as the general definition of4In the case of cooccurrence pairs, a word may be in-volved in two types of relations, being the left or rightargument of the pair.
The definitions can be easilyadopted to cases in which there are more types of rela-tions, such as provided by syntactic parsing.167sim(wl, w2) =~toetexicon sirnL(wl, w2, w) .
WL(Wl, W2, W) -t- simR(wl, w2, w) .
WR(wl, w~, w) _WL(Wl, w2, w) + WR(wl, w2, w)Y'~,o e,,,,,i~or, min(I(w, wl), I(w, w2) + min(I(wl, w), I(w~, w))~wetexicon max(I(w, Wl), I(w, w2) + max(I(wx, w), I(w2, w) )(6)Figure 2: The definition of the similarity metric.Exhaust ive  Search  Approx imat ionsimilar words sim similar words simaspects 1.000topics 0.100areas 0.088expert 0.079issues 0.076approaches 0.072aspects 1.000topics 0.100areas 0.088expert 0.079issues 0.076concerning 0.069Table 3: The mosttic and exhaustiveresults.similar words of aspects: heuris-search produce nearly the samesimilarity s.The values produced by our metric have an in-tuitive interpretation, as denoting a "typical" ra-tio between the mutual information values of eachof the two words with another third word.
Themetric is reflexive (sirn(w,w) -- 1), symmetric(sim(wz, w2) = sirn(w2, wz)), but is not transitive(the values of sire(w1, w2) and sire(w2, w3) do notimply anything on the value of sire(w1, w3)).
Theleft column of Table 3 lists the six most similarwords to the word 'aspects' according to this met-ric, based on our corpus.
More examples of simi-larity were shown in Figure 1.4.1 An  e f f i c ient  search  heur i s t i cThe estimation method of section 3 requires thatwe identify the most similar words of a given wordw.
Doing this by computing the similarity betweenw and each word in the lexicon is computationallyvery expensive (O(12), where I is the size of thelexicon, and O(l J) to do this in advance for all thewords in the lexicon).
To account for this prob-lem we developed a simple heuristic that searchesfor words that are potentially similar to w, usingthresholds on mutual information values and fre-quencies of cooccurrence pairs.
The search is basedon the property that when computing sim(wl, w2),words that have high mutual information values5The nominator in our metric resembles the similar-ity metric in (Hindle, 1990).
We found, however, thatthe difference between the two metrics is important, be-cause the denominator serves as a normalization factor.with both wl and w2 make the largest contributionsto the value of the similarity measure.
Also, highand reliable mutual information values are typicallyassociated with relatively high frequencies of the in-volved cooccurrence pairs.
We therefore search firstfor all the "strong neighbors" of w, which are de-fined as words whose cooccurrence with w has highmutual information and high frequency, and thensearch for all their "strong neighbors".
The wordsfound this way ("the strong neighbors of the strongneighbors of w") are considered as candidates forbeing similar words of w, and the similarity valuewith w is then computed only for these words.
Wethus get an approximation for the set of words thatare most similar to w. For the example given in Ta-ble 3, the exhaustive method required 17 minutesof CPU time on a Sun 4 workstation, while the ap-proximation required only 7 seconds.
This wasdone using a data base of 1,377,653 cooccurrencepairs that were extracted from the corpus, alongwith their counts.5 Eva luat ions5.1 Word  sense  d isambiguat ion  inmach ine  t rans la t ionThe purpose of the first evaluation was to testwhether the similarity based estimation methodcan enhance the performance of a disambiguationtechnique.
Typically in a disambiguation task, dif-ferent cooccurrences correspond to alternative in-terpretations of the ambiguous construct.
It istherefore necessary that the probability estimatesfor the alternative cooccurrences will reflect he rel-ative order between their true probabilities.
How-ever, a consistent bias in the estimate is usually notharmful, as it still preserves the correct relative or-der between the alternatives.To carry out the evaluation, we implementeda variant of the disambiguation method of (Daganet al, 1991), for sense disambiguation i machinetranslation.
We term this method as THIS, forTarget Word Selection.
Consider for example theHebrew phrase 'laxtom xoze shalom', which trans-lates as 'to sign a peace treaty'.
The word 'laxtom',however, is ambiguous, and can be translated to ei-ther 'sign' or 'seal'.
To resolve the ambiguity, the168Precision ApplicabilityTWS 85.5 64.3Augmented TWS 83.6 79.6Word Frequency 66.9 100Table 4: Results of TWS, Augmented TWS andWord Frequency methodsTWS method first generates the alternative lexi-cal cooccurrence patterns in the targel anguage,that correspond to alternative selections of targetwords.
Then, it prefers those target words thatgenerate more frequent patterns.
In our example,the word 'sign' is preferred upon the word 'seal',since the pattern 'to sign a treaty' is much more fre-quent than the pattern 'to seal a treaty'.
Similarly,the word 'xoze' is translated to 'treaty' rather than'contract', due to the high frequency of the pattern'peace treaty '6.
In our implementation, cooccur-rence pairs were used instead of lexical cooccur-fence within syntactic relations (as in the originalwork), to save the need of parsing the corpus.We randomly selected from a software manuala set of 269 examples of ambiguous Hebrew wordsin translating Hebrew sentences to English.
Theexpected success rate of random selection for theseexamples was 23%.
The similarity based estima-tion method was used to estimate the expected fre-quency of unobserved cooccurrence pairs, in caseswhere none of the alternative pairs occurred inthe corpus (each pair corresponds to an alternativetarget word).
Using this method, which we termAugmented TWS, 41 additional cases were disam-biguated, relative to the original method.
We thusachieved an increase of about 15% in the applica-bility (coverage) of the TWS method, with a smalldecrease in the overall precision.
The performanceof the Augmented TWS method on these 41 exam-ples was about 15% higher than that of a naive,Word Frequency method, which always selects themost frequent ranslation.
It should be noted thatthe Word Frequency method is equivalent o us-ing the frequency based estimate, in which higherword frequencies entail a higher estimate for thecorresponding cooccurrence.
The results of the ex-periment are summarized in Table 4.5.2 A data  recovery  taskIn the second evaluation, the estimation methodhad to distinguish between members of two sets of8It should be emphasized that the TWS method usesonly a monolingual t rget corpus, and not a bilingualcorpus as in other methods ((Brown et al, 1991; Galeet al, 1992)).
The alternative cooccurrence patternsin the target language, which correspond to the alter-native translations of the ambiguous source words, areconstructed using a bilingual exicon.cooccurrence pairs, one of them containing pairswith relatively high probability and the other pairswith low probability.
To a large extent, this tasksimulates a typical scenario in disambiguation, asdemonstrated in the first evaluation.Ideally, this evaluation should be carried outusing a large set of held out data, which wouldprovide good estimates for the true probabilities ofthe pairs in the test sets.
The estimation methodshould then use a much smaller training corpus,in which none of the example pairs occur, andthen should try to recover the probabilities that areknown to us from the held out data.
However, sucha setting requires that the held out corpus wouldbe several times larger than the training corpus,while the latter should be large enough for robustapplication of the estimation method.
This was notfeasible with the size of our corpus, and the rathernoisy data we had.To avoid this problem, we obtained the set ofpairs with high probability from the training cor-pus, selecting pairs that occur at least 5 times.We then deleted these pairs from the data basethat is used by the estimation method, forcingthe method to recover their probabilities using theother pairs of the corpus.
The second set, of pairswith low probability, was obtained by constructingpairs that do not occur in the corpus.
The two sets,each of them containing 150 pairs, were constructedrandomly and were restricted to words with indi-vidual frequencies between 500 and 2500.
We termthese two sets as the occurring and non-occurringsets.The task of distinguishing between membersof the two sets, without access to the deleted fre-quency information, is by no means trivial.
Tryingto use the individual word frequencies will resultin performance close to that of using random selec-tion.
This is because the individual frequencies ofall participating words are within the same rangeof values.To address the task, we used the following pro-cedure: The frequency of each cooccurrence pairwas estimated using the similarity-based estima-tion method.
If the estimated frequency was above2.5 (which was set arbitrarily as the average of 5and 0), the pair was recovered as a member of theoccurring set.
Otherwise, it was recovered as amember of the non-occurring set.Out of the 150 pairs of the occurring set, ourmethod correctly identified 119 (79%).
For th enon-occurring set, it correctly identified 126 pairs(84%).
Thus, the method achieved an 0retail ac-curacy of 81.6%.
Optimal tuning of the threshold,to a value of 2, improves the overall accuracy to85%, where about 90% of the members of the oc-curring set and 80% of those in the non-occurring169set are identified correctly.
This is contrasted withthe optimal discrimination that could be achievedby frequency based estimation, which is 58%.Figures 3 and 4 illustrate the results of the ex-periment.
Figure 3 shows the distributions of theexpected frequency of the pairs in the two sets, us-ing similarity based and frequency based estima-tion.
It clearly indicates that the similarity basedmethod gives high estimates mainly to members ofthe occurring set and low estimates mainly to mem-bers of the non-occurring set.
Frequency based es-timation, on the other hand, makes a much poorerdistinction between the two sets.
Figure 4 plots thetwo types of estimation for pairs in the occurringset as a function of their true frequency in the cor-pus.
It can be seen that while the frequency basedestimates are always low (by construction) the sim-ilarity based estimates are in most cases closer tothe true value.6 ConclusionsIn both evaluations, similarity based estimationperforms better than frequency based estimation.This indicates that when trying to estimate cooc-currence probabilities, it is useful to consider thecooccurrence patterns of the specific words andnot just their frequencies, as smoothing methodsdo.
Comparing with class based models, our ap-proach suggests the advantage ofmaking the mostspecific analogies for each word, instead of makinganalogies with all members of a class, via generalclass parameters.
This raises the question whethergeneralizations over word classes, which follow longtraditions in semantic lassification, indeed providethe best means for inferencing about properties ofwords.AcknowledgementsWe are grateful to Alon Itai for his help in initiatingthis research.
We would like to thank Ken Churchand David Lewis for their helpful comments on ear-lier drafts of this paper.REFERENCESPeter Brown, Vincent Della Pietra, Peter deSouza,Jenifer Lai, and Robert Mercer.
Class-basedn-gram models of natural language.
Computa-tional Linguistics.
(To appear).P.
Brown, S. Della Pietra, V. Della Pietra, andR.
Mercer.
1991.
Word sense disambiguationusing statistical methods.
In Proc.
of the An-nual Meeting of the ACL.Kenneth W. Church and William A. Gale.
1991.A comparison of the enhanced Good-TuringI i optimal B occurringI ithreshold (85%) II non~ i " "O0 1 2 3 4 5 6 7 8 9 10 11 12Estimated Value: Similarity BasedB!
?ptimal B occurringnon -occurring00 0.20.4 0.6 0.8 t 1.21.41.61.8 2 2.2Estimated Value: Frequency BasedFigure 3: Frequency distributions of estimated fre-quency values for occurring and non-occurring sets.170oo,."o?
?oO",.?
?, "  += / *  + + ....-~+ ?
+++li!i ;!
:6 8 10 12 14 16 18True FrequencyFigure 4: Similarity based estimation ('+') and fre-quency based estimation ('0') for the expected fre-quency of members of the occurring set, as a func-tion of the true frequency.and deleted estimation methods for estimat-ing probabilities of English bigrams.
ComputerSpeech and Language, 5:19-54.Kenneth W. Church and Patrick Hanks.
1990.Word association orms, mutual information,and lexicography.
Computational Linguistics,16(1):22-29.Kenneth W. Church and Robert L. Mercer.
1992.Introduction to the special issue in computa-tional linguistics using large corpora.
Compu-tational Linguistics.
(In press).Ido Dagan and Alon Itai.
1990.
Automatic ac-quisition of constraints for the resolution ofanaphora references and syntactic ambiguities.In Proc.
of COLING.Ido Dagan, Alon Itai, and Ulrike Schwall.
1991.Two languages are more informative than one.In Proc.
of the Annual Meeting of the ACL.R.
Fano.
1961.
Transmission of Information.Cambridge,Mass:MIT Press.William Gale, Kenneth Church, and DavidYarowsky.
1992.
Using bilingual materialsto develop word sense disambiguation meth-ods.
In Proc.
of the International Conferenceon Theoretical and Methodolgical Issues in Ma-chine Translation.I.
J.
Good.
1953.
The population frequencies ofspecies and the estimation of population pa-rameters.
Biometrika, 40:237-264.R.
Grishman, L. Hirschman, and Ngo Thanh Nhan.1986.
Discovery procedures for sublanguage s -lectional patterns - initial experiments.
Com-putational Linguistics, 12:205-214.D.
Hindle and M. Rooth.
1991.
Structural am-biguity and lexical relations.
In Proc.
of theAnnual Meeting of the ACL.D.
Hindle.
1990.
Noun classification frompredicate-argument structures.
In Proc.
of theAnnual Meeting of the ACL.L.
Hirschman.
1986.
Discovering sublanguagestructures.
In R. Grishman and R. Kittredge,editors, Analyzing Language in Restricted Do-mains: Sublanguage Description and Process-ing, pages 211-234.
Lawrence Erlbaum Asso-ciates.F.
Jelinek and R. Mercer.
1985.
Probability dis-tribution estimation from sparse data.
IBMTechnical Disclosure Bulletin, 28:2591-2594.Frederick Jelinek.
1990.
Self-organized languagemodeling for speech recognition.
In AlexWaibel and Kai-Fu Lee, editors, Readings inSpeech Recognition, pages 450-506.
MorganKaufmann Publishers, Inc., San Maeio, Cali-fornia.Slava M. Katz.
1987.
Estimation of probabilitiesfrom sparse data for the language model com-ponent of a speech recognizer.
IEEE Transac-tions on Acoustics, speech, and Signal Process-ing, 35(3):400-401.Yoelle Maarek and Frank Smadja.
1989.
Full textindexing based on lexical relations - An appli-cation: Software libraries.
In Proc.
of SIGIR.Fernando Pereira, Naftali Tishby, and Lillian Lee.1993.
Distributional clustering of Englishwords.
In Proc.
of the Annual Meeting of theACL.Philip Resnik.
1992.
Wordnet and distributionalanalysis: A class-based approach to lexical dis-covery.
In AAAI  Workshop on Statistically-based Natural Language Processing Techniques,July.V.
Sadler.
1989.
Working with analogical seman-tics: Disambiguation techniques in DLT.
ForisPublications.Frank Smadja nd Katheleen McKeown.
1990.
Au-tomatically extracting and representing collo-cations for language generation.
In Proc.
of theAnnual Meeting of the ACL.171
