Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 598?602, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsLIMSIILES: Basic English Substitution for Student Answer Assessment atSemEval 2013Martin GleizeLIMSI-CNRS & ENSB.P.
133 91403 ORSAY CEDEX, Francegleize@limsi.frBrigitte GrauLIMSI-CNRS & ENSIIEB.P.
133 91403 ORSAY CEDEX, Francebg@limsi.frAbstractIn this paper, we describe a method for as-sessing student answers, modeled as a para-phrase identification problem, based on sub-stitution by Basic English variants.
Basic En-glish paraphrases are acquired from the Sim-ple English Wiktionary.
Substitutions are ap-plied both on reference answers and studentanswers in order to reduce the diversity oftheir vocabulary and map them to a commonvocabulary.
The evaluation of our approachon the SemEval 2013 Joint Student ResponseAnalysis and 8th Recognizing Textual Entail-ment Challenge data shows promising results,and this work is a first step toward an open-domain system able to exhibit deep text un-derstanding capabilities.1 IntroductionAutomatically assessing student answers is a chal-lenging natural language processing task (NLP).
Itis a way to make test grading easier and improveadaptive tutoring (Dzikovska et al 2010), and is thegoal of the SemEval 2013?s task 7, titled Joint Stu-dent Response Analysis.
More specifically, given aquestion, a known correct ?reference answer?
and a1- or 2-sentence student answer, the goal is to deter-mine the student?s answer accuracy (Dzikovska etal., 2013).
This can be seen as a paraphrase identi-fication problem between student answers and refer-ence answers.Paraphrase identification searches whether twosentences have essentially the same meaning (Culi-cover, 1968).
Automatically generating or extract-ing semantic equivalences for the various units oflanguage ?
words, phrases, and sentences ?
is an im-portant problem in NLP and is being increasinglyemployed to improve the performance of severalNLP applications (Madnani and Dorr, 2010), likequestion-answering and machine translation.Paraphrase identification would benefit froma precise and broad-coverage semantic languagemodel.
This is unfortunately difficult to obtain to itsfull extent for any natural language, due to the sizeof a typical lexicon and the complexity of grammat-ical constructions.
Our hypothesis is that the sim-pler the language lexicon is, the easier it will be toaccess and compare meaning of sentences.
This as-sumption is justified by the multiple attempts at con-trolled natural languages (Schwitter, 2010) and es-pecially simplified forms of English.
One of them,Basic English (Ogden, 1930), has been adopted bythe Wikipedia Project as the preferred language ofthe Simple English Wikipedia1 and its sister projectthe Simple English Wiktionary2.Our method starts with acquiring paraphrasesfrom the Simple English Wiktionary?s definitions.Using those, we generate variants of both sentenceswhose meanings are to be compared.
Finally, wecompute traditional lexical and semantic similaritymeasures on those two sets of variants to producefeatures to train a classifier on the SemEval 2013datasets in order to take the final decision.2 Acquiring simplifying paraphrasesSimple Wiktionary word definitions are differentfrom usual dictionary definitions.
Aside from the1http://simple.wikipedia.org2http://simple.wiktionary.org598simplified language, they often prefer to give acomplete sentence where the word ?
e.g.
a verb ?
isused in context, along with an explanation of what itmeans.
To define the verb link, Simple Wiktionarystates that If you link two or more things, you make aconnection between them (1), whereas the standardWiktionary uses the shorter and more cryptic Toconnect two or more things.We notice in this example that the definitionfrom Simple Wiktionary consists of two clauses,linked by a subordination relation.
It?s actually thecase for a lot of verb definitions: a quick statisticalstudy shows that 70% of these definitions arecomposed of two clauses, an independent clause,and a subordinate clause (often an adverbial clause).One clause illustrates how the verb is used, theother gives the explanation and the actual dictionarydefinition, as in example (1).
These definitions arethe basis of our method for acquiring paraphrases.2.1 Pre-processingWe use the Stanford Parser to parse the definitionsand get a dependency graph (De Marneffe and Man-ning, 2008).
Using a few hand-written rules, we thenretrieve both parts of the definition, which we callthe word part and the defining part (see table 1 page3 for examples).
We can do this for definitions ofverbs, but also for nouns, like the giraffe is the tallestland animal in the world to define giraffe, or adjec-tives, like if something is bright it gives out or fillswith much light to define bright.
We only providethe details of our method for processing verb defini-tions, as they correspond to the most complex cases,but we proceed similarly for noun, adjective and ad-verb definitions.2.2 Argument matchingWord and defining parts alone are not paraphrases,but we can obtain phrasal paraphrases from them.
Ifwe see word part and defining part as two semanti-cally equivalent predications, we have to identify thetwo predicates with their arguments, then match ar-guments with corresponding meaning, i.e.
match ar-guments which designate the same entity or assumethe same semantic function in both parts, as showedin Table 2.For verb definitions, we identify the predicates asyou ?
youlink ?
make?
?
a connection?
?
betweentwo or more things ?
themTable 2: Complete matching for the definition of verb linkthe main verbs in both clauses (hence link matchingwith make in table 2) and their arguments as a POS-filtered list of their syntactic descendants.
Then,our assumption is that every argument of the wordpart predicate is present in the defining part, andthe defining part predicate can have extra arguments(like a connection).We define s(A,B), the score of the pair of argu-ments (A,B), with argument A in the word part andargument B in the defining part.
We then define amatching M as a set of such pairs, such that ev-ery element of every possible pair of arguments isfound at most one time in M .
A complete match-ing is a matching M that matches every argumentin the word part, i.e., for each word part argumentA, there exists a pair of arguments in M which con-tains A.
Finally, we compute the matching score ofM , S(M), as the sum of scores of all pairs of M .The score function s(A,B) is a hand-crafted lin-ear combination of several features computed on apair of arguments (A,B) including:?
Raw string similarity.
Sometimes the sameword is reused in the defining part.?
Having an equal/compatible dependency rela-tion with their respective main verb.?
Relative position in clause.?
Relative depth in parsing tree.
These last 3 fea-tures assess if the two arguments play the samesyntactic role.?
Same gender and number.
If different, it?sunlikely that the two arguments designate thesame entity.?
If (A,B) is a pair (noun phrase, pronoun).
Wehope to capture an anaphoric expression and itsantecedent.599Word (POS-tag) Word part Defining partlink (V) you link two or more things you make a connection between themgiraffe (N) the giraffe the tallest land animal in the worldbright (Adj) something is bright it gives out or fills with much lightTable 1: Word part and defining part of some Simple Wiktionary definitions?
WordNet similarity (Pedersen et al 2004).
Ifwords belong to close synsets, they?re morelikely to identify the same entity.2.3 Phrasal paraphrasesWe compute the complete matching M which maxi-mizes the matching score S(M).
Although it is pos-sible to enumerate all matchings, it is intractable;therefore when predicates have more than 4 argu-ments, we prefer constructing a best matching with abeam search algorithm.
After replacing each pair ofarguments with linked variables, and attaching un-matched arguments to the predicates, we finally ob-tain phrasal paraphrases of this form:?
X link Y , X make a connection between Y ?3 Paraphrasing exercise answers3.1 Paraphrase generation and pre-rankingGiven a sentence, and our Simple Wiktionary para-phrases (about 20,650 extracted paraphrases), wecan generate sentential paraphrases by simple syn-tactic pattern matching ?and do so recursively bytaking previous outputs as input?, with the intentthat these new sentences use increasingly more Ba-sic English.
We generate as many variants startingfrom both reference answers and student answers aswe can in a fixed amount of time, as an anytime al-gorithm would do.
We prioritize substituting verbsand adjectives over nouns, and non Basic Englishwords over Basic English words.Given a student answer and reference answers, wethen use a simple Jaccard distance (on lowercasedlemmatized non-stopwords) to score the closenessof student answer variants to reference answer vari-ants: we measure how close the vocabulary used inthe two statements has become.
More specifically,for each reference answer A, we compute the n clos-est variants of the student answer to A?s variant set.In our experiments, n = 10.
We finally rank thereference answers according to the average distancefrom their n closest variants to A?s variant set andkeep the top-ranked one for our classification exper-iment.
Figure 1 illustrates the whole process.RA1RA2...SA012345RA2RA11.
1A BC 1.
52.
32.
3......Figure 1: Variants are generated from all reference an-swers (RA) and the student answer (SA).
For each ref-erence answer RA, student answer variants are rankedbased on their lexical distance from the variants of RA.The reference with the n closer variants to the studentvariants is kept (here: RA1).3.2 Classifying student answersSemEval 2013 task 7 offers 3 problems: a 5-waytask, with 5 different answer judgements, and 3-wayand 2-way tasks, conflating more judgement cate-gories each time.
Two different corpora, Beetle andSciEntsBank, were labeled with the 5 following la-bels: Correct, Partially correct incomplete, Contra-dictory, Irrelevant and Non Domain, as described in(Dzikovska et al 2012).
We see the n-way task as an-way classification problem.
The instances of thisproblem are the pairs (student answer, reference an-swer).We compute for each instance the following fea-tures: For each of the n closest variants of the stu-dent answer to some variant of the reference answercomputed in the pre-ranking phase:?
Jaccard similarity coefficient on non-stopwords.?
A boolean representing if the two statementshave the same polarity or not, where polarity600is defined as the number of neg dependenciesin the Stanford Parser dependency graph.?
Number of ?paraphrasing steps?
necessary toobtain the variant from a raw student answer.?
Highest WordNet similarity of their respectivenouns.?
WordNet similarity of the main verbs.General features:?
Answer count (how many students typed thisanswer), provided in the datasets.?
Length ratio between the student answer andthe closest reference answer.?
Number of (non-stop)words which appear nei-ther in the question nor the reference answers.We train an SVM classifier (with a one-against-oneapproach to multiclass classification) on both Beetleand SciEntsBank, for each n-way task.3.3 EvaluationTable 3 presents our system?s overall accuracy on the5-way task, along with the top scores at SemEval2013, mean scores, and baselines ?majority classand lexical overlap?
described in (Dzikovska et al2012).SystemBeetleunseen answersSciEntsBankunseen questionsMajority 0.4010 0.4110Lexicaloverlap0.5190 0.4130Mean 0.5326 0.4078ETS-run-1 0.5740 0.5320ETS-run-2 0.7150 0.4010SimpleWiktio0.5330 0.4820Table 3: SemEval 2013 evaluation results.Our system performs slightly better in overall ac-curacy on Beetle unseen answers and SciEntsBankunseen questions than both baselines and the meanscores.
While results are clearly below the best sys-tem trained on the Beetle corpus questions, we holdthe third best score for the 5-way task on SciEnts-Bank unseen questions, while not fine-tuning oursystem specifically for this corpus.
This is ratherencouraging as to how suitable Simple Wiktionaryis as a resource to extract open-domain knowledgefrom.4 DiscussionThe system we present in this paper is the firststep towards an open-domain machine reading sys-tem capable of understanding and reasoning.
Di-rect modeling of the semantics of a full natural lan-guage appears too difficult.
We therefore decide tofirst project the English language onto a simpler En-glish, so that it is easier to model and draw infer-ences from.One complementary approach to a minimalisticlanguage model, is to accept that texts are repletewith gaps: missing information that cannot be in-ferred by reasoning on the text alone, but requirea certain amount of background knowledge.
Penasand Hovy (2010) show that these gaps can be filledby maintaining a background knowledge base builtfrom a large corpus.Although Simple Wiktionary is not a large corpusby any means, it can serve our purpose of acquiringbasic knowledge for assessing exercise answers, andhas the advantage to be in constant evolution and ex-pansion, as well as interfacing very easily with thericher Wiktionary and Wikipedia.Our future work will be focused on enriching andimproving the robustness of our knowledge acqui-sition step from Simple Wiktionary, as well as in-troducing a true normalization of English to BasicEnglish.AcknowledgmentsWe acknowledge the Wikimedia Foundation fortheir willingness to provide easily usable versionsof their online collaborative resources.ReferencesP.W.
Culicover.
1968.
Paraphrase generation andinformation retrieval from stored text.
In MechanicalTranslation and Computational Linguistics, 11(12),7888.601Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
Stanford typed dependencies manual.Technical report, Stanford University.Myroslava O. Dzikovska, Diana Bental, Johanna D.Moore, Natalie Steinhauser, Gwendolyn Campbell,Elaine Farrow, and Charles B. Callaway.
2010.Intelligent tutoring with natural language supportin the BEETLE II system.
In Proceedings of FifthEuropean Conference on Technology EnhancedLearning (EC-TEL 2010), Barcelona.Myroslava O. Dzikovska, Rodney D. Nielsen and ChrisBrew.
2012.
Towards Effective Tutorial Feedbackfor Explanation Questions: A Dataset and Baselines.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-HLT 2012), Montreal.Myroslava O. Dzikovska, Rodney Nielsen, Chris Brew,Claudia Leacock, Danilo Giampiccolo, Luisa Ben-tivogli, Peter Clark, Ido Dagan and Hoa Trang Dang.2013.
SemEval-2013 Task 7: The Joint StudentResponse Analysis and 8th Recognizing TextualEntailment Challenge.
In Proceedings of the 7thInternational Workshop on Semantic Evaluation(SemEval 2013), in conjunction with the Second JointConference on Lexical and Computational Seman-tics (*SEM 2013).
Atlanta, Georgia, USA.
13-14 June.Nitin Madnani and Bonnie J. Dorr.
2010.
Generatingphrasal and sentential paraphrases: A survey ofdata-driven methods.
In Computational Linguistics36 (3), 341-387.Charles Kay Ogden.
1930.
Basic English: A GeneralIntroduction with Rules and Grammar.
Paul Treber,London.Ted Pedersen, Siddharth Patwardhan, and JasonMichelizzi.
2004.
WordNet::similarity?measuringthe relatedness of concepts.
In Proceedings ofthe Nineteenth National Conference on ArtificialIntelligence(AAAI-04), pages 10241025.Anselmo Penas and Eduard H. Hovy.
2010.
FillingKnowledge Gaps in Text for Machine Reading.
COL-ING (Posters) 2010: 979-987, Beijing.Rolf Schwitter.
2010.
Controlled Natural Languages forKnowledge Representation.
COLING (Posters) 2010,Beijing.602
