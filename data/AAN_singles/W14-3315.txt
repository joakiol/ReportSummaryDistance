Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 142?149,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsThe CMU Machine Translation Systems at WMT 2014Austin Matthews Waleed Ammar Archna Bhatia Weston FeelyGreg Hanneman Eva Schlinger Swabha Swayamdipta Yulia TsvetkovAlon Lavie Chris Dyer?Language Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213 USA?Corresponding author: cdyer@cs.cmu.eduAbstractWe describe the CMU systems submittedto the 2014 WMT shared translation task.We participated in two language pairs,German?English and Hindi?English.
Ourinnovations include: a label coarseningscheme for syntactic tree-to-tree transla-tion, a host of new discriminative features,several modules to create ?synthetic trans-lation options?
that can generalize beyondwhat is directly observed in the trainingdata, and a method of combining the out-put of multiple word aligners to uncoverextra phrase pairs and grammar rules.1 IntroductionThe MT research group at Carnegie Mellon Uni-versity?s Language Technologies Institute partici-pated in two language pairs for the 2014 Workshopon Machine Translation shared translation task:German?English and Hindi?English.
Our systemsshowcase our multi-phase approach to translation,in which synthetic translation options supple-ment the default translation rule inventory that isextracted from word-aligned training data.In the German?English system, we used ourcompound splitter (Dyer, 2009) to reduce datasparsity, and we allowed the translator to backoff to translating lemmas when it detected case-inflected OOVs.
We also demonstrate our group?ssyntactic system with coarsened nonterminal types(Hanneman and Lavie, 2011) as a contrastiveGerman?English submission.In both the German?English and Hindi?Englishsystems, we used an array of supplemental ideas toenhance translation quality, ranging from lemma-tization and synthesis of inflected phrase pairs tonovel reordering and rule preference features.2 Core System ComponentsThe decoder infrastructure we used was cdec(Dyer et al., 2010).
For our primary systems,all data was tokenized using cdec?s tokenizationtool.
Only the constrained data resources pro-vided for the shared task were used for trainingboth the translation and language models.
Wordalignments were generated using both FastAlign(Dyer et al., 2013) and GIZA++ (Och and Ney,2003).
All our language models were estimatedusing KenLM (Heafield, 2011).
Translation modelparameters were chosen using MIRA (Eidelman,2012) to optimize BLEU (Papineni et al., 2002)on a held-out development set.Our data was filtered using qe-clean(Denkowski et al., 2012), with a cutoff oftwo standard deviations from the mean.
Alldata was left in fully cased form, save the firstletter of each segment, which was changed towhichever form the first token more commonlyused throughout the data.
As such, words like Thewere lowercased at the beginning of segments,while words like Obama remained capitalized.Our primary German?English and Hindi?English systems were Hiero-based (Chiang,2007), while our contrastive German?English sys-tem used cdec?s tree-to-tree SCFG formalism.Before submitting, we ran cdec?s implementa-tion of MBR on 500-best lists from each of oursystems.
For both language pairs, we used theNelder?Mead method to optimize the MBR pa-rameters.
In the German?English system, we ranMBR on 500 hypotheses, combining the output ofthe Hiero and tree-to-tree systems.The remainder of the paper will focus on ourprimary innovations in the two language pairs.1423 Common System ImprovementsA number of our techniques were used for both ourGerman?English and Hindi?English primary sub-missions.
These techniques each fall into one ofthree categories: those that create translation rules,those involving language models, or those that addtranslation features.
A comparison of these tech-niques and their performance across the two lan-guage pairs can be found in Section 6.3.1 Rule-Centric EnhancementsWhile many of our methods of enhancing thetranslation model with extra rules are language-specific, three were shared between languagepairs.First, we added sentence-boundary tokens <s>and </s> to the beginning and end of each line inthe data, on both the source and target sides.Second, we aligned all of our training data us-ing both FastAlign and GIZA++ and simply con-catenated two copies of the training corpus, onealigned with each aligner, and extracted rules fromthe resulting double corpus.Third, we hand-wrote a list of rules that trans-form numbers, dates, times, and currencies intowell-formed English equivalents, handling differ-ences such as the month and day reversal in datesor conversion from 24-hour time to 12-hour time.3.2 Employed Language ModelsEach of our primary systems uses a total of threelanguage models.The first is a traditional 4-gram model gen-erated by interoplating LMs built from each ofthe available monolingual corpora.
Interpolationweights were calculated used the SRILM toolkit(Stolcke, 2002) and 1000 dev sentences from theHindi?English system.The second is a model trained on word clus-ters instead of surface forms.
For this we mappedthe LM vocabulary into 600 clusters based on thealgorithm of Brown et al.
(1992) and then con-structed a 7-gram LM over the resulting clusters,allowing us to capture more context than our tra-ditional surface-form language model.The third is a bigram model over the source sideof each language?s respective bitext.
However, atrun time this LM operates on the target-side out-put of the translator, just like the other two.
Theintuition here is that if a source-side LM likes ouroutput, then we are probably passing through morethan we ought to.Both source and target surface-form LM usedmodified Kneser-Ney smoothing (Kneser and Ney,1995), while the model over Brown clusters(Brown et al., 1992) used subtract-0.5 smoothing.3.3 New Translation FeaturesIn addition to the standard array of features, weadded four new indicator feature templates, lead-ing to a total of nearly 150,000 total features.The first set consists of target-side n-gram fea-tures.
For each bigram of Brown clusters in theoutput string generated by our translator, we firean indicator feature.
For example, if we have thesentence, Nato will ihren Einfluss im Osten st?arkentranslating as NATO intends to strengthen its influ-ence in the East, we will fire an indicator featuresNGF C367 C128=1, NGF C128 C31=1, etc.The second set is source-language n-gram fea-tures.
Similar to the previous feature set, we firean indicator feature for each ngram of Brown clus-ters in the output.
Here, however, we use n = 1,and we use the map of source language words toBrown clusters, rather than the target language?s,despite the fact that this is examining target lan-guage output.
The intuition here is to allow thisfeature to penalize passthroughs differently de-pending on their source language Brown cluster.For example, passing through the German wordzeitung (?newspaper?)
is probably a bad idea, butpassing through the German word Obama proba-bly should not be punished as severely.The third type of feature is source path features.We can imagine translation as a two-step processin which we first permute the source words intosome order, then translate them phrase by phrase.This set of features examines that intermediatestring in which the source words have been per-muted.
Again, we fire an indicator feature for eachbigram in this intermediate string, this time usingsurface lexical forms directly instead of first map-ping them to Brown clusters.Lastly, we create a new type of rule shape fea-ture.
Traditionally, rule shape features have indi-cated, for each rule, the sequence of terminal andnon-terminal items on the right-hand side.
For ex-ample, the rule [X] ?
der [X] :: the [X] mighthave an indicator feature Shape TN TN, whereT represents a terminal and N represents a non-terminal.
One can also imagine lexicalizing suchrules by replacing each T with its surface form.We believe such features would be too sparse, soinstead of replacing each terminal by its surfaceform, we instead replace it with its Brown cluster,143creating a feature like Shape C37 N C271 N.4 Hindi?English Specific ImprovementsIn addition to the enhancements common to thetwo primary systems, our Hindi?English systemincludes improved data cleaning of developmentdata, a sophisticated linguistically-informed tok-enization scheme, a transliteration module, a syn-thetic phrase generator that improves handling offunction words, and a synthetic phrase generatorthat leverages source-side paraphrases.
We willdiscuss each of these five in turn.4.1 Development Data CleaningDue to a scarcity of clean development data, weaugmented the 520 segments provided with 480segments randomly drawn from the training datato form our development set, and drew anotherrandom 1000 segments to serve as a dev test set.After observing large discrepencies between thetypes of segments in our development data and thewell-formed news domain sentences we expectedto be tested on, we made the decision to prune ourtuning set by removing any segment that did notappear to be a full sentence on both the Hindi andEnglish sides.
While this reduced our tuning setfrom 1000 segments back down to 572 segments,we believe it to be the single largest contributor toour success on the Hindi?English translation task.4.2 Nominal NormalizationAnother facet of our system was normalization ofHindi nominals.
The Hindi nominal system showsmuch more morphological variation than English.There are two genders (masculine and feminine)and at least six noun stem endings in pronuncia-tion and 10 in writing.The pronominal system also is much richer thanEnglish with many variants depending on whetherpronouns appear with case markers or other post-positions.Before normalizing the nouns and pronouns, wefirst split these case markers / postpositions fromthe nouns / pronouns to result in two words in-stead of the original combined form.
If the casemarker was n (ne), the ergative case marker inHindi, we deleted it as it did not have any trans-lation in English.
All the other postpositions wereleft intact while splitting from and normalizing thenouns and pronouns.These changes in stem forms contribute to thesparsity in data; hence, to reduce this sparsity, weconstruct for each input segment an input latticethat allows the decoder to use the split or originalforms of all nouns or pronouns, as well as allowingit to keep or delete the case marker ne.4.3 TransliterationWe used the 12,000 Hindi?English transliterationpairs from the ACL 2012 NEWS workshop ontransliteration to train a linear-chained CRF tag-ger1that labels each character in the Hindi tokenwith a sequence of zero or more English characters(Ammar et al., 2012).
At decoding, unseen Hinditokens are fed to the transliterator, which producesthe 100 most probable transliterations.
We adda synthetic translation option for each candidatetransliteration.In addition to this sophisticated transliterationscheme, we also employ a rule-based translitera-tor that specifically targets acronyms.
In Hindi,many acronyms are spelled out phonetically, suchas NSA being rendered as enese (en.es.e).
Wedetected such words in the input segments andgenerated synthetic translation options both withand without periods (e.g.
N.S.A.
and NSA).4.4 Synthetic Handling of Function WordsIn different language pairs, individual sourcewords may have many different possible trans-lations, e.g., when the target language word hasmany different morphological inflections or is sur-rounded by different function words that have nodirect counterpart in the source language.
There-fore, when very large quantities of parallel dataare not available, we can expect our phrasal inven-tory to be incomplete.
Synthetic translation optiongeneration seeks to fill these gaps using secondarygeneration processes that exploit existing phrasepairs to produce plausible phrase translation alter-natives that are not directly extractable from thetraining data (Tsvetkov et al., 2013; Chahuneau etal., 2013).To generate synthetic phrases, we first removefunction words from the source and target sidesof existing non-gappy phrase pairs.
We manuallyconstructed English and Hindi lists of commonfunction words, including articles, auxiliaries, pro-nouns, and adpositions.
We then employ theSRILM hidden-ngram utility (Stolcke, 2002) to re-store missing function words according to an n-gram language model probability, and add the re-sulting synthetic phrases to our phrase table.1https://github.com/wammar/transliterator1444.5 Paraphrase-Based Synthetic PhrasesWe used a graph-based method to obtain transla-tion distributions for source phrases that are notpresent in the phrase table extracted from the par-allel corpus.
Monolingual data is used to constructseparate similarity graphs over phrases (word se-quences or n-grams), using distributional featuresextracted from the corpora.
The source similar-ity graph consists of phrase nodes representing se-quences of words in the source language.
In ourinstance, we restricted the phrases to bigrams, andthe bigrams come from both the phrase table (thelabeled phrases) and from the evaluation set butnot present in the phrase table (unlabeled phrases).The labels for these source phrases, namely thetarget phrasal inventory, can also be representedin a graph form, where the distributional featurescan also be computed from the target monolingualdata.
Translation information is then propagatedfrom the labeled phrases to the unlabeled phrasesin the source graph, proportional to how similarthe phrases are to each other on the source side,as well as how similar the translation candidatesare to each other on the target side.
The newlyacquired translation distributions for the unlabeledphrases are written out to a secondary phrase table.For more information, see Saluja et al.
(2014).5 German?English SpecificImprovementsOur German?English system also had its ownsuite of tricks, including the use of ?pseudo-references?
and special handling of morphologi-cally inflected OOVs.5.1 Pseudo-ReferencesThe development sets provided have only a sin-gle reference, which is known to be sub-optimalfor tuning of discriminative models.
As such,we use the output of one or more of last year?stop performing systems as pseudo-references dur-ing tuning.
We experimented with using just onepseudo-reference, taken from last year?s Spanish?English winner (Durrani et al., 2013), and withusing four pseudo-references, including the out-put of last year?s winning Czech?English, French?English, and Russian?English systems (Pino et al.,2013).5.2 Morphological OOVsExamination of the output of our baseline sys-tems lead us to conclude that the majority of oursystem?s OOVs were due to morphologically in-flected nouns in the input data, particularly thosein genitive case.
As such, for each OOV in theinput, we attempt to remove the German genitivecase marker -s or -es.
We then run the resultingform f through our baseline translator to obtain atranslation e of the lemma.
Finally, we add twotranslation rules to our translation table: f ?
e,and f ?
e?s.6 ResultsAs we added each feature to our systems, wefirst ran a one-off experiment comparing our base-line system with and without each individual fea-ture.
The results of that set of experiments areshown in Table 1 for Hindi?English and Table 2for German?English.
Features marked with a *were not included in our final system submission.The most surprising result is the strength ofour Hindi?English baseline system.
With no extrabells or whistles, it is already half a BLEU pointahead of the second best system submitted to thisshared task.
We believe this is due to our filter-ing of the tuning set, which allowed our system togenerate translations more similar in length to thefinal test set.Another interesting result is that only one fea-ture set, namely our rule shape features based onBrown clusters, helped on the test set in both lan-guage pairs.
No feature hurt the BLEU score onthe test set in both language pairs, meaning themajority of features helped in one language andhurt in the other.If we compare results on the tuning sets, how-ever, some clearer patterns arise.
Brown clusterlanguage models, n-gram features, and our newrule shape features all helped.Furthermore, there were a few features, such asthe Brown cluster language model and tuning toMeteor (Denkowski and Lavie, 2011), that helpedsubstantially in one language pair while just barelyhurting the other.
In particular, the fact that tuningto Meteor instead of BLEU can actually help bothBLEU and Meteor scores was rather unexpected.7 German?English Syntax SystemIn addition to our primary German?English sys-tem, we also submitted a contrastive German?English system showcasing our group?s tree-to-tree syntax-based translation formalism.145Test (2014) Dev Test (2012)System BLEU Met TER BLEU Met TERBaseline 15.7 25.3 68.0 11.4 22.9 70.3*Meteor Tuning 15.2 25.8 71.3 12.8 23.7 71.3Sentence Boundaries 15.2 25.4 69.1 12.1 23.4 70.0Double Aligners 16.1 25.5 66.6 11.9 23.1 69.2Manual Number Rules 15.7 25.4 68.5 11.6 23.0 70.3Brown Cluster LM 15.6 25.1 67.3 11.5 22.7 69.8*Source LM 14.2 25.1 72.1 11.3 23.0 72.3N-Gram Features 15.6 25.2 67.9 12.2 23.2 69.2Src N-Gram Features 15.3 25.2 68.9 12.0 23.4 69.5Src Path Features 15.8 25.6 68.8 11.9 23.3 70.4Brown Rule Shape 15.9 25.4 67.2 11.8 22.9 69.6Lattice Input 15.2 25.8 71.3 11.4 22.9 70.3CRF Transliterator 15.7 25.7 69.4 12.1 23.5 70.1Acronym Translit.
15.8 25.8 68.8 12.4 23.4 70.2Synth.
Func.
Words 15.7 25.3 67.8 11.4 22.8 70.4Source Paraphrases 15.6 25.2 67.7 11.5 22.7 69.9Final Submission 16.7Table 1: BLEU, Meteor, and TER results for one-off experiments conducted on the primary Hiero Hindi?English system.
Each line is the baseline plus that one feature, non-cumulatively.
Lines marked with a *were not included in our final WMT submission.Test (2014) Dev Test (2012)System BLEU Met TER BLEU Met TERBaseline 25.3 30.4 52.6 26.2 31.3 53.6*Meteor Tuning 26.2 31.3 53.1 26.9 32.2 54.4Sentence Boundaries 25.4 30.5 52.2 26.1 31.4 53.3Double Aligners 25.2 30.4 52.5 26.0 31.3 53.6Manual Number Rules 25.3 30.3 52.5 26.1 31.4 53.4Brown Cluster LM 26.4 31.0 51.9 27.0 31.8 53.2*Source LM 25.8 30.6 52.4 26.4 31.5 53.4N-Gram Features 25.4 30.4 52.6 26.7 31.6 53.0Src N-Gram Features 25.3 30.5 52.5 26.2 31.5 53.4Src Path Features 25.0 30.1 52.6 26.0 31.2 53.3Brown Rule Shape 25.5 30.5 52.4 26.3 31.5 53.2One Pseudo Ref 25.5 30.4 52.6 34.4 32.7 49.3*Four Psuedo Refs 22.6 29.2 52.6 49.8 35.0 46.1OOV Morphology 25.5 30.5 52.4 26.3 31.5 53.3Final Submission 27.1Table 2: BLEU, Meteor, and TER results for one-off experiments conducted on the primary HieroGerman?English system.
Each line is the baseline plus that one feature, non-cumulatively.Dev (2013) Dev Test (2012)System BLEU Met TER BLEU Met TERBaseline 20.98 29.81 58.47 18.65 28.72 61.80+ Label coarsening 23.07 30.71 56.46 20.43 29.34 60.16+ Meteor tuning 23.48 30.90 56.18 20.96 29.60 59.87+ Brown LM + Lattice + Synthetic 24.46 31.41 56.66 21.50 30.28 60.51+ Span limit 15 24.20 31.25 55.48 21.75 29.97 59.18+ Pseudo-references 24.55 31.30 56.22 22.10 30.12 59.73Table 3: BLEU, Meteor, and TER results for experiments conducted in the tree-to-tree German?Englishsystem.
The system in the bottom line was submitted to WMT as a contrastive entry.7.1 Basic System ConstructionSince all training data for the tree-to-tree systemmust be parsed in addition to being word-aligned,we prepared separate copies of the training, tun-ing, and testing data that are more suitable for in-put into constituency parsing.
Importantly, we leftthe data in its original mixed-case format.
We usedthe Stanford tokenizer to replicate Penn Treebanktokenization on the English side.
On the Germanside, we developed new in-house normalizationand tokenization script.We filtered tokenized training sentences by sen-146tence length, token length, and sentence length ra-tio.
The final corpus for parsing and word align-ment contained 3,897,805 lines, or approximately86 percent of the total training resources releasedunder the WMT constrained track.
Word align-ment was carried out using FastAlign (Dyer etal., 2013), while for parsing we used the Berke-ley parser (Petrov et al., 2006).Given the parsed and aligned corpus, we ex-tracted synchronous context-free grammar rulesusing the method of Hanneman et al.
(2011).In addition to aligning subtrees that natively ex-ist in the input trees, our grammar extractor alsointroduces ?virtual nodes.?
These are new andpossibly overlapping constituents that subdivideregions of flat structure by combining two adja-cent sibling nodes into a single nonterminal forthe purposes of rule extraction.
Virtual nodesare similar in spirit to the ?A+B?
extended cate-gories of SAMT (Zollmann and Venugopal, 2006),and their nonterminal labels are constructed in thesame way, but with the added restriction that theydo not violate any existing syntactic structure inthe parse tree.7.2 ImprovementsNonterminals in our tree-to-tree grammar aremade up of pairs of symbols: one from the sourceside and one from the target side.
With virtualnodes included, this led to an initial German?English grammar containing 153,219 distinct non-terminals ?
a far larger set than is used in SAMT,tree-to-string, string-to-tree, or Hiero systems.
Tocombat the sparsity introduce by this large nonter-minal set, we coarsened the label set with an ag-glomerative label-clustering technique(Hannemanand Lavie, 2011; Hanneman and Lavie, 2013).The stopping point was somewhat arbitrarily cho-sen to be a grammar of 916 labels.Table 3 shows a significant improvement intranslation quality due to coarsening the label set:approximately +1.8 BLEU, +0.6 Meteor, and ?1.6TER on our dev test set, newtest2012.2In the MERT runs, however, we noticed that thelength of the MT output can be highly variable,ranging on the tuning set from a low of 92.8% ofthe reference length to a high of 99.1% in another.We were able to limit this instability by tuning toMeteor instead of BLEU.
Aside from a modest2We follow the advice of Clark et al.
(2011) and eval-uate our tree-to-tree experiments over multiple independentMERT runs.
All scores in Table 3 are averages of two orthree runs, depending on the row.score improvement, we note that the variability inlength ratio is reduced from 6.3% to 2.8%.Specific difficulties of the German?English lan-guage pair led to three additional system compo-nents to try to combat them.First, we introduced a second language modeltrained on Brown clusters instead of surface forms.Next we attempted to overcome the sparsityof German input by making use of cdec?s latticeinput functionality introduce compound-split ver-sions of dev and test sentences.Finally, we attempted to improve our grammar?scoverage of new German words by introducingsynthetic rules for otherwise out-of-vocabularyitems.
Each token in a test sentence that the gram-mar cannot translate generates a synthetic rule al-lowing the token to be translated as itself.
The left-hand-side label is decided heuristically: a (coars-ened) ?noun?
label if the German OOV starts witha capital letter, a ?number?
label if the OOV con-tains only digits and select punctuation characters,an ?adjective?
label if the OOV otherwise startswith a lowercase letter or a number, or a ?symbol?label for anything left over.The effect of all three of these improvementscombined is shown in the fourth row of Table 3.By default our previous experiments were per-formed with a span limit of 12 tokens.
Increasingthis limit to 15 has a mixed effect on metric scores,as shown in the fifth row of Table 3.
Since two outof three metrics report improvement, we left thelonger span limit in effect in our final system.Our final improvement was to augment our tun-ing set with the same set of pseudo-referencesas our Hiero systems.
We found that using onepseudo-reference versus four pseudo-referenceshad negligible effect on the (single-reference) tun-ing scores, but four produced a better improve-ment on the test set.The best MERT run of this final system (bottomline of Table 3) was submitted to the WMT 2014evaluation as a contrastive entry.AcknowledgmentsWe sincerely thank the organizers of the work-shop for their hard work, year after year, and thereviewers for their careful reading of the submit-ted draft of this paper.
This research work wassupported in part by the U. S. Army ResearchLaboratory and the U. S. Army Research Officeunder contract/grant number W911NF-10-1-0533,by the National Science Foundation under grant147IIS-0915327, by a NPRP grant (NPRP 09-1140-1-177) from the Qatar National Research Fund (amember of the Qatar Foundation), and by com-puting resources provided by the NSF-sponsoredXSEDE program under grant TG-CCR110017.The statements made herein are solely the respon-sibility of the authors.ReferencesWaleed Ammar, Chris Dyer, and Noah A. Smith.
2012.Transliteration by sequence labeling with lattice en-codings and reranking.
In NEWS workshop at ACL.Peter F Brown, Peter V Desouza, Robert L Mercer,Vincent J Della Pietra, and Jenifer C Lai.
1992.Class-based n-gram models of natural language.Computational linguistics, 18(4):467?479.Victor Chahuneau, Eva Schlinger, Noah A. Smith, andChris Dyer.
2013.
Translating into morphologicallyrich languages with synthetic phrases.
In Proceed-ings of EMNLP.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Jonathan H. Clark, Chris Dyer, Alon Lavie, andNoah A. Smith.
2011.
Better hypothesis testingfor statistical machine translation: Crontrolling foroptimizer instability.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Short Papers, pages 176?181, Portland,Oregon, USA, June.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic metric for reliable optimization andevaluation of machine translation systems.
In Pro-ceedings of the Sixth Workshop on Statistical Ma-chine Translation, pages 85?91, Edinburgh, Scot-land, UK, July.Michael Denkowski, Greg Hanneman, and Alon Lavie.2012.
The cmu-avenue french-english translationsystem.
In Proceedings of the NAACL 2012 Work-shop on Statistical Machine Translation.Nadir Durrani, Barry Haddow, Kenneth Heafield, andPhilipp Koehn.
2013.
Edinburgh?s machine transla-tion systems for european language pairs.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proc.
of ACL.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A simple, fast, and effective reparameteriza-tion of IBM Model 2.
In Proc.
of NAACL.Chris Dyer.
2009.
Using a maximum entropy modelto build segmentation lattices for mt.
In Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,pages 406?414.
Association for Computational Lin-guistics.Vladimir Eidelman.
2012.
Optimization strategies foronline large-margin learning in machine translation.In Proceedings of the Seventh Workshop on Statisti-cal Machine Translation.Greg Hanneman and Alon Lavie.
2011.
Automaticcategory label coarsening for syntax-based machinetranslation.
In Proceedings of SSST-5: Fifth Work-shop on Syntax, Semantics, and Structure in Statis-tical Translation, pages 98?106, Portland, Oregon,USA, June.Greg Hanneman and Alon Lavie.
2013.
Improvingsyntax-augmented machine translation by coarsen-ing the label set.
In Proceedings of NAACL-HLT2013, pages 288?297, Atlanta, Georgia, USA, June.Greg Hanneman, Michelle Burroughs, and Alon Lavie.2011.
A general-purpose rule extractor for SCFG-based machine translation.
In Proceedings of SSST-5: Fifth Workshop on Syntax, Semantics, and Struc-ture in Statistical Translation, pages 135?144, Port-land, Oregon, USA, June.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In Proceedings of the SixthWorkshop on Statistical Machine Translation, Edin-burgh, Scotland, UK, July.R.
Kneser and H. Ney.
1995.
Improved backing-offfor m-gram language modeling.
In Proceedings ofIEEE Internation Conference on Acoustics, Speech,and Signal Processing, pages 181?184.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: A method for automaticevalution of machine translation.
In Proceedings ofthe 40th Annual Meeting of the Association for Com-putational Linguistics, pages 311?318, Philadelphia,Pennsylvania, USA, July.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, andinterpretable tree annotation.
In Proceedings ofthe 21st International Conference on ComputationalLinguistics and the 44th annual meeting of the Asso-ciation for Computational Linguistics, pages 433?440.
Association for Computational Linguistics.Juan Pino, Aurelien Waite, Tong Xiao, Adri`a de Gis-pert, Federico Flego, and William Byrne.
2013.The university of cambridge russian-english systemat wmt13.148Avneesh Saluja, Hany Hassan, Kristina Toutanova, andChris Quirk.
2014.
Graph-based semi-supervisedlearning of translation models from monolingualdata.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics(ACL), Baltimore, Maryland, June.Andreas Stolcke.
2002.
Srilm-an extensible languagemodeling toolkit.
In INTERSPEECH.Yulia Tsvetkov, Chris Dyer, Lori Levin, and ArchnaBatia.
2013.
Generating English determiners inphrase-based translation with synthetic translationoptions.
In Proceedings of the Eighth Workshop onStatistical Machine Translation.Andreas Zollmann and Ashish Venugopal.
2006.
Syn-tax augmented machine translation via chart parsing.In Proceedings of the Workshop on Statistical Ma-chine Translation, pages 138?141, New York, NewYork, USA, June.149
