Has a Consensus NL Generat ion Architecture Appeared,  and is itPsychol inguist ical ly  Plausible?Ehud Reiter*CoGenTex, Inc840 Hanshaw RdIthaca, NY  14850 USAemaih ehud@cogentex.
COlAbst rac tI survey some recent applications-orientedNL generation systems, and claim that de-spite very different heoretical backgrounds,these systems have a remarkably similar ar-chitecture in terms of the modules they di-vide the generation process into, the compu-tations these modules perform, and the waythe modules interact with each other.
I alsocompare this 'consensus architecture' amongapplied NLG systems with psycholinguisticknowledge about how humans peak, and ar-gue that at least some aspects of the con-sensus architecture seem to be in agreementwith what is known about human languageproduction, despite the fact that psycholin-guistic plausibility was not in general a goalof the developers of the surveyed systems.1 IntroductionIn this paper I survey some recently-developed NL gen-eration systems that (a) cover the complete generationprocess and (b) are designed to be used by applicationprograms, as well as (or even instead of) making sometheoretical point.
I claim that despite their widely dif-fering theoretical backgrounds, the surveyed systemsare similar in terms of the modules they divide thegeneration process into, the way the modules interactwith each other, and (at least in some cases) the kindsof computations each individual module performs.
Inother words, despite different heoretical claims, thereis a remarkable level of similarity in how these sys-tems 'really work'; that is, a de facto 'consensus ar-chitecture' seems to be emerging for how applied NLGsystems hould generate text.
The existence of suchagreement among the surveyed systems is especially*Most of this work was done while the author was atthe University of Edinburgh, Department of Artiticia.l In-telligence.
The Edinburgh work was supported by SERCgrant GR/F/36750.surprising because in some cases the theoretical back-grounds of the systems examined argue against someaspects of the consensus architecture.I also compare the consensus architecture to psy-cholinguistic knowledge about language generation inhuman speakers.
Such a comparison is often diffi-cult to make, because of the many gaps in our cur-rent knowledge about how humans speak.
Neverthe-less, I argue that as far as such a comparison canbe made, the specific design decisions embodied inthe consensus architecture seem to often be more orless in accord with current knowledge of human lan-guage generation.
This is again perhaps somewhatsurprising, since psycholinguistic plausibility was notin general a goal of the developers ofthe examined sys-tems.
Perhaps (being very speculative) this indicatesthat there is some connection between the engineer-ing considerations that underlie the design decisionsmade in the consensus architecture, and the maximize-performance-in-the-real-world criteria that drove theevolutionary processes that created the human lan-guage processor.
If (a big if!)
there is some truthto this hypothesis, then studying the engineering is-sues involved in building applied systems may lead toinsights about the way the human language systemworks.2 The Systems SurveyedThe analysis presented here is based on a survey ofgeneration systems that:1.
Were written (or at least substantially extended)since the late 1980s.
This excludes early systemssuch as Davey's PROTEUS or Jacobs's KING.2.
Are complete systems that start from an inten-tion, a query, or some data that needs to be com-municated, and produce actual sentences as out-put.
This rules out systems that only implementpart of the generation process, such as Cawsey'sEDGE system (discourse planning) or my own FN(noun-phrase construction).1637th International Generation Workshop *Kennebunkport, Maine * June 21-24, 19943.
Were motivated, at least to some degree, by thedesire to interface to application programs.
Thisexcludes ystems that were primarily intended tobe computational explorations of a particular lin-guistic theory, such as Patten's SLANG, or  com-putational models of observed linguistic behavior,such as Hovy's PAULINE.4.
Are well enough known that I could easily obtaininformation about them.In short, the idea was to survey recent systems thatlooked at the entire generation problem, and that weremotivated by applications and engineering considera-tions as well as linguistic theory.
The systems exam-ined were: IFUF  \[Elhadad, 19921: Developed at Columbia Uni-versity and used in several projects there, includ-ing COMET and ADVISOR If; I will use the term'FUF' in this paper to refer to both FUF itself andthe various related systems at Columbia.
Severalother universities have also recently begun to useFUF in their research.
FUF is based on Kay's func-tional unification formalism \[Kay, 1979\].IDAS \[Reiter et a/., 1992\]: Developed at EdinburghUniversity, IDAS Was a prototype online docu-mentatidn system for users of complex machinery.From a theoretical perspective, IDAS's main objec-tive was to show that a single representation andreasoning system can be used for both domain andlinguistic knowledge \[Reiter and Mellish, 1992\].JOYCE \[Rarnbow and Korelsky, 1992\]: Developedat Odyssey Research Associates, JOYCZ is takenas a representative of several NL  generation sys-terns produced by ORA and CoGenTex, includ-ing GOSSIP, FOG, and LFS.
These systems areall aimed at commercial or government applica-tions (in JoYcz's case, producing summaries ofsoftware designs), and are all based on Mel'~uk'sMeaning-Text theory \[Mel'~uk, 1988\].PENMAN \[Penman NaturalLanguage Group, 1989\]: Under development atISI since the early 1980's, PENMAN has been usedin several demonstration systems.
As usual, I willuse ~PENMAN' to refer to both PENMAN itself andthe systems that were built around it.
PENMAN'Stheoretical basis is systemic linguistics \[Halliday,1985\] and rhetorical-structure theory.SPOKESMAN \[Meteer, 1989\]: SPOKESMAN was de-veloped at BBN for various applications, and hassome of the same design goals as McDonald'sx The selection rules are of course not completely well de-fined, which means there was inevitably some arbitrarinesswhen I used them to select particular systems to includein the survey.
I encourage any reader who believes that Ihave unfairly omitted a system to contact me, so that thissystem can be included in future versions of the survey.MUMBLE system \[McDonald, 1983\], including inparticular the desire to build a system that atleast in some respects i  psycholinguistically plau-sible.
SPOKESMAN uses Tree-Adjoining Gram-mars \[Joshi, 1987\] for syntactic processing.All of the examined systems produce English, and theyalso are mostly aimed at producing technical texts (in-stead of, say, novels or newspaper articles); it wouldbe interesting to examine systems aimed at other lan-guages or other types of applications, and see if thiscaused any architectural differences.3 An  Overv iew of the  ConsensusArch i tec tureAs can be seen, the chosen systems have widely dif-ferent theoretical bases.
It is therefore quite interest-ing that they all seem to have ended up with broadlysimilar architectures, in that they break up the gener-ation process into a similar set of modules, and theyall use a pipel ine archi~eetnre to connect he modules;i.e., the modules are linearly ordered, and informationflows from each module to its successor in the pipeline,with no feedback from later modules to earlier mod-ules.
The actual modules possessed by the systems(discussed in more detail in Section 4, as is the pipelinearchitecture) are:Content  Determinat ion:  This maps the initial in-put of the generation system (e.g., a query to beanswered, or an intention to be satisfied) onto asemantic form, possibly annotated with rhetorical(e.g., RST) relations.Sentence P len~ng:  Many names have been usedfor this process; here I use one suggested by Ram-bow and Korelsky \[1992\].
The basic goal is to mapconceptual structures onto linguistic ones: this in-cludes generating referring expressions, choosingcontent words and (abstract) grammatical rela-tiouships, and grouping information into clausesand sentences.Surface Generation: I use this term in a fairly nar-row sense here, to mean a module that takes as in-put an abstract specification of information to becommunicated by syntax and function words, andproduces as output a surface form that commu-nicates this information (e.g., maps :speechactimperat?ve into an English sentence that lacksa surface subject).
All of the examined sys-tems had separate sentence-planning and surface-generation modules, and the various intermedi-ate forms used to pass information between thesemodules conveyed similar kinds of information.Morphology:  Most of the systems have a fairly sim-ple morphological component, presumably sinceEnglish morphology is quite simple.1647th International Generation Workshop *Kennebunkport, Maine * June 21-24, 1994Formatt ing:  .IDAS, JOYCE, and PENMAN also containmechanisms for formatting (in the I~TEX sense)their output, and/or adding hypertext annota-tions to enable users to click on portions of thegenerated text,4 A More  Deta i led  Examinat ion  o fthe  Arch i tec tureThis section describes the consensus architecture inmore detail, with particular emphasis on some of thedesign decisions embodied in it that more theoreti-cally motivated researchers have disagreed with.
Itfurthermore xamines the plausibility of these deci-sions from a psyeholinguistic perspective, and arguesthat in many respects they agree with what is knownabout how humansgenerate text.4.1 Modular ized Pipel ine ArchitectureThe consensus architecture divides the generation pro-cess into multiple modules, with information flowing ina 'pipeline' fashion from one module to the next.
Bypipeline, I mean that the modules are arranged in a lin-ear order, and each module receives information onlyfrom its predecessor (and the various linguistic and do-main knowledge bases), and sends information only toits successor.
Information does not flow 'backwards'from a module to its predecessor, and global 'black-boards' that all modules can access and modify arenot used.
I do not mean by 'pipeline' that generationmust be incremental in the sense that, say, syntacticprocessing of the first sentence isdone at the same timeas semantic processing of the second; I believe most ofthe systems examined could in fact do this, but theyhave not bothered to do so (probably because it wouldnot be of much benefit o the applications programs ofinterest).4.1.1 Design decision: avoid integratedarch i tec t~eMany NL generation researchers have arguedagainst dividing the generation process into modules;perhaps the best-known are Appelt \[1985\] and Dan-los \[1984\].
Others, such as Rubinoff \[1992\], have ac-cepted modules but have argued that the architecturemust allow feedback between later modules and earliermodules, which argues against the one-way informa-tion flow of the pipeline architecture.The argument against pipelines and modules is al-most always some Variant of 'there are linguistic phe-nomena that can o~ly be properly handled by lookingat constraints from different levels (intentional, seman-tic, syntactic, morphological), and this is difficult o doin a pipeline system?
To take one fairly random exam-pie, Danlos and Namer \[1988\] have pointed out thatSince the French masculine and feminine pronouns leand la are abbreviated to l' before a word that startswith a vowel, and since in some cases le and la maybe unambiguous references while l '  is not, the refer-ring expression system must have some knowledge ofsurface word order and selected content and functionwords before it can decide whether a pronoun is ac-ceptable; this will not be possible if referring expres-sions are chosen before syntactic structures are built,as happens in the consensus architecture.There is undoubtably some truth to these argu-ments, but the applications builder also has to con-sider the engineering reality that the sorts of systemsproposed by Appelt, Danlos, and Namer are extremelydifficult to build from an engineering perspective.
Theengineering argument for modularization is particu-larly strong; Mart has put this very well in \[Mart, 1976,page 485\]:Any large computation should be split up andimplemented as a collection of small subpartsthat are as nearly independent ofone anotheras the overall task allows.
If a process is notdesigned in this way a small change in oneplace will have consequences in many otherplaces.
This means that the process as awhole becomes extremely difficult to debugor improve, whether by a human designer orin the course of natural evolution, because asmall chance to improve one part has to beaccompanied by many simultaneous compen-satory changes elsewhere.Mart argues that a modularized structure makes enseboth for human engineers and for the evolutionary pro-cess that produced the human brain.
The evidence isindeed strong that the human brain is highly modu-larized.
This evidence comes from many sources (e.g.,cognitive xperiments and PET scans of brain activ-ity), but I think perhaps the most convincing evidenceis from studies of humans with brain damage.
Suchpeople tend to lose specific abilities, not suffer overalldegradation that applies equally to all abilities.
El-lis and Young \[1988\] provide an excellent summary ofsuch work, and list patients that, for example?
can produce syntactically correct utterances butcan not organize utterances into coherent wholes,i.e., can perform surface generation but not con-tent determination.?
can generate word streams that tell a narrativebut are not organized into sentences, i.e., can per-form content determination but not surface gen-eration.?
can produce coherent texts organized in grammat-ical structures, but have a severely restricted vo-cabulary; i.e., have impaired lexical choice (thesepatients till have conceptual knowledge, they justhave problems lexicalizing it).The main engineering argument for arranging mod-ules into a pipeline instead of a more complex structure1657th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994is again simplicity and ease of debugging.
In a one-waypipeline of N modules there are only N-1 interfaces be-tween modules, while a pipeline with 'two-way' infor-mation flow has 2(N-l) interfaces, and a system thatfully connects each module with every.other modulewill have N(N-1) interfaces.
A system that has a two-way interface between every possible pair of moduleswill undoubtably be able to handle many linguisticphenomena in a more powerful, elegant, principled,ere, manner than a system that arranges modules in asimple one-way pipeline; such a system will also, how-ever, be much more difficult to build and (especially)debug.It is easy to argue that a one-way pipeline is worseat handling some linguistic phenomena than a richly-connected architecture, but this is not the end of thestory for the system-building engineer; he or she hasto balance the cost of the pipeline being inefficientand/or inelegant at handling some phenomena againstthe benefit of the pipeline being a much easier struc-ture to build and debug.
We have insufficient engi-neering data at present to make any well-substantiatedclaims about whether the one-way pipeline has the op-timal cost/benefit tradeoff or not (and in any case thiswill probably depend somewhat on the circumstancesof each application \[Reiter and Mellish, 1993\]), butthe circumstantial evidence on this question is striking;despite the fact that so many theoretical papers haveargued against pipelines and very few (if any) haveargued for pipelines, every one of the applications-oriented systems examined in this survey chose to usethe one-way pipeline architecture.In other words, an applications systems builder cannot look at particular linguistic phenomena in isola-tion; he or she must weigh the benefits of 'properly'handling these phenomena gainst the cost of imple-menting the proposed architecture.
In the French pro-noun case described by Danlos and Namer, for exam-ple, the applications builder might argue that in thegreat majority of cases no harm will in fact be done ifthe referring-expression generator simply ignores thepossibility that pronouns may be abbreviated to I', es-pecially given humans' ability to use context o disam-biguate references; and if a situation does arise whereit is absolutely essential that the human reader be ableto correctly disambiguate a reference, then perhapspronouns hould not be used in any case.
Given this,and the very high engineering cost of building an in-tegrated architecture of the sort proposed by Danlosand Namer, is implementing such an architecture trulythe most effective way of using scarce engineering re-sources?Psycholinguistic research on self-monitoring andself-repair (summarized in \[Levelt, 1989, pages 458-299\]) suggests that there is some feedback in the hu-man language generation system, so the human lan-guage processor is probably more complex than a sire-ple one-way pipeline; but it may not be much morecomplex.
To the best of my knowledge, most of theobserved self-repair phenomena could be explained byan architecture that added a few feedback loops fromlater stages of the pipeline back to the initial planner;this would only sUghtly add to the number of inter-module interfaces (perhaps N+i  instead of N-l, say),and hence would have a much lower engineering costthan implementing the fully connected 'every modulecommunicates with every other module' architecture.Whether the human language ngine is organized as a'pipeline plus a few feedback loops' or an 'every moduletalks to every other module' architecture is unknownat this point; hopefully new psycholinguistic experi-ments will shed more light on this issue.
I think itwould be very interesting, for example, to test humanFrench speakers on situations of the sort described byDanlos and Namer, and see what they actually did insuch contexts; I do not believe that such an experimenthas (to date) been performed.4.2 Content  Determinat ionContent determination takes the initial input to thegeneration system, which may be, for example, a queryto be answered or an intention to be satisfied, and pro-duces from it a 'semantic form', 'conceptual represen-tation', or 'list of propositions', i.e., a specification ofthe meaning content of the output text.
I will in thispaper use the term semantic representation for thismeaning specification.
Roughly speaking, the seman-tic representations u ed by all of the examined sys-teins can be characterized as some kind of 'semanticnet' (using the term in its broadest sense, as in \[Sowa,1991\]) where the primitive lements in the net are con-ceptual instead of linguistic (e.g., domain KB conceptsinstead of English words).
In some cases the seman-tic nets also include discourse and rhetorical relationsbetween portions of the net; subsequent portions ofthe generator use these to generate discourse connec-tives (e.g., However), control formatting (e.g., the useof bulletized lists), etc.The systems examined use quite different content-determination mechanisms (i.e., there was no consen-sus); schemas \[McKeown, 1985\] were the most popularapproach.4.2.1 Design decision: integrated contentdeterminat ion and rhetorical pl~nn;ngContent determination i the systems examined ba-sically performs two functions:Deep content determinat ion:  Determine what in-formation should be communicated to the hearer.Rhetor ical  pl~nnlng: Organize this information ina rhetorically coherent manner.Hovy \[1988\] has proposed an architecture wherethese tasks are performed separately (in particular, the1667th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994application program performs deep content determina-tion, while the generation system performs rhetoricalplanning).
Among the systems exan~ned, however,ttovy is unique in taking this approach; the builders ofthe other systems (including Moore and Paris \[1989\],who also worked with PENMAN) apparently believethat these two processes are so closely related thatthey should be performed simultaneously.I am not aware of any psychological data that di-rectly address this issue.
However, Hovy's architec-ture requires the language-producing a ent to com-pletely determine the content of a paragraph beforehe/she/it can begin to utter it (since the rhetoricalplanner determines what the first sentence is, and itis not called until deep content determination is com-pleted), and intuitively it seems implausible to me thathuman speakers do this; it also goes against incremen-tal theories of human speech production \[Levelt, 1989,pages 24-27\].4.3 Sentence pl~nn;ngThe sentence planner converts the semantic represen-tation, which is specified in terms of domain entities,into an abstract linguistic representation that speci-fies content words and grammatical relationships.
Iwill use Mel'-~uk's term deep syntactic form for thisrepresentation.All of the systems analyzed possess a deep syntac-tic representation; none attempt o go from semanticsto surface form in a single step.
IDAS and PENMANuse variants of the same deep syntactic language, SPL\[Kasper, 1989\].
FUF and .JOYCE use deep syntacticlanguages that are based (respectively) on functionalunification and meaning-text theory, but these con-vey much the same information as SPL.
SPOKESMANuses the realization specification language of MUMBLE\[McDonald, 1983\] as its deep syntactic representation;I have found it difficult to compare this language tothe others, but McDonald (personal communication)agrees that it conveys essentially the same informationas SPL.Unfortunately, while all of the systems possessed amodule which converted semantic representations i todeep syntactic ones, each system used a different namefor this module.
In FUF it is the 'lexical chooser', inIDAS it is the 'text planner', in JOYCE it is the 'sentenceplanner', in SPOKESMAN it is the 'text structurer', andin PENMAN it doesn't seem to have a name at all,e.g., Hovy \[1988\] simply refers to 'pre-generation text-planning tasks'.
I use the JOYCE term here because Ithink it is the least ambiguous.The specific tasksperformed bythe sentence plannerinclude:1.
Mapping domain concepts and relations into con-tent words and grammatical relations.2.
Generating referring expressions for individual do-main entities.3.
Grouping propositions into clauses and sentences.Relatively little is said in the papers about clausegrouping and referring-expression generation, butmore information is available on the first task, map-ping domain entities onto linguistic entities.
All theexamined systems except perhaps PENMAN use a vari-ant of what I have elsewhere called the 'structure-mapping' approach JR.citer, 1991\]; z I do not knowwhat approach PENMAN uses (the papers are not clearon this).
Structure-mapping is based on a dictio-nary that lists the semantic-net quivalents of linguis-tic resources \[Meteor, 1991\] such as content words andgrammatical relationships.
This dictionary might, forexample, indicate that the English word sisteris equiv-alent (in the domain knowledge-base of interest) to thestructure Sibling with attribute Sex:Female, and thatthe domain relation Part-of can be expressed with thegrammatical possessive, .g., the car's engine.
Giventhis dictionary, the structure-mapping algorithm iter-atively replaces emantic structures by linguistic ones,until the entire semantic net has been recoded into alinguistic structure.
There may be several ways of re-coding a semantic representation into a linguistic one,which means tructure-mapping systems have a choicebetween using the first acceptable r duction they find,or doing a search for a reduction that maximizes omeoptimality criterion (e.g., fewest number of words).The papers I read were not very clear on this issue,but I believe that while most of the systems urveyeduse the first acceptable reduction found, FUF in somecases earches for an optimal reduction.4.3.1 Design decision: separat ion of lexicalchoice from surface realizationThe consensus architecture clearly separates lexicalchoice of content words (done during sentence plan-ning) from syntactic processing (performed uring sur-face generation).
In other words, it does not use anintegrated 'lexicogrammar', which systemic theoristsin particular (e.g., \[Matthiessen, 1991\]) have arguedfor, and which is implicit in some unification-based ap-proaches, such as the semantic head-driven algorithm\[Shieber etal., 1990\].Despite these theoretical arguments, none of thesystems examined used an integrated lexicogrammar,including unification-based FUF and systemic-basedPENMAN.
3 In contrast, earlier unification-based sys-2Even though I have previously argued againststructure-mapping because it does not do a good job ofhandling lexical preferences ill.citer, 1991\], I neverthelessended up using this technique when I moved from my Ph.Dresearch to the more applications-oriented IDAS project.Perhaps this is another example of engineering consider-ations overriding theoretical arguments.SThe P~NMAN papers do not explicitly say where lexicalchoice is performed.
However, all examples of PENMANSPL input that I have seen have essentially had content1677th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994terns, such as the tactical component of McKeown'sTEXT system \[McKeown, 1985\], did integrate lexicaland syntactic processing in a single 'tactical genera-tor'; also, systemic systems that have been less drivenby application eeds than PENMAN, such as GENESYS\[Fawcett and Tucker, 1990\], have used integrated lexi-cogrammars.There is psychological evidence that at least somelexicai processing is separated from syntactic process-ing, e.g., the patient mentioned in Section 4.1.1 whowas able to perform content-determination andsyn-tactic generation but had a very restricted speakingvocabulary.
I think it's also very suggestive that hu-mans have different learning patterns for content andfunction words; the former are 'open-class' and eas-ily learned, while the latter are 'closed-class' and peo-ple tend to stick to the ones they learned as children.There is less evidence on the location of lexical choicein the psycholinguistic pipeline, and on whether it isperformed in one stage or distributed among severalstages.4.4 Surface Generat ionSurface generation has been used to mean many dif-ferent things in the literature.
I use it here torefer to the "portion of the generation system thatknows how grammatical relationships are actually ex-pressed in English (or whatever the target languageis).
For example, it is the surface generator thatknows what function words and word order relation-ships are used in English for imperative, interroga-tive, and negated sentences; it is the surface gener-ator that knows which auxiliaries are required for thevarious English tenses; and it is the surface generatorthat knows when pronominalization is syntactically re-quired (John scolded himself, not John scolded John).4.4.1 Design decision: top-down algorithmwith (Almost?)
no backtrackingThe grammars and grammar epresentations u edby the systems examined are quite different, but allsystems process the grammars with a top-down algo-rithm that uses minimal, if any, backtracking.
Noneof the systems use the semantic head-driven genera-tion algorithm \[Shieber et al, 1990\], although this isprobably the single best-known algorithm for surfacegeneration; Elhadad \[1992, chapter 4\] claims that suchan algorithm is only necessary for systems that at--tempt to simultaneously perform both lexical choiceand surface generation, which none of the examinedsystems do.
Perhaps more interestingly, four of thefive systems do not allow backtracking, and the fifth,FUF, allows backtracking but does not seem to use itmuch (if at all) during surface generation (backtrack-ing is used in FUF during sentence planning).
This iswords already specified, which suggests that lexical choiceis performed before syntactic processing in PENMAN.interesting, since backtracking is usually regarded asan essential component ofunification-based generationapproaches; it is certainly used in the semantic-head-driven algorithm, and in the TEXT generator \[McKe-own, 1985\].From a psycholinguistic perspective, many peoplehave argued that human language production is in-erementai (see the summary in \[Levelt, 1989, pages24-27\]), which means that of necessity it cannot in-elude much backtracking.
The garden-path phenom-ena shows that there are limits to how much syntacticbacktracking people people perform during languageunderstanding.
This evidence is of course suggestiverather than definitive; it seems likely that there arelimitations on how much (if any) backtracking humanswill perform during syntactic processing (see also thearguments in \[McDonald, 1983\]), but there is no hardproof of this (as far as I am aware).4.5 Morphology and Formatt ingThese modules will not be further examined here,mainly because little information is given in the pa-pers on the details of how morphology and formattingare implemented.5 A Cont rovers ia l  (?)
V lewI would like to conclude with a perhaps controver-sial personal opinion.
There have been many caseswhere NL generation researchers (including myself)have claimed that a certain linguistic phenomena isbest handled by a certain architecture.
Even if this istrue, however, if it turns out that adopting this archi-tecture will substantially complicate the design of theoverall generation system, and that the most commoncases of the phenomena of interest can be adequatelyhandled by adding a few heuristics to the appropriatestage of a simpler architecture, then the engineering-oriented NL worker must ask him- or herself if thebenefits of the proposed architecture truly outweighits costs.
For instance, one cannot simply argue thatan integrated architecture is superior to a pipeline be-cause it is better suited to handling certain kinds ofpronominalization; it is also necessary to evaluate theengineering cost of shifting to an integrated architec-ture, and determine if, for example, better overall per-formance for the amount of engineering resources avail-able could be obtained by keeping the general pipelinearchitecture, and instead investing some of the engi-neering resources ' aved' by this decision into buildingmore sophisticated heuristics into the pronominaliza-tion module.In doing so, I believe (and again this is a per-sonal belief that probably cannot be substantiated bythe existing evidence) that the NL engineer is com-ing close to the 'reasoning' of the evolutionary processthat created the human language system.
Evolutiondoes not care about elegant declarative formalisms or1687th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994'proper' (as opposed to 'hacky') handling of specialcases; evolution's goal is to maximize performance inreal-world situations, while maintaining an architec-ture that can be easily tinkered with by future evolu-tionary processes.
In short, evolution is an engineer,not a mathematician.
4 It is thus perhaps not surpris-ing if NL generation systems designed to be used inreal-world applications end up with an architecturethat seem to bear some resemblance to the architec-ture of the human language processor; 5 and future at-tempts to build applications-oriented generation sys-tems may end up giving us real insights into how lan-guage processing Works in humans, even if this is notthe main purpose of these systems.
Similarly, psy-cholinguistic knowledge of how the human languagegenerator works may suggest useful algorithms for NLengineers; one such ease is described in \[Reiter andDale, 1992\].Cross-fertilization between psycholinguistics and NLengineering will only arise, however, if the results of en-gineering analyses are reported in the research litera-ture, especially when they suggest going against sometheoretical principle.
Unfortunately, to date the re-sults of such analyses have all-too-often been regardedmore as embarrassments (since they contradict he-ory) than as valuable observations, and hence havenot been published.
I would like to conclude this pa-per by encouraging eneration researchers to regardthe results of engineering analyses to be as interestingand as important to the understanding of language asconventional linguistic analyses.
After all, as Woods\[1975\] has pointed.out, while descriptive analyses oflanguage can at best tell us what the brain does, engi-neering analyses can potentially offer insights on whythe brain functions as it does.AcknowledgementsI would like to thank Jean Carletta, Robert Dale,Michael Elhadad, David McDonald, Richard Kit-tredge, Tanya Korelsky, Chris Mellish, Owen Rainbow,and Graeme Ritchie for their very helpful comments onearlier versions of this work.
It goes without saying,of course, that the views represented are my own, andthat any factual errors are entirely my fault.
This re-4 Gould's various popular books on evolutionary biology,such as \[Gould, 1983\], give an excellent feel for evolution asan engineer-cum-hackers; see also the interesting discussionof language and evolution in \[Pinker, 1994\].5Of course, the best way to do something on a machineis often not the best way to do it in nature; e.g., birds andairplanes use different mechanisms to fly.
On the otherhand, there does seem to be a remarkable congruence be-tween effective vision processing strategies in animals andcomputers \[Marr, 1982J.
One could also argue that sincelanguage (unlike flying) is purely a product of the humanmind, any effective language processor isprobably going tohave to share some of the mind's processing strategies.search was mostly done while the author was at theUniversity of Edinburgh, where he was supported bySERC grant GR/F/36750.Re ferences\[Appelt, 1985\] Doug Appelt.
Planning English Refer-ring Ezpressions.
Cambridge University Press, NewYork, 1985.\[Danlos, 1984\] Laurence Danlos.
Conceptual and lin-gnistie decisions in generation.
In Proceedings of the22nd Meeting of the Association for ComputationalLinguistics and the lOth International Conferenceon Computational Linguistics (A CL/COLING-84),pages 501-504, 1984.\[Danlos and Namer, 1988\] Laurence Danlos and Fi-arametta Namer.
Morphology and cross dependen-cies in the synthesis of personal pronouns in Ro-mance languages.
In Proceedings of the 12th Inter-national Conference on Computational Linguistics(COLING-88), volume 1, pages 139-141, 1988.\[Elhadad, 1992\] Michael Elhadad.
Using Argumenta-tion to Control Lemcal Choice: A Functional Unifi-cation Implementation.
PhD thesis, Columbia Uni-versity, 1992.\[Ellis and Young, 1988\]Andrew Ellis and Andrew Young.
Human Cogni-tive Neuropsychology.
Lawrence Erlbaum, 1988.\[Fawcett and Tucker, 1990\] Robin Fawcett and Gor-don Tucker.
Demonstration of GENESYS: A verylarge, semantically based systemic functional gram-mar.
In Proceedings of the I3th International Con-ference on Computational Linguistics (COLING-90), volume 1, pages 47--49, 1990.\[Gould, 1983\] Stephan Gould.
Hen's Teeth andHorse's Toes.
Norton, 1983.\[Halliday, 1985\] M. A. K. Halliday.
An Introductionto Functional Grammar.
Edward Arnold, London,1985.\[Hovy, 1988\] Eduard Hovy.
Planning coherent multi-sentential text.
In Proceedings of 25th Annual Meet-ing o f the Association for Computational Linguistics(ACL-88), pages 163---169, 1988.\[Joshi, 1987\] Aravind Joshi.
The relevance of tree ad-joining grammar to generation.
In G. Kempen, ed-itor, Natural Language Generation: New Directionsin Artificial Intelligence, Psychology, and Linguis-tics.
Kluwer, 1987.\[Kasper, 1989\] Robert Kasper.
A flexible interface forlinking applications to Penman's entence genera-tor.
In Proceedings of the 1989 DARPA Speechand Natural Language Workshop, pages 153-158,Philadelphia, 1989.1697th International Generation Workshop ?
Kennebunkport, Maine ?
June 21-24, 1994\[Kay, 1979\] Martin Kay.
Functional grammar.
In Pro-ceedings of the Fifth Meeting of the Berkeley Lin-guistics Society, pages 142-158, Berkeley, CA, 1%19Febuary 1979.\[Levelt, 1989\] Willem Levelt.
Speaking: From Inten-tion to Articulation.
MIT Press, 1989.\[Mart, 1976\] David Marr.
Early processing of vi-sual information.
Philosophical Transactions of theRoyal Society, B275:483-524, 1976.\[Marr, 1982\] David Marr.
Vision.
W. H. Freeman,1982.\[Matthiessen, 1991\] Christian Matthiessen.
Lex-leo(grammatical) choice in text generation.
InC. Paris, W. Swartout, and W. Mann, editors,Natural Language Generation in Artificial Intelli-gence and Computational Linguistics, pages 249-292.
Kluwer, 1991.\[McDonald, 1983\] David McDonald.
Description di-rected control.
Computers and Mathematics, 9:111-130, 1983.\[McKeown, 1985\] Kathleen McKeown.
Ten Genera-tion.
Cam_bridge University Press, 1985.\[Mel'6uk, 1988\] Igor Mel'~uk.
Dependency Syntaz:Theory and Practice.
State University of New YorkPress, Albany, N'Y', 1988.\[Mercer, 1989\] Marie Meteer.
The SPOKESMAN nat-ural language generation system.
Report 7090, BBNSystems and Technologies, Cambridge, Mass, 1989.\[Meteer, 1991\] Marie Meteer.
Bridging the generationgap between text-planning and linguistic realisation.Computational Intelligence, 7(4) :296-304, 1991.\[Moore and Paris, 1989\] Johanna Moore and CecileParis.
Planning text for advisory dialogues.
In Pro-ceedings of the 27th Annual Meeting of the Asso-ciation for Computational Linguistics (A CL-1989),pages 203-211, 1989.\[Penman Natural Language Group, 1989\]Penman Natural Language Group.
The Penmanuser guide.
Technical report, Information SciencesInstitute, Marina del Bey, CA 90292, 1989.\[Pinker, 1994\] Steven Pinker.
The Language Instinct.William Morrow, 1994.\[Rarnbow and Korelsky, 1992\] Owen Rainbow andTanya Korelsky.
Applied text generation.
In Pro-ceedings of ~he Third Conference on Applied Nat-nral Language Processing (ANLP-1092), pages 40-47; 1992.\[Reiter, 1991\] Ehud Reiter.
A new model of lexi-cal choice for nouns.
Computational Intelligence,7(4):240-251, 991.\[Reiter and Dale, 1992\] Ehud Reiter and Robert Dale.A fast algorithm for the generation of referring ex-pressions.
In Proceedings of the Fourteenth Inter-national Conference on Computational Linguistics(COLING-199~), volume 1, pages 232-238, 1992.\[Reiter and Mellish, 1992\] Ehud Reiter and ChrisMellish.
Using classification to generate text.
InProceedings of the 30th Annual Meeting of the Asso-ciation for Computational Linguistics (A CL-1992),pages 265-272, 1992.\[Reiter and Mellish, 1993\] Ehud Reiter and ChrisMellish.
Optimising the costs and benefits of natu-ral language generation.
In Proceedings of the I3thInternational Joint Conference on Artificial Intel-ligence (IarCAI-lg95), volume 2, pages 1164-1169,1993.\[Reiter et al, 1992\] Ehud Reiter, Chris Mellish, andJohn Levine.
Automatic generation ofon-fine docu-mentation i the IDAS project.
In Proceedings of theThird Conference on Applied Natural Language Pro-cessing (ANLP-199~), pages 64-71, Trento, Italy,1992.\[Rubinoff, 1992\] Robert Rubinoff.
Integrating textplanning and linguistic choice by annotating lin-guistic structures.
In R. Dale et at., editors, As-pects of Automated Natural Language Generation:Proceedings of the Siz'th International Natural Lan-guage Generation Workshop, pages 45-56.
Springer-Verlag, 1992.\[Shieber et al, 1990\] Stuart Shieber, Gertjan vanNoord, Fernando Pereira, and Robert Moore.Semantic-head-driven g eration.
ComputationalLinguistics, 16:30--42, 1990.\[Sowa, 1991\] John Sowa, editor.
Principles of Seman-tic Networks.
Morgan Kaufmann, 1991.\[Woods, 1975\] Wilfiam Woods.
Some methodologicalissues in natural anguage understanding research.In Proceedings of the Workshop on Theoretical Is-sues in Natural Language Processing (TINLAP-1975), pages 148-153, 1975.170
