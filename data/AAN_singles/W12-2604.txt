Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 28?35,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsEcological Validity and the Evaluation of Speech Summarization QualityAnthony McCallum Cosmin MunteanuUniversity of Toronto National Research Council Canada40 St. George Street 46 Dineen DriveToronto, ON, Canada Fredericton, NB, Canadamccallum@cs.toronto.edu cosmin.munteanu@nrc-cnrc.gc.caGerald Penn Xiaodan ZhuUniversity of Toronto National Research Council Canada40 St. George Street 1200 Montreal RoadToronto, ON, Canada Ottawa, ON, Canadagpenn@cs.toronto.edu xiaodan.zhu@nrc-cnrc.gc.caAbstractThere is little evidence of widespread adoption ofspeech summarization systems.
This may be due inpart to the fact that the natural language heuristicsused  to  generate  summaries  are  often  optimizedwith respect to a class of evaluation measures that,while  computationally  and  experimentally  inex-pensive,  rely on subjectively selected gold stand-ards  against  which  automatically  generated  sum-maries  are  scored.
This  evaluation  protocol  doesnot take into account the usefulness of a summaryin assisting the listener in achieving his or her goal.In this paper we study how current measuresand methods for evaluating summarization systemscompare to human-centric evaluation criteria.
Forthis, we have designed and conducted an ecologic-ally valid evaluation that determines the value of asummary  when  embedded  in  a  task,  rather  thanhow closely a summary resembles a gold standard.The results of our evaluation demonstrate  that  inthe  domain  of  lecture  summarization,  the  well-known  baseline  of  maximal  marginal  relevance(Carbonell and Goldstein, 1998) is statistically sig-nificantly  worse  than  human-generated  extractivesummaries,  and even worse than having no sum-mary at  all in  a simple quiz-taking task.
Primingseems to have no statistically significant effect onthe usefulness  of the human summaries.
In addi-tion, ROUGE scores and, in particular, the context-free annotations that are often supplied to ROUGEas references, may not always be reliable as inex-pensive proxies for ecologically valid evaluations.In fact, under some conditions, relying exclusivelyon ROUGE may even lead to scoring human-gen-erated summaries that are inconsistent in their use-fulness relative to using no summaries very favour-ably.1 Background and MotivationSummarization  maintains  a  representation  of  anentire spoken document, focusing on those utter-ances (sentence-like units) that are most importantand therefore does not require the user to processeverything that has been said.
Our work focuses onextractive summarization where a selection of ut-terances is chosen from the original spoken docu-ment in order to make up a summary.Current  speech  summarization  research  hasmade extensive use  of intrinsic  evaluation meas-ures  such  as  F-measure,  Relative  Utility,  andROUGE  (Lin,  2004),  which  score  summariesagainst  subjectively  selected  gold  standard  sum-maries obtained using human annotators.
These an-notators are asked to arbitrarily select (in or out) orrank utterances, and in doing so commit to relativesalience judgements with no attention to goal ori-entation  and  no  requirement  to  synthesize  themeanings of larger units of structure into a coher-ent message.28Given this  subjectivity,  current  intrinsic evalu-ation measures are unable to properly judge whichsummaries  are  useful  for real-world applications.For  example,  intrinsic  evaluations  have  failed  toshow that summaries created by algorithms basedon complex linguistic and acoustic features are bet-ter  than  baseline  summaries  created  by  simplychoosing the positionally first utterances or longestutterances in a spoken document (Penn and Zhu,2008).
What  is  needed  is  an  ecologically  validevaluation  that  determines  how valuable  a  sum-mary is when embedded in a task, rather than howclosely  a  summary  matches  the  subjective  utter-ance level scores assigned by annotators.Ecological validity is "the ability of experimentsto tell us how real people operate in the real world"(Cohen, 1995).
This is often obtained by using hu-man judges, but it is important to realize that themere use of human subjects provides no guaranteeas  to the ecological  validity  of their  judgements.When utterances are merely ranked with numericalscores  out  of  context,  for  example,  the  humanjudges who perform this task are not performing atask that they generally perform in their daily lives,nor does the task correspond to how they wouldcreate or use a good summary if they did have aneed for one.
In fact, there may not even be a guar-antee that they  understand the task --- the notionsof ?importance,?
?salience?
and the like, when de-fining the criterion by which utterances are selec-ted, are not easy to circumscribe.
Judgements ob-tained in this fashion are no better than those of thegenerative linguists who leaned back in their arm-chairs in the 1980s to introspect on the grammatic-ality  of  natural  language  sentences.
The  field  ofcomputational linguistics could only advance whencorpora became electronically available to invest-igate language that was written in an ecologicallyvalid context.Ours is not the first ecologically valid experimentto be run in the context of speech summarization,however.
He et al (1999; 2000) conducted a verythorough  and  illuminating  study  of  speech  sum-marization in the lecture domain that  showed (1)speech summaries are indeed very useful to havearound, if they are done properly, and (2) abstract-ive summaries do not seem to add any statisticallysignificant advantage to the quality of a summaryover  what  topline  extractive  summaries  canprovide.
This is very good news; extractive sum-maries are worth creating.
Our study extends thiswork by attempting to evaluate the relative qualityof  extractive  summaries.
We  conjecture  that  itwould be very difficult for this field to progress un-less we have a means of accurately measuring ex-tractive summarization quality.
Even if the meas-ure comes at great expense, it is important to do.Another noteworthy paper is that of Liu and Liu(2010), who, in addition to collecting human sum-maries of six meetings, conducted a subjective as-sessment of the quality of those summaries  withnumerically  scored  questionnaires.
These  areknown as Likert scales, and they form an importantcomponent of any human-subject study in the fieldof human-computer interaction.
Liu and Liu (2010)cast  considerable doubt on the value of ROUGErelative to these questionnaires.
We will focus hereon an objective, task-based measure that typicallycomplements those subjective assessments.2 Spontaneous SpeechSpontaneous speech is often not linguistically well-formed,  and  contains  disfluencies,  such  as  falsestarts,  filled pauses,  and repetitions.
Additionally,spontaneous speech is more vulnerable to automat-ic speech recognition (ASR) errors, resulting in ahigher  word  error  rate  (WER).
As  such,  speechsummarization has the most potential for domainsconsisting  of  spontaneous  speech  (e.g.
lectures,meeting recordings).
Unfortunately, these domainsare not easy to evaluate compared to highly struc-tured  domains  such  as  broadcast  news.
Further-more,  in  broadcast  news,  nearly  perfect  studioacoustic  conditions  and  professionally  trainedreaders  results  in  low  ASR WER,  making  it  aneasy domain to summarize.
The result is that mostresearch has been conducted in this domain.
How-ever,  a positional  baseline performs very well  insummarizing broadcast news (Christensen, 2004),meaning that simply taking the first  N utterancesprovides a very challenging baseline, questioningthe value of summarizing this domain.
In addition,the widespread availability  of written  sources  onthe same topics means that there is not a strong usecase for speech summarization over simply sum-marizing the equivalent  textual  articles on whichthe news  broadcasts  were  based.
This  makes  iteven more difficult to preserve ecological validity.University lectures present a much more relev-ant domain, with less than ideal acoustic conditionsbut  structured  presentations  in  which  deviation29from written sources (e.g., textbooks) is common-place.
Here,  a  positional  baseline  performs  verypoorly.
The lecture domain also lends itself well toa  task-based  evaluation  measure;  namely  univer-sity level quizzes or exams.
This constitutes a real-world problem in a domain that is also representat-ive of other spontaneous speech domains that canbenefit from speech summarization.3 Ecologically Valid EvaluationAs pointed out by Penn and Zhu (2008), currentspeech summarizers have been optimized to per-form an utterance selection task that may not ne-cessarily reflect how a summarizer is able to cap-ture the goal orientation or purpose of the speechdata.
In our study, we follow methodologies estab-lished in the field of Human-Computer Interaction(HCI) for evaluating an algorithm or system ?
thatis, determining the benefits a system brings to itsusers, namely usefulness, usability, or utility, in al-lowing a user to reach a specific goal.
Increasingly,such user-centric evaluations are carried out withinvarious  natural  language  processing  applications(Munteanu et  al.,  2006).
The  prevailing  trend inHCI  is  for  conducting  extrinsic  summary  evalu-ations (He et al, 2000; Murray et al, 2008; Tuckeret al, 2010), where the value of a summary is de-termined by how well the summary can be used toperform a specific task rather than comparing thecontent of a summary to an artificially created goldstandard.
We have conducted an ecologically validevaluation of speech summarization that has evalu-ated summaries  under real-world conditions, in atask-based manner.The university lecture domain is an example of adomain where summaries are an especially suitabletool  for  navigation.
Simply  performing  a  searchwill not result in the type of understanding requiredof students in their lectures.
Lectures have topics,and there is a clear communicative goal.
For thesereasons, we have chosen this domain for our evalu-ation.
By using actual university lectures as well asuniversity students representative of the users whowould make use of a speech summarization systemin this domain, all results obtained are ecologicallyvalid.3.1Experimental OverviewWe conducted a within-subject experiment whereparticipants  were  provided  with  first  year  soci-ology university lectures on a lecture browser sys-tem installed on a desktop computer.
For each lec-ture, the browser made accessible the audio, manu-al transcripts, and an optional summary.
Evaluationof a summary was based on how well the user ofthe summary was able to complete a quiz based onthe content of the original lecture material.It is important to note that not all extrinsic eval-uation is ecologically valid.
To ensure ecologicalvalidity in this study, great care was taken to en-sure that human subjects were placed under condi-tions that result in behavior that would be expectedin actual real-world tasks.3.2EvaluationEach quiz consisted of 12 questions, and were de-signed to be representative of what students wereexpected to learn in the class, incorporating factualquestions  only,  to  ensure  that  variation  in  parti-cipant  intelligence had a minimal impact  on res-ults.
In  addition,  questions  involved  informationthat was distributed equally throughout the lecture,but at the same time not linearly in the transcript oraudio  slider,  which  would  have  allowed  parti-cipants to predict where the next answer might belocated.
Finally, questions were designed to avoidcontent that was thought to be common knowledgein  order  to  minimize  the  chance  of  participantshaving previous knowledge of the answers.All  questions  were  short  answer  or  fill-in-the-blank.
Each quiz consisted of an equal number offour distinct  types  of questions,  designed so thatperforming a simple search would not be effective,though  no  search  functionality  was  provided.Question types do not appear in any particular or-der on the quiz and were not grouped together.Type  1: These  questions  can  be  answeredsimply  by  looking  at  the  slides.
As  such,  thesequestions  could  be  answered  correctly  with  orwithout a summary as slides were available in allconditions.Type 2:  Slides provide an indication of wherethe content required to answer these questions arelocated.
Access to the corresponding utterances isstill required to find the answer to the questions.30Type 3: Answers to these questions can only befound  in  the  transcript  and  audio.
The  slidesprovide no hint as to where the relevant content islocated.Type 4: These questions are more complicatedand require a certain level of topic comprehension.These questions often require connecting conceptsfrom various portions of the lecture.
These ques-tions are more difficult and were included to min-imize  the chance  that  participants  would alreadyknow the answer to questions without watching thelecture.A teaching assistant for the sociology class fromwhich  our  lectures  were  obtained  generated  thequizzes  used in the evaluation.
This teaching as-sistant had significant experience in the course, butwas not involved in the design of this study and didnot have any knowledge relating to our hypothesesor  the  topic  of  extractive  summarization.
Thesequizzes provided an ecologically valid quantitativemeasure of whether a given summary was useful.Having this evaluation metric in place, automatedsummaries  were  compared  to  manual  summariescreated by each participant in a previous session.3.3ParticipantsSubjects  were  recruited  from  a  large  universitycampus,  and  were  limited  to  undergraduate  stu-dents  who  had  at  least  two  terms  of  universitystudies,  to  ensure  familiarity  with  the  format  ofuniversity-level lectures and quizzes.
Students whohad taken the first year sociology course we drewlectures  from  were  not  permitted  to  participate.The study was conducted with 48 participants overthe  course  of  approximately  one  academicsemester.3.4MethodEach evaluation session began by having a parti-cipant perform a short warm-up with a portion oflecture content, allowing the participant to becomefamiliar with the lecture browser interface.
Follow-ing  this,  the  participant  completed  four  quizzes,one  for  each  of  four  lecture-condition  combina-tions.
There were a total of four lectures and fourconditions.
Twelve  minutes  were  given  for  eachquiz.
During this time, the participant was able tobrowse the audio, slides, and summary.
Each lec-ture was about forty minutes in length, establishinga time constraint.
Lectures and conditions were ro-tated using a Latin square for counter balancing.All participants completed each of the four condi-tions.One week prior to his or her evaluation session,each participant was brought in and asked to listento  and  summarize  the  lectures  beforehand.
Thisresulted  in  the  evaluation  simulating  a  scenariowhere  someone  has  heard  a  lecture  at  least  oneweek in the past and may or may not remember thecontent during an exam or quiz.
This is similar toconditions most university students experience.3.5ConditionsThe lecture audio recordings were manually tran-scribed and segmented into utterances, determinedby 200 millisecond pauses,  resulting in segmentsthat  correspond  to  natural  sentences  or  phrases.The task of summarization consisted of choosing aset of utterances for inclusion in a summary (ex-tractive summarization), where the total summarylength was bounded by 17-23% of the words in thelecture;  a percentage typical  to most  summariza-tion scoring tasks.
All participants were asked tomake use of the browser interface for four lectures,one for each of the following conditions:  no sum-mary,  generic  manual summary,  primed manualsummary, and automatic summary.No  summary: This condition  served  as  abaseline where no summary was provided, but par-ticipants  had  access  to  the  audio  and  transcript.While  all  lecture  material  was  provided,  thetwelve-minute time constraint made it impossibleto listen to the lecture in its entirety.Generic  manual  summary: I  this  condition,each  participant  was  provided  with  a  manuallygenerated summary.
Each summary was created bythe participant him or herself in a previous session.Only  audio  and text  from the  in-summary  utter-ances  were  available  for  use.
This  conditiondemonstrates how a manually created summary isable to aid in the task of taking a quiz on the sub-ject matter.Primed manual summary: Similar to above, inthis condition, a summary was created manually byselecting a set of utterances from the lecture tran-script.
For  primed  summaries,  full  access  to  apriming quiz, containing all of the questions in theevaluation quiz as well as several additional ques-tions, was available  at the time of summary cre-31ation.
This determines the value of creating sum-maries with a particular task in mind, as opposed tosimply choosing utterances that are felt to be mostimportant or salient.Automatic  summary: The  procedure  for  thiscondition was identical to the generic manual sum-mary condition from the point of view of the parti-cipant.
However, during the evaluation phase, anautomatically generated summary was provided in-stead of the summary that the participant createdhim or herself.
The algorithm used to generate thissummary was an implementation of  MMR  (Car-bonell and Goldstein, 1998).
Cosine similarity withtf-idf term weighting was used to calculate similar-ity.
Although the redundancy component of MMRmakes  it  especially  suitable  for  multi-documentsummarization,  there  is  no  negative  effect  if  re-dundancy is not an issue.
It is worth noting that ourlectures are longer than material typically summar-ized, and lectures in general are more likely to con-tain  redundant  material  than  a  domain  such  asbroadcast news.
There was only one MMR sum-mary generated for each lecture, meaning that mul-tiple participants made use of identical summaries.The automatic summary was created by adding thehighest scoring utterances one at a time until thesum of the length of all of the selected utterancesreached 20% of the number of words in the origin-al  lecture.
MMR was  chosen  as  it  is  commonlyused  in  summarization.
MMR  is  a  competitivebaseline,  even  among  state-of-art  summarizationalgorithms, which tend to correlate well with it.What  this  protocol  does  not  do  is  pit  severalstrategies  for  automatic  summary  generationagainst  each  other.
That  study,  where  more  ad-vanced summarization algorithms will also be ex-amined, is forthcoming.
The present experimentshave the collateral benefit  of  serving as a meansfor collecting ecologically valid human referencesfor that study.3.6ResultsQuizzes were scored by a teaching assistant for thesociology  course  from  which  the  lectures  weretaken.
Quizzes were marked as they would be inthe  actual  course  and  each  question  was  gradedwith equal  weight  out  of  two marks.
The scoreswere then converted to a percentage.
The resultingscores (Table 1) are 49.3+-17.3% for the  no sum-mary condition,  48.0+-16.2%  for  the  genericmanual  summary  condition,  49.1+-15.2% for  theprimed summary  condition,  and 41.0+-16.9% forMMR.
These scores are lower than averages expec-ted in a typical university course.
This can be par-tially  attributed  to  the  existence  of  a  time  con-straint.Condition Average Quiz Scoreno summary 49.3+-17.3%generic manual summary 48.0+-16.2%primed manual summary 49.1+-15.2%automatic summary (MMR) 41.0+-16.9%Table 1.
Average Quiz ScoresExecution  of  the  Shapiro-Wilk Test  confirmedthe scores are normally distributed and Mauchly'sTest of Sphericity indicates that the sphericity as-sumption holds.
Skewness and Kurtosis tests werealso  employed  to  confirm  normality.
A repeatedmeasures  ANOVA determined  that  scores  variedsignificantly between conditions (F(3,141)=5.947,P=0.001).
Post-hoc tests using the Bonferroni cor-rection  indicate  that  the  no  summary,  genericmanual  summary,  and  primed  manual  summaryconditions all  resulted  in  higher  scores  than  theautomatic (MMR) summary condition.
The differ-ence  is  significant  at  P=0.007,  P=0.014 andP=0.012 respectively.
Although normality was as-sured, the Friedman Test further confirms a signi-ficant  difference  between  conditions(?2(3)=11.684, P=0.009).4 F-measureF-measure is an evaluation metric that balancesprecision and recall which has been used to evalu-ate summarization.
Utterance level F-measurescores were calculated using the same summariesused in our human evaluation.
In addition, threeannotators were asked to create conventional goldstandard summaries using binary selection.
Annot-ators were not primed in any sense, did not watchthe lecture videos, and had no sense of the higherlevel purpose of their annotations.
We refer to theresulting summaries as context-free as they werenot created under ecologically valid conditions.
F-measure was also calculated with reference tothese.The F-measure results (Table 2) point out a fewinteresting phenomena.
Firstly, when evaluating a32given  peer  summary  type  with  the  same  modeltype,  the  generic-generic  scores  are  higher  thanboth  the  primed-primed and  context-free-con-text-free summaries.
This means that generic sum-maries tend to share more utterances with each oth-er, than primed summaries do, which are more var-ied.
This seems unintuitive at first, but could po-tentially be explained by the possibility that differ-ent participants focused on different aspects of thepriming quiz, due to either perceived importance,or lack of time (or summary space) to address allof the priming questions.Peer Type Model Type Average F-measuregeneric generic 0.388primed generic 0.365MMR generic 0.214generic primed 0.365primed primed 0.374MMR primed 0.209generic context-free 0.371primed context-free 0.351MMR context-free 0.243context-free context-free 0.374Table 2.
Average F-measureWe  also  observe  that  generic  summaries  aremore similar to conventionally annotated (context-free) summaries than either primed or MMR are.This  makes  sense  and  also  confirms  that  eventhough primed summaries do not significantly out-perform generic summaries in the quiz taking task,they are inherently distinguishable from each other.Furthermore,  when  evaluating  MMR using  F-measure,  we  see that  MMR summaries  are  mostsimilar to the context-free summaries, whose utter-ance selections can be considered somewhat arbit-rary.
Our  quiz  results  confirm MMR is  signific-antly worse  than  generic  and primed summaries.This casts doubt on the practice of using similarlyannotated  summaries  as  gold  standards  for  sum-marization evaluation using ROUGE.5 ROUGE EvaluationMore  common  than  F-measure,  ROUGE  (Lin,2004) is often used to evaluate summarization.
Al-though Lin (2004) claimed to have demonstratedthat ROUGE correlates well with human summar-ies,  both  Murray  et  al.
(2005),  and Liu  and Liu(2010) have cast doubt upon this.
It is important toacknowledge, however, that ROUGE is actually afamily of measures, distinguished not only by themanner  in  which  overlap  is  measured  (1-grams,longest  common  subsequences,  etc.
),  but  by  theprovenience of the summaries that are provided toit as references.
If these are not ecologically valid,there is no sense in holding ROUGE accountablefor an erratic result.To examine how ROUGE fairs under ecologic-ally  valid  conditions,  we  calculated  ROUGE-1,ROUGE-2,  ROUGE-L, and ROUGE-SU4 on ourdata using the standard options outlined in previ-ous DUC evaluations.
ROUGE scores were calcu-lated  for  each  of  the  generic  manual  summary,primed manual summary, and automatic summaryconditions.
Each  summary  in  a  given  conditionwas  evaluated  once  against  the  generic  manualsummaries  and  once  using  the  primed  manualsummaries.
Similar  to  Liu  and  Liu  (2010),ROUGE  evaluation  was  conducted  using  leave-one-out on the model summary type and averagingthe results.In addition to calculating ROUGE on the sum-maries from our ecologically valid evaluation, wealso followed  more  conventional  ROUGE evalu-ation  and  used  the  same  context-free  annotatorsummaries as were used in our F-measure calcula-tions above.
Using these context-free summaries,the original  generic  manual,  primed manual,  andautomatic  summaries  were  evaluated  usingROUGE.
The  result  of  these  evaluations  arepresented in Table 3.Looking at the ROUGE scores, we can see thatwhen evaluated by each type of model summary,MMR  performs  worse  than  either  generic  orprimed manual summaries.
This is consistent withour quiz results, and perhaps shows that ROUGEmay be able to distinguish human summaries fromMMR.
Looking  at  the  generic-generic,  primed-primed,  and  context-free-context-free scores,  wecan get a sense of how much agreement there wasbetween summaries.
It is not surprising that con-text-free  annotator  summaries  showed  the  leastagreement,  as  these  summaries  were  generatedwith no higher purpose in mind.
This suggests thatusing annotators to generate gold standards in sucha manner is not ideal.
In addition, real world ap-plications  for  summarization  would  conceivably33rarely consist of a situation where a summary wascreated for no apparent reason.
More interesting isthe observation that, when measured by ROUGE,primed summaries have less in common with eachother than generic summaries do.
The difference,however,  is  less  pronounced  when  measured  byROUGE than by F-measure.
This is likely due tothe fact that ROUGE can account for semanticallysimilar utterances.PeertypeModel type R-1 R-2 R-L R-SU4generic generic 0.75461 0.48439 0.75151 0.51547primed generic 0.74408 0.46390 0.74097 0.49806MMR generic 0.71659 0.40176 0.71226 0.44838generic primed 0.74457 0.46432 0.74091 0.49844primed primed 0.74693 0.46977 0.74344 0.50254MMR primed 0.70773 0.38874 0.70298 0.43802generic context-free 0.72735 0.46421 0.72432 0.49573primed context-free 0.71793 0.44325 0.71472 0.47805MMR context-free 0.69233 0.37600 0.68813 0.42413context-freecontext-free 0.70707 0.44897 0.70365 0.48019Table 3.
Average ROUGE Scores5.1Correlation with Quiz ScoresIn order to assess the ability of ROUGE to predictquiz scores, we measured the correlation betweenROUGE scores and quiz scores on a per participantbasis.
Similar to Murray et al (2005), and Liu andLiu (2010), we used Spearman?s rank coefficient(rho) to measure the correlation between ROUGEand our human evaluation.
Correlation was meas-ured both by calculating Spearman's rho on all datapoints (?all?
in Table 4) and by performing the cal-culation separately for each lecture and averagingthe results (?avg?).
Significant rho values (p-valueless than 0.05) are shown in bold.Note that there are not many bolded values, in-dicating  that  there  are  few  (anti-)correlationsbetween quiz scores and ROUGE.
The rho valuesreported by Liu and Liu (2010) correspond to the?all?
row of  our  generic-context-free  scores  (Liuand Liu (2010) did not report ROUGE-L), and weobtained  roughly  the  same  scores  as  theydid.
In  contrast  to  this,  our  "all"  generic-genericcorrelations are very low.
It is possible that the lec-tures condition the parameters of the correlation tosuch an extent that fitting all of the quiz-ROUGEpairs to the same correlation across lectures is un-reasonable.
It may therefore be more useful to lookat rho  values computed by lecture.
For these val-ues, our R-SU4 scores are not as high relative to R-1 and R-2 as those reported by Liu and Liu (2010).It is also worth noting that the use of context-freebinary selections as a reference results in increasedcorrelation for generic summaries, but substantiallydecreases correlation for primed summaries.With the exception that generic references prefergeneric  summaries  and  primed  references  preferprimed  summaries,  all  other  values  indicate  thatboth generic and primed summaries are better thanMMR.
However,  instead  of  ranking  summarytypes,  what  is  important  here  is  the  ecologicallyvalid quiz scores.
Our data provides no evidencethat ROUGE scores accurately predict quiz scores.6 ConclusionsWe have presented an investigation into how cur-rent  measures  and  methodologies  for  evaluatingsummarization systems compare to human-centricevaluation  criteria.
An  ecologically-valid  evalu-ation was conducted that determines the value of asummary  when  embedded  in  a  task,  rather  thanhow closely a summary resembles a gold standard.The  resulting  quiz  scores  indicate  that  manualsummaries  are  significantly  better  than  MMR.ROUGE scores were calculated using the summar-ies created in the study.
In addition, more conven-tional context-free annotator summaries were alsoused in ROUGE evaluation.
Spearman's rho indic-ated  no  correlation  between  ROUGE scores  andour ecologically valid quiz scores.
The results offerevidence that ROUGE scores and particularly con-text-free  annotator-generated  summaries  as  goldstandards may not always be reliably used in placeof an ecologically valid evaluation.34Peer type Model type R-1 R-2 R-L R-SU4generic generic all 0.017 0.066 0.005 0.058lec1 0.236 0.208 0.229 0.208lec2 0.276 0.28 0.251 0.092lec3 0.307 0.636 0.269 0.428lec4 0.193 -0.011 0.175 0.018avg 0.253 0.278 0.231 0.187primed generic all -0.097 -0.209 -0.090 -0.192lec1 -0.239 -0.458 -0.194 -0.458lec2 -0.306 -0.281 -0.306 -0.316lec3 0.191 0.142 0.116 0.255lec4 -0.734 -0.78 -0.769 -0.78avg -0.272 -0.344 -0.288 -0.325generic primed all 0.009 0.158 -0.004 0.133lec1 0.367 0.247 0.367 0.162lec2 0.648 0.425 0.634 0.304lec3 0.078 0.417 0.028 0.382lec4 0.129 0.079 0.115 0.025avg 0.306 0.292 0.286 0.218primed primed all 0.161 0.042 0.161 0.045lec1 0.042 -0.081 0.042 -0.194lec2 0.238 0.284 0.259 0.284lec3 0.205 0.12 0.205 0.12lec4 0.226 0.423 0.314 0.423avg 0.178 0.187 0.205 0.158generic con-text-freeall 0.282 0.306 0.265 0.347lec1 -0.067 0.296 -0.004 0.325lec2 0.414 0.414 0.438 0.319lec3 0.41 0.555 0.41 0.555lec4 0.136 0.007 0.136 0.054avg 0.223 0.318 0.245 0.313primed con-text-freeall -0.146 -0.282 -0.151 -0.305lec1 0.151 -0.275 0.151 -0.299lec2 -0.366 -0.611 -0.366 -0.636lec3 0.273 0.212 0.273 0.202lec4 -0.815 -0.677 -0.825 -0.755avg -0.189 -0.338 -0.192 -0.372Table 4.
Correlation (Spearman's rho) between QuizScores and ROUGE7 ReferencesJ.
Carbonell and J. Goldstein.
1998.
The use of mmr, di-versity-based reranking for reordering documents andproducing summaries.
In Proceedings of the 21st an-nual  international  ACM SIGIR  conference  on  Re-search  and  development  in  information  retrieval,335-336, ACM.P.
R. Cohen.
1995.
Empirical methods for artificial in-telligence.
Volume 55.
MIT press Cambridge, Mas-sachusetts.H.
Christensen,  B.  Kolluru,  Y.  Gotoh,  and S.  Renals.2004.
From text summarisation to style-specific sum-marisation for broadcast news.
Advances in Informa-tion Retrieval, 223-237.L.
He, E. Sanocki, A. Gupta, and J. Grudin.
1999.
Auto-summarization of audio-video presentations.
In  Pro-ceedings  of  the  seventh  ACM international  confer-ence on Multimedia (Part 1), 489-498.
ACM.L.
He, E. Sanocki, A. Gupta, and J. Grudin.
2000.
Com-paring presentation summaries: slides vs. reading vs.listening.
In Proc.
of the SIGCHI, 177-184, ACM.C.
Lin.
2004.
Rouge:  a  package for  automatic  evalu-ation of summaries.
In Proc.
of ACL, Text Summariz-ation Branches Out Workshop, 74?81.F.
Liu and Y. Liu.
2010.
Exploring correlation betweenrouge and human evaluation on meeting summaries.Audio,  Speech,  and  Language  Processing,  IEEETransactions on, 18(1):187-196.C.
Munteanu,  R.  Baecker,  G. Penn,  E.  Toms,  and  D.James.
2006.
The effect of speech recognition accur-acy rates on the usefulness and usability of webcastarchives.
In  Proceedings of the SIGCHI conferenceon Human Factors in  computing systems,  493-502,ACM.G.
Murray, S. Renals, J. Carletta, and J. Moore.
2005.Evaluating automatic summaries of meeting record-ings.
In Proc.
of the ACL 2005 MTSE Workshop, AnnArbor, MI, USA, 33-40.G.
Murray,  T.  Kleinbauer,  P.  Poller,  S.  Renals,  J.Kilgour, and T. Becker.
2008.
Extrinsic summariza-tion  evaluation:  A  decision  audit  task.
MachineLearning for Multimodal Interaction, 349-361.G.
Penn and X. Zhu.
2008.
A critical reassessment ofevaluation baselines for speech summarization.
Proc.of ACL-HLT.S.
Tucker, O. Bergman, A. Ramamoorthy, and S. Whit-taker.
2010.
Catchup:  a  useful  application  of  time-travel in meetings.
In Proc.
of CSCW, 99-102, ACM.S.
Tucker and S. Whittaker.
2006.
Time is  of  the  es-sence:  an  evaluation  of  temporal  compression  al-gorithms.
In Proc.
of the SIGCHI, 329-338, ACM.35
