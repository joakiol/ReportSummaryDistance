The Efficiency of Multimodal Interaction for a Map-based TaskPhilip COHEN, David McGEE, Josh CLOWCenter for Human-Computer CommunicationOregon Graduate Institute of Science & Technology20000 N.W.
Walker RoadBeaverton, Oregon 97006{ pcohen, dmcgee } @cse.ogi.eduAbstractThis paper compares the efficiency of using astandard direct-manipulation graphical userinterface (GUI) with that of using the QuickSetpen/voice multimodal interface for supporting amilitary task.
In this task, a user places militaryunits and control measures (e.g., various typesof lines, obstacles, objectives) on a map.
Fourmilitary personnel designed and entered theirown simulation scenarios via both interfaces.Analyses revealed that the multimodal interfaceled to an average 3.5-fold speed improvement inthe average ntity creation time, including allerror handling.
The mean time to repair errorsalso was 4.3 times faster when interactingmultimodally.
Finally, all subjects reported astrong preference for multimodal interaction.These results indicate a substantial efficiencyadvantage for multimodal over GUI-basedinteraction during map-based tasks.IntroductionNearly two decades ago at ACL'80, ProfessorBen Shneiderman challenged the field of naturallanguage processing as follows:In constructing computer systems which mimicrather than serve people, the developer maymiss opportunities for applying the unique andpowerful features of a computer: extreme speed,capacity to repeat edious operations accurately,virtually unlimited storage for data, anddistinctive input/output devices.
Although theslow rate of human speech makes menuselection impractical, high-speed computerdisplays make menu selection an appealingalternative.
Joysticks, light pens or the "mouse"are extremely rapid and accurate ways ofselecting and moving graphic symbols or text ona display screen.
Taking advantage of these andother computer-specific techniques will enabledesigners to create powerful tools withoutnatural language commands.
\[20, p. 139\]He also challenged us to go beyond mereclaims, but to demonstrate the benefits ofnatural language processing technologiesempirically.
Since then, not only has there beena long period of unprecedented innovation inhardware, software architectures, speechprocessing, and natural language processing, butNLP research has also embraced empiricalmethods as one of its foundations.
Still, wehave yet to defend claims empirically thattechnologies for processing natural humancommunication are more efficient, effective,and/or preferred, than interfaces that are bestviewed as "tools," especially interfacesinvolving a direct manipulation style ofinteraction.
The present research attempts totake a small step in this direction.In fact, it has often been claimed that spokenlanguage-based human-computer interactionwill not only be more natural but also moreefficient than keyboard-based interaction.Many of these claims derive from earlymodality comparison studies \[1\], which found a2-3 fold speedup in task performance whenpeople communicated with each other bytelephone vs. by keyboard.
Studies of the use ofsome of the initial commercial speechrecognition systems have reported efficiencygains of approximately 20% - 40% on a variety331of interactive hands-busy tasks [10] comparedwith keyboard input.
Although these resultswere promising, once the time needed for errorcorrection was included, the speed advantage ofspeech often evaporated [18] ~.
A recent study ofspeech-based dictation systems [9] reported thatdictation resulted in a slower and more errorfulmethod of text creation than typing.
From suchresults, it is often concluded that the age ofspoken human-computer interaction is not yetupon us.Most of these studies have compared speechwith typing, However, in order to affectmainstream computing, spoken interactionwould at a minimum need to be found to besuperior to graphical user interfaces (GUIs) fora variety of tasks.
In an early study of onecomponent of GUIs, Rudnicky [18] comparedspoken interaction with use of a scroll bar,finding that error correction wiped out the speedadvantages of speech, but users still preferred tospeak.
Pausch and Leatherby [17] examined theuse of simple speaker-dependent discrete speechcommands with a graphical editor, as comparedwith the standard menu-based interface.
With a19-word vocabulary, subjects were found tocreate drawings 21% faster using speech andmouse than with the menu-based system.
Theyconjectured that reduction in mouse-movementwas the source of the advantage.
In general,more research comparing speech and spoken-language-based interfaces with graphical userinterfaces still is needed.We hypothesize that one reason for theequivocal nature of these results is that speech isoften being asked toperform an unnatural actthe interface design requires people to speakwhen other modalities of communication wouldbe more appropriate.
In the past, strengths andweaknesses of various communicationmodalities have been described [2, 6, 13], and astrategy of developing multimodal userinterfaces has been developed using thestrengths of one mode to overcome weaknessesin another, Interface simulation studiesI See also [6, 10] for a survey of results.Figure 1.
The ExInit GUIcomparing multimodal (speech/pen) interactionwith speech-only have found a 35% reduction inuser errors, a 30% reduction in spokendysfluencies (which lead to recognition errors),a 10% increase in speed, and a 100% userpreference for multimodal interaction overspeech-only in a map-based task [14].
Theseresults suggest hat multimodal interaction maywell offer advantages over GUI's for map-basedtasks, and may also offer advantages forsupporting error correction during dictation [16,19].In order to investigate these issues, weundertook a study comparing a multimodal anda graphical user interface that were built for thesame map-based task ~.1 Study ~This study compares agraphical user interfacepen/voice multimodaldirect-manipulationwith the QuickSetinterface [4] forsupporting a common militaryplanning/simulation task.
In this task, a userarrays forces on a map by placing iconsrepresenting military units (e.g., the 82 n~Airbome Division) and "control measures,"2 A high-performance spoken language system was also developedfor a similar task [ 12] but to our knowledge it was not formallyevaluated against the r levant GUI.3 A case study of one user was reported in [3].
This paper eportsa fuller study, with different users, statistical analyses, and anexpanded set of dependent measures (including error correction).332Figure 2.
QuickSet(e.g., various types of lines, obstacles, andobjectives).
A shared backend applicationsubsystem, called Exlnit, takes the userspecifications and attempts to decompose thehigher echelon units into their constituents.
Itthen positions the constituent units on the map,subject o the control measures and features ofthe terrain.1.2 ExInit's GUIExlnit provides a direct manipulation GUI (builtby MRJ Corp.) based on the MicrosoftWindows suite of interface tools, including atree-browser, drop-down scrolling lists, buttons(see Figure 1).
Many military systemsincorporate similar user interface tools foraccomplishing these types of tasks (e.g.,ModSAF [7]).
The tree-browser is used torepresent and access the collection of militaryunits.
The user employs the unit browser toexplore the echelon hierarchy until the desiredunit is located.
The user then selects that unit,and drags it onto the map in order to position iton the terrain.
The system then asks forconfirmation of the unit's placement.
Onceconfirmed, Exlnit invokes its deployment serverto decompose the unit into its constituents andposition them on the terrain.
Because this is atime-consuming process depending on theechelon of the unit, only companies and smallerunits were considered.To create a linear or area control measure, theuser pulls down a list of all control measure4 types, then scrolls and selects the desired type.Then the user pushes a button to start enteringpoints, selects the desired locations, and finallyclicks the button to exit the point creation mode.The user is asked to confirm that the selectedpoints are correct, after which the systemconnects them and creates a control measureobject of the appropriate type.Finally, there are many more features to thisGUI, but they were not considered for thepresent comparison.
The system and its GUIwere well-received by the client, and were usedto develop the largest known distributedsimulation (60,000 entities) for the USGovernment's Synthetic Theater of Warprogram (STOW).4 There were 45 entries, viewable in a window of size 9.
Theentries consisted of linear features (boundaries, obstacles, etc.
),then areas.3331.3 QuickSet's Multimodai InterfaceQuickSet is a multimodal (pen/voice) interfacefor map-based tasks.
With this system, a usercan create entities on a map by simultaneouslyspeaking and drawing \[4\].
With pen-based,spoken, or multimodal input, the user canannotate the map, creating points, lines, andareas of various types (see Figure 2).
In virtueof its distributed multiagent architecture,QuickSet operates in various heterogeneoushardware configurations, including wearable,handheld, desktop, and wall-sized.
Moreover, itcontrols numerous backend applications,including 3D terrain visualization \[5\] militarysimulation, disaster management \[15\] andmedical informatics.The system operates as follows: When the pen isplaced on the screen, the speech recognizer isactivated, thereby allowing users to speak andgesture simultaneously.
For this task, the usereither selects a spot on the map and speaks thename of a unit to be placed there (e.g,"mechanized company"), or draws a controlmeasure while speaking its name (e.g., "phaseline green").
In response, QuickSet creates theappropriate military icon on its map and asks forconfirmation.
Speech and gesture arerecognized in parallel, with the speechinterpreted by a definite-clause natural languageparser.
For this study, IBM's Voice TypeApplication Factory, a continuous, speaker-independent speech recognition system, wasused with a bigram grammar and 662-wordvocabulary.
In general, analyses of spokenlanguage and of gesture ach produce a list ofinterpretations represented as typed featurestructures \[8\].
The language supported by thesystem essentially consists of complex nounphrases, including attached prepositionalphrases and gerunds, and a small collection ofsentence forms.
Utterances can be just spoken,or coupled with pen-based gestures.
Multimodalintegration searches among the set ofinterpretations for the best joint interpretation\[8, 22\], which often disambiguates both speechand gesture simultaneously \[15\].
Typed featurestructure unification provides the basicinformation fusion operation.
Taking advantageof the system's mutual disambiguationcapability, QuickSet confirms its interpretationof the user input after multimodal integration\[11\], thereby allowing the system to correctrecognition and interpretation errors.
If theresult is acceptable, the user needs only toproceed; only unacceptable results requireexplicit disconfirmation.
Finally, themultimodal interpretation is sent directly to theExlnit deployment server, effectively bypassingthe Exlnit GUI.2 ProcedureThe study involved four subjects who wereretired US military domain experts, including aUS Army National Guard Brigadier General, aUS Army Reserve Major, a US Marine CorpsCaptain, and a US Army communicationsspecialist.
Each of the subjects was a frequentcomputer user, and all had familiarity both withGUIs built around the Microsoft user interfacetools as well as with pen-and-paper baseddrawing of unit symbology and diagrams onmaps.
Not having used either system before, thesubjects were given 30 minutes to learn theExlnit GUI, and the same amount of time tolearn QuickSet.
The subjects created scenariosof their own design, using entities common toboth systems, first on paper, then with each ofthe two systems.
The scenarios had 8-21 units,and 9-33 control measures.
The order ofinterface styles was counterbalanced acrosssubjects in this within-subject design.
Thesystems were run on a Pentium Pro 200MHzcomputer with an Input Technologies 14" colorflat-panel display.
Stylus input was used forQuickSet, and keyboard and mouse wereemployed with the GUI.The mean time needed for each expert subject ocreate and position a unit or control measurewas calculated for each interface.
The time tocreate an entity began when the mouse enteredthe relevant interface tool or the time when themicrophone was engaged by placing the pen onthe map.
Mouse "travel" time to the desiredinterface tool was not included because the pencould not be tracked when it was out of thesensitivity range of the digitizer.
Timing ended334when the system asked for confirmation of itsimpending action.
Separate creation timecalculations were made for units and controlmeasures because the GUI employed differentuser interface tools for each.
Also, whereas theset of QuickSet units was a subset of the unitsavailable to the GUI, the set of control measureswas identical for QuickSet and the GUI.The entity creation times reported in this studyinclude correction of all errors needed for bothQuickSet and the GUI.
Error correction timewas accumulated for each attempt until a userconfirmation (explicit or implicit) was achieved,or until the entire entity creation attempt wasaborted.
Only 4 multimodal interactions (total =20.7 secs.)
and 1 GUI interaction (total = 43.2secs.)
were aborted.
Errors for QuickSetincluded out-of-vocabulary or grammar,procedural errors (e.g., not clicking on the map),disfluencies, and recognition errors.
For theGUI, errors involved failure to enter or leavedrawing mode, selecting the wrong unit in thebrowser, disconfirming, etc.
Overall, QuickSetprovided an 88.5% successful understandingrate.By saying "multiple Xs," the QuickSet usercould enter a "mode" in which he was creatingan entity of type X (e.g., a mechanizedcompany).
To support his process, the systemstored a discourse referent hat was then unifiedwith subsequent input.
The user needed only tosay "here" and touch the screen in order tocreate another entity of that type at thatlocation?
In these cases, the time taken to enterthe mode was amortized over the entitiescreated.
Likewise, the time taken to open theunit browser to show the desired unit wasamortized over the units of that type createdbefore the browser was again scrolled.3 Resu l tsAnalyses revealed that multimodal interactionresulted in a 3.7-fold speed increase in creatingunits compared to the GUI, paired t-test, t (3) =5.791, p < 0.005, one-tailed.
In addition, itprovided a 3.3-fold increase in creating controlmeasures paired t-test t (3) = 8.298, p < 0.002,one-tailed (see Table I).
6 Much of this speeddifferential can be traced to the need to browsethe echelons of the US military, scrolling longlists of units with the GUI (e.g., 126 units are inthe list of US Army companies), followed by aseparate dragging operation to position theselected unit.
In contrast, QuickSet usersspecified the type of entity directly, andsupplied its location in parallel.
Likewise, thespeed differential for the control measures maybe attributed to the user's ability to both-drawand speak in parallel, where the GUI requiredseparate actions for going into and out ofdrawing mode, for selecting the type of controlmeasure, and for selecting appropriate points onthe map.Although there were fewer errors on averagewhen using the direct manipulation GUI, theywere not significantly fewer than wheninteracting multimodally.
In contrast, the timeneeded to repair an error was significantly lowerwhen interacting multimodally than with theGUI, paired t-test, t (3) = 4.703, p<0.009, one-tailed.
On balance, the same users completingthe same tasks spent 26% more total timecorrecting errors with the GUI than with themultimodal interface.s In general, the user could at that point say anything that wouldunify with the type of entity being created, such as "facing twotwo five degrees in defensive posture."
This would add additionaldata to the type of entity being created.
Similar data could beadded via the GUI, but it required interacting with a dialogue boxthat was only created after the unit's constituents were loaded (atime-consuming operation).
Since QuickSet users could supplythe data before the constituents were loaded, it was deemed morefair to ignore this QuickSet capability even though it speeds upmultimodal interaction considerably, and employs more extensivenatural language processing.It should be pointed out that the paired t-test takes intoconsideration the number of subjects.
Thus, these findings at thesesignificance levels are particularly strong.
A second set ofnonparametric tests (Wilcox on signed ranks) were also performed,indicating that multimodal interaction was significantly faster (p <0.034, one-tailed), in creating units and control measures, and alsoin correcting errors.335ExpertSubjectSl$2S3$4MeansTable I:Create UnitsMM GUI8.4 25.66.0 14.46~3 27.24.0 18.5CreateContr61MeasuresMM GUI6 .5  27.55.2 19.04.0 17.7RepairErrorsMM GUI12.9 49,37.7 30I I .6 56.16.3 23.0i '  6 I 9i6Mean time m seconds required to createvarious types of entities and to repair errors wheninteracting multimodally versus with the Exlnit GUIThe expert users were interviewed after thestudy regarding which interface they preferredand why.
Multimodal interaction was stronglypreferred by all users.
Reasons cited included itsefficiency and its support of precise drawing oflinear and area features.ConclusionsThis study indicates that when the user knowswhat s/he wants, there can be substantialefficiency advantages of multimodal interactionover direct manipulation GUIs for a map-basedtaste.
Despite having only four subjects, theresults exhibited extremely strong statisticalsignificance.
These results stand in contrast oprior research \[6, 9, 10, 18\] in which speedadvantages of spoken input were washed out bythe cost of correcting recognition errors.In the present study, not only was multimodalinteraction substantially faster than GUI-basedinteraction, even including error correctiontimes, error correction itself was four timesmore costly with a GUI than with multimodalinteraction.
These findings do not support hoseof Karat et al \[9\] who found that for correctingerrors in a dictation task, keyboard-mouse inputled to a 2.3-fold speed increase over speech.Both sets of findings might be reconciled bynoting that advantages of any type of userinterface, especially spoken and multimodalinteraction, may be task dependent.We attribute the findings here to the ability ofmultimodal interfaces to support parallelspecification of complementary parts of acommunicative act, as well as direct rather thanhierarchical or scrolled access to types ofentities.
Moreover, because the user canemploy each mode for its strengths /he canoffload different aspects of the communicationto different human cognitive systems, leading togreater efficiency \[21\] and fewer user errors\[131.It might be claimed that these results apply onlyto this GUI, and that a different GUI might offersuperior performance.
First, it is worth notingthat the same pattern of results were found forthe two GUI elements (drop-down list andhierarchical browser).
Thus, the results cannotsimply be attributed to the misuse of ahierarchical tool.
Second, we point out that thisGUI was developed as a product, and that manymilitary systems use very similar user interfacetools for the same purposes (selecting units)/Thus, these results may have substantialpractical impact for users performing this task.More generally, one study cannot establishresults for all possible user interfaces.
Therewill certainly be occasions in which a menu-based GUI will be superior to a multimodalinterface - e.g., when the user does not in factknow what s/he wants and needs to browse.Other GUI interface tools, such as a search fieldwith command completion, can be envisionedthat would provide direct access.
However, it isarguable that such an interface lement belongssquarely to graphical user interfaces, but drawsmore on features of language.
Also, it wouldrequire the user to type, even in circumstances(such as mobile usage) where typing would beinfeasible.
Given our philosophy of using eachmodality for its strengths, we believemultimodal and graphical user interfaces shouldbe integrated, rather than cast as opposites.Finally, we would expect hat these advantagesof multimodal interaction may generalize toother tasks and other user interfaces in which7 In fact, a recent experiment by the US Marines had mobilecombatants using small portable computers with a similar directmanipulation i terface as they participated in field exercises.
Theuser interface was generally regarded as the weakest aspect of theexperiment.336selection among many possible options isrequired.Obviously, a small experiment only illuminatesa small space.
But it should be clear that whencurrent technologies are blended into asynergistic multimodal interface the result mayprovide substantial improvements on sometypes of tasks heretofore performed withgraphical user interface technologies.
Weconjecture that the more we can take advantageof the strengths of spoken language technology,the larger this advantage will become.
Futureresearch should be searching for more suchtasks, and developing more general toolkits thatsupport rapid adaptation of multimodaltechnologies tosupport hem.AcknowledgementsThis work was supported in part by theInformation Technology and InformationSystems offices of DARPA under multiplecontract numbers DABT63-95-C-007 andN66001-99-D-8503, and in part by ONR grantN00014-95-1-1164.
Many thanks to JayPittman for the ExInit integration, MichaelJohnston for ExInit vocabulary and grammardevelopment, Liang Chen for graphics andmilitary symbology, Sharon Oviatt for advice inexperimental nalysis, and to our test subjects.References1.
Chapanis, A., Ochsman, R.B., Parrish, R.N.,Weeks, G. D., Studies in interactivecommunication: I.
The effects of fourcommunication modes on the behavior ofteams during cooperative problem solving.Human Factors, 1972.
14: pp.
487-509.2.
Cohen, P.R., Dalrymple, M., Moran, D.B.,Pereira, F,, Sullivan, J., Gargan, R.,Schlossberg, J., and Tyler, S., Synergisticuse of natural language and directmanipulation, in Proc.
of the Human-Factors in Computing Systems Con-ference(CHI'89).
1989, ACM Press: New York, pp.227-234.3.
Cohen, P.R., Johnston, M., McGee, D.,Oviatt, S., Clow, J., and Smith, I., Theefficiency of multimodal interaction: Acase study, in the Proceedings of the 5thInternational Conference on SpokenLanguage Processing, Sydney, Australia,1998, 2: pp.
249-252.4.
Cohen, P.R., Johnston, M., McGee, D.,Oviatt, S., Pittman, J., Smith, I., Chen, L.,Clow, J., QuickSet: Multimodal interactionfor distributed applications, in Proc.
of theFifth A CM International MultmediaConference, E. Glinert, Editor.
1997, ACMPress: New York.
pp.
31-40.5.
Cohen, P.R., McGee, D., Oviatt, S., Wu, L.,Clow, J., King, R., Julier, S., Rosenblum, L.,Multimodal Interaction for 2D and 3DEnvironments.
IEEE Computer Graphicsand Applications, 1999.
19(4): pp.
10-13.6.
Cohen, P.R.
and Oviatt, S.L., The Role ofVoice Input for Human-MachineCommunication.
Proc.
of the NationalAcademy of Sciences, 1995.
92: pp.
9921-9927.7.
Courtemanche, A.J., Ceranowicz, A.,ModSAF Development S atus., in the Proc.of the Fifth Con-ference on ComputerGenerated Forces and Behavioral Rep-resentation, Orlando, 1995, Univ.
of CentralFlorida, pp.
3-13.8, Johnston, M., Cohen, P. R., McGee, D.,Oviatt, S. L., Pittman, J.
A., Smith., I.Unification-based multimodal integration.,in the Proc.
of the 35th Annual Meeting ofthe Association for ComputationalLinguistics (ACL) and 8th Conference of theEuropean Chapter of the ACL, 1997, pp.281-288.9.
Karat, C., Halverson, C., Horn, D., andKarat, J., Patterns of entry and correction inlarge vocabulary continuous speechrecognition systems, in the Proc.
of HumanFactors in Com-puting Systems, New York,1999, ACM Press, pp.
568-575.10.
Martin, G.L., The utility of speech input inuser-computer interfaces.
InternationalJournal of Man-machine Studies, 1989.30(4): pp.
355-375.33711.
McGee, D., Cohen, P.R., and Oviatt, S.L.,Confirmation in Multimodal Systems, inProc.
of the 17th International Conferenceon Computational Linguistics (COLING98) and 36th Annual Meeting of theAssociation for Computational Linguistics(ACL98).
1998: Montreal, Canada.
pp.
823-829.12.
Moore, R., Dowding, J., Bratt, H., Gawron,J., Gorfu, Y., Cheyer, A., CommandTalk: ASpoken-Language Interface for BattlefieldSimulations, Proc.
of the 5th Conference onApplied Natural Language Processing,Association for Computational Linguistics,1997: Washington, DC.
pp.
1-7.13.
Oviatt, S. L., Pen/Voice: Complementarymultimodal communication, Proc.
ofSpeech Tech'92, New York, 238-24114.
Oviatt, S.L., Multimodal interactive maps:Designing for human performance.
HumanComputer Interaction, 1997.
12: pp.
93-129.15.
Oviatt, S.L., Mutual disambiguation ofrecognition errors in a multimodalarchitecture, in the Proc.
of the Conferenceon Human Factors in Computing System,New York, 1999, ACM Press, pp.
576-583.16.
Oviatt, S.L., Cohen, P. R., Wu, L., Vergo,J., Duncan, L., Suhm, B., Bers, J., Holzman,T., Winograd, T., Landay, J., Larson, J.,Ferro, D., Designing the user interface formultimodal speech and gesture applications:State-of-the-art systems and researchdirections for 2000 and beyond.
Insubmission.17.
Pausch, R. and Leatherby, J. H., A studycomparing mouse-only vs. mouse-plus-voice input for a graphical editor, Journalof the American Voice Input~Output Society,9:2, July, 1991, pp 55-6618.
Rudnicky, A.I., Mode Preference in asimple data-retrieval task, in ARPA HumanLanguage Technology Workshop.
March1993: Princeton, New Jersey.19.
Suhm, B., Myers, B., and Waibel, A.,Model-based and empirical evaluation ofmultimodal interactive rror correction, inthe Proc.
of the Conf.
on Human Factorsin Computing Systems, New York, 1999,ACM Press, 584-591.20.
Shneiderman, B., Natural vs. preciseconcise languages for human operation ofcomputers: Research issues andexperimental pproaches.
Proceedings ofthe 18 u' Annual Meeting of the Associationfor Computational Linguistics, andParasession on Topics in InteractiveDiscourse, Univ.
of Pennsylvania, June,1980, pp.
139-141.21.
Wickens, C., Sandry, D., and Vidulich, M.,Compatibility and resource competitionbetween modalities of input, centralprocessing, and output.
Human Factors,1983.25(2): pp.
227-248.22.
Wu, L., Oviatt, S., L. and Cohen, P. R.,Statistical multimodal integration forintelligent HCI, in Neural Networks forSignal Processing, Y.H.
Hu, Larsen, J.,Wilson, E., and Douglas, S., Editors.
1999,IEEE Press: New York.
pp.
487-496.338
