Proceedings of the NAACL HLT 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 140?146,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsHierarchical versus Flat Classification of Emotions in TextDiman Ghazi (a), Diana Inkpen (a), Stan Szpakowicz (a, b)(a) School of Information Technology and Engineering, University of Ottawa(b) Institute of Computer Science, Polish Academy of Sciences{dghaz038,diana,szpak}@site.uottawa.caAbstractWe explore the task of automatic classifica-tion of texts by the emotions expressed.
Ournovel method arranges neutrality, polarity andemotions hierarchically.
We test the methodon two datasets and show that it outperformsthe corresponding ?flat?
approach, which doesnot take into account the hierarchical informa-tion.
The highly imbalanced structure of mostof the datasets in this area, particularly the twodatasets with which we worked, has a dramat-ic effect on the performance of classification.The hierarchical approach helps alleviate theeffect.1 IntroductionComputational approaches to emotion analysishave focused on various emotion modalities, butthere was only limited effort in the direction ofautomatic recognition of emotion in text (Aman,2007).Oleveres et al(1998), as one of the first works inemotion detection in text, uses a simple NaturalLanguage Parser for keyword spotting, phraselength measurement and emoticon identification.They apply a rule-based expert system to constructemotion scores based on the parsed text and con-textual information.
However their simple word-level analysis system is not sufficient when theemotion is expressed by more complicated phrasesand sentences.More advanced systems for textual emotion recog-nition performed sentence-level analysis.
Liu et al(2003), proposed an approach aimed at understand-ing the underlying semantics of language usinglarge-scale real-world commonsense knowledge toclassify sentences into ?basic?
emotion categories.They developed a commonsense affect modelenabling the analysis of the affective qualities oftext in a robust way.In SemEval 2007, one of the tasks was carried outin an unsupervised setting and the emphasis was onthe study of emotion in lexical semantics (Strappa-rava and Mihalcea, 2008; Chaumartin, 2007; Koza-reva et al, 2007; Katz et al, 2007).
Neviarouskayaet al(2009) applied a rule-based approach to affectrecognition from a blog text.
However, statisticaland machine learning approaches have became amethod of choice for constructing a wide variety ofNLP applications (Wiebe et al, 2005).There has been previous work using statisticalmethods and supervised machine learning, includ-ing (Aman, 2007; Katz et al, 2007; Alm, 2008;Wilson et al, 2009).
Most of that research concen-trated on feature selections and applying lexicalsemantics rather than on different learningschemes.
In particular, only flat classification hasbeen considered.According to Kiritchenko et al (2006), ?Hierar-chical categorization deals with categorizationproblems where categories are organized in hierar-chies?.
Hierarchical text categorization places newitems into a collection with a predefined hierar-chical structure.
The categories are partially or-dered, usually from more generic to more specific.Koller and Sahami (1997) carried out the firstproper study of a hierarchical text categorizationproblem in 1997.
More work in hierarchical textcategorization has been reported later.
Keshtkarand Inkpen (2009) applied a hierarchical approachto mood classification: classifying blog posts into132 moods.
The connection with our work is onlyindirect, because ?
even though moods and emo-tions may seem similar ?
their hierarchy structureand the classification task are quite different.
Thework reported in (Kiritchenko et al, 2006) is moregeneral.
It explores two main aspects of hierarchic-140al text categorization: learning algorithms and per-formance evaluation.In this paper, we extend our preliminary work(Ghazi et al, 2010) on hierarchical classification.Hierarchical classification is a new approach toemotional analysis, which considers the relationbetween neutrality, polarity and emotion of a text.The main idea is to arrange these categories andtheir interconnections into a hierarchy and leverageit in the classification process.We categorize sentences into six basic emotionclasses; there also may, naturally, be no emotion ina sentence.
The emotions are happiness, sadness,fear, anger, disgust, and surprise (Ekman, 1992).In one of the datasets we applied, we did considerthe class non-emotional.For these categories, we have considered twoforms of hierarchy for classification, with two orthree levels.
In the two-level method, we explorethe effect of neutral instances on one dataset andthe effect of polarity on the other dataset.
In thethree-level hierarchy, we consider neutrality andpolarity together.Our experiments on data annotated with emotionsshow performance which exceeds that of the corre-sponding flat approach.Section 2 of this paper gives an overview of thedatasets and feature sets.
Section 3 describes bothhierarchical classification methods and theirevaluation with respect to flat classification results.Section 4 discusses future work and presents a fewconclusions.2 Data and Feature Sets2.1 DatasetsThe statistical methods typically require trainingand test corpora, manually annotated with respectto each language-processing task to be learned(Wiebe et al, 2005).
One of the datasets in ourexperiments is a corpus of blog sentences anno-tated with Ekman?s emotion labels (Aman, 2007).The second dataset is a sentence-annotated corpusresource divided into three parts for large-scaleexploration of affect in children?s stories (Alm,2008).In the first dataset, each sentence is tagged by adominant emotion in the sentence, or labelled asnon-emotional.
The dataset contains 173 weblogposts annotated by two judges.
Table 1 shows thedetails of the dataset.In the second dataset, two annotators have anno-tated 176 stories.
The affects considered are thesame as Ekman?s six emotions, except that thesurprise class is subdivided into positive surpriseand negative surprise.
We run our experiments ononly sentences with high agreement- sentenceswith the same affective labels annotated by bothannotators.
That is the version of the dataset whichmerged angry and disgusted instances and com-bined the positive and negative surprise classes.The resulting dataset, therefore, has only fiveclasses (Alm, 2008).
Table 1 presents more detailsabout the datasets, including the range of frequen-cies for the class distribution (Min is the proportionof sentences with the most infrequent class, Max isthe proportion for sentences with the most frequentclass.)
The proportion of the most frequent classalso gives us a baseline for the accuracies of ourclassifiers (since the poorest baseline classifiercould always choose the most frequent class).Table 1.
Datasets specifications.Domain Size # classes Min-Max%Aman?sData setWeblogs 2090 7 6-38 %Alm?sData setStories 1207 5 9-36%2.2 Feature setsIn (Ghazi et al, 2010), three sets of features ?
onecorpus-based and two lexically-based ?
are com-pared on Aman?s datasets.
The first experiment is acorpus-based classification which uses unigrams(bag-of-words).
In the second experiment, classifi-cation was based on features derived from thePrior-Polarity lexicon1 (Wilson et al 2009); thefeatures were the tokens common between theprior-polarity lexicon and the chosen dataset.
In thelast experiment, we used a combination of theemotional lists of words from Roget?s Thesaurus2(Aman and Szpakowicz, 2008) and WordNet Af-fect3 (Strapparava and Valitutti, 2004); we call itthe polarity feature set.1 www.cs.pitt.edu/mpqa2 The 1987 Penguin?s Roget?s Thesaurus was used.3www.cse.unt.edu/~rada/affectivetext/data/WordNetAffectEmotioLists.tar.gz141Based on the results and the discussion in (Ghazi etal., 2010), we decided to use the polarity featureset in our experiments.
This feature set has certainadvantages.
It is quite a bit smaller than the uni-gram features, and we have observed that they ap-pear to be more meaningful.
For example, theunigram features include (inevitably non-emotional) names of people and countries.
It isalso possible to have misspelled tokens in uni-grams, while the prior-polarity lexicon features arewell-defined words usually considered as polar.Besides, lexical features are known to be moredomain- and corpus-independent.
Last but notleast, our chosen feature set significantly outper-forms the third set.2.3 ClassificationAs a classification algorithm, we use the supportvector machines (SVM) algorithm with tenfoldcross-validation as a testing option.
It is shown thatSVM obtains good performance in text classifica-tion: it scales well to the large numbers of features(Kennedy and Inkpen, 2006; Aman, 2007).We apply the same settings at each level of thehierarchy for our hierarchical approach classifica-tion.In hierarchical categorization, categories are organ-ized into levels (Kiritchenko et al, 2006).
We usethe hierarchical categories to put more knowledgeinto our classification method as the category hier-archies are carefully composed manually to repre-sent our knowledge of the subject.
We will achievethat in two forms of hierarchy.
A two-level hierar-chy represents the relation of emotion and neutral-ity in text, as well as the relation of positive andnegative polarity.
These two relations are exam-ined in two different experiments, each on a sepa-rate dataset.A three-level hierarchy is concerned with the rela-tion between polarity and emotions along with therelation between neutrality and emotion.
We as-sume that, of Ekman's six emotions, happiness be-longs to the positive polarity class, while the otherfive emotions have negative polarity.
This is quitesimilar to the three-level hierarchy of affect labelsused by Alm (2008).
In her diagram, she considershappiness and positive surprise as positive, and therest as negative emotions.
She has not, however,used this model in the classification approach:classification experiments were only run at threeseparate affect levels.
She also considers positiveand negative surprise as one Surprise class.For each level of our proposed hierarchy, we runtwo sets of experiments.
In the first set, we assumethat all the instances are correctly classified at thepreceding levels, so we only need to be concernedwith local mistakes.
Because we do not have todeal with instances misclassified at the previouslevel, we call these results reference results.In the second set of experiments, the methodologyis different than in (Ghazi et al 2010).
In that workboth training and testing of subsequent levels isbased on the results of preceding levels.
A questionarises, however: once we have good data available,why train on incorrect data which result from mis-takes at the preceding level?
That is why we de-cided to train on correctly-labelled data and whentesting, to compute global results by cumulatingthe mistakes from all the levels of the hierarchicalclassification.
In other words, classification mis-takes at one level of the hierarchy carry on as mis-takes at the next levels.
Therefore, we talk ofglobal results because we compute the accuracy,precision, recall and F-measure globally, based onthe results at all levels.
These results characterizethe hierarchical classification approach when test-ing on new sentences: the classifiers are applied ina pipeline order: level 1, then level 2 on the resultsof the previous level (then level 3 if we are in thethree-level setting).In the next section, we show the experiments andresults on our chosen datasets.3 Results and discussions3.1 Two-level classificationThis section has two parts.
The main goal of thefirst part is to find out how the presence of neutralinstances affects the performance of features fordistinguishing between emotional classes inAman?s dataset.
This was motivated by a similarwork in polarity classification (Wilson et al,2009).In the second part, we discuss the effect of consid-ering positive and negative polarity of emotions forfive affect classes in Alm?s dataset.1423.1.1 Neutral-EmotionalAt the first level, emotional versus non-emotionalclassification tries to determine whether an in-stance is neutral or emotional.
The second steptakes all instances which level 1 classified as emo-tional, and tries to classify them into one of Ek-man's six emotions.
Table 2 presents the result ofexperiments and, for comparison, the flat classifi-cation results.
A comparison of the results in bothexperiments with flat classification shows that inboth cases the accuracy of two-level approach issignificantly better than the accuracy of flat classi-fication.One of the results worth discussing further is theprecision of the non-emotional class: it increaseswhile recall decreases.
We will see the same pat-tern in further experiments.
This happens to theclasses which used to dominate in flat classifica-tion but they no longer dominate in hierarchicalclassification.
Classifiers tends to give priority to adominant class, so more instances are placed inthis class; thus, classification achieves low preci-sion and high recall.
Hierarchical methods tend toproduce higher precision.The difference between precision and recall of thehappiness class in the flat approach and the two-level approach cannot be ignored.
It can be ex-plained as follows: at the second level there are nomore non-emotional instances, so the happinessclass dominates, with 42% of all the instances.
Asexplained before, this gives high recall and lowprecision for the happiness class.
We hope to ad-dress this big gap between precision and recall ofthe happiness class in the next experiments, three-level classification.
It separates happiness from theother five emotions, so it makes the number of in-stances of each level more balanced.Our main focus is comparing hierarchical and flatclassification, assuming all the other parametersare fixed.
We mention, however, the best previousresults achieved by Aman (2007) on the same data-set.
Her best result was obtained by combiningcorpus-based unigrams, features derived fromemotional lists of words from Roget?s Thesaurus(explained in 2.2) and common words between thedataset and WordNetAffect.
She also applied SVMwith tenfold cross validation.
The results appear inTable 3.Table 3.
Aman?s best results on her data set.Precision Recall F-Measurehappiness 0.813  0.698  0.751sadness  0.605  0.416  0.493fear  0.868  0.513  0.645surprise  0.723  0.409  0.522disgust  0.672  0.488  0.566anger  0.650  0.436  0.522non-emo 0.587  0.625  0.605Table 2.
Two-level emotional classification on Aman?s dataset (the highest precision, recall, and F-measure val-ues for each class are shown in bold).
The results of the flat classification are repeated for convenience.Two-level classification Flat classificationPrecision Recall F-measure Precision Recall F-measure1st level emo non-emo0.880.880.850.810.860.84--0.54--0.87--0.672nd levelreference resultshappinesssadnessfearsurprisedisgustanger0.590.770.910.750.660.720.950.490.490.320.350.330.710.600.630.450.450.460.740.690.820.640.680.670.600.420.490.270.310.260.660.520.620.380.430.38Accuracy   68.32%   61.67%2-level experi-mentglobal resultsnon-emohappinesssadnessfearsurprisedisgustanger0.880.560.640.750.560.520.550.810.860.420.430.290.290.270.840.680.510.550.380.370.360.540.740.690.820.640.680.670.870.600.420.490.270.310.260.670.660.520.620.380.430.38Accuracy   65.50%   61.67%143By comparing the reference results in Table 2 withAman?s result shown in Table 3, our results on twoclasses, non-emo and sadness are significantly bet-ter.
Even though recall of our experiments is high-er for happiness class, the precision makes the F-measure to be lower.
The reason behind the differ-ence between the precisions is the same as theirdifference between in our hierarchical and flatcomparisons.
As it was also mentioned there wehope to address this problem in three-level classifi-cation.
Both precision and recall of the sadness inour experiments is higher than Aman?s results.
Wehave a higher precision for fear, but recall isslightly lower.
For the last three classes our preci-sion is higher while recall is significantly lower.The size of these three classes, which are the smal-lest classes in the dataset, appears to be the reason.It is possible that the small set of features that weare using will recall fewer instances of theseclasses comparing to the bigger feature sets usedby Aman (2007).3.1.2 Negative-Positive polarityThese experiments have been run on Alm?s datasetwith five emotion classes.
This part is based on theassumption that the happiness class is positive andthe remaining four classes are negative.At the first level, positive versus negative classifi-cation tries to determine whether an instance bearsa positive emotion.
The second step takes all in-stances which level 1 classified as negative, andtries to classify them into one of the four negativeclasses, namely sadness, fear, surprise and anger-disgust.
The results show a higher accuracy in ref-erence results while it is slightly lower for globalresults.
In terms of precision and recall, however,there is a high increase in precision of positive(happiness) class while the recall decreases.The results show a higher accuracy in referenceresults while it is slightly lower for global results.In terms of precision and recall, however, there is ahigh increase in precision of positive (happiness)class while the recall decreases.We also see a higher F-measure for all classes inthe reference results.
That confirms the consistencybetween the result in Table 2 and Table 4.In the global measurements, recall is higher for allthe classes at the second level, but the F-measure ishigher only for three classes.Here we cannot compare our results with the bestprevious results achieved by Alm (2008), becausethe datasets and the experiments are not the same.She reports the accuracy of the classification re-sults for three sub-corpora separately.
She random-ly selected neutral instances from the annotateddata and added them to the dataset, which makes itTable 4.
Two-level emotional classification on Alm?s dataset (the highest precision, recall, and F-measure val-ues for each class are shown in bold).Two-level classification Flat classificationPrecision Recall F-measure Precision Recall F-measure1st level neg pos0.810.840.930.640.870.72--0.56--0.86--0.682nd levelreference resultssadnessfearsurpriseanger0.650.590.450.490.680.400.210.730.660.470.290.590.670.590.350.540.530.380.100.430.590.460.160.48Accuracy   59.07%   57.41%2-level experimentglobal resultshappinesssadnessfearsurpriseanger0.840.550.450.270.430.640.610.390.210.680.720.580.420.190.530.560.670.590.350.540.860.530.380.100.430.680.590.460.160.48Accuracy   56.57%   57.41%144different than the data set we used in our experi-ments.3.2 Three-level classificationIn this approach, we go even further: we break theseven-class classification task into three levels.The first level defines whether the instance is emo-tional.
At the second level the instances defined asemotional by the first level will be classified ontheir polarity.
At the third level, we assume that theinstances of happiness class have positive polarityand the other five emotions negative polarity.
Thatis why we take the negative instances from thesecond level and classify them into the five nega-tive emotion classes.
Table 5 presents the results ofthis classification.
The results show that the accu-racy of both reference results and global results arehigher than flat classification, but the accuracy ofthe global results is not significantly better.At the first and second level, the F-measure of no-emotion and happiness classes is significantly bet-ter.
At the third level, except in the class disgust,we see an increase in the F-measure of all classesin comparison with both the two-level and flatclassification.Table 5.
Three-level emotional classification on Aman?s data-set (the highest precision, recall, and F-measure values foreach class are shown in bold)?Three-level ClassificationPrecision Recall F1st level emonon-emo0.880.880.850.810.860.842nd levelreference resultspositivenegative0.890.790.650.940.750.863rd levelreference resultssadnessfearsurprisedisgustanger0.630.880.790.420.380.540.520.370.380.710.590.650.500.400.49Accuracy   65.5%3-level experi-mentglobal resultsnon-emohappinesssadnessfearsurprisedisgustanger0.880.770.430.520.460.310.350.810.620.490.40.320.310.550.840.690.460.450.380.310.43Accuracy   62.2%Also, as shown by the two-level experiments, theresults of the second level of the reference resultsapproach an increase in the precision of the happi-ness class.
That makes the instances defined ashappiness more precise.By comparing the results with Table 3, which isthe best previous results, we see an increase in theprecision of happiness class and its F-measureconsequently; therefore in these results we get ahigher F-measure for three classes, non-emo, sad-ness and fear.
We get the same F-measure for hap-piness and slightly lower F-measure for surprisebut we still have a lower F-measure for the othertwo classes, namely, disgust and anger.
The otherdifference is the high increase in the recall valuefor fear.4 Conclusions and Future WorkThe focus of this study was a comparison of thehierarchical and flat classification approaches toemotional analysis and classification.
In the emo-tional classification we noticed that having adominant class in the dataset degrades the resultssignificantly.
A classifier trained on imbalanceddata gives biased results for the classes with moreinstances.
Our results, based on a novel method,shows that the hierarchical classification approachis better at dealing with the highly imbalanceddata.
We also saw a considerable improvement inthe classification results when we did not deal withthe errors from previous steps and slightly betterresults when we evaluated the results globally.In the future, we will consider different levels ofour hierarchy as different tasks which could behandled differently.
Each of the tasks has its ownspecification.
We can, therefore, definitely benefitfrom analyzing each task separately and definingdifferent sets of features and classification methodsfor each task rather than using the same method forevery task.145ReferencesAlm, C.: ?Affect in text and speech?, PhD disserta-tion, University of Illinois at Urbana-Champaign, Department of Linguistics (2008)Aman, S.: ?Identifying Expressions of Emotion inText?, Master's thesis, University of Ottawa, Ot-tawa, Canada (2007)Aman, S., Szpakowicz, S.: ?Using Roget?s Thesau-rus for Fine-grained Emotion Recognition?.Proc.
Conf.
on Natural Language Processing(IJCNLP), Hyderabad, India, 296-302 (2008)Chaumartin.
F.: ?Upar7: A knowledge-based sys-tem for headline sentiment tagging?, Proc.
Se-mEval-2007, Prague, Czech Republic, June(2007)Ekman, P.: ?An Argument for Basic Emotions?,Cognition and Emotion, 6, 169-200 (1992)Ghazi, D., Inkpen, D., Szpakowicz, S.: ?Hierar-chical approach to emotion recognition and clas-sification in texts?, A. Farzindar and V.
Keselj(eds.
), Proc.
23rd Canadian Conference on Ar-tificial Intelligence, Ottawa, ON.
Lecture Notesin Artificial Intelligence 6085, Springer Verlag,40?50 (2010)Katz, P., Singleton, M., Wicentowski, R.: ?Swat-mp: the semeval-2007 systems for task 5 andtask 14?, Proc.
SemEval-2007, Prague, CzechRepublic, June (2007)Kennedy, A., Inkpen, D.: ?Sentiment classificationof movie reviews using contextual valence shif-ter?, Computational Intelligence 22.
110-125(2006)Keshtkar, F., Inkpen, D.: ?Using Sentiment Orien-tation Features for Mood Classification in BlogCorpus?, IEEE International Conf.
on NLP andKE, Dalian, China, Sep. 24-27 (2009)Kiritchenko, S., Matwin, S., Nock, R., Famili,F.
: ?Learning and Evaluation in the Presence ofClass Hierarchies: Application to Text Categori-zation?, Lecture Notes in Artificial Intelligence4013, Springer, 395-406 (2006)Koller, D., Sahami, M.: ?Hierarchically Classify-ing Documents Using Very Few Words?.
Proc.International Conference on Machine Learning,170-178 (1997)Kozareva, Z., Navarro, B., Vazquez, S., Montoyo,A.
: ?UA-ZBSA: A headline emotion classifica-tion through web information?, Proc.
SemEval-2007, Prague, Czech Republic, June (2007)Liu, H., Lieberman, H., Selker, T.: ?A Model ofTextual Affect Sensing using Real-WorldKnowledge?.
In Proc.
IUI 2003, 125-132 (2003)Neviarouskaya, A., Prendinger, H., and Ishizuka,M.
: ?Compositionality Principle in Recognitionof Fine-Grained Emotions from Text?, In: Pro-ceedings of Third International Conference onWeblogs and Social Media (ICWSM?09),AAAI, San Jose, California, US, 278-281 (2009)Olveres, J., Billinghurst, M., Savage, J., Holden,A.
: ?Intelligent, Expressive Avatars?.
In Proc.
ofthe WECC?98, 47-55 (1998)Strapparava, C., Mihalcea, R.: ?SemEval-2007Task 14: Affective Text?
(2007)Strapparava, C., Mihalcea, R.: ?Learning to Identi-fy Emotions in Text?, Proc.
ACM Symposiumon Applied computing, Fortaleza, Brazil, 1556-1560 (2008)Wilson, T., Wiebe, J., Hoffmann, P.: ?Recognizingcontextual polarity: an exploration of featuresfor phrase-level sentiment analysis?, Computa-tional Linguistics 35(3), 399-433 (2009)Wiebe, J., Wilson, T., Cardie, C.: ?AnnotatingExpressions of Opinions and Emotions in Lan-guage?, Language Resources and Evaluation 39,165-210 (2005)146
