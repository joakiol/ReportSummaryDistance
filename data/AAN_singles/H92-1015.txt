SPEECH UNDERSTANDING IN OPEN TASKSWayne Ward, Sunil lssarXuedong Huang, Hsiao-Wuen Hon, Mei-Yuh HwangSheryl Young, Mike MatessaFu-Hua Liu, Richard SternSchool of  Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213ABSTRACTThe Air Traffic Information Service task is currently used byDARPA as a common evaluation task for Spoken LanguageSystems.
This task is an example of open type tasks.
Subjects aregiven a task and allowed to interact spontaneously with the sys-tem by voice.
There is no fixed lexicon or grammar, and subjectsare likely to exceed those used by any given system.
In order toevaluate system performance on such tasks, a common corpus oftraining data has been gathered and annotated.
An independenttest corpus was also created in a similar fashion.
This paperexplains the techniques used in our system and the performanceresults on the standard set of tests used to evaluate systems.1.
SYSTEM OVERVIEWOur Spoken Language System uses a speech recognizerwhich is loosely coupled to a natural language understand-ing system.
The SPHINX-II speech recognition systemproduces a single best hypothesis for the input.
It uses abacked-off class bigram language model in decoding theinput.
This type of smoothed stochastic language modelprovides some flexibility when presented with unusualgrammatical constructions.
The single best hypothesis ispassed to the natural language understanding system whichuses flexible parsing techniques to cope with novel phras-ings and misrecognitions.
In addition to the basic speechrecognition and natural anguage understanding modules,we have developed techniques to enhance the performanceof each.
We have developed an environmental robustnessmodule to minimize the effects of changing environmentson the recognition.
We have also developed a system touse a knowledge base to asses and correct the parsesproduced by our natural language parser.
We present eachof the modules separately and discuss their evaluationresults in order to understand how well the techniques per-form.
The authors on each line in the paper heading reflectthose people who worked on each module respectively.2.
FLEXIBLE PARSINGOur NL understanding system (Phoenix) is flexible atseveral levels.
It uses a simple frame mechanism torepresent task semantics.
Frames are associated with thevarious types of actions that can be taken by the system.Slots in a frame represent the various pieces of informationrelevant to the action that may be specified by the subject.For example, the most frequently used frame is the onecorresponding to a request o display some type of flightinformation.
Slots in the frame specify what information isto be displayed (flights, fares, times, airlines, etc), how it isto be tabulated (a list, a count, etc) and the constraints hatare to be used (date ranges, time ranges, price ranges, etc).The Phoenix system uses recursive Iransition networks tospecify word patterns (sequences of words) which cor-respond to semantic tokens understood by the system.
Asubset of tokens are considered as top-level tokens, whichmeans they can be recognized independently of surround-ing context.
Nets call other nets to produce a semanticparse tree.
The top-level tokens appear as slots in framestructures.
The frames erve to associate a set of semantictokens with a function.
Information is often representedredundantly in different nets.
Some nets represent morecomplex bindings between tokens, while others representsimple stand-alone values.
In our system, slots (patternspecifications) can be at different levels in a hierarchy.Higher level slots can contain the information specified inseveral ower level slots.
These higher level forms allowmore specific relations between the lower level slots to bespecified.
For example, from denver arriving in dallasafter two pm will have two parses,\[DEPART LOC\] from \[de part_loc\] \[city\] den-ver \[ARRIVE_LOC\] arnvmg In \[arrive loc\]\[ city \] dallas \[DEPART_TIME \]\[depart_time_range\] after \[start_time\]\[time\] twopmand\[DEPART LOC\] from \[depart loc\] \[city\] den-ver \[ARRIVE\] an/ving in \ [a r~ve loc\] \[city\]dallas \[ a r rive_t ime_range \] after~ s t a rt_t ime \]\[time\] twopmThe existence of the higher level slot \[ARRIVE\] allowsthis to be resolved.
It allows the two lower level nets\[arrive loc\] and \[arrive_time_range\] to bespecifically associated.
The second parse which has\[arrive loc\] and \[arrive time\] as subnets ofthe slot \[ARRIVE\] is the preferred-interpretation.
In pick-ing which interpretation is correct, higher level slots arepreferred to lower level ones because the associations be-78tween concepts is more tightly bound, thus the second(correct) interpretation is picked here.
The simple heuris-tic to select for the interpretation which has fewer slots(with the same number of words accounted for) allows thesituation to be resolved correctly.The parser operates by matching the word patterns fortokens against he input text.
A set of possible interpreta-tions are pursued simultaneously.
A subsumption algo-rithm is used to find the longest version of a phrase forefficiency purposes.
As tokens (phrases) are recognized,they are added to frames to which they apply.
The algo-rithm is basically a dynamic programming beam search.Many different frames, and several different versions of aframe, are pursued simultaneously.
The score for eachframe hypothesis i  the number of words that it accountsfor.
At the end of an utterance the parser picks the bestscoring frame as the result.The parse is flexible at the slot level in that it allows slotsto be filled independent of order.
It is not necessary torepresent all different orders in which the slot patternscould occur.
Grammatical restarts and repeats are handledby overwriting a slot if the same slot is subsequentlyrecognized again.The pattern matches are also flexible because of the waythe grammars are written.
The patterns for a semantictoken consist of mandatory words or tokens which arenecessary to the meaning of the token and optional ele-ments.
The patterns are also written to overgenerate inways that do not change the semantics.
This overgenera-tion not only makes the pattern matches more flexible butalso serves to make the networks maller.
For example,the nets are collapsed at points such that tense, number andcase restrictions are not enforced.
Articles A and AN aretreated identically.The slots in the best scoring frame are then used to buildobjects.
In this process, all dates, times, names, etc.
aremapped into a standard form for the routines that build thedatabase query.
The objects represent the information thatwas extracted from the utterance.
There is also a currentlyactive set of objects which represent constraints from pre-vious utterances.
The new objects created from the frameare merged with the current set of objects.
At this stepellipsis and anaphora re resolved.
Resolution of ellipsisand anaphora is relatively simple in this system.
The slotsin frames are semantic, thus we know the type of objectneeded for the resolution.
For ellipsis, we add the newobjects.
For anaphora, we simply have to check that anobject of that type already exists.Each frame has an associated function.
After the infor-mation is extracted and objects built, the frame function isexecuted.
This function takes the action appropriate forthe frame.
It builds a database query (if appropriate) fromobjects, sends it to SYBASE (the DataBase ManagementSystem we use) and displays output o the user.
This sys-tem has been described in previous papers.
\[1\] \[2\]2.1.
Natural Language Training DataThe frame structures and patterns for the Recursive Tran-sition Networks were developed by processing transcriptsof subjects performing scenarios of the ATIS task.
Thedata were gathered by several sites using Wizardparadigms.
This is a paradigm where the subjects are toldthat they are using a speech recognition system in the task,but an unseen experimenter is actually controlling theresponses to the subjects creen.
The data were submittedto NIST and released by them.
There have been three setsof training data released by NIST: ATIS0, ATIS1 andATIS2.
We used only data from these releases in develop-ing our system.
A subset of this data (approximately 5000utterances) has been annotated with reference answers.We have used only a subset of the ATIS2 data, includingall of the annotated ata.
The development test sets (forATIS0 and ATIS1) were not included in the training.2.2.
Natural Language Processing ResultsA set of 980 utterances comprised of 123 sessions from 37speakers was set aside as a test set.
Transcripts of theseutterances were processed by the systems to evaluate theperformance of the Natural Language Understandingmodules.
This will provide an upper bound on the perfor-mance of the Spoken Language Systems, i.e.
thisrepresents he performance given perfect recognition.
Theutterances for sessions provided ialog interaction with asystem, not just the processing of isolated utterances.
Allof the utterances were processed by the systems as dialogs.For result reporting purposes, the utterances were dividedinto three classes:?
Class A - utterances requiring no context forinterpretation?
Class D - utterances that can be interpretedonly in the context of previous utterances?
Class X - utterances that for one reason oranother were not considered answerable.Our results for processing the test set transcripts are shownin Table 1.
There were 402 utterances in Class A and 285utterances in Class D for a combined total of 687 ut-terances.
The remainder of the 980 utterances were ClassX and thus were not scored.
The database output of thesystem is scored.
The percent correct figure is the percentof the utterances for which the system returned the (ex-actly) correct output from the database.
The percent wrongis the percent of the utterances for which the systemreturned an answer from the database, but the answer wasnot correct.
The percent NO_ANS is the percentage of theutterances that the system did not attempt to answer.
TheWeighted Error measure is computed as (2 * %Wrong) +%NO_ANSWER.
These NL results (both percent correctand weighted error) were the best of any site reporting.79ClassA+DAD% Correct % Wrong % NOANS !
Weighted Error84.7 14.8 0.4 30.188.6 11.4 0.0 22.979.3 19.6 1.1 40.4Table 1: NL results from processing test set transcripts.2.3.
Comparison to February 1991 systemThe purpose of evaluations i not only to measure currentperformance, but also to measure progress over time.
Asimilar evaluation was conducted in February 1991.For Class A data, our percent correct performance in-creased from 80.7 to 88.6.
This means that the percentageof errors decreased from 19.3 to 11.4, representing adecrease in errors of 41 percent.
The weighted errordecreased from 36.0 to 22.9.For Class D data, our percent correct increased from 60.5to 79.3.
The represents a decrease in errors of 48percent.
The weighted error was reduced from 115.8 to40.4.The basic algorithms used are the same as for previousversions of the system.
The increase in performance ameprimarily from?
Bug fixes (primarily to the SQL generationcode)?
Extension of the semantics, grammar and lex-icon from processing part of the ATIS2 train-ing data.?
Improved context mechanism2.4.
Partial UnderstandingIn our system, we use the NO_ANSWER response dif-ferently than other sites.
If our results are compared toothers, we output far fewer NO_ANSWER responses.
Thisis because we use a different criteria for choosing not toanswer.
In order to optimize the weighted error measure,one would want to choose not to answer an utterance if thesystem believed that the input was not completely under-stood correctly, i.e.
if it thought hat the answer would notbe completely correct.
However, if the system chooses notto answer, it should ignore all information in the utterance.Since our goal is to build interactive spoken language un-derstanding systems, we prefer a strategy that shows theuser what is understood and engages in a clarificationdialog with the user to get missing information or correctmisunderstandings.
For this procedure we need to retainthe information that was understood from the utterance fordialog purposes.
The user must also be clearly shown whatwas understood.
Therefore, we only output aNO_ANSWER response when the system did not arrive ateven a partial understanding of the utterance.3.
SPEECH PROCESSINGFor our recognizer, we use the SPHINX-II speech recog-nition system.
In comparison with the SPHINX system, theSPHINX-II system incorporates multiple dynamic features(extended from three codebooks to four), a speaker-normalized front-end, sex-dependent semi-continuous hid-den Markov models (which replace discrete models), andthe shared-distribution representation (which replacesgeneralized between-word triphones).
\[3\] \[4\] For the Feb.1992 ATIS evaluation, we used SPmNX-II (without thespeaker normalization component) to constructvocabulary-independent models and adapted vocabulary-independent models with ATIS training data.
The systemused a backoff class bigram language model and a Viterbibeam search.3.1.
Acoustic TrainingIn order to efficiently share parameters across wordmodels, the SPHINX-II system uses shared-distributionmodels.
\[5\] The states in the phonetic HMMs are treatedas the basic unit for modeling and are referred to assenones.
\[4\] There were 6500 senones in the systems.Vocabulary-independent acoustic models were trained onapproximately 12,000 general English utterances.
Thesemodels were used to initialize vocabulary specific models(the vocabulary-independent mapping table was used)which were then trained on the task-specific data.
Ap-proximately 10,000 utterances from the ATIS0, ATIS 1 andATIS2 training sets were used in the adaptation training.The original vocabulary-independent models were then in-terpolated with the vocabulary-dependent models to givethe adapted models used in the recognition.3.2.
Lexicon and Language ModelA backoff class bigram grammar was trained on a total ofapproximately 12,000 utterances from the same three NISTATIS distributions.
The grammar used a lexicon of 1389words with 914 word classes defined.
The system usedseven models for non-speech events.80ClassA+D+X 88.2A+D 91.9A 92.8D 90.3X 78.9Correct Sub Deletions Insertions Error9.76.55.78.217.62.11.61.61.53A4.43.73.24.86.116.211.810.414.527.2Table 2: SPHINX-II Speech Recognition results.ClassA+DAD% Correct % Wrong % NOANS Weighted Error66.7 32.9 0.4 66.274.1 25.9 0.0 51.756.1 42.8 1.1 86.7Table 3: SLS results from processing test set speech input.3.3.
Speech Processing Results 4.
KNOWLEDGE BASED CORRECTIONThe Speech recognition results for the test set are shown inTable 2.
The Error column is the sum of Substitutions,Insertions and Deletions.
The output from the recognizerwas then sent to the NL system to get the complete SpokenLanguage System results.
These are shown in Table 3.3.4.
Comparison to February 1991 systemFor Class A data, our word error percentage was reducedfrom 28.7 to 10.4 representing a decrease in errors of 64percent.
The overall SLS error is a function of both thespeech recognition and natural anguage rrors.
Our per-centage of errors in SLS output decreased from 39 to 26representing a decrease in errors of 33 percent.
Theweighted error decreased from 65.5 to 51.7.For Class D data, our word error percentage was reducedfrom 26.9 to 14.5 representing a decrease in errors of 46percent.
Our percentage oferrors in SLS output decreasedfrom 61 to 44 representing a decrease in errors of 28percent.
The weighted error decreased from 116 to 87.The increase in speech recognition performance ame fromusing the SPHINX-II system where we used SPHINX in1991.
The primary differences are:?
Semi-continuous hared-distribution HMMsreplaced iscrete HMM generalized triphones?
Sex-dependent models were added?
Added second order difference cepstrumcodebookThe MINDS-II SLS system is a back-end module whichapplies constraints derived from syntax, semantics, prag-matics, and applicable discourse context and discoursestructure to detect and correct erroneous parses, skipped oroverlooked information and out of domain requests.MINDS-II transcript processor is composed of a dialogmodule, an utterance analyzer and a domain constraintsmodel.
Input to the CMU MINDS-II NL system is thetranscribed string, the parse produced by the PHOENIXcaseframe parser and the parse matrix.
The system firstlooks for out of domain requests by looking for otherwisereasonable domain objects and relations among objects notincluded in this application database.
Second, it tries todetect and correct all misparses by searching for alternateinterpretations of both strings and relations among iden-tified domain concepts.
Further unanswerable queries aredetected in this phase, although the system cannot deter-mine whether the queries are unanswerable because thespeaker mis-spoke or intentionally requested extra-domaininformation.
Third, the system evaluates all word stringsnot contained in the parsed representation to assess theirpotential importance and attempt to account for the infor-mation.
Unaccounted for information detected includesinterjections, regions with inadequate grammaticalcoverage and regions where the parser does not have theknowledge to include the information in the overall ut-terance interpretation.
All regions containing interjectionsor on-line edits and corrections are deemed unimportantand passed over.
When the system finds utterances withimportant unaccounted for information, it searches throughthe parse matrix to find all matches performed in theregion.
It then applies abductive reasoning and constraintsatisfaction techniques to form a new interpretation f theutterance.
Semantic and pragmatic knowledge isrepresented with multi-layered hierarchies of frames.
Eachknowledge layer contains multiple hierarchies and rela-tions to other layers.
Semantic information of similargranularity is represented in a single layer.
The knowledge81System Class % Correct % Wrong % NO_ANS Weighted ErrorPhoenix A + D 66.7 32.9 0.4 66.2MINDS-II A + D 64.3 25.3 10.3 61.0Table 4: UNOFFICIAL Comparison on MINDS-II and Phoenix results from processing test set speech input.base contains knowledge of objects, attributes, values, ac-tions, events, complex events, plans and goals.
Syntacticknowledge is represented as a set of rules.
The discoursemodel makes use of current focus stack, inferred speakergoals and plans, and dialog principles which constrain"what can come next" in a variety of contexts.
Goal andplan inference and tracking are performed.
Constraints arederived by first applying syntactic constraints, constrainingtheses by utterance level semantic and pragmatic on-straints followed by discourse level constraints when ap-plicable.
The system outputs either semantically inter?preted utterances represented as variables and bindings forthe database interface or error codes for "No_Anwser"items.The system was trained using 115 dialogs, approximately1000 of the utterances from the MADCOW ATIS-2 train-ing.
Previously, the system had been trained on theATIS-0 training set.
This system incorporates the SOULutterance analysis ystem as well as a dialog module forthe Feb92 benchmark tests.4.1.
Knowledge Based Processing ResultsDue to mechanical problems, the results from this test weresubmitted to NIST after the deadline for official submis-sions.
Therefore, they were not scored by NIST and are notofficial benchmark results.
However, the results weregenerated observing all procedures for benchmark tests.They were run on the official test set, without looking atthe data first.
One version control bug was fixed when thesystem crashed while running the test.
No code waschanged, we realized that the wrong version (an obsoleteone) of one function was used, and we substituted the cor-rect one.
The results were scored using the most recentcomparator software released by NIST and the officialanswers (after adjudication).5.
ENVIRONMENTAL ROBUSTNESSThis year we incorporated the Code-Word DependentCepstral Normalization (CDCN) procedure developed byAcero into the ATIS system.
For the official ATIS evalua-tions we used the original version of this algorithm, asdescribed in \[6\].
(Recent progress on this and similar al-gorithms for acoustical pre-processing of speech signalsare described in elsewhere in these proceedings \[7\].
)The recognition system used for the robust speech evalua-tion was identical to that with which the baseline resultswere obtained except hat the CDCN algorithm was usedto transform the cepstral coefficients in the test data so thatSystem Microphone % ErrorSPHINX-II HMD-414 13.9SPI-IINX-II+CDCN HMD-414 16.6SPI-IINX-II+CDCN PCC- 160 21.7Table 5: Comparison of speech recognition performanceof SPHINX-II with and without he CDCN algorithm on the447 A+D+X sentences in the test set which were recordedusing the PCC-160 microphone as well as the SennheiserHMD-414.they would most closely approximate the statistics of theensemble of cepstra observed in the training environment.All incoming speech was processed with the CDCN algo-rithm, regardless of whether the testing environment wasactually the standard Sennheiser close-talking microphoneor the desktop Crown PCC-160 microphone, and the algo-rithm does not have explicit knowledge of the identity ofthe environment within which it is operating.Because of time constraints, we did not train the systemused for the official robust-speech evaluations asthoroughly as the baseline system was trained.
Specifi-cally, the robust-speech system was trained on only 10,000sentences from the ATIS domain, while the baseline sys-tem was trained on an additional 12,000 general Englishutterances as well.
The acoustic models for the robust-speech system using CDCN were created by initializingthe HMM training process with the models used in thebaseline SPmNX-II system.
The official evaluations wereperformed after only a single iteration through trainingdata that was processed with the CDCN algorithm.The official speech recognition scores using the CDCNalgorithm and the Sennheiser HMD-414 and CrownPCC-160 microphones are summarized in Table 4.
Wesummarize the word error scores for all 447 utterances thatwere recorded using both the Sennheiser HMD-414 andCrown PCC-160 microphones.
For comparison purposes,we include figures for the baseline system on this subset ofutterances, as well as figures for the system using theCDCN algorithm for the same sentences.
We believe thatthe degradation i performance from 13.9% to 16.6% forthese sentences using the close-talking SennheiserHMD-414 microphone is at least in part a consequence ofthe more limited training of the system with the CDCNalgorithm.
We note that he change from the HMD-414 tothe PCC-160 produces only a 30% degradation i  errorrate.
Only two sites submitted ata for the present robustspeech evaluation, and CMU's percentage degradation ierror rate in changing to the new testing environment, as82System l Microphone % Correct % Wrong % NO_ANS Weighted ErrorlSPHINX-II+CDCN HMD-414 69.0 31.0 0.0 62.0SPHINX-II+CDCN PCC- 160 56.6 43.1 0.3 86.4Table 6: Comparison of SLS performance of SPHINX-II with the CDCN algorithm on the 332 A+D sentences in the test setwhich were recorded using the PCC-160 microphone as well as the Sennheiser HMD-414.well as the absolute rror rate in that environment, werethe better of the results from these two sites.2.Summary results for the corresponding SLS scores for the332 Class A+D utterances that were recorded using the 3.Crown PCC-160 microphone are provided in Table 6.Switching the testing environment from the SennheiserHMD-414 to the Crown PCC-160 degraded the number of 4. correct SQL queries by only 21.8%, which corresponds toa degradation of 39.3% for the weighted error score.
CMUwas the only site to submit SLS data using the PCC-160 5.microphone for the official evaluation.REFERENCES 6.Ward, W., "The CMU Air Travel Information Service:Understanding Spontaneous Speech", Proceedings oftheDARPA Speech and Natural Language Workshop,June1990, pp.
127, 129.1.7.Ward, W., "Evaluation of the CMU ATIS System",Proceedings ofthe DARPA Speech and Natural LanguageWorkshop, Feb1991, pp.
101, 105.Huang, Lee, Hon, and Hwang,, "Improved AcousticModeling for the SPHINX Speech Recognition System",ICASSP, 1991, pp.
345-348.Hwang and Huang, "Subphonetic Modeling withMarkov States - Senone", ICASSP, 1992.Hwang, M. and Huang X., "Acoustic Classification ofPhonetic Hidden Markov Models", EurospeechProceedings, 1991.Acero, A. and Stem, R. M., "Environmental Robusmessin Automatic Speech Recognition", ICASSP-90, April1990, pp.
849-852.Stern, R. M., Liu, F.-H., Ohshima, Y., Sullivan, T. M.,and Acero, A., "Multiple Approaches to Robust SpeechRecognition", DARPA Speech and Natural LanguageWorkshop, February 1992.83
