Preposit ional Phrase Attachment Through A HybridDisambiguation ModelHaodong Wu and Te i j i  Furugor iDepar tment  of  Computer  Sc ienceUn ivers i ty  of E lec t ro -Communicat ions1-5-1, Chofugaoka ,Chofu ,  Tokyo  182, , JAPAN{wu, fu rugor i}Ophaeton ,  cs .
uec .
ac .
j pAbstractPrepositional phrase attachment is a ma-jor cause of stru(:tural alnbiguity in nat-ural language.
Recent work has beendependent on corpus-based approachesto deal with this problem.
However,corpus-based approaches suffer from thesparse-data problem.
To cope with thisproblem, we introduce a hybrid methodof integrating corpus-based approachwith knowledge-based techniques, usinga wide-variety of information that comesfrom annotated corpora and a machine-readable dictionary.
When the occur-rence frequency on the corpora is low, weuse preference rules to determine PP at-tachment based on clues from conceptualinformation.
An experiment has proventhat our hybrid method is both effectiveand applicable in practice.1 IntroductionThe resolution of prepositional phrase attachmentambiguity is a difficult problem in NLP.
Therehave been many proposals to attack this prob-lem.
Traditional proposMs are mainly based onknowledge-based techniques which heavily dependon empirical knowledge encoded in handcraftedrules and domain knowledge in knowledge base:they are therefore not scalable.
Recent work hasturned to corpus-based or statistical approaches(e.g.
Hindle and Rooth 1993; Ratnaparkhi, Rey-nar and Roukos 1994, Brill and Resnik 1994,Collins and Brooks 1995).
Unlike traditional pro-posals, corpus-based approaches need not to pre-pare a large amount of handcrafted rules, theyhave therefore the merit of being scalable or easyto transfer to new domains.
However, corpus-based approaches shffer fi'om the notorious parse-data problem: estimations based on low occur-renee frequencies are very unreliable and often re-sult in bad performances in disambiguation.
Tocope with this problem, Brill and Resnik (1994)use word classes from Word-Net noun hierarchyto (:luster words into semantic lasses.
Collins andBrooks (1995) on the other hand use morpholog-ical analysis t)oth on test and tr~fining data.
Un-fortunately, all these smoothing methods are notefficient enough to make a significant improvementon perforlnancc.Instead of using pure statistical approachesstated above, wc propose a hybrid approach to at-tack PP attachment problem.
We employ corpus-based likelihood analysis to choose most-likely at-tachment.
Where the occurrence frequency is toolow to make a reliable choice, wc turn to use con-ceptual infornlation froln a machine-readable dic-tionary to to make decision on PP attachments.We use this disambiguation method to buihl a dis-ambiguation module in PFTE  system, lIn what follows we first outline the idea of us-ing hybrid information to sui)ply preferences forresolving ambiguous PP attachment.
We thendescribe how this information is used in disam-biguating PP attachment.
We put the hybrid ap-proach in an disambiguation algorithm.
Finally,we show an experiment and its result.2 Using Multiple Information inDisambiguationLike other work, we use fonr head words to makedecision on PP attachment: the main verb v, thehead noun (nl) ahead of the preposition (p), andthe head noun (n2) of the object of the preposi-tion.
In the later discussion, the four head wordsare referred to as a quadrul)le (v nl p n2).Analyzing the strategies human beings employin PP attachment disambiguation, we f(mnd thata wide-variety of information supplies importantclues for disambiguation.
It includes presupposi-tions, syntactic and lexical cues, collocations, syn-tactic and semantic restrictions, features of headwords, conceptual relationships, and world knowl-edge.
We use clues that are general and reliable1PFTE stands for Parser for Free Text of English.PFTE system is a versatile parsing system in develop-ment which (:overs a wide range of phenomena in lexi-cal, syntactic, semantic dimensions.
It is designed as alinguistic tool for at)plications in text understanding,database generation fi'om text and computer-basedlanguage learning.1070so that they make the computat ion efficient andextensible.
The information or clues we use arethe following:1.
Syntactic or lexical cues.
If n l  is same as n2,for exalnple, often n lTPP  is a fixed t)hr;~sesu(:h as .step by step.2.
Co-oee'wr'rences.
The (;o-o(:(:llrrences of tri-ples and pairs in (v n l  p n2) colne frmn an-notated eorl)ora (Se(:tion 4).3.
Syntactic and semantic features.
Features ofv or nl  n2 sometimes in(licate the "corre(:t"attachment.
For examt)le,if v is a movement,p is to and n2 is a t)lace or direction, the PPteuds to be attached to the verb.4.
Conceptual relationships 1)etween v and n2,or between nl  and n2.
These relationships,which reflect the role-expections of the pre-1)osition, sut)l)ly important chics for disambi-guation.
For example, in the sentence Peterbroke the window by a ,stone, we are sure thatthe PP  by a stone is att~u'hed to broke/v byknowing that stone~n2 is an instrument forbroke/v.V~fe use co-occurrence informatioi~ in corl)us-t)ased (lis;mfl)iguation and other information inrule-b,~sed isambiguation.
Later, we will discusshow to ac(tuire above information and use it indisambiguation.3 Est imat ion  based  on CorporaIn this section, we consider two kinds of PP  at-tachment in our corlms-t)ased al)l)roaeh , nalnely,attachment to verb phrase (VP atta('lmmnt) andto nmm i)hrase (NP attachment).
Here, we usetwo ammtated corpora: EDR English Corpus 2and Susanne Corpus a to SUpl)ly training data.Both of theln (-(retain tagged syntactic structurefor each sentence in thein.
That is, each PP in thecorl)ora has 1)een attached to an unique l)hrase.RA(v,nl ,p,n2),  a score fi'om 0 to 1, ix definedas a value of counts of VP attachments dividedby the total of occurrences of (v,nl,1),n2) in thetraining data.
4RA(v,nl ,p,n2) = f(,,vl,,,,,I,v,,ce) f (v ,n l  ,p,n2),~ f ( vPlv ,,~ L ,p,u2) - /(,,vl ..... I,v,-2)+y(,wl,,,,,1,v,,,u) (1)In (1), the symbol f denotes frequency of a par-ti('ular tuple in the training data.
For exami)le,2FDR English Corpus, conq)iled by Japan Ehx'-tronic Dictionary Research Institute, Ltd, eontalllS160,000 sentences with annotated nmrphologie, syn-tactic m,d semantic information.aSusaxme Corpus,cOral)ileal \])y Oeoffre.y Saml~so:n ,is an amtotated corpus coml)risiltg about 130,000words of written American English text.
'lWe assulue that only two kinds of PP atta(:h-merits: VP or NP attachment in the training data.f(vl) I share,apartment,with,friend) is the numl)er-of~.imes the quadruple (share, apartlnent, with,friend) is seelt with a VP attachment.
Thus,we could choose a attaefiment actor(ling to RAscore= if RA>0.5 choose VP attachment,  other-wise choose NP attachment.Most of quadruples in test data are not in thetraining data, however.
We thus turn to collecttriples of (vd),nl),(nl,p,n2),(v,nl, l)) and 1)airs of(v,t)),(nl,p),(l),n2) like Collins and Brooks (1995)did, and coinpute RA score by (2) and (3).RA(v,nl ,p,n2) =f(vVl,,p,n2)+f(,~p\[,~l .p,n2)+f(op\[,,,ul 4')f(v,p,n2)T f(nl ,p,n2)+ f(  v,n,p) (2)or,RA(v,nl,1),n2 ) =f ( vvl v ,p) + f ( ,:vl,~ ~ ,P )+ f ( "ph),n 2 )f (v,p)T f (n l  ,p)+ f(p,n2) (3)To avoi(l using very low frequen(:ies, we settwo thr('sholds for each one above.
For triple-combimttion, the c(mdition is:fl, riple(v,Itl,1),n2 ) ~ 2, and12*RA(v,nl,p,n2)-ll * h)g(ftrip>(v,nl,p,n2)) < 0.5here,flriple(v,Ill,p,n2) = f(v,1),n2)+f(nl,t),n2)+f(v,nl,1) )For 1)airs-(:oml)ination, the condition is:fpair(v,nl,p,n2) > 4, and12*RA(v,nl,p,n2)-I I * log(fp,,i,.
(v,ul,p,n2)) < 0.5here,fpair(v,nl,1),n2 ) = f(v,p)+f(nl,p)+f(p,n2)With  the first threshohl  in ca,oh case, we canavoid using low frequency tul)les; with the secondone in each case, we throw away the RA scorewhich is close to 0.5 ~Ls tlfis wdue is rather unsta-bh,.4 Conceptua l  In fo rmat ion  andPre ference  Ru lesAs we use only "relial)le" data from corl)ora tomake decision on PP atta('hlnellt based (m RAscore, many PPs '  attachlnents may be left unde-termined due to sparse.
(l;tta.
We deal these unde-te.rlnined PPs with a rule-based approach.
Herewe use preference rules to determine PP attach-ments 1)y judging features of head words and con-ceptual relationships among them.
Tl,is informa-tion comes from a machine-readable dictionaryEDR dictionary, sSEDII electronic dietionm'y consists of a setof machine-readable dictionaries which includesJapanese and English word dictionary, Japanese andEnglish co-occurrence dictionary, concept dictionary,and Jal)anese < > English l)ilingual dletio-nary(EI)R 1993).10714.1 Features  and  Concept  C lassesWe cluster words (verbs or nouns) ~hi~h haves~une feature or syntactical function into a (:()n-cel)t class.
For examI)le, we classify verbs intoactive and passive, and ontologicM cbusses of men-tal, movement, etc.
Similarly, we group nouns intoplace, time, state, direction, etc.We extract eoncel)t (:lass from concept classifi-cation in EDR Concept Dictionary} ~4.2 Conceptua l  Re la t ionsh ipConceptual relationships between v and n2, or be-tween nl  and n2 predict PP attaehnlent quite wellin many eases.
We use EDR concept dictionary toacquire the concel)tual relationship between twoconcet)ts.
For examt)le, given the two concet)ts ofopen and key, the dictionary will tell us that theremay be a implement relationship 1)etween them,means that key may be act its an instrument forthe action open.4.3 P re ference  Ru lesWe introduce 1)reference rules to encode syntacticand lexical clues, as well a~s clues from conceptualinformation to determine PP attachments.
Wedivide these rules into two categories: a rule whi('tl(:nit be applied to most of 1)rel)ositions is cMledglobal rule; a rule tying to a particular prel)osition,on the other hand, is called local rule.
Four globalrules used in our disambiguatioi: module are listedin Table 1.1. lexical(passivized(v) + PP) ANDprep ?
'by' - > vp_attach(PP)2. nl : n2 - > vi)_attaeh(nl + PP)3.
(prep # 'of' AND prep # 'for') AND(time(n2) OIl date(n2)) - > Vl)_attaeh(PP)4. lexicM(Adjeetive + PP) - > adjp_attach(PP)Table 1: Global rulesLocal rules use (:oncel)tual inforlnation to deter-mine PP  attachlnent.
In Table 2, we show sampleh)cal rules for preposition with.with-rules:iml)lement(v, :)2) - > Vl)_~tttach(Pl ))(a-ol)jeet(nl, n2) ()R possessor(nl, 112))AND NOT(implen:ent(v, n2)) - >np_~tttach(PP)Default - > vi)_attach(PP)Table 2: Sample local rulesOn the left hand of each rule, a one-atonl pre(t-Concet ) t  Dictionm'y consists of al)out 400,000 con:cepts, where, fbr eolleet)t classification, related con-eepts are orgmfized in hierm'chieM ar('hitecture and aconcept in h)wer level inherits the f~atures from itsupper level concepts.icate Oil the left hand presents tt subclass of con-cept ill the eon(:ept hierarchy (e.g.
tilne(n2)), anda two-atom 1)redicate describes the COlWei)t rela-tion between two at(nns (e.g.
implennult(v,n2)).Since local rules emph)y the senses of headwords (termed as concepts), we shouhl 1)rojecteach of v, ul  and n2 used by rules into one orsevera l  co i | cepts  which denote(s) "correct" wordsenses before apl)lying local rules.
The process isdescribed in (Wu and Furugori 1995).5 Disambiguation ModuleFor each sentence with aml)igu<)us PP (both illsyntaeti(' and semantie;d level), PETE  system willprodu<'e ;t structure with unattached PP(s),  andcall the disambiguation 1nodule to resolve ambigu-ous PP(s).
The algorithm used in the nn)(hfle isshown beh)w :\ [ALGORITHM\]Phase 1.
(disambiguation using gh)bal rules):Try global rules one 1)y one.
If a rule succeeds, useit to decide the attachment, and exit.Phase 2.
(statistiesd)ased dismnbiguation):RA(v,nld),n2 ) = -1 (initial value)f t r ip le(v,nl ,1) , l t2 ) : f (v ,p , : t2 )+f (n l ,1 ) ,n2)T f (v ,n l ,p )fpa i r (v , I l l , t ) ,n2 ) = f (v ,1 ) )+f (n l ,1 ) )+f (p , l l2  )if  f t r iph , (v ,n l , i ) ,n2  ) > 2, thenll A (vd i l ,1 ) ,n2)  = .f(vvl~,,r,.2)+.r( ~,~,1,,  ,v,-~)q.f(,,v\[ ,,,~, :' a,) f( v,v,n 2)-b f(~ t ,p,~ 2).4- f( v ,.. 4>)if \[2*RA(v,n 1,1),n2 )-11 *log(fi.riple( v,n1,1),n2 ) )<0.5then RA(v,nl,p,n2) =-1if RA(v,nl,p,n2)<0 and fpa i r (v ,n l , l ) , l~2)>4, thenI1A(v,nl,I),n2) = f(.vplv,p)+ f(vp\[,, 1 p)T f(,,plp,,,2}f(v,p) + f(  n I,p)+ f(p,7~ 2)if \[2*RA(v,nl,p,n2)-I I * log( fpa i r (v , l l l ,p ,n2) )<0.5then RA(v,nl,i),n2) = -1if I{A(v,nld),n2 ) > 0, then {if RA(v,nl,1),n2)<0.5, then choose NP attachmentotherwis(, choose VP attachmentexit.
}Phase 3.
(concept-based disalnl)iguation):1) Project each of v, nl, n2 into its COIteel)t sets.2) Try the rules related to the prel)osition , if onlyone rule is applicable, use it to decide the attach-ment, and then exit.Phase 4.
(attachment 1)y default):if f(p) > 0, then {if ~ < 0.5, then choose NP attachment; f(~,)otherwise choose VP attachment}otherwise choose NP attachment.This algorithm differs from the previous one de.-scribed ill (Wu and Furugori 1995) in which prefer-ence rules were applied 1)efol'e statistical comput-ing.
We have changed the order for the followingreasons: an experinlent has proven that using the\].072data of qua(lrul)les and triples, as well as tut)leswith high occurrences i,s good enough in successrate (Sec Tal)lc 3).
and statistic models ha,ve aground m~themlttical 1)asis.6 Exper iment  and Evaluat ionWe did an exl~eriment to test our lnethod.
First,we prcl)are(l test data of 3043 ambiguous PPs intexts randomly taken from a (:Olnl)uter manual, a.graalllnlar book and Japan Time.s.Phase~lobal rules_lriplcs_Dairslocal ridesTotal Number5075641093662Number  Correcl487518931557Stlccess rate,96.
l%91.8%85.3%84.1%others 2 l 7 151 69.6%Total 3043 2644 86.9%'Fable 3: Results of  the test in PP attachmentThe results are shown ill Table 3.
We success-fully disalnbiguated 86.9% of th(, test data.
Toreduce sl)ars(' data.
1)roblenl and deal wilh unde-fined wor(ls in the dictiolm.ry, we use a l)roc(!duresimihtr to th;tt of Collins and Brook 11995) to pro-(:ess head words both in training data and in testdatm Tile 1)ro(:c(lure is shown as follows:?
All 4-digit lmmbers itre tel)laced with 'date'.
* All verbs are rel)l~u:ed with their stems ill low-or  (:as(~S.e Nouns starting with it calfital letter are re-placed with 'lmme'.?
Personal 1)ronouns in the n2 field are r(,lfiacedwith 'perso\]t'.As the result, we a(:quired all ac(:urate rate of87.5% (TM)le 4), an improvemellt of 0.6% on the1)r('.violls OlI(LPhase Total Number Number Correct Success rote~global rules 507 487 96.1%l l~es  659 601 90.9%_Amirs 1134 965 84,9%local rules 628 527 83.9%others 115 81 70.4%Total 3043 2661 87.5%Table 4: Restflts with processing head wordsThe result is rather good, COlnt)aral)h' to thel)erformance of all "averag(' ."
hlIl\[la, ll looking at(v,nl,p,n2) alone (al)out 85% to 90% according toHindle ~md R ooth 1993, Collins and Brooks 1995).We attr ibute this result to the hyl)rid apl)roach weused, in which preferences with higher rdiabilitiesare used 1)rior to other on('s in the disalnl)iguationl)rocess.
We found that two thresholds are veryhell)ful in iml/roving the result.
If we set the firstthreshohl as 0 ~md throw away the second thresh-old, then l.he success rates ill tril)le-('onfl)inationwill \])(K:(,llt(' 89.1% (-1.8%), a,nd 81.2% (-3.7%) inl)aJr-(:ombilmtion.
Moreover, using h)('al rules totackle unattached PPs by statistical model is alsohellfful in improving the overall su('cess rat(, sinceloom rules in l)hase 3 work nmch b(,tter than de-fault (h,(:ision in Phase 4.7 Conclus ionPure statisticM models for disalnl)iguation tltsksSll~'('l' fl'Olll sparse-data 1)robh'nL ~V(' ltot('(l thateven when ai)plying smooth t('chniques such as se-nuultic sinfilarity or (:lustering, it is hard to avoidmalting poor est;ilnat.iol~s Oil low OCCltrr(ulces illcorpora.
()nqine dictionaries wlfich contain richsemantic or concel)tual information ml W be of helpin improving the perforlnan('e.
()ur exl)erimcntshows tha, t the hybrid al)proach we taken is botheffectiv(, and a.1)l)li('able ill practice.ReferencesBrill, E. and l{('snik, P. 1994.
A rul('-l)i~sed al)-pro~Lch to 1)relmsitional phrlme at;t~tchnwnt dis-ambigua.tion.
In Proc.
of th, e, 15th, Coling,1198 1204.Collins, M. and Brooks, J.
1995. l)rel/ositionalllhrase attachment through a backed-off lnodel.h ttl,: / /xxx.lanl.gov / c,ni,-lg/ 9506021.DMfigr('n, K. and McDowell, a.
1986.
Using con>monsense knowledge /o (lisaml)iguate l/reposi -tionM t)hrase modifiers.
In Pr'oc.
of the 5th,AAAI, 589-593.Japan Ehwtronic Dictionary Research institute,Ltd.
1993.
ED\] I  electronic dictionary sl)eciti-cations guide.Jensen, K. and Binot, J.
1987.
1)isambiguatingprepositional phrase attachments by using on-line dictionary definition.
In ComputationalLinyuistica.
1313-4) : 251-260.Hindlc, D. and Rooth, M. 1993.
Structural mn-I)iguity mM lexical rel~tions.
In ComputationalLinguistics, 1911): 103-120.Luk, A. K. 1995.
Statistical sense disand)iguationwith relatively sInall corllora using dictionarydelinitiol~S.
In Proc.
of the ,'\](h'd A CL Meeting,181-188.Whittelnore, G.; Ferrara, K.; and l~runner, H.1990.
Empirical study of predictive powers ofsiml)le attachlnent sdtelll('s for t)ost-modiliersprepositionM phrases.
In Proc.
of th, c 28thA CL Meeting, 23-30.Wu, H., Takeshi, 1. and Furugori, T. 1995.
A pre-ferential al)proach for disambigmtting ilrelmsi -.tional phrase modifiers.
In Proc.
of the 3th.
Na-tural La'ng'u, age Processing I)acific Rim Sympo-,siUm, 745-751.1073
