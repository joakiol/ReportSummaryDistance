Modern Natural Language Interfaces to Databases:Composing Statistical Parsing with Semantic TractabilityAna-Maria Popescu Alex Armanasu Oren EtzioniUniversity of Washington{amp, alexarm, etzioni, daveko, ayates}@cs.washington.eduDavid Ko Alexander YatesAbstractNatural Language Interfaces to Databases(NLIs) can benefit from the advances in statis-tical parsing over the last fifteen years or so.However, statistical parsers require training ona massive, labeled corpus, and manually cre-ating such a corpus for each database is pro-hibitively expensive.
To address this quandary,this paper reports on the PRECISE NLI, whichuses a statistical parser as a ?plug in?.
The pa-per shows how a strong semantic model cou-pled with ?light re-training?
enables PRECISEto overcome parser errors, and correctly mapfrom parsed questions to the correspondingSQL queries.
We discuss the issues in usingstatistical parsers to build database-independentNLIs, and report on experimental results withthe benchmark ATIS data set where PRECISEachieves 94% accuracy.1 Introduction and MotivationOver the last fifteen years or so, much of the NLPcommunity has focused on the use of statisticaland machine learning techniques to solve a widerange of problems in parsing, machine translation,and more.
Yet, classical problems such as buildingNatural Language Interfaces to Databases (NLIs)(Grosz et al, 1987) are far from solved.There are many reasons for the limited success ofpast NLI efforts (Androutsopoulos et al, 1995).
Wehighlight several problems that are remedied by ourapproach.
First, manually authoring and tuning a se-mantic grammar for each new database is brittle andprohibitively expensive.
In response, we have im-plemented a ?transportable?
NLI that aims to mini-mize manual, database-specific configuration.
Sec-ond, NLI systems built in the 70s and 80s had lim-ited syntactic parsing capabilities.
Thus, we have anopportunity to incorporate the important advancesmade by statistical parsers over the last two decadesin an NLI.However, attempting to use a statistical parser ina database-independent NLI leads to a quandary.
Onthe one hand, to parse questions posed to a particu-lar database, the parser has to be trained on a corpusof questions specific to that database.
Otherwise,many of the parser?s decisions will be incorrect.
Forexample, the Charniak parser (trained on the 40,000sentences in the WSJ portion of the Penn Treebank)treats ?list?
as a noun, but in the context of the ATISdatabase it is a verb.1 On the other hand, manuallycreating and labeling a massive corpus of questionsfor each database is prohibitively expensive.We consider two methods of resolving thisquandary and assess their performance individuallyand in concert on the ATIS data set.
First, we usea strong semantic model to correct parsing errors.We introduce a theoretical framework for discrim-inating between Semantically Tractable (ST) ques-tions and difficult ones, and we show that ST ques-tions are prevalent in the well-studied ATIS dataset (Price, 1990).
Thus, we show that the seman-tic component of the NLI task can be surprisinglyeasy and can be used to compensate for syntacticparsing errors.
Second, we re-train the parser usinga relatively small set of 150 questions, where eachword is labeled by its part-of-speech tag.To demonstrate how these methods work in prac-tice, we sketch the fully-implemented PRECISENLI, where a parser is a modular ?plug in?.
Thismodularity enables PRECISE to leverage continuingadvances in parsing technology over time by plug-ging in improved parsers as they become available.The remainder of this paper is organized as fol-lows.
We describe PRECISE in Section 2, sketch ourtheory in Section 3, and report on our experimentsin Section 4.
We consider related work in Section 5,and conclude in Section 6.2 The PRECISE System OverviewOur recent paper (Popescu et al, 2003) introducedthe PRECISE architecture and its core algorithm for1This is an instance of a well known machine learning prin-ciple ?
typically, a learning algorithm is effective when its testexamples are drawn from roughly the same distribution as itstraining examples.reducing semantic interpretation to a graph match-ing problem that is solved by MaxFlow.
In this sec-tion we provide a brief overview of PRECISE, focus-ing on the components necessary to understandingits performance on the ATIS data set in Section 4.To discuss PRECISE further, we must first intro-duce some terminology.
We say that a database ismade up of three types of elements: relations, at-tributes and values.
Each element is unique: an at-tribute element is a particular column in a particularrelation and each value element is the value of aparticular attribute.
A value is compatible with itsattribute and also with the relation containing thisattribute.
An attribute is compatible with its rela-tion.
Each attribute in the database has associatedwith it a special value, which we call a wh-value,that corresponds to a wh-word (what, where, etc.
).We define a lexicon as a tuple (T, E, M), whereT is a set of strings, called tokens (intuitively, tokensare strings of one or more words, like ?New York?
);E is a set of database elements, wh-values, and joinpaths; 2 and M is a subset of T ?
E ?
a binaryrelation between tokens and database elements.PRECISE takes as input a lexicon and a parser.Then, given an English question, PRECISE maps itto one (or more) corresponding SQL queries.
Weconcisely review how PRECISE works through asimple example.
Consider the following questionq: ?What are the flights from Boston to Chicago?
?First, the parser plug-in automatically derives adependency analysis for q from q?s parse tree,represented by the following compact syntactic log-ical form: LF (q) = what(0), is(0, 1), f light(1),from(1, 2), boston(2), to(1, 3), chicago(3).LF (q) contains a predicate for each question word.Head nouns correspond to unary predicates whosearguments are constant identifiers.Dependencies are encoded by equality con-straints between arguments to different predicates.The first type of dependency is represented by nounand adjective pre-modifiers corresponding to unarypredicates whose arguments are the identifiers forthe respective modified head nouns.
A second typeof dependency is represented by noun postmodifiersand mediated by prepositions (in the above exam-ple, ?from?
and ?to?).
The prepositions correspondto binary predicates whose arguments specify the at-tached noun phrases.
For instance, ?from?
attaches?flight?
to ?boston?.
Finally, subject/predicate,predicate/direct object and predicate/indirect objectdependency information is computed for the various2A join path is a set of equality constraints between the at-tributes of two or more tables.
See Section 3 for more detailsand a formal definition.verbs present in the question.
Verbs correspond tobinary or tertiary predicates whose arguments indi-cate what noun phrases play the subject and objectroles.
In our example, the verb ?is?
mediates thedependency between ?what?
and ?flight?.
3PRECISE?s lexicon is generated by automaticallyextracting value, attribute, and relation names fromthe database.
We manually augmented the lexiconwith relevant synonyms, prepositions, etc..The tokenizer produces a single completetokenization of this question and lemmatizesthe tokens: (what, is, flight, from,boston, to, chicago).
By looking up thetokens in the lexicon, PRECISE efficiently retrievesthe set of potentially matching database elementsfor every token.
In this case, what, boston andchicago are value tokens, to and from are at-tribute tokens and flight is a relation token.In addition to this information, the lexicon alsocontains a set of restrictions for tokens that areprepositions or verbs.
The restrictions specify thedatabase elements that are allowed to match to thearguments of the respective preposition or verb.
Forexample, from can take as arguments a flight anda city.
The restrictions also specify the join pathsconnecting these relations/attributes.
The syntacticlogical form is used to retrieve the relevant set ofrestrictions for a given question.The matcher takes as input the information de-scribed above and reduces the problem of satisfy-ing the semantic constraints imposed by the defi-nition of a valid interpretation to a graph matchingproblem (Popescu et al, 2003).
In order for eachattribute token to match a value token, Bostonand Chicago map to the respective values of thedatabase attribute city.cityName, from maps toflight.fromAirport or fare.fromAirport and tomaps to flight.toAirport or fare.toAirport.
Therestrictions validate the output of the matcher andare then used in combination with the syntactic in-formation to narrow down even further the possi-ble interpretations for each token by enforcing lo-cal dependencies.
For example, the syntactic in-formation tells us that ?from?
refers to ?flight?
andsince ?flight?
uniquely maps to flight, this meansthat from will map to flight.fromAirport ratherthan fare.fromAirport (similarly, to maps toflight.toAirport and whatmaps to flight.flightId).Finally, the matcher compiles a list of all relationssatisfying all the clauses in the syntactic logicalform using each constant and narrows down the set3PRECISE uses a larger set of constraints on dependencyrelations, but for brevity, we focus on those relevant to our ex-amples.of possible interpretations for each token accord-ingly.
Each set of (constant, corresponding databaseelement) pairs represents a semantic logical form.The query generator takes each semantic logicalform and uses the join path information available inthe restrictions to form the final SQL queries corre-sponding to each semantic interpretation.pronounverbnounprepnounprepnounprepnounNPNPNPPPPPPPNPNPNPVPSNPWhatareflightsfromBostontoChicagoonMonday?Figure 1: Example of an erroneous parse tree correctedby PRECISE?s semantic over-rides.
PRECISE detects that theparser attached the PP ?on Monday?
to ?Chicago?
in error.PRECISE attempts to re-attach ?on Monday?
first to the PP?to Chicago?, and then to the NP ?flights from Boston toChicago?, where it belongs.2.1 Parser EnhancementsWe used the Charniak parser (Charniak, 2000) forthe experiments reported in this paper.
We foundthat the Charniak parser, which was trained onthe WSJ corpus, yielded numerous syntactic errors.Our first step was to hand tag a set of 150 questionswith Part Of Speech (POS) tags, and re-train theparser?s POS tagger.
As a result, the probabilitiesassociated with certain tags changed dramatically.For example, initially, ?list?
was consistently taggedas a noun, but after re-training it was consistently la-beled as a verb.
This change occurs because, in theATIS domain, ?list?
typically occurs in imperativesentences, such as ?List all flights.
?Focusing exclusively on the tagger drastically re-duced the amount of data necessary for re-training.Whereas the Charniak parser was originally trainedon close to 40,000 sentences, we only required 150sentences for re-training.
Unfortunately, the re-trained parser still made errors when solving dif-ficult syntactic problems, most notably prepositionattachment and preposition ellipsis.
PRECISE cor-rects both types of errors using semantic informa-tion.We refer to PRECISE?s use of semantic informa-tion to correct parser errors as semantic over-rides.Specifically, PRECISE detects that an attachment de-cision made by the parser is inconsistent with thesemantic information in its lexicon.4 When this oc-curs, PRECISE attempts to repair the parse tree asfollows.
Given a noun phrase or a prepositionalphrase whose corresponding node n in the parse treehas the wrong parent p, PRECISE traverses the pathin the parse tree from p to the root node, search-ing for a suitable node to attach n to.
PRECISEchooses the first ancestor of p such that when n isattached to the new node, the modified parse treeagrees with PRECISE?s semantic model.
Thus, thesemantic over-ride procedure is a generate-and-testsearch where potential solutions are generated in theorder of ancestors of node n in the parse tree.
Theprocedure?s running time is linear in the depth of theparse tree.Consider, for example, the question ?What areflights from Boston to Chicago on Monday??
Theparser attaches the prepositional phrase ?on Mon-day?
to ?Chicago?
whereas it should be attached to?flights?
(see Figure 1).
The parser merely knowsthat ?flights?, ?Boston?, and ?Chicago?
are nouns.
Itthen uses statistics to decide that ?on Monday?
ismost likely to attach to ?Chicago?.
However, thissyntactic decision is inconsistent with the semanticinformation in PRECISE?s lexicon ?
the preposition?on?
does not take a city and a day as arguments,rather it takes a flight and a day.Thus, PRECISE decides to over-ride the parserand attach ?on?
elsewhere.
As shown in Figure1, PRECISE detects that the parser attached the PP?on Monday?
to ?Chicago?
in error.
PRECISE at-tempts to re-attach ?on Monday?
first to the PP ?toChicago?, and then to the NP ?flights from Bostonto Chicago?, where it belongs.
While in our ex-ample the parser violated a constraint in PRECISE?slexicon, the violation of any semantic constraint willtrigger the over-ride procedure.In the above example, we saw how semantic over-rides help PRECISE fix prepositional attachment er-rors; they also enable it to correct parser errorsin topicalized questions (e.g., ?What are Boston toChicago flights??)
and in preposition ellipsis (e.g.,when ?on?
is omitted in the question ?What areflights from Boston to Chicago Monday??
).Unfortunately, semantic over-rides do not correctall of the parser?s errors.
Most of the remainingparser errors fall into the following categories: rel-ative clause attachment, verb attachment, numeric4We say that node n is attached to node p if p is the parentof n in the parse tree.noun phrases, and topicalized prepositional phrases.In general, semantic over-rides can correct local at-tachment errors, but cannot over-come more globalproblems in the parse tree.
Thus, PRECISE can beforced to give up and ask the user to paraphrase herquestion.3 PRECISE TheoryThe aim of this section is to explain the theoreticalunder-pinnings of PRECISE?s semantic model.
Weshow that PRECISE always answers questions fromthe class of Semantically Tractable (ST) questionscorrectly, given correct lexical and syntactic infor-mation.5We begin by introducing some terminology thatbuilds on the definitions given Section 2.3.1 DefinitionsA join path is a set of equality constraints betweena sequence of database relations.
More formally, ajoin path for relations R1, .
.
.
, Rn is a set of con-straints C ?
{Ri.a = Ri+1.b|1 ?
i ?
n?1}.
Herethe notation Ri.a refers to the value of attribute a inrelation Ri.We say a relation between token set T and a setof database elements and join paths E respects alexicon L if it is a subset of M .A question is simply a string of characters.
A to-kenization of a question (with respect to a lexicon)is an ordered set of strings such that each elementof the tokenization is an element of the lexicon?s to-ken set, and the concatenation of the elements of thetokenization, in order, is equal to the original ques-tion.
For a given lexicon and question, there maybe zero, one, or several tokenizations.
Any questionthat has at least one tokenization is tokenizable.An attachment function is a function FL,q : T ?T , where L is the lexicon, q is a question, and Tis the set of tokens in the lexicon.
The attachmentfunction is meant to represent dependency informa-tion available to PRECISE through a parser.
Forexample, if a question includes the phrase ?restau-rants in Seattle?, the attachment function would at-tach ?Seattle?
to ?restaurants?
for this question.
Notall tokens are attached to something in every ques-tion, so the attachment function is not a total func-tion.
We say that a relation R between tokensin a question q respects the attachment function if?t1, t2, R(t1, t2) ?
(FL,q(t1) = t2) ?
(FL,q doesnot take on a value for t1).5We do not claim that NLI users will restrict their questionsto the ST subset of English in practice, but rather that identify-ing classes of questions as semantically tractable (or not), andexperimentally measuring the prevalence of such questions, isa worthwhile avenue for NLI research.In an NLI, interpretations of a question are SQLstatements.
We define a valid interpretation of aquestion as being an SQL statement that satisfies anumber of conditions connecting it to the tokens inthe question.
Because of space constraints, we pro-vide only one such constraint as an example: Thereexists a tokenization t of the question and a set ofdatabase elements E such that there is a one-to-onemap from t to E respecting the lexicon, and for eachvalue element v ?
E, there is exactly one equalityconstraint in the SQL clause that uses v.For a complete definition of a valid interpretation,see (Popescu et al, 2003).3.2 Semantic Tractability ModelIn this section we formally define the class ofST questions, and show that PRECISE can prov-ably map such questions to the corresponding SQLqueries.
Intuitively, ST questions are ?easy to un-derstand?
questions where the words or phrasescorrespond to database elements or constraints onjoin paths.
Examining multiple questions sets anddatabases, we have found that nouns, adjectives, andadverbs in ?easy?
questions refer to database rela-tions, attributes, or values.Moreover, the attributes and values in a question?pair up?
naturally to indicate equality constraints inSQL.
However, values may be paired with implicitattributes that do not appear in the question (e.g., theattribute ?cuisine?
in ?What are the Chinese restau-rants in Seattle??
is implicit).
Interestingly, there isno notion of ?implicit value?
?
the question ?Whatare restaurants with cuisine in Seattle??
does notmake sense.A preposition indicates a join between the rela-tions corresponding to the arguments of the prepo-sition.
For example, consider the preposition ?from?in the question ?what airlines fly from Boston toChicago??
?from?
connects the value ?Boston?
(inthe relation ?cities?)
to the relation ?airlines?.
Thus,we know that the corresponding SQL query will join?airlines?
and ?cities?.We formalize these observations about questionsbelow.
We say that a question q is semanticallytractable using lexicon L and attachment functionFL,q if:1.
It is possible to split q up into words andphrases found in L. (More formally, q is to-kenizable according to L.)2.
While words may have multiple meanings inthe lexicon, it must be possible to find a one-to-one correspondence between tokens in thequestion and some set of database elements.
(More formally, there exists a tokenization tand a set of database elements and join pathsEt such that there is a bijective function f fromt to Et that respects L.)3.
There is at least one such set Et that has exactlyone wh-value.4.
It is possible to add ?implicit?
attributes to Etto get a set E ?t with exactly one compatibleattribute for every value.
(More formally, forsome Et with a wh-value there exist attributesa1, .
.
.
, an such that E ?t = Et ?
{a1, .
.
.
, an}and there is a bijective function g from the setof value elements (including wh-values) V tothe set of attribute elements A in E ?t.)5.
At least one such E ?t obeys the syntacticrestrictions of FL,q.
(More formally, letA?
= A ?
Et.
Then we require that{(f?1(g?1(a)), f?1(a)) | a ?
A?}
respectsFL,q.
)3.3 Results and DiscussionWe say that an NLI is sound for a class of questionsQ using lexicon L and attachment function FL iffor every input q ?
Q, every output of the NLI is avalid interpretation.
We say the NLI is complete ifit returns all valid interpretations.
Our main result isthe following:Theorem 1 Given a lexicon L and attachmentfunction FL, PRECISE is sound and complete for theclass of semantically tractable questions.In practical terms, the theorem states that givencorrect and complete syntactic and lexical informa-tion, PRECISE will return exactly the set of validinterpretations of a question.
If PRECISE is missingsyntactic or semantic constraints, it can generate ex-traneous interpretations that it ?believes?
are valid.Also, if a person uses a term in a manner incon-sistent with PRECISE?s lexicon, then PRECISE willinterpret her question incorrectly.
Finally, PRECISEwill not answer a question that contains words ab-sent from its lexicon.The theorem is clearly an idealization, but the ex-periments reported in Section 4 provide evidencethat it is a useful idealization.
PRECISE, which em-bodies the model of semantic tractability, achievesvery high accuracy because in practice it either hascorrect and complete lexical and syntactic informa-tion or it has enough semantic information to com-pensate for its imperfect inputs.
In fact, as we ex-plained in Section 2.1, PRECISE?s semantic modelenables it to correct parser errors in some cases.Finding all the valid interpretations for a questionis computationally expensive in the worst case (evenjust tokenizing a question is NP-complete (Popescuet al, 2003)).
Moreover, if the various syntac-tic and semantic constraints are fed to a standardconstraint solver, then the problem of finding evena single valid interpretation is exponential in theworst case.
However, we have been able to formu-late PRECISE?s constraint satisfaction problem as agraph matching problem that is solved in polyno-mial time by the MaxFlow algorithm:Theorem 2 For lexicon L, PRECISE finds one validinterpretation for a tokenization T of a semanticallytractable question in time O(Mn2), where n is thenumber of tokens in T and M is the maximum num-ber of interpretations that a token can have in L.4 Experimental EvaluationSemantic Tractability (ST) theory and PRECISE?sarchitecture raise a four empirical questions thatwe now address via experiments on the ATIS dataset (Price, 1990): how prevalent are ST questions?How effective is PRECISE in mapping ATIS ques-tions to SQL queries?
What is the impact of se-mantic over-rides?
What is the impact of parser re-training?
Our experiments utilized the 448 context-independent questions in the ATIS ?Scoring Set A?.We chose the ATIS data set because it is a standardbenchmark (see Table 2) where independently gen-erated questions are available to test the efficacy ofan NLI.We found that 95.8% of the ATIS questions wereST questions.
We classified each question as ST(or not) by running PRECISE on the question andSystem Setup PRECISE PRECISE-1ParserORIG 61.9% 60.3%ParserORIG+ 89.7% 85.5%ParserTRAINED 92.4% 88.2%ParserTRAINED+ 94.0% 89.2%ParserCORRECT 95.8% 91.9%Table 1: Impact of Parser Enhancements.
The PRECISEcolumn records the percentage of questions where the smallset of SQL queries returned by PRECISE contains the cor-rect query; PRECISE-1 refers to the questions correctly in-terpreted if PRECISE is forced to return exactly one SQLquery.
ParserORIG is the original version of the parser,ParserTRAINED is the version re-trained for the ATIS do-main, and ParserCORRECT is the version whose output iscorrected manually.
System configurations marked by +indicate the automatic use of semantic over-rides to correctparser errors.PRECISE PRECISE-1 AT&T CMU MIT SRI BBN UNISYS MITRE HEY94.0% 89.1% 96.2% 96.2% 95.5% 93% 90.6% 76.4% 69.4% 92.5%Table 2: Accuracy Comparison between PRECISE , PRECISE-1 and the major ATIS NLIs.
Only PRECISE and the HEY NLIare database independent.
All results are for performance on the context-independent questions in ATIS.recording its response.
Intractable questions weredue to PRECISE?s incomplete semantic informa-tion.
Consider, for example, the ATIS request ?Listflights from Oakland to Salt Lake City leaving aftermidnight Thursday.?
PRECISE fails to answer thisquestion because it lacks a model of time, and socannot infer that ?after midnight Thursday?
means?early Friday morning.
?In addition, we found that the prevalence of STquestions in the ATIS data is consistent with our ear-lier results on the set of 1,800 natural language ques-tions compiled by Ray Mooney in his experimentsin three domains (Tang and Mooney, 2001).
As re-ported in (Popescu et al, 2003), we found that ap-proximately 80% of Mooney?s questions were ST.PRECISE performance on the ATIS data was alsocomparable to its performance on the Mooney datasets.Table 1 quantifies the impact of the parser en-hancements discussed in Section 2.1.
Since PRE-CISE can return multiple distinct SQL queries whenit judges a question to be ambiguous, we report itsresults in two columns.
The left column (PRECISE)records the percentage of questions where the setof returned SQL queries contains the correct query.The right column (PRECISE-1) records the percent-age of questions where PRECISE is correct if it isforced to return exactly one query per question.
Inour experiments, PRECISE returned a single query92.4% of the time, and returned two queries the restof the time.
Thus, the difference between the twocolumns is not great.Initially, plugging the Charniak parser into PRE-CISE yielded only 61.9% accuracy.
Introducing se-mantic over-rides to correct prepositional attach-ment and preposition ellipsis errors increased PRE-CISE?s accuracy to 89.7% ?
the parser?s erroneousPOS tags still led PRECISE astray in some cases.After re-training the parser on 150 POS-taggedATIS questions, but without utilizing semantic over-rides, PRECISE achieved 92.4% accuracy.
Combin-ing both re-training and semantic over-rides, PRE-CISE achieved 94.0% accuracy.
This accuracy isclose to the maximum that PRECISE can achieve,given its incomplete semantic information?
wefound that, when all parsing errors are corrected byhand, PRECISE?s accuracy is 95.8%.To assess PRECISE?s performance, we comparedit with previous work.
Table 2 shows PRECISE?saccuracy compared with the most successful ATISNLIs (Minker, 1998).
We also include, for com-parison, the more recent database-independent HEYsystem (He and Young, 2003).
All systems werecompared on the ATIS scoring set ?A?, but wedid ?clean?
the questions by introducing sentencebreaks, removing verbal errors, etc..
Since we couldadd modules to PRECISE to automatically handlethese various cases, we don?t view this as signifi-cant.Given the database-specific nature of most previ-ous ATIS systems, it is remarkable that PRECISE isable to achieve comparable accuracy.
PRECISE doesreturn two interpretations a small percentage of thetime.
However, even when restricted to returninga single interpretation, PRECISE-1 still achieved animpressive 89.1% accuracy (Table 1).5 Related WorkWe discuss related work in three categories:Database-independent NLIs, ATIS-specific NLIs,and sublanguages.Database-independent NLIs There has been ex-tensive previous work on NLIs (Androutsopoulos etal., 1995), but three key elements distinguish PRE-CISE.
First, we introduce a model of ST questionsand show that it produces provably correct inter-pretations of questions (subject to the assumptionsof the model).
We measure the prevalence of STquestions to demonstrate the practical import of ourmodel.
Second, we are the first to use a statisticalparser as a ?plug in?, experimentally measure itsefficacy, and analyze the attendant challenges.
Fi-nally, we show how to leverage our semantic modelto correct parser errors in difficult syntactic cases(e.g., prepositional attachment).
A more detailedcomparison of PRECISE with a wide range of NLIsystems appears in (Popescu et al, 2003).
Theadvances in this paper over our previous one in-clude: reformulation of ST THEORY, the parser re-training, semantic over-rides, and the experimentstesting PRECISE on the ATIS data.ATIS NLIs The typical ATIS NLIs used eitherdomain-specific semantic grammars (Seneff, 1992;Ward and Issar, 1996) or stochastic models that re-quired fully annotated domain-specific corpora forreliable parameter estimation (Levin and Pieraccini,1995).
In contrast, since it uses its model of se-mantically tractable questions, PRECISE does notrequire heavy manual processing and only a smallnumber of annotated questions.
In addition, PRE-CISE leverages existing domain-independent pars-ing technology and offers theoretical guarantees ab-sent from other work.
Improved versions of ATISsystems such as Gemini (Moore et al, 1995) in-creased their coverage by allowing an approximatequestion interpretation to be computed from themeanings of some question fragments.
Since PRE-CISE focuses on high precision rather than recall, weanalyze every word in the question and interpret thequestion as a whole.
Most recently, (He and Young,2003) introduced the HEY system, which learns asemantic parser without requiring fully-annotatedcorpora.
HEY uses a hierarchical semantic parserthat is trained on a set of questions together withtheir corresponding SQL queries.
HEY is similar to(Tang and Mooney, 2001).
Both learning systemsrequire a large set of questions labeled by their SQLqueries?an expensive input that PRECISE does notrequire?and, unlike PRECISE, both systems can-not leverage continuing improvements to statisticalparsers.Sublanguages The early work with the most sim-ilarities to PRECISE was done in the field of sublan-guages.
Traditional sublanguage work (Kittredge,1982) has looked at defining sublanguages for var-ious domains, while more recent work (Grishman,2001; Sekine, 1994) suggests using AI techniquesto learn aspects of sublanguages automatically.
Ourwork can be viewed as a generalization of tradi-tional sublanguage research.
We restrict ourselvesto the semantically tractable subset of English ratherthan to a particular knowledge domain.
Finally, inaddition to offering formal guarantees, we assess theprevalence of our ?sublanguage?
in the ATIS data.6 ConclusionThis paper is the first to provide evidence that sta-tistical parsers can support NLIs such as PRECISE.We identified the quandary associated with appro-priately training a statistical parser: without specialtraining for each database, the parser makes numer-ous errors, but creating a massive, labeled corpus ofquestions for each database is prohibitively expen-sive.
We solved this quandary via light re-trainingof the parser?s tagger and via PRECISE?s semanticover-rides, and showed that in concert these meth-ods enable PRECISE to rise from 61.9% accuracy to94% accuracy on the ATIS data set.
Even thoughPRECISE is database independent, its accuracy iscomparable to the best of the database-specific ATISNLIs developed in previous work (Table 2).ReferencesI.
Androutsopoulos, G. D. Ritchie, and P. Thanisch.1995.
Natural Language Interfaces to Databases - AnIntroduction.
In Natural Language Engineering, vol1, part 1, pages 29?81.E.
Charniak.
2000.
A Maximum-Entropy-InspiredParser.
In Proc.
of NAACL-2000.R.
Grishman.
2001.
Adaptive information extractionand sublanguage analysis.
In Proc.
of IJCAI 2001.B.J.
Grosz, D. Appelt, P. Martin, and F. Pereira.
1987.TEAM: An Experiment in the Design of Trans-portable Natural Language Interfaces.
In Artificial In-telligence 32, pages 173?243.Y.
He and S. Young.
2003.
A data-driven spoken lan-guage understanding system.
In IEEE Workshop onAutomatic Speech Recognition and Understanding.R.
Kittredge.
1982.
Variation and homogeneity of sub-languages.
In R. Kittredge and J. Lehrberger, editors,Sublanguage: Studies of Language in Restricted Se-mantic Domains, pages 107?137.
de Gruyter, Berlin.E.
Levin and R. Pieraccini.
1995.
Chronus, the next gen-eration.
In Proc.
of the DARPA Speech and NaturalLanguage Workshop, pages 269?271.W.
Minker.
1998.
Evaluation methodologies for inter-active speech systems.
In First International Confer-ence on Language Resources and Evaluation, pages801?805.R.
Moore, D. Appelt, J. Dowding, J. M. Gawron, andD.
Moran.
1995.
Combining linguistic and statisticalknowledge sources in natural-language processing foratis.
In Proc.
of the ARPA Spoken Language Technol-ogy Workshop.A.
Popescu, O. Etzioni, and H. Kautz.
2003.
Towards atheory of natural language interfaces to databases.
InProc.
of IUI-2003.P.
Price.
1990.
Evaluation of spoken language systems:the atis domain.
In Proc.
of the DARPA Speech andNatural Language Workshop, pages 91?95.S.
Sekine.
1994.
A New Direction For SublanguageNLP.
In Proc.
of the International Conference on NewMethods in Language Processing, pages 165?177.S.
Seneff.
1992.
Robust parsing for spoken languagesystems.
In Proc.
of the IEEE International Confer-ence on Acoustics, Speech and Signal Processing.L.R.
Tang and R.J. Mooney.
2001.
Using MultipleClause Constructors in Inductive Logic Programmingfor Semantic Parsing.
In Proc.
of the 12th Eu-ropean Conference on Machine Learning (ECML-2001), Freiburg, Germany, pages 466?477.W.
Ward and S. Issar.
1996.
Recent improvements in thecmu spoken language understanding system.
In Proc.of the ARPA Human Language Technology Workshop,pages 213?216.
